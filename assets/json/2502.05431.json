{
    "paper_title": "APE: Faster and Longer Context-Augmented Generation via Adaptive Parallel Encoding",
    "authors": [
        "Xinyu Yang",
        "Tianqi Chen",
        "Beidi Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Context-augmented generation (CAG) techniques, including RAG and ICL, require the efficient combination of multiple contexts to generate responses to user queries. Directly inputting these contexts as a sequence introduces a considerable computational burden by re-encoding the combined selection of contexts for every request. To address this, we explore the promising potential of parallel encoding to independently pre-compute and cache each context's KV states. This approach enables the direct loading of cached states during inference while accommodating more contexts through position reuse across contexts. However, due to misalignments in attention distribution, directly applying parallel encoding results in a significant performance drop. To enable effective and efficient CAG, we propose Adaptive Parallel Encoding ($\\textbf{APE}$), which brings shared prefix, attention temperature, and scaling factor to align the distribution of parallel encoding with sequential encoding. Results on RAG and ICL tasks demonstrate that APE can preserve 98% and 93% sequential encoding performance using the same inputs while outperforming parallel encoding by 3.6% and 7.9%, respectively. It also scales to many-shot CAG, effectively encoding hundreds of contexts in parallel. Efficiency evaluation shows that APE can achieve an end-to-end 4.5$\\times$ speedup by reducing 28$\\times$ prefilling time for a 128K-length context."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 ] . [ 1 1 3 4 5 0 . 2 0 5 2 : r APE: Faster and Longer Context-Augmented Generation via Adaptive Parallel Encoding Xinyu Yang, Tianqi Chen, Beidi Chen Carnegie Mellon University, Nvidia Context-augmented generation (CAG) techniques, including RAG and ICL, require the efficient combination of multiple contexts to generate responses to user queries. Directly inputting these contexts as sequence introduces considerable computational burden by re-encoding the combined selection of contexts for every request. To address this, we explore the promising potential of parallel encoding to independently pre-compute and cache each contexts KV states. This approach enables the direct loading of cached states during inference while accommodating more contexts through position reuse across contexts. However, due to misalignments in attention distribution, directly applying parallel encoding results in significant performance drop. To enable effective and efficient CAG, we propose Adaptive Parallel Encoding (APE), which brings shared prefix, attention temperature, and scaling factor to align the distribution of parallel encoding with sequential encoding. Results on RAG and ICL tasks demonstrate that APE can preserve 98% and 93% sequential encoding performance using the same inputs while outperforming parallel encoding by 3.6% and 7.9%, respectively. It also scales to many-shot CAG, effectively encoding hundreds of contexts in parallel. Efficiency evaluation shows that APE can achieve an end-to-end 4.5 speedup by reducing 28 prefilling time for 128K-length context. Github: https://github.com/Infini-AI-Lab/APE Website: https://infini-ai-lab.github.io/APE-Page"
        },
        {
            "title": "Introduction",
            "content": "Recent advances in context-augmented generation (CAG) techniques, particularly retrieval-augmented generation (RAG) (Gupta et al., 2024; Gao et al., 2023) and in-context learning (ICL) (Dong et al., 2022; Wei et al., 2022), have been widely adopted in large language models (LLMs) (Dubey et al., 2024; Achiam et al., 2023), improving their ability to generalize to unseen tasks with contextual information, as demonstrated in Figure 1 (top). These techniques employ sequential encoding process to ground LLM inputs with knowledge from external sources: concatenating the retrieved texts into one sequence, and encoding the sequence into key-value (KV) states as the context for subsequent queries. While this new, significantly longer input improves performance, the increased latency in context prefilling becomes bottleneck in tasks that require long inputs but generate short outputs (Bai et al., 2023; Agarwal et al., 2024; Jiang et al., 2024b). For example, prefilling 128K context takes 17 seconds, whereas generating 256 tokens requires only 6 seconds. This discrepancy leaves significant room to improve the practical efficiency of CAG systems in real-world deployments (Liu, 2022; Chase, 2022). Since texts for CAG are typically stored independently in external databases (Zayarni et al., 2024; Douze et al., 2024), pre-caching all these texts for direct loading during inference offers brute-force approach to accelerate CAG. However, for autoregressive LLMs, the KV states are inherently context-dependent. This dependency makes naive pre-caching impractical, as it would require caching all possible context permutations, leading to factorial growth in memory requirements as the database size increases. For instance, caching all permutations of just ten 256-token text chunks for the LLaMA-3-8B model would demand an impractical 22 PB of memory. To address this issue, parallel encoding (Ratner et al., 2022; Yen et al., 2024; Li et al., 2024; Sun et al., 2024) is introduced to encode each context into KV states separately, ensuring that tokens from different contexts cannot attend to each other during encoding. Next, the on-the-fly generation starts by prefilling user queries, which can attend to the cached KV states from all contexts without re-encoding, offering two benefits: Pre-caching Contexts for Fast Inference: Texts from external sources can be pre-computed and cached 1 Figure 1 Overview of Our Approach. Context-augmented generation leverages additional contexts to improve LLM response quality to user queries. Sequential encoding prefills selected context chunks as long sequence during inference, leading to high latency from on-the-fly re-encoding and low accuracy due to context window limitations. Parallel encoding offers an alternative method to pre-compute more and longer contexts within the same positional range but results in worse performance. To address these challenges, we propose Adaptive Parallel Encoding (APE) to re-align the attention weight distribution of parallel encoding with sequential encoding via three training-free steps: shared prefix, scaling factor, and adaptive temperature, leading to fast and accurate CAG systems in real-world applications. into KV states, which serve as contexts for direct loading during inference. Additionally, this approach allows for cost-free manipulation of contexts, including operations like insertion, deletion, replacement, and swapping. Re-using Positions for Long Context: Contexts can be inserted into the same range of positions in an LLMs context window, allowing for more and longer context chunks. It also mitigates the problem of lost in the middle in context ordering (Liu et al., 2024a), as each context is equally close to the generated tokens. Despite these advantages, parallel encoding leads to significant performance degradation across multiple RAG and ICL scenarios, as shown in Figure 2, with average declines of 4.9% (despite using 2-10 more contexts) and 49.0%, respectively. While prior works (Sun et al., 2024; Yen et al., 2024) have attempted to correct this with fine-tuning, these methods continue to exhibit reduced accuracy in reasoning tasks (e.g., GSM8K). This decrease arises from the limited generalization capability of models fine-tuned on simple tasks to complex ones. However, our results in Figure 2 also reveal that parallel encoding holds promise, as LLMs can still generate reasonable responses due to their inherent alignments with sequential encoding. Based on this observation, we aim to strengthen these alignments while addressing the remaining discrepancies to achieve more accurate parallel encoding. Our insight from Figure 3 and Figure 4 is that KV states from independent contexts can be naturally merged into one sequence due to their similarity in direction and magnitude, attributed to the presence of an attention sink (Xiao et al., 2023). This observation reduces our challenge to addressing residual misalignments, which manifest as anomalous distributions at the initial and recent positions within each context. Motivated by this, we propose Adaptive Parallel Encoding (APE) to align the distribution between sequential and parallel encoding, which enables accurate and fast CAG (see Figure 1 (Bottom)). Our contributions involve: We systematically analyze the distribution properties of attention weights in parallel encoding, focusing on the magnitude and direction of KV states across various samples and positions. Our observations identify major alignments and minor misalignments between parallel and sequential encoding for further improvement. We propose APE to recover the accuracy of parallel encoding with three alignment steps: (i) Prepend shared prefix to avoid the duplication of abnormal distribution of initial tokens. (ii) Adjust lower attention temperature to sharpen the distribution, focusing on contextually important tokens. (iii) Apply scaling factor to offset the increase in the magnitude of the LogSumExp value of attention scores from the context. We empirically show that (i) APE maintains 98% and 93% of the sequential encoding performance in RAG and ICL tasks, respectively. (ii) APE outperforms parallel encoding in RAG and ICL, yielding improvements of 3.6% and 7.9%, respectively. (iii) APE scales to handle hundreds of contexts in parallel, matching or exceeding sequential encoding in many-shot scenarios. (iv) APE accelerates long-context generation, achieving up to 4.5 speedup through 28 reduction in prefilling time for context including 128K tokens."
        },
        {
            "title": "2.1 Context-Augmented Generation",
            "content": "This work explores CAG problems using LLMs, where user queries are enhanced with additional contexts from external databases. CAG typically involves two scenarios: RAG (Asai et al., 2024; Gupta et al., 2024; Gao et al., 2023), which focuses on directly retrieving relevant information, and ICL (Dong et al., 2022; Wei et al., 2022; Agarwal et al., 2024), which emphasizes further acquiring emergent capabilities from in-context examples."
        },
        {
            "title": "2.2 Parallel Encoding",
            "content": "Next, we present the formulation of using parallel encoding in LLMs for CAG settings. Let represent the input sequence including contexts C1, ..., CN and one query Q. Formally, this can be denoted as: = {sC1,1, ..., sC1,l1 , sC2,1, ..., sC2,l2 {z } } Context 2 {z Context 1 }. , sQ,1, ..., sQ,l , ..., sCN ,1, ..., sCN ,lN } {z } Query {z Context (1) For simplicity, we can express this as: = {SC1 , SC2 , . . . , SCN , SQ}. Given two models ΘEnc and ΘDec (which may be the same model), response is generated to the input using parallel encoding in two steps: 1, ..., l2 , we compute its KV states offline as (KCi, VCi ) = ΘEnc(SCi = {kCi,1, . . . , kCi,li } and VCi Pre-caching Contexts. The first step is to encode and cache the KV states for each context independently using ) and store them for ΘEnc. For given context SCi direct loading during inference. Specifically, we denote KCi = {vCi,1, . . . , vCi,li}. Generating Response. Next, the user query is augmented by all relevant pre-cached KV states to generate the response: = ΘDec(SQ, KC, VC), where KC, VC are subsets of {KC1, ..., KCN } and {VC1 , ..., VCN }, respectively. Parallel encoding significantly improves efficiency compared to sequential encoding by reducing the complexity of prefilling from O((l1 + ... + lN + lQ)2) (i.e., quadratic) to linear concerning the total context length. With pre-caching, the cost becomes O((l1 + ... + lN + lQ) lQ). In the absence of pre-caching, the complexity is O(max(l2 Prior parallel encoding approaches vary in their design of ΘEnc and ΘDnc. Parallel Context Windows (PCW) (Ratner et al., 2022) directly employs pre-trained LLMs as both, resulting in significant performance drops. BlockAttention (Sun et al., 2024) further fine-tunes the model, successfully recovering performance in RAG tasks. Alternatively, CEPE (Yen et al., 2024) and FocusLLM (Li et al., 2024) train new Transformer-based encoders using encoder-only and decoder-only architectures, respectively. These methods also differ in ΘDec: CEPE trains additional cross-attention layers for processing contexts, whereas other methods directly input the context into original self-attention layers. While these trainable methods show promising results in RAG tasks, challenges remain regarding their training overheads and generalization abilities to more complex ICL scenarios. Moreover, applying parallel encoding in CAG can be viewed as kind of memory-augmented neural networks (Burtsev et al., 2020; De Jong et al., 2021; Fevry et al., 2020), where external memory is directly stored into KV states. ) + ((l1 + ... + lN + lQ) lQ), which remains efficient for multiple contexts of similar length."
        },
        {
            "title": "2.3 Attention Mechanism",
            "content": "In standard Softmax attention, we attend the query to all past KV states using the following formula: = Softmax( QK )V Rnd K, Rmd, (2) where is the query state, and and denote the key and value states, respectively. Previous research has revealed several significant insights into the distribution of attention weights (i.e., Softmax( QKT )). Attention Sink. StreamingLLM (Xiao et al., 2023) identifies the presence of an attention sink in LLMs, token that receives significantly higher attention score than other tokens but provides limited semantic information. It observes that the attention sink exists in the initial token and influences the following tokens. Position Embedding. To effectively process sequential input, LLMs require position embeddings, such as absolute position embeddings (Vaswani, 2017; Devlin, 2018) and relative position embeddings (Su et al., 2024; Press et al., 2021). However, the introduction of position embedding not only limits the context window to the training length (Chen et al., 2023) but also results in the lost in the middle (Liu et al., 2024a) issue, where LLMs struggle to produce correct answers when relevant information locates in the middle of the context."
        },
        {
            "title": "3 Observations",
            "content": "(a) Retrieval-augmented Generation (b) In-context Learning Figure 2 Comparison of sequential encoding, parallel encoding, and CEPED in RAG and ICL scenarios. Parallel encoding and CEPED degrades performance, especially on tasks such as GSM8K that requires reasoning ability. In Section 3.1, we evaluate sequential encoding, parallel encoding, and CEPE-Distilled (CEPED) (Yen et al., 2024) using the LLaMA-2-7B-chat model1. Figure 2 presents our findings on various RAG and ICL tasks, highlighting the limitations of trainable approaches in generalizing to complex reasoning tasks. Next, we explore the alignments and misalignments between parallel encoding and sequential encoding in Section 3.2, providing insights into why parallel encoding remains effective and identifying opportunities for further improvement."
        },
        {
            "title": "3.1 Trainable Approaches are only Effective for Easy Tasks.",
            "content": "In Figure 2, we compare the performance of different context encoding methods on RAG and ICL tasks, with detailed setups described in Appendix A. Our analysis of the long-context RAG capability on LongBench (Bai et al., 2023) is showcased in Figure 2a. Despite accessing more passages, CEPED only surpasses the sequential baseline in two of the three QA tasks, and it even notably underperforms parallel encoding in the summarization task (MultiNews), which requires synthesizing information from the entire context. We hypothesize that CEPED cannot process complex tasks since the encoder and decoder are only trained on the unlabeled pre-training corpus without instruction-tuning on high-quality QA samples. This conclusion is further supported by the results of ICL tasks (see Figure 2b), where CEPED performs on par with the 1-shot sequential encoding baseline 1We use the LLaMA-2 model for CEPED, as it is the only supported model. For other analyses, we employ LLaMA-3. 4 Figure 3 Top Left: Both LLaMA-3-8B-Instruct (a) and Mistral-7B-Instruct-v0.3 (b) exhibit cosine similarity larger than 0.9 for the key states from distinct initial tokens. Top Right: Initial tokens key states show similar negative values to those from other positions for LLaMA-3-8B-Instruct (c) and Mistral-7B-Instruct-v0.3 (d) models. Bottom: Value states exhibit patterns similar to those observed in key states. The X-axis shows positions of key and value states on logarithmic scale. Visualizations and analyses for more base models are provided in Appendix B. on TriviaQA but falls short of it on GSM8K and MMLU, despite using much more examples. The latter involves reasoning steps that are hard for the ill-trained model to understand. In conclusion, fine-tuning models to improve parallel encoding requires (i) more diverse and labeled data and (ii) resource-intensive instruction-tuning (e.g., SFT or RLHF (Ouyang et al., 2022)). Given this unfavorable trade-off between training costs and model capabilities, we propose developing training-free method to improve the performance of parallel encoding."
        },
        {
            "title": "3.2 Comparing Parallel Encoding and Sequential Encoding.",
            "content": "In Figure 2, we observe that parallel encoding still holds promise, as it can generate reasonable responses without further modifications. This finding is non-trivial as contexts are encoded into KV states separately without guarantee that these states can be compared or combined. However, our analysis reveals that the attention mechanism naturally builds alignments between KV states from different positions in independent contexts similar to sequential encoding. To clarify this, Figure 3 focuses on the impact of the attention sink (Xiao et al., 2023), where we visualize the direction of KV states for different samples and positions. In Figure 4, we further visualize the distribution of various components in the Softmax attention, resulting in several findings. Key states from different contexts are similar. In Figure 3a and 3b, we measure the cosine similarity between the key states of different initial tokens for the LLaMA-3-8B-Instruct and Mistral-7B-Instruct-v0.3 models, which consistently yields value close to 1. This observation indicates that the direction of the initial key state remains invariant mainly across different inputs. Figure 3c and 3d further analyze the similarity between the initial key states and their subsequent states, where we observe comparable negative values from different positions. Therefore, the angles between the initial key states and their subsequent states are similar and significantly larger than the angles between different initial key states, as demonstrated in Figure 5. It suggests that the direction of key states remains relatively consistent across contexts, as they are primarily decided by the initial key states, which exhibit similar directions across examples. These findings, combined with the small variance in key state magnitudes across examples in Figure 4b, indicate Figure 5 Geometry of Key States. 5 (a) Query-Key Similarity (b) Key Magnitude (c) Value Magnitude (d) Query-Key Product Figure 4 Visualization of Different Components in Attention. (a) The cosine similarity between query and key states increases as the distance between their positions decreases. (b) The magnitudes of key states show slowly upward trend as position increases. (c) The magnitude of value states remain constant across positions. (d) Query-key dot products keep consistently low values except at initial and recent positions. red dashed line marks the anomalous region for the first two tokens in all figures. The X-axis shows positions of KV states on log scale. Results are measured with the LLaMA-3-8B-Instruct model. Visualizations and analyses for more base models are provided in Appendix B. that key states from different contexts share similar directions and magnitudes, making them comparable. To further understand this, we experiment on HotPotQA using the LLaMA-3-8B-Instruct model. Our analysis involves applying rotations of varying degrees around random axes to the initial key states. For parallel encoding, we explore two rotation modes: one using the same rotation axis for all contexts and another employing random rotation axis for each context. Figure 6 reveals that sequential encoding keeps performance across various rotation degrees. In contrast, both modes in parallel encoding deteriorate when rotations exceed 150 degrees. This effect arises from the duplication of initial key states, intensifying our rotations impact. Notably, using separate axes for each context leads to an earlier breakdown beginning at 90 degrees. This mode disrupts the directional similarity of key states with different initial tokens (i.e., kinitial) in Figure 5 and enlarges the angle between key states from different contexts. Values states from different contexts can be combined. In Equation (2), all value states are combined through weighted summation, where the Softmax operator would normalize the weights of all value states to sum to 1. This normalization indicates that the magnitude of current value states is determined solely by those from previous positions, resulting in similar L2 norm across positions, as shown in Figure 4c. Additionally, the small variance shows that the magnitudes are comparable among samples. This finding, coupled with similar direction across samples and positions in Figure 3 (Bottom), indicates the possibility of combining value states. Figure 6 Rotation Analysis on the First Token Opportunities for improvement. Despite the KV states exhibiting similarity across contexts for most positions, the residual misalignments in Figure 4 still severely reduce accuracy. We summarize them as follows: In Figure 4, we observe notable discrepancy in direction and magnitude for the initial positions, leading to large QK dot products at these positions in Figure 4d. They are identified as an anomaly in the context. Figure 4d shows the dot products between the query state and all past key states, revealing notable increase when the states are positioned close to each other, as reflected in the larger similarity observed in Figure 4a."
        },
        {
            "title": "4 Adaptive Parallel Encoding",
            "content": "With all the lessons learned in Section 3, we will design our APE to address the residual misalignments. APE enables seamless shift to parallel encoding without requiring training while maintaining most of the models capabilities. Our approach adaptively aligns the distribution of attention weights between sequential and parallel encoding via three steps as illustrated in Figure 1, thereby boosting efficiency and performance. 6 (a) Sequential (b) Parallel (T = 1.0) (c) Parallel (T = 0.2) (d) Parallel vs. Sequential Figure 7 Comparison of Attention Weight Distribution within Contexts. (a) Sequential encoding allocates high attention scores to neighboring tokens. (b) Parallel encoding distributes attention scores more uniform across neighboring tokens from all contexts. (c) Adjusting the temperature sparsifies the distribution. (d) After adjustment, the distribution in parallel encoding becomes similar to sequential encoding. The X-axis represents token positions."
        },
        {
            "title": "4.1 Prepending Shared Prefix.",
            "content": "Figure 4 shows that the distribution of the first few tokens differs significantly from that of subsequent tokens. This discrepancy poses challenge when encoding contexts in parallel due to duplicating these abnormal KV states. To address this issue, we propose simple yet effective solution: prepending shared prefix to all contexts. This approach ensures that these KV states appear only once in each generation step. In practice, the choice of prefix varies with the model and task. We use existing system prompts and instructions as the shared prefix when available. Otherwise, we will insert few newline characters (i.e., n) before all contexts. Although the later positions are not identified as abnormal, we still observe instability at the start of LLM inputs. To mitigate this issue, we may also consider extending the existing prefix with more newline characters."
        },
        {
            "title": "4.2 Adjusting Attention Temperature.",
            "content": "In Figure 4d, the value of QK dot products increases as the relative distance decreases, with notably sharper rise when the distance approaches zero. To show its impact on parallel encoding, we set 50-token prefix and query, encoding the remaining 900 tokens either sequentially or in five parallel chunks, with attention distributions shown in Figure 7. Comparing Figure 7b with 7a, duplicating neighboring KV states in the parallel encoding will disperse the querys attention to multiple contexts, resulting in more uniform distribution. We adjust the attention temperature to value less than 1 to refocus on the most relevant tokens, sharpening the distribution after the Softmax operation. The comparison between different is shown in Figure 7c and 7d."
        },
        {
            "title": "4.3 Adding Scaling Factor.",
            "content": "While adjusting the temperature sharpens the attention distribution among context tokens, it will also alter the overall attention allocated to the whole context, as indicated by the LogSumExp value in Figure 8. Specifically, when the sum of the original QK dot product values in given layer is significantly greater than 0, reducing temperature amplifies these positive values, resulting in an increased, positive LogSumExp value. Conversely, when the sum is closer to 0, lowering temperature has stronger effect on the negative QK dot products, leading to decreased, negative LogSumExp value. These effects generally increase the absolute value of LogSumExp(QK). To compensate for these changes, we introduce scaling factor < 1 to reduce this absolute value."
        },
        {
            "title": "4.4 Formulation.",
            "content": "Given these three steps, we can formulate the modified attention in APE. We begin with the standard Softmax attention, where Q, K, and are the query, key, and value states, respectively. We use the subscript Ci for elements from the context Ci, while those without subscript correspond to user queries or generated texts. Figure 8 Parallel w/ Different . 7 = Softmax Q[K C1 , . . . , CN ! , ] [VC1 , . . . , VCN , ] [AC1 , . . . , ACN , A] PlCi j=1 aCi,j + Pl = PN i=1 j=1 aj Qk [VC1 , . . . , VCN , ], where ACi = [exp Qk Ci,1 , . . . , exp Ci,lCi ] and aCi,j = exp Qk Ci,j . Similar for and aj. After incorporating the proposed changes, the formula for our refined attention calculation becomes: = [AP , j=1 aP,j + (PN i=1 C"
        },
        {
            "title": "PlP",
            "content": ", . . . , PlCi j=1 CN Ci,j , A] )S + Pl j=1 aj [VP , VC1, . . . , VCN , ], (3) (4) (5) where Ci = [exp Qk Ci,1 , . . . , exp Qk Ci,lCi X ] ( lCiX i=1 j=1 Ci,j )S1 and Ci,j = exp Qk Ci,j . Here, AP represents the attention weights for the shared prefix, respectively. The attention temperature and the scaling factor for the context are less than 1 (T < 1, < 1). Appendix provides detailed deduction of this formula for better understanding. All these modifications are compatible with fast attention implementations such as flash attention (Dao et al., 2022) by computing the context and non-context KV states separately and merging them into the final attention output. This process will only incur negligible overhead. For the choice of hyperparameters, we conduct greedy search over small validation set. If no prefix is provided, we begin by adding two and increase the prefix length by 10, 20, and 40. and are searched in the ranges [0.1, 1.0] using 0.1 step sizes. We use instead of as the scaling factor to simplify our search."
        },
        {
            "title": "5 Experiments",
            "content": "Empirically, we present the effectiveness and efficiency of APE in CAG scenarios such as RAG and ICL. Since we focus on context encoding problems, we do not include comparisons with long-context LLMs. Specifically, In Section 5.1, APE can maintain 98% of the accuracy on ChatRAG-Bench compared to sequential encoding. Furthermore, it improves 3.3% performance for RAG on LongBench by retrieving more and longer contexts. In Section 5.2, APE outperforms parallel encoding by 7.9% on average in three ICL tasks. Moreover, APE can maintain 93% of the accuracy achieved by sequential encoding when using the same number of examples. In Section 5.3, APE can scale to many-shot CAG tasks, effectively encoding hundreds of texts in parallel. In Section 5.4, APE achieves 4.5 faster inference for 128k context through 28 reduction in prefilling time."
        },
        {
            "title": "5.1 Retrieval-Augmented Generation.",
            "content": "In the context of RAG tasks, we validate that APE retains most of the sequential encoding capability while accommodating more and longer contexts, mitigating retrieval errors, and outperforming encoding baselines. 5.1.1 Retrieval for Multi-turn Question Answering. Setup. APE is evaluated on five conversational QA tasks using ChatRAGBench (Liu et al., 2024b). For each query, we prepare about 100 text chunks. Three retrievers of varying quality are employed to retrieve up to the top-5 chunks for evaluation, including Contriever (Izacard et al., 2021), GTE-base Li et al. (2023), and Dragon-multiturn Liu et al. (2024b). We use Llama3-ChatQA-1.5-8B as the base model. To fairly measure performance drop after our modifications, the same retrieved texts are used for APE and sequential encoding. 8 Table 1 Comparison between APE and sequential encoding using three retrievers on ChatRAG-Bench."
        },
        {
            "title": "INSCIT",
            "content": "Doc2Dial"
        },
        {
            "title": "Average",
            "content": "Contriever, Sequential Contriever, APE GTE-base, Sequential GTE-base, APE Dragon-multiturn, Sequential Dragon-multiturn, APE All texts, APE 19.97 19.88 -0.09 21.58 20.85 -0. 25.42 23.84 -1.58 27.22 23.85 23.28 -0.57 32.35 30.99 -1.36 36.27 34.93 -1.34 36. 30.49 28.84 -1.65 33.41 31.92 -1.49 36.10 33.80 -2.30 35.72 46.75 46.28 -0.47 46.54 45.83 -0. 49.01 48.70 -0.31 26.57 26.80 +0.23 30.69 30.35 -0.34 35.12 34.92 -0.20 29.53 29.02 -0.51 32.91 31.99 -0. 36.38 35.24 -1.14 49.15 35.70 36.78 Results. Table 1 shows that switching from sequential encoding to APE results in performance drops of 0.51%, 0.92%, and 1.14% across different retrievers, respectively. While this drop increases with retriever quality, APE still keeps 97% of the sequential encoding performance for the best retriever. By increasing the text chunk length for 5 times, APE directly inputs all texts without any retrieval process, achieving superior performance. 5.1.2 Retrieval for Long-context Understanding. Setup. Our evaluation involves eight tasks on LongBench (Bai et al., 2023). Given the long context, we split it into chunks with size of words, employ Contriever (Izacard et al., 2021) to compute the embeddings of all chunks and the query and retrieve the top-N chunks according to the cosine similarity of their embeddings to the query embedding. and vary across different methods. We compare APE with sequential encoding with and without RAG, and PCW, using Llama-3-8B-Instruct (Dubey et al., 2024), Mistral-7B-Instructv0.3 (Jiang et al., 2023), Gemma-2-9b-it (Team et al., 2024), and Llama-3.1-8B-Instruct as base models. Results. In Table 2, APE consistently improves performance across all models, achieving 5.6% average gain over sequential encoding without RAG. It also outperforms sequential RAG baselines by 3.3% by retrieving more and longer contexts. The superior performance over PCW further showcases the effectiveness of our modifications in APE. Notably, APE surpasses the 128K-context variant of the Llama-3.1-8B-Instruct model by placing retrieved texts within the 8K context window, mitigating the lost in the middle phenomenon."
        },
        {
            "title": "5.2 In-context Learning",
            "content": "Setup. We evaluate APE on three ICL tasks using the LM Evaluation Harness (Gao et al., 2024) codebase: GSM8K (8-shot) (Cobbe et al., 2021a), TriviaQA (5-shot) (Joshi et al., 2017), and MMLU (5-shot) (Hendrycks et al., 2020a). Experiments are conducted using the same base models as in our LongBench evaluations. We compare parallel encoding (PCW) to show the improvement of APE. Sequential encoding with varying numbers of shots (i.e., 1-shot, half-shots, and full-shots) is also employed to measure the gap from the ideal scenarios. Results. In Figure 9, APE surpasses parallel encoding with average improvements of 15.4% on GSM8K, 4.7% on TriviaQA, and 3.5% on MMLU. When compared with the 1-shot sequential baseline with similar context length, our method consistently yields superior results. Moreover, APE performs better than half-shot sequential encoding in 8/12 settings and preserves 93% accuracy compared to full-shot sequential encoding. Additionally, the Llama family exhibits enhanced compatibility with parallel encoding, potentially due to the stronger directional alignment of initial tokens from different contexts (see Figure 3a). Across different tasks, the performance gap between APE and full-shot sequential encoding is the largest on GSM8K. This finding suggests that while APE keeps most capabilities, its effectiveness may decrease as task complexity increases. 9 Table 2 Comparison between APE and baselines on LongBench across different models using RAG. denotes Contriever, and indicates retrieval of the top-N chunks, each containing words. Model MuSiQue Qasper 2WikiMQA DuRead HotpotQA NarratQA MFQA zh MFQA en Avg. LLaMA-3-8B-Instruct C20020, Sequential C400020, PCW C400020, APE Mistral-7B-Instruct-v0.3 C20020, Sequential C400020, PCW C400020, APE Gemma-2-9b-it C20010, Sequential C200020, PCW C200020, APE LLaMA-3.1-8B-Instruct 128K, Sequential C20020, Sequential C400020, PCW C400020, APE 20.70 27.93 18.82 26.19 10.05 11.58 17.58 20. 22.57 30.69 26.27 33.38 22.18 28.35 30.62 21.23 26.88 41.05 42.71 42.59 42.32 31.08 21.98 35.57 36.81 39.99 42.86 46.69 47.72 46.81 47.20 42.33 41.52 43. 30.02 38.35 40.99 44.43 22.12 24.44 32.97 34.37 48.06 53.55 47.59 49.49 40.58 40.81 44.39 44.87 50.11 9.55 12.65 21.57 23.13 17.68 20.80 18.70 21. 27.40 28.04 23.43 28.43 34.61 33.34 33.51 31.11 32.10 45.90 49.60 47.09 49.71 32.09 32.79 37.05 42.33 47.49 52.05 48.95 56.62 43.97 53.46 49.97 49.47 55. 20.98 22.78 23.29 30.71 19.68 16.06 14.10 20.49 23.11 24.45 27.11 30.41 23.08 30.57 23.87 19.98 30.50 58.54 57.82 54.40 55.03 32.03 34.43 34.69 40. 50.81 50.25 56.69 56.52 61.60 61.97 56.87 60.90 62.02 45.04 48.94 45.05 45.41 40.38 38.40 40.14 44.03 45.35 48.34 49.81 50.84 51.89 53.25 55.14 51.19 52. 33.97 37.60 36.73 39.62 25.64 25.06 28.85 32.55 38.10 41.28 40.82 44.18 38.98 42.24 40.22 38.44 42.86 Table 3 Comparison between APE and sequential encoding in various many-shot RAG and ICL tasks. Retrieval-augmented Generation In-context Learning"
        },
        {
            "title": "ArguAna FEVER",
            "content": "NQ"
        },
        {
            "title": "SciFact Date",
            "content": "Salient Tracking7 Web Sequential, Zero-shot Sequential, Few-shot Sequential, Half-shot Sequential, Full-shot 11.15 11.20 15.34 12.84 7.78 9.78 13.12 14.19 APE, Full-shot 16. 14.70 17.78 17.81 19.64 24.54 21.91 7.74 9.49 16.12 16.88 15.72 20.00 36.64 45.55 46. 8.89 38.89 42.22 46.67 43.33 45.55 1.12 6.67 8.89 8.89 8.89 8.89 38.89 55.56 58. 58."
        },
        {
            "title": "5.3 Many-shot Context-Augmented Generation",
            "content": "Setup. We evaluate the scalability of APE on four RAG and ICL tasks from the LOFT benchmark (Lee et al., 2024), each involving hundreds of additional texts. We employ Llama-3.1-8B-Instruct as our base model to compare APE with sequential encoding, both applied to the same many-shot inputs. The total context lengths for the RAG and ICL tasks are 128K and 32K, respectively. We also include the zero-shot, few-shot ( 5), and half-shot sequential encoding baselines. For metrics, F1 score and EM are used in RAG and ICL tasks. Results. In Table 3, APE achieves performance comparable to sequential encoding when processing the same many-shot long-context inputs, showing its ability to encode hundreds of texts in parallel efficiently. Notably, it outperforms sequential encoding on ArguAna and FEVER for RAG tasks. While APE is expected to reduce performance, it recovers this drop by positioning all texts close to the query, mitigating the lost in the middle problem in long-context LLMs. For ICL tasks, APE can learn from examples as effective as sequential encoding."
        },
        {
            "title": "5.4 Efficiency Evaluation",
            "content": "Setup. We measure the latency for sequential encoding, MInference (Jiang et al., 2024a), and APE usingLlama3.1-8B-Instruct (Dubey et al., 2024) on an H100 GPU with batch sizes of 1 and 4. The query and generation lengths are fixed at 256 tokens, while the context lengths range from 2K to 128K tokens. We employ VLLM (Kwon et al., 2023) as our inference engine and measure both prefilling time and total inference time. Results. Comparing to sequential encoding and MInference, APE can accelerate inference up to 4.5 and 10 (a) Llama-3-8B-Instruct (b) Llama-3.1-8B-Instruct (c) Mistral-7B-Instruct-v0. (d) Gemma-2-9b-it Figure 9 Performance comparison of APE, parallel encoding, and sequential encoding on ICL tasks. (a) Prefill Time (bsz=1) (b) Prefill Time (bsz=4) (c) Total Time (bsz=1) (d) Total Time (bsz=4) Figure 10 Latency on H100 GPU: prefill and total inference time (s). The gray text in brackets is batch size. 2.2 respectively for long-context scenarios in Figure 10. For 128K-token contexts, APE reduces prefilling time by 28 compared to MInference. The prefilling cost of APE exhibits linear scaling and consumes less than 10% of inference time, whereas baselines require over 50% as context length increases. APE also shows superior versatility, while MInference slows inference with additional overhead for short contexts and large batches."
        },
        {
            "title": "6 Analysis",
            "content": "This section presents analyses to answer the following research questions: RQ1: Can APE improve performance for real-world RAG applications? RQ2: How does each component in APE contribute to the performance? RQ3: Can APE extend LLM context window size in long-context scenarios without RAG?"
        },
        {
            "title": "6.1 Can APE improve performance for real-world RAG applications?",
            "content": "In Table 4, we evaluate APE in real-world RAG scenarios using the CRAG benchmark (Yang et al., 2024). Task 1 augments the model with five webpages, while Task 2 provides an additional knowledge graph as another retrieval source. In our experiments, the sequential encoding baseline is limited to retrieving 4K tokens, whereas APE can process 20 parallel segments of 4K tokens each. By incorporating significantly more external texts, 11 APE consistently outperforms sequential encoding with limited context sizes while reducing latency. Moreover, the improvement in Task 2 shows the effectiveness of APE in merging text from multiple sources. Table 4 Performance and latency comparison using the Llama-3-8B-Instruct model on CRAG benchmark."
        },
        {
            "title": "Model",
            "content": "LLM only Llama-3-8B-Instruct Latency (ms) Accuracy (%) Hallucination Missing Scorea -26.83 48.97 22.14 28.90 Task 1 Task 2 Llama-3-8B-Instruct +APE Llama-3-8B-Instruct +APE 1140 1054 1830 23.28 25.53 24.46 27.04 29.49 21.30 28.38 18.74 47.22 37.93 47.15 37. -6.21 -0.41 -3.92 2."
        },
        {
            "title": "6.2 How does each component in APE contribute to the performance?",
            "content": "In Table 5, we conduct an ablation study to examine each component in APE, including the shared prefix (P ), attention temperature (T ), and scaling factor (S). We present results averaged across the four base models evaluated in Figure 9. Our findings indicate that incorporating each of these components can improve performance for all tasks, with average improvements of 5.19%, 0.59%, and 2.07%, respectively. Among them, adding shared prefix leads to the largest improvement, while adjusting the attention temperature yields minimal accuracy gains without the complementary effect of the scaling factor. Table 5 Ablation study of APE components on ICL tasks. : shared prefix, : attention temperature, S: scaling factor. GSM8K TriviaQA MMLU 38.25% 67.99% 63.09% 50.42% 70.76% 63.70% 51.15% 71.03% 64.49% 53.62% 72.64% 66.62%"
        },
        {
            "title": "6.3 Can APE extend context lengths in long-context scenarios without RAG?",
            "content": "Table 6 evaluates the effectiveness of APE in handling single long-context input using the Llama-3-8BInstruct model on the LongBench dataset (Bai et al., 2023). To accommodate the long context within our APE, we split it into multiple segments of less than 7,500 tokens. Additionally, we append the last 500 tokens to the query for two code completion tasks. Our results indicate that APE enhances performance across 10/11 tasks, yielding an average improvement of 6.6% compared to the sequential encoding baseline with limited context window size. More baseline results of long-context LLM approaches are provided in Appendix D. Table 6 Performance comparison across different long-context tasks on LongBench (Bai et al., 2023)."
        },
        {
            "title": "LCC",
            "content": "LLaMA-3-8B-Instruct +APE 19.32 26.87 32.83 39.14 43.38 59.12 27.89 29.10 22.40 23. 53.22 66."
        },
        {
            "title": "Method",
            "content": "RepoBench-P HotpotQA 2WikiMQA MuSiQue MultiNews Average LLaMA-3-8B-Instruct +APE 38.15 49.43 44.24 50.11 21.01 28.06 20.47 25. 23.63 22.40 31.50 38."
        },
        {
            "title": "7 Conclusion",
            "content": "This work explores the potential of parallel encoding in CAG scenarios, which can pre-cache KV states for fast inference and re-use positions for long context but lead to worse performance. To address this, we propose APE, training-free method to enable accurate, fast, and long CAG systems. APE achieves this by aligning the attention weight distribution of parallel encoding with sequential encoding via three steps: shared prefix, adaptive temperature, and scaling factor. Empirically, we show that APE improves accuracy and efficiency in various RAG and ICL tasks while successfully scaling to process hundreds of chunks in parallel for both settings."
        },
        {
            "title": "8 Limitations",
            "content": "While APE shows the effectiveness and efficiency of parallel encoding with only inference-time modification in the attention distribution, it remains sensitive to hyperparameter selection, particularly the attention temperature and scaling factor S. In real-world applications, where contexts vary in length, quantity, and content, aligning the distribution between sequential and parallel encoding automatically presents significant challenge."
        },
        {
            "title": "9 Acknowledgement",
            "content": "This work is supported in part by NSF award CNS-2211882 and gift from Qualcomm. We thank the authors of ChatQA (Liu et al., 2024b), Longbench (Bai et al., 2023), CRAG (Yang et al., 2024), LM Evaluation Harness (Gao et al., 2024), VLLM (Kwon et al., 2023), and MInference (Jiang et al., 2024a) for their useful codebase, benchmark, and models, and Yixin Dong, Hanshi Sun, Zhuoming Chen for their helpful discussions."
        },
        {
            "title": "References",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Rishabh Agarwal, Avi Singh, Lei Zhang, Bernd Bohnet, Stephanie Chan, Ankesh Anand, Zaheer Abbas, Azade Nova, John Co-Reyes, Eric Chu, et al. Many-shot in-context learning. arXiv preprint arXiv:2404.11018, 2024. Akari Asai, Zexuan Zhong, Danqi Chen, Pang Wei Koh, Luke Zettlemoyer, Hannaneh Hajishirzi, and Wen-tau Yih. Reliable, adaptable, and attributable language models with retrieval. arXiv preprint arXiv:2403.03187, 2024. Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. Longbench: bilingual, multitask benchmark for long context understanding. arXiv preprint arXiv:2308.14508, 2023. Mikhail Burtsev, Yuri Kuratov, Anton Peganov, and Grigory Sapunov. Memory transformer. arXiv preprint arXiv:2006.11527, 2020. Harrison Chase. Longchain, 2022. https://github.com/langchain-ai/langchain. Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending context window of large language models via positional interpolation. arXiv preprint arXiv:2306.15595, 2023. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021a. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021b. Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Re. Flashattention: Fast and memory-efficient exact attention with io-awareness. Advances in Neural Information Processing Systems, 35:1634416359, 2022. Michiel De Jong, Yury Zemlyanskiy, Nicholas FitzGerald, Fei Sha, and William Cohen. Mention memory: incorporating textual knowledge into transformers through entity mention attention. arXiv preprint arXiv:2110.06176, 2021. Jacob Devlin. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and Zhifang Sui. survey on in-context learning. arXiv preprint arXiv:2301.00234, 2022. Matthijs Douze, Alexandr Guzhva, Chengqi Deng, Jeff Johnson, Gergely Szilvasy, Pierre-Emmanuel Mazare, Maria Lomeli, Lucas Hosseini, and Herve Jegou. The faiss library. arXiv preprint arXiv:2401.08281, 2024. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Alexander Fabbri, Irene Li, Tianwei She, Suyi Li, and Dragomir Radev. Multi-news: large-scale multi-document summarization dataset and abstractive hierarchical model. arXiv preprint arXiv:1906.01749, 2019. Thibault Fevry, Livio Baldini Soares, Nicholas FitzGerald, Eunsol Choi, and Tom Kwiatkowski. Entities as experts: Sparse memory access with entity supervision. arXiv preprint arXiv:2004.07202, 2020. Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noach, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. framework for few-shot language model evaluation, 07 2024. https://zenodo.org/records/12608602. Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, and Haofen Wang. Retrieval-augmented generation for large language models: survey. arXiv preprint arXiv:2312.10997, 2023. AI Gradient. Llama-3-8b-instruct-262k, 2024. 14 Aman Gupta, Anup Shirgaonkar, Angels de Luis Balaguer, Bruno Silva, Daniel Holstein, Dawei Li, Jennifer Marsman, Leonardo Nunes, Mahsa Rouzbahman, Morris Sharp, et al. Rag vs fine-tuning: Pipelines, tradeoffs, and case study on agriculture. arXiv preprint arXiv:2401.08406, 2024. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020a. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020b. Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. Constructing multi-hop qa dataset for comprehensive evaluation of reasoning steps. arXiv preprint arXiv:2011.01060, 2020. Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. Unsupervised dense information retrieval with contrastive learning. arXiv preprint arXiv:2112.09118, 2021. Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. Huiqiang Jiang, Yucheng Li, Chengruidong Zhang, Qianhui Wu, Xufang Luo, Surin Ahn, Zhenhua Han, Amir Abdi, Dongsheng Li, Chin-Yew Lin, et al. Minference 1.0: Accelerating pre-filling for long-context llms via dynamic sparse attention. arXiv preprint arXiv:2407.02490, 2024a. Ziyan Jiang, Xueguang Ma, and Wenhu Chen. Longrag: Enhancing retrieval-augmented generation with long-context llms. arXiv preprint arXiv:2406.15319, 2024b. Hongye Jin, Xiaotian Han, Jingfeng Yang, Zhimeng Jiang, Zirui Liu, Chia-Yuan Chang, Huiyuan Chen, and Xia Hu. Llm maybe longlm: Self-extend llm context window without tuning. arXiv preprint arXiv:2401.01325, 2024. Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. Triviaqa: large scale distantly supervised challenge dataset for reading comprehension. arXiv preprint arXiv:1705.03551, 2017. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the 29th Symposium on Operating Systems Principles, pages 611626, 2023. Jinhyuk Lee, Anthony Chen, Zhuyun Dai, Dheeru Dua, Devendra Singh Sachan, Michael Boratko, Yi Luan, Sebastien MR Arnold, Vincent Perot, Siddharth Dalmia, et al. Can long-context language models subsume retrieval, rag, sql, and more? arXiv preprint arXiv:2406.13121, 2024. Zehan Li, Xin Zhang, Yanzhao Zhang, Dingkun Long, Pengjun Xie, and Meishan Zhang. Towards general text embeddings with multi-stage contrastive learning. arXiv preprint arXiv:2308.03281, 2023. Zhenyu Li, Yike Zhang, Tengyu Pan, Yutao Sun, Zhichao Duan, Junjie Fang, Rong Han, Zixuan Wang, and Jianyong Wang. Focusllm: Scaling llms context by parallel decoding. arXiv preprint arXiv:2408.11745, 2024. Jerry Liu. Llamaindex, 11 2022. https://github.com/jerryjliu/llama_index. Nelson Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. Lost in the middle: How language models use long contexts. Transactions of the Association for Computational Linguistics, 12:157173, 2024a. Zihan Liu, Wei Ping, Rajarshi Roy, Peng Xu, Chankyu Lee, Mohammad Shoeybi, and Bryan Catanzaro. Chatqa: Surpassing gpt-4 on conversational qa and rag. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024b. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. Zhuoshi Pan, Qianhui Wu, Huiqiang Jiang, Menglin Xia, Xufang Luo, Jue Zhang, Qingwei Lin, Victor Ruhle, Yuqing Yang, Chin-Yew Lin, et al. Llmlingua-2: Data distillation for efficient and faithful task-agnostic prompt compression. arXiv preprint arXiv:2403.12968, 2024. Ofir Press, Noah Smith, and Mike Lewis. Train short, test long: Attention with linear biases enables input length extrapolation. arXiv preprint arXiv:2108.12409, 2021. 15 Nir Ratner, Yoav Levine, Yonatan Belinkov, Ori Ram, Inbal Magar, Omri Abend, Ehud Karpas, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham. Parallel context windows for large language models. arXiv preprint arXiv:2212.10947, 2022. Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. East Sun, Yan Wang, and Lan Tian. Block-attention for efficient rag. arXiv preprint arXiv:2409.15355, 2024. Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Leonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Rame, et al. Gemma 2: Improving open language models at practical size. arXiv preprint arXiv:2408.00118, 2024. AI Together. Llama-2-7b-32k-instruct, 2023. Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. Musique: Multihop questions via single-hop question composition. Transactions of the Association for Computational Linguistics, 10:539554, 2022. Vaswani. Attention is all you need. Advances in Neural Information Processing Systems, 2017. Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models. arXiv preprint arXiv:2206.07682, 2022. Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks. arXiv preprint arXiv:2309.17453, 2023. Xiao Yang, Kai Sun, Hao Xin, Yushi Sun, Nikita Bhalla, Xiangsen Chen, Sajal Choudhary, Rongze Daniel Gui, Ziran Will Jiang, Ziyu Jiang, et al. Cragcomprehensive rag benchmark. arXiv preprint arXiv:2406.04744, 2024. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher Manning. Hotpotqa: dataset for diverse, explainable multi-hop question answering. arXiv preprint arXiv:1809.09600, 2018. Howard Yen, Tianyu Gao, and Danqi Chen. Long-context language modeling with parallel context encoding. arXiv preprint arXiv:2402.16617, 2024. Andre Zayarni, Andrey Vasnetsov, et al. Qdrant, 2024. https://qdrant.tech/."
        },
        {
            "title": "Appendix",
            "content": "A Detailed Experimental Setups for Section 3.1 RAG. We select four tasks that require processing multiple input documents from the LongBench dataset (Bai et al., 2023), including HotpotQA (Yang et al., 2018), 2WikiMultihopQA (Ho et al., 2020), MuSiQue (Trivedi et al., 2022), and MultiNews (Fabbri et al., 2019). The F1 score is used as the evaluation metric for the three QA tasks, while Rouge-L is used for the summarization task. Both parallel encoding and CEPED process each document independently using ΘEnc. For documents that exceed the length limitation of ΘEnc, we split them into multiple chunks for encoding. In sequential encoding, we will truncate lengthy inputs from the middle. ICL. We select three few-shot learning tasks from LM Evaluation Harness (Gao et al., 2024) to evaluate the ICL ability of different encoding methods, involving GSM8K (Cobbe et al., 2021b), TriviaQA Joshi et al. (2017), and MMLU (Hendrycks et al., 2020b). In parallel encoding and CEPED, we will encode each example separately and input all the resulting KV states to ΘDec. For sequential encoding, we use variants with different numbers of shots to further measure the effectiveness of other methods, including 0-shot, 1-shot, half-shot, and full-shot. More Visualization Results for Section 3.2 B.1 Similarity between Tokens from Different Samples in Each Position for Key States. In Figure 11, we showcase that key states in different layers maintain consistently high cosine similarity values for various initial tokens, with only the first layer exhibiting slightly lower similarities. Our analysis reveals that LLaMA-3-8B-Instruct and LLaMA-3.1-8B-Instruct exhibit almost the same direction (approximately 1.0) for different tokens beyond the first layer, while Mistral-7B-Instruct-v0.3 and Gemma-2-9b-it show substantial but lower similarities ranging from 0.8 to 0.9. These findings indicate inherent alignments across contexts while highlighting the potential for further improvements through the shared prefix in Section 4.1. Figure 11 For all base models, key states from distinct inital tokens exhibit large cosine similarity than the following positions, where the LLaMA family even approaches 1. The X-axis shows positions of key states on logarithmic scale. B.2 Similarity between Tokens from Different Samples in Each Position for Value States. Similarly, Figure 12 shows that value states maintain high cosine similarity across different layers for various initial tokens. There are two notable exceptions: the first layer and the Gemma-2-9b-it model. This distinctive pattern in Gemma-2-9b-it aligns with the models requirement for system prompt to function correctly. B.3 Similarity between the Initial Token and Following Tokens for Key States. Figure 13 illustrates how the cosine similarity between the initial and subsequent key states stabilizes as position increases. This similarity converges to near-constant value for all base models after 10 tokens. Figure 12 Among four models, value states from distinct inital tokens exhibit large cosine similarity than the following positions, except the first layer and Gemma-2-9b-it. The X-axis shows positions of value states on logarithmic scale. Figure 13 For all base models, the similarity between the initial key state and subsequent key states stabilizes as the position increases. The X-axis shows positions of key states on logarithmic scale. B.4 Similarity between the Initial Token and Following Tokens for Value States. Similar to key states, the value states exhibit stable similarity between the initial token and subsequent tokens in Figure 14, with all models convergent to nearly constant value after approximately 10 tokens. Figure 14 For all base models, the similarity between the initial value state and subsequent value states stabilizes as the position increases. The X-axis shows positions of value states on logarithmic scale. B.5 Similarity between the Query State and Past Key States. In Figure 15, the query states across all layers, and base models exhibit higher cosine similarity with the initial tokens. Additionally, neighboring positions tend to receive higher cosine similarity. B.6 Magnitude of Key States from Different Positions. Figure 16 illustrates that the magnitude of key states gradually increases with position, except for the first few tokens, which exhibit significantly smaller magnitudes. 18 Figure 15 For all base models, the cosine similarity between the query state and past key states stabilizes for most positions, except for the initial and recent key states. The X-axis shows positions of key states on logarithmic scale. Figure 16 For all models, key states show slowly upward trend in magnitude as position increases. red dashed line marks the anomalous region for the first few tokens. The X-axis shows positions of key states on logarithmic scale. B.7 Magnitude of Value States from Different Positions. Figure 17 For all models, the magnitude of value states remains consistent for most positions, except for the first few positions highlighted by red dashed line. The X-axis represents the positions of value states on logarithmic scale. In Figure 17, the value states across all positions exhibit similar magnitude, except for the first few positions, which show noticeable deviation. We indicate this region with red dashed line. B.8 Dot Product between the Query State and Past Key States. In Figure 18, the query states across all layers, and base models exhibit larger dot product values with the initial tokens. Additionally, neighboring positions also tend to receive larger values. Figure 18 For all base models, the dot product values between the query state and past key states stabilizes for most positions, except for the initial and recent key states. The X-axis shows positions of key states on logarithmic scale."
        },
        {
            "title": "C Formal Derivation of APE",
            "content": "C.1 Hierarchical Formula for Softmax Attention. Here, we begin with the standard Softmax attention, where Q, K, and are the query, key, and value states from the input, respectively. To distinguish different sources, we use the subscript Ci for elements originating from the context, while those without subscript correspond to user queries or generated texts. = Softmax Q[K C1 , . . . , CN ! , ] [VC1, . . . , VCN , ] [AC1, . . . , ACN , A] PlCi j=1 aCi,j + Pl j=1 aj = PN i=1 [VC1, . . . , VCN , ], where KCi = [exp Qk 1 = [kCi,1, ..., kCi,lCi , . . . , exp Qk ], VCi = [vCi,1, ..., vCi,lCi Qk ], aCi,j = exp , and aj = exp Ci,j Qk . ], ACi = [exp Qk Ci,1 , . . . , exp Qk Ci,lCi ], We can restructure the computation hierarchically, first computing Ci and Ah Ci for each context Ci: Ci = Softmax Q[k Ci,1, . . . , Ci,lCi ! ] [VCi,1, . . . , VCi,lCi ], Ah Ci = LogSumExp Q[k Ci,1, . . . , Ci,lCi Similarly, for the non-context tokens, we compute: = Softmax (cid:18) Q[k 1 , . . . , d (cid:19) ] [V1, . . . , Vl], Ah = LogSumExp (cid:18) Q[k 1 , . . . , d (cid:19) ] After we get all these values, we can combine them while renormalizing with Ah: = Softmax (cid:0)Ah C1 , ..., Ah CN , Ah(cid:1) [V , ..., CN , h] C.2 Hierarchical Formula for APE. After incorporating all components in APE, we have new Ci and Ah Ci for each context Ci: h Ci = Softmax Q[k Ci,1, . . . , Ci,lCi ! ] [VCi,1, . . . , VCi,lCi ], Ah Ci = LogSumExp Q[k Ci,1, . . . , Ci,lCi (6) (7) ! ] (8) (9) (10) ! ] (11) For the non-context tokens, including our shared prefix, the formulas of and Ah remain unchanged. Here, we introduce separate terms for the shared prefix. Combining them, we have: and Ah = Softmax (cid:16) , Ah Ah C1 , ..., Ah CN , Ah(cid:17) [V , C , ..., CN , h] (12) C.3 Relation with Equation 5. Finally, we want to further show that it can be rewritten as Equation 5. First, for the value state of token from the position in context Ci, the final attention score Ci,j is Ci,j = exp(Qk Ci,j/T d) PlCi t=1 exp(Qk Ci,t/T d) (cid:18) LogSumExp exp (cid:18) Q[k Ci,1,...,k Ci,lCi (cid:19)(cid:19) ] (cid:18) Q[k Cn,1,...,k Cn ,lCn (cid:19)(cid:19) ] (cid:16) + exp LogSumExp (cid:16) Q[k 1 ,...,k ] (cid:17)(cid:17) (cid:18) PN n=1 exp LogSumExp exp(Qk Ci,j/T d) = = Ci,t/T exp(Qk PlCi t=1 exp(Qk (PlCn t=1 PN n=1 Ci,j/T PN d) d) (PlCi t=1 (PlCn t=1 n=1 exp(Qk d))S + Pl exp(Qk Cn,t/T Ci,t/T (PlCi t=1 exp(Qk Cn,t/T d))(S1) exp(Qk / t=1 exp(Qk d))S Ci,t/T d))S + Pl t=1 exp(Qk / d) = d) (PN n=1 Ci,j t=1 Ci,t PlCn )S + Pl t=1 at (13) (14) (15) This formula is equivalent to Equation 5, except it combines the prefix and other non-context tokens for simplicity. Similarly, for the non-context tokens from position j, we can derive j as = PN n=1 (PlCn t= exp(Qk d) exp(Qk / d))S + Pl Cn,t/T t=1 exp(Qk / = d) (PN n=1 aj t=1 PlCn Ci,t )S + Pl t=1 at (16) Combining these two components, we obtain the final formula presented in Equation 5. C.4 Efficient Implementation. To combine the computation for the context and non-context parts, we will employ flash attention twiceonce for each partand then merge the results. This only introduces marginal computational overhead, as shown below. def ape attention (query , key , value , temperature , scale ) : lse context = flash attn (query , key , value , temperature = temperature) # i key and value states into context and noncontext parts key context , key other = key value context , value other = value attn output context , attn output other , attn output context = attn output context ( lse context(scale 1)) lse context = lse context( scale ) attn weights = [ lse context , attn weights = Softmax(attn weights) value states = [ attn output context , attn output other ] attn output = attn weights @ value states lse other = flash attn (query , key , value) lse other ] 21 Figure 19 Beyond the parallel cache structure discussed in the main paper, APE can be extended to handle more complex cache structures from external sources, where each context forms tree-like hierarchy. In this setup, computations can be performed hierarchically along each branch, progressively merging intermediate results into the final value state. C.5 Future Directions. The hierarchical formulation of APE can naturally extend to more complex tree structures, as illustrated in Figure 19. This flexibility allows each user query to be enriched with external knowledge organized in such structures, demonstrating APEs capability to handle structured external data effectively. Comparing APE with Long-context LLMs. In Table 7, we further compare APE with Long-context LLM, including: (i) Prompt Compression: Truncation, LLMLingua2 (Pan et al., 2024), (ii) KV Cache Eviction: StreamingLLM (Xiao et al., 2023), (iii) Longcontext FT : Llama-3-8B-Instruct-262K (Gradient, 2024), Llama-2-7B-Instruct-32K (Together, 2023), (iv) length extrapolation: Self-Extend (Jin et al., 2024). Experimental results show that APE consistently outperforms all existing long-context LLM methods. We hypothesize that this improvement stems from APE enabling queries to access all past contexts, enhancing retrieval ability. However, since APE has limitations in identifying relationships between contexts, we do not emphasize its performance on current long-context tasks."
        },
        {
            "title": "E APE Cache versus Prefix Cache",
            "content": "Finally, we compare the APE cache with the prefix cache to highlight our advantages in serving multiple queries within the CAG setting. Figure 20 illustrates an example with four contexts where both caching strategies are allocated the same budget. Each query retrieves three contexts. Under these conditions, the prefix cache can only match limited number of combinations, achieving an average hit rate of 41.7%, whereas the APE cache ensures 100% hit rate. This gap will become even more pronounced as the number of contexts increases. 22 Table 7 Performance comparison between APE and long-context LLMs on LongBench (Bai et al., 2023)."
        },
        {
            "title": "LCC",
            "content": "LLaMA-3-8B-Instruct LLMLingua2 StreamingLLM Long-context FT Self-Extend +APE 19.32 21.00 16.99 14.88 24.82 26.87 32.83 25.78 28.94 21.70 37.94 39.14 43.38 48.92 11.99 47.79 50.99 59.12 27.89 27.09 25.65 32.65 30.48 29.10 22.40 22.34 19.91 24.76 23.36 23. 53.22 16.41 40.02 55.12 58.01 66."
        },
        {
            "title": "Method",
            "content": "RepoBench-P HotpotQA 2WikiMQA MuSiQue MultiNews Average LLaMA-3-8B-Instruct LLMLingua2 StreamingLLM Long-context FT Self-Extend +APE 38.15 20.56 26.16 43.05 41.83 49.43 44.24 40.16 32.76 15.89 51.09 50.11 21.01 24.72 20.12 10.49 24.17 28.06 20.47 20.85 17.32 8.74 28.73 25. 23.63 21.34 21.49 24.28 24.11 22.40 31.50 26.29 23.76 27.21 35.96 38.11 Figure 20 Prefix Cache vs. APE Cache. Our cache can keep 100% hit rate while the prefix cache only has 42%."
        }
    ],
    "affiliations": [
        "Carnegie Mellon University",
        "Nvidia"
    ]
}