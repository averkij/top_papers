{
    "paper_title": "ShowTable: Unlocking Creative Table Visualization with Collaborative Reflection and Refinement",
    "authors": [
        "Zhihang Liu",
        "Xiaoyi Bao",
        "Pandeng Li",
        "Junjie Zhou",
        "Zhaohe Liao",
        "Yefei He",
        "Kaixun Jiang",
        "Chen-Wei Xie",
        "Yun Zheng",
        "Hongtao Xie"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "While existing generation and unified models excel at general image generation, they struggle with tasks requiring deep reasoning, planning, and precise data-to-visual mapping abilities beyond general scenarios. To push beyond the existing limitations, we introduce a new and challenging task: creative table visualization, requiring the model to generate an infographic that faithfully and aesthetically visualizes the data from a given table. To address this challenge, we propose ShowTable, a pipeline that synergizes MLLMs with diffusion models via a progressive self-correcting process. The MLLM acts as the central orchestrator for reasoning the visual plan and judging visual errors to provide refined instructions, the diffusion execute the commands from MLLM, achieving high-fidelity results. To support this task and our pipeline, we introduce three automated data construction pipelines for training different modules. Furthermore, we introduce TableVisBench, a new benchmark with 800 challenging instances across 5 evaluation dimensions, to assess performance on this task. Experiments demonstrate that our pipeline, instantiated with different models, significantly outperforms baselines, highlighting its effective multi-modal reasoning, generation, and error correction capabilities."
        },
        {
            "title": "Start",
            "content": "ShowTable: Unlocking Creative Table Visualization with Collaborative Reflection and Refinement Zhihang Liu1*, Xiaoyi Bao2*, Pandeng Li1,7, Junjie Zhou3, Zhaohe Liao4, Yefei He5, Kaixun Jiang6, Chen-Wei Xie7, Yun Zheng7, Hongtao Xie1 1 USTC 2 CASIA 3 NJU 4 SJTU 5 ZJU 6 FDU 7 Tongyi Lab Project Page: https://lntzm.github.io/showtable-page/ 5 2 0 2 5 ] . [ 1 3 0 3 3 1 . 2 1 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "While existing generation and unified models excel at general image generation, they struggle with tasks requiring deep reasoning, planning, and precise data-to-visual mapping abilities beyond general scenarios. To push beyond the existing limitations, we introduce new and challenging task: creative table visualization, requiring the model to generate an infographic that faithfully and aesthetically visualizes the data from given table. To address this challenge, we propose ShowTable, pipeline that synergizes MLLMs with diffusion models via progressive selfcorrecting process. The MLLM acts as the central orchestrator for reasoning the visual plan and judging visual errors to provide refined instructions, the diffusion execute the commands from MLLM, achieving high-fidelity results. To support this task and our pipeline, we introduce three automated data construction pipelines for training different modules. Furthermore, we introduce TableVisBench, new benchmark with 800 challenging instances across 5 evaluation dimensions, to assess performance on this task. Experiments demonstrate that our pipeline, instantiated with different models, significantly outperforms baselines, highlighting its effective multi-modal reasoning, generation, and error correction capabilities. 1. Introduction Image generation models have shown significant growth in quality in recent years [3, 13, 25, 38, 43]. Recent advancements, particularly in unified models [6, 8, 12, 29, 46, 52, 55, 56, 60], have leveraged MLLMs for fine-grained understanding of complex multi-modal instructions, leading to more semantically coherent synthesis in general-purpose image generation and editing. Despite this rapid progress in the general domain, research is increasingly shifting to- *Equal contribution, interns at Tongyi Lab Project leader Figure 1. The illustration of our proposed creative table visualization task and ShowTable pipeline. Given table about specific topic, our task requires the model to produce visualization infographic that is aesthetic and faithful to the data points. The ShowTable pipeline employs four steps: rewriting, generation, reflection, and refinement, thus achieving high-fidelity visualization. We use Wan2.5-Preview [46] here for generation and refinement. wards more complex scenarios. key area of focus is graphic design, such as poster generation [17, 21, 30] and text rendering [5, 33, 50, 55]. These tasks present higher challenge as they demand not photorealism, but rather precise textual alignment and adherence to aesthetic principles of graphic design. However, within this structured domain, data-centric visualizations (e.g., charts and graphs derived from tables) present an even more formidable challenge. These tasks require not only understanding and reasoning of data but also rigorous, high-fidelity mapping of quantitative data, where errors in visual proportions (e.g., bar heights, pie chart angles) or data labels render the output useless. To push beyond this limitation from unified reasoning and synthesis, we introduce new and challenging task for unified models: creative table visualization, requiring model to generate an infographic that is aesthetic and faithful to the 1 data points within given table. This task is defined by its dual requirements: 1) sophisticated reasoning for graphic design and aesthetic layout, and 2) strict, high-fidelity datato-visual mapping from source table. The ability to perform such synthesis could significantly streamline workflows in poster design, automatic slide generation, scientific communication, and other data-driven reporting tasks. As shown in Figure 1, current models still struggle with directly completing this task, as various errors about layout and data mapping may occur. Therefore, we propose ShowTable, novel pipeline that synergizes MLLMs and diffusion models in progressive, self-correcting loop, better addressing the task. As illustrated in Figure 1, this process is inherently suited for achieving faithful data-to-image mapping, since the self-correcting loop iteratively refine initial inaccuracies. The framework relies on two core components: an MLLM acting as central orchestrator and diffusion model as the executor. The MLLM performs two key roles: 1) Rewriting, where it first reasons over the tabular data to plan an aesthetic visual sketch and rewrites the user prompt accordingly; and 2) Reflection, where it assesses the generated output and provides precise editing instructions. Correspondingly, the diffusion model executes two stages: 1) Generation, creating an initial figure based on the MLLMs sketch; and 2) Refinement, editing the figure based on the MLLMs reflective feedback. To optimize the effectiveness of the pipeline, we conduct pioneering exploration into training specific rewriting and refining capabilities. First, as Figure 3 illustrates, the rewriting module undertakes the crucial responsibility of reasoning and planning, its quality thus largely dictates the final outcome. Therefore, we produce 30K supervisedfinetuning (SFT) data by captioning collected table visualizations and train this module. Second, as shown in Figure 4, incapable refining models may degrade performance. This observation confirms that the refinement module could be bottleneck in the pipeline. Therefore, we first train reward model (RM) using our constructed 30K comprehensive preference pairs. We then leverage this trained RM to optimize the diffusion models refinement capability using reinforcement learning (RL) on filtered set of 5K samples. Furthermore, to evaluate the performance of models on this task, we introduce comprehensive benchmark called TableVisBench. The benchmark features 800 challenging table instances, which are meticulously selected, filtered, labeled, and manually verified to ensure high quality. We design five key evaluation dimensions: data accuracy, text rendering, relative relationship, additional information accuracy, and aesthetic quality, comprehensively assessing the performance of different baselines. Extensive experiments have demonstrated that our ShowTable pipeline significantly boosts the performance of all base models on TableVisBench. In conclusion, our contribution can be summarized as: 1) New task: We propose the creative table visualization task, which requires detailed reasoning and precise alignment ability, to challenge existing unified models. 2) Data: We construct three automatic data construction pipelines to produce data for training, and present TabelVisBench benchmark, comprehensively evaluating the models capability for creative table visualization task. 3) Method: We design progressive self-correcting pipeline ShowTable that cooperate an MLLM as central orchestrator with diffusion model as executor. Experiments demonstrate the effectiveness of our framework. 2. Related Work Image generation for graphic design. Recent advances in image generation have significantly enhanced the quality and semantic controllability of synthesized content in general-purpose scenarios [3, 6, 8, 12, 13, 25, 29, 38, 43, 46, 52, 55, 56, 60]. This progress has spurred growing interest in more complex generation tasks such as graphic design. AnyText [49, 50], Glyph-ByT5 [33], and TextDiffuser [5] have made significant efforts in text rendering, focusing on accurate textual element incorporation. AutoPoster [30], PosterMaker [17], and DreamPoster [21] focus on poster generation, which addresses aesthetic layout planning. The recently proposed Qwen-Image [55] has also demonstrated capabilities in complex text rendering and infographic design. Compared to these scenarios, our proposed creative table visualization task presents more formidable challenge. Our task demands not only reasoning about graphic design for aesthetic layout but also precise data-to-visual mapping for faithful representation of tabular content. Reasoning and reflection paradigm of MLLMs. The reasoning and reflection abilities of MLLMs are gaining increasing attention for both understanding and generation. Some studies leverage the reasoning capability of MLLMs to achieve better image understanding, often termed thinking with images [9, 22, 70, 71]. For image generation, some works [12, 15, 25, 59] introduce text-based reasoning step, enhancing image generation performance. Recently, emerging research has begun to leverage the MLLM reflection process to refine the image generation itself, aiming to enhance instruction-following in complex, general-purpose scenes [23, 24, 54]. However, this category of research remains focused on general domains. The high information density and specific structural constraints of our task pose greater challenge for MLLMs with combining reasoning, planning, and reflection abilities. 3. Method 3.1. ShowTable Pipeline Overview. To address the challenge of creative table visualization and push model reasoning in the infographic 2 Figure 2. The proposed ShowTable pipeline, which synergizes an MLLM as the central orchestrator with diffusion model as the executor. Given table, the MLLM first rewrites detailed prompt for the diffusion models initial generation. The MLLM then iteratively reflects on the output to identify errors (marked in red) and provides precise instructions for refinement (corrected results shown in green). domains, we propose base pipeline, ShowTable. To confront the limitations of existing models in preserving visual relational reasoning and information consistency, ShowTable introduces structured Rewriting Generation Reflection Refinement workflow. This iterative selfcorrection loop is specifically designed to address the challenges of creative table visualization, where standard generation models often fail, producing logical inconsistencies or text rendering errors. As shown in Figure 2, ShowTable progressively improves the quality and detail fidelity by repeated reflection and refinement. Rewriting. There exists significant distinction between raw tables and typical image generation prompts. Table inputs (e.g., in markdown format) possess high information density, with each data point encapsulating complex relational semantics. Visualizing such dense and non-redundant data necessitates deliberate reasoning about data presentation and layout. When markdown-formatted table is directly used as prompt, the generation model tends to misinterpret the task, often trying to render the table itself rather than visualizing its underlying data, as illustrated in Figure 3. To bridge this gap, we use an MLLM to perform semantic and structural reasoning with compositional planning. It translates the data-dense table into detailed descriptive prompt that generation models can execute. Generation. Upon obtaining the rewritten prompt, we feed it into pre-trained text-to-image (T2I) model to produce an initial image. As shown in Figure 2, this preliminary result generally captures the overall layout and aesthetic. However, it often contains critical errors in high-fidelity details, such as incorrect data-to-visual correspondence (e.g., bar heights), axis label misalignments, or suboptimal text rendering. This imperfect initial generation serves as the foundation for the subsequent reflection and refinement stages. Reflection. Given the strict fidelity required for table visualization, we employ reflection module. This module uses an MLLM to perform critical audit of the generated image. By cross-referencing the original table with the generated image, the MLLM identifies inconsistencies and inaccuracies. It then formulates set of precise, actionable editing instructions to correct these errors. Refinement. Finally, as shown in Figure 2, these edit instructions are provided to an image editing model, which executes the corrections on the initial image to produce the refined, high-fidelity visualization. 3.2. Training Details Rewriting module. Although general LLMs have demonstrated powerful reasoning abilities on various tasks, their performance on table reasoning and compositional planning can be further enhanced. As Figure 3 shows, they may miss data points or plan poorly when encountering complex data (e.g., multi-layered structures). As this is the critical step responsible for basic layout design and content planning, fine-tuning the rewrite module is expected to significantly enhance the overall performance. Therefore, we fine-tune specific rewrite module from Qwen3-8B [63] to address this challenge. With our constructed 30K rewriting training data (Section 4.2), we train the module with the standard 3 Figure 3. Generation comparison among different rewriting. The model tends to directly render the table text, failed to visualize when disabling rewriting. General LLMs also tend to miss some data or wrongly classify data, especially for complex tables. next-token prediction format: Lrewrite = 1 (cid:88) n=1 log(ˆyn,kn) (1) where denotes the sequence length, and ˆyn,kn is the predicted probability for the true class kn at position n. Refinement module. The refinement stage is critical for correcting errors, but it may also present significant potential bottleneck. As illustrated in Figure 4, our preliminary experiments revealed this challenge. When employing recent SOTA Qwen-Image-Edit [55] within our iterative loop, we observed performance degradation with each correction round. This raised critical question: is the pipelines self-correction logic flawed, or is the editing models capability insufficient? To investigate this variable, we introduced Wan2.5-I2I-Preview [46], another editing model known for fine-grained controllability, and found that performance indeed increased with each iteration. This indicates that our pipeline structure is sound, but its effectiveness is fundamentally constrained by the refinement models ability to execute precise edits. Therefore, to resolve this bottleneck, we train the refinement module using RL, specifically employing the Group Relative Policy Optimization (GRPO) algorithm [18]. This approach requires an accurate reward signal. However, evaluating rendering quality is complex, integrating multiple dimensions. As recent pretrained MLLMs struggle to directly provide consistent, accurate scalar scores for such assessments [36] (see Appendix), there remains need to develop specialized 4 Figure 4. Demonstrating the refinement models capability as critical pipeline bottleneck. The base models performance degrades with each correction, indicating an inability to process iterative feedback. In contrast, the Wan2.5-I2I-Preview shows consistent improvement. This confirms our pipeline structure is sound and that the bottleneck is the models capability, motivating our specialized training for the refinement module. reward model (RM). Using our constructed 30K pairwise preference dataset (Section 4.2), we fine-tune Qwen2.5VL-3B [1] model, fθ, as our quality assessor. The model is trained to distinguish positive (xw) and negative (xl) samples for given prompt using the Bradley-Terry loss [4]: LBT = E(p,xw,xl)D [log σ (fθ(xw, p) fθ(xl, p))] , (2) where σ() is the Sigmoid function. To enhance training efficiency, the scalar reward score is computed by extracting the probabilities corresponding to digits 09 from the output logits and averaging their sum. Finally, using our constructed 5K refinement data (Section 4.2), we follow Flow-GRPO [32] to perform the RL. The full reward signal combines our trained fθ with an existing aesthetic reward model, ImageReward [61], to optimize the training. 4. Dataset and Benchmark 4.1. Data Collection and Filtering To support both training and benchmarking for our proposed task, we construct dataset of 30K high-quality tableimage pairs and an additional benchmark of 800 evaluation samples. We initiate this by collecting raw images from diverse public datasets, including SlideVQA [45], OpenImages [26], and Cambrian-10M [48]. We then applied the rigorous filtering and annotation pipeline shown in Figure 5. First, we discard low-resolution images (under 200 200) and images lacking text, as identified by PaddleOCR [11]. Following this, we utilize SOTA MLLMs (i.e., Gemini-2.5Figure 5. Dataset construction pipeline. We initially collect and filter images from public datasets with SOTA MLLMs, and then propose three different kinds of training data construction pipelines: rewriting training data, refinement training data, and reward training data. pro[10] and GPT-5 [39]) to perform crucial filtering and annotation step: the models filter out images that do not feature statistical data as the main body, and simultaneously annotate the table information in markdown format for all remaining images. To ensure the quality of these annotations, we implemented consensus-based verification process. Both MLLMs independently annotate the filtered images. We then retained only the samples for which the annotations from both models were consistent and mutually approved, resulting in our final set of 30K high-quality table-image pairs. 4.2. Training Data Construction Based on the collected 30K table-image pairs, we propose three automated training data construction pipelines to support the distinct stages of our method. Rewriting training data. As the rewriting module is critical for semantic reasoning and compositional planning, high-quality SFT data is essential. As shown in Figure 5, we first prompt Gemini-2.5-pro to generate detailed description of the ground-truth image based on the annotated table, covering data points, layout, color, and background. We then prompt Gemini-2.5-pro again, providing both the table and the new description, to generate chain-of-thought rationale that explains the conversion process. This results in 30K data pairs ({table, rationale} {description}), which are used to fine-tune the rewriting module. Refinement training data. To generate data for RL, we first use the descriptions from the previous step to generate initial images using Wan2.5-t2i-preview [46] and QwenImage [55]. We then use Gemini-2.5-pro to audit these generated images and produce corresponding refinement instructions. We apply rigorous filtering strategy to ensure RL training stability. For each sample (initial image + instruction), we instruct our base editing model (QwenImage-Edit [55]) to generate five refined candidates. These candidates are compared against the initial image by powerful MLLM assessor (GPT-5). We discard samples where all five attempts are judged as worse, or all five as better, than the original. This filtering isolates samples that are too hard or too easy for base model, yielding 5K challenging samples ideal for refinement training. Reward training data. Our RL approach requires reliable RM. Given that MLLMs are unstable in providing direct point-wise scores [36], we construct preference dataset for converting the preference into specific score assessor. We use GPT-5 and Gemini-2.5-pro to compare each image pair, followed by voting, and finally generate 30K positive-negative image pairs for training the RM, as shown in Figure 5. These pairs are sourced from three comparisons: 1) images refined by Wan2.5 versus images generated by Wan2.5 or Qwen; 2) images generated by Wan2.5 versus those by Qwen; and 3) collected ground-truth images versus generated or refined images. 4.3. Benchmark Design To accurately evaluate performance on our creative table visualization task, we construct TableVisBench, benchmark containing 800 challenging table-based instances. The collection and filtering process is similar to that of the training data, but with an additional step of manual verification and correction for all samples. Detailed statistics about the benchmark are provided in the Appendix. For comprehensive benchmarking, we conduct multi5 Table 1. Performance comparison of recent strong open-sourced baselines on our TableVisBench. RW refers to the rewriting module, REF refers to the reflection and refinement process. We mark the improvement of our proposed pipeline for each base generation model. Methods DA TR RR AA AQ Score Reference Image 97.7 99. 86.4 96.6 4.2 84.4 Flux [3] RW+Flux RW+Flux+REF Improvement Bagel [12] RW+Bagel RW+Bagel+REF Improvement Blip3o-Next [7] RW+Blip3o-Next RW+Blip3o-Next+REF Improvement 46.7 52.3 63.1 29.3 12.1 32.1 12.0 20.3 36.4 +8.2 +16.4 +2.9 +5.3 +0.3 +7.1 18.7 25.3 24.0 28.9 27.0 31.8 4.0 4.4 4. 1.6 18.3 54.8 0.1 10.1 3.4 19.5 32.7 18.3 +18.2 +53.2 +22.5 +8.2 +1.1 +22.6 14.2 28.9 36.7 7.7 13.0 15.9 2.7 3.4 3.8 18.0 14.5 63. 10.8 0.4 14.1 0.5 34.8 21.3 +20.9 +45.9 +29.0 +13.0 +1.1 +24.0 4.4 19.1 33.4 6.2 7.6 19.2 2.5 2.9 3.6 3.0 UniWorld-V1 [29] RW+UniWorld-V1 4.0 RW+UniWorld-V1+REF 18.7 Improvement 14.8 18.6 33.5 +15.7 +36.3 +22.9 +15.9 +0.3 +18. 14.7 23.7 37.6 2.9 11.6 18.8 18.3 20.8 54.6 3.5 3.3 3.8 OmniGen2 [56] RW+OmniGen2 RW+OmniGen2+REF Improvement 17.8 32.1 49. 14.4 3.1 21.9 4.0 16.2 29.9 +13.1 +32.0 +17.1 +11.2 +0.4 +15.5 13.5 25.0 30.6 2.6 9.5 13.8 3.5 3.9 3.9 Qwen-Image [55] RW+Qwen-Image RW+Qwen-Image+REF Improvement 47.5 51.2 52.4 +4. 26.1 50.1 54.3 44.3 14.1 90.9 54.3 40.9 83.1 82.9 54.9 40.0 -8.0 +28.2 +25.9 +0.2 +10.6 4.3 4.6 4.5 view evaluation [35] with five well-designed dimensions, focusing not only on factual accuracy but also on logical coherence and visual aesthetics. Instead of using an MLLM as subjective scorer, we leverage it as quality assurance analyst. For the first four dimensions, the MLLM is prompted to identify and quantify specific errors within the generated chart. The final score is then deterministically calculated based on the number of reported errors, thereby mitigating the instability and bias associated with direct LLM-based scoring. The final dimension is quantitatively assessed using dedicated aesthetic scoring model. The five dimensions are as follows: Data Accuracy (DA). This dimension verifies that every single data point from the source table is accurately represented in the generated image, ensuring none are missing, incorrect, or simply rendered as raw table text. Text Rendering (TR). This dimension focuses on the legibility and correctness of all textual elements in the image. Relative Relationship (RR). This dimension assesses the core visualization logic, i.e., whether the visual proportions of chart elements (e.g., bar heights, slice angles) correctly reflect the quantitative relationships between data points. 6 Table 2. Ablation of the rewriting module of our ShowTable. All results reported in the table are the generated results after the rewriting process without reflection and refinement. The generation module utilized is Qwen-Image (8 steps distilled). Rewrite DA TR RR AA AQ Score 47.5 90.9 26.1 14.1 4.3 False 30.6 71.5 46.6 34.1 5.1 Qwen3-8B [63] GPT-5 [39] 35.9 78.5 47.8 41.8 5.2 Gemini-2.5-pro [10] 40.8 79.9 53.9 41.1 5.1 Qwen3-8B* 51.2 83.1 50.1 40.9 4.6 Reference-Caption 50.3 83.4 55.1 42.8 4.5 44.3 46.8 51.2 53.3 54. 55.3 Additional information Accuracy (AA). This dimension inspects the accuracy and appropriateness of contextual information added by the model (not present in the source table), such as axes, ticks, gridlines, and extraneous artifacts. Aesthetic Quality (AQ). Independent of factual correctness, this dimension evaluates the overall visual appeal of the generated chart, including its layout, color palette, typography, and design creativity. The scores for the first four dimensions (DA, TR, RR, AA) range from 0 to 100, while AQ ranges from 0 to 10. We calculate the final score by: Score = (DA + TR + RR + AA + 10 AQ)/5 (3) We provide detailed information about these dimensions in the Appendix. To validate the reliability of our benchmark, we test our dimensions on the collected ground-truth images. As shown in Table 1, these high-quality images achieve very high scores, confirming that our evaluation metrics are well-aligned with human-annotated quality. 5. Experiments 5.1. Implementation Setup The modular design of our ShowTable pipeline allows for flexible combinations of MLLMs and diffusion models. In our default configuration, the rewriting module is trained based on Qwen3-8B [63], and the refinement module is trained based on distilled 8-step version [37] of QwenImage-Edit-2509 [55] to accelerate RL training. For the reflection module, we employ GPT-5-2025-08-07 [39]. For fair comparison, all baselines that use Qwen-Image for generation or Qwen-Image-Edit-2509 for refinement also use these same distilled versions [37]. We set the maximum self-correction round to 3, though the process can terminate early if the reflection module deems an image satisfactory. More detailed settings can be found in the Appendix. 5.2. Main Results We comprehensively evaluate the effectiveness of the ShowTable pipeline by applying it to several advanced T2I Table 3. Ablation of the different models of the reflection module. We use our trained Qwen3-8B as the rewriting module, QwenImage (distilled) as the generation module, and Qwen-Image-Edit2509 (distilled) / our trained model as the refining module here. Reflection DA TR RR AA AQ Score Refinement: Qwen-Image-Edit-2509 Qwen3-VL-235B [41] 37.7 78.1 45.1 31.3 4.4 43.2 81.2 46.6 41.1 4.4 Gemini-2.5-pro [10] 42.6 79.7 45.0 35.9 4.4 GPT-5 [39] Refinement: Qwen-Image-Edit-2509* (trained by ours) Qwen3-VL-235B [41] 46.9 81.5 48.3 37.1 4.5 48.1 83.0 48.8 44.8 4.5 Gemini-2.5-pro [10] 52.4 82.9 54.3 40.0 4.5 GPT-5 [39] 47.2 51.2 49.4 51.8 53.9 54. generation models, including Flux [3], Bagel [12], Blip3oNext [7], UniWorld-V1 [29], OmniGen2 [56], and QwenImage [55]. The evaluation is conducted on our TableVisBench based on the five dimensions. We systematically compare performance under three configurations: 1) The base generation model alone (Base); 2) The base model prefixed with our rewriting module (RW+Base); 3) The full pipeline, integrating all modules (RW+Base+REF). Quantitative analysis. The results, presented in Table 1, demonstrate three clear findings. First, base models alone are incapable of our proposed challenging task. Some models, such as Bagel and Blip3o-Next, score near zero (0.1 and 0.4, respectively) on Data accuracy, indicating fundamental failure to translate table data into visual components. Second, the rewriting (RW) module is critical for reasoning and planning. Simply adding the RW module significantly boosts performance, especially in logical coherence. For instance, with Qwen-Image, the RR score jumps from 26.1 to 50.1. This shows that converting raw markdown into reasoned, descriptive prompt is an essential first step. Third, the reflection and refinement (REF) loop is essential for accuracy. The full pipeline (RW+Base+REF) achieves the best overall score in all cases. This step yields the most significant gains in correctness-based metrics. With Blip3o-Next, the full pipeline improves DA from 0.5 to 21.3 and TR from 14.5 to 63.9. Moreover, our approach also shows strong adaptability. For models with weaker baseline capabilities (e.g., Bagel), our pipeline provides substantial boost, improving the final score by +22.6 points, respectively. For strong base models like Qwen-Image, the full pipeline unlocks their potential, achieving the highest scores in key metrics and demonstrating powerful synergistic effect, raising the overall score from 44.3 to 54.9. Qualitative analysis. Qualitative results in Figure 6 also demonstrate ShowTables ability to produce creative yet accurate visualizations across diverse table structures. The reflection-refinement mechanism effectively corrects various error types: misrendered text and numbers (Row 1Left, Row 2), incorrect proportional relationships (Row 1Left), and improper visual element representations (Row 3). Cases requiring zero refinement (Row 1-Right) confirm the pipelines adaptive efficiency. These results validate the robustness of ShowTable in achieving faithful and creative table-to-image translation. 5.3. Ablation Studies Rewriting module. We evaluate different rewriting strategies using the Qwen-Image generation model. As shown in Table 2, compared settings include: 1) no rewriting (False), 2) general-purpose LLMs (Qwen3-8B, GPT5, Gemini-2.5-pro), 3) our fine-tuned model (Qwen3-8B*), and 4) an upper-bound using reference captions (ReferenceCaption). All results are based on the initial generation (no refinement). Our fine-tuned module (Qwen3-8B*) achieves the highest overall score (54.3) among all evaluated rewriting methods, demonstrating strong, well-balanced performance. Notably, it attains the best DA (51.2), confirming that specialized training is more effective at preserving data integrity than general-purpose LLMs (30.6-40.8). While all rewriting methods slightly reduce the TR score compared to the baseline (which often just renders the table text directly), our models substantial gains in DA (+3.7 vs base) and RR (+24.0 vs base) justify this trade-off. Notably, our fine-tuned model even surpasses the Reference-Captions DA (51.2 vs. 50.3), further underscoring the advantage of training. While Gemini-2.5-pro achieves the highest RR score, our model remains highly competitive and delivers the best overall performance, highlighting its robustness. Reflection module. We evaluate the effectiveness of different MLLMs as the reflection module. As shown in Table 3, we use our fine-tuned rewriting module (Qwen3-8B*) and the Qwen-Image generation model. We then compare the final performance with Qwen3-VL-235B, Gemini-2.5-pro, and GPT-5 as reflection modules. The results demonstrate the significant impact of the reflection module on output quality. When using our trained refinement model (bottom half of table), GPT-5 achieves the strongest performance (54.9), excelling in Data Accuracy (52.4) and Relative Relationship (54.3). When paired with the base Qwen-ImageEdit module (top half of table), Gemini-2.5-pro delivers the best results (51.2). These findings confirm that more capable reflection models consistently enhance output quality, providing practical guidance for model selection. Refinement module. We evaluate different refining modules under fixed rewriting (our fine-tuned Qwen3-8B*) and reflection (GPT-5) conditions, with results in Table 4. Our trained refinement model (Qwen-Image-Edit-2509*) achieves significant improvements over its base model, increasing the overall score from 49.4 to 54.9 (+5.5). The model proves notable gains in DA (+9.8) and RR (+9.3), validating our specialized RL-based training approach. As 7 Figure 6. Case studies of the ShowTable pipeline with four examples: top row shows one case requiring one-round reflection and refinement (left) and one proper initial generation (right), while other rows display results refined through multiple reflection rounds. Examples demonstrate adaptive correction of text, proportions, and visual elements. We mark the error parts with red boxes for better reading. Table 4. Ablation of different models for the refinement module. We use our trained Qwen3-8B as the rewriting module, QwenImage (distilled) as the generation module, and GPT-5 as the reflection module here. * donates the method trained by ours. Refining DA TR RR AA AQ Score 42.6 79.7 45.0 35.9 4.4 Qwen-Image-Edit-2509 Qwen-Image-Edit-2509* 52.4 82.9 54.3 40.0 4.5 Improvement 49.4 54.9 +9.8 +3.2 +9.3 +4.1 +0.1 +5.5 Wan2.5-I2I-Preview 64.2 84.7 64.6 59.7 4.4 63.4 Table 5. Ablation of the maximum refining rounds. We keep the same setting as Table 4, and compare results of each round. Methods w/o refine Qwen-Image-EditQwen-Image-Edit-2509* Wan2.5-I2I-Preview Num DA TR RR AA AQ Score 0 1 2 3 1 2 1 2 3 51.2 83.1 50.1 40.9 4.6 54.3 45.6 81.7 48.3 38.3 4.5 42.6 80.7 46.2 36.2 4.5 42.6 79.7 45.0 35.9 4.4 50.0 82.6 51.3 39.4 4.5 50.4 83.0 52.9 41.5 4.6 52.4 82.9 54.3 40.0 4.5 60.7 85.2 61.8 53.8 4.5 63.3 85.1 64.1 57.5 4.4 64.2 84.7 64.6 59.7 4. 51.8 50.1 49.4 53.7 54.8 54.9 61.3 62.8 63.4 shown in Table 4, while the powerful Wan2.5 achieves higher performance (63.4), our trained models substantial enhancement proves that our method effectively boosts open-source base models, offering viable path for customized refinement solutions. Refining rounds. We analyze the impact of iterative refinement rounds in Table 5. The results demonstrate that the refinement models capability is critical. The base Qwen-Image-Edit model shows performance degradation with each round (Score 54.3 49.4), indicating an inability to process iterative corrections. In contrast, our trained model (Qwen-Image-Edit-2509*) maintains stable improvement (53.7 54.9), validating that our specialized training successfully addresses this error accumulation. Furthermore, the powerful Wan2.5 model achieves continuous improvement (61.3 63.4), confirming that more capable models are essential for effective multi-round refinement. This underscores the necessity of our training, which successfully enhances the open-source model to reliably support the iterative process. 6. Conclusion This work introduces the creative table visualization task that demands both aesthetic graphic reasoning and highfidelity data mapping, addressing the critical challenge of generating faithful and aesthetic data visualizations. To address this, we propose ShowTable, novel pipeline that synergizes MLLMs with diffusion models through iterative reflection and refinement, significantly improving visualdata mapping alignment. To support this task, we also present three automated data construction pipelines for different module training. Moreover, we propose new com8 prehensive benchmark TableVisBench with 5 evaluation dimensions. Experiments demonstrate our approachs effectiveness in producing accurate and aesthetically coherent table visualizations, establishing foundation for future research in multi-modal reasoning and visual synthesis."
        },
        {
            "title": "References",
            "content": "[1] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. 4, 1, 3 [2] Kevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, and Sergey Levine. Training diffusion models with reinforcement learning. arXiv preprint arXiv:2305.13301, 2023. 1 [3] BlackForest. Flux. https://github.com/blackforest-labs/flux, 2024. 1, 2, 6, 7 [4] Ralph Allan Bradley and Milton Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3/4):324345, 1952. 4, 3 [5] Jingye Chen, Yupan Huang, Tengchao Lv, Lei Cui, Qifeng Chen, and Furu Wei. Textdiffuser-2: Unleashing the power of language models for text rendering. In European Conference on Computer Vision, pages 386402. Springer, 2024. 1, 2 [6] Jiuhai Chen, Zhiyang Xu, Xichen Pan, Yushi Hu, Can Qin, Tom Goldstein, Lifu Huang, Tianyi Zhou, Saining Xie, Silvio Savarese, et al. Blip3-o: family of fully open unified multimodal models-architecture, training and dataset. arXiv preprint arXiv:2505.09568, 2025. 1, [7] Jiuhai Chen, Le Xue, Zhiyang Xu, Xichen Pan, Shusheng Yang, Can Qin, An Yan, Honglu Zhou, Zeyuan Chen, Lifu Huang, et al. Blip3o-next: Next frontier of native image generation. arXiv preprint arXiv:2510.15857, 2025. 6, 7 [8] Xiaokang Chen, Zhiyu Wu, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, and Chong Ruan. Januspro: Unified multimodal understanding and generation with data and model scaling. arXiv preprint arXiv:2501.17811, 2025. 1, 2 [9] Ethan Chern, Zhulin Hu, Steffi Chern, Siqi Kou, Jiadi Su, Yan Ma, Zhijie Deng, and Pengfei Liu. Thinking with generated images. arXiv preprint arXiv:2505.22525, 2025. 2 [10] Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. 5, 6, 7 [11] Cheng Cui, Ting Sun, Manhui Lin, Tingquan Gao, Yubo Zhang, Jiaxuan Liu, Xueqing Wang, Zelun Zhang, Changda Zhou, Hongen Liu, et al. Paddleocr 3.0 technical report. arXiv preprint arXiv:2507.05595, 2025. 4 [12] Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Song, et al. Emerging properties in unified multimodal pretraining. arXiv preprint arXiv:2505.14683, 2025. 1, 2, 6, 7 [13] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. 1, [14] Ying Fan, Olivia Watkins, Yuqing Du, Hao Liu, Moonkyung Ryu, Craig Boutilier, Pieter Abbeel, Mohammad Ghavamzadeh, Kangwook Lee, and Kimin Lee. Dpok: Reinforcement learning for fine-tuning text-to-image diffusion models. Advances in Neural Information Processing Systems, 36:7985879885, 2023. 1 [15] Rongyao Fang, Chengqi Duan, Kun Wang, Linjiang Huang, Hao Li, Shilin Yan, Hao Tian, Xingyu Zeng, Rui Zhao, Jifeng Dai, et al. Got: Unleashing reasoning capability of multimodal large language model for visual generation and editing. arXiv preprint arXiv:2503.10639, 2025. 2 [16] James Ford and Anthony Rios. Does it run and is that revisiting text-to-chart generation with multienough? agent approach. arXiv preprint arXiv:2506.06175, 2025. 1 [17] Yifan Gao, Zihang Lin, Chuanbin Liu, Min Zhou, Tiezheng Ge, Bo Zheng, and Hongtao Xie. Postermaker: Towards high-quality product poster generation with accurate text rendering. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 80838093, 2025. 1, 2 [18] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. 4, 1, 3 [19] Yucheng Han, Chi Zhang, Xin Chen, Xu Yang, Zhibin Wang, Gang Yu, Bin Fu, and Hanwang Zhang. Chartllama: multimodal llm for chart understanding and generation. arXiv preprint arXiv:2311.16483, 2023. [20] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 1 [21] Xiwei Hu, Haokun Chen, Zhongqi Qi, Hui Zhang, Dexiang Hong, Jie Shao, and Xinglong Wu. Dreamposter: unified framework for image-conditioned generative poster design. arXiv preprint arXiv:2507.04218, 2025. 1, 2 [22] Yushi Hu, Weijia Shi, Xingyu Fu, Dan Roth, Mari Ostendorf, Luke Zettlemoyer, Noah Smith, and Ranjay Krishna. Visual sketchpad: Sketching as visual chain of thought for multimodal language models. Advances in Neural Information Processing Systems, 37:139348139379, 2024. 2 [23] Minbin Huang, Yanxin Long, Xinchi Deng, Ruihang Chu, Jiangfeng Xiong, Xiaodan Liang, Hong Cheng, Qinglin Lu, and Wei Liu. Dialoggen: Multi-modal interactive dialogue system for multi-turn text-to-image generation. arXiv preprint arXiv:2403.08857, 2024. 2 [24] Wenxuan Huang, Shuang Chen, Zheyong Xie, Shaosheng Cao, Shixiang Tang, Yufan Shen, Qingyu Yin, Wenbo Hu, Xiaoman Wang, Yuntian Tang, et al. Interleaving reasoning for better text-to-image generation. arXiv preprint arXiv:2509.06945, 2025. 2 [25] Dongzhi Jiang, Ziyu Guo, Renrui Zhang, Zhuofan Zong, Hao Li, Le Zhuo, Shilin Yan, Pheng-Ann Heng, and Hongsheng Li. T2i-r1: Reinforcing image generation with collaborative semantic-level and token-level cot. arXiv preprint arXiv:2505.00703, 2025. 1, 2 [26] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Alexander Kolesnikov, et al. The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale. International journal of computer vision, 128(7):19561981, 2020. 4 [27] Junzhe Li, Yutao Cui, Tao Huang, Yinping Ma, Chun Fan, Miles Yang, and Zhao Zhong. Mixgrpo: Unlocking flowbased grpo efficiency with mixed ode-sde. arXiv preprint arXiv:2507.21802, 2025. 1 [28] Yuming Li, Yikai Wang, Yuying Zhu, Zhongyu Zhao, Ming Lu, Qi She, and Shanghang Zhang. Branchgrpo: Stable and efficient grpo with structured branching in diffusion models. arXiv preprint arXiv:2509.06040, 2025. 1 [29] Bin Lin, Zongjian Li, Xinhua Cheng, Yuwei Niu, Yang Ye, Xianyi He, Shenghai Yuan, Wangbo Yu, Shaodong Wang, Yunyang Ge, et al. Uniworld: High-resolution semantic encoders for unified visual understanding and generation. arXiv preprint arXiv:2506.03147, 2025. 1, 2, 6, 7 [30] Jinpeng Lin, Min Zhou, Ye Ma, Yifan Gao, Chenxi Fei, Yangjian Chen, Zhang Yu, and Tiezheng Ge. Autoposter: highly automatic and content-aware design system for advertising poster generation. In Proceedings of the 31st ACM International Conference on Multimedia, pages 12501260, 2023. 1, [31] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. NeurIPS, 36, 2023. 1 [32] Jie Liu, Gongye Liu, Jiajun Liang, Yangguang Li, Jiaheng Liu, Xintao Wang, Pengfei Wan, Di Zhang, and Wanli Ouyang. Flow-grpo: Training flow matching models via online rl. arXiv preprint arXiv:2505.05470, 2025. 4, 1, 3 [33] Zeyu Liu, Weicong Liang, Zhanhao Liang, Chong Luo, Ji Li, Gao Huang, and Yuhui Yuan. Glyph-byt5: customized text encoder for accurate visual text rendering. In European Conference on Computer Vision, pages 361377. Springer, 2024. 1, 2 [34] Zhihang Liu, Chen-Wei Xie, Pandeng Li, Liming Zhao, Longxiang Tang, Yun Zheng, Chuanbin Liu, and Hongtao Xie. Hybrid-level instruction injection for video token compression in multi-modal large language models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 85688578, 2025. 1 [35] Zhihang Liu, Chen-Wei Xie, Bin Wen, Feiwu Yu, Jixuan Chen, Pandeng Li, Boqiang Zhang, Nianzu Yang, Yinglu Li, Zuan Gao, Yun Zheng, and Hongtao Xie. Capability: comprehensive visual caption benchmark for evaluating both correctness and thoroughness, 2025. 6 [36] Xin Luo, Jiahao Wang, Chenyuan Wu, Shitao Xiao, Xiyan Jiang, Defu Lian, Jiajun Zhang, Dong Liu, et al. Editscore: Unlocking online rl for image editing via high-fidelity reward modeling. arXiv preprint arXiv:2509.23909, 2025. 4, [37] ModelTC. Qwen-image-lightning. https://github. com/ModelTC/Qwen-Image-Lightning, 2025. 6, 3 [38] OpenAI. Dalle 3. https://openai.com/index/ dall-e-3/, 2023. 1, 2 [39] OpenAI. Gpt-5. https://openai.com/index/ introducing-gpt-5/, 2025. 5, 6, 7 [40] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Improving latent diffusion modRobin Rombach. Sdxl: arXiv preprint els for high-resolution image synthesis. arXiv:2307.01952, 2023. [41] QwenLM. Qwen3-vl. //https://github.com/ QwenLM/Qwen3-VL, 2025. 7 [42] Md Mahinur Rashid, Hasin Kawsar Jahan, Annysha Huzzat, Riyasaat Ahmed Rahul, Tamim Bin Zakir, Farhana Meem, Md Saddam Hossain Mukta, and Swakkhar Shatabda. Text2chart: multi-staged chart generator from natural language text. In Pacific-Asia Conference on Knowledge Discovery and Data Mining, pages 316. Springer, 2022. 1 [43] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 1, 2 [44] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. 1 [45] Ryota Tanaka, Kyosuke Nishida, Kosuke Nishida, Taku Hasegawa, Itsumi Saito, and Kuniko Saito. Slidevqa: dataset for document visual question answering on multiple images. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 1363613645, 2023. 4 [46] Wan Team. Wan2.5. https://www.wan-ai.co/wan2-5, 2025. 1, 2, 4, 5, 7 [47] Yuan Tian, Weiwei Cui, Dazhen Deng, Xinjing Yi, Yurun Yang, Haidong Zhang, and Yingcai Wu. Chartgpt: Leveraging llms to generate charts from abstract natural language. IEEE Transactions on Visualization and Computer Graphics, 31(3):17311745, 2024. 1 [48] Peter Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Adithya Jairam Vedagiri IYER, Sai Charitha Akula, Shusheng Yang, Jihan Yang, Manoj Middepogu, Ziteng Wang, et al. Cambrian-1: fully open, vision-centric exploration of multimodal llms. Advances in Neural Information Processing Systems, 37:8731087356, 2024. 4 [49] Yuxiang Tuo, Wangmeng Xiang, Jun-Yan He, Yifeng Geng, and Xuansong Xie. Anytext: Multilingual visual text generation and editing. arXiv preprint arXiv:2311.03054, 2023. 2 [50] Yuxiang Tuo, Yifeng Geng, and Liefeng Bo. Anytext2: Visual text generation and editing with customizable attributes. arXiv preprint arXiv:2411.15245, 2024. 1, 2 [51] Bram Wallace, Meihua Dang, Rafael Rafailov, Linqi Zhou, Aaron Lou, Senthil Purushwalkam, Stefano Ermon, Caiming Xiong, Shafiq Joty, and Nikhil Naik. Diffusion model alignment using direct preference optimization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 82288238, 2024. 10 chart generation with automatic feedback. arXiv preprint arXiv:2410.04064, 2024. 1 [65] Songheng Zhang, Lei Wang, Toby Jia-Jun Li, Qiaomu Shen, Yixin Cao, and Yong Wang. Chartifytext: Automated chart generation from data-involved texts via llm. arXiv preprint arXiv:2410.14331, 2024. 1 [66] Yi-Fan Zhang, Xingyu Lu, Shukang Yin, Chaoyou Fu, Wei Chen, Xiao Hu, Bin Wen, Kaiyu Jiang, Changyi Liu, Tianke Zhang, et al. Thyme: Think beyond images. arXiv preprint arXiv:2508.11630, 2025. 1 [67] Shitian Zhao, Haoquan Zhang, Shaoheng Lin, Ming Li, Pyvision: arXiv preprint Qilong Wu, Kaipeng Zhang, and Chen Wei. Agentic vision with dynamic tooling. arXiv:2507.07998, 2025. [68] Xuanle Zhao, Xuexin Liu, Haoyue Yang, Xianzhen Luo, Fanhu Zeng, Jianling Li, Qi Shi, and Chi Chen. Chartedit: How far are mllms from automating chart analysis? evaluating mllms capability via chart editing. arXiv preprint arXiv:2505.11935, 2025. 1 [69] Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, Zhangchi Feng, and Yongqiang Ma. Llamafactory: Unified efficient fine-tuning of 100+ language models. arXiv preprint arXiv:2403.13372, 2024. 3 [70] Ziwei Zheng, Michael Yang, Jack Hong, Chenxiao Zhao, Guohai Xu, Le Yang, Chao Shen, and Xing Yu. Deepeyes: Incentivizing thinking with images via reinforcement learning. arXiv preprint arXiv:2505.14362, 2025. 2, 1 [71] Le Zhuo, Ruoyi Du, Han Xiao, Yangguang Li, Dongyang Liu, Rongjie Huang, Wenze Liu, Xiangyang Zhu, Fu-Yun Wang, Zhanyu Ma, et al. Lumina-next: Making lumina-t2x stronger and faster with next-dit. Advances in Neural Information Processing Systems, 37:131278131315, 2024. 2 [52] Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, et al. Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869, 2024. 1, 2 [53] Yibin Wang, Zhimin Li, Yuhang Zang, Yujie Zhou, Jiazi Bu, Chunyu Wang, Qinglin Lu, Cheng Jin, and Jiaqi Wang. Pref-grpo: Pairwise preference reward-based grpo for staarXiv preprint ble text-to-image reinforcement learning. arXiv:2508.20751, 2025. [54] Zhenyu Wang, Aoxue Li, Zhenguo Li, and Xihui Liu. Genartist: Multimodal llm as an agent for unified image generation and editing. Advances in Neural Information Processing Systems, 37:128374128395, 2024. 2 [55] Chenfei Wu, Jiahao Li, Jingren Zhou, Junyang Lin, Kaiyuan Gao, Kun Yan, Sheng-ming Yin, Shuai Bai, Xiao Xu, Yilei Chen, et al. Qwen-image technical report. arXiv preprint arXiv:2508.02324, 2025. 1, 2, 4, 5, 6, 7 [56] Chenyuan Wu, Pengfei Zheng, Ruiran Yan, Shitao Xiao, Xin Luo, Yueze Wang, Wanli Li, Xiyan Jiang, Yexin Liu, Junjie Zhou, et al. Omnigen2: Exploration to advanced multimodal generation. arXiv preprint arXiv:2506.18871, 2025. 1, 2, 6, 7 [57] Jie Wu, Yu Gao, Zilyu Ye, Ming Li, Liang Li, Hanzhong Guo, Jie Liu, Zeyue Xue, Xiaoxia Hou, Wei Liu, et al. Rewarddance: Reward scaling in visual generation. arXiv preprint arXiv:2509.08826, 2025. 1 [58] Shishi Xiao, Suizi Huang, Yue Lin, Yilin Ye, and Wei Zeng. Let the chart spark: Embedding semantic context into chart with text-to-image generative model. IEEE Transactions on Visualization and Computer Graphics, 30(1):284294, 2023. 1 [59] Yicheng Xiao, Lin Song, Yukang Chen, Yingmin Luo, Yuxin Chen, Yukang Gan, Wei Huang, Xiu Li, Xiaojuan Qi, and Ying Shan. Mindomni: Unleashing reasoning generation in vision language models with rgpo. arXiv preprint arXiv:2505.13031, 2025. [60] Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding and generation. arXiv preprint arXiv:2408.12528, 2024. 1, 2 [61] Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagereward: Learning and evaluating human preferences for textto-image generation. Advances in Neural Information Processing Systems, 36:1590315935, 2023. 4, 1, 3 [62] Zeyue Xue, Jie Wu, Yu Gao, Fangyuan Kong, Lingting Zhu, Mengzhao Chen, Zhiheng Liu, Wei Liu, Qiushan Guo, Weilin Huang, et al. Dancegrpo: Unleashing grpo on visual generation. arXiv preprint arXiv:2505.07818, 2025. 1 [63] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. 3, 6 [64] Fatemeh Pesaran Zadeh, Juyeon Kim, and Gunhee Kim. Text2chart31: Jin-Hwa Kim, Instruction tuning for"
        },
        {
            "title": "Appendix",
            "content": "Overview A. More Related Works A.1. Chart Generation with Agentic Tools . . . . A.2. Reinforcement Learning for Image Generation B. More Method Details . B.1. Prompts in Pipeline . . B.2. Rewriting Training Details . . B.3. Refinement Training Details . . . . . . . . . . C. More Dataset and Benchmark Details . . . . C.1. Training Data Format . . C.2. Benchmark Statistics . C.3. Benchmark Evaluation Details . . . . . . . . . D. More Experiments D.1. Results of Wan2.5-Preview . D.2. More Visualization Results . . . . . . . E. Limitations and Future Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 1 1 1 1 2 3 3 3 4 4 7 7 7 A. More Related Works A.1. Chart Generation with Agentic Tools With rapid development of current MLLMs [1, 31, 34, 70], the strong tool calling ability provides promising way to complete multi-modal tasks. Current approaches for chart generation predominantly rely on LLMs coupled with agentic tools, generally falling into three main categories. The first category [42, 47, 65] employs LLMs in an end-toend manner to produce charts directly from text. These methods typically parse input to identify axes, map entities, and classify chart types, subsequently generating structured specifications for rendering engines like Vega-Lite. While automated, their expressiveness is strictly confined by the predefined grammatical rules of the underlying visualization language. The second category [19, 64, 6668] focuses on generating executable plotting code (e.g., via Matplotlib/Python). Recent works have incorporated multiagent frameworks and reflection mechanisms to improve the syntactic correctness of the generated code [16]. Although these methods offer precise control, they heavily depend on external rendering engines and typically lack the capability to handle complex, artistic visual designs beyond standard plots. The third category [58] adopts retrieval-editing pipeline that selects visual templates from an image corpus and adapts them to new data. While this benefits from template reuse, it is limited by the diversity of the database and 1 often faces challenges in accurately aligning new data with retrieved visual structures. Discussion. While existing methods excel in structural automation and factual plotting, they fundamentally struggle with creativity and aesthetics. Code-based and templatebased approaches are bound by rigid rendering logic, making it difficult to produce visually striking infographics suitable for professional poster design, slide generation, or data-driven storytelling. They prioritize correctnes but often fail at presentation. In contrast, our work proposes the Creative Table Visualization task to explore the untapped potential of generative models and unified models in this domain. We argue that generative models offer significantly higher ceiling for flexibility and aesthetic quality, capable of seamlessly integrating data into artistic compositions. By validating the feasibility of this approach, we aim to break the traditional boundaries of rule-based rendering and pave the way for more robust, unified, and creative visual synthesis systems. A.2. Reinforcement Learning for Image Generation Diffusion models have established themselves as the predominant framework for text-to-image (T2I) generation [3, 20, 40, 43, 55]. The integration of reinforcement learning (RL) into this paradigm began with works utilizing policy gradient optimization to guide the denoising process [2, 14, 57, 61]. The field subsequently expanded to include preference-based alignment methods, which achieve competitive performance without explicit reward modeling [51]. significant recent development is the adoption of Group Relative Policy Optimization(GRPO) [18, 44], an efficient alternative that has inspired numerous adaptations for T2I generation. These include pioneering works [32, 62] which unified diffusion and flow matching under an SDE-based formulation. This line of inquiry further explores the design of specialized reward models and data curation strategies to enhance the frameworks capability for producing high-quality, preference-aligned visual outputs [27, 28, 53]. B. More Method Details B.1. Prompts in Pipeline In our ShowTable pipeline, the MLLM acts as the central orchestrator, performing two key roles: rewriting and reflection. We detail the specific prompts used for these modules. The rewriting prompt, shown in Figure A1, instructs the MLLM to reason over the input table and translate its dense data into detailed descriptive prompt suitable for the diffusion executor. The reflection prompt, shown in Figure A1. The system prompt and user prompt for the rewriting module. We use the same prompts for all models. Figure A2. The system prompt and user prompt for the reflection module. We use the same prompts for all models. Figure A2, guides the MLLM to critically audit the generated image against the original table, identify inaccuracies, and formulate precise, actionable editing instructions for the refinement stage. To ensure fair comparison and consistency across experiments, we use the same system and user prompts for all models tested in these roles. B.2. Rewriting Training Details As discussed in Section 3.2, we fine-tune specialized rewriting module based on Qwen3-8B to handle the critical task of reasoning and compositional planning. This module is trained on our 30K SFT data with both thinking and rewriting result, and we construct various kinds of instruction templates during training to ensure the diversity. For 2 Figure A3. The reward comparison between our RM and direct LLM scoring, we achieve stable reward increase. the implementation, we utilize the LLaMA-Factory [69] library. We train the model for 3 epochs with total batch size of 256. The training employs learning rate of 1e-5, combined with cosine learning rate decay strategy to stabilize the training process. Through targeted fine-tuning, the resulting rewrite model acquires the ability to infer appropriate data visualization and layout strategies. The thinking process enables the model to generate significantly improved prompts, thereby enhancing both data integrity and accuracy throughout the prompt rewriting and subsequent image generation pipeline. B.3. Refinement Training Details To empower the refinement model with the capability for precise, fine-grained edits on dense data infographics, we employ an on-policy reinforcement learning approach utilizing the Group Relative Policy Optimization (GRPO) [18] algorithm. In this framework, rendering accuracy serves as the critical reward signal. Reward model. Evaluating the rendering accuracy of infographics is complex, requiring the integration of multiple dimensionssuch as textual correctness, data-to-visual alignment, and spatial layout. Our preliminary experiments revealed that utilizing state-of-the-art pretrained VisionLanguage Models (VLMs) directly for point-wise reward assessment is suboptimal. As shown in Figure A3, the inconsistency in VLM scoring often leads to training instability or collapse. Furthermore, the high inference latency of VLMs significantly hampers the efficiency of the on-policy GRPO loop. To address this, we develop specialized, efficient Reward Model (RM). We construct pairwise dataset consisting of positive and negative graphic samples generated from the same prompt, as detailed in Section 4.2. We finetune Qwen2.5-VL-3B [1] model, denoted as fθ, to serve as the quality assessor. For given prompt p, with xw denoting the preferred (positive) sample and xl the dispreferred (negative) sample, the model is optimized using the BradleyTerry (BT) loss [4]: LBT = E(p,xw,xl)D [log σ (fθ(xw, p) fθ(xl, p))] , (A1) where σ() represents the Sigmoid function. To stabilize the output, the reward score is computed by extracting and averaging the probabilities corresponding to the tokens for digits 09 from the output logits. The final reward signal used in RL is weighted combination: = 0.8 fθ(x, p) + 0.2 ImageReward(x, p) [61]. Policy optimization. The refinement policy is updated using the GRPO algorithm. The objective function maximizes the expected reward while constraining policy divergence via clipped surrogate objective: JGRPO(θ) ="
        },
        {
            "title": "1\nG",
            "content": "G (cid:88) i=1 (cid:34) EyD min (cid:18) πθ(oiy) πθold(oiy) Ai, clip (cid:18) πθ(oiy) πθold(oiy) (cid:19) (cid:19) , 1 ε, 1 + ε Ai βDKL (πθ πref) (cid:35) , (A2) where ε and β are hyperparameters, and is the group size. The advantage Ai is computed by normalizing the rewards {r1, r2, , rG} within each group: Ai = ri mean({r1, , rG}) std({r1, , rG}) + ϵ . (A3) Implementation. Following the Flow-GRPO framework [32], we train our refinement model using distilled 8-step version of Qwen-Image-Edit-2509 [37] to accelerate training efficiency. The model is trained on our constructed 5K refinement dataset for 1 epoch using 32 GPUs. We set the image resolution to 10241024 and perform 16 rollouts per prompt. We utilize 8 sampling steps for both the inference rollout and the training backward pass. As illustrated in Figure A3, compared to the unstable baseline using raw LLM scores, our approach with the trained Reward Model achieves stable and consistent performance gains. C. More Dataset and Benchmark Details C.1. Training Data Format As illustrated in Figure 5, our data construction pipeline transforms raw collected images into specialized datasets tailored for the rewriting, refinement, and reward modules. Below, we detail the specific input and output formats for each training stage. Rewriting Training Data (SFT). The goal of the rewriting module is to convert raw markdown table into comprehensive visual plan. To support this, we construct Supervised Fine-Tuning (SFT) data that teaches the model to first reason about the data structure (Rationale) and then describe the visual elements (Description). Input: The raw table data in markdown format, annotated from the collected image pool. 3 Figure A4. The Statistical information about our proposed TableVisBench. (a) (Left): the data length distribution of TableVisBench. (b) (Right): The word cloud of the topic in TableVisBench. Output: composite text sequence consisting of Chain-of-Thought Rationale followed by Detailed Image Description. Construction: As shown in the green section of Figure 5, we first prompt Gemini-2.5-pro to generate descriptive caption (Table-based description). Then, we feed both the table and the description into Gemini-2.5pro again to reverse-engineer the reasoning process (Rationale to description), forming complete training sample: Table Rationale + Description. Refinement Training Data (RL). For the reinforcement learning stage, the training data consists of challenging scenarios where an initial generation with correction instructions, and no ground-truth image is needed. This data is formatted as prompt-response pairs for the policy model. Input: pair consisting of an Initial Generated Image (containing errors) and precise Refinement Instruction. Output: No refined image is needed. Construction: As shown in the blue section of Figure 5, we generate initial images from our descriptions and use Gemini-2.5-pro to compare them against the table, producing Refine command. To ensure the data is valid for training, we perform Rollout check: we discard samples where the base model either fails to improve the image over multiple attempts (too hard) or solves it trivially (too easy), retaining only those suitable for learning stable policy gradients. Reward Training Data (Preference Pairs). To train the reward model fθ as reliable quality assessor, we construct dataset of preference image pairs, focusing on the data fidelity. Input: text condition (the table) and two candidate images (xw, xl). Output: binary label indicating which image is the Winner (xw) and which is the Loser (xl). Construction: As shown in the purple section of Figure 5, we source candidates from three comparisons: (1) Refined vs. Initial images, (2) Strong (Wan2.5) vs. Weak (Qwen) model outputs, and (3) Ground-truth vs. Generated images. MLLMs (GPT-5 and Gemini-2.5-pro) act as judges to vote on the pair, establishing high-confidence Pos-Neg Pair dataset for training the reward model to discriminate fine-grained visual differences. C.2. Benchmark Statistics To demonstrate the diversity and complexity of our proposed TableVisBench, we present detailed statistical characteristics in Figure A4. Data Length Distribution. Figure A4 (a) illustrates the distribution of table lengths, defined as the number of key data points per instance. The benchmark covers broad spectrum of information density, ranging from concise tables (fewer than 5 rows) to highly complex ones. The distribution shows natural concentration between 5 to 15 data points, reflecting common real-world infographic scenarios. Notably, we also include some long-tail instances, with the final bin aggregating tables containing over 40 data points. This design ensures that the benchmark rigorously evaluates models not only on standard visualizations but also on their stability and layout planning capabilities when handling high-density data. Topic Diversity. Figure A4 (b) presents word cloud visualization derived from the topics of the collected tables. The dataset encompasses wide array of domains, with prominent keywords including Social Media, Distribution, Percentage, User, Market, and Mobile. This semantic diversity confirms that TableVisBench covers various distinct fieldssuch as business reports, sociological statistics, and technology usagethereby assessing the models generalization ability across different contexts and terminologies. C.3. Benchmark Evaluation Details To ensure rigorous and reproducible evaluation, we design deterministic scoring mechanism for our TableVisBench. Instead of asking the MLLM to directly output subjective score (e.g., 1-10), we employ the MLLM as Quality Assurance Analyst to identify and count specific errors based 4 Figure A5. The system prompt and user prompt of the Data Accuracy dimension. Figure A6. The system prompt and user prompt of the Text Rendering dimension. on strict definitions. The final scores are calculated deterministically from these counts. Below are the detailed calculation protocols for the four accuracy-based dimensions. Data Accuracy (DA). This dimension measures the completeness and correctness of the data points rendered. The MLLM identifies the total number of data points in the source table (Ntotal) and counts the number of incorrect data points (Nerror) in the image (including missing values, wrong numbers, or incorrect legend mappings). The specific prompt is shown in Figure A5. The score is calculated as: SDA = Ntotal Nerror Ntotal (A4) Text Rendering (TR). This dimension evaluates the 5 character-level correctness of textual elements. The MLLM extracts all visible text strings from the image and identifies specific substrings or characters that contain errors (e.g., typos, garbled text). The specific prompt is shown in Figure A6. Let Ltotal be the total character length of all text in the image, and Lerror be the total character length of the identified incorrect text. The score is defined as: ST = Ltotal Lerror Ltotal (A5) If no text is present (Ltotal = 0), the score is set to 0. Relative Relationship (RR). This dimension assesses the visual proportionality of the infographic (e.g., whether bar heights or pie slice angles correspond to the data values). Figure A7. The system prompt and user prompt of the Relative Relationship dimension. Figure A8. The system prompt and user prompt of the Additional information Accuracy dimension. Similar to DA, the MLLM counts the total data points (Ntotal) and identifies the number of points (Nerror) that violate visual logic relative to other elements. The specific prompt is shown in Figure A7. The score is calculated as: SRR = Ntotal Nerror Ntotal (A6) Additional information Accuracy (AA). This dimension evaluates contextual elements such as axes, ticks, and extra 6 annotations. The score is an average of up to three submetrics, depending on the elements present in the image: 1. Label Logic (Slbl): If axis indicators exist, the model estimates the percentage of incorrect tick labels (Perr). Then Slbl = 1 Perr. 2. Axis Alignment (Salign): the model checks the visual alignment of data points against axis ticks. Let Nmis be the number of misaligned points. Then Salign = NtotalNmis . If axes exist, Ntotal Table A1. Performance of Wan2.5-Preview on our TableVisBench. Rewriting Generation Reflection Refinement DA TR RR AA AQ Score Reference Image Wan2.5-T2I-Preview - Qwen3-8B* Wan2.5-T2I-Preview - - - - Qwen3-8B* Wan2.5-T2I-Preview Gemini-2.5-pro Qwen-Image-Edit-2509 Qwen3-8B* Wan2.5-T2I-Preview Gemini-2.5-pro Qwen-Image-Edit-2509* Qwen3-8B* Wan2.5-T2I-Preview Gemini-2.5-pro Wan2.5-I2I-Preview 97. 99.5 86.4 96.6 62.8 73.3 53.3 68.9 76.9 82.5 92. 88.3 91.7 91.1 62.8 70.5 53.5 64.9 74.3 56.5 79.7 59.5 71.2 74.3 4. 4.8 4.4 4.3 4.4 4.4 84.4 62.5 72.1 59.5 68.1 72.1 3. Artifact Appropriateness (Smark): If other marks (e.g., Markdown delimiters, random symbols) exist, the model estimates the percentage of inappropriate marks (Pinapp). Then Smark = 1 Pinapp. The specific prompt is shown in Figure A8. The final SAA is the arithmetic mean of the valid sub-metrics. If no additional information is present, this dimension is excluded from the calculation. Aesthetic Quality (AQ). Unlike the strictly factual dimensions above, this dimension evaluates the overall visual appeal, including layout harmony, color palette suitability, and typographic quality. Since aesthetic judgment relies on subjective perception rather than rule-based error counting, we employ dedicated pre-trained aesthetic scoring model to quantitatively assess this dimension. The model provides scalar score SAQ ranging from 0 to 10, reflecting the artistic quality of the infographic independent of its data fidelity. D. More Experiments D.1. Results of Wan2.5-Preview In the main paper, we primarily focused on open-source models to ensure reproducibility. Here, we extend our evaluation to the recently released Wan2.5-Preview series [46] to assess the ShowTable pipelines performance with stateof-the-art (SOTA) generation capabilities. The results are presented in Table A1. Impact of rewriting. Consistent with our findings on opensource models, the rewriting module provides substantial performance boost for Wan2.5-T2I-Preview. By converting the raw table into reasoned visual plan (RW), the overall score increases significantly from 62.5 to 72.1 (+9.6). This improvement is particularly evident in Data Accuracy (62.8 73.3) and Text Rendering (82.5 92.9), confirming that our rewriting strategy is model-agnostic and effective even for top-tier generation models. Analysis of refinement. The refinement stage reveals the critical importance of the editing models underlying capacity. When applying the open-source Qwen-Image-Edit2509 to refine images generated by the powerful Wan2.5T2I-Preview, we observe performance degradation (Score 72.1 59.5) with the base editor. This is attributed to the significant capacity gap between the strong generator and the relatively weaker editor. Essentially, the editor struggles to maintain the high-fidelity details produced by the SOTA generator. However, our proposed RL training demonstrates clear effectiveness in this challenging scenario. Our trained model (Qwen-Image-Edit-2509*) significantly recovers performance compared to the base editor, raising the score from 59.5 to 68.1 (+8.6). Finally, when utilizing Wan2.5-I2I-Preview as the executor, matching the generators capacity, the pipeline achieves the highest performance in key structural metrics, with Data Accuracy improving further to 76.9 and Relative Relationship to 74.3. This underscores that while our training effectively boosts open-source editors, the ceiling of the refinement stage is ultimately determined by the base models capability. D.2. More Visualization Results Qualitative comparisons. To further qualitatively demonstrate the universality and effectiveness of our approach, we present comprehensive case comparison across different base models in Figure A9. As evident in the first column of Figure A9, without the rewriting module, most base models fail to fundamentally grasp the visualization task. General unified models (e.g., Blip3o-Next, UniWorld-V1, OmniGen2) often suffer from severe hallucinations, producing chaotic layouts that lack any statistical meaning. Strong text-rendering models like Qwen-Image and Flux also fail to render the structure logits among data points. The second column (RW + gen) highlights the critical role of our rewriting module. By translating the data into visual plan, all models successfully transition from unstructured chaos to coherent infographic layout (specifically, donut chart structure in this example), though most of them still suffer from heavy logical errors. Note that Wan2.5-Preview successfully generates promising result, leading to no need of further refinement. The subsequent columns (REF round 13) demonstrate the power of the progressive self-correction loop. Visual inaccuracies, such as incorrect data segmentation, garbled text, and layout misalignments, are iteratively repaired. For example, in the Bagel and UniWorld-V1 rows, the text legibility and data mapping precision improve noticeably with each round. Additionally, the Wan2.5-Preview 7 Figure A9. The qualitative comparison of different generation baselines of the same case on our proposed creative table visualization task. The first column presents the results without rewriting. The results via our ShowTable pipeline are shown from the second column. case (bottom row) showcases the efficiency of our pipeline; due to its high initial quality, the reflection module triggers the early-stopping mechanism (Done), avoiding unnecessary computation. 8 Detailed pipeline visualization. To provide transparent view of the internal mechanisms of our ShowTable pipeline, we present series of detailed case visualizations in the following figures. These figures explicitly display the stepby-step intermediate outputs, including the source table, the MLLM-generated rewriting prompt, the initial generation, and the iterative reflection instructions that guide the refinement. To demonstrate the pipelines versatility across different base models, the first three figures (Figure A10, Figure A11, Figure A12) utilize Qwen-Image as the generation module, while the subsequent two figures (Figure A13, Figure A14) employ Wan2.5-Preview. As shown in the examples, the visualization highlights the orchestrators ability to formulate precise geometric corrections (For example, adjust the height of the bar to... resulting in ratio of approximately 2.69:1 or ensure the green Yes slice occupies exactly 81% (291.6)), effectively guiding the executor to achieve high-fidelity data alignment. E. Limitations and Future Work While our ShowTable pipeline demonstrates significant improvements in creative table visualization, there remain several limitations that open avenues for future research: Full-pipeline training. First, our current work primarily explores the training of the rewriting and refinement modules. The generation and reflection modules still rely on off-the-shelf models. We believe that extending supervised fine-tuning (SFT) or reinforcement learning (RL) to all components of the pipeline could further enhance the systems robustness and domain adaptability. Future work will investigate more holistic training strategy to optimize the entire pipeline jointly. Towards unified model. Second, the current implementation operates as cascade of distinct models for rewriting, generation, reflection, and refinement. While effective, this multi-model approach increases deployment complexity and inference latency. promising future direction is to explore unified multi-modal architecture capable of performing all four sub-tasks end-to-end. Integrating these capabilities into single model could significantly streamline the workflow and improve efficiency. Dependency on foundation models. Finally, the performance of our pipeline is inevitably constrained by the capabilities of the underlying base models. As observed in our refinement experiments, the upper bound of visualization fidelity is often determined by the precision of refinement module. We hope that our proposed task and benchmark will encourage the community to focus more on enhancing these foundational capabilities, particularly in precise fine-grained editing and logical reasoning, thereby advancing the field closer to more general and capable Artificial General Intelligence (AGI). 9 Figure A10. Some detailed pipeline visualizations. We use Qwen-Image here for base generation module with our pipeline. 10 Figure A11. Some detailed pipeline visualizations. We use Qwen-Image here for base generation module with our pipeline. 11 Figure A12. Some detailed pipeline visualizations. We use Qwen-Image here for base generation module with our pipeline. 12 Figure A13. Some detailed pipeline visualizations with more complex and difficult cases. We use Wan2.5-Preview here for generation module and refinement module with our pipeline. 13 Figure A14. Some detailed pipeline visualizations with more complex and difficult cases. We use Wan2.5-Preview here for generation module and refinement module with our pipeline."
        }
    ],
    "affiliations": [
        "CASIA",
        "FDU",
        "NJU",
        "SJTU",
        "Tongyi Lab",
        "USTC",
        "ZJU"
    ]
}