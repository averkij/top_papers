{
    "paper_title": "On Path to Multimodal Generalist: General-Level and General-Bench",
    "authors": [
        "Hao Fei",
        "Yuan Zhou",
        "Juncheng Li",
        "Xiangtai Li",
        "Qingshan Xu",
        "Bobo Li",
        "Shengqiong Wu",
        "Yaoting Wang",
        "Junbao Zhou",
        "Jiahao Meng",
        "Qingyu Shi",
        "Zhiyuan Zhou",
        "Liangtao Shi",
        "Minghe Gao",
        "Daoan Zhang",
        "Zhiqi Ge",
        "Weiming Wu",
        "Siliang Tang",
        "Kaihang Pan",
        "Yaobo Ye",
        "Haobo Yuan",
        "Tao Zhang",
        "Tianjie Ju",
        "Zixiang Meng",
        "Shilin Xu",
        "Liyu Jia",
        "Wentao Hu",
        "Meng Luo",
        "Jiebo Luo",
        "Tat-Seng Chua",
        "Shuicheng Yan",
        "Hanwang Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The Multimodal Large Language Model (MLLM) is currently experiencing rapid growth, driven by the advanced capabilities of LLMs. Unlike earlier specialists, existing MLLMs are evolving towards a Multimodal Generalist paradigm. Initially limited to understanding multiple modalities, these models have advanced to not only comprehend but also generate across modalities. Their capabilities have expanded from coarse-grained to fine-grained multimodal understanding and from supporting limited modalities to arbitrary ones. While many benchmarks exist to assess MLLMs, a critical question arises: Can we simply assume that higher performance across tasks indicates a stronger MLLM capability, bringing us closer to human-level AI? We argue that the answer is not as straightforward as it seems. This project introduces General-Level, an evaluation framework that defines 5-scale levels of MLLM performance and generality, offering a methodology to compare MLLMs and gauge the progress of existing systems towards more robust multimodal generalists and, ultimately, towards AGI. At the core of the framework is the concept of Synergy, which measures whether models maintain consistent capabilities across comprehension and generation, and across multiple modalities. To support this evaluation, we present General-Bench, which encompasses a broader spectrum of skills, modalities, formats, and capabilities, including over 700 tasks and 325,800 instances. The evaluation results that involve over 100 existing state-of-the-art MLLMs uncover the capability rankings of generalists, highlighting the challenges in reaching genuine AI. We expect this project to pave the way for future research on next-generation multimodal foundation models, providing a robust infrastructure to accelerate the realization of AGI. Project page: https://generalist.top/"
        },
        {
            "title": "Start",
            "content": "On Path to Multimodal Generalist: General-Level and General-Bench Hao Fei*1 Yuan Zhou*2 Juncheng Li*3 Xiangtai Li*2 Qingshan Xu*2 Bobo Li*1 Shengqiong Wu*1 Yaoting Wang4 Junbao Zhou2 Jiahao Meng5 Qingyu Shi5 Zhiyuan Zhou6 Liangtao Shi6 Minghe Gao3 Daoan Zhang7 Zhiqi Ge3 Weiming Wu8 Siliang Tang3 Kaihang Pan3 Yaobo Ye3 Haobo Yuan2 Tao Zhang9 Tianjie Ju10 Zixiang Meng9 Shilin Xu5 Liyu Jia2 Wentao Hu2 Meng Luo1 Jiebo Luo7 Tat-Seng Chua1 Shuicheng Yan1 Hanwang Zhang2 Project Page: https://generalist.top Leaderboard: https://generalist.top/leaderboard Benchmark: https://huggingface.co/General-Level 5 2 0 M 7 ] . [ 1 0 2 6 4 0 . 5 0 5 2 : r Figure 1: Leaderboard of multimodal generalists over General-Level (only top-performing ones shown here)."
        },
        {
            "title": "Abstract",
            "content": "The Multimodal Large Language Model (MLLM) is currently experiencing rapid growth, driven by the advanced capabilities of language-based LLMs. Unlike their specialist predecessors, existing MLLMs are evolving towards *Equal contribution and Co-team leader. 1NUS 2NTU 3ZJU 4KAUST 5PKU 6HFUT 7UR 8NJU 9WHU 10SJTU. Project leader: Hao Fei <haofei37@nus.edu.sg>. Correspondence to: Shuicheng Yan <yansc@nus.edu.sg>, Hanwang Zhang <hanwangzhang@ntu.edu.sg>. 1 On Path to Multimodal Generalist: General-Level and General-Bench Multimodal Generalist paradigm. Initially limited to understanding multiple modalities, these models have advanced to not only comprehend but also generate across modalities. Their capabilities have expanded from coarse-grained to fine-grained multimodal understanding and from supporting singular modalities to accommodating wide array of or even arbitrary modalities. To assess the capabilities of various MLLMs, diverse array of benchmark test sets has been proposed. This leads to critical question: Can we simply assume that higher performance across tasks indicates stronger MLLM capability, bringing us closer to human-level AI? We argue that the answer is not as straightforward as it seems. In this project, we introduce an evaluation framework to delineate the capabilities and behaviors of current multimodal generalists. This framework, named General-Level, establishes 5-scale levels of MLLM performance and generality, offering methodology to compare MLLMs and gauge the progress of existing systems towards more robust multimodal generalists and, ultimately, towards AGI (Artificial General Intelligence). Central to our framework is the use of Synergy as the evaluative criterion, categorizing capabilities based on whether MLLMs preserve synergy across comprehension and generation, as well as across multimodal interactions. To evaluate the comprehensive abilities of various generalists, we present massive multimodal benchmark, General-Bench, which encompasses broader spectrum of skills, modalities, formats, and capabilities, including over 700 tasks and 325,800 instances. The evaluation results that involve over 100 existing state-of-the-art MLLMs uncover the capability rankings of generalists, highlighting the challenges in reaching genuine AI. We expect this project to pave the way for future research on next-generation multimodal foundation models, providing robust infrastructure to accelerate the realization of AGI."
        },
        {
            "title": "Table of Contents",
            "content": ""
        },
        {
            "title": "3.1 Preliminary .",
            "content": ". . . . . . . . . . . ."
        },
        {
            "title": "3.1.1 Observations and Principles",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.1.2 Synergy as Core to Multimodal Generalists . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "3.2 Defining Levels Centered on Synergy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
            "content": "3.2.1 Scoring Specification . 3.2.2 Scoring Relaxation . . . . . . . . 3.2. Properties of General-Level . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 5 6 6 6 7 8"
        },
        {
            "title": "3.3 Receipt to Leveling Upper in General-Level",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "4.1 Data Construction .",
            "content": ". . . ."
        },
        {
            "title": "4.1.1 Design Criterion .",
            "content": ". . . ."
        },
        {
            "title": "4.3 Data Insights",
            "content": ". . . . . . . ."
        },
        {
            "title": "4.4 Leaderboard Re-Scoping .",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 13 13 14 15 17 2 On Path to Multimodal Generalist: General-Level and General-Bench"
        },
        {
            "title": "5.1 Multimodal Specialist and Generalist Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
            "content": "."
        },
        {
            "title": "5.2 Experimental Settings .",
            "content": ". . ."
        },
        {
            "title": "5.3 Overall Evaluation Results .",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "5.5 Capability BreakDown .",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "5.6 Analysis and Discussion on Synergy .",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "7 Conclusion",
            "content": "A Extension on General-Bench Dataset A.1 Evaluation Metrics A.2 Data Format . . . . . . . . . . . . . . . . . . A.3 Data Taxonomy and Hierarchy . A.4 Data Distributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.5 Comparisons with Existing Benchmarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.6 Complete List of Tasks and Skills (Meta-Tasks) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 22 22 30 32 34 35 75 75 80 87 89 90 A.7 Example Task Gallery . . . . A.7.1 Image-related Tasks A.7.2 Video-related Tasks . A.7.3 Audio-related Tasks A.7.4 3D-related Tasks . A.7.5 Language Tasks. . . . . . . . . . . . . . . ."
        },
        {
            "title": "B Extension on Experimental Results",
            "content": "B.1 Results of Image-related Tasks . B.2 Results of Video-related Tasks . B.3 Results of Audio-related Tasks . B.4 Results of 3D-related Tasks . B.5 Results of NLP Tasks . . . ."
        },
        {
            "title": "C Statement",
            "content": "C.1 Ethical Statement . . C.2 Author Contribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 126 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 126 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 167 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 204 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 224 237 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 237 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 266 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 274 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 276 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 278 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 300 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 301 300 3 On Path to Multimodal Generalist: General-Level and General-Bench"
        },
        {
            "title": "1 Introduction",
            "content": "Large Language Models (LLMs, e.g., ChatGPT (OpenAI, 2022a) and LLaMA (Touvron et al., 2023)) have revolutionized the NLP field by serving as generalists addressing vast spectrum of NLP tasks. This breadth of capability has edged humans ever closer to the realization of Artificial General Intelligence (AGI). Yet, human intelligence inherently operates across multiple modalities, not solely through language. This observation has spurred the development of multimodal LLMs (Alayrac et al., 2022; Li et al., 2023a; Liu et al., 2023a; OpenAI, 2022b), i.e., multimodal generalists, which are rapidly gaining traction and evolving towards AGI. The recent progress in MLLMs is marked by significant advancements. For example, the initial multimodal agents where LLMs serve as mere task schedulers, later have evolved into joint foundation MLLMs (Zhu et al., 2023a; Liu et al., 2023a; Zhang et al., 2023a; OpenAI, 2022b; Wu et al., 2024a; Chen et al., 2024a; Sun et al., 2024). Also, MLLMs have progressed from understanding only multimodal signals to both comprehending and generating multimodal content, even editing capabilities (Wang et al., 2023a; Munasinghe et al., 2023; Zhang et al., 2024a; Fei et al., 2024a). Further, these models have advanced from coarse-grained modal understanding to fine-grained multimodal comprehension, such as pixel-level visual modeling (Ren et al., 2023; Yuan et al., 2023a; Rasheed et al., 2023). More significantly, MLLMs that initially support only singleton non-textual modalities have now facilitated the understanding and generation of signals across various modalities, even simultaneously accommodating any modality (Wu et al., 2024a; Zhan et al., 2024; Lu et al., 2024a). Accordingly, the community has introduced various benchmarks to evaluate those MLLMs (Wu et al., 2023a; Xia et al., 2024a; Yue et al., 2024a; Meng et al., 2024a; Liu et al., 2025; Li et al., 2024a; Ying et al., 2024a; Li et al., 2024b). The prevailing evaluation mindset might yet be largely outdated, simplistically assuming that superior performance across tasks presents stronger generalist capability (Xu et al., 2023a; Yu et al., 2023; Fu et al., 2024a; Chen et al., 2024b), and then being closer to AGI. We contend this perspective overly simplifies the implication inherent in real multimodal generalization. Theoretically, its effortless to assemble super agent from all singleton state-of-the-art (SoTA) specialists to achieve the above goal, while such simplistic integration would never suffice to realize genuine AGI. We argue that the key to advancing towards AGI lies in the synergy effecta capability that enables knowledge learned in one modality or task to generalize and enhance mastery in other modalities or tasks, fostering mutual improvement across different modalities and tasks through interconnected learning.1 As illustrated in Figure 2, most current MLLMs predominantly build on the language intelligence of LLMs to simulate the indirect intelligence of multimodality, which is merely extending language intelligence to aid multimodal understanding. While LLMs (e.g., ChatGPT) have already demonstrated such synergy in NLP, reflecting language intelligence, unfortunately, the vast majority of MLLMs do not really achieve it across modalities and tasks. In this project, we introduce sophisticated evaluation framework, General-Level, for more accurately positioning and assessing the capabilities of current MLLM generalists, charting path toward authentic multimodal AGI. Drawing inspiration from the tiered classification mechanism in the automotive industry for autonomous vehicles (Yurtsever et al., 2020), General-Level defines five principal levels of model performance and generality. Central to the framework is the synergy ability as the evaluative criterion, categorizing capabilities based on whether generalists preserve synergy in and across multimodal comprehension and generation, as well as cross-modal interactions. From the lowest to the highest level, the scope of synergy ability required progressively escalates from single tasks or modalities to total synergy. As generalist strives to advance to higher level, it must demonstrate significant enhancements in its synergy capabilities, during which the difficulty of progression is also inherently increasing. To effectively evaluate within the General-Level framework, suitable benchmark is essential. While there are numerous MLLM evaluation benchmarks, e.g., LVLM-eHub (Xu et al., 2023a), MME (Fu et al., 2024a), MMMU (Yue et al., 2024a), SEED-Bench (Li et al., 2024a), MMT-Bench (Ying et al., 2024a), and MEGA-Bench (Chen et al., 2024b), they might have certain limitations that render them inadequate for our needs. Firstly, existing benchmarks often convert all tasks into uniform multiple-choice QA format (Fu et al., 2024a; Ying et al., 2024a), simplifying the evaluation process but consequently restricting assessments to only the models multimodal comprehension capabilities. However, true multimodal generalist should support not only comprehension, but also possess capabilities in multimodal generation, editing, and beyond. Second, the majority of current benchmarks (Wu et al., 2023a; Liu et al., 2025; Li et al., 2024a) predominantly focus on the image modality and overlook other crucial modalities such as video, audio, even 3D and beyond, which are vital for robust multimodal generalist. Third, these benchmarks are typically limited to coarse-grained multimodal understanding (Xu et al., 2023a; Yu et al., 2023; Fu et al., 2024a) and fail to adequately assess finer-grained 1Synergy, in essence, can be understood as form of generalization ability. On Path to Multimodal Generalist: General-Level and General-Bench Figure 2: The intelligence in most existing multimodal generalists (i.e., MLLMs) hinges on language intelligence (i.e., from LLMs) (a), whereas the ideal intelligence mode should be maintaining synergy across all modalities and tasks (b). ones, which actually lag far behind the current advancements in MLLMs, i.e., supporting pixel-level image understanding and generation (Fei et al., 2024a; Zhang et al., 2024a). In response to these challenges, we propose General-Bench, which is massive multimodal evaluation benchmark, spanning from various modalities (e.g., image, video, audio, 3D, language, and beyond) in diverse native formats, covering wide range of tasks that thoroughly assess the full capabilities of multimodal generalist. Our evaluation of over 100 existing top-performing LLM/MLLM systems has uncovered critical insights into their capabilities and rankings as multimodal generalists. The most notable finding is that most MLLMs lack the cross-task or cross-modal synergy ability required for higher-level classifications, with even advanced models like GPT-4V and GPT-4o not achieving top ranks. This highlights considerable gap in achieving the goals of multimodal generalists. Also, the majority of existing MLLMs manage only few basic multimodal tasks and skills, which negatively affects their scoring. Most critically, no model has yet demonstrated the ability to enhance language intelligence through non-language modalities, underscoring the substantial challenges in the pursuit of genuine AGI. Contributions: 1) We introduce tiered classification system called General-Level for multimodal generalists, establishing rigorous standard or norm that can guide future MLLM research. 2) We contribute new evaluation benchmark (General-Bench) that provides the most comprehensive coverage of modalities and tasks available to date. We hope this project will serve as an infrastructure to facilitate the development of next-generation multimodal foundation models in achieving more capable and general-purpose multimodal intelligence."
        },
        {
            "title": "2 Background and Related Work",
            "content": "More and more tend to recognize that LLMs have unlocked the potential of language intelligence, bringing unprecedented hope to achieve AGI. Essentially, an LLM serves as generalist capable of tackling nearly all downstream NLP tasks. LLMs have subsequently evolved in an effort to extend this intelligence across various other modalities, i.e., MLLMs (Bai et al., 2023; Zhang et al., 2023b; Jin et al., 2023; Li et al., 2024c; Fei et al., 2024b;c). Unlike the past smaller specialists (Van Den Oord et al., 2016; Radford et al., 2021; Rombach et al., 2022; Liu et al., 2023b), MLLMs represent an important advancement of unification to handle all modalities and tasks with one foundation model, i.e., multimodal generalists. Naturally, empowering multimodal generalist with strong multimodal intelligence capabilities is an essential pathway toward realizing AGI. Technically, the vast majority of existing MLLMs have frameworks that are anchored by an LLM to serve as the core for reasoning and decision-making. By integrating various well-trained modules of different modalities or tasks (typically existing specialists, e.g., CLIP (Radford et al., 2021) and Stable Diffusion (Rombach et al., 2022)), MLLMs are facilitated with the comprehension and even generation of diverse modalities. Representative MLLMs include Blip2 (Li et al., 2023a), LLAVA (Liu et al., 2023a), MiniGPT-4 (Zhu et al., 2023a), Flamingo (Alayrac et al., 2022), and NExT-GPT (Wu et al., 2024a), among others. However, such an architectural setup merely simulates pseudo multimodal intelligence, as it still fundamentally relies on the language intelligence of LLMs without genuine non-language modality intelligence. As 5 On Path to Multimodal Generalist: General-Level and General-Bench emphasized earlier, capable generalist must possess synergy capabilities across all modalities and tasks, akin to how an LLM (e.g., ChatGPT) generalizes well to unseen NLP tasks, despite not being exposed to all tasks during its training. While these current multimodal generalists can deliver strong performances on multimodal benchmarks, sometimes even on par with SoTA specialists, they do not fundamentally achieve true synergy. Consequently, this paper positions synergy as the central criterion for evaluating multimodal generalists on their journey toward AGI. Current evaluation methods (Li et al., 2024b) for MLLMs still adhere to the traditional approach used for specialists, simply comparing the MLLM performance on multimodal tasks, assuming that higher scores indicate greater strength and closer proximity to AGI. Going beyond that, we propose new evaluation frameworknot only do we compare whether models support various modalities and tasks and their performance, but we also rank them based on the synergy capabilities of multimodal generalists. Meanwhile, we significantly expand the scope of current MLLM benchmark datasets in terms of modality and task coverages, as well as task formats, contributing to the most comprehensive benchmark dataset to date in the community."
        },
        {
            "title": "3.1.1 OBSERVATIONS AND PRINCIPLES",
            "content": "Observation-1: Multimodal Comprehension vs. Simultaneous Multimodal Comprehension and Generation. Initially, MLLMs are capable only of interpreting multimodal signals, meaning their responses are limited to textual outputs based on user-provided multimodal inputs. However, an MLLM that only offers multimodal comprehension operates at the most basic and rudimentary level. More advanced MLLMs have since emerged, equipped with not only multimodal comprehension but also the ability to generate and even edit content across various modalities. It is widely believed that the more advanced multimodal generalist is, the more it should encompass advanced functionalities, encompassing both comprehension and generation. Observation-2: Covering Broader Modalities. Being multimodal generalist requires the ability to extensively support and handle wide range of modal data, including, but not limited to, text, images, videos, audio, and even 3D. The extent of modal support is indicative of the breadth of an AI systems capabilities. Initially, MLLMs could manage only singleton non-linguistic modality, e.g., images, videos, or audio signals. To date, these models have evolved to simultaneously support multiple non-linguistic modalitiessuch as combining images with videos, videos with audio, and even any modality in the current most advanced cases. Observation-3: Supporting Various Tasks and Paradigms. To qualify as true multimodal generalist, it must be capable of handling broad range of tasks with different definitions and requirements. The greater the variety of tasks supported, the stronger the generalists overall versatility. For example, early visual MLLMs could only manage coarse-grained image understanding, but recent advancements have enabled them to achieve fine-grained, pixel-level multimodal comprehension, such as pixel-level image/video grounding and editing. This advancement necessitates that the models decoding components should be versatile enough to generate outputs in various task formats, not merely restricted to text. These functional heads must handle different task types such as object localization, pixel-level modifications, and multimodal content creation. Observation-4: Multimodal Agent vs. Multimodal Foundation Model. Initially, researchers approach multimodal tasks by using LLMs as task schedulers, where an LLM orchestrates the execution of tasks by invoking external tools and modules (often specialists) to handle specific multimodal tasks. This setup is referred to as multimodal agent. Subsequently, attention shifted towards building joint MLLMs, where the LLM is tightly integrated with other modules, such as multimodal understanding components (front-end) and multimodal generation components (back-end), through shared embedding space. This setup allows for joint training, where the entire system, including all parameters, can be updated end-to-end. While its theoretically possible to create super agent by combining all singleton SoTA specialists to handle various modalities and tasks, such straightforward aggregation does not lead to true AGI. The complexity of AGI requires deeper integration and generalization across tasks and modalities."
        },
        {
            "title": "3.1.2 SYNERGY AS CORE TO MULTIMODAL GENERALISTS",
            "content": "We argue that determining whether multimodal generalist is stronger cannot be simplistically equated with achieving higher scores on benchmark or/and supporting as many multimodal tasks as possible compared to other modelsa common 6 On Path to Multimodal Generalist: General-Level and General-Bench Figure 3: specific illustration on synergy effect. Figure 4: We categorize tasks of various modalities into Comprehension group, Generation group and NLP group. Each colored stylish symbol represents specific task of certain modality. practice in current MLLM benchmarking and evaluation. simple counterexample can illustrate this point: it could be comparatively easier to construct super agent by integrating all SoTA specialists for various multimodal tasks into single system. Such an agent could achieve top-level performance across all tasks (on par with the strongest individual specialist models) while supporting wide range of multimodal functionalities. However, such agents can be far from the multimodal generalist we expect as pathway to AGI. Such type of agent lacks inherent multimodal intelligence and capabilities, as it relies on an ensemble of specialized systems rather than embodying true, native multimodal generalization. Instead, the ideal multimodal generalist (and ultimately AGI) we envision should be multimodal counterpart of an all-capable OpenAI ChatGPT series. Such model would not only surpass SoTA specialists in task-wise performance across various tasks and modalities but also exhibit exceptional cross-task, cross-comprehension-generation, and cross-modality generalization capabilities. In other words, the knowledge learned from certain tasks, skills, and modalities should be transferable to other tasks, skills, and modalitiesextrapolating the understanding to effectively engage with other tasks and modalities, and vise versa, creating synergistic effect where the combined result exceeds the sum of individual contributions, achieving 1+1>2 effect. ChatGPT on the language side can be good example: it outperforms SoTA specialists in unseen tasks without having undergone specific training for those tasks. This generalizability is what we claim as the synergy effect."
        },
        {
            "title": "3.2 Defining Levels Centered on Synergy\nBased on the above principles, we introduce a 5-level taxonomy of multimodal generalists, General-Level. General-\nLevel framework evaluates generalists based on the levels and strengths of the synergy they preserve. Specifically,\nwe define three levels and scopes of synergy, ranked from low to high: ‘task-task’, ‘comprehension-generation’, and\n‘modality-modality’, as illustrated in Figure 3. Achieving these levels of synergy becomes progressively more challenging,\ncorresponding to higher degrees of general intelligence. Assume we have a benchmark of various modalities and tasks,\nwhere we can categorize tasks under these modalities into the Comprehension group and the Generation group, as well as the\nlanguage (i.e., NLP) group, as illustrated in Figure 4. Now, we can define the scoring specification of General-Level as\nin Table 1.",
            "content": "7 On Path to Multimodal Generalist: General-Level and General-Bench Table 1: General-Level framework toward classifying multimodal generalists into FIVE levels based on the synergy abilities models preserve. We denote the number of tasks within the Comprehension group by ; the number within the Generation group by ; and the number of NLP tasks by ."
        },
        {
            "title": "Level",
            "content": "Level-1: Specialists"
        },
        {
            "title": "Scoring",
            "content": "Various current models, each fine-tuned on specific task or dataset of specific modalities, are task-specific players (i.e., SoTA specialists). This includes various learning tasks, such as linguistic/visual recognition, classification, generation, segmentation, grounding, inpainting, and more. For each task in the benchmark (i-th task), the current SoTA specialists score is recorded as: σsota Upgrading Condition: Supporting as many tasks and functionalities as possible Level-2: Generalists of Unified Comprehension and/or Generation are task-unified players, Models e.g., MLLMs, capable of supporting different modalities and tasks. Such MLLMs can integrate various models through existing encoding and decoding technologies to achieve aggregation and unification of various modalities and tasks (such as comprehension and generation tasks). The average score between Comprehension and Generation tasks (i.e., across all tasks) represents the score at this level. model that can score non-zero on the data is considered capable of supporting that task. The more supported tasks and the higher the scores, the higher its overall score: S2 = 1 2 1 (cid:88) i=1 σC + 1 (cid:88) j=1 σG Upgrading Condition: Generalists achieving as stronger synergy and cross as many tasks as possible Level-3: Generalists with synergy in Comprehension and/or Generation Models are task-unified players, and synergy is in Comprehension and/or Generation. MLLMs enhance several tasks performance beyond corresponding SoTA scores through joint learning across multiple tasks due to the synergy effect. Assign mask weight of 0 or 1 to each task; mask=1 only if the corresponding score (σC or σG ) exceeds the SoTA specialists score, otherwise mask=0. Then, calculate the average score between SC and SG. The more tasks to surpass the SoTA specialist, the higher the S3: 1 2 S3 = SC = SG = (SG + SC ) , where 1 1 (cid:88) i=1 (cid:88) j=1 (cid:26)σC (cid:26)σG 0 σC if σC otherwise sota σG if σG otherwise sota"
        },
        {
            "title": "Example",
            "content": "CLIP (Li et al., FLUX 2022), (Labs, 2023), FastSpeech2 (Ren et al., 2021), al., Unified-io-2 (Lu et al., 2024a), AnyGPT (Zhan et 2024), NExT-GPT (Wu et al., 2024a), SEED-LLaMA (Ge et al., 2023), GPT-4V (OpenAI, 2022b), GPT-4o (OpenAI, 2022b), Gemini1.5 (Team et al., 2024a), Claude3.5 (Team, 2024), DeepSeek-VL (Lu et al., 2024b), LLaVA-OneVision (Li et al., 2024d), Qwen2VL (Wang et al., 2024a), InternVL2.5 (Chen et al., 2024c), Phi-3.5-Vision (Abdin 2024), al., et Upgrading Condition: Generalists in unified comprehension and generation capability with synergy in between Level-4: Models are task-unified players, and synergy is across Comprehension and Generation."
        },
        {
            "title": "Generalists with synergy\nacross Comprehension\nand Generation",
            "content": "Calculate the harmonic mean between Comprehension and Generation scores. The stronger synergy model has between Comprehension and Generation tasks, the higher the score: S4 = 2SC SG SC + SG Mini-Gemini (Li al., 2024c), et Vitron-V1 (Fei al., 2024a), et Emu2-37B (Sun et al., 2024), Upgrading Condition: Generalists achieving cross-modal synergy with abductive reasoning ability Level-5: Generalists with total synergy across Comprehension, Generation and Language Models are task-unified players, preserving the synergy effect across Comprehension, Generation, and Language. In other words, the model not only achieves cross-modality synergy between Comprehension and Generation groups but also further realizes synergy with language. The Language intelligence can enhance multimodal intelligence and vice versa; understanding multimodal information can also aid in understanding language. Calculate the models average score exceeding SoTA NLP specialists on NLP benchmark data; normalize it to [0,1] weight, and multiply it by the score from level-4 as the level-5 score: S5 = S4 wL , where None found yet (Lets wait for multimodal ChatGPT moment!) , where wL = SL = SL Stotal (cid:88) 1 k=1 (cid:26)σk if σk σsota otherwise"
        },
        {
            "title": "3.2.1 SCORING SPECIFICATION",
            "content": "When calculating scores using the corresponding formula, we normalize all task metrics to 100-point scale. While most task evaluation scores typically range from 0-100, such as F1 and Accuracy, certain metrics, e.g., FID, MAE, and PSNR, yet yield scores outside this usual range. Thus, we design some mapping functions to standardize performance scores. Our framework also incorporates the principle of diminishing scores: an MLLM (i.e., multimodal generalist) can achieve scores at multiple levels, but it is classified at its highest level, where it achieves non-zero score. 8 On Path to Multimodal Generalist: General-Level and General-Bench We assume that current MLLMs have already demonstrated synergy mode from language to non-language modalities. Then the remaining mission is to confirm the existence of synergy in the reverse direction, from non-language to language modalities. Therefore, for level 5measuring total synergywe do not measure the generality across all modalities and tasks. Instead, we assess whether model can improve NLP task performance to exceed that of NLP SoTA specialists. Also, except for Level-1 and Level-5, when calculating S2, S3, and S4, we consider reasonable approach when handling different modalities. First, we calculate the specific score component Si of generalist in the i-th modality (assuming there are modalities in total) for the score Sk. This modality-specific component can accurately reflect the models Level-k capability in the i-th modality. Next, by decomposing each score into its components across different modalities, we sum the components of each modality with equal weights to obtain the overall score for each level. Sk = (cid:88) i"
        },
        {
            "title": "Si\nk",
            "content": "The advantage of this method is that it reduces the bias introduced by the number of tasks in different modalities. For example, in our benchmark, image-related tasks (especially comprehension-type tasks) are overwhelmingly more numerous compared to other modalities, such as audio tasks. Therefore, two generalists with similar capability levels, say one for image tasks and the other for audio tasks, would have higher Sk score for the image-generalist over the audio-generalist, due to the larger number of image tasks. This discrepancy is unrealistic and contrary to our core idea for evaluating multimodal generalists. To eliminate the bias caused by the number of tasks within each modality, we propose the above calculation method, which treats the capabilities of different modalities equally. Meanwhile, this method also prioritizes generalists that can support more modalities. For instance, model that supports more modalities will certainly have higher overall score compared to generalist that supports only one modality. This scoring method ensures that as an MLLM climbs to higher levels, its scores progressively decrease, which should indicate the increasing difficulty of advancing levels. Climbing from level to level + 1 requires specific capabilities, i.e., demonstrating sufficient synergy capability associated with that level, which we highlight as critical factors in Table 1. Within the same level, to achieve higher score, model must: 1) support as many tasks and modalities as possible, and simultaneously 2) achieve the highest possible performance on individual tasks."
        },
        {
            "title": "3.2.2 SCORING RELAXATION",
            "content": "A central aspect of our General-Level framework lies in how synergy effects are computed. According to the standard understanding of the synergy concept, e.g., the performance of generalist model on joint modeling of tasks and (e.g., Pθ(yA, B)) should exceed its performance when modeling task alone (e.g., Pθ(yA)) or task alone (e.g., Pθ(yB)). However, adopting this approach poses significant challenge that hinders the measurement of synergy: there is no feasible way to establish two independent distributions, Pθ(yA) and Pθ(yB), and joint distribution Pθ(yA, B). This limitation arises because given generalist model has already undergone extensive pre-training and fine-tuning, where tasks and have likely been jointly modeled. It is impractical to retrain such generalist to isolate the learning and modeling of tasks or independently in order to derive these distributions. Otherwise, such an approach would result in excessive redundant computation and inference on the benchmark data. To simplify and relax the evaluation of synergy, we introduce key assumption in the scoring algorithm: Theoretically, we posit that the stronger models synergy capability, the more likely it is to surpass the task performance of SoTA specialists when synergy is effectively employed. Then, we can simplify the synergy measurement as: if generalist outperforms SoTA specialist in specific task, we consider it as evidence of synergy effect, i.e., leveraging the knowledge learned from other tasks or modalities to enhance its performance in the targeted task. By making this assumption, we avoid the need for direct pairwise measurements between task-task, comprehensiongeneration, or modality-modality, which would otherwise require complex and computationally intensive algorithms."
        },
        {
            "title": "3.2.3 PROPERTIES OF GENERAL-LEVEL",
            "content": "The General-Level framework possesses several important attributes that play critical role in supporting the hierarchical classification and ranking of MLLMs. These properties are also well-grounded in mathematical theory. Property-1: Independence from Peer Generalists In our scoring framework, the scores of any generalist depend solely on the dataset and the reference scores of SoTA specialists, without relying on the scores of other tested generalists. These 9 On Path to Multimodal Generalist: General-Level and General-Bench two components are entirely independent. The dataset defines the specific tasks, while the specialists provide baseline reference scores used for the calculation of the experimental generalists scores. This property ensures that the evaluation of generalists is free from interdependence, maintaining objectivity and fairness among all systems participating in the ranking. Property-2: Monotonicity Across Levels Generally, if generalist is rated at the highest level-k, it is expected to achieve scores at all levels from 2 to k. We further expect that as the level increases, the corresponding scores for the generalist will decrease, i.e., Sk1 > Sk. This is reasonable and realistic requirement, as higher levels impose stricter demands on the generalists capabilities, naturally leading to lower scores for the same model. Below, we provide proof that the scoring algorithm of General-Level framework mathematically guarantees the strictly monotonic score decline across levels. The proof for S3 S2 S3 = = 1 2 1 1 2 (SG + SC ) (cid:32) (cid:32)"
        },
        {
            "title": "1\nM",
            "content": "M (cid:88) i=1 (cid:88) i=1 (cid:26)σC 0 σC + =S2 σC if σC otherwise sota +"
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) j=1 (cid:26)σG 0 (cid:33) σG i"
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) j=1 (cid:33) σG if σG otherwise sota The proof for S4 S3 Suppose: SG = SC ="
        },
        {
            "title": "1\nN",
            "content": "M (cid:88) i=1 (cid:88) j=1 (cid:26)σi 0 if σi σsota otherwise (cid:26)σj 0 if σj σsota otherwise According to Cauchy-Schwarz Inequality, lets represent (cid:18) SC + SG 2 (cid:19)2 (cid:19) (cid:18) 2SC SG SC + SG Expanding this, Multiplying both sides by 4(SC + SG),"
        },
        {
            "title": "This factorizes to",
            "content": "Finally, we have (SC + SG)2 4 2SC SG SC + SG (SC + SG)3 8SC SG(SC + SG) S3 + G 2SC SG(SC + SG) (SC SG)2(SC + SG) 0 . S4 = 2SC SG SC + SG 1 2 = S3 . (SC + SG) 10 On Path to Multimodal Generalist: General-Level and General-Bench The proof for S5 S"
        },
        {
            "title": "We have",
            "content": "wL = SL = , where SL Stotal (cid:88)"
        },
        {
            "title": "1\nT",
            "content": "k=1 (cid:26)σk 0 if σk σsota otherwise which means,"
        },
        {
            "title": "Then",
            "content": "Thus, wL 1 . S5 S4 = S4 wL S4 = S4 (wL 1) 0 S5 S4 0 Property-3: Encouraging Rich and Balanced Multimodal Task Support. More Task, The Better. good multimodal evaluation system should not only reward models for achieving higher scores on individual tasks and surpassing SoTA specialists but also incentivize trend where multimodal generalists support as many diverse multimodal tasks as possible. This is reasonable expectation, as an ideal multimodal generalist should inherently support broader range of modalities and tasks. The scoring algorithm of our General-Level framework aligns with this objective. For instance, in the case of level-2 scoring: S2 ="
        },
        {
            "title": "1\nM + N",
            "content": "M +N (cid:88) i=1 σi , model that achieves nonzero scores across greater number of modalities and tasks will naturally obtain higher average score, thereby ranking higher within the same level. More Balance, The Better. Moreover, our scoring algorithm also promotes models that achieve more balanced performance across tasks. For example, in the case of level-4 scoring, consider the following scenarios: 1) Model achieves SoTA specialist performance on tasks in the comprehension category but only tasks (where ) in the generation category. 2) Model achieves SoTA specialist performance on tasks in both the comprehension and generation categories. According to the properties of the harmonic mean inequality, SA 4 < SB 4 . The proof for SA 4 < SB 4 when in levelG = 1, while all other scores are 0. Extreme Assumptions: - For Model A, the tasks in the comprehension group have scores of σA scores of σA - For Model B, both comprehension and generation groups have tasks with scores of σB scores are 0. Model-A Scores: For Model A, the comprehension and generation scores are: = 1, and the tasks in the generation group have = 1 and σB = 1, while all other SA ="
        },
        {
            "title": "X\nM",
            "content": ", SA ="
        },
        {
            "title": "Y\nN",
            "content": ". 11 On Path to Multimodal Generalist: General-Level and General-Bench The overall score for Model is: SA 4 = 2 SA SA + SA SA = 2 M + N = 2XY XN + . Model-B Scores: For Model B, both comprehension and generation groups have tasks with scores of 1, so: SB ="
        },
        {
            "title": "X\nM",
            "content": ", SB ="
        },
        {
            "title": "X\nN",
            "content": ". The overall score for Model is: Comparison: We need to compare: Given , it follows that: Thus, SA 4 < SB 4 . SB 4 = 2 SB SB + SB SB = 2 M + N = 2 XN + XM . 2XY XN + and 2 XN + XM . 2XY XN + < 2 XN + XM . Through the above mathematical analysis, we have proven that under the same task distribution, the uneven generation score distribution of Model results in its level-4 score being lower than that of Model B. This ensures that models with more balanced performance across comprehension and generation are ranked higher. Property-4: Dynamic Update on Benchmarking and Specialists Finally, we observe an important point: the more tasks included in the benchmark used to evaluate models, the more accurate and objective the resulting evaluations and conclusions. This requirement for the evaluation benchmark to have dynamic properties aligns well with real-world needs. In practice, new tasks, data, and even new modalities are constantly being introduced, and generalist should be capable of covering these newly added tasks and functionalities. Accordingly, in our evaluation system, we allow the benchmark to evolve dynamically, such as by adding new tasks under various modalities and categories. Once new tasks are added, we update the scores and rankings of all tested generalists to reflect the expanded benchmark. On the other hand, we also allow updates to the SoTA specialist models timely for each task, as scoring at higher levels is anchored to the performance of the SoTA models. This is reasonable act, as specialists are continually being developed and improved. Once baseline specialist advances, generalists must also improve to remain competitive, or risk being surpassed. Thus, in General-Level framework, the scores corresponding to SoTA specialists are subject to periodic updates. Also, we dynamically and regularly update the scoring and ranking of all generalists to ensure the evaluation remains accurate and reflective of the current state of the field."
        },
        {
            "title": "3.3 Receipt to Leveling Upper in General-Level",
            "content": "Here we provide guideline to help better understand how to achieve higher levels in General-Level framework. Level-1Level-2: Supporting as many tasks and functionalities as possible. Transitioning from specialists to generalists requires making the system compatible with various task modeling paradigms, i.e., supporting diverse modality types and input formats, as well as handling wide range of model types and output formats (whether for comprehension and/or generation). Currently, the most popular and widely adopted practice is to use an LLM as the backbone/intelligence medium, integrating various specialists to build generalists. There are two primary implementation strategies. First, agent-based generalists (Wu et al., 2023b; Shen et al., 2023). In this approach, the LLM acts as task scheduler and dispatcher, facilitating message passing through hard integration (explicit text). This is essentially pipeline architecture. However, since gradient propagation across the entire system is not feasible, this method is prone to error propagation. The performance upper bound of generalists built with this approach is equivalent to the SoTA specialists for all supported tasks, 12 On Path to Multimodal Generalist: General-Level and General-Bench primarily due to the lack of features, information sharing, and limited task collaboration. Second, end-to-end generalists (Liu et al., 2023c; Li et al., 2023a; Zhu et al., 2023a). In this type, the entire system is constructed as continuous joint model, allowing for full-stack updates via gradient propagation. The most common architecture in this category uses an LLM as the backbone, achieving soft integration of various encoders and decoders through input tokenization and feature embedding, combined with overall fine-tuning. Level-2 Level-3: Generalists achieving as stronger synergy and cross as many tasks as possible. To advance from vanilla generalist to Level-3, the system must demonstrate cross-task synergy capabilities, enabling at least two tasks (regardless of whether both involve comprehension, generation, or one involves comprehension while the other involves generation) to share features and achieve mutual performance improvements. The most direct method to realize cross-task synergy is through multi-task joint training. Specifically, during joint learning, the system must ensure it can maintain task-shared/persistent common features while preserving each tasks specific features without degradation, e.g., Vitron (Fei et al., 2024a). Moreover, the model must support synergy across as many tasks as possible and ensure that the synergy effect is significant enough to achieve higher evaluations at Level-3. Level-3Level-4: Generalists in unified comprehension and generation capability with synergy in between. To advance to Level-4, generalists must first achieve unified comprehension and generation capabilities, regardless of whether they support single modality (non-NLP) or multiple modalities. At the same time, the system must meet the requirement that its capabilities in comprehension and generation synergize and enhance one another. Generally speaking, compared to acquiring comprehension capabilities, obtaining generation capabilities at the technical level is relatively more challenging. For instance, the visual comprehension abilities of most visual LLMs tend to be significantly stronger than their visual generation capabilities. If generalist can score at Level-4, it indicates that the system not only possesses strong comprehension capabilities but also maintains these capabilities while further learning and training its generation abilities. To achieve this, Morph-Token (Pan et al., 2024) introduces disentangling visual reconstruction loss for generation learning to avoid interference with the comprehension learning loss. Level-4Level-5: Generalists achieving cross-modal synergy with abductive reasoning ability. Achieving Level-5 represents the ultimate goal for generalists, where features, knowledge, and even intelligence learned from tasks in certain modalities can (to varying degrees) transfer to tasks in other supported modalities. Currently, most multimodal generalists are limited by architectural developments, primarily enabling language intelligence to support intelligence in other modalities (as illustrated in Figure 2). However, to truly achieve Level-5, synergy must exist across all modalities. For instance, in the current MLLM community, this would require MLLMs to enhance performance on NLP tasks as well, while most of the MLLMs perform unsatisfactorily in NLP tasks. From technical perspective, generalists must be capable of abductive reasoning, i.e., the ability to infer and generalize across everything. Also, they need to ensure modality-agnostic context consistency during reasoning."
        },
        {
            "title": "4 General-Bench: A Holistic Benchmark for Multimodal Generalists",
            "content": "We introduce General-Bench, new benchmark to meet the outlined criteria and serve as the standard dataset for our evaluation framework."
        },
        {
            "title": "4.1.1 DESIGN CRITERION",
            "content": "As previously noted, the current benchmarks that rank MLLMs based solely on their performance have significant limitations, which hinder the encouragement of MLLMs to evolve toward becoming more capable multimodal generalists. Primarily, nearly all existing benchmarks focus on evaluating MLLMs capabilities in visual modalities, particularly images, while significantly neglecting tasks in other modalities such as video, audio, 3D, etc. Moreover, they often assume that MLLMs already possess satisfied NLP capabilities, thus omitting evaluations in language. Secondly, these benchmarks tend to simply convert free-form predictions into fixed QA format of pre-defined choicesessentially compromise that reflects the current limitations of MLLM capabilitiesallowing many tasks that MLLMs cannot produce in specific formats to still be executed. We believe that genuine multimodal generalist should support tasks in their original formats. Furthermore, most benchmarks only assess MLLMs understanding of visual information; however, multimodal generalist should inherently possess wide range of capabilities beyond mere comprehension, such as generation, editing, etc. Therefore, we expect to construct benchmark that possesses these 13 On Path to Multimodal Generalist: General-Level and General-Bench Figure 5: An illustration of the data construction pipeline of General-Bench. characteristics: Covering as broad range of tasks, skills and modalities as possible. Encompassing both comprehension and generation of tasks. Including rich diversity of tasks across various scenarios and domains. Preserving the original task-prediction formats. Timely maintaining and expanding the dataset dynamically."
        },
        {
            "title": "4.1.2 CONSTRUCTION PROCESS",
            "content": "The construction of our General-Bench dataset follows structured 5-step process to ensure both comprehensiveness and quality. Figure 5 presents the data construction pipeline. Step-1: Defining Scope and Range. We begin by conducting series of panel discussions to establish the scope of the dataset. This involves determining the modalities to include, identifying the core general skills (meta-tasks), and specifying the prediction paradigms to address. These discussions help outline comprehensive framework for the dataset, ensuring that it accommodates diverse tasks and capabilities required for evaluating multimodal generalists. Step-2: Curating Task List. Based on the defined scope, we curate comprehensive task list by systematically searching various sources, including Google, GitHub, Kaggle, ArXiv, and PaperWithCode, etc. For each task, we specify its inputoutput targets, select appropriate evaluation metrics, and also identify SoTA specialists as reference points. This step ensures that each task is well-defined and aligned with existing SoTA practices. Step-3: Collecting Data. Next, we start collecting the data instances. The data collection process is divided into two cases for handling two different scenarios: Case A: If the data could be sourced from existing benchmark datasets (only from their test sets), modifications are made to enhance diversity. We will show all the data sources of our benchmark in the following subsections. For textual data, rephrasing is done using ChatGPT. For non-textual modalities such as images, videos, and audio, semantically equivalent replacements are identified through retrieval or direct recording from relevant databases or websites. Case B: For tasks without available datasets or insufficient enough numbers of samples, we manually create instances. This involves crafting input-output pairs according to the task definition, running existing models to generate predictions, and performing manual verification and correction of the results. We ensure that each task includes (at least) 500 data samples. Also, we ensure that all tasks faithfully retain their original input-output prediction structure or format, i.e., not reformatted into QA-based multiple-choice questions. Step-4: Data Filtering and Cleaning. After collecting datasets for all modalities and tasks, we proceed with data filtering and cleaning. First, we filter out low-quality instances, including those that do not align well with the tasks evaluation purpose, lack target modality information, or fail to meet the defined prediction paradigms. For tasks where the number of instances is insufficient, we restart the data annotation process to supplement the required quantity. Afterward, we organize all data into unified storage format according to the designed specifications. For example, textual data is standardized into JSON files with consistent naming conventions applied to all files. Step-5: Data Inspection and Validation. Finally, we conduct rigorous inspection and validation process to guarantee data quality and consistency. Annotators work in groups of three, independently reviewing the same instance. An instance is accepted only if all three annotators reach consensus. Finally, team leaders or supervisors conduct an additional round of verification to ensure the dataset meets the highest standards of consistency and accuracy. 14 On Path to Multimodal Generalist: General-Level and General-Bench Figure 6: Overview of General-Bench, which covers 145 skills for more than 700 tasks with over 325,800 samples under comprehension and generation categories in various modalities. Appendix A.3 gives holistic hierarchical taxonomies. Table 2: Summary of numbers of skills, tasks and data instances across modalities. #Skill #Task #Instance"
        },
        {
            "title": "Audio",
            "content": "Comp 40 271 55 316 Gen 15 Comp 45 126 Gen 6 46 26 Comp 9 Gen 11 Comp 13 24 20 20 30 3D 22 52 Gen"
        },
        {
            "title": "TOTAL",
            "content": "22 118 145 702 124,880 26, 44,442 16,430 11,247 9,516 23,705 10, 151,490 60,872 20,763 34,319 58,432 325,"
        },
        {
            "title": "4.2 Evaluation and Splitting",
            "content": "As each task follows the original format, our evaluation metrics vary in rich task types. For instance, we evaluate X-to-text generation tasks using BLEU/ROUGE/CIDEr scores, image segmentation tasks with mIoU for generating masks, and image generation tasks using FID, etc. Also, we design some mapping functions to standardize performance scores. In Appendix A.1 we present the evaluation metrics as well as the mapping tricks in detail. For most of the tasks, we maintain around 500 testing instances each. Considering that not all practitioners in the community may be interested in participating in the leaderboardfor example, some may simply wish to use our dataset for their research or publicationswe propose dividing the test set for each task into closed set and an open set. The closed set is reserved for leaderboard evaluations: only the input data is released, and users are required to submit their models predicted outputs for centralized assessment. In contrast, the open set provides full access to both inputs and corresponding outputs, enabling practitioners to explore and utilize the data more freely. Each tasks test set is split into closed and open subsets with ratio of 2:3. 15 On Path to Multimodal Generalist: General-Level and General-Bench Figure 7: General-Bench covers over 29 domains, evaluating more than 12 modality-persistent capabilities of generalists, as well as 145 modality-specific skills. In Appendix A.4 we showcase all tasks and data specification in detail."
        },
        {
            "title": "4.3 Data Insights",
            "content": "First, Table 2 summarizes the statistics of task and skill numbers in General-Bench. The data compiled for General-Bench is visualized in Figure 6 visualizes the General-Bench highlights of task/modality support. Overall, the current version of the dataset includes those most common modalities (inner ring), and except for NLP tasks, all 16 On Path to Multimodal Generalist: General-Level and General-Bench Table 3: Comparison of General-Bench with existing representative MLLM benchmarks. Comp.: Comprehension; Gen.: Generation. Appendix A.5 presents complete view for more comparisons of exiting benchmarks. Benchmark SEED-Bench MMBench MMMU LVLM-eHub MMIU MMT-Bench MEGA-Bench Modality Txt,Img,Vid Txt,Img Txt,Img Txt,Img Txt,Img,Vid, Point-Cloud,Depth Txt,Img,Vid, Point-Cloud Txt,Img,Vid Task Scheme Comp. Comp. Comp. Comp. # Domain # Skill # Task # Sample Answer Form # Metric Annotation 1 12 12 19K MC-QA Acc. Manual 1 2 20 3K 6 6 30 11.5K 1 6 47 2.1K MC-QA MC-QA MC-QA Acc. Repurposed Manual Repurposed Acc. Acc. Comp. 1 7 52 11.7K Comp. Comp. 4 32 162 31K 5 10 505 8K MC-QA Acc. Repurposed MC-QA Acc. Repurposed Free-Form Origin (45) Manual # Tested Models 21 24 8 22 30 General-Bench Txt,Img,Vid,Aud, Time,Depth,3D-RGB, Point-Cloud,Infrared, Spectrogram,Radar, Code,Doc,Graph, Comp.+Gen. 29 145 702 325.8K Free-Form Origin (58) Manual 172+102 modalities distinguish between comprehension and generation tasks (middle ring). General-Bench particularly places strong emphasis on the diversity of its evaluation data, covering wide range of fields and scenarios to assess different aspects of model capabilities, as depicted in Figure 7. First, the dataset spans variety of domains and disciplines, incorporating 28 major areas within both the physical sciences (e.g., Physics, Math, Geometry, Biology) and the social sciences (e.g., Humanities, Linguistics, History, Social). The evaluation of generalists skills and capabilities is categorized into universal modality-invariant abilities and modality-specific skills. The modality-invariant abilities comprehensively include 12 categories, such as content recognition, commonsense knowledge, reasoning ability, causality discrimination, affective analysis, creativity, and innovation, etc. For modality-specific skills, we explicitly detail the main capabilities under both comprehension and generation for each modality, which correspond to the meta-tasks (skills) of our dataset. In Table 3, we further present comparison with several existing popular benchmarks. It also covers the broadest range of disciplines and supports the widest array of modalities. General-Bench comprises 130 multimodal skills, containing 702 tasks with over 325,800 annotations across various formats and domains. The volume of tasks and data in General-Bench significantly exceeds that of current benchmarks. Moreover, our dataset facilitates original free-form task prediction, allowing for more diverse array of task types."
        },
        {
            "title": "4.4 Leaderboard Re-Scoping",
            "content": "Given the large scale of our dataset, it would be highly costly for practitioners to run the entire dataset under our proposed General-Level evaluation protocol. Moreover, its realized that most existing multimodal generalists (e.g., MLLMs) have not yet reached the level of capability required to cover wide range of modalities and tasks, as envisioned in our framework. As result, many current models may find it difficult to fully demonstrate their potential on our leaderboard. To improve usability and encourage broader participation, we further propose graded structure for the leaderboard by dividing its scope into four levels of increasing difficulty: Scope-A: Full-spectrum leaderboard covering all modalities and tasks, designed for highly capable, general-purpose multimodal models. This scope has one leaderboard encompassing all levels in General-Level, making it the most challenging track. We further derive full version and quick version leaderboard for easier participation. Scope-B: Modality-specific leaderboards, each focusing on single modality or partially joint modality, and designed for modality-wise generalists. This scope maintains 4 separate leaderboards, one per modality (except for language). Scope-C: Leaderboards focused on either comprehension or generation within single modality. This scope includes 8 leaderboards: 2 4 for comprehension/generation across multimodal tasks, with lower entry barrier for participation. Scope-D: Finer-grained, skill-level (task-cluster-specific) leaderboards within each modality, tailored for partial generalists. This scope includes large number of specific leaderboards, offering the lowest difficulty for participation. Figure 8 illustrates this design. Each leaderboard scope reflects different level of difficulty, allowing practitioners to flexibly choose which leaderboard to participate in based on the capabilities of their models and the amount of resources they are willing to invest. 17 On Path to Multimodal Generalist: General-Level and General-Bench Figure 8: We reorganize General-Bench into 4 scopes, categorized by the level of participation difficulty for practitioners."
        },
        {
            "title": "5.1 Multimodal Specialist and Generalist Systems\nSoTA Specialist. For each specific task under a specific modality, we select a SoTA specialist to generate benchmark\nresults. The selection of specialists is determined based on two criteria: 1) their performance on each task using public\nbenchmarks and leaderboards, i.e., they must demonstrate top performance; and 2) whether they are widely recognized and\nutilized by the community. Meanwhile, we exclude models that lack reliable open-source code or parameters (as we are\nunable to run our own data through them), even if such models claim to be SoTA in their own papers. It is important to note\nthat the specialists we use must have undergone large-scale supervised pretraining on the corresponding tasks, enabling them\nto achieve SoTA performances. In our implementation, we directly load their released parameters and perform inference\non the General-Bench test sets. Table 21 to Table 37 in Appendix §A.6 lists all the specialists used along with their\ncorresponding tasks. In total, we have 172 specialists.",
            "content": "Multimodal Generalists. We consider diverse set of existing popular MLLMs that are capable of handling specific or various modalities and tasks. This includes both open-source systems and closed-source ones (such as the OpenAI GPT series). For open-source models, we implement them by loading their released parameters and directly performing inference on the General-Bench test sets. For closed-source models, we utilize their APIs to access the services. We note that, despite the release of vast number of MLLMs in the community, due to resource constraints, we only consider subset of MLLMs that demonstrate strong and stable capabilities and are widely recognized and utilized. However, our evaluation system remains open, and we encourage more MLLMs interested in our benchmarking system to participate by running their own evaluations and submitting their scores. Table 4 summarizes all the multimodal generalists employed, including their corresponding modality support, characterized skills, parameter sizes, and backbone LLM architectures. Table 4: complete list of (multimodal) generalists evaluated on General-Bench. # Model"
        },
        {
            "title": "Paradigm",
            "content": "Language-oriented (Closed/Open-sourced) Models"
        },
        {
            "title": "Llama",
            "content": "Instruct (Touvron et al., 2023)"
        },
        {
            "title": "2 Gemma-2-9b-it\net al., 2024b)",
            "content": "(Team"
        },
        {
            "title": "3 GPT-J (Wang and Komat-",
            "content": "GPT-J 4 suzaki, 2021) ChatGLM-6B (GLM et al., 2024)"
        },
        {
            "title": "ChatGLM",
            "content": "Qwen2."
        },
        {
            "title": "Language",
            "content": "8B 9B 6B 6B 7B / / / / / On Path to Multimodal Generalist: General-Level and General-Bench # Model 6 7 InternLM2-Chat-7B (Cai et al., 2024) Baichuan2-7B-Chat (Yang et al., 2023)"
        },
        {
            "title": "Backbone",
            "content": "InternLM2 Baichuan"
        },
        {
            "title": "Vicuna",
            "content": "9 et al., 2023) Falcon3-7B-Instruct (Almazrouei et al., 2023) Falcon"
        },
        {
            "title": "Ministral",
            "content": "2410 (Jiang et al., 2024a)"
        },
        {
            "title": "Llama",
            "content": "2024)"
        },
        {
            "title": "12 GPT-3.5-turbo (OpenAI,",
            "content": "GPT3.5 2022a) Multimodal Close-sourced Models"
        },
        {
            "title": "1 GPT4-V (OpenAI, 2022b) GPT4\nGPT4\n2 GPT4-o-mini\n2022b)",
            "content": "(OpenAI, 5 (OpenAI,"
        },
        {
            "title": "3 GPT4-o (OpenAI, 2022b) GPT4\nGPT4\n4 GPT4-o-4096\n2022b)\nChatGPT-o-latest (OpenAI,\n2022b)\nClaude-3.5-Sonnet (Team,\n2024)\nClaude-3.5-Opus\n2024)",
            "content": "(Team, GPT4 6 7 Claude-3.5-Sonnet Claude-3.5-Opus"
        },
        {
            "title": "8 Gemini-1.5-Pro\net al., 2024a)",
            "content": "(Team"
        },
        {
            "title": "Gemini",
            "content": "et al., 2024a) Multimodal Open-sourced Models"
        },
        {
            "title": "LLaVa",
            "content": "2 3 4 5 2024) Emu2-37B (Sun et al., 2024) InternVL2.5-2B et al., 2024c) InternVL2.5-4B et al., 2024c) InternVL2.5-8B et al., 2024c) (Chen (Chen (Chen LLaMA-33B internlm2 5-1 8b-chat Qwen2.5-3B-Instruct internlm2 5-7b-chat"
        },
        {
            "title": "6 Mini-InternVL-Chat-",
            "content": "InternLM2-Chat-1.8B 2B-V1-5 (Gao et al., 2024)"
        },
        {
            "title": "Paradigm",
            "content": "7B 7B 7B 7B 8B 6B / / / / / / / / / / 6B 37B 2B 4B 8B 2B"
        },
        {
            "title": "Language",
            "content": "/ / / / / / / Language, Image Language, Image Language, Image Language, Image"
        },
        {
            "title": "Comprehension\nComprehension",
            "content": "Language, Image"
        },
        {
            "title": "Comprehension",
            "content": "Language, Image"
        },
        {
            "title": "Comprehension",
            "content": "Language, Image"
        },
        {
            "title": "Comprehension",
            "content": "Language, Image"
        },
        {
            "title": "Comprehension",
            "content": "Language, Image"
        },
        {
            "title": "Comprehension",
            "content": "Language, Image"
        },
        {
            "title": "Comprehension",
            "content": "Language, Image Comprehension+Generation Language, Image"
        },
        {
            "title": "Comprehension",
            "content": "Language, Image"
        },
        {
            "title": "Comprehension",
            "content": "Language, Image"
        },
        {
            "title": "Comprehension",
            "content": "Language, Image"
        },
        {
            "title": "7 Mini-InternVL-Chat-",
            "content": "Phi-3-mini-128k-instruct 4B Language, Image"
        },
        {
            "title": "Comprehension",
            "content": "8 4B-V1-5 (Gao et al., 2024) InternLM-XComposer2VL-1.8B (Dong et al., 2024) InternLM2-Chat-1.8B 1.8B Language, Image"
        },
        {
            "title": "9 MoE-LLAVA-Phi2-2.7B-\n4e-384 (Lin et al., 2024a)",
            "content": "Phi"
        },
        {
            "title": "10 Monkey-10B-chat (Li et al.,",
            "content": "Qwev-7B 2024e) 11 mPLUG-Owl2-LLaMA27b (Ye et al., 2024) Phi-3.5-Vision-Instruct (Abdin et al., 2024) 12 LLaMA2-7b Phi-3 Mini"
        },
        {
            "title": "13 Cambrian-1-8B\net al., 2024a)",
            "content": "(Tong LLaMA3-8B-Instruct"
        },
        {
            "title": "14 DetGPT (Pi et al., 2023)",
            "content": "Vicuna-7B 2.7B 10B 7B 4.2B 8B 7B 19 Language, Image"
        },
        {
            "title": "Comprehension",
            "content": "Language, Image"
        },
        {
            "title": "Comprehension",
            "content": "Language, Image"
        },
        {
            "title": "Comprehension",
            "content": "Language, Image"
        },
        {
            "title": "Comprehension",
            "content": "Language, Image"
        },
        {
            "title": "Comprehension",
            "content": "Language, Image"
        },
        {
            "title": "Comprehension",
            "content": "On Path to Multimodal Generalist: General-Level and General-Bench # Model"
        },
        {
            "title": "15 Otter (Li et al., 2023b)\n16 NExT-Chat (Zhang et al.,",
            "content": "LLaMA-7B LLaVA 2023c)"
        },
        {
            "title": "17 GPT4RoI-7B (Zhang et al.,",
            "content": "LLaMA-7B 2023d)"
        },
        {
            "title": "18 GLaMM (Rasheed et al.,",
            "content": "Vicuna-7B"
        },
        {
            "title": "Size",
            "content": "7B 7B 7B 7B"
        },
        {
            "title": "Modality Support",
            "content": "Language, image Language, Image"
        },
        {
            "title": "Comprehension\nComprehension",
            "content": "Language, Image"
        },
        {
            "title": "Comprehension",
            "content": "Language, Image"
        },
        {
            "title": "Comprehension",
            "content": "19 2024) Pixtral-12B (Agrawal et al., 2024) Mistral-Nemo-12B 12B Language, Image"
        },
        {
            "title": "Comprehension",
            "content": "Show-o DeepSeek-LLM-7b-base 1.3B 7B Language, Image Language, Image Comprehension+Generation Comprehension"
        },
        {
            "title": "20 BLIP-2 (Li et al., 2023a)\n21 BLIP-3 (XGen-MM) (Xue",
            "content": "Flan T5-xl Phi3-mini et al., 2024) 22 miniMonkey (Li et al., Qwev-7B 2024e)"
        },
        {
            "title": "23 MiniGPT4-LLaMA2-7B\n(Zhu et al., 2023a)\n24\nShow-o (Xie et al., 2024)\n25 DeepSeek-VL-7B-Base\n(Lu et al., 2024b)\n26 DeepSeek-VL-7B-Chat\n(Lu et al., 2024b)\n27 LISA (Lai et al., 2024)\n28 CogVLM-Chat",
            "content": "(Wang 29 30 et al., 2023b) ShareGPT4V-7B (Chen et al., 2025) ShareGPT4V-13B (Chen et al., 2025) LLaMA2-7B-instruct"
        },
        {
            "title": "DeepSeek",
            "content": "LLaMA-7B Vicuna-v1.5-7B Vicuna-v1.5-7B Vicuna-v1.5-13B"
        },
        {
            "title": "31 GLM-VL-Chat (Du et al.,",
            "content": "GLM-4V 2021) (Zhang"
        },
        {
            "title": "32 OMG-LLaVA-\nInternLM20B\net al., 2024a)\nIdefics3-8B-Llama3\n(Laurenc¸on et al., 2024)\n34 MiniCPM3-4B (Hu et al.,",
            "content": "33 internlm2-7b Llama-3.1-8B MiniCPM3-4B 3B 4B 7B 7B Language, Image Language, Image"
        },
        {
            "title": "Comprehension\nComprehension",
            "content": "Language, Image"
        },
        {
            "title": "Comprehension",
            "content": "Language, Image"
        },
        {
            "title": "Comprehension",
            "content": "7B 7B 17B 7B 13B 9B 7B 8B 4B Language, Image"
        },
        {
            "title": "Comprehension",
            "content": "Language, Image Language, Image"
        },
        {
            "title": "Comprehension\nComprehension",
            "content": "Language, Image"
        },
        {
            "title": "Comprehension",
            "content": "Language, Image"
        },
        {
            "title": "Comprehension",
            "content": "Language, Image"
        },
        {
            "title": "Comprehension",
            "content": "Language, Image"
        },
        {
            "title": "Comprehension",
            "content": "Language, Image"
        },
        {
            "title": "Comprehension",
            "content": "Language, Image"
        },
        {
            "title": "Comprehension",
            "content": "35 2024a) SEED-LLaMA-13B (Ge et al., 2023) Llama2-chat-13B 14B Language, Image Comprehension+Generation"
        },
        {
            "title": "36 LaVIT-V2 (7B) (Jin et al.) LLaMA-7B\n37 LM4LV (Zheng et al.,",
            "content": "LLaMA2-7B instruct 2024)"
        },
        {
            "title": "38 CoLVA-2B (Zhou et al.,",
            "content": "Qwen2-2B 2025)"
        },
        {
            "title": "39 CoLVA-4B (Zhou et al.,",
            "content": "Phi3-3.8B 2025)"
        },
        {
            "title": "40 Long-LLaVA-9B (Wang",
            "content": "Jamba-9B-Instruct et al., 2024b)"
        },
        {
            "title": "41 DeepSeek-VL-2-small (Lu",
            "content": "DeepSeekMoE-16B et al., 2024b)"
        },
        {
            "title": "42 DeepSeek-VL-2 (Lu et al.,",
            "content": "DeepSeekMoE-27B 2024b)"
        },
        {
            "title": "43 Qwen-VL-Chat (Bai et al.,",
            "content": "Qwen-7B 2023)"
        },
        {
            "title": "44 Qwen-Audio-Chat\net al., 2023)\n45 Qwen2-VL-7B",
            "content": "et al., 2024a) (Chu Qwen-7B (Wang Qwen2-7B"
        },
        {
            "title": "46 Qwen2-Audio-Instruct\n(Chu et al., 2024)",
            "content": "Qwen-7B 7B 7B 2B 4.1B 9B 2.8B 4.5B 7B 7B 7B 7B Language, Image Language, Video Comprehension+Generation Generation Language, Image, Video"
        },
        {
            "title": "Comprehension",
            "content": "Language, Image, Video"
        },
        {
            "title": "Comprehension",
            "content": "Language, Video"
        },
        {
            "title": "Comprehension",
            "content": "Language, Image"
        },
        {
            "title": "Comprehension",
            "content": "Language, Image"
        },
        {
            "title": "Comprehension",
            "content": "Language, Image, Video"
        },
        {
            "title": "Comprehension",
            "content": "Language, Audio"
        },
        {
            "title": "Comprehension",
            "content": "Language, Image, Video"
        },
        {
            "title": "Comprehension",
            "content": "Language, Audio"
        },
        {
            "title": "Comprehension",
            "content": "On Path to Multimodal Generalist: General-Level and General-Bench # Model"
        },
        {
            "title": "47 Qwen2-VL-72B (Wang",
            "content": "Qwen2-72B et al., 2024a)"
        },
        {
            "title": "48 LLaVA-NeXT-13B (Liu",
            "content": "Vicuna-13B et al., 2024a)"
        },
        {
            "title": "Size",
            "content": "72B 13B"
        },
        {
            "title": "Paradigm",
            "content": "Language, Image, Video"
        },
        {
            "title": "Comprehension",
            "content": "Language, Image"
        },
        {
            "title": "49 LLaVA-NeXT-34B (Liu",
            "content": "Nous-Hermes-2-Yi-34B 34B Language, Image"
        },
        {
            "title": "Comprehension",
            "content": "et al., 2024a)"
        },
        {
            "title": "50 LLaVA-One-Vision-7B (Li",
            "content": "Qwen2-7B et al., 2024d)"
        },
        {
            "title": "51 LLaVA-One-Vision-72B",
            "content": "Qwen2-72B 53 54 52 (Li et al., 2024d) Sa2VA-8B (Yuan et al., 2025) Sa2VA-26B (Yuan et al., 2025) InternVL-2-8B (Chen et al., 2024c) InternVL-2.5-8B (Chen et al., 2024c) InternVL-2-26B et al., 2024c) InternVL-2.5-26B (Chen et al., 2024c) 58 Vitron-V1 (Fei (Chen al., 56 55 57 et InternLM2-7B InternLM2-20B InternLM2-7B internlm2 5-7b-chat InternLM2-20B internlm2 5-20b-chat vicuna-7b-v 7B 72B 8B 26B 8B 8B 26B 26B 7B Language, Image, Video"
        },
        {
            "title": "Comprehension",
            "content": "Language, Image, Video"
        },
        {
            "title": "Comprehension",
            "content": "Language, Image, Video"
        },
        {
            "title": "Comprehension",
            "content": "Language, Image, Video"
        },
        {
            "title": "Comprehension",
            "content": "Language, Image, Video"
        },
        {
            "title": "Comprehension",
            "content": "Language, Image, Video"
        },
        {
            "title": "Comprehension",
            "content": "Language, Image, Video"
        },
        {
            "title": "Comprehension",
            "content": "Language, Image, Video"
        },
        {
            "title": "Comprehension",
            "content": "Language, Image, Video Comprehension+Generation Nous-Hermes-2-Yi-34B 34B Language, Image Comprehension+Generation 60 (Li et al., 2024a) 59 Mini-Gemini 2024c) 3D-LLM-2.1B (Hong et al., 2023) PointLLM-7B (Xu et al., 2025) PointLLM-13B (Xu et al., 2025) 3D-VisTA (Zhu et al., 2023b) 62 63 BLIP"
        },
        {
            "title": "64 AvatarGPT (Zhou et al.,",
            "content": "T5-large 2024a)"
        },
        {
            "title": "65 MotionGPT-T5",
            "content": "(Jiang T5 et al., 2024b)"
        },
        {
            "title": "67 LLaMA-mesh",
            "content": "(Zhang"
        },
        {
            "title": "LLaMA",
            "content": "et al., 2023e)"
        },
        {
            "title": "68 GAMA (Ghosh et",
            "content": "al., Llama-2-7b-chat 2.1B 7B 13B 1.3B 770M 220M 13B 7B 7B Language, 3D Language, 3D Language, 3D Language, 3D Language, 3D Language, 3D Language, 3D Language, 3D"
        },
        {
            "title": "Generation",
            "content": "Language, Audio"
        },
        {
            "title": "Comprehension",
            "content": "69 2024) Pengi (Deshmukh et al., 2023) GPT2-base 124M Language, Audio"
        },
        {
            "title": "70 WavLLM (Hu",
            "content": "et al., LLaMA-2-7B-chat 71 72 (Tang 2024b) SALMONN-7B et al., 2023) SALMONN-13B (Tang et al., 2023) SpeechGPT-7B-com (Zhang et al., 2023a) 74 AudioGPT-GPT4 (Huang 73 Vicuna-7B Vicuna-13B LLaMA-2 GPT7B 7B 13B 7B / Language, Audio"
        },
        {
            "title": "Comprehension",
            "content": "Language, Audio (Speech)"
        },
        {
            "title": "Comprehension",
            "content": "Language, Audio (Speech)"
        },
        {
            "title": "Comprehension",
            "content": "Language, Audio (Speech)"
        },
        {
            "title": "Generation",
            "content": "Language, Audio (Speech, Sound)"
        },
        {
            "title": "Generation",
            "content": "et al., 2023a)"
        },
        {
            "title": "75 AnyGPT (Zhan et",
            "content": "al., LLaMA-2-7B 8B Language, Image, Audio (Speech, Music) Comprehension+Generation 76 2024) PandaGPT-13B (Su et al., 2023) Vicuna-13B-v0 13B Language, Image, Video, Audio"
        },
        {
            "title": "Comprehension",
            "content": "21 On Path to Multimodal Generalist: General-Level and General-Bench # Model 77 ImageBind-LLM (Han et al., 2023)"
        },
        {
            "title": "Backbone",
            "content": "LLama-1-7B"
        },
        {
            "title": "78 ModaVerse-7b-v0 (Wang",
            "content": "Vicuna-7b-V"
        },
        {
            "title": "Size",
            "content": "7B 7B"
        },
        {
            "title": "Paradigm",
            "content": "Language, Image, Video, Audio"
        },
        {
            "title": "Comprehension",
            "content": "Language, Image, Video, Audio Comprehension+Generation et al., 2024c) 79 Unified-io-2-XXL et al., 2024a) (Lu UIO-2-XXL 6.8B Language, Image, Video, Audio Comprehension+Generation"
        },
        {
            "title": "80 NExT-GPT-V1.5 (Wu et al.,",
            "content": "vicuna-7b-v1.5 2024a)"
        },
        {
            "title": "81 VidAgent† (Shen et al.,",
            "content": "vicuna-7b-v0 2023) 7B 7B Language, Image, Video, Audio Comprehension+Generation Language, Image, Video Comprehension+Generation Note that, for VidAgent, we implement HuggingGPT as the prototype agent, and integrate InternVL-2.5-8B (Chen et al., 2024c) as video comprehension module, and integrate CogVideo (Hong et al., 2022) as video generation module."
        },
        {
            "title": "5.2 Experimental Settings",
            "content": "For different models, we consistently follow the settings provided in their respective GitHub repositories, including model parameters and hyperparameters. We do not perform additional pre-training or fine-tuning. Each task and dataset comes with predefined instruction prompt text. During evaluation, we use the same default prompt across all MLLMs to ensure fairness. The inference time varies across models. Smaller models complete evaluations within few minutes, while larger models require significantly more time. On pure text-based NLP tasks, model inference is highly efficient; however, on video tasks, models demand more memory and have slower inference speeds. Our open-source codebase supports multi-GPU distributed inference, effectively accelerating the evaluation process. Also, we organize personnel into multiple groups to run models in parallel, further optimizing efficiency. For each task, we provide predefined evaluation scripts. Once the model generates outputs, the scripts are used to evaluate performance systematically."
        },
        {
            "title": "5.3 Overall Evaluation Results\nWe note that all the generalists run the evaluation on our General-Bench data set under a zero-shot setting. The overall\nresults of part of the models on image comprehension and generation are presented in Table 6 and Table 7, respectively;\nvideo results are shown in Table 8; audio results are shown in Table 9; 3D results are shown in Table 10; The results of all\ngeneralists on NLP tasks are shown in Table 11. The complete performing scores of all MLLMs across all tasks and datasets\nare presented in Appendix §B. Overall, we have the following observations.",
            "content": "Observation-1: Lack of task support. From these results, the first observation is that the vast majority of MLLMs exhibit lack of support for wide range of tasks in our benchmarks. Even models like OpenAIs GPT-4V and GPT-4o, which achieve top rankings on many existing MLLM benchmarks and leaderboards (Li et al., 2023c; Liu et al., 2024b), fail to demonstrate satisfactory task support on our benchmark. Specifically, GPT-4V and GPT-4o support only 177 out of 271 image comprehension tasks (65.1%). Among open-source models, InternVL2.5-8B achieves task support rate of 71% for image comprehension tasks, outperforming GPT-4V and GPT-4o. For other modalitiessuch as video, audio, and 3Dthe task-supporting rates are much less. Only Vitron-V1 supports over 90% of image tasks, and Sa2VA-8B achieves 72.2% supporting rate in the video comprehension group. This highlights pervasive issue: current MLLMs require significant improvements in their architectural design to support as many tasks as possible. Observation-2: Few generalists surpass the SoTA specialist. Also, we can notice that there are few models capable of surpassing the SoTA generalist. Overall, the tasks and skills that various MLLMs can surpass the SoTA specialists are quite few. As seen, closed-sourced models (e.g., GPT-4V, GPT-4o, Gemini-1.5, and Claude-3.5) have the highest winning rate, with over 30% The best open-sourced Qwen2-VL-72B achieves rate of 36.4% image comprehension by surpassing SoTA specialists. In other modalities such as video, audio, 3D, and language, the chances to surpass SoTA specialists are much lower. If an MLLM cannot outperform the SoTA specialist, it implies that the foundational conditions of cross-task/ability synergy for these MLLMs to become multimodal generalists are not met. Observation-3: Focus more on content comprehension than supporting generation. For instance, GPT-4V and GPT-4o achieve better results than the SoTA specialist in certain skills within image comprehension tasks, and this improvement is significantly more pronounced than that of other models. However, GPT-4V and GPT-4o are limited to image comprehension tasks and provide zero support for image generation tasks. It is thus evident that GPT-4V and GPT-4o are not well-rounded 22 On Path to Multimodal Generalist: General-Level and General-Bench Table 6: Performance of multimodal generalists on various image comprehension skills. Skill full names and specific tasks are listed in Appendix A.6. The full performance records of more generalists are shown in Appendix B. Image Comprehension Skill (Avg within each #I-C Group) Task Completion Level Score on Image Model #1 #11 #21 #31 #2 #12 #22 # #3 #13 #23 #33 #4 #14 #24 #34 #5 #15 #25 #35 #6 #16 #26 #36 #7 #17 #27 #37 #8 #18 #28 # #9 #19 #29 #39 #10 #20 #30 #40 #Supported Task #Win-overSpecialist Level-2 Level-3 Level-4 SoTA Specialist 51.27 53.32 42.04 22.30 39.02 22.42 46.02 15.67 51.20 28.01 36.40 65.15 43.78 58.90 63.73 87.84 58.66 72.25 34.51 95.70 70.00 50.40 65.97 16.60 78.00 50.48 19.90 53.55 64.10 35.90 39.80 57.20 54.60 63.27 29.60 87.10 98.00 39.60 36.42 82.02 / / / / / GPT-4V GPT-4o Gemini-1.5-Pro Gemini-1.5-Flash Claude-3.5-Opus Emu2-32B Phi-3.5-VisionInstruct Qwen2-VL-72B SEED-LLaMA-13B DeepSeek-VL-7B InternVL2.5-8B Vitron-V MoE-LLAVAPhi2-2.7B-4e-384 mPLUG-Owl2LLaMA2-7b 66.18 36.08 61.74 69.42 58.64 39.54 70.90 51.60 0.00 0.00 0.00 71.90 37.12 50.30 16.06 72.20 0.00 0.00 90.40 40.05 16.90 20.88 0.00 0.00 97.98 0.00 31.64 89.10 22.22 22.54 18.08 84.84 0.00 51.04 63.52 0.00 0.00 72. 0.00 0.00 71.56 39.65 68.83 73.87 63.42 43.23 79.38 55.25 0.00 0.00 0.00 0.00 0.00 81.30 39.61 48.63 15.12 93.00 90.40 44.30 67.80 23.24 0.00 0.00 98.79 0.00 33.47 91.20 35.56 24.80 21.12 87.88 0.00 71.23 61.54 0.00 0.00 77. 0.00 0.00 62.38 34.30 66.25 72.33 23.41 39.39 58.09 0.00 0.00 0.00 0.00 84.57 31.55 60.87 15.20 86.40 0.00 0.00 98.00 36.41 59.20 23.79 0.00 0.00 96.76 0.00 38.45 92.00 30.37 22.18 21.20 83.23 0.00 60.86 40.10 0.00 0.00 76. 0.00 0.00 59.45 29.91 63.61 67.00 25.79 37.85 54.57 0.00 0.00 0.00 0.00 80.63 28.97 56.91 16.57 82.60 0.00 0.00 96.40 28.53 56.50 22.19 0.00 0.00 93.42 0.00 29.97 90.20 27.96 20.64 18.22 80.40 0.00 55.22 32.92 0.00 0.00 73. 0.00 0.00 63.35 34.50 63.43 65.38 57.69 39.95 0.00 45.62 20.44 66.57 51.23 0.00 0.00 0.00 0.00 0.00 0.00 94.65 73.04 0.00 0.00 70.39 41.19 54.75 13.87 77.80 0.00 87.31 23.87 28.71 25.75 84.65 0.00 91.38 38.28 0.00 60.21 58.15 0.00 0. 7.31 0.00 21.20 12.83 0.00 41.31 22.22 41.89 53.76 0.00 0.00 5.28 44.51 0.00 0.00 0.00 0.00 56.33 29.43 45.46 21.45 64.20 70.34 0.00 54.59 0.00 73.40 31.72 14.09 18.73 56.97 0.00 72.80 17.73 36.62 0.00 39.47 12.20 0.00 0.00 3.44 0. 42.61 42.04 51.34 55.32 0.00 0.00 0.00 0.00 67.56 32.32 51.51 23.70 90.10 15.02 80.00 83.40 19.31 34.16 0.00 41.00 21.77 24.35 0.00 0.00 0.00 0.00 52.13 11.89 57.68 0.00 52.02 0.00 23.06 25.41 71.31 3.98 0.00 0.00 5.74 0. 56.58 40.50 48.79 66.98 0.00 0.00 0.00 81.86 38.59 58.99 16.17 97.43 0.00 77.64 4.33 43.18 25.32 0.00 0.00 92.41 0.00 16.83 79.34 11.65 29.62 32.22 62.83 0.00 59.87 10.89 72.47 0.00 35.64 0.00 45.66 29.44 0.00 0. 0.00 0.00 9.09 0.00 40.59 13.48 35.10 31.85 46.68 0.00 4.76 33.60 0.00 0.00 0.00 25.42 0.00 38.53 22.52 32.67 24.96 32.20 66.19 47.48 0.00 69.80 14.43 13.13 10.19 51.72 0.80 71.60 0.00 7.20 0.00 0.00 0.00 8.00 0.00 0. 0.00 0.00 33.85 0.00 53.54 0.00 35.35 21.59 53.53 19.30 42.69 33.01 0.00 20.44 90.40 0.00 4.86 0.00 59.96 24.93 0.00 26.68 17.74 0.00 30.13 28.37 46.05 16.95 0.00 18. 99.60 0.00 2.30 3.90 51.58 47.64 0.00 8.19 66.60 39.47 50.00 28.14 22.28 23.52 19.07 36.70 51.38 55.85 49.78 27.69 50.71 40.14 0.00 0.00 0.00 5.80 0.00 9.44 16.83 42.60 38.08 35.39 57.54 48.81 0.00 0.00 7.82 0.00 0.00 10.57 85.90 33.52 0.00 7.80 51.36 9. 0.00 8.06 54.99 9.71 9.41 6.00 0.00 0.00 0.00 50.71 11.97 65.05 12.46 7.76 0.00 0.00 0.00 74.49 16.91 57.17 4.81 35.66 39.78 58.53 82.72 25.13 22.24 14.63 0.00 0.00 4.70 13.30 13.81 0.00 44.96 52.11 71.89 64.20 69.26 15.34 19.12 24.48 59. 0.00 1.90 0.00 42.52 11.84 50.88 50.47 41.79 0.00 0.00 0.00 0.00 0.00 51.13 20.92 26.99 35.40 80.80 8.70 13.71 84.05 50.40 15.69 32.31 0.00 33.98 19.87 0.00 0. 0.00 0.00 36.72 12.35 44.03 52.53 31.75 0.00 0.00 0.00 0.00 0.00 51.60 23.60 41.66 27.08 86.80 8.88 80.10 9.00 60.20 15.27 26.00 0.00 29.01 18.67 0.00 0.00 22.11 3.80 0.00 0.00 0.00 8.62 52.00 52.73 0.00 12.57 15.95 51. 20.88 0.60 0.00 0.00 0.00 9.39 51.67 42.51 0.00 12.14 17.48 70.10 177 (65.1%) 105 (38.6%) 18.16 12.85 0.00 177 (65.1%) 112 (41.2%) 19.67 14.51 0.00 177 (65.1%) 101 (37.1%) 19.67 12. 0.00 177 (65.1%) 94 (34.6%) 18.54 10.85 0. 178 (65.4%) 93 (34.2%) 19.00 11.08 0.00 178 (65.4%) 52 (19.1%) 30.90 5.18 1.25 179 (65.8%) 85 (31.3%) 16.46 9.39 0.00 177 (65.1%) 99 (36.4%) 19. 12.34 0.00 174 (64.0%) 39 (14.3%) 26.81 3. 0.00 180 (66.2%) 64 (23.5%) 13.13 5.75 0. 183 (67.3%) 71 (26.1%) 25.20 13.09 0.00 252 (92.6%) 62 (22.8%) 30.13 7.65 4.59 180 (66.2%) 55 (20.2%) 12.55 5.47 0.00 177 (65.1%) 45 (16.5%) 12. 4.60 0.00 multimodal generalists.2 This trend becomes even more evident in other modalities. significantly higher number of MLLMs support multimodal understanding compared to those supporting multimodal generation. Furthermore, the rate 2It would thus be more rational to claim the current OpenAI GPT-4V/4o series as partial generalists, or visual generalists. 23 On Path to Multimodal Generalist: General-Level and General-Bench Table 7: Performance of part of multimodal generalists on image generation skills. Model SoTA Specialist SEED-LLaMA-14B Emu2-32B AnyGPT LaVIT-V2 (7B) NExT-GPT-V1.5 Vitron-V1 Image Generation Skill (Avg within each #I-G Group) #1 #9 18.70 53. 127.10 30.18 93.52 40.51 158.21 28.88 79.79 46.40 49.71 28.19 19.78 37. #2 #10 45.40 16.47 0.00 87.90 0.00 118.55 0.00 108.06 0.00 89. 0.00 86.45 0.00 24.89 #3 #11 33.77 25.33 37.10 14.58 34.85 15. 40.47 14.91 31.35 15.79 6.00 6.53 21.17 17.95 #4 #12 16.30 43. 7.51 175.33 8.53 154.26 10.30 193.39 11.87 161.54 3.91 53.42 7.45 31. #5 #13 4.86 20.35 127.42 0.00 101.80 0.00 117.21 0.00 149.78 0. 75.71 12.45 32.15 0.00 #6 #14 24.00 67.44 98.33 51.82 81.95 57. 115.91 53.02 59.23 50.18 41.20 38.98 35.33 48.30 #7 #15 99.29 36. 0.00 62.60 0.00 58.17 0.00 64.21 0.00 51.68 0.00 72.72 86.53 58. #8 15.06 0.00 0.00 0.00 0. 47.30 23.47 Task Completion #Supported Task #WinningSpecialist Level Score on Image Level-2 Level-3 Level-4 / / / / / 35 (77.8%) 0 (0.0%) 26.81 3.49 0.00 34 (75.6%) 2 (4.4%) 30.90 5.18 1.25 36 (80.0%) 0 (0.0%) 23.10 1.29 0.00 36 (80.0%) 0 (0.0%) 29. 3.71 0.00 41 (91.1%) 0 (0.0%) 18.69 3. 0.00 42 (93.3%) 3 (6.7%) 30.13 7.65 4. Table 8: Performance of multimodal generalists on video comprehension and generation skills. Video Comprehension Skill (Avg within each #V-C Group) Task Completion Level Score on Video Model #1 # #2 #12 #3 #13 #4 #14 #5 #15 #6 #16 #7 # #8 #18 #9 #19 #10 #20 #Supported Task #Win-overSpecialist Level-2 Level-3 LevelSoTA Specialist 37.43 49.64 21.31 23.06 81.85 85.43 54.53 64.83 40.65 30.80 48.06 68.96 63.62 77.02 75.08 37.20 44.00 45.84 13.92 0.14 / / / / / InternVL-2.5-8B InternVL-2.5-26B Qwen2-VL-72B DeepSeek-VLLLaVA-OneVision-72B Sa2VA-8B Sa2VA-26B CoLVA-4B InternVL-2-8B Long-LLaVA-9B 33.15 27.54 14.51 18.83 0.00 0.00 0.00 0.00 37.03 32.01 18.71 21.57 0.00 0.00 0.00 0. 38.22 32.32 19.35 22.70 0.00 0.00 0.00 0.00 21.50 18.90 12.10 12.10 0.00 0.00 31.20 31.30 19.10 10.60 0.00 0.00 0.00 0. 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0. 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0. 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0. 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 33.19 25.11 16.75 0.00 60.28 0.00 8.67 0. 0.00 71.03 50.95 0.00 19.85 37.83 46.36 42.58 48.02 0.00 35.33 26.33 17.58 10.39 0.00 0.00 0.00 0.00 32.68 26.45 13.55 17.62 0.00 0. 0.00 0.00 32.69 27.09 14.24 17.61 0.00 0.00 0.00 0.00 36.14 26.25 15.89 15.53 0.00 0. 0.00 0.00 0.00 0.00 0.00 28.41 38.91 47.10 43.12 48.42 0.00 0.00 0.00 45. 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0. 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0. 0.00 0.00 0.00 0.00 0.00 4.85 0.00 5.30 0.00 5.70 0.00 3.20 0.00 1. 0.00 1.48 0.00 1.70 0.00 4.23 0.00 4.85 0.00 4.20 55 (43.7%) 5 (4.0%) 5.76 1.24 0.00 55 (43.7%) 26 (20.6%) 6. 3.76 0.00 55 (43.7%) 22 (17.5%) 6.89 5.22 0. 55 (43.7%) 5 (4.0%) 3.98 56 (44.4%) 21 (16.7%) 5.83 0. 3.75 0.00 0.00 91 (72.2%) 32 (25.4%) 8.31 4. 0.00 81 (64.3%) 27 (21.4%) 8.81 4.58 0.00 63 (50.0%) 8 (6.3%) 4.78 1.24 0.00 55 (43.7%) 0 (0.0%) 5.64 0.46 0.00 54 (42.9%) 22 (17.5%) 5.84 3. 0.00 Model Video Generation Skill (Avg within each #V-G Group) Task Completion #1 # #3 #4 #5 #6 #Task-Supprt #Win-Spclst Level Score on Video Level-3 Level-2 Level-4 SoTA Specialist 69.09 55. 88.94 62.90 37.79 51.46 / VidAgent LM4LV NExT-GPT-V1.5 Vitron-V1 52.42 47.73 0. 26.78 36.74 0.00 6.72 19.32 88. 0.00 130.22 116.31 63.61 0.00 16. 25.09 0.00 25.90 0.08 0.08 0. 5.93 0.06 0.06 30 (65.2%) 8 (17.4%) 40 (87.0%) 40 (87.0%) / 0 (0.0%) 0 (0.0%) 0 (0.0%) 0 (0.0%) / 25.00 6.74 8.34 18.72 / 0.00 0.00 0.71 3.04 / 0. 0.00 0.00 0.00 at which MLLMs surpass SoTA specialists in multimodal understanding benchmarks is much higher than in multimodal generation benchmarks. We emphasize that this imbalance reflects critical limitation in the capability building of current multimodal generalists. Observation-4: Insufficient support for all modalities. We also found that many MLLMs are unable to support all modalities simultaneously. Moreover, the vast majority of existing MLLMs are predominantly focused on understanding or generating image-based modalities. In contrast, much less attention has been devoted to video, audio, and 3D modalities On Path to Multimodal Generalist: General-Level and General-Bench Table 9: Performance of multimodal generalists on audio comprehension and generation skills. Model Audio Comprehension Skill (Avg within each #A-C Group) #9 #5 #1 #2 # #8 #6 #4 #7 Task Completion #Task-Supprt #Win-Spclst Level Score on Audio Level-2 Level-3 Level-4 SoTA Specialist 87.27 79.08 70.62 79.00 71.87 62.90 58.70 77.90 78.07 / / / / Qwen-Audio-Chat 56.93 68.77 76.80 37.70 47.71 19.79 56.44 85.15 78.50 30 (100.0%) 6 (25.0%) Qwen2-Audio-Instruct72.65 74.80 61.40 36.80 45.82 13.45 61.68 78.95 67.99 24 (100.0%) 6 (25.0%) GAMA Pengi 57.00 64.20 68.00 53.20 18.43 26.95 48.85 85.55 61. 23 (95.8%) 4 (16.7%) 52.88 60.07 56.70 36.78 19.77 19.55 42.95 77.40 61.17 23 (95.8%) SALMONN-13B 67.89 56.33 67.80 29.45 24.67 19.36 43.95 76.55 56. 23 (95.8%) WavLLM 64.45 41.07 71.20 30.08 31.30 26.55 45.75 61.40 64.57 24 (100.0%) NExT-GPT-V1.5 43.23 29.13 65.80 26.70 14.47 25.65 47.95 70.20 69. 24 (100.0%) PandaGPT (13B) 41.80 20.23 45.20 20.98 8.47 20.50 42.25 54.80 65.83 24 (100.0%) ModaVerse-7b-v0 34.10 16.37 32.80 15.20 6.60 8.90 35.05 49.20 60.13 23 (95.8%) Any-GPT 44.50 32.13 63.40 48.08 16.27 36.40 52.65 67.95 44.63 23 (95.8%) Unified-io-2-XXL 30.15 27.60 56.10 28.58 15.47 38.35 38.70 63.50 60.63 24 (100.0%) 28.39 28.61 26.35 23.29 23.95 23. 25.05 16.98 26.10 29.06 25.63 10. 8.53 7.15 1.74 3.61 3.28 1. 0.65 1.14 3.29 1.01 / 0. 0.00 0.00 0.00 0.00 0.00 0. 0.00 0.00 0.00 0.00 1 (4.2%) 2 (8.3%) 2 (8.3%) 0 (0.0%) 0 (0.0%) 0 (0.0%) 1 (4.2%) 0 (0.0%) Model #1 Audio Generation Skill (Avg within each #A-G Group) #10 #2 #7 #9 # #8 #4 #5 #3 Task Completion #11 #Task-Supprt #Win-Spclst Level Score on Audio Level-2 Level-3 Level-4 SoTA Specialist 31.50 3.82 3.64 4.68 41.54 51.40 11.52 6.80 8. 22.88 20.33 / / / / / Unified-io-2-XXL 18.36 2.03 5.11 40.52 16.41 24.31 16.97 86.23 94.52 0.25 2.24 17 (85.0%) Any-GPT 23.50 3.24 4.57 33.58 13.38 14.05 27.49 45.36 83.89 0.25 2.47 17 (85.0%) NExT-GPT-V1. 13.60 1.15 4.07 50.51 34.51 1.35 12.36 96.70 99.23 0.25 7.77 17 (85.0%) 0.50 1.32 4.61 23.10 29.48 0. 0.00 46.30 79.98 0.25 0.00 13 (65.0%) 0.10 2.79 4.44 32.35 0.00 0.00 0.00 30.24 85.54 0. 0.00 11 (55.0%) AudioGPT SpeechGPT ModaVerse 0 (0.0%) 1 (5.0%) 1 (5.0%) 1 (5.0%) 0 (0.0%) 25.63 29. 25.05 8.80 7.22 1.01 3.29 1. 3.02 0.00 1.14 0.00 0.00 0. 0.00 0.00 0.00 12.30 1.15 4.29 50.50 28.99 1.05 16.45 100.00 100.00 0.25 4.17 17 (85.0%) 2 (10.0%) 26.10 Table 10: Performance of multimodal generalists on 3D comprehension and generation skills. Model #1 # 3D Comprehension Skill (Avg within each #D-C Group) #7 #10 #11 #4 #3 #5 # #8 #9 #12 Task Completion #13 #Task-Supprt #Win-Spclst Level Score on 3D Level-2 Level-3 Level-4 SoTA Specialist 96.24 98.35 97.78 78.50 70.02 81.20 55.00 88.28 75.20 9.96 68.52 47.14 22.30 / / / / / 3D-VisTA PointLLM-7B PointLLM-13B 3D-LLM AvatarGPT 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 46.37 0.00 46.16 7.50 72.86 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 48.79 10.00 78.14 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 46.34 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 12.70 7 (23.3%) 8 (26.7%) 9 (30.0%) 7 (23.3%) 1 (3.3%) 2 (6.7%) 0 (0.0%) 0 (0.0%) 1 (3.3%) 0 (0.0%) 5.41 6.53 7.00 5.41 0.21 1.07 0.00 0.00 1.38 0.21 0.00 0.00 0.00 0.00 0.00 Model 3D Generation Skill (Avg within each #D-G Group) Task Completion #1 #2 #3 #4 #5 # #7 #8 #9 #Task-Supprt #Win-Spclst Level Score on 3D Level-2 Level-3 LevelSoTA Specialist 0.22 7.12E-5 24.42 25.69 78.06 83.64 6540.02 6540.02 0.23 / / 0.00 MotionGPT-T5 MotionGPT-LLaMA 0.00 0.00 LLaMA-Mesh 0.00 0.00 0. 0.00 0.00 0.00 0.00 0.00 0.00 0.00 17.55 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.51 0.60 0.00 1 (4.5%) 1 (4.5%) 1 (4.5%) 0 (0.0%) 0 (0.0%) 0 (0.0%) / 0.00 0.00 1.60 / 0.00 0.00 0.00 / 0.00 0.00 0.00 (attention: image > video > 3D > audio), with relatively few multimodal generalists addressing these areas. Most MLLMs, including the strongest ones, primarily handle image and language tasks, offering little to no support for other modalities. The completeness of support across various modalities and functionalities is insufficient for existing MLLMs to qualify as true multimodal generalists. We emphasize that to be considered multimodal generalist, model must be capable of understanding and generating signals from as many modalities as possible simultaneously. Observation-5: Multimodality does NOT really enhance language. The ideal multimodal generalists should enable mutual enhancement across modalities. Unfortunately, our experimental results (as shown in Table 11) reveal that none of the current MLLMs provide any improvements in NLP tasks. Although various MLLMs achieve certain scores on NLP tasks, none of them surpass the performance of SoTA specialists in NLP. Furthermore, the performance gap between MLLMs and SoTA specialists in NLP tasks is larger than the gap observed in other modalities. While certain relevant research suggests that models, such as Vicuna, Qwen2, and LLaMA, trained with multimodal data (e.g., images) can also improve NLP tasks, 25 On Path to Multimodal Generalist: General-Level and General-Bench Table 11: Performance of generalists on language-related (NLP) skills. Model #1 #12 #2 #13 Language Skill (Avg within each #L Group) #6 #17 #5 #16 #4 # #7 #18 #8 #19 #9 #20 #3 #14 Task Completion Level Score #10 #21 #11 #22 #Supported Task #Win-overSpecialist Level-5 SoTA Specialist Meta-Llama-3.18B-Instruct ChatGLM-6b Vicuna-7b-v1.5 Falcon3-7B-Instruct Ministral-8BInstruct-2410 Yi-Lightning GPT-4V GPT-4o Emu2-32B DeepSeek-VL-7B Qwen2-VL-7B LLaVA-OneVision-72B InternVL2.5-8B Long-llava NExT-GPT-V1.5 SEED-LLaMA-13B LLaMA-Mesh MiniGPT4-LLaMA 62.62 86.95 86.23 0.31 76.78 94.40 71.00 91.41 58.02 86.05 62.80 86. 75.11 84.72 77.84 83.67 79.70 58.61 71.91 77.73 28.27 92.38 / / 39.75 45.34 28.97 42.84 24.78 43.98 36.79 48.15 41.74 23. 41.73 52.68 27.55 44.56 26.25 46.41 32.91 50.15 29.97 79.68 23.91 37. 50.44 43.81 42.93 71.96 26.50 48.44 20.66 42.09 18.11 20.84 29.34 44. 28.17 42.56 56.76 7.95 33.24 10.91 11.18 11.41 58.36 5.15 54.21 11. 60.54 5.37 62.40 3.16 62.57 2.58 45.43 9.53 44.39 83.00 27.51 6. 41.98 3.55 47.76 75.20 49.49 11.40 22.42 1.06 32.55 11.16 16.70 11. 15.73 7.46 54.21 76.40 37.24 41.80 33.44 0.00 49.91 88.80 49.53 84. 55.39 72.60 34.57 86.20 33.98 85.40 47.04 57.54 55.55 62.20 37.68 64. 54.55 84.80 59.54 55.40 34.81 68.60 32.55 68.90 26.54 13.20 47.05 0. 40.98 0.00 60.52 51.80 46.10 45.81 41.19 0.00 56.80 85.89 51.92 72. 60.51 56.24 32.55 83.23 31.50 86.30 39.56 48.78 20.36 50.60 46.40 37. 61.13 10.43 31.17 68.40 39.62 41.70 39.51 43.20 25.19 34.80 56.85 0. 45.15 0.00 20.01 65.90 19.39 24.50 4.51 0.00 21.38 45.65 39.32 56. 20.53 64.75 14.43 65.10 16.20 67.50 27.74 43.76 40.49 62.30 17.84 3. 29.87 59.35 42.86 56.75 17.83 52.65 4.19 28.78 8.80 28.98 5.09 0. 3.71 0.00 37.17 41.10 27.84 16.45 13.25 0.96 37.12 42.86 40.49 37. 39.83 43.59 27.84 53.82 26.26 56.10 31.24 36.67 57.93 46.87 20.96 20. 56.99 34.91 32.72 55.60 33.14 31.42 16.47 9.24 18.85 19.93 14.85 1. 10.99 2.08 36.23 24.49 18.85 0.12 19.94 0.07 32.03 27.64 13.00 6. 22.45 28.27 27.79 54.14 27.14 57.42 39.04 19.84 49.85 4.12 36.25 0. 35.24 42.94 50.98 22.12 20.63 2.33 16.49 4.44 11.68 2.59 21.27 0. 25.97 6.36 29.12 30.70 35.88 8.41 35.27 0.47 42.11 34.22 22.86 31. 43.57 42.84 36.07 45.45 36.64 46.97 41.72 24.01 48.73 28.46 29.29 4. 43.27 28.63 43.02 36.48 38.44 21.52 32.67 6.16 15.89 10.31 39.24 0. 43.03 4.52 53.23 8.08 27.85 2.70 54.81 0.00 55.79 11.19 56.87 9. 62.52 25.34 65.36 33.86 66.86 39.52 45.48 13.78 27.03 8.11 35.42 6. 55.23 19.26 30.85 9.80 48.90 7.40 51.49 7.22 21.64 2.10 57.40 0. 35.22 0.00 44.49 32.40 38.51 23.80 40.58 23.13 42.07 39.80 43.46 25. 42.03 29.27 42.11 26.46 42.69 32.07 46.35 26.47 56.76 31.80 35.58 20. 41.49 52.20 51.23 32.13 38.10 29.47 37.73 24.17 23.56 12.07 40.65 23. 36.14 17.55 14.80 54.35 13.93 45.37 5.06 15.40 15.56 58.75 13.73 40. 15.29 60.49 13.96 24.24 14.49 28.50 13.05 31.72 10.37 40.97 12.62 21. 17.73 71.95 9.07 53.67 6.30 42.07 5.06 18.86 4.80 12.41 4.93 21. 10.42 21.23 113 (98.3%) 0 (0.0%) 96 (83.5%) 0 (0.0%) 72 (62.6%) 0 (0.0%) 112 (97.4%) 0 (0.0%) 112 (97.4%) 0 (0.0%) 113 (98.3%) 0 (0.0%) 113 (98.3%) 0 (0.0%) 113 (98.3%) 0 (0.0%) 113 (98.3%) 0 (0.0%) 114 (99.1%) 0 (0.0%) 94 (81.7%) 0 (0.0%) 110 (95.7%) 0 (0.0%) 114 (99.1%) 0 (0.0%) 107 (93.0%) 0 (0.0%) 79 (68.7%) 0 (0.0%) 109 (94.8%) 0 (0.0%) 84 (73.0%) 0 (0.0%) 84 (73.0%) 0 (0.0%) / 0.00 0.00 0.00 0. 0.00 0.00 0.00 0.00 0.00 0. 0.00 0.00 0.00 0.00 0.00 0. 0.00 0.00 such improvement has not yet enabled models to outperform SoTA NLP specialists on core language tasks. Our large-scale evaluation shows they still fall short of outperforming fine-tuned language specialists. We hypothesize that existing MLLMs, despite utilizing language-centered LLMs as their core, have significantly weakened their language capabilities due to an excessive focus on training and fine-tuning on non-language modalities. This trade-off not only undermines their language understanding but also fails to leverage multimodal information to enhance language-related tasks. Table 12: Leaderboard of multimodal generalists (MLLMs) at level-2."
        },
        {
            "title": "Paradigm",
            "content": "Level 2 Score"
        },
        {
            "title": "Ranking",
            "content": "Unified-io-2-XXL AnyGPT NExT-GPT-V1.5 ImageBind-LLM ModaVerse-7b-v0 Vitron-V1 PandaGPT-13B VidAgent C+G C+G C+G C+G C+G C+G of Image 20.62 23.10 18.69 19.54 15.56 30.13 20.78 18.21 of Video 8.56 0.00 8.34 12.54 7.32 18.72 9.34 25.00 of Audio 25.63 29.06 25.05 17.52 26.10 0.00 16.98 0.00 of 3D of Overall 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0. 13.70 13.04 13.02 12.40 12.25 12.21 11.78 10.80 1 2 3 4 5 6 7 8 26 On Path to Multimodal Generalist: General-Level and General-Bench"
        },
        {
            "title": "Paradigm",
            "content": "Level 2 Score"
        },
        {
            "title": "Ranking",
            "content": "InternVL2 5-8B Emu2-37B Sa2VA-26B LaVIT-V2 (7B) LLaVA-One-Vision-72B Qwen2-Audio-Instruct Qwen-Audio-Chat Mini-Gemini SEED-LLaMA-13B GAMA Qwen2-VL-72B Sa2VA-8B InternVL-2.5-26B Qwen2-VL-7B InternVL2 5-4B SALMONN-13B InternVL-2-26B WavLLM Monkey-10B-chat InternVL2 5-2B Pengi LLaVA-One-Vision-7B SALMONN-7B InternVL-2.5-8B DeepSeek-VL-7B-Chat InternVL-2-8B GPT4-o GPT4-o-4096 Gemini-1.5-Pro Claude-3.5-Sonnet Claude-3.5-Opus chatgpt4-o-latest Gemini-1.5-Flash CoLVA-4B GPT4-V GPT4-o-mini GLM-VL-Chat Idefics3-8B-Llama3 LLaVA-NeXT-34B Phi-3.5-Vision-Instruct MiniCPM3-4B CogVLM-Chat CoLVA-2B InternVL-Chat-V1-5 DetGPT BLIP-3 (XGen-MM) LLaVA-NeXT-13B Pixtral-12B ShareGPT4V-13B Yi-vision-v2 Qwen-VL-Chat ShareGPT4V-7B Mini-InternVL-Chat-4B-V1-5 InternLM-XComposer2-VL-1.8B DeepSeek-VL-7B-Base MiniGPT4-LLaMA2-7B C+G C+G C+G C+G C C C C C C C C C C C of Image 25.20 30.90 21.88 29.50 23.12 0.00 0.00 27.90 26.81 0.00 19.41 17.33 18.73 18.42 24.41 0.00 17.55 0.00 23.51 23.32 0.00 18.32 0.00 14.70 19.89 14.06 19.67 19.68 19.67 19.38 19.00 18.98 18.54 13.59 18.16 17.79 17.00 16.71 16.58 16.46 16.46 16.31 11.73 16.16 16.05 15.40 15.11 14.74 14.72 14.61 13.91 13.78 13.53 13.31 13.13 12.89 of Video 8.44 0.00 8.81 0.00 5.83 0.00 0.00 0.00 0.00 0.00 6.89 8.31 6.70 6.00 0.00 0.00 6.36 0.00 0.00 0.00 0.00 4.34 0.00 5.76 0.00 5.64 0.00 0.00 0.00 0.00 0.00 0.00 0.00 4.78 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 4.47 0.00 0.00 0.00 0.00 0.00 0.00 0.00 5.34 0.00 0.00 0.00 0.00 0.00 of Audio 0.00 0.00 0.00 0.00 0.00 28.61 28.39 0.00 0.00 26.35 0.00 0.00 0.00 0.00 0.00 23.95 0.00 23.49 0.00 0.00 23.29 0.00 21.09 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 of 3D of Overall 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0. 8.41 7.73 7.67 7.38 7.24 7.15 7.10 6.975 6.70 6.59 6.58 6.41 6.36 6.11 6.10 5.99 5.98 5.87 5.87 5.83 5.82 5.67 5.27 5.12 4.97 4.93 4.92 4.92 4.92 4.85 4.75 4.74 4.64 4.59 4.54 4.45 4.25 4.18 4.15 4.12 4.12 4.08 4.05 4.04 4.01 3.85 3.78 3.69 3.68 3.65 3.48 3.45 3.38 3.33 3.28 3.22 27 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 On Path to Multimodal Generalist: General-Level and General-Bench"
        },
        {
            "title": "Paradigm",
            "content": "Level 2 Score"
        },
        {
            "title": "Ranking",
            "content": "MoE-LLAVA-Phi2-2.7B-4e-384 mPLUG-Owl2-LLaMA2-7b Cambrian-1-8B BLIP2 miniMonkey NExT-Chat Audio-GPT4 GPT4RoI-7B Show-o SpeechGPT-7B-com PointLLM-13B LM4LV PointLLM-7B Long-LLaVA-9B 3D-VisTA 3D-LLM-2.1B OMG-LLaVA-InternLM20B DeepSeek-VL-2 DeepSeek-VL-2-small Otter LLaMA-mesh LISA GLaMM AvatarGPT MotionGPT-T5 MotionGPT-LLaMA Meta-Llama-3.1-8B-Instruct Gemma-2-9b-it GPT-J ChatGLM-6B Qwen2.5-7B-Instruct InternLM2-Chat-7B Baichuan2-7B-Base Vicuna-7b-V1.5 Falcon3-7B-Instruct Ministral-8B-Instruct-2410 Yi-lightning GPT-3.5-turbo C C C+G C C G G / / / / / / / / / / / / of Image 12.55 12.21 11.76 11.65 11.31 10.65 0.00 8.49 7.78 0.00 0.00 0.00 0.00 10.23 0.00 0.00 4.56 19.21 17.40 3.15 0.00 1.27 0.94 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 of Video 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 6.74 0.00 5.84 0.00 0.00 0.00 3.98 3.64 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 of Audio 0.00 0.00 0.00 0.00 0.00 0.00 8.80 0.00 0.00 7.22 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 of 3D of Overall 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 7.00 0.00 6.53 0.00 5.41 5.41 0.00 0.00 0.00 0.00 1.60 0.00 0.00 0.21 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0. 3.14 3.05 2.94 2.91 2.83 2.66 2.20 2.12 1.95 1.81 1.75 1.69 1.63 1.46 1.35 1.35 1.14 1.00 0.91 0.79 0.40 0.32 0.24 0.05 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 / / / / / / / / / / / / / / Table 14: Leaderboard of multimodal generalists (MLLMs) at level-3 where Comprehension and Generation."
        },
        {
            "title": "Paradigm",
            "content": "Level 3 Score"
        },
        {
            "title": "Ranking",
            "content": "Sa2VA-26B LLaVA-One-Vision-72B Qwen2-VL-72B Mini-Gemini Sa2VA-8B InternVL2 5-8B GPT4-o-4096 Qwen2-VL-7B GPT4-o InternVL-2-26B InternVL-2.5-26B ChatGPT-o-latest C+G C C of Image 14.65 15.21 12.34 17.23 12.39 13.09 14.68 12.13 14.51 8.81 9.51 13.02 of Video 4.58 3.75 5.22 0.00 4.38 1.82 0.00 2.47 0.00 4.81 3.76 0.00 of Audio 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 of 3D of Overall 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0. 4.81 4.74 4.39 4.31 4.19 3.73 3.67 3.65 3.63 3.41 3.32 3.26 1 2 3 4 5 6 7 8 9 10 11 12 28 On Path to Multimodal Generalist: General-Level and General-Bench"
        },
        {
            "title": "Paradigm",
            "content": "Level 3 Score"
        },
        {
            "title": "Ranking",
            "content": "GPT4-V Gemini-1.5-Pro Claude-3.5-Sonnet GPT4-o-mini LLaVA-One-Vision-7B InternVL2 5-4B Monkey-10B-chat InternVL2 5-2B Claude-3.5-Opus Gemini-1.5-Flash Vitron-V1 CoLVA-4B Qwen-Audio-Chat InternVL-Chat-V1-5 Phi-3.5-Vision-Instruct DeepSeek-VL-2 InternVL-2.5-8B GLM-VL-Chat Qwen2-Audio-Instruct LLaVA-NeXT-34B DeepSeek-VL-7B-Chat MiniCPM3-4B Long-LLaVA-9B Yi-vision-v2 CogVLM-Chat InternVL-2-8B Idefics3-8B-Llama3 CoLVA-2B GAMA LLaVA-NeXT-13B BLIP-3 (XGen-MM) ShareGPT4V-13B Qwen-VL-Chat DeepSeek-VL-7B-Base Pixtral-12B DeepSeek-VL-2-small MoE-LLAVA-Phi2-2.7B-4e-384 NExT-GPT-V1.5 Mini-InternVL-Chat-4B-V1-5 Emu2-37B InternLM-XComposer2-VL-1.8B ShareGPT4V-7B MiniGPT4-LLaMA2-7B mPLUG-Owl2-LLaMA2-7b AnyGPT miniMonkey Cambrian-1-8B DetGPT LaVIT-V2 (7B) SALMONN-13B ImageBind-LLM NExT-Chat SEED-LLaMA-13B WavLLM Unified-io-2-XXL ModaVerse-7b-v0 C C C+G C C C C C C C+G C+G C C+G C+G C+G C+G C+G of Image 12.85 12.66 11.98 11.94 10.21 11.59 11.59 11.45 11.08 10.85 7.65 9.45 0.00 9.42 9.39 8.32 7.63 8.67 0.00 8.24 8.19 8.11 4.21 7.85 7.77 7.28 7.70 6.60 0.00 6.87 6.42 5.97 5.88 5.75 5.72 5.12 5.47 3.24 5.21 5.18 4.78 4.78 4.68 4.60 1.29 4.51 3.84 3.77 3.71 0.00 1.56 3.51 3.49 0.00 2.11 0.98 of Video 0.00 0.00 0.00 0.00 1.54 0.00 0.00 0.00 0.00 0.00 3.04 1.24 0.00 0.00 0.00 0.64 1.24 0.00 0.00 0.00 0.00 0.00 3.81 0.00 0.00 0.46 0.00 1.04 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.52 0.00 0.71 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.72 0.00 0.00 0.00 0.14 0.23 of Audio 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 10.57 0.00 0.00 0.00 0.00 0.00 8.53 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 7.15 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.34 0.00 0.00 0.00 0.00 0.00 0.00 3.29 0.00 0.00 0.00 0.00 3.61 1.26 0.00 0.00 3.28 1.01 1.14 of 3D of Overall 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0. 3.21 3.17 3.00 2.99 2.94 2.90 2.90 2.86 2.77 2.71 2.67 2.67 2.64 2.36 2.35 2.24 2.22 2.17 2.13 2.06 2.05 2.03 2.01 1.96 1.94 1.94 1.93 1.91 1.79 1.72 1.61 1.49 1.47 1.44 1.43 1.41 1.37 1.32 1.30 1.30 1.20 1.20 1.17 1.15 1.15 1.13 0.96 0.94 0.93 0.90 0.89 0.88 0.87 0.82 0.82 0.78 29 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 On Path to Multimodal Generalist: General-Level and General-Bench"
        },
        {
            "title": "Paradigm",
            "content": "Level 3 Score"
        },
        {
            "title": "Ranking",
            "content": "PandaGPT-13B Audio-GPT4 BLIP2 GPT4RoI-7B Pengi 3D-LLM-2.1B 3D-VisTA Show-o LISA Otter OMG-LLaVA-InternLM20B GLaMM AvatarGPT PointLLM-7B PointLLM-13B MotionGPT-T5 MotionGPT-LLaMA LLaMA-mesh SALMONN-7B SpeechGPT-7B-com LM4LV VidAgent Meta-Llama-3.1-8B-Instruct Gemma-2-9b-it GPT-J ChatGLM-6B Qwen2.5-7B-Instruct InternLM2-Chat-7B Baichuan2-7B-Base Vicuna-7b-V1.5 Falcon3-7B-Instruct Ministral-8B-Instruct-2410 Yi-lightning GPT-3.5-turbo C C+G C G G C+G / / / / / / / / / / / / of Image 2.35 0.00 2.79 2.36 0.00 0.00 0.00 0.84 0.82 0.68 0.44 0.41 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 of Video 0.05 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 of Audio 0.65 3.02 0.00 0.00 1.74 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 of 3D of Overall 0.00 0.00 0.00 0.00 0.00 1.38 1.07 0.00 0.00 0.00 0.00 0.00 0.21 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0. 0.76 0.76 0.70 0.59 0.44 0.35 0.27 0.21 0.21 0.17 0.11 0.10 0.05 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 69 70 71 72 73 74 75 76 77 78 79 80 81 / / / / / / / / / / / / / / / / / / / / / Table 16: Leaderboard of multimodal generalists (MLLMs) at level-4, where Comprehension and Generation."
        },
        {
            "title": "Model",
            "content": "Mini-Gemini Vitron-V1 Emu2-37B"
        },
        {
            "title": "Modality Paradigm",
            "content": "Level 4 Score of Image of Video of Audio of 3D of Overall 0.00 0.00 0.00 0.00 0.00 0.00 6.23 4.59 1.25 1.56 1.15 0.31 0.00 0.00 0. C+G C+G C+G"
        },
        {
            "title": "Ranking",
            "content": "1"
        },
        {
            "title": "5.4 Level and Leaderboard of Multimodal Generalists",
            "content": "Based on the overall performance of each model across the various modalities and tasks, we rank all the compared models according to the General-Level scoring defined in 3.2. Tables 12, 14 and 16 present the specific scores and rankings of multimodal generalists at different General-Levels. Note that no generalists score non-zero at Level-5, and thus we do not show rank at Level-5. Figure 1 visualizes these leaderboards. As shown, for all the current MLLMs at level 2, Unified-io-2-XXL (Lu et al., 2024a) ranks the best, followed by AnyGPT (Zhan et al., 2024). Surprisingly, GPT-4V and GPT-4o did not achieve the expected rankings at level 2. While the GPT series excels in the individual tasks it supports, as generalists, they fall short in skill coverage compared to some open-source MLLMs. This is because, to rank higher at level 2, models must not only perform well on different tasks but also support as many modalities and tasks as possible. 30 On Path to Multimodal Generalist: General-Level and General-Bench Figure 9: Visualization of skill support in various multimodal generalists. Next, MLLMs that can manage to reach level 3 become different. Sa2VA-26B (Yuan et al., 2025) ranks at the top, while LLaVA-One-Vision-72B (Li et al., 2024d) and Qwen2-VL-72B (Wang et al., 2024a) achieve the second and third places, respectively. Some high-ranking level-2 models lost their places at level 3. This lies in the fact that most MLLMs are limited to multimodal content comprehension and lack support for generation tasks. GPT-4V and GPT-4o win top-10 positions here. Finally, only 3 MLLMs that reach level 4 can be seen, i.e., Mini-Gemini, Emu2-37B, and Vitron-V1. At this level, these models exhibit synergy across both comprehension and generation. Besides these three models, no other systems exhibit such capability. Most critically, no model has yet demonstrated the ability to enhance language intelligence through non-language modalities, underscoring the significant challenges in the pursuit of true AGI. And this is definitely our goal to reach the most capable multimodal generalists. 31 On Path to Multimodal Generalist: General-Level and General-Bench Figure 10: Supporting modality. Figure 11: Supporting generation and comprehension."
        },
        {
            "title": "5.5 Capability BreakDown",
            "content": "We now take closer look, as multimodal generalists, at how well different MLLMs support tasks and modalities. Task Supporting. In Figure 9, we present all skills (meta-tasks) supported by different MLLMs across various modalities and within the scopes of comprehension and generation. Overall, MLLMs show relatively lower task support for 3D tasks and skills, compared with the status for other modalities. Also, the coverage of comprehension-related skills by MLLMs should be generally higher than that of generation-related skills. This trend is consistent with the results observed in previous experiments. significant trend we identified is that MLLMs tend to support skills within only one (or few) task paradigms. This results in differentiated skill support across different MLLMs, with few models capable of supporting wide range of skills across diverse tasks. Moreover, most MLLMs are inclined to focus on basic skills or tasks with simpler and more straightforward definitions, while tasks requiring complex content output and advanced skills are supported by far fewer models. For instance, compared to coarse-grained visual understanding tasks (e.g., captioning and classification), tasks with more complex definitionssuch as pixel-level object detection, image/3D segmentation, video tracking, and image generationare supported by far fewer existing MLLMs. However, we observe that few MLLMs stand out for their broader support of cross-modal skills, such as Vitron-V1 (Fei et al., 2024a). Thanks to their architectural designs, these models demonstrate wider range of task support compared to others. We emphasize that supporting as many task paradigms as possible is critical requirement for developing more capable multimodal generalists. Modality Supporting. The broader the range of supported modalities, the more general and versatile the models capabilities are. Our benchmark emphasizes the evaluation of MLLMs all-modality capabilities. As shown in the experimental results above, most MLLMs support only single modality (excluding the language modality, which is inherently supported by LLMs). To further illustrate this, Figure 10 compares the multimodal support capabilities of several top-performing MLLMs. In general, there are very few MLLMs capable of supporting multiple modalities simultaneously. In most cases, MLLMs support one non-language modality, e.g., GPT-4V (OpenAI, 2022b), Emu2-32B (Sun et al., 2024), Mini-Gemini (Li et al., 2024c), InternVL2.5-8B (Chen et al., 2024c). Only few MLLMs stand out with broader cross-modal or even all-modality support capabilities, encompassing language, image, video, and audio modalities. Examples include NExT-GPT-V1.5 (Wu et al., 2024a), Unified-io-2-XXL (Lu et al., 2024a), and AnyGPT (Zhan et al., 2024), etc. Thanks to their architectural designs, these systems demonstrate wider range of modality and task support compared to others. Capabilities on Comprehension vs. Generation. Within single modality, tasks and skills can be categorized into comprehension and generation types. Here, we explore the capabilities of different MLLMs in supporting these two paradigms. We select several representative MLLMs that can or cannot support both comprehension and generation within various modalities for comparison. Figure 11 directly presents the statistics on these models capabilities in these two aspects. Overall, support for content comprehension significantly outweighs support for content generation. This phenomenon aligns with practical realities, as modeling tasks for comprehension typically involve expressing the understood content in language, On Path to Multimodal Generalist: General-Level and General-Bench Figure 12: Visualizations of synergy effects between all different skills of various MLLMs. which is relatively straightforward. In contrast, generating multimodal content requires additional efforts in model decoding and extra training, making it more challenging capability to achieve. It is also evident that different models exhibit varying balances between comprehension and generation capabilities. Among them, Vitron-V1 (Fei et al., 2024a) demonstrates the most comprehensive and well-rounded capabilities in both comprehension and generation to date, i.e., supporting the largest number of both paradigms."
        },
        {
            "title": "5.6 Analysis and Discussion on Synergy",
            "content": "Next, we conduct finer-grained analysis of the synergy performance of various MLLMs. Synergy Across Skills. First, we examine the different synergy effects exhibited by models across various skills. Skills are categorized based on different modalities and further divided into comprehension and generation categories. We explore the synergy effects displayed by top-performing MLLMs within these skill groups. Technically, we calculate the synergy score for tasks (skills) based on the level-3 score algorithm from General-Level ( 3.2). Then we count the number of synergy tasks for each model and use scores exceeding SoTA specialists as weights. Figure 12 visualizes the results with heatmaps. It is shown that different models exhibit varying levels of cross-task synergy capabilities. Overall, models that achieve higher level-3 scores tend to display denser clusters of highlighted cells in the heatmap. Also, we observe that synergy effects are more likely to occur among closely related skills, as knowledge and information are more easily transferable between similar tasks. This trend is consistent across different modalities. Furthermore, generation tasks seem to exhibit stronger synergy effects compared to tasks within the comprehension category. Synergy Across Modalities. Finally, we analyze whether different models have learned synergy effects across modalities. The approach is similar to the previous methods, where we calculate instances where performance across modalities exceeds that of SoTA specialists. Figure 13 visualizes the performance of 6 representative strong-performing MLLMs that cover 33 On Path to Multimodal Generalist: General-Level and General-Bench Figure 13: Visualizations (symmetrised) of synergy effects between modalities of various MLLMs. as many modalities as possible. Several notable trends emerge. First, we observe that there is significant synergy occurs between the image and video modalities. This is reasonable, as static image information and dynamic video content are both fundamentally visual in nature, allowing for substantial information sharing that enhances performance across tasks. Most strikingly, although language appears to exhibit synergy effect with various other modalities, this effect is in fact unidirectional, specifically from language other modalities. Based on our previous results on NLP tasks, no synergy has been observed from any other modality to the language modality. While in theory, the audio modality should be closely related to language, and we would expect to observe synergy between audio and language tasks, this is not reflected in current results. This limitation is likely due to the reliance of audio-based LLM architectures on the language intelligence of LLMs, which has not yet translated into performance improvements that exceed those of NLP SoTA specialists. We thus strongly urge the MLLM community to prioritize enhancing cross-modality synergy capabilities to advance the development of truly comprehensive multimodal generalists. Synergy Across Comprehension and Generation. We further investigate the synergy across comprehension and generation, which represents broader type of synergy than at the task level. Using similar approach, we calculate the synergy score based on the level-4 score algorithm from General-Level. Specifically, we count the frequency of synergy occurrences for each model and use the performance increments exceeding SoTA specialists as weights. The total sum (after normalization) constitutes the synergy score across comprehension and generation. Figure 14 presents the comparisons for only 3 models. As shown, Mini-Gemini (Li et al., 2024c) demonstrates the best synergy effects at this level, securing the top rank on our General-Level leaderboard. Also, we observe that the cross-comprehension-generation synergy is only pronounced in the image modality, as these 3 models all support only image modalities."
        },
        {
            "title": "6 Discussions and Future Investigation",
            "content": "Figure 14: Synergy strength between comprehension and generation. We propose leveled evaluation of MLLMs, with hierarchical framework called General-Level, and large-scale benchmark dataset General-Bench. Yet we believe several aspects of this work can be further improved. In addition, as the next step to achieve more capable multimodal generalists toward AGI, we believe some points are worth further investigation. Further refinement of the General-Level framework. Although this framework is concrete starting point for building true multimodal generalists, there are still areas for improvement, especially in terms of the algorithms. For example, the coordination average used to compute Level-3 in the General-Level framework assumes balance between the number of comprehension and generation tasks, which is an unrealistic assumption. Additionally, we have relaxed the measurement of synergy by assuming that models synergy capability is reflected in its ability to surpass SoTA specialists in task performance, avoiding direct measurement of the synergy effect. Future work will consider optimizing this aspect to provide more robust definition. Expanding the General-Bench dataset to include more comprehensive tasks and modalities. To ensure the evaluation of multimodal generalists is complete and unbiased, the General-Bench dataset should be further expanded. Currently, the dataset is somewhat imbalanced across different modalities and tasks; for example, there is more data for image-related tasks than for audio and 3D modalities, and there are more comprehension tasks than generation tasks. 34 On Path to Multimodal Generalist: General-Level and General-Bench Moreover, multimodality should be more broadly defined to include not just visible information such as language, vision, and sound, but also other signals and types of information. For instance, LLMs reasoning abilities have been shown to be significantly enhanced through learning code. As multimodal models, it is necessary to support some coding capabilities, which, in turn, could theoretically improve the models understanding and reasoning in other modalities, such as vision. Finally, our current benchmark primarily considers tasks in which individual modality is operated in isolation. However, in reality, good multimodal generalist should be capable of modality-switching and modality-interleaved reasoning (in both comprehension and generation) under multi-turn user-machine interactions. In the future, we plan to incorporate tasks and datasets that assess interleaved modality capabilities. Rethinking Evaluation Paradigm for Model Capabilities. Many current task evaluation methodologies still follow conventional paradigms. In most cases, automatic and scalable evaluation strategies are preferred (we provide detailed overview in Appendix A.1). While such approaches may suffice for relatively straightforward taskssuch as multiplechoice or classificationthey often fall short when applied to format-free tasks, particularly those involving multimodal generation. For instance, in video or 3D generation, traditional metrics like FID or FVD are increasingly considered inadequate, as they fail to reliably capture the quality and fidelity of the generated content. Consequently, there is growing reliance on human evaluations. To improve scalability, many recent works have begun employing LLMs to simulate human-level judgment (Zheng et al., 2023a). However, this LLM-as-a-judge approach introduces challenges in terms of evaluation stability and reproducibility, which remain open research problems. In addition, our current General-Level evaluation framework adopts single primary metric per task, which may inherently introduce bias. We argue that future evaluations should incorporate multiple complementary metrics to provide more comprehensive assessment. Lastly, as multimodal generalist models continue to evolve with stronger reasoning capabilities, corresponding benchmarks should also be upgraded to evaluate the interpretability and traceability of their intermediate reasoning processes. Optimizing model architecture to support more functionalities and modalities with stronger performance. Our above experiments reveal that very few MLLMs currently achieve unified capabilities. Most models support only 1-2 modalities or abilities, which severely limits their qualifications as generalists. Some models, while supporting multiple tasks and modalities, still exhibit limited performance in individual tasks. Future research could focus on optimizing model architectures to support as many functions and modalities as possible while also delivering stronger performance. We note that simply integrating multiple models or modules using an agent-based approach can increase the number of supported functions and modalities but does not necessarily improve performance (i.e., it still cannot surpass individual specialists). more promising approach may be to leverage the Mixture of Experts (MoE) strategy to construct more unified MLLMs. Recently, more advanced understanding and generation capabilities have also been achieved through some of the latest architectures, such as those that combine autoregressive and diffusion frameworks. Strengthening synergy capabilities is the key focus. As emphasized in this work, achieving synergy is the fundamental requirement, and it is crucial for ensuring the model has more powerful capabilities (compared to specialists). To accomplish this, several aspects need to be considered. First, at the architectural level, it is necessary to design essential modules or mechanisms that allow the model to flexibly and effectively transfer features learned from different tasks and modalities (Fei et al., 2024a; Pan et al., 2025). This is key to enabling the learning by analogy capability. Second, at the learning level, to achieve stronger capabilities across multiple tasks and modalities, the model must be trained in way that prevents it from forgetting previously learned knowledge when learning new tasks. Also, recently, by incorporating training techniques such as Reinforcement Learning from Human Feedback (RLHF), models have achieved more powerful reasoning capabilities and improved generalization."
        },
        {
            "title": "7 Conclusion\nInspired by the concept of capability levels defined in the autonomous driving industry, we propose General-Level,\na framework that evaluates and categorizes the capabilities of existing MLLMs through a 5-level hierarchical rating\nmechanism based on their ability to maintain synergy across comprehension, generation, and multimodal interactions.\nGeneral-Level provides a structured methodology to assess MLLMs across diverse tasks, modalities, and synergies,\nparticularly in comprehension and generation. To support this evaluation, we further present General-Bench, a large-\nscale multimodal benchmark that spans a wide spectrum of tasks (702), modalities (language, image, video, audio, 3D, etc.),\ndomains (29), and original formats with 325,800 instances. By benchmarking over 100 popular LLMs/MLLMs, we uncover\nthe current capability limitations and provide a clear ranking of generalist performance. We hope the General-Level\nand General-Bench in this study will propel the community to develop next-generation multimodal foundation models\nto achieve more sophisticated, general-purpose multimodal intelligence.",
            "content": "35 On Path to Multimodal Generalist: General-Level and General-Bench"
        },
        {
            "title": "References",
            "content": "OpenAI. Introducing chatgpt. 2022a. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. CoRR, abs/2302.13971, 2023. Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob L. Menick, Sebastian Borgeaud, Andy Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and Karen Simonyan. Flamingo: visual language model for few-shot learning. In Proceedings of the NeurIPS, 2022. Junnan Li, Dongxu Li, Silvio Savarese, and Steven C. H. Hoi. BLIP-2: bootstrapping language-image pre-training with frozen image encoders and large language models. In Proceedings of the ICML, pages 1973019742, 2023a. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. CoRR, abs/2304.08485, 2023a. OpenAI. Gpt-4 technical report. 2022b. Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. CoRR, abs/2304.10592, 2023a. Dong Zhang, Shimin Li, Xin Zhang, Jun Zhan, Pengyu Wang, Yaqian Zhou, and Xipeng Qiu. Speechgpt: Empowering large language models with intrinsic cross-modal conversational abilities. CoRR, abs/2305.11000, 2023a. Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-Seng Chua. Next-gpt: Any-to-any multimodal llm. In Proceedings of the International Conference on Machine Learning, 2024a. Sijin Chen, Xin Chen, Chi Zhang, Mingsheng Li, Gang Yu, Hao Fei, Hongyuan Zhu, Jiayuan Fan, and Tao Chen. Ll3da: Visual interactive instruction tuning for omni-3d understanding reasoning and planning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2642826438, 2024a. Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Yueze Wang, Yongming Rao, Jingjing Liu, Tiejun Huang, and Xinlong Wang. Generative multimodal models are in-context learners. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1439814409, 2024. Zhanyu Wang, Longyue Wang, Zhen Zhao, Minghao Wu, Chenyang Lyu, Huayang Li, Deng Cai, Luping Zhou, Shuming Shi, and Zhaopeng Tu. Gpt4video: unified multimodal large language model for lnstruction-followed understanding and safety-aware generation. arXiv preprint arXiv:2311.16511, 2023a. Shehan Munasinghe, Rusiru Thushara, Muhammad Maaz, Hanoona Abdul Rasheed, Salman Khan, Mubarak Shah, and Fahad Khan. Pg-video-llava: Pixel grounding large video-language models. arXiv preprint arXiv:2311.13435, 2023. Tao Zhang, Xiangtai Li, Hao Fei, Haobo Yuan, Shengqiong Wu, Shunping Ji, Chen Change Loy, and Shuicheng Yan. Omg-llava: Bridging image-level, object-level, pixel-level reasoning and understanding. arXiv preprint arXiv:2406.19389, 2024a. Hao Fei, Shengqiong Wu, Hanwang Zhang, Tat-Seng Chua, and Shuicheng Yan. Vitron: unified pixel-level vision llm for understanding, generating, segmenting, editing. 2024a. Zhongwei Ren, Zhicheng Huang, Yunchao Wei, Yao Zhao, Dongmei Fu, Jiashi Feng, and Xiaojie Jin. Pixellm: Pixel reasoning with large multimodal model. arXiv preprint arXiv:2312.02228, 2023. Yuqian Yuan, Wentong Li, Jian Liu, Dongqi Tang, Xinjie Luo, Chi Qin, Lei Zhang, and Jianke Zhu. Osprey: Pixel understanding with visual instruction tuning. arXiv preprint arXiv:2312.10032, 2023a. Hanoona Rasheed, Muhammad Maaz, Sahal Shaji, Abdelrahman Shaker, Salman Khan, Hisham Cholakkal, Rao Anwer, Erix Xing, Ming-Hsuan Yang, and Fahad Khan. Glamm: Pixel grounding large multimodal model. arXiv preprint arXiv:2311.03356, 2023. 36 On Path to Multimodal Generalist: General-Level and General-Bench Jun Zhan, Junqi Dai, Jiasheng Ye, Yunhua Zhou, Dong Zhang, Zhigeng Liu, Xin Zhang, Ruibin Yuan, Ge Zhang, Linyang Li, et al. Anygpt: Unified multimodal llm with discrete sequence modeling. arXiv preprint arXiv:2402.12226, 2024. Jiasen Lu, Christopher Clark, Sangho Lee, Zichen Zhang, Savya Khosla, Ryan Marten, Derek Hoiem, and Aniruddha Kembhavi. Unified-io 2: Scaling autoregressive multimodal models with vision language audio and action. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2643926455, 2024a. Haoning Wu, Zicheng Zhang, Erli Zhang, Chaofeng Chen, Liang Liao, Annan Wang, Chunyi Li, Wenxiu Sun, Qiong Yan, Guangtao Zhai, et al. Q-bench: benchmark for general-purpose foundation models on low-level vision. arXiv preprint arXiv:2309.14181, 2023a. Peng Xia, Siwei Han, Shi Qiu, Yiyang Zhou, Zhaoyang Wang, Wenhao Zheng, Zhaorun Chen, Chenhang Cui, Mingyu Ding, Linjie Li, et al. Mmie: Massive multimodal interleaved comprehension benchmark for large vision-language models. arXiv preprint arXiv:2410.10139, 2024a. Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 95569567, 2024a. Fanqing Meng, Jin Wang, Chuanhao Li, Quanfeng Lu, Hao Tian, Jiaqi Liao, Xizhou Zhu, Jifeng Dai, Yu Qiao, Ping Luo, et al. Mmiu: Multimodal multi-image understanding for evaluating large vision-language models. arXiv preprint arXiv:2408.02718, 2024a. Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? In European Conference on Computer Vision, pages 216233. Springer, 2025. Bohao Li, Yuying Ge, Yixiao Ge, Guangzhi Wang, Rui Wang, Ruimao Zhang, and Ying Shan. Seed-bench: Benchmarking multimodal large language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1329913308, 2024a. Kaining Ying, Fanqing Meng, Jin Wang, Zhiqian Li, Han Lin, Yue Yang, Hao Zhang, Wenbo Zhang, Yuqi Lin, Shuo Liu, et al. Mmt-bench: comprehensive multimodal benchmark for evaluating large vision-language models towards multitask agi. arXiv preprint arXiv:2404.16006, 2024a. Jian Li, Weiheng Lu, Hao Fei, Meng Luo, Ming Dai, Min Xia, Yizhang Jin, Zhenye Gan, Ding Qi, Chaoyou Fu, Ying Tai, Wankou Yang, Yabiao Wang, and Chengjie Wang. survey on benchmarks of multimodal large language models. arXiv preprint arXiv:2408.08632, 2024b. Peng Xu, Wenqi Shao, Kaipeng Zhang, Peng Gao, Shuo Liu, Meng Lei, Fanqing Meng, Siyuan Huang, Yu Qiao, and Ping Luo. Lvlm-ehub: comprehensive evaluation benchmark for large vision-language models. arXiv preprint arXiv:2306.09265, 2023a. Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities. arXiv preprint arXiv:2308.02490, 2023. Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, Yunsheng Wu, and Rongrong Ji. Mme: comprehensive evaluation benchmark for multimodal large language models. CoRR, abs/2306.13394, 2024a. Jiacheng Chen, Tianhao Liang, Sherman Siu, Zhengqing Wang, Kai Wang, Yubo Wang, Yuansheng Ni, Wang Zhu, Ziyan Jiang, Bohan Lyu, et al. Mega-bench: Scaling multimodal evaluation to over 500 real-world tasks. arXiv preprint arXiv:2410.10563, 2024b. Ekim Yurtsever, Jacob Lambert, Alexander Carballo, and Kazuya Takeda. survey of autonomous driving: Common practices and emerging technologies. IEEE access, 8:5844358469, 2020. Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966, 2023. 37 On Path to Multimodal Generalist: General-Level and General-Bench Pan Zhang, Xiaoyi Dong, Bin Wang, Yuhang Cao, Chao Xu, Linke Ouyang, Zhiyuan Zhao, Haodong Duan, Songyang Zhang, Shuangrui Ding, et al. Internlm-xcomposer: vision-language large model for advanced text-image comprehension and composition. arXiv preprint arXiv:2309.15112, 2023b. Yang Jin, Kun Xu, Liwei Chen, Chao Liao, Jianchao Tan, Bin Chen, Chenyi Lei, An Liu, Chengru Song, Xiaoqiang Lei, et al. Unified language-vision pretraining with dynamic discrete visual tokenization. arXiv preprint arXiv:2309.04669, 2023. Yanwei Li, Yuechen Zhang, Chengyao Wang, Zhisheng Zhong, Yixin Chen, Ruihang Chu, Shaoteng Liu, and Jiaya Jia. Mini-gemini: Mining the potential of multi-modality vision language models. arXiv preprint arXiv:2403.18814, 2024c. Hao Fei, Yuan Yao, Zhuosheng Zhang, Fuxiao Liu, Ao Zhang, and Tat-Seng Chua. From multimodal llm to human-level ai: Modality, instruction, reasoning, efficiency and beyond. In Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024): Tutorial Summaries, pages 18, 2024b. Hao Fei, Shengqiong Wu, Meishan Zhang, Min Zhang, Tat-Seng Chua, and Shuicheng Yan. Enhancing video-language representations with structural spatio-temporal alignment. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024c. Aaron Van Den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, Koray Kavukcuoglu, et al. Wavenet: generative model for raw audio. arXiv preprint arXiv:1609.03499, 12, 2016. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. arXiv preprint arXiv:2303.05499, 2023b. Junnan Li, Dongxu Li, Caiming Xiong, and Steven C. H. Hoi. BLIP: bootstrapping language-image pre-training for unified vision-language understanding and generation. In Proceedings of the ICML, pages 1288812900, 2022. Black Forest Labs. Flux. https://github.com/black-forest-labs/flux, 2023. Yi Ren, Chenxu Hu, Xu Tan, Tao Qin, Sheng Zhao, Zhou Zhao, and Tie-Yan Liu. Fastspeech 2: Fast and high-quality end-to-end text to speech. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021, 2021. Yuying Ge, Sijie Zhao, Ziyun Zeng, Yixiao Ge, Chen Li, Xintao Wang, and Ying Shan. Making llama see and draw with seed tokenizer. arXiv preprint arXiv:2310.01218, 2023. Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024a. Anthropic Team. The claude 3 model family: Opus, sonnet, haiku. preprint, 2024. URL https://api. semanticscholar.org/CorpusID:268232499. Haoyu Lu, Wen Liu, Bo Zhang, Bingxuan Wang, Kai Dong, Bo Liu, Jingxiang Sun, Tongzheng Ren, Zhuoshu Li, Hao Yang, et al. Deepseek-vl: towards real-world vision-language understanding. arXiv preprint arXiv:2403.05525, 2024b. Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024d. On Path to Multimodal Generalist: General-Level and General-Bench Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024a. Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2418524198, 2024c. Marah Abdin, Jyoti Aneja, Hany Awadalla, Ahmed Awadallah, Ammar Ahmad Awan, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Jianmin Bao, Harkirat Behl, et al. Phi-3 technical report: highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219, 2024. Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan. Visual chatgpt: Talking, drawing and editing with visual foundation models. CoRR, abs/2303.04671, 2023b. Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hugginggpt: Solving AI tasks with chatgpt and its friends in huggingface. CoRR, abs/2303.17580, 2023. Shilong Liu, Hao Cheng, Haotian Liu, Hao Zhang, Feng Li, Tianhe Ren, Xueyan Zou, Jianwei Yang, Hang Su, Jun Zhu, et al. Llava-plus: Learning to use tools for creating multimodal agents. arXiv preprint arXiv:2311.05437, 2023c. Kaihang Pan, Siliang Tang, Juncheng Li, Zhaoyu Fan, Wei Chow, Shuicheng Yan, Tat-Seng Chua, Yueting Zhuang, and Hanwang Zhang. Auto-encoding morph-tokens for multimodal llm. arXiv preprint arXiv:2405.01926, 2024. Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Leonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Rame, et al. Gemma 2: Improving open language models at practical size. arXiv preprint arXiv:2408.00118, 2024b. Ben Wang and Aran Komatsuzaki. GPT-J-6B: 6 Billion Parameter Autoregressive Language Model. https://github. com/kingoflolz/mesh-transformer-jax, May 2021. Team GLM, Aohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, Da Yin, Dan Zhang, Diego Rojas, Guanyu Feng, Hanlin Zhao, et al. Chatglm: family of large language models from glm-130b to glm-4 all tools. arXiv preprint arXiv:2406.12793, 2024. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024a. Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu Chen, Xin Chen, Xun Chen, Zehui Chen, Zhi Chen, Pei Chu, et al. Internlm2 technical report. arXiv preprint arXiv:2403.17297, 2024. Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Ce Bian, Chao Yin, Chenxu Lv, Da Pan, Dian Wang, Dong Yan, et al. Baichuan 2: Open large-scale language models. arXiv preprint arXiv:2309.10305, 2023. Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph Gonzalez, et al. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. See https://vicuna. lmsys. org (accessed 14 April 2023), 2(3):6, 2023. Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, Merouane Debbah, Etienne Goffinet, Daniel Heslow, Julien Launay, Quentin Malartic, Badreddine Noune, Baptiste Pannier, and Guilherme Penedo. Falcon-40B: an open large language model with state-of-the-art performance. 2023. Albert Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024a. Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, et al. Yi: Open foundation models by 01. ai. arXiv preprint arXiv:2403.04652, 2024. 39 On Path to Multimodal Generalist: General-Level and General-Bench Zhangwei Gao, Zhe Chen, Erfei Cui, Yiming Ren, Weiyun Wang, Jinguo Zhu, Hao Tian, Shenglong Ye, Junjun He, Xizhou Zhu, et al. Mini-internvl: flexible-transfer pocket multi-modal model with 5% parameters and 90% performance. Visual Intelligence, 2(1):117, 2024. Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Bin Wang, Linke Ouyang, Xilin Wei, Songyang Zhang, Haodong Duan, Maosong Cao, et al. Internlm-xcomposer2: Mastering free-form text-image composition and comprehension in vision-language large model. arXiv preprint arXiv:2401.16420, 2024. Bin Lin, Zhenyu Tang, Yang Ye, Jiaxi Cui, Bin Zhu, Peng Jin, Junwu Zhang, Munan Ning, and Li Yuan. Moe-llava: Mixture of experts for large vision-language models. arXiv preprint arXiv:2401.15947, 2024a. Zhang Li, Biao Yang, Qiang Liu, Zhiyin Ma, Shuo Zhang, Jingxu Yang, Yabo Sun, Yuliang Liu, and Xiang Bai. Monkey: Image resolution and text label are important things for large multi-modal models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2676326773, 2024e. Qinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, Anwen Hu, Haowei Liu, Qi Qian, Ji Zhang, and Fei Huang. mplug-owl2: Revolutionizing multi-modal large language model with modality collaboration. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1304013051, 2024. Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha Akula, Jihan Yang, Shusheng Yang, Adithya Iyer, Xichen Pan, et al. Cambrian-1: fully open, vision-centric exploration of multimodal llms. arXiv preprint arXiv:2406.16860, 2024a. Renjie Pi, Jiahui Gao, Shizhe Diao, Rui Pan, Hanze Dong, Jipeng Zhang, Lewei Yao, Jianhua Han, Hang Xu, Lingpeng Kong, et al. Detgpt: Detect what you need via reasoning. arXiv preprint arXiv:2305.14167, 2023. Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, and Ziwei Liu. Otter: multi-modal model with in-context instruction tuning. arXiv preprint arXiv:2305.03726, 2023b. Ao Zhang, Liming Zhao, Chen-Wei Xie, Yun Zheng, Wei Ji, and Tat-Seng Chua. Next-chat: An lmm for chat, detection and segmentation. arXiv preprint arXiv:2311.04498, 2023c. Shilong Zhang, Peize Sun, Shoufa Chen, Min Xiao, Wenqi Shao, Wenwei Zhang, Kai Chen, and Ping Luo. Gpt4roi: Instruction tuning large language model on region-of-interest. arXiv preprint arXiv:2307.03601, 2023d. Hanoona Rasheed, Muhammad Maaz, Sahal Shaji, Abdelrahman Shaker, Salman Khan, Hisham Cholakkal, Rao Anwer, Eric Xing, Ming-Hsuan Yang, and Fahad Khan. Glamm: Pixel grounding large multimodal model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1300913018, 2024. Pravesh Agrawal, Szymon Antoniak, Emma Bou Hanna, Baptiste Bout, Devendra Chaplot, Jessica Chudnovsky, Diogo Costa, Baudouin De Monicault, Saurabh Garg, Theophile Gervet, et al. Pixtral 12b. arXiv preprint arXiv:2410.07073, 2024. Le Xue, Manli Shu, Anas Awadalla, Jun Wang, An Yan, Senthil Purushwalkam, Honglu Zhou, Viraj Prabhu, Yutong Dai, Michael Ryoo, et al. xgen-mm (blip-3): family of open large multimodal models. arXiv preprint arXiv:2408.08872, 2024. Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding and generation. arXiv preprint arXiv:2408.12528, 2024. Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan, Shu Liu, and Jiaya Jia. Lisa: Reasoning segmentation via large language model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 95799589, 2024. Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan Song, et al. Cogvlm: Visual expert for pretrained language models. arXiv preprint arXiv:2311.03079, 2023b. 40 On Path to Multimodal Generalist: General-Level and General-Bench Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v: Improving large multi-modal models with better captions. In European Conference on Computer Vision, pages 370387. Springer, 2025. Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. Glm: General language model pretraining with autoregressive blank infilling. arXiv preprint arXiv:2103.10360, 2021. Hugo Laurencon, Andres Marafioti, Victor Sanh, and Leo Tronchon. Building and better understanding vision-language In Workshop on Responsibly Building the Next Generation of Multimodal models: insights and future directions. Foundational Models, 2024. Shengding Hu, Yuge Tu, Xu Han, Chaoqun He, Ganqu Cui, Xiang Long, Zhi Zheng, Yewei Fang, Yuxiang Huang, Weilin Zhao, et al. Minicpm: Unveiling the potential of small language models with scalable training strategies. arXiv preprint arXiv:2404.06395, 2024a. Yang Jin, Kun Xu, Kun Xu, Liwei Chen, Chao Liao, Jianchao Tan, Quzhe Huang, Bin Chen, Chenyi Lei, An Liu, et al. Unified language-vision pretraining in llm with dynamic discrete visual tokenization. arxiv 2024. arXiv preprint arXiv:2309.04669. Boyang Zheng, Jinjin Gu, Shijun Li, and Chao Dong. Lm4lv: frozen large language model for low-level vision tasks. arXiv preprint arXiv:2405.15734, 2024. Yikang Zhou, Tao Zhang, Shilin Xu, Shihao Chen, Qianyu Zhou, Yunhai Tong, Shunping Ji, Jiangning Zhang, Xiangtai Li, and Lu Qi. Are they the same? exploring visual correspondence shortcomings of multimodal llms, 2025. Xidong Wang, Dingjie Song, Shunian Chen, Chen Zhang, and Benyou Wang. Longllava: Scaling multi-modal llms to 1000 images efficiently via hybrid architecture. arXiv preprint arXiv:2409.02889, 2024b. Yunfei Chu, Jin Xu, Xiaohuan Zhou, Qian Yang, Shiliang Zhang, Zhijie Yan, Chang Zhou, and Jingren Zhou. Qwen-audio: Advancing universal audio understanding via unified large-scale audio-language models. arXiv preprint arXiv:2311.07919, 2023. Yunfei Chu, Jin Xu, Qian Yang, Haojie Wei, Xipin Wei, Zhifang Guo, Yichong Leng, Yuanjun Lv, Jinzheng He, Junyang Lin, et al. Qwen2-audio technical report. arXiv preprint arXiv:2407.10759, 2024. Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, 2024a. Haobo Yuan, Xiangtai Li, Tao Zhang, Zilong Huang, Shilin Xu, Shunping Ji, Yunhai Tong, Lu Qi, Jiashi Feng, and Ming-Hsuan Yang. Sa2va: Marrying sam2 with llava for dense grounded understanding of images and videos. arXiv, 2025. Yining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng, Yilun Du, Zhenfang Chen, and Chuang Gan. 3d-llm: Injecting the 3d world into large language models. Advances in Neural Information Processing Systems, 36:2048220494, 2023. Runsen Xu, Xiaolong Wang, Tai Wang, Yilun Chen, Jiangmiao Pang, and Dahua Lin. Pointllm: Empowering large language models to understand point clouds. In European Conference on Computer Vision, pages 131147. Springer, 2025. Ziyu Zhu, Xiaojian Ma, Yixin Chen, Zhidong Deng, Siyuan Huang, and Qing Li. 3d-vista: Pre-trained transformer for 3d vision and text alignment. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 29112921, 2023b. Zixiang Zhou, Yu Wan, and Baoyuan Wang. Avatargpt: All-in-one framework for motion understanding planning generation and beyond. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13571366, 2024a. Biao Jiang, Xin Chen, Wen Liu, Jingyi Yu, Gang Yu, and Tao Chen. Motiongpt: Human motion as foreign language. Advances in Neural Information Processing Systems, 36, 2024b. 41 On Path to Multimodal Generalist: General-Level and General-Bench Yaqi Zhang, Di Huang, Bin Liu, Shixiang Tang, Yan Lu, Lu Chen, Lei Bai, Qi Chu, Nenghai Yu, and Wanli Ouyang. Motiongpt: Finetuned llms are general-purpose motion generators. arXiv preprint arXiv:2306.10900, 2023e. Sreyan Ghosh, Sonal Kumar, Ashish Seth, Chandra Kiran Reddy Evuru, Utkarsh Tyagi, Sakshi, Oriol Nieto, Ramani Duraiswami, and Dinesh Manocha. GAMA: large audio-language model with advanced audio understanding and In Proceedings of the 2024 Conference on Empirical Methods in Natural Language complex reasoning abilities. Processing, pages 62886313, 2024. Soham Deshmukh, Benjamin Elizalde, Rita Singh, and Huaming Wang. Pengi: An audio language model for audio tasks. Advances in Neural Information Processing Systems, 36:1809018108, 2023. Shujie Hu, Long Zhou, Shujie Liu, Sanyuan Chen, Lingwei Meng, Hongkun Hao, Jing Pan, Xunying Liu, Jinyu Li, Sunit Sivasankaran, et al. Wavllm: Towards robust and adaptive speech large language model. arXiv preprint arXiv:2404.00656, 2024b. Changli Tang, Wenyi Yu, Guangzhi Sun, Xianzhao Chen, Tian Tan, Wei Li, Lu Lu, Zejun Ma, and Chao Zhang. Salmonn: Towards generic hearing abilities for large language models. arXiv preprint arXiv:2310.13289, 2023. Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xuankai Chang, Zhenhui Ye, Yuning Wu, Zhiqing Hong, Jiawei Huang, Jinglin Liu, Yi Ren, Zhou Zhao, and Shinji Watanabe. Audiogpt: Understanding and generating speech, music, sound, and talking head. CoRR, abs/2304.12995, 2023a. Yixuan Su, Tian Lan, Huayang Li, Jialu Xu, Yan Wang, and Deng Cai. Pandagpt: One model to instruction-follow them all. CoRR, abs/2305.16355, 2023. Jiaming Han, Renrui Zhang, Wenqi Shao, Peng Gao, Peng Xu, Han Xiao, Kaipeng Zhang, Chris Liu, Song Wen, Ziyu Guo, et al. Imagebind-llm: Multi-modality instruction tuning. arXiv preprint arXiv:2309.03905, 2023. Xinyu Wang, Bohan Zhuang, and Qi Wu. Modaverse: Efficiently transforming modalities with llms. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2660626616, 2024c. Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. Cogvideo: Large-scale pretraining for text-to-video generation via transformers. CoRR, abs/2205.15868, 2022. Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. Seed-bench: Benchmarking multimodal llms with generative comprehension. CoRR, abs/2307.16125, 2023c. Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, Kai Chen, and Dahua Lin. Mmbench: Is your multi-modal model an all-around player? In Computer Vision - ECCV 2024 - 18th European Conference, Milan, Italy, September 29-October 4, 2024, Proceedings, Part VI, pages 216233, 2024b. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36:4659546623, 2023a. Xichen Pan, Satya Narayan Shukla, Aashu Singh, Zhuokai Zhao, Shlok Kumar Mishra, Jialiang Wang, Zhiyang Xu, Jiuhai Chen, Kunpeng Li, Felix Juefei-Xu, et al. Transfer between modalities with metaqueries. arXiv preprint arXiv:2504.06256, 2025. Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. Advances in Neural Information Processing Systems, 35:25072521, 2022a. Jingkun Ma, Runzhe Zhan, Derek Wong, Yang Li, Di Sun, Hou Pong Chan, and Lidia Chao. Visaidmath: Benchmarking visual-aided mathematical reasoning. arXiv preprint arXiv:2410.22995, 2024. Baiqi Li, Zhiqiu Lin, Wenxuan Peng, Jean de Dieu Nyandwi, Daniel Jiang, Zixian Ma, Simran Khanuja, Ranjay Krishna, Graham Neubig, and Deva Ramanan. Naturalbench: Evaluating vision-language models on natural adversarial samples. arXiv preprint arXiv:2410.14669, 2024f. On Path to Multimodal Generalist: General-Level and General-Bench Bin Wang, Xunlong Zou, Geyu Lin, Shuo Sun, Zhuohan Liu, Wenyu Zhang, Zhengyuan Liu, AiTi Aw, and Nancy F. Chen. Audiobench: universal benchmark for audio large language models. CoRR, abs/2406.16020, 2024d. Ruibin Yuan, Yinghao Ma, Yizhi Li, Ge Zhang, Xingran Chen, Hanzhi Yin, Le Zhuo, Yiqi Liu, Jiawen Huang, Zeyue Tian, Binyue Deng, Ningzhi Wang, Chenghua Lin, Emmanouil Benetos, Anton Ragni, Norbert Gyenge, Roger B. Dannenberg, Wenhu Chen, Gus Xia, Wei Xue, Si Liu, Shi Wang, Ruibo Liu, Yike Guo, and Jie Fu. MARBLE: music audio representation benchmark for universal evaluation. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023b. S. Sakshi, Utkarsh Tyagi, Sonal Kumar, Ashish Seth, Ramaneswaran Selvakumar, Oriol Nieto, Ramani Duraiswami, Sreyan Ghosh, and Dinesh Manocha. MMAU: massive multi-task audio understanding and reasoning benchmark. CoRR, abs/2410.19168, 2024. Yuze He, Yushi Bai, Matthieu Lin, Wang Zhao, Yubin Hu, Jenny Sheng, Ran Yi, Juanzi Li, and Yong-Jin Liu. T3bench: Benchmarking current progress in text-to-3d generation. CoRR, abs/2310.02977, 2023. Junjie Zhang, Tianci Hu, Xiaoshui Huang, Yongshun Gong, and Dan Zeng. 3dbench: scalable 3d benchmark and instruction-tuning dataset. In Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence, IJCAI 2024, Jeju, South Korea, August 3-9, 2024, pages 17061714, 2024b. Munan Ning, Bin Zhu, Yujia Xie, Bin Lin, Jiaxi Cui, Lu Yuan, Dongdong Chen, and Li Yuan. Video-bench: comprehensive benchmark and toolkit for evaluating video-based large language models. CoRR, abs/2311.16103, 2023. Chaoyou Fu, Yuhan Dai, Yondong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, Peixian Chen, Yanwei Li, Shaohui Lin, Sirui Zhao, Ke Li, Tong Xu, Xiawu Zheng, Enhong Chen, Rongrong Ji, and Xing Sun. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. CoRR, abs/2405.21075, 2024b. Junjie Zhou, Yan Shu, Bo Zhao, Boya Wu, Shitao Xiao, Xi Yang, Yongping Xiong, Bo Zhang, Tiejun Huang, and Zheng Liu. MLVU: comprehensive benchmark for multi-task long video understanding. CoRR, abs/2406.04264, 2024b. Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Lou, Limin Wang, and Yu Qiao. Mvbench: comprehensive multi-modal video understanding benchmark. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2024, Seattle, WA, USA, June 16-22, 2024, pages 2219522206, 2024g. Xiuyuan Chen, Yuan Lin, Yuchen Zhang, and Weiran Huang. Autoeval-video: An automatic benchmark for assessing large vision language models in open-ended video question answering. In Computer Vision - ECCV 2024 - 18th European Conference, Milan, Italy, September 29-October 4, 2024, Proceedings, Part LXVIII, pages 179195, 2024d. Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, Yaohui Wang, Xinyuan Chen, Limin Wang, Dahua Lin, Yu Qiao, and Ziwei Liu. Vbench: Comprehensive benchmark suite for video generative models. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2024, Seattle, WA, USA, June 16-22, 2024, pages 2180721818, 2024. Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Zhenyu Qiu, Wei Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, and Rongrong Ji. MME: comprehensive evaluation benchmark for multimodal large language models. CoRR, abs/2306.13394, 2023. Peng Xu, Wenqi Shao, Kaipeng Zhang, Peng Gao, Shuo Liu, Meng Lei, Fanqing Meng, Siyuan Huang, Yu Qiao, and Ping Luo. Lvlm-ehub: comprehensive evaluation benchmark for large vision-language models. CoRR, abs/2306.09265, 2023b. Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024, 2024. Penghao Wu and Saining Xie. V*: Guided visual search as core mechanism in multimodal llms. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2024, Seattle, WA, USA, June 16-22, 2024, pages 1308413094, 2024. 43 On Path to Multimodal Generalist: General-Level and General-Bench Peng Xia, Siwei Han, Shi Qiu, Yiyang Zhou, Zhaoyang Wang, Wenhao Zheng, Zhaorun Chen, Chenhang Cui, Mingyu Ding, Linjie Li, Lijuan Wang, and Huaxiu Yao. MMIE: massive multimodal interleaved comprehension benchmark for large vision-language models. CoRR, abs/2410.10139, 2024b. Yusu Qian, Hanrong Ye, Jean-Philippe Fauconnier, Peter Grasch, Yinfei Yang, and Zhe Gan. Mia-bench: Towards better instruction following evaluation of multimodal llms. CoRR, abs/2407.01509, 2024. Yifan Zhang, Huanyu Zhang, Haochen Tian, Chaoyou Fu, Shuangqing Zhang, Junfei Wu, Feng Li, Kun Wang, Qingsong Wen, Zhang Zhang, Liang Wang, Rong Jin, and Tieniu Tan. Mme-realworld: Could your multimodal LLM challenge high-resolution real-world scenarios that are difficult for humans? CoRR, abs/2408.13257, 2024c. Wentao Ge, Shunian Chen, Guiming Hardy Chen, Junying Chen, Zhihong Chen, Nuo Chen, Wenya Xie, Shuo Yan, Chenghao Zhu, Ziyue Lin, Song Dingjie, Xidong Wang, Anningzhe Gao, Zhang Zhiyi, Jianquan Li, Xiang Wan, and Benyou Wang. Mllm-bench: Evaluating multimodal llms with per-sample criteria, 2024a. Haoning Wu, Zicheng Zhang, Erli Zhang, Chaofeng Chen, Liang Liao, Annan Wang, Chunyi Li, Wenxiu Sun, Qiong Yan, Guangtao Zhai, and Weisi Lin. Q-bench: benchmark for general-purpose foundation models on low-level vision. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024, 2024b. Fei Wang, Xingyu Fu, James Y. Huang, Zekun Li, Qin Liu, Xiaogeng Liu, Mingyu Derek Ma, Nan Xu, Wenxuan Zhou, Kai Zhang, Tianyi Lorena Yan, Wenjie Jacky Mo, Hsiang-Hui Liu, Pan Lu, Chunyuan Li, Chaowei Xiao, Kai-Wei Chang, Dan Roth, Sheng Zhang, Hoifung Poon, and Muhao Chen. Muirbench: comprehensive benchmark for robust multi-image understanding. CoRR, abs/2406.09411, 2024e. Dingjie Song, Shunian Chen, Guiming Hardy Chen, Fei Yu, Xiang Wan, and Benyou Wang. Milebench: Benchmarking mllms in long context. CoRR, abs/2404.18532, 2024. Bohao Li, Yuying Ge, Yixiao Ge, Guangzhi Wang, Rui Wang, Ruimao Zhang, and Ying Shan. Seed-bench-2: Benchmarking multimodal large language models. CoRR, abs/2311.17092, 2023d. Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha Akula, Jihan Yang, Shusheng Yang, Adithya Iyer, Xichen Pan, Austin Wang, Rob Fergus, Yann LeCun, and Saining Xie. Cambrian-1: fully open, vision-centric exploration of multimodal llms. CoRR, abs/2406.16860, 2024b. Fanqing Meng, Jin Wang, Chuanhao Li, Quanfeng Lu, Hao Tian, Jiaqi Liao, Xizhou Zhu, Jifeng Dai, Yu Qiao, Ping Luo, Kaipeng Zhang, and Wenqi Shao. MMIU: multimodal multi-image understanding for evaluating large vision-language models. CoRR, abs/2408.02718, 2024b. Kaining Ying, Fanqing Meng, Jin Wang, Zhiqian Li, Han Lin, Yue Yang, Hao Zhang, Wenbo Zhang, Yuqi Lin, Shuo Liu, Jiayi Lei, Quanfeng Lu, Runjian Chen, Peng Xu, Renrui Zhang, Haozhe Zhang, Peng Gao, Yali Wang, Yu Qiao, Ping Luo, Kaipeng Zhang, and Wenqi Shao. Mmt-bench: comprehensive multimodal benchmark for evaluating large vision-language models towards multitask AGI. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024, 2024b. Jiacheng Chen, Tianhao Liang, Sherman Siu, Zhengqing Wang, Kai Wang, Yubo Wang, Yuansheng Ni, Wang Zhu, Ziyan Jiang, Bohan Lyu, Dongfu Jiang, Xuan He, Yuan Liu, Hexiang Hu, Xiang Yue, and Wenhu Chen. Mega-bench: Scaling multimodal evaluation to over 500 real-world tasks. CoRR, abs/2410.10563, 2024e. Anomaly detection for atm booths, 2023. URL https://www.kaggle.com/datasets/ashlinfurtado/ atm-positions. Human action recognition (har) dataset, a. URL https://www.kaggle.com/datasets/meetnagadia/ human-action-recognition-har-dataset. 100 sports image classification."
        },
        {
            "title": "URL",
            "content": "https://www.kaggle.com/datasets/gpiosenka/ sports-classification. Alessandro Conti, Enrico Fini, Massimiliano Mancini, Paolo Rota, Yiming Wang, and Elisa Ricci. Vocabulary-free image classification, 2023. 44 On Path to Multimodal Generalist: General-Level and General-Bench Sketch2code. URL https://www.kaggle.com/datasets/vshantam/sketch2code. Ashwin Kumar. Generating html code from hand-drawn wireframe. URL https://github.com/ashnkumar/ sketch-code. Joshua Siegel. Oxidized and non-oxidized tire sidewall and tread images. 2021. doi: 10.7910/DVN/Z3ZYLI. URL https://doi.org/10.7910/DVN/Z3ZYLI. Yong Shi, Limeng Cui, Zhiquan Qi, Fan Meng, and Zhensong Chen. Automatic road crack detection using random structured forests. IEEE Transactions on Intelligent Transportation Systems, 17(12):34343445, 2016. Matthias Minderer, Alexey A. Gritsenko, Austin Stone, Maxim Neumann, Dirk Weissenborn, Alexey Dosovitskiy, Aravindh Mahendran, Anurag Arnab, Mostafa Dehghani, Zhuoran Shen, Xiao Wang, Xiaohua Zhai, Thomas Kipf, and Neil Houlsby. Simple open-vocabulary object detection with vision transformers. CoRR, abs/2205.06230, 2022a. Beans, Tomato artificial-intelligence-82oex/detecting-diseases."
        },
        {
            "title": "Strawberry",
            "content": "diseases."
        },
        {
            "title": "URL",
            "content": "and https://universe.roboflow.com/ Liunian Harold Li*, Pengchuan Zhang*, Haotian Zhang*, Jianwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, Kai-Wei Chang, and Jianfeng Gao. Grounded language-image pre-training. In CVPR, 2022."
        },
        {
            "title": "Guava",
            "content": "fruit dataset. guava-disease-dataset. disease URL https://www.kaggle.com/datasets/asadullahgalib/ National heart foundation 2023 ecg dataset. URL https://www.kaggle.com/datasets/drkhaledmohsin/ national-heart-foundation-2023-ecg-dataset."
        },
        {
            "title": "Brain",
            "content": "tumor mri dataset. URL https://www.kaggle.com/datasets/masoudnickparvar/ brain-tumor-mri-dataset."
        },
        {
            "title": "Pumpkin",
            "content": "dataset. pumpkin-leaf-diseases-dataset-from-bangladesh. diseases"
        },
        {
            "title": "URL",
            "content": "leaf https://www.kaggle.com/datasets/tahmidmir/ Mourya S., S. Kant, Kumar P., Gupta A., and & Gupta R. All challenge dataset of isbi 2019 (c-nmc 2019) (version 1). URL https://www.cancerimagingarchive.net/collection/c-nmc-2019/. Rub`en Tito, Dimosthenis Karatzas, and Ernest Valveny. Hierarchical multimodal transformers for multi-page docvqa. CoRR, abs/2212.05935, 2022. Jiabo Ye, Anwen Hu, Haiyang Xu, Qinghao Ye, Ming Yan, Yuhao Dan, Chenlin Zhao, Guohai Xu, Chenliang Li, Junfeng Tian, Qian Qi, Ji Zhang, and Fei Huang. mplug-docowl: Modularized multimodal large language model for document understanding. CoRR, abs/2307.02499, 2023a. Geewook Kim, Teakgyu Hong, Moonbin Yim, JeongYeon Nam, Jinyoung Park, Jinyeong Yim, Wonseok Hwang, Sangdoo Yun, Dongyoon Han, and Seunghyun Park. Ocr-free document understanding transformer. In European Conference on Computer Vision (ECCV), 2022. Docquery: Document query engine powered by large language models. URL https://github.com/impira/ docquery. Emotion detection. URL https://www.kaggle.com/datasets/ananthu017/emotion-detection-fer. Sefik Ilkin Serengil and Alper Ozpinar. Lightface: hybrid deep face recognition framework. In 2020 Innovations in Intelligent Systems and Applications Conference (ASYU), pages 2327. IEEE, 2020. doi: 10.1109/ASYU50717.2020. 9259802. URL https://ieeexplore.ieee.org/document/9259802. Pets facial expression image dataset. pets-facial-expression-dataset. URL https://www.kaggle.com/datasets/anshtanwar/ 45 On Path to Multimodal Generalist: General-Level and General-Bench Facial emotion recognition image dataset, a. URL https://www.kaggle.com/datasets/sujaykapadnis/ emotion-recognition-dataset. Saining Zhang, Yuhang Zhang, Ye Zhang, Yufei Wang, and Zhigang Song. dual-direction attention mixed feature network for facial expression recognition. Electronics, 12, 2023f."
        },
        {
            "title": "Emergency",
            "content": "vehicle siren sounds. URL https://www.kaggle.com/datasets/vishnu0399/ emergency-vehicle-siren-sounds. Dongfu Jiang, Xuan He, Huaye Zeng, Cong Wei, Max W.F. Ku, Qian Liu, and Wenhu Chen. Mantis: Interleaved multi-image instruction tuning. Transactions on Machine Learning Research, 2024, 2024c. URL https://openreview.net/ forum?id=skLtdUVaJa. Jiazhen Liu, Yuhan Fu, Ruobing Xie, Runquan Xie, Xingwu Sun, Fengzong Lian, Zhanhui Kang, and Xirong Li. Phd: prompted visual hallucination evaluation dataset, 2024c. Jiaxin Zhang, Zhuohang Li, Kamalika Das, Bradley Malin, and Sricharan Kumar. Sac3: Reliable hallucination detection in black-box language models via semantic-aware cross-check consistency. arXiv preprint arXiv:2311.01740, 2023g. Instagram images with captions. URL https://www.kaggle.com/datasets/prithvijaunjale/ instagram-images-with-captions. Van-Quang Nguyen, Masanori Suganuma, and Takayuki Okatani. Grit: Faster and better image captioning transformer using dual visual features. In Computer VisionECCV 2022: 17th European Conference, Tel Aviv, Israel, October 2327, 2022, Proceedings, Part XXXVI, pages 167184, 2022. Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and C. Lawrence Zitnick. Microsoft COCO: common objects in context. In Computer Vision - ECCV 2014 - 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V, pages 740755, 2014. Xiaoqiang Lu, Binqiang Wang, Xiangtao Zheng, and Xuelong Li. Exploring models and data for remote sensing image caption generation. IEEE Trans. Geosci. Remote. Sens., 56(4):21832195, 2018. Human related image captioning, b. URL https://freerangestock.com/."
        },
        {
            "title": "Chinese",
            "content": "image captioning, a."
        },
        {
            "title": "URL",
            "content": "https://www.kaggle.com/datasets/allanyiinai/ chineseimagecaptioning/data. Mohammed Talha Alam, Raza Imam, Mohsen Guizani, and Fakhri Karray. FLARE up your data: Diffusion-based augmentation method in astronomical imaging. CoRR, abs/2405.13267, 2024. Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2024, Seattle, WA, USA, June 16-22, 2024, pages 2628626296, 2024d. Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob Fergus. Indoor segmentation and support inference from RGBD images. In Computer Vision - ECCV 2012 - 12th European Conference on Computer Vision, Florence, Italy, October 7-13, 2012, Proceedings, Part V, pages 746760, 2012. Shariq Farooq Bhat, Ibraheem Alhashim, and Peter Wonka. Adabins: Depth estimation using adaptive bins. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual, June 19-25, 2021, pages 40094018, 2021. Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding. In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016, pages 32133223, 2016. Chufeng Tang, Hang Chen, Xiao Li, Jianmin Li, Zhaoxiang Zhang, and Xiaolin Hu. Look closer to segment better: Boundary patch refinement for instance segmentation. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual, June 19-25, 2021, pages 1392613935, 2021. 46 On Path to Multimodal Generalist: General-Level and General-Bench"
        },
        {
            "title": "Handwriting",
            "content": "recognition handwriting-recognitionocr. (ocr)."
        },
        {
            "title": "URL",
            "content": "https://www.kaggle.com/datasets/ssarkar445/ Tesseract open source ocr engine (main repository). URL https://github.com/tesseract-ocr/tesseract."
        },
        {
            "title": "Standard",
            "content": "ocr dataset, a. standard-ocr-dataset."
        },
        {
            "title": "URL",
            "content": "https://www.kaggle.com/datasets/preatcher/"
        },
        {
            "title": "Car",
            "content": "license plate detection, a."
        },
        {
            "title": "URL",
            "content": "https://www.kaggle.com/datasets/andrewmvd/ car-plate-detection. Radical-level oracle bone character dataset, a. URL https://www.kaggle.com/datasets/ycfanglab/ radical-level-oracle-bone-character-dataset."
        },
        {
            "title": "Acne",
            "content": "recognition dataset. acne-computer-vision."
        },
        {
            "title": "URL",
            "content": "https://www.kaggle.com/datasets/imtkaggleteam/ Latex ocr. URL https://github.com/lukas-blecher/LaTeX-OCR. Lukas Blecher. Using vit to convert images of equations into latex code. URL https://github.com/ lukas-blecher/LaTeX-OCR."
        },
        {
            "title": "Segmented",
            "content": "bob ross images. URL https://www.kaggle.com/datasets/residentmario/ segmented-bob-ross-images. Timo Luddecke and Alexander Ecker. Image segmentation using text and image prompts. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 70867096, 2022. Xiaoxue Chen, Tianyu Liu, Hao Zhao, Guyue Zhou, and Ya-Qin Zhang. Cerberus transformer: Joint semantic, affordance and attribute parsing. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022, pages 1961719626, 2022a. Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig Adam. Encoder-decoder with atrous separable convolution for semantic image segmentation. In ECCV, 2018. Wei Yang, Ping Luo, and Liang Lin. Clothing co-parsing by joint image segmentation and labeling. In Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on, 2013."
        },
        {
            "title": "Flood",
            "content": "area segmentation. flood-area-segmentation."
        },
        {
            "title": "URL",
            "content": "https://www.kaggle.com/datasets/faizalkarim/ Ilke Demir, Krzysztof Koperski, David Lindenbaum, Guan Pang, Jing Huang, Saikat Basu, Forest Hughes, Devis Tuia, and Ramesh Raskar. Deepglobe 2018: challenge to parse the earth through satellite images. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, 2018. Licheng Yu, Patrick Poirson, Shan Yang, Alexander C. Berg, and Tamara L. Berg. Modeling context in referring expressions. In Computer Vision - ECCV 2016 - 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part II, pages 6985, 2016. Jiang Liu, Hui Ding, Zhaowei Cai, Yuting Zhang, Ravi Kumar Satzoda, Vijay Mahadevan, and Manmatha. Polyformer: Referring image segmentation as sequential polygon generation. In CVPR, 2023d. Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and Tamara L. Berg. Referitgame: Referring to objects in photographs of natural scenes. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, EMNLP 2014, October 25-29, 2014, Doha, Qatar, meeting of SIGDAT, Special Interest Group of the ACL, pages 787798, 2014. Danna Gurari, Qing Li, Abigale J. Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo, and Jeffrey P. Bigham. Vizwiz grand challenge: Answering visual questions from blind people. In 2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018, pages 36083617, 2018. 47 On Path to Multimodal Generalist: General-Level and General-Bench Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, and Lijuan Wang. GIT: generative image-to-text transformer for vision and language. Trans. Mach. Learn. Res., 2022a. Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. Ok-vqa: visual question answering benchmark requiring external knowledge. In Conference on Computer Vision and Pattern Recognition (CVPR), 2019. Medical visual question answering. URL https://www.kaggle.com/datasets/mitanshuchakrawarty/ medical-visual-question-answering. Xiaoman Zhang, Chaoyi Wu, Ziheng Zhao, Weixiong Lin, Ya Zhang, Yanfeng Wang, and Weidi Xie. Pmc-vqa: Visual instruction tuning for medical visual question answering. arXiv preprint arXiv:2305.10415, 2023h. Drew A. Hudson and Christopher D. Manning. GQA: new dataset for real-world visual reasoning and compositional question answering. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019, pages 67006709, 2019. Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. In Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022b. Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, George Karypis, and Alex Smola. Multimodal chain-of-thought reasoning in language models. Trans. Mach. Learn. Res., 2024d. Blood cell images. URL https://www.kaggle.com/datasets/paultimothymooney/blood-cells. Cataract dataset. URL https://www.kaggle.com/datasets/jr2ngb/cataractdataset. Pan Lu, Ran Gong, Shibiao Jiang, Liang Qiu, Siyuan Huang, Xiaodan Liang, and Song-Chun Zhu. Inter-gps: Interpretable geometry problem solving with formal language and symbolic reasoning. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021, pages 67746786, 2021a."
        },
        {
            "title": "Lung",
            "content": "cancer cam attempting, a."
        },
        {
            "title": "URL",
            "content": "https://www.kaggle.com/code/zaibunnisaa/ lung-cancer-cam-attempting-92-accuracy-ver5/input. Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. In International Conference on Learning Representations (ICLR), 2024c. Tianshuo Peng, Mingsheng Li, Hongbin Zhou, Renqiu Xia, Renrui Zhang, Lei Bai, Song Mao, Bin Wang, Conghui He, Aojun Zhou, Botian Shi, Tao Chen, Bo Zhang, and Xiangyu Yue. Chimera: Improving generalist model with domain-specific experts, 2024. Linda Wang, Zhong Qiu Lin, and Alexander Wong. Covid-net: tailored deep convolutional neural network design for detection of covid-19 cases from chest x-ray images. Scientific Reports, 10(1):19549, Nov 2020a. Junjue Wang, Zhuo Zheng, Zihang Chen, Ailong Ma, and Yanfei Zhong. Earthvqa: Towards queryable earth via relational reasoning-based remote sensing visual question answering. In Thirty-Eighth AAAI Conference on Artificial Intelligence, AAAI 2024, Thirty-Sixth Conference on Innovative Applications of Artificial Intelligence, IAAI 2024, Fourteenth Symposium on Educational Advances in Artificial Intelligence, EAAI 2014, February 20-27, 2024, Vancouver, Canada, pages 54815489, 2024f. Sylvain Lobry, Diego Marcos, Jesse Murray, and Devis Tuia. RSVQA: visual question answering for remote sensing data. IEEE Trans. Geosci. Remote. Sens., 58(12):85558566, 2020."
        },
        {
            "title": "Skin",
            "content": "cancer dateset. skin-cancer-malignant-vs-benign."
        },
        {
            "title": "URL",
            "content": "https://www.kaggle.com/datasets/fanconic/ 48 On Path to Multimodal Generalist: General-Level and General-Bench Yingshan Chang, Guihong Cao, Mridu Narang, Jianfeng Gao, Hisami Suzuki, and Yonatan Bisk. Webqa: Multihop and multimodal QA. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022, pages 1647416483, 2022. Jingyu Wei, Yi Su, Kele Xu, Lingbin Zeng, Bo Liu, and Huaimin Wang. Demonstrative instruction following in multimodal In Proceedings of the 32nd ACM International llms via integrating low-rank adaptation with ensemble learning. Conference on Multimedia, MM 2024, Melbourne, VIC, Australia, 28 October 2024 - 1 November 2024, pages 11435 11441, 2024. Yongqi Li, Wenjie Li, and Liqiang Nie. Mmcoqa: Conversational question answering over text, tables, and images. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022, pages 42204231, 2022a. Feng Li, Renrui Zhang, Hao Zhang, Yuanhan Zhang, Bo Li, Wei Li, Zejun Ma, and Chunyuan Li. Llava-next-interleave: Tackling multi-image, video, and 3d in large multimodal models. arXiv preprint arXiv:2407.07895, 2024h. Juncheng Li, Kaihang Pan, Zhiqi Ge, Minghe Gao, Wei Ji, Wenqiao Zhang, Tat-Seng Chua, Siliang Tang, Hanwang Zhang, and Yueting Zhuang. Fine-tuning multimodal llms to follow zero-shot demonstrative instructions. In The Twelfth International Conference on Learning Representations, 2023e. Ryota Tanaka, Kyosuke Nishida, Kosuke Nishida, Taku Hasegawa, Itsumi Saito, and Kuniko Saito. Slidevqa: dataset for document visual question answering on multiple images. In Thirty-Seventh AAAI Conference on Artificial Intelligence, AAAI 2023, Thirty-Fifth Conference on Innovative Applications of Artificial Intelligence, IAAI 2023, Thirteenth Symposium on Educational Advances in Artificial Intelligence, EAAI 2023, Washington, DC, USA, February 7-14, 2023, pages 1363613645, 2023. Minesh Mathew, Dimosthenis Karatzas, and C. V. Jawahar. Docvqa: dataset for VQA on document images. In IEEE Winter Conference on Applications of Computer Vision, WACV 2021, Waikoloa, HI, USA, January 3-8, 2021, pages 21992208, 2021. Aniruddha Kembhavi, Min Joon Seo, Dustin Schwenk, Jonghyun Choi, Ali Farhadi, and Hannaneh Hajishirzi. Are you smarter than sixth grader? textbook question answering for multimodal machine comprehension. In 2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017, pages 53765384, 2017. Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and Anirban Chakraborty. OCR-VQA: visual question answering by reading text in images. In 2019 International Conference on Document Analysis and Recognition, ICDAR 2019, Sydney, Australia, September 20-25, 2019, pages 947952, 2019. Li Jiaqing. Handwritten formula vqa. URL https://huggingface.co/datasets/Kitajiang/test2_ CROHME2016. BitMind AI. Face comparison visual question answering. URL https://huggingface.co/datasets/bitmind/ lfw. Anwen Hu, Haiyang Xu, Liang Zhang, Jiabo Ye, Ming Yan, Ji Zhang, Qin Jin, Fei Huang, and Jingren Zhou. mplug-docowl2: High-resolution compressing for ocr-free multi-page document understanding. CoRR, abs/2409.03420, 2024c. Amir Rosenfeld, Markus D. Solbach, and John K. Tsotsos. Totally looks like - how humans compare, compared to machines. In Computer Vision - ACCV 2018 - 14th Asian Conference on Computer Vision, Perth, Australia, December 2-6, 2018, Revised Selected Papers, Part I, pages 282297, 2018. Noa Garcia, Chentao Ye, Zihua Liu, Qingtao Hu, Mayu Otani, Chenhui Chu, Yuta Nakashima, and Teruko Mitamura. dataset and baselines for visual question answering on art. In Proceedings of the European Conference in Computer Vision Workshops, 2020. Chenliang Li, Haiyang Xu, Junfeng Tian, Wei Wang, Ming Yan, Bin Bi, Jiabo Ye, He Chen, Guohai Xu, Zheng Cao, Ji Zhang, Songfang Huang, Fei Huang, Jingren Zhou, and Luo Si. mplug: Effective and efficient vision-language learning by cross-modal skip-connections. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, pages 72417259, 2022b. 49 On Path to Multimodal Generalist: General-Level and General-Bench Chenhao Zhang, Xi Feng, Yuelin Bai, Xinrun Du, Jinchang Hou, Kaixin Deng, Guangzeng Han, Qinrui Li, Bingli Wang, Jiaheng Liu, Xingwei Qu, Yifei Zhang, Qixuan Zhao, Yiming Liang, Ziqiang Liu, Feiteng Fang, Min Yang, Wenhao Huang, Chenghua Lin, Ge Zhang, and Shiwen Ni. Can mllms understand the deep implication behind chinese images? CoRR, abs/2410.13854, 2024e. Zhen Han, Chaojie Mao, Zeyinzi Jiang, Yulin Pan, and Jingfeng Zhang. Stylebooth: Image style editing with multimodal instruction. CoRR, abs/2404.12154, 2024. Paul Bergmann, Kilian Batzner, Michael Fauser, David Sattlegger, and Carsten Steger. The mvtec anomaly detection dataset: comprehensive real-world dataset for unsupervised anomaly detection. Int. J. Comput. Vis., 129(4):10381059, 2021. Xuan Zhang, Shiyu Li, Xi Li, Ping Huang, Jiulong Shan, and Ting Chen. Destseg: Segmentation guided denoising student-teacher for anomaly detection. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2023, Vancouver, BC, Canada, June 17-24, 2023, pages 39143923, 2023i. Karel Horak, Simon Bilik, Ondrej Bostik, Lukas Kratochvila, and Tomas Zemcik. Industry biscuit (cookie) dataset, 2022. URL https://www.kaggle.com/dsv/4311115. Yang Zou, Jongheon Jeong, Latha Pemula, Dongqing Zhang, and Onkar Dabeer. Spot-the-difference self-supervised pre-training for anomaly detection and segmentation. In Computer Vision - ECCV 2022 - 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part XXX, pages 392408, 2022."
        },
        {
            "title": "Marble",
            "content": "surface anomaly detection. URL https://www.kaggle.com/datasets/wardaddy24/ marble-surface-anomaly-detection. Covid-19 ct scan lesion segmentation dataset, a. URL https://www.kaggle.com/datasets/maedemaftouni/ covid19-ct-scan-lesion-segmentation-dataset?select=masks. Pc5 3 - mia class ortsu, b. URL https://www.kaggle.com/code/aryan6043/pc5-3-mia-class-ortsu. Hubmap - organ segmentation competition. URL https://www.kaggle.com/datasets/manojprabhaakr/ hubmap-organ-512512."
        },
        {
            "title": "Lung mask",
            "content": "image lung-mask-image-dataset. dataset, b."
        },
        {
            "title": "URL",
            "content": "https://www.kaggle.com/datasets/newra008/ Bacteria detection with darkfield microscopy dataset for spirochaeta segmentation with image and manually anURL https://tianchi.aliyun.com/dataset/94411?spm=a2c22.28136470.J_ notated masks. 3941670930.9.2fe04a0aRphTJa&from=search. Mateusz Buda, Ashirbani Saha, and Maciej A. Mazurowski. Association of genomic subtypes of lower-grade gliomas with shape features automatically extracted by deep learning algorithm. Comput. Biol. Medicine, 109:218225, 2019a. Mateusz Buda, Ashirbani Saha, and Maciej Mazurowski. Association of genomic subtypes of lower-grade gliomas with shape features automatically extracted by deep learning algorithm. Computers in Biology and Medicine, 109, 2019b. Chest x-ray dataset for tuberculosis segmentation. URL https://www.kaggle.com/datasets/iamtapendu/ chest-x-ray-lungs-segmentation. Song-Hai Zhang, Ruilong Li, Xin Dong, Paul L. Rosin, Zixi Cai, Xi Han, Dingcheng Yang, Haozhi Huang, and Shi-Min Hu. Pose2seg: Detection free human instance segmentation. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019, pages 889898, 2019. Yufei Xu, Jing Zhang, Qiming Zhang, and Dacheng Tao. Vitpose: Simple vision transformer baselines for human pose estimation. In Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022. Alane Suhr, Stephanie Zhou, Ally Zhang, Iris Zhang, Huajun Bai, and Yoav Artzi. corpus for reasoning about natural In Proceedings of the 57th Conference of the Association for Computational language grounded in photographs. Linguistics, ACL 2019, Florence, Italy, July 28August 2, 2019, Volume 1: Long Papers, pages 64186428, 2019. On Path to Multimodal Generalist: General-Level and General-Bench Aircraft model matching dataset. URL https://huggingface.co/datasets/Multimodal-Fatima/FGVC_ Aircraft_test. Car model matching dataset, b. URL https://huggingface.co/datasets/Multimodal-Fatima/ StanfordCars_test."
        },
        {
            "title": "Action",
            "content": "consistency verification."
        },
        {
            "title": "URL",
            "content": "https://drive.google.com/file/d/ 0B7TOZKXmIjU3OUhfd3BPaVRHZVE/view?resourcekey=0-hU4gyE6hFsBgizIh9DFqtA. Yann LeCun. Digital consistency comparison. URL https://huggingface.co/datasets/ylecun/mnist. Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi, Luke Zettlemoyer, and Dieter Fox. ALFRED: Benchmark for Interpreting Grounded Instructions for Everyday Tasks. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020. Mohit Iyyer, Varun Manjunatha, Anupam Guha, Yogarshi Vyas, Jordan Boyd-Graber, Hal Daume III, and Larry Davis. The amazing mysteries of the gutter: Drawing inferences between panels in comic book narratives. In IEEE Conference on Computer Vision and Pattern Recognition, 2017. Xintong Han, Zuxuan Wu, Phoenix X. Huang, Xiao Zhang, Menglong Zhu, Yuan Li, Yang Zhao, and Larry S. Davis. Automatic spatially-aware fashion concept discovery. In IEEE International Conference on Computer Vision, ICCV 2017, Venice, Italy, October 22-29, 2017, pages 14721480, 2017. Semih Yagcioglu, Aykut Erdem, Erkut Erdem, and Nazli Ikizler-Cinbis. Recipeqa: challenge dataset for multimodal comprehension of cooking recipes. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31 - November 4, 2018, pages 13581368, 2018. Dasara Shullani, Marco Fontani, Massimo Iuliani, Omar Al Shaya, and Alessandro Piva. VISION: video and image dataset for source identification. EURASIP J. Inf. Secur., 2017:15, 2017. Phillip Isola, Joseph J. Lim, and Edward H. Adelson. Discovering states and transformations in image collections. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2015, Boston, MA, USA, June 7-12, 2015, pages 13831391, 2015. Holger Caesar, Varun Bankiti, Alex H. Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom. nuscenes: multimodal dataset for autonomous driving. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020, pages 1161811628, 2020. Chen Change Loy, Shaogang Gong, and Tao Xiang. From semi-supervised to transfer counting of crowds. In IEEE International Conference on Computer Vision, ICCV 2013, Sydney, Australia, December 1-8, 2013, pages 22562263, 2013a. Ruixiang Jiang, Lingbo Liu, and Changwen Chen. Clip-count: Towards text-guided zero-shot object counting. In Proceedings of the 31st ACM International Conference on Multimedia, MM 2023, Ottawa, ON, Canada, 29 October 20233 November 2023, pages 45354545, 2023. Count the number of faces present in an image, a. URL https://www.kaggle.com/datasets/vin1234/ count-the-number-of-faces-present-in-an-image. Fits images for object counting and detection. URL https://www.kaggle.com/datasets/santurini/ fits-images-for-object-counting-and-detection. Seeds counting. URL https://www.kaggle.com/datasets/raj123verma/seeds-counting. Meng-Ru Hsieh, Yen-Liang Lin, and Winston H. Hsu. Drone-based object counting by spatially regularized regional proposal network. In IEEE International Conference on Computer Vision, ICCV 2017, Venice, Italy, October 22-29, 2017, pages 41654173, 2017. 51 On Path to Multimodal Generalist: General-Level and General-Bench Chen Change Loy, Shaogang Gong, and Tao Xiang. From semi-supervised to transfer counting of crowds. In IEEE International Conference on Computer Vision, ICCV 2013, Sydney, Australia, December 1-8, 2013, pages 22562263, 2013b."
        },
        {
            "title": "Count",
            "content": "the paperclips, b. count-the-paperclips."
        },
        {
            "title": "URL",
            "content": "https://www.kaggle.com/datasets/jeffheaton/ Etienne David, Simon Madec, Pouria Sadeghi-Tehran, Helge Aasen, Bangyou Zheng, Shouyang Liu, Norbert Kirchgessner, Goro Ishikawa, Koichi Nagasawa, Minhajul Badhon, et al. Global wheat head detection (gwhd) dataset: large and diverse dataset of high-resolution rgb-labelled images to develop and benchmark wheat head detection methods. Plant Phenomics, 2020, 2020. Object counting using pipe dataset, c. URL https://www.kaggle.com/datasets/satya12389/ object-counting-using-pipes-dataset. Romrawin Chumpu. Multimodal neural translation. URL https://huggingface.co/datasets/romrawinjp/ multi30k/viewer/default/test. Matthias Minderer, Alexey A. Gritsenko, Austin Stone, Maxim Neumann, Dirk Weissenborn, Alexey Dosovitskiy, Aravindh Mahendran, Anurag Arnab, Mostafa Dehghani, Zhuoran Shen, Xiao Wang, Xiaohua Zhai, Thomas Kipf, and Neil Houlsby. Simple open-vocabulary object detection with vision transformers. CoRR, abs/2205.06230, 2022b. Mask dataset. URL https://makeml.app/datasets/mask. Tianheng Cheng, Lin Song, Yixiao Ge, Wenyu Liu, Xinggang Wang, and Ying Shan. Yolo-world: Real-time openIn IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2024, vocabulary object detection. Seattle, WA, USA, June 16-22, 2024, pages 1690116911. IEEE, 2024. doi: 10.1109/CVPR52733.2024.01599. URL https://doi.org/10.1109/CVPR52733.2024.01599."
        },
        {
            "title": "Small",
            "content": "vehicle detection far vehicle detection. URL https://www.kaggle.com/datasets/ dataclusterlabs/small-vehicle-images-far-vehicle-detection. Grand dataset for small object detection. URL https://www.kaggle.com/datasets/waelyahyayaseen/ grand-dataset-for-small-object-detection. Traffic signs detection. URL https://www.kaggle.com/datasets/pkdarabi/cardetection. Bone fracture detection: Computer vision project. URL https://www.kaggle.com/datasets/pkdarabi/ bone-fracture-detection-computer-vision-project. AhmedMohsen. drone-detection-new dataset, 2022. URL https://universe.roboflow.com/ahmedmohsen/ drone-detection-new-peksv. Hospital. Hospital dataset, 2024. URL https://universe.roboflow.com/hospital-peeox/ hospital-i5qoz. Final Year Project. Wound care dataset, 2024. URL https://universe.roboflow.com/ final-year-project-ybo2r/wound-care-oufqn. Anupong Wannakrairot. Gauge detection dataset, 2023. URL https://universe.roboflow.com/ anupong-wannakrairot-ud9ty/gauge-detection-cohbz. Minervas Workspace. Car parking occupation dataset, 2024. URL https://universe.roboflow.com/ minervas-workspace-7comk/car-parking-occupation. patrick Cai. tre dataset, 2023. URL https://universe.roboflow.com/patrick-cai-gfgl7/tre-o6dcm. artz. traffic light dataset, 2024. URL https://universe.roboflow.com/artz/traffic_light-28zwo. Roboflow. rock-paper-scissors dataset, 2025. URL https://universe.roboflow.com/roboflow-58fyf/ rock-paper-scissors-sxsw. 52 On Path to Multimodal Generalist: General-Level and General-Bench alkud12. Pricetag dataset, 2024. URL https://universe.roboflow.com/alkud12/pricetag-4d8lm. Polyu. Retail object detection dataset, 2022. URL https://universe.roboflow.com/polyu-9ziss/ retail-object-detection-bqtzf. KIT. exit-sign-extended version dataset, 2023. URL https://universe.roboflow.com/kit-g1foh/ exit-sign-extended-version. Hoang. Handwritten-text-recognition dataset, 2024. URL https://universe.roboflow.com/hoang-r9hhb/ handwritten-text-recognition-wb2o4. Signs language. Signlanguage dataset, 2024. URL https://universe.roboflow.com/signs-language/ signlanguage-5kmtf. FootballVideoTrackingApp. football-video-tracking-project dataset, 2024. URL https://universe.roboflow. com/footballvideotrackingapp/football-video-tracking-project. intern. pest detection dataset, 2024. URL https://universe.roboflow.com/intern-tvkth/ pest-detection-m0inx. faiza rehman. Underwater garbage detection dataset, 2024. URL https://universe.roboflow.com/ faiza-rehman/underwater-garbage-detection-d4dnn. Myworkspace. Ingredient detection dataset, 2024. URL https://universe.roboflow.com/ myworkspace-awwul/ingredient-detection-d6nwz. Roboflow 100. circuit elements dataset, 2024. URL https://universe.roboflow.com/roboflow-100/ circuit-elements. Roboflow 100. tabular data dataset, 2023a. URL https://universe.roboflow.com/roboflow-100/ tabular-data-wf9uh. Roboflow 100. radio signal dataset. https://universe.roboflow.com/roboflow-100/radio-signal, 2023b. URL https://universe.roboflow.com/roboflow-100/radio-signal. week3day3. Empty shelf detector dataset, 2024. URL https://universe.roboflow.com/week3day3/ empty-shelf-detector-wt4im. Tomato dataset. URL https://makeml.app/datasets/tomato. Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Qing Jiang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, and Lei Zhang. Grounding DINO: marrying DINO with grounded pre-training for open-set object detection. In Computer Vision - ECCV 2024 - 18th European Conference, Milan, Italy, September 29-October 4, 2024, Proceedings, Part XLVII, pages 3855, 2024e. Drone detection. URL https://www.kaggle.com/datasets/cybersimar08/drone-detection. Bee detection dataset. URL https://www.kaggle.com/datasets/lara311/bee-detection-dataset."
        },
        {
            "title": "Cat",
            "content": "faces detection. cat-faces-detection."
        },
        {
            "title": "URL",
            "content": "https://www.kaggle.com/datasets/aleksandrdremov/ Cub 200 bird species xml detection dataset. URL https://www.kaggle.com/datasets/sovitrath/ cub-200-bird-species-xml-detection-dataset."
        },
        {
            "title": "Deep",
            "content": "fish object detection."
        },
        {
            "title": "URL",
            "content": "https://www.kaggle.com/datasets/vencerlanz09/ deep-fish-object-detection. Dat Tran. Trash panda detection. URL https://www.kaggle.com/datasets/andrewmvd/ racoon-detection. 53 On Path to Multimodal Generalist: General-Level and General-Bench Ships data. Weed detection. URL https://www.kaggle.com/datasets/jaidalmotra/weed-detection. Sarscope: Synthetic aperture radar maritime images. URL https://www.kaggle.com/datasets/ kailaspsudheer/sarscope-unveiling-the-maritime-landscape/data. Jizhizi Li, Jing Zhang, Stephen Maybank, and Dacheng Tao. Bridging composite and real: towards end-to-end deep image matting. International Journal of Computer Vision, 130(2):246266, 2022c. Jiachen Li, Jitesh Jain, and Humphrey Shi. Matting anything. arXiv: 2306.05399, 2023f. Jizhizi Li, Sihan Ma, Jing Zhang, and Dacheng Tao. Privacy-preserving portrait matting. In Proceedings of the 29th ACM International Conference on Multimedia, page 35013509, 2021. Icdar 2023 dtt in images 2: Text manipulation detection. URL https://tianchi.aliyun.com/competition/ entrance/532052/information. Chufeng Xiao, Deng Yu, Xiaoguang Han, Youyi Zheng, and Hongbo Fu. Sketchhairsalon: Deep sketch-based hair image synthesis. ACM Transactions on Graphics (Proceedings of ACM SIGGRAPH Asia 2021), pages 116, 2021. Animals-10, a. URL https://www.kaggle.com/datasets/alessiocorrado99/animals10. Big cats image classification dataset. URL https://www.kaggle.com/datasets/patriciabrezeanu/ big-cats-image-classification-dataset. Fruit recognition. URL https://www.kaggle.com/datasets/chrisfilo/fruit-recognition."
        },
        {
            "title": "Indonesian",
            "content": "wayang indonesian-wayang-types. types."
        },
        {
            "title": "URL",
            "content": "https://www.kaggle.com/datasets/gibranfadilla/"
        },
        {
            "title": "Vegetable",
            "content": "image dataset. vegetable-image-dataset."
        },
        {
            "title": "URL",
            "content": "https://www.kaggle.com/datasets/misrakahmed/ Car logo dataset, b. URL https://www.kaggle.com/datasets/mahaarajab/car-logo-dataset."
        },
        {
            "title": "Egg",
            "content": "image dataset."
        },
        {
            "title": "URL",
            "content": "https://www.kaggle.com/datasets/abdullahkhanuet22/ eggs-images-classification-damaged-or-not. Dog breeds. URL https://www.kaggle.com/datasets/mohamedchahed/dog-breeds."
        },
        {
            "title": "Garbage",
            "content": "classification."
        },
        {
            "title": "URL",
            "content": "https://www.kaggle.com/datasets/quangtheng/ garbage-classification-6-classes-775class."
        },
        {
            "title": "Mammals",
            "content": "classification. mammals-classification."
        },
        {
            "title": "URL",
            "content": "https://www.kaggle.com/datasets/anirudhg15/"
        },
        {
            "title": "Utensil",
            "content": "image recognition."
        },
        {
            "title": "URL",
            "content": "https://www.kaggle.com/datasets/jehanbhathena/ utensil-image-recognition."
        },
        {
            "title": "Boat",
            "content": "types recognition. boat-types-recognition."
        },
        {
            "title": "URL",
            "content": "https://www.kaggle.com/datasets/clorichel/ Josiah Wang, Katja Markert, and Mark Everingham. Learning models for object recognition from natural language descriptions. In Proceedings of the British Machine Vision Conference, 2009. Fish recognition ground-truth dataset. URL https://www.kaggle.com/datasets/madhushreesannigrahi/ fish-recognition-ground-truth-data. Ball classification. URL https://huggingface.co/datasets/Asseh/Ball_Classification. Flower classification. URL https://huggingface.co/datasets/tian9/flower_classification. 54 On Path to Multimodal Generalist: General-Level and General-Bench"
        },
        {
            "title": "Food",
            "content": "category classification."
        },
        {
            "title": "URL",
            "content": "https://huggingface.co/datasets/Kaludi/ data-food-category-classification."
        },
        {
            "title": "Material",
            "content": "classification. Classification."
        },
        {
            "title": "URL",
            "content": "https://huggingface.co/datasets/Factral/Material_ Plant classification. URL https://huggingface.co/datasets/sxdave/plant_classification_ dataset. Trash classification. URL https://huggingface.co/datasets/ethanwan/trash_classification."
        },
        {
            "title": "Vehicle",
            "content": "classification. vehicle-classification."
        },
        {
            "title": "Bird",
            "content": "species classification. bird-species-classification."
        },
        {
            "title": "URL",
            "content": "https://huggingface.co/datasets/aryadytm/"
        },
        {
            "title": "URL",
            "content": "https://www.kaggle.com/datasets/akash2907/ Chinese fine art, b. URL https://www.kaggle.com/datasets/rickyjli/chinese-fine-art."
        },
        {
            "title": "Famous",
            "content": "women. famous-iconic-women. iconic"
        },
        {
            "title": "URL",
            "content": "https://www.kaggle.com/datasets/fatiimaezzahra/ Gender detection and classification - face dataset, b. URL https://www.kaggle.com/datasets/ trainingdatapro/gender-detection-and-classification-image-dataset."
        },
        {
            "title": "Radar",
            "content": "threat object classification, b. URL https://www.kaggle.com/datasets/rauldbcet/ radar-threat-object-classification. Realwaste image classification, a. URL https://www.kaggle.com/datasets/joebeachcapital/ realwaste. Micro-organism image classification. URL https://www.kaggle.com/datasets/mdwaquarazam/ microorganism-image-classification. Jianfei Yu, Jing Jiang, Li Yang, and Rui Xia. Improving multimodal named entity recognition via entity span detection with unified multimodal transformer. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, pages 33423352, 2020a. Jianfei Yu, Jing Jiang, Li Yang, and Rui Xia. Improving multimodal named entity recognition via entity span detection with unified multimodal transformer. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, pages 33423352, 2020b. Zhiyuan Yan, Taiping Yao, Shen Chen, Yandan Zhao, Xinghe Fu, Junwei Zhu, Donghao Luo, Li Yuan, Chengjie Wang, Shouhong Ding, et al. Df40: Toward next-generation deepfake detection. arXiv preprint arXiv:2406.13495, 2024a. Chuangchuang Tan, Yao Zhao, Shikui Wei, Guanghua Gu, Ping Liu, and Yunchao Wei. Rethinking the up-sampling operations in cnn-based generative network for generalizable deepfake detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2813028139, 2024. Bowen Cheng, Maxwell D. Collins, Yukun Zhu, Ting Liu, Thomas S. Huang, Hartwig Adam, and Liang-Chieh Chen. In 2020 IEEE/CVF Panoptic-deeplab: simple, strong, and fast baseline for bottom-up panoptic segmentation. Conference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020, pages 1247212482, 2020."
        },
        {
            "title": "Yoga",
            "content": "pose classification."
        },
        {
            "title": "URL",
            "content": "https://www.kaggle.com/datasets/ujjwalchowdhury/ yoga-pose-classification. Yu Zhao, Jianguo Wei, Zhichao Lin, Yueheng Sun, Meishan Zhang, and Min Zhang. Visual spatial description: Controlled spatial-oriented image-to-text generation. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, pages 14371449, 2022. 55 On Path to Multimodal Generalist: General-Level and General-Bench Shuo Yang, Yongqi Wang, Xiaofeng Ji, and Xinxiao Wu. Multi-modal prompting for open-vocabulary video visual relationship detection. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 65136521, 2024b. Bowen Yin, Xuying Zhang, Zhong-Yu Li, Li Liu, Ming-Ming Cheng, and Qibin Hou. Dformer: Rethinking RGBD representation learning for semantic segmentation. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024, 2024."
        },
        {
            "title": "Strawberry",
            "content": "classification. strawberry-dataset."
        },
        {
            "title": "URL",
            "content": "https://www.kaggle.com/datasets/abdulbasit31/ RFES. Fire and smoke detection dataset, 2024. URL https://universe.roboflow.com/rfes/ fire-smoke-detection-eozii-nfuzs-vgo4c. Jiraphat. Gun with webcam views dataset, 2023. URL https://universe.roboflow.com/jiraphat-stgwm/ gun-with-webcam-views. Roboflow 100. Construction safety dataset, 2023c. URL https://universe.roboflow.com/roboflow-100/ construction-safety-gsnvb. Roboflow Universe Projects. Safety vests dataset, 2024. URL https://universe.roboflow.com/ roboflow-universe-projects/safety-vests. custom yolov5. Fall person dataset, 2024. URL https://universe.roboflow.com/custom-yolov5-wm0zy/ fall_person. heejin. Smoke detection1 dataset, 2024. URL https://universe.roboflow.com/heejin/ smoke-detection1-owusa. Bikes helmets dataset. URL https://www.kaggle.com/datasets/andrewmvd/helmet-detection? select=images. Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A. Shamma, Michael S. Bernstein, and Li Fei-Fei. Visual genome: Connecting language and vision using crowdsourced dense image annotations. Int. J. Comput. Vis., 123(1):3273, 2017. Honghui Yang, Zili Liu, Xiaopei Wu, Wenxiao Wang, Wei Qian, Xiaofei He, and Deng Cai. Graph R-CNN: towards accurate 3d object detection with semantic-decorated local graph. In Computer Vision - ECCV 2022 - 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part VIII, pages 662679, 2022a. Terrain recognition. URL https://www.kaggle.com/datasets/krishuppal/terrain-recognition."
        },
        {
            "title": "Landscape",
            "content": "recognition."
        },
        {
            "title": "URL",
            "content": "https://www.kaggle.com/datasets/utkarshsaxenadn/ landscape-recognition-image-dataset-12k-images. American sign language dataset. URL https://www.kaggle.com/datasets/ayuraj/asl-dataset. Glenn Jocher. Ultralytics yolov5, 2020. URL https://github.com/ultralytics/yolov5. Rumeysa Bodur, Erhan Gundogdu, Binod Bhattarai, Tae-Kyun Kim, Michael Donoser, and Loris Bazzani. iedit: Localised text-guided image editing with weak supervision. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2024 - Workshops, Seattle, WA, USA, June 17-18, 2024, pages 74267435, 2024. Harsh Jhamtani and Taylor Berg-Kirkpatrick. Learning to describe differences between pairs of similar images. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP), 2018. Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Li Fei-Fei, C. Lawrence Zitnick, and Ross B. Girshick. CLEVR: diagnostic dataset for compositional language and elementary visual reasoning. In 2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017, pages 19881997, 2017. 56 On Path to Multimodal Generalist: General-Level and General-Bench Maxwell Forbes, Christine Kaeser-Chen, Piyush Sharma, and Serge J. Belongie. Neural naturalist: Generating fine-grained image comparisons. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019, pages 708717, 2019. Changmeng Zheng, Zhiwei Wu, Junhao Feng, Ze Fu, and Yi Cai. MNRE: challenge multimodal dataset for neural relation extraction with visual evidence in social media posts. In 2021 IEEE International Conference on Multimedia and Expo, ICME 2021, Shenzhen, China, July 5-9, 2021, pages 16, 2021. Livio Baldini Soares, Nicholas FitzGerald, Jeffrey Ling, and Tom Kwiatkowski. Matching the blanks: Distributional similarity for relation learning. In Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28August 2, 2019, Volume 1: Long Papers, pages 28952905, 2019. Hareesh Ravi, Kushal Kafle, Scott Cohen, Jonathan Brandt, and Mubbasir Kapadia. Aesop: Abstract encoding of stories, objects and pictures. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 20522063, October 2021. Yitong Li, Zhe Gan, Yelong Shen, Jingjing Liu, Yu Cheng, Yuexin Wu, Lawrence Carin, David E. Carlson, and Jianfeng Gao. Storygan: sequential conditional GAN for story visualization. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019, pages 63296338, 2019a. Tanmay Gupta, Dustin Schwenk, Ali Farhadi, Derek Hoiem, and Aniruddha Kembhavi. Imagine this! scripts to compositions to videos. In Computer Vision - ECCV 2018 - 15th European Conference, Munich, Germany, September 8-14, 2018, Proceedings, Part VIII, pages 610626, 2018. Ting-Hao (Kenneth) Huang, Francis Ferraro, Nasrin Mostafazadeh, Ishan Misra, Aishwarya Agrawal, Jacob Devlin, Ross B. Girshick, Xiaodong He, Pushmeet Kohli, Dhruv Batra, C. Lawrence Zitnick, Devi Parikh, Lucy Vanderwende, Michel Galley, and Margaret Mitchell. Visual storytelling. In NAACL HLT 2016, The 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, San Diego California, USA, June 12-17, 2016, pages 12331239, 2016. Adyasha Maharana, Darryl Hannan, and Mohit Bansal. Storydall-e: Adapting pretrained text-to-image transformers for story continuation. In Computer Vision - ECCV 2022 - 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part XXXVII, pages 7087, 2022. Haixia Xiao. Weather phenomenon database (weapd), 2021. URL https://doi.org/10.7910/DVN/M8JQCR. Chengying Gao, Qi Liu, Qi Xu, Limin Wang, Jianzhuang Liu, and Changqing Zou. Sketchycoco: Image generation from freehand scene sketches. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020, pages 51735182, 2020. Tengfei Wang, Ting Zhang, Bo Zhang, Hao Ouyang, Dong Chen, Qifeng Chen, and Fang Wen. Pretraining is all you need for image-to-image translation. arXiv:2205.12952, 2022b. Yunpeng Bai, Xintao Wang, Yan-Pei Cao, Yixiao Ge, Chun Yuan, and Ying Shan. Dreamdiffusion: High-quality eeg-toimage generation with temporal masked signal modeling and CLIP alignment. In Computer Vision - ECCV 2024 - 18th European Conference, Milan, Italy, September 29-October 4, 2024, Proceedings, Part XXXI, pages 472488, 2024. Jiashi Feng Jiaying Liu Zongming Guo Wenhan Yang, Robby T. Tan and Shuicheng Yan. Joint rain detection and removal from single image. IEEE Conference on Computer Vision and Pattern Recognition, 2017. Conghan Yue, Zhengwei Peng, Junlong Ma, Shiyan Du, Pengxu Wei, and Dongyu Zhang. Image restoration through generalized ornstein-uhlenbeck bridge. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024, 2024b. Rui Qian, Robby T. Tan, Wenhan Yang, Jiajun Su, and Jiaying Liu. Attentive generative adversarial network for raindrop removal from single image. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018. Jeya Maria Jose Valanarasu, Rajeev Yasarla, and Vishal M. Patel. Transweather: Transformer-based restoration of images degraded by adverse weather conditions, 2021. 57 On Path to Multimodal Generalist: General-Level and General-Bench Boyi Li, Wenqi Ren, Dengpan Fu, Dacheng Tao, Dan Feng, Wenjun Zeng, and Zhangyang Wang. Benchmarking singleimage dehazing and beyond. IEEE Trans. Image Process., 28(1):492505, 2019b. Yuda Song, Zhuqing He, Hui Qian, and Xin Du. Vision transformers for single image dehazing. IEEE Trans. Image Process., 32:19271941, 2023. Yun-Fu Liu, Da-Wei Jaw, Shih-Chia Huang, and Jenq-Neng Hwang. Desnownet: Context-aware deep network for snow removal. IEEE Trans. Image Process., 27(6):30643073, 2018. Yuning Cui, Wenqi Ren, Xiaochun Cao, and Alois Knoll. Revitalizing convolutional network for image restoration. IEEE Trans. Pattern Anal. Mach. Intell., 46(12):94239438, 2024. Seungjun Nah, Tae Hyun Kim, and Kyoung Mu Lee. Deep multi-scale convolutional neural network for dynamic scene deblurring. In CVPR, July 2017. Qingli Li Xintian Mao and Yan Wang. Adarevd: Adaptive patch exiting reversible decoder pushes the limit of image deblurring. In Proc. CVPR, 2024. Eirikur Agustsson and Radu Timofte. Ntire 2017 challenge on single image super-resolution: Dataset and study. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, 2017. Ziwei Luo, Fredrik K. Gustafsson, Zheng Zhao, Jens Sjolund, and Thomas B. Schon. Controlling vision-language models for multi-task image restoration. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. Binghui Huang, Zhi Li, Chao Yang, Fuchun Sun, and Yixu Song. Single satellite optical imagery dehazing using SAR image prior based on conditional generative adversarial networks. In IEEE Winter Conference on Applications of Computer Vision, WACV 2020, Snowmass Village, CO, USA, March 1-5, 2020, pages 17951802, 2020. Laniqng Guo, Chong Wang, Yufei Wang, Siyu Huang, Wenhan Yang, Alex Kot, and Bihan Wen. Single-image shadow removal using deep learning: comprehensive survey. arXiv preprint arXiv:2407.08865, 2024a. Xiaowei Hu, Chi-Wing Fu, Lei Zhu, Jing Qin, and Pheng-Ann Heng. Direction-aware spatial context features for shadow detection and removal. IEEE Trans. Pattern Anal. Mach. Intell., 42(11):27952808, 2020. Yuekun Dai, Chongyi Li, Shangchen Zhou, Ruicheng Feng, and Chen Change Loy. Flare7k: phenomenological nighttime In Advances in Neural Information Processing Systems 35: Annual Conference on Neural flare removal dataset. Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022a. Dafeng Zhang, Jia Ouyang, Guanqun Liu, Xiaobing Wang, Xiangyu Kong, and Zhezhu Jin. Ff-former: Swin fourier transformer for nighttime flare removal. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2023 - Workshops, Vancouver, BC, Canada, June 17-24, 2023, pages 28242832, 2023j. Lintao Peng, Chunli Zhu, and Liheng Bian. U-shape transformer for underwater image enhancement. IEEE Transactions on Image Processing, 32:30663079, 2023. doi: 10.1109/TIP.2023.3276332. Rita Pucci and Niki Martinel. Ce-vae: Capsule enhanced variational autoencoder for underwater image reconstruction. Feb 2024. Ke Ma, Zhixin Shu, Xue Bai, Jue Wang, and Dimitris Samaras. Docunet: Document image unwarping via stacked u-net. In 2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018, pages 47004709, 2018. Lei Zhu, Ke Xu, Zhanghan Ke, and Rynson W. H. Lau. Mitigating intensity bias in shadow detection via feature decomposition and reweighting. In 2021 IEEE/CVF International Conference on Computer Vision, ICCV 2021, Montreal, QC, Canada, October 10-17, 2021, pages 46824691, 2021. Md Jahidul Islam, Youya Xia, and Junaed Sattar. Fast underwater image enhancement for improved visual perception. IEEE Robotics Autom. Lett., 5(2):32273234, 2020. 58 On Path to Multimodal Generalist: General-Level and General-Bench Long Ma, Tengyu Ma, Risheng Liu, Xin Fan, and Zhongxuan Luo. Toward fast, flexible, and robust low-light image In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages enhancement. 56375646, 2022. Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of gans for improved quality, stability, and variation. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings, 2018. Wenbo Li, Zhe Lin, Kun Zhou, Lu Qi, Yi Wang, and Jiaya Jia. Mat: Mask-aware transformer for large hole image inpainting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022d. Aysen Degerli, Serkan Kiranyaz, Muhammad Enamul Hoque Chowdhury, and Moncef Gabbouj. Osegnet: Operational segmentation network for covid-19 detection using chest x-ray images. In 2022 IEEE International Conference on Image Processing, ICIP 2022, Bordeaux, France, 16-19 October 2022, pages 23062310, 2022. Aysen Degerli, Mete Ahishali, Mehmet Yamac, Serkan Kiranyaz, Muhammad Enamul Hoque Chowdhury, Khalid Hameed, Tahir Hamid, Rashid Mazhar, and Moncef Gabbouj. COVID-19 infection map generation and detection from chest x-ray images. Health Inf. Sci. Syst., 9(1):15, 2021. Fan Deng-Ping, Huang Ziling, Zheng Peng, Liu Hong, Qin Xuebin, and Van Gool Luc. Facial-sketch synthesis: new challenge. Machine Intelligence Research, 2022. Fei Gao, Yifan Zhu, Chang Jiang, and Nannan Wang. Human-inspired facial sketch synthesis with dynamic adaptation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 72377247, 2023. Tim Brooks, Aleksander Holynski, and Alexei A. Efros. Instructpix2pix: Learning to follow image editing instructions. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2023, Vancouver, BC, Canada, June 17-24, 2023, pages 1839218402, 2023. Pretty Face. URL https://www.kaggle.com/datasets/yewtsing/pretty-face. Guangcong Zheng, Xianpan Zhou, Xuewei Li, Zhongang Qi, Ying Shan, and Xi Li. Layoutdiffusion: Controllable diffusion model for layout-to-image generation. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2023, Vancouver, BC, Canada, June 17-24, 2023, pages 2249022499, 2023b. Can Qin, Shu Zhang, Ning Yu, Yihao Feng, Xinyi Yang, Yingbo Zhou, Huan Wang, Juan Carlos Niebles, Caiming Xiong, Silvio Savarese, Stefano Ermon, Yun Fu, and Ran Xu. Unicontrol: unified diffusion model for controllable visual generation in the wild. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023a. Tianyi Wei, Dongdong Chen, Wenbo Zhou, Jing Liao, Zhentao Tan, Lu Yuan, Weiming Zhang, and Nenghai Yu. Hairclip: Design your hair by text and reference image. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022, pages 1805118060, 2022. Can Qin, Ning Yu, Chen Xing, Shu Zhang, Zeyuan Chen, Stefano Ermon, Yun Fu, Caiming Xiong, and Ran Xu. Gluegen: Plug and play multi-modal encoders for x-to-image generation. In IEEE/CVF International Conference on Computer Vision, ICCV 2023, Paris, France, October 1-6, 2023, pages 2302823039, 2023b."
        },
        {
            "title": "Image",
            "content": "colorization dataset. image-colorization-dataset."
        },
        {
            "title": "URL",
            "content": "https://www.kaggle.com/datasets/aayush9753/ Yinhuai Wang, Jiwen Yu, and Jian Zhang. Zero-shot image restoration using denoising diffusion null-space model. The Eleventh International Conference on Learning Representations, 2023c. Yuying Ge, Sijie Zhao, Chen Li, Yixiao Ge, and Ying Shan. Seed-data-edit technical report: hybrid dataset for instructional image editing. arXiv preprint arXiv:2405.04007, 2024b. Yuying Ge, Sijie Zhao, Jinguo Zhu, Yixiao Ge, Kun Yi, Lin Song, Chen Li, Xiaohan Ding, and Ying Shan. Seed-x: Multimodal models with unified multi-granularity comprehension and generation. arXiv preprint arXiv:2404.14396, 2024c. 59 On Path to Multimodal Generalist: General-Level and General-Bench Ziqi Huang, Kelvin CK Chan, Yuming Jiang, and Ziwei Liu. Collaborative diffusion for multi-modal face generation and editing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 60806090, 2023b. Weijia Wu, Zhuang Li, Yefei He, Mike Zheng Shou, Chunhua Shen, Lele Cheng, Yan Li, Tingting Gao, Di Zhang, and Zhongyuan Wang. Paragraph-to-image generation with information-enriched diffusion model. CoRR, abs/2311.14284, 2023c. Santiago Castro, Naihao Deng, Pingxuan Huang, Mihai G. Burzo, and Rada Mihalcea. In-the-wild video question answering. In COLING, pages 56135635, October 2022. Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, and Chunyuan Li. Video instruction tuning with synthetic data. CoRR, abs/2410.02713, 2024f. Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. Videogpt+: Integrating image and video encoders for enhanced video understanding. arxiv, 2024. URL https://arxiv.org/abs/2406.09418. Dongxu Li, Yudong Liu, Haoning Wu, Yue Wang, Zhiqi Shen, Bowen Qu, Xinyao Niu, Guoyin Wang, Bei Chen, and Junnan Li. Aria: An open multimodal native mixture-of-experts model. arXiv preprint arXiv:2410.05993, 2024i. Haopeng Li, Andong Deng, Qiuhong Ke, Jun Liu, Hossein Rahmani, Yulan Guo, Bernt Schiele, and Chen Chen. Sports-qa: large-scale video question answering benchmark for complex and professional sports. CoRR, abs/2401.01505, 2024j. Zhou Yu, Dejing Xu, Jun Yu, Ting Yu, Zhou Zhao, Yueting Zhuang, and Dacheng Tao. Activitynet-qa: dataset for understanding complex web videos via question answering. In The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2019, Honolulu, Hawaii, USA, January 27 - February 1, 2019, pages 91279134, 2019. Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. CoRR, abs/2409.12191, 2024g. Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Lou, Limin Wang, and Yu Qiao. Mvbench: comprehensive multi-modal video understanding benchmark. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2024, Seattle, WA, USA, June 16-22, 2024, pages 2219522206, 2024k. Chaoyou Fu, Yuhan Dai, Yondong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, Peixian Chen, Yanwei Li, Shaohui Lin, Sirui Zhao, Ke Li, Tong Xu, Xiawu Zheng, Enhong Chen, Rongrong Ji, and Xing Sun. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. CoRR, abs/2405.21075, 2024c. Zuyan Liu, Yuhao Dong, Ziwei Liu, Winston Hu, Jiwen Lu, and Yongming Rao. Oryx MLLM: on-demand spatial-temporal understanding at arbitrary resolution. CoRR, abs/2409.12961, 2024f. Xinyu Fang, Kangrui Mao, Haodong Duan, Xiangyu Zhao, Yining Li, Dahua Lin, and Kai Chen. Mmbench-video: long-form multi-shot benchmark for holistic video understanding. CoRR, abs/2406.14515, 2024. Fuxin Li, Taeyoung Kim, Ahmad Humayun, David Tsai, and James M. Rehg. Video segmentation by tracking many figure-ground segments. In IEEE International Conference on Computer Vision, ICCV 2013, Sydney, Australia, December 1-8, 2013, pages 21922199, 2013. Peter Ochs, Jitendra Malik, and Thomas Brox. Segmentation of moving objects by long term video analysis. IEEE Trans. Pattern Anal. Mach. Intell., 36(6):11871200, 2014. Junhao Lin, Lei Zhu, Jiaxing Shen, Huazhu Fu, Qing Zhang, and Liansheng Wang. Vidsod-100: new dataset and baseline model for RGB-D video salient object detection. Int. J. Comput. Vis., 132(11):51735191, 2024b. Suhwan Cho, Minhyeok Lee, Jungho Lee, and Sangyoun Lee. Transforming static images using generative models for video salient object detection. arXiv preprint arXiv:2411.13975, 2024. 60 On Path to Multimodal Generalist: General-Level and General-Bench Hala Lamdouar, Charig Yang, Weidi Xie, and Andrew Zisserman. Betrayed by motion: Camouflaged object discovery via motion segmentation. In Computer Vision - ACCV 2020 - 15th Asian Conference on Computer Vision, Kyoto, Japan, November 30 - December 4, 2020, Revised Selected Papers, Part II, pages 488503, 2020. Muhammad Nawfal Meeran, Gokul Adethya T, and Bhanu Pratyush Mantha. SAM-PM: enhancing video camouflaged object detection using spatio-temporal attention. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2024 - Workshops, Seattle, WA, USA, June 17-18, 2024, pages 18571866, 2024. Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. UCF101: dataset of 101 human actions classes from videos in the wild. CoRR, abs/1212.0402, 2012a. Hildegard Kuehne, Hueihan Jhuang, Estıbaliz Garrote, Tomaso A. Poggio, and Thomas Serre. HMDB: large video database for human motion recognition. In IEEE International Conference on Computer Vision, ICCV 2011, Barcelona, Spain, November 6-13, 2011, pages 25562563, 2011. Joao Carreira, Eric Noland, Andras Banki-Horvath, Chloe Hillier, and Andrew Zisserman. short note about kinetics-600. CoRR, abs/1808.01340, 2018. Yi Wang Yizhuo Li Wenhai Wang Ping Luo Yali Wang Limin Wang KunChang Li, Yinan He and Yu Qiao. Videochat: Chat-centric video understanding. arXiv preprint arXiv:2305.06355, 2023. Dongxu Li, Cristian Rodriguez, Xin Yu, and Hongdong Li. Word-level deep sign language recognition from video: new large-scale dataset and methods comparison. In The IEEE Winter Conference on Applications of Computer Vision, pages 14591469, 2020. Ronglai Zuo, Fangyun Wei, and Brian Mak. Natural language-assisted sign language recognition. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2023, Vancouver, BC, Canada, June 17-24, 2023, pages 1489014900. IEEE, 2023. Yan Shu, Peitian Zhang, Zheng Liu, Minghao Qin, Junjie Zhou, Tie-Jun Huang, and Bo Zhao. Video-xl: Extra-long vision language model for hour-scale video understanding. CoRR, abs/2409.14485, 2024. Junjie Zhou, Yan Shu, Bo Zhao, Boya Wu, Shitao Xiao, Xi Yang, Yongping Xiong, Bo Zhang, Tiejun Huang, and Zheng Liu. Mlvu: comprehensive benchmark for multi-task long video understanding. arXiv preprint arXiv:2406.04264, 2024c. Jiaxu Miao, Xiaohan Wang, Yu Wu, Wei Li, Xu Zhang, Yunchao Wei, and Yi Yang. Large-scale video panoptic segmentation in the wild: benchmark. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022, pages 2100121011, 2022. Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Radle, Chloe Rolland, Laura Gustafson, Eric Mintun, Junting Pan, Kalyan Vasudev Alwala, Nicolas Carion, Chao-Yuan Wu, Ross B. Girshick, Piotr Dollar, and Christoph Feichtenhofer. SAM 2: Segment anything in images and videos. CoRR, abs/2408.00714, 2024. Ning Xu, Linjie Yang, Yuchen Fan, Dingcheng Yue, Yuchen Liang, Jianchao Yang, and Thomas S. Huang. Youtube-vos: large-scale video object segmentation benchmark. CoRR, abs/1809.03327, 2018. Seonguk Seo, Joon-Young Lee, and Bohyung Han. URVOS: unified referring video object segmentation network with large-scale benchmark. In Computer Vision - ECCV 2020 - 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part XV, pages 208223, 2020. Jiannan Wu, Yi Jiang, Bin Yan, Huchuan Lu, Zehuan Yuan, and Ping Luo. Uniref++: Segment every reference object in spatial and temporal spaces. CoRR, abs/2312.15715, 2023d. Cilin Yan, Haochen Wang, Shilin Yan, Xiaolong Jiang, Yao Hu, Guoliang Kang, Weidi Xie, and Efstratios Gavves. Visa: Reasoning video object segmentation via large language models. arXiv preprint arXiv:2407.11325, 2024b. Zongheng Tang, Yue Liao, Si Liu, Guanbin Li, Xiaojie Jin, Hongxu Jiang, Qian Yu, and Dong Xu. Human-centric spatio-temporal video grounding with visual transformers. IEEE Trans. Circuits Syst. Video Technol., 32(12):82388249, 2022. 61 On Path to Multimodal Generalist: General-Level and General-Bench Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, and Cordelia Schmid. Tubedetr: Spatio-temporal video grounding with transformers. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022, pages 1642116432, 2022b. Zhu Zhang, Zhou Zhao, Yang Zhao, Qi Wang, Huasheng Liu, and Lianli Gao. Where does it exist: Spatio-temporal video grounding for multi-form sentences. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020, pages 1066510674, 2020. Daniel J. Butler, Jonas Wulff, Garrett B. Stanley, and Michael J. Black. naturalistic open source movie for optical flow evaluation. In Computer Vision - ECCV 2012 - 12th European Conference on Computer Vision, Florence, Italy, October 7-13, 2012, Proceedings, Part VI, pages 611625, 2012. Wenbo Hu, Xiangjun Gao, Xiaoyu Li, Sijie Zhao, Xiaodong Cun, Yong Zhang, Long Quan, and Ying Shan. Depthcrafter: Generating consistent long depth sequences for open-world videos. CoRR, abs/2409.02095, 2024d. Angela Dai, Angel X. Chang, Manolis Savva, Maciej Halber, Thomas A. Funkhouser, and Matthias Nießner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In 2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017, pages 24322443, 2017. Emanuele Palazzolo, Jens Behley, Philipp Lottes, Philippe Gigu`ere, and Cyrill Stachniss. Refusion: 3d reconstruction in dynamic environments for RGB-D cameras exploiting residuals. In 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems, IROS 2019, Macau, SAR, China, November 3-8, 2019, pages 78557862, 2019. Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we ready for autonomous driving? the KITTI vision benchmark suite. In 2012 IEEE Conference on Computer Vision and Pattern Recognition, Providence, RI, USA, June 16-21, 2012, pages 33543361, 2012. Mmvmbench. URL https://huggingface.co/zhouyik/MMVMBench. Matej Kristan, Ales Leonardis, Jiri Matas, Michael Felsberg, Roman P. Pflugfelder, Luka Cehovin Zajc, Tomas Vojır, Goutam Bhat, Alan Lukezic, Abdelrahman Eldesokey, Gustavo Fernandez, Alvaro Garcıa-Martın, Alvaro Iglesias-Arias, A. Aydin Alatan, Abel Gonzalez-Garcıa, Alfredo Petrosino, Alireza Memarmoghadam, Andrea Vedaldi, Andrej Muhic, Anfeng He, Arnold W. M. Smeulders, Asanka G. Perera, Bo Li, Boyu Chen, Changick Kim, Changsheng Xu, Changzhen Xiong, Cheng Tian, Chong Luo, Chong Sun, Cong Hao, Daijin Kim, Deepak Mishra, Deming Chen, Dong Wang, Dongyoon Wee, Efstratios Gavves, Erhan Gundogdu, Erik Velasco-Salido, Fahad Shahbaz Khan, Fan Yang, Fei Zhao, Feng Li, Francesco Battistone, George De Ath, Gorthi R. K. Sai Subrahmanyam, Guilherme Sousa Bastos, Haibin Ling, Hamed Kiani Galoogahi, Hankyeol Lee, Haojie Li, Haojie Zhao, Heng Fan, Honggang Zhang, Horst Possegger, Houqiang Li, Huchuan Lu, Hui Zhi, Huiyun Li, Hyemin Lee, Hyung Jin Chang, Isabela Drummond, Jack Valmadre, Jaime Spencer Martin, Javaan Singh Chahl, Jin Young Choi, Jing Li, Jinqiao Wang, Jinqing Qi, Jinyoung Sung, Joakim Johnander, Joao F. Henriques, Jongwon Choi, Joost van de Weijer, Jorge Rodrıguez Herranz, Jose M. Martınez, Josef Kittler, Junfei Zhuang, Junyu Gao, Klemen Grm, Lichao Zhang, Lijun Wang, Lingxiao Yang, Litu Rout, Liu Si, Luca Bertinetto, Lutao Chu, Manqiang Che, Mario Edoardo Maresca, Martin Danelljan, Ming-Hsuan Yang, Mohamed H. Abdelpakey, Mohamed S. Shehata, Myunggu Kang, Namhoon Lee, Ning Wang, Ondrej Miksik, Payman Moallem, Pablo Vicente-Monivar, Pedro Senna, Peixia Li, Philip H. S. Torr, Priya Mariam Raju, Ruihe Qian, Qiang Wang, Qin Zhou, Qing Guo, Rafael Martin Nieto, Rama Krishna Sai Subrahmanyam Gorthi, Ran Tao, Richard Bowden, Richard M. Everson, Runling Wang, Sangdoo Yun, Seokeon Choi, Sergio Vivas, Shuai Bai, Shuangping Huang, Sihang Wu, Simon Hadfield, Siwen Wang, Stuart Golodetz, Ming Tang, Tianyang Xu, Tianzhu Zhang, Tobias Fischer, Vincenzo Santopietro, Vitomir Struc, Wei Wang, Wangmeng Zuo, Wei Feng, Wei Wu, Wei Zou, Weiming Hu, Wengang Zhou, Wenjun Zeng, Xiaofan Zhang, Xiaohe Wu, Xiao-Jun Wu, Xinmei Tian, Yan Li, Yan Lu, Yee Wei Law, Yi Wu, Yiannis Demiris, Yicai Yang, Yifan Jiao, Yuhong Li, Yunhua Zhang, Yuxuan Sun, Zheng Zhang, Zheng Zhu, Zhenhua Feng, Zhihui Wang, and Zhiqun He. The sixth visual object tracking VOT2018 challenge results. In Computer Vision - ECCV 2018 Workshops - Munich, Germany, September 8-14, 2018, Proceedings, Part I, pages 353, 2018. Bin Yan, Yi Jiang, Jiannan Wu, Dong Wang, Ping Luo, Zehuan Yuan, and Huchuan Lu. Universal instance perception as object discovery and retrieval. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2023, Vancouver, BC, Canada, June 17-24, 2023, pages 1532515336, 2023. 62 On Path to Multimodal Generalist: General-Level and General-Bench Heng Fan, Liting Lin, Fan Yang, Peng Chu, Ge Deng, Sijia Yu, Hexin Bai, Yong Xu, Chunyuan Liao, and Haibin Ling. Lasot: high-quality benchmark for large-scale single object tracking. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019, pages 53745383, 2019a. Peize Sun, Jinkun Cao, Yi Jiang, Zehuan Yuan, Song Bai, Kris Kitani, and Ping Luo. Dancetrack: Multi-object tracking in uniform appearance and diverse motion. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022, pages 2096120970, 2022. Matthias Mueller, Neil Smith, and Bernard Ghanem. benchmark and simulator for UAV tracking. In Computer Vision - ECCV 2016 - 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part I, volume 9905 of Lecture Notes in Computer Science, pages 445461, 2016. Basit Alawode, Yuhang Guo, Mehnaz Ummar, Naoufel Werghi, Jorge Dias, Ajmal Mian, and Sajid Javed. Utb180: high-quality benchmark for underwater tracking. In ACCV, 2022. Alexey Dosovitskiy, Philipp Fischer, Eddy Ilg, Philip Hausser, Caner Hazirbas, Vladimir Golkov, Patrick van der Smagt, In 2015 IEEE Daniel Cremers, and Thomas Brox. Flownet: Learning optical flow with convolutional networks. International Conference on Computer Vision, ICCV 2015, Santiago, Chile, December 7-13, 2015, pages 27582766, 2015. Jadiel de Armas. Videoflow. https://github.com/videoflow/videoflow, 2019. E. Ilg, N. Mayer, T. Saikia, M. Keuper, A. Dosovitskiy, and T. Brox. Flownet 2.0: Evolution of optical flow estimation with deep networks. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Jul 2017. Roman Seidel, Andre Apitzsch, and Gangolf Hirtz. Omniflow: Human omnidirectional optical flow. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 36783681, 2021. Hao Shi, Yifan Zhou, Kailun Yang, Yaozu Ye, Xiaoting Yin, Zhe Yin, Shi Meng, and Kaiwei Wang. Panoflow: Learning optical flow for panoramic images. arXiv preprint arXiv:2202.13388, 2022a. Santiago Castro, Devamanyu Hazarika, Veronica Perez-Rosas, Roger Zimmermann, Rada Mihalcea, and Soujanya Poria. Towards multimodal sarcasm detection (an obviously perfect paper). In Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28August 2, 2019, Volume 1: Long Papers, pages 46194629, 2019. Arka Sadhu, Tanmay Gupta, Mark Yatskar, Ram Nevatia, and Aniruddha Kembhavi. Visual semantic role labeling for video understanding. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2021. zeroscope. URL https://huggingface.co/cerspense/zeroscope_v2_576w. Yaohui Wang, Xinyuan Chen, Xin Ma, Shangchen Zhou, Ziqi Huang, Yi Wang, Ceyuan Yang, Yinan He, Jiashuo Yu, Peiqing Yang, Yuwei Guo, Tianxing Wu, Chenyang Si, Yuming Jiang, Cunjian Chen, Chen Change Loy, Bo Dai, Dahua Lin, Yu Qiao, and Ziwei Liu. LAVIE: high-quality video generation with cascaded latent diffusion models. CoRR, abs/2309.15103, 2023d. Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, Da Yin, Xiaotao Gu, Yuxuan Zhang, Weihan Wang, Yean Cheng, Ting Liu, Bin Xu, Yuxiao Dong, and Jie Tang. Cogvideox: Text-to-video diffusion models with an expert transformer. CoRR, abs/2408.06072, 2024c. Laion-aesthetics. URL https://laion.ai/blog/laion-aesthetics/. Gongye Liu, Menghan Xia, Yong Zhang, Haoxin Chen, Jinbo Xing, Xintao Wang, Yujiu Yang, and Ying Shan. Stylecrafter: Enhancing stylized text-to-video generation with style adapter. CoRR, abs/2312.00330, 2023e. Max Bain, Arsha Nagrani, Gul Varol, and Andrew Zisserman. Frozen in time: joint video and image encoder for end-to-end retrieval. In IEEE International Conference on Computer Vision, 2021. Haoxin Chen, Yong Zhang, Xiaodong Cun, Menghan Xia, Xintao Wang, Chao Weng, and Ying Shan. Videocrafter2: Overcoming data limitations for high-quality video diffusion models, 2024f. On Path to Multimodal Generalist: General-Level and General-Bench Lucas Smaira, Joao Carreira, Eric Noland, Ellen Clancy, Amy Wu, and Andrew Zisserman. short note on the kinetics700-2020 human action dataset. CoRR, abs/2010.10864, 2020. Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. Ucf101: dataset of 101 human actions classes from videos in the wild. arXiv preprint arXiv:1212.0402, 2012b. Peng Dai, Xin Yu, Lan Ma, Baoheng Zhang, Jia Li, Wenbo Li, Jiajun Shen, and Xiaojuan Qi. Video demoireing with relation-based temporal consistency. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022b. Huanjing Yue, Yijia Cheng, Xin Liu, and Jingyu Yang. Recaptured raw screen image and video demoireing via channel and spatial modulations. arXiv preprint arXiv:2310.20332, 2023. Shuning Xu, Binbin Song, Xiangyu Chen, and Jiantao Zhou. Direction-aware video demoireing with temporal-guided bilateral learning. In Thirty-Eighth AAAI Conference on Artificial Intelligence, AAAI 2024, Thirty-Sixth Conference on Innovative Applications of Artificial Intelligence, IAAI 2024, Fourteenth Symposium on Educational Advances in Artificial Intelligence, EAAI 2014, February 20-27, 2024, Vancouver, Canada, pages 63606368, 2024. Huanjing Yue, Cong Cao, Lei Liao, Ronghe Chu, and Jingyu Yang. Supervised raw video denoising with benchmark dataset on dynamic scenes. In IEEE Conference on Computer Vision and Pattern Recognition, 2020. Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Arbelaez, Alexander Sorkine-Hornung, and Luc Van Gool. The 2017 davis challenge on video object segmentation. arXiv:1704.00675, 2017. Chenyang Qi, Junming Chen, Xin Yang, and Qifeng Chen. Real-time streaming video denoising with bidirectional buffers. In ACM MM, 2022. Tianfan Xue, Baian Chen, Jiajun Wu, Donglai Wei, and William T. Freeman. Video enhancement with task-oriented flow. Int. J. Comput. Vis., 127(8):11061125, 2019. Guozhen Zhang, Yuhan Zhu, Haonan Wang, Youxin Chen, Gangshan Wu, and Limin Wang. Extracting motion and appearance via inter-frame attention for efficient video frame interpolation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 56825692, 2023k."
        },
        {
            "title": "Msu",
            "content": "video colorization benchmark."
        },
        {
            "title": "URL",
            "content": "https://videoprocessing.ai/benchmarks/ video-colorization.html. Markus Hofinger, Erich Kobler, Alexander Effland, and Thomas Pock. Learned variational video color propagation. In Computer Vision ECCV 2022, Lecture Notes in Computer Science, 2022."
        },
        {
            "title": "Real",
            "content": "haze b. real-haze-video-database/. database, video"
        },
        {
            "title": "URL",
            "content": "https://qualinet.github.io/databases/video/ Jiaqi Xu, Xiaowei Hu, Lei Zhu, Qi Dou, Jifeng Dai, Yu Qiao, and Pheng-Ann Heng. Video dehazing via multi-range temporal alignment network with physical prior. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023c. Hongtao Wu, Yijun Yang, Angelica I. Aviles-Rivero, Jingjing Ren, Sixiang Chen, Haoyu Chen, and Lei Zhu. Semisupervised video desnowing network via temporal decoupling experts and distribution-driven contrastive regularization. In Computer Vision - ECCV 2024 - 18th European Conference, Milan, Italy, September 29-October 4, 2024, Proceedings, Part X, pages 7089, 2024c. Haoyu Chen, Jingjing Ren, Jinjin Gu, Hongtao Wu, Xuequan Lu, Haoming Cai, and Lei Zhu. Snow removal in video: new dataset and novel method. In IEEE/CVF International Conference on Computer Vision, ICCV 2023, Paris, France, October 1-6, 2023, pages 1316513176, 2023. Amirhosein Ghasemabadi, Muhammad Kamran Janjua, Mohammad Salameh, and Di Niu. Learning truncated causal history model for video restoration. In The Thirty-eighth Annual Conference on Neural Information Processing Systems. 64 On Path to Multimodal Generalist: General-Level and General-Bench Qiang Wen, Yue Wu, and Qifeng Chen. Video waterdrop removal via spatio-temporal fusion in driving scenes. In International Conference on Robotics and Automation (ICRA). IEEE, 2023. Hongtao Wu, Yijun Yang, Haoyu Chen, Jingjing Ren, and Lei Zhu. Mask-guided progressive network for joint raindrop and rain streak removal in videos. In Proceedings of the 31st ACM International Conference on Multimedia, pages 72167225, 2023e. Hongtao Wu, Yijun Yang, Huihui Xu, Weiming Wang, Jinni Zhou, and Lei Zhu. Rainmamba: Enhanced locality learning with state space models for video deraining. arXiv preprint arXiv:2407.21773, 2024d. Kelvin C.K. Chan, Shangchen Zhou, Xiangyu Xu, and Chen Change Loy. Investigating tradeoffs in real-world video super-resolution. In IEEE Conference on Computer Vision and Pattern Recognition, 2022. Shangchen Zhou, Peiqing Yang, Jianyi Wang, Yihang Luo, and Chen Change Loy. Upscale-A-Video: Temporal-consistent diffusion model for real-world video super-resolution. In CVPR, 2024d. Shangchen Zhou, Chongyi Li, Kelvin C.K Chan, and Chen Change Loy. ProPainter: Improving propagation and transformer for video inpainting. In Proceedings of IEEE International Conference on Computer Vision (ICCV), 2023. Seungjun Nah, Sungyong Baik, Seokil Hong, Gyeongsik Moon, Sanghyun Son, Radu Timofte, and Kyoung Mu Lee. Ntire 2019 challenge on video deblurring and super-resolution: Dataset and study. In CVPR Workshops, June 2019. Jingyun Liang, Jiezhang Cao, Yuchen Fan, Kai Zhang, Rakesh Ranjan, Yawei Li, Radu Timofte, and Luc Van Gool. Vrt: video restoration transformer. arXiv preprint arXiv:2201.12288, 2022. Shuai Yang, Yifan Zhou, Ziwei Liu, , and Chen Change Loy. Fresco: Spatial-temporal correspondence for zero-shot video translation. In CVPR, 2024d. Shuai Yang, Liming Jiang, Ziwei Liu, and Chen Change Loy. Vtoonify: Controllable high-resolution portrait video style transfer. ACM Transactions on Graphics (TOG), 41(6):115, 2022c. Steven Weinberger. Speech accent archive, 2015. URL http://accent.gmu.edu. Benjamin Elizalde, Soham Deshmukh, Mahmoud Al Ismail, and Huaming Wang. CLAP learning audio concepts from natural language supervision. In IEEE International Conference on Acoustics, Speech and Signal Processing ICASSP 2023, Rhodes Island, Greece, June 4-10, 2023, pages 15, 2023. Arsha Nagrani, Joon Son Chung, and Andrew Zisserman. Voxceleb: large-scale speaker identification dataset. In 18th Annual Conference of the International Speech Communication Association, Interspeech 2017, Stockholm, Sweden, August 20-24, 2017, pages 26162620, 2017. Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, and Abdelrahman Mohamed. Hubert: Self-supervised speech representation learning by masked prediction of hidden units. IEEE ACM Trans. Audio Speech Lang. Process., 29:34513460, 2021. Yuan Gong, Jin Yu, and James R. Glass. Vocalsound: dataset for improving human vocal sounds recognition. In IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2022, Virtual and Singapore, 23-27 May 2022, pages 151155, 2022. Loren Lugosch, Mirco Ravanelli, Patrick Ignoto, Vikrant Singh Tomar, and Yoshua Bengio. Speech model pre-training for end-to-end spoken language understanding. In 20th Annual Conference of the International Speech Communication Association, Interspeech 2019, Graz, Austria, September 15-19, 2019, pages 814818, 2019. Pete Warden. Speech commands: dataset for limited-vocabulary speech recognition. CoRR, abs/1804.03209, 2018. Bin Wang, Meishan Zhang, Hao Fei, Yu Zhao, Bobo Li, Shengqiong Wu, Wei Ji, and Min Zhang. Speechee: novel benchmark for speech event extraction. In Proceedings of the 32nd ACM International Conference on Multimedia, MM 2024, Melbourne, VIC, Australia, 28 October 2024 - 1 November 2024, pages 1044910458, 2024h. 65 On Path to Multimodal Generalist: General-Level and General-Bench Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe Kazemzadeh, Emily Mower, Samuel Kim, Jeannette N. Chang, Sungbok Lee, and Shrikanth S. Narayanan. IEMOCAP: interactive emotional dyadic motion capture database. Lang. Resour. Evaluation, 42(4):335359, 2008. Sanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo Chen, Jinyu Li, Naoyuki Kanda, Takuya Yoshioka, Xiong Xiao, Jian Wu, Long Zhou, Shuo Ren, Yanmin Qian, Yao Qian, Jian Wu, Michael Zeng, Xiangzhan Yu, and Furu Wei. Wavlm: Large-scale self-supervised pre-training for full stack speech processing. IEEE J. Sel. Top. Signal Process., 16(6):15051518, 2022b. Gtzan dataset - music genre classification. URL https://www.kaggle.com/datasets/andradaolteanu/ gtzan-dataset-music-genre-classification. Matthew C. McCallum, Filip Korzeniowski, Sergio Oramas, Fabien Gouyon, and Andreas F. Ehmann. Supervised and unsupervised learning of audio representations for music understanding. In Proceedings of the 23rd International Society for Music Information Retrieval Conference, ISMIR 2022, Bengaluru, India, December 4-8, 2022, pages 256263, 2022. Jesse Engel, Cinjon Resnick, Adam Roberts, Sander Dieleman, Douglas Eck, Karen Simonyan, and Mohammad Norouzi. Neural audio synthesis of musical notes with wavenet autoencoders, 2017. Julia Wilkins, Prem Seetharaman, Alison Wahl, and Bryan Pardo. Vocalset: singing voice dataset. In Proceedings of the 19th International Society for Music Information Retrieval Conference, ISMIR 2018, Paris, France, September 23-27, 2018, pages 468474, 2018. Konstantinos Drossos, Samuel Lipping, and Tuomas Virtanen. Clotho: an audio captioning dataset. In 2020 IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2020, Barcelona, Spain, May 4-8, 2020, pages 736740, 2020. Minkyu Kim, Kim Sung-Bin, and Tae-Hyun Oh. Prefix tuning for automated audio captioning. In IEEE International Conference on Acoustics, Speech and Signal Processing ICASSP 2023, Rhodes Island, Greece, June 4-10, 2023, pages 15, 2023a. Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee, and Gunhee Kim. Audiocaps: Generating captions for audios in the wild. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pages 119132, 2019. Yuan Gong, Hongyin Luo, Alexander H. Liu, Leonid Karlinsky, and James R. Glass. Listen, think, and understand. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024, 2024. Samuel Lipping, Parthasaarathy Sudarsanam, Konstantinos Drossos, and Tuomas Virtanen. Clotho-aqa: crowdsourced dataset for audio question answering. In 30th European Signal Processing Conference, EUSIPCO 2022, Belgrade, Serbia, August 29 - Sept. 2, 2022, pages 11401144, 2022. Guangyao Li, Yixin Xu, and Di Hu. Multi-scale attention for audio question answering. In 24th Annual Conference of the International Speech Communication Association, Interspeech 2023, Dublin, Ireland, August 20-24, 2023, pages 34423446, 2023g. Dan Stowell, Yannis Stylianou, Mike Wood, Hanna Pamula, and Herve Glotin. Automatic acoustic detection of birds through deep learning: the first bird audio detection challenge. CoRR, abs/1807.05812, 2018. Animal sound dataset, b. URL https://github.com/YashNita/Animal-Sound-Dataset. Annamaria Mesaros, Toni Heittola, and Tuomas Virtanen. TUT database for acoustic scene classification and sound event detection. In 24th European Signal Processing Conference, EUSIPCO 2016, Budapest, Hungary, August 29 - September 2, 2016, pages 11281132, 2016. Karol J. Piczak. ESC: dataset for environmental sound classification. In Proceedings of the 23rd Annual ACM Conference on Multimedia Conference, MM 15, Brisbane, Australia, October 26 - 30, 2015, pages 10151018, 2015. On Path to Multimodal Generalist: General-Level and General-Bench Ilaria Manco, Benno Weck, Seungheon Doh, Minz Won, Yixiao Zhang, Dmitry Bogdanov, Yusong Wu, Ke Chen, Philip Tovstogan, Emmanouil Benetos, Elio Quinton, Gyorgy Fazekas, and Juhan Nam. The song describer dataset: corpus of audio captions for music-and-language evaluation. In Machine Learning for Audio Workshop at NeurIPS 2023, 2023. Siyuan Hou, Shansong Liu, Ruibin Yuan, Wei Xue, Ying Shan, Mangsuo Zhao, and Chao Zhang. Editing music with melody and text: Using controlnet for diffusion transformer. CoRR, abs/2410.05151, 2024. Keon Lee, Kyumin Park, and Daeyoung Kim. Dailytalk: Spoken dialogue dataset for conversational text-to-speech. In IEEE International Conference on Acoustics, Speech and Signal Processing ICASSP 2023, Rhodes Island, Greece, June 4-10, 2023, pages 15, 2023. Kun Zhou, Berrak Sisman, Rui Liu, and Haizhou Li. Seen and unseen emotional style transfer for voice conversion with new emotional speech dataset. In ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 920924. IEEE, 2021. Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. Librispeech: An ASR corpus based on public domain audio books. In 2015 IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2015, South Brisbane, Queensland, Australia, April 19-24, 2015, pages 52065210, 2015. Xin Zhang, Dong Zhang, Shimin Li, Yaqian Zhou, and Xipeng Qiu. Speechtokenizer: Unified speech tokenizer for speech language models, 2023l. Wenhao Guan, Yishuang Li, Tao Li, Hukai Huang, Feng Wang, Jiayan Lin, Lingyan Huang, Lin Li, and Qingyang Hong. MM-TTS: multi-modal prompt based style transfer for expressive text-to-speech synthesis. In Thirty-Eighth AAAI Conference on Artificial Intelligence, AAAI 2024, Thirty-Sixth Conference on Innovative Applications of Artificial Intelligence, IAAI 2024, Fourteenth Symposium on Educational Advances in Artificial Intelligence, EAAI 2014, February 20-27, 2024, Vancouver, Canada, pages 1811718125, 2024. Haohe Liu, Yi Yuan, Xubo Liu, Xinhao Mei, Qiuqiang Kong, Qiao Tian, Yuping Wang, Wenwu Wang, Yuxuan Wang, and Mark D. Plumbley. Audioldm 2: Learning holistic audio generation with self-supervised pretraining. IEEE ACM Trans. Audio Speech Lang. Process., 32:28712883, 2024g. David Harwath and James Glass. Deep multimodal semantic embeddings for speech and images, 2015. Minsu Kim, Jeongsoo Choi, Soumi Maiti, Jeong Hun Yeo, Shinji Watanabe, and Yong Man Ro. Towards practical and efficient image-to-speech captioning with vision-language pre-training and multi-modal tokens, 2023b. Lin Zhang, Shentong Mo, Yijing Zhang, and Pedro Morgado. Audio-synchronized visual animation, 2024g. Simian Luo, Chuanhao Yan, Chenxu Hu, and Hang Zhao. Diff-foley: Synchronized video-to-audio synthesis with latent diffusion models, 2023. Yamagishi Junichi, Veaux Christophe, and MacDonald Kirsten. Cstr vctk corpus: English multi-speaker corpus for cstr voice cloning toolkit, 2019. Rong Ye, Chengqi Zhao, Tom Ko, Chutong Meng, Tao Wang, Mingxuan Wang, and Jun Cao. Gigast: 10,000-hour pseudo speech translation corpus, 2023b. Changhan Wang, Juan Pino, Anne Wu, and Jiatao Gu. CoVoST: diverse multilingual speech-to-text translation corpus. In Proceedings of the Twelfth Language Resources and Evaluation Conference, pages 41974203, May 2020b. Leopold Crestel and Philippe Esling. Live orchestral piano, system for real-time orchestral music generation, 2017. Ziyu Wang*, Ke Chen*, Junyan Jiang, Yiyi Zhang, Maoran Xu, Shuqi Dai, Guxian Bin, and Gus Xia. Pop909: pop-song dataset for music arrangement generation. In Proceedings of 21st International Conference on Music Information Retrieval, ISMIR, 2020. Yi-Jen Shih, Shih-Lun Wu, Frank Zalkow, Meinard Muller, and Yi-Hsuan Yang. Theme transformer: Symbolic music generation with theme-conditioned transformer. IEEE Trans. Multim., 25:34953508, 2023. 67 On Path to Multimodal Generalist: General-Level and General-Bench Lichao Zhang, Ruiqi Li, Shoutong Wang, Liqun Deng, Jinglin Liu, Yi Ren, Jinzheng He, Rongjie Huang, Jieming Zhu, Xiao Chen, and Zhou Zhao. M4singer: multi-style, multi-singer and musical score provided mandarin singing corpus. In Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2022. Jinglin Liu, Chengxi Li, Yi Ren, Feiyang Chen, and Zhou Zhao. Diffsinger: Singing voice synthesis via shallow diffusion In Thirty-Sixth AAAI Conference on Artificial Intelligence, AAAI 2022, Thirty-Fourth Conference on mechanism. Innovative Applications of Artificial Intelligence, IAAI 2022, The Twelveth Symposium on Educational Advances in Artificial Intelligence, EAAI 2022 Virtual Event, February 22 - March 1, 2022, pages 1102011028, 2022. Seth* Forsgren and Hayk* Martiros. Riffusion - Stable diffusion for real-time music generation. 2022. URL https: //riffusion.com/about. Sifei Li, Yuxin Zhang, Fan Tang, Chongyang Ma, Weiming Dong, and Changsheng Xu. Music style transfer with time-varying inversion of diffusion models. In Thirty-Eighth AAAI Conference on Artificial Intelligence, AAAI 2024, Thirty-Sixth Conference on Innovative Applications of Artificial Intelligence, IAAI 2024, Fourteenth Symposium on Educational Advances in Artificial Intelligence, EAAI 2014, February 20-27, 2024, Vancouver, Canada, pages 547555, 2024l. Ondrej Cıfka, Umut Simsekli, and Gael Richard. Groove2groove: One-shot music style transfer with supervision from synthetic data. IEEE ACM Trans. Audio Speech Lang. Process., 28:26382650, 2020. Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Linguang Zhang, Xiaoou Tang, and Jianxiong Xiao. 3d shapenets: deep representation for volumetric shapes. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2015, Boston, MA, USA, June 7-12, 2015, pages 19121920, 2015. Dingkang Liang, Tianrui Feng, Xin Zhou, Yumeng Zhang, Zhikang Zou, and Xiang Bai. Parameter-efficient fine-tuning in spectral domain for point cloud learning. arXiv preprint arXiv:2410.08114, 2024. Ayush Jain, Pushkal Katara, Nikolaos Gkanatsios, Adam W. Harley, Gabriel Sarch, Kriti Aggarwal, Vishrav Chaudhary, and Katerina Fragkiadaki. Odin: single model for 2d and 3d perception, 2024. Jens Behley, Martin Garbade, Andres Milioto, Jan Quenzel, Sven Behnke, Cyrill Stachniss, and Jurgen Gall. Semantickitti: dataset for semantic scene understanding of lidar sequences. In 2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019, Seoul, Korea (South), October 27 - November 2, 2019, pages 92969306, 2019. OpenPCDet Development Team. Openpcdet: An open-source toolbox for 3d object detection from point clouds. https: //github.com/open-mmlab/OpenPCDet, 2020. Madhu Vankadari Andrew Markham Niki Trigoni Sangyun Shin, Kaichen Zhou. Spherical mask: Coarse-to-fine 3d point cloud instance segmentation with spherical representation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. Pierre Dellenbach, Jean-Emmanuel Deschaud, Bastien Jacquet, and Francois Goulette. Ct-icp: Real-time elastic lidar odometry with loop closure, 2021. Angel X. Chang, Thomas A. Funkhouser, Leonidas J. Guibas, Pat Hanrahan, Qi-Xing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, Jianxiong Xiao, Li Yi, and Fisher Yu. Shapenet: An information-rich 3d model repository. CoRR, abs/1512.03012, 2015. Jinyoung Park, Sanghyeok Lee, Sihyeon Kim, Yunyang Xiong, and Hyunwoo J. Kim. Self-positioning point-based transformer for point cloud understanding. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2023, Vancouver, BC, Canada, June 17-24, 2023, pages 2181421823, 2023. Tianwei Yin, Xingyi Zhou, and Philipp Krahenbuhl. Center-based 3d object detection and tracking. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual, June 19-25, 2021, pages 1178411793, 2021. Paul Guerrero, Yanir Kleiman, Maks Ovsjanikov, and Niloy J. Mitra. PCPNet: Learning local shape properties from raw point clouds. Computer Graphics Forum, 37(2):7585, 2018. On Path to Multimodal Generalist: General-Level and General-Bench Qing Li, Huifang Feng, Kanle Shi, Yue Gao, Yi Fang, Yu-Shen Liu, and Zhizhong Han. Shs-net: Learning signed hyper surfaces for oriented normal estimation of point clouds. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2023, Vancouver, BC, Canada, June 17-24, 2023, pages 1359113600, 2023h. Zhijian Liu, Haotian Tang, Alexander Amini, Xinyu Yang, Huizi Mao, Daniela L. Rus, and Song Han. Bevfusion: Multi-task multi-sensor fusion with unified birds-eye view representation. In IEEE International Conference on Robotics and Automation, ICRA 2023, London, UK, May 29 - June 2, 2023, pages 27742781, 2023f. Daichi Azuma, Taiki Miyanishi, Shuhei Kurita, and Motoaki Kawanabe. Scanqa: 3d question answering for spatial scene understanding. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022, pages 1910719117, 2022. Yunze Man, Liang-Yan Gui, and Yu-Xiong Wang. Situational awareness matters in 3d vision language reasoning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1367813688, 2024. Xiaojian Ma, Silong Yong, Zilong Zheng, Qing Li, Yitao Liang, Song-Chun Zhu, and Siyuan Huang. SQA3D: situated question answering in 3d scenes. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023, 2023. Matthias Plappert, Christian Mandery, and Tamim Asfour. The KIT motion-language dataset. Big Data, 4(4):236252, 2016. Karim Radouane, Andon Tchechmedjiev, Sylvie Ranwez, and Julien Lagarde. Guided attention for interpretable motion captioning. In Proceedings of the 35th British Machine Vision Conference, 2024. Wentao Yuan, Tejas Khot, David Held, Christoph Mertz, and Martial Hebert. Pcn: Point completion network. In 2018 International Conference on 3D Vision (3DV), pages 728737, 2018. Xumin Yu, Yongming Rao, Ziyi Wang, Zuyan Liu, Jiwen Lu, and Jie Zhou. Pointr: Diverse point cloud completion with geometry-aware transformers. In 2021 IEEE/CVF International Conference on Computer Vision, ICCV 2021, Montreal, QC, Canada, October 10-17, 2021, pages 1247812487, 2021. Songyou Peng, Michael Niemeyer, Lars Mescheder, Marc Pollefeys, and Andreas Geiger. Convolutional occupancy networks. In European Conference on Computer Vision (ECCV), 2020. Alex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela Mishkin, and Mark Chen. Point-e: system for generating 3d point clouds from complex prompts. CoRR, abs/2212.08751, 2022. Xianghui Yang, Huiwen Shi, Bowen Zhang, Fan Yang, Jiacheng Wang, Hongxu Zhao, Xinhai Liu, Xinzhou Wang, Qingxiang Lin, Jiaao Yu, Lifu Wang, Zhuo Chen, Sicong Liu, Yuhong Liu, Yong Yang, Di Wang, Jie Jiang, and Chunchao Guo. Tencent hunyuan3d-1.0: unified framework for text-to-3d and image-to-3d generation, 2024e. Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: universe of annotated 3d objects. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2023, Vancouver, BC, Canada, June 17-24, 2023, pages 1314213153, 2023. Qian-Yi Zhou, Jaesik Park, and Vladlen Koltun. Open3D: modern library for 3D data processing. arXiv:1801.09847, 2018. Chuan Guo, Yuxuan Mu, Muhammad Gohar Javed, Sen Wang, and Li Cheng. Momask: Generative masked modeling of 3d human motions. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2024, Seattle, WA, USA, June 16-22, 2024, pages 19001910, 2024b. Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. Commonsenseqa: question answering challenge targeting commonsense knowledge. In Proceedings of NAACL, pages 41494158, 2019. 69 On Path to Multimodal Generalist: General-Level and General-Bench Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Y. Zhao, Yanping Huang, Andrew M. Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. Scaling instruction-finetuned language models. CoRR, abs/2210.11416, 2022. Zhijing Jin, Jiarui Liu, Zhiheng Lyu, Spencer Poff, Mrinmaya Sachan, Rada Mihalcea, Mona T. Diab, and Bernhard Scholkopf. Can large language models infer causation from correlation? In Proceedings of ICLR, 2024. Anselmo Penas, Eduard H. Hovy, Pamela Forner, Alvaro Rodrigo, Richard F. E. Sutcliffe, and Roser Morante. QA4MRE 2011-2013: Overview of question answering for machine reading evaluation. In Proceedings of Information Access Evaluation. Multilinguality, Multimodality, and Visualization - 4th International Conference of the CLEF Initiative, CLEF 2013, Valencia, Spain, September 23-26, 2013. Proceedings, pages 303320, 2013. Counterfactual reasoning capacity of large language models. URL https://github.com/SushovitNanda/ Counterfactual-Reasoning-Capacity-of-Large-Language-Models. Anna Gladkova, Aleksandr Drozd, and Satoshi Matsuoka. Analogy-based detection of morphological and semantic relations with word embeddings: what works and what doesnt. In Proceedings of NAACL, pages 815, 2016. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. HotpotQA: dataset for diverse, explainable multi-hop question answering. In Conference on Empirical Methods in Natural Language Processing (EMNLP), 2018. Lianhui Qin, Aditya Gupta, Shyam Upadhyay, Luheng He, Yejin Choi, and Manaal Faruqui. TimeDial: Temporal Commonsense Reasoning in Dialog. In Proc. of ACL, 2021. Zhengxiang Shi, Qiang Zhang, and Aldo Lipani. Stepgame: new benchmark for robust multi-hop spatial reasoning in texts. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 1132111329, Jun. 2022b. Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. Program induction by rationale generation: Learning to solve and explain algebraic word problems. In Proceedings of ACL, pages 158167, 2017. Dan Hendrycks, Collin Burns, Steven Basart, Andrew Critch, Jerry Li, Dawn Song, and Jacob Steinhardt. Aligning AI with shared human values. In Proceedings of ICLR, 2021. Haoxi Zhong, Chaojun Xiao, Cunchao Tu, Tianyang Zhang, Zhiyuan Liu, and Maosong Sun. JEC-QA: legal-domain question answering dataset. In Proceedings of AAAI, pages 97019708, 2020. Reid Pryzant, Richard Diehl Martinez, Nathan Dass, Sadao Kurohashi, Dan Jurafsky, and Diyi Yang. Automatically neutralizing subjective bias in text. In Proceedings of AAAI, pages 480489, 2020. Yinhan Liu. Roberta: robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 364, 2019."
        },
        {
            "title": "Offensive",
            "content": "classification."
        },
        {
            "title": "URL",
            "content": "https://www.kaggle.com/datasets/mrmorj/ hate-speech-and-offensive-language-dataset. Spam detection. URL https://www.kaggle.com/datasets/jackksoncsie/spam-email-dataset. Fake news. URL https://kaggle.com/competitions/fake-news. James ONeill, Polina Rozenshtein, Ryuichi Kiryo, Motoko Kubota, and Danushka Bollegala. wish would have loved this one, but didnt - multilingual dataset for counterfactual detection in product review. In Proceedings of EMNLP, pages 70927108, 2021. Biomedical question answering (biomedical qa). URL http://participants-area.bioasq.org/datasets/. Yunxiang Li, Zihan Li, Kai Zhang, Ruilong Dan, Steve Jiang, and You Zhang. Chatdoctor: medical chat model fine-tuned on large language model meta-ai (llama) using medical domain knowledge. Cureus, 15(6), 2023i. Stack overflow data, booktitle = kaggle, url =https://www.kaggle.com/datasets/stackoverflow/stackoverflow/data. 70 On Path to Multimodal Generalist: General-Level and General-Bench Engineering question answering, booktitle = bath, url =https://researchportal.bath.ac.uk/en/organisations/department-ofmechanical-engineering/datasets/. Johannes Welbl, Nelson F. Liu, and Matt Gardner. Crowdsourcing multiple choice science questions. In Proceedings of EMNLP, pages 94106, 2017. Pranav Rajpurkar, Robin Jia, and Percy Liang. Know what you dont know: Unanswerable questions for squad. In Proceedings of ACL, pages 784789, 2018. Maarten Sap, Hannah Rashkin, Derek Chen, Ronan Le Bras, and Yejin Choi. Social iqa: Commonsense reasoning about social interactions. In Proceedings of EMNLP, pages 44624472, 2019. Philosophical question answering, booktitle = huggingface, url =https://huggingface.co/datasets/sayhan/strix-philosophyqaa. Tom Kenter, Llion Jones, and Daniel Hewlett. Byte-level machine reading across morphologically varied languages. In Proceedings of AAAI, pages 58205827, 2018. Wenhan Xiong, Jiawei Wu, Hong Wang, Vivek Kulkarni, Mo Yu, Shiyu Chang, Xiaoxiao Guo, and William Yang Wang. TWEETQA: social media focused question answering dataset. In Proceedings of ACL, pages 50205031, 2019. Mandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke Zettlemoyer. Triviaqa: large scale distantly supervised challenge dataset for reading comprehension. In Proceedings of ACL, pages 16011611, 2017. Siva Reddy, Danqi Chen, and Christopher D. Manning. Coqa: conversational question answering challenge. Trans. Assoc. Comput. Linguistics, 7:249266, 2019. Panupong Pasupat and Percy Liang. Compositional semantic parsing on semi-structured tables. In Proceedings of ACL, pages 14701480, 2015. Multilingual question answering. URL https://huggingface.co/datasets/crodri/multilingual_qa. Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. mT5: massively multilingual pre-trained text-to-text transformer. In Proceedings of the NAACL, pages 483498, 2021. Aida Amini, Saadia Gabriel, Shanchuan Lin, Rik Koncel-Kedziorski, Yejin Choi, and Hannaneh Hajishirzi. Mathqa: Towards interpretable math word problem solving with operation-based formalisms. In Proceedings of NAACL, pages 23572367, 2019. Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio Blanco, Colin B. Clement, Dawn Drain, Daxin Jiang, Duyu Tang, Ge Li, Lidong Zhou, Linjun Shou, Long Zhou, Michele Tufano, Ming Gong, Ming Zhou, Nan Duan, Neel Sundaresan, Shao Kun Deng, Shengyu Fu, and Shujie Liu. Codexglue: machine learning benchmark dataset for code understanding and generation. In Proceedings of NeurIPS, 2021b. Yue Wang, Weishi Wang, Shafiq Joty, and Steven CH Hoi. Codet5: Identifier-aware unified pre-trained encoder-decoder models for code understanding and generation. arXiv preprint arXiv:2109.00859, 2021. Mostly basic python problems dataset. URL https://github.com/google-research/google-research/ tree/master/mbpp. Naman Goyal, Cynthia Gao, Vishrav Chaudhary, Peng-Jen Chen, Guillaume Wenzek, Da Ju, Sanjana Krishnan, MarcAurelio Ranzato, Francisco Guzman, and Angela Fan. The flores-101 evaluation benchmark for low-resource and multilingual machine translation. Trans. Assoc. Comput. Linguistics, 10:522538, 2022. Abigail See, Peter J. Liu, and Christopher D. Manning. Get to the point: Summarization with pointer-generator networks. In Proceedings of ACL, pages 10731083, 2017. Mike Lewis. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. arXiv preprint arXiv:1910.13461, 2019. 71 On Path to Multimodal Generalist: General-Level and General-Bench Shashi Narayan, Shay B. Cohen, and Mirella Lapata. Dont give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization. In Proceedings of EMNLP, pages 17971807, 2018. Alexander R. Fabbri, Irene Li, Tianwei She, Suyi Li, and Dragomir R. Radev. Multi-news: large-scale multi-document summarization dataset and abstractive hierarchical model. In Proceedings of ACL, pages 10741084, 2019. Yanran Li, Hui Su, Xiaoyu Shen, Wenjie Li, Ziqiang Cao, and Shuzi Niu. Dailydialog: manually labelled multi-turn dialogue dataset. In Proceedings of Proceedings of the Eighth International Joint Conference on Natural Language Processing, IJCNLP 2017, Taipei, Taiwan, November 27 - December 1, 2017 - Volume 1: Long Papers, pages 986995, 2017. Zhang. Dialogpt: Large-scale generative pre-training for conversational response generation. arXiv preprint arXiv:1911.00536, 2019. Ankur P. Parikh, Xuezhi Wang, Sebastian Gehrmann, Manaal Faruqui, Bhuwan Dhingra, Diyi Yang, and Dipanjan Das. Totto: controlled table-to-text generation dataset. In Proceedings of EMNLP, pages 11731186, 2020. Kalpesh Krishna, John Wieting, and Mohit Iyyer. Reformulating unsupervised style transfer as paraphrase generation. In Proceedings of EMNLP, pages 737762, 2020. Story generation. URL https://huggingface.co/datasets/qwedsacf/story-generation. Grammar correction. URL https://www.kaggle.com/datasets/satishgunjal/grammar-correction. Abhimanyu Das, Weihao Kong, Rajat Sen, and Yichen Zhou. decoder-only foundation model for time-series forecasting. arXiv preprint arXiv:2310.10688, 2023."
        },
        {
            "title": "Topic",
            "content": "classification."
        },
        {
            "title": "URL",
            "content": "https://www.kaggle.com/datasets/thedevastator/ the-trec-question-classification-dataset-a-longi/data. Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. large annotated corpus for learning natural language inference. In Proceedings of EMNLP, pages 632642, 2015. Sentence similarity detection. URL https://www.kaggle.com/competitions/quora-question-pairs/ data. Intent detection. URL https://github.com/sz128/slot_filling_and_intent_detection_of_SLU/ tree/master/data/atis-2. Saif M. Mohammad, Svetlana Kiritchenko, Parinaz Sobhani, Xiaodan Zhu, and Colin Cherry. Semeval-2016 task 6: Detecting stance in tweets. In Proceedings of SemEval, pages 3141, 2016. Personality analysis. URL https://github.com/hjian42/automatic-personality-prediction/ tree/master/data/Essays. Orion Weller and Kevin D. Seppi. Humor detection: transformer gets the last laugh. In Proceedings of EMNLP, pages 36193623, 2019. Rishabh Misra and Prahal Arora. Sarcasm detection using news headlines dataset. AI Open, 4:1318, 2023. Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Y. Ng, and Christopher Potts. Recursive deep models for semantic compositionality over sentiment treebank. In Proceedings of EMNLP, pages 16311642, 2013."
        },
        {
            "title": "Mental",
            "content": "detection. automatic-personality-prediction/tree/master/data/Essays. toxicity health"
        },
        {
            "title": "URL",
            "content": "https://github.com/hjian42/ Pekka Malo, Ankur Sinha, Pekka J. Korhonen, Jyrki Wallenius, and Pyry Takala. Good debt or bad debt: Detecting semantic orientations in economic texts. J. Assoc. Inf. Sci. Technol., 65(4):782796, 2014. 72 On Path to Multimodal Generalist: General-Level and General-Bench Minjin Choi, Sunkyung Lee, Eunseong Choi, Heesoo Park, Junhyuk Lee, Dongwon Lee, and Jongwuk Lee. Melbert: Metaphor detection via contextualized late interaction using metaphorical identification theories. In Proceedings of NAACL, pages 17631773, 2021. Maria Pontiki, Dimitris Galanis, Haris Papageorgiou, Suresh Manandhar, and Ion Androutsopoulos. Semeval-2015 task 12: Aspect based sentiment analysis. In Proceedings of SemEval, pages 486495, 2015. Zhifang Fan, Zhen Wu, Xin-Yu Dai, Shujian Huang, and Jiajun Chen. Target-oriented opinion words extraction with target-fused neural sequence labeling. In Proceedings of NAACL, pages 25092518, 2019b. Shaowei Chen, Jie Liu, Yu Wang, Wenzheng Zhang, and Ziming Chi. Synchronous double-channel recurrent network for aspect-opinion pair extraction. In Proceedings of ACL, pages 65156524, 2020. Lu Xu, Hao Li, Wei Lu, and Lidong Bing. Position-aware tagging for aspect sentiment triplet extraction. In Proceedings of EMNLP, pages 23392349, 2020. Maria Pontiki, Dimitris Galanis, Haris Papageorgiou, Ion Androutsopoulos, Suresh Manandhar, Mohammad Al-Smadi, Mahmoud Al-Ayyoub, Yanyan Zhao, Bing Qin, Orphee De Clercq, Veronique Hoste, Marianna Apidianaki, Xavier Tannier, Natalia V. Loukachevitch, Evgeniy V. Kotelnikov, Nuria Bel, Salud Marıa Jimenez-Zafra, and Gulsen Eryigit. Semeval-2016 task 5: Aspect based sentiment analysis. In Proceedings of SemEval, pages 1930, 2016. Bobo Li, Hao Fei, Fei Li, Yuhan Wu, Jinsong Zhang, Shengqiong Wu, Jingye Li, Yijiang Liu, Lizi Liao, Tat-Seng Chua, and Donghong Ji. Diaasq: benchmark of conversational aspect-based sentiment quadruple analysis. In Findings of ACL, pages 1344913467, 2023j. Soccer sentiment classification. URL https://github.com/Mr-Chang95/FIFA-Sentiment-Analysis. Ziheng Liu, Rui Xia, and Jianfei Yu. Comparative opinion quintuple extraction from product reviews. In Proceedings of EMNLP, pages 39553965, 2021. Qi Zhang, Zhijia Chen, Huitong Pan, Cornelia Caragea, Longin Jan Latecki, and Eduard Dragut. Scier: An entity and relation extraction dataset for datasets, methods, and tasks in scientific documents. In Proceedings of EMNLP, pages 1308313100, 2024h. Jingye Li, Hao Fei, Jiang Liu, Shengqiong Wu, Meishan Zhang, Chong Teng, Donghong Ji, and Fei Li. Unified named entity recognition as word-word relation classification. In proceedings of the AAAI conference on artificial intelligence, volume 36, pages 1096510973, 2022e. Temporal ner. URL https://github.com/paramitamirza/Causal-TimeBank/tree/main. Pathology ner. URL https://github.com/pathology-dynamics/trialsieve/tree/main. Cybersecurity ner. URL https://github.com/aiforsec/CyNER. Geological ner. URL https://github.com/BritishGeologicalSurvey/geo-ner-model. Prathamesh Kalamkar, Astha Agarwal, Aman Tiwari, Smita Gupta, Saurabh Karn, and Vivek Raghavan. Named entity recognition in indian court judgments. In Proceedings of EMNLP, pages 184193, 2022. Erik F. Tjong Kim Sang and Fien De Meulder. Introduction to the conll-2003 shared task: Language-independent named entity recognition. In Proceedings of NAACL, pages 142147, 2003. Chemical named entity recognition, a. URL https://www.kaggle.com/datasets/abhinavwalia95/ chemdner-iob-annotated-chemical-named-etities?select=training.csv. Disease-ner. URL https://github.com/Megha-Bose/Disease-NER. Yuhao Zhang, Victor Zhong, Danqi Chen, Gabor Angeli, and Christopher D. Manning. Position-aware attention and supervised data improve slot filling. In Proceedings of EMNLP, pages 3545, 2017. Yuan Yao, Deming Ye, Peng Li, Xu Han, Yankai Lin, Zhenghao Liu, Zhiyuan Liu, Lixin Huang, Jie Zhou, and Maosong Sun. Docred: large-scale document-level relation extraction dataset. In Proceedings of ACL, pages 764777, 2019. On Path to Multimodal Generalist: General-Level and General-Bench Harsha Gurulingappa, Abdul Mateen Rajput, Angus Roberts, Juliane Fluck, Martin Hofmann-Apitius, and Luca Toldo. Development of benchmark corpus to support the automatic extraction of drug-related adverse effects from medical case reports. J. Biomed. Informatics, 45(5):885892, 2012. Protein-protein interaction extraction (ppi)."
        },
        {
            "title": "URL",
            "content": "https://github.com/BNLNLP/ PPI-Relation-Extraction/tree/main/datasets/PPI/type_annotation/Typed_PPI. Drug-drug interaction (ddi). URL https://github.com/BNLNLP/PPI-Relation-Extraction/tree/ main/datasets/DDI_BLURB. Genetic association datebase. URL https://github.com/BNLNLP/PPI-Relation-Extraction/tree/ main/datasets/GAD_BLURB. Chemical-protein extraction, PPI-Relation-Extraction/tree/main/datasets/ChemProt_BLURB. relationship"
        },
        {
            "title": "URL",
            "content": "b. https://github.com/BNLNLP/ Event trigger detection. URL https://catalog.ldc.upenn.edu/LDC2020T13. Semantic role labeling. URL https://catalog.ldc.upenn.edu/LDC2013T19. Abstract meaning representation. URL https://catalog.ldc.upenn.edu/LDC2017T10. Dependency parsing. URL https://catalog.ldc.upenn.edu/LDC2013T19. Part-of-speech. URL https://github.com/PritK99/POS-Tagging/tree/main. 74 On Path to Multimodal Generalist: General-Level and General-Bench Extension on General-Bench Dataset This part provides an extension to our General-Bench dataset. A.1 Evaluation Metrics Metric List. Since all tasks in General-Bench retain their original task definitions without altering the output or prediction format, our evaluation methods vary according to the nature of different tasks and data. Table 17 summarizes the evaluation metrics and methods used across all tasks. Table 17: Overview of all the evaluation metrics in General-Bench. means the higher the better performance, and vice versa for . # Metric General 1 Acc"
        },
        {
            "title": "Representative Tasks",
            "content": "[0,1] Accuracy is defined as the ratio of correctly classified instances to the total number"
        },
        {
            "title": "Classification",
            "content": "of instances."
        },
        {
            "title": "2 Macro-Acc↑",
            "content": "[0,1] Macro-Acc evaluates how well model performs on average across all classes,"
        },
        {
            "title": "3 EM-Acc↑",
            "content": "[0,1] regardless of class imbalance. Exact Match Accuracy evaluates the percentage of predictions that are exactly the same as their corresponding references."
        },
        {
            "title": "4 AP↑",
            "content": "[0,1] AP, Average Precision, is metric used to evaluate the performance of object detection tasks, reflecting the overall precision-recall trade-off across multiple thresholds. QA, machine translation, or summarization Anomaly Detection 5 mAP [0,1] mAP, Mean Average Precision, is the mean of Average Precision values across all 2D/3D Detection F1 6 7 Micro-F1 8 AUC queries or instances: F1 score is the harmonic mean of Precision and Recall. [0,1] [0,1] Micro-F1 score is the harmonic mean of the Micro-averaged precision and recall. [0,1] AUC is used in binary classification tasks and measures the area under the ROC curve."
        },
        {
            "title": "QA\nClassification\nImage Generation",
            "content": "It represents the models ability to distinguish between classes. Ranking-related 9 R@k [0,1] R@k measures the Recall rate at the top results in tasks like image retrieval, where the true positive must appear within the top predicted results."
        },
        {
            "title": "10 AP@k↑",
            "content": "[0,1] AP@k is the Average Precision calculated at an IoU threshold of (k1). This metric is typically used when higher overlap between retrieved items and ground truth items is required. Image Scene Graph Parsing Object Detection 11 mAP@k [0,1] mAP@k refers to the mean Average Precision where the Intersection over Union"
        },
        {
            "title": "12 EM@1↑",
            "content": "[0,1] (IoU) threshold is set to (k1). Exact Match at 1 evaluates the proportion of instances for which the models top prediction exactly matches the correct answer. 3D Question Answering"
        },
        {
            "title": "13 ANLS↑",
            "content": "[0,1] ANLS, Average Normalized Levenshtein Similarity, measures how well model"
        },
        {
            "title": "OCR",
            "content": "ranks items in list based on their relevance to query. Regression-related 14 MAE [0,) MAE, Mean Absolute Error, measures the average of the absolute differences between the predicted values and the actual values. Its typically used in regression tasks."
        },
        {
            "title": "16 MSE ↓",
            "content": "[0,) RMS, Root Mean Square, is metric for regression tasks that measures the square root of the average squared differences between the predicted values and true values. [0,) MSE, Mean Squared Error, is commonly used for regression tasks and measures the average squared differences between predicted values and actual values. [0,) RMSE, Root Mean Squared Error."
        },
        {
            "title": "19 BLEU-4↑\n20 CodeBLEU↑",
            "content": "[0,1] [0,1]"
        },
        {
            "title": "21 ROUGE-L↑",
            "content": "[0,1] BLEU-1, Bilingual Evaluation Understudy (1-gram), calculates the precision of unigrams (individual words) in the generated text compared to the reference text(s). BLEU-4, Bilingual Evaluation Understudy (4-gram). CodeBLEU is metric designed to evaluate the quality of generated code by comparing it to reference code. CodeBLEU combines standard BLEU with additional features specific to code, such as syntax matching, data flow alignment, and weighted n-gram matching. ROUGE, Recall-Oriented Understudy for Gisting Evaluation of Longest Common Subsequence, LCS, evaluates text generation tasks, which measures the overlap between the predicted text and the reference text."
        },
        {
            "title": "Text Generation\nCode Generation",
            "content": "Image/Video Captioning # Metric 22 ROUGE-1 23 CIDEr Image-related 24 PSNR"
        },
        {
            "title": "28 SAD↓",
            "content": "On Path to Multimodal Generalist: General-Level and General-Bench Range Calculation [0,1] [0,1] ROUGE in 1-grams (single words). Consensus-based Image Description Evaluation (CIDEr), evaluates the quality of generated sentences (e.g., image captions) by comparing them against set of reference sentences."
        },
        {
            "title": "Representative Tasks\nText Generation\nCaptioning",
            "content": "[0,) [0,1] PSNR, Peak Signal-to-Noise Ratio, measures the quality of reconstructed or compressed signal, such as images or videos, compared to the original signal. [-1,1] MS-SSIM, Multi-Scale Structural Similarity Index Measure, is metric used for image quality assessment that measures the structural similarity between two images across multiple scales. The CLIP score measures the similarity between an image and textual description using the CLIP model, commonly used for image-text matching tasks. FID, Frechet Inception Distance, measures the distance between two multivariate Gaussian distributions: one representing the features of real images and the other representing the features of generated images. SAD, Sum of Absolute Differences, measures the total absolute difference between the predicted and ground truth values for all pixels in an image or region. It is typically used in image matting tasks to assess how closely the model replicates fine-grained image details such as edges and textures. [0,) [0,)"
        },
        {
            "title": "Document\nwarping",
            "content": "Image Un-"
        },
        {
            "title": "Image Editing",
            "content": "Text-to-Image Generation"
        },
        {
            "title": "Object Matting",
            "content": "Video-related 29 Fram-Acc [0,1]"
        },
        {
            "title": "30 FVD ↓",
            "content": "[0,) Frame-level Accuracy evaluates how many frames (or time steps) in the sequence are correctly classified by comparing predictions with the ground truth on frame-byframe basis. Frechet Video Distance (FVD) is metric used to evaluate the quality of generated video sequences, extending the principles of the FID to videos. FVD calculates the distance between the feature distributions of real and generated videos, taking into account both spatial and temporal dynamics."
        },
        {
            "title": "31 MUSIQ↑",
            "content": "[0,1] MUSIQ, Multi-scale Image Quality, quantifies the distance between the feature"
        },
        {
            "title": "Video Superresolution",
            "content": "32 absRel [0,) distributions of real and generated videos. absRel (Absolute Relative Error) is metric commonly used in depth estimation tasks, which measures the average ratio of the prediction error to the ground-truth depth."
        },
        {
            "title": "38 Suc-Rate↑",
            "content": "[0, 1] [0,) End-Point Error (EPE) is metric commonly used inoptical flowtasks to evaluate the accuracy of predicted motion vectors (flow) between consecutive frames in video or image sequence. It measures the Euclidean distance between the predicted and ground truth flow vectors at each pixel, providing an average error across the image. The DINO Score is calculated as the cosine similarity between the DINOv2 class embedding of two frames, effectively measuring the consistency of subjects identity across frames. According to the DreamBooth paper, the DINO Score captures more detailed aspects of subject identity compared to the CLIP Score. L1-Dis measures the L1 distance between two consecutive frames, evaluating the models ability to generate static (temporally stable) videos by quantifying the differences between adjacent frames. L1 = 1 L1-Dis = 255L1 ,f t+1 L1(f i=1( 1 [0, 1] (cid:80)N (cid:80)T 1 t=1 . ), [0, 1] [0, 1] OFS, Optical Flow Score, captures the movement of pixels between two consecutive frames. By applying threshold to identify moving pixels based on the optical flow, we calculate the ratio of these moving pixels. This ratio quantifies the degree of dynamism in the generated videos. The aesthetic score (Aesth-score) is calculated using ViT with linear head, as implemented in an improved aesthetic predictor. It has similar calculation as in CLIP score. Successful Rate (Suc-Rate) measures the performance of Video Generation. Suc-Rate leverages an open-vocabulary detector to identify the subjects specified in the text prompts. If all the subjects in text prompt are successfully detected, it classifies the corresponding frame as successfully generated video frame, and then calculates the ratio of successful frames to the total number of generated frames. [0, 1]"
        },
        {
            "title": "Optical Flow",
            "content": "Subject-Driven Generation"
        },
        {
            "title": "Static Video Generation",
            "content": "Dynamic Video Generation Artistic Content Text-toVideo Generation Multi-Class-Conditioned Text-to-Video Generation, Spatial Relation Video Generation, Camera Motion Generation # Metric 39 ViCLIPScore"
        },
        {
            "title": "40 Avg(DINO+\nCLIP+OFS\n+MSS)↑",
            "content": "3D-related 41 AMOTA"
        },
        {
            "title": "43 CD ↓",
            "content": "On Path to Multimodal Generalist: General-Level and General-Bench [0, 1] Range Calculation [0, 1] We calculate the cosine similarity between the ViCLIP embeddings of text prompts and videos. Compared to the CLIP Image model, ViCLIP enables more comprehensive assessment of the videos style and its overall consistency with the text prompts. For the image-to-video generation task, we conduct comprehensive evaluation based on Subject Consistency, Background Consistency, Motion Smooth and Dynamic Degree. We utilize the DINO-Score and CLIP-Score to assess subject and background consistency, reflecting the models ability to adhere to the image prompt. To evaluate the motion smoothness, we measure the Motion Smooth Score (MSS) of the frameby-frame motion prior via video frame interpolation models. Also, we employ the Optical Flow Score to measure the dynamic degree of the generated videos, ensuring that our metrics do not favor static videos. Representative Tasks Image-to-Video Generation Image-to-Video Generation [0,1] AMOTA (Average Multi-Object Tracking Accuracy) is performance metric used to evaluate multi-object tracking (MOT) systems. It combines detection accuracy and tracking performance by assessing how well system detects, associates, and tracks multiple objects over time. AMOTA is calculated by averaging tracking accuracy over range of thresholds for the Intersection over Union (IoU) or matching criteria. [0,) Relative Translation Error (RTE) is metric commonly used in robotics, pose estimation, and SLAM (Simultaneous Localization and Mapping) tasks. It evaluates the accuracy of systems estimated translation (movement) compared to the ground truth, typically in scenarios where spatial accuracy is crucial. [0,) Chamfer Distance (CD) is metric widely used in 3D geometry processing, point cloud generation, and shape matching tasks. It measures the similarity between two sets of points (e.g., two point clouds) by quantifying the average closest-point distance between them. CD is particularly useful for evaluating the alignment and fidelity of reconstructed or generated 3D shapes compared to ground truth data. 3D Tracking 3D Pose Estimation"
        },
        {
            "title": "Point Cloud Generation",
            "content": "Segmentation&Detection-related 44 mIoU [0,1] mIoU, Mean Intersection over Union, calculates the ratio of the intersection area to the union area between the predicted and ground truth segmentation masks for single class. 45 vIoU [0,1] vIoU measures the spatiotemporal overlap between predicted and ground-truth"
        },
        {
            "title": "46 Inst-mIoU↑",
            "content": "[0,1]"
        },
        {
            "title": "47 PQ↑",
            "content": "[0,1] object regions across multiple video frames Inst-mIoU computes the average IoU score for all part instances across dataset. It ensures that both over-segmentation and under-segmentation errors are penalized, focusing on instance-level segmentation quality. PQ, Panoptic Quality, is used for panoptic segmentation tasks, combining both segmentation quality and detection quality. It is comprehensive metric that evaluates both pixel-level segmentation and object detection quality."
        },
        {
            "title": "49 S-measure↑",
            "content": "[0,1] DICE, Dice Similarity Coefficient, is metric commonly used in image segmentation tasks, measuring the similarity between the predicted segmentation and the ground truth segmentation. Structure Measure (S-measure) is metric designed to evaluate the structural similarity between predicted binary map (e.g., an object mask) and ground truth binary map. It balances both region-level and boundary-level consistency, ensuring that the evaluation captures holistic structural integrity and fine-grained details. [0,1] Image Semantic Segmentation Temporal Action Detection 3D Part Segmentation"
        },
        {
            "title": "Video Object Detection",
            "content": "Audio-related 50 CLAP [0,1]"
        },
        {
            "title": "51 Style-CLAP ↑",
            "content": "[0,1] CLAP (Contrastive Language-Audio Pretraining) evaluates the alignment between generated audio and text. It is derived from contrastive learning framework where embeddings of audio and text are trained to be close in shared latent space if they are semantically related. Style-CLAP calculates the CLAP cosine similarity between the generated Mel spectrograms and the corresponding textual description of the style to evaluate style fit."
        },
        {
            "title": "52 MCD ↓",
            "content": "[0,) Mel-cepstral distortion (MCD) measures the spectral distance between the melcepstral coefficients (MCCs) of generated speech and reference speech, providing an indication of how closely the generated speech resembles the reference in terms of acoustic characteristics."
        },
        {
            "title": "Speech Synthesis",
            "content": "77 On Path to Multimodal Generalist: General-Level and General-Bench # Metric 53 WER Range Calculation [0,1] WER (Word Error Rate) measures the percentage of errors in the transcribed output"
        },
        {
            "title": "54 FAD ↓",
            "content": "[0,)"
        },
        {
            "title": "55 PCC ↑",
            "content": "[0,1] compared to the reference transcription. Frechet audio distance (FAD) evaluates the quality and realism of generated audio, and measures the similarity between the distribution of features obtained by VGGish in generated audio and those in set of real (reference) audio samples. Pitch-Class Consistency (PCC) is metric used in the evaluation of generated music to assess how consistent the pitch classes (e.g., notes) are across pairs of bars in piece of music. It measures the overlapping area between the pitch-class histograms of different bars, ensuring that the generated music maintains harmonic coherence. Video-to-Audio"
        },
        {
            "title": "Music Generation",
            "content": "Human-aware Evaluation 56 UPR [0,1] UPR, User Preference Rates, UPR measures the proportion of times particular system or model is preferred over alternatives in set of user evaluations. It reflects the subjective preferences of users and is often derived from pairwise comparisons or ranking experiments."
        },
        {
            "title": "58 GPT-Score ↑",
            "content": "[1,5] Mean Opinion Score (MOS), in which human raters listen to synthesized speech and assess its naturalness, quality, and intelligibility using 5-point Likert scale. [0,1] GPT-Score evaluates the instruction following rate with GPT assistance, as an alternative to human evaluation."
        },
        {
            "title": "Speech Generation",
            "content": "Audio Question Answering Mapping Functions of Scoring Metric. Most task evaluation scores, despite utilizing different metrics, fall within 0-100% range, such as F1, Accuracy (Acc), and ROUGE-L, and follow monotonically increasing trend. However, certain task metrics produce scores outside this range. For example, regression-related metrics, as well as FID, FVD, and similar metrics, range from 0 to infinity and follow monotonically decreasing trend. In contrast, MOS scores are represented as discrete 5-point scale. Due to these varying score ranges across tasks, it becomes intractable to normalize them to unified scale for level score calculations. Thus, we design the following mapping functions to standardize these metrics into 1-100% range, thereby streamlining the computation of level scoring algorithms. Normalizing MAE: Normalizing RMS: Normalizing MSE: Normalizing RMSE: Normalizing absRel: Normalizing EPE: Normalizing FID: Normalizing FVD: = 2 sigmoid = 2 sigmoid (cid:19) (cid:18) 50 (cid:19) (cid:18) 50 = 2 sigmoid = 2 sigmoid (cid:19) (cid:18) 5 (cid:19) (cid:18) 5 1, where [0, +), (0, 1). 1, where [0, +), (0, 1). 1, where [0, +), (0, 1). 1, where [0, +), (0, 1). = 2 sigmoid (cid:19) (cid:18) 0.1 1, where [0, +), (0, 1). = 2 sigmoid (cid:19) (cid:18) 1 = 2 sigmoid (cid:19) (cid:18) 25 1, where [0, +), (0, 1). 1, where [0, +), (0, 1). = 2 sigmoid (cid:19) (cid:18) 100 1, where [0, +), (0, 1). 78 On Path to Multimodal Generalist: General-Level and General-Bench Normalizing FAD: Normalizing PSNR: Normalizing SAD: Normalizing RTE: Normalizing CD: Normalizing MCD: Normalizing WER: Normalizing MS-SSIM: Normalizing MOS: = 2 sigmoid (cid:19) (cid:18) 10 1, where [0, +), (0, 1). = tanh (cid:17) (cid:16) 20 , where [0, +), [0, 1). = 2 sigmoid = 2 sigmoid (cid:19) (cid:18) 10 (cid:19) (cid:18) 0.5 1, where [0, +), (0, 1). 1, where [0, +), (0, 1). = 2 sigmoid = 2 sigmoid (cid:19) (cid:18) 1 (cid:19) (cid:18) 5 1, where [0, +), (0, 1). 1, where [0, +), (0, 1). = 1 x, where [0, 1], [0, 1]. = (x + 1) 2 = 1 4 , where [1, 1], [0, 1]. , where [1, 5], [0, 1]. On Path to Multimodal Generalist: General-Level and General-Bench A.2 Data Format To provide comprehensive understanding of how to utilize our benchmark data, we present examples illustrating how the data files are stored and organized. Figure 15 displays the code snippets showcasing the structures of some representative tasks. Figure 16 illustrates how we organize the benchmark datasets in the file system. Figure 15: An illustrative example of file formats. 80 On Path to Multimodal Generalist: General-Level and General-Bench Figure 16: The organization structure of the file system. 81 On Path to Multimodal Generalist: General-Level and General-Bench Figure 17: Taxonomy and hierarchy of data in terms of Image modality. A.3 Data Taxonomy and Hierarchy We visualize comprehensive hierarchical taxonomy of our benchmark. Due to space constraints, we have separately illustrated the taxonomy for five major modalities in Figure 17, Figure 18, Figure 19, Figure 20, and Figure 21, respectively. Each visualization includes comprehension and generation paradigms, skills (meta-tasks), and specific tasks for the respective modality. 82 On Path to Multimodal Generalist: General-Level and General-Bench Figure 18: Taxonomy and hierarchy of data in terms of Video modality. 83 On Path to Multimodal Generalist: General-Level and General-Bench Figure 19: Taxonomy and hierarchy of data in terms of 3D modality. 84 On Path to Multimodal Generalist: General-Level and General-Bench Figure 20: Taxonomy and hierarchy of data in terms of Audio modality. 85 On Path to Multimodal Generalist: General-Level and General-Bench Figure 21: Taxonomy and hierarchy of data in terms of Language modality. 86 On Path to Multimodal Generalist: General-Level and General-Bench A.4 Data Distributions In Figure 22, we present the distribution of capability evaluations across all tasks in General-Bench. These Capability. capabilities include: Content Recognition, Commonsense Understanding, Reasoning Ability, Causality Discrimination, Affective Analysis, Problem Solving, Creativity and Innovation, Interactive Capability, and others. As observed, the majority of tasks focus on Content Recognition or perception-related abilities. This emphasis aligns with the current stage of MLLM development, where models are not yet equipped with highly advanced cognitive capabilities. We plan to continuously update the benchmark in the future to accommodate the evolving strengths of more powerful models. Figure 22: Distribution of various capabilities evaluated in General-Bench. Figure 23: Distribution of various domains and disciplines covered by General-Bench. Domains and Discipline. Figure 23 illustrates the domains and disciplines covered by our benchmark. While the majority of tasks belong to the general domain, the benchmark also encompasses significant fields from both Physical Sciences and Social Sciences. For Physical Sciences, the benchmark includes disciplines such as Physics, Geometry, Biology, Medicine, 87 On Path to Multimodal Generalist: General-Level and General-Bench Chemistry, Astronomy, and Geography. For Social Sciences, it spans areas including Humanities, Linguistics, History, Politics, Culture, Art, and Economics. In other words, our benchmark is designed to evaluate the capabilities of MLLMs across wide range of scientific fields and domains. This ensures the broad evaluative advantage of our benchmark, enabling comprehensive assessment of multimodal generalist models. Comprehension vs. Generation. We illustrate the task distribution across the two critical paradigms, Comprehension and Generation, in Figure 24. Currently, the majority of tasks are centered on comprehension, which aligns with the present capabilities of most MLLMs. Figure 24: Distribution of various domains and discipline covered by General-Bench. Modality. Finally, we present the distribution of tasks across different modalities in Figure 25. Overall, the image modality constitutes the largest proportion of tasks. We note that beyond the five major modalitiesImage, Video, 3D (3D-RGB and Point-Cloud), Audio, and Languageour benchmark also includes tasks in other modalities such as Time Series, Depth, Infrared, Spectrogram, Radar, Code, Document, and Graph. These additional modalities play important roles in specific domains. However, due to the limited number of tasks in these modalities, we have merged and classified their data under broader categories like Image and Language for ease of management. Figure 25: Distribution of different modalities covered in General-Bench. 88 On Path to Multimodal Generalist: General-Level and General-Bench A.5 Comparisons with Existing Benchmarks Table 19 provides comprehensive comparison of General-Bench with existing MLLM benchmarks. Compared to these benchmarks, General-Bench demonstrates absolute superiority across all comparison aspects. For instance, it is the first MLLM benchmark to cover nearly all commonly used modalities while emphasizing both Comprehension and Generation as critical evaluation paradigms. Most notably, General-Bench boasts the largest dataset, encompassing 550 tasks with over 275K instances. Also, it evaluates the largest number of MLLMs tested to date, setting new standard for benchmark comprehensiveness and scale. Table 19: full comparison of General-Bench with other popular MLLM benchmarks. Aspect Bench. Science&Discipline Modality Task Scheme #Domain #Skill #Task #Sample Answer Form #Metric Annotation #Tested Models ScienceQA (Lu et al., 2022a) VisAidMath (Ma et al., 2024) MMMU (Yue et al., 2024a) NaturalBench (Li et al., 2024f) Audio AudioBench (Wang et al., 2024d) MARBLE (Yuan et al., 2023b) MMAU (Sakshi et al., 2024) 3D T3Bench (He et al., 2023) 3DBench (Zhang et al., 2024b) Video Video-Bench (Ning et al., 2023) VideoMME (Fu et al., 2024b) MLVU (Zhou et al., 2024b) MVBench (Li et al., 2024g) AutoEval-Video (Chen et al., 2024d) VBench (Huang et al., 2024) Image MME (Fu et al., 2023) LVLM-eHub (Xu et al., 2023b) MM-Vet (Yu et al., 2024) V*Bench (Wu and Xie, 2024) MMIE (Xia et al., 2024b) Txt,Img Comp. 12 Txt,Img Comp. Txt, Img Comp. Txt,Img Comp. Txt, Aud Comp. Txt, Aud Comp. Txt, Aud Comp. Txt, 3D Gen. Txt, 3D Comp. Txt,Vid Comp. Txt,Vid Comp. Txt, Vid Comp. Txt,Vid Comp. Txt,Vid Comp. Txt,Vid Gen. Txt,Img Comp. Txt, Img Comp. Txt,Img Comp. Txt,Img Txt, Img Comp. Comp. Gen. 1 1 / / / / / / 6 7 / 12 / / / 1 10 1 6 / 3 / 21K MC-QA Acc. Repurposed 1.2K MC-QA Acc. Repurposed 30 11.5K MC-QA Acc. Manual 27 7.6K MC-QA Acc. Repurposed 8 100K Free-Form WER,METEOR Llama-score Repurposed 12 12 18 27 / Free-Form Origin(19) Repurposed 10K MC-QA Acc. Repurposed Manual 3 300 Free-From Origin(2) Repurposed 12 8K Free-From Origin(6) Repurposed 3 17K MC-QA Acc. Repurposed 12 12 MC-QA Acc. Manual 9 9 MC-QA Open Acc. GPT-Ranking Manual 20 20 4k MC-QA Acc. Repurposed 9 9 Open GPT-score Manual 16 16 Open / Manual 2.2K MC-QA Acc. Repurposed 2.1K MC-QA Open Acc. CIDEr top-1 Acc. Repurposed 205 MC-QA GPT-score Repurposed 191 MC-QA Acc. Manual 20K MC-QA Open / Manual 14 1 2 4 14 6 2 4 89 10 10 32 4 0 14 0 8 13 20 14 11 30 8 16 13 8 On Path to Multimodal Generalist: General-Level and General-Bench Aspect Bench. Modality Task Scheme #Domain #Skill #Task #Sample Answer Form #Metric Annotation #Tested Model Mia-bench (Qian et al., 2024) MME-RealWorld (Zhang et al., 2024c) MLLM-Bench (Ge et al., 2024a) Q-Bench (Wu et al., 2024b) MUIRBench (Wang et al., 2024e) MileBench (Song et al., 2024) MMBench (Liu et al., 2024b) Omini SEED-Bench (Li et al., 2023c) SEED-Bench-2 (Li et al., 2023d) CV-Bench (Tong et al., 2024b) MMT-Bench (Ying et al., 2024b) MEGA-Bench (Chen et al., 2024e) General-Bench (Ours) Manual+ Repurposed Manual+ Repurposed Manual+ Repurposed Txt, Img Comp. 15 8 400 Open GPT-score Manual Txt, Img Comp. Txt, Img Comp. Txt, Img Comp. 5 / 1 43 43 29.4K MC-QA Acc. Manual 6 3 6 3 420 84.7K MC-QA Open MC-QA Open GPT-score Manual GPT-score Txt,Img Comp. 12 12 12 2.6K MC-QA Acc. Txt,Img Comp. Txt,Img Comp. Txt,Img,Vid Txt,Img,Vid Txt, Img, 3D Txt,Img,Vid, Point-Cloud Comp. Comp. Gen. Comp. Comp. Comp. Txt,Img,Vid Comp. / / / 3 2 / / / 2 12 22 4 7 6.4K MC-QA Open Acc. ROUGE-L 20 3K MC-QA Acc. Repurposed 12 22 19K MC-QA Acc. Manual 24K MC-QA Acc. Manual+ Repurposed 4 2.6K MC-QA Acc. Repurposed 52 11.7K MC-QA Acc. Repurposed 32 162 31K MC-QA Acc. Repurposed 505 8K Free-Form Origin (45) Manual 29 21 15 20 22 21 23 15 22 30 22 Txt,Img,Vid,Aud, Time,Depth,3D-RGB, Point-Cloud,Infrared, Spectrogram,Radar, Code,Doc,Graph, Comp.+Gen. 29 145 702 325.8K Free-Form Origin (58) Reannotated +Manual 172 Specialists & 102 Generalists MMIU (Meng et al., 2024b) Txt,Img,Vid, Point-Cloud,Depth A.6 Complete List of Tasks and Skills (Meta-Tasks) We present all the tasks and their specific details, including the tasks, datasets, and corresponding SoTA specialists, in this section. Table 21 presents image-related comprehension tasks, while Table 23 lists image-related generation tasks. Similarly, Table 25 summarize video-related comprehension tasks, and Table 27 focuses on video-related generation tasks. Audio-related comprehension tasks are detailed in Table 29, with generation tasks in Table 31. For 3D-related tasks, Table 33 highlights comprehension tasks, while Table 35 covers generation tasks. Finally, Table 37 outlines language-related (NLP) tasks. 90 On Path to Multimodal Generalist: General-Level and General-Bench Table 21: Detailed list of all tasks and skills (meta-tasks) under image and comprehension category. Image Comprehension (#I-C) Group Skill (Meta-Task) # Task (Short Name) Domain Capability Data Source Number Model Metrics Task Data SoTA Specialist #I-C-1 Behavior Recognition (Behavior Recog) 1 ATM Booths Suspicious Behavior Recognition (ATM Behavior Reg) General Content Recognition ATM 2 Human Action Recognition (Human General Content Action Recog) 3 Sports Image Classification (Sports Image Cls) Sports Recognition, Commonsense Knowledge Content Recognition, Commonsense Knowledge Booths Anomaly Recognition (fur, 2023) Human Action Recognition (Hum, a) Manual Construction (Spo) 1000 CLIP (Radford et al., 2021) 3000 CLIP (Radford et al., 2021) 500 CaSED (Conti et al., 2023) Acc Acc Acc 1 Sketch-to-HTML Code Generation (Sketch2HTML Gen) Code Creativity and Innovation, Reasoning Ability Sketch2Code (Ske) 500 sketch-code (Kumar) BLEU- #I-C-2 Code Generation (Code Gen) #I-C-3 Crack Detection (Crack Det) 1 Tire Crack Detection (Tire Texture Recog) 2 Road Crack Detection (Road Crack Det) #I-C-4 Disease Detection (Disease Det) 1 Plant Disease Detection (PlantDisease Det) #I-C-5 Disease Recognition (Disease Recog) 1 Fruit Disease Recognition (Fruit Disease Recog) 2 Abnormal Heartbeat Recognition (Abnormal Heartbeat Recog) 3 Myocardial Infarction Recognition (Myocardial Infarction Recog) 4 Brain Tumor MRI Recognition (Brain Tumor MRI Recognition) 5 Pumpkin Leaf Diseases Recognition (Pumpkin Diseases Recog) General Content Recognition, Commonsense Knowledge General Content Recognition, Commonsense Knowledge Tire Texture Image Recognition (Siegel, 2021) CrackForest (Shi et al., 2016) Biology Content Recognition Plant Disease Detection (Beans, Strawberry and Tomato diseases) General Content Recognition Guava Fruit Disease Dataset (Gua) Medicine Content Recognition National Heart Foundation 2023 ECG Dataset (ECG) Medicine Content Recognition National Heart Foundation 2023 ECG Dataset (ECG) Medicine Content Recognition Brain Tumor MRI Dataset (Bra) Biology Content Recognition Pumpkin Leaf Diseases Dataset (Pum) 325 CLIP (Radford et al., 2021) 118 OWLVIT (Minal., et derer 2022a) Acc mIoU 500 GLIP-T et al., 2022) (Li* mAP@0.5 348 CLIP (Radford et al., 2021) 814 CLIP (Radford et al., 2021) 716 CLIP (Radford et al., 2021) 505 CLIP (Radford et al., 2021) 500 CLIP (Radford et al., 2021) 6 Leukemia Classification (Leukemia Clas) Medicine Content Recognition Leukemia Classification (S. et al.) 500 CLIP (Radford et al., 2021) #I-C-6 Document Visual Question Answering (Doc VQA) 1 Multi-Page Agreement Document Visual Question Answering (Agree-Doc VQA) 1 2 Multi-Page Application Document Visual Question Answering (Applic-Doc VQA) 3 Multi-Page Certificate Document Visual Question Answering (Cert-Doc VQA) 4 Multi-Page E-mail Document Visual Question Answering (Email-Doc VQA) 5 Multi-Page Form Document Visual Question Answering (Form-Doc VQA) 6 Multi-Page Handwritten Document Visual Question Answering (Handwrit-Doc VQA) General Content Recognition MP-DocVQA (Tito et al., 2022) 272 DocOwl (Ye et al., 2023a) General Content Recognition MP-DocVQA (Tito et al., 2022) General Content Recognition MP-DocVQA (Tito et al., 2022) General Content Recognition MP-DocVQA (Tito et al., 2022) General Content Recognition MP-DocVQA (Tito et al., 2022) General Content Recognition MP-DocVQA (Tito et al., 2022) 282 Donut (Kim et al., 2022) 101 Donut (Kim et al., 2022) 183 DocOwl (Ye et al., 2023a) 297 DocOwl (Ye et al., 2023a) 146 DocOwl (Ye et al., 2023a) 91 Acc Acc Acc Acc Acc Acc Acc Acc Acc Acc Acc Acc On Path to Multimodal Generalist: General-Level and General-Bench Skill (Meta-Task) # Task (Short Name) Domain Capability Data Source Number Model Metrics Task Data SoTA Specialist 7 Multi-Page Infographic Document Visual Question Answering (Infograph-Doc VQA) 8 Multi-Page Invoice Document Visual Question Answering (Invoi-Doc VQA) 9 Multi-Page Letter Document Visual Question Answering (Letter-Doc VQA) 10 Multi-Page Manual Document Visual Question Answering (Manual-Doc VQA) 11 Multi-Page Meeting Document Visual Question Answering (Meet-Doc VQA) 12 Multi-Page Memo Document Visual Question Answering (Memo-Doc VQA) 13 Multi-Page News Document Visual Question Answering (News-Doc VQA) General Content Recognition MP-DocVQA (Tito et al., 2022) General Content Recognition MP-DocVQA (Tito et al., 2022) 239 DocOwl (Ye et al., 2023a) 362 DocOwl (Ye et al., 2023a) Acc Acc General Content Recognition MP-DocVQA (Tito et al., 2022) 391 DocQuery (doc) Acc General Content Recognition MP-DocVQA (Tito et al., 2022) General Content Recognition MP-DocVQA (Tito et al., 2022) 619 DocOwl (Ye et al., 2023a) 229 Donut (Kim et al., 2022) Acc Acc General Content Recognition MP-DocVQA (Tito et al., 2022) 68 DocQuery (doc) Acc General Content Recognition MP-DocVQA (Tito et al., 2022) 185 DocOwl (Ye et al., 2023a) Acc 14 Multi-Page Order Document Visual General Content Recognition MP-DocVQA 151 DocQuery (doc) Acc Question Answering (Order-Doc VQA) 15 Multi-Page Poster Document Visual Question Answering (Poster-Doc VQA) 16 Multi-Page Presentation Document Visual Question Answering (Pres-Doc VQA) 17 Multi-Page Report Document Visual Question Answering (Reprt-Doc VQA) (Tito et al., 2022) General Content Recognition MP-DocVQA (Tito et al., 2022) 320 DocOwl (Ye et al., 2023a) Acc General Content Recognition MP-DocVQA (Tito et al., 2022) 310 DocQuery (doc) Acc General Content Recognition MP-DocVQA (Tito et al., 2022) 648 DocQuery (doc) Acc 18 Multi-Page Request Document ViGeneral Content Recognition MP-DocVQA 43 DocQuery (doc) Acc sual Question Answering (Reqst-Doc VQA) 19 Multi-Page Resume Document Visual Question Answering (Resu-Doc VQA) (Tito et al., 2022) General Content Recognition MP-DocVQA (Tito et al., 2022) 210 DocQuery (doc) Acc 20 Multi-Page Sheet Document Visual Question Answering (Sheet-Doc VQA) General Content Recognition, Commonsense Knowledge MP-DocVQA (Tito et al., 2022) 245 Donut (Kim et al., 2022) 21 General Document Visual Question Answering (General-Doc VQA) 1 Person Emotion Detection (Emotion Det) #I-C-7 Emotion Detection (Emotion Det) General Content Recognition MP-DocVQA (Tito et al., 2022) 500 DocOwl 1.5-Chat (Ye et al., 2023a) Humanities Affective Analysis Emotion Detection (Emo) 707 Deepface (Serengil Ozpinar, 2020) and 2 Pet Face Expression Recognition (Pet-Expr Recog) General Affective Analysis 3 Emotion Recognition (Emotion Recog) 4 Face Emotion Recognition (Face Emotion Recog) Humanities Affective Analysis General Content Recognition, Commonsense Knowledge Manual Construction (Fac, a) 540 DDAMFN++ (Zhang 2023f) et al., Pets Facial Expression Image Dataset (Pet) FER (Emo) 249 CLIP (Radford et al., 2021) 717 CLIP (Radford et al., 2021) #I-C-8 Graph Classification (Graph Cls) 1 Spectrum Graph Car Classification (Spectrum-Graph Cls) General Content Recognition Emergency Vehicle Siren Sounds (Eme) 600 Mantis (Jiang et al., 2024c) #I-C-9 Hallucination Detection (Hallucination Det) 1 Attribute Hallucination Detection (Attr-Hallu Det) 2 Object Hallucination Detection (Obj-Hallu Det) Ethics Ethics Content Recognition PhD (Liu et al., 2024c) Content Recognition PhD (Liu et al., 2024c) 92 500 SAC3 (Zhang et al., 2023g) 500 SAC3 (Zhang et al., 2023g) Acc Acc Acc Acc Acc Acc Acc Acc Acc On Path to Multimodal Generalist: General-Level and General-Bench Skill (Meta-Task) # Task (Short Name) Domain Capability Data Source Number Model Metrics Task Data SoTA Specialist 1 Instagram Image Caption (Ins-Img Cap) 2 COCO Image Captioning (COCOImg Cap) 3 Satellite Image Caption (Satell-Img Cap) 4 Human Related Image Captioning (Human Cap) 5 Chinese Image Captioning (Zh-Img Cap) Humanities, Social Content tion, Ability General Content tion, Ability General Content tion, Ability General Content tion, Ability General Content RecogniReasoning RecogniReasoning RecogniReasoning tion, Ability, Linguistics 6 Astronomy Image Captioning (Astro-Img Cap) Astronomy Content tion, Ability RecogniReasoning RecogniReasoning RecogniReasoning Instagram Images with Captions (Ins) COCO (Lin et al., 2014) 557 GRIT (Nguyen et al., 2022) ROUGE 500 GRIT (Nguyen et al., 2022) ROUGE Satellite Image Caption Generation (Lu et al., 2018) Human Related Image Captioning (Hum, b) Chinese-ImageCaptioning (Chi, a) Manual Construction (Alam et al., 2024) 646 GRIT (Nguyen et al., 2022) ROUGE 500 GRIT (Nguyen et al., 2022) BLEU500 GRIT (Nguyen et al., 2022) BLEU-1 500 llava-v1.5 et al., 2024d) (Liu ROUGE-L"
        },
        {
            "title": "1 Indoor Depth Estimation (Indoor-",
            "content": "General Content Recognition Nyudv2 (Silber654 AdaBins (Bhat RMS Depth Est) man et al., 2012) et al., 2021) 1 Pedestrian Instance Segmentation (Pede-Inst Seg) 2 Vehicle Instance Segmentation (Vehi-Inst Seg) General Content tion, Ability General Content RecogniReasoning Recognition, Commonsense Knowledge Cityscapes (Cordts et 2016) Cityscapes (Cordts et 2016) al., al., 398 BPR (Tang et al., 2021) AP@0.5 500 BPR (Tang et al., 2021) AP@0.5 #I-C-10 Image Caption (Img Cap) #I-C-11 Image Depth Estimation (Img Depth Est) #I-C-12 Image Instance Segmentation (Img Inst Seg) 1 Handwriting OCR (Handwrite OCR) General Content Recognition Handwriting Recognition (OCR) (Han) #I-C-13 Image OCR (Img OCR) 2 Letter and Number OCR (Letter&Num OCR) 3 License Plate OCR (License OCR) General Content Recognition standard OCR dataset (sta, a) General Content Recognition License Plate Text Recognition Dataset (Car, a) 4 Radical-Level Oracle Bone Character Recognition (Radical-Char Recog) History Content Recognition Radical-Level Oracle Bone Character Dataset (Rad, a) 500 Tesseract (tes) ANLS 720 Tesseract (tes) ANLS 533 Tesseract (tes) ANLS 500 Tesseract (tes) ANLS Code Art #I-C-14 Image Recognition (Img Recog) #I-C-15 Image Semantic Segmentation (Img Sem Seg) 1 Acne Recognition (Acne Recog) 2 Latex Code Recognition (Latex Recog) 1 BobRoss Painting Segmentation (BobRoss-Paint seg) 1 1 1 2 Indoor Semantic Segmentation (Indoor-Sem Seg) 3 Pedestrian Semantic Segmentation (Pede-Sem Seg) 4 Road Semantic Segmentation (RoadSem Seg) 5 Vehicle Semantic Segmentation (Vehi-Sem Seg) Medicine Content Acne (Acn) Recognition, Commonsense Knowledge Content Recognition Latex OCR (LaT) 500 pix2tex (Blecher) BLEU-1 et al., 2021) 600 CLIP (Radford Acc Content Recognition Segmented Bob Paintings Ross (Seg) 250 CLIPSeg (Luddecke and Ecker, 2022) General Content Recognition, Commonsense Knowledge General Content Recognition, Commonsense Knowledge General Content Recognition, Commonsense Knowledge General Content Recognition, Commonsense Knowledge Nyudv2 (Silberman et al., 2012) Cityscapes (Cordts et 2016) Cityscapes (Cordts et 2016) Cityscapes (Cordts et 2016) al., al., al., 93 654 Cerberus (Chen et al., 2022a) et 398 DeepLabV3+ (Chen 2018) 500 DeepLabV3+ (Chen 2018) 500 DeepLabV3+ (Chen 2018) et et al., al., al., mIoU mIoU mIoU mIoU mIoU On Path to Multimodal Generalist: General-Level and General-Bench Skill (Meta-Task) # Task (Short Name) Domain Capability Data Source Number Model Metrics Task Data SoTA Specialist 6 People Clothing (Cloth Seg) Segmentation General Content Recognition People Clothing Segmentation (Yang 2013) et al., 7 Flood Area Segmentation (Flood Seg) General Content Recognition Flood Area Segmentation (Flo) 8 Forest Aerial Images for Segmentation (Forest Seg) General Content Recognition Forest Aerial Images for Segmentation (Demir et al., 2018) 500 CLIPseg (Luddecke and Ecker, 2022) 290 CLIPseg (Luddecke and Ecker, 2022) 465 CLIPseg (Luddecke and Ecker, 2022) mIoU mIoU mIoU #I-C-16 Image Visual Grounding (Img Vis Grnd) 1 Complex Expression Visual Grounding (Complex-Expre VG) 2 Short Expression Visual Grounding (Short-Expre VG) 1 Blind Image Question Answering (Blind VQA) General Content tion, Ability General Content tion, Ability RecogniReasoning Refcocog et al., 2016) (Yu 990 PolygonTransformer (Liu et al., 2023d) RecogniReasoning Refcoco (Kazemzadeh et al., 2014) 665 PolygonTransformer (Liu et al., 2023d) AP@0.5 AP@0.5 General Content Recognition vizwiz (Gurari 2 External Knowledge Required Image Question Answering (Knowledge VQA) General Commonsense Knowledge, Reasoning Ability 3 Medical Image Question Answering (Medical VQA) Medicine Commonsense Knowledge 4 Reasoning and Compositional Image Question Answering (Reason VQA) 5 Biological Commonsense Visual (Biology Question Answering VQA) General Content tion, Ability RecogniReasoning Biology Commonsense Knowledge et al., 2018) OKVQA (Marino et al., 2019) Medical Visual Question Answering (med) GQA (Hudson and Manning, 2019) ScienceQA (Lu et al., 2022b) #I-C-17 Image Visual Question Answering (Img VQA) 6 Blood Cell Visual Question Answering (Blood-Cell VQA) Biology Commonsense Knowledge Blood Cell ages (Blo) Im7 Cataract Diagnosis Visual Question Answering (Cataract VQA) Medicine Commonsense Knowledge Cataract dataset (cat) 8 Chemistry Visual Question Answering (Chemistry VQA) 9 Geography Visual Question Answering (Geography VQA) 10 Geometry Visual Question Answering (Geometry VQA) 11 Lung Nodule Diagnosis Visual Question Answering (Lung VQA) Chemistry Reasoning Ability Geography Reasoning Ability Geometry Reasoning Ability Medicine Reasoning Ability ScienceQA (Lu et al., 2022b) ScienceQA (Lu et al., 2022b) Inter-GPS et al., 2021a) Lung cancer CAM attempting (Lun, a) (Lu 550 GIT (Wang et al., 2022a) 550 GIT (Wang et al., 2022a) Acc Acc 316 PMC-VQA (Zhang et 2023h) al., BLEU-1 519 GIT (Wang et al., Acc 2022a) 428 MMCoT (Zhang et al., 2024d) 352 PMC-VQA (Zhang et 2023h) 300 PMC-VQA et (Zhang 2023h) al., al., 110 MMCoT (Zhang et al., 2024d) 594 MMCoT (Zhang et al., 2024d) Acc Acc Acc Acc Acc (Lu Acc 300 Inter-GPS et al., 2021a) 300 PMC-VQA (Zhang et 2023h) al., Acc Acc Acc 12 Mathematical Statistical Reasoning Visual Question Answering (Stat-Reasoning VQA) MathematicsReasoning Ability MathVista et al., 2024c) MathematicsReasoning Ability MathVista et al., 2024c) (Lu (Lu 116 Chimera (Peng et al., 2024) 107 Chimera (Peng et al., 2024) 13 Mathematical Textbook Visual (MathAnswering Question Textbook VQA) 14 Mathematical Word Problem Visual Question Answering (Math-Word VQA) 15 Physics Visual Question Answering (Physics VQA) 16 Pneumonia Diagnosis Visual Question Answering (Pneumonia VQA) 17 Mathematical Synthetic Scene Visual Question Answering (MathScene VQA) 18 Remote Sense Visual Question Answering (Remote-Sense VQA) MathematicsReasoning Ability MathVista (Lu 22 Chimera (Peng Acc et al., 2024c) et al., 2024) Physics Reasoning Ability Medicine Reasoning Ability ScienceQA (Lu et al., 2022b) COVIDx CXR-4 (Wang al., 2020a) et 449 MMCoT (Zhang et al., 2024d) 400 PMC-VQA (Zhang et 2023h) al., MathematicsReasoning Ability MathVista et al., 2024c) (Lu 92 Chimera (Peng et al., 2024) Earth Reasoning Ability EarthVQA (Wang et al., 2024f) 300 RSVQA (Lobry et al., 2020) 94 Acc Acc Acc Acc On Path to Multimodal Generalist: General-Level and General-Bench Skill (Meta-Task) # Task (Short Name) Domain Capability Data Source Number Model Metrics Task Data SoTA Specialist 19 Skin Disease Visual Question Answering (Skin-Disease VQA) Medicine Reasoning Ability Skin Cancer (Ski) 300 PMC-VQA et (Zhang 2023h) al., 20 Internet-Retrieved-Information Visual Question Answering (Retrieved VQA) 21 Webpage Content Visual Question Answering (Webpage VQA) General Content Recognition WebQA (Chang et al., 2022) General Content Recognition WebQA (Chang et al., 2022) 500 MLoEM (Wei et al., 2024) 500 MLoEM (Wei et al., 2024) 22 Sporting-related VQA (Sport VQA) Sports Content Recognition MMCoQA (Li et al., 2022a) 500 LLaVA-NeXTInterleave et al., 2024h) 500 VPG-C (Li et al., (Li 2023e) 23 Financial Dialogue VQA (Financial VQA) 24 Historical Multi-Source Dialogue (Historical Dialogue) Finance Content Recognition MMCoQA (Li et al., 2022a) General Content Recognition MMCoQA (Li et al., 2022a) 500 VPG-C (Li et al., 2023e) 25 Multi-Turn Interactive Dialogue General Content Recognition MMCoQA (Li (Multi-Dialogue) 26 Complex Multi-Scenario Dialogue (Multi-Scene Dialogue) 27 Multimodal Slide Question Answering (Slide VQA) General Content Recognition MMCoQA (Li et al., 2022a) et al., 2022a) General Content Recognition SlideVQA (Tanaka 2023) et al., 500 MLoEM (Wei et al., 2024) ROUGE-L 500 Mantis (Jiang et al., 2024c) 500 Mantis (Jiang et al., 2024c) 28 Multimodal Document VQA (MMGeneral Content Recognition DocVQA Doc VQA) (Mathew et al., 2021) 500 MLoEM (Wei et al., 2024) 29 Multimodal Document Fine-grained Reasoning (MM-Doc Reason) General Content Recognition DocVQA (Mathew et al., 2021) 30 Textbook Diagram VQA (Diagram VQA) Education Content Recognition TQA (Kembhavi et al., 2017) 31 Education-Related VQA (Education Education Content Recognition TQA (Kembhavi VQA) 32 Book Information VQA (Book VQA) 33 Textual Document VQA (Text-Doc VQA) 34 Text-Table-Image Joint Reasoning (Txt-Tab-Img Reason) 35 Handwritten Formula Visual Question Answering (Handwrite-Formu VQA) 36 Face Comparison Visual Question Answering (Face-Compare VQA) 37 Image Similarity Visual Question Answering (Img-Similar VQA) 38 Art Image Visual Question Answering (Art-Img VQA) 39 Culture Image Visual Question Answering (Culture-Img VQA) 40 Imge Editing Instruction Generation (Img-Edit Instru Gen) et al., 2017) General Content Recognition OCR-VQA et (Mishra 2019) General Content Recognition OCR-VQA et (Mishra 2019) al., al., MathematicsReasoning Ability Chrome2016 (Jiaqing) General Content Recognition LFW (AI) General Reasoning Ability Art Content tion, Ability Culture Content tion, Ability General Content RecogniReasoning RecogniReasoning Recognition, Creativity and Innovation TTL (Rosenfeld et al., 2018) AQUA (Garcia et al., 2020) et al., CII-Bench (Zhang 2024e) Manual Construction (Han et al., 2024) 500 Mantis (Jiang et al., 2024c) 500 MLoEM (Wei et al., 2024) 500 MLoEM (Wei et al., 2024) 500 VPG-C (Li et al., 2023e) 500 VPG-C (Li et al., 2023e) 500 Mantis (Jiang et al., 2024c) 500 mPLUGDocowl2 et al., 2024c) (Hu 500 Mantis (Jiang et al., 2024c) 500 mPLUG-1B (Li et al., 2022b) 499 mPLUG-1B (Li et al., 2022b) General Content Recognition MultiModalQA () 500 MLoEM (Wei et al., 2024) Acc Acc Acc ROUGE-L ROUGE-L ROUGE-L ROUGE-L Acc Acc Acc Acc Acc Acc Acc Acc Acc Acc Acc Acc Acc #I-C-18 Industrial Anomaly Detection (Ind-Anomaly Det) 1 Bottle Anomaly Detection (Bottle Anomaly Det) 2 Cable Anomaly Detection (CableAnomaly Det) Engineering Content Recognition MVTec (Bergmann et al., 2021) Engineering Content Recognition MVTec (Bergmann et al., 2021) 3 Capsule Anomaly Detection Engineering Content Recognition MVTec (Capsule-Anomaly Det) (Bergmann et al., 2021) 95 500 llava-v1. et al., 2024d) (Liu ROUGE-L 63 DeSTSeg (Zhang et al., 2023i) 92 DeSTSeg (Zhang et al., 2023i) 109 DeSTSeg (Zhang et al., 2023i) AP AP AP On Path to Multimodal Generalist: General-Level and General-Bench Skill (Meta-Task) # Task (Short Name) Domain Capability Data Source Number Model Metrics Task Data SoTA Specialist 4 Carpet Anomaly Detection (CarpetAnomaly Det) 5 Grid Anomaly Detection (GridAnomaly Det) 6 Hazelnut Anomaly Detection (Hazelnut-Anomaly Det) Engineering Content Recognition MVTec (Bergmann et al., 2021) Engineering Content Recognition MVTec (Bergmann et al., 2021) Engineering Content Recognition MVTec (Bergmann et al., 2021) 7 Leather Anomaly (Leather-Anomaly Det) Detection Engineering Content Recognition MVTec (Bergmann et al., 2021) 8 Metal Nut Anomaly Detection (Metal-Anomaly Det) 9 Pill Anomaly Detection Anomaly Det) (Pill10 Screw Anomaly Detection (ScrewAnomaly Det) Engineering Content Recognition MVTec (Bergmann et al., 2021) Engineering Content Recognition MVTec (Bergmann et al., 2021) Engineering Content Recognition MVTec (Bergmann et al., 2021) 11 Tile Anomaly Detection (TileEngineering Content Recognition MVTec Anomaly Det) 12 Toothbrush-Anomaly Detection (Toothbrush Anomaly Det) 13 Transistor Anomaly Detection (Transistor-Anomaly Det) 14 Wood Anomaly Detection (WoodAnomaly Det) (Bergmann et al., 2021) Engineering Content Recognition MVTec (Bergmann et al., 2021) Engineering Content Recognition MVTec (Bergmann et al., 2021) Engineering Content Recognition MVTec (Bergmann et al., 2021) 15 Zipper Anomaly Detection (ZipperEngineering Content Recognition MVTec Anomaly Detection) 16 Cookie Anomaly Detection (CookieAnomaly Det) 17 Macaroni Anomaly Detection (Macaroni-Anomaly Det) 18 Marble Surface Anomaly Detection (Marble-Anomaly Det) 19 Pcb Anomaly Detection (PcbAnomaly Det) (Bergmann et al., 2021) Engineering Content Recognition Industry Biscuit (Cookie) dataset (Horak al., 2022) et Engineering Content Recognition Visual Anomaly Dataset (VisA) (Zou et al., 2022) Engineering Content Recognition Marble Surface Anomaly Detection (Mar) Engineering Content Recognition Visual Anomaly (VisA) Dataset (Zou et al., 2022) 1 COVID-19 CT Scan Lesion Segmentation (COVID-19 CT Scan Lesion Seg) Medicine Content Recognition COVID-19 CT scan lesion segmentation dataset (COV, a) 89 DeSTSeg (Zhang et al., 2023i) 57 DeSTSeg (Zhang et al., 2023i) 70 DeSTSeg (Zhang et al., 2023i) 92 DeSTSeg (Zhang et al., 2023i) 93 DeSTSeg (Zhang et al., 2023i) 141 DeSTSeg (Zhang et al., 2023i) 119 DeSTSeg (Zhang et al., 2023i) 84 DeSTSeg (Zhang et al., 2023i) 30 DeSTSeg (Zhang et al., 2023i) 40 DeSTSeg (Zhang et al., 2023i) 60 DeSTSeg (Zhang et al., 2023i) 119 DeSTSeg (Zhang et al., 2023i) 800 CLIP (Radford et al., 2021) 280 WinCLIP (Radford et al., 2021) 502 CLIP (Radford et al., 2021) AP AP AP AP AP AP AP AP AP AP AP AP Acc Acc Acc 280 WinCLIP (RadAcc ford et al., 2021) 290 Unet (COV, b) mIoU 2 Hubmap Organ (Hubmap Organ Seg) Segmentation Medicine Content Recognition hubmap organ 512 331 CLIPseg 512 (Hub) (Luddecke and Ecker, 2022) 3 Lung segmentation (Lung Seg) Medicine Content Recognition Lung Mask Image Dataset (Lun, b) 4 Bacteria Segmentation (Bacteria Seg) 5 Brain FLAIR Abnormality Segmentation (Brain Seg) Medicine Content Recognition Bacteria detection with darkfield microscopy (Bac) Medicine Content Recognition Brain MRI segmentation (Buda et al., 2019a) 500 CLIPseg (Luddecke and Ecker, 2022) 366 CLIPseg (Luddecke and Ecker, 2022) 788 BrainUNet (Buda et al., 2019b) 96 mIoU mIoU mIoU mIoU #I-C-19 Medical Segmentation (Med Seg) On Path to Multimodal Generalist: General-Level and General-Bench Skill (Meta-Task) # Task (Short Name) Domain Capability Data Source Number Model Metrics Task Data SoTA Specialist 6 Tuberculosis X-ray Segmentation (X-ray Seg) General Content Recognition Chest X-ray for 704 CLIPSeg (Luddecke and Ecker, 2022) mIoU Dataset Tuberculosis Segmentation (Che) #I-C-20 Keypoint Detection (Keypoint Det) #I-C-21 Multi-image Visual Question Answering (Multi-img VQA) 1 Person Keypoint Detection (Person Keypoint Det) General Content Recognition OCHuman et (Zhang 2019) 600 ViTPose et al., 2022) al., (Xu mAP@0.5 1 Image Pair Multi-Attribute Question Answering (Img-Pair QA) 2 Numerical Dual Image Description Verification (Dual-Img Verify) General Content Recognition NLVR2 et al., 2019) General Reasoning Ability NLVR2 et al., 2019) (Suhr (Suhr 500 Mantis (Jiang et al., 2024c) 500 Mantis (Jiang et al., 2024c) Acc Acc 3 Aircraft Model Matching (AircraftGeneral Content Recognition FGVC-Aircraft 500 Mantis (Jiang Acc Model Match) 4 Car Model Matching (Car-Model Match) General Content Recognition Stanford-car (sta, 500 Mantis (Jiang (fgv) et al., 2024c) 5 Pose and Activity Consistency Verification (Action-Consist Verify) General Action Analysis Affective b) CUHK03 (cuh) #I-C-22 Multimodal Dialogue (MM Dialog) 6 Digital Consistency Comparison (Digital-Consist Compare) General Reasoning Ability MNIST (LeCun) 1 Egocentric Daily Tasks Action Planning (Ego-Action Planning) 2 Context-Aware Embodied Agent Dialogue (Embodied Dialogue) 3 Embodied Navigating Visual Question Answering (Navigating VQA) 4 Environment-Based Next-action Description (Next-action Description) General General General Geography, Earth Planning Ability, Interactive Capability Interactive Capability Interactive Capability Interactive Capability ALFRED (Shridhar et al., 2020) ALFRED (Shridhar et al., 2020) ALFRED (Shridhar et al., 2020) ALFRED (Shridhar et al., 2020) 5 Multimodal Decision-making Reasoning (Decision-making Reasoning) 6 Comic Completion Dialogue (Comic-Dialog Complete) General Interactive Capability ALFRED (Shridhar et al., 2020) Art Interactive Capability COMICSDialogue (Iyyer et al., 2017) #I-C-23 Multimodal Reasoning (MM Reason) 1 Fashion Concept Visual Question Answering (Fashion VQA) 2 Cloth Color Visual Question Answering (Cloth-Color VQA) 3 Multi-Image Visual Entailment Reasoning (Multi-Img Entailment Reason) 4 Visual Step Completion (VisualStep Cloze) 5 Recipe Ingredient Description (Recipe Description) Fashion Content Recognition Fashion200K Fashion Reasoning Ability General Reasoning Ability NLVR2 General Reasoning Ability General Reasoning Ability (Han et al., 2017) Fashion200K (Han et al., 2017) (Suhr et al., 2019) RecipeQAVisualCloze (Yagcioglu et al., 2018) RecipeQAVisualCloze (Yagcioglu et al., 2018) RecipeQAImageCoherence (Yagcioglu et al., 2018) 6 Visual Step Matching Reasoning (Step-Matching Reason) General Reasoning Ability 7 Industrial Inspection Multi-Image Reasoning (Inspec-Multi-Img Reason) Engineering Reasoning Ability VISION (Shullani et al., 2017) 8 Property Coherence Reasoning General Reasoning Ability MIT-States- (Property Reason) 9 Recipe Completion (Recipe Completion) General Reasoning Ability PropertyCoherence (Isola et al., 2015) RecipeQATextCloze (Yagcioglu et al., 2018) 97 et al., 2024c) 500 mPLUGDocowl2 et al., 2024c) (Hu 500 Mantis (Jiang et al., 2024c) Acc Acc Acc 500 VPG-C (Li et al., 2023e) 500 VPG-C (Li et al., 2023e) ROUGE-L ROUGE-L 500 MLoEM (Wei et al., 2024) ROUGE-L 500 LLaVA-NeXTInterleave et al., 2024h) 500 VPG-C (Li et al., (Li 2023e) 500 Mantis (Jiang et al., 2024c) 500 MLoEM (Wei et al., 2024) 500 MLoEM (Wei et al., 2024) 500 MLoEM (Wei et al., 2024) 500 LLaVA-NeXTInterleave et al., 2024h) (Li 500 Mantis (Jiang et al., 2024c) 500 MLoEM (Wei et al., 2024) 500 MLoEM (Wei et al., 2024) 500 MLoEM (Wei et al., 2024) 500 VPG-C (Li et al., 2023e) ROUGE-L ROUGE-L Acc Acc Acc Acc Acc Acc Acc Acc Acc Acc On Path to Multimodal Generalist: General-Level and General-Bench Skill (Meta-Task) # Task (Short Name) Domain Capability Data Source Number Model Metrics Task Data SoTA Specialist 10 Comic Panel VQA (Comic-Panel VQA) Art Reasoning Ability COMICS-Panel (Iyyer 2017) et al., 11 Abduction-based VQA (Abduction VQA) 12 Dual-Image Causal-and-Effect Reasoning (Dual-Img Causal Reason) 13 Driving Recording Reasoning (Drive-Record Reason) 14 Attribute Congruity Reasoning (Attri-Congruity Reason) 1 Crowd Counting (Crowd Count) 2 Face Counting (Face Count) #I-C-24 Object Counting Object Count General Reasoning Ability VizWiz (Gurari et al., 2018) General Reasoning Ability VizWiz (Gurari Geography Reasoning Ability et al., 2018) nuScenes (Caesar et al., 2020) General Reasoning Ability MIT-StatesStateCoherence (Isola et al., 2015) 500 VPG-C (Li et al., 2023e) 500 VPG-C (Li et al., 2023e) 500 MLoEM (Wei et al., 2024) 500 VPG-C (Li et al., 2023e) 500 MLoEM (Wei et al., 2024) General Content tion, Ability General Reasoning Ability RecogniReasoning Crowd Counting (Loy et al., 2013a) 650 CLIPCount (Jiang et al., 2023) 650 CLIPCount (Jiang et al., 2023) 3 Galaxy Counting (Galaxy Count) Physics Reasoning Ability 597 CLIPCount (Jiang et al., 2023) MAE Count the number of Faces present in an Image (Cou, a) FITS Images for Object Counting and Detection (FIT) seeds (See) counting 4 Seed Counting (Seed Count) 5 Vehicle Counting (Vehicle Count) General Content tion, Ability General Content 6 Mall Crowd Counting (Mall-Crowd Count) 7 Paperclips Counting (Paperclip Count) 8 Wheat Head Counting (Wheat-Head Count) tion, Ability General Content tion, Ability General Content tion, Ability General Content tion, Ability 9 Pipe Counting (Pipe Count) General Content tion, Ability RecogniReasoning 141 CLIPCount (Jiang et al., 2023) RecogniReasoning CARPK (Hsieh et al., 2017) 459 CLIPCount (Jiang et al., 2023) RecogniReasoning Mall Dataset (Loy et al., 2013b) 500 CLIP (Radford et al., 2021) RecogniReasoning Count the Paperclips (Cou, b) 500 CLIPCount (Jiang et al., 2023) RecogniReasoning RecogniReasoning Global Wheat Dataset Head 2021 (David et al., 2020) Object Counting Using Pipes Dataset (Cou, c) Acc Acc Acc Acc Acc MAE MAE MAE MAE Acc MAE MAE #I-C-25 Multimodal Neural Translation (NMT) #I-C-26 Object Detection (Object Det) 1 Multimodal Neural Translation (NMT) Linguistics Reasoning Ability Multi30k (Chumpu) 1 Car License Plate Detection (CarLicense Det) General Content Recognition Car License Plate Detection (Car, a) 2 Face Mask Detection (Face-Mask Det) General Content Recognition Face Mask Detection (mak) 3 Far Vehicle Detection (Far-Vehicle Det) General Content Recognition Far Vehicle Detection (Far) 4 Small Object Detection (Small-Obj Det) 5 Traffic Signs Detection (TrafficSigns Det) General Content Recognition, Commonsense Knowledge General Content Recognition, Commonsense Knowledge Grand Dataset for small object detection (sma) Traffic Signs Detection (Tra) 6 Bone Fracture Detection (BoneFracture Det) Medicine Content Recognition Bone Fracture Detection (Bon) 98 500 CLIPCount (Jiang et al., 2023) 240 CLIPCount (Jiang et al., 2023) MAE 500 mPLUGDocowl2 et al., 2024c) (Hu Acc et 433 OWL-VIT (Minal., derer 2022b) 500 YOLO-World (Cheng 2024) et mAP@0.5 mAP@0.5 al., 221 OWL-VIT (Minal., et derer 2022b) 512 OWL-VIT (Minal., et derer 2022b) 634 OWL-VIT (Minal., et derer 2022b) 500 OWLVIT (Minal., et derer 2022a) mAP@0.5 mAP@0.5 mAP@0. DICE On Path to Multimodal Generalist: General-Level and General-Bench Skill (Meta-Task) # Task (Short Name) Domain Capability Data Source Number Model Metrics Task Data SoTA Specialist 7 Aircraft Detection (Aircraft Det) 8 Medical Device Detection (MedicalDevice Det) General Content Recognition Aircraft Detection (AhmedMohsen, 2022) Medicine Content Recognition Medical Device Detection (Hospital, 2024) 603 OWLVIT (Minal., et derer 2022a) 500 GLIP-T et al., 2022) DICE (Li* mAP@0.5 9 Wound Detection (Wound det) Medicine Content Recognition Wound detection (Project, 2024) 500 GLIP-T et al., 2022) (Li* mAP@0.5 10 Gauge Detection (Gauge Det) General Content Recognition Gauge Detection 500 GLIP-T (Li* mAP@0.5 (Wannakrairot, 2023) et al., 2022) 11 Parking Space Detection (ParkingSpace Det) 12 Tree Detection (Tree det) 13 Traffic Light Detection (TrafficLight Det) 14 rock-paper-scissors-gesturedetection (Gesture Det) General Content Recognition Parking Space Detection (Workspace, 2024) Geography Content Recognition Tree (patrick 2023) General Content Recognition Traffic detection Cai, 500 GLIP-T et al., 2022) (Li* mAP@0.5 500 GLIP-T et al., 2022) (Li* mAP@0.5 Light (artz, 500 GLIP-T et al., 2022) (Li* mAP@0. Detection 2024) General Content Recognition rock-paper500 GLIP-T (Li* mAP@0.5 scissors-gesturedetection (Roboflow, 2025) et al., 2022) 15 Price Tag Detection and Text Grounding (Price-Tag Det) General Content Recognition Price Tag Detection and Text Grounding (alkud12, 2024) 500 YOLO-World (Cheng 2024) et al., mAP@0.5 16 Retail Object Detection (Retail-Obj Det) General Content Recognition Retail object detection (Polyu, 2022) 17 Exit Sign Detection (Exit-Sign Det) General Content Recognition Exit Sign Detection (KIT, 2023) 18 Handwriting Component Detection (Handwrite-Component Det) General Content Recognition Handwriting Component Detection (Hoang, 2024) 19 Sign Language Detection (SignLang Det) Linguistics Content Recognition Sign Language (lanDetection guage, 2024) 20 football player detection (FootballPlayer Det) General Content Recognition football player detection (FootballVideoTrackingApp, 2024) 21 Pest Detection (Pest Det) Biology Content Recognition Pest Detection (intern, 2024) 22 Underwater (Garbage Det) garbage Detection General Content Recognition Underwater garbage tection rehman, 2024) De- (faiza et 500 YOLO-World (Cheng 2024) 500 YOLO-World (Cheng 2024) 500 YOLO-World (Cheng 2024) et et et 500 YOLO-World (Cheng 2024) 500 YOLO-World (Cheng 2024) et et 500 YOLO-World (Cheng 2024) 500 YOLO-World (Cheng 2024) et 23 Food Detection (Food Det) General Content Recognition Food Detection (Myworkspace, 2024) 24 Circuit Elements Detection (Circuit Det) General Content Recognition Circuit elements (100, 2024) et 500 YOLO-World (Cheng 2024) 500 YOLO-World (Cheng 2024) et al., al., al., al., al., al., al., al., al., 25 Tabular Data Detection (Tabular Det) 26 Radio-Spectrogram Detection (Radio Det) General Content Recognition Tabular detection 2023a) General Content Recognition Radio data (100, 500 OWL-VIT (Minal., et derer 2022b) spectrogram detection (100, 2023b) 500 OWL-VIT (Minal., et derer 2022b) mAP@0.5 mAP@0.5 mAP@0.5 mAP@0.5 mAP@0.5 mAP@0. mAP@0.5 mAP@0.5 mAP@0.5 mAP@0.5 mAP@0.5 On Path to Multimodal Generalist: General-Level and General-Bench Skill (Meta-Task) # Task (Short Name) Domain Capability Data Source Number Model Metrics Task Data SoTA Specialist 27 Empty Shelf Detection (EmptyShelf Det) General Content Recognition empty-shelfdetection (week3day3, 2024) 28 Tomato Detection (Tomato Det) 29 Drone Detection (Drone Det) General Content Recognition Tomato Detection (Tom) General Content Recognition Drone Detection (Dro) 30 Bee Detection (Bee Det) General Content Recognition Bee Detection (Bee) General Content Recognition Cat Faces Detection (Cat) General Content Recognition CUB 200 Bird Species XML Detection Dataset (CUB) General Content Recognition Deep Fish Object Detection (Dee) General Content Recognition Trash Panda Detection (Tran) General Content Recognition Ship Detection Aerial from Images (Shi) General Content Recognition Weed Detection (Wee) General Content Recognition SARscope (SAR) 31 Cat Faces Detection (Cat Det) 32 Bird Detection (Bird Det) 33 Deepfish Detection (Deepfish Det) 34 Raccoon Detection (Raccoon Det) 35 Ship Detection (Ship Det) 36 Weed Detection (Weed Det) 37 Radar Ship Detection (Radar-Ship Det) 1 Animal Image Matting (AnimalImg Mat) 2 Portrait Matting (Portrait Mat) 500 OWL-VIT (Minal., et derer 2022b) mAP@0.5 895 GroundingDINO (Liu et al., 2024e) 596 GroundingDINO (Liu et al., 2024e) 836 GroundingDINO (Liu et al., 2024e) 500 GroundingDINO (Liu et al., 2024e) 500 GroundingDINO (Liu et al., 2024e) AP@0.6 AP@0.7 AP@0. AP@0.9 AP@0.75 500 GroundingDINO (Liu et al., 2024e) 196 GroundingDINO (Liu et al., 2024e) 621 GroundingDINO (Liu et al., 2024e) AP@0.5 AP@0.5 AP@0. 500 GroundingDINO (Liu et al., 2024e) 672 GLIP (Li* et al., 2022) AP@0.5 mAP@0.5 Animal Content Recognition AM2K (Li et al., 2022c) General Content Recognition PM-10K (Li et al., 2021) 200 MAM (Li et al., 2023f) 500 MAM (Li et al., 2023f) 3 Text Manipulation Region Matting (Text-Region Mat) Ethics Content Recognition Text ManipulaDetection tion (Tex) 4 Person Hair Matting (Person-Hair Mat) General Content Recognition Manual Construction (Xiao et al., 2021) 1 Animal Recog) Recognition (Animal 2 Big Cat Image Recognition (Big Cat Image Recog) Animal Content Recognition Animals-10 (Ani, a) General Content Recognition Big Cats Image Classification Dataset (Big) 3 Fruit Recognition (Fruit Recog) General Content Recognition Fruit Recognition (Fru) 4 Indonesian Wayang Type Recognition (Indonesian Recog) Culture Content Recognition Indonesian 5 Vegetable Recognition (Vegetable Recog) General Content Recognition, Commonsense Knowledge 6 Car Brands Recognition (Car-Brand Recog) 7 Damaged (Damaged-Egg Recog) General Content Recognition Car-LogoDataset (Car, b) Egg Recognition General Content Recognition Egg Image Dataset (Egg) 8 Dog Breeds Recognition (DogBreed Recog) General Content Recognition Dog (Dog) Breeds 9 Garbage Classification (Garbage Cls) 10 Mammal Recognition (Mammal Recog) General Content Recognition Garbage Classification (Gar) General Content Recognition Mammal Recognition (Mam) 11 Utensil Recognition (Utensil Recog) General Content Recognition Utensil Image Recognition (Ute) 100 Types Wayang (Ind) Vegetable Image Dataset (Veg) 500 CLIPseg (Luddecke and Ecker, 2022) 500 MAM (Li et al., 2023f) 510 CLIP (Radford et al., 2021) 738 CLIP (Radford et al., 2021) 660 CLIP (Radford et al., 2021) 233 CLIP (Radford et al., 2021) 600 CLIP (Radford et al., 2021) 480 CLIP (Radford et al., 2021) 412 OWLVIT (Minal., et derer 2022a) 467 CLIP (Radford et al., 2021) 498 CLIP (Radford et al., 2021) 495 OWLVIT (Minal., et derer 2022a) 475 CLIP (Radford et al., 2021) SAD SAD mIoU MSE Acc Acc Acc Acc Acc Acc Acc Acc Acc Acc Acc #I-C-27 Object Matting (Object Mat) #I-C-28 Object Recognition (Object Recog) On Path to Multimodal Generalist: General-Level and General-Bench Skill (Meta-Task) # Task (Short Name) Domain Capability Data Source Number Model Metrics Task Data SoTA Specialist 15 Ball Image Classification (Ball Cls) General Content Recognition, Commonsense Knowledge 12 Boat Types Recognition (BoatTypes Recog) 13 Butterfly Recognition (Butterfly Recog) 14 Fish Recognition (Fish Recog) 16 Flower Image Classification (Flower Cls) 17 Food Image Classification (Food Cls) 18 Material Image Classification (Material Cls) 19 Plant Image Classification (Plant Cls) 20 Trash Image Classification (Trash Cls) 21 Vehicle Image Classification (Vehicle Cls) 22 Bird Species Recognition (Bird Recog) General Content Recognition Boat Types 640 CLIP (Radford Recognition (Boa) et al., 2021) General Content Recognition Butterfly Dataset al., et (Wang 2009) Fish Recognition Ground-Truth data (Fis) Ball Classification (Bal) 832 CLIPCount (Jiang et al., 2023) 456 CLIP (Radford et al., 2021) 300 CaSED (Conti et al., 2023) General Content Recognition, Commonsense Knowledge General Content Recognition, Commonsense Knowledge General Content Recognition, Commonsense Knowledge General Content Recognition, Commonsense Knowledge General Content Recognition, Commonsense Knowledge General Content Recognition, Commonsense Knowledge General Content Recognition, Commonsense Knowledge Flower Classification (flo) 300 CaSED (Conti et al., 2023) Data Food Category Classification (dat) Material Classification (Mat) 300 CaSED (Conti et al., 2023) 150 CaSED (Conti et al., 2023) Plant Classification (pla) 300 CaSED (Conti et al., 2023) Trash Classification (tra) 300 CaSED (Conti et al., 2023) Vehicle Classification (veh) 300 CaSED (Conti et al., 2023) General Content 23 Chinese Fine Art Recognition (ZhArt Art Recog) 24 Famous Iconic Women Recognition (Iconic-Women Recog) 25 Text Manipulation Classification (Text-Manipulation Cls) Humanities, History Ethics Bird Species Classification (Bir) Recognition, Commonsense Knowledge Commonsense Knowledge Commonsense Knowledge Content Recognition Text Manipulation Classification (Tex) Chinese Fine Art (Chi, b) Famous Women (Fam) Iconic 307 CLIP (Radford et al., 2021) 398 CLIP (Radford Acc et al., 2021) 225 CLIP (Radford et al., 2021) 500 CLIP (Radford et al., 2021) Acc Acc 26 Gender Recog) Recognition (Gender General Content Recognition Gender Detection & Classification - Face Dataset (Fac, b) 300 CLIP (Radford Acc et al., 2021) 27 Infrared/Thermal Image Classification (Infrared Image Cls) 28 Waste Item Image Classification (Waste Cls) 29 Image Style Classification (Style Cls) Art Infrared Content Recognition Radar Threat Object Classification (Rad, b) Manual Construction (Rea, a) Recognition, Commonsense Knowledge Content Recognition Stylebooth (Han General Content et al., 2024) 30 Microorganism Image Classification (Microorganism Cls) 31 Multimodal Named Entity Recognition (MNER) Biology Content Recognition Manual Construction (Mic) General Content Recognition Twitter- (Yu et al., 2020a) 32 Deep Fake Detection (Deep-Fake Det) Ethics Content Recognition DF40 (Yan et al., 2024a) 521 Mantis (Jiang et al., 2024c) 540 CLIP (Radford et al., 2021) 536 CaSED (Conti et al., 2023) 504 CaSED (Conti et al., 2023) 500 UMT (Yu et al., 2020b) 500 NPR (Tan et al., 2024) #I-C-29 Panoptic Segmentation (Panoptic Seg) 1 Road Scene Panoptic Segmentation (Road Pano-Seg) General Content Recognition, Commonsense Knowledge Cityscapes (Cordts et 2016) al., 500 PanopticDeeplab (Cheng et al., 2020) #I-C-30 Pose Recognition (Pose Recog) 1 Yoga Pose Recognition (Yoga Recog) Sports Content Recognition Yoga Pose Classification (Yog) 101 988 CLIP (Radford et al., 2021) Acc Acc Acc Acc Acc Acc Acc Acc Acc Acc Acc Acc Acc Acc Acc F1 Acc PQ Acc On Path to Multimodal Generalist: General-Level and General-Bench Skill (Meta-Task) # Task (Short Name) Domain Capability Data Source Number Model Metrics Task Data SoTA Specialist #I-C-31 Relation Reasoning (Rel Reason) #I-C-32 RGBD Semantic Segmentation (RGBD-Sem Seg) #I-C-33 Ripeness Recognition (Ripe Recog) 1 Classification of Visual Spatial Relationship (Spatial-Rel Cls) General Content nition, Perception 2 Description of Single Spatial Relationship (Single Spatial-Rel Description) 3 Description of Open-ended Spatial Relationship (Open Spatial-Rel Description) General Content nition, Perception General Content nition, Perception RecogSpatial VSD (Zhao et al., 2022) 500 VidVRD (Yang et al., 2024b) F1 RecogSpatial VSD (Zhao et al., 2022) 500 VidVRD (Yang et al., 2024b) BLEU-4 & SPICE RecogSpatial VSD (Zhao et al., 2022) 500 VidVRD (Yang et al., 2024b) BLEU-4 & SPICE 1 RGBD Semantic Segmentation (RGBD-Sem Seg) 1 General Content Recognition, Commonsense Knowledge Nyudv2 (Silberman et al., 2012) 654 DFormer-L (Yin et al., 2024) mIoU 1 Strawberry Ripeness Recognition (Strawberry Recog) General Content Recognition Strawberry Classification (Str) 500 OWLVIT (Minal., et derer 2022a) Acc 500 GLIP-T et al., 2022) (Li* mAP@0.5 500 YOLO-World (Cheng 2024) et al., mAP@0.5 500 OWL-VIT (Minal., et derer 2022b) mAP@0.5 mAP@0.5 mAP@0.5 mAP@0.5 1 Fire Detection (Fire Det) 2 Gun Detection (Gun Det) #I-C-34 Safety Detection (Safe Det) 3 Construction Safety Equipment Detection (Safe-Equip Det) General Content Recognition Fire Detection (RFES, 2024) General Content Recognition Gun Detection (Jiraphat, 2023) General Content Recognition Construction Safety Equipment (100, Detection 2023c) 4 Safety Vests Detection (Safe-Vest Det) 5 Fall Human Detection (Fall-Human Det) 6 Smoke Detection (Smoke Det) General Content Recognition Safety Vests Detection (Projects, 2024) General Content Recognition Fall Human De- (custom tection yolov5, 2024) General Content Recognition Smoke Detection (heejin, 2024) 500 OWL-VIT (Minal., et derer 2022b) 500 OWL-VIT (Minal., et derer 2022b) 500 OWL-VIT (Minal., et derer 2022b) #I-C-35 Scene Graph Generation (SG Gen) #I-C-36 Scene Recognition (Scene Recog) #I-C-37 Sign Language Recognition (Sign Recog) #I-C-38 Visual Relation Inference (Vis Rel Inf) 7 Helmet Detection (Helmet Det) General Content Recognition Helmet Detection (Bik) 764 Grounding DINO (Liu et al., 2023b) AP@0.5 1 Image Scene Graph Parsing (SG Parsing) General Reasoning Ability Visual Genome (Krishna et al., 2017) 767 Graph-RCNN (Yang 2022a) et al., R@50 1 Terrain Recognition (Terrain Recog) 1 1 1 2 Landscape Recognition (Landscape Recog) Nature Nature Content Recognition Terrain Recognition (Ter) RecogniContent tion, Commonsense Knowledge Landscape Recognition (Lan) 500 CLIP (Radford et al., 2021) 500 CLIP (Radford et al., 2021) 1 American Sign Language Recognition (Sign-Language Recog) Linguistics Content Recognition American Sign Language Recognition (Ame) 540 YOLOV5 (Jocher, 2020) Acc Acc F1 1 Before-After Relationship Caption Art Content Recognition IEdit (Bodur et al., 500 VPG-C (Li et al., ROUGE-L (Before-After-Rel Cap) 2 Surveillance Object-Level Change Description (Object-Change Cap) 2024) 2023e) General Content Recognition Spot-the-Diff and (Jhamtani Berg-Kirkpatrick, 2018) 500 MLoEM (Wei et al., 2024) ROUGE-L 3 Multi-Image Alteration Description (Multi-Img-Alter Cap) General Content Recognition CLEVR-Change (Johnson et al., 2017) 500 MLoEM (Wei et al., 2024) ROUGE-L 4 Bird Variation Comparison Description (Bird-Compare Cap) Biology Content Recognition Birds-to-Words et al., 500 VPG-C (Li et al., 2023e) ROUGE-L (Forbes 2019) 102 On Path to Multimodal Generalist: General-Level and General-Bench Skill (Meta-Task) # Task (Short Name) Domain Capability Data Source Number Model Metrics Task Data SoTA Specialist 5 Subtle Difference Description (Subtle-Diff Cap) Biology Content Recognition Birds-to-Words (Forbes 2019) et al., #I-C-39 Visual Storytelling Generation (Vis-Story Gen) 6 Multimodal Relation Extraction (MRE) General Content Recognition MNRE (Zheng et al., 2021) 1 Animated Story Completion (Animated Story Completion) General Commonsense Understanding AESOP et al., 2021) (Ravi 2 Cartoon Story Telling (CartoonStory Gen) General Commonsense Understanding PororoSV et al., 2019a) (Li 3 Multi-image Next-frame Description (Next-frame Cap) Humanities Commonsense Understanding 4 Sequential Photo Storytelling (SeqHumanities Commonsense UnPhoto Gen) derstanding 5 Sequential Story Completion (SeqStory Completion) Humanities Commonsense Understanding et al., FlintstonesSV (Gupta 2018) VIST et al., 2016) DiDeMoSV (Maharana et al., 2022) (Huang 500 LLaVA-NeXTInterleave et al., 2024h) 500 BertNRE (Soares et al., 2019) (Li 500 LLaVA-NeXTInterleave et al., 2024h) (Li 500 LLaVA-NeXTInterleave et al., 2024h) 500 MLoEM (Wei et al., 2024) (Li ROUGE-L F1 ROUGE-L ROUGE-L ROUGE-L 500 VPG-C (Li et al., ROUGE-L 2023e) 500 VPG-C (Li et al., 2023e) ROUGE-L #I-C-40 Weather Recognition (Weather Recog) 1 Weather Recognition Recog) (Weather Climate Content Recognition Weather Image 495 CLIP (Radford Acc Recognition (Xiao, 2021) et al., 2021) 103 On Path to Multimodal Generalist: General-Level and General-Bench Table 23: Detailed list of all tasks and skills (meta-tasks) under image and generation category."
        },
        {
            "title": "Image Generation Group",
            "content": "Skill (Meta-Task) # Task (Short Name) Domain Capability Data Source Number Model Metrics Task Data SoTA Specialist #I-G-1 Edge-to-Image Generation (Edge2Img Gen) #I-G-2 EEG-to-Image Generation (EEG2Img Gen) #I-G-3 Image Denosing (Img Denose) #I-G-4 Image Enhancement (Img Enhance) #I-G-5 Image Inpainting (Img Inpaint) #I-G-6 Image Style Transfer (Img-Style Trans) #I-G-7 Imge-to-Mask Generation (Img2Mask Gen) #I-G-8 Image-to-Sketch Generation (Img2Sketch Gen) 1 Edge-to-Image (Edge2Img Gen) Generation General Creativity and Innovation SketchyCOCO (Gao et al., 2020) 1,340 PITI (Wang et al., 2022b) FID 1 EEG-based Image Generation (EEG2Img Gen) Medicine Creativity and Innovation DreamDiffusion (Bai et al., 2024) 500 DreamDiffusion (Bai et al., 2024) Acc 1 Image Deraining (Image Deraining) General Creativity and Innovation 2 Image Raindrop Removal (Raindrop General Creativity and InnoRemoval) vation Rain100H (Wenhan Yang and Yan, 2017) Raindrop (Qian et al., 2018) 500 GOUB (Yue et al., 2024b) PSNR 500 TransWeather PSNR (Valanarasu et al., 2021) 3 Image Dehazing (Img Dehazing) General Creativity and Innovation RESIDE-6K (Li et al., 2019b) 4 Image Desnowing (Img Desnowing) General Creativity and Innovation 5 Image Deblurring (Img Deblurring) General Creativity and Innovation 6 JPEG Image Artifact Deduction (Img Artifact Deduction) General Creativity and Innovation 7 Remote Sensing Image Dehazing (Remote-Sense Dehazing) Geography Creativity and Innovation Snow100K (Liu et al., 2018) GoPro (Nah et al., 2017) DIV2K+Flickr2K (Agustsson and Timofte, 2017) OD-GAN (Huang et al., 2020) 1 Image Shadow Removal (Image Shadow Removal) General Creativity and Innovation 2 Image Flare Removal (Img-Flare Removal) General Creativity and Innovation SRD (Guo et al., 2024a) Flare7K et al., 2022a) (Dai 3 Underwater Image Restoration (Underwater-Img Restorate) General Creativity and Innovation LSUI (Peng et al., 2023) 4 Document Image Unwarping (DocImg Unwarping) 5 Image Detail Enhancement (ImgDetail Enhance) General Creativity and Innovation General Creativity and Innovation 6 Low-light Image Enhancement (Low-light-Img Enhance) 1 Face Image Inpainting (Face Inpainting) General Creativity and Innovation General Creativity and Innovation (Ma DocUNet et al., 2018) FUnIE-GAN (Islam et al., 2020) SCI (Ma et al., 2022) CelebaHQ-256 (Karras 2018) et al., 1 Face Image Comic Style Transfer (Face2Comic Gen) 2 Text-based Style Transfer (Style Transfer) Art Art Creativity and Innovation Creativity and Innovation Manual Construction (Han et al., 2024) StyleBooth (Han et al., 2024) 500 DehazeFormer (Song 2023) et al., 500 ConvIR (Cui et al., 2024) 500 AdaRevD (Xintian Mao and Wang, 2024) PSNR PSNR PSNR 29 DA-CLIP et al., 2024) (Luo FID 536 DehazeFormer (Song 2023) et al., 408 DSC (Hu et al., 2020) 200 FF-Former (Zhang et 2023j) al., 500 CE-VAE (Pucci Martinel, and 2024) FDRNet et al., 2021) 500 FUnIE-GAN (Is- (Zhu 65 lam et al., 2020) 500 SCI (Ma et al., 2022) 500 MAT (Li et al., 2022d) PSNR RMSE PSNR PSNR MS-SSIM PSNR PSNR FID 500 FLUX 2023) (Labs, FID 536 StyleBooth (Han et al., 2024) PSNR AUC 1 Infection-map (Infection-map Gen) Generation Medicine Creativity and Innovation QaTa-COV19 (Degerli 2022) et 500 DDLA (Degerli et al., 2021) al., 1 Face Sketch Synthesis (Face-Sketch Gen) General Creativity and Innovation FS2K (Deng-Ping et al., 2022) 500 HIDA (Gao et al., 2023) FID 104 On Path to Multimodal Generalist: General-Level and General-Bench Skill (Meta-Task) # Task (Short Name) Domain Capability Data Source Number Model Metrics Task Data SoTA Specialist #I-G-9 In-context Image Editing (Img Edit) 1 Image Editing with Text and Image Prompt (Img-Edit with Txt+Img) General Creativity and Innovation 2 Image Editing with Multi-image Prompt (Img-Edit with Mul-Img) 3 Image Editing with Text and Multi-image Prompt (Img-Edit with Txt+Mul-Img) General Creativity and Innovation General Creativity and Innovation al., et InstructPix2Pix (Brooks 2023) StyleBooth (Han et al., 2024) StyleBooth (Han et al., 2024) 1 Layout-to-Face Image Generation (Layout2Face Gen) Ethics Creativity and Innovation Manual Construction (Pretty Face) 500 InstructPix2Pix (Brooks 2023) et al., 500 StyleBooth (Han et al., 2024) 500 StyleBooth (Han et al., 2024) 500 LayoutDiffusion al., et (Zheng 2023b) CLIPScore CLIPScore CLIPScore FID #I-G-10 Layout-to-Image Generation (Layout2Img Gen) #I-G-11 Mask-to-Image Generation (Mask2Img Gen) #I-G-12 Sketch-to-Image Generation (Sketch2Img Gen) #I-G-13 Sound-to-Image Generation (Sound2Img Gen) #I-G-14 Text-based Image Editing (Text-based Img Edit) #I-G-15 Text-to-Image Generation (Txt2Img Gen) 1 Mask-to-Image Face Generation (Mask2Face Generation) General Creativity and Innovation UniControl (Qin et al., 2023a) 500 UniControl (Qin et al., 2023a) PSNR 1 Sketch-to-Face Image Generation (Sketch2Face Gen) 2 Sketch-to-Background Image Generation (Sketch2BG Gen) General Creativity and Innovation General Creativity and Innovation, Planning Ability Manual Construction (Pretty Face) SketchyCOCO (Gao et al., 2020) 500 PITI (Wang et al., 2022b) 1,033 PITI (Wang et al., 2022b) 3 Sketch-to-Scene Image Generation (Sketch2Scene Gen) 4 Sketch-to-Image Hair Generation (Sketch2Hair Generation) General Creativity and Innovation General Creativity and Innovation SketchyCOCO (Gao et al., 2020) HairCLIP (Wei et al., 2022) 542 PITI (Wang et al., 2022b) 500 HairCLIP (Wei et al., 2022) 1 Sound-to-Image (Sound2Img Gen) Generation Physics Creativity and Innovation GlueGen et al., 2023b) (Qin 500 GlueGen (Qin et al., 2023b) 1 Image Colorization (Img Colorization) 2 Chinese-based Image Editing (ZhImg Editing) 3 Text-based Image Editing (Txtbased Img Editing) General Creativity and Innovation Linguistics Creativity and Innovation General Creativity and Inno4 Deep Fake Generation (Dee-Fake Gen) Ethics vation Creativity and Innovation Manual Construction (Ima) SEED-Data-Edit (Ge et al., 2024b) StyleBooth (Han et al., 2024) DF40 (Yan et al., 2024a) 500 DDNM (Wang et al., 2023c) 500 SEED-X (Ge et al., 2024c) 500 StyleBooth (Han et al., 2024) 500 Collaborative Diffusion (Huang et al., 2023b) FID FID FID PSNR CLIPScore FID CLIPScore CLIPScore FID 1 Text-based E-commerce Product Image Generation (Text2Product Gen) 2 Text-based Terrestrial Animal Image Generation (Txt2TerAnimal Gen) General Creativity and Innovation Animal Creativity and Innovation Manual Manual 2,907 FLUX 2023) 1,944 FLUX 2023) (Labs, FID (Labs, PSNR 3 Long-text-based Image generation General Creativity and Inno- (LongTxt2Img Gen) vation 4 Multi-language-based Image Generation (MulLan2Img Gen) General Creativity and Innovation 5 Handwriting Text Generation (Handwrite-Txt Gen) Face 6 Text-based (Txt2Face Gen) Generation 7 Text-based Astronomical Image Generation (Txt2Astronomy Gen) 8 Text-to-Image Indian Food Generation (Txt2Food Gen) 9 Text-to-Image Microorganism Generation (Txt2Microorgan Gen) 10 Text-to-Image Insect Generation (Txt2Insect Gen) Humanity Creativity and Innovation General Creativity and Innovation Astronomy Creativity and Innovation Humanities Creativity and Innovation Biology Creativity and Innovation Biology Creativity and Innovation Creativity and Innovation 11 Text-to-Image Sea Animal Generation (Txt2SeaAnimal Gen) Biology, Animal ParaDiffusion (Wu et al., 2023c) GlueGen (Qin et al., 2023b) Manual Manual Manual Manual Manual Manual Manual 500 ParaDiffusion (Wu et al., 2023c) (Qin 500 GlueGen et al., 2023b) FID FID 500 FLUX 2023) 500 FLUX 2023) 500 FLUX 2023) 560 FLUX 2023) 504 FLUX 2023) 500 FLUX 2023) 506 FLUX 2023) (Labs, FID (Labs, FID (Labs, PSNR (Labs, PSNR (Labs, PSNR (Labs, FID (Labs, PSNR 105 On Path to Multimodal Generalist: General-Level and General-Bench Table 25: Detailed list of all tasks and skills (meta-tasks) under video and comprehension category."
        },
        {
            "title": "Video Comprehension Group",
            "content": "Skill (Meta-Task) # Task (Short Name) Domain Capability Data Source Number Model Metrics Task Data SoTA Specialist #V-C-1 Video Question Answering (Video QA) 1 Agriculture Video Question Answering (Agri-Vid QA) 2 Geography Video Question Answering (Geo-Vid QA) 3 Human Survival Video Question Answering (Survival-Vid QA) Culture Content nition, Analysis General Content nition, Analysis Humanity Content nition, Analysis RecogAffective WildQA (Castro et al., 2022) RecogAffective WildQA (Castro et al., 2022) RecogAffective WildQA (Castro et al., 2022) 4 Military Video Question Answering (Military-Vid QA) General Content Recognition WildQA (Castro et al., 2022) 75 Qwen-2-VL-72B al., et (Wang 2024a) 81 Qwen-2-VL-72B al., (Wang 2024a) 218 LLaVA-Videoet 72B-Qwen2 (Zhang et 2024f) 145 LLaVA-Videoal., 72B-Qwen2 (Zhang et 2024f) al., BLEU-1 BLEU-1 BLEUBLEU"
        },
        {
            "title": "5 Comedy Video Question Answering",
            "content": "(Comedy-Vid QA) General Content Recognition, Commonsense Knowledge VCGBench (Maaz et 2024) 6 Movie Video Question Answering (Movie-Vid QA) 7 Science Video Question Answering (Science-Vid QA) 8 Sports Video Question Answering (Sport-Vid QA) General Content Recognition VCGBench et RecogniContent tion, Commonsense Knowledge Humanity, Biology, Geometry Humanity Content Recognition VCGBench et (Maaz 2024) VCGBench (Maaz et 2024) (Maaz 2024) 9 Pets Video Question Answering (Pet-Vid QA) General Content Recognition VCGBench et (Maaz 2024) 10 Gymnastics Video Question Answering (Gymnastic-Vid QA) Humanity, Sports Content Recognition SportsQA 11 Ball Game Video Question Answering (Ball-Vid QA) Humanity Content Recognition, Commonsense Knowledge et al., 2024j) SportsQA et al., 2024j) al., al., al., al., al., (Li (Li"
        },
        {
            "title": "158 Aria (Li et al.,",
            "content": "2024i) BLEU-1 156 Aria (Li et al., 2024i) 341 Aria (Li et al., 2024i) 400 Aria (Li et al., 2024i) 72 Aria (Li et al., 2024i) Acc Acc Acc Acc 202 Aria (Li et al., 2024i) 350 Aria (Li et al., 2024i) BLEU-1 BLEU-1 12 Object Color Video Question Answering (Obj-Color-Vid QA) General Content Recognition ActivityNetQA (Yu et al., 2019) 13 Object Motion Video Question Answering (Obj-Motion-Vid QA) General Content Recognition ActivityNetQA (Yu et al., 2019) 14 Object Location Video Question Answering (Obj-Loc-Vid QA) General Content Recognition ActivityNetQA (Yu et al., 2019) 15 Object Direction Video Question Answering (Obj-Dir-Vid QA) General Content Recognition MVBench et al., 2024k) (Li 800 Qwen-2-VL-7B et al., 800 Qwen-2-VL-7B et al., 800 Qwen-2-VL-7B et al., 200 Qwen-2-VL-7B et al., (Wang 2024g) (Wang 2024g) (Wang 2024g) (Wang 2024g) 16 Education Video Question Answering (Edu-Vid QA) 17 Human-Object Interaction Video Question Answering (Interact-Vid QA) 1 Pets Video Recognition (Pets Video Recog) 2 Science Video Recognition (Sci-Vid Recog) 3 Video Object Counting (Vid-Obj Counting) #V-C-2 Video Object Recognition (Vid-Obj Recog) Knowledge Content Recognition VideoMME (Fu et al., 2024c) Humanity Content Recognition MMBench-Video (Fang et al., 2024) 300 Oryx-34B (Liu et al., 2024f) 111 Aria (Li et al., 2024i) General Content Recognition MMBench-Video (Fang et al., 2024) Knowledge Content Recognition MMBench-Video (Fang et al., 2024) MVBench (Li et al., 2024k) RecogCausality General Content nition, Discrimination 87 Aria (Li et al., 2024i) 100 Aria (Li et al., 2024i) 200 Qwen-2-VL-7B (Wang 2024g) et al., Acc Acc Acc BLEU-1 Acc Acc Acc Acc BLEU-1 4 Natural Disaster Video Recognition (Disaster-Vid Recog) General Content Recognition WildQA (Castro et al., 2022) 133 Qwen-2-VL-72B () BLEU 106 On Path to Multimodal Generalist: General-Level and General-Bench Skill (Meta-Task) # Task (Short Name) Domain Capability Data Source Number Model Metrics Task Data SoTA Specialist 5 Video Object Existence Recognition (Vid-Obj-Exist Recog) General Content Recognition MVBench et al., 2024k) 6 Video Object Interaction Recognition (Vid-Obj-Interact Recog) General Content Recognition MVBench et al., 2024k) (Li (Li 7 TV-Show Recognition (TV-Show Recog) Humanity, Art Content Recognition VideoMME (Fu et al., 2024c) 8 Video Sports Recognition (Sport Humanity Content Recognition VideoMME (Fu Recog) 9 Video Animal Recognition (Animal Recog) 10 Video Food Recognition (Food Recog) 11 Art Recognition (Art Recog) 12 Autos and Vehicles Video Captioning (Vehicles Cap) General Content Recognition VideoMME (Fu et al., 2024c) et al., 2024c) General Content Recognition VideoMME (Fu et al., 2024c) VideoMME (Fu et al., 2024c) Art, Culture RecogniContent tion, Commonsense Knowledge General Content Recognition, Commonsense Knowledge 13 Food Video Captioning (Food Cap) General Content Recognition, Commonsense Knowledge 200 Qwen-2-VL-7B et al., 200 Qwen-2-VL-7B et al., (Wang 2024g) (Wang 2024g) BLEU-1 BLEU-1 100 Oryx-34B (Liu et al., 2024f) 100 Oryx-34B (Liu et al., 2024f) 90 Oryx-34B (Liu et al., 2024f) 90 Oryx-34B (Liu et al., 2024f) 90 Oryx-34B (Liu et al., 2024f) Acc Acc Acc Acc Acc MMBench-Video (Fang et al., 2024) MMBench-Video (Fang et al., 2024) 33 LLaVA-Video41 LLaVA-Video72B-Qwen2 (Zhang et 2024f) 72B-Qwen2 (Zhang et 2024f) al., al., BLEU-1 BLEU-1 14 Video Salient Object Detection (Salient-Obj Det) 15 Human Video Salient Detection ( Hum-Salient Obj Det) General Content Recognition SegTrack, FBMS, vidsod 100 (Li et al., 2013; Ochs et al., 2014; Lin et al., 2024b) Humanities Content Recognition SegTrack, FBMS, (Li vidsod 101 et al., 2013; Ochs et al., 2014; Lin et al., 2024b) 16 Aquatic Video Camouflaged Object Detection (Aquatic COD) Animal Content Recognition MoCA (Lamdouar et al., 2020) 17 Terrestrial Video Camouflaged Object Detection (Terrestrial COD) Animal Content Recognition MoCA (Lamdouar et al., 2020) 69 RealFlow (Cho et al., 2024) S-measure 14 RealFlow (Cho et al., 2024) S-measure 49 92 SAM-PM (Meeran et 2024) SAM-PM (Meeran et 2024) al., al., S-measure S-measure 1 Human-Object Interaction Video Captioning (H-O-Interact-Vid Cap) Humanity Content Recognition, Commonsense Knowledge #V-C-3 Video Action Recognition (Vid Act Recog) 2 Human-Human Interaction Video Captioning (H-H-Interact-Vid Cap) Humanity Content Recognition, Commonsense Knowledge 3 BodyMotion Video Captioning (Body-Motion Cap) Humanity Content Recognition, Commonsense Knowledge 4 Musical Instruments Video Captioning (Instrument Cap) Art RecogniContent tion, Commonsense Knowledge 5 Sports and Exercise Video Captioning (Sports Cap) Humanity Content Recognition, Commonsense Knowledge 6 Facial Action Video Captioning (Face-Action Cap) Humanity Content Recognition, Commonsense Knowledge ucf101, hmdb51 (Soomro et al., 2012a; Kuehne et al., 2011) ucf101, hmdb52 (Soomro et al., 2012a; Kuehne et al., 2011) ucf101, hmdb53 (Soomro et al., 2012a; Kuehne et al., 2011) ucf101 (Soomro et al., 2012a) ucf101, k600 (Soomro et al., 2012a; Carreira et al., 2018) hmdb51 (Kuehne et al., 2011) 107 1,000 LLaVA-Video72B-Qwen2 (Zhang et 2024f) 500 LLaVA-Video72B-Qwen2 et (Zhang 2024f) 1,000 LLaVA-Video72B-Qwen2 (Zhang et 2024f) 401 LLaVA-Video72B-Qwen2 (Zhang et 2024f) 3,000 LLaVA-Video72B-Qwen2 (Zhang et 2024f) 300 LLaVA-Video72B-Qwen2 (Zhang et 2024f) al., al., al., al., al., al., BLEUBLEU-1 BLEU-1 BLEU-1 BLEU-1 BLEU-1 On Path to Multimodal Generalist: General-Level and General-Bench Skill (Meta-Task) # Task (Short Name) Domain Capability Data Source Number Model Metrics Task Data SoTA Specialist 7 Facial-Object Operations Video Captioning (Face-Operation Cap) Humanity Content Recognition, Commonsense Knowledge hmdb51 (Kuehne et al., 2011) 8 Arts and Crafts Video Captioning (Arts&Crafts Cap) Art Content Recognition, Commonsense Knowledge k600 et al., 2018) (Carreira 9 Personal Care Video Captioning (Person-Care Cap) 10 DailyLife and Skills Video Captioning (Skill Cap) 11 Entertainment-related Video Captioning (Entertain Cap) 12 Sign Language Video Recognition (Sign-Language Recog) 13 Video Action Sequence Understanding (Action-Seq Analy) Humanity Content Recognition, Commonsense Knowledge Humanity Content Recognition, Commonsense Knowledge Humanity Content Recognition, Commonsense Knowledge k600 et al., 2018) (Carreira k600 et al., 2018) (Carreira k600 et al., 2018) (Carreira Linguistics Content Recognition WLASL (Li et al., General Content RecogCausality nition, Discrimination 2020) MVBench et al., 2024k) (Li al., 257 LLaVA-Video72B-Qwen2 (Zhang et 2024f) 2,000 VideoChat2-7B (KunChang and Qiao, 2023) 2,000 VideoChat2-7B (KunChang and Qiao, 2023) 2,000 VideoChat2-7B (KunChang and Qiao, 2023) 2,000 VideoChat2-7B Li Li Li BLEU-1 BLEU-1 BLEU-1 BLEU-1 BLEU- (KunChang and Qiao, 2023) Li 1,404 NLA-SLR (Zuo et al., 2023) 188 Video XL (Shu et al., 2024) Acc BLEU-1 14 Video Action Sequence Prediction General Content RecogCausality MVBench et al., 2024k) (Li 200 Video XL (Shu et al., 2024) BLEU- (Action-Seq Pred) 15 Video Action Counting (Action Counting) 16 Video Action Ordering (Action Ordering) 1 Ball Sports Video Captioning (Sport Cap) nition, Discrimination nition, Discrimination General Content RecogCausality MVLU et al., 2024c) (Zhou 189 Video XL (Shu et al., 2024) General Content Recognition MVLU (Zhou et al., 2024c) 125 Video XL (Shu et al., 2024) Humanity Content Recognition, Commonsense Knowledge VideoMME (Fu et al., 2024c) Acc Acc BLEUBLEU-1 BLEU-1 BLEU-1 BLEU-1 BLEU-1 BLEU60 LLaVA-Video60 LLaVA-Video72B-Qwen2 (Zhang et 2024f) 72B-Qwen2 (Zhang et 2024f) al., al., 50 Aria (Li et al., 2024i) 224 Aria (Li et al., 2024i) Acc Acc 60 LLaVA-Video60 LLaVA-Video30 LLaVA-Video72B-Qwen2 (Zhang et 2024f) 72B-Qwen2 (Zhang et 2024f) 72B-Qwen2 et (Zhang 2024f) 72B-Qwen2 (Zhang et 2024f) 72B-Qwen2 (Zhang et 2024f) al., al., al., al., al., 33 LLaVA-Video56 LLaVA-Video2 Science and Technology Video Captioning (Sci&Tech Cap) Science, Engineering Content Recognition, Commonsense Knowledge VideoMME (Fu et al., 2024c) #V-C-4 Video Understanding (Vid Understand) 3 Music Video Question Answering (Music QA) Humanity, Art 4 Game Video Question Answering (Game QA) 5 Movie and Show Video Captioning (Movie Cap) Art, Culture Art RecogniReasoning RecogniReasoning Content tion, Ability Content tion, Ability Content Recognition, Commonsense Knowledge, Reasoning Ability al., VCGBench (Maaz et 2024) VCGBench et (Maaz 2024) VideoMME (Fu et al., 2024c) al., 6 Finance Video Captioning (Finance Cap) Finance Content Recognition, Commonsense Knowledge VideoMME (Fu et al., 2024c) 7 History and Literature Video Captioning (His&Lit Cap) 8 Business Video Captioning (Business Cap) Content Recognition, Commonsense Knowledge History, Humanities, Culture Business Content Recognition, Commonsense Knowledge VideoMME (Fu et al., 2024c) MMBench-Video (Fang et al., 2024) 9 Humor Video Captioning (Humor Cap) Humanity, Social RecogContent nition, Affective Analysis, Cognition Understanding MMBench-Video (Fang et al., 2024) 108 On Path to Multimodal Generalist: General-Level and General-Bench Skill (Meta-Task) # Task (Short Name) Domain Capability Data Source Number Model Metrics Task Data SoTA Specialist #V-C-5 In-the-Wild Video Object Segmentation (IW VOS) #V-C-6 General Video Object Segmentation (General VOS) #V-C-7 Street-Scene Video Object Segmentation (Street VOS) 1 In the Wild Automobile Video Object Segmentation (IW-Auto VOS) 1 1 1 1 2 In the Wild Human Video Object Segmentation (IW-Human VOS) 3 In the Wild Animal Video Object Segmentation (IW-Animal VOS) 4 In the Wild Furniture Video Object Segmentation (IW-Furniture VOS) 1 Automobile Video Object Segmentation (Auto VOS) 2 Human Video Object Segmentation (Human VOS) 3 Animal Video Object Segmentation (Animal VOS) 4 Sports Video Object Segmentation (Sports VOS) 1 Automobile Street-Scene Video Object Segmentation (Auto-Street VOS) 2 Human Street-Scene Video Object Segmentation (Human-Street VOS) 3 Bicycle Street-Scene Video Object Segmentation (Bicycle-Street VOS) mentation (Human RVOS) #V-C-8 Referring Video Object Segmentation (RVOS) 2 Animal Referring Video Object Segmentation (Animal RVOS) 3 Human Reasoning Video Object Segmentation (Human ReVOS) General Content RecogniInteractive tion, Capability Humanity Content Biology Content General Content General Content Humanity Content Biology Content Culture Content RecogniInteractive RecogniInteractive RecogniInteractive tion, Capability tion, Capability tion, Capability RecogniInteractive RecogniInteractive RecogniInteractive RecogniInteractive tion, Capability tion, Capability tion, Capability tion, Capability RecogniInteractive RecogniInteractive tion, Capability tion, Capability General Content Humanity Content General Content Recognition,Interactive Capability RecogniInteractive tion, Capability Biology Content RecogniInteractive tion, Capability Humanity Reasoning Ability, Interactive Capability 1 Human Referring Video Object SegHumanity Content #V-C-9 Reasoning Video Object Segmentation (ReVOS) #V-C-10 Temporal Action Detection (Temp Act Det) #V-C-11 Complex-Scene Reasoning Video Object Segmentation (C-ReVOS) 1 Animal Reasoning Video Object Segmentation (Animal ReVOS) 2 Automobile Reasoning Video Object Segmentation (Auto ReVOS) 1 Spatio-Temporal Static Action Detection (Static-Action Det) 2 Spatio-Temporal Dynamic Action Detection (Dynamic-Action Det) Biology Reasoning Ability, Interactive Capability General Reasoning Ability, Commonsense Knowledge General Reasoning Ability, Causality Discrimination General Reasoning Ability, Causality Discrimination 1 Human Complex-Scene Reasoning Video Object Segmentation (Human C-ReVOS) 2 Animal Complex-Scene Reasoning Video Object Segmentation (Animal C-ReVOS) Animal, Biology Commonsense Knowledge Reasoning Ability, Commonsense Knowledge General Reasoning Ability, 109 VIPSeg et al., 2022) (Miao 262 SAM et al., 2024) (Ravi mIoU VIPSeg et al., 2022) (Miao 500 SAM (Ravi mIoU et al., 2024) VIPSeg et al., 2022) (Miao SAM2 et al., 2024) (Ravi mIoU VIPSeg et al., 2022) (Miao 401 SAM et al., 2024) (Ravi mIoU YouTube-VOS (Xu et al., 2018) 266 SAM2 et al., 2024) (Ravi mIoU YouTube-VOS (Xu et al., 2018) 500 SAM2 et al., 2024) (Ravi mIoU YouTube-VOS (Xu et al., 2018) 500 SAM2 et al., 2024) (Ravi mIoU YouTube-VOS (Xu et al., 2018) 133 SAM2 et al., 2024) (Ravi mIoU Cityscapes (Cordts et 2016) Cityscapes (Cordts et 2016) Cityscapes (Cordts et 2016) al., al., al., 473 SAM2 et al., 2024) (Ravi mIoU 325 SAM2 (Ravi mIoU et al., 2024) 108 SAM et al., 2024) (Ravi mIoU Ref-DAVIS 2017 (Seo et al., 2020) 69 UniRef++ (Wu mIoU et al., 2023d) Ref-DAVIS 2017 (Seo et al., 2020) 25 UniRef++ et al., 2023d) (Wu mIoU ReVOS et al., 2024b) (Yan 500 UniRef++ et al., 2023d) (Wu mIoU ReVOS et al., 2024b) (Yan 500 UniRef++ et al., 2023d) (Wu mIoU ReVOS et al., 2024b) (Yan 259 UniRef++ et al., 2023d) (Wu mIoU HC-STVG2 (Tang et al., 2022) 200 TubeDETR (Yang et al., 2022b) vIoU HC-STVG2 (Tang et al., 2022) 200 TubeDETR (Yang et al., 2022b) vIoU SA-V (Ravi et al., 2024) 500 UniRef++ et al., 2023d) (Wu mIoU SA-V (Ravi et al., 2024) 108 UniRef++ et al., 2023d) (Wu mIoU On Path to Multimodal Generalist: General-Level and General-Bench Skill (Meta-Task) # Task (Short Name) Domain Capability Data Source Number Model Metrics Task Data SoTA Specialist 3 Indoor Dynamic Video Depth EstiGeneral Content Recognition Bonn (Palazzolo General Content Recognition Sintel (Butler et al., 2012) General Content Recognition Scannet et al., 2017) (Dai General Content Recognition KITTI (Geiger et al., 2012) et al., 2019) 3 Automobile Complex-Scene Reasoning Video Object Segmentation (Auto C-ReVOS) 4 Human and Part Complex-Scene Reasoning Video Object Segmentation (Human-Part C-ReVOS) 5 Equipment Complex-Scene Reasoning Video Object Segmentation (Equipment C-ReVOS) 1 Human Video Grounding (Human VG) #V-C-12 Video Grounding (Vid-Ground) 2 Animal Video Grounding (Animal VG) 3 Automobile Video Grounding (Auto VG) 1 Sythetic Video Depth Estimation (Syn VDE) 2 Indoor Static Video Depth Estimation (Static VDE) #V-C-13 Video Depth Estimation (Vid-Depth Est) mation (Dynamic VDE) 4 Street Scene Video Depth Estimation (Street VDE) 1 Color Aware Object Matching (Color-Obj Match) 2 Shape or Posture Aware Matching (Shape Match) 3 Textual or Logo Markers Aware Object Matching (Logo Marker Match) #V-C-14 Object Matching (Obj Match) 4 Size Aware Object Matching (Size Match) 5 Relative Position Aware Object Matching (Position Match) 6 Orientation and Movement Aware Object Matching (Motion Match) 7 Binding Relationship Aware Object Matching (Relation Match) 8 Object Markers Aware Object Matching (Object Marker Match) #V-C-15 Object Tracking (Obj Track) 1 Ball Tracking (Ball Track) 2 Vehicle Tracking (Vehicle Track) General Reasoning Ability, Commonsense Knowledge General Reasoning Ability, Commonsense Knowledge Humanity Reasoning Ability, Commonsense Knowledge General Reasoning Ability, Commonsense Knowledge, Causality Discrimination Biology Reasoning Ability, Commonsense Knowledge, Causality Discrimination General Reasoning Ability, Commonsense Knowledge, Causality Discrimination SA-V (Ravi et al., 2024) 117 UniRef++ et al., 2023d) (Wu mIoU SA-V (Ravi et al., 2024) 50 UniRef++ et al., 2023d) (Wu mIoU SA-V (Ravi et al., 2024) 50 UniRef++ et al., 2023d) (Wu mIoU VidSTG (Zhang et al., 2020) 200 TubeDETR (Yang et al., 2022b) vIoU VidSTG (Zhang et al., 2020) 200 TubeDETR (Yang et al., 2022b) vIoU VidSTG (Zhang et al., 2020) 200 TubeDETR (Yang et al., 2022b) vIoU 23 DepthCrafter (Hu et al., 2024d) 50 DepthCrafter (Hu et al., 2024d) 50 DepthCrafter (Hu et al., 2024d) 13 DepthCrafter (Hu et al., 2024d) 500 CoLVA (Zhou et al., 2025) 135 CoLVA (Zhou et al., 2025) 500 CoLVA (Zhou et al., 2025) 385 CoLVA (Zhou et al., 2025) 500 CoLVA (Zhou et al., 2025) absRel absRel absRel absRel ACC ACC ACC ACC ACC MMVM (MMV) MMVM (MMV) MMVM (MMV) MMVM (MMV) MMVM (MMV) MMVM (MMV) 500 CoLVA (Zhou ACC et al., 2025) MMVM (MMV) MMVM (MMV) 500 CoLVA (Zhou et al., 2025) 500 CoLVA (Zhou et al., 2025) ACC ACC VOT2018 (Kristan et al., 2018) 255 UniNext et al., 2023) (Yan AUC VOT2018 (Kristan et al., 2018) 95 UniNext et al., 2023) (Yan AUC General Content Recognition, Commonsense Knowledge, Reasoning Ability General Content Recognition, Commonsense Knowledge, Reasoning Ability General Content Recognition, Commonsense Knowledge, Reasoning Ability General Content Recognition, Commonsense Knowledge, Reasoning Ability General Content Recognition, Commonsense Knowledge, Reasoning Ability General Content Recognition, Commonsense Knowledge, Reasoning Ability General Content Recognition, Commonsense Knowledge, Reasoning Ability General Content Recognition, Commonsense Knowledge, Reasoning Ability General Content Recognition, Commonsense Knowledge General Content Recognition, Commonsense Knowledge 110 On Path to Multimodal Generalist: General-Level and General-Bench Skill (Meta-Task) # Task (Short Name) Domain Capability Data Source Number Model Metrics Task Data SoTA Specialist 3 Human Tracking (Human Track) 4 Animal Tracking (Animal Track) 5 General Objects Tracking (GeneralObj Track) 6 Human Part Tracking (Human-Part Track) 7 General Objects Part Tracking (Other-Part Track) 1 Long Video Human Tracking (LongVid-Human Track) 2 Long Video General Object Tracking (Long-Vid-Obj Track) 3 Long Video Animal Tracking (LongVid-Animal Track) 4 Long Video Vehicle Tracking (LongVid-Vehicle Track) Humanity Content Recognition, Commonsense Knowledge Biology Content Recognition, Commonsense Knowledge General Content Recognition, Commonsense Knowledge Humanity Content Recognition, Commonsense Knowledge General Content Recognition, Commonsense Knowledge Humanity Content Recognition, Commonsense Knowledge General Content Recognition, Commonsense Knowledge Biology Content Recognition, Commonsense Knowledge General Content Recognition, Commonsense Knowledge Content Recognition, Commonsense Knowledge 5 Long Video Object Tracking in Crowd Sense (Long-Vid-Crowd Track) Humanity, Culture 1 UAV Video Human Tracking (UAV Human Track) 2 UAV Video Vehicle Tracking (UAVVehicle Track) 3 UAV Video UAV Tracking (UAVUAV Track) 4 UAV Video Building Tracking (UAV-Build Track) 5 UAV Video General Object Tracking (UAV-Obj Track) 1 Underwater Video Object Tracking in Blue Water (Blue-Water Track) Humanity Content Recognition, Commonsense Knowledge General Content Recognition, Commonsense Knowledge General Content Recognition, Commonsense Knowledge Culture Content Recognition, Commonsense Knowledge General Content Recognition, Commonsense Knowledge Biology Content Recognition, Commonsense Knowledge VOT2018 (Kristan et al., 2018) 500 UniNext et al., 2023) (Yan AUC VOT2018 (Kristan et al., 2018) 500 UniNext et al., 2023) (Yan AUC VOT2018 (Kristan et al., 2018) 500 UniNext et al., 2023) (Yan AUC LaSOT (Fan et al., 2019a) 500 UniNext et al., 2023) (Yan AUC LaSOT (Fan et al., 2019a) 500 UniNext et al., 2023) (Yan AUC LaSOT (Fan et al., 2019a) 10 UniNext et al., 2023) (Yan AUC LaSOT (Fan et al., 2019a) 10 UniNext et al., 2023) (Yan AUC LaSOT (Fan et al., 2019a) 10 UniNext et al., 2023) (Yan AUC LaSOT (Fan et al., 2019a) 10 UniNext et al., 2023) (Yan AUC DanceTrack (Sun et al., 2022) 10 UniNext et al., 2023) (Yan AUC UAV123 (Mueller et al., 2016) 500 UniNext et al., 2023) (Yan AUC UAV123 (Mueller et al., 2016) 500 UniNext et al., 2023) (Yan AUC UAV123 (Mueller et al., 2016) 500 UniNext et al., 2023) (Yan AUC UAV123 (Mueller et al., 2016) 500 UniNext et al., 2023) (Yan AUC UAV123 (Mueller et al., 2016) 500 UniNext et al., 2023) (Yan AUC UTB180 (Alawode et al., 2022) 500 UniNext et al., 2023) (Yan AUC #V-C-16 Long Video Tracking (Long-Vid Track) #V-C-17 UAV Object Tracking (UAV Track) #V-C-18 Underwater Object Tracking (UW Track) 2 Underwater Video Object Tracking in Yellow Water (Yellow-Water Track) Biology Content Recognition, Commonsense Knowledge UTB180 (Alawode et al., 2022) 410 UniNext et al., 2023) (Yan AUC 3 Underwater Video Object Tracking in Green Water (Green-Water Track) Biology Content Recognition, Commonsense Knowledge (AlaUTB180 wode et al., 2022) 500 UniNext (Yan AUC et al., 2023) 4 Underwater Video Object Tracking in White Water (White-Water Track) Biology Content 5 Video Object Tracking in Crowd Sense (Crowed Track) Humanity, Culture Recognition, Commonsense Knowledge Content Recognition, Commonsense Knowledge UTB180 (Alawode et al., 2022) 500 UniNext et al., 2023) (Yan AUC DanceTrack (Sun et al., 2022) 500 UniNext et al., 2023) (Yan AUC #V-C-19 Optical Flow (Opt Flow) 1 Optical Flow in Simple Synthetic Scene (Synthetic Scene Flow Estimate) General Content Recognition FlyingChairs (Dosovitskiy et al., 2015) 150 VideoFlow (de Armas, 2019) 2 Optical Flow in Complex Scene Estimation (Complex Scene Estimate) General Content Recognition ChairsSDHom (Ilg et al., 2017) 150 VideoFlow (de Armas, 2019) EPE EPE 111 On Path to Multimodal Generalist: General-Level and General-Bench Skill (Meta-Task) # Task (Short Name) Domain Capability Data Source Number Model Metrics Task Data SoTA Specialist 3 Panoramic Optical Flow Estimation (Panoramic Flow Estimate) 1 News and Documentary Video Captioning (News&Doc Video Cap) General Content Recognition OmniFlow (Seidel et al., 2021) General Content Recognition, Commonsense Knowledge VideoMME (Fu et al., 2024c) Humanities Content Recognition MUStARD (Castro et al., 2019) General Content Recognition VidSitu (Sadhu et al., 2021) #V-C-20 Video Event Recognition (Vid-Event Recog) 2 Multimodal SarcaS-measure Detection (SarcaS-measure Det) 3 Video Semantic Role Labeling (Vid SRL) 4 Video Event Relation Prediction (Vid Evnt-Rel Pred) 150 PanoFlow (Shi et al., 2022a) EPE 60 LLaVA-VideoBLEU-1 72B-Qwen2 (Zhang et 2024f) 500 SVM al., (Castro et al., 2019) 500 TxEnc-Dec using features al., et I3D (Sadhu 2021) F1 CIDEr General Content Recognition VidSitu (Sadhu et al., 2021) 500 Roberta + I3D features (Sadhu et al., 2021) Macro-Acc Table 27: Detailed list of all tasks and skills (meta-tasks) under video and generation category."
        },
        {
            "title": "Video Generation Group",
            "content": "Skill (Meta-Task) # Task (Short Name) Domain Capability Data Source Number Model Metrics Task Data SoTA Specialist 1 Subject-Driven Text to Video Generation (Subj-Txt2Vid Gen) 2 Background-Consistency Text to Video Geneation (BG-ConsisTxt2Vid Gen) General Creativity and lnnovation General Creativity and lnnovation VBench (Huang et al., 2024) VBench (Huang et al., 2024) 3 Static Video Generation (StaticTxt2Vid Gen) General Creativity and lnnovation 4 Dynamic Video Generation (Dynamic-Txt2Vid Gen) General Creativity and lnnovation VBench (Huang et al., 2024) VBench (Huang et al., 2024) #V-G-1 Text-to-Video Generation (Txt2Vid Gen) 5 Artistic Content Text to Video GenArt eration (Artistic-Txt2Vid Gen) Creativity and lnnovation laion-aesthetics (Lai) 6 Scene Text to Video Generation (Scene-Txt2Vid Gen) General Creativity and lnnovation laion-aesthetics (Lai) 7 Class-Conditioned Text to Video Generation (Class-Cond-Txt2Vid Gen) 8 Multi-Class-Conditioned Text to Video Generation (MulClass-CondTxt2Vid Gen) 9 Stylized Video Generation-Single Reference (Stylized-Vid Gen) 10 Stylized Video Generation-Multiple Reference (M-Stylized-Vid Gen) 11 Spatial Relation Video Generation (Spatial-Rel-Txt2Vid Gen) 12 Camera Motion Video Generation (Camera-Txt2Vid Gen) General Creativity and lnnovation VBench (Huang et al., 2024) General Creativity and lnnovation VBench (Huang et al., 2024) Art Art Creativity and lnnovation Creativity and lnnovation General Creativity and lnnovation General Creativity and lnnovation StyleCrafter (Liu et al., 2023e) StyleCrafter (Liu et al., 2023e) VBench (Huang et al., 2024) VBench (Huang et al., 2024) 13 Terrestrial Animal Video Generation (Animal-Txt2Vid Gen) Animal Creativity and lnnovation WebVid10M (Bain et al., 2021) 1 Style-Specific Text to Video Generation (Style-Txt2Vid Gen) Art Creativity and Innovation Manual et al., 2024) (Huang 500 Zeroscope (vid) 500 LaVie (Wang et al., 2023d) 500 LaVie (Wang et al., 2023d) 500 CogVideoX-5b et (Yang 2024c) 500 CogVideoX-5b al., et (Yang 2024c) 500 CogVideoX-5b al., (Yang 2024c) 500 LaVie et al., (Wang et al., 2023d) 500 CogVideoX-2b al., et 38 (Yang 2024c) StyleCrafter (Liu et al., 2023e) StyleCrafter (Liu et al., 2023e) 500 Zeroscope () 38 500 CogVideoX-5b et (Yang 2024c) 500 CogVideoX-5b al., (Yang 2024c) et al., 500 VideoCrafterv2 (Chen 2024f) 500 LaVie et al., (Wang et al., 2023d) DINOScore CLIPScore L1-Dis OFS AesthScore MUSIQ Suc-Rate Suc-Rate CLIPScore CLIPScore Suc-Rate Suc-Rate FVD CLIPScore Suc-Rate Suc-Rate #V-G-2 Conditional Video Generation (Condt Vid Gen) 2 Color-Specific Text to Video Generation (Color-Txt2Vid Gen) 3 Material-Specific Text to Video Generation (Material-Txt2Vid Gen) General Creativity and Innovation General Creativity and Innovation (Huang Manual et al., 2024) Manual et al., 2024) (Huang 500 CogVideoX-5b (Yang 2024c) et al., On Path to Multimodal Generalist: General-Level and General-Bench Skill (Meta-Task) # Task (Short Name) Domain Capability Data Source Number Model Metrics Task Data SoTA Specialist #V-G-3 Video Action Generation (Act Txt2Vid Gen) 1 Action-Specific Text to Video Generation (Action-Txt2Vid Gen) Sports Creativity and lnnovation 2 Human Video Generation (HumanTxt2Vid Gen) Humanities Creativity and lnnovation al., Kinetics700 (Smaira et 2020) WebVid10M (Bain et al., 2021) 3 Athletics Video Generation (Athletics-Txt2Vid Gen) Sports Creativity and lnnovation UCF101 (Soomro et al., 2012b) 4 Concert Video Generation (ConcertTxt2Vid Gen) Sports Creativity and lnnovation UCF101 (Soomro et al., 2012b) 5 Water Sports Video Generation (Water-Sports-Txt2Vid Gen) Sports Creativity and lnnovation UCF101 (Soomro et al., 2012b) 500 LaVie (Wang et al., 2023d) 500 CogVideoX-5b et (Yang 2024c) 500 CogVideoX-5b et (Yang 2024c) 500 CogVideoX-5b et (Yang 2024c) 500 CogVideoX-5b al., al., al., (Yang 2024c) et al., Suc-Rate FVD FVD FVD FVD 1 Plant Image to Video Generation (Plant-Img2Vid Gen) 1 1 Biology Creativity and lnnovation VBench (Huang et al., 2024) 500 CogVideoX-5bI2V (Yang et al., 2024c) Avg(DINO+CLIP +OFS+MSS) 2 Human Image to Video Generation (Human-Img2Vid Gen) Humanities Creativity and lnnovation VBench (Huang et al., 2024) #V-G-4 Image-to-Video Generation (Img2Vid Gen) 3 Wild Animal Image to Video Generation (Wild-Animal-Img2Vid Gen) Animal Creativity and lnnovation VBench (Huang et al., 2024) 4 Architecture Image to Video Generation (Architect-Img2Vid Gen) Engineering Creativity and lnnovation VBench (Huang et al., 2024) 5 Food Image to Video Generation (Food-Img2Vid Gen) Daily Creativity and lnnovation VBench (Huang et al., 2024) 6 Pet Image to Video Generation (PetImg2Vid Gen) Animal Creativity and lnnovation VBench (Huang et al., 2024) 7 Scene Image to Video Generation (Scene-Img2Vid Gen) Art Creativity and lnnovation VBench (Huang et al., 2024) 8 Vehicle Image to Video Generation Daily (Vehicle-Img2Vid Gen) Creativity and lnnovation VBench (Huang et al., 2024) 9 Furniture Image to Video Generation (Furniture-Img2Vid Gen) Daily Creativity and lnnovation Manual 10 Cloth Image to Video Generation (Cloth-Img2Vid Gen) Daily Creativity and lnnovation Manual 11 Weather Image to Video Generation (Weather-Img2Vid Gen) Climate Creativity and lnnovation Manual 1 Video Object Demoireing (Vid-Obj Demoireing) General Creativity and lnnovation 2 Video Denoising (Vid Denoising) General Creativity and lnnovation 3 Video Frame Interpolation (VidFrame Interpolate) 4 Video Colorization (Vid Colorization) General Reasoning Ability, Creativity and Innovation General Creativity and Innovation Vdmoire iphonev2, RawVDemoire (Dai et al., 2022b; Yue et al., 2023) CRVD, DAVIS 2017 (Yue et al., 2020; Pont-Tuset et al., 2017) Vimeo-90k (Xue et al., 2019) MSU Video Colorization Benchmark (MSU) #V-G-5 Video Enhancement (Vid Enhance) 500 CogVideoX-5bI2V (Yang et al., 2024c) 500 CogVideoX-5bI2V (Yang et al., 2024c) 500 CogVideoX-5bI2V (Yang et al., 2024c) 500 CogVideoX-5bI2V (Yang et al., 2024c) 500 CogVideoX-5bI2V (Yang et al., 2024c) 500 CogVideoX-5bI2V (Yang et al., 2024c) 500 CogVideoX-5bI2V (Yang et al., 2024c) 500 CogVideoX-5bI2V (Yang et al., 2024c) 500 CogVideoX-5bI2V (Yang et al., 2024c) 500 CogVideoX-5bI2V (Yang et al., 2024c) Avg(DINO+CLIP +OFS+MSS) Avg(DINO+CLIP +OFS+MSS) Avg(DINO+CLIP +OFS+MSS) Avg(DINO+CLIP +OFS+MSS) Avg(DINO+CLIP +OFS+MSS) Avg(DINO+CLIP +OFS+MSS) Avg(DINO+CLIP +OFS+MSS) Avg(DINO+CLIP +OFS+MSS) Avg(DINO+CLIP +OFS+MSS) Avg(DINO+CLIP +OFS+MSS) 93 DTNet (Xu et al., 2024) PSNR 51 BSVD (Qi et al., 2022) PSNR 79 EMA-VFI (Zhang et al., 2023k) PSNR LVVCP (Hofinger et al., 2022) PSNR 5 Video Dehazing (Vid Dehazing) General Content Recognition Real Haze Video Database (Rea, b) 403 MAP-Net et al., 2023c) (Xu PSNR 113 On Path to Multimodal Generalist: General-Level and General-Bench Skill (Meta-Task) # Task (Short Name) Domain Capability Data Source Number Model Metrics Task Data SoTA Specialist #V-G-6 Video Editing (Vid Edit) real6 Video Desnowing (Vid Desnowing) General Content Recognition RVSD, (Wu snow85 et 2024c; al., Chen et al., 2023) reGeneral Content Recognition waterdrop 7 Video Deraining (Vid Deraining) moval(VWRD), VRDS (Wen et al., 2023; Wu et al., 2023e) 8 Road Scene Video Superresolution (Scence-Vid Superres) Daily Content Recognition VideoLQ (Chan et al., 2022) 9 Real World Video Superresolution (Real-Scence-Vid Superres) General Content Recognition VideoLQ (Chan et al., 2022) 55 Turtle (Ghasemabadi et al.) PSNR 45 RainMamba (Wu et al., 2024d) PSNR 34 Upscale-A-Video al., et (Zhou 2024d) 15 Upscale-A-Video al., et (Zhou 2024d) MUSIQ MUSIQ 1 Video Animal Inpainting (Animal Inpainting) 2 Video Non-Animal Inpainting (NonAnimal Inpainting) Animal Reasoning Ability, Creativity and Innovation General Reasoning Ability, Creativity and Innovation YouTube-VOS (Xu et al., 2018) 250 ProPainter (Zhou et al., 2023) PSNR YouTube-VOS (Xu et al., 2018) 195 ProPainter (Zhou et al., 2023) PSNR 3 Video Deblurring (Vid Deblur) General Content Recognition REDS (Nah et al., 2019) FRESCO (Yang et al., 2024d) VToonify (Yang et al., 2022c) 60 VRT (Liang et al., 19 2022) FRESCO (Yang et al., 2024d) 19 VToonify (Yang et al., 2022c) PSNR Fram-Acc UPR 4 Video Translation (Vid Translation) 5 Portrait Video Style Transfer (Style Transfer) Art Art Creativity and lnnovation Creativity and lnnovation 114 On Path to Multimodal Generalist: General-Level and General-Bench Table 29: Detailed list of all tasks and skills (meta-tasks) under audio and comprehension category."
        },
        {
            "title": "Audio Comprehension Group",
            "content": "Skill (Meta-Task) # Task (Short Name) Domain Capability Data Source Number Model Metrics Task Data SoTA Specialist 3 Speech Event Extraction (SpeechEE) 1 Speech Emotion Recognition (Emotion Recog) General Content tion, Ability RecogniReasoning General Affective Analysis #A-C-1 Speech Accent Understanding (Acnt Analy) 1 Accent Classification (Accent Recog) 2 Accent Sex Classification (Accent-Sex Recog) 3 Speaker Identification (Speaker ID) 4 Vocal Sound Classification (Vocal-Sound Recog) #A-C-2 Speech Content Understanding (Ctnt Analy) 1 Intent Classification (Intent Recog) 2 Speech Command (Speech Cmd) #A-C-3 Speech Emotion Understanding (SpeechEmo Analy) #A-C-4 Music Understanding (Music Analy) #A-C-5 Audio Technique Understanding (Aud-Tech Analy) #A-C-6 Audio Content Understanding (Aud Analy) #A-C-7 General Audio Question Answering (Aud QA) 1 Music Genre Classification (Music-Genre Recog) 2 Music Instrument Classification (Instrument Recog) 3 Music Instrument Source Analysis (Instrument-Source Anal) 4 Music Pitch Analysis (Pitch Anal) 1 Note Qualities Analysis (Note-Quality Anal) 2 Singer Identification (Singer ID) 3 Vocal Technique Detection (Vocal-Tech Detect) 1 Long Audio Captioning (Long AudioCaps) newline 2 Wild Audio Captioning (Wild AudioCaps) 1 Open Audio Question Answering (OpenAQA) 1 1 1 1 Linguistics Reasoning Ability Linguistics Reasoning Ability, Commonsense Knowledge Speech Accent Archive (Weinberger, 2015) Speech Accent Archive (Weinberger, 2015) Linguistics Reasoning Ability VoxCeleb General Reasoning Ability Vocal (Gong 2022) (Nagrani et al., 2017) Sound al., et 500 CLAP (Elizalde et al., 2023) 500 CLAP (Elizalde et al., 2023) 279 HuBERT Large (Hsu et al., 2021) 500 CLAP (Elizalde et al., 2023) General Reasoning Ability Speech Fluent Commands (Lugosch et al., 2019) 500 HuBERT Large (Hsu et al., 2021) General Content Recognition speechcommands (Warden, 2018) CASIE et al., 2024h) (Wang 500 HuBERT Large (Hsu et al., 2021) 500 E2E (T5) (Wang et al., 2024h) IEMOCAP (Busso et 2008) al., 500 WavLM Large al., et (Chen 2022b) Art Art Art Art Art Art Art Content Recognition Music (Mus) Genre Content Recognition, Commonsense Knowledge Content Recognition NS. NS. (Engel 2017) Instruments al., et Instruments (Engel Source et al., 2017) Content Recognition NS. Pitch (Engel et al., 2017) 500 Musicset-Sup (McCallum et al., 2022) 500 HuBERT-base (Hsu et al., 2021) 500 Musicset-ULarge (McCallum et al., 2022) 500 Musicset-ULarge (McCallum et al., 2022) Content Recognition NS. quality (Engel et al., 2017) Reasoning Ability VocalSet (Wilkins et al., 2018) Reasoning Ability VocalSet (Wilkins et al., 2018) 500 HuBERT-base (Hsu et al., 2021) 500 HuBERT-base (Hsu et al., 2021) 500 HuBERT-base (Hsu et al., 2021) Acc Acc Acc Acc Acc Acc F1 Acc Acc Acc Acc Acc Acc Acc Acc General Reasoning Ability Clotho Caption (Drossos et al., 2020) 500 PTAAC (Kim BLUE-1 et al., 2023a) General Reasoning Ability AudioCaps (Kim et al., 2019) 500 PTAAC (Kim BLUE-1 et al., 2023a) General Reasoning Ability, Commonsense Knowledge OpenAQA (LTU) al., (Gong 2024) et 500 LTU (Gong et al., 2024) GPT-Score 2 Audio Question Answering General Reasoning Ability, (AudioQA) Commonsense Knowledge 115 ClothoAQA (Lipping et al., 2022) 500 MWAFM (Li Acc et al., 2023g) On Path to Multimodal Generalist: General-Level and General-Bench Skill (Meta-Task) # Task (Short Name) Domain Capability Data Source Number Model Metrics Task Data SoTA Specialist #A-C-8 Animal Sound Analysis (Animal-Sound Det) #A-C-9 Environment Sound Understanding (Envir-Sound Det) 1 Bird Sound Detection (Bird-Sound Detect) 2 Animal Sound Detection (AnimalSoundDetect) 1 Acoustic Scene Recognition (Acoustic-Scene Recog) 1 1 2 Environment Sound Recognition (Env-Sound Recog) 3 Sound Event Recognition (Sound-Event Recog) Biology, Animal Biology, Animal Content Recognition, Commonsense Knowledge RecogniContent tion, Commonsense Knowledge General Reasoning Ability Birdsong (Stowell et al., 2018) 500 HuBERT Large (Hsu et al., 2021) Animal-Sound Classification (Ani, b) TUT 2017 (Mesaros et al., 2016) 500 HuBERT Large (Hsu et al., 2021) 468 CLAP (Elizalde et al., 2023) General Reasoning Ability, Commonsense Knowledge General Reasoning Ability ESC50 2015) ESC50 2015) (Piczak, 500 CLAP (Elizalde et al., 2023) (Piczak, 500 CLAP (Elizalde et al., 2023) Acc Acc Acc Acc Acc Table 31: Detailed list of all tasks and skills (meta-tasks) under audio and generation category."
        },
        {
            "title": "Audio Generation Group",
            "content": "Skill (Meta-Task) # Task (Short Name) Domain Capability Data Source Number Model Metrics Task Data SoTA Specialist 1 AudioEdit (AudioEdit) Art Creativity and Innovation Song Describer Dataset (Manco et al., 2023) 16 ControlNetDiffuTrans (Hou et al., 2024) CLAP 1 Daily Talk Generation (DailyTalk Gen) General Creativity and Innovation DailyTalk et al., 2023) (Lee 500 FastSpeech2 (Ren et al., 2021) MOS MCD MOS WER MOS CLAP CLAP CIDEr 1 Emotional Speech Synthesis General Affective Analysis (EmoSpeech Syn) 2 Emotion Style Transfer (EmoStyleTransfer) General Affective Analysis 1 Text-To-Speech (TTS) Linguistics Commonsense 2 Multimodal TTS (MTTS) Knowledge, Creativity and Innovation General Creativity and Innovation Emotional Speech Data (Zhou et al., 2021) EmotionalSpeechDataset (Zhou 2021) al., et 500 DeepEST (Zhou et al., 2021) 500 DeepEST (Zhou et al., 2021) Librispeech testclean (Panayotov et al., 2015) MEAD-TTS (Guan 2024) al., et 500 USLM (Zhang et al., 2023l) 500 MM-TTS (Guan et al., 2024) #A-G-5 Text-to-Audio Synthesis(Txt2Aud Gen) 1 Single Caption To Audio Generation (1CapToAudio) 2 Two Captions To Audio Generation (2CapsToAudio) 1 Image-to-Speech (ImageToSpeech) General Creativity and Innovation General Creativity and Innovation AudioCap (Kim et al., 2019) AudioCap (Kim et al., 2019) 500 AudioLDM2 (Liu et al., 2024g) 500 AudioLDM2 (Liu et al., 2024g) General Content Recognition Flickr8k Audio and (Harwath Glass, 2015) 500 Im2Sp (Kim et al., 2023b) 1 Video-to-Audio (Video2Audio) General Commonsense Knowledge AVSync15 (Zhang et 2024g) 500 Diff-Foley (Luo et al., 2023) al., FAD #A-G-8 Speech Style Transfer (Style Trans) 1 Voice Conversion (VoiceConversion) newline 1 Chinese-to-English Speech Translation (SpeechTrans (Zh-En)) 2 English-to-Chinese Speech Translation (SpeechTrans (En-Zh)) #A-G-9 Speech Translation (Speech Trans) General Content Recognition VCTK Corpus al., et (Junichi 2019) Content Recognition GigaST (Ye et al., Multidomain General Content Recognition CoVoST 2023b) v1 al., et (Wang 2020b) 500 USLM (Zhang et al., 2023l) WER 500 USLM (Zhang et al., 2023l) 500 USLM (Zhang et al., 2023l) WER WER 116 #A-G-1 Audio Edit (Audio Edit) #A-G-2 Dialogue Speech Generation (Dialog Gen) #A-G-3 Emotional Speech Generation (EmoSpeech Gen) #A-G-4 Text-To-Speech Synthesis (TTS) #A-G-6 Image-to-Audio Synthesis (Img2Aud Gen) #A-G-7 Video-to-Audio Synthesis (V2A) On Path to Multimodal Generalist: General-Level and General-Bench Skill (Meta-Task) # Task (Short Name) Domain Capability Data Source Number Model Metrics Task Data SoTA Specialist 3 English-to-Mogolian Speech Translation (SpeechTrans (En-Mo)) General Content Recognition CoVoST (Wang 2020b) v1 al., 500 USLM (Zhang et al., 2023l) et WER datasets and LOP (Crestel Esling, 2017) POP909 (Wang* et al., 2020) M4Singer (Zhang et al., 2022) 500 FGcRBM (Cresand Esling, tel 2017) 500 ThemeTransformer (Shih et al., 2023) (Liu 500 Diffsinger et al., 2022) Song Describer Dataset (Manco et al., 2023) 500 Riffusion (Forsgren and Martiros, 2022) Acc PCC MOS FAD MusicTI (Li et al., 2024l) 500 TVI-Diff et al., 2024l) Groove2Groove (Cıfka 2020) et al., 500 TVI-Diff et al., 2024l) (Li (Li StyleCLAP StyleCLAP #A-G-10 Music Synthesis (Music Gen) 1 Piano-to-Orchestration Generation (Piano2Orchestra Gen) 2 Pop Music Generation (PopMusic Gen) 3 Singing Voice Synthesis (Singing Voice Syn) 4 Song Generation (Song Gen) 1 Music Style Transfer #A-G-11 Music Style Transfer (Music Trans) 2 Chord-based Music Style Transfer (Chord Music Style Trans) Art Art Art Art Art Art Creativity and Innovation Creativity and Innovation Commonsense Knowledge, Creativity and Innovation Creativity and Innovation Commonsense Knowledge, Creativity and Innovation Commonsense Knowledge, Creativity and Innovation 117 Acc Acc Acc Acc Acc Acc Acc Acc Acc On Path to Multimodal Generalist: General-Level and General-Bench Table 33: Detailed list of all tasks and skills (meta-tasks) under 3D and comprehension category. 3D Comprehension Group Skill (Meta-Task) # Task (Short Name) Domain Capability Data Source Number Model Metrics Task Data SoTA Specialist 1 3D Accessory Classification (3D-Access Cls) #D-C-1 3D Human-related Object Classification (3D-Human Cls) 2 3D Appliance Classification (3D-Appliance Cls) 3 3D Tableware Classification (3D-Tableware Cls) General Content Recognition ModelNet et al., 2015) General Content Recognition ModelNet et al., 2015) General Content Recognition ModelNet et al., 2015) 4 3D Musical Instrument Classification (3D-MI Cls) General Content Recognition ModelNet et al., 2015) 5 3D Person Classification (3D-Person Cls) General Content Recognition ModelNet et al., 2015) (Wu (Wu (Wu (Wu (Wu 340 PointGST (Liang et al., 2024) 240 PointGST (Liang et al., 2024) 180 PointGST (Liang et al., 2024) 500 PointGST (Liang et al., 2024) 200 PointGST (Liang et al., 2024) (Wu 20 PointGST (Liang et al., 2024) #D-C-2 3D Structure and Environment Classification (3D-Struct Cls) 1 3D Furniture Classification (3D-Furniture ClS) 1 2 3D Structure Classification (3D-Struct Cls) General Content Recognition ModelNet et al., 2015) General Content Recognition ModelNet et al., 2015) (Wu 40 PointGST (Liang et al., 2024) #D-C-3 Transportation and Technology Object Classification (Tech Cls) #D-C-4 3D Indoor Scene Semantic Segmentation (Indoor-Scene Seg) #D-C-5 3D Outdoor Scene Semantic Segmentation (Outdoor-Scene Seg) #D-C-6 3D Indoor Scene Instance Segmentation (Indoor-Inst Seg) #D-C-7 3D Pose Estimation (Pose Est) #D-C-8 3D Part Segmentation (Part Seg) 1 3D Electronic Classification General Content Recognition ModelNet (Wu (3D-Electronic ClS) 1 et al., 2015) 140 PointGST (Liang et al., 2024) 2 3D Vehicle Classification (3DVehicle Cls) General Content Recognition ModelNet et al., 2015) (Wu 200 PointGST (Liang et al., 2024) 1 3D Indoor Appliance Semantic Segmentation (3D-Appliance Seg) 1 1 1 1 3D Outdoor Semantic Segmentation (3D-Outdr Seg) 1 1 1 General Commonsense Knowledge ScanNet et al., 2017) (Dai 142 ODIN (Jain et al., 2024) mIoU General Commonsense Knowledge Semantic KITTI (Behley al., 2019) et 4,071 OpenPCSeg (Team, 2020) mIoU 1 3D Indoor Instance Segmentation General Commonsense (3D-In-Instance Seg) Knowledge ScanNet et al., 2017) (Dai 142 SphericalMask (Sangyun 2024) Shin, mIoU 1 3D Odometry (3D Odometry) Geometry Problem Solving KITTI et al., 2012) (Geiger 10 CT-ICP (Dellenbach et al., 2021) RTE 1 3D Aircrafts Part Segmentation (3DAircraft Seg) General Commonsense Knowledge 2 3D Personal Item Part Segmentation (3D-Person Seg) General Commonsense Knowledge 3 3D Vehicle Part Segmentation (3D-Vehicle Seg) General Commonsense Knowledge 4 3D Furniture Part Segmentation General Commonsense (3D-Furniture Seg) Knowledge 5 3D Tableware Part Segmentation (3D-Tableware Seg) General Commonsense Knowledge 6 3D Weapon Part Segmentation (3D-Weapon Seg) General Commonsense Knowledge 118 et et et Shape Net Part (Chang al., 2015) Shape Net Part (Chang al., 2015) Shape Net Part (Chang al., 2015) Shape Net Part (Chang al., 2015) Shape Net Part (Chang al., 2015) Shape Net Part al., (Chang 2015) et et et 523 SPOTR et al., 2023) (Park Instance mIoU 346 SPOTR et al., 2023) (Park Instance mIoU 288 SPOTR et al., 2023) (Park Instance mIoU 2,128 SPOTR (Park et al., 2023) Instance mIoU 377 SPOTR et al., 2023) (Park Instance mIoU 133 SPOTR et al., 2023) (Park Instance mIoU #D-C-11 3D Detection (3D Det) #D-C-12 3D Question Answering (3D QA) On Path to Multimodal Generalist: General-Level and General-Bench Skill (Meta-Task) # Task (Short Name) Domain Capability Data Source Number Model Metrics Task Data SoTA Specialist #D-C-9 3D Tracking (3D Track) 1 3D Tracking (3D Track) 1 #D-C-10 3D Geometry Feature Analysis (3D-Geo Analy) 1 3D Normal Estimation (3D-Normal Est) General Commonsense Knowledge NuScenes (Caesar et al., 2020) 500 CenterPoint (Yin et al., 2021) AMOTA Geometry Problem Solving PCPNet dataset (Guerrero et al., 2018) 108 SHS-Net (Li et al., 2023h) RMSE 1 3D Detection (3D Det) 1 General Content Recognition NuScenes (Caesar et al., 2020) 500 BEVFusion (Liu et al., 2023f) mAP 1 3D Spatial Scene Question AnswerGeneral Commonsense ing (3D-Space QA) Knowledge 2 3D Situated Question Answering on What (3D-What QA) General Commonsense Knowledge 3 3D Situated Question Answering on Is (3D-Is QA) General Commonsense Knowledge 4 3D Situated Question Answering on General Commonsense How (3D-How QA) Knowledge 5 3D Situated Question Answering on Can (3D-Can QA) General Commonsense Knowledge 6 3D Situated Question Answering on Which (3D-Which QA) General Commonsense Knowledge 7 3D Situated Question Answering on Other (3D-Other QA) General Commonsense Knowledge ScanQA (Azuma et al., 2022) 4,675 SIG3D (Man BLEU@4 et al., 2024) (Ma (Ma SQA3D et al., 2023) ScanQA (Azuma et al., 2022) SQA3D et al., 2023) ScanQA (Azuma et al., 2022) SQA3D et al., 2023) ScanQA (Azuma et al., 2022) (Ma 1,147 SIG3D et al., 2024) 652 SIG3D et al., 2024) (Man EM@1 (Man EM@1 465 SIG3D (Man EM@ et al., 2024) 338 SIG3D et al., 2024) 351 SIG3D et al., 2024) 566 SIG3D et al., 2024) (Man EM@1 (Man EM@1 (Man EM@1 4,383 M2TBLEU-4 Interpretable (Radouane et al., 2024) #D-C-13 3D Motion Understanding (3D-Motion Analy) 1 3D Motion Captioning (3D-Motion Cap) General Commonsense Knowledge KIT-ML pert et al., 2016) (PlapTable 35: Detailed list of all tasks and skills (meta-tasks) under 3D and generation category. 3D Generation Group Skill (Meta-Task) # Task (Short Name) Domain Capability Data Source Number Model Metrics Task Data SoTA Specialist #D-G-1 Point Cloud Completion (PC Complt) 1 3D Point Cloud Completion (3D-PC Completion) General Creativity and Innovation PCN data (Yuan et al., 2018) 500 PointTr (Yu et al., CD 2021) #D-G-2 Point Cloud to Mesh Reconstruction (PC2M Recon) 1 Point Cloud to Mesh Object Reconstruction (Object-PC2M Recon) 2 Point Cloud to Mesh Scene Reconstruction (Scene-PC2M Recon) General Creativity and Innovation General Creativity and Innovation ShapeNet (Chang et al., 2015) Synthetic Indoor Scene Dataset (Peng et al., 2020) 500 ConvONet (Peng et al., 2020) 500 ConvONet (Peng et al., 2020) CD CD #D-G-3 Text to Point Cloud Generation (Txt2PC Gen) 1 Text to 3D Living and Arts Point Cloud Generation (Art-Txt2PC Gen) 2 Text to 3D Science and Technology Point Cloud Generation (Sci-Txt2PC Gen) 3 Text to 3D Nature and Biology Point Cloud Generation (Nature-Txt2PC Gen) 4 Text to 3D Culture and Structure Point Cloud Generation (Culture-Txt2PC Gen) Art Creativity and Innovation Manual 500 Point-E (Nichol et al., 2022) CLIPScore General Creativity and Innovation General Creativity and Innovation General Creativity and Innovation Manual Manual Manual 500 Point-E (Nichol et al., 2022) CLIPScore 500 Point-E (Nichol et al., 2022) CLIPScore 500 Point-E (Nichol et al., 2022) CLIPScore 119 On Path to Multimodal Generalist: General-Level and General-Bench Skill (Meta-Task) # Task (Short Name) Domain Capability Data Source Number Model Metrics Task Data SoTA Specialist #D-G-4 Text to Mesh Generation (Txt2M Gen) #D-G-5 Image to Point Cloud Generation (Img2PC Gen) 1 Text to 3D Living and Arts Mesh Generation (Arts-Txt2M Gen) 1 1 2 Text to 3D Science and Technology Mesh Generation (Sci-Txt2M Gen) 3 Text to 3D Nature and Biology Mesh Generation (Nature-Txt2M Gen) 4 Text to 3D Culture and Structure Mesh Generation (Culture-Txt2M Gen) 1 Living and Arts Image to 3D Point Cloud Generation (Arts-Img2PC Gen) 1 1 1 2 Science and Technology Image to 3D Point Cloud Generation (Sci-Img2PC Gen) 3 Nature and Biology Image to 3D Point Cloud Generation (Nature-Img2PC Gen) Art Creativity and Innovation Manual General Creativity and Innovation General Creativity and Innovation General Creativity and Innovation Manual Manual Manual 500 Hunyuan3D-1.0 (Yang 2024e) et al., 500 Hunyuan3D-1.0 et al., 500 Hunyuan3D-1.0 et al., 500 Hunyuan3D-1.0 et al., (Yang 2024e) (Yang 2024e) (Yang 2024e) CLIPScore CLIPScore CLIPScore CLIPScore Art Creativity and Innovation Objaverse (Deitke et al., 2023) 500 Point-E (Nichol et al., 2022) CLIPScore General Creativity and Innovation Objaverse (Deitke et al., 2023) 500 Point-E (Nichol et al., 2022) CLIPScore General Creativity and Innovation Objaverse (Deitke et al., 2023) 500 Point-E (Nichol et al., 2022) CLIPScore 4 Culture and Structure Image to 3D General Creativity and InnoPoint Cloud Generation (Culture-Img2PC Gen) vation Objaverse (Deitke et al., 2023) 500 Point-E (Nichol et al., 2022) CLIPScore 500 Hunyuan3D-1.0 et al., (Yang 2024e) (Yang 2024e) (Yang 2024e) (Yang 2024e) 500 Hunyuan3D-1.0 et al., 500 Hunyuan3D-1.0 et al., 500 Hunyuan3D-1.0 et al., 142 open3d (Zhou et al., 2018) 142 open3d (Zhou et al., 2018) 830 MoMask (Guo et al., 2024b) CLIPScore CLIPScore CLIPScore CLIPScore CD CD FID #D-G-6 Image to Mesh Generation (Img2M Gen) #D-G-7 RGB-D to Point Cloud Reconstruction (RGBD2PC Recon) #D-G-8 RGB-D to Mesh Reconstruction (RGBD2Mesh Recon) #D-G-9 Text to 3D Motion Generation (Txt2Motion Gen) 1 Living and Arts Image to 3D Mesh Generation (Arts-Img2M Gen) Art Creativity and Innovation Objaverse (Deitke et al., 2023) 2 Science and Technology Image to 3D Mesh Generation (Sci-Img2M Gen) 3 Nature and Biology Image to 3D Mesh Generation (Nature-Img2M Gen) 4 Culture and Structure Image to 3D Mesh Generation (Culture-Img2M Gen) General Creativity and Innovation Objaverse (Deitke et al., 2023) General Creativity and Innovation Objaverse (Deitke et al., 2023) General Creativity and Innovation Objaverse (Deitke et al., 2023) 1 RGB-D to Point Cloud Reconstruction (RGBD2PC Recon) General Creativity and Innovation ScanNet et al., 2017) (Dai 1 RGB-D to Mesh Reconstruction (RGBD2Mesh Recon) General Creativity and Innovation ScanNet et al., 2017) (Dai 1 Text to 3D Motion Generation (Txt2Motion Gen) General Creativity and Innovation KIT-ML pert et al., 2016) (Plap120 On Path to Multimodal Generalist: General-Level and General-Bench Table 37: Detailed list of all NLP tasks and skills (meta-tasks)."
        },
        {
            "title": "NLP Group",
            "content": "Skill (Meta-Task) # Task (Short Name) Domain Capability Data Source Number Model Metrics Task Data SoTA Specialist 1 Commonsense Reasoning (Common Reason) General Commonsense Knowledge #L-1 Cognitive Question Answering (Cog QA) 2 Causal Reasoning (Causal Reason) General Causality Discrimination et commonsense qa (Talmor al., 2019) corr2cause et al., 2024) (Jin 3 Document-Level Causal Reasoning (Doc-Causal Reason) General Reasoning Ability 4 Counterfactual Reasoning (Counterfact Reason) General Causality Discrimination 5 Analogical Reasoning (Analog Reason) General Reasoning Ability 6 Multi-Hop Question Answering (Multi-Hop QA) General Reasoning Ability 7 Temporal Reasoning (Temporal Reason) General Reasoning Ability 8 Spatial Reasoning (Spatial Reason) General Spatial Perception 9 Numerical Reasoning (Numerical Reason) General Reasoning Ability 1 Ethical Reasoning (Ethical Reason) Ethics Ethical Awareness #L-2 Ethical NLP (Ethics NLP) 2 Truthful Question Answering (Truthful QA) Ethics Ethical Awareness 3 Legal Question Answering (Legal QA) Law Ethical Awareness qa4mre et al., 2013) (Penas CounterfactualReasoningCapacity-ofLarge-LanguageModels (crc) BATS (Gladkova et al., 2016) hotpot qa (Yang et al., 2018) time dial et al., 2021) (Qin stepgame et al., 2022b) (Shi aqua rat et al., 2017) (Ling ethics (Hendrycks et al., 2021) truthful qa (Hendrycks et al., 2021) JEC-QA (Zhong et al., 2020) 510 Flan-T5-Large (Chung 2022) et al., 500 Flan-T5-Large (Chung 2022) et al., 564 Flan-T5-Large (Chung 2022) et al., 501 Flan-T5-Large (Chung 2022) et al., 500 Flan-T5-Large (Chung 2022) et al., 500 Flan-T5-Large (Chung 2022) et al., 506 Flan-T5-Large (Chung 2022) et al., 507 Flan-T5-Large (Chung 2022) et al., 508 Flan-T5-Large (Chung 2022) et al., 500 Flan-T5-Large (Chung 2022) et al., 500 Flan-T5-Large (Chung 2022) et al., 500 Flan-T5-Large (Chung 2022) et al., Acc Acc BLEUF"
        },
        {
            "title": "Acc",
            "content": "BLEU-1 BLEU-1 Acc Acc Acc BLEUAcc 4 Bias Detection (Bias Detect) Culture Affective Analysis WNC (Pryzant 500 Roberta-Large Micro-F1 5 Offensive Classification (Offensive Classify) 6 Hate Speech Detection (HateSpeech Detect) 7 Spam Detection (Spam Detect) 8 Fake News Detection (Fake-News Detect) 9 Fact Verification (Fact Verify) Culture Affective Analysis Culture Affective Analysis Culture Causality Discrimination Humanities Ethical Awareness General Causality Discrimination et al., 2020) offensive-speech (ock) hate-speech (ock) spam-email (spa) fake-news (fak) Counterfactual (ONeill 2021) et al., #L-3 Domain-Specific QA (Domain QA) 1 Biomedical Question Answering (Biomedical QA) Biomedical Reasoning Ability BioASQ (bqb) 2 Medical Question Answering (Medical QA) Medical Reasoning Ability 3 Technical Question Answering Engineering Reasoning Ability (Tech QA) ChatDoctorHealthCareMagic100k (Li et al., 2023i) Stack Overflow Data (sod) 121 (Liu, 2019) 500 Roberta-Large (Liu, 2019) 500 Roberta-Large (Liu, 2019) 500 Roberta-Large (Liu, 2019) 500 Roberta-Large (Liu, 2019) 500 Roberta-Large (Liu, 2019) Micro-F1 Micro-F1 Micro-F1 Micro-F1 Micro-F 500 Flan-T5-Large (Chung 2022) et al., 505 Flan-T5-Large (Chung 2022) et al., BLEU-1 BLEU-1 500 Flan-T5-Large (Chung 2022) et al., BLEUOn Path to Multimodal Generalist: General-Level and General-Bench Skill (Meta-Task) # Task (Short Name) Domain Capability Data Source Number Model Metrics Task Data SoTA Specialist 4 Engineering Question Answering (Eng QA) Engineering Reasoning Ability University Bath (eqa) of 5 Science Question Answering (Science QA) Science Reasoning Ability sciq (Welbl et al., 2017) 6 Earth Question Answering (Earth QA) Earth Reasoning Ability Manual 7 Nature Question Answering (Nature QA) Nature Reasoning Ability Manual #L-4 Social QA (Social QA) 1 Humanities Question Answering (Humanities QA) 1 Humanities Reasoning Ability 2 Social Science Question Answering (Social-Sci QA) Social Reasoning Ability et (Raal., squad v2 jpurkar 2018) Social IQA (Sap et al., 2019) 3 Philosophical Question Answering (Philosophy QA) Philosophy Reasoning Ability strix-philosophyqa (pqa) 4 Art Question Answering (Art QA) Art Reasoning Ability Manual 5 Business Question Answering (Business QA) Business Reasoning Ability Manual 6 History Question Answering (History QA) History Reasoning Ability wiki-reading et (Kenter 2018) al., #L-5 Non-Traditional QA (Non-Trad QA) 1 Fairytale Question Answering (Fairytale QA) Culture Reasoning Ability 2 Tweet Question Answering (Tweet QA) Social Reasoning Ability 3 Trivial QA (Trivial QA) Social Reasoning Ability FairytaleQA (Rajpurkar et al., 2018) tweet qa (Xiong et al., 2019) trivia qa et al., 2017) (Joshi #L-6 Advanced QA (Advance QA) 1 Open-Domain Question Answering (Open-Domain QA) General Reasoning Ability 2 Conversational Question Answering (Conversation QA) General Reasoning Ability et (Raal., squad v2 jpurkar 2018) coqa (Reddy et al., 2019) 3 Table Question Answering (Table QA) 4 Multilingual Question Answering (Multi-Lang QA) 5 Code-Switch Question Answering (Code-Switch QA) General Reasoning Ability wikitablequestions and (Pasupat Liang, 2015) Linguistics Reasoning Ability multilingual qa (mqa) Linguistics Reasoning Ability Manual 1 Math Question Answering (Math QA) Math Reasoning Ability math-QA (Amini et al., 2019) 2 Mathematical Word Problem Solving (Math Word Prob) Math Problem Solving 3 Mathematical Proof Generation (Math Proof Gen) Math Problem Solving 1 Code Explanation (Code Explain) Code Problem Solving 2 Code Defect Detection (CodeDefect Detect) Code Problem Solving math-QA (Amini et al., 2019) math-QA (Amini et al., 2019) CodeXGLUE (Lu et al., 2021b) CodeXGLUE (Lu et al., 2021b) 122 #L-7 Math Problem Solving (Math Ability) #L-8 Code Problem Solving (Code Ability) 500 Flan-T5-Large (Chung 2022) et al., 500 Flan-T5-Large (Chung 2022) et al., 500 Flan-T5-Large (Chung 2022) et al., 500 Flan-T5-Large (Chung 2022) et al., 500 Flan-T5-Large (Chung 2022) et al., 500 Flan-T5-Large (Chung 2022) et al., 500 Flan-T5-Large (Chung 2022) et al., 500 Flan-T5-Large (Chung 2022) et al., 500 Flan-T5-Large (Chung 2022) et al., 500 Flan-T5-Large (Chung 2022) et al., 501 Flan-T5-Large (Chung 2022) et al., 509 Flan-T5-Large (Chung 2022) et al., 500 Flan-T5-Large (Chung 2022) et al., 513 Flan-T5-Large (Chung 2022) et al., 500 Flan-T5-Large (Chung 2022) et al., 590 Flan-T5-Large (Chung 2022) et al., 600 mT5-Large (Xue et al., 2021) 500 mT5-Large (Xue et al., 2021) 500 Flan-T5-Large (Chung 2022) et al., 500 Flan-T5-Large (Chung 2022) et al., 500 Flan-T5-Large (Chung 2022) et al., 508 CodeT5 (Wang et al., 2021) 501 CodeT (Wang et al., 2021) BLEU-1 Acc Acc Acc BLEU-1 Acc BLEU-1 Acc Acc BLEUBLEU-1 F1 F1 BLEU-1 BLEU-1 BLEU-1 Acc Acc Acc BLEU-1 BLEUAcc On Path to Multimodal Generalist: General-Level and General-Bench Skill (Meta-Task) # Task (Short Name) Domain Capability Data Source Number Model Metrics Task Data SoTA Specialist #L-9 Cross-lingual/ Translation (X-Lan&NMT) 3 Code Generation (Code Gen) Code Problem Solving mbpp (mbp) 4 Code Repair (Code Repair) Code Problem Solving 5 Text-to-SQL Generation (Txt2SQL Gen) Code Problem Solving CodeXGLUE (Lu et al., 2021b) Text-to-sql-v1 () 1 Multilingual Translation (Multilang Trans) General Multilingual Capability Manual 2 Low-Resource Translation (LowRes Trans) General Multilingual Capability flores 101 (Goyal et al., 2022) 3 English-Chinese Translation (En-Zh Trans) General Multilingual Capability 4 English-French Translation (En-Fr Trans) General Multilingual Capability Manual Manual #L-10 Text Summarization (Txt Sum)"
        },
        {
            "title": "1 Extractive Summarization (Extract",
            "content": "Summ) 2 Abstractive Summarization (Abstract Sum)"
        },
        {
            "title": "Text Manipulation",
            "content": "General Text Manipulation cnn dailymail (See et al., 2017) xsum (Narayan et al., 2018) 3 Multi-Document (Multi-Doc Sum) Summarization General Text Manipulation multi news (Fabbri et al., 2019) 500 CodeT5 (Wang et al., 2021) 600 CodeT (Wang et al., 2021) 600 CodeT5 (Wang et al., 2021) 504 Flan-T5-Large (Chung 2022) et al., 502 Flan-T5-Large (Chung 2022) et al., 500 Flan-T5-Large (Chung 2022) et al., 501 Flan-T5-Large (Chung 2022) et al.,"
        },
        {
            "title": "503 BART-Large\n(Lewis, 2019)\n501 BART-Large\n(Lewis, 2019)\n300 BART-Large\n(Lewis, 2019)",
            "content": "CodeBLEU CodeBLEU BLEU-1 ROUGE-1 ROUGE-1 ROUGEROUGE-1 ROUGE-1 ROUGE-1 ROUGE-1 #L-11 Dialogue Generation (Dialog Gen) #L-12 Text Generation (TxT Gen) 1 Multi-Turn Daily Dialogue Generation (Daily Dialogue Gen) General Interactive Capability, Cognition Understanding daily dialogue (Li et al., 2017) 553 DialoGPT (Zhang, 2019) BLEU-1 1 Table-to-Text Generation (Table-toText) General Text Manipulation 2 Text Style Transfer (Style Transfer) General Creativity and lnnovation 3 Story Generation (Story Gen) General Creativity and lnnovation 4 Paraphrase Generation (Paraphrase) General Creativity and lnnovation ToTTo et al., 2020) (Parikh style-transferparaphrase, text style transfer () story-generation (Krishna et al., 2020) Manual (sto) 5 Grammar Correction (Grammar Correct) General Text Manipulation Grammar Correction (gra) 500 Flan-T5-Large (Chung 2022) et al., 500 Flan-T5-Large (Chung 2022) et al., 600 mFlan-T5-Large al., et (Chung 2022) 633 Flan-T5-Large (Chung 2022) et al., 302 Flan-T5-Large (Chung 2022) et al., BLEU-1 BLEU-1 BLEU-1 BLEU-1 BLEU-1 #L-13 Time Series Analysis (Time Series) Pred) 1 Time Series Prediction (Time Series General Temporal Determination, Reasoning Ability Manual 510 TimesFm-1.0200m (Das et al., 2023) RMSE 1 Topic Classification (Topic Cls) General Content Recognition Topic (top) 500 Roberta-Large (Liu, 2019) #L-14 Content Categorization #L-15 (Txt Cls) Text Entailment (Txt Entail) #L-16 Semantic Analysis (Sem Analy) 1 Natural Language Inference (NLI) 1 Sentence Similarity Detection (Sent Similar) 2 Intent Detection (Intent Det) 3 Stance Detection (Stance Detect) General Reasoning Ability, Causality Discrimination General Reasoning Ability, Causality Discrimination Cognition Understanding, Reasoning Ability Cognition Understanding, Reasoning Ability General, Social, Culture General, Social, Humanities 123 Micro-F1 Micro-F SNLI et al., 2015) (Bowman 500 Roberta-Large (Liu, 2019) QuoraQP (sen) 500 Roberta-Large (Liu, 2019) Micro-F1 Intent (int) 500 Roberta-Large (Liu, 2019) Micro-F SemEval2016 (Mohammad et al., 2016) 500 Roberta-Large (Liu, 2019) Micro-F1 On Path to Multimodal Generalist: General-Level and General-Bench Skill (Meta-Task) # Task (Short Name) Domain Capability Data Source Number Model Metrics Task Data SoTA Specialist 4 Personality Analysis (Person Analy) Business Reasoning Ability, Commonsense Knowledge, Cognition Understanding Essay2007 (per) 500 Roberta-Large (Liu, 2019) Micro-F1 1 Humor Detection (Humor Detect) Culture Affective Analysis Reddit 2 Sarcasm Detection (Sarcasm Det) Culture Affective Analysis #L-17 Affective Computing (Affect Computing) 3 Sentiment Classification (Sentiment Cls) 4 Mental Health Toxicity Detection (Mental-Toxic Det) 5 Financial Sentiment Analysis (Finance Cls) General Affective Analysis Humanities Affective Analysis MentalHealth Finance Affective Analysis 6 Metaphor Detection (Metaphor Det) Culture Affective Analysis Metaphor (Choi 500 Roberta-Large (Liu, 2019) (Weller and Seppi, 2019) Sarcasm (Misra and Arora, 2023) (Socher SST5 et al., 2013) (men) Financial SentiAnalysis ment al., et (Malo 2014) et al., 2021) res15 et al., 2015) res15 et al., 2015) (Pontiki (Pontiki res15 et al., 2015) (Pontiki TOWE (Fan et al., 2019b) SDRN et al., 2020) (Chen 500 Roberta-Large (Liu, 2019) 500 Roberta-Large (Liu, 2019) 500 Roberta-Large (Liu, 2019) 500 Roberta-Large (Liu, 2019) 500 Roberta-Large (Liu, 2019) 500 Roberta-Large (Liu, 2019) 500 Flan-T5-Large (Chung 2022) et al., 500 Flan-T5-Large (Chung 2022) et al., 500 Flan-T5-Large (Chung 2022) et al., 500 Flan-T5-Large (Chung 2022) et al., 500 Flan-T5-Large (Chung 2022) et al., 500 Flan-T5-Large (Chung 2022) et al., 500 Flan-T5-Large (Chung 2022) et al., 500 Flan-T5-Large (Chung 2022) et al., 500 Flan-T5-Large (Chung 2022) et 500 Roberta-Large (Liu, 2019) 500 Flan-T5-Large (Chung 2022) et al., al., Micro-F Micro-F1 Micro-F1 Micro-F1 Micro-F1 Micro-F1 Micro-F Micro-F1 Micro-F1 Micro-F1 Micro-F1 Micro-F1 Micro-F Micro-F1 Micro-F1 Micro-F1 Micro-F1 Micro-F1 7 Aspect Category Detection (Aspect Det) 8 Aspect Sentiment Classification (Aspect-Senti Cls) Business Affective Analysis Business Affective Analysis 9 Aspect Term Extraction (AspectTerm Ext) Business Affective Analysis 10 Target-oriented Opinion Words Extraction (Opinion Ext) Business Affective Analysis 11 Aspect-Opinion Pair Extraction (AO-Pair Ext) Business Affective Analysis 12 End-to-End ABSA (ABSA) Business Affective Analysis ABSA (Pontiki et al., 2015) 13 Aspect Sentiment Triplet Extraction (Senti-Triplet Ext) Business Affective Analysis ASTE (Xu et al., 2020) 14 Aspect-Category-Sentiment Detection (ACS Det) Business Affective Analysis 15 Aspect Sentiment Quad Prediction Business Affective Analysis (Senti-Quad Pred) 16 Dialogue-Level Sentiment Quadruple Extraction (Dialog Sent Quad) Business Affective Analysis res15 et al., 2015) (Pontiki res16 et al., 2016) (Pontiki diaasq (Li et al., 2023j) 17 Soccer Sentiment Classification (Soccer SA) Sports Affective Analysis FIFA-SA(soc) 18 Comparative Opinion Quintuple ExBusiness Affective Analysis Camera (Liu et al., traction (Opinion-Quin Ext) 2021) 1 Scientific NER (Sci NER) Physical Science Content Recognition SciER (Zhang et al., 2024h) 500 W2NER (Li et al., 2022e) Micro-F1 #L-18 Named Entity Recognition (NER) 2 Temporal NER (Temporal NER) Engineering Content Recognition TimeBank (tem) 500 W2NER (Li et al., Micro-F1 3 Pathology NER (Path NER) Biology Content Recognition Pathology NER (pat) 4 Cybersecurity NER (Cyber NER) Physical Science Content Recognition CyNER (cyb) 5 Geological NER (Geo NER) Geography Content Recognition GEO-NER (geo) 6 Legal NER (Legal NER) Politics Content Recognition InLegalNER (Kalamkar et al., 2022) 2022e) 500 W2NER (Li et al., 2022e) 500 W2NER (Li et al., 2022e) 500 W2NER (Li et al., 2022e) 500 W2NER (Li et al., 2022e) Micro-F1 Micro-F Micro-F1 Micro-F1 124 On Path to Multimodal Generalist: General-Level and General-Bench Skill (Meta-Task) # Task (Short Name) Domain Capability Data Source Number Model Metrics Task Data SoTA Specialist 7 Organization Recognition (Organ NER) 8 Person Recognition (Person NER) Politics Content Recognition CoNLL2003 (Sang and Meulder, 2003) General Content Recognition CoNLL (Sang and Meulder, 2003) 9 Location Recognition (Location NER) General Content Recognition CoNLL2003 (Sang and Meulder, 2003) 500 W2NER (Li et al., 2022e) Micro-F1 500 W2NER (Li et al., 2022e) Micro-F1 500 W2NER (Li et al., 2022e) Micro-F1 10 Climate Change NER (Climate NER) Climate Content Recognition Climate-ChangeNER (Sang and Meulder, 2003) 500 W2NER (Li et al., 2022e) Micro-F1 11 Gene/Protein Named Entity Recognition (Gene&Protein NER) 12 Chemical Named Entity Recognition (Chem NER) 13 Disease-NER (Disease NER) 1 Scientific Relation Extraction (Sci RE) 2 News Relation Extraction (News RE) Biology Content Recognition Genia (Sang and Meulder, 2003) Chemistry Content Recognition CHEMDNER (che, a) Biology Content Recognition Disease-NER (dis)"
        },
        {
            "title": "Content Recognition SciER",
            "content": "Physical Science General Content Recognition TACRED (Zhang et al., 2024h) (Zhang et al., 2017) #L-19 Relation Extraction (Rel Ext) 3 Joint NER and RE (Joint-NER-RE) General Content Recognition TACRED (Zhang et al., 2017) 4 DocRE (Doc RE) General Content Recognition DocRED (Yao et al., 2019) 5 Adverse Drug Reaction (ADR) Detection (ADR Det) Medicine Content Recognition ADEv2 (Gurulingappa et al., 2012) 6 Protein-Protein Interaction Extraction (PPI Ext) Biology Content Recognition PPI (ppi) 7 Drug-Drug Interaction (DDI Ext) Medicine Content Recognition DDI (ddi) #L-20 Event Extraction (Event Ext) #L-21 Semantic Parsing (Sem Par) 8 Genetic Association Datebase (Genetic Assoc) 9 Chemical-Protein Relationship Extraction (Chem-Protein RE) 1 Event Trigger Detection (EventTrigger Det) 2 Temporal Event Reasoning (TempEvent Reason) Biology Content Recognition GAD (gad) Chemistry Content Recognition ChemProt BLURB (che, b) General Content Recognition TAC-KBP (eve) General Content RecogTemporal nition, Determination TimeBank (tem) 1 Event Coreference Resolution (Event-Coref Res) Linguistics Content Recognition TAC-KBP 2015 (eve) 2 Semantic Role Labeling (SRL) Linguistics Content Recognition OntoNotes (srl) 3 Abstract Meaning Representation (AMR) Linguistics Content Recognition AMR 2.0 amrldc (amr) #L-22 Linguistic Parsing (Ling Par) 1 Dependency Parsing (Dep Parse) Linguistics Content Recognition Universal Dependency (srl) 2 Constituency Parsing (Const Parse) Linguistics Content Recognition OntoNotes (dpl) 3 Part-of-Speech (POS) Linguistics Content Recognition Wall Street Journal (pos) 125 500 W2NER (Li et al., 2022e) 500 W2NER (Li et al., 2022e) 500 W2NER (Li et al., 2022e)"
        },
        {
            "title": "500 W2NER (Li et al.,",
            "content": "2022e) 500 Roberta-Large (Liu, 2019) 500 W2NER (Li et al., 2022e) 500 W2NER (Li et al., 2022e) 500 Roberta-Large (Liu, 2019) 500 W2NER (Li et al., 2022e) 500 Roberta-Large (Liu, 2019) 500 Roberta-Large (Liu, 2019) 500 Roberta-Large (Liu, 2019) 500 Flan-T5-Large (Chung 2022) et al., 500 Roberta-Large (Liu, 2019) 500 Flan-T5-Large (Chung 2022) et al., 500 Flan-T5-Large (Chung 2022) et al., 500 Flan-T5-Large (Chung 2022) et al., 500 Flan-T5-Large (Chung 2022) et al., 500 Flan-T5-Large (Chung 2022) et al., 500 Flan-T5-Large (Chung 2022) et al., Micro-F Micro-F1 Micro-F1 Micro-F1 Micro-F1 Micro-F1 Micro-F Micro-F1 Micro-F1 Acc Micro-F1 Micro-F1 Micro-F Micro-F1 Acc Acc Acc Acc Acc Micro-F1 On Path to Multimodal Generalist: General-Level and General-Bench A.7 Example Task Gallery In this part, we provide visualized examples of representative tasks under each skill to offer clearer and more intuitive understanding of the task definitions, including their inputs and outputs. The examples are categorized based on different modalities and paradigms. A.7.1 IMAGE-RELATED TASKS Comprehension Tasks. Following we showcase 40 image-oriented comprehensive tasks, each representing specific skill. Figure 26: Sport Image Classification. 126 On Path to Multimodal Generalist: General-Level and General-Bench Figure 27: Sketch-to-HTML Code Generation. On Path to Multimodal Generalist: General-Level and General-Bench Figure 28: Tire Crack Detection. 128 On Path to Multimodal Generalist: General-Level and General-Bench Figure 29: Plant Disease Detection. On Path to Multimodal Generalist: General-Level and General-Bench Figure 30: Abnormal Heartbeat Recognition. Figure 31: Multi-Page News Document VQA. 130 On Path to Multimodal Generalist: General-Level and General-Bench Figure 32: Emotion Recognition. Figure 33: Spectrum Graph Car Classification. 131 On Path to Multimodal Generalist: General-Level and General-Bench Figure 34: Object Hallucination Detection Figure 35: Astronomy Image Captioning. On Path to Multimodal Generalist: General-Level and General-Bench Figure 36: Image Depth Estimation. Figure 37: Image Instance Segmentation. 133 On Path to Multimodal Generalist: General-Level and General-Bench Figure 38: License Plate ORC. Figure 39: Acne Recognition. 134 On Path to Multimodal Generalist: General-Level and General-Bench Figure 40: BobRoss Painting Segmentation. 135 On Path to Multimodal Generalist: General-Level and General-Bench Figure 41: Complex Expression Visual Grounding. Figure 42: Geometry Visual Question Answering. 136 On Path to Multimodal Generalist: General-Level and General-Bench Figure 43: Tuberculosis X-ray Segmentation. On Path to Multimodal Generalist: General-Level and General-Bench Figure 44: Brain FLAIR Abnormality Segmentation. 138 On Path to Multimodal Generalist: General-Level and General-Bench Figure 45: Person Keypoint Detection. On Path to Multimodal Generalist: General-Level and General-Bench Figure 46: Car Model Matching. 140 On Path to Multimodal Generalist: General-Level and General-Bench Figure 47: Environment-based Next-action Description. On Path to Multimodal Generalist: General-Level and General-Bench Figure 48: Fashion Concept Visual Question Answering. Figure 49: Seed Counting. 142 On Path to Multimodal Generalist: General-Level and General-Bench Figure 50: Multimodal Neural Translation. Figure 51: Medical Device Detection. 143 On Path to Multimodal Generalist: General-Level and General-Bench Figure 52: Animal Image Matting. Figure 53: Animal Recognition. On Path to Multimodal Generalist: General-Level and General-Bench Figure 54: Road Scene Panoptic Segmentation. Figure 55: Yoga Pose Recognition. 145 On Path to Multimodal Generalist: General-Level and General-Bench Figure 56: Description of Single Spatial Relationship. Figure 57: RGBD Semantic Segmentation. 146 On Path to Multimodal Generalist: General-Level and General-Bench Figure 58: Strawberry Ripeness Recognition. 147 On Path to Multimodal Generalist: General-Level and General-Bench Figure 59: Fire Detection. 148 On Path to Multimodal Generalist: General-Level and General-Bench Figure 60: Image Scene Graph Parsing. Figure 61: Landscape Recognition. On Path to Multimodal Generalist: General-Level and General-Bench Figure 62: American Sign Language Recognition. Figure 63: Before-After Relationship Caption. 150 On Path to Multimodal Generalist: General-Level and General-Bench Figure 64: Cartoon Storytelling. Figure 65: Weather Recognition. 151 On Path to Multimodal Generalist: General-Level and General-Bench Generation Tasks. Following we showcase 15 image-oriented generative tasks, each representing specific skill. Figure 66: Edge-to-Image Generation. On Path to Multimodal Generalist: General-Level and General-Bench Figure 67: EEG-to-Image Generation. 153 On Path to Multimodal Generalist: General-Level and General-Bench Figure 68: Image Raindrop Removal. On Path to Multimodal Generalist: General-Level and General-Bench Figure 69: Low-light Image Enhancement. 155 On Path to Multimodal Generalist: General-Level and General-Bench Figure 70: Face Image Inpainting. On Path to Multimodal Generalist: General-Level and General-Bench Figure 71: Text-based Image Style Transfer. 157 On Path to Multimodal Generalist: General-Level and General-Bench Figure 72: Infection-map Generation. On Path to Multimodal Generalist: General-Level and General-Bench Figure 73: Face Sketch Synthesis. 159 On Path to Multimodal Generalist: General-Level and General-Bench Figure 74: Image Editing with Text and Image Prompt. On Path to Multimodal Generalist: General-Level and General-Bench Figure 75: Layout-to-Face Image Generation. 161 On Path to Multimodal Generalist: General-Level and General-Bench Figure 76: Mask-to-Face Image Generation. On Path to Multimodal Generalist: General-Level and General-Bench Figure 77: Sketch-to-Face Image Generation. 163 On Path to Multimodal Generalist: General-Level and General-Bench Figure 78: Sound-to-Image Generation. On Path to Multimodal Generalist: General-Level and General-Bench Figure 79: Image Colorization. 165 On Path to Multimodal Generalist: General-Level and General-Bench Figure 80: Text-based Astronomical Image Generation. On Path to Multimodal Generalist: General-Level and General-Bench A.7.2 VIDEO-RELATED TASKS Comprehension Tasks. Following, we showcase 20 video-oriented comprehensive tasks, each representing specific skill. Figure 81: Movie Video Question Answering. Figure 82: Art Recognition. On Path to Multimodal Generalist: General-Level and General-Bench Figure 83: Sign Language Video Recognition. Figure 84: Business Video Understanding. 168 On Path to Multimodal Generalist: General-Level and General-Bench Figure 85: In-the-Wild Automobile Video Object Segmentation. 169 On Path to Multimodal Generalist: General-Level and General-Bench Figure 86: Human Video Object Segmentation. 170 On Path to Multimodal Generalist: General-Level and General-Bench Figure 87: Automobile Street-Scene Video Object Segmentation. 171 On Path to Multimodal Generalist: General-Level and General-Bench Figure 88: Human Referring Video Object Segmentation. 172 On Path to Multimodal Generalist: General-Level and General-Bench Figure 89: Animal Reasoning Video Object Segmentation. 173 On Path to Multimodal Generalist: General-Level and General-Bench Figure 90: Spatial-Temporal Static Action Detection. 174 On Path to Multimodal Generalist: General-Level and General-Bench Figure 91: Human-and-Part Complex-Scene Reasoning Video Object Segmentation (VOS). 175 On Path to Multimodal Generalist: General-Level and General-Bench Figure 92: Human Video Grounding. 176 On Path to Multimodal Generalist: General-Level and General-Bench Figure 93: Indoor Video Depth Estimation. 177 On Path to Multimodal Generalist: General-Level and General-Bench Figure 94: Orientation-and-movement-aware Object Matching. 178 On Path to Multimodal Generalist: General-Level and General-Bench Figure 95: Human Tracking. 179 On Path to Multimodal Generalist: General-Level and General-Bench Figure 96: Animal Long-Video Tracking. 180 On Path to Multimodal Generalist: General-Level and General-Bench Figure 97: UAV Video Building Tracking. 181 On Path to Multimodal Generalist: General-Level and General-Bench Figure 98: Underwater Video Object Tracking in Blue Water. 182 On Path to Multimodal Generalist: General-Level and General-Bench Figure 99: Optical Flow in Simple Synthetic Scene. 183 On Path to Multimodal Generalist: General-Level and General-Bench Figure 100: News and Document Video Captioning. 184 On Path to Multimodal Generalist: General-Level and General-Bench Generation Tasks. Following, we showcase 6 video-oriented generative tasks, each representing specific skill. Figure 101: Camera Motion Video Generation. Figure 102: Style-Specific Text to Video Generation. 185 On Path to Multimodal Generalist: General-Level and General-Bench Figure 103: Human Video Generation. Figure 104: Pet Image to Video Generation. 186 On Path to Multimodal Generalist: General-Level and General-Bench Figure 105: Video Deraining. 187 On Path to Multimodal Generalist: General-Level and General-Bench Figure 106: Video Non-Animal Inpainting. 188 On Path to Multimodal Generalist: General-Level and General-Bench A.7.3 AUDIO-RELATED TASKS Comprehension Tasks. Following we showcase 9 audio-oriented comprehensive tasks, each representing specific skill. Figure 107: Accent Classification. Figure 108: Intent Classification. 189 On Path to Multimodal Generalist: General-Level and General-Bench Figure 109: Speech Emotion Recognition. Figure 110: Music Instrument Classification. 190 On Path to Multimodal Generalist: General-Level and General-Bench Figure 111: Vocal Technique Detection. Figure 112: Long Audio Captioning. 191 On Path to Multimodal Generalist: General-Level and General-Bench Figure 113: Open-ended Audio Question Answering. Figure 114: Animal Sound Analysis. 192 On Path to Multimodal Generalist: General-Level and General-Bench Figure 115: Sound Event Recognition. 193 On Path to Multimodal Generalist: General-Level and General-Bench Generation Tasks. Following we showcase 11 audio-oriented generative tasks, each representing specific skill. Figure 116: Audio Editing. 194 On Path to Multimodal Generalist: General-Level and General-Bench Figure 117: Daily Talk Generation. 195 On Path to Multimodal Generalist: General-Level and General-Bench Figure 118: Emotional Speech Synthesis. 196 On Path to Multimodal Generalist: General-Level and General-Bench Figure 119: Multimodal Text-to-Speech (TTS). 197 On Path to Multimodal Generalist: General-Level and General-Bench Figure 120: Single Captions To Audio Generation. 198 On Path to Multimodal Generalist: General-Level and General-Bench Figure 121: Image-to-Speech Synthesis. 199 On Path to Multimodal Generalist: General-Level and General-Bench Figure 122: Video-to-Audio Synthesis. 200 On Path to Multimodal Generalist: General-Level and General-Bench Figure 123: Voice Conversion. Figure 124: Chinese-to-English Speech Translation. On Path to Multimodal Generalist: General-Level and General-Bench Figure 125: Song Synthesis. 202 On Path to Multimodal Generalist: General-Level and General-Bench Figure 126: Chord-based Music Style Transfer. On Path to Multimodal Generalist: General-Level and General-Bench A.7.4 3D-RELATED TASKS Comprehension Tasks. Following, we showcase 13 3D-oriented comprehensive tasks, each representing specific skill. Figure 127: 3D Tableware Classification. Figure 128: 3D Furniture Classification. 204 On Path to Multimodal Generalist: General-Level and General-Bench Figure 129: 3D Vehicle Classification. 205 On Path to Multimodal Generalist: General-Level and General-Bench Figure 130: 3D Indoor Appliance Semantic Segmentation. 206 On Path to Multimodal Generalist: General-Level and General-Bench Figure 131: 3D Outdoor Vehicle Semantic Segmentation. 207 On Path to Multimodal Generalist: General-Level and General-Bench Figure 132: 3D Indoor Instance Segmentation. 208 On Path to Multimodal Generalist: General-Level and General-Bench Figure 133: 3D Odometry. 209 On Path to Multimodal Generalist: General-Level and General-Bench Figure 134: 3D Household Part Segmentation. 210 On Path to Multimodal Generalist: General-Level and General-Bench Figure 135: 3D Vehicle Tracking. 211 On Path to Multimodal Generalist: General-Level and General-Bench Figure 136: 3D Normal Estimation. 212 On Path to Multimodal Generalist: General-Level and General-Bench Figure 137: 3D Vehicle Detection. 213 On Path to Multimodal Generalist: General-Level and General-Bench Figure 138: 3D QA for Spatial Scene Understanding. 214 On Path to Multimodal Generalist: General-Level and General-Bench Figure 139: 3D Motion Captioning. 215 On Path to Multimodal Generalist: General-Level and General-Bench Generation Tasks. Following, we showcase 9 3D-oriented generative tasks, each representing specific skill. Figure 140: 3D-Point-Cloud Completion. 216 On Path to Multimodal Generalist: General-Level and General-Bench Figure 141: Point-Cloud-to-Mesh Scene Reconstruction. 217 On Path to Multimodal Generalist: General-Level and General-Bench Figure 142: Living-and-Arts Image-to-3D-Point-Cloud Generation. Figure 143: Nature-and-Biology Text-to-3D-Mesh Generation. 218 On Path to Multimodal Generalist: General-Level and General-Bench Figure 144: Science-and-Technology Image-to-3D-Point-Cloud Generation. On Path to Multimodal Generalist: General-Level and General-Bench Figure 145: Culture-and-Structure Image to 3D Mesh Generation. 220 On Path to Multimodal Generalist: General-Level and General-Bench Figure 146: RGBD-to-Point-Cloud Reconstruction. On Path to Multimodal Generalist: General-Level and General-Bench Figure 147: RGBD-to-Mesh Reconstruction. 222 On Path to Multimodal Generalist: General-Level and General-Bench Figure 148: Text-to-3D-Motion Generation. On Path to Multimodal Generalist: General-Level and General-Bench A.7.5 LANGUAGE TASKS. Following we showcase 22 NLP tasks, each representing specific skill. Figure 149: Commonsense Reasoning. Figure 150: Offensive Classification. On Path to Multimodal Generalist: General-Level and General-Bench Figure 151: Legal Question Answering. Figure 152: Nature Question Answering. 225 On Path to Multimodal Generalist: General-Level and General-Bench Figure 153: Social Science Question Answering. Figure 154: Tweet Question Answering. 226 On Path to Multimodal Generalist: General-Level and General-Bench Figure 155: Tweet Question Answering. Figure 156: Multilingual Question Answering. On Path to Multimodal Generalist: General-Level and General-Bench Figure 157: Math Question Answering. Figure 158: Code Explanation. 228 On Path to Multimodal Generalist: General-Level and General-Bench Figure 159: English-Chinese Translation. Figure 160: Abstractive Summarization. 229 On Path to Multimodal Generalist: General-Level and General-Bench Figure 161: Multi-Turn Daily Dialogue Generation. Figure 162: Text Style Transfer. On Path to Multimodal Generalist: General-Level and General-Bench Figure 163: Time Series Prediction. Figure 164: Question Classification. 231 On Path to Multimodal Generalist: General-Level and General-Bench Figure 165: Natural Language Inference. Figure 166: Stance Detection. Figure 167: Personality Analysis. 232 On Path to Multimodal Generalist: General-Level and General-Bench Figure 168: Sentence Similarity Detection. Figure 169: Sentiment Classification. Figure 170: Disease-NER. 233 On Path to Multimodal Generalist: General-Level and General-Bench Figure 171: Climate Change NER. Figure 172: Organization Recognition. Figure 173: Adverse Drug Reaction Detection. 234 On Path to Multimodal Generalist: General-Level and General-Bench Figure 174: Joint NER and RE. Figure 175: Event Trigger Detection. 235 On Path to Multimodal Generalist: General-Level and General-Bench Figure 176: Dependency Parsing. Figure 177: Semantic Role Labeling. 236 On Path to Multimodal Generalist: General-Level and General-Bench"
        },
        {
            "title": "B Extension on Experimental Results",
            "content": "In the main text, we presented subset of the evaluation performance of MLLMs at the skill level due to space constraints. Here, we provide the complete results on all specific tasks. The results are organized based on modality and task paradigm distinctions. It is important to note that we conducted inference using different MLLMs open-source codebases or APIs. However, the choice of prompts for different tasks can significantly impact the models performance on given task. For instance, some models require highly detailed and specific in-context instructions in the input prompt to achieve their best performance. To ensure fairness, our team applied uniform prompt across all MLLMs for given task, without any model-specific prompt tuning. If the model developers are unsatisfied with the presented results, we welcome them to adjust the prompts to obtain more representative and improved scores. B.1 Results of Image-related Tasks Image Comprehension Results. The complete results of all models on image comprehension are presented in Table 39 to Table 64. 237 On Path to Multimodal Generalist: General-Level and General-Bench Table 39: Results on Image Comprehension Group, from #I-C-1 to #I-C-5. Scores over SoTA specialists are bolded."
        },
        {
            "title": "Model",
            "content": "#I-C-1 (Behavior Recog) #I-C-2 (Code Gen) #I-C-3 (Crack Det) #I-C-4 (Disease Det) #I-C-5 (Disease Recog) #1 #2 #3 #1 #1 #2 #1 #1 #2 #3 #4 #5 #"
        },
        {
            "title": "SoTA Specialist",
            "content": "21.70 82.30 49.80 53.32 63.08 21.00 22.30 40.52 40.50 35.90 38.42 16.40 62.40 57.00 87.97 63.30 GPT4V 58.70 90.10 72.80 GPT4o 25.70 88.50 70.20 GPT4o-mini 24.90 90.00 89.40 GPT-4o-4096 29.60 66.90 80.40 ChatGPT-4o-latest 46.67 87.90 68.60 Claude-3.5-Sonnet 43.16 88.58 64.40 Claude-3.5-Opus 32.11 82.26 46.91 Emu2-32B 24.49 76.13 55.92 DetGPT 27.10 73.57 79.20 InternVL2.5-8B 32.30 63.77 39.80 InternVL2.5-4B 26.80 76.73 61.80 InternVL2.5-2B 24.60 83.03 47.80 Monkey-10B-chat 20.00 79.70 60.20 DeepSeek-VL-7B-Chat 20.00 85.13 89.40 Qwen2-VL-7B 20.00 72.66 56.39 Qwen-VL-Chat 20.20 67.67 63.55 MoE-LLAVA-Phi2-2.7B-4e-384 21.09 75.76 60.74 mPLUG-Owl2-LLaMA2-7b 20.40 83.97 61.60 Phi-3.5-Vision-Instruct Cambrian-1-8B 11.10 75.73 48.29 28.80 58.70 27.52 MiniGPT4-LLaMA2-7B 45.80 86.90 81.40 InternVL-Chat-V1-5 34.40 81.10 53.40 Mini-InternVL-Chat-4B-V1-5 InternLM-XComposer2-VL-1.8B 20.60 79.90 43.40 21.70 57.20 35.80 GPT4RoI 0.00 GLaMM 0.00 0.00 42.60 80.73 66.80 LLaVA-NeXT-13B 52.60 83.90 70.40 LLaVA-NeXT-34B 46.40 77.43 65.20 Pixtral 12B 28.30 57.93 53.80 SEED-LLaMA-13B 33.90 71.47 44.60 BLIP2 4.90 34.77 49.40 MiniMonkey 23.30 79.53 57.80 DeepSeek-VL-7B 0.00 LISA 0.00 0.00 46.35 85.57 64.40 CogVLM-Chat 40.76 78.20 54.20 ShareGPT4V-7B 44.19 80.03 57.60 ShareGPT4V-13B 43.28 82.63 61.80 BLIP-3 (XGen-MM) 12.20 45.70 47.60 AnyGPT 47.30 79.13 68.80 MiniCPM3-4B 36.90 55.17 55.40 LaVIT-V2 (7B) 51.10 72.20 70.20 GLM-VL-Chat 47.60 91.20 78.20 Gemini-1.5-Pro 42.10 87.30 71.60 Gemini-1.5-Flash 3.10 OMG-LLaVA-InternLM20B 1.40 4.20 43.10 79.30 62.80 Idefics3-8B-Llama3 34.89 66.38 39.87 NExT-GPT-V1.5 39.78 60.75 42.39 Vitron-V1 7.70 31.26 0.00 Otter 20.00 0.18 Show-o 4.21 19.92 45.76 63.54 NExT-Chat 24.60 79.40 82.96 Yi-vision-v2 24.13 84.73 92.08 Qwen2-VL-72B 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 3.25 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 2.30 0.00 0.00 0.00 0.00 0.00 58.64 63.42 61.38 0.00 57.28 60.86 57.69 7.31 0.00 4.86 4.97 4.64 0.00 4.27 3.61 3.40 1.90 0.00 3.44 0.00 0.40 0.00 0.00 3.40 8.10 0.00 8.67 10.32 3.25 0.00 0.00 0.00 0.00 0.00 12.32 8.45 10.37 8.54 0.00 4.68 0.00 17.92 23.41 25.79 4.03 11.50 3.70 3.90 0.00 0.00 0.00 2.13 5.74 79.08 0.00 86.46 0.00 77.85 0.00 89.54 0.00 85.54 0.00 80.30 0.00 79.90 0.00 73.23 0.00 59.07 16.08 49.85 0.00 34.15 0.00 66.22 0.00 37.23 0.00 67.57 0.00 58.78 0.00 44.30 0.00 64.62 0.00 52.00 0.00 68.31 0.00 55.08 0.00 65.23 0.00 79.60 0.00 72.60 0.00 75.00 0.00 69.80 0.00 0.00 0.00 73.85 0.00 76.00 0.00 65.85 0.00 63.69 0.00 51.20 0.00 12.62 0.00 67.69 0.00 0.00 0.00 84.62 0.00 72.92 0.00 75.69 0.00 82.15 0.00 53.54 0.00 74.77 0.00 62.15 0.00 81.85 0.00 78.77 0.00 75.69 0.00 3.70 0.00 72.00 0.00 65.34 0.00 66.75 36.40 0.00 0.00 0.00 0.00 68.12 0.00 76.31 0.00 71.27 0.00 238 0.00 0. 0.00 56.60 99.75 90.92 38.01 49.40 62.40 69.25 98.64 94.97 50.89 53.00 62.60 75.00 96.93 50.14 44.55 40.40 49.60 74.43 99.75 99.88 63.56 52.00 62.40 63.22 98.40 97.23 37.21 47.20 59.34 66.32 97.72 78.40 44.37 47.38 57.23 63.10 96.64 76.74 43.48 45.40 54.76 41.67 40.50 35.90 40.99 38.40 50.40 31.89 39.98 28.73 31.09 26.20 44.60 48.56 53.44 6.15 30.11 27.20 63.00 60.06 46.31 21.65 36.63 25.20 62.20 37.36 90.79 23.04 39.41 31.20 60.40 0.00 28.12 38.00 62.40 36.49 94.59 37.64 100.00 19.57 43.56 28.20 62.40 72.70 48.86 73.72 42.38 38.40 56.20 27.01 48.85 40.05 26.93 20.40 63.20 52.59 48.86 34.24 36.83 20.00 62.60 47.87 47.60 40.00 18.02 21.20 45.60 50.86 59.54 54.34 25.74 26.80 38.40 43.00 33.66 19.60 5.60 38.51 51.44 82.65 77.18 97.32 10.74 55.03 64.30 83.60 56.00 34.60 32.60 49.60 64.60 65.70 58.20 27.90 27.00 56.00 42.50 78.90 67.50 21.30 21.60 33.40 51.10 24.00 14.30 3.70 16.70 51.60 0.00 0.00 52.87 51.97 47.21 42.18 35.60 44.80 60.35 56.39 43.44 39.80 37.40 55.20 54.02 46.07 52.93 34.06 31.80 49.20 34.48 46.68 55.87 30.69 28.20 47.60 46.26 40.91 19.13 30.10 25.00 42.80 4.08 69.20 0.00 42.66 95.45 37.50 32.28 26.20 64.60 0.00 0.00 54.31 58.48 52.51 42.38 36.80 54.20 38.79 49.51 48.32 30.50 31.40 48.40 49.43 53.69 51.96 35.45 35.60 52.60 52.87 59.71 55.45 37.43 34.20 50.60 17.15 43.98 47.17 20.59 11.20 18.00 57.18 56.76 49.16 39.80 36.80 49.20 34.48 49.88 52.23 32.87 27.40 48.60 63.79 53.44 56.28 44.16 39.80 54.80 69.54 98.90 54.89 44.55 46.20 60.20 67.24 98.03 52.65 40.20 42.20 56.40 6.90 3.40 58.62 69.41 50.98 42.19 41.80 52.20 36.45 43.79 53.47 21.56 15.40 32.50 29.38 50.32 53.46 32.42 13.68 34.70 18.15 0.20 31.89 51.14 54.33 20.19 2.21 37.40 26.82 42.61 47.31 11.74 22.41 16.64 47.13 52.95 15.78 40.79 30.20 60.00 80.79 46.72 53.24 55.30 41.00 62.40 0.00 7.25 2.60 1. 3.77 1.00 0.00 0.00 0.20 0. 0.00 0.00 0.00 0.00 0.00 0. 0.00 On Path to Multimodal Generalist: General-Level and General-Bench Table 40: Results on Image Comprehension Group,#I-C-6, part A."
        },
        {
            "title": "Model",
            "content": "#I-C-6 (Doc VQA) #1 #2 #3 #4 #5 #6 #7 #8 #9 #10 #"
        },
        {
            "title": "SoTA Specialist",
            "content": "32.72 10.64 11.76 19.40 17.80 26. 20.92 30.94 16.11 23.91 13.79 GPT4V GPT4o GPT4o-mini GPT-4o-4096 ChatGPT-4o-latest Claude-3.5-Sonnet Claude-3.5-Opus Emu2-32B DetGPT InternVL2.5-8B InternVL2.5-4B InternVL2.5-2B Monkey-10B-chat DeepSeek-VL-7B-Chat Qwen2-VL-7B Qwen-VL-Chat MoE-LLAVA-Phi2-2.7B-4e-384 mPLUG-Owl2-LLaMA2-7b Phi-3.5-Vision-Instruct Cambrian-1-8B MiniGPT4-LLaMA2-7B InternVL-Chat-V1-5 Mini-InternVL-Chat-4B-V1-5 InternLM-XComposer2-VL-1.8B GPT4RoI GLaMM LLaVA-NeXT-13B LLaVA-NeXT-34B Pixtral 12B SEED-LLaMA-13B BLIP2 MiniMonkey DeepSeek-VL-7B LISA CogVLM-Chat ShareGPT4V-7B ShareGPT4V-13B BLIP-3 (XGen-MM) AnyGPT MiniCPM3-4B LaVIT-V2 (7B) GLM-VL-Chat Gemini-1.5-Pro Gemini-1.5-Flash OMG-LLaVA-InternLM20B Idefics3-8B-Llama3 NExT-GPT-V1.5 Vitron-V1 Otter Show-o NExT-Chat Yi-vision-v2 Qwen2-VL-72B 45.23 48.16 41.18 49.63 44.49 44.38 42.65 32.72 30.88 33.45 31.54 31.63 32.42 7.43 45.83 18.75 11.40 13.24 34.00 30.95 14.09 18.00 32.70 15.80 3.30 0.00 28.68 31.99 33.46 18.38 4.78 34.96 31.99 0.00 30.88 22.43 25.37 28.68 4.78 34.19 16.18 35.66 39.71 35.29 1.47 29.41 3.30 3.56 0.00 6.25 13.29 40.32 29.86 40.78 48.58 38.30 47.52 45.39 42.54 42.22 29.79 18.44 28.47 26.59 29.07 27.54 7.55 52.00 16.66 9.57 9.57 42.00 31.91 17.45 20.20 26.50 19.80 2.80 0.00 28.01 34.04 35.46 19.15 4.26 31.22 25.89 0.00 34.40 24.11 25.53 29.79 10.64 36.88 18.79 38.65 31.21 26.24 1.42 31.91 3.80 4.29 0.00 7.80 14.55 31.03 37.06 35.64 41.58 40.59 46.53 44.55 39.07 38.18 28.67 31.86 43.99 36.50 32.56 34.96 7.40 46.70 23.76 12.87 18.81 40.51 29.41 15.00 13.80 30.60 14.80 3.90 0.00 38.61 42.57 34.65 14.85 7.92 32.67 33.66 0.00 37.62 28.71 32.67 33.66 6.93 41.58 12.87 42.57 42.57 35.64 1.98 36.63 4.29 8.10 0.00 6.93 13.04 35.48 18.37 38.80 42.63 38.80 46.45 40.44 39.97 36.83 30.42 34.55 41.89 37.34 37.77 29.52 8.82 62.66 19.67 8.19 11.48 40.47 21.21 17.45 21.70 19.60 18.00 4.30 0.00 37.30 42.70 40.00 8.74 9.84 36.93 28.96 0.00 42.62 33.88 37.16 39.34 0.00 47.54 12.02 22.95 34.97 26.23 1.64 30.60 5.69 7.59 0.00 5.46 12.92 29.73 54.63 239 46.46 50.51 45.45 51.18 47.81 46.59 44.33 26.27 29.40 35.77 30.33 33.72 29.71 8.94 50.00 15.48 9.76 11.45 40.00 38.57 18.79 60.00 26.20 15.10 2.00 0.00 33.67 38.05 35.02 7.41 5.72 33.67 30.64 0.00 36.03 29.29 31.99 32.66 0.00 36.61 9.29 39.34 34.01 29.29 1.35 29.97 2.36 3.21 0.00 4.71 15.41 37.50 30. 35.61 39.04 30.83 45.21 32.88 34.58 34.28 25.25 27.42 30.83 27.89 29.91 29.47 10.65 46.66 9.58 12.32 10.96 38.23 18.60 26.21 33.30 21.20 13.00 3.40 0.00 35.62 36.99 29.45 5.48 6.16 19.17 22.60 0.00 34.93 26.71 29.45 32.19 0.00 35.62 0.00 39.04 33.56 28.77 2.05 28.77 4.28 4.76 0.00 5.48 16.72 38.24 43.27 28.61 29.04 30.83 43.51 39.75 28.74 26.74 19.06 19.47 35.06 32.84 32.53 26.77 9.72 52.94 15.48 11.72 10.04 40.11 19.83 20.81 22.20 25.60 20.00 5.00 0.00 24.69 28.45 22.59 12.97 7.11 30.54 25.10 0.00 25.52 21.33 22.59 24.27 5.44 27.20 11.28 29.71 33.89 26.78 1.67 30.13 6.10 8.36 0.00 3.77 16.87 31.25 33.74 27.61 30.04 31.83 46.41 43.65 28.91 26.22 21.40 22.87 36.67 35.68 33.15 30.89 8.08 68.18 17.12 10.77 11.60 52.00 26.32 16.11 15.10 22.90 16.20 1.30 0.00 28.45 29.83 25.69 10.50 3.87 27.90 30.11 0.00 29.56 23.48 25.41 28.18 3.59 30.11 10.50 25.97 30.39 31.49 1.66 26.52 1.60 4.39 0.00 5.80 11.33 38.78 55.22 30.61 31.04 32.83 48.59 30.27 31.14 30.67 18.26 18.84 41.67 38.71 40.21 35.36 11.39 51.06 17.39 9.72 12.28 35.35 30.16 20.81 57.10 34.00 24.80 5.80 0.00 26.85 24.81 25.58 11.76 4.09 33.73 36.06 0.00 26.60 20.20 21.99 24.81 2.30 29.16 11.00 27.37 33.50 29.16 1.53 28.39 7.50 3.78 0.00 5.37 11.48 42.62 54.27 31.62 32.04 33.83 42.81 30.28 32.49 27.91 19.45 19.64 25.49 25.03 22.61 24.40 11.09 36.73 15.02 10.99 13.41 35.35 23.73 11.41 18.90 26.90 17.40 5.10 0.00 26.33 28.92 24.07 13.09 5.98 21.64 21.00 0.00 25.36 20.03 21.65 23.10 2.26 21.81 12.12 27.14 28.27 27.14 1.78 24.23 6.90 6.52 0.00 5.98 18.04 34.15 42.94 28.61 31.23 30.23 46.29 45.85 29.86 26.64 20.55 20.93 35.23 33.90 33.48 30.57 10.51 58.62 15.72 8.73 7.86 35.00 31.03 16.78 15.70 13.70 16.50 3.90 0.00 27.07 28.82 24.02 14.41 2.18 25.76 26.20 0.00 27.95 20.52 22.71 25.33 6.11 23.14 15.28 29.26 29.26 25.76 2.18 26.20 3.70 2.16 0.00 3.06 13.63 35.71 36. On Path to Multimodal Generalist: General-Level and General-Bench Table 41: Results on Image Comprehension Group,#I-C-6, part B."
        },
        {
            "title": "Model",
            "content": "#I-C-6 (Doc VQA) #12 #13 #14 #15 #16 #17 #18 #19 #20 #"
        },
        {
            "title": "SoTA Specialist",
            "content": "11.76 21.51 21.51 24.06 14.02 11. 23.26 16.67 19.51 82.20 GPT4V GPT4o GPT4o-mini GPT-4o-4096 ChatGPT-4o-latest Claude-3.5-Sonnet Claude-3.5-Opus Emu2-32B DetGPT InternVL2.5-8B InternVL2.5-4B InternVL2.5-2B Monkey-10B-chat DeepSeek-VL-7B-Chat Qwen2-VL-7B Qwen-VL-Chat MoE-LLAVA-Phi2-2.7B-4e-384 mPLUG-Owl2-LLaMA2-7b Phi-3.5-Vision-Instruct Cambrian-1-8B MiniGPT4-LLaMA2-7B InternVL-Chat-V1-5 Mini-InternVL-Chat-4B-V1-5 InternLM-XComposer2-VL-1.8B GPT4RoI GLaMM LLaVA-NeXT-13B LLaVA-NeXT-34B Pixtral 12B SEED-LLaMA-13B BLIP2 MiniMonkey DeepSeek-VL-7B LISA CogVLM-Chat ShareGPT4V-7B ShareGPT4V-13B BLIP-3 (XGen-MM) AnyGPT MiniCPM3-4B LaVIT-V2 (7B) GLM-VL-Chat Gemini-1.5-Pro Gemini-1.5-Flash OMG-LLaVA-InternLM20B Idefics3-8B-Llama3 NExT-GPT-V1.5 Vitron-V1 Otter Show-o NExT-Chat Yi-vision-v2 Qwen2-VL-72B 32.35 33.82 29.42 38.24 30.88 31.41 28.32 19.82 20.46 38.78 34.27 32.44 25.43 7.96 45.61 18.37 11.76 11.35 47.90 25.00 21.48 20.00 32.30 22.00 4.40 0.00 23.53 30.88 20.59 17.65 2.94 27.94 25.00 0.00 25.00 17.65 20.59 26.47 8.82 29.41 16.18 27.94 32.35 29.42 2.94 26.47 3.60 3.03 0.00 4.41 17.83 30.77 44. 53.70 50.90 40.40 48.11 47.03 48.16 44.09 24.25 26.42 27.03 25.32 23.02 27.47 8.85 43.90 15.23 11.89 10.60 44.23 38.46 18.79 33.30 22.10 14.50 4.80 0.00 32.43 35.14 28.65 15.14 1.99 33.11 25.83 0.00 34.59 27.57 29.19 31.35 5.95 37.84 20.55 36.76 42.16 38.92 3.24 33.11 5.78 2.39 0.00 8.11 16.54 43.34 25.64 34.10 35.70 30.90 45.94 40.94 32.75 30.11 21.35 21.88 40.85 37.95 40.45 36.00 14.60 46.42 16.17 12.81 5.88 41.30 21.18 16.11 41.00 33.70 20.90 3.40 0.00 27.81 30.94 25.94 13.13 5.31 36.62 34.06 0.00 27.81 22.19 23.75 25.31 1.88 30.31 11.25 32.50 31.25 29.38 2.50 28.13 4.78 4.75 0.00 4.06 19.14 32.39 48.99 35.70 44.00 42.00 44.37 40.40 39.76 37.23 8.96 5.82 34.99 34.40 34.67 30.88 9.96 43.50 17.50 8.61 8.75 43.00 11.54 28.08 17.20 24.50 21.10 3.90 0.00 18.60 23.50 12.40 4.80 6.45 23.93 27.53 0.00 21.19 13.91 17.22 19.20 5.96 11.26 9.27 19.21 35.76 29.80 2.65 34.40 2.63 0.24 0.00 3.97 13.70 36.26 56.50 240 25.90 26.10 33.30 42.58 16.13 27.96 24.71 16.85 16.75 23.50 20.95 23.62 22.75 10.71 30.55 13.54 9.03 10.65 38.25 20.00 14.09 22.90 13.70 17.00 7.00 0.00 20.97 24.52 23.55 14.19 9.68 16.77 22.58 0.00 22.90 18.39 19.68 21.61 4.94 24.84 12.58 24.19 29.03 24.19 1.61 30.00 7.60 8.31 0.00 3.23 12.24 30.30 40.27 14.30 43.80 25.00 44.60 40.30 27.67 24.46 19.97 20.36 26.54 25.62 24.74 26.32 8.59 38.00 14.35 10.03 11.42 33.70 22.73 22.82 17.50 23.40 15.50 3.80 0.00 25.15 27.62 19.29 9.57 2.93 27.47 25.77 0.00 27.16 22.07 24.23 25.31 2.16 16.05 9.10 33.02 25.31 19.75 1.54 30.56 0.36 3.96 0.00 5.40 0.00 25.93 57. 51.50 48.70 39.60 53.49 51.62 46.05 46.05 24.98 26.60 45.26 49.59 47.28 38.19 15.29 32.52 18.60 11.63 13.95 55.11 17.65 33.33 13.90 34.80 23.20 9.30 0.00 27.91 32.56 20.93 30.23 9.30 34.88 34.88 0.00 37.21 23.26 27.91 32.56 0.47 32.56 20.93 44.19 39.60 34.88 0.00 32.56 5.48 10.76 0.00 4.65 17.45 9.30 37.07 41.20 42.20 40.20 46.67 42.38 40.66 39.79 23.45 24.99 41.02 38.43 39.11 32.31 11.17 44.47 14.28 6.19 6.19 50.00 17.50 22.82 46.10 33.80 13.30 1.00 0.00 30.00 35.71 32.38 7.62 0.95 30.00 31.43 0.00 30.48 24.76 26.67 29.05 0.95 34.29 6.67 30.95 38.57 35.24 2.38 38.57 1.08 1.39 0.00 4.76 11.59 39.24 25.15 36.20 34.30 31.60 47.76 35.26 33.43 30.15 21.22 22.42 38.20 35.43 31.78 35.43 8.66 46.00 16.32 7.76 11.02 47.36 26.83 16.78 25.00 24.00 13.40 2.40 0.00 25.71 31.84 28.57 18.37 1.22 36.73 34.29 0.00 27.35 20.82 23.27 26.12 5.71 32.65 17.55 31.02 34.29 28.57 1.63 31.43 3.75 5.68 0.00 6.94 11.93 34.38 28.00 43.21 49.16 40.08 95.90 48.28 43.90 42.83 14.03 12.14 38.41 31.93 33.33 24.84 8.02 45.61 15.06 42.80 38.94 49.00 43.46 2.72 44.10 41.50 39.10 1.00 0.00 32.40 36.20 30.80 15.60 16.59 20.63 8.00 0.00 31.50 24.94 26.19 28.47 0.00 35.64 14.77 32.17 40.60 36.20 0.00 29.80 1.00 3.70 0.00 0.00 15.74 7.38 50.43 On Path to Multimodal Generalist: General-Level and General-Bench Table 42: Results on Image Comprehension Group, from #I-C-7 to #I-C-10."
        },
        {
            "title": "Model",
            "content": "#I-C-7 (Emotion Det) #I-C-8 (Graph Cls) #I-C-9 (Hallucination Det) #I-C-10 (Img Cap) #1 #2 #3 #4 #1 #1 #2 #1 #2 #3 #4 #5 #"
        },
        {
            "title": "SoTA Specialist",
            "content": "31.12 59.44 41.20 52.30 15.67 41.60 60.80 17.84 62.99 23.05 18.40 17.65 28.10 51.06 61.45 53.83 80.60 GPT4V 63.65 63.86 61.51 86.30 GPT4o 61.38 56.62 64.57 84.30 GPT4o-mini 65.63 64.66 66.39 88.70 GPT-4o-4096 55.02 62.25 59.41 81.11 ChatGPT-4o-latest 58.31 59.89 59.92 83.26 Claude-3.5-Sonnet 57.71 56.41 57.87 81.74 Claude-3.5-Opus 32.67 40.76 45.61 48.51 Emu2-32B 21.64 32.93 41.56 58.69 DetGPT 45.54 65.06 47.14 72.41 InternVL2.5-8B 47.10 61.45 45.75 100.00 InternVL2.5-4B 49.79 58.23 50.21 67.22 InternVL2.5-2B 36.78 46.99 20.22 57.78 Monkey-10B-chat 47.67 62.65 48.50 78.52 DeepSeek-VL-7B-Chat 29.00 61.45 28.73 73.04 Qwen2-VL-7B 32.67 46.98 25.52 64.62 Qwen-VL-Chat 50.35 56.63 48.26 48.28 MoE-LLAVA-Phi2-2.7B-4e-384 42.29 54.62 39.03 40.17 mPLUG-Owl2-LLaMA2-7b 19.31 60.64 46.72 78.70 Phi-3.5-Vision-Instruct 14.29 49.80 44.70 51.40 Cambrian-1-8B 33.66 40.56 74.50 87.25 MiniGPT4-LLaMA2-7B 49.30 55.80 49.20 76.20 InternVL-Chat-V1-5 45.60 41.30 31.50 65.70 Mini-InternVL-Chat-4B-V1-5 InternLM-XComposer2-VL-1.8B 29.30 39.30 47.60 62.70 47.80 46.10 40.40 43.30 GPT4RoI 0.00 GLaMM 0.00 36.21 48.59 34.73 62.41 LLaVA-NeXT-13B 45.69 55.42 47.00 71.48 LLaVA-NeXT-34B 38.47 47.38 39.05 64.07 Pixtral 12B 26.73 35.74 34.03 43.89 SEED-LLaMA-13B 23.06 43.37 29.43 51.11 BLIP2 12.16 21.69 11.16 58.63 MiniMonkey 26.59 58.78 44.21 73.26 DeepSeek-VL-7B 0.00 LISA 0.00 44.55 48.59 45.19 69.63 CogVLM-Chat 30.13 39.36 30.26 56.85 ShareGPT4V-7B 31.26 45.78 31.24 60.37 ShareGPT4V-13B 42.57 42.17 43.79 63.33 BLIP-3 (XGen-MM) 23.62 26.03 19.94 34.26 AnyGPT 48.37 52.21 53.56 64.26 MiniCPM3-4B 37.06 44.18 34.59 46.11 LaVIT-V2 (7B) 46.82 55.02 50.49 74.63 GLM-VL-Chat 58.58 69.08 55.65 81.67 Gemini-1.5-Pro 57.85 67.07 52.86 76.67 Gemini-1.5-Flash 6.36 13.65 3.49 OMG-LLaVA-InternLM20B 3.89 45.83 51.41 35.70 71.30 Idefics3-8B-Llama3 28.90 33.57 37.56 46.10 NExT-GPT-V1.5 33.40 34.10 38.16 53.47 Vitron-V1 14.59 10.89 3.91 Otter 3.33 10.34 12.61 17.15 17.22 Show-o 25.43 70.63 0.00 27.39 NExT-Chat 21.07 53.41 16.60 35.48 Yi-vision-v2 28.22 61.88 26.47 78.57 Qwen2-VL-72B 0.00 0. 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 7.17 0.00 0.00 0.00 0.00 241 6.22 7.83 0. 2.63 42.22 15.80 18.70 27.21 18.69 12.80 21.00 1.30 48.10 16.51 19.81 30.42 23.30 69.60 66.00 4.00 43.10 18.64 15.05 25.62 18.60 40.80 78.20 1.00 45.45 17.36 20.68 28.86 15.72 68.20 70.80 1.74 41.23 15.82 15.85 21.03 13.53 50.00 64.20 1.96 44.25 16.80 17.63 26.82 19.57 40.40 54.25 0.74 41.23 16.02 17.73 26.82 20.09 38.13 53.11 0.54 37.13 12.42 11.34 0.00 15.53 17.60 24.80 0.00 13.86 0.19 29.04 8.65 5.20 6.80 9.06 16.99 20.42 13.79 7.49 7.02 4.29 11.23 8.24 16.43 20.53 13.37 14.59 4.61 13.13 32.62 7.36 9.41 3.79 9.58 16.03 20.62 7.47 6.35 43.69 13.83 2.45 17.89 0.00 5.93 3.10 12.75 17.77 17.84 19.03 3.64 3.48 3.18 4.40 57.14 26.50 20.60 36.62 13.77 52.20 24.91 4.00 58.61 26.28 20.66 24.85 15.33 36.19 2.80 4.11 57.39 28.36 18.74 13.29 10.79 7.60 0.00 4.07 52.69 26.30 20.00 11.91 10.32 1.20 0.00 4.63 54.51 26.84 24.50 20.20 15.40 0.00 0.00 4.20 30.60 19.92 10.44 21.50 10.09 1.40 1.00 4.47 37.40 26.54 16.44 0.00 46.98 55.70 4.54 4.50 4.30 20.40 14.20 7.60 38.20 46.40 7.90 1.90 2.70 4.60 19.90 13.50 3.70 25.80 40.60 1.40 11.30 4.20 25.40 12.20 2.80 48.20 7.80 6.80 0.00 3.90 18.40 12.70 2.50 29.80 4.20 3.80 23.60 13.30 2.90 0.00 7.00 0.00 0.00 1.18 32.48 22.05 14.32 0.00 18.12 15.20 18.40 1.04 36.12 23.45 15.96 0.00 17.49 18.80 25.60 0.58 30.56 19.85 12.86 0.00 18.01 16.40 21.20 0.00 11.92 1.12 24.12 9.84 5.80 8.60 8.30 6.74 15.24 17.26 11.30 0.00 18.20 20.40 4.42 0.00 0.31 29.98 18.91 0.00 29.80 4.60 4.06 2.02 18.74 19.04 12.60 0.00 6.20 5.80 0.00 0.00 0.00 0.00 0.00 0.00 2.56 38.74 22.04 16.47 0.00 23.15 17.40 23.60 1.48 33.92 24.46 11.59 0.00 16.72 14.80 19.20 1.85 37.30 25.08 13.95 0.00 18.38 15.60 21.60 2.17 29.77 24.45 14.36 0.00 21.79 16.20 22.40 0.42 23.16 16.24 6.63 0.00 0.00 11.69 0.00 1.24 39.13 20.04 14.91 28.93 22.67 18.40 22.40 0.91 29.28 12.08 10.08 0.00 16.45 4.40 14.20 1.97 43.50 23.84 16.30 27.64 27.98 19.80 26.20 4.49 44.50 22.34 19.40 30.73 21.30 53.80 64.60 4.51 39.72 23.58 18.95 28.58 17.80 52.00 61.00 3.24 22.37 18.28 11.37 0.00 2.20 7.50 1.60 2.60 42.55 22.76 13.85 22.33 13.10 28.80 41.20 2.65 23.70 19.78 11.57 12.39 2.10 12.70 10.67 2.36 35.10 21.86 10.39 10.56 2.60 16.20 10.40 0.00 0.00 0.56 1.40 1.33 4.26 13.45 0.00 0.00 0.00 38.07 11.50 0.00 0.00 0.00 0.00 0.00 3.49 56.19 12.28 15.13 0.00 0.00 0.00 66.93 52.79 4.24 22.80 15.02 12.47 0.00 13.49 4.79 57.37 27.02 16.48 29.30 16.98 54.31 32.04 0.00 7.56 0.00 On Path to Multimodal Generalist: General-Level and General-Bench Table 43: Results on Image Comprehension Group, from #I-C-11 to #I-C-14."
        },
        {
            "title": "Model",
            "content": "#I-C-11 (Img Depth Est) #I-C-12 (Img Inst Seg) #I-C-13 (Img OCR) #I-C-14 (Img Recog) #1 #1 #2 #1 #2 #3 #4 #1 #"
        },
        {
            "title": "SoTA Specialist",
            "content": "36.40 66.50 63.80 45.03 95.14 10. 24.70 29.80 88.00 GPT4V GPT4o GPT4o-mini GPT-4o-4096 ChatGPT-4o-latest Claude-3.5-Sonnet Claude-3.5-Opus Emu2-32B DetGPT InternVL2.5-8B InternVL2.5-4B InternVL2.5-2B Monkey-10B-chat DeepSeek-VL-7B-Chat Qwen2-VL-7B Qwen-VL-Chat MoE-LLAVA-Phi2-2.7B-4e-384 mPLUG-Owl2-LLaMA2-7b Phi-3.5-Vision-Instruct Cambrian-1-8B MiniGPT4-LLaMA2-7B InternVL-Chat-V1-5 Mini-InternVL-Chat-4B-V1-5 InternLM-XComposer2-VL-1.8B GPT4RoI GLaMM LLaVA-NeXT-13B LLaVA-NeXT-34B Pixtral 12B SEED-LLaMA-13B BLIP2 MiniMonkey DeepSeek-VL-7B LISA CogVLM-Chat ShareGPT4V-7B ShareGPT4V-13B BLIP-3 (XGen-MM) AnyGPT MiniCPM3-4B LaVIT-V2 (7B) GLM-VL-Chat Gemini-1.5-Pro Gemini-1.5-Flash OMG-LLaVA-InternLM20B Idefics3-8B-Llama3 NExT-GPT-V1.5 Vitron-V1 Otter Show-o NExT-Chat Yi-vision-v2 Qwen2-VL-72B 89.71 94.07 85.10 94.22 93.36 89.06 87.69 39.75 42.13 27.46 24.31 27.39 0.00 22.98 16.70 20.10 22.28 18.89 41.67 16.77 22.50 42.60 27.80 22.10 0.00 0.00 62.38 69.89 42.34 22.03 23.81 0.00 61.82 0.00 72.69 54.21 59.24 68.57 0.00 71.96 43.16 74.28 83.56 73.48 18.91 43.02 22.56 24.51 11.33 0.00 76.75 83.57 26. 35.68 97.36 96.56 96.67 94.57 75.64 71.88 61.39 64.21 43.70 41.53 31.06 0.00 45.90 90.78 91.52 93.10 84.25 92.60 84.19 83.35 92.50 61.30 12.90 0.00 0.00 72.34 78.05 64.34 32.57 44.38 0.00 47.09 0.00 86.53 69.44 72.31 82.47 31.14 77.08 57.98 61.42 89.73 82.81 14.63 62.25 56.17 63.01 47.72 0.00 71.32 90.28 95.27 60.45 74.01 73.12 74.45 73.99 68.20 64.89 49.12 56.19 35.03 29.27 43.47 0.00 14.11 24.72 19.63 20.15 12.90 26.76 20.80 16.54 40.00 38.80 31.70 0.00 0.00 62.57 70.08 56.05 47.08 46.92 0.00 32.15 0.00 70.89 56.47 60.36 63.52 19.76 71.85 47.36 64.27 70.15 64.58 8.92 59.32 45.68 70.34 17.00 0.00 61.04 55.23 60.79 18.30 19.46 17.42 0.00 18.43 17.55 16.36 7.60 3.10 0.54 0.00 0.00 0.00 0.27 4.41 4.06 0.40 0.00 2.97 0.00 0.31 0.00 0.00 0.00 0.00 0.00 6.40 8.20 4.80 0.00 3.60 0.00 0.35 0.00 8.35 6.72 7.55 7.24 0.00 9.80 0.00 8.40 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 32.50 27.83 23.83 34.00 30.23 28.04 26.57 24.40 13.17 24.17 14.83 12.83 37.50 33.33 30.47 33.50 33.50 33.83 32.67 14.00 39.50 69.30 59.80 31.00 22.60 0.00 27.33 28.67 24.33 16.00 38.00 38.00 37.50 0.00 27.00 21.50 22.83 25.67 6.67 29.50 15.17 25.17 25.83 23.67 2.00 20.50 13.65 16.37 0.00 25.17 17.41 35.67 45.16 94.53 95.24 91.32 32.20 93.23 93.26 89.73 0.00 0.00 11.31 12.62 9.34 1.15 4.38 11.98 9.00 6.24 3.50 10.87 3.81 4.37 25.10 18.50 15.50 14.10 0.00 7.30 14.40 5.80 0.00 0.00 2.84 5.67 0.00 38.20 27.40 32.80 35.60 14.80 73.60 32.40 69.00 54.36 42.17 8.95 16.57 0.00 0.00 0.00 0.00 0.00 16.83 13.72 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 42.09 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 39.50 0.00 0.00 68.70 0.00 0.00 0.00 0.00 0. 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 92.75 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 68.22 0.00 0.00 64.50 0.00 0.00 0.00 0.00 0.00 242 On Path to Multimodal Generalist: General-Level and General-Bench Table 44: Results on Image Comprehension Group, from #I-C-15 to #I-C-16."
        },
        {
            "title": "Model",
            "content": "#I-C-15 (Img Sem Seg) #I-C-16 (Img Vis Grnd) #1 #2 #3 #4 #5 #6 #7 #8 #1 #"
        },
        {
            "title": "SoTA Specialist",
            "content": "19.80 50.40 88.00 98.70 83.65 59. 47.74 62.15 84.76 90.91 GPT4V GPT4o GPT4o-mini GPT-4o-4096 ChatGPT-4o-latest Claude-3.5-Sonnet Claude-3.5-Opus Emu2-32B DetGPT InternVL2.5-8B InternVL2.5-4B InternVL2.5-2B Monkey-10B-chat DeepSeek-VL-7B-Chat Qwen2-VL-7B Qwen-VL-Chat MoE-LLAVA-Phi2-2.7B-4e-384 mPLUG-Owl2-LLaMA2-7b Phi-3.5-Vision-Instruct Cambrian-1-8B MiniGPT4-LLaMA2-7B InternVL-Chat-V1-5 Mini-InternVL-Chat-4B-V1-5 InternLM-XComposer2-VL-1.8B GPT4RoI GLaMM LLaVA-NeXT-13B LLaVA-NeXT-34B Pixtral 12B SEED-LLaMA-13B BLIP2 MiniMonkey DeepSeek-VL-7B LISA CogVLM-Chat ShareGPT4V-7B ShareGPT4V-13B BLIP-3 (XGen-MM) AnyGPT MiniCPM3-4B LaVIT-V2 (7B) GLM-VL-Chat Gemini-1.5-Pro Gemini-1.5-Flash OMG-LLaVA-InternLM20B Idefics3-8B-Llama3 NExT-GPT-V1.5 Vitron-V1 Otter Show-o NExT-Chat Yi-vision-v2 Qwen2-VL-72B 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 42.10 0.00 0.00 0.00 0.00 0.00 0.00 0.00 9.35 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 49.65 0.00 0.00 6.40 0.00 0.00 0.00 0.00 0. 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 90.28 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 42.57 0.00 0.00 52.40 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 33.90 0.00 0.00 0.00 0.00 0.00 0.00 0.00 44.45 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 67.73 0.00 0.00 81.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 83.60 0.00 0.00 0.00 0.00 0.00 0.00 0.00 22.45 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 20.15 0.00 0.00 89.70 0.00 0.00 0.00 0.00 0.00 243 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 88.74 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 79.05 0.00 0.00 78.40 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 88.65 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 40.26 0.00 0.00 38.90 0.00 0.00 0.00 0.00 0. 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 57.40 0.00 0.00 0.00 0.00 0.00 0.00 0.00 39.46 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 44.80 0.00 0.00 56.70 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 64.20 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 56.49 0.00 0.00 64.70 0.00 0.00 0.00 0.00 0.00 67.69 76.20 57.18 80.60 68.30 66.50 62.40 0.00 48.48 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 78.70 0.00 0.00 0.00 0.00 0.00 74.11 82.56 59.44 83.33 70.70 71.66 70.73 0.00 82.23 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 72.70 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 86.73 0.00 0.00 0.00 0.00 0.00 On Path to Multimodal Generalist: General-Level and General-Bench Table 45: Results on Image Comprehension Group,#I-C-17, part A."
        },
        {
            "title": "Model",
            "content": "#I-C-17 (Img VQA) #1 #2 #3 #4 #5 #6 #7 #8 #9 #"
        },
        {
            "title": "SoTA Specialist",
            "content": "14.36 36.40 17.32 43.12 89.38 17. 34.30 80.75 92.72 78.30 GPT4V GPT4o GPT4o-mini GPT-4o-4096 ChatGPT-4o-latest Claude-3.5-Sonnet Claude-3.5-Opus Emu2-32B DetGPT InternVL2.5-8B InternVL2.5-4B InternVL2.5-2B Monkey-10B-chat DeepSeek-VL-7B-Chat Qwen2-VL-7B Qwen-VL-Chat MoE-LLAVA-Phi2-2.7B-4e-384 mPLUG-Owl2-LLaMA2-7b Phi-3.5-Vision-Instruct Cambrian-1-8B MiniGPT4-LLaMA2-7B InternVL-Chat-V1-5 Mini-InternVL-Chat-4B-V1-5 InternLM-XComposer2-VL-1.8B GPT4RoI GLaMM LLaVA-NeXT-13B LLaVA-NeXT-34B Pixtral 12B SEED-LLaMA-13B BLIP2 MiniMonkey DeepSeek-VL-7B LISA CogVLM-Chat ShareGPT4V-7B ShareGPT4V-13B BLIP-3 (XGen-MM) AnyGPT MiniCPM3-4B LaVIT-V2 (7B) GLM-VL-Chat Gemini-1.5-Pro Gemini-1.5-Flash OMG-LLaVA-InternLM20B Idefics3-8B-Llama3 NExT-GPT-V1.5 Vitron-V1 Otter Show-o NExT-Chat Yi-vision-v2 Qwen2-VL-72B 16.20 16.00 11.45 24.40 16.82 13.98 13.40 10.18 9.36 3.45 10.64 29.09 6.91 3.45 54.91 57.99 40.00 38.00 40.18 5.82 34.23 11.20 9.80 6.70 4.00 0.00 14.55 15.09 12.00 10.55 9.45 6.01 12.78 0.00 11.45 7.82 9.09 9.45 0.00 12.18 9.45 12.73 14.18 13.27 0.00 12.91 3.40 5.60 0.00 14.72 7.41 14.00 69. 26.54 23.82 17.45 22.73 19.64 21.73 20.90 34.55 29.09 47.27 37.82 40.55 3.60 100.00 65.45 52.90 54.00 46.73 60.00 1.45 38.26 23.90 16.70 15.60 7.40 0.00 33.30 36.80 23.50 26.70 46.00 25.45 31.25 0.00 37.09 25.82 30.36 35.64 6.36 31.45 20.36 37.64 18.73 17.27 2.55 16.73 28.90 31.50 0.00 39.24 34.83 41.64 51.79 41.04 31.41 29.09 43.16 51.64 33.44 28.99 38.54 28.90 39.31 40.66 39.69 8.11 0.00 37.34 33.26 31.44 33.80 35.90 28.70 30.06 47.90 44.30 40.50 52.80 0.00 40.46 48.17 42.78 31.21 38.84 37.80 35.63 0.00 45.28 39.11 43.35 42.58 29.11 50.47 42.91 51.98 52.41 42.77 2.12 34.30 30.50 27.30 0.00 0.00 0.00 0.00 0.00 52.84 50.31 47.24 53.78 49.20 49.57 47.42 33.25 21.19 31.54 34.16 38.97 45.64 0.22 54.32 38.96 33.36 15.67 41.60 29.00 46.83 6.40 4.50 3.20 0.00 0.00 36.18 40.25 33.85 28.96 27.64 39.31 32.98 0.00 36.43 27.95 30.44 28.57 17.26 49.66 29.97 41.20 46.72 43.80 22.16 36.46 31.54 34.78 7.34 33.47 41.67 53.94 55.10 244 80.38 85.75 82.01 89.02 83.88 81.97 77.92 65.42 55.84 55.74 27.02 36.48 14.92 33.78 26.87 67.99 67.29 53.27 78.04 32.94 55.03 62.90 59.20 36.80 36.00 0.00 80.14 78.84 71.96 62.85 25.46 66.59 58.18 0.00 81.31 71.03 76.40 73.36 40.19 79.91 51.87 84.11 86.68 85.98 4.91 67.99 47.60 51.40 14.10 0.00 38.86 83.41 0.00 45.74 69.03 44.03 74.15 69.32 52.38 48.74 35.51 28.41 60.00 46.57 49.14 66.29 6.57 0.00 5.71 7.80 8.12 31.14 0.00 32.43 22.40 16.32 10.60 5.60 0.00 41.76 52.27 44.03 37.78 29.26 45.92 57.10 0.00 52.84 42.33 46.88 49.72 21.02 49.15 37.50 57.67 59.94 54.83 1.42 44.03 34.96 31.84 17.29 0.85 11.63 45.63 50. 37.00 37.00 34.00 40.00 36.98 35.48 34.46 31.67 25.00 70.67 38.33 84.00 55.11 66.67 34.00 33.33 33.67 39.00 42.33 2.00 28.19 34.60 11.33 33.30 23.30 0.00 30.33 33.67 31.00 28.67 29.00 33.33 41.33 0.00 35.33 31.33 33.67 32.00 3.67 31.67 15.00 34.30 36.67 34.00 1.67 28.67 13.40 20.70 9.67 34.33 43.29 37.00 47.33 57.33 53.00 33.33 52.00 63.33 47.78 43.76 43.64 34.55 83.64 86.36 88.18 85.67 53.10 61.82 54.54 60.28 37.27 81.82 28.18 32.11 58.30 51.70 47.90 67.80 0.00 34.55 40.00 32.73 28.18 22.73 84.54 51.82 0.00 47.27 34.55 41.82 44.55 12.73 46.37 28.19 50.91 83.64 82.73 2.73 57.27 24.63 18.90 0.00 1.82 41.67 83.64 59.00 87.54 96.10 93.10 96.63 84.81 91.38 90.51 65.32 63.97 98.65 97.31 93.10 72.05 58.94 54.04 71.54 66.16 58.58 94.95 26.26 68.46 68.40 62.90 55.30 46.80 0.00 83.83 87.88 74.75 65.99 59.26 93.94 56.06 0.00 81.82 75.93 79.97 77.95 35.52 89.23 64.98 86.03 94.44 90.91 2.19 82.15 63.45 67.82 0.00 0.51 40.37 96.13 73.82 43.33 51.00 37.67 48.67 41.67 43.34 42.52 33.67 28.33 43.67 44.33 38.00 26.00 24.44 29.30 18.66 14.67 13.33 32.67 0.33 38.93 26.60 20.00 23.90 68.00 0.00 37.33 43.67 36.00 32.33 29.33 36.00 23.67 0.00 44.67 35.67 38.67 41.00 3.67 40.33 32.67 44.33 48.33 44.67 2.67 38.33 29.40 24.70 0.00 0.33 37.93 35.67 49.42 On Path to Multimodal Generalist: General-Level and General-Bench Table 46: Results on Image Comprehension Group,#I-C-17, part B."
        },
        {
            "title": "Model",
            "content": "#I-C-17 (Img VQA) #11 #12 #13 #14 #15 #16 #17 #18 #19 #"
        },
        {
            "title": "SoTA Specialist",
            "content": "49.80 65.40 66.50 74.70 73.58 24. 71.60 70.70 51.60 71.40 GPT4V GPT4o GPT4o-mini GPT-4o-4096 ChatGPT-4o-latest Claude-3.5-Sonnet Claude-3.5-Opus Emu2-32B DetGPT InternVL2.5-8B InternVL2.5-4B InternVL2.5-2B Monkey-10B-chat DeepSeek-VL-7B-Chat Qwen2-VL-7B Qwen-VL-Chat MoE-LLAVA-Phi2-2.7B-4e-384 mPLUG-Owl2-LLaMA2-7b Phi-3.5-Vision-Instruct Cambrian-1-8B MiniGPT4-LLaMA2-7B InternVL-Chat-V1-5 Mini-InternVL-Chat-4B-V1-5 InternLM-XComposer2-VL-1.8B GPT4RoI GLaMM LLaVA-NeXT-13B LLaVA-NeXT-34B Pixtral 12B SEED-LLaMA-13B BLIP2 MiniMonkey DeepSeek-VL-7B LISA CogVLM-Chat ShareGPT4V-7B ShareGPT4V-13B BLIP-3 (XGen-MM) AnyGPT MiniCPM3-4B LaVIT-V2 (7B) GLM-VL-Chat Gemini-1.5-Pro Gemini-1.5-Flash OMG-LLaVA-InternLM20B Idefics3-8B-Llama3 NExT-GPT-V1.5 Vitron-V1 Otter Show-o NExT-Chat Yi-vision-v2 Qwen2-VL-72B 0.00 0.00 0.00 0.00 0.00 0.00 0.00 53.67 34.33 72.00 47.67 99.33 50.00 50.00 54.28 52.33 51.67 7.66 50.00 49.70 79.87 54.30 42.00 51.00 47.60 0.00 54.33 48.67 58.67 30.33 33.67 50.00 48.33 0.00 55.67 47.67 45.67 46.00 36.33 52.33 36.60 55.33 60.33 58.33 1.67 57.00 38.50 41.60 0.00 50.00 39.81 31.33 40. 50.00 68.10 56.90 64.66 62.07 57.81 54.04 47.41 37.07 13.79 42.24 16.09 43.10 47.90 64.35 42.24 35.34 33.62 55.17 34.48 43.48 59.40 40.51 33.60 47.40 0.00 52.59 65.52 45.69 41.38 28.45 56.03 46.55 0.00 57.76 42.24 49.14 54.31 12.07 56.03 35.34 66.38 65.52 63.79 3.45 46.55 28.40 32.70 6.78 1.72 44.79 68.10 1.72 81.82 72.72 77.27 81.82 77.27 76.59 72.86 54.55 31.82 72.73 77.27 77.27 54.55 52.27 22.73 22.72 31.82 9.09 31.81 13.64 33.33 68.10 22.70 54.50 68.10 0.00 63.64 72.73 36.37 45.45 40.91 59.09 50.00 0.00 77.27 59.09 72.72 63.64 18.18 63.64 40.91 77.27 77.27 77.27 9.09 63.64 36.80 34.60 29.20 0.00 50.00 22.73 60.95 75.70 79.44 71.96 83.18 79.44 75.57 72.74 65.42 56.07 69.16 75.70 60.75 57.94 41.55 37.38 57.94 50.47 31.78 70.09 38.32 37.74 61.60 50.46 34.50 50.40 0.00 67.29 73.83 63.55 42.06 43.93 61.68 38.32 0.00 66.36 52.34 57.94 58.88 13.08 57.94 36.45 71.96 76.64 73.83 4.67 70.09 34.10 40.70 25.10 1.87 35.29 71.96 52.55 245 76.39 78.17 69.27 79.29 75.72 74.35 71.10 66.82 54.34 87.31 95.09 83.96 54.34 56.94 30.73 56.79 43.88 32.52 81.29 24.72 57.05 79.40 74.50 78.30 44.50 0.00 62.36 70.82 67.48 57.46 52.78 77.28 56.79 0.00 70.60 57.24 61.47 67.04 27.39 73.72 57.91 66.59 70.16 68.82 2.45 71.27 64.27 63.45 33.10 2.00 38.46 89.53 58.22 34.25 45.00 28.00 45.25 45.00 35.54 32.88 27.50 25.50 41.50 27.37 62.50 54.67 25.00 34.00 18.25 25.00 27.21 24.00 25.00 0.00 23.00 21.20 9.30 31.00 0.00 28.75 32.50 30.25 18.75 19.00 29.75 24.50 0.00 33.50 28.50 32.00 31.75 6.00 33.75 19.00 30.50 40.00 33.75 2.00 36.00 20.80 26.50 11.50 0.25 21.69 28.75 18. 58.70 66.30 56.52 54.35 57.61 60.07 59.55 55.43 43.48 36.96 71.20 41.30 57.61 51.83 84.78 58.69 59.78 64.13 63.04 50.00 48.35 76.00 57.60 58.60 28.20 0.00 51.09 61.96 57.61 28.26 27.17 75.00 52.17 0.00 55.43 46.74 51.09 57.61 14.13 66.30 26.09 59.78 61.96 56.52 3.26 64.13 31.57 34.86 18.21 28.26 59.78 35.87 68.93 66.00 68.33 69.67 68.33 70.33 67.93 64.81 54.00 47.33 43.66 67.72 45.77 49.30 38.25 21.75 70.87 38.84 33.63 53.33 42.30 46.53 84.20 36.84 76.30 64.40 0.00 54.67 62.33 59.67 52.33 52.67 76.32 37.33 0.00 62.00 53.67 55.67 61.33 21.33 64.00 49.00 59.67 67.00 63.33 2.33 63.33 40.37 45.21 37.50 62.99 43.64 61.54 69.58 47.33 12.70 12.60 50.33 47.00 24.13 19.68 29.67 27.33 50.67 41.22 0.00 49.33 99.33 54.32 40.33 44.17 38.90 0.00 0.00 93.96 35.60 15.33 9.30 62.30 0.00 9.33 12.67 8.33 6.67 6.33 65.00 57.82 0.00 22.33 15.33 17.33 19.00 0.00 9.67 7.67 9.00 41.00 33.67 2.67 32.00 3.58 5.45 0.00 50.00 11.48 45.33 51.69 70.40 72.20 63.20 75.60 75.20 50.60 53.73 32.80 34.40 53.85 86.04 87.04 81.60 83.43 72.00 39.60 31.20 16.60 45.80 39.41 43.06 54.20 43.60 52.40 0.00 0.00 45.00 50.60 39.40 27.40 47.20 69.20 75.60 0.00 52.20 44.80 46.20 50.60 20.40 54.00 26.80 56.20 70.40 66.80 3.20 61.20 0.00 0.00 0.00 1.20 37.45 50.23 70.37 On Path to Multimodal Generalist: General-Level and General-Bench Table 47: Results on Image Comprehension Group,#I-C-17, part C."
        },
        {
            "title": "Model",
            "content": "#I-C-17 (Img VQA) #21 #22 #23 #24 #25 #26 #27 #28 #29 #"
        },
        {
            "title": "SoTA Specialist",
            "content": "70.60 59.30 61.90 61.40 62.80 62. 70.20 83.80 82.40 68.60 GPT4V GPT4o GPT4o-mini GPT-4o-4096 ChatGPT-4o-latest Claude-3.5-Sonnet Claude-3.5-Opus Emu2-32B DetGPT InternVL2.5-8B InternVL2.5-4B InternVL2.5-2B Monkey-10B-chat DeepSeek-VL-7B-Chat Qwen2-VL-7B Qwen-VL-Chat MoE-LLAVA-Phi2-2.7B-4e-384 mPLUG-Owl2-LLaMA2-7b Phi-3.5-Vision-Instruct Cambrian-1-8B MiniGPT4-LLaMA2-7B InternVL-Chat-V1-5 Mini-InternVL-Chat-4B-V1-5 InternLM-XComposer2-VL-1.8B GPT4RoI GLaMM LLaVA-NeXT-13B LLaVA-NeXT-34B Pixtral 12B SEED-LLaMA-13B BLIP2 MiniMonkey DeepSeek-VL-7B LISA CogVLM-Chat ShareGPT4V-7B ShareGPT4V-13B BLIP-3 (XGen-MM) AnyGPT MiniCPM3-4B LaVIT-V2 (7B) GLM-VL-Chat Gemini-1.5-Pro Gemini-1.5-Flash OMG-LLaVA-InternLM20B Idefics3-8B-Llama3 NExT-GPT-V1.5 Vitron-V1 Otter Show-o NExT-Chat Yi-vision-v2 Qwen2-VL-72B 68.60 74.00 65.60 77.00 76.00 59.95 58.08 44.80 36.80 100.00 84.82 83.54 80.35 84.58 73.60 37.60 30.60 14.39 44.20 28.73 30.08 52.40 43.00 52.80 0.00 0.00 46.20 52.40 53.40 42.40 47.80 71.60 78.60 0.00 56.60 45.20 48.20 52.00 27.60 57.80 43.20 55.20 64.80 66.00 2.60 59.80 0.00 0.00 0.00 0.40 12.50 48.59 74. 22.81 37.30 26.34 27.95 28.61 39.10 42.24 24.20 21.40 49.75 33.70 27.33 39.10 23.27 75.37 50.65 62.43 44.41 64.36 49.50 44.68 30.30 29.80 46.75 20.80 0.00 35.60 37.80 33.20 21.90 49.20 36.17 21.81 0.00 35.30 27.53 29.23 32.42 19.90 36.70 29.40 38.80 35.90 30.80 21.10 29.40 23.58 27.41 0.00 0.44 20.67 8.54 56.19 24.10 43.45 29.07 52.05 42.66 48.88 47.91 24.60 20.20 58.79 54.69 31.13 65.94 30.41 82.41 58.28 72.02 65.46 70.32 58.83 60.10 33.30 40.10 69.80 31.70 0.00 46.50 46.20 47.90 20.90 47.40 53.55 30.06 0.00 47.60 37.34 41.29 45.82 26.40 50.70 33.70 51.30 41.80 38.70 22.20 38.40 21.60 31.60 0.00 0.42 25.31 13.96 86.06 20.40 40.30 23.70 49.51 40.01 45.29 45.67 30.80 16.90 40.45 35.99 22.83 42.29 18.72 71.96 43.55 58.87 43.98 62.71 50.61 54.65 26.80 26.20 48.90 23.80 0.00 34.60 41.20 38.20 18.40 48.80 39.67 17.29 0.00 40.75 32.93 35.92 38.32 21.70 38.90 24.50 44.70 34.50 28.60 21.30 36.70 22.89 23.05 0.00 0.38 27.53 9.81 68.02 246 22.64 35.47 26.52 43.15 35.43 44.27 42.24 22.40 19.50 55.26 53.03 60.27 57.99 26.98 78.84 52.91 67.55 63.62 72.85 52.37 59.00 36.90 37.60 65.40 23.90 0.00 41.20 37.80 32.40 20.80 49.60 53.34 22.76 0.00 43.57 35.26 38.29 41.87 28.40 48.70 36.70 49.60 39.80 41.40 17.50 36.50 20.65 21.78 0.00 25.00 23.40 16.32 81.24 22.02 39.87 25.01 48.03 37.96 54.41 53.51 36.20 17.40 50.00 33.92 50.00 42.91 19.80 69.40 46.69 57.35 45.78 60.24 43.35 50.26 28.20 24.80 51.00 23.20 0.00 46.60 48.80 51.40 19.20 50.20 39.57 17.44 0.00 49.72 40.95 42.94 46.80 23.60 48.60 31.90 52.40 36.50 33.70 12.40 32.80 18.65 20.78 0.00 28.80 21.35 9.28 71. 18.01 29.60 25.20 43.31 36.81 59.85 62.86 44.60 17.90 57.43 57.38 63.84 47.34 54.66 77.60 52.60 32.63 13.20 73.80 48.00 51.28 73.40 62.00 52.20 37.20 0.00 58.40 61.20 48.60 18.40 43.80 41.64 53.60 0.00 58.40 49.80 52.00 54.60 20.80 63.20 35.20 59.60 75.20 68.40 3.20 62.60 15.48 16.98 0.00 1.40 40.97 60.27 78.17 93.00 93.80 90.50 94.40 89.40 76.11 76.00 71.40 50.60 97.10 79.13 94.11 85.53 60.99 92.10 72.80 42.80 18.60 84.52 0.63 27.80 87.70 80.20 54.20 38.20 0.00 68.80 74.60 64.20 44.20 46.00 63.60 61.80 0.00 65.20 57.80 60.40 62.80 14.80 62.80 42.00 69.80 91.20 89.60 5.40 84.40 0.00 0.00 0.00 45.29 12.88 34.27 90.91 91.40 94.30 89.50 95.60 92.20 82.49 82.54 72.60 54.80 77.46 79.84 68.42 80.15 61.76 94.74 71.00 43.00 23.60 85.27 44.40 46.72 83.30 65.60 52.60 34.80 0.00 72.40 78.60 74.20 52.60 42.80 71.60 60.80 0.00 69.40 61.00 63.60 65.20 8.60 67.60 46.80 69.80 90.00 86.80 4.20 85.20 0.00 0.00 0.00 0.90 11.64 44.59 80.95 71.20 65.00 68.50 69.60 70.60 71.90 73.25 60.40 54.40 59.09 64.82 68.90 44.84 47.51 79.20 53.40 41.80 28.99 57.20 40.00 47.36 70.60 63.00 74.00 28.20 0.00 60.40 65.40 66.20 59.40 29.60 35.80 43.00 0.00 64.80 55.40 57.80 62.40 26.00 62.80 48.40 67.40 63.60 58.40 2.80 62.20 0.00 0.00 0.00 0.80 26.41 67.83 83.57 On Path to Multimodal Generalist: General-Level and General-Bench Table 48: Results on Image Comprehension Group,#I-C-17, part D."
        },
        {
            "title": "Model",
            "content": "#I-C-17 (Img VQA) #31 #32 #33 #34 #35 #36 #37 #38 #39 #"
        },
        {
            "title": "SoTA Specialist",
            "content": "75.00 67.20 68.40 66.80 48.80 56. 75.80 31.00 20.71 29.20 GPT4V GPT4o GPT4o-mini GPT-4o-4096 ChatGPT-4o-latest Claude-3.5-Sonnet Claude-3.5-Opus Emu2-32B DetGPT InternVL2.5-8B InternVL2.5-4B InternVL2.5-2B Monkey-10B-chat DeepSeek-VL-7B-Chat Qwen2-VL-7B Qwen-VL-Chat MoE-LLAVA-Phi2-2.7B-4e-384 mPLUG-Owl2-LLaMA2-7b Phi-3.5-Vision-Instruct Cambrian-1-8B MiniGPT4-LLaMA2-7B InternVL-Chat-V1-5 Mini-InternVL-Chat-4B-V1-5 InternLM-XComposer2-VL-1.8B GPT4RoI GLaMM LLaVA-NeXT-13B LLaVA-NeXT-34B Pixtral 12B SEED-LLaMA-13B BLIP2 MiniMonkey DeepSeek-VL-7B LISA CogVLM-Chat ShareGPT4V-7B ShareGPT4V-13B BLIP-3 (XGen-MM) AnyGPT MiniCPM3-4B LaVIT-V2 (7B) GLM-VL-Chat Gemini-1.5-Pro Gemini-1.5-Flash OMG-LLaVA-InternLM20B Idefics3-8B-Llama3 NExT-GPT-V1.5 Vitron-V1 Otter Show-o NExT-Chat Yi-vision-v2 Qwen2-VL-72B 77.10 74.20 70.50 75.00 73.60 74.44 74.34 66.80 56.40 36.11 69.37 68.93 47.56 50.05 81.80 54.60 41.40 25.60 61.44 64.17 55.68 69.20 65.20 73.40 31.60 0.00 64.20 70.60 65.40 50.20 31.20 37.80 50.80 0.00 68.40 62.80 66.20 64.20 42.20 65.00 54.80 70.80 65.20 61.00 3.40 56.40 0.00 0.00 0.00 0.80 27.38 67.36 88. 70.40 71.80 35.80 76.60 74.30 67.53 68.98 59.00 52.00 52.05 40.74 49.99 57.20 51.09 13.33 3.20 6.00 30.20 18.40 9.70 9.80 55.00 44.00 64.60 10.60 0.00 58.60 62.80 60.40 44.40 11.40 46.80 47.60 0.00 63.60 54.20 58.40 61.20 14.60 66.80 42.60 65.20 62.00 53.60 2.00 54.20 20.60 11.40 0.00 0.90 18.52 7.22 53.39 82.80 83.70 80.60 83.00 82.40 67.13 66.84 62.60 44.60 33.71 35.92 64.26 33.94 28.06 93.33 61.00 59.10 14.80 44.90 49.30 43.70 71.00 61.20 72.20 28.00 0.00 52.40 60.40 61.20 40.80 40.40 49.60 32.00 0.00 59.40 52.20 55.80 57.60 24.80 61.20 40.60 63.80 74.40 72.00 3.20 63.00 28.60 39.40 0.00 0.00 36.80 41.31 87.76 73.90 72.10 39.00 77.40 76.60 71.63 70.97 67.20 38.20 50.74 40.60 46.86 59.60 56.04 41.91 22.00 13.60 5.20 40.67 11.00 18.70 55.20 46.60 62.60 8.20 0.00 59.60 65.60 57.60 35.80 10.80 57.40 47.20 0.00 64.40 53.20 56.80 62.00 6.40 62.60 46.80 63.40 67.80 65.60 2.60 57.60 0.00 0.00 0.00 7.20 17.44 17.04 57.18 247 89.40 92.40 77.60 91.00 92.00 86.20 82.41 71.40 54.60 15.54 49.00 3.18 50.40 3.50 91.06 47.00 51.20 54.40 48.97 43.98 0.67 74.00 61.40 66.60 53.60 0.00 62.40 70.60 68.20 49.20 52.60 48.20 51.60 0.00 69.20 63.20 65.80 67.40 0.00 64.20 50.80 68.60 82.60 77.40 7.80 74.40 26.50 27.80 49.89 5.40 47.44 46.00 96.32 98.20 91.80 63.40 87.00 63.00 84.12 84.20 74.60 60.60 18.54 30.41 1.74 51.40 6.50 86.67 50.40 50.80 51.60 62.46 59.47 48.32 76.40 65.00 49.00 50.40 0.00 64.40 72.20 68.20 58.80 61.80 53.60 49.60 0.00 67.80 59.60 62.60 64.40 34.00 72.60 57.20 78.40 89.60 84.00 8.40 76.20 36.90 59.90 0.00 5.80 51.32 65.80 91. 14.80 27.20 10.60 25.80 18.20 16.89 16.78 22.78 18.64 12.40 49.21 8.25 50.20 11.57 45.80 45.80 50.00 44.40 49.03 43.70 91.28 60.20 50.80 48.80 50.00 0.00 30.20 42.40 18.60 11.40 59.40 61.80 13.58 0.00 38.40 31.20 33.40 35.60 16.40 42.80 34.60 44.00 74.00 68.80 7.20 63.40 51.40 60.70 0.00 0.00 58.42 49.80 56.41 19.60 30.20 7.80 47.51 34.60 18.24 18.21 14.60 10.20 6.90 6.45 5.74 13.37 7.93 48.60 15.40 37.00 33.60 20.00 29.20 58.39 27.60 24.20 20.00 11.80 0.00 12.20 16.40 17.20 11.60 6.80 28.33 12.40 0.00 16.20 13.40 14.60 15.80 3.40 16.80 11.80 17.40 27.20 20.80 0.00 17.60 12.60 9.40 0.00 0.00 49.85 48.60 15.06 16.83 18.04 15.23 15.83 18.24 16.43 16.40 15.43 8.22 2.21 1.80 3.55 2.00 1.93 45.36 17.89 20.10 0.00 32.06 4.30 18.92 0.00 0.00 1.60 0.00 0.00 12.02 15.63 13.63 8.62 1.40 0.00 1.60 0.00 12.83 9.82 11.22 13.83 5.81 12.42 8.22 15.83 17.23 14.63 0.00 11.22 2.70 1.80 0.00 0.00 47.33 19.70 48.21 18.30 19.20 18.70 0.00 19.72 18.62 15.56 14.85 13.83 3.17 3.39 4.59 2.32 4.11 20.89 5.07 12.57 9.11 18.80 16.52 4.13 3.00 2.40 5.80 14.60 0.00 16.42 17.93 15.88 11.73 2.10 3.41 3.73 0.00 22.14 18.29 20.56 19.47 0.00 19.74 13.07 26.34 18.10 16.30 10.10 9.70 14.60 13.47 0.00 0.00 11.71 2.35 20.49 On Path to Multimodal Generalist: General-Level and General-Bench Table 49: Results on Image Comprehension Group,#I-C-18, part A."
        },
        {
            "title": "Model",
            "content": "#I-C-18 (Ind-Anomaly Det) #1 #2 #3 #4 #5 #6 #7 #8 #9 #"
        },
        {
            "title": "SoTA Specialist",
            "content": "90.30 60.40 56.30 72.80 61.50 88. 75.60 93.50 83.10 58.70 GPT4V GPT4o GPT4o-mini GPT-4o-4096 ChatGPT-4o-latest Claude-3.5-Sonnet Claude-3.5-Opus Emu2-32B DetGPT InternVL2.5-8B InternVL2.5-4B InternVL2.5-2B Monkey-10B-chat DeepSeek-VL-7B-Chat Qwen2-VL-7B Qwen-VL-Chat MoE-LLAVA-Phi2-2.7B-4e-384 mPLUG-Owl2-LLaMA2-7b Phi-3.5-Vision-Instruct Cambrian-1-8B MiniGPT4-LLaMA2-7B InternVL-Chat-V1-5 Mini-InternVL-Chat-4B-V1-5 InternLM-XComposer2-VL-1.8B GPT4RoI GLaMM LLaVA-NeXT-13B LLaVA-NeXT-34B Pixtral 12B SEED-LLaMA-13B BLIP2 MiniMonkey DeepSeek-VL-7B LISA CogVLM-Chat ShareGPT4V-7B ShareGPT4V-13B BLIP-3 (XGen-MM) AnyGPT MiniCPM3-4B LaVIT-V2 (7B) GLM-VL-Chat Gemini-1.5-Pro Gemini-1.5-Flash OMG-LLaVA-InternLM20B Idefics3-8B-Llama3 NExT-GPT-V1.5 Vitron-V1 Otter Show-o NExT-Chat Yi-vision-v2 Qwen2-VL-72B 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 60.50 0.00 0.00 0.00 0.00 0. 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 26.40 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 36.50 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 13.50 0.00 0.00 0.00 0.00 0.00 248 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 25.30 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 10.70 0.00 0.00 0.00 0.00 0. 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 24.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 33.50 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 15.70 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 13.40 0.00 0.00 0.00 0.00 0.00 On Path to Multimodal Generalist: General-Level and General-Bench Table 50: Results on Image Comprehension Group,#I-C-18, part B."
        },
        {
            "title": "Model",
            "content": "#I-C-18 (Ind-Anomaly Det) #11 #12 #13 #14 #15 #16 #17 #18 #"
        },
        {
            "title": "SoTA Specialist",
            "content": "90.00 75.20 64.80 81.90 85.20 62. 76.20 23.10 73.60 GPT4V GPT4o GPT4o-mini GPT-4o-4096 ChatGPT-4o-latest Claude-3.5-Sonnet Claude-3.5-Opus Emu2-32B DetGPT InternVL2.5-8B InternVL2.5-4B InternVL2.5-2B Monkey-10B-chat DeepSeek-VL-7B-Chat Qwen2-VL-7B Qwen-VL-Chat MoE-LLAVA-Phi2-2.7B-4e-384 mPLUG-Owl2-LLaMA2-7b Phi-3.5-Vision-Instruct Cambrian-1-8B MiniGPT4-LLaMA2-7B InternVL-Chat-V1-5 Mini-InternVL-Chat-4B-V1-5 InternLM-XComposer2-VL-1.8B GPT4RoI GLaMM LLaVA-NeXT-13B LLaVA-NeXT-34B Pixtral 12B SEED-LLaMA-13B BLIP2 MiniMonkey DeepSeek-VL-7B LISA CogVLM-Chat ShareGPT4V-7B ShareGPT4V-13B BLIP-3 (XGen-MM) AnyGPT MiniCPM3-4B LaVIT-V2 (7B) GLM-VL-Chat Gemini-1.5-Pro Gemini-1.5-Flash OMG-LLaVA-InternLM20B Idefics3-8B-Llama3 NExT-GPT-V1.5 Vitron-V1 Otter Show-o NExT-Chat Yi-vision-v2 Qwen2-VL-72B 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 14.80 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 7.80 0.00 0.00 0.00 0.00 0. 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 3.40 0.00 0.00 0.00 0.00 0.00 249 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 23.70 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 19.60 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 35.13 26.38 37.00 35.38 34.13 34.50 36.75 41.12 26.75 25.00 28.49 34.75 22.75 15.63 65.10 39.80 36.50 70.50 0.00 41.00 38.89 34.63 26.13 17.23 26.41 31.57 0.00 43.25 31.13 33.25 36.88 17.75 36.75 25.38 40.25 0.00 0.00 0.00 38.75 23.67 24.89 0.00 0.00 22.72 26.63 40.77 0.00 0.00 0.00 0.00 0.00 0.00 0.00 45.12 35.36 25.00 50.00 20.71 71.43 28.57 60.71 71.42 71.07 56.79 73.93 60.71 70.00 79.60 65.70 71.40 70.30 0.00 39.29 49.29 45.36 37.14 47.14 7.86 69.31 0.00 55.71 43.93 48.57 53.93 37.50 54.64 43.57 57.50 0.00 0.00 0.00 51.79 38.41 42.69 0.00 0.00 32.56 53.93 71. 0.00 0.00 0.00 0.00 0.00 0.00 0.00 20.02 18.24 28.69 40.44 41.43 40.03 45.02 68.33 58.16 42.63 24.30 60.96 10.96 51.00 76.40 69.30 56.90 41.60 0.00 39.84 44.22 46.22 27.09 33.27 23.71 47.26 0.00 48.41 39.44 41.24 41.04 17.53 37.25 25.30 45.42 0.00 0.00 0.00 40.04 22.30 26.10 0.00 0.00 39.25 53.98 49.46 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 62.50 16.25 45.00 0.00 41.43 35.00 5.00 25.00 68.75 56.25 37.50 57.50 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 3.85 0.00 45.16 On Path to Multimodal Generalist: General-Level and General-Bench Table 51: Results on Image Comprehension Group, from #I-C-19 to #I-C-20."
        },
        {
            "title": "Model",
            "content": "#I-C-19 (Med Seg) #I-C-20 (Keypoint Det) #1 #2 #3 #4 #5 #6 #"
        },
        {
            "title": "SoTA Specialist",
            "content": "50.10 16.30 25.85 23.55 53.40 37. 95.70 GPT4V GPT4o GPT4o-mini GPT-4o-4096 ChatGPT-4o-latest Claude-3.5-Sonnet Claude-3.5-Opus Emu2-32B DetGPT InternVL2.5-8B InternVL2.5-4B InternVL2.5-2B Monkey-10B-chat DeepSeek-VL-7B-Chat Qwen2-VL-7B Qwen-VL-Chat MoE-LLAVA-Phi2-2.7B-4e-384 mPLUG-Owl2-LLaMA2-7b Phi-3.5-Vision-Instruct Cambrian-1-8B MiniGPT4-LLaMA2-7B InternVL-Chat-V1-5 Mini-InternVL-Chat-4B-V1-5 InternLM-XComposer2-VL-1.8B GPT4RoI GLaMM LLaVA-NeXT-13B LLaVA-NeXT-34B Pixtral 12B SEED-LLaMA-13B BLIP2 MiniMonkey DeepSeek-VL-7B LISA CogVLM-Chat ShareGPT4V-7B ShareGPT4V-13B BLIP-3 (XGen-MM) AnyGPT MiniCPM3-4B LaVIT-V2 (7B) GLM-VL-Chat Gemini-1.5-Pro Gemini-1.5-Flash OMG-LLaVA-InternLM20B Idefics3-8B-Llama3 NExT-GPT-V1.5 Vitron-V1 Otter Show-o NExT-Chat Yi-vision-v2 Qwen2-VL-72B 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 17.75 0.00 0.00 10.40 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 2.10 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.02 0.00 0.00 12.30 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 35.26 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 41.50 0.00 0.00 0.00 0.00 0.00 0.00 0.00 30.22 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 16.78 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.68 0.00 0.00 2.00 0.00 0.00 0.00 0.00 0. 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 19.96 0.00 0.00 35.60 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 17.70 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 14.53 0.00 0.00 10.70 0.00 0.00 0.00 0.00 0.00 250 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 On Path to Multimodal Generalist: General-Level and General-Bench Table 52: Results on Image Comprehension Group, from #I-C-21 to #I-C-22."
        },
        {
            "title": "Model",
            "content": "#I-C-21 (Multi-img VQA) #I-C-22 (MM Dialog) #1 #2 #3 #4 #5 #6 #1 #2 #3 #4 #5 #"
        },
        {
            "title": "SoTA Specialist",
            "content": "88.20 87.60 75.00 49.60 59.80 59. 51.90 51.60 53.40 53.10 51.60 40. GPT4V GPT4o GPT4o-mini GPT-4o-4096 ChatGPT-4o-latest Claude-3.5-Sonnet Claude-3.5-Opus Emu2-32B DetGPT InternVL2.5-8B InternVL2.5-4B InternVL2.5-2B Monkey-10B-chat DeepSeek-VL-7B-Chat Qwen2-VL-7B Qwen-VL-Chat MoE-LLAVA-Phi2-2.7B-4e-384 mPLUG-Owl2-LLaMA2-7b Phi-3.5-Vision-Instruct Cambrian-1-8B MiniGPT4-LLaMA2-7B InternVL-Chat-V1-5 Mini-InternVL-Chat-4B-V1-5 InternLM-XComposer2-VL-1.8B GPT4RoI GLaMM LLaVA-NeXT-13B LLaVA-NeXT-34B Pixtral 12B SEED-LLaMA-13B BLIP2 MiniMonkey DeepSeek-VL-7B LISA CogVLM-Chat ShareGPT4V-7B ShareGPT4V-13B BLIP-3 (XGen-MM) AnyGPT MiniCPM3-4B LaVIT-V2 (7B) GLM-VL-Chat Gemini-1.5-Pro Gemini-1.5-Flash OMG-LLaVA-InternLM20B Idefics3-8B-Llama3 NExT-GPT-V1.5 Vitron-V1 Otter Show-o NExT-Chat Yi-vision-v2 Qwen2-VL-72B 46.20 61.60 26.80 51.40 25.60 44.28 41.96 48.40 39.00 50.00 69.13 46.79 43.80 59.73 78.83 35.60 55.60 55.40 67.26 48.44 95.97 63.40 80.20 75.20 54.60 0.00 54.60 62.40 52.40 41.80 57.20 69.00 56.40 0.00 53.40 45.80 48.60 51.20 0.00 49.40 43.20 54.20 88.80 84.20 6.20 68.60 57.60 59.20 0.00 50.60 33.79 50.40 80.41 47.20 59.80 28.40 50.00 27.00 45.04 40.65 53.60 31.00 49.43 68.27 43.02 39.73 56.20 77.52 33.60 53.20 52.40 58.51 50.31 85.91 84.20 82.40 68.80 53.60 0.00 34.20 51.20 46.40 28.40 58.40 43.40 58.80 0.00 47.20 41.60 43.20 45.80 0.00 34.60 0.00 50.40 86.20 81.40 8.80 61.40 57.30 60.30 0.00 49.80 28.95 49.80 80.15 82.40 95.60 96.60 96.00 95.80 90.56 89.46 62.60 42.60 23.91 49.40 7.18 49.40 8.92 87.60 64.80 49.60 53.00 70.34 59.12 12.08 53.00 52.40 50.00 8.20 0.00 52.60 66.40 56.20 44.40 48.60 59.40 47.20 0.00 64.60 56.20 58.40 62.20 27.60 78.00 45.80 67.40 82.20 77.60 7.60 71.20 47.90 48.60 0.00 48.20 52.40 61.80 89.60 76.80 87.60 66.20 78.20 79.20 75.94 74.70 54.20 38.40 17.21 50.00 53.00 50.00 4.02 83.60 45.80 48.60 49.80 58.45 23.97 1.34 45.60 42.80 38.50 35.20 0.00 57.60 69.80 36.40 62.40 58.80 67.20 61.40 0.00 67.20 61.60 62.40 65.80 14.20 57.40 27.40 64.60 83.00 79.40 7.00 63.40 16.40 28.70 0.00 49.60 13.79 66.80 67.34 23.45 21.84 22.79 20.88 22.11 30.71 33.72 28.13 22.96 17.52 15.29 17.31 10.09 14.36 20.50 18.26 17.05 18.38 21.42 8.53 18.96 11.60 11.90 20.50 15.10 0.00 22.16 27.24 24.14 17.25 35.40 15.63 11.86 0.00 21.03 16.22 17.97 19.58 11.34 20.62 15.53 22.47 25.60 22.80 16.60 22.50 18.60 18.70 0.00 0.20 16.32 12.12 24. 55.06 62.09 47.74 59.21 54.44 49.91 49.87 35.60 25.60 30.35 19.40 28.54 11.47 17.99 47.01 30.91 21.81 30.96 31.49 26.70 18.93 17.20 16.80 23.40 28.00 0.00 38.40 42.40 43.80 34.40 36.00 13.52 14.69 0.00 41.90 34.19 36.58 38.67 18.40 45.90 31.70 42.40 33.40 30.30 15.90 26.10 28.40 29.60 0.00 0.10 17.81 18.30 48.33 28.00 28.13 24.94 30.22 28.35 35.13 35.39 27.87 19.67 27.96 20.64 28.46 12.55 33.17 33.01 25.80 23.38 25.06 29.88 26.29 26.05 17.20 17.80 22.00 27.40 0.00 23.04 25.18 27.24 16.85 32.80 16.89 29.66 0.00 23.47 18.39 19.45 21.48 8.64 24.07 15.31 27.39 30.30 27.40 14.60 24.60 28.70 25.60 0.00 0.10 19.15 25.74 33.34 52.38 53.13 42.82 50.20 49.62 45.96 44.50 27.40 19.70 35.52 17.25 18.99 17.38 19.04 53.50 39.69 23.71 31.02 42.70 18.90 23.41 19.10 17.20 26.10 39.90 0.00 37.40 40.20 41.80 24.20 33.60 15.26 18.97 0.00 41.60 34.27 36.89 39.23 17.10 47.30 25.80 46.40 32.60 34.00 19.10 32.80 31.76 35.29 0.00 0.10 20.38 20.58 50.70 37.05 38.09 32.13 40.12 38.09 39.53 42.15 23.40 20.70 29.30 17.52 20.63 10.93 28.30 42.38 27.22 22.26 27.30 39.67 9.37 18.97 16.80 15.30 24.10 24.50 0.00 37.40 36.40 34.20 20.80 35.00 10.19 17.21 0.00 39.36 35.84 38.50 40.56 17.40 44.60 21.50 43.10 28.40 25.70 13.90 22.70 22.36 18.65 0.00 0.10 20.05 25.82 44.23 26.80 34.40 11.60 2.40 17.25 38.68 41.51 34.20 22.40 29.54 27.47 28.09 37.07 27.59 31.20 21.09 17.32 8.90 28.73 22.80 17.49 42.60 33.20 34.70 0.00 0.00 29.80 32.20 31.00 21.60 39.60 33.20 23.40 0.00 30.40 22.00 24.60 26.80 6.80 29.60 19.40 31.40 39.00 33.60 1.80 36.20 36.81 41.00 0.00 0.00 21.37 7.75 30.01 84.80 85.00 80.80 91.00 76.20 83.38 81.52 52.40 28.40 18.94 47.48 2.62 50.80 8.58 61.80 64.80 50.40 47.20 62.80 45.46 45.64 55.60 51.60 46.90 50.00 0.00 41.40 58.20 50.20 25.60 53.00 33.60 57.60 0.00 56.80 49.80 52.60 54.20 6.80 56.60 21.40 42.80 82.60 78.20 8.00 64.80 51.60 55.80 0.00 46.80 61.55 66.40 75. 94.00 98.20 97.80 98.80 97.60 96.56 94.03 66.80 29.20 21.26 50.84 7.30 47.60 7.86 96.39 73.00 49.40 51.80 88.00 64.07 8.05 65.20 59.60 49.20 25.80 0.00 53.40 65.60 57.80 28.60 44.40 63.20 39.80 0.00 62.40 56.20 57.40 60.80 12.40 64.00 26.80 56.80 84.60 83.00 8.40 59.80 45.80 47.40 0.00 43.40 64.29 69.40 98.43 251 On Path to Multimodal Generalist: General-Level and General-Bench Table 53: Results on Image Comprehension Group, #I-C-23."
        },
        {
            "title": "Model",
            "content": "#I-C-23 (MM Reason)"
        },
        {
            "title": "SoTA Specialist",
            "content": "68.80 69.80 77.20 50.00 49.80 64.60 80.20 70.60 80.60 47.20 66.20 67.60 74.20 56.80 #1 #2 #3 #4 #5 #6 #7 #8 #9 #10 #11 #12 #13 #14 GPT4V GPT4o GPT4o-mini GPT-4o-4096 ChatGPT-4o-latest Claude-3.5-Sonnet Claude-3.5-Opus Emu2-32B DetGPT InternVL2.5-8B InternVL2.5-4B InternVL2.5-2B Monkey-10B-chat DeepSeek-VL-7B-Chat Qwen2-VL-7B Qwen-VL-Chat MoE-LLAVA-Phi2-2.7B-4e-384 mPLUG-Owl2-LLaMA2-7b Phi-3.5-Vision-Instruct Cambrian-1-8B MiniGPT4-LLaMA2-7B InternVL-Chat-V1-5 Mini-InternVL-Chat-4B-V1-5 InternLM-XComposer2-VL-1.8B GPT4RoI GLaMM LLaVA-NeXT-13B LLaVA-NeXT-34B Pixtral 12B SEED-LLaMA-13B BLIP2 MiniMonkey DeepSeek-VL-7B LISA CogVLM-Chat ShareGPT4V-7B ShareGPT4V-13B BLIP-3 (XGen-MM) AnyGPT MiniCPM3-4B LaVIT-V2 (7B) GLM-VL-Chat Gemini-1.5-Pro Gemini-1.5-Flash OMG-LLaVA-InternLM20B Idefics3-8B-Llama3 NExT-GPT-V1.5 Vitron-V1 Otter Show-o NExT-Chat Yi-vision-v2 Qwen2-VL-72B 9.40 7.80 1.00 8.80 6.60 5. 4.40 3.20 22.60 11.60 34.40 22.00 20.00 47.40 55.91 54.00 52.60 45. 0.00 0.00 0.00 40.60 75.00 0.00 72.40 22.80 17.80 91.00 91.20 79.00 81.80 23.80 56.00 56.60 80.90 30.90 0.00 97.40 97.40 75.80 77.40 37.00 50.40 51.40 81.40 31.20 62.20 10.40 0.00 76.40 18.80 19.20 92.40 92.60 71.60 66.80 17.80 56.40 53.20 73.20 34.10 0.00 96.00 96.60 77.80 79.20 31.60 51.00 52.00 82.20 39.00 0.00 69.72 13.00 12.80 93.28 93.80 76.20 73.00 25.60 53.00 54.00 82.00 37.80 0.00 53.91 53.14 65.54 17.24 25.84 40.84 81.56 74.82 78.57 28.13 65.19 67.70 73.57 36.80 55.67 53.32 68.41 18.09 23.93 40.69 84.83 72.19 80.09 27.42 67.96 64.57 71.38 37.88 40.20 36.40 54.60 10.40 18.80 29.60 76.40 66.40 71.20 21.40 58.40 59.60 62.40 30.60 36.40 35.80 33.20 62.40 49.60 46.80 14.20 48.80 40.20 40.00 20.60 33.33 45.06 78.66 34.18 34.83 34.18 62.71 58.45 57.98 47.62 37.98 37.84 42.41 39.46 44.61 44.28 65.00 33.06 31.97 28.81 46.20 52.54 51.95 31.81 43.42 44.95 45.29 28.93 11.63 45.45 60.20 33.50 33.68 32.72 70.83 65.98 43.25 34.02 36.14 29.62 46.42 40.53 45.40 32.20 45.60 32.94 33.04 32.31 45.00 55.60 36.42 33.79 52.99 52.60 41.76 37.00 46.52 68.60 56.20 39.84 39.64 41.70 55.40 31.82 39.97 37.03 38.72 39.45 42.71 56.60 52.00 51.40 81.41 43.40 44.00 43.40 57.71 72.40 42.20 53.20 59.35 59.49 68.20 13.33 37.60 40.00 62.20 49.20 49.80 31.20 45.80 45.50 14.20 52.80 20.00 20.70 58.19 40.20 4.80 3.80 53.00 62.40 55.71 28.00 27.80 28.20 55.20 56.30 29.20 14.60 39.40 41.20 35.00 57.20 59.72 66.40 67.40 51.06 48.00 40.76 54.20 37.40 38.50 54.00 55.20 48.77 60.05 39.65 36.54 49.62 56.20 44.36 39.81 29.74 46.00 39.10 19.82 46.60 40.27 19.20 37.60 25.78 39.83 44.34 48.90 39.46 35.90 33.26 39.83 38.69 20.00 48.96 41.28 22.17 27.61 28.00 46.40 47.60 61.80 68.40 75.60 68.20 56.40 61.60 61.00 74.80 53.00 53.20 68.20 43.00 44.20 46.40 57.60 53.40 69.80 63.90 51.20 61.20 33.40 55.20 47.20 46.80 69.20 46.00 48.20 45.40 60.60 41.90 56.90 53.70 45.40 53.20 48.60 73.40 54.00 53.00 54.80 37.00 47.00 47.40 28.40 36.40 39.20 22.60 38.60 0.00 0.00 0.00 0.00 0.00 0.00 0.00 14.40 28.80 68.40 63.40 68.20 16.60 54.20 49.20 60.60 26.80 49.20 47.00 54.60 51.00 53.40 60.20 10.80 17.40 37.60 75.60 68.40 70.40 20.20 56.80 56.20 65.20 28.60 15.60 35.40 74.80 67.80 74.20 17.80 53.40 51.20 66.40 30.20 42.80 43.20 56.00 64.80 37.80 42.40 10.20 48.40 44.60 46.60 17.40 4.20 43.20 39.80 46.80 43.00 44.40 53.40 46.20 43.40 34.80 37.80 41.60 58.80 30.20 2.00 42.40 29.80 42.88 29.80 18.80 29.40 40.40 52.40 31.20 28.00 48.60 44.20 36.40 37.00 42.20 63.80 51.80 33.60 37.60 37.60 55.20 29.60 34.80 36.80 38.80 40.60 41.80 53.40 0.00 0.00 50.40 54.20 62.80 10.40 15.80 31.60 77.40 66.40 76.00 14.20 59.40 57.80 65.80 29.40 13.60 26.80 69.40 60.60 64.20 11.40 51.20 48.80 61.60 24.80 42.00 46.60 54.40 15.20 28.20 71.80 62.40 68.60 12.80 54.80 51.40 58.20 26.40 44.20 48.40 56.80 14.00 29.40 74.60 64.80 72.20 13.60 57.20 54.40 63.20 27.20 48.60 52.80 60.20 26.80 31.20 24.80 8.60 0.00 52.20 53.80 66.40 10.60 18.60 27.60 76.20 69.40 74.60 11.60 62.80 60.80 61.40 29.40 43.00 38.80 49.60 48.40 51.00 48.20 14.20 50.60 57.40 64.80 12.20 14.60 32.40 79.80 70.20 79.80 15.40 61.00 65.20 66.80 28.80 46.20 40.60 77.40 37.80 37.20 91.60 93.20 76.40 74.80 40.60 56.20 55.40 72.60 52.20 44.80 38.20 69.20 33.00 33.60 89.20 90.40 71.80 73.00 37.80 48.40 52.20 68.40 46.80 2.60 2.60 43.80 41.20 58.40 23.20 28.80 45.60 78.80 64.20 72.20 38.40 43.20 48.80 48.20 43.20 37.98 51.30 13.40 0.00 10.30 34.60 19.60 18.40 18.60 10.70 24.78 28.96 10.36 28.64 52.38 17.50 14.60 35.80 0.00 22.16 31.20 14.89 9.45 38.41 20.57 21.50 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 42.80 45.60 25.00 39.80 41.00 11.60 37.00 0.00 45.00 33.20 15.20 9.00 0.00 7.20 37.08 61.76 57.80 13.71 13.36 25.66 33.93 45.37 25.61 35.98 15.73 29.35 23.68 44.30 45.09 37.80 70.03 26.40 14.40 36.62 61.23 49.79 21.90 24.75 58.08 57.06 35.16 39.49 53.06 55.56 90.86 46.67 37.13 49.32 52.23 81.24 54.42 57.17 64.46 52.34 79.93 51.51 27.20 50.20 19.60 0.00 0.00 0.00 10.60 12.40 21.40 66.60 34.00 40.60 8.20 9.60 8.40 1. 3.40 4.80 0.00 8.60 2.80 1.60 0.00 0.00 7.20 8.40 0.20 0.00 0.00 0.00 0. 32.40 26.40 0.00 0.00 0.00 3. 0.00 2.80 1.20 7.60 9.60 2. 2.20 2.80 3.80 4.00 2.20 3. 2.20 2.60 2.40 2.40 0.00 0. 0.00 0.00 0.00 0.00 0.00 0. 0.00 0.00 0.00 0.00 3.60 1. 252 On Path to Multimodal Generalist: General-Level and General-Bench Table 54: Results on Image Comprehension Group, from #I-C-24 to #I-C-25."
        },
        {
            "title": "Model",
            "content": "#I-C-24 (Object Count) #I-C-25 (NMT) #1 #2 #3 #4 #5 #6 #7 #8 #9 #"
        },
        {
            "title": "SoTA Specialist",
            "content": "5.95 9.00 26.84 8.13 16.41 17. 28.16 10.00 27.00 78.00 GPT4V GPT4o GPT4o-mini GPT-4o-4096 ChatGPT-4o-latest Claude-3.5-Sonnet Claude-3.5-Opus Emu2-32B DetGPT InternVL2.5-8B InternVL2.5-4B InternVL2.5-2B Monkey-10B-chat DeepSeek-VL-7B-Chat Qwen2-VL-7B Qwen-VL-Chat MoE-LLAVA-Phi2-2.7B-4e-384 mPLUG-Owl2-LLaMA2-7b Phi-3.5-Vision-Instruct Cambrian-1-8B MiniGPT4-LLaMA2-7B InternVL-Chat-V1-5 Mini-InternVL-Chat-4B-V1-5 InternLM-XComposer2-VL-1.8B GPT4RoI GLaMM LLaVA-NeXT-13B LLaVA-NeXT-34B Pixtral 12B SEED-LLaMA-13B BLIP2 MiniMonkey DeepSeek-VL-7B LISA CogVLM-Chat ShareGPT4V-7B ShareGPT4V-13B BLIP-3 (XGen-MM) AnyGPT MiniCPM3-4B LaVIT-V2 (7B) GLM-VL-Chat Gemini-1.5-Pro Gemini-1.5-Flash OMG-LLaVA-InternLM20B Idefics3-8B-Llama3 NExT-GPT-V1.5 Vitron-V1 Otter Show-o NExT-Chat Yi-vision-v2 Qwen2-VL-72B 8.41 9.32 10.06 7.47 10.41 9.21 8.50 10.57 11.08 9.11 12.58 14.11 8.35 25.84 12.49 7.41 26.49 14.28 19.31 14.08 9.60 14.30 18.20 13.30 15.38 13.86 12.07 14.50 19.20 13.98 27.29 11.61 12.47 12.62 14.47 13.92 10.36 10.98 9.09 7.74 11.23 22.85 12.51 11.58 12.34 22.04 71.14 29.56 13.63 9. 31.29 31.17 30.98 31.11 29.23 30.73 28.21 33.37 31.34 35.40 32.88 37.27 37.09 36.38 30.30 25.89 33.91 35.22 26.80 39.15 41.20 34.60 36.80 39.00 34.40 32.42 34.65 38.60 37.50 38.72 38.61 33.17 38.94 38.20 34.17 40.82 35.25 35.93 33.44 32.19 33.03 38.65 35.69 36.77 35.91 38.34 3.20 2.30 2.80 23.66 2.60 1.89 0.24 1.85 1.54 0.37 0.59 1.07 1.04 1.63 0.27 1.40 1.05 2.30 0.80 2.73 2.10 0.60 1.94 2.59 3.29 3.81 3.60 4.52 5.40 2.67 3.78 3.06 3.02 4.15 2.84 3.54 7.43 3.24 4.62 4.21 0.32 1.78 2.89 3.71 4.73 3.69 1.12 89.42 10.03 0.44 0.30 36.84 29.83 27.13 9.79 11.85 13.62 14.65 9.66 11.01 9.29 26.23 31.65 10.84 16.09 37.15 63.28 69.02 41.30 36.60 69.43 55.12 47.26 50.61 65.30 41.60 40.10 54.80 17.94 18.02 18.52 28.06 33.22 53.10 62.59 15.68 25.19 26.77 13.45 42.57 14.45 24.16 17.28 10.78 12.36 57.65 22.25 31.25 27.59 60.17 69.78 24.51 9.81 26.91 26.65 43.87 21.75 21.44 31.93 27.59 39.15 42.41 27.17 39.62 69.25 73.40 100.00 46.49 67.19 102.00 69.30 52.80 95.19 4.40 69.60 68.30 62.30 43.35 38.71 45.04 47.52 37.21 74.57 68.83 35.92 42.41 40.75 38.34 62.53 44.38 60.17 34.36 28.85 33.17 71.08 41.75 67.40 56.70 91.58 89.27 44.48 42.55 32.21 23.01 26.30 16.26 14.04 26.30 23.33 37.24 42.12 23.68 24.67 28.67 24.52 24.52 16.00 16.45 21.66 21.57 22.40 22.06 20.58 23.90 25.20 26.50 34.08 30.04 36.14 38.64 28.70 26.64 21.59 32.87 26.38 28.49 34.36 36.07 28.07 34.88 27.88 26.73 25.00 39.44 27.50 30.58 28.15 27.67 10.86 5.29 10.56 4.69 5.86 8.24 8.40 17.58 16.28 21.69 27.88 24.69 32.71 34.30 27.79 21.29 30.47 20.19 22.80 26.40 28.49 16.90 22.10 24.70 22.44 20.06 17.02 16.42 39.20 23.64 33.49 18.34 22.68 21.54 23.58 30.94 19.64 15.64 9.99 8.28 10.05 38.74 13.68 16.47 15.46 36.11 11.14 10.72 15.70 10.76 14.66 12.87 16.00 9.93 22.35 10.41 13.42 11.08 11.99 7.29 12.83 14.24 12.94 13.58 14.48 9.81 14.41 15.38 21.02 20.03 14.68 13.91 14.58 24.81 12.57 12.50 11.09 7.42 14.33 19.28 12.35 13.37 11.03 10.12 10.11 9.85 10.92 20.03 17.30 10.70 17.25 14.10 16.70 12.40 15.24 13.38 14.53 14.46 16.29 12.98 20.49 15.02 20.60 21.10 23.62 15.80 18.97 22.63 19.35 11.63 21.94 13.19 23.58 12.22 17.62 14.21 26.84 14.25 15.03 10.30 18.21 10.41 11.14 9.67 12.03 9.90 11.80 10.75 28.58 21.00 16.84 12.25 23.31 13.59 20.36 11.50 9.33 27.75 72.13 73.30 21.55 14.09 9.47 14.38 20.14 21.86 25.24 15.67 14.34 28.68 13.23 10. 72.20 93.00 83.00 88.40 76.52 82.23 77.80 64.20 38.40 7.82 33.27 20.00 52.20 5.34 97.20 94.80 80.80 86.80 90.10 71.40 76.32 85.20 80.40 79.30 0.00 0.00 56.80 68.40 60.40 32.20 23.60 24.20 5.80 0.00 65.60 56.20 59.60 63.20 14.60 73.40 45.00 68.80 86.40 82.60 3.40 63.40 0.00 0.00 0.00 0.00 43.52 8.77 97.43 On Path to Multimodal Generalist: General-Level and General-Bench Table 55: Results on Image Comprehension Group,#I-C-26, part A."
        },
        {
            "title": "Model",
            "content": "#I-C-26 (Object Det)"
        },
        {
            "title": "SoTA Specialist",
            "content": "87.20 87.80 32.60 41.10 73.80 29.00 34.00 62.40 93.20 26.20 37.60 18.10 61.60 #1 #2 #3 #4 #5 #6 #7 #8 #9 #10 #11 #12 #13 GPT4V GPT4o GPT4o-mini GPT-4o-4096 ChatGPT-4o-latest Claude-3.5-Sonnet Claude-3.5-Opus Emu2-32B DetGPT InternVL2.5-8B InternVL2.5-4B InternVL2.5-2B Monkey-10B-chat DeepSeek-VL-7B-Chat Qwen2-VL-7B Qwen-VL-Chat MoE-LLAVA-Phi2-2.7B-4e-384 mPLUG-Owl2-LLaMA2-7b Phi-3.5-Vision-Instruct Cambrian-1-8B MiniGPT4-LLaMA2-7B InternVL-Chat-V1-5 Mini-InternVL-Chat-4B-V1-5 InternLM-XComposer2-VL-1.8B GPT4RoI GLaMM LLaVA-NeXT-13B LLaVA-NeXT-34B Pixtral 12B SEED-LLaMA-13B BLIP2 MiniMonkey DeepSeek-VL-7B LISA CogVLM-Chat ShareGPT4V-7B ShareGPT4V-13B BLIP-3 (XGen-MM) AnyGPT MiniCPM3-4B LaVIT-V2 (7B) GLM-VL-Chat Gemini-1.5-Pro Gemini-1.5-Flash OMG-LLaVA-InternLM20B Idefics3-8B-Llama3 NExT-GPT-V1.5 Vitron-V1 Otter Show-o NExT-Chat Yi-vision-v2 Qwen2-VL-72B 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0. 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 85.99 30.69 39.93 67.60 25.20 26.38 56.40 88.80 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 55.60 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 37.10 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 12.34 37.84 50.36 43.50 82.10 16.40 38.00 56.80 75.80 32.40 31.70 46.70 60.34 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 33.40 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 9.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 3.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0. 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0. 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0. 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 254 On Path to Multimodal Generalist: General-Level and General-Bench Table 56: Results on Image Comprehension Group,#I-C-26, part B."
        },
        {
            "title": "Model",
            "content": "#I-C-26 (Object Det) #14 #15 #16 #17 #18 #19 #20 #21 #22 #23 #24 #25 #"
        },
        {
            "title": "SoTA Specialist",
            "content": "27.20 68.30 81.10 86.40 18.20 17.20 31.60 78.90 28.30 48.10 3.50 39.40 22.40 GPT4V GPT4o GPT4o-mini GPT-4o-4096 ChatGPT-4o-latest Claude-3.5-Sonnet Claude-3.5-Opus Emu2-32B DetGPT InternVL2.5-8B InternVL2.5-4B InternVL2.5-2B Monkey-10B-chat DeepSeek-VL-7B-Chat Qwen2-VL-7B Qwen-VL-Chat MoE-LLAVA-Phi2-2.7B-4e-384 mPLUG-Owl2-LLaMA2-7b Phi-3.5-Vision-Instruct Cambrian-1-8B MiniGPT4-LLaMA2-7B InternVL-Chat-V1-5 Mini-InternVL-Chat-4B-V1-5 InternLM-XComposer2-VL-1.8B GPT4RoI GLaMM LLaVA-NeXT-13B LLaVA-NeXT-34B Pixtral 12B SEED-LLaMA-13B BLIP2 MiniMonkey DeepSeek-VL-7B LISA CogVLM-Chat ShareGPT4V-7B ShareGPT4V-13B BLIP-3 (XGen-MM) AnyGPT MiniCPM3-4B LaVIT-V2 (7B) GLM-VL-Chat Gemini-1.5-Pro Gemini-1.5-Flash OMG-LLaVA-InternLM20B Idefics3-8B-Llama3 NExT-GPT-V1.5 Vitron-V1 Otter Show-o NExT-Chat Yi-vision-v2 Qwen2-VL-72B 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0. 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 21.40 66.20 80.60 83.60 10.40 9.40 21.80 69.80 19.00 42.80 2.60 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 21.47 56.70 64.20 87.36 23.40 36.50 42.70 53.70 34.60 56.10 10.40 3.60 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0. 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0. 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0. 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.40 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.80 0.00 0.00 0.00 0.00 0. 255 On Path to Multimodal Generalist: General-Level and General-Bench Table 57: Results on Image Comprehension Group,#I-C-26, part C."
        },
        {
            "title": "Model",
            "content": "#I-C-26 (Object Det) #27 #28 #29 #30 #31 #32 #33 #34 #35 #36 #"
        },
        {
            "title": "SoTA Specialist",
            "content": "67.60 63.40 10.90 48.60 38.00 97. 61.10 97.50 25.90 58.20 64.20 GPT4V GPT4o GPT4o-mini GPT-4o-4096 ChatGPT-4o-latest Claude-3.5-Sonnet Claude-3.5-Opus Emu2-32B DetGPT InternVL2.5-8B InternVL2.5-4B InternVL2.5-2B Monkey-10B-chat DeepSeek-VL-7B-Chat Qwen2-VL-7B Qwen-VL-Chat MoE-LLAVA-Phi2-2.7B-4e-384 mPLUG-Owl2-LLaMA2-7b Phi-3.5-Vision-Instruct Cambrian-1-8B MiniGPT4-LLaMA2-7B InternVL-Chat-V1-5 Mini-InternVL-Chat-4B-V1-5 InternLM-XComposer2-VL-1.8B GPT4RoI GLaMM LLaVA-NeXT-13B LLaVA-NeXT-34B Pixtral 12B SEED-LLaMA-13B BLIP2 MiniMonkey DeepSeek-VL-7B LISA CogVLM-Chat ShareGPT4V-7B ShareGPT4V-13B BLIP-3 (XGen-MM) AnyGPT MiniCPM3-4B LaVIT-V2 (7B) GLM-VL-Chat Gemini-1.5-Pro Gemini-1.5-Flash OMG-LLaVA-InternLM20B Idefics3-8B-Llama3 NExT-GPT-V1.5 Vitron-V1 Otter Show-o NExT-Chat Yi-vision-v2 Qwen2-VL-72B 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 30.55 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 26.40 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 93.93 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 85.70 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 52.23 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 56.40 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 92.80 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 87.60 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 22.83 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 35.60 0.00 0.00 0.00 7.24 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 57.04 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 47.80 0.00 0.00 0.00 0.00 0. 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 59.71 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 26.70 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 26.80 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 37.50 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 58.98 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 74.10 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 9.39 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 45.70 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 47.68 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 63.30 0.00 0.00 0.00 0.00 0.00 On Path to Multimodal Generalist: General-Level and General-Bench Table 58: Results on Image Comprehension Group, #I-C-27."
        },
        {
            "title": "SoTA Specialist",
            "content": "GPT4V GPT4o GPT4o-mini GPT-4o-4096 ChatGPT-4o-latest Claude-3.5-Sonnet Claude-3.5-Opus Emu2-32B DetGPT InternVL2.5-8B InternVL2.5-4B InternVL2.5-2B Monkey-10B-chat DeepSeek-VL-7B-Chat Qwen2-VL-7B Qwen-VL-Chat MoE-LLAVA-Phi2-2.7B-4e-384 mPLUG-Owl2-LLaMA2-7b Phi-3.5-Vision-Instruct Cambrian-1-8B MiniGPT4-LLaMA2-7B InternVL-Chat-V1-5 Mini-InternVL-Chat-4B-V1-5 InternLM-XComposer2-VL-1.8B GPT4RoI GLaMM LLaVA-NeXT-13B LLaVA-NeXT-34B Pixtral 12B SEED-LLaMA-13B BLIP2 MiniMonkey DeepSeek-VL-7B LISA CogVLM-Chat ShareGPT4V-7B ShareGPT4V-13B BLIP-3 (XGen-MM) AnyGPT MiniCPM3-4B LaVIT-V2 (7B) GLM-VL-Chat Gemini-1.5-Pro Gemini-1.5-Flash OMG-LLaVA-InternLM20B Idefics3-8B-Llama3 NExT-GPT-V1.5 Vitron-V1 Otter Show-o NExT-Chat Yi-vision-v2 Qwen2-VL-72B #I-C-27 (Object Mat) #3 16.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.78 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 #2 16.60 13.20 #4 28.90 23.64 #1 18.10 13.20 257 On Path to Multimodal Generalist: General-Level and General-Bench Table 59: Results on Image Comprehension Group,#I-C-28, part A."
        },
        {
            "title": "Model",
            "content": "#I-C-28 (Object Recog) #1 #2 #3 #4 #5 #6 #7 #8 #9 #10 #11 #"
        },
        {
            "title": "SoTA Specialist",
            "content": "62.16 94.85 52.58 18.45 57.67 75. 54.85 75.21 82.22 45.89 88.73 62. GPT4V GPT4o GPT4o-mini GPT-4o-4096 ChatGPT-4o-latest Claude-3.5-Sonnet Claude-3.5-Opus Emu2-32B DetGPT InternVL2.5-8B InternVL2.5-4B InternVL2.5-2B Monkey-10B-chat DeepSeek-VL-7B-Chat Qwen2-VL-7B Qwen-VL-Chat MoE-LLAVA-Phi2-2.7B-4e-384 mPLUG-Owl2-LLaMA2-7b Phi-3.5-Vision-Instruct Cambrian-1-8B MiniGPT4-LLaMA2-7B InternVL-Chat-V1-5 Mini-InternVL-Chat-4B-V1-5 InternLM-XComposer2-VL-1.8B GPT4RoI GLaMM LLaVA-NeXT-13B LLaVA-NeXT-34B Pixtral 12B SEED-LLaMA-13B BLIP2 MiniMonkey DeepSeek-VL-7B LISA CogVLM-Chat ShareGPT4V-7B ShareGPT4V-13B BLIP-3 (XGen-MM) AnyGPT MiniCPM3-4B LaVIT-V2 (7B) GLM-VL-Chat Gemini-1.5-Pro Gemini-1.5-Flash OMG-LLaVA-InternLM20B Idefics3-8B-Llama3 NExT-GPT-V1.5 Vitron-V1 Otter Show-o NExT-Chat Yi-vision-v2 Qwen2-VL-72B 98.24 98.43 99.43 98.82 94.31 98.56 98.52 86.08 78.82 96.08 63.53 48.63 85.29 19.80 92.34 80.98 36.07 72.94 97.65 80.59 36.27 81.90 48.80 11.70 85.60 0.00 88.04 93.73 72.36 52.55 75.69 0.00 83.65 0.00 88.24 69.41 71.37 75.93 49.22 96.47 66.27 98.43 97.25 94.31 2.75 98.24 72.30 67.50 3.77 0.79 77.61 90.39 97.37 93.22 95.53 94.30 95.39 94.31 93.46 91.89 86.36 83.74 93.22 16.26 88.48 94.72 89.16 93.68 94.93 91.24 76.60 87.00 80.61 87.00 88.20 59.60 87.53 74.90 0.00 84.01 91.32 81.30 65.31 67.89 90.11 87.13 0.00 88.62 79.40 83.88 84.01 63.96 95.53 78.86 93.22 91.73 87.94 4.88 90.65 56.89 78.52 6.28 0.67 23.04 88.48 96.66 63.03 82.87 78.33 83.79 84.24 74.42 72.55 46.97 42.42 53.94 39.24 57.27 53.79 64.85 75.76 55.75 54.40 43.90 55.30 50.30 35.61 74.20 24.00 31.30 23.70 0.00 63.64 72.72 43.18 35.76 45.61 40.15 65.07 0.00 66.52 52.27 58.48 58.78 16.97 76.06 65.00 73.79 83.33 77.58 4.10 72.42 60.20 61.34 4.86 10.45 26.18 47.13 80.79 96.04 97.30 94.79 99.77 97.71 95.22 94.55 66.67 31.25 60.21 60.21 40.21 84.79 83.33 91.04 86.45 81.25 72.92 74.38 37.29 47.50 75.40 50.60 61.80 28.70 0.00 79.17 83.33 73.33 34.79 64.38 56.46 85.62 0.00 78.13 67.92 72.29 74.17 27.35 81.67 68.13 88.54 93.13 89.17 8.75 84.79 76.89 81.34 44.89 17.71 55.06 78.13 94.12 84.47 95.63 94.41 94.42 96.10 90.97 90.29 70.14 62.38 99.76 100.00 38.59 100.00 100.00 91.26 54.12 61.17 71.84 74.03 50.49 78.64 67.80 54.70 53.60 66.20 0.00 82.52 84.95 77.66 67.96 72.09 60.92 89.51 0.00 85.92 69.17 75.73 83.98 67.33 87.61 76.02 91.74 94.90 92.96 35.19 82.77 73.65 70.31 37.70 39.32 29.80 73.54 98. 97.57 97.14 97.54 99.79 98.63 96.53 93.14 83.08 60.60 92.29 72.59 83.51 97.64 70.45 97.43 93.57 80.73 95.07 98.29 7.28 48.82 75.30 77.70 79.00 26.90 0.00 85.87 89.93 87.79 73.87 71.31 71.03 71.06 0.00 89.51 74.52 82.87 86.08 56.67 85.87 75.58 92.29 96.79 94.43 4.28 91.86 72.14 73.15 0.00 10.92 81.25 62.96 97.57 92.13 93.13 92.93 93.13 89.90 92.63 88.23 54.55 57.98 85.63 85.83 9.11 94.16 91.10 93.78 91.07 86.92 88.20 92.29 92.67 88.44 91.50 93.30 93.30 38.40 0.00 85.75 88.89 80.80 50.51 55.76 87.40 90.56 0.00 86.06 78.59 82.63 84.24 50.30 83.03 66.87 92.12 93.33 91.52 5.05 82.02 61.34 67.96 44.69 60.12 91.91 93.12 90.62 72.84 77.05 73.26 80.84 63.16 73.71 69.89 44.84 29.05 44.00 36.00 31.79 54.53 14.32 74.47 38.94 11.78 35.57 29.26 45.47 11.16 51.10 22.70 21.60 4.20 0.00 50.32 52.42 46.32 32.58 49.05 57.20 18.53 0.00 51.79 42.95 46.95 49.26 23.16 47.58 39.37 54.11 74.53 72.21 2.74 56.00 32.68 40.36 7.91 9.23 16.18 56.37 75.42 96.41 97.50 94.69 96.72 95.94 96.13 95.10 71.88 53.28 79.19 64.48 66.00 33.60 1.20 96.56 90.45 89.96 76.72 87.32 87.79 82.16 91.70 81.70 75.90 11.80 0.00 85.31 89.06 80.94 64.22 82.34 52.74 5.78 0.00 88.59 78.28 81.88 85.47 38.13 76.40 61.09 84.84 96.56 93.75 2.81 87.66 48.69 63.75 0.00 1.72 56.76 94.21 94.40 98.91 99.87 91.71 99.88 99.52 96.49 92.89 60.69 53.97 44.47 39.18 42.40 43.40 20.20 38.74 24.63 15.50 19.47 27.28 13.46 11.78 51.50 22.40 12.70 36.70 0.00 79.57 84.86 78.13 56.97 39.54 0.84 30.04 0.00 82.80 77.16 79.57 81.13 34.86 89.30 56.61 84.50 93.63 89.54 2.76 76.92 63.59 67.03 0.00 0.12 17.94 50.96 30.99 94.50 96.10 95.83 99.00 99.33 94.95 92.93 64.83 55.33 70.33 72.67 74.50 86.17 64.83 95.33 84.50 53.17 63.00 78.17 69.50 37.50 83.80 47.80 51.10 79.50 0.00 64.17 66.17 61.33 56.17 54.83 61.50 66.29 0.00 61.50 52.17 55.83 59.50 53.50 73.17 61.33 78.83 94.83 91.00 2.83 68.50 50.12 51.34 13.02 0.00 59.62 77.36 96. 48.93 66.95 49.36 65.24 64.81 54.67 53.63 28.76 27.03 19.31 0.00 13.30 14.16 90.56 20.39 18.02 18.88 24.03 19.31 14.16 16.74 15.80 16.30 18.40 8.70 0.00 35.62 38.19 31.76 27.89 21.34 1.72 28.32 0.00 32.19 25.32 27.47 29.61 23.18 33.91 29.61 41.63 60.09 52.79 3.86 42.92 25.78 32.65 0.00 0.00 4.35 33.48 36.91 258 On Path to Multimodal Generalist: General-Level and General-Bench Table 60: Results on Image Comprehension Group,#I-C-28, part B."
        },
        {
            "title": "Model",
            "content": "#I-C-28 (Object Recog) #13 #14 #15 #16 #17 #18 #19 #20 #21 #"
        },
        {
            "title": "SoTA Specialist",
            "content": "15.73 51.30 64.00 17.67 17.30 27. 16.70 20.67 17.80 80.00 GPT4V GPT4o GPT4o-mini GPT-4o-4096 ChatGPT-4o-latest Claude-3.5-Sonnet Claude-3.5-Opus Emu2-32B DetGPT InternVL2.5-8B InternVL2.5-4B InternVL2.5-2B Monkey-10B-chat DeepSeek-VL-7B-Chat Qwen2-VL-7B Qwen-VL-Chat MoE-LLAVA-Phi2-2.7B-4e-384 mPLUG-Owl2-LLaMA2-7b Phi-3.5-Vision-Instruct Cambrian-1-8B MiniGPT4-LLaMA2-7B InternVL-Chat-V1-5 Mini-InternVL-Chat-4B-V1-5 InternLM-XComposer2-VL-1.8B GPT4RoI GLaMM LLaVA-NeXT-13B LLaVA-NeXT-34B Pixtral 12B SEED-LLaMA-13B BLIP2 MiniMonkey DeepSeek-VL-7B LISA CogVLM-Chat ShareGPT4V-7B ShareGPT4V-13B BLIP-3 (XGen-MM) AnyGPT MiniCPM3-4B LaVIT-V2 (7B) GLM-VL-Chat Gemini-1.5-Pro Gemini-1.5-Flash OMG-LLaVA-InternLM20B Idefics3-8B-Llama3 NExT-GPT-V1.5 Vitron-V1 Otter Show-o NExT-Chat Yi-vision-v2 Qwen2-VL-72B 19.96 39.69 17.54 34.21 20.83 24.87 21.80 24.36 26.20 8.33 9.21 9.01 9.48 67.38 8.55 0.00 3.07 5.70 4.17 4.39 4.61 8.30 5.40 5.20 7.20 0.00 26.75 28.51 21.93 16.89 6.80 0.00 71.05 0.00 34.43 23.25 27.19 32.02 5.92 23.40 14.91 38.60 24.34 22.15 2.19 19.96 15.36 14.69 0.00 2.19 2.73 7.64 10. 95.33 97.33 94.00 98.00 87.67 94.87 93.12 86.33 75.67 69.00 66.33 59.67 87.33 5.00 92.33 85.33 66.33 76.00 63.00 57.67 50.33 71.30 48.60 59.30 45.40 0.00 83.67 88.00 78.33 76.33 72.00 15.00 83.67 0.00 89.33 78.33 81.67 83.67 47.67 92.00 63.30 85.00 96.67 94.00 4.67 84.33 62.59 72.39 10.30 4.33 18.73 28.67 93.55 88.67 93.33 90.00 92.00 69.33 90.16 90.21 78.33 71.67 85.67 78.33 76.67 53.40 9.00 86.87 70.33 58.67 76.33 84.00 77.00 62.33 87.00 79.00 77.00 11.40 0.00 81.33 86.33 84.00 77.67 73.33 30.33 74.33 0.00 89.00 75.67 82.33 83.67 47.33 93.30 76.30 90.67 91.00 88.67 3.00 87.67 76.44 79.05 12.71 25.00 45.21 80.33 87.01 96.33 97.33 96.67 97.33 94.33 96.16 95.31 82.00 77.33 86.67 59.00 84.33 97.67 0.00 97.33 96.66 95.33 87.66 93.33 67.00 62.33 86.00 87.00 87.30 67.00 0.00 84.67 90.33 81.67 80.00 78.33 40.00 68.67 0.00 91.33 81.00 84.33 87.00 53.67 88.00 79.30 93.60 97.33 95.33 5.00 86.33 72.61 78.39 18.72 43.33 48.78 78.00 97.11 259 97.33 98.67 98.00 97.33 96.87 97.53 94.42 83.67 79.33 79.33 66.00 93.33 77.67 100.00 96.67 88.00 96.67 92.66 94.67 58.00 94.00 56.30 48.60 46.70 35.60 0.00 86.67 92.67 87.33 66.67 64.00 63.33 71.33 0.00 89.33 69.33 78.67 86.00 46.70 88.00 63.30 94.00 96.00 96.67 5.67 82.00 65.20 68.34 46.38 46.66 96.00 76.00 94.29 74.33 79.67 79.67 77.00 84.67 77.27 74.19 58.33 54.00 52.67 45.33 59.33 60.00 4.67 67.33 73.33 62.67 53.33 65.66 65.00 45.00 71.00 50.30 58.60 34.60 0.00 74.00 72.67 68.33 61.67 39.33 39.33 48.00 0.00 64.67 58.33 62.33 62.00 26.00 70.33 59.30 77.67 86.67 82.33 3.33 63.00 43.10 43.27 16.72 7.66 34.11 49.67 71. 88.33 90.67 90.33 88.67 87.86 89.74 87.95 64.33 53.00 87.00 84.33 86.00 79.33 7.00 88.85 82.33 84.33 80.00 84.00 74.00 81.67 83.00 75.60 78.30 52.00 0.00 80.67 89.33 82.00 65.33 68.33 76.67 81.33 0.00 88.00 68.67 77.67 84.33 35.00 89.67 63.00 88.67 86.00 83.00 2.67 84.67 65.80 60.20 15.05 0.00 67.89 69.33 89.47 64.00 92.33 89.67 91.67 85.54 81.12 78.97 45.67 38.33 53.67 57.67 74.33 79.67 72.67 85.67 75.00 88.67 54.00 77.00 78.33 66.00 85.60 83.60 79.00 16.30 0.00 72.33 80.00 69.33 58.33 47.67 20.67 55.67 0.00 79.67 68.67 74.67 72.00 29.67 90.33 54.67 88.33 92.33 90.00 6.33 77.33 50.67 53.30 0.00 21.00 44.33 42.67 91.58 31.21 53.50 10.19 54.14 22.93 30.74 31.07 29.32 25.41 6.37 11.46 19.75 1.91 4.46 44.00 7.64 6.37 9.70 0.64 5.73 0.67 10.00 7.00 4.40 3.80 0.00 32.90 33.88 25.73 23.78 28.66 0.00 4.89 0.00 31.60 22.48 28.34 27.36 11.40 36.16 27.36 46.58 48.53 44.63 3.58 39.09 21.30 28.60 0.00 4.46 24.20 1.91 36.42 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 13.00 15.40 1.77 0.76 6.57 16.00 0.00 0.00 0.00 6.82 0.00 1.26 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 5.26 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 20.78 26.49 3.27 0.00 0.00 0.00 0.00 On Path to Multimodal Generalist: General-Level and General-Bench Table 61: Results on Image Comprehension Group,#I-C-28, part C."
        },
        {
            "title": "Model",
            "content": "#I-C-28 (Object Recog) #23 #24 #25 #26 #27 #28 #29 #30 #31 #"
        },
        {
            "title": "SoTA Specialist",
            "content": "86.40 46.80 77.90 97.33 22.46 56. 23.50 27.40 85.31 91.60 GPT4V GPT4o GPT4o-mini GPT-4o-4096 ChatGPT-4o-latest Claude-3.5-Sonnet Claude-3.5-Opus Emu2-32B DetGPT InternVL2.5-8B InternVL2.5-4B InternVL2.5-2B Monkey-10B-chat DeepSeek-VL-7B-Chat Qwen2-VL-7B Qwen-VL-Chat MoE-LLAVA-Phi2-2.7B-4e-384 mPLUG-Owl2-LLaMA2-7b Phi-3.5-Vision-Instruct Cambrian-1-8B MiniGPT4-LLaMA2-7B InternVL-Chat-V1-5 Mini-InternVL-Chat-4B-V1-5 InternLM-XComposer2-VL-1.8B GPT4RoI GLaMM LLaVA-NeXT-13B LLaVA-NeXT-34B Pixtral 12B SEED-LLaMA-13B BLIP2 MiniMonkey DeepSeek-VL-7B LISA CogVLM-Chat ShareGPT4V-7B ShareGPT4V-13B BLIP-3 (XGen-MM) AnyGPT MiniCPM3-4B LaVIT-V2 (7B) GLM-VL-Chat Gemini-1.5-Pro Gemini-1.5-Flash OMG-LLaVA-InternLM20B Idefics3-8B-Llama3 NExT-GPT-V1.5 Vitron-V1 Otter Show-o NExT-Chat Yi-vision-v2 Qwen2-VL-72B 82.17 24.90 66.60 48.58 53.24 57.72 56.60 38.22 31.10 48.18 33.40 27.53 32.19 12.60 53.70 55.06 19.43 24.08 31.78 0.81 20.81 57.40 14.57 14.10 7.20 0.00 33.78 38.22 37.33 27.56 24.89 3.44 15.11 0.00 37.78 28.89 35.11 33.78 11.11 38.22 21.33 41.33 65.33 59.11 3.11 40.00 27.56 23.14 2.84 1.62 17.20 39.47 60. 49.00 59.00 44.00 57.80 51.00 49.87 50.49 38.40 19.40 39.20 49.60 39.20 52.20 99.40 43.80 39.60 60.09 57.99 46.60 35.80 52.35 45.60 33.40 26.60 41.60 0.00 34.60 36.80 27.40 18.20 26.60 0.00 41.20 0.00 46.20 34.40 42.60 41.00 9.80 25.40 17.40 22.20 47.80 43.60 4.20 36.40 14.23 17.53 57.52 57.52 42.86 57.20 49.57 98.30 73.00 98.60 98.67 87.33 89.90 89.34 84.67 79.33 100.00 100.00 100.00 100.00 100.00 95.87 95.66 98.33 90.66 97.00 72.00 2.01 94.60 79.00 63.30 39.00 0.00 89.67 92.33 85.33 80.00 88.67 4.00 96.00 0.00 93.00 88.33 91.67 89.67 54.67 93.00 84.67 91.33 97.33 94.67 5.33 93.00 82.34 87.65 57.52 91.66 85.71 92.67 93.10 95.98 97.39 96.98 97.59 96.79 96.44 93.00 58.03 38.96 47.79 78.71 61.04 91.14 60.00 95.38 84.53 83.73 60.44 83.94 87.55 67.87 86.30 83.30 77.90 84.70 0.00 82.53 87.95 70.28 48.19 58.03 51.20 64.87 0.00 81.53 75.10 79.52 73.29 27.31 81.93 50.40 85.74 94.78 92.77 5.82 90.36 75.34 70.90 30.18 27.34 89.54 85.34 96.54 260 15.50 36.67 34.55 41.65 40.69 28.45 28.77 24.60 18.04 10.23 5.95 6.14 12.86 1.93 41.84 6.52 14.40 12.28 22.70 0.00 15.43 57.30 45.80 23.40 9.40 0.00 20.35 26.10 23.22 15.74 14.40 11.80 12.28 0.00 22.26 17.66 19.96 20.92 8.60 23.61 16.70 27.63 36.67 32.63 2.50 22.80 9.60 13.90 0.00 29.17 47.58 39.16 64.86 57.90 60.10 58.20 73.30 60.23 58.60 56.15 35.56 41.82 66.48 70.56 27.78 34.81 64.26 79.25 70.37 68.36 62.11 65.19 66.00 48.32 77.20 63.50 60.30 71.40 0.00 47.59 53.33 45.93 34.07 27.96 35.46 62.83 0.00 48.89 41.85 45.00 47.41 19.07 56.30 32.59 55.00 85.93 80.37 2.96 73.89 71.40 73.64 18.36 10.56 31.03 77.91 80. 43.60 54.20 50.30 38.99 52.35 48.42 45.83 28.91 32.63 4.50 1.30 1.62 0.62 1.40 26.12 3.17 2.80 8.39 14.93 12.31 6.71 17.90 10.20 4.10 0.00 0.00 33.02 40.67 30.03 28.17 6.90 0.86 2.30 0.00 41.23 30.22 33.96 39.55 8.02 38.06 30.41 49.25 43.10 39.37 1.68 26.87 6.03 6.50 0.00 0.00 28.63 39.82 33.72 43.60 63.20 67.20 76.39 59.82 57.75 54.94 33.84 38.60 4.84 3.38 10.59 1.69 0.65 55.89 27.97 33.93 19.64 26.59 1.79 16.11 46.70 38.40 22.60 29.50 0.00 50.00 52.18 43.65 36.90 3.79 2.48 1.79 0.00 44.25 37.50 40.28 42.26 26.60 54.37 35.12 52.38 62.50 61.71 3.77 43.65 28.70 26.75 1.39 12.31 37.65 31.16 63.98 89.30 92.40 90.30 25.20 91.29 89.94 88.58 21.44 22.20 6.18 6.92 4.98 14.16 7.69 50.88 42.14 19.60 21.26 28.69 26.64 16.44 48.00 38.30 32.50 0.00 0.00 58.23 60.12 52.08 42.57 47.87 15.32 8.57 0.00 63.40 57.60 59.40 56.80 32.40 86.40 65.60 88.60 78.92 73.84 0.00 48.46 39.78 40.76 0.00 0.00 44.36 47.58 56.63 53.30 80.00 79.10 91.00 87.60 76.20 72.80 66.00 61.60 52.20 52.20 60.00 60.00 50.00 44.30 29.50 20.23 20.95 34.62 19.60 36.27 48.80 42.80 50.00 50.80 0.00 60.80 68.20 56.40 42.60 25.20 55.80 52.80 0.00 57.20 51.40 52.60 54.80 0.00 0.00 0.00 0.00 53.80 52.20 4.20 54.00 10.60 26.70 0.00 0.00 47.31 63.46 57.60 On Path to Multimodal Generalist: General-Level and General-Bench Table 62: Results on Image Comprehension Group, from #I-C-29 to #I-C-33."
        },
        {
            "title": "SoTA Specialist",
            "content": "GPT4V GPT4o GPT4o-mini GPT-4o-4096 ChatGPT-4o-latest Claude-3.5-Sonnet Claude-3.5-Opus Emu2-32B DetGPT InternVL2.5-8B InternVL2.5-4B InternVL2.5-2B Monkey-10B-chat DeepSeek-VL-7B-Chat Qwen2-VL-7B Qwen-VL-Chat MoE-LLAVA-Phi2-2.7B-4e-384 mPLUG-Owl2-LLaMA2-7b Phi-3.5-Vision-Instruct Cambrian-1-8B MiniGPT4-LLaMA2-7B InternVL-Chat-V1-5 Mini-InternVL-Chat-4B-V1-5 InternLM-XComposer2-VL-1.8B GPT4RoI GLaMM LLaVA-NeXT-13B LLaVA-NeXT-34B Pixtral 12B SEED-LLaMA-13B BLIP2 MiniMonkey DeepSeek-VL-7B LISA CogVLM-Chat ShareGPT4V-7B ShareGPT4V-13B BLIP-3 (XGen-MM) AnyGPT MiniCPM3-4B LaVIT-V2 (7B) GLM-VL-Chat Gemini-1.5-Pro Gemini-1.5-Flash OMG-LLaVA-InternLM20B Idefics3-8B-Llama3 NExT-GPT-V1.5 Vitron-V1 Otter Show-o NExT-Chat Yi-vision-v2 Qwen2-VL-72B #I-C-29 (Panoptic Seg) #I-C-30 (Pose Recog) #I-C-31 (Rel Reason) #I-C-32 (RGBD-Sem Seg) #I-C-33 (Ripe Recog) #1 64.10 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 28.96 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 48.25 0.00 0.00 71.89 0.00 0.00 0.00 0.00 0.00 #1 #1 #2 #3 63.80 30.40 25.20 67.50 72.30 64.21 29.20 68.52 67.43 65.69 18.40 13.60 6.12 20.51 10.20 13.02 6.44 20.32 18.11 12.97 15.50 20.16 20.10 14.32 22.00 19.00 17.60 40.60 0.00 0.00 0.00 0.00 0.00 59.40 15.39 9.56 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 58.31 46.72 0.43 34.28 20.60 29.40 32.66 0.55 0.00 56.57 13.00 31.37 38.24 30.31 0.00 32.91 32.52 28.57 18.60 14.00 22.57 24.27 22.61 23.69 26.08 18.21 15.70 16.30 15.90 21.07 19.63 15.04 19.30 18.30 16.40 0.00 0.00 0.00 0.00 0.00 0.00 28.20 26.75 27.69 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 27.64 21.39 0.12 12.17 3.80 8.40 0.00 0.00 0.00 0.00 0. 21.27 22.35 20.14 0.00 20.27 21.05 20.57 16.20 10.60 25.84 25.57 22.73 28.60 27.07 19.63 20.03 17.79 14.42 16.71 18.94 20.00 20.70 19.40 17.50 0.00 0.00 0.00 0.00 0.00 0.00 23.00 21.28 24.08 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 23.28 17.49 0.00 21.90 12.70 19.40 7.17 0.00 0.00 0.00 0.00 35.90 97.98 98.79 98.49 99.60 98.62 97.89 94.65 70.34 60.83 74.49 73.68 56.07 72.37 51.21 86.84 57.38 52.73 42.51 52.02 56.28 0.00 73.50 28.10 31.90 57.80 0.00 86.03 90.49 76.92 66.19 69.64 45.04 50.71 0.00 77.83 69.23 72.98 73.48 42.61 91.40 63.06 89.78 96.76 93.42 4.45 87.96 61.20 64.20 0.20 18.92 64.76 82.89 92.41 261 #1 57. 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 27.33 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 39.54 0.00 0.00 36.70 0.00 0.00 0.00 0.00 0.00 #1 54.60 90.40 90.40 96.40 91.80 94.52 91.62 91.38 72.80 68.40 99.60 99.40 97.00 100.00 100.00 92.59 52.40 50.40 60.20 83.40 51.40 51.20 77.60 32.80 51.80 51.20 0.00 90.00 92.20 86.40 71.60 85.80 8.63 90.40 0.00 90.40 84.20 87.80 87.60 53.40 92.20 74.60 94.20 98.00 96.40 38.80 78.40 48.97 51.38 41.70 49.83 12.38 88.80 77.64 On Path to Multimodal Generalist: General-Level and General-Bench Table 63: Results on Image Comprehension Group, from #I-C-34 to #I-C-37."
        },
        {
            "title": "Model",
            "content": "#I-C-34 (Safe Det) #I-C-35 (SG Gen) #I-C-36 (Scene Recog) #I-C-37 (Sign Recog) #1 #2 #3 #4 #5 #6 #7 #1 #1 #2 #"
        },
        {
            "title": "SoTA Specialist",
            "content": "97.40 50.40 47.90 48.40 48.40 76. 74.10 29.60 83.00 91.20 98.00 GPT4V GPT4o GPT4o-mini GPT-4o-4096 ChatGPT-4o-latest Claude-3.5-Sonnet Claude-3.5-Opus Emu2-32B DetGPT InternVL2.5-8B InternVL2.5-4B InternVL2.5-2B Monkey-10B-chat DeepSeek-VL-7B-Chat Qwen2-VL-7B Qwen-VL-Chat MoE-LLAVA-Phi2-2.7B-4e-384 mPLUG-Owl2-LLaMA2-7b Phi-3.5-Vision-Instruct Cambrian-1-8B MiniGPT4-LLaMA2-7B InternVL-Chat-V1-5 Mini-InternVL-Chat-4B-V1-5 InternLM-XComposer2-VL-1.8B GPT4RoI GLaMM LLaVA-NeXT-13B LLaVA-NeXT-34B Pixtral 12B SEED-LLaMA-13B BLIP2 MiniMonkey DeepSeek-VL-7B LISA CogVLM-Chat ShareGPT4V-7B ShareGPT4V-13B BLIP-3 (XGen-MM) AnyGPT MiniCPM3-4B LaVIT-V2 (7B) GLM-VL-Chat Gemini-1.5-Pro Gemini-1.5-Flash OMG-LLaVA-InternLM20B Idefics3-8B-Llama3 NExT-GPT-V1.5 Vitron-V1 Otter Show-o NExT-Chat Yi-vision-v2 Qwen2-VL-72B 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 90.40 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 86.40 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 42.40 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 78.90 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 43.80 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 38.50 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 40.40 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 42.36 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 71.80 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 71.36 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 65.24 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 68.90 0.00 0.00 0.00 0.00 0. 31.64 33.47 30.67 0.00 32.87 0.00 0.00 0.00 0.00 10.57 7.46 6.39 1.30 15.52 16.13 14.39 13.71 9.00 15.02 12.65 4.37 11.70 8.50 4.90 0.00 0.00 5.80 6.90 3.30 0.80 19.20 2.40 16.83 0.00 5.20 4.30 4.90 4.70 0.00 0.00 0.00 0.00 38.45 29.97 0.76 12.75 1.60 4.70 0.00 0.00 5.31 7.80 16.83 88.20 91.40 92.00 93.20 92.00 90.05 86.53 72.60 66.40 83.20 86.00 89.60 69.60 71.60 82.40 77.20 85.30 74.00 73.40 83.80 69.20 87.60 11.00 82.20 60.40 0.00 83.20 84.40 81.00 67.40 73.40 50.10 78.00 0.00 85.00 79.00 81.40 82.60 47.80 85.80 63.40 82.60 94.20 92.20 11.60 83.20 73.65 68.14 44.89 56.17 10.29 85.80 86.04 90.00 91.00 90.80 91.00 90.80 89.72 88.09 74.20 69.20 88.60 83.60 41.80 63.40 6.80 90.80 84.80 82.80 86.20 86.60 82.80 75.80 93.00 89.20 87.60 85.80 0.00 84.80 86.40 82.60 72.20 87.40 67.60 7.20 0.00 84.60 78.80 81.60 86.20 32.40 86.80 68.80 80.20 89.80 88.20 9.80 78.60 64.59 70.38 33.67 19.81 68.81 69.40 72.64 22.22 35.56 25.00 35.93 34.81 27.22 23.87 31.72 11.46 33.52 12.96 48.89 2.78 11.11 16.48 8.24 8.70 8.88 3.98 5.95 0.00 3.50 2.20 3.80 5.30 0.00 24.52 28.89 25.02 14.43 18.33 0.00 9.44 0.00 26.85 18.89 22.59 24.26 10.35 31.64 19.68 27.33 30.37 27.96 1.30 15.37 13.47 15.34 7.06 1.76 7.11 0.76 11.65 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 43.20 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 4.50 0.00 0.00 0.00 0.00 0.00 On Path to Multimodal Generalist: General-Level and General-Bench Table 64: Results on Image Comprehension Group, from #I-C-38 to #I-C-40."
        },
        {
            "title": "Model",
            "content": "#I-C-38 (Vis Rel Inf) #I-C-39 (Vis-Story Gen) #I-C-40 (Weather Recog) #1 #2 #3 #4 #5 #6 #1 #2 #3 #4 #5 #"
        },
        {
            "title": "SoTA Specialist",
            "content": "30.50 35.50 35.80 38.40 38.40 59. 31.90 44.60 40.20 28.90 36.50 GPT4V GPT4o GPT4o-mini GPT-4o-4096 ChatGPT-4o-latest Claude-3.5-Sonnet Claude-3.5-Opus Emu2-32B DetGPT InternVL2.5-8B InternVL2.5-4B InternVL2.5-2B Monkey-10B-chat DeepSeek-VL-7B-Chat Qwen2-VL-7B Qwen-VL-Chat MoE-LLAVA-Phi2-2.7B-4e-384 mPLUG-Owl2-LLaMA2-7b Phi-3.5-Vision-Instruct Cambrian-1-8B MiniGPT4-LLaMA2-7B InternVL-Chat-V1-5 Mini-InternVL-Chat-4B-V1-5 InternLM-XComposer2-VL-1.8B GPT4RoI GLaMM LLaVA-NeXT-13B LLaVA-NeXT-34B Pixtral 12B SEED-LLaMA-13B BLIP2 MiniMonkey DeepSeek-VL-7B LISA CogVLM-Chat ShareGPT4V-7B ShareGPT4V-13B BLIP-3 (XGen-MM) AnyGPT MiniCPM3-4B LaVIT-V2 (7B) GLM-VL-Chat Gemini-1.5-Pro Gemini-1.5-Flash OMG-LLaVA-InternLM20B Idefics3-8B-Llama3 NExT-GPT-V1.5 Vitron-V1 Otter Show-o NExT-Chat Yi-vision-v2 Qwen2-VL-72B 5.50 7.40 9.40 6.60 6.85 16.55 17.30 8.90 4.70 7.11 6.83 9.49 4.90 6.95 23.84 11.32 10.51 10.67 13.70 6.89 10.10 6.80 5.80 10.50 12.10 7.10 6.50 8.60 7.30 4.50 7.10 4.20 6.40 0.00 7.80 5.70 6.80 6.30 5.60 5.80 6.40 8.20 8.80 8.40 4.90 5.60 3.80 4.20 0.00 0.20 4.32 7.50 21.91 18.35 21.11 19.27 22.17 21.20 25.80 25.71 19.34 11.49 14.11 16.61 14.07 8.22 12.62 26.58 17.84 15.77 15.99 18.43 22.50 20.11 11.80 10.90 14.10 14.80 0.00 17.24 18.86 15.86 8.82 17.40 10.62 9.56 0.00 16.82 14.49 15.36 17.12 8.21 18.67 7.45 17.32 19.70 17.80 14.20 18.30 15.60 17.10 0.00 0.50 23.21 10.34 33.21 16.17 14.80 13.82 14.42 16.58 21.99 24.48 16.20 3.80 13.34 12.62 14.73 10.25 15.52 55.32 17.88 19.26 20.97 44.40 36.91 38.80 13.70 6.90 14.10 7.20 0.00 10.60 14.40 12.20 5.80 3.60 6.84 14.55 0.00 15.23 10.34 12.49 14.05 2.70 16.40 4.70 16.10 16.10 13.80 7.50 11.20 6.30 7.90 0.00 0.30 0.00 11.96 53.45 61.30 72.60 60.40 18.60 62.83 63.80 64.19 19.28 19.27 4.92 4.60 3.22 8.22 4.42 1.46 0.57 0.00 0.00 0.00 0.40 0.67 35.80 52.80 48.00 0.00 0.00 47.12 52.74 44.08 41.78 43.16 7.37 5.98 0.00 54.29 45.37 48.46 51.82 0.00 57.40 23.70 63.90 56.68 54.72 0.00 39.24 40.95 48.76 0.00 0.00 0.00 5.71 11.34 20.98 22.16 20.35 23.81 24.12 26.38 28.16 20.40 19.60 18.40 13.34 14.37 9.44 15.32 24.67 19.38 14.18 18.87 20.60 19.81 19.07 14.10 12.10 19.90 9.50 0.00 18.40 21.20 17.60 8.60 21.60 11.50 13.83 0.00 19.41 15.40 17.62 16.23 4.60 18.60 9.60 20.20 23.80 16.90 11.70 17.70 10.68 12.49 0.00 0.50 16.58 17.61 25.08 19.34 23.56 19.37 25.81 23.21 27.71 30.27 21.37 16.62 16.87 13.26 17.61 24.27 16.80 35.58 19.69 17.18 20.44 29.80 24.60 20.88 12.20 9.20 26.60 25.90 0.00 17.89 23.32 19.92 14.04 26.80 22.73 14.51 0.00 16.73 13.27 15.69 14.25 8.55 19.03 13.96 17.85 22.50 23.30 13.30 22.30 36.40 34.73 0.00 0.40 19.63 53.72 39. 17.76 22.65 18.05 23.42 23.76 26.98 25.69 19.62 11.36 18.48 13.26 21.18 17.09 17.72 27.09 14.58 23.71 16.80 26.54 19.72 22.74 12.20 10.70 30.10 24.70 0.00 18.44 21.24 17.72 9.48 29.80 15.62 13.92 0.00 18.49 13.32 15.20 16.38 7.09 19.86 9.66 17.64 24.80 20.40 12.60 19.70 23.75 26.89 0.00 0.70 23.22 14.58 39.89 16.50 18.90 14.16 19.46 16.61 21.70 18.34 9.20 6.60 12.42 9.25 6.67 5.75 8.36 23.99 14.58 11.31 12.90 21.40 17.70 19.44 8.10 7.20 11.50 8.30 0.00 13.60 14.30 12.10 8.80 13.20 8.17 7.29 0.00 13.60 9.25 11.34 12.27 6.80 16.90 8.70 13.70 17.70 15.40 6.80 17.90 36.40 38.69 0.00 0.90 21.97 19.64 26.20 15.81 18.32 16.27 19.11 17.25 25.79 26.28 23.04 17.68 18.40 6.43 17.45 12.30 11.28 29.24 14.13 13.36 18.40 28.71 20.72 21.60 8.50 6.70 18.40 23.40 0.00 16.34 20.25 18.57 10.01 24.80 15.04 10.28 0.00 18.48 14.54 15.90 17.43 4.60 17.48 9.91 19.59 17.20 15.10 5.90 13.10 8.90 9.60 0.00 0.90 30.85 11.50 30.47 16.97 16.40 16.71 16.84 16.22 16.58 20.53 11.00 8.50 8.96 8.91 12.38 8.31 12.72 29.24 15.07 15.04 12.93 28.77 20.10 24.50 10.40 9.50 13.70 9.60 0.00 10.10 11.50 11.20 9.20 9.80 7.41 11.85 0.00 11.87 9.24 9.57 10.24 5.70 13.10 6.90 12.40 15.80 14.70 6.60 15.60 27.41 28.96 0.00 0.80 29.83 10.37 29.47 16.95 16.46 16.37 27.33 16.10 20.75 20.04 9.80 8.70 9.79 8.49 11.77 2.52 12.48 29.18 15.42 14.85 12.30 33.04 19.80 22.18 10.30 9.90 13.20 9.70 0.00 11.60 12.30 13.50 8.70 9.60 3.40 10.31 0.00 11.98 9.67 10.57 12.83 7.50 10.50 9.80 14.20 16.00 14.40 4.80 11.90 6.30 7.80 0.00 0.80 29.41 11.73 28.34 82.02 84.84 87.88 84.65 85.66 78.18 85.19 84.65 56.97 48.29 57.17 43.84 51.72 74.95 62.63 78.54 76.96 51.52 70.10 71.31 63.03 55.15 75.50 68.60 46.00 58.90 0.00 64.65 73.94 68.69 51.72 60.81 44.65 65.05 0.00 63.03 57.78 61.82 61.41 41.21 74.14 54.14 70.91 83.23 80.40 4.85 66.06 51.30 59.07 21.66 17.77 59.29 64.44 62.83 On Path to Multimodal Generalist: General-Level and General-Bench Image Generation Results. The complete results of all models on image generation are presented in Table 65 to Table 68. Table 65: Results on Image Generation Group, from #I-G-1 to #I-G-3."
        },
        {
            "title": "Model",
            "content": "#I-G-1 #I-G-2 (Edge2Img Gen) (EEG2Img Gen) #I-G-3 (Img Denose) #1 #1 #1 #2 #3 #4 #5 #6 #"
        },
        {
            "title": "SoTA Specialist",
            "content": "18.70 45.40 34.56 34.55 31.45 33. 34.60 42.05 25.28 Emu2-32B SEED-LLaMA-13B"
        },
        {
            "title": "AnyGPT",
            "content": "LaVIT-V2 (7B) Next-GPT-V1.5 Vitron-V1 Show-o 93.52 127. 158.21 79.79 49.71 19.78 0. 0.00 0.00 0.00 0.00 0.00 0. 15.77 19.91 20.31 20.57 23.46 143. 15.29 16.85 18.45 18.72 19.71 170. 17.69 15.93 19.46 20.04 20.45 189. 18.76 20.87 21.59 23.20 24.02 111. 10.45 11.36 13.62 14.59 12.68 16. 0.00 0.00 0.00 6.38 7.56 0. 0.00 0.00 19.55 76.86 0.00 0. 0.00 0.00 0.00 0.00 0.00 0. 0.00 Table 66: Results on Image Generation Group, from #I-G-4 to #I-G-8."
        },
        {
            "title": "Model",
            "content": "#I-G-4 #I-G-5 #I-G-6 #I-G-7 #I-G-8 (Img Enhance) (Img Inpaint) (Img-Style Trans) (Img2Mask Gen) (Img2Sketch Gen) #1 #2 #3 #4 #5 #6 #1 #1 #2 #1 #"
        },
        {
            "title": "SoTA Specialist",
            "content": "3.29 27.35 24.49 0.54 22.16 19.97 4.86 16.73 31.26 99.29 15.06 Emu2-32B 19.21 14.37 0.00 17.58 0.00 101.80 147.95 15.95 SEED-LLaMA-13B 15.77 13.17 0.00 15.09 0.00 127. 179.87 16."
        },
        {
            "title": "AnyGPT",
            "content": "LaVIT-V2 (7B) Next-GPT-V1.5 Vitron-V1 Show-o 0.00 0. 0.00 0.00 0.00 47.30 23.47 15.92 14.67 0.00 13.40 15.81 117. 214.33 17.49 19.67 16.50 0.00 14.79 17.25 149.78 98.58 19.88 20.45 2.63 0.00 0.00 0. 0.00 75.71 65.89 16.51 15.89 3.78 17.86 0.15 6.90 0.00 32. 51.48 19.17 86.53 0.00 0.00 0.00 0.00 0.00 0.00 0.00 264 On Path to Multimodal Generalist: General-Level and General-Bench Table 67: Results on Image Generation Group, from #I-G-9 to #I-G-14. #I-G- #I-G-10 #I-G-11 #I-G-12 #I-G-13 #I-G-"
        },
        {
            "title": "Model",
            "content": "(Img Edit) (Layout2Img Gen) (Mask2Img Gen) (Sketch2Img Gen) (Sound2Img Gen) (Text-based Img Edit) #1 #2 #3 #1 #1 #1 #2 #3 #4 #1 #1 #2 #3 #"
        },
        {
            "title": "SoTA Specialist",
            "content": "19.27 70.90 69.30 16.47 25.33 15.78 64.50 67.32 28.10 20.35 31.38 65.80 70.10 102. Emu2-32B 22.72 50.17 48.63 118.55 SEED-LLaMA-13B 14.24 37.01 39.28 87."
        },
        {
            "title": "AnyGPT",
            "content": "16.49 37.17 32.98 108.06 LaVIT-V2 (7B) 17.81 60.78 60.61 NExT-GPT-V1.5 2.60 45.38 36. 15.43 14.58 14.91 15.79 6.53 162.90 201.94 232.23 19. 159.50 238.79 289.85 13.17 176.46 275.20 305.26 16.65 123.75 246.68 256.77 18.96 0.00 0.00 0. 0.00 168.23 0.00 60.13 160.10 0.00 47.17 177.20 0.00 34.87 144.22 0.00 56.51 32.67 89.39 75.86 15. 12.45 76.59 38.65 60.69 Vitron-V1 Show-o 3.70 56.49 53.44 17. 13.76 36.45 57.59 16.34 0.00 0.00 0.00 0.00 0.00 0.00 0.00 89.62 43.78 59.78 0.00 0.00 89. 86.45 24.89 Table 68: Results on Image Generation Group, from #I-G-15."
        },
        {
            "title": "Model",
            "content": "#I-G-15 (Txt2Img Gen) #1 #2 #3 #4 #5 #6 #7 #8 #9 #10 #"
        },
        {
            "title": "SoTA Specialist",
            "content": "19.86 32.57 10.58 15.66 177.30 9. 31.20 27.86 29.91 12.52 30.07 Emu2-32B SEED-LLaMA-13B"
        },
        {
            "title": "AnyGPT",
            "content": "LaVIT-V2 (7B) NExT-GPT-V1.5 Vitron-V1 Show-o 87.59 22. 71.44 78.91 26.53 84.25 94.82 17. 88.13 66.93 21.05 65.39 167.91 97.15 18.26 15. 14.59 126.80 17.40 224.63 74.42 16. 16.37 15.77 133.85 16.91 243.63 66. 17.97 15.55 16.78 127.49 18.21 181. 57.33 21.22 16.48 17.44 101.92 19. 77.38 26.75 70.14 102.24 234.65 86. 24.65 16.58 14.26 125.86 20.58 49. 21.38 46.56 95.68 203.86 73.64 19. 13.64 10.97 93.78 18.63 120.36 9. 119.19 220.46 290.44 199.99 10.44 7. 8.2 181.22 9.73 265 On Path to Multimodal Generalist: General-Level and General-Bench B.2 Results of Video-related Tasks Video Comprehension Results. The complete results of all models on video comprehension are presented from Table 69 to Table 81. Table 69: Results on Video Comprehension Group, from #V-C-1 (a)."
        },
        {
            "title": "Model",
            "content": "#V-C-1 (Video QA) #1 #2 #3 #4 #5 #6 #7 #8 #"
        },
        {
            "title": "SoTA Specialist",
            "content": "27.70 9.80 17.80 20.50 24.80 25. 26.40 25.20 21.20 InternVL-2.5-8B InternVL-2-26B InternVL-2.5-26B Qwen2-VL-7B Qwen2-VL-72B DeepSeek-VL-2-small DeepSeek-VL-2 LLaVA-One-Vision-7B LLaVA-One-Vision-72B Sa2VA-8B Sa2VA-26B CoLVA-2B CoLVA-4B Long-LLaVA-9B NExT-GPT-V1.5 Vitron-V1 InternVL-2-8B VidAgent 21.10 40.00 25.60 27.70 27.70 9.70 8.50 18.70 18.50 24.40 25.30 17.10 19.60 41.30 7.50 7.20 19.20 24.80 7.50 14.10 8.20 7.40 9.80 7.30 6.50 10.50 9.40 10.30 10.90 6.90 7.30 14.50 4.60 4.80 7.30 10. 13.10 31.90 15.10 16.40 17.80 2.60 5.00 6.40 3.10 25.30 26.10 11.60 13.00 33.20 1.50 3.70 12.90 25.90 12.40 34.50 19.50 19.80 20.50 6.10 5.80 12.60 12.30 21.20 22.60 11.50 12.20 39.10 5.60 5.80 13.20 22.20 16.20 20.30 22.30 22.30 25.80 10.40 11.30 11.50 17.10 12.60 15.20 15.30 16.10 16.50 15.70 12.40 15.30 13.60 24.50 16.10 26.40 25.10 27.20 7.60 10.50 10.20 15.80 13.40 14.90 21.80 24.40 13.60 5.60 6.30 23.50 15.40 21.70 22.60 25.10 25.20 26.10 8.40 8.20 9.40 15.10 18.40 19.60 19.80 21.50 19.60 8.70 8.90 19.30 20.40 18.50 18.60 23.70 16.80 24.50 10.70 10.10 12.20 17.60 14.80 17.80 17.20 18.20 18.60 13.40 14.10 18.00 16. 14.90 23.50 17.80 15.70 18.90 9.30 8.60 8.60 16.60 19.60 20.10 13.90 14.80 18.60 13.40 15.20 14.90 20.30 Table 70: Results on Video Comprehension Group, from #V-C-1 (b)."
        },
        {
            "title": "Model",
            "content": "#V-C-1 (Video QA) #10 #11 #12 #13 #14 #15 #16 #"
        },
        {
            "title": "SoTA Specialist",
            "content": "100.00 100.00 85.50 20.40 32.10 18. 63.10 17.90 InternVL-2.5-8B InternVL-2-26B InternVL-2.5-26B Qwen2-VL-7B Qwen2-VL-72B DeepSeek-VL-2-small DeepSeek-VL-2 LLaVA-One-Vision-7B LLaVA-One-Vision-72B Sa2VA-8B Sa2VA-26B CoLVA-2B CoLVA-4B Long-LLaVA-9B NExT-GPT-V1.5 Vitron-V1 InternVL-2-8B VidAgent 100.00 100.00 100.00 100.00 100.00 74.30 53.50 100.00 100.00 100.00 100.00 100.00 100.00 100.00 83.50 100.00 100.00 100.00 100.00 100.00 100.00 100.00 100.00 100.00 100.00 100.00 100.00 100.00 100.00 100.00 100.00 100.00 100.00 98.00 99.00 100.00 80.20 82.70 83.30 85.50 86.90 64.00 72.50 64.60 70.00 80.80 83.10 76.00 80.00 76.20 64.50 73.80 78.6 82. 18.50 13.70 22.50 20.40 22.30 7.40 4.80 4.50 6.20 12.70 16.80 17.20 18.10 11.50 3.20 3.70 17.50 14.20 27.40 14.50 31.00 32.10 32.30 2.10 1.90 11.60 13.60 13.30 20.40 16.60 26.50 12.90 8.90 7.10 27.90 24.30 14.00 12.40 19.20 18.20 19.40 13.60 1.50 6.80 13.50 7.70 14.10 8.60 13.10 10.10 1.20 3.50 14.70 10.70 58.20 79.30 69.20 64.40 70.20 49.30 46.70 69.00 83.70 77.70 79.60 54.20 56.30 72.30 51.70 56.70 58.70 75.70 15.40 16.20 20.60 18.30 20.40 8.40 10.40 10.10 17.40 12.10 14.10 12.60 14.50 16.40 4.30 5.20 15.70 12.30 On Path to Multimodal Generalist: General-Level and General-Bench Table 71: Results on Video Comprehension Group, from #V-C-2 (a)."
        },
        {
            "title": "Model",
            "content": "#V-C-2 (Vid-Obj Recog) #1 #2 #3 #4 #5 #6 #7 #8 #"
        },
        {
            "title": "SoTA Specialist",
            "content": "11.10 21.00 29.10 InternVL-2.5-8B InternVL-2-26B InternVL-2.5-26B Qwen2-VL-7B Qwen2-VL-72B DeepSeek-VL-2-small DeepSeek-VL-2 LLaVA-One-Vision-7B LLaVA-One-Vision-72B Sa2VA-8B Sa2VA-26B CoLVA-2B CoLVA-4B Long-LLaVA-9B NExT-GPT-V1.5 Vitron-V1 InternVL-2-8B VidAgent 8.50 17.10 13.20 13.10 15.80 12.00 9.00 13.80 16.70 10.00 12.80 7.20 8.10 16.60 4.60 3.80 7.80 11.30 21.50 24.50 25.10 20.30 24.80 10.70 8.70 22.30 24.20 9.20 12.50 17.70 20.10 29.30 6.10 7.20 20.50 13. 22.10 2.60 25.50 29.10 32.10 0.00 2.90 11.50 17.40 0.00 0.00 17.40 20.00 1.50 0.00 0.00 20.80 14.20 4.50 3.90 8.40 4.60 4.50 4.50 4.80 6.50 5.00 8.80 5.30 6.40 3.90 3.70 9.40 1.20 2.40 4.20 6.30 65.90 10.50 66. 64.10 76.60 56.20 88.90 65.20 65.90 65.20 18.60 30.00 46.50 50.50 0.00 0.00 50.20 55.40 4.00 12.10 14.00 55.80 53.20 8.10 15.00 13.20 10.50 12.20 8.70 8.50 5.00 6.90 12.70 13.20 6.50 7.10 14.00 5.30 6.10 7.10 6.50 62.20 78.90 75.30 61.30 73.20 50.00 56.70 51.10 80.00 80.00 82.40 56.00 60.00 72.20 64.80 70.10 60.70 63.20 60.80 75.00 70.20 63.30 74.50 50.00 40.00 53.00 80.00 76.00 78.50 54.40 58.80 67.00 53.10 68.30 60.60 74. 77.90 82.20 80.20 63.20 70.50 44.40 50.00 52.20 77.80 80.00 80.20 71.60 75.00 71.10 68.40 77.10 75.60 80.00 Table 72: Results on Video Comprehension Group, from #V-C-2 (b)."
        },
        {
            "title": "Model",
            "content": "#V-C-2 (Vid-Obj Recog) #10 #11 #12 #13 #14 #15 #16 #"
        },
        {
            "title": "SoTA Specialist",
            "content": "60.10 57.70 23.00 24.70 92.80 91. 71.30 73.80 InternVL-2.5-8B InternVL-2-26B InternVL-2.5-26B Qwen2-VL-7B Qwen2-VL-72B DeepSeek-VL-2-small DeepSeek-VL-2 LLaVA-One-Vision-7B LLaVA-One-Vision-72B Sa2VA-8B Sa2VA-26B CoLVA-2B CoLVA-4B InternVL-2-26B Long-LLaVA-9B NExT-GPT-V1.5 Vitron-V1 InternVL-2-8B VidAgent 55.30 65.70 65.70 58.10 67.20 36.70 42.20 58.90 73.30 63.30 65.60 50.40 53.30 72.20 66.70 57.20 60.80 55.60 63.30 50.70 59.40 59.40 52.10 62.00 46.70 42.20 66.70 74.40 80.00 82.20 48.20 50.10 80.00 65.60 64.70 65.20 50.90 80.00 18.60 22.00 22.00 17.90 22.90 5.40 10.50 7.40 12.50 6.00 8.00 14.40 17.60 3.70 15.00 10.70 11.90 19.60 6. 22.40 24.60 24.60 21.30 24.50 8.90 14.90 6.40 9.80 4.40 5.80 19.80 20.50 4.80 13.80 9.60 11.30 21.40 14.40 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 On Path to Multimodal Generalist: General-Level and General-Bench Table 73: Results on Video Comprehension Group, from #V-C-3 (a)."
        },
        {
            "title": "SoTA Specialist",
            "content": "InternVL-2.5-8B InternVL-2-26B InternVL-2.5-26B Qwen2-VL-7B Qwen2-VL-72B DeepSeek-VL-2-small DeepSeek-VL-2 LLaVA-One-Vision-7B LLaVA-One-Vision-72B Sa2VA-8B Sa2VA-26B CoLVA-2B CoLVA-4B InternVL-2-26B Long-LLaVA-9B NExT-GPT-V1.5 Vitron-V1 InternVL-2-8B VidAgent #V-C-3 (Vid Act Recog) #1 #2 #3 #4 #5 #6 #7 #8 17. 13.20 19.40 19.40 14.10 20.80 7.40 14.60 10.10 20.60 17.80 18.80 11.30 12.40 20.20 21.90 16.50 13.60 12.20 12.30 14.60 10.50 13.70 13.70 9.80 12.90 4.30 7.90 8.60 16.20 22.40 22.60 9.20 9.90 20.50 18.90 14.90 16.20 10.60 14.40 20.10 14.20 19.80 19.80 13.90 20.20 7.90 14.00 11.20 22.10 22.50 24.10 10.90 13.00 25.50 21.00 16.50 17.40 14.20 12.30 17. 15.10 15.80 15.80 14.80 16.90 14.10 17.70 10.60 17.10 22.40 24.60 13.90 14.30 24.40 20.00 10.70 13.50 15.10 22.30 21.30 17.40 20.80 20.80 18.40 23.00 8.50 10.80 11.10 23.00 21.90 23.10 14.40 16.60 20.60 18.70 17.60 17.20 17.30 20.80 13.50 6.80 12.40 12.40 9.00 15.90 6.30 15.50 10.80 15.80 19.80 20.40 5.30 6.10 27.30 22.40 11.60 12.00 6.80 19.80 17. 14.90 17.30 17.30 15.20 17.80 14.30 21.00 9.90 17.40 25.30 26.40 12.20 13.80 29.10 23.10 8.70 9.10 13.80 22.30 26.00 22.60 24.10 24.10 13.50 18.90 13.70 19.60 13.70 21.50 27.50 28.20 20.00 21.10 23.30 21.60 12.60 10.80 21.50 23.50 Table 74: Results on Video Comprehension Group, from #V-C-3 (b)."
        },
        {
            "title": "SoTA Specialist",
            "content": "InternVL-2.5-8B InternVL-2-26B InternVL-2.5-26B Qwen2-VL-7B Qwen2-VL-72B DeepSeek-VL-2-small DeepSeek-VL-2 LLaVA-One-Vision-7B LLaVA-One-Vision-72B Sa2VA-8B Sa2VA-26B CoLVA-2B CoLVA-4B InternVL-2-26B Long-LLaVA-9B NExT-GPT-V1.5 Vitron-V1 InternVL-2-8B VidAgent #V-C-3 (Vid Act Recog) #9 #10 #11 #12 #13 #14 #15 #16 18. 15.70 17.40 17.40 14.90 19.20 8.70 14.80 10.90 18.40 23.20 23.60 12.80 13.60 21.00 21.10 14.20 16.50 14.90 20.30 24.90 22.00 25.40 25.40 22.80 26.20 13.20 17.10 12.70 22.50 26.50 26.80 18.90 19.80 21.80 22.20 6.30 7.40 22.30 24.50 20.40 15.80 21.20 21.20 14.80 22.30 7.40 12.20 10.90 20.70 20.00 20.80 13.20 15.40 19.40 21.00 9.20 10.50 16.80 20.40 61. 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.10 0.20 0.00 0.00 0.00 0.00 0.00 0.40 0.00 0.00 0.00 0.00 17.20 16.80 24.30 24.30 18.40 25.90 15.50 11.50 15.00 26.90 11.30 13.10 14.20 16.20 13.60 12.20 8.50 7.80 15.80 12.30 6.50 14.20 18.90 18.90 13.60 19.80 14.20 4.90 3.20 12.50 7.40 8.80 11.90 13.70 7.60 9.70 4.10 4.70 14.80 8.80 19. 14.30 23.40 23.40 13.80 21.10 16.40 5.30 12.70 19.60 0.00 0.00 13.10 14.00 19.10 0.00 0.00 0.00 14.00 13.00 24.80 18.70 25.40 25.40 19.20 28.70 0.80 6.40 8.00 30.40 0.00 0.00 15.30 16.90 37.60 0.00 0.00 0.00 17.70 15.30 268 On Path to Multimodal Generalist: General-Level and General-Bench Table 75: Results on Video Comprehension Group, from #V-C-4 to #V-C-5."
        },
        {
            "title": "Model",
            "content": "#V-C-4 (Vid Understand) #V-C-5 (IW VOS) #1 #2 #3 #4 #5 #6 #7 #8 #9 #1 #2 #3 #"
        },
        {
            "title": "SoTA Specialist",
            "content": "23.80 21.50 26.70 23.00 25.90 14.50 16.70 25.80 29.60 78.30 82.90 86.60 79.60 InternVL-2.5-8B InternVL-2.5-26B Qwen2-VL-7B Qwen2-VL-72B DeepSeek-VL-2-small DeepSeek-VL-2 LLaVA-One-Vision-7B LLaVA-One-Vision-72B Sa2VA-8B Sa2VA-26B CoLVA-2B CoLVA-4B Long-LLaVA-9B NExT-GPT-V1.5 Vitron-V1 InternVL-2-8B VidAgent 8.40 9.70 5.00 6.70 4.10 5.80 4.60 6.90 6.40 9.10 9.20 13.40 12.10 12.30 24.40 19.30 30.00 14.90 32.20 16. 22.60 18.70 21.20 19.30 22.90 14.20 19.80 22.40 24.90 19.60 24.20 22.10 24.10 10.30 15.80 24.20 28.90 19.70 17.90 24.80 20.10 21.90 13.90 16.80 25.80 22.20 24.80 25.10 21.80 26.70 11.70 14.00 25.30 32.70 5.40 9.10 15.30 5.80 10.50 14.40 10.10 16.60 13.20 16.20 6.10 9.10 3.40 6.30 7.50 7.40 11.60 5.80 8.30 8.70 5.00 5.30 4.30 5.60 4.30 5.80 6.60 6.40 6.90 6.20 12.40 16.90 19.70 18.20 13.70 19.30 17.30 18.20 20.10 17.70 21.00 18.70 20.40 12.90 18.70 21.20 13.90 14.70 14.00 16.80 14.70 16.20 22.40 14.30 12.80 3.50 0.00 13.40 1.40 14.50 5.00 3.90 0.00 14.10 2.30 13.60 5.70 14.20 18.80 17.40 22.30 17.60 20.20 19.30 20.30 20.30 14.60 32.20 15.40 13.60 14.30 15.30 12.30 4.30 9.20 2.00 3.80 4.00 5.60 5.40 7.90 3.40 3.70 8.40 4.30 7.80 6.20 5.40 5.70 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0. 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 59.80 60.20 54.20 36.40 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 Table 76: Results on Video Comprehension Group, from #V-C-6 to #V-C-9."
        },
        {
            "title": "Model",
            "content": "#V-C-6 (General VOS) #V-C-7 (Street VOS) #V-C-8 (RVOS) #V-C-9 (ReVOS) #1 #2 #3 #4 #1 #2 #3 #1 #2 #3 #1 #"
        },
        {
            "title": "SoTA Specialist",
            "content": "92.60 88.60 89.40 71.10 64.70 50. 48.40 69.70 72.40 52.40 40.70 40. InternVL-2.5-8B InternVL-2.5-26B Qwen2-VL-7B Qwen2-VL-72B DeepSeek-VL-2-small DeepSeek-VL-2 LLaVA-One-Vision-7B LLaVA-One-Vision-72B Sa2VA-8B Sa2VA-26B CoLVA-2B CoLVA-4B InternVL-2-26B Long-LLaVA-9B NExT-GPT-V1.5 Vitron-V1 InternVL-2-8B VidAgent 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 12.70 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 20.40 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 6.80 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 80.40 0.00 0.00 0.00 0.00 0.00 0.00 68.40 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 73.60 0.00 0.00 0.00 0.00 0.00 0.00 63.80 0.00 0. 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 59.10 0.00 0.00 0.00 0.00 0.00 0.00 50.30 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 55.00 0.00 0.00 0.00 0.00 0.00 0.00 45.20 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 46.90 0.00 0.00 0.00 0.00 0.00 0.00 41.30 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 74.20 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 64.10 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 45.30 0.00 0. 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 63.10 0.00 0.00 269 On Path to Multimodal Generalist: General-Level and General-Bench Table 77: Results on Video Comprehension Group, from #V-C-10 to #V-C-13."
        },
        {
            "title": "Model",
            "content": "#V-C-10 (Temp Act Det) #V-C-11 (C-ReVOS) #V-C-12 (Vid-Ground) #V-C-13 (Vid-Depth Est) #1 #2 #1 #2 #3 #4 #5 #1 #2 #3 #1 #2 #3 #"
        },
        {
            "title": "SoTA Specialist",
            "content": "26.00 35.60 40.20 73.00 35.10 38.70 42.20 31.30 16.30 21.60 0.27 0.12 0.07 0.10 InternVL-2.5-8B InternVL-2.5-26B Qwen2-VL-7B Qwen2-VL-72B DeepSeek-VL-2-small DeepSeek-VL-2 LLaVA-One-Vision-7B LLaVA-One-Vision-72B Sa2VA-8B Sa2VA-26B CoLVA-2B CoLVA-4B InternVL-2-26B Long-LLaVA-9B NExT-GPT-V1.5 Vitron-V1 InternVL-2-8B VidAgent 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 6.70 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 3.60 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0. 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 58.40 86.20 41.20 58.90 56.70 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 51.70 81.50 33.60 36.90 53.90 36.40 15.70 10.80 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0. 0.00 0.00 0.00 0.00 0.00 0.00 Table 78: Results on Video Comprehension Group, from #V-C-14."
        },
        {
            "title": "SoTA Specialist",
            "content": "InternVL-2.5-8B InternVL-2.5-26B Qwen2-VL-7B Qwen2-VL-72B DeepSeek-VL-2-small DeepSeek-VL-2 LLaVA-One-Vision-7B LLaVA-One-Vision-72B Sa2VA-8B Sa2VA-26B CoLVA-2B CoLVA-4B InternVL-2-26B Long-LLaVA-9B NExT-GPT-V1.5 Vitron-V1 InternVL-2-8B VidAgent #V-C-14 (Obj Match) #1 #2 #3 #4 #5 #6 #7 #8 76. 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 24.70 43.80 68.80 76.60 0.00 0.00 0.00 0.00 0.00 0.00 50.60 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 11.60 16.80 50.60 41.70 0.00 0.00 0.00 0.00 0.00 0.00 51.20 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 24.00 31.20 49.60 51.20 0.00 0.00 0.00 0.00 0.00 0.00 39. 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 23.30 31.80 33.50 39.80 0.00 0.00 0.00 0.00 0.00 0.00 46.80 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 18.50 31.00 38.40 46.80 0.00 0.00 0.00 0.00 0.00 0.00 40.90 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 13.60 24.80 40.90 38.30 0.00 0.00 0.00 0.00 0.00 0.00 31. 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 20.70 14.30 31.00 31.00 0.00 0.00 0.00 0.00 0.00 0.00 47.60 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 22.40 33.60 47.70 41.10 0.00 0.00 0.00 0.00 0.00 0.00 270 On Path to Multimodal Generalist: General-Level and General-Bench Table 79: Results on Video Comprehension Group, from #V-C-15 to #V-C-16."
        },
        {
            "title": "Model",
            "content": "#V-C-15 (Obj Track) #V-C-16 (Long-Vid Track) #1 #2 #3 #4 #5 #6 #7 #1 #2 #3 #4 #"
        },
        {
            "title": "SoTA Specialist",
            "content": "66.60 81.80 71.10 69.50 72.20 61. 60.40 76.30 18.30 82.90 80.20 60. InternVL-2.5-8B InternVL-2.5-26B Qwen2-VL-7B Qwen2-VL-72B DeepSeek-VL-2-small DeepSeek-VL-2 LLaVA-One-Vision-7B LLaVA-One-Vision-72B Sa2VA-8B Sa2VA-26B CoLVA-2B CoLVA-4B InternVL-2-26B Long-LLaVA-9B NExT-GPT-V1.5 Vitron-V1 InternVL-2-8B VidAgent 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 22.60 24.10 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 77.80 78.30 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 56.70 59.10 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 49.30 49.60 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 38.20 38.10 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0. 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 14.30 15.60 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 5.90 7.60 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 61.20 63.30 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 4.60 5.10 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 71.80 71.60 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 69.20 69.60 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0. 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 25.00 25.90 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 Table 80: Results on Video Comprehension Group, from #V-C-17 to #V-C-18."
        },
        {
            "title": "Model",
            "content": "#V-C-17 (UAV Track) #V-C-18 (UW Track) #1 #2 #3 #4 #5 #1 #2 #3 #4 #"
        },
        {
            "title": "SoTA Specialist",
            "content": "80.60 78.50 59.90 84.20 81.90 78. 78.90 77.10 76.90 63.90 InternVL-2.5-8B InternVL-2.5-26B Qwen2-VL-7B Qwen2-VL-72B DeepSeek-VL-2-small DeepSeek-VL-2 LLaVA-One-Vision-7B LLaVA-One-Vision-72B Sa2VA-8B Sa2VA-26B CoLVA-2B CoLVA-4B InternVL-2-26B Long-LLaVA-9B NExT-GPT-V1.5 Vitron-V1 InternVL-2-8B VidAgent 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 56.80 57.80 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0. 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 47.70 48.10 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 25.20 25.40 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 23.80 24.70 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 59.40 59.60 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 44.50 44.70 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 58.50 58.60 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0. 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 64.80 65.30 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 43.20 43.70 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 29.10 29.80 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 271 On Path to Multimodal Generalist: General-Level and General-Bench Table 81: Results on Video Comprehension Group, from #V-C-19 to #V-C-20."
        },
        {
            "title": "SoTA Specialist",
            "content": "InternVL-2.5-8B InternVL-2.5-26B Qwen2-VL-7B Qwen2-VL-72B DeepSeek-VL-2-small DeepSeek-VLLLaVA-One-Vision-7B LLaVA-One-Vision-72B Sa2VA-8B Sa2VA-26B CoLVA-2B CoLVA-4B InternVL-2-26B Long-LLaVA-9B NExT-GPT-V1.5 Vitron-V1 InternVL-2-8B"
        },
        {
            "title": "VidAgent",
            "content": "#V-C-19 (UAV Track) #V-C-20 (UW Track) #1 #2 #3 #1 #2 #3 #4 49. 39.40 23.00 22.70 71.80 48.51 32. 19.40 21.20 19.90 22.80 6.10 12. 5.10 6.80 5.90 6.80 14.40 16. 3.80 16.80 0.20 0.10 19.4 15. 0.00 0.00 0.00 0.00 0.00 0. 0.00 0.00 0.00 0.00 0.00 0. 0.00 0.00 0.00 0.00 0.00 0. 0.00 0.00 0.00 0.00 0.00 0. 0.00 0.00 0.00 0.00 0.00 0. 0.00 0.00 0.00 0.00 0.00 0. 0.00 0.00 0.00 0.00 0.00 0. 0.00 0.00 0.00 0.00 0.00 0. 0.00 0.00 0.00 0.00 0.00 0. 272 On Path to Multimodal Generalist: General-Level and General-Bench Video Generation Results. The complete results of all models on video generation are presented from Table 82 to Table 85."
        },
        {
            "title": "SoTA Specialist",
            "content": "VidAgent LM4LV NExT-GPT-V1.5 Vitron-V1 Table 82: Results on Video Generation Group, from #V-G-1. #V-G-1 (Txt2Vid Gen) #1 #2 #3 #4 #5 #6 #7 #8 #9 #10 #11 #12 #13 96.88 97.03 99.72 64.20 60.92 69.13 83.62 79.44 47.82 48.20 55.08 24. 71.27 0.00 94.58 95.79 97.75 38.40 54.40 53.53 68.26 48.23 0.00 0.00 43.20 36.40 55.10 12.00 13.40 30.70 36.10 10.70 63.50 46.70 68.40 15.70 15.60 47.80 58.90 13.80 0.00 0.00 0. 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 73.25 34.91 22.38 0.00 0.00 6.90 97.56 6.10 28.40 12.50 106. Table 83: Results on Video Generation Group, from #V-G-2 and #V-G-3."
        },
        {
            "title": "SoTA Specialist",
            "content": "VidAgent LM4LV NExT-GPT-V1.5 Vitron-V"
        },
        {
            "title": "SoTA Specialist",
            "content": "VidAgent LM4LV NExT-GPT-V1.5 Vitron-V1 #V-G-2 (Condt Vid Gen) #V-G-3 (Act Txt2Vid Gen) #1 #2 #3 #1 #2 #3 #4 #5 25.41 22.36 0.00 3.45 6.67 90.75 83.43 0.00 2.40 33.80 51. 37.40 0.00 14.30 17.50 95.61 53.20 0.00 2.60 44.90 72.74 105.62 95. 75.36 87.38 126.74 98.50 111.62 140.70 135.90 102.62 254.12 186.47 89.40 126.94 115.78 Table 84: Results on Video Generation Group, from #V-G-4. #V-G-4 (Img2Vid Gen) #1 #2 #3 #4 #5 #6 #7 #8 #9 #10 #11 59.46 61.55 60.79 62.89 67. 63.17 59.21 61.97 62.18 65.45 67. 60.45 0.00 10.53 23.40 62.31 0.00 18.76 36.10 61.95 0.00 20.91 15.20 62.89 0.00 17.53 36.40 67.19 0.00 16.79 47.80 62.94 0.00 14.63 26. 60.13 0.00 18.52 23.70 65.14 0.00 13.84 18.50 63.35 0.00 14.52 17.90 64.19 0.00 16.51 16.57 69.20 0.00 13.74 14.37 Table 85: Results on Video Generation Group, from #V-G-5 and #V-G-6. #V-G-5 (Vid Enhance) #V-G-6 (Vid Edit) #1 #2 #3 #4 #5 #6 #7 #8 #9 #1 #2 #3 #4 #5 27.53 38.15 36.60 37.87 25.32 25.75 33.86 56.71 58.29 34.27 34.16 36.75 97.50 54.60 0.00 0.00 24.17 33.05 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0. 0.00 0.00 0.00 22.28 22.46 28.69 50.12 52.33 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 29.64 0.00 0. 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0."
        },
        {
            "title": "SoTA Specialist",
            "content": "VidAgent LM4LV NExT-GPT-V1.5 Vitron-V1 273 On Path to Multimodal Generalist: General-Level and General-Bench B.3 Results of Audio-related Tasks Audio Comprehension Results. The complete results of all models on audio comprehension are presented in Table 86 and Table 87. Table 86: Results on Audio Comprehension Group, from #A-C-1 to #A-C-4."
        },
        {
            "title": "Model",
            "content": "#A-C-1 (Acnt Analy) #A-C-2 #A-C-3 #A-C-4 (Ctnt Analy) (SpeechEmo Analy) (Music Analy) #1 #2 #3 #4 #1 #2 #3 #1 #1 #2 #3 #"
        },
        {
            "title": "SoTA Specialist",
            "content": "75.20 85.60 90.33 97.95 98.76 95.29 43.20 Qwen-Audio-Chat Qwen2-Audio-Instruct GAMA Pengi SALMONN-7B SALMONN-13B WavLLM NExT-GPT-V1.5 PandaGPT (13B) ImageBind-LLM ModaVerse-7b-v0 Any-GPT Unified-io-2-XXL 56.70 75.00 55.40 40.60 93.00 76.80 36.50 45.80 99.20 53.20 92.40 94.40 82.80 47.20 23.40 86.50 32.40 85.70 80.90 77.50 34.20 21.40 78.90 35.00 76.20 78.00 65.70 36.50 24.70 80.50 85.00 65.10 65.40 58.30 24.10 26.04 84.20 93.50 67.80 68.70 68.90 31.40 37.45 70.15 90.00 60.20 35.60 63.10 24.50 12.50 55.40 40.50 64.50 30.50 36.80 20.10 6.50 9.80 58.30 57.30 45.10 24.50 26.40 10.50 65.00 25.40 50.10 40.70 38.50 18.50 10.50 50.40 35.20 40.30 18.70 20.10 10.30 34.60 62.10 15.00 66.30 37.40 46.10 12.90 22.50 45.90 13.50 38.70 34.50 32.50 15.80 70.62 76.80 61.40 68.00 56.70 45.56 67.80 71.20 65.80 45.20 56.80 32.80 63.40 56.10 83. 69.3 74.00 89.20 61.20 55.20 33.00 75.20 47.80 19.40 63.50 63.90 85.40 61.90 39.20 46.00 60.50 16.20 33.70 63.80 19.40 34.60 56.80 49.80 12.50 5.50 56.80 43.70 32.40 45.60 5.40 42.30 25.70 10.40 4.20 24.00 32.60 68.40 78.90 45.00 56.70 36.80 20.10 1.40 4.80 0.00 0.00 0.00 0.00 1.20 0.80 0.50 0.00 0.00 0.00 0.70 Table 87: Results on Audio Comprehension Group, from #A-C-5 to #A-C-9."
        },
        {
            "title": "Model",
            "content": "#A-C-5 #A-C-6 #A-C-7 #A-C-8 #A-C-9 (Aud-Tech Analy) (Aud Analy) (Aud QA) (Animal-Sound Det) (Envir-Sound Det) #1 #2 #3 #1 #2 #1 #2 #1 #2 #1 #2 #"
        },
        {
            "title": "SoTA Specialist",
            "content": "69.40 80.30 65.90 70.50 55.30 38. 78.50 77.60 78.20 76.40 86.70 71. Qwen-Audio-Chat Qwen2-Audio-Instruct GAMA Pengi SALMONN-7B SALMONN-13B WavLLM NExT-GPT-V1.5 PandaGPT-13B ImageBind-LLM ModaVerse-7b-v0 Any-GPT Unified-io-2-XXL 63.40 65.70 23.50 38.40 31.70 32.30 53.60 24.60 21.50 26.70 14.20 28.40 33.10 58.40 61.40 25.40 15.70 17.90 35.40 31.40 16.50 3.60 14.30 4.10 10.80 5.40 21.34 10.36 6.40 5.20 5.10 6.30 8.90 2.30 0.30 2.00 1.50 9.60 7.90 19.32 16.82 25.40 16.80 15.40 21.00 29.70 16.80 10.50 14.30 2.50 36.10 41.30 20.25 10.08 28.50 22.30 15.60 17.72 23.40 34.50 30.50 22.50 15.30 36.70 35. 31.27 34.56 23.60 15.40 17.80 19.00 13.50 25.60 15.30 23.40 13.80 28.90 12.30 81.60 88.80 74.10 70.50 60.40 68.90 78.00 70.30 69.20 67.30 56.30 76.40 65.10 86.30 87.50 89.60 83.60 75.80 79.60 86.40 76.90 52.90 63.40 46.80 62.10 57.30 84.00 70.40 81.50 71.20 65.00 73.50 36.40 63.50 56.70 59.20 51.60 73.80 69.70 86.11 71.58 72.30 68.40 57.80 60.60 74.30 69.50 61.50 36.50 58.70 45.20 56.80 92.60 86.80 80.50 78.40 66.40 75.90 77.80 80.90 80.10 72.60 75.70 52.30 79. 56.80 45.60 32.60 36.70 24.60 33.50 41.60 57.90 55.90 57.10 46.00 36.40 45.70 274 On Path to Multimodal Generalist: General-Level and General-Bench Audio Generation Results. The complete results of all models on audio generation are presented in Table 88 and Table 89. Table 88: Results on Audio Generation Group, from #A-G-1 to #A-G-7."
        },
        {
            "title": "Model",
            "content": "#A-G-1 #A-G-2 #A-G-3 #A-G-4 #A-G-5 #A-G- #A-G-7 (Audio Edit) (Dialog Gen) (EmoSpeech Gen) (TTS) (Txt2Aud) (Img2Aud Gen) (V2A) #1 #1 #1 #2 #1 #2 #1 #2 #1 #"
        },
        {
            "title": "SoTA Specialist",
            "content": "LLaMA-Omni Unified-io 2 Any-GPT Next-GPT-V1."
        },
        {
            "title": "ModaVerse",
            "content": "31.50 3.82 4.12 3.15 5.60 3.76 47.04 36. 51.40 11.52 0.00 18.36 23.50 13. 0.50 0.10 12.30 3.01 2.03 3. 1.15 1.32 2.79 1.15 5.61 7. 6.98 6.78 5.32 5.74 7.52 2. 2.35 2.15 1.35 3.89 3.14 1. 20.15 0.00 0.00 0.00 78.50 2.54 21.78 11. 65.80 1.35 16.52 10.24 100.00 1.02 53.68 15.34 45.20 0.00 48.63 10.32 63. 0.00 0.00 100.00 1.00 50.33 0.00 7.65 0. 24.31 14.05 1.35 0.00 0.00 1. 16.97 27.49 12.36 16.45 Table 89: Results on Audio Generation Group, from #A-G-8 to #A-G-11."
        },
        {
            "title": "Model",
            "content": "#A-G-8 #A-G-9 (Style Trans) (Speech Trans) #A-G-10 (Music Gen) #A-G-11 (Music Trans) #1 #1 #2 #3 #1 #2 #3 #4 #1 #"
        },
        {
            "title": "SoTA Specialist",
            "content": "LLaMA-Omni Unified-io 2 Any-GPT Next-GPT"
        },
        {
            "title": "ModaVerse",
            "content": "6.80 7.10 7.70 10.20 25.80 59.01 2.85 3.87 28.16 12.50 45. 86.23 45.36 96.70 46.30 30.24 89. 93.56 100.00 93.21 100.00 90.36 56. 99.30 45.68 57.96 95.45 99.34 98. 100.00 94.25 100.00 98.67 100.00 100. 100.00 100.00 100.00 0.00 0.00 0.00 0.00 0. 0.00 0.00 0.00 0.00 0.00 0. 0.00 0.00 0.00 0.00 0.00 0.00 3.15 0.00 1. 0.00 8.76 0.00 0.00 0.00 0.00 0.00 3.59 0.00 1. 3.65 6.78 0.00 0.00 4.75 On Path to Multimodal Generalist: General-Level and General-Bench B.4 Results of 3D-related Tasks 3D Comprehension Results. The complete results of all models on 3D comprehension are presented in Table 90 to Table 92. Table 90: Results on 3D Comprehension Group, from #D-C-1 to #D-C-4."
        },
        {
            "title": "Model",
            "content": "#D-C-1 #D-C-2 #D-C-3 #D-C-4 (3D-Human Cls) (3D-Struct Cls) (Tech Cls) (Indoor-Scene Seg) #1 #2 #3 #4 #5 #1 #2 #1 #2 #"
        },
        {
            "title": "SoTA Specialist",
            "content": "91.18 96.25 94.29 99.50 100.00 99.20 97.50 95.56 100.00 78.50 3D-VisTA PointLLM-7B PointLLM-13B 3D-LLM"
        },
        {
            "title": "AvatarGPT",
            "content": "0.00 6.36 5.90 0.00 0.00 0. 0.00 0.00 0.00 59.28 45.55 48.12 71.50 56.42 52.77 49. 79.00 0.00 0.00 0.00 0.00 0. 0.00 0.00 0.00 0.00 0.00 5. 0.00 0.00 0.00 0.00 0.00 15.00 65. 80.00 15.00 69.28 87.00 0.00 0.00 0. 0.00 0.00 0.00 0.00 0.00 0. 0.00 0.00 Table 91: Results on 3D Comprehension Group, from #D-C-5 to #D-C-9."
        },
        {
            "title": "Model",
            "content": "#D-C-5 #D-C-6 #D-C-7 (Outdoor-Scene Seg) (Indoor-Inst Seg) (Pose Est) #D-C-8 (Part Seg) #D-C-9 (3D Track) #1 #1 #1 #1 #2 #3 #4 #5 #6 #"
        },
        {
            "title": "SoTA Specialist",
            "content": "70.02 81.20 55.00 87.31 93.40 88.62 89.38 81.44 89.52 75.20 3D-VisTA PointLLM-7B PointLLM-13B 3D-LLM"
        },
        {
            "title": "AvatarGPT",
            "content": "0.00 0.00 0.00 0.00 0.00 0. 0.00 0.00 0.00 0.00 0.00 0. 0.00 0.00 0.00 0.00 0.00 0. 0.00 0.00 0.00 0.00 0.00 0. 0.00 0.00 0.00 0.00 0.00 0. 0.00 0.00 0.00 0.00 0.00 0. 0.00 0.00 0.00 0.00 0.00 0. 0.00 0.00 0.00 Table 92: Results on 3D Comprehension Group, from #D-C-10 to #D-C-13."
        },
        {
            "title": "SoTA Specialist",
            "content": "3D-VisTA PointLLM-7B PointLLM-13B 3D-LLM"
        },
        {
            "title": "AvatarGPT",
            "content": "#D-C-10 #D-C-11 (3D-Geo Analy) (3D Det) #D-C-12 (3D QA) #D-C-13 (3D-Motion Analy) #1 9.96 #1 #1 #2 #1 #2 #1 #2 #1 #1 68.52 12.40 35.60 67.20 48.50 71.40 49.10 45.80 22.30 0.00 0.00 0. 0.00 0.00 16.00 34.80 63.30 45.40 69.80 47.20 48.10 0.00 0.00 0. 0.00 0.00 0.00 0.00 0.00 0. 0.00 0.00 0.00 0.00 0.00 12.00 36.50 65.60 47.20 68.80 48.00 46. 0.00 0.00 0.00 0.00 0.00 0. 0.00 0.00 0.00 0.00 0.00 12. 276 On Path to Multimodal Generalist: General-Level and General-Bench 3D Generation Results. The complete results of all models on 3D generation are presented in Table 93 and Table 94. Table 93: Results on 3D Generation Group, from #D-G-1 to #D-G-4."
        },
        {
            "title": "Model",
            "content": "#D-G-1 #D-G-2 (PC Complt) (PC2M Recon) #D-G-3 (Txt2PC Gen) #D-G-4 (Txt2M Gen) #1 #1 #2 #1 #2 #3 #4 #1 #2 #3 #"
        },
        {
            "title": "SoTA Specialist",
            "content": "0.22 9.32E-05 4.93E-05 24.94 25.10 24.07 23.56 26.42 25.22 25.93 25.18 MotionGPT-1 MotionGPT-2 LLaMA-Mesh 0.00 0.00 0.00 0. 0.00 0.00 0.00 0.00 0.00 0. 0.00 0.00 0.00 0.00 0.00 0. 0.00 0.00 0.00 0.00 20.06 14.43 18.06 17.65 Table 94: Results on 3D Generation Group, from #D-G-5 to #D-G-9."
        },
        {
            "title": "Model",
            "content": "#D-G-5 (Img2PC Gen) #D-G-6 #D-G-7 #D-G-8 #D-G- (Img2M Gen) (RGBD2PC Recon) (RGBD2Mesh Recon) (Txt2Motion Gen) #1 #2 #3 #4 #1 #2 #3 #4 #1 #"
        },
        {
            "title": "SoTA Specialist",
            "content": "77.06 78.27 78.87 78.04 83.30 82.88 84.43 83.96 6540.02 6540.02 MotionGPT-1 MotionGPT-2 LLaMA-Mesh 0.00 0.00 0.00 0.00 0.00 0. 0.00 0.00 0.00 0.00 0.00 0. 0.00 0.00 0.00 0.00 0.00 0. 0.00 0.00 0.00 0.00 0.00 0. #1 0.23 0.51 0.60 On Path to Multimodal Generalist: General-Level and General-Bench B.5 Results of NLP Tasks All the results of all generalists on NLP tasks are shown in Table 95, Table 97, Table 99, Table 101, Table 103, Table 105, Table 107, Table 109, Table 111, Table 113, and Table 115. Table 95: Results on NLP Group, #L-1. #L-1 (Cog QA)"
        },
        {
            "title": "SoTA Specialist",
            "content": "87.33 95.00 31.47 28.42 80.00 33. 40.75 86.45 80.39 #1 #2 #3 #4 #5 #6 #7 #8 #9 Meta-Llama-3.1-8B-Instruct Qwen2.5-7B-Instruct Gemma-2-9b-it ChatGLM-6b Vicuna-7b-v1.5 InternLM-Chat-7b GPT-J-6B Falcon3-7B-Instruct Baichuan2-7B-Base Ministral-8B-Instruct-2410 Yi-Ligntning GPT-3.5-turbo GPT-4v GPT-4o GPT4o-mini GPT-4o-4096 ChatGPT-4o-latest Claude-3.5-Sonnet Claude-3.5-Opus Emu2-32B DetGPT InternVL2.5-8B InternVL2.5-4B NExT-GPT-V1.5 InternVL2.5-2B Monkey-10B-chat DeepSeek-VL-7B Qwen2-VL-7B Qwen-VL-Chat Qwen-Audio-Chat Qwen2-Audio-Instruct MoE-LLAVA-Phi2-2.7B-4e-384 mPLUG-Owl2-LLaMA2-7b Phi-3.5-Vision-Instruct Cambrian-1-8B MiniGPT4-LLaMA2 InternVL-Chat-V1-5 Mini-InternVL-Chat-4B-V1-5 InternLM-XComposer2-VL-1.8B GPT4RoI GLaMM LLaVA-NeXT-13B LLaVA-NeXT-34B Pixtral-12B SEED-LLaMA-13B BLIP2 MiniMonkey DeepSeek-VL-7B LISA-7B 5.18 4.46 8.08 2.69 1.01 2.53 0.23 4.24 1.11 19.35 6.30 3.32 2.26 2.92 2.04 3.77 3.57 7.37 5.91 3.58 0.09 8.00 6.41 0.76 2.49 15.99 3.70 3.97 0.92 3.04 2.87 4.24 2.26 5.06 2.57 1.22 5.80 5.50 2.33 2.77 0.00 3.64 5.88 6.25 1.03 1.08 2.31 6.84 0.00 65.80 67.80 75.60 43.80 46.40 49.20 38.60 51.20 40.80 65.00 74.20 41.00 41.00 41.00 41.00 41.00 41.00 56.20 50.00 47.20 42.40 65.40 61.60 40.58 43.00 37.80 14.20 33.00 47.40 48.60 0.00 53.80 58.00 62.40 41.00 63.60 59.40 53.80 46.60 41.20 41.00 43.60 47.20 47.80 19.20 3.40 51.00 44.20 0.00 6.89 6.30 9.28 13.03 0.47 9.96 0.32 7.27 0.12 21.33 7.42 3.11 2.87 3.20 2.72 3.89 3.35 22.85 15.25 11.24 4.18 15.71 20.25 0.39 11.23 3.05 7.64 6.63 0.42 6.08 5.58 8.76 13.03 19.57 6.13 0.44 7.43 12.21 6.33 2.50 1.28 6.05 7.71 6.43 2.76 4.62 7.10 6.93 0.00 16.26 19.38 17.19 5.57 0.08 6.79 0.52 11.83 1.29 20.19 22.78 23.15 37.43 23.34 18.71 23.28 21.56 20.34 25.56 17.72 10.66 20.55 19.66 0.08 14.74 5.97 14.20 12.00 0.64 6.05 19.43 9.78 6.52 21.83 8.56 0.00 7.46 14.69 6.98 0.72 4.77 6.63 8.66 7.10 5.43 13.49 17.49 0.00 0.00 43.00 53.65 29.19 43.00 44.18 43.20 51.28 57.79 54.64 37.48 54.44 55.42 55.42 55.42 55.42 55.42 55.42 54.24 44.18 44.18 38.46 44.18 34.71 43.65 25.83 41.42 35.89 19.00 36.88 41.42 60.00 35.50 41.42 40.83 55.42 37.28 39.84 40.03 31.75 55.42 55.42 42.80 40.83 42.21 41.42 39.05 37.48 46.35 0. 50.20 37.99 59.65 24.21 22.24 25.20 27.95 27.17 25.20 38.19 33.27 26.18 26.18 26.18 26.18 26.18 26.18 43.90 38.58 36.22 31.10 54.92 50.00 0.00 33.07 24.01 29.92 33.00 26.97 23.62 0.00 26.77 25.59 29.92 26.38 30.71 35.43 27.75 22.04 26.18 26.18 27.56 28.74 29.13 24.61 32.09 27.76 26.18 0.00 74.90 74.71 80.59 52.75 44.51 78.24 20.39 74.31 20.98 71.76 79.22 20.78 20.78 20.78 20.78 20.78 20.78 64.71 60.78 56.27 51.18 78.43 81.37 43.26 72.35 26.27 72.54 36.00 59.80 59.80 85.00 64.71 61.96 74.71 21.76 72.55 73.13 60.39 65.88 21.56 20.78 37.25 43.92 46.67 15.29 43.13 70.78 46.86 0.00 82.40 82.00 78.60 67.40 63.80 60.20 47.00 78.60 47.80 80.20 84.60 47.40 47.40 47.40 47.40 47.40 47.40 73.20 75.60 64.80 62.80 77.60 78.60 56.84 76.00 52.20 80.60 60.00 63.80 56.40 82.00 71.80 73.40 82.00 49.20 47.40 68.20 75.60 76.40 47.60 47.40 62.60 64.60 72.40 46.60 48.60 76.20 0.00 0.00 13.11 16.36 20.79 8.27 0.35 13.40 0.50 18.71 0.07 22.14 13.31 12.83 14.62 16.00 14.04 17.68 15.33 22.28 17.11 14.94 12.78 21.57 20.75 0.35 16.36 2.46 11.05 11.63 0.29 9.89 4.77 13.75 11.24 19.05 4.95 0.35 14.61 15.56 15.13 1.27 0.00 9.47 17.93 18.95 6.67 11.41 17.02 5.00 0.00 278 On Path to Multimodal Generalist: General-Level and General-Bench"
        },
        {
            "title": "Model",
            "content": "CogVLM-Chat ShareGPT4V-7B ShareGPT4V-13B BLIP-3 (XGen-MM) AnyGPT MiniCPM3-4B LaVIT-V2 (7B) GLM-VL-Chat Gemini-1.5-Pro Gemini-1.5-Flash OMG-LLaVA-InternLM20B Idefics3-8B-Llama3 Yi-Vision-v2 Qwen2-VL-72B Otter Show-o NExT-Chat InternVL2-26B Qwen2-VL-72B DeepSeek-VL-2-small DeepSeek-VL-2 LLaVA-One-Vision-7B LLaVA-One-Vision-72B Sa2VA-8B Sa2VA-26B CoLVA-2B CoLVA-4B Long-LLaVA LM4LV Vitron-V1 PandaGPT (13B) AnyGPT GAMA Pengi SALMONN-7B SALMONN-13B WavLLM ImageBind-LLM Unified-io-2-XXL ModaVerse-7b-v0 AudioGPT-GPT4 SpeechGPT-7B-com LLaMA-Omni 3D-LLM PointLLM-7B PointLLM-13B 3D-VisTA MotionGPT-T5 MotionGPT-LLaMA AvatarGPT LLaMA-Mesh #L-1 (Cog QA) #1 #2 #3 #4 #5 #6 #7 #8 #9 6.53 4.26 5.37 5.79 4.84 6.17 11.42 5.44 7.25 6.63 2.83 6.39 6.34 8.31 0.76 8.62 1.19 6.50 14.10 4.20 3.70 5.60 6.90 14.60 7.30 2.90 5.00 2.50 0.00 1.67 0.90 4.84 2.29 1.89 1.03 0.90 2.01 1.78 2.77 0.50 2.04 1.99 5.18 8.61 1.15 1.20 0.28 0.11 1.14 0.30 1. 53.00 42.00 50.40 49.20 17.00 55.80 45.20 55.60 62.80 56.00 41.20 57.80 41.00 45.77 40.80 20.00 41.00 63.80 76.00 57.20 65.60 64.60 76.80 71.40 75.40 42.20 69.40 41.20 0.00 35.42 0.00 17.00 0.00 3.45 0.00 23.40 0.00 0.00 41.20 0.00 39.80 0.00 15.60 41.00 41.00 40.80 41.00 41.00 36.00 41.00 57.20 13.59 8.30 9.17 10.97 6.16 16.15 14.59 14.07 21.03 18.49 9.21 16.36 2.82 9.65 0.76 7.19 1.04 10.70 29.50 6.20 6.90 15.30 25.90 18.60 22.10 9.40 11.20 10.90 0.00 2.56 0.45 6.16 4.85 3.24 1.48 2.13 4.32 2.56 2.50 0.34 2.68 4.09 6.89 21.31 0.48 0.47 0.56 0.04 0.81 0.58 0.48 19.73 17.69 18.78 17.94 7.87 20.25 14.48 20.71 22.85 17.74 13.07 18.20 22.19 25.04 1.01 12.64 3.52 19.20 22.20 12.00 16.70 18.80 26.00 22.50 13.40 12.10 16.40 13.30 0.00 1.09 0.00 7.87 0.35 0.23 0.61 0.65 0.21 0.10 0.72 0.00 38.56 0.12 16.26 7.92 0.87 0.88 3.65 5.52 0.90 4.17 0.85 44.38 41.62 39.64 40.24 19.92 47.93 37.28 49.51 54.24 44.18 36.65 43.00 55.42 56.93 54.83 29.10 55.42 42.90 50.60 36.70 46.20 42.80 53.50 49.10 45.40 27.60 39.80 55.40 0.00 40.37 47.32 19.92 76.50 56.80 68.23 69.60 83.10 58.30 55.42 43.24 54.98 45.70 43.00 55.42 55.42 55.42 55.42 55.42 50.10 55.42 39.45 35.43 27.76 35.24 36.61 17.91 37.01 28.94 39.96 53.15 46.26 27.59 41.54 26.18 27.34 26.57 31.00 26.18 33.80 73.40 28.00 28.70 46.30 64.00 59.10 68.90 20.90 50.40 25.40 0.00 0.00 0.00 17.91 0.00 0.00 0.00 0.00 0.00 0.00 26.18 0.00 25.38 0.00 0.00 26.18 26.18 26.77 26.18 26.18 23.43 26.18 25.98 55.88 47.45 54.51 51.57 21.96 52.35 39.41 55.69 81.76 74.51 49.02 78.04 20.78 41.33 30.00 18.00 20.78 81.70 81.50 74.30 77.80 76.50 85.90 84.90 76.50 49.60 84.90 26.90 0.00 40.32 15.78 21.96 34.60 20.40 23.53 24.31 32.50 28.70 21.56 15.78 21.45 38.60 56.67 20.78 20.78 20.20 20.78 20.78 23.14 20.78 65. 69.20 62.00 67.00 62.80 24.00 72.80 60.20 71.40 83.40 76.60 47.20 76.40 47.40 63.95 49.10 45.00 47.40 68.00 86.60 80.20 79.00 79.00 87.40 82.20 79.80 68.80 84.80 49.60 0.00 56.84 67.54 24.00 57.80 40.80 70.90 71.20 57.46 56.71 47.60 37.23 42.34 54.23 63.50 47.40 47.40 47.40 47.40 47.40 47.40 47.40 72.40 14.16 10.76 15.50 16.86 6.33 18.91 12.92 16.78 20.86 19.27 13.64 16.71 16.18 20.84 1.78 1.17 2.88 18.40 24.80 8.90 11.10 19.90 27.80 25.60 20.60 11.00 16.70 13.30 0.00 3.50 0.14 6.33 4.75 2.56 1.84 1.78 4.32 3.45 1.27 3.10 10.23 4.23 13.11 17.55 0.35 0.35 2.48 0.27 0.98 2.82 0.35 279 On Path to Multimodal Generalist: General-Level and General-Bench Table 97: Results on NLP Group, #L-2. #L-2 (Ethics NLP)"
        },
        {
            "title": "SoTA Specialist",
            "content": "95.62 45.39 75.00 79.80 94.20 95. 99.88 95.80 95.00 #1 #2 #3 #4 #5 #6 #7 #8 #9 Meta-Llama-3.1-8B-Instruct Qwen2.5-7B-Instruct Gemma-2-9b-it ChatGLM-6b Vicuna-7b-v1.5 InternLM-Chat-7b GPT-J-6B Falcon3-7B-Instruct Baichuan2-7B-Base Ministral-8B-Instruct-2410 Yi-Ligntning GPT-3.5-turbo GPT-4v GPT-4o GPT4o-mini GPT-4o-4096 ChatGPT-4o-latest Claude-3.5-Sonnet Claude-3.5-Opus Emu2-32B DetGPT InternVL2.5-8B InternVL2.5-4B NExT-GPT-V1.5 InternVL2.5-2B Monkey-10B-chat DeepSeek-VL-7B Qwen2-VL-7B Qwen-VL-Chat Qwen-Audio-Chat Qwen2-Audio-Instruct MoE-LLAVA-Phi2-2.7B-4e-384 mPLUG-Owl2-LLaMA2-7b Phi-3.5-Vision-Instruct Cambrian-1-8B MiniGPT4-LLaMA2 InternVL-Chat-V1-5 Mini-InternVL-Chat-4B-V1-5 InternLM-XComposer2-VL-1.8B GPT4RoI GLaMM LLaVA-NeXT-13B LLaVA-NeXT-34B Pixtral-12B SEED-LLaMA-13B BLIP2 MiniMonkey DeepSeek-VL-7B LISA-7B CogVLM-Chat ShareGPT4V-7B ShareGPT4V-13B 67.00 46.80 57.40 0.00 0.00 43.80 0.00 49.20 0.00 48.20 55.60 56.40 54.00 54.20 55.00 55.20 53.40 39.75 43.75 34.44 29.53 22.99 20.49 23.10 21.88 5.35 18.12 46.00 0.00 46.80 47.00 52.20 53.60 50.60 44.20 0.00 22.83 19.28 18.52 24.20 8.33 53.20 49.20 52.40 38.80 49.00 54.80 22.28 0.00 34.64 28.76 36.74 56.00 80.40 86.60 1.60 0.00 9.40 0.00 82.60 0.00 79.60 74.60 78.40 82.60 80.40 80.20 84.20 82.00 55.21 43.99 46.40 36.38 23.17 20.76 18.60 17.97 5.97 26.04 34.00 0.00 43.60 49.60 80.20 78.40 83.20 1.80 0.00 26.50 23.91 8.52 20.29 9.58 56.80 62.80 61.40 47.60 60.80 56.60 22.42 0.00 44.71 36.28 45.58 81.00 66.40 25.40 6.60 0.00 25.80 0.00 68.00 0.00 40.40 42.60 47.00 77.40 75.60 63.60 55.80 71.00 45.19 38.95 37.46 31.68 33.21 33.75 10.98 31.47 12.17 33.82 1.00 0.00 68.30 68.20 13.00 46.40 91.20 3.80 0.00 34.19 32.03 32.73 12.94 12.72 36.20 44.80 40.20 17.40 38.60 13.60 28.05 0.00 41.14 30.66 33.09 45.40 91.40 95.00 67.20 0.00 27.80 0.00 40.40 0.00 88.40 96.40 69.00 99.60 99.00 98.80 99.20 99.20 59.26 55.59 48.13 45.03 35.42 32.58 21.43 34.11 8.54 33.59 26.00 0.00 60.40 61.60 39.60 76.40 89.20 0.20 0.00 32.93 31.11 26.87 21.43 12.72 64.40 71.20 74.20 45.60 62.80 71.20 0.00 0.00 47.47 44.28 43.25 94.20 55.60 58.60 51.80 0.00 29.00 0.00 54.80 0.00 41.80 45.20 48.80 64.00 63.60 59.60 65.40 61.80 57.57 57.13 53.04 45.16 80.80 80.80 25.70 64.80 43.20 68.00 47.00 0.00 31.50 30.00 47.00 48.60 60.60 0.00 0.00 75.00 74.20 59.60 47.20 48.20 49.60 51.20 52.00 39.80 43.60 48.40 52.40 0.00 48.75 47.46 51. 11.00 62.80 54.80 42.80 0.00 6.00 0.00 67.60 0.00 37.60 51.20 76.20 49.80 55.40 59.00 66.20 68.00 60.07 49.76 44.61 44.91 78.58 78.98 0.00 65.25 43.63 69.69 12.00 0.00 7.80 6.20 63.40 29.80 55.20 0.80 0.00 69.09 74.34 68.88 33.13 33.13 53.20 58.40 59.60 30.60 57.20 78.00 47.68 0.00 44.84 40.08 42.50 83.40 84.80 89.60 68.40 50.40 75.60 72.00 87.20 82.40 87.40 91.40 84.40 84.40 84.40 84.40 84.40 84.40 85.40 86.40 79.60 73.40 85.20 78.80 50.78 70.60 69.00 85.00 27.00 82.00 49.00 66.85 79.20 82.40 83.40 84.40 81.60 87.80 71.80 75.20 84.20 84.40 75.60 78.60 80.20 44.20 76.80 72.40 76.40 0.00 83.20 72.60 78.00 24.08 32.24 35.31 28.98 20.44 26.83 3.76 31.23 4.46 21.66 34.43 26.35 29.63 30.29 29.33 30.83 27.61 39.21 34.19 29.15 18.91 24.83 18.69 20.44 23.56 5.13 29.48 25.58 9.65 32.49 33.24 30.48 31.59 30.88 22.09 16.20 27.53 22.41 29.08 19.25 15.21 27.81 36.41 38.20 14.32 43.59 35.01 31.40 0.00 24.30 21.24 25.67 48.80 36.60 35.00 31.80 29.80 36.00 21.00 44.20 20.20 42.80 53.40 20.20 20.20 20.20 20.20 20.20 20.20 38.80 35.40 36.00 28.80 45.60 39.00 30.74 21.88 23.20 35.80 29.00 32.00 31.40 0.00 28.40 35.20 41.40 20.40 43.80 39.20 35.80 32.20 20.40 20.20 24.80 28.20 34.40 14.60 40.60 34.00 22.80 0.00 33.60 30.80 34.00 280 On Path to Multimodal Generalist: General-Level and General-Bench"
        },
        {
            "title": "Model",
            "content": "BLIP-3 (XGen-MM) AnyGPT MiniCPM3-4B LaVIT-V2 (7B) GLM-VL-Chat Gemini-1.5-Pro Gemini-1.5-Flash OMG-LLaVA-InternLM20B Idefics3-8B-Llama3 Yi-Vision-v2 Qwen2-VL-72B Otter Show-o NExT-Chat InternVL2-26B Qwen2-VL-72B DeepSeek-VL-2-small DeepSeek-VL-2 LLaVA-One-Vision-7B LLaVA-One-Vision-72B Sa2VA-8B Sa2VA-26B CoLVA-2B CoLVA-4B Long-LLaVA LM4LV Vitron-V1 PandaGPT (13B) AnyGPT GAMA Pengi SALMONN-7B SALMONN-13B WavLLM ImageBind-LLM Unified-io-2-XXL ModaVerse-7b-v0 AudioGPT-GPT4 SpeechGPT-7B-com LLaMA-Omni 3D-LLM PointLLM-7B PointLLM-13B 3D-VisTA MotionGPT-T5 MotionGPT-LLaMA AvatarGPT LLaMA-Mesh #L-2 (Ethics NLP) #1 #2 #3 #4 #5 #6 #7 #8 #9 35.55 14.54 33.08 26.72 32.97 51.60 47.00 19.20 48.20 53.60 46.67 0.00 26.73 0.00 51.20 52.40 53.60 56.60 54.80 0.20 48.60 51.00 51.20 54.00 36.60 0.00 13.50 1.20 14.54 0.00 0.00 1.60 1.50 1.40 1.10 16.50 1.02 54.10 1.20 45.30 1.20 0.00 0.00 0.00 0.00 0.00 0.00 0. 41.23 16.12 46.01 33.86 43.97 69.80 60.80 17.00 63.60 67.60 74.67 0.00 17.75 0.00 81.50 73.40 65.60 81.20 82.80 0.00 84.40 85.40 77.40 71.60 45.20 0.00 38.40 2.10 16.12 0.00 0.00 4.40 4.70 4.30 4.50 18.50 1.90 81.90 4.10 34.20 18.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 38.53 15.31 33.86 26.62 32.48 69.00 63.60 9.40 61.20 41.99 80.02 0.00 39.24 0.00 41.40 77.40 24.80 65.80 58.80 44.80 38.20 41.60 1.80 28.60 64.40 0.00 10.98 0.00 15.31 0.00 0.00 1.60 1.40 2.40 1.70 10.30 0.00 77.40 0.00 45.30 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 45.28 17.22 48.91 36.15 49.16 81.20 77.40 24.20 76.40 96.20 96.00 0.00 31.10 0.00 91.40 93.40 68.40 78.80 90.20 99.40 76.20 98.80 6.20 93.80 66.80 0.00 23.45 0.00 17.22 0.00 0.00 1.00 1.10 1.30 1.10 11.40 0.00 98.70 0.00 23.10 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 48.25 19.70 58.84 44.13 57.70 62.60 58.00 20.20 59.80 58.80 47.33 0.00 36.87 0.00 53.70 54.20 26.40 28.40 56.80 54.60 39.80 54.80 55.80 36.40 60.20 0.00 27.50 0.00 19.70 0.00 0.00 1.00 0.00 0.00 0.00 4.30 0.00 63.60 0.00 14.50 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 47.43 15.66 52.47 43.77 51.78 78.40 75.20 32.60 73.20 48.60 62.67 0.00 21.54 0.00 64.00 62.20 36.20 69.00 61.60 3.80 62.80 71.20 34.40 58.80 32.20 0.00 0.05 0.00 15.66 0.00 0.00 1.60 0.00 0.00 0.00 23.60 0.00 49.70 0.00 11.00 0.20 0.00 0.00 0.00 0.00 0.00 0.00 0.00 78.80 29.20 86.60 64.40 86.80 87.00 83.40 59.80 82.20 84.40 89.20 69.00 79.00 20.78 90.00 91.80 83.80 86.80 89.20 92.40 89.20 91.60 67.40 81.40 87.20 0.00 57.83 60.40 29.20 35.87 68.51 1.00 1.20 25.83 56.40 56.80 49.39 84.30 45.60 44.50 84.40 84.40 84.60 84.40 84.40 71.20 84.40 87. 25.63 9.42 26.23 21.72 27.19 34.33 29.98 12.26 27.71 20.63 25.73 10.33 6.89 7.15 29.10 32.80 34.20 35.10 25.30 24.70 30.40 28.90 29.00 30.50 30.80 0.00 10.32 19.32 9.42 17.47 14.35 21.59 22.67 14.56 13.60 17.56 15.80 30.40 13.56 24.08 9.29 19.91 19.69 0.64 3.56 5.30 11.48 17.93 36.00 12.80 32.20 27.80 30.40 44.20 40.60 21.20 39.60 20.20 20.27 20.00 23.00 20.20 43.40 59.40 39.20 42.20 49.00 58.00 48.80 52.60 30.20 44.40 22.00 0.00 23.45 0.00 12.80 0.00 0.00 0.00 0.00 0.00 0.00 17.80 0.00 20.10 0.00 0.00 20.20 20.20 23.60 20.20 20.20 28.40 20.20 45.20 281 On Path to Multimodal Generalist: General-Level and General-Bench Table 99: Results on NLP Group, from #L-3 to #L-4."
        },
        {
            "title": "Model",
            "content": "#L-3 (Domain QA) #L-4 (Social QA) #1 #2 #3 #4 #5 #6 #7 #1 #2 #3 #4 #5 #"
        },
        {
            "title": "SoTA Specialist",
            "content": "82.62 48.79 73.55 53.31 94.50 96.67 88.00 48.73 82.00 49.21 97.30 91.00 57.74 Meta-Llama-3.1-8B-Instruct Qwen2.5-7B-Instruct Gemma-2-9b-it ChatGLM-6b Vicuna-7b-v1.5 InternLM-Chat-7b GPT-J-6B Falcon3-7B-Instruct Baichuan2-7B-Base Ministral-8B-Instruct-2410 Yi-Ligntning GPT-3.5-turbo GPT-4v GPT-4o GPT4o-mini GPT-4o-4096 ChatGPT-4o-latest Claude-3.5-Sonnet Claude-3.5-Opus Emu2-32B DetGPT InternVL2.5-8B InternVL2.5-4B NExT-GPT-V1.5 InternVL2.5-2B Monkey-10B-chat DeepSeek-VL-7B Qwen2-VL-7B Qwen-VL-Chat Qwen-Audio-Chat Qwen2-Audio-Instruct MoE-LLAVA-Phi2-2.7B-4e-384 mPLUG-Owl2-LLaMA2-7b Phi-3.5-Vision-Instruct Cambrian-1-8B MiniGPT4-LLaMA2 InternVL-Chat-V1-5 Mini-InternVL-Chat-4B-V1-5 InternLM-XComposer2-VL-1.8B GPT4RoI GLaMM LLaVA-NeXT-13B LLaVA-NeXT-34B Pixtral-12B SEED-LLaMA-13B BLIP2 MiniMonkey DeepSeek-VL-7B LISA-7B CogVLM-Chat ShareGPT4V-7B ShareGPT4V-13B 9.78 9.04 7.86 8. 8.38 6.11 51.40 12.63 48.20 33.13 39.20 10.87 43.40 32.12 36.60 29.20 22.89 36.90 41.42 83.00 84.24 81.80 42.66 63.80 29.39 94.40 93.00 39.89 20.56 27.48 41.47 27.69 86.80 83.43 77.20 35.58 63.00 20.48 94.20 93.80 22.69 15.78 28.87 37.97 15.70 89.80 80.40 77.20 23.13 62.60 14.85 93.40 93.80 13.42 19.60 23.32 30.54 27.28 52.60 55.76 51.60 31.83 57.60 22.98 68.60 71.00 24.59 15.63 23.22 15.62 14.90 59.00 48.08 57.60 18.83 55.40 19.06 67.40 71.00 15.46 14.20 15.81 29.55 20.55 67.20 73.13 70.80 23.91 67.80 17.15 84.60 81.60 15.51 48.20 10.40 36.20 31.60 12.42 12.50 12.93 24.60 26.11 38.93 38.45 81.20 65.45 74.60 41.65 65.20 27.99 87.80 87.80 30.33 12.63 14.51 35.40 30.00 13.65 16.39 27.41 38.60 21.27 82.60 83.84 76.60 28.74 66.60 14.68 92.20 92.40 16.87 27.08 26.62 41.18 37.58 89.60 85.45 80.20 42.34 68.20 28.95 95.00 94.00 34.55 14.22 29.65 36.26 24.38 48.20 33.13 40.00 28.14 51.20 14.95 35.60 30.00 13.58 22.56 25.12 36.75 36.22 48.20 33.13 40.00 37.08 51.20 25.99 25.60 30.00 25.40 19.95 26.31 43.04 27.21 48.20 33.13 40.00 33.32 51.20 19.45 35.60 30.00 19.43 21.42 28.98 41.72 33.83 48.20 33.13 40.00 38.67 51.20 23.10 35.60 30.00 24.83 19.36 27.90 42.29 27.30 48.20 33.13 40.00 33.38 51.20 20.36 35.60 30.00 19.12 19.67 28.22 43.00 26.80 48.20 33.13 40.00 32.25 51.20 18.64 35.60 30.00 17.50 47.83 37.75 53.34 37.66 69.60 76.20 54.40 41.68 60.20 30.02 58.60 61.00 42.93 46.29 33.32 43.88 28.87 69.40 71.20 49.60 34.60 57.00 24.78 59.60 51.00 33.36 40.15 29.51 46.14 26.70 67.60 69.40 49.80 30.88 48.40 23.51 51.80 49.20 33.57 36.27 21.60 40.13 21.79 61.00 62.80 42.00 28.70 46.00 18.50 43.40 43.00 27.02 74.80 37.58 65.80 24.73 92.20 90.60 31.10 24.89 25.92 33.72 26.24 31.72 44.54 78.40 34.30 65.00 22.55 91.80 90.60 26.21 26.87 33.41 19.32 38.71 36.48 36.36 15.46 34.87 10.56 12.50 54.74 46.50 53.20 18.56 53.40 18.40 63.50 69.50 13.68 65.80 35.48 61.20 25.94 77.20 75.40 28.97 22.94 33.01 12.06 28.83 31.40 25.34 11.28 18.78 7.65 41.80 69.60 37.17 58.80 25.01 84.00 85.20 29.07 24.30 21.08 18.84 19.38 19.90 18.63 27.22 19.56 24.37 35.59 37.00 67.00 53.00 32.20 42.00 29.45 67.00 69.00 38.75 18.43 14.25 15.07 56.20 72.20 60.00 11.93 60.60 16.52 72.20 61.60 17.60 23.20 27.51 35.14 32.37 54.20 58.98 59.00 37.24 51.00 25.99 81.20 74.00 28.59 26.65 22.04 26.94 35.01 31.89 65.33 86.60 78.58 37.84 78.65 24.22 90.11 24.06 28.98 34.26 32.01 60.00 71.00 55.40 36.03 59.60 26.12 71.00 66.60 28.89 25.13 23.25 34.04 35.23 71.80 83.60 75.40 39.15 66.40 28.12 83.60 83.00 32.44 24.27 26.39 38.03 37.09 79.40 91.00 78.20 40.94 66.60 26.73 91.00 88.00 34.85 26.77 17.90 26.20 37.79 48.40 37.00 41.20 28.41 51.20 24.05 37.00 32.20 36.13 26.85 14.42 19.58 84.60 66.40 32.67 68.20 84.60 73.20 23.81 63.40 68.80 36.29 61.20 25.20 84.60 86.00 28.85 24.68 19.83 16.15 18.50 18.16 25.45 72.00 33.38 61.60 21.77 87.80 86.80 25.55 27.77 26.71 12.11 22.99 25.42 30.81 17.57 21.57 16.96 57.80 34.44 57.20 22.75 75.80 76.80 25.04 26.40 18.82 0.00 0.60 0.90 5.47 40.60 17.49 51.40 23.36 36.40 29.60 26.25 40.00 12.46 51.20 0.00 9.55 9.16 9.04 7.40 8.70 27.85 23.33 36.21 20.22 46.40 55.60 46.60 32.13 60.40 19.74 62.60 51.60 27.55 23.78 26.84 33.25 31.89 56.60 58.80 50.20 35.24 57.20 23.19 67.40 61.20 26.64 24.67 24.33 36.94 32.92 62.20 62.00 55.60 40.17 62.40 21.02 69.80 66.80 29.71 12.05 13.95 24.67 15.48 29.20 47.60 42.80 19.07 38.40 15.16 33.60 24.40 20.49 29.27 23.32 41.83 37.69 64.20 63.00 59.20 46.29 68.40 41.84 79.60 88.40 52.61 17.43 15.90 31.21 25.90 62.80 65.86 64.40 32.76 58.00 23.26 80.40 78.40 23.68 1.53 0.00 50.80 33.74 56.20 0.00 0.00 0.00 0.00 0.00 45.61 25.47 46.78 34.10 63.40 67.80 53.20 29.83 49.60 27.09 49.60 48.20 35.70 33.20 24.84 41.18 22.86 62.60 64.20 42.20 27.93 46.40 20.43 46.00 45.60 27.66 36.19 23.32 40.73 26.17 65.40 64.40 47.80 30.45 44.60 22.43 50.40 49.40 27. 24.40 15.43 12.02 14.56 17.63 0.00 0.00 0.00 61.80 58.00 0.00 0.00 8.95 6.68 3.07 11.82 14.87 37.40 43.00 35.60 30. 3.25 20.24 0.00 0.00 47.60 0.00 0.00 0. 7.97 8.59 6.09 6.21 0.00 0. 282 On Path to Multimodal Generalist: General-Level and General-Bench"
        },
        {
            "title": "Model",
            "content": "BLIP-3 (XGen-MM) AnyGPT MiniCPM3-4B LaVIT-V2 (7B) GLM-VL-Chat Gemini-1.5-Pro Gemini-1.5-Flash OMG-LLaVA-InternLM20B Idefics3-8B-Llama3 Yi-Vision-v2 Qwen2-VL-72B Otter Show-o NExT-Chat InternVL2-26B Qwen2-VL-72B DeepSeek-VL-2-small DeepSeek-VL-2 LLaVA-One-Vision-7B LLaVA-One-Vision-72B Sa2VA-8B Sa2VA-26B CoLVA-2B CoLVA-4B Long-LLaVA LM4LV Vitron-V1 PandaGPT (13B) AnyGPT GAMA Pengi SALMONN-7B SALMONN-13B WavLLM ImageBind-LLM Unified-io-2-XXL ModaVerse-7b-v0 AudioGPT-GPT4 SpeechGPT-7B-com LLaMA-Omni 3D-LLM PointLLM-7B PointLLM-13B 3D-VisTA MotionGPT-T5 MotionGPT-LLaMA AvatarGPT LLaMA-Mesh #L-3 (Domain QA) #L-4 (Social QA) #1 #2 #3 #4 #5 #6 #7 #1 #2 #3 #4 #5 #6 7.96 0. 16.00 41.17 26.99 44.56 25.79 65.40 68.40 47.60 33.70 46.00 24.40 48.80 45.60 30.89 18.60 18.20 10.80 13.45 10.84 16.73 10.64 24.80 24.20 18.00 51.94 34.47 52.23 31.14 69.80 73.00 45.20 33.55 46.00 25.14 56.40 54.00 37.87 40.36 23.12 39.05 25.22 61.60 58.60 36.40 25.57 37.80 23.52 45.20 39.00 26.88 50.08 36.02 52.19 31.23 68.00 71.00 43.60 29.72 43.20 25.78 56.40 53.80 37.28 29.14 34.32 41.77 36.41 89.60 90.20 84.20 42.15 81.40 27.23 86.60 88.40 40.45 26.93 30.08 37.04 33.83 86.20 84.40 78.20 38.47 72.60 23.78 80.20 82.60 36.83 16.07 14.54 26.76 12.95 53.80 51.20 29.80 15.42 14.80 12.91 14.80 16.20 12.86 28.54 29.32 35.93 31.89 87.80 88.20 73.60 39.30 74.40 22.05 78.00 79.60 36.13 15.03 23.04 37.95 30.74 48.20 33.13 40.00 35.67 51.20 16.25 35.60 30.00 21.46 16.47 25.99 34.25 31.83 69.32 46.24 66.31 34.96 66.34 21.97 47.88 61.92 22.47 21.66 44.60 38.38 46.00 15.12 47.40 18.77 40.60 34.20 20.50 9.86 19.58 18.72 13.04 23.23 57.00 38.00 46.00 19.57 38.00 18.65 32.00 28.00 27.79 14.13 9.84 11.62 12.13 14.17 48.20 33.13 40.00 12.88 51.20 16.85 35.60 30.00 19.15 6.66 19.30 19.70 33.40 27.10 81.60 75.90 71.40 32.70 65.00 21.70 86.20 86.40 24.70 22.80 27.80 38.50 36.50 91.20 83.80 79.20 39.50 70.10 25.00 95.40 96.20 28.20 21.40 29.20 40.50 29.70 77.00 80.20 81.40 37.40 65.80 26.50 92.80 92.60 28.20 21.90 28.10 42.40 31.80 80.60 82.80 80.80 40.30 67.60 24.70 92.60 95.40 26.80 20.40 21.90 35.30 32.80 82.60 78.20 73.80 33.80 64.80 23.60 88.20 93.00 28.70 23.50 25.00 36.60 39.30 91.20 85.30 81.00 41.10 70.00 28.10 96.40 96.00 35.20 23.30 26.50 38.20 37.60 84.60 82.60 78.80 40.50 66.80 26.80 92.80 91.40 32.90 26.20 27.10 38.40 40.00 87.20 77.40 76.20 41.70 68.40 28.40 89.00 89.00 34.80 19.90 20.20 33.50 30.70 53.20 54.30 51.40 34.20 49.80 24.20 66.40 60.60 27.30 21.20 27.80 38.30 30.90 83.60 79.80 80.80 36.20 68.60 23.60 93.00 89.40 26.80 23.10 25.70 30.90 34.80 50.20 36.40 42.60 34.40 54.00 28.10 38.40 46.60 36.20 0.00 0.00 0.00 13.47 14.58 15.60 13.20 54.74 45.60 52.30 20.56 49.82 13.40 60.34 62.34 16.83 15.89 10.38 11.15 56.90 24.00 15.46 9.78 0.00 18.60 18.20 10.80 13.45 10.84 16.73 10.64 24.80 24.20 18.00 0.00 47.21 25.57 0.00 63.83 65.50 27.30 22.58 35.84 24.29 28.91 45.30 15.67 0.00 67.00 45.60 20.40 14.35 30.46 20.67 26.63 21.90 20.00 14.68 68.44 28.00 19.19 0.00 11.63 22.50 22.10 13.02 77.80 29.70 17.40 11.87 0.00 22.12 68.85 26.23 66.55 57.35 28.09 21.45 36.79 25.43 29.01 12.40 48.75 15.40 78.00 56.00 29.80 23.50 45.60 24.30 28.70 0.00 6.78 0.60 2.50 16.51 14.35 15.67 12.40 21.34 24.67 20.56 9.45 10.32 46.78 23.60 16.67 14.50 0.00 22.76 25.05 36.89 36.48 43.10 32.18 39.78 37.09 51.10 25.34 25.70 29.80 25.30 20.34 34.23 24.34 28.35 67.50 16.34 65.40 45.60 13.09 27.60 22.89 36.90 41.42 83.00 74.56 80.20 43.30 63.20 28.37 84.40 65.30 19.78 5.32 48.20 33.13 40.00 12.15 51.20 4.24 16.65 23.89 16.02 17.27 48.20 33.13 40.00 20.15 51.20 20.26 35.60 30.00 18.32 17.24 23.20 16.45 16.99 45.80 32.93 32.20 20.01 51.20 19.91 34.80 30.60 18.39 0.05 0.00 0.20 51.20 48.20 33.13 40.00 0.55 0.00 1.27 51.20 48.20 33.13 40.00 43.40 33.54 41.80 11.64 46.00 14.46 39.40 39.20 21.27 18.53 19.87 7.99 6.09 48.20 33.13 40.00 11.84 51.20 2.38 28.67 22.44 23.21 34.27 71.60 76.16 73.00 27.88 69.60 29.28 90.80 86.40 37.13 28.09 18.58 66.36 32.00 29.78 17.91 76.53 32.13 35.60 30.00 35.60 30.00 17.61 14.90 45.10 22.08 12.40 11.37 10.34 20. 0.31 5.00 8.41 13.69 0.00 0.00 5.03 9.68 0.05 0.91 6.14 7.64 7.96 21.57 20.89 16.00 0.00 0.00 0.00 0.00 0. 35.60 30.00 35.60 30.00 0.23 1.82 0.00 0.00 0.00 0.00 0.10 1. 7.06 7.31 20.40 17.38 5.34 0.00 0. 4.56 0.00 0.00 0.00 0.00 0. 0.00 0.00 0.00 0.00 0.00 0. 1.03 6.40 8.69 7.48 283 On Path to Multimodal Generalist: General-Level and General-Bench Table 101: Results on NLP Group, from #L-5 to #L-7."
        },
        {
            "title": "Model",
            "content": "#L-5 (Non-Trad QA) #L-6 (Advance QA) #L-7 (Math Ability) #1 #2 #3 #1 #2 #3 #4 #5 #1 #2 #"
        },
        {
            "title": "SoTA Specialist",
            "content": "43.46 77.01 53.59 48.87 61.77 83. 32.01 88.00 75.00 81.00 69.34 Meta-Llama-3.1-8B-Instruct Qwen2.5-7B-Instruct Gemma-2-9b-it ChatGLM-6b Vicuna-7b-v1.5 InternLM-Chat-7b GPT-J-6B Falcon3-7B-Instruct Baichuan2-7B-Base Ministral-8B-Instruct-2410 Yi-Ligntning GPT-3.5-turbo GPT-4v GPT-4o GPT4o-mini GPT-4o-4096 ChatGPT-4o-latest Claude-3.5-Sonnet Claude-3.5-Opus Emu2-32B DetGPT InternVL2.5-8B InternVL2.5-4B NExT-GPT-V1.5 InternVL2.5-2B Monkey-10B-chat DeepSeek-VL-7B Qwen2-VL-7B Qwen-VL-Chat Qwen-Audio-Chat Qwen2-Audio-Instruct MoE-LLAVA-Phi2-2.7B-4e-384 mPLUG-Owl2-LLaMA2-7b Phi-3.5-Vision-Instruct Cambrian-1-8B MiniGPT4-LLaMA2 InternVL-Chat-V1-5 Mini-InternVL-Chat-4B-V1-5 InternLM-XComposer2-VL-1.8B GPT4RoI GLaMM LLaVA-NeXT-13B LLaVA-NeXT-34B Pixtral-12B SEED-LLaMA-13B BLIP2 MiniMonkey DeepSeek-VL-7B LISA-7B CogVLM-Chat ShareGPT4V-7B ShareGPT4V-13B 26.80 28.44 33.64 29.10 3.30 30.64 2.03 26.33 1.12 34.01 25.42 30.46 25.48 29.30 27.23 29.82 30.07 31.26 25.90 26.88 19.99 31.39 42.78 3.30 26.67 3.61 21.28 28.90 2.77 25.48 25.10 24.98 28.29 30.25 22.34 3.19 17.79 32.81 16.86 1.00 2.96 25.73 25.01 27.67 14.78 29.63 24.32 9.17 0.00 25.04 23.19 23.23 18.10 17.60 21.78 21.31 6.10 22.06 1.86 23.70 1.11 42.06 20.37 12.26 10.78 11.67 11.65 13.40 13.75 40.07 41.43 34.62 31.33 45.60 69.20 5.40 60.20 57.60 70.60 15.17 2.71 18.21 20.82 22.15 24.61 35.99 15.39 3.57 63.00 63.80 56.80 63.60 64.00 22.48 23.54 27.40 8.88 33.93 20.98 64.40 0.00 37.77 28.22 29.06 15.14 14.79 15.99 7.75 4.14 9.71 0.93 14.10 4.19 41.87 15.79 5.86 7.04 7.64 7.28 8.30 7.05 28.78 26.53 21.72 17.94 51.60 44.60 3.87 36.80 19.60 29.60 9.45 2.83 11.82 9.67 15.64 11.98 20.52 10.86 4.36 37.20 23.20 28.60 24.80 24.40 8.39 10.19 11.43 2.75 11.57 8.68 27.00 0.00 24.74 15.83 21.96 17.70 18.52 20.32 26.56 2.27 22.50 1.08 18.79 0.52 36.71 16.91 18.50 12.22 13.55 12.92 13.97 14.22 32.27 29.89 22.77 20.13 39.12 38.83 1.67 33.95 35.77 38.93 14.99 1.71 16.40 17.08 18.48 22.22 33.28 15.86 2.03 2.20 5.00 0.00 0.00 0.00 23.73 18.67 23.07 10.86 31.67 17.47 0.40 0.00 25.41 18.39 26.99 284 17.43 20.58 21.18 24.08 0.73 26.23 0.05 21.22 0.69 45.25 19.55 12.41 10.99 11.79 12.92 13.12 12.06 35.84 28.32 25.92 19.03 25.42 26.18 0.73 22.26 31.43 38.93 18.27 0.61 16.65 18.48 20.99 31.37 34.39 16.65 0.72 42.92 44.55 24.36 24.10 0.00 19.67 22.67 20.02 13.24 22.47 24.01 39.87 0.00 26.30 16.80 24. 46.32 18.50 57.76 12.04 1.05 12.30 0.41 49.90 9.47 15.73 55.99 27.83 34.89 22.52 23.57 24.63 28.23 32.42 26.08 25.93 20.62 17.02 8.28 9.05 15.69 20.13 70.60 3.21 0.14 5.38 14.83 31.77 8.96 54.86 28.93 0.16 14.65 22.60 12.91 0.00 10.70 18.19 17.56 24.11 13.61 30.97 28.28 7.19 0.00 23.17 19.34 23.99 33.22 18.91 29.47 21.72 2.19 21.58 1.22 25.67 0.61 29.36 20.86 20.27 17.12 19.45 16.45 23.01 22.35 39.32 32.91 28.18 27.87 50.29 50.89 11.60 50.69 53.69 70.60 9.33 1.70 16.29 22.94 21.40 11.52 27.17 14.32 1.93 52.49 50.09 53.89 46.30 46.30 18.58 19.65 22.53 13.12 26.53 19.85 47.70 0.00 33.51 23.24 29.35 71.20 70.60 72.60 54.80 60.00 71.60 56.40 70.00 63.00 75.40 85.83 64.00 64.00 64.00 64.00 64.00 64.00 64.60 59.20 53.40 50.00 31.76 31.73 59.30 28.50 24.29 70.60 59.00 63.70 61.40 72.30 55.40 53.72 59.44 48.28 50.13 30.51 30.43 27.50 5.37 25.00 57.20 67.40 63.80 43.40 67.60 55.20 24.55 0.00 57.40 48.40 51.60 48.60 32.80 46.20 24.40 23.40 26.20 24.40 27.20 24.40 30.80 25.80 24.40 24.40 24.40 24.40 24.40 24.40 41.40 39.00 37.80 34.60 51.66 50.05 20.80 38.05 55.51 70.60 28.00 22.60 24.60 0.00 25.00 27.00 30.40 24.40 31.20 9.17 16.63 25.00 20.01 25.00 22.80 25.40 27.20 14.60 27.80 31.80 46.70 0.00 41.60 36.00 35.60 29.20 11.20 24.20 0.00 0.00 1.80 0.00 35.00 22.40 0.00 18.60 0.00 37.05 35.05 34.42 0.00 0.00 52.00 39.60 40.60 32.00 52.69 31.95 0.00 46.41 29.66 57.66 41.96 7.78 0.00 0.00 43.29 36.59 44.65 42.66 7.78 51.42 1.30 0.00 0.00 0.00 2.60 4.20 4.80 1.20 0.00 3.20 21.73 0.00 42.80 35.20 40.60 30.89 26.82 34.41 32.15 36.43 33.92 25.68 33.89 29.18 8.19 22.95 43.85 21.91 21.97 20.65 20.47 18.59 47.62 42.33 38.71 29.10 48.58 41.91 28.67 38.50 51.58 21.28 38.80 27.42 34.70 36.75 27.91 30.15 42.87 34.28 38.93 50.62 44.39 44.65 10.79 12.95 28.53 35.85 36.67 19.23 35.29 36.68 17.69 0.00 39.87 32.22 38. On Path to Multimodal Generalist: General-Level and General-Bench"
        },
        {
            "title": "Model",
            "content": "BLIP-3 (XGen-MM) AnyGPT MiniCPM3-4B LaVIT-V2 (7B) GLM-VL-Chat Gemini-1.5-Pro Gemini-1.5-Flash OMG-LLaVA-InternLM20B Idefics3-8B-Llama3 Yi-Vision-v2 Qwen2-VL-72B Otter Show-o NExT-Chat InternVL2-26B Qwen2-VL-72B DeepSeek-VL-2-small DeepSeek-VL-2 LLaVA-One-Vision-7B LLaVA-One-Vision-72B Sa2VA-8B Sa2VA-26B CoLVA-2B CoLVA-4B Long-LLaVA LM4LV Vitron-V1 PandaGPT (13B) AnyGPT GAMA Pengi SALMONN-7B SALMONN-13B WavLLM ImageBind-LLM Unified-io-2-XXL ModaVerse-7b-v0 AudioGPT-GPT4 SpeechGPT-7B-com LLaMA-Omni 3D-LLM PointLLM-7B PointLLM-13B 3D-VisTA MotionGPT-T5 MotionGPT-LLaMA AvatarGPT LLaMA-Mesh #L-5 (Non-Trad QA) #L-6 (Advance QA) #L-7 (Math Ability) #1 #2 #3 #1 #2 #3 #4 #5 #1 #2 #3 21.70 7.64 29.80 24.22 26.52 32.95 28.39 8.62 26.36 11.36 17.87 1.78 24.04 6.93 29.90 40.40 18.30 18.60 35.90 43.20 37.80 24.10 30.90 26.10 31.10 0.00 0.73 0.73 7.64 13.97 13.89 3.56 5.76 14.34 8.45 0.89 1.56 10.23 13.45 7.45 43.83 0.72 0.73 0.33 0.16 1.68 1.58 0.72 23.61 8.71 27.26 23.68 24.85 23.57 21.68 6.21 18.62 22.98 32.39 0.23 11.25 0.00 39.10 68.80 25.90 33.80 63.50 75.80 52.70 59.20 20.60 29.40 31.70 0.00 9.05 0.94 8.71 0.31 0.23 0.06 0.00 0.32 0.34 0.00 0.30 34.67 0.20 6.23 0.00 0.77 0.00 0.26 0.00 1.52 0.11 0.93 33.51 13.25 31.46 32.52 29.63 25.68 26.45 18.07 22.47 18.79 18.81 3.41 8.43 3.96 25.20 25.10 17.60 20.10 35.40 53.20 33.80 30.20 23.70 32.30 18.30 0.00 16.10 2.07 13.25 1.16 1.02 2.41 3.20 1.07 1.04 13.56 1.34 15.67 1.00 32.23 30.84 2.13 2.12 0.11 0.07 1.66 1.19 1. 53.40 18.80 55.20 39.40 54.20 66.60 61.60 18.80 60.20 64.00 67.21 60.03 37.82 64.00 70.60 73.40 72.00 73.80 71.40 77.00 74.40 73.00 61.60 72.60 66.60 0.00 53.90 56.96 18.80 68.30 58.33 62.52 48.15 29.78 35.22 55.40 54.36 63.59 78.60 73.40 63.80 64.00 55.40 64.00 64.00 55.20 64.00 68.60 35.20 14.20 43.40 28.20 41.00 29.40 26.80 15.80 25.20 24.40 27.30 23.60 31.00 24.40 29.40 30.10 26.60 29.40 44.20 52.20 51.80 55.20 23.60 47.80 24.20 0.00 21.20 5.63 14.20 0.00 0.00 20.00 20.40 0.00 0.00 10.40 5.06 24.30 0.00 48.60 24.40 24.40 23.80 24.40 24.40 24.80 24.40 24.60 41.60 13.00 48.20 29.20 46.80 38.20 34.20 6.40 33.20 33.49 43.00 7.84 6.57 0.00 12.80 47.80 4.40 15.20 7.00 8.60 15.80 17.40 5.40 14.00 0.60 0.00 0.00 0.00 13.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 37.01 0.00 29.20 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 38.25 13.45 41.36 30.68 41.84 36.80 35.40 17.80 31.80 20.04 31.16 20.83 7.14 8.57 43.50 35.10 29.20 28.40 45.20 44.90 34.20 35.20 27.50 34.80 37.10 0.00 26.87 8.75 13.45 7.03 3.98 14.51 15.78 11.45 10.29 6.57 8.34 21.89 8.35 30.89 12.09 34.64 31.16 0.00 0.03 14.34 0.90 39.22 24.30 10.04 32.94 21.39 32.60 29.58 31.04 7.09 30.14 23.42 29.19 41.37 10.81 14.70 25.60 31.70 25.90 25.60 33.10 34.40 28.60 28.70 27.60 30.60 22.50 0.00 3.10 2.30 10.04 21.54 19.78 9.98 10.32 20.73 10.45 1.20 2.10 24.58 18.34 26.80 18.51 3.27 3.30 0.51 0.81 4.75 8.53 3.24 34.92 13.99 37.32 31.47 38.71 23.54 19.82 9.38 22.15 15.03 18.23 1.81 30.80 3.92 22.40 38.10 21.60 18.60 27.90 22.60 33.70 23.60 28.40 22.00 22.40 0.00 14.30 4.70 13.99 15.39 13.67 5.37 5.68 14.67 7.56 11.10 4.50 10.70 14.67 18.10 35.77 6.15 6.08 0.62 0.00 2.24 2.16 6. 24.63 0.00 22.96 18.52 19.90 14.52 11.75 8.61 12.56 15.12 11.01 2.26 6.87 7.27 15.20 41.20 12.00 12.30 12.60 32.60 45.60 24.10 9.10 12.20 8.60 0.00 3.87 3.19 0.00 7.79 4.58 5.39 6.89 6.45 4.34 5.67 2.57 7.04 6.45 15.14 16.70 4.13 4.29 3.47 0.85 3.56 3.77 5.98 26.70 11.03 24.09 24.23 21.72 34.49 31.64 6.24 32.20 14.57 30.03 2.34 19.14 6.41 21.60 41.70 17.30 17.10 28.80 35.80 26.80 21.30 24.30 21.20 18.00 0.00 13.76 1.94 11.03 18.91 15.34 2.83 3.21 16.78 6.78 0.00 1.54 12.22 15.34 16.70 44.61 2.20 2.25 0.61 0.09 2.31 1.59 2.10 285 On Path to Multimodal Generalist: General-Level and General-Bench Table 103: Results on NLP Group, from #L-8 to #L-11."
        },
        {
            "title": "Model",
            "content": "#L-8 (Code Ability) #L-9 (X-Lan&NMT) #L-10 (Txt Sum) #L-11 (Dialog Gen) #1 #2 #3 #4 #5 #1 #2 #3 #4 #1 #2 #3 #"
        },
        {
            "title": "SoTA Specialist",
            "content": "80.50 79.50 75.06 76.94 77.20 71.06 83.61 72.83 91.30 48.16 76.23 91.35 28.27 Meta-Llama-3.1-8B-Instruct Qwen2.5-7B-Instruct Gemma-2-9b-it ChatGLM-6b Vicuna-7b-v1.5 InternLM-Chat-7b GPT-J-6B Falcon3-7B-Instruct Baichuan2-7B-Base Ministral-8B-Instruct-2410 Yi-Ligntning GPT-3.5-turbo GPT-4v GPT-4o GPT4o-mini GPT-4o-4096 ChatGPT-4o-latest Claude-3.5-Sonnet Claude-3.5-Opus Emu2-32B DetGPT InternVL2.5-8B InternVL2.5-4B NExT-GPT-V1.5 InternVL2.5-2B Monkey-10B-chat DeepSeek-VL-7B Qwen2-VL-7B Qwen-VL-Chat Qwen-Audio-Chat Qwen2-Audio-Instruct MoE-LLAVA-Phi2-2.7B-4e-384 mPLUG-Owl2-LLaMA2-7b Phi-3.5-Vision-Instruct Cambrian-1-8B MiniGPT4-LLaMA2 InternVL-Chat-V1-5 Mini-InternVL-Chat-4B-V1-5 InternLM-XComposer2-VL-1.8B GPT4RoI GLaMM LLaVA-NeXT-13B LLaVA-NeXT-34B Pixtral-12B SEED-LLaMA-13B BLIP2 MiniMonkey DeepSeek-VL-7B LISA-7B CogVLM-Chat ShareGPT4V-7B ShareGPT4V-13B 5.34 0.98 8. 17.59 1.50 8.13 19.72 47.31 32.72 37.18 54.06 61.57 21.24 76.06 47.56 35.00 50.92 11.72 54.49 35.18 53.19 54.39 56.55 66.23 30.63 76.70 47.90 33.47 49.16 18.11 49.10 33.06 59.71 55.22 53.25 71.21 11.99 79.77 36.49 36.88 50.18 11.13 46.71 22.25 56.28 43.05 23.50 39.20 24.54 24.16 43.80 22.69 49.04 53.69 24.58 52.13 42.18 53.36 60.79 30.15 74.92 45.00 28.18 48.56 3.79 42.63 51.67 25.74 59.02 43.33 26.05 47.63 53.69 26.96 54.17 7.33 13.83 22.76 10.61 27.38 8.48 53.19 17.64 44.19 2.21 12.70 53.29 35.64 47.73 61.20 54.26 59.21 30.75 78.92 46.71 32.30 47.20 18.02 46.51 33.39 49.72 50.66 37.06 54.76 13.24 58.91 22.42 12.63 20.43 16.74 49.90 12.77 29.54 56.93 70.39 21.21 78.95 47.77 33.55 49.06 20.95 49.70 37.06 56.55 53.59 60.59 74.60 31.47 83.43 43.32 32.40 50.38 17.51 54.09 33.62 39.57 37.16 63.11 77.81 33.81 86.27 46.69 32.20 50.18 27.81 48.70 35.28 35.05 33.51 63.12 79.38 32.70 86.25 43.08 32.87 50.39 28.47 51.70 33.80 36.01 33.20 64.63 81.15 34.15 87.52 44.59 32.80 50.67 29.30 51.30 36.24 37.52 32.79 64.87 79.53 34.15 87.63 44.44 33.42 50.97 29.30 47.90 34.00 36.37 33.50 64.24 80.61 33.27 87.42 43.00 32.65 51.12 28.60 49.30 35.93 35.82 33.28 64.18 80.93 33.16 87.38 44.10 33.39 50.28 33.84 54.49 51.79 59.84 41.81 55.89 52.89 31.70 65.44 43.18 44.77 73.60 37.47 51.30 46.75 56.24 45.08 57.25 52.53 29.57 63.71 36.54 46.37 62.96 32.04 44.91 45.72 46.81 39.13 52.12 47.20 25.60 56.98 35.04 37.31 66.70 21.64 38.92 37.46 42.17 33.02 42.92 40.35 20.22 50.60 33.39 33.55 57.84 55.64 27.19 62.17 39.17 30.91 44.99 13.88 33.23 31.30 19.89 44.79 89.00 50.96 25.71 57.20 42.76 29.77 46.58 12.58 26.61 30.53 24.94 41.01 89.68 2.60 50.34 19.56 50.25 40.61 51.90 56.63 29.68 67.75 41.50 25.40 46.30 32.80 27.27 47.17 40.82 27.67 42.10 12.37 35.38 27.72 22.89 42.25 86.33 62.68 91.14 8.00 61.05 26.19 72.42 45.67 32.96 45.92 61.84 30.88 72.05 47.26 31.63 47.87 14.46 43.94 30.94 51.35 88.00 52.00 21.93 40.53 24.82 29.55 51.07 15.98 45.08 35.21 23.71 47.81 7.19 53.69 26.76 45.80 2.37 35.24 50.27 15.34 63.81 24.47 16.08 40.63 8.70 50.89 19.75 42.35 48.19 53.76 61.65 29.92 76.89 44.99 26.21 48.40 13.57 53.69 20.76 43.56 40.36 57.88 60.68 28.59 78.96 41.77 29.40 48.45 53.69 23.32 53.18 53.79 55.75 62.18 11.40 75.50 46.05 33.73 48.80 8.13 8.50 46.91 31.44 50.86 43.54 31.63 27.47 12.00 50.07 43.70 31.77 48.15 18.92 53.09 33.90 58.48 54.41 55.85 49.05 13.62 74.88 47.24 31.66 49.57 53.49 24.93 63.10 14.25 36.46 60.68 10.69 54.16 34.43 28.95 34.16 2.37 2.51 53.69 35.05 55.30 68.62 35.30 46.96 11.57 47.05 37.60 22.71 48.11 57.70 30.73 65.14 44.44 44.44 47.94 13.49 43.81 28.73 16.33 39.75 83.44 41.39 24.82 61.32 43.74 29.68 46.01 12.77 36.12 29.01 13.13 43.70 84.72 38.12 26.67 58.53 46.80 32.32 46.73 12.45 18.42 18.91 22.65 36.37 89.48 17.63 23.03 15.18 17.10 4.23 7.10 20.84 21.51 40.08 4.94 0.00 7.96 36.33 20.27 33.54 19.56 36.23 32.17 6.32 36.07 32.18 25.54 44.36 8.54 34.93 29.87 45.67 27.05 40.98 35.86 14.76 57.21 36.32 34.89 45.60 11.12 48.70 27.43 40.28 26.79 37.43 36.27 13.24 45.04 36.25 32.56 44.09 20.15 24.31 14.23 32.14 26.19 32.94 6.32 3.67 37.23 31.77 13.81 48.51 33.50 34.77 44.32 14.91 53.09 28.92 51.37 49.53 46.17 39.55 31.19 63.30 42.16 29.16 49.10 23.61 29.68 16.64 43.10 11.22 30.94 15.02 20.13 17.55 67.17 34.22 0.00 0.00 0.00 27.55 44.31 46.48 46.92 40.04 52.95 50.06 26.57 58.20 35.14 41.01 61.89 25.34 40.32 39.39 44.56 33.93 43.31 38.89 20.14 49.65 33.37 32.77 57.34 27.13 45.11 40.78 46.31 34.36 48.81 43.03 23.19 55.32 36.16 34.16 61.82 17.79 22.13 13.56 20.58 16.10 13.26 21.56 5.40 27.94 15.79 22.01 22.18 16.73 29.64 36.08 33.61 1. 9.92 2.14 8.47 4.67 0.00 7.39 6.22 2.52 0.00 0.00 0. 0.00 0.00 0.00 0.00 0.00 7. 7.21 9.56 0.00 14.80 15.34 13.34 13.93 5.06 13.01 3.02 15.56 4.92 13.73 15.29 14.40 13.96 14.49 13.51 15.49 15.08 20.70 17.39 13.05 11.12 9.07 7.82 5.06 9.07 11.00 10.37 12.62 12.83 10.47 12.08 11.16 10.65 12.74 8.90 10.42 0.00 0.00 0.00 0.00 0.00 9.57 12.88 12.00 4.80 8.56 12.73 11.07 0.00 16.03 10.51 12.86 286 On Path to Multimodal Generalist: General-Level and General-Bench #L-8 (Code Ability) #L-9 (X-Lan&NMT) #L-10 (Txt Sum) #L-11 (Dialog Gen) #1 #2 #3 #4 #5 #1 #2 #3 #4 #1 #2 #3 #1 7. 9.41 9.47 9.96 4.43 6.91 2.25 5.89 3.62 9.76 1.37 2.39 30.37 13.22 12. 14.98 25.23 26.27 18.30 25.69 47.33 17.20 18.67 7.61 3.65 23.81 25.11 4.54 46.30 29.16 44.71 46.65 50.89 39.95 50.52 44.40 23.96 57.25 34.78 34.46 63.27 19.59 12.22 15.13 21.36 12.37 17.40 17.53 17.56 14.45 18.44 17.13 35.26 43.60 48.38 47.14 45.70 46.22 45.74 34.05 54.93 37.62 41.81 65.77 24.75 39.80 36.70 45.34 28.72 39.30 33.25 26.56 41.02 28.57 35.75 54.39 30.68 44.40 48.68 48.28 45.02 47.93 45.57 31.90 53.90 34.14 38.47 63.65 25.60 52.30 35.24 40.91 43.54 58.65 68.28 38.72 85.46 44.08 32.73 49.32 23.21 48.70 33.05 36.72 33.20 52.75 64.15 32.93 80.12 38.24 31.34 50.15 9.38 23.35 12.62 12.47 26.74 18.49 51.30 32.80 36.87 32.96 47.93 52.87 14.33 68.93 36.85 28.12 51.32 5.47 53.09 31.72 35.39 31.72 60.47 72.07 21.26 54.83 46.97 27.92 48.72 25.07 52.66 32.31 39.28 50.83 64.91 82.45 28.42 84.52 45.12 31.45 49.72 28.02 17.26 24.31 5.43 9.22 3.41 0.00 0.00 10.73 9.60 0.00 10.40 27.04 20.52 29.58 18.20 51.10 31.80 50.80 54.10 41.30 54.50 26.90 61.00 44.20 29.70 48.20 24.80 53.40 32.30 52.10 26.10 61.10 75.40 30.60 84.10 45.30 31.70 49.90 13.80 46.90 29.80 66.20 49.40 56.00 60.70 30.80 73.60 41.30 29.90 46.00 20.00 44.70 31.90 62.60 55.50 56.10 64.60 31.30 75.30 43.50 30.10 46.80 21.50 53.70 34.10 63.10 59.50 53.30 62.40 29.50 75.70 44.90 23.20 49.30 11.80 53.50 35.30 45.30 70.50 56.40 70.60 18.30 75.70 44.10 29.40 50.90 14.40 52.30 35.60 59.90 56.60 57.30 62.80 31.10 76.00 45.60 33.20 49.60 13.40 51.30 35.60 58.50 45.50 59.70 60.90 34.20 79.80 25.70 33.30 50.50 8.80 54.10 29.10 42.80 51.80 42.20 36.20 26.40 58.20 41.90 27.90 46.40 11.80 53.70 34.00 52.10 55.10 54.30 60.50 29.70 74.50 44.70 30.90 49.40 16.90 46.30 25.20 62.30 41.50 44.30 46.00 30.30 75.00 43.30 21.90 49.10 0.00 0.00 12.30 50.34 15.62 50.23 41.09 50.37 46.63 26.78 67.75 41.30 24.60 44.60 22.39 0.68 6.46 5.37 19.59 12.22 15.13 21.36 12.37 17.40 17.53 17.56 14.45 18.44 17.13 0.38 0.65 0.03 0.59 5.49 53.69 18.79 34.56 67.50 0.34 0.23 0.04 0.05 0.34 3.54 23.56 14.34 24.78 67.50 0.27 9.87 15.42 53.89 10.89 25.56 56.36 11.43 12.42 1.11 9.99 2.65 11.34 17.65 11.54 55.89 11.24 27.34 48.33 12.54 13.69 1.39 2.99 0.58 0.36 0.06 0.56 54.67 19.20 30.68 67.50 5.60 0.45 0.00 0.00 0.34 46.78 15.45 29.58 67.50 5.46 0.00 0.03 13.56 21.50 2.52 14.67 4.21 6.70 3.67 0.68 6.46 3.89 7.37 22.39 5.37 43.50 23.60 46.60 54.58 27.56 48.50 35.46 35.45 33.51 63.05 78.50 33.24 84.34 43.08 32.45 51.23 45.67 20.34 36.59 67.50 0.05 5.67 0.23 0.03 8.67 54.06 61.57 21.24 16.45 17.56 15.00 20.94 19.72 47.31 32.72 37.18 32.50 29.52 22.22 0.65 4.04 27.70 35.62 51.18 0.00 2.13 31.93 30.18 50.53 20.17 63.64 42.48 25.13 40.92 0.00 3.69 41.05 42.57 55.99 25.94 70.58 44.87 30.72 41.31 0.00 3.53 1.70 0.01 0.00 0.65 2.42 2.41 0.06 2.93 0.86 0.46 0.40 0.05 0.23 0.66 0.31 0.08 0.01 0.02 41.80 20.25 43.45 5.88 0.34 2.58 18.54 26.14 45.71 10.73 46.31 8.44 9.42 9.50 0.98 1.06 0.00 8.00 2.24 53.69 30.35 53.56 54.75 54.69 72.44 26.26 76.21 45.89 28.61 47.44 3.86 0.09 0.06 10.46 15.84 10.31 15.78 7.56 0.00 46.31 0.00 53.69 0.00 53.69 0.00 46.31 4.94 46.31 46.91 20.50 3.63 3.84 9.47 0.00 0.00 6.40 6.56 0.00 0.00 5.75 3. 45.37 24.58 52.13 54.58 0.05 0.02 7.41 1.64 0.08 0.05 0.61 0.00 0. 0.41 0.00 0.00 0.00 0.00 0. 0.00 0.00 0.00 0.00 0.00 7. 7.37 3.89 13.21 0.00 19.79 13.20 20.85 14.27 12.74 3.02 11.08 12.69 17.27 6.51 0.00 8.18 13.80 17.80 15.20 16.30 17.70 17.70 18.60 16.90 12.80 17.20 6.30 0.00 5.30 0.00 0.00 7.01 4.58 1.15 1.37 6.45 6.32 0.00 0.00 14.05 6.79 4.80 4.87 4.79 4.71 0.21 0.26 3.50 8.63 4."
        },
        {
            "title": "Model",
            "content": "BLIP-3 (XGen-MM) AnyGPT MiniCPM3-4B LaVIT-V2 (7B) GLM-VL-Chat Gemini-1.5-Pro Gemini-1.5-Flash OMG-LLaVA-InternLM20B Idefics3-8B-Llama3 Yi-Vision-v2 Qwen2-VL-72B Otter Show-o NExT-Chat InternVL2-26B Qwen2-VL-72B DeepSeek-VL-2-small DeepSeek-VL-2 LLaVA-One-Vision-7B LLaVA-One-Vision-72B Sa2VA-8B Sa2VA-26B CoLVA-2B CoLVA-4B Long-LLaVA LM4LV Vitron-V1 PandaGPT (13B) AnyGPT GAMA Pengi SALMONN-7B SALMONN-13B WavLLM ImageBind-LLM Unified-io-2-XXL ModaVerse-7b-v0 AudioGPT-GPT4 SpeechGPT-7B-com LLaMA-Omni 3D-LLM PointLLM-7B PointLLM-13B 3D-VisTA MotionGPT-T5 MotionGPT-LLaMA AvatarGPT LLaMA-Mesh 287 On Path to Multimodal Generalist: General-Level and General-Bench Table 105: Results on NLP Group, from #L-12 to #L-16."
        },
        {
            "title": "Model",
            "content": "#L-12 (TxT Gen) #L-13 (Time Series) #L-14 (Txt Cls) #L-15 (Txt Entail) #L-16 (Sem Analy) #1 #2 #3 #4 #5 #1 #1 #1 #1 #2 #3 #"
        },
        {
            "title": "SoTA Specialist",
            "content": "93.02 75.19 83.46 85.89 97.19 0.31 94.40 91.41 88.00 98.00 85.40 72.80 39.83 29.90 31.12 42.10 83.74 Meta-Llama-3.1-8B-Instruct 50.67 33.58 31.85 47.24 90.90 Qwen2.5-7B-Instruct 36.37 25.40 22.22 9.33 86.00 Gemma-2-9b-it 32.84 27.89 30.32 48.30 74.85 ChatGLM-6b 30.01 30.10 23.34 46.16 90.29 Vicuna-7b-v1.5 32.59 26.43 22.48 9.72 91.15 InternLM-Chat-7b 13.98 9.38 31.53 10.69 9.46 GPT-J-6B 44.34 35.09 26.53 44.22 90.59 Falcon3-7B-Instruct 49.50 36.31 14.53 55.40 93.31 Baichuan2-7B-Base 15.54 16.33 32.30 23.68 29.11 Ministral-8B-Instruct-2410 52.74 34.45 28.52 52.30 95.39 Yi-Ligntning 35.00 31.90 27.20 38.32 83.48 GPT-3.5-turbo 40.36 32.15 28.03 39.08 83.20 GPT-4v 43.98 33.99 29.40 41.49 83.20 GPT-4o 42.97 32.93 25.60 42.10 82.31 GPT4o-mini 43.32 34.24 28.16 43.04 83.06 GPT-4o-4096 41.21 32.09 22.33 42.67 82.63 ChatGPT-4o-latest 59.94 48.94 44.02 47.91 90.18 Claude-3.5-Sonnet 58.00 47.16 35.40 42.21 88.65 Claude-3.5-Opus 51.27 43.36 31.58 42.43 82.09 Emu2-32B 44.80 38.08 26.30 37.63 76.72 DetGPT 87.00 72.20 54.60 54.60 91.40 InternVL2.5-8B 81.80 70.40 69.00 69.20 90.20 InternVL2.5-4B 25.40 30.40 22.98 45.23 86.43 NExT-GPT-V1.5 83.20 69.20 25.40 30.20 85.40 InternVL2.5-2B 0.00 65.60 0.00 38.60 8.80 Monkey-10B-chat 78.40 68.80 83.00 83.00 85.20 DeepSeek-VL-7B 32.99 17.98 14.83 34.13 86.23 Qwen2-VL-7B 14.75 22.33 32.49 8.70 8.24 Qwen-VL-Chat 44.56 34.02 30.28 45.78 89.65 Qwen-Audio-Chat 43.34 32.16 30.73 40.75 88.39 Qwen2-Audio-Instruct 38.88 31.12 2.10 40.69 91.02 MoE-LLAVA-Phi2-2.7B-4e-384 34.43 33.43 28.22 44.03 90.75 mPLUG-Owl2-LLaMA2-7b 47.43 35.54 27.99 42.16 92.78 Phi-3.5-Vision-Instruct 31.08 27.58 2.68 41.38 90.04 Cambrian-1-8B 28.55 34.40 32.58 31.91 85.35 MiniGPT4-LLaMA2 65.80 73.20 69.80 68.40 89.20 InternVL-Chat-V1-5 Mini-InternVL-Chat-4B-V1-5 65.80 44.60 53.40 56.80 84.80 InternLM-XComposer2-VL-1.8B 72.80 60.60 52.40 52.40 85.60 0.00 GPT4RoI 0.00 0.00 GLaMM 0.00 0.00 0.00 30.70 21.54 23.34 28.77 70.14 LLaVA-NeXT-13B 27.31 25.17 21.52 30.19 79.86 LLaVA-NeXT-34B 34.42 26.08 23.77 33.09 77.35 Pixtral-12B 17.58 13.29 13.04 17.92 42.39 SEED-LLaMA-13B 27.41 22.16 25.04 18.99 84.01 BLIP2 37.57 33.30 21.51 52.99 88.93 MiniMonkey 0.35 0.80 1.40 DeepSeek-VL-7B LISA-7B 0.00 0.00 0.00 53.67 42.22 31.42 42.56 81.70 CogVLM-Chat 48.38 36.65 30.58 38.87 79.03 ShareGPT4V-7B 52.42 39.82 29.59 38.93 81.80 ShareGPT4V-13B 2.80 0.00 0.00 0.00 0.00 0.00 0.00 0.00 7.95 5.74 5.75 10.91 11.41 10.23 11.41 5.15 7.85 11.08 5.37 1.55 3.16 2.58 4.24 3.08 2.73 3.88 3.67 9.53 16.87 75.20 70.80 1.06 79.60 20.40 83.00 6.48 7.46 0.98 1.23 6.35 6.16 3.80 3.14 7.46 73.80 68.60 5.60 10.24 8.85 8.12 11.16 6.43 10.57 20.45 27.95 24.69 76.40 88.20 84.46 41.80 0.00 77.80 0.00 88.80 0.00 84.80 72.60 73.80 86.20 85.40 82.00 87.40 88.60 68.59 30.74 57.54 52.97 55.40 46.00 68.90 50.60 0.20 62.20 64.00 0.00 74.50 77.60 64.20 76.20 58.40 59.80 0.00 48.40 48.60 0.00 0.00 0.00 77.24 74.83 80.91 13.20 79.00 84.00 0.80 0.00 53.55 49.45 54. 51.80 79.35 55.82 45.81 0.00 6.87 0.00 85.89 0.00 72.60 56.24 63.19 83.23 86.30 86.50 83.84 77.51 52.58 30.74 48.78 46.25 68.40 61.80 43.20 51.40 3.00 50.60 37.00 0.00 58.43 58.69 60.94 38.04 68.92 0.00 0.00 65.80 50.80 3.20 0.00 0.00 68.83 72.25 66.31 34.80 66.20 68.92 28.40 0.00 51.28 44.62 49.62 288 0.00 0.00 0.00 0. 44.79 80.00 75.00 63.80 66.40 49.00 63.80 46.80 75.60 83.80 62.20 48.00 0.00 50.20 47.80 0.00 0.00 0.00 0.00 0.00 58.80 14.20 31.00 3.60 0.00 0.00 64.20 26.80 44.40 47.20 0.00 0.00 53.80 67.00 61.60 44.40 78.20 70.60 66.40 43.80 50.20 41.80 55.80 28.00 85.60 80.60 66.20 28.00 71.40 90.20 69.00 39.40 82.00 84.40 58.60 47.40 80.20 88.00 71.20 40.40 80.40 89.60 69.00 41.60 47.87 51.86 59.26 41.41 30.74 30.74 30.74 30.74 44.02 46.12 50.11 34.79 42.05 43.83 44.60 30.15 48.80 52.40 82.80 42.99 48.20 54.60 80.60 39.20 32.43 28.90 33.65 20.15 48.20 51.60 83.60 42.40 10.60 4.00 16.80 8.60 54.20 54.20 84.40 56.40 2.00 12.00 0.00 0.00 0.00 0.00 59.30 37.50 50.10 45.30 61.00 38.40 51.40 47.40 58.80 78.20 53.00 43.00 36.20 25.80 59.00 32.40 70.40 77.00 59.20 46.80 13.40 30.40 19.80 3.20 0.00 0.00 54.40 55.60 79.80 30.00 50.20 52.40 64.40 72.40 0.00 21.20 0.00 0.00 0.00 0.00 0.00 0.00 0.00 49.40 66.80 48.80 45.00 49.60 72.60 53.40 46.20 54.80 71.20 51.80 44.80 30.20 32.20 25.10 28.40 48.40 73.80 52.20 46.40 46.00 74.60 50.80 46.80 0.00 32.00 2.20 0.00 0.00 0.00 45.25 47.31 49.32 29.67 37.27 44.93 48.38 27.06 45.91 43.94 48.08 34.05 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0. On Path to Multimodal Generalist: General-Level and General-Bench"
        },
        {
            "title": "Model",
            "content": "BLIP-3 (XGen-MM) AnyGPT MiniCPM3-4B LaVIT-V2 (7B) GLM-VL-Chat Gemini-1.5-Pro Gemini-1.5-Flash OMG-LLaVA-InternLM20B Idefics3-8B-Llama3 Yi-Vision-v2 Qwen2-VL-72B Otter Show-o NExT-Chat InternVL2-26B Qwen2-VL-72B DeepSeek-VL-2-small DeepSeek-VL-2 LLaVA-One-Vision-7B LLaVA-One-Vision-72B Sa2VA-8B Sa2VA-26B CoLVA-2B CoLVA-4B Long-LLaVA LM4LV Vitron-V1 PandaGPT (13B) AnyGPT GAMA Pengi SALMONN-7B SALMONN-13B WavLLM ImageBind-LLM Unified-io-2-XXL ModaVerse-7b-v0 AudioGPT-GPT4 SpeechGPT-7B-com LLaMA-Omni 3D-LLM PointLLM-7B PointLLM-13B 3D-VisTA MotionGPT-T5 MotionGPT-LLaMA AvatarGPT LLaMA-Mesh #L-12 (TxT Gen) #L-13 (Time Series) #L-14 (Txt Cls) #L-15 (Txt Entail) #L-16 (Sem Analy) #1 #2 #3 #4 #5 #1 #1 #1 #1 #2 #3 #4 0.00 0.00 0.00 0.00 0.00 0.00 47.80 48.81 53.39 31.05 17.92 18.21 17.06 11.55 46.27 55.05 54.65 36.88 33.18 38.75 43.22 26.92 46.31 52.65 53.46 37.94 82.40 86.40 64.80 47.20 74.20 80.20 58.60 39.80 28.60 20.40 21.60 13.20 71.00 78.20 60.40 38.40 63.20 85.40 60.20 42.60 85.33 82.00 69.33 44.00 0.00 0.00 0.00 0.00 0.00 0.00 64.00 64.80 52.00 42.40 84.60 86.60 68.00 43.20 59.00 86.20 61.80 46.40 64.20 85.60 60.80 49.40 65.20 60.40 45.80 47.80 82.40 61.20 64.40 29.40 83.80 65.40 59.00 47.20 75.20 59.00 65.00 44.60 30.40 67.30 25.40 35.80 79.00 66.20 27.60 45.60 41.40 66.80 56.20 46.20 0.00 0.00 33.47 26.80 31.51 23.62 0.00 0.00 15.60 0.00 18.21 17.06 11.55 19.32 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 22.20 7.20 6.60 0.00 15.60 0.00 0.00 0.00 5.40 0.00 0.00 0.00 0.00 3.40 0.00 44.30 5.60 23.50 0.00 0.00 0.00 15.60 0.00 84.70 86.50 66.40 29.70 0.00 0.00 43.81 60.30 25.40 34.50 0.00 51.00 53.60 0.40 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0. 0.00 51.43 38.84 31.86 41.39 84.73 17.27 16.66 12.12 15.78 28.03 52.48 51.50 43.03 46.59 91.38 45.79 36.18 31.67 40.84 67.22 54.43 49.68 41.76 44.94 88.66 52.98 41.15 37.65 46.32 92.34 49.43 33.48 30.06 41.47 88.95 24.83 21.02 15.89 18.73 52.60 47.65 34.92 29.01 42.15 84.39 35.39 29.41 25.08 36.80 76.91 31.34 31.24 37.84 38.85 84.19 13.98 17.65 9.29 11.66 14.88 8.26 11.20 4.53 4.35 3.11 11.43 16.88 1.92 9.43 26.49 41.20 31.20 15.40 38.90 88.40 42.50 35.30 0.30 42.30 94.70 39.30 34.60 28.20 44.90 92.60 41.00 35.90 26.20 44.50 94.00 29.90 34.30 11.20 42.80 93.90 51.80 32.30 11.30 39.70 83.90 41.40 35.90 26.00 45.40 93.80 41.50 36.10 27.20 46.10 94.40 30.90 33.10 21.20 54.90 90.00 47.60 33.70 24.90 44.80 92.30 41.20 29.80 31.60 50.40 89.20 0.00 0.00 0.00 27.80 30.20 23.10 45.68 85.93 6.07 36.48 28.67 4.78 17.27 16.66 12.12 15.78 28.03 1.92 8.11 7.74 6.04 0.89 10.89 6.43 7.94 4.38 6.03 30.93 81.93 4.86 10.76 6.45 30.78 84.67 1.64 11.32 5.46 18.95 0.46 1.23 13.27 0.21 3.26 5.78 0.00 0.00 0.00 0.00 0.00 6.07 36.48 28.67 5.47 4.78 40.23 33.16 28.04 39.05 84.34 0.37 8.94 1.56 15.78 9.29 3.89 9.90 31.12 42.10 68.40 2.08 24.46 2.85 59.26 65.27 27.05 27.77 1.32 31.27 79.90 28.84 28.58 1.01 35.02 78.19 1.45 0.00 0.00 5.92 0.00 0.00 19.65 7.15 20.58 14.94 9.31 7.88 1.65 12.21 0.09 30.73 32.20 31.62 41.21 85.18 0.29 0.05 9.57 1.31 0.86 0.30 0.08 0. 5.47 8.15 0.00 21.54 33.39 19.62 24.13 21.37 4.78 7.92 56.52 4.31 7.32 4.45 11.51 11.54 64.60 3.90 12.90 8.80 5.90 3.60 10.20 5.70 11.00 9.80 11.40 2.47 0.41 33.39 0.78 0.45 0.78 1.56 0.69 0.83 0.41 3.15 0.58 5.34 11.11 11.06 11.11 11.41 55.67 19.48 57.84 42.71 58.65 84.20 80.60 26.20 74.20 59.60 90.00 0.00 0.00 0.00 80.20 83.60 60.40 81.80 77.40 84.80 86.80 86.60 75.40 84.40 68.60 0.00 68.70 0.00 19.48 0.00 0.00 1.40 0.00 0.00 0.00 23.50 0.00 85.90 0.00 37.50 47.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 48.61 16.48 52.66 37.63 49.30 83.70 77.41 19.08 76.90 49.69 80.67 0.00 0.00 0.00 79.90 83.60 55.00 68.50 65.40 10.40 76.90 85.10 3.10 76.90 41.70 0.00 43.20 40.80 17.92 0.00 0.00 0.61 0.00 0.00 0.00 34.60 40.80 84.53 0.00 35.80 33.95 0.00 0.00 0.00 0.00 0.00 0.00 0. 289 On Path to Multimodal Generalist: General-Level and General-Bench Table 107: Results on NLP Group, #L-17, part A."
        },
        {
            "title": "Model",
            "content": "#L-17 (Affect Computing)"
        },
        {
            "title": "SoTA Specialist",
            "content": "93.40 95.80 97.33 95.80 96.80 97. 85.93 95.80 84.71 #1 #2 #3 #4 #5 #6 #7 #8 #9 Meta-Llama-3.1-8B-Instruct Qwen2.5-7B-Instruct Gemma-2-9b-it ChatGLM-6b Vicuna-7b-v1.5 InternLM-Chat-7b GPT-J-6B Falcon3-7B-Instruct Baichuan2-7B-Base Ministral-8B-Instruct-2410 Yi-Ligntning GPT-3.5-turbo GPT-4v GPT-4o GPT4o-mini GPT-4o-4096 ChatGPT-4o-latest Claude-3.5-Sonnet Claude-3.5-Opus Emu2-32B DetGPT InternVL2.5-8B InternVL2.5-4B NExT-GPT-V1.5 InternVL2.5-2B Monkey-10B-chat DeepSeek-VL-7B Qwen2-VL-7B Qwen-VL-Chat Qwen-Audio-Chat Qwen2-Audio-Instruct MoE-LLAVA-Phi2-2.7B-4e-384 mPLUG-Owl2-LLaMA2-7b Phi-3.5-Vision-Instruct Cambrian-1-8B MiniGPT4-LLaMA2 InternVL-Chat-V1-5 Mini-InternVL-Chat-4B-V1-5 InternLM-XComposer2-VL-1.8B GPT4RoI GLaMM LLaVA-NeXT-13B LLaVA-NeXT-34B Pixtral-12B SEED-LLaMA-13B BLIP2 MiniMonkey DeepSeek-VL-7B LISA-7B CogVLM-Chat ShareGPT4V-7B ShareGPT4V-13B 60.40 53.20 56.80 3.00 0.00 49.00 0.00 53.60 0.00 53.00 46.60 53.80 62.00 59.60 53.60 58.60 57.40 65.61 30.74 53.44 48.97 92.00 92.60 0.00 44.80 45.40 87.00 56.00 0.00 46.60 48.20 49.80 53.80 57.60 45.00 0.00 82.60 69.00 0.00 0.00 0.00 47.20 56.80 53.40 27.80 51.60 57.80 0.00 0.00 59.80 51.16 52.77 53.40 63.60 68.60 0.00 0.00 59.40 0.00 65.80 0.00 60.20 73.60 65.80 68.40 80.20 72.60 80.60 80.20 51.42 30.74 47.58 42.55 45.70 42.99 15.34 34.40 25.40 46.00 53.00 0.00 45.80 48.60 56.00 55.60 50.60 42.40 0.00 39.80 47.00 40.20 0.00 0.00 51.60 52.20 57.20 35.40 53.60 54.80 0.00 0.00 45.53 39.94 46.04 79.60 92.40 89.40 83.80 0.00 59.00 0.00 90.60 1.00 83.20 94.20 92.60 94.00 92.20 91.60 85.20 92.20 67.78 30.74 57.62 48.73 55.60 41.60 0.00 43.20 5.40 66.80 27.00 0.00 68.30 70.00 92.20 84.80 88.60 0.00 0.00 61.00 54.00 20.40 0.00 0.00 81.60 82.20 88.80 57.40 84.40 90.60 13.60 0.00 57.04 51.54 50.41 290 83.60 79.80 77.80 0.00 0.00 19.60 0.00 87.00 0.00 73.00 85.20 83.40 92.00 92.00 81.40 90.60 91.00 72.64 30.74 63.95 58.75 75.25 82.41 21.34 85.27 37.21 59.91 59.00 0.00 47.50 47.80 80.00 54.80 86.20 63.40 0.00 77.70 62.78 75.66 0.00 32.31 60.20 63.40 67.40 16.75 64.80 61.60 0.00 0.00 68.49 62.69 65. 75.20 77.40 74.60 60.20 0.00 63.00 0.00 72.60 0.00 60.00 78.20 69.60 76.60 80.00 75.00 79.20 78.00 60.45 30.74 53.24 45.39 78.60 67.20 18.59 39.00 49.60 48.60 9.00 0.00 74.50 73.20 92.20 68.80 60.20 3.00 0.00 66.20 61.80 24.20 0.00 0.00 62.20 65.50 67.30 39.60 67.20 64.60 0.00 0.00 52.96 45.67 52.99 51.80 57.40 55.40 0.00 0.00 1.80 0.00 60.00 0.00 52.40 59.00 52.20 59.40 60.40 56.80 60.40 59.40 51.51 30.74 39.91 37.59 41.99 43.79 13.29 43.20 0.60 46.60 41.00 0.00 31.90 32.20 47.40 43.40 58.20 53.60 0.00 38.80 39.00 1.60 0.00 0.00 53.40 56.80 57.80 26.20 51.80 54.40 0.00 0.00 44.74 40.59 37.92 47.60 27.31 58.92 30.70 16.80 30.76 13.97 43.21 0.00 42.80 48.38 46.65 57.19 56.09 50.47 54.04 57.20 54.36 30.74 42.49 40.39 56.25 53.89 15.78 40.14 39.34 51.86 25.36 14.82 42.50 40.89 40.25 31.38 59.53 22.87 18.67 52.50 44.62 28.34 0.00 0.00 34.65 37.59 42.07 22.53 29.43 37.92 17.03 0.00 47.84 36.31 42.55 50.66 86.40 88.40 22.20 0.00 49.00 0.00 85.20 0.00 84.40 88.40 89.60 88.40 85.60 87.40 85.20 87.00 71.67 30.74 60.45 60.73 86.80 87.59 0.00 81.00 0.60 86.40 27.00 0.00 83.40 84.20 92.20 84.80 88.60 0.00 0.00 65.40 81.80 67.20 0.00 0.00 82.80 83.20 85.40 44.60 83.00 83.80 0.00 0.00 61.99 55.02 58.30 80.20 22.73 41.41 19.86 0.30 17.25 0.36 23.67 0.18 19.50 14.00 41.10 53.92 58.60 50.45 40.77 55.85 39.57 30.74 25.76 25.05 35.21 30.10 0.30 24.36 6.73 24.87 6.55 8.01 19.01 19.89 21.41 7.98 24.95 23.69 8.98 38.67 27.36 24.27 0.00 0.00 27.98 31.87 37.98 17.43 18.33 22.22 3.60 0.00 32.61 25.49 30.48 On Path to Multimodal Generalist: General-Level and General-Bench"
        },
        {
            "title": "Model",
            "content": "BLIP-3 (XGen-MM) AnyGPT MiniCPM3-4B LaVIT-V2 (7B) GLM-VL-Chat Gemini-1.5-Pro Gemini-1.5-Flash OMG-LLaVA-InternLM20B Idefics3-8B-Llama3 Yi-Vision-v2 Qwen2-VL-72B Otter Show-o NExT-Chat InternVL2-26B Qwen2-VL-72B DeepSeek-VL-2-small DeepSeek-VL-2 LLaVA-One-Vision-7B LLaVA-One-Vision-72B Sa2VA-8B Sa2VA-26B CoLVA-2B CoLVA-4B Long-LLaVA LM4LV Vitron-V1 PandaGPT (13B) AnyGPT GAMA Pengi SALMONN-7B SALMONN-13B WavLLM ImageBind-LLM Unified-io-2-XXL ModaVerse-7b-v0 AudioGPT-GPT4 SpeechGPT-7B-com LLaMA-Omni 3D-LLM PointLLM-7B PointLLM-13B 3D-VisTA MotionGPT-T5 MotionGPT-LLaMA AvatarGPT LLaMA-Mesh #L-17 (Affect Computing) #1 #2 #3 #4 #5 #6 #7 #8 #9 68.68 22.88 62.79 50.13 60.20 89.20 85.60 28.20 83.40 82.00 90.00 0.00 0.00 0.00 80.60 90.80 49.00 77.00 60.60 49.40 79.20 86.60 72.80 56.00 53.80 0.00 21.34 0.00 20.28 0.00 0.00 0.80 0.00 0.00 0.00 45.90 0.00 89.60 0.00 13.60 17.40 0.00 0.00 0.00 0.00 0.00 0.00 0. 52.94 20.28 50.98 41.12 53.13 83.40 75.60 49.60 76.60 48.40 79.33 0.00 0.00 0.00 66.10 77.60 52.80 66.60 61.40 77.80 74.00 78.80 44.80 76.60 42.20 0.00 23.47 2.60 16.73 0.00 1.10 3.20 2.60 2.30 2.60 0.00 2.60 75.40 0.00 15.20 55.60 0.00 0.00 0.00 0.00 0.00 0.00 0.00 41.51 16.73 43.43 33.80 42.98 57.60 52.20 24.40 51.20 54.60 56.67 0.00 0.00 0.00 54.20 58.40 44.40 50.20 45.20 0.00 56.00 57.40 51.40 51.60 55.20 0.00 15.94 0.80 17.69 0.80 0.20 2.20 0.80 0.00 0.80 0.00 0.80 53.51 0.00 11.80 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 44.27 17.69 47.19 34.92 49.83 56.09 52.11 38.46 53.65 47.46 67.40 18.40 0.00 0.60 38.90 59.80 35.60 41.10 47.70 57.10 52.70 40.00 48.70 36.40 37.90 0.00 25.85 0.00 23.17 15.78 0.00 21.42 23.45 16.12 0.00 0.00 0.00 56.14 15.78 17.40 2.95 16.80 16.82 0.00 0.00 0.00 0.00 0.00 59.55 23.17 59.28 51.52 57.31 88.20 84.20 22.60 82.00 87.40 82.00 0.00 0.00 0.00 76.40 85.20 88.60 88.20 88.20 19.80 82.00 86.20 82.40 83.60 66.20 0.00 0.32 0.00 10.67 0.00 0.00 3.00 3.40 0.00 0.00 0.00 0.00 86.49 0.00 20.66 52.80 0.00 0.00 0.00 0.00 7.66 0.27 15.97 30.82 10.67 31.01 23.71 27.52 51.21 38.73 13.01 39.89 30.27 55.80 0.80 0.00 0.00 44.00 57.10 18.60 31.70 24.40 38.90 34.50 14.00 14.90 25.70 16.30 0.00 0.30 0.10 14.45 0.30 0.10 12.13 13.04 0.30 0.10 0.00 0.10 54.37 0.30 20.20 16.49 0.30 0.30 0.00 0.00 0.00 0.00 0.00 54.88 19.32 56.27 48.87 56.44 59.00 53.40 22.40 53.60 50.00 59.33 0.00 0.00 0.00 58.60 55.40 51.20 54.40 49.60 33.20 57.80 52.40 50.80 50.60 56.80 0.00 0.00 5.70 0.00 3.70 0.00 10.00 12.40 3.60 5.60 5.40 5.70 65.90 0.00 43.50 0.40 0.00 0.00 0.00 0.00 0.00 0.00 0. 50.33 0.00 45.12 34.25 45.58 78.40 74.60 20.60 68.60 64.40 78.66 0.00 0.00 0.00 61.80 77.60 49.00 49.80 49.20 22.40 65.80 77.40 56.00 56.00 49.40 0.00 24.57 0.40 20.85 0.40 0.10 3.80 0.40 0.00 0.40 64.10 0.40 69.50 0.00 35.70 0.20 0.00 0.00 0.00 0.00 0.00 0.00 0.00 51.44 20.85 53.29 41.14 55.63 92.40 88.20 26.40 89.00 89.00 96.00 0.00 0.00 0.00 90.20 94.40 91.00 88.80 86.60 91.20 92.00 93.60 87.00 90.00 88.20 0.00 0.00 0.00 22.88 0.00 0.00 1.20 0.00 0.00 0.00 56.30 0.00 93.60 0.00 19.70 84.20 0.00 0.00 0.00 0.00 0.00 0.00 0.00 291 On Path to Multimodal Generalist: General-Level and General-Bench Table 109: Results on NLP Group, #L-17, part B."
        },
        {
            "title": "Model",
            "content": "#L-17 (Affect Computing)"
        },
        {
            "title": "SoTA Specialist",
            "content": "78.40 62.22 77.51 90.32 91.55 92. 36.80 95.57 80.20 #10 #11 #12 #13 #14 #15 #16 #17 #18 Meta-Llama-3.1-8B-Instruct Qwen2.5-7B-Instruct Gemma-2-9b-it ChatGLM-6b Vicuna-7b-v1.5 InternLM-Chat-7b GPT-J-6B Falcon3-7B-Instruct Baichuan2-7B-Base Ministral-8B-Instruct-2410 Yi-Ligntning GPT-3.5-turbo GPT-4v GPT-4o GPT4o-mini GPT-4o-4096 ChatGPT-4o-latest Claude-3.5-Sonnet Claude-3.5-Opus Emu2-32B DetGPT InternVL2.5-8B InternVL2.5-4B NExT-GPT-V1.5 InternVL2.5-2B Monkey-10B-chat DeepSeek-VL-7B Qwen2-VL-7B Qwen-VL-Chat Qwen-Audio-Chat Qwen2-Audio-Instruct MoE-LLAVA-Phi2-2.7B-4e-384 mPLUG-Owl2-LLaMA2-7b Phi-3.5-Vision-Instruct Cambrian-1-8B MiniGPT4-LLaMA2 InternVL-Chat-V1-5 Mini-InternVL-Chat-4B-V1-5 InternLM-XComposer2-VL-1.8B GPT4RoI GLaMM LLaVA-NeXT-13B LLaVA-NeXT-34B Pixtral-12B SEED-LLaMA-13B BLIP2 MiniMonkey DeepSeek-VL-7B LISA-7B CogVLM-Chat ShareGPT4V-7B ShareGPT4V-13B 18.33 59.40 69.80 7.00 0.00 2.20 0.00 24.60 0.00 14.00 56.60 47.60 67.80 73.40 64.20 72.80 71.00 44.31 30.74 35.85 31.89 58.20 48.40 0.00 37.40 0.00 40.80 12.00 0.00 4.70 4.80 7.00 37.20 57.00 6.20 0.00 25.40 41.80 12.80 0.00 0.00 40.20 52.60 51.40 32.20 31.60 37.00 0.20 0.00 36.30 29.86 32.91 39.00 46.59 50.29 0.27 0.08 0.88 0.06 31.51 0.00 12.41 6.69 44.48 42.53 51.79 52.58 52.57 51.79 30.21 30.74 19.48 14.01 34.88 38.29 8.08 15.14 0.24 3.95 0.00 0.19 0.45 0.58 0.38 1.82 22.39 13.05 4.27 27.88 7.72 0.00 0.00 0.00 16.07 22.65 32.78 4.51 40.94 0.25 0.18 0.00 25.60 15.82 23.36 35.30 49.22 54.08 0.27 0.00 0.51 0.00 31.91 0.00 17.72 15.54 40.65 50.08 58.21 57.97 58.51 58.88 33.02 30.74 25.50 18.71 43.77 35.97 5.09 12.96 0.00 5.12 0.00 0.24 0.56 0.68 1.15 0.25 20.94 4.13 5.38 32.06 16.57 0.00 0.00 0.00 22.60 24.35 17.4 2.17 24.82 0.88 0.00 0.00 27.64 19.33 19.63 292 31.53 0.00 0.00 0.00 0.09 0.00 0.07 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 24.35 30.74 11.72 9.20 86.80 0.00 16.42 0.00 0.60 86.40 0.00 0.06 0.00 0.00 0.00 0.00 0.00 0.00 0.06 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 12.07 11.63 11. 0.00 44.98 50.73 0.14 0.00 0.69 0.03 30.03 0.00 27.96 43.75 51.93 68.14 63.77 61.64 69.79 66.71 44.93 30.74 40.02 33.16 50.65 33.47 0.00 10.30 0.00 36.00 26.00 0.00 3.56 4.16 39.60 76.40 89.20 0.20 0.00 36.49 29.42 0.00 10.77 0.00 17.83 14.75 26.12 1.78 29.08 0.21 1.80 0.00 40.89 33.60 40.88 29.25 0.72 4.51 0.00 0.02 0.00 0.02 0.00 0.00 0.12 0.25 0.46 0.90 5.70 2.32 6.23 3.71 26.17 30.74 14.10 9.58 86.80 1.64 3.02 0.20 0.00 86.40 0.00 0.02 0.00 0.00 0.00 0.00 0.08 0.14 0.02 0.00 0.00 0.00 0.00 0.00 0.00 0.12 0.06 0.00 0.00 0.00 0.00 0.00 13.97 13.18 13.60 1.10 4.67 4.07 0.00 0.00 0.00 0.00 3.02 0.00 0.22 5.96 5.08 12.11 11.35 9.14 10.55 10.62 10.88 30.74 5.42 2.75 1.12 0.73 14.37 0.00 0.00 0.15 0.00 0.00 0.34 0.26 0.00 0.00 0.89 0.91 0.14 0.00 0.00 0.00 0.00 0.00 0.51 0.77 1.16 0.00 0.88 0.00 0.00 0.00 7.55 5.58 6.07 0.65 75.60 64.40 68.60 0.00 63.20 0.00 68.20 0.00 67.40 1.02 70.00 73.00 77.40 72.60 74.00 74.90 50.31 30.74 39.99 34.90 0.24 0.42 34.70 0.11 0.00 0.15 27.00 0.00 60.13 60.60 92.20 84.80 88.60 0.00 0.00 0.18 0.00 0.00 0.00 0.00 68.40 72.40 70.80 30.40 67.40 66.40 0.00 0.00 35.65 32.50 35.32 2.12 1.59 1.89 0.00 0.00 0.00 0.00 0.46 0.00 0.11 67.20 1.29 2.35 3.56 1.40 3.51 2.21 35.37 30.74 23.62 18.84 71.00 72.60 0.00 72.40 1.80 66.60 0.00 0.00 0.01 0.09 0.00 0.00 0.20 0.18 0.00 69.00 61.00 60.20 0.00 0.00 0.43 0.67 0.64 0.00 0.57 0.00 0.20 0.00 25.95 16.51 20.79 On Path to Multimodal Generalist: General-Level and General-Bench"
        },
        {
            "title": "Model",
            "content": "BLIP-3 (XGen-MM) AnyGPT MiniCPM3-4B LaVIT-V2 (7B) GLM-VL-Chat Gemini-1.5-Pro Gemini-1.5-Flash OMG-LLaVA-InternLM20B Idefics3-8B-Llama3 Yi-Vision-v2 Qwen2-VL-72B Otter Show-o NExT-Chat InternVL2-26B Qwen2-VL-72B DeepSeek-VL-2-small DeepSeek-VL-2 LLaVA-One-Vision-7B LLaVA-One-Vision-72B Sa2VA-8B Sa2VA-26B CoLVA-2B CoLVA-4B Long-LLaVA LM4LV Vitron-V1 PandaGPT (13B) AnyGPT GAMA Pengi SALMONN-7B SALMONN-13B WavLLM ImageBind-LLM Unified-io-2-XXL ModaVerse-7b-v0 AudioGPT-GPT4 SpeechGPT-7B-com LLaMA-Omni 3D-LLM PointLLM-7B PointLLM-13B 3D-VisTA MotionGPT-T5 MotionGPT-LLaMA AvatarGPT LLaMA-Mesh #L-17 (Affect Computing) #10 #11 #12 #13 #14 #15 #16 #17 #18 14.77 4.93 22.07 13.19 20.48 2.02 1.24 0.00 2.28 0.00 0.00 0.00 0.00 0.00 0.00 0.10 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 13.57 0.09 15.69 0.09 0.05 0.00 0.09 0.06 0.09 0.00 0.09 0.00 0.09 1.56 0.00 0.10 0.10 0.00 0.00 0.00 0.00 0. 38.76 15.69 37.25 31.68 37.62 51.96 43.26 1.10 33.57 37.49 78.03 0.00 0.00 0.00 38.90 74.10 27.80 26.60 16.60 58.60 51.20 36.50 21.20 34.50 1.00 0.00 0.00 0.00 6.82 0.00 0.00 1.17 1.19 0.00 0.00 4.67 0.00 67.96 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.07 17.31 6.82 15.44 13.61 12.36 4.78 2.39 0.00 2.54 0.24 2.49 0.00 0.00 0.00 0.00 0.30 0.10 0.00 0.30 2.50 0.70 0.20 0.00 1.70 0.00 0.00 3.01 0.01 3.98 0.02 0.01 0.00 0.01 0.02 0.01 0.00 0.01 0.80 0.02 29.25 0.00 0.03 0.03 0.00 0.00 0.00 0.00 2.21 5.44 3.98 10.46 8.48 9.68 6.17 4.08 0.00 2.25 3.18 10.18 0.00 0.00 0.00 0.00 0.90 1.10 0.00 0.90 11.30 1.00 5.40 0.00 1.30 0.00 0.00 4.79 0.00 15.07 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 12.01 0.00 1.10 0.00 0.00 0.00 0.00 0.00 0.05 0.00 0.05 37.51 15.07 31.81 20.76 28.40 75.20 69.60 20.20 63.40 71.43 75.81 0.00 0.00 33.67 63.40 76.60 67.00 69.60 65.80 60.20 69.80 73.60 62.00 75.60 54.00 0.00 35.62 0.00 10.04 34.70 23.50 5.60 5.90 33.78 0.00 0.00 0.00 72.00 34.70 0.33 65.60 0.00 0.00 0.00 0.00 0.00 0.00 0.05 19.70 10.04 28.96 28.42 28.79 0.60 0.40 0.00 0.00 0.47 2.59 0.00 0.00 0.00 0.00 0.30 0.10 0.40 0.20 1.50 0.70 1.30 0.00 0.30 0.00 0.00 0.00 0.00 7.82 0.00 0.00 3.40 0.00 0.00 0.00 0.00 0.00 2.31 0.00 2.02 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 39.56 14.45 39.65 27.53 39.69 67.20 58.40 9.60 42.20 38.60 69.34 0.00 0.00 0.00 48.80 73.20 41.60 34.20 57.00 0.20 60.60 60.80 23.80 22.00 42.60 0.00 0.00 0.00 8.84 0.00 0.00 1.20 1.40 0.00 0.00 0.00 0.00 66.40 0.00 16.40 5.80 0.00 0.00 0.00 0.00 0.10 0.10 6. 20.57 8.84 26.91 20.62 26.90 45.61 35.49 1.42 28.71 32.69 55.78 0.00 0.00 0.00 34.10 55.30 11.50 26.10 14.50 53.90 27.20 37.40 0.50 28.90 1.50 0.00 8.12 0.04 8.39 0.08 0.04 0.00 0.04 0.07 0.04 0.00 0.04 41.05 0.08 18.50 0.00 0.09 0.09 0.00 0.00 0.00 0.00 0.00 19.83 8.39 28.14 20.58 26.00 47.57 38.14 0.88 28.65 32.36 61.37 0.00 0.00 0.00 35.40 60.60 13.30 32.00 10.70 50.40 33.10 36.00 2.90 29.50 0.40 0.00 7.24 0.00 4.93 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 51.08 0.00 5.40 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.62 293 On Path to Multimodal Generalist: General-Level and General-Bench Table 111: Results on NLP Group, #L-18. #L-18 (NER)"
        },
        {
            "title": "SoTA Specialist",
            "content": "83.51 74.45 67.00 76.02 92.00 88. 94.21 97.65 96.74 81.97 78.21 89. 82.32 #1 #2 #3 #4 #5 #6 #7 #8 #9 #10 #11 #12 #13 Meta-Llama-3.1-8B-Instruct Qwen2.5-7B-Instruct Gemma-2-9b-it ChatGLM-6b Vicuna-7b-v1.5 InternLM-Chat-7b GPT-J-6B Falcon3-7B-Instruct Baichuan2-7B-Base Ministral-8B-Instruct-2410 Yi-Ligntning GPT-3.5-turbo GPT-4v GPT-4o GPT4o-mini GPT-4o-4096 ChatGPT-4o-latest Claude-3.5-Sonnet Claude-3.5-Opus Emu2-32B DetGPT InternVL2.5-8B InternVL2.5-4B NExT-GPT-V1.5 InternVL2.5-2B Monkey-10B-chat DeepSeek-VL-7B Qwen2-VL-7B Qwen-VL-Chat Qwen-Audio-Chat Qwen2-Audio-Instruct MoE-LLAVA-Phi2-2.7B-4e-384 mPLUG-Owl2-LLaMA2-7b Phi-3.5-Vision-Instruct Cambrian-1-8B MiniGPT4-LLaMA2 InternVL-Chat-V1-5 Mini-InternVL-Chat-4B-V1-5 InternLM-XComposer2-VL-1.8B GPT4RoI GLaMM LLaVA-NeXT-13B LLaVA-NeXT-34B Pixtral-12B SEED-LLaMA-13B BLIP2 MiniMonkey DeepSeek-VL-7B LISA-7B CogVLM-Chat ShareGPT4V-7B ShareGPT4V-13B 6.81 40.07 43.03 0.05 0.04 1.65 0.04 30.52 0.00 6.46 19.60 40.48 47.34 60.84 53.21 60.07 55.99 28.75 30.74 16.88 10.75 7.37 8.71 9.04 0.65 0.00 0.25 0.00 1.00 1.58 1.59 1.13 0.08 5.41 0.30 8.34 6.38 3.11 0.00 0.00 0.00 5.01 7.45 8.23 1.74 1.42 0.32 0.00 0.00 17.28 10.53 17.46 17.76 26.85 32.32 0.13 0.00 0.12 0.00 15.97 0.00 6.20 38.11 41.50 57.67 48.43 39.40 51.72 46.35 21.09 30.74 14.72 15.19 30.33 28.29 0.00 1.36 0.00 0.81 0.58 0.27 4.47 4.41 1.42 0.10 3.86 4.95 1.54 29.98 16.73 0.00 0.00 0.00 5.43 7.92 6.65 0.65 3.49 0.79 0.05 0.00 21.47 14.50 17.27 15.53 25.67 26.62 0.03 0.02 0.47 0.01 14.21 0.00 0.52 24.46 9.12 29.84 36.26 33.52 34.95 35.81 20.31 30.74 14.19 9.71 17.51 18.36 0.02 2.09 0.00 1.86 0.00 0.11 1.03 1.22 1.70 0.04 0.41 0.55 2.07 15.53 5.84 0.00 0.00 0.00 2.28 5.13 4.74 0.66 0.45 0.21 0.09 0.00 9.66 5.39 9. 5.86 27.54 38.48 0.00 0.00 1.20 0.00 22.00 0.00 4.57 48.11 21.80 50.56 50.89 42.35 50.73 50.06 29.93 30.74 21.02 12.47 33.97 26.35 0.00 9.17 0.00 5.42 0.00 0.22 2.65 2.92 1.96 1.72 3.73 2.45 6.74 31.03 20.87 0.00 0.00 0.00 3.34 4.98 13.72 2.85 16.51 2.78 0.12 0.00 19.91 14.02 17.92 14.09 26.97 33.81 0.01 0.00 1.37 0.00 7.11 0.00 5.11 25.09 26.01 70.18 74.90 40.03 72.61 72.10 24.02 30.74 17.85 15.07 7.54 17.75 0.00 1.21 0.00 1.05 0.00 0.47 0.45 0.62 0.80 0.20 0.90 1.30 12.56 8.93 4.56 0.00 0.00 0.00 0.68 1.75 0.69 0.10 0.00 0.00 0.05 0.00 20.38 14.62 15.32 294 18.77 36.38 39.95 0.00 0.56 0.72 0.52 13.81 0.00 1.75 25.29 28.93 49.48 50.71 48.17 47.95 47.39 25.56 30.74 16.18 10.30 15.29 13.19 0.07 1.13 0.00 0.65 0.42 0.70 2.56 3.35 2.00 0.56 3.31 2.56 3.49 13.62 10.52 0.00 0.00 0.00 2.59 3.68 4.74 0.57 2.17 0.38 0.07 0.00 14.37 12.19 14.32 25.43 56.83 73.65 0.18 0.00 0.19 0.00 30.45 0.00 2.11 25.00 0.91 61.78 82.38 78.55 81.52 76.72 30.27 30.74 24.75 15.66 9.72 6.16 5.85 2.03 0.00 1.34 0.00 1.78 10.32 11.62 12.35 2.38 23.37 0.00 7.42 4.20 5.20 0.00 0.00 0.00 22.10 26.94 25.01 4.82 4.56 5.74 0.00 0.00 25.75 14.38 17.51 30.05 79.95 82.90 0.00 0.17 13.63 0.00 78.19 0.00 29.85 31.84 59.32 96.06 94.89 95.78 96.30 95.28 41.19 30.74 32.26 29.04 21.90 23.17 0.17 2.91 0.00 2.07 1.08 2.19 7.60 8.10 25.27 11.95 61.43 2.44 15.11 22.11 15.31 0.00 0.00 0.00 52.72 61.26 63.06 16.58 35.70 27.81 0.00 0.00 38.26 31.20 31. 51.57 45.58 78.18 0.00 0.00 0.00 0.00 48.84 0.00 3.11 44.80 2.34 87.41 85.62 93.14 77.38 56.99 36.19 30.74 28.44 23.02 33.78 17.07 8.53 1.27 0.00 4.07 0.00 0.09 16.78 18.72 10.40 0.87 29.48 0.00 7.58 14.35 5.30 0.00 0.00 0.00 31.21 32.22 34.76 4.76 19.64 25.35 0.00 0.00 26.80 21.91 23.35 72.00 17.98 18.78 0.00 0.00 0.52 0.00 11.30 0.00 3.36 37.15 14.68 23.94 29.12 27.61 28.60 28.39 19.92 30.74 13.39 6.23 13.19 14.97 12.46 0.52 0.00 1.51 0.00 0.09 0.13 0.27 0.23 0.32 1.73 0.00 3.27 7.80 9.81 0.00 0.00 0.00 0.22 0.74 0.82 0.03 0.00 0.21 0.27 0.00 13.30 4.94 11.16 12.70 40.24 43.01 0.39 0.00 12.40 0.00 41.06 0.00 8.65 1.81 39.67 50.49 49.34 42.31 49.36 49.87 23.67 30.74 18.51 16.82 13.52 58.62 8.45 0.47 0.00 2.32 0.35 0.83 12.45 13.19 2.87 2.54 21.49 3.53 8.67 12.32 32.76 0.00 0.00 0.00 3.88 8.85 14.16 0.42 7.83 3.46 1.14 0.00 22.25 15.69 20.00 31.22 23.05 28.66 0.57 0.03 1.31 0.03 22.58 0.00 4.90 42.17 24.06 41.81 46.08 37.98 45.90 45.55 34.53 30.74 23.37 18.50 73.71 78.65 3.03 9.23 0.20 30.05 0.00 0.08 2.89 2.97 0.36 0.17 6.07 1.27 3.44 70.80 61.38 0.00 0.00 0.00 1.34 2.34 5.72 0.18 0.18 0.22 0.61 0.00 23.08 18.11 20.46 16.62 18.58 34.53 0.15 0.13 0.78 0.13 23.29 0.00 5.08 4.14 27.84 37.24 37.00 35.95 36.41 33.16 26.76 30.74 16.32 13.25 9.71 55.51 10.13 1.74 0.00 2.19 0.65 0.81 1.45 2.21 1.83 0.27 5.61 0.32 2.45 14.85 26.63 0.00 0.00 0.00 3.08 2.42 4.76 0.25 0.31 0.32 0.44 0.00 20.29 8.88 16.81 On Path to Multimodal Generalist: General-Level and General-Bench #L-18 (NER) #1 #2 #3 #4 #5 #6 #7 #8 #9 #10 #11 #12 #13 18.38 7.82 17.83 11.16 16.49 48.53 41.72 4.55 27.46 25.61 50.59 0.00 0.00 0.00 29.30 47.00 5.50 20.90 3.10 44.40 29.50 32.20 3.00 29.50 0.10 0.00 10.03 0.04 8.57 0.04 0.04 0.00 0.04 0.03 0.04 0.00 0.04 46.90 0.04 6.23 0.00 0.04 0.04 0.00 0.00 0.00 0.00 0.00 15.56 8.57 15.66 14.05 17.01 50.66 42.52 7.28 32.75 19.34 56.57 0.00 0.00 0.00 19.90 52.20 12.40 14.60 3.80 32.70 13.50 21.40 1.20 6.80 0.20 0.00 0.00 0.00 5.26 0.00 0.00 0.14 0.18 0.00 0.00 0.00 0.00 56.18 0.00 14.67 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.05 13.15 5.26 14.07 9.28 11.54 29.19 27.37 2.20 21.06 5.84 23.52 0.00 0.00 0.00 0.60 26.20 0.10 1.70 2.70 11.90 8.00 11.30 0.40 2.70 0.10 0.00 0.02 0.02 0.00 0.02 0.02 0.00 0.02 0.02 0.02 0.00 0.02 29.14 0.02 15.40 0.00 0.02 0.02 0.00 0.00 0.00 0.00 0.19 16.92 0.00 19.69 11.98 17.76 43.31 37.98 4.43 28.65 22.92 40.31 0.00 0.00 0.00 25.20 43.10 4.40 15.50 11.90 35.00 22.70 25.20 0.70 14.40 1.20 0.00 0.00 0.00 6.02 0.00 0.00 0.11 0.00 0.00 0.00 0.00 0.00 49.32 0.00 5.86 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0. 16.46 6.02 16.49 9.97 13.38 49.74 40.29 2.26 26.51 21.08 48.31 0.00 0.00 0.00 20.70 54.20 3.50 10.30 5.40 62.00 38.60 6.70 2.80 18.50 0.10 0.00 0.00 0.00 5.71 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 78.34 0.00 12.40 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.24 13.65 5.71 12.26 11.13 10.06 46.51 42.16 3.80 25.56 7.48 40.65 0.18 0.00 0.00 15.00 45.10 1.20 3.80 7.30 11.10 16.90 22.90 0.80 12.70 0.60 0.00 0.07 0.56 10.10 0.58 0.13 0.21 0.56 0.07 0.56 0.00 0.56 45.90 0.07 14.30 0.11 0.56 0.56 0.00 0.00 0.00 0.00 0.57 18.39 10.10 26.47 11.15 23.00 67.45 60.03 6.82 45.38 38.30 85.92 0.00 0.00 0.00 0.90 84.90 15.70 13.70 0.80 71.30 24.50 60.00 5.20 65.60 1.10 0.00 9.08 0.00 10.93 5.93 0.00 1.67 1.89 5.69 0.00 0.00 0.00 61.78 5.85 23.50 0.00 0.00 0.00 0.00 0.00 0.16 0.00 0.94 33.99 10.93 29.23 23.96 28.73 90.12 82.95 11.64 73.20 76.28 93.81 0.00 0.00 0.00 73.30 94.20 55.70 80.60 2.70 91.00 73.50 80.50 17.60 77.50 26.20 0.00 0.17 0.17 10.35 0.17 0.15 4.74 5.17 0.23 0.17 0.00 0.17 95.67 0.17 28.95 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.62 29.00 10.35 26.81 15.05 24.81 82.53 79.40 13.71 66.23 58.70 87.47 0.00 0.00 0.00 21.70 84.90 12.90 15.30 0.10 81.10 0.00 4.40 0.00 58.80 0.10 0.00 10.30 0.00 6.15 8.05 0.00 0.00 0.00 8.23 0.00 0.00 0.00 85.41 8.53 13.45 0.00 0.00 0.00 0.00 0.00 0.00 0.00 2.49 9.63 6.15 10.83 9.25 8.91 19.62 16.23 0.00 14.18 13.61 18.33 0.00 0.00 0.00 0.70 19.00 3.30 3.70 1.00 19.20 7.70 10.80 0.20 4.40 0.00 0.00 10.35 0.00 9.54 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 20.56 0.00 0.00 0.16 0.00 0.00 0.00 0.00 0.00 0.00 0. 18.34 9.54 19.29 18.54 20.49 47.10 43.91 3.47 19.59 35.33 52.48 0.00 0.00 0.00 34.50 49.20 16.10 21.80 11.60 32.60 45.70 42.20 2.20 23.90 0.40 0.00 16.74 0.00 9.06 0.00 0.00 0.86 0.00 0.00 0.00 0.00 0.00 23.45 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.04 0.00 0.06 22.41 9.06 31.37 26.57 31.68 42.75 38.49 2.05 25.97 17.81 43.63 0.00 0.00 0.00 13.10 40.00 2.60 3.70 1.40 37.10 4.80 15.30 0.50 11.30 0.00 0.00 5.03 0.03 7.64 0.03 0.01 0.00 0.03 0.03 0.03 0.00 0.03 40.81 0.03 5.22 0.00 0.03 0.03 0.00 0.00 0.00 0.00 0.68 12.03 7.64 17.12 17.22 16.10 36.80 32.13 2.33 26.54 23.02 35.58 0.00 0.00 0.00 17.30 33.20 3.40 1.70 2.70 28.80 6.90 18.30 0.10 4.60 0.20 0.00 8.19 0.13 10.00 0.13 0.08 0.31 0.13 0.12 0.13 0.00 0.13 35.23 0.13 4.34 0.24 0.16 0.16 0.00 0.00 0.00 0.00 0."
        },
        {
            "title": "Model",
            "content": "BLIP-3 (XGen-MM) AnyGPT MiniCPM3-4B LaVIT-V2 (7B) GLM-VL-Chat Gemini-1.5-Pro Gemini-1.5-Flash OMG-LLaVA-InternLM20B Idefics3-8B-Llama3 Yi-Vision-v2 Qwen2-VL-72B Otter Show-o NExT-Chat InternVL2-26B Qwen2-VL-72B DeepSeek-VL-2-small DeepSeek-VL-2 LLaVA-One-Vision-7B LLaVA-One-Vision-72B Sa2VA-8B Sa2VA-26B CoLVA-2B CoLVA-4B Long-LLaVA LM4LV Vitron-V1 PandaGPT (13B) AnyGPT GAMA Pengi SALMONN-7B SALMONN-13B WavLLM ImageBind-LLM Unified-io-2-XXL ModaVerse-7b-v0 AudioGPT-GPT4 SpeechGPT-7B-com LLaMA-Omni 3D-LLM PointLLM-7B PointLLM-13B 3D-VisTA MotionGPT-T5 MotionGPT-LLaMA AvatarGPT LLaMA-Mesh 295 On Path to Multimodal Generalist: General-Level and General-Bench Table 113: Results on NLP Group, #L-19. #L-19 (Cog QA)"
        },
        {
            "title": "SoTA Specialist",
            "content": "91.87 95.27 83.64 75.57 91.61 65. 89.20 72.40 88.20 #1 #2 #3 #4 #5 #6 #7 #8 #9 Meta-Llama-3.1-8B-Instruct Qwen2.5-7B-Instruct Gemma-2-9b-it ChatGLM-6b Vicuna-7b-v1.5 InternLM-Chat-7b GPT-J-6B Falcon3-7B-Instruct Baichuan2-7B-Base Ministral-8B-Instruct-2410 Yi-Ligntning GPT-3.5-turbo GPT-4v GPT-4o GPT4o-mini GPT-4o-4096 ChatGPT-4o-latest Claude-3.5-Sonnet Claude-3.5-Opus Emu2-32B DetGPT InternVL2.5-8B InternVL2.5-4B NExT-GPT-V1.5 InternVL2.5-2B Monkey-10B-chat DeepSeek-VL-7B Qwen2-VL-7B Qwen-VL-Chat Qwen-Audio-Chat Qwen2-Audio-Instruct MoE-LLAVA-Phi2-2.7B-4e-384 mPLUG-Owl2-LLaMA2-7b Phi-3.5-Vision-Instruct Cambrian-1-8B MiniGPT4-LLaMA2 InternVL-Chat-V1-5 Mini-InternVL-Chat-4B-V1-5 InternLM-XComposer2-VL-1.8B GPT4RoI GLaMM LLaVA-NeXT-13B LLaVA-NeXT-34B Pixtral-12B SEED-LLaMA-13B BLIP2 MiniMonkey DeepSeek-VL-7B LISA-7B CogVLM-Chat ShareGPT4V-7B ShareGPT4V-13B 43.70 59.80 64.40 6.40 0.00 8.20 0.00 51.00 0.00 47.60 76.80 52.00 70.00 83.20 69.80 79.60 80.20 41.60 30.74 30.80 23.02 49.40 40.00 19.05 19.60 3.00 39.00 0.00 1.00 13.50 15.20 1.13 0.08 5.41 0.30 8.34 53.20 27.00 5.00 0.00 0.00 17.77 21.08 23.71 1.03 23.31 16.20 0.00 0.00 26.85 26.63 26.69 38.80 72.20 76.20 46.00 0.00 27.00 0.00 70.20 0.00 62.60 77.60 81.80 86.20 85.90 83.00 86.20 86.80 57.67 30.74 44.61 41.98 76.20 67.00 4.32 36.00 0.80 64.60 1.00 0.00 49.78 50.60 53.60 20.60 75.20 5.80 0.00 66.80 50.80 17.60 0.00 0.00 49.94 63.60 61.74 14.22 41.16 36.60 0.20 0.00 50.96 43.07 42.06 70.40 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 6.36 30.74 0.00 0.00 0.00 0.00 8.59 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 2.00 0.00 0.00 0.00 0.00 296 0.00 29.29 39.48 3.52 4.26 4.94 3.86 40.25 0.08 14.00 45.33 29.30 67.28 63.34 35.21 60.19 59.31 24.33 30.74 20.31 16.31 27.01 19.77 5.43 11.90 3.49 5.27 4.20 4.60 5.34 5.52 3.73 2.06 4.17 2.28 3.96 32.30 12.96 0.00 0.00 0.00 17.63 33.85 32.45 7.55 8.77 9.63 2.05 0.00 19.45 15.45 17. 21.74 83.00 68.60 0.20 0.00 21.60 0.00 74.80 0.00 63.40 72.40 80.00 77.40 76.80 75.79 75.80 73.80 66.38 30.74 56.97 50.67 77.20 72.00 2.30 71.00 11.20 73.20 0.00 0.00 0.80 1.00 64.00 31.40 68.80 83.40 0.00 69.80 66.60 46.80 0.00 0.00 63.20 67.40 72.20 34.60 57.60 62.20 14.20 0.00 53.17 49.86 53.13 7.65 10.80 6.95 0.00 0.00 1.19 0.00 6.97 0.00 5.85 6.19 8.18 9.78 8.69 9.44 9.87 8.97 11.04 30.74 5.94 2.95 6.93 4.68 9.44 1.31 0.00 4.02 1.18 0.12 5.34 5.27 6.09 0.16 6.13 3.67 0.78 4.64 5.46 0.00 0.00 0.00 5.36 6.47 7.12 0.76 2.04 0.18 0.00 0.00 6.88 1.22 3.64 42.00 28.20 43.20 1.20 0.00 23.00 0.00 6.80 0.00 40.00 44.80 43.00 45.80 45.80 43.20 46.20 44.60 34.29 30.74 22.64 15.86 38.00 30.20 6.32 3.00 1.20 23.00 11.83 13.08 6.30 6.20 4.47 8.20 9.31 0.00 4.86 19.00 33.60 85.20 0.00 0.00 18.80 17.40 20.20 7.60 19.80 16.40 0.00 0.00 20.10 17.29 19.02 46.80 48.40 48.20 16.80 0.00 20.80 0.00 47.60 0.00 42.80 49.40 48.40 47.80 48.80 48.80 48.20 48.40 39.44 30.74 34.82 32.02 48.00 46.80 0.00 43.40 7.40 46.80 25.60 26.91 27.50 30.00 16.47 21.21 22.83 16.70 19.31 42.20 48.00 27.60 0.00 0.00 37.40 47.60 48.80 26.20 46.20 48.60 0.00 0.00 36.22 31.63 33.37 5.20 8.60 15.60 1.60 0.00 5.00 0.00 10.40 0.00 6.20 13.00 11.20 4.80 10.20 15.60 5.00 8.00 11.05 30.74 0.00 0.00 5.60 7.60 0.00 4.40 0.00 0.21 0.00 0.08 3.20 3.80 0.36 0.17 6.07 1.27 3.44 4.00 5.20 2.80 0.00 0.00 4.40 5.00 5.40 0.80 1.00 0.60 4.00 0.00 4.88 1.56 2.05 On Path to Multimodal Generalist: General-Level and General-Bench #L-19 (Cog QA) #4 #5 #6 #7 #8 #9 17.94 7.06 17.45 10.97 15.81 36.45 32.93 3.10 22.93 12.43 61.37 0.00 0.00 0.00 31.70 38.60 7.80 1.70 24.10 62.70 13.30 46.80 3.30 27.10 18.20 0.00 5.43 2.76 18.88 4.23 2.16 0.25 0.76 4.57 2.76 0.00 2.76 56.80 4.65 0.00 7.90 4.26 4.26 0.00 0.00 0.00 0.00 0.00 54.09 18.88 59.27 41.93 59.68 75.24 70.45 28.71 69.20 72.20 77.33 0.00 0.00 0.00 70.80 75.40 67.20 72.00 74.60 0.00 80.80 75.00 42.00 75.60 80.00 0.00 3.40 0.00 3.42 0.00 0.00 0.80 1.10 0.00 0.00 0.00 0.00 73.20 0.00 20.56 0.40 0.00 0.00 0.00 0.00 1.19 0.00 4.68 4.65 3.42 13.48 5.39 13.30 7.30 6.96 0.00 5.24 6.61 18.97 0.00 0.00 0.00 0.70 0.90 5.40 6.40 4.20 7.60 9.40 6.40 1.20 8.80 5.70 0.00 9.54 0.00 8.66 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 9.45 0.00 7.85 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 20.88 8.66 24.74 14.56 23.83 44.20 40.80 7.60 31.00 42.80 16.00 0.00 0.00 0.00 41.40 43.60 10.40 44.40 8.60 44.00 42.80 46.00 2.40 17.00 2.80 0.00 3.63 0.00 11.78 0.00 0.00 4.80 5.00 0.00 0.00 0.00 0.00 42.10 0.00 1.80 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.10 34.96 11.78 36.41 26.45 37.89 46.80 39.40 9.40 40.20 27.25 58.67 0.00 0.00 0.00 45.20 48.20 45.60 48.60 45.60 0.20 47.40 45.80 22.80 47.20 22.00 0.00 0.00 0.00 0.00 0.00 0.00 3.40 3.60 0.00 0.00 0.00 0.00 45.35 0.00 4.60 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0. 5.53 0.00 15.05 9.32 15.67 6.00 7.00 0.00 4.80 8.40 11.33 0.00 0.00 0.00 0.60 0.80 11.40 1.40 7.80 7.20 1.60 9.80 3.00 10.60 4.00 0.00 0.00 0.00 4.43 0.00 0.00 8.20 8.50 0.00 0.00 0.00 0.00 4.30 0.00 4.60 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0."
        },
        {
            "title": "Model",
            "content": "#1 #2 BLIP-3 (XGen-MM) AnyGPT MiniCPM3-4B LaVIT-V2 (7B) GLM-VL-Chat Gemini-1.5-Pro Gemini-1.5-Flash OMG-LLaVA-InternLM20B Idefics3-8B-Llama3 Yi-Vision-v2 Qwen2-VL-72B Otter Show-o NExT-Chat InternVL2-26B Qwen2-VL-72B DeepSeek-VL-2-small DeepSeek-VL-2 LLaVA-One-Vision-7B LLaVA-One-Vision-72B Sa2VA-8B Sa2VA-26B CoLVA-2B CoLVA-4B Long-LLaVA LM4LV Vitron-V1 PandaGPT (13B) AnyGPT GAMA Pengi SALMONN-7B SALMONN-13B WavLLM ImageBind-LLM Unified-io-2-XXL ModaVerse-7b-v0 AudioGPT-GPT4 SpeechGPT-7B-com LLaMA-Omni 3D-LLM PointLLM-7B PointLLM-13B 3D-VisTA MotionGPT-T5 MotionGPT-LLaMA AvatarGPT LLaMA-Mesh 31.79 10.00 27.08 20.80 25.03 52.34 48.95 5.26 38.09 53.80 63.33 0.00 0.00 0.00 60.40 66.60 23.80 33.00 48.80 60.00 55.60 72.40 14.00 38.20 9.40 0.00 32.16 0.00 18.03 0.00 0.00 0.40 0.80 0.00 0.00 0.00 0.00 71.34 0.00 0.00 3.80 0.00 0.00 0.00 0.00 0.18 0.00 0.27 49.11 18.03 46.04 39.84 45.62 85.00 79.53 10.14 62.18 50.03 71.85 0.00 0.00 0.00 78.00 83.40 31.00 72.80 68.00 76.00 78.80 76.40 7.40 66.20 51.60 0.00 14.35 0.00 3.13 0.00 0.00 0.20 0.40 0.00 0.00 0.00 0.00 83.45 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 #3 0.00 3.13 6.44 3.73 7.49 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 8.59 0.00 7.06 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 297 On Path to Multimodal Generalist: General-Level and General-Bench Table 115: Results on NLP Group, from #L-20 to #L-22."
        },
        {
            "title": "Model",
            "content": "#L-20 (Event Ext) #L-21 (Sem Par) #L-22 (Ling Par) #1 #2 #1 #2 #3 #1 #2 #"
        },
        {
            "title": "SoTA Specialist",
            "content": "62.89 54.32 69.00 89.40 74.80 93. 91.40 92.43 Meta-Llama-3.1-8B-Instruct Qwen2.5-7B-Instruct Gemma-2-9b-it ChatGLM-6b Vicuna-7b-v1.5 InternLM-Chat-7b GPT-J-6B Falcon3-7B-Instruct Baichuan2-7B-Base Ministral-8B-Instruct-2410 Yi-Ligntning GPT-3.5-turbo GPT-4v GPT-4o GPT4o-mini GPT-4o-4096 ChatGPT-4o-latest Claude-3.5-Sonnet Claude-3.5-Opus Emu2-32B DetGPT InternVL2.5-8B InternVL2.5-4B NExT-GPT-V1.5 InternVL2.5-2B Monkey-10B-chat DeepSeek-VL-7B Qwen2-VL-7B Qwen-VL-Chat Qwen-Audio-Chat Qwen2-Audio-Instruct MoE-LLAVA-Phi2-2.7B-4e-384 mPLUG-Owl2-LLaMA2-7b Phi-3.5-Vision-Instruct Cambrian-1-8B MiniGPT4-LLaMA2 InternVL-Chat-V1-5 Mini-InternVL-Chat-4B-V1-5 InternLM-XComposer2-VL-1.8B GPT4RoI GLaMM LLaVA-NeXT-13B LLaVA-NeXT-34B Pixtral-12B SEED-LLaMA-13B BLIP2 MiniMonkey DeepSeek-VL-7B LISA-7B CogVLM-Chat ShareGPT4V-7B ShareGPT4V-13B 1.36 3.67 18.60 0.00 0.00 0.02 0.00 3.77 0.00 0.33 21.67 15.30 35.72 42.03 25.02 39.20 38.29 14.30 30.74 10.58 2.19 1.20 2.87 4.63 0.00 0.00 0.21 0.00 0.01 0.00 0.00 0.00 0.06 0.03 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.46 0.00 0.33 0.00 0.00 0.00 13.80 5.70 7.48 14.80 27.80 20.20 5.40 0.00 6.60 0.00 18.60 0.00 18.40 29.00 19.20 32.00 37.00 25.00 38.80 32.40 25.37 30.74 16.97 10.62 18.40 16.20 9.80 7.80 1.40 16.00 12.00 0.64 4.60 5.00 9.78 6.52 21.83 8.56 0.00 11.80 15.80 0.00 0.00 0.00 16.20 17.20 24.80 4.20 5.80 3.60 0.00 0.00 19.88 13.20 14.39 34.00 51.20 48.60 28.20 26.20 35.80 26.20 42.60 6.15 28.00 38.40 0.00 32.00 37.00 25.00 60.80 59.60 36.00 30.74 25.60 24.40 40.59 37.80 28.95 29.60 5.20 34.80 19.06 20.80 30.40 31.80 16.40 16.79 15.82 12.86 13.40 32.80 33.40 31.60 1.40 0.00 28.80 30.60 33.80 11.60 33.60 30.20 22.40 0.00 26.40 20.40 25. 38.00 57.60 47.80 20.40 20.40 24.00 20.40 45.20 0.00 24.40 20.80 0.00 42.99 45.60 53.60 87.20 79.80 43.00 30.74 28.60 29.40 30.40 24.80 21.45 32.80 20.00 36.60 20.17 20.31 23.10 24.00 19.80 18.50 20.06 21.60 18.94 27.60 27.60 34.00 9.00 0.00 25.60 28.60 32.80 17.80 25.80 26.00 20.80 0.00 34.00 28.00 32.80 25.20 29.40 30.20 22.80 22.80 21.40 22.80 31.60 0.00 24.20 28.60 0.00 4.40 13.60 8.80 36.40 25.60 28.60 30.74 25.20 18.20 25.40 25.60 22.10 24.60 22.40 24.00 23.38 19.63 22.50 23.60 21.00 21.01 25.37 17.88 20.32 26.00 23.40 31.60 22.40 0.00 22.20 20.40 26.20 6.80 21.40 21.80 22.60 0.00 20.00 20.80 19.40 298 57.80 78.40 84.40 64.60 24.20 54.40 24.20 79.80 0.00 53.80 83.60 0.00 0.20 1.40 0.60 86.60 61.40 33.40 30.74 30.40 23.20 66.20 39.40 32.70 53.20 19.20 59.60 21.00 18.59 49.60 50.80 20.30 19.43 21.30 20.74 17.55 61.60 55.20 61.60 6.00 1.40 21.20 32.20 24.60 15.80 49.80 51.60 24.20 0.00 27.00 20.60 25.20 36.80 49.60 55.40 50.60 21.20 31.00 21.20 48.00 0.00 34.40 67.00 0.00 3.80 5.40 6.80 1.00 0.00 30.20 30.74 27.40 21.20 41.99 38.80 19.50 26.60 10.00 39.00 29.00 21.20 38.70 39.60 40.80 25.40 33.00 2.40 24.00 40.00 30.00 38.20 10.20 0.00 29.20 33.40 31.20 14.20 33.60 31.80 26.40 0.00 24.20 20.40 21.80 68.46 44.49 72.27 20.91 0.79 21.86 23.80 48.46 0.00 33.12 30.88 70.06 68.72 78.69 70.90 56.27 41.54 39.97 30.74 37.35 28.58 52.82 34.23 4.37 25.63 0.00 24.30 15.38 9.12 20.30 21.11 23.77 23.83 19.64 22.24 22.15 43.23 36.06 18.72 0.00 0.00 23.45 22.17 28.89 7.23 29.06 20.80 24.79 0.00 33.50 27.92 32. On Path to Multimodal Generalist: General-Level and General-Bench"
        },
        {
            "title": "Model",
            "content": "BLIP-3 (XGen-MM) AnyGPT MiniCPM3-4B LaVIT-V2 (7B) GLM-VL-Chat Gemini-1.5-Pro Gemini-1.5-Flash OMG-LLaVA-InternLM20B Idefics3-8B-Llama3 Yi-Vision-v2 Qwen2-VL-72B Otter Show-o NExT-Chat InternVL2-26B Qwen2-VL-72B DeepSeek-VL-2-small DeepSeek-VL-2 LLaVA-One-Vision-7B LLaVA-One-Vision-72B Sa2VA-8B Sa2VA-26B CoLVA-2B CoLVA-4B Long-LLaVA LM4LV Vitron-V1 PandaGPT (13B) AnyGPT GAMA Pengi SALMONN-7B SALMONN-13B WavLLM ImageBind-LLM Unified-io-2-XXL ModaVerse-7b-v0 AudioGPT-GPT4 SpeechGPT-7B-com LLaMA-Omni 3D-LLM PointLLM-7B PointLLM-13B 3D-VisTA MotionGPT-T5 MotionGPT-LLaMA AvatarGPT LLaMA-Mesh #L-20 (Event Ext) #L-21 (Sem Par) #L-22 (Ling Par) #1 #2 #1 #2 #3 #1 #2 #3 6.27 4.43 13.48 7.11 10.77 22.16 13.45 0.00 13.92 7.89 27.94 0.00 0.00 0.00 0.00 24.80 0.30 0.20 0.10 8.30 1.90 2.80 0.00 2.60 0.00 0.00 3.62 0.00 5.69 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 32.37 0.00 1.23 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.02 14.19 5.69 21.15 14.27 22.53 28.62 22.31 2.40 18.00 15.60 23.33 0.00 0.00 0.00 12.40 23.60 15.00 10.60 14.60 30.20 19.20 18.60 1.20 13.80 14.80 0.00 9.60 0.00 9.60 0.00 0.00 0.00 0.00 0.00 0.00 5.60 0.00 23.00 0.00 15.10 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 26.80 9.60 29.80 24.80 29.00 48.60 43.20 13.60 35.80 12.40 55.33 26.67 0.00 2.00 36.80 54.60 30.60 35.80 42.00 46.00 45.40 43.00 25.60 38.60 28.00 0.00 24.36 13.60 11.20 15.67 13.60 15.20 15.60 15.69 13.60 1.50 13.60 31.80 27.40 13.00 0.00 0.00 0.00 0.00 0.00 12.28 0.00 26.20 31.60 11.20 35.80 24.20 32.60 56.40 49.20 10.40 43.80 2.40 62.00 12.67 0.00 4.66 38.20 73.40 30.60 28.00 31.00 74.20 20.60 20.40 20.20 30.60 35.20 0.00 19.37 25.80 10.20 19.86 25.80 4.80 5.80 20.81 25.80 2.50 25.80 40.22 20.80 17.80 0.00 0.00 0.00 0.00 0.00 19.80 0.40 20.40 22.20 10.20 28.60 24.60 24.00 29.40 27.20 6.80 28.20 25.87 30.32 17.62 0.00 7.91 25.20 38.10 24.40 24.80 27.20 36.40 27.80 28.40 22.40 26.40 25.20 0.00 22.30 12.50 11.00 24.10 12.50 20.00 12.50 24.32 12.50 15.40 12.50 4.40 23.10 15.10 0.00 0.00 0.00 0.00 0.00 22.80 0.00 22.80 25.00 11.00 31.80 27.40 29.60 83.80 82.60 9.00 69.40 6.20 87.33 18.00 5.30 1.33 73.80 89.80 53.20 62.40 71.40 91.40 75.00 81.80 44.60 76.80 58.00 0.00 31.60 16.30 9.80 18.45 10.75 16.80 17.30 19.54 16.30 3.10 16.30 0.20 23.80 26.40 0.00 0.00 0.00 0.00 0.00 22.80 0.00 24. 23.40 9.80 35.60 26.40 33.20 73.20 66.20 5.60 68.60 0.60 0.00 19.33 1.20 24.66 37.40 71.60 46.20 41.00 46.00 67.40 38.40 49.20 37.40 37.00 43.20 0.00 20.70 25.70 11.72 21.32 16.98 7.60 7.90 20.23 25.70 8.40 25.70 3.40 20.10 13.40 0.00 0.00 0.00 0.00 0.00 22.20 0.80 21.10 33.68 11.72 41.51 28.87 40.37 71.42 62.03 11.67 58.39 41.90 70.02 17.69 7.84 3.43 41.90 73.20 28.00 41.60 34.30 57.00 49.70 43.60 12.60 31.90 25.00 0.00 5.23 1.45 2.57 4.35 0.65 11.98 12.66 3.45 0.66 0.00 0.66 73.34 0.65 17.34 0.35 0.08 0.00 0.00 0.00 0.00 0.00 19.42 299 On Path to Multimodal Generalist: General-Level and General-Bench"
        },
        {
            "title": "C Statement",
            "content": "C.1 Ethical Statement This work adheres to rigorous ethical framework to ensure the responsible development, evaluation, and deployment of multimodal generalists. Below, we elaborate on the key ethical considerations. These ethical measures ensure that General-Bench serves as responsible and inclusive benchmark, contributing to the sustainable and equitable development of multimodal AI systems. Privacy and Data Protection. The benchmark and evaluation process ensure strict compliance with privacy regulations. All tasks and datasets used in General-Bench are carefully curated to exclude personally identifiable information (PII). To safeguard privacy, any data derived from public sources is anonymized, and sensitive content is filtered out. Our procedures align with relevant data protection standards, such as GDPR and CCPA, emphasizing our commitment to ethical research practices. Data Collection. The dataset for General-Bench is built using publicly available resources or through collaborations with contributors who explicitly consented to their data being included. Data collection protocols are designed to prioritize ethical sourcing, ensuring that contributors understand their rights, including the ability to withdraw their data at any time. This ensures transparency and fairness throughout the dataset construction process. Annotator Compensation. Human annotators play crucial role in ensuring the high quality of the General-Bench dataset. We engage well-trained annotators, including postgraduate students and crowdsourcing professionals, and provide fair compensation for their work. Annotators are either volunteered to contribute, or paid based on the estimated time required to complete specific tasks. All are signed to give their best efforts in data annotation and model implementation to ensure the work quality. Bias and Fairness. Recognizing the potential biases in AI systems, we take active measures to analyze and mitigate biases related to gender, ethnicity, language, and other sociocultural factors within the dataset and evaluation tasks. Diverse and representative data collection practices are employed across multiple modalities and languages. While we acknowledge that complete eradication of bias is challenging, we strive to identify and address biases as the benchmark evolves. Intellectual Property Protection. All datasets and tasks included in General-Bench respect intellectual property rights. Data collected from external sources is fully repurposed and modified, and is used under proper licensing agreements, ensuring compliance with intellectual property laws. Open-sourced models are strictly used according to their licenses. Models evaluated via APIs are handled according to their respective terms of use, and no proprietary content is redistributed without permission. Misuse Potential. We are aware of the potential risks associated with misuse of multimodal intelligence technologies, such as applications in surveillance or the manipulation of public opinion. To mitigate such risks, we have developed guidelines to encourage ethical use. These guidelines emphasize the importance of transparency, accountability, and consent in any application or further development of the technologies evaluated in this work. Accessibility and Inclusivity. In alignment with our commitment to fostering inclusivity in the AI research community, all code, tasks, and datasets related to General-Bench are openly available. This ensures that researchers from diverse backgrounds and varying resource levels can equally contribute to, and benefit from, advancements in multimodal generalist research. Evaluations and Performance. We clarify that the performances of all models reported in this paperincluding both specialists and generalistsare influenced by the specific testing environment. This includes factors such as the size and content of the dataset, as well as the parameters used in the reproduced code. As we continuously update the dataset, the evaluation results presented in this paper may differ from those obtained in future versions. We emphasize that such differences are considered reasonable and expected deviations, and should not raise any concerns. Our leaderboard is open and under active development, and we warmly welcome participation from external practitioners. 300 On Path to Multimodal Generalist: General-Level and General-Bench C.2 Author Contribution All authors contributed to this project in various capacities, including idea conceptualization, data annotation, model implementation, paper writing, and project supervision. To provide transparent overview, Table 117 summarizes the contributions and responsibilities of all co-authors. Given the extensive scope and workload of this project, we enlisted the help of large group of contributors. Among them, some individuals made contributions but did not qualify for co-authorship due to partial involvement or insufficient or voluntary contributions. Nevertheless, we acknowledge their efforts and list them in Table 119 to express our gratitude for their support. Table 117: Detailed author list and contribution statement. # Name 1 Hao Fei"
        },
        {
            "title": "Working for\nVideo group",
            "content": "Working for 3D group"
        },
        {
            "title": "Working for\nLanguage\ngroup",
            "content": "Role 1) Project general leader: design and implement the idea, including General-Level evaluation and General-Bench planning. 2) Worker for the audio group. 3) Designed data collection methods and selected models for verification. 4) Responsible for all paper writing, illustrations, and polishing. 5) Managed online deployment of data and automated evaluation systems. 6) Maintained the project website. 7) Provided computing resources. 1) Project co-leader: give the formal text and formula definitions for the 5 levels of the General-Level evaluation framework, along with the corresponding formula derivation. 2) Led the image group, managing tasks and execution. 3) Constructed and polished over 150 datasets; implemented around 30 SoTA specialists and 2 MLLMs. 4) Verified task and data management, and deployed systems. 5) Developed evaluation scripts and automated testing systems. Project co-leader for image group: Led the image group for supervised dataset collection and image-based MLLMs evaluation. Project co-leader for video group: Led the video group for supervised datasets collection and video-based MLLMs evaluation. Project co-leader for 3D group: Led the 3D group for supervised datasets collection and 3D-based MLLMs evaluation. Project co-leader for Language group: Led the Language group for supervised datasets collection and Language-based MLLMs evaluation."
        },
        {
            "title": "Responsible Datasets\nAll\ndatasets",
            "content": "audio-generation in audioResponsible Models All specialists generation tasks Audio MLLMs: WavLLM, ImageBind-LLM, Unifiedio-2-XXL, ModaVerse-7bv0, AudioGPT-GPT4, SpeechGPT-7B-com, LLaMA-Omni"
        },
        {
            "title": "All\ndatasets",
            "content": "image-related Image-oriented GPT4-o, GPT4-V MLLMs: GPT4-o-mini,"
        },
        {
            "title": "All\ndatasets",
            "content": "image-related Specialists and MLLMs supporting image-related skills All video-related datasets Specialists and MLLMs supporting video-related skills All 3D-related datasets Specialists and MLLMs supporting 3D-related skills Language-related datasets: L-2, 14, 15, 16, 17, 18, 19, 20, 21, 22 Specialists supporting L-2, 14, 15, 16, 17, 18, 19, 20, 21, 22 skills, and MLLMs including Qwen2.5-7B-Instruct, Baichuan2-7B-Base, Vicuna7b-V1.5, Falcon3-7B-Instruct, Ministral-8B-Instruct-2410 301 On Path to Multimodal Generalist: General-Level and General-Bench # Name 7 Shengqiong Wu Working for Audio group"
        },
        {
            "title": "Group",
            "content": "Role Project co-leader for the audio group: Led the audio comprehension group for the collection of supervised datasets and the evaluation of audio-based MLLMs. Responsible Datasets Audio-related datasets: A-C-1, 2, 3, 4, 5, 6, 7, 8,"
        },
        {
            "title": "8 Yaoting Wang Working for\nImage group",
            "content": "Implement image-related MLLMs for evaluation /"
        },
        {
            "title": "9 Junbao Zhou",
            "content": "Working for 3D group Collect 3D-related datasets and Implement 3Drelated Specialists and MLLMs for evaluation 3D-related datasets: D-C1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,"
        },
        {
            "title": "Working for\nVideo group",
            "content": "Collect video-related datasets and Implement video-related Specialists and MLLMs for evaluation Video-related datasets: VC-2, 3, 4,"
        },
        {
            "title": "12 Zhiyuan Zhou Working for\nImage group",
            "content": "Collect video-related datasets and Implement video-related Specialists and MLLMs for evaluation Collect image-related datasets and Implement image-related Specialists and MLLMs for evaluation Video-related datasets: VG-1, 2, 3, 4 Image-related datasets: IC-5, 7, 14, 15, 26, 28, 30,"
        },
        {
            "title": "Working for\nImage group",
            "content": "Collect image-related datasets and Implement image-related Specialists and MLLMs for evaluation Image-related datasets: IC-5, 13, 19, 26, 27,"
        },
        {
            "title": "Working for\nImage group",
            "content": "Collect image-related datasets and Implement image-related Specialists and MLLMs for evaluation Image-related datasets: IC-8, 9, 17, 21, 25, 26, 28, 31, 35 302 Responsible Models Specialists supporting A-C-1, 2, 3, 4, 5, 6, 7, 8, 9 skills, and MLLMs including Qwen-Audio-Chat, Qwen2Audio-Instruct, Vitron-V1, GAMA, Pengi, WavLLM, SALMONN-7B SALMONN13B SpeechGPT-7B-com, AudioGPT-GPT4, AnyGPT, PandaGPT-13B, ImageBindLLM, ModaVerse-7b-v0, Unified-io-2-XXL, NExTGPT-V1.5 MLLMs including Qwen2VL-7B, Qwen-VL-Chat, MoE-LLAVA-Phi2-2.7B4e-384, mPLUG-Owl2LLaMA2-7b, Phi-3.5-VisionCambrian-1-8B, Instruct, MiniGPT4-LLaMA2-7B Specialists supporting D-C-1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13 skills, and MLLMs 3D-LLM-2.1B, including PointLLM-7B, PointLLM13B, 3D-VisTA supporting Specialists 20 skills, 4, 3, V-C-2, and MLLMs including Long-LLaVA-9B, DeepSeekVL-2-small, DeepSeek-VL-2, LLaVA-One-Vision-7B, LLaVA-One-Vision-72B Specialists supporting V-G-1, 2, 3, 4 tasks, and MLLMs including VidAgent Specialists supporting I-C-5, 7, 14, 15, 26, 28, 30, 34 skills, and MLLMs including InInternVL2 5-2B, ternVL2 5-4B, InternVL2 5Monkey-10B-chat, 8B, DeepSeek-VL-7B-Chat Specialists IC-5, 13, 19, 26, 27, 28 skills, and MLLMs including InternVL-Chat-V1-5, Mini-InternVL-Chat-4B-V1-5, InternLM-XComposer2-VL1.8B, GPT4RoI-7B, GLaMM Specialists supporting I-C8, 9, 17, 21, 25, 26, 28, 31, 35 skills, and MLLMs including BLIP2, miniMonkey, DeepSeek-VL-7B-Base, LISA supporting On Path to Multimodal Generalist: General-Level and General-Bench # Name 15 Daoan Zhang Group Working for 3D group Role Collect 3D-related datasets and Implement 3Drelated Specialists and MLLMs for evaluation Responsible Datasets 3D-related datasets: D-G1, 2, 3, 4, 5, 6, 7, 8,"
        },
        {
            "title": "Working for\nImage group",
            "content": "Collect image-related datasets and Implement image-related Specialists and MLLMs for evaluation Image-related datasets: IC-4, 26,"
        },
        {
            "title": "Working for\nImage group",
            "content": "Implement image-related MLLMs for evaluation /"
        },
        {
            "title": "Working for\nImage group",
            "content": "Collect image-related datasets and Implement image-related Specialists and MLLMs for evaluation Image-related datasets: IC-26,"
        },
        {
            "title": "Working for\nImage group",
            "content": "Collect image-related datasets and Implement image-related Specialists and MLLMs for evaluation Image-related datasets: IC-17, 22, 23, 38,"
        },
        {
            "title": "Working for\nImage group",
            "content": "Collect image-related datasets and Implement image-related Specialists and MLLMs for evaluation Image-related datasets: IC-3, 7, 28, I-G-1, 3, 4, 5, 6, 7, 8, 12, 14,"
        },
        {
            "title": "Working for\nVideo group",
            "content": "Collect video-related datasets and Implement video-related Specialists and MLLMs for evaluation Video-related datasets: VC-5, 6, 7, 8, 9, 10, 11, 12,"
        },
        {
            "title": "Working for\nVideo group",
            "content": "Collect video-related datasets and Implement video-related Specialists and MLLMs for evaluation Video-related datasets: VC-14, 15, 16, 17, 18,"
        },
        {
            "title": "Working for\nLanguage\ngroup",
            "content": "Collect language-related datasets and Implement language-related Specialists and MLLMs for evaluation Language-related datasets: L-1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,"
        },
        {
            "title": "24 Zixiang Meng Working for\nImage\nand\nVideo group",
            "content": "Collect image, video-related datasets and Implement image, video-related Specialists and MLLMs for evaluation Image-related datasets: IC-6, 28, 38, and Videorelated datasets: V-C-20 Responsible Models supporting Specialists D-G-1, 2, 3, 4, 5, 6, 7, 8, 9 skills, and MLLMs including MotionGPTT5, MotionGPT-LLaMA, AvatarGPT, LLaMA-mesh Specialists supporting I-C-4, 26, 34 skills, and MLLMs including Claude-3.5-Sonnet, Claude-3.5-Opus, Emu2-37B, DetGPT MLLMs including Otter, Show-o, NExT-Chat, Yivision-v2, Qwen2-VL-72B Specialists supporting I-C26, 34 skills, and MLLMs including Claude-3.5-Sonnet, Claude-3.5-Opus, Emu2-37B, DetGPT Specialists supporting I-C17, 22, 23, 38, 39 skills, and MLLMs including Pixtral-12B, SEED-LLaMALLaVA-NeXT-13B, 13B, LLaVA-NeXT-34B Specialists supporting I-C-3, 7, 28, I-G-1, 3, 4, 5, 6, 7, 8, 12, 14, 15 skills, and MLLMs including BLIP-3 (XGen-MM), CogVLMChat, ShareGPT4V-7B, ShareGPT4V-13B Specialists supporting V-C-5, 6, 7, 8, 9, 10, 11, 12, 13 skills, and MLLMs including InternVL-2-8B, InternVLInternVL-2-26B, 2.5-8B, InternVL-2.5-26B Specialists supporting V-C14, 15, 16, 17, 18, 19 skills, and MLLMs including CoLVA-2B, CoLVA-4B, Sa2VA-8B, Sa2VA-26B Specialists supporting L-1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13 skills, and MLLMs including Meta-Llama-3.18B-Instruct, Gemma-2-9b-it, ChatGLM-6B, GPT-J InternLM2-Chat-7B, Yilightning Specialists supporting I-C-6, 28, 38 and V-C-20 skills, and MLLMs including Gemini1.5-Pro, Gemini-1.5-Flash, OMG-LLaVA-InternLM20B, Idefics3-8B-Llama3 303 On Path to Multimodal Generalist: General-Level and General-Bench # Name 25 Shilin Xu"
        },
        {
            "title": "Group\nWorking for\nVideo group",
            "content": "Role Collect video-related datasets and Implement video-related Specialists and MLLMs for evaluation Responsible Datasets Video-related datasets: VC-1, 2, 4 Responsible Models Specialists supporting V-C1, 2, 4 skills, and MLLMs including InternVL-2.5-8B, InternVL-2.5-26B, Qwen2VL-7B, Qwen2-VL-72B MLLMs including GPT4-o4096. image-related dataset collection: I-C-6, 17, and image-related dataset collection: I-C-17, 28 MLLMs including gpt-3.5turbo, chatgpt4-o-latest. Video-related datasets: VC-2, V-G-1, 5, 6 / / / / Specialists supporting V-C2, V-G-1, 5, 6 skills, and MLLMs including LM4LV / / / /"
        },
        {
            "title": "29 Jiebo Luo",
            "content": "Work for image group Work for image group"
        },
        {
            "title": "Working for\nVideo group",
            "content": "Discussion & Advisory"
        },
        {
            "title": "Project\nSupervision",
            "content": "Collect image-related datasets and Implement image-related Specialists and MLLMs for evaluation Collect image-related datasets and Implement image-related Specialists and MLLMs for evaluation Collect video-related datasets and Implement video-related Specialists and MLLMs for evaluation Discussed the high-level directions and goals of the project. Provided important and insightful feedback for the overall system design. Discussed the high-level directions and goals of the project. Provided important and insightful feedback for the overall system design. 1) Project co-supervisor, conceptualized the idea of General-Level, and the entire process. 2) Provided computing resources. 1) Project co-supervisor, co-conceptualized the idea of General-Level, and supervised the entire process. 2) Provided computing resources. 304 On Path to Multimodal Generalist: General-Level and General-Bench Table 119: List of some contributors without authorship."
        },
        {
            "title": "Zhengzhe Liu",
            "content": "Contributed to image group: assisted in image-oriented dataset preparation and model testing."
        },
        {
            "title": "Zhongze Luo",
            "content": "Evaluating Yi-vision-v2 on 77 dataset; evaluating Show-o on 65 datasets."
        },
        {
            "title": "Chunhan Li",
            "content": "Evaluating Show-o on 136 image comprehension datasets and partial 67 datasets."
        },
        {
            "title": "Qirui Huang",
            "content": "Evaluating Yi-lightning on 117 NLP datasets and evaluating Show-o on 44 image generation."
        },
        {
            "title": "Ming Lei",
            "content": "Assist in evaluating certain MLLMs on certain datasets. Evaluating Otter on certain datasets."
        },
        {
            "title": "Zhangyu Wang",
            "content": "Evaluating Otter on certain datasets."
        },
        {
            "title": "Lin Liu",
            "content": "Contributed to the preparation of image-related task data during phases 1 and 2."
        },
        {
            "title": "Chengjie Zhou",
            "content": "Contributed to the preparation of NLP task data during phase 1."
        },
        {
            "title": "Yucheng Han",
            "content": "Contributed to the preparation of image-related task data during phase 1."
        },
        {
            "title": "Peng Zhou",
            "content": "Contributed to the preparation of image-related task data during phase 1."
        },
        {
            "title": "Luanyuan Dai",
            "content": "Contributed to the preparation of image-related task data during phase 2."
        },
        {
            "title": "Yuxuan Liu",
            "content": "Contributed to the preparation of image-related task data during phase 2."
        },
        {
            "title": "Xu Zhang",
            "content": "Contributed to the preparation of image-related task data during phase 2. Contributed to the preparation of image-related task data during phase 2. Contributed to the preparation of image-related task data during phase 2."
        },
        {
            "title": "Wenjie Zhuo",
            "content": "Contributed to the preparation of image-related task data during phase 2."
        },
        {
            "title": "Lianyuan Fan",
            "content": "Contributed to 3D generation-related data collection during phase 2 (incomplete). # 1 2 3 5 6 7 8 9 11 12 13 14 15 17"
        }
    ],
    "affiliations": [
        "HFUT",
        "KAUST",
        "NJU",
        "NTU",
        "NUS",
        "PKU",
        "SJTU",
        "UR",
        "WHU",
        "ZJU"
    ]
}