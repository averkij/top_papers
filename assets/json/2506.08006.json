{
    "paper_title": "Dreamland: Controllable World Creation with Simulator and Generative Models",
    "authors": [
        "Sicheng Mo",
        "Ziyang Leng",
        "Leon Liu",
        "Weizhen Wang",
        "Honglin He",
        "Bolei Zhou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large-scale video generative models can synthesize diverse and realistic visual content for dynamic world creation, but they often lack element-wise controllability, hindering their use in editing scenes and training embodied AI agents. We propose Dreamland, a hybrid world generation framework combining the granular control of a physics-based simulator and the photorealistic content output of large-scale pretrained generative models. In particular, we design a layered world abstraction that encodes both pixel-level and object-level semantics and geometry as an intermediate representation to bridge the simulator and the generative model. This approach enhances controllability, minimizes adaptation cost through early alignment with real-world distributions, and supports off-the-shelf use of existing and future pretrained generative models. We further construct a D3Sim dataset to facilitate the training and evaluation of hybrid generation pipelines. Experiments demonstrate that Dreamland outperforms existing baselines with 50.8% improved image quality, 17.9% stronger controllability, and has great potential to enhance embodied agent training. Code and data will be made available."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 ] . [ 1 6 0 0 8 0 . 6 0 5 2 : r Dreamland: Controllable World Creation with Simulator and Generative Models Sicheng Mo Ziyang Leng Leon Liu Weizhen Wang Honglin He Bolei Zhou University of California, Los Angeles Figure 1: Controllable world creation with Dreamland. It combines simulator for physically grounded scene generation and large-scale pretrained generative model for creating realistic visual world following user-provided text prompts."
        },
        {
            "title": "Abstract",
            "content": "Large-scale video generative models can synthesize diverse and realistic visual content for dynamic world creation, but they often lack element-wise controllability, hindering their use in editing scenes and training embodied AI agents. We propose Dreamland, hybrid world generation framework combining the granular control of physics-based simulator and the photorealistic content output of large-scale pretrained generative models. In particular, we design layered world abstraction that encodes both pixel-level and object-level semantics and geometry as an intermediate representation to bridge the simulator and the generative model. This approach enhances controllability, minimizes adaptation cost through early alignment with real-world distributions, and supports off-the-shelf use of existing and future pretrained generative models. We further construct D3Sim dataset to facilitate the training and evaluation of hybrid generation pipelines. Experiments demonstrate that Dreamland outperforms existing baselines with 50.8% improved image quality, 17.9% stronger controllability, and has great potential to enhance embodied agent training. Code and data will be made available at https://metadriverse.github.io/dreamland/. Equal Contribution Preprint. Under review."
        },
        {
            "title": "Introduction",
            "content": "Large-scale pre-training with scalable model architectures has significantly advanced generative modeling in recent years. This progress has led to surge of foundation models across various modalities, including language [16], image [712], and video generation [1316]. These foundation models capture structured information about entities, their interactions, and temporal dynamicsoften referred to as the \"world knowledge\". As such, they hold great promise for providing synthetic data to train embodied AI agents beyond just generating visual content. By providing interactive feedback, they can potentially replace human supervision or physical environments, enabling more efficient and scalable agent learning. Despite the capabilities of current world models being exciting, they often lack the fine-grained control required for agent learning, e.g., autonomous driving. For instance, training autonomous vehicles requires simulating complex scenarios. These include specific vehicle maneuvers like lane changes in dense traffic or responses to stop signs. Current generative world models often struggle to offer object-level controllability, thereby constraining their efficacy in situations where precise scene layout and object configuration are critical. Recognizing these challenges, hybrid approach that combines physical simulators and data-driven generative models has emerged as promising solution. In such pipelines, the simulators first provide the accurate physical and spatial information, such as vehicle dynamics, traffic rules, and environmental function zones. Then, image or video generative models synthesize realistic visual content following these conditions. Despite these improved pipelines, current hybrid methods struggle to balance simulator fidelity with generative freedom. Some methods directly render scenes without any hallucination [17, 18], which necessitates high-fidelity simulators and compromises their generalizability and scalability for diverse world creation. On the other hand, other work [19] fully utilizes generative models for scene re-rendering without stringent constraints, which consequently sacrifices fine-grained simulator control. To balance the controllability from simulators and the creative freedom of generative models with rich visual details in synergetic way, we present Dreamland, bridging simulator and generative models through Layered World Abstraction (LWA). LWA records structured meta information that contains pixel-level information like depth and RGB value, and object-level information like object categories. Instead of directly generating the scene from the initial simulator layered world abstraction (SimLWA), Dreamland first augments it to real-world layered world abstraction (Real-LWA) that better aligns with real-world visual distributions. During inference, Dreamland follows user instructions to divide the scene into preserved and editable regions. Editing models could help diversify the scene structure within the editable region while keeping the preserved region untouched. Therefore, by strictly following conditions in Real-LWA during visual re-rendering, the final generated scenes can achieve both high-quality appearance and precise object control that adheres to the simulator and user instructions. Dreamland design offers several strengths. First, our pipeline largely enhances controllability and flexibility over the hybrid approach, outperforming the previous state-of-the-art [19] by 52.3% and 17.9% in image quality and controllability. Notably, Dreamland also demonstrates benefits in downstream agent training, boosting visual question answering performance on the real-world test set by 3.9 absolute accuracy. Second, since the Real-LWA is already aligned with real-world distributions, it can be seamlessly integrated with future, more potent conditional-generation models without introducing significant adaptation costs. Finally, Dreamland pipeline is generalizable and scalable, thus unlocking various applications based on simulators, including video generation and scene editing, as illustrated in Figure 1. We also construct large-scale dataset for training and evaluating such hybrid pipelines. We summarize our key contributions as follows:(1) Dreamland - hybrid generation pipeline that connects physics-based simulator and generative models with layered world abstraction (LWA) to achieve controllable and configurable world creation. (2) Our approach demonstrates superior results in the scene generation task, exceeding previous stateof-the-art by 52.8% image quality and 17.9% controllability. Also, we demonstrate how Dreamland improves the adaptation of embodied agents to the real world. (3) We construct dataset called D3Sim (Diverse Driving Scenario in Real WorlD and Simulation) for training and benchmarking hybrid generation pipelines that combine simulators and generative models. 2 Figure 2: Illustration of the Dreamland. It is three-stage pipeline including: (1) Simulation, (2) Layered World Abstraction Simulator-to-Real-World, (3) Mix-Condition Generation."
        },
        {
            "title": "2 Related Work",
            "content": "Controllable Visual Generative Models. Large-scale foundational generative models have been developed rapidly in recent years for image generation [712], video generation [1316], and multimodal generation [2025]. While those foundation models usually rely solely on text prompts, various approaches have been proposed to add additional controllability to the generation process, including structure control [2632], subject control [3336], and adding image editing capability [37, 38]. Meanwhile, another line of work addresses the more challenging multi-condition generation problem. Specifically, UniControl [39] and UniControlNet [40] propose unified adapters to process various conditioning signals. Distinguished from the above works that could obtain high-quality paired data with pre-trained detection models, the simulator and real-world paired data are expensive to annotate. Therefore, Dreamland focuses on adapting from existing conditional generation models with low cost while preserving their rich world knowledge. Generative Model for Autonomous Driving. Generative models have largely advanced autonomous driving research in recent years. One line of work [4146] trains generative models, including GANs and Diffusion Models, to generate multi-view images from driving scene graph, from bird-eye-view to HD maps. Specifically, Panacea [47], InfiniteCube [48], MagicDrive [43], and MagicDrive-V2 [44] build on pre-trained video diffusion models to add temporal consistency into the generated scene. Another line of work [4952] takes historical information as the condition signal to generate future driving scenes, but they often lack control over the generated scenes. Dreamland differs from previous literature in its hybrid approach, allowing fine-grained control over each driving vehicle, such as specifying their individual paths, actions, and interactions. Synthetic Data for Embodied AI Training. The significant visual domain gap between simulators and the real world data has historically presented challenge in training agents in simulation. Due to recent advances in generative and reconstruction techniques, this problem has been partially addressed by integrating simulators, vision generative models, and 3D reconstruction methods. For example, Vid2Sim [53] and VR-Robo [54] employ Gaussian-Splatting techniques to convert real scene to an interactive digital twin, and trained agents can zero-shot deploy to the real world. On the other hand, Lucidsim [17] trains robot dog to achieve great adaptability using simulator with pre-trained depth-conditioned diffusion model. Compared to previous literature, the Dreamland pipeline ensures granular controllability while being applicable to various generative models."
        },
        {
            "title": "3 Method",
            "content": "Dreamland enables controllable driving scene creation via three key stages shown in Figure 2: (i) Stage-1 Simulation: scene construction with physics-based simulator. (ii) Stage-2 LWA-Sim2Real: transferring the Sim-LWA from simulation to Real-LWA with an instructional editing model and user instructions. (iii) Stage-3 Mixed-Condition Generation: rendering an aesthetic and realistic scene with large-scale pretrained image or video generation model. The pipeline starts with the simulator rendering scene according to the scene record, where the agents motion is derived from trajectory replay or trained policy. The Sim-LWA captured from the simulator is then refined and rendered into realistic frames. We first introduce the layered world abstraction in 3.1. We then talk about the design of Stage-2 in 3.2, and Stage-3 in 3.3. Lastly, we describe our training scheme and implementation details in 3.4."
        },
        {
            "title": "3.1 Layered World Abstraction",
            "content": "We define Layered World Abstraction (LWA) as an intermediate representation to align simulators and generative models in the pixel space. It enables fine-grained control over the generation process. LWA composes scene from multiple world layers, where each layer corresponds to different classes of objects or regions. The representation is structured as = (cid:91) i=0 Li Vi, (1) where Li RHW denotes the ith world layers, and Vi RHW is the corresponding visibility mask. Each world layer encodes both pixel-level and object-level geometry and semantics through modalities of condition. Thus, the ith layer is formulated as (cid:40) (cid:41) Li = cj RHW Cj = 0, 1, . . . , K; Cj = , (2) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:88) j=0 where cj denotes the jth condition with Cj channels. This flexible design allows for customizable, pixel-level control over both the object-of-interest and the region-of-interest. Given driving frame rendered by the simulator following the scene record, we decompose it into Sim-LWA according to the pixel semantics. Sim-LWA contains accurate traffic participants and layout, which is physically grounded by the simulator and serves well for layers requiring precise control. However, due to the limited assets and lack of background details, those corresponding layers hold disparity from real-world distribution and hinder the realistic scene generation."
        },
        {
            "title": "3.2 Stage-2: LWA-Sim2Real",
            "content": "The LWA-Sim2Real refinement process involves refining the Sim-LWA to real-world distributions. In the Dreamland pipeline, we compose our LWA from three world layers: traffic participants layer Ld that contains dynamic objects (e.g., vehicles, pedestrians, cyclists), map layout layer Ll that defines static layout (e.g., roads, crosswalk, intersections), and background layer Lb that covers static objects and background regions (e.g., buildings, vegetation, sky). The first two layers, Ld and Ll, are derived from the simulator and preserved to provide precise control, while the last layer Lb is editable and refined by an editing model as described below. Given preserved world layers and their masks from simulators {Ld, Ll}, {V d, l}, we employ an instructional editing model ϵe to selectively refine layer Lb according to the provided text instruction c. This process can be formulated as, Lb = ϵe (cid:16) ˆW, b, (cid:17) . (3) The ˆW = (cid:8)Ld Ll l(cid:9) denotes the preserved LWA from the simulator, and the = Ω (V l) is the editing mask given the pixel domain Ω. This process transfers the Sim-LWA to real-world distribution while preserving the grounding information from the simulator. It bridges the domain gap between the general simulators and generation models, alleviating the requirements for high-fidelity simulation rendering and minimizing the adaptation costs for generation models."
        },
        {
            "title": "3.3 Stage-3: Mixed-Condition Generation",
            "content": "In this stage, we utilize pre-trained conditional generative model to generate realistic views according to our refined Real-LWA. As the representation is already aligned with real-world distribution, minimal adaptation is required to employ pre-trained model on it, thus preserving its world knowledge. For pre-trained conditional world models that cover the condition modalities in our LWA, we split the channel dimension of LWA according to condition modalities and serve as the respective condition inputs. However, some of the pre-trained conditional world models are trained on specific condition modalities that do not fully cover the modalities within our LWA. Therefore, we have two choices to further minimize the adaptation cost while preserving the pre-trained world knowledge: (1) Extracting only specific condition modalities, e.g., depth or segmentation maps, from LWA. (2) Adapting the 4 pre-trained conditional world model to our LWA by updating small number of parameters. We empirically found that the second approach yields better controllability thus adopted it in our design. Given the Real-LWA, which contains conditions, we first encode them into the latent space with encoder E, and concatenate them along the channel dimension. Then, we use linear layer ξ to project the concatenated condition latent into the same dimension as the noise latent of the world model. Finally, we add the projected condition latent with the noise latent to incorporate structural guidance. The whole process can be formulated as = + ξ (cid:33) E(cj) . (cid:32) (cid:77) j=0 (4) The output noise latent with structural information serves as the input to the diffusion transformer blocks of the world model. Thereby, we achieve mixed-condition generation by fine-tuning the projection layer ξ, rendering realistic scene from the refined LWA."
        },
        {
            "title": "3.4 Training Scheme and Implementation Details",
            "content": "We train our Dreamland pipeline in two steps. The first training step corresponds to the LWASim2Real stage, where it enables the instructional editing model to learn Sim2Real refining of LWA. The second step is for the mixed-condition generation stage, which involves fine-tuning conditional generation model to take structural control from the Real-LWA. We design our instructional editing model following ACE++ [38], training on our image dataset for 4K iterations with batch size of 128. To align with the pre-trained editing model, we expand the world representation into single image by concatenating the condition maps vertically. The loss for LWA Sim2Real transfer follows LSim2Real = Lb 0 (cid:2)Lb Lb 02 2 (cid:3) , (5) where Lb is the refined layer defined in Eq. 3 and Lb 0 denotes the corresponding world layer in real-world distribution. We employ Flux Depth [55], an image generation model ϵθ with structure control based on depth maps, for our second step and adapt it to our LWA using the loss formulated as Ladapt = Ex0 (cid:2)ϵθ(x) x02 2 (cid:3) , (6) where the is from Eq. 4 and x0 is the latent of the realistic view. To balance the inference cost and visual quality, we set the default resolution(width) of our second and third-stage models to 512 and 1024, respectively. Additional details are provided in Appendix B."
        },
        {
            "title": "4 Data curation",
            "content": "We curate large-scale dataset called D3Sim (Diverse Driving Scenario in Real WorlD and Simulation) that contains diverse driving scenarios in the real world and the simulation. It provides realistic perspective views and high-quality condition data to facilitate the Sim2Real transfer. Previous digital twin driving datasets (e.g., DIVA-real [19]) comprise limited samples with the image mismatch problem, which is suboptimal for training our editing model. Thus, we precisely aligned our digital twin driving scenes for LWA-Sim2Real transfer. Based on the nuPlan dataset [56], we obtain realistic conditions (Real Conditions) using pre-trained models. We then utilize ScenarioNet [57] to construct the corresponding digital twin simulation scene in MetaDrive simulator [58]. By replaying the ego vehicle trajectory, we obtain conditions in the simulation domain (Sim Conditions). We organize the constructed digital twins into two dataset variants for the Dreamland pipeline. Details of the data curation pipeline, as well as the video dataset used in the Dreamland-Video pipeline, are provided in the Appendix A. Training Dataset. This dataset contains digital twin driving scenarios that capture the real-world distribution, enabling the second-stage model to learn Sim2Real transfer. We construct the dataset with paired Sim and Real conditions, along with realistic perspective views. We split the conditions into three world layers according to the semantics and construct the LWA accordingly. This results in high-quality digital twin training data from approximately 1,800 scenarios with around 60,000 samples at 2 Hz sample rate. We use this dataset to train our Dreamland pipeline. 5 Figure 4: Rendering diverse appearances align to the simulators conditions. Dreamland can simulate the driving scene and render it into high-quality scenes with generative models. Validation Dataset. The validation dataset is used to evaluate both the visual quality and controllability of the generation pipeline. While the digital twin scenarios provide realistic views and distribution of the real-world driving, the complex scene compositions, including partially occluded or overlapping objects, affect the accurate evaluation of controllability. Therefore, we derived this validation dataset from real-world scenarios by selectively modifying the traffic participants, eliminating ambiguous object placements, while keeping the scene layout unchanged. Diverse text prompts involving more than 30 cities, 20 weather conditions, different times of day, and various street types are randomly generated for each sample. This yields diverse and clean validation data from more than 600 scenarios with 16,000 samples at 2 Hz sample rate, with object distribution shown in Figure. 3. Figure 3: Object distribution of the validation dataset."
        },
        {
            "title": "5 Experiments",
            "content": "Extensive qualitative and quantitative results demonstrate the proposed pipelines generation performance (Sec. 5.1). We also present experiments on flexible control using Dreamland, an extension to Dreamland-Video for continuous simulator re-rendering (Sec. 5.2), an ablation study of our key designs (Sec. 5.3), and an application to downstream agent learning task (Sec. 5.4). Evaluation Metrics. There are two aspects regarding the evaluation of the simulator-controlled world creation pipeline: the fidelity of the re-rendered scene to the original simulator output, and the overall visual quality of these visual re-renderings. We render 16K scenes from our D3Sim validation dataset and report the FID to DIVA-Real Dataset constructed in SimGen [19]. We also evaluate depth si-RMSE and segmentation mIoU following previous works [26, 59] for rendered scenes."
        },
        {
            "title": "5.1 Simulator Conditioned Generation",
            "content": "Qualitative Results. As shown in Figure 4, Dreamland is able to re-render scene into diverse high-quality scenes closely aligned with the spatial scene layout in the simulator but differ in weather, location, and light condition following user-provided text prompts. Leveraging the robust capabilities of pre-trained Text-to-Image diffusion models, Dreamland is capable of synthesizing realistic driving scenes with resolution of up to 1024 576 pixels. 6 Figure 5: Qualitative comparison with baseline methods. Dreamland with fine-tuned Flux matches the visual fidelity of the frozen Flux model while improving adherence to simulator conditions. Figure 6: Extension of Dreamland. Dreamland pipeline is generalized to various downstream tasks. Baselines. Our main baselines include generative models for autonomous driving that take scene layout conditions. SimGen [19] first obtains front-view observation from the MetaDrive simulator [58], then trains cascade diffusion models to render high-quality images. Alongside our main approach, we create variations of Dreamland by extracting depth condition from our LWA and directly employing pretrained depth-conditioned diffusion models for as third-stage models, notated as Dreamland (Frozen {Model}). Comparison with Baselines. Method SimGen Image Quality Controllability Figure 5 and Table 1 compare our methods against several baselines. We observe that Dreamland vastly outperforms SimGen with 52.3% lower FID and 17.9% better si-RMSE. Meanwhile, the Dreamland pipeline exhibits superior scalability: it benefits from plugging in progressively stronger pretrained models for Stage-3 (SDXL SD3 Flux), yielding marked gains in image quality and controllability. Notably, Dreamland fine-tuned on Flux matches the visual fidelity of the frozen Flux model while slightly improving adherence to simulator conditions, highlighting the world-knowledge preservation inherent in our Dreamland pipeline. Table 1: Comparison on image quality and controllability. Dreamland is strong and scalable. Dreamland (Frozen SDXL) Dreamland (Frozen SD3) Dreamland (Frozen Flux) Dreamland (Flux) 0.698 0.747 0.785 0.791 0.744 0.673 0.639 0.646 84.48 63.74 48.48 50.58 si-RMSE mIoU 106.02 FID 0.752 0."
        },
        {
            "title": "5.2 Extension of Dreamland.",
            "content": "Simulator Editing. Figure 6 (a) shows that Dreamland supports new application that edits generated scene by adjusting the corresponding source scene. Building upon our current pipeline, we employ pretrained Flux-fill model to remove the truck and add speedy car on the road. By updating the LWA, we could provide additional editing mask support, such as pretrained model without additional adaptation cost, showcasing the flexibility of our Dreamland pipeline. Safety-critical Generation. Figure 6 (b) shows that by designing the driving scenario with the simulator, Dreamland could generate safety-critical scenes that are dangerous to collect in real world, revealing the great potential of applying Dreamland for agent learning. 7 Figure 7: Qualitative Results. Our Dreamland-Video pipeline can re-render the simulator reference video into realistic and diverse video frames while maintaining the pretrained world knowledge. Method f-30 FID f-60 FID f-90 FID f-120 FID si-RMSE mIoU Dreamland-Video 90. 88.93 91.51 88.78 0.659 0.717 Visual Quality Controllability Table 2: Quantitative Results of Dreamland-Video. Our pipeline preserved strong video generation and outstanding condition-following capabilities while adapting to our world representations. Extension to Additional Simulator. With the simple and generalized pipeline design, our LWA naively supports various simulator types. Figure 6 (c) shows Dreamlands zero-shot capability on the MetaUrban [60] simulator for mobility agent learning. Extension to Video Re-Rendering. Benefit from the simple pipeline design and low adaptation cost of Dreamland, it extends naturally to video generation without any specialized architectural changes. In Stage-2, we fine-tune the Cosmos-Predict1-7B [59] Text-to-Video diffusion model into an instructional video-editing model. The training uses our D3Sim video dataset for 10K iterations with batch size of 8. Following standard practice in image editing models, we initialize from the pretrained generative model and concatenate the extra conditioning latents with the noise latent along the channel dimension. For Stage-3, we employ multi-condition video diffusion model, Cosmos-Transfer1-7B [59], without finetuning. Leveraging this state-of-the-art model, Dreamland-Video can produce videos from 1K up to 4K resolution and as long as 121 frames. We evaluate our pipeline using 800 videos from the D3Sim dataset and report FID using all extracted frames. Table 2 and Figure 7 demonstrate the strong performance of our pipeline. By preserving the world knowledge embedded in these models, Dreamland-Video achieves the same level of controllability as Dreamland while consistently delivering high visual fidelity. We provide additional video results on our project webpage, which includes three sections: (1) Simulator-controlled scene generation, (2) Diverse text-controlled scene generation with the simulator, and (3) Simulator-controlled safety-critical driving scene generation Dreamland-Video demonstrates strong performance in world creation with physical simulator and generative models. Its zero-shot capability to generate safety-critical scenarios allows efficient sampling of dangerous driving behaviors, laying the foundation for training autonomous driving agents with safe behavioral responses. It is worth noting that, by swapping in other pretrained world models, the Dreamland-Video framework can be extended to even longer or fully autoregressive video re-rendering, we leave further exploration of this promising direction to future research."
        },
        {
            "title": "5.3 Ablation Study",
            "content": "We evaluate the effectiveness of our Stage-2 instructional editing model by comparing it against baseline that directly generates the abstraction. Specifically, we finetune the Flux-Fill-Dev [55] model via LoRA [61] for 4K iterations under two setting: (1) editing pipeline: our Real-LWA serves as 8 Figure 8: Qualitative Comparison of Stage-2 models design choices. Image Quality Pipeline Controllability Stage-2 Controllability Method Dreamland w/o editing Dreamland FID 63.21 50.58 si-RMSE mIoU si-RMSE mIoU 0.713 0.647 0.658 0.791 0.709 0.672 0.649 0.950 Table 3: Ablation Study on Stage-2 design choices. Dreamlands Stage-2 design largely improved alignment to Stage-1s simulator, leading to better visual quality. targets, and we provide an editing mask to the model as input; (2) generation pipeline: segmentation and depth maps of the realistic views derived from pretrained networks are set as the learning target. At inference time, we again provide an editing mask to indicate regions that should be preserved to our editing models, and we employ the same Stage-3 model to render these condition maps into realistic scenes. Across 16K generated samples on our validation set, qualitative results (Figure 8) reveal that, without the editing approach, the Stage-2 model struggles to produce condition maps with sharp, accurate object boundaries (e.g., vehicles and pedestrians) and loses the fine-grained control inherited from the simulator (e.g., specifying the exact height of truck). We report the quantitative results in Table 3 and additionally evaluate the refined condition maps alignment to the simulator condition within the preserved region. Stage-2s editing model, trained with our editing data, constantly outperformed the baseline, showcasing the effectiveness of our pipeline design."
        },
        {
            "title": "5.4 Embodied Agent Training",
            "content": "Real Overall Synthetic Training Set MetaVQA-Synthetic MetaVQA+Dreamland 65.6% 69.5% (+3.9%) 65.7% 68.4% (+2.7%) 65.7% 67.3% (+1.6%) We further demonstrate that the output synthetic data of Dreamland as data augmentation can enable the downstream task of embodied agent training. As presented in MetaVQA [62], synthetic images benefit the learning of situational awareness for general-purpose Vision-Language Models (VLMs) through Visual-QuestionAnswering. To validate improved learning from enriched visual observations through Dreamland, we set up two LoRA [61] finetuning trials for InternVL2-8B [63]. Generated from an identical set of top-down layouts, the first training sets observation comprises only synthetic images rendered from the MetaDrive [58]. In comparison, half of the second training sets images are rendered via Dreamland. Question-answer pairs are generated for corresponding image sets, and the model is fine-tuned on the two equally-sized VQA sets and tested on curated test set. Table 4 shows that the VLM model improves the test set especially on real-image-VQAs. These improvements showcase Dreamlands applicability in complex downstream tasks. Further experiment details will be in Appendix C.5. Table 4: Improved Model Learning with Dreamland. InternVL2-8B, fine-tuned with Dreamland re-rendering, demonstrates significantly improved test accuracy on the curated MetaVQA test set."
        },
        {
            "title": "6 Conclusion",
            "content": "We present Dreamland, hybrid generation framework that, via novel layered world abstraction (LWA), bridges physics-based simulators with large-scale pre-trained generative models to balance precise control and rich visual realism. To facilitate Sim2Real transfer for such systems, we introduce D3Sim, large-scale dataset of paired simulated and real-world driving scenarios. Furthermore, we show Dreamlands strong semantic and geometric control enhances the real-world adaptation of downstream embodied agents. We hope our pipeline design can shed light on Sim2Real generation and more broadly embodied agent learning with generative models. Limitation. Dreamland introduced an additional editing model to the current hybrid pipeline, which led to longer inference time. Also, the high-quality simulator and real-world paired data are expensive to annotate, which may limit the performance of the instructional editing model."
        },
        {
            "title": "References",
            "content": "[1] Llama 2 team. Llama 2: Open foundation and fine-tuned chat models, 2023. 2 [2] Abhimanyu Dubey and et al. The llama 3 herd of models. ArXiv, abs/2407.21783, 2024. [3] Gemma Team. Gemma: Open models based on gemini research and technology, 2024. [4] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023. [5] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In NeurIPS, 2023. [6] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning, 2023. 2 [7] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In IEEE/CVF Computer Vision and Pattern Recognition Conference, 2022. 2, 3 [8] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. SDXL: Improving latent diffusion models for high-resolution image synthesis. In International Conference on Learning Representations, 2024. 21 [9] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. 21 [10] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. In International Conference on Machine Learning, 2022. [11] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In International Conference on Machine Learning, 2021. [12] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022. 2, 3 [13] Sora: Creating video from text openai.com. https://openai.com/index/sora/. [Accessed 30-042025]. 2, 3 [14] Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and Yang You. Open-sora: Democratizing efficient video production for all. arXiv preprint arXiv:2412.20404, 2024. [15] Bin Lin, Yunyang Ge, Xinhua Cheng, Zongjian Li, Bin Zhu, Shaodong Wang, Xianyi He, Yang Ye, Shenghai Yuan, Liuhan Chen, et al. Open-sora plan: Open-source large video generation model. arXiv preprint arXiv:2412.00131, 2024. [16] Xin Ma, Yaohui Wang, Gengyun Jia, Xinyuan Chen, Ziwei Liu, Yuan-Fang Li, Cunjian Chen, and Yu Qiao. Latte: Latent diffusion transformer for video generation. arXiv preprint arXiv:2401.03048, 2024. 2, 3 10 [17] Alan Yu, Ge Yang, Ran Choi, Yajvan Ravan, John Leonard, and Phillip Isola. Learning visual parkour from generated images. In 8th Annual Conference on Robot Learning, 2024. 2, 3 [18] Xinqing Li, Ruiqi Song, Qingyu Xie, Ye Wu, Nanxin Zeng, and Yunfeng Ai. Simworld: unified benchmark for simulator-conditioned scene generation via world model. arXiv preprint arXiv:2503.13952, 2025. 2 [19] Yunsong Zhou, Michael Simon, Zhenghao Mark Peng, Sicheng Mo, Hongzi Zhu, Minyi Guo, and Bolei Zhou. Simgen: Simulator-conditioned driving scene generation. Advances in Neural Information Processing Systems, 37:4883848874, 2024. 2, 5, 6, 7, 18, 21, [20] Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma, Luke Zettlemoyer, and Omer Levy. Transfusion: Predict the next token and diffuse images with one multi-modal model. arXiv preprint arXiv:2408.11039, 2024. 3 [21] Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding and generation. arXiv preprint arXiv:2408.12528, 2024. [22] Weijia Shi, Xiaochuang Han, Chunting Zhou, Weixin Liang, Xi Victoria Lin, Luke Zettlemoyer, and Lili Yu. Llamafusion: Adapting pretrained language models for multimodal generation. arXiv preprint arXiv:2412.15188, 2024. [23] Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Zhengxiong Luo, Yueze Wang, Yongming Rao, Jingjing Liu, Tiejun Huang, and Xinlong Wang. Generative multimodal models are in-context learners. 2023. [24] Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, et al. Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869, 2024. [25] Sicheng Mo, Thao Nguyen, Xun Huang, Siddharth Srinivasan Iyer, Yijun Li, Yuchen Liu, Abhishek Tandon, Eli Shechtman, Krishna Kumar Singh, Yong Jae Lee, Bolei Zhou, and Yuheng Li. X-fusion: Introducing new modality to frozen large language models, 2025. [26] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In International Conference on Computer Vision, 2023. 3, 6 [27] Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jianwei Yang, Jianfeng Gao, Chunyuan Li, and Yong Jae Lee. Gligen: Open-set grounded text-to-image generation. In IEEE/CVF Computer Vision and Pattern Recognition Conference, 2023. [28] Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhongang Qi, Ying Shan, and Xiaohu Qie. T2iadapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. In Association for the Advancement of Artificial Intelligence, 2024. [29] Xuan Ju, Ailing Zeng, Chenchen Zhao, Jianan Wang, Lei Zhang, and Qiang Xu. HumanSD: native skeleton-guided diffusion model for human image generation. In IEEE/CVF Computer Vision and Pattern Recognition Conference, 2023. [30] Sicheng Mo, Fangzhou Mu, Kuan Heng Lin, Yanli Liu, Bochen Guan, Yin Li, and Bolei Zhou. Freecontrol: Training-free spatial control of any text-to-image diffusion model with any condition. In IEEE/CVF Computer Vision and Pattern Recognition Conference, 2024. [31] Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali Dekel. Plug-and-play diffusion features for textdriven image-to-image translation. In IEEE/CVF Computer Vision and Pattern Recognition Conference, pages 19211930, 2023. [32] Kuan Heng Lin, Sicheng Mo, Ben Klingher, Fangzhou Mu, and Bolei Zhou. Ctrl-x: Controlling structure and appearance for text-to-image generation without guidance. Advances in Neural Information Processing Systems, 37:128911128939, 2024. 3 [33] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In IEEE/CVF Computer Vision and Pattern Recognition Conference, 2023. 3 [34] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ip-adapter: Text compatible image prompt adapter for text-to-image diffusion models. arXiv preprint arxiv:2308.06721, 2023. [35] Omri Avrahami, Kfir Aberman, Ohad Fried, Daniel Cohen-Or, and Dani Lischinski. Break-a-scene: Extracting multiple concepts from single image. In ACM Special Interest Group on Computer Graphics and Interactive Techniques Asia, 2023. [36] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Null-text inversion for editing real images using guided diffusion models. In IEEE/CVF Computer Vision and Pattern Recognition Conference, 2023. 3 [37] Tim Brooks, Aleksander Holynski, and Alexei Efros. Instructpix2pix: Learning to follow image editing instructions. In IEEE/CVF Computer Vision and Pattern Recognition Conference, 2023. 3 [38] Chaojie Mao, Jingfeng Zhang, Yulin Pan, Zeyinzi Jiang, Zhen Han, Yu Liu, and Jingren Zhou. Ace++: Instruction-based image creation and editing via context-aware content filling. arXiv preprint arXiv:2501.02487, 2025. 3, 5, 18 [39] Shihao Zhao, Dongdong Chen, Yen-Chun Chen, Jianmin Bao, Shaozhe Hao, Lu Yuan, and Kwan-Yee Wong. Uni-controlnet: All-in-one control to text-to-image diffusion models. In Advances in Neural Information Processing Systems, 2023. [40] Shihao Zhao, Dongdong Chen, Yen-Chun Chen, Jianmin Bao, Shaozhe Hao, Lu Yuan, and KwanYee Wong. Uni-controlnet: All-in-one control to text-to-image diffusion models. Advances in Neural Information Processing Systems, 36:1112711150, 2023. 3 [41] Alexander Swerdlow, Runsheng Xu, and Bolei Zhou. Street-view image generation from birds-eye view layout. IEEE Robotics and Automation Letters, 2024. 3, 21 [42] Kairui Yang, Enhui Ma, Jibin Peng, Qing Guo, Di Lin, and Kaicheng Yu. Bevcontrol: Accurately controlling street-view elements with multi-perspective consistency via bev sketch layout. arXiv preprint arXiv:2308.01661, 2023. 21, 24 [43] Ruiyuan Gao, Kai Chen, Enze Xie, Lanqing Hong, Zhenguo Li, Dit-Yan Yeung, and Qiang Xu. Magicdrive: Street view generation with diverse 3d geometry control, 2024. [44] Ruiyuan Gao, Kai Chen, Bo Xiao, Lanqing Hong, Zhenguo Li, and Qiang Xu. Magicdrivedit: High-resolution long video generation for autonomous driving with adaptive control. arXiv preprint arXiv:2411.13807, 2024. 3 [45] Xiaofeng Wang, Zheng Zhu, Guan Huang, Xinze Chen, and Jiwen Lu. Drivedreamer: Towards real-worlddriven world models for autonomous driving. arXiv preprint arXiv:2309.09777, 2023. [46] Xiaofan Li, Yifu Zhang, and Xiaoqing Ye. Drivingdiffusion: Layout-guided multi-view driving scene video generation with latent diffusion model. arXiv preprint arXiv:2310.07771, 2023. 3, 21, 24 [47] Yuqing Wen, Yucheng Zhao, Yingfei Liu, Fan Jia, Yanhui Wang, Chong Luo, Chi Zhang, Tiancai Wang, Xiaoyan Sun, and Xiangyu Zhang. Panacea: Panoramic and controllable video generation for autonomous driving. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 69026912, 2024. 3, 21, [48] Yifan Lu, Xuanchi Ren, Jiawei Yang, Tianchang Shen, Zhangjie Wu, Jun Gao, Yue Wang, Siheng Chen, Infinicube: Unbounded and controllable dynamic 3d driving scene Mike Chen, Sanja Fidler, et al. generation with world-guided video models. arXiv preprint arXiv:2412.03934, 2024. 3 [49] Wenzhao Zheng, Ruiqi Song, Xianda Guo, Chenming Zhang, and Long Chen. Genad: Generative end-toend autonomous driving. In European Conference on Computer Vision, pages 87104. Springer, 2024. 3 [50] Shenyuan Gao, Jiazhi Yang, Li Chen, Kashyap Chitta, Yihang Qiu, Andreas Geiger, Jun Zhang, and Hongyang Li. Vista: generalizable driving world model with high fidelity and versatile controllability. arXiv preprint arXiv:2405.17398, 2024. [51] Anthony Hu, Lloyd Russell, Hudson Yeo, Zak Murez, George Fedoseev, Alex Kendall, Jamie Shotton, and Gianluca Corrado. Gaia-1: generative world model for autonomous driving, 2023. [52] Seung Wook Kim, Jonah Philion, Antonio Torralba, and Sanja Fidler. Drivegan: Towards controllable high-quality neural simulation, 2021. 3 [53] Ziyang Xie, Zhizheng Liu, Zhenghao Peng, Wayne Wu, and Bolei Zhou. Vid2sim: Realistic and interactive simulation from video for urban navigation. arXiv preprint arXiv:2501.06693, 2025. 3 12 [54] Shaoting Zhu, Linzhan Mou, Derun Li, Baijun Ye, Runhan Huang, and Hang Zhao. Vr-robo: real-tosim-to-real framework for visual robot navigation and locomotion. arXiv preprint arXiv:2502.01536, 2025. [55] Black Forest Labs. Flux. https://github.com/black-forest-labs/flux, 2024. 5, 8, 18, 21 [56] K. Tan et al. H. Caesar, J. Kabzan. Nuplan: closed-loop ml-based planning benchmark for autonomous vehicles. In CVPR ADP3 workshop, 2021. 5, 16 [57] Quanyi Li, Zhenghao Peng, Lan Feng, Zhizheng Liu, Chenda Duan, Wenjie Mo, and Bolei Zhou. Scenarionet: Open-source platform for large-scale traffic scenario simulation and modeling. Advances in Neural Information Processing Systems, 2023. 5, 16, 24 [58] Quanyi Li, Zhenghao Peng, Lan Feng, Qihang Zhang, Zhenghai Xue, and Bolei Zhou. Metadrive: Composing diverse driving scenarios for generalizable reinforcement learning. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022. 5, 7, 9, 16, 24 [59] NVIDIA, :, Niket Agarwal, Arslan Ali, Maciej Bala, Yogesh Balaji, Erik Barker, Tiffany Cai, Prithvijit Chattopadhyay, Yongxin Chen, Yin Cui, Yifan Ding, Daniel Dworakowski, Jiaojiao Fan, Michele Fenzi, Francesco Ferroni, Sanja Fidler, Dieter Fox, Songwei Ge, Yunhao Ge, Jinwei Gu, Siddharth Gururani, Ethan He, Jiahui Huang, Jacob Huffman, Pooya Jannaty, Jingyi Jin, Seung Wook Kim, Gergely Klár, Grace Lam, Shiyi Lan, Laura Leal-Taixe, Anqi Li, Zhaoshuo Li, Chen-Hsuan Lin, Tsung-Yi Lin, Huan Ling, Ming-Yu Liu, Xian Liu, Alice Luo, Qianli Ma, Hanzi Mao, Kaichun Mo, Arsalan Mousavian, Seungjun Nah, Sriharsha Niverty, David Page, Despoina Paschalidou, Zeeshan Patel, Lindsey Pavao, Morteza Ramezanali, Fitsum Reda, Xiaowei Ren, Vasanth Rao Naik Sabavat, Ed Schmerling, Stella Shi, Bartosz Stefaniak, Shitao Tang, Lyne Tchapmi, Przemek Tredak, Wei-Cheng Tseng, Jibin Varghese, Hao Wang, Haoxiang Wang, Heng Wang, Ting-Chun Wang, Fangyin Wei, Xinyue Wei, Jay Zhangjie Wu, Jiashu Xu, Wei Yang, Lin Yen-Chen, Xiaohui Zeng, Yu Zeng, Jing Zhang, Qinsheng Zhang, Yuxuan Zhang, Qingqing Zhao, and Artur Zolkowski. Cosmos world foundation model platform for physical ai, 2025. 6, [60] Wayne Wu, Honglin He, Jack He, Yiran Wang, Chenda Duan, Zhizheng Liu, Quanyi Li, and Bolei Zhou. Metaurban: An embodied ai simulation platform for urban micromobility. International Conference on Learning Representation, 2025. 8 [61] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2022. 8, 9 [62] Weizhen Wang, Chenda Duan, Zhenghao Peng, Yuxin Liu, and Bolei Zhou. Embodied scene understanding for vision language models via metavqa. arXiv preprint arXiv:2501.09167, 2025. 9, 24 [63] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2418524198, 2024. 9, 24 [64] Lihe Yang, Bingyi Kang, Zilong Huang, Zhen Zhao, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything v2. arXiv:2406.09414, 2024. 16, 18 [65] Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose Alvarez, and Ping Luo. Segformer: Simple and efficient design for semantic segmentation with transformers. Advances in neural information processing systems, 34:1207712090, 2021. 16, 18 [66] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. arXiv preprint arXiv:2303.05499, 2023. 16 [67] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Rädle, Chloe Rolland, Laura Gustafson, Eric Mintun, Junting Pan, Kalyan Vasudev Alwala, Nicolas Carion, Chao-Yuan Wu, Ross Girshick, Piotr Dollár, and Christoph Feichtenhofer. Sam 2: Segment anything in images and videos, 2024. 16 [68] Aaron Grattafiori et al. The llama 3 herd of models, 2024. 16 [69] Ozgur Kara, Bariscan Kurtkaya, Hidir Yesiltepe, James M. Rehg, and Pinar Yanardag. Rave: Randomized noise shuffling for fast and consistent video editing with diffusion models. In IEEE/CVF Computer Vision and Pattern Recognition Conference, 2024. 13 [70] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. 18 [71] David Eigen, Christian Puhrsch, and Rob Fergus. Depth map prediction from single image using multi-scale deep network. Advances in neural information processing systems, 27, 2014. 18 [72] Ruiyuan Gao, Kai Chen, Enze Xie, Lanqing Hong, Zhenguo Li, Dit-Yan Yeung, and Qiang Xu. Magicdrive: Street view generation with diverse 3d geometry control. arXiv preprint arXiv:2310.02601, 2023. 21, 24 [73] Holger Caesar, Varun Bankiti, Alex H. Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom. nuscenes: multimodal dataset for autonomous driving. arXiv preprint arXiv:1903.11027, 2019. 21, [74] Alexander Swerdlow, Runsheng Xu, and Bolei Zhou. Street-view image generation from birds-eye view layout. IEEE Robotics and Automation Letters, 2024. 24 [75] Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien Chouard, Vijaysai Patnaik, Paul Tsui, James Guo, Yin Zhou, Yuning Chai, Benjamin Caine, Vijay Vasudevan, Wei Han, Jiquan Ngiam, Hang Zhao, Aleksei Timofeev, Scott Ettinger, Maxim Krivokon, Amy Gao, Aditya Joshi, Yu Zhang, Jonathon Shlens, Zhifeng Chen, and Dragomir Anguelov. Scalability in perception for autonomous driving: Waymo open dataset. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2020."
        },
        {
            "title": "Appendix",
            "content": "A D3Sim Dataset A.1 D3Sim Dataset Construction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.2 D3Sim Training . . A.3 D3Sim Validation . A.4 D3Sim Video . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "B Implementation Details",
            "content": "B.1 Dreamland Implementation Details . . . . . . . . . . . . . . . . . . . . . . . . . . B.2 Dreamland-Video Implementation Details . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "C Experiments",
            "content": "C.1 Evaluation Metrics C.2 More Ablation . . . C.3 Qualitative Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.4 Comparison with Other Baselines . . . . . . . . . . . . . . . . . . . . . . . . . . C.5 Embodied Agent Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 16 16 18 18 18 18 18 19 19 21 24 15 D3Sim Dataset A.1 D3Sim Dataset Construction Data preparation. We construct our D3Sim dataset based on the nuPlan dataset [56], which contains diverse real-world driving scenarios and more than 120 hours of sensor data. To obtain the driving data in the simulation domain, we reconstruct the digital twin of real-world driving scenarios in the MetaDrive simulator [58] using ScenarioNet [57]. This results in scene records corresponding to more than 20,000 digital twin scenarios of 15-20 seconds in length and up to 10 Hz sample rate. To get the conditions (e.g., depth, segmentation) in the simulation domain (Sim Conditions), we replay the ego vehicle trajectory recorded in the scene record in the simulator, and capture the simulated sensor data from the front view camera. To achieve pixel-wise alignment between the digital twins, we calibrated the simulator sensors according to the camera parameters (intrinsics and extrinsics) recorded in the nuPlan dataset for each scenario. Then we capture the Sim Conditions synthesized using Panda3D engine based on OpenGL rendering backend, which includes depth map, semantic map, instance map, rendered RGB, and top-down view. The Sim-LWA is constructed from these Sim Conditions. Annotation. We annotate our digital twin scenarios to obtain realistic conditions (Real Conditions) using pre-trained foundation models. Given realistic front camera view from the nuPlan sensor data, we use DepthAnything2 [64] to obtain the depth map, SegFormer [65] for the segmentation map, GroundingDino [66] + SAM2 [67] for instance map, and Llama 3 [68] for text descriptions. All of the Real Conditions align with the Sim Conditions in the pixel space, which enables us to construct pairwise training data for Sim2Real transfer described below. A.2 D3Sim Training Given the digital driving scenario with Sim and Real conditions, we construct our training dataset for the second and third stages. We first split the conditions into three world layers according to the segmentation mapping in Table A, the visibility mask for each layer is derived as the pixels that belong to the corresponding semantic classes. Then we construct the concatenated conditions along the channel dimension, and construct the Sim-LWA and Real-LWA according to the predefined preserved and editable layers. The whole construction process is shown in Figure A, where we only include the segmentation map as condition for demonstration. World Layer Traffic Participants Ld Map Layout Ll Background Layer Lb Semantic Classes CAR TRUCK BUS PEDESTRIAN BICYCLE MOTORCYCLE ROAD CROSSWALK SIDEWALK FENCE TRAFFIC_LIGHT TRAFFIC_SIGN SKY TERRAIN BUILDING VEGETATION WALL POLE Table A: Semantic mapping for world layers. Figure A: Layered World Abstraction construction pipeline. 16 Data Diversity. To demonstrate the layout diversity of our D3Sim dataset, we visualize the topdown view of various driving scenarios in Figure B, including unprotected cross turn, dense vehicle interactions, pickup/dropoff area, and following vehicle. Figure B: Diverse Scene Layouts from D3Sim. Ego vehicle and other traffic participants are marked with colored rectangles. A.3 D3Sim Validation We derive the D3Sim validation dataset from digital twins to evaluate controllability and visual quality. Despite driving scenarios from the nuPlan dataset reflecting real-world distribution, its complex object layout and interaction result in overlapping and occlusion, which hinders the accurate evaluation of controllability. Therefore, we construct an automated derivation pipeline to obtain scene records with diverse static layouts and unambiguous traffic participants placement. Given scene record, we first filter out occluded and non-visible objects according to the ego agents view. Then we derived different variations of the scene by sampling from the combination of the objects. This process ensures that all objects in the sampled scenarios are clearly visible for controllability evaluation. We also randomly generate multiple diverse prompts for each sample, which are used as the text condition for the world model to evaluate its visual quality and diversity. 17 A.4 D3Sim Video To extend our pipeline to video-based world models, we construct the video dataset with paired Sim and Real video conditions, along with realistic perspective view videos. The dataset contains around 20,000 video clips, each corresponding to 15-20 second driving scenario captured at 10 Hz sample rate. We follow the process described in Sec. A.2 to construct the Sim-LWA, Real-LWA, and the edit mask for each frame, then concatenate the processed frames into videos. These paired videos are used to train the second stage of the Dreamland-Video pipeline."
        },
        {
            "title": "B Implementation Details",
            "content": "B.1 Dreamland Implementation Details We follow ACE++ [38] to train our Stage-2 instructional editing model. Inspired by REVA [69], we vertically concatenate the depth and segmentation maps, each of size 512 288, into single image of shape 512 576. We find that the instructional editing model can accurately generate structures in both the upper and lower conditional maps simultaneously, without any additional loss. We compare this approach with SimGens method, which encodes the segmentation and depth maps into the Red and Blue channels, respectively. Empirically, we find that our approach performs better, as it is more aligned with the generative models pre-training setting. For the Stage-3 conditional generation model, we initialize it from the FLux Depth [55] model and only train the linear projection layer ξ to incorporate additional segmentation maps as conditioning inputs. To achieve this, we increase the input channels of the projection layer from 128 to 192, while keeping the output channel count unchanged. We also experimented with using LoRA to finetune the Stage-3 model; however, we empirically found that this approach tends to overfit to the finetuning dataset and degrade the models world knowledge. Therefore, we only finetune the linear projection layer to preserve world knowledge while maintaining controllability. B.2 Dreamland-Video Implementation Details For the Stage-2 video-editing model, we increase the projection layers input channel number by twice to take additional editing frames and editing masks conditions. Following standard practice in image editing models, we initialize from the pretrained generative model and concatenate the extra conditioning latents with the noise latent along the channel dimension."
        },
        {
            "title": "C Experiments",
            "content": "C.1 Evaluation Metrics We evaluate Dreamland from two key aspects: Generation Quality and Controllability. Generation Quality. For generation quality, we use Fréchet Inception Distance (FID) [70]. It measures the distribution distance of features between generated and original frames in the dataset. The features are extracted using pre-trained Inception model. For quantitative results and comparisons, FID is evaluated on 10,000 samples from DIVA-Real [19] to reflect the real-world driving distribution if not explicitly specified. Controllability. For the quantitative evaluation of adherence to control input conditions, we transform the generated samples into shared representation spaces by applying depth estimation and semantic segmentation operations. We then compare the transformed representation with the original conditions captured from the simulator in the preserved region to measure the alignment between the generated world and the simulation. For depth alignment, we compute the scale-invariant Root Mean Squared Error (si-RMSE) [71] between the depth map from the simulation and transformed by DepthAnythingV2 [64], where lower value represents better alignment. For semantic alignment, we compute the mean Intersection over Union (mIoU) between the segmentation map from the simulation and the generated frames derived using SegFormer [65]. Higher value means better alignment."
        },
        {
            "title": "Methods",
            "content": "Stage3 Input Stage3 Model"
        },
        {
            "title": "Controllability",
            "content": "Dreamland (Variations)"
        },
        {
            "title": "Dreamland",
            "content": "Sim-LWA Sim-LWA Sim-LWA Sim-LWA Real-LWA Real-LWA Real-LWA Real-LWA Frozen SDXL Frozen SD3 Frozen Flux Finetuned Flux Frozen SDXL Frozen SD3 Frozen Flux Finetuned Flux FID 106.80 78.30 76.21 106. 78.99 51.65 45.19 44.61 si-RMSE mIoU 0.806 0.808 0.737 0.641 0.744 0.673 0.640 0.647 0.505 0.638 0.623 0.595 0.699 0.747 0.786 0. Table B: Ablation: Real-LWA improves generation fidelity to real driving scene. Figure C: Ablation: Dreamland with Real-LWA and Sim-LWA. Dreamland using Sim-LWA as the Stage-3 input generates driving scenes with depth identical to the MetaDrive conditions, but loses fidelity to real driving scenes. C.2 More Ablation Scalability. Dreamlands simple pipeline design unlocks the potential of better image generation capability and controllability by upgrading the Stage-3 model to stronger image generative foundation models. Support by Table 1, Table D, and user study results in Table C, Dreamland could constantly combine with state-of-the-art conditional generation models to enhance the existing pipeline. Real-LWA. We ablate the effectiveness of our Stage-2 model design by forwarding Stage-3 with either Sim-LWA or Real-LWA . As illustrated in Table B, using Real-LWA consistently improves the FID, indicating that the generated content achieves better fidelity to real driving scenes. We also observe similar results in experiments on the nuScenes dataset, as recorded in Table D. These observations highlight the necessity of using Real-LWA in hybrid pipeline for scene creation. C.3 Qualitative Results Alignment to text prompts. Dreamland is capable of rendering the simulator into realistic frames while taking user-provided text prompts. As shown in Figure D, Dreamland can synthesize realistic scenes with different locations, weather, times, or style, demonstrating world knowledge preservation. 19 Figure D: Text-grounded scene creation with Dreamland. Dreamland could create diverse and visually realistic driving scenes by preserving the world knowledge in the pre-trained generative models. 20 Image Quality Controllability Baseline Ours Better Opponent Better Both Good Both Bad Ours Better Opponent Better Both Good Both Bad Dreamland w/ Sim-LWA SimGen Dreamland (Frozen SDXL) Dreamland (Frozen SD3) Dreamland (Frozen Flux) 36.6 95.4 90.9 83.4 33. 32.0 0.6 0.6 4.6 44.0 17.1 0.6 3.4 6.3 14.3 14.3 3.4 5.1 5.7 8. 52.6 71.4 73.7 86.9 46.3 18.9 10.3 9.1 1.7 31.4 24.6 6.3 10.3 1.1 12. 4.0 12.0 6.9 10.3 9.7 Table C: User Study. We study the user preference by comparing Dreamland and its baseline methods. The result suggests that Dreamland achieved strong visual generation quality and outperforms all other baselines in alignment with simulator conditions. Alignment to simulator conditions. We show more qualitative results of the Dreamland in Figure E. Dreamland can re-render complex driving scenes into realistic frames with preserved scene layout. Flexible simulator control with LWA. Dreamlands LWA design can dynamically divide the simulator world into preserved and editable regions based on user instructions. This allows users to control specific vehicles or road layouts by adding them to or removing them from the preservation region. In Figure F, we show that Dreamlands output closely aligns with the user-defined Sim-LWA and effectively respects the user-specified editable regions, indicating that fine-grained simulator control can be achieved through this hybrid approach. User study. To fully evaluate our method, we conducted survey to study user preference between Dreamland and its baseline methods and variations. We randomly selected 50 samples from our D3Sim validation dataset and compared the generated figures between ours and baseline method regarding image quality and simulator controllability. Users are asked to choose from four options, including: \"Image is better\", \"Image is better\", \"Both are equally good\", and \"Both are equally bad\". We recruited 25 users, each of whom evaluated 7 different samples, resulting in total of 175 user votes for each comparison between our method and the baseline. Figure shows our survey template and Table reports the user preference between our methods and selected baselines. In the comparison between Dreamland and Dreamland use Sim-LWA as the input for Stage-3, noted as Dreamland w/ Sim-LWA, using Real-LWA leads to 4.6 and 33.7 absolute percentage in image quality and controllability, showcasing the importance of our Stage-2 model. It is also worth noting that Dreamland largely outperforms previous state-of-the-art method (SimGen [19]), achieving 95.4% user preference in image quality and 71.4% in simulator controllability. C.4 Comparison with Other Baselines Experimental Setup. We compare Dreamland with previous generative models for driving scene generation, including BEVGen [41], BEVControl [42], MagicControl [72], Panacea [47], DrivingDiffusion [46], and SimGen [19]. SimGen shares the same setup as ours, employing simulator to first render scene records and then using generative model to re-render the simulator frames. The other baselines directly encode driving scene maps and generate driving scenes based on them. We conduct the experiment using the 6K spatial conditions from the nuScenes validation dataset and compute the FID with source images from nuScenes or DIVA-Real. nuScenes FID Assessment of Generative Models. In recent years, it has been common practice for generative models in autonomous driving tasks to report FID scores based on natural driving images, such as nuScenes dataset [73]. However, recent large-scale foundation models [8, 9, 55] are often pre-trained on datasets rich in high-quality, visually aesthetic images. This leads them to generate stylized, artistic outputs that differ from naturally captured driving scenes. As result, evaluating generative models for autonomous driving using nuScenes FID has become more and more questionable. Zhou et al. [19] observed that generative models trained on the DIVA-Real dataset demonstrate noticeably improved visual fidelity. Therefore, we compute FID using DIVA-Real, since it captures high-quality, aesthetically realistic driving scenes aligned with modern generative models. While previous state-of-the-art models achieve low FID scores on nuScenes, their FID scores on DIVA-Real are significantly higher. For example, both Panacea [47] and SimGen [19] achieve nuScenes FID scores around 16, but their FID scores on DIVA-Real exceed 60. We validate the effectiveness of using FID on DIVA-Real through human evaluation study, with results reported in Figure E: Rendering diverse appearances aligned to the simulators conditions. Dreamland can re-render complex driving scenes into realistic frames with preserved scene layout. 22 Figure F: Flexible control with LWA. Top: Visualization of Sim-LWA, where layers/objects not under user control are made transparent. Bottom: Dreamland output. Users can choose to control specific vehicles or the road layout by adding them to or removing them from the preservation region. We use red bounding box to highlight the small car that the user prefers to control in both the LWA and Dreamland outputs. Figure G: Screenshot of user study templates. Users are asked to select between \"Image is better\", \"Image is better\", \"Both are equally good\" , and \"Both are equally bad\" options. We randomly shuffle the order of images in the user study. Table C. In our user study, 95.4% of participants agreed that Dreamland produces higher visual quality compared to SimGen, and over 80% preferred Dreamland over the Dreamland variants using SDXL and SD3. This aligns with the FID scores on DIVA-Real, where Dreamland significantly outperforms the aforementioned methods. It is also worth noting that when Dreamland and Dreamland (Frozen Flux) achieve similar FID scores (44.61 vs. 45.19), human preference is also consistent with the scores, with winning rates of 33.7% and 44.0%, respectively. Quantitative results. Table presents comparison between Dreamland and previous state-of-theart models for autonomous driving tasks. Under the actual inference setting, Dreamland achieves the lowest FID on DIVA-Real (44.61), outperforming the previous best method, MagicDrive, by 12.8%, demonstrating the strong capability of Dreamland in generating realistic driving scenes. Notably,"
        },
        {
            "title": "Method",
            "content": "BEVGen [74] BEVControl [42] MagicDrive [72] Panacea [47] DrivingDiffusion [46] SimGen [19] Dreamland (Variations)"
        },
        {
            "title": "Dreamland",
            "content": "Stage3 Input Stage3 Model"
        },
        {
            "title": "GT Conditions\nGT Conditions\nGT Conditions\nGT Conditions",
            "content": "Frozen SDXL Frozen SD3 Frozen Flux Finetuned Flux Sim-LWA Sim-LWA Sim-LWA Sim-LWA Real-LWA Real-LWA Real-LWA Real-LWA Frozen SDXL Frozen SD3 Frozen Flux Finetuned Flux Frozen SDXL Frozen SD3 Frozen Flux Finetuned Flux"
        },
        {
            "title": "Image Quality",
            "content": "FID - nuScenes FID - DIVA Real 25.50 24.90 16.60 16.96 15.90 15.60 54.20 43.68 43.22 42.36 93.66 55.49 52.66 51.35 68.69 48.73 46.00 47.93 - - 51.20 61.83 - 68.20 65.47 50.01 44.63 42.27 100.57 59.92 51.68 52.47 78.99 51.65 45.19 44.61 Table D: Quantitative comparison with baseline methods. All methods are evaluated using the spatial conditions from the nuScenes validation dataset, and the FID is computed with source images from nuScenes or DIVA-Real. For existing baselines, we use the reported FID on the nuScenes dataset from their respective publications, and compute the FID on DIVA-Real when open-source checkpoints are available. We mark scores reproduced by us with an asterisk (*). Dreamland using ground truth condition maps (e.g., depth and segmentation maps) achieves similar FID score to Dreamland with Real-LWA , highlighting the effectiveness of our LWA design. C.5 Embodied Agent Training We explain the experimental setting in greater detail in this section. To curate the two training sets for this task mentioned before, we use shared set of traffic scenarios leveraging the Scenarionet [57] data platform. These scenarios are rendered with both the MetaDrive [58] simulator and Dreamland(see Figure for illustration). For each scenario, we generate questions using the MetaVQA [62] annotation tools, and randomly sample 1000 VQA tuples for each observation domain. We use the InternVL2 [63] codebase for the LoRA fine-tuning of pre-trained InternVL2-8 B. We train the model for two complete epochs and evaluate the models performance on test set, which is generated using nuScenes [73] and Waymo [75] datasets. The test set also comprises real images(from nuScenes) and simulator-rendered images(from Waymo) for holistic evaluation of the learned model under different visual domains. The answer parsing and metrics calculation directly use the toolkit provided by the MetaVQA codebase. Refer to the main paper for our results. 24 (a) MetaDrive-rendered Observation. (b) Dreamland-rendered Observation. Figure H: Example questions. we perform action KEEP_STRAIGHT\" for 1.0 seconds. <2>, provided that it remains still? Select the best option from: (B) No. The answer is (B) Suppose our current speed is slow(0-10 mph), and Will we run into object (A) Yes;"
        }
    ],
    "affiliations": [
        "University of California, Los Angeles"
    ]
}