{
    "paper_title": "REARANK: Reasoning Re-ranking Agent via Reinforcement Learning",
    "authors": [
        "Le Zhang",
        "Bo Wang",
        "Xipeng Qiu",
        "Siva Reddy",
        "Aishwarya Agrawal"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present REARANK, a large language model (LLM)-based listwise reasoning reranking agent. REARANK explicitly reasons before reranking, significantly improving both performance and interpretability. Leveraging reinforcement learning and data augmentation, REARANK achieves substantial improvements over baseline models across popular information retrieval benchmarks, notably requiring only 179 annotated samples. Built on top of Qwen2.5-7B, our REARANK-7B demonstrates performance comparable to GPT-4 on both in-domain and out-of-domain benchmarks and even surpasses GPT-4 on reasoning-intensive BRIGHT benchmarks. These results underscore the effectiveness of our approach and highlight how reinforcement learning can enhance LLM reasoning capabilities in reranking."
        },
        {
            "title": "Start",
            "content": "REARANK: Reasoning Re-ranking Agent via Reinforcement Learning Le Zhang1,2* Bo Wang3 Xipeng Qiu3 Siva Reddy1,4,5 Aishwarya Agrawal1,2,5 1Mila - Quebec AI Institute 2Université de Montréal 3Fudan University 4McGill University 5Canada CIFAR AI Chair 5 2 0 2 6 2 ] . [ 1 6 4 0 0 2 . 5 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "We present REARANK, large language model (LLM)-based listwise reasoning reranking agent. REARANK explicitly reasons before reranking, significantly improving both performance and interpretability. Leveraging reinforcement learning and data augmentation, REARANK achieves substantial improvements over baseline models across popular information retrieval benchmarks, notably requiring only 179 annotated samples. Built on top of Qwen2.5-7B, our REARANK-7B demonstrates performance comparable to GPT-4 on both indomain and out-of-domain benchmarks and even surpasses GPT-4 on reasoning-intensive BRIGHT benchmarks. These results underscore the effectiveness of our approach and highlight how reinforcement learning can enhance LLM reasoning capabilities in reranking. The code is available https://github.com/ lezhang7/Rearank."
        },
        {
            "title": "Introduction",
            "content": "Information retrieval (IR) is core building block of intelligent systems, providing the foundation for accessing, organizing, and reasoning over information. Modern IR systems (Reimers and Gurevych, 2019, 2020; Wang et al., 2021; Formal et al., 2021) typically follow two-stage approach: initial retrieval (e.g., fast lexical methods (Robertson et al., 2009)) to gather candidates, followed by reranking for fine-grained prioritization of relevant results. This two-stage process is particularly crucial for Retrieval-Augmented Generation systems (Lewis et al., 2020; Borgeaud et al., 2022; Zhang et al., 2023), where accurate retrieval and effective reranking of context passages significantly impact generated output quality. Recent advances in large language models (LLMs) have shown strong promise for this reranking phase (Sun et al., 2023), particularly through *equal contribution 1 Figure 1: (Top) Average rerank results on popular benchmarks (over BM25 top 100), the performance improves with RL training; (Bottom) REARANK inference example. The agent provides the reasoning and final ranking of all passages, unlike current agents (Sun et al., 2023; Pradeep et al., 2023b) that only output the final answer. their use as reranking agents that rely on direct outputs rather than internal states (e.g. logits) . This paradigm, exemplified by zero-shot prompting methods (Sun et al., 2023; Ma et al., 2023; Zhuang et al., 2024), offers significant deployment flexibility, especially in model-as-a-service scenarios. However, effectively adapting LLMs specifically as reranking agents presents several key challenges: (i) LLMs are not inherently optimized for ranking objectives, and crucially, current zero-shot methods do not learn from the interaction signals generated during the reranking process; (ii) Achieving competitive performance often necessitates supervised fine-tuning, process severely constrained by the scarcity and high cost of acquiring highquality labeled ranking data (Sun et al., 2023); (iii) The decision-making processes within these models frequently lack transparent and interpretable reasoning, which limits explainability and fails to leverage test-time scaling properties of LLMs; (iv) State-of-the-art reranking agents frequently depend on large, often proprietary models (e.g., GPT-4) or face significant challenges in local deployment. This dependency on large models incurs substantial computational costs and significant inference latency (e.g., reranking 20 passages with reasoning process using DeepSeek-R1 (Guo et al., 2025) via API takes around 90-120 seconds). In this work, we propose REARANK, the first reasoning listwise reranking agent. REARANKs optimization for listwise reranking with explicit reasoning is incentivized using reinforcement learning (RL). This approach effectively leverages the rich, order-based signals inherent in listwise reranking. To address the significant data scarcity challenge typically associated with training listwise models, we introduce data augmentation pipeline capable of generating extensive listwise ranking sets from remarkably small seed of only 179 annotated queries. At inference, REARANK generates explicit, interpretable reasoning for each ranking step, as shown in fig. 1, enabling test-time scaling. Built upon the principle of injecting strong reasoning capabilities into compact model, REARANK operates with low operational cost. The combination of its practical model size and listwise reranking strategy enhances inference efficiency by minimizing LLM calls, thereby facilitating local deployment. Our experimental results demonstrate REARANKs effectiveness: it significantly surpasses baseline models and achieves performance comparable to strong models like GPT-4 and the recent powerful reasoning model Qwen3-32B on standard and out-of-domain benchmarks. Notably, REARANK even surpasses strong GPT-4 performance on reasoning-intensive tasks, highlighting its advantages in combining reasoning with practical efficiency, as summarized in fig. 1. Our main contributions are threefold: (i) We introduce REARANK, novel reasoning-based reranking agent based on the listwise reranking strategy that effectively integrates explicit reasoning capabilities into the reranking process. We formularize the reranking problem in RL framework, and propose data synthesis method requiring only 179 annotated queries, and new reward model leveraging ranking information for training, enabling efficient RL training of REARANK; and (ii) REARANK achieves significant performance improvements over strong baselines and matches or surpasses results from competitive models like GPT-4 and strong reasoning model Qwen3, particularly on reasoning-intensive tasks, while offering substantially improved inference efficiency due to its compact model size. (iii) We provide comprehensive analysis on reasoning transferability and examining the relationship between reasoning length and final ranking performance to better understand the role of reasoning in reranking."
        },
        {
            "title": "2 Related Work",
            "content": "Large Reasoning Models Recent advancements in LLMs have yielded increasingly sophisticated reasoning capabilities, often emerging with scale (Wei et al., 2022a; Kojima et al., 2022). Techniques like Chain-of-Thought prompting (Wei et al., 2022b) and its variants (Kojima et al., 2022; Wang et al., 2022) further enhance these skills by enabling explicit reasoning processes. Beyond prompting, training methods like RL are used to incentivize long0 CoT reasoning; models such as Deepseek-R1 (Guo et al., 2025), OpenAI o1, and o3 leverage RL for enhanced reasoning, showing general task improvements. These developments enable LLM application in complex domains like math problems and planning. Our work applies these advanced reasoning capabilities to reranking. LLMs for Re-ranking LLMs are increasingly being used for reranking, moving beyond traditional feature-based models (Zhang et al., 2024b; Lee et al., 2024; Ma et al., 2024a; BehnamGhader et al., 2024; Sachan et al., 2022). Approaches range from pointwise (Liang et al., 2022), pairwise (Qin et al., 2023), setwise (Zhuang et al., 2024) to listwise methods (Ma et al., 2023; Sun et al., 2023; Pradeep et al., 2023b). While few-shot/zero-shot prompting was explored, supervised fine-tuning on ranking data shows further gains (Pradeep et al., 2023b,a). Building on this, our work proposes novel RL approach without cold start for training listwise LLM reranker, focusing on robust performance and reasoning capabilities, especially in out-of-domain and reasoning-intensive scenarios. concurrent work (Zhuang et al., 2025) also trains an LLM as setwise reranker using RL. It simplifies the task to finding the single most relevant passage index, relying on sparse textmatching binary reward signal. This signal lacks the rich, order-based information present in listwise ranking, which consequently necessitates extensive 2 training data. Furthermore, setwise inference is highly inefficient as it ranks only one passage at time, unlike our listwise method which reranks an entire passages simultaneously as shown in fig. 2. Therefore, we adopt the listwise reranking strategy. Figure 2: Listwise vs. Setwise Reranking. Setwise reranking yields binary scores (0 or 1); listwise reranking offers richer, continuous scores between 0 and 1."
        },
        {
            "title": "3 Method",
            "content": "3.1 Listwise Re-ranking Agent Given query and an initial set of retrieved passages = (p1, . . . , pn), the objective of reranking is to find the optimal permutation (ranking) of these passages. This can be formally expressed as maximizing ranking quality score: max σKn r({pσ(1), pσ(2), . . . , pσ(n)}), (1) where Kn is the set of all possible permutations of the passages , σ represents specific permutation (a ranking), pσ(i) is the passage located at rank in the ranking defined by σ, and is scoring function that measures the quality of the entire ranked list. Listwise reranking methods (Sun et al., 2023; Ma et al., 2024b; Pradeep et al., 2023b) reorder subsets of passages using sliding window. This is necessary due to the limited context length of LLMs, preventing simultaneous processing of all passage at time. Given an initial ranking τ over passages , an LLM-based permutation function (see fig. 2) is applied to window of size starting at index to determine the reordering within the final ranking σ based on relevance to q: {pσ(i), . . . , pσ(i+w1)}, =h({pτ (i), . . . , pτ (i+w1)}, q), (2) The final top-k list is constructed by iteratively applying using sliding window that processes the whole passages list, often from the end towards the beginning. This window is typically shifted by 3 w/2 steps to create overlap, resulting in approximately O(2n/w) total LLM calls for passages and offering significant efficiency advantages. 3.2 RL for Listwise Re-ranking common mathematical framework for reinforcement learning is the Markov Decision Process (MDP), formally defined as tuple (S, A, T, r, γ). Here, represents the state space, is the action space, : [0, 1] denotes transition probabilities, : is the reward function, and γ [0, 1) is the discount factor. In the context of passage reranking, we model the process as an MDP where the agent is our LLM policy πθ. The environment is defined by the query and an initial ranking τ over set of passages = (p1, . . . , pn). The state space is defined by the current ranking of passages and the query, specifically = ({pτ (1), . . . , pτ (n)}, q). The action space corresponds to the set of possible permutation functions that the LLM can apply to the current state. The transition function models how actions lead to new states (rankings). The reward function quantifies the quality of reranking action based on relevance metrics, providing feedback to the agent. We train πθ, an LLM fine-tuned to generate an output sequence consists of reasoning process and new permutation σθ based on the input = ({pτ (1), . . . , pτ (n)}, q). The learning objective is to maximize the expected reward: θ = arg max θ E(q,P )D[r(σθ)], where is the data distribution. Inspired by DeepSeek-R1 (Guo et al., 2025), we employ the simple Grouped Policy Optimization (GRPO) algorithm. Training involves sampling group of output sequences = {o1, o2, . . . , oG} for each input with the system prompt. rulebased reward ri is computed for each output sequence oi and normalized within the group to yield advantages ˆAi. Following DeepSeek-Maths (Shao et al., 2024) approach using current policy samples, the token-level objective is: JGRPO(θ) = 1 G (cid:88) i=1 1 oi oi (cid:88) t=1 li,t, (3) Figure 3: Pipeline of the proposed GRPO-based RL framework for listwise passage reranking. Training utilizes data generated by sampling multiple passage sets per query and evaluating them with consistent relevance judgments. where li,t = πθ (oi,t x, oi,<t) [πθ (oi,t x, oi,<t)]nograd ˆAi,t βDKL [πθ( x, oi,<t)πref ( x, oi,<t)] . (4) Here, is the number of output sequences sampled from current policy πθ given input x, oi is sequence length, and li,t is the per-token loss. πθ(oi,t x, oi,<t) is the policy probability for token oi,t given and preceding tokens oi,<t, with []nograd denoting gradient detachment. The token-wise advantage ˆAi,t is derived from outcome supervision, calculated as the Z-score of the instance reward ri relative to the batch rewards r: ˆAi,t = rimean(r) . The KL penalty, using fixed reference policy πref and coefficient β, encourages stability. The algorithm pipeline is shown in fig. 3. std(r) 3.3 Reward Design The reward signal guides the RL agent by evaluating the quality of its generated output sequence. The output sequence Gi includes structured components for reasoning (<think>...<think>) and ranking (<answer>...<answer>), as illustrated in fig. 1. The total reward is composite signal designed to encourage both high ranking performance and adherence to the desired output format. The primary reward signal is based on the rich, order-based information inherent in listwise reranking, measured by Normalized Discounted Cumulative Gain (NDCG). Considering LLM context limits, we use NDCG@10 to evaluate the ranking of top-10 passages. For generated permutation {pσ(1), pσ(2), . . . , pσ(n)} for q, its score is rrerank = NDCG@10 = DCG@10 IDCG@10 , where DCG@10 = relideal log2(i+1) and IDCG@10 = (cid:80)10 (cid:80)10 log2(i+1) . i=1 Here, each query has an associated relevance judgements including set of passages with scores annotated by human expert, reli is the relevance score reli i= from the annotated relevance judgments for pσ(i) in the generated permutation, and relideal is the ideal relevance score from the relevance judgments for the passage that would be at rank in the ideal ranking for that query. Since the maximum possible NDCG@10 can vary depending on whether the randomly sampled passage set contains relevant documents, we define the ranking reward rrank as relative improvement score. This approach uses min-max normalization to normalize for the differences in scales of reward scores between two different candidate sets and reduces reward variance (Greensmith et al., 2004; Schulman et al., 2015): rrank = rrerank rinit rinit , (5) where = maxσKn r({pσ(1), pσ(2), . . . , pσ(n)}) is the best achievable NDCG@10 for that specific passages set ; rinit = r({pτ (1), pτ (2), . . . , pτ (n)}) is the NDCG@10 of the initial ranking τ over . We also incorporate format rewards to encourage the desired output structure. reward rformat1 = 1 is given if both <think> and <answer> tags are present in the output sequence. reward rformat2 = 1 is given if the content within the <answer> tags follows the specified ranking list format (e.g., [3] > [1] >[2]). The final reward for generated sequence is weighted sum of these components: = 0.8 rrank + 0.1 rformat1 + 0.1 rformat2 (6) 3.4 Initial State Expansion Training effective listwise rerankers faces primary challenge in the scarcity of high-quality training data, defined as query paired with relevance judgments. To address this, we introduce multisampling data augmentation method. Utilizing small dataset of 179 queries from MSMARCO-V2 with fine-grained relevance judgments (0-3), we 4 randomly sample multiple diverse sets of candidate passages for each query from its BM25 top 100 retrieval results. The core idea is to evaluate new ranking {pσ(1), . . . , pσ(n)} produced from these varied initial ranking τ over diverse using the same set of relevance judgments. This multi-sampling approach shares conceptual similarities with negative sampling in contrastive learning (Xiong et al., 2020; Zhang et al., 2024a,c; BehnamGhader et al., 2024; Lee et al., 2024). By sampling varied initial passage sets and intial ranking, we effectively generate diverse ranking scenarios for given query, allowing the model to learn robustly from wider range of non-ideal inputs. This method generates rich listwise ranking data from limited annotations, enabling the model to learn robustly from diverse initial conditions and significantly reducing the need for large-scale, fully annotated query sets."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Experimental Setup Training Details Training data instances are generated by randomly sampling 20 candidate passages per query, repeated 50 times for each of the 179 queries. Samples without any passages marked as relevant (with score = 0) in the relevance judgments or the initial nDCG@10 < 0.1 are filtered out, resulting in 12k training instances. We choose Qwen2.5-7B-Instruct as our baseline model. Training is conducted using the VeRL (Sheng et al., 2024) framework with batch size of 128 and 32 rollouts per step. We trained the model directly via RL, without an initial SFT phase. The training runs for over 160 steps across 8 H100 GPUs. Sliding window reranking Our REARANK employs sliding window to get top-k passages as introduced in section 3.1. Following common practice (Sun et al., 2023; Zhuang et al., 2025; Zhang et al., 2024b), we retrieve the top 100 passages using BM25 with plain query (n = 100). We set the window size to = 20, resulting in 10 LLM calls per query (2 100/20 = 10) to get top-10 passages. Baselines To evaluate the effectiveness of REARANK, we compare it against several strong baselines representing different reranking paradigms. Our zero-shot baselines include powerful large language models: Qwen2.5-7B (Yang et al., 2024) and GPT-4 (Achiam et al., 2023). We also incorporate the state-of-the-art open-source reasoning language model Qwen3-32B (Yang et al., 2025) and Qwen3-235B-A22B, which claim to surpass 671B Deepseek-R1. We adapt these models using the same sliding window strategy and prompt as REARANK, naming them RankQwen and RankGPT, respectively. As supervised fine-tuning (SFT) baseline, we include RankZephyr (Pradeep et al., 2023b), which is distilled on 105k synthetic ranking data sourced from RankGPT. Furthermore, to compare different RL training strategies, we include Rank-R1 (Zhuang et al., 2024), concurrent LLM reranker using the same base model (Qwen2.5-7B) but adopting setwise reranking strategy. This provides valuable insights into the differences between the listwise (used by REARANK) and setwise RL approaches for reranking. The prompts are shown at app. A. evaluate Benchmarks To our thoroughly performance reasoning-enhanced rerankers and generalization, we select three distinct benchmark suites. We evaluate on the in-domain TREC-DL19 (Craswell et al., 2020a) and DL20 (Craswell et al., 2020b) datasets, both derived from MS-MARCO-V1. For assessing out-of-domain (OOD) generalization, we use BEIR (Thakur et al., 2021), diverse collection from sources outside of MS-MARCO. Crucially, real-world information retrieval tasks often demand capabilities beyond simple keyword matching or semantic similarity, requiring deeper reasoning to understand complex relationships or logic within the content and the query. To systematically evaluate our reasoning-enhanced REARANK, we utilize the BRIGHT (Su et al., 2024) benchmark, which is specifically designed to test reasoning abilities in retrieval contexts. For all evaluations, we report the nDCG@10 as the evaluation metric. 4.2 In-domain & OOD Retrieval Results As shown in table 1, GPT-4 achieves the best performance across benchmarks, due to its superior text understanding. The Qwen2.5-7B also performs strongly, surpassing the legacy GPT-3.5 on both sets. Our REARANK, based on Qwen2.5-7B trained via our RL approach with reasoning ability, demonstrates performance closely comparable to GPT-4. This suggests the significant potential of advanced reasoning capabilities learned via RL for reranking. REARANK achieves considerable improvements over the RankQwen2.5-7B baseline: significant 5 Model #Train In-Domain Out-of-Domain DL19 DL20 Covid NFCorpus DBPedia SciFact Signal News Robust04 BEIR (Avg) BM25 RankQwen2.5-7B RankGPT3.5 RankGPT4 RankZephyr-7B - 50.58 zero-shot 68.25 zero-shot 65.80 75.59 zero-shot 105K SFT 73. 47.96 62.73 62.91 70.56 70.60 59.47 77.74 76.67 85.51 83.54 30.75 37.40 35.62 38.47 38.38 31.80 39.83 44.47 47.12 44.34 Reasoning Language Model RankQwen3-32B RankQwen3-235B Rank-R1-7B REARANK-7B over baseline* zero-shot zero-shot 72k RL 179 RL 83.86 70.00 73.13 83.68 69.37 71.94 83.12 68.50 72.70 74.16 81.28 70.00 +5.91 +7.27 +3.54 36.28 35.64 35.97 35.20 -2.20 45.44 41.33 43.43 45.23 +5.40 67.89 70.83 70.43 74.95 75. 71.78 63.30 74.47 75.02 +4.19 33.05 31.73 32.10 34.40 31.44 32.06 32.53 32.16 36.00 +4.27 39.52 43.24 48.85 52.89 52.35 40.70 49.95 50.62 57.55 54.19 57.27 51.73 58.16 50.79 55.17 48.43 51.88 57.49 +8.64 +7. 43.31 50.10 51.25 55.84 54.20 54.06 52.20 53.25 54.59 +4.49 Table 1: Reranking Agent Results (nDCG@10) on TREC-DL and BEIR Benchmarks. denotes initial BM25 retrieval performance. RankGPT4 reranks top 30 passages from RankGPT3.5. All other models rerank on the BM25 top 100 passages. Training data size indicates the number of annotated queries used. *Baseline is RankQwen2.5-7B 6.5% improvement in nDCG@10 on the in-domain benchmarks and notable 4.5% improvement in nDCG@10 on the OOD benchmark. These substantial gains are particularly impressive given that they are achieved using only 179 annotated queries (used for RL training). Comparing against SFT RankZephyr-7B, REARANK-7B demonstrates comparable performance on TREC-DL while achieving superior results in BEIR. This finding suggests that while SFT can be effective for in-domain data, our RL approach may offer enhanced robustness and better generalization capabilities when applied to out-of-domain tasks. Evaluating against powerful reasoning language models, including the large Qwen3 and the concurrent Setwise Rank-R1, reveals significant strengths of REARANK. Despite being trained on dataset of only 179 queries (0.2% of Rank-R1s reported data), REARANK-7B surpasses Setwise Rank-R1 across benchmarks. Furthermore, REARANK-7Bs performance exceeds both Qwen3-32B and Qwen3235B. REARANK-7B achieves superior reranking performance with less training data and more compact size than state-of-the-art methods. Interestingly, Qwen3-32B surpasses Qwen3-235B. Our investigation, consistent with (Marjanovic et al., 2025), suggests Qwen3-235Bs excessive self-reflection (marked by \"wait\") leads to confusion and degraded performance. 4.3 Reasoning-intensive Retrieval Results Table 2 presents performance results on the reasoning-intensive BRIGHT benchmark. Notably, REARANK-7B even outperforms the powerful GPT4 model on this benchmark, highlighting its strong reasoning capabilities developed through RL training. The SFT method, RankZephyr-7B, performs considerably poorer on BRIGHT, falling below the lexical-based BM25 baseline. This reinforces our observation that SFT generalizes poorly to out-ofdomain and reasoning-intensive scenarios. Comparing with the concurrent Setwise RankR1, our listwise REARANK-7B achieves better performance on the BRIGHT benchmark also. This superior performance, particularly against another RL-trained model, underscores the effectiveness of our approach. Furthermore, while REARANK-7B is smaller and does not undergo general reasoning training (e.g. math, coding, agent) like Qwen3, its performance remains closely comparable. This near parity, despite inherent disadvantages, further confirms our methods effectiveness. These strong results on reasoning-intensive task against competitive LLM baselines (GPT-4, Rank-R1) highlight the efficacy of REARANKs design. Its RL framework, trained on limited highquality dataset via our synthesis pipeline, enables learning complex reranking strategies. The listwise nature provides richer signal for robust ranking compared to setwise methods, while also offering inference efficiency (fewer LLM calls). 4.4 Ablation Studies To quantify the effectiveness of individual components within REARANK, we conducted ablation studies, summarized in table 3. Our first investigation examined the effect of applying the REARANKs reasoning prompt directly to Qwen2.57B. While Qwen2.5-7B shows marginal improvement with the reasoning prompt over its zero-shot baseline, it significantly underperforms REARANK. This suggests that prompting alone is insufficient for eliciting robust reasoning in this context. We then investigate the impact of data filtering. 6 Model #Train StackExchange Coding Theorem-based Bio. Earth. Econ. Psy. Rob. Stack. Sus. Pony LC. AoPS TheoT. TheoQ. BM25 RankQwen2.5-7B RankGPT4 RankZephyr-7B 18.2 - 22.7 zero-shot zero-shot 33.8 105K SFT 21.9 27.9 25.8 34.2 23. 16.4 14.6 16.7 14.4 13.4 18.7 27.0 10.3 10.9 14.2 22.3 7.6 16.3 11.7 27.7 13.7 16.1 21.4 11.1 16.6 4.3 5.3 15.6 6. 24.7 23.9 3.4 24.7 6.5 6.0 1.2 6.8 RankQwen3-32B RankQwen3-235B Rank-R1-7B REARANK-7B over baseline* zero-shot zero-shot 72k RL 179 RL 29.4 24.9 26.7 26.4 28.5 26.0 23.4 27.4 +0.8 +1. Reasoning Language Model 18.3 25.7 18.8 26.3 19.1 24.2 24.2 17.4 +5.5 +3.3 16.0 17.0 10.4 16.3 +4.6 20.9 22.1 17.2 18.5 +4.0 7.6 8.2 4.3 8.0 23.2 24.9 24.2 25.1 +3.7 +2.7 7.8 7.7 4.3 7. 27.6 27.2 19.8 27.0 +3.0 +1.4 2.1 7.4 8.6 2.0 8.9 11.7 10.9 9.5 +2.1 7.3 7.9 0.2 7.3 8.4 8.6 8.3 7.9 0.0 Avg. 13.7 15.0 16.8 13.0 18.2 18.8 16.4 17.7 +2.7 Table 2: Reranking Agent Results (nDCG@10) on BRIGHT . represent initial retrieval; all other models show reranking performance on the top 100 BM25 results. *Baseline is RankQwen2.5-7B Model Variant TREC-DL BEIR BRIGHT robust and reasoning-capable reranking agent. Qwen2.5-7B (baseline) +Reasoning Prompt wo/ Filter nDCG@10 < 0.1 w/ rrank = Srerank w/ rrank = Srerank Sinit w/ direct SFT REARANK-7B (full model) 65.5 65.9 71.3 70.9 71.5 66.7 72. 50.1 51.4 53.6 53.2 54.0 50.7 54.6 15.0 15.4 16.9 16.7 17.2 14.7 17. Table 3: Ablation of components of the approach. The \"wo/ Filter nDCG@10 < 0.1\" variant removes the filter on low-quality candidate sets (best possible nDCG@10 < 0.1)many lacking relevant passages and yielding zero rewardwhich degrades performance. This underscores the importance of curating high-quality training instances. Next, we explored reward function design. Using raw NDCG@10 (\"w/ rrank = Srerank\") results in lower performance due to high variance. Subtracting the initial score (\"w/ rrank = Srerank Sinit\") improved stability but still underperforms REARANK. This could be due to small learning signals as result of small reward value scale. Our full models normalized reward function provided more effective guidance, yielding the best results. Finally, to isolate the benefits of our RL training approach from those gained solely from training on high-quality data, we also trained SFT baseline (\"w/ direct SFT\"). This model was trained on 12k instances representing the best possible rankings derived from our multi-sampled candidate sets based on relevance judgments. Trained on this small dataset, the SFT baseline showed marginal improvements on in-domain and OOD tasks compared to the base Qwen2.5-7B model, but importantly, it negatively impacted the models reasoning ability on the BRIGHT benchmark. This underscores the necessity of the RL approach for effectively leveraging small, high-quality data to train Figure 4: (Top) Reward evolving curve (Bottom) Response length curve."
        },
        {
            "title": "5 Analysis",
            "content": "Analysis of Reward Functions We investigate the impact of different reward functions on REARANKs training. We compare three nDCG@10 formulations: Normalized nDCG (our method: scaled by ideal nDCG), Absolute nDCG (Srerank), and Difference nDCG (rrank = Srerank Sinit, improvement over initial). The top panel of Figure 4 shows reward curves. Normalized nDCG consistently grows, reaching 0.8 by 100 steps, signifying that the achieved ranking quality is approximately 80% of the ideal quality for the candidate set. Absolute nDCG saturates early ( 50 steps). Difference nDCG shows limited progress, plateauing at 0.15 average improvement. This suggests Normalized nDCG provides more effective signal for learning towards optimal ranking quality. The bottom panel shows response length trends. Normalized nDCG yields generally longer responses ( 850 tokens). Other functions saturate earlier ( 700 tokens), suggesting that the stronger signal from Normalized nDCG encourages more detailed responses and reasoning. 7 Figure 5: Reasoning patterns: Beforevs. After-RL training under identical prompt and query. Model Reasoning TREC-DL BEIR BRIGHT Qwen3-32B Qwen3-32B REARANK-7B REARANK-7B Qwen2.5-7B - 71.6 70.9 +0.6 72.0 68.2 +3.9 65.5 54.1 54.3 -0.2 54.6 52.9 +1.7 50. 18.2 17.8 +0.4 17.7 16.4 +1.3 15.0 Table 4: Performance with Reasoning activated and disactivated. is improvement with reasoning. Is reasoning helpful? The reasoning ability of REARANK is activated by the system prompt used in RL training (see app.A). While Qwen3-32B also provides switch to enable or disable the \"thinking\" mode, our observations, detailed in table 4, indicate that enabling reasoning yields only marginal improvements in its reranking performance. Specifically, Qwen3-32B shows slight gains on TRECDL and BRIGHT (+0.6 and +0.4, respectively) and minor decrease on BEIR (-0.2), suggesting its high performance stems primarily from strong pretraining, rather than reasoning capacity. In contrast, reasoning is core to our specialized reranking agent, REARANK, and significantly enhances its performance. As the table illustrates, REARANK-7B without reasoning (use zero-shot prompt), already outperforms the Qwen2.5-7B baseline, demonstrating an inherent improvement in reranking capacity. However, incorporating reasoning leads to considerable gains, underscoring the importance of reasoning learned via RL. Reasoning Pattern As illustrated in fig. 5, RL training profoundly impacts reasoning patterns. The trained model learns strategic reranking approach, reasoning about the relevance of the passage to the query and extracting key judgment words. It also intelligently leverages terms like \"same\" for concise comparisons with prior passages, reducing verbose reasoning while still providing keywords. The model without RL training, even with reasoning prompt, does not show such strategy and employs shorter reasoning chains. Is improved reasoning transferable? To investigate the transferability of improved reasoning gained from our reasoning-based reranking RL training, we evaluate the model on mathematical reasoning questions. Following (Ye et al., 2025), we report the pass@1 score averaged over 16 samples with temperature of 0.7 on the AIME 2024 and AMC datasets. As shown in Table 5, we observe consistent improvements on both the challenging math tasks. This improvement suggests that training on the reranking task can, to certain extent, transfer to other reasoning tasks. Model AIME 2024 AMC Qwen2.5-7B-Instruct (Ours) Rearank-7B 11.87 12.92 51.41 52.66 Table 5: Math Reasoning Transfer Results (Pass@1) Figure 6: Impact of reasoning length. Data points are binned into 20 equal-width intervals by token count. 8 Impact of Reasoning Length on Performance We analyzed the impact of reasoning length on REARANKs reranking performance by repeatedly reranking biology samples from BRIGHT (50 times per query, temp. = 0.6), filtering instances with 0 scores. Contrary to (Marjanovic et al., 2025), Figure 6 reveals no clear correlation between reasoning length and reranking performance."
        },
        {
            "title": "6 Conclusion",
            "content": "We presented REARANK, pioneering reasoning listwise reranking agent trained via Reinforcement Learning. REARANK significantly outperforms baselines and achieves competitive, even superior, results compared to GPT-4 and Qwen3 models across various benchmarks, notably with only 179 labeled training samples. Our analysis confirms that the RL-acquired reasoning capabilities transfer, facilitating new and effective reranking strategies."
        },
        {
            "title": "Limitations",
            "content": "While REARANKdemonstrates promising results, limitations should be acknowledged. several Firstly, the quality and faithfulness of its generated explanations for ranking decisions, which may contain certain degree of hallucination, have not been formally evaluated. Secondly, its performance heavily relies on the quality of initial candidates provided by BM25, which potentially limits improvements in scenarios with poor initial retrieval."
        },
        {
            "title": "Acknowledgement",
            "content": "We sincerely appreciate the valuable feedback provided by Rabiul Awal, and Kanishk Jain, as well as the thoughtful input from all MAIR Lab members on multiple occasions. We thank the Mila IDT team and their technical support for maintaining the Mila compute cluster. This research was enabled in part by support provided by Calcul Québec and the Digital Research Alliance of Canada. We also acknowledge the material support of NVIDIA in the form of computational resources. Throughout this project, Aishwarya Agrawal received support from the Canada CIFAR AI Chair award."
        },
        {
            "title": "References",
            "content": "Parishad BehnamGhader, Vaibhav Adlakha, Marius Mosbach, Dzmitry Bahdanau, Nicolas Chapados, and Siva Reddy. 2024. Llm2vec: Large language models are secretly powerful text encoders. arXiv preprint arXiv:2404.05961. Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, and 1 others. 2022. Improving language models by retrieving from trillions of tokens. In International conference on machine learning, pages 22062240. PMLR. Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Campos, and Ellen Voorhees. 2020a. Overview of the trec 2019 deep learning track. arXiv preprint arXiv:2003.07820. Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Campos, and Ellen Voorhees. 2020b. Overview of the trec 2019 deep learning track. arXiv preprint arXiv:2003.07820. Thibault Formal, Benjamin Piwowarski, and Stéphane SPLADE: Sparse Lexical and Clinchant. 2021. Expansion Model for First Stage Ranking, page 22882292. Association for Computing Machinery, New York, NY, USA. Evan Greensmith, Peter Bartlett, and Jonathan Baxter. 2004. Variance reduction techniques for gradient estimates in reinforcement learning. Journal of Machine Learning Research, 5(Nov):14711530. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, and 1 others. 2025. Deepseek-r1: Incentivizing reasoning capability in arXiv preprint llms via reinforcement learning. arXiv:2501.12948. Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35:22199 22213. Chankyu Lee, Rajarshi Roy, Mengyao Xu, Jonathan Raiman, Mohammad Shoeybi, Bryan Catanzaro, and Wei Ping. 2024. Nv-embed: Improved techniques for training llms as generalist embedding models. arXiv preprint arXiv:2405.17428. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, and 1 others. 2020. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in neural information processing systems, 33:9459 9474. Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, and 1 others. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774. Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, and 1 others. 2022. Holistic evaluation of language models. arXiv preprint arXiv:2211.09110. 9 Xueguang Ma, Liang Wang, Nan Yang, Furu Wei, and Jimmy Lin. 2024a. Fine-tuning llama for multistage text retrieval. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 2421 2425. Xueguang Ma, Liang Wang, Nan Yang, Furu Wei, and Jimmy Lin. 2024b. Fine-tuning llama for multistage text retrieval. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 2421 2425. Xueguang Ma, Xinyu Zhang, Ronak Pradeep, and Jimmy Lin. 2023. Zero-shot listwise document arXiv reranking with large language model. preprint arXiv:2305.02156. Sara Vera Marjanovic, Arkil Patel, Vaibhav Adlakha, Milad Aghajohari, Parishad BehnamGhader, Mehar Bhatia, Aditi Khandelwal, Austin Kraft, Benno Krojer, Xing Han Lù, and 1 others. 2025. Deepseek-r1 thoughtology: Lets< think> about llm reasoning. arXiv preprint arXiv:2504.07128. Ronak Pradeep, Sahel Sharifymoghaddam, and Jimmy Lin. 2023a. Rankvicuna: Zero-shot listwise document reranking with open-source large language models. arXiv preprint arXiv:2309.15088. Ronak Pradeep, Sahel Sharifymoghaddam, and Jimmy Lin. 2023b. Rankzephyr: Effective and robust zeroshot listwise reranking is breeze! arXiv preprint arXiv:2312.02724. Zhen Qin, Rolf Jagerman, Kai Hui, Honglei Zhuang, Junru Wu, Le Yan, Jiaming Shen, Tianqi Liu, Jialu Liu, Donald Metzler, and 1 others. 2023. Large language models are effective text rankers with pairwise ranking prompting. arXiv preprint arXiv:2306.17563. Nils Reimers and Iryna Gurevych. 2019. Sentence-bert: Sentence embeddings using siamese bert-networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics. Nils Reimers and Iryna Gurevych. 2020. Making monolingual sentence embeddings multilingual usIn Proceedings of the ing knowledge distillation. 2020 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics. Stephen Robertson, Hugo Zaragoza, and 1 others. 2009. The probabilistic relevance framework: Bm25 and beyond. Foundations and Trends in Information Retrieval, 3(4):333389. Devendra Singh Sachan, Mike Lewis, Mandar Joshi, Armen Aghajanyan, Wen-tau Yih, Joelle Pineau, and Luke Zettlemoyer. 2022. Improving passage retrieval with zero-shot question generation. arXiv preprint arXiv:2204.07496. John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. 2015. High-dimensional continuous control using generalized advantage estimation. arXiv preprint arXiv:1506.02438. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, and 1 others. 2024. Deepseekmath: Pushing the limits of mathematical reasonarXiv preprint ing in open language models. arXiv:2402.03300. Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. 2024. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv:2409.19256. Hongjin Su, Howard Yen, Mengzhou Xia, Weijia Shi, Niklas Muennighoff, Han-yu Wang, Haisu Liu, Quan Shi, Zachary Siegel, Michael Tang, and 1 others. 2024. Bright: realistic and challenging benchmark for reasoning-intensive retrieval. arXiv preprint arXiv:2407.12883. Weiwei Sun, Lingyong Yan, Xinyu Ma, Shuaiqiang Wang, Pengjie Ren, Zhumin Chen, Dawei Yin, and Zhaochun Ren. 2023. Is chatgpt good at search? investigating large language models as re-ranking agents. arXiv preprint arXiv:2304.09542. Nandan Thakur, Nils Reimers, Andreas Rücklé, Abhishek Srivastava, and Iryna Gurevych. 2021. Beir: heterogenous benchmark for zero-shot evaluation of information retrieval models. arXiv preprint arXiv:2104.08663. Shuai Wang, Shengyao Zhuang, and Guido Zuccon. 2021. Bert-based dense retrievers require interpolation with bm25 for effective passage retrieval. In Proceedings of the 2021 ACM SIGIR international conference on theory of information retrieval, pages 317324. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2022. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171. Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, and 1 others. 2022a. Emergent abilities of large language models. arXiv preprint arXiv:2206.07682. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, and 1 others. 2022b. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824 24837. Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul Bennett, Junaid Ahmed, and Arnold Overwijk. 2020. Approximate nearest neighbor negative contrastive learning for dense text retrieval. arXiv preprint arXiv:2007.00808. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, and 1 others. arXiv preprint 2025. Qwen3 technical report. arXiv:2505.09388. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, and 1 others. 2024. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115. Yixin Ye, Zhen Huang, Yang Xiao, Ethan Chern, Shijie Xia, and Pengfei Liu. 2025. Limo: Less is more for reasoning. arXiv preprint arXiv:2502.03387. Le Zhang, Rabiul Awal, and Aishwarya Agrawal. 2024a. Contrasting intra-modal and ranking crossmodal hard negatives to enhance visio-linguistic comIn Proceedings of the positional understanding. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1377413784. Le Zhang, Yihong Wu, Fengran Mo, Jian-Yun Nie, and Aishwarya Agrawal. 2023. Moqagpt: Zeroshot multi-modal open-domain question answerarXiv preprint ing with large language model. arXiv:2310.13265. Le Zhang, Yihong Wu, Qian Yang, and Jian-Yun Nie. 2024b. Exploring the best practices of query expansion with large language models. arXiv preprint arXiv:2401.06311. Le Zhang, Qian Yang, and Aishwarya Agrawal. 2024c. Assessing and learning alignment of unimodal vision and language models. arXiv preprint arXiv:2412.04616. Shengyao Zhuang, Xueguang Ma, Bevan Koopman, RankJimmy Lin, and Guido Zuccon. 2025. r1: Enhancing reasoning in llm-based document rerankers via reinforcement learning. arXiv preprint arXiv:2503.06034. Shengyao Zhuang, Honglei Zhuang, Bevan Koopman, and Guido Zuccon. 2024. setwise approach for effective and highly efficient zero-shot ranking with large language models. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 3847."
        },
        {
            "title": "A Prompt Template",
            "content": "We provide the prompt template used in the experiments. Each prompt is concatenation of system prompt and user instruction. The user iteratively provides each passage paired with its original rank identifier. The prompt concludes with post promt message containing the ranking query and explicit output format requirements. A.1 Reasoning prompt This is the prompt used by REARANK. Reasoning System Prompt Zero-shot User Instruction Iterative User Message (per passage): [RANK] [Passage Content (truncated)] Iterative Assistant Message (per passage): Received passage [RANK]. Post Prompt: Search Query: [QUERY]. Rank the [NUM] passages above based on their relevance to the search query. The passages should be listed in descending order using identifiers. The most relevant passages should be listed first. The output format should be [] > [], e.g., [1] > [2]. Only response the ranking results, do not say any word or explain. You are DeepRerank, an intelligent assistant that can rank passages based on their relevancy to the search query. You first thinks about the reasoning process in the mind and then provides the user with the answer. will provide you with passages, each indicated by number identifier []. Rank the passages based on their relevance to the search query. Search Query: [QUERY]. Rank the [NUM] passages above based on their relevance to the search query. The passages should be listed in descending order using identifiers. The most relevant passages should be listed first. The output format should be <answer> [] > [] </answer>, e.g., <answer> [1] > [2] </answer>. Reasoning User Instruction Iterative User Message (per passage): [RANK] [Passage Content (truncated)] Iterative Assistant Message (per passage): Received passage [RANK]. Post Prompt: Please rank these passages according to their relevance to the search query: \"[QUERY]\" Follow these steps exactly: 1. First, within <think> tags, analyze EACH passage individually: Evaluate how well it addresses the query Note specific relevant information 2. Then, within <answer> tags, provide ONLY the final ranking in descending order of relevance using the format: [X] > [Y] > [Z] A.2 Zero-shot prompt This is original prompt used for RankQwen and RankGPT. Zero-shot System Prompt You are RankGPT, an intelligent assistant that can rank passages based on their relevancy to the query. will provide you with [NUM] passages, each indicated by number identifier []. Rank the passages based on their relevance to query: [QUERY]. 12 Figure 7: Multiple reranking results of REARANK. Multiple Rerank Pass Analyzing the impact of multiple reranking passes with REARANK(evaluating performance with and without reasoning) reveals mixed results across benchmarks  (fig. 7)  . Multiple passes improve performance on TERC-DL and BEIR without reasoning, but degrade it on BRIGHT. With reasoning, gains are seen on TREL-DL, but performance is harmed on other benchmarks. These findings indicate that for REARANK, single reranking pass is sufficient."
        },
        {
            "title": "B Case Study",
            "content": "Figure 8 provides an illustrative example from the BRIGHT dataset, detailing the inference reasoning process employed by REARANK. For each passage, the agent first analyzes its content to understand its key themes and then determines its relationship to the query. Relevant information, typically keywords, is then extracted based on this analysis. We conducted manual evaluation of the generated reasoning content and found it to be of high quality, contributing to the trustworthiness of the systems outputs. This reasoning process ultimately informs the final ranking of passages presented in the answer. Figure 8: Examples from biology split of BRIGHT, only 3 passages selected for better visibility. Relevant sentences in the passages and Key reasoning content in the thinking process are highlight."
        }
    ],
    "affiliations": [
        "Canada CIFAR AI Chair",
        "Fudan University",
        "McGill University",
        "Mila - Quebec AI Institute",
        "Université de Montréal"
    ]
}