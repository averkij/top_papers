{
    "paper_title": "UVE: Are MLLMs Unified Evaluators for AI-Generated Videos?",
    "authors": [
        "Yuanxin Liu",
        "Rui Zhu",
        "Shuhuai Ren",
        "Jiacong Wang",
        "Haoyuan Guo",
        "Xu Sun",
        "Lu Jiang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "With the rapid growth of video generative models (VGMs), it is essential to develop reliable and comprehensive automatic metrics for AI-generated videos (AIGVs). Existing methods either use off-the-shelf models optimized for other tasks or rely on human assessment data to train specialized evaluators. These approaches are constrained to specific evaluation aspects and are difficult to scale with the increasing demands for finer-grained and more comprehensive evaluations. To address this issue, this work investigates the feasibility of using multimodal large language models (MLLMs) as a unified evaluator for AIGVs, leveraging their strong visual perception and language understanding capabilities. To evaluate the performance of automatic metrics in unified AIGV evaluation, we introduce a benchmark called UVE-Bench. UVE-Bench collects videos generated by state-of-the-art VGMs and provides pairwise human preference annotations across 15 evaluation aspects. Using UVE-Bench, we extensively evaluate 16 MLLMs. Our empirical results suggest that while advanced MLLMs (e.g., Qwen2VL-72B and InternVL2.5-78B) still lag behind human evaluators, they demonstrate promising ability in unified AIGV evaluation, significantly surpassing existing specialized evaluation methods. Additionally, we conduct an in-depth analysis of key design choices that impact the performance of MLLM-driven evaluators, offering valuable insights for future research on AIGV evaluation. The code is available at https://github.com/bytedance/UVE."
        },
        {
            "title": "Start",
            "content": "UVE: Are MLLMs Unified Evaluators for AI-Generated Videos? Yuanxin Liu 1 Rui Zhu 2 Shuhuai Ren 1 Jiacong Wang 3 Haoyuan Guo 2 Xu Sun 1 Lu Jiang 2 5 2 0 2 3 1 ] . [ 1 9 4 9 9 0 . 3 0 5 2 : r Abstract With the rapid growth of video generative models (VGMs), it is essential to develop reliable and comprehensive automatic metrics for AIgenerated videos (AIGVs). Existing methods either use off-the-shelf models optimized for other tasks or rely on human assessment data to train specialized evaluators. These approaches are constrained to specific evaluation aspects and are difficult to scale with the increasing demands for finer-grained and more comprehensive evaluations. To address this issue, this work investigates the feasibility of using multimodal large language models (MLLMs) as unified evaluator for AIGVs, leveraging their strong visual perception and language understanding capabilities. To evaluate the performance of automatic metrics in unified AIGV evaluation, we introduce benchmark called UVE-Bench. UVE-Bench collects videos generated by state-of-the-art VGMs and provides pairwise human preference annotations across 15 evaluation aspects. Using UVE-Bench, we extensively evaluate 16 MLLMs. Our empirical results suggest that while advanced MLLMs (e.g., Qwen2VL-72B and InternVL2.5-78B) still lag behind human evaluators, they demonstrate promising ability in unified AIGV evaluation, significantly surpassing existing specialized evaluation methods. Additionally, we conduct an indepth analysis of key design choices that impact the performance of MLLM-driven evaluators, offering valuable insights for future research on AIGV evaluation. The code is available at https://github.com/bytedance/UVE. 1. Introduction Video generative models (VGMs) have rapidly evolved in recent years, greatly aiding visual content creation in numer1Peking University 2ByteDance Seed 3School of Artificial Intelligence, University of Chinese Academy of Sciences. Correspondence to: Yuanxin Liu <liuyuanxin@stu.pku.edu.cn>, Xu Sun <xusun@pku.edu.cn>. Preprint. 1 ous fields. However, even current state-of-the-art (SOTA) VGMs (Brooks et al., 2024; Polyak et al., 2024; Kong et al., 2024; Kuaishou., 2024) still suffer from issues like incorrect subject structure, unnatural motion, and imperfect alignment with the text prompt. Consequently, it is crucial to develop automatic metrics that can effectively identify these imperfections in modern AI-generated videos (AIGVs), so as to facilitate the advancement of VGMs and enhance the application of AIGVs in real-world scenarios. Existing AIGV evaluation metrics fall into two categories. The first employs off-the-shelf models (e.g., CLIP (Radford et al., 2021) and DINO (Caron et al., 2021)) and designs heuristic rules to assess various specific aspects of AIGVs (Huang et al., 2024; Liu et al., 2024b). The second category involves collecting human assessments for specific aspects of interests and training models to imitate these assessments (He et al., 2024; Bansal et al., 2024; Zhang et al., 2024d; Wu et al., 2024a; Wang et al., 2024a). Such specifically trained automatic evaluators have shown better correlation with human evaluations compared to the offthe-shelf models. However, with the rapid development of VGMs and AIGV applications, there is an urgent need for finer-grained and more comprehensive evaluations. Continuously collecting human assessments and training models to accommodate emerging evaluation aspects and frequently changing standards is cost-intensive and difficult to scale. Unlike existing automatic metrics, we humans can assess any aspect of AIGVs as long as proper guideline is provided. This ability stems from our robust visual perception and language understanding. Leveraging the knowledge learned from vast amounts of visual and language data, the latest multimodal large language models (MLLMs) (Wang et al., 2024c; OpenAI, 2023; Chen et al., 2024b) also exhibit strong ability in joint vision-language understanding. This progress naturally raises question: Can MLLMs be utilized as unified AIGV evaluator like humans? To address this question, we propose an approach to unify AIGV evaluations by prompting pre-trained MLLMs and mapping their outputs to evaluation results. This method enables zero-shot evaluation of any aspect of AIGV by simply modifying the prompts. It supports both single video ratings and video pair comparisons. While using MLLMs for AIGV evaluation is straightforward concept and has been explored in previous works (He et al., 2024; Bansal UVE: Are MLLMs Unified Evaluators for AI-Generated Videos? et al., 2024; Wang et al., 2024a), these studies are limited by evaluating only few aspects and relying on humanannotated ratings to train the MLLMs. To evaluate the capability of MLLMs as unified AIGV evaluators, we require benchmark that (1) encompasses broad range of AIGV aspects, (2) provides accurate human evaluations as references, and (3) includes AIGVs that highlight the weaknesses of state-of-the-art VGMs. Since no existing AIGV dataset meets all these criteria, we introduce new benchmark called UVE-Bench (short for Unified Video Evaluation). UVE-Bench has three key features: First, it covers wide range of 15 evaluation aspects. Second, it provides human annotation in the form of pairwise video preference, avoiding the inconsistent standards of absolute ratings between humans (Wu et al., 2024b), and can be used to evaluate both single video ratings and video pair comparisons. Third, the videos in UVE-Bench are generated by the latest VGMs (e.g., MovieGenVideo (Polyak et al., 2024) and HunyuanVideo (Kong et al., 2024)), challenging the evaluators to identify the weaknesses of such videos. Based on UVE-Bench, we conduct an extensive evaluation of 16 MLLMs. Our results indicate that unified evaluator powered by advanced MLLMs, such as Qwen2-VL72B (Wang et al., 2024c) and InternVL2.5-78B (Chen et al., 2024b), indeed show promising abilities in evaluating various AIGV aspects, outperforming existing approaches that focus on specific aspects. However, there remains significant gap between these MLLMs and human evaluators, particularly in aspects requiring fine-grained understanding of temporal dynamics in videos. Additionally, we performed series of analytical studies on the design choices that impact the performance of our MLLM-driven AIGV evaluation framework, providing valuable insights for future research in the field. The contributions of this work are summarized as follows: 1. We introduce unified approach to evaluate any aspect of AIGV using pre-trained MLLMs. 2. We propose UVE-Bench, comprehensive benchmark to assess the capability of unified AIGV evaluation. 3. We conduct in-depth analysis on the pros and cons of MLLMs in unified AIGV evaluation and the key design choices that impact their performance. 2. Related Work 2.1. Video Generative Models The emergence of diffusion models (Ho et al., 2020; Song et al., 2021) has revolutionized generative applications, spanning from image synthesis (Saharia et al., 2022; 2 Ramesh et al., 2022; Rombach et al., 2022) to video generation (Gupta et al., 2023; Yu et al., 2023; Ho et al., 2022a;b). On the one hand, recent VGMs like SORA (Brooks et al., 2024), Kling (Kuaishou., 2024), and MovieGen (Polyak et al., 2024) have achieved remarkable success in producing highly realistic videos that closely resemble real ones. On the other hand, the condition signal for video generation has been expanded from text (T2V) to RGB images (I2V) (Li et al., 2023c; Chen et al., 2023a) and other videos (V2V) (Zhao et al., 2024). This work specifically focuses on evaluating text-conditioned video generation. 2.2. Multimodal Large Language Models Multimodal large language models (OpenAI, 2024; Wang et al., 2024c; Li et al., 2024; 2023a) integrate LLMs with additional perception modules, enabling them to comprehend information beyond the text modality. With advancements in LLM backbones and the increasing scale of multimodal training, MLLMs are showing promising performance in understanding real-world images (Liu et al., 2023a; 2024a; Gao et al., 2024; Wang et al., 2024b) and videos (Zhang et al., 2024b; Li et al., 2023b; Lin et al., 2023). However, understanding AI-generated visual content presents unique challenges compared to real images, and the application of MLLMs in this field is still in its early stages. 2.3. AI-Generated Video Evaluation Despite the progress in VGMs, they still struggle to consistently generate high-quality videos, which necessitate comprehensive and fine-grained evaluation. Initial evaluation approaches (Huang et al., 2024; Liu et al., 2024b) employ off-the-shelf models to evaluate specific aspects, such as CLIP (Radford et al., 2021) for video-text alignment and DINO (Caron et al., 2021) for frame-to-frame consistency. With the advancement of MLLMs, recent studies have incorporated these models in AIGV evaluation (He et al., 2024; Wu et al., 2024a; Wang et al., 2024a; Wu et al., 2024c; Bansal et al., 2024). However, they are constrained to fixed set of evaluation aspects and typically rely on human annotations to further fine-tune the MLLMs for AIGV evaluation. By contrast, in this work explores using MLLMs as unified evaluators for any AIGV aspect by leveraging their inherent vision-language understanding capabilities, eliminating the need for human annotations. 3. Unifying AIGV Evaluation with MLLMs In this section, we introduce how to utilize single MLLM to evaluate any aspect of AIGVs, focusing on both single video rating and video pair comparison. UVE: Are MLLMs Unified Evaluators for AI-Generated Videos? Single Video Rating <video> Watch the above frames of an AI-generated video and evaluate <aspect-specific description> Complete your evaluation by answering this question: <aspect-specific question>? <answer prompt> Video Pair Comparison The first video: <video> The second video: <video> Watch the above two AI-generated videos and evaluate <aspect-specific description> Complete your evaluation by answering this question: Which video is <aspect-specific question>? You should make your judgment based on the following rules: <instructions on how to make the choice> Now give your judgment: Table 1. MLLM prompting templates for unified AIGV evaluation. 3.1. Problem Formulation Single Video Rating. Given generated video V, textual evaluation guideline Ga and the text-to-video (T2V) prompt used to generate the video, the objective of single video rating is to predict numerical score measuring the quality of based on the aspect specified by Ga. Video Pair Comparison. Given pair of generated videos V1, V2, the corresponding T2V prompts T1, T2 used to generate them, and textual evaluation guideline Ga, the objective of video pair comparison is to make choice from one of the four options: = {V1better, V2better, same good, same bad}, according to the aspect described by Ga. 3.2. Method Single Video Rating. Research on image quality evaluation (Zhang et al., 2024e; Lin et al., 2024) has demonstrated that, given an appropriate prompt, the generative likelihood of certain tokens (e..g, yes/no or good/poor) by MLLMs can be used an effective indicator of image visual quality and image-text alignment. Motivated by this, we formulate single video rating in unified manner as: = Pθ(tposV, , Ga) Pθ(tposV, , Ga) + Pθ(tnegV, , Ga) (1) Video Source Gen-3 (Runway., 2024) Kling 1.5 (Kuaishou., 2024) LUMA 1.6 (LumaLabs., 2024) Movie Gen Video (Polyak et al., 2024) Open-Sora 1.2 (Zheng et al., 2024) CogVideoX-5B (Yang et al., 2024b) HunyuanVideo (Kong et al., 2024) Mochi 1 (Team, 2024) ShutterStock Prompt Source VideoGen-Eval VideoGen-Eval VideoGen-Eval Movie Gen VideoGen-Eval Movie Gen ShutterStock VideoGen-Eval Movie Gen ShutterStock Movie Gen ShutterStock Movie Gen ShutterStock ShutterStock Resolution Duration FPS Release Date 2024.06 2024.09 2024.09 2024.10 2024. Open Source 24 30 24 24 24 8 2024.08 24 2024.12 2024.10 24, 25 30, 60 - - 5.3 5.1 5.0 5.0 4.3 5.3 6.1 5.4 5. 5.0 1280768 19201080 1360752 848480 1280720 848480 720480 960544 848480 600316 898506 596 Table 2. Information of the videos in UVE-Bench. where tpos and tneg are predefined positive/negative scoring tokens. Pθ(t) is the probability of generating token using MLLM θ. Ga ends with question related to aspect and an answer prompt encouraging the model to generate the scoring tokens (e.g., Please directly answer yes or no:). Video Pair Comparison. straightforward way to perform video pair comparison is directly feeding the video pair into MLLMs, as the latest MLLMs (Wang et al., 2024c; Li et al., 2024; Chen et al., 2024b) already support interleaved vision-language input with multiple videos. Specifically, we feed the video pair, along with the corresponding T2V prompts if needed, into the MLLM and prompts it to make choice from O: = fθ(V1, V2, T1, T2, Ga) (2) Table 1 illustrates the prompting templates for single video rating and video pair comparison, which can be adapted to any evaluation aspect by modifying the aspect-specific prompt. The specific prompts being used for different aspects are provided in the supplementary material. 4. UVE-Bench As illustrated in Figure 1, UVE-Bench is designed to assess AIGV automatic evaluation methods. This section details the process of video collection (4.1), the evaluation aspects covered by UVE-Bench (4.2), the criterion for assessing the performance of automatic AIGV evaluators using human preferences (4.3), the data annotation procedure (4.4) and comparison with existing AIGV datasets (4.5). 4.1. Video Collection The videos in UVE-Bench are created using eight of the latest video generative models via text-to-video generation. To ensure diverse video content, we select text prompts from three sources: (1) VideoGen-Eval (Zeng et al., 2024) provides over 700 prompts, targeting various application scenarios (e.g., animation and advertisement) and key capabilities (e.g., text alignment and motion diversity) of video UVE: Are MLLMs Unified Evaluators for AI-Generated Videos? Figure 1. Overview of UVE-Bench. (a) The distribution of video sources. (b) The distribution of data example over 15 fine-grained AIGV evaluation aspects. (c) The distribution of human preference over the four categories. (d) Data examples illustrating how to evaluate both single video rating and video pair comparison using the human preference annotations. More examples can be found in Appendix B.4. generation. (2) Movie Gen Video Bench (Polyak et al., 2024) offers 1,003 prompts covering various categories of video content (e.g., human activity, animals, and scenery). (3) We also modify small portion of video captions from the ShutterStock1 platform to generate videos with different degrees of Light & Color change (an evaluation aspect that will be discussed in 4.2). For VideoGen-Eval, we directly collect videos generated by Gen-3, Kling 1.5, LUMA 1.6, Open-Sora 1.2, and CogVideoX-5B, as released by the authors of Zeng et al. (2024). For Movie Gen Video Bench and ShutterStock, we generate videos using Open-Sora 1.2, CogVideoX-5B, Mochi 1 and HunyuanVideo. For the Movie Gen Video model, we directly collect the generated videos released by Polyak et al. (2024) and select the first five seconds of the videos. Additionally, we collect some non-AIGV videos related to Light & Color from ShutterStock. As shown in Table 2, the videos have duration of around 5.0 seconds, with resolution ranging from 596 336 (360p level) to 1280 720 (720p level) and 1920 1080 (1080p level). 4.2. Evaluation Aspects Inspired by previous work in AIGV evaluation (Huang et al., 2024; Liu et al., 2023b; 2024b), we identify four fundamental evaluation aspects to build our benchmark: Static Quality, Temporal Quality, Dynamic Degree and Video-Text Alignment, each of them is further divided into several subaspects. 1https://www.shutterstock.com/ Static Quality. This aspect evaluates the visual quality of individual video frames. It is broken down into four subaspects: (1) Aesthetic Quality, which assesses the aesthetic elements, including frame layout, lighting, and color harmony. (2) Technical Quality, which examines the presence of unwanted noise, blur, and distortion. (3) Structural Correctness, which checks for abnormal subject structures that contradict common sense (e.g., person with three eyes). (4) Overall Static Quality, which jointly considers the above three subaspects. Temporal Quality. This aspect assesses visual quality from temporal perspective, focusing on four subaspects: (1) Appearance Consistency, which evaluates whether the appearance and identity of subjects or backgrounds remain consistent across frames. (2) Temporal Flickering, which identifies undesirable flickering and jittering that degrade visual quality. (3) Motion Naturalness, which determines if subject motions and interactions appear natural and adhere to physical laws. (4) Overall Temporal Quality, which comprehensively assesses the temporal visual quality by integrating these three subaspects. Dynamic Degree. This aspect evaluates the degree of dynamic in the generated video. It includes four subaspects: (1) Subject Motion, which focuses on the motion degree of subjects. (2) Camera Motion, which assesses the motion degree of the camera. (3) Light & Color, which considers changes in lighting conditions and color. (4) Overall Dynamic Degree, which combines these three subaspects to 4 UVE: Are MLLMs Unified Evaluators for AI-Generated Videos? provide comprehensive measure of the videos dynamic degree. Video-Text Alignment. This aspect evaluates how well the generated video aligns with the given text prompt, divided into three subaspects: (1) Appearance Alignment, which focuses on the alignment of subject or scene appearance with the text. (2) Motion Alignment, which assesses the alignment of subject or camera motions with the text. (3) Overall Alignment, which considers how faithfully the video content reflects the entire text prompt. 4.3. Evaluation Criterion Dataset Aspects Eval Task Latest VGM FETV (Liu et al., 2023b) VBench (Huang et al., 2024) EvalCrafter (Liu et al., 2024b) T2VQA-DB (Kou et al., 2024) VideoFeedback (He et al., 2024) LGVQ (Zhang et al., 2024d) TVGE (Wu et al., 2024c) GAIA (Mialon et al., 2023) Q-Bench-Video (Zhang et al., 2024c) AIGV-Assessor (Wang et al., 2024a) UVE-Bench (Ours) 9 16 4 1 5 3 2 3 4 4 15 Single Pair Single Single Single Single Single Single Understanding Single&Pair Single&Pair ZeroScope (2023.06) VideoCrafter1 (2023.10) Gen-2 (2023.12) AnimateDiff (2023.12) SORA (2024.02) Gen-2 (2023.12) Gen-2 (2023.12) Mora (2024.03) SORA (2024.02) SORA (2024.02) HunyuanVideo (2024.12) Table 3. Comparison between UVE-Bench and existing AIGV datasets. Single denotes single video rating. Pair denotes video pair comparison. Understanding denotes general video quality understanding via open-ended question answering. shown in Figure 1, As Bench consists of three elements: {V1, V2}, ing human preference annotation {V1better, V2better, same good, same bad}. each example in UVEa pair of videos the evaluation aspect and correspond- = For single video rating, the automatic evaluator is required to predict numerical rating scores S1, S2 [0, 1] for each video, respectively. Then, we assess the correctness of rating according to the following criteria: Asingle = 1(S1 > S2) 1(S1 < S2) fc(S1β) fc(S2β) c(S1α) fc(S2α) if = V1better elif = V2better elif = same good elif = same bad (3) Here, 1() {0, 1} is binary indicator function and α < β are the thresholds for bad and good, respectively. fc(Sβ) is piecewise function that equals to 1 when [β, 1], indicating that the model evaluation aligns with the human judgment of same good. When [0, β), fc(Sβ) exponentially decays from 1 to 0, indicating gradual deviation from same good. Similarly, c(Sα) is constructed in reversed manner to assess whether agrees with the human judgment of same bad. Details of fc(Sα) and c(Sβ) can be found in Appendix A.2. When it comes to video pair comparison, the automatic evaluator is asked to make choice from in the same way as humans. Therefore, we directly adopt accuracy as the evaluation criteria: Apair = 1(C = P). 4.4. Data Annotation and Quality Review With the collected videos, the authors of this paper, who have rich research experience in VGMs and MLLMs and are proficient in English, annotate the evaluation aspects and pairwise video preferences. Specifically, videos generated by the same text prompt are presented in pairwise manner to the annotators, who are asked to determine preference choice from and identify the aspect that influenced their 5 decision. To minimize subjectivity, annotators are instructed to only annotate video pairs for which they could confidently make preference judgment, and the remaining video pairs are discarded. Following this procedure, we obtained 4,042 pairwise preference annotations and retained 1,230 individual videos. The distributions of the videos and preference annotations are illustrated in Figure 1. To verify the quality of annotations, we randomly sample subset of the dataset, consisting of 50 samples for each subaspect and assign different group of three annotators to label pairwise video preferences. Including the original annotations, the inter-annotator agreement, as measured by Fleiss Kappa, is 0.803. This indicates high level of agreement among the human annotators. 4.5. Comparison with Existing AIGV Datasets Table 3 compares UVE-Bench with existing AIGV datasets across three dimensions. First, current datasets typically focus on few basic aspects (such as video-text alignment, static and temporal visual quality), making them not suitable for assessing MLLMs as unified evaluators across various aspects. Second, existing datasets predominantly provide human annotations in the form of single-video rating, which not only precludes video pair comparison analysis but also introduces greater subjectivity compared to pairwise preference annotation (Wu et al., 2024b; Zhang et al., 2024e). Third, the VGMs employed in existing datasets are relatively outdated, failing to represent the current SOTA performance of AIGVs. Identifying the weaknesses of such videos is both important and challenging for automatic AIGV evaluators. UVE-Bench addresses these limitations and provides more comprehensive and accurate assessment for automatic AIGV evaluation models. UVE: Are MLLMs Unified Evaluators for AI-Generated Videos? Method Random Video-LLaVA LongVA-DPO ShareGPT4Video VideoLLaMA2.1 mPLUG-Owl3 VideoChat2-Mistral MiniCPM-V-2.6 LLaVA-OneVision LLaVA-OneVision LLaVA-Video LLaVA-Video Qwen2-VL Qwen2-VL InternVL-2.5-MPO InternVL-2.5-MPO GPT-4o VideoScore-v1.1 VBench Model Size Overall Dynamic Subject Motion Camera Motion Light& Color Overall Static Technical Quality Structural Correctness Aesthetic Quality Overall Temporal Appearance Consistency Motion Naturalness Temporal Flickering Overall Alignment Motion Alignment Appearance Alignment AVG - 7B 7B 8B 7B 7B 7B 8B 7B 72B 7B 72B 7B 72B 8B 78B - 8B - 48.8 52.4 59.8 77.5 72.1 78.6 83.1 81.4 81.0 82.3 80.4 82.8 84.6 86.5 81.3 84.3 79.0 57.4 87.8 49. 74.8 76.2 81.0 80.5 84.8 92.2 86.3 87.6 87.8 85.5 86.1 89.7 92.6 86.1 86.6 84.3 - - 48.9 63.4 69.9 77.1 67.1 77.8 89.6 80.3 83.2 78.4 80.9 82.9 94.2 92.7 80.4 82.4 74.9 - - 49. 47.6 57.5 82.5 77.2 79.9 74.9 88.8 84.9 88.2 81.5 86.9 79.7 86.0 88.0 91.6 81.8 - - 49.2 47.8 62.4 59.4 61.4 76.0 68.1 70.9 70.7 71.3 66.2 70.2 64.6 70.6 68.1 72.8 74.0 40.1 - 47. 66.1 74.9 68.7 78.7 83.1 76.2 75.0 78.2 77.6 74.2 80.0 67.3 76.9 77.9 84.0 81.6 30.4 62.5 49.0 44.5 56.2 54.1 47.5 55.6 53.8 52.9 50.6 60.2 49.5 55.4 50.7 60.2 53.6 67.2 65.2 47.7 - 48. 63.4 72.0 69.4 72.6 80.0 74.1 80.6 83.1 81.5 75.7 77.7 70.6 83.4 77.5 82.3 84.2 41.9 75.6 49.0 44.1 56.0 54.4 46.1 59.8 58.7 61.1 62.9 64.6 58.3 60.1 51.1 52.5 60.9 61.5 70.0 - - 48. 51.3 47.3 48.1 50.9 59.5 58.3 59.4 60.4 61.9 58.2 59.2 51.3 58.0 54.9 65.2 79.8 43.1 54.0 48.2 55.5 40.7 42.6 50.2 42.0 52.8 51.7 41.5 39.9 39.3 40.4 48.0 48.9 50.9 61.8 54.0 36.1 50.2 46. 48.6 54.3 60.9 61.8 72.0 85.6 70.9 85.9 86.8 82.3 83.5 62.7 71.0 72.5 88.4 58.6 - - 48.4 59.4 68.7 62.7 72.6 80.4 75.6 82.1 79.3 84.4 80.5 84.8 85.4 89.0 80.6 87.4 80.8 38.9 54.4 48. 54.9 63.9 54.3 65.7 75.2 78.0 74.3 66.9 71.7 69.9 73.7 78.7 81.8 73.2 79.3 77.2 - - 48.3 66.8 71.6 70.3 77.7 87.6 80.9 90.8 86.7 93.5 90.0 94.6 92.2 95.0 90.5 95.5 89.6 - - 48. 54.5 61.6 63.9 64.4 72.6 72.6 73.4 73.0 75.0 71.0 74.0 70.9 75.4 72.6 78.2 75.7 - - Table 4. Performance of single video rating measured by Asingle in Eq. 3. The best and second-best results are highlighted with yellow and light yellow, respectively. Method Random LLaVA-OneVision LLaVA-OneVision LLaVA-Video LLaVA-Video Qwen2-VL Qwen2-VL InternVL-2.5-MPO InternVL-2.5-MPO GPT-4o Human Model Size Overall Dynamic Subject Motion Camera Motion Light& Color Overall Static Technical Quality Structural Correctness Aesthetic Quality Overall Temporal Appearance Consistency Motion Naturalness Temporal Flickering Overall Alignment Motion Alignment Appearance Alignment AVG - 7B 72B 7B 72B 7B 72B 8B 78B - - 25.0 44.2 36.5 37.0 42.5 46.4 51.4 42.5 43.1 42.0 87. 25.0 52.6 55.1 52.6 57.7 56.4 66.7 51.3 57.7 48.7 85.3 25.0 40.0 40.0 37.5 32.5 43.8 71.2 33.8 41.2 38.8 87. 25.0 38.5 53.8 44.9 56.4 39.7 53.8 43.6 61.5 59.0 85.3 25.0 35.8 37.0 44.4 41.6 42.8 47.7 38.3 54.7 53.5 90. 25.0 35.2 51.4 43.8 55.2 38.1 62.9 31.4 62.9 54.3 92.0 25.0 25.7 22.2 38.9 31.2 20.8 19.4 40.3 27.1 41.0 88. 25.0 51.0 54.1 62.2 60.2 54.1 60.2 55.1 71.4 71.4 91.3 25.0 37.4 25.6 33.7 32.6 29.8 41.0 35.1 45.2 44.9 88. 25.0 23.9 43.3 28.9 41.1 24.4 40.0 32.2 47.8 38.9 90.0 25.0 29.1 34.5 31.1 27.7 29.7 31.8 27.0 33.8 31.8 78. 25.0 52.0 45.7 57.5 55.9 42.5 40.2 44.9 70.9 59.8 92.0 25.0 43.1 63.0 43.1 60.3 54.9 69.7 53.5 67.7 58.9 88. 25.0 39.7 58.9 39.7 53.0 50.3 62.9 50.3 55.6 57.0 84.7 25.0 43.4 69.7 46.9 66.2 57.9 78.6 53.8 76.6 61.4 92. 25.0 38.6 44.3 41.1 46.1 41.1 51.6 41.8 53.7 50.2 88.0 Table 5. Performance of video pair comparison measured by accuracy. The best and second-best results are highlighted with yellow and light yellow, respectively. 5. Experiments 5.1. Experimental Settings Evaluated Systems. We conduct zero-shot evaluations on 16 MLLMs, including 15 open-sourced models along with the proprietary GPT-4o model (OpenAI, 2024). Additionally, we assess two evaluation systems specialized in particular aspects: VBench (Huang et al., 2024) employs offthe-shelf models for evaluation and VideoScore (He et al., 2024) is fine-tuned version of Mantis-Idefics2-8B (Jiang et al., 2024), trained to imitate human rating scores. Details of the automatic evaluators are presented in Appendix A.1. To establish human baseline, we engage three annotators to perform video pair comparisons on subset of UVE-Bench, evaluating 50 samples for each subaspect. Single Video Rating. We sample 16 frames per video for our evaluation, except for Video-LLaVA, which has an 8-frame limitation. Unless otherwise specified, we adopt yes/no as the default scoring tokens for single video rating. Video Pair Comparison. In this evaluation mode, we utilize 12 frames per video. As shown in Figure 1 (c), the original UVE-Bench contains more V1/2better compared to same good/bad. While this imbalance does not compromise the fairness of single video ratings (since Asingle handles better and same independently) it introduces bias toward V1/2better when computing four-way selection accuracy for video pair comparisons. To mitigate this bias, we create subset of 2,411 samples, with more balanced distribution across the four preference categories, in evaluating video pair comparison. 5.2. Performance of MLLMs as Unified Evaluator Single Video Rating. Table 4 presents the results of MLLMs in single video rating, from which we can obtain the following findings: (1) SOTA MLLMs demonstrate strong capabilities in assessing Dynamic Degree and Video-Text Alignment, with Qwen2-VL-72B achieving over 80 Asingle score across all eight subaspects. (2) current MLLMs show limitations in evaluating Temporal Quality aspects, which require nuanced understanding of video temporal dynamics. Additionally, their performance in Motion Alignment lags behind that of Appearance Alignment. These observations echo with previous research (Liu et al., 2024c; Shangguan et al., 2024) which reveal the limitation of MLLMs in fine-grained video temporal understanding. Interestingly, Video-LLaVA and VideoChat2-Mistral, despite their lower average Asingle, perform well in assessing motion naturalness, surpassing the 72B Qwen2-VL and LLaVA-OneVision. We attribute this to their use of native video encoders, rather than the frame-by-frame processing approach used by most MLLMs. (3) While advanced 6 UVE: Are MLLMs Unified Evaluators for AI-Generated Videos? (a) Single Video Rating. (b) Video Pair Comparison. Figure 2. Results of different prompting strategies. Figure 3. Results of different scoring strategies with Qwen2-VL-7B for single video rating. MLLMs can effectively assess Technical Quality and Aesthetic Quality, they still struggle to identify incorrect subject structures (e.g., human hands with six fingers). (4) MLLMs performance in single video rating strongly correlates with their general multimodal understanding capability: Larger models, such as Qwen2-VL-72B, InternVL2.5-78B-MPO, and GPT-4o, demonstrate superior performance compared to the smaller ones at 7B scale. (5) The unified evaluators consistently outperform or match specialized methods across all evaluation aspects. This indicates that utilizing MLLMs for unified AIGV evaluation is promising direction, which showcases both versatility and effectiveness. Single Video Rating. The results in Figure 2 (a) demonstrate that using simplified prompts (without aspect-specific descriptions) generally led to decreased performance in InternVL-2.5-78B-MPO. The performance decline was particularly pronounced in several categories: Temporal Quality subaspects, Structural Correctness, Technical Quality, and Light & Color. However, other aspects maintain relatively stable performance levels. This phenomenon suggests that it is generally beneficial to provide detailed description of the aspect to evaluate. Results of other models also showcase an advantage of detailed aspect description on the average performance (Appendix B.1). Video Pair Comparison. As shown in Table 5, the MLLMs performance in video pair comparison exhibits similar pattern as single video rating: They achieve relative higher accuracy when evaluating Dynamic Degree subaspects, Technical Quality, Aesthetic Quality and Appearance Alignment, while showing weakness in assessing Structural Correctness, Motion Naturalness and Motion Alignment. Additionally, the significant performance gap of over 30 points between SOTA MLLMs and human evaluators indicates that current MLLMs have not yet achieved reliable pairwise video quality assessment capabilities. 5.3. Effect of Prompting Strategy We investigate how different prompting strategies affect model performance by modifying our original prompt templates  (Table 1)  . We test two variations: (1) removing the aspect-specific descriptions while maintaining only the question and answer instructions, and (2) for video pair comparisons, eliminating video order indicators (The first/second video). The latter modification was motivated by Zhang et al. (2024f), which emphasizes the importance of specifying relative order in image quality comparisons. The modified prompts can be found in the supplementary material. Video Pair Comparison. Similarly, Figure 2 (b) shows that simplified prompts result in reduced performance for video pair comparisons, reinforcing the value of detailed aspect descriptions in the prompts. Interestingly, contrary to established findings in image quality comparison (Zhang et al., 2024f), removing video order indicators does not significantly impact performance. We attribute this robustness to the enhanced temporal order understanding capabilities of recent MLLMs. 5.4. Effect of Scoring Strategy In the previous experiments, we adopt yes/no as the default scoring tokens and calculate single video ratings using the yes probability according to Eq. 1. Here, we explore three different scoring strategies: (1) using good/bad as scoring tokens, (2) an adaptive approach that alternates between yes/no and good/bad tokens, and (3) prompting the MLLM to directly generate rating scores as texts, ranging from 0 to 100. The detailed prompts are shown in the supplementary material. The results of Qwen2-VL-7B, presented in Figure 3, reveal that: (1) good/bad demonstrates superior performance for Temporal Quality and Static Quality assessments, while yes/no performs better for Dynamic Degree evaluation. (2) 7 UVE: Are MLLMs Unified Evaluators for AI-Generated Videos? (a) Single Video Rating. (b) Video Pair Comparison. Figure 5. Results of direct video pair comparison versus adapting single video rating to pairwise comparison. Figure 4. Results with varying numbers of video frames. When the frame number equals to 0, MLLMs receive pure-text prompt without visual input. The adaptive strategy yields the best overall results for Qwen2-VL-7B. However, for other advanced MLLMs like InternVL-2.5-78B-MPO, the improvement from adaptive scoring is minimal (see Appendix B.2). (3) Directly prompting MLLMs to generate rating scores performs significantly worse than probability-based scoring methods. These findings suggest that using yes/no as unified scoring tokens strikes good balance between simplicity and effectiveness. 5.5. Effect of Frame Number Figure 4 illustrates the impact of varying video frame numbers on model performance. We can see that: (1) In the zero-frame scenario (pure-text input), MLLMs perform poorly as their outputs remain constant regardless of visual information, confirming that language shortcuts cannot be exploited to achieve good performance in UVE-Bench. (2) As the frame count increases, aspects requiring temporal understanding (Light & Color) show more consistent improvement compared to aspects focusing on individual frame information (Aesthetic Quality). (3) The 72B-scale models demonstrate more consistent improvement in average accuracy with increasing frame numbers compared to their 7B-scale counterparts. This suggests that larger models possess superior capabilities in processing and integrating information from additional video frames. 5.6. Adapting Single Ratings for Pairwise Comparison While our previous experiments involve direct pairwise video comparisons by simultaneously feeding two videos into MLLMs, we now explore an alternative approach. This method first independently rates each video using Eq. 1 and then converts these individual ratings into four-way selection from set O. The detailed conversion methodology is discussed in Appendix A.3. As shown in Figure 5, adaptation from single video rating brings substantial improvement to Qwen2-VL-7B compared to directly performing video pair comparison. However, Qwen2-VL-72B shows only minimal improvements under the same approach. Similar patterns were observed with InternVL-2.5-MPO, as shown in Appendix B.3. We hypothesize that smaller-scale models, like the 7B variant, have inherently weaker pairwise comparison capabilities, and adapting from single video rating bypasses this limitation. Based on these findings, we recommend implementing the single-to-pairwise adaptation strategy particularly for MLLMs with limited pairwise comparison capabilities. 6. Conclusion and Future Work This study investigates the potential of MLLMs to serve as unified evaluators for AI-generated videos. To answer this question, we introduce the UVE-Bench, benchmark designed to assess the capability of unified AIGV evaluation. Compared to existing AIGV datasets, UVE-Bench highlights (1) comprehensive evaluation aspects, (2) videos produced by SOTA video generative models and (3) reliable evaluation of both single video rating and video pair comparison. Based on UVE-Bench, we extensively evaluates 16 MLLMs. Our findings reveal that while SOTA MLLMs 8 UVE: Are MLLMs Unified Evaluators for AI-Generated Videos? cannot fully replace human evaluators, they significantly outperform existing specialized evaluation methods, showcasing promising potential as unified AIGV evaluators. Our analytical studies highlight several design choices, including: providing detailed aspect-specific description in the prompts (5.3), using yes/no as the scoring tokens for single video rating (5.4) and adapting single video rating for pairwise comparison when using 7B-scale MLLMs (5.6). Z., Wei, X., Weng, Q., Wu, F., Xiong, Y., Zhao, X., and et al. Internlm2 technical report. CoRR, abs/2403.17297, 2024. Caron, M., Touvron, H., Misra, I., Jegou, H., Mairal, J., Bojanowski, P., and Joulin, A. Emerging properties in self-supervised vision transformers. In ICCV, pp. 9630 9640. IEEE, 2021. Future work of this study will extend to additional evaluation scenarios, including safety, ethics, and bias assessment, as well as diverse video generation conditions such as imageto-video and video-to-video generation. Chen, H., Xia, M., He, Y., Zhang, Y., Cun, X., Yang, S., Xing, J., Liu, Y., Chen, Q., Wang, X., et al. Videocrafter1: Open diffusion models for high-quality video generation. arXiv preprint arXiv:2310.19512, 2023a."
        },
        {
            "title": "Impact Statement",
            "content": "The evaluation of video generative models (VGMs) currently relies predominantly on human assessment (Polyak et al., 2024; Kong et al., 2024) or qualitative analysis (Brooks et al., 2024). Our research demonstrates that multimodal large language models (MLLMs) could potentially serve as unified evaluators for AI-generated videos. The transition from human to automated evaluation frameworks would have significant dual implications: On the one hand, this shift would substantially reduce evaluation costs, thereby accelerating the development cycle of VGMs. On the other hand, if VGMs are optimized solely based on model feedback, there is potential risk of divergence between the automated evaluators preferences and human values. Therefore, it essential to carefully scrutinize the correlation between automatic metrics and humans in the future research on AIGV evaluation."
        },
        {
            "title": "References",
            "content": "Bansal, H., Lin, Z., Xie, T., Zong, Z., Yarom, M., Bitton, Y., Jiang, C., Sun, Y., Chang, K.-W., and Grover, A. Videophy: Evaluating physical commonsense for video generation. arXiv preprint arXiv:2406.03520, 2024. Brooks, T., Peebles, B., Holmes, C., DePue, W., Guo, Y., Jing, L., Schnurr, D., Taylor, J., Luhman, T., Luhman, E., Ng, C., Wang, R., and Ramesh, A. Video generation models as world simulators. 2024. Cai, Z., Cao, M., Chen, H., Chen, K., Chen, K., Chen, X., Chen, X., Chen, Z., Chen, Z., Chu, P., Dong, X., Duan, H., Fan, Q., Fei, Z., Gao, Y., Ge, J., Gu, C., Gu, Y., Gui, T., Guo, A., Guo, Q., He, C., Hu, Y., Huang, T., Jiang, T., Jiao, P., Jin, Z., Lei, Z., Li, J., Li, J., Li, L., Li, S., Li, W., Li, Y., Liu, H., Liu, J., Hong, J., Liu, K., Liu, K., Liu, X., Lv, C., Lv, H., Lv, K., Ma, L., Ma, R., Ma, Z., Ning, W., Ouyang, L., Qiu, J., Qu, Y., Shang, F., Shao, Y., Song, D., Song, Z., Sui, Z., Sun, P., Sun, Y., Tang, H., Wang, B., Wang, G., Wang, J., Wang, J., Wang, R., Wang, Y., Wang, Chen, L., Wei, X., Li, J., Dong, X., Zhang, P., Zang, Y., Chen, Z., Duan, H., Lin, B., Tang, Z., Yuan, L., Qiao, Y., Lin, D., Zhao, F., and Wang, J. Sharegpt4video: Improving video understanding and generation with better captions. ArXiv preprint, abs/2406.04325, 2024a. Chen, Z., Wu, J., Wang, W., Su, W., Chen, G., Xing, S., Zhong, M., Zhang, Q., Zhu, X., Lu, L., Li, B., Luo, P., Lu, T., Qiao, Y., and Dai, J. Internvl: Scaling up vision foundation models and aligning for generic visuallinguistic tasks. CoRR, abs/2312.14238, 2023b. Chen, Z., Wang, W., Cao, Y., Liu, Y., Gao, Z., Cui, E., Zhu, J., Ye, S., Tian, H., Liu, Z., et al. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271, 2024b. Cheng, Z., Leng, S., Zhang, H., Xin, Y., Li, X., Chen, G., Zhu, Y., Zhang, W., Luo, Z., Zhao, D., and Bing, L. Videollama 2: Advancing spatial-temporal modeling and audio understanding in video-llms. CoRR, abs/2406.07476, 2024. Chiang, W.-L., Li, Z., Lin, Z., Sheng, Y., Wu, Z., Zhang, H., Zheng, L., Zhuang, S., Zhuang, Y., Gonzalez, J. E., Stoica, I., and Xing, E. P. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. URL https://lmsys.org/blog/ 2023-03-30-vicuna/. Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan, A., Goyal, A., Hartshorn, A., Yang, A., Mitra, A., Sravankumar, A., Korenev, A., Hinsvark, A., Rao, A., Zhang, A., Rodriguez, A., Gregerson, A., Spataru, A., Rozi`ere, B., Biron, B., Tang, B., Chern, B., Caucheteux, C., Nayak, C., Bi, C., Marra, C., McConnell, C., Keller, C., Touret, C., Wu, C., Wong, C., Ferrer, C. C., Nikolaidis, C., Allonsius, D., Song, D., Pintz, D., Livshits, D., Esiobu, D., Choudhary, D., Mahajan, D., Garcia-Olano, D., Perino, D., Hupkes, D., Lakomkin, E., AlBadawy, E., Lobanova, E., Dinan, E., Smith, E. M., Radenovic, F., Zhang, F., 9 UVE: Are MLLMs Unified Evaluators for AI-Generated Videos? Synnaeve, G., Lee, G., Anderson, G. L., Nail, G., Mialon, G., Pang, G., Cucurell, G., Nguyen, H., Korevaar, H., Xu, H., Touvron, H., Zarov, I., Ibarra, I. A., Kloumann, I. M., Misra, I., Evtimov, I., Copet, J., Lee, J., Geffert, J., Vranes, J., Park, J., Mahadeokar, J., Shah, J., van der Linde, J., Billock, J., Hong, J., Lee, J., Fu, J., Chi, J., Huang, J., Liu, J., Wang, J., Yu, J., Bitton, J., Spisak, J., Park, J., Rocca, J., Johnstun, J., Saxe, J., Jia, J., Alwala, K. V., Upasani, K., Plawiak, K., Li, K., Heafield, K., Stone, K., and et al. The llama 3 herd of models. CoRR, abs/2407.21783, 2024. Gao, P., Zhang, R., Liu, C., Qiu, L., Huang, S., Jin, P., Lin, W., Zhao, S., Geng, S., Lin, Z., J., Zhang, K., Shao, W., Xu, C., He, C., He, Shao, H., Lu, P., Li, H., and Qiao, Y. Sphinx-x: Scaling data and parameters for family of multimodal large language models. ArXiv, abs/2402.05935, 2024. URL https://api.semanticscholar. org/CorpusID:267547619. Jiang, D., He, X., Zeng, H., Wei, C., Ku, M., Liu, Q., and Chen, W. MANTIS: interleaved multi-image instruction tuning. CoRR, abs/2405.01483, 2024. Ke, J., Wang, Q., Wang, Y., Milanfar, P., and Yang, F. MUSIQ: multi-scale image quality transformer. In ICCV, pp. 51285137. IEEE, 2021. Kong, W., Tian, Q., Zhang, Z., Min, R., Dai, Z., Zhou, J., Xiong, J., Li, X., Wu, B., Zhang, J., et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. Kou, T., Liu, X., Zhang, Z., Li, C., Wu, H., Min, X., Zhai, G., and Liu, N. Subjective-aligned dateset and metric for text-to-video quality assessment. arXiv preprint arXiv:2403.11956, 2024. Kuaishou. Kling ai. https://klingai.kuaishou.com/, 2024. LAION-AI. aesthetic-predictor, 2022. Gupta, A., Yu, L., Sohn, K., Gu, X., Hahn, M., Fei-Fei, L., Essa, I., Jiang, L., and Lezama, J. Photorealistic video generation with diffusion models. arXiv preprint arXiv:2312.06662, 2023. Li, B., Zhang, Y., Guo, D., Zhang, R., Li, F., Zhang, H., Zhang, K., Li, Y., Liu, Z., and Li, C. Llava-onevision: Easy visual task transfer. ArXiv preprint, abs/2408.03326, 2024. He, X., Jiang, D., Zhang, G., Ku, M., Soni, A., Siu, S., Chen, H., Chandra, A., Jiang, Z., Arulraj, A., et al. Videoscore: Building automatic metrics to simulate finegrained human feedback for video generation. arXiv preprint arXiv:2406.15252, 2024. Ho, J., Jain, A., and Abbeel, P. Denoising diffusion probabilistic models. In NeurIPS, 2020. Ho, J., Chan, W., Saharia, C., Whang, J., Gao, R., Gritsenko, A., Kingma, D. P., Poole, B., Norouzi, M., Fleet, D. J., et al. Imagen video: High definition video generation with diffusion models. arXiv preprint arXiv:2210.02303, 2022a. Ho, J., Salimans, T., Gritsenko, A. A., Chan, W., Norouzi, M., and Fleet, D. J. Video diffusion models. In NeurIPS, 2022b. Huang, Z., He, Y., Yu, J., Zhang, F., Si, C., Jiang, Y., Zhang, Y., Wu, T., Jin, Q., Chanpaisit, N., Wang, Y., Chen, X., Wang, L., Lin, D., Qiao, Y., and Liu, Z. Vbench: Comprehensive benchmark suite for video generative models. In CVPR, pp. 2180721818. IEEE, 2024. Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D. S., de Las Casas, D., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., Lavaud, L. R., Lachaux, M., Stock, P., Scao, T. L., Lavril, T., Wang, T., Lacroix, T., and Sayed, W. E. Mistral 7b. CoRR, abs/2310.06825, 2023. Li, J., Li, D., Savarese, S., and Hoi, S. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International conference on machine learning, pp. 1973019742. PMLR, 2023a. Li, K., Wang, Y., He, Y., Li, Y., Wang, Y., Liu, Y., Wang, Z., Xu, J., Chen, G., Luo, P., Wang, L., and Qiao, Y. Mvbench: comprehensive multi-modal video understanding benchmark. ArXiv, abs/2311.17005, 2023b. URL https://api.semanticscholar. org/CorpusID:265466214. Li, X., Chu, W., Wu, Y., Yuan, W., Liu, F., Zhang, Q., Li, F., Feng, H., Ding, E., and Wang, J. Videogen: referenceguided latent diffusion approach for high definition textto-video generation. arXiv preprint arXiv:2309.00398, 2023c. Li, Z., Zhu, Z., Han, L., Hou, Q., Guo, C., and Cheng, M. AMT: all-pairs multi-field transforms for efficient frame interpolation. In CVPR, pp. 98019810. IEEE, 2023d. Lin, B., Zhu, B., Ye, Y., Ning, M., Jin, P., and Yuan, L. Video-llava: Learning united visual representation by alignment before projection. ArXiv, abs/2311.10122, 2023. URL https://api.semanticscholar. org/CorpusID:265281544. Lin, Z., Pathak, D., Li, B., Li, J., Xia, X., Neubig, G., Zhang, P., and Ramanan, D. Evaluating text-to-visual generation with image-to-text generation. In ECCV (9), 10 UVE: Are MLLMs Unified Evaluators for AI-Generated Videos? volume 15067 of Lecture Notes in Computer Science, pp. 366384. Springer, 2024. instruction tuning. Liu, H., Li, C., Wu, Q., and Lee, Y. J. ViArXiv, abs/2304.08485, sual 2023a. URL https://api.semanticscholar. org/CorpusID:258179774. Liu, H., Li, C., Li, Y., Li, B., Zhang, Y., Shen, S., Improved reasonJanuary 2024a. https://llava-vl.github.io/blog/ and Lee, Y. J. ing, ocr, URL 2024-01-30-llava-next/. and world knowledge, Llava-next: Liu, Y., Li, S., Wu, Y., Chen, C. W., Shan, Y., and Qie, X. transformers Umt: Unified multi-modal for joint video moment retrieval and highlight detec2022 IEEE/CVF Conference on Computer Vition. sion and Pattern Recognition (CVPR), pp. 30323041, 2022. URL https://api.semanticscholar. org/CorpusID:247627801. Liu, Y., Li, L., Ren, S., Gao, R., Li, S., Chen, S., Sun, X., and Hou, L. FETV: benchmark for fine-grained evaluation of open-domain text-to-video generation. In NeurIPS, 2023b. Liu, Y., Cun, X., Liu, X., Wang, X., Zhang, Y., Chen, H., Liu, Y., Zeng, T., Chan, R., and Shan, Y. Evalcrafter: Benchmarking and evaluating large video generation models. In CVPR, pp. 2213922149. IEEE, 2024b. Liu, Y., Li, S., Liu, Y., Wang, Y., Ren, S., Li, L., Chen, S., Sun, X., and Hou, L. TempCompass: Do video LLMs really understand videos? In Ku, L.-W., Martins, A., and Srikumar, V. (eds.), Findings of the Association for Computational Linguistics ACL 2024, pp. 87318772, 2024c. LumaLabs. Dream machine. https://lumalabs.ai/dreammachine,, 2024. Mialon, G., Fourrier, C., Swift, C., Wolf, T., LeCun, Y., and Scialom, T. Gaia: benchmark for general ai assistants. arXiv preprint arXiv:2311.12983, 2023. OpenAI. Gpt-4 technical report. ArXiv, abs/2303.08774, 2023. URL https://api.semanticscholar. org/CorpusID:266362871. OpenAI. Gpt-4o system card, 2024. Polyak, A., Zohar, A., Brown, A., Tjandra, A., Sinha, A., Lee, A., Vyas, A., Shi, B., Ma, C.-Y., Chuang, C.-Y., et al. Movie gen: cast of media foundation models. arXiv preprint arXiv:2410.13720, 2024. 11 Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., and Sutskever, I. Learning transferable visual models from natural language supervision. In ICML, volume 139 of Proceedings of Machine Learning Research, pp. 87488763. PMLR, 2021. Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., and Chen, M. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. High-resolution image synthesis with latent diffusion models. In CVPR, 2022. Runway. Gen-3. https://runwayml.com/research/introducinggen-3-alpha, 2024. Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E. L., Ghasemipour, K., Gontijo Lopes, R., Karagol Ayan, B., Salimans, T., et al. Photorealistic text-to-image diffusion models with deep language understanding. In NeurIPS, 2022. Shangguan, Z., Li, C., Ding, Y., Zheng, Y., Zhao, Y., Fitzgerald, T., and Cohan, A. Tomato: Assessing visual temporal reasoning capabilities in multimodal foundation models. arXiv preprint arXiv:2410.23266, 2024. Song, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Ermon, S., and Poole, B. Score-based generative modeling through stochastic differential equations. In ICLR, 2021. Team, G. Mochi 1. https://github.com/ genmoai/models, 2024. Teed, Z. and Deng, J. RAFT: recurrent all-pairs field transforms for optical flow. In ECCV (2), volume 12347 of Lecture Notes in Computer Science, pp. 402419. Springer, 2020. Wang, J., Duan, H., Zhai, G., Wang, J., and Min, X. Aigvassessor: Benchmarking and evaluating the perceptual quality of text-to-video generation with lmm. arXiv preprint arXiv:2411.17221, 2024a. Wang, J., Wu, B., Jiang, H., Xun, Z., Xiao, X., Guo, H., and Xiao, J. World to code: Multi-modal data generation via self-instructed compositional captioning and filtering. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 46084623, 2024b. Wang, P., Bai, S., Tan, S., Wang, S., Fan, Z., Bai, J., Chen, K., Liu, X., Wang, J., Ge, W., Fan, Y., Dang, K., Du, M., Ren, X., Men, R., Liu, D., Zhou, C., Zhou, J., and Lin, J. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024c. UVE: Are MLLMs Unified Evaluators for AI-Generated Videos? Wang, Y., He, Y., Li, Y., Li, K., Yu, J., Ma, X., Li, X., Chen, G., Chen, X., Wang, Y., Luo, P., Liu, Z., Wang, Y., Wang, L., and Qiao, Y. Internvid: large-scale video-text dataset for multimodal understanding and generation. In ICLR. OpenReview.net, 2024d. Wu, H., Zhang, Z., Zhang, W., Chen, C., Liao, L., Li, C., Gao, Y., Wang, A., Zhang, E., Sun, W., Yan, Q., Min, X., Zhai, G., and Lin, W. Q-align: Teaching lmms for visual scoring via discrete text-defined levels. In ICML. OpenReview.net, 2024a. Wu, H., Zhu, H., Zhang, Z., Zhang, E., Chen, C., Liao, L., Li, C., Wang, A., Sun, W., Yan, Q., Liu, X., Zhai, G., Wang, S., and Lin, W. Towards open-ended visual quality comparison. In ECCV (3), volume 15061 of Lecture Notes in Computer Science, pp. 360377. Springer, 2024b. Wu, J. Z., Fang, G., Wu, H., Wang, X., Ge, Y., Cun, X., Zhang, D. J., Liu, J., Gu, Y., Zhao, R., Lin, W., Hsu, W., Shan, Y., and Shou, M. Z. Towards better metric for text-to-video generation. CoRR, abs/2401.07781, 2024c. Yang, A., Yang, B., Hui, B., Zheng, B., Yu, B., Zhou, C., Li, C., Li, C., Liu, D., Huang, F., Dong, G., Wei, H., Lin, H., Tang, J., Wang, J., Yang, J., Tu, J., Zhang, J., Ma, J., Yang, J., Xu, J., Zhou, J., Bai, J., He, J., Lin, J., Dang, K., Lu, K., Chen, K., Yang, K., Li, M., Xue, M., Ni, N., Zhang, P., Wang, P., Peng, R., Men, R., Gao, R., Lin, R., Wang, S., Bai, S., Tan, S., Zhu, T., Li, T., Liu, T., Ge, W., Deng, X., Zhou, X., Ren, X., Zhang, X., Wei, X., Ren, X., Liu, X., Fan, Y., Yao, Y., Zhang, Y., Wan, Y., Chu, Y., Liu, Y., Cui, Z., Zhang, Z., Guo, Z., and Fan, Z. Qwen2 technical report. CoRR, abs/2407.10671, 2024a. Yang, Z., Teng, J., Zheng, W., Ding, M., Huang, S., Xu, J., Yang, Y., Hong, W., Zhang, X., Feng, G., et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024b. Yao, Y., Yu, T., Zhang, A., Wang, C., Cui, J., Zhu, H., Cai, T., Li, H., Zhao, W., He, Z., Chen, Q., Zhou, H., Zou, Z., Zhang, H., Hu, S., Zheng, Z., Zhou, J., Cai, J., Han, X., Zeng, G., Li, D., Liu, Z., and Sun, M. Minicpm-v: GPT4V level MLLM on your phone. CoRR, abs/2408.01800, 2024. Ye, J., Xu, H., Liu, H., Hu, A., Yan, M., Qian, Q., Zhang, J., Huang, F., and Zhou, J. mplug-owl3: Towards long image-sequence understanding in multi-modal large language models. CoRR, abs/2408.04840, 2024. Yu, L., Cheng, Y., Sohn, K., Lezama, J., Zhang, H., Chang, H., Hauptmann, A. G., Yang, M.-H., Hao, Y., Essa, I., et al. Magvit: Masked generative video transformer. In CVPR, 2023. 12 Zeng, A., Yang, Y., Chen, W., and Liu, W. The dawn of video generation: Preliminary explorations with sora-like models. CoRR, abs/2410.05227, 2024. Zhai, X., Mustafa, B., Kolesnikov, A., and Beyer, L. Sigmoid loss for language image pre-training. In ICCV, pp. 1194111952. IEEE, 2023. Zhang, P., Zhang, K., Li, B., Zeng, G., Yang, J., Zhang, Y., Wang, Z., Tan, H., Li, C., and Liu, Z. Long context transfer from language to vision. ArXiv preprint, abs/2406.16852, 2024a. Zhang, Y., Wu, J., Li, W., Li, B., Ma, Z., Liu, Z., and Li, C. Video instruction tuning with synthetic data. CoRR, abs/2410.02713, 2024b. Zhang, Z., Jia, Z., Wu, H., Li, C., Chen, Z., Zhou, Y., Sun, W., Liu, X., Min, X., Lin, W., et al. Q-bench-video: Benchmarking the video quality understanding of lmms. arXiv preprint arXiv:2409.20063, 2024c. Zhang, Z., Li, X., Sun, W., Jia, J., Min, X., Zhang, Z., Li, C., Chen, Z., Wang, P., Ji, Z., et al. Benchmarking aigc video quality assessment: dataset and unified model. arXiv preprint arXiv:2407.21408, 2024d. Zhang, Z., Wu, H., Zhang, E., Zhai, G., and Lin, W. Qbench$ˆ+$+: benchmark for multi-modal foundation models on low-level vision from single images to pairs. IEEE Trans. Pattern Anal. Mach. Intell., 46(12):10404 10418, 2024e. Zhang, Z., Zhou, Y., Li, C., Zhao, B., Liu, X., and Zhai, G. Quality assessment in the era of large models: survey. CoRR, abs/2409.00031, 2024f. Zhao, R., Gu, Y., Wu, J. Z., Zhang, D. J., Liu, J., Wu, W., Keppo, J., and Shou, M. Z. Motiondirector: Motion customization of text-to-video diffusion models. In ECCV (56), volume 15114 of Lecture Notes in Computer Science, pp. 273290. Springer, 2024. Zheng, Z., Peng, X., Yang, T., Shen, C., Li, S., Liu, H., Zhou, Y., Li, T., and You, Y. Open-sora: Democratizing efficient video production for all, March 2024. URL https: //github.com/hpcaitech/Open-Sora. Zhu, B., Lin, B., Ning, M., Yan, Y., Cui, J., Wang, H., Pang, Y., Jiang, W., Zhang, J., Li, Z., Zhang, C., Li, Z., Liu, W., and Yuan, L. Languagebind: Extending video-language pretraining to n-modality by language-based semantic alignment. In ICLR. OpenReview.net, 2024. UVE: Are MLLMs Unified Evaluators for AI-Generated Videos? A. More Details of Experimental Settings A.1. Evaluated Systems MLLMs. We conduct zero-shot evaluations on 16 MLLMs, the detailed information of which is summarized in Table 6. VideoScore-v1.1. (He et al., 2024) This model is fine-tuned version of Mantis-Idefics2-8B (Jiang et al., 2024), trained to imitate human rating scores. In their work, He et al. (2024) gathered human ratings (on discrete scale from 1 to 4) for five key evaluation aspects: Visual Quality, Temporal Consistency, Dynamic Degree, Text-to-Video Alignment, and Factual Consistency. The VideoScore model is trained using regression approach to predict these human rating scores. According to the definition of these aspects, we associate them with 8 out of the 15 aspects in UVE-Bench, as detailed in Table 7. VBench. (Huang et al., 2024) VBench is benchmark suite that comprehensively evaluates 16 aspects of AIGV using off-the-shelf models and specifically designed rules. As shown in Table 7, we identified an overlap of 6 aspects between VBench and our UVE-Bench, and we adopted the VBench metrics to evaluate these 6 aspects in UVE-Bench. A.2. Evaluation Criteria for Single Video Rating We rewrite the evaluation criteria for single video rating in Eq. 3 as follows: Asingle = 1(S1 > S2) 1(S1 < S2) fc(S1β) fc(S2β) c(S1α) fc(S2α) if = V1better elif = V2better elif = same good elif = same bad (4) fc(Sβ) and good and same bad, respectively: c(Sα) exponentially decays from 1 to 0 when the predicted rating score gradually deviates from same fc(Sβ) = (cid:40) 1 es(βS) if β < <= 1 elif 0 =< <= β (cid:40) c(Sα) = 1 es(Sα) if 0 <= < α elif α =< <= 1 (5) (6) where is the coefficient that controls the speed of decaying. The graph of fc(Sβ) and c(Sα) are illustrated in Figure 6. A.3. Methodology for Adapting Single Video Rating for Pairwise Comparison We first independently rate the two videos using Eq. 1 to obtain the rating scores S1 and S2. Based on these scores, we then make choice from O: = where f1({V1better, V2better}, S1, S2) if S1 S2 > τ or α < S1 < β or α < S2 < β f1({same good, same bad}, S1, S2) else f1({C1, C2}, S1, S2) = (cid:40) C1 if S1 > S2 else f2({C1, C2}, S1, S2) = (cid:40) C1 C2 if S1 > β and S2 > β elif S1 < α and S2 < α (7) (8) (9) In the above equations, α < β are the thresholds for bad and good videos, respectively. τ is threshold of the difference between S1 and S2, which controls whether we should select from {V1better, V2better} or {same good, same bad}. By default, we set α = 0.4, β = 0.8, τ = 0.05. 13 UVE: Are MLLMs Unified Evaluators for AI-Generated Videos? Model Model Size LLM Vision Encoder Frames per Video Video-LLaVA (Lin et al., 2023) LongVA-7B-DPO (Zhang et al., 2024a) ShareGPT4Video (Chen et al., 2024a) VideoLLaMA2.1 (Cheng et al., 2024) mPLUG-Owl3-7B (Ye et al., 2024) VideoChat2-Mistral (Li et al., 2023b) MiniCPM-V-2.6 (Yao et al., 2024) LLaVA-OneVision-7B (Li et al., 2024) LLaVA-OneVision-72B (Li et al., 2024) LLaVA-Video-7B (Zhang et al., 2024b) LLaVA-Video-72B (Zhang et al., 2024b) Qwen2-VL-7B (Wang et al., 2024c) Qwen2-VL-72B (Wang et al., 2024c) InternVL-2.5-8B (Chen et al., 2024b) InternVL-2.5-78B (Chen et al., 2024b) GPT-4o-2024-08-06 (OpenAI, 2024) 7B 7B 8B 7B 7B 7B 7B 7B 72B 7B 72B 7B 72B 8B 78B - Vicuna-1.5 (Chiang et al., 2023) Qwen2 (Yang et al., 2024a) LLaMA3 (Dubey et al., 2024) Qwen2 (Yang et al., 2024a) Qwen2 (Yang et al., 2024a) Mistral (Jiang et al., 2023) Qwen2 (Yang et al., 2024a) Qwen2 (Yang et al., 2024a) Qwen2 (Yang et al., 2024a) Qwen2 (Yang et al., 2024a) Qwen2 (Yang et al., 2024a) - - InternLM2.5 (Cai et al., 2024) InternLM2.5 (Cai et al., 2024) - LanguageBind (Zhu et al., 2024) CLIP-ViT-L-336px (Radford et al., 2021) CLIP-ViT-L-336px (Radford et al., 2021) SigLip-400M (Zhai et al., 2023) SigLip-400M (Zhai et al., 2023) UMT-L/16 (Liu et al., 2022) SigLip-400M (Zhai et al., 2023) SigLip-400M (Zhai et al., 2023) SigLip-400M (Zhai et al., 2023) SigLip-400M (Zhai et al., 2023) SigLip-400M (Zhai et al., 2023) - - InternViT (Chen et al., 2023b) InternViT (Chen et al., 2023b) - 8 16 16 16 16 16 16 12,16 12,16 12,16 12,16 12,16 12,16 12,16 12,16 12,16 Table 6. Summary of MLLMs evaluated in the experiments. B. More Experimental Results B.1. Effect of Prompting Strategy Figure B.1 presents the results of different prompting strategy with InternVL-2.5-8B-MPO, Qwen2-VL-7B and Qwen2VL-72B. Unlike InternVL-2.5-78B-MPO (previously presented in Figure 2), these three models showed less pronounced advantage when using detailed aspect-specific descriptions, with the simplified prompt achieving better performance in several aspects. We conjecture that this is because these three models cannot comprehend the detailed description as effectively as InternVL-2.5-78B-MPO. Nevertheless, when considering overall performance, the full prompt consistently maintained slight improvement over the simplified version across all three models. Consistent with our findings for InternVL-2.5-78B-MPO, removing video order indicator does not result in significant performance change. B.2. Effect of Scoring Strategy In 5.4, we analyze the effect of different scoring strategies with Qwen2-VL-7B. Here, we extended the investigation to three additional models: Qwen2-VL-72B, InternVL-2.5-8B-MPO, and InternVL-2.5-78B. The results are presented in Figure 8. Our findings reveal consistent patterns across all models. Similar to Qwen2-VL-7B, the good/bad and yes/no scoring strategies demonstrate distinct strengths in different evaluation aspects. Direct score generation through MLLMs consistently yields suboptimal results across all models. However, compared with Qwen2-VL-7B, the advantage of adaptive scoring token is less pronounced in Qwen2-VL-72B, InternVL-2.5-8B-MPO and InternVL-2.5-78B. This suggests that these three models are more robust to the variation of scoring tokens. These findings confirm that using yes/no as unified scoring token is simple yet effective. B.3. Adapting Single Ratings for Pairwise Comparison The effectiveness of adapting single video ratings for video pair comparisons using InternVL-2.5 models is illustrated in Figure 9. The results aligns with our observations with Qwen2-VL (shown in Figure 5), revealing model size-dependent pattern. While this adaptation strategy shows no benefit when applied to the 78B-scale model, it yields substantial improvements for the 8B-scale model. B.4. Case Studies Table 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18 showcase UVE-Bench data examples of different subaspects, along with the evaluation results by InternVL-2.5-8B-MPO and InternVL-2.5-78B-MPO. 14 UVE: Are MLLMs Unified Evaluators for AI-Generated Videos? Original Aspect VideoScore Visual Quality Definition Evaluation Model UVE-Bench Aspect the quality of the video in terms of clearness, resolution, brightness, and color VideoScore-v1.1 Overall Static Quality Aesthetic Quality Technical Quality Temporal Consistency the consistency of objects or humans in video VideoScore-v1.1 Appearance Consistency Dynamic Degree the degree of dynamic changes VideoScore-v1.1 Overall Dynamic Degree Text-to-Video Alignment the alignment between the text prompt and the video content VideoScore-v1.1 Overall Alignment Factual Consistency VBench Overall Consistency Motion Smoothness Aesthetic Quality the consistency of the video content with the common-sense and factual knowledge VideoScore-v1. Structural Correctness Motion Naturalness overall video-text consistency ViCLIP (Wang et al., 2024d) Overall Alignment whether the motion in the generated video is smooth and follows the physical law of the real world reflect aesthetic aspects such as the layout, the richness and harmony of colors, the photo-realism, naturalness, and artistic quality of the video frames AMT (Li et al., 2023d) Motion Naturalness LAION aesthetic predictor (LAION-AI, 2022) Aesthetic Quality Imaging Quality refers to the distortion (e.g., over-exposure, noise, blur) presented in the generated frames MUSIQ (Ke et al., 2021) Technical Quality Dynamic Degree the degree of dynamics (i.e., whether it contains large motions) RAFT (Teed & Deng, 2020) Overall Dynamic Degree Subject Consistency whether the subject appearance remains consistent throughout the whole video DINO (Caron et al., 2021) Appearance Consistency Table 7. Association between VideoScore and VBench aspects with UVE-Bench aspects. Figure 6. Functions fc(Sβ), c(Sα) used in the evaluation criteria of single video rating. UVE: Are MLLMs Unified Evaluators for AI-Generated Videos? (a) Single Video Rating with InternVL2.5-8B-MPO. (b) Video Pair Comparison with InternVL2.5-8B-MPO. (c) Single Video Rating with Qwen2-VL-7B. (d) Video Pair Comparison with Qwen2-VL-7B. (e) Single Video Rating with Qwen2-VL-72B. (f) Video Pair Comparison with Qwen2-VL-72B. Figure 7. Results of different prompting strategies with InternVL-2.5-8B-MPO, Qwen2-VL-7B and Qwen2-VL-72B. 16 UVE: Are MLLMs Unified Evaluators for AI-Generated Videos? (a) Qwen2-VL-72B. (b) InternVL-2.5-8B-MPO. Figure 8. Results of different scoring strategies for single video rating with Qwen2-VL-72B, InternVL-2.5-8B-MPO and InternVL-2.578B-MPO. (c) InternVL-2.5-78B-MPO. Figure 9. Results of direct video pair comparison versus adapting single video rating to pairwise comparison, using InternVL-2.5-MPO. 17 UVE: Are MLLMs Unified Evaluators for AI-Generated Videos? Video 1: Video 2: Aspect: Subject Motion (Dynamic Degree) Human Preference: same bad Single Video Rating InternVL-2.5-78B - Video 1: 0.076 - Video 2: 0.0028 - Asinlge = 1 InternVL-2.5-8B - Video 1: 0.085 - Video 2: 0.047 - Asinlge = 1 Video Pair Comparison InternVL-2.5-78B - Model Preference: same bad - Accuracy = InternVL-2.5-8B - Model Preference: Video 2 - Accuracy = Table 8. Data example of Subject Motion and corresponding evaluations by InternVL-2.5-78B-MPO and InternVL-2.5-8B-MPO. Video 1: Video 2: Aspect: Camera Motion (Dynamic Degree) Human Preference: Video 1 is better Single Video Rating InternVL-2.5-78B - Video 1: 1.2106 - Video 2: 8.8108 - Asinlge = InternVL-2.5-8B - Video 1: 1.5103 - Video 2: 3.4104 - Asinlge = 1 Video Pair Comparison InternVL-2.5-78B - Model Preference: same bad - Accuracy = InternVL-2.5-8B - Model Preference: same bad - Accuracy = Table 9. Data example of Camera Motion and evaluations by InternVL-2.5-78B-MPO and InternVL-2.5-8B-MPO. 18 UVE: Are MLLMs Unified Evaluators for AI-Generated Videos? Video 1: Video 2: Aspect: Light Change (Dynamic Degree) Human Preference: Video 1 is better Single Video Rating InternVL-2.5-78B - Video 1: 0.531 - Video 2: 0.004 - Asinlge = InternVL-2.5-8B - Video 1: 0.201 - Video 2: 0.047 - Asinlge = 1 Video Pair Comparison InternVL-2.5-78B - Model Preference: Video 1 - Accuracy = InternVL-2.5-8B - Model Preference: Video 1 - Accuracy = Table 10. Data example of Light Change and evaluations by InternVL-2.5-78B-MPO and InternVL-2.5-8B-MPO. Video 1: Video 2: Aspect: Technical Quality (Static Quality) Human Preference: Video 2 is better Single Video Rating InternVL-2.5-78B - Video 1: 0.500 - Video 2: 0.999 - Asinlge = 1 InternVL-2.5-8B - Video 1: 0.651 - Video 2: 0.940 - Asinlge = 1 Video Pair Comparison InternVL-2.5-78B - Model Preference: Video 2 - Accuracy = InternVL-2.5-8B - Model Preference: Video 2 - Accuracy = Table 11. Data example of Technical Quality and evaluations by InternVL-2.5-78B-MPO and InternVL-2.5-8B-MPO. 19 UVE: Are MLLMs Unified Evaluators for AI-Generated Videos? Video 1: Video 2: Aspect: Aesthetic Quality (Static Quality) Human Preference: same good Single Video Rating InternVL-2.5-78B - Video 1: 0.986 - Video 2: 0.991 - Asinlge = InternVL-2.5-8B - Video 1: 0.731 - Video 2: 0.755 - Asinlge = 0.319 Video Pair Comparison InternVL-2.5-78B - Model Preference: same good - Accuracy = InternVL-2.5-8B - Model Preference: Video 2 - Accuracy = Table 12. Data example of Aesthetic Quality and evaluations by InternVL-2.5-78B-MPO and InternVL-2.5-8B-MPO. Video 1: Video 2: Aspect: Structural Correctness (Static Quality) Human Preference: same bad Single Video Rating InternVL-2.5-78B - Video 1: 0.999 - Video 2: 0.971 - Asinlge = 8.6 106 InternVL-2.5-8B - Video 1: 0.971 - Video 2: 0.915 - Asinlge = 1.9 105 Video Pair Comparison InternVL-2.5-78B - Model Preference: Video 1 - Accuracy = InternVL-2.5-8B - Model Preference: Video 1 - Accuracy = Table 13. Data example of Structural Correctness and evaluations by InternVL-2.5-78B-MPO and InternVL-2.5-8B-MPO. 20 UVE: Are MLLMs Unified Evaluators for AI-Generated Videos? Video 1: Video 2: Aspect: Appearance Consistency (Static Quality) Human Preference: Video 1 is better Single Video Rating InternVL-2.5-78B - Video 1: 0.990 - Video 2: 0.893 - Asinlge = InternVL-2.5-8B - Video 1: 0.971 - Video 2: 0.980 - Asinlge = 0 Video Pair Comparison InternVL-2.5-78B - Model Preference: Video 1 - Accuracy = InternVL-2.5-8B - Model Preference: Video 2 - Accuracy = Table 14. Data example of Appearance Consistency and evaluations by InternVL-2.5-78B-MPO and InternVL-2.5-8B-MPO. Video 1: Video 2: Aspect: Temporal Flickering (Temporal Quality) Human Preference: Video 2 is better Single Video Rating InternVL-2.5-78B - Video 1: 0.852 - Video 2: 0.998 - Asinlge = 1 InternVL-2.5-8B - Video 1: 0.731 - Video 2: 0.905 - Asinlge = 1 Video Pair Comparison InternVL-2.5-78B - Model Preference: Video 2 - Accuracy = InternVL-2.5-8B - Model Preference: Video 2 - Accuracy = Table 15. Data example of Flickering and evaluations by InternVL-2.5-78B-MPO and InternVL-2.5-8B-MPO. 21 UVE: Are MLLMs Unified Evaluators for AI-Generated Videos? Video 1: Video 2: Aspect: Motion Naturalness (Temporal Quality) Human Preference: Video 1 is better Single Video Rating InternVL-2.5-78B - Video 1: 0.924 - Video 2: 0.986 - Asinlge = InternVL-2.5-8B - Video 1: 0.731 - Video 2: 0.798 - Asinlge = 0 Video Pair Comparison InternVL-2.5-78B - Model Preference: Video 2 - Accuracy = InternVL-2.5-8B - Model Preference: Video 2 - Accuracy = Table 16. Data example of Motion Naturalness and evaluations by InternVL-2.5-78B-MPO and InternVL-2.5-8B-MPO. Video 1: Video 2: Aspect: Appearance Alignment (Video-Text Alignment) Human Preference: Video 1 is better Single Video Rating InternVL-2.5-78B - Video 1: 0.999 - Video 2: 0.009 - Asinlge = 1 InternVL-2.5-8B - Video 1: 0.984 - Video 2: 0.755 - Asinlge = 1 Video Pair Comparison InternVL-2.5-78B - Model Preference: Video 1 - Accuracy = InternVL-2.5-8B - Model Preference: Video 1 - Accuracy = Table 17. Data example of Appearance Fine and evaluations by InternVL-2.5-78B-MPO and InternVL-2.5-8B-MPO. 22 UVE: Are MLLMs Unified Evaluators for AI-Generated Videos? Video 1: Video 2: Aspect: Motion Alignment (Video-Text Alignment) Human Preference: Video 1 is better Single Video Rating InternVL-2.5-78B - Video 1: 0.269 - Video 2: 0.245 - Asinlge = InternVL-2.5-8B - Video 1: 0.076 - Video 2: 0.268 - Asinlge = 0 Video Pair Comparison InternVL-2.5-78B - Model Preference: same good - Accuracy = InternVL-2.5-8B - Model Preference: Video 2 - Accuracy = Table 18. Data example of Motion Fine and evaluations by InternVL-2.5-78B-MPO and InternVL-2.5-8B-MPO."
        }
    ],
    "affiliations": [
        "ByteDance Seed",
        "Peking University",
        "School of Artificial Intelligence, University of Chinese Academy of Sciences"
    ]
}