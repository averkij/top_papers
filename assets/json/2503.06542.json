{
    "paper_title": "ARMOR v0.1: Empowering Autoregressive Multimodal Understanding Model with Interleaved Multimodal Generation via Asymmetric Synergy",
    "authors": [
        "Jianwen Sun",
        "Yukang Feng",
        "Chuanhao Li",
        "Fanrui Zhang",
        "Zizhen Li",
        "Jiaxin Ai",
        "Sizhuo Zhou",
        "Yu Dai",
        "Shenglin Zhang",
        "Kaipeng Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Unified models (UniMs) for multimodal understanding and generation have recently received much attention in the area of vision and language. Existing UniMs are designed to simultaneously learn both multimodal understanding and generation capabilities, demanding substantial computational resources, and often struggle to generate interleaved text-image. We present ARMOR, a resource-efficient and pure autoregressive framework that achieves both understanding and generation by fine-tuning existing multimodal large language models (MLLMs). Specifically, ARMOR extends existing MLLMs from three perspectives: (1) For model architecture, an asymmetric encoder-decoder architecture with a forward-switching mechanism is introduced to unify embedding space integrating textual and visual modalities for enabling natural text-image interleaved generation with minimal computational overhead. (2) For training data, a meticulously curated, high-quality interleaved dataset is collected for fine-tuning MLLMs. (3) For the training algorithm, we propose a ``what or how to generate\" algorithm to empower existing MLLMs with multimodal generation capabilities while preserving their multimodal understanding capabilities, through three progressive training stages based on the collected dataset. Experimental results demonstrate that ARMOR upgrades existing MLLMs to UniMs with promising image generation capabilities, using limited training resources. Our code will be released soon at https://armor.github.io."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 ] . [ 1 2 4 5 6 0 . 3 0 5 2 : r ARMOR v0.1: Empowering Autoregressive Multimodal Understanding"
        },
        {
            "title": "Model with Interleaved Multimodal Generation via Asymmetric Synergy",
            "content": "Jianwen Sun1,2* Yukang Feng1,2* Chuanhao Li5 Fanrui Zhang2,3 Zizhen Li1,2 Jiaxin Ai2,4 Sizhuo Zhou2,3 Yu Dai1 kaipeng Zhang2,5 Nankai University1 Shanghai Innovation Institute2 University of Science and Technology of China3 Wuhan University 4 Shanghai AI Laboratory5 https://armor.github.io Shenglin Zhang"
        },
        {
            "title": "Abstract",
            "content": "1. Introduction Unified models (UniMs) for multimodal understanding and generation have recently received much attention in the area of vision and language. Existing UniMs are designed to simultaneously learn both multimodal understanding and generation capabilities, demanding substantial computational resources, and often struggle to generate interleaved text-image. We present ARMOR, resourceefficient and pure autoregressive framework that achieves both understanding and generation by fine-tuning existing multimodal large language models (MLLMs). Specifically, ARMOR extends existing MLLMs from three perspectives: (1) For model architecture, an asymmetric encoder-decoder architecture with forward-switching mechanism is introduced to unify embedding space integrating textual and visual modalities for enabling natural text-image interleaved generation with minimal computational overhead. (2) For training data, meticulously curated, high-quality interleaved dataset is collected for fine-tuning MLLMs. (3) For the training algorithm, we propose what or how to generate algorithm to empower existing MLLMs with multimodal generation capabilities while preserving their multimodal understanding capabilities, through three progressive training stages based on the collected dataset. Experimental results demonstrate that ARMOR upgrades existing MLLMs to UniMs with promising image generation capabilities, using limited training resources. Our code will be released soon at https://armor.github.io. *Equal contribution Corresponding author. Unified understanding and generation is crucial direction in the development of vision-and-language models, requiring model to simultaneously handle understanding tasks (e.g., visual question answering) and generation tasks (e.g., text-to-image generation). Existing Unified Models (UniMs) for understanding and generation, such as Showo [41] and Janus-pro [7], are designed to simultaneously learn both multimodal understanding and generation capabilities. Despite their impressive performance, the training of such models demands substantial computational resources, which significantly hinders their scalability and ability to accommodate personalized modifications. Furthermore, most UniMs struggle to generate interleaved textimage. To this end, we present resource-efficient autoregressive framework named ARMOR, which fine-tunes existing multimodal large language model (MLLMs) to upgrade them to UniMs from three perspectives, including model architecture, training data and training algorithm. Sample generation from ARMOR is shown in Figure 1. We first introduce an asymmetric encoder-decoder architecture into existing MLLMs to enable them output natural interleaved text-image. Specifically, ARMOR fully retains the encoder and decoder in MLLMs while incorporating an asymmetric image decoder, as shown in Figure 2. In doing so, the strong semantic perception and understanding capabilities of the MLLMs are almost preserved, while the asymmetric image decoder enables image generation with minimal computational overhead. To alleviate the long-tail distribution problem across different modalities in the answer space, we propose forward-switching mechanism to dynamically control which modalitys answer space is used for prediction based on the model input. Then we collect meticulously curated, high-quality interleaved dataset, and propose What or How to Generate Figure 1. ARMOR can output images, text, or interleaved text-image. We show some examples of chat with ARMOR. (WoHG) training algorithm to fune-tune existing MLLMs with the collected dataset. The training algorithm consists of three training stages with different objectives: what to generate, how to generate and how to answer better. These different objectives are achieved by freezing different parameters and training on different types of data. In the first stage, MLLMs learn to decide the response modality, such as generating text or producing images. In the second stage, MLLMs specifically improve their shortcomingsimage generation capability, further enhancing the quality of the generated images. In the third stage, MLLMs refine their responses to better integrate text and visual modalities, providing high-quality text-image interleaved responses. The three-stage training algorithm ensures structured progression by first establishing modality awareness, then targeting capability gaps, and ultimately achieving holistic multimodal synergy through iterative refinement. To validate the effectiveness of the proposed ARMOR framework, we incorporate InternVL2.5 [8] into ARMOR (named Ours here), and conduct extensive experiments on 9 benchmarks to evaluate multimodal understanding and generation capabilities. Experimental results demonstrate that Ours outperforms existing UniMs in multimodal understanding with large margin (e.g., 78.8 and 62.6 in score for Ours and Janus-pro [7] on the MMB benchmark [26], respectively) while achieving comparable performance for multimodal generation (e.g., 0.37 and 0.39 in score for Ours and Chameleon [37] on the GenEval benchmark [17], respectively). Notably, ARMOR only introduces 7% more parameters for fine-tuning InternVL2.5, whereas existing UniMs require full parameter training from scratch. Our contributions are summarized as follows. (1) We propose the first framework to bulid UniMs by empowering existing MLLMs with generation capabilities in resource-efficient manner. (2) We present an asymmetric encoder-decoder architecture to enable natural text-image interleaved generation for MLLMs while minimizing computational overhead. (3) We collect high-quality dataset for training MLLMs, and propose WoHG training algorithm to upgrade existing MLLMs to UniMs while preserving their understanding capabilities through three progressive training stages on the collected dataset. (4) Extensive experimental results on 9 benchmarks demonstrate the effectiveness of our framework, further affirming the potential of fully autoregressive architecture for building UniMs. 2. Related Work 2.1. Multimodal Understanding CLIP [29] pioneers cross-modal alignment via contrastive learning, inspiring MLLMs that bridge vision encoders and large language models. Two dominant alignment strategies have emerged: 1) Explicit attention interaction, exemplified by BLIP-2s Q-Former [22] and Flamingos crossattention [1], enables deep vision-language fusion by projecting visual features into text-aligned tokens. 2) Implicit space mapping, as seen in InternVL [9] and Qwen2.5VL [3], transforms visual features into token sequences using MLPs for modality alignment. While differing in encoder choices, alignment designs, and training data strategies, most MLLMs share core components (e.g., pretrained vision backbones, lightweight adapters, and autoregressive text decoders), and continue to evolve through improved visual representations and more efficient alignment methods. 2.2. Visual Generation The field of visual generation has evolved through autoregressive and diffusion-based approaches. Early autoregressive models [6, 12, 27, 32] leverage Transformer for pixellevel text-to-image synthesis but face challenges in highresolution fidelity. Subsequent token-based methods [35] improve sequence prediction but remained limited in finegrained detail. In contrast, diffusion models like stable diffusion series [14, 28, 33], DALL-E 2 [31], and FLUX [19] gain prominence by iteratively denoising images to achieve state-of-the-art visual quality. These methods excel in generating high-fidelity, detailed outputs for text-to-image tasks. However, diffusion frameworks focus primarily on generation, lacking integrated text-image understanding capabilities. While autoregressive models emphasize crossmodal dependencies, diffusion approaches prioritize photorealism through progressive refinement. 2.3. Unified Understanding and Generation Recently, an increasing number of studies focus on unified models for understanding and generation. Next-GPT [39] and SEED-X [16] achieve this goal by combining separate understanding and generation systems. Show-o [41] and TransFusion [44] employ hybrid diffusion-autoregressive methods. Emu2 [36] use fully autoregressive architecture for predicting the next multimodal elements, using classification tokens for text prediction and regressing visual embeddings for image prediction. Chameleon [37], VILAU [40], and Emu3 [40] convert images into tokens, intertwining image tokens with text tokens from the very beginning, to support joint text-image reasoning and autoregressive prediction. Although the above models are unified for understanding and generation, they are trained to simultaneously learn both multimodal understanding and generation capabilities, which is resource-intensive and leads to failure in outputting interleaved text-image. In contrast, we propose the ARMOR framework, which builds UniMs by fine-tuning existing MLLMs in resource-efficient manner to empower them with the capabilities text-to-image generation and outputting interleaved text-image. 3. ARMOR Framework 3.1. Preliminary To improve training performance, we introduce weighted loss calculation method, allowing for dynamic adjustments at different training stages. We optimize the loss by adopting two learning objectives: the loss function for text prediction and for image prediction. We create label masks to ensure that the model can separately compute the loss of text or image prediction. Text Prediction Loss Calculation: Ltext = (cid:88) t=1 Itext(t) log P1(yt y<t, ) (1) : total length of the target sequence (total time steps) Itext(t): 1 if time step is in text output mode yt: the t-th token in the target sequence (text token) y<t: the previously generated tokens before time step : the joint representation of the multimodal input P1: probability distribution for text output Figure 2. Structural comparison of UniMs. Figure 3. Architecture of ARMOR. Image Prediction Loss Calculation: Limg = (cid:88) t=1 Iimg(t) log P2(yt y<t, ) (2) Iimg(t): 1 if time step is in image output mode yt: the t-th token in the target sequence (image token) P2: probability distribution for image output Overall Loss Calculation: Ltotal = αLtext + βLimg (3) α: weight for text loss (default=1.0) β: weight for image loss (default=1.0) 3.2. Architecture Rather than focusing on enhancing the comprehension capabilities of UniMs, we have chosen to explore the generative potential of MLLMs. So the core challenge we face is how to enable UniMs to achieve generation capability comparable to those of MLLMs. Therefore, we have to solve the following two problems: 1. How can MLLMs obtain generation ability? 2. While MLLMs acquire generation ability, how can we avoid catastrophic forgetting of its understanding abilities? In the question of how to endow MLLM with generation ability, we choose to integrate pre-trained VQGAN [13]- decoder (originating from Chameleon) for InternVL2.5 [8] (vit-mlp-llm) and achieve breakthrough in generation ability whit an asymmetric encoder-decoder manner. Specifically, we adopted simple and effective approach: integrating text information and image information by extending the indices of the pre-trained VQGAN to codebook of MLLM. Specifically, we added new tokens from Table. 1 to the InternVL2.5 model. Some of these tokens are used to map the indices of VQGAN, and we expanded other related structures of the model (embedding layer and output layer) to enable the model to learn image information. For the newly added parameters, we use random initialization and freeze the weights of the original text tokens. Regarding the problem of how to inherit the original capabilities and avoid catastrophic forgetting during the training process, we proposed two schemes in the early stage of the research: 1. Use data hybrid training. Collect large amount of training data for MLLM or distill high-quality Supervised Fine-Tuning (SFT) data. Mix text-image interleaved data to train the model backbone and retain the original capabilities as much as possible during the training process. 2. Modify the model structure. Add additional modules to enable the models original weights to participate in training as little as possible, thereby reducing the impact on the models original capabilities. the following several scenarios: Q1: When should the model respond solely with text? Applicable scenarios: Natural language dialogue tasks and multimodal understanding tasks requiring textual responses without image generation Training data: t2t (Text-to-Text) and ti2t (TextImage-to-Text) datasets Q2: When should the model only generate images? Applicable scenarios: Explicit image generation tasks with direct visual content requests Training data: t2i (Text-to-Image) datasets Q3: When should the model produce mixed responses? Applicable scenarios: Multimodal interaction tasks mirroring human communication patterns requiring combined textual and visual outputs Training data: t2ti (Text-to-Text-Image) datasets To sum up, training data consists of the following types: 1. t2t: Standard dialogue-based questions and answers. This part of the dataset is entirely self-constructed. 2. ti2t: Image comprehension tasks where the model is given an image and asked to generate textual descriptions. This part of the dataset is selected from ShareGPT4V [5]. 3. t2i: Text-based image generation tasks, where the model generates an image based on given prompt. This part of the dataset is composed of three parts: selfconstruction, screening from LAION-nolang-aesthetics27M [34], and screening from text-to-image-2M [45]. 4. t2ti: Mixed-modality dialogue tasks where both textual responses and generated images are required. This part of the dataset is entirely self-constructed. 3.4. WoHG Training Algorithm For the model training, we proposed three-stage training algorithm named WoHG (what or how to generate). We designed specific training objectives for each stage to ensure that the models capabilities steadily improve during the training process without causing the problem of catastrophic forgetting. The specific modules trained in each stage are shown in Figure 5. (a) First Stage: What to Generate? We deem that it is important for the model to autonomously make different responses according to the form of the question. Relying on external information to select response mode will affect the interaction. Therefore, what content to generate in conversational context is the training focus of this step. We call this what to generate. Figure 4. Forward switching mechanism. After comparison and discussion, the first scheme requires training large number of the models original parameters. Moreover, the ability level after training is related to the quality of SFT data, making it difficult to guarantee the same level as before. In addition, it also needs to deal with the heavy workload of data distillation and fusion. Therefore, we finally decided to adopt the second scheme. We added several transformer layers as adapter for the backbone. The newly added transformer layers are consistent with the model. Such changes will not damage the original structure of the model. The unification of understanding and generation capabilities is achieved entirely based on the next token prediction method. After modifying the model according to the above ideas, when predicting image tokens in the forward process, the output layer needs to conduct full-scale classification of the entire codebook. large number of text tokens lacking image information may interfere with the prediction of image tokens. Conversely, the same is true. This redundant classification increases the learning difficulty of the model. Therefore, we designed forward-switching mechanism, which outputs through two different output heads according to different modalities. The architecture of the model is shown in Figure 3. The forward-switching is controlled by special tokens and the mechanism of forwardswitching is shown in Figure 4, and the relationship between the newly added tokens and the output layer is described in Table 1. In the process of autoregressive output, there is no need to classify all tokens in the codebook. Instead, only focus on the information of certain modality. This gated classification output mechanism not only reduces the models search space but also enables the model to autonomously determine the modal generation path and achieve natural mixed-modality output. Furthermore, all the modifications we made to the model only added total of 0.7 billion trainable parameters. 3.3. Data Collection To achieve the models mixed modality input and output capabilities, we consider the data required for training through In this stage, the types and quantities of datasets we employ are t2t (100K), ti2t (100K), t2i (100K) and t2ti (100K). Token Type Special Tokens: <imgbos> <imgend> <imgpad> Image Content Tokens: 8192 image tokens Head Functional Description text output visual output visual output Switch model to visual output mode, begin image generation Terminate image generation, revert to text output mode Padding placeholder in image token sequences visual output Content representation tokens Table 1. Special token and image content token specifications. (a) step 1. (b) step 2. (c) step 3. Figure 5. Demonstration of the proposed three-stage WoHG training algorithm. We adjust the calculation weights of loss function as follows: α = 1.0 and β = 0.0. The trainable parameters in this stage are shown in Table 5a. In the first stage of training, we use the datasets of the above four different question types to train the models ability to distinguish question types. This stage lays the foundation for the models ability to handle multimodal inputs and outputs in later stages. (b) Second Stage: How to Generate? After the first stage of training, the model is able to generate appropriate answer pattern in any given question. The second stage requires activating the models image generation ability and the corresponding relationship between images and text content. Train all parameters related to generation of the model to ensure that it can accurately generate images that meet the requirements according to the input text information. So the core of the second stage is how to generate appropriate images when facing generation demands. We call this how to generate. In this stage, the types and quantities of datasets are t2i (2.5M) and t2ti (2.5M). We adjust the calculation weights of loss function as follows: α = 0.0 and β = 1.0. The trainable parameters in this stage are shown in Figure 5b. After this stage of training, the model is able to generate impressive images. The changes in image generation quality during part of the training process are shown in Figure 6. (c) Third Stage: How to Answer Better? In this stage, we use carefully selected high-quality text-image interleaved dataset to fine-tune the model. The focus of this stage is how to generate better text-image interleaved responses and ensure better synergistic effect between the generated text and images. We call this how to answer better. In this stage, the types and quantities of datasets are t2t (50K), ti2t (50K), t2i (50K) and t2ti (50K). Furthermore, we adjust the calculation weights of the loss function as follows:α = 1.0 and β = 1.0. Through this stage of training, the model is able to output high-quality interleaved messages. It is worth mentioning that for the ti2ti dialogue pattern, we did not specifically integrate this ability for the model. However, we found that the model has obtained ti2ti capability unintentionally. The trainable parameters in this stage are shown in Figure 5c. MLLMs naturally support interleaved text-image input for handling comprehension tasks. After the abovementioned training, ARMOR extends its generation ability to achieve interleaved text-image output. Ultimately, ARMOR can successfully integrate these two abilities to realize interleaved text-image input-output. Figure 6. Changes in image generation quality during part of the training process (epochs: 4, 6, 8... 18, from left to right). Prompt 1: Could you generate an image of the aurora for me?; Prompt 2: Please help me draw picture of the tropical rainforest.. 4. Experiments 4.1. Settings 4.1.1. Benchmarks To comprehensively evaluate the capabilities of ARMOR, we conduct detailed evaluation experiments from two dimensions: multimodal understanding evaluation and visual generation evaluation. For evaluating the multimodal understanding ability, we use the VLMEvalkit [11] platform and evaluate on series of standardized benchmarks, including MMMU [43], MME-P [15], MME [15], MMvet [42], MMB [26], SEEDBench-img [20], HallusionBench [18], POPE [23] and LLaVABench [25]. These datasets cover wide range of multimodal understanding tasks, providing comprehensive evaluation of the models understanding ability across different contexts. For evaluating the visual generation capability (e.g., textto-image generation), we conduct extensive experiments via the GenEval [17] platform. The GenEval platform provides standardized evaluation framework for text-to-image generation tasks, effectively measuring the quality and accuracy of text-to-image generation. 4.1.2. Baselines 1. Understanding models: These models mainly focus on the understanding ability of multimodal data and can process information from different modalities (such as text and image). Representative models include QwenVL [2, 3, 38], InternVL [8, 9], InstructBLIP [10, 21, 22] and the LLava [24, 25] series, etc. 2. Generation models: These models have strong generation ability and can generate high-quality images, including DALLE [4, 30, 31], SDv1.5 [14, 28], LlamaGen [35], etc. 3. Unified understanding and generation models: These models not only have multimodal understanding ability but also have good generation ability. Such as Chameleon [37], Show-o [41],VILA-U [40], etc. 4.2. Quantitative Evaluation The experimental results on multimodal understanding benchmarks are shown in Table 2. We can observe that: (1) Our ARMOR consistently outperforms all existing UniMs across all 9 benchmarks, though they have more parameters (e.g., 49.8 vs 35.6 in score for Ours ARMOR-8B and SEED-X-17B, respectively). (2) Our ARMOR achieves comparable results with MLLMs, and preserves over 95% understanding capabilities of InternVL2.5. The observations highlight the limitations of existing UniMs in multimodal understanding, while demonstrating the significant potential of our framework in endowing existing MLLMs with generation capabilities. In addition, the experimental results on the visual generation benchmark (i.e., GenEval) are listed in Table 3. The experimental results demonstrate that ARMOR achieves promising visual generation performance while requiring significantly fewer trainable parameters and lower training costs compared to existing models. This compelling evidence validates the feasibility of constructing UniMs through upgrading existing MLLMs. 4.3. Qualitative Evaluation We validated the necessity of the second training stage through qualitative evaluation, with experimental results shown in Figure 6. It can be observed that for two distinct prompts in line 1 and line 2, as the number of training epochs in the second training stage increases, the quality of the generated images improves correspondingly. This indicates that the second training stage (the how to generate phase) significantly enhances the image quality generated by MLLMs, while implicitly demonstrating the effectiveness of our proposed WoHG training algorithm. 4.4. Ablation Studies We conduct ablation studies to verify the promoting effect of the forward-switching mechanism proposed in this paper on image generation training. We used 0.5M dataset to Method Params MMMU (val) [43] MME-P [15] MME [15] MMvet [42] MMB [26] SEEDBench-img [20] HallusionBench [18] POPE [23] LLaVABench [25] Understanding modals Qwen2.5-VL InternVL 2.5 Qwen2-VL LLaVA-Next-Vicuna LLaVA-ov Llama-3-VILA1.5 DeepSeek-VL2 LLaVA-v1.5 InstructBLIP Qwen-VL-Chat Emu3 Chat 7B 8B 7B 13B 7B 8B 16B 7B 7B 7B 8B 56.2 53.5 53.7 37.3 47.9 37.4 54.0 35.7 30.6 37.0 33.9 Uni modals without interleaved text - image output Show-o-256 SEED-X VILA-U-384 LWM TokenFlow-B TokenFlow-L TokenFlow-XL-Vicuna TokenFlow-XL-Qwen SynerGen-VL Janus-Pro 1.3B 17B 7B 7B 13B 13B 13B 14B 2.4B 7B 25.1 35.6 - - 34.2 34.4 38.7 43.2 34.2 41.6 Uni modals with interleaved text - image output chameleon ARMOR (InternVL2.5) 7B 8B 22.4 49. 1685.2 1688.2 1639.2 1448.4 1577.8 1438.8 1632.7 1506.2 1137.1 1467.8 1334.1 948.4 1435.7 1401.8 - 1353.6 1365.4 1545.9 1551.1 1381.0 1516.7 153.1 1619.4 2299.2 2338.9 2276.3 1745.6 1993.6 1698.5 2230.2 1808.4 1391.4 1860.0 1610.5 - - - - 1660.4 1622.9 1840.9 1922.2 1837.0 1791.7 202.7 2229. 66.6 59.6 61.8 44.9 51.9 41.9 60.0 32.9 33.1 47.3 29.1 - - 33.5 9.6 22.4 27.7 40.7 48.2 34.5 45.1 8.3 53.1 83.5 82.0 82.8 70.0 83.2 62.1 84.1 66.5 33.9 61.8 63.8 - - - - - - - - 53.7 62.6 15.4 78. 71.0 77.0 76.0 71.4 76.7 65.0 77.0 65.8 44.5 64.8 69.2 - - 59.0 - 60.4 62.6 68.7 72.6 62.0 70.1 30.5 74.8 56.3 49.0 50.4 31.8 31.6 35.3 45.3 27.6 31.2 36.8 31.7 - - - - - - - - - 39.5 17.1 46. 86.1 88.9 88.4 87.8 88.4 83.3 - 86.1 86.1 74.9 83.3 73.8 84.2 85.8 75.2 84.0 85.0 86.8 87.8 85.3 78.9 19.4 87.7 80.6 80.3 70.1 73.9 81.0 71.7 89.7 61.8 59.8 67.7 49.2 - - - - - - - - - 74.4 26.6 78. Table 2. Evaluation on multimodal understanding benchmarks. We include several methods with their results on multiple benchmarks. The results of ARMOR are highlighted in bold. Type Gen. Method #Param #Train Images Train Cost(GPU days) Image Res. Score LlamaGen LDM SDv1.5 PixArt-alpha SDv2.1 Emu3-Gen DALL-E2 SDXL SDv3 (d=24) NoILO-Uni. VILA-U Show-o D-DiT SEED-X TokenFlow-XL SynerGen-VL Janus-Pro-1B Janus-Pro-7B ILO-Uni. Chameleon ARMOR 0.8B 1.4B 0.9B 0.6B 0.9B 8B 6.5B 7B 2B 7B 1.3B 2B 17B 14B 2.4B 1.5B 7B 7B 8B 60M 400M 2000M 25M - - 650M 2000M - 15M 36M 400M 158M 60M 667M 72M 72M 1.4B 5M - - 6250/A100 753/A100 8333/A100 - 4166/A100 - - - - - 480/H800 - - 1568/A100 3584/A100 35687/A100 160/H100 512 1024 512 1024 768 512 1024 1024 1024 384 512 512 - 384 512 384 384 512 256 0.32 0.37 0.43 0.48 0.50 0.54 0.52 0.55 0. 0.42 0.53 0.65 0.49 0.55 0.61 0.73 0.80 0.39 0.37 Table 3. Evaluation on the GenEval benchmark. Gen. denotes generation and NoILO-Uni. denotes UniMs without interILO-Uni. denotes UniMs with inleaved text-image output. terleaved text-image output. Figure 7. The training loss of the two model structures. 5. Conclusion train 5 epochs for the model using the forward-switching mechanism and the model without any special treatment (Normal) respectively. Figure 7 shows the convergence speed of the loss function during the training process, and the image generation effects under the same number of training epochs are presented in Figure 8. From the experimental results, it can be seen that the model using the forward-switching mechanism has significantly faster convergence speed of the loss function. And due to the absence of the long-tail distribution problem, Under the same number of training epochs, its generation effect is also better than that of the Normal model. In this paper, we have presented the ARMOR framework to build UniMs by upgrading existing MLLMs. By introducing an asymmetric encoder-decoder architecture, the framework enables MLLMs to output natural interleaved text-image. We have collected high-quality interleaved dataset and developed three-stage training algorithm named WoHG to fine-tune existing MLLMs. The algorithm enables MLLMs to achieve unified capabilities in both understanding and generation tasks. Experimental results on 9 benchmarks demonstrate that our framework can effectively empower existing MLLMs with generation capabilities while preserving their understanding capabilities. data and model scaling. arXiv preprint arXiv:2501.17811, 2025. 1, 3 [8] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and testtime scaling. arXiv preprint arXiv:2412.05271, 2024. 2, 4, 7 [9] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2418524198, 2024. 3, 7 [10] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards generalpurpose vision-language models with instruction tuning, 2023. 7 [11] Haodong Duan, Junming Yang, Yuxuan Qiao, Xinyu Fang, Lin Chen, Yuan Liu, Xiaoyi Dong, Yuhang Zang, Pan Zhang, Jiaqi Wang, et al. Vlmevalkit: An open-source toolkit for In Proceedings evaluating large multi-modality models. of the 32nd ACM International Conference on Multimedia, pages 1119811201, 2024. 7 [12] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In CVPR, pages 1287312883, 2021. [13] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming In Protransformers for high-resolution image synthesis. ceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1287312883, 2021. 4 [14] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, 2024. 3, 7 [15] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, et al. Mme: comprehensive evaluation benchmark for multimodal large language models. arXiv preprint arXiv:2306.13394, 2023. 7, 8 [16] Yuying Ge, Sijie Zhao, Jinguo Zhu, Yixiao Ge, Kun Yi, Lin Song, Chen Li, Xiaohan Ding, and Ying Shan. Seed-x: Multimodal models with unified multi-granularity comprehension and generation. arXiv preprint arXiv:2404.14396, 2024. 3 [17] Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating textto-image alignment. Advances in Neural Information Processing Systems, 36, 2024. 3, 7 [18] Tianrui Guan, Fuxiao Liu, Xiyang Wu, Ruiqi Xian, Zongxia Li, Xiaoyu Liu, Xijun Wang, Lichang Chen, Furong Huang, Yaser Yacoob, Dinesh Manocha, and Tianyi Zhou. Hallusionbench: An advanced diagnostic suite for entangled language hallucination and visual illusion in large visionFigure 8. Generated images with the prompt Can you help me draw picture of teddy bear doll?. Images (a), (b) and (c) are from model with one output layer, and images (d), (e) and (f) are from model with two output layers."
        },
        {
            "title": "References",
            "content": "[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: visual language model for few-shot learning. Advances in Neural Information Processing Systems, 35:2371623736, 2022. 3 [2] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: versatile vision-language model for understanding, localization, text reading, and beyond. arXiv preprint arXiv:2308.12966, 2023. 7 [3] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and JunarXiv preprint yang Lin. Qwen2.5-vl technical report. arXiv:2502.13923, 2025. 3, 7 [4] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, et al. Improving image generation with better captions. Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf, 2(3):8, 2023. 7 [5] Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v: Improving large multi-modal models with better captions. arXiv preprint arXiv:2311.12793, 2023. 5 [6] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever. Generative pretraining from pixels. In ICML, pages 16911703, 2020. 3 [7] Xiaokang Chen, Zhiyu Wu, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, and Chong Ruan. Januspro: Unified multimodal understanding and generation with language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1437514385, 2024. 7, 8 [19] Black Forest Labs. Flux. https://github.com/ black-forest-labs/flux, 2024. 3 [20] Bohao Li, Yuying Ge, Yixiao Ge, Guangzhi Wang, Rui Wang, Ruimao Zhang, and Ying Shan. Seed-bench: Benchmarking multimodal large language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1329913308, 2024. 7, 8 [21] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified In ICML, vision-language understanding and generation. 2022. 7 [22] Junnan Li, Dongxu Li, Silvio Savarese, and Steven C. H. Hoi. BLIP-2: bootstrapping language-image pre-training with frozen image encoders and large language models. In ICML, pages 1973019742, 2023. 3, [23] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Evaluating object hallucinaarXiv preprint Zhao, and Ji-Rong Wen. tion in large vision-language models. arXiv:2305.10355, 2023. 7, 8 [24] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning, 2023. 7 [25] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning, 2023. 7, [26] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an In European Conference on Computer all-around player? Vision, pages 216233. Springer, 2025. 3, 7, 8 [27] Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, and Dustin Tran. Image transformer. In ICML, pages 40554064, 2018. 3 [28] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Improving latent diffusion modRobin Rombach. Sdxl: arXiv preprint els for high-resolution image synthesis. arXiv:2307.01952, 2023. 3, 7 [29] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In ICML, pages 87488763, 2021. 3 [30] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In International conference on machine learning, pages 88218831. Pmlr, 2021. 7 [31] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1 (2):3, 2022. 3, 7 [33] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, pages 10684 10695, 2022. 3 [34] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. LAION400M: open dataset of clip-filtered 400 million image-text pairs. CoRR, abs/2111.02114, 2021. [35] Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan. Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525, 2024. 3, 7 [36] Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Zhengxiong Luo, Yueze Wang, Yongming Rao, Jingjing Liu, Tiejun Huang, and Xinlong Wang. Generative multimodal models are in-context learners, 2023. 3 [37] Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. 3, 7 [38] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 7 [39] Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-Seng Chua. NExT-GPT: Any-to-any multimodal LLM. In Proceedings of the International Conference on Machine Learning, pages 5336653397, 2024. 3 [40] Yecheng Wu, Zhuoyang Zhang, Junyu Chen, Haotian Tang, Dacheng Li, Yunhao Fang, Ligeng Zhu, Enze Xie, Hongxu Yin, Li Yi, et al. Vila-u: unified foundation model integrating visual understanding and generation. arXiv preprint arXiv:2409.04429, 2024. 3, [41] Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding and generation. arXiv preprint arXiv:2408.12528, 2024. 1, 3, 7 [42] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities. arXiv preprint arXiv:2308.02490, 2023. 7, 8 [43] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of CVPR, 2024. 7, 8 [32] Suman Ravuri and Oriol Vinyals. Classification accuracy score for conditional generative models. NeurIPS, 32, 2019. 3 [44] Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma, Luke Zettlemoyer, and Omer Levy. Transfusion: Predict the next token and diffuse images with one multi-modal model. arXiv preprint arXiv:2408.11039, 2024. 3 [45] zk. text-to-image-2m (revision e64fca4), 2024. ARMOR v0.1: Empowering Autoregressive Multimodal Understanding"
        },
        {
            "title": "Supplementary Material",
            "content": "6. Appendix 6.1. Dataset Examples 1. t2t (Text-to-Text) Example: From: human Value: What kind of scenery can be found beside the Nile River? From: gpt Value: The Nile River offers desert lush farmlands, palm trees, small villages, ancient monuments, wildlife, and the fertile Nile Delta, creating mix of natural beauty and historical significance. landscapes, 2. t2ti (Text-to-Text-Image) Example: From: human Value: What kind of scenery can be found beside the Nile River? Can you draw an image to depict it for me? From: gpt Value: Alongside the Nile River, you can find lush, verdant banks filled with vibrant greenery and towering papyrus plants. The scene reflects serene environment with occasional traditional felucca sailboats gliding across the water. 3. t2i (Text-to-Image) Example: From: human Value: Please draw an image of sunset over the sea for me. From: gpt Value: 4. ti2t (Text-Image-to-Text) Example: From: human Value: <image>What is this photo about? From: gpt Value: In the heart of vast grassland, family of three elephants is on journey. The adult elephant, with its majestic brown skin, leads the way in the center, its size significantly larger than the two smaller ones. On either side of this gentle giant, slightly behind, are two smaller elephants, their youthful energy evident in their stride. The elephants are making their way through field of tall grass, their brown bodies contrasting beautifully with the greenery around them. The grassland extends far into the distance, dotted here and there with few trees standing tall and solitary. Above them, the sky is canvas of overcast clouds, casting soft light over the scene. The relative positions of the elephants suggest protective formation, with the adult elephant guiding its young ones through the grassland. This image captures moment of tranquility and familial bond in the wild. 6.2. Mixed-modality Output Test In this subsection, we list some examples of ARMORs interleaved input and output, as well as comparisons with other models. 6.2.1. Text-Image-to-Text-Image Test Show-o Yes, the image shows scene of athletes exercising, with person holding pair of scissors and doughnut with chocolate frosting and sprinkles on cooling rack. The doughnut is placed in the foreground, while the athlete is in the background. The athlete is holding the scissors, possibly preparing to cut the doughnut. The scene is set against black background, which emphasizes the focus on the athlete and the doughnut. This is evaluation integrating understanding and generation requirements. Through the evaluation, Janus-Pro can recognize that the image doesnt show athletes but cookie, and responds to the subsequent image-generation request by clearly stating its language model unable to perform visual generation. VILA-U can also identify that its not an athlete scene, yet makes no response to the imagegeneration demand. However, Show-o fails to accurately understand the image content. It even describes false image caption in seemingly plausible way and doesnt respond to the image-generation request at all. Compared with Janus-Pro, VILA-U and Show-o, ARMOR can accurately understand the image content, confirm its not about athletes, and precisely recognize the image-generation requirement, generating an image that meets the needs. 6.2.2. Text-to-Text-Image Test Question Whats the Northern Hemisphere like in June? Can you draw an image about it for me? ARMOR In the Northern Hemisphere during June, its typically summer with warmer temperatures and longer daylight hours. Regions closer to the poles experience the midnight sun, where the sun remains visible at midnight due to the regions tilt towards the sun. Plants are lush, and wildlife is active Question Does the picture show the scene of athletes exercising? Can you help me draw one similar to this picture? ARMOR No, the picture does not show athletes in action. It depicts freshly baked chocolate chip cookies cooling on wire rack. Janus-Pro The picture you provided shows freshly baked chocolate chip cookies cooling on wire rack. There are no athletes or exercise scenes in this image. cant help you draw one similar to this picture as am an AI language assistant and not capable of visual creation. If you have any other questions or need assistance with different topic, feel free to ask! VILA-U No, the picture does not show the scene of athletes exercising. It is close-up image of freshly baked cookie on cooling rack. The cookie is round in shape and has chocolate chips on top. Janus-Pro VILA-U Show-o of these two images presents real natural scenery. JanusPros image adopts retro illustration style, while VILAUs follows flat illustration style. Now, take look at Show-o. It made natural scenery image, but it misunderstood the term Hemisphere. The image it drew is round, and there are barely any elements showing summer features. When comparing Janus-Pro, VILA-U and Show-o, ARMOR stands out. It can accurately describe the Northern Hemispheres scene in June using words. Moreover, it successfully generates an image that perfectly matches the described scene. Question Whats the Southern Hemisphere like in June? Can you draw an image about it for me? ARMOR In June, the Southern Hemisphere is experiencing winter, with varying conditions across different regions. High latitudes may see continuous daylight or polar night depending on their proximity to the poles. Temperate zones will have cold days typical of winter, with possible snowfall in mountainous areas. Tropical regions may experience rainy seasons with warmer temperatures. Janus-Pro This evaluation focuses on multimodal generation tasks that require both text descriptions and corresponding images to fully address user requests. Unlike standard image generation, successful responses must provide accurate textual explanations paired with visually appropriate images. Lets check the output of these models. Janus-Pro, VILA-U and Show-o can only generate single image yet ARMOR can output with interleaved text-image response. Janus-Pro and VILA-U both created snowy mountain scenes. Since June typically represents summer, these snowy mountain depictions might not match the general summer scenes people expect in June. Additionally, neither VILA-U Show-o This question simply replaces the Northern Hemisphere in the previous example with the Southern Hemisphere. Through the test, neither Janus-Pro, VILA-U nor Show-o generated pictures that met the requirements. Although the generated works of Janus-Pro and Show-o are of good quality, they do not conform to the scene in the Southern Hemisphere in June. VILA-U tried to draw globe, which not only fails to meet the requirements of the question but also has poor generation quality. Besides, the picture drawn by Show-o is still circular. Compared with Janus-Pro, VILAU and Show-o, ARMOR can accurately describe the cold scene in the Southern Hemisphere in June with words and generate picture that matches the scene. 6.3. WoHG Algorithm Explore In order to endow the model with generation capabilities while preserving its original abilities, we employed fullscale fine-tuning method using our dataset and obtained the results shown in Table 4. We conducted trials on different training schemes presented in the table. Initially, we used large dataset with interleaved text and images for training. During the training process, we tested the models comprehension ability. The tests revealed that both the text output layer+embedding+ visual output layer+adapter and text output layer+visual output Epoch 10 10 20 20 Text L. Emb. Visual L. Adp. Und. 0.78 0.72 0.64 0.48 Table 4. Single Train Stage Result of Understanding Ability. Text denotes the text output layer. Visual denotes the visual output layer. Adp. denotes the added transformers adapter. Und. denote the percentage of the original InternVL2.5 capabilities mantained after training. layer+adapter training schemes severely affected the inheritance of the original capabilities. When training text output layer, large amount of text output information significantly impacts the output of text output layer, undermining the original comprehension ability. Therefore, to minimize the impact on the output of text output layer, we explored phased - training scheme. In the first phase, we used small amount of data to enable the model to learn when to output text, images, and combination of text and images. In the first phase, to enable the model to acquire the ability to output image start token, we conducted training with small amount of data. Stage Stage1 Text L. Emb. Visual L. Adp. Und. 0.72 Table 5. Train The First Stage with Small Amount of Data. After testing, after the first-stage phased training, the model can distinguish output modes well while maintaining its original comprehension ability. This lays the foundation for subsequently endowing the model with generation capabilities. The result is shown in Table 5. In stage 2, there are two remaining schemes: the first is to train visual output layer+adapter, the second is to train embedding+visual output layer+adapter (which is the finally selected scheme) When we only trained with the visual output layer+adapter scheme, we found that the loss of the model remained at relatively high level for long time, and the quality of the generated images remained at certain stage as follows Figure /reffig:combined. We believe that if the newly added embedding part of the model does not participate in the training, it may result in the failure of establishing connection between the newly added image embedding of the model and the text embedding. Merely relying on the subsequent adapter and visual output layer cannot achieve the association between text information and image information. Therefore, we finally decided to train the model with the embedding+visual output layer+adapter scheme to realize the generation ability of the model. Figure 9. The Generated Images Trained with Visual Output Layer and Adapter."
        }
    ],
    "affiliations": [
        "Nankai University",
        "Shanghai AI Laboratory",
        "Shanghai Innovation Institute",
        "University of Science and Technology of China",
        "Wuhan University"
    ]
}