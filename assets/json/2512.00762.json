{
    "paper_title": "Seeing the Wind from a Falling Leaf",
    "authors": [
        "Zhiyuan Gao",
        "Jiageng Mao",
        "Hong-Xing Yu",
        "Haozhe Lou",
        "Emily Yue-Ting Jia",
        "Jernej Barbic",
        "Jiajun Wu",
        "Yue Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "A longstanding goal in computer vision is to model motions from videos, while the representations behind motions, i.e. the invisible physical interactions that cause objects to deform and move, remain largely unexplored. In this paper, we study how to recover the invisible forces from visual observations, e.g., estimating the wind field by observing a leaf falling to the ground. Our key innovation is an end-to-end differentiable inverse graphics framework, which jointly models object geometry, physical properties, and interactions directly from videos. Through backpropagation, our approach enables the recovery of force representations from object motions. We validate our method on both synthetic and real-world scenarios, and the results demonstrate its ability to infer plausible force fields from videos. Furthermore, we show the potential applications of our approach, including physics-based video generation and editing. We hope our approach sheds light on understanding and modeling the physical process behind pixels, bridging the gap between vision and physics. Please check more video results in our \\href{https://chaoren2357.github.io/seeingthewind/}{project page}."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 3 ] . [ 1 2 6 7 0 0 . 2 1 5 2 : r a"
        },
        {
            "title": "Seeing the Wind from a Falling Leaf",
            "content": "Zhiyuan Gao1 Jiageng Mao1 Hong-Xing Yu2 Haozhe Lou1 Emily Yue-Ting Jia1 Jernej Barbic1 Jiajun Wu2 Yue Wang1 1University of Southern California 2Stanford University {gaozhiyu, jiagengm, haozhelo, eyjia, jnb, yue.w}@usc.edu {koven, jiajunwu}@cs.stanford.edu Figure 1: We propose an end-to-end differentiable framework capable of estimating invisible forces directly from video data, mimicking the human ability to perceive unseen physical effects through vision alone. This approach enables applications such as physics-based video generation, where new objects can be seamlessly introduced into scene and simulated within the same force field. Force strength: from low to high (best viewed in colors)."
        },
        {
            "title": "Abstract",
            "content": "A longstanding goal in computer vision is to model motions from videos, while the representations behind motions, i.e. the invisible physical interactions that cause objects to deform and move, remain largely unexplored. In this paper, we study how to recover the invisible forces from visual observations, e.g., estimating the wind field by observing leaf falling to the ground. Our key innovation is an end-to-end differentiable inverse graphics framework, which jointly models object geometry, physical properties, and interactions directly from videos. Through backpropagation, our approach enables the recovery of force representations from object motions. We validate our method on both synthetic and real-world scenarios, and the results demonstrate its ability to infer plausible force fields from videos. Furthermore, we show the potential applications of our approach, including physics-based video generation and editing. We hope our approach sheds light on understanding and modeling the physical process behind pixels, bridging the gap Equal contribution. 39th Conference on Neural Information Processing Systems (NeurIPS 2025). between vision and physics. Please check more video results in our project page https://chaoren2357.github.io/seeingthewind/. Who has seen the wind? Neither nor you: But when the leaves hang trembling, the Christina Rossetti wind is passing through."
        },
        {
            "title": "Introduction",
            "content": "Watching leaves swirl and glide through the autumn breeze, we can almost sense the wind gently guiding them in natural choreography. Similarly, as cherry blossom petals drift in spring, it feels as though the air cradles them, orchestrating their delicate descent. Although we cannot directly see the wind, humans can seamlessly infer these invisible physical interactions from visible cues in their surroundings, such as those captured in videos. While this intuitive physics capability has long existed in human vision, it remains underexplored in computer vision. In this paper, we bridge the gap by introducing differentiable framework to revealing invisible forces from visual data. The key challenge of this problem lies in extracting insights about an unseen targetdynamic forceswhile relying exclusively on visible inputs. To address this, it is essential to understand how videos, as visible cues, connect to the underlying invisible dynamics. Consider video of leaf falling: external forces like wind, apply to leaf with known shape, appearance, and physical properties, producing motion that aligns with physical laws and is captured visually. By developing an end-to-end differentiable model of this physical process, we can learn and predict these invisible forces, such as wind, based on video evidence alone. To this end, we propose differentiable inverse graphics framework, which models objects inherent properties (geometry, appearance, and physical properties), invisible force representations, and physical processes from video inputs. For object modeling, we leverage 3D Gaussians [1] as representations for shape and appearance, which can be easily obtained from videos. To model objects physical properties, we propose novel approach that leverages commonsense about physical properties in vision-language models and attaches the knowledge to 3D Gaussians. For force representations, we adopt the Eulerian perspective and introduce novel causal tri-plane representation, which models the spatio-temporal continuity and intrinsic causality of forces with high fidelity. For physical processes, we implement differentiable physics simulator for deformable objects to animate object motions based on object properties and forces. We note that our object representation (Gaussians as Lagrangian elements) and the force representation (causal tri-plane as grids) perfectly fit into the formulations of the material point method [2], allowing us to accurately model the physical process. Together, these components form differentiable framework that bridges perception and physics, so that we can estimate forces from video object motions via backpropagation. While our framework accurately models the physical process, recovering force representations from object motions in videos remains highly challenging. Unlike system identification approaches which estimate only few physical parameters, forces are omni-directional and can present throughout the 3D space. Estimating such dense and complex force representations poses great challenges to optimization. Moreover, time integration in the physics simulator leads to unstable backpropagation, with gradients often exploding as they accumulate over time. To address the challenges, we propose novel 4D sparse tracking objective, where we represent object motions as the movements of sparse keypoints in the spatio-temporal space, and the movements of the Lagrangian elements, i.e., the 3D Gaussians, are further controlled by their neighboring keypoints via barycentric interpolation. With this objective, we greatly reduce the complexity of the prediction space and facilitate the estimating of force representations. We evaluated our estimated force representations on both synthetic and real-world scenarios. The results demonstrate our methods ability to recover invisible forces from videos. Moreover, we show that with the estimated force representation, we can generate novel and physically plausible object motions by changing object types, physical properties, or boundary conditions, which enables realistic physics-based video generation and editing. To conclude, we summarize our contributions as follows: We identify an important problem in physics understanding from videos: recovering invisible forces from object motions. To tackle this problem, we propose novel inverse-graphics framework 2 that jointly models object properties, forces, and physical processes, enabling the estimation of underlying forces directly from video observations. We introduce novel sparse tracking objective, which effectively handles the optimization challenges in differentiable physics and enables robust estimation of forces from visual inputs. We demonstrate our methods ability to recover forces from motion, and showcase its potential for generating physically plausible motions and enabling physics-based video generation and editing."
        },
        {
            "title": "2 Related Works",
            "content": "Intuitive Physics. Understanding the physical world is fundamental aspect of human intelligence. Researchers have long sought to bring this intuitive physics understanding ability to machine intelligence. Galileo [3] and the following works [4, 5] integrated deep learning with physics simulation to estimate physical object properties from visual observations. More recent approaches performed system identification by leveraging differentiable physics [69], neural fields [1012], 3D Gaussian splatting [13, 14], vision-language models [15, 16], and video generation [1720], enabling more accurate estimation of physical object properties. However, these methods primarily focus on single physical parameter, such as mass, friction, and Youngs modulus. In contrast, estimating forces is significantly more challenging, as they are vectors that can exist throughout the 3D space. In this paper, we propose novel framework that successfully recovers force fields from visual inputs. Differentiable Physics. Differentiable physics simulators [2129] have been widely used to bridge perception and physics by enabling the backpropagation of particle motion gradients to physical parameters. However, using gradients from physics simulators to optimize physical properties can be notoriously difficult, as the inherent discontinuous behavior and the time integration of physics simulation often lead to vanishing or exploding gradients. To handle this challenge, we propose an optimization scheme with novel sparse tracking objective, which greatly stabilizes the estimation process and enables robust recovery of high-dimensional forces. Force Estimation. Researchers explored modeling contact forces for robotic manipulation [30 34] and human-object interactions [3540]. However, most approaches rely on controlled robotic environments with tactile sensors or require strong priors on hand and object shapes, as well as physical properties, to estimate forces. In contrast, our method operates on natural videos with minimal assumptions about object properties, enabling force estimation in unconstrained scenarios. Physics-based Generation. Researchers explored reconstructing physically interactive scenes [41, 42] and generating physically plausible videos [4346]. Most approaches rely on physics simulators or physics-informed neural networks [47] to animate motion, but they typically require manually specified forces and environmental conditions. Beyond these, interactive editing methods [48, 49] drive visual changes by optimizing displacement fields in the image or feature space under generative priors; such formulations specify apparent motion without estimating underlying physical forces. An alternative approach learns 3D velocity fields directly from videos [50, 51], producing smooth trajectories yet lacking explicit force representations, which makes parameter-aware edits (e.g., changing mass) less principled. In contrast, our approach automatically recovers forces and physical conditions from natural videos and applies them to novel objects, enabling physics-driven video generation without manual parameter tuning."
        },
        {
            "title": "3 Method",
            "content": "We study recovering invisible forces from videos. Our inverse graphics framework first models object properties (Section 3.1), force representations (Section 3.2), and physical processes (Section 3.3) from videos. To optimize force representations, we introduce sparse tracking objective (Section 3.4). An overview of our method is in Figure 2. 3.1 Object Modeling Capturing the essence of dynamic objects requires modeling both their shape for accurate physical interactions and their appearance for visual fidelity. This necessitates representation that seamlessly integrates precise Lagrangian shape modeling with photorealistic rendering. To this end, we adopt 3D Gaussians [1] as the representation for shape and appearance. Specifically, an object in video is 3 Figure 2: We propose differentiable inverse graphics framework to recover invisible forces from videos by integrating object modeling, physics simulation, and optimization. Objects are represented with 3D Gaussians and assigned physical properties via Vision-Language Models. Forces are modeled as causal tri-plane, and object motions are simulated using differentiable physics simulator. sparse tracking objective enables robust differentiable force recovery from videos. represented by set of Gaussian kernels in the 3D space. Each Gaussian kernel is parameterized by = {x, v, Σ, σ, SH, D, m, E, ν}, (1) where R3 and R3 are the spatial location and velocity of Gaussian kernel respectively. The covariance matrix Σ represents the shape, and opacity σ and spherical harmonics SH represent the appearance of Gaussian. Moreover, we attach each Gaussian with its physical properties: deformation gradient D, the mass m, Youngs modulus E, and the Poisson ratio ν, which we will discuss later. Since objects in video undergo motion and deformation due to external forces, their corresponding 3D Gaussians also evolve over time. Let denote timestep in the video. The Gaussians at time is then defined as Gt = {xt, vt, Σt, σ, SH, Dt, m, E, ν}, (2) where the spatial position xt, velocity vt, covariance Σt, and deformation gradient Dt change over time t. We initialize the Gaussians {G0} at = 0 only using the first frame of the video. Specifically, we use pixel-aligned point clouds that are extracted from the first image 0 via pretrained metric-depth model [52] to initialize the Gaussian positions {x0}, and we optimize {Σ0, σ, SH} via Gaussian splatting on 0. Notably, although point clouds from single image can be incomplete, we can still obtain robust force estimates thanks to the proposed sparse tracking objective, which will be discussed later. We have also explored multiview object reconstruction in our experiments. For v0 and D0, we initialized them as 0 and respectively. To model the physical interactions between objects and forces, we also need to know the objects physical properties from videos. To this end, we introduce simple but effective approach that leverages commonsense knowledge from vision-language models to assign the physical properties {m, E, ν} to each 3D Gaussian. Specifically, given the first image 0, we first query vision-language model [53] to infer the object types and provide an estimate of the physical properties {m, E, ν} from commonsense knowledge. Then, we query grounded segmentation model [54] to generate object segmentation masks based on the object types. Finally, the pixel-aligned Gaussians {G0} that are inside the object masks are assigned with the corresponding estimated physical properties {m, E, ν}. For common objects, the estimated physical properties from the vision-language model are quite robust. Hence, even without accurate system identification, our framework could provide robust force estimate with the commonsense physical properties(see Experiments 4.3). Leveraging foundation models [5254], our framework automatically recovers objects geometry, appearance, and physical properties from videos without manual effort. The recovered object Gaussians serve as unified representation for modeling physical interactions in dynamic videos. 4 3.2 Force Representations Properly modeling force representations is essential to our framework. For point contact forces, we can directly define force vectors on Gaussian particles. However, for forces that are distributed throughout 3D space, e.g., wind, we adopt the Eulerian perspective and introduce causal tri-plane to represent forces in 3D space. This representation is based on the observations that forces are spatially continuous and causally dependent over time. Specifically, we define the force at the position and the time as (x, t) = D(γ(x) + φ(t; φ(t 1))), (3) where D() is feature decoder and γ() represents the tri-plane feature map from [55]. φ() is small MLP that encodes the time t, initialized using the learned weights from the previous timestep 1, i.e., φ(t 1). Compared to other 4D representations [5658], this representation disentangles space and time, leading to superior computational efficiency. Additionally, the recursive dependency of φ(t) on φ(t 1) enables accurate modeling of evolving force dynamics over time. 3.3 Physical Process With the object Gaussians and force representations, we are ready to simulate object motion following physical laws. To this end, we implemented differentiable physics simulator for deformable objects using the Material Point Method (MPM) [2]. Empirically, we found that our object representation, where Gaussians act as Lagrangian elements, and our force representation, modeled as causal tri-plane on grid, naturally align with the formulations of [2], enabling accurate modeling of the physical process. In detail, the forward physical process Fphysics takes the object Gaussians {Gt} at time and the force field (x, t), and outputs {Gt+1} at the next timestep + 1: {Gt+1} = Fphysics({Gt}, (x, t)). (4) The physical process relies on multiple sub-steps δt to update motions from to + 1 incrementally. In the following section, we introduce the computational flow in sub-step δt. For simplicity, we consider single Gaussian Gt and omit the particle-to-grid, grid computation, and grid-to-particle process in MPM, focusing solely on the core physics principles and update formulas. For Gaussian Gt : {xt, vt, Σt, σ, SH, Dt, m, E, ν} at the time t, we first characterize the object deformations by updating the deformation gradient Dt: Dt = (I + vtδtδt)Dtδt, (5) where vtδt is the velocity gradient and is the identity matrix. Then, we update the Gaussian velocity vt by incorporating both external and internal forces: vt = vtδt + δt (xt, t) + δt fi(xt, t, E, ν, Dt) , (6) where (xt, t) is the external force by querying the casual tri-plane (x, t) at the Gaussian position xt, fi(xt, t, E, ν, Dt) represents internal forces fi determined by the constitutive model. is the mass matrix derived from the mass of the Gaussian. Note that external forces are computed on particles before the particle-to-grid step, while internal forces are calculated on the corresponding grid during the grid-to-particle step. External forces are applied directly to particles because the scene volume is much larger than the occupied regions. Acting on particles instead of grid nodes avoids empty cells and yields finer, less noisy fields aligned with the moving mass. Next, we update the position xt of the Gaussian following standard time integration: xt = xtδt + δtvt. Finally, the covariance matrix Σ is calculated based on the deformation gradient: Σt = DtΣ0(Dt)T . (7) (8) Through multiple sub-step updates, we can evolve the Gaussian state from Gt to Gt+1. Notably, the computational process is fully differentiable. Hence, given the per-Gaussian motion ˆxt ˆxt+1 5 extracted from adjacent video frames, we leverage ˆxt ˆxt+1 as the motion tracking target to optimize the force field (x, t) via backpropagation. ˆxt+1 xt+1, min (x,t) (ˆxt, xt+1) (Gt, Gt+1), Gt+1 = Fphysics(Gt, (ˆxt, t)). s.t. (9) In the following sections, we will discuss how to establish per-Gaussian motion ˆxt ˆxt+1 from videos, and how to optimize the force field (x, t) robustly. 3.4 Recovering Forces from Videos To optimize the force field (x, t), it is essential to track per-Gaussian motions ˆxt ˆxt+1 from videos as the optimization target. straightforward approach is to use photometric loss, i.e. comparing the pixel differences in adjacent frames t+1 by projecting the Gaussians onto the image plane: = π({Gt}), where π is the projection function. Nevertheless, we found that photometric loss alone fails to provide sufficient motion constraints, often resulting in vanishing gradients during optimization. Alternatively, we can extract dense 3D scene flows from videos using off-the-shelf depth and optical flow prediction or 4D reconstruction [5961]. Nevertheless, the accuracy of these pre-trained models is limited, resulting in noisy dense 3D flows that significantly hinder the optimization process. We found the key to robust optimization is to reduce the target space and adopt more reliable motion estimates. To this end, for objects with bending-only deformations (e.g., paper folding) or small deformations, we introduce novel 4D sparse-tracking objective. Specifically, we adopt more reliable point-tracking algorithm [62] that provides sparse, pixel-level estimates of object keypoint motions pt pt+1, where RN 2 is keypoint pixel coordinates. Next, we want to establish the keypoint correspondences in 3D: Pt Pt+1, where we use RN 3 and RN 2 to denote the associated keypoints in the 3D and pixel space respectively. To obtain Pt Pt+1 from pt pt+1, we first estimate the 3D keypoint locations P0 in the first frame by unprojecting p0 into 3D with depth estimates d: P0 = π1(p0, d), (10) where comes from metric-depth model [52] and π1 is an inverse-projection function. Then, for each adjacent frames, we obtain Pt Pt+1 from Pt and pt+1 by optimizing the following objective: min PtPt+1 π(Pt+1) pt+1 + λLarap, where the as-rigid-as-possible loss Larap is represented as Pt+1 Larap = Σi,jP(Pt+ ) (Pt Pt j). (11) (12) By minimizing the re-projection errors while keeping the object skeleton as rigid as possible, we obtain robust estimate of the 3D keypoint motions Pt Pt+1. Notably, without reliance on per-frame depth estimation, our method circumvents the inconsistent video depth estimation problem and enables more robust 3D motion estimates. Next, we leverage the sparse keypoint motions Pt Pt+1 to control the per-Gaussian motions ˆxt ˆxt+1 via ˆx = αiPi + αjPj + αkPk, (13) where the Equation 13 is the barycentric interpolation, Pi,j,k are the 3-nearest neighbors of ˆx, and the coefficients αi,j,k are computed in the first frame and fixed in the following frames. The sparse keypoints characterize the object skeletons and control the fine-grained Gaussian positions ˆx. Compared to the 3D scene flow approach that directly tracks each Gaussians motion, estimating sparse keypoint motions Pt Pt+1 reduces the prediction space and demonstrates superior robustness and accuracy, allowing us to obtain high-quality ˆxt ˆxt+1 for optimizing the force field. Finally, we optimize the force (x, t) using the motion-tracking loss Lmotion in Equation 9, with the estimated Gaussian motions ˆxt ˆxt+1. Moreover, we add two regularization terms Lspace and Ltime for spatial and temporal smoothness, respectively: = Lmotion + λ1Lspace + λ2Ltime, (14) where Lspace follows [56] and penalize the total variation in space, and Ltime encourages temporal smoothness by penalizing the parameter differences of the time encoder φ(): Ltime = φt+1 θ φt θ, (15) where φt θ is the parameters of the time encoder φ at the time in Equation 3. With the losses in Equation 14, we are able to recover the force (x, t) by tracking the Gaussian motions ˆxt ˆxt+1 extracted from videos."
        },
        {
            "title": "4 Experiments",
            "content": "In this section, we conduct comprehensive experiments to investigate the following key questions: Can our method successfully recover forces from both synthetic and real-world videos? (Section 4.2) How do the proposed components, i.e., force representation and loss function affect the final performance, and how robust is the proposed VLM framework to variations in object physical properties?(Section 4.3) How can our method be applied to physics-based video generation and editing? (Section 4.4) 4.1 Experimental Setup We conduct experiments on both real-world and synthetic data. For real-world data, we leverage Internet videos to verify the physical plausibility of recovered forces by visualizing the force field. In addition, we conduct real physical experiments with force gauge to measure the actual forces, and we evaluate our recovered forces via re-simulation. For synthetic data, we use synthetic objects in [63, 18] to evaluate the numerical accuracy of recovered forces. We leverage objects from 3 distinct material types, i.e., elastic, elastoplastic, viscoplastic, 6-8 unique force fields, and 2 different camera viewpoints to build the synthetic scenarios in the physics simulator [24]. We use rendered videos as inputs to our system to estimate the forces and compare them with the ground truth forces in the simulator. For numerical comparisons in synthetic scenarios, we adopt image reconstruction metrics, i.e., PSNR, SSIM [64], and LPIPS [65], to compare the re-simulated videos with the recovered forces and the original input videos, to demonstrate the accuracy of recovered forces to match the ground truth object motions in simulation. Moreover, we compare the recovered forces with the ground truth forces using two metrics: average magnitude error (reported as percentages) and direction error (measured in degrees). 4.2 Force Recovery In-the-Wild Videos. We evaluate our method on real Internet videos to demonstrate its ability to recover plausible force fields from natural object motions. Figure 4 presents qualitative results on various scenes. Our method successfully infers the underlying forces by observing object deformations and trajectories over time. The visualized forces dynamically adapt to object motion, demonstrating physically plausible force field that varies over time. Controlled Real-World Physics Experiments. Since the ground truth forces in real Internet videos are unknown, we conduct controlled real-world experiments to further validate our method. Using force gauge, we apply known forces to an object while capturing its motion on video. We then use our method to recover the force field and reapply it to the object in simulation. As shown in Figure 3, our method successfully reconstructs the objects motion and deformation, closely aligning with real-world observations. The experimental results demonstrate the accuracy of our recovered forces in real-world scenarios. Synthetic Scenarios. To evaluate the numerical accuracy of recovered forces, we build synthetic scenarios in the physics simulator to obtain the ground truth\" forces. As shown in Table 1, our method successfully recovers the original force fields with low numerical errors. These quantitative results provide strong evidence that our approach accurately estimates the underlying force dynamics and generalizes well across various objects and physical properties. Material Type Method PSNR SSIM LPIPS Mag. Error (%) Dir. Error () Elastic Elastoplastic Viscoplastic Point K-Planes Ours Point K-planes Ours Point K-planes Ours 20.57 26.25 39.79 31.40 30.14 39.93 17.49 41.73 39.00 0.94 0.96 0. 0.98 0.98 0.99 0.95 0.99 0.99 0.04 0.03 0.01 0.01 0.01 0.01 0.12 0.03 0.01 95.91 18.03 5. 98.36 87.81 75.06 98.30 89.04 21.44 76.48 39.83 4.38 26.8 61.42 45.50 92.02 33.09 7.50 Table 2: Quantitative comparison of force representations. Loss functions PSNR SSIM LPIPS Mag. Error(%) Dir. Error () Image Loss Flow+Depth Loss Ours 37.24 41.54 39.79 0.99 0.99 0.99 0.01 0.01 0. 86.74 27.90 5.14 50.23 16.07 4.38 Table 3: Quantitative comparison of loss functions. Figure 3: Comparison of observed data (left in each frame) and re-simulated results (right in each frame) for two different frames in the real-world experiment. Material type Object PSNR SSIM LPIPS Mag. Error (%) Dir. Error () Material Type #Samples Type F1 ρ MAPE (%) log-MAPE(%) Elastic Elastoplastic Lego Ficus Sunflower Toy Chair Viscoplastic Hotdog 33.70 25.92 34.08 41.35 40.10 30. 0.98 0.94 0.99 0.99 0.99 0.96 0.01 0.03 0.01 0.00 0.00 0. 19.53 23.97 14.38 29.19 33.31 15.09 7.02 11.55 7.85 8.11 23.40 11. Table 1: Force recovery in synthetic scenarios. Elastic Elastoplastic Viscoplastic Overall 6 6 5 17 1 1 1 2.38 10.80 0 4.65 5.31 3.36 13.98 7.17 Table 4: VLM performance on the daily-item dataset. Figure 4: Our method estimates invisible force fields from real-world videos, producing physically plausible motion interpretations. 4.3 Empirical Study Force Representations. To evaluate the effectiveness of our force representation in Section 3.2, we compare our causal tri-plane with other 4D representations such as K-planes [56] and point contact forces. The results in Table 2 demonstrate that our causal tri-plane force representation performs better than other 4D representations. This is mainly because our force representation accurately models the spatial continuity and temporal dependence of forces. Loss Functions. To evaluate the effectiveness of our loss functions in Section 3.4, we compare our sparse tracking loss with the image reconstruction loss and the dense 3D scene flow loss derived from depth and flow estimation (Flow+Depth loss). The results in Table 3 demonstrate that our sparse tracking loss shows better performance than others, especially in force accuracy. This is because sparse tracking provides more robust motion estimates that can be leveraged as more accurate signals to optimize the forces. VLM Material-Property Estimation. To evaluate the robustness of leveraging vision-language model for physical parameter estimation, we measure the estimation errors by utilizing the GPT4o-Vision to infer material type and material properties(density ρ and Youngs modulus E) from single image and comparing with the ground truth values. small benchmark of 17 everyday objects Figure 5: Our method recovers force fields from input videos and enables the insertion of novel objects while maintaining physically plausible motion. We demonstrate new object insertion, and modifications of physical conditions (e.g. mass and external force), showcasing the models ability to generate physically plausible videos. Figure 6: Our method allows modifying object motion by adjusting external constraints while preserving physical realism. We demonstrate how altering boundary conditions (e.g., fixing parts of an object) influences motion under the same estimated force field. These results highlight the flexibility of our approach for controllable, physics-based video editing. is collected, each made of single, well-documented engineering material. Ground-truth ρ and values are taken from MatWeb database[66]. Results are summarised in Table 4. The model achieves an overall F1 of 1.0 for material classification, an 4.65% mean-absolute-percentage error (MAPE) on density, and log-MAPE of 7.17%, which indicates that the VLM can have robust initial estimate of the physical parameters. Moving Cameras For non-stationary cameras we adopt 4D reconstruction pipelines to supply camera poses and trajectories, which we consume without retraining. Qualitative videos for dynamic-camera sequences are best viewed on the project page. 4.4 Physics-based Generation With the recovered force field, we demonstrate the potential of our approach for physics-based video generation and editing. Figure 5 shows the results of physics-based video generation. Our framework enables the replacement of novel objects within the same force field, generating physically plausible motions for novel objects via physics simulation. This flexibility also allows us to modify the objects 9 physical properties and force strengths to create distinct object motions that adhere to physical laws. Compared to other video generation methods, our approach produces more controllable and physically-accurate videos. Figure 6 shows the results of physics-based video editing, where we can modify the boundary conditions in the scenes, e.g., fixing point, to generate different physically plausible object motions in the same video. These results highlight the versatility of our framework in generating and editing videos while maintaining physical consistency. We qualitatively compare against interactive editing / motion-driven methods [48, 49] and velocityfield learning baselines [50, 51] on matched inputs. These approaches optimize displacements or velocities under strong priors and thus yield kinematically plausible results, but they do not estimate identifiable physical forces and do not enforce Newtonian consistency when physical parameters are edited (e.g., doubling mass). Consequently, the resulting motion may continue to match an appearance prior yet diverge from the dynamics implied by the edited parameters. By explicitly recovering time-varying forces inside differentiable simulator, our method preserves dynamical consistency under parameter edits and non-uniform fields. Qualitative videos are best viewed on the project page."
        },
        {
            "title": "5 Conclusion",
            "content": "In this work, we introduced differentiable inverse graphics framework to recover invisible forces from video object motions, bridging vision and physics. By modeling object properties, forces, and physical processes, our method enables robust force estimation via backpropagation. Experiments in both real-world and synthetic data have demonstrated accurate force recovery and controllable physics-based video generation and editing of our approach. Limitations. Our framework is primarily applied to objects with small deformation or bending-only deformation. Fluids or other object types that require different, differentiable physical processes are out of the scope of this paper, which is left for future works. As in our demo, we model foreground physics and composite over static background, as off-the-shelf per-frame inpainting can introduce temporal flicker that obscures our contribution."
        },
        {
            "title": "Acknowledgments and Disclosure of Funding",
            "content": "This work is in part supported by NSF RI #2211258 and #2338203, ONR MURI N00014-22-1-2740, and the Okawa Research Grant. The USC Geometry, Vision, and Learning Lab acknowledges generous supports from Toyota Research Institute, Dolby, Google DeepMind, Capital One, Nvidia, and Qualcomm. Yue Wang is also supported by Powell Research Award."
        },
        {
            "title": "References",
            "content": "[1] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkühler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Trans. Graph., 42(4):1391, 2023. [2] Chenfanfu Jiang, Craig Schroeder, Joseph Teran, Alexey Stomakhin, and Andrew Selle. The material point method for simulating continuum materials. In ACM SIGGRAPH 2016 courses, pages 152, 2016. [3] Jiajun Wu, Ilker Yildirim, Joseph Lim, Bill Freeman, and Josh Tenenbaum. Galileo: Perceiving physical object properties by integrating physics engine with deep learning. Advances in neural information processing systems, 28, 2015. [4] Jiajun Wu, Joseph Lim, Hongyi Zhang, Joshua Tenenbaum, and William Freeman. Physics 101: Learning physical object properties from unlabeled videos. In BMVC, volume 2, page 7, 2016. [5] Jiajun Wu, Erika Lu, Pushmeet Kohli, Bill Freeman, and Josh Tenenbaum. Learning to see physics via visual de-animation. Advances in neural information processing systems, 30, 2017. [6] Jennifer Cardona, Michael Howland, and John Dabiri. Seeing the wind: Visual wind speed prediction with coupled convolutional and recurrent neural network. Advances in Neural Information Processing Systems, 32, 2019. 10 [7] David Hahn, Pol Banzet, James M. Bern, and Stelian Coros. Real2sim: visco-elastic parameter estimation from dynamic motion. ACM Trans. Graph., 38(6), November 2019. [8] Krishna Murthy Jatavallabhula, Miles Macklin, Florian Golemo, Vikram Voleti, Linda Petrini, Martin Weiss, Breandan Considine, Jérôme Parent-Lévesque, Kevin Xie, Kenny Erleben, et al. gradsim: Differentiable simulation for system identification and visuomotor control. arXiv preprint arXiv:2104.02646, 2021. [9] Mingyu Ding, Zhenfang Chen, Tao Du, Ping Luo, Josh Tenenbaum, and Chuang Gan. Dynamic visual reasoning by learning differentiable physics models from video and language. Advances In Neural Information Processing Systems, 34:887899, 2021. [10] Xuan Li, Yi-Ling Qiao, Peter Yichen Chen, Krishna Murthy Jatavallabhula, Ming Lin, Chenfanfu Jiang, and Chuang Gan. Pac-nerf: Physics augmented continuum neural radiance fields for geometry-agnostic system identification. arXiv preprint arXiv:2303.05512, 2023. [11] Ri-Zhao Qiu, Ge Yang, Weijia Zeng, and Xiaolong Wang. Feature splatting: Language-driven physics-based scene synthesis and editing, 2024. [12] Albert Zhai, Yuan Shen, Emily Chen, Gloria Wang, Xinlei Wang, Sheng Wang, Kaiyu Guan, and Shenlong Wang. Physical property understanding from language-embedded feature fields. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2829628305, 2024. [13] Junhao Cai, Yuji Yang, Weihao Yuan, Yisheng He, Zilong Dong, Liefeng Bo, Hui Cheng, and Qifeng Chen. Gaussian-informed continuum for physical property identification and simulation. arXiv preprint arXiv:2406.14927, 2024. [14] Licheng Zhong, Hong-Xing Yu, Jiajun Wu, and Yunzhu Li. Reconstruction and simulation of elastic objects with spring-mass 3d gaussians. In European Conference on Computer Vision, pages 407423. Springer, 2025. [15] Xingrui Wang, Wufei Ma, Angtian Wang, Shuo Chen, Adam Kortylewski, and Alan Yuille. Compositional 4d dynamic scenes understanding with physics priors for video question answering. arXiv preprint arXiv:2406.00622, 2024. [16] Wei Chow, Jiageng Mao, Boyi Li, Daniel Seita, Vitor Guizilini, and Yue Wang. Physbench: Benchmarking and enhancing vision-language models for physical world understanding. arXiv preprint arXiv:2501.16411, 2025. [17] Tianyuan Zhang, Hong-Xing Yu, Rundi Wu, Brandon Feng, Changxi Zheng, Noah Snavely, Jiajun Wu, and William Freeman. Physdreamer: Physics-based interaction with 3d objects via video generation. In European Conference on Computer Vision, pages 388406. Springer, 2025. [18] Fangfu Liu, Hanyang Wang, Shunyu Yao, Shengjun Zhang, Jie Zhou, and Yueqi Duan. Physics3d: Learning physical properties of 3d gaussians via video diffusion. arXiv preprint arXiv:2406.04338, 2024. [19] Tianyu Huang, Yihan Zeng, Hui Li, Wangmeng Zuo, and Rynson WH Lau. Dreamphysics: Learning physical properties of dynamic 3d gaussians with video diffusion priors. arXiv preprint arXiv:2406.01476, 2024. [20] Quentin Garrido, Nicolas Ballas, Mahmoud Assran, Adrien Bardes, Laurent Najman, Michael Rabbat, Emmanuel Dupoux, and Yann LeCun. Intuitive physics understanding emerges from self-supervised pretraining on natural videos. arXiv preprint arXiv:2502.11831, 2025. [21] Tao Du, Kui Wu, Pingchuan Ma, Sebastien Wah, Andrew Spielberg, Daniela Rus, and Wojciech Matusik. Diffpd: Differentiable projective dynamics. ACM Trans. Graph., 41(2), nov 2021. [22] William Gilpin. Generative learning for nonlinear dynamics. Nature Reviews Physics, 6(3):194 206, 2024. [23] Eric Heiden, David Millard, Erwin Coumans, Yizhou Sheng, and Gaurav Sukhatme. Neuralsim: Augmenting differentiable simulators with neural networks. In 2021 IEEE International Conference on Robotics and Automation (ICRA), pages 94749481. IEEE, 2021. 11 [24] Yuanming Hu, Yu Fang, Ziheng Ge, Ziyin Qu, Yixin Zhu, Andre Pradhana, and Chenfanfu Jiang. moving least squares material point method with displacement discontinuity and two-way rigid body coupling. ACM Transactions on Graphics (TOG), 37(4):114, 2018. [25] Yuanming Hu, Tzu-Mao Li, Luke Anderson, Jonathan Ragan-Kelley, and Frédo Durand. Taichi: language for high-performance computation on spatially sparse data structures. ACM Transactions on Graphics (TOG), 38(6):201, 2019. [26] Yuanming Hu, Luke Anderson, Tzu-Mao Li, Qi Sun, Nathan Carr, Jonathan Ragan-Kelley, and Frédo Durand. Difftaichi: Differentiable programming for physical simulation. ICLR, 2020. [27] Yuanming Hu, Jiafeng Liu, Xuanda Yang, Mingkuan Xu, Ye Kuang, Weiwei Xu, Qiang Dai, William T. Freeman, and Frédo Durand. Quantaichi: compiler for quantized simulations. ACM Transactions on Graphics (TOG), 40(4), 2021. [28] Junior Rojas, Eftychios Sifakis, and Ladislav Kavan. Differentiable implicit soft-body physics. arXiv preprint arXiv:2102.05791, 2021. [29] Zizhou Huang, Davi Colli Tozoni, Arvi Gjoka, Zachary Ferguson, Teseo Schneider, Daniele Panozzo, and Denis Zorin. Differentiable solver for time-dependent deformation problems with contact. ACM Transactions on Graphics, 43(3):130, May 2024. [30] Barza Nisar, Philipp Foehn, Davide Falanga, and Davide Scaramuzza. Vimo: Simultaneous visual inertial model-based odometry and force estimation. IEEE Robotics and Automation Letters, 4(3):27852792, 2019. [31] Xingyu Chen, Jialei Shi, Helge Wurdemann, and Thomas George Thuruthel. Vision-based tip force estimation on soft continuum robot. In 2024 IEEE International Conference on Robotics and Automation (ICRA), pages 76217627. IEEE, 2024. [32] Wenzhen Yuan, Rui Li, Mandayam Srinivasan, and Edward Adelson. Measurement of shear and slip with gelsight tactile sensor. In 2015 IEEE International Conference on Robotics and Automation (ICRA), pages 304311. IEEE, 2015. [33] Eric Heiden, Miles Macklin, Yashraj Narang, Dieter Fox, Animesh Garg, and Fabio Ramos. Disect: differentiable simulation engine for autonomous robotic cutting. arXiv preprint arXiv:2105.12244, 2021. [34] Haozhe Lou, Yurong Liu, Yike Pan, Yiran Geng, Jianteng Chen, Wenlong Ma, Chenglong Li, Lin Wang, Hengzhen Feng, Lu Shi, et al. Robo-gs: physics consistent spatial-temporal model for robotic arm with hybrid representation. arXiv preprint arXiv:2408.14873, 2024. [35] Hyun Soo Park, Jianbo Shi, et al. Force from motion: decoding physical sensation in first person video. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 38343842, 2016. [36] Zongmian Li, Jiri Sedlar, Justin Carpentier, Ivan Laptev, Nicolas Mansard, and Josef Sivic. Estimating 3d motion and forces of person-object interactions from monocular video. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 86408649, 2019. [37] Davis Rempe, Leonidas Guibas, Aaron Hertzmann, Bryan Russell, Ruben Villegas, and Jimei Yang. Contact and human dynamics from monocular video. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part 16, pages 7187. Springer, 2020. [38] Kiana Ehsani, Shubham Tulsiani, Saurabh Gupta, Ali Farhadi, and Abhinav Gupta. Use the force, luke! learning to predict physical forces by simulating effects. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 224233, 2020. [39] Yufei Zhang, Jeffrey Kephart, Zijun Cui, and Qiang Ji. Physpt: Physics-aware pretrained transformer for estimating human dynamics from monocular videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 23052317, 2024. [40] Bin Wang, Longhua Wu, KangKang Yin, Uri Ascher, Libin Liu, and Hui Huang. Deformation capture and modeling of soft objects. ACM Trans. Graph., 34(4):941, 2015. [41] Gengshan Yang, Shuo Yang, John Zhang, Zachary Manchester, and Deva Ramanan. Ppr: Physically plausible reconstruction from monocular videos. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 39143924, 2023. [42] Yandan Yang, Baoxiong Jia, Peiyuan Zhi, and Siyuan Huang. Physcene: Physically interactable 3d scene synthesis for embodied ai. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1626216272, 2024. [43] Katerina Fragkiadaki, Pulkit Agrawal, Sergey Levine, and Jitendra Malik. Learning visual predictive models of physics for playing billiards. arXiv preprint arXiv:1511.07404, 2015. [44] Zhengqi Li, Richard Tucker, Noah Snavely, and Aleksander Holynski. Generative image dynamics, 2024. [45] Shaowei Liu, Zhongzheng Ren, Saurabh Gupta, and Shenlong Wang. Physgen: Rigid-body physics-grounded image-to-video generation. In European Conference on Computer Vision, pages 360378. Springer, 2024. [46] Tianyi Xie, Zeshun Zong, Yuxing Qiu, Xuan Li, Yutao Feng, Yin Yang, and Chenfanfu Jiang. Physgaussian: Physics-integrated 3d gaussians for generative dynamics. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 43894398, 2024. [47] Chayan Banerjee, Kien Nguyen, Clinton Fookes, and Karniadakis George. Physics-informed computer vision: review and perspectives. ACM Computing Surveys, 57(1):138, 2024. [48] Xingang Pan, Ayush Tewari, Thomas Leimkühler, Lingjie Liu, Abhimitra Meka, and Christian Theobalt. Drag your gan: Interactive point-based manipulation on the generative image manifold. In ACM SIGGRAPH 2023 conference proceedings, pages 111, 2023. [49] Daniel Geng, Charles Herrmann, Junhwa Hur, Forrester Cole, Serena Zhang, Tobias Pfaff, Tatiana Lopez-Guevara, Carl Doersch, Yusuf Aytar, Michael Rubinstein, Chen Sun, Oliver Wang, Andrew Owens, and Deqing Sun. Motion prompting: Controlling video generation with motion trajectories. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2025. [50] Jinxi Li, Ziyang Song, and Bo Yang. Nvfi: Neural velocity fields for 3d physics learning from dynamic videos. Advances in Neural Information Processing Systems, 36:3472334751, 2023. [51] Jinxi Li, Ziyang Song, Siyuan Zhou, and Bo Yang. Freegave: 3d physics learning from dynamic videos by gaussian velocity. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1243312443, 2025. [52] Luigi Piccinelli, Yung-Hsu Yang, Christos Sakaridis, Mattia Segu, Siyuan Li, Luc Van Gool, and Fisher Yu. Unidepth: Universal monocular metric depth estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1010610116, 2024. [53] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. [54] Tianhe Ren, Shilong Liu, Ailing Zeng, Jing Lin, Kunchang Li, He Cao, Jiayu Chen, Xinyu Huang, Yukang Chen, Feng Yan, et al. Grounded sam: Assembling open-world models for diverse visual tasks. arXiv preprint arXiv:2401.14159, 2024. [55] Eric Chan, Connor Lin, Matthew Chan, Koki Nagano, Boxiao Pan, Shalini De Mello, Orazio Gallo, Leonidas Guibas, Jonathan Tremblay, Sameh Khamis, et al. Efficient geometryaware 3d generative adversarial networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1612316133, 2022. 13 [56] Sara Fridovich-Keil, Giacomo Meanti, Frederik Rahbæk Warburg, Benjamin Recht, and Angjoo Kanazawa. K-planes: Explicit radiance fields in space, time, and appearance. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1247912488, 2023. [57] Ang Cao and Justin Johnson. Hexplane: fast representation for dynamic scenes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 130141, 2023. [58] Haotong Lin, Qianqian Wang, Ruojin Cai, Sida Peng, Hadar Averbuch-Elor, Xiaowei Zhou, and Noah Snavely. Neural scene chronology. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2075220761, 2023. [59] Qianqian Wang, Vickie Ye, Hang Gao, Weijia Zeng, Jake Austin, Zhengqi Li, and Angjoo Kanazawa. Shape of motion: 4d reconstruction from single video. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 96609672, 2025. [60] Quankai Gao, Qiangeng Xu, Zhe Cao, Ben Mildenhall, Wenchao Ma, Le Chen, Danhang Tang, and Ulrich Neumann. Gaussianflow: Splatting gaussian dynamics for 4d content creation, 2024. [61] Zhengqi Li, Richard Tucker, Forrester Cole, Qianqian Wang, Linyi Jin, Vickie Ye, Angjoo Kanazawa, Aleksander Holynski, and Noah Snavely. Megasam: Accurate, fast and robust structure and motion from casual dynamic videos. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1048610496, 2025. [62] Nikita Karaev, Ignacio Rocco, Benjamin Graham, Natalia Neverova, Andrea Vedaldi, and Christian Rupprecht. Cotracker: It is better to track together. In European Conference on Computer Vision, pages 1835. Springer, 2024. [63] Ben Mildenhall, Pratul Srinivasan, Matthew Tancik, Jonathan Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 65(1):99106, 2021. [64] Zhou Wang, Alan Bovik, Hamid Sheikh, and Eero Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing, 13(4):600 612, 2004. [65] Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric, 2018. [66] MatWeb, LLC. Matweb: Online materials information resource, 2025. Accessed: 2025-05-13."
        }
    ],
    "affiliations": [
        "Stanford University",
        "University of Southern California"
    ]
}