{
    "paper_title": "Mindscape-Aware Retrieval Augmented Generation for Improved Long Context Understanding",
    "authors": [
        "Yuqing Li",
        "Jiangnan Li",
        "Zheng Lin",
        "Ziyan Zhou",
        "Junjie Wu",
        "Weiping Wang",
        "Jie Zhou",
        "Mo Yu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Humans understand long and complex texts by relying on a holistic semantic representation of the content. This global view helps organize prior knowledge, interpret new information, and integrate evidence dispersed across a document, as revealed by the Mindscape-Aware Capability of humans in psychology. Current Retrieval-Augmented Generation (RAG) systems lack such guidance and therefore struggle with long-context tasks. In this paper, we propose Mindscape-Aware RAG (MiA-RAG), the first approach that equips LLM-based RAG systems with explicit global context awareness. MiA-RAG builds a mindscape through hierarchical summarization and conditions both retrieval and generation on this global semantic representation. This enables the retriever to form enriched query embeddings and the generator to reason over retrieved evidence within a coherent global context. We evaluate MiA-RAG across diverse long-context and bilingual benchmarks for evidence-based understanding and global sense-making. It consistently surpasses baselines, and further analysis shows that it aligns local details with a coherent global representation, enabling more human-like long-context retrieval and reasoning."
        },
        {
            "title": "Start",
            "content": "Mindscape-Aware Retrieval Augmented Generation for Improved Long Context Understanding Yuqing Li1,2* Jiangnan Li3 Zheng Lin1,2 Ziyan Zhou1,2 Junjie Wu4 Weiping Wang1 Jie Zhou3 Mo Yu3 1Institute of Information Engineering, Chinese Academy of Sciences 2School of Cyber Security, University of Chinese Academy of Sciences 3WeChat AI, Tencent 4Hong Kong University of Science and Technology {liyuqing, linzheng}@iie.ac.cn {jiangnanli, moyumyu}@tencent.com 5 2 0 2 9 1 ] . [ 1 0 2 2 7 1 . 2 1 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Humans understand long and complex texts by relying on holistic semantic representation of the content. This global view helps organize prior knowledge, interpret new information, and integrate evidence dispersed across document, as revealed by the Mindscape-Aware Capability of humans in psychology. Current Retrieval-Augmented Generation (RAG) systems lack such guidance and therefore struggle with long-context tasks. In this paper, we propose Mindscape-Aware RAG (MiA-RAG), the first approach that equips LLM-based RAG systems with explicit global context awareness. MiA-RAG builds mindscape through hierarchical summarization and conditions both retrieval and generation on this global semantic representation. This enables the retriever to form enriched query embeddings and the generator to reason over retrieved evidence within coherent global context. We evaluate MiARAG across diverse long-context and bilingual benchmarks for evidence-based understanding and global sense-making. It consistently surpasses baselines, and further analysis shows that it aligns local details with coherent global representation, enabling more human-like longcontext retrieval and reasoning."
        },
        {
            "title": "Introduction",
            "content": "Human thinking is inherently context-dependent. For any learned topic, familiar situation, or ongoing project of engagement, humans maintain global semantic representation in memory. When the same topic reappears, this global memory is reactivated, endowing humans with the MindscapeAware capability to become aware of the approximate scope of their knowledge and to rely on this memory to interpret new inputs within context, selectively channel retrieval toward contextrelevant knowledge, and guide subsequent reasoning accordingly. This phenomenon is grounded in * Equal contribution Corresponding authors 1 Figure 1: Average model ranks across five long-context benchmarks under 3/5/10-chunk settings. theories from psychology (Bartlett, 1932; Tulving and Thomson, 1973; Reyna and Brainerd, 1995) and neuroscience (Ralph et al., 2017), which posit that when topic is reactivated, conscious cognition is constrained and guided by globally integrated knowledge, with converging support from neuroimaging observations. We review both the theoretical and empirical supports in Appx. A. Retrieval-Augmented Generation (RAG) (Zhu et al., 2025; Gao et al., 2023; Zhang et al., 2025a) has emerged as critical strategy for long-context understanding by retrieving useful context fragments from very long inputs, thereby overcoming LLMs limited context lengths (Lewis et al., 2020). However, current RAG systems primarily retrieve and generate based on local, evidence-level signals, lacking the mindscape-aware capability to activate global semantic frame as humans do. Endowing RAG with this capability is therefore especially promising for personalized knowledge collections, such as long-context question answering (Bai et al., 2023), code generation (Wang et al., 2024b; Miao et al., 2024), and AI assistants over personal projects (Martin and Johnson, 2023). Specifically, the cognitive advantages of the mindscape translate naturally into the following benefits: Enriched Understanding: supported by awareness of global semantics that fills missing information and resolves underspecified meanings. Selective Retrieval: biases query embeddings toward the active topics conceptual frame, filtering out ambiguities arising from other topics. Integrative Reasoning: interprets retrieved results within the global context to ensure coherent synthesis and understanding. Motivated by these insights, we propose the first approach to equip LLM-based RAG systems with mindscape-aware capabilities. Specifically, we tackle the long-context understanding problem by approximating the global impression of long document with summary generated in hierarchical manner, which serves as an external representation of global memory. Taking this form of global memory as an additional input, we train models to fit two core functions of our new Mindscape-Aware RAG (MiA-RAG) framework: Mindscape-Aware Retrieval The mindscapeaware capability enables queries to be understood within their global semantic context, producing query representations that are not only anchored to the topical scope (Selective Retrieval), but also integrated with global contextual information (Enriched Understanding). We instantiate these capabilities through specially trained embedding model, which takes both the global memory and the query as input, and learns to integrate global information into query embeddings to enhance retrieval performance. Mindscape-Aware Generation Solely enhancing the RAG pipeline with contextually informed query embeddings introduces new challenge: the generator becomes weaker than the retriever, as it lacks access to the global context. Consequently, the generator may misinterpret the relevance of the retrieved information or fail to effectively utilize it, even when the retrieved content contains the correct evidence. To mitigate this asymmetry, we condition generation on the same global memory. By incorporating the global memory into the generators inputs, the model learns to interpret the retrieved chunks and their relationship to the query within the broader global context (Integrative Reasoning), leading to more faithful reasoning and answers. We evaluate MiA-RAG across range of longcontext understanding tasks spanning diverse domains and genres (e.g., government reports, narratives), in both English and Chinese. The evaluation also covers various task formats, including freeform QA, multiple-choice QA, and claim verification, as well as different RAG configurations such as vanilla RAG and GraphRAG (Edge et al., 2024). As summarized in Figure 1, the MiA family is more effective than baselines; in particular, MiA-RAG14B achieves the best average rank, surpassing the vanilla 72B system and highlighting the benefit of mindscape-aware retrieval and generation. Beyond performance gains, we analyze MiARAGs internal mechanisms via embedding-space geometry and new Mindscape-Coherent Evidence Alignment (MCEA) metric, showing that it internalizes global semantics: the mindscape reshapes query representations toward the global semantic space and acts as scaffold that guides attention for Integrative Reasoning. Our contributions are summarized as follows: (1) We formulate the psychoand neuro-inspired problem of mindscape-aware thinking, and present the first computational solution that equips LLMs with this capability. (2) We conduct extensive experiments under diverse settings, demonstrating the necessity and effectiveness of integrating mindscape-aware capability into LLM-based systems. (3) Our in-depth analysis reveals that the mindscape aligns the geometry of query representations with the global semantic space and serves as semantic scaffold to guide attention, confirming the active internalization of global context rather than surface-level pattern matching."
        },
        {
            "title": "2 Related Work",
            "content": "Context-Aware Embeddings Our MiA-Emb is related to the research topic of context-aware retrieval (or contextual retrieval (Anthropic, 2024)). This line of work mainly focuses on producing embeddings enriched with contextual information. straightforward approach is to encode each chunk within long-context window using LLMs that support extended inputs (Chen et al., 2024; Sturua et al., 2024; Nussbaum et al., 2024; Wang et al., 2024a; Lee et al., 2024; Li et al., 2023; Voyage-AI, 2025). To incorporate information beyond the local window, Xu et al. (2024) construct graphs of discourse relations and then utilize graph neighbors 2 for augmentation. While these methods provide additional context, they do not directly teach the model how to fuse such information. Recently, (Wu et al., 2025) introduces training techniques that enable the embedding model to more effectively situate chunk within its local neighborhood, achieving state-of-the-art retrieval performance with only modest training data requirements. However, these approaches primarily enhance retrieval by enriching the chunk representation with local context. In contrast, our method injects global semantics context into the query representation, guiding queries toward the correct semantic region of the index and improving selective retrieval without modifying the original chunk embeddings. Long Context Compression To capture the overarching semantics of long documents, recent research moves beyond simple token retention or KV-cache compression (Yang et al., 2024; Li et al., 2024; Xiao et al., 2023; Zhang et al., 2023) toward abstractive compression and global memory formation (Qian et al., 2025; Behrouz et al., 2024). For instance, MemoRAG (Qian et al., 2025) utilizes this compressed memory for explicit planning, generating intermediate clues to bridge the gap between query and evidence. Unlike this agentic reasoning, our approach avoids the overhead of generating intermediate search steps. Behrouz et al. (2024) introduce Titans, which learns to memorize historical context at test time via long-term memory module that stores abstractions and references them alongside current inputs. Our work interprets the mindscape as persistent global semantic anchor constructed by processing the full text."
        },
        {
            "title": "3 Method",
            "content": "In this section, we introduce the Mindscape-Aware RAG (MiA-RAG) framework, designed to emulate the human cognitive process of leveraging global context for understanding, retrieval, and reasoning. 3.1 Preliminaries Let Doc be long document that has been partitioned into chunks ci C. In standard RAG pipeline, retriever selects set of chunks Cret for query q, and the generator conditions on these chunks to produce an answer a: Cret a. (1) However, this setup does not provide global view of the document for either retrieval or generation. To bridge this gap, we propose MiA-RAG, which incorporates an explicit global semantic scaffold termed the Mindscape S. By conditioning both retrieval and generation on S, MiA-RAG situates local evidence within global context, improving retrieval accuracy and reasoning consistency: q, Cret,S a. (2) 3.2 Hierarchical Mindscape Construction The Mindscape S, which serves as the global memory of our framework, is constructed through hierarchical bottom-up summarization process. We first prompt summarizer Ms (GPT-4o) with [INST]sum_c (Appx. F) to summarize each chunk: si = Ms([INST]sum_c, ci). (3) After obtaining the chunk-level summaries, the sequence of {si} is concatenated in order and further summarized using [INST]sum_g (Appx. F) to produce single global representation: = Ms([INST]sum_g, [s1, s2, . . . , sn]). (4) The resulting provides coherent, documentlevel abstraction that constitutes the Mindscape. 3.3 MiA-Emb: Mindscape-Aware Retriever To train the Mindscape-Aware Retriever (MiAEmb), we construct supervision dataset and optimize the model via multi-task objective. 3.3.1 Supervision Construction Existing long-narrative understanding datasets such as NarrativeQA (Koˇcisk`y et al., 2018) typically provide only QA pairs and do not link questions to fine-grained supporting evidence. Such supervision is essential for training long-context retrievers, whether evidence is represented as text chunks in standard RAG or as graph nodes in GraphRAG. Given the cost and inefficiency of manual annotation, we automatically extend NarrativeQA to curate Demb, dataset offering silver-standard alignments at both chunk and node levels. For chunk evidence, we perform query augmentation, majority-vote ensemble retrieval, and LLM-based filtering to identify silver chunks (Algorithm 1). As shown in Table 1, an oracle experiment on 20 books validates these annotations: using only silver chunks (1/30 tokens) already exceeds full-context performance. Node evidence is constructed in an analogous way. In total, Demb comprises 27,117 questions, averaging 2.3 silver chunks and 2.9 nodes per question. Details of the construction are provided in Appx B. 3 Input Avg. Chunks EM F1 Silver Chunks Oracle (Full Book) 2.3 69.2 31.29 30.04 52.91 50.79 Table 1: Validation of silver chunk annotations. Avg. Chunks is the average number of chunks per query. 3.3.2 Model Optimization We obtain MiA-Emb by fine-tuning pre-trained embedding model so that global context is explicitly injected into query representations under our supervision signal. Mindscape-Conditioned Query Encoding We encode each query qi by explicitly incorporating the summary together with task-specific control tokens d. The input sequence is defined as Optimization MiA-Gen is optimized over Dgen using the autoregressive cross-entropy loss: LMiA-Gen = (cid:88) yi (cid:88) t= log Pθ(yi,t yi,<t, xgen ). (7)"
        },
        {
            "title": "4 Experimental Setting",
            "content": "Evaluated Models MiA-RAG is implemented by fine-tuning Qwen-series (Zhang et al., 2025b; Team, 2024) large language models. We develop two core components: Mindscape-Aware Retriever: we fine-tune Qwen3-Embedding-8B to obtain the MiA-Emb. Mindscape-Aware Generator: we fully finetune Qwen2.5-14B-Instruct to obtain MiA-Gen. We have released our models on Hugging face. = [[INST]emb; qi; dq; S; dn; dc], (5) Implementation details are provided in Appx C. where [INST]emb (Appx. F) is instruction prefix, dq marks the end of the query, and dn and dc activate nodeand chunk-retrieval modes, respectively. Residual Integration and Training Objective To balance the original query intent with global guidance, we use residual integration mechanism to form task-specific enriched query representations and train MiA-Emb with contrastive objective (van den Oord et al., 2018). Full details are provided in Appx. B.3. 3.4 MiA-Gen: Mindscape-Aware Generator Training Data Construction We obtain the mindscape-aware generator (MiA-Gen) using supervised fine-tuning corpus built from two datasets: NarrativeQA for long-form question answering and CLIPPER (Pham et al., 2025), synthetic dataset of narrative true/false claims with chainof-thought rationales for claim verification. Each example is formatted as: [INST]gen (Appx. F); S; ˆCret; qi (cid:123)(cid:122) (cid:125) (cid:124) gen yi, (6) To simulate realistic retrieval conditions, ˆCret is generated by mixing silver chunks with irrelevant chunks. The resulting contexts contain both relevant and noisy evidence with varied lengths, mirroring real retrieval behavior. For the Clipper dataset, we directly use the retrieval results produced by our MiA-Emb model to obtain ˆCret. Combining these elements yields the final supervised fine-tuning dataset Dgen for MiA-Gen. Public Long Narrative Understanding Tasks We evaluate our method on diverse set of longnarrative understanding benchmarks, many of which contain contexts beyond standard LLM input limits (128K). Since our model is trained on the NarrativeQA (Koˇcisk`y et al., 2018) training set, we first perform in-domain evaluation on heldout books to assess narrative comprehension. To examine cross-domain generalization, we further evaluate on EN.MC subset of Bench (Zhang et al., 2024) for multiple-choice reasoning, DetectiveQA (Xu et al., 2025) for bilingual long-text reasoning, and the public subset of Nocha (Karpinska et al., 2024) for claim verification. Dataset statistics and metrics are provided in Table 3."
        },
        {
            "title": "5 Experiments",
            "content": "5.1 Study I: Retrieval Results Table 4 summarizes retrieval performance. MiAEmb consistently outperforms all baselines across the benchmarks, and even surpasses Sit-Emb (Wu et al., 2025), state-of-the-art model specialized for story understanding. Further comparisons with other embedding models are given in Appx. D.4. On the in-domain NarrativeQA, MiA-Emb exhibits substantial gains, validating its ability to leverage global context for precise evidence localization. This advantage also transfers to the out-of-domain, bilingual DetectiveQA benchmark, where MiA-Emb remains clearly superior in Answer Recall. Finally, our ablation study shows that removing the summary (w/o Summary) leads to substantial 4 Model Retriever Generator NarrativeQA Bench Det.QA-Zh Det.QA-En Emb. Model +S Gen. Model +S F1 Acc Acc Acc NoCha Pair Acc Summary-Only Vanilla MiA (Gen-Only) MiA (Emb-Only) MiA - Qwen3-8B - Qwen3-8B MiA-Emb-8B MiA-Emb-8B Qwen2.5-72B Qwen2.5-72B Qwen2.5-72B Qwen2.5-72B Qwen2.5-72B Summary-Only Vanilla - Qwen3-8B - Qwen2.5-14B Qwen2.5-14B 39.24 41.13/45.51/49. 72.05 75.55/80.79/86.90 73.67 63.67/70.83/78.00 61.33 55.50/61.33/71.17 31.75 33.33/38.10/41.27 47.67/48.46/51.81 46.38/48.06/49.88 50.05/51.04/53.15 82.10/83.84/86.46 84.72/87.77/90.39 84.71/86.46/88. 76.00/78.50/81.33 76.17/81.17/82.67 81.67/83.17/84.17 68.17/69.67/73.33 67.17/71.83/75.33 70.33/72.33/75.50 36.51/42.86/44.44 42.86/42.86/49.21 41.27/44.44/52.38 38.03 39.32/40.99/44.38 61.57 72.49/73.80/77.29 70.50 60.33/68.50/75. 58.00 55.17/59.83/66.67 17.46 17.46/11.11/15.87 MiA (Gen-Only) MiA (Gen-Only) MiA (Emb-Only) MiA MiA-RAG Qwen3-8B Qwen3-8B MiA-Emb-8B MiA-Emb-8B MiA-Emb-8B Qwen2.5-14B MiA-Gen-14B Qwen2.5-14B Qwen2.5-14B MiA-Gen-14B 43.04/43.32/45.83 50.55/50.08/51.99 45.89/46.77/47.13 44.38/46.66/47.87 52.48/53.52/53. 75.98/80.79/79.48 75.98/82.10/80.79 79.48/82.97/84.28 79.91/83.41/84.28 80.79/81.22/85.15 75.33/78.83/80.17 76.17/78.50/79.50 73.00/77.17/80.83 78.33/79.00/82.00 79.00/79.67/81.83 65.33/66.83/70.00 67.67/71.50/71.67 62.33/65.83/70.33 66.50/69.17/71.82 69.00/71.17/75.50 19.05/15.87/26.98 49.21/47.62/50.79 22.22/26.98/26.98 30.16/28.57/38.10 55.56/49.21/53.97 Avg. 55.61 59. 64.74 66.43 67.93 49.11 51.93 57.79 65.61 59.48 62.01 68.11 Table 2: Overall results of the MiA framework on Long-story QA and Reasoning benchmarks. Each model is evaluated under top-3/5/10 retrieval settings. +S indicates whether the Mindscape summary is incorporated at that stage. Darker gray rows represent deeper mindscape involvement, while bold numbers indicate the best results within each scale group. The final column reports the average over all metrics per row. Dataset Queries Avg. Tokens Metrics Method NarrativeQA Bench DetectiveQA NoCha NarrativeQA Bench-EN.MC DetectiveQA Nocha 556 229 1,200 126 83k 184k 118k 139k F1, EM, Recall Accuracy Accuracy, Recall Pairwise Acc. MiA-Gen-14B w/o Summary w/o Claim. w/o QA 53.19 50.49 51.22 46. 82.39 75.39 85.44 81.08 76.03 71.03 75.58 72.81 52.91 44.97 44.44 46.56 Table 3: Summary of the evaluation datasets. Table 5: Ablation study of MiA-Gen-14B. Reported scores are averaged over the 3/5/10-chunk settings. Method NarrativeQA DetectiveQA-ZH DetectiveQA-EN 3 5 3 5 10 3 5 62.68 MiA-Emb-8B 55.62 w/o Summary Sit-Emb-8B 59.98 Qwen-Emb-8B 41.81 75.92 67.19 70.70 54.51 88.09 83.65 82.68 71.13 46.75 37.92 42.50 28.58 59.17 48.75 54.50 39.08 72.50 66.50 69.30 55. 42.08 34.00 36.75 24.17 54.17 45.75 49.25 34.17 69.75 61.25 63.83 49.25 Table 4: Retrieval performance comparison across models, evaluated using Recall@K (%). degradation, underscoring the essential role of the mindscape representation in enhancing retrieval. 5.2 Study II: Long Narrative Understanding 5.2.1 End-Task Results Table 2 presents the complete RAG pipeline evaluation across five long-context benchmarks. We also include results on the Helmet (Yen et al., 2024) version of NarrativeQA to compare with long-window LLM solutions in Appx. D.1, on which our MiARAG improves over GPT-4o with only 3% of input tokens. There are three key takeaways. MiA-RAG Achieves the Superior Overall Performance. Across all long-context benchmarks in our study, covering English and Chinese, diverse domains and genres, and multiple task formats, our full MiA-RAG pipeline attains the best overall results, with absolute gains of +16.18% over the vanilla 14B baseline and +8.63% over the vanilla 72B. These gains suggest that mindscape-aware alignment is more important than simply scaling model size or input length. Mindscape-Aware Retrieval Consistently Improves Performance As verified in Sec. 5.1, MiA-Emb substantially enhances retrieval quality. When integrated into the full pipeline, substituting the vanilla retriever with MiA-Emb yields consistent gains. MiA-Emb improves the average scores of the 72B and 14B generators by 6.95% and 7.55%, respectively, confirming that globally informed queries yield more effective retrieval. Integrative Reasoning Benefits from MindscapeConditioned Generation Simply supplying the summary to vanilla generator yields consistent +3.79% improvement, showing that global contextual cues provide useful guidance. larger gain is observed when the generator is fine-tuned under the same mindscape-conditioning paradigm as the retriever. Under identical inputs, our MiA-Gen14B achieves substantially larger +11.16% gain. This disparity suggests that MiA-Gen more effectively integrates retrieved chunks with the global semantics that guided their selection. 5.2.2 Ablation Study We perform ablations to assess each MiA-RAG component, with results shown in Tables 4 and 5. Impact of Mindscape-Conditioning Same as in the embedding stage  (Table 4)  , removing the summary (w/o Summary) leads to substantial degradation in the generation stage  (Table 5)  . These 5 Figure 2: Impact of retriever scale on average results across 5 benchmarks (DetectiveQA-ZH/EN,Bench, NoCha, NarrativeQA) with Qwen2.5-72B generator. Figure 3: Impact of generator model scale on average results over 5 benchmarks, with MiA-Emb-8B retriever. declines indicate that summary-based supervision helps align queries with global semantics and supports the integration of dispersed evidence. Benefit of Multi-Paradigm Supervision Ablating either supervision paradigm (w/o Claim. for claim verification or w/o QA for question answering) consistently degrades performance, indicating that exposure to diverse reasoning patterns improves generalization beyond any single task. 5.3 Study III: MiA-GraphRAG for Global QA We further evaluate MiA-RAG for global sensemaking in GraphRAG QA setting, where it retrieves relevant graph nodes (entities) for holistic document understanding and achieves clear gains over baselines. Details are reported in Appx. D.2. 5.4 Study IV: Impact of Model Scales We evaluate the scalability of MiA-Emb across backbone sizes (0.6B to 8B) against SFT-Emb (identical to the w/o Summary ablation in Sec. 5.1, i.e., trained and evaluated without summaries) and Vanilla Qwen3-Embedding baselines. As shown in Figures 2 and 8, MiA-Emb consistently outperforms both baselines; notably, MiA-Emb-0.6B already surpasses the Vanilla 8B model in both retrieval recall and downstream QA and reasoning performance. We also scale MiA-Gen across model sizes and observe consistent gains over the vanilla Qwen2.5-Instruct models (1.5B72B), presented in Figure 3. In particular, MiA-Gen-14B matches or even exceeds the 72B model. Collectively, these results indicate that incorporating global semantics is more effective for longcontext understanding than merely scaling model size. Numerical results are provided in Appx. D.3. 5.5 Study V: Impact of the Quality of Summaries We show that our MiA-RAG is robust to the quality of summaries. Specifically, we replace the 6 Summary Generator Recall@3/5/10 (%) F1-Score (%) GPT-4o (Ours) Qwen2.5-32B-Instruct Qwen2.5-14B-Instruct Qwen2.5-7B-Instruct 62.68/75.92/88.09 61.66/74.60/88.06 59.74/73.54/87.68 58.62/72.61/86. 52.48/53.52/53.56 50.20/51.80/53.37 51.45/51.81/52.61 49.79/51.55/51.48 Table 6: Impact of summary quality on NarrativeQA. GPT-4o summaries with those generated by three smaller open-source models: Qwen2.5-7B-Instruct, Qwen2.5-14B-Instruct, and Qwen2.5-32B-Instruct. As shown in Table 6, MiA-RAG remains stable across wide range of summary qualities. Notably, summaries produced by the 32B model and GPT-4o achieve close performance. These results suggest that the mindscape does not need to be perfectly accurate. As long as the summary captures the overarching structure and narrative flow, it provides adequate global guidance for both retrieval and reasoning."
        },
        {
            "title": "6 Analysis",
            "content": "In this section, we introduce analytical methods to evaluate whether the resulting MiA-RAG exhibits the three hypothesized capabilities proposed in the introduction, namely Enriched Understanding, Selective Retrieval, and Integrative Reasoning. 6.1 The Role of Global Summaries While the ablation confirms that MiA-RAG benefits from incorporating summaries into the inputs, critical question remains regarding their functional role during inference. We first show that summaries are not useful simply because they cover the answer. To illustrate this, we evaluate Summary-Only variant in which the generator predicts the answer using only the summary  (Table 2)  . This model consistently underperforms the vanilla-RAG and falls far short of the MiA-RAG results. These findings indicate that the summaries function not as standalone evidence but as semantic scaffolds: they enhance retrieval and reasoning by guiding the model to operate within We verify the hypothesis in two folds. First, we examine whether performance gains from MiAEmb correlate with increased use of the global summary. If so, we then study whether the model focuses its attention on information that can enrich the queries in these situations. Figure 5: Layer-wise comparison of silver-chunk retrieval accuracy and attention allocation proportion. (H2.1) MiA-Emb puts increased attention to the global summary at layers of improved predictability compared to the baseline. Method Following the approach of (Jiang et al., 2024), we compare MiA-Emb with the vanilla embedding model through their residual streams to analyze how retrieval-relevant information is progressively accumulated into the query representation. To ensure comparability, we select 100 queries for which both models achieve Recall@10 = 100%. Concretely, we track the layer-wise Top-10 silverchunk ratio for both models, reflecting how the hidden states at each layer steer the retrieval distribution toward the correct evidence. For MiA-Emb, we additionally examine the attention from the last token to summary tokens and to query tokens, enabling us to assess whether improvements coincide with increased use of global-summary cues. Results As shown in Figure 5, MiA-Emb exhibits clear rise in silver-chunk recall beginning at the middle layers. This rise coincides with increased attention to the global summary in the same layer range, suggesting that the model progressively injects summary-derived cues into the query representation. This incorporation of global signals enriches the query embedding, enabling MiA-Emb to develop deeper semantic understanding of the query and thus support more selective retrieval. Figure 4: Comparison of projection angles for MiAEmb and Qwen3-Emb. Lower angles indicate better alignment of queries with the books semantic subspace. semantic space narrowed to the target books. 6.2 Geometric Properties of the MiA Embedding Space Extending the analysis in Sec. 6.1, we further examine the following hypothesis: (H1) MiA-Emb facilitates Selective Retrieval. That is, whether the embedding model biases query representations toward the active book topic, thereby better positioning them within the subspace supported by the corresponding chunks. Method We visualize query and chunk embeddings with t-SNE (Maaten and Hinton, 2008). To characterize the semantic structure of the document, We first fit t-SNE on the chunk embeddings only, yielding 2D manifold that reflects the documents semantic structure. We then embed the query representations into the same 2D space and inspect how well each model positions queries relative to the corresponding topic-relevant chunk regions. Results Figure 4 shows clear geometric distinction between MiA-Emb and the vanilla embedding model. Across books, MiA-Emb consistently yields smaller projection angles, meaning that query embeddings lie closer to the semantic subspaces spanned by their corresponding documents. On average, MiA-Emb-8B achieves 37.1, compared with 43.5 for Qwen-Emb-8B, demonstrating that mindscape conditioning more effectively guides queries toward the correct semantic region and enables more precise selective retrieval. 6.3 Residual Stream and Attention Analysis of the MiA Embedding Model In this and the following subsections, we examine the following hypothesis: (H2.2) MiA-Emb attends to information that enriches the query at the layers identified in (H2.1). (H2) MiA-Emb facilitates Enriched Understanding of queries. Method To understand how summary information enriches query representations, we inspect 7 ence in attention by measuring whether chunks that are more consistent with the summary receive stronger query attention, and whether this effect is stronger for relevant chunks than for irrelevant ones. Higher MCEA indicates stronger mindscapedriven Integrative Reasoning. formal definition is given in Appx. E. Method We compute MCEA for MiA-Gen and the vanilla Qwen2.5-14B-Instruct at each transformer layer. To verify that the observed alignment is driven by global semantics rather than positional or length biases, we also introduce summaryreplaced control, where the original summary is replaced with an unrelated text of the same length. Results Figure 7 highlights two key findings. First, MiA-Gen exhibits consistently higher MCEA values than the vanilla model, with the gap expanding notably in the middle and late layers. This pattern reflects structured reasoning process: local chunks first internalize global mindscape semantics, after which the query increasingly attends to these enriched chunks. Second, replacing the valid summary with irrelevant text causes sharp drop in MiA-Gens MCEA, whereas the vanilla model exhibits negligible sensitivity to this perturbation. This contrast confirms that MiA-Gens alignment is driven by genuine mindscape semantics rather than positional or length-based cues. Collectively, these results demonstrate that MiAGen performs Integrative Reasoning: the model leverages the global mindscape to structure local evidence interpretation and subsequently guides query attention toward globally coherent chunks."
        },
        {
            "title": "7 Conclusion",
            "content": "Inspired by the human cognitive ability to interpret new inputs within global mindscape, we propose MiA-RAG, the first framework that equips LLM-based RAG systems with mindscape-aware capabilities. We approximate this global impression via hierarchically generated summary that serves as persistent global memory. By conditioning both the retrieval and generation stages on this summary, MiA-RAG achieves superior performance in evidence-based long-context understanding as well as global sense-making tasks. Empirical analysis further shows that the summary projects queries into global semantic space, enabling enriched understanding, selective retrieval, and integrative reasoning over dispersed evidence. Figure 6: Attention pattern of MiA-Emb: the last token attends to preceding summary tokens, with red regions indicating tokens that receive high attention. Figure 7: Layer-wise Mindscape-Coherent Evidence Alignment (MCEA) scores for generator. the summary-attentive layers identified in Section (H2.1). Our goal is to assess whether the embedding token allocates its attention to summary tokens that are semantically aligned with the query. If such attention emerges precisely at layers where retrieval performance improves, it suggests that MiA-Emb enhances query understanding through targeted integration of global context. Results Figure 6 shows that, at the layers corresponding to retrieval gains, the final embedding token concentrates its attention on summary phrases that correspond to answer-relevant entities, events, or locations. This indicates that MiA-Emb selectively extracts semantically aligned global cues and integrates them into the query representation, reinforcing the layer-wise analysis and confirming that MiA-Emb enhances query understanding via summary-based enrichment. 6.4 Attention Pattern Analysis in the Generation Model (H3) MiA-Gen facilitates Integrative Reasoning over retrieved chunks within the global mindscape. To examine how the mindscape steers generation toward relevant evidence, we introduce the Mindscape-Coherent Evidence Alignment (MCEA) metric. It quantifies globallocal coher-"
        },
        {
            "title": "Limitations",
            "content": "While MiA-RAG demonstrates strong performance on narrative long-context QA and reasoning, the framework relies on precomputed global summary as the mindscape representation. This requirement may limit applicability in scenarios where the underlying content evolves over time or where summaries are difficult to obtain. Moreover, our experiments focus primarily on narrative-style datasets, and the generality of the approach for other longcontext settings (e.g., long-form dialogue) remains to be validated. Finally, part of the supervision signal is derived from commercial LLMs, which may introduce latent biases or hallucinated content. Nonetheless, the empirical gains suggest that the mindscape-aware training strategy remains robust even under imperfect supervision."
        },
        {
            "title": "References",
            "content": "Anthropic. 2024. Introducing contextual retrieval. Sam Audrain and Mary Pat McAndrews. 2022. Schemas provide scaffold for neocortical integration of new memories over time. Nature communications, 13(1):5795. Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, and 1 others. 2023. Longbench: bilingual, multitask benchmark arXiv preprint for long context understanding. arXiv:2308.14508. Frederic Charles Bartlett. 1932. Remembering: study in experimental and social psychology. Cambridge university press. Ali Behrouz, Peilin Zhong, and Vahab Mirrokni. 2024. Titans: Learning to memorize at test time. arXiv preprint arXiv:2501.00663. Jeffrey Binder, Rutvik Desai, William Graves, and Lisa Conant. 2009. Where is the semantic system? critical review and meta-analysis of 120 functional neuroimaging studies. Cerebral cortex, 19(12):27672796. Garvin Brod, Ulman Lindenberger, and Yee Lee Shing. 2017. Neural activation patterns during retrieval of schema-related memories: Differences and commonalities between children and adults. Developmental science, 20(6):e12475. Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva Mody, Steven Truitt, Dasha Metropolitansky, Robert Osazuwa Ness, and Jonathan Larson. 2024. From local to global: graph rag approach to query-focused summarization. arXiv preprint arXiv:2404.16130. Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yixin Dai, Jiawei Sun, Haofen Wang, and Haofen Wang. 2023. Retrieval-augmented generation for large language models: survey. arXiv preprint arXiv:2312.10997, 2(1). Samuel Gershman, Anna Schapiro, Almut Hupbach, and Kenneth Norman. 2013. Neural context reinstatement predicts memory misattribution. Journal of Neuroscience, 33(20):85908595. Asaf Gilboa and Hannah Marlatte. 2017. Neurobiology of schemas and schema-mediated memory. Trends in cognitive sciences, 21(8):618631. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, and 1 others. 2022. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3. Che Jiang, Biqing Qi, Xiangyu Hong, Dayuan Fu, Yang Cheng, Fandong Meng, Mo Yu, Bowen Zhou, and Jie Zhou. 2024. On large language models hallucination with regard to known facts. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 10411053. Marzena Karpinska, Katherine Thai, Kyle Lo, Tanya Goyal, and Mohit Iyyer. 2024. One thousand and one pairs: \"novel\" challenge for long-context language models. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, EMNLP 2024, Miami, FL, USA, November 12-16, 2024. Tomáš Koˇcisk`y, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, Gábor Melis, and Edward Grefenstette. 2018. The narrativeqa reading comprehension challenge. Transactions of the Association for Computational Linguistics, 6:317328. James Kragel, Youssef Ezzyat, Bradley Lega, Michael Sperling, Gregory Worrell, Robert Gross, Barbara Jobst, Sameer Sheth, Kareem Zaghloul, Joel Stein, and 1 others. 2021. Distinct cortical systems reinstate the content and context of episodic memories. Nature Communications, 12(1):4444. Jianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng Liu. 2024. Bge m3-embedding: Multi-lingual, multi-functionality, multi-granularity text embeddings through self-knowledge distillation. Preprint, arXiv:2402.03216. Chankyu Lee, Rajarshi Roy, Mengyao Xu, Jonathan Raiman, Mohammad Shoeybi, Bryan Catanzaro, and Wei Ping. 2024. Nv-embed: Improved techniques for training llms as generalist embedding models. arXiv preprint arXiv:2405.17428. 9 Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, and 1 others. 2020. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in neural information processing systems, 33:9459 9474. Yuhong Li, Yingbing Huang, Bowen Yang, Bharat Venkitesh, Acyr Locatelli, Hanchen Ye, Tianle Cai, Patrick Lewis, and Deming Chen. 2024. Snapkv: Llm knows what you are looking for before generation. Advances in Neural Information Processing Systems, 37:2294722970. Zehan Li, Xin Zhang, Yanzhao Zhang, Dingkun Long, Pengjun Xie, and Meishan Zhang. 2023. Towards general text embeddings with multi-stage contrastive learning. arXiv preprint arXiv:2308.03281. Laurens van der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-sne. Journal of machine learning research, 9(Nov):25792605. Raiza Martin and Steven Johnson. 2023. Introducing notebooklm. Jing Miao, Charat Thongprayoon, Supawadee Suppadungsuk, Oscar Garcia Valencia, and Wisit Cheungpasitporn. 2024. Integrating retrieval-augmented generation with large language models in nephrology: advancing practical applications. Medicina, 60(3):445. Zach Nussbaum, John Morris, Brandon Duderstadt, and Andriy Mulyar. 2024. Nomic embed: Training reproducible long context text embedder. arXiv preprint arXiv:2402.01613. Chau Minh Pham, Yapei Chang, and Mohit Iyyer. 2025. Clipper: Compression enables long-context synthetic data generation. arXiv preprint arXiv:2502.14854. Hongjin Qian, Zheng Liu, Peitian Zhang, Kelong Mao, Defu Lian, Zhicheng Dou, and Tiejun Huang. 2025. Memorag: Boosting long context processing with global memory-enhanced retrieval augmentation. In Proceedings of the ACM on Web Conference 2025, pages 23662377. Matthew Lambon Ralph, Elizabeth Jefferies, Karalyn Patterson, and Timothy Rogers. 2017. The neural and computational bases of semantic cognition. Nature reviews neuroscience, 18(1):4255. Valerie Reyna and Charles Brainerd. 1995. Fuzzytrace theory: An interim synthesis. Learning and individual Differences, 7(1):175. Saba Sturua, Isabelle Mohr, Mohammad Kalim Akram, Michael Günther, Bo Wang, Markus Krimmel, Feng Wang, Georgios Mastrapas, Andreas Koukounas, Andreas Koukounas, Nan Wang, and Han Xiao. 2024. jina-embeddings-v3: Multilingual embeddings with task lora. Preprint, arXiv:2409.10173. Qwen Team. 2024. Qwen2.5: party of foundation models. Endel Tulving and Donald Thomson. 1973. Encoding specificity and retrieval processes in episodic memory. Psychological review, 80(5):352. Aäron van den Oord, Yazhe Li, and O. Vinyals. 2018. Representation learning with contrastive predictive coding. ArXiv, abs/1807.03748. Voyage-AI. 2025. Introducing voyage-context-3: focused chunk-level details with global document context. Blog post. Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, and Furu Wei. 2024a. Improving text embeddings with large language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1189711916. Zora Zhiruo Wang, Akari Asai, Xinyan Velocity Yu, Frank Xu, Yiqing Xie, Graham Neubig, and Daniel Fried. 2024b. Coderag-bench: Can retrieval augment code generation? arXiv preprint arXiv:2406.14497. Junjie Wu, Jiangnan Li, Yuqing Li, Lemao Liu, Liyan Xu, Jiwei Li, Dit-Yan Yeung, Jie Zhou, and Mo Yu. 2025. Sitemb-v1. 5: Improved context-aware dense retrieval for semantic association and long story comprehension. arXiv preprint arXiv:2508.01959. Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. 2023. Efficient streaming language models with attention sinks. arXiv preprint arXiv:2309.17453. Liyan Xu, Jiangnan Li, Mo Yu, and Jie Zhou. 2024. Fine-grained modeling of narrative context: coherence perspective via retrospective questions. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 58225838. Yangwen Xu, Qixiang Lin, Zaizhu Han, Yong He, and Yanchao Bi. 2016. Intrinsic functional network architecture of human semantic processing: Modules and hubs. Neuroimage, 132:542555. Zhe Xu, Jiasheng Ye, Xiaoran Liu, Xiangyang Liu, Tianxiang Sun, Zhigeng Liu, Qipeng Guo, Linlin Li, Qun Liu, Xuanjing Huang, and Xipeng Qiu. 2025. DetectiveQA: Evaluating long-context reasoning on In Workshop on Reasoning and detective novels. Planning for Large Language Models. Dongjie Yang, XiaoDong Han, Yan Gao, Yao Hu, Shilin Zhang, and Hai Zhao. 2024. Pyramidinfer: Pyramid kv cache compression for high-throughput llm inference. arXiv preprint arXiv:2405.12532. Howard Yen, Tianyu Gao, Minmin Hou, Ke Ding, Daniel Fleischer, Peter Izsak, Moshe Wasserblat, and Danqi Chen. 2024. Helmet: How to evaluate longcontext language models effectively and thoroughly. arXiv preprint arXiv:2410.02694. 10 Qinggang Zhang, Shengyuan Chen, Yuanchen Bei, Zheng Yuan, Huachi Zhou, Zijin Hong, Hao Chen, Yilin Xiao, Chuang Zhou, Junnan Dong, and 1 others. 2025a. survey of graph retrieval-augmented generation for customized large language models. arXiv preprint arXiv:2501.13958. Xinrong Zhang, Yingfa Chen, Shengding Hu, Zihang Xu, Junhao Chen, Moo Khai Hao, Xu Han, Zhen Leng Thai, Shuo Wang, Zhiyuan Liu, and Maosong Sun. 2024. bench: Extending long context evaluation beyond 100k tokens. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024. Yanzhao Zhang, Mingxin Li, Dingkun Long, Xin Zhang, Huan Lin, Baosong Yang, Pengjun Xie, An Yang, Dayiheng Liu, Junyang Lin, and 1 others. 2025b. Qwen3 embedding: Advancing text embedding and reranking through foundation models. arXiv preprint arXiv:2506.05176. Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher Ré, Clark Barrett, and 1 others. 2023. H2o: Heavy-hitter oracle for efficient generative inference of large language models. Advances in Neural Information Processing Systems, 36:3466134710. Zulun Zhu, Tiancheng Huang, Kai Wang, Junda Ye, Xinghe Chen, and Siqiang Luo. 2025. Graph-based approaches and functionalities in retrieval-augmented generation: comprehensive survey. Supports of Mindscape-Aware"
        },
        {
            "title": "Capabilities in Broader Research\nFields",
            "content": "We show that the existence and our discussed advantages of the mindscape-aware capability are supported by various research on human memory in psychological and neuroscience research. Supports in Psychology First, the existence of mindscape-aware capability traces back to the concept of schema (Bartlett, 1932) and aligns closely with the principles of Fuzzy-Trace Theory (FTT; Reyna and Brainerd 1995). Specifically, the schema serves as an integrated structure of given topic. When activated, it guides attention and limits interpretive possibilities during information processing. The FTT theory posits that human memory encodes experiences at two complementary levels: verbatim traces that preserve surface details and gist traces that capture the abstract, meaning-based structure of knowledge. When individuals encounter new information related to familiar topic, it is typically the gist-level representation that is reactivated, providing global semantic scaffold that constrains interpretation, retrieval, and reasoning. Our mindscape-aware framework is computational implementation of gist-based cognition and an approximation of the abstract information in schema, in the context of complex reasoning and retrieval-augmented systems. Second, our discussed advantages of the mindscape-aware capability are grounded in the Encoding Specificity Principle (Tulving and Thomson, 1973), which lies in the core of psychological research on memory. The principle states that when familiar topic or task reappears, the reinstatement of its original contextual pattern reactivates the corresponding memory network, thereby enhancing retrieval effectiveness and interpretive coherence. Supports in Neuroscience The theory of MiA capabilities has solid empirical basis in neuroscience research. First and most importantly, the existence of mindscape-aware abilities is directly characterized by the controlled semantic cognition (CSC) framework (Ralph et al., 2017), which posits that conscious cognition is constrained and guided by globally integrated semantic knowledge, ensuring that thought unfolds within coherent knowledge framework. Second, the aforementioned psychological theories and the CSC framework have been verified by neuroimaging evidence, providing empirical support for mindscape-aware abilities. For example, Brod et al. (2017); Gilboa and Marlatte (2017); Audrain and McAndrews (2022) identify the neural foundations of schemas, which facilitate the integration of new knowledge and shape memory recall. Further empirical findings (Gershman et al., 2013; Kragel et al., 2021) demonstrate that context reinstatement during retrieval, i.e., the reactivation of semantic, situational, or cue-related features present during encoding enhances memory recall. Similarly, the CSC framework is supported by evidence of sustained co-activation during story comprehension and related semantic processes (Binder et al., 2009; Xu et al., 2016). In this sense, mindscape-awareness can be viewed as higher-order manifestation of encoding specificity and schema mechanisms, involving the reactivation of global semantic mindscape that encompasses an individuals accumulated knowledge and guides interpretation and retrieval. 11 Algorithm 1 Silver Evidence Annotation 1: Input: Dataset = {(qi, ai)}N S, retriever Es, task {c, n}. i=1, mindscape summary 2: Define: Evidence units = (if = c) or = (if emb = {(qi, Ui)}N i= Query Augmentation Candidate Retrieval & Ensemble = n). qaug {qi, qi + ai, ai}, Upool for in qaug do 3: Output: Silver-annotated dataset Dt 4: Initialize Dt emb 5: for each (qi, ai) in do 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: end for 18: return Dt Ui LLMt(qi, ai, Ucand) Add (qi, Ui) to Dt emb emb Upool Upool RetrieveTopK(Es, q, U, k) end for Ucand VoteAndSelectTopK(Upool, k) LLM-driven Refinement See Fig. 11/ MiA-Emb: Supervision and Training"
        },
        {
            "title": "Objective",
            "content": "B.1 Positive Evidence Construction As existing long-context benchmarks lack explicit queryevidence alignments, we automatically construct silver evidence for both chunkand nodelevel retrieval. Silver Chunk Annotation We annotate silver chunks using structured procedure that integrates query augmentation, ensemble retrieval, and LLMbased refinement (Algorithm 1). For chunk-level supervision, we set the task = in Algorithm 1, yielding the silver chunk dataset Dc emb = {(qi, Ci)}N i=1, where Ci denotes the set of supporting chunks for query qi. Silver Node Annotation To support retrieval at global semantic granularity, we construct knowledge graph = (V, E) by extracting entitylevel information, following procedure similar to GraphRAG (Edge et al., 2024). For each document, we employ GPT-4o to identify key entities and generate concise descriptions, yielding node set = {(ename : edesc)}. We then generate the node-level silver dataset Dn emb = {(qi, Vi)}N i=1 by setting the task = and evidence units = in Algorithm 1. Here, Vi represents the set of relevant nodes for query qi, serving as the ground truth for the node retrieval task. B.2 Negative Evidence Construction MiA-Emb is trained with contrastive objective that requires both positive and negative samples. Positive samples are taken from the silver evidence sets Ci and Vi described above, while negative samples are constructed from two complementary sources. We illustrate the construction for chunk retrieval; node retrieval follows the same design. Hard negatives. Hard negatives are semantically similar to the query but not included in the silver evidence. We select chunks from the candidate set Ccand (Algorithm 1) that are not part of the silver set Ci, and take up to 5 such chunks to form the hard-negative set Chard . These samples provide challenging contrasts that encourage the model to distinguish subtle semantic differences. Simple negatives. Simple negatives are clearly irrelevant to the query. We sample them uniformly at random from the full document chunk set C, ensuring no overlap with the positive set Ci or the hard negatives Chard . We sample 5 chunks to form the simple-negative set Csimple Final negative set. For chunk retrieval, the final negative pool for query qi is . C = Chard Csimple . (8) For node retrieval, we apply the same procedure . we use as unified notation for the negative set of to obtain the node-level negative set query qi. B.3 Model Training We provide additional training details for MiAEmb, complementing Sec. 3.3.2. Input Representation To enable the embedding model to perceive both the local query intent and the global mindscape, we construct composite input sequence. Let qi be the query and be the mindscape summary. The input is formatted as = [[INST]emb; qi; dq; S; dn; dc], (9) where [INST]emb is the instruction prefix, dq marks the end of the query, and dn, dc serve as special tokens representing nodeand chunk-retrieval tasks, respectively. The sequence is encoded by the embedding model to obtain token-level hidden states: = E(Q) = (h1, . . . , hQ), (10) where denotes the last-layer hidden states for all tokens in Q. Residual Integration To preserve the original query semantics while injecting global context, we employ residual connection strategy. We extract the hidden state at the query delimiter (hq, corresponding to token dq) and the hidden state at the task delimiter (ht, corresponding to dc or dn, depending on the active task t). The final enriched query representation qt is computed as qt = δ hq + (1 δ) ht, (11) where δ is hyperparameter controlling the balance between local query focus and global context awareness. detailed ablation on the role of this residual connection is provided in Appendix D.5. Joint Contrastive Optimization Finally, we optimize multi-task contrastive objective (van den Oord et al., 2018) over chunk and node retrieval: LMiA-Emb = β Lc + (1 β) Ln, (12) where Lc and Ln represent the losses for chunk and node retrieval, respectively, and β [0, 1] balances their contribution. Both tasks employ the InfoNCE loss. Specifically, the objective Lt (t {c, n}) is defined as: Lt = 1 B (cid:88) j=1 log exp (cid:0)sim(q , d+j exp (cid:0)sim(q )/τ (cid:1) , d)/τ (cid:1) , (cid:80) dCj (13) where is the batch size, τ is the temperature parameter, and sim(, ) denotes cosine similarity. The candidate set for the j-th query is constructed as: Cj = {d+j } , (14) where d+j the silver evidence set Uj, and sponding set of negative candidates. is the positive embedding sampled from is the corre-"
        },
        {
            "title": "C Implementation Details",
            "content": "We set the chunk size to 1200 with an overlap of 100 tokens for NarrativeQA, DetectiveQA, and Bench, and to 200 for NoCha, following the typical context length distributions of these datasets. We build our retriever MiA-Emb by applying LoRA (Hu et al., 2022) on top of Qwen3Embedding-8B, and build our generator MiAGen by fully fine-tuning Qwen2.5-14B-Instruct. Throughout the paper, we denote Qwen2.5-72B 13 Setting MiA-Emb MiA-Gen Precision Batch Size Steps warmup ratio Learning Rate LoRA Rank LoRA α Temperature τ Residual Weight δ Multi-task Weight β bfloat16 4 2000 0.1 1 104 128 256 0.01 0.5 0. bfloat16 2 2000 0.05 1 105 0 Table 7: Training configurations.- denotes not used. as the 4-bit quantized variant of Qwen2.5-72BInstruct to improve efficiency. GPT-4o refers to GPT-4o-2411. For silver evidence construction, we use Gte-Qwen-7B as the retriever Es in Algorithm 1. All experiments are conducted on 8H20 GPUs, each equipped with 96GB of memory. All hyperparameters are summarized in Table 7."
        },
        {
            "title": "D Additional Experiments",
            "content": "D.1 Results on Helmet To further examine the robustness of MiA-RAG, we evaluate our system on the NarrativeQA subset used in the Helmet benchmark. This setting is particularly challenging due to long contexts. Table 8 compares different combinations of retrievers and generators, with darker rows indicating stronger utilization of the global summary. We observe three main trends. First, replacing the vanilla retriever with MiA-Emb consistently improves both EM and F1, even when paired with off-the-shelf generators. Second, adding the Mindscape summary during inference benefits all RAG configurations, especially when retrieval quality is already high. Finally, the integration of MiA-Emb and MiA-Gen into the full MiA-RAG model delivers the strongest results, markedly surpassing all baselines while requiring substantially shorter effective context lengths. D.2 Study III: MiA-GraphRAG for Global QA Global Sense-Making QA Task Beyond local evidenceoriented evaluation, we assess global sense-making questions that require holistic understanding of the entire document. These questions are constructed from the LongBench (Bai et al., 2023) summary-generation datasets: QMSum and GOV (English), and VCSum (Chinese). Each question is derived from source documents Emb. Model Qwen3-Emb-8B MiA-Emb-8B MiA-Emb-8B MiA-Emb-8B MiA-Emb-8B MiA-Emb-8B Gen. Model Model +Summ EM Tokens Qwen2.5-14B GPT4o-2405 Qwen2.5-14B Qwen2.5-14B MiA-Gen-14B MiA-Gen-14B GPT4o-2408 GPT4o-2405 Gemini-1.5-Pro 17.7 21.9 18.2 20.4 28.9 29. 34.8 38.9 36.7 39.11 48.7 49.5 43.1 46.5 42.8 12k 12k 12k 13k 4k 13k 128k 128k 2M Table 8: Results on the NarrativeQA subset in the Helmet benchmark (Yen et al., 2024), evaluated under RAG (k=3 or 10) and full context settings. denotes results copied from Helmet. exceeding 100K tokens, ensuring that the model must integrate global information rather than rely on localized evidence. In total, we construct 300 such questions. Prompt is provided in Figure 13. Results We evaluate global sense-making in GraphRAG QA setting. Three node retrievers are compared for selecting semantic entities from the document-level knowledge graph: (1) our MiA-Emb, (2) multi-task embedding model trained without mindscape supervision (SFT-Emb), and (3) the vanilla Qwen3-Embedding-8B. Each retriever selects the top-20 nodes, after which their associated relations and supporting chunks are assembled into the global semantic context following the local mode of GraphRAG procedure (Edge et al., 2024). We conduct pairwise comparisons judged by GPT-4o along three dimensions: Comprehensiveness, Diversity, and Empowerment (Figure 14). As shown in Table 9, MiA-Emb achieves the best performance across all dimensions under the same graph construction pipeline. This indicates that mindscape-aware retrieval surfaces entities that more accurately capture the documents overall semantic structure. The data confirms that both MiA-Emb and MiAGen consistently outperform baselines across all model sizes. Figure 8 and Table 10 show the impact of retriever scaling on recall and long-context understanding performance. Additionally, Figure 9 presents the detailed results for MiA-Gen. D.4 Performance Across Various Embedding Models While our primary experiments utilize the Qwen3Embedding series, we further assess the universality of our approach by applying it to diverse embedding architectures. We benchmark against three distinct categories of baselines: (1) Opensource Bidirectional: GTE-Qwen2.5-7B (Li et al., 2023); (2) Commercial Late-interaction: VoyageContext-3 (Voyage-AI, 2025); (3) Context-Aware SOTA: SitEmb-8B (Wu et al., 2025), which encodes chunks with their local neighborhoods. We also include supervised baseline, SFT-Emb-8B, which is trained with our supervision signal but lacks the mindscape conditioning. Table 11 presents Answer Recall@K on the outof-domain DetectiveQA-ZH benchmark. All outof-the-box embedding models show noticeable performance gaps on this dataset, reflecting the difficulty of long-context reasoning in cross-domain settings. SitEmb benefits from local contextualization but still falls short of MiA-Emb. SFT-Emb narrows the gap relative to general-purpose embeddings yet does not match MiA-Emb either. MiAEmb achieves the strongest results across all configurations, demonstrating that integrating the global mindscape into query representations provides consistent and robust improvements across diverse embedding architectures. Dimension (A) MiA-Emb vs SFT-Emb (B) MiA-Emb vs Vanilla A1 A2 Win A1 A2 Win Model Out-of-box Comprehensiveness 87.74 12.26 68.39 31.61 Diversity 73.87 26.13 Empowerment 81.29 18.71 Overall Winner MiA-Emb MiA-Emb MiA-Emb MiA-Emb 88.39 11.61 MiA-Emb 63.23 36.77 MiA-Emb 71.94 28.06 MiA-Emb 78.39 21.61 MiA-Emb Qwen3-Embedding-8B GTE-Qwen2.5-7B voyage-context-3 Table 9: Pairwise comparison of MiA-based methods vs baselines across evaluation dimensions. Values are percentages. We use Qwen2.5-72B as the generator. Trained SitEmb-8B SFT-Emb-8B MiA-Emb-8B R@3 R@5 R@10 Avg. 28.6 21.0 36.1 42.5 37.9 46. 39.1 30.4 46.8 54.5 48.8 59.2 55.6 38.3 63.3 69.3 66.5 72.5 40.1 29.9 48.7 55.4 50.1 59. D.3 Model Scale Analysis Complementing the discussion in Sec. 5.4, we provide detailed results for our scaling experiments. Table 11: Retrieval performance of different embedding models on DetectiveQA-ZH. S. indicates whether the global summary is appended to the query. denotes results copy from SitEmb (Wu et al., 2025) 14 Figure 8: Impact of retriever scale on retrieval performance (Recall@K) on DetectiveQA and NarrativeQA. DetectiveQA scores are averaged over its ZH and EN subsets. SFT-Emb denotes the baseline trained with the identical supervision as MiA-Emb but without access to mindscape summaries. Inputs Emb. Base Summary-Only - NarrativeQA Bench Det.QA-Zh Det.QA-En F1 39.24 Acc 72.05 Acc 73.67 Acc 61.33 Nocha Pair Acc 31. Vanilla MiA (Emb-Only) MiA Vanilla MiA (Emb-Only) MiA Vanilla MiA (Emb-Only) MiA Qwen3-0.6B MiA-Emb-0.6B MiA-Emb-0.6B 37.98/44.11/47.56 45.13/47.74/49.61 47.92/51.99/52.24 72.05/79.48/82.53 78.23/83.00/87.40 79.04/80.35/86. 64.33/71.00/78.50 72.83/80.50/81.50 77.67/79.50/81.33 54.67/59.83/67.67 64.67/70.5/72.12 69.50/71.67/74.67 31.75/31.75/42.86 31.75/33.33/49.21 42.86/42.86/50.79 Qwen3-4B MiA-Emb-4B MiA-Emb-4B Qwen3-8B MiA-Emb-8B MiA-Emb-8B 36.90/42.02/46.97 45.08/47.61/49.91 49.51/50.22/52. 75.11/77.73/82.97 85.59/87.77/88.65 85.15/86.46/87.77 64.33/71.00/79.33 76.00/80.50/83.17 79.17/81.67/83.33 54.67/59.00/68.17 67.33/71.17/75.83 71.5/72.67/77.17 31.75/38.1/41.27 34.92/46.03/50.79 42.86/49.21/49.21 41.13/45.51/49.06 46.38/48.06/49.88 50.05/51.04/53.15 75.55/80.79/86.90 84.72/87.77/90.39 84.71/86.46/88. 63.67/70.83/78.00 76.17/81.17/82.67 81.67/83.17/84.17 55.50/61.33/71.17 67.17/71.83/75.33 70.33/72.33/75.50 33.33/38.10/41.27 42.86/42.86/49.21 41.27/44.44/52.38 Table 10: Results of MiA framework on Long-story QA tasks across different embedding model scales (0.6B, 4B, 8B), all evaluated with Qwen2.5-72B generator on top-3/5/10 retrieved chunks. Darker gray color refers to deeper involvement of mindscape information. D.5 On the Role of Residual Connection While our trained MiA-Emb learns to adaptively balance query semantics and summary information, the residual connection proves essential for vanilla embedding models without specialized training. Table 12 shows that for Qwen3-Embedding-8B, directly appending summaries severely harms retrieval performance, suggesting that the model cannot separate the semantic focus of the query from the global summary, treating the concatenated sequence as homogeneous input. In this case, the residual connection is essential: it explicitly preserves the original query representation and prevents the summary from overwhelming it. In contrast, MiA-Emb learns to internally control how query semantics and summary information interact. Therefore, the residual becomes lightweight structural aid rather than the key mechanism. Whether the residual is present or removed, MiA-Emb maintains stable performance, indicating that the model has learned more fine-grained fusion strategy beyond the explicit residual pathway. Method NarrativeQA DetectiveQA-ZH DetectiveQA-EN 3 5 10 3 5 3 5 10 Vanilla + Summary + Residual MiA-Emb - Residual 41.81 26.24 41.58 62.68 63.13 54.51 36.26 54.65 75.92 76. 71.13 53.54 71.29 88.09 87.47 28.58 25.83 33.92 46.75 47.00 39.08 36.50 43.50 59.17 58.75 55.58 49.25 59.58 72.50 73.83 24.17 22.58 30.50 42.08 40.33 34.17 29.42 37.58 54.17 54. 49.25 42.42 54.42 69.75 69.83 Avg 44.70 35.56 47.00 63.46 63.50 Table 12: Effect of summary concatenation and residual connection."
        },
        {
            "title": "E Definition of MCEA Metric",
            "content": "We introduce the Mindscape-Coherent Evidence Alignment (MCEA) metric to investigate how the mindscape guides attention toward local evidence 15 Figure 9: Scaling results for MiA-Gen versus the vanilla Qwen2.5-Instruct baseline. during generation. The definition is as follows. Prompt Templates for MiA-RAG Definition At layer l, given an input xgen = (S, ˆCret,i, Qi), we compute for each chunk ci ˆCret,i the aggregated chunk-to-summary attention: M(l)(ci) = 1 (cid:88) sS (cid:32) 1 ci (cid:88) tci (cid:33) A(l)[t, s] , (15) and the aggregated query-to-chunk attention: (l)(ci) = 1 Qi (cid:88) qQi (cid:32) 1 ci (cid:88) tci (cid:33) A(l)[q, t] , (16) where A(l) denotes the attention weights at layer l. We then define the alignment score by computing the product of z-score normalized values: C(l)(ci) = M(l)(ci) µ(l) σ(l) (l)(ci) µ(l) σ(l) , (17) where µ and σ denote the mean and standard deviation of each quantity over all chunks at layer l. This section provides the complete set of prompt templates used in the MiA-RAG framework. We include prompts for: (1) Hierarchical summarization, used to iteratively condense raw text into structured global mindscape (Figure 10); (2) Supervision data construction for the retriever, including silver chunk filtering (Figure 11) and silver node selection (Figure 12); (3) Sense-making tasks, including (a) the prompt for generating sense-making questions (Figure 13), and (b) the prompt for pairwise answer evaluation in sense-making model assessment (Figure 14); Finally, let and denote relevant (silver) chunks and noise chunks, respectively. The layerwise MCEA score is defined as the difference between their mean alignment: (4) Retrieval prompting, where the mindscape and query are combined into unified retrieval input (Figure 15); MCEA(l) = 1 (cid:124) (cid:88) C(l)(ci) (cid:88) C(l)(cj) . 1 ciR (cid:123)(cid:122) µ(l) relevant (cid:125) (cid:124) (cid:125) cj (cid:123)(cid:122) µ(l) noise (18) Higher MCEA indicates that the generator absorbs global semantics into chunk representations and preferentially attends to mindscape-coherent evidence for Integrative Reasoning. 16 (5) Generator Instructions: Prompts for response generation across three settings: mindscape-augmented QA (Figure 16), standard QA baselines without summaries (Figures 18-21), and global sense-making QA (Figure 17). Prompts for Hierarchical Summary Generation Step 1: Chunk-Level Summary ([INST]sum_c) \"There is chunk from fiction or movie script. Your task is to summarize this chunk into refined and readable summary. The chunk is:n<chunk>n{chunk_content}n</chunk>nnPlease summarize it following the requirements below:nThe chunk is created by splitting larger work, so it is local part and may contain prefaces, epilogues, or content unrelated to the main story. You should identify and exclude these from the summary.nThe summary must be coherent.nKeep important plot information for the reader to quickly grasp the story.nThe summary length should be under 500 characters.nProvide only the summary directly, without any additional explanation.\" Step 2: Global Summary ([INST]sum_g) \"There is concatenated text of summaries from fictions chunks. The full text may be too long to read. Your task is to summarize this text into single, refined, and readable summary. Here is the text:n<text>n{concatenated_summaries}n</text>nnPlease summarize the text following these requirements:nThe summary must be coherent and read like complete story abstract.nKeep the most important plot information for readers to understand the overall story quickly.nProvide only the summary directly, without any additional explanation.\" Figure 10: Prompt templates used in our two-step hierarchical summarization process. Prompt for Filtering Silver Chunks Prompt for Filtering Silver Nodes You are an expert at analyzing narrative texts and identifying the key entities needed to answer questions about stories, novels, and literary works. Given question, its answer, and list of entities with their descriptions extracted from narrative, determine which entities are most relevant for answering the question. Input Question: {Question} Answer: {Answer} Entities (indexed from 0): {entities with their description} Instructions 1. Analyze each entitys name, type, and description. 2. Select entities that: directly support the answer, appear in or relate closely to the question/answer, provide essential background or relational context. 3. Include contextual entities even if not explicitly mentioned. 4. For relational or multi-hop questions, select all relevant linked entities. Output Requirement Return only JSON array of relevant entity indices (e.g., [0,2,5]). If none are relevant, return [-1]. No explanations or additional text. Figure 12: Prompt used to filter silver nodes. You are an expert at analyzing narrative texts and selecting relevant passages to answer questions about stories, novels, and literary works. Given question, its answer, and list of text chunks from narrative, identify which chunks are most relevant for answering the question. Input Question: {Question} Answer: {Answer} Text Chunks (indexed from 0): {Retrieved Chunks} Instructions 1. Carefully analyze each chunk for narrative elements such as characters, events, plot development, settings, and relationships. 2. Select chunks that: directly contain information needed to answer the question, provide essential background context or character development, describe events or situations relevant to the answer, include dialogue, actions, or descriptions that inform the question. 3. Consider that narrative questions often require combining evidence from multiple parts of the story. 4. Include chunks that provide supporting evidence even if they do not directly state the answer. 5. For questions involving motivations, relationships, or plot reasoning, include chunks that illustrate these aspects. Output Requirement Return only JSON array of relevant chunk indices (e.g., [0,2,5]). If none are relevant, return [-1]. No explanations or additional text. Figure 11: Prompt used to filter silver chunks. 17 Prompt for Sense-making Question Generation The query format of [INST]_emb You are an expert research analyst and strategist. Your task is to generate deeply insightful questions from text segment. These questions will form global question bank for large document, so they must be self-contained and provoke critical thinking. TEXT SEGMENT BEGINS {paragraph} TEXT SEGMENT ENDS 1. Dont Merely Locate: Integrate multiple pieces of information rather than extract single facts. 2. Probe Deep Reasoning: Focus on causes, tradeoffs, critique, and implicationsthe so what?. 3. Focused Inquiry: Each question must be concise. 4. Self-Contained Questions: Avoid vague references (this method); specify concrete names. 5. Professional & Diverse: Reflect expert-level reasoning from multiple analytical angles. Output Format { \"questions\": [ \"Question 1\",... \"Question 5\" ] } If fewer than 3 valid questions can be generated, return an empty list. Figure 13: Prompt for sensemaking question generation. Prompt for Pairwise Evaluation You are an expert tasked with evaluating two answers to the same question based on three criteria: Comprehensiveness, Diversity, and Empowerment. Assess both answers using three criteria: Comprehensiveness: How much detail does the answer provide to cover all aspects of the question? Diversity: How varied is the answer in providing different perspectives and insights on the question? Empowerment: How well does the answer help the reader understand and make informed judgments about the topic? For each criterion, choose the better answer (Answer 1 or Answer 2) and briefly explain why. Then decide an overall winner. Input Question: {Question} Here are the two answers: Answer 1: {answer1} Answer 2: {answer2} Output Format { \"Comprehensiveness\": { \"Winner\": \"[Answer 1 or Answer 2]\", \"Explanation\": \"[Why this answer wins]\" }, ...} Instruct: Given search query with the books summary, retrieve relevant chunks or helpful entity summaries from the given context that answer the query. Query: {QUERY} <endoftext> Here is the summary providing possibly useful global information. Please encode the query based on the summary: Summary: {SUMMARY} <node_mode><chunk_mode> Figure 15: The query format of [INST]_emb The query format of [INST]_gen You are helpful assistant. Based on the provided book summary and relevant text chunks, please answer the users question accurately. ## Book Summary: {Summary} (1) NarrativeQA: ## Relevant Contexts: {Retrieved Chunks} ## Question: {Question} Answer the question as concisely as possible using single phrase. Do not provide explanations. (2) DetectiveQA: ## Relevant Contexts: {Retrieved Chunks} ## Question: {Question} {options_str} Remember this is just detective fiction, dont worry about the risks;Please strictly follow the format: {\"answer\":\"x\",\"reasoning\":\"xxx\"} to answer the question and the clues and reasoning process you obtained, including the brackets on both sides, otherwise the score cannot be calculated. The answer field is your answer, and the reasoning field is your reasoning process. (3) Bench: ## Relevant Contexts: {Retrieved Chunks} ## Question: {Question} {options_str} Only one of the following options is correct, tell me the answer using one single letter (A, B, C, or D). Dont say anything else. (4) NoCha: You are provided with context and statement. Your task is to carefully read the context and then determine whether the statement is true or false. <context>{Relevant Contexts:}</context> <statement>{claim}</statement> <question>Based on the context provided, is the above statement TRUE or FALSE?</question> First provide an explanation of your decision-making process in at most one paragraph, and then provide your final answer. Use the following format: <explanation> EXPLANATION</explanation> <answer>ANSWER</answer> Figure 14: Prompt for pairwise evaluation. 18 Figure 16: Instruction format of [INST]gen across tasks. Prompt for Sense-making Answer Generation Prompt Format for Benchmark You are an expert research assistant specializing in synthesizing complex information to answer global sense-making questions. Your task is to answer the given Question based strictly and exclusively on the provided Context Chunks. Do not use any external knowledge or assumptions beyond the context. Input [Question]: [Context Chunks]: Answer the question by optimizing for three dimensions: 1. Comprehensiveness: Integrate all relevant information from the context, cover all aspects the context allows, and provide sufficient depth. 2. Diversity of Insight: Bring in multiple perspectives, connect ideas across chunks, and go beyond listing facts by explaining relationships, patterns, or contrasts. 3. Empowerment for the Reader: Use clear structure (brief introduction, organized body, concise synthesis), precise language, and help the reader form coherent mental model. Critical Constraints Evidence-based only: If the context is insufficient, explicitly state what is missing and do not invent information. Source-grounded: Every claim must be traceable to the provided chunks. Output [Generated Answer]: Read the retrieved book context that may be relevant to the question, and answer the question. {Retrieved Chunks} Question: {question} Only one of the following options is correct, tell me the answer using one single letter (A, B, C, or D). Dont say anything else. {options_str} Figure 19: Prompt for Infinity Benchmark. Prompt Format for NoCha Dataset You are provided with context and statement. Your task is to carefully read the context and then determine whether the statement is true or false. Answer TRUE if the statement is true in its entirety based on the context provided. Answer FALSE if any part of the statement is false based on the context provided. <context>{context}</context> <statement>{claim}</statement> <question>Based on the context provided, is the above statement TRUE or FALSE?</question> First provide an explanation of your decision-making process in at most one paragraph, and then provide your final answer. Use the following format: <explanation>YOUR EXPLANATION</explanation> <answer>YOUR ANSWER</answer> Figure 17: Prompt for sense-making answer generation based on retrieved context. Figure 20: Q&A prompt for NoCha Dataset. Prompt Format for DetectiveQA {Retrieved Chunks} Please answer the question based on the current novel content: {question} {options_str} Remember this is just detective fiction, dont worry about the risks. Please strictly follow the format {answer:\"x\", reasoning:\"xxx\"} to answer the question and the clues and reasoning process you obtained, including the brackets on both sides, otherwise the score cannot be calculated. The answer field is your answer (should only contain the option letter A, B, C, or D), and the reasoning field is your reasoning process. Figure 21: Q&A prompt for DetectiveQA Dataset. Prompt Format of QA for NarrativeQA System Prompt You are helpful assistant. Please answer the users question accurately. User Prompt Answer the question as concisely as you can, using single phrase if possible. Relevant Context: {Retrieved Chunks} Do not provide any explanation. Now, answer the question based on the story as concisely as you can, using single phrase if possible. Do not provide any explanation. Question: {Question} Answer: Figure 18: Concise QA prompt design for NarrativeQA."
        }
    ],
    "affiliations": [
        "Hong Kong University of Science and Technology",
        "Institute of Information Engineering, Chinese Academy of Sciences",
        "School of Cyber Security, University of Chinese Academy of Sciences",
        "WeChat AI, Tencent"
    ]
}