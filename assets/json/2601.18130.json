{
    "paper_title": "RouteMoA: Dynamic Routing without Pre-Inference Boosts Efficient Mixture-of-Agents",
    "authors": [
        "Jize Wang",
        "Han Wu",
        "Zhiyuan You",
        "Yiming Song",
        "Yijun Wang",
        "Zifei Shan",
        "Yining Li",
        "Songyang Zhang",
        "Xinyi Le",
        "Cailian Chen",
        "Xinping Guan",
        "Dacheng Tao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Mixture-of-Agents (MoA) improves LLM performance through layered collaboration, but its dense topology raises costs and latency. Existing methods employ LLM judges to filter responses, yet still require all models to perform inference before judging, failing to cut costs effectively. They also lack model selection criteria and struggle with large model pools, where full inference is costly and can exceed context limits. To address this, we propose RouteMoA, an efficient mixture-of-agents framework with dynamic routing. It employs a lightweight scorer to perform initial screening by predicting coarse-grained performance from the query, narrowing candidates to a high-potential subset without inference. A mixture of judges then refines these scores through lightweight self- and cross-assessment based on existing model outputs, providing posterior correction without additional inference. Finally, a model ranking mechanism selects models by balancing performance, cost, and latency. RouteMoA outperforms MoA across varying tasks and model pool sizes, reducing cost by 89.8% and latency by 63.6% in the large-scale model pool."
        },
        {
            "title": "Start",
            "content": "RouteMoA: Dynamic Routing without Pre-Inference Boosts Efficient Mixture-of-Agents Jize Wang1 Han Wu1 Zhiyuan You2 Yiming Song1 Yijun Wang3 Zifei Shan3 Yining Li4 Songyang Zhang4 Xinyi Le1 Cailian Chen1* Xinping Guan1 Dacheng Tao5 1 Shanghai Jiao Tong University 2 CUHK 3 Tencent 4Shanghai AI Laboratory 5Nanyang Technological University {jizewang2000,lexinyi,cailianchen}@sjtu.edu.cn 6 2 0 2 6 2 ] . [ 1 0 3 1 8 1 . 1 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Mixture-of-Agents (MoA) improves LLM performance through layered collaboration, but its dense topology raises costs and latency. Existing methods employ LLM judges to filter responses, yet still require all models to perform inference before judging, failing to cut costs effectively. They also lack model selection criteria and struggle with large model pools, where full inference is costly and can exceed context limits. To address this, we propose RouteMoA, an efficient mixture-of-agents framework with It employs lightweight dynamic routing. scorer to perform initial screening by predicting coarse-grained performance from the query, narrowing candidates to high-potential subset without inference. mixture of judges then refines these scores through lightweight selfand cross-assessment based on existing model outputs, providing posterior correction without additional inference. Finally, model ranking mechanism selects models by balancing performance, cost, and latency. RouteMoA outperforms MoA across varying tasks and model pool sizes, reducing cost by 89.8% and latency by 63.6% in the large-scale model pool."
        },
        {
            "title": "Introduction",
            "content": "Large Language Models (LLMs) (Ouyang et al., 2022; Zhao et al., 2023) demonstrate strong capabilities across diverse tasks. While generalpurpose models (e.g., Llama-3.1 (Grattafiori et al., 2024), Qwen2.5 (Yang et al., 2024a)) show broad competence, specialized variants (e.g., Qwen2.5Math (Yang et al., 2024b), Qwen2.5-Coder (Hui et al., 2024)) excel in specific domains. This diversity in expertise makes the effective integration of multiple LLMs promising direction to achieve performance beyond individual models. Among various LLM-based multi-agent collaboration strategies, Mixture-of-Agents (MoA) (Wang *Corresponding Authors. 1 Figure 1: Significant variations in model capabilities. Values are normalized to [0,1]. Models exhibit clear specialization: Qwen2.5-Coder leads in coding but lags in biomedical tasks; Qwen2.5-Math excels in mathematics but struggles elsewhere; Bio-Medical-Llama dominates in biomedical knowledge but performs poorly in math and coding; Gemma stands out in reasoning and reading. These distinct profiles make it feasible to predict model performance only based on specific user queries. et al., 2024a) is typical and effective approach. As shown in Figure 2(a), this method enables multiple LLMs to refer to each others responses, engaging in iterative rounds (i.e., layers in Figure 2) of replies and summaries to achieve results superior to those of single model. Despite the advantages, MoA-based methods are highly resource-intensive. As shown in Figure 2(a), classical MoA (Wang et al., 2024a) requires forwarding multiple LLMs per layer and concatenating all outputs as the input to the next, leading to high cost and latency. Sparse MoA (Li et al., 2024) (Fig. 2(b)) introduces judge to filter responses, yet still invokes all LLMs plus an additional judge model, further increasing overhead. These approaches also lack principled model selection and do not scale to large pools (e.g., >10 models), as full inference becomes prohibitively costly and often exceeds context limits. To address the efficiency challenge, we propose RouteMoA, dynamically-routed mixture-ofagents framework. Our approach is motivated by the complementary capabilities of LLMs (Figure 1): for example, Qwen2.5-Math excels in mathematics but underperforms in reasoning and biomedical tasks. Such specialization makes it feasible to Figure 2: Concept comparison between our RouteMoA and previous MoA-based methods. (a) Classical MoA (Wang et al., 2024a) forwards all LLMs in each layer, and concatenates all outputs as the input of the next layer. (b) Sparse MoA (Li et al., 2024) introduces an LLM-based judge to select some good responses as the input of the next layer. This reduces the number of input tokens, but still needs to forward all LLMs and another LLM-based judge. (c) RouteMoA uses lightweight router to select parts of LLMs for inference, significantly reducing computational cost. predict model performance from the query, thus narrowing the initial pool to few high-potential candidates and reducing cost. Specifically, RouteMoA leverages lightweight scorer that performs initial screening. Using only prior knowledge from the query, it estimates model suitability without executing inference. It assigns coarse-grained scores to identify promising candidates, enabling activation of only subset of models and significantly lowering inference overhead. To correct potential scoring errors, we introduce mixture of judges to combine the scorer with selfand cross-assessment. These judges operate post-hoc, leveraging posterior knowledge from previously-generated responses without requiring additional inference. This design enhances assessment reliability at no extra cost, ensuring robust model selection throughout the routing process. Finally, model ranking mechanism selects models by balancing performance, cost, and latency. In summary, our contributions are as follows: We propose RouteMoA, dynamically-routed MoA framework that significantly cuts cost and latency while maintaining strong performance. We design lightweight scorer for initial model screening based on query-aware prior knowledge, narrowing the candidate pool to few high-potential models without pre-inference. We introduce mixture of judges that refines model scores through selfand cross-assessment, leveraging posterior knowledge from model outputs to correct prediction errors without introducing additional inference overhead. Extensive experiments on both smalland largescale model pools, along with out-of-distribution tasks, show RouteMoA matches or surpasses strong baselines in accuracy while greatly boosting efficiency and scalability."
        },
        {
            "title": "2 Related Work",
            "content": "General and task-specific LLMs. Large Language Models (LLMs) have shown strong performance in text understanding and generation (Achiam et al., 2023; Cai et al., 2024; Grattafiori et al., 2024; Yang et al., 2024a). They can be categorized into general-purpose modelssuch as Llama-3.1 (AI, 2025), Qwen2.5 (Yang et al., 2024a), Mistral (Jiang et al., 2023), and Gemma (Team et al., 2023)and domain-specific fine-tuned variants like Qwen2.5-Math (Yang et al., 2024b), Qwen2.5-Coder (Hui et al., 2024), and Bio-Medical-Llama (ContactDoctor, 2024). While specialized models excel in their domains, they For instance, often underperform elsewhere. Bio-Medical-Llama achieves 87.0 on the MMLU biomedical subset, outperforming Qwen2.5-Math by 43.5 points, yet scores only 11.7 on MATH. Since developing universally capable model is costly, integrating multiple models strengths offers more viable path. LLM-based multi-agent collaboration. Multiagent frameworks provide effective ways to leverage diverse model capabilities. Majority voting (Chen et al., 2024a) selects the most frequent answer from multiple models as the final output. LLM cascading (Yue et al., 2024) sequentially invokes models until response meets quality threshold. Multi-agent debate (Liang et al., 2024) enhances accuracy through iterative discussion. Mixture-of-Agents (MoA) (Wang et al., 2024a) refines answers via multi-round parallel reasoning but incurs high computational cost. Sparse 2 Figure 3: RouteMoA architecture. The framework operates layer-wise (left). At each layer l, the router selects subset of suitable LLMs, whose outputs are aggregated and passed to the next layer. The router (right) consists of two stages: b.1 Mixture of Judges, which includes scorer (trained as in a. Scorer Training), self-assessment, and cross-assessment. The scorer predicts candidate performance in layer-1 using prior knowledge from the query; subsequent layers refine scores via selfand cross-assessment using posterior knowledge from model outputs. b.2 Model Ranking selects LLMs by balancing performance, cost, and latency. MoA (Li et al., 2024) introduces judge to filter responses, saving input tokens but still requiring inference from all models. In contrast, our approach adopts lightweight router that selects suitable models dynamically for each layer without preinference, significantly cutting cost and latency. LLM routing. LLM routing (Ding et al., 2024; Stripelis et al., 2024) selects the best-performing model per query without invoking all candidates. Meta-models are trained to predict model performance based on input, improving cost efficiency. Benchmarks such as RouterBench (Hu et al., 2024) and RouterEval (Huang et al., 2025) assess routing effectiveness. ZOOTER (Lu et al., 2023) distills reward signals into an SLM router via KL-divergence, while RouterDC (Chen et al., 2024b) uses dual contrastive loss for better accuracy. Eagle adopts training-free approach using similarity-based retrieval. RouteLLM (Ong et al., 2024) focuses on binary routing between strong and weak models to minimize expensive calls. In contrast to routing methods that rely solely on query-based prior knowledge, our approach also leverages posterior knowledge from actual model outputs to update performance scores. This design relaxes the requirement for precise performance prediction, and the subsequent multi-agent collaboration further enhances robustness and overall performance beyond what is achievable by routing to single model."
        },
        {
            "title": "3 Methodology",
            "content": "In this section, we introduce RouteMoA, an efficient mixture-of-agents framework with dyIt dynamically selects subset namic routing. of top-performing LLMs each round without preinference, thus reducing cost and latency while maintaining performance. We first overview the whole routing process in Section 3.1, then describe its key components: the scorer, mixture of judges, and model ranking (Sections 3.23.4). 3.1 Overview The framework operates layer-wise, as shown in the left of Figure 3, following the Mixture-of-Agents It consists of structure (Wang et al., 2024a). layers. In intermediate layers (l = 1, 2, . . . , 1), nl LLMs act as proposers Ml,i P, where is pool of available models, = 1, 2, ..., nl . Each Ml,i processes input xl and generates response: yl,i = Ml,i(xl). (1) The output of layer is: ol = nl i=1yl,i + x1, xl+1 = ol, (2) Each response is scored using combination of ground-truth accuracy and reward model (e.g., InternLM2-1.8B-Reward): where + denotes concatenation and denotes an aggregation prompt (see Appendix A). x1 is the user query. The final layer uses single LLM to aggregate prior outputs into the final response. To balance performance and efficiency, RouteMoA dynamically selects models for each layer = 1, 2, ..., through the following process: Step 1: Score Acquisition. For the first layer (l = 1), an SLM-based scorer performs an initial screening by predicting coarse-grained performance scores for each model in on query x1: s1 = S(x1), s1 [0, 1]N . (3) This scorer is not required to provide precise performance score estimates; rather, its goal is to efficiently narrow down the candidate set to small group of high-potential models. For subsequent layers (l > 1), mixture of judges refines the initial scores by incorporating both s1 and responses from the previous layer, enabling more accurate and context-aware model selection: sl = (s1, yl1,1, . . . , yl1,nl1), > 1. (4) Step 2: Model Ranking and Selection. The model ranking module selects active models for layer based on performance sl, cost, and latency: [Ml,1, . . . , Ml,nl] = R(sl, cost, latency). (5) An early-stopping mechanism determines when to terminate, ensuring efficient inference. 3.2 SLM-based Scorer The scorer conducts an initial screening by predicting coarse-grained performance scores to each model in the pool = M1, M2, . . . , MN given the input xl, as defined in Equation 3. Dataset generation. We construct training dataset of the form: = {(x(k), s(k) 1 , s(k) 2 , ..., s(k) )}D k=1, (6) where s(k) denotes the performance score of model Mj on input x(k). To build D, we collect queries and ground-truth answers Draw = {(x(k), ˆy(k))}D k=1 from multiple datasets spanning mathematics, reasoning, coding, reading comprehension, biomedical domains, etc. For each query, we gather responses from all models in P: )}D k=1. Dy = {(x(k), y(k) 2 , ..., y(k) 1 , y(k) (7) = λ 1(ˆy(k) = y(k) s(k) ) + (1 λ) R(x(k), ˆy(k), y(k) ), (8) where λ (0, 1). 1() is the indicator function that returns 1 if the condition is true and 0 otherwise. Model structure. Inspired by matrix factorization techniques in recommendation systems (Chen et al., 2024b; Koren et al., 2009; Ong et al., 2024), we model the scorer as an embedding-based similarity function. Each model Mj is assigned learnable embedding kj Rd. The input is encoded by small language model mDeBERTaV3-base (He et al., 2021) into an embedding E(x). The performance score is computed as: = (x, Mj) = σ(E(x)kj), [0, 1], (9) where σ() is the sigmoid function. Thus the full score vector is: = S(x) = [f (x, M1), (x, M2), ..., (x, MN )]. (10) Training and inference. During training, we adopt dual contrastive loss functions from (Chen et al., 2024b). The sample-LLM contrastive loss ensures that the embeddings of models capable of answering query are closer to the querys embedding: Lsample-LLM(x, s; θ) (cid:88) = log j+I+ eE(x)kj+ eE(x)kj+ + (cid:80) jI , eE(x)kj (11) where + and denote the top-K+ and bottomK scoring models, respectively. θ denotes the parameters to be optimized. The sample-sample contrastive loss encourages semantically similar queries to have closer embeddings. It is formulated as: Lsample-sample(x; θ) = log eE(x)E(x+) eE(x)E(x+) + (cid:80) x eE(x)E(x) . (12) where x+ is query from the same cluster as x, and contains out-cluster queries. The clustering method is detailed in the Appendix C. 4 The total loss is: 3.4 Model Ranking = Lsample-LLM + αLsample-sample, (13) where α 0. 3.3 Mixture of Judges The design of the mixture of judges is motivated by two key capabilities of large language models: Self-knowledge awareness: Research has shown that LLMs possess the ability to evaluate their own knowledge and determine whether they understand question (Kadavath et al., 2022). Cross-model evaluation: LLMs can effectively judge responses from other models (Li et al., 2025), making them capable evaluators in multiagent settings. Thus, for layer > 1, we introduce mixture of judges to refine the scorers predictions using selfand cross-assessment signals from previous layers. For self-assessment, each active model in layer l1,j along with 1 outputs confidence score sself its response: l1 = [sself sself l1,1, sself l1,2, ..., sself l1,nl1 ]. (14) For cross-assessment, to avoid the computational cost of having all models generate evaluation scores, we selectively employ only the highestscoring model from layer l1 to evaluate responses from layer 2, producing scores scross l2,j: l2 = [scross scross l2,1, scross l2,2, ..., scross l2,nl2 ]. (15) Since cross-assessment relies on evaluating outputs from prior layer, it is only applicable from the second layer onward (l 2), as no prior outputs exist for the first layer. The final mixture of judges function is: sl = (s1, yl1,1, yl1,2, ..., yl1,nl1) (cid:40) = U(s1, sself U(s1, sself l1), l1, scross l2 ), = 2, > 2, (16) (17) where performs score normalization followed by element-wise averaging. The model ranking module selects the top-k models based on the adjusted scores sl, with the following priority: performance > output token cost > input token cost > latency. Model pricing and latency data are sourced from OpenRouter1. Early stopping criterion is set as: max(sl,1, sl,2, ..., sl,N ) > sth. (18) where sth is threshold score. If the criterion is met, or the max layer number is reached, the system will enter the aggregation stage and produce the final output: yf inal = Ml,agg(xl). (19)"
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Experimental Setup Baselines. We focus on improving the computational efficiency of multi-agent collaboration while maintaining the accuracy. The compared baselines include: (1) MoA (Wang et al., 2024a), leveraging multiple LLMs in layered architecture, where each agent uses outputs from previous layers to enhance its response generation; (2) SMoA (Li et al., 2024), improving the token efficiency of MoA by employing judge model to assess and forward only the most optimal responses to the next round. (3) To explore the impact of self-assessment and cross-assessment, we also compare RouteMoA with the version that without self-assessment and without cross-assessment for ablation study. Implementation Details. We use OpenCompass (Contributors, 2023) for data generation and evaluation. For scorer training, we employ mDeBERTaV3-base (He et al., 2021) as the encoder, small language model with only 86M parameters. Each LLM embedding is projected to 768-dimensional vector space. The training parameters are set as α = 0.2 and λ = 0.5, which are observed to be insensitive within the ranges of [0.2, 2] and [0.3, 0.9], respectively. The number of k-means clusters is set to 6. Training is conducted using the AdamW optimizer with learning rate of 5 105, weight decay of 0.01, and mini-batch size of 64. We report average performance, cost, and latency. Experiments are run on 80GB GPUs. Exp1: Scalability Evaluation on Large-Scale Model Pool. To validate the practical scalability 1https://openrouter.ai/ 5 Table 1: Performance and efficiency comparison on the large-scale model pool (15 LLMs). indicates an improvement over MoA, while represents degradation compared to MoA. Both are denoted by percentage. Method Language Understanding Reading&QA Logic Reasoning Math Reasoning Language Generation Avg. Accuracy (%) Cost ($) Latency (s) MoA SMoA RouteMoA MoA SMoA RouteMoA MoA SMoA RouteMoA 83.4 78.4 6.00% 84.0 0.72% 321.7 47.8 85.1% 24.9 92.3% 126.3 76.4 39.5% 43.4 65.6% 88.0 85.3 3.07% 88.0 0.00% 303.4 53.8 82.3% 14.2 95.3% 101.4 94.1 7.20% 27.8 72.6% 93.3 91.1 2.36% 95.6 2.50% 385.3 57.2 85.2% 28.7 92.6% 134.2 76.1 43.3% 58.9 56.1% 49.7 53.1 6.84% 73.3 47.5% 751.1 232.5 69.0% 94.6 87.4% 619.5 471.2 23.9% 211.4 65.9% 41.9 40.3 3.82% 51.9 23.9% 477.5 110.4 76.9% 65.7 86.2% 258.9 257.1 0.70% 109.3 57.8% 71.3 69.7 2.24% 78.6 10.2% 447.8 100.4 77.6% 45.6 89.8% 248.1 195.0 21.4% 90.2 63.6% Table 2: Performance and efficiency comparison on the small-scale model pool (5 LLMs). Oracle means using ground truth assessment scores for LLM selection. Best results of multi-LLM methods are bold. paired t-test confirms that the improvement of RouteMoA over SMoA is statistically significant (t = 2.296, = 0.0217 < 0.05). Type Method MATH ARC-c MBPP RACE-high MMLU-bio Accuracy(%) Single LLM Single LLM with Routing Multi-LLMs Ours Resource Gemma-2-9B-it Ministral-8B-Instruct-2410 Qwen2.5-Coder-7B-Instruct Qwen2.5-Math-7B-Instruct Bio-Medical-Llama-3-8B Oracle RouteLLM RouterDC MoA SMoA RouteMoA 46.5 51.0 65.3 80.7 11.7 83.8 64.3 72.8 90.2 85.3 85.5 50.6 75.1 96.8 84.8 87.3 66.2 63.0 79.8 52.9 16. 86.7 76.3 72.6 85.6 80.3 80.6 55.0 77.3 94.5 79.4 78.3 78.6 70.7 67.2 43.5 87.0 95.6 65.7 70.9 Avg. 75.9 72.7 77.5 63.0 46.2 92.5 76.2 78.9 73.6 73.5 0.10% 87.0 89.4 2.80% 75.5 79.4 5.20% 80.1 84.0 4.90% 76.0 75.7 0.40% 80.9 82.6 2.10% 76.0 3.30% 88.2 1.40% 79.8 5.70% 81.0 1.10% 79.3 4.30% 83.1 2.70% Cost ($) Latency (s) Dataset MATH ARC-c MBPP RACE-high MMLU-bio Total MATH ARC-c MBPP RACE-high MMLU-bio Avg. MoA SMoA 19.68 2.27 0.61 8.53 1. 36.03 26.62 12.05 15.52 13.45 14. 16.32 4.4077.6% 0.4779.3% 0.1870.5% 2.2274.0% 0.3679.8% 8.2377.2% 23.1613.0% 10.4513.3% 10.3033.6% 11.0218.1% 11.8615.7% 13.3118.4% RouteMoA 4.0379.5% 0.2887.7% 0.3739.3% 1.8278.7% 0.2188.2% 6.7181.4% 19.0528.4% 9.7319.3% 7.3152.9% 4.4566.9% 9.5132.4% 10.0138.7% and efficiency of RouteMoA in real-world deployment scenarios, we construct large-scale model pool consisting of 15 state-of-the-art LLMs of varying sizes (from 4B to 235B parameters) and capabilities, including general-purpose, reasoningspecialized, and code/math-focused models (see Table 7 in Appendix). Notably, this pool contains models with both standard (no-think) and advanced reasoning (think) modes, presenting diverse and challenging testbed for multi-agent collaboration. Evaluation Benchmark. We conduct comprehensive evaluation on collection of 30 datasets spanning five critical capability categories: Language Understanding, Reading & QA, Logic Reasoning, Math Reasoning, and Language Generation (see Table 8, 9 for the full list). This broad coverage ensures rigorous assessment of generalizability. Exp2: Performance on Small-Scale Model Pool. To enable direct and fair comparison with MoA and SMoA (which are limited to small pools due to their full-model inference design), we further evaluate on compact but diverse pool of 5 LLMs: Gemma-2-9B-it (Team et al., 2024), Ministral8B-Instruct (Jiang et al., 2023), Qwen2.5-Coder7B-Instruct (Hui et al., 2024), Qwen2.5-Math-7BInstruct (Yang et al., 2024b), and Bio-MedicalLlama-3-8B (ContactDoctor, 2024). The evaluation covers 5 datasets (MATH-500 (Hendrycks et al., 2021), ARC-Challenge (Clark et al., 2018), MBPP (Austin et al., 2021), RACE-high (Lai et al., 2017), MMLU-bio (Hendrycks et al., 2020)) representing mathematics, reasoning, coding, reading, and biomedical knowledge. Exp3: Out-of-Distribution Generalization. We further evaluate generalization on the challenging AGIEval-Gaokao (Zhong et al., 2024) benchmark, which spans nine subjects (Biology, Chemistry, Chinese, English, Geography, History, MathCloze, MathQA, Physics). This human-exam benchmark tests the models ability to handle diverse, unseen tasks requiring human-like reasoning. 4.2 Main Results Scalability Evaluation on Large-Scale Model Pool. As shown in Table 1, RouteMoA demonstrates exceptional scalability, performance, and 6 Table 3: Out-of-distribution benchmark comparison between SMoA and RouteMoA. Method Biology Chemistry Chinese English Geography History MathCloze MathQA Physics OOD Avg. Accuracy (%) Cost ($) Latency (s) SMoA 53.33 RouteMoA 58.10 SMoA 4.71 RouteMoA 4.18 SMoA 11.25 RouteMoA 6.12 37.68 37. 6.90 7.77 15.41 14.46 49.59 49.19 8.05 6.33 10.90 4.57 80.39 77. 5.25 3.46 9.40 2.71 60.80 67.84 3.80 4.15 9.79 5.59 64.26 69. 3.49 3.33 9.86 4.26 27.12 27.12 8.79 6.08 19.09 17.72 64.10 60. 9.04 7.57 19.85 20.08 39.00 43.50 7.31 7.91 15.91 15.93 52.92 54. 6.37 5.64 13.50 10.16 Table 4: Ablation study for mixture of judges on smallscale model pool. Method Performance(%) Cost($) Latency(s) RouteMoA w/o self. w/o cross. 83.1 82.6 82.7 7.68 7.99 7.25 10.64 10.49 10.29 efficiency. It achieves an average accuracy of 78.6, significantly surpassing MoA (71.3) and SMoA (69.7), with especially large gains in Math Reasoning (+47.5%) and Language Generation (+23.9%). Unlike MoA and SMoA, which lack clear model selection criteria and become infeasible at scale due to prohibitive costs and context limits, RouteMoA remains practical by dynamically routing queries to an optimal model subset. This approach reduces total cost by 89.8% and latency by 63.6% compared to MoA, while also outperforming SMoA in both efficiency and accuracy. Furthermore, RouteMoA consistently achieves the best accuracy, lowest cost, and lowest latency across all five capability categories. In Language Understanding and Reading&QA, it matches or surpasses MoAs accuracy while reducing cost by 95.3%. These results demonstrate that dynamic routing tailored for multi-agent systems enables efficient and effective collaboration in large, heterogeneous model pools, particularly for complex tasks requiring complementary model strengths. Performance on Small-Scale Model Pool. As shown in Table 2, RouteMoA substantially improves efficiency, reducing inference cost by 81.4% compared to MoA (6.71 vs. 36.03), and by 88.2% on domain-specific scenarios such as MMLUbio, demonstrating effective avoidance of expenIt also lowers average sive generalist models. latency by 38.7% (10.01s vs. 16.32s) due to lightweight scoring and targeted model selection. Meanwhile, RouteMoA achieves the highest average score (83.1), outperforming single models and MoA, with statistically significant gains over SMoA (paired t-test shows = 2.296, = 0.0217 < 0.05). These results confirm that routing comFigure 4: Average values of three scorer assessment metrics (Top-1-Hit, Top-3-Hit, and Top-3-Agree) under different training hyperparameters (λ and α). bined with response aggregation forms an effective paradigm for multi-LLM collaboration. Out-of-Distribution Generalization. As shown in Table 3, RouteMoA outperforms SMoA with higher average accuracy (54.62 vs. 52.92) while reducing cost by 11.5% and latency by 24.7%. It achieves notable accuracy gains in humanities and science subjects, including Geography (+7.04), History (+5.10), Physics (+4.50), and Biology (+4.77). These results demonstrate that RouteMoA effectively exploits specialized models on unseen tasks requiring human-like reasoning, exhibiting strong out-of-distribution generalization. 4.3 Analysis Scorer Assessment. In RouteMoA, the scorer plays key role in providing an initial screening of candidate models. Rather than requiring precise performance prediction, the scorer is designed to identify small set of high-potential models for subsequent refinement via mixture-of-judges (including self-assessment and cross-assessment). To evaluate its effectiveness, we introduce three metrics: Top-1 Hit Rate (Top-1-Hit), Top-3 Hit Rate (Top-3-Hit) and Top-3 Agreement Rate (Top-3Agree). Top-1-Hit and Top-3-Hit measure the probability that the ground-truth best model appears in the scorers top-one / top-three predictions, which reaches 90.7% and 97.9% when α = 0.2, λ = 0.5. 7 Figure 5: Case study of adjusting wrong scorer predictions with selfand cross-assessment. The Top-3-Agree quantifies the overlap between the scorers top-three selections and the groundtruth top-three models, achieving 96.2%, indicating that the scorer successfully narrows down the candidate pool to small subset containing highperforming models in the majority of cases. The detailed calculation method and results under different α, λ combinations are shown in Appendix D. Mixture-of-Judges Ablation Study. To explore the impact of self-assessment and cross-assessment in mixture of judges, we conduct ablation studies. As shown in Table 4, the average performance of RouteMoA without self-assessment is 82.6, and the performance without cross-assessment is 82.7, both of them are lower than the 83.1 achieved by RouteMoA. RouteMoA without cross-assessment achieves the lowest cost and latency. It is natural result since the additional judging token will not be generated without cross-assessment. Case Study. To illustrate how the routing pipeline, especially the mixture of judges works, we present an example from the RACE-high dataset In this dataset, Gemma-2, Minisin Figure 5. tral, and Qwen2.5-Coder usually perform well, whereas Qwen2.5-Math and Bio-Medical-Llama show weaker performance. In layer-1, the scorer incorrectly assigns high score (0.74) to Qwen2.5Math, while giving relatively low score to the high-quality model Qwen2.5-Coder. In layer-2, models generate self-assessment scores. Qwen2.5Math and Bio-Medical-Llama produce low selfscores due to their inability to accurately follow instructions, thus the self-assessment mechanism effectively filters out low-quality models. In layer3, cross-assessment further widens the score gap between highand low-quality models, since low cross-score is assigned to Qwen2.5-Math. Examining the model responses across layers, we observe increasing participation of high-quality models and progressive improvement in response quality as the number of layers increases. Eventually, models achieve consensus at the final layer. This correction process is supported by the high Top-3 Hit Rate (97.9%) of the scorer, indicating that in 97.9% of cases, at least one correct model is included in the initial candidate set. Once present, the multi-agent collaboration mechanism effectively identifies and amplifies the correct response through answer aggregation and self/cross-assessment."
        },
        {
            "title": "5 Conclusion",
            "content": "In conclusion, we present RouteMoA, an efficient Mixture-of-Agents framework that overcomes the resource limitations of classical MoA through dynamic routing. The framework employs lightweight scorer to perform an initial screening of candidates using prior knowledge from the query, followed by mixture of judges that refines scores with posterior knowledge from model outputs. RouteMoA significantly reduces cost and latency while maintaining strong performance. Experimental results also show strong OOD generalization ability and large-scale model pool scalability. This prior-posterior routing approach offers scalable and practical path toward efficient multiLLM collaboration."
        },
        {
            "title": "6 Limitation",
            "content": "The scorer requires retraining to support new LLMs. However, integrating new LLM only involves training lightweight scorer on small curated query set, which takes about 25 minutes. Future work will explore retrain-free routing."
        },
        {
            "title": "References",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, and 1 others. 2023. GPT-4 technical report. arXiv preprint arXiv:2303.08774. Meta AI. 2025. Introducing LLaMA 3.1: Our most capable models to date. Alon Albalak, Duy Phung, Nathan Lile, Rafael Rafailov, Kanishk Gandhi, Louis Castricato, Anikait Singh, Chase Blagden, Violet Xiang, Dakota Mahan, and 1 others. 2025. Big-math: large-scale, high-quality math dataset for reinforcement learning in language models. arXiv preprint arXiv:2502.17387. Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, and 1 others. 2021. Program synthesis with large language models. arXiv preprint arXiv:2108.07732. Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu Chen, Xin Chen, Xun Chen, Zehui Chen, Zhi Chen, Pei Chu, and 1 others. 2024. InternLM2 technical report. arXiv preprint arXiv:2403.17297. Daniel Cera, Mona Diabb, Eneko Agirrec, Inigo LopezGazpioc, Lucia Speciad, and Basque Country Donostia. Semeval-2017 task 1: Semantic textual similarity multilingual and cross-lingual focused evaluation. Yingshan Chang, Mridu Narang, Hisami Suzuki, Guihong Cao, Jianfeng Gao, and Yonatan Bisk. 2022. Webqa: Multihop and multimodal qa. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1649516504. Lingjiao Chen, Jared Quincy Davis, Boris Hanin, Peter Bailis, Ion Stoica, Matei Zaharia, and James Zou. 2024a. Are more LLM calls all you need? towards the scaling properties of compound ai systems. Shuhao Chen, Weisen Jiang, Baijiong Lin, James Kwok, and Yu Zhang. 2024b. RouterDC: Query-based router by dual contrastive learning for assembling large language models. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, and 1 others. 2021. Training verifiers arXiv preprint to solve math word problems. arXiv:2110.14168. ContactDoctor. 2024. Bio-Medical: highperformance biomedical language model. OpenCompass Contributors. 2023. OpenCompass: universal evaluation platform for foundation models. Yiming Cui, Ting Liu, Wanxiang Che, Li Xiao, Zhipeng Chen, Wentao Ma, Shijin Wang, and Guoping Hu. 2019. span-extraction dataset for chinese machine reading comprehension. In Proceedings of the 2019 conference on empirical methods in natural language processing and the 9th international joint conference on natural language processing (EMNLP-IJCNLP), pages 58835889. Dujian Ding, Ankur Mallick, Chi Wang, Robert Sim, Subhabrata Mukherjee, Victor Rühle, Laks VS Lakshmanan, and Ahmed Hassan Awadallah. 2024. Hybrid LLM: Cost-efficient and quality-aware query routing. William Dolan and Chris Brockett. 2005. Automatically constructing corpus of sentential paraphrases. In Proceedings of the third international workshop on paraphrasing (IWP2005). Markus Freitag, George Foster, David Grangier, Viresh Ratnakar, Qijun Tan, and Wolfgang Macherey. 2021. Experts, errors, and context: large-scale study of human evaluation for machine translation. Preprint, arXiv:2104.14478. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad AlDahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, and 1 others. 2024. The LLaMA 3 herd of models. arXiv preprint arXiv:2407.21783. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, and 1 others. 2025. Deepseek-r1: Incentivizing reasoning capability in arXiv preprint llms via reinforcement learning. arXiv:2501.12948. Pengcheng He, Jianfeng Gao, and Weizhu Chen. 2021. DeBERTaV3: Improving deberta using electra-style pre-training with gradient-disentangled embedding sharing. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020. Measuring massive multitask language understanding. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021. Measuring mathematical problem solving with the math dataset. 9 Baotian Hu, Qingcai Chen, and Fangze Zhu. 2015. Lcsts: large scale chinese short text summarization dataset. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 19671972. Hai Hu, Kyle Richardson, Liang Xu, Lu Li, Sandra Kübler, and Lawrence Moss. 2020. Ocnli: Original chinese natural language inference. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 35123526. Qitian Jason Hu, Jacob Bieker, Xiuyu Li, Nan Jiang, Benjamin Keigwin, Gaurav Ranganath, Kurt Keutzer, and Shriyash Kaustubh Upadhyay. 2024. RouterBench: benchmark for multi-llm routing system. arXiv preprint arXiv:2403.12031. Zhongzhan Huang, Guoming Ling, Vincent Liang, Yupei Lin, Yandong Chen, Shanshan Zhong, Hefeng Wu, and Liang Lin. 2025. RouterEval: comprehensive benchmark for routing llms to explore arXiv preprint model-level scaling up in llms. arXiv:2503.10657. Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Keming Lu, and 1 others. 2024. Qwen2.5-Coder technical report. arXiv preprint arXiv:2409.12186. Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. In The Thirteenth International Conference on Learning Representations. Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. 2023. Mistral 7b. arXiv preprint arXiv:2310.06825. Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli Tran-Johnson, and 1 others. 2022. Language models (mostly) know what they know. arXiv preprint arXiv:2207.05221. Yehuda Koren, Robert Bell, and Chris Volinsky. 2009. Matrix factorization techniques for recommender systems. Computer. Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. 2017. Race: Large-scale reading comprehension dataset from examinations. Gina-Anne Levow. 2006. The third international Chinese language processing bakeoff: Word segmentation and named entity recognition. In Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 108117, Sydney, Australia. Association for Computational Linguistics. Dawei Li, Bohan Jiang, Liangjie Huang, Alimohammad Beigi, Chengshuai Zhao, Zhen Tan, Amrita Bhattacharjee, Yuxuan Jiang, Canyu Chen, Tianhao Wu, and 1 others. 2025. From generation to judgment: Opportunities and challenges of llm-as-a-judge. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 27572791. Dawei Li, Zhen Tan, Peijia Qian, Yifan Li, Kumar Satvik Chaudhary, Lijie Hu, and Jiayi Shen. 2024. SMoA: Improving multi-agent large language models with sparse mixture-of-agents. arXiv preprint arXiv:2411.03284. Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Shuming Shi, and Zhaopeng Tu. 2024. Encouraging divergent thinking in large language models through multi-agent debate. Stephanie Lin, Jacob Hilton, and Owain Evans. 2022. Truthfulqa: Measuring how models mimic human falsehoods. In Proceedings of the 60th annual meeting of the association for computational linguistics (volume 1: long papers), pages 32143252. Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, and 1 others. 2024. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437. Xin Liu, Qingcai Chen, Chong Deng, Huajun Zeng, Jing Chen, Dongfang Li, and Buzhou Tang. 2018. Lcqmc: large-scale chinese question matching corpus. In Proceedings of the 27th international conference on computational linguistics, pages 19521962. Keming Lu, Hongyi Yuan, Runji Lin, Junyang Lin, Zheng Yuan, Chang Zhou, and Jingren Zhou. 2023. Routing to the expert: Efficient reward-guided ensemble of large language models. arXiv preprint arXiv:2311.08692. Chengqian Ma, Wei Tao, and Steven Guo. 2025. C3: bilingual benchmark for spoken dialogue models exploring challenges in complex conversations. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 2278922807. James MacQueen. 1967. Some methods for classification and analysis of multivariate observations. In Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics and Probability, Volume 1: Statistics. Nasrin Mostafazadeh, Michael Roth, Annie Louis, Nathanael Chambers, and James Allen. 2017. Lsdsem 2017 shared task: The story cloze test. In Proceedings of the 2nd workshop on linking models of lexical, sentential and discourse-level semantics, pages 4651. 10 Hwee Tou Ng, Siew Mei Wu, Ted Briscoe, Christian Hadiwinoto, Raymond Hendy Susanto, and Christopher Bryant. 2014. The conll-2014 shared task on grammatical error correction. In Proceedings of the eighteenth conference on computational natural language learning: shared task, pages 114. Gemini Team, Rohan Anil, Sebastian Borgeaud, JeanBaptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, and 1 others. 2023. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805. Isaac Ong, Amjad Almahairi, Vincent Wu, Wei-Lin Chiang, Tianhao Wu, Joseph Gonzalez, Waleed Kadous, and Ion Stoica. 2024. RouteLLM: Learning to route LLMs with preference data. arXiv preprint arXiv:2406.18665. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, and 1 others. 2022. Training language models to follow instructions with human feedback. Xipeng Qiu, Jingjing Gong, and Xuanjing Huang. 2017. Overview of the nlpcc 2017 shared task: Chinese news headline categorization. In National CCF Conference on Natural Language Processing and Chinese Computing, pages 948953. Springer. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 23832392. Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. 2020. Winogrande: An adversarial winograd schema challenge at scale. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 87328740. Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over sentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 16311642, Seattle, Washington, USA. Association for Computational Linguistics. Dimitris Stripelis, Zhaozhuo Xu, Zijian Hu, Alay Shah, Han Jin, Yuhang Yao, Jipeng Zhang, Tong Zhang, Salman Avestimehr, and Chaoyang He. 2024. TensorOpera Router: multi-model router for efficient llm inference. Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc Le, Ed Chi, Denny Zhou, and 1 others. 2023. Challenging big-bench tasks and whether chain-of-thought can solve them. In ACL (Findings). Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. 2019. Commonsenseqa: question answering challenge targeting commonsense knowlIn Proceedings of the 2019 Conference of edge. the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 41494158. Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Léonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ramé, and 1 others. 2024. Gemma 2: Improving open language models at practical size. arXiv preprint arXiv:2408.00118. Laurens Van der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-sne. Journal of Machine Learning Research. Bingning Wang, Ting Yao, Qi Zhang, Jingfang Xu, and Xiaochuan Wang. 2020. Reco: large scale chinese reading comprehension dataset on opinion. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 91469153. Junlin Wang, Jue Wang, Ben Athiwaratkun, Ce Zhang, and James Zou. 2024a. Mixture-of-agents enhances large language model capabilities. arXiv preprint arXiv:2406.04692. Yan Wang, Xiaojiang Liu, and Shuming Shi. 2017. Deep neural solver for math word problems. In Proceedings of the 2017 conference on empirical methods in natural language processing, pages 845854. Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, and 1 others. 2024b. Mmlu-pro: more robust and challenging multi-task language understanding benchmark. Advances in Neural Information Processing Systems, 37:9526695290. Yuxia Wang, Zenan Zhai, Haonan Li, Xudong Han, Shom Lin, Zhenxuan Zhang, Angela Zhao, Preslav Nakov, and Timothy Baldwin. 2024c. chinese dataset for evaluating the safeguards in large language models. In Findings of the Association for Computational Linguistics: ACL 2024, pages 3106 3119. Alex Warstadt, Amanpreet Singh, and Samuel Bowman. 2019. Neural network acceptability judgments. Transactions of the Association for Computational Linguistics, 7:625641. Liang Xu, Hai Hu, Xuanwei Zhang, Lu Li, Chenjie Cao, Yudong Li, Yechen Xu, Kai Sun, Dian Yu, Cong Yu, and 1 others. 2020. Clue: chinese language understanding evaluation benchmark. In Proceedings of the 28th International Conference on Computational Linguistics, pages 47624772. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, and 1 others. arXiv preprint 2025. Qwen3 technical report. arXiv:2505.09388. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, and 1 others. 2024a. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115. An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, and 1 others. 2024b. Qwen2.5-Math technical report: Toward mathematarXiv ical expert model via self-improvement. preprint arXiv:2409.12122. Murong Yue, Jie Zhao, Min Zhang, Liang Du, and Ziyu Yao. 2024. Large language model cascades with mixture of thought representations for cost-efficient reasoning. Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015. Character-level convolutional networks for text classification. Advances in neural information processing systems, 28. Xiaotian Zhang, Chunyang Li, Yi Zong, Zhengyu Ying, Liang He, and Xipeng Qiu. 2023. Evaluating the performance of large language models on gaokao benchmark. arXiv preprint arXiv:2305.12474. Yuan Zhang, Jason Baldridge, and Luheng He. 2019. Paws: Paraphrase adversaries from word scrambling. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 12981308. Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, and 1 others. 2023. survey of large language models. arXiv preprint arXiv:2303.18223. Yuanyuan Zhao, Nan Jiang, Weiwei Sun, and Xiaojun Wan. 2018. Overview of the nlpcc 2018 shared task: Grammatical error correction. In CCF International Conference on Natural Language Processing and Chinese Computing, pages 439445. Springer. Chujie Zheng, Minlie Huang, and Aixin Sun. 2019. Chid: large-scale chinese idiom dataset for cloze test. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 778787. Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan Duan. 2024. Agieval: human-centric In benchmark for evaluating foundation models. Findings of the Association for Computational Linguistics: NAACL 2024, pages 22992314. Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou. 2023. Instruction-following evaluation for large language models. arXiv preprint arXiv:2311.07911. Aggregation Prompt, Selfand Cross-assessment Prompt The prompts used in the inference stage of RouteMoA are shown in Figure 6, Figure 7, and Figure 8. These prompts include instructions for the LLM to aggregate responses from models in the previous layer, to score its own answer (selfassessment prompt), and to evaluate answers from other LLMs (cross-assessment prompt). Specifically, the prompt at layer-1 (Figure 6) includes both the aggregation prompt and the self-assessment prompt. For intermediate layer-l1<l<L (Figure 7), the prompts consist of the aggregation prompt, the self-assessment prompt, and the cross-assessment prompt. Finally, in the last layer (Figure 8), only the aggregation prompt is included. You are participating in multi-agent reasoning task. **Your objectives** 1. Produce the best possible answer to the users query. 2. Critically evaluate your own answer and give it quality score **between 0 and 1** (0 = completely wrong, 1 = perfect). **Output format** - return **ONLY** valid JSON object: json { \"answer\": \"<your answer>\", \"self_score\": <float between 0 and 1> } Do **not** add any keys, comments or extra text. Figure 6: The prompt used in layer-1."
        },
        {
            "title": "Generation",
            "content": "To generate training data that meets the requirements of layer-1, we prompt the LLMs to answer questions from the datasets. The prompts used for each dataset are presented in Figure 5. The model outputs are then compared with the ground 12 You are participating in multi-agent reasoning task.Here are several answers from other LLMs: answer_block **Your objectives** 1. Taking every answer in the previous round into account and produce an improved answer to the users query. 2. Critically evaluate your own answer and give it quality score **between 0 and 1** (0 = completely wrong, 1 = perfect). 3. Critically evaluate **each** ANSWER_i above with value in [0, 1] representing its quality. (0 = completely wrong, 1 = perfect) **Output format** - return **ONLY** valid JSON object: json { \"answer\": \"<your improved answer>\", \"self_score\": <float>, \"peer_scores\": [<float_score_for_ANSWER_0>, <float_score_for_ANSWER_1>, ...] } Do not include any other text. Figure 7: The prompt used in intermediate layeri1<i<l. You have been provided with set of responses from various open-source models to the latest user query. Your task is to synthesise these responses into single, high-quality answer. Critically evaluate the information given, correct any mistakes, and produce coherent, well-structured response that meets the highest standards of accuracy. Responses from models: 1.model_response_1 2.model_response_2 ... Figure 8: The prompt used in the last layer. truth answers. For layer-ll>1, we prompt the models to generate aggregated answers based on reference answers from all models. These aggregated responses are subsequently evaluated against the ground truth. Additionally, judge model is employed to assess the quality of each answer. The prompt used for generating the aggregated answers is shown in Figure 9. We use InternLM2-1.8BReward as the judge model. Clustering Details for Sample-Sample"
        },
        {
            "title": "Loss",
            "content": "The sample-sample contrastive loss encourages semantically similar queries to have closer embeddings. To achieve this, we use t-SNE (Van der Maaten and Hinton, 2008) and k-means (MacQueen, 1967) algorithm to transfer input prompt embeddings to low-dimensional vectors and cluster them into groups{K1, K2, ..., KQ}. We randomly select an in-group query x+ Kq, and an out-group set {q=qKq} of queries from the training mini-batch. Top-1-Hit, Top-3-Hit and Top-3-Agree"
        },
        {
            "title": "Calculation Details",
            "content": "In this section, we provide detailed definitions and calculation methods for the three evaluation metrics used to assess the effectiveness of the scorer in RouteMoA: Top-1 Hit Rate (Top-1-Hit), Top-3 Hit Rate (Top-3-Hit), and Top-3 Agreement Rate (Top3-Agree). 13 Dataset MATH GSM8k ARC-c MBPP Table 5: The prompt of each dataset. Prompt Answer the following multiple choice question. The last line of your response should be of the following format: ANSWER: $LETTER (without quotes) where LETTER is one of ABCD. Think step by step before answering. {question} {options} {question} Please reason step by step, and put your final answer within boxed{}. {problem} Please reason step by step, and put your final answer within boxed{}. You are an expert Python programmer, and here is your task: {prompt} Your code should pass these tests: {test_list} RACE-high Answer the following multiple choice question. The last line of your response should be of the following format: ANSWER: $LETTER (without quotes) where LETTER is one of ABCD. Think step by step before answering. {question}{options} MMLU-biomed Answer the following multiple choice question. The last line of your response should be of the following format: ANSWER: $LETTER (without quotes) where LETTER is one of ABCD. Think step by step before answering. Article: {article} Q:{questions} {options} It is crucial You have been provided with set of responses from various open-source models to the latest user query. Your task is to synthesize these responses into single, to high-quality response. critically evaluate the information provided in these responses, recognizing that some of it may be biased or incorrect. Your response should not simply replicate the given answers but should offer refined, accurate, and comprehensive reply to the instruction. Ensure your response is well-structured, coherent, and adheres to the highest standards of accuracy and reliability. This is the original question answered by these models: original question Responses from models: 1.model_response_1 2.model_response_2 ... Figure 9: The prompt used to generate model response with reference answers. Top-1 Hit Rate (Top-1-Hit) The Top-1 Hit Rate (Top-1-Hit) measures the probability that the scorers top-1 prediction is one of the models that are able to provide correct answer. Let be the index of the top 1 model according to the scorer, and = {t1, ..., tk} be the set of indices of the models that are able to provide correct answer. We define the Top-1 Hit Rate as: Top-1-Hit = (cid:40) 1, 0, if T, otherwise. The final Top-1-Hit is obtained by averaging this score over all test cases that can be answered correctly by at least one model. Top-3 Hit Rate (Top-3-Hit) The Top-3 Hit Rate (Top-3-Hit) measures the probability that the ground-truth best model is included in the scorers top-three predictions. Let = {p1, p2, p3} be the set of indices of the top 3 models according to the scorer, and = {t1, ..., tk} be the set of indices of the models that are able to provide correct answer. We define the Top-3 Hit Rate as: Top-3-Hit = (cid:40) 1, 0, if 1, otherwise. The final Top-3-Hit is obtained by averaging this score over all test cases that can be answered correctly by at least one model. 14 Top-3 Agreement Rate (Top-3-Agree) The Top-3 Agreement Rate (Top-3-Agree) is metric to evaluate whether the top three models selected by the scorer align with those that have the best true performance. For each test case or dataset, the scorer selects the top 3 models based on predicted scores, and we compare them with the top 3 models according to ground truth performance. Let = {p1, p2, p3} be the set of indices of the top 3 models according to the scorer, and = {t1, t2, t3} be the set of indices of the top 3 models according to the ground truth. We define the Top-3 Agreement Rate as: Top-3-Agree = 1, 0.6, 0.3, 0, if = 3, if = 2, if = 1, otherwise. This scoring rule assigns full score if all top 3 models are correctly identified, partial score if two / one out of the top 3 are correct, and zero otherwise. The final Top-3-AR is obtained by averaging this score over all test cases that can be answered correctly by at least one model. Scorer evaluation under different α and λ We calculate the three evaluation metrics: Top-1Hit, Top-3-Hit, and Top-3-Agree under different combinations of scorer training hyperparameters α and λ, the results are shown in Figure 10, 11, 12, respectively. Figure 10: Top-1-Hit under different training hyperparameters (λ and α) for the scorer module."
        },
        {
            "title": "E Dataset Statistics for Scorer Training of",
            "content": "Small-Scale Model Pool The dataset statistics for scorer training are presented in Table 6. The train/dev/test splits generally 15 Figure 11: Top-3-Hit under different training hyperparameters (λ and α) for the scorer module. Figure 12: Top-3-Agree under different training hyperparameters (λ and α) for the scorer module. follow the original partitioning of each dataset. For datasets that lack dev split, we further divide the original training portion into train and dev subsets. Dataset MATH ARC-c MBPP RACE-high MMLU-biomed train 7125 1119 120 7000 92 dev 375 299 43 300 test 5000 1165 257 3498 817 Table 6: Dataset statistics for scorer training. Evaluation Details on Large-Scale"
        },
        {
            "title": "Model Pool",
            "content": "To explore the ability of RouteMoA to handle largescale agent pools, we conduct experiments with an agent pool containing 15 newest LLMs including Qwen (Yang et al., 2025) and Deepseek (Guo et al., 2025; Liu et al., 2024) series with varying sizes (from 4B to 235B parameters), capabilities, and think mode, as listed in Table 7. Note that both MoA and SMoA are infeasible to handle the larger agent pool, since these methods need all LLMs to infer and then aggregate the responses. When the agent pool is large, it is huge cost for all model to perform inference. Besides, the context will be too long and exceeds the limit. Although we can choose subset of agents from the large agent pool for MoA and SMoA to alleviate these problems, these two methods lack clear criteria for selecting such model subsets. If we select high-performing models such as Deepseek-R1, the total cost will be high, and the context will be long. If we select smaller models such as Qwen2.5-7B, although the cost will be lower, the performance can not be guaranteed. In contrast, our RouteMoA method is designed to deal with such situation. It has clear criteria on how to select agent subsets according to the categories and complexity of user queries. It lowers cost while ensures competitive performance. Specifically, to handle the larger agent pool, we collect wide range of datasets as query pool, shown in Table 9. We only select subset from each dataset to enable that the whole query pool is not very large. The dataset statistics are also shown in Table 9. Maintaining smaller query pool benefits the scalability of the scorer. If new LLM is added into the agent pool, it infers on these queries (the process will be shorter if the query pool is relatively small) and results are used to train new scorer. The training process can be completed in less than 30 minutes, using less than 50GB GPU memory. We evaluate RouteMoA on 30 test sets, as shown in Table 8, each set contains 15 samples (not overlapping with the training set). Among these, lcqmc (Liu et al., 2018), mrpc (Dolan and Brockett, 2005), and cluewsc2020 (Xu et al., 2020) are out-of-distribution test sets, which do not appear in the training set. Model & Data License and Intended"
        },
        {
            "title": "Use Statement",
            "content": "Our experiments utilize collection of publicly available models and benchmark datasets. To the best of our knowledge, our use of these models and datasets is consistent with their intended research purposes as specified by their original creators. All models and datasets used in this work are cited. For any model or dataset we use, we adhere to its stipulated terms of use. Size Model Thinking Mode Small Qwen3-4B Qwen3-8B Qwen2.5-7B-Instruct no-think no-think no-think Medium Large no-think Qwen3-14B no-think Qwen3-32B think/no-think Qwen3-30B-A3B QwQ-32B think DeepSeek-R1-Distill-Qwen-14B think DeepSeek-R1-Distill-Qwen-32B think Qwen2.5-72B-Instruct Qwen3-235B-A22B DeepSeek-R1 DeepSeek-R1-0528 DeepSeek-V3 DeepSeek-V3-0324 no-think think/no-think think think no-think no-think Table 7: Models that forms larger agent pool. Category Dataset Language Understanding Reading&QA Logic Reasoning Math Reasoning Language Generation lcqmc, ocnli, sst2 cola, mrpc, msra, qqp sts_b, ag_news qnli, chid_baidu webqa c3 cmrc race story_cloze cluewsc2020 winogrande_wsc truthful_qa bigmath gsm8k GAOKAO-2023_Math_en geometry prealgebra precalculus word_manipulation_v2 nlpcc2017_task2 lcsts nlpcc2018_task2 conll2014 Table 8: The test dataset categories for the large agent pool. 16 Dataset Description Data Num News topic classification ag_news (Zhang et al., 2015) Algebra math problems algebra (Hendrycks et al., 2020) BIG-Bench Hard subset BBH-100 (Suzgun et al., 2023) bigmath (Albalak et al., 2025) Complex math problems Chinese multiple-choice QA c3 (Ma et al., 2025) Chinese idiom cloze test chid (Zheng et al., 2019) Baidu Chinese idiom dataset chid_baidu (Zheng et al., 2019) Safety and bias evaluation chinese_safety_test_bias (Wang et al., 2024c) cmrc (Cui et al., 2019) Chinese machine reading comprehension Linguistic acceptability corpus cola (Warstadt et al., 2019) Commonsense question answering commonsense_qa (Talmor et al., 2019) conll2014 (Ng et al., 2014) Grammatical error correction counting_and_probability (Hendrycks et al., 2020) Math combinatorics problems GAOKAO-2023_Math_en (Zhang et al., 2023) gsm8k_test_100 (Cobbe et al., 2021) IFEval (Zhou et al., 2023) lcsts (Hu et al., 2015) LiveCodeBench100-2305-2409 (Jain et al.) math (Hendrycks et al., 2021) math23k_test_100 (Wang et al., 2017) MATH-500 (Hendrycks et al., 2021) mmlu_pro (Wang et al., 2024b) msra (Levow, 2006) newstest2020 (Freitag et al., 2021) nlpcc2017_task2 (Qiu et al., 2017) nlpcc2018_task2 (Zhao et al., 2018) number_theory (Hendrycks et al., 2020) ocnli (Hu et al., 2020) prealgebra (Hendrycks et al., 2020) precalculus (Hendrycks et al., 2020) qnli (Rajpurkar et al., 2016) qqp (Zhang et al., 2019) race (Lai et al., 2017) reco (Wang et al., 2020) squad (Rajpurkar et al., 2016) sst2 (Socher et al., 2013) story_cloze_test (Mostafazadeh et al., 2017) sts_b (Cera et al.) truthful_qa (Lin et al., 2022) webqa (Chang et al., 2022) winogrande_wsc (Sakaguchi et al., 2020) Chinese college entrance exam math Grade school math (subset) Instruction following evaluation Chinese short text summarization Live programming evaluation General math problems Math word problems (subset) Challenging math competition problems Massive multi-task understanding Named entity recognition Machine translation evaluation set News headline categorization Grammatical error correction Math number theory problems Chinese natural language inference Pre-algebra math problems Precalculus math problems Question-answering NLI Quora question pairs similarity Reading comprehension Chinese reading comprehension Reading comprehension Sentiment analysis (binary) Story completion and reasoning Semantic textual similarity benchmark Truthfulness evaluation in QA Web-based question answering Coreference resolution 80 80 80 16 80 80 80 80 80 77 80 80 80 80 80 320 79 80 80 80 400 224 80 80 80 80 80 80 80 80 80 80 80 80 80 80 80 80 80 80 80 Total 4596 Table 9: The query pool used to train the scorer for the larger agent pool."
        }
    ],
    "affiliations": [
        "CUHK",
        "Nanyang Technological University",
        "Shanghai AI Laboratory",
        "Shanghai Jiao Tong University",
        "Tencent"
    ]
}