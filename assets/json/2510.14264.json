{
    "paper_title": "AlphaQuanter: An End-to-End Tool-Orchestrated Agentic Reinforcement Learning Framework for Stock Trading",
    "authors": [
        "Zheye Deng",
        "Jiashu Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "While Large Language Model (LLM) agents show promise in automated trading, they still face critical limitations. Prominent multi-agent frameworks often suffer from inefficiency, produce inconsistent signals, and lack the end-to-end optimization required to learn a coherent strategy from market feedback. To address this, we introduce AlphaQuanter, a single-agent framework that uses reinforcement learning (RL) to learn a dynamic policy over a transparent, tool-augmented decision workflow, which empowers a single agent to autonomously orchestrate tools and proactively acquire information on demand, establishing a transparent and auditable reasoning process. Extensive experiments demonstrate that AlphaQuanter achieves state-of-the-art performance on key financial metrics. Moreover, its interpretable reasoning reveals sophisticated strategies, offering novel and valuable insights for human traders. Our code for data acquisition and agent training is publicly available at: https://github.com/AlphaQuanter/AlphaQuanter"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 1 ] . [ 1 4 6 2 4 1 . 0 1 5 2 : r AlphaQuanter: An End-to-End Tool-Orchestrated Agentic Reinforcement Learning Framework for Stock Trading"
        },
        {
            "title": "Jiashu Wang",
            "content": "HKUST {zdengah, jwangjv}@cse.ust.hk https://alphaquanter.github.io/ https://github.com/AlphaQuanter/AlphaQuanter"
        },
        {
            "title": "Abstract",
            "content": "While Large Language Model (LLM) agents show promise in automated trading, they still face critical limitations. Prominent multi-agent frameworks often suffer from inefficiency, produce inconsistent signals, and lack the end-to-end optimization required to learn coherent strategy from market feedback. To address this, we introduce AlphaQuanter, single-agent framework that uses reinforcement learning (RL) to learn dynamic policy over transparent, tool-augmented decision workflow, which empowers single agent to autonomously orchestrate tools and proactively acquire information on demand, establishing transparent and auditable reasoning process. Extensive experiments demonstrate that AlphaQuanter achieves state-of-the-art performance on key financial metrics. Moreover, its interpretable reasoning reveals sophisticated strategies, offering novel and valuable insights for human traders. Our code for data acquisition and agent training is publicly available at: https://github.com/AlphaQuanter/AlphaQuanter."
        },
        {
            "title": "Introduction",
            "content": "The exploration of automated trading systems in modern financial markets is flourishing. Traditional machine learning methods (such as SVM, Random Forests, etc.) [13] typically simplify the problem into discrete predictions of price direction at the next moment, making it difficult to effectively integrate multi-source heterogeneous trading signals. Although Deep Reinforcement Learning (DRL) can directly optimize decisions around long-term portfolio returns [4, 5], its black-box nature leads to trading decisions that lack interpretability and traceability. Recently, Large Language Models (LLMs) have demonstrated tremendous potential in the field of financial trading [68]. However, existing LLM-based attempts still face critical challenges. (1) First, the lack of tool orchestration and active information acquisition capabilities makes it difficult for models to autonomously invoke and sequentially utilize external tools during the reasoning process, identify information gaps, and fill them on demand. (2) Second, decision transparency and interpretability are insufficient; current training paradigms are mostly black-box end-to-end optimization or offline answer fitting, lacking transparent display of decision reasoning trajectories and visual analysis of decision patterns, making it difficult to establish user trust and support regulatory audits. (3) Last but not least, prompt-based methods exhibit poor robustness, are extremely sensitive to prompt engineering, and lack effective coordination and constraint mechanisms in multi-agent debate scenarios, frequently leading to low decision efficiency, system fragility, and signal inconsistency. Overall, conducting reasoning under partially observable conditions, integrating heterogeneous signals, and executing actions with calibrated confidence remain core challenges that urgently need to be addressed. Preprint. To address these gaps, we propose AlphaQuanter, single agent trading framework designed to enable informative, explainable and robust trading decisions. First, AlphaQuanter unifies the workflows into one ReAct-like agent [9] tailored for trading-oriented planning and reasoning. We define several tools for various information sources and our framework starts from guided plan followed by iterative tool use and information seeking as well as in-depth analysis. Second, to further enhance decision making capabilities and improve model transparency, we leverage reinforcement learning with verifiable rewards [10, 11] to end-to-end optimize models that can selectively invoke useful tools and effectively gather supporting evidence. We further curate high-quality outcomeand process-based reward signals to guide RL training across diverse actions. This design eliminates the need for extensive prompt engineering across multiple agents, while ensuring both explainability and flexibility in the final decision-making process. Finally we evaluate our framework through comprehensive backtesting protocols. Our key contributions are summarized as follows: We propose novel single-agent framework with effective reasoning chains that ensure both decision consistency and interpretability. We design an end-to-end reinforcement learning approach that trains the agent to actively acquire useful information and select evidences for in-depth analysis. It directly optimizes the entire decision-making process for long-term profitability. Our extensive empirical evaluations demonstrate that AlphaQuanter not only achieves state-of-theart performance on key financial metrics, but also learns sophisticated strategies that offer practical insight for human experts."
        },
        {
            "title": "2 Related Work",
            "content": "Early approaches use traditional machine learning methods, such as SVM and random forest, to frame the task as simple price direction classification [13], which has been proven insufficient due to oversimplification and poor generalization in trading environments [12]. Deep Reinforcement Learning Moody and Saffell [4] pioneered the application of deep reinforcement learning to stock trading, directly optimizing trading performance end-to-end and outperforming supervised learning in long-horizon S&P 500 backtests. iRDPG [13] integrates imitation learning under partially observable Markov decision process framework, using expert behavior to stabilize the training process and improve robustness, but overly relies on existing strategies. DeepTrader [5] introduces macro states and risk-sensitive rewards, achieving dynamic adjustment of long-short positions and risk control. MTS [14] improves returns across multiple datasets through time-aware encoding, parallel short selling, and CVaR-based risk management. However, these methods belong to end-to-end black-box optimization, lacking necessary interpretability, and cannot integrate external signals such as news and fundamentals on demand. LLMs-Based Trading Agents TradingAgents [6] introduces multi-agent framework that forms trading decisions through the collaborative interaction of LLM agents simulating analysts, traders, risk controllers and other roles, with backtesting results outperforming multiple benchmark methods. FinAgent [7] combines multimodal information fusion with tool enhancement, achieving state-of-theart performance across six evaluation metrics. However, both approaches lack explicit coordination and constraint mechanisms, making debate-style decision processes yield inconsistent or conflicting signals, and high sensitivity to prompt design. Alpha-GPT [8] adopts human-in-the-loop paradigm that enables factor mining through natural-language interaction, but it is difficult to autonomously scale and automate in high-frequency trading environments. LLM-Based Reinforcement Learning Optimization Motivated by the recent success of DeepSeek-R1 [10], growing work explores RL approaches to optimize LLMs for quantitative trading. FLAG-Trader [15] employs partially fine-tuned LLMs as policy networks, optimizing trading rewards through policy gradient methods. Trading-R1 [16] constructs large-scale financial corpora and implements three-stage curriculum learning framework that combines SFT with RL. However, both types of methods generally lack end-to-end simulation of real trading processes and autonomous exploration capabilities and have not yet endowed models with spontaneous perception of information gaps or proactive orchestration of external tools. 2 Figure 1: The overall architecture and workflow of AlphaQuanter. The central panel shows the agents iterative rollout process. Starting from an initial state (S0), the agent first forms an initial plan (P0) before generating further reasoning traces (Ri+1) with <think> tag. In each step, it decides whether to continue acquiring information by executing tool-based action (Ai+1) and receiving its environmental feedback (Ei+1), or to conclude by outputting final decision with an <answer> tag. Throughout this process, the agent can query multi-dimensional financial data sources (bottom panel, Section 6.2). Once decision is made, the entire trajectory will be evaluated to compute reward (top-right panel, Section 4.2), which updates the agents policy. The overall workflow (top-left panel, Section 4.1) is designed to mimic human traders cognitive process of reasoning and acquiring data on demand."
        },
        {
            "title": "3 Problem Definition",
            "content": "To navigate partially observable market within single trading day, we model the agents task as tool-augmented Markov Decision Process (MDP), defined by the tuple S, A, , R. The central challenge within this framework is to learn strategy that sequences tool use and the final action to maximize return. State Space state captures the agents accumulated information, represented as the tuple = (initial_context, query_history, query_result), where initial_context includes basic metadata (e.g., stock symbol, date), query_history records the tools invoked so far, and query_result stores their corresponding outputs. Action Space The action space comprises two distinct types. First, the agent can execute query action from Aq = {f1, f2, , fAq} to actively gather information from four source categories (market data, fundamental indicators, sentiment analysis, and macroeconomic metrics), thereby updating its state. Detailed descriptions of the data sources are provided in Section 6.2. Finally, the agent can execute decision action from Ad = {BUY, SELL, HOLD}, which terminates the decisionmaking process. The state transitions are deterministic. When the agent selects query Transition Dynamics action at Aq at time step t, the current state st transitions to st+1 by appending the query and its result to query_history and query_results, respectively. Conversely, when the agent selects decision action at Ad, the episode terminates immediately. Reward Function An episode yields trajectory τ = (s0, a0, s1, a1, , sT , aT ), sequence of states and actions beginning at the initial state s0 and terminating with the first decision action aT , where all intermediate actions a0, a1, , aT 1 are query actions. The agents objective is to learn policy π that maximizes the cumulative trajectory reward R(τ ) = t=0 R(st, at). The step-wise reward function is designed to promote strategic and profitable decision-making: rewarding BUY when the outlook is positive, SELL when negative, and HOLD when conditions are neutral or non-directional, while guiding tool use toward informative queries."
        },
        {
            "title": "4.1 Cognitive Workflow",
            "content": "Inspired by the ReAct paradigm [9], AlphaQuanter interleaves reasoning traces with discrete actions, as illustrated in Figure 1. The workflow begins with an initial Plan generation, followed by an iterative loop with three stages: (i) identify an information gap and Acquire new evidence via tool call; (ii) Reason over the acquired evidence to update beliefs, and (iii) Act by either continuing the loop to gather more information or committing to trading decision. This design enforces stepwise hypothesis testing while keeping evidence collection tightly coupled to reasoning. The full prompt design is provided in Appendix B.1."
        },
        {
            "title": "4.2 Reward Formulation",
            "content": "Outcome Score To train robust, forward-looking agent under market noise, we encourage actions only on high-conviction signals, correctly classifying market states as strongly bullish, bearish, or neutral, while ignoring noise. We therefore smooth future returns by blending multiple horizons, akin to label smoothing [17, 18]. Specifically, we define the exponentially weighted forward return rt to filter short-lived fluctuations and emphasize the medium-term trajectory: rt = h=1 ωh ( pt+h+1 pt+1 1) , where pt is the asset price on day t, is the maximum horizon, ωh = ηh i=1 ηi is the normalized exponential weight, and η (0, 1) is the decay factor. Thresholding rt at θ yields the market regime, and we assign discrete rewards by action as specified in Table 1. / We adopt an asymmetric penalty scheme to provide more informative learning signal: taking the opposite side of strong trend (reward 1.0) is penalized more than failing to act on an opportunity (reward 0.75), nudging the policy toward risk-aware behavior consistent with professional trading practice. Table 1: Discrete reward structure for Rresult. Future Market State Highly Bullish (rt > θ) Highly Bearish (rt < θ) Sideways (rt θ) at = BUY +1.0 -1.0 -0. at = SELL -1.0 +1.0 -0.5 at = HOLD -0.75 -0.75 +1.0 Process Score The process score comprises format score Rformat and tool score Rtool. The format score regulates the length of the reasoning trace: given target token interval [mintoken, maxtoken], outputs outside this interval incur penalties, encouraging sufficiency without verbosity. The tool-use score governs acquisition efficiency by penalizing total number of tool calls outside [mintool, maxtool]. It further discourages the degenerate collect-then-conclude pattern, acquiring all data in single round and immediately producing final answer, which can cause training to collapse. In addition, malformed tool calls that violate the function signature (e.g., missing or incorrect arguments) incur additional penalties. In conclusion, we define the total reward = αRresult + Rformat + Rtool, where the hyperparameter α places greater emphasis on the outcome score, reflecting its primary importance."
        },
        {
            "title": "5.1 Backtesting Protocol",
            "content": "While the policy is optimized for day-to-day decisions, its ultimate value is determined by the strategys risk-adjusted performance over extended horizons [19]. To evaluate this, we conduct systematic backtesting simulations in which the daily-trained policy π is applied sequentially across historical period, generating series of trades from which we measure the resulting portfolio performance."
        },
        {
            "title": "5.2 Portfolio State and Transition Dynamics",
            "content": "Table 2: Backtesting Simulation Parameters Symbol Description pi hi ci Vi λ κ Closing price of the asset on day i. Number of shares held at the end of day i. Cash balance at the end of day i. Total portfolio value at the end of day i. (Vi = ci + hi pi) Transaction fee rate for BUY/SELL orders. Capital utilization ratio for BUY orders (slippage buffer). Table 3: State transition rules for different actions"
        },
        {
            "title": "Action",
            "content": "ht+1 ct+"
        },
        {
            "title": "HOLD",
            "content": "ht + κ ct pt+1 0 ht ct (1 + λ) κ ct pt+ pt+1 ct + (1 λ) ht pt+1 ct We define the core variables of our portfolio in Table 2. The transition from the portfolio state (ht, ct) to the new state (ht+1, ct+1) is determined by the action at, as summarized in Table 3."
        },
        {
            "title": "5.3 Evaluation Metrics",
            "content": "Following prior work [7, 20, 21], we employ three widely used portfolio-level metrics as follows: Annualized Rate of Return (ARR) measures profitability by annualizing total return: where V0 and VT are initial and final portfolio values, is trading days, 252 is annual trading days. Sharpe Ratio (SR) measures risk-adjusted performance: ARR = (VT /V0) 252/T 1 where rt = VtVt1 Vt1 risk-adjusted returns. , = 1 t=1 rt, and σr = SR = r/σr 1 1 t=1(rt r) 2. Higher SR indicates better Maximum Drawdown (MDD) measures the largest peak-to-trough decline: MDD = max 1tT ( max1st Vs Vt max1st Vs ) Lower MDD reflects better downside risk control."
        },
        {
            "title": "6.1 Dataset and Simulation Period",
            "content": "We focus on five large-cap, event-driven stocks: Alphabet Inc. (GOOGL); Microsoft Corporation (MSFT); Meta Platforms, Inc. (META); NVIDIA Corporation (NVDA); and Tesla, Inc. (TSLA). These firms operate in information-dense markets and generate rich, rapidly evolving signals that stress an agents capacity for iterative information gathering and analysis. Their volatility and frequent news shocks also create settings where dynamic planning outperforms fixed policies. To avoid look-ahead bias and mirror realistic research-to-deployment workflow, we partition the data chronologically into non-overlapping training, validation, and test sets, as shown in Table 4. Crucially, we insert an approximately 30-trading-day gap between successive sets to eliminate leakage from overlapping feature windows and to strengthen the test of out-of-sample generalization. 6."
        },
        {
            "title": "Information Sources",
            "content": "The tool integrates four primary data categories summarized below. More details are provided in Appendix A. Table 4: Dataset splits and trading-day counts."
        },
        {
            "title": "End Date",
            "content": "#Trading Days Market Data Daily price data (e.g., Open, High, Low, Close, Volume) for each stock, along with curated set of technical indicators grouped by function: trend (e.g., SMA, EMA), momentum (e.g., RSI, STOCH), volatility (e.g., BBANDS), and volume (e.g., OBV). These support technical analysis of market dynamics."
        },
        {
            "title": "Training\nValidation\nTest",
            "content": "2022-09-01 2024-05-15 2025-01-01 2024-03-30 2024-11-14 2025-06-30 395 128 122 5 Fundamental Data Financial data from corporate filings, including income statements, balance sheets, cash flow statements, insider trading activity, dividend history, and earnings estimates. These support the assessment of company intrinsic value. Sentiment Data Textual signals from financial news and social media platforms (e.g., Reddit) to quantify market sentiment and investor psychology. These signals capture short-horizon sentiment and narrative shifts. Macroeconomic Indicators Series characterizing the broader economy, including inflation (e.g., CPI), interest rates (e.g., federal funds rate), and industry activity (e.g., commodity prices). These indicators provide the macroeconomic context for asset pricing. 6."
        },
        {
            "title": "Implementation",
            "content": "Baselines We compare with four categories of baselines: (1) passive buy and hold strategy; (2) classic rule-based strategies, including MACD and ZMR [7]; (3) TradingAgent [6], state-of-the-art multi-agent LLM framework that generates trading decisions via debate-style reasoning without an explicit RL training phase; and (4) an ablated variant of AlphaQuanter that operates via zero-shot reasoning. For the LLM-based baselines, we use multiple backbone models, including Qwen2.53B-Instruct, Qwen2.5-7B-Instruct [22], Qwen3-30B-A3B-Instruct [23], DeepSeek-V3.1 [24], KimiK2 [25], GPT-4o-mini, and GPT-4o [26]. Training Details We train the AlphaQuanter agents using Qwen2.5-3B-Instruct and Qwen2.57B-Instruct [22] as backbones, with the verl framework [27], optimizing the policy with the GRPO algorithm [28]. At inference time, we use deterministic decoding (temperature = 0). For each configuration, we report the mean performance over three independent runs (distinct random seeds) to mitigate variance. All experiments are conducted on NVIDIA A100 GPUs (80GB). complete list of hyperparameters and training settings is provided in Appendix B."
        },
        {
            "title": "7.1 Overall Performance Comparison",
            "content": "To systematically evaluate current LLM-based trading paradigms and demonstrate the effectiveness of our approach, we address three research questions. Detailed metrics for all methods are summarized in Table 5. RQ1: Single or multi-agent, which is better? We compare single-agent and multi-agent frameworks across multiple LLM backbones. The results show that, except for GPT-4o, the single-agent framework consistently outperforms the multi-agent framework on key metrics, particularly ARR and SR. This supports our hypothesis that, for smaller-scale models, multi-agent debate can inject noise or amplify hallucinations rather than yield complementary insights, ultimately degrading performance. These results provide clear justification for adopting single-agent architecture in our approach. RQ2: Is prompt-based reasoning sufficient for trading decisions? We compare the strongest prompt-based baselines against the simple buy-and-hold strategy. On average, all backbones except GPT-4o fail to beat the market method. We attribute the underperformance to difficulty learning actionable decision boundaries. Although the models can infer bullish or bearish sentiment, the prompt-based baseline does not reliably calibrate the decision threshold at which signal should trigger BUY rather than HOLD. This exposes fundamental limitation of current small-scale LLMs for trading and indicates that prompt-only reasoning is insufficient; agents should be explicitly trained to map high-dimensional market states to optimal trading actions. RQ3: How effective is the AlphaQuanter? We compare the fully trained AlphaQuanter against all baselines. Both the 3B and 7B variants significantly outperform the strongest baseline, with absolute ARR gains of 6.54% and 18.45%, respectively. Moreover, the 7B model is notably consistent, outperforming all baselines on three of the five stocks. These results show that end-to-end RL training enables small LLMs to learn robust trading policies, including proactive tool use and information 6 Table 5: Backtesting performance comparison of different methods over 122-day backtesting period. For the ARR of each stock and the overall average, we mark the highest value in bold red and the second-highest in bold orange."
        },
        {
            "title": "Model",
            "content": "Market Buy and Hold Rule-Based MACD ZMR Multi-Agent Qwen2.5-3B-Instruct Qwen2.5-7B-Instruct Qwen3-30B-A3B-Instruct DeepSeek-V3.1 Kimi-K2 GPT-4o-mini GPT-4o Single-Agent Qwen2.5-3B-Instruct Qwen2.5-7B-Instruct Qwen3-30B-A3B-Instruct DeepSeek-V3.1 Kimi-K2 GPT-4o-mini GPT-4o AlphaQuanter (Ours) AlphaQuanter-3B AlphaQuanter-7B"
        },
        {
            "title": "GOOGL",
            "content": "ARR ()"
        },
        {
            "title": "META",
            "content": "ARR ()"
        },
        {
            "title": "MSFT",
            "content": "ARR ()"
        },
        {
            "title": "NVDA",
            "content": "ARR ()"
        },
        {
            "title": "TSLA",
            "content": "ARR ()"
        },
        {
            "title": "Average",
            "content": "ARR () SR () MDD () -14.49% 45.64% 36.80% 25.47% -28.91% 12.90% 0.57 31.13% -3.17% -2.26% 46.82% -0.98% -9.58% 8.53% -12.89% 35.01% 22.77% 16.74% 8.79% 11.41% 1.73% 9.33% -18.09% -12.43% -23.40% -18.08% -14.95% 3.06% -22.42% -26.33% -25.15% -40.48% -24.02% -9.01% 36.25% 28.98% 1.36% -9.48% -9.52% 0.73% 29.69% 23.08% 35.50% 32.86% 32.49% 25.83% 44.42% 57.18% 40.89% -4.50% 9.84% 14.13% 12.60% 16.27% 38.62% 5.10% 17.55% 37.45% 25.45% -3.39% 43.42% 19.39% -3.28% -17.22% 10.22% -24.02% -8.33% -5.38% -7.83% -7.43% 1.47% 29.61% 10.30% -3.27% 13.61% 17.60% -76.98% -9.11% -16.51% 0.00% 8.88% 5.20% 36.92% -32.21% -9.63% -46.41% -1.21% 13.05% -43.71% -38.04% -0.28% 1.50% -2.64% -6.36% -3.95% -0.25% 16.49% -1.68% 4.49% 5.44% 8.38% -1.65% 6.74% 9.42% -14.68% -2.52% 56.15% 41.91% 9.82% 47.23% 30.55% 45.41% 33.33% 42.67% 23.03% 34.94% 0.44 0. -0.13 -0.08 0.06 -0.26 -0.11 -0.06 0.50 0.08 0.16 0.12 0.24 0.15 0.25 0.25 0.43 0.65 21.24% 20.86% 20.95% 6.43% 22.20% 12.49% 26.62% 18.28% 21.82% 25.99% 28.96% 30.08% 30.70% 25.30% 26.78% 28.27% 25.16% 24.93% seeking, that even surpass powerful zero-shot setting such as GPT-4o. In short, the evidence indicates that specialized training paradigms may be more critical than model scale for achieving state-of-the-art performance in automated trading systems."
        },
        {
            "title": "7.2 Training Dynamics and Validation Performance",
            "content": "To capture the training process comprehensively, we track (1) training dynamics (Figure 2), reflecting reward signals and behavioral patterns during learning; and (2) validation performance (Figure 3), evaluating trading performance on unseen data. This joint analysis reveals how the agents evolving behaviors translate into practical outcomes. Figure 2: Comparison of training dynamics for the AlphaQuanter-3B and -7B models. Training Dynamics The upward reward trajectory demonstrates effective learning from market interactions. However, the remaining metrics reveal markedly divergent learning trajectories between the two models. Both exhibit an initial volatile exploration phase, but their subsequent paths differ substantially. The 3B model transitions into simplistic exploitation phase characterized by fewer tool calls and decreasing response length, suggesting premature convergence to less robust policy. In contrast, the 7B model achieves stable exploitation around step 200 and subsequently enters 7 policy refinement phase, evidenced by increased response length and dialogue turns. This pattern indicates that the larger model explores more sophisticated reasoning chains and information-seeking strategies to extract marginal performance gains. Figure 3: Comparison of key backtesting metrics for the AlphaQuanter-3B and -7B models on the validation set. Validation Performance The validation metrics confirm the successful generalization of the learned policies to unseen data. For both 3B and 7B, ARR and SR exhibit clear upward trends that closely mirror the training reward curves. Notably, the 7B model shows downward trend in MDD, indicating it has learned not only to maximize returns but also to effectively manage downside risk. Conversely, the 3B models MDD oscillates with an upward bias, revealing its failure to internalize risk-aware trading behavior despite improving returns."
        },
        {
            "title": "7.3 Tool Usage Patterns",
            "content": "Policy Evolution To fully understand how AlphaQuanter achieves its result and validate the interpretability of its strategies, we analyze the agents underlying decision-making process. The heatmaps in Figure 4 show that tool usage for both the 3B and 7B models is dynamic rather than static, evolving over the course of training. This confirms that the agents actively learn and refine their information-seeking policies instead of relying on fixed routine. Divergent Strategies: 3B vs. 7B The two models exhibit divergent learned strategies. The 3B model exhibits diffuse, low-contrast usage pattern across tools, suggesting limited ability to distinguish informative from uninformative signals. In contrast, the 7B model develops concentrated, high-contrast pattern, consistent with selective, discriminative policy for filtering and prioritizing information. Expert-like Heuristic Closer examination of the 7B models learned policy reveals sophisticated, expert-level heuristic. It learns to rely heavily on trend, momentum, and volume indicators as primary signals, while treating sentiment and macroeconomic context as secondary but important inputs for decision-making. At the same time, it largely downweights low-frequency fundamental data, likely because such signals add limited value to the rapid decisions required by the task. Figure 4: Evolution of the tool selection strategies for the AlphaQuanter-3B and -7B models during training. The heatmap color intensity shows the percentile-based reliance on each information at different training steps. The symbols [M], [S], [X], and [F] represent the four categories of data sources, respectively."
        },
        {
            "title": "7.4 Ablation Studies",
            "content": "Table 6: the threshold θ on the performance of AlphaQuanter-7B model."
        },
        {
            "title": "Impact of reward components and\nthe",
            "content": "Model AlphaQuanter-7B w/o Rformat w/o Rtool θ 0.005 θ 0.005 ARR () 34.94% 16.36% (53.2%) 19.90% (43.0%) 21.25% (39.2%) 20.23% (42.1%) SR () MDD () 0.65 0.40 0.49 0.28 0. 24.93% 26.49% 24.08% 9.18% 32.67% Figure 5: The effect of different decision threshold (θ) values on the agents action distribution during training. We conduct an ablation study to validate the contributions of our key designs, with all results shown in Table 6. First, we evaluate the effectiveness of the process score by selectively removing the format score Rformat and the tool score Rtool. Their removal causes the average ARR to drop by 53.2% and 43.0%, respectively, confirming their critical roles in guiding the agent toward an effective and structured decision-making process. Next, we evaluate the sensitivity of the decision threshold θ. Perturbing θ by 0.005 yields substantial ARR reductions of 42.1% and 39.2%. We also observe distinct trade-off with MDD: larger θ induces more HOLD signals, lowering trading frequency and MDD, whereas smaller θ increases both activity and risk. As shown in Figure 5, θ is crucial for balancing exploration against exploitation; an improperly calibrated value causes the agent to converge on single action (e.g., only BUY or HOLD), whereas our setting maintains dynamic, adaptive policy in noisy financial environments."
        },
        {
            "title": "8 Conclusion",
            "content": "In this paper, we present AlphaQuanter, single-agent framework that leverages RL to directly optimize the entire decision-making process. By training the agent to learn dynamic policy over transparent, tool-augmented workflow, AlphaQuanter treats information acquisition as strategic action, enabling it to adaptively reason and strategically use tools to maximize long-term profitability. Our work demonstrates that optimizing the decision-making process itself, not just final prediction, is crucial step toward building more robust and intelligent automated trading systems. Future work will generalize AlphaQuanter to interact with more adaptive tools in dynamic markets like real-time search, and ground its learning through long-horizon evaluation."
        },
        {
            "title": "References",
            "content": "[1] David Rumelhart, Geoffrey Hinton, and Ronald Williams. Learning representations by back-propagating errors. nature, 323(6088):533536, 1986. [2] Corinna Cortes and Vladimir Vapnik. Support-vector networks. Machine learning, 20(3): 273297, 1995. [3] Leo Breiman. Random forests. Machine learning, 45(1):532, 2001. [4] John Moody and Matthew Saffell. Reinforcement learning for trading. In M. Kearns, S. Solla, and D. Cohn, editors, Advances in Neural Information Processing Systems, volume 11. MIT Press, 1998. URL https://proceedings.neurips.cc/paper_files/paper/1998/ file/4e6cd95227cb0c280e99a195be5f6615-Paper.pdf. [5] Zhicheng Wang, Biwei Huang, Shikui Tu, Kun Zhang, and Lei Xu. Deeptrader: deep reinforcement learning approach for risk-return balanced portfolio management with market conditions embedding. In Proceedings of the AAAI conference on artificial intelligence, volume 35, pages 643650, 2021. [6] Yijia Xiao, Edward Sun, Di Luo, and Wei Wang. Tradingagents: Multi-agents llm financial trading framework. arXiv preprint arXiv:2412.20138, 2024. [7] Wentao Zhang, Lingxuan Zhao, Haochong Xia, Shuo Sun, Jiaze Sun, Molei Qin, Xinyi Li, Yuqing Zhao, Yilei Zhao, Xinyu Cai, Longtao Zheng, Xinrun Wang, and Bo An. multimodal foundation agent for financial trading: Tool-augmented, diversified, and generalist. In Ricardo Baeza-Yates and Francesco Bonchi, editors, Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD 2024, Barcelona, Spain, August 25-29, 2024, pages 43144325. ACM, 2024. doi: 10.1145/3637528.3671801. URL https://doi.org/10. 1145/3637528.3671801. [8] Saizhuo Wang, Hang Yuan, Leon Zhou, Lionel Ni, Heung-Yeung Shum, and Jian Guo. Alpha-gpt: Human-ai interactive alpha mining for quantitative investment. arXiv preprint arXiv:2308.00016, 2023. [9] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R. Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. URL https://openreview.net/forum?id=WE_vluYUL-X. [10] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [11] Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, et al. Tulu 3: Pushing frontiers in open language model post-training. arXiv preprint arXiv:2411.15124, 2024. [12] Lin Zhong. Advancements and applications of artificial intelligence in stock market prediction. 2025. [13] Yang Liu, Qi Liu, Hongke Zhao, Zhen Pan, and Chuanren Liu. Adaptive quantitative trading: An imitative deep reinforcement learning approach. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 21282135, 2020. [14] Fengchen Gu, Zhengyong Jiang, Ángel F. García-Fernández, Angelos Stefanidis, Jionglong Su, and Huakang Li. MTS: deep reinforcement learning portfolio management framework with time-awareness and short-selling. CoRR, abs/2503.04143, 2025. doi: 10.48550/ARXIV.2503. 04143. URL https://doi.org/10.48550/arXiv.2503.04143. [15] Guojun Xiong, Zhiyang Deng, Keyi Wang, Yupeng Cao, Haohang Li, Yangyang Yu, Xueqing Peng, Mingquan Lin, Kaleb Smith, Xiao-Yang Liu, et al. Flag-trader: Fusion llm-agent with gradient-based reinforcement learning for financial trading. arXiv preprint arXiv:2502.11433, 2025. [16] Yijia Xiao, Edward Sun, Tong Chen, Fang Wu, Di Luo, and Wei Wang. Trading-r1: Financial trading with llm reasoning via reinforcement learning. arXiv preprint arXiv:2509.11420, 2025. [17] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. In 2016 IEEE Conference on Rethinking the inception architecture for computer vision. Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016, pages 28182826. IEEE Computer Society, 2016. doi: 10.1109/CVPR.2016.308. URL https://doi.org/10.1109/CVPR.2016.308. [18] Xiao-Yang Liu, Hongyang Yang, Qian Chen, Runjia Zhang, Liuqing Yang, Bowen Xiao, and Christina Dan Wang. Finrl: deep reinforcement learning library for automated stock trading in quantitative finance. CoRR, abs/2011.09607, 2020. URL https://arxiv.org/abs/2011.09607. [19] Harry Markowitz. Portfolio selection. The Journal of Finance, 7(1):7791, 1952."
        },
        {
            "title": "ISSN",
            "content": "00221082, 15406261. URL http://www.jstor.org/stable/2975974. [20] Molei Qin, Shuo Sun, Wentao Zhang, Haochong Xia, Xinrun Wang, and Bo An. Earnhft: Efficient hierarchical reinforcement learning for high frequency trading. In Michael J. Wooldridge, Jennifer G. Dy, and Sriraam Natarajan, editors, Thirty-Eighth AAAI Conference on Artificial Intelligence, AAAI 2024, Thirty-Sixth Conference on Innovative Applications of Artificial Intelligence, IAAI 2024, Fourteenth Symposium on Educational Advances in Artificial Intelligence, EAAI 2014, February 20-27, 2024, Vancouver, Canada, pages 1466914676. AAAI Press, 2024. doi: 10.1609/AAAI.V38I13.29384. URL https://doi.org/10.1609/aaai.v38i13.29384. [21] Guojun Xiong, Zhiyang Deng, Keyi Wang, Yupeng Cao, Haohang Li, Yangyang Yu, Xueqing Peng, Mingquan Lin, Kaleb E. Smith, Xiao-Yang Liu, Jimin Huang, Sophia Ananiadou, and Qianqian Xie. FLAG-TRADER: fusion llm-agent with gradient-based reinforcement In Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and learning for financial trading. 10 Mohammad Taher Pilehvar, editors, Findings of the Association for Computational Linguistics, ACL 2025, Vienna, Austria, July 27 - August 1, 2025, pages 1392113934. Association for Computational Linguistics, 2025. URL https://aclanthology.org/2025.findings-acl.716/. [22] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report. CoRR, abs/2412.15115, 2024. doi: 10.48550/ARXIV.2412.15115. URL https://doi.org/10.48550/ arXiv.2412.15115. [23] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jian Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, and Zihan Qiu. Qwen3 technical report. CoRR, abs/2505.09388, 2025. doi: 10.48550/ARXIV.2505.09388. URL https://doi.org/10.48550/arXiv.2505.09388. [24] DeepSeek-AI, Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Daya Guo, Dejian Yang, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Haowei Zhang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Li, Hui Qu, J. L. Cai, Jian Liang, Jianzhong Guo, Jiaqi Ni, Jiashi Li, Jiawei Wang, Jin Chen, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, Junxiao Song, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Lei Xu, Leyi Xia, Liang Zhao, Litong Wang, Liyue Zhang, Meng Li, Miaojun Wang, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Mingming Li, Ning Tian, Panpan Huang, Peiyi Wang, Peng Zhang, Qiancheng Wang, Qihao Zhu, Qinyu Chen, Qiushi Du, R. J. Chen, R. L. Jin, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, Runxin Xu, Ruoyu Zhang, Ruyi Chen, S. S. Li, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shaoqing Wu, Shengfeng Ye, Shirong Ma, Shiyu Wang, Shuang Zhou, Shuiping Yu, Shunfeng Zhou, Shuting Pan, T. Wang, Tao Yun, Tian Pei, Tianyu Sun, W. L. Xiao, and Wangding Zeng. Deepseek-v3 technical report. CoRR, abs/2412.19437, 2024. doi: 10.48550/ARXIV.2412.19437. URL https://doi.org/10.48550/arXiv.2412.19437. [25] Yifan Bai, Yiping Bao, Guanduo Chen, Jiahao Chen, Ningxin Chen, Ruijue Chen, Yanru Chen, Yuankun Chen, Yutian Chen, Zhuofu Chen, Jialei Cui, Hao Ding, Mengnan Dong, Angang Du, Chenzhuang Du, Dikang Du, Yulun Du, Yu Fan, Yichen Feng, Kelin Fu, Bofei Gao, Hongcheng Gao, Peizhong Gao, Tong Gao, Xinran Gu, Longyu Guan, Haiqing Guo, Jianhang Guo, Hao Hu, Xiaoru Hao, Tianhong He, Weiran He, Wenyang He, Chao Hong, Yangyang Hu, Zhenxing Hu, Weixiao Huang, Zhiqi Huang, Zihao Huang, Tao Jiang, Zhejun Jiang, Xinyi Jin, Yongsheng Kang, Guokun Lai, Cheng Li, Fang Li, Haoyang Li, Ming Li, Wentao Li, Yanhao Li, Yiwei Li, Zhaowei Li, Zheming Li, Hongzhan Lin, Xiaohan Lin, Zongyu Lin, Chengyin Liu, Chenyu Liu, Hongzhang Liu, Jingyuan Liu, Junqi Liu, Liang Liu, Shaowei Liu, T. Y. Liu, Tianwei Liu, Weizhou Liu, Yangyang Liu, Yibo Liu, Yiping Liu, Yue Liu, Zhengying Liu, Enzhe Lu, Lijun Lu, Shengling Ma, Xinyu Ma, Yingwei Ma, Shaoguang Mao, Jie Mei, Xin Men, Yibo Miao, Siyuan Pan, Yebo Peng, Ruoyu Qin, Bowen Qu, Zeyu Shang, Lidong Shi, Shengyuan Shi, Feifan Song, Jianlin Su, Zhengyuan Su, Xinjie Sun, Flood Sung, Heyi Tang, Jiawen Tao, Qifeng Teng, Chensi Wang, Dinglu Wang, Feng Wang, and Haiming Wang. Kimi K2: open agentic intelligence. CoRR, abs/2507.20534, 2025. doi: 10.48550/ARXIV.2507.20534. URL https://doi.org/10.48550/arXiv.2507.20534. [26] Aaron Hurst, Adam Lerer, Adam P. Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, Aleksander Madry, Alex BakerWhitcomb, Alex Beutel, Alex Borzunov, Alex Carney, Alex Chow, Alex Kirillov, Alex Nichol, Alex Paino, Alex Renzin, Alex Tachard Passos, Alexander Kirillov, Alexi Christakis, Alexis 11 Conneau, Ali Kamali, Allan Jabri, Allison Moyer, Allison Tam, Amadou Crookes, Amin Tootoonchian, Ananya Kumar, Andrea Vallone, Andrej Karpathy, Andrew Braunstein, Andrew Cann, Andrew Codispoti, Andrew Galu, Andrew Kondrich, Andrew Tulloch, Andrey Mishchenko, Angela Baek, Angela Jiang, Antoine Pelisse, Antonia Woodford, Anuj Gosalia, Arka Dhar, Ashley Pantuliano, Avi Nayak, Avital Oliver, Barret Zoph, Behrooz Ghorbani, Ben Leimberger, Ben Rossen, Ben Sokolowsky, Ben Wang, Benjamin Zweig, Beth Hoover, Blake Samic, Bob McGrew, Bobby Spero, Bogo Giertler, Bowen Cheng, Brad Lightcap, Brandon Walkin, Brendan Quinn, Brian Guarraci, Brian Hsu, Bright Kellogg, Brydon Eastman, Camillo Lugaresi, Carroll L. Wainwright, Cary Bassin, Cary Hudson, Casey Chu, Chad Nelson, Chak Li, Chan Jun Shern, Channing Conger, Charlotte Barette, Chelsea Voss, Chen Ding, Cheng Lu, Chong Zhang, Chris Beaumont, Chris Hallacy, Chris Koch, Christian Gibson, Christina Kim, Christine Choi, Christine McLeavey, Christopher Hesse, Claudia Fischer, Clemens Winter, Coley Czarnecki, Colin Jarvis, Colin Wei, Constantin Koumouzelis, and Dane Sherburn. Gpt-4o system card. CoRR, abs/2410.21276, 2024. doi: 10.48550/ARXIV.2410.21276. URL https://doi.org/10.48550/arXiv.2410.21276. [27] Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient RLHF framework. In Proceedings of the Twentieth European Conference on Computer Systems, EuroSys 2025, Rotterdam, The Netherlands, 30 March 2025 - 3 April 2025, pages 12791297. ACM, 2025. doi: 10.1145/3689031.3696075. URL https://doi.org/10.1145/3689031.3696075. [28] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024."
        },
        {
            "title": "A Detailed Information Sources",
            "content": "A.1 Market Data Market data consists of two tiers: raw price/volume, and curated set of popular technical indicators. This selection is carefully designed to offer the agent comprehensive and non-redundant toolkit for analyzing market dynamics. These data are extracted via API from Yahoo Finance1 and Alpha Vantage2. A.1.1 Price and Volume Data This data consists of the daily Open, High, Low, and Close (OHLC) prices, the Adjusted Close price, and Volume for each stock, which represents the fundamental record of an assets trading activity for given day. A.1.2 Technical Indicators To facilitate deeper analysis of the raw price data, we allow the agent to query set of the most widely used technical indicators, which we group by their analytical function: Trend Indicators These help identify the direction and strength of price trend. SMA(20): 20-day Simple Moving Average of the price. EMA(10): 10-day Exponential Moving Average, giving more weight to recent prices. VWMA(20): 20-day Volume-Weighted Moving Average, emphasizing periods with higher trading volume. Momentum Indicators These measure the speed and change of price movements to identify overbought or oversold conditions. RSI(14): The 14-day Relative Strength Index. STOCH(14, 3, 3): The Stochastic Oscillator with the parameters defining its calculation: 14 sets the look-back period for the high-low price range, the first 3 is the smoothing period for the main oscillator line (%K), and the second 3 is the moving average period for its signal line (%D). CCI(21): The 21-day Commodity Channel Index. Volatility Indicators These quantify the magnitude of price fluctuations. BBANDS(20, 2): Bollinger Bands. The parameter 20 sets the period for the SMA that forms the middle band. The 2 specifies that the upper and lower bands are plotted at two standard deviations above and below this middle band, respectively. ATR(14): The 14-day Average True Range, measure of market volatility. Volume Indicators These use trading volume to confirm trends or signal potential reversals. OBV: On Balance Volume, which relates price changes to volume. CMF: The Chaikin Money Flow, which measures money flow volume over period."
        },
        {
            "title": "Hybrid Indicator",
            "content": "MACD(12, 26, 9): The Moving Average Convergence Divergence, calculated by subtracting the 26-period EMA from the 12-period EMA. The 9 refers to 9-period EMA of the MACD line itself, which serves as signal line to generate trading triggers. A.2 Fundamental Data To ground the agents reasoning in companys intrinsic financial health and valuation, we integrated several categories of fundamental data extracted via Alpha Vantage API. These sources provide 1https://developer.yahoo.com/api/ 2https://www.alphavantage.co/documentation/ 13 holistic view, covering core financial statements, forward-looking analyst expectations, and significant corporate events. The specific data types are detailed below: Earnings Estimates: Forward-looking analyst projections, including annual and quarterly estimates for Earnings Per Share (EPS) and revenue. This dataset also provides metadata such as the number of contributing analysts and their revision histories. Income Statement: Annual and quarterly income statements detailing companys revenues, expenses, and profitability. Balance Sheet: Annual and quarterly balance sheets providing snapshot of companys assets, liabilities, and shareholders equity. Cash Flow: Annual and quarterly cash flow statements that report the flow of cash from operating, investing, and financing activities, normalized to standard accounting principles. Insider Transactions: Data on historical and recent transactions of company stock executed by key stakeholders, such as executives and board members, which can serve as signal of internal sentiment. Dividends: record of historical dividend payments and future declared distributions, offering insight into companys policy on returning capital to shareholders. A.3 Sentiment Data We incorporated two sources of sentiment data: news and Reddit. For news data, we source headlines, summaries, and associated sentiment scores for each stock from the Alpha Vantage API. For Reddit data, we retrieve the most relevant submissions from publicly available Reddit data dump3, focusing on content from the 11 most popular stock-trading subreddits (e.g., wallstreetbets, stocks, Daytrading, etc.). To manage the input context length, the content of each original post was then summarized using the Qwen3-30B-A3B-Instruct model [23]. A.4 Macroeconomic Indicators We integrate set of key macroeconomic indicators extracted from the Alpha Vantage API. These indicators provide context on monetary policy, inflation, and the health of the real economy. The specific data sources are as follows: Treasury Yield: Data on the yields of U.S. Treasury securities across maturities, considered as benchmark for risk-free interest rates and future economic growth expectations. Federal Funds Rate: The target interest rate set by the U.S. Federal Reserve, provided every month. This is primary driver of monetary policy and affects borrowing costs throughout the economy. Consumer Price Index (CPI): Monthly data reflecting the average change in prices paid by consumers for basket of goods and services, serving as primary measure of inflation. WTI Crude Oil Price: The spot price of West Texas Intermediate crude oil. It reflects global energy prices, supply-demand dynamics, and inflationary pressures. Copper Price: The spot price of copper, critical industrial metal often considered leading indicator of global economic health and manufacturing activity."
        },
        {
            "title": "B Implementation Details",
            "content": "B.1 Prompt Design We design the prompt to promote flexible, evidence-driven exploration rather than predetermined outputs. To achieve this, we first provide clear task description, the specific target stock and date, and available tools, with constraints on the maximum number of tool calls to ensure efficiency. Then, we enforce the designed workflow by instructing the agent to form and test hypotheses, call only one tool at time, and clearly show its thinking process within structured format before each action. The complete prompt is provided in the Figure 6. 3https://academictorrents.com/details/ba051999301b109eab37d16f027b3f49ade2de13 14 Your goal is to generate You are professional trading strategy analyst. well-reasoned final trade decision (BUY/SELL/HOLD) for given stock and date through systematic, evidence-based exploration using all available tools. At most 8 tool calls. You have access to the following tools use them intentionally and iteratively to test hypotheses and deepen your analysis: - [MUST] get_market_data (historical OHLCV) - [MUST] get_stock_indicators (trend indicators(SMA20, EMA10, VWMA20), momentum (RSI, STOCH, CCI), volatility (BBANDS, ATR), and volume-based (OBV, CMF), and hybrid(MACD)) - [OPTIONAL] get_news_data - [OPTIONAL] get_reddit_data - [OPTIONAL] get_macro_indicators - [OPTIONAL] get_balance_sheet - [OPTIONAL] get_cashflow - [OPTIONAL] get_income_statements - [OPTIONAL] get_insider_transactions - [OPTIONAL] get_dividends - [OPTIONAL] get_earnings_estimate GUIDELINES: ## Think Like an Analyst, Not Script. Approach the problem creatively. Use your reasoning to form hypotheses, then leverage tools flexibly to explore, validate, or refute your ideas. Be curious and iterative. There is no single fixed workflow. ## Start with High-Level Hypothesis. Begin by outlining your initial perspective and what you aim to investigate. This Youre encouraged to adapt as new isnt rigid plan-its starting point. evidence emerges. Before calling any tool, briefly state your current ## Plan, Execute, Then Analyze in the format: <think> ... </think> - First, Briefly Plan: hypothesis or what you aim to learn with the next step. - Then, Call One Tool: Execute only one tool call per step. You must wait for and receive the result before proceeding. - Finally, Analyze and Adapt: Interpret the result. Does it confirm your hypothesis? Does it reveal something new? Use this insight to refine your next step. ## One Step at Time. You are strictly permitted to make only one tool call at time. The subsequent analysis and planning must be based on the returned result before any further tool is called. This ensures deliberate and evidence-driven investigative process. ## Conclude with Decision. After synthesizing all evidence, provide clear and justified trade recommendation in the format: <answer>BUY SELL HOLD</answer> - Current date: {date} - Target stock ticker: {stock} Figure 6: Full prompt for the AlphaQuanter agent. B.2 Hyperparameters for RL Training We train AlphaQuanter using verl [27]. In Table 7, we list the important parameter settings for the verl framework as well as the hyperparameters referenced in this paper. 15 Table 7: Hyperparameters for training AlphaQuanter. Key algorithm.use_kl_in_reward actor_rollout_ref.actor.clip_ratio_low actor_rollout_ref.actor.clip_ratio_high actor_rollout_ref.actor.clip_ratio_c actor_rollout_ref.actor.entropy_coeff actor_rollout_ref.actor.kl_loss_coef actor_rollout_ref.actor.optim.lr actor_rollout_ref.actor.use_kl_loss actor_rollout_ref.rollout.multi_turn.max_user_turns actor_rollout_ref.rollout.multi_turn.max_assistant_turns actor_rollout_ref.rollout.n algorithm.kl_ctrl.kl_coef data.max_prompt_length data.max_response_length data.train_batch_size λ κ θ α mintoken maxtoken mintool maxtool Value false 0.1 0.1 3 0 0.05 1e-6 true 32 32 16 0.0 3072 16384 32 7 0.001 0.9 0.015 5 200 600 4 8 B.3 Detail of Baseline For the buy-and-hold strategy, we implement it by having the agent generate BUY signal on every trading day. For rule-based strategy, we implement two standard baselines. The first is Moving Average Convergence Divergence (MACD), trend-following strategy that uses indicator crossovers to generate trading signals; we employ the standard (12, 26, 9) parameterization for the fast, slow, and signal periods. The second baseline is Z-score Mean Reversion (ZMR), which assumes price reversion to historical mean. We enters trade when the Z-score (calculated over 20-period lookback) exceeds threshold of 1.0 and exits upon reversion to the mean (Z-score = 0). In addition to the classic quantitative strategies, we establish two LLM-based baselines for comparison. For the multi-agent baseline, we adapt the framework from Xiao et al. [6]. In our implementation, we replace the original data sources with our four designated categories of financial data, while retaining the prompts and agent architecture as specified in the original paper. For the single-agent baseline, we design configuration that utilizes our custom prompt structure, as shown in Figure 6, in conjunction with the same four data categories. This baseline serves to isolate the performance of single agent with full informational access but without the RL-optimized workflow of AlphaQuanter."
        },
        {
            "title": "C Detailed Result Analysis",
            "content": "C.1 Full Results of Main Table Table 8 and Table 9 present the complete backtesting results, providing detailed breakdown of the ARR, SR, and MDD for each individual stock summarized in Table 5. Our asset-specific analysis reveals several key findings. For GOOGL, most models struggle to generate positive returns, although few baseline methods achieve marginal gains. For META, the majority of strategies are profitable. Notably, the single-agent version of GPT-4o achieved the highest ARR, result matched by AlphaQuanter-3B, which does so with superior risk profile, evidenced by higher SR and lower MDD. On MSFT, AlphaQuanter-7B delivers the highest ARR, concurrently achieving strong SR and relatively low MDD. For NVDA, the results are mixed, with returns split between positive and negative. We observe that multi-agent methods are more prone to negative returns, whereas single-agent approaches more frequently yield positive returns with SRs greater than zero, although with high MDD. Here, AlphaQuanter-7B again secure the highest ARR, with its SR and MDD being comparable to the market baseline. For TSLA, the performance is similarly divided. It is particularly noteworthy that DeepSeek-V3.1 consistently outputs HOLD signal, resulting in zero values for all metrics. This behavior empirically validates our earlier assertion that prompting-based models struggle to differentiate between BUY and HOLD signals under uncertainty. AlphaQuanter-7B once again achieves the highest ARR with satisfactory SR and MDD. 16 Table 8: Performance comparison of different methods over 122-day backtesting period (1/2): detailed results for [GOOGL, META, MSFT]."
        },
        {
            "title": "Model",
            "content": "Market Buy and Hold Rule-Based MACD ZMR Multi-Agent Qwen2.5-3B-Instruct Qwen2.5-7B-Instruct Qwen3-30B-A3B-Instruct DeepSeek-V3.1 Kimi-K2 GPT-4o-mini GPT-4o Single-Agent Qwen2.5-3B-Instruct Qwen2.5-7B-Instruct Qwen3-30B-A3B-Instruct DeepSeek-V3.1 Kimi-K2 GPT-4o-mini GPT-4o AlphaQuanter (Ours) AlphaQuanter-3B AlphaQuanter-7B"
        },
        {
            "title": "MSFT",
            "content": "ARR () SR () MDD () ARR () SR () MDD () ARR () SR () MDD () -14.49% -0.35 27.35% 45.64% 1.25 31.59% 36.80% 1.41 18.79% -3.17% -2.26% 1.73% 9.33% -18.09% -12.43% -23.40% -18.08% -14.95% 3.06% -22.42% -26.33% -25.15% -40.48% -24.02% -9.01% -14.68% -2.52% -0.04 0.01 0.1 1.38 -0.46 -0.66 -1.09 -0.94 -0. 0.07 -0.43 -0.5 -0.47 -0.39 -0.56 -0.12 -0.29 0.05 14.14% 18.47% 46.82% -0.98% 5.52% 1.40% 26.36% 12.01% 17.57% 18.86% 25.93% 18.18% 28.59% 28.39% 29.77% 24.67% 23.20% 19.72% 36.25% 28.98% 1.36% -9.48% -9.52% 0.73% 29.69% 23.08% 35.50% 32.86% 32.49% 25.83% 44.42% 57.18% 25.60% 21.37% 56.15% 41.91% 2.17 0.12 0.85 0.87 0.29 -0.25 -0.1 0.04 0. 0.52 0.56 0.81 0.61 0.68 0.97 0.99 1.08 0.78 12.51% 15.19% -9.58% 8.53% 15.28% 6.54% 16.29% 17.18% 16.12% 11.11% 14.05% 24.91% 28.49% 28.18% 34.14% 21.65% 23.84% 25.02% 40.89% -4.50% 9.84% 14.13% 12.60% 16.27% 38.62% 5.10% 17.55% 37.45% 25.45% -3.39% 43.42% 19.39% 23.75% 25.65% 9.82% 47.23% -0.49 0.56 1.06 -1.05 0.42 0.6 0.51 0.48 0. 0.14 0.48 0.87 0.64 -0.03 1.1 0.53 0.30 1.17 19.97% 9.59% 12.23% 2.27% 15.88% 10.09% 9.11% 18.52% 19.83% 14.66% 19.60% 21.15% 19.94% 19.21% 12.92% 23.04% 21.06% 14.85% Table 9: Performance comparison of different methods over 122-day backtesting period (2/2): detailed results for [NVDA, TSLA] and average."
        },
        {
            "title": "Model",
            "content": "Market Buy and Hold Rule-Based MACD ZMR Multi-Agent Qwen2.5-3B-Instruct Qwen2.5-7B-Instruct Qwen3-30B-A3B-Instruct DeepSeek-V3.1 Kimi-K2 GPT-4o-mini GPT-4o Single-Agent Qwen2.5-3B-Instruct Qwen2.5-7B-Instruct Qwen3-30B-A3B-Instruct DeepSeek-V3.1 Kimi-K2 GPT-4o-mini GPT-4o AlphaQuanter (Ours) AlphaQuanter-3B AlphaQuanter-7B"
        },
        {
            "title": "Average",
            "content": "ARR () SR () MDD () ARR () SR () MDD () ARR () SR () MDD () 25.47% 0.74 33.83% -28.91% -0.2 44.10% 12.90% 0.57 31.13% -12.89% 35.01% -3.28% -17.22% 10.22% -24.02% -8.33% -5.38% -7.83% -7.43% 1.47% 29.61% 10.30% -3.27% 13.61% 17.60% 30.55% 45.41% -0.22 1.03 -0.06 -0.99 0.31 -0.97 -0.28 0.01 0. 0.14 0.22 0.51 0.31 0.11 0.35 0.39 0.51 0.66 30.76% 16.72% 22.77% 16.74% 18.77% 14.12% 23.78% 23.18% 18.88% 36.61% 38.74% 34.63% 40.24% 33.48% 39.81% 34.92% 37.60% 38.53% -76.98% -9.11% -16.51% 0.00% 8.88% 5.20% 36.92% -32.21% -9.63% -46.41% -1.21% 13.05% -43.71% -38.04% 29.04% 34.91% 33.33% 42.67% 0.78 0.59 -2.6 -0.59 -0.25 0.0 0.4 0.1 1. -0.46 -0.04 -1.08 0.13 0.36 -0.59 -0.54 0.57 0.58 28.83% 44.33% 8.79% 11.41% 52.95% 7.82% 28.71% 0.00% 71.40% 6.30% 10.56% 37.59% 27.88% 39.22% 29.82% 26.05% 36.32% 35.06% -0.28% 1.50% -2.64% -6.36% -3.95% -0.25% 16.49% -1.68% 4.49% 5.44% 8.38% -1.65% 6.74% 9.42% 26.34% 27.88% 23.03% 34.94% 0.44 0.46 -0.13 -0.08 0.06 -0.26 -0.11 -0.06 0. 0.08 0.16 0.12 0.24 0.15 0.25 0.25 0.43 0.65 21.24% 20.86% 20.95% 6.43% 22.20% 12.49% 26.62% 18.28% 21.82% 25.99% 28.96% 30.08% 30.70% 25.30% 26.78% 28.27% 25.16% 24.93% C.2 Reward Decomposition Analysis To complement the analysis in Section 7.2, Figure 7 displays the learning curves for the primary reward and its constituent components, the result, format, and tool scores, on the validation set during training. key observation is that the 7B model consistently outperforms the 3B model across all scoring metrics. The result score exhibits clear upward trend for both models, indicating steady improvement in the accuracy of the agents final actions. The rate of improvement gradually 17 Figure 7: comparative analysis of the training dynamics for the AlphaQuanter-3B and -7B models, illustrating the evolution of the total reward and its score components. decelerates as the models converge. Regarding the format score, which reflects the length of the agents reasoning trace, both models initially show an increase. However, after approximately 250 steps, their paths become different: the 7B model continues to generate more detailed reasoning, while the 3B models reasoning length begins to decrease, leading to decline in its format score. For the tool score, the 3B model initially performs poorly and incurs significant penalties. case study of its rollouts reveals that in the early stages, the 3B model fails to adhere to instructions by making multiple tool calls within single turn, which is the primary cause of its low score. This behavior is gradually rectified through further training. C.3 Full Results of Ablation Study Table 10: Impact of reward components and the threshold θ on the performance of the AlphaQuanter7B model (1/2): detailed results for [GOOGL, META, MSFT]."
        },
        {
            "title": "Model",
            "content": "AlphaQuanter-7B w/o Rformat w/o Rtool θ 0.005 θ 0."
        },
        {
            "title": "MSFT",
            "content": "ARR () SR () MDD () ARR () SR () MDD () ARR () SR () MDD () -2.52% -6.40% -14.22% 2.83% -13.05% 0.05 -0.09 -0.25 0.10 -0.16 21.37% 41.91% 24.86% 25.28% 4.59% 28.66% 12.99% 47.29% 16.07% 50.82% 0.78 0.66 0.85 0.27 0. 25.65% 47.23% 25.03% 24.23% 10.91% 34.50% 13.94% 28.40% 16.53% 38.16% 1.17 0.51 0.72 0.48 0.87 14.85% 18.93% 19.81% 2.40% 20.01% Table 11: Impact of reward components and the threshold θ on the performance of the AlphaQuanter7B model (2/2): detailed results for [NVDA, TSLA] and average."
        },
        {
            "title": "Model",
            "content": "AlphaQuanter-7B w/o Rformat w/o Rtool θ 0.005 θ 0."
        },
        {
            "title": "Average",
            "content": "ARR () SR () MDD () ARR () SR () MDD () ARR () SR () MDD () 45.41% 33.70% 20.73% 40.00% 31.73% 0.66 0.49 0.43 0.22 0.53 34.91% 42.67% 35.55% 35.24% 20.88% 36.50% 27.59% 17.28% 30.84% -6.50% 0.58 0.43 0.70 0.32 0. 27.88% 34.94% 28.06% 15.85% 7.14% 43.66% 16.36% 19.90% 21.25% 20.23% 0.65 0.40 0.49 0.28 0.43 24.93% 26.49% 24.08% 9.18% 32.67% Table 10 and Table 11 present the full results of our ablation study, detailing the ARR, SR, and MDD for each configuration across all five stocks. On MSFT, NVDA, and TSLA, as well as on average, all ablated or adjusted models yield inferior performance compared to the complete AlphaQuanter-7B approach. Focusing on the SR metric, AlphaQuanter-7B remains superior to all other settings on 18 MSFT, NVDA, and in the average case. Notably, the results demonstrate strong inverse relationship between the decision threshold θ and MDD. Increasing the threshold consistently leads to significant decrease in MDD across all stocks, while decreasing it results in significant rise in MDD. This phenomenon is consistent with our analysis in Section 7.4, which attributes this behavior to changes in trading frequency."
        },
        {
            "title": "D A Working Example",
            "content": "We present an example of AlphaQuanter-7Bs decision-making process for MSFT on May 16, 2025, detailing its complete analysis and tool invocation trajectory. AlphaQuanter-7Bs decision-making process for MSFT on 2025-05-16 < think > will start by gathering historical market data for Microsoft ( MSFT ) to understand its recent price movements and trends . will look back 14 days from the current date , 2025 -05 -16. </ think > < tool_call > {\" name \": \" get_market_data \", \" arguments \": {\" symbol \": \" MSFT \", \" curr_date \": \"2025 -05 -16\" , \" look_back_days \": 14}} </ tool_call > user < tool_response > Open 431.74 432.87 432.20 433.84 437.93 440.00 445.94 447.78 448.14 450.77 452.05 Date 2025 -05 -02 2025 -05 -05 2025 -05 -06 2025 -05 -07 2025 -05 -08 2025 -05 -09 2025 -05 -12 2025 -05 -13 2025 -05 -14 2025 -05 -15 2025 -05 -16 0 1 2 3 4 5 6 7 8 9 10 </ tool_response > assistant < think > From the market data , it appears that MSFT has been showing bullish trend over the past 14 days , Adj Close 434.48 435.37 432.52 432.56 437.37 437.93 448.44 448.32 452.11 453.13 454.27 Volume 30757400 20136100 15104200 23295300 23491300 15324200 22821900 23618800 19902800 21992300 Low 429.99 432.11 431.17 431.11 435.66 435.88 439.78 445.36 448.14 450.43 448.73 High 439.44 439.50 437.73 438.12 443.67 440.74 449.37 450.67 453.90 456.19 454.36 Close 435.28 436.17 433.31 433.35 438.17 438.73 449.26 449.14 452.94 453.13 454.27 with the price closing above the previous close on most days . The volume also seems to be increasing , which could indicate growing interest in the stock . Next , will analyze the technical indicators to get more nuanced view of the stock momentum and volatility . </ think > < tool_call > {\" name \": \" get_stock_indicators \", \" arguments \": {\" symbol \": \" MSFT \", \" indicator \": \" RSI \", \" curr_date \": \"2025 -05 -16\" , \" look_back_days \": 14}} </ tool_call > user < tool_response > ## RSI values from 2025 -05 -02 to 2025 -05 -16: 71.99 - > 72.23 - > 70.16 - > 70.17 - > 71.76 - > 71.94 - > 75.24 - > 75.14 - > 76.30 - > 76.62 - > 76.99 RSI : Measures momentum to flag overbought / oversold conditions . Usage : Apply 70/30 thresholds and watch for divergence to signal reversals . </ tool_response > assistant < think > The RSI values for MSFT over the past 14 days have been consistently above 70 , indicating that the stock is in an overbought condition . This suggests that the stock might be due for correction . However , the RSI has been trending slightly lower , which could indicate that the selling pressure is increasing . will now look at the Bollinger Bands to see if the stock is trading above the upper band , which would also suggest overbought conditions . </ think > < tool_call > {\" name \": \" get_stock_indicators \", \" arguments \": {\" symbol \": \" MSFT \", \" indicator \": \" BBANDS \", \" curr_date \": \"2025 -05 -16\" , \" look_back_days \": 14}} </ tool_call > user < tool_response > ## BBANDS values from 2025 -05 -02 to 2025 -05 -16: ( Middle =382.60 , Upper =423.34 , Lower =341.87) -> ( Middle =386.41 , Upper =431.68 , Lower =341.15) -> ( Middle =390.18 , Upper =437.58 , Lower =342.78) -> ( Middle =394.11 , Upper =441.87 , Lower =346.35) -> ( Middle =396.49 , Upper =447.76 , Lower =345.22) -> ( Middle =399.36 , Upper =453.11 , Lower =345.61) -> ( Middle =402.39 , Upper =459.90 , Lower =344.88) -> ( Middle =405.45 , Upper =465.82 , Lower =345.08) -> ( Middle =408.81 , Upper =471.67 , Lower =345.94) -> ( Middle =412.92 , Upper =476.08 , Lower =349.75) -> ( Middle =417.27 , Upper =479.21 , Lower =355.33) Bollinger Bands : Consist of Middle Band ( typically 20 - period SMA ) and Upper / Lower Bands set at Âś2 standard deviations from the middle . Usage : The middle band serves as dynamic benchmark for price , the upper band highlights potential overbought or breakout zones , and the lower band signals possible oversold conditions . </ tool_response > assistant 19 < think > The Bollinger Bands for MSFT show that the stock price has been trading above the upper band for several days , which is strong indication of overbought conditions . This aligns with the RSI readings , suggesting that the stock might be due for correction . However , the price is still within the upper band , which means it not yet at extreme overbought levels . will now check the Moving Average Convergence Divergence ( MACD ) to see if there are any signals that might indicate potential trend reversal . </ think > < tool_call > {\" name \": \" get_stock_indicators \", \" arguments \": {\" symbol \": \" MSFT \", \" indicator \": \" MACD \", \" curr_date \": \"2025 -05 -16\" , \" look_back_days \": 14}} </ tool_call > user < tool_response > ## MACD values from 2025 -05 -02 to 2025 -05 -16: ( MACD =7.65 , Signal =0.92 , Histogram =6.73) -> ( MACD =10.16 , Signal =2.77 , Histogram =7.40) -> ( MACD =11.79 , Signal =4.57 , Histogram =7.22) -> ( MACD =12.94 , Signal =6.24 , Histogram =6.69) -> ( MACD =14.07 , Signal =7.81 , Histogram =6.26) -> ( MACD =14.84 , Signal =9.22 , Histogram =5.63) -> ( MACD =16.12 , Signal =10.60 , Histogram =5.52) -> ( MACD =16.92 , Signal =11.86 , Histogram =5.06) -> ( MACD =17.67 , Signal =13.02 , Histogram =4.64) -> ( MACD =18.06 , Signal =14.03 , Histogram =4.03) -> ( MACD =18.26 , Signal =14.88 , Histogram =3.38) MACD : Momentum indicator composed of the MACD line ( difference between two EMAs ) , the Signal line ( EMA of the MACD line ) , and the Histogram ( gap between MACD and Signal ). Usage : Identify trend changes through MACDâĂŞSignal crossovers , gauge momentum strength via Histogram size , and watch for divergence between MACD and price as early reversal signals . </ tool_response > assistant < think > The MACD values for MSFT show bullish trend , with the MACD line consistently above the Signal line and the Histogram indicating strong upward momentum . However , the recent readings of the Histogram suggest that the momentum might be starting to weaken . This , combined with the overbought conditions indicated by the RSI and Bollinger Bands , suggests that the stock might be due for correction . Given the current bullish momentum , it possible that the stock could see pullback before continuing its upward trend . However , the recent weakening of the momentum and the overbought conditions suggest that pullback is likely . </ think > < tool_call > {\" name \": \" get_news_data \", \" arguments \": {\" symbol \": \" MSFT \", \" curr_date \": \"2025 -05 -16\"}} </ tool_call > user < tool_response > ## MSFT News , from 2025 -05 -14 to 2025 -05 -16: Interpret the sentiment score x: values near 0 are Neutral , larger positive values indicate increasingly Bullish , and larger negative values indicate increasingly Bearish . 2025 -05 -14 [ Sentiment score = 0.01 , Neutral ] Should You Forget Alphabet and Buy These 2 Tech Stocks Instead ? 2025 -05 -14 [ Sentiment score = 0.27 , Somewhat - Bullish ] Bullish For Nvidia , Tesla , Palantir , Microsoft , Alphabet , Amazon : AI Revolution Is Coming To The Saudi Kingdom , Says Wedbush Dan Ives - Alphabet ( NASDAQ : GOOG ) , Amazon . com ( NASDAQ : AMZN ) 2025 -05 -14 [ Sentiment score = 0.22 , Somewhat - Bullish ] Correction or Not : This Artificial Intelligence ( AI ) Stock Is Worth Buying for the Long Haul 2025 -05 -14 [ Sentiment score = 0.04 , Neutral ] The Zacks Analyst Blog Highlights C3 .ai , UiPath , Microsoft , Alphabet and Amazon 2025 -05 -14 [ Sentiment score = 0.13 , Neutral ] Alphabet Down 16% YTD : Are GOOGL Shares Buy , Sell or Hold on the Dip ? 2025 -05 -14 [ Sentiment score = 0.02 , Neutral ] Marc Benioff of Salesforce : You re going to have to throw lot against the wall before you figure out what sticks 2025 -05 -14 [ Sentiment score = 0.09 , Neutral ] How To Trade SPY and Top Tech Stocks After Softer Inflation Print - Microsoft ( NASDAQ : MSFT ) 2025 -05 -14 [ Sentiment score = -0.22 , Somewhat - Bearish ] Caught Off Guard , Institutions Chase Stock Market Rally - SPDR S&P 500 ( ARCA : SPY ) 2025 -05 -14 [ Sentiment score = -0.03 , Neutral ] Nine Carahsoft Executives Recognized on the Prestigious 2025 CRNÂő Channel Chiefs List 2025 -05 -15 [ Sentiment score = 0.12 , Neutral ] Nvidia Just Rejoined Apple and Microsoft in the $ Trillion Club . Could It Become the First $4 Trillion Stock by 2026? ... 2025 -05 -16 [ Sentiment score = 0.14 , Neutral ] Steve Ballmer Missed Call : $1000 Investment Then Worth $289 ,000 To Apple Investors Today - Why Ex - Microsoft CEO Once Admitted Religious Transformation Came Too Late - Microsoft ( NASDAQ : MSFT ) , Apple ( NASDAQ : AAPL ) 2025 -05 -16 [ Sentiment score = 0.20 , Somewhat - Bullish ] Satya Nadella Once Bet Big On ChatGPT - Maker OpenAI - Now He Reportedly Embracing Rivals Like DeepSeek And Building Cheaper AI Models To Future - Proof Microsoft - Alphabet ( NASDAQ : GOOG ) , Alphabet ( NASDAQ : GOOGL ) 2025 -05 -16 [ Sentiment score = 0.18 , Somewhat - Bullish ] Why Is CoreWeave Stock Surging Today ? - CoreWeave ( NASDAQ : CRWV ) 2025 -05 -16 [ Sentiment score = 0.15 , Neutral ] Should You Buy , Sell or Hold INOD Stock After Q1 Earnings Beat ? 2025 -05 -16 [ Sentiment score = -0.11 , Neutral ] Why memories are crucial for AI chatbots 2025 -05 -16 [ Sentiment score = -0.04 , Neutral ] Why Alphabet Stock Ticked Higher Today 2025 -05 -16 [ Sentiment score = 0.00 , Neutral ] Look Ahead - Options Expiration May Bring Change , Aerospace Stocks Are On Fire From Trump Deals 2025 -05 -16 [ Sentiment score = 0.17 , Somewhat - Bullish ] How To Trade SPY And Top Tech Stocks Today 2025 -05 -16 [ Sentiment score = 0.20 , Somewhat - Bullish ] Prediction : This \" Magnificent Seven \" Stock Will Be the Most Valuable Company in the World in 10 Years 2025 -05 -16 [ Sentiment score = 0.21 , Somewhat - Bullish ] CloudKeeper Launches 30 - Day Challenge to Help Businesses Slash AWS Costs </ tool_response > assistant 20 < think > The news sentiment for MSFT over the past few days is mixed , with few neutral and slightly bullish articles . However , the overall sentiment is not strongly bullish , which aligns with the technical indicators suggesting overbought conditions and potential for pullback . The bullish news could provide some support for the stock , but the technical indicators suggest that the stock might need correction . Given the overbought conditions and the recent weakening of momentum , will consider the possibility of pullback . However , the strong support from bullish news could mean that the pullback might be short - lived . will now check the insider transactions to see if there are any significant buying or selling activities by company insiders , which can provide insights into the management view of the stock . </ think > < tool_call > {\" name \": \" get_insider_transactions \", \" arguments \": {\" symbol \": \" MSFT \", \" curr_date \": \"2025 -05 -16\" , \" look_back_days \": 7}} </ tool_call > user < tool_response > ## MSFT insider transactions from 2025 -05 -09 to 2025 -05 -16: ### Transaction Date : 2025 -05 -15 , COLEMAN , AMY ( EVP , Chief Human Resources Off ) Type : Common Stock Shares : 77.894 ( Disposal ) ### Transaction Date : 2025 -05 -15 , COLEMAN , AMY ( EVP , Chief Human Resources Off ) Type : Common Stock Shares : 13242.774 ( Disposal ) </ tool_response > assistant < think > The insider transactions for MSFT show that Amy Coleman , the EVP of Human Resources , has been disposing of significant number of shares , which could indicate lack of confidence in the stock . This is negative signal for the stock and could contribute to the overbought conditions . Given the technical indicators and the recent insider selling , it suggests that the stock might be due for correction . However , the bullish news and the strong support from the market data might mean that the correction could be short - lived . Considering the current overbought conditions , the recent insider selling , and the potential for pullback , will recommend HOLD strategy . This allows for the stock to potentially correct and provides safer entry point for more bullish stance in the future . </ think > < answer > HOLD </ answer >"
        }
    ],
    "affiliations": [
        "HKUST"
    ]
}