{
    "paper_title": "Can a Single Model Master Both Multi-turn Conversations and Tool Use? CALM: A Unified Conversational Agentic Language Model",
    "authors": [
        "Emre Can Acikgoz",
        "Jeremiah Greer",
        "Akul Datta",
        "Ze Yang",
        "William Zeng",
        "Oussama Elachqar",
        "Emmanouil Koukoumidis",
        "Dilek Hakkani-Tür",
        "Gokhan Tur"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) with API-calling capabilities enabled building effective Language Agents (LA), while also revolutionizing the conventional task-oriented dialogue (TOD) paradigm. However, current approaches face a critical dilemma: TOD systems are often trained on a limited set of target APIs, requiring new data to maintain their quality when interfacing with new services, while LAs are not trained to maintain user intent over multi-turn conversations. Because both robust multi-turn management and advanced function calling are crucial for effective conversational agents, we evaluate these skills on three popular benchmarks: MultiWOZ 2.4 (TOD), BFCL V3 (LA), and API-Bank (LA), and our analyses reveal that specialized approaches excel in one domain but underperform in the other. To bridge this chasm, we introduce CALM (Conversational Agentic Language Model), a unified approach that integrates both conversational and agentic capabilities. We created CALM-IT, a carefully constructed multi-task dataset that interleave multi-turn ReAct reasoning with complex API usage. Using CALM-IT, we train three models CALM 8B, CALM 70B, and CALM 405B, which outperform top domain-specific models, including GPT-4o, across all three benchmarks."
        },
        {
            "title": "Start",
            "content": "Can Single Model Master Both Multi-turn Conversations and Tool Use? CALM: Unified Conversational Agentic Language Model Emre Can Acikgoz1, Jeremiah Greer2, Akul Datta1, Ze Yang1, William Zeng2, Oussama Elachqar2, Emmanouil Koukoumidis2, Dilek Hakkani-Tür1, Gokhan Tur1 1University of Illinois Urbana-Champaign, 2Oumi {acikgoz2, akuld2, zey2, dilek, gokhan}@illinois.edu {jeremy, william, oussama, manos}@oumi.ai 5 2 0 2 2 1 ] . [ 1 0 2 8 8 0 . 2 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) with APIcalling capabilities enabled building effective Language Agents (LA), while also revolutionizing the conventional task-oriented dialogue (TOD) paradigm. However, current approaches face critical dilemma: TOD systems are often trained on limited set of target APIs, requiring new data to maintain their quality when interfacing with new services, while LAs are not trained to maintain user intent over multi-turn conversations. Because both robust multi-turn management and advanced function calling are crucial for effective conversational agents, we evaluate these skills on three popular benchmarks: MultiWOZ 2.4 (TOD), BFCL V3 (LA), and API-Bank (LA)and our analyses reveal that specialized approaches excel in one domain but underperform in the other. To bridge this chasm, we introduce CALM (Conversational Agentic Language Model), unified approach that integrates both conversational and agentic capabilities. We created CALM-IT, carefully constructed multi-task dataset that interleave multi-turn ReAct reasoning with complex API usage. Using CALMIT, we train three models CALM 8B, CALM 70B, and CALM 405B, which outperform top domain-specific models, including GPT-4o, across all three benchmarks1."
        },
        {
            "title": "Introduction",
            "content": "The concept of intelligent agents has been the cornerstone of artificial intelligence research for long time (Minsky, 1986), developing in parallel with the field of human-to-machine conversation (Young, 2002). The advent of LLMs (OpenAI et al., 2024; Dubey et al., 2024) has revolutionized both fields and enabled powerful Language Agents (LA) (Schick et al., 2024) while transforming modular dialogue systems into end-to-end solutions (Hudeˇcek 1https://emrecanacikgoz.github.io/CALM/ Figure 1: Unifying Capabilities of TOD Systems and LAs. TOD systems excel in multi-turn conversations and task completion but lack advanced API capabilities, while LA handle APIs well but struggle with coherent multi-turn dialogue. and Dusek, 2023). Despite sharing LLM foundations, they are typically focused and analyzed separately from each other; dialogue models focused on tasks such as multi-turn interactions, delivering relevant information to users, and dialogue management with state-tracking, on the other hand LAs concentrated exclusively on tool calling skills. What if single model could master both conversational and agentic tasks at the same time? The narrative of our paper aims to address the vision of unified conversational agent. Such an agent must excel not only in handling multi-turn conversations and TOD tasks but also in leveraging advanced LA capabilities, such as compound tool usage. Previous research has focused on training dialogue agents using structured flows in controlled scenarios (e.g., booking and reservation tasks) with limited set of actions (Li et al., 2024), or, relied on hand-crafted long prompts tied to small set of predefined APIs (Xu et al., 2024). However, Figure 2: Overview of the CALM Pipeline. This figure illustrates our dataset generation and fine-tuning framework. The top three rows depict the data transformation processes, along with corresponding sample shown on the right. In each training sample, green text highlights the input components of the instruction sample, while purple text indicates the target outputs optimized during fine-tuning. For detailed examples, refer to Figures 4 - 7. these approaches face limitations in real-world applications. Specifically, existing systems cannot easily adapt to new services without expensive finetuning or prompt engineering, yet real-world users often need access to diverse range of APIs and functionalities according to their needs. Moreover, there is notable performance gap reported in TOD tasks between proprietary models and open-source (Hudeˇcek and Dusek, 2023; Xu et al., 2024; Li et al., 2024). This tension underscores the need for an integrated open-source framework that supports both long-term dialogue state tracking and complex function calling from wide variety of APIs2. We propose CALM (Conversational Agentic Language Model), unified approach that integrates TOD strengths (e.g., multi-turn state tracking) with LA capabilities (e.g., dynamic function calls). As illustrated in Figure 1, we mitigate limitations on both sides by introducing CALM model series CALM-8B, CALM-70B, and CALM-405B, trained on CALM-IT, carefully created dataset that interleaves TOD tasks with complex function calling scenarios and ReAct-based multi-turn conversations where the agent integrates its reasoning process with actions before providing the response (Yao et al., 2023). To comprehensively eval2In this work, words such as \"tool use\", \"function calling\", and \"API calling\" are used interchangeably. uate these capabilities, we perform experiments on one TOD benchmark, MultiWOZ 2.4 (Ye et al., 2022), and two popular function calling benchmarks, the Berkeley Function Calling Leaderboard V3 (BFCL) (Yan et al., 2024) and API-Bank (Li et al., 2023) in completely zero-shot settings3. Our experiments reveal stark gap in existing models: while LAs excel at tool calling on BFCL V3, they falter on MultiWOZ 2.4 with poor task completion. Conversely, base LLMs and traditional TOD systems show limited function calling abilities, as evidenced by the low performance on BFCL V3 and API-Bank. In contrast, our CALM models, excel across both TOD and LA tasks. Our larger-scale open-source variantsCALM70B and CALM-405Boutperform GPT-4o and other domain-specific models on both TOD (MultiWOZ) and function calling benchmarks (BFCL V3 and API-Bank). In this paper, we study: How can we craft single conversational agentic LLM that elegantly interweaves multi-turn dialogue mastery with powerful function calling capabilities? Our key contributions are as follows: We analyze the gap between two domains: 3Here, \"zero-shot\" refers to none of the evaluation benchmark train-set was used while training the CALM models with CALM-IT. TOD systems and LA through evaluations on MultiWOZ 2.4, BFCL V3, and APIBank, highlighting limitations of existing approaches. We introduce CALM-IT, hybrid multi-task dataset for conversational agents featuring unique ReAct reasoning steps in multi-turn settings, encompassing samples across diverse domains, tasks, and abilities. We propose CALM, family of model series at different scales CALM 8B, CALM 70B and the largest open-source conversational agent CALM 405Ball unified by multi-turn dialogue skills and advanced function calling capabilities. Our larger models, CALM 70B and CALM 405B, outperform GPT-4o and GPT-4o-mini on both TOD and LA tasks, narrowing the gap between closed-source and open-source models. To foster further research within the open-source community, we publicly release code, all model weights, datasets, intermediate checkpoints, and training configurations."
        },
        {
            "title": "2 Related Work",
            "content": "The Era of Large Language Models. GPT-3s introduction of in-context learning shifted focus to advanced reasoning techniques through prompt engineering (Brown et al., 2020), without parameter updates (Wei et al., 2022; Wang et al., 2023). While divide emerged between closedand open-source models (OpenAI et al., 2024; Dubey et al., 2024), instruction tuning (Longpre et al., 2023; Chung et al., 2024) and human alignment efforts (Ouyang et al., 2022; Rafailov et al., 2024) improved LLMs ability to follow instructions and reason effectively, enabling powerful context-aware conversational systems. Language Agents. Tool learning and tool usage for LLMs has evolved from simple few-shot prompting to more sophisticated fine-tuning approaches (Ling et al., 2023; Yao et al., 2023). Early work relied on few-shot prompting to enable tool usage (Schick et al., 2024; Paranjape et al., 2023), but more recent research has focused on specialized fine-tuning on specific function calling formats (Patil et al., 2023; Wang et al., 2024; Abdelaziz et al., 2024). Toolformer (Schick et al., 2024) and related efforts have explored how LLMs autonomously learn when and how to call APIs, leading to improved performance in task-specific settings. Other methods like ToolAce (Liu et al., 2024) and Hammer (Lin et al., 2024) relied on fine-tuning syntactically generated datasets that integrates complex tool-calling capabilities. However, these methods are more focused on tool usage and missing emphasis on conversational abilities crucial for multi-turn user interactions in real-world scenarios. Dialogues and the Domain Shift. LLMs have transformed dialogue paradigm, especially for multi-turn TOD. Recent work has explored various approaches to harness capabilities of LLMs to dialogue applications, including fine-tuning (Gupta et al., 2022; Su et al., 2022) and prompting techniques (Hu et al., 2022). Several studies have examined the dialogue agentic abilities of instructionfinetuned LLMs in handling multi-turn conversations (Hudeˇcek and Dusek, 2023; Raffel et al., 2019). On the other hand, some works have focused on developing autonomous agents using an instruction-based schema (Xu et al., 2024) or enhancing conversational dialogues through API integration (Li et al., 2024; King and Flanigan, 2024). But in real-world settings, users often require access to diverse range of APIs, making it challenging for traditional TOD systems to adapt without costly re-training or continuously modifying handcrafted prompts for each new API."
        },
        {
            "title": "3 Preliminaries",
            "content": "A Conversational Agent, at its core, must understand user intents, maintain context across multiturn interactions, and respond contextually. Beyond traditional TOD tasks, modern conversational agents are also expected to exhibit agentic abilities, like tool calling, planning, and decision making, to fulfill complex user requests. An effective conversational agent integrates these capabilities as skills, ensuring natural and relevant interactions while efficiently completing the users objectives. The detailed task formulations for TOD systems and LA are provided in Appendix A. 3.1 Why we need both TOD and LA Capabilities? Multi-turn interactions are critical for refining ambiguous user requests. For example, when user says \"Find me hotel\", the system can ask clarifying questions to clarify the users intention (e.g., location, price range) instead of returning generic Data Domain Data Type Data Name Data Format # of Data Samples # of Total Tokens Avg. Tokens Per Sample TOD LA CRA Single-Turn SNIPS State Tracking Single-Turn Multi-Turn Multi-Turn Hammer ToolAce API Call API Call SGD ReAct API Call 13, 028 13, 819 202, 500 82, 236 12, 278, 10, 199, 147 129, 001, 612 59, 704, 782 Total 311, 583 211, 184, 321 942. 738.05 637.04 726.02 760.90 Table 1: CALM-IT Dataset Details. Statistical details of our proposed CALM-IT dataset showcasing the training mixtures. Generated CRA denotes the Conversational ReAct API dataset. results. This ensures meaningful and task-specific conversations. That said, traditional TOD systems excel at handling these multi-turn interactions but over small set of APIs (e.g., query_restaurant, book_hotel) (Ye et al., 2022). By training on structured dialogue flows, they achieve high task success rates in controlled scenarios (e.g., standard booking or reservation tasks) without requiring complex function-calling capabilities. However, these systems struggle to adapt to new services (e.g., airline, retail) without expensive re-training. In real-world settings, users may need to access wide variety of APIs (e.g., search_direct_flight, get_product_details). This is where LA shines: they leverage LLMs and can rapidly learn how to use unseen new tools since they are already proficient with determining when to invoke an API and decide which API to use from diverse set of available functions. Without these skills, agents fail to fulfill complex user goals, limiting their utility. Together, these skills form the backbone of unified conversational agents, enabling them to transition from being passive responders to proactive collaborators capable of managing intricate tasks and sustaining user engagement. 3.2 Can TOD Systems Solve Function Calling Tasks? The benchmark results demonstrate the limitations of TOD systems in function calling scenarios. Despite achieving top performance on MultiWOZ metrics as in Table 2, these systems show significantly lower accuracy on both API-Bank  (Table 3)  and BFCL  (Table 4)  benchmarks. This performance gap reveals that TOD systems traditional strengths in dialogue management do not translate well to handling diverse, unseen, and complex API calls. 3.3 Can LAs Handle Task-oriented Multi-turn Conversations? Conversely, agentic models like ToolAce (Liu et al., 2024), Hammer (Lin et al., 2024), and Granite (Abdelaziz et al., 2024) while achieving accurate results on API-Bank and BFCL V3, perform poorly on MultiWOZs task completion metrics. These results highlight critical weakness: while they deliver strong performance on function execution tasks, they fall short in maintaining coherent multiturn conversations and properly fulfilling user intents. Their specialized optimization for tool calling impairs their dialogue management abilities, indicating that current LAs need more balanced capabilities to handle task-oriented conversations more effectively."
        },
        {
            "title": "4 Methodology",
            "content": "Our approach, illustrated in Figure 2, develops unified agent skilled in goal-oriented multi-turn conversations and function calling. First, we build the CALM-IT, broad instruction-tuning (IT) dataset that spans multiple domains, tasks, and reasoning structures. Next, we do fine-tuning on the proposed CALM-IT dataset to produce CALM; balanced conversational agent model series capable of complex reasoning, fluent dialogue, user intent fulfillment, and function calling. 4.1 Conversational Agent Dataset Generation To develop conversational agent with diverse domain capabilities, we created comprehensive IT corpus that combines samples across multiple skills essential for task-oriented conversations and tool utilization. Our dataset encompasses three key components of TOD: state-tracking and APIsupported multi-turn conversations. We complemented these custom TOD datasets with existing function calling datasets from prior LA research. Figure 2 summarizes how the datasets are gathered for different optimization purposes and Table 1 provides detailed statistics of the overall CALM-IT corpus. TOD Datasets. An accurate dialogue system needs to master three fundamental capabilities: providing accurate information to users (Inform), fulfilling user goals (Success), and tracking dialogue states (DST) to understand user intents and goals throughout conversations (Walker et al., 1997). To equip our model with these skills, we utilized the SNIPS dataset, originally designed for language understanding but repurposed for single-turn dialogue state tracking (DST). We extracted its training split and converted it into the state tracking IT format by crafting detailed instruction prompt, as illustrated in Figure 4. This transformation resulted in training corpus of 24,542 IT samples for effective DST. Function Calling Datasets. Tool calling capability is the ability to select appropriate APIs and access external knowledge, which is crucial in modern LAs. An effective agent must not only choose the correct API but also provide properly typed parameters (e.g., integers or strings) and manage complex scenarios involving sequential or parallel function calls. To develop these skills, we incorporated datasets from two state-of-the-art agent models: Hammer (Lin et al., 2024) and ToolACE (Liu et al., 2024). Hammers training dataset incorporates random noise by replacing function and parameter names to prevent overfitting (see Figure 2), forcing the model to reason about API functionality through provided descriptions rather than memorizing specific identifiers. ToolACE provides multi-turn conversational scenarios in open-domain settings, where function calls may occur across multiple turns, but no database is provided. We post-process these datasets by incorporating changing the prompt instructions and adding conversation history if available. As reported in Table 1, the combined API calling corpus contains 216,319 samples. function calling training sample for the Hammer dataset can be seen in Figure 5. Conversational ReAct-based API-Calling (CRA) Dataset. While state tracking enables the understanding of user intent and function calling provides external knowledge access, integrating these capabilities within multi-turn task-oriented conversations requires additional reasoning about when to make API calls and how to interpret their results. Our primary contribution is completely new User and Agent conversation structure as User-Thought1-Action-Observation-Thought2Response. Although we start with an existing multi-turn dataset (SGD) (Rastogi et al., 2020), we systematically transform each dialogue to include two distinct thought steps (Thought1 and Thought2) and potential API calls (Action and Observation) following ReAct format (Yao et al., 2023), using GPT-4o for generation (Figure 2 top row). This transformation is achieved via carefully designed prompt in Table 6, which enforces strict Role Definition, Task Information, and Output Format. Since CRA is generated via GPT4o (OpenAI et al., 2024), it is also validated by human evaluators (Appendix D). Best of our knowledge, this is the first ReAct-based Conversational API dataset that incorporates detailed intermediate reasoning steps. This process yielded 82,236 samples, specifically tailored for task-oriented domains such as hotel bookings and restaurant reservations. We merge all three datasets into single training set called CALM-IT, please refer to Table 1 for details. We fine-tune our CALM models on this merged dataset in one pass. By interleaving samples from TOD, LA, and CRA, the model continuously practices different conversational skills without overfitting to any single domain or task type. 4.2 Fine-tuning Towards Conversational Agents We followed multitask fine-tuning approach to develop CALM models diverse capabilities across TOD, function calling, and multi-turn reasoning training on CALM-IT. Our training process is structured to target specific skills through different optimization objectives completely in zero-shot settings, as our CALM-IT dataset does not contain any of the evaluation benchmark training sets. Multitask Fine-tuning. As described in Section 4.1 and illustrated in Figure 2, our CALM-IT dataset combines samples from three distinct domains, each designed to cultivate specific skill: (i) TOD (Task-Oriented Dialogue) for strengthening dialogue state tracking, (ii) LA (Language-Action) for teaching the model when and how to invoke function calls, and (iii) ReAct for multi-turn conversation, multi-step reasoning and function calling. For TOD, we augment SNIPS data with prompt instructions (Figure 4), training the model to generate structured dialogue states in response to user queries. For function calling (LA), we optimize CALM to select the correct APIs and produce accurate function calls with proper parameter types (Figure 5), emphasizing reasoning over memorized patterns. We then address complex multi-turn conversations with API integration using our CRA dataset, formatted in the ReAct style. This stage uses two objectives: (1) action prediction (Figure 6), where the model learns to issue the appropriate function call given the conversation history, and (2) response generation (Figure 7), where it synthesizes coherent replies based on both API results and intermediate reasoning steps. Rather than merely producing answers, the model learns to reason, decide, and act in multiple stages before arriving at final response. Notably, we trained our models on CALM-IT by interleaving TOD, LA, and CRA samples, enabling the model to continuously practice diverse conversational skills while avoiding overfitting to any single domain or task type. Training Details. We developed the CALM model series by fine-tuning Llama 3.1 8B, Llama 3.3 70B, and Llama 3.1 405B (Dubey et al., 2024) using consistent Alpaca (Instruction-Input-Output) format. To balance efficiency and model quality, we applied LoRA (Hu et al., 2021) rank (r) = 16 and scaling factor (α) = 32 to all linear layers, and trained in mixed-precision bfloat16 (bf16) on 8 NVIDIA H100 GPUs. Under these settings, CALM-8B required approximately 8 hours of training, while CALM-70B took about 60 hours. We used global batch size of 8, trained for 3 epochs with learning rate of 1e 4, and employed linear warm-up schedule with 0.1 ratio. For CALM 405B, we fine-tuned Llama 3.1 405B and using QLoRA (Dettmers et al., 2023) with the same rank and scaling factor using bitsandbytes (BitsAndBytes, 2025) with quantization type of normalized float 4 (nf4). The precise training configurations for CALM 8B, CALM 70B and CALM 405 are included in the HuggingFace pages. Our training pipeline leveraged the Oumi framework4 to ensure reproducibility and streamlined management (Oumi, 2025)."
        },
        {
            "title": "5 Experiments",
            "content": "This section presents results highlighting CALMs effectiveness in unifying conversational management and advanced API-calling, outperforming specialized models across both TOD and LA benchmarks. 5.1 Experimental Settings Evaluation Benchmarks. We evaluate our approach on three complementary benchmarks that 4https://github.com/oumi-ai/oumi Method CALM 8B (ours) CALM 70B (ours) CALM 405B (ours) Hammer 2.0 7B ToolAce Granite-20B-Code CodeActAgent Llama 3.1 8B Instruct Llama 3.3 70B Instruct Mistral-7B-Instruct-v0.3 FNCTOD (Li et al., 2024) NC-Latent-TOD (King and Flanigan, 2024) GPT 3.5 Turbo (Hudeˇcek and Dusek, 2023) GPT4o-mini GPT4o Success JGA 51.6 69.4 66.7 23.5 18.0 10.7 9.5 19.9 67.6 31.2 44.4 68.3 - 69.9 75.5 30.4 43.8 38. 21.7 34.4 21.8 20.2 26.3 40.8 27.0 37.9 39.7 13.5 38.4 36.9 Table 2: MultiWOZ 2.4 Benchmark Results. Performance comparison across models on MultiWOZ 2.4 dialogue benchmark. Best scores are highlighted with bold. The asterisk (*) on CALM 405B denotes the checkpoint from one completed epoch, as the model is still under training. assess different aspects of model performance: MultiWOZ 2.4 (TOD), API-Bank (LA), and BFCL V3 (LA). Specifically, MultiWOZ 2.4 (Ye et al., 2022) is multi-domain TOD dataset covering scenarios such as hotel booking and transportation, where we measure Success Rate and Joint Goal Accuracy (JGA); in our zero-shot setting, we rely on the test set of 999 samples, using slightly modified AutoTOD prompt (Xu et al., 2024). APIBank (Li et al., 2023) focuses on evaluating toolaugmented LAs through 314 tool-use dialogues and 753 API calls, tested at two levels: L-1 (invoking known API) and L-2 (retrieving and calling from multiple candidates). Lastly, BFCL V35 (Patil et al., 2023) provides over 1,800 test cases spanning tasks like simple, multiple, and parallel function calls, evaluated by Abstract Syntax Tree (AST) accuracy and Executable Function Accuracy. See Appendix for further details. Baselines. In the LA tasks, we included strong baselines like Hammer (Lin et al., 2024), ToolAce (Liu et al., 2024), Granite (Abdelaziz et al., 2024) which represent state-of-the-art models in agentic tasks, including OpenAI models. For MultiWOZ evaluations, we recognize that many existing TOD models are trained with classification-based supervised fine-tuning, focusing primarily on DST. Such models do not support free-form dialogue generation, nor do they exhibit broader chat capabilities. In contrast, our approach aims to unify both conver5https://gorilla.cs.berkeley.edu/blogs/13_ bfcl_v3_multi_turn.html Model Rouge-L* L-2 L-1 Rouge-1 L-2 L-1 Rouge-2 L-2 L-1 BLEU-4 L-2 L-1 CALM 8B (ours) CALM 70B (ours) CALM 405B (ours) Llama 3.1 8B Instruct Qwen2.5 7B Instruct Hammer 2.0 7B ToolAce Granite-20B-Code Fnc-TOD 13B LDST tod-zero-bqag3oyb nc-latent-tod-step-2 92.8 92.7 93.4 72.7 84.3 90.1 81.5 60.3 3.9 8.3 3.7 3.2 81.9 83.2 77. 75.2 73.9 74.0 63.6 45.7 3.3 7.1 4.2 3.2 94.1 94.5 94.5 84.0 88.9 92.3 88.8 64.7 22.1 12.8 11.5 14.3 81.2 82.7 77. 81.4 78.5 74.1 71.3 48.9 23.4 11.6 12.4 13.3 91.9 92.5 92.4 79.8 84.6 89.9 85.0 59.5 8.0 2.7 1.1 3.2 76.4 78.9 71. 76.3 71.2 68.5 63.0 43.4 9.2 2.4 2.2 1.5 89.4 89.5 90.3 62.3 76.4 85.4 76.1 43.8 1.5 6.2 1.0 0.8 69.7 72.4 64. 65.1 64.2 58.4 67.0 29.3 1.1 5.7 0.9 0.8 Table 3: API-Bank Benchmark Results. Performance comparison across models on API-Bank function calling benchmark. Best scores are highlighted with bold and the second-best results are underlined. The asterisk (*) on CALM 405B denotes one completed epoch, as the model is still in the training process. sational (LA) and agentic (TOD) tasks into single, generative framework. On the other hand, there are some models evaluated in zero-shot settings but as per domain JGA, rather than overall JGA. That said, we used top popular zero-shot models FNCTOD (Li et al., 2024) and NC-Latent-TOD (King and Flanigan, 2024) as our TOD baselines in TOD. Please see Appendix for more details of these baseline models. 5.2 Results on MultiWOZ LA models struggle with TOD. Table 2 summarizes results on MultiWOZ 2.4. Baseline models optimized for function calling (ToolAce, Hammer, Granite, CodeAct) achieve low Success Rate and JGA. Although these agents can call APIs effectively, they fail to track user intents across multiple sessions or deliver correct final answers to the user, except ToolAce JGA reaches 34.4% accuracy close with domain-specific TOD models like FNCTOD. Instruction-tuned base LLMs like Llama 3.1 8B perform moderately better on MultiWOZ, reaching 19.9% Success rate and 26.3% JGA. CALM surpasses and generalizes in TOD. In contrast, our smallest CALM 8B achieves 51.6% Success Rate, more than doubling the Success performance compared to Llama 3.1 8B and surpassing other LAs. Moreover, our CALM 70B model achieves top results on DST with achieving 43.8% JGA, even outperforming GPT-4o and GPT-4o-mini. This shows CALMs ability with coherent multi-turn state-tracking, outperforming existing baselines and domain-specific models like FNCTOD. Notably, CALMs strong performance is achieved without any MultiWOZ samples in its CALM-IT training dataset, demonstrating its robustness in out-of-distribution (OOD) generalization. 5.3 Results on API-Bank and BFCL CALM adeptly orchestrates function calls. Table 3 shows API-Bank scores to test models API calling capabilities where Rouge-L is the primary evaluation metric. TOD models in the bottom row yield suboptimal results in this task. On the other hand, CALM 8B achieves Rouge-L score of 92.8 at Level-1 and 81.9 at Level-2, surpassing both TOD-oriented models and tool-centric LAs by significant margin. It also achieves top performance on nearly all metrics. Moreover, we scale CALM 8B accuracy with CALM 70B and CALM 405B models achieving top best and second best scores. This suggests that CALMs balanced approach enables it not only to retrieve and call the correct API but also to generate precise responses grounded in the returned results, fulfilling complex user requests effectively. CALM outperforms specialized LAs and GPT4o. We next assess function-calling accuracy on BFCL V3  (Table 4)  . Models trained only for TOD or basic instruction-following underperform. While LAs like Hammer and ToolAce fare better, our smallest model CALM 8B surpasses them (see Figure 3 for error analysis examples). Our larger scale models outperform GPT-4o, GPT-4o-mini and Llama-3.1-405B in overall accuracy. Remarkably, CALM 405B achieves 100% accuracy on the relevance detection task, highlighting its agentic reasoning capabilities through hallucination. That said, CALM 405B stands as the top-performing fully open-source model on the BFCL V3 leaderboard. Our comprehensive evaluation reveals several key insights. CALM model series successfully bridges the performance gap between TOD systems and LA, achieving top scores not in one but across all benchmarks. It points out that our approach successfully unifies different skills without significant trade-offs. 5.4 Domain Impact on Performance Table 5 highlights the performance impact of CALM-ITs fine-tuning components across TOD and function-calling tasks, using Llama 3.1 8B for computational efficiency. Removing API datasets Model Overall Acc Non-Live AST Acc Non-Live Exec Acc Live Acc Multi Turn Acc Relevance Detection Irrelevance Detection Mistral-7B-Instruct-v0.3 Llama-3.1-8B-Instruct Llama-3.3-70B-Instruct ToolAce Hammer2.0-7b Llama-3.1-405B-Instruct GPT-4o-mini (2024-07-18) GPT-4o (2024-08-06) CALM 8B (ours) CALM 70B (ours) CALM 405B (ours) 38.35% 49.84% 51.36% 52.55% 52.13% 56.38% 59.40% 59.83% 54.11% 60.49% 63.34% 56.33% 84.25% 84.85% 82.19% 86.94% 89.71% 86.52% 70.08% 85.17% 82.94% 90.46% 63.77% 79.75% 90.05% 86.98% 83.66% 84.70% 85.05% 60.79% 78.61% 81.36% 84.75% 57.31% 60.33% 62.51% 71.08% 71.17% 70.77% 73.26% 76.41% 72.59% 72.19% 74.59% 0.25% 10.25% 7.25% 0.88% 0.38% 11.75% 19.00% 34.62% 7.00% 26.25% 28.25% 77.78% 75.61% 95.12% 70.73% 95.12% 88.89% 78.05% 51.22% 77.78% 72.22% 100.00% 41.84% 47.92% 48.33% 87.29% 73.20% 70.86% 76.97% 87.34% 83.00% 85.36% 72.26% Table 4: BFCL V3 Benchmark Results. Performance comparison on the BFCL V3 function-calling benchmark. The best results are highlighted in bold, while the second-best results are underlined. The asterisk (*) on CALM 405B denotes one completed epoch, as the model continues training. TOD Task MultiWOZ 2.4 Function Calling Tasks API-Bank BFCL-V3 Model Success DST Rouge-L Rouge-L2 Overall Success 19.9 Llama 3.1 8B Instruct + CALM-IT w/o LA 46.0 (26.1 , 5.6 ) + CALM-IT w/o TOD 42.0 (22.1 , 9.6 ) + CALM-IT w/o CRA 50.0 (30.1 , 1.6 ) 26.3 28.5 (2.2 , 1.9 ) 19.4 (6.9 , 11.0 ) 34.5 (8.2 , 4.1 ) 72.7 45.5 (27.2 , 47.3 ) 92.7 (20.0 , 0.1 ) 91.3 (18.6 , 1.5 ) 75.2 48.8 (26.4 , 33.1 ) 78.9 (13.7 , 3.0 ) 78.8 (3.6 , 3.1 ) 49.8 35.4 (14.4 , 18.3 ) 54.1 (4.3 , 0.4 ) 56.6 (10.6 , 2.9 ) CALM 8B 51.6 30.4 92.8 81. 53.7 Table 5: Dataset Domain Effects. Experimental results highlighting the impact of excluding specific domain datasets during CALM fine-tuning. w/o indicates excluding the corresponding dataset during fine-tuning. Each row displays performance changes in parentheses with respect to base model (Llama) and final model (CALM), i.e. ( Llama, CALM). Performance gains are highlighted in green, while drops are marked in red. (LA row in Table 1) significantly reduces functioncalling performance, with API-Bank Rouge-L1 dropping 47.3% and BFCL success falling 18.3%. Excluding the DST dataset (TOD row in Table 1) leads to notable decline in CALMs DST accuracy, dropping by 11.0% relative to CALM and even underperforming base Llama by 6.9%. This underscores the essential role of fine-tuning on state tracking to capture user intents effectively. Finally, removing the GPT-4-generated CRA dataset (Final row in Table 1) has negative impact on MultiWOZ 2.4s Success metric, which plummets by 11.7%. Also, multi-turn function-calling accuracy dropped in API-Bank, both in L1 and L2 metrics. This indicates that the CRA dataset is instrumental in developing coherent and contextually aware responses in multi-turn settings. However, DST accuracy and BFCLs overall success see slight improvements, suggesting that certain specialized skills may benefit marginally in the absence of broader conversational reasoning. These results confirm that each dataset is crucial for balanced task performance, enabling CALM to generalize effectively across TOD and functioncalling tasks without overfitting to one domain."
        },
        {
            "title": "6 Conclusion and Future Work",
            "content": "In this work, we highlighted critical gap between LA and TOD systems, where each excels in complementary capabilities - function calling and multi-turn conversation management, respectively. To solve this, we introduced CALM, unified conversational agents that seamlessly integrates sophisticated API usage with natural multi-turn dialogue. Through fine-tuning on CALM-IT with hybrid fine-tuning strategy, CALM achieves leading performance on both TOD and LA benchmarks, demonstrating that single model can indeed master multi-turn conversations and tool use effectively. Future work can investigate using reinforcement learning (RL) to generate large-scale interaction trajectories supported with API calls could further enhance the self-evolution of conversational agents through purely RL-based optimization. Another direction is, improving multi-turn function calling and user interaction abilities of these models, which remains difficult problem with generally low accuracy. We believe that our findings, methodologies, and published resources will foster future research to create more capable and versatile conversational systems."
        },
        {
            "title": "Impact Statement",
            "content": "The CALM agent demonstrates improved performance across both conversational TOD and agentic tasks, offering global approach that combines these capabilities. By addressing the question Can Single Model Master Both Multi-turn Conversations and Tool Use?, we tried to establish new paradigm for developing unified conversational agents. However, we emphasize that our primary goal is neither to top specific leaderboards nor to create state-of-the-art agent that can be surpassed in the coming months through improved baselines or task-specific optimization with more data. Instead, our aim is to advance the field by identifying critical gaps in both Conversational AI and Language Agent research, highlighting why addressing these gaps is essential for developing reliable conversational systems that contribute to the path toward Artificial General Intelligence (AGI)."
        },
        {
            "title": "8 Acknowledgements",
            "content": "We would like to acknowledge the Oumi AI team (Oumi, 2025) for their assistance in training and scaling with the larger CALM models. We would also like to thank Together AI (TogetherAI, 2025) for providing the cluster resources necessary to enable CALM-405B training. This project also has benefited from the Microsoft Accelerate Foundation Models Research (AFMR) grant program, through which leading foundation models hosted by Microsoft Azure and access to Azure credits were provided to conduct the research."
        },
        {
            "title": "References",
            "content": "Ibrahim Abdelaziz, Kinjal Basu, Mayank Agarwal, Sadhana Kumaravel, Matthew Stallone, Rameswar Panda, Yara Rizk, Shrivatsa Bhargav, Maxwell Crouse, Chulaka Gunasekara, Shajith Ikbal, Sachindra Joshi, Hima Karanam, Vineet Kumar, Asim Munawar, Sumit Neelam, Dinesh Raghu, Udit Sharma, Adriana Meza Soria, Dheeraj Sreedhar, Praveen Venkateswaran, Merve Unuvar, David Daniel Cox, Salim Roukos, Luis A. Lastras, and Pavan Kapanipathi. 2024. Granite-function calling model: Introducing function calling abilities via multi-task learning of granular tasks. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: Industry Track, pages 1131 1139, Miami, Florida, US. Association for Computational Linguistics. BitsAndBytes. 2025. bitsandbytes. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems, volume 33, pages 18771901. Curran Associates, Inc. Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2024. Scaling instruction-finetuned language models. Journal of Machine Learning Research, 25(70):1 53. Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. 2023. Qlora: Efficient finetuning of quantized llms. Preprint, arXiv:2305.14314. Abhimanyu Dubey et al. 2024. The llama 3 herd of models. ArXiv, abs/2407.21783. Yujie Feng, Zexin Lu, Bo Liu, Liming Zhan, and XiaoMing Wu. 2023. Towards LLM-driven dialogue state tracking. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 739755, Singapore. Association for Computational Linguistics. Prakhar Gupta, Cathy Jiao, Yi-Ting Yeh, Shikib Mehri, Maxine Eskenazi, and Jeffrey Bigham. 2022. InstructDial: Improving zero and few-shot generalization in dialogue through instruction tuning. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 505525, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685. Yushi Hu, Chia-Hsuan Lee, Tianbao Xie, Tao Yu, Noah A. Smith, and Mari Ostendorf. 2022. Incontext learning for few-shot dialogue state tracking. In Findings of the Association for Computational Linguistics: EMNLP 2022, pages 26272643, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. Vojtˇech Hudeˇcek and Ondrej Dusek. 2023. Are large language models all you need for task-oriented dialogue? In Proceedings of the 24th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 216228, Prague, Czechia. Association for Computational Linguistics. Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023. Mistral 7b. arXiv preprint arXiv:2310.06825. OpenAI, Josh Achiam, et al. 2024. Gpt-4 technical report. Preprint, arXiv:2303.08774. Oumi. 2025. Oumi: an open, end-to-end platform for building large foundation models. Brendan King and Jeffrey Flanigan. 2024. Unsupervised end-to-end task-oriented dialogue with LLMs: The power of the noisy channel. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 82838300, Miami, Florida, USA. Association for Computational Linguistics. Minghao Li, Yingxiu Zhao, Bowen Yu, Feifan Song, Hangyu Li, Haiyang Yu, Zhoujun Li, Fei Huang, and Yongbin Li. 2023. API-bank: comprehensive benchmark for tool-augmented LLMs. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 31023116, Singapore. Association for Computational Linguistics. Zekun Li, Zhiyu Zoey Chen, Mike Ross, Patrick Huber, Seungwhan Moon, Zhaojiang Lin, Xin Luna Dong, Adithya Sagar, Xifeng Yan, and Paul A. Crook. 2024. Large language models as zero-shot dialogue state tracker through function calling. Preprint, arXiv:2402.10466. Qiqiang Lin, Muning Wen, Qiuying Peng, Guanyu Nie, Junwei Liao, Jun Wang, Xiaoyun Mo, Jiamu Zhou, Cheng Cheng, Yin Zhao, and Weinan Zhang. 2024. Hammer: Robust function-calling for on-device language models via function masking. ArXiv, abs/2410.04587. Yuan Ling, Fanyou Wu, Shujing Dong, Yarong Feng, George Karypis, and Chandan K. Reddy. 2023. International workshop on multimodal learning - 2023 theme: Multimodal learning with foundation models. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD 23, page 58685869, New York, NY, USA. Association for Computing Machinery. Weiwen Liu, Xu Huang, Xingshan Zeng, Xinlong Hao, Shuai Yu, Dexun Li, Shuai Wang, Weinan Gan, Zhengying Liu, Yuanqing Yu, Zezhong Wang, Yuxian Wang, Wu Ning, Yutai Hou, Bin Wang, Chuhan Wu, Xinzhi Wang, Yong Liu, Yasheng Wang, Duyu Tang, Dandan Tu, Lifeng Shang, Xin Jiang, Ruiming Tang, Defu Lian, Qun Liu, and Enhong Chen. 2024. Toolace: Winning the points of llm function calling. ArXiv, abs/2409.00920. Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc Le, Barret Zoph, Jason Wei, et al. 2023. The flan collection: Designing data and methods for effective instruction tuning. In International Conference on Machine Learning, pages 2263122648. PMLR. Marvin Minsky. 1986. The Society of Mind. Simon & Schuster. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744. Bhargavi Paranjape, Scott Lundberg, Sameer Singh, and Hannaneh Hajishirzi, Luke Zettlemoyer, Marco Tulio Ribeiro. 2023. Art: Automatic multistep reasoning and tool-use for large language models. arXiv preprint arXiv:2303.09014. Shishir Patil, Tianjun Zhang, Xin Wang, and Joseph Gonzalez. 2023. Gorilla: Large language model connected with massive apis. arXiv preprint arXiv:2305.15334. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. 2024. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36. Colin Raffel, Noam M. Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2019. Exploring the limits of transfer learning with unified text-to-text transformer. J. Mach. Learn. Res., 21:140:1140:67. Abhinav Rastogi, Xiaoxue Zang, Srinivas Sunkara, Raghav Gupta, and Pranav Khaitan. 2020. Towards scalable multi-domain conversational agents: The schema-guided dialogue dataset. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 86898696. Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2024. Toolformer: Language models can teach themselves to use tools. Advances in Neural Information Processing Systems, 36. Yixuan Su, Lei Shu, Elman Mansimov, Arshit Gupta, Deng Cai, Yi-An Lai, and Yi Zhang. 2022. Multi-task pre-training for plug-and-play task-oriented dialogue system. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 46614676, Dublin, Ireland. Association for Computational Linguistics. TogetherAI. 2025. Together ai: The ai acceleration cloud. Marilyn A. Walker, Diane J. Litman, Candace A. Kamm, and Alicia Abella. 1997. PARADISE: framework for evaluating spoken dialogue agents. In 35th Annual Meeting of the Association for Computational Linguistics and 8th Conference of the European Chapter of the Association for Computational Linguistics, pages 271280, Madrid, Spain. Association for Computational Linguistics. Xingyao Wang, Yangyi Chen, Lifan Yuan, Yizhe Zhang, Yunzhu Li, Hao Peng, and Heng Ji. 2024. Executable code actions elicit better llm agents. In ICLR 2024 Workshop on Large Language Model (LLM) Agents. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2023. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. 2022. Chain-of-thought prompting elicits reasoning in large language models. In Advances in Neural Information Processing Systems, volume 35, pages 2482424837. Curran Associates, Inc. Heng-Da Xu, Xian-Ling Mao, Puhai Yang, Fanshu Sun, and Heyan Huang. 2024. Rethinking taskoriented dialogue systems: From complex modularity to zero-shot autonomous agent. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 27482763, Bangkok, Thailand. Association for Computational Linguistics. Fanjia Yan, Huanzhi Mao, Charlie Cheng-Jie Ji, Tianjun Zhang, Shishir G. Patil, Ion Stoica, and Joseph E. Gonzalez. 2024. Berkeley function calling https://gorilla.cs.berkeley. leaderboard. edu/blogs/8_berkeley_function_calling_ leaderboard.html. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2023. React: Synergizing reasoning and acting in language models. Preprint, arXiv:2210.03629. Fanghua Ye, Jarana Manotumruksa, and Emine Yilmaz. 2022. MultiWOZ 2.4: multi-domain task-oriented dialogue dataset with essential annotation corrections to improve state tracking evaluation. In Proceedings of the 23rd Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 351360, Edinburgh, UK. Association for Computational Linguistics. Steve Young. 2002. Talking to machines (statistically speaking). In 7th International Conference on Spoken Language Processing (ICSLP 2002), pages 916."
        },
        {
            "title": "A Problem Formulation",
            "content": "A.1 End-to-End TOD Systems with LLMs LLM-based end-to-end TOD systems generate contextually relevant responses based on dialogue history and task instructions. Let be language model parameterized by θ, which maps an input context given as prompt to an output system response yt. At each dialogue turn t, the system receives three key comtask instructions G, dialogue history ponents: Ht comprising of prior user-system interactions {(u1, y1), ..., (ut1, yt1)}, and the current user input ut. These elements are combined to form the complete prompt Tt = (G, Ht, ut). The model generates response yt by modeling the conditional probability: (ytTt; θ) = (ytG, Ht, ut; θ), (1) where (stTt; θ) denotes the probability of generating the response yt given the prompt Tt and the model parameters θ. The dialogue progresses by updating the history after each turn Ht+1 = Ht + [(ut, st)], maintaining the sequential nature of the interaction while preserving task orientation through G. A.2 Function Calling with Language Agents language model Fθ maps an input = (G, u, Ω), where is the task prompt, is the user query, and Ω = {f1, . . . , fn} is the set of available functions with their arguments and descriptions to structured function call y. The model generates target function call in structured format, such as JSON or text schema. The generation probability is defined as: (y x; θ) = (y G, u, Ω; θ) (2) This formulation enables the model to translate natural language inputs into precise and wellstructured function calls, facilitating seamless integration with external systems. ReAct Prompting. ReAct (Yao et al., 2023) integrate reasoning and action-taking to enable more effective decision-making. It facilitates intermediate reasoning by breaking down complex tasks into smaller, interpretable reasoning steps. Additionally, it enables interaction with external tools or APIs by producing structured actions that integrate effectively with external systems. As result of an API execution, ReAct incorporates observations dynamically, adapting subsequent reasoning and actions based on the results of previous steps, thus improving the systems responsiveness and overall task performance."
        },
        {
            "title": "B Details of the Evaluation Benchmarks",
            "content": "MultiWOZ 2.4. MultiWOZ 2.4 (Ye et al., 2022) is multi-domain TOD dataset designed to evaluate dialogue systems ability to handle complex conversations across multiple domains such as hotel booking, restaurant reservations, and transportation. We employ two different metrics during our TOD evaluations MultiWOZ: Success Rate, which assesses whether all user-requested information related to the entity is successfully provided and Joint Goal Accuracy (JGA) which measures the accuracy of predicted dialogue states, reflecting the systems ability to track user intents. During our zero-shot evaluations, we used its test set that contains 999 samples and incorporated AutoTOD prompt (Xu et al., 2024) with slight modifications, thereby generating system responses analogous to those produced in chat-based inference setting. API-Bank. API-Bank (Li et al., 2023) is designed to evaluate tool-augmented LAs, focusing on their ability to plan, retrieve, and invoke APIs effectively. It includes 314 tool-use dialogues and 753 API calls, with two evaluation levels: Level 1 (L-1), which tests the accuracy of invoking known API based on given query, and Level 2 (L-2), which assesses the retrieval and invocation of APIs from candidate list, simulating real-world scenarios with multiple API options. By addressing these challenges, API-Bank advances the understanding and enhancement of tool-augmented reasoning in LLMs. During evaluations, we used the official evaluation code from the repository of previous works (Lin et al., 2024). Berkeley Function Calling Leaderboard. In addition to API-Bank, we also used BFCL V36 (Patil et al., 2023) which provides diverse evaluation framework for assessing the models ability to perform function calls across various objectives. It includes more than 1,800 test cases that span tasks such as simple functions, multiple functions, and parallel functions for Python and other environ6https://gorilla.cs.berkeley.edu/blogs/13_ bfcl_v3_multi_turn.html ments such as REST APIs and JavaScript. Models are evaluated using two primary metrics: (i) Abstract Syntax Tree (AST) accuracy, which ensures syntactic correctness by verifying function structures, parameters, and types against predefined documentation and (ii) Executable Function Accuracy, which evaluates whether generated functions execute correctly and produce the expected outputs, emphasizing real-world applicability. In our experiments, we employed the official repository released by authors and followed the provided instructions to get model results."
        },
        {
            "title": "Experiments",
            "content": "In this section, we provide an overview of the models used in our experiments, including their brief descriptions, checkpoints, and the training re-production code references. C.1 Base Models Llama 3.1. The Llama (Large Language Model Meta AI) (Dubey et al., 2024) family is set of open-source language models from Meta AI, ranging from 7 to 405 billion parameters. It trained on large corpus of web content, academic texts, and books, they excel at reasoning, question-answering, and code generation. Their architecture supports efficient fine-tuning and deployment. In our experiments, we use Llama-3.1-8B-Instruct7, released in July 2024, which offers improved multilingual capabilities, longer context windows, and state-ofthe-art performance in general knowledge, math, and tool usage Mistral v03. Mistral 7B (Jiang et al., 2023) is one of the state-of-the-art, open-source LLMs produced by Mistral AI. It employs innovative mechanisms such as grouped-query and sliding window attention, which enable efficient processing of longer sequences and faster inference times. In our experiments, we use Mistral-7B-Instruct-v0.38, released on May 22, 2024, and available on Hugging Face. C.2 TOD Models LDST. LDST (LLM-driven Dialogue State Tracking) (Feng et al., 2023) is an approach that overcomes the limitations of proprietary models in state tracking by leveraging fine-tuned LLaMa 7B model. The approach combines novel assembled domain-slot instruction tuning technique with parameter-efficient strategies, enabling resourceefficient performance that tries matches larger models. During our experiments and to fine-tune LDST we used the provided checkpoints and implementation details for LDST are available in their public repository 9. Fnc-TOD. FNC-TOD (Function-Calling for Task-Oriented Dialogue) focuses on DST in LLMs through function calling mechanisms. The method conceptualizes domain schemas as functions and embeds function specifications within the system prompt. This approach achieved improved conversation state tracking in task-oriented dialogues using fine-tuned Llama-2-13b-chat-hf model, trained on focused dataset of 7,200 task-oriented dialogues spanning 36 domains. For our experiments, we utilized the authors publicly released Zekunli/FncTOD-Llama-13b model available on Huggingface 10. NC-Latent-TOD. This work introduces an unsupervised approach to TOD systems that operates solely with API schemas and unlabeled dialogues, eliminating the need for costly turn-level annotations. The system generates pseudo-labels for API calls and system actions while using HardExpectation maximization approach with LLM predictions for iterative fine-tuning, enhanced by noisy-channel reranking method (King and Flanigan, 2024). During our experiments, we used two different models nc-latent-tod-step-2-final11 and tod-zero-bqag3oyb-3200012 shared by the authors. C.3 Language Agents CodeAct-Agent. CodeAct (Wang et al., 2024) is framework that enables LLM agents to generate and execute Python code as actions to interact with environment, rather than being limited to JSON or structured text formats. By integrating Python interpreter, it allows agents to dynamically adjust their actions based on execution results, leverage existing Python packages, and utilize programming constructs like loops and conditionals for complex 9https://github.com/WoodScene/LDST 10https://huggingface.co/Zekunli/ FncTOD-Llama-13b 7https://huggingface.co/meta-llama/Llama-3. 11https://huggingface.co/Brendan/ 1-8B-Instruct 8https://huggingface.co/mistralai/ Mistral-7B-Instruct-v0. nc-latent-tod-step-2-final 12https://huggingface.co/Brendan/ tod-zero-bqag3oyb-32000 operations. authors developed CodeActAgent by fine-tuning both Mistral 7B and Llama2 7B models on the CodeAct-Instruct dataset. For our experiments, we utilized the authors officially released CodeActAgent-Mistral-7b-v0.1 model, available on Huggingface 13. Granite-20B. This work introduces Granite-20B, an open-source LLM, specifically designed for function calling capabilities. The model is trained using multi-task approach on seven core function calling tasks: Nested Function Calling, Function Chaining, Parallel Functions, Function Name Detection, Parameter-Value Pair Detection, Next-Best Function, and Response Generation. We used the offical model weights granite-20b-code-instruct-8k provided in Huggingface14. Hammer2.0-7B. Hammer (Lin et al., 2024) is small scale model family up to 7B parameter models designed for on-device function calling and addresses generalization challenges in function calling through two key innovations: an irrelevanceaugmented dataset that enhances models ability to identify inappropriate functions, and function masking technique that reduces naming-based misinterpretations by focusing on function descriptions. Built by fine-tuning the xLAM-functioncalling dataset15 with 7,500 additional instances for irrelevance detection, Hammer achieves stateof-the-art performance on BFCL Benchmark. For our experiments, we utilized the official Hammer 2.0 model weights available on Huggingface16, along with training it from scratch for reproducibility using provided public repository and training scripts17. ToolAce 8B. This work introduces ToolACE (Liu et al., 2024), an automated pipeline for generating high-quality function-calling training data. The system features self-evolution synthesis process that curates pool of 26,507 diverse APIs, coupled with multi-agent dialogue generation system and dual-layer verification process for ensuring data accuracy. Using data generated and fine-tuning on Llama-3.1-8B-Instruct, ToolACE achieve top 13https://huggingface.co/xingyaoww/ CodeActAgent-Mistral-7b-v0.1 14https://huggingface.co/ibm-granite/ granite-20b-code-instruct-8k 15https://huggingface.co/datasets/Salesforce/ xlam-function-calling-60k 16https://huggingface.co/MadeAgents/Hammer2. 0-7b 17https://github.com/MadeAgents/Hammer results on the BFCL Leaderboard. We used the official Huggingface checkpoint18 and dataset19."
        },
        {
            "title": "Dataset",
            "content": "To analyze the quality of generated conversations, we implemented systematic random sampling approach. From the generated dataset, we randomly selected 100 dialogue instances for validation. We conducted the evaluation against predefined set of 51 available functions, covering transportation, booking, entertainment, and utility services. We scrutinized each functions schema, including its parameters and expected usage, to ensure compliance. We asked senior Computer Science student to evaluate these generated samples across four key dimensions: Undefined Function Call: Validating API names and parameters against the predefined function list to identify undefined functions or invalid arguments. Incorrect Argument Type: Checking argument structures to ensure compliance with the function schemas. Argument Hallucination: Detecting unnecessary or irrelevant arguments misaligned with the conversation context. Low-Quality Reasoning and Planning: Identifying logical gaps in though steps or unnecessary API calls in ReAct structure. We asked for binary score (1 for no errors, 0 for detected issues) for each generated dialogue and provided mandatory feedback for any errors. Our evaluation of 100 dialogues showed 9% error rate, mostly in restaurant reservations where key details like the restaurant name or dining time were missing. These errors stemmed from Argument Hallucination and Low-Quality Reasoning. Results, including dialogue IDs, scores, and feedback, were systematically collected to identify areas for improvement. 18https://huggingface.co/Team-ACE/ToolACE-8B 19https://huggingface.co/datasets/Team-ACE/ ToolACE Figure 3: Error Analysis of Function-Calling Results. Illustrated performance comparison on function calling benchmarks API-Bank L1 (top) and BFCL V3 parallel function call (bottom). Results demonstrate CALMs consistent performance compared to other baselines. SNIPS SFT Sample Format: Dialogue State Tracking Instruction: You are helpful assistant who is assigned to find the intents shown by the user on 7 domains - GetWeather, AddToPlaylist, SearchScreeningEvent, BookRestaurant, SearchCreativeWork, RateBook, PlayMusic. The user can seek for BookRestaurant by slots - poi, restaurant_type, served_dish, timeRange, party_size_number, restaurant_name, state, country, party_size_description, sort, city, spatial_relation, cuisine, facility. The user can seek for GetWeather by slots - condition_temperature, geographic_poi, current_location, timeRange, condition_description, state, country, city, spatial_relation. The user can seek for SearchCreativeWork by slots - object_type, object_name. The user can seek for PlayMusic by slots - track, playlist, service, genre, year, album, music_item, sort, artist. The user can seek for SearchScreeningEvent by slots - movie_name, location_name, timeRange, object_type, movie_type, object_location_type, spatial_relation. The user can seek for RateBook by slots - ject_part_of_series_type, best_rating, object_name. Do not capture any other slots! rating_unit, object_type, object_select, obrating_value, # Task You will be provided with an user utterance. You must find all the user intents and output them in JSON format. # Sample Output \"domain\": \"AddToPlaylist\", \"slot_values\": \"music_item\": \"abc\", \"artist\": \"xyz\" Input: User: Book table at restaurant in Portugal with parking for me and bonnie in 19 minutes Output: System: \"domain\": \"BookRestaurant\", \"slot_values\": \"restaurant_type\": \"restaurant\", \"country\": \"Portugal\", \"facility\": \"parking\", \"party_size_description\": \"me and bonnie\", \"timeRange\": \"in 19 minutes\"\" Figure 4: SNIPS fine-tuning sample example. Hammer SFT Sample Format: Function Calling Instruction: [BEGIN OF TASK INSTRUCTION] You are tool calling assistant. In order to complete the users request, you need to select one or more appropriate tools from the following tools and fill in the correct values for the tool parameters. Your specific tasks are: 1. Make one or more function/tool calls to meet the request based on the question. 2. If none of the function can be used, point it out and refuse to answer. 3. If the given question lacks the parameters required by the function, also point it out. [END OF TASK INSTRUCTION] [BEGIN OF AVAILABLE TOOLS] [\"name\": \"LxOm64zLyg\", \"description\": \"Gets hourly weather forecast information for given geographical coordinates using the RapidAPI service.\", \"parameters\": \"TDpjPd\": \"description\": \"The latitude of the geographical location.\", \"type\": \"int\", \"default\": 46.95828, \"78th2U3lFj\": \"description\": \"The longitude of the geographical location.\", \"type\": \"int\", \"default\": 10.87152, \"name\": \"WoDdNSe7e7K5\", \"description\": \"Fetches weather updates for given city using the RapidAPI Weather API.\", \"parameters\": \"LzZsvxUC\": \"description\": \"The name of the city for which to retrieve weather information.\", \"type\": \"str\", \"default\": \"London\", \"name\": \"CBrCNmwOERb\", \"description\": \"Fetches the hourly weather forecast for given location using the RapidAPI service.\", \"parameters\": \"TDEJ.ZwMt\": \"description\": \"The name of the location for which to retrieve the hourly weather forecast.\", \"type\": \"str\", \"default\": \"Berlin\", \"name\": \"1YTQVXkwLY\", \"description\": \"Returns an air quality forecast for given location.\", \"parameters\": \"2bkgDA\": \"description\": \"The latitude of the location for which the air quality forecast is to be retrieved.\", \"type\": \"int\", \"default\": \"35.779\", \"DQi.ReZ16\": \"description\": \"The longitude of the location for which the air quality forecast is to be retrieved.\", \"type\": \"int\", \"default\": \"-78.638\", \"hF.1\": \"description\": \"The number of hours for which the forecast is to be retrieved (default is 72).\", \"type\": \"int\", \"default\": \"72\"] [END OF AVAILABLE TOOLS] [BEGIN OF FORMAT INSTRUCTION] The output MUST strictly adhere to the following JSON format, and NO other text MUST be included. The example format is as follows. Please make sure the parameter type is correct. If no function call is needed, please directly output an empty list [] [ \"name\": \"func_name1\", \"arguments\": \"argument1\": \"value1\", \"argument2\": \"value2\", ... (more tool calls as required) ] [END OF FORMAT INSTRUCTION] Input: [BEGIN OF QUERY] What are the current weather conditions in Sydney? [END OF QUERY] Output: [\"name\": \"WoDdNSe7e7K5\", \"arguments\": \"LzZsvxUC\": \"Sydney\"] Figure 5: Hammer fine-tuning sample example. SGD Instruction Sample Format: Action Optimization Instruction: [BEGIN OF TASK INSTRUCTION] You are helpful conversational assistant who can perform API function calling. Your goal is to understand user queries and respond using the appropriate API functions. In order to complete the users request, you need to select tool from the following functions and fill in the correct values for the function parameters. Your specific tasks are: 1. Analyze the users query within the given dialogue context to identify their intent and relevant details. 2. Make function/tool call and provide the necessary arguments to meet the request based on the user query. 3. Formulate natural and coherent response, guiding the conversation towards resolving the users request. [END OF TASK INSTRUCTION] [BEGIN OF AVAILABLE TOOLS] 1. FindBus(from_location, to_location, leaving_date) 2. ReserveCar(pickup_location, pickup_date, pickup_time, dropoff_date, type) . . . 50. FindProvider(city, type) 51. GetRide(destination, number_of_riders, shared_ride) [END OF AVAILABLE TOOLS] [BEGIN OF FORMAT INSTRUCTION] The output MUST strictly adhere to the following structured text format. Example Output API Call Format: function_name(argument1=value1, argument2=value2, ...) [END OF FORMAT INSTRUCTION] [BEGIN OF CONVERSATION HISTORY] User: Im looking for events happening in New York this weekend. Thought: The user wants to find events in specific location and timeframe. Ill need to call the FindEvents API with the appropriate category and city. Action: FindEvents(category=all, city_of_event=New York) Observation: events: [name: Jazz Festival, date: 2023-10-07, location: Central Park, name: Food Truck Rally, date: 2023-10-08, location: Union Square] Thought: have the event details now. should summarize the events for the user. System: This weekend in New York, you can attend the Jazz Festival at Central Park on October 7th and the Food Truck Rally at Union Square on October 8th. Would you like more information about any of these events? User: Yes, can you tell me more about the Jazz Festival? Thought: need to provide more details about the Jazz Festival. dont need an API call for that. System: The Jazz Festival in Central Park will feature various artists performing live jazz music throughout the day. It starts at 11 AM and goes until 8 PM. Would you like to know how to get tickets? [END OF CONVERSATION HISTORY] Input: User: Yes, please tell me how to get tickets for the Jazz Festival. Thought: The user is interested in purchasing tickets. need to call the BuyEventTickets API for the Jazz Festival. Output: Action: BuyEventTickets(event_name=Jazz Festival, number_of_seats=2, date=2023-10-07, city_of_event=New York) Figure 6: SGD fine-tuning sample example targeting function optimization. SGD Instruction Sample Format: System Response Optimization Instruction: [BEGIN OF TASK INSTRUCTION] You are helpful conversational assistant specializing in understanding user queries and providing accurate, reasoned responses. Your goal is to analyze the users input, reason about their intent and needs, and provide coherent and contextually appropriate system response. Your specific tasks are: - Ensure your response is informative and contextually relevant, guiding the conversation toward successful task completion. - Analyze the users input in the context of the conversation history (if available) to identify their intent and relevant details. - Use logical reasoning to determine the most suitable response, considering the users needs and the dialogue context. - Generate natural and coherent system response to address the users request or query effectively. [END OF TASK INSTRUCTION] [BEGIN OF FORMAT INSTRUCTION] The output MUST strictly adhere to the following structured text format. Example Output API Call Format: function_name(argument1=value1, argument2=value2, ...) [END OF FORMAT INSTRUCTION] [BEGIN OF CONVERSATION HISTORY] User: Im looking for events happening in New York this weekend. Thought: The user wants to find events in specific location and timeframe. Ill need to call the FindEvents API with the appropriate category and city. Action: FindEvents(category=all, city_of_event=New York) Observation: events: [name: Jazz Festival, date: 2023-10-07, location: Central Park, name: Food Truck Rally, date: 2023-10-08, location: Union Square] Thought: have the event details now. should summarize the events for the user. System: This weekend in New York, you can attend the Jazz Festival at Central Park on October 7th and the Food Truck Rally at Union Square on October 8th. Would you like more information about any of these events? User: Yes, can you tell me more about the Jazz Festival? Thought: need to provide more details about the Jazz Festival. dont need an API call for that. System: The Jazz Festival in Central Park will feature various artists performing live jazz music throughout the day. It starts at 11 AM and goes until 8 PM. Would you like to know how to get tickets? [END OF CONVERSATION HISTORY] Input: User: Yes, please tell me how to get tickets for the Jazz Festival. Thought: The user is interested in purchasing tickets. need to call the BuyEventTickets API for the Jazz Festival. Action: BuyEventTickets(event_name=Jazz Festival, number_of_seats=2, date=2023-10-07, city_of_event=New York) Observation: status: success, message: You have successfully purchased 2 tickets for the Jazz Festival. Thought: The tickets were successfully purchased. need to confirm this with the user. Output: System: You have successfully purchased 2 tickets for the Jazz Festival in Central Park! Enjoy the music! Figure 7: SGD fine-tuning sample example targeting system response optimization. You are an advanced AI assistant specializing in conversational dialogues. You have access to variety of services and APIs to assist users with their requests and your goal is to provide helpful and informative responses to user queries and commands. You can interact with multiple services and APIs to fulfill user requests. Your responses should be natural, informative, and tailored to the users needs. # Task Information: - You are asked to create dataset in the format: User - Thought1 - API - API Input Arguments - API Result - Thought2 - System, or User - Thought - System. - For the given # User Input, generate multi-turn dialogue that follows this format, with each turn exhibiting realistic context reasoning, thought processes, and API interaction where applicable. - The dialogues should be converted to follow specific # Output Format, which includes reasoning on whether an API call is needed or if the system can respond directly. - If the system decides that an API call is necessary, use this format: User - Thought1 - API - API Input Arguments - API Result - Thought2 - System. - Call the right API from # Avaliable Functions and provide the necessary input arguments to fulfill the users request. - If you think function argument is not necessary, you can skip it. Dont provide unnecessary arguments and None values. - Ensure that the API calls are used logically and that the dialogue remains coherent and natural throughout the exchange. - If the system determines that an API call is not necessary, use this format: User - Thought - System. - Include intermediate thoughts where appropriate to capture the models internal reasoning, and clearly separate the different components of the format. # Available Functions: 1. CheckBalance(account_type) 2. TransferMoney(account_type, amount, recipient_account_name) . . . 52. FindAttractions(location) 53. GetWeather(city) # Output Format: - If an API Call is Needed: User: [User Input] Thought1: [I need to call an API] API Name: [API Call Name: CheckBalance(), TransferMoney(),..., FindAttractions(), GetWeather()] API Input: [The input parameters for the API] API Result: [API output result] Thought2: [2nd thought after API Result that ensure if the information is enough before the system response] System: [Your system response here] - If an API Call is Not Needed: User: [User Input] Thought: [I dont need an API and want to respond to the user] System: [Your system response here] Below is an example of the format: # Example: User: Add Don and Sherri to my \"Meditate to Sounds of Nature\" playlist. Thought1: The user wants to add two artists to specific playlist. Ill need to extract the playlist name and the artists names. API Name: AddToPlaylist API Input: playlist_name: Meditate to Sounds of Nature, artists: Don Sherri API Result: status: success, message: Don and Sherri have been added to your playlist. Thought2: The API call was successful, and the artists were added to the playlist. need to inform the user about this. System: Don and Sherri have been successfully added to your \"Meditate to Sounds of Nature\" playlist. Enjoy your music! User: Thank you for adding them. think can add more songs to the playlist. Thought: dont need an API call for that, want to respond to the user. System: Would you like to add more songs? User: Yes, please add \"Calm River\" to the playlist. Thought1: The user wants to add another song to the playlist. Ill note the song title and proceed with the request. API Name: AddToPlaylist API Input: playlist_name: Meditate to Sounds of Nature, songs: Calm River API Result: status: success, message: Calm River has been added to your playlist. Thought2: The song was successfully added. should let the user know. System: \"Calm River\" has been successfully added to your \"Meditate to Sounds of Nature\" playlist. Would you like to do anything else? User: No, thats all for now. Thank you! Thought: The user is satisfied and doesnt need further assistance. should acknowledge their gratitude. System: Youre welcome! If you need anything else, feel free to ask. Enjoy your relaxing music! # User Input: Table 6: API-driven conversation prompt to generate SGD fine-tuning samples with GPT4-o. Only functions used across all dialogues are included due to their large number."
        }
    ],
    "affiliations": [
        "Oumi",
        "University of Illinois Urbana-Champaign"
    ]
}