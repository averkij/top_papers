{
    "paper_title": "$π^3$: Scalable Permutation-Equivariant Visual Geometry Learning",
    "authors": [
        "Yifan Wang",
        "Jianjun Zhou",
        "Haoyi Zhu",
        "Wenzheng Chang",
        "Yang Zhou",
        "Zizun Li",
        "Junyi Chen",
        "Jiangmiao Pang",
        "Chunhua Shen",
        "Tong He"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce $\\pi^3$, a feed-forward neural network that offers a novel approach to visual geometry reconstruction, breaking the reliance on a conventional fixed reference view. Previous methods often anchor their reconstructions to a designated viewpoint, an inductive bias that can lead to instability and failures if the reference is suboptimal. In contrast, $\\pi^3$ employs a fully permutation-equivariant architecture to predict affine-invariant camera poses and scale-invariant local point maps without any reference frames. This design makes our model inherently robust to input ordering and highly scalable. These advantages enable our simple and bias-free approach to achieve state-of-the-art performance on a wide range of tasks, including camera pose estimation, monocular/video depth estimation, and dense point map reconstruction. Code and models are publicly available."
        },
        {
            "title": "Start",
            "content": "π3: Scalable Permutation-Equivariant Visual Geometry Learning Yifan Wang1 Zizun Li1 Jianjun Zhou123 Haoyi Zhu1 Wenzheng Chang1 Yang Zhou1 Junyi Chen1 Jiangmiao Pang1 Chunhua Shen2 Tong He13 1Shanghai AI Lab Equal Contribution 2ZJU 3SII"
        },
        {
            "title": "Project Page",
            "content": "5 2 0 2 7 1 ] . [ 1 7 4 3 3 1 . 7 0 5 2 : r Figure 1. π3 effectively reconstructs diverse set of open-domain images in feed-forward manner, encompassing various scenes such as indoor, outdoor, and aerial-view, as well as cartoons, with both dynamic and static content. 1 Figure 2. π3 demonstrates significant convergence acceleration (a) and enhanced scalability (b) compared to its non-equivariant counterpart. PI denotes the Permutation-EquIvariant property."
        },
        {
            "title": "Abstract",
            "content": "We introduce π3, feed-forward neural network that offers novel approach to visual geometry reconstruction, breaking the reliance on conventional fixed reference view. Previous methods often anchor their reconstructions to designated viewpoint, an inductive bias that can lead to instability and failures if the reference is suboptimal. In contrast, π3 employs fully permutation-equivariant architecture to predict affine-invariant camera poses and scale-invariant local point maps without any reference frames. This design makes our model inherently robust to input ordering and highly scalable. These advantages enable our simple and bias-free approach to achieve state-of-the-art performance on wide range of tasks, including camera pose estimation, monocular/video depth estimation, and dense point map reconstruction. Code and models are publicly available. 1. Introduction Learning always beats inductive biases. Jonathan T. Barron, Radiance Fields and the Future of Generative Media Visual geometry reconstruction, long-standing and fundamental problem in computer vision, holds substantial potential for applications such as augmented reality [7], robotics [50], and autonomous navigation [17]. While traditional methods addressed this challenge using iterative optimization techniques like Bundle Adjustment (BA) [11], the field has recently seen remarkable progress with feed-forward neural networks. End-to-end models like DUSt3R [39] and its successors have demonstrated the power of deep learning for reconstructing geometry from image pairs [13, 46], videos, or multi-view collections [34, 42, 47]. Despite these advances, critical limitation persists in both classical and modern approaches: the reliance on selecting single, fixed reference view. The camera coordinate Figure 3. Performance comparison across different reference frames. While previous methods, even with DINO-based selection, show inconsistent results, π3 consistently delivers superior and stable performance, demonstrating its robustness. system of this chosen view is treated as the global frame of reference, practice inherited from traditional Structurefrom-Motion (SfM) [4, 11, 20, 24] or Multi-view Stereo (MVS) [9, 25]. We contend that this design choice introduces an unnecessary inductive bias that fundamentally constrains the performance and robustness of feed-forward neural networks. As we demonstrate empirically, this reliance on an arbitrary reference makes existing methods, including the state-of-the-art (SOTA) VGGT [34], highly sensitive to the initial view selection. poor choice can lead to dramatic degradation in reconstruction quality, hindering the development of robust and scalable systems  (Fig. 3)  . To overcome this limitation, we introduce π3, robust, scalable, and fully permutation-equivariant method that eliminates reference view-based biases in visual geometry learning. π3 accepts varied inputsincluding single images, video sequences, or unordered image sets from static or dynamic sceneswithout designating reference view. Instead, our model predicts an affine-invariant camera pose and scale-invariant local pointmap for each frame, all relative to that frames own camera coordinate system. By eschewing order-dependent components like frame index positional embeddings and employing transformer architecture that alternates between view-wise and global self-attention (similar to [34]), π3 achieves true permutation equivariance. This guarantees consistent one-to-one mapping between visual inputs and the reconstructed geometry, making the model inherently robust to input order and immune to the reference view selection problem. Our design yields significant advantages. First, it demonstrates remarkable scalability, with performance consistently improving as the model size increases (Fig. 2 and Fig. 9). Second, it achieves significantly faster training convergence 2 (Fig. 2 and Fig. 9). Finally, our model is substantially more robust; in contrast to previous methods, it shows minimal performance degradation and low standard deviation when the reference frame is changed (Fig. 3 and Table 4.5). Through extensive experiments, π3 establishes new SOTA across numerous benchmarks and tasks. For example, it achieves comparable performance to existing methods like MoGe [37] in monocular depth estimation, and outperforms VGGT [34] in video depth estimation and camera pose estimation. On the Sintel benchmark, π3 reduces the camera pose estimation ATE from VGGTs 0.167 down to 0.074 and improves the scale-aligned video depth absolute relative error from 0.299 to 0.233. Furthermore, π3 is both lightweight and fast, achieving an inference speed of 57.4 FPS compared to DUSt3Rs 1.25 FPS and VGGTs 43.2 FPS. Its ability to reconstruct both static and dynamic scenes makes it robust and optimal solution for real-world applications. In summary, the contributions of this work are as follows: We are the first to systematically identify and challenge the reliance on fixed reference view in visual geometry reconstruction, demonstrating how this common design choice introduces detrimental inductive bias that limits model robustness and performance. We propose π3, novel, fully permutation-equivariant architecture that eliminates this bias. Our model predicts affine-invariant camera poses and scale-invariant pointmaps in purely relative, per-view manner, completely removing the need for global coordinate system. We demonstrate through extensive experiments that π3 establishes new state-of-the-art on wide range of benchmarks for camera pose estimation, monocular/video depth estimation, and pointmap reconstruction, outperforming prior leading methods. We show that our approach is not only substantially more robust to input view ordering and more scalable with model size, but also converges significantly faster during training. 2. Related Work 2.1. Traditional 3D Reconstruction Reconstructing 3D scenes from images is foundational problem in computer vision. Classical methods, such as Structure-from-Motion (SfM) [4, 11, 20, 24] and MultiView Stereo (MVS) [9, 25], have achieved considerable success. These techniques leverage the principles of multiview geometry to establish feature correspondences across images, from which they estimate camera poses and generate dense 3D point clouds. Although robust, particularly in controlled environments, these methods typically rely on complex, multi-stage pipelines. Moreover, they often involve time-consuming iterative optimization problems, such as Bundle Adjustment (BA), to jointly refine the 3D structure and camera poses. 2.2. Feed-Forward 3D Reconstruction Recently, feed-forward models have emerged as powerful alternative, capable of directly regressing the 3D structure of scene from set of images in single pass. Pioneering efforts in this domain, such as Dust3R [39], focused on processing image pairs to predict point cloud within the coordinate system of the first camera. While effective for two views, scaling this to larger scenes requires subsequent global alignment step, process that can be both time-consuming and prone to instability. Subsequent work has focused on overcoming this limitation. Fast3R [42] represents significant advance by enabling simultaneous inference on thousands of images, thereby eliminating the need for costly and fragile global alignment stage. Other approaches have explored simplifying the learning problem itself. For instance, FLARE [47] decomposes the task by first predicting camera poses and then estimating the scene geometry. The current state-of-the-art, VGGT [34], leverages multi-task learning and large-scale datasets to achieve superior accuracy and performance. unifying characteristic of these methods is their reliance on anchoring the predicted 3D structure to designated reference frame. Our work departs from this paradigm by presenting fundamentally different approach. 3. Method 3.1. Permutation-Equivariant Architecture To ensure our models output is invariant to the arbitrary ordering of input views, we designed our network ϕ to be permutation-equivariant. Let the input be sequence of images, = (I1, . . . , IN ), where each image Ii RHW 3. The network ϕ maps this sequence to corresponding tuple of output sequences: ϕ(S) = ((T1, . . . , TN ), (X1, . . . , XN ), (C1, . . . , CN )) (1) Here, Ti SE(3) R44 is the camera pose, Xi RHW 3 is the associated pixel-aligned 3D point map represented in its own camera coordinate system, and Ci RHW is the confidence map of Xi, each corresponding to the input image Ii. For any permutation π, let Pπ be an operator that permutes the order of sequence. The network ϕ satisfies the permutation-equivariant property: ϕ(Pπ(S)) = Pπ(ϕ(S)) (2) This means that permuting the input sequence, Pπ(S) = (Iπ(1), . . . , Iπ(N )), results in an identically permuted output 3 Figure 4. Unlike prior methods that designate reference view by concatenating special token (Type A) or adding learnable embedding (Type B), π3 achieves permutation equivariance by eliminating this requirement altogether. Instead, it employs relative supervision, making our approach inherently robust to the order of input views. tuple: Pπ(ϕ(S)) = (cid:0)(Tπ(1), . . . , Tπ(N )), (Xπ(1), . . . , Xπ(N )), (Cπ(1), . . . , Cπ(N ))(cid:1) (3) This property guarantees consistent one-to-one correspondence between each image and its respective output (e.g., geometry or pose). This design offers several key advantages. First, reconstruction quality becomes independent of the reference view selection, in contrast to prior methods that suffer from performance degradation when the reference view changes. Second, the model becomes more robust to uncertain or noisy observations. Finally, this architecture exhibits superior scalability. These claims are empirically validated in Section 4. To realize this equivariance in practice, our implementation (illustrated in Fig. 4) omits all order-dependent components. Specifically, we discard all order-dependent components, such as positional embeddings used to differentiate between frames and specialized learnable tokens that designate reference view, like the camera tokens found in VGGT [34]. Our pipeline begins by embedding each view into sequence of patch tokens using DINOv2 [18] backbone. These tokens are then processed through series of alternating view-wise and global self-attention layers, similar to [34], before final decoder generates the output. The detailed architecture of our model is provided in Appendix A.1. 3.2. Scale-Invariant Local Geometry For each input image Ii, our network predicts the geometry as pixel-aligned 3D point map ˆXi. Each point cloud is initially defined in its own local camera coordinate system. well-known challenge in monocular reconstruction is the inherent scale ambiguity. To address this, our network predicts the point clouds up to an unknown, yet consistent, scale factor across all images of given scene. Consequently, the training process requires aligning the predicted point maps, ( ˆX1, . . . , ˆXN ), with the corresponding ground-truth (GT) set, (X1, . . . , XN ). This alignment is accomplished by solving for single optimal scale factor, ˆs, which minimizes the depth-weighted L1 distance across the entire image sequence. The optimization problem is formulated as: = arg min (cid:88) HW (cid:88) i=1 j=1 1 zi,j sˆxi,j xi,j1 (4) Here, ˆxi,j R3 denotes the predicted 3D point at index of the point map ˆXi. Similarly, xi,j is its ground-truth counterpart in Xi. The term zi,j is the ground-truth depth, which is the z-component of xi,j. This problem is solved using the ROE solver proposed by [37]. Finally, the point cloud reconstruction loss, Lpoints, is defined using the optimal scale factor ˆs: Lpoints = (cid:88) HW (cid:88) i=1 j=1 1 zi,j ˆsˆxi,j xi,j1 (5) To encourage the reconstruction of locally smooth surfaces, we also introduce normal loss, Lnormal. For each point in the predicted point map ˆXi, its normal vector ˆni,j is computed from the cross product of the vectors to its adjacent neighbors on the image grid. We then supervise these 4 normals by minimizing the angle between them and their ground-truth counterparts ni,j: Lnormal = (cid:88) HW (cid:88) i= j=1 arccos(ˆni,j ni,j) (6) We supervise the predicted confidence map Ci using Binary Cross-Entropy (BCE) loss, denoted Lconf. The ground-truth target for each point is set to 1 if its L1 reconˆsˆxi,j xi,j1, is below threshold ϵ, struction error, and 0 otherwise. 1 zi,j 3.3. Affine-Invariant Camera Pose The models permutation equivariance, combined with the inherent scale ambiguity of multi-view reconstruction, implies that the output camera poses ( ˆT1, . . . , ˆTN ) are only defined up to an arbitrary similarity transformation. This specific type of affine transformation consists of rigid transformation and single, unknown global scale factor. ˆTj To resolve the ambiguity of the global reference frame, we supervise the network on the relative poses between views. The predicted relative pose ˆTij from view to is computed as: ˆTij = ˆT1 (7) Each predicted relative pose ˆTij is composed of rotation ˆRij SO(3) and translation ˆtij R3. While the relative rotation is invariant to this global transformation, the relative translations magnitude is ambiguous. We resolve this by leveraging the optimal scale factor, ˆs, that is computed by aligning the predicted point map to the ground truth (as detailed in previous section). This single, consistent scale factor is used to rectify all predicted camera translations, allowing us to directly supervise both the rotation and the correctly-scaled translation components. The camera loss Lcam is weighted sum of rotation loss term and translation loss term, averaged over all ordered view pairs where = j: Lcam = 1 (N 1) (cid:88) i=j (Lrot(i, j) + λLtrans(i, j)) (8) where λ is hyperparameter to balance the two terms. The rotation loss minimizes the geodesic distance (angle) between the predicted relative rotation ˆRij and its groundtruth target Rij: (cid:16) Tr Lrot(i, j) = arccos (Rij) ˆRij 2 (cid:17) 1 (9) For the translation loss, we compare our scaled prediction against the ground-truth relative translation, tij. We use the Huber loss, Hδ, for its robustness to outliers: Ltrans(i, j) = Hδ(sˆtij tij) (10) 5 Figure 5. Comparison of predicted pose distributions. Our predicted pose distribution exhibits clear low-dimensional structure. Our affine-invariant camera model builds on key insight: real-world camera paths are highly structured, not random. They typically lie on low-dimensional manifoldfor instance, camera orbiting an object moves along sphere, while car-mounted camera follows curve. We quantitatively analyze the structure of the predicted pose distributions in Fig. 5. The eigenvalue analysis confirms that the variance of our predicted poses is concentrated along significantly fewer principal components than VGGT, validating the low-dimensional structure of our output. We discuss this further in Appendix A.3. 3.4. Model Training Our model is trained end-to-end by minimizing composite loss function, L, which is weighted sum of the point reconstruction loss, the confidence loss, and the camera pose loss: = Lpoints + λnormalLnormal + λconfLconf + λcamLcam (11) To ensure robustness and wide applicability, we train the model on large-scale aggregation of 15 diverse datasets. This combined dataset provides extensive coverage of both indoor and outdoor environments, encompassing wide variety of scenes from synthetic renderings to real-world captures. The specific datasets include GTASfM [35], CO3D [21], WildRGB-D [41], Habitat [23], ARKitScenes [2], TartanAir [40], ScanNet [5], ScanNet++ [44], BlendedMVG [43], MatrixCity [15], MegaDepth [16], Hypersim [22], Taskonomy [45], Mid-Air [8], and an internal dynamic scene dataset. Details of model training can be found in Appendix A.2. 4. Experiments We evaluate our method on four tasks: camera pose estimation (Sec. 4.1), point map estimation (Sec. 4.2), video depth estimation (Sec. 4.3) and monocular depth estimation (Sec. 4.4). Across all tasks, our method achieves state-ofthe-art(SOTA) or comparable performance against existing feed-forward 3D reconstruction methods. To validate the effectiveness of our design, We also conduct several analyses: (1) robustness evaluation against input image sequence permutations (Sec. 4.5), (2) an ablation study on scale-invariant point maps and affine-invariant camera poses (Sec. 4.6), and Table 1. Camera Pose Estimation on RealEstate10K [49] and Co3Dv2 [21]. Metrics measure the ratio of angular accuracy of rotation/translation under an error of 30 degrees, the higher the better. All methods have seen Co3Dv2 samples during training time, while RealEstate10K is excluded from trainset except for CUT3R. Method Fast3R [42] CUT3R [36] FLARE [47] VGGT [34] π3 (Ours) RealEstate10K (unseen) Co3Dv RRA@30 RTA@30 AUC@30 RRA@30 RTA@30 AUC@30 99.05 99.82 99.69 99.97 99.99 81.86 95.10 95.23 93.13 95.62 61.68 81.47 80.01 77.62 85.90 97.49 96.19 96.38 98.96 99.05 91.11 92.69 93.76 97.13 97. 73.43 75.82 73.99 88.59 88.41 (3) scalability analysis with respect to model size (Sec. 4.7). 4.1. Camera Pose Estimation We assess predicted camera pose using two distinct sets of metrics: angular accuracy (following [33, 34, 39]) and distance error (following [36, 46, 48]). Angular Accuracy Metrics. Following prior work [34, 39], we evaluate predicted camera poses on the scene-level RealEstate10K [49] and object-centric Co3Dv2 [21] datasets, both featuring over 1000 test sequences. For each sequence, we randomly sample 10 images, form all possible pairs, and compute the angular errors of the relative rotation and translation vectors. This process yields the Relative Rotation Accuracy (RRA) and Relative Translation Accuracy (RTA) at given threshold (e.g., RRA@30 for 30 degrees). The Area Under the Curve (AUC) of the min(RRA,RTA)- threshold curve serves as unified metric. As shown in Tab. 1, our method sets new SOTA benchmark in zero-shot generalization on RealEstate10K and achieves performance comparable to the SOTA on the in-domain Co3Dv2 dataset. These results underscore our models strong generalization capabilities while maintaining excellent performance on familiar data distributions. Distance Error Metrics. Following [36], we report the Absolute Trajectory Error (ATE), Relative Pose Error for translation (RPE trans), and Relative Pose Error for rotation (RPE rot) on the synthetic outdoor Sintel [3] dataset, as well as the real-world indoor TUM-dynamics [29] and ScanNet [5] datasets. Predicted camera trajectories are aligned with the ground truth via Sim(3) transformation before calculating the errors. The results in Tab. 2 show that our method significantly outperforms other approaches on Sintel while achieving competitive SOTA results alongside VGGT on TUM-Dynamics and ScanNet. 4.2. Point Map Estimation Following the evaluation settings in [36], we evaluate the quality of reconstructed multi-view point maps on the scene-level 7-Scenes [27] and NRGBD [1] datasets under both sparse and dense view conditions. For sparse views, Table 2. Camera Pose Estimation on Sintel [3], TUMdynamics [29] and ScanNet [5]. Metrics measure the distance error of rotation/translation, the lower the better. All methods except Aether have seen ScanNet or ScanNet++ [44] samples during training time. Zero-shot pose estimation accuracy is evaluated on Sintel and TUM-dynamics for all methods. Method ATE RPE trans RPE rot ATE RPE trans RPE rot ATE RPE trans RPE rot Sintel TUM-dynamics ScanNet (seen) Fast3R [42] 0.371 CUT3R [36] 0.217 Aether [31] 0.189 FLARE [47] 0.207 VGGT [34] 0.167 π3 (Ours) 0.074 0.298 0.070 0.054 0.090 0.062 0.040 13.75 0.636 0.694 3.015 0.491 0.282 0.090 0.047 0.092 0.026 0.012 0.014 0.101 0.015 0.012 0.013 0.010 0.009 1.425 0.451 1.106 0.475 0.311 0. 0.155 0.094 0.176 0.064 0.035 0.031 0.123 0.022 0.028 0.023 0.015 0.013 3.491 0.629 1.204 0.971 0.382 0.347 Table 3. Point Map Estimation on DTU [12] and ETH3D [26]. Keyframes are selected every 5 images. DTU ETH3D Method Acc. Comp. N.C. Acc. Comp. N.C. Mean Med. Mean Med. Mean Med. Mean Med. Mean Med. Mean Med. Fast3R [42] 3.340 1.919 2.929 1.125 0.671 0.755 0.832 0.691 0.978 0.683 0.667 0.766 CUT3R [36] 4.742 2.600 3.400 1.316 0.679 0.764 0.617 0.525 0.747 0.579 0.754 0.848 FLARE [47] 2.541 1.468 3.174 1.420 0.684 0.774 0.464 0.338 0.664 0.395 0.744 0.864 VGGT [34] 1.338 0.779 1.896 0.992 0.676 0.766 0.280 0.185 0.305 0.182 0.853 0.950 π3 (Ours) 1.198 0.646 1.849 0.607 0.678 0.768 0.194 0.131 0.210 0.128 0.883 0.969 Table 4. Point Map Estimation on 7-Scenes [27] and NRGBD [1]. Keyframes are selected every 200 images (for 7-Scenes) and 500 images (for NRGBD) for sparse view, and every 40 images (for 7-Scenes) and 100 images (for NRGBD) for dense view. 7-Scenes NRGBD Method View Acc. Comp. NC. Acc. Comp. NC. Mean Med. Mean Med. Mean Med. Mean Med. Mean Med. Mean Med. Fast3R [42] CUT3R [36] FLARE [47] VGGT [34] π3 (Ours) Fast3R [42] CUT3R [36] FLARE [47] VGGT [34] π3 (Ours) sparse 0.096 0.065 0.145 0.093 0.672 0.760 0.135 0.091 0.163 0.104 0.759 0.877 0.094 0.051 0.101 0.050 0.703 0.804 0.104 0.041 0.079 0.031 0.822 0.968 0.085 0.058 0.142 0.104 0.695 0.779 0.053 0.024 0.051 0.025 0.877 0.988 0.046 0.026 0.057 0.034 0.728 0.842 0.051 0.029 0.066 0.038 0.890 0.981 0.048 0.028 0.072 0.047 0.742 0.842 0.026 0.015 0.028 0.014 0.916 0.992 dense 0.038 0.015 0.056 0.018 0.645 0.725 0.072 0.030 0.050 0.016 0.790 0.934 0.022 0.010 0.027 0.009 0.668 0.762 0.086 0.037 0.048 0.017 0.800 0.953 0.018 0.007 0.027 0.014 0.681 0.781 0.023 0.011 0.018 0.008 0.882 0.986 0.022 0.008 0.027 0.013 0.663 0.757 0.017 0.010 0.015 0.005 0.893 0.988 0.015 0.007 0.022 0.011 0.687 0.790 0.015 0.008 0.013 0.005 0.898 0.987 keyframes are sampled with stride of 200 (7-Scenes) or 500 (NRGBD), while for dense views, the stride is reduced to 40 (7-Scenes) or 100 (NRGBD). We also extend our evaluation to the object-centric DTU [12] and scene-level ETH3D [26] datasets, sampling keyframes every 5 images. Predicted point maps are aligned to the ground truth using the Umeyama algorithm for coarse Sim(3) alignment, followed by refinement with the Iterative Closest Point (ICP) algorithm. Consistent with prior works [1, 32, 36, 39], we report Accuracy (Acc.), Completion (Comp.), and Normal Consistency (N.C.) in Tab. 3 and Tab. 4. These results highlight the effectiveness of our method in broad spectrum of 3D reconstruction tasks, spanning object-level and scene-level cases (Tab. 3), and demonstrating robustness on both real-world and synthetic datasets (Tab. 4). To provide comprehensive evaluation, we further analyze performance under sparse-view and dense-view conditions using the 7-Scenes and NRGBD datasets (Tab. 4). 6 Figure 6. Qualitative comparison of multi-view 3D reconstruction. Compared to other multi-frame feed-forward reconstruction methods, π3 produces cleaner, more accurate and more complete reconstructions with fewer artifacts. The sparse-view setup, defined by limited inter-frame overlap, presents highly ill-posed problem requiring the model to exploit strong spatial priors. For completeness, we also consider the dense-view scenario, where ample observations facilitate reconstruction. The results confirm that our method achieves consistently robust performance in both challenging sparse-view and favorable dense-view settings. 4.3. Video Depth Estimation Following the methodology of [36, 46], we evaluate our method on the task of video depth estimation using the Sintel [3], Bonn [19], and KITTI [10] datasets. We report the Absolute Relative Error (Abs Rel) and the prediction accuracy at threshold of δ < 1.25. The metrics are evaluated under two alignment settings: (i) scale-only alignment and (ii) joint scale and 3D translation alignment. As reported in Tab. 5, our method achieves new stateof-the-art performance across all three datasets and both alignment settings within feed-forward 3D reconstruction methods. Notably, it also delivers exceptional efficiency, running at 57.4 FPS on KITTI, significantly faster than VGGT (43.2 FPS) and Aether (6.14 FPS), despite having smaller model size. 4.4. Monocular Depth Estimation Similar to our video depth evaluation, we compare our predicted monocular depth against other feed-forward reconstruction methods. Following [36, 46], we use four datasets to evaluate the accuracy of scale-invariant monocular depth. We continue to use the Absolute Relative Error (Abs Rel) and threshold accuracy (δ<1.25) as metrics. However, in this setting, each depth map is aligned independently with its ground truth, in contrast to the video depth evaluation, where single scale (and shift) factor is applied to the entire image sequence. As reported in Tab. 6, our method achieves state-of-theart results among multi-frame feed-forward reconstruction approaches, even though it is not explicitly optimized for single-frame depth estimation. Notably, it performs competitively with MoGe [37, 38], one of the top-performing monocular depth estimation models. 4.5. Robustness Evaluation key property of our proposed architecture is permutation equivariance, ensuring that its outputs are robust to variations in the input image sequence order. To empirically verify this, we conduct experiments on the DTU [12] and ETH3D [26] datasets. For each input sequence of length , we perform -fold separate inferences, where in each run we replace the original first frame with different frame from the sequence. We then compute the standard deviation of the reconstruction metrics across these outputs. lower standard deviation indicates higher robustness to input order variations. As reported in Tab. 4.5, our method achieves near-zero standard deviation across all metrics on DTU and ETH3D, outperforming existing approaches by several orders of magnitude. For instance, on DTU, our mean accuracy standard deviation is 0.003, while VGGT reports 0.033. On ETH3D, 7 Table 5. Video Depth Estimation on Sintel [3], Bonn [19] and KITTI [10]. FPS is evaluated on KITTI using one A800 GPU. Method Params Align Abs Rel δ < 1.25 Abs Rel δ < 1.25 Abs Rel δ < 1.25 Sintel Bonn KITTI DUSt3R [39] MASt3R [13] MonST3R [46] Fast3R [42] MVDUSt3R [30] CUT3R [36] Aether [31] FLARE [47] VGGT [34] π3 (Ours) DUSt3R [39] MASt3R [13] MonST3R [46] Fast3R [42] MVDUSt3R [30] CUT3R [36] Aether [31] FLARE [47] VGGT [34] π3 (Ours) 571M 689M 571M 648M 661M 793M 5.57B 1.40B 1.26B 959M 571M 689M 571M 648M 661M 793M 5.57B 1.40B 1.26B 959M scale scale & shift 0.662 0.558 0.399 0.638 0.805 0.417 0.324 0.729 0.299 0.233 0.570 0.480 0.402 0.518 0.619 0.534 0.314 0.791 0.230 0.210 0.434 0.487 0.519 0.422 0.283 0.507 0.502 0.336 0.638 0. 0.493 0.517 0.526 0.486 0.332 0.558 0.604 0.358 0.678 0.726 0.151 0.188 0.072 0.194 0.426 0.078 0.273 0.152 0.057 0.049 0.152 0.189 0.070 0.196 0.482 0.075 0.308 0.142 0.052 0.043 0.839 0.765 0.957 0.772 0.357 0.937 0.594 0.790 0.966 0.975 0.835 0.771 0.958 0.768 0.357 0.943 0.602 0.797 0.969 0.975 0.143 0.115 0.107 0.138 0.456 0.122 0.056 0.356 0.062 0. 0.135 0.115 0.098 0.139 0.401 0.111 0.054 0.357 0.052 0.037 0.814 0.848 0.884 0.834 0.342 0.876 0.978 0.570 0.969 0.986 0.818 0.849 0.883 0.808 0.355 0.883 0.977 0.579 0.968 0.985 FPS 1.25 1.01 1.27 65.8 0.69 6.98 6.14 1.75 43.2 57.4 1.25 1.01 1.27 65.8 0.69 6.98 6.14 1.75 43.2 57. Table 6. Monocular Depth Estimation on Sintel [3], Bonn [19], KITTI [10] and NYU-v2 [28]. Method DUSt3R [39] MASt3R [13] MonST3R [46] Fast3R [42] CUT3R [36] FLARE [47] VGGT [34] MoGe - v1 [37] - v2 [38] π3 (Ours) Sintel Bonn KITTI NYU-v2 Abs Rel δ < 1.25 Abs Rel δ < 1.25 Abs Rel δ < 1.25 Abs Rel δ < 1.25 0.488 0.413 0.402 0.544 0.418 0.606 0.335 0.273 - 0.273 - 0.277 0.277 0.532 0.569 0.525 0.509 0.520 0.402 0.599 0.695 - 0.695 - 0.687 0.614 0.139 0.123 0.069 0.169 0.058 0.130 0.053 0.050 - 0.050 - 0.063 0. 0.832 0.833 0.954 0.796 0.967 0.836 0.970 0.976 - 0.976 - 0.973 0.976 0.109 0.077 0.098 0.120 0.097 0.312 0.082 0.049 - 0.054 - 0.049 0.060 0.873 0.948 0.895 0.861 0.914 0.513 0.947 0.979 - 0.977 - 0.979 0.971 0.081 0.110 0.094 0.093 0.081 0.089 0.056 0.055 - 0.055 - 0.060 0.054 0.909 0.865 0.887 0.898 0.914 0.898 0.951 0.952 - 0.952 - 0.940 0.956 Table 7. Standard Deviation of Point Cloud Estimation on DTU [12] and ETH3D [26]. DTU ETH3D Method Acc. std. Comp. std. N.C. std. Acc. std. Comp. std. N.C. std. Mean Med. Mean Med. Mean Med. Mean Med. Mean Med. Mean Med. Fast3R [42] 0.578 0.451 0.677 0.376 0.007 0.009 0.182 0.205 0.381 0.273 0.047 0.072 CUT3R [36] 1.750 1.047 1.748 1.273 0.013 0.017 0.214 0.225 0.430 0.391 0.062 0.074 FLARE [47] 0.720 0.494 1.346 1.134 0.009 0.012 0.171 0.187 0.251 0.188 0.048 0.053 VGGT [34] 0.033 0.022 0.054 0.036 0.007 0.007 0.049 0.040 0.062 0.042 0.022 0.015 π3 (Ours) 0.003 0.002 0.006 0.003 0.001 0.001 0.000 0.000 0.000 0.000 0.001 0. our model achieves effectively zero variance. This stark contrast highlights the limitations of reference-frame-dependent methods, which exhibit significant sensitivity to input order. Our results provide compelling evidence that the proposed architecture is genuinely permutation-equivariant, ensuring consistent and order-independent 3D reconstruction. 4.6. Ablation Study To make fair comparison, we use modified version of VGGT [34], with the tracking and global point map heads disabled, as our Baseline model (1) [Baseline]. We sequentially integrated the Scale-Invariant Pointmap and the Affine8 Invariant Camera Pose, resulting in model (2) and model (3) [Full Model], respectively. Model (1) predicts camera and depth similar to VGGT. Both models (1) and (2) utilize an additional token to designate reference frame and predict camera poses within that frames coordinate system, while model (3) is fully permutation-equivariant. All models are trained for 80 epochs on images with resolution of 224 224. The reported metric is the mean of Accuracy and Completeness, averaged across the 7-Scenes [27] and NRGBD [1] datasets. The camera loss for the baseline model remains the same as in VGGT. The detailed architectures of Model (2) and Model (3) can be found in Appendix A.1. All models in the ablation study are trained from scratch, except for the encoder, which is initialized with DINOv2 [18] weights. As summarized in Table 8, the addition of the scaleinvariant pointmap improves the metric from 0.1498 to 0.1145. Our full model, which also incorporates the affineinvariant pose, achieves significantly better score of 0.0625. This confirms that both components are essential and contribute substantially to the final accuracy and robustness of our architecture. Table 8. Ablation study on the key components of our model. We show how the performance metric (lower is better) improves as each component is added to the baseline. ID Model Configuration (1) Baseline (2) Baseline + Scale-Invariant Pointmap (3) Model (2) + Affine-Invariant Camera Pose (Full Model) Metric 0.1498 0.1145 0.0625 Figure 7. Qualitative comparison of in-the-wild multi-view 3D reconstruction. π3 demonstrates superior robustness on challenging in-the-wild sequences, consistently producing more coherent and complete 3D structures for both dynamic and complex static scenes compared to other feed-forward approaches. 4.7. Scalability and Performance Evaluation Building on the experimental setup described in Section 4.6, we further evaluate the scalability of our fully permutationequivariant (PI) architecture. We compare it against the non-PI baseline (model (2)) across three model sizes: Small (196.49M parameters), Base (390.13M), and Large (892.37M). The Small model employs ViT-S encoder, with alternating attention modules configured with 384dimensional hidden state, 6 heads, and 24 layers. The Base model uses ViT-B encoder alongside attention modules with 768-dimensional hidden state, 12 heads, and 24 layers. The Large model adopts ViT-L encoder, with 1024dimensional hidden state, 16 heads, and 36 layers. As shown in Figure 2, our PI architecture demonstrates clear and consistent advantages. It not only achieves superior final performance with the Large model, achieving 45% improvement over its baseline, but also exhibits significantly faster convergence, reflecting enhanced sample efficiency. Importantly, these benefits hold across all three model scales, confirming the strong scalability of our approach. Additional experiments are provided in Appendix A.4. 5. Conclusion In this work, we introduced π3, feed-forward neural network that presents new paradigm for visual geometry reconstruction by eliminating the reliance on fixed reference view. By leveraging fully permutation-equivariant architecture, our model is inherently robust to input ordering and highly scalable. This design choice removes critical inductive bias found in previous methods, allowing our simple yet powerful approach to achieve state-of-the-art performance on wide array of tasks, including camera pose estimation, depth estimation, and dense reconstruction. π3 demonstrates that reference-free systems are not only viable but can lead to more stable and versatile 3D vision models."
        },
        {
            "title": "References",
            "content": "[1] Dejan Azinovic, Ricardo Martin-Brualla, Dan Goldman, Matthias Nießner, and Justus Thies. Neural rgb-d surface reconstruction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6290 6301, 2022. [2] Gilad Baruch, Zhuoyuan Chen, Afshin Dehghan, Tal Dimry, Yuri Feigin, Peter Fu, Thomas Gebauer, Brandon Joffe, Daniel Kurz, Arik Schwartz, et al. Arkitscenes: diverse real-world dataset for 3d indoor scene understanding using mobile rgb-d data. arXiv preprint arXiv:2111.08897, 2021. [3] Aljaz Bozic, Pablo Palafox, Justus Thies, Angela Dai, and Matthias Nießner. Transformerfusion: Monocular rgb scene reconstruction using transformers. Advances in Neural Information Processing Systems, 34:14031414, 2021. [4] Hainan Cui, Xiang Gao, Shuhan Shen, and Zhanyi Hu. Hsfm: Hybrid structure-from-motion. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 12121221, 2017. [5] Angela Dai, Angel Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Nießner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 58285839, 2017. [6] Siyan Dong, Shuzhe Wang, Shaohui Liu, Lulu Cai, Qingnan Fan, Juho Kannala, and Yanchao Yang. Reloc3r: Large-scale training of relative camera pose regression for generalizable, fast, and accurate visual localization. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1673916752, 2025. [7] Jakob Engel, Kiran Somasundaram, Michael Goesele, Albert Sun, Alexander Gamino, Andrew Turner, Arjang Talattof, Arnie Yuan, Bilal Souti, Brighid Meredith, et al. Project aria: new tool for egocentric multi-modal ai research. arXiv preprint arXiv:2308.13561, 2023. [8] Michael Fonder and Marc Van Droogenbroeck. Mid-air: multi-modal dataset for extremely low altitude drone flights. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops, pages 00, 2019. [9] Yasutaka Furukawa, Carlos Hernández, et al. Multi-view stereo: tutorial. Foundations and trends in Computer Graphics and Vision, 9(1-2):1148, 2015. [10] Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel Urtasun. Vision meets robotics: The kitti dataset. The international journal of robotics research, 32(11):12311237, 2013. [11] Richard Hartley and Andrew Zisserman. Multiple view geometry in computer vision. Cambridge university press, 2003. [12] Rasmus Jensen, Anders Dahl, George Vogiatzis, Engin Tola, and Henrik Aanæs. Large scale multi-view stereopsis evaluation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 406413, 2014. [13] Vincent Leroy, Yohann Cabon, and Jérôme Revaud. Grounding image matching in 3d with mast3r. In European Conference on Computer Vision, pages 7191. Springer, 2024. [14] Jake Levinson, Carlos Esteves, Kefan Chen, Noah Snavely, Angjoo Kanazawa, Afshin Rostamizadeh, and Ameesh Makadia. An analysis of svd for deep rotation estimation. Advances in Neural Information Processing Systems, 33:2255422565, 2020. [15] Yixuan Li, Lihan Jiang, Linning Xu, Yuanbo Xiangli, Zhenzhi Wang, Dahua Lin, and Bo Dai. Matrixcity: large-scale city dataset for city-scale neural rendering and beyond. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 32053215, 2023. [16] Zhengqi Li and Noah Snavely. Megadepth: Learning singleview depth prediction from internet photos. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 20412050, 2018. [17] Raul Mur-Artal, Jose Maria Martinez Montiel, and Juan Tardos. Orb-slam: versatile and accurate monocular slam system. IEEE transactions on robotics, 31(5):11471163, 2015. [18] Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. [19] Emanuele Palazzolo, Jens Behley, Philipp Lottes, Philippe Giguere, and Cyrill Stachniss. Refusion: 3d reconstruction in dynamic environments for rgb-d cameras exploiting residuals. In 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 78557862. IEEE, 2019. [20] Linfei Pan, Dániel Baráth, Marc Pollefeys, and Johannes Schönberger. Global structure-from-motion revisited. In European Conference on Computer Vision, pages 5877. Springer, 2024. [21] Jeremy Reizenstein, Roman Shapovalov, Philipp Henzler, Luca Sbordone, Patrick Labatut, and David Novotny. Common objects in 3d: Large-scale learning and evaluation of real-life 3d category reconstruction. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1090110911, 2021. [22] Mike Roberts, Jason Ramapuram, Anurag Ranjan, Atulit Kumar, Miguel Angel Bautista, Nathan Paczan, Russ Webb, and Joshua Susskind. Hypersim: photorealistic synthetic dataset for holistic indoor scene understanding. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1091210922, 2021. 10 [23] Manolis Savva, Abhishek Kadian, Oleksandr Maksymets, Yili Zhao, Erik Wijmans, Bhavana Jain, Julian Straub, Jia Liu, Vladlen Koltun, Jitendra Malik, et al. Habitat: platform for embodied ai research. In Proceedings of the IEEE/CVF international conference on computer vision, pages 9339 9347, 2019. [24] Johannes Schonberger and Jan-Michael Frahm. Structurefrom-motion revisited. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4104 4113, 2016. [25] Johannes Schönberger, Enliang Zheng, Jan-Michael Frahm, and Marc Pollefeys. Pixelwise view selection for unstructured multi-view stereo. In Computer VisionECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part III 14, pages 501518. Springer, 2016. [26] Thomas Schops, Johannes Schonberger, Silvano Galliani, Torsten Sattler, Konrad Schindler, Marc Pollefeys, and Andreas Geiger. multi-view stereo benchmark with highresolution images and multi-camera videos. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 32603269, 2017. [27] Jamie Shotton, Ben Glocker, Christopher Zach, Shahram Izadi, Antonio Criminisi, and Andrew Fitzgibbon. Scene coordinate regression forests for camera relocalization in rgbd images. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 29302937, 2013. [28] Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob Fergus. Indoor segmentation and support inference from rgbd images. In European conference on computer vision, pages 746760. Springer, 2012. [29] Jürgen Sturm, Nikolas Engelhard, Felix Endres, Wolfram Burgard, and Daniel Cremers. benchmark for the evaluation of rgb-d slam systems. In 2012 IEEE/RSJ international conference on intelligent robots and systems, pages 573580. IEEE, 2012. [30] Zhenggang Tang, Yuchen Fan, Dilin Wang, Hongyu Xu, Rakesh Ranjan, Alexander Schwing, and Zhicheng Yan. Mvdust3r+: Single-stage scene reconstruction from sparse views in 2 seconds. arXiv preprint arXiv:2412.06974, 2024. [31] Aether Team, Haoyi Zhu, Yifan Wang, Jianjun Zhou, Wenzheng Chang, Yang Zhou, Zizun Li, Junyi Chen, Chunhua Shen, Jiangmiao Pang, and Tong He. Aether: Geometric-aware unified world modeling. arXiv preprint arXiv:2503.18945, 2025. [32] Hengyi Wang and Lourdes Agapito. 3d reconstruction with spatial memory. arXiv preprint arXiv:2408.16061, 2024. [33] Jianyuan Wang, Christian Rupprecht, and David Novotny. Posediffusion: Solving pose estimation via diffusion-aided bundle adjustment. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 97739783, 2023. [34] Jianyuan Wang, Minghao Chen, Nikita Karaev, Andrea Vedaldi, Christian Rupprecht, and David Novotny. Vggt: arXiv preprint Visual geometry grounded transformer. arXiv:2503.11651, 2025. [35] Kaixuan Wang and Shaojie Shen. Flow-motion and depth network for monocular stereo and beyond. IEEE Robotics and Automation Letters, 5(2):33073314, 2020. [36] Qianqian Wang, Yifei Zhang, Aleksander Holynski, Alexei Efros, and Angjoo Kanazawa. Continuous 3d perception model with persistent state. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 10510 10522, 2025. [37] Ruicheng Wang, Sicheng Xu, Cassie Dai, Jianfeng Xiang, Yu Deng, Xin Tong, and Jiaolong Yang. Moge: Unlocking accurate monocular geometry estimation for open-domain images with optimal training supervision. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 52615271, 2025. [38] Ruicheng Wang, Sicheng Xu, Yue Dong, Yu Deng, Jianfeng Xiang, Zelong Lv, Guangzhong Sun, Xin Tong, and Jiaolong Yang. Moge-2: Accurate monocular geometry with metric scale and sharp details. arXiv preprint arXiv:2507.02546, 2025. [39] Shuzhe Wang, Vincent Leroy, Yohann Cabon, Boris Chidlovskii, and Jerome Revaud. Dust3r: Geometric 3d In Proceedings of the IEEE/CVF Convision made easy. ference on Computer Vision and Pattern Recognition, pages 2069720709, 2024. [40] Wenshan Wang, Delong Zhu, Xiangwei Wang, Yaoyu Hu, Yuheng Qiu, Chen Wang, Yafei Hu, Ashish Kapoor, and Sebastian Scherer. Tartanair: dataset to push the limits of visual slam. In 2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 49094916. IEEE, 2020. [41] Hongchi Xia, Yang Fu, Sifei Liu, and Xiaolong Wang. Rgbd objects in the wild: scaling real-world 3d object learning from rgb-d videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22378 22389, 2024. [42] Jianing Yang, Alexander Sax, Kevin Liang, Mikael Henaff, Hao Tang, Ang Cao, Joyce Chai, Franziska Meier, and Matt Feiszli. Fast3r: Towards 3d reconstruction of 1000+ images in one forward pass. arXiv preprint arXiv:2501.13928, 2025. [43] Yao Yao, Zixin Luo, Shiwei Li, Jingyang Zhang, Yufan Ren, Lei Zhou, Tian Fang, and Long Quan. Blendedmvs: largescale dataset for generalized multi-view stereo networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 17901799, 2020. [44] Chandan Yeshwanth, Yueh-Cheng Liu, Matthias Nießner, and Angela Dai. Scannet++: high-fidelity dataset of 3d indoor scenes. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1222, 2023. [45] Amir Zamir, Alexander Sax, William Shen, Leonidas Guibas, Jitendra Malik, and Silvio Savarese. Taskonomy: Disentangling task transfer learning. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 37123722, 2018. [46] Junyi Zhang, Charles Herrmann, Junhwa Hur, Varun Jampani, Trevor Darrell, Forrester Cole, Deqing Sun, and Ming-Hsuan Yang. Monst3r: simple approach for estimating geometry in the presence of motion. arXiv preprint arXiv:2410.03825, 2024. 11 [47] Shangzhan Zhang, Jianyuan Wang, Yinghao Xu, Nan Xue, Christian Rupprecht, Xiaowei Zhou, Yujun Shen, and Gordon Wetzstein. Flare: Feed-forward geometry, appearance and camera estimation from uncalibrated sparse views. arXiv preprint arXiv:2502.12138, 2025. [48] Wang Zhao, Shaohui Liu, Hengkai Guo, Wenping Wang, and Yong-Jin Liu. Particlesfm: Exploiting dense point trajectories In European for localizing moving cameras in the wild. Conference on Computer Vision, pages 523542. Springer, 2022. [49] Tinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe, and Noah Snavely. Stereo magnification: Learning view synthesis using multiplane images. arXiv preprint arXiv:1805.09817, 2018. [50] Haoyi Zhu, Yating Wang, Di Huang, Weicai Ye, Wanli Ouyang, and Tong He. Point cloud matters: Rethinking the impact of different observation spaces on robot learning. Advances in Neural Information Processing Systems, 37: 7779977830, 2024. 12 A. Appendix A.1. Architecture Details The encoder and alternating attention modules are the same as those in VGGT [34], with the exception that we use only 36 layers for the alternating attention module, whereas VGGT uses 48. The decoders for camera poses, local point maps, and confidence scores share the same architecture but do not share weights. This architecture is lightweight, 5-layer transformer that applies self-attention exclusively to the features of each individual image. Following the decoder, the output heads vary by task. The heads for local point maps and confidence scores consist of simple MLP followed by pixel shuffle operation. For camera poses, the head is adapted from Reloc3r [6] and uses an MLP, average pooling, and another MLP. The rotation is initially predicted in 9D representation [14] and is then converted to 33 rotation matrix via SVD orthogonalization. A.2. Training Details We train π3 in two stages, process similar to Dust3R [39]. First, the model is trained on low resolution of 224 224 pixels. Then, it is fine-tuned on images of random resolutions where the total pixel count is between 100,000 and 255,000 and the aspect ratio is sampled from the range [0.5, 2.0], strategy similar to MoGe [37]. We use dynamic batch sizing strategy similar to VGGT. In the first stage, we sample 64 images per GPU, and in the second stage, we sample 48 images per GPU. Each batch is composed of 2 to 24 images. Each training stage runs for 100 epochs, with each epoch comprising 1,000 iterations. Our final model is not trained from scratch. Instead, we initialize the weights for the encoder and the alternating attention module from the pre-trained VGGT model, and we keep the encoder frozen during training. For each stage, we use the Adam optimizer with learning rate that starts at 1 104 and then decays. We train the first stage on 16 A100 GPUs and the second stage on 64 A100 GPUs. Figure 8. Comparison of predicted pose distributions. We visualize the predicted pose distributions in 3D space. π3 shows clear low-dimensional structure, while VGGTs distribution is scattered. 13 A.3. Discussion for Predicted Pose Distribution In Fig. 8, we present 3D visualization of the camera pose distributions predicted by our method and by VGGT. To visualize the rotational component, we map it to the RGB color space. It is clear that our predictions form distinct lowdimensional structure, while the distribution from VGGT is much more scattered and random. That may be one of the potential reasons for the fast speed of training convergence. A.4. More Experiments on Scalability and Convergence Speed To evaluate the scalability and convergence speed of our approach, we conducted further experiments using three different model sizes. The results in Fig. 9, plotted as metric curves against training epochs, demonstrate that our method achieves consistent performance gains across all scales. Furthermore, our models converge significantly faster than the baseline. These findings underscore the effectiveness and importance of our permutation-equivariant design. Figure 9. Comparison of the PI model (solid colored lines) against the non-PI baseline (dashed lines) across three model scales. The metric shown is the average of Accuracy and Completeness on the 7-Scenes [27] and NRGBD [1] datasets. The shaded area visually represents the performance improvement gained by the PI model. A.5. Limitations Our model demonstrates strong performance, but it also has several key limitations. First, it is unable to handle transparent objects, as our model does not explicitly account for complex light transport phenomena. Second, compared to contemporary diffusion-based approaches, our reconstructed geometry lacks the same level of fine-grained detail. Finally, the point cloud generation relies on simple upsampling mechanism using an MLP with pixel shuffling. While efficient, this design can introduce noticeable grid-like artifacts, particularly in regions with high reconstruction uncertainty."
        }
    ],
    "affiliations": [
        "SII",
        "Shanghai AI Lab",
        "ZJU"
    ]
}