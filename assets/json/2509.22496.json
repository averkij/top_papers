{
    "paper_title": "Where MLLMs Attend and What They Rely On: Explaining Autoregressive Token Generation",
    "authors": [
        "Ruoyu Chen",
        "Xiaoqing Guo",
        "Kangwei Liu",
        "Siyuan Liang",
        "Shiming Liu",
        "Qunli Zhang",
        "Hua Zhang",
        "Xiaochun Cao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Multimodal large language models (MLLMs) have demonstrated remarkable capabilities in aligning visual inputs with natural language outputs. Yet, the extent to which generated tokens depend on visual modalities remains poorly understood, limiting interpretability and reliability. In this work, we present EAGLE, a lightweight black-box framework for explaining autoregressive token generation in MLLMs. EAGLE attributes any selected tokens to compact perceptual regions while quantifying the relative influence of language priors and perceptual evidence. The framework introduces an objective function that unifies sufficiency (insight score) and indispensability (necessity score), optimized via greedy search over sparsified image regions for faithful and efficient attribution. Beyond spatial attribution, EAGLE performs modality-aware analysis that disentangles what tokens rely on, providing fine-grained interpretability of model decisions. Extensive experiments across open-source MLLMs show that EAGLE consistently outperforms existing methods in faithfulness, localization, and hallucination diagnosis, while requiring substantially less GPU memory. These results highlight its effectiveness and practicality for advancing the interpretability of MLLMs. The code is available at https://github.com/RuoyuChen10/EAGLE."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 2 ] . [ 1 6 9 4 2 2 . 9 0 5 2 : r a"
        },
        {
            "title": "Preprint",
            "content": "WHERE MLLMS ATTEND AND WHAT THEY RELY ON: EXPLAINING AUTOREGRESSIVE TOKEN GENERATION Ruoyu Chen1,2, Xiaoqing Guo3, Kangwei Liu1,2, Siyuan Liang4, Shiming Liu5, Qunli Zhang6, Hua Zhang1, Xiaochun Cao7, 1Institute of Information Engineering, Chinese Academy of Sciences 2School of Cyber Security, University of Chinese Academy of Sciences 3Department of Computer Science, Hong Kong Baptist University 5RAMS Lab, Huawei Technologies Co., Ltd. 6RAMS Lab, Munich Research Center, Huawei Technologies Düsseldorf GmbH 7School of Cyber Science and Technology, Shenzhen Campus of Sun Yat-sen University 4School of Computing, NUS chenruoyu@iie.ac.cn caoxiaochun@mail.sysu.edu.cn"
        },
        {
            "title": "ABSTRACT",
            "content": "Multimodal large language models (MLLMs) have demonstrated remarkable capabilities in aligning visual inputs with natural language outputs. Yet, the extent to which generated tokens depend on visual modalities remains poorly understood, limiting interpretability and reliability. In this work, we present EAGLE, lightweight black-box framework for explaining autoregressive token generation in MLLMs. EAGLE attributes any selected tokens to compact perceptual regions while quantifying the relative influence of language priors and perceptual evidence. The framework introduces an objective function that unifies sufficiency (insight score) and indispensability (necessity score), optimized via greedy search over sparsified image regions for faithful and efficient attribution. Beyond spatial attribution, EAGLE performs modality-aware analysis that disentangles what tokens rely on, providing fine-grained interpretability of model decisions. Extensive experiments across open-source MLLMs show that EAGLE consistently outperforms existing methods in faithfulness, localization, and hallucination diagnosis, while requiring substantially less GPU memory. These results highlight its effectiveness and practicality for advancing the interpretability of MLLMs. The code is available at https://github.com/RuoyuChen10/EAGLE."
        },
        {
            "title": "INTRODUCTION",
            "content": "Multimodal large language models (MLLMs) (Achiam et al., 2023; Wang et al., 2025; Bai et al., 2025; Comanici et al., 2025) have achieved significant progress in visionlanguage understanding and generation. By jointly modeling visual and textual modalities, they can now perform wide range of tasks, such as image captioning and visual question answering (VQA) (Li et al., 2025b). These advances have enabled MLLMs to approach human-level performance on many benchmarks and to underpin various real-world applications (Liang et al., 2024; Li et al., 2024). However, alongside these advances come critical challenges in transparency and reliability (Zhang et al., 2025b). As parameter scales and modality coverage continue to expand, MLLMs become increasingly opaque, making it difficult to trace how specific inputs influence generated outputs (Xing et al., 2025; Chen et al., 2025c;b). Furthermore, MLLMs are susceptible to hallucinations (Chen et al., 2025b;a), which undermine trust in safety-critical domains such as healthcare (Ahmed et al., 2025) and autonomous driving (Chen et al., 2024a). These limitations highlight the urgent need for efficient and faithful attribution methods to improve decision transparency, diagnose errors, and enhance the safety and trustworthiness of MLLMs (Lin et al., 2025; Dang et al., 2024; Liang et al., 2025b; 2023; 2025a; Lu et al., 2025). Attribution in MLLMs is particularly challenging because they generate tokens autoregressively, making classification-based attribution methods difficult to adapt. Attention visualization approaches (Ben Corresponding author."
        },
        {
            "title": "Preprint",
            "content": "Figure 1: EAGLE attribution which perceptual regions drive the generation (Where MLLMs Attend) and quantifies modality reliance (What They Rely On). Melech Stan et al., 2024) often fail to capture complex cross-modal interactions, while gradient-based extensions (Zhang et al., 2025b; Xing et al., 2025) aggregate token logits but remain confounded by textual priors. More recently, TAM (Li et al., 2025a) employed activation maps to explain individual tokens and showed promising localization on Qwen2-VL (Wang et al., 2024), yet it cannot generalize to all MLLMs or capture multi-token contributions. In summary, attribution methods based on activation maps or gradients face inherent limitations: (1) activation-based approaches lack direct causal link between inputs and outputs, reflecting only intermediate layer preferences often misaligned with human intuition; and (2) gradient-based approaches are sensitive to cumulative effects in long sequences and easily disturbed by noise and modality imbalance. To more faithfully explain the generation of MLLMs, we propose EAGLE (Explaining Autoregressive Generation by Language priors or Evidence), black-box attribution framework for interpreting autoregressive token generation. As shown in Fig 1, our method supports attribution for any chosen set of output tokens, revealing the perceptual regions that drive their generation and quantifying the relative roles of language priors and visual evidence. Inspired by submodular subset selection, we aim to find the minimal set of perceptual regions that maximizes token logits, conditioned on the prompt and context. We design an objective function with two components: the insight score, capturing regions sufficient for generation, and the necessity score, identifying regions whose removal impairs generation. By applying greedy search over sparsified image regions, we construct an ordered ranking that attributes which perceptual regions promote generation in MLLMs, addressing the question of Where MLLMs Attend. Beyond spatial attribution, we also assess What They Rely On. By tracking how token logits evolve as salient regions are progressively introduced, we measure whether each token depends more on perceptual evidence or language priors, offering faithful and comprehensive view of model decisions. We evaluate our method on open-source MLLMs, including LLaVA-1.5 (Liu et al., 2024), Qwen2.5VL (Bai et al., 2025), and InternVL3.5 (Wang et al., 2025), using the MS COCO Lin et al. (2014) and MMVP (Tong et al., 2024) datasets for image captioning and VQA. On faithfulness metrics, our approach outperforms existing attribution methods (LLaVA-CAM (Zhang et al., 2025b), IGOS++ (Xing et al., 2025), and TAM (Li et al., 2025a)) by an average of 20.0% in insertion and 13.4% in deletion for image captioning, and by 20.6% and 8.1% on the same metrics for VQA. At the word level, our method achieves more rational explanations of object tokens, surpassing TAM by 36.42% and 42.63% on the Pointing Game under box-level and mask-level annotations, respectively. Finally, on the RePOPE benchmark Neuhaus & Hein (2025) for object hallucination, our method accurately localizes the visual elements responsible for hallucinations and mitigates them by removing only minimal set of interfering regions. These results demonstrate the versatility of our method across diverse tasks and benchmarks. In summary, the contributions of this paper are: 1. We propose EAGLE, lightweight black-box attribution framework for autoregressive token generation, which attributes any selected set of tokens to compact perceptual regions with low GPU memory cost. 2. An objective function that unifies sufficiency (insight score) and indispensability (necessity score), optimized via greedy search strategy that balances interpretability with efficiency, yielding faithful attributions. 3. modality analysis that quantifies whether each generated token is driven more by language priors or perceptual evidence, enabling finer-grained interpretability."
        },
        {
            "title": "Preprint",
            "content": "4. Experiments across diverse MLLMs show state-of-the-art interpretability in faithfulness, localization, and hallucination diagnosis."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Multimodal LLMs Attribution. Research on input-level attribution for Multimodal Large Language Models (MLLMs) is still nascent. LVLM-Interpret (Ben Melech Stan et al., 2024) visualizes alignment between LLaVA outputs and images using raw attention, while LLaVA-CAM (Zhang et al., 2025b) adapts Smooth-CAM (Omeiza et al., 2019) to token-level probabilities, but both suffer from layer sensitivity and limited faithfulness. VPS (Chen et al., 2025b) introduces search-based method for object-level tasks, yet it is restricted to grounding and detection. IGOS++(Xing et al., 2025) identifies visually aligned tokens but remains parameter-sensitive. More recently, TAM(Li et al., 2025a) reduces contextual noise in activation maps, improving token-level attribution. However, gradient-based methods remain memory-intensive and unstable. In contrast, we propose blackbox attribution framework that localizes outputs to compact input regions without relying on token selection, quantifies the influence of language priors versus perceptual evidence, and further explains the causes of object hallucinations in MLLMs. Interpreting Hallucinations in MLLMs. Several studies have applied interpretability techniques to examine hallucinations. Jiang et al. (2025) investigated how image latent representations in visionlanguage models are projected into the language vocabulary, thereby shaping the models confidence in both real and hallucinatory objects, and further proposed representation correction method to mitigate hallucinations. Zhang et al. (2025a) examined whether MLLMs attend to incorrect regions when producing wrong answers, leveraging their internal attention maps. VaLSe (Chen et al., 2025a) employs gradientand attention-based attribution maps to identify noisy regions that contribute to hallucinations. In this work, we primarily focus on interpreting which input regions lead to incorrect decisions, aiming to suppress hallucinations by removing as few regions as possible."
        },
        {
            "title": "3.1 TASK FORMULATION",
            "content": "For multimodal large language model (MLLM), such as VLLM, given an input image and textual prompt, the model generates an output sequence = [y1, y2, . . . , yl]. Let p() denote the conditional probability distribution over the token vocabulary. The probability of generating each token is expressed as p(yt x, Prompt, y<t), where y<t = [y1, . . . , yt1] denotes the previously generated tokens. For interpretability analysis, our objective is to identify the image regions that most strongly drive the models decisions. Image features in MLLMs are typically high-dimensional and informationdense but also redundant and less directly interpretable than text. We therefore focus on decomposing into semantically meaningful subregions. Specifically, the image is sparsified into = {x1, x2, . . . , xN } using the SLICO (Achanta et al., 2012) superpixel segmentation method, where xi denotes the i-th subregion. The attribution problem is then cast as subset selection task (Chen et al., 2024b): maxSV,S<k F(S), where is the maximum number of selected subregions and F() is set function measuring interpretability. Beyond the unordered case, attribution also depends on the order in which regions contribute to the decision. We therefore extend the formulation to ordered subsets: max πP(V ),π<k π (cid:88) r=1 F(π:r), (1) where π is an ordered subset, P(V ) the collection of all ordered subsets of , and the prefix length. The problem thus reduces to designing F() and optimizing it efficiently."
        },
        {
            "title": "3.2 EXPLAINING AUTOREGRESSIVE GENERATION",
            "content": "We propose EAGLE, novel attribution framework for explaining autoregressive token generation, as shown in Fig. 2. For the set function in Eq. 1, we design submodular-inspired objective to measure"
        },
        {
            "title": "Preprint",
            "content": "Figure 2: Overview of the proposed EAGLE framework. The input image is first sparsified into sub-regions, then attributed via greedy search with the designed objective, and finally analyzed for modality relevance between language priors and perceptual evidence. interpretability. This objective encourages diminishing returns as more regions are added, although it may not be strictly submodular for MLLMs. Let = [t1, t2, . . . , tn] denote the token positions of interest, and = [v1, v2, . . . , vn] their corresponding vocabulary indices. Insight Score: key metric for interpretability is the identification of the minimal set of input regions sufficient to maximize the probability of generating the target label, thereby highlighting the most informative evidence underlying the models decision. Given an input prompt and an image x, we denote the corresponding target sequence as y, which is generated conditioned on both. For candidate subregion S, the insight score is defined as: sinsight(S, Prompt, y, T, V) = (cid:88) p(yti = vi S, Prompt, y<ti) , (2) i=1 where p(yti = vi S, Prompt, y<ti ) denotes the probability of generating the ground-truth token yti at position ti, conditioned on the selected subregion S, the input prompt, and the previously generated tokens. Necessity Score: Another key metric for interpretability is the identification of the minimal set of input regions whose removal leads to significant decrease in the probability of generating the target label, thereby revealing the indispensable evidence that the model relies on. Formally, for candidate subregion S, the necessity score is defined as: (cid:88) (cid:17) (cid:16) snecessity(V S, Prompt, y, T, V) = 1 p(yti = vi S, Prompt, y<ti) , (3) i=1 where denotes the remaining regions after removing S. This score provides an effective criterion in the search phase for uncovering subtle but critical regions that contribute to the final decision. Objective Function: We integrate the insight and necessity scores into unified objective function that jointly captures sufficiency and necessity for interpreting autoregressive token generation: F(S, V, Prompt, y, T, V) = sinsight(S, Prompt, y, T, V)+snecessity(V S, Prompt, y, T, V), (4) where larger objective value indicates that the selected input combination is more important and thus provides stronger interpretability. Saliency Map Generation: To optimize the objective in Eq. 1, an P-hard problem, we adopt greedy search strategy. At each step, the region yielding the largest marginal gain with respect to the objective function is added to the current set until the budget is reached, producing an ordered set π. Beyond ranking, it is also important to assess the relative saliency differences among subregions. We evaluate these differences by examining the marginal gains of the objective function as the ordered subset expands. larger gain indicates that the newly added subregion remains highly influential, whereas diminishing gains approaching zero suggest that subsequent subregions contribute negligibly and exhibit limited saliency distinction. The attribution score Ai for subregion πi within the ordered set π is defined as: Ai = (cid:26)0 Ai1 (cid:12) (cid:12)F(π:i) F(π:i1)(cid:12) (cid:12) if = 1, if > 1, (5) where π:i denotes the combination of the top subregions, and the attribution scores start from zero, decrease progressively with each step, and are subsequently normalized."
        },
        {
            "title": "3.3 LANGUAGE PRIOR VS. PERCEPTION EVIDENCE",
            "content": "Beyond identifying which perceptual regions promote the generation of specific autoregressive tokens, we further analyze whether each generated token is more strongly influenced by language priors or by perceptual evidence. Existing approaches often assess token relevance to the visual modality by observing changes in probability when the input image is masked Xing et al. (2025). However, simply comparing the probability with the full image against that without the image is not reliable indicator of visual relevance, as the probability may first increase and then decrease when visual inputs are progressively inserted (Chen et al., 2024b). By contrast, if token is truly irrelevant to the visual modality, its probability should remain stable regardless of how the image is modified. To address this limitation, we leverage the ordered subset π obtained in Section 3.2 and examine how each token is affected as the subregions in π are progressively expanded, thereby quantifying the extent to which the token is influenced by perceptual evidence. Specifically, for each target token position ti , the influence score is defined as: Iti = π (cid:88) (cid:16) r= p(yti = vi π:r, Prompt, y<ti) min 1jπ p(yti = vi π:j, Prompt, y<ti) (cid:17) , (6) where vi denotes the vocabulary index of the target token yti. The influence score Iti measures the impact of perceptual evidence on the generation of token yti. larger score indicates that the token generation is more strongly driven by perceptual evidence, whereas smaller score suggests greater reliance on language priors, as shown in Fig. 2. The detailed calculation process of the proposed EAGLE algorithm is outlined in Algorithm 1. Remark 1 (Weak Submodularity). Our objective function F() is not strictly submodular in MLLMs. However, it exhibits weak submodularity, relaxed condition that bounds the deviation from true submodularity. Formally, let γ (0, 1] denote the submodularity ratio of F: γ = min LU,SU (cid:80) iS (cid:0)F(L {i}) F(L)(cid:1) F(L S) F(L) . When γ = 1, is strictly submodular; smaller γ values indicate weaker submodularity. Under weak submodularity, greedy selection is still guaranteed to achieve (1 eγ)-approximation of the optimal solution (Bian et al., 2017). Thus, the stronger the submodular property of in MLLMs (i.e., larger γ), the tighter the theoretical bound and the more reliable the approximation. Remark 2 (Token-Agnostic Attribution). Gradient-based methods (Xing et al., 2025) rely on selecting visually relevant tokens; choosing tokens dominated by language priors can distort attribution and yield unreliable explanations. In contrast, our approach is token-agnostic: even when applied to tokens strongly influenced by language priors, the visual attribution remains unaffected. Moreover, after attribution, our framework explicitly evaluates whether the selected tokens are primarily driven by perceptual evidence or language priors. Remark 3 (Interactive Token-Level Explanation). Our framework also allows users to select specific sentences, words, or tokens for targeted attribution. This flexibility enables fine-grained interpretation at arbitrary granularity and naturally supports human-in-the-loop analysis and interactive explanation. Remark 4 (Computational Complexity). The algorithm has time complexity O(2V ). With the greedy strategy, all subregions are ordered with total of 1 2 inferences, yielding time complexity of O(V 2). The space complexity is O(V ), only the ordered subset needs to be stored. 2 2 +"
        },
        {
            "title": "4.1 EXPERIMENTAL SETUP",
            "content": "Datasets. We evaluate across three representative tasks: MS COCO Caption (Lin et al., 2014; Chen et al., 2015) for image captioning, MMVP (Tong et al., 2024) for visual question answering (VQA), and RePOPE (Neuhaus & Hein, 2025) for object hallucination assessment. Baselines. We compare EAGLE against state-of-the-art attribution methods for MLLMs, including gradient-based approaches (LLaVA-CAM (Zhang et al., 2025b) and IGOS++ adaptation (Xing et al., 5 Preprint Table 1: Evaluation of sentence-level faithfulness metrics (Deletion, Insertion AUC, and Average Highest Score) on the MS COCO and MMVP datasets using LLaVA-1.5, Qwen2.5-VL, and InternVL3.5. Datasets MLLMs Methods Sentence-level Faithfulness Ins. () Del. () Ave. high. score () Sensitive Tokens-level Faithfulness Ins. () Del. () Ave. high. score () GPU Memory () MS COCO (Lin et al., 2014) (Image caption task) MMVP (Tong et al., 2024) (VQA task) LLaVA-1.5 7B (Liu et al., 2024) Qwen2.5-VL 3B (Bai et al., 2025) Qwen2.5-VL 7B (Bai et al., 2025) InternVL3.5 4B (Wang et al., 2025) LLaVA-1.5 7B (Liu et al., 2024) Qwen2.5-VL 3B (Bai et al., 2025) Qwen2.5-VL 7B (Bai et al., 2025) InternVL3.5 4B (Wang et al., 2025) LLaVA-CAM (Zhang et al., 2025b) iGOS++ (w/ GNC) (Xing et al., 2025) EAGLE LLaVA-CAM (Zhang et al., 2025b) iGOS++ (w/ GNC) (Xing et al., 2025) EAGLE LLaVA-CAM (Zhang et al., 2025b) iGOS++ (w/ GNC) (Xing et al., 2025) EAGLE LLaVA-CAM (Zhang et al., 2025b) iGOS++ (w/ GNC) (Xing et al., 2025) EAGLE LLaVA-CAM (Zhang et al., 2025b) iGOS++ (w/ GNC) (Xing et al., 2025) EAGLE LLaVA-CAM (Zhang et al., 2025b) iGOS++ (w/ GNC) (Xing et al., 2025) EAGLE LLaVA-CAM (Zhang et al., 2025b) iGOS++ (w/ GNC) (Xing et al., 2025) EAGLE LLaVA-CAM (Zhang et al., 2025b) iGOS++ (w/ GNC) (Xing et al., 2025) EAGLE 0.5298 0.5293 0.5970 0.4978 0.5328 0. 0.5605 0.5603 0.7006 0.6116 0.6271 0.7665 0.7756 0.7717 0.7960 0.7742 0.7719 0.8052 0.7505 0.7394 0.7824 0.7348 0.7277 0. 0.5317 0.5168 0.4554 0.5562 0.4891 0.4345 0.5464 0.5072 0.4597 0.6235 0.5726 0.4650 0.7745 0.7698 0.7474 0.7770 0.7613 0. 0.7486 0.7211 0.6996 0.7458 0.7160 0.6782 0.6031 0.6004 0.6259 0.6662 0.6672 0.7039 0.7235 0.7237 0.7578 0.8032 0.7999 0. 0.7980 0.7965 0.8086 0.8181 0.8183 0.8339 0.8042 0.8036 0.8119 0.8325 0.8302 0.8471 0.4124 0.4101 0.5344 0.3541 0.4021 0. 0.4467 0.4400 0.6337 0.4948 0.5088 0.7042 0.6076 0.5825 0.6867 0.5925 0.5719 0.6634 0.4974 0.4505 0.5901 0.4897 0.4743 0. 0.4115 0.3815 0.2809 0.4497 0.3273 0.2710 0.4209 0.3623 0.2988 0.5100 0.4337 0.3042 0.6044 0.5781 0.5027 0.6006 0.5356 0. 0.4847 0.3853 0.3675 0.5213 0.4454 0.4027 0.5783 0.5731 0.5993 0.6424 0.6473 0.6840 0.7010 0.6695 0.7285 0.7764 0.7715 0. 0.7275 0.7236 0.7507 0.7476 0.7437 0.7689 0.7242 0.7185 0.7362 0.7575 0.7535 0.7762 37.25 GB 48.18 GB 16.07 GB 28.99 GB 71.62 GB 8.75 GB 47.17 GB 96.90 GB 17.68 GB 81.84 GB 60.93 GB 12.45 GB 34.38 GB 92.90 GB 15.40 GB 19.17 GB 19.79 GB 8.76 GB 37.54 GB 32.76 GB 17.40 GB 27.20 GB 62.31 GB 12.26 GB 2025)) and the activation-based method TAM (Li et al., 2025a). Note that TAM is restricted to attributing single token at time and cannot handle token combinations. Models. We validate our approach on three multimodal large language models: LLaVA-1.5-7B (Liu et al., 2024), Qwen2.5-VL (3B and 7B) (Bai et al., 2025), and InternVL 3.5-4B (Wang et al., 2025). Evaluation Metrics. We consider three categories of attribution metrics: faithfulness, localization, and correction-oriented. (1) Faithfulness metrics evaluate whether explanations align with the models decision process. We adopt Insertion (Petsiuk et al., 2018), Deletion (Petsiuk et al., 2018), and Average Highest Score (Chen et al., 2024b), computed as the mean probability over selected tokens. (2) Localization metrics assess whether explanations overlap with ground-truth regions using the Point Game (Zhang et al., 2018), under both box-level and mask-level annotations, where correctness is defined by the maximum attribution point falling inside the bounding box or segmentation mask. (3) Correction-oriented metrics address hallucination evaluation by testing whether attributions reveal regions causing hallucinated outputs. We use Average Minimal Correction Region (AMCR), the average proportion of regions that must be removed to correct hallucinations, and Correction Success Rate under Budget (CSR@10%), the percentage of cases corrected when no more than 10% of regions are removed."
        },
        {
            "title": "4.2 FAITHFULNESS ON SENTENCE-LEVEL EXPLANATIONS",
            "content": "We begin by evaluating our attribution method on two common MLLM tasks, image captioning and visual question answering (VQA), with the goal of identifying which image regions drive the full content generated by the model. We primarily compare our approach against LLaVA-CAM (Zhang et al., 2025b) and IGOS++ (w/ GNC) (Xing et al., 2025). Table 1 reports results on faithfulness metrics, evaluated in two ways: (1) using the sum of logits over all predicted tokens, and (2) using the sum over sensitive tokens, defined as those whose logits change by more than 0.2 when the entire image is masked. For the image captioning task, our method consistently achieves state-of-the-art performance across all models and metrics. On the LLaVA-1.5 7B model, it surpasses the best results of LLaVA-CAM and IGOS++ (w/ GNC) by 12.7%, 11.9%, and 3.8% in sentence-level insertion, deletion, and average highest score, respectively. At the sensitive-token level, the improvements are even larger, reaching 29.6%, 26.3%, and 3.6%. These stronger gains arise because sensitive tokens are more strongly grounded in visual evidence, making them particularly responsive to well-localized attribution maps. Similar trends are observed on the Qwen2.5-VL 7B model, where our method improves over the best baselines by 25.0%, 9.4%, and 4.7% at the sentence level, and by 41.9%, 17.5%, and 3.9% at the sensitive-token level. On the InternVL3.5 4B model, the corresponding improvements are 22.2%, 18.8%, and 3.8% at the sentence level, and 38.4%, 29.9%, and 3.7% at the sensitive-token level. For the VQA task, our method also achieves state-of-the-art performance across all models and metrics, though the margins are generally smaller than for captioning. On the LLaVA-1.5 7B model, it improves over the best baselines by 2.6%, 3.0%, and 1.3% at the sentence level, and by 13.0%, 13.0%, and 3.2% at the sensitive-token level. On the Qwen2.5-VL 7B model, the corresponding improvements are 4.3%, 3.0%, and 1.0% at the sentence level, and 18.6%, 1.8%, and 1.7% at the sensitive-token level. On the InternVL3.5 4B model, our method achieves 9.0%, 3.8%, and 1.8%"
        },
        {
            "title": "Preprint",
            "content": "Figure 3: Visualization of explanation results for LLaVA-1.5, Qwen2.5-VL, and InternVL3.5 on the MS COCO and MMVP datasets. Table 2: Evaluation of word-level faithfulness metrics (Deletion, Insertion AUC, and Average Highest Score) and location metrics (Point Game) on the MS COCO. Datasets MLLMs Methods Word-level Faithfulness Metrics Insertion () Deletion () Ave. high. score () MS COCO (Lin et al., 2014) (Image caption task) LLaVA-1.5 7B (Liu et al., 2024) Qwen2.5-VL 3B (Bai et al., 2025) Qwen2.5-VL 7B (Bai et al., 2025) InternVL3.5 4B (Wang et al., 2025) LLaVA-CAM (Zhang et al., 2025b) iGOS++ (w/ GNC) (Xing et al., 2025) TAM (Li et al., 2025a) EAGLE LLaVA-CAM (Zhang et al., 2025b) iGOS++ (w/ GNC) (Xing et al., 2025) TAM (Li et al., 2025a) EAGLE LLaVA-CAM (Zhang et al., 2025b) iGOS++ (w/ GNC) (Xing et al., 2025) TAM (Li et al., 2025a) EAGLE LLaVA-CAM (Zhang et al., 2025b) iGOS++ (w/ GNC) (Xing et al., 2025) TAM (Li et al., 2025a) EAGLE 0.4063 0.4093 0.3860 0.6395 0.3417 0.4141 0.5130 0.7353 0.4170 0.4816 0.5768 0.8109 0.4988 0.5192 0.6317 0. 0.4035 0.3812 0.4162 0.2047 0.4575 0.2901 0.2797 0.1628 0.4771 0.3478 0.3167 0.2127 0.5040 0.3983 0.3517 0.1706 0.6053 0.6084 0.5988 0.7213 0.7263 0.7250 0.7985 0. 0.8041 0.8080 0.8240 0.9194 0.8588 0.8604 0.8712 0.9585 Localization Metrics Point Gamebbox () 0.2468 0.6623 0.1818 0.8052 Point Gamemask () 0.1168 0.5584 0.1428 0.7792 0.1045 0.5822 0.5294 0. 0.2176 0.6734 0.5369 0.7785 0.3201 0.5775 0.5775 0.8052 0.0621 0.4967 0.4379 0.7745 0.1428 0.5959 0.4060 0.7383 0.2212 0.5181 0.4653 0.7755 GPU Memory () 36.73 GB 93.12 GB 16.60 GB 16.31 GB 26.01 GB 58.1 GB 9.56 GB 9.22 GB 44.26 GB 82.14 GB 18.75 GB 18.03 GB 81.84 GB 60.06 GB 14.23 GB 7.61 GB improvements at the sentence level, and 30.3%, 9.6%, and 2.5% at the sensitive-token level. The smaller margins in VQA reflect the fact that much of the generated output relies on reasoning and language priors rather than purely on perceptual evidence. In addition to higher attribution fidelity, EAGLE demonstrates strong efficiency, requiring only 17.68 GB on Qwen2.5-VL 7B compared to 96.90 GB for IGOS++, making it practical for modern MLLMs. Overall, it provides more faithful and resource-efficient explanations than gradient-based baselines. As shown in Fig. 3, LLaVA-CAM often misses key regions and IGOS++ yields redundant maps, while our method highlights critical regions that align closely with visually grounded tokens, producing concise and human-consistent explanations."
        },
        {
            "title": "4.3 FAITHFULNESS AND LOCALIZATION ON WORD-LEVEL EXPLANATIONS",
            "content": "Next, we evaluate the ability of the proposed attribution method to provide word-level explanations. Specifically, we use samples with object bounding box annotations from the MS COCO dataset to verify whether the objects mentioned in image captions are accurately grounded in the visual input. We also include TAM (Li et al., 2025a) as an additional baseline, since it is particularly effective at explaining object localization. Table 2 reports the results of faithfulness and localization evaluations, where our method consistently achieves state-of-the-art performance across all models and metrics. For faithfulness, on the LLaVA-1.5 7B model, it surpasses the strongest baseline by 56.2%, 46.3%, and 19.3% in insertion, deletion, and average highest score, respectively. On the Qwen2.5-VL 7B model, the corresponding improvements are 40.6%, 10.4%, and 11.6%, while on the InternVL3.5 4B model, they are 36.5%, 51.5%, and 10.0%. We also observe that TAM performs well only on stronger MLLMs such as Qwen2.5-VL and InternVL3.5, since it relies solely on activation maps rather than capturing strong causal relationships. In contrast, our method is broadly applicable across models and can faithfully explain word-level decisions even for LLaVA-1.5."
        },
        {
            "title": "Preprint",
            "content": "Figure 4: Visualization of word-level explanation results for LLaVA-1.5, Qwen2.5-VL, and InternVL3.5 on the MS COCO datasets. Table 3: Evaluation of faithfulness metrics (Deletion and Insertion AUC scores) on hallucination interpretation. Datasets MLLMs Methods Correction-oriented Metrics Insertion () Deletion () Ave. high. score () AMCR () CSR@10% () Faithfulness Metrics GPU Memory () RePOPE (Neuhaus & Hein, 2025) (Object Hallucination Benchmark) LLaVA-1.5 7B (Liu et al., 2024) Qwen2.5-VL 3B (Bai et al., 2025) Qwen2.5-VL 7B (Bai et al., 2025) InternVL3.5 4B (Wang et al., 2025) LLaVA-CAM (Zhang et al., 2025b) iGOS++ (w/ GNC) (Xing et al., 2025) TAM (Li et al., 2025a) EAGLE LLaVA-CAM (Zhang et al., 2025b) iGOS++ (w/ GNC) (Xing et al., 2025) TAM (Li et al., 2025a) EAGLE LLaVA-CAM (Zhang et al., 2025b) iGOS++ (w/ GNC) (Xing et al., 2025) TAM (Li et al., 2025a) EAGLE LLaVA-CAM (Zhang et al., 2025b) iGOS++ (w/ GNC) (Xing et al., 2025) TAM (Li et al., 2025a) EAGLE 0.4095 0.4232 0.4168 0.6999 0.3994 0.4056 0.3905 0. 0.2444 0.3017 0.2717 0.7987 0.4079 0.3651 0.3794 0.9114 0.4191 0.4182 0.4166 0.2652 0.3783 0.4471 0.4090 0.1610 0.2901 0.3330 0.3177 0.0331 0.3733 0.4556 0.4221 0. 0.6596 0.6794 0.6705 0.7877 0.6992 0.7235 0.6900 0.8717 0.5898 0.7125 0.6792 0.9381 0.9296 0.9393 0.9115 0.9941 0.5613 0.4770 0.5826 0.0844 0.4555 0.4461 0.4747 0. 0.6620 0.5357 0.5844 0.1442 0.4078 0.4299 0.4801 0.0676 19.70% 37.50% 18.75% 77.50% 42.21% 37.57% 29.75% 80.41% 35.37% 32.41% 22.45% 73.94% 36.43% 38.76% 28.57% 80.00% 37.07 GB 93.88 GB 16.59 GB 16.04 GB 27.10 GB 35.16 GB 9.66 GB 9.20 GB 45.05 GB 70.86 GB 18.57 GB 18.26 GB 87.93 GB 66.26 GB 14.04 GB 12.31 GB For localization, our method achieves the best Pointing Game results under both boxand mask-level settings, confirming that predictions are grounded in specific objects. While TAM performs well on stronger models but poorly on LLaVA-1.5, IGOS++ gains from overly redundant maps. In contrast, our method yields sparse yet focused highlights that more accurately localize the objects mentioned in captions  (Fig. 4)  . 4."
        },
        {
            "title": "INTERPRETING OBJECT HALLUCINATION",
            "content": "We next apply our interpretable algorithm to analyze why MLLMs produce hallucinations. Experiments are conducted on the object hallucination benchmark RePOPE (Neuhaus & Hein, 2025). We focus on samples where the MLLM makes prediction errors, including cases where the model incorrectly answers no instead of yes, and vice versa. Assuming that hallucinations have already been identified, our objective is to identify which image regions trigger the hallucination and to assess whether blocking these regions can mitigate it. In practice, we attribute the first token of the answer, restricted to the vocabulary IDs Yes and No. For example, if the model incorrectly outputs Yes, the attribution is computed with respect to No, thereby providing counterfactual perspective on which regions would support the correct response. Table 3 reports the results of attributing hallucinations to specific input regions. On the LLaVA-1.5 7B model, our method improves over the strongest baseline by 65.4%, 36.3%, and 15.9% in insertion, deletion, and average highest score, respectively. On the Qwen2.5-VL 7B model, the gains are even larger, reaching 164.7%, 88.6%, and 31.7%, while on the InternVL3.5 4B model, the improvements are 123.4%, 88.2%, and 5.8%. These substantial margins highlight the strength of our approach in faithfully uncovering the input regions responsible for hallucinated predictions and in explaining the underlying causes of incorrect decisions, revealing not only where the model looked, but also why it went wrong. Next, we examine whether hallucinations can be eliminated by progressively removing the responsible regions. Instead of relying on logits, we evaluate direct model outputs (Yes or No with the corresponding rationale) using correction-oriented metrics. On the LLaVA-1.5 7B model, our method surpasses 8 Preprint Table 4: Ablation of objective function components on Qwen2.5-VL 7B for MS COCO captioning. Table 5: Ablation of subregion number on Qwen2.5-VL 7B for MS COCO captioning. Insight Necessity (Eq. 2) (Eq. 3) Faithfulness Metrics Ins. () Del. () Avg. High ()"
        },
        {
            "title": "Number",
            "content": "Faithfulness Metrics Ins. () Del. () Avg. High () 0.6176 0.6981 0.7006 0.4613 0.5253 0.4597 0.7282 0.7566 0.7578 36 50 64 0.6869 0.6901 0. 0.4587 0.4514 0.4597 0.7452 0.7482 0.7578 Figure 5: Hallucination attribution on RePOPE. Our method produces sparse, focused maps that more accurately reveal regions responsible for hallucinated outputs, compared with IGOS++ and TAM. the strongest baseline by 82.3% and 106.6% in Average Minimal Correction Region (AMCR) and Correction Success Rate under Budget (CSR@10%), respectively. On the Qwen2.5-VL 7B model, the improvements are 73.1% and 109.0%, and on the InternVL3.5 4B model they are 83.4% and 106.4%. These results show that removing only small portion of the input is sufficient to eliminate hallucinations, demonstrating the effectiveness of our attribution approach. Fig. 5 visualizes the results, including the Hallucination Map, where highlighted purple regions indicate areas prone to hallucinations identified by our method. Hallucination Mitigation denotes the minimal region that must be removed to eliminate hallucinations. The curve illustrates changes in the logit of the ground-truth token as hallucination-prone regions are progressively deleted, with the red line marking the deletion point determined by Hallucination Mitigation. Our method rapidly localizes regions that cause hallucinations, while TAM and IGOS++ produce diffuse maps. On LLaVA-1.5, it attributes the false detection of snowboard to surfboard, highlighting confusion between similar objects. InternVL3.5 fails to recognize spoon that is partially occluded by fork. By precisely attributing and removing the fork head, our method enables the model to correctly identify the spoon, revealing its limited ability to disambiguate overlapping objects."
        },
        {
            "title": "4.5 ABLATION STUDY",
            "content": "We conduct ablations on the MS COCO captioning task with Qwen2.5-VL 7B to evaluate both the objective function design and the impact of subregion partitioning. As shown in Table 4, only the joint use of the Insight and Necessity Scores consistently improves all faithfulness metrics, demonstrating their complementary effects. Table 5 further shows that finer image partitions generally enhance faithfulness, though at the expense of increased attribution time, suggesting the importance of developing more scalable attribution strategies in future work."
        },
        {
            "title": "5 CONCLUSION AND LIMITATION",
            "content": "In this paper, we present EAGLE, black-box attribution framework for autoregressive MLLMs. By unifying sufficiency and indispensability in submodular-inspired objective, EAGLE faithfully explains token generation, revealing both where models attend and what they rely on. Experiments across diverse models and datasets show clear gains in faithfulness, localization, and hallucination diagnosis. Moreover, by identifying and removing minimal interfering regions, EAGLE also mitigates hallucinations, serving as both an interpretability and correction-oriented tool."
        },
        {
            "title": "Preprint",
            "content": "Limitations. Despite its effectiveness, our work has two main limitations. First, the iterative subset selection and greedy search limit scalability compared to lightweight visualization methods. Second, the framework focuses on hallucination explanation and partial mitigation, leaving proactive prevention unexplored. Future work will explore faster search strategies and explanation-guided debiasing for training MLLMs."
        },
        {
            "title": "REFERENCES",
            "content": "Radhakrishna Achanta, Appu Shaji, Kevin Smith, Aurelien Lucchi, Pascal Fua, and Sabine Süsstrunk. Slic superpixels compared to state-of-the-art superpixel methods. IEEE Transactions on Pattern Analysis and Machine Intelligence, 34(11):22742282, 2012. 3 Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 1 Abdulaziz Ahmed, Mohammad Saleem, Mohammed Alzeen, Badari Birur, Rachel Fargason, Bradley Burk, Hannah Rose Harkins, Ahmed Alhassan, and Mohammed Ali Al-Garadi. Leveraging large language models to enhance machine learning interpretability and predictive performance: case study on emergency department returns for mental health patients. arXiv preprint arXiv:2502.00025, 2025. 1 Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2.5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. 1, 2, 6, 7, 8 Gabriela Ben Melech Stan, Estelle Aflalo, Raanan Yehezkel Rohekar, Anahita Bhiwandiwalla, ShaoYen Tseng, Matthew Lyle Olson, Yaniv Gurwicz, Chenfei Wu, Nan Duan, and Vasudev Lal. Lvlm-intrepret: An interpretability tool for large vision-language models. In IEEE Conf. Comput. Vis. Pattern Recog. (CVPR) Workshops, pp. 81828187, 2024. 1, 3 Andrew An Bian, Baharan Mirzasoleiman, Joachim Buhmann, and Andreas Krause. Guaranteed nonconvex optimization: Submodular maximization over continuous domains. In Artificial Intelligence and Statistics, pp. 111120, 2017. Boxu Chen, Ziwei Zheng, Le Yang, Zeyu Geng, Zhengyu Zhao, Chenhao Lin, and Chao Shen. Seeing it or not? interpretable vision-aware latent steering to mitigate object hallucinations. arXiv preprint arXiv:2505.17812, 2025a. 1, 3 Li Chen, Penghao Wu, Kashyap Chitta, Bernhard Jaeger, Andreas Geiger, and Hongyang Li. End-toend autonomous driving: Challenges and frontiers. IEEE Transactions on Pattern Analysis and Machine Intelligence, 46(12):1016410183, 2024a. 1 Ruoyu Chen, Hua Zhang, Siyuan Liang, Jingzhi Li, and Xiaochun Cao. Less is more: Fewer interpretable region via submodular subset selection. In ICLR, 2024b. 3, 5, 6 Ruoyu Chen, Siyuan Liang, Jingzhi Li, Shiming Liu, Maosen Li, Zhen Huang, Hua Zhang, and Xiaochun Cao. Interpreting object-level foundation models via visual precision search. In CVPR, 2025b. 1, 3 Ruoyu Chen, Siyuan Liang, Jingzhi Li, Shiming Liu, Li Liu, Hua Zhang, and Xiaochun Cao. Less is more: Efficient black-box attribution via minimal interpretable subset selection. arXiv preprint arXiv:2504.00470, 2025c. Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dollár, and Lawrence Zitnick. Microsoft coco captions: Data collection and evaluation server. arXiv preprint arXiv:1504.00325, 2015. 5 Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025."
        },
        {
            "title": "Preprint",
            "content": "Yunkai Dang, Kaichen Huang, Jiahao Huo, Yibo Yan, Sirui Huang, Dongrui Liu, Mengxi Gao, Jie Zhang, Chen Qian, Kun Wang, et al. Explainable and interpretable multimodal large language models: comprehensive survey. arXiv preprint arXiv:2412.02104, 2024. 1 Nicholas Jiang, Anish Kachinthaya, Suzanne Petryk, and Yossi Gandelsman. Interpreting and editing vision-language representations to mitigate hallucinations. In ICLR, 2025. 3 Bohao Li, Yuying Ge, Yixiao Ge, Guangzhi Wang, Rui Wang, Ruimao Zhang, and Ying Shan. Seed-bench: Benchmarking multimodal large language models. In CVPR, pp. 1329913308, 2024. 1 Yi Li, Hualiang Wang, Xinpeng Ding, Haonan Wang, and Xiaomeng Li. Token activation map to visually explain multimodal llms. In ICCV, 2025a. 2, 3, 6, 7, Yifan Li, Zhixin Lai, Wentao Bao, Zhen Tan, Anh Dao, Kewei Sui, Jiayi Shen, Dong Liu, Huan Liu, and Yu Kong. Visual large language models for generalized and specialized applications. arXiv preprint arXiv:2501.02765, 2025b. 1 Chia Xin Liang, Pu Tian, Caitlyn Heqi Yin, Yao Yua, Wei An-Hou, Li Ming, Tianyang Wang, Ziqian Bi, and Ming Liu. comprehensive survey and guide to multimodal large language models in vision-language tasks. arXiv preprint arXiv:2411.06284, 2024. 1 Siyuan Liang, Mingli Zhu, Aishan Liu, Baoyuan Wu, Xiaochun Cao, and Ee-Chien Chang. Badclip: Dual-embedding guided backdoor attack on multimodal contrastive learning. arXiv preprint arXiv:2311.12075, 2023. 1 Siyuan Liang, Tianmeng Fang, Zhe Liu, Aishan Liu, Yan Xiao, Jinyuan He, Ee-Chien Chang, and Xiaochun Cao. Safemobile: Chain-level jailbreak detection and automated evaluation for multimodal mobile agents. arXiv preprint arXiv:2507.00841, 2025a. 1 Siyuan Liang, Jiawei Liang, Tianyu Pang, Chao Du, Aishan Liu, Mingli Zhu, Xiaochun Cao, and Dacheng Tao. Revisiting backdoor attacks against large vision-language models from domain shift. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 94779486, 2025b. 1 Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV, pp. 740755, 2014. 2, 5, 6, Zihao Lin, Samyadeep Basu, Mohammad Beigi, Varun Manjunatha, Ryan Rossi, Zichao Wang, Yufan Zhou, Sriram Balasubramanian, Arman Zarei, Keivan Rezaei, et al. survey on mechanistic interpretability for multi-modal foundation models. arXiv preprint arXiv:2502.17516, 2025. 1 Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In CVPR, pp. 2629626306, 2024. 2, 6, 7, 8 Liming Lu, Shuchao Pang, Siyuan Liang, Haotian Zhu, Xiyu Zeng, Aishan Liu, Yunhuai Liu, and Yongbin Zhou. Adversarial training for multimodal large language models against jailbreak attacks. arXiv preprint arXiv:2503.04833, 2025. 1 Yannic Neuhaus and Matthias Hein. Repope: Impact of annotation errors on the pope benchmark. arXiv preprint arXiv:2504.15707, 2025. 2, 5, Daniel Omeiza, Skyler Speakman, Celia Cintas, and Komminist Weldermariam. Smooth GradCAM++: An enhanced inference level visualization technique for deep convolutional neural network models. arXiv preprint arXiv:1908.01224, 2019. 3 Vitali Petsiuk, Abir Das, and Kate Saenko. Rise: Randomized input sampling for explanation of black-box models. In BMVC, pp. 151, 2018. 6 Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, and Saining Xie. Eyes wide shut? exploring the visual shortcomings of multimodal llms. In CVPR, pp. 95689578, 2024. 2, 5,"
        },
        {
            "title": "Preprint",
            "content": "Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 2 Weiyun Wang, Zhangwei Gao, Lixin Gu, Hengjun Pu, Long Cui, Xingguang Wei, Zhaoyang Liu, Linglin Jing, Shenglong Ye, Jie Shao, et al. Internvl3.5: Advancing open-source multimodal models in versatility, reasoning, and efficiency. arXiv preprint arXiv:2508.18265, 2025. 1, 2, 6, 7, 8 Xiaoying Xing, Chia-Wen Kuo, Li Fuxin, Yulei Niu, Fan Chen, Ming Li, Ying Wu, Longyin Wen, and Sijie Zhu. Where do large vision-language models look at when answering questions? arXiv preprint arXiv:2503.13891, 2025. 1, 2, 3, 5, 6, 7, 8 Jianming Zhang, Sarah Adel Bargal, Zhe Lin, Jonathan Brandt, Xiaohui Shen, and Stan Sclaroff. Top-down neural attention by excitation backprop. International Journal of Computer Vision, 126 (10):10841102, 2018. 6 Jiarui Zhang, Mahyar Khayatkhoei, Prateek Chhikara, and Filip Ilievski. Mllms know where to look: Training-free perception of small visual details with multimodal llms. In ICLR, 2025a. Xiaofeng Zhang, Yihao Quan, Chen Shen, Xiaosong Yuan, Shaotian Yan, Liang Xie, Wenxiao Wang, Chaochen Gu, Hao Tang, and Jieping Ye. From redundancy to relevance: Enhancing explainability in multimodal large language models. In NAACL, 2025b. 1, 2, 3, 5, 6, 7,"
        },
        {
            "title": "A LLM USAGE",
            "content": "During the preparation of this manuscript, large language models (LLMs) were employed in limited and auxiliary capacity. Specifically, their usage was restricted to the following three aspects: (1) checking grammar and expression at the sentence level, thereby providing local linguistic refinement; (2) performing global polishing after the draft was completed, ensuring that the overall exposition conforms to idiomatic English usage. At no stage were LLMs used for generating research ideas, developing arguments, or modifying the substantive content of this work. Their sole role was to assist in enhancing the clarity and effectiveness of communication."
        },
        {
            "title": "B EAGLE ALGORITHM",
            "content": "The detailed calculation process of the proposed EAGLE algorithm is outlined below. Algorithm 1: EAGLE: Explaining Autoregressive Generation by Language priors or Evidence in multimodal large language models (MLLMs) Input: Image Rhw3, partitioning algorithm Div(), prompt Prompt, generated sequence y, target token positions , vocabulary indices V. Output: Ordered subset π, saliency map Rhw, influence scores It. /* Initialize ordered subset */"
        },
        {
            "title": "1 V ← Div(I);\n2 π ← ∅ ;\n3 A1 ← 0;\n4 for i = 1 to |V | do\n5",
            "content": "9 10 end 11 for = 1 to do 12 6 7 8 13 Sd S; α arg maxαSd F(π {α}); π π {α}; if > 1 then Ai Ai1 (cid:12) (cid:12)F(π:i) F(π:i1)(cid:12) (cid:12) ; smax max1jπ p(yti = vi π:j, Prompt, y<ti); Iti (cid:80)π smax p(yti = vi π:r, Prompt, y<ti ) perception evidence */ r=1 (cid:16) /* Saliency update */ (cid:17) ; /* Language prior vs. 14 end 15 return π, norm(A), norm(It)"
        },
        {
            "title": "C ADDITIONAL EXPERIMENTAL DETAILS",
            "content": "For the image captioning task on MS COCO, the prompt used for all MLLMs is: Describe the image in one factual English sentence of no more than 20 words. Do not include information that is not clearly visible. For the hallucination detection task on RePOPE, the prompt used is: You are asked visual question answering task. First, answer strictly with \"Yes\" or \"No\". Then, provide short explanation if necessary. Question: {question} Answer:"
        },
        {
            "title": "D ADDITIONAL QUALITATIVE RESULTS",
            "content": "In this appendix, we provide extended qualitative visualizations that complement the main findings in Fig. 3, Fig. 4, and Fig. 5. These supplementary results aim to offer finer-grained perspective on how competing attribution methods and our proposed approach behave across diverse settings. Specifically, we present: (i) sentence-level explanations on both MS COCO and MMVP, (ii) objectlevel explanations on MS COCO, and (iii) hallucination attribution visualizations on additional samples. Collectively, these results provide deeper insights into the consistency, precision, and interpretability of our method. D.1 SENTENCE-LEVEL EXPLANATIONS ON MS COCO AND MMVP As shown in Fig. 6 and Fig. 7, our method produces faithful explanations for LLaVA-1.5 by tightly aligning highlighted regions with relevant caption tokens (e.g., smiling, hat, motor) or VQA queries (e.g., Is the sharks belly visible?). In contrast, LLaVA-CAM often distributes attention diffusely across the scene, while IGOS++ over-activates irrelevant background regions. For Qwen2.5-VL, Fig. 8 and Fig. 9 show that our method generates concise and semantically meaningful attribution maps. For example, in captions mentioning multiple objects, our approach selectively highlights the relevant ones while avoiding redundancy. In VQA tasks, it accurately isolates queried entities such as remote button, whereas baselines either miss the target or introduce noise. Similarly, for InternVL3.5 (Fig. 10, Fig. 11), our method highlights precise object-centric regions corresponding to key caption tokens (e.g., sandwich, frisbee) and VQA queries (e.g., Does the snowman have arms made of branches?). Baseline methods either scatter attention broadly or fail to capture the queried object, reducing interpretability. These results collectively demonstrate that our approach consistently improves faithfulness and transparency across different models and datasets. D.2 OBJECT-LEVEL EXPLANATIONS ON MS COCO Beyond sentence-level results, we further evaluate our method at the object level with ground-truth bounding boxes. Fig. 12, Fig. 13, and Fig. 14 illustrate that our method produces sparse yet highly accurate localization of queried objects such as boat, keyboard, or truck. By contrast, IGOS++ frequently covers overly broad regions, while LLaVA-CAM and TAM often fail to precisely localize objects. These comparisons highlight the advantage of our method in generating interpretable, object-centric attributions."
        },
        {
            "title": "Preprint",
            "content": "Figure 6: Sentence-level explanation results for LLaVA-1.5 on the MS COCO dataset. Our method consistently identifies semantically critical regions that align with highlighted tokens in the caption, while baseline methods either fail to capture relevant areas (LLaVA-CAM) or over-highlight irrelevant background regions (IGOS++)."
        },
        {
            "title": "Preprint",
            "content": "Figure 7: Sentence-level explanation results for LLaVA-1.5 on the MMVP dataset. Compared to the baselines, our method highlights regions that are directly related to the VQA queries, resulting in explanations that are more interpretable and trustworthy."
        },
        {
            "title": "Preprint",
            "content": "Figure 8: Sentence-level explanation results for Qwen2.5-VL on the MS COCO dataset. Our method highlights critical objects with strong correspondence to the generated captions, reducing redundancy in comparison to IGOS++."
        },
        {
            "title": "Preprint",
            "content": "Figure 9: Sentence-level explanation results for Qwen2.5-VL on the MMVP dataset. Our method improves alignment between highlighted visual regions and VQA-relevant words, enhancing interpretability."
        },
        {
            "title": "Preprint",
            "content": "Figure 10: Sentence-level explanation results for InternVL3.5 on the MS COCO dataset. Our method captures object-centric regions more consistently than baseline methods."
        },
        {
            "title": "Preprint",
            "content": "Figure 11: Sentence-level explanation results for InternVL3.5 on the MMVP dataset. Our approach ensures strong consistency between highlighted evidence and the VQA queries."
        },
        {
            "title": "Preprint",
            "content": "Figure 12: Object-level explanation results for LLaVA-1.5 on the MS COCO dataset. Bounding box overlays show that our method provides sparse yet highly accurate localization."
        },
        {
            "title": "Preprint",
            "content": "Figure 13: Object-level explanation results for Qwen2.5-VL on the MS COCO dataset. Our method produces localized attribution maps with high correspondence to ground-truth bounding boxes."
        },
        {
            "title": "Preprint",
            "content": "Figure 14: Object-level explanation results for InternVL3.5 on the MS COCO dataset. Our method captures object-centric highlights with strong correspondence to caption tokens and bounding boxes."
        },
        {
            "title": "Preprint",
            "content": "Figure 15: Hallucination attribution for LLaVA-1.5 on the MS COCO dataset. Our method highlights the minimal hallucination-inducing regions across different queries, such as snowboard, traffic light, and cup. D.3 ADDITIONAL HALLUCINATION ATTRIBUTION VISUALIZATIONS We also provide supplementary hallucination attribution results on MS COCO (Fig. 15, Fig. 16, Fig. 17). Unlike the main paper, these figures focus exclusively on our method to illustrate how it identifies hallucination-prone regions across diverse queries. For LLaVA-1.5  (Fig. 15)  , hallucinations typically arise from visually similar structures. For example, queries about snowboard lead to confusions with surfboard-like regions, while small background cues induce false detections for traffic light or cup. Our attribution maps isolate these exact regions, providing interpretable evidence of failure modes. For Qwen2.5-VL  (Fig. 16)  , hallucinations are often caused by small or occluded objects. For instance, reflective regions resembling phone screen mislead the model when asked about cell phones, while circular patterns in the background induce false positives for bicycle. Our approach sharply localizes these misleading cues, enhancing transparency. Finally, for InternVL3.5  (Fig. 17)  , hallucinations are triggered by overlapping or occluded objects. For example, confusion between fork and spoon is precisely localized, as are reflective regions falsely identified as TVs or cluttered areas misinterpreted as dining tables. These examples underscore the effectiveness of our method in diagnosing hallucination sources in fine-grained and transparent manner."
        },
        {
            "title": "Preprint",
            "content": "Figure 16: Hallucination attribution for Qwen2.5-VL on the MS COCO dataset. Our method isolates misleading cues leading to hallucinations in queries such as cell phone, bicycle, and truck."
        },
        {
            "title": "Preprint",
            "content": "Figure 17: Hallucination attribution for InternVL3.5 on the MS COCO dataset. Our method identifies hallucination-prone regions for queries such as spoon, tv, and dining table, especially in cases of overlapping or occluded objects."
        }
    ],
    "affiliations": [
        "Department of Computer Science, Hong Kong Baptist University",
        "Institute of Information Engineering, Chinese Academy of Sciences",
        "RAMS Lab, Huawei Technologies Co., Ltd.",
        "RAMS Lab, Munich Research Center, Huawei Technologies Düsseldorf GmbH",
        "School of Computing, NUS",
        "School of Cyber Science and Technology, Shenzhen Campus of Sun Yat-sen University",
        "School of Cyber Security, University of Chinese Academy of Sciences"
    ]
}