{
    "paper_title": "Hypencoder: Hypernetworks for Information Retrieval",
    "authors": [
        "Julian Killingback",
        "Hansi Zeng",
        "Hamed Zamani"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The vast majority of retrieval models depend on vector inner products to produce a relevance score between a query and a document. This naturally limits the expressiveness of the relevance score that can be employed. We propose a new paradigm, instead of producing a vector to represent the query we produce a small neural network which acts as a learned relevance function. This small neural network takes in a representation of the document, in this paper we use a single vector, and produces a scalar relevance score. To produce the little neural network we use a hypernetwork, a network that produce the weights of other networks, as our query encoder or as we call it a Hypencoder. Experiments on in-domain search tasks show that Hypencoder is able to significantly outperform strong dense retrieval models and has higher metrics then reranking models and models an order of magnitude larger. Hypencoder is also shown to generalize well to out-of-domain search tasks. To assess the extent of Hypencoder's capabilities, we evaluate on a set of hard retrieval tasks including tip-of-the-tongue retrieval and instruction-following retrieval tasks and find that the performance gap widens substantially compared to standard retrieval tasks. Furthermore, to demonstrate the practicality of our method we implement an approximate search algorithm and show that our model is able to search 8.8M documents in under 60ms."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 ] . [ 1 4 6 3 5 0 . 2 0 5 2 : r Hypencoder: Hypernetworks for Information Retrieval Julian Killingback jkillingback@cs.umass.com University of Massachusetts Amherst Amherst, MA, USA Hansi Zeng hzeng@cs.umass.edu University of Massachusetts Amherst Amherst, MA, USA Hamed Zamani zamani@cs.umass.edu University of Massachusetts Amherst Amherst, MA, USA ABSTRACT The vast majority of retrieval models depend on vector inner products to produce relevance score between query and document. This naturally limits the expressiveness of the relevance score that can be employed. We propose new paradigm, instead of producing vector to represent the query we produce small neural network which acts as learned relevance function. This small neural network takes in representation of the document, in this paper we use single vector, and produces scalar relevance score. To produce the little neural network we use hypernetwork, network that produce the weights of other networks, as our query encoder or as we call it Hypencoder. Experiments on in-domain search tasks show that Hypencoder is able to significantly outperform strong dense retrieval models and has higher metrics then reranking models and models an order of magnitude larger. Hypencoder is also shown to generalize well to out-of-domain search tasks. To assess the extent of Hypencoders capabilities, we evaluate on set of hard retrieval tasks including tip-of-the-tongue retrieval and instructionfollowing retrieval tasks and find that the performance gap widens substantially compared to standard retrieval tasks. Furthermore, to demonstrate the practicality of our method we implement an approximate search algorithm and show that our model is able to search 8.8M documents in under 60ms."
        },
        {
            "title": "1 INTRODUCTION\nEfficient neural retrieval models are based on a bi-encoder (or two\ntower) architecture, in which queries and documents are repre-\nsented separately using either high-dimensional sparse [17, 18, 75]\nor relatively low-dimensional dense vectors [19, 32, 33, 73, 76].\nThese models use simple and light-weight similarity functions,\ne.g., inner product or cosine similarity, to compute the relevance\nscore for a given pair of query and document representations. We\ndemonstrate theoretically that inner product similarity functions\nfundamentally limit the types of relevance that retrieval models can\nexpress. Specifically, we prove that there is always a set of relevant\ndocuments which cannot be perfectly retrieved regardless of the\nquery vector and specific encoder model.",
            "content": "Motivated by this theoretical argument, we introduce new category of retrieval models that can capture complex relationship between query and document representations. Building upon the hypernetwork literature in machine learning [22, 59, 66], we propose Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). Conference17, July 2017, Washington, DC, USA 2025 Copyright held by the owner/author(s). ACM ISBN 978-x-xxxx-xxxx-x/YY/MM. https://doi.org/10.1145/nnnnnnn.nnnnnnn Hypencodera generic framework that learns query-dependent multi-layer neural network as similarity function that is applied to the document representations. In more detail, Hypencoder applies attention-based hypernetwork layers, called hyperhead layers, to the contextualized query embeddings output by backbone transformer encoder. Each hyperhead layer produces the weight and bias matrices for neural network layer in the query-dependent similarity network, called the q-net. The q-net is then applied to each document representation, which results in scalar relevance score. We demonstrate that the Hypencoder framework can be optimized end-to-end and can be used for efficient retrieval from large corpus. Specifically, we propose graph-based greedy search algorithm that approximates exhaustive retrieval using Hypencoder while being substantially more efficient. We conduct extensive experiments on wide range of datasets to demonstrate the efficacy of Hypencoder. We demonstrate that our implementation of Hypencoder for single vector document representations outperforms competitive single vector dense and sparse retrieval models on MS MARCO [49] and TREC Deep Learning Track data [10, 13], in addition to complex retrieval tasks, such as TREC DL-Hard [43], TREC Tip-of-the-Tongue (TOT) Track [3], and the instruction following dataset FollowIR [69]. Across these benchmarks Hypencoder demonstrates consistent performance gain across experiments. Note that using the proposed approximation approach, retrieval from MS MARCO [49] with approximately 8.8 million documents only takes an average of 59.6 milliseconds per query on single NVIDIA L40S GPU. main advantage of hypernetworks in machine learning is their ability to learn generalizable representations. To demonstrate that Hypencoder also inherits this generalization quality, we evaluate our model under various domain adaptation settings: (1) adaptation to question answering datasets in biomedical and financial domains, and (2) adaptation to other retrieval tasks, including entity and argument retrieval, where Hypencoder again demonstrates superior performance compared to the baselines. We believe that these performance gains are just the by-product of our main contributions; Hypencoder introduces new way to think about what retrieval and relevance functions can be, it opens new world of possibilities by bridging the gap between neural networks and retrieval similarity functions. We believe Hypencoder is especially important at this time given the new demands for longer and more complex queries brought on by the widespread usage of large language models and it is our belief that Hypencoder represents an important step towards this goal. To help facilitate this goal we will open source all our code for training, retrieval, and evaluation. 1Available at https://github.com/jfkback/hypencoder-paper Conference17, July 2017, Washington, DC, USA Julian Killingback, Hansi Zeng, and Hamed Zamani IP ğ¸ğ‘ ğ¸ğ‘‘ pooler pooler ğ¸ğ‘‘ pooler s"
        },
        {
            "title": "MLP",
            "content": "combine ğ¸ğ‘ q-net input ğ¸ğ‘‘ pooler score-head pooler hyper-head encoder encoder encoder encoder encoder encoder encoder Vector Similarity Learned Similarity q Cross-encoding Similarity"
        },
        {
            "title": "Hypencoder",
            "content": "Figure 1: Overview comparing Hypencoder to existing retrieval and reranking paradigms. Gray arrows indicate the arrow does not represent an entity, it is the same as what it points to, in contrast black arrows do indicate unique entity which is always labeled."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Vector Space Models. Vector-based models that use sparse vectors have existed for decades, with each index representing term in the corpus vocabulary. Document-query similarity is computed using measures like ğ‘™2 distance, inner product, or cosine similarity, with term weighting methods such as TF-IDF being substantial focus to improve performance. With the emergence of deep neural networks, focus shifted to learning representations for queries and documents. SNRM by Zamani et al. [75] was the first deep learning model to retrieve documents from large corpora by learning latent sparse vectors. Following works leveraged pretrained transformer models like BERT [16] using single dense vector representations [32]. Recent improvements have focused on training techniques including self-negative mining [52, 53, 73, 77], data augmentation [38, 53], distillation [26, 37, 38], corpus-pretraining [19, 30, 40, 71], negative-batch construction [28] and curriculum learning [39, 76]. Alternative approaches include ColBERT [33], which uses multiple dense vectors, and SPLADE [18], which revisits sparse representations using pretrained masked language models. Though these methods vary substantially, they all share fundamental commonality, that relevance is based on an inner product (or in some cases cosine similarity). We believe that this is significant limitation of these methods and one which hampers the performance of these models on complex retrieval tasks. Our method circumvents this limitation by learning query-dependent small neural network that is fast enough to run on the entire collection (or used in an approximate way; see Section 3.6 for details). Learned Relevance Models. Light-weight relevance models using neural networks have demonstrated improved retrieval performance compared to simple methods like inner products. Early iterations came in the form of learning-to-rank models [8, 9] which use query and document features to produce relevance scores for reranking. While these models traditionally used engineered features, more recent approaches adopted richer inputs. For instance, MatchPyramid [51] and KNRM [72] use similarity matrices between non-contextualized word embeddings, while Duet [47, 48] combines sparse and dense term features in multi-layer perceptron. DRMM [74] utilized histogram features as input to neural networks for scoring. Since the advent of BERT [16], focus has shifted to making transformer models more efficient, such as PreTTR [42] which separately precomputes query and document hidden states. Recently, LITE [31] extended ColBERTs similarity using column-wise and row-wise linear layers for scoring. In the recommender systems community, learned similarity measures have been widely used [24, 25]. The common usage of neural scoring methods in recommendation has inspired research into efficient retrieval with more learned scoring signals. For instance, BFSG [62] supports efficient retrieval with arbitrary relevance functions by using graph of item representations and greedy search strategy over nodes of the graph. recent improvement on BFSG uses the scoring models gradient to prune directions that are unlikely to have relevant items [79]. Other works make use of queries to form query-item graph to produce more informative neighbors [61]. Our work differs from these works in one major way, we do not have query representation and document representation thus our method requires no combination step, instead we produce query-conditioned neural network for each query and directly apply this to the document representation. This approach can reduce the similarity networks size and does not require choosing between inference speed and larger query representations. Furthermore the flexibility of our framework means we can replicate any existing learned relevance model as discussed in Section 3.7. On broader note there has been surprisingly little work on neural based scoring for full-scale retrieval, especially in the modern era of transformer based encoders. We hope our work can be useful foundation and proof-of-concept for future work in this area. Hypernetworks. Hypernetworks also known as hypernets are neural networks which produce the weights for other neural networks. The term was first used by Ha et al. [22] who demonstrated the effectiveness of hypernetworks to generate weights for LSTM networks. Since then, hypernetworks have been used in variety of ways including neural architecture search [78], continual learning Hypencoder: Hypernetworks for Information Retrieval Conference17, July 2017, Washington, DC, USA [66], and few-shot learning [57, 59] to name few. Generally, hypernetworks take set of input embeddings that provide information about the type of task or network where the weights will be used. These embeddings are then projected to the significantly larger dimension of the weights of the main network. As the outputs of most hypernetworks are so large the hypernetworks themselves are often very simple such as few feed-forward layers in order to keep computation feasible. Our case is unique in that our hypernetwork, the Hypencoder, is much larger than the small scoring network which we call q-net (i.e. the main network). Additionally, to the best of our knowledge, this paper represents the first work to explore hypernetworks for first stage retrieval."
        },
        {
            "title": "3 HYPENCODER\nNeural ranking models can be generally categorized into early-\ninteraction and late-interaction models [14, 20, 27, 33, 34, 50, 68].\nCurrently, the most common implementation of early-interaction\nmodels is in the form of cross-encoders (Figure 1 (second from the\nleft)), where the query text ğ‘ and document text ğ‘‘ are concatenated\n(together with some predefined tokens or templates) and fed to a\ntransformer network that learns a joint representation of query\nand document and finally produces a relevance score. The joint rep-\nresentation prevents these models from being able to precompute\ndocument representations, thus they cannot be used efficiently on\nlarge corpora [21, 46].",
            "content": "The most popular implementation of late-interaction models follows bi-encoder (or two tower) network architecture (Figure 1 (left)), where query and document representations are computed separately and scoring function is used to estimate the relevance score. Formally, let ğ¸ğ‘ Rğ‘›â„ denote the representation learned for query ğ‘ consisting of ğ‘› â„-dimensional vectors. Similarly, ğ¸ğ‘‘ Rğ‘šâ„ denotes the representation learned for document ğ‘‘ consisting of ğ‘š vectors of the same dimensionality. The relevance score between ğ‘ and ğ‘‘ is computed as follows: ğœ“ (ğ¸ğ‘, ğ¸ğ‘‘ ) (1) where ğœ“ : Rğ‘›â„ Rğ‘šâ„ denotes the scoring function. In order to take advantage of efficient indexing techniques, such as an inverted index in the case of sparse representations [18, 75] or approximate nearest neighbor (ANN) search in the case of dense representations [32], many existing works use pooling techniques to obtain single vector representation for each query and document and then employs simple and light-weight scoring functions, such as inner product or cosine similarity. There also exist more expensive methods that do not use pooling and perform such light-weight scoring functions at the vector level and then aggregate them, such as the maximum inner product similarity used in ColBERT [33]. On the Limitations of Linear Similarity Functions (e.g., Inner Product). We believe the simple similarity functions used by existing bi-encoder models are not sufficient for modeling complex relationships between queries and documents. These functions inherently limit retrieval models to judge relevance in way that can be represented by an inner product. Furthermore, it has been shown that the ability to compress and reconstruct information is correlated with the size, and thus complexity, of neural models [15]. This result indicates that using relevance function as simple as an inner product likely reduces the amount of information that can be stored in fixed representation size. These factors explain why state-of-the-art dense retrieval models continue to underperform cross-encoder models, in terms of retrieval quality [36]. In the following, we show the limitations of inner products (as linear similarity function) by theoretically demonstrating the impossibility of inner products to produce perfect rankings for some queries, regardless of the method used to create the query and document embeddings. Let ğ¶ denote corpus of ğ‘ documents, each being represented by an â„-dimensional vector. perfect ranking of documents in ğ¶ for provided query is ranking where all relevant documents are ranked above all non-relevant documents. According to Radons Theorem [54], any set of â„ + 1 document vectors with â„ dimensions can be partitioned into two sets whose convex hulls intersect. An important application of Radons Theorem is in calculating the VapnikChervonenkis (VC) dimension [64] of â„-dimensional vectors with respect to linear separability. For any â„ + 2 vectors, the two subsets of Radon partition cannot be linearly separated. In other words, for ğ‘ > â„ + 1, there exists at least one group of documents that is not linearly separable from the rest. In the real world, since ğ‘ â„ + 1, there are indeed many such non-separable subsets. If any two of these subsets contain all the relevant documents for query, then no linear similarity function can perfectly separate relevant from irrelevant documents. This includes inner product similarity and guarantees that, for some query, there will be an imperfect ranking. To overcome these limitations with inner product similarity we use multi-layer neural network with query-conditioned weights as our similarity measure. As neural networks are universal approximators [29], Hypencoders similarity function can express far more complex functions than those expressed by inner products. related alternative approach with the same benefits takes the query and document representations, combines them (e.g., through concatenation or similarity matrices), and feeds them to neural network to serve as similarity function (Figure 1 (second from the right)). However, this approach suffers from the following shortcomings: (1) query and document representations now need to be combined before scoring adding latency proportional to the complexity of the method used to combine them; (2) having separate query and document representations increases the input dimension to the neural network further increasing latency; (3) for efficiency reasons, the query representation is often pooled or compressed before being input into network which reduces the information the model receives. Hypencoder addresses these shortcomings. Since the query is directly encoded as the neural networks weights no concatenation or other form of combining inputs is needed, the document representation can be directly input to the scoring network. This, in addition to the reduced network size from having only document representations as input, allows for substantial latency improvement. Further, as Hypencoder produces query-specific neural network, every weight can be used to store query-related information without any need for compression or additional overhead. Lastly, we show in Section 3.7 that existing learned relevance methods can be exactly replicated by Hypencoder with the additional flexibility of learning query-specific weights when desirable. Conference17, July 2017, Washington, DC, USA Julian Killingback, Hansi Zeng, and Hamed Zamani"
        },
        {
            "title": "3.1 Hypencoder Overview\nAn overview of our model is depicted in Figure 1 (right); it repre-\nsents a new category of models that sit between a cross-encoder and\na bi-encoder model. Like a bi-encoder model, our method computes\nthe query and document representations separately, but unlike most\nexisting retrieval methods, our method allows for more compli-\ncated matching signals like those present in cross-encoder models.\nFollowing existing methods, we have a query encoder and a docu-\nment encoder. When a document ğ‘‘ is input into to the document\nencoder, we obtain a representation similar to existing encoder\nmodels, namely a set of one or more vectors ğ¸ğ‘‘ âˆˆ Rğ‘šÃ—â„ that rep-\nresent the documentâ€™s content, where ğ‘š is the number of vectors\nand â„ is the dimension of the vectors. Though we focus on vectors\nin this work, in theory, the representation can be anything a neural\nnetwork can output.",
            "content": "Now comes our unique contribution that allows our method to consider more complex similarity signals. Given the query ğ‘, the query encoder Î¦ first produces set of contextualized embeddings in similar way to existing encoder models which we will call ğ¸ğ‘ Rğ‘›â„, where ğ‘› is the number of embeddings and â„ is the dimension of the embeddings. At this point while existing methods apply simple pooling mechanism, our query encoder instead uses hyper-head. The hyper-head takes ğ¸ğ‘ and produces set of matrices and vectors that are then used as the weights and biases for small neural network which we coin the q-net. The q-net is query-dependent function for estimating relevance scores for each document, meaning each q-net is unique to the query that created it, unlike existing neural scoring methods which use shared set of weights for all queries. To find the relevance of document, the document representation ğ¸ğ‘‘ is passed as input to the q-net which outputs the relevance score. Hypencoder is generic framework which allows direct application of existing paradigms from neural retrieval and, more broadly, machine learning. For example, Hypencoder could easily work with multiple vectors similar to existing multi-vector models, e.g., [33], or use training routines popularized in dense retrieval, e.g., [38, 52, 73, 76]. As an initial exploration, this paper focuses on showing the efficacy of Hypencoder without additional complexity and thus uses single vector document representation and no complex training recipes."
        },
        {
            "title": "3.2 Query and Document Encoders\nThe Hypencoder framework is generic and can be applied to any\nimplementation of query and document encoders. In this work, we\nuse pretrained transformer-based encoder models commonly used\nin the recent neural network literature. Specifically, we use a pre-\ntrained BERT base model [16] for encoding queries and documents.\nEven though Hypencoder can operate on all token representations\nproduced for each document this work focuses on a single vector\nrepresentation of documents, which is more efficient in terms of\nquery latency, memory requirements, and disk usage. To do so,\nwe can either use the contextualized embedding representing the\n[CLS] token or take the mean of all the contextualized embed-\ndings for all the non-pad input tokens. Empirically, we found that\nusing the [CLS] token performs better. Therefore, the document\nrepresentation produced by the encoder is a single vector with 768",
            "content": "dimensions, i.e., the same as BERTs output dimensionality. We refer to it as ğ¸ğ‘‘ Rğ‘šâ„, where ğ‘š = 1 in our setting. Since Hypencoder only uses the contextualized-query-token representations once to produce the q-net, it can skip pooling tokens without adding much cost. Therefore, we use all non-pad-token representations produced by the query encoder as the intermediate representation of the queries, denoted by ğ¸ğ‘ Rğ‘›â„, where ğ‘› is the number of tokens in the query ğ‘ and â„ is the embedding dimensionality (â„ = 768 in BERT)."
        },
        {
            "title": "3.3 The Hyperhead Layers\nThe method to transform ğ¸ğ‘ into the weights and biases for the q-\nnet is performed by the hyperhead layers and is completely flexible.\nDuring our experimentation, we tried two mechanisms to do this\ntransformation as well as many minor variants and found them all\nto have stable training, which suggests the Hypencoder framework\nis robust to the exact hyperhead layer implementation. Though\nwe tried two approaches, we settled on one for the final set of\nexperiments in this paper which we will now describe.",
            "content": "For improved clarity, we focus only on the weight creation process as the biases are created in the exact same way. The contextualized query embeddings ğ¸ğ‘ Rğ‘›â„ produced by the query encoder are independently transformed by ğ‘™ hyperhead layers, each of which corresponds to layer in the q-net. Each hyperhead layer converts the embeddings ğ¸ğ‘ into key and value matrices: ğ¾ğ‘ ğ‘– = ğœƒğ¾ğ‘– [ğ¸ğ‘; 1] ğ‘‰ ğ‘ ğ‘– = ğœƒğ‘‰ğ‘– [ğ¸ğ‘; 1] (2) where ğœƒğ¾ğ‘– , ğœƒğ‘‰ğ‘– Râ„â„ denote learnable parameters for constructing key and value matrices. In the above equation, the embedding matrix ğ¸ğ‘ is concatenated with column of all ones (i.e., [ğ¸ğ‘; 1]) to model both weight multiplication and bias addition. Each key matrix ğ¾ğ‘– and value matrix ğ‘‰ğ‘– will be used for the creation of the weights in the ğ‘–th layer of the q-net. With the keys and values in hand, single-head scaled-dot-product attention [65] is performed using query matrix ğ‘„ğ‘– Rğ‘Ÿ â„ where ğ‘Ÿ is the layer dimensionality in the ğ‘–th layer of q-net. In our case, all of the weights except the last layer are square matrices, making ğ‘Ÿ = â„. Each ğ‘„ğ‘– is set of learnable embeddings, similar to those used as input tokens for transformer models. Hence, the hidden layer representation ğ»ğ‘– Rğ‘Ÿ â„ is then computed as follows: (cid:32) ğ‘„ğ‘–ğ¾ğ‘‡ ğ‘– â„ ğ»ğ‘– = softmax ğ‘‰ğ‘– (3) (cid:33) ReLU activation [1] is then applied to each ğ»ğ‘– followed by layer normalization [4]. Next point-wise feed-forward layer is applied to produce (cid:98)ğ»ğ‘ ğ‘– : (cid:98)ğ»ğ‘ ğ‘– = ğœƒğ‘Šğ‘– L-Norm (ReLU(ğ»ğ‘– )) + ğœƒğ‘ğ‘– (4) where L-Norm denotes layer normalization. Note that each weight in q-net has unique ğœƒğ‘Šğ‘– and ğœƒğ‘ğ‘– . There are no learnable parameters in layer normalization. The final operation to get the ğ‘–th weight ğ‘Š ğ‘ ğ‘– for q-net is: ğ‘– = (cid:98)ğ»ğ‘ ğ‘Š ğ‘ ğ‘– + ğœƒğ»ğ‘– (5) Hypencoder: Hypernetworks for Information Retrieval Conference17, July 2017, Washington, DC, USA where ğœƒğ»ğ‘– Rğ‘Ÿ â„ is the same size as (cid:98)ğ»ğ‘ ğ‘– and acts as base weight which allows the model to learn universal (i.e., query-independent) patterns that are applicable for all queries. The process for the bias vectors is identical except the query matrix used in the attention operation ğ‘„ğ‘– Rğ‘Ÿ â„ has ğ‘Ÿ = 1 as there is only single column in the output."
        },
        {
            "title": "3.4 The q-net Network\nWeights and biases produced by the hyperhead layers are not by\nthemselves a neural network. They need a certain arrangement\nand additional components (e.g. non-linearity). This is where the\nHypencoderâ€™s q-net converter comes in. The converter knows the\narchitecture of the q-net and given the weights and biases from\nthe hyperhead layers, it produces a callable neural network object\nwhich takes as input the document representation ğ¸ğ‘‘ .",
            "content": "It is worth highlighting that because the q-nets architecture is not strictly tied to how the hyperhead layer produces the weights and biases, it is simple to modify the architecture of the q-net. All the hyperhead layers need to know is how many weights and biases are needed and what shape they should be. In our experiments, we use simple feed-forward architecture ğ‘–+1 for the input ğ‘¥ğ‘‘ ğ‘– at given layer ğ‘– is for the q-net. The output ğ‘¥ğ‘‘ given by: ğ‘¥ğ‘‘ ğ‘–+1 = L-Norm (cid:16) ReLU(ğ‘Š ğ‘ ğ‘– (ğ‘¥ğ‘‘ ğ‘– ) + ğ‘ğ‘ ğ‘– ) (cid:17) + ğ‘¥ğ‘‘ ğ‘– (6) where L-Norm represents layer normalization without learnable parameters and the addition of ğ‘¥ğ‘‘ is residual connection. No ğ‘– residual connection is applied before the final layer (i.e., layer ğ‘™). The layer in Equation (6) is repeated ğ‘™ times. Finally, relevance score is produced using linear projection layer with an output dimensionality of 1."
        },
        {
            "title": "3.6 Efficient Retrieval using Hypencoder\nBeing able to perform efficient retrieval is crucial for many real-\nworld search scenarios where an exhaustive search is not feasible.\nFor Hypencoder models, there is a clear parallel to dense models\nas both represent documents as dense vectors, but the differences\nbetween Hypencoder and dense models make it unclear whether\nthe same efficient search techniques will work. For instance, it\nis clear that due to the linear nature of inner products, similar\ndocument vectors are likely to have similar inner products with\na query vector; in the case of Hypencoder this assumption may\nnot hold true as the non-linear nature of the Hypencoder scoring\nfunction could mean small differences in the input vector produce\nsignificant differences in the output score.",
            "content": "To study the extent to which Hypencoders retrieval can be approximated for efficient retrieval, we developed an approximate search technique based loosely on navigating small world graphs [35, 45]. In the index stage we construct graph where documents are nodes connected to their neighbors by edges. We use ğ¿2 distance between document embeddings similar to [62]. After constructing the document graph, approximate search is performed following Algorithm 1. In brief, set of initial candidate documents ğ¶ is selected at random, these candidates are scored with the q-net (line 5) and in lines 16-19 the best ğ‘›ğ¶ğ‘ğ‘›ğ‘‘ğ‘–ğ‘‘ğ‘ğ‘¡ğ‘’ğ‘  and their neighbors become the next candidates. In lines 12-15, the top scoring candidates are added to ğ‘‡ set which stores the ğ‘˜ best scoring documents so far. The algorithm terminates when one of three conditions is met: (1) the number of iterations equals ğ‘šğ‘ğ‘¥ğ¼ğ‘¡ğ‘’ğ‘Ÿ ; see line 4, (2) there are no more candidates; see line 4, or (3) no new documents are added to ğ‘‡ at given step; see line 8. We also consider an option without the final termination condition which we call without early stopping. As the number of operations is dependent on the number of initial candidates ğ¶ , the running time is not tied to the number of documents, resulting in run time complexity of ğ‘‚ ( ğ¶ + ğ‘›ğ¶ğ‘ğ‘›ğ‘‘ğ‘–ğ‘‘ğ‘ğ‘¡ğ‘’ğ‘  ğ‘šğ‘ğ‘¥ğ¼ğ‘¡ğ‘’ğ‘Ÿ ). With this algorithm, we found that Hypencoder is able to significantly increase retrieval speed without large loss in quality. See the results in Section 4.4.4. Algorithm 1 Hypencoder Efficient Search Input: q-net ğ‘, #NN to return ğ‘˜, initial candidates ğ¶, # candidates to explore every iteration ğ‘›ğ¶ğ‘ğ‘›ğ‘‘ğ‘–ğ‘‘ğ‘ğ‘¡ğ‘’ğ‘ , ğ‘šğ‘ğ‘¥ğ¼ğ‘¡ğ‘’ğ‘Ÿ Output: ğ‘˜ closest neighbors to ğ‘ 1: ğ‘£ ğ¶ set of visited elements 2: ğ‘‡ {} Stores top ğ‘˜ nearest neighbors to ğ‘ at any given time 3: ğ‘– 0 4: while ğ¶ > 0 and ğ‘– < ğ‘šğ‘ğ‘¥ğ¼ğ‘¡ğ‘’ğ‘Ÿ do 5: ğ‘ find top ğ‘›ğ¶ğ‘ğ‘›ğ‘‘ğ‘–ğ‘‘ğ‘ğ‘¡ğ‘’ğ‘  values in ğ¶ using ğ‘ ğ‘“ get lowest scoring element from ğ‘‡ if max Ë†ğ‘ ğ‘ Ë†ğ‘ < ğ‘“ then Current iteration break all candidates are worse than ğ‘‡ so stop now Reset ğ¶ ğ¶ {} for each ğ‘’ ğ‘ do ğ‘“ get lowest scoring element from ğ‘‡ if ğ‘(ğ‘’) > ğ‘“ or ğ‘‡ < ğ‘˜ then ğ‘‡ ğ‘‡ ğ‘’ if ğ‘‡ > ğ‘˜ then ğ‘‡ ğ‘‡ {ğ‘“ } for each ğ‘› NEIGHBORS(ğ‘’) do if ğ‘› ğ‘£ then ğ¶ ğ¶ ğ‘› ğ‘£ ğ‘£ ğ‘› 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: ğ‘– ğ‘– + 1 20: 21: return ğ‘‡"
        },
        {
            "title": "3.7 Comparison to Existing Neural IR Methods\nWe argue that Hypencoder can exactly reproduce existing neural\nranking models. Let us start by formalizing the main components\nof existing neural methods: (1) a query representation ğ¸ğ‘, (2) a",
            "content": "Conference17, July 2017, Washington, DC, USA Julian Killingback, Hansi Zeng, and Hamed Zamani document representation ğ¸ğ‘‘ , (3) some combination function ğ‘“ğ‘ (, ), and (4) the final neural network that produces score ğ‘“ğ‘  (). In comparison, Hypencoder does not have an ğ‘“ğ‘ (, ) as the q-net takes ğ¸ğ‘‘ as its only input. We will now demonstrate that Hypencoder can exactly replicate any neural retrieval method that has the components above. The first step is to include ğ¸ğ‘ and ğ‘“ğ‘ (, ) in the q-net. This allows the q-net to exactly produce the input to ğ‘“ğ‘  () that the existing neural retriever used. Next we reproduce the neural function from ğ‘“ğ‘  () with query-dependent weights in the q-net. When ğ¸ğ‘‘ is input to the q-net, all the original components of the existing neural retrieval model are present and thus the score can be exactly replicated. There is one difference, which is the weights of ğ‘“ğ‘  () are querydependent. However, this can be remedied in two ways: (1) shared weights can be used for all queries exactly replicating the original neural method (2) the weights for ğ‘“ğ‘ can have common non-querydependent base weight, similar to our implementation (see details in Section 3.3), this way if there is no benefit using query-dependent weights the shared weight can be used, but if there is additional benefit the model and optimizer can learn to take advantage of it. Thus, Hypencoder can not only exactly replicate all existing neural retrieval methods it also allows the model to dynamically leverage query-dependent weights when the model determines they are beneficial."
        },
        {
            "title": "4 EXPERIMENTS\n4.1 Datasets\n4.1.1 Training Dataset. The dataset used for training our models\nis the training split of the MSMARCO passage retrieval dataset [49]\nwhich contains 8.8M passages and has 503K training queries with\nat least one corresponding relevant passage. The queries in the\nMSMARCO training set are short natural language questions asked\nby users of the Bing search engine.",
            "content": "To create the training pairs, we first retrieved 800 passages for every query using an early iteration of Hypencoder. From these, we sampled 200 passages the top 100 passages and another 100 randomly sampled from the remaining 700 passages. These querypassage pairs were then labeled using the MiniLM cross-encoder2 from the Sentence Transformers Library [55]. 4.1.2 Validation Dataset. For validating and parameter tuning, we use the TREC Deep Learning (DL) 2021 [11] and 2022 passage task [11, 12]. As the passage collection for TREC DL 21 and 22 is large and we wanted validation to be fast we created subset with only passages in the QREL files. 4.1.3 Evaluation Datasets. Our evaluation explores retrieval performance in three different areas: in-domain performance, out-ofdomain performance, and performance on hard retrieval tasks. For in-domain performance, we use the MSMARCO Dev set [49], TREC Deep Learning 2019 [13], and TREC Deep Learning 2020 [10]. The MSMARCO Dev set contains around 7k queries with shallow labels, the majority of queries only have single passage labeled as relevant. This collection uses queries from the same distribution as the training queries making it clear test of the in-domain performance. On this dataset we report the standard evaluation metrics: MRR and Recall@1000. The TREC Deep Learning 2019 and 2020 datasets have similar query distribution to MSMARCO Dev but feature far fewer queries, i.e., 97 queries combined. The lower number of queries is compensated by far deeper annotations with every query having several annotated passages. To assess out-of-domain performance, we evaluate on question answering tasks on different domains, specifically, the TREC COVID [56] and NFCorpus datasets [7] for the biomedical domain and FiQA [44] for the financial domain. We also evaluate on DBPedia [23] as an entity retrieval dataset and on TouchÃ© [6] as an argument retrieval dataset. We use the BEIR [63] versions of these datasets from the ir_datasets library. To explore the full capabilities of Hypencoder we want to evaluate how it performs on retrieval tasks that are more challenging than standard question-passage retrieval tasks. To some extent hardness is subjective, but we tried to define clear set of criteria to define difficulty: (1) current neural retrieval models should struggle on the task, (2) term matching models like BM25 should also struggle on the task, (3) the queries are longer or otherwise more complicated than standard web queries. An additional requirement we had was that for tasks that were significantly different from the MSMARCO training data we wanted adequate training data to finetune the models before evaluation. We believe this is reasonable as we are not investigating the models zero-shot performance but the inherent limits of the model. The first dataset we select was the TREC Tip-of-the-Tongue (TOT) 2023 [3] that contains queries written by users that know many aspects of the item they are looking for but not the name of the item. Thus TOT queries tend to be verbose and can include many facets. The TREC TOT 2023 dataset specifically looks at TOT queries for movies with the corresponding movies Wikipedia page as the golden passage. We use the development set as the test set relevance labels are not public yet. There are 150 queries. Each query has single relevant passage. For training we use the data from Bhargav et al. [5] which is around 15k TOT queries from Reddit for the book and movie domain. The second dataset is FollowIR [69] for instruction following in retrieval. This dataset is built on top of three existing TREC datasets: TREC Robust 04 [67], TREC News 21 [60], and TREC Core 17 [2]; it uses the fact that these datasets include instructions to the annotators which can act as complex instruction. To test how well retrieval system follows the instruction the creators of FollowIR modify the instruction to be more specific and re-annotate the known relevant documents. As training data we use MSMARCO with Instructions, recent modification of MSMARCO which adds instructions to the queries as well as new positive passages and hard negative passages which consider the instruction [70]. The final dataset is subset of TREC DL-HARD [43]. The full dataset uses some of the queries from TREC DL 2019 and 2020 as well as some queries that were considered for DL 2019 and 2020 but were not included in the final query collection. TREC DL-HARD is built specifically with the hardest queries from the TREC DL pool. The authors do so by using commercial search engine to find queries that are not easily answered. The standard TREC DLHARD dataset has 50 queries half of which appear in TREC DL 2Available at https://huggingface.co/cross-encoder/ms-marco-MiniLM-L-12-v2. 3Available at https://ir-datasets.com/. Hypencoder: Hypernetworks for Information Retrieval Conference17, July 2017, Washington, DC, USA 0.72 0.7 0.68 0.66 0 1 @ n ) ( e y Q 150 100 50 0 1 @ n 0.72 0.7 0.68 0.66 0. ) ( e y Q 800 600 400 200 0 0 1 @ n 0.7 0.6 0.5 0.4 0. 0 w/ early stop w/o early stop 10 20 ğ‘šğ‘ğ‘¥ğ¼ğ‘¡ğ‘’ğ‘Ÿ 30 200 150 100 ) ( e y Q 0 500 ğ‘›ğ¶ğ‘ğ‘›ğ‘‘ğ‘–ğ‘‘ğ‘ğ‘¡ğ‘’ğ‘  1, 101 105 103 ğ¶ Figure 2: Relationship between the three main parameters of our efficient search: the size of the initial set of candidates ğ¶, the number of neighbors to explore ğ‘›ğ¶ğ‘ğ‘›ğ‘‘ğ‘‘ğ‘–ğ‘‘ğ‘ğ‘¡ğ‘’ğ‘ , and the number of iterations ğ‘šğ‘ğ‘¥ğ¼ğ‘¡ğ‘’ğ‘Ÿ and both effectiveness in terms of TREC DL 19 nDCG@10 and efficiency in terms of Query Latency. 2019 or TREC DL 2020 and half of which are new queries which are labeled by the authors of TREC DL-HARD. We found that the queries labeled by the authors had far fewer judged documents in the top 10 documents compared to those labeled by TREC (around 15% versus 93%), this made the evaluation metrics unreliable so we decided to only use those with TREC labeling."
        },
        {
            "title": "4.2 Experimental Setup\n4.2.1 Training Details. All the Hypencoders use BERT [16] base\nuncased as the base model. We use PyTorch and Huggingface for\nmodel implementation and training. All of our q-nets use a input\ndimension of 768 and hidden dimension of 768. Unless otherwise\nstated, we use 6 linear layers in the model not including the final\noutput projection layer.",
            "content": "We use training batch size of 64 per device and 128 in total. single example in the batch is query, positive document, and 8 additional documents ranging in relevance. The positive document is the top ranked document by our teacher model. The other 8 documents are sampled randomly from the passages associated with the query. For more details about the dataset see Section 4.1.1. Passages were truncated to 196 tokens and queries to 32 tokens. Our primary loss function is Margin MSE [26]. When computing the loss, we construct (query, positive document, negative document) triplets where all of the negatives for query form their own triplet. The loss is found by averaging the individual loss of all the triplets. In addition to Margin MSE, we use an in-batch cross entropy loss where the (query, positive document) is assumed to be true positive and all the other queries positive documents are assumed to be negatives. We do not consider the additional hard negatives from the query in the cross entropy loss as many of these documents are relevant to the query. We use AdamW as our optimizer with default settings and learning rate of 2e-5 with constant scheduler after warm up of 6k steps. Our training hardware consist of two A100 GPUs with 80GB memory. Training took around 6 days. To select the best model we evaluate each model on the validation set every 50k steps and pick the model with the best Precision within the first 800k steps. We selected Precision due to the fact that it balances both recall and precision in single metric and does not require predefined cutoff. When training for the harder tasks we use AdamW with the learning rate 8e-6 with linear scheduler and warm-up ratio of 1e-1. For TOT training we train for 25 epochs or 3.3k steps. For FollowIR training we train for 1 epoch or around 10k steps. We use batch-size of 96 and cross entropy loss. Each example in the batch includes query, positive document, and hard negative document. We use maximum document and query length of 512 tokens."
        },
        {
            "title": "4.4 Results and Discussion\nIn-Domain Results. Our in-domain results are presented in\n4.4.1\nTable 1; they demonstrate that compared with baselines and even\nthe reference models Hypencoder has very strong performance.\nHypencoder is significantly better than each baseline in nDCG@10\non the combined TREC DL â€™19 and â€™20 and statistically better than\nall but CL-DRD on MSMARCO Dev RR@10. The most direct com-\nparison, BE-Base, has far lower nDCG@10, RR, and RR@10 values\nindicating the Hypencoder is able to bring a large boost in precision\nbased metrics over dense retrieval. In terms of recall Hypencoder\nis either as good or better than all the baselines though the gap is\nnot as large as for precision based metrics.",
            "content": "Conference17, July 2017, Washington, DC, USA Julian Killingback, Hansi Zeng, and Hamed Zamani Table 1: Comparison on in-domain evaluation datasets. The symbols next to each baseline indicate significance values with ğ‘ < 0.05. Note, that is group of baselines. Table 2: Out-of-domain results in nDCG@10. We only compare significance with BE-Base. Significance results with ğ‘ < 0.05 are shown with the and ğ‘ < 0.1 are shown with . Model nDCG@10 RR R@1000 RR@10 TREC-DL 19 & 20 MSMARCO Dev R@1000 Single Vector Dense Retrieval Models & BM25 (Baselines) BM25 ANCE TCT-ColBERT Margin MSE TAS-B CL-DRD BE-Base 0.491 0.646 0.669 0.669 0.700 0.701 0.713 0.736 0.679 0.811 0.820 0.845 0.863 0.844 0.855 0.885 0.735 0.767 0.806 0.782 0.861 0.838 0.868 0.871 0.184 0.330 0.335 0.325 0.344 0.382 0.359 0.386 Hypencoder Other Retrieval Models (Reference Models) ColBERT v2 0.749 SPLADE++ SD 0.723 RepLLaMA 0.731 DRAGON 0.734 MonoBERT 0.722 cross-SimLM 0.735 - - - - - - - - - - - - 0.397 0.368 0.412 0.393 0.372 0.437 0.853 0.958 0.964 0.955 0.978 0.981 0.980 0.981 0.984 0.979 0.994 0.985 0.853 0.987 Impressively Hypencoder is able to surpass DRAGON on nDCG@10 on the combined TREC DL 19 and 20 query set, though DRAGON uses the same base model and is bi-encoder, it uses 32 A100s to train, 40x the training queries, and complex 5 teacher curriculum learning distillation training technique. In other words, DRAGON is likely close to if not the ceiling for BERT-based bi-encoders and still Hypencoder is able to match it with simple distillation training setup and far less training compute. Hypencoder also beats both rerankers MonoBERT and crossSimLM; demonstrating that reranking cannot make up for weak retrievers performance. Continuing in TREC-DL 19 and 20 we find that Hypencoder even surpassed RepLLaMA which is more than 60x larger and which also uses significantly larger document embedding dimension of 4096. In fact the only model beating Hypencoder in nDCG@10 is ColBERTv2 which uses an embedding for every token in the document compared to Hypencoders fixed 768 dimension token. MSMARCO Dev results are also good with Hypencoder outperforming all the baselines and outperforming few of the reference models such as SPLADE++ and MonoBERT. Overall Hypencoders in-domain results are exceptionally strong given the simple training routine used, small encoder model size, and document representation size. To the best of our knowledge, Hypencoder sets new record for combined TREC-DL 19 and 20 nDCG@10 with 768 dimension dense document vector. 4.4.2 Out-of-Domain Results. Table 2 shows our results on the select out-of-domain datasets, we only include our main baseline models and BM25 due to space limitations. The general trend is that Hypencoder has strong out-of-domain performance in question answering tasks (Q&A) and entity retrieval tasks. This indicates that despite Hypencoders more complex similarity function it is still able to generalize well in zero-shot manner to new tasks. 4.4.3 Results on Harder Retrieval Tasks. The results on the harder retrieval tasks are in Table 3, like in the out-of-domain section we only consider the main baseline models and BM25. We can see that in the harder tasks Hypencoder remains dominant over Baselines Ours Rep type sparse dense dense dense hypernet BM TAS-B CL-DRD BE-Base Hypecoder & TREC-Covid FiQA NFCorpus Misc. DBPedia TouchÃ© v2 0.656 0.236 0.325 0.481 0.300 0.319 0.584 0.308 0. 0.313 0.367 0.384 0.162 0.381 0.203 0.651 0.309 0.327 0.405 0.240 0.688 0.314 0. 0.419 0.258 the baseline models with higher retrieval metrics in all but one column. Additionally, the relative improvement compared to the in-domain results are higher (on all metrics that Hypencoder is the best for) suggesting that on harder tasks the added complexity that can be captured by Hypencoders similarity function is especially important. Additionally the high performance on TREC tip-of-thetongue (TOT) and FollowIR indicate that Hypencoder adapts well to different domains through domain-specific fine-tuning. On the evaluated subset of TREC DL-HARD we see that Hypencoder has stronger precision metrics than the baselines by large margin. As mentioned previously the higher relative improvement suggests that Hypencoder is especially dominant on harder tasks which, in part, explains its higher performance on TREC DL 19 and 20. Though on in-domain dataset Hypencoder does better or the same on recall metrics, on TREC DL-HARD BE-Base has higher recall than Hypencoder. We suspect that this may be because the relevance function that the q-net applies is not smooth, which has the benefit of being more discerning and likely accounts for some of the precision gains. However, if the q-net makes mistake the non-smooth scoring could result in much harsher score than the linear inner product is capable of producing. Moving to TREC tip-of-my-tongue (TOT) we see that Hypencoder continues to perform well. Tip-of-the-tongue is complex retrieval task with long queries and passages and multiple aspects, the fact Hypencoder outperforms the baselines by large margin validates the need for more complex relevance function. Finally we have FollowIR which has three subsets on all three Hypencoder has the best performance on the retrieval evaluation metrics of choice, in many cases by sizable amount. Beside the retrieval evaluation metrics we also include p-MRR which is metric released in the FollowIR [69] paper. The metric measures the change in document ranks before and after an instruction is modified to see how well the model responses to the additional requirements. p-MRR of 0 indicates no change in document rank based on the instruction change and p-MRR of +100 indicates the documents were perfectly changed based on the instruction while -100 indicates the opposite. For additional details we refer readers to the original FollowIR paper [69]. As p-MRR is relative to each models performance before the instructions are modified it is not indicative of stand-alone retrieval performance. With that said, Hypencoder is the only model to achieve positive p-MRR Hypencoder: Hypernetworks for Information Retrieval Conference17, July 2017, Washington, DC, USA Table 3: Evaluation metrics for the harder set of tasks which include TREC DL-HARD, TREC Tip-of-my-tongue TOT, and FollowIR. Significance is shown at ğ‘ < 0.1. For FollowIR we do not perform significance tests on BM25. Model nDCG@10 RR R@ TREC DL-HARD nDCG@10 TREC TOT DEV RR nDCG@1000 FollowIR Robust 04 AP p-MRR FollowIR News 21 p-MRR nDCG@5 BM25 TAS-B CL-DRD BE-Base Hypencoder 0.466 0.574 0.573 0.607 0.630 0.813 0.789 0.790 0.864 0.887 0.646 0.777 0.719 0.805 0.798 0.086 0.097 0.088 0.121 0.134 0.088 0.089 0.082 0.110 0.125 0.131 0.162 0.151 0.179 0.182 0.121 0.203 0.206 0.207 0.212 -3.1 -5.4 -7.2 -3. -3.5 0.193 0.263 0.240 0.239 0.272 -2.1 -0.8 -0.3 -1.1 2.0 FollowIR Core AP 0.081 0.170 0.162 0.178 0.193 p-MRR -1.1 -10.0 -12.1 -7.7 -11. indicating it correctly modified the document ranking based on the instruction. This is no small feat as in the original FollowIR paper no retrieval model of the size of Hypencoder was able to get positive p-MRR and even many much larger models trained on large instruction retrieval datasets could not get positive result. 4.4.4 Analysis of Efficiency. To be able to search large-scale collections Hypencoder has to work well while only doing computation on small subset of the document corpus. This led us to develop the efficient algorithm in Section 3.6. To quantify the performance we do set of experiments varying the key parameters to see how each impacts both search quality and query latency. The results of these experiments can be seen in Figure 2. Note the document-todocument graph used has 100 neighbors per document. The figures show that each parameter has an important role in search quality. The largest role is played by ğ‘šğ‘ğ‘¥ğ¼ğ‘¡ğ‘’ğ‘Ÿ which can drastically reduce search performance if not set high enough, but which plateaus after around 12. The value of ğ‘›ğ¶ğ‘ğ‘›ğ‘‘ğ‘–ğ‘‘ğ‘ğ‘¡ğ‘’ğ‘  follows similar pattern with drastic increase followed by plateau. The parameter ğ¶, the number of initial candidates, is unique in that it has more gradual rise and is the only parameter that causes decrease in effectiveness if raised too high. We suspect this happens because only ğ‘›ğ¶ğ‘ğ‘›ğ‘‘ğ‘–ğ‘‘ğ‘ğ‘¡ğ‘’ğ‘  are explored at each iteration but all candidates in ğ¶ are considered visited and thus are not explored in the future. This could mean that many good results from the initial candidates are not explored, leaving potentially good neighbors of these nodes undiscovered. Another interesting aspect of ğ¶ is that time decreases as it increases when early stopping is used. This is likely because high quality candidates are found sooner allowing search to end more quickly. In general, our approximation seems to behave as expected in terms of both time and retrieval performance. With the insights from our analysis above we developed two configurations, one which optimizes for speed and one that optimizes for retrieval quality. These two configurations compared against exhaustive search can be seen in Table 4. Both approximate configurations significantly decrease the retrieval time when compared to the exhaustive approach. Impact of q-net Depth. Hypencoder performance suggests 4.4.5 that having more complex relevance function does indeed help improve retrieval performance as we had hypothesized. This raises the question: what is the optimal complexity of this relevance function. To answer this question we trained four versions of Hypencoder each trained to produce q-net with different number of layers. We selected [2, 4, 6, 8] as the number of q-net layers. The results can be seen in Figure 3. The experiment shows that there is benefit beyond two layers and that at least four is required Table 4: Average query latency and nDCG@10 on TREC DL 19 and 20 with efficient search. Efficient 1 uses parameters ( ğ¶ = 10000, ğ‘›ğ¶ğ‘ğ‘›ğ‘‘ğ‘–ğ‘‘ğ‘ğ‘¡ğ‘’ğ‘  = 64, ğ‘šğ‘ğ‘¥ğ¼ğ‘¡ğ‘’ğ‘Ÿ = 16), Efficient 2 uses parameters ( ğ¶ = 100000, ğ‘›ğ¶ğ‘ğ‘›ğ‘‘ğ‘–ğ‘‘ğ‘ğ‘¡ğ‘’ğ‘  = 328, ğ‘šğ‘ğ‘¥ğ¼ğ‘¡ğ‘’ğ‘Ÿ = 20). All model inference was performed on an NVIDIA L40S with BF16 precision. Search Type Query Lat. (ms) DL 19 DL 20 Exhaustive Efficient 1 Efficient 2 1769.8 59.6 231.1 0.742 0.702 0.722 0.731 0.730 0.731 0 1 @ n 0.74 0.74 0.73 0.73 0.72 2 4 6 8 Number of q-net layers Figure 3: Average nDCG@10 on TREC DL 19 and 20 versus number of layers in the q-net. for the best performance. Performance stays the same with six, indicating that this might be the point of diminishing returns. Lastly, eight layers decrease performance. There could be number of reasons for this including that eight layers are harder to optimize or because effectively learning how to use all eight layers takes longer and thus might achieve better performance with extended training time."
        },
        {
            "title": "5 CONCLUSION\nWe propose a new class of retrieval model, the Hypencoder which\novercomes the limitations of inner product based similarity func-\ntions that we prove to exist. Our model achieves a new state-of-the-\nart on TREC DL â€™19 and â€™20 for BERT sized encoder models with\na single dense document vector and shows even stronger relative\nimprovement on harder retrieval tasks such as tip-of-the-tongue\nqueries. Further we demonstrate that learned relevance models can\nbe applied to large-scale search corpus in an efficient way with\nour proposed approximate search algorithm. As Hypencoder is a\nflexible framework there is much interesting future work to ex-\nplore, such as multi-vector document representations and corpus\npretraining to name a few.",
            "content": "Conference17, July 2017, Washington, DC, USA Julian Killingback, Hansi Zeng, and Hamed Zamani ACKNOWLEDGMENTS This work was supported in part by the Center for Intelligent Information Retrieval, in part by the NSF Graduate Research Fellowships Program (GRFP) Award #1938059, and in part by the Office of Naval Research contract number N000142412612. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsor. REFERENCES [1] Abien Fred Agarap. 2018. Deep Learning using Rectified Linear Units (ReLU). CoRR abs/1803.08375 (2018). arXiv:1803.08375 http://arxiv.org/abs/1803.08375 [2] James Allan, Donna K. Harman, E. Kanoulas, Dan Li, Christophe Van Gysel, and Ellen M. Voorhees. 2017. TREC 2017 Common Core Track Overview. In Text Retrieval Conference. https://api.semanticscholar.org/CorpusID:38019792 [3] Jaime Arguello, Samarth Bhargav, Fernando Diaz, Evangelos Kanoulas, and Bhaskar Mitra. 2023. Overview of the TREC 2023 Tip-of-the-Tongue Track. In The Thirty-Second Text REtrieval Conference Proceedings (TREC 2023), Gaithersburg, MD, USA, November 14-17, 2023 (NIST Special Publication, Vol. 500-xxx), Ian Soboroff and Angela Ellis (Eds.). National Institute of Standards and Technology (NIST). https://trec.nist.gov/pubs/trec32/papers/Overview_tot.pdf [4] Lei Jimmy Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. 2016. Layer Normalization. CoRR abs/1607.06450 (2016). arXiv:1607.06450 http://arxiv.org/abs/1607. [5] Samarth Bhargav, Georgios Sidiropoulos, and Evangelos Kanoulas. 2022. Its on the tip of my tongue: new Dataset for Known-Item Retrieval. In WSDM 22: The Fifteenth ACM International Conference on Web Search and Data Mining, Virtual Event / Tempe, AZ, USA, February 21 - 25, 2022, K. Selcuk Candan, Huan Liu, Leman Akoglu, Xin Luna Dong, and Jiliang Tang (Eds.). ACM, 4856. https: //doi.org/10.1145/3488560.3498421 [6] Alexander Bondarenko, Maik FrÃ¶be, Meriem Beloucif, Lukas Gienapp, Yamen Ajjour, Alexander Panchenko, Chris Biemann, Benno Stein, Henning Wachsmuth, Martin Potthast, and Matthias Hagen. 2020. Overview of TouchÃ© 2020: Argument Retrieval. In Working Notes of CLEF 2020 - Conference and Labs of the Evaluation Forum, Thessaloniki, Greece, September 22-25, 2020 (CEUR Workshop Proceedings, Vol. 2696), Linda Cappellato, Carsten Eickhoff, Nicola Ferro, and AurÃ©lie NÃ©vÃ©ol (Eds.). CEUR-WS.org. https://ceur-ws.org/Vol-2696/paper_261.pdf [7] Vera Boteva, Demian Gholipour Ghalandari, Artem Sokolov, and Stefan Riezler. 2016. Full-Text Learning to Rank Dataset for Medical Information Retrieval. In Advances in Information Retrieval - 38th European Conference on IR Research, ECIR 2016, Padua, Italy, March 20-23, 2016. Proceedings (Lecture Notes in Computer Science, Vol. 9626), Nicola Ferro, Fabio Crestani, Marie-Francine Moens, Josiane Mothe, Fabrizio Silvestri, Giorgio Maria Di Nunzio, Claudia Hauff, and Gianmaria Silvello (Eds.). Springer, 716722. https://doi.org/10.1007/978-3-319-30671-1_58 [8] Christopher Burges, Robert Ragno, and Quoc Le. 2006. Learning to rank with nonsmooth cost functions. Advances in neural information processing systems 19 (2006). [9] Christopher J. C. Burges, Tal Shaked, Erin Renshaw, Ari Lazier, Matt Deeds, Nicole Hamilton, and Gregory N. Hullender. 2005. Learning to rank using gradient descent. In Machine Learning, Proceedings of the Twenty-Second International Conference (ICML 2005), Bonn, Germany, August 7-11, 2005 (ACM International Conference Proceeding Series, Vol. 119), Luc De Raedt and Stefan Wrobel (Eds.). ACM, 8996. https://doi.org/10.1145/1102351.1102363 [10] Nick Craswell, Bhaskar Mitra, Emine Yilmaz, and Daniel Campos. 2020. Overview of the TREC 2020 Deep Learning Track. In Proceedings of the Twenty-Ninth Text REtrieval Conference, TREC 2020, Virtual Event [Gaithersburg, Maryland, USA], November 16-20, 2020 (NIST Special Publication, Vol. 1266), Ellen M. Voorhees and Angela Ellis (Eds.). National Institute of Standards and Technology (NIST). https://trec.nist.gov/pubs/trec29/papers/OVERVIEW.DL.pdf [11] Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Campos, and Jimmy Lin. 2021. Overview of the TREC 2021 Deep Learning Track. In Proceedings of the Thirtieth Text REtrieval Conference, TREC 2021, online, November 15-19, 2021 (NIST Special Publication, Vol. 500-335), Ian Soboroff and Angela Ellis (Eds.). National Institute of Standards and Technology (NIST). https://trec.nist.gov/pubs/trec30/ papers/Overview-DL.pdf [12] Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Campos, Jimmy Lin, Ellen M. Voorhees, and Ian Soboroff. 2022. Overview of the TREC 2022 Deep Learning Track. In Proceedings of the Thirty-First Text REtrieval Conference, TREC 2022, online, November 15-19, 2022 (NIST Special Publication, Vol. 500-338), Ian Soboroff and Angela Ellis (Eds.). National Institute of Standards and Technology (NIST). https://trec.nist.gov/pubs/trec31/papers/Overview_deep.pdf [13] Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Campos, and Ellen M. Voorhees. 2020. Overview of the TREC 2019 deep learning track. CoRR abs/2003.07820 (2020). arXiv:2003.07820 https://arxiv.org/abs/2003.07820 [14] Mostafa Dehghani, Hamed Zamani, Aliaksei Severyn, Jaap Kamps, and W. Bruce Croft. 2017. Neural Ranking Models with Weak Supervision. In Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval (Shinjuku, Tokyo, Japan) (SIGIR 17). Association for Computing Machinery, New York, NY, USA, 6574. https://doi.org/10.1145/ 3077136.3080832 [15] GrÃ©goire DelÃ©tang, Anian Ruoss, Paul-Ambroise Duquenne, Elliot Catt, Tim Genewein, Christopher Mattern, Jordi Grau-Moya, Li Kevin Wenliang, Matthew Aitchison, Laurent Orseau, Marcus Hutter, and Joel Veness. 2024. Language Modeling Is Compression. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net. https://openreview.net/forum?id=jznbgiynus [16] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), Jill Burstein, Christy Doran, and Thamar Solorio (Eds.). Association for Computational Linguistics, 41714186. https://doi.org/10.18653/V1/N19-1423 [17] Thibault Formal, Carlos Lassance, Benjamin Piwowarski, and StÃ©phane Clinchant. 2022. From Distillation to Hard Negative Sampling: Making Sparse Neural IR Models More Effective. In SIGIR 22: The 45th International ACM SIGIR Conference on Research and Development in Information Retrieval, Madrid, Spain, July 11 - 15, 2022, Enrique AmigÃ³, Pablo Castells, Julio Gonzalo, Ben Carterette, J. Shane Culpepper, and Gabriella Kazai (Eds.). ACM, 23532359. https://doi.org/10.1145/ 3477495. [18] Thibault Formal, Benjamin Piwowarski, and StÃ©phane Clinchant. 2021. SPLADE: Sparse Lexical and Expansion Model for First Stage Ranking. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval. Association for Computing Machinery, New York, NY, USA, 22882292. https://doi.org/10.1145/3404835.3463098 [19] Luyu Gao and Jamie Callan. 2022. Unsupervised Corpus Aware Language Model Pre-training for Dense Passage Retrieval. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022, Smaranda Muresan, Preslav Nakov, and Aline Villavicencio (Eds.). Association for Computational Linguistics, 28432853. https://doi.org/10.18653/V1/2022.ACL-LONG.203 [20] Luyu Gao, Zhuyun Dai, and Jamie Callan. 2021. COIL: Revisit Exact Lexical Match in Information Retrieval with Contextualized Inverted List. In North American Chapter of the Association for Computational Linguistics. https: //api.semanticscholar.org/CorpusID:233241070 [21] Jiafeng Guo, Yixing Fan, Liang Pang, Liu Yang, Qingyao Ai, Hamed Zamani, Chen Wu, W. Bruce Croft, and Xueqi Cheng. 2020. Deep Look into neural ranking models for information retrieval. Information Processing & Management 57, 6 (2020), 102067. https://doi.org/10.1016/j.ipm.2019.102067 [22] David Ha, Andrew M. Dai, and Quoc V. Le. 2017. HyperNetworks. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net. https://openreview. net/forum?id=rkpACe1lx [23] Faegheh Hasibi, Fedor Nikolaev, Chenyan Xiong, Krisztian Balog, Svein Erik Bratsberg, Alexander Kotov, and Jamie Callan. 2017. DBpedia-Entity v2: Test Collection for Entity Search. In Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval, Shinjuku, Tokyo, Japan, August 7-11, 2017, Noriko Kando, Tetsuya Sakai, Hideo Joho, Hang Li, Arjen P. de Vries, and Ryen W. White (Eds.). ACM, 12651268. https://doi.org/ 10.1145/3077136. [24] Xiangnan He and Tat-Seng Chua. 2017. Neural Factorization Machines for Sparse Predictive Analytics. In Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval, Shinjuku, Tokyo, Japan, August 7-11, 2017, Noriko Kando, Tetsuya Sakai, Hideo Joho, Hang Li, Arjen P. de Vries, and Ryen W. White (Eds.). ACM, 355364. https://doi.org/10. 1145/3077136.3080777 [25] Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng Chua. 2017. Neural Collaborative Filtering. In Proceedings of the 26th International Conference on World Wide Web, WWW 2017, Perth, Australia, April 3-7, 2017, Rick Barrett, Rick Cummings, Eugene Agichtein, and Evgeniy Gabrilovich (Eds.). ACM, 173182. https://doi.org/10.1145/3038912.3052569 [26] Sebastian HofstÃ¤tter, Sophia Althammer, Michael SchrÃ¶der, Mete Sertkan, Improving Efficient Neural Ranking Models and Allan Hanbury. 2020. with Cross-Architecture Knowledge Distillation. CoRR abs/2010.02666 (2020). arXiv:2010.02666 https://arxiv.org/abs/2010.02666 [27] Sebastian HofstÃ¤tter, O. Khattab, Sophia Althammer, Mete Sertkan, and Allan Hanbury. 2022. colberter. Proceedings of the 31st ACM International Conference on Information & Knowledge Management (2022). https://api.semanticscholar. org/CorpusID:247628023 [28] Sebastian HofstÃ¤tter, Sheng-Chieh Lin, Jheng-Hong Yang, Jimmy Lin, and Allan Hanbury. 2021. Efficiently Teaching an Effective Dense Retriever with Balanced Topic Aware Sampling. In SIGIR 21: The 44th International ACM SIGIR Conference Hypencoder: Hypernetworks for Information Retrieval Conference17, July 2017, Washington, DC, USA on Research and Development in Information Retrieval, Virtual Event, Canada, July 11-15, 2021, Fernando Diaz, Chirag Shah, Torsten Suel, Pablo Castells, Rosie Jones, and Tetsuya Sakai (Eds.). ACM, 113122. https://doi.org/10.1145/3404835.3462891 [29] Kurt Hornik, Maxwell B. Stinchcombe, and Halbert White. 1989. Multilayer feedforward networks are universal approximators. Neural Networks 2, 5 (1989), 359366. https://doi.org/10.1016/0893-6080(89)90020-8 [30] Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. 2022. Unsupervised Dense Information Retrieval with Contrastive Learning. Trans. Mach. Learn. Res. 2022 (2022). https://openreview.net/forum?id=jKN1pXi7b0 [31] Ziwei Ji, Himanshu Jain, Andreas Veit, Sashank J. Reddi, Sadeep Jayasumana, Ankit Singh Rawat, Aditya Krishna Menon, Felix Yu, and Sanjiv Kumar. 2024. Efficient Document Ranking with Learnable Late Interactions. CoRR abs/2406.17968 (2024). https://doi.org/10.48550/ARXIV.2406.17968 arXiv:2406.17968 [32] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick S. H. Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense Passage Retrieval for Open-Domain Question Answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020, Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu (Eds.). Association for Computational Linguistics, 67696781. https://doi.org/10.18653/ V1/2020.EMNLP-MAIN.550 [33] Omar Khattab and Matei Zaharia. 2020. ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT. In Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval, SIGIR 2020, Virtual Event, China, July 25-30, 2020, Jimmy X. Huang, Yi Chang, Xueqi Cheng, Jaap Kamps, Vanessa Murdock, Ji-Rong Wen, and Yiqun Liu (Eds.). ACM, 3948. https://doi.org/10.1145/3397271. [34] Minghan Li, Sheng-Chieh Lin, Barlas OÄŸuz, Asish Ghoshal, Jimmy J. Lin, Yashar Mehdad, Wen tau Yih, and Xilun Chen. 2022. CITADEL: Conditional Token Interaction via Dynamic Lexical Routing for Efficient and Effective Multi-Vector Retrieval. In Annual Meeting of the Association for Computational Linguistics. https://api.semanticscholar.org/CorpusID:253708231 [35] Yury Lifshits and Shengyu Zhang. 2009. Combinatorial algorithms for nearest neighbors, near-duplicates and small-world design. In Proceedings of the Twentieth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2009, New York, NY, USA, January 4-6, 2009, Claire Mathieu (Ed.). SIAM, 318326. https://doi.org/10. 1137/1.9781611973068.36 [36] J. Lin, R. Nogueira, and A. Yates. 2022. Pretrained Transformers for Text Ranking: BERT and Beyond. Springer International Publishing. [37] Sheng-Chieh Lin, Jheng-Hong Yang, and Jimmy Lin. 2020. Distilling Dense Representations for Ranking using Tightly-Coupled Teachers. CoRR abs/2010.11386 (2020). arXiv:2010.11386 https://arxiv.org/abs/2010.11386 [38] Sheng-Chieh Lin, Akari Asai, Minghan Li, Barlas Oguz, Jimmy Lin, Yashar Mehdad, Wen-tau Yih, and Xilun Chen. 2023. How to Train Your Dragon: Diverse Augmentation Towards Generalizable Dense Retrieval. In Findings of the Association for Computational Linguistics: EMNLP 2023, Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for Computational Linguistics, Singapore, 63856400. https://doi.org/10.18653/v1/2023.findings-emnlp. [39] Zhenghao Lin, Yeyun Gong, Xiao Liu, Hang Zhang, Chen Lin, Anlei Dong, Jian Jiao, Jingwen Lu, Daxin Jiang, Rangan Majumder, and Nan Duan. 2023. PROD: Progressive Distillation for Dense Retrieval. In Proceedings of the ACM Web Conference 2023 (Austin, TX, USA) (WWW 23). Association for Computing Machinery, New York, NY, USA, 32993308. https://doi.org/10.1145/3543507. 3583421 [40] Xinyu Ma, Jiafeng Guo, Ruqing Zhang, Yixing Fan, Yingyan Li, and Xueqi Cheng. 2021. B-PROP: Bootstrapped Pre-training with Representative Words Prediction for Ad-hoc Retrieval. Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval (2021). https://api.semanticscholar.org/CorpusID:233307194 [41] Xueguang Ma, Liang Wang, Nan Yang, Furu Wei, and Jimmy Lin. 2024. FineTuning LLaMA for Multi-Stage Text Retrieval. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2024, Washington DC, USA, July 14-18, 2024, Grace Hui Yang, Hongning Wang, Sam Han, Claudia Hauff, Guido Zuccon, and Yi Zhang (Eds.). ACM, 24212425. https://doi.org/10.1145/3626772.3657951 [42] Sean MacAvaney, Franco Maria Nardini, Raffaele Perego, Nicola Tonellotto, Nazli Goharian, and Ophir Frieder. 2020. Efficient Document Re-Ranking for Transformers by Precomputing Term Representations. In Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval, SIGIR 2020, Virtual Event, China, July 25-30, 2020, Jimmy X. Huang, Yi Chang, Xueqi Cheng, Jaap Kamps, Vanessa Murdock, Ji-Rong Wen, and Yiqun Liu (Eds.). ACM, 4958. https://doi.org/10.1145/3397271.3401093 [43] Iain Mackie, Jeffrey Dalton, and Andrew Yates. 2021. How Deep is your Learning: the DL-HARD Annotated Deep Learning Dataset. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval. [44] Macedo Maia, Siegfried Handschuh, AndrÃ© Freitas, Brian Davis, Ross McDermott, Manel Zarrouk, and Alexandra Balahur. 2018. WWW18 Open Challenge: Financial Opinion Mining and Question Answering. In Companion of the The Web Conference 2018 on The Web Conference 2018, WWW 2018, Lyon , France, April 23-27, 2018, Pierre-Antoine Champin, Fabien Gandon, Mounia Lalmas, and Panagiotis G. Ipeirotis (Eds.). ACM, 19411942. https://doi.org/10.1145/3184558.3192301 [45] Yury A. Malkov and Dmitry A. Yashunin. 2020. Efficient and Robust Approximate Nearest Neighbor Search Using Hierarchical Navigable Small World Graphs. IEEE Trans. Pattern Anal. Mach. Intell. 42, 4 (2020), 824836. https://doi.org/10.1109/ TPAMI.2018. [46] Bhaskar Mitra and Nick Craswell. 2018. An Introduction to Neural Information Retrieval. Found. Trends Inf. Retr. 13, 1 (Dec. 2018), 1126. https://doi.org/10. 1561/1500000061 [47] Bhaskar Mitra and Nick Craswell. 2019. An Updated Duet Model for Passage Re-ranking. CoRR abs/1903.07666 (2019). arXiv:1903.07666 http://arxiv.org/abs/ 1903.07666 [48] Bhaskar Mitra, Fernando Diaz, and Nick Craswell. 2017. Learning to Match using Local and Distributed Representations of Text for Web Search. In Proceedings of the 26th International Conference on World Wide Web, WWW 2017, Perth, Australia, April 3-7, 2017, Rick Barrett, Rick Cummings, Eugene Agichtein, and Evgeniy Gabrilovich (Eds.). ACM, 12911299. https://doi.org/10.1145/3038912.3052579 [49] Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and Li Deng. 2016. MS MARCO: Human Generated MAchine Reading COmprehension Dataset. In Proceedings of the Workshop on Cognitive Computation: Integrating neural and symbolic approaches 2016 co-located with the 30th Annual Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain, December 9, 2016 (CEUR Workshop Proceedings, Vol. 1773), Tarek Richard Besold, Antoine Bordes, Artur S. dAvila Garcez, and Greg Wayne (Eds.). CEUR-WS.org. https://ceur-ws.org/Vol-1773/CoCoNIPS_2016_paper9.pdf [50] Rodrigo Frassetto Nogueira, Wei Yang, Kyunghyun Cho, and Jimmy Lin. 2019. Multi-Stage Document Ranking with BERT. CoRR abs/1910.14424 (2019). arXiv:1910.14424 http://arxiv.org/abs/1910.14424 [51] Liang Pang, Yanyan Lan, Jiafeng Guo, Jun Xu, Shengxian Wan, and Xueqi Cheng. 2016. Text Matching as Image Recognition. In Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, February 12-17, 2016, Phoenix, Arizona, USA, Dale Schuurmans and Michael P. Wellman (Eds.). AAAI Press, 27932799. https: //doi.org/10.1609/AAAI.V30I1.10341 [52] Prafull Prakash, Julian Killingback, and Hamed Zamani. 2021. Learning Robust Dense Retrieval Models from Incomplete Relevance Labels. In SIGIR 21: The 44th International ACM SIGIR Conference on Research and Development in Information Retrieval, Virtual Event, Canada, July 11-15, 2021, Fernando Diaz, Chirag Shah, Torsten Suel, Pablo Castells, Rosie Jones, and Tetsuya Sakai (Eds.). ACM, 1728 1732. https://doi.org/10.1145/3404835. [53] Yingqi Qu, Yuchen Ding, Jing Liu, Kai Liu, Ruiyang Ren, Wayne Xin Zhao, Daxiang Dong, Hua Wu, and Haifeng Wang. 2021. RocketQA: An Optimized Training Approach to Dense Passage Retrieval for Open-Domain Question Answering. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021, Online, June 6-11, 2021, Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-TÃ¼r, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, and Yichao Zhou (Eds.). Association for Computational Linguistics, 58355847. https://doi.org/10.18653/V1/2021.NAACL-MAIN.466 [54] Johann Radon. 1921. Mengen konvexer KÃ¶rper, die einen gemeinsamen Punkt enthalten. Math. Ann. 83, 1 (1921). https://doi.org/10.1007/BF01464231 [55] Nils Reimers and Iryna Gurevych. 2019. Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics. https://arxiv.org/abs/1908.10084 [56] Kirk Roberts, Tasmeer Alam, Steven Bedrick, Dina Demner-Fushman, Kyle Lo, Ian Soboroff, Ellen M. Voorhees, Lucy Lu Wang, and William R. Hersh. 2021. Searching for scientific evidence in pandemic: An overview of TREC-COVID. J. Biomed. Informatics 121 (2021), 103865. https://doi.org/10.1016/J.JBI.2021.103865 [57] Andrei A. Rusu, Dushyant Rao, Jakub Sygnowski, Oriol Vinyals, Razvan Pascanu, Simon Osindero, and Raia Hadsell. 2019. Meta-Learning with Latent Embedding Optimization. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net. https://openreview. net/forum?id=BJgklhAcK7 [58] Keshav Santhanam, Omar Khattab, Jon Saad-Falcon, Christopher Potts, and Matei Zaharia. 2022. ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Marine Carpuat, Marie-Catherine de Marneffe, and Ivan Vladimir Meza Ruiz (Eds.). Association for Computational Linguistics, Seattle, United States, 3715 3734. https://doi.org/10.18653/v1/2022.naacl-main. [59] Marcin Sendera, Marcin Przewiezlikowski, Konrad Karanowski, Maciej Zieba, Jacek Tabor, and Przemyslaw Spurek. 2023. HyperShot: Few-Shot Learning by Kernel HyperNetworks. In IEEE/CVF Winter Conference on Applications of Conference17, July 2017, Washington, DC, USA Julian Killingback, Hansi Zeng, and Hamed Zamani [74] Zhou Yang, Qingfeng Lan, Jiafeng Guo, Yixing Fan, Xiaofei Zhu, Yanyan Lan, Yue Wang, and Xueqi Cheng. 2018. Deep Top-K Relevance Matching Model for Adhoc Retrieval. In Information Retrieval - 24th China Conference, CCIR 2018, Guilin, China, September 27-29, 2018, Proceedings (Lecture Notes in Computer Science, Vol. 11168), Shichao Zhang, Tie-Yan Liu, Xianxian Li, Jiafeng Guo, and Chenliang Li (Eds.). Springer, 1627. https://doi.org/10.1007/978-3-030-01012-6_2 [75] Hamed Zamani, Mostafa Dehghani, W. Bruce Croft, Erik G. Learned-Miller, and Jaap Kamps. 2018. From Neural Re-Ranking to Neural Ranking: Learning Sparse Representation for Inverted Indexing. In Proceedings of the 27th ACM International Conference on Information and Knowledge Management, CIKM 2018, Torino, Italy, October 22-26, 2018, Alfredo Cuzzocrea, James Allan, Norman W. Paton, Divesh Srivastava, Rakesh Agrawal, Andrei Z. Broder, Mohammed J. Zaki, K. SelÃ§uk Candan, Alexandros Labrinidis, Assaf Schuster, and Haixun Wang (Eds.). ACM, 497506. https://doi.org/10.1145/3269206.3271800 [76] Hansi Zeng, Hamed Zamani, and Vishwa Vinay. 2022. Curriculum Learning for Dense Retrieval Distillation. In SIGIR 22: The 45th International ACM SIGIR Conference on Research and Development in Information Retrieval, Madrid, Spain, July 11 - 15, 2022, Enrique AmigÃ³, Pablo Castells, Julio Gonzalo, Ben Carterette, J. Shane Culpepper, and Gabriella Kazai (Eds.). ACM, 19791983. https://doi.org/ 10.1145/3477495.3531791 [77] Jingtao Zhan, Jiaxin Mao, Yiqun Liu, Jiafeng Guo, M. Zhang, and Shaoping Ma. 2021. Optimizing Dense Retrieval Model Training with Hard Negatives. Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval (2021). https://api.semanticscholar.org/ CorpusID: [78] Chris Zhang, Mengye Ren, and Raquel Urtasun. 2019. Graph HyperNetworks for Neural Architecture Search. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net. https://openreview.net/forum?id=rkgW0oA9FX [79] Weijie Zhao, Shulong Tan, and Ping Li. 2024. GUITAR: Gradient Pruning toward Fast Neural Ranking. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2024, Washington DC, USA, July 14-18, 2024, Grace Hui Yang, Hongning Wang, Sam Han, Claudia Hauff, Guido Zuccon, and Yi Zhang (Eds.). ACM, 163173. https://doi.org/10. 1145/3626772.3657728 Computer Vision, WACV 2023, Waikoloa, HI, USA, January 2-7, 2023. IEEE, 2468 2477. https://doi.org/10.1109/WACV56688.2023.00250 [60] Ian Soboroff, Shudong Huang, and Donna Harman. 2020. TREC 2020 News Track Overview. In Proceedings of the Twenty-Ninth Text REtrieval Conference, TREC 2020, Virtual Event [Gaithersburg, Maryland, USA], November 16-20, 2020 (NIST Special Publication, Vol. 1266), Ellen M. Voorhees and Angela Ellis (Eds.). National Institute of Standards and Technology (NIST). https://trec.nist.gov/pubs/trec29/ papers/OVERVIEW.N.pdf [61] Shulong Tan, Weijie Zhao, and Ping Li. 2021. Fast Neural Ranking on Bipartite Graph Indices. Proc. VLDB Endow. 15, 4 (2021), 794803. https://doi.org/10.14778/ 3503585.3503589 [62] Shulong Tan, Zhixin Zhou, Zhaozhuo Xu, and Ping Li. 2020. Fast Item Ranking under Neural Network based Measures. In WSDM 20: The Thirteenth ACM International Conference on Web Search and Data Mining, Houston, TX, USA, February 3-7, 2020, James Caverlee, Xia (Ben) Hu, Mounia Lalmas, and Wei Wang (Eds.). ACM, 591599. https://doi.org/10.1145/3336191. [63] Nandan Thakur, Nils Reimers, Andreas RÃ¼cklÃ©, Abhishek Srivastava, and Iryna Gurevych. 2021. BEIR: Heterogeneous Benchmark for Zero-shot Evaluation of Information Retrieval Models. In Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual, Joaquin Vanschoren and Sai-Kit Yeung (Eds.). https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/ 65b9eea6e1cc6bb9f0cd2a47751a186f-Abstract-round2.html [64] V. N. Vapnik and A. Ya. Chervonenkis. 1971. On the Uniform Convergence of Relative Frequencies of Events to Their Probabilities. Theory of Probability & Its Applications 16, 2 (1971), 264280. https://doi.org/10.1137/1116025 [65] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is All you Need. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 49, 2017, Long Beach, CA, USA, Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett (Eds.). 59986008. https://proceedings.neurips.cc/paper/2017/hash/ 3f5ee243547dee91fbd053c1c4a845aa-Abstract.html [66] Johannes von Oswald, Christian Henning, JoÃ£o Sacramento, and Benjamin F. Grewe. 2020. Continual learning with hypernetworks. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net. https://openreview.net/forum?id=SJgwNerKvB [67] Ellen M. Voorhees. 2004. Overview of the TREC 2004 Robust Track. In Proceedings of the Thirteenth Text REtrieval Conference, TREC 2004, Gaithersburg, Maryland, USA, November 16-19, 2004 (NIST Special Publication, Vol. 500-261), Ellen M. Voorhees and Lori P. Buckland (Eds.). National Institute of Standards and Technology (NIST). http://trec.nist.gov/pubs/trec13/papers/ROBUST.OVERVIEW.pdf [68] Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, and Furu Wei. 2023. SimLM: Pre-training with Representation Bottleneck for Dense Passage Retrieval. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (Eds.). Association for Computational Linguistics, Toronto, Canada, 22442258. https: //doi.org/10.18653/v1/2023.acl-long.125 [69] Orion Weller, Benjamin Chang, Sean MacAvaney, Kyle Lo, Arman Cohan, Benjamin Van Durme, Dawn J. Lawrie, and Luca Soldaini. 2024. FollowIR: Evaluating and Teaching Information Retrieval Models to Follow Instructions. CoRR abs/2403.15246 (2024). https://doi.org/10.48550/ARXIV.2403.15246 arXiv:2403. [70] Orion Weller, Benjamin Van Durme, Dawn J. Lawrie, Ashwin Paranjape, Yuhao Zhang, and Jack Hessel. 2024. Promptriever: Instruction-Trained Retrievers Can Be Prompted Like Language Models. CoRR abs/2409.11136 (2024). https: //doi.org/10.48550/ARXIV.2409.11136 arXiv:2409.11136 [71] Shitao Xiao, Zheng Liu, Yingxia Shao, and Zhao Cao. 2022. RetroMAE: PreTraining Retrieval-oriented Language Models Via Masked Auto-Encoder. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (Eds.). Association for Computational Linguistics, 538548. https://doi.org/10.18653/V1/2022.EMNLP-MAIN.35 [72] Chenyan Xiong, Zhuyun Dai, Jamie Callan, Zhiyuan Liu, and Russell Power. 2017. End-to-End Neural Ad-hoc Ranking with Kernel Pooling. In Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval, Shinjuku, Tokyo, Japan, August 7-11, 2017, Noriko Kando, Tetsuya Sakai, Hideo Joho, Hang Li, Arjen P. de Vries, and Ryen W. White (Eds.). ACM, 5564. https://doi.org/10.1145/3077136.3080809 [73] Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul N. Bennett, Junaid Ahmed, and Arnold Overwijk. 2021. Approximate Nearest Neighbor Negative Contrastive Learning for Dense Text Retrieval. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net. https://openreview.net/forum?id=zeFrfgyZln"
        }
    ],
    "affiliations": [
        "University of Massachusetts Amherst"
    ]
}