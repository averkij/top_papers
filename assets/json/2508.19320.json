{
    "paper_title": "MIDAS: Multimodal Interactive Digital-human Synthesis via Real-time Autoregressive Video Generation",
    "authors": [
        "Ming Chen",
        "Liyuan Cui",
        "Wenyuan Zhang",
        "Haoxian Zhang",
        "Yan Zhou",
        "Xiaohan Li",
        "Xiaoqiang Liu",
        "Pengfei Wan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recently, interactive digital human video generation has attracted widespread attention and achieved remarkable progress. However, building such a practical system that can interact with diverse input signals in real time remains challenging to existing methods, which often struggle with high latency, heavy computational cost, and limited controllability. In this work, we introduce an autoregressive video generation framework that enables interactive multimodal control and low-latency extrapolation in a streaming manner. With minimal modifications to a standard large language model (LLM), our framework accepts multimodal condition encodings including audio, pose, and text, and outputs spatially and semantically coherent representations to guide the denoising process of a diffusion head. To support this, we construct a large-scale dialogue dataset of approximately 20,000 hours from multiple sources, providing rich conversational scenarios for training. We further introduce a deep compression autoencoder with up to 64$\\times$ reduction ratio, which effectively alleviates the long-horizon inference burden of the autoregressive model. Extensive experiments on duplex conversation, multilingual human synthesis, and interactive world model highlight the advantages of our approach in low latency, high efficiency, and fine-grained multimodal controllability."
        },
        {
            "title": "Start",
            "content": "MIDAS: MULTIMODAL INTERACTIVE DIGITALHUMAN SYNTHESIS VIA REAL-TIME AUTOREGRESSIVE VIDEO GENERATION Ming Chen1 Yan Zhou1 Liyuan Cui1,2 Wenyuan Zhang1,3 Xiaohan Li1 Xiaoqiang Liu1 Pengfei Wan1 Haoxian Zhang 5 2 0 2 6 2 ] . [ 1 0 2 3 9 1 . 8 0 5 2 : r 1Kling Team, Kuaishou Technology 2Zhejiang University 3 Tsinghua University"
        },
        {
            "title": "ABSTRACT",
            "content": "Recently, interactive digital human video generation has attracted widespread attention and achieved remarkable progress. However, building such practical system that can interact with diverse input signals in real time remains challenging to existing methods, which often struggle with high latency, heavy computational cost, and limited controllability. In this work, we introduce an autoregressive video generation framework that enables interactive multimodal control and low-latency extrapolation in streaming manner. With minimal modifications to standard large language model (LLM), our framework accepts multimodal condition encodings including audio, pose, and text, and outputs spatially and semantically coherent representations to guide the denoising process of diffusion head. To support this, we construct large-scale dialogue dataset of approximately 20,000 hours from multiple sources, providing rich conversational scenarios for training. We further introduce deep compression autoencoder with up to 64 reduction ratio, which effectively alleviates the long-horizon inference burden of the autoregressive model. Extensive experiments on duplex conversation, multilingual human synthesis, and interactive world model highlight the advantages of our approach in low latency, high efficiency, and fine-grained multimodal controllability. Project Page: https://chenmingthu.github.io/milm/."
        },
        {
            "title": "INTRODUCTION",
            "content": "Condition-driven human video generation transforms static portraits into dynamic, interactive virtual avatars that synchronize speech with natural facial expressions, body movements, and emotional cues (Qi et al., 2025; Lin et al., 2025a; Xu et al., 2024). Such technologies enhance digital communication by making human-AI interactions more engaging and natural, and opens promising avenues for future applications such as virtual education and creative media. To this end, practical system must simultaneously satisfy three demands: accept and respond to broad spectrum of input signals, enable low-latency interaction, and preserve visual and identity consistency over long generation horizons. These requirements pose substantial challenges to existing approaches. To meet these requirements, extensive efforts have been devoted across multiple directions. GANbased approaches (Guo et al., 2024; Qi et al., 2025) produce high-fidelity local facial details but struggle with global expressiveness and smooth temporal transitions. Video diffusion transformers (Tian et al., 2024; 2025; Cui et al., 2024; Wei et al., 2025) achieve high-quality generation but rely on bidirectional attention and iterative denoising, which makes them unsuitable for real-time applications. Although various techniques have been proposed to accelerate diffusion inference (Lu et al., 2025; Zhai et al., 2024), they still face trade-offs among generality, fidelity, and speed. Moreover, most existing methods are limited to unimodal control signals such as audio or text, and require all control parameters to be specified before generation begins, which significantly limits the richness and interactivity of digital human video synthesis. Equal Contribution."
        },
        {
            "title": "Technical Report",
            "content": "Recently, autoregressive models offer promising solution to these limitations by generating tokens conditioned on historical inputs (Deng et al., 2025; Teng et al., 2025). This paradigm not only enables flexible interaction by adjusting future predictions based on past frames, but also supports arbitrary-length video synthesis while avoiding the computational overhead of bidirectional attention during inference. In parallel, recent advances in multimodal large language models (MLLMs) (Liu et al., 2023; Wu et al., 2024) show remarkable potential in bridging generation and understanding, driven by conditioning transformers on diverse multimodal inputs. These developments motivate us to unify language modeling with generative video frameworks into an end-to-end multimodal autoregressive architecture for digital human video generation. In this work, we introduce MIDAS, multimodal interactive digital human synthesis framework that supports low-latency inference, multimodal conditioning, and open-ended generation. The core of our framework is an autoregressive large language model (LLM) that predicts the evolution of video frames in latent space guided by multimodal inputs. Specifically, we design multimodal condition projector that encodes diverse input signals, including audio, pose, and text, into shared latent space. These multimodal tokens are concatenated with frame tokens to form the input sequence for the autoregressive model. The frame tokens are encoded via carefully designed Deep Compression Autoencoder (DC-AE) with 64 spatial reduction ratio, which significantly reduces the extrapolation burden of the autoregressive backbone. The autoregressive model outputs hidden states that capture both spatial and semantic coherence, which are then passed to an external diffusion head to render high-quality video frames. During training, the autoregressive model and diffusion head are jointly optimized in teacher forcing manner, while the inference proceeds via next-frame prediction, enabling real-time and streamable animation generation. To further mitigate exposure bias, we introduce controlled noise injection mechanism that corrupts context frames with Gaussian noise of varying magnitudes, thereby simulating degraded predictions and teaching the model to recover from imperfect histories during inference. We additionally construct large-scale dialogue dataset of approximately 20,000 hours, collected from both online sources and custom-recorded sessions. These dialogues are carefully segmented, annotated and post-processed, providing strong foundation for our model in general digital human dialogue scenarios. We validate the effectiveness of our method through wide experiments on duplex conversation and multi-language human synthesis. We also demonstrate its potential to incorporate richer control signals to act as general interactive world model. In summary, we make the following contributions: Multimodal Control via Instruction Tokens. We construct multimodal architecture that encodes diverse control signals (text, audio, and pose) into global instruction tokens. These tokens condition the autoregressive model for video generation, enabling coherent and expressive latent frame progression over time. Causal Latent Prediction with Diffusion Rendering. Leveraging the causal structure of autoregressive frameworks, our system predicts one frames latent tokens at time. Each predicted representation is processed by lightweight diffusion head, which recovers highquality frames with only few denoising steps. This design enables flexible-length, lowlatency video generation ideal for interactive applications. Efficient Representation with High Compression Autoencoder. We introduce an deep compression autoencoder with up to 64 reduction ratio. This compact latent representation significantly reduces the number of video tokens, which lowers the computational demands on the autoregressive model while retaining reconstruction fidelity."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Speech-driven Talking Face Generation. Animating human portraits represents challenging frontier in computer vision. Traditional methods (Zhou et al., 2020; Thies et al., 2020; Suwajanakorn et al., 2017) rely on intermediate representations such as 3D face models or 2D landmarks, which require complex preprocessing pipelines and restrict the generalization of identity. Early approaches (Guo et al., 2024; Qi et al., 2025) mainly focuses on manipulating fine-grained facial details (gaze, lip movements, muscle dynamics), but inherently compromise vividness. This creates perceptual gap between virtual and real humans despite their local consistency. Recent advances (Xu et al., 2024; Jiang et al., 2024) map audio features and facial motion variations into shared latent spaces using diffusion models. Contemporary works (Wei et al., 2025; Tian et al., 2025; Chen"
        },
        {
            "title": "Technical Report",
            "content": "et al., 2025), inspired by DiT architectures, have developed keypoint-free, text-driven end-to-end diffusion models that significantly improve expressiveness and control precision, producing lifelike facial expressions with rich emotional detail. OmniHuman-1 (Lin et al., 2025b) introduces unified framework integrating facial, body, and hand motions, demonstrating the potential of scaled diffusion models for comprehensive human animation synthesis. Other methods extend this paradigm by incorporating multiple speech inputs to enable multi-speaker dialogue in video generation (Kong et al., 2025; Huang et al., 2025). However, these methods usually freeze the control conditions across the whole sequence, thereby limiting their applicability to real-time interactive synthesis. Real-time Video Generation. Real-time generation of expressive and responsive portrait videos remains fundamental challenge. Traditional GAN-based approaches (Wang et al., 2021; Qi et al., 2025; Guo et al., 2024) employ two-stage pipelines: first extracting explicit motion representations, then rendering the final image with identity information. The emergence of diffusion models has shifted research toward implicit motion latents (Xu et al., 2024; Drobyshev et al., 2022; Liu et al., 2024), enabling more flexible control. During inference, lightweight generators combine predicted motion sequences with appearance features to synthesize animations at lower computational cost. However, they are constrained to fixed-length inputs, limiting the flexibility of content extrapolation. Most methods also rely on unimodal control signals and require all parameters to be specified in advance, which restricts expressiveness and real-time intervention. In addition, the lengthy DiT-based denoising process introduce significant computational burden. While the denoising procedure can be accelerated through distillation or skip paths (Zhai et al., 2024; Ma et al., 2024), such techniques inevitably bring trade-off between efficiency and high-fidelity synthesis. Recent innovations (Low & Wang, 2025; Yin et al., 2025; Kim et al., 2025) distill pre-trained diffusion transformers with sparse causal attention as autoregressive models for real-time performance. These advances demonstrate progression toward more efficient, high-quality, and temporally consistent video generation capabilities. To address these limitations, we propose video generation framework based on autoregressive model, which enables real-time digital human control and synthesis under multimodal conditions."
        },
        {
            "title": "3 METHOD",
            "content": "Our objective is to enable real-time synthesis of expressive and controllable portrait videos under diverse multimodal conditions. The framework takes multimodal signals (e.g., audio, pose, or text) as inputs and generates coherent video sequences, ensuring low latency, spatiotemporal consistency and flexible controllability. To support this, we first construct large-scale dialogue dataset of approximately 20,000 hours (Section 3.1). We then design highly compressive frame tokenizer and multimodal condition encoders with strong expressiveness (Section 3.2). Based on these components, we develop an autoregressive model for real-time rendering and interaction (Section 3.3). Finally, we describe the training strategy (Section 3.4) and inference strategy (Section 3.5) adopted in our framework. 3.1 DATASET REPRESENTATION The training dataset includes single-person and two-person speech content collected from three sources: (1) publicly available benchmarks (VoxCeleb1/2 (Nagraniy et al., 2017; Chung et al., 2018), TED-LRS (Afouras et al., 2018)); (2) curated online videos including podcasts, interviews, talk shows, and public speeches; and (3) custom-recorded sessions featuring controlled two-person interactions. The data processing pipeline consists of three stages: Pre-processing, Annotation and synthetic data construction, and Post-processing. The processing pipeline is illustrated in Figure 1. Pre-processing. We apply shot boundary detection and active speaker detection (ASD) to achieve temporal segmentation, followed by filtering of human subjects via face and body detection. Each segmented clip is then subjected to rigorous evaluation in terms of visual quality, audio quality, and lip synchronization accuracy. Annotation and synthetic data construction. This stage includes quality assessment, caption generation, emotion labeling, and automatic speech recognition (ASR) transcription. subset of single-person data is further adapted into conversational formats through semantic analysis and textto-speech (TTS) synthesis."
        },
        {
            "title": "Technical Report",
            "content": "Figure 1: Overview of the dataset pipeline. The process consists of three main stages: (1) Data collection and pre-processing; (2) Annotation and data construction; (3) Post-processing. Illustration of our Deep Compression Autoencoder (DC-AE). We first train the DCFigure 2: AE with spatial compression ratio of 64. In the second phase, we perform causal temporal module training. Then we apply full-model fine-tuning using an 8-frame temporal window in the third phase. Post-processing. The annotated data undergoes manual review combined with automatic sampling to ensure balanced and high-quality subsets. The final dataset contains approximately 20,000 hours of pre-training video data and over 400 hours of supervised fine-tuning (SFT) data. 3.2 MULTIMODAL REPRESENTATION 3.2.1 FRAME TOKEN REPRESENTATION For real-time AR video generation, we employ an autoencoder to transform visual inputs into compact latent representations that meet two key criteria. First, the representations must achieve satisfactory reconstruction accuracy under high spatial compression ratios, enabling efficient processing by the language model backbone. Second, to support real-time generation, the temporal dimension remains uncompressed. However, it is crucial to model temporal dependencies during both encoding and decoding, which facilitates LLM-based autoregressive generation and ensures temporally coherent, flicker-free decoding."
        },
        {
            "title": "Technical Report",
            "content": "Figure 3: Overview of our model architecture. Our streaming generation framework processes inputs in chunks, where each chunk contains condition tokens (audio, pose, text) followed by frame tokens. We apply teacher forcing during training, while performing next-frame prediction during inference. We corrupt frame tokens with Gaussian noise to mitigate exposure bias. The AR output serves as guidance for the diffusion head for denoising. Here, denotes the number of frames per chunk, and chunk boundaries (dashed lines) indicate positions where the inference process can be restarted with updated conditioning inputs. To meet these requirements, we begin by training Deep Compression Autoencoder (DC-AE) with spatial compression ratio of 64 and 128 channels that jointly addresses spatial efficiency and temporal consistency. The model learns residuals based on the space-to-channel transformed features for efficient high spatial-compression. To capture dynamics, temporal causal 3D convolution layers and RoPE-based attention layers are inserted after each spatial convolution layer. All temporal convolutions employ asymmetric replicate padding. In the second stage, we perform temporal module training, followed by full-model fine-tuning with an 8-frame temporal window in the third stage. During inference, we cache each frames temporal features (3D-conv outputs and key/value caches) and perform streaming encoding and decoding in frame-by-frame manner using 5-frame history. This streaming paradigm supports real-time, autoregressive frame-by-frame generation while preserving temporal consistency in video decoding. Although longer histories may yield better reconstructions, our 5-frame window provides practical trade-off between computational efficiency and quality. In addition, when AR video generation is performed autoregressively with sliding window of multiple frames, the framework can be extended to support non-causal temporal compression within each window, while maintaining causal streaming temporal encoding and decoding across windows. 3.2.2 CONDITION REPRESENTATION Our method accepts single portrait image and set of control conditions spanning multiple modalities (audio, pose sequences, captions) to guide the generation process toward specific motion styles or semantic goals. To unify these heterogeneous inputs, we introduce multimodal condition projector that encodes each signal into shared latent space. For simplicity, we omit the final linear projection layer that aligns all modal representations into the same dimension. Audio: RT Da faudio(A) RT NaDh, where and Da denote the sequence length and the sampling rate of the audio, and Na, Dh represent the number and dimension of encoded audio tokens, respectively. We resample each 80ms audio segment to 16kHz and encode it using the Whisper-VQ (Radford et al., 2022) module, compressing the audio segment into single audio token representation (Na = 1)."
        },
        {
            "title": "Technical Report",
            "content": "Pose: RT KpDp fpose(P ) RT NpDh, where Kp, Dp denote the number and the dimension of the keypoints, respectively. Instead of static keypoints, we use joint velocities to represent pose trajectories. These velocities are calculated as the difference in corresponding joint positions between consecutive frames. linear layer is employed to encode the trajectory of each frame into Np = 10 tokens that capture essential motion dynamics for subsequent modeling. Text: RTchL ftext(X) RTchNxDh, where Tch, denotes the number of chunk, the length of the textual sequence aligned with each chunk, respectively. We employ pretrained T5 encoder (Raffel et al., 2020) to transform text conditions into textual embeddings with token number Nx = 256."
        },
        {
            "title": "3.3.1 AUTOREGRESSIVE MODEL",
            "content": "Our approach leverages an open-source large language model (Yang et al., 2024) as the autoregressive backbone without architectural modifications, capitalizing on its inherent capabilities for sequential generation. The models pre-trained weights and transformer architecture provide strong foundation for modeling temporal dependencies in video sequences. To enable efficient streaming generation, we organize inputs and outputs into logical chunks, where each chunk contains sequence of multimodal tokens corresponding to 6 frames. Specifically, each chunk representation consists of 6 audio tokens, 60 pose tokens and 256 text tokens, concatenated sequentially from left to right. After these multimodal tokens, we append reference image tokens as well as tokens for the 6 target frames to be generated. The number of tokens per frame is determined by the spatial resolution. In our experiments, we allocate up to 60 tokens per frame, supporting image resolutions of up to 384 640. This setting can be easily scaled up to accommodate higher resolutions. Our structured token organization enables both streaming control input and sequential output generation, providing real-time responsiveness while maintaining contextual coherence across chunks, as shown in Figure 3. We also design specialized frame-level causal attention mask for our multimodal autoregressive framework, as shown in Figure 4. Within each chunk, the condition tokens are accessible to all subsequent frame tokens, while frame tokens are restricted to attending only to the condition tokens, previous frame tokens, and their intra-frame tokens with full attention. This mask design jointly enforces temporal causality and frame-level coherence, providing reliable mechanism for controllable multimodal generation. For frame prediction, we employ flow matching approach inspired by the token prediction mechanism in large language models. Our system directly predicts the representation of frame + 1 during the forward pass at time t, eliminating the need for placeholder tokens. The autoregressive generation process is formulated as Figure 4: Illustration of our causal attention mask for multimodal streaming generation. p(C, x1, x2, ..., xN ) = (cid:89) i=1 p(xiC, x1, x2, ..., xi1), (1) where represents the multimodal conditioning signals (audio, pose, text) and xi represents the tokens of the i-th video frame. This one-step prediction strategy offers three key advantages: (1) it enables straightforward teacher forcing during training, significantly improving training efficiency; (2) it allows for frame-by-frame generation without iterative refinement steps; and (3) it reduces computational overhead by avoiding multiple forward passes per frame."
        },
        {
            "title": "3.3.2 DIFFUSION HEAD",
            "content": "Our diffusion head functions as specialized renderer that transforms the autoregressive predictions into high-quality video frames. Drawing inspiration from MAR (Li et al., 2024), we inject the outputs of the autoregressive model as conditioning signals into the diffusion process, but with key distinction that we entirely eliminate mask modeling. Since the spatial relationships and semantic coherence between tokens have already been implicitly modeled by our LLM backbone, the diffusion component only needs to focus on denoising and rendering clean frames from the conditioned input. This simplified approach enhances computational efficiency while maintaining visual fidelity. The diffusion head serves as refinement mechanism rather than handling the full complexity of spatiotemporal modeling. For the architectural design of the diffusion component, we explored multiple options, including standard MLP-based architecture and more sophisticated DiT-based design (Chen et al., 2023). The latter offers stronger guarantees for structural consistency in character rendering, requiring only minor modifications to the conditioning injection mechanism."
        },
        {
            "title": "3.4 TRAINING STRATEGY",
            "content": "Our training approach addresses two key challenges: effective next-frame prediction and mitigation of exposure bias. We train the autoregressive backbone to predict frame + 1 directly from frame t, mapping sequences spanning frames 1 through to corresponding sequences from frames 2 through + 1. This forward-shifted prediction eliminates the need for placeholder tokens while maintaining temporal coherence. Autoregressive models inherently suffer from exposure bias due to the discrepancy between training on ground-truth inputs (teacher forcing) and inference on self-generated, potentially imperfect predictions. This mismatch leads to error accumulation and rapid quality degradation over long sequences. To mitigate this gap, we follow (Valevski et al., 2024) and adopt controlled noise injection strategy during training. Context frames are corrupted by add Gaussian noise of varying magnitudes to their latents, thereby simulating the imperfect predictions encountered at inference. We simultaneously provide noise level as an conditioning signals to the model, making it adapt to varying input quality levels. Specifically, we uniformly set noise levels up to maximum noise scale of 0.5, discretize them into 20 buckets, and learn dedicated embedding for each bucket. For each batch, we randomly determine noise level ID from 0 to 19 for each frame. These IDs are mapped to corresponding noise embeddings through learnable embedder and expanded to match the dimensions of the frame tokens. The noise is applied by interpolating between the original latents and Gaussian noise according to the sampled corruption level, following xnoisy = σ ϵ + (1 σ) x, (2) where σ is the noise level and ϵ (0, I) is the Gaussian noise. This approach systematically bridges the domain gap between training and inference by teaching the model to recover from corrupted context. It enables the network to correct errors from previously generated frames, which proves critical for maintaining frame quality and temporal consistency over long sequences. For multimodal training, we combine diverse datasets (audio-to-video, pose-to-video, etc.) using consistent token organization pattern: audio tokens, followed by pose tokens, text tokens, and frame tokens. For datasets missing certain conditioning signals, we employ special placeholder tokens to maintain structural consistency across heterogeneous inputs. During inference, the same token organization allows flexible combinations of different modalities as needed. The training objective follows the flow matching formulation: L(θ) = Ex0,x1,t[vθ(x(t), t) v(x0, x1)2 2], x(t) = (1 t)x0 + tx1, v(x0, x1) = x1 x0, (3) where x0 pdata(x), x1 pnoise(x) denote the clean target frame and noise distribution, respectively. This objective guides the model to learn the optimal vector field for transforming noisy representations to clean frames."
        },
        {
            "title": "Technical Report",
            "content": "Figure 5: Multi-speaker digital conversation with audio-driven avatars. Speaker 1 (top) and Speaker 2 (bottom) demonstrating turn-taking dialogue, with corresponding audio waveforms shown in blue and green. For long-duration generation, we introduce specialized post-training phase using identity-specific data at higher resolution and frame rates (12.5 fps 25fps). Unlike prior approaches that rely on vast amounts of person-specific data (Ao, 2024), our method efficiently leverages limited but focused datasets to reduce error accumulation. This targeted adaptation enables stable, high-quality video synthesis over extended periods (up to one hour), with minimal quality degradation compared to pretraining-only baselines. 3.5 INFERENCE STRATEGY During inference, we organize our generation process into temporal chunks of 480ms to enable efficient streaming. Within each chunk, the autoregressive model sequentially generates frame tokens, which are then passed to the diffusion head for denoising. After completing one chunk, the model proceeds to the next, enabling continuous video generation without the need of processing the entire sequence at once. To ensure visual consistency, we apply uniform noise level to all frames within each inference step, which can be adjusted to optimize quality. Even with minimal added noise, we observe significant improvements in temporal stability compared to standard autoregressive generation. This controlled noise design ensures temporal coherence while simplifying the denoising process. For extended inference periods, we implement frame truncation strategy inspired by FAR (Gu et al., 2025), which recognizes that historical frames beyond certain temporal distance cease to provide useful information and may instead introduce error accumulation. By limiting the context window to the most recent 8 chunks and discarding low-relevance historical frames, our system effectively reduces cumulative errors while preserving essential temporal dependencies for coherent motion synthesis."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "4.1 IMPLEMENTATION DETAILS. Our model is trained using the DeepSpeed ZeRO-2 optimization framework across 64 NVIDIA H800 GPUs for approximately 7 days. We adopt bfloat16 precision and apply gradient clipping at 1.0 to ensure numerical stability. We use AdamW as the optimizer with learning rate of 5."
        },
        {
            "title": "Technical Report",
            "content": "Figure 6: Cross-lingual singing generation with synchronized lip movements. Our model accurately renders lip synchronization across multiple languages, demonstrating the models fine-grained understanding of cross-lingual phoneme-to-viseme mapping without explicit language identification. 105 for both the main model and pretraining components, combined with cosine learning rate scheduler, 1000 warmup steps, and weight decay of 0.01. The training is performed for total of 100 epochs with per-device batch size of 2. The autoregressive backbone is based on Qwen2.53B (Yang et al., 2024), while the diffusion head follows PixArt-α architecture (Chen et al., 2023) with approximately 0.5B parameters. We adopt 4 denoising steps during both training and inference, which we find to provide an effective balance between efficiency and synthesis quality. 4.2 QUALITATIVE RESULTS. In the experiments, we present real-time digital human generation results driven by audio and the reference image. We leave pose and text conditioning as future work, which can be readily scaled up by incorporating corresponding modality encodings into our framework. Duplex conversation. Figure 5 showcases our system enabling natural turn-taking dialogue between digital avatars with synchronized audio-visual responses. Each avatar displays appropriate listening behaviors when the other is speaking, and becomes animated with synchronized lip movements and facial expressions when driven by its corresponding audio input. The audio waveforms (visualized in blue and green) clearly delineate the speaking turns. This demonstrates our models abilities to generate contextually appropriate reactions and maintain speaker identity while handling the complex dynamics of conversational interaction."
        },
        {
            "title": "Technical Report",
            "content": "Figure 7: General controllable video generation on Minecraft dataset. By incorporating directional control into our multimodal condition encoding, we realize real-time interactive world model that exhibits remarkable visual consistency and memory capabilities. Multi-language Long Video Synthesis. We present an example of cross-lingual singing synthesis in Figure 6, where we achieve precise phoneme-level synchronization across diverse language families. Our system can generate convincing lip movements for songs in Mandarin Chinese, Japanese, and English, adapting to the distinct phonetic characteristics of each language without requiring language-specific training. Notably, our model is capable of generating videos up to 4 minutes in length without significant drift. Please refer to our project page for video visualizations. General Interactive Video Generation. Our model architecture can also flexibly accomodate arbitrary modal conditions as inputs, making it seamlessly applicable to general interactive video generation tasks. By reformulating multimodal conditions into directional control signals and training on the Minecraft dataset (Yan et al., 2023), our approach effectively serves as real-time interactive world model. As shown in Figure 7, our world model achieves strong visual consistency and exhibits notable memory capabilities."
        },
        {
            "title": "5 CONCLUSION",
            "content": "In this work, we present MIDAS, multimodal interactive digital human synthesis framework for real-time video generation. Our approach builds on an LLM-based autoregressive model combined with lightweight diffusion head, enabling low-latency, streaming synthesis under diverse multimodal controls. We additionally introduce controlled noise injection strategy to mitigate the exposure bias between training and inference. To support training, we construct large-scale dialogue dataset from multiple sources, and design deep compression autoencoder to ease the long-horizon inference burden. Extensive experiments on duplex conversation, multilingual human synthesis and general interactive world model validate the effectiveness of our framework, demonstrating its advantages in responsiveness, controllability, and temporal stability. We believe that our exploration provides solid step toward scalable and interactive digital human generation and will inspire future research in this emerging area."
        },
        {
            "title": "REFERENCES",
            "content": "Triantafyllos Afouras, Joon Son Chung, and Andrew Zisserman. Lrs3-ted: large-scale dataset for visual speech recognition. arXiv preprint arXiv:1809.00496, 2018. Tenglong Ao. Body of her: preliminary study on end-to-end humanoid agent. arXiv preprint arXiv:2408.02879, 2024. Junsong Chen, YU Jincheng, GE Chongjian, Lewei Yao, Enze Xie, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-α: Fast training of diffusion transformer for photorealistic text-to-image synthesis. In The Twelfth International Conference on Learning Representations, 2023. Zhiyuan Chen, Jiajiong Cao, Zhiquan Chen, Yuming Li, and Chenguang Ma. Echomimic: Lifelike In Proceedings of the audio-driven portrait animations through editable landmark conditions. AAAI Conference on Artificial Intelligence, volume 39, pp. 24032410, 2025. Joon Son Chung, Arsha Nagrani, and Andrew Zisserman. Voxceleb2: Deep speaker recognition. In Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH, volume 2018, pp. 10861090, 2018. Jiahao Cui, Hui Li, Yun Zhan, Hanlin Shang, Kaihui Cheng, Yuqi Ma, Shan Mu, Hang Zhou, Jingdong Wang, and Siyu Zhu. Hallo3: Highly dynamic and realistic portrait image animation with video diffusion transformer. arXiv preprint arXiv:2412.00733, 2024. Haoge Deng, Ting Pan, Haiwen Diao, Zhengxiong Luo, Yufeng Cui, Huchuan Lu, Shiguang Shan, Yonggang Qi, and Xinlong Wang. Autoregressive video generation without vector quantization. In The Thirteenth International Conference on Learning Representations, 2025. Nikita Drobyshev, Jenya Chelishev, Taras Khakhulin, Aleksei Ivakhnenko, Victor Lempitsky, and Egor Zakharov. Megaportraits: One-shot megapixel neural head avatars. In Proceedings of the 30th ACM International Conference on Multimedia, pp. 26632671, 2022. Yuchao Gu, Weijia Mao, and Mike Zheng Shou. Long-context autoregressive video modeling with next-frame prediction. arXiv preprint arXiv:2503.19325, 2025. Jianzhu Guo, Dingyun Zhang, Xiaoqiang Liu, Zhizhou Zhong, Yuan Zhang, Pengfei Wan, and Di Zhang. Liveportrait: Efficient portrait animation with stitching and retargeting control. arXiv preprint arXiv:2407.03168, 2024. Yubo Huang, Weiqiang Wang, Sirui Zhao, Tong Xu, Lin Liu, and Enhong Chen. Bind-your-avatar: Multi-talking-character video generation with dynamic 3d-mask-based embedding router. arXiv preprint arXiv:2506.19833, 2025. Jianwen Jiang, Chao Liang, Jiaqi Yang, Gaojie Lin, Tianyun Zhong, and Yanbo Zheng. Loopy: arXiv preprint Taming audio-driven portrait avatar with long-term motion dependency. arXiv:2409.02634, 2024. Yeongmin Kim, Sotiris Anagnostidis, Yuming Du, Edgar Schonfeld, Jonas Kohler, Markos Georgopoulos, Albert Pumarola, Ali Thabet, and Artsiom Sanakoyeu. Autoregressive distillation of diffusion transformers. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1574515756, 2025. Zhe Kong, Feng Gao, Yong Zhang, Zhuoliang Kang, Xiaoming Wei, Xunliang Cai, Guanying Chen, and Wenhan Luo. Let them talk: Audio-driven multi-person conversational video generation. arXiv preprint arXiv:2505.22647, 2025. Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, and Kaiming He. Autoregressive image generation without vector quantization. Advances in Neural Information Processing Systems, 37: 5642456445, 2024. Gaojie Lin, Jianwen Jiang, Jiaqi Yang, Zerong Zheng, and Chao Liang. Omnihuman-1: RearXiv preprint thinking the scaling-up of one-stage conditioned human animation models. arXiv:2502.01061, 2025a."
        },
        {
            "title": "Technical Report",
            "content": "Gaojie Lin, Jianwen Jiang, Jiaqi Yang, Zerong Zheng, and Chao Liang. Omnihuman-1: RearXiv preprint thinking the scaling-up of one-stage conditioned human animation models. arXiv:2502.01061, 2025b. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023. Tao Liu, Feilong Chen, Shuai Fan, Chenpeng Du, Qi Chen, Xie Chen, and Kai Yu. Anitalker: animate vivid and diverse talking faces through identity-decoupled facial motion encoding. In Proceedings of the 32nd ACM International Conference on Multimedia, pp. 66966705, 2024. Chetwin Low and Weimin Wang. Talkingmachines: Real-time audio-driven facetime-style video via autoregressive diffusion models. arXiv preprint arXiv:2506.03099, 2025. Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver++: Fast solver for guided sampling of diffusion probabilistic models. Machine Intelligence Research, pp. 122, 2025. Xinyin Ma, Gongfan Fang, and Xinchao Wang. Deepcache: Accelerating diffusion models for free. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1576215772, 2024. Arsha Nagraniy, Joon Son Chungy, and Andrew Zisserman. Voxceleb: large-scale speaker identification dataset. In Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH, volume 2017, pp. 26162620, 2017. Jinwei Qi, Chaonan Ji, Sheng Xu, Peng Zhang, Bang Zhang, and Liefeng Bo. Chatanyone: Stylized real-time portrait video generation with hierarchical motion diffusion model. arXiv preprint arXiv:2503.21144, 2025. Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. Robust speech recognition via large-scale weak supervision, 2022. URL https://arxiv. org/abs/2212.04356. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21(140):167, 2020. Supasorn Suwajanakorn, Steven Seitz, and Ira Kemelmacher-Shlizerman. Synthesizing obama: learning lip sync from audio. In ACM Transactions on Graphics (TOG), volume 36, pp. 113, 2017. Hansi Teng, Hongyu Jia, Lei Sun, Lingzhi Li, Maolin Li, Mingqiu Tang, Shuai Han, Tianning Zhang, WQ Zhang, Weifeng Luo, et al. Magi-1: Autoregressive video generation at scale. arXiv preprint arXiv:2505.13211, 2025. Justus Thies, Mohamed Elgharib, Ayush Tewari, Christian Theobalt, and Matthias Nießner. Neural voice puppetry: Audio-driven facial reenactment. European Conference on Computer Vision, pp. 716731, 2020. Linrui Tian, Qi Wang, Bang Zhang, and Liefeng Bo. Emo: Emote portrait alive generating expressive portrait videos with audio2video diffusion model under weak conditions. In European Conference on Computer Vision, pp. 244260. Springer, 2024. Linrui Tian, Siqi Hu, Qi Wang, Bang Zhang, and Liefeng Bo. Emo2: End-effector guided audiodriven avatar video generation. arXiv preprint arXiv:2501.10687, 2025. Dani Valevski, Yaniv Leviathan, Moab Arar, and Shlomi Fruchter. Diffusion models are real-time game engines. arXiv preprint arXiv:2408.14837, 2024. Ting-Chun Wang, Arun Mallya, and Ming-Yu Liu. One-shot free-view neural talking-head synthesis In Proceedings of the IEEE/CVF conference on computer vision and for video conferencing. pattern recognition, pp. 1003910049, 2021."
        },
        {
            "title": "Technical Report",
            "content": "Cong Wei, Bo Sun, Haoyu Ma, Ji Hou, Felix Juefei-Xu, Zecheng He, Xiaoliang Dai, Luxin Zhang, Kunpeng Li, Tingbo Hou, et al. Mocha: Towards movie-grade talking character synthesis. arXiv preprint arXiv:2503.23307, 2025. Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-Seng Chua. Next-gpt: Any-to-any multimodal llm. In International Conference on Machine Learning, pp. 5336653397. PMLR, 2024. Sicheng Xu, Guojun Chen, Yu-Xiao Guo, Jiaolong Yang, Chong Li, Zhenyu Zang, Yizhong Zhang, Xin Tong, and Baining Guo. Vasa-1: Lifelike audio-driven talking faces generated in real time. Advances in Neural Information Processing Systems, 37:660684, 2024. Wilson Yan, Danijar Hafner, Stephen James, and Pieter Abbeel. Temporally consistent transformers for video generation. In International Conference on Machine Learning, pp. 3906239098. PMLR, 2023. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2024. Tianwei Yin, Qiang Zhang, Richard Zhang, William Freeman, Fredo Durand, Eli Shechtman, and Xun Huang. From slow bidirectional to fast autoregressive video diffusion models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 2296322974, 2025. Yuanhao Zhai, Kevin Lin, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Chung-Ching Lin, David Doermann, Junsong Yuan, and Lijuan Wang. Motion consistency model: Accelerating video diffusion with disentangled motion-appearance distillation. Advances in Neural Information Processing Systems, 37:111000111021, 2024. Yang Zhou, Xintong Han, Eli Shechtman, Jose Echevarria, Evangelos Kalogerakis, and Dingzeyu Li. Makeittalk: Speaker-aware talking-head animation. In ACM Transactions on Graphics (TOG), volume 39, pp. 115, 2020."
        }
    ],
    "affiliations": [
        "Kuaishou Technology",
        "Tsinghua University",
        "Zhejiang University"
    ]
}