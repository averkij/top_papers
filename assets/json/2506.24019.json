{
    "paper_title": "Ella: Embodied Social Agents with Lifelong Memory",
    "authors": [
        "Hongxin Zhang",
        "Zheyuan Zhang",
        "Zeyuan Wang",
        "Zunzhe Zhang",
        "Lixing Fang",
        "Qinhong Zhou",
        "Chuang Gan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce Ella, an embodied social agent capable of lifelong learning within a community in a 3D open world, where agents accumulate experiences and acquire knowledge through everyday visual observations and social interactions. At the core of Ella's capabilities is a structured, long-term multimodal memory system that stores, updates, and retrieves information effectively. It consists of a name-centric semantic memory for organizing acquired knowledge and a spatiotemporal episodic memory for capturing multimodal experiences. By integrating this lifelong memory system with foundation models, Ella retrieves relevant information for decision-making, plans daily activities, builds social relationships, and evolves autonomously while coexisting with other intelligent beings in the open world. We conduct capability-oriented evaluations in a dynamic 3D open world where 15 agents engage in social activities for days and are assessed with a suite of unseen controlled evaluations. Experimental results show that Ella can influence, lead, and cooperate with other agents well to achieve goals, showcasing its ability to learn effectively through observation and social interaction. Our findings highlight the transformative potential of combining structured memory systems with foundation models for advancing embodied intelligence. More videos can be found at https://umass-embodied-agi.github.io/Ella/."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 3 ] . [ 1 9 1 0 4 2 . 6 0 5 2 : r Ella: Embodied Social Agents with Lifelong Memory Hongxin Zhang1, Zheyuan Zhang2, Zeyuan Wang3, Zunzhe Zhang3, Qinhong Zhou1, Lixing Fang1, Chuang Gan1 1 University of Massachusetts Amherst 2 Johns Hopkins University 3 Tsinghua University"
        },
        {
            "title": "Abstract",
            "content": "We introduce Ella, an embodied social agent capable of lifelong learning within community in 3D open world, where agents accumulate experiences and acquire knowledge through everyday visual observations and social interactions. At the core of Ellas capabilities is structured long-term multimodal memory system that stores, updates, and retrieves information effectively. It consists of namecentric semantic memory for organizing acquired knowledge and spatiotemporal episodic memory for capturing multimodal experiences. By integrating this lifelong memory system with foundation models, Ella retrieves relevant information for decision-making, plans daily activities, builds social relationships, and evolves autonomously while coexisting with other intelligent beings in the open world. We conduct capability-oriented evaluations in dynamic 3D open world where 15 agents engage in social activities for days and are assessed with suite of unseen controlled evaluations. Experimental results show that Ella can influence, lead, and cooperate with other agents well to achieve goals, showcasing its ability to learn effectively through observation and social interaction. Our findings highlight the transformative potential of combining structured memory systems with foundation models for advancing embodied intelligence. More videos can be found at https: //umass-embodied-agi.github.io/Ella/."
        },
        {
            "title": "Introduction",
            "content": "Its long-standing goal to create intelligent beings capable of survival in the human community [22, 46, 71], which requires lifelong learning in an open and social world. The embodied agents must accumulate experiences, including visual observations and social interactions with other intelligent beings, such as conversations; and acquire knowledge from these multi-modal experiences, build new concepts of objects, agents, and events, and identify the connections among these concepts. With the rapid advancement of Foundation Models [66, 76, 26], surge of powerful agents has emerged [85]. These range from agents operating solely in the text domain [27, 83] to multimodal agents capable of controlling screens [31], playing games [92, 93], and even functioning as robots in the physical world [1, 34, 18]. Despite these advancements, one crucial component remains underexplored in current agent research: long-term memory. Humans organize accumulated experiences in Episodic Memory [90, 91, 65] and acquired knowledge in Semantic Memory[52], enabling them to make long-term plans and exhibit higher-level cognitive capabilities [43, 88]. In contrast, current work in embodied agents is limited to constrained spatial regions (primarily indoor spaces) and brief temporal scales (seconds for robotic manipulation or minutes for navigation tasks). For agents to thrive in an ever-evolving world, it is essential to develop long-term memory system that supports learning new concepts and forming new relationships. In this direction, Generative Agents [68] introduced textual temporal episodic memory, assuming oracle perception in sandbox 2D environment. Similarly, Voyager [92] designed single agent with long-term procedural memory, denotes equal contribution. Preprint. Under review. (a) (b) Figure 1: (a) Embodied agents require lifelong learning to accumulate experiences and acquire knowledge through everyday visual observation and social interaction within community in 3D open world. (b) Ella self-evolves by growing episodic and semantic memory over time. enabling it to acquire new skills in Minecraft through oracle perception and self-training. However, the challenge of constructing effective lifelong memory systems for embodied agents in an open and social worldwhere they must learn from visual observations and engage in social interactions with other intelligent beings, as illustrated in Figure 1aremains largely unexplored. In this work, we propose to build robust long-term multi-modal memory system that can store, update, and retrieve information effectively. Borrowing the concepts from psychology and cognitive neuroscience [90], we construct the long-term memory in two forms: name-centric semantic memory with hierarchical scene graph and knowledge graph to organize acquired knowledge, and spatiotemporal episodic memory to capture the agents multi-modal experiences. We present Ella, an embodied lifelong learning agent that can accumulate experiences and acquire knowledge effectively through visual perception and social interaction with other agents within community in an open 3D world, by integrating this structured memory with foundation models. To plan robustly and behave consistently through days of social life, Ella adopts planning-reaction framework where it first retrieves related context from memory to make structured daily schedule, then updates the memory with new visual observations and social interactions, and make reactions to the new context, which could be revising the schedule, interacting with the environment, or engaging in social interactions. We simulate Ella and other baseline agents in Virtual Community [105], an open world simulation platform for multi-agent embodied AI, featuring large-scale community scenarios derived from the real world with realistic physics and renderings. Unlike traditional task-oriented evaluations for agents, assessing high-level cognitive capabilities in lifelong setting is more critical [15]. To this end, we first simulate 15 agents for 9 hours (with decision frequency of 1 second), representing their first day in the community. During this phase, agents must plan their day based on their unique characteristics and acclimate to the environment and other agents. Then we test the agents with unseen controlled evaluations: Influence Battle and Leadership Quest, where the agents work in groups to persuade others to attend their party at specific location despite conflicting schedules or lead their group to prepare for an activity under resource constraints. Experimental results across three communities show that Ella demonstrates advanced cognitive abilities including social reasoning and leadership, showcasing its ability to learn effectively through visual observation and social interaction. In sum, our contribution includes: We propose structured long-term memory with name-centric semantic memory and spatiotemporal episodic memory to support lifelong learning in an open and social world. We introduce Ella, the first embodied social agent that can self-evolve through visual observation and social interaction by integrating structured memory with foundation models. We conduct capability-oriented experiments in dynamic 3D open world with 15 agents for days, and demonstrate Ellas advanced cognitive abilities including social reasoning and leadership. 2 Figure 2: An example community of 15 agents and 4 social groups in New York. The character and observation of agent Elizabeth Mensah are shown on the right."
        },
        {
            "title": "2.1 Embodied Social Intelligence",
            "content": "Social intelligence has been widely studied in embodied multi-agent environments [105, 58, 11, 2, 7, 39, 70, 89, 72, 95, 87, 100, 48], while one branch focuses on simplified symbolic or gamelike environments [78, 84, 38, 6, 64, 81, 99], often ignoring the challenges present in an open world, including perception and diverse personalities of agents. Specifically, generative agents [68] developed unified temporal language memory, demonstrating the robust simulation of human-like agents within symbolic community. Following this line of research, series of works have explored socially intelligent agents within text-based sandbox environments [47, 106, 53, 13, 55, 56, 16]. The other branch, including works on human-robot interaction [23, 24, 9, 17, 63, 77, 57, 62, 44], focuses on real-world domains but is limited to specific task settings. Different from above, we explore embodied social intelligence within community in an open 3D world, featuring expansive spatial regions and temporal scale spanning multiple days. 2.2 Agent Memory Memory has been studied for long time in AI, especially related to cognitive architectures [96, 52, 85]. However, most modern agent architecture primarily assumes temporal memory due to the constraints of specific domains or the limited time horizon for which the agent is designed. visual memory as type of semantic memory has been implemented using various structures in computer vision, including voxels [12, 8, 61, 74], scene graphs [49, 75, 41, 25], Octrees [32, 101, 4, 103], or implicit continuous representations [80, 20, 33, 21]. Recently, several works have explored agent memory for longer time horizons. [41, 97] and [104] introduce updating mechanisms for scene graph-based memory, adapting it to long-term tasks. [92] and [50] develop procedural memory tailored for specific game environments to support long-term planning. [40] proposes long-term memory with graph-based structure to enable self-evolution in LLM tasks. [94] further integrates long-term and short-term memory to address long-horizon tasks within household environments. Another line of work studies how to better retrieve knowledge from external data sources to help Large Language Models answer questions [10, 28, 29, 67, 30, 82, 98]. However, none of the above has studied how to build long-term memory system that could learn from both visual observations of the environment and social interactions with other agents, which we tackled with dual-form structured memory and foundation models."
        },
        {
            "title": "3 Problem Setting",
            "content": "In our setting, agents with unique visual appearance vi and character profile ci inhabit an open, socially interactive world , forming social groups, as illustrated in Figure 2. Each character is defined by basic attributes such as name, age, occupation, values [79], hobbies, lifestyle, and current goals within the community. These attributes guide the agents daily decision-making. Social groups consist of subset of agents selected based on character compatibility, and are defined by group 3 name, detailed textual description, and designated physical location for group activities. These groups connect the agents into cohesive community, allowing rich and complex social interactions grounded in the 3D environment. Each agent is initialized with partial knowledge about the world, including known places and familiar agents, such as their residence and fellow group members, based on their character. The simulation runs at fine temporal resolution of one second per step, during which each agent receives an observation oi including posed RGB and depth images, as well as dialogue content from nearby agents. Communication is spatial-constrained: agents can only engage in conversation if they are within threshold distance θs, mimicking realistic spatial constraints on verbal interactions. Every second, agents execute an action ai which may involve interacting with the environment or other agents. During controlled evaluations, intervention occurs solely through modifications to agents community goals. Agents are required to make optimal decisions ai based on their updated character profiles ci and incoming observations oi."
        },
        {
            "title": "4 Ella: Embodied Lifelong Learning Agent",
            "content": "To enable the embodied agents to continually learn within community in 3D open world, robust and efficient long-term memory is the key. Borrowing the concepts from psychology and cognitive neuroscience [90], we build long-term memory in two forms: name-centric semantic memory (Section 4.1) and spatiotemporal episodic memory (Section 4.2). Then in Section 4.3, we introduce how we leverage the foundation models to integrate this memory system to facilitate the agents everyday planning and social interactions. 4.1 Name-centric Semantic Memory Semantic memory stores facts about the agent and world, which is continually updated while the agent interacts with the world and other agents. Different from language agents, which normally take external databases like Wikipedia as form of knowledge to help reasoning [85, 45, 10], embodied agents need knowledge grounded in the environment they inhabit. We organize the different types of knowledge in name-centric way and connect the related ones into graph as shown in Figure 3 (a). Specifically, we build hierarchical scene graph on the fly to serve as spatial memory to help the agent navigate the visual world. The semantic memory is updated whenever there is new visual observation made or conversation finished, as introduced in Section 4.3.3. 4.1.1 Hierarchical Scene Graph as Spatial Memory Maintaining spatial memory of the surrounding world is vital for embodied agents to act in 3D world. To serve this purpose, we incrementally build hierarchical scene graph [36, 25] on the fly as shown in Figure 3 (a). Volume Grid Layer Given posed RGB and depth observation, we first project them to 3D space and represent them in volume grid representations to act as low-level geometric memory. We then obtain an occupancy map based on it to facilitate navigation while avoiding obstacles in the 3D world. We divided the entire map into blocks of 0.5m 0.5m and subdivided each block into smaller cells of 0.1m 0.1m. We identified the lowest position within each small cell that could accommodate person. cell was classified as containing an obstacle if the height difference between this position and any of its neighboring cells exceeded 0.5m. Object Layer Taking inspiration from previous works [25, 59], we employ multi-stage perception pipeline to process RGB observations in an open world. Specifically, we utilize combination of openset vision modelsincluding tagging [35], object detection [54], and segmentation [76]to form the perception module. This module extracts sequence of semantically labeled masks mi, tagi as object candidates. Using depth and pose observations, each mask mi is projected into 3D point cloud pi, enabling the computation of geometric similarity sim(pi, pj) between objects based on their spatial overlap. Additionally, we extract visual features vi for each object by encoding the corresponding cropped image [73]. The detected object candidates from the current frame are then merged with existing objects based on similarity measurements. Unlike [25], we handle the additional complexity of dynamic objects such as agents and vehicles. Due to the relatively low perception rate (1 FPS), conventional tracking techniques are impractical. Instead, we rely on visual similarity to associate and merge dynamic objects across frames. Region Layer We also implemented region layer to further classify the buildings. First, we used the occupancy map and breadth-first search to compute the Generalized Voronoi Diagram (GVD)[36] of 4 Figure 3: Method Overview. We build long-term memory in two forms: (a) name-centric semantic memory organizes the knowledge in name-centric graph including hierarchical scene graph serving as the spatial memory; (b) spatiotemporal episodic memory stores the experience as series of events consisting of time, location, and multimodal contents. (c) Ella first generates daily schedule according to the knowledge and experiences retrieved from the long-term memory, (d) then updates the memory based on visual observations of the environment, and (e) social interactions with other agents and (f) makes reactions accordingly including (f1) revising the schedule, (f2) interacting with the environment, (f3) and engaging in conversation. 1 the map. For each point in the GVD, we determined the set = arg min{dist(p, b)b B}, where represents the set of all buildings. We then connected all buildings in with edges weighted by dist(p,s)2 , where S. Finally, we connected all previously unconnected buildings by adding edges with zero weight, resulting in complete graph. To group nodes connected by higher-weight edges, we applied spectral clustering, partitioning the graph into (cid:112)B regions. This clustering facilitated more structured geometric partitioning of the buildings. 4.2 Spatiotemporal Episodic Memory Episodic memory is responsible for storing personal experiences [90, 91, 52]. As noted by [60], episodic memory encodes not only when and what events occurred but also where they took placehighlighting the crucial role of spatial information. Unlike [68], our episodic memory module incorporates both temporal and spatial information, in addition to multi-modal content, enabling the agent to retrieve experiences relevant to its current location. Experiences are stored as series of events, each composed of temporal attributes (event creation time and last access time), spatial attributes (event location and place), and content attributes (a textual description and corresponding egocentric image), as illustrated in Figure 3(b). Retrieval The episodic memory supports spatiotemporal retrieval. Given querycomprising time, location, and contentall stored experience items are ranked based on the following three criteria: Spatial Proximity measures the distance between the event location pe and the query location pq. proximity(e, q) = 1 pepq+ϵ Content Relevance measures how well an events content aligns with the given query by evaluating both textual and visual similarity. Specifically, we compute the cosine similarity between the encoded representations of the event and query, considering both their text descriptions and images I. The final relevance score is obtained by averaging these two similarities. Relevance(e, q) = (cos(Te, Tq) + cos(Ie, Iq))/2 5 Temporal Recency is higher for events recently accessed. Following [68], we model recency using an exponential decay function based on the time elapsed since the memory was last accessed. Recency(e) = exp (te tq) All three scores are then normalized to the range of [0, 1] with min-max scaling and averaged as the final score, and the top events are retrieved. 4.3 Planning, Reaction, and Communication With this structured long-term memory, Ella leverages foundation models to make efficient and robust everyday planning. Following [68], we adopt planning and reaction framework with several modifications to facilitate efficient daily planning. Ella first generates an environmentand charactersgrounded daily schedule according to the knowledge and experiences retrieved from the long-term memory, then updates the memory based on observations and makes reactions accordingly, including revising the schedule, engaging in conversation, and interacting with the environment. specific communication module is incorporated to generate the utterance to chat about, summarize the conversations, and extract knowledge from it. More details on the submodules are provided in Appendix C. All prompt templates are provided in Appendix D. 4.3.1 Daily Schedule At the start of each day, Ella will retrieve experience and knowledge from the long-term memory with query of Things to consider for my schedule today., then use foundation models to generate the daily schedule. Different from [68], we generate the daily schedule in structured manner directly with each activity represented with start time, an ending time, an activity description, and the corresponding activity place. Specifically, we consider the required commute time between adjacent activities happening in different places explicitly, due to the actual cost of navigating in an expansive 3D environment. For example, commuting from the office to party place may take more than 15 minutes on foot, without considering that the agent may miss the party if they planned to attend the party at the party starting time. Figure 3 (c) shows generated daily schedule for agent Elisabeth Mansah. The daily schedule may be revised later by the reaction module given new experience and knowledge obtained from observations and social interactions during the day. 4.3.2 Reaction Upon receiving new observations, the agent first processes visual information and updates its semantic memory using the perception module introduced in Section 4.1.1. If new objects are detected or messages are heard, the agent invokes the reaction module. This module begins by retrieving relevant memories using the query Important things to react to., then use foundation models to reason about the character, current time, place, schedule, and retrieved memory and make one of the four choices: revising the schedule, interacting with the environment, engaging in conversation, or no reactions needed, as illustrated in Figure 3 (f). Additionally, the reaction module is automatically triggered if the time elapsed since the last reaction exceeds θreact seconds. 4.3.3 Communication When the agent generates reaction of engaging in conversation, the communication module is revoked to generate the utterance to chat about by first retrieving the related knowledge and experience from the long-term memory with query of the latest sentence in the conversation or Things to chat about with conversation targets if the agent is initiating new conversation, then use foundation models to synthesize the appropriate utterance. When the conversation finishes, the communication module will summarize it and store the summarized conversation in episodic memory. Ella will also try to extract new knowledge it learned from the conversation by prompting foundation model with some demonstration knowledge items, and use it to update the semantic memory."
        },
        {
            "title": "5 Experiments",
            "content": "5.1 Experimental Setup We instantiate our embodied social agents community in textit Virtual Community [105], an open world simulation platform for multi-agent embodied AI, featuring large-scale community scenarios derived from the real world with realistic physics simulation and rendering based on Genesis [5]. We conducted experiments with 15 agents of unique characters in 3 different scenes and communities. 6 Table 1: Main results. We report the show-up rate and the total number of conversations for Influence Battle, and the completion rate and the total number of conversations for Leadership Quest. + Oracle Perception assumes ground truth 2D segmentation. The best results are in bold. Ella achieves higher show-up rate and completion rate across all three communities. Influence Battle Leadership Quest New York London Detroit Average New York London Detroit Average CoELA [100] Generative Agents [68] + Oracle Perception Ella (Ours) + Oracle Perception 46.7, 57 40.0, 3 46.7, 5 46.7, 12 60.0, 20.0, 27 40.0, 0 53.3, 153 66.7, 19 60.0, 28 6.7, 17 20.0, 0 26.7, 0 46.7, 15 53.3, 17 24.5, 33.7 33.3, 1.0 42.2, 52.7 53.4, 15.3 57.8, 18.7 0.0, 72 8.3, 169 4.2, 649 33.3, 15 39.6, 87 0.0, 957 0.0, 55 0.0, 5 26.7, 17 35.0, 35 11.5, 625 16.7, 14 16.7, 2 37.5, 14 25.0, 3.8, 551.3 8.3, 79.3 7.0, 218.7 32.5, 15.3 33.2, 49.3 The observation includes posed 512 512 RGB and depth images, the content of the heard messages within range, and current states including pose, place, time, cash, held objects, and vehicles being taken. The agents action space consists of navigation actions of move forward m, turn left degree, turn right degree, enter place or vehicle, and exit vehicle; interaction actions of pick object with hand y, drop object in hand x; and converse message with range of m. The message transmission range threshold θmsg is set to 10m. There are two stages of the experiment. In the first stage, 15 agents are simulated for 9 hours (34200 steps) for their first day in the community, during which the agents could familiarize themselves with the 600m * 600m scene and other agents and build memories. Then in the second stage, we test them with two controlled evaluations in the days following: Influence Battle and Leadership Quest. In Influence Battle, two of the four groups will be asked to organize party at specific place in 6 hours, and the members need to go around the city, find and invite agents outside of their group to attend the party. This evaluation tests the agents capability to impact other agents by persuading them to attend the parties, which requires the capability of social reasoning, persuasion, and decision-making. In Leadership Quest, each of the four groups is assigned task to purchase several items from various stores in the city and return within 3 hours. One member from each group is designated as the leader and is the only one given full details of the task, while the remaining members are simply instructed to assist the leader. This controlled evaluation setting challenges the agents leadership abilities, particularly in assigning sub-tasks based on the diverse personalities and resources of group members. Metrics We evaluate agents capability to influence others with show up rate, the total number of agents showing up at any party organizing place during the 30-minute party time divided by the total number of agents; and the total number of conversations the organizing parties engaged in, reflecting the efficiency of the invitations. In Leadership Quest, we measure the success of agents leadership and cooperation by average completion rate, the number of fulfilled target items divided by the number of all target items averaged across all groups; and the total number of conversations reflecting the efficacy of the communications among the agents. Baselines To the best of our knowledge, there hasnt been any embodied social agent framework supporting social interaction within community with open-world 3D scenes. The most related methods are CoELA [100], which only considered two agents within constrained indoor scene for specific task, and Generative Agents [68], which assume oracle perception and use predefined communication mechanism. We re-implemented these two methods in our setting as the baselines. CoELA [100] is cooperative embodied agent framework. We replace their perception module with ours since there isnt pretrained 2D segmentation model available under our open-world setting. We provide the character description to replace the CoELAs task-specific description. Generative Agent [68] is believable simulacrum of human behavior with an unimodality long-term memory. We adopt the perception module of Ella to convert visual observations into text descriptions and use the same occupancy map and a* algorithm for visual navigation. Implementation Details For the perception module, we use open-set tagging model RAM++ [35], object detection model GroundingDINO [54], and segmentation model SAM2 [76]. For the embedding models, we use CLIP [73] model ViT-B-32-256 from openclip [37] for images and text-embedding-3-small from Azure for text. We use gpt-4o2 as the foundation model backbone for our method and CoELA, and gpt-35-turbo3 for Generative Agent4. We also test our method with 2model version 2024-11-20 3model version 0125 4We tried to implement Generative Agent with gpt-4o, but the original prompts broke often and its too costly given its large quantity of API call 7 (a) (b) Figure 4: (a) Social interaction pattern in Influence Battle. The thickness of line reflects the frequency of interaction. Members from Creative Minds Collective successfully persuaded Elizabeth Mensah to join their groups party. (b) Comparison of memory growth over time. The number of memory nodes averaged over 15 agents is shown here. Our structured memory system allows for more stable and organized growth. Table 2: Results with open-source foundation model backbone. We report the results of Ella w/ Oracle Perception with different backbones averaged over three communities here. Influence Battle Leadership Quest show-up rate # conversation completion rate # conversation gpt-4o-1120 DeepSeek-R1-Distill-Qwen-14B Qwen2.5-14B-Instruct 57.8 40.0 22.2 18.7 48.3 89.0 33.2 8.0 1. 49.3 46.0 57.3 open source foundation models DeepSeek-R1-Distill-Qwen-14B and Qwen2.5-14B-Instruct served with vLLM [42] as the backbone in the experiments with oracle perception. 5.2 Results Ellas structured long-term memory is efficient. As shown in Figure 1b, Ella continuously accumulates new experiences and acquires new knowledge on the first day, covering nearly 50% of the environment. An example of the final spatial coverage in the Detroit community is illustrated in Figure 7 in the Appendix. Figure 4b further shows that Ellas structured memory system allows for more stable and organized growth of memory nodes compared to the Generative Agents baseline. This structure enables more efficient retrieval as memory scales, supporting timely access to relevant events even as the memory grows. Ella can influence other agents effectively. As shown in Table 1, Ella achieves higher showup rate in the Influence Battle by successfully inviting more agents to the party across all three communities. This demonstrates its strong capabilities in social reasoning and persuasion. Although the CoELA baseline engaged in twice as many conversations as Ella, its show-up rate was only half as high. This discrepancy arises from its lack of long-term memory, preventing it from effectively recalling the party details after several hours (thousands of simulation steps). Meanwhile, Generative Agents engaged in so few conversations that they failed to invite other agents, despite being explicitly instructed to do so in their daily requirements. As illustrated in Figure 4a, the party news propagates over time through the efforts of the organizer agents. Ella can lead the group well. As shown in Table 1, Ella completes four times more goals than other baselines in the Leadership Quest. Notably, CoELA had completion rate of zero across all scenesexcept in Detroit, where the leader partially completed the task alonedespite engaging in numerous conversations. This failure stems from its inability to retain memory of the required items. Among all scenarios, the London community posed the greatest challenge, where only Ella achieved non-zero performance, demonstrating the robustness of our approach. 8 Robust perception is important for embodied social agents. Different from [68]s setting where two agents knowing each other could only engage in conversation when situated in the same grid, or [100]s setting where two already-known agents could converse with each other anytime anywhere, our setting requires the agent to identify the agent to talk to according to their visual appearance or conversation contents and calculate the transmission range of their message according to the 3D location of the target agents to converse with, therefore robust perception is critical for the agents to engage in social interactions in 3D world. Comparing the results of Ella and w/ Oracle Perception in Table 1, we can see the performance further boots with Oracle perception, and there tend to be more conversations among the agents since theyre more confident in identifying each other. Open source foundation models backbone is promising. With the strong open-source foundation models like DeepSeek-R1 [26] becoming available recently, we wonder how well our framework works out-of-the-box on open-source foundation model backbones. We test Ella w/ Oracle Perception Agent with different backbones across all three communities and the two controlled evaluations, the results are shown in Table 2. Using DeepSeek-R1-Distill-Qwen-14B as the backbone without any further prompt engineering, Ella w/ Oracle Perception achieves reasonable performance close to that of using backbone of gpt-4o, while Qwen2.5-14B-Instruct performs much worse."
        },
        {
            "title": "6 Limitations",
            "content": "Leverage the graph structure of the name-centric semantic memory. Although the namecentric semantic memory is maintained as graph structure, the current implementation retrieves knowledge based solely on text and image feature similarity. Enhancing our memory system with more sophisticated graph-based retrieval methods [102, 86, 28] could enable effective multi-hop reasoning, paving the way for addressing reasoning-intensive challenges. This represents promising direction for future work. Lifelong simulation of community of agents in visually rich, physics-realistic environment is computationally expensive. Although our experiments span only 1.5 simulated daysseemingly short for \"lifelong\" settingwe adopt the widely used interpretation of lifelong learning as an agents ability to accumulate, retain, and reuse knowledge across experiences [14]. Despite extensive system-level optimizations to accelerate simulation, each simulated second still requires at least one second of real time. This is due to the intensive demands of multi-camera rendering, skinned motion computation, and the invocation of multiple models or APIs during each agents decisionmaking process. As result, simulating one day in the environment consumes an entire real-world day, significantly constraining the scale of experimentation. Continued progress in graphics and simulation technologies is expected to ease this bottleneck and support faster development of embodied social agents in high-fidelity, physics-grounded environments. All agents thinking processes are assumed to finish synchronously. Human cognition is bounded by limited computational resources [51]. In our current setting, Agents are assumed to think synchronously with unlimited computational resources, which means whatever deliberate the agents thinking process is, it costs only 1 second in their world. Its interesting to consider the time cost of thinking given the same limited computational resources to all agents explicitly and study how agents could switch between slow system-2 thinking and fast system-1 thinking [19] adaptively."
        },
        {
            "title": "7 Conclusion",
            "content": "In this work, we build structured long-term memory with name-centric semantic memory and spatiotemporal episodic memory and introduce Ella, an embodied social agent that uses foundation models and retrieved memory to reason, make daily plans, and engage in social activities. We conducted capability-oriented experiments in the Virtual Community with 15 agents in 3 different communities and demonstrated Ella can use long-term memory effectively to influence, cooperate, and lead other agents in an open world while accumulating multi-modal experience and acquiring knowledge continuously from visual observations of the environment and social interactions with other agents. Our findings imply the power of combining structured long-term memory and foundation models to advance embodied general intelligence that could co-exist with humans."
        },
        {
            "title": "References",
            "content": "[1] M. Ahn, A. Brohan, N. Brown, Y. Chebotar, O. Cortes, B. David, C. Finn, K. Gopalakrishnan, K. Hausman, A. Herzog, et al. Do as can, not as say: Grounding language in robotic affordances. arXiv preprint arXiv:2204.01691, 2022. [2] C. Amato, G. Konidaris, L. P. Kaelbling, and J. P. How. Modeling and planning with macroactions in decentralized pomdps. Journal of Artificial Intelligence Research, 64:817859, 2019. [3] S. Amershi, D. Weld, M. Vorvoreanu, A. Fourney, B. Nushi, P. Collisson, J. Suh, S. Iqbal, P. N. Bennett, K. Inkpen, et al. Guidelines for human-ai interaction. In Proceedings of the 2019 chi conference on human factors in computing systems, pages 113, 2019. [4] A. Asgharivaskasi and N. Atanasov. Semantic octree mapping and shannon mutual information computation for robot exploration. IEEE Transactions on Robotics, 39(3):19101928, 2023. [5] G. Authors. Genesis: universal and generative physics engine for robotics and beyond, December 2024. [6] B. Baker, I. Kanitscheider, T. Markov, Y. Wu, G. Powell, B. McGrew, and I. Mordatch. Emergent tool use from multi-agent autocurricula. In International Conference on Learning Representations, 2020. [7] N. Bard, J. N. Foerster, S. Chandar, N. Burch, M. Lanctot, H. F. Song, E. Parisotto, V. Dumoulin, S. Moitra, E. Hughes, et al. The hanabi challenge: new frontier for ai research. Artificial Intelligence, 280:103216, 2020. [8] V. Blukis, C. Paxton, D. Fox, A. Garg, and Y. Artzi. persistent spatial semantic representation for high-level natural language instruction execution. In Conference on Robot Learning, pages 706717. PMLR, 2022. [9] A. Bobu, A. Peng, P. Agrawal, J. Shah, and A. D. Dragan. Aligning robot and human representations. arXiv preprint arXiv:2302.01928, 2023. [10] S. Borgeaud, A. Mensch, J. Hoffmann, T. Cai, E. Rutherford, K. Millican, G. B. Van Den Driessche, J.-B. Lespiau, B. Damoc, A. Clark, et al. Improving language models by retrieving from trillions of tokens. In International conference on machine learning, pages 22062240. PMLR, 2022. [11] M. Carroll, R. Shah, M. K. Ho, T. Griffiths, S. Seshia, P. Abbeel, and A. Dragan. On the utility of learning about humans for human-ai coordination. Advances in neural information processing systems, 32, 2019. [12] D. S. Chaplot, D. P. Gandhi, A. Gupta, and R. R. Salakhutdinov. Object goal navigation using goal-oriented semantic exploration. Advances in Neural Information Processing Systems, 33:42474258, 2020. [13] H. Chen, H. Chen, M. Yan, W. Xu, G. Xing, W. Shen, X. Quan, C. Li, J. Zhang, and F. Huang. Socialbench: Sociality evaluation of role-playing conversational agents. In Findings of the Association for Computational Linguistics ACL 2024, pages 21082126, 2024. [14] Z. Chen and B. Liu. Lifelong machine learning. Morgan & Claypool Publishers, 2018. [15] M. Crosby, B. Beyret, and M. Halina. The animal-ai olympics. Nature Machine Intelligence, 1(5):257257, 2019. [16] G. Dai, W. Zhang, J. Li, S. Yang, S. Rao, A. Caetano, M. Sra, et al. Artificial leviathan: Exploring social evolution of llm agents through the lens of hobbesian social contract theory. arXiv preprint arXiv:2406.14373, 2024. [17] K. Dautenhahn. Socially intelligent robots: dimensions of humanrobot interaction. Philosophical transactions of the royal society B: Biological sciences, 362(1480):679704, 2007. 10 [18] Y. Du, M. Yang, P. Florence, F. Xia, A. Wahid, B. Ichter, P. Sermanet, T. Yu, P. Abbeel, J. B. Tenenbaum, et al. Video language planning. arXiv preprint arXiv:2310.10625, 2023. [19] J. S. B. Evans. In two minds: dual-process accounts of reasoning. Trends in cognitive sciences, 7(10):454459, 2003. [20] S. Y. Gadre, K. Ehsani, S. Song, and R. Mottaghi. Continuous scene representations for embodied ai. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1484914859, 2022. [21] S. Y. Gadre, M. Wortsman, G. Ilharco, L. Schmidt, and S. Song. Cows on pasture: Baselines In Proceedings of the and benchmarks for language-driven zero-shot object navigation. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2317123181, 2023. [22] C. Gan, J. Schwartz, S. Alter, D. Mrowca, M. Schrimpf, J. Traer, J. D. Freitas, J. Kubilius, A. Bhandwaldar, N. Haber, M. Sano, K. Kim, E. Wang, M. Lingelbach, A. Curtis, K. T. Feigelis, D. Bear, D. Gutfreund, D. D. Cox, A. Torralba, J. J. DiCarlo, J. B. Tenenbaum, J. Mcdermott, and D. L. Yamins. ThreeDWorld: platform for interactive multi-modal physical simulation. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1), 2021. [23] M. C. Gombolay, R. A. Gutierrez, S. G. Clarke, G. F. Sturla, and J. A. Shah. Decisionmaking authority, team efficiency and human worker satisfaction in mixed humanrobot teams. Autonomous Robots, 39:293312, 2015. [24] M. A. Goodrich, A. C. Schultz, et al. Humanrobot interaction: survey. Foundations and Trends in HumanComputer Interaction, 1(3):203275, 2008. [25] Q. Gu, A. Kuwajerwala, S. Morin, K. M. Jatavallabhula, B. Sen, A. Agarwal, C. Rivera, W. Paul, K. Ellis, R. Chellappa, et al. Conceptgraphs: Open-vocabulary 3d scene graphs for perception and planning. In 2024 IEEE International Conference on Robotics and Automation (ICRA), pages 50215028. IEEE, 2024. [26] D. Guo, D. Yang, H. Zhang, J. Song, R. Zhang, R. Xu, Q. Zhu, S. Ma, P. Wang, X. Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [27] I. Gur, H. Furuta, A. Huang, M. Safdari, Y. Matsuo, D. Eck, and A. Faust. real-world webagent with planning, long context understanding, and program synthesis. arXiv preprint arXiv:2307.12856, 2023. [28] B. J. Gutiérrez, Y. Shu, Y. Gu, M. Yasunaga, and Y. Su. Hipporag: Neurobiologically inspired long-term memory for large language models. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. [29] B. J. Gutiérrez, Y. Shu, W. Qi, S. Zhou, and Y. Su. From rag to memory: Non-parametric continual learning for large language models, 2025. [30] H. Han, Y. Wang, H. Shomer, K. Guo, J. Ding, Y. Lei, M. Halappanavar, R. A. Rossi, S. Mukherjee, X. Tang, et al. Retrieval-augmented generation with graphs (graphrag). arXiv preprint arXiv:2501.00309, 2024. [31] W. Hong, W. Wang, Q. Lv, J. Xu, W. Yu, J. Ji, Y. Wang, Z. Wang, Y. Dong, M. Ding, et al. Cogagent: visual language model for gui agents. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1428114290, 2024. [32] A. Hornung, K. M. Wurm, M. Bennewitz, C. Stachniss, and W. Burgard. Octomap: An efficient probabilistic 3d mapping framework based on octrees. Autonomous robots, 34:189206, 2013. [33] C. Huang, O. Mees, A. Zeng, and W. Burgard. Visual language maps for robot navigation. In 2023 IEEE International Conference on Robotics and Automation (ICRA), pages 1060810615. IEEE, 2023. [34] W. Huang, C. Wang, R. Zhang, Y. Li, J. Wu, and L. Fei-Fei. Voxposer: Composable 3d value maps for robotic manipulation with language models. In Conference on Robot Learning, pages 540562. PMLR, 2023. [35] X. Huang, Y.-J. Huang, Y. Zhang, W. Tian, R. Feng, Y. Zhang, Y. Xie, Y. Li, and L. Zhang. Open-set image tagging with multi-grained text supervision. arXiv e-prints, pages arXiv2310, 2023. [36] N. Hughes, Y. Chang, and L. Carlone. Hydra: real-time spatial perception system for 3d scene graph construction and optimization. arXiv preprint arXiv:2201.13360, 2022. [37] G. Ilharco, M. Wortsman, R. Wightman, C. Gordon, N. Carlini, R. Taori, A. Dave, V. Shankar, H. Namkoong, J. Miller, H. Hajishirzi, A. Farhadi, and L. Schmidt. Openclip, July 2021. If you use this software, please cite it as below. [38] M. Jaderberg, W. M. Czarnecki, I. Dunning, L. Marris, G. Lever, A. G. Castaneda, C. Beattie, N. C. Rabinowitz, A. S. Morcos, A. Ruderman, et al. Human-level performance in 3d multiplayer games with population-based reinforcement learning. Science, 364(6443):859 865, 2019. [39] U. Jain, L. Weihs, E. Kolve, A. Farhadi, S. Lazebnik, A. Kembhavi, and A. Schwing. cordial sync: Going beyond marginal policies for multi-agent embodied tasks. In Computer Vision ECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part 16, pages 471490. Springer, 2020. [40] X. Jiang, F. Li, H. Zhao, J. Wang, J. Shao, S. Xu, S. Zhang, W. Chen, X. Tang, Y. Chen, et al. Long term memory: The foundation of ai self-evolution. arXiv preprint arXiv:2410.15665, 2024. [41] A. Kurenkov, M. Lingelbach, T. Agarwal, E. Jin, C. Li, R. Zhang, L. Fei-Fei, J. Wu, S. Savarese, In and R. Martın-Martın. Modeling dynamic environments with scene graph memory. International Conference on Machine Learning, pages 1797617993. PMLR, 2023. [42] W. Kwon, Z. Li, S. Zhuang, Y. Sheng, L. Zheng, C. H. Yu, J. E. Gonzalez, H. Zhang, and I. Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. [43] J. E. Laird. Introduction to soar. arXiv preprint arXiv:2205.03854, 2022. [44] P. A. Lasota, T. Fong, J. A. Shah, et al. survey of methods for safe human-robot interaction. Foundations and Trends in Robotics, 5(4):261349, 2017. [45] P. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal, H. Küttler, M. Lewis, W.-t. Yih, T. Rocktäschel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems, 33:94599474, 2020. [46] C. Li, R. Zhang, J. Wong, C. Gokmen, S. Srivastava, R. Martín-Martín, C. Wang, G. Levine, M. Lingelbach, J. Sun, et al. Behavior-1k: benchmark for embodied ai with 1,000 everyday activities and realistic simulation. In Conference on Robot Learning, pages 8093. PMLR, 2023. [47] G. Li, H. Hammoud, H. Itani, D. Khizbullin, and B. Ghanem. Camel: Communicative agents for\" mind\" exploration of large language model society. Advances in Neural Information Processing Systems, 36:5199152008, 2023. [48] S. Li, Y. Wu, X. Cui, H. Dong, F. Fang, and S. Russell. Robust multi-agent reinforcement learning via minimax deep deterministic policy gradient. In Proceedings of the AAAI conference on artificial intelligence, volume 33, pages 42134220, 2019. [49] X. Li, D. Guo, H. Liu, and F. Sun. Embodied semantic scene graph generation. In Conference on robot learning, pages 15851594. PMLR, 2022. [50] Z. Li, Y. Xie, R. Shao, G. Chen, D. Jiang, and L. Nie. Optimus-1: Hybrid multimodal memory empowered agents excel in long-horizon tasks. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. 12 [51] F. Lieder and T. L. Griffiths. Resource-rational analysis: Understanding human cognition as the optimal use of limited computational resources. Behavioral and brain sciences, 43:e1, 2020. [52] P. Lindes and J. E. Laird. Toward integrating cognitive linguistics and cognitive language processing. In Proceedings of the 14th International Conference on Cognitive Modeling (ICCM), 2016. [53] R. Liu, R. Yang, C. Jia, G. Zhang, D. Yang, and S. Vosoughi. Training socially aligned language models on simulated social interactions. In The Twelfth International Conference on Learning Representations, 2024. [54] S. Liu, Z. Zeng, T. Ren, F. Li, H. Zhang, J. Yang, Q. Jiang, C. Li, J. Yang, H. Su, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. arXiv preprint arXiv:2303.05499, 2023. [55] X. Liu, J. Zhang, S. Guo, H. Shang, C. Yang, and Q. Zhu. Exploring prosocial irrationality for llm agents: social cognition view. arXiv preprint arXiv:2405.14744, 2024. [56] Z. Liu, A. Anand, P. Zhou, J.-t. Huang, and J. Zhao. Interintent: Investigating social intelligence of llms via intention understanding in an interactive game context. arXiv preprint arXiv:2406.12203, 2024. [57] D. P. Losey, H. J. Jeon, M. Li, K. Srinivasan, A. Mandlekar, A. Garg, J. Bohg, and D. Sadigh. Learning latent actions to control assistive robots. Autonomous robots, 46(1):115147, 2022. [58] R. Lowe, A. Tamar, J. Harb, O. Pieter Abbeel, and I. Mordatch. Multi-agent actor-critic for mixed cooperative-competitive environments. Advances in neural information processing systems, 30, 2017. [59] D. Maggio, Y. Chang, N. Hughes, M. Trang, D. Griffith, C. Dougherty, E. Cristofalo, L. Schmid, and L. Carlone. Clio: Real-time task-driven open-set 3d scene graphs. arXiv preprint arXiv:2404.13696, 2024. [60] M. Mastrogiuseppe, N. Bertelsen, M. F. Bedeschi, and S. A. Lee. The spatiotemporal organization of episodic memory and its disruption in neurodevelopmental disorder. Scientific reports, 9(1):18447, 2019. [61] S. Y. Min, D. S. Chaplot, P. K. Ravikumar, Y. Bisk, and R. Salakhutdinov. Film: Following instructions in language with modular methods. In International Conference on Learning Representations, 2022. [62] M. Natarajan and M. Gombolay. Effects of anthropomorphism and accountability on trust in human robot interaction. In Proceedings of the 2020 ACM/IEEE international conference on human-robot interaction, pages 3342, 2020. [63] S. Nikolaidis, R. Ramakrishnan, K. Gu, and J. Shah. Efficient model learning from jointaction demonstrations for human-robot collaborative tasks. In Proceedings of the tenth annual ACM/IEEE international conference on human-robot interaction, pages 189196, 2015. [64] Y. Niu, R. R. Paleja, and M. C. Gombolay. Multi-agent graph-attention communication and teaming. In AAMAS, volume 21, page 20th, 2021. [65] A. M. Nuxoll and J. E. Laird. Extending cognitive architecture with episodic memory. In AAAI, pages 15601564, 2007. [66] OpenAI. Gpt-4 technical report, 2023. [67] C. Packer, V. Fang, S. Patil, K. Lin, S. Wooders, and J. Gonzalez. Memgpt: Towards llms as operating systems. arXiv preprint, 2023. [68] J. S. Park, J. C. OBrien, C. J. Cai, M. R. Morris, P. Liang, and M. S. Bernstein. Generative agents: Interactive simulacra of human behavior. arXiv preprint arXiv:2304.03442, 2023. 13 [69] G. Pavlakos, V. Choutas, N. Ghorbani, T. Bolkart, A. A. A. Osman, D. Tzionas, and M. J. Black. Expressive body capture: 3D hands, face, and body from single image. In Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), pages 1097510985, 2019. [70] X. Puig, T. Shu, S. Li, Z. Wang, Y.-H. Liao, J. B. Tenenbaum, S. Fidler, and A. Torralba. Watchand-help: challenge for social perception and human-ai collaboration. In International Conference on Learning Representations, 2021. [71] X. Puig, E. Undersander, A. Szot, M. D. Cote, T.-Y. Yang, R. Partsey, R. Desai, A. Clegg, M. Hlavac, S. Y. Min, et al. Habitat 3.0: co-habitat for humans, avatars, and robots. In The Twelfth International Conference on Learning Representations, 2024. [72] X. Puig, E. Undersander, A. Szot, M. D. Cote, T.-Y. Yang, R. Partsey, R. Desai, A. W. Clegg, M. Hlavac, S. Y. Min, et al. Habitat 3.0: co-habitat for humans, avatars and robots. arXiv preprint arXiv:2310.13724, 2023. [73] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. [74] S. K. Ramakrishnan, D. S. Chaplot, Z. Al-Halah, J. Malik, and K. Grauman. Poni: Potential In Proceedings of the functions for objectgoal navigation with interaction-free learning. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1889018900, 2022. [75] K. Rana, J. Haviland, S. Garg, J. Abou-Chakra, I. D. Reid, and N. Suenderhauf. Sayplan: Grounding large language models using 3d scene graphs for scalable task planning. CoRR, 2023. [76] N. Ravi, V. Gabeur, Y.-T. Hu, R. Hu, C. Ryali, T. Ma, H. Khedr, R. Rädle, C. Rolland, L. Gustafson, E. Mintun, J. Pan, K. V. Alwala, N. Carion, C.-Y. Wu, R. Girshick, P. Dollár, and C. Feichtenhofer. Sam 2: Segment anything in images and videos, 2024. [77] L. Rozo, S. Calinon, D. G. Caldwell, P. Jimenez, and C. Torras. Learning physical collaborative robot behaviors from human demonstrations. IEEE Transactions on Robotics, 32(3):513527, 2016. [78] M. Samvelyan, T. Rashid, C. Schroeder de Witt, G. Farquhar, N. Nardelli, T. G. Rudner, C.-M. Hung, P. H. Torr, J. Foerster, and S. Whiteson. The starcraft multi-agent challenge. In Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems, pages 21862188, 2019. [79] S. H. Schwartz. An overview of the schwartz theory of basic values. Online readings in Psychology and Culture, 2(1):11, 2012. [80] N. M. M. Shafiullah, C. Paxton, L. Pinto, S. Chintala, and A. Szlam. Clip-fields: Weakly supervised semantic fields for robotic memory. arXiv preprint arXiv:2210.05663, 2022. [81] G. Sharon, R. Stern, A. Felner, and N. R. Sturtevant. Conflict-based search for optimal multi-agent pathfinding. Artificial intelligence, 219:4066, 2015. [82] W. Shi, S. Min, M. Yasunaga, M. Seo, R. James, M. Lewis, L. Zettlemoyer, and W.-t. Yih. Replug: Retrieval-augmented black-box language models. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 83648377, 2024. [83] N. Shinn, F. Cassano, A. Gopinath, K. Narasimhan, and S. Yao. Reflexion: Language agents with verbal reinforcement learning. Advances in Neural Information Processing Systems, 36, 2024. [84] J. Suarez, Y. Du, P. Isola, and I. Mordatch. Neural mmo: massively multiagent game environment for training and evaluating intelligent agents. arXiv preprint arXiv:1903.00784, 2019. 14 [85] T. Sumers, S. Yao, K. Narasimhan, and T. L. Griffiths. Cognitive architectures for language agents. arXiv preprint arXiv:2309.02427, 2023. [86] J. Sun, C. Xu, L. Tang, S. Wang, C. Lin, Y. Gong, H.-Y. Shum, and J. Guo. Think-on-graph: Deep and responsible reasoning of large language model with knowledge graph. arXiv preprint arXiv:2307.07697, 2023. [87] A. Szot, U. Jain, D. Batra, Z. Kira, R. Desai, and A. Rai. Adaptive coordination in social embodied rearrangement. In International Conference on Machine Learning, pages 33365 33380. PMLR, 2023. [88] J. B. Tenenbaum, C. Kemp, T. L. Griffiths, and N. D. Goodman. How to grow mind: Statistics, structure, and abstraction. science, 331(6022):12791285, 2011. [89] N. Tsoi, M. Hussein, J. Espinoza, X. Ruiz, and M. Vázquez. Sean: Social environment for autonomous navigation. In Proceedings of the 8th international conference on human-agent interaction, pages 281283, 2020. [90] E. Tulving. Episodic and semantic memory. Organization of memory/Academic Press, 1972. [91] E. Tulving. Elements of episodic memory, 1983. [92] G. Wang, Y. Xie, Y. Jiang, A. Mandlekar, C. Xiao, Y. Zhu, L. Fan, and A. Anandkumar. Voyager: An open-ended embodied agent with large language models. arXiv preprint arXiv:2305.16291, 2023. [93] Z. Wang, S. Cai, G. Chen, A. Liu, X. Ma, and Y. Liang. Describe, explain, plan and select: interactive planning with llms enables open-world multi-task agents. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. [94] Z. Wang, B. Yu, J. Zhao, W. Sun, S. Hou, S. Liang, X. Hu, Y. Han, and Y. Gan. Karma: Augmenting embodied ai agents with long-and-short term memory systems. arXiv preprint arXiv:2409.14908, 2024. [95] M. Wen, J. Kuba, R. Lin, W. Zhang, Y. Wen, J. Wang, and Y. Yang. Multi-agent reinforcement learning is sequence modeling problem. Advances in Neural Information Processing Systems, 35:1650916521, 2022. [96] J. Weston, S. Chopra, and A. Bordes. Memory networks. arXiv preprint arXiv:1410.3916, 2014. [97] Y. Yang, H. Yang, J. Zhou, P. Chen, H. Zhang, Y. Du, and C. Gan. Snapmem: Snapshot-based 3d scene memory for embodied exploration and reasoning. arXiv preprint arXiv:2411.17735, 2024. [98] M. Yasunaga, A. Aghajanyan, W. Shi, R. James, J. Leskovec, P. Liang, M. Lewis, L. Zettlemoyer, and W.-T. Yih. Retrieval-augmented multimodal language modeling. In International Conference on Machine Learning, pages 3975539769. PMLR, 2023. [99] X. Yu, J. Fu, R. Deng, and W. Han. Mineland: Simulating large-scale multi-agent interactions with limited multimodal senses and physical needs. arXiv preprint arXiv:2403.19267, 2024. [100] H. Zhang, W. Du, J. Shan, Q. Zhou, Y. Du, J. B. Tenenbaum, T. Shu, and C. Gan. Building cooperative embodied agents modularly with large language models, 2023. [101] L. Zhang, L. Wei, P. Shen, W. Wei, G. Zhu, and J. Song. Semantic slam based on object detection and improved octomap. IEEE Access, 6:7554575559, 2018. [102] Q. Zhang, S. Chen, Y. Bei, Z. Yuan, H. Zhou, Z. Hong, J. Dong, H. Chen, Y. Chang, and X. Huang. survey of graph retrieval-augmented generation for customized large language models, 2025. [103] K. Zheng, A. Paul, and S. Tellex. Asystem for generalized 3d multi-object search. In 2023 IEEE International Conference on Robotics and Automation (ICRA), pages 16381644. IEEE, 2023. 15 [104] F. Zhou, H. Liu, H. Zhao, and L. Liang. Long-term object search using incremental scene graph updating. Robotica, 41(3):962975, 2023. [105] Q. Zhou, H. Zhang, X. Lin, Z. Zhang, Y. Chen, W. Liu, Z. Zhang, S. Chen, L. Fang, Q. Lyu, X. Sun, J. Yang, Z. Wang, B. C. Dang, Z. Chen, D. Ladia, J. Liu, and C. Gan. Virtual community: An open world for humans, robots, and society. 2025. [106] X. Zhou, H. Zhu, L. Mathur, R. Zhang, H. Yu, Z. Qi, L.-P. Morency, Y. Bisk, D. Fried, G. Neubig, et al. Sotopia: Interactive evaluation for social intelligence in language agents. In The Twelfth International Conference on Learning Representations, 2024."
        },
        {
            "title": "A Broader Impact",
            "content": "As embodied social agents become more advanced, their integration into human-centered environments raises critical ethical and societal considerations. Its important to design and follow best practices in human-AI interactions [3]. One key concern is the impact of AI-driven persuasion on human and agent interactions. In our Influence Battle evaluation, Ella successfully convinces other agents to attend an event, demonstrating its ability to shape group behavior. While such social reasoning capabilities are essential for cooperative AI, they could be misused in real-world applications, leading to manipulation, misinformation, or undue influence. To mitigate this, AI-driven persuasive agents must be designed with transparent intent disclosure and value alignment, ensuring they do not engage in deceptive or coercive behaviors. Another concern is that their decision-making processes may inadvertently reflect and reinforce societal biases present in their training data or interaction patterns. For example, in our Leadership Quest, Ella demonstrated superior leadership capabilities, but the fairness of leadership selection criteria in AI-driven systems remains an open question. Ensuring diversity and fairness in AI leadership roles requires robust bias mitigation strategies, careful dataset curation, and continuous evaluation of AI decision-making in diverse social contexts."
        },
        {
            "title": "B Additional Experiment Details",
            "content": "B.1 Virtual Community Virtual Community (ViCo), introduced by [105], is an open world simulation platform for multi-agent embodied AI, featuring large-scale community scenarios derived from the real world with realistic physics and renderings. It was developed using Genesis [5] as its core engine, generative physics simulator capable of modeling wide variety of materials and an extensive array of robotic tasks, all while maintaining full differentiability. Additionally, Genesis features real-time renderer based on OpenGL and path-tracing renderer powered by Luisa. ViCo primarily offers scalable 3D scene creation and the generation of an embodied agent community. B.1.1 Scenes ViCo develops an online pipeline to transform existing 3D geospatial data into high-quality simulationready scenes. Moreover, the pipeline automatically annotates the scenes from these geospatial data to facilitate real-world alignment. It supports the creation of expansive outdoor and indoor environments at any location and scale. Currently, ViCo has generated 57 scenes of various cities worldwide. In this paper, we use subset of 3 scenes from the generated scenes for our evaluation: New York City, Detroit, and London. Figure 6 presents views of different scenes within Virtual Community. B.1.2 Agents ViCo has 74 avatar skins, consisting of ordinary skins from the Mixamo 5 and celebrity skins generated from real-world images using Avatar SDK 6. We randomly sampled 15 skins for each of the 3 scenes. ViCo combines SMPL-X human skeletons [69] with created avatar skins to support up to 2,299 unique motions from Mixamo. Additionally, ViCo can generate scene-grounded characters that are socially connected at community level. Figure 5 illustrates generated community in New York City with places of different functionalities annotated. B.2 Compute We conducted our experiments using single NVIDIA A100 GPU. Stage one of each community life simulation was run for 20 hours, while stage two of each task and community was executed for an additional 10 hours. On average, each agents saved memoryincluding both episodic and semantic componentsoccupies approximately 161 MB after 9 hours of simulation. During runtime, agents consume additional memory for perception, planning, and retrieval. In particular, the perception 5https://www.mixamo.com/ 6https://avatarsdk.com 17 module alone requires around 4 GB of GPU memory per agent. The peak RAM usage per agent process is approximately 1 GB. Figure 5: An illustration of community in New York City with places of different functionalities annotated. There are 6 types of functional places: accommodation, entertainment, food, office, stores, and transit, each labeled with different colors on the figure. Social group information is also annotated with the group name, the group meeting place, and the group description. Figure 6: Close-up views of different scenes in Virtual Community. 18 Figure 7: visualization of the final spatial coverage on the Detroit community. Explored regions are shown in red, buildings are shown in white, and unexplored regions are shown in black. The buildings in the agents schedule are denoted with green circles."
        },
        {
            "title": "C Additional Implementation Details",
            "content": "C.1 Navigation Given the volume grid maintained in the semantic memory introduced in Section 4.1.1, we construct the occupancy map and partition the entire map into three types of grid points: unknown, known obstacles, and known non-obstacles, as illustrated in Figure 7. The A* algorithm is employed to search for the shortest path, where the weight of known non-obstacle points is set to 1, unknown points are assigned weight of 5, and obstacle points are given an infinite weight. Additionally, to mitigate the issue of agents getting stuck near obstacles due to potential wall-clipping, points closer to obstacles are assigned higher weights. Specifically, point at distance from an obstacle is assigned an additional weight of 100 . Finally, to prevent the agent from wandering in place due to significant discrepancies between consecutive navigation paths, the previously computed path is prioritized unless it is found to be infeasible (i.e., it crosses an obstacle)."
        },
        {
            "title": "D Prompt Templates",
            "content": "We provide the full prompt template for the modules introduced in Section 4.3 in Figure 8 - Figure 12. 19 Figure 8: Prompt template for generating the daily schedule. $Character$ is replaced with the agents character description, $Context$ is replaced with the retrieved memory. 20 Figure 9: Prompt template for generating the reaction. $Character$ is replaced with the agents character description, $Schedule$ is replaced with todays remaining schedules, $Experience$ is replaced with the retrieved memory, $Context$ is replaced with the latest memory. Figure 10: Prompt template for generating the utterance. $Character$ is replaced with the agents character description, $Target_knowledge$, $Target_experience$, $Context$ are replaced with the retrieved memory, $Conversation_history$ is replaced with the last 4 messages. template Figure $Conversation_history$ is replaced with the full conversation. generating Prompt the 11: for summarization of the conversation. 22 Figure 12: Prompt template for extracting knowledge from conversation. $Conversation_history$ is replaced with the full conversation, $Knowledge_items$ is replaced with sampled knowledge items from semantic memory."
        }
    ],
    "affiliations": [
        "Johns Hopkins University",
        "Tsinghua University",
        "University of Massachusetts Amherst"
    ]
}