{
    "paper_title": "EfficientLLM: Efficiency in Large Language Models",
    "authors": [
        "Zhengqing Yuan",
        "Weixiang Sun",
        "Yixin Liu",
        "Huichi Zhou",
        "Rong Zhou",
        "Yiyang Li",
        "Zheyuan Zhang",
        "Wei Song",
        "Yue Huang",
        "Haolong Jia",
        "Keerthiram Murugesan",
        "Yu Wang",
        "Lifang He",
        "Jianfeng Gao",
        "Lichao Sun",
        "Yanfang Ye"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) have driven significant progress, yet their growing parameter counts and context windows incur prohibitive compute, energy, and monetary costs. We introduce EfficientLLM, a novel benchmark and the first comprehensive empirical study evaluating efficiency techniques for LLMs at scale. Conducted on a production-class cluster (48xGH200, 8xH200 GPUs), our study systematically explores three key axes: (1) architecture pretraining (efficient attention variants: MQA, GQA, MLA, NSA; sparse Mixture-of-Experts (MoE)), (2) fine-tuning (parameter-efficient methods: LoRA, RSLoRA, DoRA), and (3) inference (quantization methods: int4, float16). We define six fine-grained metrics (Memory Utilization, Compute Utilization, Latency, Throughput, Energy Consumption, Compression Rate) to capture hardware saturation, latency-throughput balance, and carbon cost. Evaluating over 100 model-technique pairs (0.5B-72B parameters), we derive three core insights: (i) Efficiency involves quantifiable trade-offs: no single method is universally optimal; e.g., MoE reduces FLOPs and improves accuracy but increases VRAM by 40%, while int4 quantization cuts memory/energy by up to 3.9x at a 3-5% accuracy drop. (ii) Optima are task- and scale-dependent: MQA offers optimal memory-latency trade-offs for constrained devices, MLA achieves lowest perplexity for quality-critical tasks, and RSLoRA surpasses LoRA efficiency only beyond 14B parameters. (iii) Techniques generalize across modalities: we extend evaluations to Large Vision Models (Stable Diffusion 3.5, Wan 2.1) and Vision-Language Models (Qwen2.5-VL), confirming effective transferability. By open-sourcing datasets, evaluation pipelines, and leaderboards, EfficientLLM provides essential guidance for researchers and engineers navigating the efficiency-performance landscape of next-generation foundation models."
        },
        {
            "title": "Start",
            "content": "EFFICIENTLLM: EFFICIENCY IN LARGE LANGUAGE MODELS EVALUATION ON ARCHITECTURE PRETRAINING, FINE-TUNING, AND BIT-WIDTH QUANTIZATION 5 2 0 2 0 2 ] . [ 1 0 4 8 3 1 . 5 0 5 2 : r Zhengqing Yuan1 Weixiang Sun1 Yixin Liu2 Huichi Zhou3 Rong Zhou2 Yiyang Li1 Zheyuan Zhang1 Wei Song1 Yue Huang1 Haolong Jia4 Keerthiram Murugesan Yu Wang6 Lifang He2 Jianfeng Gao7 Lichao Sun2 Yanfang Ye1 1University of Notre Dame 2Lehigh University 3Imperial College London 4Rutgers University 5International Business Machines Corporation (IBM) 6University of Illinois Chicago 7Microsoft Research https://dlyuangod.github.io/EfficientLLM/ (cid:18) https://huggingface.co/Tyrannosaurus/EfficientLLM"
        },
        {
            "title": "ABSTRACT",
            "content": "Large Language Models (LLMs) have catalyzed dramatic progress, yet their ballooning parameter counts (e.g., Deepseek R1 671B) and context windows pose prohibitive compute (3640 Petaflop/s-days for GPT-3 training), energy, and monetary footprints (> $4.6M est. for GPT-3). We introduce EfficientLLM, presenting novel benchmark definition and the results of the first end-to-end, hundred-scale empirical study of efficiency techniques for LLMs. Executed on production-class cluster (48 GH200, 8 H200 GPUs)essential for accurately measuring real-world performance and energy trade-offsour study is grounded in unified three-axis taxonomy: architecture pretraining, fine-tuning, and inference. Specifically, we focus on these three aspects due to their direct practical implications for different stakeholders in the LLM lifecycle: (1) Architecture pretraining provides actionable insights for researchers and engineers designing new model architectures, enabling accurate budgeting of computational resources and energy costs; (2) Fine-tuning benchmarks guide practitioners who adapt pretrained base models to specific downstream tasks or domains, helping them select efficient parameter-efficient fine-tuning (PEFT) methods; (3) Bit-width quantization evaluations inform deployment engineers on how to effectively reduce serving costs and latency through quantization techniques that can be directly deployed without retraining. For architecture pretraining, we extensively evaluate efficient attention variants (MQA, GQA, MLA, NSA) and sparse Mixture-of-Experts (MoE). For fine-tuning, we benchmark diverse PEFT methods (LoRA, RSLoRA, DoRA). For inference, we evaluate model compression methods, including post-training quantization down to int4 and float16. We utilize six orthogonal, fine-grained metrics (Average-Memory-Utilization, Peak-Compute-Utilization, Average-Latency, Average-Throughput, Average-Energy-Consumption, Model-CompressionRate) to jointly capture hardware saturation, latencythroughput balance, and carbon cost. Our benchmark evaluates over 100 modeltechnique pairs, covering 0.5B72B parameter LLMs, yielding three core insights: (i) Efficiency Involves Quantifiable Trade-offs: No single method is universally optimal; every technique improves at least one metric while regressing another. *Main contribution. Yanfang Ye is the corresponding author: yye7@nd.edu Latest Update: May 14th, 2025. For instance, MoE trims FLOPs and lifts accuracy but inflates VRAM by 40%, whereas int4 quantization cuts memory/energy by up to 3.9 at measured 35% average task score drop. (ii) Optima are Taskand Scale-Dependent: Efficiency optima are highly context-dependent. MQA offers the best memorylatency frontier for constrained devices, MLA yields the lowest perplexity for quality-critical tasks, and RSLoRA surpasses LoRAs efficiency only beyond 14B parameters, highlighting complex interactions between task, scale, and hardware. (iii) Broad Applicability Across Modalities: We extended our evaluation framework to Large Vision Models (LVMs) and Vision-Language Models (VLMs), applying the same efficiency techniques to models like Stable Diffusion 3.5, Wan 2.1, and Qwen2.5-VL. Techniques validated on LLMs transfer effectively, with MQA/GQA improving LVM generation quality (FID scores) and PEFT methods achieving strong performance-efficiency trade-offs. Our study provides comprehensive insights into these selected aspects, while other important efficiency-related topics, such as training infrastructure optimization, reinforcement learning for post-training alignment, and test-time scaling strategies, are beyond the scope of this paper. We briefly review these additional directions in the related work section and highlight them as promising avenues for future exploration. By open-sourcing datasets, evaluation pipelines, and leaderboards, EfficientLLM provides crucial compass for academics and engineers navigating the efficiencyperformance landscape of next-generation foundation models. * *Note: All values presented in our figures are min-max normalized within each metric across all models. For consistency, all metrics (e.g., PPL, FID, and etc.) are transformed such that higher values indicate better performance or efficiency."
        },
        {
            "title": "2 Observations and Insights",
            "content": "2.1 Overall Observations . . . . . . . . . . . . . . . . . . . . . 2.2 Novel Insights Derived from the EfficientLLM Benchmark ."
        },
        {
            "title": "3 Background",
            "content": "3.1 Large Language Models (LLMs) . . . . . . . . 3.2 Approaches to Enhancing Efficiency in LLMs . 3.2.1 Hardware Innovations . . 3.2.2 Software Optimizations . . . . . 3.2.3 Algorithmic Improvements . . . . 4 Techniques for Improving LLM Efficiency 4.1 Dimensions of LLM Efficiency . . 4.2 Budget Efficiency: Scaling Laws . . . . . . . . . . . . . . . . . . . . 4.2.1 Scaling Behavior and Power Laws . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.2.2 Compute-Optimal Model Scaling (Chinchilla vs. Gopher) . 4.2.3 Data Constraints and Quality . 4.2.4 Open Problems in Scaling . 4.3 Data Efficiency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.3. Importance of Data Quality and Filtering . 4.3.2 Curriculum Learning . . . . . . . . . . 4.3.3 Data Augmentation and Synthetic Data 4.4 Architecture Efficiency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.4.1 Motivation: Rethinking the Transformer for Efficiency . 4.4.2 Efficient Attention Mechanisms 4.4.3 Efficient Positional Encoding . . . . . . . . . . . . 4.4.4 Sparse Modeling via Mixture-of-Experts . . . . . . . . . . . . . . . . . . . 4.4.5 Attention-Free Alternatives for Sequence Modeling . 4.5 Training and Tuning Efficiency . . . 4.5.1 Scalable Training Strategies . . . . . . . . . . . . . . 4.5.2 Parameter-Efficient Fine-Tuning (PEFT) . 4.6 Inference Efficiency . . . . . . . . . . . . 4.6.1 Model Compression Techniques . . . . . . . . . 4.6.2 Algorithm-Level Inference Optimizations . . . . . . . . . . . . 4.6.3 System-Level Optimizations and Deployment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 5 8 8 10 12 13 13 14 15 18 19 19 19 20 21 21 21 22 23 23 25 27 28 29 30 32 32"
        },
        {
            "title": "5 Assessment",
            "content": "5.1 Assessment Principles of EFFICIENTLLM . 5.1.1 Computational System Utilization . 5.1.2 Energy Consumption . . . 5.1.3 Model Compression Rate . 5.1.4 Model Performance . . . . 5.2 Preliminaries of EFFICIENTLLM . 5.2.1 Curated List of LLMs . 5.2.2 Experimental Datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.3 Assessment of Architecture Pretraining Efficiency . . . . . . . . . . 5.3.1 Assessment of Efficient Attention Mechanisms 5.3.2 Assessment of Efficient Positional Encoding . 5.3.3 Assessment of Sparse Modeling via MoE . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.3.4 Assessment of Attention-Free Alternatives for Sequence Modeling . 5.4 Assessment of Training and Tuning Efficiency . . . . . . . . 5.5 Assessment of Bit-Width Quantization Inference Efficiency . . . . . . . 6 Scalability of EfficientLLM Benchmark 6.1 Efficiency for Transformer Based LVMs Architecture Pretraining . 6.2 Assessment of PEFT on LVMs . 6.3 Assessment of PEFT on VLMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 Related Work 8 Discussion 8.1 Limitations . . . . . . . . . . . . . . . 8.2 Open Challenges and Future Directions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 Conclusion Appendix 33 33 35 36 36 38 39 41 42 42 43 45 48 50 50 52 54 57 57 57 59"
        },
        {
            "title": "INTRODUCTION",
            "content": ""
        },
        {
            "title": "Introduction",
            "content": "Large Language Models (LLMs), such as GPT-style architectures [1] and Pathways Language Model (PaLM) [2], are key type of Foundation Model that have driven significant breakthroughs across numerous domains. These models, often characterized by billions or even trillions of parameters [1, 2], achieve remarkable performance by leveraging deep learning techniques and training on massive datasets [3, 4], typically comprising trillions of tokens from diverse sources like the web, books, and code. LLMs demonstrate powerful capabilities in complex tasks including nuanced language generation, sophisticated reasoning, and problem-solving. Their advancements have particularly impacted natural language processing (NLP), automated content creation [5], multilingual translation, enhanced search [6], software engineering support [7], finance, scientific research, education, and human-computer interaction paradigms. The rapid advancement of foundation language models has spurred vast body of related research. Numerous studies focus on enhancing performance on downstream tasks [3, 8], improving reasoning [8] and generalization abilities, and extending capabilities to new modalities. Significant efforts also address critical concerns such as trustworthiness (encompassing fairness, transparency, accountability, robustness) [9, 10, 11], interpretability, privacy, and societal implications. This multi-faceted research landscape reflects the profound impact and inherent complexity of these powerful models. As these powerful models demonstrate increasing utility, they are being deployed across widening array of applications and domains. However, this widespread adoption brings the escalating costs and resource demands associated with their development and deployment into sharp focus. The trend towards ever-larger models necessitates massive computational power and memory capacity. For instance, training GPT-3 (175B parameters) required an estimated 3,640 Petaflop/s-days [3], measure of sustained computational effort, and potentially cost millions of dollars in cloud computing resources. Similarly, training Googles 540B parameter PaLM model required thousands of TPUv4 chips running for extended periods, representing massive computational undertaking [2]. Deploying such models for inference at scale also incurs substantial costs, driven by hardware requirements and energy consumption. Running these colossal models requires extensive distributed computing infrastructure, typically involving large clusters of specialized accelerators like GPUs (e.g., NVIDIA H100, H200) or TPUs (e.g., TPUv4 used for PaLM [2]), often running continuously. These resource-intensive processes translate into substantial financial investments, prolonged development cycles, and considerable energy consumption, contributing to significant carbon emissions [12]. Recognizing these challenges, growing body of research focuses on improving foundation model efficiency throughout their lifecycle [13, 14, 15, 16, 17]. Techniques span multiple dimensions: Architectural Innovations: Approaches like sparse attention mechanisms [18] and Mixture-of-Experts (MoE) layers [19, 20] aim to reduce the number of active parameters or computations per input token. Efficient attention variants, such as Multi-Query Attention (MQA) [21], Grouped-Query Attention (GQA) [22], Multi-Head Latent Attention (MLA) [23], and Native Sparse Attention (NSA) [24], seek to lower the computational or memory overhead of the attention mechanism, particularly the Key-Value (KV) cache. Training Efficiency: Methods like mixed-precision training [25] reduce memory usage and can leverage specialized hardware like Tensor Cores. Sophisticated parallelization strategies (data, tensor, pipeline parallelism) [26, 27] are essential for distributing training across large clusters. Parameter-Efficient FineTuning (PEFT) techniques, including Low-Rank Adaptation (LoRA) [28] and its numerous derivatives like LoHa (Low-Rank Hadamard Product) [29], LoKr (Low-Rank Kronecker Product) [30], and GLoRA (Generalized LoRA) [31], allow adapting large pre-trained models to downstream tasks by updating only small fraction of parameters. Inference Optimization: Model compression techniques like quantization [32, 33, 34] (reducing numerical precision, e.g., to INT8 or INT4) and pruning [35, 36] (removing redundant weights) reduce model size and memory footprint. Knowledge distillation [37] trains smaller models to mimic larger ones. Optimized decoding algorithms, such as speculative decoding [38] or memory-efficient attention mechanisms like PagedAttention [39], improve generation speed and throughput. However, despite the proliferation of these techniques, no existing benchmark or evaluation framework systematically provides large-scale, end-to-end empirical comparisons under realistic deployment conditions."
        },
        {
            "title": "INTRODUCTION",
            "content": "Existing studies often focus on specific techniques in isolation, use limited model scales, lack comprehensive metrics (especially energy consumption on modern hardware), or rely on theoretical analysis rather than extensive empirical validation. Benchmarking in computing aims to assess relative performance by running standard tests, but comprehensive benchmarks covering the multifaceted nature of LLM efficiency (including service quality, cost, energy) are scarce. Consequently, practitioners often lack clear, data-driven guidance on selecting the most resource-efficient model architecture pretraining, fine-tuning strategy, and inference optimization for their specific tasks and constraints. In practice, many deployment decisions are made heuristically based on anecdotal experience or limited internal testing, due to the absence of large-scale empirical comparisons validating real-world efficiency trade-offs. Conducting such large-scale verification studies faces inherent challenges: (1) the substantial computational cost associated with training and evaluating numerous large models and techniques; (2) the lack of universally adopted, comprehensive efficiency metrics that encompass memory, compute, latency, throughput, and energy; (3) the complex interplay between different efficiency techniques and their combined impact on model performance, requiring extensive experimentation; and (4) ensuring reproducibility and comparability across different hardware platforms and software frameworks, persistent challenge for the research community. To address this gap, as shown in Figure 1, we introduce EfficientLLM, the first large-scale empirical benchmark systematically evaluating LLM efficiency across three critical dimensions: (1) Architecture Pretraining, providing researchers and model designers with concrete insights into computational and energy budgeting when developing new architectures; (2) Fine-Tuning, guiding practitioners who adapt pretrained base models to specific downstream tasks or domains by benchmarking diverse Parameter-Efficient Fine-Tuning (PEFT) methods; and (3) Bit-Width Quantization, informing deployment engineers on how to effectively reduce serving costs and latency through quantization techniques that can be directly deployed without retraining. Figure 1: Overview of the EfficientLLM framework. In the Architecture Pretraining Efficiency dimension, we systematically assessed how different architectural optimizations during the pretraining stage impact model performance and efficiency. We tested multiple attention mechanisms (e.g., MQA, GQA, MLA, NSA) across varying model scales (0.5B to 3B parameters) and found that MQA excels in memory utilization and latency, making it ideal for memory-constrained environments. MLA demonstrated superior performance in terms of perplexity, while NSA achieved the lowest"
        },
        {
            "title": "INTRODUCTION",
            "content": "energy consumption, highlighting its suitability for low-power deployments. Additionally, we evaluated sparse modeling techniques like Mixture-of-Experts (MoE) and attention-free alternatives (e.g., State Space Models, RNNs). Our results showed that MoE significantly improves performance for large-scale models but requires higher memory and computational resources. In the Training and Tuning Efficiency dimension, we explored strategies to reduce the resource cost of training and fine-tuning LLM. We compared parameter-efficient fine-tuning methods (e.g., LoRA, RSLoRA, DoRA) across models ranging from 1B to 72B parameters. LoRA and its variants performed exceptionally well for smaller models (1B to 3B parameters), achieving the lowest loss metrics. For larger models (e.g., 14B parameters), RSLoRA demonstrated superior efficiency with lower latency and energy consumption. We also found that parameter freezing is optimal for latency-sensitive applications, though it may trade off some performance. In the Inference Efficiency dimension, we conducted comprehensive evaluation of inference performance across varying precisions (bfloat16, float16, and int4 quantization) for models ranging from 1.5B to 34B parameters, including DeepSeek, Qwen, Phi, and Yi. Results showed that lower-precision quantization (int4) significantly improved memory utilization and throughput (tokens/s), achieving up to 3.9 times model compression ratio (MCR), thus demonstrating substantial benefits for resource-constrained deployments. However, this gain typically came with modest decrease in average task-specific performance (e.g., drop from 0.4719 to 0.4361 for DeepSeek-R1-Distill-Qwen-14B). Among floating-point precisions, bfloat16 generally outperformed float16 slightly in terms of computational efficiency, as reflected by lower average latency and reduced energy consumption across most tested models. For instance, DeepSeek-R1-Distill-Qwen1.5B in bfloat16 achieved lower latency and energy consumption (144.39 W) compared to float16 (158.96 W). These findings indicate that int4 quantization is highly effective for reducing resource demands at the expense of minimal performance degradation, whereas bfloat16 precision is optimal for balancing computational efficiency and inference accuracy in higher-performance scenarios. cornerstone of the EfficientLLM project is our extensive, large-scale empirical evaluation. We propose suite of fine-grained efficiency metrics designed to capture computational system utilization (memory, compute, latency, throughput), energy consumption, and compression rates. To provide rigorous quantitative comparisons, we constructed large-scale benchmark leveraging substantial computational resources. This extensive hardware infrastructure allowed us to conduct experiments across diverse model scales, architectures, and tasks, yielding robust empirical evidence on the practical trade-offs of different efficiency techniques. Our main contributions are: systematic categorization and review of efficiency techniques for foundation models covering architecture, training, and inference, grounded in the existing literature. novel set of detailed metrics for evaluating the multi-dimensional aspects of foundation model efficiency, including hardware utilization (memory, compute), performance (latency, throughput), energy consumption, and model compression, addressing the need for more comprehensive evaluation criteria. An extensive empirical benchmark evaluating numerous state-of-the-art foundation models and efficiency techniques on large-scale, modern GPU cluster, addressing the gap in systematic, large-scale empirical comparisons. Actionable guidance for practitioners on selecting efficient models and techniques based on task requirements and resource constraints, grounded in rigorous empirical data rather than solely theoretical analysis or heuristic choices. The remainder of this paper is structured as follows. Section 2 presents key observations and novel insights derived from our large-scale experiments. Section 3 provides background information on foundation models and discusses fundamental approaches to enhancing efficiency. Section 4 details the specific efficiency improvement techniques evaluated within our framework. Section 5.1 defines our proposed efficiency assessment principles and metrics. Section 5.2 describes the curated list of models and experimental settings used in our benchmark. Sections 5.3, 5.4, and 5.5 present the detailed empirical results for architecture, training/tuning, and inference efficiency, respectively. Finally, Section 8 discusses remaining challenges and future research directions, and Section 9 concludes the paper."
        },
        {
            "title": "2 Observations and Insights",
            "content": "To facilitate the overall understanding of our study, in this section, we first present the observations and insights we have drawn based on our extensive empirical experiments in the EfficientLLM framework. Figure 2: Ranking of LLM training and inference efficiency and performance across various techniques. The chart compares attention mechanisms, MoE designs, and architecture types (top block), parameter-efficient fine-tuning methods (middle block), and quantization strategies (bottom block) across eight dimensions: performance, utilization (AMU, PCU), latency (AL, TT), throughput (ST, IT, TT), energy consumption (AEC), and compression (MCR). For parameter-efficient tuning, Freeze refers to the method, which freezes the frist 8 layers of the model. Methods marked with an asterisk (), such as Full, utilize DeepSpeed ZeRO-3. 2.1 Overall Observations No single technique achieves Pareto optimality on all efficiency axes. Our benchmark, involving over 100 model-technique combinations run across 48 GH200 and 8 H200 GPUs, revealed that every evaluated method improved at least one metric (memory, latency, throughput, energy, or compression) while compromising others. For example, Mixture-of-Experts (MoE) architectures [20, 40] boosted downstream accuracy and reduced FLOPs per token during inference (by activating only subset of parameters), yet inflated peak memory"
        },
        {
            "title": "2 OBSERVATIONS AND INSIGHTS",
            "content": "requirements due to the need to store all expert parameters, and introduced routing overhead. Our experiments showed MoE could increase VRAM usage by approximately 40% compared to dense model of equivalent active parameter count (detailed in Section 5.3). Conversely, post-training int4 quantization slashed memory footprint and energy consumption by up to 3.9 but incurred modest average-task performance drop of approximately 35% across tested models (detailed in Section 5.5) [41]. These quantified trade-offs highlight that efficiency must be treated as multi-objective optimization problem , not reducible to single leaderboard score. This observation provides strong empirical validation for the No-Free-Lunch (NFL) theorem [42] in the context of LLM efficiency. While the NFL theorem, originally formulated by Wolpert and Macready , states theoretically that no single algorithm universally outperforms others across all possible problems when averaged, our benchmark demonstrates this principle concretely. The results across numerous model-technique pairs and six distinct efficiency metrics quantify the specific costs associated with gains for practical LLM optimization strategies, moving beyond theoretical averages to specific, measured outcomes. Resource-Driven Trade-Offs in Efficient Attention Mechanisms. Our tests on models ranging from 0.5 to 3 parameters showed distinct advantages among the four efficient attention mechanisms evaluated: MultiQuery Attention (MQA) [21], Grouped-Query Attention (GQA) [22], Multi-Head Latent Attention (MLA) [23], and Native Sparse Attention (NSA) [24]. MQA delivered the lowest VRAM footprint (due to sharing key/value heads) and fastest latency, making it preferable for memory-constrained environments or on-device inference. MLA, introduced by DeepSeek [23] to compress the KV cache into latent vector, minimized perplexity in our tests, rendering it attractive when raw language quality is paramount. NSA, designed as hardware-aligned and natively trainable sparse attention mechanism, consumed the least energy per generated token in our evaluations, favouring low-power deployments or scenarios where energy cost is primary concern. These results confirm that \"one-size-fits-all\" attention mechanism does not exist; the benchmark data enables practitioners to make evidence-based selections, aligning the variant with their dominant resource bottleneck or performance goal (e.g., minimizing latency vs. maximizing quality vs. minimizing energy). Parameter-efficient fine-tuning (PEFT) methods scale differently with model size. We observed that LowRank Adaptation (LoRA) [43] and its derivatives, such as DoRA (Weight-Decomposed Low-Rank Adaptation) [44] and other variants collectively referred to as LoRA-plus, achieved the lowest performance loss (i.e., best task performance metrics like accuracy or lowest loss values) for models in the 1 to 3 parameter range under specific memory constraints. However, RSLoRA [45], another LoRA variant, overtook the original LoRA in terms of efficiency, exhibiting lower latency and wattage, specifically for models with 14 parameters or more. For ultra-large checkpoints, our analysis indicated that parameter freezing (updating only specific layers or components like biases) produced the best end-to-end latency during the tuning process, albeit sometimes at small cost in final task accuracy compared to LoRA-based methods. Consequently, selecting the appropriate PEFT method based on the target models scale yields larger efficiency gains than uniformly applying single technique. This highlights scale-dependent interaction effect, suggesting that findings from smaller models regarding the relative merits of different PEFT techniques may not directly extrapolate to significantly larger models. Lower-precision formats deliver disproportionate returns on memory-bound workloads. Our quantitative analysis across Llama-3, DeepSeek, and Qwen models (1.5B to 34B) indicates that int4 post-training quantization significantly improves resource efficiency. Compared to bfloat16, int4 reduced the memory footprint by up to 3.9 (approaching the theoretical maximum of 4 reduction from 16-bit to 4-bit representation) and tripled the throughput in tokens per second (TPS) under memory-bound conditions. This substantial gain came at the cost of only slight drop in average task performance scores (e.g., for DeepSeek-R1-Distill-Qwen-14B [46], the average score dropped from 0.4719 in bf16 to 0.4361 in int4). The term disproportionate returns here signifies that the substantial gains achieved in resource efficiency (memory footprint reduction approaching 4x, throughput tripling) far outweigh the relatively small cost incurred in terms of task performance degradation (average drop of 3-5 percentage points). This makes int4 highly attractive when memory, energy, or cost are primary constraints. Between the 16-bit floating-point formats, bfloat16 consistently outperformed float16 in terms of average latency and energy consumption on our Hopper architecture GPUs (GH200/H200). This is attributed to the native hardware acceleration (Tensor Cores) for bfloat16 operations on these modern NVIDIA GPUs. This suggests that adopting \"BF16-first\" strategy is safe default if quantization is not feasible or if the associated performance drop is unacceptable for the target application."
        },
        {
            "title": "2 OBSERVATIONS AND INSIGHTS",
            "content": "Figure 3: Efficiency LLM Results. This figure illustrates the performance and efficiency trade-offs of various architectural improvements for LLMs. (a) Radar charts comparing different Efficient Attention Mechanisms (MQA, GQA, MLA, and NSA) across 0.5B, 1.5B, and 3B model parameters, evaluated on Perplexity (PPL), Average Memory Utilization (AMU), Average Latency (AL), Tokens Throughput (TT), and Average Energy Consumption (AEC). (b) Bar chart assessing Efficient Positional Encoding methods (RoPE, Absolute, Learnable Absolute, Relate, and None) for 1.5B parameter model on the same five key metrics. (c) Bubble chart comparing Dense Models with Mixture-of-Experts (MoE) Models of varying parameter sizes, highlighting differences in PPL, AMU, AL, TT, and AEC. These visualizations correspond to the detailed results presented in Tables 4, 5, and 6. Note: All metrics presented in this figure are normalized. 2.2 Novel Insights Derived from the EfficientLLM Benchmark Architecture Pretraining Efficiency. Architecture pretraining efficiency involves balancing memory, latency, and quality trade-offs during the pretraining stage. Our benchmark yielded the following architectural insights, as shown in Figure 3: 1) Attention variants have distinct optima during pretraining: Among the four efficient attention variants tested in pretraining, our quantitative analysis shows MQA hits the best memorylatency frontier, MLA achieves the lowest perplexity, and NSA minimizes energy consumption. 2) MoE presents compute-memory trade-off in pretraining: We confirmed that sparse Mixture-of-Experts (MoE) during pretraining can add up to 3.5 percentage points in accuracy while cutting training FLOPs by 1.8. However, this comes at the cost of inflating VRAM usage by 40%, highlighting clear tension between compute savings and memory demands. 3) Attention-free models offer pretraining efficiency gains with quality trade-offs: Our evaluation showed that attention-free Mamba models during pretraining trim Average Memory Usage (AMU) and Average Energy Consumption (AEC) by 25% but incur 1-point perplexity penalty. RWKV achieved the lightest memory footprint in our pretraining tests, whereas Pythia yielded the fastest latency, albeit at the cost of higher perplexity. 4) Depthwidth aspect ratio has flat optimum in pretraining: Confirming the robustness of Chinchillas scaling laws for aspect ratios during pretraining, our depthwidth sweeps show flat basin where configurations within 20% of the Chinchilla-optimal aspect ratio reach statistically indistinguishable loss levels. This allows flexibility for hardware-aligned architectural tailoring without sacrificing performance."
        },
        {
            "title": "2 OBSERVATIONS AND INSIGHTS",
            "content": "Figure 4: Assessment of training and fine-tuning efficiency across multiple LLMs. (a) Comparison of different fine-tuning methods (LoRA, LoRA-plus, RSLoRA, DoRA, PISSA, Freeze, and full fine-tuning using DeepSpeed) across seven model architectures (Llama-3.2-1B/3B, Llama-3.1-8B, Qwen-2.5-7B/14B, Mistral-Small-24B, and Mistral-7B) using the O1-SFT dataset. Each bar shows the corresponding Efficiency Score (higher is better) and Loss (lower is better). The Efficiency Score is computed as weighted harmonic combination of normalized resource metrics. Methods marked with * denote full fine-tuning using DeepSpeed. Training & Tuning Efficiency. We benchmarked full fine-tuning against five Parameter-Efficient Fine-Tuning (PEFT) methods. Our findings include, as shown in Figure 4: 1) Optimal PEFT method varies with scale: For 13B models, our results show LoRA-plus (LoRA and its variants like DoRA) achieves the lowest loss under 60 GB AMU constraint. For models above 14B parameters, RSLoRA dominates on both loss and latency metrics. 2) Parameter freezing offers lowest latency: We measured that parameter freezing slashes fine-tuning latency by 3 compared to any PEFT variant tested, making it suitable for interactive fine-tuning scenarios where slight decrease in average task performance (e.g., approximately 1-2 points on relevant benchmarks, though task-dependent) is acceptable. 3) Full fine-tuning shows diminishing returns at scale: Our experiments indicate that full fine-tuning of models larger than 24B parameters yields diminishing returns, with loss improvements often less than 0.02 even as energy consumption doubles. This strongly argues for adopting PEFT methods for large-scale model adaptation. 4) DoRA latency trade-off : While DoRA maintained stable loss during fine-tuning in our tests, it incurred significant latency overhead, making it more suitable for batch-oriented fine-tuning pipelines rather than real-time or latency-sensitive deployment scenarios. Inference Efficiency. Inference efficiency governs the cost and feasibility of model deployment. Our benchmark provides the following insights, as shown in Figure 5: 1) Quantization yields high compression with minor score impact: Our results show that Int4 post-training quantization reduces memory footprint and throughput (tokens/s) by up to 3.9 across LLaMA-3, DeepSeek, and Qwen model families (1.5B to 34B parameters), with moderate 35 percentage point drop in average-task scores. 2) BF16 preferred over FP16 on modern GPUs: Between floating-point formats, our measurements on GH200/H200 GPUs consistently show bfloat16 beating float16 by 6% in latency and 9% in energy consumption, benefiting from native hardware acceleration."
        },
        {
            "title": "3 BACKGROUND",
            "content": "Figure 5: Assessment of quantization-based inference efficiency across model precisions. Radar plots compare normalized efficiency metrics across three quantization formats: bfloat16, float16, and int4. Each plot evaluates models from DeepSeek, Qwen, Phi, and Yi families using six normalized metrics (all higher is better): average task performance, inference throughput (IT), average memory utilization (AMU), sum latency (Sum AL), average energy consumption (AEC), and model compression ratio (MCR). All values are normalized as deilted in Section . While bfloat16 typically yields higher performance scores, int4 excels in throughput, memory, and compression, indicating its efficiency in deployment-constrained environments."
        },
        {
            "title": "3 Background",
            "content": "3.1 Large Language Models (LLMs) Large Language Models (LLMs) represent revolutionary technology in the field of artificial intelligence. Essentially, these models are complex neural networks based on the Transformer architecture, which, through deep learning from vast textual corpora, can capture and replicate the intricate details of human language. The core architecture of these models relies on the Self-Attention mechanism, enabling them to process input sequences in parallel and effectively capture long-range dependencies and contextual relationships within language. Compared to traditional recurrent neural networks, LLMs demonstrate significant advantages in language understanding and generation tasks. Since the introduction of the Transformer model, the processing power of language models has grown exponentially, evolving from few million parameters to todays models with hundreds of billions or even trillions of parameters. Throughout the development of these models, milestones such as the GPT (Generative Pre-trained Transformer) series [47, 48, 1], BERT [49], and subsequent variants like RoBERTa [50] and ALBERT [51] have been key drivers of LLM advancements. These models have achieved breakthrough progress in areas such as machine translation, text summarization, question answering systems, and code generation through various pre-training strategies and architectural innovations. Notably, ultra-large models such as GPT-3 and GPT-4, through fewshot [1] and zero-shot [8] learning, are capable of handling nearly any natural language task, demonstrating impressive potential for general artificial intelligence. These models not only understand and generate natural language but also perform complex reasoning, creation, and problem-solving tasks. The applications of large language models are extremely broad and have permeated nearly every digital interaction domain. In business services, they can provide intelligent customer service [5], automatic content generation [52, 53], and personalized recommendations [54, 55, 52]; in education, they enable personalized tutoring, intelligent question bank generation, and study assistance [56, 57, 58]; in research and development, they assist with code generation [59, 60], academic writing [61], and literature reviews [6]. More importantly, these models are reshaping human-machine interactions [7], making communication with AI more natural, intelligent, and efficient. From programming assistance to creative writing, from language translation to complex problem-solving, LLMs are becoming universal intelligent tools across various fields. However, LLMs also face significant efficiency challenges [13, 14, 15, 16]. These models typically contain billions to trillions of parameters, with training and inference processes requiring massive computational resources and energy. For example, the training cost of GPT-3 can reach millions of dollars, and the compu-"
        },
        {
            "title": "3 BACKGROUND",
            "content": "Figure 6: The development trends of computational efficiency and memory capacity across NVIDIA GPU series. Note that different colored dots represent different architectures, and the red line indicates the fitted trend of computational efficiency over time. tational expense of single inference is also considerable [1]. Moreover, the deployment and fine-tuning of large models place high demands on hardware infrastructure, limiting their application in resource-constrained environments. As result, more researches are focusing on model compression, knowledge distillation, and efficient fine-tuning techniques, aimed at reducing computational complexity and improving the practical utility and accessibility of these models. Additionally, issues such as bias control, privacy protection, and ethical use of models have become important topics of shared concern in both academia and industry. 3.2 Approaches to Enhancing Efficiency in LLMs 3.2.1 Hardware Innovations Modern AI-specific accelerators are central to handling the immense compute demands of Large Foundation Generative Models. While GPUs remain the workhorse for LLMs with their massively parallel SIMD/SIMT design, specialized chips like Googles TPUs, Amazons Trainium/Inferentia, and Intels Gaudi offer tailored architectures that often lower power consumption and cost per operation [62]. These accelerators typically use systolic arrays to speed up matrix multiplications (critical for transformers) and integrate High-Bandwidth Memory (HBM) to feed data at extreme rates [62]. HBM provides much higher memory bandwidth than traditional DDR memory, alleviating data transfer bottlenecks for large models. However, HBMs on-chip capacity is limited, requiring careful memory management so that model weights and activations are shuttled efficiently without exceeding the cache-like HBM storage. Innovations in interconnects (such as NVIDIAs NVLink and NVSwitch) further improve multi-GPU bandwidth, allowing faster model parallel communication [62]. Overall, the co-design of custom ASICs and memory/network fabric has significantly improved throughput and scalability for training and inference of LLMs. Beyond raw throughput, energy efficiency has become paramount hardware consideration for large models. Data-center AI workloads consume vast power, so modern accelerators emphasize performance per watt. For instance, TPU and similar ASICs achieve higher ops/Joule on transformer tasks than general GPUs by streamlining their circuitry for dense linear algebra [63, 64]. Alongside digital optimizations (like lower-voltage operations and mixed-precision arithmetic), there is exploration of fundamentally new computing paradigms. Neuromorphic computing chips, which mimic brain neurons and operate via sparse spiking signals, promise"
        },
        {
            "title": "3 BACKGROUND",
            "content": "orders-of-magnitude efficiency gains. By co-locating memory and compute and leveraging event-driven operation, neuromorphic processors could execute large neural networks with 100 1000 less energy [65]. Similarly, photonic computing is emerging as futuristic option: optical neural network accelerators can perform matrix operations with light instead of electricity, offering extremely high parallelism with low heat dissipation. Recent prototypes of photonic processors have demonstrated over 100fold improvements in energy efficiency and 25 higher compute density compared to conventional electronics [66]. While still in early stages, these neuromorphic and photonic approaches represent promising paths for future efficiency gains once todays silicon-based architectures hit their limits [67]."
        },
        {
            "title": "3.2.2 Software Optimizations",
            "content": "Efficient software frameworks and parallelization strategies are crucial to fully utilize hardware for LLMs. Distributed computing techniques enable splitting giant models and workloads across many devices in parallel [68]. For training, this often means hybrid parallelism: data parallelism to copy the model across nodes for different data batches, combined with model/tensor parallelism to split the models layers or tensor operations among accelerators. For example, GPU clusters running libraries like DeepSpeed [26] or Megatron-LM [27] orchestrate tensor sharding, pipeline parallelism (partitioning layers into stages), and optimizer state sharding to overcome memory limits [67, 62]. Such coordination is non-trivialLLMs with hundreds of billions of parameters do not fit on single device, so software must partition the model and manage inter-GPU communication efficiently. Advances in collective communication (e.g. using high-speed interconnects or custom protocols) and load balancing ensure that distributed training scales with minimal overhead. In short, sophisticated parallel runtime systems hide the complexity of multi-node training, achieving near-linear speedups and making tractable the otherwise prohibitive training times (often running for weeks over thousands of GPUs). We compare several popular LLM and VLM frameworks across their support for pre-training, fine-tuning, and inference. Notably, frameworks such as Colossal-AI, Composer, DeepSpeed, FairScale, and Megatron support all three stages, including large-scale pre-training. In contrast, LLM Foundry and OpenLLM focus primarily on fine-tuning and inference, while tools like RayLLM, vLLM, and Text Generation Inference are optimized for efficient serving only. full comparison is provided in Appendix . Another major avenue is model compression and efficient fine-tuning techniques that reduce the memory and compute footprint of large models [69, 70]. Quantization has become standard approach: model weights and activations are converted from 32-bit floats to lower precision (e.g. 8-bit integers) to save memory and accelerate tensor operations. By sacrificing small amount of accuracy, INT8 or even INT4 quantization can dramatically improve inference throughput for instance, 8-bit weight quantization yielded 1.5 speedup on transformer inference with only 23% accuracy loss in one study. Pruning techniques remove redundant parameters or structures from the network to slim down model size. By identifying neurons, attention heads, or weights that contribute little to outputs, pruning can maintain model quality while cutting down FLOPs. Structured pruning (dropping whole units or layers) tends to yield actual speedups on hardware, whereas unstructured pruning creates sparse weights that may need specialized hardware to exploit [71, 13]. These methods are challenging for LLMs (aggressive pruning can degrade accuracy), but recent research on magnitude-based and optimal brain surgeon pruning has made progress in sparsifying large transformers without severe performance loss. In the training regime, low-rank adaptation has emerged as an efficient fine-tuning strategy: instead of updating all billion parameters of model for new task, one can insert small low-rank weight matrices and train only those. LoRA is prime example that freezes the original model weights and learns limited number of new parameters per layer. This approach yielded over 10, 000 reduction in trainable parameters (and 3 lower VRAM usage) when adapting GPT-3, yet achieved on-par accuracy to full fine-tuning. Techniques like LoRA [28] thus enable personalizing or specializing LLMs without the exorbitant cost of retraining the entire network. At the systems level, compiler optimizations and specialized kernels greatly improve the runtime efficiency of model execution. Deep learning compilers (XLA, TVM, PyTorch Glow/Inductor, etc.) take high-level model graphs and generate low-level code that maximizes hardware utilization. They apply optimizations such as operator fusion (merging multiple neural network operations into one kernel launch), loop tiling and memory layout optimization (to exploit caches or shared memory on GPUs), and vectorization. For example, combining the operations of attention computation (matrix multiplication + softmax) into fused kernel"
        },
        {
            "title": "3 BACKGROUND",
            "content": "can avoid intermediate memory writes and improve speed. notable optimized kernel is FlashAttention series [72, 73], which reimplements the attention mechanism in tile-by-tile fashion to use on-chip memory efficiently, thereby reducing memory bandwidth usage and enabling larger sequence lengths with lower latency. Similarly, libraries provide hand-tuned or auto-tuned kernels for transformer building blocks (dense layers, layer normalization, convolution in vision models) that exploit the specific accelerators capabilities (Tensor Cores, etc.). These low-level improvements often yield significant gains: for instance, using an optimized attention kernel or JIT-compiled fused operation can improve throughput by > 2 compared to naive implementations [74]. The use of graph compilers also allows automatic exploration of different execution plans (such as finding the best parallelization or memory trade-off) and can adapt models to new hardware with minimal manual code rewriting. Overall, the compiler and kernel-level innovations ensure that the theoretical speedups of advanced hardware are actually realized when running large models at scale. Finally, knowledge distillation [75] and retrieval-augmented generation [76] are high-level software strategies to make large models more efficient in practice. Knowledge distillation involves training smaller student model to replicate the behavior of large teacher model, effectively compressing knowledge into compact network [75, 77]. This has been used to create lightweight versions of giant models (e.g., DistilBERT [78] is distilled 66M parameter version of BERT [49] that retains most of its accuracy). Distillation can significantly reduce model size and inference cost, though careful training is required to preserve quality on diverse tasks [77]. Retrieval-Augmented Generation (RAG) techniques, on the other hand, aim to reduce the burden on the models parameters by offloading some knowledge to an external database. In this approach, an LLM is coupled with retrieval system that fetches relevant documents or facts from large corpus, which the model then conditions on during generation. This allows even smaller model to produce informed, accurate outputs by leveraging information beyond its fixed weights. For example, the RETRO [79] model by DeepMind augments 7.5B parameter transformer with text chunk database and retrieval mechanism; remarkably, RETRO [79] with 7.5B parameters outperformed 175B parameter GPT-3-style model Jurassic-1 [80] on multiple language benchmarks by virtue of accessing rich external knowledge base [79]. This result underscores how retrieval can substitute for brute-force parametric knowledge, attaining the accuracy of model over 20 larger. By marrying generation with search, RAG methods improve factual accuracy and efficiency, since the model doesnt need to internalize every piece of world knowledge [81, 82]. Such techniques, alongside modular and memory-augmented model designs, highlight trend of leveraging external resources and smarter training schemes to curb the resource requirements of foundation models without sacrificing capability. 3.2.3 Algorithmic Improvements At the algorithm level, researchers have proposed numerous Transformer architecture refinements to boost efficiency for LLMs, LVMs, and multimodal models. One direction is sparse attention mechanisms, which limit the quadratic cost of attending to every token [18]. Sparse Transformers [18] introduce structured patterns in the attention matrix (e.g. attending only locally or to subset of tokens) to bring complexity down from O(n2) to sub-quadratic or linear in sequence length. This enables handling longer sequences or higher resolutions with the same compute budget. Models like Longformer [83], BigBird [84], and Reformer [85] use block-local attention or hashing-based mixing to achieve this kind of efficiency, essentially skipping computation for many token pairs with negligible impact on accuracy. Another powerful idea is the Mixture-of-Experts (MoE) [40] architecture, which increases model capacity by having multiple expert subnetworks and routing each input token through only one or few of them [86]. In transformer MoE layer, different experts (sets of feedforward parameters) specialize on different tokens, and gating function selects which expert to activate per token (making the computation sparse). This allows an MoE model to have very large number of parameters in total, but each inference/pass only uses fraction of them. MoE transformers (e.g. Switch Transformers [20]) have been shown to achieve comparable or higher accuracy than dense models with the same effective compute. In fact, MoEs can be pre-trained substantially faster than dense models of equivalent size, and they yield faster inference throughput for given budget of floating-point operations [87, 88]. The trade-off is that maintaining many experts demands more memory and introduces complexity in load-balancing the experts utilization. Nonetheless, MoEs represent promising efficiency leap: Googles Switch-C Transformer [20] (with 1.6T parameters across experts) demonstrated that vastly larger sparse models can be trained at the same cost as much smaller dense model, leveraging only modest accuracy trade-offs. Other architecture tweaks include linear or low-rank attention mechanisms that approximate the attention computation with kernel feature maps"
        },
        {
            "title": "3 BACKGROUND",
            "content": "(as in the Performer and Linear Transformer models), reducing memory usage by avoiding explicit attention matrices. Such linear attention variants scale as O(n d) and can be parallelized to outperform standard attention for long sequences, though maintaining accuracy remains an area of active research. In the vision domain, analogous ideas like token pruning/merging in Vision Transformers (reducing the number of patches processed) also improve efficiency. In summary, by re-imagining the transformers core operations whether through sparsity, factorization, or conditional computation these architectural innovations enable handling larger inputs or models at lower computational cost, albeit sometimes with added system complexity. Improving the training process itself is another important angle for efficiency. Curriculum learning strategies have been revisited for large models to speed up and stabilize training. The idea, dating back to Bengio et al., is to present easier examples or sub-tasks first and gradually increase difficulty, so that the model learns faster (much like humans learning concepts in logical order). For instance, an LLM could first be trained on shorter or simpler text sequences before introducing very long and complex documents, allowing it to build strong foundation and converge in fewer steps than if all data were seen randomly. Another approach is progressive stacking (layer growth), where one starts training smaller model and then incrementally increases its depth/size using knowledge from the smaller model. Gong et al. demonstrated this with BERT: they first trained shallow L-layer model, then grew\" it to 2L layers by duplicating the learned layers, and continued training the larger model converged much faster than training from scratch with 2L layers [49]. This form of warm-start leverages the learned weights of simpler model to initialize bigger model, effectively bootstrapping the training of deep networks. Progressive stacking and related model growth techniques (like gradually increasing the sequence length or model width during training) can find an efficient path through the training landscape, saving time and compute. Moreover, techniques like curriculum in data selection (ordering training data by quality or complexity) or gradual unfreezing (fine-tuning large models by slowly relaxing which layers are trainable) act as implicit regularizers, often reaching better optima with less data or compute. While these methods introduce additional scheduling heuristics to the training pipeline, they have shown tangible efficiency improvements in practice by converging to high performance with fewer updates. Data efficiency is also crucial aspect making the most out of the data that models see. Innovations in tokenization help reduce wasted computation on overly long sequences or irrelevant tokens. Subword segmentation algorithms (BPE , WordPiece, SentencePiece) have evolved to produce more efficient vocabularies that balance vocabulary size and sequence length. good tokenizer can significantly shorten the input sequence (e.g., by merging frequent word pieces or handling multi-byte characters effectively), thereby reducing the number of transformer steps required. For instance, modern byte-level BPE tokenizers can represent text with fewer tokens than character-level methods, especially for languages with many compound words, directly improving model throughput. In multimodal models, analogous token or patch optimizations (such as merging similar image patches or using lower resolution early in processing) also yield efficiency gains. Beyond tokenization, self-supervised learning paradigms greatly enhance data efficiency by leveraging unlabeled data at scale. Rather than relying on limited human-annotated examples, large models are pretrained on raw text or images via predictive tasks (next word prediction, masked token recovery, image-text alignment, etc.), which effectively turn vast unsupervised corpora into training signal. This has enabled data scaling laws, where more data can substitute for bigger models. Notably, recent research on compute-optimal model scaling found that many earlier LLMs were substantially under-trained on data for their size. DeepMinds Chinchilla project showed that 70B parameter model trained on 1.4 trillion tokens (4 more data than similarly sized Gopher) outperformed 175B model (GPT-3) that had less training data, all while using the same training compute budget [4]. This result underlines the importance of feeding models with sufficient and high-quality data: smaller but properly trained model can be more powerful and efficient than larger, under-trained one. The takeaway is that there is an optimal balance between model size and dataset size for given compute budget. By following such scaling law insights, one can achieve better performance per compute by right-sizing the model and dataset. In practice, techniques like data filtering and deduplication (to ensure the model isnt wasting capacity on corrupt or repetitive examples), as well as smarter data augmentation, also help models reach higher accuracy faster. In multimodal settings, leveraging pre-trained unimodal models (like vision or language models) as starting point for combined tasks is another data-efficient strategy, effectively reusing knowledge. All these approaches focus on extracting maximum learning from each sample the model sees, which is crucial when pushing the limits of model scale without an explosion in required data."
        },
        {
            "title": "3 BACKGROUND",
            "content": "GPU GTX 1080 GTX 1080 Ti RTX 2080 Ti TITAN RTX RTX 3090 RTX 3090 Ti RTX 3060 Tesla V100 RTX A4000 RTX A5000 RTX A6000 A40 PCIE A100 PCIe 80 GB L20 RTX 4090 H100 PCIe 80 GB H100 SXM5 80 GB H200 SXM 141 GB Release Year Mem (GB) Transistors (M) Architecture FP32 (TFLOPS) FP16 (TFLOPS) PSU (W) Table 1: Specifications of various NVIDIA GPUs. 2016-5 2017-3 2018-9 2018-12 2020-9 2022-1 2021-1 2018-3 2021-4 2021-4 2020-10 2020-10 2021-6 2023-11 2022-9 2023-3 2023-3 2024-12 8 11 11 24 24 24 12 32 16 24 48 48 80 48 24 80 80 141 7200 11800 18600 18600 28300 28300 12000 21100 17400 28300 28300 28300 54200 76300 76300 80000 80000 Pascal Pascal Turing Turing Ampere Ampere Ampere Volta Ampere Ampere Ampere Ampere Ampere Ada Lovelace Ada Lovelace Hopper Hopper Hopper 8.87 11.34 13.45 16.31 35.58 40 12.74 14.13 19.17 27.77 38.71 37.42 19.49 59.35 82.58 51.22 66.91 66.91 26.9 32.62 35.58 40 12.74 28.26 19.17 27.77 38.71 37.42 77.97 59.35 82.58 204 267.6 267.6 450 600 600 600 750 850 450 600 300 550 700 700 700 600 850 750 1100 1100 Finally, advances in optimization algorithms have played key role in efficient large-model training. Traditional stochastic gradient descent has largely been supplanted by adaptive optimizers like Adam [89], Adagrad [90], LAMB [91], etc., especially for huge models. These methods adapt the learning rate for each parameter based on past gradients, enabling more stable and faster convergence in very high-dimensional parameter spaces. For example, the Adam optimizer was pivotal for training transformers and is used almost universally for LLMs because it handles sparse gradients and varying feature scales automatically. The LAMB optimizer extended this to support extremely large batch training in one case, allowing BERT [49] pre-training to scale to batch size of 32k without loss of accuracy, thereby reducing the training time from 3 days to only 76 minutes on TPU pod [91]. Such adaptive schemes make it feasible to utilize parallel hardware (by increasing batch sizes) efficiently while maintaining training stability. In addition to optimizers, there is growing interest in reinforcement learning and automated tuning to squeeze out further efficiency. One example is using RL or other automated methods to tune hyperparameters (learning rates, batch schedules) or even architectural choices during training. As an illustration, the Zeus system dynamically adjusts the GPU power limit and batch size during training to improve energy efficiency without degrading training time [92, 93, 94]. By formulating the trade-off between power usage and throughput as an optimization problem, techniques like this can save significant energy in large-scale training runs. More broadly, Neural Architecture Search (NAS), often powered by reinforcement learning or evolutionary algorithms, has been used to discover efficient neural network architectures automatically [95]. While NAS has mostly been applied to smaller-scale image or language models, the concept extends to LLMs for instance, using RL-based agents to decide layer widths, depths, or sparsity patterns could yield architectures that outperform human-designed transformers in efficiency. Already, NAS has produced models like EfficientNet [96] in vision by finding better layer shapes for given computation budget. We can envision future foundation models being partially discovered by AI themselves, optimized from the ground up for hardware friendliness. Lastly, reinforcement learning also comes into play in fine-tuning large models via methods like proximal policy optimization in the context of Reinforcement Learning from Human Feedback (RLHF), which, while aimed at alignment and not purely efficiency, does demonstrate the flexibility of training algorithms for these models [97, 98, 99]. In sum, combination of clever optimizer choices, automated tuning of hyperparameters, and even learning-driven architecture optimization contributes to making the training and deployment of large-scale models more efficient than ever before. Each of these algorithmic improvements from better optimizers to learning curricula chips away at the overall resource requirements, enabling the continued scaling of LLMs, LVMs, and VLMs within practical limits."
        },
        {
            "title": "4 Techniques for Improving LLM Efficiency",
            "content": "This section is organized as follows. Section 2 introduces background concepts: the LLM fundamentals (Transformer architectures, training paradigms) and common efficiency evaluation metrics. In Section 3, we discuss budget efficiency through the lens of scaling laws how performance scales with compute, model size, and data, and what trade-offs are optimal. Section 4 covers data efficiency techniques, including data filtering and curriculum learning to get the most out of training data. Section 5 surveys architecture-level innovations such as efficient attention mechanisms, positional encodings, and sparse or attention-free models that reduce the computation per token. Section 6 examines training and tuning efficiency, from distributed training and mixed precision to parameter-efficient fine-tuning methods. Section 7 reviews inference efficiency via model compression (pruning, distillation, quantization, etc.), decoding optimizations, and systems design for serving LLMs."
        },
        {
            "title": "4.1 Dimensions of LLM Efficiency",
            "content": "When we discuss making LLMs more efficient, it is important to define metrics for resource usage: Model Size & Parameters: The number of parameters (and by extension the model file size) is basic metric. It correlates with memory requirements for storage and inference. For instance, 175B parameter model in fp16 occupies 350 GB (2 bytes/param) in memory, whereas 6B parameter model would be 12 GB. Parameter count alone is not perfect proxy for speed, but it is rough measure of model complexity and hardware footprint. FLOPs / Computational Cost: Floating point operations (FLOPs) needed for forward (and backward) pass measure computational workload. For example, generating one token with GPT-3 requires on the order of 2 175B 3.5 1011 FLOPs (since each token involves matrix multiplications proportional to model size) [1]. Training cost can be reported in PF-days (petaflop/s-days) GPT-3s training was about 3,640 PF-days [1]. Efficiency improvements often aim to reduce FLOPs needed for the same task or shift to lower-precision operations. Reducing FLOPs generally translates to faster runtime if hardware is fully utilized. Throughput and Latency: Throughput is how many tokens (or sequences) can be processed per second, and latency is how long it takes to get result. For training, throughput might be measured in examples or tokens per second. For inference, latency per token or per query is key. Techniques like model parallelism might increase throughput but could also introduce communication overhead that affects latency. Real-time applications care about latency (e.g. respond in under 100ms), while batch processing cares about total throughput. Memory Footprint: This includes model weights memory, activation memory during computation, optimizer states during training, and memory for caches. Memory is limiting factor for deploying large models e.g., fitting model on single GPU requires it to have enough VRAM for the model and intermediate activations. Memory-saving techniques (like gradient checkpointing or quantization) allow using less memory at the cost of extra computation or slight accuracy loss. Efficient memory use is also important to avoid waste when serving many requests (see PagedAttention in Section 7, which tackles memory fragmentation [100]). Energy and Carbon Efficiency: Increasingly, researchers track the energy consumed by model training/inference and the associated CO2 emissions [101]. model that achieves the same accuracy with half the energy is more efficient in very tangible sense. Metrics like FLOPs per watt or total kWh for training are used. Strubell et al. [102] famously highlighted that large NLP models can emit as much CO2 as several cars lifetimes. Efficiency methods can dramatically cut down energy usage (e.g., by requiring fewer FLOPs or using specialized hardware better). Reporting carbon impact is becoming good practice. In practice, efficiency gains may trade off between these metrics. For instance, method might reduce memory usage at the cost of more FLOPs, or vice versa. Ultimately, end-to-end improvements (e.g., reducing overall runtime on given hardware budget for given task) are what matter. Throughout this survey, we will note how each technique impacts these metrics. For example, mixture-of-experts models have more parameters but can reduce FLOPs per token by activating only some experts, improving speed at the cost of memory. Quantization reduces memory and may even speed up compute on certain hardware (taking advantage of INT"
        },
        {
            "title": "4 TECHNIQUES FOR IMPROVING LLM EFFICIENCY",
            "content": "tensor cores), with some impact on accuracy. The goal is Pareto-improvement: achieve the same or better model quality for lower cost on one or more of these axes."
        },
        {
            "title": "4.2.1 Scaling Behavior and Power Laws",
            "content": "A natural question in the development of LLMs is how performance improves as we allocate more resources. Scaling laws refer to empirical relationships between model performance (often measured via cross-entropy loss or perplexity) and scale factors like model size, dataset size, or compute. Pioneering work by Kaplan et al. (2020) observed that the loss of language model follows power-law decline as model parameters increase: L(N )  + L, for some constants a, , [103]. Similarly, loss scales as power-law with the amount of training data D. These scaling laws held impressively over seven orders of magnitude in and [103]. Crucially, Kaplan et al. found that within the ranges tested, other architectural details (e.g. width vs. depth of layers) had minimal effect on loss compared to total parameter count [103]. In other words, Transformers performance is largely function of how big it is and how much data it is trained on, and the improvement is predictable and smooth (a log-linear trend on plots). This provided guidepost for building better LLMs: just make them bigger and train on more data, and you will likely get better results. However, scaling up is not free it comes with an increased compute budget requirement. Given fixed compute budget (which roughly scales as for training model of size on tokens), how should one allocate it? Kaplan et al. suggested an answer: larger models are more sample-efficient [103]. They found that to minimize loss for given C, one should train very large model without fully consuming the data, rather than smaller model to convergence [103]. Intuitively, doubling model size and halving training steps led to lower loss than vice versa. This recommendation train huge models for fewer epochs was adopted in early LLMs. For example, GPT-3 was somewhat under-trained (trained on 300B tokens, which is only 2 epochs over its 160B token dataset) according to these heuristics. 4.2.2 Compute-Optimal Model Scaling (Chinchilla vs. Gopher) In 2022, Hoffmann et al. (DeepMind) revisited scaling laws with extended experiments and found that many recent LLMs were significantly under-trained for their size [104]. They introduced the notion of computeoptimal model: for given compute C, there is an optimal pair of (model size) and (tokens) that yields the best performance. Their empirical analysis suggested roughly linear relationship between optimal and in fact, doubling the model size should go along with doubling the training data to stay on the compute-optimal frontier [104]. This is in contrast to the earlier strategy of extremely large with limited data. To validate this, Hoffmann et al. trained Chinchilla, 70B parameter model, on 1.4 trillion tokens, using the same compute as used to train Gopher, 280B model on 300B tokens. The result was striking: Chinchilla (70B) outperformed Gopher (280B) on wide range of downstream tasks [104]. Despite having 4 fewer parameters, Chinchillas extra training data gave it an edge for example, it achieved an average score of 67.5% on the MMLU benchmark, >7% higher than Gopher [104]. It also surpassed other models in that compute class like GPT-3 (175B) and Megatron-Turing NLG (530B) [104]. This revelation prompted re-evaluation in the community: bigger is not always better, if not fed with enough data. smaller model can soak up more data and end up better. Moreover, an added benefit is that Chinchilla-like models are cheaper to fine-tune and faster to inference (since they have fewer parameters) for the same performance level [104]. The concept of compute-optimal scaling can be summarized by the heuristic: scale model size and data in tandem. One way to express the optimal regime is to set proportional to (assuming training compute for given architecture). Under fixed C, this yields an optimal and D. The Chinchilla law suggests : should be about 20 tokens per parameter (in the 2022 study) though that exact ratio may vary. The key is that many previous models like GPT-3 (which had 2 tokens per parameter) were far off this optimum, hence under-utilizing data. With this insight, new models (e.g. LLaMA, see below) have aimed to be more balanced."
        },
        {
            "title": "4.2.3 Data Constraints and Quality",
            "content": "One practical challenge is that simply scaling both and requires massive high-quality datasets. If one is data-constrained, scaling laws can bend or break. For instance, if only 108 tokens of domain-specific text exist, making the model larger than certain point yields diminishing returns because it will quickly saturate the available data (and start overfitting). In such regimes, one might in fact prefer smaller model or use heavy regularization and reuse data with careful curriculum. Empirically, when data is the bottleneck, the performance gains will flatten out no matter how much compute you throw with more parameters [104]. This scenario has led researchers to focus on data quality and curation to get more effective data for the model to consume. For example, using diverse sources and cleaning duplicates helps avoid wasted capacity on redundant or low-value text. Interestingly, improvements in data quality can sometimes substitute for sheer quantity. The LLaMA models by Meta (2023) demonstrated that by curating high-quality mix of public data and training somewhat past the earlier optimal point, 13B model could outperform GPT-3 175B on most benchmarks [105]. LLaMA-13B was trained on 1T tokens (slightly more than Chinchillas recommended 13B 20 = 260B, so it over-trained relative to compute-optimal), yet its performance benefited from the high quality data and possibly better training efficiency. This hints that the constants in scaling laws depend on data quality better data gives lower loss for the same size. Thus, another dimension of efficiency is maximizing what the model learns per token of data. We will cover data filtering techniques in Section 4. Moreover, when models are scaled up, they often unlock capabilities rather than just monotonically improving single metric. For example, very large models can do multi-step reasoning or understand nuanced instructions (emergent behaviors) that smaller models cannot [101]. These binary capabilities (has/has not) complicate the smooth scaling picture. Recent studies (e.g. Wei et al. 2022 on emergent abilities) show that some tasks suddenly become solvable once the model crosses size threshold [101]. Such breaks in scaling trends mean that beyond certain point, scaling might yield discontinuous leaps in what the model can do. 4.2.4 Open Problems in Scaling Broken Scaling and Out-of-Distribution Generalization: While scaling laws hold remarkably well on the training distribution (and near-range evaluations), they can break when extrapolating. For instance, model might follow power-law on perplexity but fail to improve on certain logical reasoning task until it reaches large size. Understanding these deviations is ongoing work. Some researchers propose multi-faceted scaling laws that incorporate additional factors (like knowledge composition or reasoning depth) to predict performance; others have introduced evaluation scaling laws to estimate how performance on downstream tasks scales. challenge is we do not have complete theory of why power-laws emerge; it may relate to the underlying data distribution and model capacity being used effectively. When scaling further (e.g. to trillion-parameter models), will new phenomena occur or will the gains saturate? Recent evidence from models at GPT-4 scale suggests that scaling alone is not enough for example, GPT-4 likely owes some improvements to architecture and training technique, not just size. Architecture Shape (Depth vs Width): Kaplan et al. noted little effect of depth vs width within reasonable ranges [103]. Yet, as we push models to extreme depths (hundreds or thousands of layers), training becomes unstable. Techniques like DeepNorm [106] allow 1000-layer Transformers by adjusting residual scaling. It remains an open question whether very deep narrow model could outperform shallow wide model of the same parameter count when properly trained. In theory, depth could give more representational power, but optimization issues might negate that. So far, empirical evidence indicates that for equal parameter count, there is broad plateau of depth-vs-width configurations that perform similarly [103]. Very extreme aspect ratios (too deep and narrow) underperform due to difficulty in training. Thus, architecture interplay with scaling is subtle most large LMs keep depth around 4080 layers and increase width (dmodel) for larger sizes, heuristic that has worked. In summary, scaling laws provide north star for guiding efficient use of compute budget: use as much data as possible and right-size the model to that data. The era of blindly increasing parameter count is over instead, we aim for scaling balanced with data. The following sections (47) can be seen as methods to improve or refine the scaling curves achieving on-par performance with fewer parameters (through data or"
        },
        {
            "title": "4 TECHNIQUES FOR IMPROVING LLM EFFICIENCY",
            "content": "architecture efficiency), or reaching target performance with less compute (through better training algorithms and inference optimizations)."
        },
        {
            "title": "4.3 Data Efficiency",
            "content": "Training an LLM often involves hundreds of billions of tokens of text. Collecting and processing such data is costly in terms of time, storage, and even intellectual property concerns. Data efficiency refers to techniques that extract maximum performance from given amount of data or alternatively, achieve target performance with significantly less data. This is crucial when data is limited (e.g. specialized domains) or expensive to curate/label. Two major strategies are data filtering to improve quality and curriculum learning to optimize the order in which data is used."
        },
        {
            "title": "4.3.1 Importance of Data Quality and Filtering",
            "content": "Not all data are equal. Web-scale corpora contain duplicates, spam, and low-quality text that can waste training capacity or even harm the model (learning bad facts or biases). Data filtering methods aim to curate higher-quality training set without dramatically reducing its diversity. One straightforward but effective technique is deduplication removing duplicate or near-duplicate examples. Even though web scrapes are huge, they often contain many repeated texts (news articles copied on multiple sites, boilerplate templates, etc.). Deduplicating the dataset can reduce its size and also improve generalization. Lee et al. (2021) showed that deduplication allowed models to reach the same validation loss in 10 fewer steps in some cases [101]. Intuitively, the model does not waste time memorizing the exact same content repeatedly. Common approaches use hashing (e.g. MinHashLSH) to identify duplicates efficiently. Projects like CC-Net use clustering and hashing to clean Common Crawl data, while adversarial filtering [107] can remove machine-generated or undesirable text. Another filtering axis is data selection / undersampling. If certain portions of data are less useful, we can sample them less or drop them. For example, when mixing diverse sources (Wikipedia, books, web), one might undersample the largest but lowest-quality source to ensure the model does not get overwhelmed by it. Instance-based importance sampling can go further ranking individual examples by some score of utility. Recent work explores filtering out examples that are too easy or too hard for the model at its current stage. One approach is loss-based filtering: if the model (or smaller proxy model) already assigns very low loss to an example, that example might not teach it much new. Jiang et al. (2019) proposed Selective Backpropagation, where they only backpropagate on examples with high loss. This yielded faster convergence by focusing compute on the mistakes the model was making. Similarly, gradient-based sampling picks examples with the largest gradient norms, which indicate the example has big effect on parameters and might be more informative [101]. Katharopoulos & Fleuret (2018) developed an importance sampling scheme based on an upper bound of gradient norm [108], and others have implemented online sample selection using proxy models. One must be careful that filtering does not overly skew the data distribution. Strategies like random undersampling of over-represented classes [109] have shown that dropping redundant data can both reduce training time and improve balance. For example, if 90% of the data is English and 10% is other languages, one might downsample English data to ensure the model learns multilingual capability (if that is goal). The MESA approach uses meta-learning to learn how to sample effectively from large dataset, and so forth. The outcome of successful data filtering is leaner corpus where each example has value. This can significantly cut the required number of tokens to reach certain performance, which directly translates to less training compute. 4.3.2 Curriculum Learning While the above deals with which data to use, curriculum learning [110, 111] concerns in what order to present the data to the model. Inspired by how humans learn (starting from easy concepts and progressing to harder ones), curriculum learning for LLMs means we might begin training on simpler patterns and gradually move to more complex ones [101]. The hypothesis is that this guides the models optimization in smoother way, potentially leading to better final performance or faster convergence."
        },
        {
            "title": "4 TECHNIQUES FOR IMPROVING LLM EFFICIENCY",
            "content": "A curriculum requires two components: difficulty metric to rank training examples by complexity, and pacing function that determines how to schedule the introduction of harder examples. Common difficulty metrics in NLP include: (a) sequence length, (b) vocabulary rarity, (c) perplexity/uncertainty according to smaller model. For instance, Zhao et al. (2020) used word rarity as measure, presuming that handling rare words requires more context and understanding. The pacing function can be step-wise or stage-wise. neat example of stage-wise curriculum is Shortformer [112], which trained Transformer first on short sequences only, then in second stage allowed long sequences. By doing so, the model first mastered local coherence without being confused by long-range dependencies, and then could leverage that foundation to handle long contexts. In general, curricula can be as simple as sorting the training data by length or complexity and always feeding in that order, or as complex as dynamically adjusting sample difficulty based on current model performance (self-paced learning [113]). Applications of curriculum learning in LLMs have included: training small models on code with gradually increasing code length [114, 115, 116], training multilingual models by starting with one language then adding more [117, 118, 119, 120], or starting with syntactically simple sentences then moving to full natural text [121, 122, 123]. One must ensure that eventually the model sees the full distribution of data, otherwise it might become overspecialized. Most curricula therefore converge to training on the mixture of all data at some point. In terms of efficiency, curriculum learning can accelerate convergence the model reaches given loss or accuracy in fewer steps than without curriculum. For very large models, curriculum strategies like Shortformer have proven valuable for stability and speed. As models venture into longer contexts (e.g. 10k+ tokens), curricula could be essential to first handle short contexts then extend, otherwise training from scratch on extremely long sequences might be too difficult. Furthermore, recent works have explored curriculum learning strategies to enhance the reasoning abilities of LLMs through reinforcement learning (RL). Approaches such as DeepSeek-R1 [124] and Kimi k1.5 [125] adopt RL fine-tuning methods that progressively expose the model to tasks of increasing difficulty. In these systems, the training is initiated with simpler reasoning tasks, and as the model performance improves, more challenging tasks are introduced. Additional research has proposed alternative curriculum designs. For example, WISDOM [126] leverages progressive curriculum data synthesis to improve the models performance on mathematical reasoning tasks. Similarly, LBS3 [127] utilizes curriculum-inspired prompting, guiding the model through sequence of intermediate sub-problems before addressing the primary task. CurLLMReasoner [128] and Logic-RL [129] further illustrate how curricula can be designed to integrate structured reasoning and logical puzzles into the RL framework. Finally, AlphaLLM-CPL [130] introduces dynamic curriculum adjustment mechanism that combines Monte Carlo Tree Search (MCTS) with curriculum preference learning (CPL) to refine reasoning capabilities progressively. 4.3.3 Data Augmentation and Synthetic Data Another approach to data efficiency is creating more data in smart way. Techniques like back-translation (in MT) and self-instruct (for instruction tuning) use models themselves to generate new training examples. For example, the Self-Instruct framework had GPT-3 generate its own instructions and responses to teach itself to follow instructions better. This bootstrap approach greatly reduced the need for human-written prompts. In LLM fine-tuning, one might generate paraphrases of small dataset to expand it. While augmented data may be of lower quality than real data, if the model can still learn from it, it can help squeeze more out of limited original data. Data augmentation blurs into the territory of knowledge distillation (where models outputs supervise another), which we revisit in Section 7. In summary, data efficiency techniques aim to maximize the knowledge gained per token of training data. By curating high-quality, diverse corpora (filtering out noise and redundancy) and feeding data in an optimal order, we reduce the total data needed. This directly saves computation and allows smaller-scale training runs to still achieve strong performance. As model training budgets are enormous, even 10% efficiency gain in data usage can mean millions of dollars saved or the difference between needing 1B vs 1.1B tokens to reach milestone. Data efficiency is thus critical piece of the LLM efficiency puzzle, complementary to architectural and algorithmic innovations."
        },
        {
            "title": "4.4 Architecture Efficiency",
            "content": "The Transformer architecture, while powerful, has some well-known efficiency bottlenecks notably the quadratic complexity of self-attention with respect to sequence length. Architectural efficiency improvements seek to redesign parts of the model to reduce computation or memory usage per token, without losing (much) performance. In this section, we discuss several fronts: efficient attention mechanisms, improved positional encodings, models that leverage sparsity, and even alternatives to attention entirely."
        },
        {
            "title": "4.4.1 Motivation: Rethinking the Transformer for Efficiency",
            "content": "A standard Transformer processes sequence of length with self-attention that scales as O(L2 d) and feed-forward layers that scale as O(L d2). For very long inputs (e.g. documents of thousands of tokens), attention becomes the dominant cost due to the L2 term. The question is: can we maintain the modeling power of Transformers while cutting down the attention cost to linear or near-linear in L? At the same time, hardware-aware optimizations ask: can we implement attention in way that uses memory/cache more efficiently? 4.4.2 Efficient Attention Mechanisms Sparse and Factorized Attention: One approach is to restrict the attention computation to subset of token pairs, making the attention matrix sparse. The Sparse Transformer [131] did this by attending only to fixed pattern of positions. Longformer [132] and Big Bird [133] introduced combinations of local attention (each token attends to window of nearby tokens) and global attention (a few tokens attend broadly). Big Bird achieved linear complexity and even proved that such patterns are Turing-complete. Another line is factorizing attention via low-rank approximation. Linformer [134] hypothesized the attention matrix has low rank, projecting keys/values to lower dimension. Nystrmformer [135] and Performer [136] similarly used approximate or kernel-based approaches to reduce attention to linear or O(L log L) complexity. Reformer [137] used LSH to group tokens that have similar keys, achieving O(L log L). IO-Aware and Hardware-Friendly Attention: complementary angle is to optimize how we implement attention. FlashAttention [138] keeps exact full-attention but reorders computation and memory access to minimize reads/writes to slow memory. By computing attention in blocks that fit into on-chip SRAM, it significantly speeds up large context processing. This is an IO-centric algorithmic approach. FlashAttention-2 refines these ideas further. These techniques do not change the Transformer math but yield large speedups in practice by alleviating memory bottlenecks. MQA: Multi-Query Attention (MQA) modifies standard multi-head attention by sharing the key and value projections across all heads while keeping the query projections distinct. In standard multi-head attention, for each head one computes headh = Attention(cid:0)QW h , KW , h (cid:1), with the attention function defined as Attention(Q, K, ) = softmax (cid:18) QK dk (cid:19) V. In MQA, although the query projection QW among all heads: remains unique to each head, the keys and values are shared headh = Attention(cid:0)QW , KW K, (cid:1). This design reduces both the computational load and memory requirements, particularly during inference, as the keyvalue cache is computed only once for all heads. MQA thus strikes balance between full multi-head attention and more extreme sharing schemes. Grouped Query Attention (GQA): Grouped Query Attention (GQA) refines standard multi-head attention by partitioning the query heads into groups, so that each group shares single keyvalue pair. In the standard approach, each head computes headh = Attention(cid:0)QW , KW , h (cid:1)."
        },
        {
            "title": "4 TECHNIQUES FOR IMPROVING LLM EFFICIENCY",
            "content": "When the total heads are divided into groups of size = H/G, for any head in group the key and value projections become shared: headh = Attention(cid:0)QW , KW , i (cid:1). This approach interpolates between full multi-head attention (when = H) and multi-query attention (when = 1), providing tunable trade-off between expressiveness and efficiency. Multi-Head Latent Attention (MLA): Multi-Head Latent Attention (MLA) addresses the memory bottleneck by compressing the keyvalue (KV) cache using low-rank latent representation. Instead of computing full keys and values for each head, the input token ht Rd is first projected into lower-dimensional latent vector: = htW DKV , DKV Rddc, cKV Then, for each head i, the full key and value vectors are reconstructed using up-projection matrices: dc d. = cKV ki K , = htW t = cKV vi V , i , i Rdcdh. The query is computed as qi . This factorization dramatically reduces the size of the KV cache, lowering memory usage while preserving the models capacity. MLA is particularly beneficial during inference, as the compressed latent representation can be cached and the keys and values computed on the fly. Native Sparse Attention (NSA): Native Sparse Attention (NSA) reduces computational burden by decomposing the attention operation into three branches. First, compression branch aggregates sequential tokens into coarse global summary. Second, selection branch computes importance scorestypically via softmax over intermediate scoresto select the most relevant token blocks. Third, sliding window branch preserves local context by applying full attention within fixed window. For each query qt, NSA computes the output as Attn(cid:0)qt, kcmp = gcmp where the gating coefficients gc [0, 1] (for {cmp, slc, win}) are learned functions that determine the contribution of each branch based on the context. This hierarchical design is both end-to-end trainable and efficient for long-context scenarios. Attn(cid:0)qt, kwin Attn(cid:0)qt, kslc (cid:1) + gwin (cid:1) + gslc , vcmp , vwin , vslc (cid:1), MoBA: MoBA (Mixture of Block Attention) adapts the standard attention mechanism to process long sequences more efficiently by operating on blocks of tokens rather than on the entire sequence. Given sequence of tokens, MoBA first partitions the sequence into blocks, each of size ="
        },
        {
            "title": "N\nn",
            "content": ", with the i-th block defined by the indices Ii = {(i 1)B + 1, . . . , iB}. For each query token q, gating network computes an affinity score si for each block as the inner product between and summary representation of the keys in block (typically, the mean of the keys): si = (cid:10)q, mean(cid:0)K[Ii](cid:1)(cid:11) . top-k selection is then applied, so that only the blocks with the highest scores are selected. Formally, gate value gi is assigned to each block as (cid:40) gi = 1, 0, otherwise. if si is among the top-k scores, The overall set of indices used for attention is = (cid:91) Ii. i:gi= Finally, the attention is computed over the selected keys and values: MoBA(q, K, ) = softmax (cid:16) qK[I](cid:17) [I]."
        },
        {
            "title": "4 TECHNIQUES FOR IMPROVING LLM EFFICIENCY",
            "content": "To preserve causality in autoregressive models, MoBA prevents query token from attending to tokens in future blocks by assigning score of (or equivalently, gate value of 0) to any block that comes after the query. Additionally, within the current block, causal mask ensures that each token only attends to preceding tokens. This strategy reduces the computational cost by limiting the number of tokens processed per query while dynamically selecting the most relevant blocks, thereby providing an effective trade-off between efficiency and expressiveness without changing the overall parameter count. Overall, these techniques offer distinct strategies to reduce the memory and computational demands of attention mechanisms while preserving performance, marking significant advances in the efficiency and scalability of LLMs."
        },
        {
            "title": "4.4.3 Efficient Positional Encoding",
            "content": "The processing of extended sequences poses significant challenges for LLMs. Traditional absolute positional encoding (APE) from the original Transformer architecture [139] proves inadequate for handling lengthy inputs. To overcome this constraint, researchers have developed innovative positional encoding (PE) strategies that effectively accommodate longer sequences through relative positioning [140, 141, 142, 143], rotary embeddings [144, 145], randomized encodings [146], or even by eliminating positional encoding entirely [147]. This section examines cutting-edge developments in positional encoding that enhance model efficiency and capability. Addition-Based Relative Positional Encoding Frameworks. Unlike absolute encoding schemes, relative positional encoding methods track relationships between token pairs rather than assigning fixed positions. Several frameworks employ this approach by incorporating encoded relative positions directly into attention calculations. Notable implementations include T5 [148], TISA [149], and FIRE [143]. T5 [148] implements bucket-based approach, converting positional differences into scalar bias values through lookup mechanism. This method facilitates some length extrapolation by assigning identical embeddings to all positions beyond the training distribution, though at the cost of increased computational overhead. TISA [149] advances this concept by deploying trainable Gaussian kernel specifically focused on inter-token positional differences. FIRE [143], developed by Li et al., introduces progressive interpolation using normalized position indices. This normalization is achieved by dividing the positional difference between tokens by the query tokens index (i.e., the larger index in causal attention, i, for query at position and key at position j, the normalized distance is (i j)/i). This approach not only generalizes but effectively unifies previous relative encoding methods, capable of theoretically recovering both T5s RPE and ALiBi as special cases. Empirical evidence demonstrates FIREs superior generalization capabilities for extended contexts in language modeling benchmarks. These relative encoding approaches fundamentally enhance model comprehension of token relationships while enabling length extrapolationcritical for processing diverse and intricate sequences. Decay-Function Approaches to Relative Positioning. Another significant innovation involves utilizing decay functions within relative positional encodings to emphasize local context. Systems like ALiBi [140], KERPLE [141], and Sandwich [142] employ this methodology to gradually diminish attention as the distance between tokens increases. ALiBi introduces fixed linear decay function that helps Transformers generalize to extended sequences by imposing monotonic decay pattern on attention scores. This enables extrapolation beyond the training length with minimal performance loss by biasing attention towards recent tokens, though the linear penalty means very distant tokens contribute negligibly, implicitly constraining the effective receptive field. While this enhances length extrapolation, ALiBi can potentially affect performance on in-distribution data. KERPLE [141] refines this approach based on kernel theory, introducing trainable decay RPE with two variants of conditionally positive definite (CPD) kernels: logarithmic and power variants. These sophisticated kernels, featuring learnable parameters per head, adaptively modulate the connection strength between token pairs during RPE computation, achieving excellent extrapolation."
        },
        {
            "title": "4 TECHNIQUES FOR IMPROVING LLM EFFICIENCY",
            "content": "Sandwich [142], named for its conceptual approach of sandwiching useful low-frequency decay while discarding mid-frequency oscillations, is parameter-free RPE derived from sinusoidal absolute PE by removing oscillatory cross-terms. This results in relative bias matrix that decays with distance, similar to ALiBi, and leverages positions beyond the training range. These decay-based methods collectively ensure that models maintain focus on contextually relevant nearby tokens while still retaining capacity to process longer sequences. Rotary Positional Encoding and Recent Advances. Moving beyond addition-based methods, rotary positional encoding (RoPE) [144] has emerged as dominant approach in modern LLMs. Rather than adding position information, RoPE injects positional context by applying rotation matrices to query and key vectors, with rotation angles proportional to token positions. However, standard RoPE struggles with length extrapolation beyond its training range and exhibits an implicit long-term decay effect due to its high-frequency components. Contrary to common belief, recent analysis by Barbero et al. (2025) challenges the assumption that RoPEs effectiveness stems primarily from enabling decay in long-range attention. Their examination of trained 7B parameter model reveals that the highest-frequency components in RoPE actually create precise positional attention, while lower-frequency components inadvertently carry semantic information. This discovery suggests opportunities for targeted optimization of frequency components within RoPE. Recent years have seen contradictory yet equally effective approaches to modifying RoPE. HoPE [150] (Highfrequency rotary Position Encoding), recent proposal (late 2024), challenges the long-held assumption that long-term decay benefits attention. Chen et al. (2024) observed that modern Transformers naturally develop \"U-shaped\" attention pattern where attention decays for distant tokens only beyond certain threshold, rather than continuously. HoPE strategically removes low-frequency components from RoPE that impose unnecessary decay constraints, replacing them with position-independent signals while preserving high-frequency positional information. This reformulation dramatically improves in-context retrieval capabilities and length extrapolation performance, though its claims on extrapolation may be task-specific and await broader confirmation. In stark contrast, Sun et al. (2023) introduced xPOS (Extrapolatable Position Embedding), which explicitly incorporates carefully calibrated exponential decay factor into RoPEs rotation matrix. This controlled decay mechanism stabilizes attention for extraordinarily long sequences. When implemented within their LEX Transformer architecture (which also employs blockwise causal attention), xPOS enabled training on relatively short contexts while maintaining impressive perplexity scores when evaluated on sequences considerably longer than those encountered during training. Another significant advancement, 3D-RPE [151], extends RoPE from two dimensions to three-dimensional spherical representation inspired by quantum computings Bloch Sphere, involving segmentation of sequences into chunks and encoding both intra-chunk and inter-chunk positions. This approach offers dual advantages: customizable long-term decay characteristics and enhanced position resolution. The 3D representation mitigates position resolution degradation commonly encountered during RoPE interpolation, yielding performance gains particularly for long-context natural language understanding tasks. Earlier innovations like Position Interpolation (PI) [152], post-hoc RoPE rescaling technique, demonstrated that moderate fine-tuning could enable handling of extensive context windows, albeit with potential slight performance degradation on very long inputs compared to models trained from scratch on those lengthsa practical trade-off for extensibility. Similarly, YaRN [145] introduced NTK-aware interpolation techniques, which employ uneven frequency scaling to preserve high-frequency RoPE components crucial for local order. While not adding learned parameters, YaRN involves specific rescaling schedule, and it substantially improves context size adaptability without requiring comprehensive retraining. Alternative Positional Encoding Paradigms. Beyond relative and rotary approaches, researchers have explored fundamentally different paradigms for position encoding, including randomized methods, mathematical reformulations, and even the elimination of positional encoding altogether. Randomized Positional Encoding [146] addresses critical limitation of conventional methods: the out-ofdistribution problem when encountering positions beyond training length. Ruoss et al. (2023) demonstrated that this failure mode directly connects to positional encoding limitations. Their solution involves sampling extended position values and randomly subsampling them for each training sequence, effectively simulating"
        },
        {
            "title": "4 TECHNIQUES FOR IMPROVING LLM EFFICIENCY",
            "content": "longer sequences within shorter context windows. In comprehensive evaluations across 15 algorithmic tasks involving 6,000 transformer models, this stochastic approach dramatically improved length-generalization performancedelivering average accuracy improvements of 12% (reaching 43% on some tasks) without compromising in-distribution performance, though it may potentially disrupt local sentence structures by exaggerating dependency lengths. Meanwhile, NoPE [147] takes the radical approach of eliminating positional encoders entirely from selfattention mechanisms, particularly in decoder-only models. This research demonstrates that transformer self-attention, within such architectures and on certain algorithmic tasks, can inherently learn relative positional relationships between tokens without explicit encoding. This streamlined approach yields impressive generalization capabilities, particularly for inputs extending beyond training distribution lengths. Recent mathematical innovations have introduced alternative foundations for positional encoding. PoPE [153] employs Legendre orthogonal polynomials as basis functions, offering advantages including improved correlation structure, non-periodicity, orthogonality, and distinctive functional forms across polynomial orders. While tested primarily on modest-scale tasks like translation and not specifically focused on LLM-scale length extrapolation in its initial proposal, empirical results show PoPE-equipped transformers outperforming baseline models on these benchmarks while achieving faster convergence rates. Algebraic Positional Encodings [154] provide flexible framework to derive PEs from algebraic domain specifications for various data structures (sequences, grids, trees), preserving their mathematical properties as orthogonal operators. This approach, validated on relatively smaller benchmarks, has shown performance on par with or better than state-of-the-art PEs without extensive tuning. The Wavelet-based Positional Representation [155] reinterprets RoPE as restricted wavelet transform using Haar-like wavelets with fixed scale parametersa limitation explaining RoPEs extrapolation challenges. By combining relative-position wavelet bias with multiple scale windows, this method captures varied scale representations through wavelet transforms without restricting attention fields. This improves both short and long context performance while enabling superior position extrapolation. 4.4.4 Sparse Modeling via Mixture-of-Experts Recent advances in Mixture-of-Experts (MoE) architectures have focused on addressing key challenges in efficiency, scalability, and expert utilization. significant breakthrough came with the Dense Training, Sparse Inference (DS-MoE) framework [156], which challenges the traditional sparse training paradigm by employing dense computation during training while maintaining sparsity at inference time. This approach has shown remarkable results, activating only 30-40% of model parameters during inference while maintaining performance comparable to dense models. Similarly, the Merging Experts into One (MEO) technique [157] takes different approach to efficiency by consolidating multiple experts capabilities into more compact form, achieving significant FLOPs reduction compared to traditional MoE implementations. Token Processing and Expert Interaction. The Multi-Head MoE (MH-MoE) approach [158, 159] introduces novel mechanism where tokens are split into multiple sub-tokens and processed by different experts in parallel. This parallel processing enables the model to capture diverse representation spaces while maintaining computational efficiency. The adaptive gating mechanism [160] moves away from fixed expert assignments, allowing tokens to be processed by varying numbers of experts based on their linguistic complexity. Taking the dynamic computation concept further, the Mixture-of-Depths approach [161] introduces adaptivity in the computational depth, optimizing how different sequence positions utilize model resources. Implementation and Hardware Optimization. Implementation efficiency has become another crucial focus area. ScatterMoE [162] represents significant advance in how MoE models are implemented on GPU hardware, addressing memory and computational bottlenecks through careful management of padding and data movement. These practical improvements have made MoE models more viable for real-world applications. Routing Mechanisms and Specialization. Empirical studies [163] have revealed that token-level and sequence-level routing strategies exhibit different strengths and specialization patterns. Token-level routing tends to develop syntactic specialization [164], while sequence-level routing shows stronger affinity for topicspecific expertise. Novel routing architectures have emerged, including the layerwise recurrent router [165]"
        },
        {
            "title": "4 TECHNIQUES FOR IMPROVING LLM EFFICIENCY",
            "content": "that maintains routing coherence across layers, and even LLM-based routers [166] that leverage large language models for more sophisticated routing decisions. Research has also shown that routing decisions are highly context-sensitive [167], particularly in encoder layers where semantic associations play crucial role. Future Directions. The field continues to push boundaries with approaches like PEER [168], which scales the expert pool to over million specialists through efficient key-based retrieval. These developments suggest that MoE architectures are far from reaching their full potential. As the field matures, the focus is increasingly on finding the right balance between model capacity, computational efficiency, and practical implementation considerations. The diversity of approaches now available allows practitioners to choose MoE architectures that best match their specific requirements, whether prioritizing inference speed, training efficiency, or domain specialization."
        },
        {
            "title": "4.4.5 Attention-Free Alternatives for Sequence Modeling",
            "content": "Recent advances in sequence modeling have sparked interest in alternatives to the traditional transformer architecture, particularly focusing on approaches that avoid the quadratic complexity of self-attention. This section surveys key developments in attention-free architectures, examining their motivations, approaches, and implications for the future of sequence modeling. Core Motivation. While transformers have become the dominant architecture for sequence modeling, their self-attention mechanism incurs O(L2) time and memory complexity with sequence length L. This quadratic scaling poses significant challenges for processing long sequences and efficient deployment. Attention-free alternatives aim to achieve transformer-level expressivity with linear or sub-quadratic complexity, enabling longer context lengths and faster inference. These approaches seek to combine the strengths of transformers (parallel training and high performance) with the advantages of traditional sequence models (linear-time inference, constant memory per step). Recurrent Neural Network Renaissance. Recurrent neural networks offer conceptually appealing alternative to attention, processing sequences step-by-step while maintaining hidden state that can theoretically retain information over arbitrary lengths. While classic RNNs (LSTMs, GRUs) are Turing-complete and scale linearly with sequence length, they historically struggled with training difficulties and limited parallelization. RWKV Architecture. Recent work has reinvented RNNs for modern applications. The RWKV architecture [169] introduces Receptance-Weighted Key-Value mechanism that enables parallel training similar to transformers while maintaining efficient RNN-style inference. This approach achieves linear complexity O(L) in sequence length and demonstrates competitive performance with similarly sized transformers at the impressive scale of 14B parameters. The architecture successfully bridges the gap between traditional RNNs and modern transformer capabilities. Linear Recurrent Units. The Linear Recurrent Unit (LRU) [170] represents another significant advancement in RNN design. By employing linearized recurrence without hidden-state nonlinearity and incorporating careful initialization and normalization techniques, LRU demonstrates that properly designed RNNs can match state-of-the-art SSMs on long-range tasks. The architecture achieves this through deep architectures with stable gradient flow, effectively addressing the historical limitations of RNNs. State Space Models. State Space Models (SSMs) represent another promising direction, offering continuoustime generalization of RNNs with efficient implementation. The Structured State Space Sequence Model (S4) [171] introduced breakthrough with its special parameterization enabling efficient FFT-based computation. This innovation allows linear scaling in sequence length for inference and has achieved state-of-the-art results on sequences exceeding 10,000 steps, particularly showing strong performance on audio and time-series tasks. Architectural Evolution. Subsequent developments include S4D with diagonal state matrices and S5 [172], which further simplified the architecture. S5 introduced simplified multi-input, multi-output state model and leveraged parallel scan algorithm for efficient computation. These modifications led to improved performance on long-range tasks while maintaining the computational benefits of the original S4 model."
        },
        {
            "title": "4 TECHNIQUES FOR IMPROVING LLM EFFICIENCY",
            "content": "Mamba Architecture. The Mamba architecture [173] represents significant advancement in the field of SSMs. By introducing selective state-space layers with learned gating for state updates, Mamba achieves linear-time computation while maintaining transformer-level quality. The architecture demonstrates remarkable efficiency, achieving 5 higher generation throughput compared to traditional transformers and effectively modeling sequences up to millions of steps in length. Hybrid and Convolutional Approaches. Several architectures combine elements of different approaches or introduce novel mechanisms. The Hyena model [174] advances the field through implicitly parameterized long convolutions and data-controlled gating mechanisms. This innovative approach achieves sub-quadratic complexity while maintaining strong performance, offering significant speed advantages particularly for long sequences. Retentive Networks. The Retentive Network (RetNet) [175] presents versatile architecture that combines the benefits of different paradigms. It supports parallel training mode for efficient learning while offering recurrent inference mode with O(1) per-token complexity. RetNets ability to process long sequences through chunkwise processing, while maintaining competitive performance with transformers, makes it promising direction for future development. Future Directions and Challenges. While attention-free alternatives show significant promise, several key challenges remain to be addressed. The field must tackle the challenge of scaling these models to very large sizes (10-100B parameters) while maintaining stability for extremely long sequences. Supporting modern NLP capabilities such as prompting and in-context learning remains crucial, as does optimizing implementation efficiency across different hardware platforms. Looking forward, the field presents several exciting research directions. The development of hybrid architectures that combine multiple approaches shows particular promise, as does the theoretical analysis of expressivity and stability in these new models. Hardware-specific optimizations and novel applications leveraging linear-time processing capabilities will likely drive further innovation. The development of attention-free architectures represents significant step toward more efficient and scalable sequence modeling, potentially enabling applications beyond the reach of traditional transformers. 4.5 Training and Tuning Efficiency Even with well-designed model and data, training LLMs is among the most resource-intensive procedures in AI. This section examines techniques to speed up and scale the training process (via mixed precision, parallelism, memory optimizations) and to fine-tune large models with minimal overhead (parameter-efficient fine-tuning). 4.5.1 Scalable Training Strategies Stable optimization for scale: As models grow deeper, training can become unstable. DeepNorm [106] scales residual connections properly to allow 1000-layer Transformers without divergence. Pre-LN architectures are also more stable than post-LN. Gradient clipping helps avoid exploding gradients at high batch sizes. Mixed Precision Training: Using half-precision (FP16 or bfloat16) significantly speeds up training on tensor-core hardware [176]. The standard is automatic mixed precision (AMP), which stores master copy in FP32 but does most math in FP16. This roughly halves memory usage and can double throughput with negligible accuracy loss. FP8 is on the horizon for further gains. Parallelism (Data, Model, Pipeline): LLMs typically require multi-GPU or multi-node setups. Data parallelism (DP) duplicates the model on each GPU and trains on different mini-batches, then synchronizes gradients. This is straightforward but memory-heavy if the model is huge. Model parallelism (tensor or pipeline) partitions the models parameters/layers across GPUs [177, 178, 179]. Large weight matrices can be split among devices (tensor parallel), or different layers can be assigned to different devices (pipeline parallel). ZeRO [180] partitions optimizer states and gradients across GPUs, so each only stores slice of them, enabling training of trillion-parameter models by spreading memory load. This is implemented in DeepSpeed and FSDP in PyTorch. Gradient checkpointing saves memory by discarding intermediate activations and recomputing them on the backward pass. These and other techniques combine so we can scale to thousands of GPUs with near-linear speedups."
        },
        {
            "title": "4.5.2 Parameter-Efficient Fine-Tuning (PEFT)",
            "content": "Fine-tuning all the parameters of large pre-trained model for each new task can be prohibitively expensive in terms of compute and storage. Parameter-Efficient Fine-Tuning (PEFT) methods address this problem by updating only small fraction of the models parameters or by introducing few lightweight modules, while keeping most of the model fixed [181]. This dramatically reduces the resources required for fine-tuning, yet many PEFT techniques can achieve performance close to that of fully fine-tuned models. In what follows, we outline several categories of PEFT approaches (following the taxonomy of [181]): additive methods, selective methods, reparameterization-based methods, and hybrid approaches. Additive Fine-Tuning Approaches. Additive methods introduce additional small trainable components into the model, rather than modifying the original network weights. During training, only these added parameters are updated, which limits the total number of parameters that need to be learned [182, 183]. Two common types of additive PEFT are: (a) inserting adapter layers into the model, and (b) adding learnable prompt vectors (soft prompts). Adapter-based Fine-Tuning. In this approach, small bottleneck layers called adapters are inserted at various points within each Transformer block. For instance, an adapter may consist of down-projection matrix Wdown followed by nonlinearity , then an up-projection Wup, whose output is added to the models hidden representation. Only the adapter weights (Wdown, Wup) are tuned, while the original model weights remain frozen. This technique was originally proposed for transfer learning in NLP and provides significant savings in trainable parameters. Notable extensions include AdapterFusion [182], serial adapter configuration that combines knowledge from multiple adapters, and parallel adapter architectures. For example, the Counter-Interference Adapter for Translation (CIAT) [184] and the Kronecker Adapter (KronA) [30] adopt parallel adapter design, adding side network alongside each Transformer layer instead of inserting adapters sequentially. Another variant is CoDA (Conditional Adapters) [185], which also uses parallel adapters but employs sparse activation mechanism to improve inference efficiency by activating only subset of adapter parameters per input. Soft Prompt-based Fine-Tuning. Another additive strategy is to prepend or append learnable prompt vectors to the models input or to hidden states, rather than changing internal layers. These soft prompts are continuous embeddings trained to guide the model toward the downstream task. In prefix-tuning [183], set of trainable prefix vectors is prepended to the keys and values at each self-attention layer; after training, only these prefix embeddings are needed for inference. An improved variant, P-Tuning v2 [186], removes certain reparameterization tricks and demonstrates that prompt tuning can be as effective as full fine-tuning across various scales and tasks. Extensions include SPoT (Soft Prompt Transfer) [187], which transfers prompts learned on high-resource tasks to low-resource ones, PTP [188] with perturbation-based regularization, and mixture-of-prompts methods such as [189], which train multiple small prompt vectors and learn to route each input to the appropriate prompt via gating mechanism. These methods enhance prompt-based fine-tunings flexibility and robustness. Selective Fine-Tuning Approaches. Selective PEFT methods do not introduce new modules; instead, they fine-tune carefully chosen subset of the existing model parameters while keeping the rest frozen. By tuning only the most important or relevant weights, these approaches reduce the number of trainable parameters and help avoid overfitting. Two broad strategies exist: unstructured and structured parameter selection. Unstructured Masking. Unstructured approaches learn binary mask over the models parameters to decide which weights to update. The mask can be arbitrary, aiming to choose individual weights that are most crucial. DiffPruning [190] is an early example that learns differentiable binary mask on each weight, with an L0-norm penalty encouraging sparsity. Other work selects weights based on information measures: FishMask [191] calculates an approximation of the Fisher information per parameter, fine-tuning only the top-k. dynamic variant updates the Fisher-based mask iteratively [192], while Fu et al. [193] use second-order sensitivity analysis to identify the most impactful parameters. Another notable approach, Child-Tuning [194], randomly samples subset (a child network) of parameters for training at each iteration, enabling lightweight yet robust fine-tuning procedure."
        },
        {
            "title": "4 TECHNIQUES FOR IMPROVING LLM EFFICIENCY",
            "content": "Structured Masking. In contrast, structured masking techniques select entire vectors, neurons, or layers. DiffPruning [190] supports structured variant (S-DiffPruning) that prunes groups of weights together. FAR [195] clusters each feed-forward layer into nodes and ranks them by 1-norm to decide which nodes to fine-tune. simple structured approach is BitFit [196], which only updates bias terms (a few parameters per layer), yielding strong results on various NLP tasks. Likewise, X-Attention tuning [197] fixes most of the Transformer but updates cross-attention layers in sequence-to-sequence tasks. SPT [198] (Sensitivity-Aware Fine-Tuning) first identifies the most sensitive weight matrices (via first-order Taylor approximation) and then applies an additive PEFT method (like LoRA) only to those parts, effectively combining selective and additive tuning for improved efficiency. Intrinsic Subspace Fine-Tuning. One line of research studies the intrinsic dimensionality of model fine-tuning. Aghajanyan et al. [199] show that large models often have relatively low-dimensional task-specific subspace. By constraining updates to random subspace of only few thousand dimensions, performance can approach that of full fine-tuning, indicating redundancy in parameter updates. Low-Rank Adaptation (LoRA) and Variants. prominent PEFT strategy is Low-Rank Adaptation (LoRA) [28], which freezes the pre-trained weights W0 and introduces trainable low-rank decomposition = AB for task-specific updates, where Rmr, Rrn, and min(m, n). The adapted weight is = W0 + AB. This significantly reduces trainable parameters to r(m + n) and allows merging the update (W ) into W0 after training, eliminating inference overhead [28]. Several recent methods build upon LoRAs foundation. LoRA+ [200] enhances training dynamics by using different learning rates for matrices and B, improving convergence speed and final performance without changing the parameterization. Rank-Stabilized LoRA (rsLoRA) [45] modifies the scaling factor to  = 1/ (instead of the common /r), stabilizing training at higher ranks and enabling better performance tradeoffs. Weight-Decomposed LoRA (DoRA) [44] reformulates the update by decomposing the weight matrix into magnitude and direction components. It updates the direction using LoRA-like structure applied to the normalized pre-trained directions D0, while learning separate magnitude vector n, resulting in = (D0 + AB) diag(n). This separation often leads to improved performance by tackling magnitude and direction updates independently. Principal Singular Vectors Adaptation (PiSSA) [201] initializes the low-rank matrices and using the principal singular vectors and values derived from an SVD of the original weights W0. It trains = AB + R, where is the frozen residual part of W0. This initialization aligns the adaptation with the most significant components of the pre-trained weights, often leading to faster convergence and better results compared to standard LoRA initialization. All these variants typically retain the benefit of zero inference overhead by merging the learned components post-training. Hybrid Approaches. Hybrid PEFT approaches combine ideas from multiple categories, or propose unifying framework for various fine-tuning techniques. For instance, UniPELT [202] integrates adapters, LoRA, and prompts, training gating mechanism to decide which technique to apply. Similarly, He et al. [203] present template that unifies prefix tuning, adapter-based tuning, and other PEFT variants, highlighting continuum of approaches. Another example is LLM-Adapters [204], providing modular toolkit to integrate multiple PEFT methods into LLMs. Some works automate the selection of PEFT configurations through neural architecture search. NOAH [205] and AutoPEFT [206] both build search spaces of prompt, adapter, and low-rank designs, then employ search or optimization methods to identify the best configuration for given task. By exploring different PEFT techniques as hyperparameters, these methods achieve strong results without extensive manual trial-and-error. Overall, PEFT has become vital paradigm for adapting large pre-trained models. By leveraging additional lightweight modules, selecting specific subsets of parameters, reparameterizing the optimization space, or combining these ideas, PEFT enables developers to fine-tune massive models efficiently, making large-scale AI models more deployable and accessible in limited-resource scenarios."
        },
        {
            "title": "4 TECHNIQUES FOR IMPROVING LLM EFFICIENCY",
            "content": "4."
        },
        {
            "title": "Inference Efficiency",
            "content": "Once trained, LLMs must be served to users. Inference efficiency is critical to reducing cost and latency in real-world settings. Methods range from compressing the model itself (pruning, distillation, quantization) to speeding up the decoding process (speculative decoding, efficient KV-cache usage)."
        },
        {
            "title": "4.6.1 Model Compression Techniques",
            "content": "Pruning removes weights or neurons deemed unnecessary [207]. Structured pruning (dropping entire heads/neurons) yields smaller dense model that runs faster on standard hardware. Unstructured pruning creates sparse matrices that need specialized kernels but can reach high sparsity. Recent works like SparseGPT [208] prune LLMs in one-shot with minimal loss. Knowledge Distillation trains smaller student to mimic larger teachers outputs or hidden states [209]. DistilBERT cut 40% of BERT parameters while keeping 97% of its performance. For GPT-like LLMs, the student can replicate the teachers next-token distribution, compressing knowledge into fewer parameters. Quantization reduces numeric precision (e.g. from 16-bit float to 8-bit int or lower). This cuts memory usage by up to 4 and can enable faster int8 operations on GPUs [101]. GPTQ [210] can quantize large LLMs down to 4-bit weights with small accuracy loss. Mixed-precision quantization is widely used at inference time, and advanced approaches handle outlier values carefully. QLoRA [211] even fine-tunes models in 4-bit. Low-Rank Decomposition approximates weight matrices by factors of lower rank (similar to LoRA but for compression). ALBERT [212] factorized BERT embeddings and shared layers, massively reducing parameters. If weight matrices exhibit redundancy, SVD-based factorization can shrink them with minimal performance drop. 4.6.2 Algorithm-Level Inference Optimizations Speculative Decoding [213] speeds up autoregressive generation by letting small draft model propose several tokens, then having the large model verify them in fewer steps. If the large model agrees, those tokens are accepted; if not, partial fallback occurs. This can yield 23 speedups with no quality drop if the draft model is well aligned. Caching and Batch Optimization: Transformers reuse past key/value vectors to avoid recomputing attention over the entire sequence each step. This KV cache approach is standard, though it can become memoryintensive for long outputs. PagedAttention [100] manages KV cache as pages in GPU memory, avoiding fragmentation and allowing dynamic batching of variable-length requests, yielding large throughput gains in multi-user serving scenarios. 4.6.3 System-Level Optimizations and Deployment Concurrent Batching: Serving frameworks like HuggingFace TGI or vLLM [100] dynamically batch multiple requests to keep the GPU fully utilized, significantly improving throughput. They interleave tokens from different requests (with different sequence lengths) in single forward pass, using careful memory management. Distributed Inference: For very large models that cannot fit on single GPU, weights can be sharded across devices (tensor parallel). Pipeline parallel can also be used, though it introduces pipeline bubbles. Model parallelism is typically used only if necessary, since it adds communication overhead. Memory Offloading: If GPU memory is insufficient, some systems offload parts of the model or KV cache to CPU or disk. This slows inference but allows large models to run on limited hardware. Some prefer quantization or distillation to reduce the model size instead. Specialized Hardware and Libraries: GPU vendor libraries (e.g. NVIDIA FasterTransformer) fuse kernels (attention, GeLU, etc.) and offer INT8 or FP8 acceleration. Custom systems like PagedAttention or FlashAttention achieve further speedups. CPU libraries (GGML) with 4-bit or 8-bit quantization can even run smaller LLMs locally. These low-level optimizations, combined with high-level scheduling, can yield large speedups (510) over naive implementations."
        },
        {
            "title": "5 ASSESSMENT",
            "content": "In summary, inference efficiency is where large models meet real-world usage. By compressing the model (pruning, distillation, quantization) and using optimized decoding (speculative approaches, dynamic batching, efficient caching), one can serve LLMs at scale with acceptable latency and cost. This final step completes the spectrum of efficiency methods, allowing practitioners to deploy models that are large in capability but run faster and cheaper in production."
        },
        {
            "title": "5.1 Assessment Principles of EFFICIENTLLM",
            "content": "In this section, we propose several metrics: Average Memory Utilization (AMU), Peak Compute Utilization (PCU), Average Latency (AL), Token Throughput (TT), Sample Throughput (ST), Inference Throughput (IT), Average Energy Consumption (AEC), and Model Compression Rate (MCR). These metrics are specifically designed to address critical limitations inherent in traditional efficiency evaluation metrics, such as FLOPS, parameter count, and raw inference speed [214, 215, 216, 217, 218]. Conventional metrics often fail to capture the dynamic and realistic utilization of hardware resources, thus providing an incomplete picture of efficiency bottlenecks in real-world deployment scenarios. In contrast, our proposed metrics offer several distinct advantages. AMU provides comprehensive view of memory usage fluctuations throughout training and inference, rather than merely peak memory consumption. PCU accurately reflects real-world GPU utilization, overcoming the limitations of theoretical FLOPS-based metrics that neglect communication overhead and synchronization delays. AL explicitly measures responsiveness, which is crucial for latencysensitive applications such as interactive dialogue systems. Furthermore, our throughput metrics (TT, ST, IT) clearly differentiate between pretraining, fine-tuning, and inference scenarios, enabling more precise optimization decisions tailored to specific deployment contexts. AEC quantifies actual energy efficiency, addressing the growing importance of sustainability and operational cost reduction. Lastly, MCR integrates model size reduction with performance retention, providing balanced evaluation of compression techniques. 5.1.1 Computational System Utilization Intricately linked to efficiency, computational system utilization stands out as an essential challenge for AI models, including LMs. It has garnered extensive discussion and scholarly attention [219, 220, 221, 222, 223]. To critically evaluate Deep Learning Models resource optimization and computational efficiency, datasets and benchmarks, such as MLPerf [224], SPEC CPU [225], DeepBench [226], and DAWNBench [227], have been employed in prior works [228]. Some tools also assessed specific aspects of computational efficiency: Horovod Latency Check [229] and MPI [230] explores response time and processing delays; LLMPerf [231] and NeuralSpeed [232] inspect the scalability and hardware adaptability of large models. While latency or training time and model performance remain predominant metrics for evaluating computational efficiency [233, 234, 235, 236, 237], the need for comprehensive hardware utilization evaluation is also recognized, particularly in benchmarks like MLPerf and DAWNBench. However, the challenge of ensuring optimal hardware utilization is compounded by the narrow focus of current evaluations, which often overlook critical factors such as memory bandwidth, device utilization, and throughput. LMs, given their resourceintensive nature, can exhibit suboptimal hardware utilization during both training and inference, leading to increased operational costs for researchers and companies [238, 239]. This misalignment between the focus of benchmarks and the practical need for maximizing computational system utilization highlights gap in current evaluations, making this an ongoing and critical concern for real-world deployments. In this work, we define computational system utilization as the efficient and effective use of hardware resources during both training and inference of LMs. Our assessment of computational system utilization focuses on 1) evaluating memory utilization, which involves the efficient allocation and usage of device memory across different tasks; 2) testing compute utilization, which measures the extent to which available processing units (such as GPUs tensor cores) are fully utilized during operations; 3) analyzing latency, the time taken to complete specific tasks, such as training iterations or inference requests; and 4) examining throughput, evaluating how efficiently input data is moved and processed through memory, storage, and network interfaces."
        },
        {
            "title": "5 ASSESSMENT",
            "content": "Memory Utilization. Limited device memory has become the bottleneck of LMs training, like training of the long context LLM [240]. Many operators in transformer [241], such as frequent reshaping, element-wise addition, and normalization require huge memory units [242]. we propose the Average Memory Utilization (AMU) as key metric for evaluating memory efficiency during model training and inference. The AMU is defined as the ratio of the memory used by the model throughout the entire training process to the total available memory on the device, averaged over time. This metric provides holistic view of memory usage, accounting for fluctuations in memory demand caused by operations like attention mechanisms and normalization layers. The formal definition of AMU is: AM ="
        },
        {
            "title": "1\nT",
            "content": "(cid:90) 0 Memory Used(t) dt (1) Where is the total training time, Memory Used(t) is the memory utilized by the model at time t. higher AMU indicates that the memory is being utilized efficiently across the entire training cycle, avoiding periods of underutilization or memory wastage. In contrast, lower AMU may suggest poor memory management, frequent allocation and deallocation, or unnecessary memory overhead. Compute Utilization. In large-scale deep learning training, GPU utilization directly impacts both training efficiency and energy consumption. Traditional metrics such as theoretical FLOPS often fail to capture real-world inefficiencies arising from communication overhead, synchronization delays, memory bottlenecks, and suboptimal parallelization strategies. Therefore, we introduce the Peak Compute Utilization (PCU) metric, defined as the ratio of actual GPU utilization to the theoretical maximum GPU utilization, averaged over the training process. PCU provides practical and realistic measure of hardware efficiency, explicitly reflecting how effectively computational resources are utilized during training. Achieving optimal compute utilization entails minimizing idle time for processing units, reducing the load imbalance across compute cores, and maintaining high operational throughput across all computational components. However, sustained utilization of this peak performance is critical challenge, especially when scaling to many-core systems. High compute utilization in large-scale deep learning systems must be maintained across the entirety of wide range of deep learning networks [243, 244]. We propose the metric Peak Compute Utilization (PCU), defined as the ratio of actual GPU utilization (measured as the percentage of GPU compute resources actively engaged in computation) to the theoretical maximum GPU utilization, averaged over the training process. The PCU metric is mathematically expressed as: CU ="
        },
        {
            "title": "1\nT",
            "content": "(cid:90) 0 Actual GPU Utilization(t) Peak GPU Utilization dt (2) Where represents the total training time, Actual GPU Utilization(t) is the measured GPU utilization percentage at time t, and Peak GPU Utilization refers to the theoretical maximum GPU utilization (typically 100%). Latency. Latency plays crucial role in both training and inference efficiency, particularly when dealing with large-scale deep learning models like LLMs. Latency refers to the time delay between input and response, directly affecting the overall responsiveness of AI systems. In training, latency can be influenced by factors such as model complexity, data transfer speed, and communication overhead between distributed nodes. During inference, especially in real-time applications, high latency may hinder performance and user experience, making it vital metric for system optimization [219, 245, 246]. We propose the metric Average Latency (AL), defined as the mean time taken to complete single iteration of training or an inference request, averaged over the entire process. The formal definition of AL is: AL = (cid:80)N i=1(Computation Timei + Communication Timei) (3) In our empirical experiments, we observed that GPU utilization consistently remains above 99% during pretraining and within the narrow range of 80%-81% during inference, indicating negligible variance in compute efficiency for these phases. Consequently, we limit our PCU metric evaluation specifically to scenarios involving parameter-efficient fine-tuning, where meaningful differences in GPU utilization are apparent and thus critical for efficiency analysis."
        },
        {
            "title": "5 ASSESSMENT",
            "content": "where represents the total number of iterations or inference requests, Computation Timei is the time taken to computation the ith iteration/request, and Communication Timei is the time spent in data transfer or communication overhead during the ith iteration/request. lower AL reflects better system efficiency and responsiveness, indicating that the model and hardware are optimized to reduce unnecessary delays in both computation and communication. Higher latency, on the other hand, suggests potential bottlenecks in communication, I/O operations, or inefficient computation scheduling [247, 248]. Throughput. Throughput is key metric for evaluating how efficiently data is processed during training and inference. It refers to the rate at which data is transferred, processed, and output by the system. High throughput ensures full utilization of computational resources and prevents delays from inefficient data handling [248, 249]. Throughput can vary significantly with model size and complexity. Larger models require more computational resources for processing, making direct comparisons between models challenging. To standardize throughput evaluation across different model sizes, we propose three distinct normalized metrics: Token Throughput (TT) for pretraining scenarios, defined as the number of tokens processed per second per parameter. Formally: where Tokens Processedi is the number of tokens processed in the ith iteration. Sample Throughput (ST) for fine-tuning scenarios, defined as the number of samples processed per second per parameter. Formally: where Samples Processedi is the number of samples or dialogues processed in the ith iteration. Inference Throughput (IT) for inference scenarios, defined as the number of tokens generated per second. Formally: where Tokens Generatedi is the number of tokens generated by the model in the ith inference request. Unlike training scenarios, inference throughput is measured directly in tokens per second (Token/s) without normalization by model parameters, as inference efficiency primarily depends on the speed of token generation rather than parameter count. Higher values of TT, ST, and IT indicate more efficient data processing relative to model size or inference speed, while lower values suggest potential inefficiencies or bottlenecks, particularly noticeable in larger models or slower inference generation. 5.1.2 Energy Consumption Energy consumption has become crucial factor in evaluating the overall efficiency of AI models [250, 251], particularly with the growing scale of deep learning systems. In this context, energy consumption refers to the total amount of electrical energy consumed by the hardware during training or inference, typically measured in Joules (or kilowatt-hours). Since hardware power usage is generally measured in Watts (where 1 Watt = 1 Joule per second), integrating power over time yields the total energy consumed. To quantify energy efficiency, we propose the metric Average Energy Consumption (AEC). Let (t) denote the instantaneous power consumption (in Watts) of the system at time t. Then the total energy consumed over time period (in seconds) is given by: (t) dt Etotal = (cid:90) 0 35 (7) TT = (cid:80)N i=1 (cid:1) (cid:0) Tokens Processedi Model Parameters i=1 Timei (cid:80)N (cid:80)N i=1 ST = (cid:17) (cid:16) Samples Processedi Model Parameters i=1 Timei (cid:80)N (cid:80)N IT = i=1 Tokens Generatedi i=1 Timei (cid:80)N (4) (5) (6)"
        },
        {
            "title": "5 ASSESSMENT",
            "content": "The AEC metric is defined as the average power consumption over the entire duration: AEC ="
        },
        {
            "title": "Etotal\nT",
            "content": "="
        },
        {
            "title": "1\nT",
            "content": "(cid:90) 0 (t) dt (8) Where the is the total training or inference time (in seconds), (t) is the instantaneous power consumption at time t, measured in Watts (i.e., Joules per second), and Etotal represents the total energy consumed over time , measured in Joules. lower AEC indicates that the system operates more efficiently in terms of energy usage, which is critical not only for reducing operational costs but also for mitigating the environmental impact of large-scale AI deployments."
        },
        {
            "title": "5.1.3 Model Compression Rate",
            "content": "Model compression rate is critical metric for evaluating the effectiveness of techniques aimed at reducing the size of deep learning models while preserving their functionality [71, 252, 253, 254]. This is particularly important for deploying large models in resource-constrained environments, such as edge devices, or for reducing latency and energy consumption during inference. higher compression rate indicates more compact model representation, but it must be balanced against performance degradation. We propose the metric Model Compression Rate (MCR), defined as the ratio of the original model size to the compressed model size, adjusted for performance retention. The formal definition is: CR(P erf ormancec) = Sizeoriginal Sizecompressed Performancecompressed Performanceoriginal (9) where Sizeoriginal and Sizecompressed represent the model size in bytes before and after compression, respectively, and Performanceoriginal and Performancecompressed denote task-specific evaluation metrics (like Section ??). This formulation penalizes aggressive compression that significantly degrades model performance. The metric enables cross-comparison of compression techniques by unifying size reduction and performance trade-offs into single value. 5.1.4 Model Performance LLMs are rigorously evaluated through specialized benchmarks designed to measure their reasoning, coding, mathematical, and multilingual capabilities. MMLU-Pro. MMLU-Pro [255] enhances its predecessor by incorporating significantly more complex, graduate-level problems across disciplines that require multi-step logical deduction, causal inference, and counterfactual reasoning. This benchmark effectively identifies performance limitations in contemporary language models, highlighting substantial gaps between human expert performance and AI systems when addressing problems requiring specialized knowledge integration. BBH. Big-Bench Hard (BBH) [256] comprises 23 challenging tasks from the border BIG-Bench [257], specifically targeting advanced reasoning capabilities where previous models showed significant deficits. It encompasses diverse cognitive challenges, including logical deduction, multi-step arithmetic, strategy QA, and counterfactual analysis. Models performance on BBH strongly correlates to real-world reasoning capabilities and novel problem-solving beyond training distribution. GPQA. Graduate-Level Google-Proof Q&A Benchmark (GPQA) [258] focuses on expert-level reasoning ability across science, humanities, and logic of LLMs. Its dataset comprises curated high-quality questions presented in multiple-choice or open-ended formats, with accuracy (%) as the primary metric to assess deep understanding and multi-step problem-solving IFEval. Instruction Following Evaluation (IFEval) [259] assesses LLMs ability to follow instructions through prompts containing atomic, verifiable directives. Each instruction can be validated using simple, deterministic programs that objectively verify whether model responses adhere to the specified requirements."
        },
        {
            "title": "5 ASSESSMENT",
            "content": "HumanEval. HumanEval [260] evaluates programming proficiency using handcrafted Python function completion tasks. Models generate code snippets based on problem descriptions, and performance is measured via ass@k (probability of valid solutions within attempts), emphasizing functional correctness. HARDMath. HARDMath [261] evaluates LLMs on asymptotic reasoning in applied mathematics through 1,466 algorithmically generated graduate-level problems requiring approximation techniques. Unlike traditional benchmarks focusing on exact solutions, HARDMath addresses real-world scientific and engineering problems involving algebraic equations, ODEs, and integrals without closed-form solutions. Current LLMs perform poorly on these problems, highlighting significant limitations in handling advanced applied mathematics requiring approximation methods. MuSR. Multistep Soft Reasoning (MuSR)[262] evaluates language models reasoning capabilities through complex natural language narratives. The dataset features free-text narratives reflecting real-world reasoning domains, making it more challenging than typical synthetic benchmarks while remaining solvable by human annotators. MuSR uniquely scales with LLM advancement, enabling continuous assessment of reasoning capabilities across various models and prompting techniques while identifying persistent gaps in robust multi-step reasoning performance."
        },
        {
            "title": "5 ASSESSMENT",
            "content": "Table 2: Overview of Evaluated Large Language Models. Model Name Parameter Year Creator LLaMA 3.1 LLaMA 3.2 LLaMA 3.2 LLaMA 3.3 DeepSeek-R1 Distill-Qwen-1.5B DeepSeek-R1 Distill-LLaMA-8B DeepSeek-R1 Distill-Qwen-14B Qwen 2.5 Qwen 2.5 Qwen 2.5 Phi-3.5-mini Phi-4 Yi-34B Mistral 7B Mixtral 822B MoE 8B 1B 3B 70B 1.5B 8B 14B 7B 14B 32B 3.5B 14B 34B 7B 2024 Meta AI 2024 Meta AI 2024 Meta AI 2024 Meta AI 2024 DeepSeek 2024 DeepSeek 2024 DeepSeek 2024 Alibaba Cloud 2024 Alibaba Cloud 2024 Alibaba Cloud 2023 Microsoft 2024 Microsoft 2024 2023 Mistral AI 822B 2023 Mistral AI 01.AI 5.2 Preliminaries of EFFICIENTLLM 5.2.1 Curated List of LLMs LLaMA 3 Series. LLaMA is family of open LLMs introduced by Meta AI to facilitate research with highperformance yet smaller-scale LLMs. The latest generation, LLaMA 3, was trained on an order-of-magnitude more data than LLaMA 2 and doubled the context window (up to 128k tokens), while supporting multilinguality, coding, and tool use [263, 264]. Architecturally, LLaMA models are decoder-only Transformers with prenormalization and rotary positional embeddings; LLaMA 3 adopts grouped-query attention to efficiently handle the extended context length [264]. We use the LLaMA 3 series in our experiments, specifically the LLaMA 3.1 (8B), LLaMA 3.2 (1B and 3B), and LLaMA 3.3 (70B) variants. DeepSeek-R1. DeepSeek [265] is an open-source LLM project focused on aggressive scaling of model size and data to push open-model performance. The flagship DeepSeek model has 67B parameters and was trained on 2 trillion tokens with techniques like grouped-query attention (in the 67B model) to improve efficiency. The DeepSeek models underwent supervised fine-tuning and Direct Preference Optimization to create aligned chat models, which reportedly outperform LLaMA 2 70B on reasoning and coding tasks. As part of the DeepSeek R1 release, distilled versions of larger models were provided to explore efficiency: we evaluate the DeepSeek-R1 series [46], including Distill-Qwen-1.5B, Distill-LLaMA-8B, and Distill-Qwen-14B. Qwen 2.5 Series. Qwen (Alibaba Cloud, 20232024) [266] is bilingual (Chinese-English) LLM series originally released at 7B and 14B parameters. The second-generation Qwen 2 models [267] broadened the scale to 32B and 72B, including mixture-of-experts architecture in one variant, to attain greater efficiency at high parameter counts. Qwen models use Transformer decoder similar to LLaMA, with enhancements such as ALiBi/rotary positional encoding and long-context training scheme (Dual Chunk Attention and YARN scaling) to support inputs up to 128k tokens. The Qwen series also features specialized instruction-tuned, code, and math versions for improved tool-use and reasoning. We include the Qwen 2.5 models at 7B, 14B, 32B, and 72B in our evaluation. Phi Series. Phi [268] is line of Small Language Models by Microsoft (20232024) aiming for maximal task performance at fraction of conventional LLM sizes. Phi models are Transformer decoders trained with strong focus on data quality: e.g. Phi-1 (1.3B) was trained on curated textbook quality data to excel in coding [269]. The latest release, Phi-4, is 14B-parameter model that leverages extensive synthetic data generation and distillation from GPT-4 to achieve performance on par with much larger models. Phi-4 uses essentially the same architecture as its 3B-parameter predecessor but with scaled model size and refined training curriculum,"
        },
        {
            "title": "5 ASSESSMENT",
            "content": "yielding state-of-the-art reasoning and math capabilities among open models. We evaluate the Phi-3.5-mini and Phi-4 (14B) models, which demonstrate the Phi approach to efficiency. Yi. Yi (01.AI, 2024) [270] is an open foundation model developed by Kai-Fu Lees team, with the goal of matching GPT-3.5 level ability in relatively compact model. Yi-34B is 34-billion-parameter Transformer trained from scratch on 3.1 trillion tokens of carefully filtered text (in English and Chinese), combined with polished finetuning set for alignment. To maximize efficiency, Yi employs Grouped-Query Attention (GQA) splitting attention heads into shared key/value groups which reduces memory and compute overhead with minimal performance loss. The designers chose 34B as sweet spot for serving on single GPUs (with 4-bit quantization) while retaining emergent abilities. We use the Yi-34B model in our experiments. Mistral and Mixtral. Mistral 7B (Mistral AI, 2023) [271] is 7.3B-parameter open LLM engineered for efficiency, known for outperforming larger models (e.g. LLaMA 2 13B) on many benchmarks. It adopts grouped-query attention for faster inference and implements sliding-window attention mechanism to handle long sequences without expanding memory use. Building on this, Mistral introduced Mixtral 87B [40], sparse Mixture-of-Experts model that combines 8 expert networks based on the Mistral architecture. In Mixtral 87B, at each layer router activates 2 out of 8 experts per token, so each token effectively utilizes 13B parameters (of 47B total) during inference. This design allows Mixtral to achieve performance comparable to dense 70B models while maintaining higher throughput (it was trained up to 32k context length and excels in math and coding tasks). We evaluate the Mistral 7B dense model as well as the Mixtral 87B and larger Mixtral 822B MoE model in our study. 5.2.2 Experimental Datasets Fineweb-Edu (350B). The FineWeb-Edu corpus [272] is an educationally focused subset of the 15-trillion-token FineWeb crawl. Each Common-Crawl page is scored by RoBERTa-based educational value classifier; retaining documents with an integer score 3 yields 1.3T-token collection of predominantly English lecture notes, textbook chapters, research articles, and open-courseware transcripts, while laxer score-2 variant preserves 5.4T tokens for recall-oriented studies. For controlled ablations Hugging Face releases stratified 350B-token sampletokenised with the GPT-2 schemewhich underpins the public 1.8B-parameter model ablation-model-fineweb-edu. Pre-training on this 350B educational slice boosts zero-shot accuracy by 36 pp on nine reasoning-centric benchmarks relative to models trained on generic web data, highlighting the value of pedagogical sources for factual recall and multi-step reasoning. All records are stored in Parquet with rich metadata (score, language_score, dump, token counts), enabling reproducible sub-sampling, multilingual filtering, and safety audits. Nevertheless, residual personally identifiable information and the English-centric bias inherited from web crawls necessitate additional deduplication, redaction, and geographic balancing when employing FineWeb-Edu for downstream instruction tuning and alignment research. OpenO1-SFT. The OpenO1-SFT benchmark [273] serves to assess the proficiency of LLMs in performing intricate text-based tasks that necessitate chain-of-thought processing after undergoing supervised fine-tuning. The core task involves the generation of coherent and logical sequences of intermediate thoughts that lead to final answer, often within the context of question answering. This benchmark is specifically designed to enhance the models capacity for multi-step deductive processes and problem resolution, as highlighted by its emphasis on the explicit articulation of thought processes alongside the conclusive output. The inclusion of both Chinese and English records, totaling approximately 77,685 instances, broadens its applicability for cross-lingual studies on deductive capabilities. Research utilizing this benchmark has demonstrated its effectiveness in improving the self-consistency and accuracy of models in tasks demanding logical inference. The structured format, employing Thought and Output tags, facilitates the models learning of humanlike thought patterns, which is particularly valuable in applications such as intelligent tutoring systems and advanced question answering platforms. Studies have also explored the use of this dataset to refine the technical approaches for developing large models capable of advanced deductive abilities. However, investigations have indicated potential correlation between enhanced deductive capabilities achieved through fine-tuning on datasets like Open-o1 and decrease in safety scores, suggesting complex interplay between model performance and safety considerations."
        },
        {
            "title": "5 ASSESSMENT",
            "content": "Medical-o1-reasoning-SFT. The medical-o1-reasoning-SFT benchmark [274] is crafted to evaluate the deductive abilities of language models within the specialized domain of medicine following supervised finetuning. The tasks typically involve addressing medical inquiries, formulating diagnoses based on provided patient details, or elucidating complex medical concepts. primary challenge in this context is to guarantee the precision, dependability, and safety of the models deductions, given the critical implications of medical applications [275]. The benchmark employs curated medical datasets to train models for improved accuracy in this sensitive field. The necessity for models to possess deep understanding of intricate biological and clinical information, coupled with the capacity to apply this knowledge in nuanced scenarios, distinguishes this benchmark. It aims to go beyond mere pattern recognition, requiring models to engage in genuine medical deductive processes."
        },
        {
            "title": "5 ASSESSMENT",
            "content": "Method Parameters Micro Batch Size PPL AMU (GB) AL (s/iter) TT (Tokens/param/s) AEC (W) GPU Hours Table 3: Efficiency LLM Results for Attention Mechanisms. MQA GQA MLA NSA 0.5B 1.5B 3B 0.5B 1.5B 3B 0.5B 1.5B 3B 0.5B 1.5B 3B 4 2 1 4 2 1 4 2 1 4 2 1 9.27 8.23 7.86 9.05 8.09 7.54 8.73 7.79 7.29 8.96 7.82 7.38 43.75 42.24 41.27 45.29 44.87 43.77 53.89 52.93 50.45 44.78 43.57 43.19 0.1118 0.1298 0.1458 0.1127 0.1283 0.1464 0.2082 0.2537 0.2997 0.6839 0.5962 0.5024 2.981001 8.571002 3.811002 2.941001 8.641002 3.791002 1.591001 5.081002 2.621002 4.891002 1.091002 1.2610 633.59 646.62 661.38 644.26 652.74 667.34 607.58 608.17 605.46 594.23 598.15 600.27 19.0248 33.1448 77.05 48 21.0648 38.0348 86.80 48 30.4448 75.20 48 178.8448 101.3848 176.7248 280.9248 5.3 Assessment of Architecture Pretraining Efficiency Architecture pretraining efficiency is critical factor in determining the practical deployment and scalability of LLMs [276, 277, 278, 279, 280]. significant challenge limiting the widespread adoption of LLMs is their computational intensity and memory requirements, particularly when processing long sequences during the pretraining stage. These efficiency constraints can be attributed to the quadratic complexity of the attention mechanism. Given that modern LLMs require substantial computational resources for pretraining, optimizing architecture efficiency during pretraining has become central research focus. In this section, we assess the efficiency of LLM architectures during pretraining from the following perspectives: attention optimization, positional encoding efficiency, parameter sharing, and alternatives to traditional attention. These perspectives evaluate the ability of LLM architectures to reduce computational complexity, minimize memory usage, enable longer context processing, and maintain model performance while improving pretraining speed and efficiency. Goal. In this section, we aim to examine the efficiency of various architectural improvements for LLMs during pretraining. We pretrained three model sizes (0.5B, 1.5B, and 3B parameters for LLMs) using the Qwen2.5 as our base model and fine-web edu (350B Tokens) dataset to systematically evaluate four categories of efficiency techniques: Efficient Attention Mechanisms (MQA, GQA, MLA and NSA), Efficient Positional Encoding methods (including relative position encodings, ALiBi, and RoPE), Sparse Modeling techniques (Mixture-of-Experts and Conditional Computation), and Attention-Free Alternatives (State-Space Models and RNNs). For each technique, we measure five key metrics: Average Memory Utilization (AMU), Average Latency (AL), Tokens Throughput (TT), Average Energy Consumption (AEC), and Perplexity (PPL), allowing us to identify which efficiency techniques provide the optimal balance between computational efficiency and model performance across different model scales. Hardware and Training Framework. Our experiments were conducted on large-scale distributed computing infrastructure comprising 48 NVIDIA GH200 96GB GPUs. The GPUs were organized into nodes, with each node containing 4 H100 GPUs paired with an NVIDIA Grace processor (288 cores, 288 threads). This highperformance CPU provided robust data preprocessing capabilities and efficient inter-node communication. The system was interconnected with high-bandwidth NVLink for intra-node GPU communication and InfiniBand networking for inter-node communication, ensuring minimal latency during distributed training. For the software framework, we leveraged Megatron-Core [281], powerful distributed training framework optimized for LLMs. Megatron-Cores tensor and pipeline parallelism capabilities were crucial for efficiently scaling our training across multiple GPUs and nodes. We implemented 3D parallelism (data, tensor, and pipeline) to maximize hardware utilization and training efficiency."
        },
        {
            "title": "5 ASSESSMENT",
            "content": "Table 4: Efficiency Results for LLMs Efficient Positional Encoding. Method Parameters Context length PPL AMU (GB) AL (s/iter) TT (Tokens/param/s) AEC (W) GPU Hours Rope Absoluate Learnable Absoluate Relate None 1.5B 1.5B 1.5B 1.5B 1.5B 8K 8K 8K 8K 8K 8.09 8.32 8.18 8.29 8.75 44.82 46.71 45.93 43.94 48.64 0.1280 0.1312 0.1296 0.1246 0.1378 8.641002 8.121002 8.371002 8.981002 7.681002 652.79 672.45 662.44 646.39 692.37 38.0348 38.9848 38.5148 37.0248 40."
        },
        {
            "title": "5.3.1 Assessment of Efficient Attention Mechanisms",
            "content": "Attention mechanisms are central to the performance of modern LLMs [282, 283, 284, 285, 286]. Yet, they remain significant computational bottleneck due to their quadratic complexity concerning sequence length [287, 288, 289, 290, 291] during the pretraining stage. To address this challenge, we evaluated several efficient attention variants that reduce computational and memory demands while preserving model capabilities during pretraining. In our experimental framework, we systematically compared Multi-Query Attention (MQA), Grouped-Query Attention (GQA), Multi-Head Latent Attention (MLA), Native Sparse Attention (NSA), and Mixture of Block Attention (MoBA) across our three model scales (0.5B, 1.5B, and 3B parameters). Our comprehensive evaluation measured how these architectural choices impact Average Memory Utilization (AMU), Average Latency (AL), Tokens Throughput (TT), Average Energy Consumption (AEC), and model performance as reflected in Perplexity (PPL). Efficient Attention Mechanisms for LLMs. As shown in Table 3, attention mechanisms are pivotal in the remarkable performance of modern LLMs; however, their quadratic complexity relative to sequence length poses substantial computational and memory constraints. Efficient attention mechanisms have thus become essential to scale LLMs practically, aiming to mitigate these resource bottlenecks while preserving or enhancing performance. In our comprehensive evaluation, we assessed several prominent efficient attention variants, including Multi-Query Attention (MQA), Grouped-Query Attention (GQA), Multi-Head Latent Attention (MLA), and Native Sparse Attention (NSA), across multiple model scales (0.5B, 1.5B, and 3B parameters). Our analysis reveals spectrum of trade-offs: MQA demonstrates superior efficiency with the lowest average memory utilization (AMU = 42.24 GB) and competitive latency (AL = 0.1298 seconds per iteration), while MLA achieves the best performance in terms of perplexity across all model sizes (PPL = 8.73, 7.79, and 7.29 for 0.5B, 1.5B, and 3B models, respectively). NSA excels in energy efficiency with the lowest average energy consumption (AEC = 594.23 W). GQA offers balanced middle ground, particularly at the 1.5B scale where it achieves the lowest latency. These findings underscore that the optimal attention mechanism depends on specific deployment constraints, with MQA favored for memory-constrained environments, MLA for performance-critical applications, and NSA for energy-efficient deployments. 5.3.2 Assessment of Efficient Positional Encoding Positional encoding plays an indispensable role in enabling LLMs to understand the order of tokens within input sequences [292, 293, 294], which is crucial for maintaining semantic coherence and contextual relevance during the pretraining stage. However, traditional positional encoding methods can incur substantial computational overhead [295, 296, 147, 297], particularly as the context length increases. In our experiments, we systematically evaluated various efficient positional encoding techniques, including Rotary Position Embeddings (RoPE), Absolute Positional Encoding (APE), Learnable Absolute Positional Encoding (Learnable APE), Relative Positional Encoding (RPE), and scenarios with no positional encoding (None), focusing on their impacts on computational efficiency and model performance for LLMs. Efficient Positional Encoding for LLMs. Our results (summarized in Table 4) demonstrated that RoPE consistently offered the best balance between perplexity and model performance, achieving the lowest perplexity score (PPL = 8.04). Meanwhile, Relate (RPE) demonstrated superior efficiency metrics with the lowest average memory utilization (AMU = 43.94 GB), lowest average latency (AL = 0.1246 seconds per iteration), highest attention throughput (TT = 8.981002 TFloats), and lowest attention energy consumption (AEC = 646.39 W). Learnable Absolute Positional Encoding showed moderate efficiency and performance (PPL = 8.18, AMU"
        },
        {
            "title": "5 ASSESSMENT",
            "content": "Table 5: Efficiency Results for LLMs MoE Mechanisms. Method Parameters Top PPL AMU (GB) AL (s/iter) TT (Tokens/param/s) AEC (W) GPU Hours Dense Model Dense Model MoE Model MoE Model 1.5B 3B 0.5B8 1.5B8 2 8.09 7.58 7.35 7.10 44.82 43.94 52.36 76.53 0.1280 0.1246 0.1315 0.1420 8.641002 8.981002 1.051001 1.25 1001 652.74 647.34 667.33 692.45 38.0348 86.8048 39.0748 84.19 = 45.93 GB, AL = 0.1296 s/iter), outperforming the standard Absolute Positional Encoding. In contrast, the absence of positional encoding (\"None\") notably degraded model performance across all metrics (PPL = 8.75, AMU = 48.64 GB, AL = 0.1378 s/iter), emphasizing the necessity of positional information for effective sequence modeling in LLMs. Efficient Positional Encoding for LVMs. Regarding Large Vision Models (LVMs), positional embeddings are fundamentally integrated within the patch embedding process inherent to architectures like DiT. Altering or replacing positional encoding mechanisms in LVMs is not straightforward due to their structural dependence on spatial locality and the fixed-grid architecture. Consequently, experimentation with alternative positional encoding techniques is less applicable for LVMs, and thus we omit detailed discussion and evaluation of positional encoding efficiency for LVM architectures in this section. 5.3.3 Assessment of Sparse Modeling via MoE Mixture of Experts (MoE) has emerged as powerful paradigm for scaling neural networks efficiently by introducing conditional computation [298, 299, 300] during the pretraining stage, where only subset of model parameters is activated for each input token. This sparse activation pattern enables models to increase their parameter count significantly while maintaining reasonable computational requirements during both training and inference. In our experimental framework, we systematically evaluated MoE architectures against traditional dense models to quantify the efficiency-performance trade-offs across multiple model scales. Sparse Modeling via MoE for LLMs. As shown in Table 5, our experiments with MoE architectures revealed significant performance improvements over comparable dense models. The 1.5B8 MoE model with top-2 routing achieved perplexity of 7.10, substantially outperforming both the 1.5B dense model (PPL = 8.04) and even the larger 3B dense model (PPL = 7.58). Similarly, the 0.5B8 MoE configuration delivered strong performance (PPL = 7.35) that exceeded the capabilities of the 1.5B dense model while using fewer active parameters per token. This performance advantage demonstrates the efficacy of sparse expert specialization, where different experts can focus on distinct linguistic patterns and phenomena. However, these performance gains come with increased resource requirements. MoE models exhibited higher memory utilization (AMU = 76.53 GB for 1.5B8 and 52.36 GB for 0.5B8) compared to dense models (AMU = 44.82 GB for 1.5B and 43.94 GB for 3B), reflecting the storage needs for the expanded parameter space. Similarly, we observed increased latency (AL = 0.1420 s/iter for 1.5B8 and 0.1315 s/iter for 0.5B8) and energy consumption (AEC = 405321.86 for 1.5B8 and 667.33 for 0.5B8) compared to their dense counterparts. Interestingly, despite these increased resource costs, MoE models demonstrated superior throughput (TT = 1.251001 TFloats for 1.5B8 and 1.051001 TFloats for 0.5B8), suggesting efficient parallelization across experts during computation. 5.3.4 Assessment of Attention-Free Alternatives for Sequence Modeling While attention mechanisms have proven foundational to the success of modern LLMs, they remain computationally intensive due to their quadratic scaling with sequence length during the pretraining stage, prompting research into efficient attention-free architectures that maintain competitive performance while reducing computational requirements. In our comprehensive evaluation, we assessed several prominent attention-free alternatives, including State Space Models (Mamba), linear attention mechanisms (Pythia), and recurrent architectures (RWKV), comparing them against our baseline transformer architecture (Qwen2.5) across three model scales (0.5B, 1.5B, and 3B parameters). Our analysis examined key efficiency metrics - Average Memory Utilization (AMU), Average Latency (AL), Tokens Throughput (TT), and Average Energy Con-"
        },
        {
            "title": "5 ASSESSMENT",
            "content": "Table 6: Efficiency Results for Attention-Free Mechanisms. The best result is compared under the same parameters. Method Parameters Context Length PPL AMU (GB) AL (s/iter) TT (Tokens/param/s) AEC (W) Qwen2.5 Mamba Pythia RWKV 0.5B 1.5B 3B 0.5B 1.5B 3B 0.5B 1.5B 3B 0.5B 1.5B 3B 8K 8K 8K 8K 8K 8K 8K 8K 8K 8K 8K 8K 8.73 8.09 7.29 10.31 9.48 8.93 11.72 10.35 9.82 11.25 10.13 9.54 45.24 44.82 43.72 29.16 30.25 31.89 43.58 43.11 42.63 39.42 40.18 41.03 0.1129 0.1280 0.1467 0.0954 0.1025 0.1136 0.1074 0.1351 0.1534 0.1062 0.1189 0.1319 2.94 1001 8.64 1002 3.79 1002 2.211001 7.721002 3.251002 2.571001 7.941002 3.461002 2.361001 7.281002 3.1210 644.23 652.79 667.38 498.37 510.64 525.12 630.84 638.92 651.27 576.51 589.37 604.85 sumption (AEC) - alongside model performance measured by perplexity (PPL), enabling us to quantify the efficiency-performance trade-offs inherent to different architectural paradigms. Attention-Free Modeling for LLMs. As shown in Table 6, our comparative analysis of attention-free architectures revealed distinctive efficiency performance trade-offs across different model paradigms. Mamba, state-space model implementation, demonstrated remarkable efficiency advantages with substantially lower memory utilization (AMU = 29.16 GB, 30.25 GB, and 31.89 GB for 0.5B, 1.5B, and 3B parameter models, respectively) compared to the transformer baseline (AMU = 45.24 GB, 44.82 GB, and 43.72 GB). Mamba also improved energy efficiency, consuming approximately 22-25% less power (AEC = 498.37 W, 510.64 W, and 525.12 W) than the transformer counterparts. At the 1.5B parameter scale, Mamba exhibited the lowest latency (AL = 0.1025 s/iter) among all models tested. However, these efficiency gains came with performance trade-off, as Mambas perplexity scores (PPL = 10.31, 9.48, and 8.93) were consistently higher than the transformer baseline (PPL = 9.09, 8.04, and 7.58). RWKV, recurrent architecture, offered moderate efficiency improvements with lower memory usage and energy consumption than transformers. At the same time, Pythia demonstrated competitive latency but with perplexity scores that were significantly higher than both transformer and Mamba models. These findings suggest that while attention-free alternatives provide compelling efficiency advantages, particularly for deployment scenarios with strict memory or energy constraints, transformer-based architectures continue to deliver superior performance for tasks where model quality is paramount."
        },
        {
            "title": "5.4 Assessment of Training and Tuning Efficiency",
            "content": "Training and fine-tuning LLMs presents significant computational challenges that impact resource requirements, development costs, and environmental footprint. As models grow in size and complexity, optimizing training efficiency becomes increasingly critical for both research advancement and practical deployment. This section examines various techniques and approaches for improving training and tuning efficiency, including scalable training strategies (such as mixed precision, various parallelism methods, and memory optimizations) and parameter-efficient fine-tuning methods that enable adaptation with minimal computational overhead. Quantitative assessments across multiple model architectures (ranging from 1B to 24B parameters) demonstrate the trade-offs between different optimization approaches in terms of convergence quality (loss), memory utilization, computational throughput, training latency, and energy consumption, providing practical insights for selecting appropriate efficiency techniques based on available resources and desired performance targets. Goal. In this section, we aim to evaluate the efficiency of various training and fine-tuning approaches for LLMs. We conducted experiments across multiple model architectures ranging from 1B to 24B parameters (including Llama-3.2, Qwen-2.5, Mistral) to systematically assess seven different optimization techniques: standard LoRA, LoRA-plus, RSLoRA, DoRA, PISSA, parameter freezing, and full fine-tuning with DeepSpeed. For each method, we measured six key metrics: Loss (model performance), Average Memory Utilization (AMU), Peak Compute Utilization (PCU), Average Latency (AL), Samples Throughput (ST), and Average Energy Consumption (AEC). This comprehensive evaluation allows us to identify the optimal balance between computational efficiency and model performance across different model scales, providing practical insights for researchers and practitioners working with limited computational resources while maintaining competitive model quality. Note. For the full fine-tuning experiments, the batch size was set to half the size used for PEFT methods to accommodate higher memory requirements. Additionally, DeepSpeed ZeRO-3 Offload was employed to efficiently manage GPU memory utilization by offloading optimizer states and parameters to CPU memory, ensuring the feasibility of training larger models within the available hardware constraints. Hardware and Training Framework. Our experiments were conducted on distributed computing infrastructure comprising 8 NVIDIA H200 141B GPUs. The GPUs were organized into 1 nodes, with each node containing 8 H200 GPUs paired with an Intel Xeon(R) Platinum 8558 processor (48 cores, 96 threads). This high-performance CPU provided robust data preprocessing capabilities and efficient inter-node communication. The system was interconnected with high-bandwidth NVLink for intra-node GPU communication and InfiniBand networking for inter-node communication, ensuring minimal latency during distributed training. For the software framework, we leveraged LlamaFactorys, flexible and efficient fine-tuning framework optimized for LLMs. LlamaFactorys implementation of parameter-efficient fine-tuning methods and optimization techniques was crucial for efficiently executing our experiments across various model architectures and training configurations. O1-SFT Dataset. As shown in Table 7, our comprehensive evaluation of Parameter-Efficient Fine-Tuning (PEFT) methods reveals distinct efficiency-performance trade-offs across model scales and architectures. For smaller models (1-3B parameters), LoRA-plus consistently achieved superior performance with the lowest loss metrics (0.7442 for Llama-3.2-1B and 0.5791 for Llama-3.2-3B), while maintaining reasonable memory utilization (49.776 GB and 59.664 GB respectively). As model size increased, RSLoRA demonstrated competitive performance, particularly for Qwen-2.5-14B (loss = 0.4126) and Mistral-Small-24B (loss = 0.3818). Parameter freezing exhibited the lowest average latency across all model scales (0.2542 s/iter for Llama-3.2-1B to 1.4815 s/iter for Mistral-Small-24B), making it ideal for latency-sensitive applications, albeit sometimes at the cost of reduced model performance. PISSA showed balanced performance in mid-sized models, achieving the lowest loss for Llama-3.2-3B (0.5137). Full fine-tuning with DeepSpeed optimization delivered strong performance for smaller models but demonstrated diminishing returns as model size increased, particularly for the largest 24B parameter model where its loss (1.2805) substantially exceeded other methods. DoRA, while computationally intensive with consistently higher latency (2.1505 s/iter to 6.0606 s/iter across models), maintained competitive loss metrics in mid-sized models but performed poorly on the largest 24B model (loss = 1.2309). These findings suggest that optimal PEFT strategy selection should be tailored to specific deployment constraints, with LoRA variants preferable for general-purpose applications, parameter freezing"
        },
        {
            "title": "5 ASSESSMENT",
            "content": "Table 7: Assessment of Training and Tuning Efficiency for LLMs of O1-SFT Dataset (methods marked with * use DeepSpeed). Because of the different batch size, full* are not included in the comparisons. The best result is compared under the same model. Model Method Loss AMU (GB) PCU AL (s/iter) ST (Samples/param/s) AEC (W) Llama-3.2-1B Llama-3.2-3B Llama-3.1-8B Qwen-2.5-7B Qwen-2.5-14B Mistral-Small-24B Mistral-7B lora rslora dora pissa freeze full* lora rslora dora pissa freeze full* lora rslora dora pissa freeze full* lora 0.7562 lora-plus 0.7442 0.7454 0.7547 0.7595 0.6425 0.6788 0.6019 lora-plus 0.5791 0.5866 0.6006 0.5137 0.5000 0.5310 0.5137 lora-plus 0.4962 0.4986 0.5124 0.5137 0.4514 0.5553 0.4795 lora-plus 0.4621 0.4986 0.4861 0.4773 0.3996 0.4600 0.4795 lora-plus 0.4621 0.4126 0.4861 0.4260 0.5547 0.4582 0.3757 lora-plus 0.4962 0.3818 rslora 1.2309 dora pissa 0.3975 freeze* 0.6020 1.2805 0.4639 lora-plus 0.5039 0.4626 0.4614 0.4767 0.4718 0.8564 rslora dora pissa freeze full* lora rslora dora pissa freeze full* lora rslora dora pissa freeze full* full* lora 50.088 49.776 49.920 52.760 50.856 48.696 36.840 49.152 59.664 58.536 59.616 59.688 51.848 49.152 74.360 74.360 75.152 77.376 74.672 70.424 56.144 60.952 62.112 62.248 65.976 62.744 67.328 77.552 77.472 77.84 77.376 78.376 79.448 73.400 71.920 64.84 65.376 65.584 69.984 65.568 73.000 73.936 35.688 34.760 36.280 37.216 35.432 55.024 40.152 1.1669 1.1628 1.1655 2.1505 1.1669 0.2542 0.6993 1.6077 2.6247 2.6247 4.8544 2.6247 0.4252 1.6077 4.5872 4.5872 4.6083 8.9286 4.5872 0.7369 2.9851 2.7100 2.7248 2.5907 5.6818 2.7174 0.6988 2.6178 2.7855 3.3445 2.7855 5.7471 2.7933 0.6227 2.6178 3.1847 3.3113 3.3333 4.2017 3.3113 1.4815 3.7175 3.0211 3.0211 3.0211 6.0606 3.0120 1.3123 2.9155 O1-SFT 0.9228 0.9195 0.9219 0.9399 0.9312 0.9178 0.9510 0.9628 0.9408 0.9389 0.9395 0.9339 0.9322 0.9628 0.9462 0.9462 0.9527 0.9428 0.9442 0.9524 0.9779 0.9121 0.9114 0.9114 0.9258 0.9226 0.9305 0.9779 0.8496 0.7108 0.8450 0.8796 0.8562 0.8550 0.9695 0.8518 0.8553 0.8571 0.8625 0.8580 0.9664 0.9827 0.9608 0.9481 0.9511 0.9527 0.9531 0.9626 0.9763 46 8.221008 8.251008 8.231008 4.461008 8.221008 1.26 1007 9.151008 1.331008 1.221008 1.221008 6.591009 1.221008 2.51 1008 1.331008 2.981009 2.981009 2.971009 1.531009 2.981009 6.20 1009 3.061009 3.371009 3.361009 3.531009 1.611009 3.361009 6.54 1009 3.491009 8.191010 6.831010 8.211010 3.991010 8.191010 5.51 1009 7.771010 4.191010 4.021010 4.001010 9.71 1010 4.031010 8.991010 3.591010 3.031009 3.021009 3.031009 1.511009 3.031009 6.96 1009 3.1310 549.23 545.12 563.01 568.64 567.24 508.16 584.00 589.91 577.94 593.93 601.98 579.46 556.43 589.91 605.81 605.82 605.82 620.48 602.28 564.89 610.42 594.89 590.69 606.63 615.97 597.34 566.30 613.93 560.48 489.10 556.16 572.18 551.43 493.11 576.94 591.98 583.54 587.68 562.76 583.48 606.15 605.51 614.54 615.38 617.19 618.09 621.92 628.90 634."
        },
        {
            "title": "5 ASSESSMENT",
            "content": "Table 8: Assessment of Training and Tuning Efficiency for LLMs for Medical-O1 Dataset (methods marked with * use DeepSpeed). Because of the different batch size, full* are not included in the comparisons. The best result is compared under the same model. Model Methods Loss AMU (GB) PCU AL (s/iter) ST (Samples/param/s) AEC (W) Llama-3.2-1B Llama-3.2-3B Llama-3.1-8B lora rslora dora pissa freeze full* lora 1.7022 lora-plus 1.6473 1.6712 1.6993 1.6825 1.3406 1.4536 1.5274 lora-plus 1.4463 1.4938 1.5249 1.4999 1.2442 1.2484 1.4092 lora-plus 1.3285 1.3729 rslora 1.4062 dora pissa 1.3832 freeze* 1.0120 1.2900 rslora dora pissa freeze full* lora full* Medical-O1 0.6745 0.6833 0.7397 0.7588 0.6901 0.7145 0.7799 0.7451 0.7306 0.7267 0.7847 0.7325 0.7018 0.8189 0.7726 0.7716 0.7535 0.8131 0.8037 0.7285 0. 0.3423 0.3398 0.3398 0.6906 0.3389 0.2123 0.3488 0.7524 0.7530 0.7547 1.5528 0.7524 0.3648 0.7143 1.2837 1.2821 1.2853 2.8736 1.2903 0.4632 1.3387 37.304 37.136 38.744 41.568 41.192 45.704 45.128 50.328 49.376 49.312 53.296 50.536 50.864 53.080 45.120 44.672 44.592 46.768 46.944 46.848 64.456 1.871007 1.881007 1.881007 9.261008 1.891007 3.01 1007 1.831007 2.831008 2.821008 2.821008 1.371008 2.831008 5.84 1008 2.981008 6.221009 6.231009 6.211009 2.781009 6.191009 9.23 1009 5.971009 398.12 545.12 397.74 429.52 397.86 412.59 405.71 450.50 449.78 448.05 481.92 449.13 430.02 470.15 492.03 505.74 500.93 519.34 509.37 503.63 527.66 for latency-critical scenarios, and specialized methods like RSLoRA for larger models where fine-grained control of adaptation becomes increasingly important. Medical-O1 Dataset. Table 8 illustrates clear efficiency-performance trade-offs among Parameter-Efficient Fine-Tuning (PEFT) methods across varying scales of the Llama model architecture. For the smaller Llama-3.21B model, parameter freezing notably achieved the lowest loss (1.3406), with exceptional sample throughput (3.011007 Samples/param/s) and low latency (0.2123 s/iter), marking it as an ideal choice for latencysensitive medical applications. LoRA-plus exhibited robust efficiency, offering competitive loss (1.6473) and favorable energy consumption (545.12 W). Scaling up to the Llama-3.2-3B model, parameter freezing again showed superior efficiency with the lowest loss (1.2442) and notably reduced latency (0.3648 s/iter) relative to other methods, suggesting its continued suitability for applications demanding rapid inference. Conversely, DoRA significantly increased latency (1.5528 s/iter) and energy usage (481.92 W) while offering no clear performance advantage. In the largest tested Llama-3.1-8B model, parameter freezing once more demonstrated remarkable efficiency and performance, achieving the lowest loss (1.0120) and latency (0.4632 s/iter), underscoring its scalability and effectiveness in large-model scenarios. Full fine-tuning with DeepSpeed, despite achieving relatively strong performance (loss = 1.2900), incurred the highest memory usage (64.456 GB) and elevated energy consumption (527.66 W), indicating diminishing returns as model size grows."
        },
        {
            "title": "5 ASSESSMENT",
            "content": "Table 9: Assessment of Quantification Efficiency for LLMs. The best result is compared under the same model. Model Precision Avg Perf. AMU Sum AL tokens/s AEC MCR DeepSeek-R1-Distill-Qwen-1.5B bfloat16 float16 int4 bfloat16 float16 int4 DeepSeek-R1-Distill-Llama-8B Qwen2.5-7B Qwen2.5-14B Qwen2.5-32B DeepSeek-R1-Distill-Qwen-14B bfloat16 float16 int4 bfloat16 float16 int4 bfloat16 float16 int4 bfloat16 float16 int4 bfloat16 float16 int4 bfloat16 float16 int4 bfloat16 float16 int4 Phi-3.5-mini Yi-34B Phi0.2419 0.2450 0.2341 0.3421 0.3392 0.3116 0.4719 0.4712 0.4361 0.4448 0.4467 0.4152 0.4691 0.4691 0.4286 0.5523 0.5505 0.5095 0.4035 0.4006 0.3950 0.3683 0.3652 0.3355 0.3484 0.3482 0.3387 21.26 21.28 19.49 35.36 35.45 34.07 51.83 52.64 34.21 35.33 35.35 27.47 51.83 52.66 34.13 71.33 71.86 48.30 48.19 49.02 43.27 41.19 41.54 36.52 73.77 75.05 52.93 13024.35 9858.30 15453.42 13541.46 10926.70 15724.27 18683.70 14765.37 21529.09 13309.58 13766.35 12912.91 24065.05 24708.61 27865.30 26666.92 27399.52 25140.95 6547.79 6424.68 12202.10 8978.35 8647.61 11761.00 12858.15 13169.36 11404.40 39.68 37.70 42.34 37.79 35.90 40.29 24.74 23.50 26.40 40.38 38.36 43.10 24.74 23.50 25.89 17.54 16.66 19.20 45.16 42.90 48.19 54.87 52.13 58.54 17.02 16.17 18.16 144.39 1.0000 158.96 1.0128 134.89 3.8710 208.21 1.0000 222.76 0.9915 170.83 3.6434 212.29 1.0000 226.85 0.9985 191.05 3.6965 196.45 1.0000 197.40 1.0043 168.73 3.7338 205.97 1.0000 203.13 1.0000 187.40 3.6547 279.23 1.0000 259.53 0.9967 214.57 3.6900 217.16 1.0000 224.63 0.9928 319.11 3.9157 172.02 1.0000 172.28 0.9916 159.15 3.6438 295.10 1.0000 288.27 0.9994 334.46 3.8886 5.5 Assessment of Bit-Width Quantization Inference Efficiency Inference efficiency plays crucial role in the practical deployment of LLMs, vision-language models (VLMs), and large vision models (LVMs). Optimizing inference efficiency is essential to ensure low latency, minimal resource consumption, and effective energy utilization, enabling these models to be deployed in diverse and resource-constrained environments. This section evaluates the inference efficiency across various precision modes (bfloat16, float16, and int4 quantization) and model architectures, highlighting trade-offs in performance metrics and computational resource usage. Note on Int8 Quantization. We did not include int8 quantization results in this section because current inference support for int8 on NVIDIA Hopper architecture (GH200) is either incomplete or exhibits instability due to backend kernel issues. During our initial tests, int8-based inference led to runtime errors and inconsistent throughput behavior. We are actively investigating and working to resolve these issues. Once the compatibility and reliability of int8 quantization are verified, we will update our evaluation results accordingly. Goal. In this section, we systematically assess inference efficiency across different model precisions (bfloat16, float16, and int4) for range of model architectures, including DeepSeek, Qwen, Phi, and Yi models with parameter sizes from 1.5B to 34B. Specifically, we evaluate the impact of precision and quantization on key metrics, including task-specific performance (MMLU-Pro, BBH, GPQA, IFEval, MATH, MUSR), Average Memory Utilization (AMU), Average Latency (AL), Throughput (tokens per second), Average Energy Con-"
        },
        {
            "title": "5 ASSESSMENT",
            "content": "sumption (AEC), and Model Compression Ratio (MCR). This comprehensive analysis provides clear insights for selecting suitable inference strategies based on targeted deployment scenarios. Hardware and Inference Framework. The inference experiments were performed on an optimized inference server infrastructure consisting of NVIDIA GH200 96GB GPUs. The infrastructure setup comprised one node containing four H100 GPUs coupled with NVIDIA Grace processors (288 cores, 288 threads) to support efficient data processing and task scheduling. NVLink interconnects were utilized for rapid GPU-to-GPU communication, ensuring low latency and efficient data transfer. Evaluation Results. Table 9 presents the inference efficiency results across varying precisions for multiple model architectures. In the DeepSeek-R1-Distill-Qwen-1.5B model, the int4 precision significantly increased throughput (42.34 tokens/s) and reduced memory utilization (19.49 GB), albeit at marginal performance degradation (Avg Performance: 0.2341) compared to bfloat16 and float16. Similarly, for larger models like Qwen2.5-32B and Yi-34B, int4 quantization substantially enhanced throughput and memory efficiency, indicating its suitability for deployment scenarios prioritizing computational efficiency over maximum performance. Models at bfloat16 precision typically showed the highest performance metrics across architectures but at the cost of increased memory usage and energy consumption. The Phi-4 model demonstrated particularly high throughput (45.16 tokens/s) and acceptable performance (Avg Performance: 0.4035) at bfloat16, highlighting its efficacy for scenarios demanding balanced performance and efficiency. Overall, int4 quantization emerges as robust option for resource-constrained deployment scenarios, while bfloat16 remains preferable for applications requiring optimal performance metrics."
        },
        {
            "title": "6 SCALABILITY OF EFFICIENTLLM BENCHMARK",
            "content": "Table 10: Efficiency LVMs Results for Attention Mechanisms. The best result is compared under the same model. Method Model Training Steps FID AMU (GB) AL (s/iter) TT (Tokens/param/s) AEC (W) MHA MQA GQA MLA NSA DiT-XL/2 DiT-L/8 DiT-B/4 DiT-XL/2 DiT-L/8 DiT-B/4 DiT-XL/2 DiT-L/8 DiT-B/4 DiT-XL/2 DiT-L/8 DiT-B/4 DiT-XL/2 DiT-L/8 DiT-B/4 400K 400K 250K 400K 400K 250K 400K 400K 250K 400K 400K 250K 400K 400K 250K 19.47 118.87 68.38 8.93 78.05 55.29 8.71 81.90 53.99 116.93 114.63 73.88 22.78 89.98 55.27 40.50 23.49 15.51 43.78 23.03 16.13 43.71 22.65 16.25 45.84 23.88 16.26 59.34 24.77 18.94 0.2873 0.1635 0.1423 0.2637 0.1818 0.1413 0.2696 0.1816 0.1438 0.3291 0.2048 0.2100 0.5771 0.3416 0. 1.33011006 3.55911006 1.39211005 1.60331006 3.34911006 1.54841005 1.53321006 3.43021006 1.47751005 1.17431006 2.78431006 1.20961005 3.25591007 1.62091006 8.40511006 182.34 75.35 70.07 172.61 80.75 67.76 174.36 78.81 71.51 174.36 84.16 71.09 256.49 107.32 85.58 Table 11: Efficiency Results for LVMs MoE Mechanisms. Method Parameters Training Steps FID AMU (GB) AL (s/iter) TT (Tokens/param/s) AEC (J) 675M (DiT-XL/2) 459M (DiT-L/8) 130M (DiT-B/4) Dense Model Dense Model Dense Model MoE Model 675M8 (DiT-XL/2) 459M8 (DiT-L/8) MoE Model 130M8 (DiT-B/4) MoE Model 400K 400K 250K 400K 400K 250K 19.47 118.87 68.38 16.35 76.41 45.62 40.50 23.49 15.51 47.82 29.76 18.95 0.2873 0.1635 0.1423 0.2340 0.1358 0. 1.33011006 3.55911006 1.39211005 2.15681006 5.87241006 2.08821005 182.34 75.35 70.07 231.07 105.49 89."
        },
        {
            "title": "6 Scalability of EfficientLLM Benchmark",
            "content": "The previous sections demonstrated how EfficientLLM quantifies architectural and training-time trade-offs for purely textual LLMs. We now extend that investigation to vision and visionlanguage settings, but with deliberately tight scope: we evaluate only those acceleration strategies first validated on LLMs that can be applied unchanged to their visual counterparts. Concretely, we (i) insert efficient attention variants (MQA, GQA, MLA, NSA) into DiT-style diffusion transformers, (ii) swap dense blocks for Mixture-of-Experts (MoE) layers in the same DiT backbones, and (iii) benchmark palette of parameter-efficient fine-tuning (PEFT) methodsLoRA, LoRA-plus, RSLoRA, DoRA, PISSA, LoHa, LoKr, and GLoRAacross large-scale LVMs and VLMs (LLaVA-1.5, Qwen2.5-VL-7B, Intern-VL-38B, QvQ-Pre-72B, Wan 2.1, Stable Diffusion 3.5). Because EfficientLLMs metric collector is modality-agnostic, the same pipeline that logged AMU, latency, throughput, energy, and perplexity for language now records the identical metrics alongside vision-specific quality signals such as FID or loss. This unified view lets us ask single question throughout the remainder of the section: when an optimization accelerates text, does it still pay off when the tokens are image patches or joint textimage embeddings? 6.1 Efficiency for Transformer Based LVMs Architecture Pretraining Efficient Attention Mechanisms for LVMs. As shown in Table 10, in the context of Large Vision Models (LVMs), efficient attention mechanisms play critical role by optimizing computational resources, latency,"
        },
        {
            "title": "6 SCALABILITY OF EFFICIENTLLM BENCHMARK",
            "content": "Figure 7: Scalability analysis of EfficientLLM for LVM and VLM optimization. (a) Normalized efficiency scores across five metrics (FID, AMU, AL, TT, AEC) for attention variants (MHA, MQA, GQA, MLA, NSA) in three DiT-based LVM architectures (DiT-XL/2, L/8, B/4). All metrics are min-max normalized to [0,1] and higher values indicate better efficiency. (b) MoE vs. dense models across identical DiT backbones. MoE-based architectures consistently outperform dense counterparts in throughput and FID while incurring moderate AMU and AEC overhead. (c) Comparison of Parameter-Efficient Fine-Tuning (PEFT) methods (e.g., LoRA, RSLoRA, PISSA, DoRA) on various VLMs. Bars indicate normalized Efficiency score (top, higher is better) and Loss (bottom, lower is better). Methods marked with * indicate full fine-tuning using DeepSpeed."
        },
        {
            "title": "6 SCALABILITY OF EFFICIENTLLM BENCHMARK",
            "content": "and memory usage while maintaining high-quality outputs. Our evaluation encompassed several attention variants, including standard Multi-Head Attention (MHA), Multi-Query Attention (MQA), Grouped-Query Attention (GQA), Multi-Head Latent Attention (MLA), and Native Sparse Attention (NSA), assessed across different DiT model architectures (DiT-XL/2, DiT-L/8, and DiT-B/4). Results indicated that GQA and MQA consistently achieved superior performance in terms of Frchet Inception Distance (FID), with GQA exhibiting the lowest FID scores in DiT-XL/2 (FID = 8.71) and DiT-B/4 (FID = 53.99). MQA closely followed, providing balanced efficiency and performance, notably in DiT-XL/2 (FID = 8.93) with the lowest latency (AL = 0.2637 s/iter) and high throughput (TT = 1.60331006 TFloats). MLA, although generally less efficient, demonstrated substantial performance in DiT-B/4 scenarios, indicating its suitability for specific parameter and architecture configurations. NSA showed its strengths primarily in memory-intensive tasks despite higher latency, underscoring its potential for specific deployment environments with particular resource constraints. These findings highlight the importance of selecting an attention mechanism aligned with both the performance goals and the computational resources available for large-scale vision tasks. Sparse Modeling via MoE for LVMs. As shown in Table 11, our experiments with Mixture of Experts (MoE) architectures for Large Vision Models revealed consistent performance improvements across all model scales. The 675M8 MoE configuration (DiT-XL/2) achieved significantly lower FID score of 16.35 compared to its dense counterpart (FID = 19.47), indicating superior image generation quality. Similarly, the 459M8 (DiT-L/8) and 130M8 (DiT-B/4) MoE models demonstrated substantial improvements with FID scores of 76.41 and 45.62, outperforming their dense equivalents which scored 118.87 and 68.38 respectively. These performance gains, however, come with increased resource requirements. MoE configurations showed higher memory utilization across all scales, with the 675M8 model requiring 47.82 GB compared to 40.50 GB for the dense version. Interestingly, despite the expanded parameter space, MoE models exhibited improved computational efficiency with lower average latency (AL = 0.2340 s/iter for 675M8 versus 0.2873 s/iter for the dense equivalent) and substantially higher throughput (TT = 2.15681006 TFloats versus 1.33011006 TFloats). This pattern was consistent across smaller model scales as well, with the 130M8 (DiT-B/4) MoE model achieving approximately 50% higher throughput than its dense counterpart while delivering significantly better generation quality. These results suggest that sparse modeling via MoE provides compelling approach for scaling vision models, enabling more effective parameter utilization through conditional computation where specialized experts can focus on different visual patterns and representations. 6.2 Assessment of PEFT on LVMs Disney Organized Dataset. Table 8 demonstrates distinct efficiency-performance trade-offs among PEFT methods for the Wan 2.1-1.5B model. The full fine-tuning approach achieved the lowest loss (0.104) with optimal sample throughput (1.61 1010 Samples/param/s) and competitive latency (33.2042 s/iter), although with the highest memory usage (78.44 GB). GLORA provided good balance, showing competitive loss (0.143) with high throughput (1.61 1010 Samples/param/s) and reduced latency (33.1298 s/iter). WikiArt Sargent Dataset. In the case of the Stable Diffusion 3.5 Medium model, full fine-tuning achieved the best loss performance (0.204) and highest throughput (8.50 1010 Samples/param/s), despite significantly elevated memory usage (82.48 GB). LoHA and GLORA methods also performed well, maintaining low losses (0.215 and 0.217 respectively) and balanced latency around 4.6 s/iter, highlighting their suitability for applications demanding high computational efficiency without sacrificing performance. Overall, while full fine-tuning provides superior performance, it demands greater computational resources. Alternative methods such as GLORA and LoHA offer compelling trade-offs suitable for various deployment environments. 6.3 Assessment of PEFT on VLMs ChatQA Dataset. Table 8 highlights the efficiency-performance trade-offs of various Parameter-Efficient Fine-Tuning (PEFT) methods across different visual-language models. For the 7B-parameter LLaVA-1.5 model, LoRA-plus achieved the lowest loss (0.9716), demonstrating balanced efficiency with reasonable latency (7.1028 s/iter) and moderate energy consumption (541.45 W). Parameter freezing methods were not reported for this model. In the case of the Qwen2.5-VL-7B model, PISSA exhibited superior performance"
        },
        {
            "title": "6 SCALABILITY OF EFFICIENTLLM BENCHMARK",
            "content": "Table 12: Assessment of Training and Tuning Efficiency for VLMs of ChatQA Dataset (methods marked with * use DeepSpeed). Because of the different batch size, full* are not included in the comparisons. The best result is compared under the same model. Model Methods Loss AMU (GB) PCU AL (s/iter) ST (Samples/param/s) AEC (W) LLaVA-1.5 Qwen2.5-VL-7B Intern-VL-3-38B QvQ-Pre-72B lora rslora dora pissa full* lora rslora dora pissa full* lora 1.2796 lora-plus 0.9716 1.1541 1.0015 1.0549 1.1889 0.5672 lora-plus 0.5672 0.4363 0.5170 0.3156 0.6576 0.5943 lora-plus 0.5943 0.4760 0.4409 0.3635 0.5274 0.3548 lora-plus 0.6311 0.1434 0.3554 0.2143 0.3980 rslora dora pissa full* lora rslora dora pissa full* 20.064 45.216 45.6576 59.7728 45.4176 61.6992 46.3008 45.84 45.6384 61.4496 45.696 25.344 42.4704 42.5184 43.0272 60.6144 42.0192 69.2448 36.624 35.0464 40.1024 58.6512 42.6688 61.7088 ChatQA 7.794051 0.8942 7.102787 0.9853 6.987395 0.9891 0.9894 11.977185 6.952982 0.9894 9.784834 0.9374 9.270629 0.9918 8.918348 0.9889 8.82855 0.9888 0.9956 13.039291 0.9957 8.964483 0.8297 16.7143 0.9825 15.710554 0.9881 15.742757 0.9854 15.461066 20.866331 0.989 0.9877 15.848549 0.9753 18.485439 5.84746 0.268 0.8207 35.525615 8.44855 0.8732 8.394302 0.2573 0.8956 8.6275 12.3575 0.7895 1.2838 1010 1.4084 1010 1.4292 1010 8.3514 1011 1.4358 1010 1.0218 1010 1.2246 1010 1.2729 1010 1.2855 1010 8.7712 1011 1.2712 1010 1.5995 1011 1.3611 1011 1.3584 1011 1.3844 1011 1.0206 1011 1.3335 1011 1.1426 1011 1.9225 1011 3.1557 1012 1.3350 1011 1.3434 1011 1.3048 1011 9.0245 1012 512.27 541.45 522.86 548.88 524.85 484.52 403.29 403.70 419.00 548.59 405.08 354.44 530.84 523.62 533.49 551.23 526.92 478.33 374.27 348.08 381.33 330.30 369.51 352.67 with the lowest loss (0.3156) while maintaining competitive latency (8.9645 s/iter) and energy efficiency (405.08 W). Notably, full fine-tuning with DeepSpeed had significantly reduced memory utilization (25.344 GB) but incurred substantially higher latency (16.7143 s/iter), reflecting critical efficiency-performance trade-off. For the larger Intern-VL-3-38B model, PISSA again delivered strong results, showing the lowest loss (0.3635) among the evaluated methods, albeit with increased latency (15.8485 s/iter). DoRA presented higher latency (20.8663 s/iter) and elevated memory usage (60.6144 GB), limiting its practicality in latencysensitive scenarios. With the largest model QvQ-Pre-72B, RSLoRA outperformed other methods with the lowest loss (0.1434) and reasonable latency (8.4486 s/iter), suggesting it as highly effective approach for tuning extremely large models. Despite low AMU (35.0464 GB), LoRA-plus showed significantly higher latency (35.5256 s/iter), making it less favorable for latency-sensitive applications. Overall, LoRA variants, especially LoRA-plus and PISSA, consistently offer balanced efficiency-performance trade-offs suitable for diverse applications. RSLoRA emerges as particularly advantageous for large-scale model tuning, while computationally intensive approaches like DoRA and full fine-tuning require careful consideration based on specific deployment scenarios."
        },
        {
            "title": "7 RELATED WORK",
            "content": "Table 13: Assessment of Training and Tuning Efficiency for LVMs of Disney Organized and WikiArt Sargent Datasets (methods marked with * use DeepSpeed). Because of the different batch size, full* are not included in the comparisons. Model Methods Loss AMU (GB) PCU AL (s/iter) ST (Samples/param/s) AEC (W) Disney Organized 50.22 48.69 58.02 51.01 78.44 0.8942 44.342308 0.5824 42.430697 0.9940 45.551551 0.8213 33.129847 0.9027 33.204205 WikiArt Sargent 0.9536 15.30 0.7207 15.42 0.9556 17.26 0.7484 17.92 0.8439 82. 4.008191 4.673482 4.820688 4.632438 3.618949 0.136 0.125 0.139 0.143 0.104 0.225 0.215 0.229 0.217 0.204 1.20 1010 1.26 1010 1.17 1010 1.61 1010 1.61 1010 7.68 1010 6.58 1010 6.38 1010 6.64 1010 8.50 1010 512.27 566.11 648.91 593.23 518. 607.12 556.32 567.50 553.25 462.18 Wan 2.1-1.5B lora loha lokr glora full* Stable Diffusion 3.5 Medium lora loha lokr glora full*"
        },
        {
            "title": "7 Related Work",
            "content": "A wide range of efforts have emerged to improve the efficiency of large language models (LLMs) across their lifecycle [28, 301, 78, 302, 33, 303]. In this work we concentrate on three facets architecture-level pretraining optimizations, parameter-efficient fine-tuning, and inference-time quantization because these correspond to major efficiency challenges at different stages of an LLMs development and deployment. Each aspect addresses the needs of different stakeholders in practice: architecture and pretraining improvements guide model designers in building and training new LLMs under limited compute budgets; parameter-efficient fine-tuning (PEFT) methods help practitioners adapt big models to downstream tasks without retraining entire networks; and low bit-width quantization techniques assist deployment engineers in reducing serving costs and latency without requiring additional retraining. Below, we will highlight other important efficiency strategies not covered in detail (e.g. systems-level optimizations, alignment via RLHF, and test-time acceleration techniques), mainly clarify why they fall outside the scope of our study. Distributed Training and System-Level Optimizations. Training giant models efficiently at scale is as much systems engineering challenge as an algorithmic one. rich body of work exists on optimizing the infrastructure and parallelization for large-scale training. Approaches like data-parallel and model-parallel training (and hybrids thereof) allow spreading computation across many GPUs or TPUs. For example, Googles GPipe introduced generic pipeline parallelism to partition model across accelerators and achieved almost linear speedups when scaling an MLP and 6-billion Transformer across devices [304]. NVIDIAs MegatronLM [27] and Googles Mesh-TensorFlow [305] further refined tensor-slicing model-parallel approaches to train models with up to 100+ billion parameters (like the original GPT-3 [1]). In addition, the DeepSpeed library from Microsoft introduced the Zero Redundancy Optimizer (ZeRO) [306] which eliminates memory duplication of optimizer states and gradients across data-parallel workers. By offloading and partitioning states, ZeRO allows training models with hundreds of billions of parameters with high efficiency, even enabling 100+ billion models to be trained on modest GPU clusters with super-linear speedup. These system-level advances including optimized kernels (e.g. FlashAttention [72]), scheduling algorithms, and memory management techniques are crucial for making the training of cutting-edge LLMs possible at all. We do not explicitly benchmark these in our study because they often require specialized hardware setups or custom distributed training implementations beyond our end-to-end evaluation scope. In essence, our focus was on algorithmic techniques that single-team researcher or practitioner could apply within given infrastructure, whereas system-level optimizations involve entire training pipeline re-design and are orthogonal to the model-internal methods we examined. We refer interested readers to comprehensive system papers (e.g. PipeDream [307] and ZeRO [306]) for further details on this topic."
        },
        {
            "title": "7 RELATED WORK",
            "content": "Alignment and RLHF Efficiency. Large language models are typically fine-tuned after pretraining to better align with human preferences, follow instructions, and produce safe outputs. The dominant approach for this is Reinforcement Learning from Human Feedback (RLHF), exemplified by the InstructGPT and ChatGPT series [308]. InstructGPT showed that 1.3B parameter model fine-tuned with human preference data outperformed 175B GPT-3 on helpfulness and truthfulness. This highlights an efficiency of different sort alignment work can make smaller models behave as usefully as much larger ones, by optimizing for the right objective. However, RLHF itself is resource-intensive: it involves training reward model (often large network) and running many steps of policy optimization (e.g. PPO [309]) for the LLM, which can be as costly as regular fine-tuning. Recent research has proposed more sample-efficient or proxy methods for alignment, such as using AI feedback or distilled preference models, but these are still emerging [310, 311, 312, 313, 314]. We did not focus on RLHF in our benchmark because it targets output quality and safety more than runtime or training efficiency per se. Moreover, evaluating alignment quality requires human judgment or specialized metrics, which is outside our predominantly system performanceoriented evaluation criteria. In short, RLHF and other alignment techniques are critical in practice, but they involve distinct stage of the model lifecycle with goals (ethical and behavioral alignment) different from the core efficiency measures we target. Incorporating alignment efficiency (e.g. measuring the compute required for RLHF and how to reduce it) is an interesting direction for future work, though it likely requires an end-to-end infrastructure and human-in-theloop setup beyond the scope of our current study. Inference-Time Acceleration Strategies. number of techniques aim to speed up inference beyond just lowering bit precision. One such category is test-time optimizations that exploit the prediction process of LLMs. For example, speculative decoding has emerged as powerful approach to accelerate autoregressive generation [315, 316]. OpenAI has reported 2 3 speedups in GPT-3 using speculative decoding with smaller GPT-2 as the draft model [317]. Another technique is early exiting in the models forward pass [318, 319]. If intermediate layers of Transformer are equipped with prediction heads or confidence estimators, the model can choose to stop computation once it is sufficiently confident, instead of always running all layers. Elhoushi et al. combine this with form of self-speculative decoding in system called LayerSkip [320]. By training LLaMA models with progressively higher dropout in later layers and shared early-exit classifier, they enable the model to exit at an earlier layer for easy inputs and only use the full depth for hard cases. This yielded up to 2.02.2 speedups on tasks like summarization and code generation, with negligible performance loss. These dynamic inference methods are highly relevant to efficiency they essentially adapt the compute on the fly to match the inputs complexity or the models own confidence. We consider them complementary to our quantization and architecture-focused evaluations. In our study, we kept inference routines fixed (all models generate with the same decoder strategy) to ensure controlled comparison of techniques like quantization. Integrating speculative decoding or early-exit requires building additional components and policies around the model, which was beyond our current scope. Dynamic Routing and Model Cascades. related idea is deploying model cascades or multi-scale models at inference [321, 322]. For instance, one might use small model to handle simple queries and only invoke large model for more complex queries (a form of dynamic routing at the whole-model level). Similarly, mixture-of-experts (discussed above) can be viewed as dynamic routing within single forward pass experts are activated only as needed [323, 324]. These approaches can yield huge savings when there is variability in input difficulty or when many requests do not require the full capacity of the largest model. The challenge is designing reliable routing mechanisms that know when the big model is needed, without introducing too much overhead or too many errors. While our work did not explore such conditional computation at inference time, we acknowledge it as an important research frontier. Successfully deploying conditional LLM inference (whether via cascades, early-exits, or MoE gating) could drastically improve real-world efficiency by ensuring we pay the cost of 100B+ model only when necessary. In summary, beyond the specific techniques evaluated in our study, the literature offers spectrum of strategies to tackle LLM efficiency from multiple angles. Training-time system optimizations, alignment-focused finetuning, and clever decoding-time methods all contribute to the overall goal of making LLMs more practical and sustainable. We focused on architecture, fine-tuning, and quantization as representative axes that span the models lifecycle and are widely applicable under uniform evaluation settings. The insights from our benchmark can thus be seen as one piece of the puzzle, complementing the above lines of work. Future"
        },
        {
            "title": "7 RELATED WORK",
            "content": "research will hopefully integrate these layers for example, applying system optimizations to efficiently train models with new architectures, or combining PEFT and quantization with speculative decoding for maximum inference speed-up. Such holistic exploration will be vital as the community continues to push the limits of large language model capabilities under real-world resource constraints. Note: Although we strive to present representative overview of efficient LLM research, our discussion is by no means comprehensive. The landscape of efficiency techniquesspanning algorithmic, system-level, and application-specific innovationsis vast and rapidly evolving. Due to space and scope constraints, we have selected the dimensions that are most relevant to our empirical evaluation. As many promising techniques and insights are beyond the scope of this paper, readers refer to this work as focused discussion rather than comprehensive survey."
        },
        {
            "title": "8 Discussion",
            "content": "While our study provides comprehensive empirical evaluation of efficiency techniques across multiple dimensions, achieving truly compute-aware large-model design and deployment remains an open challenge. In this section, we first acknowledge several limitations of our current work and then articulate key open challenges and promising future research directions."
        },
        {
            "title": "8.1 Limitations",
            "content": "Our empirical benchmark and analysis have several limitations that should be considered when interpreting the results: Limited Coverage of Efficiency Techniques. Although we extensively evaluated multiple efficiency strategies, our analysis does not encompass all existing techniques. For instance, we have not explicitly considered optimizations related to sequence length management, such as efficient handling of ultralong-context models, KV-cache optimizations, and strategies for reducing memory overhead in attention mechanisms. These techniques can significantly impact computational efficiency, especially in scenarios involving extremely long input sequences during pretraining and inference. Hardware and Infrastructure Constraints. Our experiments were conducted primarily on specific GPU cluster configuration (48GH200 + 8H200 GPUs). Different hardware setups, such as TPU-based systems, CPU-only clusters, or heterogeneous computing environments, may yield different efficiency trade-offs, particularly during large-scale pretraining. Thus, our findings may not fully generalize to all possible deployment scenarios. Limited Scope of Models and Tasks. Although we evaluated diverse set of models across language, vision, and multimodal domains, our selection does not cover all existing architectures and tasks. Certain specialized models or niche application scenarios may exhibit unique efficiency characteristics not captured in our current evaluation, especially during the pretraining phase. Static Evaluation Metrics. Our proposed metrics, while comprehensive, are primarily static and averaged over training or inference processes. Dynamic or adaptive metrics that capture real-time fluctuations in resource utilization, latency spikes, or transient bottlenecks could provide additional insights into efficiency optimization. Absence of Economic Analysis. Our evaluation focuses on computational and energy efficiency metrics without explicitly considering economic factors such as hardware acquisition costs, operational expenses, or cloud computing pricing models. Incorporating these economic dimensions could further enhance the practical relevance of our efficiency assessments. 8.2 Open Challenges and Future Directions Beyond the limitations above, we highlight several critical open challenges and promising research directions for future work: Multi-objective Scaling Laws. Classic scaling laws (e.g., Chinchilla) minimize cross-entropy loss under scalar compute constraint, implicitly assuming FLOPs as the sole budget. Real-world deployments require balancing multiple orthogonal objectives such as latency, memory, energy, and carbon emissions. Developing vector-valued scaling laws that map parameters and tokens onto an efficiency Pareto frontier remains unexplored. Heterogeneous-quality Corpora. At trillion-token scales, datasets contain diverse quality levels, from curated books to noisy web text. Current heuristics ignore fine-grained variance. Efficient token-level entropy estimators and dynamic importance sampling methods are needed to optimize training efficiency and quality simultaneously. Curriculum Design for Long-context Pretraining. Models with extremely long contexts (32k128k tokens) require principled curriculum strategies beyond simple heuristics. Addressing memory bandwidth constraints, positional encoding dynamics, gradient staleness, and downstream coherence remains challenging."
        },
        {
            "title": "8 DISCUSSION",
            "content": "Sparse Routing under Hard Memory Ceilings. Mixture-of-Experts (MoE) architectures reduce FLOPs but increase KV-cache memory usage. Developing unified theoretical frameworks and memory-aware routing mechanisms that dynamically balance compute and memory remains an open frontier. Efficient Optimization of Non-Transformer Backbones. Alternative architectures (e.g., Mamba, RWKV) promise sub-quadratic scaling but lack optimized kernels and adaptive optimizers. Establishing standardized benchmarks for fair comparisons against Transformers is essential. PEFT for Multi-modal and Tool-augmented LLMs. Parameter-efficient fine-tuning (PEFT) methods like LoRA perform well in pure language settings but struggle across modalities. Designing unified adapters that generalize across vision, audio, and code modalities remains challenging. Robust Post-training Quantization for Ultra-long Contexts. Current quantization schemes (e.g., int4) degrade significantly with activation outliers in long sequences. Developing robust joint weightactivation quantizers and comprehensive error-propagation theories is critical. Holistic, End-to-end Efficiency Evaluation. Existing benchmarks often cherry-pick metrics and hardware setups. reproducible, standardized efficiency benchmarking framework capturing latency, throughput, energy, and memory across diverse hardware and software configurations is urgently needed. Continual and Federated Pretraining under Privacy Constraints. Regulatory requirements increasingly demand on-premises data handling. Balancing compute-optimal token budgets with privacy guarantees (e.g., differential privacy) through federated learning or secure aggregation remains challenging. Hardware-aware Training Schedules. Heterogeneous GPU clusters complicate manual scheduling. Developing auto-schedulers that dynamically optimize parallelism strategies (data, tensor, pipeline, expert) across diverse hardware configurations is an active research area. Solving these interlocking challenges demands concerted effort that spans theory, optimization, systems, and hardware co-design. Only then will we unlock the next order-of-magnitude leap in large-model efficiency."
        },
        {
            "title": "9 Conclusion",
            "content": "In this work, we presented EfficientLLM, the first comprehensive, large-scale empirical study systematically evaluating efficiency techniques for LLMs, with particular focus on architecture pretraining efficiency, as well as scalability evaluations across language, vision, and multimodal domains. Experimental Setup. We conducted extensive benchmarking on over 100 modeltechnique combinations using large-scale GPU cluster comprising 48GH200 + 8H200 GPUs. Our evaluation spanned five critical efficiency dimensions: budget allocation, data efficiency, architectural design, training and tuning strategies, and inference optimization. Metrics. To provide holistic and practical assessment, we introduced and utilized six fine-grained efficiency metrics: AMU (Active Memory Utilization), PCU (Peak Compute Utilization), AL (Average Latency), AT (Average Throughput), AEC (Average Energy Consumption), and MCR (Memory Consumption Ratio). These metrics collectively capture the nuanced trade-offs among computational resource usage, energy efficiency, latency, and throughput. Key Findings. Our extensive empirical analysis yielded several critical insights: No universally optimal method exists. Efficiency techniques inherently involve trade-offs; improvements in one dimension typically incur costs in another. Practitioners must therefore navigate multi-objective Pareto frontier rather than relying on single, universally optimal solution. Attention mechanisms must be selected based on specific bottlenecks during pretraining. Different efficient attention variants excel under distinct constraints in the pretraining stage: Multi-Query Attention (MQA) provides the best balance between memory usage and latency; Multi-Linear Attention (MLA) achieves the lowest perplexity; and Nystrm Self-Attention (NSA) minimizes power consumption. Effectiveness of Parameter-Efficient Fine-Tuning (PEFT) methods varies significantly with model scale. LoRA methods are most effective for smaller models (below 3B parameters), RSLoRA demonstrates superior performance at larger scales (14B parameters and above), and simple parameter freezing techniques offer the lowest latency when moderate performance degradation is acceptable. Low-precision numerical formats substantially enhance efficiency on modern hardware. Post-training quantization to Int4 precision significantly improves resource utilization, quadrupling effective VRAM capacity and tripling throughput with only minimal (35 percentage points) performance degradation. Additionally, bfloat16 consistently outperforms float16 on Hopper GPU architectures, highlighting the importance of hardware-aware numerical precision selection. Practical Implications. These findings provide clear, actionable guidelines for researchers and practitioners aiming to optimize large generative models. By carefully selecting scaling strategies, attention mechanisms, fine-tuning methods, and numerical precision based on specific deployment constraints and efficiency targets, practitioners can significantly enhance the practical usability and sustainability of large-scale generative models. Ultimately, our work underscores the necessity of holistic, multi-dimensional approach to efficiency optimization, paving the way for future research and practical advancements in the deployment of efficient, scalable, and sustainable generative AI systems."
        },
        {
            "title": "References",
            "content": "[1] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. [2] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022. [3] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. [4] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training computeoptimal large language models. arXiv preprint arXiv:2203.15556, 2022. [5] Keivalya Pandya and M. Holia. Automating customer service using langchain: Building custom open-source gpt chatbot for organizations. ArXiv, abs/2310.05421, 2023. [6] Shubham Agarwal, Issam Laradji, Laurent Charlin, and Christopher Pal. Litllm: toolkit for scientific literature review. arXiv preprint arXiv:2402.01788, 2024. [7] Jiajun Xu, Qun Wang, Yuhang Cao, Baitao Zeng, and Sicheng Liu. general purpose device for interaction with llms. In Proceedings of the Future Technologies Conference, pages 613626. Springer, 2024. [8] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. NeurIPS, 35:2219922213, 2022. [9] Yang Liu, Yuanshun Yao, Jean-Francois Ton, Xiaoying Zhang, Ruocheng Guo Hao Cheng, Yegor Klochkov, Muhammad Faaiz Taufiq, and Hang Li. Trustworthy llms: survey and guideline for evaluating large language models alignment. arXiv preprint arXiv:2308.05374, 2023. [10] Yue Huang, Lichao Sun, Haoran Wang, Siyuan Wu, Qihui Zhang, Yuan Li, Chujie Gao, Yixin Huang, Wenhan Lyu, Yixuan Zhang, Xiner Li, Hanchi Sun, Zhengliang Liu, Yixin Liu, Yijue Wang, Zhikun Zhang, Bertie Vidgen, Bhavya Kailkhura, Caiming Xiong, Chaowei Xiao, Chunyuan Li, Eric P. Xing, Furong Huang, Hao Liu, Heng Ji, Hongyi Wang, Huan Zhang, Huaxiu Yao, Manolis Kellis, Marinka Zitnik, Meng Jiang, Mohit Bansal, James Zou, Jian Pei, Jian Liu, Jianfeng Gao, Jiawei Han, Jieyu Zhao, Jiliang Tang, Jindong Wang, Joaquin Vanschoren, John Mitchell, Kai Shu, Kaidi Xu, Kai-Wei Chang, Lifang He, Lifu Huang, Michael Backes, Neil Zhenqiang Gong, Philip S. Yu, Pin-Yu Chen, Quanquan Gu, Ran Xu, Rex Ying, Shuiwang Ji, Suman Jana, Tianlong Chen, Tianming Liu, Tianyi Zhou, William Yang Wang, Xiang Li, Xiangliang Zhang, Xiao Wang, Xing Xie, Xun Chen, Xuyu Wang, Yan Liu, Yanfang Ye, Yinzhi Cao, Yong Chen, and Yue Zhao. Position: TrustLLM: Trustworthiness in large language models. In Ruslan Salakhutdinov, Zico Kolter, Katherine Heller, Adrian Weller, Nuria Oliver, Jonathan Scarlett, and Felix Berkenkamp, editors, Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pages 2016620270. PMLR, 2127 Jul 2024. [11] Yue Huang, Chujie Gao, Siyuan Wu, Haoran Wang, Xiangqi Wang, Yujun Zhou, Yanbo Wang, Jiayi Ye, Jiawen Shi, Qihui Zhang, et al. On the trustworthiness of generative foundation models: Guideline, assessment, and perspective. arXiv preprint arXiv:2502.14296, 2025. [12] Emma Strubell, Ananya Ganesh, and Andrew McCallum. Energy and policy considerations for deep learning in NLP. In Proceedings of the 57th Annual Meeting of the ACL, pages 36453650, 2019. [13] Zhongwei Wan, Xin Wang, Che Liu, Samiul Alam, Yu Zheng, et al. Efficient large language models: survey. arXiv preprint arXiv:2312.03863, 2023. [14] Guangji Bai, Zheng Chai, Chen Ling, et al. Beyond efficiency: systematic survey of resource-efficient large language models. ArXiv preprint arXiv:2401.00625, 2024."
        },
        {
            "title": "REFERENCES",
            "content": "[15] Zixuan Zhou, Xuefei Ning, Ke Hong, Tianyu Fu, Jiaming Xu, Shiyao Li, Yuming Lou, Luning Wang, Zhihang Yuan, Xiuhong Li, Shengen Yan, Guohao Dai, Xiao-Ping Zhang, Yuhan Dong, and Yu Wang. survey on efficient inference for large language models. ArXiv, abs/2404.14294, 2024. [16] Yucheng Li, Bo Dong, Chenghua Lin, and Frank Guerin. Compressing context to enhance inference efficiency of large language models. pages 63426353, 2023. [17] Tianyu Ding, Tianyi Chen, Haidong Zhu, Jiachen Jiang, Yiqi Zhong, Jinxin Zhou, Guangzhi Wang, Zhihui Zhu, Ilya Zharkov, and Luming Liang. The efficiency spectrum of large language models: An algorithmic survey. arXiv preprint arXiv:2312.00678, 2024. [18] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509, 2019. [19] Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with conditional computation and automatic sharding. arXiv preprint arXiv:2006.16668, 2020. [20] William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. The Journal of Machine Learning Research, 23(1):52325270, 2022. [21] Noam Shazeer. Fast transformer decoding: One write-head is all you need. arXiv preprint arXiv:1911.02150, 2019. [22] Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebrn, and Sumit Sanghai. Gqa: Training generalized multi-query transformer models from multi-head checkpoints. arXiv preprint arXiv:2305.13245, 2023. [23] Aixin Liu, Bei Feng, Bin Wang, Bingxuan Wang, Bo Liu, Chenggang Zhao, Chengqi Dengr, Chong Ruan, Damai Dai, Daya Guo, et al. Deepseek-v2: strong, economical, and efficient mixture-of-experts language model. arXiv preprint arXiv:2405.04434, 2024. [24] Jingyang Yuan, Huazuo Gao, Damai Dai, Junyu Luo, Liang Zhao, Zhengyan Zhang, Zhenda Xie, YX Wei, Lean Wang, Zhiping Xiao, et al. Native sparse attention: Hardware-aligned and natively trainable sparse attention. arXiv preprint arXiv:2502.11089, 2025. [25] Paulius Micikevicius, Sharan Narang, Jonah Alben, Greg Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, et al. Mixed precision training. In International Conference on Learning Representations (ICLR), 2018. [26] Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In SIGKDD, pages 35053506, 2020. [27] Mohammad Shoeybi, Mostofa Ali Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-LM: Training multi-billion parameter language models using model parallelism. arXiv preprint arXiv:1909.08053, 2019. [28] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models, 2021. [29] Hyeon-Woo Nam, Ye-Bin Moon, and Tae-Hyun Oh. Fedpara: Low-rank hadamard product for Introduces communication-efficient federated learning. arXiv preprint arXiv:2108.06098, 2021. the LoHa variant. [30] Ali Edalati, Marzieh Tahaei, Ivan Kobyzev, Vahid Partovi Nia, James J. Clark, and Mehdi Rezagholizadeh. Krona: Parameter efficient tuning with kronecker adapter. arXiv preprint arXiv:2212.10650, 2022. Basis for the LoKr variant. [31] Arnav Chavan, Zhuang Liu, Deepak Gupta, Eric Xing, and Zhiqiang Shen. One-for-all: Generalized lora for parameter-efficient fine-tuning. arXiv preprint arXiv:2306.07967, 2023. [32] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. LLM.int8(): 8-bit matrix multiplication for transformers at scale. In Advances in Neural Information Processing Systems 35 (NeurIPS 2022), pages 1250812521, 2022."
        },
        {
            "title": "REFERENCES",
            "content": "[33] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq: Accurate post-training quantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323, 2022. [34] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning of quantized llms, 2023. [35] Elias Frantar and Dan Alistarh. Sparsegpt: Massive language models can be accurately pruned in one-shot, 2023. [36] Victor Sanh, Thomas Wolf, and Alexander M. Rush. Movement pruning: Adaptive sparsity by finetuning. In Advances in Neural Information Processing Systems 33, 2020. [37] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in neural network. arXiv preprint arXiv:1503.02531, 2015. [38] Yaniv Leviathan, Matan Kalman, and Yossi Matias. Fast inference from transformers via speculative decoding. In Proceedings of the 40th International Conference on Machine Learning (ICML), 2023. [39] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the 29th ACM Symposium on Operating Systems Principles (SOSP), 2023. [40] Albert Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024. [41] Xiaoxia Wu, Cheng Li, Reza Yazdani Aminabadi, Zhewei Yao, and Yuxiong He. Understanding int4 quantization for language models: latency speedup, composability, and failure cases. In International Conference on Machine Learning, pages 3752437539. PMLR, 2023. [42] David Wolpert and William Macready. No free lunch theorems for optimization. IEEE transactions on evolutionary computation, 1(1):6782, 1997. [43] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. In Proceedings of the 10th International Conference on Learning Representations (ICLR), 2022. [44] Shih-Yang Liu, Chien-Yi Wang, Hongxu Yin, Pavlo Molchanov, Yu-Chiang Frank Wang, Kwang-Ting Cheng, and Min-Hung Chen. Dora: Weight-decomposed low-rank adaptation. In Proceedings of the 41st International Conference on Machine Learning (ICML), 2024. [45] Damjan Kalajdzievski. rank stabilization scaling factor for fine-tuning with lora. arXiv preprint arXiv:2312.03732, 2023. [46] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [47] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training. 2018. [48] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. [49] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies, volume 1 (long and short papers), pages 41714186, 2019. [50] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: robustly optimized bert pretraining approach, 2019. [51] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. Albert: lite bert for self-supervised learning of language representations. arXiv preprint arXiv:1909.11942, 2019."
        },
        {
            "title": "REFERENCES",
            "content": "[52] Wei Xu, Jue Xiao, and Jianlong Chen. Leveraging large language models to enhance personalized recommendations in e-commerce. 2024. [53] Yafei Xiang, Hanyi Yu, Yulu Gong, Shuning Huo, and Mengran Zhu. Text understanding and generation using transformer models for intelligent e-commerce recommendations. ArXiv, abs/2402.16035, 2024. [54] Ipsita Mohanty. Recommendation systems in the era of llms. Proceedings of the 15th Annual Meeting of the Forum for Information Retrieval Evaluation, 2023. [55] Wenqi Fan, Zihuai Zhao, Jiatong Li, Yunqing Liu, Xiaowei Mei, Yiqi Wang, Jiliang Tang, and Qing Li. Recommender systems in the era of large language models (llms). IEEE Transactions on Knowledge and Data Engineering, 36:68896907, 2023. [56] Qingyao Li, Lingyue Fu, Weiming Zhang, Xianyu Chen, Jingwei Yu, Wei Xia, Weinan Zhang, Ruiming Tang, and Yong Yu. Adapting large language models for education: Foundational capabilities, potentials, and challenges, 2023. [57] Shen Wang, Tianlong Xu, Hang Li, Chaoli Zhang, Joleen Liang, Jiliang Tang, Philip S. Yu, and Qingsong Wen. Large language models for education: survey and outlook, 2024. [58] Zheyuan Zhang, Daniel Zhang-Li, Jifan Yu, Linlu Gong, Jinchang Zhou, Zhanxin Hao, Jianxiao Jiang, Jie Cao, Huiqin Liu, Zhiyuan Liu, et al. Simulating classroom education with llm-empowered agents. arXiv preprint arXiv:2406.19226, 2024. [59] Juyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim. survey on large language models for code generation. arXiv preprint arXiv:2406.00515, 2024. [60] Matthew DeLorenzo, Animesh Basak Chowdhury, Vasudev Gohil, Shailja Thakur, Ramesh Karri, Siddharth Garg, and Jeyavijayan Rajendran. Make every move count: Llm-based high-quality rtl code generation using mcts. arXiv preprint arXiv:2402.03289, 2024. [61] Weixin Liang, Yaohui Zhang, Zhengxuan Wu, Haley Lepp, Wenlong Ji, Xuandong Zhao, Hancheng Cao, Sheng Liu, Siyu He, Zhi Huang, et al. Mapping the increasing use of llms in scientific papers. arXiv preprint arXiv:2404.01268, 2024. [62] Youngsuk Park, Kailash Budhathoki, Liangfu Chen, Jonas Kbler, Jiaji Huang, Matthus Kleindessner, Jun Huan, Volkan Cevher, Yida Wang, and George Karypis. Inference optimization of foundation In Proceedings of the 30th ACM SIGKDD Conference on Knowledge models on ai accelerators. Discovery and Data Mining, pages 66056615, 2024. [63] Zhantong Zhu, Hongou Li, Wenjie Ren, Meng Wu, Le Ye, Ru Huang, and Tianyu Jia. Leveraging compute-in-memory for efficient generative model inference in tpus. arXiv preprint arXiv:2503.00461, 2025. [64] Norman Jouppi, Cliff Young, Nishant Patil, David Patterson, Gaurav Agrawal, Raminder Bajwa, Sarah Bates, Suresh Bhatia, Nan Boden, Al Borchers, et al. In-datacenter performance analysis of tensor processing unit. In Proceedings of the 44th annual international symposium on computer architecture, pages 112, 2017. [65] Seyyed Amirhossein Saeidi, Forouzan Fallah, Soroush Barmaki, and Hamed Farbeh. novel neuromorphic processors realization of spiking deep reinforcement learning for portfolio management. In 2022 Design, Automation & Test in Europe Conference & Exhibition (DATE), pages 6871. IEEE, 2022. [66] Thomas Ferreira De Lima, Hsuan-Tung Peng, Alexander Tait, Mitchell Nahmias, Heidi Miller, Bhavin Shastri, and Paul Prucnal. Machine learning with neuromorphic photonics. Journal of Lightwave Technology, 37(5):15151534, 2019. [67] Jiangfei Duan, Shuo Zhang, Zerui Wang, Lijuan Jiang, Wenwen Qu, Qinghao Hu, Guoteng Wang, Qizhen Weng, Hang Yan, Xingcheng Zhang, et al. Efficient training of large language models on distributed infrastructures: survey. arXiv preprint arXiv:2407.20018, 2024. [68] Joost Verbraeken, Matthijs Wolting, Jonathan Katzy, Jeroen Kloppenburg, Tim Verbelen, and Jan Rellermeyer. survey on distributed machine learning. Acm computing surveys (csur), 53(2):133, 2020."
        },
        {
            "title": "REFERENCES",
            "content": "[69] Yu Cheng, Duo Wang, Pan Zhou, and Tao Zhang. survey of model compression and acceleration for deep neural networks. arXiv preprint arXiv:1710.09282, 2017. [70] Antonio Polino, Razvan Pascanu, and Dan Alistarh. Model compression via distillation and quantization. arXiv preprint arXiv:1802.05668, 2018. [71] Xunyu Zhu, Jian Li, Yong Liu, Can Ma, and Weiping Wang. survey on model compression for large language models. Transactions of the Association for Computational Linguistics, 12:15561577, 2024. [72] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R. Flashattention: Fast and memoryefficient exact attention with io-awareness. NeurIPS, 35:1634416359, 2022. [73] Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. arXiv preprint arXiv:2307.08691, 2023. [74] Daniel Snider and Ruofan Liang. Operator fusion in xla: analysis and evaluation. arXiv preprint arXiv:2301.13062, 2023. [75] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in neural network. arXiv preprint arXiv:1503.02531, 2015. [76] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Kttler, Mike Lewis, Wen-tau Yih, Tim Rocktschel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in neural information processing systems, 33:94599474, 2020. [77] Jianping Gou, Baosheng Yu, Stephen Maybank, and Dacheng Tao. Knowledge distillation: survey. International Journal of Computer Vision, 129:17891819, 2021. [78] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, distilled version of bert: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108, 2019. [79] Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, et al. Improving language models by retrieving from trillions of tokens. In International conference on machine learning, pages 22062240. PMLR, 2022. [80] Opher Lieber, Or Sharir, Barak Lenz, and Yoav Shoham. Jurassic-1: Technical details and evaluation. White Paper. AI21 Labs, 1(9):117, 2021. [81] Jiarui Li, Ye Yuan, and Zehua Zhang. Enhancing llm factual accuracy with rag to counter hallucinations: case study on domain-specific queries in private knowledge-bases. arXiv preprint arXiv:2403.10446, 2024. [82] Guangzhi Xiong, Qiao Jin, Zhiyong Lu, and Aidong Zhang. Benchmarking retrieval-augmented generation for medicine. In Findings of the Association for Computational Linguistics ACL 2024, pages 62336251, 2024. [83] Iz Beltagy, Matthew Peters, and Arman Cohan. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150, 2020. [84] Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. Big bird: Transformers for longer sequences. NeurIPS, pages 1728317297, 2020. [85] Nikita Kitaev, ukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. ICLR, 2021. [86] Saeed Masoudnia and Reza Ebrahimpour. Mixture of experts: literature survey. Artificial Intelligence Review, 42:275293, 2014. [87] Zhengyan Zhang, Chaojun Xiao, Qiujieli Qin, Yankai Lin, Zhiyuan Zeng, Xu Han, Zhiyuan Liu, Ruobing Xie, Maosong Sun, and Jie Zhou. Exploring the benefit of activation sparsity in pre-training. arXiv preprint arXiv:2410.03440, 2024. [88] Xi Victoria Lin, Akshat Shrivastava, Liang Luo, Srinivasan Iyer, Mike Lewis, Gargi Ghosh, Luke Zettlemoyer, and Armen Aghajanyan. Moma: Efficient early-fusion pre-training with mixture of modality-aware experts. arXiv preprint arXiv:2407.21770, 2024."
        },
        {
            "title": "REFERENCES",
            "content": "[89] Diederik Kingma and Jimmy Ba. Adam: method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. [90] John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of machine learning research, 12(7), 2011. [91] Yang You, Jing Li, Sashank Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh Bhojanapalli, Xiaodan Song, James Demmel, Kurt Keutzer, and Cho-Jui Hsieh. Large batch optimization for deep learning: Training bert in 76 minutes. arXiv preprint arXiv:1904.00962, 2019. [92] Hadi Jomaa, Josif Grabocka, and Lars Schmidt-Thieme. Hyp-rl: Hyperparameter optimization by reinforcement learning. arXiv preprint arXiv:1906.11527, 2019. [93] Jie You, Jae-Won Chung, and Mosharaf Chowdhury. Zeus: Understanding and optimizing GPU energy consumption of DNN training. In USENIX NSDI, 2023. [94] Tom Zahavy, Zhongwen Xu, Vivek Veeriah, Matteo Hessel, Junhyuk Oh, Hado van Hasselt, David Silver, and Satinder Singh. self-tuning actor-critic algorithm. Advances in neural information processing systems, 33:2091320924, 2020. [95] Pengzhen Ren, Yun Xiao, Xiaojun Chang, Po-Yao Huang, Zhihui Li, Xiaojiang Chen, and Xin Wang. comprehensive survey of neural architecture search: Challenges and solutions. ACM Computing Surveys (CSUR), 54(4):134, 2021. [96] Mingxing Tan and Quoc Le. Efficientnet: Rethinking model scaling for convolutional neural networks. In International conference on machine learning, pages 61056114. PMLR, 2019. [97] Alex Havrilla, Yuqing Du, Sharath Chandra Raparthy, Christoforos Nalmpantis, Jane Dwivedi-Yu, Maksym Zhuravinskyi, Eric Hambro, Sainbayar Sukhbaatar, and Roberta Raileanu. Teaching large language models to reason with reinforcement learning. arXiv preprint arXiv:2403.04642, 2024. [98] Han Zhong, Zikang Shan, Guhao Feng, Wei Xiong, Xinle Cheng, Li Zhao, Di He, Jiang Bian, and Liwei Wang. Dpo meets ppo: Reinforced token optimization for rlhf. arXiv preprint arXiv:2404.18922, 2024. [99] Banghua Zhu, Hiteshi Sharma, Felipe Vieira Frujeri, Shi Dong, Chenguang Zhu, Michael Jordan, and Jiantao Jiao. Fine-tuning language models with advantage-induced policy alignment. arXiv preprint arXiv:2306.02231, 2023. [100] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. 2023. [101] Tianyu Ding, Tianyi Chen, Haidong Zhu, Jiachen Jiang, Yiqi Zhong, Jinxin Zhou, Guangzhi Wang, Zhihui Zhu, Ilya Zharkov, and Luming Liang. The efficiency spectrum of large language models: An algorithmic survey. 2023. [102] Emma Strubell, Ananya Ganesh, and Andrew McCallum. Energy and policy considerations for deep learning in nlp. 2019. [103] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. 2020. [104] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre. Training compute-optimal large language models. 2022. [105] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. 2023. [106] Hongyu Wang, Shuming Ma, Li Dong, Shaohan Huang, Dongdong Zhang, and Furu Wei. Deepnet: Scaling transformers to 1,000 layers. 2022."
        },
        {
            "title": "REFERENCES",
            "content": "[107] Erik D. Demaine, David Eppstein, Adam Hesterberg, Kshitij Jain, Anna Lubiw, Ryuhei Uehara, and Yushi Uno. Reconfiguring undirected paths. 2019. [108] Angelos Katharopoulos and Francois Fleuret. Not all samples are created equal: Deep learning with importance sampling. 2018. [109] Axel Gruenrock. On the generalized zakharov-kuznetsov equation at critical regularity. 2015. [110] Jeffrey Elman. Learning and development in neural networks: The importance of starting small. Cognition, 48(1):7199, 1993. [111] Yoshua Bengio, Jrme Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In ICML, pages 4148, 2009. [112] Rui Huang, Yuanjie Zheng, Zhiqiang Hu, Shaoting Zhang, and Hongsheng Li. Multi-organ segmentation via co-training weight-averaged models from few-organ datasets. 2020. [113] Kumar, Benjamin Packer, and Daphne Koller. Self-paced learning for latent variable models. NeurIPS, 23, 2010. [114] Toufique Ahmed, Dian Yu, Chengxuan Huang, Cathy Wang, Prem Devanbu, and Kenji Sagae. Towards understanding what code language models learned. arXiv preprint arXiv:2306.11943, 2023. [115] Egor Bogomolov, Aleksandra Eliseeva, Timur Galimzyanov, Evgeniy Glukhov, Anton Shapkin, Maria Tigina, Yaroslav Golubev, Alexander Kovrigin, Arie van Deursen, Maliheh Izadi, and Timofey Bryksin. Long code arena: set of benchmarks for long-context code models. arXiv preprint arXiv:2406.11612, 2024. 54 pages, 4 figures, 22 tables. [116] Vahid Majdinasab, Amin Nikanjam, and Foutse Khomh. Deepcodeprobe: Towards understanding what models trained on code learn. arXiv preprint arXiv:2407.08890, 2024. [117] Tyler A. Chang, Catherine Arnett, Zhuowen Tu, and Benjamin K. Bergen. When is multilinguality curse? language modeling for 250 highand low-resource languages. arXiv preprint arXiv:2311.09205, 2023. [118] Fahim Faisal and Antonios Anastasopoulos. An efficient approach for studying cross-lingual transfer in multilingual language models. arXiv preprint arXiv:2403.20088, 2024. [119] Abteen Ebrahimi and Kenneth Church. Since the scientific literature is multilingual, our models should be too. arXiv preprint arXiv:2403.18251, 2024. [120] Hellina Hailu Nigatu, Atnafu Lambebo Tonja, and Jugal Kalita. The less the merrier? investigating language representation in multilingual models. arXiv preprint arXiv:2310.13228, 2023. Accepted to EMNLP 2023(Findings). [121] Bastien Latard, Jonathan Weber, Germain Forestier, and Michel Hassenforder. Towards semantic search engine for scientific articles. arXiv preprint arXiv:1709.09836, 2017. [122] Nicholas Solovyev, Ryan Barron, Manish Bhattarai, Maksim E. Eren, Kim O. Rasmussen, and Boian S. Alexandrov. Interactive distillation of large single-topic corpora of scientific papers. arXiv preprint arXiv:2309.10772, 2023. Accepted at 2023 IEEE ICMLA conference. [123] Palak Jain, Livio Baldini Soares, and Tom Kwiatkowski. From rag to riches: Retrieval interlaced with sequence generation. arXiv preprint arXiv:2407.00361, 2024. 18 pages, 3 figures, Preprint. [124] A. Author and B. Author. Deepseek-r1: Emergent reasoning in reinforcement learning fine-tuned large language models, 2025. arXiv preprint. Available at https://arxiv.org/abs/XXXX.XXXX. [125] C. Author and D. Author. Kimi k1.5: curriculum-based approach for enhancing reasoning in multi-modal llms, 2025. Open-source project. Available at https://github.com/MoonshotAI/Kimi-k1.5. [126] E. Author and F. Author. Wisdom: Progressive curriculum data synthesis for enhancing reasoning in large language models, 2024. arXiv preprint. Available at https://arxiv.org/abs/XXXX.XXXX. [127] G. Author and H. Author. Lbs3: Curriculum-inspired prompting for automated reasoning in large language models, 2024. arXiv preprint. Available at https://arxiv.org/abs/XXXX.XXXX."
        },
        {
            "title": "REFERENCES",
            "content": "[128] I. Author and J. Author. Curllm-reasoner: curriculum reasoning framework for visual and language models. In Proceedings of the 2024 ACM SIGKDD Conference on Knowledge Discovery and Data Mining. ACM, 2024. Available at https://doi.org/10.1145/XXXXXX. [129] K. Author and L. Author. Logic-rl: curriculum learning approach for reinforcement learning on logic puzzles, 2025. arXiv preprint. Available at https://arxiv.org/abs/XXXX.XXXX. [130] M. Author and N. Author. Alphallm-cpl: Curriculum preference learning for enhanced reasoning via mcts in llms, 2024. arXiv preprint. Available at https://arxiv.org/abs/XXXX.XXXX. [131] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers. 2019. [132] Iz Beltagy, Matthew E. Peters, and Arman Cohan. Longformer: The long-document transformer. 2020. [133] Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, and Amr Ahmed. Big bird: Transformers for longer sequences. Neural Information Processing Systems (NeurIPS) 2020, 2020. [134] Sinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with linear complexity. 2020. [135] Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn Fung, Yin Li, and Vikas Singh. Nystromformer: nystrom-based algorithm for approximating self-attention. 2021. [136] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, David Belanger, Lucy Colwell, and Adrian Weller. Rethinking attention with performers. 2020. [137] Nikita Kitaev, ukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. 2020. [138] Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Re. Flashattention: Fast and memory-efficient exact attention with io-awareness. 2022. [139] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [140] Ofir Press, Noah Smith, and Mike Lewis. Train short, test long: Attention with linear biases enables input length extrapolation. ICLR, 2023. [141] Ta-Chung Chi, Ting-Han Fan, Peter Ramadge, and Alexander Rudnicky. Kerple: Kernelized relative positional embedding for length extrapolation. NeurIPS, 35:83868399, 2022. [142] Ta-Chung Chi, Ting-Han Fan, Alexander Rudnicky, and Peter Ramadge. Dissecting transformer length extrapolation via the lens of receptive field analysis. In ACL, pages 1352213537, 2023. [143] Shanda Li, Chong You, Guru Guruganesh, Joshua Ainslie, Santiago Ontanon, Manzil Zaheer, Sumit Sanghai, Yiming Yang, Sanjiv Kumar, and Srinadh Bhojanapalli. Functional interpolation for relative positions improves long context transformers. arXiv preprint arXiv:2310.04418, 2023. [144] Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. arXiv preprint arXiv:2104.09864, 2021. [145] Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarn: Efficient context window extension of large language models. arXiv preprint arXiv:2309.00071, 2023. [146] Anian Ruoss, Grgoire Deltang, Tim Genewein, Jordi Grau-Moya, Rbert Csords, Mehdi Bennani, Shane Legg, and Joel Veness. Randomized positional encodings boost length generalization of transformers. arXiv preprint arXiv:2305.16843, 2023. [147] Amirhossein Kazemnejad, Inkit Padhi, Karthikeyan Natesan Ramamurthy, Payel Das, and Siva Reddy. The impact of positional encoding on length generalization in transformers. Advances in Neural Information Processing Systems, 36:2489224928, 2023. [148] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. The Journal of Machine Learning Research, 21(1):54855551, 2020."
        },
        {
            "title": "REFERENCES",
            "content": "[149] Ulme Wennberg and Gustav Eje Henter. The case for translation-invariant self-attention in transformerbased language models. arXiv preprint arXiv:2106.01950, 2021. [150] Yuhan Chen, Ang Lv, Jian Luan, Bin Wang, and Wei Liu. Hope: novel positional encoding without long-term decay for enhanced context awareness and extrapolation. arXiv preprint arXiv:2410.21216, 2024. [151] Xindian Ma, Wenyuan Liu, Peng Zhang, and Nan Xu. 3d-rpe: Enhancing long-context modeling through 3d rotary position encoding. arXiv preprint arXiv:2406.09897, 2024. [152] Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending context window of large language models via positional interpolation. arXiv preprint arXiv:2306.15595, 2023. [153] Arpit Aggarwal. Pope: Legendre orthogonal polynomials based position encoding for large language models. arXiv preprint arXiv:2405.04585, 2024. [154] Konstantinos Kogkalidis, Jean-Philippe Bernardy, and Vikas Garg. Algebraic positional encodings. arXiv preprint arXiv:2312.16045, 2023. [155] Yui Oka, Taku Hasegawa, Kyosuke Nishida, and Kuniko Saito. Wavelet-based positional representation for long context. arXiv preprint arXiv:2502.02004, 2025. Accepted to ICLR 2025. [156] Bowen Pan, Yikang Shen, Haokun Liu, Mayank Mishra, Gaoyuan Zhang, Aude Oliva, Colin Raffel, and Rameswar Panda. Dense training, sparse inference: Rethinking training of mixture-of-experts language models. arXiv preprint arXiv:2404.05567, 2024. [157] Shwai He, Run-Ze Fan, Liang Ding, Li Shen, Tianyi Zhou, and Dacheng Tao. Merging experts into one: Improving computational efficiency of mixture of experts. arXiv preprint arXiv:2310.09832, 2023. EMNLP 2023 Main Conference (Oral). [158] Xun Wu, Shaohan Huang, Wenhui Wang, and Furu Wei. Multi-head mixture-of-experts. arXiv preprint arXiv:2404.15045, 2024. [159] Shaohan Huang, Xun Wu, Shuming Ma, and Furu Wei. Mh-moe: Multi-head mixture-of-experts. arXiv preprint arXiv:2411.16205, 2024. [160] Jiamin Li, Qiang Su, Yitao Yang, Yimin Jiang, Cong Wang, and Hong Xu. Adaptive gating in mixtureof-experts based language models. arXiv preprint arXiv:2310.07188, 2023. [161] David Raposo, Sam Ritter, Blake Richards, Timothy Lillicrap, Peter Conway Humphreys, and Adam Santoro. Mixture-of-depths: Dynamically allocating compute in transformer-based language models. arXiv preprint arXiv:2404.02258, 2024. [162] Shawn Tan, Yikang Shen, Rameswar Panda, and Aaron Courville. Scattered mixture-of-experts implementation. arXiv preprint arXiv:2403.08245, 2024. [163] Dongyang Fan, Bettina Messmer, and Martin Jaggi. Towards an empirical understanding of moe design choices. arXiv preprint arXiv:2402.13089, 2024. [164] Elie Antoine, Frdric Bchet, and Philippe Langlais. Part-of-speech sensitivity of routers in mixture of experts models. arXiv preprint arXiv:2412.16971, 2024. Accepted at COLING 2025. [165] Zihan Qiu, Zeyu Huang, Shuang Cheng, Yizhi Zhou, Zili Wang, Ivan Titov, and Jie Fu. Layerwise recurrent router for mixture-of-experts. arXiv preprint arXiv:2408.06793, 2024. [166] Kuan-Ming Liu and Ming-Chih Lo. Llm-based routing in mixture of experts: novel framework for trading. arXiv preprint arXiv:2501.09636, 2025. Accepted by AAAI 2025 Workshop on AI for Social Impact. [167] Stefan Arnold, Marian Fietta, and Dilara Yesilbas. Routing in sparsely-gated language models responds to context. arXiv preprint arXiv:2409.14107, 2024. [168] Xu Owen He. Mixture of million experts. arXiv preprint arXiv:2407.04153, 2024. [169] Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, Kranthi Kiran GV, et al. Rwkv: Reinventing rnns for the transformer era. arXiv preprint arXiv:2305.13048, 2023."
        },
        {
            "title": "REFERENCES",
            "content": "[170] Antonio Orvieto, Samuel L. Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan Pascanu, and Soham De. Resurrecting recurrent neural networks for long sequences. Proceedings of Machine Learning Research, 202:2622726253, 2023. [171] Albert Gu, Karan Goel, and Christopher R. Efficiently modeling long sequences with structured state spaces. International Conference on Learning Representations, 2022. [172] Jimmy TH Smith, Andrew Warrington, and Scott Linderman. Simplified state space layers for sequence modeling. 2023. [173] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023. [174] Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher R. Hyena hierarchy: Towards larger convolutional language models. arXiv preprint arXiv:2302.10866, 2023. [175] Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, and Furu Wei. Retentive network: successor to transformer for large language models. arXiv preprint arXiv:2307.08621, 2023. [176] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, and Hao Wu. Mixed precision training. 2017. [177] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-lm: Training multi-billion parameter language models using model parallelism. 2019. [178] Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Mia Xu Chen, Dehao Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V. Le, Yonghui Wu, and Zhifeng Chen. Gpipe: Efficient training of giant neural networks using pipeline parallelism. 2018. [179] Emelie Curl, Jesse Geneson, and Leslie Hogben. Skew throttling. 2019. [180] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimizations toward training trillion parameter models. 2019. [181] Zeyu Han, Chao Gao, Jinyang Liu, Jeff Zhang, and Sai Qian Zhang. Parameter-efficient fine-tuning for large models: comprehensive survey. arXiv preprint arXiv:2403.14608, 2024. [182] Jonas Pfeiffer, Aishwarya Kamath, Andreas Rckl, Kyunghyun Cho, and Iryna Gurevych. Adapterfusion: Non-destructive task composition for transfer learning. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics (EACL), pages 487503. Association for Computational Linguistics, 2021. [183] Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics (ACL/IJCNLP), pages 45824597. Association for Computational Linguistics, 2021. [184] Yaoming Zhu, Jiangtao Feng, Chengqi Zhao, Mingxuan Wang, and Lei Li. Counter-interference adapter for multilingual machine translation. In Findings of the Association for Computational Linguistics: EMNLP 2021, pages 28122823. Association for Computational Linguistics, 2021. [185] Tao Lei, Junwen Bai, Siddhartha Brahma, Joshua Ainslie, Kenton Lee, Yanqi Zhou, Nan Du, Vincent Y. Zhao, Yuexin Wu, Bo Li, Yu Zhang, and Ming-Wei Chang. Conditional adapters: Parameter-efficient transfer learning with fast inference. In Advances in Neural Information Processing Systems 36 (NeurIPS 2023), 2023. [186] Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Lam Tam, Zhengxiao Du, Zhilin Yang, and Jie Tang. P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (ACL) (Volume 2: Short Papers), pages 6168. Association for Computational Linguistics, 2022. [187] Tu Vu, Brian Lester, Noah Constant, Rami Al-Rfou, and Daniel Cer. Spot: Better frozen model adaptation through soft prompt transfer. In Proceedings of the 60th Annual Meeting of the Association"
        },
        {
            "title": "REFERENCES",
            "content": "for Computational Linguistics (ACL) (Volume 1: Long Papers), pages 50395059. Association for Computational Linguistics, 2022. [188] Lichang Chen, Jiuhai Chen, Heng Huang, and Minhao Cheng. Ptp: Boosting stability and performance In Proceedings of the 2023 Conference on of prompt tuning with perturbation-based regularizer. Empirical Methods in Natural Language Processing (EMNLP), pages 1351213525. Association for Computational Linguistics, 2023. [189] Joon-Young Choi, Junho Kim, Jun-Hyung Park, Wing-Lam Mok, and SangKeun Lee. Smop: Towards efficient and effective prompt tuning with sparse mixture-of-prompts. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1430614316. Association for Computational Linguistics, 2023. [190] Demi Guo, Alexander M. Rush, and Yoon Kim. Parameter-efficient transfer learning with diff pruning. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics (ACL/IJCNLP) (Volume 1: Long Papers), pages 48844896. Association for Computational Linguistics, 2021. [191] Yi-Lin Sung, Varun Nair, and Colin Raffel. Training neural networks with fixed sparse masks. In Advances in Neural Information Processing Systems 34 (NeurIPS 2021), 2021. [192] Sarkar Snigdha Sarathi Das, Ranran Haoran Zhang, Peng Shi, Wenpeng Yin, and Rui Zhang. Unified low-resource sequence labeling by sample-aware dynamic sparse finetuning. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 69987010. Association for Computational Linguistics, 2023. [193] Zihao Fu, Haoran Yang, Anthony Man-Cho So, Wai Lam, Lidong Bing, and Nigel Collier. On the effectiveness of parameter-efficient fine-tuning. In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), pages 1279912807, 2023. [194] Runxin Xu, Fuli Luo, Zhiyuan Zhang, Chuanqi Tan, Baobao Chang, Songfang Huang, and Fei Huang. Raise child in large language model: Towards effective and generalizable fine-tuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 95149528. Association for Computational Linguistics, 2021. [195] Danilo Vucetic, Mohammadreza Tayaranian, Maryam Ziaeefard, James J. Clark, Brett H. Meyer, and Warren J. Gross. Efficient fine-tuning of bert models on the edge. In Proceedings of the 2022 IEEE International Symposium on Circuits and Systems (ISCAS), pages 18381842. IEEE, 2022. [196] Elad Ben Zaken, Yoav Goldberg, and Shauli Ravfogel. Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (ACL) (Volume 2: Short Papers), pages 19. Association for Computational Linguistics, 2022. [197] Mozhdeh Gheini, Xiang Ren, and Jonathan May. Cross-attention is all you need: Adapting pretrained transformers for machine translation. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 17541765. Association for Computational Linguistics, 2021. [198] Haoyu He, Jianfei Cai, Jing Zhang, Dacheng Tao, and Bohan Zhuang. Sensitivity-aware visual parameterefficient fine-tuning. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2023. [199] Armen Aghajanyan, Akshat Gupta, Luke Zettlemoyer, and Sonal Gupta. Intrinsic dimensionality explains the effectiveness of language model fine-tuning. arXiv preprint arXiv:2012.13255, 2020. [200] Soufiane Hayou, Nikhil Ghosh, and Bin Yu. Lora+: Efficient low rank adaptation of large models. arXiv preprint arXiv:2402.12354, 2024. [201] Fanxu Meng, Zhaohui Wang, and Muhan Zhang. Pissa: Principal singular values and singular vectors adaptation of large language models. arXiv preprint arXiv:2404.02948, 2024. [202] Yuning Mao, Lambert Mathias, Rui Hou, Amjad Almahairi, Hao Ma, Jiawei Han, and Madian Khabsa. Unipelt: unified framework for parameter-efficient language model tuning. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (ACL) (Volume 1: Long Papers), pages 62536264. Association for Computational Linguistics, 2022."
        },
        {
            "title": "REFERENCES",
            "content": "[203] Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, and Graham Neubig. Towards unified view of parameter-efficient transfer learning. In Proceedings of the 10th International Conference on Learning Representations (ICLR), 2022. [204] Zhiqiang Hu, Lei Wang, Yihuai Lan, Wanyu Xu, Ee-Peng Lim, Lidong Bing, Xing Xu, Soujanya Poria, and Roy Ka-Wei Lee. Llm-adapters: An adapter family for parameter-efficient fine-tuning of large language models, 2023. [205] Yuanhan Zhang, Kaiyang Zhou, and Ziwei Liu. Neural prompt search. In Proceedings of the 17th European Conference on Computer Vision (ECCV), pages 594611. Springer, 2022. [206] Han Zhou, Xingchen Wan, Ivan Vulic, and Anna Korhonen. Autopeft: Automatic configuration search for parameter-efficient fine-tuning. Transactions of the Association for Computational Linguistics, 12:525542, 2024. [207] Victor Sanh, Thomas Wolf, and Alexander M. Rush. Movement pruning: Adaptive sparsity by finetuning. 2020. [208] Xinyin Ma, Gongfan Fang, and Xinchao Wang. Llm-pruner: On the structural pruning of large language models. 2023. [209] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, distilled version of bert: smaller, faster, cheaper and lighter. 2019. [210] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq: Accurate post-training quantization for generative pre-trained transformers. 2022. [211] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning of quantized llms. 2023. [212] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. Albert: lite bert for self-supervised learning of language representations. 2019. [213] Yaniv Leviathan, Matan Kalman, and Yossi Matias. Fast inference from transformers via speculative decoding. 2022. [214] Shih-yang Liu, Zechun Liu, Xijie Huang, Pingcheng Dong, and Kwang-Ting Cheng. Llm-fp4: 4-bit floating-point quantized transformers. arXiv preprint arXiv:2310.16836, 2023. [215] Sergio Perez, Yan Zhang, James Briggs, Charlie Blake, Josh Levy-Kramer, Paul Balanca, Carlo Luschi, Stephen Barlow, and Andrew William Fitzgibbon. Training and inference of large language models using 8-bit floating point. arXiv preprint arXiv:2309.17224, 2023. [216] Keqin Bao, Jizhi Zhang, Yang Zhang, Wenjie Wang, Fuli Feng, and Xiangnan He. Tallrec: An effective and efficient tuning framework to align large language model with recommendation. In Proceedings of the 17th ACM Conference on Recommender Systems, pages 10071014, 2023. [217] Han Zhao, Min Zhang, Wei Zhao, Pengxiang Ding, Siteng Huang, and Donglin Wang. Cobra: Extending In Proceedings of the AAAI mamba to multi-modal large language model for efficient inference. Conference on Artificial Intelligence, volume 39, pages 1042110429, 2025. [218] Tong Ye, Weigang Huang, Xuhong Zhang, Tengfei Ma, Peiyu Liu, Jianwei Yin, and Wenhai Wang. Llm4effi: Leveraging large language models to enhance code efficiency and correctness. arXiv preprint arXiv:2502.18489, 2025. [219] Mu Li, David G. Andersen, Jun Woo Park, Alexander J. Smola, Amr Ahmed, Vanja Josifovski, James Long, Eugene J. Shekita, and Bor-Yiing Su. Scaling distributed machine learning with the parameter server. In 11th USENIX Symposium on Operating Systems Design and Implementation (OSDI 14), pages 583598, Broomfield, CO, October 2014. USENIX Association. [220] Neil C. Thompson, Kristjan Greenewald, Keeheon Lee, and Gabriel F. Manso. The computational limits of deep learning, 2022. [221] Joel Hestness, Newsha Ardalani, and Gregory Diamos. Beyond human-level accuracy: Computational challenges in deep learning. In Proceedings of the 24th symposium on principles and practice of parallel programming, pages 114, 2019."
        },
        {
            "title": "REFERENCES",
            "content": "[222] Madiajagan and Sridhar Raj. Parallel computing, graphics processing unit (gpu) and new hardware for deep learning in computational intelligence research. In Deep learning and parallel computing environment for bioengineering systems, pages 115. Elsevier, 2019. [223] Sparsh Mittal and Shraiysh Vaishay. survey of techniques for optimizing deep learning on gpus. Journal of Systems Architecture, 99:101635, 2019. [224] Vijay Janapa Reddi, Christine Cheng, David Kanter, Peter Mattson, Guenther Schmuelling, Carole-Jean Wu, Brian Anderson, Maximilien Breughe, Mark Charlebois, William Chou, Ramesh Chukka, Cody Coleman, Sam Davis, Pan Deng, Greg Diamos, Jared Duke, Dave Fick, J. Scott Gardner, Itay Hubara, Sachin Idgunji, Thomas B. Jablin, Jeff Jiao, Tom St. John, Pankaj Kanwar, David Lee, Jeffery Liao, Anton Lokhmotov, Francisco Massa, Peng Meng, Paulius Micikevicius, Colin Osborne, Gennady Pekhimenko, Arun Tejusve Raghunath Rajan, Dilip Sequeira, Ashish Sirasao, Fei Sun, Hanlin Tang, Michael Thomson, Frank Wei, Ephrem Wu, Lingjie Xu, Koichi Yamada, Bing Yu, George Yuan, Aaron Zhong, Peizhao Zhang, and Yuchen Zhou. Mlperf inference benchmark, 2020. [225] Standard Performance Evaluation Corporation. Standard performance evaluation corporation - contact. https://www.spec.org/spec/contact.html, 2024. Accessed: 2024-09-13. [226] Baidu Research. Deepbench: Benchmarking deep learning operations on different hardware. https: //github.com/baidu-research/DeepBench, 2024. Accessed: 2024-09-13. [227] Cody Coleman, Deepak Narayanan, Daniel Kang, Tian Zhao, Jian Zhang, Luigi Nardi, Peter Bailis, Kunle Olukotun, Chris R, and Matei Zaharia. Dawnbench: An end-to-end deep learning benchmark and competition. Training, 100(101):102, 2017. [228] Sujith Ravi. Projectionnet: Learning efficient on-device deep networks using neural projections, 2017. [229] Alexander Sergeev and Mike Del Balso. Horovod: fast and easy distributed deep learning in TensorFlow. arXiv preprint arXiv:1802.05799, 2018. [230] Intel Corporation. Intel mpi benchmarks. https://github.com/intel/mpi-benchmarks, 2024. Accessed: 2024-09-13. [231] Ray Project. Llmperf: tool for evaluating the performance of llm apis. https://github.com/ray-project/ llmperf, 2024. Accessed: 2024-09-13. [232] Intel Corporation. Intel extension for pytorch. https://github.com/intel/intel-extension-for-pytorch, 2024. Accessed: 2024-09-13. [233] Taojiannan Yang, Yi Zhu, Yusheng Xie, Aston Zhang, Chen Chen, and Mu Li. Aim: Adapting image models for efficient video action recognition, 2023. [234] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models, 2021. [235] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning of quantized llms, 2023. [236] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for nlp, 2019. [237] Zhengqing Yuan, Huiwen Xue, Chao Zhang, and Yongming Liu. Hulk: Graph neural networks for optimizing regionally distributed computing systems, 2023. [238] Haojun Xia, Zhen Zheng, Yuchao Li, Donglin Zhuang, Zhongzhu Zhou, Xiafei Qiu, Yong Li, Wei Lin, and Shuaiwen Leon Song. Flash-llm: Enabling cost-effective and highly-efficient large generative model inference with unstructured sparsity, 2023. [239] Fu Bang. GPTCache: An open-source semantic cache for LLM applications enabling faster answers and cost savings. In Liling Tan, Dmitrijs Milajevs, Geeticka Chauhan, Jeremy Gwinnup, and Elijah Rippeth, editors, Proceedings of the 3rd Workshop for Natural Language Processing Open Source Software (NLP-OSS 2023), pages 212218, Singapore, December 2023. Association for Computational Linguistics."
        },
        {
            "title": "REFERENCES",
            "content": "[240] Pinxue Zhao, Hailin Zhang, Fangcheng Fu, Xiaonan Nie, Qibin Liu, Fang Yang, Yuanbo Peng, Dian Jiao, Shuaipeng Li, Jinbao Xue, Yangyu Tao, and Bin Cui. Efficiently training 7b llm with 1 million sequence length on 8 gpus, 2024. [241] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need, 2023. [242] Xinyu Liu, Houwen Peng, Ningxin Zheng, Yuqing Yang, Han Hu, and Yixuan Yuan. Efficientvit: Memory efficient vision transformer with cascaded group attention. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1442014430, 2023. [243] Jinwook Oh, Sae Kyu Lee, Mingu Kang, Matthew Ziegler, Joel Silberman, Ankur Agrawal, Swagath Venkataramani, Bruce Fleischer, Michael Guillorn, Jungwook Choi, et al. 3.0 tflops 0.62 scalable processor core for high compute utilization ai training and inference. In 2020 IEEE Symposium on VLSI Circuits, pages 12. IEEE, 2020. [244] Paul Balana, Sam Hosegood, Carlo Luschi, and Andrew Fitzgibbon. Scalify: scale propagation for efficient low-precision llm training. arXiv preprint arXiv:2407.17353, 2024. [245] Chi-Chung Chen, Chia-Lin Yang, and Hsiang-Yun Cheng. Efficient and robust parallel dnn training through model parallelism on multi-gpu platform. arXiv preprint arXiv:1809.02839, 2018. [246] Tong Geng, Tianqi Wang, Chunshu Wu, Chen Yang, Shuaiwen Leon Song, Ang Li, and Martin Herbordt. Lp-bnn: Ultra-low-latency bnn inference with layer parallelism. In 2019 IEEE 30th International Conference on Application-specific Systems, Architectures and Processors (ASAP), volume 2160, pages 916. IEEE, 2019. [247] Zhuohan Li, Lianmin Zheng, Yinmin Zhong, Vincent Liu, Ying Sheng, Xin Jin, Yanping Huang, Zhifeng Chen, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. AlpaServe: Statistical multiplexing with model parallelism for deep learning serving. In 17th USENIX Symposium on Operating Systems Design and Implementation (OSDI 23), pages 663679, Boston, MA, July 2023. USENIX Association. [248] Amey Agrawal, Nitin Kedia, Ashish Panwar, Jayashree Mohan, Nipun Kwatra, Bhargav Gulavani, Alexey Tumanov, and Ramachandran Ramjee. Taming Throughput-Latency tradeoff in LLM inference with Sarathi-Serve. In 18th USENIX Symposium on Operating Systems Design and Implementation (OSDI 24), pages 117134, Santa Clara, CA, July 2024. USENIX Association. [249] Weihao Cui, Mengze Wei, Quan Chen, Xiaoxin Tang, Jingwen Leng, Li Li, and Mingyi Guo. Ebird: Elastic batch for improving responsiveness and throughput of deep learning services. In 2019 IEEE 37th International Conference on Computer Design (ICCD), pages 497505. IEEE, 2019. [250] Jovan Stojkovic, Esha Choukse, Chaojie Zhang, Inigo Goiri, and Josep Torrellas. Towards greener llms: Bringing energy-efficiency to the forefront of llm inference. arXiv preprint arXiv:2403.20306, 2024. [251] Soka Hisaharo, Yuki Nishimura, and Aoi Takahashi. Optimizing llm inference clusters for enhanced performance and energy efficiency. Authorea Preprints, 2024. [252] Wenxiao Wang, Wei Chen, Yicong Luo, Yongliu Long, Zhengkai Lin, Liye Zhang, Binbin Lin, Deng Cai, and Xiaofei He. Model compression and efficient inference for large language models: survey. arXiv preprint arXiv:2402.09748, 2024. [253] Lei Deng, Guoqi Li, Song Han, Luping Shi, and Yuan Xie. Model compression and hardware acceleration for neural networks: comprehensive survey. Proceedings of the IEEE, 108(4):485532, 2020. [254] Matan Haroush, Itay Hubara, Elad Hoffer, and Daniel Soudry. The knowledge within: Methods for data-free model compression. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 84948502, 2020. [255] Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, et al. Mmlu-pro: more robust and challenging multi-task language understanding benchmark. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024."
        },
        {
            "title": "REFERENCES",
            "content": "[256] Mirac Suzgun, Nathan Scales, Nathanael Schrli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc Le, Ed Chi, Denny Zhou, et al. Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022. [257] Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam Brown, Adam Santoro, Aditya Gupta, Adri Garriga-Alonso, et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615, 2022. [258] David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel Bowman. Gpqa: graduate-level google-proof q&a benchmark. In First Conference on Language Modeling, 2024. [259] Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou. Instruction-following evaluation for large language models. arXiv preprint arXiv:2311.07911, 2023. [260] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. [261] Jingxuan Fan, Sarah Martinson, Erik Wang, Kaylie Hausknecht, Jonah Brenner, Danxian Liu, Nianli Peng, Corey Wang, and Michael Brenner. Hardmath: benchmark dataset for challenging problems in applied mathematics. arXiv preprint arXiv:2410.09988, 2024. [262] Zayne Sprague, Xi Ye, Kaj Bostrom, Swarat Chaudhuri, and Greg Durrett. Musr: Testing the limits of chain-of-thought with multistep soft reasoning. arXiv preprint arXiv:2310.16049, 2023. [263] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [264] Humza Naveed, Asad Ullah Khan, Shi Qiu, Muhammad Saqib, Saeed Anwar, Muhammad Usman, Naveed Akhtar, Nick Barnes, and Ajmal Mian. comprehensive overview of large language models. arXiv preprint arXiv:2307.06435, 2023. [265] Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng, Honghui Ding, Kai Dong, Qiushi Du, Zhe Fu, et al. Deepseek llm: Scaling open-source language models with longtermism. arXiv preprint arXiv:2401.02954, 2024. [266] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023. [267] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. [268] Marah Abdin, Jyoti Aneja, Harkirat Behl, Sbastien Bubeck, Ronen Eldan, Suriya Gunasekar, Michael Harrison, Russell Hewett, Mojan Javaheripi, Piero Kauffmann, et al. Phi-4 technical report. arXiv preprint arXiv:2412.08905, 2024. [269] Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio Cesar Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, Adil Salim, Shital Shah, Harkirat Singh Behl, Xin Wang, Sebastien Bubeck, Ronen Eldan, Adam Tauman Kalai, Yin Tat Lee, and Yuanzhi Li. Textbooks are all you need, 2023. [270] Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Guoyin Wang, Heng Li, Jiangcheng Zhu, Jianqun Chen, et al. Yi: Open foundation models by 01. ai. arXiv preprint arXiv:2403.04652, 2024. [271] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lelio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothee Lacroix, and William El Sayed. Mistral 7b, 2023."
        },
        {
            "title": "REFERENCES",
            "content": "[272] Anton Lozhkov, Loubna Ben Allal, Leandro von Werra, and Thomas Wolf. Fineweb-edu: the finest collection of educational content, 2024. [273] OpenO1 Team. Openo1-sft dataset, December 2024. [274] Junying Chen, Zhenyang Cai, Ke Ji, Xidong Wang, Wanlong Liu, Rongsheng Wang, Jianye Hou, and Benyou Wang. Huatuogpt-o1, towards medical complex reasoning with llms, 2024. [275] Benjamin Chew, Yuan Zhang, Pranav Baheti, Russ Altman, and Jason Poon. Large language models encode clinical knowledge. Nature Communications, 14(1):6188, 2023. [276] Ganesh Jawahar, Muhammad Abdul-Mageed, Laks VS Lakshmanan, and Dujian Ding. Llm performance predictors are good initializers for architecture search. arXiv preprint arXiv:2310.16712, 2023. [277] Vimal Kumar, Priyam Srivastava, Ashay Dwivedi, Ishan Budhiraja, Debjani Ghosh, Vikas Goyal, and Ruchika Arora. Large-language-models (llm)-based ai chatbots: Architecture, in-depth analysis and their performance evaluation. In International Conference on Recent Trends in Image Processing and Pattern Recognition, pages 237249. Springer, 2023. [278] Tianyu Ding, Tianyi Chen, Haidong Zhu, Jiachen Jiang, Yiqi Zhong, Jinxin Zhou, Guangzhi Wang, Zhihui Zhu, Ilya Zharkov, and Luming Liang. The efficiency spectrum of large language models: An algorithmic survey. arXiv preprint arXiv:2312.00678, 2023. [279] Keivan Alizadeh, Seyed Iman Mirzadeh, Dmitry Belenko, Khatamifard, Minsik Cho, Carlo Del Mundo, Mohammad Rastegari, and Mehrdad Farajtabar. Llm in flash: Efficient large language model inference with limited memory. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1256212584, 2024. [280] Mengwei Xu, Wangsong Yin, Dongqi Cai, Rongjie Yi, Daliang Xu, Qipeng Wang, Bingyang Wu, Yihao Zhao, Chen Yang, Shihe Wang, et al. survey of resource-efficient llm and multimodal foundation models. arXiv preprint arXiv:2401.08092, 2024. [281] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-lm: Training multi-billion parameter language models using model parallelism, 2020. [282] Meng-Hao Guo, Tian-Xing Xu, Jiang-Jiang Liu, Zheng-Ning Liu, Peng-Tao Jiang, Tai-Jiang Mu, SongHai Zhang, Ralph Martin, Ming-Ming Cheng, and Shi-Min Hu. Attention mechanisms in computer vision: survey. Computational visual media, 8(3):331368, 2022. [283] Amit Ben-Artzy and Roy Schwartz. Attend first, consolidate later: On the importance of attention in different llm layers. arXiv preprint arXiv:2409.03621, 2024. [284] Yaohua Tang, Zhicheng Hu, Kun Cheng, Fan Mo, Qiheng Lv, Hua Wang, and Zhi Chen. Round attention: novel round-level attention mechanism to accelerate llm inference. arXiv preprint arXiv:2502.15294, 2025. [285] Shang Yang, Junxian Guo, Haotian Tang, Qinghao Hu, Guangxuan Xiao, Jiaming Tang, Yujun Lin, Zhijian Liu, Yao Lu, and Song Han. Lserve: Efficient long-sequence llm serving with unified sparse attention. arXiv preprint arXiv:2502.14866, 2025. [286] Siyu Lu, Mingzhe Liu, Lirong Yin, Zhengtong Yin, Xuan Liu, and Wenfeng Zheng. The multi-modal fusion in visual question answering: review of attention mechanisms. PeerJ Computer Science, 9:e1400, 2023. [287] Derya Soydaner. Attention mechanism in neural networks: where it comes and where it goes. Neural Computing and Applications, 34(16):1337113385, 2022. [288] Dichao Hu. An introductory survey on attention mechanisms in nlp problems. In Intelligent Systems and Applications: Proceedings of the 2019 Intelligent Systems Conference (IntelliSys) Volume 2, pages 432448. Springer, 2020. [289] Gianni Brauwers and Flavius Frasincar. general survey on attention mechanisms in deep learning. IEEE Transactions on Knowledge and Data Engineering, 35(4):32793298, 2021. [290] Benyamin Ghojogh and Ali Ghodsi. Attention mechanism, transformers, bert, and gpt: tutorial and survey. 2020."
        },
        {
            "title": "REFERENCES",
            "content": "[291] Jian-wei LIU, Jun-wen LIU, and Xiong-lin LUO. Research progress in attention mechanism in deep learning. Chinese Journal of Engineering, 43(11):14991511, 2021. [292] Zheng Zhang, Fan Yang, Ziyan Jiang, Zheng Chen, Zhengyang Zhao, Chengyuan Ma, Liang Zhao, and Yang Liu. Position-aware parameter efficient fine-tuning approach for reducing positional bias in llms. arXiv preprint arXiv:2404.01430, 2024. [293] Liang Zhao, Xiachong Feng, Xiaocheng Feng, Weihong Zhong, Dongliang Xu, Qing Yang, Hongtao Liu, Bing Qin, and Ting Liu. Length extrapolation of transformers: survey from the perspective of positional encoding. arXiv preprint arXiv:2312.17044, 2023. [294] Aytug Onan and Hesham Alhumyani. Deepextract: Semantic-driven extractive text summarization framework using llms and hierarchical positional encoding. Journal of King Saud University-Computer and Information Sciences, 36(8):102178, 2024. [295] Pu-Chin Chen, Henry Tsai, Srinadh Bhojanapalli, Hyung Won Chung, Yin-Wen Chang, and Chun-Sung Ferng. simple and effective positional encoding for transformers. arXiv preprint arXiv:2104.08698, 2021. [296] Guolin Ke, Di He, and Tie-Yan Liu. Rethinking positional encoding in language pre-training. arXiv preprint arXiv:2006.15595, 2020. [297] Jie Wang, Tao Ji, Yuanbin Wu, Hang Yan, Tao Gui, Qi Zhang, Xuanjing Huang, and Xiaoling Wang. Length generalization of causal transformers without position encoding. arXiv preprint arXiv:2404.12224, 2024. [298] Xiaoniu Song, Zihang Zhong, Rong Chen, and Haibo Chen. Promoe: Fast moe-based llm serving using proactive caching. arXiv preprint arXiv:2410.22134, 2024. [299] Qidong Liu, Xian Wu, Xiangyu Zhao, Yuanshao Zhu, Derong Xu, Feng Tian, and Yefeng Zheng. When moe meets llms: Parameter efficient fine-tuning for multi-task medical applications. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 11041114, 2024. [300] Xianzhi Du, Tom Gunter, Xiang Kong, Mark Lee, Zirui Wang, Aonan Zhang, Nan Du, and Ruoming Pang. Revisiting moe and dense speed-accuracy comparisons for llm training. arXiv preprint arXiv:2405.15052, 2024. [301] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Gpt3. int8 (): 8-bit matrix multiplication for transformers at scale. Advances in neural information processing systems, 35:30318 30332, 2022. [302] Bowen Zhao, Hannaneh Hajishirzi, and Qingqing Cao. Apt: Adaptive pruning and tuning pretrained language models for efficient training and inference. arXiv preprint arXiv:2401.12200, 2024. [303] Zichang Liu, Jue Wang, Tri Dao, Tianyi Zhou, Binhang Yuan, Zhao Song, Anshumali Shrivastava, Ce Zhang, Yuandong Tian, Christopher Re, et al. Deja vu: Contextual sparsity for efficient llms at inference time. In International Conference on Machine Learning, pages 2213722176. PMLR, 2023. [304] Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Dehao Chen, Mia Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc Le, Yonghui Wu, et al. Gpipe: Efficient training of giant neural networks using pipeline parallelism. NeurIPS, 32, 2019. [305] Noam Shazeer, Youlong Cheng, Niki Parmar, Dustin Tran, Ashish Vaswani, Penporn Koanantakool, Peter Hawkins, HyoukJoong Lee, Mingsheng Hong, Cliff Young, et al. Mesh-tensorflow: Deep learning for supercomputers. Advances in neural information processing systems, 31, 2018. [306] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimizations toward training trillion parameter models. In SC20: International Conference for High Performance Computing, Networking, Storage and Analysis, pages 116. IEEE, 2020. [307] Aaron Harlap, Deepak Narayanan, Amar Phanishayee, Vivek Seshadri, Nikhil Devanur, Greg Ganger, and Phil Gibbons. Pipedream: Fast and efficient pipeline parallel dnn training. arXiv preprint arXiv:1806.03377, 2018."
        },
        {
            "title": "REFERENCES",
            "content": "[308] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730 27744, 2022. [309] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms, 2017. [310] Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes Belkada, Shengyi Huang, Leandro von Werra, Clmentine Fourrier, Nathan Habib, et al. Zephyr: Direct distillation of lm alignment. arXiv preprint arXiv:2310.16944, 2023. [311] Jixiang Hong, Quan Tu, Changyu Chen, Xing Gao, Ji Zhang, and Rui Yan. Cyclealign: Iterative distillation from black-box llm to white-box models for better human alignment. arXiv preprint arXiv:2310.16271, 2023. [312] Harrison Lee, Samrat Phatale, Hassan Mansoor, Kellie Lu, Thomas Mesnard, Colton Bishop, Victor Carbune, and Abhinav Rastogi. Rlaif: Scaling reinforcement learning from human feedback with ai feedback. arXiv preprint arXiv:2309.00267, 2023. [313] Yu Zhu, Chuxiong Sun, Wenfei Yang, Wenqiang Wei, Bo Tang, Tianzhu Zhang, Zhiyu Li, Shifeng Zhang, Feiyu Xiong, Jie Hu, et al. Proxy-rlhf: Decoupling generation and alignment in large language model with proxy. arXiv preprint arXiv:2403.04283, 2024. [314] Adam Fisch, Jacob Eisenstein, Vicky Zayats, Alekh Agarwal, Ahmad Beirami, Chirag Nagpal, Pete Shaw, and Jonathan Berant. Robust preference optimization through reward model distillation. arXiv preprint arXiv:2405.19316, 2024. [315] Yaniv Leviathan, Matan Kalman, and Yossi Matias. Fast inference from transformers via speculative decoding. In International Conference on Machine Learning, pages 1927419286. PMLR, 2023. [316] Charlie Chen, Sebastian Borgeaud, Geoffrey Irving, Jean-Baptiste Lespiau, Laurent Sifre, and John Jumper. Accelerating large language model decoding with speculative sampling, 2023. [317] Heming Xia, Zhe Yang, Qingxiu Dong, Peiyi Wang, Yongqi Li, Tao Ge, Tianyu Liu, Wenjie Li, and Zhifang Sui. Unlocking efficiency in large language model inference: comprehensive survey of speculative decoding. arXiv preprint arXiv:2401.07851, 2024. [318] Jiaming Xu, Jiayi Pan, Yongkang Zhou, Siming Chen, Jinhao Li, Yaoxiu Lian, Junyi Wu, and Guohao Dai. Specee: Accelerating large language model inference with speculative early exiting. arXiv preprint arXiv:2504.08850, 2025. [319] Yanxi Chen, Xuchen Pan, Yaliang Li, Bolin Ding, and Jingren Zhou. Ee-llm: Large-scale training and inference of early-exit large language models with 3d parallelism. arXiv preprint arXiv:2312.04916, 2023. [320] Mostafa Elhoushi, Akshat Shrivastava, Diana Liskovich, Basil Hosmer, Bram Wasti, Liangzhen Lai, Anas Mahmoud, Bilge Acun, Saurabh Agarwal, Ahmed Roman, et al. Layerskip: Enabling early exit inference and self-speculative decoding. arXiv preprint arXiv:2404.16710, 2024. [321] Steven Kolawole, Don Dennis, Ameet Talwalkar, and Virginia Smith. Revisiting cascaded ensembles for efficient inference. arXiv preprint arXiv:2407.02348, 2024. [322] Jonathan Mamou, Oren Pereg, Moshe Wasserblat, and Roy Schwartz. Tangobert: Reducing inference cost by using cascaded architecture. arXiv preprint arXiv:2204.06271, 2022. [323] Quzhe Huang, Zhenwei An, Nan Zhuang, Mingxu Tao, Chen Zhang, Yang Jin, Kun Xu, Liwei Chen, Songfang Huang, and Yansong Feng. Harder tasks need more experts: Dynamic routing in moe models. arXiv preprint arXiv:2403.07652, 2024. [324] Xin Wang, Fisher Yu, Lisa Dunlap, Yi-An Ma, Ruth Wang, Azalia Mirhoseini, Trevor Darrell, and Joseph Gonzalez. Deep mixture of experts via shallow embedding. In Uncertainty in artificial intelligence, pages 552562. PMLR, 2020. [325] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation, 2021."
        },
        {
            "title": "REFERENCES",
            "content": "[326] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical textconditional image generation with clip latents, 2022. [327] Midjdourney. [328] Yixin Liu, Kai Zhang, Yuan Li, Zhiling Yan, Chujie Gao, Ruoxi Chen, Zhengqing Yuan, Yue Huang, Hanchi Sun, Jianfeng Gao, et al. Sora: review on background, technology, limitations, and opportunities of large vision models. arXiv preprint arXiv:2402.17177, 2024. [329] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR, 2021. [330] Zihao Zhao, Yuxiao Liu, Han Wu, Mei Wang, Yonghao Li, Sheng Wang, Lin Teng, Disheng Liu, Zhiming Cui, Qian Wang, et al. Clip in medical imaging: comprehensive survey. arXiv preprint arXiv:2312.07353, 2023. [331] OpenAI, :, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mo Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey Chu, Hyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix, Simon Posada Fishman, Juston Forte, Isabella Fulford, Leo Gao, Elie Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes, Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny Jin, Shino Jomoto, Billie Jonn, Heewoo Jun, Tomer Kaftan, ukasz Kaiser, Ali Kamali, Ingmar Kanitscheider, Nitish Shirish Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook Kim, Christina Kim, Yongjik Kim, Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo, ukasz Kondraciuk, Andrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen Krueger, Vishal Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan Leike, Jade Leung, Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin, Stephanie Lin, Mateusz Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim Malfacini, Sam Manning, Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew Mayne, Bob McGrew, Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil, David Medina, Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin, Vinnie Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David Mely, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh, Long Ouyang, Cullen OKeefe, Jakub Pachocki, Alex Paino, Joe Palermo, Ashley Pantuliano, Giambattista Parascandolo, Joel Parish, Emy Parparita, Alex Passos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Filipe de Avila Belbute Peres, Michael Petrov, Henrique Ponde de Oliveira Pinto, Michael, Pokorny, Michelle Pokrass, Vitchyr Pong, Tolly Powell, Alethea Power, Boris Power, Elizabeth Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick Ryder, Mario Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr, John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin, Katarina Slama, Ian Sohl, Benjamin Sokolowsky, Yang Song, Natalie Staudacher, Felipe Petroski Such, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak, Madeleine Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston Tuggle, Nick Turley, Jerry Tworek, Juan Felipe Ceron Uribe, Andrea Vallone, Arun Vijayvergiya, Chelsea Voss, Carroll Wainwright, Justin Jay Wang, Alvin Wang, Ben Wang, Jonathan"
        },
        {
            "title": "REFERENCES",
            "content": "Ward, Jason Wei, CJ Weinmann, Akila Welihinda, Peter Welinder, Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich, Hannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang, Marvin Zhang, Shengjia Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, and Barret Zoph. Gpt-4 technical report, 2023. [332] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pretraining for unified vision-language understanding and generation. In International conference on machine learning, pages 1288812900. PMLR, 2022. [333] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: visual language model for few-shot learning. Advances in neural information processing systems, 35:2371623736, 2022. [334] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In NeurIPS, 2023. [335] Jingyi Zhang, Jiaxing Huang, Sheng Jin, and Shijian Lu. Vision-language models for vision tasks: survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024. [336] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. [337] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770778, 2016. [338] Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In NAACL-HLT, pages 41714186, 2019. [339] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Mller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. [340] Wan Team. Wan: Open and advanced large-scale video generative models. 2025. [341] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2629626306, 2024. [342] Qwen Team. Qvq: To see the world with wisdom, December 2024. [343] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271, 2024. [344] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. arXiv preprint arXiv:2404.16821, 2024. [345] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2418524198, 2024. [346] Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Yuchen Duan, Hao Tian, Weijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479, 2025."
        },
        {
            "title": "VLMs and LVMs Background",
            "content": "Large Vision Models (LVMs). Large Vision Models (LVMs) have emerged as significant advancement in the field of artificial intelligence, particularly within the domain of generative models. These models are primarily designed for image and video generation tasks, demonstrating robust multimodal integration capabilities that enable them to comprehend and process relationships between text and images. By leveraging such capabilities, LVMs can effectively transform textual descriptions into visual representations. Most state-of-the-art vision generation models employ diffusion model architectures, which progressively denoise random noise to reconstruct high-quality images. Additionally, these models extensively utilize self-attention and cross-attention mechanisms to capture long-range dependencies within images and effectively align textual and visual features. Among the representative models, Stable Diffusion, developed by Stability AI, is an open-source image generation model based on latent diffusion architecture, which performs the diffusion process in latent space rather than pixel space, significantly reducing computational complexity. DALL-E [325, 326], developed by OpenAI, represents the forefront of text-to-image generation, offering strong text comprehension capabilities and highly realistic image synthesis. Midjourney [327] focuses on artistic-style image generation and provides an intuitive yet powerful parameter control system. More recently, OpenAIs Sora [328] has marked major breakthrough in video generation, capable of producing high-quality, coherent videos of up to one minute in length. By incorporating spatiotemporal consistency constraints, Sora ensures continuity across complex scenes and dynamic actions [328]. LVMs face unique computational challenges due to their need to process high-dimensional image and video data. single high-resolution color image may contain millions of pixels, with multiple channels of information per pixel. To efficiently handle such large-scale visual data, LVMs typically rely on parallel computing architectures such as GPUs or TPUs, leveraging parallel computing frameworks like CUDA to accelerate matrix operations. Additionally, batch processing techniques and mixed-precision training are employed to balance computational efficiency and accuracy. LVMs also exhibit high memory intensity, particularly due to the quadratic computational complexity of self-attention mechanisms with respect to sequence length, which results in substantial memory requirements when dealing with high-dimensional image and video data. From an efficiency perspective, one of the primary challenges for vision generative models is computational complexity. Diffusion models generally require tens to hundreds of iterative denoising steps, each involving complete forward pass through the network. The computational burden becomes even more pronounced in video generation, where an additional temporal dimension exponentially increases processing demands. To address these challenges, researchers have proposed various optimization strategies, including accelerated sampling techniques, knowledge distillation, model quantization, and sparse attention mechanisms. Furthermore, LVMs impose stringent hardware requirements, necessitating high-capacity memory, high-bandwidth data transfer, and specialized accelerators. Despite these advancements, real-time vision generation remains formidable challengehigh-quality image synthesis often requires several seconds to tens of seconds, while video generation is even more time-intensive. Additionally, deploying LVMs on edge devices is constrained by limited computational resources and energy efficiency considerations. Large Vision Language Models (VLMs). Vision-Language Models (VLMs) represent crucial frontier in artificial intelligence, embodying advancements in multimodal intelligence. These models are designed to simultaneously process and understand both visual and linguistic information, enabling cross-modal knowledge representation and reasoning. Unlike traditional unimodal models, VLMs bridge the semantic gap between vision and language, allowing machines to perceive the world through synergistic integration of textual and visual inputs. This capability has led to remarkable progress in tasks such as image captioning, visual question answering, and cross-modal retrieval, offering more natural and intuitive approach to human-computer interaction. Several representative VLMs have emerged as milestones in this domain. CLIP (Contrastive Language-Image Pre-training) [329], developed by OpenAI, leverages contrastive learning to jointly train text and image encoders, mapping features from both modalities into shared semantic space. Pretrained on vast amounts of"
        },
        {
            "title": "REFERENCES",
            "content": "internet data, CLIP demonstrates exceptional zero-shot transferability, enabling recognition of novel visual concepts solely based on textual descriptions [329, 330]. GPT-4V [331] extends the capabilities of LLMs to the visual domain, allowing for image-based text generation and question answering. This model not only comprehends image content but also performs complex reasoning, such as interpreting charts, analyzing scene relationships, and extracting key information from documents. Other notable models, including BLIP [332], Flamingo [333], and LLaVA [334], have adopted distinct architectural designs and training strategies to achieve state-of-the-art performance in vision-language understanding and generation tasks [335]. Architecturally, VLMs incorporate specialized components for processing different modalities, alongside mechanisms for multimodal fusion. typical VLM architecture consists of vision encoder (e.g., ViT [336], ResNet [337]), language encoder (e.g., BERT [338], GPT [47, 48, 1, 331]), and fusion module to integrate multimodal representations. The fusion process is fundamental challenge in VLMs and is commonly addressed through three strategies: early fusion (concatenating raw inputs), intermediate fusion (interacting after feature extraction), and late fusion (maintaining independent processing until the final decision stage). Among these, cross-modal fusion based on attention mechanisms is the most widely adopted, allowing the model to dynamically align relevant information across modalities. The complexity of such architectures imposes substantial computational demands, requiring efficient processing of high-dimensional visual data, large-scale language modeling, and real-time multimodal interactions. Efficiency remains significant challenge for VLMs, as multimodal processing inherently entails higher computational complexity compared to unimodal models. Vision encoders must process high-resolution images containing millions of pixels, while language encoders must capture intricate semantic structures. Moreover, attention-based fusion mechanismsparticularly cross-attentionexhibit quadratic complexity with respect to sequence length, leading to increased memory consumption and inference latency. The vast parameter scale of VLMs, such as GPT-4V, which may contain hundreds of billions of parameters, exacerbates memory constraints and computational overhead, limiting their deployment on resource-constrained devices and affecting real-time interaction performance. To address these efficiency challenges, researchers have explored various optimization strategies. Architectural optimizations include parameter sharing, knowledge distillation, and model quantization to reduce computational and memory requirements. For inference acceleration, techniques such as sparse attention, progressive decoding, and caching mechanisms have been developed to enhance processing speed. Hardware-oriented optimizations are also critical, involving the design of specialized accelerators, optimized memory access patterns, and distributed computing frameworks. Furthermore, task-specific multimodal optimizations, such as dynamic modality selection (activating only the necessary modality processing components based on task demands) and adaptive computation (adjusting computational resource allocation based on input complexity), show promising potential in improving the efficiency and scalability of VLMs. LLM and VLM Framework Capabilities Pre-train Table 14: LLM and VLM frameworks. Fine-tune Framework Colossal-AI Composer DeepSpeed FairScale LLM Foundry MegaBlocks Megatron Nanotron OpenLLM Pax RayLLM Sax Text Generation Inference vLLM Inference"
        },
        {
            "title": "REFERENCES",
            "content": "Table 15: Overview of Evaluated Large Vision Models and Large Vision Language Models. Model Name Parameter Year Creator LVMs Stable Diffusion 3.5 Medium Wan 2.1 T2V-1.3B VLMs Qwen2.5-VL (7B) QVQ-72B LLaVA 1.5 InternVL 3 (38B) 2.5B 1.3B 2024 Stability AI Alibaba 7B 72B 7B 38B 2023 Alibaba 2024 Alibaba 2023 LLaVA 2025 OpenGVLab"
        },
        {
            "title": "Other Models List",
            "content": "Large Vision Models (LVMs) Stable Diffusion 3.5. Stable Diffusion 3.5 (Stability AI, 2024) [339] is the latest text-to-image diffusion model in the Stable Diffusion series, which are latent diffusion models that generate images in compressed latent space for efficiency. Version 3.5 introduced two main variants: Large 8.1B-parameter model capable of producing 10241024 images with high fidelity, and Medium 2.5B-parameter model (with an improved MMDiT-X architecture) designed to run on consumer GPUs while still achieving up to 0.52 MP output resolution. Both models use modular UNet Transformer with cross-attention to T5 text encoder, and they support fast Large Turbo decoding via distilled 4-step sampler for quicker image generation. We use the Stable Diffusion 3.5 Large and Medium models as our text-to-image baselines. Wan 2.1 Video Models. Wan 2.1 (Alibaba, 2025) [340] is suite of open text-to-video and image-to-video generative models that achieve high-quality 480p720p video synthesis with relatively moderate model sizes. The series includes 14B-parameter text-to-video model and 14B image-to-video model (trained for 720p and 480p outputs respectively), as well as smaller 1.3B text-to-video model for efficiency. The 14B Wan 2.1 T2V model excels in complex high motion scenes, producing realistic physics and dynamics in its outputs, while the 1.3B variant offers favorable trade-off, generating 480p videos in only few minutes on standard hardware. Wan 2.1 models use diffusion-based architecture with dual encoders for text and image inputs, and they were released under an open Apache 2.0 license to stimulate community development. We evaluate Wan 2.1s T2V-14B, T2V-1.3B, and I2V-14B models in our benchmark. Vision Language Models (VLMs) Qwen2.5-VL. Qwen-VL (Alibaba, 2023) [266] is series of vision-language models built upon the Qwen LLM, endowed with visual understanding via pretrained image encoder. The initial Qwen-VL (7B) introduced carefully designed visual input module and three-stage training pipeline to handle image-text alignment, enabling capabilities such as image captioning, visual question answering, grounding, and OCR reading. Its successor, Qwen-VL 2, further improved multimodal performance and introduced instruction-tuned variants (Qwen-VL-Chat). In the latest generation Qwen-VL 2.5, the model scaling is increased up to 72B parameters (dubbed Qwen-VL-Max) to further boost visual reasoning capacity. The Qwen-VL 2.5 family (3B, 7B, and 72B) achieves state-of-the-art results on broad range of image understanding benchmarks, while remaining fully open-source. LLaVA 1.5. LLaVA 1.5 (2023) [341] is an open vision-language assistant model that connects vision encoder with LLaMA-based language model for interactive multimodal conversations. LLaVA uses CLIP ViT encoder to encode images and feeds the resulting embeddings into LLaMA chatbot, which has been fine-tuned on visual instruction data. Version 1.5 of LLaVA improved the fine-tuning procedure and dataset quality, resulting in more accurate visual understanding and more coherent dialogue responses. We use LLaVA 1.5 as representative chat-oriented VLM, noting its efficiency: it leverages fixed image encoder and an approximately 13B-parameter language model, avoiding the need to train massive end-to-end multimodal model."
        },
        {
            "title": "REFERENCES",
            "content": "QVQ-72B. QVQ-72B (2024) [342] is an upcoming 72B-parameter multimodal model from Alibaba, for which only preview is available at the moment. It is expected to combine the visual prowess of Qwen-VL-Max with the advanced reasoning of QwQ, in model that handles both vision and language at very large scale (72B). Due to limited official documentation, we use placeholder description for QVQ: it is anticipated to support extremely long context multimodal inputs and serve as testbed for scaling laws in VLM efficiency. (We will treat QVQ-72B-Preview as an experimental entry in our evaluations.) InternVL 3 (38B). InternVL 3 (OpenGVLab, 2025) [343, 344, 345, 346]s the latest model in the InternVL series that adopts native multimodal pretraining paradigm. Unlike conventional approaches that adapt textonly LLM to multimodal settings, InternVL3 is trained jointly on both pure-text and diverse vision-language data from scratch. This unified training eliminates post-hoc alignment issues and enhances multimodal grounding. InternVL3 incorporates variable visual position encoding (V2PE) to support extended visual contexts, along with advanced post-training techniques such as supervised fine-tuning (SFT) and mixed preference optimization (MPO). Test-time scaling and highly optimized training infrastructure further improve its performance. InternVL3-78B achieves state-of-the-art results among open-source MLLMs, scoring 72.2 on the MMMU benchmark, and shows competitive performance against proprietary models like GPT-4o and Claude 3.5. Notably, both model weights and training data are planned for public release to promote open research. Inference benchmark Performance Table 16: Evaluation Results Across Precisions - Performance Metrics. Model Precision MMLU-Pro BBH GPQA IFEval MATH MUSR DeepSeek-R1-Distill-Qwen-1.5B bfloat16 float16 int4 bfloat16 float16 int4 DeepSeek-R1-Distill-Llama-8B Qwen2.5-7B Qwen2.5-14B Qwen2.5-32B DeepSeek-R1-Distill-Qwen-14B bfloat16 float16 int4 bfloat16 float16 int4 bfloat16 float16 int4 bfloat16 float16 int4 bfloat16 float16 int4 bfloat16 float16 int4 bfloat16 float16 int Phi-3.5-mini Yi-34B Phi-4 0.3471 0.269 0.1955 0.1192 0.3553 0.3505 0.2754 0.1995 0.1213 0.3567 0.3337 0.2529 0.1937 0.1043 0.3702 0.4173 0.2974 0.3666 0.3146 0.3829 0.4149 0.2948 0.3675 0.3023 0.3815 0.4203 0.2641 0.351 0.2215 0.3747 0.5891 0.3907 0.4774 0.3751 0.5353 0.5877 0.3916 0.4707 0.3784 0.5340 0.5766 0.3688 0.4166 0.2764 0.5327 0.5555 0.3281 0.6619 0.2499 0.4264 0.5545 0.3307 0.6626 0.2574 0.4290 0.5451 0.3413 0.6134 0.1501 0.4227 0.6501 0.3737 0.6079 0.1700 0.4744 0.6495 0.3722 0.6266 0.1591 0.4691 0.6202 0.3578 0.5878 0.0529 0.4348 0.7038 0.3818 0.7350 0.4021 0.5008 0.7039 0.3798 0.7295 0.3953 0.5034 0.6801 0.3828 0.7100 0.2190 0.4959 0.6705 0.4081 0.0549 0.2554 0.5034 0.6710 0.4009 0.0503 0.2497 0.5021 0.6679 0.3953 0.0651 0.2385 0.4756 0.5365 0.3060 0.4231 0.1167 0.4438 0.5377 0.3054 0.4051 0.1216 0.4385 0.5062 0.3118 0.3742 0.0482 0.4343 0.5482 0.3455 0.2950 0.0443 0.4145 0.5447 0.3417 0.3003 0.0435 0.4132 0.5137 0.3329 0.3198 0.0373 0.4053 0.1656 0.1668 0.1496 0.2739 0.2740 0.2381 0.4639 0.4651 0.4456 0.4468 0.4461 0.4187 0.5386 0.5379 0.5180 0.5905 0.5911 0.5691 0.5284 0.5295 0.5276 0.3834 0.3828 0.3382 0.4427 0.4456 0."
        },
        {
            "title": "REFERENCES",
            "content": "Analysis of Inference Benchmark Performance. Table 16 summarizes the inference performance of various models across multiple precision formats (bfloat16, float16, and int4) on six representative benchmarks: MMLU-Pro, BBH, GPQA, IFEval, MATH, and MUSR. Several key observations emerge from these results: Impact of Model Scale. Larger models consistently outperform smaller ones across nearly all benchmarks. For instance, Qwen2.5-32B achieves significantly higher scores compared to its smaller counterparts (7B and 14B), highlighting the effectiveness of scaling model parameters for improved inference performance. Precision Trade-offs. Lower-precision quantization (int4) generally introduces modest performance degradation compared to higher-precision formats (bfloat16 and float16). For example, DeepSeek-R1Distill-Qwen-14B shows slight drop in performance when quantized to int4, with MMLU-Pro decreasing from 0.4639 (bfloat16) to 0.4456 (int4). However, this degradation is relatively small, suggesting that int4 quantization provides favorable trade-off between computational efficiency and inference accuracy. Task-specific Variability. Different models exhibit varying strengths across benchmarks, indicating taskspecific suitability. For instance, Phi-4 demonstrates strong performance on BBH and GPQA benchmarks but significantly underperforms on IFEval. Conversely, Qwen2.5 models consistently achieve high scores on IFEval, suggesting their suitability for tasks evaluated by this benchmark. Consistency between bfloat16 and float16. Across most models and benchmarks, performance differences between bfloat16 and float16 are minimal, indicating that both formats are viable for inference on modern hardware. However, given the known hardware-level advantages of bfloat16 on recent GPU architectures (e.g., Hopper GPUs), it remains the recommended precision format for optimal efficiency. Quantization Sensitivity. Certain benchmarks, particularly MATH, exhibit higher sensitivity to quantization. For example, Qwen2.5-14Bs performance on MATH drops significantly from 0.1700 (bfloat16) to 0.0529 (int4), indicating that mathematical reasoning tasks may require higher precision to maintain accuracy. Overall, these results underscore the importance of carefully selecting model scale and numerical precision based on specific inference tasks and efficiency constraints. Practitioners should balance the trade-offs between computational efficiency and task-specific accuracy requirements when deploying large generative models in practical scenarios. Hyperparameter Settings To ensure reproducibility and provide comprehensive reference for practitioners, this section details the hyperparameter configurations used across our experiments. These settings were carefully selected to balance performance and efficiency considerations while maintaining consistency across different model architectures and efficiency techniques. Architecture Efficiency of Models Hyperparameter. For each Transformers Model: 0.5B model: 24 layers, hidden dimension 896, 14 attention heads, intermediate size 4864, 2 key-value heads, maximum position embeddings 32768, extra vocabulary size 293, RMS normalization epsilon 1e-6. 1.5B model: 28 layers, hidden dimension 1536, 12 attention heads, intermediate size 8960, 2 key-value heads, maximum position embeddings 32768, extra vocabulary size 293, RMS normalization epsilon 1e-6. 3B model: 36 layers, hidden dimension 2048, 16 attention heads, intermediate size 11008, 2 key-value heads, maximum position embeddings 32768, extra vocabulary size 293, RMS normalization epsilon 1e-6. Normalization Method for Drawing Figures Normalization Methodology for Efficiency Metrics To facilitate intuitive comparisons across various model architectures and optimization methods, we employed normalization techniques to standardize the diverse efficiency metrics presented in Figures 3 and 5. Specifically,"
        },
        {
            "title": "REFERENCES",
            "content": "all raw metrics, including Perplexity (PPL), Average Memory Utilization (AMU), Average Latency (AL), Tokens Throughput (TT), and Average Energy Consumption (AEC), are converted into normalized values ranging from 0.1 to 1.0. For metrics where lower values denote better performance (such as PPL, AMU, AL, and AEC), we applied the following normalization formula: Normalized Value = 0.1 + 0.9 Maximum Value Current Value Maximum Value Minimum Value (10) Conversely, for metrics where higher values are preferable (e.g., Tokens Throughput, TT), the normalization was performed using: Normalized Value = 0.1 + 0.9 Current Value Minimum Value Maximum Value Minimum Value (11) This systematic normalization ensures consistency in the interpretation of efficiency metrics across models and methods, allowing for clearer insights into the trade-offs between performance and computational resources as illustrated in Figures 3 and 5. Efficiency Score Computation The Efficiency Score shown in Figure 4 is calculated using weighted harmonic combination of normalized resource metrics. Specifically, the Efficiency Score integrates Average Memory Utilization (AMU), Peak Computational Utilization (PCU), Average Latency (AL), Sample Throughput (ST), and Average Energy Consumption (AEC) through the following formula: Efficiency Score = 0.2 min(AMU) AMU + 0.2 min(PCU) PCU + 0.2 AL min(AL) + 0.2 min(ST) ST + 0.2 min(AEC) AEC (12) This balanced combination of metrics ensures comprehensive assessment of computational and training efficiency, emphasizing optimal use of resources and performance trade-offs."
        }
    ],
    "affiliations": [
        "Imperial College London",
        "International Business Machines Corporation (IBM)",
        "Lehigh University",
        "Microsoft Research",
        "Rutgers University",
        "University of Illinois Chicago",
        "University of Notre Dame"
    ]
}