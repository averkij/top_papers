{
    "paper_title": "Hyperdimensional Probe: Decoding LLM Representations via Vector Symbolic Architectures",
    "authors": [
        "Marco Bronzini",
        "Carlo Nicolini",
        "Bruno Lepri",
        "Jacopo Staiano",
        "Andrea Passerini"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Despite their capabilities, Large Language Models (LLMs) remain opaque with limited understanding of their internal representations. Current interpretability methods, such as direct logit attribution (DLA) and sparse autoencoders (SAEs), provide restricted insight due to limitations such as the model's output vocabulary or unclear feature names. This work introduces Hyperdimensional Probe, a novel paradigm for decoding information from the LLM vector space. It combines ideas from symbolic representations and neural probing to project the model's residual stream into interpretable concepts via Vector Symbolic Architectures (VSAs). This probe combines the strengths of SAEs and conventional probes while overcoming their key limitations. We validate our decoding paradigm with controlled input-completion tasks, probing the model's final state before next-token prediction on inputs spanning syntactic pattern recognition, key-value associations, and abstract inference. We further assess it in a question-answering setting, examining the state of the model both before and after text generation. Our experiments show that our probe reliably extracts meaningful concepts across varied LLMs, embedding sizes, and input domains, also helping identify LLM failures. Our work advances information decoding in LLM vector space, enabling extracting more informative, interpretable, and structured features from neural representations."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 2 ] . [ 1 5 4 0 5 2 . 9 0 5 2 : r HYPERDIMENSIONAL PROBE: DECODING LLM REPRESENTATIONS VIA VECTOR SYMBOLIC ARCHITECTURES"
        },
        {
            "title": "A PREPRINT",
            "content": "Marco Bronzini University of Trento, Trento, Italy Ipazia S.p.A., Milan, Italy Carlo Nicolini Ipazia S.p.A., Milan, Italy Bruno Lepri Fondazione Bruno Kessler (FBK), Trento, Italy Ipazia S.p.A., Milan, Italy Jacopo Staiano University of Trento, Trento, Italy Andrea Passerini University of Trento, Trento, Italy"
        },
        {
            "title": "ABSTRACT",
            "content": "Despite their capabilities, Large Language Models (LLMs) remain opaque with limited understanding of their internal representations. Current interpretability methods, such as direct logit attribution (DLA) and sparse autoencoders (SAEs), provide restricted insight due to limitations such as the models output vocabulary or unclear feature names. This work introduces Hyperdimensional Probe, novel paradigm for decoding information from the LLM vector space. It combines ideas from symbolic representations and neural probing to project the models residual stream into interpretable concepts via Vector Symbolic Architectures (VSAs). This probe combines the strengths of SAEs and conventional probes while overcoming their key limitations. We validate our decoding paradigm with controlled inputcompletion tasks, probing the models final state before next-token prediction on inputs spanning syntactic pattern recognition, keyvalue associations, and abstract inference. We further assess it in question-answering setting, examining the state of the model both before and after text generation. Our experiments show that our probe reliably extracts meaningful concepts across varied LLMs, embedding sizes, and input domains, also helping identify LLM failures. Our work2 advances information decoding in LLM vector space, enabling extracting more informative, interpretable, and structured features from neural representations. Keywords Information Decoding LLMs Interpretability Probing Vector Symbolic Architectures"
        },
        {
            "title": "Introduction",
            "content": "Although LLMs excel across tasks, their black-box nature limits interpretability. Recent work has focused on decoding human-interpretable concepts from LLM latent representations [Gurnee and Tegmark, 2023, Park et al., 2023, Zhang et al., 2024]. Three main paradigms [Ferrando et al., 2024, Elhage et al., 2021] are currently proposed to inspect models residual stream: Supervised Probes, Direct Logit Attribution (DLA), and Sparse Autoencoders (SAEs). Probes are supervised models for task-specific probing objectives that map models vector space to meaningful features [Gurnee and Tegmark, 2023, Marks and Tegmark, 2023, Diego Simon et al., 2024], though their decoding capabilities have been debated [Hewitt and Liang, 2019]. DLA projects representations on the LLMs output vocabulary [Belrose et al., 2023], but this constrains the abstraction to the level of LLM tokens. SAEs learn sparse proxy representation [Bricken et al., 2023, Lieberum et al., 2024, Kissane et al., 2024], but naming triggered features often suffers from vagueness, verbosity, and data dependence. Vector Symbolic Architectures (VSAs) are computational framework [Schlegel et al., 2022, Gayler, 1998] inspired by cognitive science [Hawkins, 2021, Piantadosi et al., 2024], increasingly used to map neural representations to human-readable symbols. It has been used for tasks ranging from visual problems Corresponding author. marco.bronzini-1@unitn.it 2github.com/Ipazia-AI/hyperprobe"
        },
        {
            "title": "Hyperdimensional Probe",
            "content": "Figure 1: We first compress neural representations of the LLMs next-token task (F , blue). Next, we train neural VSA encoder to map these neural embeddings into proxy space, VSA encodings structured with input-related concepts (T , orange). We then probe the LLMs embeddings by extracting concepts from VSA encodings using hypervector algebra (I, green). This process of concept extraction ultimately enables deeper analysis of the models erroneous answers (red). such as multi-attribute digit recognition [Frady et al., 2020] and Ravens progressive matrices [Hersche et al., 2023] to mechanistic interpretability [Knittel et al., 2024]. This work introduces novel paradigm for decoding information from LLM vector spaces by integrating ideas from symbolic representations and neural probing. We propose Hyperdimensional Probe, novel approach for decoding human-interpretable information from latent representations of LLMs using VSAs and hypervector operations. We design supervised and shallow neural network (encoder) to map the LLMs residual stream into controlled vector space structured by VSA encodings, projecting its internal activations into human-interpretable and context-relevant concepts. Functioning as hybrid supervised probe, it harnesses the orthogonality property of VSAs to combine the SAEs ability to uncouple superposed subspaces with the interpretability advantages offered by traditional probes. Our method addresses several limitations of prior approaches by: (i) avoiding dependence on the models output vocabulary used in DLA, (ii) mitigating the potential confounding effects of task performance in conventional probes, and (iii) eliminating the need for explicit feature naming of SAEs. Section 4 describes our novel decoding paradigm, and Section 5 validates it in controlled setting, also demonstrating its effectiveness in debugging LLM failures. Section contrasts our VSA-based results with DLA. Section 5.3 applies our methodology to the Stanford Question Answering Dataset [Rajpurkar et al., 2016], validating it in question-answering setting. Figure 1 shows our framework, from LLM token mbedding processing, and neural VSA encoder training, to LLM answer explanation. The primary methodological contributions of this work are: Hyperdimensional probe: novel paradigm for information decoding in LLMs via VSAs; Effective compression of LLM embeddings to probe wide range of models residual stream while reducing the overall computational cost of probing; Enhanced interpretability of neural representations and LLMs erroneous outputs."
        },
        {
            "title": "2 Related work",
            "content": "The latent representations of transformers, also known as residual stream, is high-dimensional linear vector space that aggregates the outputs of all hidden layers [Elhage et al., 2021].Probing this additive space requires the identification of human-interpretable features across different layers [Ferrando et al., 2024]. This investigation is grounded in the linear representation hypothesis, which posits that latent features can be encoded as linear subspaces [Engels et al., 2024], formed and accessed during the forward pass [Park et al., 2023]. In recent years, three main paradigms [Ferrando et al., 2024] have been proposed to extract information from this vector space."
        },
        {
            "title": "Hyperdimensional Probe",
            "content": "Supervised Probes is generic mapping paradigm that maps the models residual stream to task-relevant features, measuring how much information about them is embedded [Tenney et al., 2019]. Previous works have shown that several features are linearly encoded in transformers, from syntactical information [Hernández López et al., 2023, Diego Simon et al., 2024], to complex concepts such as space-time coordinates [Gurnee and Tegmark, 2023] and truthfulness [Marks and Tegmark, 2023]. However, their probing effectiveness is debated due to difficulties in separating information decoding from probe learning [Ferrando et al., 2024, Hewitt and Liang, 2019]. Direct Logit Attribution (DLA) projects latent representations onto its output vocabulary through the unbundling layer [Geva et al., 2022]. This method, also known as Logit Lens [Belrose et al., 2023], interprets outputs as predicted logits at given point in the forward pass. DLA reveals next-token predictions, assuming that all subsequent layers are bypassed and providing insight into prediction dynamics [Jastrzebski et al., 2017]. However, this approach faces key limitations in uncovering features in the LLM vector space, as it relies solely on the next-token representation and is constrained by the models token vocabulary. Thus, abstraction is limited, while additional vector transformations might also be necessary [Belrose et al., 2023, Sakarvadia et al., 2023]. Sparse AutoEncoders (SAEs) use sparse dictionary learning [Olshausen and Field, 1997] to disentangle overlapping subspaces created by superposition [Cunningham et al., 2023]. An autoencoder reconstructs the residual stream in an unsupervised fashion, enforcing sparsity in its learned representations. Once trained, these serve as proxy layer for analysis. SAE activated neurons are interpreted via two strategies: identify representative tokens via DLA [Kissane et al., 2024, Dunefsky et al., 2024]; clustering inputs by shared SAE neurons, followed by manual [Jing et al., 2025] or automatic [Bricken et al., 2023, Lieberum et al., 2024] feature naming. While SAEs help addressing superposition, interpreting the resulting features is challenging. Feature naming plays crucial role in SAE-based analyses but remains problematic: DLA approaches restrict feature abstraction to LLM tokens, whereas example-based approaches can be overly broad and heavily data-dependent. Our Hyperdimensional Probe functions as hybrid supervised probe, taking advantage of the orthogonality property of VSAs to combine SAEs ability to uncouple superposed subspaces with the interpretability advantages offered by conventional probes. Our controlled vector space mimics the proxy layer of SAEs without requiring subsequent feature-naming step. Moreover, learning vector transformation, rather than directly performing downstream task, as in traditional probes, may better isolate encoded information by reducing task performance confounds. Finally, our method overcomes the key limitation of DLA-based analysis, its dependence on the models output vocabulary, by supporting concept sets with unrestricted levels of abstraction, cardinality, and data types. recent study [Knittel et al., 2024] uses VSAs for the mechanistic interpretability of transformers (GPT-2), showing layer-wise dynamics of neural weights can be seen as VSA-related circuits of word embeddings, attention, and MLP outputs. In contrast, our work decodes the semantics of the models residual stream instead of examining the contributions of the model components to its construction during inference."
        },
        {
            "title": "3 Background",
            "content": "Vector Symbolic Architectures (VSAs), also known as Hyperdimensional Computing, assume entities or data structures can be represented as random points in high-dimensional space. Owing to the concentration of measure phenomenon [Ledoux, 2001, Kanerva, 2009], exponentially many distinct concepts can be encoded as nearly orthogonal random vectors. codebook Φ maps predefined set of concepts to their hypervectors, while orthogonality and simple hypervector operations allow composition into more complex concepts. VSA codebook. We adopt the Multiply-Add-Permute architecture (MAP-Bipolar, MAP-B) from VSAs [Schlegel et al., 2022, Gayler, 1998], using bipolar hypervectors in 1, 1D. Dimensionality D, typically 102104, depends on the number of concepts [Kanerva, 1988] and representation complexity. MAP-B can theoretically encode 2D orthogonal, independent elements [Schlegel et al., 2022]. Its codebook Φ 1, 1ncD stores nc atomic concepts as bipolar random vectors, generated deterministically from seeds to ensure orthogonality and independence. Each vector is associated with concept, and Φ enables evaluation of representations by comparing them with known vectors. Since MAP-B operates in the bipolar domain, cosine similarity is used [Schlegel et al., 2022]. Hypervector algebra. The hypervector algebra [Kanerva, 2009] relies on two operations: binding and bundling, which support representing complex cognitive structures, such as textual propositions, in distributed, noise-tolerant manner [Gayler, 1998, Kanerva, 2009]. Binding operation () encodes input features with their associated values. For example, it can associate concepts with contextual information, such as (USA dollar). The bundling operation (+), or superposition, creates set of (contextualized) concepts by combining multiple concepts into one, such as (USA + Mexico). The resulting bundled vector is by design similar to each of its constituents, enabling retrieval."
        },
        {
            "title": "Hyperdimensional Probe",
            "content": "Binding is obtained via Hadamard product (element-wise) while bundling is element-wise sum. Polarization (sign) is typically required after bundling [Kleyko et al., 2020] to maintain the bipolar domain. This process irreversibly blends the parts, diminishing their similarity to the originals in proportion to their number. Conversely, unbinding () in VSAs recovers elemental vectors from binding operation by factoring out one vector via multiplication with its inverse (itself in MAP-B)."
        },
        {
            "title": "4 Hyperdimensional probe",
            "content": "This section presents our VSA-based paradigm for extracting human-interpretable information from LLM latent representations. In Section 4.1, we introduce synthetic corpus of diverse analogies, providing simple and controlled environment to validate our decoding method. Section 5.3 applies our methodology in QA setting, while Section discusses other settings. Section 4.2 then presents the construction of input representations using the hypervector algebra. We then illustrate our three-stage pipeline: (a) processing LLM embeddings (Section 4.3, in Figure 1); (2) the neural VSA encoder that maps embeddings onto our controlled proxy space, yielding VSA encodings (Section 4.4, ); and (3) the extraction of concepts from this proxy space (Section 4.5, I). 4.1 Synthetic corpus We build textual dataset to evaluate the key components of our decoding paradigm in simple, controlled, and interpretable testbed. Using controlled input-completion tasks also allows us to focus LLMs on concepts and their relationships, while testing inputs demanding diverse reasoning from syntactical pattern recognition and key-value association to abstract inference. Knowledge bases. This work focuses on analogies, textual inputs representing pairs of concepts connected by the same type of factual, syntactic, or semantic relationship. We collect pairs of analogies from two knowledge bases: Google analogy test set [Mikolov, 2013], and the Bigger Analogy Test Set (BATS, [Gladkova et al., 2016]). These span 44 domains across five distinct categories, covering wide range of factual and linguistic relationships, including analogies related to factual knowledge (e.g., countrys currency), semantic relations (e.g., grammatical gender), and morphological modifiers (e.g., verb+men). We also design mathematical analogies using three-digit integers and basic operations such as doubling, cubing, division, and extraction of roots. Textual analogies. After collecting these pairs, we generate 114,099 distinct textual examples, denoted as S, by combining all possible domain pairings. Each training example is formatted as: a1 : a2 = b1 : b2 (1) where a1 and b1 represent the keys of the two pairs, and a2 and b2 are their corresponding values. For example, Denmark:krone = Mexico:peso for the countries currencies, and queen:king = mother:father for the grammatical gender. Table 11 and Table 12 in Section show the domains grouped by knowledge base and category, respectively. Some concepts span multiple domains, such as Australia links to Canberra, English, and Australian. These overlaps can help mitigate the confounding effect of memorizing key-value pairs. For our experiments in Section 5, we further limit confounding effects by using the same pairs but generating set of textual inputs ( S) with verbose template: a1 is to a2 as b1 is to b2 (Figure 2). Conversely, for training (Section 4.4), we apply data augmentation strategies on S, such as key-value swapping, effectively tripling the corpus size which results in 395,944 training inputs (Section M). 4.2 Input representations This section describes the process of constructing VSA encodings for the training stage. This procedure, illustrated with our textual templates (Equation 1), generalizes to other templates (e.g., question-answer in Equation 5) or tasks (e.g., toxicity detection; Section H) since VSAs and hypervector algebra can encode complex structures across diverse inputs. Figure 2: Our experimental setup uses textual inputs with syntactic structures unseen during training."
        },
        {
            "title": "Hyperdimensional Probe",
            "content": "Codebook construction. The codebook defines the set of all input features; in our case, the contextually relevant concept, and is later used to construct and query VSA encodings. In our controlled setting, the codebook Φ (feature set) is constructed directly using all unique words included in the corpus, such as: mexico ϕmexico Φ, and krone ϕkrone Φ. Thus, we create matrix Φ {1, 1}ncD, using the torch-hd library [Heddes et al., 2023], where is the VSA dimension and nc = 2, 996 is the number of concepts/features. We set = 4096 as an adequate hidden dimension, given the cardinality of our codebook ( 103), which remains well below the theoretical capacity limit of the MAP-B architecture (Section 3). The average pairwise cosine similarity of the concepts in the codebook is 0 0.02, confirming orthogonality (full distribution in Section J). VSA encodings. With well-structured textual inputs, extracting input features and building their VSA-based representation is straightforward. Scalability to other input types is addressed in Section H.1. For each training input S, we generate its encoding by exploiting its constructive words (Equation 1), retrieving their corresponding hypervectors: {ϕa1, ϕa2, ϕb1 , ϕb2 } Φ. To encode an input sentence, we then exploit hypervector operations: binding and bundling (Section 3). Given that the input template represents two conceptual keyvalue pairs, we first bind each key to its corresponding value, such as linking each country to its currency in Equation 2. The full text is then encoded through bundling, producing superposed set of contextualized concepts represented as keyvalue associations. Ultimately, we polarize it, with the sign function, to maintain the bipolar domain. The input encoding in VSA for given sentence is then computed as: ys = (ϕkey ϕvalue) + (ϕkey ϕvalue) + . . . = (ϕa1 ϕa2) + (ϕb1 ϕb2) (2) Denmark : krone = Mexico : peso (cid:55) (ϕdenmark ϕkrone) + (ϕmexico ϕpeso) 4.3 Processing LLM embeddings The first stage of our pipeline involves feeding textual inputs to an autoregressive transformer model, followed by obtaining and preprocessing its residual stream (F in Figure 1). Using our corpus, we prompt an LLM with an input sentence S, LLM (s). For each textual input, its final word (b2) is removed beforehand as it represents the value of the second analogy, our target concept. Caching token embeddings. Our probing goal is to inspect the complete internal state of language model prior to its next-token prediction, capturing all encoded concepts without assuming beforehand the type of relationship with its prediction. To this end, we examine the residual stream in the final token representation, focusing on the middle to last layers. Emerging evidence shows that transformers encode next-token information in the final token due to their autoregressive nature [Elhage et al., 2021, Olsson et al., 2022], refining it in later residual stream layers [Belrose et al., 2023, Hernandez et al., 2023]. Specifically, for an autoregressive language model with hidden layers, we consider the embeddings (with size d) of the last token (:) in the latter half, for all [L/2, . . . , L], yielding matrix in RL/2d. However, considering such wide range of layers presents computational challenge, as probing high-dimensional matrix can significantly increase the computational footprint of the probing pipeline. Further, adjacent layer-wise embeddings are highly correlated (0.9) as shown in Section O.1, likely encoding redundant numerical patterns, and thus similar information. Here, we define representation redundancy as the approximate linear dependence among LLM hidden layer embeddings. Section O.2 shows that the LLM embedding space is roughly low-rank, with only few rows/layers (or their combinations) contribute meaningful structure. Dimensionality reduction. To reduce the computational cost of our approach, we lower the input dimensionality for our encoder by introducing two dimensionality-reduction steps: k-means clustering [Jain and Dubes, 1988], and sum pooling. Clustering reduces representation redundancy by grouping similar vector regions in LLM embedding space and computing centroids, accomplishing knowledge distillation. To determine the optimal range for k, we adopt the silhouette score [Rousseeuw, 1987]. trade-off between reduction, granularity, and model variability emerges with 37 clusters (Section O.3). We set = 5 to maintain the essential data structure while supporting effective dimensionality reduction.3 We then apply sum pooling, which consists of summing all centroid embeddings;4 merging group representatives (k-dimensional matrix) into vector exploiting the additivity property of LLM embeddings demonstrated in previous work [Bronzini et al., 2024]. For example, these reduction steps allows us to downsize the probed embedding space of OLMo-2: R335120 R5120. 3Section O.4 shows that the clusters consistently group adjacent layers. 4Preliminary evidence suggests that directly summing all layers (up to 32) results in noisier representation."
        },
        {
            "title": "Hyperdimensional Probe",
            "content": "Section O.5 presents an ablation showing that skipping these two compression steps increases the encoders trainable parameters tenfold. In summary, the neural representation of textual input from language model is processed through the ingestion procedure , as summarized in Algorithm 1. 4.4 Neural VSA encoder We train supervised model to map token embeddings from an autoregressive transformer into VSA encodings with known representation (T in Figure 1). We define supervised regression model M, shallow feedforward neural network, to map the LLM vector space to bipolar hypervectors. The model is trained on the LLM-VSA dataset generated using the corpus (Section 4.1), which consists of paired LLM embeddings (es in Algorithm 1) and their corresponding VSA representations (ys in Equation 2). The model infers latent features from the unknown LLM vector space to translate the encoded semantics into VSA representations with explicit and interpretable semantics. We define the neural VSA encoder model as three-layer MLP with 55M71M parameters (depending on the input embedding size d; see Section C), performing non-linear transformation: : Rd {1, 1}D, (3) We use the hyperbolic tangent function (tanh) in the output layer for bipolar outputs and incorporate residual connections to enhance training stability and convergence. The training process minimizes the Binary Cross-Entropy (BCE) error between the bipolar target hypervectors and the predictions. To ensure compatibility with such binary loss function, targets are temporarily converted to binary based on their sign; and predictions are smoothly mapped to the range [0, 1] using the sigmoid function. Mean Squared Error (MSE) regularization term is added to the loss, with coefficient of 0.1.5 Implementation details for the training process are reported in Section D.1. es ys. Language models. We valide our methodology on embeddings from popular open-weight LLMs available on the Hugging Face platform with 355M-109B parameters, experimenting with different embedding sizes and layer counts. In particular, we test the latest Meta AIs Llama 4 Scout, [AI, 2025] multi-modal mixture of 16 experts (MoE), Llama 3.1-8B [Grattafiori et al., 2024], Microsofts Phi-4 [Abdin et al., 2024], EleutherAIs Pythia-1.4b [Biderman et al., 2023], AllenAIs OLMo-2-32B [OLMo et al., 2024], and OpenAIs legacy GPT-2-medium [Radford et al., 2019]. Performance. The LLM-VSA dataset uses random 70-15-15 split of for training, validation, and test sets. Since our setting can be interpreted both as vector-based regression task and multi-label classification problem,6 we evaluate our approach using two distinct metrics: cosine similarity and multi-label binary accuracy. For binary accuracy, targets and predictions are binarized based on sign. First, evaluating the cosine similarity between the predicted and target VSA encodings yields test-set average score of 0.89 (best LLM in Section D, Llama 3.1-8B), indicating strong numerical alignment between our encoders outputs and the target encodings. Second, we obtain an average binary accuracy of 0.94, which indicates robust classification accuracy after polarizing the predictions with the sign function. This means that on average, the VSA encodings produced by our trained model deviated from the targets by only 6% of the vector elements, negligible error given VSAs large tolerance to noise. All tested models exhibit consistent performance;7 layer count has no effect, whereas reducing the embedding dimension is found to be slightly detrimental. This empirical evidence supports the effectiveness of our proposed methodology and the hypothesis that LLM embeddings can be represented using fully distributed encodings such as MAP-B in VSAs. 4.5 Probing VSA encodings In the third, and experimental stage of our work (I in Figure 1, Section 5), we examine the VSA encodings produced by our trained neural VSA encoder M, extracting the embedded concepts. To retrieve the embedded atomic concepts, 5Empirical results demonstrated better performance than other coefficients tested, ranging from 0.01 to 1. 6VSA encodings can be viewed as vectors with distinct labels, each assuming one of two possible values. 7Section reports the training performance of our neural VSA encoder for all of the six models. Textual input LLM (s) = es Ingestion algorithm Proxy space : Rd {1, 1}D VSA encoding (ys) Figure 3: The regression model that maps the neural representations into controlled vector space."
        },
        {
            "title": "Hyperdimensional Probe",
            "content": "we use the unbinding operation from VSA algebra (, Section 3). This vector operation reverses binding, which in our case links pairs key with its corresponding value, enabling one vector to be extracted from another. Since the generated VSA encoding may encode either no or several concepts, we attempt to extract the target concept (b2) by dynamically testing the unbinding operation with various candidates. This concept-related flexibility represents the novelty and added value of VSA-based probing, allowing us to query our proxy space without prior assumptions on the number of concepts. Consequently, we distinguish between two scenarios: in the first, no unbinding operations are required when the model encodes none or single concept; in the second scenario, when multiple concepts are embedded, we test the unbinding operation with different concepts to isolate single one. For example, unbinding VSA encoding with the concept of Mexico and obtaining Peso suggests that the probed encoding originally incorporated both the key and value of the target analogy pair: LET := Denmark:krone=Mexico: (cid:55) peso COMPUTE ys = M(F (s)) QUERY ys ϕmexico = ϕpeso + noise THEN ys (ϕmexico ϕpeso) (4) When probing an encoding (ys in Equation 4), we pick in-context concepts (ϕdenmark, ϕkrone, and ϕmexico), and their combinations, as candidates for unbinding. The best candidate was chosen by benchmarking the resulting concept after unbinding, against the in-context and target concepts through cosine similarity. If no relevant match was found (sim < 0.1), no operation was applied. In the experiments reported in Section 5, 80% of unbinding operations, averaged across all models, relied on the key of the target pair. In contrast, no operation was applied in 12% of the cases. Section shows the proportions of other candidates and highlights the variations among models."
        },
        {
            "title": "5 Experiments and results",
            "content": "This section presents insights into experiments with our trained encoders. We first outline the experimental setup in Section 5.1. We then report findings on LLM performance and concept extraction using our hyperdimensional probe in Section 5.2. Section contrasts our results with DLA, showing inferior probing capabilities, likely due to its reliance on the next-token representation, and thus surface-level features. Lastly, Section 5.3 extends our approach to the question-answering task. 5.1 Experimental setup Data. We test our trained neural VSA encoders on the set of textual inputs formatted using the verbose template ( S, Section 4.1). Thus, we validate our methodology using inputs with syntactic structures that differ from those seen during the training stage. Therefore, we perform information decoding from the vector representation of different token, shifting from the colon token of the training template (Equation 1) to the token to. This aims to further mitigate confounding effects from probes task performance in relation to information decoding. Metrics. Our experimental evaluation has two-fold objective (Equation 4): we assess the performance of LLMs in the next-token prediction, and our VSA-based probing method for retrieving targets from their latent representations using precision@k. We measure the LLMs performance via: binary precision on the next-token prediction against the target word; softmax score of the most likely next token and the target one; and rank of the target token on the ordered softmax scores. We compute the precision of LLM predictions by considering the most likely next token based on softmax scores (next-token@1), and the top-5 most likely ones (next-token@5). To address scenarios where the token generated by the LLM includes the initial part of the target word due to tokenization, we introduce value of 0.5 in the LLMs precision metrics (see also Section A). For example, this value is assigned if the model predicts the next token as ack for the target word acknowledge. To evaluate the performance of our VSA-based probing approach, we assess the binary precision of retrieving the target VSA concept from LLM latent representations via probing@1, and probing@5. 5.2 Extracting next-token concepts Figure 4 shows VSA-based probing for target concept retrieval, and LLM performance to complete analogies with targets. Section displays the same results in table, with statistical variability and two control tests. High variability in LLM performance. In an unexpected contrast, the largest model evaluated (109B; Llama 4, Scout) exhibited the lowest precision@1 in the next-token prediction task (8%, Figure 4), even underperforming the legacy GPT-2. Yet, its next-token@5 was comparable to others (still the lowest), but ranked among the best in"
        },
        {
            "title": "Hyperdimensional Probe",
            "content": "Figure 4: LLM performance in completing the analogies with target words (left), and the effectiveness of our decoding method in extracting the targets from LLM latent representations (right). Table 1: Concepts extracted by hyperdimensional probe. KeyTarget indicates extraction of the key (b1) and value (b2) of the target analogy; Key for only b1. Example refers to a1 and a2, the in-context examples concepts; ContextTarget for all concepts. Key Values shows concepts linked in different domain, Out-of-context for those unrelated to input. NONE means no concepts. On average, our probe captures concepttarget combinations in about 80% of cases. Extracted Concepts (%) GPT-2 Pythia Llama 3.1 Phi-4 OLMo-2 Llama4, Scout AVERAGE Key Target NONE Key Example Context Target Key Key Values Out-of-context Example Value Key Values Key Values Target Target 60.0 21.9 4.5 5.8 1.1 1.3 1.6 1.5 0.4 0. 66.9 16.7 6.1 2.4 1.9 1.4 1.2 1.3 0.1 0.1 85.4 6.9 0.6 1.5 1.5 1.8 0.5 0.3 0.3 0.1 84.8 7.6 1.0 1.3 1.2 1.5 0.7 0.0 0.4 0.2 80.1 8.5 1.8 2.0 4.4 1.1 0.8 0.2 0.1 0.1 79.0 11.5 3.4 0.8 2.4 0.8 1.1 0.1 0.1 0.1 76.0 9.4 12.2 5.4 2.9 2.0 2.3 1.6 2.1 1.1 1.3 0.3 1.0 0.4 0.6 0.6 0.2 0.1 0.1 0. probing@1. Strong probing performance suggests the final state encodes the target concept, but the model often fails to output it. This might be caused by exogenous (e.g., prompt design) and endogenous factors (e.g., tokenization). As shown in Section F.3, the model frequently predicted space instead of the correct word, which still often appeared in its top five predictions. This emphasizes variability introduced by tokenization and prompt design, which might have greater impact on token-based probing methods such as DLA. VSA probing exposes varying conceptual richness. Regarding the concepts extracted by our hyperdimensional probe, we achieve an average probing@1 across all models equal to 83% (right side of Figure 4, Section F), extracting the target concept with its key for most cases (60% for GPT-2, 85% for Llama 3.1, Table 1). Notably, GPT-2 shows the highest percentage of cases where no concepts are extracted (22%) and only the concepts from the in-context example (6%), ranking second in extracting only the keys of the target concept (5%). This underscores its struggle with the NLP task of completing analogies, even when it appears to grasp the context. On the other hand, OLMo-2 has the highest proportion of instances in which our probing approach retrieves the target concept alongside all in-context concepts (Context Target, 4%), indicating its richer representation in its final state for both the input context and next word. This latent richness is then reflected in its performance on next-token prediction, achieving the highest next token@1 equal to 48% (Figure 4). In cases where the target word was not among the top five predictions of Llama 4, nearly 50%"
        },
        {
            "title": "Hyperdimensional Probe",
            "content": "of the instances (Section L.1; 28% for OLMo-2 in Section L.2),8 our probing method successfully extracted the target concept and its associated key in 70% of instances, while no concept was retrieved in 18% of cases (26% for OLMo-2). Although the first outcome supports previous observations, the absence of extracted concepts merits more granular analysis across analogy categories (Table 12 in Section M). Our probe most frequently encounters conceptually-empty representations in mathematical analogies (88%, Section F.2, also for OLMo-2 comparison), followed by semantic hierarchies (39%). Factual and morphological analogies show much lower rates, at 5.5% and 1.1%, respectively. As elaborated in Section F.2, these differences likely stem from the type of reasoning involved: linguistic analogies depend on syntactical patterns, factual and semantic relations on keyvalue associations, and hierarchies or mathematical analogies on abstract inference. 5.3 From input-completion tasks to question-answering To further validate our approach in real-world scenarios and beyond the controlled analogy task explored previously, we apply our methodology to question answering task, using the popular SQuAD dataset [Rajpurkar et al., 2016]. This dataset evaluates extractive question answering i.e. each answer is text span within the input context through questions generated by crowdworkers over Wikipedia articles. It fits our concept-focused probing objective, as its questions/answers map to concepts presented in the given context. This elicits the language model to focus on those concepts, allowing us to benchmark the extracted concepts against features derived from both questions and answers. We extract input features based on lexical semantics, exploiting WordNet [Miller, 1995] and DBpedia [Lehmann et al., 2015] as knowledge bases. The input representations (Section 4.2) are then created using bundling, such as: What was the name of the ship that Napoleon sent to the Black Sea? Charlemagne (cid:55) (ϕname + ϕship + ϕnapoleon + ϕsend + ϕtheBlackSea) + ϕcharlemagne We generate 693,886 training inputs by incrementally considering questions with their corresponding features (see also Section I). Our trained encoder (Section 4.4) achieves test-set cosine similarity of 0.44, and binary accuracy of 0.70 with Llama3.1. For our experiments, we consider 10,000 sampled questions Q, each now prefixed with its contextual text. We then probe the models final state before and after the autoregressive text generation, and extract concepts encoded in VSA encodings (Section 4.5) by comparing them directly with the codebook Φ, as binding is not involved. LLM performance on shows an average F1 score of 0.69 0.38, exact match of 0.52 0.5, with 68% of outputs mentioning the target answers. On average, our probe extracts three concepts both before and after the models text generation. (5) Observing concept drift. We evaluate semantic-based concept relevance by computing cosine similarity between concept embeddings and question-answer features. The average similarity of extracted concepts related to the question decreases after text generation: by 4.8% for the entire sample and by 8.0% in the LLM error subset (32% of the sample), while no significant differences are observed prior to generation (Figure 5; left). For answer-related concepts, overall no change is observed before and after text generation, but the LLM error subset shows slight increase (+3.23.5%; Figure 5, right). This suggests that LLM failures may stem from losing focus on the question rather than from lack of answer-related knowledge. This hypothesis is supported by weak positive Spearman correlation (0.2 with p-value of 1e99; Section K) between LLMs F1 score and the proportion of question-related concepts extracted after text generation. For example, for the SQuAD query What do laboratories try to produce hydrogen from? (target answer: solar energy and water), the model erroneously outputs water and heat (F1 = 0.57). Before the models text generation, our proposed approach extracts the concepts ϕtry, ϕproduce, ϕhydrogen (question) and ϕsolar, ϕwater (answer); after generation, the question-related concept set reduced to ϕproduce and the answer set gained the concept ϕenergy. 8We report the target word is absent from next token@k, including also tokens that represent its beginning. Figure 5: Concepts extracted before and after the LLMs text generation, with respect to question and answer features. Red denotes the subset of failure instances, while green the full sample Q."
        },
        {
            "title": "Hyperdimensional Probe",
            "content": "While answer-related concepts refined, the model lost focus on the subject hydrogen, drifting toward generic notion of production and ultimately an error."
        },
        {
            "title": "6 Conclusions",
            "content": "This work offers empirical evidence supporting the hypothesis that LLM embeddings can be accurately represented using Vector Symbolic Architectures (VSAs), combining ideas from symbolic representations and neural probing. Our Hyperdimensional probe is found to effectively extract human-interpretable information from latent representations of LLMs, as reported in Section 5. Our novel decoding paradigm combines SAEs ability to disentangle superposed subspaces with the interpretability of conventional probes, overcoming DLAs vocabulary dependence and feature-naming of SAE-based analysis. Although illustrated within controlled testbed for input-competition tasks, our approach readily extends to other experimental settings, such as the question-answering scenario described in Section 5.3. Section discusses further applications, including bias and toxicity detection. Our probe reveals non-trivial insights into LLM representations, from GPT2s context-related richness to the richer embeddings for linguistic analogies. Our VSA-based probing paradigm is computationally efficient, with lightweight probe that inspects wide range of the models residual stream at minimal memory cost (see also Section Q). It applies to any autoregressive transformer, and its implementation works with any Hugging Face language models. Additionally, the absence of theoretical limits in VSAs regarding the types of data, with the potentiality of hyperdimensional algebra, enables decoding multimodal latent features from LLM vector space. For example, the proof of concept in Section shows VSA-based probing for MNIST-based mathematical analogy [LeCun et al., 2010] and VSA encoding for multimodal input from the COCO dataset [Lin et al., 2014]. Limitations. The main limitation of our work (Section A) is its reliance on predefined set of concepts. While we applied several strategies to mitigate confounding effects in probe learning, such as testing on syntactically different inputs, we could not measure their effectiveness. Section F.1 presents two control tests [Hewitt and Liang, 2019] to evaluate confounding effects on decoding."
        },
        {
            "title": "Acknowledgments",
            "content": "We thank Marco Baroni for valuable feedback on experiments. Funded by the European Union. Views and opinions expressed are however those of the author(s) only and do not necessarily reflect those of the European Union or the European Health and Digital Executive Agency (HaDEA). Neither the European Union nor the granting authority can be held responsible for them. Grant Agreement no. 101120763 - TANGO. The work of JS has been partially funded by Ipazia S.p.A. BL and AP acknowledge the support of the PNRR project FAIR - Future AI Research (PE00000013), under the NRRP MUR program funded by the NextGenerationEU. BL has been also partially supported by the European Unions Horizon Europe research and innovation program under grant agreement No. 101120237 (ELIAS)."
        },
        {
            "title": "Reproducibility statement",
            "content": "The submission includes both the source code and our synthetic corpus, which will be made publicly available upon acceptance. README.md file is provided with the code, containing detailed instructions to reproduce our methodology (Section 4) and experimental results (Section 5). Section 4 presents our methodology, covering the entire pipeline from data creation (Section 4.1 and Section 4.2) to the training process of our proposed method (Section 4.3 and Section 4.4). Additional details of the training procedure are provided in Section D.1, while the overall model architecture is shown in Section C. The ingestion algorithm for LLM embeddings described in Section 4.3 is further illustrated in Section B. Finally, Section D.2 provides the Hugging Face links for all the LLMs used in our work, and Section reports the computational workload of our methodology."
        },
        {
            "title": "References",
            "content": "Wes Gurnee and Max Tegmark. Language models represent space and time. arXiv preprint arXiv:2310.02207, 2023. Kiho Park, Yo Joong Choe, and Victor Veitch. The linear representation hypothesis and the geometry of large language models. arXiv preprint arXiv:2311.03658, 2023."
        },
        {
            "title": "Hyperdimensional Probe",
            "content": "Liyi Zhang, Michael Li, and Thomas Griffiths. What should embeddings embed? autoregressive models represent latent generating distributions. arXiv preprint arXiv:2406.03707, 2024. Javier Ferrando, Gabriele Sarti, Arianna Bisazza, and Marta Costa-Jussà. primer on the inner workings of transformer-based language models. arXiv preprint arXiv:2405.00208, 2024. Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. mathematical framework for transformer circuits. Transformer Circuits Thread, 2021. https://transformer-circuits.pub/2021/framework/index.html. Samuel Marks and Max Tegmark. The geometry of truth: Emergent linear structure in large language model representations of true/false datasets. arXiv preprint arXiv:2310.06824, 2023. Pablo Diego Simon, Stéphane dAscoli, Emmanuel Chemla, Yair Lakretz, and Jean-Rémi King. polar coordinate system represents syntax in large language models. Advances in Neural Information Processing Systems, 37: 105375105396, 2024. John Hewitt and Percy Liang. Designing and interpreting probes with control tasks. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan, editors, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 27332743, Hong Kong, China, November 2019. Association for Computational Linguistics. doi:10.18653/v1/D191275. URL https://aclanthology.org/D19-1275/. Nora Belrose, Zach Furman, Logan Smith, Danny Halawi, Igor Ostrovsky, Lev McKinney, Stella Biderman, and Jacob Steinhardt. Eliciting latent predictions from transformers with the tuned lens. arXiv preprint arXiv:2303.08112, 2023. Trenton Bricken, Adly Templeton, Joshua Batson, Brian Chen, Adam Jermyn, Tom Conerly, Nick Turner, Cem Anil, Carson Denison, Amanda Askell, et al. Towards monosemanticity: Decomposing language models with dictionary learning. Transformer Circuits Thread, page 2, 2023. Tom Lieberum, Senthooran Rajamanoharan, Arthur Conmy, Lewis Smith, Nicolas Sonnerat, Vikrant Varma, János Kramár, Anca Dragan, Rohin Shah, and Neel Nanda. Gemma scope: Open sparse autoencoders everywhere all at once on gemma 2. arXiv preprint arXiv:2408.05147, 2024. Connor Kissane, Robert Krzyzanowski, Joseph Isaac Bloom, Arthur Conmy, and Neel Nanda. Interpreting attention layer outputs with sparse autoencoders. arXiv preprint arXiv:2406.17759, 2024. K. Schlegel, P. Neubert, and P. Protzel. comparison of vector symbolic architectures. Artificial Intelligence Review, 55:45234555, 2022. R. W. Gayler. Multiplicative binding, representation operators & analogy. In D. Gentner, K. J. Holyoak, and B. N. Kokinov, editors, Advances in Analogy Research: Integration of Theory and Data from the Cognitive, Computational, and Neural Sciences, pages 14, 1998. Jeff Hawkins. thousand brains: new theory of intelligence. Basic Books, 2021. Steven Piantadosi, Dyana CY Muller, Joshua Rule, Karthikeya Kaushik, Mark Gorenstein, Elena Leib, and Emily Sanford. Why concepts are (probably) vectors. Trends in Cognitive Sciences, 2024. Paxon Frady, Spencer Kent, Bruno Olshausen, and Friedrich Sommer. Resonator networks, 1: An efficient solution for factoring high-dimensional, distributed representations of data structures. Neural computation, 32(12): 23112331, 2020. Michael Hersche, Mustafa Zeqiri, Luca Benini, Abu Sebastian, and Abbas Rahimi. neuro-vector-symbolic architecture for solving ravens progressive matrices. Nature Machine Intelligence, 5(4):363375, 2023. Johannes Knittel, Tushaar Gangavarapu, Hendrik Strobelt, and Hanspeter Pfister. Gpt-2 through the lens of vector symbolic architectures. arXiv preprint arXiv:2412.07947, 2024. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+ questions for machine comprehension of text. arXiv preprint arXiv:1606.05250, 2016. Joshua Engels, Isaac Liao, Eric Michaud, Wes Gurnee, and Max Tegmark. Not all language model features are linear. arXiv e-prints, pages arXiv2405, 2024. Ian Tenney, Dipanjan Das, and Ellie Pavlick. Bert rediscovers the classical nlp pipeline. arXiv preprint arXiv:1905.05950, 2019."
        },
        {
            "title": "Hyperdimensional Probe",
            "content": "José Antonio Hernández López, Martin Weyssow, Jesús Sánchez Cuadrado, and Houari Sahraoui. Ast-probe: Recovering In Proceedings of the 37th abstract syntax trees from hidden representations of pre-trained language models. IEEE/ACM International Conference on Automated Software Engineering, ASE 22, New York, NY, USA, 2023. Association for Computing Machinery. ISBN 9781450394758. doi:10.1145/3551349.3556900. URL https: //doi.org/10.1145/3551349.3556900. Mor Geva, Avi Caciularu, Kevin Wang, and Yoav Goldberg. Transformer feed-forward layers build predictions by promoting concepts in the vocabulary space. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 3045, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi:10.18653/v1/2022.emnlpmain.3. URL https://aclanthology.org/2022.emnlp-main.3/. Stanislaw Jastrzebski, Devansh Arpit, Nicolas Ballas, Vikas Verma, Tong Che, and Yoshua Bengio. Residual connections encourage iterative inference. CoRR, abs/1710.04773, 2017. URL http://arxiv.org/abs/1710.04773. Mansi Sakarvadia, Arham Khan, Aswathy Ajith, Daniel Grzenda, Nathaniel Hudson, André Bauer, Kyle Chard, and Ian Foster. Attention lens: tool for mechanistically interpreting the attention head information retrieval mechanism. arXiv preprint arXiv:2310.16270, 2023. Bruno Olshausen and David Field. Sparse coding with an overcomplete basis set: strategy employed by v1? Vision research, 37(23):33113325, 1997. Hoagy Cunningham, Aidan Ewart, Logan Riggs, Robert Huben, and Lee Sharkey. Sparse autoencoders find highly interpretable features in language models. arXiv preprint arXiv:2309.08600, 2023. Jacob Dunefsky, Philippe Chlenski, and Neel Nanda. Transcoders find interpretable llm feature circuits. arXiv preprint arXiv:2406.11944, 2024. Yi Jing, Zijun Yao, Lingxu Ran, Hongzhu Guo, Xiaozhi Wang, Lei Hou, and Juanzi Li. Sparse auto-encoder interprets linguistic features in large language models. arXiv preprint arXiv:2502.20344, 2025. Michel Ledoux. The concentration of measure phenomenon. Number 89 in Mathematical Surveys and Monographs. American Mathematical Soc., 2001. Pentti Kanerva. Hyperdimensional computing: An introduction to computing in distributed representation with high-dimensional random vectors. Cognitive computation, 1:139159, 2009. Pentti Kanerva. Sparse distributed memory. MIT press, 1988. D. Kleyko, R. W. Gayler, and E. Osipov. Commentaries on \"learning sensorimotor control with neuromorphic sensors: Toward hyperdimensional active perception\" [science robotics vol. 4 issue 30 (2019) 1-10]. arXiv:2003.11458, pages 110, 2020. Tomas Mikolov. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781, 3781, 2013. Anna Gladkova, Aleksandr Drozd, and Satoshi Matsuoka. Analogy-based detection of morphological and semantic relations with word embeddings: what works and what doesnt. In Proceedings of the NAACL Student Research Workshop, pages 815. ACL, 2016. doi:10.18653/v1/N16-2002. Mike Heddes, Igor Nunes, Pere Vergés, Denis Kleyko, Danny Abraham, Tony Givargis, Alexandru Nicolau, and Alexander Veidenbaum. Torchhd: An open source python library to support research on hyperdimensional computing and vector symbolic architectures. Journal of Machine Learning Research, 24(255):110, 2023. Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, et al. In-context learning and induction heads. arXiv preprint arXiv:2209.11895, 2022. Evan Hernandez, Arnab Sen Sharma, Tal Haklay, Kevin Meng, Martin Wattenberg, Jacob Andreas, Yonatan Belinkov, and David Bau. Linearity of relation decoding in transformer language models. arXiv preprint arXiv:2308.09124, 2023. Anil Jain and Richard Dubes. Algorithms for clustering data. Prentice-Hall, Inc., 1988. Peter Rousseeuw. Silhouettes: graphical aid to the interpretation and validation of cluster analysis. Journal of computational and applied mathematics, 20:5365, 1987. Marco Bronzini, Carlo Nicolini, Bruno Lepri, Jacopo Staiano, and Andrea Passerini. Unveiling llms: The evolution of latent representations in dynamic knowledge graph. In First Conference on Language Modeling, 2024. URL https://openreview.net/forum?id=dWYRjT501w. Meta AI. The Llama 4 herd: The beginning of new era of natively multimodal AI innovation. https://ai.meta.com/blog/llama-4-multimodal-intelligence, 2025."
        },
        {
            "title": "Hyperdimensional Probe",
            "content": "Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Marah Abdin, Jyoti Aneja, Harkirat Behl, Sébastien Bubeck, Ronen Eldan, Suriya Gunasekar, Michael Harrison, Russell Hewett, Mojan Javaheripi, Piero Kauffmann, et al. Phi-4 technical report. arXiv preprint arXiv:2412.08905, 2024. Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle OBrien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. Pythia: suite for analyzing large language models across training and scaling. In International Conference on Machine Learning, pages 23972430. PMLR, 2023. Team OLMo, Pete Walsh, Luca Soldaini, Dirk Groeneveld, Kyle Lo, Shane Arora, Akshita Bhagia, Yuling Gu, Shengyi Huang, Matt Jordan, et al. 2 olmo 2 furious. arXiv preprint arXiv:2501.00656, 2024. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. George Miller. Wordnet: lexical database for english. Communications of the ACM, 38(11):3941, 1995. Jens Lehmann, Robert Isele, Max Jakob, Anja Jentzsch, Dimitris Kontokostas, Pablo Mendes, Sebastian Hellmann, Mohamed Morsey, Patrick Van Kleef, Sören Auer, et al. Dbpediaa large-scale, multilingual knowledge base extracted from wikipedia. Semantic web, 6(2):167195, 2015. Yann LeCun, Corinna Cortes, and CJ Burges. Mnist handwritten digit database. ATT Labs, 2, 2010. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer visionECCV 2014: 13th European conference, zurich, Switzerland, September 6-12, 2014, proceedings, part 13, pages 740755. Springer, 2014. Gotelli, Nicholas J, and Werner Ulrich. Statistical challenges in null model analysis. Oikos, 121(2):171180, 2012."
        },
        {
            "title": "A Limitations",
            "content": "While we apply data augmentation and test on syntactically different inputs to mitigate confounding on information decoding (Section 4.1 and Section 5.1), we could not measure the effectiveness of these strategies. Table 1, Table 8 and Table 9 report on the actual concepts identified by our probing method. The label Key values denotes instances where the probe retrieves key of an analogy pair with concept linked to it in different domain (see Australia in Section 4.1). This outcome can be viewed as an artifact of our probe, revealing the confounding influence of memorized key-value associations. Nevertheless, such cases constitute only small fraction, 2% of the 114,099 textual inputs processed across all models, covering Key Key Values, Example Value Key Values, and Key Values Target shown in Table 1. To further investigate potential confounding effects from probe learning, we introduce two control tests, as proposed in [Hewitt and Liang, 2019]. Using randomly-permuted input embeddings (es) as null model [Gotelli et al., 2012], and applying the unbinding operation on VSA encodings (ys) with concept pairs unrelated to inputs, respectively, permuted and unrelated baseline in Section F. In Section 5.1, we introduce the value of 0.5 in the precision metric for LLMs next-token prediction due to tokenization. While this approach is suitable for our needs, it may underestimate the models performance, resulting in lower precision@k scores in next-token prediction. However, in our experiments, only 8.6% of all instances fail in this scenario for the next token@1, considering the averaged ratio across models (Llama 4: 0.7%, Pythia: 16.2%; see also Section L.1 and Section L.2). On the other hand, models tokenizers could introduce variability in language models by themselves. While our approach avoids dependence on the LLMs vocabulary of DLA-based methods (Section 2) due to the data-agnostic nature of VSAs, it still requires predefined set of concepts. This set can however be seen as an alphabet with no practical constraints on the cardinality, type and source of its symbols. Algorithm to process LLM embeddings as described in Section 4.3 Algorithm 1: Ingestion procedure Data: Textual sequence Result: Compressed model state for its next token prediction begin // Get the residual stream from the language model LLM(s) RLT ; // Retain embeddings of the last token from the bottom half of the layers H[L/2 : L, 1]; // Apply K-Means clustering KMeansK=5(H) RKd ; // Sum pooling across the centroids k=1 Ck Rd ; es (cid:80)5 return es"
        },
        {
            "title": "C Architecture of our Hyperdimensional probe",
            "content": "Table 2: Configuration of the neural VSA encoder for an input embedding dimension equal to d. Component Input Layer"
        },
        {
            "title": "Linear Layer\nNormalization\nActivation",
            "content": "Residual Block"
        },
        {
            "title": "Linear Layer\nNormalization\nDropout\nResidual Connection",
            "content": "Residual Block 2 Linear Layer Normalization Dropout Residual Connection Output Layer Normalization Linear Layer Activation Input Dim Output Dim Note - - 4096 - - - 4096 - - - - 4096 - 4096 - - 4096 - - - 4096 - - - - 4096 - - LayerNorm (4096) GELU GELU activation LayerNorm (4096) = 0.5 Identity GELU activation LayerNorm (4096) = 0.5 Identity LayerNorm (4096) - Tanh Trainable parameters with: = 1024, 55M = 2048, 59M = 4096, 67M = 5120, 71M"
        },
        {
            "title": "D Training performance of the neural VSA encoders",
            "content": "Table 3: Training performance of our neural VSA encoder on the test set. Order by model size. Large Language Model Name Parameters Embedding dimension Layers from residual stream Cosine similarity Binary accuracy Llama 4, Scout, 17B-16E OLMo-2 Phi 4 Llama 3.1-8B Pythia-1.4b GPT-2, medium 109 32 14 8 1.4 355 5120 5120 5120 4096 2048 1024 24th to 48th 25 32nd to 64th 33 20th to 40th 21 16th to 32nd 17 12th to 24th 13 12th to 24th 13 0.890 0.878 0.881 0.892 0.861 0.865 0.934 0.926 0.930 0.937 0.916 0.920 AVERAGE 0.878 0.01 0.927 0.01 D.1 Training details The neural VSA encoder was trained for 421 epochs on average via PyTorch Lighting,9 using early stopping (patient set at 100 epochs) and batch size of 32. The optimal learning rate was automatically determined using the learning rate finder provided by the aforementioned library, and was approximately set to 3e5 on average. We use AdamW as the optimizer (weight decay of 1e4), applying learning rate schedule based on Cosine Annealing with Warm Restarts, starting from the 100th epoch and doubling the restart period thereafter. To adapt the batch size after LR restarts, we employed Gradient Accumulation Scheduler: the effective batch size was doubled at the 110th epoch, quadrupled at the 310th, and increased eightfold at the 410th epoch. During training, the models outputs are dynamically binarized using the sigmoid function to ensure compatibility with the loss function (Section 4.4). This approach demonstrated better empirical performance than linear min-max normalization. D.2 Hugging Face repositories for the considered LLMs 1. Meta AIs Llama 4, Scout, huggingface.co/meta-llama/Llama-4-Scout-17B-16E 2. Meta AIs Llama 3.1, huggingface.co/meta-llama/Llama-3.1-8B 3. Microsofts Phi-4, huggingface.co/microsoft/phi-4 4. EleutherAIs Pythia, huggingface.co/eleutherai/pythia-1.4b 5. AllenAIs OLMo-2, huggingface.co/allenai/OLMo-2-0325-32B 6. OpenAIs GPT-2, huggingface.co/openai-community/gpt2-medium 9lightning.ai/docs/pytorch/stable"
        },
        {
            "title": "Hyperdimensional Probe",
            "content": "E Unbinding stage from Section 4.5 Table 4: Unbinding stage: Proportions of the best unbinding concepts used for extracting concepts from VSA encodings across different models, with overall mean and standard deviation. Key refers to cases where the candidate concept corresponds to the key of the target pair (b1), while NONE indicates that no unbinding operations were applied to the probed VSA encoding. Example denotes concept where the key (a1) and value (a2) from the in-context example were pre-bound. Lastly, Context represents scenario where the in-context example (a1, a2) was pre-bound together with the key of the target pair (b1). On the other hand, Greedy means using concept candidate from the vocabulary, rather than picking it among those of the input. The table has been trimmed to highlight the relevant and common items across the models. We consider the first four strategies to be the most relevant, as they account for 97% of all unbinding operations across models. Concept for unbinding (%) GPT-2 Pythia Llama 4, Scout OLMo-2 Phi-4 Llama 3.1 AVERAGE Key NONE Example Key Context Greedy Example Value Cleaned Example Key Cleaned Example Value Cleaned Key Cleaned Original Example Example Value & Key Example Key & Key 65.9 22.0 6.0 1.2 2.1 1.6 0.2 0.9 0.0 0.0 0.0 0.0 0.0 74.4 16.9 2.6 2.0 1.9 1.5 0.5 0.1 0.0 0.0 0.0 0.0 0. 83.2 11.6 1.0 2.6 1.3 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 83.2 8.6 2.1 4.5 0.9 0.4 0.1 0.1 0.0 0.0 0.0 0.0 0.0 87.4 7.7 1.5 1.3 1.2 0.7 0.2 0.1 0.0 0.0 0.0 0.0 0.0 87.9 7.0 1.7 1.5 0.9 0.5 0.1 0.1 0.1 0.0 0.0 0.0 0.0 80.3 7.8 12.3 5.4 2.5 1.7 2.2 1.2 1.4 0.5 0.9 0.5 0.2 0.2 0.2 0.3 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0."
        },
        {
            "title": "F Experimental results",
            "content": "Table 5: Experimental results on the LLMs next-token prediction, along with our probing method for retrieving the target concept from models latent representations. The model are ordered based on precision@1 for next-token prediction, and standard deviation is reported for each. To control for randomness, we also introduce two control tests using Llama 3.1-8B: comparison against null model with randomly-permuted input embeddings (es, permuted baseline), and extraction of concept pairs unrelated to inputs (ys, unrelated baseline). MODEL Next Token Prediction VSA-based Probing Precison@1 Precison@ Precison@1 Precison@5 Permuted baseline Unrelated baseline - - - - 0.080 0.27 0.099 0. 0.103 0.30 0.105 0.31 Llama 4 Scout, 17B-16E 0.077 0.26 0.227 0.39 0.288 0.41 0.309 0.44 0.478 0.48 0.482 0.48 GPT-2, medium Pythia-1.4b Llama 3.1-8B Phi 4 OLMo-2 0.463 0.48 0.471 0.46 0.541 0.44 0.490 0.47 0.683 0.43 0.656 0.44 0.866 0.34 0.692 0.46 0.778 0.42 0.891 0.31 0.887 0.32 0.879 0.33 0.875 0.33 0.702 0.46 0.790 0.41 0.908 0.29 0.904 0.30 0.890 0. AVERAGE 0.310 0.41 0.551 0.45 0.832 0.35 0.845 0.33 F.1 Validation strategy To assess the effectiveness of our probe, we conduct two control tests (see Table 5) as proposed in Designing and interpreting probes with control tasks by Hewitt and Liang [2019]: 1. Permuted Baseline: We compared our outputs against null model by inputting the trained probe with randomly permuted LLM embeddings;"
        },
        {
            "title": "Hyperdimensional Probe",
            "content": "2. Unrelated Baseline: We attempt to extract concepts that are unrelated to the input using VSA-based probing. Both tests yielded very low precision probing scores, reinforcing the effectiveness of our method. These results show that: Applying our VSA-based probing (see Equation 4) using concepts irrelevant to input texts results in meaningless outputs; Corrupted or nonsensical input embeddings also produce poor results. That said, it is crucial to recognize fundamental limitation of all probing approaches: by definition, the humaninterpretable information encoded in LLM embeddings is not explicitly known. Consequently, no probing method can provide absolute certainty in decoding such information. To address this, we further validated our method by evaluating the trained probes on textual inputs distinct from those used during training, thereby reinforcing the reliability of our information decoding approach. F.2 Distribution of instances with no concepts extracted We examine probe performance across different LLM input types, defining success and failure by the presence or absence of concepts extracted by VSA probing. Table 6 displays the distribution of instances with no concept extracted grouped by input categories. While we observe model-wise variability, this preliminary analysis shows common pattern in representation blankness. 1. Linguistic analogies yield the lowest rate of missing concept extraction (11.8%), suggesting richer LLM representations, likely due to reliance on all concepts to capture implicit syntactic patterns. 2. Factual knowledge and semantic relations show slightly higher but still low blank rates (5.37%). Since these analogies rely on keyvalue associations, blanks may reflect missing associations in the model. 3. Semantic hierarchies (34.8%) and mathematical analogies (89.5%) yield the highest blank rates. Both require more abstract reasoning, but the large gap in mathematics likely stems from the rarity of analogical tasks with numbers, compared to equation solving or standard math problems more common in training data. Table 6: Analogies by Area (%) for the subset of instances with no retrieved concepts for Llama 4 and OLMo2, mentioned in Section 5.2. OLMo-2 shows richer embeddings than Llama 4, with lower proportions of instances with conceptually-blank representations for most of the areas. Llama 4 slightly outperform OLMo2 in mathematical and grammatical analogies. Area Llama 4 (docs, %) OLMo2 (docs, %) Mathematics Semantic Hierarchies Semantic Relations Factual Knowledge Verbal & Grammatical Forms Morphological Modifiers 87.8 38.8 10.0 5.5 1.4 1.1 91.1 30.8 3.9 5.1 2.1 0.8 AVG 89.5 34.8 7.0 5.3 1.9 1.0 Sample Domain 80 is to 160 as 98 is to limousine is to car as monorail is to Croatia is to Croatian as Switzerland is to euclid is to Greek as galilei is to seeing is to saw as describing is to agree is to agreement as excite is to math double hyponyms nationality adjective name nationality past tense verb+ment F.3 Diagnosing erroneous answers from Llama 4 Llama 4 most frequently generated white space token for our corpus S, accounting for 76% of its outputs, considerably higher than the 8% average observed in the other models (30% for Llama 3.1). Its next most common tokens were: ? (9%), what (6%) and (0.7%). The target token had median rank of 5, with its SoftMax score trailing the top-1 token by median difference of 0.85 (Section L.1), which starkly contrasts other models with 0.05. Thus, the model confidently predicted space, with the target word often within its top five predictions. These insights, and the strong performance of our hyperdimensinal probe (probing@1 = 87%), suggest issues in handling the syntactical structure of our corpus rather than lack of analogical reasoning. Possibly influenced by its tokenizer (see space-token frequency in the other Llama), which emphasizes prompt engineering importance and variability caused by models tokenizers. This may be further worsened by the models multimodality and the complexity of its MoE architecture."
        },
        {
            "title": "G Experimental comparison",
            "content": "We compare our VSA-based results to those yieled by the Direct Logit Attribution (DLA) technique; because, unlike SAEs, it requires no extra steps such as feature-naming, making it the most direct and unambiguous comparison for our approach. Our neural VSA encoder (Section 4.4) does qualify as supervised probe, as it is trained to map LLM internal representations (i.e., residual stream activations) into interpretable, human-understandable features (i.e., VSA encodings). Supervised probes are typically designed for specific experimental goals or target features, ranging from syntactic structure, as in Polar Coordinate System Represents Syntax in Large Language Models [Diego Simon et al., 2024]; to real-world knowledge, as in Language Models Represent Space and Time [Gurnee and Tegmark, 2023]; and to abstract semantics, as in The Geometry of Truth [Marks and Tegmark, 2023]. Our probe is specifically designed around VSA principles, so direct comparisons with non-VSA probes would require fundamentally different approaches not grounded in VSAs. While our controlled vector space (VSA encodings) parallels the SAE proxy layer, our approach uses top-down strategy by querying it with predefined concepts (Equation 4), whereas SAEs rely on bottom-up process that names all triggered features post hoc. This bottom-up approach reveals an unbounded set of latent features without relevance filtering, requiring exhaustive feature naming and additional filtering to isolate those aligned with our bounded inputoutput concept framework. In addition, while SAEs typically target single layer, our probing approach examines nearly the entire residual stream simultaneously, complicating direct and precise comparisons. This manual intervention involved in SAE-based methods, from feature naming to filtering, prevent them from being fully automated, and directly comparable to our supervised approach. By contrast, DLA outputs single, unique and unambiguous feature (token) constrained by the models output vocabulary, enabling direct comparison through fuzzy token-to-concept matching with our concept set. In summary, DLA is the most direct comparison, as SAE comparisons require additional steps, making them indirect and ambiguous, and supervised probes reflect only generic mapping paradigm. G.1 DLA-based experimental results To validate our results, we apply DLA to all models using S, as it allows direct baseline without extra steps such as feature naming or filtering required in SAE analysis. See Section for details. We adopt simple, fuzzy token-to-concept matching approach with our concept set (e.g., pes (cid:55) peso), and consider projected next-token predictions (Section G.3) from the models middle to last layers of the last token, as VSA probing. DLA produces no concepts in nearly 30% of analogies on average (see NONE in Table 7; +17% compared to VSA, Table 1), while yielding the target with its key in 26% of the cases (-50%). In instances without concepts from DLA, our VSA-based probe extracts, on average, the key-target pair in 57% of all analogies  (Table 8)  , while returning none for 28%. For instance, for the analogy king is to queen as son is to (cid:55) daughter, using OLMo-2, our probe extracts the key-target concepts (son and daughter), while DLA produces no concepts. The model predicts the next token prediction as ? with softmax score of 0.06, followed by father (0.05); the target word has rank of 37. Focusing on next-token representations, and thus capturing surface-level features, DLA exhibits inferior probing capabilities compared to ours, which compromise subsequent interpretability analyses of LLM embeddings. On the other hand, we observe substantial variance within this subset during VSA probing. Across models  (Table 8)  , our probe fails to retrieve any concepts in 43% of cases for Llama 4, compared to only 14% for Llama 3.1. GPT-2 confirms greater representativeness for the in-context example. There is also variation across analogy categories in this subset  (Table 9)  : for OLMo-2, linguistic analogies show the highest retrieval rates for Context Target (7.4% and 4.4%), whereas mathematical analogies shows nearly no concept retrieval (91%), confirming common blank representations. Section G.2 shows that, in cases where VSA fails, also DLA frequently yields no concepts rather than other relevant concepts."
        },
        {
            "title": "Hyperdimensional Probe",
            "content": "Table 7: Concepts extracted using the DLA probing technique on the full corpus with all LLMs. Likewise in our VSA-based probing, we focus on the same middle-to-bottom range of models hidden layers of the last token. The table highlights the key common items across models, with the first six cases covering over 95% of all extracted concepts. Extracted Concepts (docs, %) GPT-2 Pythia Llama4, Scout OLMo-2 Phi-4 Llama 3.1 AVERAGE VSA NONE Target Key Target Key Example Value Example Example Value Target Example Key Context Target 33.9 15.0 12.6 9.7 12.8 9.0 1.0 3.0 0.6 32.8 18.0 19.3 10.4 5.7 5.3 2.1 1.5 0.3 15.4 36.7 38.4 6.3 0.3 0.3 0.7 0.1 0. 14.6 29.0 38.5 10.4 0.7 1.7 0.6 0.2 1.5 33.1 34.4 22.7 6.0 0.9 1.0 0.3 0.1 0.3 47.4 22.5 22.1 4.5 0.5 0.5 0.9 0.1 0.4 29.5 11.4 25.9 8.1 25.6 9.7 7.9 2.4 3.5 4.6 3.0 3.2 0.9 0.6 0.8 1.1 0.6 0.4 +17.3 +25.8 - 50.4 +5.0 +3.5 +0.7 +0.9 +0.7 -1.5 Table 8: Concepts extracted though VSA-based probing when DLA yields no concepts. The table highlights VSA can also capture models variability (e.g., in-context concepts, target concepts). The table highlights key shared items across models, covering nearly 98% of all extracted concepts. DLA failures (docs, %) 33.9 32.8 47.4 33.1 14. 15.4 29.5 11.4 GPT-2 Pythia Llama 3.1 Phi-4 OLMo-2 Llama 4, Scout AVERAGE Concepts extracted by VSA (docs, %) Key Target NONE Example Key Out-of-context Key Pair Values Context Target Target 53.5 26.8 6.8 5.2 2.0 1.8 0.7 0. 56.5 24.3 3.0 7.3 1.8 2.1 1.2 0.1 76.6 13.7 2.1 0.7 1.1 2.5 1.1 0.2 70.5 18.6 2.1 0.9 1.9 3.2 0.7 0.0 44.5 41.0 3.4 1.6 3.8 2.4 1.9 0.1 42.6 43.2 2.0 3.0 4.5 1.8 1.4 0.0 57.4 12.5 27.9 10.9 3.2 1.7 3.1 2.4 2.5 1.2 2.3 0.5 1.2 0.4 0.1 0. Table 9: Percentages of extracted factors by analogy category considering the subset of instances when the DLA yields no concept for OLMo-2. Extracted concepts (docs, %) Morphological Modifiers Verbal & Grammatical Forms Factual Knowledge Semantic Relations Mathematics Semantic Hierarchies Key Target NONE Example Key Key Pair Values Context Target Out-of-Context Context 90.3 1.6 0.7 1.6 1.3 4.4 0.2 0.0 83.4 2.7 0.7 2.7 0.9 7.4 0.6 0.3 79.4 1.7 8.5 1.4 5.1 1.6 0.0 0. 0.0 91.1 0.0 0.0 0.0 0.0 8.8 0.0 41.5 21.1 15.0 5.1 11.6 0.6 0.9 0.3 70.1 14.3 4.7 3.8 0.0 0.8 1.3 0.1 20 AVERAGE 60.8 30.3 22.1 31.1 4.9 5.1 2.4 1.7 3.2 4.2 2.5 2.6 1.9 2.9 0.1 0."
        },
        {
            "title": "Hyperdimensional Probe",
            "content": "G.2 Concepts extracted by DLA when VSA yields no concepts Table 10: Concepts extracted though DLA-based probing when VSA yields no concepts. The table highlights DLA also extract no concepts in the majority of the instances (59 15 %), highlighting high variability among models. Extracted concepts (docs,%) GPT-2 Pythia Llama3 Phi-4 OLMo-2 Llama4 AVERAGE None Target Key Key Target Example Value Example Example Key Example Value Target Example Value Key Example Key Key Example Value Key Target Context Target Example Key Target Target Example 40.9 8.1 7.5 6.9 17.9 12.1 3.3 1.0 1.0 0.3 0.3 0.3 0.2 0.1 46.5 9.0 11.7 8.7 11.1 6.6 2.7 1.7 0.6 0.4 0.3 0.2 0.1 0. 83.1 5.3 3.4 5.7 1.3 0.4 0.1 0.1 0.1 0.1 0.0 0.3 0.0 0.0 72.7 15.7 4.3 3.6 1.0 2.2 0.1 0.0 0.0 0.0 0.0 0.2 0.0 0.0 55.9 14.3 13.2 11.8 1.3 1.7 0.2 0.2 0.4 0.1 0.3 0.4 0.0 0.1 53.9 23.3 9.1 11.9 0.6 0.1 0.2 0.2 0.3 0.1 0.2 0.1 0.0 0.0 58.8 14.8 12.6 6.4 8.2 3.7 8.1 3.3 5.5 6.7 3.9 4.3 1.1 1.2 0.5 0.7 0.4 0.4 0.2 0.2 0.2 0.1 0.3 0.1 0.1 0.1 0.1 0.1 G.3 Raw results obtained though the DLA probing technique Figure 6: Comprehensive raw outputs obtained though DLA on OLMo-2 for sampled analogy."
        },
        {
            "title": "H Applicability to other domains",
            "content": "H.1 Generalization of input representation VSA representations are automatically generated from input features, with their construction guided by the probing objective and the target latent features. While our work focuses on textual inputs with well-defined semantics, allowing straightforward extraction of input features (i.e., words), the underlying principle is flexible and generalizable. Equation 2 illustrates the creation of input representations via binding and bundling operations for our specific input template and downstream task. The hyperdimensional algebra underlying VSA allows this approach to generalize to other textual formats, NLP tasks, and even multi-modal data (see section P). Scalability challenges depend largely on the nature of the input features. For tasks such as toxicity detection, expertlabeled data or specialized feature extraction pipelines may be required. For example, mapping the phrase You are pathetic excuse for human just like the rest of your kind to conceptual form such as (ϕattackϕinsult)+(ϕattakϕidentity) requires human expertise. Once features are extracted, however, constructing VSA encodings is automatic, efficient, and scalable. VSA probing can then uncover encoded concepts in the LLM vector space, for instance: ys ϕattack = ϕidentity + noise In contrast, tasks based on syntactic structures offer more scalable input extraction. For example, the sentence The city of Turin is in Italy can be processed with conventional techniques such as POS tagging and Semantic Role Labeling (SRL). VSA encoding can then be automatically created: (ϕNOUN ϕcity) + (ϕPROPN ϕTurin) + (ϕVERB ϕbe) + (ϕPROPN ϕItaly) H.2 Applicability to other downstream tasks Although we demonstrate VSA-based probing using analogy-competition tasks, the methodology is generalizable to other experimental settings. The analogy-based dataset was chosen to: provide simple, controlled, and interpretable evaluation environment; elicit LLMs to focus on concepts and their inherent relationships; probe the LLM vector space with inputs spanning spectrum of reasoning tasks. Thanks to the flexibility of VSAs and hypervector algebra, VSA-based probing can be applied to wide variety of experimental settings with different: 1. Downstream tasks. Our decoding paradigm can be used for linguistic feature extraction, toxicity detection, or bias classification; 2. Textual templates. For example, in question-answering setting, an input text in such as Who wrote the play Romeo and Juliet? can be encoded as (ϕtask ϕquestion) + (ϕrelation ϕwrittenBy) + (ϕplay ϕRomeo&Juliet) allowing the VSA to query LLM representations and reveal which concepts are strongly represented or linked to the predicted answer; 3. Modalities. As discussed in Section P, inputs combining text with other modalities could also be encoded and probed via VSAs. VSA-based probing thus provides unified, flexible framework for examining how LLMs encode and relate abstract input features, from syntactic structures to high-level concepts such as gender bias or toxic language."
        },
        {
            "title": "Hyperdimensional Probe",
            "content": "I Question-answering setting from Section 5.3 We generate 693,886 training examples from the SQuAD dataset using an augmenting strategy by incrementally considering textual questions with their corresponding features: (A1) What was the name (cid:55) ϕname (A2) What was the name of the ship (cid:55) ϕname + ϕship (A3) . . . (cid:55) . . . (An1) What was the name of the ship that Napoleon sent to the Black Sea? (cid:55) ϕname (cid:55) ϕname + ϕship + ϕnapoleon + ϕsend + ϕtheBlackSea (An) What was the name of the ship that Napoleon sent to the Black Sea? Charlemagne (cid:55) (ϕname + ϕship + ϕnapoleon + ϕsend + ϕtheBlackSea) + ϕcharlemagne For our experiments, we generate another corpus including also the contextual text (Wikipedia article) provided for each SQuADs items: Napoleon III responded with show of force . . . by the Greek Orthodox Church. Q: What was the name of the ship that Napoleon sent to the Black Sea? (6) ( words): Lastly, we apply our entire pipeline by probing the final state of language model at the last token (colon) and extracting concepts through comparison with the codebook Φ. We analyze the models internal state across the text generation process, considering the residual stream at initialization (H[seq0]) and after the autoregressive generation of tokens (H[seqt])."
        },
        {
            "title": "J Cosine similarities among the items of the VSA codebook",
            "content": "Figure 7: Distribution of pair-wise cosine similarities among the items of the codebook."
        },
        {
            "title": "Hyperdimensional Probe",
            "content": "K Spearman correlation for the QA-related experiments Figure 8: Spearman correlation coefficients computed on Q. Figure 9: P-values of the Spearman correlation coefficients."
        },
        {
            "title": "L Overview of the experimental metrics",
            "content": "L.1 Llama 4, Scout Figure 10: Experimental metrics of the LLMs next-token prediction task and probing performance for Llama 4. Precision@k is displayed as categorical variable, with its binary values portrayed as boolean. The category initial token is associated to the special case (0.5) introduced in Section 5.1. We measure VSA noise by computing the cosine similarity between the retrieved target concept and its codebook version Φ."
        },
        {
            "title": "Hyperdimensional Probe",
            "content": "L.2 OLMo-2 Figure 11: Experimental metrics of the LLMs next-token prediction task and probing performance for OLMo-2. Precision@k is displayed as categorical variable, with its binary values portrayed as boolean. The category initial token is associated to the special case (0.5) introduced in Section 5.1."
        },
        {
            "title": "M Synthetic corpus",
            "content": "Table 11: Knowledge bases for our synthetic corpus S. Dataset Domains Sample example Google Analogy Test Set Bigger Analogy Test Set Mathematics 12 33 52 capital world, currency, plural, . . . verb+ment, occupation, gender, . . . double, square, division2, . . . 33,812 73,471 6,816 114,099 Denmark : krone = Mexico : peso king = mother : father queen : 25 4 : 16 = 5 : Table 12: Overview of our experimental set, grouped by tasking an LLM to cluster the domains. Category Domains Morphological Modifiers Verbal & Grammatical Forms Factual Knowledge Semantic Relations Mathematics Semantic Hierarchies noun+less, adj+ness, . . . past tense, plural, . . . country capital, occupation, . . . family, genders, . . . 14 13 7 8 7 math double, math division5, . . . hypernyms, hyponyms, . . . 52 Docs 34,308 (30%) 31,219 (27%) 18,800 (17%) 16,831 (15%) 6,816 (6%) 6,125 (5%) 114,099 (100%) Table 13: All domains, and their corresponding cardinality after data augmentation for training."
        },
        {
            "title": "Examples",
            "content": "country_capital antonyms_gradable adj+ly_reg noun_plural_reg verb_inf_3pSg name_nationality verb+able_reg hypernyms_animals verb+ment_irreg name_occupation verb+tion_irreg past_tense present_participle adjective_to_adverb superlative math_division5 math_division10 math_cubes DOMAINS: 52 21801 capital_world 11268 adj_superlative 10576 adj_comparative 10216 noun_plural_irreg 10112 animal_sound 9998 verb+er_irreg 9849 adj+ness_reg 9831 over+adj_reg 9807 verb_inf_Ved 9801 noun+less_reg 9801 hypernyms_misc 6313 plural plural_verbs 3401 2977 math_double 2545 math_division2 family 641 hyponyms_misc"
        },
        {
            "title": "N Declaration of LLM usage",
            "content": "country_language un+adj_reg verb_Ving_Ved verb_inf_Ving verb_Ving_3pSg animal_shelter re+verb_reg 18561 10942 10519 male_female 10206 10083 9865 9849 9828 9805 UK_city_county verb_3pSg_Ved 9801 antonyms_binary 9719 comparative 4129 currency 3055 nationality_adjective 2918 2498 opposite 529 math_squares 102 math_root 12299 10614 10236 10164 10008 9861 9833 9821 9805 9801 9603 3765 2983 2818 2221 402 77 TEXTUAL EXAMPLES: 395,944 The paper presents pipeline that treats LLMs as subjects of study, not tools. To enhance interpretability, we adopted an LLM (GPT-4o) to categorize the 52 distinct analogy domains into semantically coherent macro categories (Table 12 in Section M)."
        },
        {
            "title": "O Dimensionality reduction",
            "content": "O.1 Average correlations among models hidden layer Figure 12: Average Person correlations among the second half of models hidden layers for Llama3.1 O.2 Analysis of representation redundancy In Section 4.3, we hypothesize that highly correlated rows (models adjacent layers) could cause redundant representations, since they likely encode similar numerical patterns, and thus information. Here, we present an analysis of representation redundancy, defined as approximate linear dependence among LLM hidden layer embeddings. We computed the Gram matrix = HH , where is the models residual stream, and analyzed its eigenvalues. Table 14 shows results for the OLMo-2 model (considering the 32nd-to-64th range of hidden layers; Section D), averaged on 100K training input sample. The spectrum reveals few dominant eigenvalues (around 3-4 modes) followed by many smaller ones, indicating that the embedding space is approximately low-rank. This suggests that, when considering the full matrix (R335120 for OLMo-2), most hidden layer representations (rows) are redundant, since only few rows (or their combinations) contribute meaningful structure. The first mode is by far the most dominant, with normalized eigenvalue of 0.65, compared to 0.17 for the second. We hypothesize that this leading component might correspond to next-token prediction representations, while the remaining modes capture secondary structures or auxiliary information. Our hyperdimensional probe aims to capture also these auxiliary latent structures, rather than limiting solely on the single predominant component."
        },
        {
            "title": "Hyperdimensional Probe",
            "content": "Table 14: Eigenvalues (EV) of the Gram matrix from OLMo-2s residual stream. Comp. EV (mean std) Norm. EV 0 1 2 3 4 5 6 7 8 9 10 . . . 30 31 32 58084 5293 15450 2056 5972 608 2539 330 2057 220 1187 166 727 119 505 83 363 59 282 48 230 37 . . . 30 6 27 6 22 6 0.650 0.170 0.070 0.030 0.020 0.010 0.010 0.010 0.000 0.000 0.000 . . . 0.000 0.000 0.000 O.3 Silhouette analysis for determining optimal range of clusters Figure 13: Silhouette scores for varying numbers of clusters, computed using random sample of 10,000 textual inputs from S. The six language models have varied layer counts (see Table 3), which results in different maximum possible cluster numbers."
        },
        {
            "title": "Hyperdimensional Probe",
            "content": "O.4 Distribution of cluster assignments for grouping models hidden layers Figure 14: Distribution of models hidden layers grouped by k-means clustering within the ingestion algorithm for Llama3.1-8B. It portrays the percentages of cluster assignments across all instances. O.5 Ablation study on the dimensionality-reduction steps This section presents an analysis of skipping the dimensionality reduction steps introduced in Section 4.3. While our VSA-based methodology would work without these compression steps, the overall computational cost of probing would dramatically increase. For example, our ingestion procedure (Section B; Section 4.3) reduces the probed OLMo-2s embeddings from R335120 to R5120. This allows our neural VSA encoder to have an input dimension = 5012 with only 71M trainable parameters (see Section C). If the two steps are eliminated, and thus the entire residual stream of the model R335120 is considered, the encoder receives flat input vector, creating an input dimension = 168960 R168960. Although the encoder would internally handle feature extraction, since the flattened input holds all the information encoded in the LLM embeddings, this approach would increase the number of trainable parameters to 742 million, representing tenfold increase. Additionally, adopting lazy feature extraction stage in an input vector space of size 105, which is approximately low-rank (see Section O.5), would result in computationally inefficient approach. Removing one of the two steps, such as sum pooling, should lead to just an increase of the overall computational cost for the encoder (R55120 (cid:55) R25600; = 25600; 155M trainable parameters; x2), rather than affecting probes outputs. Further, since our neural VSA encoder is found effective to extract latent features even from our heavilycompressed input representation (Section 5), other dimensionality reduction approaches could also be as effective as ours (Section B). In summary, skipping the compressing steps is possible and the only drawbacks should be the increase of footprint of both the training and inference stages of the VSA-based probing (see also Section Q)."
        },
        {
            "title": "P Proof of concept for hyperdimensional probe in multimodal settings",
            "content": "Figure 15: Proof of concept for using hyperdimensional probe in multimodal settings. Figure shows complete probing procedure for MNIST-based mathematical analogy. Figure exhibits VSA encodings describing multimodal input using textual and image features."
        },
        {
            "title": "Q Computational workload",
            "content": "The computational workload of this work is split into two parts: LLM inference (exogenous, Section 4.3) and the training and probing stages of our method (endogenous, Section 4.4 and 4.5). The exogenous factor, running the Large Language Models, was the most computationally demanding task. For our experiments, we tested six different Large Language Models in inference mode, caching their embeddings for our training phase and probing them dynamically during the inference phase of our work (Figure 1). We worked with LLMs ranging from 355M parameters (GPT-2) to 109B parameters (Llama 4, Scout), using between one and three NVIDIA A100-80GB GPUs, depending on the model size. Quantization is not employed. In contrast, the computational demands of our VSA-based methodology is relatively low. The most resource-intensive stage was training our neural VSA encoder, but due to its modest size (ranging from 55M to 71M parameters, see Section C), this process remained lightweight. We performed this training on single GPU, though it could easily be handled with much less powerful and lower-memory GPUs. The probing stage is then composed of simple vector multiplications (unbinding, Section 3), after loading the heavy LLM and our lightweight trained neural VSA encoder into memory (from 800 MB of the 55M version to 1 GB of the biggest one). Future research could explore even further reducing the latent dimension of our neural VSA encoder (Section C) or adopt VSA encodings with lower dimensionality (e.g. = 512, leading to more lightweight encoder."
        }
    ],
    "affiliations": [
        "Fondazione Bruno Kessler (FBK), Trento, Italy",
        "Ipazia S.p.A., Milan, Italy",
        "University of Trento, Trento, Italy"
    ]
}