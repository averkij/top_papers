{
    "paper_title": "When and How Much to Imagine: Adaptive Test-Time Scaling with World Models for Visual Spatial Reasoning",
    "authors": [
        "Shoubin Yu",
        "Yue Zhang",
        "Zun Wang",
        "Jaehong Yoon",
        "Huaxiu Yao",
        "Mingyu Ding",
        "Mohit Bansal"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Despite rapid progress in Multimodal Large Language Models (MLLMs), visual spatial reasoning remains unreliable when correct answers depend on how a scene would appear under unseen or alternative viewpoints. Recent work addresses this by augmenting reasoning with world models for visual imagination, but questions such as when imagination is actually necessary, how much of it is beneficial, and when it becomes harmful, remain poorly understood. In practice, indiscriminate imagination can increase computation and even degrade performance by introducing misleading evidence. In this work, we present an in-depth analysis of test-time visual imagination as a controllable resource for spatial reasoning. We study when static visual evidence is sufficient, when imagination improves reasoning, and how excessive or unnecessary imagination affects accuracy and efficiency. To support this analysis, we introduce AVIC, an adaptive test-time framework with world models that explicitly reasons about the sufficiency of current visual evidence before selectively invoking and scaling visual imagination. Across spatial reasoning benchmarks (SAT, MMSI) and an embodied navigation benchmark (R2R), our results reveal clear scenarios where imagination is critical, marginal, or detrimental, and show that selective control can match or outperform fixed imagination strategies with substantially fewer world-model calls and language tokens. Overall, our findings highlight the importance of analyzing and controlling test-time imagination for efficient and reliable spatial reasoning."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 9 ] . [ 1 6 3 2 8 0 . 2 0 6 2 : r When and How Much to Imagine: Adaptive Test-Time Scaling with World Models for Visual Spatial Reasoning Shoubin Yu * 1 Yue Zhang * 1 Zun Wang 1 Jaehong Yoon 2 Huaxiu Yao 1 Mingyu Ding 1 Mohit Bansal 1 https://adaptive-visual-tts.github.io"
        },
        {
            "title": "Abstract",
            "content": "1. Introduction Despite rapid progress in Multimodal Large Language Models (MLLMs), visual spatial reasoning remains unreliable when correct answers depend on how scene would appear under unseen or alternative viewpoints. Recent work addresses this by augmenting reasoning with world models for visual imagination, but questions such as when imagination is actually necessary, how much of it is beneficial, and when it becomes harmful, remain poorly understood. In practice, indiscriminate imagination can increase computation and even degrade performance by introducing misleading evidence. In this work, we present an in-depth analysis of test-time visual imagination as controllable resource for spatial reasoning. We study when static visual evidence is sufficient, when imagination improves reasoning, and how excessive or unnecessary imagination affects accuracy and efficiency. To support this analysis, we introduce AVIC, an adaptive test-time framework with world models that explicitly reasons about the sufficiency of current visual evidence before selectively invoking and scaling visual imagination. Across spatial reasoning benchmarks (SAT, MMSI) and an embodied navigation benchmark (R2R), our results reveal clear scenarios where imagination is critical, marginal, or detrimental, and show that selective control can match or outperform fixed imagination strategies with substantially fewer world-model calls and language tokens. Overall, our findings highlight the importance of analyzing and controlling test-time imagination for efficient and reliable spatial reasoning. *Equal contribution 1Department of Computer Science, University of North Carolina, Chapel Hill 2Nanyang Technological University, Singapore. Correspondence to: Shoubin Yu <shoubin@cs.unc.edu>. Preprint. February 10, 2026. 1 Recent advances in multimodal large language models (MLLMs) (Li et al., 2024a; 2023) have led to impressive progress in visual understanding and reasoning across various tasks. These models can follow natural language instructions, perceive visual scenes, and reason over multimodal input to support decision making. Despite the progress, visual spatial reasoning remains persistent challenge (Yang et al., 2024; Cheng et al., 2024; Ray et al., 2024; Tong et al., 2024), particularly for questions whose answer depends on unseen regions, viewpoint changes, or transformations that cannot be reliably inferred from single static observation. natural way to address this challenge, mirroring how humans operate, is through visual imagination (Kosslyn et al., 2006): when the observed visual evidence is insufficient, people mentally simulate how scene would appear from alternative viewpoints or after potential movements, leveraging strong world priors learned from years of physical interaction and visual experience. Inspired by this intuition, recent work (Yang et al., 2025b; Cao et al., 2025; Qian et al., 2026) has begun to integrate MLLMs with visual world models that can generate controlled novel views conditioned on hypothetical action at inference time. However, existing approaches often invoke visual imagination using fixed and exhaustive strategies (see Figure 1), without first reasoning about whether additional imagination is necessary and helpful. This lack of deliberation can lead to problematic imagination, producing misleading (Figure 1 (b)) or redundant (Figure 1(c)) views that not only incur substantial computational overhead but can also distract downstream reasoning and result in worse performance than relying on the original observation alone. Through systematic analysis of always-on imagination (Section 3), we show that such strategies are both inefficient and unreliable, motivating the need for more adaptive use of world models. Based on these observations, we aim to answer two fundamental questions for visual spatial reasoning with world model imagination: when should model invoke visual imagination, and how much imagined visual evidence is When and How Much to Imagine: Adaptive Test-Time Scaling with World Models for Visual Spatial Reasoning Figure 1. Different cases in always-on visual imagination. Imagined views are generated independently for different beam-searched actions (shown by multiple arrows). Case 1 (Helpful): Visual imagination reveals previously unseen viewpoints, enabling helpful spatial reasoning. Case 2 (Misleading): Imagination fails to preserve task-relevant objects (e.g., the white table in the red box), resulting in incorrect spatial inference and wrong answers. Case 3 (Unnecessary): The required information is already clearly observable in the original view (e.g., the bathtub in the blue box), making additional imagined views redundant. necessary if imagination is required. Rather than treating visual imagination as an always-on operation, we seek to make it controllable, self-adaptive component during inference time. In this paper, we introduce Adaptive Visual Imagination Control (AVIC). Given an observation and question, we first regulate visual world model invocation via policy model. Specifically, the policy model first reasons about the sufficiency of the available visual evidence and conditionally decides whether to invoke the world model. If it decides not to invoke the world model, it answers directly from the observed view. Otherwise, when additional visual information is expected to be beneficial, the policy generates dynamic length action plan that specifies how the imagination should move or reorient to acquire informative viewpoints, which are then rendered by the visual world model. This design enables instance-dependent visual test-time scaling, allowing us to move beyond fixed or exhaustive imagination strategies and to study different visual spatial reasoning systematically. We evaluate our approach on challenging spatial reasoning benchmarks (SAT (Ray et al., 2024), MMSI (Yang et al., 2025a)), and navigation benchmark (R2R (Anderson et al., 2018)). Across these settings, adaptive test-time scaling achieves SoTA or competitive performance while requiring substantially fewer extra language tokens and world-model calls compared to fixed imagination strategies. Overall, beyond improved performance, our results reveal that the benefits of visual imagination are highly instance-dependent and structured by the nature of the spatial reasoning query. In particular, we find that world models are most beneficial for action-conditioned spatial reasoning, where answers depend on how scene would evolve under specific movements or viewpoint changes, while offering limited gains for queries that can be resolved from existing observations. At the same time, our analysis shows that effective visual spatial reasoning typically requires only targeted imagination, and excessive or indiscriminate simulation can introduce noise and degrade performance. Together, these findings indicate that visual imagination is selective, query-dependent test-time resource, requiring adaptive, 2 When and How Much to Imagine: Adaptive Test-Time Scaling with World Models for Visual Spatial Reasoning uncertainty-aware allocation of world-model computation. 2. Related Work Visual Spatial Reasoning with MLLMs. The rapid evolution of Multimodal Large Language Models (MLLMs) has made significant progress in various downstream tasks (Li et al., 2022; Radford et al., 2021; Liu et al., 2023; Hong et al., 2023; Zhang et al., 2024c; Yu et al., 2025b;c;a; Guo et al., 2025; Yoon et al., 2024; Deng et al., 2025). In particular, spatial reasoning has attracted considerable attention due to its critical role in bridging visual perception with embodied tasks (Zhang et al., 2024e; Wang et al., 2023; Zhang & Kordjamshidi, 2023; Zhang et al., 2024f). However, recent comprehensive evaluations indicate that current MLLMs still struggle with robust spatial reasoning (Yang et al., 2024; Cheng et al., 2024; Ray et al., 2024; Tong et al., 2024). While recent efforts aim to enhance spatial capabilities through scaling training data (Chen et al., 2024; Huang et al., 2023; Li et al., 2024b) or chain-of-thought prompting (Zhang et al., 2024b; Ji et al., 2025), they fundamentally process visual information as static 2D snapshots. In contrast, robust spatial reasoning requires an active and dynamic process where the agent can selectively acquire new visual evidence, similar to human mental simulation. World Models and Visual Imagination. Recent advances in video generation have demonstrated the potential of serving as world models, enabling agents to imagine future frames or outcomes for improved decision-making (Zhou et al., 2018; Du et al., 2023; Li & Bansal, 2023; Huang et al., 2025; Qian et al., 2026). This capability is further boosted by the emergence of controllable video generation, which allows for action-conditioned simulation (He et al., 2024; Bahmani et al., 2024; Yu et al., 2024; Wang et al., 2025b). Notably, recent works such as MindJourney (Yang et al., 2025b) have pioneered the use of world models to enhance visual spatial reasoning by synthesizing novel viewpoints. However, their model blindly generates set number of views regardless of the questions difficulty or necessity. In contrast, we show that the utility of visual imagination is highly query-dependent, motivating selective rather than uniform use of world models at test time. Test-Time Scaling. Test-time scaling (TTS) improves performance by allocating additional inference computation without retraining. Prior work has explored various scaling strategies in language/visual-language models, including self-consistency (Wang et al., 2022), tree-based search (Yao et al., 2023; Wang et al., 2025c), verifier-guided method (Lifshitz et al., 2025; Zhang et al., 2024a), and (multimodal) CoT (Yan et al., 2025; Xu et al., 2025; Wang et al., 2025a). In the visual spatial reasoning, recent works (Yang et al., 2025b; Cao et al., 2025) realized through generating novel views and ensembling, but typically apply uniform computation across instances. Our method introduces adaptive visual test-time scaling, enabling targeted imagination only when necessary and improving computational efficiency. 3. Analysis of Always-on World Model Calling Our aim in this section is to analyze the limitations of current world-model-based visual imagination strategies. We first provide background on existing world-model-based approaches in Sec. 3.1. We then design controlled empirical cases to categorize different results of always-on world model calling and discuss our observations in Sec. 3.2. 3.1. Always-on World-Model-Based Visual Imagination We consider test-time setting where an MLLM is equipped with visual world model that can generate imagined observations from imagined views. common design choice in existing methods is to invoke the world model in an always-on and greedy search manner, by calling the world model for every instance and by exhaustively exploring action branches at test time. This strategy implicitly assumes that additional visual imagination is consistently beneficial. However, visual imagination is neither free nor always reliable. Invoking the world model incurs substantial computational cost, and imagined observations may be ambiguous or noisy. As result, additional imagined views can be redundant when the answer is already evident from the initial observation, or even misleading when the world model produces noise. In this section, we take this always-on strategy as the object of study and analyze its empirical behavior and limitations. 3.2. Empirical Statistics of Always-On Imagination We begin by empirically analyzing how the always-on world model calls behavior. Specifically, as illustrated in Figure 1, we categorize each instance into three cases: Case 1 (Imagination Helpful): The model calls the world model and produces correct answer, indicating that imagined views provide beneficial spatial information. Case 2 (Imagination Misleading): The model calls the world model, but produces wrong answer because imagined views introduce misleading or noisy information. Case 3 (Imagination Unnecessary): The model produces correct answer without calling the world model, suggesting that visual imagination is redundant. We conduct our analysis on the SAT-Real benchmark (Ray et al., 2024) from the following aspects: (1) The distribution of different cases. (2) the relationship between performance and the number of imagined views, and (3) the computation 3 When and How Much to Imagine: Adaptive Test-Time Scaling with World Models for Visual Spatial Reasoning (a) Statistics of Different Cases of Alwayson Imagination (b) Performance Gain v.s. Amount of Imagined Views (c) Trade-off Among Accuracy, Token Numbers and Inference Time Figure 2. (a): In the majority of cases, visual imagination is unnecessary, while smaller fraction is helpful or misleading, highlighting the need for selective invocation rather than uniform use. (b): Accuracy gain over the baseline over the number of imagined views. Performance improvements are non-monotonic, indicating that additional imagination does not consistently translate to better reasoning and may even degrade accuracy when there are too many generated views. (c): Accuracy versus average token usage. Bubble size indicates average running time. Fixed imagination strategies achieve higher accuracy at the cost of substantially increased computation, motivating adaptive test-time scaling that balances performance and efficiency. cost of imagination. Figure 2 shows our analyzed results, and we have the following discussion. Case distribution. Figure 2a shows the distribution of different categories. We observe that the majority of instances (54%) fall into Case 3, where the model already answers correctly without any world model invocation. In contrast, imagination is genuinely helpful in only 14% of instances (Case 1). These results indicate that always-on imagination is unnecessary for most instances. Performance vs. Amount of Imagination. In Figure 2b, we analyze how accuracy changes compared with the baseline as the number of imagined views increases. The results demonstrate that adding more imagined views does not consistently improve accuracy and leads to performance degradation. It suggests that simply increasing the amount of imagination is not an ideal strategy. CostAccuracy Trade-off. Figure 2c shows the trade-off between accuracy and computational cost. While always-on imagination yields limited accuracy improvement over the baseline (4.6%), it requires nearly two orders of magnitude more tokens and incurs about 30 higher inference time. This highlights significant inefficiency of the always-on strategy, where large computational overhead is incurred for limited performance gains. Selective Imagination Upper Bound. We further report selective imagination upper bound to quantify the performance potential of selective world model usage. This analysis assumes that visual imagination is applied only when it leads to correct prediction. As shown in Table 1, the selective imagination upper bound substantially outperforms both the baseline and the always-on strategy, demonstrating that more selective test-time imagination policies are Table 1. Accuracy under different imagination strategies. Setting Accuracy (%) Baseline (No Imagination) Always-on Imagination (MJ) Selective Imagination Upper Bound 62.0 66.6 75. strongly motivated. Overall, these findings indicate that always-on world model calling is both inefficient and unreliable, motivating the need for selective and adaptive imagination strategies. 4. Adaptive Imagination Control We study visual spatial reasoning under partial observability, where single or multiple egocentric images may not provide sufficient evidence to answer question. Our goal is not to maximize the use of visual imagination, but to understand when and how much test-time visual scaling is required to achieve better visual spatial reasoning. To this end, as shown in Figure 3 (c), we introduce an adaptive test-time framework that selectively and adaptively invokes world model only when additional visual evidence is likely to be useful. The figure also directly compares our approach with standard QA baselines in Figure 3 (a) and always-on world-model methods in Figure 3 (b). In this section, we first elaborate on our task setting and then introduce our framework design in detail. 4.1. Problem Formulation We consider visual spatial reasoning task defined by an input tuple I, q, A, where is the current egocentric observation, which might include one or multiple egocentric 4 When and How Much to Imagine: Adaptive Test-Time Scaling with World Models for Visual Spatial Reasoning Figure 3. Comparison with other methods. (a) Answers directly from the current observation without any imagination. (b) Always invokes the world model w. full exploration to generate imagined views for downstream reasoning. (c) Ours: Uses policy model to first decide whether visual imagination is necessary and to plan actions accordingly. It selectively queries the world model (both when and how much) and otherwise performs direct reasoning. images/views, is multiple-choice spatial question, and = {a1, . . . , aK} is set of answer choices if provided. The correct answer may depend on spatial relations that are ambiguous, occluded, or unobserved in the current views in I. In addition to static reasoning over I, the agent may optionally invoke visual world model that can render novel imagined observations Iπ corresponding to sequence of egocentric actions π (possibly empty), and Iπ = corresponds to answering the question without imagination. The predicted answer is given by: ˆa = arg max aA (cid:0)a I, Iπ, q(cid:1), (1) As shown in the bottom of Figure 3 (c), to enable instancelevel adaptive control over visual imagination, we introduce AVIC (ADAPTIVE VISUAL IMAGINATION CONTROL), scheme that couples policy-guided gating with selective action execution. 4.2. Gating World Model via Scaled Policy Planning AVIC begins with policy model θ that explicitly reasons about the necessity of visual imagination. Given the current observation I, question q, and answer choices (if provided), the policy model outputs decision {skip, call_wm}. If = call_wm, the policy model additionally proposes short discrete action plan π = (u1, . . . , uT ), where each ut is drawn from fixed, low-level egocentric action space (e.g., turning with predefined step sizes). If = skip, the framework bypasses visual imagination and proceeds directly to final reasoning. 4.3. Action Plan Execution and Trajectory Selection When the gating mechanism selects = call_wm, the policy models action plans are executed via visual world model . Given an initial observation and an action sequence π, the world model renders sequence of imagined views Iπ = {I1, . . . , IT } corresponding to the agents egocentric trajectory. In contrast to exhaustive exploration, we only introduce action sequences proposed by the policy model. This significantly reduces the number of calls to the world model while still allowing the framework to acquire targeted visual evidence. Such that: Iπ(m) = (I, π(m)) = {I (m) 1 , . . . , (m) Tm }. (3) Different policy samples may yield different action plans and imagined trajectories, which can vary significantly in usefulness. Some trajectories reveal new spatial evidence, while others are long/redundant or degraded by generation noise. To address this variability, we introduce trajectorylevel verification mechanism that selects single, targeted imagined trajectory for downstream reasoning. Unlike prior approaches such as MindJourney, which score and select individual imagined views as keyframes during beam search, our verifier evaluates the entire imagined trajectory as coherent unit. Only the top-ranked imagined trajectory is retained for downstream reasoning. This design explicitly accounts for temporal and geometric consistency across sequential actions. In contrast, selecting isolated keyframes may discard important inter-frame dependencies, potentially harming the consistency of spatial reasoning. Such that: (d, π) θ(d, π I, q, A), (cid:40), (u1, . . . , uT ), ut U, = call_wm. = skip, π = (2) s(m) = (I, q, Iπ(m)) , π = arg max π(m) s(m) (4) (5) To improve the robustness of the gating decisions, we do not rely on single policy prediction. Instead, we perform policy-level test-time scaling by sampling the policy model times under identical inputs but independent decoding. The final gating decision is determined by majority voting across these samples, which provides simple yet effective form of self-consistency and reflects uncertainty in the necessity of additional visual evidence. 4.4. Final Answer Prediction Finally, vision-language model performs spatial reasoning using the original observation and the selected imagined views (if any) to predict the answer. If the gating mechanism selects skip, the model reasons directly over only (where Iπ = ). Such that: ˆa = arg max aA Pϕ(a I, Iπ , q) . (6) 5 When and How Much to Imagine: Adaptive Test-Time Scaling with World Models for Visual Spatial Reasoning Overall, AVIC enables an adaptive TTS that selectively invokes world model imagination for visual spatial reasoning. 5. Experiments We evaluate our framework to answer the following questions: (i) Does adaptive visual test-time scaling improve spatial reasoning performance compared to static imagination strategies? (ii) How much world-model-based imagination is actually required to achieve strong performance? 5.1. Experiment Setup Datasets and Benchmarks. We validate our proposed framework on both visual spatial reasoning benchmarks and embodied navigation tasks, covering range of spatial ambiguities and interaction requirements. For visual spatial reasoning, we evaluate on SAT (Ray et al., 2024) and MMSI (Yang et al., 2025a), two benchmarks designed to test visual spatial reasoning with single/multiple images. We also evaluate on the Room-to-Room (R2R) (Anderson et al., 2018) for the embodied navigation task. In this setting, the agent must follow natural language instructions in indoor environments. We integrate our adaptive visual test-time scaling framework into the navigation pipeline and measure performance. More details are in the Appendix. Implementation Details. Our framework is implemented on top of vision-language model and pretrained visual world model, stable virtual camera (SVC) (Zhou et al., 2025). The policy model, verifier, and final QA model are instantiated using the same base MLLM, with different prompting for gating, planning, verification, and answer prediction. All decisions are made at test time without additional fine-tuning. We scale action planning by 5 times as the default setting. More details are in the Appendix. 5.2. Main Results Visual Spatial Reasoning. As shown in Tab. 2, we first compare our adaptive visual test-time scaling framework with baselines on the SAT-Real benchmark (Ray et al., 2024), spanning five categories of visual spatial reasoning. We evaluate both reasoning performance and computational cost, measured by total token usage and running time. Across all open-source and proprietary MLLM models, our method consistently improves average accuracy over the corresponding base MLLMs and achieves performance competitive with, or superior to, the always-on/dense imagination baseline (MindJourney). With GPT-4.1, our method raises average accuracy from 74.0% to 79.3%, surpassing MindJourney while using far fewer tokens. With o1, we achieve the best overall accuracy (81.3%), improving by 6.7% over the base model. We observed that the performance gains are most pronounced on Egocentric Movement, Action Consequence, and Perspective tasks, which require egocentric or action-conditioned spatial reasoning and benefit most from selective visual imagination. Crucially, our method is substantially more efficient than dense rollout baselines. While MindJourney requires 3-9 times tokens and calls the world model on average 12 times per example, our approach achieves strong performance with only 10% tokens and an average runtime of 30 seconds. It demonstrates that adaptive world-model invocation can deliver the benefits of test-time scaling without prohibitive computational cost. Overall, these results show that selectively deciding when to imagine is more effective and efficient than uniformly scaling inference. In Tab. 3, we further apply our method to another visual spatial reasoning benchmark, MMSI-Bench (Yang et al., 2025a), and observe consistent improvements across MLLMs, highlighting the generalizability of our approach. Embodiment Navigation. As shown in Tab. 4, we further applied our adaptive visual test-time scaling method to the embodiment navigation task, and compared it with previous works (NavGPT (Zhou et al., 2024) and MapGPT (Zhang et al., 2024d)). We follow the 72-scene evaluation setting in MapGPT, and re-implement MapGPT with the same API. We apply our method within MapGPTs step-wise navigation framework. At each step, the agent reasons over navigation graph that includes the current view and previously visited views as nodes, and selects the next node as an action. Our policy model determines whether additional visual evidence is needed and, if so, selects subset of views for world-model imagination. The world model renders novel views by simulating short action sequences proposed by the policy model from the selected views. These imagined views are concatenated with the original observations to form an augmented view set, which the agent uses to predict the next navigation action. Compared to MapGPT with GPT-4o, our method achieves higher OSR, SR, and SPL while also reducing navigation error (NE), indicating more reliable goal reaching with shorter, less redundant trajectories. These gains suggest that our world-model imagination strategy helps resolve ambiguous visual-spatial decisions. Overall, the results demonstrate that our adaptive visual test-time scaling is an effective mechanism and transfers the gains for embodied navigation. 5.3. Ablation Studies As listed in Tab. 5, we analyze the contributions of worldmodel (WM) imagination, gating, and action-level test-time scaling. Vanilla baseline achieves 74.0% w.o any scaling. Always-on invoking the WM with spatial beam search and without gating or action scaling improves performance to 77.3%, but at the cost of excessive computation, requiring an average of 12.34 WM calls. Introducing gating mechanism When and How Much to Imagine: Adaptive Test-Time Scaling with World Models for Visual Spatial Reasoning Table 2. Comparison between test-time scaling methods on SAT-Real dataset with different MLLMs. The best results are denoted by the bold. Avg. WM: average world model calling times over the dataset. Method EgoM ObjM EgoAct Goal Pers Avg. # Token (K) Avg. WM InternVL3-14B (Zhu et al., 2025) + MindJourney + AVIC GPT-4o (OpenAI, 2024a) + MindJourney + AVIC GPT-4.1 (OpenAI, 2024b) + MindJourney + AVIC o1 (Jaech et al., 2024) + MindJourney + AVIC 56.5 69.6 82.6 56.5 78.3 86.9 95.7 100.0 100.0 78.3 100.0 100.0 69.5 60.9 60. 85.0 60.9 60.9 73.9 82.6 78.2 82.6 65.2 86.9 54.0 78.4 81.0 50.0 78.4 67.5 78.3 86.5 83. 73.0 78.4 86.4 73.5 79.4 91.1 64.0 70.6 79.4 88.2 79.4 85.2 73.5 82.4 91.1 45.4 42.4 54. 45.0 57.6 48.4 39.4 45.4 54.5 69.7 63.7 66.6 59.3 66.7 74.6 60.3 69.3 68.0 74.0 77.3 79. 74.6 77.3 85.3 0.2 2.5 2.0 0.9 26.0 9.5 0.7 67.1 7.6 1.4 39.4 14.6 0 12.34 0. 0 12.34 0.72 0 12.34 0.73 0 12.34 1.28 Table 3. Evaluation results on MMSI-Bench. Best results are highlighted in bold. Model GPT-4o + AVIC GPT-4.1 + AVIC Positional Relationship Attribute Motion MSR Avg. CamCam ObjObj RegReg CamObj ObjReg CamReg Meas. Appr. Cam. Obj. 34.4 31.1 36.6 32.2 24.5 29.9 26.6 24.4 23.5 27. 27.2 27.1 19.8 25.0 29.1 26.7 37.6 36.5 36.5 44.7 27.7 36. 27.7 32.5 32.8 51.2 37.5 51.5 31.8 39.7 24.2 37.8 35.1 31. 36.5 33.7 36.8 32.3 32.9 36.8 30.8 28.2 28.8 32.3 30.3 32.3 30.9 33.8 Table 4. Results on R2R embodied navigation dataset. Methods NavGPT MapGPT MapGPT MapGPT + AVIC LLMs NE OSR SR SPL GPT-3.5 GPT-4 GPT-4o GPT-4o 8.02 5.80 6.04 5.97 26.4 61.6 41.6 45.3 16.7 41.2 36.0 37. 13.0 25.4 30.8 31.9 Table 5. Ablation studies over action scaling, gating via policy model and world model. Our experiment is based on GPT-4.1. Action Scaling Gating WM Avg. WM Acc 0 12.34 0.51 0.73 74.0 77.3 73.3 79.3 via policy model alone drastically reduces WM usage (0.51 calls) but also hurts accuracy (73.3%), indicating that binary WM invocation without action-level control is insufficient and can suppress necessary imagination. In contrast, our full method that combines gating with action scaling achieves the best performance (79.3%) while keeping WM usage low (0.73 calls). This demonstrates that when to invoke the WM and how to use it are both critical: selective gating must be paired with targeted action planning to ensure that limited imagination is informative rather than restrictive. Overall, it highlights that effective visual TTS requires control over both WM invocation and action planning. 5.4. When and How Much World Model is Needed for Visual Spatial Reasoning? reasoning deficiency. Their categories and distribution are: 15.2% Limited Observability (LO). This category includes cases where the required information is not directly observable from the current view due to occlusion, limited field of view, or truncation. 42.4% Viewpoint Dependence (VD). These errors arise when the correct answer depends on transforming coordinates between egocentric and object-centric views. 31.8% Action-Conditioned Reasoning (AC). It captures cases where answering the question requires reasoning about how the scene would change after taking one or more actions, such as camera rotation or object movement. To analyze when world model imagination is necessary for visual spatial reasoning, we first manually check and group MLLM failures into four error types based on the underlying 10.6% Dynamics Understanding (DU). Errors in this category stem from failures to reason about temporal dynamics, including camera motion and object motion. 7 When and How Much to Imagine: Adaptive Test-Time Scaling with World Models for Visual Spatial Reasoning (a) Error types vary across task categories. (b) World-model usage and accuracy gains depend on error type. (c) Accuracy saturates as more imagined views are generated. Figure 4. Analysis of when and how much to invoke world-model imagination. As shown in Figure 4a, task categories in SAT do not correspond one-to-one with error types, but instead exhibit clear compositional patterns. Specifically, the Action Consequence (EgoAct) tasks are dominated by action-conditioned reasoning errors, reflecting the need to reason about viewpoint changes after taking actions. Allocentric Perspective tasks primarily suffer from viewpoint Dependence errors, indicating failures in reference-frame transformation rather than missing evidence. Object Movement tasks are largely associated with dynamic understanding errors, highlighting challenges in temporal reasoning. While limited observability errors appear across multiple tasks, suggesting that occlusion and limited field of view are general failure sources. Overall, our human analysis shows that different tasks share underlying error types, providing insights into how world models relate to different types of visual spatial reasoning. RQ1: When to Call WM? WM should be used selectively, primarily when spatial reasoning requires predicting future states under hypothetical actions, rather than reinterpreting existing visual evidence. World Models Are Most Needed for Action-Conditioned Spatial Reasoning. As shown in Figure 4b, world-model (WM) imagination yields the largest gains (+57.1%) in cases where the correct answer depends on the post-action state of the scene. In particular, questions involving counterfactual actions (e.g., what if turn left by 90?) or reasoning about how objects would appear from new viewpoint consistently require explicit visual rollout. In these cases, WMs are necessary to generate novel views through simulated actions such as turning or moving forward. In contrast, errors related to camera or object motion often require only reference-frame transformation over the current observation, where imagination provides relatively smaller benefits (+28.5%) and accounts for minority of errors (10.6%). These findings suggest that while world models can be powerful, their utility is highly instance-dependent. World model imagination is more helpful for action-conditioned reasoning scenarios rather than static reinterpretation of observed views. Together, these analyses clarify the conditions under which invoking world model is most necessary. RQ2: How much imagination is needed? Visual spatial reasoning benefits most from targeted rather than extensive WM imagination. Visual Spatial Reasoning Requires Limited WorldModel Imagination. As shown in Figure 4c, we evaluate fixed test-time scaling baselines that generate predetermined number of imagined views, in contrast to our adaptive world-model (WM) invocation and action planning. We find that introducing small amount of WM imagination yields substantial gains, with one to two imagined views improving average accuracy from 74.0% to 80.0%. Beyond this range, additional imagination increases computational cost without further accuracy gains, and can even degrade performance due to redundant or noisy views. Notably, our adaptive method achieves strong performance while using only 0.88 imagined views on average, demonstrating that selective and targeted WM invocation is more effective than fixed, exhaustive scaling. Together, these analyses answer RQ2 by clarifying how much world-model imagination is helpful for visual spatial reasoning. Qualitative Analysis. We also provide qualitative examples as illustrated at the top of Figure 5. We compare our adaptive visual TTS method with the always-on imagination method, MindJourney (MJ). In the first example, the target object (the cash counter) is already clearly visible in the observed view. Our method correctly identifies that additional visual imagination is unnecessary and directly skips world model. In contrast, MJ indiscriminately invokes the world model, generating multiple imagined views that introduce misleading evidence and ultimately lead to an incorrect prediction. In the second example, AVIC yields the correct answer by selectively imagining the state where the agent 8 When and How Much to Imagine: Adaptive Test-Time Scaling with World Models for Visual Spatial Reasoning Figure 5. Qualitative examples on SAT of the always-on imagination method and our adaptive method, as well as the R2R navigation task. In the navigation example, the green option is selected by the model with adaptive imagination via our method, while the red one is without world model imagination. is in front of the trash bin. In contrast, MJ performs dense imagination and generates views that do not accurately reflect this critical spatial condition, leading to an incorrect prediction. Furthermore, we present qualitative navigation example at the bottom of Figure 5. Our adaptive visual test-time scaling selectively augments informative indoor observations (e.g., zooming in or turning to explore nearby views), enabling the agent to better inspect the environment and align its actions with the global instruction (go to the kitchen). In contrast, the baseline without visual imagination lacks sufficient perceptual evidence and consequently chooses an incorrect direction. 6. Conclusion In this paper, we study visual spatial reasoning with world models through the lens of adaptive test-time scaling, showing that always-on imagination is often unnecessary and can be misleading. To address this, we introduce AVIC, which selectively decides when to invoke world model and how much visual imagination to perform at inference time. Across spatial reasoning benchmarks, AVIC achieves SoTA or competitive results while substantially reducing world-model calls, token usage, and inference time compared to fixed imagination strategies. Our analysis reveals that world models are most beneficial for action-conditioned reasoning, and typically require only limited, targeted imagination. These results highlight the importance of instancedependent test-time scaling for efficient and reliable reasoning with world models. 7. Acknowledgments This work was supported by ONR Grant N00014-23-1-2356, ARO Award W911NF2110220, DARPA ECOLE Program No. HR00112390060, Microsoft Accelerate Foundation Models Research (AFMR) grant program, NSF AI Engage Institute DRL-2112635, and Cisco and Capital One Faculty Awards. The views contained in this article are those of the authors and not of the funding agency."
        },
        {
            "title": "References",
            "content": "Anderson, P., Wu, Q., Teney, D., Bruce, J., Johnson, M., Sünderhauf, N., Reid, I., Gould, S., and Van Den Hengel, A. Vision-and-language navigation: Interpreting visuallygrounded navigation instructions in real environments. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 36743683, 2018. Bahmani, S., Skorokhodov, I., Qian, G., Siarohin, A., Menapace, W., Tagliasacchi, A., Lindell, D. B., and Tulyakov, S. Ac3d: Analyzing and improv9 When and How Much to Imagine: Adaptive Test-Time Scaling with World Models for Visual Spatial Reasoning ing 3d camera control in video diffusion transformers. 2025 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2287522889, 2024. URL https://api.semanticscholar. org/CorpusID:274423482. Cao, M., Li, X., Liu, X., Reid, I., and Liang, X. Spatialdreamer: Incentivizing spatial reasoning via active mental imagery. arXiv preprint arXiv:2512.07733, 2025. Chen, B., Xu, Z., Kirmani, S., Ichter, B., Driess, D., Florence, P., Sadigh, D., Guibas, L. J., and Xia, F. Spatialvlm: Endowing vision-language models with spatial reasoning capabilities. 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1445514465, 2024. URL https://api.semanticscholar. org/CorpusID:267069344. Cheng, A.-C., Yin, H., Fu, Y., Guo, Q., Yang, R., Kautz, J., Wang, X., and Liu, S. Spatialrgpt: Grounded spatial reasoning in vision language model. ArXiv, abs/2406.01584, 2024. URL https://api.semanticscholar. org/CorpusID:270215984. Deng, A., Chen, T., Yu, S., Yang, T., Spencer, L., Tian, Y., Mian, A. S., Bansal, M., and Chen, C. Motion-grounded video reasoning: Understanding and perceiving motion at pixel level. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 86258636, 2025. Du, Y., Yang, M., Dai, B., Dai, H., Nachum, O., Tenenbaum, J. B., Schuurmans, D., and Abbeel, P. Learning universal policies via text-guided video generation. ArXiv, abs/2302.00111, 2023. URL https: //api.semanticscholar.org/CorpusID: 256459809. Guo, X., Song, X., Zhang, Y., Liu, X., and Liu, X. Rethinking vision-language model in face forensics: Multi-modal interpretable forged face detector. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 105116, 2025. He, H., Xu, Y., Guo, Y., Wetzstein, G., Dai, B., Li, H., and Yang, C. Cameractrl: Enabling camera control for text-to-video generation. ArXiv, abs/2404.02101, 2024. URL https://api.semanticscholar. org/CorpusID:268857272. Hong, Y., Zhen, H., Chen, P., Zheng, S., Du, Y., Chen, Z., and Gan, C. Injecting the 3d world into large language models. ArXiv, abs/2307.12981, 2023. URL https://api.semanticscholar. org/CorpusID:260356619. 3d-llm: Huang, J., Yong, S., Ma, X., Linghu, X., Li, P., Wang, Y., Li, Q., Zhu, S.-C., Jia, B., and Huang, S. An embodied 10 generalist agent in 3d world. ArXiv, abs/2311.12871, 2023. URL https://api.semanticscholar. org/CorpusID:265351495. Huang, Y., Wang, Z., Lin, H., Kim, D.-K., Omidshafiei, S., Yoon, J., Zhang, Y., and Bansal, M. Planning with sketchguided verification for physics-aware video generation. arXiv preprint arXiv:2511.17450, 2025. Jaech, A., Kalai, A., Lerer, A., Richardson, A., El-Kishky, A., Low, A., Helyar, A., Madry, A., Beutel, A., Carney, A., et al. OpenAI o1 system card, 2024. Ji, B., Agrawal, S., Tang, Q., and Wu, Y. Enhancing spatial reasoning in vision-language models via chain-of-thought prompting and reinforcement learning. ArXiv, abs/2507.13362, 2025. URL https: //api.semanticscholar.org/CorpusID: 280048774. Kosslyn, S. M., Thompson, W. L., and Ganis, G. The case for mental imagery. Oxford University Press, 2006. Li, B., Zhang, Y., Guo, D., Zhang, R., Zhang, F., Zhang, H., Zhang, K., Li, Y., Liu, Z., and Li, C. LLaVA-onevision: Easy visual task transfer, 2024a. Li, J. and Bansal, M. Panogen: Text-conditioned panoramic environment generation for vision-andlanguage navigation. ArXiv, abs/2305.19195, 2023. https://api.semanticscholar.org/ URL CorpusID:258967291. Li, J., Li, D., Xiong, C., and Hoi, S. C. H. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In International Conference on Machine Learning, 2022. URL https://api.semanticscholar. org/CorpusID:246411402. Li, J., Li, D., Savarese, S., and Hoi, S. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International conference on machine learning, pp. 1973019742. PMLR, 2023. Li, J., Cho, J., Sung, Y.-l., Yoon, J., and Bansal, M. Selma: Learning and merging skill-specific text-to-image experts with auto-generated data. In Neural Information Processing Systems, 2024b. Lifshitz, S., McIlraith, S. A., and Du, Y. Multi-agent verification: Scaling test-time compute with multiple verifiers. arXiv preprint arXiv:2502.20379, 2025. instruction tuning. Liu, H., Li, C., Wu, Q., and Lee, Y. J. ViArXiv, abs/2304.08485, sual 2023. URL https://api.semanticscholar. org/CorpusID:258179774. When and How Much to Imagine: Adaptive Test-Time Scaling with World Models for Visual Spatial Reasoning OpenAI. Hello GPT-4o. OpenAI Blog, 2024a. OpenAI. GPT-4.1 technical overview, 2024b. URL https: //openai.com/index/gpt-4-1/. Qian, C., Acikgoz, E. C., Li, B., Chen, X., Zhang, Y., He, B., Luo, Q., Hakkani-Tür, D., Tur, G., Li, Y., et al. Current agents fail to leverage world model as tool for foresight. arXiv preprint arXiv:2601.03905, 2026. Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., and Sutskever, I. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, 2021. URL https://api.semanticscholar. org/CorpusID:231591445. Ray, A., Duan, J., Brown, E., Tan, R., Bashkirova, D., Hendrix, R., Ehsani, K., Kembhavi, A., Plummer, B. A., Krishna, R., et al. SAT: Dynamic spatial aptitude training for multimodal language models, 2024. Tong, S., Brown, E., Wu, P., Woo, S., Middepogu, M., Akula, S. C., Yang, J., Yang, S., Iyer, A., Pan, X., Wang, A., Fergus, R., LeCun, Y., and Xie, S. Cambrian-1: fully open, vision-centric exploration of multimodal llms. ArXiv, abs/2406.16860, 2024. URL https://api.semanticscholar. org/CorpusID:270703300. Wang, Z., Yoon, J., Yu, S., Islam, M., Bertasius, G., and Bansal, M. Video-rts: Rethinking reinforcement learning and test-time scaling for efficient and enhanced video reasoning. In Conference on Empirical Methods in Natural Language Processing, 2025c. Xu, Y., Guo, X., Zeng, Z., and Miao, C. Softcot++: Testtime scaling with soft chain-of-thought reasoning. arXiv preprint arXiv:2505.11484, 2025. Yan, Z., Li, X., He, Y., Yue, Z., Zeng, X., Wang, Y., Qiao, Y., Wang, L., and Wang, Y. Videochat-r1. 5: Visual test-time scaling to reinforce multimodal reasoning by iterative perception. arXiv preprint arXiv:2509.21100, 2025. Yang, J., Yang, S., Gupta, A. W., Han, R., Li, F.- F., and Xie, S. Thinking in space: How multimodal large language models see, remember, and recall spaces. 2025 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1063210643, 2024. URL https://api.semanticscholar. org/CorpusID:274822996. Yang, S., Xu, R., Xie, Y., Yang, S., Li, M., Lin, J., Zhu, C., Chen, X., Duan, H., Yue, X., et al. Mmsi-bench: benchmark for multi-image spatial intelligence. arXiv preprint arXiv:2505.23764, 2025a. Yang, Y., Liu, J., Zhang, Z., Zhou, S., Tan, R., Yang, J., Du, Y., and Gan, C. Mindjourney: Test-time scaling with world models for spatial reasoning, 2025b. Wang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., Narang, S., Chowdhery, A., and Zhou, D. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022. Yao, S., Yu, D., Zhao, J., Shafran, I., Griffiths, T., Cao, Y., and Narasimhan, K. Tree of thoughts: Deliberate problem solving with large language models. Advances in Neural Information Processing Systems, 36:1180911822, 2023. Wang, Y., Wu, S., Zhang, Y., Yan, S., Liu, Z., Luo, J., and Fei, H. Multimodal chain-of-thought reasoning: comprehensive survey. arXiv preprint arXiv:2503.12605, 2025a. Wang, Z., Li, J., Hong, Y., Wang, Y., Wu, Q., Bansal, M., Gould, S., Tan, H., and Qiao, Y. Scaling data generation in vision-and-language nav2023 IEEE/CVF International Conferigation. ence on Computer Vision (ICCV), pp. 1197511986, 2023. URL https://api.semanticscholar. org/CorpusID:260315945. Wang, Z., Cho, J., Li, J., Lin, H., Yoon, J., Zhang, Epic: Efficient video camera Y., and Bansal, M. learning with precise anchor-video guidcontrol ance. ArXiv, abs/2505.21876, 2025b. URL https: //api.semanticscholar.org/CorpusID: 278960105. Yoon, J., Yu, S., and Bansal, M. Raccoon: versatile instructional video editing framework with auto-generated narratives. arXiv preprint arXiv:2405.18406, 2024. Yu, S., Liu, D., Ma, Z., Hong, Y., Zhou, Y., Tan, H., Chai, J., and Bansal, M. Veggie: Instructional editing and reasoning video concepts with grounded generation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 1514715158, 2025a. Yu, S., Yoon, J., and Bansal, M. Crema: Generalizable and efficient video-language reasoning via multimodal modular fusion. In International Conference on Learning Representations, 2025b. Yu, S., Zhang, Y., Wang, Z., Yoon, J., and Bansal, M. Mexa: Towards general multimodal reasoning with dynamic multi-expert aggregation. Findings of the 2025 Conference on Empirical Methods in Natural Language Processing, 2025c. 11 When and How Much to Imagine: Adaptive Test-Time Scaling with World Models for Visual Spatial Reasoning Stereo magnification. Zhou, T., Tucker, R., Flynn, J., Fyffe, G., and Snavely, ACM Transactions on N. Graphics (TOG), 37:1 12, 2018. URL https: //api.semanticscholar.org/CorpusID: 219893035. Zhu, J., Wang, W., Chen, Z., Liu, Z., Ye, S., Gu, L., Duan, Y., Tian, H., Su, W., Shao, J., et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models, 2025. Yu, W., Xing, J., Yuan, L., Hu, W., Li, X., Huang, Z., Gao, X., Wong, T.-T., Shan, Y., and Tian, Y. Viewcrafter: Taming video diffusion models for highIEEE transactions fidelity novel view synthesis. on pattern analysis and machine intelligence, PP, 2024. URL https://api.semanticscholar. org/CorpusID:272366673. Zhang, L., Hosseini, A., Bansal, H., Kazemi, M., Kumar, A., and Agarwal, R. Generative verifiers: Reward modeling as next-token prediction. arXiv preprint arXiv:2408.15240, 2024a. Zhang, R., Zhang, B., Li, Y., Zhang, H., Sun, Z., Gan, Improve vision Z., Yang, Y., Pang, R., and Yang, Y. language model chain-of-thought reasoning. In Annual Meeting of the Association for Computational Linguistics, 2024b. URL https://api.semanticscholar. org/CorpusID:273507544. Zhang, Y. and Kordjamshidi, P. Vln-trans: Translator for the vision and language navigation agent. In The 61st Annual Meeting Of The Association For Computational Linguistics, 2023. Zhang, Y., Colman, B., Guo, X., Shahriyari, A., and Bharaj, G. Common sense reasoning for deepfake detection. In European conference on computer vision, pp. 399415. Springer, 2024c. Zhang, Y., He, Z., Li, J., Lin, J., Guan, Q., and Yu, W. Mapgpt: an autonomous framework for mapping by integrating large language model and cartographic tools. Cartography and Geographic Information Science, 51(6): 717743, 2024d. Zhang, Y., Ma, Z., Li, J., Qiao, Y., Wang, Z., Chai, J., Wu, Q., Bansal, M., and Kordjamshidi, P. Vision-andlanguage navigation today and tomorrow: survey in the era of foundation models. Trans. Mach. Learn. Res., 2024, 2024e. URL https://api.semanticscholar. org/CorpusID:271064503. Zhang, Y., Xu, Z., Shen, Y., Kordjamshidi, P., and Huang, L. Spartun3d: Situated spatial understanding of 3d world in large language models. arXiv preprint arXiv:2410.03878, 2024f. Zhou, G., Hong, Y., and Wu, Q. Navgpt: Explicit reasoning in vision-and-language navigation with large language models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pp. 76417649, 2024. Zhou, J., Gao, H., Voleti, V., Vasishta, A., Yao, C.-H., Boss, M., Torr, P., Rupprecht, C., and Jampani, V. Stable virtual camera: Generative view synthesis with diffusion models, 2025. When and How Much to Imagine: Adaptive Test-Time Scaling with World Models for Visual Spatial Reasoning A. Implementation Details Metrics. We evaluate SAT/MMSI spatial reasoning benchmark with multiple-choice QA accuracy. We evaluate navigation performance using four standard metrics: Navigation Error (NE), Oracle Success Rate (OSR), Success Rate (SR), and Success weighted by Path Length (SPL). NE measures the geodesic distance between the agents final position and the target, while SR reports the fraction of episodes where the final position is within predefined success threshold. OSR measures whether the agent ever reaches within the success threshold at any point along its trajectory, reflecting exploration ability independent of stopping. SPL jointly evaluates success and efficiency by weighting successful episodes by the ratio between shortest-path length and the actual trajectory length. Prompts. We provide extra technical details of our adaptive visual test time scaling framework. In Tab. 8, we provide verifier prompts that are used to score each generated trajectory, and in Tab. 9, we provide prompts for world model gating and action planning in our policy model, B. Extra Experiments Effect of Policy Model and QA Model. As listed in Tab. 6, we compare different combinations of policy models and QA models on SAT-Real. We find upgrading the QA model yields substantial improvements regardless of the policy model used (68.0% 80.0%). These results indicate that SAT performance is primarily bottlenecked by the QA models spatial reasoning capability, but stronger policy model can also bring improvements. It also implies that policy modelling mainly affects efficiency and control of world-model invocation. Framework Error Analysis. Our adaptive world-model (WM) invocation policy does not call the WM uniformly across tasks. It triggers WM imagination most frequently for Egocentric Movement tasks (EgoM, 82.6%) and Action Consequence tasks (EgoAct, 70.2%), while being much more conservative for goal-oriented tasks (Goal, 26.4%). While frequent WM usage on EgoM improves accuracy, it is misaligned with the dominant error sources identified manually in Observation 1 and 2, where many failures instead stem from action-conditioned and viewpoint-dependent reasoning. This mismatch results in low recall and precision for cases that truly require world-model imagination, as we reported in Tab. 7. It indicates that the current policy design remains significant chance for improvement. Overall, these results reveal substantial room for improving adaptive WM calling strategies, motivating future work on error-aware and state-aware invocation policies that better align WM usage with underlying reasoning demands. Table 6. Comparison of the different combinations of policy models and QA models over the SAT-Real dataset. Table 7. Recall and Precision of our method on error cases that may need world model imagination. Policy Model QA Model SAT Acc Metrics EgoM ObjM EgoAct Goal Per All GPT-4o GPT-4o o1 o1 GPT-4o o1 o1 GPT-4o 68.0 80.0 81.3 68.6 C. Impact Statement Recall Precision 100.00 33.33 50.0 5.62 55.55 19.23 33.33 52.63 43.90 27.14 62.5 22.2 This work studies world-model-based visual imagination in visual spatial reasoning and highlights the limitations of existing always-on test-time imagination methods. Through systematic analysis, we show that indiscriminate visual imagination can be computationally inefficient and, in some cases, harmful due to misleading or redundant imagined views. Our findings emphasize the importance of adaptive test-time computation, demonstrating that effective spatial reasoning requires selectively invoking visual imagination only when necessary and scaling it appropriately. Beyond the specific benchmarks studied, our insights are broadly applicable to multimodal agents that rely on test-time simulation, including embodied AI and interactive systems. 13 When and How Much to Imagine: Adaptive Test-Time Scaling with World Models for Visual Spatial Reasoning Table 8. Verifier prompts for scoring imagined view plans. Role. You are an independent evaluator for visual spatial reasoning. Input. multiple-choice question, answer options, the current observation image(s), and one candidate action plan. The plan includes imagined views rendered by world model. Task. Score how useful the imagined views are for answering the question. Score Range. Integer from 0 (not helpful, irrelevant, or low quality) to 9 (highly helpful and informative). Scoring Guidelines. Assign higher scores if the imagined views reveal missing evidence needed to answer the question (e.g., resolving occlusion or viewpoint ambiguity). Assign higher scores if the imagined views are sharp, coherent, and visually consistent. Assign lower scores if the views are redundant, uninformative, distorted, or unrelated to the question. If the original observations are already sufficient, most plans should receive low scores. Rules. Do not answer the question. Output only single integer between 0 and 9. Do not output any additional text. Output Example. 14 When and How Much to Imagine: Adaptive Test-Time Scaling with World Models for Visual Spatial Reasoning Table 9. Policy model prompts for world model gating and action planning. Role. You are policy model for spatial reasoning in 3D indoor environment. Your goal is to decide whether to invoke world model (WM) and, if needed, plan actions that acquire the most informative imagined views. Input. One or more images, multiple-choice question, and answer options. Tasks. Decide whether to SKIP or CALL the world model. If CALL, generate short action plan (16 actions) to gather additional visual evidence. Action Space (Discrete, Fixed). move-forward 0.25 meters turn-left 9 degrees turn-right 9 degrees Action Composition Guidelines. Repeated turns approximate larger rotations (e.g., 2 turns 18, 5 turns 45, 10 turns 90). When question specifies larger angle, approximate it using repeated 9 turns. When to Call the World Model. The answer is not directly observable from the current view. The question depends on perspective, facing direction, rotation, or left/right relations. The question requires reasoning about motion or state changes over time. Constraints. Do not generate cancelling or oscillating actions (e.g., left then right). If turning, choose single direction and turn monotonically. Output Format (JSON only). { \"decision\": \"skip\" \"call_wm\", \"reason\": \"<one sentence>\", \"actions\": [ {\"type\": \"move-forward\" \"turn-left\" \"turn-right\", \"value\": <number>} ] }"
        }
    ],
    "affiliations": [
        "Department of Computer Science, University of North Carolina, Chapel Hill",
        "Nanyang Technological University, Singapore"
    ]
}