{
    "paper_title": "Tell me why: Visual foundation models as self-explainable classifiers",
    "authors": [
        "Hugues Turbé",
        "Mina Bjelogrlic",
        "Gianmarco Mengaldo",
        "Christian Lovis"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Visual foundation models (VFMs) have become increasingly popular due to their state-of-the-art performance. However, interpretability remains crucial for critical applications. In this sense, self-explainable models (SEM) aim to provide interpretable classifiers that decompose predictions into a weighted sum of interpretable concepts. Despite their promise, recent studies have shown that these explanations often lack faithfulness. In this work, we combine VFMs with a novel prototypical architecture and specialized training objectives. By training only a lightweight head (approximately 1M parameters) on top of frozen VFMs, our approach (ProtoFM) offers an efficient and interpretable solution. Evaluations demonstrate that our approach achieves competitive classification performance while outperforming existing models across a range of interpretability metrics derived from the literature. Code is available at https://github.com/hturbe/proto-fm."
        },
        {
            "title": "Start",
            "content": "Tell me why: Visual foundation models as self-explainable classifiers Hugues Turbe 1 2 Mina Bjelogrlic 1 2 Gianmarco Mengaldo 3 Christian Lovis 1 2 5 2 0 2 6 2 ] . [ 1 7 7 5 9 1 . 2 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Visual foundation models (VFMs) have become increasingly popular due to their state-of-the-art performance. However, interpretability remains crucial for critical applications. In this sense, selfexplainable models (SEM) aim to provide interpretable classifiers that decompose predictions into weighted sum of interpretable concepts. Despite their promise, recent studies have shown that these explanations often lack faithfulness. In this work, we combine VFMs with novel prototypical architecture and specialized training objectives. By training only lightweight head (approximately 1M parameters) on top of frozen VFMs, our approach (ProtoFM) offers an efficient and interpretable solution. Evaluations demonstrate that our approach achieves competitive classification performance while outperforming existing models across range of interpretability metrics derived from the literature. Code is available at https: //github.com/hturbe/proto-fm. 1. Introduction Deep neural networks have shown impressive results in vision tasks, including segmentation and classification. Recent progress has been largely driven by the development of visual foundation models (VFMs), that is, models trained on vast image datasets that serve various downstream tasks. As VFMs are increasingly applied in diverse domains, interpretability is becoming crucial for critical fields like medicine and remote sensing. Given the ambiguity around what interpretability means (Lipton, 2018), in this work we focus on the ability to reflect the influence of the input fea1Department of Radiology and Medical Informatics, University of Geneva, Geneva, Switzerland 2Division of Medical Information Sciences, Geneva University Hospitals, Geneva, Switzerland 3Department of Mechanical Engineering, College of Design and Engineering, National University of Singapore, Singapore. Correspondence to: Hugues Turbe <hugues.turbe@unige.ch>. Preliminary work currently under review. tures on the models prediction. Besides legal requirements (Metikoˇs & Ausloos, 2024), the absence of interpretability has often been pointed out as barrier to the adoption of deep learning models in various domains, such as the healthcare sector (Reddy, 2022). Beyond the adoption challenges, interpretability may allow for scientific discovery where model might have discovered some rules in the data unknown to the scientific community that can be uncovered through explaining the models predictions (Mengaldo, 2025). Methods for model interpretability can be broadly categorized into two paradigms based on the stage of model development where interpretability is incorporated: (i) the intrinsic paradigm, and (ii) the post-hoc paradigm (Madsen et al., 2024). Post-hoc methods aim to provide explanations for already trained models. These methods are often modelagnostic (e.g., SHAP (Lundberg & Lee, 2017)), and do not interfere with model training or performance. However, these methods are often criticized for their lack of faithfulness (Adebayo et al., 2018; Hooker et al., 2019a; Turbe et al., 2023; Wei et al., 2024), raising significant concerns about their reliability in critical applications. This has led to growing advocacy for the intrinsic paradigm (Rudin, 2019), which involves models designed to be interpretable by design, which are also referred to as self-explainable models (SEM). notable direction within SEM is the development of partprototype models, which aim to decompose predictions into weighted sum of interpretable concepts. Although these models are theoretically designed to provide consistent explanations, recent findings indicate that the explanations they generate often lack faithfulness. Specifically, these models tend to mislocalize critical regions of the input that are important for classification (Sacha et al., 2024; Carmichael et al., 2024a) and fail to represent coherent concepts in the input space (Hoffmann et al., 2021), aspects that undermine their interpretability. Another general criticism of the intrinsic paradigm is that previous approaches are not designed to leverage pre-trained models, such as vision foundation models (VFMs) (Madsen et al., 2024). In this work, we aim to address the challenges just described that affect SEMs . We achieve this goal by designing new Tell me why: Visual foundation models as self-explainable classifiers architecture, ProtoFM, depicted in Figure 1, that (i) significantly improves the quality of the explanations provided, and (ii) allows using frozen VFMs. One of the key motivations behind using existing VFMs, such as Dinov2 (Oquab et al., 2024), AMRADIO (Ranzinger et al., 2024), and SAM (Kirillov et al., 2023), is their state-of-the-art performance on variety of downstream tasks, including classification and segmentation in general image domains (Oquab et al., 2024; Ranzinger et al., 2024), and specialized tasks e.g., RADDINO (Perez-Garcıa et al., 2025) for chest radiography, and SkySense (Guo et al., 2024) for remote sensing. We argue that using frozen VFMs enables efficient training on diverse classification tasks with minimal number of parameters trained. Furthermore, VFMs trained with patchlevel objectives, yield strong local representations, as evidenced by their high performance in segmentation (Oquab et al., 2024; Perez-Garcıa et al., 2025). This property is important to addresses the critical issue of spatial alignment in prototypical models. Contributions 1. Introduce novel architecture which allows to leverage frozen VFMs providing lightweight approach ( 1M trained parameter). The evaluation demonstrates that the architecture is competitive in terms of classification performance achieves SOTA performance across the range of interpretability desiderata derived from the literature. 2. We extensively evaluate number of prototypical models from the literature, highlighting important issues regarding the correctness and contrastivity of the explanations obtained with these models, justifying the introduction of the novel architecture. 2. Related work The development of Prototypical Part Network research was initiated by the ProtoPNet architecture (Chen et al., 2019). ProtoPNet works by comparing parts of an image to learned prototypes, which are meant to represent semantically coherent concepts. This approach allows the model to make predictions based on the similarity of image parts to these prototypes, offering form of reasoning that is qualitatively similar to human decision-making. Several challenges with the original ProtoPNet have led to the development of more advanced models. The original ProtoPNet used fixed number of class-specific prototypes. This means that each class had pre-determined number 2 of prototypes associated with it, which could lead to large explanations and redundant prototypes (Nauta et al., 2021a). ProtoPShare was developed to address this issue by enabling prototypes to be shared across different classes, reducing the total number of prototypes needed and allowing the model to find similarities between classes (Rymarczyk et al., 2021). ProtoPool built on this work and introduced fully differentiable assignment of prototypes to classes, allowing end-to-end training of the model (Rymarczyk et al., 2022). Other approaches have explored different prototype representations. Deformable ProtoPNet introduced spatially flexible prototypes (Donnelly et al., 2022). ST-ProtoPNet aimed to improve classification accuracy by learning support and trivial prototypes, drawing an analogy with Support Vector Machine theory (Wang et al., 2023). On different note, ProtoTree looked at replacing the final linear layer with decision tree (Nauta et al., 2021b), while PIP-Net focused on producing sparse explanations (Nauta et al., 2023a). Most of the presented works described above use different variations of CNN backbones, but the ProtoVit architecture recently looked at leveraging Vision Transformers (ViT) (Ma et al., 2024). While part-prototype networks are theoretically designed to be interpretable, researchers have investigated their interpretability and identified several limitations that can undermine this promise. Studies have shown that such models may exhibit: (i) semantic gap, where prototypes fail to consistently represent the same concepts across different images (Hoffmann et al., 2021; Kim et al., 2022b; Nauta et al., 2023a), and (ii) spatial misalignment, where the pixels used by the model for predictions are not correctly localized (Carmichael et al., 2024b; Sacha et al., 2024). While several studies have examined specific aspects of interpretability evaluation (Gautam et al., 2022; Carmichael et al., 2024b; Huang et al., 2023), none have provided holistic and quantitative assessment of prototypical models interpretability that addresses its multifaceted nature. The Co-12 properties were recently introduced as comprehensive framework for evaluating explanation quality (Nauta et al., 2023b). These properties have been designed for interpretable methods in general, including both post-hoc interpretability methods and SEMs. We summarize the most important properties in Table 1. They encompass many desiderata that have been independently formulated in the literature focused on SEMs with metrics such as Prototypical part Location Change (PLC) (Sacha et al., 2024) or Relevance Ordering Test (ROT) (Carmichael et al., 2024b) which aim to evaluate the spatial alignment and fall under the Correctness property. Metrics to evaluate the consistency, stability and compactness desiderata have also been proposed in the literature (Huang et al., 2023; Nauta et al., 2023a). Tell me why: Visual foundation models as self-explainable classifiers Table 1. Properties and associated metrics for evaluating interpretability. denotes metrics from the FunnyBirds framework. indicates metrics from the literature adapted to the FunnyBirds dataset Property Definition Correctness Completeness Contrastivity Consistency Stability Compactness Composition The explanation faithfully represents the models behavior. The models behavior is fully captured by the explanation. Discriminative parts are correctly captured by the explanations Prototypes are consistent in the input space. Prototype attribution should be stable under small perturbations. The explanation is compact to be intelligible by the user. The explanation presentation should reflect the models behavior. Associated Metrics SD CSDC, PC, DC, TS Consistency Stability Global and Local size SEC However, the metrics and their evaluation introduced to evaluate Prototypical models suffer from two main issues: i) Distribution shift under pixel ablation and ii) Lack of precise part annotations. Regarding the first issue, interpretability evaluations are typically conducted by removing pixels that are considered important by an interpretability method and assessing the resulting change in the models prediction. However, this ablation creates distribution shift between the training dataset and the dataset used for the interpretability evaluation which might alter the evaluation (Hooker et al., 2019b). The second aspect is related more specifically to the evaluation of SEM which often aims to measure the consistency of the prototype used for the classification based on part annotations (e.g. beak, wing in the CUB dataset) (Nauta et al., 2023a; Carmichael et al., 2024b; Huang et al., 2023). However, the datasets used for these evaluations do not include precise annotations for individual parts. As result, the evaluation relies on estimating part localization by placing fixed-size box around point that represents the parts position.Evaluation performed with these annotations considers only the top patches activated by prototype which is then compared to the fixed-size box. This approach has been criticized as not correctly reflecting the models decision process (Carmichael et al., 2024a). The FunnyBirds dataset has been developed to evaluate three of the Co-12 properties, namely correctness, completeness and contrastivity. We list the associated metrics from FunnyBirds in Table 1 and refer the reader to the paper introducing these metrics for more details (Hesse et al., 2023). The developed metrics and dataset address the issues stated above and unify the evaluation of post-hoc interpretability methods and SEM models under single framework. As part of their benchmark, 24 combinations of interpretability methods and models are evaluated. However, the only prototypical model evaluated is ProtoPNet. Recently, the methodology to compute the metrics presented in FunnyBirds for prototypical models was slightly modified, but again only ProtPNet was evaluated (Opłatek et al., 2024). Next, we present the novel architecture along with set of metrics to improve the overall explainability of selfexplainable models and thoroughly evaluate the explanations provided by these models. 3. Methodology Problem setting We consider the classification task that consists of mapping an image C to labelled target ND where H, , represent, respectively, the height, width, and number of channels of the input image, and is the number of classes. 3.1. Model architecture As depicted in Figure 1, our model ProtoFM leverages frozen VFM as visual feature extractor mapping image to patch embedding Fi RC for patch index [1, . . . , I], and = with indicating the patch size of the VFM and cf the embedding dimension. Similarly version augmented with geometric and color transformation x, is mapped to . projector then maps the feature from the backbone into an embedding space, of dimension cz, such that the image and its augmented version are mapped to = z(F ) and = z(F ) RczI . cosine similarity is then computed between and set of trainable prototypes {s,t} = {pn Rcz }, with [1, , ] where is the number of prototypes. The prototypes aim to represent the concepts that can decompose the image and be used to classify the latter. The and exponents respectively refer to the student and teacher prototypes, with the latter being updated through an exponential moving average (EMA) of the student prototype. To improve the consistency of the prototypes assignment, we follow student-teacher approach similar to the segmentation model proposed in SmooSeg (Lan et al., 2023). Two masks, Ms and Mt attributing pixels from the image to prototype are computed by measuring the cosine similarity between the prototypes and the projections as follows: Ms = cos(sg(Z); Φs) ; Mt = cos(Z ; sg (Φt)) (1) 3 Tell me why: Visual foundation models as self-explainable classifiers Figure 1. Model architecture. The model is composed of frozen VFM followed by projector and classification head in order to classify images from set of learned concepts. where sg stands for the stop-gradient operations. To compare the two masks which come from two different views of given image, the two masks are aligned into overlapping regions using RoIAlign (He et al., 2017) to obtain Ms and Mt RIN . The prototypes soft-assignments are then computed for the aligned and non-aligned mask: A{s,t} = σ (cid:16) {s,t}/τ (cid:17) ; A{s,t} = σ (cid:16) {s,t}/τ (cid:17) (2) ; = mean(cid:0)topk(Ms hs where σ denotes the softmax operation across dimension and τ the temperature . The prototype assignments are aggregated at the image level using top-k mean operation, which captures the presence of each prototype within given image by averaging its largest activation values: n)(cid:1) n)(cid:1) (3) where hn quantifies the presence of prototype in the image. The final classification head consists of linear classifier with weights constrained to be positive, design choice aimed at enhancing the models interpretability. This linear layer processes the vector = {hn [1, . . . , ]} to produce an importance matrix = (rd,n) RDN with the class-specific importance score rd,n of prototype toward class d: = mean(cid:0)topk(Mt ht rd,n = Wd,n hn. (4) The final prediction for class is obtained by summing the contributions of all prototypes relevant to that class: taking inspiration from (Wen et al., 2022) as follows: = 1 Ai (cid:80) (cid:88) At Zi , RN cz (6) where denotes the Hadamard product. This operation is repeated with the teacher branch to obtain t. These representations are used to contrast the prototype representations (or slots) in contrastive loss. The representation from the teacher branch is projected using two-layer MLP to get Qt = q(St) RN Cz . We next describe the overall optimization objective. 3.2. Optimization objective The optimization objective is composed of losses promoting consistent and local assignment of the patches along joint-classification objective. The different losses used in this sense are described next. Assignment loss: To promote consistent and confident assignment of the prototypes, we first aim to encourage the same patches from two different views, with both geometric and color transformations, to be assigned to the same prototype with high confidence through the assignment loss Las Las = 1 (cid:88) log (cid:88) As i,d At i,d (7) yd = (cid:88) rd,n. (5) Alignment loss: Prototypes assignments are also aligned to the backbone through correspondence distillation loss (Hamilton et al., 2022): To improve the consistency of the prototypes representation, we further compute prototype representation per view, Lintra cd = (cid:16) ˆF (cid:17) ˆAt (8) 4 Tell me why: Visual foundation models as self-explainable classifiers where ˆF denotes the intra-sample cosine correlation between the extracted feature and similarly for ˆAt with the prototypes assignment At. Following the method in (Kim et al., 2024), the shift is adapted through the training (see Appendix C). The operation is further repeated by comparing given sample with random sample from the same batch to create Linter . The final alignment loss Lal is: cd Lal = Lintra cd + 1 (cid:88) m"
        },
        {
            "title": "Linter\ncd",
            "content": "(9) where denotes the number of times negative sample is drawn from the batch. We note that the alignment of the prototypes assignment to the backbone resembles the smoothness loss presented in (Lan et al., 2023). However, we found that their loss does not prevent model collapse under the pressure of the Lassign term (see Appendix for more details). Contrastive loss: We next leverage the prototype representations {s,t} and the projection Qt in contrastive loss LN CE as initially presented in (Wen et al., 2022). This objective encourages consistent representation of given prototype across views of given image as well as minimizing the similarity with different prototype representations. The contrastive loss LN CE is defined as: LN CE (cid:0)S s, t(cid:1) = 1 (cid:88) n=1 log 1n,s1n,t exp (qt 1n,s1n,t exp (qt ss n/τ ) ss n/τ ) (cid:80) . (10) where denotes ℓ2-normalization, qn and sn denotes respectively the entry of and for prototype and 1n is binary indicator representing prototypes which are dominant in at least single patch: 1n,s = such that arg max (As) [I] = (11) and similarly for 1n,t based on the teacher assignment At. Sparsity loss: To prune prototypes in the classification layer with low importance, we introduce sparsity loss Lsp based on the Hoyer-Square (HS) regularizer (Yang et al., 2019) applied on the importance matrix R: Lsp = α R2 1 R2 2 + γR2. (12) Classification loss: The classification loss is simple crossentropy loss between the models prediction and the label : LCE = yq log yq (13) (cid:88) This loss is applied both on the student and teacher predictions ys and yt. 5 The final objective is: = λ1Las + λ2Lal + λ3LN CE + λ4Lsp + λ5LCE (14) where λ[1,...,5] are hyper-parameters. 3.3. Benchmark for evaluation of prototypical-part models The set of metrics used in our evaluation aims to provide general evaluation of the explanations provided by the prototypical models. This evaluation can be decomposed into two main steps. We first evaluate all models with the metrics from the FunnyBirds metrics which cover three dimensions of the Co-12 properties, namely correctness, completeness, and contrastivity. The metrics in the framework rely on part importance function I() which aims to reflect the importance of the different parts in the image towards the models prediction. We follow similar approach as (Opłatek et al., 2024) described in more detail in Appendix to adapt the function to prototypical models. This set of metrics allows to derive mean explainability score mX which allows us to compare the different models to different interpretability methods which are not necessarily based on prototypical models. In second part, we focus on metrics specific to prototypical model. The consistency and stability score initially developed on the CUB dataset by (Huang et al., 2023) were adapted to the FunnyBirds dataset to leverage the precise part annotations provided as part of this dataset. The consistency metrics evaluate whether the prototypes are consistently attributed to one of the five parts defined in the Funnybirds dataset, that is beak, eye, foot, tail and wing. The stability metrics focus on measuring whether the part attribution of prototype is stable when the input is perturbed with noise. The idea is to evaluate whether prototype assignments change under perturbations which are invisible to human eyes. More details on these metrics are included in Appendix To assess the compactness of the evaluation, we utilize the local and global size metrics from (Nauta et al., 2023a). The local size quantifies the total number of prototypes model uses for prediction, while the global size represents the number of prototypes with non-zero weight in the classification head. The composition property is often overlooked, with score sheets failing to indicate that the displayed prototypes contribute often for less than 50% of the final prediction. To address this, we introduce the Score Explained by Composition (SEC) metric. We propose incorporating this metric into score sheets produced by prototypical models, as it measures the fraction of the total prediction explained by the prototypes presented in given score sheet. All the properties included in our benchmarks along the Tell me why: Visual foundation models as self-explainable classifiers corresponding metrics are summarized in Table 1. 4. Experiments 4.1. Implementation details The proposed architecture leverages DINOv2 ViT-B/14 with registers (Oquab et al., 2024; Darcet et al., 2023) as well as the ViT-L from the OpenClip architecture (Cherti et al., 2022) for the general datasets as our backbone. In addition, an experiment on chest X-Ray dataset with RADDINO (Perez-Garcıa et al., 2025) was also performed to demonstrate the possibility of leveraging domain specific VFMs. Full experimental setups including the number of epochs for the different experiments are described in Appendix B. 4.2. Classification performance Table 2. Performance comparison across SOTA models in terms of classification accuracies (Acc.), global (G.) and local (L.) size. the most recent results are reported (Xue et al., 2022) CUB CARS Acc. G. / L. size Acc. G. / L. size DINOv2-B ProtoPNet Proto Tree ProtoPShare ProtoPool PIP-Net ViT-NeT PixPNet ST-ProtoPNet ProtoViT ProtoFM w/DINO ProtoFM w/CLIP 89.6 79.2 82.2 74.7 85.5 84.3 84.5 81.8 86.1 85. 86.3 77.4 88.2 86.1 86.6 86.4 88.9 88.2 92.6 92.7 92.6 92.4 93.6 2000/10 202/ 400/ 495/ 495/ 2000/10 8000/40 2000/10 74/6 57/6 2000/10 195/ 480/ 195/ 515/4 8000/40 2000/10 76/7 6/46 We compare the proposed approach in term of classification performance to non-explainable baseline and SOTA prototypical models. For the non-explainable baseline, we present results for one of the frozen backbone, i.e. DINOv2 ViTB/14, with linear classifier reporting results from the initial model publication (Oquab et al., 2024). In addition, we consider range of SOTA prototypical models, namely ProtoPNet (Chen et al., 2019), ProtoTree (Nauta et al., 2021b), ProtoPShare (Rymarczyk et al., 2021), ProtoPool (Rymarczyk et al., 2022), PIP-Net (Nauta et al., 2023a), ViT-Net (Kim et al., 2022a), ST-ProtoPNet (Wang et al., 2023), PixPNet (Carmichael et al., 2024b), ProtoViT (Ma et al., 2024). Three datasets for image classification tasks were used to benchmark the model on general classification tasks. Two are common benchmarks: i) CUB-200-2011 (Wah et al., Figure 2. Score sheet for predictions on three random samples of the CUB dataset. Each row shows prediction on different sample. The first column indicates the position of the top four prototypes. Each subsequent column shows prototype along with its importance towards the predicted class. The total score for the predicted class and the SEC metric are presented above the first column. 2011) (200 bird species), ii) Stanford Cars (Krause et al., 2013) (196 car models). third general dataset, OxfordIIIT Pets (37 cat and dog species) (Parkhi et al., 2012), was added as it has been used to evaluate PIP-Net as well DINOv2. The proposed architecture was further evaluated on the RSNA pneumonia detection (Presence/absence of pneumonia on chest radiographs) (Shih et al., 2019) as clinical test case. The classification accuracy of the proposed architectures along the baselines described above is shown for CUB and CARS datasets in Table 2. Examples of score-sheet prediction on CUB are shown in Figure 2, while set of prototypes learned along their most similar patches are shown in Figure 3. More results are provided in Appendix and the additional materials ( Appendix A). Results on PETS are shown in Appendix F. For baseline models, we report all results available in the literature, that is either in the paper presenting the model or in further work. Regarding the classification accuracy of the proposed architecture, we observe that the model achieves state-of-theart performance on the two benchmarks, that is CUB and CARS, compared to the other prototypical models presented in the literature. Interestingly, ProtoFM w/ DINO outperforms the performance of DINOv2 with linear probing on the CARS dataset. We also note that this performance is achieved by retaining the backbone frozen and training the rest of the architecture, which is limited to only 1.3M paTell me why: Visual foundation models as self-explainable classifiers Figure 3. Nearest patches to four prototypes; two for the CARS dataset (orange and blue boxes) and two for CUB (yellow and green boxes). The predicted class along the max similarity between the prototype of interest and the patches are indicated above each image. rameters on the two datasets presented in Table 2. Previous approaches have often leveraged pre-trained CNN backbones or even more recently ViT (Ma et al., 2024), but prototypical models found in the literature require training both the backbone and the prototypical head. By integrating our architecture with RAD-DINO, we achieved an AUROC of 86.1 on the RSNA dataset, when RAD-DINO reached 88.4 using linear probing. While there is slight performance gap, this result highlights the potential of our architecture to transfer prototypical models to more specialized datasets. This is especially significant as more VFMs are being developed in domains where interpretability is crucial, such as the clinical field. 4.3. Interpretability evaluation We performed an extensive evaluation of the explanations provided by our model on the metrics described in Section 3.3. The model is compared with PIP-Net, STProtoPNet and ProtoVit. These models were selected because ST-ProtoPnet and ProtoVit achieved the highest accuracy among prototypical models in the literature, while PIP-Net offered the most compact explanations in terms of both local and global size. The metrics presented in this section aim to cover the desiderata presented in Table 1. Metrics which cover the first five desiderata are summarized in the radar plot shown in Figure 4. The local and global sizes, which measure the compactness of the explanations, are reported in Table 2 and all metrics are reported individually in Appendix E. The evaluation performed as part of our work demonstrates Figure 4. Radar plot summarizing model performance both in terms of Accuracy (Acc.) as well as explainability quality with the following metrics Global Size (Glob. Size), and Local Size (Loc. Size), Completeness (Compl.), Correctness (Correct.), and Contrastivity (Contrast.), Consistency (Consist.), and Stability (Stabil.). the multi-faceted nature of interpretability. As recognized in the literature, explanations produced by different models can be quite deceptive, and it is, therefore, important to perform strong quantitative evaluation to ensure the explanations reflect at best the models behavior. Our analysis highlighted different failure modes for two recently published models. Regarding PIP-Net, our analysis revealed that its prototypes often fail to highlight the discriminative features the model used to classify an image. Notably, PIP-Net scored zero Tell me why: Visual foundation models as self-explainable classifiers on the target sensitivity metric, introduced by (Hesse et al., 2023). This metric quantifies the models reliance on known class-discriminative features in synthetic dataset. This finding underscores the limitations of interpretability assessments based solely on human analysis. Although PIP-Net produces highly consistent prototypes, as evidenced by both the qualitative evaluation and the consistency metric in Figure 4, our analysis also identifies specific failure modes. Regarding ProtoVit, our analysis indicates that the model performs well across range of benchmark metrics. However, we observed that the prototypes exhibit considerable inconsistency in the input space, with individual prototypes often being assigned to different bird parts. This inconsistency significantly lowers the models performance on the consistency metric. Additionally, the architecture lacks sparsity constraint, resulting in high local size. For instance, on the FunnyBirds dataset, an average of 504 prototypes have non-zero importance. Low sparsity is common issue in prototypical models. Most score sheets include only three to four prototypes, yet the cumulative importance of these prototypes often represents only small fraction of the final prediction. To partially address this issue, we introduce the Score Explained by Composition (SEC) metric, which we advocate for inclusion in all prototypical model score sheets. The SEC metric quantifies the extent to which the final prediction is explained by the prototypes presented in the score sheet, thereby improving transparency. The proposed architecture, ProtoFM, achieves mean explainability score (mX) of 0.92 based on the metrics defined in the FunnyBirds framework. This score is not limited to prototypical models, enabling comparisons across various interpretability approaches, including post-hoc methods. Our architecture outperforms all prototypical models evaluated in this paper, as well all 24 combinations of models and interpretability methods evaluated by (Hesse et al., 2023). Regarding the metrics specific to prototypical model, our architecture scored 0.99 on the stabilit metrics but attained consistency score of 0.65, ranking higher than ST-ProtoPNet and ProtoViT but lower than PIP-Net which we observed produced very consistent prototypes. user-study on CUB was performed to better understand the consistency of the prototypes generated by ProtoFM and all results are discussed in Appendix F.1. One design choice of our architecture is to not push prototypes to specific patch of the training set. This choice is supported by the neuroscience literature which propose two models for concept representation: (i) the exemplar model, where concepts are represented by multiple exemplars, and (ii) the prototype model, where concepts are abstracted from specific exemplars (Zeithamova et al., 2019). Forcing prototypes to match specific patches fails to align with either the exemplar or prototype model of concept representation in Table 3. Ablation results on the Funny Birds dataset. Term from the loss objective are individually removed. Acc. stands for Accuracy, Conc. for consistency, mX for the mean interpretability score and Loc. size for the local size Las Lal LN CE Lsp Acc Conc. mX 96.2 96.0 95.2 94.6 95.8 0.41 0.37 0.61 0.58 0.65 0.90 0.90 0.92 0.92 0.92 Loc. size 9 8 7 9 6 human cognition. In this work, we take the first approach and represent concepts through exemplars as shown in Figure 3. Instead of pushing the prototypes to specific patch of the training set, we enforce all patches to be strongly assigned to prototype through the alignment loss Lal. We further discuss the effects of the different losses in the next section. 4.4. Ablation studies An ablation study to understand the effects of the different terms in the objective function was performed and the results are presented in Table 3. This study was conducted on FunnyBirds by individually removing the different terms from the loss function and evaluating the model both in terms of classification performance and interpretability metrics. We find that the assignment loss Las and the alignment loss Lal play crucial role in enhancing prototype consistency. The significance of Lal , which leverages the similarity of features extracted by the backbone, further supports the decision to keep the backbone frozennot only for efficiency but also for its contribution to consistency. Additionally, we observe that the contrastive loss LN CE positively influences prototype consistency, while the sparsity loss Lsp contributes to reducing the local size of the explanation and hence promotes better interpretability of the model for given accuracy. 5. Conclusion This work aimed to demonstrate that the proposed architecture ProtoFM effectively adapts visual foundation models into self-explainable classifiers. Through extensive evaluation, we showed that our approach not only produces models with competitive classification accuracy but also surpasses other prototypical models in the quality of explanations provided. While we believe this interpretability framework enhances understanding of the models decision-making process, natural next step would be incorporating textual descriptions to further clarify the concepts utilized by the model. Tell me why: Visual foundation models as self-explainable classifiers"
        },
        {
            "title": "Impact Statement",
            "content": "This paper presents work whose goal is to advance the field of Machine Learning (ML). There are many potential societal consequences of our work; however, we believe that interpretability, which is the focus of this work, could help to partially mitigate some of the risks of using ML-based models in critical applications."
        },
        {
            "title": "Acknowledgments",
            "content": "HT, MB and CL acknowledge financial support from the Fondation Carlos et Elsie De Reuter and Fondation Ceres. MB acknowledges support from Nicolas Pictet. GM acknowledges support from MOE Tier 1 grant no. 22-4900A0001-0: Discipline-Informed Neural Networks for Interpretable Time-Series Discovery. The computations were performed at University of Geneva using Baobab HPC service."
        },
        {
            "title": "References",
            "content": "Adebayo, J., Gilmer, J., Muelly, M., Goodfellow, I., Hardt, M., and Kim, B. Sanity checks for saliency maps. In Bengio, S., Wallach, H., Larochelle, H., Grauman, K., Cesa-Bianchi, N., and Garnett, R. (eds.), Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018. Carmichael, Z., Lohit, S., Cherian, A., Jones, M. J., and Scheirer, W. J. Pixel-grounded prototypical part networks. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), pp. 47684779, January 2024a. Carmichael, Z., Lohit, S., Cherian, A., Jones, M. J., and Scheirer, W. J. Pixel-grounded prototypical part networks. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 47684779, 2024b. Chen, C., Li, O., Tao, D., Barnett, A., Rudin, C., and Su, J. K. This looks like that: Deep learning for interpretable image recognition. In Wallach, H., Larochelle, H., Beygelzimer, A., d'Alche-Buc, F., Fox, E., and Garnett, R. (eds.), Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. Cherti, M., Beaumont, R., Wightman, R., Wortsman, M., Ilharco, G., Gordon, C., Schuhmann, C., Schmidt, L., and Jitsev, J. Reproducible scaling laws for contrastive language-image learning. arXiv preprint arXiv:2212.07143, 2022. Darcet, T., Oquab, M., Mairal, J., and Bojanowski, P. arXiv preprint Vision transformers need registers. arXiv:2309.16588, 2023. Donnelly, J., Barnett, A. J., and Chen, C. Deformable protopnet: An interpretable image classifier using deformable prototypes. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1026510275, 2022. Gautam, S., Boubekki, A., Hansen, S., Salahuddin, S., Jenssen, R., Hohne, M., and Kampffmeyer, M. Protovae: trustworthy self-explainable prototypical variational model. Advances in Neural Information Processing Systems, 35:1794017952, 2022. Guo, X., Lao, J., Dang, B., Zhang, Y., Yu, L., Ru, L., Zhong, L., Huang, Z., Wu, K., Hu, D., et al. Skysense: multi-modal remote sensing foundation model towards universal interpretation for earth observation imagery. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2767227683, 2024. Hamilton, M., Zhang, Z., Hariharan, B., Snavely, N., and Freeman, W. T. Unsupervised semantic segmentation by distilling feature correspondences. In International Conference on Learning Representations, 2022. He, K., Gkioxari, G., Dollar, P., and Girshick, R. Mask rcnn. In Proceedings of the IEEE international conference on computer vision, pp. 29612969, 2017. Hesse, R., Schaub-Meyer, S., and Roth, S. Funnybirds: synthetic vision dataset for part-based analysis of explainable ai methods. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 3981 3991, 2023. Hoffmann, A., Fanconi, C., Rade, R., and Kohler, J. This looks like that... does it? shortcomings of latent space prototype interpretability in deep networks. arXiv preprint arXiv:2105.02968, 2021. Hooker, S., Erhan, D., Kindermans, P.-J., and Kim, B. benchmark for interpretability methods in deep neural networks. In Wallach, H., Larochelle, H., Beygelzimer, A., d'Alche-Buc, F., Fox, E., and Garnett, R. (eds.), Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019a. Hooker, S., Erhan, D., Kindermans, P.-J., and Kim, B. benchmark for interpretability methods in deep neural networks. Advances in neural information processing systems, 32, 2019b. Huang, Q., Xue, M., Huang, W., Zhang, H., Song, J., Jing, Y., and Song, M. Evaluation and improvement of interpretability for self-explainable part-prototype networks. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 20112020, 2023. Tell me why: Visual foundation models as self-explainable classifiers Kim, C., Han, W., Ju, D., and Hwang, S. J. Eagle: Eigen aggregation learning for object-centric unsupervised semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 35233533, 2024. Kim, S., Nam, J., and Ko, B. C. ViT-NeT: Interpretable vision transformers with neural tree decoder. In Chaudhuri, K., Jegelka, S., Song, L., Szepesvari, C., Niu, G., and Sabato, S. (eds.), Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pp. 11162 11172. PMLR, 1723 Jul 2022a. Kim, S. S., Meister, N., Ramaswamy, V. V., Fong, R., and Russakovsky, O. Hive: Evaluating the human interpretability of visual explanations. In 17th European Conference on Computer Vision, ECCV 2022, pp. 280 298. Springer Science and Business Media Deutschland GmbH, 2022b. Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., Xiao, T., Whitehead, S., Berg, A. C., Lo, W.-Y., et al. Segment anything. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 40154026, 2023. Krause, J., Stark, M., Deng, J., and Fei-Fei, L. 3d object representations for fine-grained categorization. In Proceedings of the IEEE international conference on computer vision workshops, pp. 554561, 2013. Lan, M., Wang, X., Ke, Y., Xu, J., Feng, L., and Zhang, W. Smooseg: Smoothness prior for unsupervised semantic segmentation. In Oh, A., Naumann, T., Globerson, A., Saenko, K., Hardt, M., and Levine, S. (eds.), Advances in Neural Information Processing Systems, volume 36, pp. 1135311373. Curran Associates, Inc., 2023. Lipton, Z. C. The mythos of model interpretability: In machine learning, the concept of interpretability is both important and slippery. Queue, 16(3):3157, 2018. Lundberg, S. M. and Lee, S.-I. unified approach to interpreting model predictions. In Guyon, I., Luxburg, U. V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., and Garnett, R. (eds.), Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. Madsen, A., Lakkaraju, H., Reddy, S., and Chandar, S. Interpretability needs new paradigm. arXiv preprint arXiv:2405.05386, 2024. Mengaldo, G. Explainable ai and the scientific method: arXiv Interpretability-guided knowledge discovery. preprint arXiv:2406.10557, 2025. Metikoˇs, L. and Ausloos, J. The right to an explanation in practice: Insights from case law for the gdpr and the ai act. Forthcoming in Law, Innovation, and Technology, 17, 2024. Nauta, M., Jutte, A., Provoost, J., and Seifert, C. This looks like that, because... explaining prototypes for interpretable image recognition. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pp. 441456. Springer, 2021a. Nauta, M., Van Bree, R., and Seifert, C. Neural prototype trees for interpretable fine-grained image recognition. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1493314943, 2021b. Nauta, M., Schlotterer, J., Van Keulen, M., and Seifert, C. Pip-net: Patch-based intuitive prototypes for interpretable image classification. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 27442753, 2023a. Nauta, M., Trienes, J., Pathak, S., Nguyen, E., Peters, M., Schmitt, Y., Schlotterer, J., Van Keulen, M., and Seifert, C. From anecdotal evidence to quantitative evaluation methods: systematic review on evaluating explainable ai. ACM Computing Surveys, 55(13s):142, 2023b. Opłatek, S., Rymarczyk, D., and Zielinski, B. Revisiting funnybirds evaluation framework for prototypical parts networks. In World Conference on Explainable Artificial Intelligence, pp. 5768. Springer, 2024. Oquab, M., Darcet, T., Moutakanni, T., Vo, H. V., Szafraniec, M., Khalidov, V., Fernandez, P., HAZIZA, D., Massa, F., El-Nouby, A., et al. Dinov2: Learning robust visual features without supervision. Transactions on Machine Learning Research, 2024. Parkhi, O. M., Vedaldi, A., Zisserman, A., and Jawahar, C. Cats and dogs. In 2012 IEEE conference on computer vision and pattern recognition, pp. 34983505. IEEE, 2012. Ma, C., Donnelly, J., Liu, W., Vosoughi, S., Rudin, C., and Chen, C. Interpretable image classification with adaptive prototype-based vision transformers. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. Perez-Garcıa, F., Sharma, H., Bond-Taylor, S., Bouzid, K., Salvatelli, V., Ilse, M., Bannur, S., Castro, D. C., Schwaighofer, A., Lungren, M. P., et al. Exploring scalable medical image encoders beyond text supervision. Nature Machine Intelligence, pp. 112, 2025. 10 Tell me why: Visual foundation models as self-explainable classifiers Wei, J., Turbe, H., and Mengaldo, G. Revisiting the robustness of post-hoc interpretability methods. arXiv preprint arXiv:2407.19683, 2024. Wen, X., Zhao, B., Zheng, A., Zhang, X., and Qi, X. Selfsupervised visual representation learning with semantic grouping. Advances in neural information processing systems, 35:1642316438, 2022. Xue, M., Huang, Q., Zhang, H., Cheng, L., Song, J., Wu, M., and Song, M. Protopformer: Concentrating on prototypical parts in vision transformers for interpretable image recognition. arXiv preprint arXiv:2208.10431, 2022. Yang, H., Wen, W., and Li, H. Deephoyer: Learning sparser neural network with differentiable scale-invariant sparsity measures. In International Conference on Learning Representations, 2019. Zeithamova, D., Mack, M. L., Braunlich, K., Davis, T., Seger, C. A., Van Kesteren, M. T., and Wutz, A. Brain Journal of Neuromechanisms of concept learning. science, 39(42):82598266, 2019. Ranzinger, M., Heinrich, G., Kautz, J., and Molchanov, P. Am-radio: Agglomerative vision foundation model In Proceedings of the reduce all domains into one. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1249012500, 2024. Reddy, S. Explainability and artificial intelligence in medicine. The Lancet Digital Health, 4(4):e214e215, 2022. Rudin, C. Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead. Nature machine intelligence, 1(5):206 215, 2019. Rymarczyk, D., Struski, Ł., Tabor, J., and Zielinski, B. Protopshare: Prototypical parts sharing for similarity discovery in interpretable image classification. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining, pp. 14201430, 2021. Rymarczyk, D., Struski, Ł., Gorszczak, M., Lewandowska, K., Tabor, J., and Zielinski, B. Interpretable image classification with differentiable prototypes assignment. In European Conference on Computer Vision, pp. 351368. Springer, 2022. Sacha, M., Jura, B., Rymarczyk, D., Struski, Ł., Tabor, J., and Zielinski, B. Interpretability benchmark for evaluating spatial misalignment of prototypical parts explanations. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pp. 2156321573, 2024. Scott, M., Su-In, L., et al. unified approach to interpreting model predictions. Advances in neural information processing systems, 30:47654774, 2017. Shih, G., Wu, C. C., Halabi, S. S., Kohli, M. D., Prevedello, L. M., Cook, T. S., Sharma, A., Amorosa, J. K., Arteaga, V., Galperin-Aizenberg, M., et al. Augmenting the national institutes of health chest radiograph dataset with expert annotations of possible pneumonia. Radiology: Artificial Intelligence, 1(1):e180041, 2019. Turbe, H., Bjelogrlic, M., Lovis, C., and Mengaldo, G. Evaluation of post-hoc interpretability methods in timeseries classification. Nature Machine Intelligence, 5(3): 250260, 2023. Wah, C., Branson, S., Welinder, P., Perona, P., and Belongie, S. The caltech-ucsd birds-200-2011 dataset. California Institute of Technology, 2011. Wang, C., Liu, Y., Chen, Y., Liu, F., Tian, Y., McCarthy, D., Frazer, H., and Carneiro, G. Learning support and trivial prototypes for interpretable image classification. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pp. 20622072, October 2023. 11 Tell me why: Visual foundation models as self-explainable classifiers A. Additional materials Additional materials are available on Zenodo. B. Experimental Setup B.1. General dataset The proposed architecture was implemented in PyTorch with all experiment trained on on single NVIDIA GeForce RTX 3090, 12 cores and 64 GB of memory. Models were trained for 120 epochs for PETS, CARS and CUB. For FunnyBirds, the model was trained for 50 epochs. All models were trained with an AdamW optimiser and the learning linearly increased during the first five epochs. After the warm-up, the learning rate was progressively decreased following cosine-decay schedule. Models hyper-parameters are listed in Table 4. Table 4. Parameter Settings for ProtoFM on the general dataset"
        },
        {
            "title": "Parameter",
            "content": "Batch Size Base Learning Rate Weight Decay Number of Prototype Image Size teacher momentum lr multiplier classification head λ1 λ2 λ3 λ4 λ5 α γ"
        },
        {
            "title": "Value",
            "content": "128 0.01 0.01 300 224 0.995 10 2 5 1 0.1 2 0.1 0.1 B.2. RSNA dataset We pre-process the RSNA dataset following the instructions provided by (Perez-Garcıa et al., 2025). Images are initially resized using B-spline interpolation such that the shorter side is equal to 518. Min-max scaling is used to rescale the pixels in the range [0,255] and images are then saved in PNG. For training the image processor attached to RAD-DINO on hugging face is used. All hyper-parameters are kept the same, except the image size set to 518, λ5 equals to 5 and the model being trained for 25 epochs. C. Alignment loss For the alignment loss, we followed the method by (Kim et al., 2024), which updates the shift bintra and binter for comparison with respective intra-sample features correlation and inter-sample correlation through the training as follows: 1 (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:32) bintra = binter = (cid:88) i=1 ˆFI 1 (cid:88) i=1 ˆAI kshift (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:33) 1 (cid:88) i=1 ˇFI + 1 (cid:88) i=1 ˆAI kshift vshift, (15) (16) kshift and vshift are set to 0.1 and 3. This loss was used rather than the one from SmooSeg (Lan et al., 2023) which collapses in our case given it can be equal to zero if all patches are assigned to the same prototype. Indeed it mutiplies the correlation matrix from the backbone with 12 Tell me why: Visual foundation models as self-explainable classifiers one minus the correlation of the prototypes assignment. Therefore in the case where all patches are assigned to the same prototype, the loss is equal to zero. D. Additional details on interpretability metrics We first adapt the part importance function which aims to determine the importance of each part and is used to compute the metrics from the FunnyBirds framework. We align the redistribution of explanation on the input space such that our explanation follows the additive properties (Scott et al., 2017). Given the prototypes assignment matrix A, weight from the classification head , and the vector which represent the activation of the prototype, for given class at location is equal to: I[i, d] = (cid:88) Ai,n Wd,n hn 1 Ai,n (cid:80) (17) Regarding the stability and consistency metrics, they are both based for each image on the vector on. This vector is binary vector indicating whether prototype pn is related to category . There are five different parts categories for the FunnyBirds dataset: beak, eye, foot, tail and wing. For each category, we set the entry of the vector op to one if an entry of the attribution matrix An weighted by the importance of the corresponding prototype rd,n is larger than 0.1 within the binary segmentation mask corresponding to the given category Nu, where Nu is equal to one if the category is present at this location and zero otherwise: ou pn = max {rd,n (An Nu)} > 0.1 (18) The consistency and stability scores are then evaluated using the same equation as (Huang et al., 2023) with our modified vector op. However as the initial paper considers prototypical models where prototypes only belong to one class, we repeat the operations across all classes. Only the prototype that appears in the prediction for the considered class are included and the result is averaged across all classes. E. Interpretability metrics We present all metrics used to evaluate the models interpretability along the metrics they are related to in Table 5. Table 5. Interpretability metrics for ProtoFM and three SOTA models. stand for accuracy, with the next 6 columns referring to the metrics from (Hesse et al., 2023), mX stand for the mean explainability score, while Con stands for consistency and Stab for stability as defined in the method of this paper. L.Size refers to the local size. PIP-Net ST-ProtoPNet ProtoViT ProtoFM 0.99 1.00 0.94 0. CSDC PC 0.45 0.78 0.97 0.95 0.22 0.69 0.93 0.99 DC 0.21 0.67 0.96 0. 0.93 0.69 0.40 0.92 SD 0.58 0.52 0.75 0.72 TS 0.00 0.64 0.92 0. BI 1.00 1.00 1.00 0.99 Com Cor Con mX 0.61 0.70 0.68 0.94 0.58 0.52 0.75 0. 0.00 0.64 0.92 0.99 0.64 0.77 0.86 0.92 Con 1.00 0.55 0.00 0.65 Stab 1.00 0.99 0.98 0. L. Size 2 20 504 6 F. Additional results The proposed architecture was also evaluated on PETS to compare its performance to the non-explainable baseline, DINO-B/14 as well as PIP-NET. Results are presented in Table 6. Table 6. Classification accuracy along local and global size on PETS dataset Accuracy [%] Local Global DINO-B/14 PIP-Net ProtoFM (ours) 96.2 92 95.1 13 2 4 172 Tell me why: Visual foundation models as self-explainable classifiers (a) Prototype 26: could be associated to breast part (100% consistent). (b) Prototype 108: could be associated to human (82% consistent). Figure 5. Examples of random selection for each prototype of 50 samples where this prototype plays role toward the models prediction. F.1. Qualitative evaluation of prototypes consistency In addition to the quantitative metrics used to assess the quality of the explanations provided by the designed architecture, one user study was carried out to better understand the consistency of the prototypes with respect to concepts humans would associate together. The user study relies on random selection for each prototype of 50 samples where this prototype plays role toward the models prediction. The samples used for the user study can be found in Supplementary Materials Appendix A. percentage of consistent sample (the most prevalent category) was calculated for each prototype (global size is 77). On average, the prototypes are 86% consistent with one category (see analysis per prototype in Supplementary Materials (see Appendix A), with 17 prototypes being 100% consistent. Two examples are displayed in Figure 5. The model provides 14 Tell me why: Visual foundation models as self-explainable classifiers consistent explanations for fine-grained bird parts (such as breast or beak) but is also able to produce more coarse-grained prototypes such as human. One prototype (# 81) identified potential unwanted bias in the dataset, which are the textual parts present in the image (such as the signature). This study focuses on qualitative assessment only of the consistency of the prototypes, without considering the effects of displaying the explanation in the context of justifying prediction for specific class. As result, protocols like HIVE (Kim et al., 2022b) are not applicable. Assessing the practical impact of the proposed explanation is beyond the scope of this paper. Evaluating its real-world implications, such as in clinical applications, would necessitate complex user study incorporating protocols like HIVE alongside additional metrics that account for various factors influenced by actual clinical workflows. G. Additional visualizations We present five score sheets per dataset in this section. Additional visualizations are available in the Additional materials. 15 Tell me why: Visual foundation models as self-explainable classifiers Figure 6. Score sheet for predictions on five random samples of the CARS dataset. Each row shows prediction on different sample. The first column indicates the position of the top four prototypes. Each subsequent column shows prototype along with its importance towards the predicted class. The total score for the predicted class and the SEC metric are presented above the first column. 16 Tell me why: Visual foundation models as self-explainable classifiers Figure 7. Score sheet for predictions on five random samples of the CUB dataset. Each row shows prediction on different sample. The first column indicates the position of the top four prototypes. Each subsequent column shows prototype along with its importance towards the predicted class. The total score for the predicted class and the SEC metric are presented above the first column. 17 Tell me why: Visual foundation models as self-explainable classifiers Figure 8. Score sheet for predictions on five random samples of the PETS dataset. Each row shows prediction on different sample. The first column indicates the position of the top four prototypes. Each subsequent column shows prototype along with its importance towards the predicted class. The total score for the predicted class and the SEC metric are presented above the first column. 18 Tell me why: Visual foundation models as self-explainable classifiers Figure 9. Score sheet for predictions on five random samples of the FunnyBirds dataset. Each row shows prediction on different sample. The first column indicates the position of the top four prototypes. Each subsequent column shows prototype along with its importance towards the predicted class. The total score for the predicted class and the SEC metric are presented above the first column. 19 Tell me why: Visual foundation models as self-explainable classifiers Figure 10. Score sheet for predictions on five random samples of the RSNA dataset. Each row shows prediction on different sample. The first column indicates the position of the top four prototypes. Each subsequent column shows prototype along with its importance towards the predicted class. The total score for the predicted class and the SEC metric are presented above the first column."
        }
    ],
    "affiliations": [
        "Department of Mechanical Engineering, College of Design and Engineering, National University of Singapore, Singapore",
        "Department of Radiology and Medical Informatics, University of Geneva, Geneva, Switzerland",
        "Division of Medical Information Sciences, Geneva University Hospitals, Geneva, Switzerland"
    ]
}