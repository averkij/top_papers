{
    "paper_title": "TokenFormer: Rethinking Transformer Scaling with Tokenized Model Parameters",
    "authors": [
        "Haiyang Wang",
        "Yue Fan",
        "Muhammad Ferjad Naeem",
        "Yongqin Xian",
        "Jan Eric Lenssen",
        "Liwei Wang",
        "Federico Tombari",
        "Bernt Schiele"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Transformers have become the predominant architecture in foundation models due to their excellent performance across various domains. However, the substantial cost of scaling these models remains a significant concern. This problem arises primarily from their dependence on a fixed number of parameters within linear projections. When architectural modifications (e.g., channel dimensions) are introduced, the entire model typically requires retraining from scratch. As model sizes continue growing, this strategy results in increasingly high computational costs and becomes unsustainable. To overcome this problem, we introduce TokenFormer, a natively scalable architecture that leverages the attention mechanism not only for computations among input tokens but also for interactions between tokens and model parameters, thereby enhancing architectural flexibility. By treating model parameters as tokens, we replace all the linear projections in Transformers with our token-parameter attention layer, where input tokens act as queries and model parameters as keys and values. This reformulation allows for progressive and efficient scaling without necessitating retraining from scratch. Our model scales from 124M to 1.4B parameters by incrementally adding new key-value parameter pairs, achieving performance comparable to Transformers trained from scratch while greatly reducing training costs. Code and models are available at \\url{https://github.com/Haiyang-W/TokenFormer}."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 0 3 ] . [ 1 8 6 1 3 2 . 0 1 4 2 : r TOKENFORMER: RETHINKING TRANSFORMER SCALING WITH TOKENIZED MODEL PARAMETERS Haiyang Wang1,3, Yue Fan1, Muhammad Ferjad Naeem2, Yongqin Xian2, Jan Eric Lenssen1, Liwei Wang3, Federico Tombari2, Bernt Schiele1 1Max Planck Institute for Informatics 2Google 3Peking University {haiwang, schiele}@mpi-inf.mpg.de"
        },
        {
            "title": "ABSTRACT",
            "content": "Transformers have become the predominant architecture in foundation models due to their excellent performance across various domains. However, the substantial cost of scaling these models remains significant concern. This problem arises primarily from their dependence on fixed number of parameters within linear projections. When architectural modifications (e.g., channel dimensions) are introduced, the entire model typically requires retraining from scratch. As model sizes continue growing, this strategy results in increasingly high computational costs and becomes unsustainable. To overcome this problem, we introduce Tokenformer, natively scalable architecture that leverages the attention mechanism not only for computations among input tokens but also for interactions between tokens and model parameters, thereby enhancing architectural flexibility. By treating model parameters as tokens, we replace all the linear projections in Transformers with our token-parameter attention layer, where input tokens act as queries and model parameters as keys and values. This reformulation allows for progressive and efficient scaling without necessitating retraining from scratch. Our model scales from 124M to 1.4B parameters by incrementally adding new key-value parameter pairs, achieving performance comparable to Transformers trained from scratch while greatly reducing training costs. Code and models are available at https://github.com/Haiyang-W/TokenFormer."
        },
        {
            "title": "INTRODUCTION",
            "content": "Designing powerful neural network architecture is long-standing goal in machine learning. Recent developments in foundation models (FMs) have shown the potential of Transformers (Vaswani et al., 2017) as universal computational architecture. Thanks to their flexibility and scalability, Transformers have achieved state-of-the-art performance across various domains, including natural language processing (NLP) (Radford et al., 2018; Alec et al., 2019; Brown et al., 2020), visual modeling (Dosovitskiy et al., 2021; Liu et al., 2021), vision-language (Liu et al., 2023; Wang et al., 2024), graph representation (Ying et al., 2021), and 3D vision (Wang et al., 2023a;b). Transformers typically divide the computation required to process single token into two distinct parts: interactions with other input tokens (token-token interaction) and computations involving the models parameters (token-parameter interaction). The attention mechanism (Vaswani et al., 2017) facilitates token-token interactions, allowing modern general-purpose foundation models to encode multi-modal data into unified token sequence and effectively capture complex dependencies among them (Liu et al., 2023; Zhu et al., 2023; Wang et al., 2023d). Conversely, token-parameter computations rely heavily on linear projections (Dunford & Schwartz, 1988), where input tokens are multiplied by fixed set of parameters. This prescribed design limits scalability because increasing the model size requires altering core architectural components, often necessitating retraining the entire model from scratch. As models grow larger, this results in excessive resource consumption, making it increasingly impractical. In this paper, we introduce novel architecture that enhances the flexibility of token-parameter interactions, allowing for incremental scaling of model parameters and effectively reusing previously trained models, thus significantly reducing the training burden. 1 Figure 1: Traditionally, large transformer architectures are trained from scratch without reusing previous smaller-scale models (represented by blue dots on the left). In this paper, we propose novel fully attention-based architecture that allows scaling model incrementally, thus greatly reducing the overall cost of training large transformer architectures (depicted by red dots on the left). The right panel delineates comparison between conventional Transformer and our Tokenformer. To achieve this objective, we introduce Tokenformer, novel architecture that unifies the computations of token-token and token-parameter interactions by entirely employing the attention mechanism. The flexibility of our token-parameter attention layer, along with its ability to handle variable number of parameters, inherently enhances the models scalability, facilitating progressively efficient scaling. As shown in Figure 1, we extend the Transformer architecture by preserving the computational patterns between input tokens while reformulating all the linear projections using cross-attention mechanism. Specifically, to project features with input and output dimensions D1 and D2, we employ two sets of parameters, each comprising learnable tokens with channel dimensions of D1 and D2, respectively. In this formulation, input tokens serve as queries, and model parameters as keys and values. This flexibility renders our models parameters inherently scalable with variable , allowing for efficient expansion by continuously adding new key-value parameter pairs. Figure 1 shows that our model can be scaled incrementally from 124M to 1.4B parameters, achieving performance similar to training from scratch while saving more than half of the training cost. The key contributions of this work are summarized as 1) As shown in Figure 1, we propose Tokenformer, fully attention-driven neural network that treats model parameters as tokens, maximizing the flexibility of token-parameter computations while achieving competitive performance on standard benchmarks across both language and vision domains. 2) Thanks to this design, our model can be naturally scaled by progressively adding new key-value parameter pairs. Compared with the train-from-scratch approach (Biderman et al., 2023; Kaplan et al., 2020), our method achieves nearly the same performance while greatly reducing training costs."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Transformer (Vaswani et al., 2017) has emerged as foundational architecture in deep learning due to its versatile attention mechanism, enabling it to process any tokenized data and adapt to numerous domains, including language modeling (Radford et al., 2018; Touvron et al., 2023), image processing (Dosovitskiy et al., 2021), multi-modal understanding (Liu et al., 2023; Wang et al., 2024; 2023b; 2022), decision making (Chen et al., 2021b), graph learning (Yun et al., 2019), among others. While the Transformer effectively handles interactions among input tokens with flexibility, this property does not extend to computations involving model parameters, which are conducted via prescribed linear projections. In this work, we seek to restructure token-parameter interactions by developing fully attention-based network that unifies both token-token and token-parameter computations through attention mechanisms, thus further extending the networks flexibility. Large Scale Training has proven to be an effective approach for developing powerful foundation models. As demonstrated by models like the GPT series (Radford et al., 2018; Alec et al., 2019; Brown et al., 2020), simple architectureswhen supported by larger training datasets and increased model sizes (measured in parameters)often outperform more complex algorithms. Scaling up data 2 is generally more cost-effective because it is independent of the models architecture and allows for the continuous integration of new data through fine-tuning existing models (Kaplan et al., 2020). In contrast, increasing the model size often incurs extremely high costs, as it alters architectural details and usually requires retraining the entire dataset from scratch at each scaling step (Biderman et al., 2023). This significantly raises the expenses for building progressively larger models in the industry. Model Reusing. Previous methods for reusing models have typically involved initializing larger models with pre-trained smaller models by duplicating (Chen et al., 2015; 2021a), stacking (Gong et al., 2019), or combining (Wang et al., 2023c) model weights. While these approaches can be effective, they often disturb the pre-established distribution of the smaller model, increasing the risk of losing pre-trained knowledge and slowing convergence. In contrast, our model allows for parameter scaling in natural and seamless manner and preserves the integrity of the existing model."
        },
        {
            "title": "3 METHODOLOGY",
            "content": "In this section, we first revisits the conventional attention mechanism in Section 3.1. Then, Section 3.2 introduces Tokenformer, natively scalable architecture centered around flexible token-parameter attention layer. Finally, incremental model scaling of Tokenformer is detailed in Section 3.3. 3.1 PRELIMINARIES Transformer models (Vaswani et al., 2017) have established themselves as fundamental architectures in deep learning, demonstrating outstanding performance across wide range of tasks. The cornerstone of their success is the self-attention mechanism, which allows the model to dynamically assess the importance of each token, efficiently modeling complex dependencies among them. Given set of input tokens RT with channel dimension d, the self-attention block first derives input-dependent query Q, key K, and value , with three distinct linear projections as = Q, = K, (1) where the Q, Rddk and Rddv are learnable weight matrices. The attention scores are calculated by measuring the similarity between query and key vectors, followed by softmax function to obtain normalized weights. These scores are subsequently used to compute the output of the scaled dot-product attention as, = , Attention(Q, K, ) = softmax[ ] V, (2) is scale factor for alleviating small gradients caused by softmax. Finally, the output is, where = Xatt O, (3) with Xatt being the attention output and Rdvd as the output projection matrix. The above architectural design enables the model to flexibly manage interactions between tokens of varying lengths, thereby allowing modern general models to concurrently process any form and quantity of tokenized multi-modal data. This capability markedly enhances the development of current AI domain and is fundamental to the success of transformer-based systems. 3.2 TOKENFORMER Although transformers excel across various domains, their scalability is limited by high computational overheads resulting from prescribed token-parameter interactions (i.e., linear projections). As result, scaling strategies that adjust architectural components (e.g., channel dimensions) typically require retraining the entire model from the beginning, leading to inefficient use of computational resources. To overcome this challenge, we propose Tokenformer, an architecture entirely based on attention mechanisms. The central innovation of Tokenformer is token-Parameter attention (Pattention) layer, which incorporates set of trainable tokens functioning as model parameters and then employs cross-attention to manage interactions between input tokens and these parameter tokens. In this way, the Pattention layer introduces an additional dimensionthe number of parameter tokenswhich 3 Figure 2: Tokenformer is fully attention-driven architecture featuring new token-Parameter attention (Pattention) layer. The Pattention uses set of learnable tokens to represent model parameters and lets the input tokens attend to them. As the model scales, Tokenformer adds new learnable tokens to expand the existing key-value parameter sets, while keeping the feature dimension constant and leaving the rest of the computation unaffected. operates independently of the input and output channel dimensions. This decoupling enables input data to dynamically interact with variable number of parameters, providing the flexibility required for incremental model scaling by reusing pre-trained models. Consequently, training larger models is greatly accelerated while achieving performance on par with transformers trained from scratch. Pattention Layer. Let the input tokens and output tokens be represented as RT d1 and RT d2 , where is the sequence length, and d1 and d2 are the input and output dimensions, respectively. To implement our Pattention mechanism, we introduce two sets of learnable parameter tokens: KP Rnd1 representing the keys, and VP Rnd2 representing the values. The output from the scaled dot-product Pattention layer is computed as: Pattention(X, KP , VP ) = Θ (cid:0)X (cid:1) VP , (4) where Θ is modified softmax operation for stable optimization of Pattention layer. The output Pattention scores, Rnn, are formulated as, (cid:32) Sij = (cid:33) (cid:112)(cid:80)n Aij τ k=1 Aik2 , i, 1...n, (5) where is the score derived from (X by default, and is non-linearity function, which in our formulation is set to the GeLU function (Hendrycks & Gimpel, 2016). This design improves gradient stability in our architecture and results in better performance compared to the standard softmax operation (see Appendix and Table 4 for details). ), τ is the scale factor, which is set to Our Pattention layer employs cross-attention mechanism to manage interactions between tokens and parameters, thereby fully preserving the adaptability characteristic of attention mechanisms. Similar to how self-attention in Transformer models handles sequences with variable lengths, our Pattention layer is designed to process flexible number of parameters independently of the input and output channel dimensions used in feature projection. This allows network parameters to be expanded seamlessly along the parameter token axis, enabling the effective reuse of pre-trained weights and offering naturally incremental manner for model scaling. Overall Architecture. Figure 2 illustrates the architecture of Tokenformer. Given the input tokens Xin RT d, we follow the design of the pre-norm transformer, the computation for the output of Tokenformer layer is represented as follows: Xinter = Xin + MHA(LN(Xin)), (6) Xout = Xinter + FFN(LN(Xinter)), (7) where LN denotes the layer normalization (Ba, 2016; Zhang & Sennrich, 2019), and MHA and FFN refer to our modified multi-head self-attention and feed-forward layer, respectively. In the multi-head self-attention block, for simplicity, we consider single-head variant and set both dk and dv equal to d. Then we replace all the linear projections with our Pattention layers. Let LN(Xin) be denoted as X, this block is formulated as follows: = Pattention(X, , ), = Pattention(X, (cid:21) ), = Pattention(X, , ), (8) , Xatt = softmax V, (9) (cid:20) Oatt = Pattention (cid:0)Xatt, (10) where Eq. 8 and 10 represent token-parameter attention while Eq. 9 represents token-token attention. The key-value parameter tokens for the QKV projections are (K ) Rnkd, (K , , ) Rnod is used for the output projection layer. ) Rnvd, while (K ) Rnqd, (K , , P , (cid:1) , For consistency and simplicity, the feed-forward block in Tokenformer utilizes single Pattention Layer. Denote LN(Xinter) as Xffn, and the FFN computation is given by: Offn = Pattention (cid:0)Xffn, ffn ) Rnffnd are learnable key-value pairs for FFN block. , ffn (cid:1) , where (K ffn , ffn (11) By designing the architecture in this manner, we represent all fundamental components-including both input data and model parametersas tokens within the computational framework. This token-centric perspective allows the utilization of successful attention mechanisms to unify two primary computations within the transformer, token-token and token-parameter interactions, thereby establishing fully attention-based neural network characterized by exceptional flexibility. Architecture Configurations. Our model meticulously mirrors the hyper-parameter configuration of the standard Transformer architecture. Taking GPT-2 (Radford et al., 2018) as an exemplar, which features 12 Transformer layers and hidden dimension of 768, our model replicates this configuration with identical layer counts and dimensionality. The number of key-value parameter pairs in both the query-key-value and output projections corresponds directly to the hidden dimension. In contrast, the FFN module utilizes four times the number of parameter pairs relative to the hidden size. This architectural alignment facilitates the initialization of our models parameters using pre-trained Transformer, thereby ensuring seamless integration into the Transformer pre-training ecosystem. 3.3 PROGRESSIVE MODEL SCALING Our model demonstrates strong suitability for large-scale model training along the parameter axis, attributable to the versatile design of the Pattention layer, which allows for the incremental development of larger models by reusing parameters from smaller, pre-trained counterparts. To facilitate understanding without compromising generality, we employ single Pattention layer to exemplify the intricacies of model scaling. Consider an existing Tokenformer model equipped Rnd. As shown in with set of pre-trained key-value parameter tokens, denoted as old Figure 2, to scale the model, we augment this set by appending new key-value parameter tokens new , old , new Rmd as scale = (cid:2)K old , new (cid:3) , = (cid:2)V old scale , new (cid:3) , where [old, new] means the concatenation operation along the token dimension and scale R(m+n)d are scaled parameter sets. The forward pass of the scaled model is then defined as = Pattention (cid:0)X, scale (cid:1) . , scale , scale (12) (13) This scaling scheme permits the integration of an arbitrary number of parameters without altering the input or output dimensions. As demonstrated in Figure 3, this approach notably enhances training efficiency for models at greater scales without degrading performance. Importantly, by initializing new with zero, similar to LoRA technique (Hu et al., 2022), our model can perfectly resume the model state from the pre-training phase without losing the well-learned knowledge, facilitating faster convergence and accelerating the overall scaling process. 5 Figure 3: Evaluating model scaling costs through cumulative computational budgets. The Transformer baseline incurs expenses for each individual scaling step performed independently from scratch, whereas Tokenformer aggregates costs across all scaling stages, including training 124M model initially, progressively scaling to 354M, 757M, and 1.4B parameters. Figure 4: Evaluating model scaling costs by measuring the budget required at each scaling stage. The Transformer baselines used are consistent with those depicted in Figure 3, trained with 30B and 300B tokens. Similarly, for Tokenformer, the cost is the budget required for each incremental scaling step from smaller one. All the experiments were conducted on TPU v4 hardware."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "In this section, we present experimental results for the techniques described above. Section 4.1 validates the continual expansion capability of our model. Section 4.2 highlights the models efficacy in handling tasks within both language and vision domains. Section 4.3 offers an in-depth comparison, highlighting our models advantages over standard Transformer models. Finally, Section 4.4 details the ablation experiments conducted to assess the significance of each module in Tokenformer. 4.1 PROGRESSIVE MODEL SCALING Datasets. Our models are trained using the OpenWebText Corpus described in (Gokaslan & Cohen, 2019). This corpus serves as widely recognized open-source approximation of OpenAIs proprietary WebText dataset, which was employed in the development of GPT-2 (Alec et al., 2019). The dataset comprises textual data extracted from 8,013,769 Reddit-shared documents. During training, we randomly sample segments from these documents. Baseline Transformer Training from Scratch. To evaluate the effectiveness of our progressive model scaling strategy, we established baseline by training Transformer model from scratch. Following the training procedures outlined in Karpathy (2022); Kaplan et al. (2020), we employed the AdamW optimizer (Loshchilov & Hutter, 2019) with batch size of 512 sequences, each containing 1024 tokens. For fair comparison with our incremental scaling approach, we configured two training variants based on the total number of training tokens. The first variant underwent 6 105 steps (approximately 300B tokens), consistent with the training steps utilized by Karpathy (2022) to replicate GPT-2 performance. The second variant was limited to 6 104 steps (approximately 30B tokens) to ensure comparability with each stage of our progressive scaling. In all trainings included in our analysis, unless otherwise indicated, learning rate of 6 104 was employed, featuring 2000-step warmup followed by cosine decay to zero. Tokenformer with Progressive Model Scaling. Building upon the above training protocols, we testify the performance of our model scaling with parameter sizes ranging from 124M to 1.4B. Unlike the aforementioned scratch-training approach, each scaling iteration leverages pre-trained smaller Tokenformer to partially initialize the weights of the larger one described in Section 3.3. The scaling procedure begins with training the initial source model from scratch on approximately 300B tokens, mirroring the Transformer baseline. For scaling, we select the pre-trained model closest in parameter count to the target size for weight initialization. For example, to train model with 354M parameters, we employ the 124M model as partial initializer and retrain the entire model using reduced Model #Param Pile ppl Pythia-160M (Biderman et al., 2023) Ours (TokenFormer-150M) 160M 29.64 150M 10.45 Pythia-410M (Biderman et al., 2023) Ours (TokenFormer-450M) 410M 9.95 450M 8.28 Pythia-1B (Biderman et al., 2023) Ours (TokenFormer-900M) 1B 7.82 900M 7.38 GPT-Neo 1.3B (Black et al., 2021) OPT-1.3B (Zhang et al., 2022) Pythia-1.3B (Biderman et al., 2023) GPT-Neo 2.7B (Black et al., 2021) OPT-2.7B (Zhang et al., 2022) Pythia-2.8B (Biderman et al., 2023) Ours (TokenFormer-1.5B) 1.3B 1.3B 1.3B 2.7B 2.7B 2.8B 1.5B - - 7.51 - - - 6.91 LAMBADA LAMBADA HellaSwag acc acc ppl PIQA Arc-E Arc-C WinoGrande Average acc acc acc acc acc 37.25 16.38 10.84 7.69 7.92 5. 7.50 6.64 6.08 5.63 5.12 5.04 5.24 35.4 45.0 51.4 57.3 56.1 64.0 57.2 58.0 61.7 62.2 63.6 64.7 64.7 30.3 35. 40.6 47.5 47.2 55.3 48.9 53.7 52.1 55.8 60.6 59.3 60.0 62.3 64.9 66.9 69.5 70.7 72. 71.1 72.4 71.0 71.1 74.8 74.0 74.8 43.6 47.3 52.1 56.2 57.0 59.9 56.2 56.7 60.5 61.1 60.8 64.1 64.8 23.6 24. 24.6 26.7 27.1 30.6 25.9 29.6 28.5 30.2 31.3 32.9 32.0 51.3 50.4 53.8 54.6 53.5 56. 54.9 59.5 57.2 57.6 61.0 59.7 59.7 40.1 44.7 48.2 52.0 51.9 56.4 52.4 55.0 55.2 56.5 58.7 59.1 59.3 Table 1: (Zero-shot Evaluations.) The best performance for each model size is highlighted in bold. Our comparisons are made with publicly available transformer-based LMs with various tokenizers. Following Pythia (Biderman et al., 2023), our model is trained for up to 300B tokens on pile dataset. Method ViT-B/16 (Dosovitskiy et al., 2021) DeiT-B/16 (Touvron et al., 2021) ViT-B/16 (MAE) (He et al., 2022) Ours (TokenFormer-B/16 ) Ours (TokenFormer-B/16) ViT-L/16 (Dosovitskiy et al., 2021) ViT-L/16 (MAE) (He et al., 2022) Ours (TokenFormer-L/16) Ours (TokenFormer-L/16) Image Size 3842 2242 2242 2242 2242 3842 2242 2242 2242 #Param Top-1 acc 86M 86M 86M 86M 109M 307M 307M 307M 407M 77.9 81.8 82.3 82.1 82.5 76.5 82.6 83.0 83.1 Table 2: (Image Classification.) Comparison of standard vision transformer on ImageNet-1K. The training hyperparameters are completely consistent (batch size, learning rate, etc.) with He et al. (2022). denotes models where the parameter size has been matched to that of the standard ViT. computational budget (e.g., 15B, 30B, or 60B tokens). This iterative process continues for scaling to 757M and then to 1.4B parameters. Notably, to simplify the scaling procedure, both new and existing parameters are trained equivalently with identical training hyperparameters throughout the process. Our training optimizes the autoregressive log-likelihood (i.e., cross-entropy loss) averaged over 1024-token context and the log perplexity evaluated on the test set as the test score. Experimental Analysis. As illustrated in Figure 3, our progressive scaling methodology employing Tokenformer achieves performance comparable to that of Transformer model trained from scratch, while substantially reducing the training budget. Specifically, Starting with 124M parameter model trained on 300B tokens, we progressively scaled to 354M, 757M, and 1.4B parameters, requiring only an additional 30B tokensjust one-tenth of the computational budget compared to the scratch-trained Transformer. This scaling process achieved test perplexity of 11.77 at the 1.4B parameter level. In comparison, Transformer model of the same size trained from scratch achieved similar perplexity of 11.63 but with 3 the training cost. Importantly, our approach reports cumulative training costs, encompassing all scaling stages, unlike the Transformer baseline that only accounts for individual stages. Even with this comparison, our method demonstrates substantially lower computational cost than training Transformer from scratch, thereby validating the effectiveness of our approach. Figure 4 presents the training costs at each scaling stage for both our model and the standard Transformer. When compared to Figure 3, the cost savings are even more significant. Specifically, our model requires only one-tenth of the training costs associated with Transformer baselines. To mitigate the effects of varying training data, we also included the performance curve of Transformer trained from scratch using an equivalent computational budget of 30B tokens. Under the same computational constraints, our progressively scaled model achieves lower perplexity of 11.77 compared to the Transformers 13.34, thereby highlighting the superior efficiency and scalability of our approach. 4.2 BENCHMARKING OF MODEL EXPRESSIVENESS Language Modeling. We assess the efficacy of our proposed architecture through standard autoregressive language modeling tasks, benchmarking against existing transformer-based models. Evaluations are conducted using both pre-training metrics, specifically perplexity, and zero-shot 7 Transformer Ours Parameter Training FLOPs Transformer Ours Operation Embed Attention: QKV Project Attention: Token-Token Attention: Output Project Feedforward De-embed nvocabdmodel 3nlayerd2 model - nlayerd2 8nlayerd2 model model - nvocabdmodel nlayerdtoken(nq + nk + nv) - nlayerdtokenno 2nlayerdtokennff - Total (Non-Embedding) = 12nlayerd2 model = nlayerdtoken(nq + nk + nv + no + 2nff) - 6nlayerd2 modelT 4nlayerdmodelT 2 2nlayerd2 modelT 16nlayerd2 modelT 2nvocabdmodel 2N + 4nlayerdmodelT 2 - 2nlayerdtoken(nq + nk + nv)T 4nlayerdtokenT 2 2nlayerdtokennoT 4nlayerdtokennffT 2nvocabdmodel 2N + 4nlayerdtokenT 2 Table 3: Parameter counts and training compute estimates for Transformer and our Tokenformer. Sub-leading terms such as nonlinearities, biases, and layer normalization are omitted. Figure 5: The relationship between FLOPs and text length for both Transformer and Tokenformer. As shown in Table 3, Transformer exhibits an increase in computational cost for token-token interactions as dmodel scales upwards. Our Tokenformer model, however, offers flexible parameter scaling mechanism that maintains dtoken at constant value. This strategy results in controllable computational costs for token-token interactions and markedly enhances the efficiency of long-text modeling. performance measures. Training is performed on the Pile dataset (Gao et al., 2020), following the training protocol described in Biderman et al. (2023). Detailed training procedures and model sizes (depth and width) are provided in the Appendix F. Table 1 presents the performance of Tokenformer across various widely-recognized zero-shot downstream tasks. Comparisons are drawn against leading open-source transformer models of equivalent scale, notably Pythia (Biderman et al., 2023), which utilizes the same tokenizer, dataset, and training duration (300B tokens) as our models. As shown in this table, our model achieves competitive performance compared to the standard Transformer, demonstrating the potential of our architecture in terms of expressive power as foundation model. Visual Modeling. Table 2 validates the expressiveness of our model in visual tasks. We compare our approach against the standard Vision Transformer (ViT) (Dosovitskiy et al., 2021) trained with supervised learning on the ImageNet-1K dataset (Deng et al., 2009). For fair comparison, we used the MMDetection code base (MMDetection Contributors, 2018) and followed the hyperparameters and training strategy used in He et al. (2022). As shown in the table, our model achieves the same performance as ViT in visual modeling, confirming its expressiveness in visual tasks. 4.3 COMPARISON WITH STANDARD TRANSFORMER Transformer can also achieve model reuse to certain extent. Net2Net (Chen et al., 2015), classical model growth method, proposes technique to expand the width of neural networks by duplicating neurons. In this method, the pre-trained weight matrix of transformer layer in the smaller model Rdldl (dl > ds) to fill the denoted old larger model. This expansion is formulated as follows, Rdsds, is used to create larger weight matrix new new = (cid:20) old new l(12) new l(21) new l(22) (cid:21) , (14) l(22) R(dlds)(dlds) are new l(21) Rds(dlds), and new where new parameters for expansion. The scaling procedures are the same as schemes introduced in Section 4.1. l(12) R(dlds)ds, new 8 Figure 6: Loss curves comparing pre-trained Transformer and Tokenformer as their parameters are scaled during continued training on enwik8. Figure 7: Performance benchmarking on incremental model scaling between Transformer with Net2Net scheme and our Tokenformer. Controllable cost of token-token interaction for long-context modeling. Recent advancements in Chain-of-Thought (CoT) modeling (Wei et al., 2022) have emphasized the critical importance of efficiently processing lengthy textual sequences (Tay et al., 2020) within Large Language Models (LLMs). As delineated in Section 1, the training costs of transformer architectures are primarily divided into two components: interactions involving model parameters and interactions among input sequences. Table 3 demonstrates that the computational complexity of transformer-based models exhibits quadratic dependence on text length, scaling linearly with token-parameter interactions and quadratically with token-token interactions. Consequently, it is imperative to expand model parameters while controlling the computational burden of token-token interaction part. Conventionally, scaling transformer models involves increasing the channel dimension. For fixed text length, this results in higher computational costs, mainly because dominant token-token interactions become more intensive, which hampers the models performance with long texts. Our proposed model takes different approach by decoupling the computation cost of token-token interactions from model scaling. We increase the parameter size without changing the token channel dimension, thereby maintaining the computational cost associated with token-token interactions. As shown in Figure 5, our model exhibits increasingly significant computational advantages over Transformers as the number of parameters grows, especially when processing longer sequences. Scaling without losing the well-learned distribution. Our Tokenformer can maintain the existing output distribution when new key parameters are initialized to zero. This characteristic is beneficial for continuously scaling models to incorporate additional data, as it facilitates an increase in model capacity without disrupting the ongoing training process, thereby promoting rapid convergence. To evaluate Tokenformers scaling efficacy, we compare the loss curves of Net2Net-based transformer scaling against Tokenformer scaling. Both models, initially with 354M parameters, were pre-trained on the OpenWebText dataset. We then introduced the EnWik8 dataset and continued training with one epoch, expanding the models to 757M parameters to accommodate new data. Figure 6 demonstrates Tokenformer not only converges more rapidly but also reaches lower final loss, attributable to its ability to preserve the output distribution during the resumption of training. Performance benchmarking on incremental scaling. In this study, we progressively scale the standard Transformer using Net2Net approach detailed earlier. For fair comparison, we aligned all hyperparameters, including the parameter size, learning rate, dataset, and so on. As shown in Figure 7, our model performs better in scaling compared to the standard Transformer. 4.4 ABLATION STUDY Optimized Softmax Function in Pattention Layer. Within the token-parameter attention layer, we address training instabilities arising from the diminished gradients associated with the traditional softmax function. The conventional softmax operation comprises two primary steps: computing the Nonlinear Function Normalization Top-1 acc Learnable Weight (γ) Learnable Bias (β) Top-1 acc ex GeLU GeLU L1 Norm L1 Norm L2 Norm 79.6 81.7 82.5 - - - 82.6 82.5 82.5 Table 4: Ablation of Softmax part on ImageNet classification with base model. Table 5: Ablation of non-parametric layer normalization on ImageNet classification with base model. exponential of attention scores followed by L1 normalization. As shown in Table 4, to mitigate the issue of small gradients, we substitute the exponential non-linearity with the GeLU function (Hendrycks & Gimpel, 2016), resulting in performance enhancement of +2.1 points on the ImageNet classification benchmark. Subsequently, we replace the L1 normalization with an L2 normalization, yielding an additional improvement of +0.8 points. These modifications collectively allow our model to achieve performance parity with the standard Vision Transformer. Non-Parametric Layer Normalization. In pursuit of enabling model expansion and the merging of two separately trained parameter token sets for subsequent studies, we modified the Transformers layer normalization to non-parametric variant by removing its trainable weights and biases. This adjustment guarantees that only the key-value parameters are subject to learning within the model. Empirical results presented in Table 5 demonstrate that the model maintains comparable performance after discarding the learnable weights and biases."
        },
        {
            "title": "5 FUTURE WORK",
            "content": "Extending the Mixture-of-Experts Paradigm. We interpret Tokenformer as an extreme instantiation of the Mixture of Experts (MoE) framework, where each key-value parameter pair functions as an individual expert. This innovative MoE-like architecture has the potential to significantly reduce the computational costs associated with token-parameter interactions. Additionally, Tokenformers adjustable computational load for token-token interactions complements the MoE feature, facilitating the development of more resource-effective foundational models. Advancing Parameter-Efficient Tuning. The scaling approach of Tokenformer, which involves integrating additional key-value parameter pairs, exemplifies strategy for parameter-efficient tuning. When confronted with new tasks or datasets, the model can augment its pre-trained parameters by incorporating these new parameter tokens, thereby adapting to specific task requirements quickly. Integrating Vision and Language Models. Leveraging the parameter-efficient tuning capabilities of Tokeformer, we can achieve seamless integration of visual and linguistic modalities. This can be accomplished by unifying the key-value parameter tokens derived from pre-trained visual Tokenformer and language Tokenformer into single parameter set. Then, the new learnable tokens are introduced to perform vision-language alignment and instruction tuning. Device-Cloud Collaboration. Tokenformer can serve as the cloud-side knowledge base in devicecloud collaboration of on-device LLMs, with each pair of key-value parameter tokens representing learnable pattern, leveraging the device for real-time processing and the cloud for intensive tasks. Enhancing Model Interpretability. As Tokenformer is entirely based on attention mechanisms, it inherently benefits from the interpretability associated with attention in token-parameter interactions. This characteristic enhances the models explainability, contributing to the AI communitys efforts to develop more transparent and understandable models."
        },
        {
            "title": "6 CONCLUSION",
            "content": "This paper introduces Tokenformer, naturally scalable architecture that leverages the attention mechanism to facilitate not only inter-token computations but also interactions between tokens and model parameters, thereby enhancing architectural flexibility. By representing model parameters as tokens, we replace all linear projection layers in the Transformer with our Pattention layers, allowing for seamless and efficient incremental scaling without the need for retraining from scratch. We believe that this architecture, offering greater flexibility than traditional Transformers, will further contribute to the development of foundation models."
        },
        {
            "title": "REFERENCES",
            "content": "Radford Alec, Wu Jeffrey, Child Rewon, Luan David, Amodei Dario, Sutskever Ilya, et al. Language models are unsupervised multitask learners. OpenAI blog, 2019. Jimmy Lei Ba. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle OBrien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. Pythia: suite for analyzing large language models across training and scaling. In ICML. PMLR, 2023. Sid Black, Leo Gao, Phil Wang, Connor Leahy, and Stella Biderman. Gpt-neo: Large scale autoregressive language modeling with mesh-tensorflow. 2021. URL https://api. semanticscholar.org/CorpusID:245758737. Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, et al. Gpt-neox-20b: An open-source autoregressive language model. ACL Workshop, 2022. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. In NeurIPS, 2020. Cheng Chen, Yichun Yin, Lifeng Shang, Xin Jiang, Yujia Qin, Fengyu Wang, Zhi Wang, Xiao Chen, Zhiyuan Liu, and Qun Liu. bert2bert: Towards reusable pretrained language models. In ACL, 2021a. Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning via sequence modeling. In NeurIPS, 2021b. Tianqi Chen, Ian Goodfellow, and Jonathon Shlens. Net2net: Accelerating learning via knowledge transfer. In ICLR, 2015. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In CVPR, 2009. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021. Nelson Dunford and Jacob Schwartz. Linear operators, part 1: general theory. John Wiley & Sons, 1988. Daniel Fu, Tri Dao, Khaled Kamal Saab, Armin Thomas, Atri Rudra, and Christopher Re. Hungry hungry hippos: Towards language modeling with state space models. In ICLR, 2023. Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020. Aaron Gokaslan and Vanya Cohen. Openwebtext corpus. http://Skylion007.github.io/ OpenWebTextCorpus, 2019. Linyuan Gong, Di He, Zhuohan Li, Tao Qin, Liwei Wang, and Tieyan Liu. Efficient training of bert by progressively stacking. In ICML, 2019. Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023. Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. Masked autoencoders are scalable vision learners. In CVPR, 2022. 11 Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415, 2016. Edward Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In ICLR, 2022. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. Andrej Karpathy. Nanogpt, 2022. URL https://github.com/karpathy/nanoGPT. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In NeurIPS, 2023. Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In ICCV, 2021. Loshchilov. Decoupled weight decay regularization. In ICLR, 2019. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In ICLR, 2019. Matt Mahoney. Large text compression benchmark, 2011. URL http://mattmahoney.net/ dc/text.html. MMDetection Contributors. OpenMMLab Detection Toolbox and Benchmark, August 2018. URL https://github.com/open-mmlab/mmdetection. Bo Peng, Eric Alcaide, Quentin Gregory Anthony, Alon Albalak, Samuel Arcadinho, Stella Biderman, Huanqi Cao, Xin Cheng, Michael Nguyen Chung, Leon Derczynski, Xingjian Du, Matteo Grella, Kranthi Kiran GV, Xuzheng He, Haowen Hou, Przemyslaw Kazienko, Jan Kocon, Jiaming Kong, Bartłomiej Koptyra, Hayden Lau, Jiaju Lin, Krishna Sri Ipsit Mantri, Ferdinand Mom, Atsushi Saito, Guangyu Song, Xiangru Tang, Johan S. Wind, Stanisław Wozniak, Zhenyuan Zhang, Qinghua Zhou, Jian Zhu, and Rui-Jie Zhu. RWKV: Reinventing RNNs for the transformer era. In EMNLP, 2023. Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training. OpenAI, 2018. Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efficient transformers: survey. arXiv preprint arXiv:2009.06732, 2020. Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Hervé Jégou. Training data-efficient image transformers & distillation through attention. In ICML, 2021. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017. Haiyang Wang, Chen Shi, Shaoshuai Shi, Meng Lei, Sen Wang, Di He, Bernt Schiele, and Liwei Wang. Dsvt: Dynamic sparse voxel transformer with rotated sets. In CVPR, 2023a. Haiyang Wang, Hao Tang, Shaoshuai Shi, Aoxue Li, Zhenguo Li, Bernt Schiele, and Liwei Wang. Unitr: unified and efficient multi-modal transformer for birds-eye-view representation. In ICCV, 2023b. Haiyang Wang, Hao Tang, Li Jiang, Shaoshuai Shi, Muhammad Ferjad Naeem, Hongsheng Li, Bernt Schiele, and Liwei Wang. Git: Towards generalist vision transformer through universal language interface. In ECCV, 2024. 12 Peihao Wang, Rameswar Panda, Lucas Torroba Hennigen, Philip Greengard, Leonid Karlinsky, Rogerio Feris, David Daniel Cox, Zhangyang Wang, and Yoon Kim. Learning to grow pretrained models for efficient transformer training. In ICLR, 2023c. Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, and Hongxia Yang. Ofa: Unifying architectures, tasks, and modalities through simple sequence-to-sequence learning framework. In ICML, 2022. Wenhui Wang, Hangbo Bao, Li Dong, Johan Bjorck, Zhiliang Peng, Qiang Liu, Kriti Aggarwal, Owais Khan Mohammed, Saksham Singhal, Subhojit Som, et al. Image as foreign language: Beit pretraining for vision and vision-language tasks. In CVPR, 2023d. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. NeurIPS, 2022. Chengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He, Yanming Shen, and Tie-Yan Liu. Do transformers really perform badly for graph representation? In NeurIPS, 2021. Seongjun Yun, Minbyul Jeong, Raehyun Kim, Jaewoo Kang, and Hyunwoo Kim. Graph transformer networks. In NeurIPS, 2019. Biao Zhang and Rico Sennrich. Root mean square layer normalization. In NeurIPS, 2019. Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022. Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023."
        },
        {
            "title": "A GRADIENT OF PATTENTION LAYER",
            "content": "Our token-parameter attention mechanism employs L2-normalization followed by the GeLU (Hendrycks & Gimpel, 2016) activation function, in contrast to the conventional SoftMax function in standard token-token attention layers, which utilizes an exponential transformation followed by L1-normalization. This design choice is motivated by our experimental observation that SoftMax tends to increase the magnitude of outputs, often pushing them into regions where the gradients become extremely small, leading to an inferior overall performance (see Table 4). Specifically, given query token and key-value pairs with dimension d, let the similarity scores between the query and key tokens be represented as R1n. In standard SoftMax attention, the attention scores R1n are computed as follows: Si = d) exp(Ai/ j=1 exp(Aj/ (cid:80)n , 1...n, d) The derivative of the SoftMax function with respect to Ai is given by: Si Aj = 1 Si(1i=j Sj) = 1 Si(1 Sj) = 1 SiSj = j. In contrast, our activation function uses L2-normalization followed by the GeLU function. Denoting GeLU as , the attention scores R1n are computed as: ˆSi = (Zi) = ( Ai (cid:113)(cid:80)n j=1 Aj2 ), 1...m, For the derivative of our attention function when = j, we have: ˆSi Ai = A2 Ai A2 A2 Ai A2 2 2 A2 A2 2 2 A2 A2 A2 1 A2 (1 2 d) (d 2 ) = = = = 1 1 A2 1 A2 1 When = j, the derivative becomes: ˆSi Ai = = n = 1 Aj Ai A2 A2 2 AiAj 1 A2 A2 2 1 A2 ZiZj Thus, the derivative of our attention function is: ˆSi aj = 1 1 1 A2 1 A2 ZiZj (n ZiZj) = = j. 14 (15) (16) (17) (18) (19) (20) (21) (22) (23) (24) (25) (26) Comparing the gradients of SoftMax (Eq. 16) and our method (Eq. 26), the key difference is that our gradient depends on the product ZiZj, whereas SoftMax relies on SiSj. Due to the exponential nature of SoftMax, the distribution of Si tends to be sharper and more concentrated, which often drives the gradients toward zero. Conversely, our activation function produces smoother distribution of Z, mitigating the vanishing gradient problem and enabling more stable training dynamics."
        },
        {
            "title": "B ZERO INITIALIZATION IN TOKENFORMER",
            "content": "As shown in Section 3.3, during model scaling, initializing new key parameters to zero allows the model to continue training with minimal disruption. This is because zero initialization preserves the models original output distribution, preventing significant interference to the learned representations. In this section, we demonstrate that the Pattention layer is invariant to newly added parameters when they are zero-initialized. Let Rd be the input vector, and let the Pattention layer have key-value pairs, represented as KP , VP Rnd. The output of the Pattention layer is computed as: = KP X, = ( (cid:113)(cid:80) A2 ), = S. (27) (28) (29) When scaling the model by adding new key-value pairs with zero initialization, the output becomes: ˆA = = KP 0, .., 0 ... 0, .., 0 , 0 ... 0 ˆS = ( ˆA (cid:113)(cid:80) ˆA2 ˆO = (cid:2)V , new , ) = 0 ... 0 (cid:3) ˆS = O. (30) (31) (32) Since the newly added key parameters are initialized to zero, the attention mechanism does not modify the original output. Therefore, the output ˆO remains identical to O. This property is advantageous for scaling models because it increases the model capacity without interrupting the well-learned distribution and the ongoing training process, leading to faster convergence."
        },
        {
            "title": "C TABULAR MAIN RESULTS",
            "content": "Here, we provide the tabulated results corresponding to Figure 3 from the main paper. Table 7 presents the perplexity on the validation set of OpenWebText. The Transformer models are trained from scratch, while Tokenformer models leverage parameter reuse from smaller models (except for the first Tokenformer model with 124M parameters, which is also trained from scratch). We observe that Tokenformer achieves on-par performance to Transformers trained from scratch, but with significantly reduced training costs due to parameter reuse. Notably, the largest Tokenformer (1.4B) achieves even slightly better perplexity than its Transformer counterpart (11.60 vs. 11.63). Table 6 compares Transformer models trained from scratch and Tokenformer trained by parameter reusing with varying amounts of seen tokens during training. It is evident that Transformers trained with the same number of seen tokens do not reach the performance level of Tokenformer with parameter reusing. This demonstrates that parameter reusing successfully transfers knowledge from smaller models, reducing training time without sacrificing performance. 15 Perplexity #Param #Param Tokenformer Transformer Tokens Perplexity Tokens Perplexity Transformer Train from scratch 300B Transformer Train from scratch 30B Tokenformer Parameter Reusing 30B 354M 757M 1.4B 13.02 11.99 11. 15.97 13.78 13.34 14.02 12.59 11. Table 6: Tabular results of Figure 4. The perplexity of models trained with different numbers of schemes is compared. Transformers are trained from scratch, while Tokenformer are progressively scaled up via parameter resuing. When trained with the same number of tokens (30B), Tokenformer demonstrates superior performance. 124M 354M 354M 354M 757M 757M 757M 1.4B 1.4B 1.4B 60B 15B 30B 60B 15B 30B 60B 15B 30B 60B 16.41 14.58 14.02 13.59 13.08 12.59 12. 12.14 11.77 11.60 300B 17.06 300B 13.02 300B 11.93 300B 11.63 Table 7: Tabular results of Figure 3. Due to parameter reusing, Tokenformer achieves the same performance while using much lower number of training tokens when scaling up model sizes."
        },
        {
            "title": "D EXPERIMENTAL DETAILS ON PROGRESSIVE MODEL SCALING",
            "content": "In this experiment, we utilize the OpenWebText dataset (Gokaslan & Cohen, 2019) to evaluate the model scaling capability. The dataset comprises 8,013,769 documents, from which we randomly select 5% to serve as the validation set and report perplexity on this subset. We investigate four model sizes: 124M, 354M, 757M, and 1.4B parameters. Please find model specifications in Table 8. During parameter reusing of Tokenformer, we partially resume the old model parameters and add new key-value pairs to the Pattention layers and do not alter the number of layers or feature dimensions. The training recipe is the same for both Transformers and Tokenformer: We do not implement dropout in either model and the logits are computed at the final layer using the embedding layer. The tokenizer is from GPT-NeoX-20B (Black et al., 2022). We employ the AdamW optimizer (Loshchilov, 2019) with β1 = 0.9 and β2 = 0.95. learning rate of 6 104 is employed, with linear warmup over 2000 steps followed by cosine decay to zero. The training is conducted with batch size of 512 and sequence length of 1024."
        },
        {
            "title": "E EXPERIMENTAL DETAILS ON SCALING WITHOUT LOSING THE",
            "content": "WELL-LEARNED DISTRIBUTION In this experiment, we utilize the EnWik8 (Mahoney, 2011) dataset to evaluate the models capacity for continued adaptation to new data. The EnWik8 dataset comprises the first 100 million bytes of English Wikipedia in XML format. We begin with model size of 354M parameters, which has been pre-trained on OpenWebText (Gokaslan & Cohen, 2019), and then scale it to 757M parameters for both the Transformer and Tokenformer models. In the case of the Transformer, the 354M and 757M models differ solely in feature dimension and the number of heads in the multi-head attention mechanism, and we follow Net2Net(Chen et al., 2015) methodology for parameter expansion. For Tokenformer, we increase the number of key-value pairs from 2140 to 4850. We employ constant learning rate of 6 104 and process the dataset for single pass, resulting in total of 2204 training steps. The AdamW optimizer (Loshchilov, 2019) is utilized, and we do not resume the optimizers internal state from any previous runs. The batch size is set to 512, consistent with the batch size used during pre-training."
        },
        {
            "title": "F EXPERIMENTS ON LANGUAGE MODELING BENCHMARKING",
            "content": "We evaluate the expressiveness and performance of our proposed architecture using standard autoregressive language modeling benchmarks, comparing its results to those of existing open-source LLMs, including RNN-based methods (Peng et al., 2023; Gu & Dao, 2023), in Table 9. Evaluations 16 Model Layers Hidden size Attention KV Pairs FFN KV Pairs Heads #Params Language Modeling Tokenformer-150M Tokenformer-450M Tokenformer-900M Tokenformer-1.5B Visual Modeling Tokenformer-Base Tokenformer-Base Tokenformer-Large Tokenformer-Large Parameter Reusing Tokenformer-124M Tokenformer-354M Tokenformer-757M Tokenformer-1.4B 12 24 32 12 12 24 24 12 12 12 12 768 1024 1280 1536 768 768 1024 1024 768 768 768 768 768 1024 1280 576 768 768 1024 576 2140 4850 8620 3072 4096 5120 6144 2304 3072 768 4096 2304 8560 19400 34480 12 16 16 12 12 16 16 12 12 12 12 150M 450M 900M 1.5B 86M 109M 307M 407M 124M 354M 757M 1.4B Table 8: Details of Tokenformer model variants used in Section 4. indicates models whose key-value pairs are chosen to match the parameter numbers of Transformer of equivalent sizes. Model Hybrid H3-130M (Fu et al., 2023) Pythia-160M (Biderman et al., 2023) Mamba-130M (Gu & Dao, 2023) Ours (TokenFormer-150M) Hybrid H3-360M (Fu et al., 2023) Pythia-410M (Biderman et al., 2023) Mamba-370M (Gu & Dao, 2023) Ours (TokenFormer-450M) Pythia-1B (Biderman et al., 2023) Mamba-790M (Gu & Dao, 2023) Ours (TokenFormer-900M) GPT-Neo 1.3B (Black et al., 2021) Hybrid H3-1.3B (Fu et al., 2023) OPT-1.3B (Zhang et al., 2022) Pythia-1.3B (Biderman et al., 2023) RWKV-1.5B (Peng et al., 2023) Mamba-1.4B (Gu & Dao, 2023) GPT-Neo 2.7B (Black et al., 2021) Hybrid H3-1.3B (Fu et al., 2023) OPT-2.7B (Zhang et al., 2022) Pythia-2.8B (Biderman et al., 2023) Ours (TokenFormer-1.5B) #Param Pile ppl - 130M 160M 29.64 130M 10.56 150M 10.45 - 360M 410M 9.95 370M 8.28 450M 8.28 1B 7.82 790M 7.33 900M 7.38 1.3B 1.3B 1.3B 1.3B 1.5B 1.4B 2.7B 1.3B 2.7B 2.8B 1.5B - - - 7.51 7.70 6.80 - - - - 6.91 LAMBADA LAMBADA HellaSwag acc acc ppl PIQA Arc-E Arc-C WinoGrande Average acc acc acc acc acc 89.48 37.25 16.07 16. 12.58 10.84 8.14 7.69 7.92 6.02 5.46 7.50 11.25 6.64 6.08 7.04 5.04 5.63 11.25 5.12 5.04 5.24 25.8 35.4 44.3 45.0 48.0 51.4 55.6 57.3 56.1 62.7 64. 57.2 49.6 58.0 61.7 56.4 64.9 62.2 49.6 63.6 64.7 64.7 31.7 30.3 35.3 35.5 41.5 40.6 46.5 47.5 47.2 55.1 55.3 48.9 52.6 53.7 52.1 52.5 59.1 55.8 52.6 60.6 59.3 60.0 64.2 62.3 64.5 64. 68.1 66.9 69.5 69.5 70.7 72.1 72.4 71.1 71.3 72.4 71.0 72.4 74.2 71.1 71.3 74.8 74.0 74.8 44.4 43.6 48.0 47.3 51.4 52.1 55.1 56.2 57.0 61.2 59. 56.2 59.2 56.7 60.5 60.5 65.5 61.1 59.2 60.8 64.1 64.8 24.2 23.6 24.3 24.9 24.7 24.6 28.0 26.7 27.1 29.5 30.6 25.9 28.1 29.6 28.5 29.4 32.8 30.2 28.1 31.3 32.9 32.0 50.6 51.9 51.9 50. 54.1 53.8 55.3 54.6 53.5 56.1 56.4 54.9 56.9 59.5 57.2 54.6 61.5 57.6 61.4 61.0 59.7 59.7 40.1 40.6 44.7 44.7 48.0 48.2 50.0 52.0 51.9 56.1 56. 52.4 53.0 55.0 55.2 54.3 59.7 56.5 58.0 58.7 59.1 59.3 Table 9: (Zero-shot Evaluations.) The best results for each model size are highlighted in bold. We compare our models against open-source language models (LMs), including RNN-based methods, with various tokenizers, trained for up to 300B tokens. are conducted using both pre-training metrics, specifically perplexity, and zero-shot performance measures. For training, we used the Pile dataset (Gao et al., 2020) over single epoch, adhering to the training strategy used by Biderman et al. (2023). The Adam optimizer is applied with 6 104 learning rate, and each batch contains 1024 samples with sequence length of 2048 tokens, mirroring the setup of Mamba. The training process consists of 14,300 steps, with 1430-step warmup, equivalent to 1% of total training steps. Detailed model specifications are in Table 8."
        }
    ],
    "affiliations": [
        "Google",
        "Max Planck Institute for Informatics",
        "Peking University"
    ]
}