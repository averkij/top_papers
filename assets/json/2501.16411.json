{
    "paper_title": "PhysBench: Benchmarking and Enhancing Vision-Language Models for Physical World Understanding",
    "authors": [
        "Wei Chow",
        "Jiageng Mao",
        "Boyi Li",
        "Daniel Seita",
        "Vitor Guizilini",
        "Yue Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Understanding the physical world is a fundamental challenge in embodied AI, critical for enabling agents to perform complex tasks and operate safely in real-world environments. While Vision-Language Models (VLMs) have shown great promise in reasoning and task planning for embodied agents, their ability to comprehend physical phenomena remains extremely limited. To close this gap, we introduce PhysBench, a comprehensive benchmark designed to evaluate VLMs' physical world understanding capability across a diverse set of tasks. PhysBench contains 10,002 entries of interleaved video-image-text data, categorized into four major domains: physical object properties, physical object relationships, physical scene understanding, and physics-based dynamics, further divided into 19 subclasses and 8 distinct capability dimensions. Our extensive experiments, conducted on 75 representative VLMs, reveal that while these models excel in common-sense reasoning, they struggle with understanding the physical world -- likely due to the absence of physical knowledge in their training data and the lack of embedded physical priors. To tackle the shortfall, we introduce PhysAgent, a novel framework that combines the generalization strengths of VLMs with the specialized expertise of vision models, significantly enhancing VLMs' physical understanding across a variety of tasks, including an 18.4\\% improvement on GPT-4o. Furthermore, our results demonstrate that enhancing VLMs' physical world understanding capabilities can help embodied agents such as MOKA. We believe that PhysBench and PhysAgent offer valuable insights and contribute to bridging the gap between VLMs and physical world understanding."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 2 ] . [ 2 1 1 4 6 1 . 1 0 5 2 : r Published as conference paper at ICLR 2025 PHYSBENCH: BENCHMARKING AND ENHANCING VISION-LANGUAGE MODELS FOR PHYSICAL WORLD UNDERSTANDING Wei Chow1, Jiageng Mao1, Boyi Li2, Daniel Seita1, Vitor Guizilini3, Yue Wang1 1University of Southern California, 2UC Berkeley, 3Toyota Research Institute"
        },
        {
            "title": "ABSTRACT",
            "content": "Understanding the physical world is fundamental challenge in embodied AI, critical for enabling agents to perform complex tasks and operate safely in real-world environments. While Vision-Language Models (VLMs) have shown great promise in reasoning and task planning for embodied agents, their ability to comprehend physical phenomena remains extremely limited. To close this gap, we introduce PhysBench, comprehensive benchmark designed to evaluate VLMs physical world understanding capability across diverse set of tasks. PhysBench contains 10,002 entries of interleaved video-image-text data, categorized into four major domains: physical object properties, physical object relationships, physical scene understanding, and physics-based dynamics, further divided into 19 subclasses and 8 distinct capability dimensions. Our extensive experiments, conducted on 75 representative VLMs, reveal that while these models excel in common-sense reasoning, they struggle with understanding the physical worldlikely due to the absence of physical knowledge in their training data and the lack of embedded physical priors. To tackle the shortfall, we introduce PhysAgent, novel framework that combines the generalization strengths of VLMs with the specialized expertise of vision models, significantly enhancing VLMs physical understanding across variety of tasks, including an 18.4% improvement on GPT-4o. Furthermore, our results demonstrate that enhancing VLMs physical world understanding capabilities can help embodied agents such as MOKA. We believe that PhysBench and PhysAgent offer valuable insights and contribute to bridging the gap between VLMs and physical world understanding. Project Page is here"
        },
        {
            "title": "1\nUnderstanding the physical world is a fundamental challenge in embodied AI (Gupta et al., 2021;\nSrivastava et al., 2021). Embodied agents are required to understand the physical properties of\nobjects (e.g., mass, stiffness) to accurately interact with these objects. They also need to understand\nthe relationships of physical objects to operate efficiently in cluttered environments, understand\nthe structure of physical scenes for safe navigation and manipulation, and anticipate the outcomes\nof interactions and physics-based dynamics for better planning and preventing accidents. These\ncapabilities of intuitive physics (McCloskey et al., 1983; Carey, 2000) are innate to humans and can\nalso greatly benefit embodied agents, allowing them to perform complex tasks and operate safely in\nreal-world scenarios (Kill & Kim, 2020).",
            "content": "Vision-language models (VLMs) (Liu et al., 2024c; Achiam et al., 2023; Team et al., 2023) have emerged as promising solutions for building embodied agents (Liu et al., 2024a; Nasiriany et al., 2024; Huang et al., 2023a). Trained on large amounts of human knowledge, these models have developed strong capabilities in reasoning and task planning (Yue et al., 2024; Lu et al., 2024b; Kim et al., 2024; Niu et al., 2024; Zhen et al., 2024). However, relying solely on these capabilities is insufficient for developing generalist embodied agents. series of studies have highlighted gap in understanding the physical world, leading to operational errors (Liu et al., 2024a), such as mishandling fragile objects (Wang et al., 2023c) or failing to recognize appropriate grasping affordances (Guo et al., 2024). Since these agents operate in and interact with the real world, VLMs must possess comprehensive understanding of the physical worlda critical yet underexplored domain. This deficiency in physical world understanding limits the effective deployment of VLMs in embodied applications (Liu et al., 2024a; Guo et al., 2024; Gao et al., 2024a). Equal contribution. 1 Published as conference paper at ICLR 2025 Figure 1: Common VQA tasks typically involve questions about visual content and general knowledge. PhysBench emphasizes understanding the physical world, encompassing 4 dimensions. To further investigate this issue, we pose two fundamental questions: (1) Do VLMs possess an understanding of the physical world, and if not, what factors contribute to this limitation? (2) How can we enhance VLMs physical world understanding capabilities and facilitate the effective deployment of embodied agents like MOKA? To answer the above questions and comprehensively assess the extent of the gap between VLMs and physical world understanding, we introduce PhysBench, dataset comprising 10,002 interleaved video-image-text entries. Given the difficulty of acquiring such data, where expressing specific properties often requires multiple images, we undertook five-step process, spending total of 4,000 hours on annotation. We systematically evaluate 75 representative VLM across four domainsphysical object properties, physical object relationships, physical scene understanding, and physics-based dynamicsencompassing 19 sub-tasks, as shown in Figure 1. Our extensive experiments reveal that (1) most current VLMs exhibit poor understanding of the physical world, particularly in physical scene understanding and physics-based dynamics, with closed-source models significantly outperforming open-source ones; and (2) the training data for VLMs is likely major factor contributing to their subpar performance, as it often lacks the necessary physical knowledge. Notably, when VLMs were fine-tuned on our physically grounded data, their performance improved. To further improve VLMs physical world understanding capabilities, we propose PhysAgent, unified framework that incorporates vision foundation models and physics knowledge memory. By analyzing the sources of errors for VLMs on PhysBench, we identified perceptual inaccuracies and insufficient knowledge as the primary causes of mistakes. To address these issues, we incorporated vision foundation models to enhance perceptual capabilities and assist VLMs in handling tasks they typically struggle with, such as depth estimation and numerical distance calculation. Additionally, we integrated knowledge memory module to embed essential knowledge about the physical world, which can be selectively invoked by PhysAgent. Unlike previous methods designed for physical reasoning (Zheng et al., 2024b; Tung et al., 2023), PhysAgent retains the strong generalization abilities of VLMs and their capacity to solve open-ended problems, without relying on manually predefined processing logic or being limited to specific tasks. Experimental results demonstrate that PhysAgent improves GPT-4os zero-shot performance on PhysBench by 18.4%. Furthermore, we investigate how physical world understanding helps the deployment of embodied agents through extensive robotic manipulation experiments on MOKA (Liu et al., 2024a). Specifically, we employ two approaches: fine-tuning the VLM with PhysBench and utilizing PhysAgent for zero-shot inference across five representative manipulation tasks. The improvement in those tasks further validates that PhysBench and PhysAgent can facilitate the deployment of embodied agents like MOKA. We hope this work offers valuable insights and contributes to bridging the gap between VLMs and physical world understanding, ultimately advancing embodied AI toward human-level capabilities. In summary, this paper has two technical contributions: (1) We present PhysBench, large-scale benchmark for evaluating the performances of vision-language models in physical world understanding. We identify the key challenges through extensive studies and provide insights into why the existing VLMs have insufficient physical world understanding capabilities. (2) We propose PhysAgent, unified approach that improves VLMs physical world understanding abilities. Through extensive experiments, we demonstrate that enhancing VLMs comprehension of physical environments can significantly facilitate the deployment of embodied agents. 2 Published as conference paper at ICLR"
        },
        {
            "title": "2 RELATED WORK",
            "content": "Physical Comprehension Datasets. Early benchmarks (Riochet et al., 2018; Rajani et al., 2020) were developed primarily for vision-only models, while more recent efforts (Yi et al., 2019; Chen et al., 2022; Wang et al., 2024g) have predominantly focused on simple visual primitives, such as spheres, cubes, and rigid object collision events, often restricted to limited set of simulated scenarios (Zheng et al., 2024b; Tung et al., 2023). We summarize the key features of these various benchmarks and compare them against our benchmark in Table 1. However, existing VQA datasets assessing physical knowledge (Lu et al., 2022; He et al., 2024) mainly focus on commonsense reasoning rather than physical world perception. Spatial VQA benchmarks (Chen et al., 2024a; Lyu et al., 2024; Bonnen et al., 2024; Wang et al., 2024d) emphasize geometric relationships in 3D sence, which represent only part of the physical. In contrast, PhysBench is the first comprehensive dataset designed to evaluate models understanding of the physical world, encompassing wide variety of scenarios and tasks not covered by previous benchmarks. Physical Reasoning Models. Models for understanding the physical world generally fall into two categories. The first comprises physics-specialized models (Guen & Thome, 2020; Duan et al., 2022), which are typically limited to predicting the next state and are not applicable to other tasks. The second includes physical oracle models (Zheng et al., 2024b; Tung et al., 2023), which are suitable for only narrow range of tasks due to their reliance on predefined rules. These models often require training additional modules like R-CNN, and their probabilistic outputs restrict them to classification tasks, limiting their ability to handle open-ended questions. In contrast, PhysAgent offers greater flexibility and adaptability across broader spectrum of problems without these limitations. Vision-Language Models. Vision-Language Models (VLMs) are large-scale models that integrate visual modalities with language understanding (Wu et al., 2023b; Zhan et al., 2024; Dai et al., 2024). In recent years, there has been surge of work leveraging VLMs as agents for embodied AI (Liu et al., 2024a; Nasiriany et al., 2024). Although these approaches are generalizable, they face challenges due to weak physical world understanding capabilities (Liu et al., 2024a; Guo et al., 2024). By employing PhysBench and PhysAgent, these shortcomings can be mitigated, enhancing the physical world understanding capabilities of VLMs and enabling more reliable robotic control. Additionally, spatial VLMs (Bonnen et al., 2024) have identified that most VLMs lack 3D spatial reasoning capabilities due to insufficient data. However, since spatial reasoning represents only subset of physical world understanding, our work aims to provide more comprehensive evaluation and improvement of VLMs physical world understanding abilities. For additional related work, see Appendix G. Table 1: comparison between PhysBench and other physical understanding question-answering benchmarks. PhysBench is comprehensive dataset, covering wide range of tasks related to physical world understanding. Property Attribute Location Motion Temperature Viewpoint Light Collision Manipulation CLEVRER (Yi et al., 2019) Cater (Girdhar & Ramanan, 2019) CRIPP-VQA (Patel et al., 2022) ComPhy (Chen et al., 2022) EmbSpatial (Du et al., 2024) Physion (Bear et al., 2021) Physion++ (Tung et al., 2023) ContPhy (Zheng et al., 2024b) SuperCLEVR (Wang et al., 2024g) PhysBench Fluid Interleaved Size More than cube 300,000 5,500 5,000 99,844 3,600 17,200 2,000 6,500 1,200 10,"
        },
        {
            "title": "3 PHYSBENCH",
            "content": "To assess VLMs physical world understanding ability, we first define the concept of physical world understanding and introduce PhysBench in Section 3.1. Next, we provide detailed description of the data collection process in Section 3.2. Utilizing PhysBench, we conduct experiments to determine whether VLMs can effectively comprehend the physical world in Section 3.3. Finally, in Section 3.4, we discuss the potential reasons for poor performance."
        },
        {
            "title": "3.1 OVERVIEW OF PHYSBENCH\nUnderstanding the physical world is essential yet fundamentally challenging for embodied AI, as\nsystems must perceive, interpret, and predict the properties and dynamics of objects and environ-\nments. This involves comprehending object properties and relationships, interpreting environmental\nscenes, and anticipating interaction outcomes based on visual cues and core physical principles to\nensure safe and effective operation.",
            "content": "3 Published as conference paper at ICLR 2025 Figure 2: Sampled PhysBench examples from four major dimensions mentioned in Section 3.1. Due to space constraints, we present only the correct answers (as each question in our dataset is fouroption multiple-choice with one correct answer) and defer additional examples to Appendix C. However, existing datasets often focus solely on image content and commonsense reasoning, neglecting the four fundamental aspects of the physical world mentioned above. To address this gap, we propose PhysBench, which comprehensively evaluates VLMs perception of the physical world across four major task categories of the physical world: (1) Physical Object Property: Assessment of physical attributes of objects such as mass, size, density, tension, friction, bending stiffness, elasticity, and plasticity. (2) Physical Object Relationships: Evaluation of spatial relationships in- (3) Physical Scene Understanding: volving objects relative or absolute positions and motions. Interpretation of environmental factors, including light sources, viewpoints, temperature, etc. (4) Physics-based Dynamics: Understanding of physical events like collisions, throwing, fluid dynamics, explosions, and other phenomena. Each category corresponds to specific sub-task types and ability types, whose distributions are shown in Figures 3. Detailed examples of specific tasks are illustrated in Figure 2, with additional examples provided in Appendix H. comprehensive description of sub-task types and ability types is available in Appendix C. PhysBench is structured as multiple-choice questionnaire, presenting four options for each question, with only one correct answer. The primary statistics of PhysBench are presented in Table 2 and detailed in Appendix D. Recognizing that different types of tasks possess unique characteristics, we utilize videos and multiple images to effectively convey features that are difficult to capture in single imagesuch as elasticity, mass, density, and environmental factors like temperature, humidity, light source, and viewpoint. The dataset also includes objects with similar initial states but differing properties, leading to different future outcomes. This enriches the dataset and allows for wider range of observable physical behaviors. Consequently, PhysBench draws its data from the internet, real-world captures, and simulations, making it mixed-format benchmark that integrates text, images, and videos. For convenience, PhysBench-test consists of 10,002 entries, organized into 19 subclasses, as the test set, and 200 entries as the validation set for parameter choosing. We also present 89,998 entries for further research. The experimental results presented in this paper, unless otherwise specified, are based on the test set. The performance of VLMs on PhysBench-val can be found in Appendix F.4. Benchmark release details can be found in Appendix B.8. Published as conference paper at ICLR 2025 Statistic Total questions - only one image - only one video - interleave Number 10,002 1,766 (18.6%) 2,749 (44.8%) 1,902 (20.1%) Unique number of images Unique number of videos 10,058 3,260 3D Assets Maximum question length Maximum choice length Average question length Average choice length 678 48 20 16.5 4.4 Table 2: Key statistics. Figure 3: Subtype distribution and ability distribution"
        },
        {
            "title": "3.2 DATASET COLLECTION PROCESS",
            "content": "To ensure data quality, all questions were manually annotated by graduate students in STEM fields and further refined through rigorous review process after collecting and clipping the raw images or videos. To maintain consistency in annotations, we implemented multiple rounds of cleaning and validation throughout the following steps. We have preserved intermediate outputs from the annotation process, such as depth and reflectance maps for simulator-generated data and human-annotated physical principles for many web-sourced videos. The process involves the following sequential steps: (a) Video Collection. Videos and images are gathered from web searches, simulations, and real-world captures. The collection process uses predefined simulation rules, LLM-guided queries, and other strategies to find related images or videos (see Appendix A). Human annotators further refine the data by clipping and annotating physical principles in the images or videos. (b) Video Captioning. Human-annotated raw videos are processed through automatic filtering, followed by GPT-4o annotations that generate captions with human check. (c) Questions Design. For videos annotated with physical principles, we generate physics-related questions using both manual design and GPT-4o, following predefined rules. An automated filter and manual review processes eliminate irrelevant questions. (d) File Organization. The remaining valid questions are categorized by task, sub-task, and ability type by human experts. (e) Quality Check. The organized dataset undergoes human review to ensure that the questions are physical world relevant, rely on all input information, are not grounded in common sense, and are accurately categorized with clear questions and corresponding answers. Due to space limitations, the collection guidelines are provided in Appendix B."
        },
        {
            "title": "3.3 CAN VLMS UNDERSTAND THE PHYSICAL WORLD",
            "content": "To assess whether VLMs can understand the physical world, we evaluated 75 representative VLMs on PhysBench and found that significant performance gap remains between VLMs and humanlevel understanding. The primary results are presented in Table 3, while detailed analyses of sub-task performance and ability types across the four task categories are provided in Appendix F.3. Setup. Our evaluation was conducted under three configurations: (a) Image VLMs, which support only single-image input (e.g., LLaVA-1.5 and BLIP-2); (b) Video VLMs, designed for video comprehension (e.g., Chat-UniVi and PLLaVA); and (c) General VLMs, which support multiple images and interleaved inputs (e.g., VILA-1.5 and GPT-4o). It is important to note that the data used for evaluating setups (a) and (b) is subset of PhysBench test subset with interleaved QA pairs removed, whereas setup (c) was evaluated on the full dataset. For most models, we followed the standard protocol outlined in VLMEvalKit Contributors (2023), setting the temperature to 0. For models that do not support multiple images as input, we employed two methods: the merge method, where video frames are concatenated into single image (Fu et al., 2024; Zhang et al., 2024a; Jiang et al., 2024), and the seq method, where video frames are input sequentially as individual images. Notably, only models using the seq setup can handle interleaved text-image sequences. For details on VLM prompts and hyperparameters, see Appendix E. VLMs exhibit limited understanding of the physical world. Our evaluation indicates that most models achieve an average accuracy of approximately 40%, which is significantly below humanlevel performance. Even the best-performing model, GPT-4o, attains only 49.49% accuracy, underscoring the substantial gap between current VLMs and true comprehension of the physical world. 5 Published as conference paper at ICLR 2025 Figure 4: (a) Correlation map between 4 tasks in PhysBench and 15 other vision-language benchmarks. (b) The visualization of model performance across 19 sub-tasks is presented, where different colors represent the respective categories. The four colors, from left to right, represent physical object property, physical object relationships, physical scene, and physical-based dynamics. Random Choice Human Size Format 2Property Relationships Scene (cid:1)Dynamics Avg 25.00 95.87 25.00 97.10 25.00 95.68 25.00 95.67 25.00 94. - - - - Image VLM InstructBLIP-t5-xl (Dai et al., 2024) InstructBLIP-t5-xxl (Dai et al., 2024) InstructBLIP-7B (Dai et al., 2024) InstructBLIP-13B (Dai et al., 2024) BLIP-2 (Li et al., 2023c) LLaVA-1.5-7B (Liu et al., 2023a) LLaVA-1.5-13B (Liu et al., 2023a) LLaVA1.6-mistral (Liu et al., 2024b) LLaVA1.6-vicuna (Liu et al., 2024b) Qwen-VL-Chat (Bai et al., 2023b) InternVL-Chat1.5 (Chen et al., 2024c) Cambrian-8B (Tong et al., 2024) Claude-3-opus (Anthropic, 2024) Claude-3-sonnet (Anthropic, 2024) Claude-3-haiku (Anthropic, 2024) Claude-3.5-sonnet (Anthropic, 2024) 4B merge 12B merge 7B merge 13B merge 12B merge 7B merge 13B merge 7B merge 7B merge 9B merge 26B merge 8B merge - merge - merge - merge - merge 35.35 41.11 21.94 31.69 41.70 38.44 41.31 29.77 40.26 35.97 53.08 23.27 41.97 37.86 43.28 46. Video-LLaVA (Lin et al., 2023a) Chat-Univi-7B (Jin et al., 2023) Chat-Univi-13B (Jin et al., 2023) PLLaVA-7B (Xu et al., 2024) PLLaVA-13B (Xu et al., 2024) Video VLM 7B 7B 13B 7B 13B seq seq seq seq seq 36.82 19.28 4.30 38.02 39.91 General VLM + Interleaved data LLaVA-interleave (Li et al., 2024d) 7B LLaVA-interleave-dpo (Li et al., 2024d) 7B 3B VILA-1.5-3B (Lin et al., 2023b) 3B VILA-1.5-3B-s2 (Lin et al., 2023b) 8B VILA-1.5-8B (Lin et al., 2023b) 13B VILA-1.5-13B (Lin et al., 2023b) 4B Phi-3V (Abdin et al., 2024) 7B LLaVA-NV (Zhang et al., 2024b) 7B LLaVA-NV-dpo (Zhang et al., 2024b) 8B Mantis-Idefics2 (Jiang et al., 2024) Mantis-LLaVA (Jiang et al., 2024) 7B Mantis-siglip-llama3 (Jiang et al., 2024) 8B 8B Mantis-clip-llama3 (Jiang et al., 2024) - GPT-4V (Achiam et al., 2023) - GPT-4o (Achiam et al., 2023) - GPT-4o-mini (Achiam et al., 2023) - Gemini-1.5-flash (Team et al., 2023) - Gemini-1.5-pro (Team et al., 2023) seq seq seq seq seq seq seq seq seq seq seq seq seq seq seq seq seq seq 47.23 47.97 32.40 33.14 33.41 40.53 43.67 38.33 38.83 41.97 44.48 42.47 40.61 49.59 56.91 53.54 57.41 57.26 36.67 38.47 29.00 33.19 40.83 41.53 42.50 22.22 59.72 43.33 70.14 17.92 40.97 40.00 53.33 41.11 36.11 20.97 11.53 35.83 38.33 44.62 42.67 33.02 30.26 29.88 40.15 37.92 30.83 44.31 41.44 30.45 32.78 35.11 45.77 64.80 44.24 52.24 63. 37.45 37.89 19.53 23.13 36.25 38.60 34.40 8.54 38.60 26.47 37.01 23.02 30.63 32.23 30.06 27.89 33.69 18.86 15.67 36.34 31.52 35.64 33.73 34.84 35.72 30.85 31.96 34.93 34.00 33.86 29.53 36.25 36.83 32.45 26.34 30.15 30.59 34.32 36.52 35.95 36.42 27.45 30.64 36.93 42.69 44.38 20.58 42.65 41.27 44.78 29.29 36.50 36.89 39.93 37.60 40.52 28.46 11.47 39.89 40.76 37.21 38.78 35.78 33.00 35.91 36.07 36.92 37.17 37.21 36.56 34.73 37.51 38.36 42.15 46.99 42.90 40.93 41. 36.24 38.51 23.82 29.94 38.61 40.09 40.45 20.30 42.28 35.63 47.51 24.61 37.00 36.18 39.44 38.05 37.04 22.19 10.36 37.94 37.70 41.00 40.83 34.11 33.07 32.85 37.15 38.42 35.42 37.43 37.39 36.69 37.64 36.92 41.26 49.49 43.15 46.07 49.11 Table 3: Evaluation results for 39 VLMs. The evaluation of General VLMs is based on the data from Video and Image VLM evaluations, with the addition of interleaved data. Seq refers to sequential input of frames of videos, while merge refers to merging video frames into single image. Bold indicates the best result, and underline indicates the second best in each group. 6 Published as conference paper at ICLR As shown in Figure 4(b), considerable room for improvement remains, particularly in tasks related to physical scene understanding and physics-based dynamics. Closed-source models generally perform better. As shown in Figure 5(b), the GPT series and Gemini-1.5 models significantly outperform open-source models. Notably, GPT-4 surpasses the best open-source model, LLaVA-interleave, by 20.7%, indicating substantial gap between opensource and closed-source models. However, we did not observe clear advantage with Claude, finding that aligns with results from other benchmarks (Cao et al., 2024; Wu et al., 2024c). Figure 5: (a) The performance of 8 representative open-source General VLMs across 19 sub-tasks in PhysBench, which support interleaved inputs. The closer it is to the circular boundary, the better. (b) The overall performance of those 8 VLMs. Closed-source models generally perform better."
        },
        {
            "title": "3.4 WHY DO VLMS STRUGGLE WITH PHYSICAL WORLD UNDERSTANDING",
            "content": "To further investigate why VLMs struggle with physical world understanding, we analyzed PhysBench and discovered that it differs significantly from common VQA tasks. Additionally, we found that the performance of larger model size or more training data does not result in clear improvements on PhysBench, which may be due to lack of physical world knowledge in the training data. Furthermore, we found that many errors stem from this deficiency; when we augmented the models with physical world knowledge, their performance improved. This further suggests that the gap between VLMs and physical world understanding may be attributed to limitations in the training data. Physical world understanding differs significantly from common VQA tasks. To assess the relationships between our tasks and other VLM benchmarks, we adopted the methodology proposed by (Tong et al., 2024; Fang et al., 2024) to construct correlation map, as shown in Figure 4(a). Details on the construction of the correlation map are provided in Appendix F.6. Our analysis reveals that PhysBench differs significantly from traditional VLM benchmarks, exhibiting closer alignment with POPE (Li et al., 2023h) in tasks such as hallucination detection, while also showing that performance does not consistently improve with increased data or model scale. VLMss physical world understanding ability does not scale with model size, data, or frames. (1) Model Size Scalability. Figure 6(a) shows that increasing model size using the same dataset significantly enhances performance on common QA tasks. However, this improvement does not extend to PhysBench, where gains are limited or even negative. For instance, while VILA-1.5s performance improves by 7.1% on common QA tasks when scaling from 3B to 7B parameters, it Figure 6: (a) Model size scalability. The solid line shows the average performance across 14 common QA tasks  (Table 23)  , while the dashed line represents PhysBench results. (b) Data scalability. VILA and PLLaVA expand upon LLaVAs architecture by utilizing more data. (c) Frame scalability. 7 Published as conference paper at ICLR decreases by 3.8% on PhysBench. (2) Data Scalability. As shown in Figure 6(b), scaling up the dataset offers limited benefits for physical comprehension. PLLaVA and VILA-1.5, larger-data variants of LLaVA-1.5, exhibit minimal improvement or even decline in performance on PhysBench compared to LLaVA-1.5. Analysis of the additional data (Appendix D.2) reveals it is predominantly descriptive, focusing on content description rather than enhancing physical understanding. Nevertheless, VILA-1.5s spatial reasoning abilities have significantly improved, aligning with trends observed in other benchmarks (Yu et al., 2023b; Li et al., 2023a). (3) Frame Scalability. Figure 6(c) indicates that the three open-source models are insensitive to the number of frames, performing similarly to single-frame inputs, with performance sometimes decreasing as frames increase. This suggests that current models cannot effectively utilize multi-frame information. Notably, increasing the number of frames led Mantis to frequently fail to follow instructions or refuse to answer, and expanding beyond eight frames did not yield further improvements. Perceptual and knowledge gaps constitute the majority of errors. To investigate the poor performance of VLMs on PhysBench, we randomly selected 500 questions and obtained explanations from three modelsGPT-4o, Phi-3V, and Gemini-1.5-flash. Expert annotators classified the root causes of the mispredictions into six categories: perception errors, reasoning errors, lack of knowledge, refusal to answer, failure to follow instructions, and annotation errors in the dataset. The distribution of these error types is shown in Figure 7, with selected cases and detailed analyses provided in Appendix I. The error distribution reveals that perceptual errors account for 37%, 40%, and 45% of the mistakes made by GPT-4o, Gemini-1.5-flash, and Phi-3V, respectively, while lack of knowledge constitutes 34%, 35%, and 23% of errors for these models. This analysis suggests that perceptual errors and knowledge gaps are the primary sources of mispredictions, indicating that while the models are adept at extracting information from text and visual inputs, their physical world understanding and complex reasoning abilities remain limited. Figure 7: Distribution of error types for GPT-4V, Gemini-1.5-flash, Phi-3V. Can VLMs transfer physical world knowledge? Our error analysis revealed that inadequate physical world knowledge and reasoning capabilities were key contributors to the models poor performance. To investigate whether introducing additional examples could enhance performance, we conducted tests on 200 entries of PhysBench, pairing each with similar example. These additional examples were incorporated through fine-tuning or in-context learning. As shown in Figure 9(b), the performance improvements after adding physical world knowledge examples indicate that VLMs can transfer physical knowledge to some extent. This suggests that the original datas lack of physical world knowledge was significant factor in the models suboptimal performance."
        },
        {
            "title": "4 PHYSAGENT",
            "content": "Recognizing perceptual inaccuracies and knowledge gaps as key sources of error shown in Section 3.4, we introduce PhysAgent in Section 4.1 to improve VLMs understanding of the physical world by integrating vision foundation models for enhanced perception and incorporating physical knowledge memory. To verify whether enhancing VLMs physical understanding facilitates the deployment of embodied agents, we conducted five embodied agent tasks as detailed in Section 4.2."
        },
        {
            "title": "4.1 HOW TO ENHANCE VLMS FOR PHYSICAL WORLD UNDERSTANDING",
            "content": "We propose PhysAgent, novel framework that integrates knowledge memory and vision foundation models to enhance physical world understanding in VLMs. This framework is inspired by our findings in Section 3.4, where we identified perceptual errors and insufficient knowledge as the 8 Published as conference paper at ICLR 2025 primary causes of mistakes in VLMs. To address these shortcomings, we establish knowledge memory that provides prior physical world knowledge and rules. Additionally, we utilize vision foundational models namely Depth Anything (Yang et al., 2024b), SAM (Kirillov et al., 2023), and GroundingDINO (Liu et al., 2023b) to achieve enhanced visual perception. These models enable us to identify object types and spatial locations, and further acquire information about objects dynamics through VLM reasoning or retrieval from memory. They also help solve problems that VLMs cannot address, such as estimating depth and numerical distances. Unlike prior physical reasoning models that are confined to specific tasks and struggle to adapt to natural language queries, our method aims to fully leverage the reasoning and generalization capabilities of VLMs. Experiments on PhyBench show that PhysAgent improves performance by 18.4% on GPT-4o. As illustrated in Figure 8, given question, PhysAgent follows three key steps: (1) Task-specific Prompt Activation: PhysAgent first classifies the question (manually or automatically) and activates task-specific prompts, incorporating relevant physical knowledge for different tasks. For instance, for question about light, it retrieves knowledge on the relationship between light source movement and shadow direction to assist the VLMs. (2) Foundation Models Integration: PhysAgent processes the foundation models outputs, leveraging VLM reasoning capabilities. For example, it identifies objects in the scene using GroundingDINO and retrieves relevant attributes from the knowledge memory. (3) Chain-of-Thoughts Reasoning: PhysAgent then engages in chain-of-thought reasoning, conducting self-verification step to ensure logical consistency before providing the final answer. Figure 8: Architecture of PhysAgent. PhysAgent employs three-step reasoning process to address the problem: activating task-specific prompts, integrating foundation models, and reasoning. Baselines. We utilized three prompt methods: Chain of Thought (CoT) (Kojima et al., 2023), DespCoT (Wu et al., 2023d), and Pure Language Reasoning (PLR), in addition to an oracle method, ContPhy (Zheng et al., 2024b), which served as our baseline. Detailed descriptions of the prompt strategies and the implementation of ContPhy are provided in Appendix E.6 and Appendix E.7. Results. The results in Figure 9(a), lead to the following conclusions: (1) Prompting methods are unstable, and using pure language yields catastrophic results. As observed, the CoT strategy has minimal impact, while both Desp-CoT and PLR show decline in performance. This suggests that descriptive prompts are not particularly effective for addressing the questions, implying that our dataset requires deeper understanding of the videos or images to answer accurately. (2) ContPhy even worsens performance. In three out of four tasks, ContPhy underperforms compared to its base model, GPT-4o, due to suboptimal module invocation and limited flexibility in its logical templates, which struggle to adapt to diverse scenarios. Additionally, ContPhy relies on models like RCNN to process visual information instead of directly leveraging GPT-4o, leading to potential information loss and subsequent performance degradation. (3) PhysAgent consistently improves zero-shot performance, notably achieving 49.5% improvement for GPT-4o in Scene. Compared to the CoT, Desp-CoT, and PLR prompting strategies, our method demonstrates consistent improvements."
        },
        {
            "title": "4.2 CAN PHYSICAL WORLD UNDERSTANDING HELP IN EMBODIED APPLICATIONS",
            "content": "Despite gaining significant attention in recent years for their strong generalization capabilities, VLMs as embodied agents (Liu et al., 2024a) still exhibit fundamental operational errors during physical world interactions. In this section, we investigate whether enhancing the physical world perception abilities of VLMs can improve their performance in downstream embodied agent tasks. 9 Published as conference paper at ICLR 2025 Property Relationships Scene Dynamics Phi-3V + CoT + Desp-CoT + PLR + PhysAgent ContPhy GPT-4o + CoT + Desp-CoT + PLR + PhysAgent 43.6 42.5 42.7 38.6 44.5 52.1 56.9 58.6 57.7 51.5 58. 37.9 34.5 35.3 32.6 47.0 52.9 64.8 70.5 64.0 45.6 84.2 34.9 29.8 36.2 36.7 38.6 37.2 30.1 36.0 36.9 31.1 45.0 36.9 36.7 34.9 34.0 37.1 42.8 46.9 47.7 46.2 33.6 51.3 Affordance Force Color Location Tool MOKA reasoning error execution error success + PhysAgent reasoning error execution error success + Fine-tune reasoning error execution error success 0.3 0.1 0.6 0.0 0.2 0.8 0.0 0.1 0.9 0.5 0.3 0. 0.2 0.3 0.5 0.1 0.3 0.6 0.1 0.2 0.7 0.0 0.2 0.8 0.1 0.1 0.8 0.1 0.2 0. 0.1 0.2 0.7 0.0 0.2 0.8 0.3 0.3 0.4 0.2 0.3 0.5 0.0 0.3 0.7 (a) (b) (c) Figure 9: (a) Performance comparison of various methods. (b) Analysis of physical world knowledge transfer. (c) Performance evaluation across five embodied tasks as described in Figure 10. To evaluate embodied agents, we designed five fundamental manipulation tasks as shown in Figure 10(a). The specifics of these tasks, along with the corresponding testing methods and language instructions, can be found in Appendix F.5. These tasks require the agents to possess basic understanding of spatial relations and the physical properties of objects. Specifically, we utilized MuJoCo (Todorov et al., 2012) and the 7-DoF Franka Emika robotic arm from Menagerie (Zakka et al., 2022), building our simulation platform based on MOKA (Liu et al., 2024a) as the embodied agent approach to test these embodied tasks. The VLM we used in these tasks is GPT-4o. Affordance Grasp pot, knife, spoon, monitor, tennis racket. Force Grasp fragile items (egg, ripe persimmon), soft items (jelly, plastic cup), and rigid objects (iron ball). Grasp the specific color cube. Color Location Grab the cube in specific location and move it to Tool the plate. Grasp specific tools, depending on the problem. (a) (b) Figure 10: (a) Description of each of the testing tasks. (b) Marked observation, predicted affordances, and motion in MOKA. MOKA leverages VLM to generate key points and waypoints, and then converts these affordance representations into executable motions for the robotic arm. As illustrated in Figure 10(b), MOKA prompts the VLM to generate key points and additional attributes for affordance representation based on free-form language instructions and visual observations of the environment. Since the five tasks we tested were relatively basic and did not require decomposition into subtasks, we could directly invoke the VLM in question-answering format to address the operational challenges. This approach ensures seamless compatibility between the pipelines of PhysAgent and MOKA. Once the key points and waypoints were obtained from the VLM, MOKA converted these affordance representations into executable motions for the robotic arm. To evaluate the impact of enhanced physical-world understanding on embodied tasks, we applied two methods to MOKAs VLM: (1) fine-tuning it with PhysBench, and (2) employing PhysAgent to zero-shot assist in reasoning about affordance representations. As shown in Figure 9(c), we observe consistent improvements after fine-tuning with subset of PhysBench, indicating that the benchmarks data is of high quality and suitable for use as demonstration data in open-world robotics tasks. Additionally, PhysAgent consistently yields stable zero-shot gains across all five tasks, with significant progress observed in the force task in Figure 10(a)."
        },
        {
            "title": "5 CONCLUSION",
            "content": "In conclusion, we introduce PhysBench, benchmark designed to assess Vision-Language Models understanding of the physical world. Through experiments on 75 models, we identified significant gaps in physical world understanding, particularly in open-source models, due to inadequate training data. To address this, we developed PhysAgent, novel framework that improves physical reasoning by 18.4% on GPT-4o. Additionally, we demonstrated the utility of our dataset and approach in robotic tasks, helping to advance the understanding of the physical world in machine intelligence. Statement. We provide detailed discussion of limitations, broader impacts, ethical considerations, and reproducibility in Appendix J. 10 Published as conference paper at ICLR"
        },
        {
            "title": "ACKNOWLEDGMENTS",
            "content": "Jiageng Mao and Yue Wang acknowledge funding supports from Toyota Research Institute, Dolby, and Google DeepMind. Yue Wang is also supported by Powell Faculty Research Award."
        },
        {
            "title": "REFERENCES",
            "content": "Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, et al. Phi-3 technical report: highly capable language model locally on your phone. arXiv, 2024. Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv, 2023. 01. AI, :, Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, Kaidong Yu, Peng Liu, Qiang Liu, Shawn Yue, Senbin Yang, Shiming Yang, Tao Yu, Wen Xie, Wenhao Huang, Xiaohui Hu, Xiaoyi Ren, Xinyao Niu, Pengcheng Nie, Yuchi Xu, Yudong Liu, Yue Wang, Yuxuan Cai, Zhenyu Gu, Zhiyuan Liu, and Zonghong Dai. Yi: Open foundation models by 01.ai, 2024. Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: visual language model for few-shot learning. Advances in neural information processing systems, 35:23716 23736, 2022. Zahid Ali, Chesser Luke, and Carbone Timothy. Unsplash, 2023. Anthropic. About claude models. https://docs.anthropic.com/en/docs/ about-claude/models, 2024. Accessed: 2024-09-03. Daman Arora, Himanshu Gaurav Singh, and Mausam. Have llms advanced enough? challenging problem solving benchmark for large language models, 2023. Tayfun Ates, Samil Atesoglu, Cagatay Yigit, Ilker Kesen, Mert Kobas, Erkut Erdem, Aykut Erdem, Tilbe Goksun, and Deniz Yuret. Craft: benchmark for causal reasoning about forces and interactions. arXiv, 2020. AzureML. Phi-3.5-vision instruct (128k). https://github.com/marketplace/models/ azureml/Phi-3-5-vision-instruct, 2024. Architecture: Phi-3.5-vision has 4.2B parameters with image encoder, connector, projector, and Phi-3 Mini language model. Inputs: Text and Image (best suited for chat format). Context length: 128K tokens. GPUs: 256 A100-80G. Training time: 6 days. Training data: 500B tokens (vision + text tokens). Outputs: Generated text. Trained between July and August 2024. License: MIT. Release date: August 20, 2024. Status: Static model with offline text dataset cutoff on March 15, 2024. Jinbin Bai, Chunhui Liu, Feiyue Ni, Haofan Wang, Mengying Hu, Xiaofeng Guo, and Lele Cheng. Lat: latent translation with cycle-consistency for video-text retrieval. arXiv, 2022. Jinbin Bai, Wei Chow, Ling Yang, Xiangtai Li, Juncheng Li, Hanwang Zhang, and Shuicheng Yan. Humanedit: high-quality human-rewarded dataset for instruction-based image editing. arXiv, 2024a. Jinbin Bai, Tian Ye, Wei Chow, Enxin Song, Qing-Guo Chen, Xiangtai Li, Zhen Dong, Lei Zhu, and Shuicheng Yan. Meissonic: Revitalizing masked generative transformers for efficient highresolution text-to-image synthesis. arXiv, 2024b. Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, 11 Published as conference paper at ICLR 2025 Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. Qwen technical report. arXiv, 2023a. Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: frontier large vision-language model with versatile abilities. arXiv, 2023b. Max Bain, Arsha Nagrani, Gul Varol, and Andrew Zisserman. Frozen in time: joint video and image encoder for end-to-end retrieval. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 17281738, 2021. Hritik Bansal, Zongyu Lin, Tianyi Xie, Zeshun Zong, Michal Yarom, Yonatan Bitton, Chenfanfu Jiang, Yizhou Sun, Kai-Wei Chang, and Aditya Grover. Videophy: Evaluating physical commonsense for video generation. arXiv, 2024. Fabien Baradel, Natalia Neverova, Julien Mille, Greg Mori, and Christian Wolf. Cophy: Counterfactual learning of physical dynamics. arXiv, 2019. Daniel Bear, Elias Wang, Damian Mrowca, Felix Binder, Hsiao-Yu Fish Tung, RT Pramod, Cameron Holdaway, Sirui Tao, Kevin Smith, Fan-Yun Sun, et al. Physion: Evaluating physical prediction from vision in humans and machines. arXiv, 2021. Ali Furkan Biten, Ruben Tito, Andres Mafla, Lluis Gomez, Marcal Rusinol, Ernest Valveny, CV Jawahar, and Dimosthenis Karatzas. Scene text visual question answering. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 42914301, 2019. OC Blender. Blendera 3d modelling and rendering package. Retrieved. represents the sequence of Constructs1 to, 4, 2018. Tyler Bonnen, Stephanie Fu, Yutong Bai, Thomas OConnell, Yoni Friedman, Nancy Kanwisher, Joshua Tenenbaum, and Alexei Efros. Evaluating multiview object consistency in humans and image models. arXiv, 2024. Minwoo Byeon, Beomhee Park, Haecheon Kim, Sungjun Lee, Woonhyuk Baek, and Saehoon Kim. Coyo-700m: Image-text pair dataset, 2022. Holger Caesar, Varun Bankiti, Alex H. Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom. nuscenes: multimodal dataset for autonomous driving. arXiv, 2019. Xu Cao, Bolin Lai, Wenqian Ye, Yunsheng Ma, Joerg Heintz, Jintai Chen, Jianguo Cao, and James Rehg. What is the visual cognition gap between humans and multimodal llms? arXiv, 2024. Susan Carey. The origin of concepts. Journal of Cognition and Development, 1(1):3741, 2000. Keshigeyan Chandrasegaran, Agrim Gupta, Lea M. Hadzic, Taran Kota, Jimming He, Cristobal Eyzaguirre, Zane Durante, Manling Li, Jiajun Wu, and Fei-Fei Li. Hourvideo: 1-hour videoIn Advances in Neural Information Processing Systems, volume 37, language understanding. 2024. Angel Chang, Angela Dai, Thomas Funkhouser, Maciej Halber, Matthias Niessner, Manolis Savva, Shuran Song, Andy Zeng, and Yinda Zhang. Matterport3d: Learning from rgb-d data in indoor environments. arXiv, 2017. Boyuan Chen, Zhuo Xu, Sean Kirmani, Brain Ichter, Dorsa Sadigh, Leonidas Guibas, and Fei Xia. Spatialvlm: Endowing vision-language models with spatial reasoning capabilities. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1445514465, June 2024a. Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v: Improving large multi-modal models with better captions. arXiv, 2023a. Qiguang Chen, Libo Qin, Jin Zhang, Zhi Chen, Xiao Xu, and Wanxiang Che. M3cot: novel benchmark for multi-domain multi-step multi-modal chain-of-thought, 2024b. URL https: //arxiv.org/abs/2405.16473. 12 Published as conference paper at ICLR 2025 Xi Chen, Xiao Wang, Lucas Beyer, Alexander Kolesnikov, Jialin Wu, Paul Voigtlaender, Basil Mustafa, Sebastian Goodman, Ibrahim Alabdulmohsin, Piotr Padlewski, Daniel Salz, Xi Xiong, Daniel Vlasic, Filip Pavetic, Keran Rong, Tianli Yu, Daniel Keysers, Xiaohua Zhai, and Radu Soricut. Pali-3 vision language models: Smaller, faster, stronger, 2023b. URL https: //arxiv.org/abs/2310.09199. Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, Bin Li, Ping Luo, Tong Lu, Yu Qiao, and Jifeng Dai. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. arXiv, 2023c. Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. arXiv, 2024c. Zhenfang Chen, Kexin Yi, Yunzhu Li, Mingyu Ding, Antonio Torralba, Joshua Tenenbaum, and Chuang Gan. Comphy: Compositional physical reasoning of objects and events from videos. arXiv, 2022. An-Chieh Cheng, Hongxu Yin, Yang Fu, Qiushan Guo, Ruihan Yang, Jan Kautz, Xiaolong Wang, and Sifei Liu. Spatialrgpt: Grounded spatial reasoning in vision-language models. In NeurIPS, 2024. Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. URL https: //lmsys.org/blog/2023-03-30-vicuna/. Wei Chow, Juncheng Li, Qifan Yu, Kaihang Pan, Hao Fei, Zhiqi Ge, Shuai Yang, Siliang Tang, Hanwang Zhang, and Qianru Sun. Unified generative and discriminative training for multi-modal large language models. arXiv, 2024. Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. Journal of Machine Learning Research, 25(70):153, 2024. OpenCompass Contributors. Opencompass: universal evaluation platform for foundation models. GitHub repository, 2023. Chenhang Cui, Yiyang Zhou, Xinyu Yang, Shirley Wu, Linjun Zhang, James Zou, and Huaxiu Yao. Holistic analysis of hallucination in gpt-4v(ision): Bias and interference challenges, 2023. URL https://arxiv.org/abs/2311.03287. Angela Dai, Angel Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Nießner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 58285839, 2017. Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Instructblip: Towards general-purpose visionBoyang Li, Pascale Fung, and Steven Hoi. language models with instruction tuning. Advances in Neural Information Processing Systems, 36, 2024. Abhishek Das, Satwik Kottur, Khushi Gupta, Avi Singh, Deshraj Yadav, Jose MF Moura, Devi Parikh, and Dhruv Batra. Visual dialog. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 326335, 2017. Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, et al. Molmo and pixmo: Open weights and open data for state-of-the-art multimodal models. arXiv, 2024. Mengfei Du, Binhao Wu, Zejun Li, Xuanjing Huang, and Zhongyu Wei. Embspatial-bench: Benchmarking spatial understanding for embodied tasks with large vision-language models. arXiv, 2024. 13 Published as conference paper at ICLR 2025 Jiafei Duan, Samson Yu, Soujanya Poria, Bihan Wen, and Cheston Tan. Pip: Physical interaction In European Conference on Computer prediction via mental simulation with span selection. Vision, pp. 405421. Springer, 2022. Desmond Elliott, Stella Frank, Khalil Simaan, and Lucia Specia. Multi30k: Multilingual englishgerman image descriptions. arXiv, 2016. Taoran Fang, Wei Zhou, Yifei Sun, Kaiqiao Han, Lvbin Ma, and Yang Yang. Exploring correlations of self-supervised tasks for graphs. arXiv, 2024. Taoran Fang, Tianhong Gao, Chunping Wang, Yihao Shang, Wei Chow, Lei Chen, and Yang Yang. Kaa: Kolmogorov-arnold attention for enhancing attentive graph neural networks. arXiv, 2025. Yuxin Fang, Wen Wang, Binhui Xie, Quan Sun, Ledell Wu, Xinggang Wang, Tiejun Huang, Xinlong Wang, and Yue Cao. Eva: Exploring the limits of masked visual representation learning at scale. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1935819369, 2023. Xingyu Fu, Yushi Hu, Bangzheng Li, Yu Feng, Haoyu Wang, Xudong Lin, Dan Roth, Noah Smith, Wei-Chiu Ma, and Ranjay Krishna. Blink: Multimodal large language models can see but not perceive. arXiv, 2024. Chuang Gan, Jeremy Schwartz, Seth Alter, Damian Mrowca, Martin Schrimpf, James Traer, Julian De Freitas, Jonas Kubilius, Abhishek Bhandwaldar, Nick Haber, et al. Threedworld: platform for interactive multi-modal physical simulation. arXiv, 2020. Jensen Gao, Bidipta Sarkar, Fei Xia, Ted Xiao, Jiajun Wu, Brian Ichter, Anirudha Majumdar, and Dorsa Sadigh. Physically grounded vision-language models for robotic manipulation. In International Conference on Robotics and Automation. IEEE, 2024a. Zhangwei Gao, Zhe Chen, Erfei Cui, Yiming Ren, Weiyun Wang, Jinguo Zhu, Hao Tian, Shenglong Ye, Junjun He, Xizhou Zhu, et al. Mini-internvl: flexible-transfer pocket multimodal model with 5% parameters and 90% performance. arXiv, 2024b. Zhiqi Ge, Juncheng Li, Qifan Yu, Wei Zhou, Siliang Tang, and Yueting Zhuang. Demon24: Acm mm24 demonstrative instruction following challenge. In Proceedings of the 32nd ACM International Conference on Multimedia, pp. 1142611428, 2024. James Gibson. The ecological approach to visual perception: classic edition. Psychology press, 2014. Rohit Girdhar and Deva Ramanan. Cater: diagnostic dataset for compositional actions and temporal reasoning. arXiv, 2019. Raghav Goyal, Samira Ebrahimi Kahou, Vincent Michalski, Joanna Materzynska, Susanne Westphal, Heuna Kim, Valentin Haenel, Ingo Fruend, Peter Yianilos, Moritz Mueller-Freitag, et al. The something something video database for learning and evaluating visual common sense. In Proceedings of the IEEE international conference on computer vision, pp. 58425850, 2017a. Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the in vqa matter: Elevating the role of image understanding in visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 69046913, 2017b. Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. Ego4d: Around the world in 3,000 hours of egocentric video. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1899519012, 2022. Cylingo Group. Xinyuan-vl-2b: high-performance multimodal large model. https:// huggingface.co/Cylingo/Xinyuan-VL-2B, 2024. Accessed: 2024-12-02. 14 Published as conference paper at ICLR 2025 Shuhao Gu, Jialing Zhang, Siyuan Zhou, Kevin Yu, Zhaohu Xing, Liangdong Wang, Zhou Cao, Jintao Jia, Zhuoyi Zhang, Yixuan Wang, Zhenchong Hu, Bo-Wen Zhang, Jijie Li, Dong Liang, Yingli Zhao, Yulong Ao, Yaoqi Liu, Fangxiang Feng, and Guang Liu. Infinity-mm: Scaling multimodal performance with large-scale and high-quality instruction data, 2024. URL https: //arxiv.org/abs/2410.18558. Vincent Le Guen and Nicolas Thome. Disentangling physical dynamics from unknown factors for unsupervised video prediction. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1147411484, 2020. Dingkun Guo, Yuqi Xiang, Shuqi Zhao, Xinghao Zhu, Masayoshi Tomizuka, Mingyu Ding, and Wei Zhan. Phygrasp: Generalizing robotic grasping with physics-informed large multimodal models, 2024. URL https://arxiv.org/abs/2402.16836. Agrim Gupta, Silvio Savarese, Surya Ganguli, and Li Fei-Fei. Embodied intelligence via learning and evolution. Nature communications, 12(1):5721, 2021. Danna Gurari, Qing Li, Abigale Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo, and Jeffrey Bigham. Vizwiz grand challenge: Answering visual questions from blind people. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 36083617, 2018. John Haas. history of the unity game engine. Diss. Worcester Polytechnic Institute, 2014. Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, et al. Olympiadbench: challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. arXiv, 2024. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770778, 2016. Kaiming He, Georgia Gkioxari, Piotr Dollar, and Ross Girshick. Mask r-cnn. In Proceedings of the IEEE international conference on computer vision, pp. 29612969, 2017. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. arXiv, 2020. Jinchang Hou, Chang Ao, Haihong Wu, Xiangtao Kong, Zhigang Zheng, Daijia Tang, Chengming Li, Xiping Hu, Ruifeng Xu, Shiwen Ni, and Min Yang. E-eval: comprehensive chinese k-12 education evaluation benchmark for large language models, 2024. Shengding Hu, Yuge Tu, Xu Han, Chaoqun He, Ganqu Cui, Xiang Long, Zhi Zheng, Yewei Fang, Yuxiang Huang, Weilin Zhao, et al. Minicpm: Unveiling the potential of small language models with scalable training strategies. arXiv, 2024. Wenlong Huang, Chen Wang, Ruohan Zhang, Yunzhu Li, Jiajun Wu, and Li Fei-Fei. Voxposer: Composable 3d value maps for robotic manipulation with language models. arXiv, 2023a. Xuanwen Huang, Wei Chow, Yang Wang, Ziwei Chai, Chunping Wang, Lei Chen, and Yang Yang. One graph model for cross-domain dynamic link prediction. arXiv, 2024. Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Jiayi Lei, Yao Fu, Maosong Sun, and Junxian He. C-eval: multi-level multi-discipline chinese evaluation suite for foundation models, 2023b. Drew Hudson and Christopher Manning. Gqa: new dataset for real-world visual reasoning and compositional question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 67006709, 2019. Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv, 2024. 15 Published as conference paper at ICLR 2025 Baoxiong Jia, Yixin Chen, Huangyue Yu, Yan Wang, Xuesong Niu, Tengyu Liu, Qing Li, and Siyuan Huang. Sceneverse: Scaling 3d vision-language learning for grounded scene understanding. In European Conference on Computer Vision, pp. 289310. Springer, 2025. Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv, 2023. Dongfu Jiang, Xuan He, Huaye Zeng, Cong Wei, Max W.F. Ku, Qian Liu, and Wenhu Chen. Mantis: Interleaved multi-image instruction tuning. arXiv, 2024. Peng Jin, Ryuichi Takanobu, Caiwan Zhang, Xiaochun Cao, and Li Yuan. Chat-univi: Unified visual representation empowers large language models with image and video understanding. arXiv, 2023. Justin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Li Fei-Fei, Lawrence Zitnick, and Ross Girshick. Clevr: diagnostic dataset for compositional language and elementary visual reasoning. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 29012910, 2017. Kushal Kafle and Christopher Kanan. An analysis of visual question answering algorithms. Proceedings of the IEEE international conference on computer vision, pp. 19651973, 2017. In Andrej Karpathy and Li Fei-Fei. Deep visual-semantic alignments for generating image descriptions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 31283137, 2015. Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, et al. The kinetics human action video dataset. arXiv, 2017. Alexander Khazatsky, Karl Pertsch, Suraj Nair, Ashwin Balakrishna, Sudeep Dasari, Siddharth Karamcheti, Soroush Nasiriany, Mohan Kumar Srirama, Lawrence Yunliang Chen, Kirsty Ellis, Peter David Fagan, Joey Hejna, Masha Itkina, Marion Lepert, Yecheng Jason Ma, Patrick Tree Miller, Jimmy Wu, Suneel Belkhale, Shivin Dass, Huy Ha, Arhan Jain, Abraham Lee, Youngwoon Lee, Marius Memmel, Sungjae Park, Ilija Radosavovic, Kaiyuan Wang, Albert Zhan, Kevin Black, Cheng Chi, Kyle Beltran Hatch, Shan Lin, Jingpei Lu, Jean Mercat, Abdul Rehman, Pannag Sanketi, Archit Sharma, Cody Simpson, Quan Vuong, Homer Rich Walke, Blake Wulfe, Ted Xiao, Jonathan Heewon Yang, Arefeh Yavary, Tony Z. Zhao, Christopher Agia, Rohan Baijal, Mateo Guaman Castro, Daphne Chen, Qiuyu Chen, Trinity Chung, Jaimyn Drake, Ethan Paul Foster, Jensen Gao, David Antonio Herrera, Minho Heo, Kyle Hsu, Jiaheng Hu, Donovon Jackson, Charlotte Le, Yunshuang Li, Kevin Lin, Roy Lin, Zehan Ma, Abhiram Maddukuri, Suvir Mirchandani, Daniel Morton, Tony Nguyen, Abigail ONeill, Rosario Scalise, Derick Seale, Victor Son, Stephen Tian, Emi Tran, Andrew E. Wang, Yilin Wu, Annie Xie, Jingyun Yang, Patrick Yin, Yunchu Zhang, Osbert Bastani, Glen Berseth, Jeannette Bohg, Ken Goldberg, Abhinav Gupta, Abhishek Gupta, Dinesh Jayaraman, Joseph Lim, Jitendra Malik, Roberto Martın-Martın, Subramanian Ramamoorthy, Dorsa Sadigh, Shuran Song, Jiajun Wu, Michael C. Yip, Yuke Zhu, Thomas Kollar, Sergey Levine, and Chelsea Finn. Droid: large-scale in-the-wild robot manipulation dataset. 2024. Jihyung Kil, Zheda Mai, Justin Lee, Zihe Wang, Kerrie Cheng, Lemeng Wang, Ye Liu, Arpita Chowdhury, and Wei-Lun Chao. Compbench: comparative reasoning benchmark for multimodal llms. arXiv, 2024. Carina Kill and Ohnshim Kim. Mental mechanics: How humans reason through physical world. Mental, 2020. Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Balakrishna, Suraj Nair, Rafael Rafailov, Ethan Foster, Grace Lam, Pannag Sanketi, Quan Vuong, Thomas Kollar, Benjamin Burchfiel, Russ Tedrake, Dorsa Sadigh, Sergey Levine, Percy Liang, and Chelsea Finn. Openvla: An open-source vision-language-action model. arXiv, 2024. Published as conference paper at ICLR 2025 Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 40154026, 2023. Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35:2219922213, 2022. Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners, 2023. URL https://arxiv.org/abs/2205. 11916. Eric Kolve, Roozbeh Mottaghi, Winson Han, Eli VanderBilt, Luca Weihs, Alvaro Herrasti, Matt Deitke, Kiana Ehsani, Daniel Gordon, Yuke Zhu, et al. Ai2-thor: An interactive 3d environment for visual ai. arXiv, 2017. Jonathan Krause, Justin Johnson, Ranjay Krishna, and Li Fei-Fei. hierarchical approach for generating descriptive image paragraphs. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 317325, 2017. Zihang Lai, Senthil Purushwalkam, and Abhinav Gupta. The functional correspondence problem. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 1577215781, 2021. Paul Lerner, Olivier Ferret, Camille Guinaudeau, Herve Le Borgne, Romaric Besancon, Jose Moreno, and Jesus Lovon Melgarejo. Viquae, dataset for knowledge-based visual question answering about named entities. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval, pp. 31083120, 2022. Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra. Solving quantitative reasoning problems with language models, 2022. Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. Seed-bench: Benchmarking multimodal llms with generative comprehension. arXiv, 2023a. Bohao Li, Yuying Ge, Yixiao Ge, Guangzhi Wang, Rui Wang, Ruimao Zhang, and Ying Shan. Seed-bench: Benchmarking multimodal large language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1329913308, 2024a. Boyi Li, Yue Wang, Jiageng Mao, Boris Ivanovic, Sushant Veer, Karen Leung, and Marco Pavone. Driving everywhere with large language model policy adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1494814957, 2024b. Boyi Li, Ligeng Zhu, Ran Tian, Shuhan Tan, Yuxiao Chen, Yao Lu, Yin Cui, Sushant Veer, Max Ehrlich, Jonah Philion, et al. Wolf: Captioning everything with world summarization framework. arXiv, 2024c. Feng Li, Renrui Zhang, Hao Zhang, Yuanhan Zhang, Bo Li, Wei Li, Zejun Ma, and Chunyuan Li. Llava-next-interleave: Tackling multi-image, video, and 3d in large multimodal models, 2024d. URL https://arxiv.org/abs/2407.07895. Juncheng Li, Kaihang Pan, Zhiqi Ge, Minghe Gao, Wei Ji, Wenqiao Zhang, Tat-Seng Chua, Siliang Tang, Hanwang Zhang, and Yueting Zhuang. Fine-tuning multimodal llms to follow zero-shot demonstrative instructions. In The Twelfth International Conference on Learning Representations, 2023b. Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International conference on machine learning, pp. 1973019742. PMLR, 2023c. KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao. Videochat: Chat-centric video understanding. arXiv, 2023d. 17 Published as conference paper at ICLR 2025 Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, Limin Wang, and Yu Qiao. Mvbench: comprehensive multi-modal video understanding benchmark, 2023e. Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al. Mvbench: comprehensive multi-modal video understanding benchmark. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2219522206, 2024e. Lei Li, Yuwei Yin, Shicheng Li, Liang Chen, Peiyi Wang, Shuhuai Ren, Mukai Li, Yazheng Yang, Jingjing Xu, Xu Sun, Lingpeng Kong, and Qi Liu. M3it: large-scale dataset towards multimodal multilingual instruction tuning. arXiv, 2023f. Yanwei Li, Chengyao Wang, and Jiaya Jia. Llama-vid: An image is worth 2 tokens in large language models. arXiv, 2023g. Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. Evaluating object hallucination in large vision-language models. arXiv, 2023h. Yuncheng Li, Yale Song, Liangliang Cao, Joel Tetreault, Larry Goldberg, Alejandro Jaimes, and Jiebo Luo. Tgif: new dataset and benchmark on animated gif description. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 46414650, 2016. Yunzhu Li, Jiajun Wu, Russ Tedrake, Joshua Tenenbaum, and Antonio Torralba. Learning particle dynamics for manipulating rigid bodies, deformable objects, and fluids. arXiv, 2018. Zekun Li, Xianjun Yang, Kyuri Choi, Wanrong Zhu, Ryan Hsieh, HyeonJung Kim, Jin Hyuk Lim, Sungyoung Ji, Byungju Lee, Xifeng Yan, et al. Mmsci: multimodal multi-discipline dataset for phd-level scientific comprehension. arXiv, 2024f. Bin Lin, Bin Zhu, Yang Ye, Munan Ning, Peng Jin, and Li Yuan. Video-llava: Learning united visual representation by alignment before projection. arXiv, 2023a. Ji Lin, Hongxu Yin, Wei Ping, Yao Lu, Pavlo Molchanov, Andrew Tao, Huizi Mao, Jan Kautz, Mohammad Shoeybi, and Song Han. Vila: On pre-training for visual language models, 2023b. Xiongkun Linghu, Jiangyong Huang, Xuesong Niu, Xiaojian Ma, Baoxiong Jia, and Siyuan Huang. Multi-modal situated reasoning in 3d scenes. arXiv, 2024. Fangchen Liu, Kuan Fang, Pieter Abbeel, and Sergey Levine. Moka: Open-vocabulary robotic manipulation through mark-based visual prompting. arXiv, 2024a. Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning, 2023a. Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, January 2024b. URL https:// llava-vl.github.io/blog/2024-01-30-llava-next/. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024c. Junpeng Liu, Yifan Song, Bill Yuchen Lin, Wai Lam, Graham Neubig, Yuanzhi Li, and Xiang Yue. Visualwebbench: How far have multimodal llms evolved in web page understanding and grounding?, 2024d. Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. arXiv, 2023b. Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? arXiv, 2023c. Published as conference paper at ICLR 2025 Yuanxin Liu, Shicheng Li, Yi Liu, Yuxiang Wang, Shuhuai Ren, Lei Li, Sishuo Chen, Xu Sun, and Lu Hou. Tempcompass: Do video llms really understand videos? arXiv, 2024e. Zhijian Liu, Ligeng Zhu, Baifeng Shi, Zhuoyang Zhang, Yuming Lou, Shang Yang, Haocheng Xi, Shiyi Cao, Yuxian Gu, Dacheng Li, Xiuyu Li, Yunhao Fang, Yukang Chen, Cheng-Yu Hsieh, De-An Huang, An-Chieh Cheng, Vishwesh Nath, Jinyi Hu, Sifei Liu, Ranjay Krishna, Daguang Xu, Xiaolong Wang, Pavlo Molchanov, Jan Kautz, Hongxu Yin, Song Han, and Yao Lu. Nvila: Efficient frontier visual language models, 2024f. URL https://arxiv.org/abs/2412. 04468. Haoyu Lu, Wen Liu, Bo Zhang, Bingxuan Wang, Kai Dong, Bo Liu, Jingxiang Sun, Tongzheng Ren, Zhuoshu Li, Yaofeng Sun, Chengqi Deng, Hanwei Xu, Zhenda Xie, and Chong Ruan. Deepseekvl: Towards real-world vision-language understanding, 2024a. Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. Advances in Neural Information Processing Systems, 35:25072521, 2022. Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, KaiWei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. In International Conference on Learning Representations, 2024b. Ruiyuan Lyu, Tai Wang, Jingli Lin, Shuai Yang, Xiaohan Mao, Yilun Chen, Runsen Xu, Haifeng Huang, Chenming Zhu, Dahua Lin, et al. Mmscan: multi-modal 3d scene dataset with hierarchical grounded language annotations. arXiv, 2024. Xiaojian Ma, Silong Yong, Zilong Zheng, Qing Li, Yitao Liang, Song-Chun Zhu, and Siyuan Huang. Sqa3d: Situated question answering in 3d scenes. In International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=IDJx97BC38. Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. Video-chatgpt: Towards detailed video understanding via large vision and language models. arXiv, 2023a. Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. Video-chatgpt: Towards detailed video understanding via large vision and language models. arXiv:2306.05424, 2023b. Arjun Majumdar, Anurag Ajay, Xiaohan Zhang, Pranav Putta, Sriram Yenamandra, Mikael Henaff, Sneha Silwal, Paul Mcvay, Oleksandr Maksymets, Sergio Arnaud, Karmesh Yadav, Qiyang Li, Ben Newman, Mohit Sharma, Vincent Berges, Shiqi Zhang, Pulkit Agrawal, Yonatan Bisk, Dhruv Batra, Mrinal Kalakrishnan, Franziska Meier, Chris Paxton, Sasha Sax, and Aravind Rajeswaran. In Conference on Openeqa: Embodied question answering in the era of foundation models. Computer Vision and Pattern Recognition, 2024. Karttikeya Mangalam, Raiymbek Akshulakov, and Jitendra Malik. Egoschema: diagnostic benchmark for very long-form video language understanding. Advances in Neural Information Processing Systems, 36, 2024. Jiageng Mao, Yuxi Qian, Junjie Ye, Hang Zhao, and Yue Wang. Gpt-driver: Learning to drive with gpt. arXiv, 2023a. Jiageng Mao, Junjie Ye, Yuxi Qian, Marco Pavone, and Yue Wang. language agent for autonomous driving. arXiv, 2023b. Jiageng Mao, Siheng Zhao, Siqi Song, Tianheng Shi, Junjie Ye, Mingtong Zhang, Haoran Geng, Jitendra Malik, Vitor Guizilini, and Yue Wang. Learning from massive human videos for universal humanoid pose control. arXiv, 2024. Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. Ok-vqa: visual question answering benchmark requiring external knowledge. In Conference on Computer Vision and Pattern Recognition, 2019. 19 Published as conference paper at ICLR 2025 Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. Docvqa: dataset for vqa on document images. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pp. 22002209, 2021. Michael McCloskey, Allyson Washburn, and Linda Felch. Intuitive physics: the straight-down belief and its origin. Journal of Experimental Psychology: Learning, Memory, and Cognition, 9(4):636, 1983. Fanqing Meng, Jin Wang, Chuanhao Li, Quanfeng Lu, Hao Tian, Jiaqi Liao, Xizhou Zhu, Jifeng Dai, Yu Qiao, Ping Luo, Kaipeng Zhang, and Wenqi Shao. Mmiu: Multimodal multi-image understanding for evaluating large vision-language models, 2024. URL https://arxiv. org/abs/2408.02718. Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapaswi, Ivan Laptev, and Josef Sivic. Howto100m: Learning text-video embedding by watching hundred million narrated video clips. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 26302640, 2019. Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and Anirban Chakraborty. Ocr-vqa: Visual question answering by reading text in images. In International Conference on Document Analysis and Recognition, pp. 947952. IEEE, 2019. Niklas Muennighoff, Luca Soldaini, Dirk Groeneveld, Kyle Lo, Jacob Morrison, Sewon Min, Weijia Shi, Pete Walsh, Oyvind Tafjord, Nathan Lambert, Yuling Gu, Shane Arora, Akshita Bhagia, Dustin Schwenk, David Wadden, Alexander Wettig, Binyuan Hui, Tim Dettmers, Douwe Kiela, Ali Farhadi, Noah A. Smith, Pang Wei Koh, Amanpreet Singh, and Hannaneh Hajishirzi. Olmoe: Open mixture-of-experts language models, 2024. URL https://arxiv.org/abs/2409. 02060. Soroush Nasiriany, Fei Xia, Wenhao Yu, Ted Xiao, Jacky Liang, Ishita Dasgupta, Annie Xie, Danny Driess, Ayzaan Wahid, Zhuo Xu, Quan Vuong, Tingnan Zhang, Tsang-Wei Edward Lee, KuangHuei Lee, Peng Xu, Sean Kirmani, Yuke Zhu, Andy Zeng, Karol Hausman, Nicolas Heess, Chelsea Finn, Sergey Levine, and Brian Ichter. Pivot: Iterative visual prompting elicits actionable knowledge for vlms. arXiv, 2024. Dantong Niu, Yuvan Sharma, Giscard Biamby, Jerome Quenum, Yutong Bai, Baifeng Shi, Trevor Darrell, and Roei Herzig. Llarva: Vision-action instruction tuning enhances robot learning. arXiv, 2024. Octo Model Team, Dibya Ghosh, Homer Walke, Karl Pertsch, Kevin Black, Oier Mees, Sudeep Dasari, Joey Hejna, Charles Xu, Jianlan Luo, Tobias Kreiman, You Liang Tan, Lawrence Yunliang Chen, Pannag Sanketi, Quan Vuong, Ted Xiao, Dorsa Sadigh, Chelsea Finn, and Sergey Levine. Octo: An open-source generalist robot policy. In Proceedings of Robotics: Science and Systems, Delft, Netherlands, 2024. Kaihang Pan, Siliang Tang, Juncheng Li, Zhaoyu Fan, Wei Chow, Shuicheng Yan, Tat-Seng Chua, Yueting Zhuang, and Hanwang Zhang. Auto-encoding morph-tokens for multimodal llm. arXiv, 2024. Maitreya Patel, Tejas Gokhale, Chitta Baral, and Yezhou Yang. Cripp-vqa: Counterfactual reasoning about implicit physical properties via video question answering. arXiv, 2022. Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei. Kosmos-2: Grounding multimodal large language models to the world. ArXiv, abs/2306.14824, 2023. URL https://api.semanticscholar.org/CorpusID:259262263. Charles Qi, Hao Su, Kaichun Mo, and Leonidas Guibas. Pointnet: Deep learning on point sets for 3d classification and segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 652660, 2017. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pp. 87488763. PMLR, 2021. 20 Published as conference paper at ICLR 2025 Nazneen Fatema Rajani, Rui Zhang, Yi Chern Tan, Stephan Zheng, Jeremy Weiss, Aadit Vyas, Abhijit Gupta, Caiming Xiong, Richard Socher, and Dragomir Radev. Esprit: Explaining solutions to physical reasoning tasks. arXiv, 2020. Tianhe Ren, Shilong Liu, Ailing Zeng, Jing Lin, Kunchang Li, He Cao, Jiayu Chen, Xinyu Huang, Yukang Chen, Feng Yan, et al. Grounded sam: Assembling open-world models for diverse visual tasks. arXiv, 2024. Ronan Riochet, Mario Ynocente Castro, Mathieu Bernard, Adam Lerer, Rob Fergus, Veronique Izard, and Emmanuel Dupoux. Intphys: framework and benchmark for visual intuitive physics reasoning. arXiv, 2018. Paul-Edouard Sarlin, Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabinovich. Superglue: Learning feature matching with graph neural networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 49384947, 2020. Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: cleaned, hypernymed, image alt-text dataset for automatic image captioning. In Proceedings of ACL, 2018. Oleksii Sidorov, Ronghang Hu, Marcus Rohrbach, and Amanpreet Singh. Textcaps: dataset for image captioning with reading comprehension. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part II 16, pp. 742758. Springer, 2020. Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv, 2014. Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, In Proceedings of the IEEE/CVF and Marcus Rohrbach. Towards vqa models that can read. conference on computer vision and pattern recognition, pp. 83178326, 2019. Noah Snavely, Steven M. Seitz, and Richard Szeliski. Photo tourism: Exploring photo collections in 3d. In SIGGRAPH Conference Proceedings, pp. 835846, New York, NY, USA, 2006. ACM Press. ISBN 1-59593-364-6. Sanjana Srivastava, Chengshu Li, Michael Lingelbach, Roberto Mart, Fei Xia, Kent Vainio, Zheng Lian, Cem Gokmen, Shyamal Buch, Karen Liu, Silvio Savarese, Hyowon Gweon, Jiajun Wu, and Li Fei-Fei. Behavior: Benchmark for everyday household activities in virtual, interactive, and ecological environments. In Conference in Robot Learning, pp. accepted, 2021. Andreas Steiner, Andre Susano Pinto, Michael Tschannen, Daniel Keysers, Xiao Wang, Yonatan Bitton, Alexey Gritsenko, Matthias Minderer, Anthony Sherbondy, Shangbang Long, Siyang Qin, Reeve Ingle, Emanuele Bugliarello, Sahar Kazemzadeh, Thomas Mesnard, Ibrahim Alabdulmohsin, Lucas Beyer, and Xiaohua Zhai. Paligemma 2: family of versatile vlms for transfer. arXiv, 2024. Alane Suhr, Mike Lewis, James Yeh, and Yoav Artzi. corpus of natural language for visual In Proceedings of the 55th Annual Meeting of the Association for Computational reasoning. Linguistics, pp. 217223, 2017. Alane Suhr, Stephanie Zhou, Ally Zhang, Iris Zhang, Huajun Bai, and Yoav Artzi. corpus for reasoning about natural language grounded in photographs. arXiv, 2018. Deborah Sulsky, Shi-Jian Zhou, and Howard Schreyer. Application of particle-in-cell method to solid mechanics. Computer physics communications, 87(1-2):236252, 1995. Ddac Surs, Sachit Menon, and Carl Vondrick. Vipergpt: Visual inference via python execution for reasoning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 1188811898, 2023. Ryota Tanaka, Kyosuke Nishida, and Sen Yoshida. Visualmrc: Machine reading comprehension on document images. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp. 1387813888, 2021. 21 Published as conference paper at ICLR 2025 Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, et al. Gemini: family of highly capable multimodal models. arXiv, 2023. Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Leonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Rame, et al. Gemma 2: Improving open language models at practical size. arXiv, 2024. Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: physics engine for model-based control. In 2012 IEEE/RSJ international conference on intelligent robots and systems, pp. 50265033. IEEE, 2012. Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha Akula, Jihan Yang, Shusheng Yang, Adithya Iyer, Xichen Pan, Austin Wang, Rob Fergus, Yann LeCun, and Saining Xie. Cambrian-1: fully open, vision-centric exploration of multimodal llms. arXiv, 2024. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv, 2023. Fish Tung, Mingyu Ding, Zhenfang Chen, Daniel M. Bear, Chuang Gan, Joshua B. Tenenbaum, Daniel L. K. Yamins, Judith Fan, and Kevin A. Smith. Physion++: Evaluating physical scene understanding that requires online inference of different physical properties. arXiv, 2023. Andong Wang, Bo Wu, Sunli Chen, Zhenfang Chen, Haotian Guan, Wei-Ning Lee, Li Erran Li, and Chuang Gan. Sok-bench: situated video reasoning benchmark with aligned open-world knowledge. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1338413394, 2024a. Chen Wang, Linxi Fan, Jiankai Sun, Ruohan Zhang, Li Fei-Fei, Danfei Xu, Yuke Zhu, and Anima Anandkumar. Mimicplay: Long-horizon imitation learning by watching human play. arXiv, 2023a. Fei Wang, Xingyu Fu, James Huang, Zekun Li, Qin Liu, Xiaogeng Liu, Mingyu Derek Ma, Nan Xu, Wenxuan Zhou, Kai Zhang, et al. Muirbench: comprehensive benchmark for robust multi-image understanding. arXiv, 2024b. Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv, 2024c. Tai Wang, Xiaohan Mao, Chenming Zhu, Runsen Xu, Ruiyuan Lyu, Peisen Li, Xiao Chen, Wenwei Zhang, Kai Chen, Tianfan Xue, Xihui Liu, Cewu Lu, Dahua Lin, and Jiangmiao Pang. Embodiedscan: holistic multi-modal 3d perception suite towards embodied ai. In Conference on Computer Vision and Pattern Recognition, 2024d. Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan Song, et al. Cogvlm: Visual expert for pretrained language models. arXiv, 2023b. Weiyun Wang, Zhe Chen, Wenhai Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Jinguo Zhu, Xizhou Zhu, Lewei Lu, Yu Qiao, and Jifeng Dai. Enhancing the reasoning ability of multimodal large language models via mixed preference optimization. arXiv, 2024e. Xiaoxuan Wang, Ziniu Hu, Pan Lu, Yanqiao Zhu, Jieyu Zhang, Satyen Subramaniam, Arjun R. Loomba, Shichang Zhang, Yizhou Sun, and Wei Wang. Scibench: Evaluating college-level scientific problem-solving abilities of large language models, 2024f. Xingrui Wang, Wufei Ma, Angtian Wang, Shuo Chen, Adam Kortylewski, and Alan Yuille. Compositional 4d dynamic scenes understanding with physics priors for video question answering. arXiv, 2024g. Published as conference paper at ICLR 2025 Yi Ru Wang, Jiafei Duan, Dieter Fox, and Siddhartha Srinivasa. Newton: Are large language models capable of physical reasoning? arXiv, 2023c. Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew Dai, and Quoc Le. Finetuned language models are zero-shot learners. arXiv, 2021. Johannes Welbl, Nelson F. Liu, and Matt Gardner. Crowdsourcing multiple choice science quesIn Leon Derczynski, Wei Xu, Alan Ritter, and Tim Baldwin (eds.), Proceedings of tions. the 3rd Workshop on Noisy User-generated Text, pp. 94106, Copenhagen, Denmark, Septemdoi: 10.18653/v1/W17-4413. URL ber 2017. Association for Computational Linguistics. https://aclanthology.org/W17-4413. Bo Wu, Shoubin Yu, Zhenfang Chen, Joshua Tenenbaum, and Chuang Gan. Star: benchmark for situated reasoning in real-world videos. arXiv, 2024a. Haoning Wu, Zicheng Zhang, Erli Zhang, Chaofeng Chen, Liang Liao, Annan Wang, Chunyi Li, Wenxiu Sun, Qiong Yan, Guangtao Zhai, et al. Q-bench: benchmark for general-purpose foundation models on low-level vision. arXiv, 2023a. Haoning Wu, Dongxu Li, Bei Chen, and Junnan Li. Longvideobench: benchmark for long-context interleaved video-language understanding. arXiv, 2024b. Jiajun Wu, Joseph Lim, Hongyi Zhang, Joshua Tenenbaum, and William Freeman. Physics In BMVC, volume 2, pp. 7, 101: Learning physical object properties from unlabeled videos. 2016. Qiucheng Wu, Handong Zhao, Michael Saxon, Trung Bui, William Yang Wang, Yang Zhang, and Shiyu Chang. Vsp: Assessing the dual challenges of perception and reasoning in spatial planning tasks for vlms. arXiv, 2024c. Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-Seng Chua. Next-gpt: Any-to-any multimodal llm. ArXiv, abs/2309.05519, 2023b. URL https://api.semanticscholar.org/ CorpusID:261696650. Weijia Wu, Yuzhong Zhao, Zhuang Li, Jiahong Li, Hong Zhou, Mike Zheng Shou, and Xiang Bai. large cross-modal video retrieval dataset with reading comprehension. arXiv, 2023c. Yifan Wu, Pengchuan Zhang, Wenhan Xiong, Barlas Oguz, James C. Gee, and Yixin Nie. The role of chain-of-thought in complex vision-language reasoning task, 2023d. URL https:// arxiv.org/abs/2311.09193. Yuxin Wu, Alexander Kirillov, Francisco Massa, Wan-Yen Lo, and Ross Girshick. Detectron2. https://github.com/facebookresearch/detectron2, 2019. Junbin Xiao, Xindi Shang, Angela Yao, and Tat-Seng Chua. Next-qa: Next phase of questionanswering to explaining temporal actions. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 97779786, 2021a. Junbin Xiao, Xindi Shang, Angela Yao, and Tat-Seng Chua. Next-qa: Next phase of questionanswering to explaining temporal actions. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 97779786, 2021b. Binzhu Xie, Sicheng Zhang, Zitang Zhou, Bo Li, Yuanhan Zhang, Jack Hessel, Jingkang Yang, and Ziwei Liu. Funqa: Towards surprising video comprehension. arXiv, 2023. Dejing Xu, Zhou Zhao, Jun Xiao, Fei Wu, Hanwang Zhang, Xiangnan He, and Yueting Zhuang. Video question answering via gradually refined attention over appearance and motion. In Proceedings of the 25th ACM international conference on Multimedia, pp. 16451653, 2017. Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: large video description dataset for bridging In Proceedings of the IEEE conference on computer vision and pattern video and language. recognition, pp. 52885296, 2016. 23 Published as conference paper at ICLR 2025 Lin Xu, Yilin Zhao, Daquan Zhou, Zhijie Lin, See Kiong Ng, and Jiashi Feng. Pllava : Parameterfree llava extension from images to videos for video dense captioning, 2024. Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, and Cordelia Schmid. Just ask: Learning to answer questions from millions of narrated videos. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 16861697, 2021. Jihan Yang, Shusheng Yang, Anjali Gupta, Rilyn Han, Li Fei-Fei, and Saining Xie. Thinking in space: How multimodal large language models see, remember, and recall spaces. arXiv, 2024a. Lihe Yang, Bingyi Kang, Zilong Huang, Zhen Zhao, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything v2. arXiv:2406.09414, 2024b. Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He, Qianyu Chen, Huarong Zhou, Zhensheng Zou, Haoye Zhang, Shengding Hu, Zhi Zheng, Jie Zhou, Jie Cai, Xu Han, Guoyang Zeng, Dahai Li, Zhiyuan Liu, and Maosong Sun. Minicpm-v: gpt-4v level mllm on your phone. arXiv, 2024. URL https://arxiv. org/abs/2408.01800. Jiabo Ye, Haiyang Xu, Haowei Liu, Anwen Hu, Ming Yan, Qi Qian, Ji Zhang, Fei Huang, and Jingren Zhou. mplug-owl3: Towards long image-sequence understanding in multi-modal large language models, 2024. URL https://arxiv.org/abs/2408.04840. Kexin Yi, Chuang Gan, Yunzhu Li, Pushmeet Kohli, Jiajun Wu, Antonio Torralba, and Joshua Tenenbaum. Clevrer: Collision events for video representation and reasoning. arXiv, 2019. Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, and Enhong Chen. survey on multimodal large language models. arXiv, 2023. Kaining Ying, Fanqing Meng, Jin Wang, Zhiqian Li, Han Lin, Yue Yang, Hao Zhang, Wenbo Zhang, Yuqi Lin, Shuo Liu, Jiayi Lei, Quanfeng Lu, Runjian Chen, Peng Xu, Renrui Zhang, Haozhe Zhang, Peng Gao, Yali Wang, Yu Qiao, Ping Luo, Kaipeng Zhang, and Wenqi Shao. Mmt-bench: comprehensive multimodal benchmark for evaluating large vision-language models towards multitask agi, 2024. Qifan Yu, Juncheng Li, Yu Wu, Siliang Tang, Wei Ji, and Yueting Zhuang. Visually-prompted language model for fine-grained scene graph generation in an open world. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 2156021571, 2023a. Qifan Yu, Wei Chow, Zhongqi Yue, Kaihang Pan, Yang Wu, Xiaoyang Wan, Juncheng Li, Siliang Tang, Hanwang Zhang, and Yueting Zhuang. Anyedit: Mastering unified high-quality image editing for any idea. arXiv, 2024a. Qifan Yu, Juncheng Li, Longhui Wei, Liang Pang, Wentao Ye, Bosheng Qin, Siliang Tang, Qi Tian, and Yueting Zhuang. Hallucidoctor: Mitigating hallucinatory toxicity in visual instruction data. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024b. Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities. arXiv, 2023b. Zhou Yu, Dejing Xu, Jun Yu, Ting Yu, Zhou Zhao, Yueting Zhuang, and Dacheng Tao. Activitynetqa: dataset for understanding complex web videos via question answering. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pp. 91279134, 2019. Shenghai Yuan, Jinfa Huang, Yongqi Xu, Yaoyang Liu, Shaofeng Zhang, Yujun Shi, Ruijie Zhu, Xinhua Cheng, Jiebo Luo, and Li Yuan. Chronomagic-bench: benchmark for metamorphic evaluation of text-to-time-lapse video generation. arXiv, 2024. Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. Mmmu: massive multi-discipline multimodal understanding and reasoning In Proceedings of the IEEE/CVF Conference on Computer Vision benchmark for expert agi. and Pattern Recognition, 2024. 24 Published as conference paper at ICLR 2025 Kevin Zakka, Yuval Tassa, and MuJoCo Menagerie Contributors. MuJoCo Menagerie: collection of high-quality simulation models for MuJoCo, 2022. URL http://github.com/ google-deepmind/mujoco_menagerie. Michał Zawalski, William Chen, Karl Pertsch, Oier Mees, Chelsea Finn, and Sergey Levine. Robotic control via embodied chain-of-thought reasoning. arXiv, 2024a. Michał Zawalski, William Chen, Karl Pertsch, Oier Mees, Chelsea Finn, and Sergey Levine. Robotic control via embodied chain-of-thought reasoning. arXiv, 2024b. Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training, 2023. Jun Zhan, Junqi Dai, Jiasheng Ye, Yunhua Zhou, Dong Zhang, Zhigeng Liu, Xin Zhang, Ruibin Yuan, Ge Zhang, Linyang Li, Hang Yan, Jie Fu, Tao Gui, Tianxiang Sun, Yugang Jiang, and Xipeng Qiu. Anygpt: Unified multimodal llm with discrete sequence modeling. ArXiv, abs/2402.12226, 2024. URL https://api.semanticscholar.org/ CorpusID:267750101. Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audio-visual language model for video understanding. arXiv, 2023. URL https://arxiv.org/abs/2306. 02858. Jieyu Zhang, Weikai Huang, Zixian Ma, Oscar Michel, Dong He, Tanmay Gupta, Wei-Chiu Ma, Ali Farhadi, Aniruddha Kembhavi, and Ranjay Krishna. Task me anything. arXiv, 2024a. Yuanhan Zhang, Bo Li, haotian Liu, Yong jae Lee, Liangke Gui, Di Fu, Jiashi Feng, Ziwei Liu, and Chunyuan Li. Llava-next: strong zero-shot video understanding model, April 2024b. URL https://llava-vl.github.io/blog/2024-04-30-llava-next-video/. Haoyu Zhen, Xiaowen Qiu, Peihao Chen, Jincheng Yang, Xin Yan, Yilun Du, Yining Hong, and Chuang Gan. 3d-vla: 3d vision-language-action generative world model. arXiv, 2024. Chujie Zheng, Zhenru Zhang, Beichen Zhang, Runji Lin, Keming Lu, Bowen Yu, Dayiheng Liu, Jingren Zhou, and Junyang Lin. Processbench: Identifying process errors in mathematical reasoning, 2024a. URL https://arxiv.org/abs/2412.06559. Zhicheng Zheng, Xin Yan, Zhenfang Chen, Jingzhou Wang, Qin Zhi Eddie Lim, Joshua Tenenbaum, and Chuang Gan. Contphy: Continuum physical concept learning and reasoning from videos. In International Conference on Machine Learning. PMLR, 2024b. Junjie Zhou, Yan Shu, Bo Zhao, Boya Wu, Shitao Xiao, Xi Yang, Yongping Xiong, Bo Zhang, Tiejun Huang, and Zheng Liu. Mlvu: comprehensive benchmark for multi-task long video understanding. arXiv, 2024. Luowei Zhou, Chenliang Xu, and Jason Corso. Towards automatic learning of procedures from web instructional videos. In Proceedings of the AAAI Conference on Artificial Intelligence, 2018. Shuyan Zhou, Frank Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Yonatan Bisk, Daniel Fried, Uri Alon, et al. Webarena: realistic web environment for building autonomous agents. arXiv, 2023. Wanrong Zhu, Jack Hessel, Anas Awadalla, Samir Yitzhak Gadre, Jesse Dodge, Alex Fang, Youngjae Yu, Ludwig Schmidt, William Yang Wang, and Yejin Choi. Multimodal c4: An open, billionscale corpus of images interleaved with text. Advances in Neural Information Processing Systems, 36, 2024. 25 Published as conference paper at ICLR"
        },
        {
            "title": "CONTENTS",
            "content": "A Detailed Dataset Collection Process A.1 Simulation . A.2 Web . . . . A.3 Real-world . . . . . . . . . . . . . . . . . . . . . . . . . Data Annotation Protocol B.1 General Guidelines . . . . B.2 Data Format and Structure . . . . . . . . . . . . . . . . B.3 Quality Control and Validation . B.4 Handling Ambiguities . B.5 Ethical Considerations . . . . . . . . . . . . . . . . . . . . . . . . . . . B.6 Data Contamination Considerations . B.7 Annotation Platform . . . . . . . . . B.8 Benchmark Preparation and Release . . . . . . . . . . . . B.9 More Details of the Annotation Pipeline Detailed Task Description C.1 Ability Description . . . . . . . . C.2 Physical Object Property Sub-task . . . . . . . . . . . . . . . . . . C.3 Physical Object Relationships Sub-task . C.4 Physical Scene Understanding Sub-task . C.5 Physics-based Dynamics Sub-task . . . . More Dataset Analysis D.1 Global Statics . . . . . . . . . D.2 Word Statics and Word Cloud . More Details on the Setup E.1 Prompt for LLMs E.2 3D Assets . . . . . . . . . . . . E.3 Model Hyperparameters . E.3.1 Image VLM . E.3.2 Video VLM . . . E.3.3 General VLM . E.4 Prompt for VLM test . . . . . . . . . . . . . . . . . . . . . . . . . . E.5 Reference Datasets Summary . E.6 Prompt Strategies . . . . . E.7 ViperGPT Implementation . E.8 Human Performance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . More Experiments Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28 28 28 29 29 30 30 31 31 31 31 32 33 33 33 35 35 36 36 37 39 41 41 41 44 47 48 48 49 50 Published as conference paper at ICLR 2025 F.1 GroundingDINO Configuration . F.2 Effect of Visual Prompting . F.3 More PhysBench Results F.4 PhysBench-val Results . . . . . . . . . . . . . . . . . . . . . . . F.5 Embodied Tasks Detailed Description . F.6 Correlation Map . . . . . . . . . . . . F.7 Performance on Related Benchmarks . . . . . . . . More Related Works More Examples H.1 Physical Object Property Sub-task . . . . H.2 Physical Object Relationships Sub-task . H.3 Physical Scene Understanding Sub-task . H.4 Physics-based Dynamics Sub-task . . . . Error Study I.1 Detailed Statics . . . . I.2 Main Reason Analysis . I.3 Case Study . . . . . . . Discussion and Statement J.1 Limitation . . . J.2 Boarder Impact . . J.3 Ethics Statement . . . . . . . . . . . . . . . . . . . . . . J.4 Reproducibility Statement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Latest Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50 50 51 59 61 62 63 65 65 65 65 79 79 79 141 . 141 . 141 . 142 . 142 27 Published as conference paper at ICLR"
        },
        {
            "title": "A DETAILED DATASET COLLECTION PROCESS",
            "content": "A.1 SIMULATION We use (Blender, 2018) as our simulation platform. We utilized 679 objects and 470 HDR images to generate simulated videos and images. During each simulation, we concurrently save images or videos of depth, normal, and albedo, as well as the corresponding configuration files, which include the position, angle, movement ,and other properties of the object light source. Image generation. In addition to ambient lighting, we employed two point lights and one sunlight. To ensure data diversity, the positions of the camera and the arrangement of objects (ensuring no overlap and that all objects are captured by the camera) were randomized to some extent. Drawing from the object attribute annotation methods described in Newton (Wang et al., 2023c), we cleaned and re-annotated our data to develop comprehensive table of objects and their attributes. Utilizing this table, we delineate the relational semantics between different objects and the corresponding queries. Following the approach in BLINK (Fu et al., 2024), each object in the simulated images is demarcated with bounding box rather than being explicitly mentioned in the text, thereby enhancing the evaluation of the models image comprehension capabilities. Despite imposing considerable constraints on our code and meticulously annotating the 3D assets, there remains the possibility of minor object overlaps or incomplete captures of objects by the camera. To ensure that objects in the data are clearly identifiable, we employed GroundingDINO (Liu et al., 2023b) for detection. We only accept images where the labels detected by GroundingDINO match exactly in content and quantity with the generated labels. This process also provides us with the bounding boxes of objects for subsequent annotation. During the later stages of annotation, manual inspection of the images is conducted to ensure accuracy. To improve the detection success rate of GroundingDINO and reduce the probability of false detections, we set the box threshold to 0.2 and the text threshold to 0.2. Specifically, these parameter settings were obtained through grid search, with detailed results presented in Table 14. Videos with varying lighting conditions. We used only one point light source and arranged objects on plane to render shadows. The variations in lighting include three aspects: the color of the light, the position of the light source, and the intensity of the light. In terms of the light source position, the movement involves translations along the x, y, and axes. To avoid ambiguity in lateral directions (Du et al., 2024), during the dataset generation, the movement questions are typically framed in terms of moving along the line connecting two objects rather than simply asking for the direction of movement. Videos with varying camera conditions. We used the same lighting and other configurations as in the image generation process. During video recording, we randomly altered the cameras position or shooting angle to capture the videos. Fulid. We used assets from ContPhy (Zheng et al., 2024b) and Unity (Haas, 2014) to generate videos across four types: fluid, rope, cloth, and ball, with 350, 250, 200, and 200 videos respectively. The videos were then manually annotated. A.2 WEB For web data collection, we primarily use predefined topics (e.g., gases) to retrieve relevant videos or images from the internet (such as middle school physics experiments). After filtering and cleaning the data, we proceed with annotation. Additionally, we leverage large language models (LLMs) to generate suitable descriptions of physics-related concepts, which we then use to search for corresponding videos, followed by further cleaning and annotation. In addition to the network data collection process described in Section 3.2, we employ the following methods to gather data. Unsplash. We use high-quality and high-resolution images from Unsplash Ali et al. (2023). 57,859 images are downloaded, and finally we use only about 6,000 images. Published as conference paper at ICLR 2025 Manipulation. We sampled approximately 500 videos from DROID (Khazatsky et al., 2024), Ego4D (Grauman et al., 2022), and MimicPlay (Wang et al., 2023a), providing detailed annotations to generate QA pairs categorized under object-manipulation tasks. The primary focus of these questions is to determine the appropriate sequence of actions based on given instructions, which are derived from the original datasets descriptions of actions. Figure 42s both first and second examples provide an example of this task. First, we filtered the videos to select those with strong alignment between the instructions and the visuals, ensuring that the videos were clear, unambiguous, and matched the instructions well. Next, we identified 3-4 keyframes from these videos. The task involved sorting these key frames in the correct order to execute the instructions properly. Additionally, we used FunKPoint (Lai et al., 2021) to annotate the affordance (Gibson, 2014) points in individual images from the original dataset. Specific examples of these annotations can be found in Figure 42s third and fourth examples. nuScenes. We cropped and annotated videos from the nuScenes (Caesar et al., 2019) mini and test datasets, ultimately obtaining 1,356 QA pairs for spatial movement tasks. We categorized the questions into types such as left turn, straight, and right turn, and included arrows on the images to indicate the direction. The questions asked participants to identify which image they might see based on the indicated direction, as illustrated in Figure 28. Visual Prompt. In certain tasks, we utilized Visual Prompts (Fu et al., 2024), and through experimentation, we identified an alternative annotation method, detailed in Appendix F.2. For tasks using Visual Prompts, we set the image size to 1024 1024 pixels. In this scale, we standardized the Visual Prompt to red circle with 30-pixel radius and white text options with font size of 65 pixels. The positions of the options centers in the dataset are recorded in the following format: A: [ 734, 922 ], B: [ 202, 898 ], C: [ 343, 115 ], D: [ 410, 559 ] Visual Correspondences. Drawing inspiration from Fu et al. (2024); Sarlin et al. (2020), we also annotated portion of the corresponding point data using visual prompts. Specific examples can be found in Figure 24. A.3 REAL-WORLD We also collected some real-world videos and images, primarily covering sub-tasks related to light, camera, and physical dynamics such as collisions. An iPhone 13 Pro Max was used as the recording device, and all images are in RGBD format."
        },
        {
            "title": "B DATA ANNOTATION PROTOCOL",
            "content": "B.1 GENERAL GUIDELINES As previously discussed, there is significant gap in existing benchmarks, which primarily assess vision-language models (VLMs) based on descriptive tasks without adequately addressing their physical perception and reasoning abilities. To bridge this gap, our benchmark, PhysBench, is designed to provide comprehensive evaluation framework for physical perception, integrating visual understanding with the assessment of physical properties, spatial relationships, and dynamic phenomena. This approach aims to advance AI systems toward more general-purpose capabilities in real-world physical environments. Our benchmark follows the guidelines outlined below for data collection: General Principles: Annotations must be accurate, consistent, and adhere to high standard of academic rigor. It covers multiple tasks and topics to mirror real-world applications. It incorporates diverse visual contexts and physics knowledge to foster well-rounded evaluation. It offers varying levels of challenge to effectively probe and uncover the potential limitations of current models. 29 Published as conference paper at ICLR 2025 It provides robust evaluation settings for deterministic assessments. Specific Instructions: All questions must contain one or more images. All questions should be written in English. All questions should meet the college-level difficulty. Questions should not be ambiguous and must be answerable with one of the given options. Clearly categorize each question. Annotate all fields, including the question, answer options and other things that follow the format requirement. Review Process: Ensure that every annotation undergoes peer review to maintain high standards and minimize errors. Annotations such as physical properties, spatial relationships, dynamic interactions, and environmental factors are also collected, providing detailed examples that demonstrate the physical perception and reasoning capabilities of the models for further analysis and usage. B.2 DATA FORMAT AND STRUCTURE Detailed examples of annotated question examples as shown in Figure 11 are provided in the guidance to serve as reference for the annotators. JSON File Format: The structured JSON format will include fields for number, question type, question text, answer options (for multiple-choice), correct answer, question difficulty, and explanation (if there exists). Naming Conventions: Each collected sample will be stored in separate JSON file following standard naming rule: subject {Number}.json Image Files: image {QuesNum} {ImageNum}.png Figure 11: question-answer pair case in PhysBench and its JSON representation. B.3 QUALITY CONTROL AND VALIDATION secondary review team will rigorously vet annotations for quality and adherence to guidelines. Regular audits of random samples from the dataset will be conducted to ensure sustained quality and consistency. Periodic training sessions will be held to update annotators on best practices and any changes in annotation guidelines. Feedback mechanisms will be established to promptly address and rectify any identified errors or inconsistencies in the annotations. 30 Published as conference paper at ICLR 2025 B.4 HANDLING AMBIGUITIES Instances of ambiguity or unclear data should be flagged for detailed review. These instances will be collaboratively examined during team meetings to establish standardized approach for annotation. B.5 ETHICAL CONSIDERATIONS Copyright and Licensing: Adherence to copyright and licensing regulations is strictly enforced. Data from sources that prohibit copying or redistribution will be explicitly avoided. Data Privacy: Compliance with privacy laws and ethical standards in data handling is paramount. Annotators must avoid collecting questions that contain any private information. Ethical Data Usage: All data collection and usage must respect ethical guidelines. This includes avoiding biased or harmful content and ensuring that the datasets promote fairness and inclusivity. B.6 DATA CONTAMINATION CONSIDERATIONS The risk of data contamination can be mitigated by assigning annotators to carefully select questions that extend beyond straightforward queries with easily accessible answers. It is essential that tasks rely on provided videos or images for answers rather than the common knowledge of large language models. This approach is beneficial for creating benchmarks that genuinely test the models ability to comprehend and synthesize information from diverse and challenging sources. B.7 ANNOTATION PLATFORM We developed GUI-based annotation platform, as shown in Figure 12, designed to assist human experts in the data annotation process. Through this program, experts can easily view various media content, such as videos and images, and perform annotations and edits directly within an intuitive interface. The streamlined layout enhances the user experience, ensuring that experts can complete annotation tasks efficiently and accurately, thereby improving the quality and efficiency of the annotations. The purpose of this tool is to simplify the complex annotation workflow, reduce manual effort, and make the annotation process more efficient. Figure 12: Annotation Platform. B.8 BENCHMARK PREPARATION AND RELEASE For convenience, PhysBench-test consists of 10,002 entries, organized into 19 subclasses, as the test set, and 200 entries as the validation set for parameter choosing (PhysBench-val) since the answers 31 Published as conference paper at ICLR for the PhysBench will not be publicly available and are hosted similarly to Yue et al. (2024). We also present 89,998 entries for further research. The experimental results presented in this paper, unless otherwise specified, are based on the test set. Importantly, the answer labels for the remaining set will not be publicly released to prevent data leakage, and we will maintain an online evaluation platform. It should be noted that the scores we refer to for PhysBench are based on the entire test dataset, including the validation split. To ensure that each source dataset is well represented in the validation split and that the distribution of sub-task types and ability types in the validation set is similar to that of the entire dataset, we adopted the following sampling strategy: 1. Randomly sample questions to ensure that the distribution of sub-task and ability types in the validation set matches the full dataset. 2. Randomly sample the remaining questions from each source dataset based on its proportion in the entire dataset. Additionally, we conducted several quality checks to address any potential errors. B.9 MORE DETAILS OF THE ANNOTATION PIPELINE As described in Section 3.2, all questions were manually annotated by graduate students in STEM fields and subsequently refined through rigorous review process. The detailed workflow is depicted in Figure 13, where the icon denotes stages involving human participation. The annotators were divided into three groups, each comprising six individuals. During the annotation process, either GUI interface similar to Figure 12 or direct editing of JSON files was employed. The first group was responsible for video collection, the final group handled quality checks, and the intermediate group completed the remaining steps. comprehensive explanation of this process is provided below. Video Collection. Videos and images were sourced through web searches, simulations, and realworld recordings. The collection process utilized predefined simulation rules, LLM-guided queries, and other strategies to identify relevant content. Specifically, for data that could be simulated, we generated it using pre-written simulation scripts, as described in Section A. Additionally, we conducted YouTube searches for videos, leveraging captions generated by GPT to guide annotators in collecting videos. We also sought compilations on topics like interesting physical phenomena. For real-world recordings, we pre-designed scenarios and objects to capture. Given the complexity of collecting such data, we employed GPT as heuristic tools to expand the search scope and enrich the datasets diversity. Finally, annotators curated the videos by trimming them to retain only relevant segments and provided detailed physical descriptions, such as the direction of shadow movement or the causes of observed events. Question Design. In the previous step, we utilized GPT to generate annotations for videos. This approach was adopted after observing that directly inputting video and prompting GPT to generate question-answer pairs in an in-context learning format often led to suboptimal adherence to instructions. Instead of focusing on generating questions, GPT tended to explain the video content. Additionally, inputting videos directly consumed more tokens, increasing computational costs. Furthermore, annotating videos with captions was considered beneficial for subsequent research leveraging our dataset. Given the complexity of collecting such data, we employed GPT as heuristic strategy to broaden the scope of search and enhance the diversity of our dataset. GPT generated questions using an in-context learning approach, with the templates provided in Appendix E.1. Notably, all final questions were curated and verified by human annotators, with GPT serving only as reference. File Organization. We presented annotators with examples and detailed classification criteria, as outlined in Appendix and Appendix H. Annotators were then instructed to categorize tasks accordingly and structure each task in the JSON format shown in Figure 11. Quality Check. After organizing the dataset, we conduct two-stage review process to ensure its quality. The dataset undergoes human verification to confirm that the questions are relevant to the physical world, rely on all provided input information, are not solely based on common sense, and are accurately categorized with clear questions and corresponding answers. The first stage involves filtering and refining the questions, while the second stage focuses on thorough validation to ensure data accuracy and consistency. Given the difficulty of acquiring such data, where expressing specific properties often requires multiple images, we undertook five-step process, spending total of 4,000 hours on annotation. 32 Published as conference paper at ICLR Figure 13: Annotation Pipeline. Icon indicate stages involving human participation."
        },
        {
            "title": "C DETAILED TASK DESCRIPTION",
            "content": "This section primarily introduces the definitions of the various capabilities and sub-tasks. Specific examples can be found in Appendix H. As illustrated in Figure 1, humans not only accurately comprehend visual content but also draw upon knowledge to explain and reason about the scenes they observe. This is the primary goal of most VQA datasets. Building on this foundation, several studies have focused on the commonsense understanding of the geometric relationships and properties within the 3D world. However, the real physical world encompasses not only 3D geometric relationships but also includes object properties (e.g., elasticity, ductility), physical object relationships (e.g., velocity, acceleration, depth), physical scene and environmental factors understanding (e.g., temperature, camera parameters, lighting conditions), and the principles and mechanisms of physical dynamics (e.g., the sequence of collisions, energy transfer, fluid flow, simple physical and chemical reactions, optical and electromagnetic dynamics). To address this gap, we propose PhysBench, which aims to bring Vision-Language Models closer to achieving spatial intelligence (Gupta et al., 2021; Srivastava et al., 2021). C.1 ABILITY DESCRIPTION Table 4 presents the description of capability types for PhysBench. The tasks are categorized into four types: physical properties, physical object relationships, physical scene, and physics-based dynamics. Each task type corresponds to different capability types such as recognition, comparison, prediction, judgment, reasoning, and perception, with detailed descriptions provided for each. Specifically, the physical properties task type includes the recognition of object properties and comparisons between the physical properties of different objects. The physical object relationships task type distinguishes between static and dynamic relationships of objects. The physical scene understanding task type involves the prediction, judgment, reasoning, and perception of changes in environmental conditions. The physics-based dynamics task type evaluates the ability to predict, judge, reason, and perceive various physics-based dynamics or phenomena. C.2 PHYSICAL OBJECT PROPERTY SUB-TASK Table 5 describes the sub-tasks for object types in PhysBench. The object types are divided into four subcategories: number, mass, color, and attributes. Specifically, the number subcategory involves the count of certain objects or changes in their quantity; the mass subcategory focuses on approximate changes in object mass, mass estimation, or mass comparison; the color subcategory describes the color of objects or color changes; and the attributes subcategory covers object characteristics such as rigidity, fluidity, gas, hardness, malleability, elasticity, smoothness, and sharpness. Notably, our dataset imposes more rigorous evaluations on conventional attributes like mass and color. For instance, in the case of counting tasks, previous works (Kafle & Kanan, 2017; Yi et al., 2019; Chen et al., 2022) typically only require identifying the number of objects in an image. In contrast, our dataset often ties quantities to specific object attributes (e.g., How many objects of certain color are outside the plate? or How many objects are not blurred by the camera?). C.3 PHYSICAL OBJECT RELATIONSHIPS SUB-TASK Table 6 presents the descriptions of physical object relationships sub-tasks in PhysBench. The relationships types are divided into five subcategories: size, location, depth, distance, and movement. 33 Published as conference paper at ICLR 2025 Table 4: Ability Type Description for PhysBench. Task type Ability type Description Physical Object Property Physical Object Relationships Physical Scene Understanding Physicsbased Dynamics Identify Comparison Static Dynamic Prediction Judgment Reasoning Perception Prediction Judgment Reasoning Perception Identify physical property of an object. Comparison of the same physical property between different objects, or changes in specific property over time. The static spatial properties of an object. The dynamic spatial properties of an object, meaning the spatial properties are changing dynamically. Predict what might happen if certain environmental condition is changed, or what will happen next. Judge what will be different if certain environmental condition is modified. Explain the occurrence reason or condition of the environment. Understand what environment condition change has occurred and what its definition is, or determine the environmental attributes that caused the phenomenon to occur. Predict what might happen if certain object or object attribute in the video is changed, or what will happen next. Judge what will change if certain attribute is modified, or, for example, the sequence in which actions occur. Explaining the occurrence reason or condition of phenomena. Understand what phenomenon has occurred and what its definition is, or determine the physical attributes that caused the phenomenon to occur. Table 5: Sub-task Description for Physical Object Property Type in PhysBench."
        },
        {
            "title": "Color\nAttribute",
            "content": "The number of certain objects or changes in the number of objects. Approximate changes in mass, mass estimation, or mass comparisons. Color of an object or changes in color. The attributes or types of objects, such as rigid body, fluid, gas, stiffness, malleability, elasticity, smoothness, sharpness, etc. Specifically, the size subcategory relates to the dimensions of an object; the location subcategory describes the absolute and relative positions of objects, including tasks related to object localization and spatial information processing; the depth subcategory focuses on an objects depth relative to the camera or depth comparisons between different objects; the distance subcategory involves the comparison or estimation of distances between objects as well as their absolute size; and the movement subcategory addresses the analysis of movement direction, changes in speed, and changes in acceleration. Table 6: Sub-task Description for Physical Object Relationships Type in PhysBench."
        },
        {
            "title": "Motion",
            "content": "The size of the object. Positional relationships (absolute and relative), including directly or indirectly locating objects and other tasks involving spatial information. Depth of an object relative to the camera or depth comparisons between different objects. Comparison or estimation of distances between objects or their absolute sizes. Motion, velocity, acceleration and the direction of movement, changes in speed, or changes in acceleration. 34 Published as conference paper at ICLR 2025 C.4 PHYSICAL SCENE UNDERSTANDING SUB-TASK Table 7 describes the sub-tasks for physical scene understanding types in PhysBench. The physical scene understanding types are divided into four subcategories: temperature, camera, gas, and light. Specifically, the temperature subcategory involves temperature and its fluctuations, as well as dynamics caused by these changes; the camera subcategory focuses on changes in camera position and the resulting effects; the gas subcategory covers conditions of the gas environment, such as high pressure, low pressure, or vacuum states; and the light subcategory describes the color tone of the light source (warm or cool), changes in the position of the light source, light intensity, and the nature of the light source (point or surface). Table 7: Sub-task Description for Physical Scene Understanding Type in PhysBench."
        },
        {
            "title": "Light",
            "content": "The temperature and its changes, as well as phenomena caused by temperature fluctuations. The position of the camera and its changes, along with phenomena caused by shifts in camera position. The air environment encompasses conditions such as air pressure, humidity, airflow direction, and intensity. Includes the color tone of the light source (warm or cool), changes in the position of the light source, its intensity, and the nature of the light source (point or surface). C.5 PHYSICS-BASED DYNAMICS SUB-TASK Table 8 describes the sub-tasks for physics-based dynamics types in PhysBench. The physics-based dynamics types are divided into six subcategories: collision, throwing, manipulation, fluid, chemistry, and other physics-based dynamics. Specifically, the collision subcategory includes physicsbased dynamics such as friction between objects, collisions, and explosions; the throwing subcategory involves physical world dynamics and phenomena such as throwing and falling; the manipulation subcategory focuses on the manipulation of deformable objects and affordance-based sequence arrangements; the fluid subcategory covers fluid motion, shapes, and other fluid-related dynamics; the chemistry subcategory involves basic chemical reactions or other dynamics or phenomena related to chemistry in the physical world; and the other subcategory includes various other dynamics related to the physical world. Table 8: Sub-task Description for Physics-based Dynamics Type in PhysBench"
        },
        {
            "title": "Description",
            "content": "Friction between objects, collisions, explosions, and similar dynamics. Throwing, falling, and other physical world dynamics. Throwing Manipulation Manipulation of deformable objects and affordance-based se-"
        },
        {
            "title": "Others",
            "content": "quence arrangements. Fluid motion, shapes, and other fluid-related dynamics. Basic chemical reactions or other dynamics involving chemical knowledge in the physical world. Other dynamics related to the physical world. All of our questions are designed as multiple-choice, with only one correct answer among the four options. Referring to BLINK (Fu et al., 2024), the visual prompt is set as red circle with 10px size, which has been found to be the most suitable. 35 Published as conference paper at ICLR"
        },
        {
            "title": "D MORE DATASET ANALYSIS",
            "content": "D.1 GLOBAL STATICS Question Distribution. Figure 14 further elucidates the distribution of word counts, highlighting the diverse patterns of questions. The average word of the questions within PhysBench is 16.53, while the maximum number of words in question reaches 48. Figure 14: The distribution of the number of words per question in PhysBench. Questions with length greater than 48 are categorized as 47 for visualization simplicity. Option Distribution. Figure 15 further elucidates the distribution of word counts, highlighting the diverse patterns of options. The average number of words in the options within PhysBench is 4.36, while the maximum number of words in question reaches 20. It is worth noting that an option here refers to the text following choice, such as A. Point A, where the character length is 2. Figure 15: The distribution of the number of words per question in PhysBench. Options with length greater than 20 are categorized as 20 for visualization simplicity. Image Resolution. The resolution of most images falls within the 1024-2048 range, accounting for approximately 79.7% of the total images. Only four images in the dataset have resolution below 256. The minimum resolution is 183, while the maximum is 7201. Video Resolution. Similarly, the majority of videos have resolution between 1024-2048, covering about 98.6% of the total videos. Only 1.3% of videos have resolution below 1024. The highest video resolution is 1920, and the lowest is 370. Video Frames. Considering that VLMs typically extract keyframes when processing videosgenerally selecting 6-8 framesthe total number of frames in the videos doesnt need to be large. However, this doesnt imply that our videos are limited to 68 frames. The frame counts are primarily distributed in the ranges of 3045 frames and 60+ frames, accounting for 59.4% and 35.9% of the total, respectively. The distribution charts for image and video resolution, as well as video frame counts, can be seen in Figure 16. 36 Published as conference paper at ICLR 2025 Figure 16: The distribution charts for image and video resolution, as well as video frame counts. From left to right: the distribution of image resolution, the distribution of video resolution, and the distribution of video frame counts. D.2 WORD STATICS AND WORD CLOUD We created word cloud and word frequency chart for our dataset, as shown in Figure 17. It reveals significant presence of terms related to physical world perception, such as direction, camera, phenomenon, effects, relationship, and light. Additionally, we generated word cloud and word frequency chart for the LLaVA-1.5-13B training data, which includes 595K pretraining samples and 665K instruction-tuning samples, as illustrated in Figure 18. VILA-1.5-13B. In the pretraining data, CC3M (Sharma et al., 2018) and COYO (Byeon et al., 2022) 25M primarily consist of image captions, while MMC4 (Zhu et al., 2024) 25M includes webpage descriptions, none of which significantly contribute to the models understanding of the physical world. Therefore, we mainly focus on the instruction-tuning data. The LLaVA-1.5-SFT-665K instructiontuning data has already been used in LLaVA-1.5-13B, so it is not considered further. Additionally, FLAN (Wei et al., 2021) consists purely of text data and is thus also excluded from consideration. The data used for VILA-1.5-13B includes the following: Captioning: Image Paragraph Captioning (Krause et al., 2017), MSR-VTT (Xu et al., 2016), TextCaps (Sidorov et al., 2020) Reasoning: CLEVR (Johnson et al., 2017), NLVR (Suhr et al., 2017), VisualMRC (Tanaka et al., 2021) Translation: Multi30k (Elliott et al., 2016) VQA: ActivityNet-QA (Yu et al., 2019), DocVQA (Mathew et al., 2021), GQA (Hudson & Manning, 2019), iVQA (Yang et al., 2021), MSRVTT-QA (Xu et al., 2017), MSVDQA (Xu et al., 2017), OCR-VQA (Mishra et al., 2019), ST-VQA (Biten et al., 2019), ViQuAE (Lerner et al., 2022), VQAv2 (Goyal et al., 2017b), Visual Dialog (Das et al., 2017) According to the guidelines from the VILA repository, the aforementioned instruction-tuning data is all included in M3IT (Li et al., 2023f). Therefore, we use M3IT to analyze the training data for VILA-1.5. The final word frequency statistics and word cloud for the VILA-1.5-13B training data are shown in Figure 19. PLLaVA-13B. PLLaVA-13B is based on LLaVA-Next (Liu et al., 2024b), with LLaVA-Next incorporating additional data from ShareGPT-4V (Chen et al., 2023a) compared to LLaVA-1.5-13B. The main enhancement from LLaVA-Next to PLLaVA-13B is the addition of 783k instructional video-to-text tuning data, enabling LLaVA-Next to handle video input. Specifically, the training data are sourced from the dataset used in VideoChat2 (Li et al., 2023e), which includes data for various video understanding tasks, such as 27k conversation videos from VideoChat (Li et al., 2023d) and Video-ChatGPT (Maaz et al., 2023b), 80k classification task data from Kinetics (Kay et al., 2017) and SthSthV2 (Goyal et al., 2017a), 450k captioned data from Webvid (Bain et al., 2021), YouCook2 (Zhou et al., 2018), TextVR (Wu et al., 2023c), and VideoChat, 117 reasoning data points from NextQA (Xiao et al., 2021b) and CLEVRER (Yi et al., 2019), and 109k annotated questionanswering samples from Webvid, TGIF (Li et al., 2016), and Ego4D (Grauman et al., 2022). Published as conference paper at ICLR 2025 We used ShareGPT-4V and downloaded all the training data from the PLLaVA repository, creating word cloud and word frequency chart of the training data, as shown in Figure 20. Figure 17: Word Statics and Word Cloud for PhysBench. Figure 18: Word Statics and Word Cloud for LLaVA-1.5-13B Training Data. Figure 19: Word Statics and Word Cloud for VILA-1.5-13B Training Data. Figure 20: Word Statics and Word Cloud for PLLaVA-13B Training Data. 38 Published as conference paper at ICLR 2025 Observing Figure 18 19 20, we can see that the most frequent words in the training data of LLaVA1.5-13B, VILA-1.5-13B, and PLLaVA-13B are terms like description, phrase, summary, region, and similar words. Keywords such as direction appear much less frequently. We have listed the frequency of several key terms from our dataset in the training data of LLaVA-1.5-13B, VILA-1.5-13B, and PLLaVA-13B, as shown in Figure 21. Although PLLaVA-13B includes words like collides and camera, they are primarily used to describe phenomena without addressing the underlying physical mechanisms, which may explain why PLLaVA-13B shows no substantial improvement. Figure 21: The frequency of common terms in PhysBench within the training data of the LLaVA1.5-13B, VILA-1.5-13B, and PLLaVA-13B models."
        },
        {
            "title": "E MORE DETAILS ON THE SETUP",
            "content": "E.1 PROMPT FOR LLMS Video Cases Prompt. The general system prompt for LLMs to generate video or image cases is as follows: You are an assistant that communicates only in JSON and is an expert in physics. Do not write normal text. Your response should be in JSON format with the following string fields: 1. video description: description of the video. 2. question: multiple-choice question based on the videos content. 3. answer: response to the question, with one of the values being A, B, C, or D. For the same video description, you may provide multiple question-answer pairs, which should be presented in list format. Important notes: 1. The question should not contain details from the video description. 2. The relationship between each question-answer pair is one of juxtaposition, not progression. The following is conversation between the user and the assistant. Ensure that the assistants answers are as rich and informative as possible. Then we will the below sentence as the first turn chat and let the LLMs give the more cases in the latter turns. We replace the examples provided in the first turn to enable GPT to generate more diverse responses. Notably, some of the examples generated by GPT do not fully align with the data standards defined in Appendix B. We use these examples solely as references to collect more diverse data. However, as described in Section 3.2, the subsequent data annotation undergoes rigorous five-step manual process to ensure high data quality. The process of generating VQA templates is similar: questions are generated based on the captions of the input video or image. However, QA templates are more focused, targeting specific types of relationships, such as those related to spatial positioning. Published as conference paper at ICLR 2025 {role: system, content: (Contents of the color block above)} {role: user, content: give me some cases} {role: assistant, content: Video Description: ball thrown horizontally from cliff Questions: 1. What causes the ball to fall downward? A. Air resistance B. Gravitational force C. Magnetism D. Elastic collision 2. Which of the following affects how far the ball travels horizontally? A. Mass of the ball B. Initial velocity of the ball C. Color of the ball D. Shape of the ball 3. What happens if the ball is thrown with higher initial velocity? A. It travels farther horizontally B. It travels shorter horizontally C. It stops mid-air D. It moves vertically upward Answers: [B, B, A] Video Description: feather falling in vacuum chamber Questions: 1. What happens if the vacuum chamber is filled with air? A. The feather falls faster B. The feather falls at the same speed C. The feather floats D. The feather rises upward 2. What would happen to the action if it happened in strong magnetic field? A. It would fall faster B. It would float C. It would fall slower D. It would remain unaffected Answers: [A, D] }, {role: user, content: give me some cases} Caption. video or image caption system prompt is as follows: You are an outstanding video annotator and physicist. Your task is to provide detailed description of given video based on the users input video and brief description of the video. The description should include, but is not limited to, the following aspects: 1. The content of the video. 2. The principles behind the phenomena observed in the video. 3. The properties of certain objects within the video, such as estimated size, mass, color, and type. 4. Various spatial information, including relative and absolute positions, sizes, distances, and movements. Ensure that the descriptions are thorough, precise, and reflect your expertise in physics. Answer Extraction. The prompt used to instruct GPT-4o is illustrated as follows: The following sentences contain answers (one of A, B, C, D) and corresponding analysis. Your role is to find the answer. Please return only one of the four letters: A, B, C, or D. The sentences are: Published as conference paper at ICLR 2025 E.2 3D ASSETS The overview of the 3D assets is shown in Figure 22. After defining the object attribute table, we selected 678 objects and annotated their attributes accordingly. For each object, we then adjusted its size and position to ensure that multiple objects (typically 4-5 in our dataset) do not overlap and remain clearly visible. These objects were subsequently used for simulations. Figure 22: 3D Assets Sample. We have 678 objects and 470 HDR environment in total for simulation. E.3 MODEL HYPERPARAMETERS In our experiments, we conducted comparisons with some of the most recent and representative MLLMs in the following. In our experiments, we conducted comparisons with some of the most recent and representative MLLMs in the following. We divided the models into three categories: Image VLM, Video VLM, and General VLM, according to whether they support only one image input, support video input (also supports image), or support interlaced video and multiple images. Following Zhang et al. (2024a), for Image VLMs, we concatenate multiple frames from video (defaulting to 8 frames) into single image, arranged from left to right and bottom to top, before inputting it into the model. Thus, both Image VLMs and Video VLMs were tested on questionanswer pairs involving only single image or video, which represent subset of PhysBench. In contrast, General VLMs, which support interleaved image-text sequences, were tested on the full PhysBench test split. For most open-source models, we used the hyperparameter torch dtype = torch.float16. However, for some models that only support torch dtype = torch.bfloat16, we used torch dtype = torch.bfloat16 for those cases. Additionally, for other parameter settings, we generally followed the configurations provided in the original papers, their code repositories, or the examples from Hugging Face. E.3."
        },
        {
            "title": "IMAGE VLM",
            "content": "The following is description of the specific models reviewed, as well as the specific parameter configurations, can be found in Table 9. BLIP-2 (Li et al., 2023c) employs dual-stage strategy to bridge the modality gap effectively, utilizing lightweight Q-Former pre-trained on 129 million image-text pairs. In the first stage, the model initiates the learning of vision-language representations by leveraging frozen image encoder, ViT-g/14 from EVA-CLIP (Fang et al., 2023). In the subsequent stage, frozen LLM, FlanT5 (Chung et al., 2024), is employed to facilitate vision-to-language generative learning. This innovative approach enables effective zeroshot instructed image-to-text generation. The tested version is BLIP2-t5-xxl (we call it as BLIP-2 in this paper). InstructBLIP (Dai et al., 2024) is derived from the pre-trained BLIP-2 model (Li et al., 2023c), which integrates ViT-g/14 image encoder, Vicuna (Chiang et al., 2023), and QFormer that bridges these two components. During the vision-language instruction tuning process, only the Q-Former is fine-tuned, utilizing data from 13 distinct visual questionanswering datasets. The tested version is InstructBLIP-t5-xl (we call it as InstructBLIP41 Published as conference paper at ICLR 2025 t5-xl in this paper), InstructBLIP-t5-xxl (we call it as InstructBLIP-t5-xxl in this paper), InstructBLIP-vicuna-7b (we call it as InstructBLIP-7B in this paper) and InstructBLIPvicuna-13b (we call it as InstructBLIP-13B in this paper). It is worth noting that both InstructBLIP-vicuna-7b and InstructBLIP-vicuna-13b provided no responses to small part of questions. For these instances, we disregarded these questions and did not assign any scores. LLaVA-1.5 Liu et al. (2023a). LLaVA (Liu et al., 2024c) establishes connection between the visual encoder ViT-L/14 from CLIP (Radford et al., 2021) and the language decoder LLaMA (Touvron et al., 2023) using lightweight, fully connected (FC) layer. Initially, the system trains this FC layer with 595K image-text pairs, while keeping both the visual encoder and LLM static. Subsequently, LLaVA fine-tunes both the FC layer and the LLM using dataset of 158K instructional vision-language pairs. LLaVa-1.5 (Liu et al., 2023a) is an enhanced version of LLaVA, trained on additional datasets. The tested version is LLaVa-1.5-7b and LLaVa-1.5-13b (we call it as LLaVA-1.5-7B and LLaVA-1.5-13B in this paper). Qwen-VL-Chat (Bai et al., 2023b) introduces series of large-scale vision-language models (LVLMs) designed to perceive and understand both text and images. The Qwen-VL series is built upon the Qwen-LM (Bai et al., 2023a) foundation, augmented with visual capabilities through meticulously designed visual receptor, input-output interface, and three-stage training pipeline using multilingual multimodal cleaned corpus. These models excel in image description, question-answering, grounding, and text-reading by aligning image-caption-box tuples. The series includes Qwen-VL and Qwen-VL-Chat, both of which achieve state-of-the-art performance on broad range of visual-centric benchmarks and real-world dialog benchmarks, demonstrating their superiority over existing visionlanguage chatbots. The tested version is Qwen-VL-Chat, as Qwen-VL-Chat have stronger instruction fellow ability than Qwen-VL. LLaVA-NeXT (Liu et al., 2024b) is the latest iteration in the LLaVA series (Liu et al., 2024c), building upon the foundation of LLaVA-1.5 (Liu et al., 2023a). This new model enhances visual reasoning, OCR, and world knowledge capabilities. It increases input image resolution to four times more pixels, supporting resolutions up to 672x672, 336x1344, and 1344x336. LLaVA-NeXT features an improved visual instruction tuning data mixture, further enhancing its visual reasoning and OCR capabilities. Additionally, it supports better visual conversations across various scenarios and applications, demonstrating improved world knowledge and logical reasoning. Despite these enhancements, LLaVANeXT maintains the minimalist design and data efficiency of its predecessor, utilizing less than 1M visual instruction tuning samples. The tested versions are LLaVA-NeXT-mistral7b (LLaVA1.6-mistral) and LLaVA-NeXT-vicuna-7b (LLaVA1.6-vicuna), with the base models being Mistral-7b (Jiang et al., 2023) and Vicuna-7b (Chiang et al., 2023), respectively (we call it as LLaVA1.6-mistral and LLaVA1.6-vicuna in this paper). InternVL-Chat-V1-5 (Chen et al., 2024c) is an open-source multimodal large language model with enhanced visual understanding through continuous learning vision encoder, dynamic high-resolution image processing, and high-quality bilingual dataset. The tested version is InternVL-Chat-V1-5-quantable (we call it as InternVL-Chat1.5), as the GPU memory size restriction of 40G A6000. Cambrian-1 (Tong et al., 2024) is family of multimodal large language models (MLLMs) designed with vision-centric approach. This model series addresses the gap between language models and visual representation learning by thoroughly evaluating various visual representations. Cambrian-1 introduces the Spatial Vision Aggregator (SVA), dynamic and spatially-aware connector that integrates high-resolution vision features with large language models (LLMs) via cross-attention layers (Dai et al., 2024) while reducing the number of tokens. The tested version is Cambrian-1-8b (we call it as Cambrian-8B in this paper). MiniCPM-V (Yao et al., 2024). MiniCPM-V2 is robust multimodal large language model designed for efficient end-side deployment. The model is constructed by integrating SigLip-400M (Zhai et al., 2023) and MiniCPM-2.4B (Hu et al., 2024), connected through perceiver resampler. The latest iteration, MiniCPM-V2.5, further improves upon 42 Published as conference paper at ICLR 2025 its predecessors. Built on SigLip-400M and Llama3-8B-Instruct with total of 8B parameters, MiniCPM-V2.5 demonstrates significant performance enhancements over MiniCPMV2. The most recent and advanced model in the MiniCPM-V series, MiniCPM-V2.6, is built on SigLip-400M and Qwen2-7B (Bai et al., 2023a), also with total of 8B parameters. MiniCPM-V2.6 shows substantial performance improvements over MiniCPMLlama3-V2.5 and introduces new capabilities for multi-image and video understanding. MolmoE (Deitke et al., 2024). MolmoE is family of open vision-language models developed by the Allen Institute for AI. The Molmo models are trained on PixMo, dataset of 1 million highly curated image-text pairs, and exhibit state-of-the-art performance among multimodal models of similar size, while remaining fully open-source. MolmoE-1B is multimodal Mixture-of-Experts large language model (LLM) with 1.5B active and 7.2B total parameters, based on OLMoE-1B-7B-0924 (Muennighoff et al., 2024). It closely matches the performance of GPT-4V on both academic benchmarks and human evaluations, achieving state-of-the-art performance among similarly-sized open multimodal models. Molmo 7B-O, based on OLMo-7B-1024 (a preview of the next generation of OLMo models), utilizes OpenAI CLIP as its vision backbone, performing between GPT-4V and GPT-4o on both academic benchmarks and human evaluations. Molmo 7B-D, based on Qwen2-7B and also using OpenAI CLIP as its vision backbone, performs similarly, sitting between GPT-4V and GPT-4o in both academic benchmarks and human evaluations. It powers the Molmo demo available at molmo.allenai.org. Finally, Molmo 72B, based on Qwen2-72B with OpenAI CLIP as the vision backbone, achieves the highest academic benchmark score and ranks second in human evaluations, trailing GPT-4o by only small margin. Xinyuan-VL (Group, 2024). Xinyuan-VL-2B is high-performance multimodal large model developed by the Cylingo Group for end-user applications. It is fine-tuned based on Qwen2-VL-2B (Wang et al., 2024c) and trained on over 5 million multimodal data samples, with additional training on small amount of plain text data. Aquila-VL (Gu et al., 2024). The Aquila-VL-2B model is VLM trained using the LLava framework. The Qwen2.5-1.5B (Wang et al., 2024c) model serves as the LLM, while siglip-so400m-patch14-384 is employed as the vision tower. The model was trained on our custom-built Infinity-MM dataset, which consists of approximately 40 million image-text pairs. This dataset combines open-source data collected from the internet with synthetic instruction data generated using open-source VLM models. DeepSeek-VL (Lu et al., 2024a). DeepSeek-VL is an open-source Vision-Language (VL) model designed for real-world applications involving vision and language understanding. It demonstrates broad multimodal capabilities, enabling the processing of logical diagrams, web pages, formula recognition, scientific literature, natural images, and embodied intelligence in complex scenarios. The models we tested, with sizes of 1B and 7B parameters, are respectively sourced from deepseek-ai/deepseek-vl-1.3b-chat and deepseek-ai/deepseekvl-7b-chat. PaliGemma 2 (Steiner et al., 2024) represents significant advancement in visionlanguage modeling, building upon its predecessor by incorporating the sophisticated Gemma 2 architecture. Following the approach of PaLI-3 (Chen et al., 2023b), the PaliGemma model family utilizes open-source components, specifically combining the SigLIP vision model with Gemma 2 language models (Team et al., 2024). This multimodal system effectively processes both visual and textual inputs to generate multilingual outputs. The architecture has been optimized to achieve superior fine-tuning performance across comprehensive range of vision-language tasks, including image and short video captioning, visual question answering, optical character recognition, object detection, and instance segmentation. The model variants include paligemma2-10b-ft-docci-448, paligemma2-3bft-docci-448. Claude (Anthropic, 2024) is large language model developed by Anthropic with strong focus on safety, interpretability, and alignment. It has been designed to align with human intent and values, using various methodologies like reinforcement learning from human feedback (RLHF) to ensure that the model behaves as intended. Claude integrates mechanisms to reduce harmful or biased outputs and is optimized for interactive dialogue across range of domains. In this paper, the tested version is Claude-3-opus, Claude-3-sonnet, 43 Published as conference paper at ICLR 2025 Claude-3-haiku, Claude-3.5-sonnet. Except for Claude-3-haiku, where images were used in their original size, the images for the other three models were resized to 128128 due to cost considerations. Notably, both Claude-3-sonnet and Claude-3.5-sonnet demonstrated poor adherence to instructions, often failing to provide direct answers as requested in the prompt, instead offering explanations alongside the options. Therefore, we used GPT-4o to extract the answers for scoring, with the prompt provided in Appendix E.1. E.3.2 VIDEO VLM The following is description of the specific models reviewed, as well as the specific parameter configurations, can be found in Table 10. Chat-Univi (Jin et al., 2023) is unified vision-language model capable of comprehending and engaging in conversations involving both images and videos through unified visual representation. Chat-Univi employs set of dynamic visual tokens to uniformly represent images and videos, enabling it to efficiently utilize limited number of visual tokens to capture the spatial details necessary for images and the comprehensive temporal relationships required for videos. This approach is supported by multiscale representation that allows the model to perceive both high-level semantic concepts and low-level visual details. The tested version is Chat-Univi-7B and Chat-Univi-13B. Video-LLaVA (Lin et al., 2023a)is sophisticated multi-modal framework designed to empower large language models (LLMs) with the ability to understand both visual and auditory content in videos. Unlike previous models that handle either visual or audio signals independently, Video-LLaVA integrates both to achieve comprehensive video comprehension. The tested version is Video-LLaVA-7b (we call it as Video-LLaVA in this paper). PLLaVA (Xu et al., 2024)(Pooling LLaVA) is an advanced video understanding model designed to extend image-based models to video, enabling dense video caption generation. The model employs simple yet powerful pooling module to bridge image-finetuned Vision-Language Models (Vision-LLM) into the video domain, achieving significant performance improvements in video captioning tasks. The tested versions are PLLaVA-7B and PLLaVA-13B. Notably, since the model uses num frame as parameter during loading, our approach for processing images involved duplicating the image for the num frames input. We also evaluated VideoChatGPT (Maaz et al., 2023a), Video-LLaMA (Zhang et al., 2023), and VideoChat2 (Li et al., 2024e), and observed that these models exhibit poor instruction-following capabilities. Despite repeatedly prompting them to provide options, the models consistently responded with descriptions of the video or image content rather than addressing the questions posed. As result, we did not include specific evaluation metrics for these models in our study. E.3.3 GENERAL VLM The following is description of the specific models reviewed, as well as the specific parameter configurations, can be found in Table 11, 12. LLaVA-NeXT-Interleave (Li et al., 2024d) is multimodal large language model that extends LLaVAs capabilities to handle multi-image, video, and 3D data. By using an interleaved data format, it unifies single-image and multi-modal tasks, enabling it to transfer knowledge across different scenarios. The model is trained on the M4-Instruct dataset, which includes over million samples spanning multi-image, video, and 3D tasks. The tested versions are llava-interleave-qwen-7b-hf (we call it as LLaVA-interleave) and llavainterleave-qwen-7b-dpo-hf (we call it as LLaVA-interleave-dpo in this paper). VILA-1.5 (Lin et al., 2023b) is multimodal visual language model (VLM) designed to handle both multi-image and video understanding tasks. It is pretrained on large-scale interleaved image-text data, which enhances its ability to perform tasks such as video reasoning and in-context learning. The tested versions are VILA-1.5-3B, VILA-1.5-3B-s2, VILA-1.5-8B and VILA-1.5-13B. NVILA (Liu et al., 2024f) is family of open Vision-Language Models (VLMs) optimized for efficient video and multi-image understanding. We enhance VILAs architecture 44 Published as conference paper at ICLR 2025 Model BLIP-2 Generation Setup torch dtype = torch.float16, max new tokens = 200 InstructBLIP-t5-xl torch dtype = torch.float16, max new tokens = 200 InstructBLIP-t5-xxl torch dtype = torch.float16, max new tokens = 200 InstructBLIP-7B torch dtype = torch.float16, max new tokens = 200 InstructBLIP-13B torch dtype = torch.float16, max new tokens = 200 LLaVA-1.5-7B LLaVA-1.5-13B Qwen-VL-Chat torch dtype = torch.float16, max new tokens = 200, do sample = False torch dtype = torch.float16, max new tokens = 200, do sample = False torch dtype = torch.bfloat16 LLaVA1.6-mistral torch dtype = torch.float16, max new tokens = 200, do sample = False LLaVA1.6-vicuna torch dtype = torch.float16, max new tokens = 200, do sample = False InternVL-Chat1.5 torch dtype = torch.bfloat16, max new tokens = 512, num beams = 1, do sample = False torch dtype = torch.float16, max new tokens = 512, num beams = 1, use cache = True temperature = 0, dtype = torch.float32, context = None, sampling = False, temperature = 0.1, max new token = 10 dtype = torch.float32, context = None, sampling = False, temperature = 0.1, max new token = 10 dtype = torch.float32, context = None, sampling = False, temperature = 0.1, max new token = 10 dtype = torch.float32, stop strings = stop strings=\"<endoftext>\", dtype = auto, max new tokens = 200 dtype = torch.float32, stop strings = stop strings=\"<endoftext>\", dtype = auto, max new tokens = dtype = torch.float32, stop strings = stop strings=\"<endoftext>\", dtype = auto, max new tokens = 200 dtype = torch.float32, stop strings = stop strings=\"<endoftext>\", dtype = auto, max new tokens = 200 max new tokens = 128, resize = (1024, 1024) do sample = False, temperature = 0, max new tokens = 4096 Cambrian-8B MiniCPM-V MiniCPM-V2.5 MiniCPM-V2.6 MolmoE-1B MolmoE-7B-O MolmoE-7B-D MolmoE-72B Xinyuan-VL Aquila-VL DeepSeek-VL-1B dtype = torch.bfloat16, = max new tokens do sample=False, 10, use cache=True DeepSeek-VL-7B dtype = torch.bfloat16, = max new tokens 10, do sample=False, use cache=True Claude-3-opus Claude-3-sonnet Claude-3-haiku max new tokens = 1000, temperature = 0 max new tokens = 1000, temperature = 0 max new tokens = 1000, temperature = 0 Claude-3.5-sonnet max new tokens = 1000, temperature = Table 9: Generating parameters for Image VLM. Parameters not explicitly stated indicate the use of the models default system settings. 45 Published as conference paper at ICLR 2025 Model Generation Setup Video-LLaVA Chat-Univi-7B Chat-Univi-13B PLLaVA-7B PLLaVA-13B torch dtype = torch.float16, max new tokens = 1024, temperature = 0.1, use cache = True, do sample = True temperature = 0, torch dtype = torch.float16, max new tokens = 10, num beams = 1, use cache = True, do sample = False, top = None, output scores = True, return dict in generate = True, length penalty = torch dtype = torch.float16, max new tokens = 10, temperature = 0, num beams = 1, use cache = True, do sample = False, top = None, output scores = True, return dict in generate = True, length penalty = 1 conv mode = conv eval videoqabench, max new tokens = 256, do sample = False conv mode = conv eval videoqabench, max new tokens = 256, do sample = False Table 10: Generating parameters for Video VLM. Parameters not explicitly stated indicate the use of the models default system settings. through scale-then-compress approach, increasing spatial and temporal resolutions before compressing visual tokens, enabling efficient processing of high-resolution images and long videos. Through systematic optimization across training, fine-tuning, and deployment phases, NVILA achieves comparable or superior performance to leading VLMs while reducing training costs by 4.5, fine-tuning memory by 3.4, and latency by up to 2.8. The code and models are publicly available for reproducibility. Phi-3V (Abdin et al., 2024). Phi-3 is family of open AI models developed by Microsoft, designed to be the most capable and cost-effective small language models (SLMs) available. Phi-3 models outperform other models of the same size and even those in the next size up across various language, reasoning, coding, and math benchmarks. Phi-3V is the VLM based on Phi-3. The tested version is Phi-3V-128k (we call it as Phi-3V in this paper). Phi-3.5V (AzureML, 2024). Phi-3.5V, released on November 15, 2024, is an advanced version of Phi-3V. This state-of-the-art, lightweight multimodal model is built on datasets comprising synthetic data and curated, publicly available web content, with focus on highquality, reasoning-rich data in both text and vision. As part of the Phi-3 model family, the multimodal version supports context length of 128K tokens. The model has undergone comprehensive enhancement process, including supervised fine-tuning and direct preference optimization, to ensure accurate adherence to instructions and robust safety measures. mPLUG-Owl3 (Ye et al., 2024). Heres refined version of your sentence: mPLUGOwl3 is state-of-the-art multimodal large language model designed to address the challenges of understanding long image sequences. mPLUG-Owl3 introduces Hyper Attention, method that enhances the speed of visual sequence processing in multimodal large language models by sixfold, enabling the handling of sequences up to eight times longer. Simultaneously, mPLUG-Owl3 maintains exceptional performance across singleimage, multi-image, and video tasks. The tested versions are mPLUG-Owl3-1B (mPLUGOwl3-1B-241014), mPLUG-Owl3-2B (mPLUG-Owl3-2B-241014) and mPLUG-Owl37B (mPLUG-Owl3-7B-241101). InternVL2 (Wang et al., 2024e). InternVL 2.0 is the latest iteration in the InternVL series of multimodal large language models. It includes range of instruction-tuned models, with parameter sizes ranging from 1 billion to 108 billion. In comparison to state-of-theart open-source multimodal large language models, InternVL 2.0 outperforms most opensource alternatives and demonstrates competitive performance that rivals proprietary commercial models. Its capabilities include document and chart comprehension, infographics question answering, scene text understanding and OCR tasks, scientific and mathematical problem-solving, as well as cultural understanding and integrated multimodal processing. InternVL 2.0 is trained with an 8K context window and incorporates training data consisting of long texts, multiple images, and videos. This training significantly enhances its ability to process and understand these types of inputs, surpassing the capabilities of InternVL 46 Published as conference paper at ICLR 2025 1.5 (Chen et al., 2023c). For larger model variants, we implement merge-based approach rather than sequential processing for video data to optimize GPU memory consumption, as demonstrated in Table 28. InternVL 2.5 (Gao et al., 2024b) was released on December 9, 2024, representing an advanced iteration in the multimodal large language model (MLLM) series. While maintaining the core architecture of InternVL 2.0, it introduces significant enhancements in training strategies, testing methodologies, and data quality. The model preserves the ViTMLP-LLM paradigm established by its predecessors, InternVL 1.5 and 2.0. This new version integrates newly incrementally pre-trained InternViT with various pre-trained LLMs, including InternLM 2.5 and Qwen 2.5, utilizing randomly initialized MLP projector. Consistent with previous implementations, InternVL 2.5 employs pixel unshuffle operation, reducing visual tokens to one-quarter of their original count, and adopts dynamic resolution strategy similar to InternVL 1.5, processing images in 448448 pixel tiles. significant advancement since InternVL 2.0 is the expansion of capabilities to include multi-image and video data processing. For larger model variants, we implement merge-based approach rather than sequential processing for video data to optimize GPU memory consumption, as demonstrated in Table 28. LLaVA-NeXT-Video (Zhang et al., 2024b) is multimodal large language model designed to excel in video understanding tasks through zero-shot modality transfer. Trained primarily on image data, it demonstrates impressive performance on video tasks by leveraging deep learning models with DPO training and AI feedback. The model supports various deployment scenarios, from cloud environments to edge devices, making it highly versatile. It is part of the broader LLaVA-NeXT suite, focused on advancing visual-language integration. The tested version are LLaVA-NeXT-Video-7B-Qwen and LLaVA-NeXT-Video-7BQwen-dpo, we call them as LLaVA-NV and LLaVA-NV-dpo respectively in this paper. Mantis (Jiang et al., 2024) is multimodal large language model designed for interleaved multi-image tasks. Built on the LLaMA-3 architecture, it excels in co-reference, reasoning, comparison, and temporal understanding. Mantis uses the Mantis-Instruct dataset, containing 721K examples, to train on various multi-image skills. It achieves state-of-theart performance on some interleaved benchmarks, while maintaining strong single-image performance on par with CogVLM (Wang et al., 2023b). The tested version are MantisIdefics2, Mantis-LLaVA, Mantis-siglip-llama3 and Mantis-clip-llama3. GPT (Achiam et al., 2023). GPT-4V is an advanced multimodal model that extends GPT4s capabilities with integrated vision processing, allowing it to understand and generate text based on visual inputs. GPT-4o is an optimized variant designed for better performance in language tasks while maintaining lower computational requirements. GPT-4o-mini is more lightweight version of GPT-4o, designed for deployment in resource-constrained environments while still providing strong performance in language understanding and generation tasks. The version of GPT-4V used is GPT-4-turbo. Notably, due to token length limitations, all images were resized to 512512 before being input into GPT. Our testing was conducted around mid-August 2024, using the most advanced model available at that time. The remaining models were also tested during this period. Gemini-1.5 (Team et al., 2023). Gemini-1.5-Flash and Gemini-1.5-Pro are advanced multimodal large language models, each designed with distinct capabilities for different performance needs. Gemini-1.5-Flash emphasizes fast processing and efficient memory usage, making it ideal for tasks requiring speed on devices with limited resources. Our testing of Gemini-1.5-Flash and Gemini-1.5-Pro was conducted around mid-August 2024. E.4 PROMPT FOR VLM TEST Referring to Liu et al. (2024c), during testing, we appended an end prompt to each question-answer pair (i.e., the value corresponding to the question key in the Figure 11). The end prompt is as follows: Answer with the options letter from the given choices directly. You can only answer one letter from A, B, C, or D. 47 Published as conference paper at ICLR 2025 Consistent with Zhang et al. (2024a), when asking video-related questions to Image VLMs, we prepend the prompt with the following statement: This is series of images sampled at equal intervals from the beginning to the end of video. Based on the series of images, output the best option for the question. For Video VLMs, due to their weaker instruction-following capabilities, we provided additional prompts during testing to guide the models in selecting the correct answer rather than simply describing the video content. (Despite this, many models still responded with video descriptions rather than answering the questions, as noted in Appendix E.3.2.) For Video LLMs, if the question involves only single image input, we prepend the following statement: Based on the image, output the best option for the question. You must only output the option. Add the last line in the prompt: The best choice option is: If the input involves only single video, we prepend the following statement: This is series of images sampled at equal intervals from the beginning to the end of video. Based on the series of images, answer the question. Based on the video, output the best option for the question. You must only output the option. E.5 REFERENCE DATASETS SUMMARY Our dataset was entirely annotated by humans and underwent two rounds of rigorous filtering and screening. The data sources we utilized, along with their respective uses, are detailed in Table 13. E.6 PROMPT STRATEGIES In Section 4.1, we explore several prompting strategies:Chain of Thought (CoT) (Kojima et al., 2023) with the prompt Lets think step-by-step!; Desp-CoT (Wu et al., 2023d), which begins with an image description prompt; and Pure Language Reasoning (PLR), similar to Mathvista (Lu et al., 2024b). The specific prompt content can be found in Appendix E.6. Following the settings of Kojima et al. (2022); Chen et al. (2024b), we extract the final generated answer through GPT-4omini. In this section, we provide the prompts used to implement these three strategies. Note that for Phi-3V: (1.1) The prompt for CoT (Kojima et al., 2023) is: Lets think step by step! Start by selecting the correct options letter from the given choices, then provide detailed explanation of your thought process. (1.2) The prompt for Desp-CoT (Wu et al., 2023d) is: Each image or video is followed by description, which you can refer to. Answer with the options letter from the given choices directly. (1.3) The prompt for PLR is: Each image or video is replaced by description. Answer with the options letter from the given choices directly. For GPT-4o: (2.1) The prompt for CoT (Kojima et al., 2023) is: Lets think step by step! Start by selecting the correct options letter from the given choices, then provide detailed explanation of your thought process. 48 Published as conference paper at ICLR 2025 (2.2) The prompt for Desp-CoT (Wu et al., 2023d) is: Each image or video is followed by description, which you can refer to. Answer with the options letter from the given choices directly. (2.3) The prompt for PLR is: Each image or video is replaced by description. Answer with the options letter from the given choices directly. We used the following prompts to generate the corresponding descriptions for each model (for image descriptions, video is replaced with image where applicable): Give me the description of this video. E.7 VIPERGPT IMPLEMENTATION Following the methodology described in ContPhy (Zheng et al., 2024b), since the code for its model, ContPro, has not been released, we implemented an oracle neural-symbolic model using ViperGPT (Surs et al., 2023), which we refer to as ContPhy, and evaluated it on the PhysBench. Similar to ContPhy (Zheng et al., 2024b), we decomposed the question-answering task into four main modules: video perception, physical simulation, program parser, and symbolic execution. Given raw video, the video perception module detects objects and their associated static attributes using the MASK-RCNN detector (He et al., 2017). The physical simulator takes point clouds as input and predicts object dynamics across various scenarios using dynamic prediction models (Li et al., 2018; Sulsky et al., 1995). The program parser, powered by large language model (we use GPT-4o), translates the question query into executable Python programs. Based on the detected object attributes and predicted dynamics, the symbolic executor runs the programs to derive the answer to the question. Since neuro-symbolic approaches rely on pre-trained domain-specific modules to extract objects static attributes, physical properties, and dynamic trajectories, this method is not suitable for the complex and diverse tasks in PhysBench. For instance, in ContPhy, simulating four types of tasksrope, cloth, ball, and fluidrequires pre-training three models: MASK R-CNN (He et al., 2017) based on Detectron2 (Wu et al., 2019), DPI-Net (Li et al., 2018), and Material Point Method (MPM) (Sulsky et al., 1995). Given the variety of tasks in PhysBench, where even individual tasks within certain subcategories can differ significantly, it is impractical to design and call specific module for every task. Therefore, we designed an Oracle model for the QA pairs generated through specific simulations to ensure that the tasks are similar enough to allow the use of consistent logical processing templates following Zheng et al. (2024b), while GPT-4o directly provided answers for the remaining questions without invoking additional modules or running Python code. Following the approach in ContPhy, we use ResNet-50 (He et al., 2016) as the backbone for MASK R-CNN to densely detect object locations in each frame and associate static attributes such as color and material. We use the default config from Detectron2, while the number of classes is different across scenarios. Specifically, the batch size is 16 for 8 GPUs thus each mini-batch has 2 images per GPU. We train the model for about 10k iterations, with learning rate of 0.01. For image size, we keep the original resolution. We fine-tune the network using the training set data from all simulations. For scenes involving fluid and soft-body dynamics, we adopt MPM, while DPI-Net is used for other tasks. The training data for these two modules is similar to that of MASK R-CNN, and the overall training procedure is comparable to ContPhy. After extracting objects static attributes, physical properties, and dynamic trajectories, and parsing the natural language query into an executable program, we run the program using the object states as input and produce the predicted answer. Due to the rich diversity and versatile resources of our dataset, it is challenging to ensure that templates perfectly fit all predefined modules. For example, some videos become significantly distorted when rescaled into square format, and in certain videos, fluids are difficult to extract as points due to their similar color to the environment. For the specific training parameters of MPM and DPI-Net, we have kept them consistent with ContPhy. 49 Published as conference paper at ICLR 2025 E.8 HUMAN PERFORMANCE To evaluate human performance on PhysBench, we recruited 12 graduate students in STEM fields and provided monetary compensation for their participation. Each question was assigned to all annotators. To ensure the quality of the results, we followed the methodology of MathVista (Lu et al., 2024b) by implementing qualification questions during participant recruitment. These questions tested basic knowledge of physical world concepts, and only those who answered the qualification questions correctly were deemed eligible for the study. Given the large number of questions in PhysBench, the testing was divided into 10 sessions, delivered as online questionnaires with no time constraints for completion. The average score across all participants was used as the final measure of human performance."
        },
        {
            "title": "F MORE EXPERIMENTS RESULTS",
            "content": "F.1 GROUNDINGDINO CONFIGURATION GroundingDINO has two key parameters: box threshold and text threshold. The box threshold parameter is used to filter predictions based on the confidence level of the detected bounding boxes, ensuring that only boxes with confidence scores above this threshold are retained. On the other hand, the text threshold parameter filters predictions based on their relevance to the input text prompt, retaining only those that meet or exceed this threshold. It is important to note that the filtered phrases may contain significant number of duplicates. To address this, we consider prediction successful only if the set of output phrases matches exactly with the set of input phrases, ensuring both completeness and accuracy. The detailed experimental results are presented in Table 14. F.2 EFFECT OF VISUAL PROMPTING To enhance the VLMs responsiveness to visual prompts, we assessed its sensitivity to these prompts. The images used in our tests were all 10241024 pixels. We employed two different annotation methods for depth and attribute sub-tasks, as illustrated in Figure 23, drawing on BLINK (Fu et al., 2024) for reference. For depth, the default color for circles is red. Method (a) follows the approach introduced in the BLINK paper, while the second method aligns exactly with the instances in BLINK-eval. Additionally, the prompts for both methods are largely similar to those in the BLINKeval examples. Similarly, for attributes, we experimented with two annotation methods, denoted as (c) and (d) in Figure 23. However, since the white text doesnt look very clear in the yellow checkbox in the first method, the yellow color was selected. Figure 23: Several different labeling methods. (a) (b) are two methods for labeling depth. (c) (d) are two labeling methods for attributes. Attribute also try (a)(b) The test images comprise 1000 samples, with uniform circle radius of 20 pixels and font size of 25 pixels. The detailed experimental results are presented in Table 15. For samples (a) and (b), the text prompts require selecting the location or object indicated by the prompt, while samples (c) and 50 Published as conference paper at ICLR 2025 (d) involve identifying bounding box or point. For instance, answers for (c) and (d) might be A. The object enclosed in the red box at point using both color and letter identifiers. Based on the experimental results, we find that the outcomes for (a) and (b) are similar in terms of depth and attribute, though (a) performs slightly better. For attributes, (c) and (d) yield results close to random, likely because the tested models cannot perceive color differences in the bounding boxes, thus failing to answer effectively. However, (a) and (b) significantly improve accuracy. Therefore, we ultimately decided to use the annotation method in (a). Building on the findings of the previous experiment, we further tested the impact of varying circle sizes using the annotation method, which showed the best model performance. The corresponding results are detailed in Table 16. Additionally, the text size was consistently 5 pixels larger than the center of the circle. Based on the previous experiment, we adopted the annotation method (a) for all subsequent annotations. Therefore, we used method (a) to test depth, with sample size of 1000 images. We have tested 6 scales as shown in Figure 24 and the results indicate that for LLaVA-1.5, larger sizes generally yield better results. For Phi-3V and VILA-1.5, the optimal radius is 30 pixels, although performance does not show significant relationship on either side of 30 pixels. Most models show low sensitivity to size variations, except for LLaVA-1.5-7b. To balance performance and aesthetics, we ultimately selected radius size of 30 pixels. Figure 24: Visual Prompt with Different Size. F.3 MORE PHYSBENCH RESULTS The performance of 39 models across 8 ability categories in PhysBench is presented in Table 17. Note that the ability classifications for the physical scene understanding and physics-based dynamics categories are identical, though their content differs slightly; these are combined here for clarity. Detailed descriptions of the ability classifications can be found in Appendix C.1, while the remaining 36 newly tested models are listed in Appendix K. The details for Ability Type, Physical Object Property, Physical Object Relationships, Physical Scene Understanding, and Physics-based Dynamics can be found in Table 17,Table 18,Table 19,Table 20,Table 21, respectively. Published as conference paper at ICLR 2025 Model Generation Setup LLaVA-interleave torch dtype = torch.float16, max new tokens = 10, do sample = False LLaVA-interleave-dpo torch dtype = torch.float16, max new tokens = 10, do sample = False VILA-1.5-3B VILA-1.5-3B-s2 VILA-1.5-8B VILA-1.5-13B Phi-3V Phi-3.5V LLaVA-NV LLaVA-NV-dpo Mantis-Idefics2 Mantis-LLaVA Mantis-siglip-llama Mantis-clip-llama3 torch dtype = torch.float16, max new tokens = 4000, temperature = 0.1, num beams = 1, use cache = False, do sample = False, top = None torch dtype = torch.float16, max new tokens = 4000, temperature = 0.1, num beams = 1, use cache = False, do sample = False, top = None torch dtype = torch.float16, max new tokens = 4000, temperature = 0.1, num beams = 1, use cache = False, do sample = False, top = None torch dtype = torch.float16, max new tokens = 4000, temperature = 0.1, num beams = 1, use cache = False, do sample = False, top = None temperature = 0, torch dtype = torch.float16, max new tokens = 500, do sample = False, length penalty = 1, repetition penalty = 1 (When in error analysis the max new tokens is set to 5000) torch dtype = torch.float16, max new tokens = 500, temperature = 0, do sample = False, length penalty = 1, repetition penalty = 1 (When in error analysis the max new tokens is set to 5000) torch dtype = torch.float16, max new tokens = 10 torch dtype = torch.bfloat16, max new tokens = 10 torch dtype = torch.bfloat16, max new tokens = 10, do sample = False torch dtype = torch.bfloat16, max new tokens = 1, num beams = 1, do sample = False, length penalty = 1, repetition penalty = 1 torch dtype = torch.bfloat16, max new tokens = 1, num beams = 1, do sample = False torch dtype = torch.bfloat16, max new tokens = 1, num beams = 1, do sample = False mPLUG-Owl3-1B max new tokens = 100, decode text = True mPLUG-Owl3-2B max new tokens = 100, decode text = True mPLUG-Owl3-7B max new tokens = 100, decode text = True InternVL2-1B InternVL2-2B InternVL2-4B InternVL2-8B InternVL2-26B InternVL2-40B InternVL2-76B GPT-4o GPT-4o-mini GPT-4V Gemini-1.5-flash Gemini-1.5-pro dtype = torch.bfloat16, max new tokens = 10, do sample = False, max num = 12 dtype = torch.bfloat16, max new tokens = 10, do sample = False, max num = 12 dtype = torch.bfloat16, max new tokens = 10, do sample = False, max num = 12 dtype = torch.bfloat16, max new tokens = 10, do sample = False, max num = 12 dtype = torch.bfloat16, max new tokens = 10, do sample = False, max num = dtype = torch.bfloat16, max new tokens = 10, do sample = False, max num = 1 dtype = torch.bfloat16, max new tokens = 10, do sample = False, max num = 1 max new tokens = 300, temperature = 0, seed = 42 (When in error analysis the max new tokens is set to 2000) max new tokens = 300, temperature = 0, seed = 42 max new tokens = 300, temperature = 0, seed = 42 stream = True stream = True Table 11: Generating parameters for General VLM. Parameters not explicitly stated indicate the use of the models default system settings. 52 Published as conference paper at ICLR 2025 Model Generation Setup InternVL2.5-1B InternVL2.5-2B InternVL2.5-4B InternVL2.5-8B InternVL2.5-26B InternVL2.5-38B InternVL2.5-78B NVILA-8B NVILA-15B NVILA-Lite-8B NVILA-Lite-15B dtype = torch.bfloat16, max new tokens = 10, do sample = False, max num = 12 dtype = torch.bfloat16, max new tokens = 10, do sample = False, max num = 12 dtype = torch.bfloat16, max new tokens = 10, do sample = False, max num = 12 dtype = torch.bfloat16, max new tokens = 10, do sample = False, max num = dtype = torch.bfloat16, max new tokens = 10, do sample = False, max num = 1 dtype = torch.bfloat16, max new tokens = 10, do sample = False, max num = 1 dtype = torch.bfloat16, max new tokens = 10, do sample = False, max num = 1 default default default default Table 12: Generation parameters for more models (continued from Table 11). All other parameters not specified here use the default model configurations. Table 13: Datasets Reference for our PhysBench. None indicates that the dataset does not explicitly state which license is being used. License Description of usage https://unsplash.com/license We crawled some images for annotation and labeling. Dataset Unsplash Ali et al. (2023) ContPhy Zheng et al. (2024b) ChronoMagic-Bench Yuan et al. (2024) DROID Khazatsky et al. (2024) Ego4D Grauman et al. (2022) MimicPlay Wang et al. (2023a) CC-BY 4.0 Apache 2.0 CC-BY 4.0 CC-BY 4.0 CC-BY 4.0 nuScenes Caesar et al. (2019) CC BY-NC-SA 4. Netwon Wang et al. (2023c) CC-BY 4.0 FunKPoint Lai et al. (2021) PhotoTourism Snavely et al. (2006) None None We used his code to generate some of the simulation data. As source data, we annotated some QA. As source data, we annotated some QA data belonging to physics-based dynamicsmanuplation type. As source data, we annotated some QA data belonging to physics-based dynamicsmanuplation type. As source data, we annotated some QA data belonging to physics-based dynamicsmanuplation type. As source data, we annotated some QA data belonging to spatital movement type. As reference data for us to create the attribute table for objects. As source data, we annotated some QA data belonging to physics-based dynamicsmanuplation type. As source data, we annotated some QA data belonging to physical scene understandingcamera type. Table 14: GroundingDINO Accuracy with different box threshold and text threshold. The number of samples tested is 1000. box threshold text threshold 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.1 0.015 0.015 0.01 0.005 0.005 0.005 0. 0.2 0 0.075 0.03 0.015 0.005 0 0 0.3 0 0.005 0.06 4 0 0 0 0.4 0 0.005 0.01 4 0 0 0 0.5 0 0 0.005 0 0 0 0 0.6 0 0 0 0 0 0 0 0.7 0 0 0 0 0 0 Table 15: Comparison of Visual Prompts for Depth and Attribute Annotation. Depth Attribute LLaVA-1.5-7b LLaVA-1.5-13b Phi-3V-128k VILA-1.5-8b Prompt (a) 27.3 21.2 78.2 19.9 Prompt (b) 27.2 20.2 75.8 20. Prompt (a) 31.5 41.1 41.2 27.9 Prompt (b) 29.9 38.5 40.1 26.1 Prompt (c) 25.3 26.0 24.8 25.9 Prompt(d) 25.2 25.6 25.1 24.7 53 Published as conference paper at ICLR Table 16: Effects of different visual prompt circle radius on Depth task performance. LLaVA-1.5-7b LLaVA-1.5-13b Phi-3V-128k VILA-1.5-8b 10 19.2 19.9 74.7 20.3 20 27.3 21.2 78.2 19.9 30 27.3 24.9 78.8 26.9 40 28.4 25.2 76.8 25. 50 30.9 25.8 73.1 26.7 60 31.2 27.9 73.0 22.2 Model Identify Comparison Static Dynamic Perception Prediction Judgment Reasoning InstructBLIP-t5-xl Dai et al. (2024) InstructBLIP-t5-xxl Dai et al. (2024) InstructBLIP-7B Dai et al. (2024) InstructBLIP-13B Dai et al. (2024) BLIP-2 Li et al. (2023c) LLaVA-1.5-7B Liu et al. (2023a) LLaVA-1.5-13B Liu et al. (2023a) LLaVA1.6-mistral Liu et al. (2024b) LLaVA1.6-vicuna Liu et al. (2024b) Qwen-VL-Chat Bai et al. (2023b) InternVL-Chat1.5 Chen et al. (2024c) Cambrian-8B Tong et al. (2024) Claude-3-opus Anthropic (2024) Claude-3-sonnet Anthropic (2024) Claude-3-haiku Anthropic (2024) Claude-3.5-sonnet Anthropic (2024) Video-LLaVA Lin et al. (2023a) Chat-Univi-7B Jin et al. (2023) Chat-Univi-13B Jin et al. (2023) PLLaVA-7B Xu et al. (2024) PLLaVA-13B Xu et al. (2024) LLaVA-interleave Li et al. (2024d) LLaVA-interleave-dpo Li et al. (2024d) VILA-1.5-3B Lin et al. (2023b) VILA-1.5-3B-s2 Lin et al. (2023b) VILA-1.5-8B Lin et al. (2023b) VILA-1.5-13B Lin et al. (2023b) Phi-3V Abdin et al. (2024) LLaVA-NV Zhang et al. (2024b) LLaVA-NV-dpo Zhang et al. (2024b) Mantis-Idefics2 Jiang et al. (2024) Mantis-LLaVA Jiang et al. (2024) Mantis-siglip-llama3 Jiang et al. (2024) Mantis-clip-llama3 Jiang et al. (2024) GPT-4V Achiam et al. (2023) GPT-4o Achiam et al. (2023) GPT-4o-mini Achiam et al. (2023) Gemini-1.5-flash Team et al. (2023) Gemini-1.5-pro Team et al. (2023) 48.30 57.63 25.02 39.49 55.97 47.78 52.48 36.79 52.48 51.44 64.78 28.60 53.71 47.78 53.71 61.46 47.08 25.46 6.10 47.95 50.39 59.37 61.73 38.01 38.71 39.06 50.74 55.01 47.78 47.52 55.36 59.81 55.01 52.40 65.13 71.23 74.46 74.89 72.28 Image VLM 36.38 33.21 30.98 32.46 36.01 40.30 42.16 23.13 65.49 44.96 74.81 20.52 35.82 36.75 52.05 36.19 Video VLM 35.45 18.10 12.50 31.72 36.94 25.39 28.40 19.31 25.70 30.78 31.26 32.70 23.82 30.78 23.75 43.96 18.57 32.70 29.97 34.95 34. 29.15 14.54 3.07 30.31 31.60 35.76 53.64 24.50 35.33 53.64 45.70 42.38 21.19 42.38 39.74 56.29 13.25 58.28 52.32 60.93 58.94 35.10 31.79 7.95 49.67 43.71 General VLM + Interleaved Data 37.68 37.41 28.46 29.22 29.15 32.90 34.68 31.26 32.22 31.88 32.76 32.63 31.60 37.41 45.73 36.86 43.55 45.46 44.21 42.90 32.15 29.12 28.97 39.25 36.64 29.85 45.28 41.18 29.28 32.72 34.71 44.05 64.20 42.90 51.51 63. 49.02 36.60 40.52 39.87 39.87 47.71 52.94 38.56 40.52 41.83 41.18 30.72 36.60 64.71 71.90 60.13 60.78 65.36 40.06 41.90 32.21 35.88 42.51 47.40 51.17 29.15 49.75 45.16 53.62 22.02 43.93 38.43 47.20 43.32 42.71 24.16 13.86 42.71 44.44 47.40 47.81 39.65 39.04 37.41 44.24 47.81 42.41 43.53 42.92 41.18 42.41 40.16 55.15 56.37 55.45 54.23 53.41 34.07 32.62 23.74 27.12 33.24 39.90 40.87 15.93 39.08 37.09 41.35 33.65 32.90 36.47 35.71 34.00 40.59 29.74 9.00 39.42 39. 39.07 40.84 36.15 30.97 42.48 35.94 35.06 38.76 38.35 36.76 36.28 40.84 42.75 34.99 42.75 41.25 39.05 37.92 42.74 47.58 29.91 33.62 48.15 45.01 43.02 28.21 43.30 50.14 44.16 11.68 43.30 43.02 43.87 47.01 43.59 31.05 9.69 40.46 41.60 28.52 28.77 32.99 31.71 21.23 31.97 32.35 31.63 32.01 33.25 30.43 31.46 31.33 41.43 43.99 35.04 33.76 37.08 35.83 36.28 18.28 20.18 34.29 37.38 32.65 4.97 37.13 24.25 34.39 25.65 27.83 30.22 27.73 25.15 31.16 18.29 17.10 34.74 29. 32.74 31.40 33.65 34.99 30.54 29.44 32.74 32.22 31.74 27.00 33.75 34.47 30.68 24.41 28.63 27.96 31.69 34.85 Table 17: Evaluation Results for 39 VLMs Categorized by Ability Dimensions. PhysBench includes two evaluation dimensions: ability and task. Table 3 presents results categorized by the task-type dimension. 54 Published as conference paper at ICLR"
        },
        {
            "title": "Image VLM",
            "content": "InstructBLIP-t5-xl Dai et al. (2024) InstructBLIP-t5-xxl Dai et al. (2024) InstructBLIP-7B Dai et al. (2024) InstructBLIP-13B Dai et al. (2024) BLIP-2 Li et al. (2023c) LLaVA-1.5-7B Liu et al. (2023a) LLaVA-1.5-13B Liu et al. (2023a) LLaVA1.6-mistral Liu et al. (2024b) LLaVA1.6-vicuna Liu et al. (2024b) Qwen-VL-Chat Bai et al. (2023b) InternVL-Chat1.5 Chen et al. (2024c) Cambrian-8B Tong et al. (2024) Claude-3-opus Anthropic (2024) Claude-3-sonnet Anthropic (2024) Claude-3-haiku Anthropic (2024) Claude-3.5-sonnet Anthropic (2024) 47.14 52.86 11.23 29.12 51.30 38.99 41.59 34.49 41.07 48.35 53.73 38.13 45.75 38.30 40.73 56."
        },
        {
            "title": "Video VLM",
            "content": "Video-LLaVA Lin et al. (2023a) Chat-Univi-7B Jin et al. (2023) Chat-Univi-13B Jin et al. (2023) PLLaVA-7B Xu et al. (2024) PLLaVA-13B Xu et al. (2024) 38.13 22.70 2.43 35.36 34.84 22.18 44.33 43.00 54.00 34.93 35.57 28.67 43.33 50.85 47.33 36.86 52.67 52.22 56.67 32.42 32.67 35.15 61.33 21.50 51.33 57.68 71.00 14.68 25.67 54.95 50.33 44.71 50.00 53.24 62.00 58.02 60.33 28.33 53.00 22.87 23.00 3.07 4.00 48.46 56.33 47.44 65.00 General VLM + Interleaved Data LLaVA-interleave Li et al. (2024d) LLaVA-interleave-dpo Li et al. (2024d) VILA-1.5-3B Lin et al. (2023b) VILA-1.5-3B-s2 Lin et al. (2023b) VILA-1.5-8B Lin et al. (2023b) VILA-1.5-13B Lin et al. (2023b) Phi-3V Abdin et al. (2024) LLaVA-NV Zhang et al. (2024b) LLaVA-NV-dpo Zhang et al. (2024b) Mantis-Idefics2 Jiang et al. (2024) Mantis-LLaVA Jiang et al. (2024) Mantis-siglip-llama3 Jiang et al. (2024) Mantis-clip-llama3 Jiang et al. (2024) GPT-4V Achiam et al. (2023) GPT-4o Achiam et al. (2023) GPT-4o-mini Achiam et al. (2023) Gemini-1.5-flash Team et al. (2023) Gemini-1.5-pro Team et al. (2023) 51.30 55.29 39.17 38.82 39.86 48.01 43.15 35.70 36.40 47.49 58.41 45.41 42.29 52.51 61.18 69.67 72.10 64.99 47.44 65.33 41.30 67.33 29.01 27.00 30.72 28.00 24.23 33.33 36.52 40.00 37.88 63.00 38.57 55.67 39.59 54.67 32.76 65.33 40.27 61.33 37.88 61.00 40.96 57.67 48.81 71.00 54.27 71.00 50.85 72.33 54.95 72.33 58.02 75.00 31.37 33.22 20.66 30.77 34.63 35.42 35.63 26.76 36.41 30.45 48.12 18.52 35.77 33.64 38.18 36.69 34.49 16.32 5.32 33.07 35.06 41.66 42.23 31.37 32.29 32.72 38.47 40.81 35.70 36.34 36.69 35.98 38.11 36.05 43.79 52.52 43.29 48.65 50.04 Table 18: Evaluation Results for 39 VLMs in PhysBench Physical Object 2Property Sub-task (the fifth last column of Table 3). 55 Published as conference paper at ICLR"
        },
        {
            "title": "Size Location Depth Distance Motion",
            "content": "38.72 41.35 30.92 39.10 44.74 43.23 46.24 19.55 51.88 42.11 66.17 7.89 38.72 42.11 54.14 46.24 Video-LLaVA Lin et al. (2023a) Chat-Univi-7B Jin et al. (2023) Chat-Univi-13B Jin et al. (2023) PLLaVA-7B Xu et al. (2024) PLLaVA-13B Xu et al. (2024) InstructBLIP-t5-xl Dai et al. (2024) InstructBLIP-t5-xxl Dai et al. (2024) InstructBLIP-7B Dai et al. (2024) InstructBLIP-13B Dai et al. (2024) BLIP-2 Li et al. (2023c) LLaVA-1.5-7B Liu et al. (2023a) LLaVA-1.5-13B Liu et al. (2023a) LLaVA1.6-mistral Liu et al. (2024b) LLaVA1.6-vicuna Liu et al. (2024b) Qwen-VL-Chat Bai et al. (2023b) InternVL-Chat1.5 Chen et al. (2024c) Cambrian-8B Tong et al. (2024) Claude-3-opus Anthropic (2024) Claude-3-sonnet Anthropic (2024) Claude-3-haiku Anthropic (2024) Claude-3.5-sonnet Anthropic (2024) Image VLM 37.50 48.21 33.93 26.79 58.93 64.29 57.14 26.79 39.29 33.93 75.00 14.29 62.50 73.21 64.29 71.43 Video VLM 39.29 28.57 17.86 48.21 50.00 General VLM + Interleaved Data 48.21 LLaVA-interleave Li et al. (2024d) LLaVA-interleave-dpo Li et al. (2024d) 41.07 50.00 VILA-1.5-3B Lin et al. (2023b) 57.14 VILA-1.5-3B-s2 Lin et al. (2023b) 37.50 VILA-1.5-8B Lin et al. (2023b) 64.29 VILA-1.5-13B Lin et al. (2023b) 66.07 Phi-3V Abdin et al. (2024) 32.14 LLaVA-NV Zhang et al. (2024b) 32.14 LLaVA-NV-dpo Zhang et al. (2024b) Mantis-Idefics2 Jiang et al. (2024) 39.29 37.50 Mantis-LLaVA Jiang et al. (2024) Mantis-siglip-llama3 Jiang et al. (2024) 41.07 55.36 Mantis-clip-llama3 Jiang et al. (2024) 75.00 GPT-4V Achiam et al. (2023) 85.71 GPT-4o Achiam et al. (2023) 69.64 GPT-4o-mini Achiam et al. (2023) 67.86 Gemini-1.5-flash Team et al. (2023) 66.07 Gemini-1.5-pro Team et al. (2023) 53.95 53.26 40.89 37.80 22.34 52.92 53.61 35.40 35.74 58.42 47.08 41.58 39.86 59.45 70.45 51.55 70.79 71.48 40.60 19.55 16.17 34.59 42. 30.53 27.72 26.35 25.26 29.47 35.09 32.63 24.56 72.98 45.96 76.14 31.93 31.23 28.77 47.37 25.96 27.72 17.89 6.32 28.42 28.42 69.47 62.46 45.61 37.19 29.47 60.35 74.39 49.47 53.68 65.61 31.93 27.72 33.68 61.40 73.33 57.19 66.32 82.11 41.67 51.67 20.00 43.33 46.67 41.67 45.00 10.00 51.67 38.33 58.33 11.67 46.67 31.67 51.67 36.67 43.33 25.00 11.67 41.67 38.33 60.00 63.33 46.67 36.67 40.00 53.33 48.33 45.00 45.00 65.00 46.67 38.33 43.33 58.33 83.33 63.33 71.67 68. 53.93 57.30 37.08 48.31 59.55 51.69 60.67 37.08 59.55 52.81 61.80 5.62 71.91 62.92 68.54 69.66 50.56 35.96 15.73 60.67 57.30 37.48 36.17 28.42 26.76 31.47 32.57 26.69 26.00 50.55 32.50 26.56 32.23 33.89 38.87 60.58 39.21 44.81 58.30 Table 19: Evaluation Results for 39 VLMs in PhysBench Object Relationships Sub-task (the forth last column of Table 3). 56 Published as conference paper at ICLR"
        },
        {
            "title": "Image VLM",
            "content": "InstructBLIP-t5-xl Dai et al. (2024) InstructBLIP-t5-xxl Dai et al. (2024) InstructBLIP-7B Dai et al. (2024) InstructBLIP-13B Dai et al. (2024) BLIP-2 Li et al. (2023c) LLaVA-1.5-7B Liu et al. (2023a) LLaVA-1.5-13B Liu et al. (2023a) LLaVA1.6-mistral Liu et al. (2024b) LLaVA1.6-vicuna Liu et al. (2024b) Qwen-VL-Chat Bai et al. (2023b) InternVL-Chat1.5 Chen et al. (2024c) Cambrian-8B Tong et al. (2024) Claude-3-opus Anthropic (2024) Claude-3-sonnet Anthropic (2024) Claude-3-haiku Anthropic (2024) Claude-3.5-sonnet Anthropic (2024) 70.59 80.88 36.76 57.35 79.41 61.76 50.00 45.59 57.35 66.18 72.06 11.76 72.06 54.41 73.53 75.00 Video-LLaVA Lin et al. (2023a) Chat-Univi-7B Jin et al. (2023) Chat-Univi-13B Jin et al. (2023) PLLaVA-7B Xu et al. (2024) PLLaVA-13B Xu et al. (2024)"
        },
        {
            "title": "Video VLM",
            "content": "67.65 50.00 20.59 60.29 66.18 General VLM + Interleaved Data LLaVA-interleave Li et al. (2024d) LLaVA-interleave-dpo Li et al. (2024d) VILA-1.5-3B Lin et al. (2023b) VILA-1.5-3B-s2 Lin et al. (2023b) VILA-1.5-8B Lin et al. (2023b) VILA-1.5-13B Lin et al. (2023b) Phi-3V Abdin et al. (2024) LLaVA-NV Zhang et al. (2024b) LLaVA-NV-dpo Zhang et al. (2024b) Mantis-Idefics2 Jiang et al. (2024) Mantis-LLaVA Jiang et al. (2024) Mantis-siglip-llama3 Jiang et al. (2024) Mantis-clip-llama3 Jiang et al. (2024) GPT-4V Achiam et al. (2023) GPT-4o Achiam et al. (2023) GPT-4o-mini Achiam et al. (2023) Gemini-1.5-flash Team et al. (2023) Gemini-1.5-pro Team et al. (2023) 64.71 61.76 54.41 51.47 44.12 60.29 52.94 52.94 51.47 63.24 58.82 51.47 52.94 88.24 91.18 82.35 85.29 85.29 43.26 43.36 22.94 21.61 40.90 44.58 40.06 5.66 44.67 20.55 40.43 22.05 29.59 31.39 26.86 27.80 39.40 14.80 13.29 40.81 30. 36.48 35.34 43.26 46.18 36.29 32.52 38.93 36.29 35.25 27.43 41.28 42.41 33.93 25.07 22.05 28.56 33.74 37.32 26.00 30.03 40.00 30.30 12.00 14.74 30.61 19.86 30.00 29.48 24.00 31.76 38.00 28.03 40.00 8.01 32.00 31.76 20.00 29.66 34.00 31.48 20.00 24.66 86.00 27.93 36.00 31.30 42.00 29.94 68.00 24.48 30.00 26.02 20.00 20.75 16.00 17.93 22.00 31.12 36.00 30.39 41.82 32.85 47.27 30.21 14.55 26.11 10.91 25.48 23.64 25.02 14.55 30.21 65.45 29.48 18.18 31.12 20.00 31.94 47.27 29.39 23.64 30.57 43.64 30.57 27.27 30.21 85.45 22.02 83.64 32.67 83.64 27.75 78.18 30.48 76.36 31.85 Table 20: Evaluation Results for 39 VLMs in PhysBench Physical Scene Understanding Sub-task (the third last column of Table 3). Published as conference paper at ICLR 2025 Model Collision Throwing Manipulation Fluid Chemistry Others InstructBLIP-t5-xl Dai et al. (2024) InstructBLIP-t5-xxl Dai et al. (2024) InstructBLIP-7B Dai et al. (2024) InstructBLIP-13B Dai et al. (2024) BLIP-2 Li et al. (2023c) LLaVA-1.5-7B Liu et al. (2023a) LLaVA-1.5-13B Liu et al. (2023a) LLaVA1.6-mistral Liu et al. (2024b) LLaVA1.6-vicuna Liu et al. (2024b) Qwen-VL-Chat Bai et al. (2023b) InternVL-Chat1.5 Chen et al. (2024c) Cambrian-8B Tong et al. (2024) Claude-3-opus Anthropic (2024) Claude-3-sonnet Anthropic (2024) Claude-3-haiku Anthropic (2024) Claude-3.5-sonnet Anthropic (2024) Video-LLaVA Lin et al. (2023a) Chat-Univi-7B Jin et al. (2023) Chat-Univi-13B Jin et al. (2023) PLLaVA-7B Xu et al. (2024) PLLaVA-13B Xu et al. (2024) Image VLM 34.68 31.35 26.60 15.44 35.39 37.77 34.92 11.64 35.39 33.25 39.90 39.43 28.98 30.64 39.90 37.05 Video VLM 38.48 37.77 12.59 40.86 39. 31.49 32.72 19.97 28.26 33.64 36.25 33.18 24.12 33.03 32.57 35.94 24.88 35.79 38.25 35.94 34.25 35.48 29.80 5.38 33.79 36.25 27.89 29.90 27.96 27.89 30.40 41.21 46.73 12.81 44.97 37.19 42.71 28.89 25.63 26.13 34.42 27.64 29.15 5.78 12.81 28.89 29.90 General VLM + Interleaved Data LLaVA-interleave Li et al. (2024d) LLaVA-interleave-dpo Li et al. (2024d) VILA-1.5-3B Lin et al. (2023b) VILA-1.5-3B-s2 Lin et al. (2023b) VILA-1.5-8B Lin et al. (2023b) VILA-1.5-13B Lin et al. (2023b) Phi-3V Abdin et al. (2024) LLaVA-NV Zhang et al. (2024b) LLaVA-NV-dpo Zhang et al. (2024b) Mantis-Idefics2 Jiang et al. (2024) Mantis-LLaVA Jiang et al. (2024) Mantis-siglip-llama3 Jiang et al. (2024) Mantis-clip-llama3 Jiang et al. (2024) GPT-4V Achiam et al. (2023) GPT-4o Achiam et al. (2023) GPT-4o-mini Achiam et al. (2023) Gemini-1.5-flash Team et al. (2023) Gemini-1.5-pro Team et al. (2023) 31.98 33.63 31.07 25.64 35.29 32.43 33.18 29.71 29.41 32.88 27.75 34.09 36.95 35.29 43.74 37.41 38.86 34.24 41.57 40.38 38.00 35.63 37.05 35.87 35.39 41.57 39.43 33.25 40.14 35.87 41.33 25.18 46.32 39.90 38.95 39.43 26.30 28.59 30.88 30.76 18.58 30.88 28.95 29.35 30.92 28.59 28.11 25.33 22.92 35.22 35.22 25.33 24.61 27.62 32.41 33.33 29.94 35.65 31.17 45.37 50.15 7.56 45.22 47.84 43.67 30.56 27.78 35.19 30.40 28.09 46.30 26.39 11.27 41.20 39.20 40.43 44.60 39.35 33.02 52.16 38.73 35.34 44.44 44.44 38.43 40.74 46.14 45.06 41.20 39.20 46.76 38.43 39. 62.16 52.70 27.03 33.78 59.46 59.46 60.81 62.16 52.70 59.46 55.41 36.49 58.11 44.59 67.57 50.00 54.05 52.70 20.27 59.46 58.11 45.95 50.00 35.14 40.54 51.35 45.95 59.46 59.46 54.05 41.89 43.24 60.81 52.70 70.27 62.16 71.62 70.27 68.92 58.16 58.84 38.10 46.60 58.84 55.10 60.20 50.34 61.90 59.86 73.47 23.47 69.73 57.48 68.37 71.09 53.74 40.48 17.01 57.14 63.61 56.40 54.22 45.78 45.78 41.42 46.87 56.68 45.90 46.45 57.49 41.96 50.14 56.13 81.20 86.92 78.75 77.38 80. Table 21: Evaluation Results for 39 VLMs in PhysBench Physics-based (cid:1)Dynamics Sub-task (the second last column of Table 3). 58 Published as conference paper at ICLR 2025 F.4 PHYSBENCH-VAL RESULTS Random Choice Size Format 2Property Relationships Scene (cid:1)Dynamics Avg 25. 25.00 25.00 25.00 25.00 - - Image VLM InstructBLIP-t5-xl Dai et al. (2024) InstructBLIP-t5-xxl Dai et al. (2024) InstructBLIP-7B Dai et al. (2024) InstructBLIP-13B Dai et al. (2024) BLIP-2 Li et al. (2023c) LLaVA-1.5-7B Liu et al. (2023a) LLaVA-1.5-13B Liu et al. (2023a) LLaVA1.6-mistral Liu et al. (2024b) LLaVA1.6-vicuna Liu et al. (2024b) Qwen-VL-Chat Bai et al. (2023b) InternVL-Chat1.5 Chen et al. (2024c) Cambrian-8B Tong et al. (2024) Claude-3-opus Anthropic (2024) Claude-3-sonnet Anthropic (2024) Claude-3-haiku Anthropic (2024) Claude-3.5-sonnet Anthropic (2024) 4B merge 12B merge 7B merge 13B merge 12B merge 7B merge 13B merge 7B merge 7B merge 9B merge 26B merge 8B merge - merge - merge - merge - merge 40.54 48.65 28.57 30.56 51.35 37.84 56.76 43.24 59.46 40.54 62.16 8.11 45.95 40.54 56.76 54.05 Video-LLaVA Lin et al. (2023a) Chat-Univi-7B Jin et al. (2023) Chat-Univi-13B Jin et al. (2023) PLLaVA-7B Xu et al. (2024) PLLaVA-13B Xu et al. (2024) Video VLM 7B 7B 13B 7B 13B seq seq seq seq seq 43.24 24.32 8.11 48.65 51.35 General VLM + Interleaved data LLaVA-interleave Li et al. (2024d) 7B LLaVA-interleave-dpo Li et al. (2024d) 7B 3B VILA-1.5-3B Lin et al. (2023b) 3B VILA-1.5-3B-s2 Lin et al. (2023b) 8B VILA-1.5-8B Lin et al. (2023b) 13B VILA-1.5-13B Lin et al. (2023b) 4B Phi-3V Abdin et al. (2024) 7B LLaVA-NV Zhang et al. (2024b) 7B LLaVA-NV-dpo Zhang et al. (2024b) 8B Mantis-Idefics2 Jiang et al. (2024) Mantis-LLaVA Jiang et al. (2024) 7B Mantis-siglip-llama3 Jiang et al. (2024) 8B 8B Mantis-clip-llama3 Jiang et al. (2024) - GPT-4V Achiam et al. (2023) - GPT-4o Achiam et al. (2023) - GPT-4o-mini Achiam et al. (2023) - Gemini-1.5-flash Team et al. (2023) - Gemini-1.5-pro Team et al. (2023) seq seq seq seq seq seq seq seq seq seq seq seq seq seq seq seq seq seq 48.65 40.54 37.84 35.14 24.32 32.43 54.05 45.95 48.65 40.54 43.24 67.57 54.05 62.16 72.97 64.86 64.86 59.46 47.62 52.38 30.95 32.50 52.38 54.76 52.38 30.95 42.86 45.24 61.90 21.43 61.90 66.67 59.52 69.05 33.33 28.57 21.43 47.62 50.00 59.57 48.94 51.06 51.06 29.79 51.06 61.70 40.43 41.86 46.81 29.79 34.04 42.55 68.09 74.47 57.45 74.47 70.21 45.95 43.24 36.36 60.00 37.84 43.24 29.73 35.14 37.84 40.54 54.05 21.62 45.95 43.24 35.14 40.54 48.65 29.73 18.92 32.43 40. 43.24 48.65 43.24 43.24 43.24 37.84 51.35 32.43 29.73 43.24 45.95 40.54 35.14 43.24 54.05 45.95 54.05 56.76 44.44 49.21 35.48 36.51 46.03 52.38 42.86 39.68 49.21 44.44 57.14 28.57 57.14 38.10 49.21 50.79 52.38 33.33 17.46 53.97 58.73 51.90 50.63 36.71 36.71 36.71 45.57 49.37 46.15 46.15 45.57 35.44 40.51 44.30 67.09 62.03 63.29 63.29 68.35 44.69 48.60 33.14 37.80 46.93 48.04 45.25 37.43 47.49 43.02 58.66 21.23 53.63 46.37 50.28 53.63 45.25 29.61 16.76 46.93 51. 51.50 48.00 41.50 41.00 34.00 43.00 53.50 42.21 42.56 44.50 37.50 44.00 44.00 62.00 65.50 59.00 64.50 65.00 Table 22: Evaluation results for 39 vision-language models in PhysBench-val. Note that the evaluation of General VLMs is based on the data from Video and Image VLM evaluations, with the addition of interleaved data, meaning that the full test dataset of PhysBench is being assessed. In this context, seq refers to the sequential input of images after frame selection from videos, while merge refers to merging video frames into single image. 59 Published as conference paper at ICLR 2025 F.5 EMBODIED TASKS DETAILED DESCRIPTION To further validate the effectiveness of our data and method, we built simulation platform using MuJoCo (Todorov et al., 2012) and the Franka Emika Panda from Menagerie (Zakka et al., 2022), and conducted tests on five embodied tasks. Using the proposed mark-based visual prompting technique with GroundedSAM (Ren et al., 2024) and farthest point sampling (Qi et al., 2017), MOKA (Liu et al., 2024a) converts affordance reasoning into series of visual question-answering problems that pre-trained VLMs can solve. The setup matches MOKA, and the general visual setup can be seen in Figure 25(a)(b). Our tabletop environment only has one top-down camera, which is the primary camera used in MOKA to capture RGBD images. For each task, we report the number of successes out of 10 trials following the setting of Liu et al. (2024a). (a) (b) (c) (d) (e) (f) (g) Figure 25: (a) Overview of the simulation platform. (b) Top-down view of the area. (c) Affordance test: Testing whether the robotic arm correctly grasps the object. (d) Force test: Testing whether the robotic arm can properly grasp deformable, fragile, and rigid objects. (e) Color test: Testing whether the robotic arm can pick up the correct colored object among identical ones. (f) Number test: Testing whether the robotic arm can grasp specific number of objects. (g) Tool test: Testing whether the robotic arm can select the correct tool given specific scenario. We present the specific language instructions given to the VLM in Figure 26. It is important to note that, for each task, we only provide single example. For instance, while we tested five objects in the Affordance taskpot, knife, spoon, monitor, and tennis racketwe only used the tennis racket as an example here. For each task, we report the success rate over 10 trials, following the MOKA protocol. Specifically, the test content and evaluation methods for each task are as follows: (1) Affordance test: This test evaluates whether the robotic arm can correctly grasp various objects. Figure 25(c) illustrates the robotic arm successfully grasping pot. In total, we tested the grasping ability on 10 items, including pot, knife, spoon, spatula, monitor, tennis racket, phone, and others. Specifically, we tested 5 objectspot, knife, spoon, monitor, and tennis racketattempting to grasp each object twice. (2) Force Test: This test evaluates the robotic arms capacity to properly grasp deformable, fragile, and rigid objects. Due to simulation constraints, the evaluation was based on the robotic arms output metrics. We tested fragile items (e.g., egg, ripe persimmon), soft items (e.g., jelly, plastic cup), and rigid objects (e.g., iron ball), with two attempts per object. It should be noted that the simulation system models all objects as rigid bodies, meaning that breakage during grasping is not depicted. Furthermore, while using the MOKA system to control the Panda robotic arm, we were unable to directly manipulate the grippers size. Instead, we provided the VLM with approximate object dimensions, allowing the VLM to determine the necessary gripping force to evaluate success or failure. (3) Color test: This test evaluates whether the robotic arm can accurately pick up the correctly colored object from set of identical items. As shown in Figure 25(e), the blocks are identical except for their color, and the task requires selecting the object of the designated color. We tested five colorsblue, pink, brown, green, and orangeconducting two trials for each color. 60 Published as conference paper at ICLR 2025 (4) Location test: This test evaluates whether the robotic arm can correctly grasp objects at specific locations. The goal is to ensure that the robotic arm accurately grasps the required objects based on their positions. Specifically, we tested with three blocks, requiring the arm to grasp the middle block (4 attempts), the block farthest from the plate (3 attempts), and the block closest to the plate (3 attempts). (5) Tool test: This test assesses whether the robotic arm can select the appropriate tool for given task. For instance, as shown in Figure 25(g), the task is: If you need to cut watermelon, which tool should you grasp? In this scenario, the robotic arm is expected to grasp the fruit knife. In total, we posed 5 questions, with 2 attempts per question, requiring the robotic arm to select and grasp different target tools for each task. On the other hand, these five tasks require minimal consideration of height (or depth) information, making the evaluation more fundamental. For MOKAs waypoints selected from free space, their height must be explicitly specified for accurate deprojection into 3D space, as they are not anchored to any objects. For this reason, in typical tabletop manipulation scenarios, MOKA primarily focuses on cases where the waypoints are at the same height as the target point. Affordance Grasp the tennis racket. Force Color Location Tool Grasp the egg. Grasp the blue cube. Grab the block farthest from the plate and move it to the plate. Grasp the tools for cutting watermelon. Figure 26: The language description of the testing tasks. We present an overview of our implementation of MOKA in Algorithm 1. Our experiments aim to enhance the VLMs ability to understand the physical world and validate its impact on downstream embodied agent tasks. Specifically, we employ two methods to improve the VLM: first, fine-tuning the VLM using PhysBench, and second, incorporating the PhysAgent to assist during VLM inference. It is worth noting that the five tasks we address are relatively fundamental, unlike the complex multi-action tasks described in the MOKA paper, which require hierarchical decomposition from highto low-level actions. In our case, the tasks can be executed directly without such decomposition. Algorithm 1 MOKA Pipeline 1: Input: Vision-language Model M, Task instruction l, text prompt for low-level reasoning plow and initial observation 2: Get observation from the top-down camera 3: Propose keypoint and waypoint candidates and get annotated image (sk) 4: Query for low-level motion reasoning, obtain ylow = M([plow, l, (s)]) 5: Execute ylow on the robot in simulation F.6 CORRELATION MAP Following the approach of Tong et al. (2024); Fang et al. (2024; 2025), we used the Pearson correlation coefficient to construct relationship matrix. The data used to build this matrix can be found in Table 23. VQAv2 GQA VisWiz SQA TextVQA POPE MME MMB MMBCN SEED SEEDI MMMUval MMMUtest LLaVA-bench 78.5 LLaVA-1.5-7B 80.0 LLaVA-1.5-13B InstructBLIP-7B 61.1 InstructBLIP-13B 62.3 78.2 Qwen-VL-Chat 80.4 VILA-1.5-3B 79.8 VILA-1.5-3B-s2 80.9 VILA-1.5-8B 82.8 VILA-1.5-13B 41.0 BLIP62.0 63.3 49.2 49.5 57.5 61.5 61.4 61.9 64.3 44.6 50.0 53.6 34.5 33.4 38.9 53.5 61.3 58.7 62.6 29.4 66.8 71.6 60.5 63.1 68.2 69.0 69.6 79.9 80.1 61.0 58.2 61.3 50.1 50.7 61.5 60.4 63.4 66.3 65.0 42.1 85.9 1510.7 64.3 85.9 1531.3 67.7 78.8 1210.1 36.0 78.9 1212.8 42.0 85.6 1487.5 60.6 85.9 1442.4 63.4 85.3 1431.7 62.8 84.4 1577.0 72.3 86.3 1569.6 74.9 85.3 1293.8 44.0 58.3 63.6 23.7 25.0 56.7 52.7 52.2 66.2 66.3 27. 61.5 62.4 53.4 55.2 58.2 60.9 60.0 64.2 65.1 46.4 67.0 68.2 58.8 61.7 65.4 67.9 66.4 71.4 72.6 49.7 33.2 36.4 32.9 35.7 35.9 33.3 32.8 36.9 37.9 35.4 31.1 33.6 30.6 33.8 32.9 30.8 31.3 36.0 33.6 34.0 63.4 70.7 60.9 58.2 64.1 75.9 76.7 80.0 80.8 56.2 Table 23: The performance of the 10 models used to construct the correlation map across 15 other VLM benchmarks. 61 Published as conference paper at ICLR 2025 The correlation presented in Figure 4(a) illustrates the relationships between the four major categories in PhysBench and other tasks. Additionally, we provide detailed correlation map between PhysBench and 15 other vision-language benchmarks in Figure 27 below. Figure 27: Correlation map between PhysBench and 15 other vision-language benchmarks. F.7 PERFORMANCE ON RELATED BENCHMARKS To further evaluate the contribution of our data and model to understanding the physical world, we conducted tests on three existing benchmarks related to physical-world perception. Notably, these benchmarks focus on specific aspects of physical-world perception, whereas our PhysBench provides more comprehensive and holistic evaluation, as summarized in Table 1. Furthermore, since these datasets were not originally designed for VLMs, we applied necessary preprocessing to adapt them for our evaluations. Setup.EmbSpatial (Du et al., 2024) is benchmark designed to evaluate spatial understanding within embodied environments with source image come from MP3D (Chang et al., 2017), ScanNet (Dai et al., 2017) and AI2-THOR (Kolve et al., 2017). We utilized its benchmark dataset for testing purposes. ContPhy (Zheng et al., 2024b) is benchmark aimed at assessing visual models capabilities in perceiving continuous physical phenomena and properties. It comprises four simulation systems based on Unity3D: Fluid Hourglass, Rope-Pulley System, Cloth Magic Trick, and Ball Playground. Since ContPhy primarily targets visual and physical models, we selected 200 items for each of the four categories (split evenly between property-based and dynamics-based tasks) and translate them into multiple-choice format suitable for VLMs. The question prompts were modified to better align with the answering capabilities of VLMs. Physion++ (Tung et al., 2023) evaluates the impact of physical properties such as mass, friction, elasticity, and deformability on physical phenomena. It leverages the ThreeDWorld simulation platform (Gan et al., 2020) to generate series of videos, each paired with corresponding question. The videos consist of an inference phase, where artificial systems can identify objects mechanical properties, followed by prediction phase, where the model must predict whether two specified objects will collide after the video ends. Physion++ is primarily designed for vision models such as ResNet (He et al., 2016) and VGG (Simonyan & Zisserman, 2014), with answers in the form of binary classification (yes/no), derived by converting model outputs into probabilities. To adapt Physion++ for VLMs, we processed the videos and 62 Published as conference paper at ICLR 2025 reformulated the questions into natural language prompts, adding necessary contextual hints. For example, if video includes transition phase, we explicitly include statement like, The black screen in the video marks transition in objects or scenes in the prompt. Due to the small size of the test set, we combined the train and test sets, resulting in 250 VQA pairs for evaluation. During fine-tuning with PhysBench, we modified the options to an open-ended format. The results, presented in Table 24, are reported in terms of accuracy. All parameter settings and configurations are kept consistent with those outlined in the main text. Table 24: Performance on Three Related Benchmarks. ContPhy-Property ContPhy-Dynamics Phi-3V Phi-3V + finetune Phi-3V + PhysAgent 49.25 64.50 52.00 43.25 62.75 44.75 Physion++ 68.80 82.20 78.40 EmbSpatial 55.91 66.95 62.28 Results. As presented in Table 24, leveraging PhysBench data for fine-tuning or in zero-shot setting with PhysAgent leads to performance improvements across the benchmarks, particularly in Physion++, where improvements of 19.50% and 9.6% are observed, with fine-tuning achieving the most significant gains. These results highlight the effectiveness of our data and methods in enhancing the capability of Vision-Language Models to comprehend the physical world."
        },
        {
            "title": "G MORE RELATED WORKS",
            "content": "Vision-Language Models. Vision-Language Models (VLMs) are large language models that integrate visual modalities, such as images and videos, with language knowledge (Wu et al., 2023b; Huang et al., 2024; Bai et al., 2022; Zhan et al., 2024; Dai et al., 2024; Bai et al., 2024b). Notable models like BLIP-2 (Li et al., 2023c) and LLaVA (Liu et al., 2024c) have advanced image-captioning datasets and visual instruction tuning, with LLaVA-Next further improving single-image performance at higher computational costs (Liu et al., 2024b). Subsequent models, such as QwenVL (Bai et al., 2023b), CogVLM (Wang et al., 2023b), and Yi-VL (AI et al., 2024), have followed similar architecture to LLaVA. As single-image and text interaction technologies continue to mature, many recent VLMs (Alayrac et al., 2022; Peng et al., 2023; Pan et al., 2024; Lin et al., 2023b) are now capable of handling complex visual tasks with interleaved images or videos, enabling VLMs to tackle more sophisticated tasks (Lu et al., 2024b; Yu et al., 2024b) and paving the way for interactions with the real physical world. Vision-Language Benchmarks VLMs (Liu et al., 2024c; Achiam et al., 2023; Pan et al., 2024; Yu et al., 2023a; Chow et al., 2024; ?; Li et al., 2024b;c) have inherited and advanced many intriguing features from text-only LMs. Benchmarks for VLMs have rapidly emerged to evaluate performance in areas such as image question answering (Ying et al., 2024), in-context response (Yu et al., 2023b), chart understanding (Li et al., 2024f), and web comprehension (Liu et al., 2024d; Zhou et al., 2023). Some benchmarks cover spatial relations understanding (Li et al., 2023a), but often overlook the ability to devise complex spatial action plans based on physical world comprehension. Recently, new benchmarks have also emerged that focus on the ability to understand multiple images in long contexts (Zhang et al., 2024a; Kil et al., 2024; Jiang et al., 2024; Wu et al., 2023a; Li et al., 2024e; Ge et al., 2024) and complex realistic environments (Fu et al., 2024; Bai et al., 2024a). However, these benchmarkswhether based on answering questions from images, videos, or tables, or using visual prompts (Fu et al., 2024; Yu et al., 2024a)ultimately rely on responding to the content of the given images rather than the true perception of the physical world, thus falling short of achieving spatial intelligence (Gupta et al., 2021; Yang et al., 2024a). Video Benchmarks. With the growing interest in video understanding, the development of benchmarks for VLMs has become increasingly emphasized. In video comprehension, the research community has made significant strides, particularly for short videos. There are specialized benchmarks for temporal perception (Yu et al., 2019; Wu et al., 2024a), action understanding (Liu et al., 2024e; Mangalam et al., 2024), video classification (Kay et al., 2017), video reasoning (Xiao et al., 2021a; Xie et al., 2023), video captioning (Miech et al., 2019; Xu et al., 2016), video questionanswering (Zhou et al., 2024; Li et al., 2023g; 2024e), long video comprehension (Wu et al., 2024b; Chandrasegaran et al., 2024), video generation (Bansal et al., 2024), and interleaved video-text 63 Published as conference paper at ICLR 2025 Table 25: Comparison between PhysBench and other vision-language benchmarks. In the format, I, T, present text, image, and video. Annotated means annotate the existing dataset, like MSCOCO Karpathy & Fei-Fei (2015). LLaVAWd: LLaVA-Bench(In-the-Wild)-Detail Liu et al. (2024c). Reasoning means that it requires the VLMs reasoning ability to answer the question. Dataset Size (k) Format Interleaving Source Domain Reasoning VQA-v2 Goyal et al. (2017b) GQA Hudson & Manning (2019) VizWiz Gurari et al. (2018) TextVQA Singh et al. (2019) OKVQA Marino et al. (2019) SEED Li et al. (2023a) MMBench Liu et al. (2023c) MME Yin et al. (2023) POPE Li et al. (2023h) MM-Vet Yu et al. (2023b) LLaVAWd Liu et al. (2024c) SQAI Lu et al. (2022) NLVR2 Suhr et al. (2018) MathVista Lu et al. (2024b) BLINK Fu et al. (2024) Mantis-eval Jiang et al. (2024) Q-Bench Wu et al. (2023a) MMMU Yue et al. (2024) PhysBench 1,105,904 22,669,678 32,000 45,000 14,000 19,000 3,000 1,297 18,000 200 60 6,000 6,967 6,141 1,901 217 2,990 11,500 10,002 I+T I+T I+T I+T I+T V+I+T I+T I+T I+T I+T I+T I+T I+T I+T I+T I+T I+T I+T V+I+T Annotated Annotated Annotated Annotated Annotated Annotated Annotated Annotated Annotated Annotated Annotated Annotated Annotated Annotated Annotated, Chart Annotated Annotated Annotated, Web, Textbook Annotated, Web, Simulation, Real-world Image content Image content Image content Image content Image content Image and video content Image content Image content Image hallucination detection Image chat Image chat Image content Image content Math Visual prompt Image chat Image content Image content Physical property and dynamics question-answering (Wang et al., 2024a). However, these works primarily focus on evaluating video content and do not explore the underlying mechanisms of video representation or address true physical world perception. Furthermore, during our experiments, we observed significant challenges with current video VLMs in following instructions and answering questions, as the models frequently output descriptions of the video rather than directly addressing the posed questions. Interleaved Vision-Language Benchmarks. VLMs are increasingly processing longer and more complex inputs. Along with this development, several benchmarks with interleaved inputs have emerged (Li et al., 2023b; Wang et al., 2024b; Meng et al., 2024). For example, SEED-Bench (Li et al., 2023a; 2024a) focuses on video understanding, BLINK (Fu et al., 2024) introduces visual prompts, and NLVR2 (Suhr et al., 2018) differentiates between two images. However, all of these benchmarks still primarily assess content description based on images, without evaluating physical understanding or perception abilities. Furthermore, current interleaved benchmarks only involve images and text, while PhysBench is dataset that interweaves video, image, and text inputs. detailed comparison with the previously mentioned benchmarks and other vision-language benchmarks can be found in Table 25. Science-related Benchmarks. In addition to the benchmarks related to physical world comprehension mentioned in Section 2, there are also benchmarks that assess models understanding through middle or university-level physics exam questions. SciQ (Welbl et al., 2017), ScienceQA (Lu et al., 2022), E-EVAL (Hou et al., 2024), MMLU-STEM (Hendrycks et al., 2020), and C-EvalSTEM (Huang et al., 2023b) include some physics-related questions, but these datasets often allow questions to be answered simply by analyzing the provided images, lacking the complexity of reasoning and computational tasks. JEEBench (Arora et al., 2023) requires multistep reasoning with physics knowledge but is limited in scope and purely text-based. SciBench (Wang et al., 2024f), OlympiadBench (He et al., 2024), MathVista (Lu et al., 2024b), and OCWCourses (Lewkowycz et al., 2022) provide college-level physics questions. However, these benchmarks mainly consist of homework and exam-style questions, focusing more on mathematical reasoning (Zheng et al., 2024a) and general knowledge rather than true physical world comprehension. In contrast, our PhysBench is the first systematic and comprehensive question-answering benchmark specifically designed for understanding the real physical world. Table 26: comparison between PhysBench and other physical understanding benchmarks not in question-answering format. Property Attribute Location Velocity Temperature Camera Light Collision Manipulation Physics 101 Wu et al. (2016) IntPhys Riochet et al. (2018) ESPRIT Rajani et al. (2020) CRAFT Ates et al. (2020) CoPhy Baradel et al. (2019) PhysBench 64 Fluid Interleaved Size 17,408 15,000 2,441 57,000 216,000 10,002 Published as conference paper at ICLR 2025 3D Scence VQA. Recently, multi-modal 3D perception has garnered increasing attention due to its connection to the physical world, driving rapid advancements in the field. SQA3D (Ma et al., 2023) highlights the importance of situations within contextual understanding. EmbodiedScan (Wang et al., 2024d) and SceneVerse (Jia et al., 2025) expand the scope by collecting more scenes or annotating additional objects, scaling annotations to the millions. OpenEQA (Majumdar et al., 2024), SpatialRGPT-Bench (Cheng et al., 2024), MMScan (Lyu et al., 2024), and MSNN (Linghu et al., 2024) integrate comprehensive annotations, adapting the task into VQA format. However, these studies primarily focus on geometric relationships, which represent only subset of the broader understanding of the physical world, as discussed in Section 2. Our work prioritizes more holistic evaluation of the physical world perception capabilities of VLMs across four major task categories: Physical Object Properties, Physical Object Relationships, Physical Scene Understanding, and Physics-based Dynamics. Additionally, the spatial components in our dataset differ significantly from those in existing Spatial VQA datasets. While such benchmarks typically rely on 3D point cloud scenes or interleaved 2D images from multiple viewpoints, our dataset uses interleaved images to capture physical world dynamics, such as viewpoint rotations and the progression of physical phenomena. VLMs for Robotic Manipulation. Recently, two main approaches have been proposed for applying Vision-Language Models (VLMs) (Liu et al., 2024c; Achiam et al., 2023; Team et al., 2023; Mao et al., 2023a;b) to robotic manipulation: (a) directly generating actions (Kim et al., 2024; Octo Model Team et al., 2024; Zawalski et al., 2024a; Niu et al., 2024; Zawalski et al., 2024b) and (b) employing VLMs as agents (Liu et al., 2024a; Nasiriany et al., 2024; Huang et al., 2023a). Approach (a) involves directly outputting actions, which requires extensive trainingfor instance, OpenVLA was trained using 64A100 GPUsand produces embodied-specific actions that necessitate targeted fine-tuning. In contrast, approach (b) generates affordance representations through VLMs, which are subsequently converted into actions. This method requires less training and exhibits stronger generalization capabilities. However, it suffers from weaker perception capabilities in the physical world, leading to performance limitations (Liu et al., 2024a; Mao et al., 2024)."
        },
        {
            "title": "H MORE EXAMPLES",
            "content": "We use red color as right answer. We use red to indicate the correct answer (correct answer). It is important to note that, to reduce difficulty and facilitate evaluation, we employed multiple-choice format rather than open-ended responses. For space-saving purposes, the example figures in the main text do not display the available options. H.1 PHYSICAL OBJECT PROPERTY SUB-TASK Question The color of the original spots on the chameleons body has not changed during the process of changing its color. What color are these spots? Answer A. Black B. Green C. White D. Blue Figure 28: Example for property color. Ability Type is identify. H.2 PHYSICAL OBJECT RELATIONSHIPS SUB-TASK H.3 PHYSICAL SCENE UNDERSTANDING SUB-TASK H.4 PHYSICS-BASED DYNAMICS SUB-TASK 65 Published as conference paper at ICLR 2025 Question The weight of the object about the size of ping-pong ball is closest to which of the options? Answer A. 5g B. 50g C. 500g D. 5kg Question Which fruit in the picture is most likely to be the heaviest individually? Answer A. Watermelon B. Pear C. Plum D. Peach Question What can be said about the mass of the brown cube compared to the white sphere? Answer A. The brown cube is less massive B. The white sphere is less massive C. They have the same mass D. Cannot answer Figure 29: Three examples related to property mass, categorized by the following ability types: identification, comparison, and comparison. 66 Published as conference paper at ICLR 2025 Question How many blueberries are there outside the plate? Answer A. 1 B. 2 C. 3 D. 4 Question How many of the books pictured have covers with titles beginning with P? Answer A. 1 B. 2 C. 3 D. 4 Question Following the content of the video, How many eggs were broken? Answer A. 1 B. 2 C. 3 D. 4 Question How many balls are moving close together without separating? Answer A. 2 B. 3 C. 4 D. 6 Figure 30: Four examples related to property number, ability types are all identification. 67 Published as conference paper at ICLR 2025 Question In the photo, which point with option signifies the object with the most sharp? Answer A. Point B. Point C. Point D. Point Question brittle? Can you tell me which point with option in the image points to the object that has the least Answer A. Point B. Point C. Point D. Point Question Which point with option in the photograph pinpoints the object with the most stiff? Answer A. Point B. Point C. Point D. Point Question Which point with option in the image marks the object that exhibits the most elastic? Answer A. Point B. Point C. Point D. Point Question Which point with option in the photograph captures the object that has the most malleable? Answer A. Point B. Point C. Point D. Point Question Which point with option in the image isolates the object with the most soft? Answer A. Point B. Point C. Point D. Point Figure 31: Six examples of property attributes include sharpness, brittleness, stiffness, elasticity, malleability, and softness. 68 Published as conference paper at ICLR Question Moving according to the arrow in the picture, which of the following options are you most likely to encounter? Answer A. C. Question B. D. Following the direction indicated by the arrow in the picture, which of the following options are you most likely to encounter? Answer A. C. Question B. D. What is the relationship between the speeds of the two objects in the video after they begin to fall? Answer A. Both objects have the same speed. B. The egg consistently falls faster than the feather. C. The feather consistently falls faster than the egg. D. Initially, the egg falls faster, but the feather eventually surpasses it. Figure 32: Three examples for relationships motion, categorized by the following ability types: static, static and dynamic. Question Determine which point is nearest to the camera: Answer A. Point is nearest B. Point is nearest C. Point is nearest D. Point is nearest Figure 33: An example for relationships depth. Ability Type is static. 69 Published as conference paper at ICLR 2025 Question Find the point that is closest to the river: Answer A. Point is the closest B. Point is the closest C. Point is the closest D. Point is the closest Question closest to? What value is the distance between each of the three small bowls placed side by side Answer A.3mm B.3cm C.10cm D.20cm Question What is farthest from the key in the picture? Answer A. Pen B. Camera C. Watch D. Glasses Figure 34: Three examples for relationships distance. Ability Type is static, static and dynamic. Question What is happening to the size of the ice cube in the video? Answer A. Increasing B. Decreasing C. No change D. Increasing first and then decreasing Question Answer A. red cube Which object is the biggest in volume? B. purple cube C. green cube D. orange cube Figure 35: Two examples for relationships size. Ability Type is dynamic and static. 70 Published as conference paper at ICLR 2025 Question Among these points, which one is not on the same sunflower as the other 3 points? Answer A. Point B. Point C. Point D. Point Question What is the object above the egg? Answer A. Milk B. Yogurt C. Banana D. Egg Question How does the moon pass through the sun in the video? Answer A. From the lower left corner to the upper right corner of the picture B. From the upper left corner to the lower right corner of the picture C. From the lower right corner to the upper left corner of the picture D. From the upper right corner to the lower left corner of the picture Question Which point with the option in the image is corresponds to the reference point in the How does the moon pass through the sun in the video? Answer A. Point B. Point C. Point D. Point Figure 36: Four examples for relationships location. Ability Type is static, static, dynamic and static. 71 Published as conference paper at ICLR 2025 Question Where is the light source in the picture? Answer A. In the clouds on the upper right of the screen B. In the lower left corner of the screen C. little above the center of the screen D. In the exact center of the screen Question Reflecting on the events in the video, which of the following alterations to the light source is most likely to result in the phenomenon observed? Answer A. The color of the light changes from yellow to pink B. Its just that the light source is weaker and the light source position remains the same C. Move parallel to the line between the drumstick and the ballet skirt D. Its just that the light source is stronger and the light source position remains the same Question From the events in the video, which of the listed changes to the light source is most likely to have resulted in the observed phenomenon? Answer A. Move parallel to the line between the postcard and the cake B. The color of the light changes from orange to blue C. The color of the light changes from lime yellow to green D. The light source moves downward Question Taking into account the phenomena observed in the video, which of the following changes to the light source is most likely to have led to this result? Answer A. Its just that the light source is stronger and the light source position remains the same B. Its just that the light source is weaker and the light source position remains the same C. The color of the light changes from red to purple D. The color of the light changes from yellow to blue Figure 37: Four examples illustrating scene environmental lighting conditions. The corresponding ability types are perception, reasoning, reasoning, and reasoning. Published as conference paper at ICLR 2025 Question How does the brightness in the picture change? Answer A. First it gets brighter, then it gets darker B. First it gets darker, then it gets brighter C. It keeps getting darker D. keeps changing Figure 38: Examples for scene environmental lighting conditions (Continued). Ability Type is judgement. Question What happens to the gas pressure inside the bottle before open it? Answer A. Increases B. Decreases C. Stays the same D. Varies randomly Figure 39: Example for scene environmental air conditions and the ability type of it is perception. Question What oven temperature range is typically used for the phenomenon shown in the video? Answer A. Under 100C B. Approximately 200C C. Nearly 400C D. Over 600C Question What is the possible cause of the phenomenon in the video? Answer A. Increased humidity B. Increased temperature C. Decreased temperature D. Decreased humidity Figure 40: Two examples for scene temperature conditions and the ability types of them are all perception. 73 Published as conference paper at ICLR 2025 Question At the beginning of the video, how does the cameras focal length change? Answer A. Focus length remains unchanged B. Focus length increases C. Focus length decreases D. Unknown Question Based on the phenomenon in the video, which of the following camera changes can produce the effect in the video? Answer A. Move parallel to the line between the motor and the wash basin B. Move parallel to the line between the bunk bed and the wash basin C. The camera moves upward or downward D. The camera rotates along the horizontal axis (left or right) Question From the video, which of these camera changes could be responsible for the depicted phenomenon? Answer A. The camera is farther away from the objects B. Move parallel to the line between the cupcake and the sponge C. The camera is closer to the objects D. Move parallel to the line between the cupcake and the blackboard Question Based on the phenomenon in the video, which of the following camera changes can produce the effect in the video? Answer A. The camera rotates along the vertical axis (upside or downside). B. The camera is closer to the objects C. The camera rotates along the horizontal axis (left or right). D. The camera moves upward or downward Figure 41: Four examples illustrating scene viewpoint conditions. The corresponding ability types are perception, reasoning, reasoning and reasoning. 74 Published as conference paper at ICLR Question Select the option that shows the correct procedure to put on pair of gloves. image #1: image #2: image #3: Answer A. 1 - 3 - 2 C. 2 - 1 - Question B. 3 - 2 - 1 D. 1 - 2 - 3 Which of the following options lists the steps in the correct sequence to put the carrot in the microwave? image #1: image #2: image #3: image #4: Answer A. 2 - 1 - 4 - 3 C. 2 - 3 - 1 - 4 Question B. 1 - 3 - 2 - 4 D. 2 - 4 - 3 - 1 To poke the watering can, which point is most suitable? Answer A. B. Question C. D. In order to pick up the cup, which of the following color points has reasonable affordance? Answer A. B. Question C. D. What operation is used to transform the object from Image to Image ? Answer A. Remove the small ball from the clay block. B. Add another ball to the clay block. C. Air-dry the clay block. D. Press the small ball into the clay block. Figure 42: Five examples illustrating dynamics manipulation. The corresponding ability types are judgment, judgment, perception, perception and reasoning. 75 Published as conference paper at ICLR 2025 Question Following the content of the video, which options corresponding picture will happen first? Answer A. B. C. D. Figure 43: Example for dynamics collision and the ability type of it is prediction. Question Why are the phenomena happens in the video? Answer A. Because they are lighter than air. B. Due to the presence of ferric oxide in the match heads. C. Because they are coated with special glue. D. Due to static electricity on the magnet. Figure 44: Example for dynamics chemistry and the ability type of it is perception. Question What will happen to the white ball? Answer A. It will drop into the right pit. B. It will not drop into the right pit. C. Its movement cannot be determined. D. It will drop into the left pit. Figure 45: Example for dynamics throwing and the ability type of it is prediction. 76 Published as conference paper at ICLR 2025 Question Which bottle contains the fastest diffusion rate? Answer A. The one on the left B. The one in the middle C. The one on the right D. Both are equally fast Question We already know that the liquid poured in is water, so what is the original yellow liquid? Answer A. Orange juice B. Oil C. Beer D. Urine Question Which color object looks the most viscous in the video? Answer A. Transparent liquid B. Light yellow color liquid C. Thick yellow liquid D. Dark blue liquid Question We already know that the red particles in the picture are liquid particles. In which area of the picture does the liquid flow fastest? Answer A. All parts of the image are at the same speed. B. the leftmost part of the picture C. the middle part of the picture D. the rightmost part of the picture Figure 46: Four examples illustrating dynamics fluid. The corresponding ability types are perception, reasoning, perception and perception. 77 Published as conference paper at ICLR 2025 Question Among the listed choices, which one outlines the proper sequence of events in candle burning? image #1: image #2: image #3: Answer A. 2 - 3 - 1 Question B. 1 - 2 - 3 C. 1 - 3 - D. 3 - 1 - 2 Which of the following options presents the correct order of occurrences in fruit rotting? image #1: image #2: image #3: Answer A. 3 - 2 - Question B. 2 - 3 - 1 C. 1 - 3 - 2 D. 1 - 2 - 3 The light first passes through the convex lens and then the concave lens. Slide the concave lens close to the convex lens. Which of the following options will correspond to the phenomenon in the picture? Answer A. C. B. D. Figure 47: Three examples illustrating dynamics others. The corresponding ability types are reasoning, reasoning and prediction. 78 Published as conference paper at ICLR"
        },
        {
            "title": "I ERROR STUDY",
            "content": "I.1 DETAILED STATICS In this section, we present case study analysis of the error types made by GPT-4V, Gemini-1.5flash, and Phi-3V across various tasks. The errors are classified into the following five categories. Other less frequent error types are not included in this analysis. For the analysis, as mentioned in Section 3.4, we selected 500 samples for each model, but due to space limitations, we present only 60 of them here, as shown in Table 27. Perception Error : VLMs fail to recognize, classify, or detect the objects or content in images. VLMs are constrained by the representation power of visual encoders, and these errors account for the majority of them. See examples in Figure 48, Figure 50, etc. Reasoning Error : VLMs can recognize the text and visual content exactly but make errors in reasoning, leading to incorrect results. See examples in Figure 65, Figure 81, etc. Lack of Knowledge : VLMs do not have specific knowledge, so they finally get wrong answer. See examples in Figure 57, Figure 71. Refuse to Answer : VLMs refuse to answer questions and stop answering immediately. See examples in Figure 54. Fail to Follow Instruction : VLMs fail to correctly understand instructions and provide erroneous answers. For example, VLMs may not understand the specified conditions in the instruction (see Figure 74). I.2 MAIN REASON ANALYSIS Perceptual Errors: Perceptual errors can be classified into two categories: basic perceptual errors and domain-specific perceptual errors. Basic perceptual errors occur when the model successfully understands the given information but fails to interpret fundamental visual objects correctly. In contrast, domain-specific perceptual errors arise when the model misinterprets visual inputs due to lack of understanding of specialized conditions. Moreover, GPT-4V often exhibits bias towards textual information, prioritizing text over visual inputsa trend highlighted in recent studies Cui et al. (2023). notable example is shown in Figure 68, where the model mistakenly identified the plate as part of the background, leading to incorrect depth estimation. This underscores the importance of seeking balanced approach to enhance the models interpretative capabilities. Reasoning Errors: Sometimes, even when the model correctly interprets the text, images, and question, it fails to establish rigorous logical chain, leading to incorrect conclusions. These types of mistakes, known as reasoning errors, are illustrated in Figures 65 and 78. In the first example, the model mistakenly assumes that friction can only exist if the machine rotates faster than the object, overlooking the presence of another object, which results in an incorrect answer. In the second example, while the model correctly identifies that the temperature of the wrapped paper is significantly lower than that of the other object, it incorrectly attributes this phenomenon to the flames temperature, leading to an erroneous conclusion. Lack of Knowledge: Another major cause of errors is the models lack of relevant knowledge. notable example is shown in Figure 71, where the model incorrectly assumes that light bending in water is due to refraction. The model clearly does not understand the concept of total internal reflection in water, leading to an incorrect conclusion. Other Errors: The remaining errors, such as those related to textual understanding, refusal to answer, annotation mistakes, and answer extraction issues, account for only small proportion. However, they remain significant and should not be overlooked. I.3 CASE STUDY 79 Published as conference paper at ICLR 2025 Table 27: Table index of case study figures by meta-task with associated error categories. Case Figure Meta-task Subtask GPT-4o Gemini-1.5-flash Phi-3V Figure 48 Figure 49 Figure 50 Figure 51 Figure 52 Figure 53 Figure 54 Figure 55 Figure 56 Figure 57 Figure 58 Figure 59 Figure 60 Figure 61 Figure 62 Figure 63 Figure 64 Figure 65 Figure 66 Figure 67 Figure 68 Figure 69 Figure 70 Figure 71 Figure 72 Figure 73 Figure 74 Figure 75 Figure 76 Figure 77 Figure 78 Figure 79 Figure 80 Figure 81 Figure 82 Figure 83 Figure 84 Figure 85 Figure 86 Figure 87 Figure 88 Figure 89 Figure 90 Figure 91 Figure 92 Figure 93 Figure 94 Figure 95 Figure 96 Figure 97 Figure 98 Figure 99 Figure 100 Figure 101 Figure 102 Figure 103 Figure 104 Figure 105 Figure 106 Figure 107 Scene Dynamics Dynamics Scene Scene Scene Dynamics Dynamics Dynamics Property Property Property Property Property Property Property Relationships Relationships Relationships Relationships Relationships Relationships Relationships Dynamics Relationships Scene Relationships Relationships Scene Scene Scene Dynamics Dynamics Dynamics Relationships Dynamics Dynamics Dynamics Dynamics Scene Scene Relationships Property Dynamics Scene Scene Relationships Relationships Dynamics Scene Scene Dynamics Dynamics Property Scene Scene Scene Scene Scene Scene Light Collision Throwing Light Viewpoint Viewpoint Chemistry Manipulation Manipulation Attribute Color Mass Mass Number Number Attribute Motion Motion Depth Depth Distance Distance Size Others Size Viewpoint Location Location Viewpoint Temperature Temperature Air Air Manipulation Depth Fluid Others Others Collision Light Viewpoint Location Attribute Manipulation Viewpoint Light Location Location Others Viewpoint Light Manipulation Collision Attribute Light Light Light Light Light Light Perception Error Reasoning Error Perception Error Perception Error Perception Error Perception Error Reasoning Error Perception Error Perception Error Perception Error Perception Error Reasoning Error Success Perception Error Perception Error Lack of Knowledge Perception Error Reasoning Error Reasoning Error Success Perception Error Reasoning Error Perception Error Lack of Knowledge Success Perception Error Perception Error Perception Error Perception Error Perception Error Reasoning Error Perception Error Reasoning Error Reasoning Error Reasoning Error Perception Error Perception Error Perception Error Perception Error Reasoning Error Perception Error Success Perception Error Perception Error Perception Error Perception Error Perception Error Perception Error Reasoning Error Perception Error Reasoning Error Perception Error Perception Error Perception Error Success Perception Error Perception Error Success Reasoning Error Perception Error 80 Perception Error Perception Error Perception Error Perception Error Reasoning Error Reasoning Error Refuse to Answer Perception Error Perception Error Perception Error Reasoning Error Success Success Perception Error Perception Error Lack of Knowledge Perception Error Reasoning Error Reasoning Error Success Reasoning Error Reasoning Error Reasoning Error Lack of Knowledge Success Perception Error Perception Error Perception Error Perception Error Perception Error Reasoning Error Perception Error Reasoning Error Reasoning Error Perception Error Perception Error Perception Error Perception Error Perception Error Perception Error Perception Error Perception Error Reasoning Error Success Perception Error Perception Error Perception Error Perception Error Reasoning Error Reasoning Error Reasoning Error Success Perception Error Perception Error Reasoning Error Perception Error Perception Error Perception Error Perception Error Perception Error Perception Error Reasoning Error Perception Error Perception Error Reasoning Error Success Success Perception Error Perception Error Reasoning Error Perception Error Perception Error Success Refuse to Answer Success Perception Error Reasoning Error Reasoning Error Success Success Success Lack of Knowledge Reasoning Error Lack of Knowledge Success Success Fail to follow instruction Reasoning Error Success Perception Error Reasoning Error Perception Error Reasoning Error Reasoning Error Perception Error Perception Error Perception Error Perception Error Perception Error Success Perception Error Perception Error Perception Error Perception Error Perception Error Perception Error Perception Error Perception Error Reasoning Error Success Reasoning Error Reasoning Error Success Reasoning Error Success Perception Error Reasoning Error Reasoning Error Reasoning Error Perception Error Published as conference paper at ICLR Figure 48: sample case of physical scene understandinglight. Back to List of Figures. 81 Published as conference paper at ICLR 2025 Figure 49: sample case of physics-based dynamicscollision. Back to List of Figures. 82 Published as conference paper at ICLR Figure 50: sample case of physics-based dynamicsthrowing. Back to List of Figures. 83 Published as conference paper at ICLR 2025 Figure 51: sample case of physical scene understandinglight. Back to List of Figures. 84 Published as conference paper at ICLR Figure 52: sample case of physical scene understandingviewpoint. Back to List of Figures. 85 Published as conference paper at ICLR 2025 Figure 53: sample case of physical scene understandingviewpoint. Back to List of Figures. 86 Published as conference paper at ICLR Figure 54: sample case of physics-based dynamicschemistry. Back to List of Figures. 87 Published as conference paper at ICLR 2025 Figure 55: sample case of physics-based dynamicsmanipulation. Back to List of Figures. 88 Published as conference paper at ICLR Figure 56: sample case of physics-based dynamicsthrowing. Back to List of Figures. 89 Published as conference paper at ICLR 2025 Figure 57: sample case of physical object propertyattribute. Back to List of Figures. 90 Published as conference paper at ICLR Figure 58: sample case of physical object propertycolor. Back to List of Figures. 91 Published as conference paper at ICLR 2025 Figure 59: sample case of physical object propertymass. Back to List of Figures. 92 Published as conference paper at ICLR Figure 60: sample case of physical object propertymass. Back to List of Figures. 93 Published as conference paper at ICLR 2025 Figure 61: sample case of physical object propertynumber. Back to List of Figures. 94 Published as conference paper at ICLR Figure 62: sample case of physical object propertynumber. Back to List of Figures. 95 Published as conference paper at ICLR 2025 Figure 63: sample case of physical object propertyattribute. Back to List of Figures. 96 Published as conference paper at ICLR Figure 64: sample case of physical object relationshipsmotion. Back to List of Figures. 97 Published as conference paper at ICLR 2025 Figure 65: sample case of physical object relationshipsmotion. Back to List of Figures. 98 Published as conference paper at ICLR Figure 66: sample case of physical object relationshipsdepth. Back to List of Figures. 99 Published as conference paper at ICLR 2025 Figure 67: sample case of physical object relationshipsdepth. Back to List of Figures. 100 Published as conference paper at ICLR Figure 68: sample case of physical object relationshipsdistance. Back to List of Figures. 101 Published as conference paper at ICLR 2025 Figure 69: sample case of physical object relationshipsdistance. Back to List of Figures. 102 Published as conference paper at ICLR Figure 70: sample case of physical object relationshipssize. Back to List of Figures. 103 Published as conference paper at ICLR 2025 Figure 71: sample case of physics-based dynamicsothers. Back to List of Figures. 104 Published as conference paper at ICLR Figure 72: sample case of physical object relationshipssize. Back to List of Figures. 105 Published as conference paper at ICLR 2025 Figure 73: sample case of physical scene understandingviewpoint. Back to List of Figures. 106 Published as conference paper at ICLR Figure 74: sample case of physical object relationshipslocation. Back to List of Figures. 107 Published as conference paper at ICLR 2025 Figure 75: sample case of physical object relationshipslocation. Back to List of Figures. 108 Published as conference paper at ICLR Figure 76: sample case of physical scene understandinglight. Back to List of Figures. 109 Published as conference paper at ICLR 2025 Figure 77: sample case of physical scene understandingtemperature. Back to List of Figures. 110 Published as conference paper at ICLR Figure 78: sample case of physical scene understandingtemperature. Back to List of Figures. 111 Published as conference paper at ICLR 2025 Figure 79: sample case of physics-based dynamicsair. Back to List of Figures. 112 Published as conference paper at ICLR Figure 80: sample case of physics-based dynamicsair. Back to List of Figures. 113 Published as conference paper at ICLR 2025 Figure 81: sample case of physics-based dynamicsmanipulation. Back to List of Figures. 114 Published as conference paper at ICLR Figure 82: sample case of physics-based dynamicsfluid. Back to List of Figures. 115 Published as conference paper at ICLR 2025 Figure 83: sample case of physics-based dynamicsfluid. Back to List of Figures. 116 Published as conference paper at ICLR Figure 84: sample case of physics-based dynamicsothers. Back to List of Figures. 117 Published as conference paper at ICLR 2025 Figure 85: sample case of physics-based dynamicsothers. Back to List of Figures. 118 Published as conference paper at ICLR Figure 86: sample case of physics-based dynamicscollision. Back to List of Figures. 119 Published as conference paper at ICLR 2025 Figure 87: sample case of physical scene understandinglight. Back to List of Figures. 120 Published as conference paper at ICLR Figure 88: sample case of physical scene understandingviewpoint. Back to List of Figures. 121 Published as conference paper at ICLR 2025 Figure 89: sample case of physical object relationshipslocation. Back to List of Figures. 122 Published as conference paper at ICLR Figure 90: sample case of physical object propertyattribute. Back to List of Figures. 123 Published as conference paper at ICLR 2025 Figure 91: sample case of physics-based dynamicsmanipulation. Back to List of Figures. 124 Published as conference paper at ICLR Figure 92: sample case of physical scene understandingviewpoint. Back to List of Figures. 125 Published as conference paper at ICLR 2025 Figure 93: sample case of physics-based dynamicscollision. Back to List of Figures. 126 Published as conference paper at ICLR Figure 94: sample case of physics-based dynamicsthrowing. Back to List of Figures. 127 Published as conference paper at ICLR 2025 Figure 95: sample case of physical object relationshipslocation. Back to List of Figures. 128 Published as conference paper at ICLR Figure 96: sample case of physics-based dynamicsfluid. Back to List of Figures. 129 Published as conference paper at ICLR 2025 Figure 97: sample case of physical scene understandingviewpoint. Back to List of Figures. 130 Published as conference paper at ICLR Figure 98: sample case of physical scene understandinglight. Back to List of Figures. 131 Published as conference paper at ICLR 2025 Figure 99: sample case of physics-based dynamicsmanipulation. Back to List of Figures. 132 Published as conference paper at ICLR Figure 100: sample case of physics-based dynamicscollision. Back to List of Figures. 133 Published as conference paper at ICLR 2025 Figure 101: sample case of physical object propertyattribute. Back to List of Figures. 134 Published as conference paper at ICLR Figure 102: sample case of physical scene understandinglight. Back to List of Figures. 135 Published as conference paper at ICLR 2025 Figure 103: sample case of physical object propertyattribute. Back to List of Figures. 136 Published as conference paper at ICLR Figure 104: sample case of physics-based dynamicsthrowing. Back to List of Figures. 137 Published as conference paper at ICLR 2025 Figure 105: sample case of physical object relationshipssize. Back to List of Figures. 138 Published as conference paper at ICLR Figure 106: sample case of physics-based dynamicsfluid. Back to List of Figures. 139 Published as conference paper at ICLR 2025 Figure 107: sample case of physical object relationshipsdepth. Back to List of Figures. 140 Published as conference paper at ICLR"
        },
        {
            "title": "J DISCUSSION AND STATEMENT",
            "content": "J.1 LIMITATION Portions of our data are constructed based on pre-existing datasets, as detailed in Table 13. We have made every effort to ensure that the images included in this paper comply with applicable copyright laws and are appropriately credited. Should you be the copyright holder of any image used in our work and believe that its usage conflicts with your licensing agreements, dont hesitate to get in touch with us directly. We are committed to promptly addressing any legitimate concerns. Although PhysBench is categorized into 4 major categories and 19 subcategories, making it the first benchmark aimed at evaluating vision-language models understanding of the physical world, it still does not encompass all aspects of the real physical environment. We invested thousands of hours in data collection, organization, and annotation, following multiple rigorous processes and repeated reviews to ensure data quality. Nevertheless, minor issues may persist in small portion of the dataset. In the experiments described in Section 3.4 and 4.1, we utilized GPT-4o-mini to extract answers, which, while currently being one of the most reliable and reproducible methods, may still unavoidably introduce hallucinations. Considering the challenges of using LLMs for evaluation, such as hallucinations and knowledge limitations, assessing open-ended formats is particularly difficult (Yu et al., 2023b; Ge et al., 2024; Li et al., 2023b). Moreover, automatically evaluating the quality of reasoning processes presents additional challenges (Lu et al., 2024b). To address these difficulties and simplify testing, we adopted multiple-choice format. Nevertheless, PhysBench remains highly challenging, with even the most advanced GPT-4o model achieving less than 50% accuracy. We believe that exploring more complex evaluation formats is crucial direction for future research. These challenges are not unique to our dataset but are common across various datasets in VLMs. Nevertheless, we believe that the potential benefits outweigh the associated risks, promoting continued advancement and societal progress. To the best of our knowledge, PhysBench and PhysAgent represent the first effort aimed at benchmarking and enhancing VLMs for physical world understanding, marking significant step forward in the field. We are committed to the ongoing refinement of our dataset to further improve machine intelligences comprehension of the physical world, advancing embodied AI toward human-level capabilities. J.2 BOARDER IMPACT The broader impact of PhysBench and PhysAgent carries both potential benefits and risks upon deployment and release. While some considerations pertain specifically to the nature of the dataset, others reflect broader challenges inherent to instruction-following vision-language models (VLMs). Below, we outline key risks and corresponding mitigation strategies. Biases. PhysAgent may inherit biases from its foundational models, both in vision and language foundation models. These biases can manifest in skewed outcomes or unfair representations, necessitating careful evaluation and mitigation efforts. Anticipated Societal Implications. major societal concern is the potential misuse of the dataset and system, including the generation of fabricated content, which may contribute to misinformation, privacy infringements, and other detrimental outcomes. To mitigate these risks, strict adherence to ethical guidelines and ongoing oversight are essential. Environmental Considerations. In alignment with environmental sustainability goals, we commit to publicly releasing the dataset and scripts to reduce unnecessary carbon emissions by regenerating similar datasets. Throughout our experiments, we ensure compliance with model and data licensing requirements. 141 Published as conference paper at ICLR J.3 ETHICS STATEMENT This study does not raise any ethical concerns, as it exclusively utilizes publicly available preexisting models, with no involvement of subjective evaluations. All research presented in this paper strictly adheres to the ethical guidelines set forth by the ICLR Code of Ethics. J.4 REPRODUCIBILITY STATEMENT We have adhered to the standard baseline settings employed by existing evaluation benchmarks or the original testing benchmarks of specific models. All necessary implementation details of our method are provided in Appendix and F. Furthermore, we are committed to releasing the data and code under an open-access license, accompanied by comprehensive instructions to ensure the accurate reproduction of the primary experimental results presented in this paper. All research conducted complies fully with the ICLR Reproducibility Requirements."
        },
        {
            "title": "K LATEST RESULTS",
            "content": "The models listed in Table 3 are current as of August 2024. Given the rapid evolution of VLMs, we evaluated an additional 36 models in December 2024, as shown in Table 28. description of these newly added models and their hyperparameters, along with those of the original 39 models, can be found in Appendix E.3. Due to space limitations, we have not combined the two tables, but consolidated version is available on our project page at Our Project Page. We will continue to update the results to reflect the latest advancements in VLMs. In Figure 6(a), the largest model size tested was 13B. In this experiment, we have updated to larger models to evaluate whether increasing the model size significantly impacts performance on PhysBench. Based on the new models added in Table 3, we present Figure 108 below. The results in Figure 108 reveal that models ranging from 1B to larger sizes generally show improvements, while the performance between 3B and 13B does not exhibit significant gains, and in some cases, such as with VILA and PLLaVA, decrease in performance is observed. This observation constitutes the primary area of interest in Figure 6(a). Notably, at larger scales, particularly at 26B and 40B, we observe substantial improvements compared to previous sizes. However, the scalability patterns in PhysBench differ from traditional VQA benchmarks, which typically demonstrate strong positive correlation between model size and performance. We selected widely-used benchmarks including TextVQA (Singh et al., 2019), MathVista (Lu et al., 2024b), and MMMU (Yue et al., 2024) for comparison. The relationship between model size and performance on these benchmarks is illustrated in Figures 111, 112, and 113, which demonstrate this consistent scaling pattern. In contrast, PhysBench exhibits less pronounced scalability compared to these benchmarks, with performance not always correlating positively with model size, as shown in Figures 108, 109, and 110. This phenomenon is particularly evident in the Scene subcategory of PhysBench, where performance remains relatively stagnant in the 5-20B parameter range, only showing notable improvements with models exceeding 25B parameters, as demonstrated in Figure 109. 142 Published as conference paper at ICLR Figure 108: The performance of models of different sizes on PhysBench. Figure 109: The performance of models of different sizes on PhysBench-Scene. Figure 110: The performance of models of different sizes on PhysBench-Dynamics. 143 Published as conference paper at ICLR 2025 Figure 111: The performance of models of different sizes on TextVQA (Singh et al., 2019). Figure 112: The performance of models of different sizes on MathVista (Lu et al., 2024b). Figure 113: The performance of models of different sizes on MMMU (Yue et al., 2024). 144 Published as conference paper at ICLR 2025 Size Format 2Property Relationships Scene (cid:1)Dynamics Avg Image VLM 1B merge MolmoE-1B Deitke et al. (2024) 7B merge MolmoE-7B Deitke et al. (2024) 7B merge MolmoE-7B-D Deitke et al. (2024) 72B merge MolmoE-72B Deitke et al. (2024) 3B merge MiniCPM2 (Yao et al., 2024) 4B merge MiniCPM2.5 (Yao et al., 2024) 8B merge MiniCPM2.6 (Yao et al., 2024) 2B merge Xinyuan-VL (Group, 2024) 2B merge Aquila-VL (Gu et al., 2024) 1B merge DeepSeek-VL-1B (Lu et al., 2024a) 7B merge DeepSeek-VL-7B (Lu et al., 2024a) 3B merge PaliGemma2-3B (Steiner et al., 2024) PaliGemma2-10B (Steiner et al., 2024) 10B merge 43.17 42.35 49.48 55.13 45.62 44.26 51.43 47.97 47.04 38.37 43.79 22.03 35.31 General VLM + Interleaved data Phi-3.5V (AzureML, 2024) NVILA-8B (Liu et al., 2024f) NVILA-15B (Liu et al., 2024f) NVILA-Lite-8B (Liu et al., 2024f) NVILA-Lite-15B (Liu et al., 2024f) mPLUG-Owl3-1B (Ye et al., 2024) mPLUG-Owl3-2B (Ye et al., 2024) mPLUG-Owl3-7B (Ye et al., 2024) InternVL2-1B (Wang et al., 2024e) InternVL2-2B (Wang et al., 2024e) InternVL2-4B (Wang et al., 2024e) InternVL2-8B (Wang et al., 2024e) InternVL2-26B (Wang et al., 2024e) InternVL2-40B (Wang et al., 2024e) InternVL2-76B (Wang et al., 2024e) InternVL2.5-1B (Gao et al., 2024b) InternVL2.5-2B (Gao et al., 2024b) InternVL2.5-4B (Gao et al., 2024b) InternVL2.5-8B (Gao et al., 2024b) InternVL2.5-26B (Gao et al., 2024b) InternVL2.5-38B (Gao et al., 2024b) InternVL2.5-78B (Gao et al., 2024b) o1 (Jaech et al., 2024) seq seq seq seq seq seq seq seq seq seq seq seq 4B 8B 15B 8B 15B 1B 2B 7B 1B 2B 4B 8B 26B merge 40B merge 76B merge 1B 2B 4B 8B 26B merge 38B merge 78B merge - merge seq seq seq seq 45.72 55.79 59.16 53.81 55.44 38.02 40.92 49.25 37.05 44.17 47.12 49.05 51.92 55.79 57.65 44.25 49.63 51.03 55.87 59.08 58.77 60.32 59.27 58.06 63.33 67.78 72.08 44.17 47.08 67.92 57.50 62.50 40.28 55.97 23.33 41.53 40.15 40.29 42.34 39.25 40.15 31.54 35.11 45.62 33.06 35.06 39.96 43.58 45.20 50.05 52.43 33.30 38.15 44.77 48.67 58.33 67.51 62.13 73.79 31.12 28.86 30.81 36.61 27.53 25.68 29.44 33.91 30.15 31.65 30.10 24.75 28.73 33.02 33.95 38.78 34.62 38.11 21.87 26.69 35.90 22.84 30.54 30.94 27.05 37.94 35.86 38.07 26.87 29.44 31.34 29.35 36.61 39.04 37.32 40. 41.58 42.22 47.18 46.20 39.92 37.95 45.44 42.22 45.49 38.12 38.71 31.73 39.69 39.40 43.43 45.72 41.17 44.38 33.68 35.64 40.61 34.92 35.64 39.76 39.47 39.34 41.33 40.12 38.13 38.39 41.79 41.20 41.79 45.00 46.11 49.22 40.63 40.41 45.18 48.67 38.66 37.35 44.89 43.09 43.22 36.58 39.46 25.94 35.40 39.75 43.82 46.91 42.55 44.93 31.68 34.87 42.83 32.35 36.57 39.71 40.00 43.50 45.66 46.77 36.15 39.22 42.44 43.88 48.56 51.94 51.16 55.11 Table 28: Evaluation results for latest 36 VLMs. The evaluation of General VLMs is based on the data from Video and Image VLM evaluations, with the addition of interleaved data. Seq refers to sequential input of frames of videos, while merge refers to merging video frames into single image. Results of original 39 VLMs can be seen in Table 3. Published as conference paper at ICLR 2025 Model Size Identify Comparison Static Dynamic Perception Prediction Judgment Reasoning Avg MolmoE-1B Deitke et al. (2024) MolmoE-7B Deitke et al. (2024) MolmoE-7B-D Deitke et al. (2024) MolmoE-72B Deitke et al. (2024) MiniCPM2 (Yao et al., 2024) MiniCPM2.5 (Yao et al., 2024) MiniCPM2.6 (Yao et al., 2024) Xinyuan-VL (Group, 2024) Aquila-VL (Gu et al., 2024) DeepSeek-VL-1B (Lu et al., 2024a) DeepSeek-VL-7B (Lu et al., 2024a) PaliGemma2-3B (Steiner et al., 2024) PaliGemma2-10B (Steiner et al., 2024) 56.76 1B 54.84 7B 7B 67.04 72B 68.70 62.41 3B 49.48 4B 65.82 8B 63.12 2B 62.51 2B 48.82 1B 55.36 7B 3B 18.31 10B 46.21 58.59 Phi-3.5-Vision-Instruct (AzureML, 2024) 4B 8B NVILA-8B (Liu et al., 2024f) 70.71 15B 75.85 NVILA-15B (Liu et al., 2024f) 69.05 8B NVILA-Lite-8B (Liu et al., 2024f) 15B 68.09 NVILA-Lite-15B (Liu et al., 2024f) 46.56 1B mPLUG-Owl3-1B (Ye et al., 2024) 54.23 2B mPLUG-Owl3-2B (Ye et al., 2024) 65.39 7B mPLUG-Owl3-7B (Ye et al., 2024) 48.04 1B InternVL2-1B (Wang et al., 2024e) 60.77 2B InternVL2-2B (Wang et al., 2024e) 61.20 4B InternVL2-4B (Wang et al., 2024e) 8B InternVL2-8B (Wang et al., 2024e) 64.95 26B 66.35 InternVL2-26B (Wang et al., 2024e) 40B 69.83 InternVL2-40B (Wang et al., 2024e) 76B 71.23 InternVL2-76B (Wang et al., 2024e) 63.12 1B InternVL2.5-1B (Gao et al., 2024b) 68.35 2B InternVL2.5-2B (Gao et al., 2024b) 66.52 4B InternVL2.5-4B (Gao et al., 2024b) 8B InternVL2.5-8B (Gao et al., 2024b) 72.62 26B 75.94 InternVL2.5-26B (Gao et al., 2024b) 38B 74.02 InternVL2.5-38B (Gao et al., 2024b) 38B 78.47 InternVL2.5-78B (Gao et al., 2024b) 78.73 o1 (Jaech et al., 2024) - Image VLM 64.74 72.01 74.44 76.49 45.71 45.90 72.20 63.43 66.04 40.49 59.51 23.69 42.35 32.42 32.49 35.63 44.51 32.49 40.07 40.07 35.77 35.02 29.90 35.22 25.19 27.03 39.07 37.75 49.67 58.94 38.41 51.66 56.95 41.06 51.66 43.05 41.72 21.19 39.07 General VLM + Interleaved data 35.84 44.10 46.28 41.77 45.60 31.26 30.65 36.59 28.67 31.19 36.18 36.79 40.55 44.91 47.37 29.56 35.02 38.84 43.00 45.94 46.89 46.21 44. 39.09 38.05 40.03 37.47 38.31 30.64 34.24 44.68 32.36 34.24 38.31 42.28 44.02 49.01 51.10 32.41 37.58 43.37 47.39 58.04 67.38 61.17 73.38 50.33 64.71 64.71 59.48 58.17 41.83 41.83 56.21 39.22 43.79 56.86 56.86 58.56 60.13 64.05 41.18 42.48 60.78 60.78 60.78 69.28 71.90 78.43 48.22 49.85 54.74 54.33 47.40 40.88 52.50 49.34 50.97 41.59 45.77 24.57 44.04 48.32 55.76 57.70 53.82 59.53 41.28 40.77 53.62 40.06 43.12 53.62 52.19 52.31 55.66 52.91 47.09 47.50 54.43 51.58 55.15 58.92 60.35 55.56 37.84 39.22 43.82 43.13 36.33 36.06 42.03 39.56 41.00 35.78 35.23 37.29 37.57 39.55 44.18 48.54 41.18 45.41 36.22 37.58 41.59 35.60 35.47 42.95 39.96 47.11 43.50 40.16 41.39 39.28 43.02 42.82 40.84 41.93 42.61 43. 41.88 45.01 49.00 50.71 44.86 41.43 48.57 45.30 46.72 40.17 40.74 20.51 40.17 32.99 34.02 33.12 33.12 31.46 25.83 31.46 30.69 30.31 33.25 27.24 32.35 38.18 28.13 33.38 25.96 29.92 30.69 31.33 33.89 38.87 39.77 52.56 29.22 25.20 27.19 33.00 24.12 23.62 26.01 30.86 28.68 30.47 27.73 25.20 26.59 30.64 30.45 35.42 31.16 34.71 18.53 23.89 32.50 20.68 27.62 26.28 22.40 32.18 32.17 34.47 24.03 27.00 27.91 26.09 33.60 36.57 34.90 40.35 40.63 40.41 45.18 48.67 38.66 37.35 44.89 43.09 43.22 36.58 39.46 25.94 35.40 39.75 43.82 46.91 42.55 44.93 31.68 34.87 42.83 32.35 36.57 39.71 40.00 44.18 45.66 46.77 36.15 39.22 42.44 43.88 48.56 51.94 51.16 55. Table 29: Evaluation Results for 36 new VLMs Categorized by Ability Dimensions. PhysBench includes two evaluation dimensions: ability and task. Table 28 presents results categorized by the task-type dimension. 146 Published as conference paper at ICLR"
        },
        {
            "title": "Image VLM",
            "content": "MolmoE-1B Deitke et al. (2024) MolmoE-7B Deitke et al. (2024) MolmoE-7B-D Deitke et al. (2024) MolmoE-72B Deitke et al. (2024) MiniCPM2 (Yao et al., 2024) MiniCPM2.5 (Yao et al., 2024) MiniCPM2.6 (Yao et al., 2024) Xinyuan-VL (Group, 2024) Aquila-VL (Gu et al., 2024) DeepSeek-VL-1B (Lu et al., 2024a) DeepSeek-VL-7B (Lu et al., 2024a) PaliGemma2-3B (Steiner et al., 2024) PaliGemma2-10B (Steiner et al., 2024) 1B 56.85 7B 48.01 65.16 7B 72B 63.95 56.52 3B 46.26 4B 54.09 8B 54.25 2B 55.63 2B 48.18 1B 51.99 7B 20.80 3B 10B 40.03 General VLM + Interleaved data"
        },
        {
            "title": "49.05\nPhi-3.5-Vision-Instruct (AzureML, 2024) 4B\n8B\nNVILA-8B (Liu et al., 2024f)\n61.35\n15B 774.18\nNVILA-15B (Liu et al., 2024f)\n8B\nNVILA-Lite-8B (Liu et al., 2024f)\n60.14\n15B 61.18\nNVILA-Lite-15B (Liu et al., 2024f)\n37.95\n1B\nmPLUG-Owl3-1B (Ye et al., 2024)\n44.19\n2B\nmPLUG-Owl3-2B (Ye et al., 2024)\n57.02\n7B\nmPLUG-Owl3-7B (Ye et al., 2024)\n38.99\n1B\nInternVL2-1B (Wang et al., 2024e)\n57.89\n2B\nInternVL2-2B (Wang et al., 2024e)\n49.39\n4B\nInternVL2-4B (Wang et al., 2024e)\n8B\nInternVL2-8B (Wang et al., 2024e)\n58.75\n26B 57.19\nInternVL2-26B (Wang et al., 2024e)\n40B 64.30\nInternVL2-40B (Wang et al., 2024e)\n76B 67.76\nInternVL2-76B (Wang et al., 2024e)\n58.58\n1B\nInternVL2.5-1B (Gao et al., 2024b)\n63.95\n2B\nInternVL2.5-2B (Gao et al., 2024b)\n56.15\n4B\nInternVL2.5-4B (Gao et al., 2024b)\n8B\nInternVL2.5-8B (Gao et al., 2024b)\n67.94\n26B 73.31\nInternVL2.5-26B (Gao et al., 2024b)\n38B 68.11\nInternVL2.5-38B (Gao et al., 2024b)\n5.22\n38B\nInternVL2.5-78B (Gao et al., 2024b)\n75.22\n-\no1 (Jaech et al., 2024)",
            "content": "23.89 52.67 30.03 56.67 36.52 63.33 51.19 64.00 42.66 66.56 50.17 45.82 58.70 70.23 47.44 71.67 43.69 64.33 32.76 49.33 39.59 52.33 17.75 17.67 29.01 45.67 44.37 65.00 57.34 75.67 55.29 73.67 59.04 75.00 55.97 72.33 44.37 50.33 37.54 61.00 36.52 71.67 34.47 61.67 34.47 69.33 39.93 71.00 36.18 74.67 44.71 68.00 55.29 67.00 60.41 64.67 35.49 71.00 42.32 72.67 52.90 75.00 50.85 75.33 57.00 72.67 51.88 73.00 55.63 73.67 49.83 74.67 39.39 39.46 42.73 50.25 37.26 41.80 44.71 40.38 40.38 33.29 39.46 24.34 32.51 40.45 48.90 50.53 45.42 49.25 34.07 35.84 43.79 31.44 35.13 42.51 42.09 47.69 49.82 51.24 34.49 40.38 43.29 47.69 50.67 53.16 52.24 51.31 Table 30: Evaluation Results for 36 new VLMs in PhysBench Physical Object 2Property Sub-task (the fifth last column of Table 28). Published as conference paper at ICLR"
        },
        {
            "title": "Model",
            "content": "MolmoE-1B Deitke et al. (2024) MolmoE-7B Deitke et al. (2024) MolmoE-7B-D Deitke et al. (2024) MolmoE-72B Deitke et al. (2024) MiniCPM2 (Yao et al., 2024) MiniCPM2.5 (Yao et al., 2024) MiniCPM2.6 (Yao et al., 2024) Xinyuan-VL (Group, 2024) Aquila-VL (Gu et al., 2024) DeepSeek-VL-1B (Lu et al., 2024a) DeepSeek-VL-7B (Lu et al., 2024a) PaliGemma2-3B (Steiner et al., 2024) PaliGemma2-10B (Steiner et al., 2024) 1B 7B 7B 72B 3B 4B 8B 2B 2B 1B 7B 3B 10B 39.29 42.86 58.93 76.79 50.00 66.07 71.43 55.36 69.64 41.07 37.50 23.21 48.21 51.13 61.65 60.15 67.29 45.49 36.84 67.67 54.14 60.90 43.23 51.13 21.05 45.11 General VLM + Interleaved data Phi-3.5-Vision-Instruct (AzureML, 2024) NVILA-8B (Liu et al., 2024f) NVILA-15B (Liu et al., 2024f) NVILA-Lite-8B (Liu et al., 2024f) NVILA-Lite-15B (Liu et al., 2024f) mPLUG-Owl3-1B (Ye et al., 2024) mPLUG-Owl3-2B (Ye et al., 2024) mPLUG-Owl3-7B (Ye et al., 2024) InternVL2-1B (Wang et al., 2024e) InternVL2-2B (Wang et al., 2024e) InternVL2-4B (Wang et al., 2024e) InternVL2-8B (Wang et al., 2024e) InternVL2-26B (Wang et al., 2024e) InternVL2-40B (Wang et al., 2024e) InternVL2-76B (Wang et al., 2024e) InternVL2.5-1B (Gao et al., 2024b) InternVL2.5-2B (Gao et al., 2024b) InternVL2.5-4B (Gao et al., 2024b) InternVL2.5-8B (Gao et al., 2024b) InternVL2.5-26B (Gao et al., 2024b) InternVL2.5-38B (Gao et al., 2024b) InternVL2.5-78B (Gao et al., 2024b) o1 (Jaech et al., 2024) 4B 8B 15B 8B 15B 1B 2B 7B 1B 2B 4B 8B 26B 40B 76B 1B 2B 4B 8B 26B 38B 38B - 57.14 76.79 76.79 73.21 62.50 46.43 60.71 69.64 44.64 78.57 69.64 75.00 78.57 80.36 80.36 48.21 48.21 75.00 67.86 75.00 85.71 80.36 83.93 59.45 67.35 69.07 61.86 67.35 39.52 41.58 60.14 53.26 64.95 59.11 67.01 64.95 64.26 67.01 53.26 57.73 62.54 72.16 65.64 68.73 69.42 76.63 72.28 75.44 79.30 76.49 39.65 51.23 68.77 62.46 64.56 31.93 61.40 27.02 36.14 66.67 70.88 80.35 71.23 73.33 31.93 30.88 69.82 35.79 73.68 75.44 77.89 73.68 77.54 82.81 47.72 68.07 72.63 78.25 79.65 75.79 77.54 78.25 48.33 53.33 66.67 68.33 46.67 43.33 56.67 50.00 61.67 50.00 61.67 26.67 41. 55.00 73.33 75.00 60.00 70.00 35.00 43.33 58.33 50.00 63.33 55.00 56.67 63.33 73.33 76.67 50.00 50.00 55.00 75.00 76.67 80.00 86.67 80.00 50.56 50.56 59.55 74.16 59.55 67.42 73.03 61.80 59.55 51.69 59.55 13.48 50.56 30.36 26.83 27.59 27.18 26.97 29.46 34.09 37.34 27.66 34.30 28.08 31.05 34.30 40.18 41.91 25.73 28.08 34.92 37.00 51.66 64.59 56.43 71.51 Table 31: Evaluation Results for 36 new VLMs in PhysBench Object Relationships Sub-task (the forth last column of Table 28). 148 Published as conference paper at ICLR"
        },
        {
            "title": "Size Temperature Viewpoint Air Light",
            "content": "MolmoE-1B Deitke et al. (2024) MolmoE-7B Deitke et al. (2024) MolmoE-7B-D Deitke et al. (2024) MolmoE-72B Deitke et al. (2024) MiniCPM2 (Yao et al., 2024) MiniCPM2.5 (Yao et al., 2024) MiniCPM2.6 (Yao et al., 2024) Xinyuan-VL (Group, 2024) Aquila-VL (Gu et al., 2024) DeepSeek-VL-1B (Lu et al., 2024a) DeepSeek-VL-7B (Lu et al., 2024a) PaliGemma2-3B (Steiner et al., 2024) PaliGemma2-10B (Steiner et al., 2024) Image VLM 1B 7B 7B 72B 3B 4B 8B 2B 2B 1B 7B 3B 10B General VLM + Interleaved data 50.00 64.71 79.41 76.79 66.18 48.53 67.65 64.71 61.76 50.00 58.82 17.65 48.53 Phi-3.5-Vision-Instruct (AzureML, 2024) 4B NVILA-8B (Liu et al., 2024f) 8B 15B NVILA-15B (Liu et al., 2024f) NVILA-Lite-8B (Liu et al., 2024f) 8B 15B NVILA-Lite-15B (Liu et al., 2024f) 1B mPLUG-Owl3-1B (Ye et al., 2024) 2B mPLUG-Owl3-2B (Ye et al., 2024) 7B mPLUG-Owl3-7B (Ye et al., 2024) 1B InternVL2-1B (Wang et al., 2024e) 2B InternVL2-2B (Wang et al., 2024e) 4B InternVL2-4B (Wang et al., 2024e) 8B InternVL2-8B (Wang et al., 2024e) 26B InternVL2-26B (Wang et al., 2024e) 40B InternVL2-40B (Wang et al., 2024e) 76B InternVL2-76B (Wang et al., 2024e) 1B InternVL2.5-1B (Gao et al., 2024b) 2B InternVL2.5-2B (Gao et al., 2024b) 4B InternVL2.5-4B (Gao et al., 2024b) InternVL2.5-8B (Gao et al., 2024b) 8B 26B InternVL2.5-26B (Gao et al., 2024b) 38B InternVL2.5-38B (Gao et al., 2024b) 38B InternVL2.5-78B (Gao et al., 2024b) - o1 (Jaech et al., 2024) 51.47 72.06 83.82 76.47 82.35 57.35 63.24 76.47 45.59 50.00 52.94 73.53 76.47 94.12 91.18 52.94 58.82 67.65 69.12 88.24 88.24 89.71 95.59 30.73 21.11 42.70 67.29 20.83 13.85 19.51 33.46 24.79 32.33 25.64 26.20 27. 33.65 31.29 38.36 31.39 38.55 10.84 25.35 37.51 18.47 28.46 28.37 17.91 40.25 37.89 39.87 20.17 21.58 28.09 18.94 34.31 41.47 40.62 47.31 26.00 30.21 36.00 33.76 86.00 26.84 76.49 68.33 40.00 31.12 42.00 35.21 42.00 36.21 48.00 32.21 28.00 33.48 32.00 29.75 28.00 32.85 28.00 23.66 40.00 28.57 50.91 30.85 50.91 33.58 52.73 35.85 25.45 35.30 61.82 34.03 50.91 29.48 29.09 25.75 78.18 30.66 18.18 25.75 21.82 31.48 54.55 31.39 36.36 32.67 29.09 33.30 58.18 29.39 52.73 32.58 14.55 32.03 16.36 35.58 29.09 32.30 47.27 36.49 74.55 34.58 78.18 32.48 90.91 29.30 81.82 30.30 Table 32: Evaluation Results for 36 new VLMs in PhysBench Physical Scene Understanding Subtask (the third last column of Table 28. 149 Published as conference paper at ICLR Model Size Collision Throwing Manipulation Fluid Chemistry Others MolmoE-1B Deitke et al. (2024) MolmoE-7B Deitke et al. (2024) MolmoE-7B-D Deitke et al. (2024) MolmoE-72B Deitke et al. (2024) MiniCPM2 (Yao et al., 2024) MiniCPM2.5 (Yao et al., 2024) MiniCPM2.6 (Yao et al., 2024) Xinyuan-VL (Group, 2024) Aquila-VL (Gu et al., 2024) DeepSeek-VL-1B (Lu et al., 2024a) DeepSeek-VL-7B (Lu et al., 2024a) PaliGemma2-3B (Steiner et al., 2024) PaliGemma2-10B (Steiner et al., 2024) Image VLM 32.72 28.26 38.56 37.02 34.82 433.44 35.29 35.94 38.10 32.10 31.64 37.63 37.17 33.02 28.74 39.43 40.38 34.20 36.82 45.37 39.67 40.14 33.49 35.39 34.68 40.62 1B 7B 7B 72B 3B 4B 8B 2B 2B 1B 7B 3B 10B General VLM + Interleaved data Phi-3.5-Vision-Instruct (AzureML, 2024) 4B 8B NVILA-8B (Liu et al., 2024f) 15B NVILA-15B (Liu et al., 2024f) 8B NVILA-Lite-8B (Liu et al., 2024f) 15B NVILA-Lite-15B (Liu et al., 2024f) 1B mPLUG-Owl3-1B (Ye et al., 2024) 2B mPLUG-Owl3-2B (Ye et al., 2024) 7B mPLUG-Owl3-7B (Ye et al., 2024) 1B InternVL2-1B (Wang et al., 2024e) 2B InternVL2-2B (Wang et al., 2024e) 4B InternVL2-4B (Wang et al., 2024e) 8B InternVL2-8B (Wang et al., 2024e) 26B InternVL2-26B (Wang et al., 2024e) 40B InternVL2-40B (Wang et al., 2024e) 76B InternVL2-76B (Wang et al., 2024e) 1B InternVL2.5-1B (Gao et al., 2024b) 2B InternVL2.5-2B (Gao et al., 2024b) 4B InternVL2.5-4B (Gao et al., 2024b) 8B InternVL2.5-8B (Gao et al., 2024b) 26B InternVL2.5-26B (Gao et al., 2024b) 38B InternVL2.5-38B (Gao et al., 2024b) 38B InternVL2.5-78B (Gao et al., 2024b) - o1 (Jaech et al., 2024) 33.94 38.76 40.42 38.31 41.48 29.41 34.69 37.10 31.83 37.10 35.29 38.46 38.91 43.14 40.27 40.27 33.48 38.76 39.52 38.16 43.89 42.08 48.27 38.72 45.13 43.23 46.08 40.38 32.07 35.63 36.34 29.22 32.54 41.09 38.95 41.09 47.51 42.99 37.05 40.38 42.52 36.82 43.23 41.81 46.32 41.09 43.47 36.68 39.45 30.90 34.92 27.89 38.19 38.44 42.21 38.44 35.43 25.13 33.92 29.19 31.24 29.31 28.59 31.60 23.16 24.85 28.35 30.16 29.43 27.50 29.19 26.42 20.39 22.07 26.42 28.11 28.83 24.25 25.69 31.85 33.53 37. 44.75 53.55 50.15 48.30 34.88 35.03 43.06 39.81 42.75 39.35 37.04 34.57 35.80 43.67 43.21 54.17 38.43 45.83 42.59 38.58 43.06 41.82 35.65 47.53 38.43 37.96 39.20 37.04 43.36 42.13 45.37 46.14 38.58 36.88 37.35 43.52 71.62 67.57 68.92 63.51 67.57 54.05 64.86 50.00 59.46 48.65 54.05 36.49 56.76 59.46 70.27 77.03 77.03 70.27 51.35 56.76 67.57 48.65 50.00 63.51 58.11 58.11 67.57 64.86 52.70 62.16 60.81 67.57 67.57 78.38 75.68 71.62 57.48 67.01 72.11 80.61 67.69 60.88 76.19 64.63 75.17 52.04 61.90 18.03 54.08 58.58 69.75 71.93 65.40 72.21 45.78 50.14 64.03 43.32 47.96 52.04 61.31 64.58 74.39 74.93 49.86 55.86 63.49 70.03 78.75 83.92 85.29 89. Table 33: Evaluation Results for 36 new VLMs in PhysBench Physics-based (cid:1)Dynamics Sub-task (the second last column of Table 28)."
        }
    ],
    "affiliations": [
        "Toyota Research Institute",
        "UC Berkeley",
        "University of Southern California"
    ]
}