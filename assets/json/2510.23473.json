{
    "paper_title": "Video-Thinker: Sparking \"Thinking with Videos\" via Reinforcement Learning",
    "authors": [
        "Shijian Wang",
        "Jiarui Jin",
        "Xingjian Wang",
        "Linxin Song",
        "Runhao Fu",
        "Hecheng Wang",
        "Zongyuan Ge",
        "Yuan Lu",
        "Xuelian Cheng"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in image reasoning methods, particularly \"Thinking with Images\", have demonstrated remarkable success in Multimodal Large Language Models (MLLMs); however, this dynamic reasoning paradigm has not yet been extended to video reasoning tasks. In this paper, we propose Video-Thinker, which empowers MLLMs to think with videos by autonomously leveraging their intrinsic \"grounding\" and \"captioning\" capabilities to generate reasoning clues throughout the inference process. To spark this capability, we construct Video-Thinker-10K, a curated dataset featuring autonomous tool usage within chain-of-thought reasoning sequences. Our training strategy begins with Supervised Fine-Tuning (SFT) to learn the reasoning format, followed by Group Relative Policy Optimization (GRPO) to strengthen this reasoning capability. Through this approach, Video-Thinker enables MLLMs to autonomously navigate grounding and captioning tasks for video reasoning, eliminating the need for constructing and calling external tools. Extensive experiments demonstrate that Video-Thinker achieves significant performance gains on both in-domain tasks and challenging out-of-domain video reasoning benchmarks, including Video-Holmes, CG-Bench-Reasoning, and VRBench. Our Video-Thinker-7B substantially outperforms existing baselines such as Video-R1 and establishes state-of-the-art performance among 7B-sized MLLMs."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 2 ] . [ 1 3 7 4 3 2 . 0 1 5 2 : r a"
        },
        {
            "title": "Preprint",
            "content": "VIDEO-THINKER: VIDEOS VIA REINFORCEMENT LEARNING SPARKING THINKING WITH Shijian Wang1,2,3, Jiarui Jin3, Xingjian Wang2, Linxin Song4, Runhao Fu2, Hecheng Wang5, Zongyuan Ge2, Yuan Lu3, Xuelian Cheng2 1Southeast University, 2Monash University, 3Xiaohongshu Inc., 4University of Southern California, 5Fudan University {wangshijian,jinjiarui,luyuan3}@xiaohongshu.com Code: shijian2001/Video-Thinker Model: ShijianW01/Video-Thinker-7B"
        },
        {
            "title": "ABSTRACT",
            "content": "Recent advances in image reasoning methods, particularly Thinking with Images, have demonstrated remarkable success in Multimodal Large Language Models (MLLMs); however, this dynamic reasoning paradigm has not yet been extended to video reasoning tasks. In this paper, we propose Video-Thinker, which empowers MLLMs to think with videos by autonomously leveraging their intrinsic grounding and captioning capabilities to generate reasoning clues throughout the inference process. To spark this capability, we construct Video-Thinker-10K, curated dataset featuring autonomous tool usage within chain-of-thought reasoning sequences. Our training strategy begins with Supervised Fine-Tuning (SFT) to learn the reasoning format, followed by Group Relative Policy Optimization (GRPO) to strengthen this reasoning capability. Through this approach, VideoThinker enables MLLMs to autonomously navigate grounding and captioning tasks for video reasoning, eliminating the need for constructing and calling external tools. Extensive experiments demonstrate that Video-Thinker achieves significant performance gains on both in-domain tasks and challenging out-of-domain video reasoning benchmarks, including Video-Holmes, CG-Bench-Reasoning, and VRBench. Our Video-Thinker-7B substantially outperforms existing baselines such as Video-R1 and establishes state-of-the-art performance among 7B-sized MLLMs."
        },
        {
            "title": "INTRODUCTION",
            "content": "Multimodal Large Language Models (MLLMs) have embraced revolutionary paradigm shift toward Thinking with Images for image understanding and reasoning tasks, evolving from passively treating images as static context to actively localizing, zooming in, and reasoning over image content during the thinking process (Zheng et al., 2025; Liu et al., 2024b; Shen et al., 2024; Wang et al., 2025c; Ma et al., 2024). This dynamic multimodal reasoning paradigm has yielded substantial advances on MLLMs across diverse image reasoning tasks, including visual question answering (Liu et al., 2023; Zhao et al., 2025; Gupta & Kembhavi, Figure 1: Overall Performance of Video-Thinker Equal contribution. Work done when Shijian internship at Xiaohongshu Inc. Corresponding Authors"
        },
        {
            "title": "Preprint",
            "content": "Figure 2: Video-Thinker integrates grounding and captioning capabilities throughout the reasoning process using end-to-end reinforcement learning. 2023; Liu et al., 2024c), visual mathematical problem solving (Chen et al., 2025a; Shao et al., 2024a; Wang et al., 2025a; Yue et al., 2024; Li et al., 2025a; An et al., 2025), and complex scene understanding (Luo et al., 2024; You et al., 2023; Yang et al., 2023; Zhang et al., 2025; Zheng et al., 2025; Ma et al., 2025b; Lin et al., 2025). However, the extension of these capabilities to video understanding presents significant challenges. Unlike static images, videos inherently contain temporal dependencies, motion patterns, and evolving visual narratives that require sophisticated temporal reasoning mechanisms, whereas MLLMs struggle to dynamically manipulate and reason over temporal sequences without relying on explicitly pre-designed chain-of-thought prompting strategies (Fei et al., 2024; Feng et al., 2025; Shi et al., 2024; An et al., 2024). In this paper, we propose novel framework named Video-Thinker to enhance MLLMs by enabling them to perform visual reasoning through structured video analysis capabilities. Drawing inspiration from spatial visual operations in Thinking with Images (OpenAI, 2024) for image understanding such as crop for region localization and zoom-in for detailed region comprehension we introduce the following temporal visual operations - namely grounding and captioning. The grounding operation serves as temporal localization mechanism that identifies and extracts key frames containing critical visual information within the video sequence, while the captioning operation functions as comprehension mechanism that analyzes these key frames to extract, interpret, and synthesize relevant visual cues into coherent understanding. Fortunately, these video localization and comprehension capabilities can be developed within MLLMs themselves, thereby eliminating the need for MLLMs to adapt to and invoke external handcrafted tools. Hence, our Video-Thinker can enable structured temporal reasoning through chain-of-thought (CoT) processes, allowing models to autonomously navigate and analyze specific temporal segments rather than treating videos as monolithic inputs. The framework orchestrates these temporal manipulation capabilities through systematic reasoning traces that synthesize visual cues across multiple video segments. Our approach differs fundamentally from previous investigations in two key aspects. First, unlike video-of-thoughts methodologies that rely on sophisticated pre-designed CoT processes (Fei et al., 2024), our framework develops intrinsic temporal reasoning capabilities that emerge naturally from the training process. Second, in contrast to general visual reasoning models that require extensive datasets exceeding 160K samples (Feng et al., 2025), our approach demonstrates that effective video reasoning capabilities can be achieved with significantly greater efficiency using only 10K carefully curated training examples."
        },
        {
            "title": "Preprint",
            "content": "To instantiate our framework, we carefully construct Video-Thinker-10K, curated training dataset of 10K samples spanning diverse video-reasoning tasks and domains. Each sample comprises strategically selected key video segments, detailed captions describing visual clues for each temporal window, and structured reasoning traces that demonstrate how to synthesize these multimodal cues for complex video understanding tasks. As illustrated in Figure 2, our reasoning trace adopts structured format wherein each key video segment is systematically processed through three specialized annotation tags: the <time></time> tag for precise temporal localization, the <caption></caption> tag for comprehensive visual cue extraction, and the <think></think> tag for analytical reasoning that synthesizes the extracted visual information. Our training methodology employs two-stage approach: we first conduct supervised fine-tuning (SFT) using our curated thought processes as ground truth supervision to establish foundational formatfollowing capabilities. We subsequently apply Group Relative Policy Optimization (GRPO) (Shao et al., 2024b) for reinforcement learning, where only the final answer serves as the outcome reward. This approach enables the model to intrinsically acquire both grounding and captioning capabilities, facilitating autonomous temporal navigation for sophisticated video reasoning tasks. Our extensive experiments demonstrate that Video-Thinker achieves the state-of-the-art (SOTA) performance among 7B-sized MLLMs across various challenging out-of-domain video reasoning benchmarks, including Video-Holmes (Cheng et al., 2025), CG-Bench-Reasoning Chen et al. (2024a), and VRBench (Yu et al., 2025b), as demonstrated in Figure 1. Our main contributions are summarized as follows: (i) proposing new paradigm (Video-Thinker) of Thinking with Videos by intrinsically integrating grounding and captioning capabilities within the CoT process, eliminating the dependency on external tools; (ii) contributing meticulously curated video reasoning dataset (Video-Thinker-10K) encompassing comprehensive localization annotations and rich comprehension information; and (iii) empirically setting new SOTA performances across multiple video reasoning benchmarks."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Recent advances in reinforcement learning-based post-training have demonstrated significant improvements in reasoning capabilities, as evidenced by OpenAI-o1 (Jaech et al., 2024) and Deepseek-R1 (Guo et al., 2025b). Building upon this foundation, the field of MLLMs is undergoing paradigmatic shift in how visual information is integrated into reasoning processes. Traditionally, MLLMs have treated images as static inputs, relegating the reasoning process entirely to the textual domain (Su et al., 2025). An emerging paradigm, however, elevates visual information to an explicit, manipulable intermediate within the reasoning process itself, transforming vision from passive input into an active cognitive tool (OpenAI, 2024). This approach is exemplified by several recent works: Deepeyes (Zheng et al., 2025) employs end-to-end reinforcement learning to train models that autonomously invoke visual tools (e.g., magnification) while interleaving visual and textual CoT reasoning, effectively enabling models to Think with Images. Visual-ARFT (Liu et al., 2025) utilizes GRPO (Shao et al., 2024b) to develop capabilities in task planning, stepwise reasoning, and tool use, allowing models to strategically employ Python-based image-processing operators. The natural extension of these advances lies in video reasoning, which represents core capability for MLLMs seeking to capture the logical structure of temporal visual contenta crucial step beyond mere video perception toward genuine video understanding (Wang & Peng, 2025; Dang et al., 2025; Yu et al., 2025a). Recent efforts have begun addressing this challenge: Video-R1 (Feng et al., 2025) extends GRPO into the video domain, promoting implicit temporal reasoning alongside spatial reasoning capabilities. VideoChat-R1 (Li et al., 2025c) leverages reinforcement fine-tuning to strengthen spatiotemporal localization while preserving conversational proficiency. Temporal-R1 (Li et al., 2025b) employs explicit temporal grounding rewards and variance-aware data selection strategies to enhance both semantic and temporal reasoning with improved data efficiency. Despite these advances, current approaches remain largely confined to either temporal localization or standalone video reasoning, falling short of integrating temporal grounding seamlessly into the CoT processes. Our proposed Video-Thinker framework extending the paradigm of Think with Images enables MLLMs to Think with Videos by facilitating dynamic navigation of temporal content within the reasoning process. Specifically, Video-Thinker incorporates grounding and captioning"
        },
        {
            "title": "Preprint",
            "content": "Figure 3: Data synthesis pipeline of Video-Thinker-10K where the data distribution is depicted in Figure 5 in Appendix B. capabilities as integral components of the CoT reasoning, allowing MLLMs to systematically attend to, interpret, and analyze relevant temporal segments throughout video-based tasks."
        },
        {
            "title": "3 THINK WITH VIDEOS: FROM DATA SYNTHESIS TO MODEL TRAINING",
            "content": "As video reasoning tasks require temporal localization and comprehension capabilities in MLLMs, we propose grounding and captioning as fundamental anchors for model enhancement. To address this requirement, we first establish high-quality curated data termed Video-Thinker-10K, using new hindsight-curation reasoning method, as detailed in Section 3.1. Subsequently, we train our Video-Thinker models on these datasets through supervised fine-tuning and reinforcement learning approaches, as described in Section 3.2. 3.1 DATA SYNTHESIS VIA HINDSIGHT-CURATION REASONING Here, we curate diverse collection of source data from the following six prominent datasets, namely ActivityNet (Caba Heilbron et al., 2015), TutorialVQA (Colas et al., 2019), YouCook2 (Zhou et al., 2018b), STAR (Wu et al., 2024), ScaleLong (Ma et al., 2025a), and LVBench (Wang et al., 2024). These sources span wide spectrum of domains ranging from human activities and instructional tutorials to cooking procedures, situated reasoning, and long-form content such as TV series. Within these datasets, we identified the following two complementary categories of data: (i) Caption-labeled datasets, including ActivityNet, TutorialVQA, and YouCook2, provide detailed, human-annotated captions for specific temporal intervals within key video segments but lack complex questions that require deep reasoning capabilities. (ii) QA-labeled datasets, comprising STAR, ScaleLong, and LVBench, offer challenging question-answer pairs designed for deep reasoning but lack the granular, per-segment visual descriptions essential for our structured reasoning framework. To inspire MLLMs with intrinsic capabilities for grounding and captioning, our training data curation is guided by two core principles. One is: our training data requires questions that compel MLLMs to localize multiple key segments, accurately summarize their content, and synthesize this information to derive comprehensive answers. The other one is: our training data must provide supervision through structured reasoning trace that includes the <time></time> tag for temporal localization, the <caption></caption> tag for visual cue description, and the <think></think> tag for analytical reasoning, explicitly integrating temporal actions within the CoT process. To bridge the gap between the collected source data and the expected structured data samples described above, we developed systematic data transformation pipeline, as demonstrated in Figure 3). We first applied quality filters to remove corrupted videos and exclude videos with fewer than 64 frames to ensure adequate temporal content. Our pipeline then branches into two distinct generation strategies based on dataset characteristics: (i) For caption-labeled datasets (namely, ActivityNet, TutorialVQA, YouCook2) that are rich in temporal annotations and segment descriptions, we focused on synthesizing corresponding reasoning questions. We leveraged DeepSeek-R1 (Guo et al., 2025a) to generate complex multiple-choice questions that necessitate reasoning across multiple video segments, using the existing detailed segment descriptions as the contextual foundation. (ii) For QA-labeled datasets (namely, STAR, ScaleLong, LVBench) that provide high-quality question-answer pairs but lack granular per-segment descriptions, we concentrated on generating the missing visual cues. Given the ground-truth answers and temporal annotations, we employed Gemini-2.5-Flash-Lite (Comanici et al., 2025) to produce answer-conditioned descriptive captions for video segments, ensuring that the generated visual descriptions are relevant to the reasoning process. Finally, with both question-answer pairs and segment-level visual descriptions now available across all data samples, we perform the final reasoning trace synthesis. We use DeepSeek-V3 (Liu et al., 2024a) for reverse-curation generation, where the model receives the ground-truth answer, generated"
        },
        {
            "title": "Preprint",
            "content": "visual descriptions (captions), and temporal annotations to produce high-quality reasoning processes that articulate step-by-step temporal analysis. Each trace adheres to our predefined structured format, incorporating the <time></time> tag for temporal localization, the <caption></caption> tag for visual evidence summarization, and the <think></think> tag for analytical reasoning elaboration, thereby creating complete training instances for our Video-Thinker-10K dataset. To ensure that the generated grounding and captioning components are beneficial for the final response, previous data synthesis pipelines such as Video-Holmes (Cheng et al., 2025) employ manual sampling inspection to ensure quality and relevance. To reduce the cost of human evaluation and annotation, we propose novel hindsight curation process. For each sample, the generated content within the <time></time> and <caption></caption> tags is input into Qwen2.5-VL-7B-Instruct (Bai et al., 2025) to evaluate whether the model can derive the correct answer. If the model fails to produce the accurate answer, we regenerate the reasoning trace. This iterative process repeats up to three times, ensuring that all samples are equipped with high-quality and relevant reasoning trace that effectively guides the model toward the correct solution. Also, we carefully sample from these sources to ensure balanced distribution across various tasks and domains, as detailed in Figure 5 in Appendix B. We also provide the specific prompt templates used in this generation pipeline in Appendix D. 3.2 TRAINING STRATEGY OF VIDEO-THINKER Let = (V, Q, T, ) DVideo-Thinker denote any sample in Video-Thinker-10K constructed in the above subsection, where represents the video, is the question, is the ground-truth reasoning trace containing grounding and captioning contents, and is the ground-truth answer. SFT Optimization for Format-Following. We start by Supervised Fine-tuning (SFT) to bootstrap Video-Thinkers ability to generate structured reasoning traces over grounding and captioning contents. Since pre-trained MLLMs lack exposure to our specialized reasoning format with <time></time> , <caption></caption> , and <think></think> tags, SFT provides essential cold-start initialization by teaching the model to follow high-quality reasoning patterns from our Video-Thinker10K dataset. Formally, the SFT objective is to minimize the negative log-likelihood of the target reasoning trace and final answer , where the loss function can be formulated as: LSFT(θ) = E(V,Q,Y )DVideo-Thinker [T ;Y ] (cid:88) t=1 (cid:32) log pθ [T ; ]t (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) V, Q, [T ; ]<t (cid:33) , (1) where [T ; ] denotes the concatenation of and , and pθ is the policy of Video-Thinker model parameterized by θ. Namely, the model is trained to predict each subsequent token [T ; ]t of the reasoning trace and the final answer, conditioned on the video , the question Q, and the preceding tokens [T ; ]<t. GRPO Optimization for Autonomous Navigation over Grounding and Captioning Capabilities. To achieve sophisticated video reasoning with autonomous navigation over grounding and captioning capabilities, we employ Group Relative Policy Optimization (GRPO) to further optimize VideoThinker beyond the above SFT stage. GRPO eliminates the need for value function approximation by generating multiple candidate responses for each (V, Q, ) sample and assessing their relative quality through verifiable rewards. Formally, for each (V, Q, ) sampled from DVideo-Thinker, GRPO generates distinct reasoning traces {T (1), (2), . . . , (G)} using the current policy pθold. The policy is optimized by maximizing: JGRPO(θ) = E(V,Q,T,Y )DVideo-Thinker (cid:34) 1 (cid:32) (cid:88) i=1 min (cid:16) πθ πθold Ai, clip (cid:16) πθ πθold , 1 ϵ, 1 + ϵ (cid:17) (cid:17) Ai (cid:16) β KL pθ(V, Q) (cid:13) (cid:13) (cid:13)pref(V, Q) (cid:17) (cid:33)(cid:35) , (2) where πθ = pθ(T (i)V, Q), πθold = pθold (T (i)V, Q), KL(pθ(V, Q)pref(V, Q)) denotes the KL divergence (Van Erven & Harremos, 2014) between the current policy pθ(V, Q) and reference"
        },
        {
            "title": "Preprint",
            "content": "Table 1: Comparison of model performance on video reasoning datasets in both in-domain and out-of-domain settings. The best results are marked in red bold and the second best in blue. Model Out of Domain In Domain Video-Holmes CG-Bench-Reasoning VRBench ActivityNet Star ScaleLong YouCook2 LVBench InternVL-2.5-8B InternVL-3-8B Qwen2.5-VL-7B-Instruct Qwen2.5-Omni-7B Temporal-R1-7B Open-R1-Video-7B TW-GRPO-7B Video-R1-7B Time-R1-7B VideoChat-R1-7B VideoChat-R1-Thinking-7B GRPO-CARE-7B 20.52% 18.67% 34.02% 29.99% 33.81% 21.83% 33.32% 38.54% 34.73% 35.65% 37.45% 34.34% Open-source Vanilla Models 19.39% 24.23% 27.10% 23.85% 26.74% 41.14% 63.42% 49.04% 45.52% 48.56% 70.96% 63.92% 49.85% 26.81% 51.34% 29.34% 69.25% 40.06% 59.40% 36.91% Open-source Reasoning Models 25.27% 16.46% 22.11% 27.81% 28.28% 29.26% 29.44% 27.49% 60.92% 50.15% 53.46% 69.25% 66.48% 67.65% 67.81% 66.39% 70.88% 55.76% 70.00% 76.00% 72.00% 70.88% 70.88% 70.96% 70.15% 39.75% 44.48% 31.86% 71.04% 39.12% 67.76% 47.32% 70.44% 44.47% 73.13% 40.69% 71.64% 41.95% 71.34% 40.69% SFT Models 40.84% 51.15% 63.74% 54.58% 63.74% 50.76% 63.74% 65.65% 64.50% 69.08% 66.79% 68.32% 23.91% 25.93% 33.33% 31.65% 32.66% 26.94% 29.97% 34.68% 32.65% 32.99% 35.01% 33.33% Video-Thinker-SFT-7B 31.52% 24.95% 62.40% 70.80% 64.18% 43.22% 56.11% 35.69% Our Models Video-Thinker-7B 43.22% 33.25% 80.69% 78.72% 70.66% 49.53% 73.66% 37.04% policy pref(V, Q)), Ai is the advantage for the i-th reasoning trace, and ϵ and β are hyperparameters Here, the advantage Ai is computed using outcome supervision based on normalized rewards within each group. Specifically, for each reasoning trace (i), we assign reward r(i) comprising both correctness and format components: r(i) = r(i) correct + r(i) format, (3) where r(i) ground truth , and r(i) <caption></caption> , and <think></think> tags. The advantages are then computed as: correct {0, 1} indicates whether the extracted answer from reasoning trace (i) matches the format measures adherence to the structured reasoning format with <time></time> , Ai = r(i) = r(i) mean({r(j)}G std({r(j)}G j=1) j=1) (4) This approach enables the model to learn from relative comparisons within each group, promoting both accurate reasoning and proper temporal structure adherence. Aha Moment. We find that Video-Thinker demonstrates the capacity for complex reasoning through self-reflective behaviors, which can be characterized as aha moments (Guo et al., 2025a). The model exhibits metacognitive processes by periodically revisiting its initial interpretations of video grounding and captioning tasks, critically evaluating and refining its outputs when necessary. This self-corrective behavior suggests that Video-Thinker transcends simple pattern matching and instead engages in dynamic internal feedback mechanisms similar to Video-R1 (Feng et al., 2025), while requiring substantially less training data (10K compared to 160K samples). This phenomenon is illustrated in Figure 4, with additional examples provided in Appendix G."
        },
        {
            "title": "4 EXPERIMENT",
            "content": "4.1 EXPERIMENTAL SETUP Datasets and Benchmarks. To comprehensively assess the video reasoning performance of VideoThinker, we conduct evaluations under both in-domain and out-of-domain settings. For the in-domain evaluation, since the TutorialVQA (Colas et al., 2019) training set contains only 76 samples, we do not"
        },
        {
            "title": "Preprint",
            "content": "Figure 4: An example of Video-Thinker-7Bs reasoning output on CG-Bench-Reasoning dataset. construct corresponding test set. Instead, we derive held-out test sets from the five training datasets - ActivityNet (Caba Heilbron et al., 2015), LVBench (Wang et al., 2024), ScaleLong (Ma et al., 2025a), Star (Wu et al., 2024), and YouCook2 (Zhou et al., 2018a) - by splitting them at ratio of 1:9 between test and training subsets. For the out-of-domain evaluation, we select three datasets featuring complex video reasoning tasks: Video-Holmes (Cheng et al., 2025), CG-Bench-Reasoning (Chen et al., 2024a), and VRBench (Yu et al., 2025b). Baseline Models. To comprehensively evaluate the effectiveness of Video-Thinker, we conduct extensive comparisons against two distinct categories of baseline models: (i) open-source vanilla models, including InternVL-2.5-8B (Chen et al., 2024b), InternVL-3-8B (Zhu et al., 2025), Qwen2.5VL-7B-Instruct (Bai et al., 2025), and Qwen2.5-Omni-7B (Xu et al., 2025); and (ii) open-source reasoning models, comprising Temporal-R1-7B (Li et al., 2025b), Open-R1-Video-7B (Wang & Peng, 2025), TW-GRPO-7B (Dang et al., 2025), Video-R1-7B (Feng et al., 2025), Time-R1-7B (Wang et al., 2025b), VideoChat-R1-7B (Li et al., 2025c), VideoChat-R1-Thinking-7B (Li et al., 2025c), and GRPO-CARE-7B (Chen et al., 2025b). Training Details. We employ Qwen2.5-VL-7B-Instruct (Bai et al., 2025) as our base model. During the SFT stage, we train the model on our Video-Thinker-10K dataset for 1 epoch using learning rate of 1 105 and batch size of 16. For the subsequent GRPO stage, we set the hyperparameter β in the KL divergence term to 0.04. To ensure training stability, we apply weight decay rate of 0.01 and clip the maximum gradient norm to 5. The initial learning rate is configured to 5 106 with batch size of 8. Both training stages utilize the same prompt template, as detailed in Appendix D. For computational efficiency during both training phases, we subsample each video to maximum of 16 frames and process each frame at maximum resolution of 128 28 28 pixels."
        },
        {
            "title": "Preprint",
            "content": "Table 2: Comparison of model performance on video reasoning datasets with different numbers of frames during inference in both in-domain and out-of-domain settings. The best results are marked in red bold and the second best in blue. Model # Frames Out of Domain In Domain Video-Holmes CG-Bench-Reasoning VRBench ActivityNet Star ScaleLong YouCook2 LVBench Qwen2.5-VL-7B-Instruct Video-R1-7B Video-Thinker-7B 32 64 16 32 64 32 64 34.02% 34.89% 37.56% 38.54% 40.56% 40.94% 43.22% 43.39% 44.15% 27.10% 30.33% 32.16% 27.81% 29.29% 30.12% 33.25% 33.88% 35.59% 63.42% 64.45% 65.91% 69.25% 69.44% 70.23% 80.69% 80.91% 70.96% 73.36% 74.40% 76.00% 77.20% 77.76% 78.72% 69.25% 40.06% 71.04% 43.53% 74.03% 45.18% 67.76% 47.32% 70.15% 49.84% 72.54% 50.26% 70.66% 49.53% 79.68% 72.24% 51.74% 63.74% 64.89% 68.32% 65.65% 66.03% 66.79% 73.66% 74.05% 81.29% 78.96% 72.24% 52.04% 74.05% 33.33% 36.36% 39.39% 34.68% 37.37% 37.04% 37.04% 38.38% 37.71%"
        },
        {
            "title": "4.2 PERFORMANCE COMPARISONS AND ANALYSIS",
            "content": "We evaluate all baseline models on the aforementioned dataset using accuracy as the primary evaluation metric. The performance of our Video-Thinker-7B compared to various baseline methods is summarized in Table 4. The results yield the following key findings. Video-Thinker-7B achieves new SOTA performance on video reasoning benchmarks among 7B-sized MLLMs. As demonstrated in Table 1, our proposed Video-Thinker-7B establishes new SOTA results both in-domain and out-of-domain settings across various video reasoning benchmarks. The model demonstrates particularly strong performance on challenging out-of-domain tasks, achieving 43.22% on Video-Holmes (a 4.68% improvement over the best baseline), 33.25% on CGBench-Reasoning (3.81% improvement over the best baseline), and 80.69% on VRBench (11.44% improvement over the best baseline). These substantial improvements validate the effectiveness of our Video-Thinker framework in inspiring MLLMs grounding and captioning capabilities over video sequences. GRPO stage yields substantial improvements in MLLM out-of-domain generalization over SFT stage. critical finding from our experimental analysis is that GRPO training performance substantially outperforms that of SFT in terms of video reasoning generalization. The GRPO-trained Video-Thinker-7B demonstrates marked superiority over its SFT counterpart, with improvements of 11.70% on Video-Holmes (43.22% vs. 31.52%), 8.30% on CG-Bench-Reasoning (33.25% vs. 24.95%), and 18.29% on VRBench (80.69% vs. 62.40%). These gains are particularly pronounced in out-of-domain evaluation scenarios. Importantly, Video-Thinker-SFT-7B consistently underperforms relative to most baseline methods and even degrades below the base model Qwen2.5-VL-7B-Instruct across several benchmarks, revealing the limited generalization capacity of SFT alone. Nevertheless, SFT serves an essential role in enabling the model to acquire our structured reasoning format. These findings establish the necessity of two-stage training paradigm: initial SFT stage for format acquisition, followed by GRPO stage for data-efficient performance enhancement and robust crossdomain generalization. Video-Thinker-7B constantly outperforms the baseline methods with different numbers of video frames during inference. To investigate the impact of video frame count on model performance, we evaluate Video-Thinker-7B against two baseline models, Qwen2.5-VL-7B and Video-R1-7B, using 16, 32, and 64 frames during inference across all in-domain and out-of-domain settings. As presented in Table 2, several key observations emerge from this analysis. First, increasing the number of input frames consistently enhances performance across most benchmarks and all evaluated models, with 64 frames yielding optimal results in the majority of cases. This trend suggests that richer temporal information enables more comprehensive video understanding and reasoning. Second, Video-Thinker7B consistently outperforms both baseline models across all tested frame counts, demonstrating superior capability in processing and integrating temporal information. The performance gap between Video-Thinker-7B and the baselines remains substantial regardless of frame count, indicating that our models performance improvements for video reasoning are effective across different temporal sampling strategies. In addition to analyzing the impact of video frame count, we also present the performance of VideoThinker-7B under varying training steps and learning rates during the GRPO stage in Appendix F."
        },
        {
            "title": "Preprint",
            "content": "Table 3: Comparison of model performance on video grounding and captioning tasks. The best results are marked in red bold and the second best in blue. Model Grounding Captioning mIOU Recall@0.3 Recall@0.5 Average Meteor ROUGE-L BLEU@1 Average Qwen2.5-VL-7B 27.47 39.52 23.71 30.23 Video-R1-7B Video-Thinker-7B 48.22 79. 51.49 59.67 14.10 12.72 15.87 14. 11.64 20.11 10.15 7.52 15.34 13. 10.63 17.11 4.3 IN-DEPTH ANALYSIS OF GROUNDING AND CAPTIONING CAPABILITIES One of the main ideas underlying Video-Thinker is that grounding and captioning capabilities serve as key tools for video reasoning. Therefore, we further investigate whether the performance gains of Video-Thinker stem from enhanced grounding and captioning capabilities. To validate the improved temporal manipulation capabilities of Video-Thinker, we conduct quantitative experiments to analyze the grounding and captioning abilities of Video-Thinker-7B, comparing it against the base model Qwen2.5-VL-7B-Instruct and the previous SOTA model Video-R1-7B. For both experiments, we select 1K samples from caption-labeled in-domain test dataset with ground truth caption annotations and temporal annotations (sourced from ActivityNet (Caba Heilbron et al., 2015), YouCook2 (Zhou et al., 2018a), and TutorialVQA (Colas et al., 2019)). Each sample contains one or multiple ground truth question-relevant key segment time annotations for grounding ability verification and corresponding ground truth captions for captioning ability evaluation. Video-Thinker-7B demonstrates superior performance across all evaluated metrics in video grounding tasks. To assess temporal grounding capabilities, we employ structured evaluation protocol wherein models are prompted to answer questions while simultaneously outputting questionrelevant time segments within <time></time> tags (detailed prompt specifications provided in Appendix D). We subsequently extract model-predicted temporal segments and evaluate their alignment with ground truth annotations using two complementary metrics: mean Intersection-over-Union (mIoU) and Recall@K. As demonstrated in Table 3, Video-Thinker-7B consistently outperforms baseline models across all evaluation metrics. Our model achieves an mIoU of 48.22%, representing substantial 75.5% improvement over Qwen2.5-VL-7Bs 27.47%. For recall metrics, Video-Thinker-7B attains 79.29% and 51.49% for Recall@0.3 and Recall@0.5, respectively, nearly doubling the baseline performance (39.52% and 23.71%). The overall averaged performance of 59.67% constitutes 97% relative improvement compared to the baselines 30.23%. Note that Video-R1 is excluded from this evaluation due to its inability to follow our prompt to generate temporal annotations within our templates. Video-Thinker-7B demonstrates superior performance across all evaluated metrics in video captioning tasks. To evaluate captioning capabilities, we prompt models to generate descriptions for video segments using the instruction Describe the video segment, then compare predicted captions against ground truth references. We employ three established metrics: BLEU@1 (Papineni et al., 2002), METEOR (Banerjee & Lavie, 2005), and ROUGE-L (Lin, 2004). The captioning results presented in Table 3 demonstrate that Video-Thinker-7B achieves superior performance across all three evaluation metrics. Specifically, our model attains 15.87% METEOR, 20.11% ROUGE-L, and 15.34% BLEU@1, yielding an overall average of 17.11%. Compared to the base model Qwen2.5-VL-7B-Instruct, Video-Thinker exhibits consistent improvements of 1.77%, 5.20%, and 5.19%, respectively, representing 31.2% relative enhancement in overall performance. When compared against Video-R1-7B, the improvements are even more pronounced, with gains of 3.15%, 8.47%, and 7.82% respectively, achieving 61.0% relative improvement in overall performance. These results substantiate Video-Thinkers enhanced capacity for generating contextually accurate and temporally relevant video descriptions. Moreover, to further validate the importance of grounding and captioning capabilities for video understanding, we conduct additional experiments by providing ground-truth grounding and captioning annotations to Video-R1-7B and evaluating its performance on the Video-Holmes benchmark (Cheng et al., 2025). As detailed in Appendix E, these oracle experiments demonstrate that access to accurate video grounding and captioning information significantly enhances MLLM performance."
        },
        {
            "title": "5 CONCLUSION AND FUTURE WORK",
            "content": "In this work, we introduce Video-Thinker, novel approach that extends the Thinking with Images paradigm to video reasoning by empowering MLLMs to autonomously leverage their intrinsic grounding and captioning capabilities. Through the construction of the Video-Thinker-10K dataset and two-stage training strategy combining SFT and GRPO, our method enables MLLMs to generate reasoning clues throughout the inference process without relying on external tools, and our resulting Video-Thinker-7B model establishes SOTA performance among 7B-sized models. Looking forward, it is interesting to scale Video-Thinker with larger model sizes or with additional intrinsic capabilities beyond grounding and captioning, or with more modalities such as audio."
        },
        {
            "title": "ETHICS STATEMENT",
            "content": "This work focuses on the study of multimodal video understanding and reasoning. All datasets used in our experiments are publicly available and commonly adopted in prior research. We followed the respective dataset licenses and usage terms. No personally identifiable information (PII) or sensitive private data was collected, generated, or annotated by the authors. Our study does not raise direct ethical concerns such as misuse of personal data, harmful content, or bias amplification beyond what is already inherent in the benchmark datasets. We acknowledge that large-scale visionlanguage models may inherit biases present in training data. To mitigate risks, our evaluations were restricted to established academic benchmarks for fair comparison. We encourage future researchers and practitioners to be mindful of potential social implications when applying these systems in downstream applications."
        },
        {
            "title": "REPRODUCIBILITY STATEMENT",
            "content": "In order to ensure reproducibility, we provide comprehensive description of datasets, model implementations, and experimental settings in the main paper and the appendix. The benchmarks and evaluation metrics we used are standard and publicly available. All baselines are either taken from released model checkpoints or trained/evaluated with publicly accessible open-source implementations. To further promote reproducibility, hyperparameters, training details, and evaluation protocol are clearly documented. We commit to following general academic guidelines for transparency and reproducibility in scientific reporting."
        },
        {
            "title": "REFERENCES",
            "content": "Ruichuan An, Sihan Yang, Ming Lu, Renrui Zhang, Kai Zeng, Yulin Luo, Jiajun Cao, Hao Liang, Ying Chen, Qi She, et al. Mc-llava: Multi-concept personalized vision-language model. arXiv preprint arXiv:2411.11706, 2024. Ruichuan An, Sihan Yang, Renrui Zhang, Zijun Shen, Ming Lu, Gaole Dai, Hao Liang, Ziyu Guo, Shilin Yan, Yulin Luo, et al. Unictokens: Boosting personalized understanding and generation via unified concept tokens. arXiv preprint arXiv:2505.14671, 2025. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. Satanjeev Banerjee and Alon Lavie. Meteor: An automatic metric for mt evaluation with improved correlation with human judgments. In Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization, pp. 6572, 2005. Fabian Caba Heilbron, Victor Escorcia, Bernard Ghanem, and Juan Carlos Niebles. Activitynet: large-scale video benchmark for human activity understanding. In Proceedings of the ieee conference on computer vision and pattern recognition, pp. 961970, 2015."
        },
        {
            "title": "Preprint",
            "content": "Guo Chen, Yicheng Liu, Yifei Huang, Yuping He, Baoqi Pei, Jilan Xu, Yali Wang, Tong Lu, and Limin Wang. Cg-bench: Clue-grounded question answering benchmark for long video understanding. arXiv preprint arXiv:2412.12075, 2024a. Xinyan Chen, Renrui Zhang, Dongzhi Jiang, Aojun Zhou, Shilin Yan, Weifeng Lin, and Hongsheng Li. Mint-cot: Enabling interleaved visual tokens in mathematical chain-of-thought reasoning. arXiv preprint arXiv:2506.05331, 2025a. Yi Chen, Yuying Ge, Rui Wang, Yixiao Ge, Junhao Cheng, Ying Shan, and Xihui Liu. Grpocare: Consistency-aware reinforcement learning for multimodal reasoning, 2025b. URL https: //arxiv.org/abs/2506.16141. Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271, 2024b. Junhao Cheng, Yuying Ge, Teng Wang, Yixiao Ge, Jing Liao, and Ying Shan. Video-holmes: Can mllm think like holmes for complex video reasoning? arXiv preprint arXiv:2505.21374, 2025. Anthony Colas, Seokhwan Kim, Franck Dernoncourt, Siddhesh Gupte, Daisy Zhe Wang, and Doo Soon Kim. Tutorialvqa: Question answering dataset for tutorial videos. arXiv preprint arXiv:1912.01046, 2019. Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. Jisheng Dang, Jingze Wu, Teng Wang, Xuanhui Lin, Nannan Zhu, Hongbo Chen, Wei-Shi Zheng, Meng Wang, and Tat-Seng Chua. Reinforcing video reasoning with focused thinking. arXiv preprint arXiv:2505.24718, 2025. Hao Fei, Shengqiong Wu, Wei Ji, Hanwang Zhang, Meishan Zhang, Mong-Li Lee, and Wynne Hsu. Video-of-thought: Step-by-step video reasoning from perception to cognition. arXiv preprint arXiv:2501.03230, 2024. Kaituo Feng, Kaixiong Gong, Bohao Li, Zonghao Guo, Yibing Wang, Tianshuo Peng, Junfei Wu, Xiaoying Zhang, Benyou Wang, and Xiangyu Yue. Video-r1: Reinforcing video reasoning in mllms. arXiv preprint arXiv:2503.21776, 2025. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025a. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025b. Tanmay Gupta and Aniruddha Kembhavi. Visual programming: Compositional visual reasoning without training. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1495314962, 2023. Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. Can Li, Ting Zhang, Mei Wang, and Hua Huang. Visiomath: Benchmarking figure-based mathematical reasoning in lmms. arXiv preprint arXiv:2506.06727, 2025a. Hongyu Li, Songhao Han, Yue Liao, Junfeng Luo, Jialin Gao, Shuicheng Yan, and Si Liu. Reinforcement learning tuning for videollms: Reward design and data efficiency. arXiv preprint arXiv:2506.01908, 2025b."
        },
        {
            "title": "Preprint",
            "content": "Xinhao Li, Ziang Yan, Desen Meng, Lu Dong, Xiangyu Zeng, Yinan He, Yali Wang, Yu Qiao, Yi Wang, and Limin Wang. Videochat-r1: Enhancing spatio-temporal perception via reinforcement fine-tuning. arXiv preprint arXiv:2504.06958, 2025c. Chin-Yew Lin. Rouge: package for automatic evaluation of summaries. In Text summarization branches out, pp. 7481, 2004. Weifeng Lin, Xinyu Wei, Ruichuan An, Tianhe Ren, Tingwei Chen, Renrui Zhang, Ziyu Guo, Wentao Zhang, Lei Zhang, and Hongsheng Li. Perceive anything: Recognize, explain, caption, and segment anything in images and videos. arXiv preprint arXiv:2506.05302, 2025. Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024a. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning, 2023. Shilong Liu, Hao Cheng, Haotian Liu, Hao Zhang, Feng Li, Tianhe Ren, Xueyan Zou, Jianwei Yang, Hang Su, Jun Zhu, et al. Llava-plus: Learning to use tools for creating multimodal agents. In European conference on computer vision, pp. 126142. Springer, 2024b. Ziqiang Liu, Feiteng Fang, Xi Feng, Xeron Du, Chenhao Zhang, Noah Wang, Qixuan Zhao, Liyang Fan, CHENGGUANG GAN, Hongquan Lin, et al. Ii-bench: An image implication understanding benchmark for multimodal large language models. Advances in Neural Information Processing Systems, 37:4637846480, 2024c. Ziyu Liu, Yuhang Zang, Yushan Zou, Zijian Liang, Xiaoyi Dong, Yuhang Cao, Haodong Duan, Dahua Lin, and Jiaqi Wang. Visual agentic reinforcement fine-tuning. arXiv preprint arXiv:2505.14246, 2025. Yulin Luo, Ruichuan An, Bocheng Zou, Yiming Tang, Jiaming Liu, and Shanghang Zhang. Llm as dataset analyst: Subpopulation structure discovery with large language model. In European Conference on Computer Vision, pp. 235252. Springer, 2024. David Ma, Huaqing Yuan, Xingjian Wang, Qianbo Zang, Tianci Liu, Xinyang He, Yanbin Wei, Jiawei Guo, Ni Jiahui, Zhenzhu Yang, et al. Scalelong: multi-timescale benchmark for long video understanding. arXiv preprint arXiv:2505.23922, 2025a. David Ma, Yuanxing Zhang, Jincheng Ren, Jarvis Guo, Yifan Yao, Zhenlin Wei, Zhenzhu Yang, Zhongyuan Peng, Boyu Feng, Jun Ma, et al. Iv-bench: benchmark for image-grounded video perception and reasoning in multimodal llms. arXiv preprint arXiv:2504.15415, 2025b. Zixian Ma, Jianguo Zhang, Zhiwei Liu, Jieyu Zhang, Juntao Tan, Manli Shu, Juan Carlos Niebles, Shelby Heinecke, Huan Wang, Caiming Xiong, et al. Taco: Learning multi-modal action models with synthetic chains-of-thought-and-action. arXiv preprint arXiv:2412.05479, 2024. OpenAI. Image thinking: Breakthroughs in visual chain-of-thought reasoning with OpenAI o3 and o4-mini. OpenAI Blog, April 2024. URL https://openai.com/research/ imagethinking. Accessed: 2025-08-09. Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pp. 311318, 2002. Hao Shao, Shengju Qian, Han Xiao, Guanglu Song, Zhuofan Zong, Letian Wang, Yu Liu, and Hongsheng Li. Visual cot: Unleashing chain-of-thought reasoning in multi-modal language models. CoRR, 2024a. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024b."
        },
        {
            "title": "Preprint",
            "content": "Haozhan Shen, Kangjia Zhao, Tiancheng Zhao, Ruochen Xu, Zilun Zhang, Mingwei Zhu, and Jianwei Yin. Zoomeye: Enhancing multimodal llms with human-like zooming capabilities through tree-based image exploration. arXiv preprint arXiv:2411.16044, 2024. Yudi Shi, Shangzhe Di, Qirui Chen, and Weidi Xie. Enhancing video-llm reasoning via agent-ofthoughts distillation. arXiv preprint arXiv:2412.01694, 2024. Zhaochen Su, Peng Xia, Hangyu Guo, Zhenhua Liu, Yan Ma, Xiaoye Qu, Jiaqi Liu, Yanshu Li, Kaide Zeng, Zhengyuan Yang, et al. Thinking with images for multimodal reasoning: Foundations, methods, and future frontiers. arXiv preprint arXiv:2506.23918, 2025. Tim Van Erven and Peter Harremos. Rényi divergence and kullback-leibler divergence."
        },
        {
            "title": "IEEE",
            "content": "Transactions on Information Theory, 60(7):37973820, 2014. Ke Wang, Junting Pan, Linda Wei, Aojun Zhou, Weikang Shi, Zimu Lu, Han Xiao, Yunqiao Yang, Houxing Ren, Mingjie Zhan, et al. Mathcoder-vl: Bridging vision and code for enhanced multimodal mathematical reasoning. arXiv preprint arXiv:2505.10557, 2025a. Weihan Wang, Zehai He, Wenyi Hong, Yean Cheng, Xiaohan Zhang, Ji Qi, Xiaotao Gu, Shiyu Huang, Bin Xu, Yuxiao Dong, et al. Lvbench: An extreme long video understanding benchmark. arXiv preprint arXiv:2406.08035, 2024. Xiaodong Wang and Peixi Peng. Open-r1-video. Wang-Xiaodong1899/Open-R1-Video, 2025. https://github.com/ Ye Wang, Ziheng Wang, Boshen Xu, Yang Du, Kejun Lin, Zihan Xiao, Zihao Yue, Jianzhong Ju, Liang Zhang, Dingyi Yang, Xiangnan Fang, Zewen He, Zhenbo Luo, Wenxuan Wang, Junqi Lin, Jian Luan, and Qin Jin. Time-r1: Post-training large vision language model for temporal video grounding. arXiv preprint arXiv:2503.13377, 2025b. Yikun Wang, Siyin Wang, Qinyuan Cheng, Zhaoye Fei, Liang Ding, Qipeng Guo, Dacheng Tao, and Xipeng Qiu. Visuothink: Empowering lvlm reasoning with multimodal tree search. arXiv preprint arXiv:2504.09130, 2025c. Bo Wu, Shoubin Yu, Zhenfang Chen, Joshua Tenenbaum, and Chuang Gan. Star: benchmark for situated reasoning in real-world videos. arXiv preprint arXiv:2405.09711, 2024. Jin Xu, Zhifang Guo, Jinzheng He, Hangrui Hu, Ting He, Shuai Bai, Keqin Chen, Jialin Wang, Yang Fan, Kai Dang, et al. Qwen2. 5-omni technical report. arXiv preprint arXiv:2503.20215, 2025. Jianwei Yang, Hao Zhang, Feng Li, Xueyan Zou, Chunyuan Li, and Jianfeng Gao. Set-of-mark prompting unleashes extraordinary visual grounding in gpt-4v. arXiv preprint arXiv:2310.11441, 2023. Haoxuan You, Haotian Zhang, Zhe Gan, Xianzhi Du, Bowen Zhang, Zirui Wang, Liangliang Cao, Shih-Fu Chang, and Yinfei Yang. Ferret: Refer and ground anything anywhere at any granularity. arXiv preprint arXiv:2310.07704, 2023. En Yu, Kangheng Lin, Liang Zhao, Yana Wei, Zining Zhu, Haoran Wei, Jianjian Sun, Zheng Ge, Xiangyu Zhang, Jingyu Wang, et al. Unhackable temporal rewarding for scalable video mllms. arXiv preprint arXiv:2502.12081, 2025a. Jiashuo Yu, Yue Wu, Meng Chu, Zhifei Ren, Zizheng Huang, Pei Chu, Ruijie Zhang, Yinan He, Qirui Li, Songze Li, et al. Vrbench: benchmark for multi-step reasoning in long narrative videos. arXiv preprint arXiv:2506.10857, 2025b. Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 95569567, 2024. Zeyu Zhang, Zijian Chen, Zicheng Zhang, Yuze Sun, Yuan Tian, Ziheng Jia, Chunyi Li, Xiaohong Liu, Xiongkuo Min, and Guangtao Zhai. Puzzlebench: fully dynamic evaluation framework for large multimodal models on puzzle solving. arXiv preprint arXiv:2504.10885, 2025."
        },
        {
            "title": "Preprint",
            "content": "Qingqing Zhao, Yao Lu, Moo Jin Kim, Zipeng Fu, Zhuoyang Zhang, Yecheng Wu, Zhaoshuo Li, Qianli Ma, Song Han, Chelsea Finn, et al. Cot-vla: Visual chain-of-thought reasoning for vision-language-action models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 17021713, 2025. Ziwei Zheng, Michael Yang, Jack Hong, Chenxiao Zhao, Guohai Xu, Le Yang, Chao Shen, and Xing Yu. Deepeyes: Incentivizing\" thinking with images\" via reinforcement learning. arXiv preprint arXiv:2505.14362, 2025. Luowei Zhou, Chenliang Xu, and Jason Corso. Towards automatic learning of procedures from web instructional videos. In Proceedings of the AAAI conference on artificial intelligence, volume 32, 2018a. Luowei Zhou, Chenliang Xu, and Jason Corso. Towards automatic learning of procedures from web instructional videos. In Proceedings of the AAAI conference on artificial intelligence, volume 32, 2018b. Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen Duan, Weijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479, 2025."
        },
        {
            "title": "Preprint",
            "content": "A OVERALL ALGORITHM OF VIDEO-THINKER Generate missing visual captions and reasoning questions. Synthesize structured reasoning trace with hindsight curation as detailed in Section 3.1. Algorithm 1 Video-Thinker Input: Collected dataset Dsource according to Section 3.1, pre-trained MLLM with parameters θ Output: MLLM trained by the Video-Thinker 1: Phase 1: Data Synthesis via Hindsight-curation Reasoning according to Section 3.1 2: for each sample (V, Q, T, ) Dsource do 3: 4: 5: end for 6: Construct Video-Thinker-10K dataset DVideo-Thinker. 7: Phase 2: SFT Optimization for Format-Following according to Section 3.2 8: for each (V, Q, T, ) DVideo-Thinker do 9: 10: end for 11: Phase 3: GRPO Optimization for Autonomous Navigation according to Section 3.2 12: for each (V, Q, T, ) DVideo-Thinker do Generate reasoning traces {T (i)}G 13: Compute rewards r(i) = r(i) correct + r(i) Calculate normalized advantages Ai = r(i)mean({r(j)}) Optimize GRPO objective JGRPO(θ) with clipped importance sampling according to Eq. (2). i=1 using current policy. format according to Eq. (3). Compute and minimize: LSFT(θ) according to Eq. (1). according to Eq. (4). std({r(j)}) 15: 14: 16: 17: end for 18: return MLLM with tuned θ DATA DISTRIBUTION OVER SOURCE DATASETS IN SECTION 3.1 Figure 5: The data distribution of our Video-Thinker-10K dataset."
        },
        {
            "title": "C EXPERIMENT CONFIGURATION",
            "content": "C.1 DATASETS AND BENCHMARKS ActivityNet (Caba Heilbron et al., 2015) is large-scale VideoQA benchmark, consisting of 5,800 long untrimmed videos (average length 180s) and 58K bilingual (Chinese/English) human-annotated QA pairs. Introducing question templates over motion, spatial and temporal relations as well as freeform queries, offering robust testbed for spatio-temporal reasoning and fine-grained comprehension."
        },
        {
            "title": "Preprint",
            "content": "STAR (Wu et al., 2024) focuses on situated reasoning in daily life scenarios, covering 22K short clips and 60K structured questions spanning interaction, sequence, prediction, and feasibility reasoning. Constructing situational hyper-graphs to capture entities, actions, and relations, ensuring explicit logical grounding and reducing shortcut biases. ScaleLong (Ma et al., 2025a) targets multi-scale temporal understanding in long videos, with 269 videos (avg. 86 minutes) and 1.7K well-curated QA pairs. Each question is aligned with one of four temporal granularitiesclip, shot, event, storythus isolating evaluation across distinct timescales without conflating video content. YouCook2 (Zhou et al., 2018a) contains 2,000 instructional cooking videos from 89 recipes, with temporal annotations and imperative descriptions for stepwise procedures. As standard benchmark for instructional video understanding, it enables research into activity recognition, weakly supervised object grounding, and cross-video procedural knowledge transfer. LVBench (Wang et al., 2024) evaluates long-horizon multimodal reasoning with 103 YouTube videos (117 total hours) and 1.5K QA pairs. Tasks emphasize summarization, causal reasoning, and temporal localization, with additional clue-length annotations specifying the minimal evidence span required. Video-Holmes (Cheng et al., 2025) uniquely probes narrative-driven reasoning via 270 mystery films and 1.8K QA pairs. It emphasizes multi-clue integration, causal inference, and social relation reasoning, filling crucial gap in evaluating complex video storylines beyond surface perception. CG-Bench (Chen et al., 2024a) consists of 1.2K long videos and 12K QA pairs, introducing cluegrounded paradigm for perception, reasoning, and hallucination queries. Its white-box and black-box evaluations require explicit evidence retrieval, mitigating guess-based shortcuts and incentivizing faithful video-grounded reasoning. We used the reasoning section of CG-Bench while evaluating. VRBench (Yu et al., 2025b) benchmarks multi-step reasoning over 1,010 narrative videos spanning 8 languages. Providing high-quality stepwise reasoning annotations and multi-phase evaluation pipeline to jointly assess reasoning process and outcome, is first benchmark to explicitly measure both the how and what of video reasoning. C.2 BASELINE MODELS InternVL-2.5-8B (Chen et al., 2024b) refines the InternVL architecture with progressive scaling strategies, improved training pipelines, and high-quality data filtering. It achieves competitive results against leading commercial systems, excelling in multi-image/video understanding, document parsing, and multimodal reasoning benchmarks. InternVL-3-8B (Zhu et al., 2025) further enhances perception and reasoning by introducing Native Multimodal Pre-Training, Variable Visual Position Encoding, and Mixed Preference Optimization. Beyond vision-language tasks, it extends capabilities to GUI agents, 3D vision perception, and tool usage, setting new standards for multimodal flexibility. Qwen2.5-VL-7B (Bai et al., 2025) emphasizes long-form video understanding with dynamic temporal modeling and efficient frame-rate training. It supports structured outputs for documents and visual grounding, while also enabling agentic tool-use behaviors across vision and language tasks. Qwen2.5-VL-Omni-7B (Xu et al., 2025) unifies text, image, audio, and video into novel end-toend architecture (Thinker-Talker) with real-time speech generation and streaming interaction. Its multimodal coverage allows robust conversational agents that can handle both text and voice outputs. Temporal-R1-7B (Li et al., 2025b) introduces dual-reward reinforcement learning scheme that balances semantic correctness with temporal localization accuracy. Promoting more robust spatiotemporal reasoning in long video contexts. Time-R1-7B (Wang et al., 2025b) extends beyond retrospective understanding to future event prediction and hypothetical scenario generation. It showcases efficient training curricula for advancing temporal intelligence in MLLMs. Open-R1-Video-7B (Wang & Peng, 2025) and Video-R1 (Feng et al., 2025) adapt the R1 reinforcement learning paradigm to video reasoning with GRPO-driven optimization. Both emphasize temporal-aware training strategies, achieving strong results on challenging video benchmarks."
        },
        {
            "title": "Preprint",
            "content": "TW-GRPO-7B (Dang et al., 2025) refines RL pipelines with token-wise weighting and soft reward mechanisms, producing denser and more fine-grained reasoning chains. GRPO-CARE-7B (Chen et al., 2025b) enhances logical consistency using coherence-aware reward design, improving the alignment between intermediate reasoning steps and final predictions. VideoChat-R1-7B (Li et al., 2025c) integrates structured video reasoning with interactive dialogue, supporting temporally grounded conversation in multimodal applications. It represents step toward practical, user-facing video reasoning systems. C.3 EVALUATION METRICS Mean Intersection-over-Union (mIoU) comes from Intersection-over-Union (IoU), which is standard measure of overlap between two temporal segments. Given predicted segment = [tp s, tp e] and ground-truth segment = [tg e], IoU is computed as: s, tg IoU = B B For each ground-truth segment, the maximum IoU across all predicted segments is recorded. The mean IoU (mIoU) is then obtained by averaging these values over all instances in the test set. mIoU provides holistic measure of temporal localization accuracy, reflecting how closely predictions align with annotated spans. It is sensitive to both prediction boundary precision and temporal coverage, making it particularly suitable for localization evaluation in long-form videos. Recall@K assesses whether ground-truth segments are successfully retrieved by model predictions at varying strictness levels. Specifically, for ground-truth span g, if there exists prediction such that IoU(p, g) K, the ground-truth is considered recalled. Recall@K is then the fraction of recalled spans across all annotations. Typically, {0.3, 0.5} is used, where Recall@0.3 emphasizes coarse localization (lenient overlap) and Recall@0.5 emphasizes fine-grained alignment (stricter overlap). This metric complements mIoU by quantifying success rates under different quality thresholds, highlighting trade-offs between coverage and precision. BLEU@1 (Papineni et al., 2002) comes from BLEU (Bilingual Evaluation Understudy), which is one of the earliest and most influential metrics for text generation evaluation. BLEU@1 focuses on unigram precision, i.e., the proportion of generated words appearing in reference captions. Formally, BLEU@1 = min 1, exp 1 (cid:18) (cid:18) len(reference) len(candidate) (cid:19)(cid:19) (cid:80) unigramcandidate Countclip(unigram) (cid:80) unigramcandidate Count(unigram) The score ranges from 0 to 1, with higher scores indicating stronger lexical overlap. Although BLEU@1 provides straightforward measure of word-level accuracy, it does not capture semantic adequacy or fluency beyond exact token matches. In video captioning, it remains useful as proxy for surface-level similarity, particularly for frequent objects and actions. METEOR (Banerjee & Lavie, 2005) (Metric for Evaluation of Translation with Explicit ORdering) addresses several limitations of BLEU by combining unigram precision and recall, alongside synonymy, stemming, and paraphrase matching. The score is computed as harmonic mean of precision and recall (with recall typically weighted higher), and adjusted with fragmentation penalty to account for word order: METEOR = (1 Penalty) Fmean where Fα balances precision and recall, and enalty penalizes disordered matches. METEOR ranges from 0 to 1, yielding higher values when generated captions are both semantically complete and linguistically coherent. Its ability to match semantically related words makes it suited for evaluating paraphrased or stylistically varied captions. ROUGE-L (Lin, 2004) comes from ROUGE (Recall-Oriented Understudy for Gisting Evaluation) metrics, which are widely applied in summarization and captioning. ROUGE-L specifically uses the Longest Common Subsequence (LCS) between candidate and reference sequences to compute recall, precision, and an F1-like score: ROUGE-L = (cid:80) S{ReferenceSummaries} (cid:80) S{ReferenceSummaries} (cid:80) gramnS Countmatch(gramn) (cid:80) gramnS Count(gramn)"
        },
        {
            "title": "Preprint",
            "content": "Here, Precision and Recall are based on the length of the LCS relative to the candidate and reference lengths, respectively. The metric rewards captions that preserve overall sentence structure and ordering of key tokens. Unlike BLEU@1, which prioritizes exact n-gram matches, ROUGE-L emphasizes global sequence-level correspondence, providing balanced view of content fidelity."
        },
        {
            "title": "D PROMPTS",
            "content": "D.1 TRAINING AND EVALUATION"
        },
        {
            "title": "Prompt Template for Training and Evaluation",
            "content": "System Prompt: You are an expert video analyst tasked with solving problems based on video content. When answering question about video, you should carefully observe and analyze important visual clues from the videos to answer. For each important segment you notice, first observe the key visual elements, then analyze their significance using the following format: specify the time range with <time>start_time-end_time</time>, describe the key visual clues with <caption>Description of key visual clues</caption>, and provide your analysis about what this means with Your analysis and thoughts about this segment. Throughout your analysis, think about the question as if you were human pondering deeply, engaging in an internal dialogue using natural thought expressions such as let me think, wait, Hmm, oh, see, lets break it down, etc, or other natural language thought expressions. After examining the key visual clues, continue with deeper reasoning that connects your observations to the answer. Self-reflection or verification in your reasoning process is encouraged when necessary, though if the answer is straightforward, you may proceed directly to the conclusion. Finally, conclude by placing your final answer in <answer> </answer> tags. Question Template: {Question} Please analyze the video carefully by identifying key segments and their important visual clues within<time> </time>, <caption> </caption>, <think> </think> tags. Then conduct deep analysis and reasoning to arrive at your answer to the question. Finally, provide only the single option letter (e.g., A, B, C, D, E, etc.) within the <answer> </answer> tags. Follow the format specified in the instructions. D.2 VIDEO CAPTION GENERATION Prompt Template for Video Caption Generation System Prompt: You are professional video analysis assistant. Your task is to analyze video segments and provide natural, factual descriptions of the key visual evidence that supports the correct answer to the given question. Focus on describing the essential visual elements, actions, objects, or events that are directly relevant to the question and answer. Provide clear, objective descriptions of what you observe without any reasoning or analysis simply describe the important visual clues that are present in the video. Avoid referring to the content as this video or adding any reasoning and thinking instead, describe what you see directly. User Prompt: {Question} {Answer} Based on the video segment shown, provide natural and concise description of the key visual evidence that supports the correct answer. Focus on describing the essential visual elements, actions, objects, or details that are directly relevant to both the question and the correct answer. Describe what you observe factually without any reasoning or analysis simply state the important visual clues that are present. Write in natural, descriptive style without referring to this video or video segment. D.3 QA GENERATION Prompt Template for ActivityNet QA Generation System Prompt: You are an expert at creating sophisticated multiple-choice questions that test video comprehension through analysis of key visual segments. You will receive: 1. Background context describing the overall video content 2. chronologically ordered list of event descriptions corresponding to key visual segments in the video"
        },
        {
            "title": "Preprint",
            "content": "Your task is to generate one multiple-choice question that requires viewers to locate, synthesize, and reason across these multiple key visual segments to determine the correct answer. Question generation strategy: - If events show clear relationships or logical connections: Create reasoning question that tests understanding of cause-effect relationships, intentions, motivations, or sequential logic - If events appear disconnected or simple: Create complex perceptual question that tests detailed observation, accurate pattern recognition, or comprehensive summarization across segments. Requirements for your question: - Ask directly and naturally without referencing based on, events, segments, or sequences - Must require analysis of multiple event descriptions from different visual segments - Cannot be answerable from any single event description alone - Should demand synthesis of information across the chronological sequence - Must test either analytical reasoning or sophisticated perceptual skills - Base your question strictly on the information provided in the key visual segment descriptions do not introduce any external knowledge, assumptions, or fabricated details Requirements for answer options: - Provide 46 options with one definitively correct answer - Include sophisticated distractors that require careful discrimination - Ensure the correct answer emerges only through comprehensive analysis of all provided events - All options must be derivable from or directly contradicted by the given descriptions - Avoid directly quoting phrases from the event descriptions Output format: Respond with valid JSON object containing these exact keys: question, options, answer. The options value must be list of strings. User Prompt: Background: {caption} Descriptions of Key Visual Segments (chronological order): {events text} Generate multiple-choice question that requires viewers to locate and synthesize information across these specific segments. Prompt Template for YouCook2 QA Generation System Prompt: You are an expert at creating sophisticated multiple-choice questions that test cooking video comprehension through analysis of key visual segments. You will receive: chronologically ordered list of cooking step descriptions corresponding to key visual segments in the cooking video. Your task is to generate one multiple-choice question that requires viewers to locate, synthesize, and reason across these multiple key visual segments to determine the correct answer. Question generation strategy: - You can create reasoning question that tests understanding of cause-effect relationships, cooking techniques, ingredient interactions, or sequential cooking logic - Or you can create complex perceptual question that tests detailed observation, accurate pattern recognition, or comprehensive summarization across segments Requirements for your question: - Ask directly and naturally without referencing based on, steps, segments, or sequences - Must require analysis of multiple cooking step descriptions from different visual segments - Cannot be answerable from any single step description alone - Should demand synthesis of information across the chronological cooking sequence - Must test either analytical reasoning or sophisticated culinary perceptual skills - Base your question strictly on the information provided in the key visual cooking step descriptions do not introduce any external knowledge, assumptions, or fabricated details Requirements for answer options: - Provide 46 options with one definitively correct answer - Include sophisticated distractors that require careful discrimination - Ensure the correct answer emerges only through comprehensive analysis of all provided cooking steps - All options must be derivable from or directly contradicted by the given descriptions - Avoid directly quoting phrases from the cooking step descriptions Output format: Respond with valid JSON object containing these exact keys: question, options, answer. The options value must be list of strings. User Prompt: Descriptions of Key Video Segments about Cooking Steps (chronological order): {steps text} Generate multiple-choice question that requires viewers to locate and synthesize information across these specific segments."
        },
        {
            "title": "Prompt Template for TutorialVQA QA Generation",
            "content": "System Prompt: You are an expert at creating sophisticated multiple-choice questions that test video comprehension through analysis of key visual segments. You will receive: 1. Video Title: The title of the video 2. Transcript: The spoken content or narration from the video 3. Descriptions of key video segments of main steps covered: chronologically ordered list of step descriptions corresponding to key visual segments in the video Your task is to generate one multiple-choice question that requires viewers to locate, synthesize, and reason across these multiple key visual segments to determine the correct answer. Question generation strategy: - You can create reasoning question that tests understanding of cause-effect relationships, intentions, motivations, or sequential logic - Or you can create complex perceptual question that tests detailed observation, accurate pattern recognition, or comprehensive summarization across segments Requirements for your question: - Ask directly and naturally without referencing based on, steps, segments, or sequences - Must require analysis of multiple step descriptions from different visual segments - Cannot be answerable from any single step description alone - Should demand synthesis of information across the chronological sequence - Must test either analytical reasoning or sophisticated perceptual skills - Base your question strictly on the information provided in the key visual segment descriptions do not introduce any external knowledge, assumptions, or fabricated details Requirements for answer options: - Provide 46 options with one definitively correct answer - Include sophisticated distractors that require careful discrimination - Ensure the correct answer emerges only through comprehensive analysis of all provided steps - All options must be derivable from or directly contradicted by the given descriptions - Avoid directly quoting phrases from the step descriptions Output format: Respond with valid JSON object containing these exact keys: question, options, answer. The options value must be list of strings. User Prompt: Video Title: {video title} Full Transcript: {full transcript text} Descriptions for key video segments of main steps covered (chronological order): {main steps} Generate multiple-choice question that requires viewers to locate and synthesize information across these specific segments. Table 4: Performance comparisons of including grounding and captioning CoT content with Video-R1 as the base model. Experimental Setup Accuracy Base w/ Caption w/ Grounding w/ Caption + Grounding 37% 56% 53% 63%"
        },
        {
            "title": "CAPABILITIES",
            "content": "To investigate the impact of incorporating grounding and captioning information on video reasoning performance, we conduct comprehensive experiments using Video-R1-7B (Feng et al., 2025) as our test model on the Video-Holmes (Cheng et al., 2025) dataset. This dataset provides rich annotations, including question-relevant key temporal segments (grounding information) and comprehensive video descriptions (captioning information). We evaluate the model under four distinct experimental configurations: (i) Base: Direct inference without any additional input information, serving as our baseline; (ii) w/ Grounding: Each question is augmented with temporally-grounded key segment information that highlights relevant video portions; (iii) w/ Captioning: Each question is supplemented with comprehensive caption information describing the entire video content; (iv) w/ Grounding &"
        },
        {
            "title": "Preprint",
            "content": "Table 5: Performance change of Video-Thinker with different training steps. The best results are marked in red bold and the second best in blue. Training Step Out of Domain In Domain Avg. Video-Holmes CG-Bench-Reasoning VRBench ActivityNet Star ScaleLong YouCook2 LVBench 500 1000 1500 2500 3000 3500 4000 4500 37.40% 38.32% 41.86% 40.94% 43.22% 39.36% 40.56% 41.21% 41.92% 41.26% 29.03% 30.30% 32.99% 30.83% 33.25% 32.46% 31.36% 32.84% 32.93% 32.01% 73.40% 71.81% 80.03% 74.80% 80.69% 79.33% 79.73% 79.44% 81.79% 78.79% 77.04% 78.16% 78.56% 63.58% 44.48% 68.06% 43.53% 69.85% 69.08% 38.05% 54.10% 35.35% 54.33% 64.78% 48.26% 74.43% 37.71% 57.33% 80.96% 62.39% 46.06% 78.72% 78.72% 80.24% 80.00% 80.88% 80.72% 70.66% 49.53% 67.16% 48.58% 68.36% 47.63% 70.15% 46.69% 69.25% 48.26% 71.64% 49.21% 68.32% 73.66% 64.12% 66.79% 66.41% 69.85% 70.23% 38.38% 55.34% 37.04% 58.35% 36.36% 55.76% 38.05% 56.59% 38.72% 56.93% 36.70% 57.70% 36.36% 57.53% Table 6: Performance change of Video-Thinker with different learning rates. The best results are marked in red bold and the second best in blue. Model Qwen2.5-VL-7B-Instruct Video-R1-7B Video-Thinker-7B LR - - 1e3e-6 5e-6 1e-5 Out of Domain In Domain Video-Holmes CG-Bench-Reasoning VRBench ActivityNet Star ScaleLong YouCook2 LVBench 34.02% 38.54% 39.14% 36.91% 43.22% 16.44% 27.10% 27.81% 28.97% 24.45% 33.25% 6.86% 63.42% 69.25% 72.79% 77.18% 80.69% 18.74% 70.96% 76.00% 80.08% 73.20% 69.25% 67.76% 63.88% 57.01% 40.06% 47.32% 46.37% 41.01% 63.74% 65.65% 66.79% 63.74% 33.33% 34.68% 36.70% 32.32% 78.72% 70.66% 49.53% 73.66% 37.04% 21.20% 23.58% 15.14% 1.14% 16.16% Captioning: Questions are enhanced with both temporal grounding and captioning information. We employ accuracy as our primary evaluation metric to assess reasoning performance across all configurations. As shown in Table 4, both grounding and captioning information significantly enhance video reasoning performance. Captioning provides the largest individual improvement (37%56%), while grounding contributes substantial gain (37%53%). The combination of both information types achieves the best performance at 63% accuracy, demonstrating clear synergistic effects. This suggests that grounding and captioning provide complementary benefits: grounding enables temporal focus on relevant segments, while captioning offers comprehensive contextual understanding."
        },
        {
            "title": "F ABLATION STUDIES",
            "content": "Impact of Training Steps. To investigate the impact of GRPO training steps on Video-Thinkers reasoning capabilities and generalization performance, we perform GRPO on Video-Thinker-SFT-7B for varying steps from 500 to 5000 steps, saving checkpoints every 500 steps and evaluating each on both in-domain and out-of-domain benchmarks. As shown in Table 5, Video-Thinker achieves optimal performance at 2500 training steps with an average score of 58.35%, demonstrating superior results across most benchmarks. This peak performance at 2500 steps indicates an effective balance between sufficient learning and avoiding overfitting, as further training beyond this point leads to performance degradation on several benchmarks, particularly in out-of-domain scenarios, suggesting that excessive training steps may compromise the models generalization ability while potentially overfitting to the training distribution. Impact of Learning Rate. To investigate the impact of learning rate in GRPO on Video-Thinkers performance, we conduct GRPO training with four different initial learning rates (1e-6, 3e-6, 5e-6, 1e-5) and compare the results against the base model Qwen2.5-VL-7B-Instruct and the previous state-of-the-art Video-R1-7B across all in-domain and out-of-domain benchmarks. As demonstrated in Table 6, Video-Thinker achieves optimal performance with learning rate of 5e-6, significantly"
        },
        {
            "title": "Preprint",
            "content": "outperforming both baseline models, including substantial improvements on out-of-domain tasks, while maintaining strong in-domain performance. Notably, the dramatic performance degradation at 1e-5 learning rate indicates that excessively high learning rates lead to training instability and poor convergence, while the moderate 5e-6 setting strikes an optimal balance between effective learning and stable optimization, enabling Video-Thinker to achieve superior video reasoning capabilities. Figure 6: An example of Video-Thinker-7Bs reasoning output on Video-Holmes dataset"
        },
        {
            "title": "Preprint",
            "content": "Figure 7: An example of Video-Thinker-7Bs reasoning output on Video-Holmes dataset"
        },
        {
            "title": "G CASES",
            "content": "In addition to the cases presented in Figure 4, we provide supplementary examples of Video-Thinker7Bs performance across diverse datasets in Figures 6, 7, 8, 9, 10, 11, 12, which demonstrate the models capacity for iterative reasoning and error correction. This self-corrective behavior suggests that Video-Thinker transcends simple pattern matching and instead engages in dynamic internal feedback mechanism."
        },
        {
            "title": "H USE OF LLMS",
            "content": "During the preparation of this manuscript, we made limited use of publicly available large language models (LLMs) to assist with English writing. All technical content, including the formulation of ideas, design of methodologies, implementation of experiments, and interpretation of results, was entirely conceived and written by the authors without the involvement of LLMs. The role of LLMs was strictly confined to stylistic and linguistic improvements, in manner comparable to grammaror spell-checking software. We ensured that no novel research insights, data, or analyses were generated by LLMs, and all scientific claims and results presented in this work remain the sole responsibility of the authors."
        },
        {
            "title": "Preprint",
            "content": "Figure 8: An example of Video-Thinker-7Bs reasoning output on VRBench dataset"
        },
        {
            "title": "Preprint",
            "content": "Figure 9: An example of Video-Thinker-7Bs reasoning output on VRBench dataset"
        },
        {
            "title": "Preprint",
            "content": "Figure 10: An example of Video-Thinker-7Bs reasoning output on CG-Bench dataset"
        },
        {
            "title": "Preprint",
            "content": "Figure 11: An example of Video-Thinker-7Bs reasoning output on CG-Bench dataset"
        },
        {
            "title": "Preprint",
            "content": "Figure 12: An example of Video-Thinker-7Bs reasoning output on CG-Bench dataset"
        },
        {
            "title": "Preprint",
            "content": "Figure 13: An example of Video-Thinker-7Bs reasoning output on Video-Holmes dataset"
        },
        {
            "title": "Preprint",
            "content": "Figure 14: An example demonstrates Video-R1-7Bs inability to follow instructions for generating temporal grounding content within <time></time> tags, thereby illustrating the rationale behind the statement in Section 4.3: Note that Video-R1 is excluded from this evaluation due to its inability to follow our prompt to generate temporal annotations within our templates.."
        }
    ],
    "affiliations": [
        "Fudan University",
        "Monash University",
        "Southeast University",
        "University of Southern California",
        "Xiaohongshu Inc."
    ]
}