{
    "paper_title": "How Far Are Surgeons from Surgical World Models? A Pilot Study on Zero-shot Surgical Video Generation with Expert Assessment",
    "authors": [
        "Zhen Chen",
        "Qing Xu",
        "Jinlin Wu",
        "Biao Yang",
        "Yuhao Zhai",
        "Geng Guo",
        "Jing Zhang",
        "Yinlu Ding",
        "Nassir Navab",
        "Jiebo Luo"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Foundation models in video generation are demonstrating remarkable capabilities as potential world models for simulating the physical world. However, their application in high-stakes domains like surgery, which demand deep, specialized causal knowledge rather than general physical rules, remains a critical unexplored gap. To systematically address this challenge, we present SurgVeo, the first expert-curated benchmark for video generation model evaluation in surgery, and the Surgical Plausibility Pyramid (SPP), a novel, four-tiered framework tailored to assess model outputs from basic appearance to complex surgical strategy. On the basis of the SurgVeo benchmark, we task the advanced Veo-3 model with a zero-shot prediction task on surgical clips from laparoscopic and neurosurgical procedures. A panel of four board-certified surgeons evaluates the generated videos according to the SPP. Our results reveal a distinct \"plausibility gap\": while Veo-3 achieves exceptional Visual Perceptual Plausibility, it fails critically at higher levels of the SPP, including Instrument Operation Plausibility, Environment Feedback Plausibility, and Surgical Intent Plausibility. This work provides the first quantitative evidence of the chasm between visually convincing mimicry and causal understanding in surgical AI. Our findings from SurgVeo and the SPP establish a crucial foundation and roadmap for developing future models capable of navigating the complexities of specialized, real-world healthcare domains."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 ] . [ 1 5 7 7 1 0 . 1 1 5 2 : r How Far Are Surgeons from Surgical World Models? Pilot Study on Zero-shot Surgical Video Generation with Expert Assessment Zhen Chen1, Qing Xu2, Jinlin Wu3, Biao Yang4, Yuhao Zhai5, Geng Guo4, Jing Zhang5, Yinlu Ding5, Nassir Navab6, Jiebo Luo7 1Yale University 2University of Nottingham 3Institute of Automation, Chinese Academy of Sciences 4Department of Neurosurgery, The First Hospital, Shanxi Medical University 5Department of Gastrointestinal Surgery, The Second Qilu Hospital, Shandong University 6Technical University of Munich 7University of Rochester Corresponding author Foundation models in video generation are demonstrating remarkable capabilities as potential world models for simulating the physical world. However, their application in high-stakes domains like surgery, which demand deep, specialized causal knowledge rather than general physical rules, remains critical unexplored gap. To systematically address this challenge, we present SurgVeo, the first expert-curated benchmark for video generation model evaluation in surgery, and the Surgical Plausibility Pyramid (SPP), novel, four-tiered framework tailored to assess model outputs from basic appearance to complex surgical strategy. On the basis of the SurgVeo benchmark, we task the advanced Veo-3 model with zero-shot prediction task on surgical clips from laparoscopic and neurosurgical procedures. panel of four board-certified surgeons evaluates the generated videos according to the SPP. Our results reveal distinct \"plausibility gap\": while Veo-3 achieves exceptional Visual Perceptual Plausibility, it fails critically at higher levels of the SPP, including Instrument Operation Plausibility, Environment Feedback Plausibility, and Surgical Intent Plausibility. This work provides the first quantitative evidence of the chasm between visually convincing mimicry and causal understanding in surgical AI. Our findings from SurgVeo and the SPP establish crucial foundation and roadmap for developing future models capable of navigating the complexities of specialized, real-world healthcare domains. Date: October 30, 2025 Correspondence: zchen.francis@gmail.com"
        },
        {
            "title": "1 Introduction",
            "content": "The pursuit of Artificial General Intelligence (AGI) hinges on developing systems that can understand and interact with the world in flexible, generalizable ways rather than excelling at narrow, predefined tasks [1]. fundamental challenge in this pursuit is enabling machines to build internal representations of how the world works, i.e., understanding not just what is observed, but how environments evolve, how actions lead to consequences, and how future states can be anticipated and reasoned about [2]. This challenge has catalyzed fundamental research into world models, as learning representations that encode environmental dynamics and enable prediction, planning, and reasoning. Serving as cornerstone for AGI systems, world models capture the underlying principles governing how the world evolves in response to actions and interventions [3, 4, 5, 6]. These models are not merely passive observers but active simulators capable of anticipating future states, evaluating potential outcomes, and supporting decision-making across diverse contexts [7, 8]. The ability to build accurate world models represents critical step toward systems that can understand, interact with, and reason about complex environments in ways that approach human-level intelligence. Recent advances in video generation have demonstrated paradigm shift in the pursuit of world models, with state-of-the-art models demonstrating remarkable capabilities that position them as potential general-purpose world simulators [9, 10, 11]. For instance, Veo-3 [10], trained on large-scale video data, introduces the Chain-of-Frames (CoF) reasoning mechanism, which parallels chain-of-thought reasoning in language models but operates across the spatiotemporal dimensions of the real world. While language models manipulate human-invented symbols through sequential reasoning steps [12, 13], video generation models apply changes frame-by-frame across time and space, enabling step-by-step visual reasoning that tackles challenging problems requiring temporal and spatial understanding. Through this sophisticated CoF approach integrated with advanced spatiotemporal architectures, these models exhibit emergent physical understanding in natural scenes [10]. In particular, they perceive 3D spatial relationships and object properties, model temporal dynamics and long-range coherence, manipulate object interactions and material responses, and reason about physical causality and event sequences, thereby achieving photorealistic video generation with temporal consistency, maintaining object permanence, realistic physics including gravity and collision dynamics, and coherent camera motion with proper parallax effects. These impressive results have garnered significant attention across diverse research communities, highlighting the potential of video generation models as foundation world models for understanding and simulating the physical world [14, 15]. Despite the potential of video generation models as world simulators and their remarkable performance in natural scenes, their application to healthcare contexts remains underexplored, critical gap given that medical domains require not only visual perception but also deep expert knowledge fundamentally different from general physical understanding [16]. Healthcare is characterized by complex causal relationships involving physiological processes, anatomical structures, and biological mechanisms, where interventions produce outcomes governed by intricate interactions between tissues, organs, and therapeutic agents. Understanding and predicting these dynamics demands specialized domain expertise spanning anatomy, pathology, biomechanics, and clinical reasoning accumulated through years of rigorous training. Surgical procedures represent particularly demanding testbed for evaluating whether world models can transcend common-sense reasoning to capture this specialized knowledge. Surgery epitomizes the complexity of medical practice, involving real-time dynamic interactions between surgical instruments, anatomical tissues, and biological systems under precise spatiotemporal constraints [17, 18, 19, 20, 21]. Unlike diagnostic imaging [22] or treatment planning [23], where static or slowly-evolving states dominate, surgical contexts require understanding rapid instrument movements, tissue deformation and biomechanical responses, fluid dynamics of bleeding and irrigation, and the intricate cause-and-effect relationships between instrument actions and anatomical changes governed by expert procedural knowledge [24, 25, 26, 27, 28, 29]. The gap between the general physical rules learned from natural videos and the specialized knowledge required for high-stakes surgical domains poses fundamental question: can current world models bridge this divide to achieve not merely visual plausibility but surgical plausibility necessary for meaningful clinical applications? To answer this fundamental question, we present the first systematic evaluation of state-of-the-art video generation models potential to serve as world model in the surgical domain. We introduce the SurgVeo benchmark, novel dataset curated specifically for this task. It comprises 50 video clips sourced from six independent recordings of laparoscopic hysterectomy [30] and endoscopic pituitary surgery [31], covering diverse range of procedural stages and complexities to ensure representative testbed. Using this benchmark, we task the Veo-3 model [10] with generating 8-second video continuations from single input frame under two prompting conditions: baseline prompt and stage-aware prompt. To move beyond superficial visual metrics and enable deeper, clinically-grounded assessment, we devise the Surgical Plausibility Pyramid (SPP), four-tiered evaluation hierarchy designed to dissect models capabilities, as elaborated in Fig. 1. The SPP progresses from the concrete to the abstract: (1) Visual Perceptual Plausibility, assessing the basic appearance of the scene, (2) Instrument Operation Plausibility, evaluating the physical action of instruments, (3) Environment Feedback Plausibility, measuring the causal consequence of actions on tissue, and (4) Surgical Intent Plausibility, examining the overarching strategy behind the procedure. On the basis of SPP, the SurgVeo benchmark is assessed by panel of four board-certified surgeons, as illustrated in Fig. 2. Our results reveal that Veo-3 achieves high scores in visual perceptual plausibility, demonstrating its remarkable capability in generating photorealistic surgical imagery, with some outputs even surprising expert surgeons. However, the assessment exhibits significant shortcomings in higher-level domains, such as Instrument Operation Plausibility and Environment Feedback Plausibility, failing to produce realistic instrument trajectories and biomechanical tissue responses. Additionally, in terms of Surgical Intent Plausibility, Veo-3 struggles to infer the intent behind surgical actions based on the input video and stage information, highlighting its inability to recognize procedural goals. These findings expose critical gap between generating visually convincing videos and capturing the specialized causal knowledge essential for expert-level understanding in high-stakes surgical contexts. On the basis of the SurgVeo benchmark and the SPP, our professional evaluations provide valuable insights into the challenges in developing surgical world models that bridge general physical reasoning and Figure 1 (a) The Surgical Plausibility Pyramid (SPP) framework, illustrating four hierarchical assessment dimensions: (i) Visual Perceptual Plausibility at the appearance level, assessing the clarity and stability of generated videos, (ii) Instrument Operation Plausibility at the action level, judging the accuracy and technical proficiency of instrument manipulation, (iii) Environment Feedback Plausibility at the consequence level, measuring the realism and credibility of scene feedback, and (iv) Surgical Intent Plausibility at the Strategy level, evaluating the appropriateness and clinical reasoning of surgical actions. (b) Detailed 5-point scoring rubrics (5=excellent to 1=poor) for evaluating each dimension. domain-specific expertise. Our results have significant implications for clinical applications, including surgical training, preoperative planning, intraoperative guidance systems, and autonomous surgical robotics. To foster further research, we will publicly release the SurgVeo benchmark and feedback from expert surgeons."
        },
        {
            "title": "2 The SurgVeo Benchmark",
            "content": "To systematically evaluate the capabilities of state-of-the-art video generation models as world models in the surgical domain, we curate the SurgVeo benchmark and conduct comprehensive assessment with the help of panel of four surgeon experts. This study mainly comprises three core components, including (1) the surgical data preparation, (2) standardized zero-shot video generation task, and (3) rigorous, multi-dimensional evaluation protocol executed by four board-certified surgeons. The pipeline of this study is summarized in Fig."
        },
        {
            "title": "2.1 Surgical Data Preparation\nWe introduce the SurgVeo benchmark, the first publicly available benchmark specifically designed for evaluating\nthe world modeling capabilities of video generation models in surgical contexts. Unlike existing video generation\nbenchmarks that focus on natural scenes [32] or general-purpose video quality metrics [33], the SurgVeo\nbenchmark is tailored to assess whether state-of-the-art video generation models can capture the specialized\ndynamics, causal relationships, and domain-specific knowledge inherent in surgical procedures.",
            "content": "2.1.1 Rationale and Selection of Surgical Procedures The primary objective of the SurgVeo benchmark is to assess the capability of the video generation models to comprehend complex causal relationships within the specialized surgery domain. To this end, we select two representative yet distinct surgical procedures that present unique challenges: laparoscopic hysterectomy [30] and endoscopic pituitary surgery [31]. For the laparoscopic surgery track, laparoscopic hysterectomy is common minimally invasive procedure, and represents the challenges of endoscopic camera views, including instrument interaction in confined space, soft tissue deformation, and adherence to specific surgical workflow. For the neurosurgery track, endoscopic pituitary surgery is high-precision neurosurgical procedure, and represents the demand for simulating delicate, high-stakes maneuvers, including navigating fine anatomical Figure 2 The overall pipeline of this study. (a) The overview of the SurgVeo benchmark preparation and evaluation workflow. The surgical video dataset is processed to create the paired surgical frame and surgical video continuation. The Veo model takes the surgical frame with prompt as input to generate the surgical video prediction. panel of four board-certified surgeons evaluates the generated surgical videos against the real surgical video continuation as reference under the Surgical Plausibility Pyramid (SPP). (b) The illustration of the generation and evaluation process for single sample in the SurgVeo benchmark. starting surgical frame and text prompt are fed into the Veo model to generate an 8-second surgical video prediction. This output is then scored by expert surgeons by comparing it to the real 8-second reference video with focus on four dimensions of surgical plausibility, particularly at the 1-second, 3-second, and 8-second time points. structures from an endoscopic view, avoiding critical neurovascular bundles, and modeling extremely subtle instrument manipulations. 2.1.2 Data Sourcing and Structure To ensure data diversity and representativeness, we source full-length, high-definition surgical video recordings from established public datasets. For the laparoscopic surgery track, we collect 3 independent laparoscopic hysterectomy surgeries from the AutoLaparo dataset [30] with the resolution of 1, 920 1, 080 at 25 framesper-second (fps). For the neurosurgery track, we adopt 3 independent endoscopic pituitary surgeries from the PitVis dataset [31] with the resolution of 1, 280 720 at 24 fps. From these surgical video recordings, we curate surgical video clips that comprehensively cover the procedural workflow. The laparoscopic hysterectomy is categorized into seven distinct stages, including the preparation, dividing the ligament and peritoneum, dividing the uterine vessels and ligament, transecting the vagina, specimen removal, suturing, and washing. The endoscopic pituitary surgery is segmented into twelve stages, including the nasal corridor creation, anterior sphenoidotomy, septum displacement, sphenoid sinus clearance, hemostasis, sellotomy, durotomy, tumor excision, synthetic graft placement, fat graft placement, dural sealant, and debris clearance. In total, the SurgVeo benchmark comprises 50 distinct surgical clips. As such, we process each surgical clip into sample in the SurgVeo benchmark, which consists of two components, including the input frame and the referenced surgical video. In particular, the input frame captures the surgical scene immediately preceding procedural action, and the referenced surgical video, from the original surgical recording that immediately follows the input frame, serves as the reference for expert evaluation. Detailed information about the benchmark tracks, statistics, and data structure can be found in Appendix A, Appendix and Appendix C. In this way, the inclusion of different surgical specialties, multiple independent procedures, and comprehensive stage coverage ensures the representativeness and diversity of the SurgVeo benchmark. To foster further research and enable rigorous comparison of future models, the complete SurgVeo benchmark will be publicly released (see Appendix I)."
        },
        {
            "title": "2.2 Surgical Video Generation",
            "content": "2.2.1 Zero-Shot Video Generation Task The core task of this study is to evaluate the zero-shot surgical video generation of the advanced video generation models. The Veo-3 model is adopted without any fine-tuning on surgical video data, and is tasked to generate logically coherent and surgically plausible 8-second video continuation based solely on the input frame and text prompt. This formulation tests whether video generation models have internalized sufficient understanding of surgical dynamics, instrument behavior, tissue responses, and procedural logic to extrapolate plausible future states from limited visual information. The input frame constraint is deliberately challenging, designed to assess whether models possess genuine surgical knowledge rather than simply extending patterns observable in longer contexts. The 8-second prediction horizon requires maintaining temporal coherence, physical plausibility, and surgical logic over clinically meaningful duration, encompassing complete surgical actions such as tissue dissection sequences, instrument exchanges, or hemostatic maneuvers. 2.2.2 Prompting Strategy To investigate the influence of procedural knowledge on the video quality of video generation models, we design two distinct prompting conditions, including the baseline prompt and the stage-aware prompt. 1. Baseline Prompt: Provide only the procedure type (e.g., \"This is laparoscopic hysterectomy\") and general requirements for visual quality and anatomical realism. This condition tests whether the model can autonomously infer appropriate next steps from visual context alone. 2. Stage-aware Prompt: In addition to baseline information, we further specify the current surgical stage explicitly (e.g., \"The current stage is vessel ligation\"). This condition tests whether providing explicit contextual knowledge improves the surgical plausibility of the generated video. This dual-prompting design enables systematic assessment of whether current video generation models can recognize surgical stages from visual information or require additional textual guidance, distinction critical for understanding the depth of their learned surgical knowledge. Complete details of the task formulation, prompting strategy, and specific prompt templates used for both surgical tracks are provided in Appendix D, and Appendix E."
        },
        {
            "title": "2.3 Expert Evaluation",
            "content": "A distinguishing strength of this work lies in the rigorous expert evaluation protocol. This expert-driven approach is critical because it transcends superficial visual quality metrics to examine whether generated videos demonstrate genuine understanding of surgical reasoning, instrument operation, and tissue biomechanics. 2.3.1 Evaluation Panel The evaluation is conducted by panel of four board-certified surgeons with extensive clinical experience. To ensure domain-specific expertise, two laparoscopic surgeons independently evaluate the laparoscopic hysterectomy videos of the laparoscopic surgery track, and two neurosurgeons independently evaluate the endoscopic pituitary surgery videos of the neurosurgery track. We posit that human expertise is indispensable for accurately assessing the subtle yet critical aspects of surgical plausibility, nuance that cannot be captured by automated metrics. Table 1 Evaluation scores of SurgVeo benchmark on the laparoscopic surgery track across different time points and prompt strategies. Scores are reported on 1-5 scale, where 5 is the best, and the standard deviation is calculated with the average score of two laparoscopic surgery experts. Prompt Strategy Baseline Prompt Stage-aware Prompt Time Point 1 3 8 1 3 8 Visual Perceptual Plausibility 3.72 0.24 3.69 0.20 3.56 0.31 3.61 0.24 3.58 0.27 3.39 0.47 Instrument Operation Plausibility 3.36 0.20 2.33 0.16 1.78 0.00 3.22 0.16 2.31 0.04 1.69 0.20 Environment Feedback Plausibility 3.06 0.08 2.06 0.24 1.64 0.12 3.11 0.24 2.08 0.04 1.53 0.12 Surgical Intent Plausibility 3.11 0.16 2.03 0.35 1.61 0.16 3.22 0.00 2.11 0.24 1.81 0. 2.3.2 Surgical Plausibility Pyramid We propose the Surgical Plausibility Pyramid (SPP) framework to structure the evaluation, with four hierarchical dimensions progressing from concrete visual attributes to abstract strategic reasoning. From the base to the apex of the pyramid, these four hierarchical dimensions are summarized as follows: 1. Visual Perceptual Plausibility: Assess the fundamental visual quality and appearance of the generated surgical scene, including clarity, lighting, tissue texture realism, and video smoothness. 2. Instrument Operation Plausibility: Evaluate the physical action, focusing on whether the changes, trajectories, and manipulation techniques of surgical instruments are technically sound and physically plausible. 3. Environment Feedback Plausibility: Measure the direct consequence of the action, evaluating whether the responses of tissues and organs (e.g., deformation, bleeding) conform to biomechanical and anatomical principles. 4. Surgical Intent Plausibility: Examine the highest level of abstraction, the underlying strategy, by assessing whether the predicted sequence of actions demonstrates clear, logical, and stage-appropriate objective. In essence, these dimensions provide multi-level assessment of the generated video, evaluating its appearance, the plausibility of the depicted action, the realism of its consequence, and the coherence of the underlying surgical strategy, as elaborated in Fig. 1. We further demonstrate the detailed scoring criteria of the SPP dimensions in Appendix G. For the evaluation, surgeons use the 8-second real surgical video as professional reference to understand the context and what correct surgical progression entails. With this reference in mind, they then independently score the 8-second generated video, providing scores on 5-point integer scale (1 = Very Poor; 5 = Indistinguishable from Reality) at three specific temporal checkpoints: 1-second, 3-second, and 8-second. This temporal evaluation structure allows for tracking the degradation or maintenance of plausibility over the prediction horizon. Taken together, this multi-faceted evaluation protocol establishes robust and reproducible framework for quantifying the gap between visual realism and true surgical understanding in advanced video generation models, providing critical tool for future research in this domain."
        },
        {
            "title": "3 Results",
            "content": "Our experimental results reveal profound disconnect between Veo-3s capacity for visual synthesis and its understanding of surgical knowledge. The detailed evaluation scores and score distributions, presented for the laparoscopic surgery track in Table 1 and Fig. 3 and for the neurosurgery track in Table 2 and Fig. 4, provide comprehensive quantitative evidence for this gap. In addition, we present case-by-case analysis of representative high-scoring and low-scoring examples from the SurgVeo benchmark in Appendix H, contextualized with synthesized expert feedback. The findings are organized below by an analysis of performance across the Surgical Plausibility Pyramid, the temporal degradation of plausibility, discrepancies Table 2 Evaluation scores of SurgVeo benchmark on neurosurgery track across different time points and prompt strategies. Scores are reported on 1-5 scale, where 5 is the best and the standard deviation is calculated with the average score of two neurosurgery experts. Prompt Strategy Baseline Prompt Stage-aware Prompt Time Point 1 3 8 1 3 Visual Perceptual Plausibility 3.88 0.09 3.53 0.22 3.41 0.22 3.84 0.04 3.39 0.07 3.25 0.09 Instrument Operation Plausibility 2.77 0.02 2.08 0.11 1.75 0.04 2.58 0.07 2.02 0.07 1.75 0.04 Environment Feedback Plausibility 2.84 0.09 2.16 0.00 1.78 0.18 2.64 0.02 2.00 0.00 1.73 0.07 Surgical Intent Plausibility 2.03 0.09 1.42 0.20 1.13 0.04 1.97 0.13 1.42 0.07 1.17 0.02 between surgical specialties, the ineffectiveness of prompting strategies, and detailed qualitative and quantitative video error analysis."
        },
        {
            "title": "3.1 Performance Across the Surgical Plausibility Pyramid",
            "content": "Our primary finding is stark dichotomy in the performance of the generated surgical videos, clearly illustrated across both surgical types (Table 1 and Table 2). The Veo-3 consistently excels at the base of the Surgical Plausibility Pyramid, achieving high scores in Visual Perceptual Plausibility. For both laparoscopic and neurosurgical procedures, the mean initial scores for this dimension are high (e.g., the baseline prompt achieves 3.72 0.24 and 3.88 0.09, respectively). In addition, the violin plots (Fig. 3 and Fig. 4) visually confirm this: the score distributions for this dimension are tightly clustered at the high end of the scale (i.e., mostly between 3.0 and 5.0). Surgeons note the imagery is often \"shockingly clear\" and texturally realistic. However, this quality collapses when assessed against the higher levels of the pyramid, which require causal understanding. The mean scores in Table 1 and Table 2 for Instrument Operation, Environment Feedback, and Surgical Intent Plausibility are much lower, often falling below 2.0. In laparoscopic procedures with stage-aware prompt  (Table 1)  , the average scores for Instrument Operation (1.69 0.20), Environment Feedback (1.53 0.12), and Surgical Intent (1.81 0.20) at the 8-second are all critically low. The deficit is even more pronounced in neurosurgery  (Table 2)  , where scores for the same dimensions are consistently poor from the outset (e.g., 1.13 0.04 and 1.17 0.02 at the 8-second with the baseline prompt and stage-aware prompt, respectively). On the other hand, the violin plots in Fig. 3 and Fig. 4 dramatically visualize this failure: the distributions for these three dimensions are heavily skewed, with the vast majority of scores concentrated at the bottom of the scale (between 1.0 and 2.5). This \"plausibility gap\", the difference between high visual quality and low surgical logic, is the central quantitative finding of our study."
        },
        {
            "title": "3.2 Temporal Degradation of Plausibility",
            "content": "Analysis of the scoring data across time points reveals consistent and significant degradation in surgical plausibility as the prediction horizon extends. This trend is detailed in the mean scores of Table 1 and Table 2 and visualized in the score distributions of Fig. 3 and Fig. 4. critical observation from the violin plots (Fig. 3 and Fig. 4) is the differential stability of the SPP dimensions over time. For both surgery types and both prompting strategies, the score distributions for Visual Perceptual Plausibility remain relatively high and stable across the 1, 3, and 8-second time points. In contrast, the distributions for the three higher-level dimensions, Instrument Operation, Environment Feedback, and Surgical Intent Plausibility, show clear and significant downward trend, with the violins becoming progressively more compressed at the bottom of the scale as time progresses. This is numerically evident in the mean scores. For instance, in the laparoscopic evaluations (Table 1 with the baseline prompt), the score for Environment Feedback Plausibility plummeted from 3.06 0.08 at 1 second to 1.64 0.12 by 8 seconds, drop of nearly 46%. Similarly, Surgical Intent Plausibility fell from 3.11 0.16 to 1.61 0.16. This pattern is consistent with our central SPP conclusion: the Veo model excels at generating basic visual appearance, which it can maintain, but fails at complex surgical logic. As the prediction horizon Figure 3 Violin plots illustrating the performance on the laparoscopic surgery track in the SurgVeo benchmark. Results are shown for (a) the baseline prompt and (b) the stage-aware prompt. The performance is assessed across four evaluation dimensions in the SPP, with three progressively deeper shades representing evaluations at 1-second, 3-second, and 8-second. Each sample point reflects the average score provided by two laparoscopic surgery experts. Figure 4 Violin plots illustrating the performance on the neurosurgery track in the SurgVeo benchmark. Results are shown for (a) the baseline prompt and (b) the stage-aware prompt. The performance is assessed across four evaluation dimensions in the SPP, with three progressively deeper shades representing evaluations at 1-second, 3-second, and 8-second. Each sample point reflects the average score provided by two neurosurgery experts. increases, the difficulty of maintaining this logic compounds, leading to an accumulation of predictive errors and catastrophic failure in long-range coherence."
        },
        {
            "title": "3.3 Discrepancies Between Surgical Specialties",
            "content": "While the performance of high-level plausibilities is poor across both surgical specialties, the results indicate that simulating neurosurgery is more challenging task. This is evident not only in the mean scores from Table 1 and Table 2 but also in the score distributions shown in Fig. 3 and Fig. 4. direct comparison shows that the plausibility scores for neurosurgery are typically lower since the very first second of generation. For example, under the baseline prompt at the 1-second, Instrument Operation Plausibility is rated 3.36 0.20 for laparoscopy but only 2.77 0.02 for neurosurgery. The violin plots reinforce this: the distributions for the three higher-level plausibility dimensions in the neurosurgery track  (Fig. 4)  are even more severely compressed at the bottom of the scale than their counterparts in the laparoscopic surgery track  (Fig. 3)  . This supports our hypothesis that the heightened precision, delicate tissue handling, and microscopic scale of neurosurgery present more complex set of implicit \"rules\" that the Veo model fails to capture."
        },
        {
            "title": "3.4 Ineffectiveness of Stage-Aware Prompting",
            "content": "Our comparative analysis between prompting strategies reveals that providing additional contextual information yields no significant or consistent improvement in surgical plausibility. Across both Table 1 and Table 2, the scores for the stage-aware prompt are not meaningfully higher than those for the baseline prompt. In several instances, such as for Instrument Operation Plausibility in laparoscopy at the 1-second, the stage-aware score (3.22 0.16) is even lower than the baseline (3.36 0.20). The violin plots in Fig. 3 (a vs. b) and Fig. 4 (a vs. b) make this finding visually irrefutable. side-by-side comparison of the baseline and stage-aware plots for each surgery type shows similar distributions for all plausibility dimensions. This finding strongly reinforces our conclusion that the Veo models limitations arise from fundamental inability to reason about domain-specific knowledge, rather than mere lack of context. Figure 5 Qualitative examples of typical failures identified in the generated videos. Each example presents side-by-side comparison of the real surgical frame (left) and the generated surgical frame (right). These examples elaborate on failures across the Surgical Plausibility Pyramid, including: (a) visual quality distortions, (b) surgical instrument errors, (c) inappropriate surgical operations, (d) inappropriate surgical targets, (e) environment feedback errors, and (f) surgical intent errors. Red arrows indicate specific illogical, anatomically incorrect, or physically impossible artifacts."
        },
        {
            "title": "3.5 Video Error Analysis: Qualitative Examples and Quantitative Distribution",
            "content": "To provide comprehensive overview of the Veo models failures, we compile the primary error categories identified by our four expert surgeons during their evaluation of the entire SurgVeo benchmark. Fig. 5 presents typical visual examples of these critical failures, comparing the generated frame to the real-world reference video. These examples visually corroborate the surgeons qualitative feedback, showcasing specific, critical failures: Visual quality distortions: The generated video exhibits sudden, unnatural increase in brightness, inconsistent with stable surgical lighting conditions. Surgical instrument errors: The model hallucinates non-existent, fabricated instrument, whereas the real procedure uses standard scalpel. Inappropriate surgical operations: The generated video shows the dissector moving to the right, which is procedurally incorrect for the scene; the real operation requires leftward movement. Inappropriate surgical targets: The model depicts an instrument manipulating mucus, while the correct procedure involves coordinated action of irrigation and suction on different target. Environment feedback errors: The model violates physical laws by showing suction tool pulling an entire block of gel-foam as if it is solid, attached mass, rather than suctioning loose fluid and debris from its surface. Surgical intent errors: The real procedure involves injecting biologic glue onto the dura, but the model incorrectly predicts completely different intent, i.e., wiping the dura with cotton patty. Then, we quantify the frequency of these error types, with the distribution compiled from the baseline prompt evaluations shown in Fig. 6. This quantitative analysis provides striking confirmation of the \"plausibility gap\". Across both surgery types, basic visual quality distortions account for only minuscule Figure 6 Distribution of error types identified by expert surgeons in generated videos of the SurgVeo benchmark. The charts quantify the frequency of different failures in surgical plausibility for (a) the laparoscopic surgery track and (b) the neurosurgery track. Across both specialties, errors related to high-level surgical logic, such as Surgical intent errors, Surgical instrument errors, and Inappropriate surgical operations, constitute the vast majority of all failures. In contrast, basic Visual quality distortions represent only small fraction of the total errors, reinforcing the finding of the plausibility gap. fraction of all identified failures: 6.2% in laparoscopic surgery (Fig. 6(a)) and mere 2.8% in neurosurgery (Fig. 6(b)). Conversely, the vast majority of errors (over 93% in both cases) are critical failures in surgical logic. In both procedures, the four most dominant error categories are surgical intent errors (21.9% in laparoscopic surgery, 22.0% in neurosurgery), environment feedback errors (17.2% in laparoscopic surgery, 22.0% in neurosurgery), surgical instrument errors (17.2% in laparoscopic surgery, 17.7% in neurosurgery), and inappropriate surgical operations (15.6% in laparoscopic surgery, 21.3% in neurosurgery). In neurosurgery (Fig. 6(b)), these four high-level logical failures alone constitute 83% of all errors. This quantitative breakdown irrefutably demonstrates that the Veo models failure is not in visual rendering, but in its fundamental lack of understanding of surgical intent, action, and consequence."
        },
        {
            "title": "4 Discussions",
            "content": "Our study provides sobering but essential assessment of the current capabilities of video generation models as world models for the specialized domain of surgery. The central finding is the profound \"plausibility gap\" between the Veo models ability to render visually convincing surgical scenes and its complete failure to comprehend the underlying causal principles governing them. While Veo-3 can replicate the appearance of surgery with stunning fidelity, it lacks any semblance of surgical understanding, operating as sophisticated pattern-matcher rather than knowledgeable simulator. The ineffectiveness of our stage-aware prompting strategy is key diagnostic result. Providing the model with explicit contextual information about the surgical stage did not significantly improve plausibility, revealing that the core deficit is not lack of information but fundamental inability to process it. The model appears to lack the foundational knowledge required to translate an abstract concept like \"vessel ligation\" into concrete, physically sound sequence of actions and consequences. This suggests that the models internal representations, learned from general-domain videos, are not structured to accommodate the complex, rule-based logic of surgery. This highlights critical challenge for the future of world models. While models trained on massive, diverse datasets can learn the \"common-sense physics\" of the everyday world, surgery operates on different set of principles: the \"expert-sense\" of anatomy, physiology, and biomechanics. Our findings suggest that merely scaling up training on general data will be insufficient to bridge this divide. Achieving true world modeling in expert domains will likely require new architectural paradigms capable of integrating structured, domain-specific knowledge and enforcing hard physical and logical constraints on the generative process. Despite these limitations, the clinical appetite for true surgical world model is immense. Such model could revolutionize medical education by providing trainees with interactive, high-fidelity simulators for procedural practice. It could also enhance patient safety by enabling pre-operative rehearsal of complex cases or powering intra-operative guidance systems that monitor for risks and deviations from the optimal surgical plan. Our work, by establishing the SurgVeo benchmark and quantifying the current performance gap, takes crucial first step toward this future. The path from the current state-of-the-art to clinically useful surgical simulator requires targeted research to address the specific deficits identified by SurgVeo. Future efforts should focus on two key directions. First, explicitly incorporating surgical knowledge into world models is paramount. This can leverage existing research in surgical and endoscopic video generation [34, 35], providing the necessary foundation for models to learn the rules of surgical practice, instrument usage, and anatomical interactions. Second, advancements in the world model technology itself are needed to better capture physical laws and long-range causal dependencies. Explorations into physics-informed modeling and architectures designed to mitigate catastrophic error accumulation over time will be crucial [15, 36]. These dual approaches, enhancing domain-specific knowledge and improving core modeling capabilities, are essential to ensure that generated simulations conform to the complex realities of the operating room and can reliably predict outcomes over extended durations. By providing rigorous, clinically-grounded framework for evaluation, we hope our study will guide the development of the next generation of generative models, which can transition from simple visual mimicry to genuine causal understanding, unlocking their transformative potential for medicine."
        },
        {
            "title": "5 Conclusion",
            "content": "In this study, we sought to determine how far current state-of-the-art video generation models are from functioning as true world models for surgery. Our findings provide clear yet sobering answer: significant gap remains. While these models can master the appearance of surgery with remarkable photorealism, they fundamentally lack an understanding of its practice, failing to adhere to the basic principles of surgical action, consequence, and strategy. The critical contribution of this work is the establishment of the SurgVeo benchmark and the Surgical Plausibility Pyramid, which together provide standardized and clinically-grounded framework to measure and guide progress in this challenging domain. This is not dismissal of the technologys potential but foundational step and call to action. By systematically identifying the current limitations, our research provides clear roadmap for the field to move beyond superficial visual mimicry. The ultimate goal is to develop models with the deep causal understanding necessary to create truly intelligent simulations, thereby unlocking the immense potential of AI to enhance surgical training, planning, and patient safety."
        },
        {
            "title": "References",
            "content": "[1] D. Ha and J. Schmidhuber, World models, arXiv preprint arXiv:1803.10122, vol. 2, no. 3, 2018. [2] B. M. Lake, T. D. Ullman, J. B. Tenenbaum, and S. J. Gershman, Building machines that learn and think like people, Behavioral and brain sciences, vol. 40, p. e253, 2017. [3] Y. LeCun, path towards autonomous machine intelligence version 0.9. 2, 2022-06-27, Open Review, vol. 62, no. 1, pp. 162, 2022. [4] J. Bai, Y. Lei, H. Wu, Y. Zhu, S. Li, Y. Xin, X. Li, M. Tao, A. Grover, and M.-H. Yang, From masks to worlds: hitchhikers guide to world models, arXiv preprint arXiv:2510.20668, 2025. [5] X. He, Bridging the gap between multimodal foundation models and world models, arXiv preprint arXiv:2510.03727, 2025. [6] J. Zhang, M. Jiang, N. Dai, T. Lu, A. Uzunoglu, S. Zhang, Y. Wei, J. Wang, V. M. Patel, P. P. Liang et al., World-in-world: World models in closed-loop world, arXiv preprint arXiv:2510.18135, 2025. [7] D. Hafner, T. Lillicrap, I. Fischer, R. Villegas, D. Ha, H. Lee, and J. Davidson, Learning latent dynamics for planning from pixels, in International Conference on Machine Learning. PMLR, 2019, pp. 25552565. [8] D. Hafner, T. Lillicrap, J. Ba, and M. Norouzi, Dream to control: Learning behaviors by latent imagination, arXiv preprint arXiv:1912.01603, 2019. [9] T. Brooks, B. Peebles, C. Holmes, W. DePue, Y. Guo, L. Jing, D. Schnurr, J. Taylor, T. Luhman, E. Luhman et al., Video generation models as world simulators, OpenAI Blog, vol. 1, no. 8, p. 1, 2024. [10] T. Wiedemer, Y. Li, P. Vicol, S. S. Gu, N. Matarese, K. Swersky, B. Kim, P. Jaini, and R. Geirhos, Video models are zero-shot learners and reasoners, arXiv preprint arXiv:2509.20328, 2025. [11] H. He, Y. Zhang, L. Lin, Z. Xu, and L. Pan, Pre-trained video generative models as world simulators, arXiv preprint arXiv:2502.07825, 2025. [12] K. Sanderson, Gpt-4 is here: what scientists think, Nature, vol. 615, no. 7954, p. 773, 2023. [13] D. Guo, D. Yang, H. Zhang, J. Song, P. Wang, Q. Zhu, R. Xu, R. Zhang, S. Ma, X. Bi et al., Deepseek-r1 incentivizes reasoning in llms through reinforcement learning, Nature, vol. 645, no. 8081, pp. 633638, 2025. [14] S. Reed, K. Zolna, E. Parisotto, S. G. Colmenarejo, A. Novikov, G. Barth-Maron, M. Gimenez, Y. Sulsky, J. Kay, J. T. Springenberg et al., generalist agent, arXiv preprint arXiv:2205.06175, 2022. [15] S. Yuan, J. Huang, Y. Shi, Y. Xu, R. Zhu, B. Lin, X. Cheng, L. Yuan, and J. Luo, Magictime: Time-lapse video generation models as metamorphic simulators, IEEE Transactions on Pattern Analysis and Machine Intelligence, 2025. [16] M. Moor, O. Banerjee, Z. S. H. Abad, H. M. Krumholz, J. Leskovec, E. J. Topol, and P. Rajpurkar, Foundation models for generalist medical artificial intelligence, Nature, vol. 616, no. 7956, pp. 259265, 2023. [17] C. Varghese, E. M. Harrison, G. OGrady, and E. J. Topol, Artificial intelligence in surgery, Nature medicine, vol. 30, no. 5, pp. 12571268, 2024. [18] H. Chen, L. Gou, Z. Fang, Q. Dou, H. Chen, C. Chen, Y. Qiu, J. Zhang, C. Ning, Y. Hu et al., Artificial intelligence assisted real-time recognition of intra-abdominal metastasis during laparoscopic gastric cancer surgery, npj Digital Medicine, vol. 8, no. 1, p. 9, 2025. [19] Y. Long, A. Lin, D. H. C. Kwok, L. Zhang, Z. Yang, K. Shi, L. Song, J. Fu, H. Lin, W. Wei et al., Surgical embodied intelligence for generalized task autonomy in laparoscopic robot-assisted surgery, Science Robotics, vol. 10, no. 104, p. eadt3093, 2025. [20] Y. Zhai, Z. Chen, Z. Zheng, X. Wang, X. Yan, X. Liu, J. Yin, J. Wang, and J. Zhang, Artificial intelligence for automatic surgical phase recognition of laparoscopic gastrectomy in gastric cancer, International Journal of Computer Assisted Radiology and Surgery, vol. 19, no. 2, pp. 345353, 2024. [21] K. Yuan, N. Navab, N. Padoy et al., Procedure-aware surgical video-language pretraining with hierarchical knowledge augmentation, Advances in Neural Information Processing Systems, vol. 37, pp. 122 952122 983, 2024. [22] X. Liu, Z. Chen, and Y. Yuan, Most: multi-formation soft masking for semi-supervised medical image segmentation, in International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer, 2024, pp. 469480. [23] Z. Chen, Z. Peng, X. Liang, C. Wang, P. Liang, L. Zeng, M. Ju, and Y. Yuan, Map: Evaluation and multi-agent enhancement of large language models for inpatient pathways, arXiv preprint arXiv:2503.13205, 2025. [24] R. H. Taylor and D. Stoianovici, Medical robotics in computer-integrated surgery, IEEE Transactions on Robotics and Automation, vol. 19, no. 5, pp. 765781, 2003. [25] Z. Chen, Q. Guo, L. K. Yeung, D. T. Chan, Z. Lei, H. Liu, and J. Wang, Surgical video captioning with mutualmodal concept alignment, in International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer, 2023, pp. 2434. [26] X. Luo, Y. Pang, Z. Chen, J. Wu, Z. Zhang, Z. Lei, and H. Liu, Surgplan: Surgical phase localization network for phase recognition, in IEEE International Symposium on Biomedical Imaging. IEEE, 2024, pp. 15. [27] Z. Chen, X. Luo, J. Wu, D. T. Chan, Z. Lei, S. Ourselin, and H. Liu, Surgfc: Multimodal surgical function calling framework on the demand of surgeons, in IEEE International Conference on Bioinformatics and Biomedicine. IEEE, 2024, pp. 30763081. [28] G. Wang, L. Bai, J. Wang, K. Yuan, Z. Li, T. Jiang, X. He, J. Wu, Z. Chen, Z. Lei, H. Liu, J. Wang, F. Zhang, N. Padoy, N. Navab, and H. Ren, Endochat: Grounded multimodal large language model for endoscopic surgery, Medical Image Analysis, vol. 107, p. 103789, 2026. [29] J. Wu, X. Liang, X. Bai, and Z. Chen, Surgbox: Agent-driven operating room sandbox with surgery copilot, in IEEE International Conference on Big Data. IEEE, 2024, pp. 20412048. [30] Z. Wang, B. Lu, Y. Long, F. Zhong, T.-H. Cheung, Q. Dou, and Y. Liu, Autolaparo: new dataset of integrated multi-tasks for image-guided surgical automation in laparoscopic hysterectomy, in International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer, 2022, pp. 486496. [31] A. Das, D. Z. Khan, D. Psychogyios, Y. Zhang, J. G. Hanrahan, F. Vasconcelos, Y. Pang, Z. Chen, J. Wu, X. Zou, G. Zheng, A. Qayyum, M. Mazher, I. Razzak, T. Li, J. Ye, J. He, S. Płotka, J. Kaleta, A. Yamlahi, A. Jund, P. Godau, S. Kondo, S. Kasai, K. Hirasawa, D. Rivoir, S. Speidel, A. Pérez, S. Rodriguez, P. Arbeláez, D. Stoyanov, H. J. Marcus, and S. Bano, Pitvis-2023 challenge: Workflow recognition in videos of endoscopic pituitary surgery, Medical Image Analysis, vol. 106, p. 103716, 2025. [32] Z. Huang, Y. He, J. Yu, F. Zhang, C. Si, Y. Jiang, Y. Zhang, T. Wu, Q. Jin, N. Chanpaisit et al., Vbench: Comprehensive benchmark suite for video generative models, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 21 80721 818. [33] C. Hou and Z. Chen, Training-free camera control for video generation, in International Conference on Learning Representations, 2025. [34] C. Li, H. Liu, Y. Liu, B. Y. Feng, W. Li, X. Liu, Z. Chen, J. Shao, and Y. Yuan, Endora: Video generation models as endoscopy simulators, in International conference on medical image computing and computer-assisted intervention. Springer, 2024, pp. 230240. [35] T. Chen, S. Yang, J. Wang, L. Bai, H. Ren, and L. Zhou, Surgsora: Object-aware diffusion model for controllable surgical video generation, in International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer, 2025, pp. 521531. [36] Y. Yang, Z. Zhang, X. Zhang, Y. Zeng, H. Li, and W. Zuo, Physworld: From real videos to world models of deformable objects via physics-aware demonstration synthesis, arXiv preprint arXiv:2510.21447, 2025."
        },
        {
            "title": "A Tracks of SurgVeo Benchmark",
            "content": "The SurgVeo benchmark comprises two distinct tracks to encompass diverse surgical environments and procedural complexities: 1. Laparoscopic Surgery Track: This track features video clips from minimally invasive abdominal procedures, characterized by endoscopic camera views, pneumoperitoneum environments, and characteristic instrument configurations including graspers, scissors, and electrocautery devices. Laparoscopic procedures present unique challenges, including limited field of view, indirect tissue manipulation through long instruments, and specific lighting and depth perception constraints. 2. Neurosurgery Track: This track features video clips from microscopic cranial procedures, characterized by high magnification microscopic views, extremely delicate tissue handling of neural and vascular structures, and specialized neurosurgical instrumentation. Neurosurgical procedures demand exceptional precision, with operations involving critical anatomical structures where millimeter-scale errors can have significant clinical consequences."
        },
        {
            "title": "B Statistics of SurgVeo Benchmark",
            "content": "The SurgVeo benchmark dataset consists of total of 50 surgical video segments, divided into two distinct tracks: the laparoscopic surgery track and the neurosurgery track. This division ensures diverse representation of surgical procedures, with each track capturing the unique characteristics and key stages of its respective domain. The laparoscopic surgery track focuses on 7 major stages, such as preparation, ligament division, and suturing, while the neurosurgery track covers 12 stages, including nasal corridor creation, tumor excision, and graft placement. Specifically, the laparoscopic surgery track in Fig. 7(a) contains 18 video segments, while the neurosurgery track in Fig. 7(a) includes 32 video segments. Please note that not all surgeries include every stage, reflecting the variability in surgical workflows. In this way, the balanced distribution of videos and stages across both tracks provides comprehensive dataset for benchmarking surgical scene understanding, enabling robust evaluations of models across different procedures and stages."
        },
        {
            "title": "C Data Structure of SurgVeo Benchmark",
            "content": "Each sample in the SurgVeo benchmark consists of continuous surgical video sequence designed to test the models predictive capabilities: Input Frame: The first frame of the real video clip capturing the surgical scene immediately preceding procedural action. This minimal temporal context challenges the model to extrapolate future dynamics from limited information, testing whether it has internalized sufficient surgical knowledge to predict plausible continuations. Figure 7 The surgical stage distribution of the proposed SurgVeo benchmark in two tracks, including (a) the distribution of the laparoscopic surgery track, and (b) the distribution of the neurosurgery track. Note that not every surgery includes all possible surgical stages. Generated Video (8 seconds): The model is tasked with generating an 8-second video continuation that predicts the subsequent surgical developments. This extended prediction horizon requires maintaining temporal coherence, physical plausibility, and surgical logic over clinically meaningful duration. Reference Video (8 seconds): The actual surgical footage following the input frame serves as the reference for expert evaluation. Surgeons compare the AI-generated predictions against these real-world continuations across multiple assessment dimensions. The design philosophy emphasizes prediction under uncertainty: given minimal visual context, can the video generation model demonstrate understanding of instrument dynamics, tissue behavior, and surgical intent sufficient to generate plausible future scenarios? This formulation mirrors the cognitive demands placed on surgical trainees learning to anticipate procedural progression."
        },
        {
            "title": "D Task Formulation of Surgical Video Generation",
            "content": "The core task is surgical video prediction: given an input frame showing the surgical scene immediately before procedural action, the Veo-3 model generates an 8-second continuation predicting the subsequent surgical developments. This formulation tests whether the model has internalized sufficient understanding of surgical dynamics, instrument behavior, and tissue responses to extrapolate plausible future states from minimal visual context. To investigate the impact of procedural context on generation quality, we evaluate Veo-3 under two prompting conditions for each surgical track: 1. Baseline Prompt for Veo-3: Provide only the procedure type and general expectations for visual quality and anatomical realism, without specifying the current surgical stage. This condition tests whether the model can infer the appropriate next steps purely from visual context. 2. Stage-Aware Prompt for Veo-3: Explicitly specify the current surgical stage (e.g., suturing for laparoscopic procedures, debris clearance for neurosurgical procedures) in addition to the baseline information. This condition tests whether providing explicit procedural context improves the models ability to generate stage-appropriate actions. This dual-prompting design allows us to assess whether current video generation models can autonomously recognize surgical stages from visual information alone, or whether they require explicit textual guidance to produce contextually appropriate predictionsa distinction critical for understanding the depth of their learned surgical knowledge."
        },
        {
            "title": "E Prompting Strategy with Templates",
            "content": "E.1 Laparoscopic Surgery Track For the laparoscopic hysterectomy samples in the laparoscopic surgery track, the following prompts are used: Baseline Prompt: Please generate an 8-second video depicting the next step in laparoscopic hysterectomy procedure. The scene should present realistic surgical environment viewed through the laparoscopic camera, showing clear anatomical structures. The lighting should mimic the cool-light illumination typical of laparoscopic surgery. Ensure that the generated video includes smooth camera adjustments to maintain focus on the surgical site, as well as subtle movements of surrounding tissues caused by instrument interaction. The tissue texture, color, and responsiveness should be anatomically accurate. The overall flow of the procedure must be logical and consistent with real-world surgical practices. Stage-Aware Prompt: Please generate an 8-second video depicting the next step in laparoscopic hysterectomy procedure. The current stage of the surgery is [STAGE]. The scene should present realistic surgical environment viewed through the laparoscopic camera, showing clear anatomical structures. The lighting should mimic the cool-light illumination typical of laparoscopic surgery. Ensure that the generated video includes smooth camera adjustments to maintain focus on the surgical site, as well as subtle movements of surrounding tissues caused by instrument interaction. The tissue texture, color, and responsiveness should be anatomically accurate. The overall flow of the procedure must be logical and consistent with real-world surgical practices. E.2 Neurosurgery Track For the endoscopic pituitary surgery samples in the neurosurgery track, the following prompts are used: Baseline Prompt: Please generate an 8-second video depicting the next step in an endoscopic pituitary surgery procedure. The scene should present realistic surgical environment viewed through the endoscopic camera, showing clear anatomical structures. The lighting should mimic the cool-light illumination typical of endoscopic surgery. Ensure that the generated video includes smooth camera adjustments to maintain focus on the surgical site, as well as subtle movements of surrounding tissues caused by instrument interaction. The texture, color, and responsiveness of the tissue should be anatomically accurate. The overall flow of the procedure must be logical and consistent with real-world surgical practices. Stage-Aware Prompt: Please generate an 8-second video depicting the next step in an endoscopic pituitary surgery procedure. The current stage of the surgery is [STAGE]. The scene should present realistic surgical environment viewed through the endoscopic camera, showing clear anatomical structures. The lighting should mimic the cool-light illumination typical of endoscopic surgery. Ensure that the generated video includes smooth camera adjustments to maintain focus on the surgical site, as well as subtle movements of surrounding tissues caused by instrument interaction. The texture, color, and responsiveness of the tissue should be anatomically accurate. The overall flow of the procedure must be logical and consistent with real-world surgical practices."
        },
        {
            "title": "F Implementation Details of Surgical Video Generation",
            "content": "All video generations are performed using the Veo-3 model accessed through the Google Flow platform1 with the Veo-3.0-quality version. For each sample in the SurgVeo benchmark, the input frame is provided as visual prompt, accompanied by the appropriate text prompt (either baseline or stage-aware one, depending on the experimental condition). The Veo-3 model generates 8-second continuation videos at its default resolution and frame rate settings. No additional fine-tuning or domain-specific adaptation is applied to the model, ensuring that our evaluation reflects the zero-shot surgical prediction capabilities of the state-of-the-art video generation model."
        },
        {
            "title": "G Surgical Plausibility Pyramid for Expert Evaluation",
            "content": "G.1 Visual Perceptual Plausibility Evaluation Focus: Comprehensive assessment of the overall visual quality of the video, including image clarity, lighting, tissue texture and color realism, as well as video playback smoothness, motion fluidity, and the presence of artifacts, jitter, or teleportation issues. Scoring Criteria: 5-point: The video is clear and stable with smooth motion, visually indistinguishable from real highquality surgical recordings. 1https://labs.google/fx/tools/flow 4-point: The video is generally clear and fluid, but contains minor visual imperfections, such as occasional subtle jitter in localized areas, slightly blurred texture details, or unnatural lighting transitions. These issues are only noticeable upon careful observation and do not affect the overall viewing experience. 3-point: The video is generally clear, but some details (e.g., tissue reflections) are somewhat blurred, or instruments exhibit slight stuttering during movement. 2-point: The video exhibits obvious quality issues, such as multiple blurred areas, frequent noticeable stuttering or discontinuity in instrument movement, significant color distortion, or visible artifacts. However, the overall video structure remains recognizable, without severe errors such as objects disappearing into thin air. 1-point: Severe image distortion or blurriness, or instruments/tissues exhibiting teleportation, disappearance, or other illogical phenomena. G.2 Instrument Operation Plausibility Evaluation Focus: Assessment of the plausibility of the instrument operation, including the instruments appearance and physical handling. In particular, we evaluate the instruments appearance (e.g., is it real, non-hallucinated surgical instrument appropriate for the procedure?) and assess its action (i.e., whether the movement trajectories, manipulation techniques, and operational execution of surgical instruments are reasonable and technically sound). This includes appropriate instrument selection, movement paths, grasping angles, force application, and coordination between instruments. Scoring Criteria: 5-point: The instruments shown are correct, realistic, and visually indistinguishable from real surgical tool. Instrument movements are precise and technically proficient, mirroring expert surgical technique. 4-point: The instruments shown are accurate and reasonable. Instrument movements have minor technical imperfections (e.g., slightly suboptimal angles, minor inefficiencies, or less fluid motion) that remain effective. 3-point: The instruments shown are recognizable but have minor flaws or locally unrealistic details (e.g., slightly incorrect proportions or texture). Instrument movements are in generally correct directions but are executed clumsily or inefficiently, such as awkward grasping angles, hesitant movements, or suboptimal instrument positioning that reduces operational effectiveness. 2-point: The instruments shown are not very realistic (e.g., have distorted shape or clearly incorrect features) but are still loosely recognizable as surgical tool. Instrument movements exhibit obvious technical problems, such as inappropriate trajectories or ineffective manipulations. 1-point: The instruments shown have serious violations. This includes the generation of fabricated, non-existent, or physically impossible instruments, or real instruments moving in physically impossible ways or performing actions they are not designed for (e.g., attempting to cut with grasping forceps). G.3 Environment Feedback Plausibility Evaluation Focus: Assessment of whether the feedback of the surgical scene (tissues, organs, etc.) to surgical operations is realistic and conforms to physical laws and anatomical knowledge. This dimension specifically measures whether the direct consequences of instrument actions (e.g., tissue deformation, bleeding patterns, eschar formation) are physically and anatomically accurate when compared to the reference video. Scoring Criteria: 5-point: The feedback completely conforms to reality, such as morphological changes after traction, bleeding patterns after cutting, eschar formation after coagulation, etc., all appearing highly realistic with correct anatomical structures. 4-point: The feedback is generally realistic and credible, with physical and biological responses largely correct, but with minor inaccuracies in details, such as slight deviations in the magnitude of tissue deformation, somewhat unnatural bleeding volume or diffusion speed, or imprecise degree of tissue discoloration after coagulation. These deviations do not affect the overall judgment of surgical scene authenticity. 3-point: The feedback is partially realistic but with obvious deviations, such as incorrect bleeding volume or color, or tissue deformation appearing somewhat rigid. 2-point: The feedback exhibits significant problems, such as minimal or severely incorrect bleeding patterns from cut tissues, obviously unnatural movement patterns when tissues are pulled, or noticeable errors in anatomical structures (e.g., incorrect organ position relationships). These issues are sufficient to raise concerns among experienced surgeons but have not reached the level of completely violating medical common sense. 1-point: Serious violations of physical or medical common sense occur. For example: no bleeding after cutting major blood vessel; after removing an organ, the exposed anatomical structures below are completely incorrect; tissues are stretched by instruments to impossible lengths without tearing. G.4 Surgical Intent Plausibility Evaluation Focus: Assessment of whether the predicted surgical operations, as displayed from the input frame up to the current time point (e.g., 1-second, 3-second, or 8-second), demonstrate appropriate surgical intent and strategic reasoning for the current procedure. This evaluates whether the overall purpose, goal, and cumulative surgical effect of the predicted actions are consistent with logical surgical progression, appropriate procedural steps, and sound clinical decision-making when compared to the reference video. Scoring Criteria: 5-point: The surgical intent is completely appropriate for the current procedure, with operations demonstrating clear, logical purpose that aligns perfectly with standard surgical protocols and clinical objectives. The predicted actions show coherent strategic planning. 4-point: The surgical intent is generally appropriate and clinically reasonable, with operations aligned with the current procedure, but the strategic choices are slightly suboptimal or less efficient than ideal approaches. The underlying surgical reasoning remains sound despite minor strategic inefficiencies. 3-point: The surgical intent is discernible and partially appropriate, but shows questionable alignment with the current procedure. Operations suggest unclear priorities or strategic confusion, though some logical connection to surgical goals can still be identified. 2-point: The surgical intent is unclear or obviously mismatched with the current procedure. Operations appear to lack coherent purpose, address already-completed objectives, or demonstrate significant strategic errors. However, some basic surgical reasoning can still be inferred, even if fundamentally flawed. 1-point: Complete absence of appropriate surgical intent or serious violations of surgical logic. For example: performing tissue dissection in areas where dissection is already complete and hemostasis achieved; attempting coagulation when the priority should be addressing active bleeding; or conducting operations that contradict the evident clinical needs of the current procedure."
        },
        {
            "title": "H Qualitative Case Studies of Generated Videos",
            "content": "To provide deeper insight into the model performance beyond aggregated scores, this section presents case-by-case analysis of representative high-scoring  (Fig. 8)  and low-scoring  (Fig. 9)  examples from the SurgVeo benchmark, contextualized with synthesized expert feedback. H.1 Analysis of High-Scoring Examples The examples in Fig. 8 illustrate the upper bound of the Veo models current capabilities, showcasing success primarily in mimicking visually plausible, often simple or even non-operative, actions. Laparoscopic Dissection: Experts consider this strong generation, noting that the dissection appears \"relatively natural\" with \"few errors\" in Fig. 8(a). This suggests the Veo model can effectively replicate continuous, visually predictable actions like smooth tissue manipulation when the underlying surgical logic is straightforward. Neurosurgical Pause: This case highlights high-quality generation in Fig. 8(b). The generated video is visually \"almost identical\" to the reference, where the Veo model successfully mirrors this calm period. H.2 Analysis of Low-Scoring Examples The examples in Fig. 9 demonstrate the catastrophic failures in surgical logic identified quantitatively in Section 3.5, often occurring despite acceptable visual quality. Laparoscopic Suturing Attempt: Experts identify failures across multiple SPP dimensions, as illustrated in Fig. 9(a). The Veo model completely fails to recognize the intended complex action (suturing). It hallucinates \"non-existent\" surgical instrument and depicts an \"unrecognizable, non-standard operation\". Consequently, the simulated \"tissue feedback is also poor\". This case exemplifies cascade of errors stemming from failure to understand both the required action and the appropriate tools. Neurosurgical Glue Application: This example represents complete failure at the higher levels of the SPP in Fig. 9(b), despite adequate visual rendering. Experts state that while the \"visual quality itself is acceptable, everything else, instrument generation, surgical intent, operation, and feedback, is wrong\". The Veo model entirely misses the crucial surgical intent of applying biologic glue, instead generating unrelated and nonsensical actions. This starkly illustrates the Veo models inability to grasp the strategic purpose (apex of the SPP) and translate it into coherent sequence of plausible actions and consequences."
        },
        {
            "title": "I Public Release and Reproducibility",
            "content": "To foster further research and enable rigorous comparison of future models, the complete SurgVeo benchmark will be publicly released at https://github.com/franciszchen/SurgVeo. The release includes: All input frames and real surgical continuation videos Generated video outputs from Veo-3 for each benchmark sample Anonymized expert evaluation scores across all four assessment dimensions Detailed evaluation protocols and scoring guidelines This comprehensive release enables the research community to reproduce our findings, benchmark alternative generative models, and advance the understanding of world modeling capabilities in specialized professional domains. Figure 8 Qualitative examples of high-scoring video generations from the SurgVeo benchmark. Each pair shows the real surgical video (top row) and the generated video (bottom row) evolving from the same starting frame. (a) laparoscopic procedure where the Veo model generates naturally flowing and plausible dissection. (b) neurosurgical procedure where the generated video is nearly identical to the real reference video, presenting high-quality case. Figure 9 Qualitative examples of low-scoring video generations, demonstrating catastrophic failures in plausibility. Each pair shows the real surgical video (top row) and the generated video (bottom row). (a) laparoscopic procedure where the Veo model is expected to perform suturing. Instead, it hallucinates fabricated surgical instrument and performs an unrecognizable, non-standard operation. (b) neurosurgical procedure where the real intent is to apply biologic glue. The generated video completely misses this intent, failing on all three high-level SPP dimensions despite acceptable visual quality."
        }
    ],
    "affiliations": [
        "Department of Gastrointestinal Surgery, The Second Qilu Hospital, Shandong University",
        "Department of Neurosurgery, The First Hospital, Shanxi Medical University",
        "Institute of Automation, Chinese Academy of Sciences",
        "Technical University of Munich",
        "University of Nottingham",
        "University of Rochester",
        "Yale University"
    ]
}