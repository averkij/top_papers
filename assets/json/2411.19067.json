{
    "paper_title": "MaskRIS: Semantic Distortion-aware Data Augmentation for Referring Image Segmentation",
    "authors": [
        "Minhyun Lee",
        "Seungho Lee",
        "Song Park",
        "Dongyoon Han",
        "Byeongho Heo",
        "Hyunjung Shim"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Referring Image Segmentation (RIS) is an advanced vision-language task that involves identifying and segmenting objects within an image as described by free-form text descriptions. While previous studies focused on aligning visual and language features, exploring training techniques, such as data augmentation, remains underexplored. In this work, we explore effective data augmentation for RIS and propose a novel training framework called Masked Referring Image Segmentation (MaskRIS). We observe that the conventional image augmentations fall short of RIS, leading to performance degradation, while simple random masking significantly enhances the performance of RIS. MaskRIS uses both image and text masking, followed by Distortion-aware Contextual Learning (DCL) to fully exploit the benefits of the masking strategy. This approach can improve the model's robustness to occlusions, incomplete information, and various linguistic complexities, resulting in a significant performance improvement. Experiments demonstrate that MaskRIS can easily be applied to various RIS models, outperforming existing methods in both fully supervised and weakly supervised settings. Finally, MaskRIS achieves new state-of-the-art performance on RefCOCO, RefCOCO+, and RefCOCOg datasets. Code is available at https://github.com/naver-ai/maskris."
        },
        {
            "title": "Start",
            "content": "MaskRIS: Semantic Distortion-aware Data Augmentation for Referring Image Segmentation Minhyun Lee 1 Seungho Lee1 Song Park2 Dongyoon Han2 Byeongho Heo2 Hyunjung Shim3 1Yonsei University 2NAVER AI Lab 3KAIST AI 4 2 0 2 8 2 ] . [ 1 7 6 0 9 1 . 1 1 4 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Referring Image Segmentation (RIS) is an advanced visionlanguage task that involves identifying and segmenting objects within an image as described by free-form text descriptions. While previous studies focused on aligning visual and language features, exploring training techniques, such as data augmentation, remains underexplored. In this work, we explore effective data augmentation for RIS and propose novel training framework called Masked Referring Image Segmentation (MaskRIS). We observe that the conventional image augmentations fall short of RIS, leading to performance degradation, while simple random masking significantly enhances the performance of RIS. MaskRIS uses both image and text masking, followed by Distortionaware Contextual Learning (DCL) to fully exploit the benefits of the masking strategy. This approach can improve the models robustness to occlusions, incomplete information, and various linguistic complexities, resulting in significant performance improvement. Experiments demonstrate that MaskRIS can easily be applied to various RIS models, outperforming existing methods in both fully supervised and weakly supervised settings. Finally, MaskRIS achieves new state-of-the-art performance on RefCOCO, RefCOCO+, and RefCOCOg datasets. Code is available at https://github.com/naver-ai/maskris. 1. Introduction Referring Image Segmentation (RIS) [16] involves precisely delineating objects within an image based on text descriptions. Unlike semantic and instance segmentation, which are constrained to pre-defined classes, RIS offers unique flexibility by segmenting objects specified by freeform expressions. This capability has led to extensive applications in diverse domains, such as language-driven humanrobot interaction [43] and advanced image editing [3, 39]. One of the main challenges in RIS is effectively bridging the Equal contribution. Work done at NAVER AI Lab. Correspondence to Byeongho Heo and Hyunjung Shim. Figure 1. Conventional data augmentations (DA) in semantic segmentation are incompatible with referring image segmentation. Random crop and horizontal flip could change the referred object (e.g., lady under the red umbrella on left) to another one, and color distortion could make the described object disappear. modality gap between visual and language features. Recent research [33, 49, 56] has focused on developing sophisticated architectures for cross-modal alignment. While significant progress has been made in aligning different modalities, effective training techniques for RIS remain underexplored. The trend toward developing larger and more complex models requires challenging training settings like data augmentation. Previous studies [4, 5, 47] have demonstrated the crucial role of data augmentation in model training. However, designing data augmentation for RIS is challenging: standard augmentation used in semantic segmentation is often incompatible with RIS, as some textual descriptions directly conflict with augmentation. As illustrated in Fig. 1, spatial augmentation, such as random crop and horizontal flip, alters the meaning of left, causing the target mask to become incorrect after augmentation. Similarly, color distortion also conflicts with color-specific descriptions like red. Due to the incompatibilities, naıvely applying augmentations is not beneficial - 1 our empirical analysis reveals that conventional data augmentation degrades RIS [33] performance (Fig. 2(a)). This finding explains why previous RISs [22, 33, 49, 52, 56, 61] have taken cautious approach to augmentation, predominantly relying on simple resizing while avoiding complex techniques that could severely distort the data. In this paper, we explore an effective data augmentation framework to improve RIS model training. We propose straightforward yet robust baseline, coined Masked Referring Image Segmentation (MaskRIS), which consists of two key components: (1) input masking as data augmentation technique for RIS and (2) Distortion-aware Contextual Learning (DCL) designed to maximize the benefits of input masking. In RIS tasks, it is crucial to preserve essential spatial information (e.g., relative positions, ordering) and key attribute details (e.g., color) for accurately understanding and segmenting objects based on referring expressions. Unlike conventional data augmentation, which often distorts these critical aspects, input masking minimizes such distortions by preserving spatial and attribute information while significantly expanding data diversity. Our approach extends input masking to both images and text to strengthen the models capability to handle visual and linguistic complexity. Masking parts of the text encourages the model to infer missing or ambiguous details, improving its understanding of diverse referring expressions and reducing reliance on specific terms. This masking strategy directly addresses the need for robust RIS training framework, helping to overcome the performance bottleneck caused by limited augmentation. To fully exploit the advantages of input masking, we propose Distortion-aware Contextual Learning (DCL) framework that processes inputs through two complementary paths: primary path with original inputs and secondary path with masked inputs. The primary path ensures baseline training stability and maintains model accuracy, while the secondary path enhances data diversity and robustness. The distillation loss aligns predictions from both paths, encouraging the model to make consistent predictions on both original and masked inputs. This DCL framework enhances feature robustness to masked inputs through the secondary path, while the primary path provides stability and mitigates potential harmful effects of severe distortion. Additionally, DCL acts as regularizer, adding beneficial challenges to the RIS task. The regularization effect aligns with findings from [38], where self-distillation is shown to constrain model representations, reducing overfitting and enhancing robustness - process similarly achieved by aligning original and masked predictions in our framework. Our contributions are as follows: (1) We provide comprehensive analysis of data augmentation techniques for robust RIS training, identifying limitations in previous methods for the first time. (2) Building on these insights, we introduce Masked Referring Image Segmentation (MaskRIS), simple yet effective baseline that overcomes semantic conflicts of conventional augmentations while increasing data diversity. This framework maximizes the benefits of original and masked inputs for stable training and enhanced accuracy. (3) Our MaskRIS achieves significant performance gains over existing RIS methods, including weakly supervised approaches, setting new state-of-the-art results on RefCOCO, RefCOCO+, and RefCOCOg datasets. 2. Related Work Referring Image Segmentation. Referring Image Segmentation (RIS) aims to segment objects in images based on text descriptions. This field evolved from early CNNRNN/LSTM combinations for image and text processing [16, 26, 37, 45] to advanced approaches using crossmodal attention [9, 17, 18, 29, 32, 57] and pre-trained transformer encoders [22, 33, 49, 56]. While previous studies have focused on image-text alignment, our study shifts focus to the training strategy in RIS, offering complementary approach to enhance existing studies. This orthogonal perspective allows us to enjoy the performance benefits established by previous methods. Our concurrent work, NeMo [10], introduces mosaic-based augmentation that combines target image with carefully selected negative samples. While effective, it requires additional overhead for constructing and retrieving from negative sample pool. In contrast, our approach provides more efficient framework that enhances data diversity with broader analysis of augmentation types and model robustness. Masked Input Training. Masked input training is selfsupervised learning technique where random parts of input data are masked, and the model learns to predict these masked elements from visible data. Originally successful in natural language processing (NLP) as masked language modeling (MLM) [7, 34], it expanded to computer vision as masked image modeling (MIM) showing both training efficiency and strong generalization performance [2, 11, 53]. These masking strategies are also employed to develop robust feature representations. RandomErasing [62] and Cutout [8] erase randomly selected rectangle areas to reduce over-fitting and handle occlusion. MIC [15] leverages context clues from the target domain through masked image consistency for domain adaptation. SMKD [27] uses masked knowledge distillation for few-shot classification. Self-distillation. Self-distillation leverages the model itself as both teacher and student, preventing the need for separate teacher networks to facilitate knowledge distillation [14]. PS-KD [21] demonstrates the utility of softening hard targets through teacher model predictions, acting as an effective regularization strategy. Some studies [40, 60] explored self-distillation by employing an en2 tures to generate the segmentation mask. Training the RIS models is essentially pixel-wise binary classification task using the cross-entropy loss function: Lce(y, ˆy) ="
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) i=1 yi log(ˆyi), (2) where denotes the total number of pixels, yi is the binary label at each pixel in the ground truth mask (1 indicating the object, 0 the background), and ˆyi is the predicted probability of the pixel belonging to the object. Limited Data Augmentation in RIS. Previous studies [33, 49, 56] have only focused on aligning vision and language features to bridge the modality gap. Still, effective training techniques for RIS remain underexplored, limiting the potential of vision-language features. As visionlanguage models grow in size and complexity [1, 19, 41], optimizing these training techniques becomes more crucial. Among training techniques, we regard data augmentation as the most promising approach, with the potential to enhance model robustness and generalization [4, 5, 24, 42, 59]. Current RIS methods [33, 49, 54, 56] employ only simple resizing operations for data augmentations and dont utilize complex augmentations used in semantic segmentation. This is due to the fact that the complex data augmentations in segmentation are incompatible with RIS as shown in Fig. 1, often altering or eliminating the referred objects. Consequently, these augmentations lead to decline in RIS model performance on RefCOCO as demonstrated in Fig. 2. Thus, we believe tackling the limited data augmentation problem is essential for overcoming major performance bottleneck in RIS. 3.2. Masking Strategy for Image and Text To overcome the limitations, we introduce masking-based augmentation that combines both image and text masking to generate diverse image-text training pairs. Unlike conventional data augmentation, which often distorts essential spatial and attribute details, image masking minimizes such distortions, preserving this critical information while significantly expanding data diversity. In addition, image masking is beneficial in recognizing partially visible or occluded objects, as illustrated in Fig. 3(a) and (b). State-of-the-art models, such as CARIS [33], often struggle to identify partially visible objects. This issue highlights prevalent challenge: the presence of occlusions often leads to the failure of models trained on clear and unoccluded objects. Similarly, as seen from Fig. 3(b), text descriptions often rely on contextual objects (e.g., spoon handle to describe the position of broccoli), suggesting that the impact of occlusion extends beyond the target objects themselves. Beyond managing image occlusions, RIS requires flexibility in interpreting varied text descriptions, as single (a) Performance with conventional augmentations (b) Performance with individual augmentation on CARIS [33] Figure 2. Existing RIS methods show noticeable decline in their performance when applying conventional image augmentations (random cropping, color jittering, and horizontal flipping). In contrast, image masking (I-Mask) and text masking (T-Mask) improve model performance. tire network as the teacher and an early exit network as the student. CoSub [48] notably improved model performance in visual recognition tasks with sub-model-based self-distillation. MaskSub [13] introduced drop-based technique for sub-model self-distillation, improving model performance and cost efficiency for image classification tasks. To fully exploit the benefits of input masking, we employ self-distillation between the predictions of the original and masked input. 3. Method 3.1. Referring Image Segmentation (RIS) RIS Pipeline. Given an image RHW 3 and text description that specifies an object within the image, the RIS model outputs pixel-level mask delineating the referred object. The text description is first tokenized into sequence of words = {w1, w2, . . . , wL}, where wi RD represents word embedding with as the embedding dimension and as the number of words in the description. Here, and denote the height and width of the image, and 3 represents the RGB color channels. We formalize this process as follows: ˆy = fθ(x, w), where ˆy RHW is the predicted mask, generated through the interaction between the input image and the tokenized words by the neural network parameterized by θ. (1) In conventional RIS approaches, consists of the image encoder [35, 41] and the text encoder [7, 41] to extract high-dimensional visual and language features, along with vision-language fusion module that integrates these fea3 Figure 3. The existing RIS method tends to be inaccurate when faced with occluded context. CARIS [33] represents the SoTA method in RIS. Words highlighted in red represent occluded objects in the image (left and center) and masked words in the text query (right). referred object can be described in multiple ways. This diversity in textual description is essential for generalizing to different referring expressions. While our image masking addresses visual occlusions, we introduce text masking to enhance the models capability to interpret diverse and incomplete descriptions by training it to rely on broader contextual cues. By masking parts of the text, we encourage the model to infer missing details, improving its understanding and reducing dependence on specific terms. For instance, as shown in Fig. 3(c), CARIS can accurately identify middle guy light blue tie with complete text. However, it struggles with partial descriptions, such as [MASK] guy light blue tie, indicating the limitations in handling incomplete textual clues. By incorporating text masking, we enable the model to adapt to various referring expressions, enhancing its comprehension and robustness in RIS tasks. We next describe our image and text masking techniques in detail, demonstrating how they address the aforementioned challenges in RIS. Image Masking. We divide the input image into nonoverlapping patches and randomly mask fixed ratio of patches. Following the MAE sampling strategy [11], we create binary mask RHW by sampling patches uniformly without replacement, where 1 indicates masked patch. The masked image xM is obtained by element-wise multiplication: xM = (1 ) x, where denotes the element-wise multiplication operation. This mechanism ensures that only subset of the images patches is exposed to the model during training, compelling the neural network to infer missing information and thereby enhancing its robustness and predictive accuracy. While there might be concerns that our approach could hinder training by completely masking target objects, such cases are extremely rare in practice. For example, even with relatively high masking ratio of 75%, the probability of target object being completely masked is only 0.19% - remarkably low. This probability becomes even lower with reduced masking ratios. Furthermore, this potential issue is mitigated during training as the same samples are seen multiple times. These statistics support that our masking approach effectively augments the training data while maintaining crucial visual information. 2 , . . . , wM Text Masking. We mask word tokens to generate masked word tokens wM = {wM 1 , wM } using probabilistic strategy in MLM [7]. Specifically, each word wi has 15% chance of being masked. Of these, 80% are replaced with [MASK] token, making it invisible to the model during training; 10% are replaced with random word from the vocabulary, introducing noise and variability; and 10% remain unchanged, preserving the original context. This strategy allows the model to experience variety of textual scenarios, enhancing its ability to comprehend context and meaning even when parts of the text are masked or altered. Notably, as Wei and Zou [50] witnessed, random word masking effectively preserves sentence meaning, further validating our methods balance between sentence coherence and data diversity. 3.3. Distortion-aware Contextual Learning In the training phase, we employ both image and text masking simultaneously, enriching the diversity of the training dataset. Specifically, we formalize the processing of masked inputs through the model as follows: ˆyM = fθ(xM , wM ), (3) where ˆyM denotes the models output for the masked inputs. By adopting this strategy, MaskRIS learns to effectively segment objects under varying conditions of occlusion and incomplete descriptions. This simple integration effectively mitigates the intrinsic challenge of RIS that stems from the absence of an effective data augmentation strategy. To further facilitate the RIS training, we introduce Distortion-aware Contextual Learning (DCL), as shown in 4 Figure 4. The overall framework of MaskRIS. Both image and text masking are employed to generate diverse image-text training pairs (Sec. 3.2). To maximize the benefits of the masking strategy, Distortion-aware Contextual Learning (DCL) is introduced (Sec. 3.3). Fig. 4. This approach uses primary path for processing original inputs and secondary path for masked inputs. The primary path focuses on preserving the original image context, ensuring training stability, and promoting distortion-aware training. Meanwhile, the secondary path introduces variability by processing masked data, which improves the models robustness. The training objective of MaskRIS consists of two parts, the distillation loss Ldist and the original pixel-wise classification loss Lce in Eq. (2), i.e., Ltotal = Ldist + Lce. The distillation loss is defined through the binary crossentropy loss between the predictions from both original and masked inputs as follows: Ldist = Lce(sg(ˆy), ˆyM ) ="
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) i=1 ˆyi log(ˆyM ), (4) where sg is the stop-gradient function, which blocks gradient back-propagation. Inspired by [13], MaskRIS employs self-distillation framework that leverages soft targets from the original inputs, reducing training complexity and enhancing model generalization. This approach allows MaskRIS to effectively mitigate the learning instability caused by input masking while maintaining the benefits of increased data diversity and robustness. 4. Experiments 4.1. Experimental Setups Datasets. We evaluate our method using three popular benchmarks in RIS: RefCOCO, RefCOCO+, and RefCOCOg. RefCOCO [58] provides 142,209 expressions (3.61 words per expression) for about 50,000 objects across 19,994 images, emphasizing object locations and appearances. RefCOCO+ [58] offers over 141,564 expressions (3.53 words per expression), excluding spatial language to focus on object attributes. RefCOCOg [58] introduces more complexity into the evaluation by including 85,474 longer and more complex expressions (8.43 words per expression) for 54,822 objects in 26,711 images. All datasets are collected on top of the MS COCO dataset [28]. For RefCOCOg, we report results on the UMD partition [58], following the previous studies [23, 31, 49]. Evaluation Metrics. To evaluate model performance, we use mean Intersection-over-Union (mIoU), overall Intersection-over-Union (oIoU), and P@X. While mIoU calculates the average IoU across all test samples, oIoU measures the ratio of the total intersection to the total union areas across all samples. P@X measures the percentage of test samples with an IoU above threshold {0.5, 0.7, 0.9}. These metrics ensure consistent and fair evaluation across different RIS methods. Implementation Details. To validate MaskRIS, we applied it to various RIS models [33, 49, 54, 56]. Designed as plug-and-play training strategy, our approach maintains the essential hyperparameters, such as learning rate, epochs, and batch size, as they were in the original settings. Notably, we primarily implemented our method on CARIS [33], leading SoTA method, unless stated otherwise. Images are resized to 448 448 for both training and testing. For image masking, we set 32 as the patch size. 4.2. Main Results Comparison with State of the Arts. We compare MaskRIS with previous methods on three popular benchmarks. As shown in Tab. 1, MaskRIS consistently outperforms previous methods by significant margins. Specifically, compared to the second-best performing method, CARIS [33], our approach improves oIoU scores by 1.82%p, 1.33%p, and 2.25%p on the RefCOCO validation, 5 Method mIoU CRIS [49] ETRIS [54] RefTR [25] LAVT [56] CGFormer [46] MaskRIS oIoU LAVT [56] CGFormer [46] LQMFormer [44] NeMo [10] ReMamber [55] CARIS [33] MaskRIS oIoU PolyFormer [31] CARIS [33] MaskRIS Image Encoder Text Encoder RefCOCO testA Standard: Training on the training split of each dataset. RefCOCO+ testA testB val val RN101 RN101 RN101 Swin-B Swin-B Swin-B CLIP CLIP BERT BERT BERT BERT 70.47 71.06 74.34 74.46 76.93 78.35 73.18 74.11 76.77 76.89 78.70 80.24 66.10 66.66 70.87 70.94 73.32 76. 62.27 62.23 66.75 65.81 68.56 71.68 68.08 68.51 70.58 70.97 73.76 76.73 Swin-B Swin-B Swin-B Swin-B Mamba-B Swin-B Swin-B BERT BERT BERT BERT CLIP BERT BERT 75.82 77.30 76.82 76.32 76.74 77.63 78.96 Combined: Training on the combination of three datasets. 72.73 74.75 74.16 74.48 74.54 74.67 76. 68.79 70.64 71.04 71.51 70.89 71.71 73.96 62.14 64.54 65.91 62.86 65.00 66.17 67.54 68.38 71.00 71.84 69.92 70.78 71.70 74.46 testB RefCOCOg test val 53.68 52.79 59.40 59.23 61.72 64. 55.10 57.14 57.59 55.56 57.53 57.46 59.39 59.87 60.28 66.63 63.34 67.57 69.31 61.24 64.68 64.73 64.40 63.90 64.66 65.55 60.36 60.42 67.39 63.62 67.83 69.42 62.09 65.09 66.04 64.80 64.00 65.45 66.50 Swin-B Swin-B Swin-B"
        },
        {
            "title": "BERT\nBERT\nBERT",
            "content": "74.82 76.63 78.71 76.64 79.40 80.64 71.06 73.52 75.10 67.64 68.03 70.26 72.89 73.70 75.15 59.33 60.41 62. 67.76 67.95 69.12 69.05 69.75 71.09 Table 1. Comparison with state-of-the-art methods on three benchmark datasets. denotes the reproduced results across all experiments. testA, and testB sets, respectively. On RefCOCO+, our method leads by 1.37%p, 2.76%p, and 1.93%p on the validation, testA, and testB splits, respectively. Even on the challenging RefCOCOg dataset, MaskRIS still outperforms CARIS by 0.89%p and 1.05%p on the validation and test splits. These results validate the effectiveness of our masking strategy for RIS model training. To further demonstrate the capability of our method, we conduct experiments on larger, more comprehensive dataset. This dataset is combination of the training sets from RefCOCO, RefCOCO+, and RefCOCOg, following the approach used in previous studies [31, 33]. Training on this combined dataset yields even better results, with oIoU improvements of 1.174.04%p over previous methods. This result indicates that our method is effective across different types of data and highlights its generalizability. Fig. 5 shows qualitative examples on RefCOCO, where our method more comprehensively captures the extent of objects by expanding or reducing their coverage (1st row and 2nd row). Additionally, it demonstrates robustness against occlusion (3rd row) and precisely identifies target objects in alignment with the provided text descriptions (4th row). These results confirm the ability of our approach to enhance model robustness and leverage textual cues effectively. Robustness Evaluation. key strength of our approach lies in its robustness across visually and linguistically complex scenarios. To this end, we first evaluate our models robustness to various image corruptions. We use the benchmark provided by ImageNet-C [12], which includes types of corruption, each with 5 severity levels. To evaluate the performance, we compute the oIoU at each severity level and average these scores for each corruption type. As shown in Fig. 6(a), MaskRIS consistently outperforms the previous SoTA method, CARIS [33], with oIoU improvements of 1.345.07%p. This demonstrates that MaskRIS effectively reduces overfitting to clean data, providing greater robustness against wide range of visual distortions. In addition, we evaluate MaskRISs robustness to linguistically complex situations, such as occlusion, relative position, and ordering. For occlusion scenarios, we modify the input data by occluding parts of objects. For relative position and ordering, we select samples containing relevant linguistic keywords from the validation set. As shown in Fig. 6(b), MaskRIS achieves superior performance in these complex scenarios on the RefCOCO validation set, further highlighting its robustness to both visual and linguistic complexity. Compatibility with Other Methods. Our training strategy is highly compatible, integrates seamlessly with various RIS methods, and achieves significant performance improvements, as shown in Tab. 2. Performance gains range from 0.75%p to 2.25%p, even in challenging scenarios like weakly supervised approaches (e.g., TRIS [30] and SaG [20]), which rely solely on text descriptions without ground truth masks. For TRIS on RefCOCO, our approach increases mIoU by 1.43%p on the validation set, 1.83%p on testA, and 0.76%p on testB. Similarly, our method increases SaGs performance from 37.21 to 38.85 mIoU on 6 Figure 5. Qualitative examples of segmentation results on the RefCOCO dataset. Figure 6. Robustness on various image corruptions provided by ImageNet-C [12] and linguistically complex situations. The results (oIoU) are evaluated on the RefCOCO validation set. the validation set, from 36.60 to 37.70 on testA, and from 39.41 to 40.20 on testB, achieving improvements of up to 1.64%p. Overall, MaskRIS consistently improves RIS model performance across fullyand weakly-supervised frameworks, demonstrating its adaptability and effectiveness without modifying the original model architecture. 4.3. Ablation Study Impact of Each Component of MaskRIS. We analyze the impact of each MaskRIS component in model training. Tab. 3 shows segmentation results on the RefCOCO validation set. The results clearly show that masking images and text separately improves model performance. However, the most significant improvement occurs when we apply masking to both images and text together. This demonstrates that the combination of visual and textual masking strategies significantly improves the models ability to accurately identify and segment the referred objects. In addition, Distortion-aware Contextual Learning (DCL) provides further performance gains. It effectively mitigates the learning instability caused by input masking, while maintaining the benefits of increased data diversity and robustness. Fig. 7 illustrates the benefits of image and text masking. CARIS often fails to correctly identify objects when they or their adjacent objects are partially obscured. However, by incorporating image masking, our approach achieves accurate object recognition (1st and 2nd rows). For example, in Figure 7. Qualitative examples of masking strategies on the RefCOCO dataset. I-Mask, T-Mask, and Both denote the results of applying image masking, text masking, and both, respectively. the scenario shown in the first row with baby girl whose legs are obscured, CARIS inaccurately identifies the legs of an adult as part of the target object. On the other hand, our method successfully separates the obscured elements, demonstrating the superior accuracy of our method in complex environments. Conversely, text masking improves the alignment between the text description and the target object (3rd row). For example, with the left marine, CARIS inaccurately identifies the target object by focusing too much on spatial information (i.e., left), as shown in Fig. 3(c). Text masking intervenes in such cases, ensuring the accurate recognition of the target object. Furthermore, by inte7 Method Image Encoder Text Encoder val RefCOCO testA testB Weakly-supervised approach mIoU TRIS [30] TRIS [30]+MaskRIS SaG [20] SaG [20]+MaskRIS mIoU CRIS [49] CRIS [49]+MaskRIS ETRIS [54] ETRIS [54]+MaskRIS oIoU LAVT [56] LAVT [56]+MaskRIS CARIS [33] CARIS [33]+MaskRIS RN50 RN50 ViT-S/16 ViT-S/16 CLIP CLIP BERT BERT 31.17 32.60 (+1.43) 37.21 38.85 (+1.64) 32.43 34.26 (+1.83) 36.60 37.70 (+1.1) 29.56 30.32 (+0.76) 39.41 40.20 (+0.79) Fully-supervised approach RN50 RN50 R101 R101 Swin-B Swin-B Swin-B Swin-B CLIP CLIP CLIP CLIP BERT BERT BERT BERT 69.52 70.73 (+1.21) 71.06 72.39 (+1.33) 72.72 74.06 (+1.34) 74.11 74.99 (+0.88) 64.70 66.82 (+2.12) 66.66 68.55 (+1.89) 72.73 73.79 (+1.06) 74.67 76.49 (+1.82) 75.82 76.57 (+0.75) 77.63 78.96 (+1.33) 68.79 70.31 (+1.52) 71.71 73.96 (+2.25) Table 2. Compatibility of MaskRIS with various RIS methods. MaskRIS consistently enhances existing methods in both fully supervised and weakly supervised settings. IM TM DCL P@0.5 87.73 87.76 87.72 88.00 88.60 88.34 88. P@0.7 80.20 80.52 80.45 81.35 81.86 81.19 81.95 P@0.9 39.60 38.73 39.26 40.11 41.08 39.24 41.19 oIoU 74.67 75.31 75.32 75.71 76.02 75.76 76.49 Table 3. Impact of each component of MaskRIS on the RefCOCO validation set, where IM (TM) refers to image (text) masking. IM ratio - - - - - 0.25 0.50 0.75 TM ratio 0.15 0.15 0.15 0.50 0.75 0.15 0.15 0. pm 0.8 0.8 0.5 0.5 0.5 0.8 0.8 0.8 pr 0.1 0.2 0.5 0.5 0.5 0.1 0.1 0.1 pu 0.1 0 0 0 0 0.1 0.1 0.1 oIoU 75.76 75.43 75.70 74.88 74.14 76.07 76.43 76.49 Table 4. Impact of the masking ratio on the RefCOCO validation set. When word wi is selected based on the masking ratio, it has pm probability of being masked with [MASK] token, pr probability of being replaced with random word from the vocabulary, and pu probability of remaining unchanged. grating both image and text masking, our approach exploits the strengths of each masking strategy. Impact of Masking Ratio. We investigate how masking ratios affect model performance, focusing on the balance between masking images and text. Tab. 4 provides insights into the optimal use of masking in model training. For text masking, we follow the masking ratio used by BERT [7]. Note that we refer to the BERT setting as it is, setting pu to 10% is equivalent to combining pm to 89% with pr to 11%. We observe significant drop in performance as the text masking ratio increases, indicating that removing too much textual information interferes with model training. On the other hand, image masking shows an optimal improvement at masking ratio of 75%, suggesting that higher degree of image masking improves model performance. These observations highlight the balance of masking ratios across different modalities. Excessive text masking can negatively affect learning by causing the loss of crucial information, whereas increasing image masking up to certain threshold can be beneficial. 5. Conclusion In this study, we introduce Masked Referring Image Segmentation (MaskRIS), an effective training framework for Referring Image Segmentation (RIS) that combines image and text masking with Distortion-aware Contextual Learning (DCL). MaskRIS addresses the challenges of conventional data augmentation in RIS by minimizing semantic distortion and enhancing data diversity. This approach strengthens the models robustness to occlusions and incomplete information, achieving new state-of-the-art accuracies on the RefCOCO, RefCOCO+, and RefCOCOg datasets. Our results demonstrate MaskRISs effectiveness and adaptability in both fully and weakly-supervised settings, highlighting its potential as versatile and powerful baseline for advancing the field of RIS."
        },
        {
            "title": "References",
            "content": "[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: visual language model for few-shot learning. Advances in Neural Information Processing Systems, 35:2371623736, 2022. 3 [2] Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. Beit: arXiv preprint Bert pre-training of image transformers. arXiv:2106.08254, 2021. 2, 1 [3] Jianbo Chen, Yelong Shen, Jianfeng Gao, Jingjing Liu, and Xiaodong Liu. Language-based image editing with recurrent In Proceedings of the IEEE Conference attentive models. on Computer Vision and Pattern Recognition, pages 8721 8729, 2018. 1 [4] Ekin Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc Le. Autoaugment: Learning augmentation policies from data. arXiv preprint arXiv:1805.09501, 2018. 1, 3 [5] Ekin Cubuk, Barret Zoph, Jonathon Shlens, and Quoc Randaugment: Practical automated data augmenLe. In Proceedings of tation with reduced search space. the IEEE/CVF conference on computer vision and pattern recognition workshops, pages 702703, 2020. 1, [6] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 248255. IEEE, 2009. 3 [7] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Pre-training of deep bidirectional arXiv preprint Toutanova. transformers for language understanding. arXiv:1810.04805, 2018. 2, 3, 4, 8 Bert: [8] Terrance DeVries and Graham Taylor. Improved regularization of convolutional neural networks with cutout. arXiv preprint arXiv:1708.04552, 2017. 2, 1 [9] Henghui Ding, Chang Liu, Suchen Wang, and Xudong Jiang. Vision-language transformer and query generation for referring segmentation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1632116330, 2021. [10] Seongsu Ha, Chaeyun Kim, Donghwa Kim, Junho Lee, Sangho Lee, and Joonseok Lee. In Proceedings of the 18th European Conference on Computer Vision (ECCV), 2024. 2, 6 [11] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollar, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 16000 16009, 2022. 2, 4, 1 [12] Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. arXiv preprint arXiv:1903.12261, 2019. 6, 7 [13] Byeongho Heo, Taekyung Kim, Sangdoo Yun, and Dongyoon Han. Augmenting sub-model to improve main model. arXiv preprint arXiv:2306.11339, 2023. 3, 5, 1 [14] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. DistillarXiv preprint ing the knowledge in neural network. arXiv:1503.02531, 2015. [15] Lukas Hoyer, Dengxin Dai, Haoran Wang, and Luc Van Gool. Mic: Masked image consistency for contextthe enhanced domain adaptation. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1172111732, 2023. 2, 1 In Proceedings of [16] Ronghang Hu, Marcus Rohrbach, and Trevor Darrell. Segmentation from natural language expressions. In Computer VisionECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 1114, 2016, Proceedings, Part 14, pages 108124. Springer, 2016. 1, 2 [17] Zhiwei Hu, Guang Feng, Jiayu Sun, Lihe Zhang, and Bi-directional relationship inferring netHuchuan Lu. In Proceedings of work for referring image segmentation. the IEEE/CVF conference on computer vision and pattern recognition, pages 44244433, 2020. 2 [18] Tianrui Hui, Si Liu, Shaofei Huang, Guanbin Li, Sansi Yu, Faxi Zhang, and Jizhong Han. Linguistic structure guided context modeling for referring image segmentation. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part 16, pages 5975. Springer, 2020. 2 [19] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representaIn International tion learning with noisy text supervision. conference on machine learning, pages 49044916. PMLR, 2021. [20] Dongwon Kim, Namyup Kim, Cuiling Lan, and Suha Kwak. Shatter and gather: Learning referring image segmentation with text supervision. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 15547 15557, 2023. 6, 8 [21] Kyungyul Kim, ByeongMoon Ji, Doyoung Yoon, and Sangheum Hwang. Self-knowledge distillation with progressive refinement of targets. In Proceedings of the IEEE/CVF international conference on computer vision, pages 6567 6576, 2021. 2 [22] Namyup Kim, Dongwon Kim, Cuiling Lan, Wenjun Zeng, and Suha Kwak. Restr: Convolution-free referring imIn Proceedings of age segmentation using transformers. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1814518154, 2022. 2 [23] Seoyeon Kim, Minguk Kang, and Jaesik Park. Risclip: Referring image segmentation framework using clip. arXiv preprint arXiv:2306.08498, 2023. 5 [24] Lin Li and Michael Spratling."
        },
        {
            "title": "Data augmentation\narXiv preprint",
            "content": "alone can improve adversarial training. arXiv:2301.09879, 2023. 3 [25] Muchen Li and Leonid Sigal. Referring transformer: one-step approach to multi-task visual grounding. Advances in neural information processing systems, 34:1965219664, 2021. 6 [26] Ruiyu Li, Kaican Li, Yi-Chun Kuo, Michelle Shu, Xiaojuan Qi, Xiaoyong Shen, and Jiaya Jia. Referring image seg9 In Proceedmentation via recurrent refinement networks. ings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 57455753, 2018. 2 [27] Han Lin, Guangxing Han, Jiawei Ma, Shiyuan Huang, Xudong Lin, and Shih-Fu Chang. Supervised masked knowledge distillation for few-shot transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1964919659, 2023. [28] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer VisionECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part 13, pages 740755. Springer, 2014. 5 [29] Chenxi Liu, Zhe Lin, Xiaohui Shen, Jimei Yang, Xin Lu, and Alan Yuille. Recurrent multimodal interaction for referring image segmentation. In Proceedings of the IEEE international conference on computer vision, pages 12711280, 2017. 2 [30] Fang Liu, Yuhao Liu, Yuqiu Kong, Ke Xu, Lihe Zhang, Baocai Yin, Gerhard Hancke, and Rynson Lau. Referring image segmentation using text supervision. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2212422134, 2023. 6, 8 [31] Jiang Liu, Hui Ding, Zhaowei Cai, Yuting Zhang, Ravi Kumar Satzoda, Vijay Mahadevan, and Manmatha. Polyformer: Referring image segmentation as sequential polygon generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18653 18663, 2023. 5, 6 [32] Si Liu, Tianrui Hui, Shaofei Huang, Yunchao Wei, Bo Li, and Guanbin Li. Cross-modal progressive comprehension IEEE Transactions on Pattern for referring segmentation. Analysis and Machine Intelligence, 44(9):47614775, 2021. 2 [33] Sun-Ao Liu, Yiheng Zhang, Zhaofan Qiu, Hongtao Xie, Yongdong Zhang, and Ting Yao. Caris: Context-aware referring image segmentation. In Proceedings of the 31st ACM International Conference on Multimedia, pages 779788, 2023. 1, 2, 3, 4, 5, 6, [34] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019. 2 [35] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1001210022, 2021. 3 [36] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. 3 [37] Edgar Margffoy-Tuay, Juan Perez, Emilio Botero, and Pablo Arbelaez. Dynamic multimodal instance segmentation In Proceedings of the guided by natural language queries. European Conference on Computer Vision (ECCV), pages 630645, 2018. 2 [38] Hossein Mobahi, Mehrdad Farajtabar, and Peter Bartlett. Self-distillation amplifies regularization in hilbert space. Advances in Neural Information Processing Systems, 33:3351 3361, 2020. [39] Or Patashnik, Zongze Wu, Eli Shechtman, Daniel Cohen-Or, and Dani Lischinski. Styleclip: Text-driven manipulation In Proceedings of the IEEE/CVF Inof stylegan imagery. ternational Conference on Computer Vision (ICCV), pages 20852094, 2021. 1 [40] Mary Phuong and Christoph Lampert. Distillation-based In Proceedings of training for multi-exit architectures. the IEEE/CVF international conference on computer vision, pages 13551364, 2019. 2 [41] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. 3 [42] Sylvestre-Alvise Rebuffi, Sven Gowal, Dan Andrei Calian, Florian Stimberg, Olivia Wiles, and Timothy Mann. Data augmentation can improve robustness. Advances in Neural Information Processing Systems, 34:2993529948, 2021. 3 [43] Dhruv Shah, Błazej Osinski, Sergey Levine, et al. Lmnav: Robotic navigation with large pre-trained models of language, vision, and action. In Conference on Robot Learning, pages 492504. PMLR, 2023. 1 [44] Nisarg Shah, Vibashan VS, and Vishal Patel. Lqmformer: Language-aware query mask transformer for referring image segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1290312913, 2024. 6 [45] Hengcan Shi, Hongliang Li, Fanman Meng, and Qingbo Wu. Key-word-aware network for referring expression image segmentation. In Proceedings of the European Conference on Computer Vision (ECCV), pages 3854, 2018. [46] Jiajin Tang, Ge Zheng, Cheng Shi, and Sibei Yang. Contrastive grouping with transformer for referring image segIn Proceedings of the IEEE/CVF Conference mentation. on Computer Vision and Pattern Recognition, pages 23570 23580, 2023. 6 [47] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herve Jegou. Training data-efficient image transformers & distillation through atIn International conference on machine learning, tention. pages 1034710357. PMLR, 2021. 1 [48] Hugo Touvron, Matthieu Cord, Maxime Oquab, Piotr Bojanowski, Jakob Verbeek, and Herve Jegou. Co-training In Proceedings of 2l submodels for visual recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1170111710, 2023. 3 [49] Zhaoqing Wang, Yu Lu, Qiang Li, Xunqiang Tao, Yandong Guo, Mingming Gong, and Tongliang Liu. Cris: ClipIn Proceedings of driven referring image segmentation. the IEEE/CVF conference on computer vision and pattern recognition, pages 1168611695, 2022. 1, 2, 3, 5, 6, 8 [50] Jason Wei and Kai Zou. Eda: Easy data augmentation techniques for boosting performance on text classification tasks. 10 In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), 2019. [62] Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and Yi Yang. Random erasing data augmentation. In Proceedings of the AAAI conference on artificial intelligence, pages 1300113008, 2020. 2 [51] Ross Wightman. https : / / github . com / rwightman / pytorch - image - models, 2019. 1 Pytorch image models. [52] Yixuan Wu, Zhao Zhang, Chi Xie, Feng Zhu, and Rui Zhao. Advancing referring expression segmentation beyond single image. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 26282638, 2023. 2 [53] Zhenda Xie, Zheng Zhang, Yue Cao, Yutong Lin, Jianmin Bao, Zhuliang Yao, Qi Dai, and Han Hu. Simmim: simple framework for masked image modeling. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 96539663, 2022. 2 [54] Zunnan Xu, Zhihong Chen, Yong Zhang, Yibing Song, Xiang Wan, and Guanbin Li. Bridging vision and language encoders: Parameter-efficient tuning for referring image segIn Proceedings of the IEEE/CVF International mentation. Conference on Computer Vision, pages 1750317512, 2023. 3, 5, 6, [55] Yuhuan Yang, Chaofan Ma, Jiangchao Yao, Zhun Zhong, Ya Zhang, and Yanfeng Wang. Remamber: Referring arXiv preprint image segmentation with mamba twister. arXiv:2403.17839, 2024. 6 [56] Zhao Yang, Jiaqi Wang, Yansong Tang, Kai Chen, Hengshuang Zhao, and Philip HS Torr. Lavt: Language-aware vision transformer for referring image segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1815518165, 2022. 1, 2, 3, 5, 6, 8 [57] Linwei Ye, Mrigank Rochan, Zhi Liu, and Yang Wang. Cross-modal self-attention network for referring image segIn Proceedings of the IEEE/CVF conference mentation. on computer vision and pattern recognition, pages 10502 10511, 2019. 2 [58] Licheng Yu, Patrick Poirson, Shan Yang, Alexander Berg, and Tamara Berg. Modeling context in referring expresIn Computer VisionECCV 2016: 14th European sions. Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part II 14, pages 6985. Springer, 2016. 5 [59] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regularization strategy to train strong classifiers with localizable features. In Proceedings of the IEEE/CVF international conference on computer vision, pages 60236032, 2019. 3 [60] Linfeng Zhang, Jiebo Song, Anni Gao, Jingwei Chen, Chenglong Bao, and Kaisheng Ma. Be your own teacher: Improve the performance of convolutional neural networks via self distillation. In Proceedings of the IEEE/CVF international conference on computer vision, pages 37133722, 2019. 2 [61] Zicheng Zhang, Yi Zhu, Jianzhuang Liu, Xiaodan Liang, and Wei Ke. Coupalign: Coupling word-pixel with sentencemask alignments for referring image segmentation. Advances in Neural Information Processing Systems, 35: 1472914742, 2022. 11 MaskRIS: Semantic Distortion-aware Data Augmentation for Referring Image Segmentation"
        },
        {
            "title": "Contents",
            "content": "A. Ablation Study A.1. Analysis of Mask Sampling Strategy . . . . . . . . A.2. Analysis of Computational Cost . . . . . A.3. Analysis of Training Loss . . . . . A.4. Impact of Hyperparameters . . . . . . . . . . . . B. More Qualitative Examples C. Implementation Details 1 1 1 2 2 2 Figure A. Various types of mask sampling strategies. We use patch-wise sampling [11] as our default. For block-wise sampling, we follow BEiT[2] to remove large random blocks. For Cutout [8], we follow the implementation of timm [51]. For all strategies except Cutout, we maintain consistent masking ratio of 75%. Patch-wise Grid-wise [11] Block-wise [2] Cutout [8] oIoU 76.49 76.14 75. 76.07 Table A. Impact of mask sampling strategy. The performance (oIoU) is evaluated across various mask sampling strategies on the RefCOCO validation set. A. Ablation Study A.1. Analysis of Mask Sampling Strategy To rigorously validate the impact of image masking on model training, we explored various mask sampling strategies and their effects. Fig. visually illustrates each method. Our baseline, patch-wise sampling [11, 13, 15] divides images into non-overlapping patches and randomly masks certain patches. This approach is widely used due to its simplicity and effectiveness. We also tested grid-wise sampling [11], where rows and columns are masked systematically rather than randomly. Additionally, we investigated block-wise masking, as used in BEiT [2], where contiguous regions of the image are masked, potentially eliminating Method Epoch val testA testB CARIS [33] CARIS [33] MaskRIS MaskRIS 50 100 25 50 74.67 74.80 75.87 76.49 77.63 77. 77.99 78.96 71.71 70.63 73.51 73.96 Table B. Comparison of computational cost. All results (oIoU) are evaluated on the RefCOCO dataset. MaskRIS outperforms CARIS even with half of the training epochs. more important structural details by covering larger areas. Finally, we evaluated Cutout [8], which masks randomly selected rectangular section of the image. Tab. summarizes the results (oIoU) of each mask sampling strategy on the RefCOCO validation set. Remarkably, all the strategies we tested consistently outperform the previous SoTA method, CARIS [33] (74.67 oIoU), with significant margins. Among these strategies, our patchwise random sampling approach achieved superior performance, demonstrating its ability to generate more varied training data than grid-wise sampling and minimize training data distortion more effectively than block-wise sampling. These results confirm the importance of selecting an appropriate masking strategy to improve performance. A.2. Analysis of Computational Cost The MaskRIS framework introduces additional computational overhead due to its Distortion-aware Contextual Learning (DCL) mechanism, which processes both the original and masked inputs during training. To ensure fair comparison, we adjusted the training epochs to evaluate whether the performance gains are simply due to increased computation. As shown in Tab. B, doubling the training epochs for CARIS does not improve its performance, confirming that MaskRISs superior results are not attributable to increased computation. Furthermore, even when MaskRIS is trained for only 25 epochs, it still outperforms CARIS trained for 50 epochs. For computational efficiency, we measured the total training time on setup with A60002 GPUs. CARIS required 18.4 hours for 50 epochs, whereas MaskRIS achieved better performance in only 16.7 hours with 25 epochs. These results demonstrate the effectiveness of our masking strategy and DCL framework in achieving both high efficiency and strong performance. Through reducing the training time required for superior results, MaskRIS establishes itself as practical and resource-efficient solution for improving model training. Figure B. Training loss analysis of MaskRIS on RefCOCO. In the CARIS w/ Masking setting, we employ both image and text masking as data augmentation strategies within the CARIS [33] framework. We visualize (a) the training loss for the original (i.e., unmasked) inputs, (b) the training loss for the masked inputs, and (c) the performance of the model on the RefCOCO validation set. ps 8 32 64 112 λ 0.1 0. 0.5 0.75 1.0 oIoU 76.58 76.49 76. 76.47 76.01 oIoU 75.48 76.62 76.49 76. 74.67 Table C. Impact of patch size on image masking. The performance (oIoU) is evaluated across different patch sizes on the RefCOCO validation set. ps denotes the patch size. Table D. Impact of λ on the DCL procedure. The performance (oIoU) is evaluated across various λ on the RefCOCO validation set. A.3. Analysis of Training Loss We analyzed the MaskRISs effectiveness through an indepth analysis of training loss, comparing three distinct settings: CARIS [33], CARIS w/ Masking, and MaskRIS. For CARIS w/ Masking, we employ both image and text masking as data augmentation strategies, applying them with 50% probability. The training loss curves and performance trends over different epochs are shown in Fig. B. In the baseline CARIS, training loss with the original inputs rapidly converges, while training loss with the masked inputs increases over time. This indicates that CARIS tends to overfit to clean inputs as training progresses. Introducing masking in CARIS helps mitigate this overfitting by incorporating data augmentation. However, it disrupts learning stability and slows down loss convergence, limiting its effectiveness. In contrast, MaskRIS effectively resolves these issues by significantly improving loss convergence for both original and masked inputs while achieving superior performance. This is due to its ability to effectively mitigate the learning instability caused by input masking while maintaining the benefits of increased data diversity and robustness. These results highlight the robustness and efficiency of MaskRIS in optimizing training and improving model performance. serve that patch size of 8 yields the highest performance. Nevertheless, the results suggest that our method exhibits stable and strong performance as long as the patch size remains within reasonable range, highlighting its robustness to patch size variations. Loss Weight of DCL. As described in Sec. 3.3, our training objective of Distortion-aware Contextual Learning (DCL) is Ltotal = Ldist +Lce. To balance the scale of the loss values with previous methods [33] during the training phase, we adapt the equation to: Ltotal = λ Ldist + (1 λ) Lce, (5) where λ is set to 0.5 for all experiments to avoid extensive hyperparameter search for optimal performance. λ of 1.0 indicates training only with Lce, which is the equivalent to CARIS [33]. In Tab. D, we explore how varying λ affects the DCL procedure. As shown in Tab. D, while the best performance is achieved with λ = 0.25, the performance with λ of 0.5 remains significantly better than the results without Ldist. These results confirm that MaskRIS does not require extensive hyperparameter search for λ and performs reasonably well with λ of 0.5. A.4. Impact of Hyperparameters B. More Qualitative Examples Patch Size of Image Masking. To further analyze the hyperparameters of image masking, we examined the training results of different patch sizes for patch-wise image masking. Tab. shows that our method consistently achieves superior performance regardless of patch size, outperforming the current SoTA method, CARIS [33]. Specifically, we obFig. provides additional qualitative examples on the RefCOCO dataset across different occlusion scenarios as described in Sec. 3.2. We categorize our analysis into three cases: (a) intra-object context masking, where occlusions occur within the object of interest; (b) inter-object context masking, where occlusions occur within non-target objects; 2 and the others, respectively, with polynomial learning rate schedule with power of 0.9. The model was trained for 50 epochs with batch size of 16, and the input images were resized to 448 448. To validate the adaptability of MaskRIS with other RIS methods, we integrated it into several existing models in both fully and weakly supervised settings, as shown in Tab. 2. Importantly, we follow the original training recipes of each method without any changes to the original architecture. This simple integration demonstrates the flexibility of MaskRIS and its seamless compatibility with existing frameworks. and (c) text context masking, where parts of textual descriptions are obscured. In scenario (a), CARIS [33] often struggles to recognize objects that are only partially visible, highlighting its difficulty in handling incomplete visual information. The challenge is not limited to occlusions of the target objects. In scenario (b), CARIS [33] also faces challenges when occlusions occur in non-target areas surrounding the target object, reducing its ability to identify the intended objects correctly. Similar limitations are observed in the text description domain, as shown in Fig. C(c). While CARIS can successfully recognize the target objects with complete text descriptions, its performance drops when only partial text information is available. In contrast, our MaskRIS demonstrates robustness across these occlusion scenarios by improving its comprehension and utilization of diverse contextual details in the image and text data. Fig. shows more qualitative examples on the RefCOCO dataset. As shown in Fig. D(a), our MaskRIS accurately captures the target objects without overor undercapturing them. In Fig. D(b), it demonstrates strong robustness to occlusion, effectively identifying partially visible objects. Furthermore, as highlighted in Fig. D(c), MaskRIS successfully aligns target objects with the given text descriptions. These results demonstrate that our approach significantly improves the robustness of the model and its ability to utilize textual clues for accurate object identification. We also provide failure cases in Fig. D(d). When the referred object in an image or the given text description is ambiguous or difficult to recognize, our approach sometimes struggles to identify the correct target. For instance, in the first example of the top row, MaskRIS fails to recognize the drawing (flowers) on the bottle, instead capturing different object. Another failure occurs when text descriptions include contrasting relative positions, such as left and right, which confuse the model. Insufficient text descriptions also pose significant challenges. When text descriptions do not sufficiently characterize the target object such as yellow or her in the bottom row, MaskRIS captures the other objects as the target. These examples illustrate that while MaskRIS shows significant improvements in robustness and accuracy across various scenarios, it can still be limited in handling complex visual scenes or overly vague and ambiguous text descriptions. C. Implementation Details Most of our experimental results are based on CARIS [33]. For the image encoder, we used the Swin-Base Transformer [35], pre-trained on ImageNet-22k [6], and for the text encoder, we employed BERT-Base [7]. The maximum length of the text is set to 20 words. We used the AdamW [36] optimizer with weight decay of 0.01. We applied different learning rates of 1e5 and 1e4 to encoders 3 Figure C. Qualitative examples under various occluded contexts on the RefCOCO dataset. Although CARIS [33] tends to be inaccurate under occluded contexts, MaskRIS produces accurate predictions, demonstrating its robustness to occlusion and incomplete information. For text context masking, the word highlighted in red is masked. 4 Figure D. More qualitative examples on the RefCOCO dataset. MaskRIS mitigates the limitations of CARIS [33] by (a) capturing target objects more precisely, (b) being robust to occlusions, and (c) effectively using various textual clues. (d) We also provide failure cases."
        }
    ],
    "affiliations": [
        "KAIST AI",
        "NAVER AI Lab",
        "Yonsei University"
    ]
}