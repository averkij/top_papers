{
    "paper_title": "BizGen: Advancing Article-level Visual Text Rendering for Infographics Generation",
    "authors": [
        "Yuyang Peng",
        "Shishi Xiao",
        "Keming Wu",
        "Qisheng Liao",
        "Bohan Chen",
        "Kevin Lin",
        "Danqing Huang",
        "Ji Li",
        "Yuhui Yuan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recently, state-of-the-art text-to-image generation models, such as Flux and Ideogram 2.0, have made significant progress in sentence-level visual text rendering. In this paper, we focus on the more challenging scenarios of article-level visual text rendering and address a novel task of generating high-quality business content, including infographics and slides, based on user provided article-level descriptive prompts and ultra-dense layouts. The fundamental challenges are twofold: significantly longer context lengths and the scarcity of high-quality business content data. In contrast to most previous works that focus on a limited number of sub-regions and sentence-level prompts, ensuring precise adherence to ultra-dense layouts with tens or even hundreds of sub-regions in business content is far more challenging. We make two key technical contributions: (i) the construction of scalable, high-quality business content dataset, i.e., Infographics-650K, equipped with ultra-dense layouts and prompts by implementing a layer-wise retrieval-augmented infographic generation scheme; and (ii) a layout-guided cross attention scheme, which injects tens of region-wise prompts into a set of cropped region latent space according to the ultra-dense layouts, and refine each sub-regions flexibly during inference using a layout conditional CFG. We demonstrate the strong results of our system compared to previous SOTA systems such as Flux and SD3 on our BizEval prompt set. Additionally, we conduct thorough ablation experiments to verify the effectiveness of each component. We hope our constructed Infographics-650K and BizEval can encourage the broader community to advance the progress of business content generation."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 2 ] . [ 1 2 7 6 0 2 . 3 0 5 2 : r BizGen: Advancing Article-level Visual Text Rendering for Infographics Generation Yuyang Peng Shishi Xiao Keming Wu Qisheng Liao Bohan Chen Kevin Lin Danqing Huang Ji Li Yuhui Yuan Tsinghua University, Brown University, University of Liverpool, Microsoft Research Asia, Microsoft https://bizgen-msra.github.io (a) 386 characters / OCR: 93% (b) 426 characters / OCR: 96% (c) 545 characters / OCR: 99% (d) 594 characters / OCR: 99% (e) 737 characters / OCR: 97% Figure 1. Accurate article-level visual text rendering results in infographics generated with BizGen, ranging from 386 to 737 characters."
        },
        {
            "title": "Abstract",
            "content": "Recently, state-of-the-art text-to-image generation models, such as FLUX and Ideogram 2.0, have made significant progress in sentence-level visual text rendering. In this paper, we focus on the more challenging scenarios of articlelevel visual text rendering and address novel task of generating high-quality business content, including infographics and slides, based on user provided article-level descriptive prompts and ultra-dense layouts. The fundamental challenges are twofold: significantly longer context lengths and the scarcity of high-quality business content data. In contrast to most previous works that focus on limited number of sub-regions and sentence-level prompts, ensuring precise adherence to ultra-dense layouts with tens or even hundreds of sub-regions in business content is far more challenging. We make two key technical contributions: (i) the construction of scalable, high-quality business content dataset, i.e., Infographics-650K, equipped with ultradense layouts and prompts by implementing layer-wise retrieval-augmented infographic generation scheme; and (ii) layout-guided cross attention scheme, which injects tens of region-wise prompts into set of cropped region latent space according to the ultra-dense layouts, and refine each subregions flexibly during inference using layout conditional CFG. We demonstrate the strong results of our system compared to previous SOTA systems such as FLUX and SD3 on our BizEval prompt set. Additionally, we conduct thorough ablation experiments to verify the effectiveness of each component. We hope our constructed Infographics-650K and BizEval can encourage the broader community to advance the progress of business content generation. 1. Introduction Recently, graphic design generation [2, 3, 6, 811, 18, 24, 35, 38, 42] has attracted increasing interest, as modern textto-image generation models like SD3, DALLE3, FLUX, (b) 426 characters / OCR: 30% (c) 545 characters / OCR: 22% (a) 386 characters / OCR: 11% Figure 2. Infographics generation results based on SD3-Large. While these results appear appealing, the spelling accuracy of the article-level visual text is inadequate, as shown in the zoomed-in marked rectangle regions in the second row. (e) 737 characters / OCR: 46% (d) 594 characters / OCR: 55% and Ideogram 2.0 demonstrate unprecedented capabilities in following complex prompts and rendering accurate visual text. Graphic design encompasses diverse array of media, including posters, flyers, diagrams, charts, infographics, and slides, each tailored for distinct contexts. In this study, we concentrate on two of the most complex forms of graphic design: infographics and slides. These particular media types, especially infographics, demand advanced visual planning and sophisticated reasoning skills, as they are crafted to communicate complex and article-level messages effectively to their audiences. Our empirical analysis reveals that even the latest models, such as FLUX and Ideogram 2.0, continue to struggle with generating usable infographics due to inaccurate visual text rendering. Figure 2 shows representative infographic examples generated by the latest SD3 Large. We observe that these models frequently exhibit significant artifacts, including disorganized layout, numerous visual text spelling errors, and generally poor aesthetics, when compared to the visual results generated with our approach in Figure 1. We analyze the fundamental challenges of infographics generation from the following two perspectives: (i) context length: Infographics require significantly longer context, such as article-level prompt, to be accurately described. Typically, more than 500 words are needed to describe an infographic in detail, whereas conventional image generation tasks often require fewer than 50 words. Furthermore, typical infographic has resolution of 2240 896 to ensure the accurate and dense rendering of visual text, which is nearly twice the size of the conventional 1024 1024 resolution adopted by existing text-to-image generation systems. Figure 3 illustrates the comparisons of the context length (ii) data scarcity: with previous image generation tasks. Figure 3. Significant challenge of business content generation: the context lengths increase by more than 10 compared to the previous textto-image generation models. High-quality infographics data is much more challenging to collect. The availability of infographics data on the internet is scarce, primarily due to the substantial human effort and commercial licensing required to obtain it. Consequently, amassing large dataset of infographics is considerably more difficult compared to other types of graphic design images. To tackle these two fundamental challenges, we make two key contributions: First, we develop an innovative and scalable infographics data engine. This engine generates comprehensive dataset Infographics-650K of high-quality, high-resolution infographics, effectively addressing the issue of data scarcity. This dataset comprises over 650, 000 samples, with each sample containing not only detailed global captions and images but also ultra-dense layouts and region-specific captions. Second, unlike the previous Glyph-ByT5 [22], which only supports layer-wise control for visual text, we introduce more flexible layout-guided cross attention design that supports complicated layer-wise control for any visual elements rather than only visual text. This pipeline can transform the original extremely long articlelevel prompt to image generation into series of shorter, more manageable sentence-level prompts to region generation tasks. We further devise layout conditional CFG method during inference to eliminate artifacts in every subregion. By leveraging the relationship between ultra-dense layouts and their corresponding shorter region-specific captions, our approach effectively addresses the challenge of much longer context lengths. To validate the effectiveness of our approach, we build challenging BizEval benchmark that requires generating accurate article-level visual text in infographics and slides. We empirically show that our approach significantly outperforms the latest state-of-the-art methods like FLUX in terms of visual text rendering and accurate region-wise control. Additionally, we find that previous approaches suffer from severe layer missing issues, whereas our approach can ensure the generation of almost all the requested layers. We also conduct thorough user study to assess the generated business content from three aspects: visual aesthetics, visual (a) BizGen v.s. DALLE3 (b) BizGen v.s. SD3 Large (c) BizGen v.s. FLUX Figure 4. BizGen v.s. SOTAs on BizEval: Illustrating the win-lose rates based on our user study, which collected feedback from 10 users. text spelling accuracy, and prompt following. We demonstrate the advantages of our approach and report the detailed user study results in Figure 4. 2. Related Work 2.1. Typography Generation Typography generation is known to be fundamental limitation for the latest text-to-image generation models. Several efforts [3, 4, 13, 14, 21, 24, 26, 30, 36, 42, 44] have been made to improve the visual text spelling accuracy, but most of them still only handle visual texts with limited length of tens of characters. State-of-the-art text-to-image generation models, such as FLUX [12], Ideogram 2.01, and Stable Diffusion 3 [7], have made significant improvements in sentence-level visual text rendering, but spelling accuracy can drop dramatically when the textual content becomes longer. The Glyph-ByT5 [22, 23] series pretrains ByT5 model to encode visual text and fine-tunes LoRA to empower the SDXL model with the capacity to handle textual content longer than 100 characters. These methods still cannot tackle article-level visual text rendering tasks, which typically contain tens of sentence-level text boxes in one imagequite common in infographics and slides. In this study, we focus on the challenging article-level visual text rendering scenario. 2.2. Layout Guided Image Generation Due to the limited ability of representing spatial layout in text prompt, prior works add fine grained spatial controls in the generation process either through encoding layout information[1, 15, 31, 37, 41, 43], or through introducing plug-and-play modules that inject visual guidance[25, 41]. In addition to introducing auxiliary modules and extra training, some research manipulates latent or cross-attention maps according to spatial or semantic constraints during inference[5, 27, 32]. Currently, some work incorporates LLMs as visual planmner to construct reasonable layout, guiding the next generation of high-faithfulness 1https://about.ideogram.ai/2.0 images[16, 29, 34, 39] However,these methods struggle to generate satisfactory results in our scenario because of the ultra-dense distribution and heavily overlapping of elements in infographics and slides. Besides, their training pipelines rely on annotating for existing data to get layout-image pairs, which is not scalable. In contrast, we propose scalable data engine to obtain high-quality, high-resolution infographic data, and introduce flexible layout-guided latent fusion method to model the complex layout information. 3. Our Approach In Section 3.1, we illustrate the formulation of the business content generation task by taking infographics as an example and analyze the fundamental challenges of building business content generation model. In Section 3.2, we explain the first key contribution of this work: scalable infographics data engine. In Section 3.3, we further introduce flexible layout-guided layer-wise latent fusion method that supports ultra-dense region-wise control for business content. Last, we explain the implementation of the BizGen framework based on the aforementioned designs and introduce layout conditional CFG scheme for flexible control of the layer-wise generation quality. 3.1. Business Content Generation In this paper, we study the generation of complex business content, including infographics and slides, from article-level text prompts. Figure 5 shows representative example of an infographic. It consists of multiple layers, including background layer, several visual text layers, object layers, and decorative layers. The main distinctions between business content generation and previous poster generation lie in (i) precise spatial arrangement of tens of visual design elements and grouping relevant layers together, and (ii) the accurate rendering of article-level visual text formed by title, subtitle, and body text comprising hundreds of words. Key Challenges As shown in Figure 2, when generating infographics, it is evident that the SOTA generative models exhibit significant artifacts, such as disorganized layout arrangements, numerous visual text spelling errors, and generally poor aesthetics. We attribute these challenges to two primary factors: the scarcity of high-quality business content data and the significantly longer context length. We address these two fundamental challenges through the following key contributions. 3.2. Scalable Infographics Data Engine We collect an internal dataset consisting of over 5,000 infographics, obtaining raw data that includes all the layers and their layout information. We then propose scalable infographics data engine comprising two stages: the construction of high-quality transparent layer database and Figure 5. Decomposed visual elements (layers) that form representative infographic sample. The right side displays the ultra-dense layout that specifies the spatial arrangements of each visual element. Figure 6. Overall pipeline of retrieval-augmented infographics generation: we begin with template infographic and perform layer selection to identify the most important visual layers. Then we conduct layer-wise retrieval from constructed database of high-quality transparent layers and execute layer-wise replacement to generate various variants of the infographics. the implementation of retrieval-augmented infographic generation, resulting in the generation of over 1 million infographics, as detailed below. High-Quality Transparent Layer Generation. First, we create over 250K design-oriented layer-wise prompts and apply the latest LayerDiffuse[40] to generate high-resolution transparent graphic design layers based on the recently posttrained SDXL [17]. To further increase the number of layers and support diverse style control, we integrate four popular LoRAs to generate transparent layers in various styles. We filter the data by eliminating non-transparent layers based on their alpha channel to ensure the visual harmony of synthesized data. As result, we construct high-quality, highresolution transparent layer database consisting of over 1M transparent layers. Layer-wise Retrieval-Augmented Infographics Generation. As shown in Figure 6, we propose retrievalaugmented pipeline to transform given reference infographic into multiple new infographics with different visual elements. First, we use GPT-4o to identify the most important or visually dominant transparent object layers for each infographic separately. Next, we treat these selected layers as queries and search for the top 10 relevant layers with the highest CLIP similarity, discarding layers with significant discrepancies in aspect ratio, and use the remaining layers to replace the original layers."
        },
        {
            "title": "To further diversify our",
            "content": "synthesized infographics database, we randomly replace the solid-colored background (a) #text layers (b) #non-text layers (c) #total layers (d) #chars/layer Figure 7. Distribution of the number of layers, visual text layers, nontext layers, and the number of characters in the visual text layers. We mark the median values with red dashed lines. layers with their counterparts from the retrieval database. Consequently, we generate over 650,000 high-quality multilingual infographics of various styles, collectively referred to as Infographics-650K. Figure 6 illustrates the entire pipeline of our layer-wise retrieval-augmented generation approach. Therefore, these generated high-quality multilingual infographics are of significant value to the community for studying the fundamental challenges of business content generation. Statistics of Infographics-650K The constructed dataset comprises over 650K high-quality infographic samples, evenly distributed across ten different languages: English, French, German, Spanish, Italian, Portuguese, Russian, Chinese, Japanese, and Korean. All the samples have resolution of 2240 896 thanks to our collected original data and generated high-quality transparent layers, which is critical for improving training results. Figure 7 provides main statistical information about the extracted layouts across the entire dataset, including the number of text layers, non-text layers, total layers, and characters. We observe that the infographic samples are highly layout-intensive, with an average of approximately 30 layers. Layer-wise Caption for Infographics-650K Given our access to both ground-truth element layers and visual textual data, we propose using the state-of-the-art LLaVA-1.6-34B model [19, 20] to generate comprehensive layer-wise captions and global captions corresponding to the collected infographics. To generate the multilingual infographics samples, we utilize the latest GPT-4o to translate the original English text into other languages and then render it back to obtain captioned multilingual business content data. 3.3. Layout-Guided Cross Attention Key Idea. As illustrated in Figure 8, the key idea of the layout guided cross attention is to decompose the long context modeling task (10 times longer than for standard image generation tasks) into multiple shorter context modeling task by explicitly divide the vision tokens and text tokens into multiple groups according to the layout prior. Instead of letting each vision token interact with nearly 1,000 text tokens, we only need to perform cross-attention for each pair of the grouped vision tokens and text tokens, where the grouped tokens belong to the same rectangle area according to the layout prior and the text tokens are based on the region-wise prompt with only within 100 text tokens. Therefore, the (a) Layout Guided Cross Attention (b) Training pipeline of BizGen (c) Evaluation pipeline of BizGen Figure 8. Illustrating the Framework of Our Approach: (a) Layout Guided Cross Attention: we first crop the latent representations of the entire image into multiple groups of layer-wise visual tokens and extract the layer-wise text tokens according to the region prompts. Then, we perform region-wise cross attention over the corresponding layer-wise visual tokens and layer-wise text tokens to control the visual content generation within each region. (b) BizGen training pipeline: the inputs to the LDM include noisy latent feature map, layout, and the regional prompts, and the output of the LDM is noise prediction. (c) BizGen evaluation pipeline: the inputs to the LDM include noise map, layout, and the regional prompts, and the outputs of the LDM (the combination of multiple iterative refinement) form predicted clean latent that can be decoded into an infographic image that follows the given layout and regional prompts, which are provided by either users or an LLM like GPT-4o. , prompt context length is decreased by nearly 10 with our layoutguided cross-attention design. Formulation. We denote the ultra dense layout prior i=1 {bText as {bNon-Text , where bNon-Text represent the bounding boxes for the nontext object layer and visual text layer, respectively. The terms prompt denote the prompts for each layer, while and are the numbers of non-text visual object layers and visual text layers, respectively. and prompt and bText , prompt j}M j=1 i}N i i"
        },
        {
            "title": "Then we crop the input latent representation f before sent",
            "content": "into the l-th Cross-Attention block and according to: ), xi = Crop(f , bNon-Text xj = Crop(f , bText ), zi = CrossAttention(xi, CLIP(prompt zj = CrossAttention(xj, GlyphByT5(prompt i)), (1) (2) (3) (4) j)), where xi and xj represent the layer-wise vision tokens for the i-th non-text visual layer and j-th visual text layer, respectively. We use CLIP() to denote the CLIP text encoder and GlyphByT5() to denote the Glyph-ByT5 text encoder [22, 23]. Last, we integrate the updated region-wise representations into combined whole image representation as follows: = (cid:88) i=1 Paste(zi, bNon-Text ) + (cid:88) j=1 Paste(zj, bText ), (5) where each Paste() operation inserts the updated output from the previous cross-attention into zero-valued tensors of the same shape as the input feature map . We then combine them following their z-order to maintain their original spatial arrangement along the layer dimension. To ensure efficient implementation, we batch the cropped visual tokens and the region-wise text tokens across multiple layers. BizGen. We implement our business content generative model (BizGen) based on the latest Glyph-SDXLv2 [22, 23] as it supports accurate multilingual visual text generation. The key modification involves replacing the original region-wise multi-head cross-attention scheme with our layout-guided cross-attention scheme, which explicitly selects the visual tokens and text tokens for each element within layout prior. This approach allows for more flexible control not only over text layers but also over visual object layers. The model weights are initialized from the pretrained multilingual Glyph-SDXL [22, 23], to which we add set of LoRA weights. We encode the prompts for all non-text layers using the CLIP text encoders and the prompts for the text layers with the Glyph-ByT5 text encoder. Since most layerwise prompts exceed CLIPs default maximum length of 77, we divide the prompts into smaller chunks before sending them through the CLIP encoder and then concatenate the results. We also apply lightweight mapper to align the text embeddings from Glyph-ByT5 with the original SDXL latent space. We directly fine-tune the LoRA weights and the mapper weights with the constructed infographics dataset with the following hybrid loss: (cid:34) = Et,z0,ϵ (1 Mtext) (cid:13) (cid:13)ϵ ϵθ (cid:0)zt, t, {bNon-Text , prompt i}(cid:1)(cid:13) 2 (cid:13) 2 + β Mtext (cid:13) (cid:13) (cid:13)ϵ ϵθ (cid:16) zt, t, {bText , prompt } (cid:35) , (cid:17)(cid:13) (cid:13) (cid:13) 2 2 (6) where Mtext is binary mask that encodes all visual text layer regions as 1 and 0 otherwise, and β is hyperparameter that controls the loss weight on the visual text regions. Layout Conditional CFG. To enable flexible control for each layer separately, we further propose layoutconditional CFG strategy that applies dense guidance scale map with different guidance values for positions from different layers. During training, we randomly drop the layer-wise captions and replace them with null prompt. During inference, we first generate set of binary masks {Mi}N +M that record the positions belonging to the bounding box region, e.g., {bNon-Text , in the given layer with }M j=1 1, and 0 otherwise. Then, we compute the dense guidance scale map by adding the above rescaled binary masks i=1 {bText }N i=1 following their layer order: = +M (cid:88) i=1 γiMi, ˆxLCFG = xt + (xcond xt), (7) (8) where is 2D matrix of the same shape as the latent representation xt, γi is the guidance weight used to control the CFG strength for the i-th layer guidance mask, and we perform the above process for certain timesteps (0 αT ). Here, we assume these layers already follow the correct order from index = 1 to index = (M + ) for simplicity. We empirically demonstrate that this layout-conditional CFG scheme is important for addressing artifacts and present the qualitative results in Section 4.5. The ablation experiments on the effect of applying such layout-conditional CFG strategy across different timestep ranges are presented in the Appendix K. 4. Experiment First, we explain the training settings and construct BizEval (Business Content Generation Benchmark) to evaluate business content generation results conditioned on various complex business-oriented layouts. Second, we introduce novel evaluation metric, the layer-wise generation success rate, to reflect whether the generation model can follow the ultra-dense layout and reliably generate each layer. We also report metrics for the entire infographics or slides from different perspectives, such as prompt following, aesthetics, and OCR. Third, we present comparison results, including the aforementioned metrics and thorough user study, with state-of-the-art image generation models, including SD3 and DALLE3, on our BizEval. Finally, we conduct comprehensive ablation study to examine the influence of different factors within our system. 4.1. Training Settings We follow the default training settings of Glyph-ByT5 [22, 23] to fine-tune the SDXL using LoRA on our constructed Infographics-650K. We also construct Slides-500K for multi-page slides generation. We detail the hyperparameter choices for different types of business content in the Appendix B. All these models are trained with 16A100 GPUs for 280 hours (w/o InfiniBand). Additionally, we follow Glyph-ByT5-v2 [23] to opt for Glyph-SDXL+SPO [17] to improve visual aesthetics. 4.2. Business Content Generation Benchmark We have constructed BizEval benchmark to evaluate the business content generation task, containing detailed articlelevel prompts and ultra-dense layouts for approximately 10100 infographic, 1090 sets of slides (approximately Figure 9. Local region quality accessment pipeline: this pipeline estimates the layer generation success rate based on the reference ultra-dense layout and region-wise prompts as conditions. We find that given only the GT layer caption and corresponding cropped out area of the generated image, GPT-4o tends to give conservative answers. For example, GPT-4o may mistaken the black-and-white cat for black-and-white dog, or imagine there to be green apple in the occluded area as shown in the left side of the dashed line. So we design the Classify-Caption-Score pipeline on the right to eliminate the illusion. 10 450 pages) in ten languages. We cover varying number of visual text layers and non-visual text layers of different generation difficulties, as detailed in Figure 7 and Appendix K. All experiments are conducted on BizEval with resolution of 2240 896 by default. During testing, all methods are provided with global prompts, while only methods supporting layout-guided cross attention are fed with layer-wise captions and layouts. 4.3. Evaluation Metrics Global Image Quality. We do not adopt the CLIP score as assessment, as our article-level global prompts are typically longer than 77 tokens. Instead, we utilize the latest GPT-4o to assess the generated infographics and slides from two perspectives: visual aesthetics and (global) prompt following. Additionally, we evaluate style consistency across different pages for slides. The GPT-4o prompt is provided in Appendix E. To evaluate the visual text spelling accuracy, we employ character-level precision for character-based languages, specifically Chinese, Japanese, and Korean, while using word-level precision for the other seven alphabetic languages. Local Region Quality. Considering that an infographic image typically consists of numerous elements, including visual text layers and non-text layers, it is crucial to assess whether the generated infographics adhere to the regionwise prompts and accurately generate each layer, particularly the quality of the non-text layers. To this end, we propose novel layout-conditional region-wise evaluation scheme using GPT-4o to rate the local region quality of all non-text layers. As shown in Figure 9, we first use GPT-4o to classify all layer prompts into two categories: block, which typically refers to substrate layers, and object, which usually describes specific objects. This classification is important because block layers are often occluded by visual text layers or other elements, so we focus on their color and texture. In contrast, for an object layer, we concentrate on its semantic content. Next, we crop each layer from the generated image (with occlusions from higher layers masked out) and ask GPT-4o to provide detailed description of the layers without supplying their ground-truth captions. Following this, we provide GPT-4o with the following information as outlined in Set-of-Mark [33]: (i) the global caption, (ii) the generated image and its annotated replica, (iii) detailed information for each layer, including the layer caption, bounding box, occlusion relationships, and element type from the first step, and (iv) the layer descriptions from the second step, which serve as auxiliary information to avoid confusion. We then ask GPT-4o to assign layer-wise prompt alignment score ranging from 0 to 10 for each layer, with score of 5 set as the threshold to calculate the LGSR (Layer Generation Success Rate) scores. The LGSR score is reported only when regional prompts and layouts are provided as conditions. The detailed prompt is included in the supplementary material. 4.4. Comparison with State-of-the-art We compare our BizGen with DALLE3, SD3 Large [7], and the latest FLUX [12] on our BizEval benchmark. The detailed comparison results are summarized in Table 1 and Table 2. Our approach significantly outperforms these strong models in terms of visual text spelling accuracy and prompt following, even though it is based solely on SDXL. We believe that combining our approach with the latest FLUX will yield even stronger results. According to Table 1, we observe substantial drop in spelling accuracy across all approaches when handling greater number of visual layers. Nevertheless, we achieve over 25% absolute performance gain over FLUX when generating infographics with more than 20 layers. Figure 10 illustrates some qualitative comparison results, showing that our BizGen achieves better quality than the other models. Additionally, since BizGen is based on SDXL, it significantly outmatches FLUX.1-dev in inference efficiency (40 seconds v.s. 68 seconds per image tested on subset of our BizEval). We also conducted user study involving 10 infographic users with design backgrounds, asking them to choose their preferred results. The user study results are summarized in Figure 4. We found that BizGen was preferred in 57.9%, 83.4%, and 84.3% of cases compared to DALLE3, SD3 Large, and FLUX, respectively, in terms of typography quality. An interesting observation is that FLUX demonstrated unprecedented capability in generating relatively better infographics than both DALLE3 and SD3 Large. reasonable hypothesis is that FLUX may have been trained on an infographic dataset. OCR: 97% OCR: 100% OCR: 100% OCR: 98% OCR: 98% OCR: 37% OCR: 52% OCR: 15% OCR: 65% OCR: 19% OCR: 21% OCR: 14% OCR: 10% OCR: 38% OCR: 43% OCR: 16% OCR: 0% OCR: 40% OCR: 14% OCR: 27% Figure 10. Qualitative comparison results with SOTAs. The 1st, 2nd, 3rd, and 4th rows correspond to the results generated with our BizGen, FLUX, SD3 Large, and DALLE3 . 4.5. Ablation Study Scaling Infographics Training Data We study the effect of scaling the infographics dataset size as illustrated in Tasble 3. Here baseline means directly using our layer-guided cross attention in inference without tuning the model. We observe significantly improved performance with more data across all metrics. For the trade-off between efficiency and performance, we choose Infographics-65K to conduct all other ablation experiments by default. Layer-Guided Cross Attention Table 4 ablates the benMethod Infographics Visual Text Spelling Precision (%) Slides Visual Text Spelling Precision (%) 10 layers 10-15 layers 15-20 layers 20 layers 10 layers 10-20 layers 20-30 layers 30 layers Method Text Spelling Precision (%) Non-text Layer Precision/LGSR (%) GTP-4o Score (1-10) 10 L. 10-15 L. 15-20 L. 20 L. 10 L. 10-20 L. 20-30 L. 30 L. Aesthetics Prompt Fo. DALLE3 SD3 Large FLUX Glyph-SDXL-v2 BizGen 16.42 30.92 41.33 78.18 92.98 92.98 92. 14.94 32.38 29.06 68.83 84.25 84.25 84.25 21.64 24.10 28.47 51.73 66.42 66.42 66.42 24.29 20.52 27.12 34.46 55.48 55.48 55.48 24.55 38.09 62.30 80.18 88.13 88.13 88.13 24.88 42.96 70.54 85.34 88.41 88.41 88.41 30.59 43.46 67.57 79.89 80.29 80.29 80. 22.52 31.39 55.37 65.07 66.81 66.81 66.81 Table 1. Comparison with SOTAs on visual text spelling precision for infographic and slide generation. Method DALLE3 SD3 Large FLUX BizGen Infographics Aesthetics Prompt Fo. Aesthetics Slides Prompt Fo. Style Cons. 7.73 6.04 6.20 7.747.747.74 5.32 3.88 4.11 8.098.098. 8.078.078.07 6.49 7.16 7.64 6.80 5.29 6.32 7.617.617.61 8.23 6.67 7.84 8.378.378.37 Table 2. Comparison with SOTAs on GPT-4o assessment scores for infographic and slide generation. Baseline 78.18 + Infographics-6.5K 83.69 + Infographics-65K 86.47 + Infographics-650K 92.98 92.98 92.98 68.83 75.68 79.04 84.25 84.25 84. 51.73 55.30 58.48 66.42 66.42 66.42 34.46 61.38 46.96 90.00 48.24 94.71 95.12 55.48 95.12 55.48 95.12 55.48 55.85 84.10 89.38 90.39 90.39 90.39 44.33 78.47 82.33 88.34 88.34 88.34 48.01 83.32 86.07 91.54 91.54 91.54 5.98 7.03 7.20 7.747.747. 5.39 7.42 7.78 8.098.098.09 Table 3. Scaling the infographics dataset improves performance. Method Text Spelling Precision (%) 10 layers 10-15 layers 15-20 layers Region-wise CA Layout-Guided CA 84.91 86.47 86.47 86.47 77.28 79.04 79.04 79.04 57.18 58.48 58.48 58.48 20 layers 50.56 50.56 50.56 48. GTP-4o Score (1-10) Aesthetics Prompt Fo. 6.82 7.207.207.20 6.99 7.787.787.78 Table 4. Layout-guided cross attention improves performance. Resolution Text Spelling Precision (%) Non-text Layer Precision/LGSR (%) GTP-4o Score (1-10) 10 L. 10-15 L. 15-20 L. 20 L. 10 L. 10-20 L. 20-30 L. 30 L. Aesthetics Prompt Fo. 1120 448 44.68 1600 640 76.07 86.47 86.47 2240 896 86.47 32.47 53.47 79.04 79.04 79.04 30.55 38.26 58.48 58.48 58. 24.39 86.46 29.64 87.50 94.71 48.24 94.71 48.24 94.71 48.24 83.21 86.37 89.38 89.38 89.38 75.17 80.56 82.33 82.33 82.33 77.11 76.62 86.07 86.07 86.07 6.56 6.51 7.207.207.20 6.73 7.05 7.787.787. Table 5. High-resolution is critical for the improvements. OCR: 100% OCR: 100% OCR: 100% OCR: 100% OCR: 100% OCR: 93% OCR: 83% OCR: 81% OCR: 91% Figure 11. Qualitative results of generated slides. Each row displays three selected pages of the generated slides. Figure 12. LCFG removes layer artifacts: the image on the left is generated with global CFG of 7, with some local artifacts marked by red boxes. By adjusting the CFG values of different layers, these artifacts are eliminated in the second image. efits of applying layout-guided cross attention compared to the original region-wise cross attention used in GlyphByT5 [22, 23]. According to the results, we find that our layout-guided cross attention primarily improves visual aesthetics and prompt-following capability, while also ensuring better visual text spelling accuracy when the number of layers are less than 20. High Resolution Matters Table 5 summarizes the results of choosing different resolutions, including 1120448, 1600 640, and 2240 896, which correspond to approximately 0.5, 1, and 2 the size of the universal 1024 1024 image resolution. According to the results, we find that high resolution is critical for improving visual text spelling precision, as there can be many small-sized visual texts in an infographic. An interesting observation is that finetuning with lower resolution can actually hurt performance compared to the baseline reported in Table 3. Layout Conditional Classifier-free Guidance Figure 12 illustrates the benefits of applying our LCFG to refine the artifacts in specific layers. For example, we observe that choosing the CFG values as 3, 1.5, 1.5, 3, 3 and 5 for layer 1, layer 2, layer 3, layer 4, layer 5, and layer 7 helps remove the observable artifacts. We also visualize the final layoutconditional CFG mask on the right. We set the timesteprelated hyperparameter α as 0.5 in this case. More Experiments We provide additional experiments and discussion on the support for multilingual infographic and slide generation, the application of our Infographics-650K to train multi-layer transparent infographic generation models, and more ablation experiments in the supplementary material. 5. Conclusion This paper presents novel generation framework, BizGen, to tackle the challenging task of business content generation with ultra-dense layouts and region-wise prompts. By leveraging our scalable infographics generation engine and the layout-guided cross-attention scheme, our system addresses two fundamental challenges: data scarcity and extremely long context length. We empirically demonstrate that our approach can generate high-quality multilingual and multistyle infographics and slides. Additionally, we show that BizGen outperforms DALLE3, SD3, and FLUX by significant margin on our BizEval benchmark."
        },
        {
            "title": "References",
            "content": "[1] Omri Avrahami, Thomas Hayes, Oran Gafni, Sonal Gupta, Yaniv Taigman, Devi Parikh, Dani Lischinski, Ohad Fried, and Xi Yin. Spatext: Spatio-textual representation for controllable image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1837018380, 2023. 3 [2] Haoxing Chen, Zhuoer Xu, Zhangxuan Gu, Jun Lan, Xing Zheng, Yaohui Li, Changhua Meng, Huijia Zhu, and Weiqiang Wang. Diffute: Universal text editing diffusion model. In Thirty-seventh Conference on Neural Information Processing Systems (NeurIPS), 2023. 1 [3] Jingye Chen, Yupan Huang, Tengchao Lv, Lei Cui, Qifeng Chen, and Furu Wei. Textdiffuser-2: Unleashing the power of language models for text rendering. In European Conference on Computer Vision, pages 386402. Springer, 2024. 1, 3 [4] Jingye Chen, Yupan Huang, Tengchao Lv, Lei Cui, Qifeng Chen, and Furu Wei. Textdiffuser: Diffusion models as text painters. Advances in Neural Information Processing Systems, 36, 2024. 3 [5] Minghao Chen, Iro Laina, and Andrea Vedaldi. Training-free layout control with cross-attention guidance. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 53435353, 2024. 3 [6] Yutao Cheng, Zhao Zhang, Maoke Yang, Hui Nie, Chunyuan Li, Xinglong Wu, and Jie Shao. Graphic design with large multimodal model. arXiv preprint arXiv:2404.14368, 2024. 1 [7] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. 3, [8] Naoto Inoue, Kento Masui, Wataru Shimoda, and Kota Yamaguchi. OpenCOLE: Towards Reproducible Automatic Graphic Design Generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), 2024. 1 [9] Jiabao Ji, Guanhua Zhang, Zhaowen Wang, Bairu Hou, Zhifei Zhang, Brian Price, and Shiyu Chang. Improving diffusion models for scene text editing with dual encoders. arXiv preprint arXiv:2304.05568, 2023. [10] Peidong Jia, Chenxuan Li, Zeyu Liu, Yichao Shen, Xingru Chen, Yuhui Yuan, Yinglin Zheng, Dong Chen, Ji Li, Xiaodong Xie, et al. Cole: hierarchical generation framework for graphic design. arXiv preprint arXiv:2311.16974, 2023. [11] Kotaro Kikuchi, Naoto Inoue, Mayu Otani, Edgar SimoSerra, and Kota Yamaguchi. Multimodal markup document models for graphic design completion. arXiv preprint arXiv:2409.19051, 2024. 1 [12] Black Forest Labs. https : //blackforestlabs.ai/announcing-black-forestlabs/, 2024. 3, 7 Flux.1 model family. Towards accurate training-free glyph-enhanced image generation. ArXiv, abs/2403.16422, 2024. 3 [14] Wenbo Li, Guohao Li, Zhibin Lan, Xue Xu, Wanru Zhuang, Jiachen Liu, Xinyan Xiao, and Jinsong Su. Empowering backbone models for visual text generation with input granularity control and glyph-aware training. In Conference on Empirical Methods in Natural Language Processing, 2024. 3 [15] Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jianwei Yang, Jianfeng Gao, Chunyuan Li, and Yong Jae Lee. Gligen: Open-set grounded text-to-image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2251122521, 2023. 3 [16] Long Lian, Boyi Li, Adam Yala, and Trevor Darrell. Llmgrounded diffusion: Enhancing prompt understanding of textto-image diffusion models with large language models. arXiv preprint arXiv:2305.13655, 2023. 3 [17] Zhanhao Liang, Yuhui Yuan, Shuyang Gu, Bohan Chen, Tiankai Hang, Ji Li, and Liang Zheng. Step-aware preference optimization: Aligning preference with denoising performance at each step. arXiv preprint arXiv:2406.04314, 2024. 4, 6 [18] Kevin Lin, Zhengyuan Yang, Linjie Li, Jianfeng Wang, and Lijuan Wang. Designbench: Exploring and benchmarking dall-e 3 for imagining visual design. arXiv preprint arXiv:2310.15144, 2023. [19] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning, 2023. 4 [20] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2629626306, 2024. 4 [21] Rosanne Liu, Daniel Garrette, Chitwan Saharia, William Chan, Adam Roberts, Sharan Narang, Irina Blok, R. J. Mical, Mohammad Norouzi, and Noah Constant. Character-aware models improve visual text rendering. In Annual Meeting of the Association for Computational Linguistics, 2022. 3 [22] Zeyu Liu, Weicong Liang, Zhanhao Liang, Chong Luo, Ji Li, Gao Huang, and Yuhui Yuan. Glyph-byt5: customized text encoder for accurate visual text rendering. In European Conference on Computer Vision, pages 361377. Springer, 2024. 2, 3, 5, 6, 8 [23] Zeyu Liu, Weicong Liang, Yiming Zhao, Bohan Chen, Ji Li, and Yuhui Yuan. Glyph-byt5-v2: strong aesthetic baseline for accurate multilingual visual text rendering. arXiv preprint arXiv:2406.10208, 2024. 3, 5, 6, 8 [24] Jian Ma, Mingjun Zhao, Chen Chen, Ruichen Wang, Di Niu, Haonan Lu, and Xiaodong Lin. Glyphdraw: Learning to draw chinese characters in image synthesis models coherently. arXiv preprint arXiv:2303.17870, 2023. 1, 3 [25] Chong Mou, Xintao Wang, Liangbin Xie, Yanze Wu, Jian Zhang, Zhongang Qi, and Ying Shan. T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 42964304, 2024. [13] Sanyam Lakhanpal, Shivang Chopra, Vinija Jain, Aman Chadha, and Man Luo. Refining text-to-image generation: [26] Shubham Paliwal, Arushi Jain, Monika Sharma, Vikram Jamwal, and Lovekesh Vig. Customtext: Customized tex- [39] Abhay Zala, Han Lin, Jaemin Cho, and Mohit Bansal. Diagrammergpt: Generating open-domain, open-platform diagrams via llm planning. In COLM, 2024. 3 [40] Lvmin Zhang and Maneesh Agrawala. Transparent image arXiv preprint layer diffusion using latent transparency. arXiv:2402.17113, 2024. 4 [41] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 38363847, 2023. [42] Yiming Zhao and Zhouhui Lian. Udifftext: unified framework for high-quality text synthesis in arbitrary images via character-aware diffusion models. In European Conference on Computer Vision, pages 217233. Springer, 2024. 1, 3 [43] Guangcong Zheng, Xianpan Zhou, Xuewei Li, Zhongang Qi, Ying Shan, and Xi Li. Layoutdiffusion: Controllable diffusion model for layout-to-image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2249022499, 2023. 3 [44] Yuanzhi Zhu, Jiawei Liu, Feiyu Gao, Wenyu Liu, Xinggang Wang, Peng Wang, Fei Huang, Cong Yao, and Zhibo Yang. Visual text generation in the wild. In European Conference on Computer Vision, pages 89106. Springer, 2024. 3 tual image generation using diffusion models. arXiv preprint arXiv:2405.12531, 2024. 3 [27] Quynh Phung, Songwei Ge, and Jia-Bin Huang. Grounded text-to-image synthesis with attention refocusing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 79327942, 2024. 3 [28] Yifan Pu, Yiming Zhao, Zhicong Tang, Ruihong Yin, Haoxing Ye, Yuhui Yuan, Dong Chen, Jianmin Bao, Sirui Zhang, Yanbin Wang, Lin Liang, Lijuan Wang, Ji Li, Xiu Li, Zhouhui Lian, Gao Huang, and Baining Guo. Art: Anonymous region transformer for variable multi-layer transparent image generation. arXiv preprint arXiv:2502.18364, 2025. 13 [29] Leigang Qu, Shengqiong Wu, Hao Fei, Liqiang Nie, and TatSeng Chua. Layoutllm-t2i: Eliciting layout guidance from llm for text-to-image generation. In Proceedings of the 31st ACM International Conference on Multimedia, pages 643 654, 2023. [30] Yuxiang Tuo, Wangmeng Xiang, Jun-Yan He, Yifeng Geng, and Xuansong Xie. Anytext: Multilingual visual text generation and editing. ArXiv, abs/2311.03054, 2023. 3 [31] Xudong Wang, Trevor Darrell, Sai Saketh Rambhatla, RoInstancediffusion: Instancehit Girdhar, and Ishan Misra. level control for image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 62326242, 2024. 3 [32] Jinheng Xie, Yuexiang Li, Yawen Huang, Haozhe Liu, Wentian Zhang, Yefeng Zheng, and Mike Zheng Shou. Boxdiff: Text-to-image synthesis with training-free box-constrained diffusion. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 74527461, 2023. 3 [33] Jianwei Yang, Hao Zhang, Feng Li, Xueyan Zou, Chunyuan Li, and Jianfeng Gao. Set-of-mark prompting unleashes extraordinary visual grounding in gpt-4v. arXiv preprint arXiv:2310.11441, 2023. 7 [34] Ling Yang, Zhaochen Yu, Chenlin Meng, Minkai Xu, Stefano Ermon, and CUI Bin. Mastering text-to-image diffusion: Recaptioning, planning, and generating with multimodal llms. In Forty-first International Conference on Machine Learning, 2024. 3 [35] Yukang Yang, Dongnan Gui, Yuhui Yuan, Haisong Ding, Han Hu, and Kai Chen. Glyphcontrol: Glyph conditional control for visual text generation, 2023. 1 [36] Yukang Yang, Dongnan Gui, Yuhui Yuan, Weicong Liang, Haisong Ding, Han Hu, and Kai Chen. Glyphcontrol: Glyph conditional control for visual text generation. Advances in Neural Information Processing Systems, 36, 2024. [37] Zhengyuan Yang, Jianfeng Wang, Zhe Gan, Linjie Li, Kevin Lin, Chenfei Wu, Nan Duan, Zicheng Liu, Ce Liu, Michael Zeng, et al. Reco: Region-controlled text-to-image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1424614255, 2023. 3 [38] Zhengyuan Yang, Jianfeng Wang, Linjie Li, Kevin Lin, Chung-Ching Lin, Zicheng Liu, and Lijuan Wang. Idea2img: Iterative self-refinement with gpt-4v for automatic image design and generation. In European Conference on Computer Vision, pages 167184. Springer, 2024. 1 BizGen: Advancing Article-level Visual Text Rendering for Infographics Generation"
        },
        {
            "title": "Appendix",
            "content": "A. Dataset Statistics of Slides-500K We present statistics on the number of text layers, non-text layers, total layers, characters, and pages across the entire Slides-500K dataset in Figure 16. Each set consists of multiple slide pages with the same style. Compared to Figure 7 in the main paper, we find that single-page slide typically has sparser layout than an infographic; however, the total number of layers accumulated across set of slides can far exceed that of an infographic. B. Training Settings Table 7 details the training hyperparameter choices for our infographic generation model and slides generation model, respectively. We use smaller glyph loss weight for infographics to avoid artifacts, such as shadows, after extensive tuning on data with denser layouts. C. Layer-wise Retrieval-Augmented Infographics Data Engine Dominant Layer Selection. We present the GPT-4o prompt used to identify the main layers in our originally collected infographic data. These layers will be replaced by generated high-quality transparent layers in our data engine. Select dominant object layer Prompt for GPT-4o You are an experienced visual designer. You are given 1+N images. The first one is the whole image of an infographic. Assume the infographic has layers. The following images are different layers of the first image. The caption of the layer images are also given. You are required to classify the layer images into two categories: Main element or Others. Main element means the layer should contain at least one specific object, the object(s) should be intact in the layer, and the object(s) should be aesthetically beautiful and the layer shouldnt be too small. Others include solid-colored blocks, decoration layers and any other layers that do not meet the requirements of Main element. Your answer should be in list format, containing only Others or Main element and nothing else. The length of the returned list should be equal to N. Filtering of the Generated Transparent Layers. Figure 20 presents examples of both good and bad layers generated with our scalable data engine. The bad layers are filtered out because: (i) objects fill the entire canvas (1st and 2nd in the second row), or (ii) the image has solid-colored background with non-zero alpha channel (3rd, 4th, 5th, and 6th in the second row). The remaining layers after the filtering are transparent and of high aesthetic quality. Discussion on directly assembling through data engine v.s. generating through BizGen.Since we already conFigure 13. Comparison of assembling through data engine against BizGen Method DataEngine BizGen Infographics Aesthetics Prompt Fo. Aesthetics 6.33 7.747.747.74 6.50 8.098.098.09 7.00 7.647.647.64 Slides Prompt Fo. 6.87 7.617.617.61 Style Cons. 7.73 8.378.378.37 Table 6. Comparison with the assembled results produced by data engine. struct the automatic infographic data engine, it seems like we can directly assemble infographics through layer-wise retrieval and text rendering, which ensures 100% OCR accuracy. However, to generate infographics with high diversity, such approach requires retrieving all the non-text layers (different from just doing augmentation where we only select dominant layers), and assembling them. The assembled results lag behind BizGen in terms of limited template styles, poorer aesthetics, and weaker prompt following. We visualize the quantitative comparison results in Table 6 and qualitative comparison results in Figure 13. We argue that the poor performance of the assembled infographics lies in the complex relevance between text and non-text layers within an image. First, the explanatory text is usually spatially adjacent to the corresponding non-text elements, enhancing the mapping between visuals and semantics. Second, rather than relying on post-hoc combinations of predefined transparent layers from database, BizGen jointly generates both elements end-to-end within unified generative model ensures intrinsic contextual alignment. Third, texts can have spatially overlapped substrate layers, requiring spatial alignment and contrast in colors, as illustrated in Figure 13. Hyperparameter Backbone UNet Learning-rate Batch Size Epochs Weight Decay Text-Encoder Dropout Gradient Clipping Resolution UNet LoRA Rank Text Feature Length Glyph Loss Weight Dataset BizGen-Infographics Glyph-SDXL 1.00E-04 128 5 0.01 0.1 1.0 [2240,896] 128 2048 1 Infographics-650K BizGen-Slides Glyph-SDXL 1.00E-04 128 10 0.01 0.1 1.0 [864,1536] 128 2048 5 Slides-500K Table 7. BizGen Training hyper-parameter choices. Figure 14. Input and output examples of BizGen Infographics Global Visual Quality Assurance Prompt for GPT-4o You are an autonomous AI Assistant who aids designers by providing insightful, objective, and constructive critiques of graphic design projects. Your goals are: Deliver comprehensive and unbiased evaluations of graphic designs based on established design principles and industry standards. Maintain consistent and high standard of critique. You will be given an image of graphic design (poster, infographic, slide, etc.) and the caption that describes the content in the image. Please abide by the following rules: Strive to score as objectively as possible. Grade seriously. flawless design can earn 10 points, mediocre design can only earn 7 points, design with obvious shortcomings can only earn 4 points, and very poor design can only earn 1-2 points. Keep your reasoning concise when rating, and describe it as briefly as possible. If the output is too long, it will be truncated. Only respond in JSON format, no other information. Grading criteria: Aesthetics (1-10): The graphic design should be aesthetically beautiful, with pleasing color palette, harmonious composition, and appropriate use of space. There should not be artifacts, visual noise, blurriness, wrong words, incomplete text, or any other issues that detract from the overall visual experience. score of 10 indicates design that is visually stunning, while score of 1 indicates design that is visually unappealing. Prompt Following (1-10): The image should be well aligned with the prompt given. Every object mentioned in the prompt should be present in the image. The objects attribute such as color, size, and position should also match the description in the prompt if assigned. If the prompt mentions text in the image, it should be present and legible, and the texts should also match the logical flow of the prompt. The overall style of the image should also align with the tone of the prompt. score of 10 indicates perfect alignment, while score of 1 indicates complete mismatch. Your answer should be dictionary with two keys: Aesthetics and Prompt Following, each containing two keys: score and reason. The score should be an integer between 1 and 10, and the reason should be string explaining the score. D. Model Architecture Figure 14 shows an example of the imput layout and regional prompts and the output infographic of BizGen. We discuss the architecture difference with Glyph-SDXL as follows: While we adopt Glyph-SDXL as our backbone, the key difference lies in the implementation of the region-wise cross-attention scheme. The original GlyphByT5 computes full attention between all text tokens and visual tokens, using pre-computed attention mask to determine what to generate and where. To deal with far more text and visual tokens in infographic setting, our BizGen explicitly crops the visual tokens based on the layout and partitions the text tokens within each region. Attention is then computed only between each set of cropped visual tokens and the corresponding regional text tokens, which is critical for higher efficiency. Additionally, we introduce layout-conditional CFG scheme during inference to further enhance the results. E. Global Quality Assurance Prompt Slides Global Visual Quality Assurance Prompt for GPT-4o You are an autonomous AI Assistant who aids designers by providing insightful, objective, and constructive critiques of graphic design projects. Your goals are: Deliver comprehensive and unbiased evaluations of graphic designs based on established design principles and industry standards. Maintain consistent and high standard of critique. You will be given set of slides and the caption that describes the content in each of the single page. Your score should be based on the overall quality of the slides, not single page. Please abide by the following rules: Strive to score as objectively as possible. Grade seriously. flawless design can earn 10 points, mediocre design can only earn 7 points, design with obvious shortcomings can only earn 4 points, and very poor design can only earn 1-2 points. Keep your reasoning concise when rating, and describe it as briefly as possible. If the output is too long, it will be truncated. Only respond in JSON format, no other information. Grading criteria: Aesthetics (1-10): The slides should be aesthetically beautiful, with pleasing color palette, harmonious composition, and appropriate use of space. There should not be artifacts, visual noise, blurriness, wrong words, incomplete text, or any other issues that detract from the overall visual experience. score of 10 indicates design that is visually stunning, while score of 1 indicates design that is visually unappealing. Prompt Following (1-10): Every page of the slide should be well aligned with the corresponding prompt given. Every object mentioned in the prompt should be present in the corresponding page. The objects attribute such as color, size, and position should also match the description in the prompt if assigned. If the prompt mentions text in slide, it should be present and legible, and the texts should also match the logical flow of the prompt. The overall style of the slides should also align with the tone of the prompt. score of 10 indicates perfect alignment, while score of 1 indicates complete mismatch. Style Consistency (1-10): The style of different pages in set of slide should be consisitent. The color palette should be harmonious, the styles of elements in different pages should match, and the font type of text in different pages should also be consistent. score of 10 indicates set of slides that are perfectly consistent, while score of 1 indicates set of slides that are completely inconsistent. Your answer should be dictionary with three keys: Aesthetics, Prompt Following and Style Consistency , each containing two keys: score and reason. The score should be an integer between 1 and 10, and the reason should be string explaining the score. Layer-wise Quality Assurance Prompt for GPT-4o You are an objective, strict, cold-blooded, experienced visual art designer. You are given an image and detailed information about it. The information includes: 1. The caption of the full image, which describes the overall content of the image. 2. Two images. The first one is the original image, and the second one is exactly the same, while bounding boxes of the layers and indexes are added to the second one. The layers are given in z-order, from bottom to top, so higher layers may occlude lower layers. Each of the bounding boxes has its index labeled on it, in the same color of the box. 3. The detailed information of each layer in the image, including the caption of the layer and the bounding box of the layer. Bounding boxes are in the format of [top left x, top left y, bottom right x, bottom right y]. The bbox coordinates are normalized to [0,1]. The occlusion relationship between layers is also provided, which can also be told from the second image with bounding boxes. 4. For each layer, two auxiliary information is provided: element type and layer description. element type can either be block or object for non-background layers. block means the layer doesnt contain specific objects, while object means the layer contains specific objects. For block layers, you should focus on the color and texture of the layer. For object layers, you should focus on the specific objects in the layer. layer description (if given) is description of the content of the target layer without occlusions. You should decide the score based on the two images given, while the layer description is only for your reference. Now for the specified layer, you have to decide whether the region within the bounding box is consistent with the caption of the layer. Your answer should be score ranging from 0 to 10. 0 means the region is completely irrelevant with the caption, while 10 means the region is perfectly aligned with the caption. The text in the image should be neglected. The key point is to determine whether the object described in the caption appear in the region. You should also provide detailed reason for your decision. There are some rules for your decision: (1). You should consider only the bounded region and the caption of the layer. (2). If the caption of the layer is too vague to determine whether the object appears in the region, you should give low score. (3). Most attention should be paid to the main object in the caption. If the main object is missing in the region, you should give very low score. The main object must be clearly visible, different from the background. This is the most important rule and you must always follow it. If the main object is missing, or the region has no difference from the background, zero score. Specifically, for object layers, you should focus on the specific objects in the layer. You can also refer to the layer description to judge whether its consistent with the caption. (4). If the caption specifies multiple objects, all of them should appear in the region. If some objects are missing, you should deduct points accordingly. (5). If the caption specifies specific attribute of the object, the attribute should be visible in the region and consistent with the caption. For example, if the caption specifies the color of the object, the object should be in the specified color, or you should give low score. Specifically, for block layers, you only have to consider the color and texture of the layer, and layer description only describes the color. If the color is close to the caption, such as beige compared to white, you shouldnt give low score. If the caption described that the main object is on colored back- (6). ground, the mismatch of color of the background could be tolerated. But if the caption only describes the color of the object or the background, the object or background should be in the specified color. (7). Remember the higher layers may occlude lower layers, so you should consider the occlusion relationship between layers. Specifically, higher layer occlude lower layer, and something not described in the caption of the lower layer appears in the region, this should be tolerated and you shouldnt deduct scores because of such reasons. Only layers with larger index occlude lower layers. To judge occlusion, you have to refer to the second image with bounding boxes, and also the detailed bounding box information of each layer if necessary. (8). Text in the image region should be neglected. If the caption specifies no text in the region but there is text, you dont have to deduct points. (9). If there are artifacts or implausible objects in the region, you should deduct points accordingly. (10). You have to be strict and objective in your evaluation and shouldnt easily give very high score unless the bounded region is consistent with the caption. Important: Your answer should be in dict format following the example given below, and no other answer needed. We design Global Quality Assurance Prompt for GPT4o to evaluate the overall quality of generated business content from two aspects: visual aesthetics and adherence to the global prompt. Additionally, style consistency is assessed across different pages in slide set. The detailed system prompts are shown in the brown and sky blue blocks, respectively. F. Details about Layer Generation Success Rate We demonstrate the scoring prompt in our LGSR assessment pipeline in the section titled Layer-wise Quality Assurance Prompt for GPT-4o. It consists of two parts: (i) an introduction to the information and images provided, and (ii) rules to follow when scoring the layers in the generated images. We further provide detailed example in Figure 22 to give clearer picture of how our LGSR assessment pipeline works and what information is required at each step. G. Multi-Layer Transparent Infographic Generation As our Infographics-650K essentially consists of multiple transparent layers, we follow [28] and train multi-layer generation model on Infographics-650K and present representative result in Figure 15, which consists of more than 20 transparent layers, including various visual element layers and visual text layers. We believe our Infographics-650K will be valuable contribution not only to infographic image generation but also to the multi-layer transparent infographic generation task. H. Multilingual Infographics and Slides Generation Our BizGen model can render visual text in ten different languages, including English, German, Spanish, French, Italian, Portuguese, Russian, Chinese, Japanese, and Korean, thanks to our multilingual datasets Infographics-650K and Slides-500K. Table 8 demonstrates the visual text spelling precision of the generated multilingual infographics and slides, while Figures 18 and 19 visualize some qualitative results. We find that our BizGen can achieve approximately 90% visual text spelling precision in all ten languages, except in cases with dense, tiny visual texts, which slightly drag down the overall performance. We do not compare our multilingual generation results with state-of-the-art methods such as FLUX, as these approaches are primarily trained on English data. I. Multi-Style Infographics Generation As mentioned in Section 3.2, four different LoRAs are used to generate diverse transparent layers for the retrieval database in our infographic data engine. During training, Comp. #0 #1 #2 #3 #4 # #6 #7 #8 #9 #10 # #12 #13 #14 #15 #16 # #18 #19 Figure 15. Multi-layer Transparent Infographics Generation Results. Table 9. For typesetting convenience, we only list the global prompts and all the text layer prompts. Language English French Spanish German Portuguese Italian Russian Chinese Japanese Korean Infographics Visual Text Spelling Precision (%) Slides Visual Text Spelling Precision (%) 10 layers 10-15 layers 15-20 layers 20 layers 10 layers 10-20 layers 20-30 layers 30 layers 92.98 89.68 87.31 84.11 89.15 87.23 83.10 92.50 94.07 91.94 84.25 81.02 81.68 73.99 80.74 80.40 69.11 91.71 93.05 85.57 66.42 60.07 62.37 54.24 65.73 64.12 52.11 85.43 91.79 80. 55.48 50.97 51.89 44.34 50.58 53.70 42.59 69.18 87.36 74.94 88.13 88.91 84.38 87.06 87.40 93.53 83.04 92.56 95.68 92.1 88.41 85.32 83.17 82.11 88.69 88.41 81.17 93.22 94.32 94.02 80.29 78.86 74.19 73.80 77.04 77.43 70.16 89.39 95.65 89.90 66.81 60.71 60.38 52.91 63.07 60.87 49.41 85.68 90.46 83.51 Table 8. Illustrating the visual text spelling precision of the multilingual infographics and slides generation results. we add the corresponding trigger words to the prompts of the replaced layers, allowing the model to learn the style information. This approach enables us to customize our generation by simply adding trigger words to different layer prompts during inference, resulting in aesthetically pleasing infographics in various styles, as shown in Figure 21. J. Comparison with State-of-the-art on Slides"
        },
        {
            "title": "Generation",
            "content": "In Tables 1 and 2, we have already shown that BizGen outperforms state-of-the-art methods in almost every metric for slide generation. Figure 17 provides some qualitative comparison results. An interesting observation is that state-ofthe-art methods such as FLUX may sometimes generate only background instead of reasonable slide page (columns 2, 5, and 6), which is likely related to their training data. K. Ablation on Layout Conditional CFG"
        },
        {
            "title": "Timestep Range",
            "content": "We demonstrate the effect of choosing different α values for layout conditional CFG in Figure 23. Here, α is the hyperparameter that controls the starting timestep at which layout conditional CFG takes effect in the denoising process, as introduced in Section 3.3. Our goal is to eliminate local artifacts in specific regions without deteriorating others; therefore, we assume that LCFG should be used when is close to 0, given that details are settled in the later stages of the diffusion process. Thus, we only alter the starting timestep and apply LCFG toward the end. From Figure 23 we can see that small α may bring too little changes that are not enough to remove the flaws, while big α can bring unwanted changes to other regions. Taking the first row as an example, when α is 0.1 or 0.2, the shadows and irregular color still exist; when α is 0.8 or 0.9, the navy blue block and the light blue rings begin to fade. To strike the balance, we empirically find that 0.5 is the optimal choice for α in most cases. L. Detailed Prompt List We illustrate the detailed prompts for generated infographics and slides shown in Figure 1, Figure 10 and Figure 11 in (a) (b) (c) (d) (e) (f) (g) (h) Figure 16. Illustrating the statistics of our Slides-500K:(a)# of text layers/set, (b)# of non-text layers/set, (c) # of total layers/set, (d) # of pages/set, (e)# of text layers/page, (f)# of non-text layers/page, (g) # of total layers/page, (h) # of chars/text layer. We mark the median values with red dashed lines. Figure 17. Qualitative comparison results of slides generation with SOTAs. The 1st, 2nd, 3rd, and 4th rows correspond to the results generated with our BizGen, FLUX, SD3 Large, and DALLE3. The left three columns are in the same set, while the right three columns are in another. Chinese Japanese Korean German Spanish French Italian Portuguese Russian Figure 18. Qualitative results of multilingual infographics generation. Figure 19. Qualitative results of multilingual slides generation.We show the Chinese, Japanese, Korean, German, Spanish, French, Italian, Portuguese, and Russian visual text results in the nine rows subsequently. Figure 20. Illustrating the transparent layers generated in the data engine: The first row shows examples of the generated multi-style highquality transparent layers, while the second row demonstrates the filtered ones. Chinese Comic Illustration Minimalism Figure 21. Qualitative results of multi-style infographics generation. The four columns respectively attribute to four different styles: Chinese, Comic, Illustration and Minimalism. Figure 22. Example of LGSR Assessment Pipeline: We give the examples of two layers go through our LGSR assessment pipeline, which cover the two layer types: object and block. We demonstrate the different information fed to GPT-4o in every step, and highlight the response by GPT-4o, including its explanation. Figure 23. Effect of different choices of α for LCFG: We use red boxes to mark the artifacts in the images generated without layout conditional CFG and use green boxes to highlight the most aesthetically flawless images across all the others generated with different α."
        },
        {
            "title": "Image",
            "content": "Fig 1, Col1 Fig 1, Col2 Fig 1, Col3 Fig 1, Col4 Prompt Global: The image is digital graphic with blue background and yellow border. At the top, in large white letters, the text reads Business Agency. Below this title, there are three sections, each with white background and yellow dot, containing text and icons.The first section is titled Services Offered and includes brief description: Our comprehensive range of services is designed to address your diverse needs and support your business growth.The second section is titled Team Expertise and highlights: bring diverse range of expertise to the table. We are passionate about collaborating with you to achieve your goals.The third section is titled Successful Projects and mentions: Our commitment to excellence and innovative problem-solving shines through in every endeavor.At the bottom of the image, there is stylized illustration depicting group of people engaged in meeting or presentation. The individuals are shown with various expressions and postures, suggesting dynamic and collaborative environment. The central figure is gesturing towards chart or graph, likely representing data or progress. The overall style of the image is clean, modern, and professional, with focus on conveying the agencys capabilities and values. Text: Text Business in <color-0>, <en-font-421>. Text Agency in <color-5>, <en-font-371>. Text Services Offered in <color-5>, <en-font-421>. Text Our comprehensive range of services is designed to address your diverse needs and support your business growth. in <color-0>, <en-font-403>. Text Team Expertise in <color-5>, <en-font-421>. Text bring diverse range of expertise to the table. We are passionate about collaborating with you to achieve your goals. in <color-0>, <en-font-403>. Text Successful Projects in <color-5>, <en-font-421>. Text Our commitment to excellence and innovative problem-solving shines through in every endeavor. in <color-0>, <en-font-403>. Global: The image is an infographic titled Benefits of living with Pet. It features cartoon illustration of person holding cat. The person is smiling and appears to be enjoying the company of the cat. The cat is white with black stripes and is being held gently.Below the illustration, there are four numbered points, each describing different benefit of living with pet. The points are as follows:1. Increase your social interactions. Having friend can make it easier to start up conversation.2. Help with your fitness. Having pet will, of course, help to increase your energy levels and keep you active.3. Help with loneliness. Having pet in our side when were out and about can make us feel less alone.4. Reduce stress and anxiety. Spending time with pet can help boost our mental health.Each point is accompanied by small illustration of cat, reinforcing the theme of the infographic. The cats in the illustrations are depicted in various poses and colors, adding visual interest to the text. The overall style of the image is friendly and informative, aimed at highlighting the positive effects of pet ownership on social, physical, emotional, and mental well-being. Text: Text Having friend can make it easier to start up conversation. in <color-1>, <en-font-0>. Text Increase your social interactions in <color-1>, <en-font-316>. Text Having pet will, of course, help to increase your energy levels and keep you active. in <color-1>, <en-font-0>. Text 1 in <color-0>, <en-font-316>. Text Benefits of living with in <color-1>, <en-font-316>. Text Pet in <color-1>, <en-font-316>. Text 2 in <color-0>, <en-font-316>. Text 3 in <color-0>, <en-font-316>. Text 4 in <color-0>, <en-font-316>. Text Help with your fitness in <color-1>, <en-font-316>. Text Having pet in our side when were out and about can make us feel less alone. in <color-1>, <en-font-0>. Text Help with loneliness in <color-1>, <en-font-316>. Text Spending time with pet can help boost our mental health. in <color-1>, <en-font-0>. Text Reduce stress and anxiety in <color-1>, <en-font-316>. Global: The image is graphic with blue background and white text, divided into four sections, each with different title and accompanying illustration. The title at the top reads Tax Planning Strategies in bold, capital letters.The first section is titled Understand Your Tax Bracket and features an illustration of target with an arrow pointing at it. The text below the title explains By understanding which brackets you fall into, you can make informed decisions about income deferral or acceleration.The second section is titled Maximize Tax-Advantaged Accounts and shows an illustration of computer monitor displaying graph with two lines, one in blue and the other in yellow. The accompanying text explains These accounts offer tax deductions, tax-free growth, or tax-free withdrawals for specific purposes.The third section is titled Tax Loss Harvesting and depicts hand holding coin with dollar sign on it. The text below the title explains Offset capital gains by strategically selling investments that have experienced losses.The fourth and final section is titled Plan for Charitable Giving and features an illustration of two hands shaking, one holding blue envelope and the other holding yellow envelope. The text below the title suggests Charitable donations can be tax-deductible, so consider making strategic contributions to causes you care about.Each section is separated by horizontal line, and the overall style of the image is informational and educational, designed to provide advice on tax planning strategies. Text: Text Tax Planning Strategies in <color-0>, <en-font-78>. Text By understanding which brackets you fall into, you can make informed decisions about income deferral or acceleration. in <color-1>, <en-font-15>. Text Understand Your Tax Bracket in <color-0>, <en-font-15>. Text These accounts offer tax deductions, tax-free growth, or tax-free withdrawals for specific purposes. in <color-1>, <en-font-15>. Text Maximize Tax-Advantaged Accounts in <color-0>, <en-font-15>. Text Offset capital gains by strategically selling investments that have experienced losses. in <color-1>, <en-font-15>. Text Tax Loss Harvesting in <color-0>, <en-font-15>. Text Charitable donations can be tax-deductible, so consider making strategic contributions to causes you care about. in <color-1>, <en-font-15>. Text Plan for Charitable Giving in <color-0>, <en-font-15>. Global: The image is graphic with warm, earthy color palette, primarily in shades of brown and beige. It features series of circular icons with illustrations of dog, each accompanied by text that provides tips on how to train dog. The tips are numbered from 1 to 5 and are presented in clear, instructional manner.The first tip, 1. One Goal at Time, suggests focusing on one behavior at time during training, explaining Potty training, Play dead oe Shake hand whatever it is start out with only one goal in mind. The second tip, 2. Use Short Cue Word, advises Use short cue word while training the dog, long sentence only lead to confusion. The third tip, 3. Use Treats, emphasizes Treats motivate the dog to do the correct action and reward treats for every good behavior. The fourth tip, 4. Practice, Practice, and Practice, highlights that Repetition is key in perfecting dog trict keep repeating it until your get hand of it. The fifth tip, 5. Take it Slow, suggests After practice, give youe dog the much needed rest it deserves, to make him remember the trick.The text is written in simple, sans-serif font, and the overall style of the image is clean and modern. The dog in the illustrations is Corgi, characterized by its short legs and long body, and is depicted in various poses that correspond to the training tips. The background is plain and does not distract from the content. The image includes the URL www.yourwebsite.com at the bottom and is titled How to Train Your Dog? Text: Text 1. One Goal at Time in <color-3>, <en-font-458>. Text Potty training, Play dead oe Shake hand whatever it is start out with only one goal in mind. in <color-3>, <en-font-0>. Text 2. Use Short Cue Word in <color-3>, <en-font-458>. Text Use short cue word while training the dog, long sentence only lead to confusion. in <color-3>, <en-font-0>. Text 3. Use Treats in <color-3>, <en-font-458>. Text Treats motivate the dog to do the correct action and reward treats for every good behavior. in <color-3>, <en-font-0>. Text 4. Practice, Practice, and Practice in <color-3>, <en-font-458>. Text Repetition is key in perfecting dog trict keep repeating it until your get hand of it. in <color-3>, <en-font-0>. Text 5. Take it Slow in <color-3>, <en-font-458>. Text After practice, give youe dog the much needed rest it deserves, to make him remember the trick. in <color-3>, <en-font-0>. Text www.yourwebsite.com in <color-86>, <en-font-458>. Text How to Train Your Dog? in <color-86>, <en-font-458>. Continued on next page..."
        },
        {
            "title": "Image",
            "content": "Fig 1, Col5 Fig 10, Col1 Fig 10, Col2 Fig 10, Col3 Fig 10, Col4 Prompt Global: The image is an infographic titled MOVING TIPS with light blue background and white border. It features four illustrated tips for moving, each accompanied by brief description. The tips are as follows:1. 1.Create Moving Budget: Before moving into new apartment, create budget to ensure you have enough money saved for the move. Include deposits, rental fees, moving costs, and any necessary furniture.2. 2.Ask About Utilities: Ask your landlord about the utilities that are included in your rent. Make sure you understand which ones are included and which ones youll have to pay for yourself.3. 3.Pack Smart: Be sure to pack everything in the right boxes and label them appropriately. Youll thank yourself when youre unpacking your stuff in the new apartment.4. 4.Make Plan For today: Have plan before moving day. Make sure you have enough help and the right tools to move all of your stuff safely and efficiently.In the background, there are two illustrated characters who appear to be in the process of moving. One is carrying box, while the other is holding plant. They are standing in front of moving truck, which is parked next to stack of boxes.The infographic also includes website address at the bottom: www.reallygoodsite.com.The overall style of the image is clean and modern, with clear focus on providing practical advice for those preparing to move. Text: Text moving in <color-4>, <en-font-247>. Text tips in <color-27>, <en-font-247>. Text Before moving into new apartment, create budget to ensure you have enough money saved for the move. Include deposits, rental fees, moving costs, and any necessary furniture. in <color-1>, <en-font-326>. Text 1.Create Moving Budget in <color-4>, <en-font-247>. Text Ask your landlord about the utilities that are included in your rent. Make sure you understand which ones are included and which ones youll have to pay for yourself. in <color-1>, <en-font-326>. Text 2.Ask About Utilities in <color-4>, <en-font-247>. Text Be sure to pack everything in the right boxes and label them appropriately. Youll thank yourself when youre unpacking your stuff in the new apartment in <color-1>, <en-font-326>. Text 3.Pack Smart in <color-4>, <en-font-247>. Text Have plan before moving day. Make sure you have enough help and the right tools to move all of your stuff safely and efficiently. in <color-1>, <en-font-326>. Text 4.Make Plan For today in <color-4>, <en-font-247>. Text www.reallygoodsite.com in <color-4>, <en-font-247>. Global: The image is digital graphic with dark brown background. At the top, in large white letters, the text reads HOW TO TAME PETS. Below this title, there are four separate sections, each with heart icon and piece of advice on how to tame pets. The first section suggests, Approach the animal calmly and slowly, using gentle body language to convey that you are not threat. The second section advises, Offer the animal treats or food to establish positive association and encourage them to trust you. The third section recommends, Gradually introduce touch and physical contact, starting with gentle strokes or pats, respecting the animals boundaries. The fourth section advises, Consistently spend time with the animal, engaging in activities they enjoy, and providing safe and build bond of trust.At the bottom of the image, there is photograph of small white dog with black nose and dark eyes. The dog appears to be French Bulldog. The dogs expression is joyful, with its mouth open and ears perked up, and it is looking directly at the camera. The photograph is framed by light beige border with thin brown line. Text: Text how to tame pets in <color-81>, <en-font-403>. Text Offer the animal treats or food to establish positive association and encourage them to trust you in <color-2>, <en-font-403>. Text Approach the animal calmly and slowly, using gentle body language to convey that you are not threat in <color-81>, <en-font-403>. Text Consistently spend time with the animal, engaging in activities they enjoy, and providing safe and build bond of trust in <color-81>, <en-font-403>. Text Gradually introduce touch and physical contact, starting with gentle strokes or pats, respecting the animals boundaries in <color-2>, <en-font-403>. Global: The image is graphic representation of consulting process overview. The title CONSULTING PROCESS OVERVIEW is displayed prominently at the top in bold, black letters within bordered box. Below the title, there is subtitle that reads, The Consulting Process Overview provides concise yet comprehensive view of the consulting journey.The infographic is divided into four distinct stages, each represented by different section:1. Discovery and Research: This stage is depicted with an icon of computer monitor and light bulbs, suggesting the process of gathering information and analyzing data.2. Design and Visualization: This stage is illustrated with design symbol, indicating the idea generation and conceptualization phase.3. Review and Refinement: This stage is represented with review icon, implying the process of reviewing and refining the design or concept.4. Finalization and Delivery: This stage is shown with delivery icon, representing the finalization of the project and the delivery of the completed work.Each stage is visually distinguished with different background color and icon, making it easy to follow the sequence of the consulting process. The overall style of the image is clean and modern, with clear and straightforward layout that is easy to understand. Text: Text The Consulting Process Overview provides concise yet comprehensive view of the consulting journey in <color-2>, <en-font-216>. Text Design and Visualization in <color-1>, <en-font-50>. Text Review and Refinement in <color-20>, <en-font-50>. Text Finalization and Delivery in <color-1>, <en-font-50>. Text Consulting Process Overview in <color-1>, <en-font-216>. Text Discovery and Research in <color-20>, <en-font-50>. Global: The image is vibrant infographic that outlines the benefits of pet hotels. Its designed to be visually appealing with pink background and playful, cartoon-like style. The infographic is divided into sections, each highlighting different benefit.At the top, the title Benefits of Pet Hotels is prominently displayed in bold, purple letters. Just below the title, theres cute illustration of cat peeking out from suitcase, symbolizing the pet-friendly nature of the hotels.The first section, titled Professional Care:, features an illustration of cat being groomed by professional. This suggests that pet hotels offer grooming services.The second section, Safety and Security:, shows an illustration of cat in safe, secure environment, possibly indicating that pet hotels provide safe haven for pets.The third section, Exercise and Playtime:, depicts cat playing with toy, implying that pet hotels offer opportunities for pets to engage in play and exercise.The fourth section, Professional Grooming Services:, features another illustration of cat being groomed, reinforcing the point made in the first section.The final section, Peace of Mind for Pet Owners:, shows cat sitting contentedly next to bottle of water, which might symbolize the care and attention given to pets at these hotels, providing pet owners with peace of mind.At the bottom of the infographic, theres website address www.Pethotel.com, suggesting that this is the website for the pet hotels being advertised.Overall, the infographic is colorful and engaging way to present the advantages of pet hotels to potential customers. Text: Text Benefits in <color-83>, <en-font-179>. Text of Pet Hotels in <color-83>, <en-font-179>. Text www.Pethotel.com in <color-83>, <en-font-71>. Text Peace of Mind for Pet Owners: in <color-0>, <en-font-71>. Text Safety and Security: in <color-0>, <en-font-71>. Text Professional Grooming Services: in <color-0>, <en-font-71>. Text Professional Care: in <color-0>, <en-font-71>. Text Exercise and Playtime: in <color-0>, <en-font-71>. Global: The image is vibrant orange poster with white background. It features two illustrations of fluffy brown and white dog, which appears to be corgi, sitting on yellow background. The dog is depicted in relaxed pose, looking directly at the viewer.At the top of the poster, there is bold, white text that reads Lets Give Our Pet New Home!. Below this, there are two pieces of text in smaller font. The first one states Fluffy buddy still needs new adopters to bring them to become human new homie! and the second one says Around 60% of volunteers have already adopted fluffy buddies into their new home!At the bottom of the poster, there is call to action with white text that reads Are you ready to adopt your new homie or give donations? Contact us on: followed by Call:+123 44 55 66 77, Visit:www.yoursite.com, and E-mail:youremail@gmail.com.The overall design of the poster is simple and clear, with focus on the message of adopting pets and the contact information provided. Text: Text Fluffy buddy still needs new adopters to bring them to become human new homie! in <color-14>, <en-font-0>. Text Around 60% of volunteers have already adopted fluffy buddies into their new home! in <color-14>, <en-font-0>. Text Are you ready to adopt your new homie or give donations? Contact us on: in <color-0>, <en-font-0>. Text Call:+123 44 55 66 77 in <color-0>, <en-font-0>. Text Visit:www.yoursite.com in <color-0>, <en-font-0>. Text E-mail:youremail@gmail.com in <color-0>, <en-font-0>. Text Lets Give Our Pet New Home! in <color-0>, <en-font-0>. Continued on next page..."
        },
        {
            "title": "Image",
            "content": "Fig 10, Col5 Fig 11, Row1, Col1 Fig 11, Row1, Col2 Fig 11, Row1, Col3 Fig 11, Row2, Col1 Fig 11, Row2, Col Prompt Global: The image is digital graphic with warm, earthy color scheme, primarily in shades of orange and brown. It features textured background that resembles sandy beach or textured surface. At the top, there is bold title that reads 4 Ways to Create and FINANCIAL PLAN in capital letters, with the word FINANCIAL PLAN being the most prominent.Below the title, there is paragraph of text that explains that Business financial health can be determined by implementing financial plan. Prepare your financial plan by following the steps below.The graphic is divided into four sections, each representing step in creating financial plan. Each section is labeled with number from 1 to 4 and includes circular icon with different color and symbol for each step. The icons are simple and abstract, with no specific objects or characters depicted.The first step, labeled 1, is titled Compare Your Goals and includes an icon with document and magnifying glass, suggesting focus on reviewing or analyzing goals. The description reads: Thinking about what your company wants to accomplish with strategic plan helps. Look at numbers first, then consider what you need to achieve your goals. The second step, 2, is titled Plan Your Finances Goals and features an icon with document and pencil, indicating planning or writing. The description reads: Consider what it will cost to achieve your goals. Include various scenarios. Create range to predict the impact of each optimistic and pessimistic scenario. The third step, 3, is titled Contingency Planning and shows an icon with gears and document, which might imply strategizing or organizing. The description reads: Examine your cash flow statement and assets and formulate plan for when the business encounters crisis or when the cash flow is interrupted. The fourth step, 4, is titled Compare Your Goals again, with an icon that includes document and magnifying glass, similar to the first step. The description reads: Consider what it will cost to achieve your goals. Include various scenarios. Create range to predict the impact of each optimistic and pessimistic scenario.Each step has brief description underneath the title, providing brief explanation of the action to be taken. The text is clear and legible, and the overall layout is clean and organized, designed to be informative and easy to follow. Text: Text 4 Ways to Create in <color-2>, <en-font-400>. Text Financial Plan in <color-1>, <en-font-485>. Text Business financial health can be determined byimplementing financial plan.Prepare your financial plan by following the steps below. in <color-2>, <en-font-342>. Text Compare Your Goals in <color-2>, <en-font-342>. Text 1 in <color-2>, <en-font-342>. Text Thinking about what your company wants to accomplish with strategic plan helps. Look at numbers first, then consider what you need to achieve your goals. in <color-2>, <en-font-342>. Text Plan Your Finances Goals in <color-2>, <en-font-342>. Text 2 in <color-2>, <en-font-342>. Text Consider what it will cost to achieve your goals. Include various scenarios. Create range to predict the impact of each optimistic and pessimistic scenario. in <color-2>, <en-font-342>. Text Contingency Planning in <color-2>, <en-font-342>. Text 3 in <color-2>, <en-font-342>. Text Examine your cash flow statement and assets and formulate plan for when the business encounters crisis or when the cash flow is interrupted. in <color-2>, <en-font-342>. Text Compare Your Goals in <color-2>, <en-font-342>. Text 4 in <color-2>, <en-font-342>. Text Consider what it will cost to achieve your goals. Include various scenarios. Create range to predict the impact of each optimistic and pessimistic scenario. in <color-2>, <en-font-342>. Global: The image shows minimalist concept displayed on wooden surface. There are three framed pieces of artwork or photographs. The first frame is gold, the second is white with brown paper insert. To the right of the frames, there is small potted plant. Below the frames, there are three brown leather-bound books stacked vertically. The overall style of the image is clean and modern, with focus on simplicity and the use of natural materials. The background is plain, light-colored wall, which enhances the minimalist aesthetic. There is circular brown icon on the left side of the image with white line drawing of couch and picture frame, accompanied by the text Minimalist and CONCEPT. Text: Text Minimalist in <color-0>, <en-font-57>. Text Concept in <color-0>, <en-font-0>. Global: The image shows modern bedroom interior with focus on lighting. On the left side of the image, there is bed with white bedspread and beige blanket. The bed is positioned against white brick wall. To the right of the bed, there is small wooden side table with black floor lamp on it. On the right side of the image, there is text overlay that reads Lighting. Below this title, there is suggestion for improving lighting in room: use more natural lighting by installing glass windows and also use less lighting, such as study lamps. The text is set against brown background. At the bottom of the image, there is credit line that says By Company Name. The overall style of the image is clean and minimalistic, with focus on interior design and lighting solutions. Text: Text Lighting in <color-0>, <en-font-57>. Text use more natural lighting by installing glass windows and also use less lighting, such as study lamps in <color-0>, <en-font-0>. Text By Company Name in <color-0>, <en-font-0>. Global: The image depicts room with focus on interior decoration. On the wall, there is large abstract painting with mix of colors, predominantly in shades of beige, brown, and white, which gives it textured appearance. Below the painting, there is wooden desk with few items on it, including small plant in white pot, small figure, some paintbrushes in clear container, and stack of books. To the right of the desk, there is small figure of person, possibly decorative statue or figurine.On the left side of the image, there is text box with the word Decoration in bold, serif font. Below this title, there is brief description that reads, Use abstract painting decorations with minimalist colors and plants and also small figure. At the bottom left, there is text that reads, By Company Name.The overall style of the image is clean and modern, with focus on simplicity and the use of natural materials. The lighting in the room is soft, creating warm and inviting atmosphere. The image appears to be promotional or inspirational graphic, possibly for an interior design company or lifestyle blog. Text: Text Decoration in <color-7>, <en-font-57>. Text Use abstract painting decorations with minimalist colors and plants and also small figure in <color-7>, <en-font-0>. Text By Company Name in <color-7>, <en-font-0>. Global: The image is digital graphic with combination of text and photograph. The text at the top reads COMPANY NAME, followed by Crafting Winning BUSINESS STRATEGY in large, bold font. The photograph shows man standing in an office environment. He is wearing glasses, suit, and tie. Behind him, there is large window that lets in natural light, and through the window, you can see cityscape with buildings and trees. The overall style of the image is professional and corporate, likely intended for business-related context. Text: Text business strategy in <color-98>, <en-font-250>. Text Crafting Winning in <color-98>, <en-font-250>. Text company name in <color-98>, <en-font-37>. Global: The image is slide from presentation, specifically an introduction slide. The slide is divided into two main sections. On the left side, there is photograph of three individuals engaged in business meeting. They are seated around table, with papers and calculator in front of them, suggesting they are discussing financial or strategic matters. The individuals appear to be focused on their work, with one person gesturing towards the papers.The left side of the slide also includes the text COMPANY NAME and INTRODUCTION in large, bold font, accompanied by graphic of yellow sunburst.On the right side of the slide, there is text that serves as an introduction to the topic of the presentation. The text is in sans-serif font and is written in clear, readable font size. The text reads:Greet the audience and introduce the topic of business strategy.Define business strategy as roadmap to achieve long-term objectives.Explain that the presentation aims to explore the key components of an effective business strategy.The text is aligned to the left and is set against light green background, which contrasts with the darker green background of the left side of the slide. The overall layout of the slide is professional and designed to be informative and engaging for the audience. Text: Text introduction in <color-98>, <en-font-250>. Text company name in <color-98>, <en-font-37>. Text Greet the audience and introduce the topic of business strategy. in <color-4>, <en-font-37>. Text Define business strategy as roadmap to achieve long-term objectives. in <color-4>, <en-font-37>. Text Explain that the presentation aims to explore the key components of an effective business strategy. in <color-4>, <en-font-37>. Continued on next page..."
        },
        {
            "title": "Image",
            "content": "Fig 11, Row2, Col3 Fig 11, Row3, Col1 Fig 11, Row3, Col2 Fig 11, Row3, Col3 Prompt Global: The image is composite of two separate sections, each with its own content.On the left side, there is dark green background with light green text. The text at the top reads COMPANY NAME. Below this, in larger font size, is the phrase SECTION BREAK. Underneath the section break title, there is smaller text that says Explain the importance of allocating resources effectively to execute the strategy.On the right side of the image, there is photograph of man sitting at desk. He is wearing glasses and suit, and he appears to be engaged in work, possibly reading or reviewing documents. The desk has laptop, cup that might contain beverage, and some papers or documents. The background of the photograph shows window with daylight coming through, suggesting an office environment. Text: Text section break in <color-98>, <en-font-250>. Text company name in <color-98>, <en-font-37>. Text Explain the importance of allocating resources effectively to execute the strategy. in <color-98>, <en-font-37>. Global: The image features vibrant green vineyard with rows of grapevines stretching into the distance. The sun is low on the horizon, casting warm, golden light over the scene, suggesting either sunrise or sunset. In the foreground, there is graphic design element that resembles book cover or sign. This design includes dark green background with ornate, swirling patterns in lighter green. At the center, there is logo with the text BOURGON in stylized font, and below it, the words Family Vineyard are written in larger, bold font. The phrase in Italy is placed at the bottom of the design. At the very bottom, the website bourgon.it is mentioned. The overall style of the image is blend of real-life photograph of the vineyard with graphic overlay, possibly for promotional or informational purposes. Text: Text BOURGON in <color-0>, <en-font-71>. Text bourgon.it in <color-0>, <en-font-71>. Text FamilyVineyard in <color-0>, <en-font-53>. Text in Italy in <color-35>, <en-font-71>. Global: The image is digital graphic with split layout, featuring photograph on the left and text on the right. The photograph depicts lush green landscape with rows of grapevines. The vines are well-maintained and appear to be in healthy state, with green leaves and visible grape clusters. In the background, there is large, historic building with multiple towers and prominent clock tower, suggesting castle or similar architectural structure. The sky is partly cloudy, and the overall atmosphere of the photograph is serene and picturesque.On the left side of the image, there is dark green section with decorative golden borders. This section includes the text BOURGON, About us, and 01 / 04.On the right side of the image, there is text that provides information about the location and climate of the depicted winery. The text is organized into two sections, with the first section titled Location and the second titled Climate. The text in the Location section reads, The winery is located in southern Italy, region that preserved special rustic charm and unique atmosphere. Wine hills, filled with delicate fragrances, and beautiful scenery over the Mediterranean Sea leave wine lovers with an unforgettable impression. The text in the Climate section reads, The excellent micro-climate of this region and the lime-rich, well-aerated soils provide ideal conditions for producing premium wines. The winemakers produce wide range of wine grape varieties to everyones taste: Pinot Noir, Bonarda, Cabernet Sauvignon, Barbera, Malvasia, Chardonnay, Riesling Italico and Sauvignon Blanc. The text is in serif font, which gives it formal and elegant appearance. The color of the text is dark shade, contrasting with the lighter background, making it easy to read. The overall style of the image suggests it is likely promotional or informational graphic for winery, designed to showcase the beauty of the location and the quality of the wines produced there. Text: Text The excellent micro-climate of this region and the lime-rich,well-aerated soils provide ideal conditions for producing premium wines. The winemakers produce wide range of wine grape varieties to everyones taste: Pinot Noir, Bonarda, Cabernet Sauvignon, Barbera, Malvasia, Chardonnay, Riesling Italico and Sauvignon Blanc. in <color-2>, <en-font-71>. Text The winery is located in southernItaly, region that preserved aspecial rustic charm and unique atmosphere. Wine hills, filled with delicate fragrances, and beautiful scenery over the Mediterranean Sea leave wine lovers with an unforgettable impression. in <color-2>, <en-font-71>. Text Climate in <color-2>, <en-font-53>. Text BOURGON in <color-0>, <en-font-71>. Text Location in <color-2>, <en-font-53>. Text About us in <color-0>, <en-font-53>. Text 01 / 04 in <color-35>, <en-font-53>. Global: The image is digital graphic with split layout. On the left side, there is photograph of vineyard with green grapevines and clusters of green grapes. The vines are densely packed, and the leaves are lush and green, indicating healthy plant.On the right side of the image, there is dark green background with white and gold text. The text is organized into sections with headers such as Distance, Address, and Contact us. Under the Distance header, there are listed distances to various locations such as Airport Mussony 35 km, Railway Station Vulcanum 18 km, Bus Stop 8 Main Rd. 1 km, and Highway 267 Hickory St. 3 km. The Address section provides physical address: 8539 West William Lane (30) 2349 4871, (30) 2349 4710 info@bourgon.it. At the bottom of the right section, the website bourgon.it is listed. The overall style of the image suggests it is promotional or informational graphic, possibly for winery or vineyard, given the context of the vineyard photograph. The design is clean and professional, with clear focus on providing contact and location information. The logo BOURGON is also displayed prominently. The footer shows 04 / 04. Text: Text 8539 West William Lane(30) 2349 4871, (30) 2349 4710info@bourgon.it in <color-0>, <en-font-71>. Text bourgon.it in <color-0>, <en-font-71>. Text AirportRailway Station Bus Stop Highway in <color-0>, <en-font-71>. Text Mussony Vulcanum8 Main Rd.267 Hickory St. in <color-35>, <en-font-71>. Text 35 km18 km1 km3 km in <color-0>, <en-font-71>. Text Address in <color-35>, <en-font-53>. Text BOURGON in <color-0>, <en-font-71>. Text Distance in <color-35>, <en-font-53>. Text Contact us in <color-0>, <en-font-53>. Text 04 / 04 in <color-35>, <en-font-53>. Table 9. Detailed prompt for generated infographics and slides in Figure 1, Figure 10 and Figure 11."
        }
    ],
    "affiliations": [
        "Brown University",
        "Microsoft",
        "Microsoft Research Asia",
        "Tsinghua University",
        "University of Liverpool"
    ]
}