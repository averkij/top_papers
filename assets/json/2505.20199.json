{
    "paper_title": "Adaptive Classifier-Free Guidance via Dynamic Low-Confidence Masking",
    "authors": [
        "Pengxiang Li",
        "Shilin Yan",
        "Joey Tsai",
        "Renrui Zhang",
        "Ruichuan An",
        "Ziyu Guo",
        "Xiaowei Gao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Classifier-Free Guidance (CFG) significantly enhances controllability in generative models by interpolating conditional and unconditional predictions. However, standard CFG often employs a static unconditional input, which can be suboptimal for iterative generation processes where model uncertainty varies dynamically. We introduce Adaptive Classifier-Free Guidance (A-CFG), a novel method that tailors the unconditional input by leveraging the model's instantaneous predictive confidence. At each step of an iterative (masked) diffusion language model, A-CFG identifies tokens in the currently generated sequence for which the model exhibits low confidence. These tokens are temporarily re-masked to create a dynamic, localized unconditional input. This focuses CFG's corrective influence precisely on areas of ambiguity, leading to more effective guidance. We integrate A-CFG into a state-of-the-art masked diffusion language model and demonstrate its efficacy. Experiments on diverse language generation benchmarks show that A-CFG yields substantial improvements over standard CFG, achieving, for instance, a 3.9 point gain on GPQA. Our work highlights the benefit of dynamically adapting guidance mechanisms to model uncertainty in iterative generation."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 2 ] . [ 1 9 9 1 0 2 . 5 0 5 2 : r Adaptive Classifier-Free Guidance via Dynamic Low-Confidence Masking Pengxiang Li1, Shilin Yan2, Joey Tsai3, Renrui Zhang4, Ruichuan An5, Ziyu Guo4, Xiaowei Gao 1PolyU 2FDU 3THU 4CUHK 5PKU 6ICL {2040gis, tattoo.ysl}@gmail.com Equal Contribution Project Leader"
        },
        {
            "title": "Abstract",
            "content": "Classifier-Free Guidance (CFG) significantly enhances controllability in generative models by interpolating conditional and unconditional predictions. However, standard CFG often employs static unconditional input, which can be suboptimal for iterative generation processes where model uncertainty varies dynamically. We introduce Adaptive Classifier-Free Guidance (A-CFG), novel method that tailors the unconditional input by leveraging the models instantaneous predictive confidence. At each step of an iterative (masked) diffusion language model, A-CFG identifies tokens in the currently generated sequence for which the model exhibits low confidence. These tokens are temporarily re-masked to create dynamic, localized unconditional input. This focuses CFGs corrective influence precisely on areas of ambiguity, leading to more effective guidance. We integrate A-CFG into state-of-the-art masked diffusion language model and demonstrate its efficacy. Experiments on diverse language generation benchmarks show that A-CFG yields substantial improvements over standard CFG, achieving, for instance, 3.9 point gain on GPQA. Our work highlights the benefit of dynamically adapting guidance mechanisms to model uncertainty in iterative generation. Code is available at https://github.com/pixeli99/A-CFG."
        },
        {
            "title": "Introduction",
            "content": "Diffusion models [33, 15] have recently revolutionized generative modeling, demonstrating remarkable capabilities in synthesizing high-fidelity data in continuous domains such as image and audio [9, 29]. This success has ignited surge of interest in extending their power to discrete data, with natural language generation standing as particularly compelling frontier [2, 20, 11]. Among these efforts, Masked Diffusion Models (MDMs), exemplified by frameworks like LLaDA [26], have emerged as promising direction. These models learn to reverse gradual masking process, iteratively infilling masked tokens to construct coherent text, offering principled and flexible alternative to traditional autoregressive language generation. pivotal advancement that significantly amplified the practical utility of diffusion models, especially in conditional settings, is Classifier-Free Guidance (CFG) [14]. Originally conceived for continuous models, CFG provides an elegant way to steer the generation process towards desired conditioning signal (e.g., textual prompt) by interpolating between conditional and unconditional model predictions during the reverse diffusion (denoising) phase. This is achieved without the need for an auxiliary classifier, making CFG versatile and widely adopted mechanism for enhancing sample quality and controllability. Naturally, the application of CFG has extended to textual diffusion models, where it plays similar role in guiding text generation. Preprint. Under review. Figure 1: Overview of model confidence dynamics during iterative generation. (a) Token-level confidence heatmap across token positions and generation steps (darker shades indicate higher confidence). (b) Average and minimum confidence scores per generation step. This visualization highlights the dynamic and non-uniform nature of model confidence that A-CFG aims to leverage. However, the conventional application of CFG within iterative (masked) diffusion language models often encounters subtle yet significant limitation: the \"unconditional\" prediction typically relies on static or generic construct. This often involves using null prompt or sequence where all target tokens are uniformly masked to simulate an unconditional state. While straightforward, such fixed approach to unconditioning may not fully harness CFGs potential in the dynamic context of iterative text refinement. As an MLM progressively fills in sequence, its internal state of certainty can vary considerably across different tokens and denoising steps. static unconditional baseline fails to adapt to these nuances, potentially leading to guidance that is either too weak, too diffuse, or misaligned with the models specific points of ambiguity at given step. This observation sparks crucial question: can the \"unconditional\" component of CFG, when applied to iterative diffusion language models, be rendered more intelligent and responsive to the models own evolving understanding of the sequence? We posit that the models instantaneous predictive confidence during the iterative denoising process, which, as visualized in Figure 1, can fluctuate significantly across tokens and generation steps, offers rich, yet largely untapped signal. Instead of blanket, context-agnostic unconditioning, what if we could dynamically shape the unconditional input to reflect and address the models current uncertainties? This would allow the guidance mechanism to concentrate its corrective influence precisely where it is most needed. In this paper, we introduce Adaptive Classifier-Free Guidance (A-CFG), novel framework designed to realize this vision for iterative (masked) diffusion language models. A-CFG dynamically synthesizes the input for the unconditional prediction by identifying and temporarily re-masking tokens for which the conditional diffusion model exhibits low predictive confidence during given denoising step. By doing so, A-CFG creates localized \"unconditional\" state that compels the model to reconsider its predictions at these specific points of ambiguity. The standard CFG formula is then applied, leveraging this adaptively constructed unconditional state to steer the generation with greater precision and efficacy. We integrate and evaluate A-CFG within the LLaDA [26] framework. Our extensive experiments on range of standard language generation benchmarks demonstrate that A-CFG yields substantial improvements in complex reasoning accuracy and adherence to conditional prompts over both baseline LLaDA without CFG and LLaDA employing traditional CFG with static unconditional inputs. Specifically, A-CFG achieves up to 3.9 point absolute improvement on the GPQA benchmark and enhances Sudoku task success by 8.0 points when compared to standard CFG. Our contributions are thus threefold: We identify and articulate the limitations of static unconditioning in standard CFG when applied to iterative masked language models. We propose Adaptive Classifier-Free Guidance (A-CFG), novel method that dynamically constructs the unconditional input based on the models predictive confidence, enabling more targeted and effective guidance. 2 We demonstrate through comprehensive experiments that A-CFG significantly enhances the performance of the LLaDA model on various generation tasks, outperforming standard CFG."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 Diffusion Models for Language Generation Autoregressive (AR) models, such as large language models (LLMs) like GPT-style architectures [27, 5] and more recent powerful open-source models including LLaMA [35, 36], Qwen [6], and Mistral [18], have become the dominant paradigm in natural language generation. These models generate text token by token, conditioning each new token on the previously generated sequence, and have demonstrated remarkable capabilities across wide array of tasks. Their success has also spurred extensions into traditional multimodal domains [17, 40, 38, 39, 37], combining language understanding with other modalities like vision [19, 16, 24, 3, 10, 1, 22, 41]. However, the sequential nature of AR generation can lead to challenges such as error propagation and limitations in bidirectional context modeling for certain tasks. In response to these and other considerations, diffusion models [33, 15] have emerged as powerful alternative. While initially demonstrating success in continuous domains like images [9, 29], significant effort has been dedicated to adapting them for discrete data, particularly text [2, 20, 11]. Early approaches explored discrete state-space diffusion [2] or continuous diffusion in embedding spaces (e.g., Diffusion-LM [20], DiffuSeq [11]), showcasing potential for controllability but often lagging behind AR models in likelihood or efficiency. particularly relevant and successful direction has been the development of Masked Diffusion Models (MDMs) [30, 32]. These models formulate text generation as an iterative mask-infilling process, learning to reverse gradual masking procedure. Prominent examples like LLaDA [26] have demonstrated that MDMs can achieve competitive performance with strong AR models on various language tasks, even at scale. These models operate by iteratively refining sequence, making them prime candidate for fine-grained guidance techniques. Our work focuses specifically on enhancing conditional generation within such iterative, masked diffusion frameworks like LLaDA. 2.2 Classifier-Free Guidance in Generative Models Classifier-Free Guidance (CFG) [14] has become cornerstone technique for improving sample quality and conditional control in diffusion models, initially popularized in image synthesis [29]. It elegantly steers generation towards condition by interpolating between conditional and unconditional model predictions during the reverse process, avoiding the need for separate classifier training. This is typically achieved by training the diffusion model with occasional dropout of the conditioning signal (e.g., null text prompt), enabling it to produce both conditional and unconditional outputs. The adaptation of CFG to language diffusion models [23, 25] presents unique considerations. common practice is to simulate the unconditional prediction by providing static input, such as fully masked target sequence. While effective, this static unconditioning strategy poses limitation, particularly for iterative MDMs. As the model refines the text sequence over multiple steps, its internal state of certainty varies across different token positions and time steps. fixed unconditional baseline fails to adapt to these dynamics, potentially leading to suboptimal or misaligned guidance."
        },
        {
            "title": "3 Methodology",
            "content": "Our work introduces Adaptive Classifier-Free Guidance (A-CFG), novel enhancement to the Classifier-Free Guidance (CFG) paradigm. A-CFG is specifically designed for iterative masked language models (MLMs) and aims to improve generative control by dynamically constructing the unconditional input required for CFG. This is achieved by leveraging the models instantaneous predictive confidence regarding its current non-[MASK] tokens, allowing guidance to be more precisely targeted towards regions of the sequence where the model exhibits uncertainty. Figure 2 provides high-level comparison of standard CFG with our proposed A-CFG. 3 Figure 2: Overview of (left) standard Null Prompt Classifier-Free Guidance and (right) our proposed Adaptive Classifier-Free Guidance (A-CFG) at single generation step k. In standard CFG, the unconditional input often involves masking the entire prompt or using null prompt. In A-CFG, after computing conditional logits from x(k), token-level confidences for all non-[MASK] tokens in x(k) are assessed. Tokens with low confidence (orange/red in illustration) are temporarily re-masked to [MASK] to create the dynamic unconditional input x(k) uncond. This allows the CFG mechanism to focus guidance on areas of model uncertainty within the current sequence. 3.1 Preliminaries Before detailing A-CFG, we briefly review the foundational concepts: iterative masked language modeling and standard classifier-free guidance. Iterative Masked Language Models (MLMs). Our A-CFG framework operates within the context of iterative generation, characteristic of many masked language models like LLaDA. Text generation commences with an input sequence that is either partially or entirely populated with special [MASK] tokens. The generation unfolds over series of steps. At each step k, the model Mθ predicts replacement tokens for subset (or all) of the extant [MASK] tokens. This iterative process progressively refines the sequence x(k) until complete output x(0) is achieved (where typically decreases from an initial number of steps down to 0). The core predictive mechanism involves the model Mθ(x(k)) producing logits over the vocabulary for the positions designated for infilling. Classifier-Free Guidance (CFG). Classifier-Free Guidance [14] is widely adopted technique for enhancing sample quality and controllability in conditional generative models. CFG operates by linearly interpolating the outputs derived from conditional model prediction, Lcond(x(k), c), and an unconditional model prediction, Luncond(x(k), ). Here, represents the conditioning information (e.g., textual prompt), and signifies null or broadly unconditional context. The construction of this unconditional input can vary; for instance, some approaches derive an unconditional-like term from masked version of the conditioning prompt itself [25]. The guided logits, Lguided, are computed as: Lguided(x(k), c) = Luncond(x(k), ) + (w + 1) (Lcond(x(k), c) Luncond(x(k), )), (1) where denotes the models output logits, and is the guidance scale. guidance scale > 0 amplifies the influence of the conditioning signal c. central challenge in applying CFG, particularly in iterative MLM frameworks, is the effective definition and derivation of the unconditional logits Luncond(x(k), ), an issue directly addressed by our A-CFG approach. 3.2 Adaptive Classifier-Free Guidance (A-CFG) Standard CFG, while effective, often relies on static or generic definition for the unconditional prediction Luncond(x(k), ) when applied to iterative MLMs. Typically, this involves using null 4 guided. cond Mθ(x(k)) remaskable {j (x(k))j = [MASK]} Algorithm 1 Adaptive Classifier-Free Guidance (A-CFG) for one generation step 1: Input: Current sequence x(k), conditioning c, model Mθ, guidance w, re-masking proportion ρ. 2: Output: Guided logits L(k) 3: L(k) 4: C(k) 5: CON (k) 6: for C(k) maxv(softmax(L(k) c(k) 7: Add (c(k) , j) to CON (k) Compute conditional logits Identify all non-[MASK] token indices Assess confidence for remaskable tokens remaskable do cond))j,v 8: 9: end for 10: (k) 11: if C(k) 12: low-conf remaskable , C(k) remaskable > 0 then ρ C(k) target min(N target actual if actual > 0 then Sort CON (k) by confidence values c(k) (k) low-conf indices of the first actual remaskable) 13: 14: 15: uncond x(k) end if 16: 17: 18: end if 19: x(k) 20: for (k) (x(k) 21: 22: end for 23: L(k) 24: L(k) 25: return L(k) guided low-conf do uncond)j [MASK] uncond Mθ(x(k) guided L(k) uncond) uncond + (w + 1) (L(k) cond L(k) uncond) in ascending order. elements in sorted CON (k). Create dynamic unconditional input Compute unconditional logits Apply CFG formula prompt or masking all prompt tokens to simulate an unconditional state. In complex generation scenarios, the models uncertainty can fluctuate significantly. static or predefined unconditioning strategy might therefore apply guidance indiscriminately, potentially misdirecting the generation process or failing to provide sufficient correction where it is most needed. This observation motivates A-CFG. Our core intuition is that the \"unconditional\" component of CFG can be made more potent and targeted if it is dynamically informed by the models own state of uncertainty regarding its current, non-masked tokens. Instead of global, context-agnostic unconditioning, A-CFG focuses the guidance mechanism on specific token positions within the sequence x(k) where the conditional model currently exhibits the greatest predictive ambiguity. By temporarily re-masking these lowconfidence non-[MASK] tokens to form the input for Luncond, we compel the model to reconsider its predictions at these critical junctures. This adaptive unconditioning aims to make the guidance signal (Lcond Luncond) more discriminative and effective, leading to more nuanced and efficient control over the generation process. 3.2.1 A-CFG Process The A-CFG process is executed at each iterative generation step k. detailed algorithmic description of this process is provided in Algorithm 1. Given the current sequence x(k) (which includes the prompt and partially generated text), A-CFG involves the following operations: Conditional Logit Computation. The base model Mθ first computes the standard conditional logits based on the current full input x(k): L(k) cond = Mθ(x(k)). 5 (2) These logits represent the models initial predictions under full conditioning by and any already filled tokens in x(k). Token-Level Confidence Assessment. From L(k) tions for all non-[MASK] token positions within the current sequence x(k). Let C(k) indices of all token positions such that (x(k))j = [MASK]. For each position C(k) cond, we assess the models confidence in its predicremaskable be the set of remaskable: We compute the softmax probability distribution (k) cond = softmax(L(k) cond) over the vocabulary. The confidence score for position is defined as the maximum probability in this distribution: cond)j,v. This corresponds to the probability of the token that the model suggests the c(k) = maxv(P (k) cond. low c(k) would predict with highest likelihood for position based on L(k) model is uncertain about the token (x(k))j or its alternatives at that position. While other confidence metrics (e.g., entropy of (k) maximum softmax probability provides simple yet effective measure. cond,j) could be considered, we find that the for tokens at positions C(k) Identification of Low-Confidence Tokens for Re-masking. Based on the assessed confidences c(k) remaskable of positions exhibiting the lowest confidence is selected for adaptive re-masking. The extent of this adaptive intervention is controlled by re-masking proportion hyperparameter, ρ. The target number of tokens to re-mask, target remaskable at step k: , is calculated as proportion of the total number of non-[MASK] tokens within C(k) remaskable, subset (k) low-conf C(k) target = ρ C(k) remaskable. (3) This heuristic scales the intensity of A-CFG intervention with the amount of non-masked content available for re-evaluation. The actual number of tokens selected for re-masking, actual = remaskable). If C(k) min(N target remaskable = 0 or actual = 0, no re-masking occurs for A-CFG, and low-conf is empty. Otherwise, (k) (k) tokens with the lowest confidence scores. low-conf contains the indices of these actual , is actual , C(k) Construction of the Dynamic Unconditional Input. localized unconditional input sequence, x(k) uncond, is synthesized by modifying the current sequence x(k). Specifically, the non-[MASK] tokens at positions identified in (k) low-conf are replaced with the special [MASK] token: (x(k) uncond)j = (cid:40) [MASK] if (k) (x(k))j otherwise. low-conf, (4) low-conf is empty (i.e., no tokens were selected for re-masking), then x(k) If (k) uncond is identical to x(k). When re-masking occurs, this transformation yields an input where the model is explicitly prompted to reconsider its predictions for positions it was previously uncertain about, effectively creating more challenging or less informed context for these specific tokens by erasing its prior commitment at those positions. Unconditional Logit Computation. Using this dynamically constructed input x(k) Mθ computes the unconditional logits: uncond, the model uncond = Mθ(x(k) L(k) uncond). (5) If no adaptive re-masking occurred (i.e., x(k) logits, L(k) obscured (or not, if no such points met the criteria), providing targeted baseline for guidance. uncond = x(k)), then L(k) cond. These uncond, reflect the models predictions when key points of prior uncertainty are deliberately uncond will be identical to L(k) 6 Application of CFG Formula for Guided Logits. Finally, the guided logits L(k) guided for the current step are computed using the standard CFG formula (Equation 1), now employing the adaptively derived L(k) uncond and the original L(k) cond: L(k) guided = L(k) uncond + (w + 1) (L(k) cond L(k) uncond). (6) uncond = L(k) cond (e.g., due to no adaptive re-masking), then L(k) If L(k) applies no effective guidance shift in this specific scenario. These L(k) select the tokens to infill the [MASK] positions for the next iteration x(k1). guided = L(k) cond, implying that A-CFG guided are then used to sample or"
        },
        {
            "title": "4 Experiments",
            "content": "In this section, we empirically evaluate the effectiveness of Adaptive Classifier-Free Guidance (A-CFG). We first describe our experimental setup, including datasets, baseline models, evaluation metrics, and key implementation details. We then present quantitative results from Table 1, comparing LLaDA with A-CFG against LLaDA with standard CFG, LLaDA without guidance, and other stateof-the-art models. Subsequently, we conduct ablation studies to analyze the impact of A-CFGs core hyperparameter. Finally, we provide qualitative examples to illustrate the behavior and benefits of our proposed method. 4.1 Experimental Setup 4.1.1 Datasets and Metrics We evaluate A-CFG on diverse suite of standard benchmarks covering general language understanding, mathematical and scientific reasoning, and planning tasks. General Language Understanding: MMLU (Massive Multitask Language Understanding) [12], BBH (Big-Bench Hard) [34], ARC-C (AI2 Reasoning Challenge - Challenge Set) [7], Hellaswag [44], TruthfulQA [21], WinoGrande [31], and PIQA (Physical Interaction QA) [4]. Mathematics & Science Reasoning: GSM8K (Grade School Math 8K) [8], MATH [13], and GPQA (Graduate-Level Google-Proof Q&A) [28]. Planning Tasks: Countdown [42] and Sudoku [42]. Evaluation mode. Closed-form tasks supply prompt with finite set of candidate answers; we compute each candidates conditional log-likelihood and select the most likely. Open-ended tasks require free-form generation; we sample responses and score them with task-specific metrics such as exact-match accuracy. Likelihood estimation. For likelihood-based evaluations we approximate the conditional perplexity bound with Monte-Carlo sampling. single sample suffices when only one target token is queried (e.g. MMLU). We adopt the same setting as LLaDA, for all other multiple-token tasks we draw 128 samples, which we found to stabilise variance without adding prohibitive cost. Generation hyper-parameters. Unless otherwise stated, we set the answer length to 256 tokens and run the reverse diffusion process for 256 steps (one token revealed per step). 4.1.2 Baseline Models and Methods Our primary evaluation centers on the LLaDA 8B model, assessed under three guidance scenarios: 1) No Guidance (base LLaDA), 2) Standard CFG (Std CFG), where conventional Classifier-Free Guidance [14] uses fully masked target sequence for unconditioning, and 3) our proposed Adaptive CFG (A-CFG). For both Std CFG and A-CFG, the guidance scale is tuned. To investigate A-CFGs broader applicability, we also evaluate it on the Dream-7B diffusion model [43] against its baseline. All results are contextualized against publicly reported scores from comparable autoregressive (AR) models like LLaMA3 8B [35], LLaMA2 7B [36], and Qwen2 7B [6], as detailed in Table 1. 7 Table 1: Benchmark Results of Pre-trained LLMs. LLaDA and Dream-7B are diffusion models. Baseline scores for LLaDA 8B and Dream-7B reflect our own re-evaluation under consistent experimental protocol. Results indicated by are sourced from [6]. The numbers in parentheses represent the number of shots used for evaluation. - indicates unknown data or data not applicable. Benchmark LLaDA 8B LLaDA 8B (Std CFG) LLaDA 8B (A-CFG) Dream-7B Dream-7B (A-CFG) LLaMA3 8B LLaMA2 7B Qwen2 7B Model Diffusion Diffusion Diffusion Diffusion Diffusion AR AR AR MMLU ARC-C Hellaswag TruthfulQA WinoGrande PIQA 65.9 (5) 45.5 (0) 70.8 (0) 45.5 (0) 74.5 (5) 74.9 (0) GSM8K GPQA 70.7 (4) 26.1 (5) Countdown Sudoku 15.3 (8) 35.0 (8) 65.8 (5) 46.3 (0) 71.4 (0) 45.1 (0) 75.1 (5) 74.4 (0) 70.8 (4) 29.4 (5) 14.2 (8) 34.0 (8) 4.1.3 Implementation Details General Tasks 66.1 (5) 47.8 (0) 72.6 (0) 46.2 (0) 75.9 (5) 76.1 (0) 69.5 (5) 59.8 (0) 73.3 (0) 43.9 (0) 73.3 (5) 75.8 (0) Mathematics & Science 73.5 (4) 33.3 (5) 15.8 (8) 42.0 (8) 76.9 (4) 36.6 (5) Planning Tasks 14.6 (8) 72.0 (8) 69.7 (5) 60.8 (0) 74.4 (0) 45.1 (0) 72.5 (5) 76.2 (0) 77.9 (4) 36.8 (5) 15.2 (8) 80.0 (8) 65.4 (5) 53.1 (0) 79.1 (0) 44.0 (0) 77.3 (5) 80.6 (0) 45.9 (5) 46.3 (0) 76.0 (0) 39.0 (0) 72.5 (5) 79.1 (0) 70.3 (5) 60.6 (25) 80.7 (10) 54.2 (0) 77.0 (5) - 53.1 (4) 25.9 (5) 14.3 (4) 25.7 (5) 80.2 (4) 30.8 (5) 3.7 (8) 0.0 (8) - - - - For LLaDAs iterative generation, we use 256 sampling steps with low-confidence remasking. For Standard CFG, the guidance scale was selected from {0.5, 1.0, 1.5, 2.0} based on performance on the validation set of each respective task. For our A-CFG, the guidance scale was similarly tuned. Once value of is chosen for given model, the same is kept fixed across all downstream benchmarks for that model. The adaptive re-masking proportion ρ (determining the fraction of previously generated tokens to re-mask based on low confidence, as defined in Section 3.2.1) was set to 0.7. The confidence for token selection in A-CFG is based on the softmax probability of the predicted token at each masked position. All experiments were conducted using NVIDIA H800 GPUs. 4.2 Benchmark Results The efficacy of Adaptive Classifier-Free Guidance is demonstrated in Table 1, which presents comprehensive comparison of LLaDA 8B equipped with A-CFG against its counterparts using no guidance and standard CFG, alongside other leading diffusion and autoregressive models. A-CFG Enhances LLaDA Performance: Our results clearly indicate that A-CFG substantially elevates the performance of LLaDA 8B. Crucially, A-CFG consistently outperforms LLaDA 8B with Standard CFG, underscoring the benefits of its dynamic, confidence-aware unconditioning mechanism. The advantages are particularly pronounced on complex reasoning and planning benchmarks; for instance, on GPQA, A-CFG achieves score of 33.3, +3.9 point improvement over Standard CFG (29.4), and on the Sudoku planning task, A-CFG (42.0) surpasses Standard CFG (34.0) by significant +8.0 points. This trend of superior performance over Standard CFG extends to mathematical reasoning (e.g., +2.7 points on GSM8K) and across general language understanding tasks such as ARC-C, Hellaswag, and WinoGrande. When compared to LLaDA 8B with No Guidance, A-CFG also yields substantial gains, for example, +7.2 points on GPQA and +7.0 points on Sudoku. These findings highlight A-CFGs capability to more effectively steer the iterative generation process in LLaDA, leading to improved task adherence and overall output quality compared to both unguided generation and conventional CFG. Generalizability to Other Diffusion Models: To assess whether the principles of A-CFG extend beyond LLaDA, we integrated it into the Dream-7B model. Preliminary results in Table 1 suggest that A-CFG brings similar benefits, for instance, improving Sudoku performance by +8.0 points (80.0 vs. 72.0) and ARC-C by +1.0 point (60.8 vs. 59.8) for Dream-7B. These observations suggest that A-CFGs adaptive unconditioning is promising method for enhancing other iterative masked diffusion models. 8 Competitive Standing Against Autoregressive Models: Equipped with A-CFG, the diffusion-based LLaDA 8B demonstrates strong competitive posture against contemporary autoregressive (AR) models of comparable scale. LLaDA 8B (A-CFG) particularly excels in mathematical reasoning, with GSM8K score of 73.5 that surpasses several listed AR counterparts like LLaMA3 8B (53.1). On the challenging GPQA benchmark, its score of 33.3 is notably higher than LLaMA3 8B (25.9) and competitive with Qwen2 7B (30.8). The Sudoku planning task further showcases this strength, where LLaDA 8B (A-CFG) achieves 42.0, markedly outperforming LLaMA3 8B (0.0). While leading AR models such as Qwen2 7B still exhibit an advantage on some general language understanding benchmarks, A-CFG significantly narrows the performance gap and, in specific domains demanding complex reasoning or planning, positions LLaDA as compelling alternative. In summary, the empirical results affirm A-CFG as potent enhancement for iterative diffusion language models. It not only improves upon standard CFG techniques but also enables diffusion models like LLaDA to achieve highly competitive, and in some cases superior, performance compared to strong AR baselines, especially in tasks requiring sophisticated reasoning. 4.3 Ablation Studies To elucidate the contributions of A-CFGs core components and assess its sensitivity to key hyperparameters, we conducted targeted ablation studies. This section focuses on the impact of the adaptive re-masking proportion, critical parameter in A-CFG. 4.3.1 Impact of the Adaptive Re-masking Proportion (ρ) We investigated the influence of ρ on the ARC-C test set, chosen as representative benchmark where A-CFG demonstrated clear benefits and sensitivity to guidance parameters. The main LLaDA 8B (A-CFG) result for ARC-C (47.8 accuracy) reported in Table 1 employed ρ = 0.7. Table 2a presents the performance on ARC-C as ρ is varied across the range [0.1, 0.9]. The results show clear trend: ARC-C accuracy improves steadily as ρ increases from 0.1 (45.9%) to 0.3 (46.5%), 0.5 (46.8%), and culminates at 0.7 (47.8%). This suggests that for task like ARC-C, more substantial re-masking of low-confidence generated tokens is beneficial, allowing A-CFG to exert stronger corrective influence. However, increasing ρ further to 0.9 leads to decline in performance, indicating that excessively aggressive re-masking can become counterproductive, potentially by erasing too much valuable context from the already generated sequence. 4.3.2 Impact of the Guidance Scale (w) Beyond the re-masking proportion ρ, the guidance scale is critical hyperparameter for any CFG-based method. We varied across the set {0.5, 1.0, 1.5, 2.0}, the same range used for tuning in our main experiments. Table 2b illustrates the performance on ARC-C as is adjusted. We observe that A-CFG performance is sensitive to the guidance scale. Specifically, small = 0.0 (equivalent to no CFG guidance beyond the adaptive masking) yields baseline accuracy of 45.5%. As increases, accuracy improves, reaching peak of 47.8% at = 0.5 and = 1.0. This suggests that moderate guidance strength effectively leverages the dynamically constructed unconditional input from A-CFG. However, further increasing to 1.5 and 2.0 leads to slight degradation in performance (47.5% and 47.6%, respectively). This indicates that an overly strong guidance scale might overemphasize the conditional signal at the expense of fluency or correctness, even with A-CFGs targeted unconditioning. The optimal performance at = 0.5 aligns with the value used for ARC-C in our main results  (Table 1)  . 4.4 Qualitative Analysis To provide further insight into A-CFGs dynamic mechanism, Table 3 visualizes the iterative refinement process for mathematical reasoning examples from the GSM8K dataset. These examples illustrate how A-CFG navigates the generation process. For instance, in the \"Natalias clips\" problem, one can observe that while foundational elements are established in early steps (e.g., Natalia, sold), crucial components of the arithmetic reasoning, such as operators, intermediate results, or the final sum, are often resolved or corrected in later iterations. This behavior aligns with A-CFGs core principle: by identifying tokens or positions where the model exhibits low predictive confidence during the iterative process (potentially due to incomplete or inconsistent intermediate reasoning 9 Table 2: Ablation studies on ARC-C. (a) Impact of guidance scale (w). (b) Impact of adaptive re-masking proportion (ρ). The main result for ARC-C in Table 1 used ρ = 0.7 and = 0.5. Scores are Accuracy (%). (a) Re-masking Proportion (ρ) (b) Guidance Scale (w) Benchmark Re-masking Proportion (ρ) Benchmark Guidance Scale (w) 0.1 0. 0.5 0.7 0.9 0.0 0.5 1. 1.5 2.0 ARC-C 45.9 46.5 46. 47.8 46.0 ARC-C 45.5 47.8 47. 47.5 47.6 Table 3: Visualization of A-CFGs iterative refinement process for math reasoning tasks. Darker shades indicate tokens that were filled or corrected in later stages of the adaptive generation, often representing points of initial uncertainty that A-CFG helped resolve. Task A-CFG Refinement Process Prompt: Natalia sold clips to 4 of her friends. She sold 8 clips to each friend. Then she bought more clips. How many clips does Natalia have now? Natalia sold 4 friends. clips each. So, 4 * = 32 clips. Then bought more. Natalia has 32 + = 47 clips. Answer: 47 . Prompt: John has 12 apples. He gives half to Mary. Then Mary buys twice as many apples as she received from John. How many apples does Mary have now? John has 12 apples. Gives half to Mary. So Mary gets 12 / 2 = 6 apples. Then Mary buys twice as many (so 6 * 2 = 12 ). Mary now has 6 + = 18 apples. Answer: 18 . steps), A-CFG dynamically re-masks these specific points. This targeted re-masking compels the model to reconsider and refine its predictions in these areas of ambiguity, thereby facilitating the construction of coherent and accurate multi-step reasoning chain. Similarly, in the \"Johns apples\" example, later steps refine the calculation, ensuring the intermediate and final quantities are correctly derived (e.g., 6+12=18). These qualitative examples underscore A-CFGs ability to leverage its adaptive unconditioning to focus guidance on evolving points of uncertainty, thereby enhancing the models capacity to resolve errors and improve the fidelity of complex, multi-step generations."
        },
        {
            "title": "5 Conclusion",
            "content": "This paper introduced Adaptive Classifier-Free Guidance (A-CFG), novel method to enhance conditional generation in iterative masked language models. By dynamically constructing the unconditional input for CFG based on the models instantaneous predictive confidence in its already generated tokens, A-CFG offers more targeted and responsive guidance mechanism. Our extensive experiments, particularly within the LLaDA framework, demonstrate that A-CFG significantly outperforms standard CFG approaches and unguided baselines, yielding substantial improvements on diverse benchmarks, especially in complex reasoning and planning tasks. The results also highlight A-CFGs potential to bolster the competitiveness of diffusion-based language models against autoregressive counterparts. This work underscores the value of leveraging model uncertainty for more nuanced control in discrete diffusion, opening promising avenues for future research into adaptive generation strategies."
        },
        {
            "title": "References",
            "content": "[1] Ruichuan An, Sihan Yang, Ming Lu, Renrui Zhang, Kai Zeng, Yulin Luo, Jiajun Cao, Hao Liang, Ying Chen, Qi She, et al. Mc-llava: Multi-concept personalized vision-language model. arXiv preprint arXiv:2411.11706, 2024. 10 [2] Jacob Austin, Daniel Johnson, Jonathan Ho, Daniel Tarlow, and Rianne Van Den Berg. Structured denoising diffusion models in discrete state-spaces. Advances in Neural Information Processing Systems, 34:1798117993, 2021. [3] Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Shiori Sagawa, et al. Openflamingo: An opensource framework for training large autoregressive vision-language models. arXiv preprint arXiv:2308.01390, 2023. [4] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical commonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence, 2020. [5] Tom Brown. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020. [6] Yunfei Chu, Jin Xu, Qian Yang, Haojie Wei, Xipin Wei, Zhifang Guo, Yichong Leng, Yuanjun Lv, Jinzheng He, Junyang Lin, et al. Qwen2-audio technical report. arXiv preprint arXiv:2407.10759, 2024. [7] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. [8] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. [9] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:87808794, 2021. [10] Rongyao Fang, Shilin Yan, Zhaoyang Huang, Jingqiu Zhou, Hao Tian, Jifeng Dai, and Hongsheng Li. Instructseq: Unifying vision tasks with instruction-conditioned multi-modal sequence generation. arXiv preprint arXiv:2311.18835, 2023. [11] Shansan Gong, Mukai Li, Jiangtao Feng, Zhiyong Wu, and LingPeng Kong. Diffuseq: Sequence to sequence text generation with diffusion models. arXiv preprint arXiv:2210.08933, 2022. [12] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020. [13] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. [14] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. [15] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. [16] Jack Hong, Shilin Yan, Jiayin Cai, Xiaolong Jiang, Yao Hu, and Weidi Xie. Worldsense: Evaluating real-world omnimodal understanding for multimodal llms. arXiv preprint arXiv:2502.04326, 2025. [17] Lingyi Hong, Shilin Yan, Renrui Zhang, Wanyun Li, Xinyu Zhou, Pinxue Guo, Kaixun Jiang, Yiting Chen, Jinglun Li, Zhaoyu Chen, et al. Onetracker: Unifying visual object tracking with foundation models and efficient tuning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1907919091, 2024. [18] Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. 11 [19] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. [20] Xiang Li, John Thickstun, Ishaan Gulrajani, Percy Liang, and Tatsunori Hashimoto. Diffusion-lm improves controllable text generation. Advances in Neural Information Processing Systems, 35:43284343, 2022. [21] Stephanie Lin, Jacob Hilton, and Owain Evans. Truthfulqa: Measuring how models mimic human falsehoods. arXiv preprint arXiv:2109.07958, 2021. [22] Weifeng Lin, Xinyu Wei, Ruichuan An, Peng Gao, Bocheng Zou, Yulin Luo, Siyuan Huang, Shanghang Zhang, and Hongsheng Li. Draw-and-understand: Leveraging visual prompts to enable MLLMs to comprehend what you want. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=bfa58H1nQ8. [23] Justin Lovelace, Varsha Kishore, Yiwei Chen, and Kilian Weinberger. Diffusion guided language modeling. arXiv preprint arXiv:2408.04220, 2024. [24] Feipeng Ma, Yizhou Zhou, Zheyu Zhang, Shilin Yan, Hebei Li, Zilong He, Siying Wu, Fengyun Rao, Yueyi Zhang, and Xiaoyan Sun. Ee-mllm: data-efficient and compute-efficient multimodal large language model. arXiv preprint arXiv:2408.11795, 2024. [25] Shen Nie, Fengqi Zhu, Chao Du, Tianyu Pang, Qian Liu, Guangtao Zeng, Min Lin, and Chongxuan Li. Scaling up masked diffusion models on text. arXiv preprint arXiv:2410.18514, 2024. [26] Shen Nie, Fengqi Zhu, Zebin You, Xiaolu Zhang, Jingyang Ou, Jun Hu, Jun Zhou, Yankai Lin, Ji-Rong Wen, and Chongxuan Li. Large language diffusion models. arXiv preprint arXiv:2502.09992, 2025. [27] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. [28] David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel Bowman. Gpqa: graduate-level google-proof q&a benchmark. arXiv preprint arXiv:2311.12022, 2023. [29] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. [30] Subham Sekhar Sahoo, Marianne Arriola, Yair Schiff, Aaron Gokaslan, Edgar Marroquin, Justin Chiu, Alexander Rush, and Volodymyr Kuleshov. Simple and effective masked diffusion language models. arXiv preprint arXiv:2406.07524, 2024. [31] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99106, 2021. [32] Jiaxin Shi, Kehang Han, Zhe Wang, Arnaud Doucet, and Michalis Titsias. Simplified and generalized masked diffusion for discrete data. arXiv preprint arXiv:2406.04329, 2024. [33] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International conference on machine learning, pages 22562265. PMLR, 2015. [34] Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc Le, Ed Chi, Denny Zhou, et al. Challenging bigbench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022. [35] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. 12 [36] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. [37] Zehao Xiao, Shilin Yan, Jack Hong, Jiayin Cai, Xiaolong Jiang, Yao Hu, Jiayi Shen, Qi Wang, and Cees GM Snoek. Dynaprompt: Dynamic test-time prompt tuning. arXiv preprint arXiv:2501.16404, 2025. [38] Shilin Yan, Ouxiang Li, Jiayin Cai, Yanbin Hao, Xiaolong Jiang, Yao Hu, and Weidi Xie. sanity check for ai-generated image detection. arXiv preprint arXiv:2406.19435, 2024. [39] Shilin Yan, Xiaohao Xu, Renrui Zhang, Lingyi Hong, Wenchao Chen, Wenqiang Zhang, and Wei Zhang. Panovos: Bridging non-panoramic and panoramic views with transformer for video segmentation. In European Conference on Computer Vision, pages 346365. Springer, 2024. [40] Shilin Yan, Renrui Zhang, Ziyu Guo, Wenchao Chen, Wei Zhang, Hongyang Li, Yu Qiao, Hao Dong, Zhongjiang He, and Peng Gao. Referred by multi-modality: unified temporal transformer for video object segmentation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 64496457, 2024. [41] Shilin Yan, Jiaming Han, Joey Tsai, Hongwei Xue, Rongyao Fang, Lingyi Hong, Ziyu Guo, and Ray Zhang. Crosslmm: Decoupling long video sequences from lmms via dual cross-attention mechanisms. arXiv preprint arXiv:2505.17020, 2025. [42] Jiacheng Ye, Jiahui Gao, Shansan Gong, Lin Zheng, Xin Jiang, Zhenguo Li, and Lingpeng Kong. Beyond autoregression: Discrete diffusion for complex reasoning and planning. arXiv preprint arXiv:2410.14157, 2024. [43] Jiacheng Ye, Zhihui Xie, Lin Zheng, Jiahui Gao, Zirui Wu, Xin Jiang, Zhenguo Li, and Lingpeng Kong. Dream 7b, 2025. URL https://hkunlp.github.io/blog/2025/dream. [44] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can machine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019."
        }
    ],
    "affiliations": [
        "CUHK",
        "FDU",
        "ICL",
        "PKU",
        "PolyU",
        "THU"
    ]
}