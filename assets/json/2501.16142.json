{
    "paper_title": "Towards General-Purpose Model-Free Reinforcement Learning",
    "authors": [
        "Scott Fujimoto",
        "Pierluca D'Oro",
        "Amy Zhang",
        "Yuandong Tian",
        "Michael Rabbat"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reinforcement learning (RL) promises a framework for near-universal problem-solving. In practice however, RL algorithms are often tailored to specific benchmarks, relying on carefully tuned hyperparameters and algorithmic choices. Recently, powerful model-based RL methods have shown impressive general results across benchmarks but come at the cost of increased complexity and slow run times, limiting their broader applicability. In this paper, we attempt to find a unifying model-free deep RL algorithm that can address a diverse class of domains and problem settings. To achieve this, we leverage model-based representations that approximately linearize the value function, taking advantage of the denser task objectives used by model-based RL while avoiding the costs associated with planning or simulated trajectories. We evaluate our algorithm, MR.Q, on a variety of common RL benchmarks with a single set of hyperparameters and show a competitive performance against domain-specific and general baselines, providing a concrete step towards building general-purpose model-free deep RL algorithms."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 2 ] . [ 1 2 4 1 6 1 . 1 0 5 2 : r Published as conference paper at ICLR 2025 TOWARDS GENERAL-PURPOSE MODEL-FREE REINFORCEMENT LEARNING Scott Fujimoto, Pierluca DOro, Amy Zhang, Yuandong Tian, Michael Rabbat Meta FAIR"
        },
        {
            "title": "ABSTRACT",
            "content": "Reinforcement learning (RL) promises framework for near-universal problemsolving. In practice however, RL algorithms are often tailored to specific benchmarks, relying on carefully tuned hyperparameters and algorithmic choices. Recently, powerful model-based RL methods have shown impressive general results across benchmarks but come at the cost of increased complexity and slow run times, limiting their broader applicability. In this paper, we attempt to find unifying model-free deep RL algorithm that can address diverse class of domains and problem settings. To achieve this, we leverage model-based representations that approximately linearize the value function, taking advantage of the denser task objectives used by model-based RL while avoiding the costs associated with planning or simulated trajectories. We evaluate our algorithm, MR.Q, on variety of common RL benchmarks with single set of hyperparameters and show competitive performance against domain-specific and general baselines, providing concrete step towards building general-purpose model-free deep RL algorithms. Gym - Locomotion Cont. actions, vector obs. DMC - Proprioceptive Cont. actions, vector obs. DMC - Visual Cont. actions, pixel obs. Atari - 10M Discrete actions, pixel obs. i r - 3 1.5 1.2 0.9 0.6 0.3 0. 0 2 D - 3 m D . M 7 ) 1 ( w 0.8 0. 0.4 0.2 0.0 2 D - . M 7 3 m D ) 1 ( w 0.6 0.4 0.2 0.0 2 D - 3 m D . 2 - r z m - u 4.0 3.0 2.0 1.0 0. 3 m D n . N P Parameter Count Gym (HalfCheetah-v4) Training FPS Gym (HalfCheetah-v4) Evaluation FPS Gym (HalfCheetah-v4) Parameter Count Atari (Any) MR.Q (4.1M) MR.Q (49) MR.Q (1.9k) MR.Q (4.4M) DreamerV3 (9.7M) DreamerV3 (18) DreamerV3 (236) DreamerV3 (187.3M) TD-MPC2 (5.5M) TD-MPC2 (14) TD-MPC2 (27) Rainbow (6.5M) 4 2 6 Parameters (1M) 8 10 0 20 10 30 Frames per second 50 0.0 0.5 2.0 1.0 Frames per second (1k) 1.5 50 100 Parameters (1M) 150 200 Figure 1: Summary of results. Aggregate mean performance across four common RL benchmarks and 118 environments featuring diverse characteristics (e.g., observation and action spaces, task types). Error bars capture 95% stratified bootstrap confidence interval. Our algorithm, MR.Q, achieves competitive performance against both state-of-the-art domain-specific and general baselines, while using single set of hyperparameters. Notably, MR.Q accomplishes this with fewer network parameters and substantially faster training and evaluation speeds than general-purpose model-based methods."
        },
        {
            "title": "INTRODUCTION",
            "content": "The conceptual premise of RL is inherently general-purposean RL agent can learn optimal behavior with only two basic elements: well-defined objective and data describing its interactions with Correspondence: sfujimoto@meta.com. Code: https://github.com/facebookresearch/MRQ. 1 Published as conference paper at ICLR 2025 the environment. In reality, however, most RL algorithms are anything but general-purpose. Instead, RL algorithms are highly specialized and typically characterized by specific problem classes, such as discrete versus continuous actions or vector versus pixel observations, with each category requiring its own set of algorithmic choices and hyperparameters. For example, Rainbow and TD3 (Hessel et al., 2018; Fujimoto et al., 2018), common methods for Atari and MuJoCo respectively (Bellemare et al., 2013; Todorov et al., 2012), have more differences than similarities in their shared hyperparameters  (Table 1)  without accounting for further algorithmic differences. Table 1: Hyperparameter differences between Rainbow (Hessel et al., 2018) and TD3 (Fujimoto et al., 2018). TD3 uses an expected moving average (EMA) update with an effective frequency of"
        },
        {
            "title": "Hyperparameter",
            "content": "1 10."
        },
        {
            "title": "Rainbow",
            "content": "= 200. TD3 Discount factor Optimizer Learning Rate Adam Ïµ Replay buffer size Minibatch size Target network update Effective target update freq. 8k 20k Initial random steps 0.99 0.99 Adam Adam 6.25 105 103 108 1.5 104 1M 1M 100 32 EMA Iterative 200 1k To some extent, general-purpose algorithms do existpolicy gradient methods (Williams, 1992; Schulman et al., 2015; 2017) and many evolutionary approaches (Rechenberg, 1978; Back, 1996; Rubinstein, 1997; Salimans et al., 2017) require few assumptions on the underlying problem. Unfortunately, these methods often offer poor sample efficiency and asymptotic performance compared more domain-specific approaches, and in some instances, can require extensive re-tuning over numerous implementation-level details (Engstrom et al., 2020; Huang et al., 2022). Recently, DreamerV3 (Hafner et al., 2023) and TD-MPC2 (Hansen et al., 2024), have showcased the potential of general-purpose model-based approaches, achieving impressive single-task performance on diverse set of benchmarks without re-tuning hyperparameters. However, despite their success, model-based methods also introduce substantial algorithmic and computational complexity, making them less practical than lightweight domain-specific model-free algorithms. This paper presents general model-free RL algorithm that leverages model-based representations to achieve the sample efficiency and performance of model-based methods, without the computational overhead. recent surge of high-performing model-free RL algorithms with dynamics-based representations (Guo et al., 2020; 2022; Schwarzer et al., 2020; 2023; Zhao et al., 2023; Fujimoto et al., 2024; Zheng et al., 2024; Scannell et al., 2024) has showcased the potential of this family of algorithms when tailored for single benchmark. Recognizing the similarity between these modelbased and model-free approaches, our hypothesis is that the true benefit of model-based objectives is in the implicitly learned representation, rather than the model itself, and thus prompting the question: Can model-based representations alone enable sample-efficient general-purpose learning? Our proposed approach is based on learning features that approximately capture linear relationship between state-action pairs and value. To do so, we draw heavily from modern dynamics-based representation learning methods (see Related Work) as well as the work of Parr et al. (2008), who show that both model-based and model-free objectives converge to the same solution in linear space. By mapping states and actions into single, unified embedding, we eliminate any environmentspecific characteristics of the input space and allow for standardized set of hyperparameters. We evaluate our method, MR.Q, on four widely used RL benchmarks and 118 environments, and achieve competitive performance against state-of-the-art domain-specific and general baselines without algorithmic or hyperparameter changes between environments or benchmarks."
        },
        {
            "title": "2 RELATED WORK",
            "content": "General-purpose RL. Although many traditional RL methods are general-purpose in principle, practical constraints often force assumptions about the task domain. For example, algorithms like Q-learning and SARSA (Watkins, 1989; Rummery & Niranjan, 1994) can be conceptually extended to continuous spaces, but are typically implemented using discrete lookup tables. In practice, early examples of general decision-making approaches can be found in on-policy methods with function approximation. For instance, both evolutionary algorithms (Rechenberg, 1978; Back, 1996; Rubinstein, 1997; Salimans et al., 2017) and policy gradient methods (Williams, 1992; Sutton et al., 1999; 2 Published as conference paper at ICLR 2025 Schulman et al., 2015; 2017) offer update rules with convergence guarantees and independence to the input space. However, despite their generality, these methods are also hindered by poor sample efficiency and are prone to local minima, limiting their suitability for many practical applications. In contrast, the design of deep RL algorithms tends to favor more specialized approaches that align closely with single benchmarke.g., DQNAtari (Bellemare et al., 2013; Mnih et al., 2015), DDPGMuJoCo (Todorov et al., 2012; Lillicrap et al., 2015), or AlphaGoGo (Silver et al., 2016). Generalizing beyond these initial benchmarks can often require significant engineering, tuning, or algorithmic discovery (Luong et al., 2019; Schrittwieser et al., 2020; Haydari & YÄ±lmaz, 2020; Ibarz et al., 2021). In imitation learning, GATO achieved generalist behavior, but relied on large expert datasets (Reed et al., 2022). Recently, DreamerV3 (Hafner et al., 2023) demonstrated strong capability over many benchmarks without re-tuning, but used costly large models and simulated rollouts. Our objective is to discover lightweight model-free approach to general-purpose learning. Dynamics-based representation learning. Building representations from system dynamics is long-standing approach for adaptation, partial observability, and feature selection (Dayan, 1993; Littman & Sutton, 2001; Parr et al., 2008). Numerous model-free methods have been developed to learn representations by predicting future latent states (Munk et al., 2016; Van Hoof et al., 2016; Zhang et al., 2018; Gelada et al., 2019; Lee et al., 2020; Guo et al., 2020; 2022; Schwarzer et al., 2020; 2023; Zintgraf et al., 2021; Yu et al., 2021; 2022; Fujimoto et al., 2021; 2024; McInroe et al., 2021; Seo et al., 2022; Kim et al., 2022; Tang et al., 2023; Zhao et al., 2023; Zheng et al., 2024; Ni et al., 2024; Scannell et al., 2024). Unsurprisingly, these model-free approaches closely relate to model-based counterparts which learn latent dynamics model for planning or value estimation (Watter et al., 2015; Finn et al., 2016; Karl et al., 2017; Ha & Schmidhuber, 2018; Schrittwieser et al., 2020; 2021; Ye et al., 2021; Hansen et al., 2022; 2024; Hafner et al., 2019; 2023; Wang et al., 2024). Our approach, MR.Q, is most closely related to the state-action representation learning in TD7 (Fujimoto et al., 2024). At high level, MR.Q differs from TD7 by discarding the original input and including losses over the reward and termination. MR.Q also differs significantly in implementation, drawing inspiration from prior work to determine set of design choices that performs well across benchmarks, including multi-step returns, unrolled dynamics, and categorical losses. Our motivation also relates to linear MDPs (Jin et al., 2020; Agarwal et al., 2020) and linear spectral representation (Ren et al., 2022; 2023; Zhang et al., 2022; Shribak et al., 2024). The latter aims to learn low-rank decomposition of the transition dynamics of the MDP and recover linear relationship between an embedding and the value function. Similarly, our work connects to two-stage linear RL, where non-linear embedding is learned for linear RL (Levine et al., 2017; Chung et al., 2019). State abstraction. Our work is closely related to bisimulation metrics (Ferns et al., 2004; 2011; Castro, 2020) and MDP homomorphisms (Ravindran, 2004; van der Pol et al., 2020a;b; RezaeiShoshtari et al., 2022) which rely on measures of similarity in reward and dynamics for state or action abstraction. These concepts have inspired practical approximations to bisimulation metrics as means of shaping representations in deep RL agents, particularly those using image-based observations (Zhang et al., 2020; Castro et al., 2021; Zang et al., 2022)."
        },
        {
            "title": "3 BACKGROUND",
            "content": "Reinforcement learning (RL) problems are described by Markov Decision Process (MDP) (Bellman, 1957), which we define by tuple (S, A, p, R, Î³) of state space S, action space A, dynamics function p, reward function and discount factor Î³. Value-based RL methods learn value function QÏ(s, a) = EÏ[ t=0 Î³trts0 = s, a0 = a] that models the expected discounted sum of rewards rt R(st, at) by following policy Ï which maps states to actions a. The true value function QÏ is estimated by an approximate value function QÎ¸. We use subscripts to indicate the network parameters Î¸. Target networks, which are used to introduce stationarity in prediction targets, have parameters denoted by an apostrophe, e.g., QÎ¸ . These parameters are periodically synchronized with the current network parameters (Î¸ Î¸)."
        },
        {
            "title": "4 MODEL-BASED REPRESENTATIONS FOR Q-LEARNING",
            "content": "This section presents the MR.Q algorithm (Model-based Representations for Q-learning), modelfree RL algorithm that learns an approximately linear representation of the value function through 3 Published as conference paper at ICLR 2025 model-based objectives. Value-based RL algorithms learn value function that maps state-action pairs (s, a) to values in and policy Ï that maps states to actions a. Like many representation learning methods for RL, MR.Q adds an initial step that transforms states and state-action pairs into embeddings zs and zsa, which serves as inputs to the downstream policy and value function. fÏ zs, ÏÏ zs a, gÏ (s, a) zsa, QÎ¸ zsa R. (1) (2) While neither the value function nor policy require explicit representation learning, using intermediate embeddings has two main benefits: 1. Introducing an explicit representation learning stage can enable richer alternative learning signals that are grounded in the dynamics and rewards of the MDP, as opposed to relying exclusively on non-stationary value targets used in both value and policy learning. 2. Representation learning can transform the input into unified, abstract space that is decoupled from the original input characteristics, e.g., images or action spaces. This abstraction allows us to filter irrelevant or spurious details and use unified downstream architectures, improving robustness to environment variations. To learn these embeddings, we draw inspiration from linear feature selection, revisiting the work of Parr et al. (2008), as well as MDP homomorphisms (Ravindran & Barto, 2002). In Section 4.1 we highlight how model-based objectives can be used to learn features that share an approximately linear relationship with the true value function. Then in Section 4.2, we relax our theoretical motivation for practical algorithm based on recent advances in dynamics-based representation learning."
        },
        {
            "title": "4.1 THEORETICAL MOTIVATION",
            "content": "Consider linear decomposition of the value function, where the value function Q(s, a) is represented by features zsa and linear weights w: (3) Our primary objective is to learn features zsa that share an approximately linear relationship with the true value function QÏ. However, since this relationship is only approximate, we use these features as input to non-linear function ËQ(zsa), rather than relying solely on linear function approximation. Q(s, a) = saw. We start by exploring how to find features that can linearly represent the true value function. Given dataset of tuples (s, a, r, s, a), we consider two possible approaches for learning value function Q: model-free update based on semi-gradient TD (Sutton, 1988; Sutton & Barto, 1998): Î±ED [w (z saw + Î³z sa wsg )2] . (4) model-based approach to learn wmb, based on rolling out estimates of the dynamics and reward: wmb = t=0 Î³tW pwr, wr = argmin ED [(z saw r)2] , Wp = argmin ED [(z saW zsa)2] . (5) (6) Closely following Parr et al. (2008) and Song et al. (2016), we can show that these approaches converge to the same solution (proofs for this section can be found in Appendix A). Theorem 1. The fixed point of the model-free approach (Equation 4) and the solution of the modelbased approach (Equation 5) are the same. From the insight of Theorem 1, we can connect the value error VE, the difference between an approximate value function and the true value function QÏ, VE(s, a) = Q(s, a) QÏ(s, a) (7) to the accuracy of reward and dynamics components of the estimated model (Theorem 2). Theorem 2. The value error of the solution described by Theorem 1 is bounded by the accuracy of the estimated dynamics and reward: 1 1 Î³ saWp Es,as,a[zsa ]) . sawr Ers,a[r] + max VE(s, a) max (s,a)SA wi (z (8) 4 Published as conference paper at ICLR 2025 Parr et al. (2008) and Song et al. (2016) use related insight regarding the Bellman error to infer an approach for feature selection. However, with the advent of deep learning, we can instead directly learn the features zsa by jointly optimizing them alongside the linear weights wr and Wp. This is accomplished by treating the features and linear weights as unified end-to-end model and balancing the losses in Equation 6 with hyperparameter Î»: L(zsa, wr, Wp) = ED [(z sawr r)2] + Î»ED [(z saWp zsa )2] . (9) Reward learning Dynamics learning However, the resulting Equation 9 has some notable drawbacks. Dependency on Ï. The dynamics target zsa depends on an action determined by the policy Ï. In policy optimization problems, this introduces non-stationarity, where the target embedding must be continually updated to reflect changes in the policy. This creates an undesirable interdependence between the policy and encoder. Undesirable local minima. Jointly optimizing both the features zsa and the dynamics target can lead to undesirable local minima, similar to the issues encountered with Bellman residual minimization (Baird, 1995; Fujimoto et al., 2022). This can result in collapsed or trivial solutions when the dataset does not fully cover the state and action space or when the reward is sparse. To address these issues, we suggest relaxations on our proposed, theoretically grounded approach: L(zsa, wr, Wp) = ED [(z sawr r)2] + Î»ED[(z saWp zs 2 ) ]. (10) Adjustment We propose two key modifications to alleviate the aforementioned issues. Firstly, we use statedependent embedding zs as the dynamics target, rather than the state-action embedding zsa. This eliminates any dependency on the current policy while still capturing the environments dynamics. Secondly, to mitigate the issue of local minima, we use target network fÏ(s) to generate the dynamics target zs, where the parameters Ï are periodically updated to track the current network parameters Ï. Empirical evidence from prior work suggests that this approach can yield significant performance gains (Grill et al. (2020); Assran et al. (2023), see Related Work), although it no longer guarantees convergence to fixed point. Due to these two changes, even if the modified objective defined by Equation 10 is minimized, we can no longer assume there is linear relationship between the embedding zsa and the value function. However, we can instead allow for non-linear relationship, replacing linear weights with non-linear function ËQ(zsa). We can show that this relationship exists as long as the features are sufficiently rich (i.e., such that MDP homomorphism is satisfied (Ravindran & Barto, 2002)). Theorem 3. Given functions (s) = zs and g(zs, a) = zsa, then if there exists functions Ëp and ËR such that for all (s, a) A: ËR [ ËR(zsa)] = ER [R(s, a)] , Ëp(zszsa) = p(Ëss, a), (11) ËszËs=zs then for any policy Ï where there exists corresponding policy ËÏ(azs) = Ï(as), there exists function ËQ equal to the true value function QÏ over all possible state-action pairs (s, a) A: ËQ(zsa) = QÏ(s, a). (12) Furthermore, Equation 11 guarantees the existence of an optimal policy ËÏ(azs) = Ï(as). Consequently, even if the features zsa do not linearly represent the true value function, i.e., the loss in Equation 9 cannot be not exactly minimized, zsa can still be used in non-linear relationship to represent the value function. Furthermore, Theorem 3 outlines similar objective as the original linear objective defined in Equation 9, in learning the reward and dynamics of the MDP. These results motivates the practical algorithm discussed in the following section. Using the adjusted loss defined in Equation 10, we will aim to learn features with an approximately linear relationship to the true value function, but use non-linear value function with those features to account for the error induced by our approximations. 5 Published as conference paper at ICLR"
        },
        {
            "title": "4.2 ALGORITHM",
            "content": "We now present the details of MR.Q (Model-based Representations for Q-learning). Building on the insights from the previous section, our key idea is to learn state-action embedding zsa that is approximately linear with the true value function QÏ. To account for approximation errors, these features are used with non-linear function approximation to determine the value. The state embedding vector zs is obtained as an intermediate component by training end-to-end with the state-action encoder. MR.Q handles different input modalities by swapping the architecture of the state encoder. Since zs is vector, the remaining networks are independent of the observation space and use feedforward networks. Given the transition (s, a, r, d, s) from the replay buffer: Output MR.Q Trained end-to-end State Encoder State-Action Encoder MDP predictor zs = fÏ(s) zsa = gÏ(zs, a) zs , r, = sam"
        },
        {
            "title": "Decoupled RL",
            "content": "Qi = QÎ¸(zsa) aÏ = ÏÏ(zs) Update MR.Q if % Ttarget = 0 then Target networks: Î¸, Ï, Ï Î¸, Ï, Ï. Reward scaling: r, meanDr. for Ttarget time steps do Encoder update: Equation 14. Value update: Equation 19. Policy update: Equation 20. The encoder loss is composed of three terms based on the reward, dynamics and terminal signal that are unrolled over short horizon. The value function and policy are trained independently, using standard losses (Silver et al., 2014; Fujimoto et al., 2018). We use LAP (Fujimoto et al., 2020) to sample transitions with priority according to their TD errors (Schaul et al., 2016), the absolute difference between the predicted value and the target value in Equation 19. The target network, reward scaling (defined in Equation 19), and the encoder are updated periodically every Ttarget time steps. This synchronized update schedule keeps the input and target output fixed for the downstream value function and policy within each iteration, thus reducing nonstationarity in the optimization (Fujimoto et al., 2024)."
        },
        {
            "title": "4.2.1 ENCODER",
            "content": "The encoder loss is based on unrolling the dynamics of the learned model over short horizon. Given subsequence of an episode (s0, a0, r1, d1, s1, ..., rHEnc , dHEnc, sHEnc ), the model is unrolled by encoding the initial state s0, then by repeatedly applying the state-action encoder gÏ and linear MDP predictor m: zt, rt, dt = gÏ(zt1, at1)m, where z0 = fÏ(s0). (13) The final loss is summed over the unrolled model and balanced by corresponding hyperparameters: LEncoder(f, g, m) = HEnc t= Î»RewardLReward(rt) + Î»DynamicsLDynamics(zt s) + Î»TerminalLTerminal( dt). (14) Î»Terminal is set to 0 until the first terminal transition (i.e., = 0) is viewed. This approach is commonly used in model-based RL (Oh et al., 2015; Hafner et al., 2023; Hansen et al., 2024), as well as dynamics-based representation learning (Schwarzer et al., 2020; 2023; Scannell et al., 2024). Reward loss. While our theoretical analysis suggests using the mean-squared error to train the predicted reward, we find that categorical representation of the reward is more effective in practice for predicting sparse rewards and is robust to reward magnitude. This empirical benefit is consistent with prior work (Schrittwieser et al., 2020; Hafner et al., 2023; Hansen et al., 2024; Wang et al., 2024). Our reward loss function uses the cross entropy CE between the predicted reward and two-hot encoding of the reward r: LReward(r) = CE (r, Two-Hot(r)) . (15) 6 Published as conference paper at ICLR 2025 To handle wide range of reward magnitudes without prior knowledge, the locations of the two-hot encoding are spaced at increasing non-uniform intervals, according to symexp(x) = sign(x)(exp (x) 1) (Hafner et al., 2023). Dynamics loss. The dynamics loss minimizes the mean-squared error between the predicted next state embedding zs and the next state embedding zs from the target encoder fÏ : LDynamics(zs) = (zs zs )2 . (16) As discussed in the previous section, using the next state embedding zs eliminates the dependency on the policy that would occur when using state-action embedding target. Terminal loss. The predicted scalar terminal signal is trained simply using MSE loss with the binary terminal signal d: LTerminal( d) = ( d)2. (17)"
        },
        {
            "title": "4.2.2 VALUE FUNCTION",
            "content": "Value learning is primarily based on TD3 (Fujimoto et al., 2018). Specifically, we train two value functions and take the minimum output between their respective target networks to determine the value target. Similar to TD3, the target action is determined by the target policy ÏÏ , perturbed by small amount of clipped Gaussian noise: aÏ = {argmax clip(a, 1, 1) for discrete A, for continuous A, where = ÏÏ(s) + clip(Ïµ, c, c), Ïµ (0, Ï2). (18) Discrete actions are represented by one-hot encoding, where the Gaussian noise is added to each dimension. Action noise and the clipping is scaled according the range of the action space. We modify the TD3 loss in few ways. Firstly, following numerous prior work across benchmarks (Hessel et al., 2018; Barth-Maron et al., 2018; Yarats et al., 2022; Schwarzer et al., 2023), we predict multi-step returns over horizon HQ. Secondly, we use the Huber loss instead of meansquared error to eliminate bias from prioritized sampling (Fujimoto et al., 2020). Finally, the target value is normalized according to the average absolute reward in the replay buffer: LValue( Qi) = Huber Qi, 1 HQ1 t=0 Î³trt + Î³HQ , = min j=1,2 QÎ¸ (zsHQ aHQ ,Ï ). (19) The value captures the target average absolute reward, which is the scaling factor used to the most recently copied value functions QÎ¸ . This value is updated simultaneously with the target networks r. Maintaining consistent reward scale keeps the loss magnitude constant across different benchmarks, thus improving the robustness of single set of hyperparameters."
        },
        {
            "title": "4.2.3 POLICY",
            "content": "For both continuous and discrete action spaces, the policy is updated using the deterministic policy gradient (Silver et al., 2014): LPolicy(aÏ) = 0.5 Qi(zsaÏ ) + Î»pre-activz2 Ï, where aÏ = activ(zÏ). (20) i={1,2} To make the loss universal between action spaces, we use Gumbel-Softmax (Jang et al., 2017; Lowe et al., 2017; Cianflone et al., 2019) for discrete actions, and Tanh for continuous actions. small regularization penalty is added to the square of the pre-activations zÏ before the policys final activation to help avoid local minima when the reward, and value, is sparse (Bjorck et al., 2021). For exploration, Gaussian noise is added to each dimension of the action (or one-hot encoding of the action). Similar to Equation 18, the resulting action vector is clipped to the range of the action space for continuous actions. For discrete actions, the final action is determined by the argmax operation. 7 Published as conference paper at ICLR 2025 Gym - Locomotion 5 tasks DMC - Proprioceptive 28 tasks DMC - Visual 28 tasks Atari 57 tasks i r - 3 1.5 1. 0.9 0.6 0.3 0.0 0.0 ) 1 ( a l T 0.8 0.6 0.4 0. 0.0 0.0 1.0 ) 1 ( w a T 0.6 0.5 0.4 0.3 0.2 0. 0.0 0.0 0.5 i r - u 4.0 3. 2.0 1.0 0.0 0.5 0.0 0. 0.1 0.4 Time Steps (1M) 0.3 0.2 0.1 0.4 Time Steps (1M) 0.3 1. 0.5 2.0 Time Steps (1M) 1.5 2.5 0.4 0.2 0.8 Time Steps (1M) 0. MR.Q DreamerV3 TD-MPC"
        },
        {
            "title": "PPO",
            "content": "TD7 DrQ-v"
        },
        {
            "title": "DQN",
            "content": "Figure 2: Aggregate learning curves. Average performance over each benchmark. Results are over 10 seeds. The shaded area captures 95% stratified bootstrap confidence interval. Due to action repeat, 500k time steps in DMC correspond to 1M frames in the original environment and 2.5M time steps in Atari corresponds to 10M frames in the original environment."
        },
        {
            "title": "5 EXPERIMENTS",
            "content": "We evaluate MR.Q on four popular RL benchmarks and 118 environments, and compare its performance against strong domain-specific baselines, general model-based approaches, DreamerV3 (Hafner et al., 2023) and TD-MPC2 (Hansen et al., 2024), and general model-free algorithm, PPO (Schulman et al., 2017). Rather than establish MR.Q as the state-of-the-art approach in any particular benchmark, our objective is to demonstrate its broad applicability and effectiveness across diverse set of tasks with single set of hyperparameters. The baselines use author-suggested default hyperparameters and are fixed across environments. Additional details can be found in Appendix B."
        },
        {
            "title": "5.1 MAIN RESULTS",
            "content": "Aggregate learning curves are displayed in Figure 2, with full results displayed in Appendix C. Gym - Locomotion. This subset of the Gym benchmark (Brockman et al., 2016; Towers et al., 2024) considers 5 locomotion tasks in the MuJoCo simulator (Todorov et al., 2012) with continuous actions and low level states. Agents are trained for 1M time steps without any environment preprocessing. We evaluate against three baselines: TD7 (Fujimoto et al., 2024), state-of-the-art (or near) approach for this benchmark, as well as TD-MPC2, DreamerV3, and PPO. To aggregate results, we normalize using the performance of TD3 (Fujimoto et al., 2018). DMC - Proprioceptive. The DeepMind Control suite (DMC) (Tassa et al., 2018) is collection of continuous control robotics tasks built on the MuJoCo simulator. These tasks use the proprioceptive states as the observation space, meaning that the input is vector, and limit the total reward for each episode at 1000, making it easy to aggregate results. We report results on all 28 default tasks that were used by either TD-MPC2 or DreamerV3. Agents are trained for 500k time steps, equivalent to 1M frames in the original environment due to action repeat. For comparison, we evaluate against the same three algorithms as in the Gym benchmark, with TD-MPC2 considered state-of-the-art (or near) for this benchmark. We also include TD7 due to its strong performance in the Gym benchmark. DMC - Visual. The visual DMC benchmark includes the same 28 tasks as the proprioceptive benchmark, but uses image-based observations instead. Agents are trained for 500k time steps. For baselines, we include DrQ-v2 (Yarats et al., 2022), given its state-of-the-art (or near) performance in model-free RL, alongside TD-MPC2, DreamerV3, and PPO. Atari. The Atari benchmark is built on the Arcade Learning Environment (Bellemare et al., 2013). This benchmark uses pixel observations and discrete actions and includes the 57 games used by DreamerV3. We follow standard preprocessing steps, including sticky actions (Machado et al., 2018) (full details in Appendix B.3). Agents are trained for 2.5M time steps (equivalent to 10M frames), setting which has been considered by prior work (Sokar et al., 2023). For comparison, we evaluate against three baselines: the model-based approach DreamerV3, as well as model-free approaches, DQN (Mnih et al., 2015), Rainbow (Hessel et al., 2018), and PPO. Results are aggregated by normalizing scores against human performance. Published as conference paper at ICLR 2025 Discussion. Throughout our experiments, we find the presence of no free lunch, where the topperforming baseline in one benchmark fails to replicate its success in another. Regardless, MR.Q achieves the highest performance in both DMC benchmarks, showcasing its ability to handle different observation spaces. Although it falls slightly behind TD7 in the Gym benchmark, MR.Q is the strongest method overall across all continuous control benchmarks. In Atari, while DreamerV3 outperforms MR.Q, it relies on model with 40 times more parameters and struggles comparatively in the remaining benchmarks. When compared to the model-free baselines, MR.Q surpasses PPO, DQN, and Rainbow, demonstrating its effectiveness with discrete action spaces."
        },
        {
            "title": "5.2 DESIGN STUDY",
            "content": "To better understand the impact of certain design choices and hyperparameters, we attempt variations of MR.Q, and report the aggregate results in Table 2. Table 2: Design study. Average difference in normalized performance from varying design choices across each benchmark over 5 seeds. Negative changes are highlighted lightly [0.01, 0.2). Damaging changes are highlighted moderately [0.2, 0.5). Catastrophic changes are highlighted boldly ( 0.5). Positive changes are similarly highlighted (> 0.01)."
        },
        {
            "title": "Design",
            "content": "Gym - Locomotion DMC - Proprioceptive TD3-Normalized Reward (1k) DMC - Visual Reward (1k) Atari - 1M Human-Normalized"
        },
        {
            "title": "Relaxations",
            "content": "Linear value function -1.17 [-1.19, -1.15] Dynamics target -0.10 [-0.17, -0.04] -0.53 [-0.60, -0.46] No target encoder -1.47 [-1.54, -1.39] Revert -0.01 [-0.07, 0.03] Non-linear model -0.58 [-0.59, -0.56] -0.15 [-0.15, -0.15] -0.35 [-0.35, -0.34] -0.72 [-0.73, -0.72] -0.00 [-0.02, 0.01] -0.41 [-0.42, -0.39] -0.05 [-0.05, -0.04] -0.15 [-0.15, -0.15] -0.52 [-0.52, -0.51] -0.01 [-0.02, -0.00] -1.35 [-1.41, -1.29] -0.38 [-0.81, 0.05] -0.86 [-0.89, -0.83] -1.69 [-1.70, -1.67] -0.07 [-0.32, 0.18]"
        },
        {
            "title": "MSE reward loss\nNo reward scaling\nNo min\nNo LAP\nNo MR",
            "content": "0.10 [-0.02, 0.19] -0.04 [-0.09, 0.02] -0.09 [-0.16, -0.01] -0.10 [-0.24, -0.00] -0.56 [-0.69, -0.43] -0.06 [-0.08, -0.05] -0.01 [-0.02, 0.00] -0.01 [-0.02, 0.01] 0.00 [-0.00, 0.01] -0.19 [-0.19, -0.18] -0.05 [-0.07, -0.04] -0.00 [-0.01, 0.01] 0.00 [-0.01, 0.01] -0.01 [-0.02, -0.01] -0.07 [-0.09, -0.03] -0.79 [-0.86, -0.73] 0.18 [-0.25, 0.56] 0.13 [-0.10, 0.58] -0.13 [-0.38, 0.14] -0.78 [-0.88, -0.69]"
        },
        {
            "title": "Loss functions",
            "content": "1-step return No unroll -0.33 [-0.46, -0.21] 0.07 [0.01, 0.14] -0.04 [-0.05, -0.02] -0.01 [-0.01, -0.00] -0.03 [-0.03, -0.02] -0.04 [-0.06, -0.01] -0.70 [-0.81, -0.59] -0.33 [-0.41, -0.28]"
        },
        {
            "title": "Horizons",
            "content": "Relaxations. In Section 4.1, we outlined loss (Equation 9) that, if globally minimized, would provide features that are linear with the true value function. MR.Q in practice relaxes this theoretical result by modifying the loss and using non-linear value function. In Linear value function, we replace the non-linear value function with linear function. In Dynamics target, we replace the state embedding dynamics target with state-action embedding zsa determined from the target state-action encoder gÏ. In No target encoder, we use the current encoder to generate the dynamics target zsa, and jointly optimize it within the encoder loss. In Revert, we consider all of the aforementioned changes simultaneously, using linear value functions and setting the dynamics target as state-action embedding determined by the current encoder. In Non-linear model, we replace the linear MDP predictor with individual networks that predict each component separately from zsa. Loss functions. MR.Qs loss functions use several unconventional choices. In MSE reward loss, we replace the categorical loss function on the predicted reward in Equation 15 with the meansquared error (MSE). In No reward scaling, we remove the reward scaling in Equation 19, setting = = 1. In No min, we take the mean over the target value functions instead of the minimum in Equation 19. In No LAP, we remove prioritized sampling (Fujimoto et al., 2020) and use the MSE instead of the Huber loss in the value update. Lastly, in No MR, we remove model-based representation learning and train the encoder end-to-end with the value function. Horizons. Finally, we consider the role of extended predictions. In 1-step return, we remove multi-step value predictions and use TD learning. In No unroll, we remove the dynamics unrolling in Equation 14, by setting the encoder horizon HEnc = 1. 9 Published as conference paper at ICLR 2025 Discussion. The results of our design study show the benefit of balancing theory with practical relaxations. The experiments further validate our design choices and hyperparameters. We highlight two results in particular: (1) increasing the model capacity in the non-linear model experiment, does not improve performance. This outcome suggests that maintaining an approximately linear relationship with the value function can be more impactful than increased capacity. (2) Our study also reveals key distinction between the Gym and Atari benchmarkswhile the MSE reward loss and No unroll variants offer moderate performance gains in Gym, they significantly degrade performance in Atari. This discrepancy highlights how hyperparameters can overfit to individual benchmarks, emphasizing the importance of evaluating algorithms across multiple benchmarks."
        },
        {
            "title": "6 DISCUSSION AND CONCLUSION",
            "content": "This paper introduces MR.Q, general model-free deep RL algorithm that achieves strong performance across diverse benchmarks and environments. Drawing inspiration from the theory of model-based representation learning, MR.Q demonstrates that model-free deep RL is promising avenue for building general-purpose algorithms that achieve high performance across environments, while being simpler and less expensive than model-based alternatives. Our work also reveals insights on which design choices matter when building general-purpose model-free deep RL algorithms and how common benchmarks respond to these design choices. Model-based and model-free RL. MR.Q integrates model-based objectives with model-free backbone during training, effectively blurring the boundary between traditional model-based and model-free RL. While MR.Q could be extended to the model-based setting by incorporating planning or simulated trajectories with the state-action encoder, these components can add significant execution time and increase the overall complexity and tuning required by method. Moreover, the performance of MR.Q in these common RL benchmarks demonstrates that these model-based components may be simply unnecessarysuggesting that the representation itself could be the most valuable aspect of model-based learning, even in methods that do use planning. This argument is echoed by DreamerV3 and TD-MPC2, which rely on short planning horizons and trajectory generation, while including both value functions and traditional model-free policy updates. As such, it may be necessary to examine more complex settings, to reliably see benefit from model-based search or planning, e.g., (Silver et al., 2016). Universality of RL benchmarks. Our results demonstrate that there is striking lack of positive transfer between benchmarks. For example, despite the similarities in tasks and the same underlying MuJoCo simulator, the top performers in Gym and DMC fail to replicate their success on the opposing benchmark. Similarly, although DreamerV3 excels at Atari, these performance benefits do not translate to continuous control environments, underperforming TD3 in Gym and outright failing to learn the Dog and Humanoid tasks in DMC (see Appendix C). These findings show the limitations of single-benchmark evaluations, indicating that success on one benchmark may not translate easily to others, and highlights the need for more comprehensive benchmarks. Limitations. MR.Q is only the first step towards new generation of general-purpose model-free deep RL algorithms. Many challenges remains for fully general algorithm. In particular, MR.Q is not equipped to handle settings such as hard exploration tasks or non-Markovian environments. Another limitation is our evaluation only considers standard RL benchmarks. Although this allows direct comparison with other methods, established algorithms such as PPO have demonstrated their effectiveness in highly unique settings, such as team video games (Berner et al., 2019), drone racing (Kaufmann et al., 2023), and large language models (Achiam et al., 2023; Touvron et al., 2023). To demonstrate similar versatility, new algorithms must undergo the same rigorous testing across range of tasks that is beyond the scope of any single study. As the community continues to push the boundaries of what is possible with deep RL, we believe that building simpler general-purpose algorithms has the potential to make this technology more accessible to wider audience, ultimately enabling users to train agents with ease. Perhaps one daywith just the click of button. 10 Published as conference paper at ICLR"
        },
        {
            "title": "ACKNOWLEDGMENTS",
            "content": "We would like to thank Brandon Amos, Mikhael Henaff, Luis Pineda, Paria Rashidinejad, and Qinqing Zheng for insightful discussions and comments."
        },
        {
            "title": "REFERENCES",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Alekh Agarwal, Sham Kakade, Akshay Krishnamurthy, and Wen Sun. Flambe: Structural complexity and representation learning of low rank mdps. Advances in neural information processing systems, 33:2009520107, 2020. Mahmoud Assran, Quentin Duval, Ishan Misra, Piotr Bojanowski, Pascal Vincent, Michael Rabbat, Yann LeCun, and Nicolas Ballas. Self-supervised learning from images with joint-embedding In Proceedings of the IEEE/CVF Conference on Computer Vision and predictive architecture. Pattern Recognition, pp. 1561915629, 2023. Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. Thomas Back. Evolutionary algorithms in theory and practice: evolution strategies, evolutionary programming, genetic algorithms. Oxford university press, 1996. Adri`a Puigdom`enech Badia, Bilal Piot, Steven Kapturowski, Pablo Sprechmann, Alex Vitvitskyi, Zhaohan Daniel Guo, and Charles Blundell. Agent57: Outperforming the atari human benchmark. In International Conference on Machine Learning, pp. 507517. PMLR, 2020. Leemon Baird. Residual algorithms: Reinforcement learning with function approximation. In Machine Learning Proceedings 1995, pp. 3037. Elsevier, 1995. Gabriel Barth-Maron, Matthew Hoffman, David Budden, Will Dabney, Dan Horgan, Dhruva TB, Alistair Muldal, Nicolas Heess, and Timothy Lillicrap. Distributional policy gradients. International Conference on Learning Representations, 2018. Marc Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environment: An evaluation platform for general agents. Journal of Artificial Intelligence Research, 47: 253279, 2013. Richard Bellman. markovian decision process. Journal of mathematics and mechanics, pp. 679 684, 1957. Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemyslaw Debiak, Christy Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, et al. Dota 2 with large scale deep reinforcement learning. arXiv preprint arXiv:1912.06680, 2019. Johan Bjorck, Carla Gomes, and Kilian Weinberger. Is high variance unavoidable in rl? case study in continuous control. In International Conference on Learning Representations, 2021. Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym, 2016. Pablo Samuel Castro. Scalable methods for computing state similarity in deterministic markov decision processes. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pp. 1006910076, 2020. Pablo Samuel Castro, Subhodeep Moitra, Carles Gelada, Saurabh Kumar, and Marc BellearXiv preprint mare. Dopamine: research framework for deep reinforcement learning. arXiv:1812.06110, 2018. 11 Published as conference paper at ICLR Pablo Samuel Castro, Tyler Kastner, Prakash Panangaden, and Mark Rowland. Mico: Improved representations via sampling-based state similarity for markov decision processes. Advances in Neural Information Processing Systems, 34:3011330126, 2021. Wesley Chung, Somjit Nath, Ajin Joseph, and Martha White. Two-timescale networks for nonlinear value function approximation. In International Conference on Learning Representations, 2019. Andre Cianflone, Zafarali Ahmed, Riashat Islam, Avishek Joey Bose, and William Hamilton. Discrete off-policy policy gradient using continuous relaxations, 2019. Djork-Arne Clevert, Thomas Unterthiner, and Sepp Hochreiter. Fast and accurate deep network learning by exponential linear units (elus). arXiv preprint arXiv:1511.07289, 2015. Peter Dayan. Improving generalization for temporal difference learning: The successor representation. Neural Computation, 5(4):613624, 1993. Logan Engstrom, Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Firdaus Janoos, Larry Rudolph, and Aleksander Madry. Implementation matters in deep rl: case study on ppo and trpo. In International Conference on Learning Representations, 2020. Norm Ferns, Prakash Panangaden, and Doina Precup. Metrics for finite markov decision processes. In UAI, volume 4, pp. 162169, 2004. Norm Ferns, Prakash Panangaden, and Doina Precup. Bisimulation metrics for continuous markov decision processes. SIAM Journal on Computing, 40(6):16621714, 2011. Chelsea Finn, Xin Yu Tan, Yan Duan, Trevor Darrell, Sergey Levine, and Pieter Abbeel. Deep spatial autoencoders for visuomotor learning. In 2016 IEEE International Conference on Robotics and Automation (ICRA), pp. 512519. IEEE, 2016. Scott Fujimoto, Herke van Hoof, and David Meger. Addressing function approximation error in actor-critic methods. In International Conference on Machine Learning, volume 80, pp. 1587 1596. PMLR, 2018. Scott Fujimoto, David Meger, and Doina Precup. An equivalence between loss functions and nonuniform sampling in experience replay. Advances in Neural Information Processing Systems, 33, 2020. Scott Fujimoto, David Meger, and Doina Precup. deep reinforcement learning approach to marginalized importance sampling with the successor representation. In Proceedings of the 38th International Conference on Machine Learning, volume 139, pp. 35183529. PMLR, 2021. Scott Fujimoto, David Meger, Doina Precup, Ofir Nachum, and Shixiang Shane Gu. Why should trust you, bellman? The Bellman error is poor replacement for value error. In International Conference on Machine Learning, volume 162, pp. 69186943. PMLR, 2022. Scott Fujimoto, Wei-Di Chang, Edward J. Smith, Shixiang Shane Gu, Doina Precup, and David Meger. For SALE: State-action representation learning for deep reinforcement learning. In Thirtyseventh Conference on Neural Information Processing Systems, 2024. Carles Gelada, Saurabh Kumar, Jacob Buckman, Ofir Nachum, and Marc Bellemare. Deepmdp: Learning continuous latent space models for representation learning. In International Conference on Machine Learning, pp. 21702179. PMLR, 2019. Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In Proceedings of the thirteenth international conference on artificial intelligence and statistics, pp. 249256. JMLR Workshop and Conference Proceedings, 2010. Jean-Bastien Grill, Florian Strub, Florent Altche, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your own latent-a new approach to self-supervised learning. Advances in neural information processing systems, 33:2127121284, 2020. 12 Published as conference paper at ICLR Zhaohan Daniel Guo, Bernardo Avila Pires, Bilal Piot, Jean-Bastien Grill, Florent Altche, Remi Munos, and Mohammad Gheshlaghi Azar. Bootstrap latent-predictive representations for multitask reinforcement learning. In International Conference on Machine Learning, pp. 38753886. PMLR, 2020. Zhaohan Daniel Guo, Shantanu Thakoor, Miruna PËÄ±slar, Bernardo Avila Pires, Florent Altche, Corentin Tallec, Alaa Saade, Daniele Calandriello, Jean-Bastien Grill, Yunhao Tang, et al. Byolexplore: Exploration by bootstrapped prediction. Advances in neural information processing systems, 35:3185531870, 2022. David Ha and Jurgen Schmidhuber. World models. arXiv preprint arXiv:1803.10122, 2018. Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with stochastic actor. In International Conference on Machine Learning, volume 80, pp. 18611870. PMLR, 2018. Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and James Davidson. Learning latent dynamics for planning from pixels. In International conference on machine learning, pp. 25552565. PMLR, 2019. Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy Lillicrap. Mastering diverse domains through world models. arXiv preprint arXiv:2301.04104, 2023. Nicklas Hansen, Hao Su, and Xiaolong Wang. Td-mpc2: Scalable, robust world models for continuous control. In The Twelfth International Conference on Learning Representations, 2024. Nicklas Hansen, Hao Su, and Xiaolong Wang. Temporal difference learning for model predictive control. In International Conference on Machine Learning, pp. 83878406. PMLR, 2022. Charles R. Harris, K. Jarrod Millman, Stefan J. van der Walt, Ralf Gommers, Pauli Virtanen, David Cournapeau, Eric Wieser, Julian Taylor, Sebastian Berg, Nathaniel J. Smith, Robert Kern, Matti Picus, Stephan Hoyer, Marten H. van Kerkwijk, Matthew Brett, Allan Haldane, Jaime Fernandez del RÄ±o, Mark Wiebe, Pearu Peterson, Pierre Gerard-Marchant, Kevin Sheppard, Tyler Reddy, Warren Weckesser, Hameer Abbasi, Christoph Gohlke, and Travis E. Oliphant. Array programming with NumPy. Nature, 585(7825):357362, September 2020. Ammar Haydari and Yasin YÄ±lmaz. Deep reinforcement learning for intelligent transportation systems: survey. IEEE Transactions on Intelligent Transportation Systems, 23(1):1132, 2020. Matteo Hessel, Joseph Modayil, Hado Van Hasselt, Tom Schaul, Georg Ostrovski, Will Dabney, Dan Horgan, Bilal Piot, Mohammad Azar, and David Silver. Rainbow: Combining improvements in deep reinforcement learning. In Thirty-second AAAI conference on artificial intelligence, 2018. Shengyi Huang, Rousslan Fernand Julien Dossa, Antonin Raffin, Anssi Kanervisto, and Weixun In ICLR Blog Track, Wang. The 37 implementation details of proximal policy optimization. 2022. Julian Ibarz, Jie Tan, Chelsea Finn, Mrinal Kalakrishnan, Peter Pastor, and Sergey Levine. How to train your robot with deep reinforcement learning: lessons we have learned. The International Journal of Robotics Research, 40(4-5):698721, 2021. Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. In International Conference on Learning Representations, 2017. Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael Jordan. Provably efficient reinforcement learning with linear function approximation. In Conference on learning theory, pp. 21372143. PMLR, 2020. Maximilian Karl, Maximilian Soelch, Justin Bayer, and Patrick van der Smagt. Deep variational bayes filters: Unsupervised learning of state space models from raw data. stat, 1050:3, 2017. Elia Kaufmann, Leonard Bauersfeld, Antonio Loquercio, Matthias Muller, Vladlen Koltun, and Davide Scaramuzza. Champion-level drone racing using deep reinforcement learning. Nature, 620(7976):982987, 2023. 13 Published as conference paper at ICLR Kyungsoo Kim, Jeongsoo Ha, and Yusung Kim. Self-predictive dynamics for generalization of vision-based reinforcement learning. In IJCAI, pp. 31503156, 2022. Arsenii Kuznetsov, Pavel Shvechikov, Alexander Grishin, and Dmitry Vetrov. Controlling overestimation bias with truncated mixture of continuous distributional quantile critics. In International Conference on Machine Learning, pp. 55565566. PMLR, 2020. Alex Lee, Anusha Nagabandi, Pieter Abbeel, and Sergey Levine. Stochastic latent actor-critic: Deep reinforcement learning with latent variable model. Advances in Neural Information Processing Systems, 33:741752, 2020. Nir Levine, Tom Zahavy, Daniel Mankowitz, Aviv Tamar, and Shie Mannor. Shallow updates for deep reinforcement learning. Advances in Neural Information Processing Systems, 30, 2017. Timothy Lillicrap, Jonathan Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971, 2015. Michael Littman and Richard Sutton. Predictive representations of state. Advances in neural information processing systems, 14, 2001. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2019. Ryan Lowe, Yi Wu, Aviv Tamar, Jean Harb, OpenAI Pieter Abbeel, and Igor Mordatch. Multiagent actor-critic for mixed cooperative-competitive environments. Advances in neural information processing systems, 30, 2017. Nguyen Cong Luong, Dinh Thai Hoang, Shimin Gong, Dusit Niyato, Ping Wang, Ying-Chang Liang, and Dong In Kim. Applications of deep reinforcement learning in communications and networking: survey. IEEE communications surveys & tutorials, 21(4):31333174, 2019. Marlos Machado, Marc Bellemare, Erik Talvitie, Joel Veness, Matthew Hausknecht, and Michael Bowling. Revisiting the arcade learning environment: Evaluation protocols and open problems for general agents. Journal of Artificial Intelligence Research, 61:523562, 2018. Trevor McInroe, Lukas Schafer, and Stefano Albrecht. Learning temporally-consistent representations for data-efficient reinforcement learning. arXiv preprint arXiv:2110.04935, 2021. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei Rusu, Joel Veness, Marc Bellemare, Alex Graves, Martin Riedmiller, Andreas Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529533, 2015. Jelle Munk, Jens Kober, and Robert BabuËska. Learning state representation for deep actor-critic control. In 2016 IEEE 55th Conference on Decision and Control (CDC), pp. 46674673. IEEE, 2016. Tianwei Ni, Benjamin Eysenbach, Erfan Seyedsalehi, Michel Ma, Clement Gehring, Aditya Mahajan, and Pierre-Luc Bacon. Bridging state and history representations: Understanding selfpredictive rl. In The Twelfth International Conference on Learning Representations, 2024. Junhyuk Oh, Xiaoxiao Guo, Honglak Lee, Richard Lewis, and Satinder Singh. Action-conditional video prediction using deep networks in atari games. Advances in neural information processing systems, 28, 2015. Ronald Parr, Lihong Li, Gavin Taylor, Christopher Painter-Wakefield, and Michael Littman. An analysis of linear models, linear value-function approximation, and feature selection for reinforcement learning. In Proceedings of the 25th international conference on Machine learning, pp. 752759, 2008. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, highperformance deep learning library. In Advances in Neural Information Processing Systems, pp. 80248035, 2019. 14 Published as conference paper at ICLR 2025 Antonin Raffin, Ashley Hill, Adam Gleave, Anssi Kanervisto, Maximilian Ernestus, and Noah Dormann. Stable-baselines3: Reliable reinforcement learning implementations. Journal of Machine Learning Research, 22(268):18, 2021. URL http://jmlr.org/papers/v22/20-1364.html. Balaraman Ravindran. An algebraic approach to abstraction in reinforcement learning. University of Massachusetts Amherst, 2004. Balaraman Ravindran and Andrew Barto. Model minimization in hierarchical reinforcement learning. In Abstraction, Reformulation, and Approximation: 5th International Symposium, SARA 2002 Kananaskis, Alberta, Canada August 24, 2002 Proceedings 5, pp. 196211. Springer, 2002. Ingo Rechenberg. Evolutionsstrategien. In Simulationsmethoden in der Medizin und Biologie: Workshop, Hannover, 29. Sept.1. Okt. 1977, pp. 83114. Springer, 1978. Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov, Gabriel Barth-maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, Tom Eccles, Jake Bruce, Ali Razavi, Ashley Edwards, Nicolas Heess, Yutian Chen, Raia Hadsell, Oriol Vinyals, Mahyar Bordbar, and Nando de Freitas. generalist agent. Transactions on Machine Learning Research, 2022. ISSN 2835-8856. Tongzheng Ren, Tianjun Zhang, Csaba Szepesvari, and Bo Dai. free lunch from the noise: Provable and practical exploration for representation learning. In Uncertainty in Artificial Intelligence, pp. 16861696. PMLR, 2022. Tongzheng Ren, Chenjun Xiao, Tianjun Zhang, Na Li, Zhaoran Wang, Dale Schuurmans, Bo Dai, et al. Latent variable representation for reinforcement learning. In The Eleventh International Conference on Learning Representations, 2023. Sahand Rezaei-Shoshtari, Rosie Zhao, Prakash Panangaden, David Meger, and Doina Precup. Continuous mdp homomorphisms and homomorphic policy gradient. In Advances in Neural Information Processing Systems, 2022. Reuven Rubinstein. Optimization of computer simulation models with rare events. European Journal of Operational Research, 99(1):89112, 1997. Gavin Rummery and Mahesan Niranjan. On-line Q-learning using connectionist systems, volume 37. University of Cambridge, Department of Engineering Cambridge, UK, 1994. Tim Salimans, Jonathan Ho, Xi Chen, Szymon Sidor, and Ilya Sutskever. Evolution strategies as scalable alternative to reinforcement learning. arXiv preprint arXiv:1703.03864, 2017. Aidan Scannell, Kalle Kujanpaa, Yi Zhao, Mohammadreza Nakhaei, Arno Solin, and Joni Pajarinen. iqrlimplicitly quantized representations for sample-efficient reinforcement learning. arXiv preprint arXiv:2406.02696, 2024. Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay. In International Conference on Learning Representations, Puerto Rico, 2016. Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, et al. Mastering atari, go, chess and shogi by planning with learned model. Nature, 588(7839):604609, 2020. Julian Schrittwieser, Thomas Hubert, Amol Mandhane, Mohammadamin Barekatain, Ioannis Antonoglou, and David Silver. Online and offline reinforcement learning by planning with learned model. Advances in Neural Information Processing Systems, 34:2758027591, 2021. John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In International Conference on Machine Learning, pp. 18891897, 2015. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. 15 Published as conference paper at ICLR 2025 Max Schwarzer, Ankesh Anand, Rishab Goel, Devon Hjelm, Aaron Courville, and Philip Bachman. Data-efficient reinforcement learning with self-predictive representations. In International Conference on Learning Representations, 2020. Max Schwarzer, Johan Samir Obando Ceron, Aaron Courville, Marc Bellemare, Rishabh Agarwal, and Pablo Samuel Castro. Bigger, better, faster: Human-level atari with human-level efficiency. In International Conference on Machine Learning, pp. 3036530380. PMLR, 2023. Younggyo Seo, Kimin Lee, Stephen James, and Pieter Abbeel. Reinforcement learning with In International Conference on Machine Learning, pp. action-free pre-training from videos. 1956119579. PMLR, 2022. Dmitry Shribak, Chen-Xiao Gao, Yitong Li, Chenjun Xiao, and Bo Dai. Diffusion spectral representation for reinforcement learning. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Riedmiller. Deterministic policy gradient algorithms. In International Conference on Machine Learning, pp. 387395, 2014. David Silver, Aja Huang, Chris Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural networks and tree search. Nature, 529(7587):484489, 2016. Ghada Sokar, Rishabh Agarwal, Pablo Samuel Castro, and Utku Evci. The dormant neuron phenomenon in deep reinforcement learning. In International Conference on Machine Learning, pp. 3214532168. PMLR, 2023. Zhao Song, Ronald Parr, Xuejun Liao, and Lawrence Carin. Linear feature encoding for reinforcement learning. Advances in neural information processing systems, 29, 2016. Richard Sutton. Learning to predict by the methods of temporal differences. Machine learning, 3 (1):944, 1988. Richard Sutton and Andrew Barto. Reinforcement Learning: An Introduction, volume 1. MIT press Cambridge, 1998. Richard Sutton, David McAllester, Satinder Singh, and Yishay Mansour. Policy gradient methods for reinforcement learning with function approximation. Advances in neural information processing systems, 12, 1999. Yunhao Tang, Zhaohan Daniel Guo, Pierre Harvey Richemond, Bernardo Avila Pires, Yash Chandak, Remi Munos, Mark Rowland, Mohammad Gheshlaghi Azar, Charline Le Lan, Clare Lyle, et al. Understanding self-predictive learning for reinforcement learning. In International Conference on Machine Learning, pp. 3363233656. PMLR, 2023. Yuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego de Las Casas, David Budden, Abbas Abdolmaleki, Josh Merel, Andrew Lefrancq, et al. Deepmind control suite. arXiv preprint arXiv:1801.00690, 2018. Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: physics engine for model-based control. In IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 50265033. IEEE, 2012. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. Mark Towers, Ariel Kwiatkowski, Jordan Terry, John Balis, Gianluca De Cola, Tristan Deleu, Manuel Goulao, Andreas Kallinteris, Markus Krimmel, Arjun KG, et al. Gymnasium: standard interface for reinforcement learning environments. arXiv preprint arXiv:2407.17032, 2024. 16 Published as conference paper at ICLR Elise van der Pol, Thomas Kipf, Frans Oliehoek, and Max Welling. Plannable approximations to mdp homomorphisms: Equivariance under actions. In Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems, pp. 14311439, 2020a. Elise van der Pol, Daniel Worrall, Herke van Hoof, Frans Oliehoek, and Max Welling. Mdp homomorphic networks: Group symmetries in reinforcement learning. Advances in Neural Information Processing Systems, 33:41994210, 2020b. Herke Van Hoof, Nutan Chen, Maximilian Karl, Patrick van der Smagt, and Jan Peters. Stable reinforcement learning with autoencoders for tactile and visual data. In 2016 IEEE/RSJ international conference on intelligent robots and systems (IROS), pp. 39283934. IEEE, 2016. Guido Van Rossum and Fred Drake Jr. Python tutorial. Centrum voor Wiskunde en Informatica Amsterdam, The Netherlands, 1995. Shengjie Wang, Shaohuai Liu, Weirui Ye, Jiacheng You, and Yang Gao. Efficientzero v2: Mastering discrete and continuous control with limited data. arXiv preprint arXiv:2403.00564, 2024. Ziyu Wang, Tom Schaul, Matteo Hessel, Hado Hasselt, Marc Lanctot, and Nando Freitas. Dueling network architectures for deep reinforcement learning. In International Conference on Machine Learning, pp. 19952003, 2016. Christopher John Cornish Hellaby Watkins. Learning from delayed rewards. PhD thesis, Kings College, Cambridge, 1989. Manuel Watter, Jost Springenberg, Joschka Boedecker, and Martin Riedmiller. Embed to control: locally linear latent dynamics model for control from raw images. Advances in neural information processing systems, 28, 2015. Ronald Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 8(3-4):229256, 1992. Denis Yarats, Rob Fergus, Alessandro Lazaric, and Lerrel Pinto. Mastering visual continuous control: Improved data-augmented reinforcement learning. In International Conference on Learning Representations, 2022. Weirui Ye, Shaohuai Liu, Thanard Kurutach, Pieter Abbeel, and Yang Gao. Mastering atari games with limited data. Advances in neural information processing systems, 34:2547625488, 2021. Tao Yu, Cuiling Lan, Wenjun Zeng, Mingxiao Feng, Zhizheng Zhang, and Zhibo Chen. Playvirtual: Augmenting cycle-consistent virtual trajectories for reinforcement learning. Advances in Neural Information Processing Systems, 34:52765289, 2021. Tao Yu, Zhizheng Zhang, Cuiling Lan, Yan Lu, and Zhibo Chen. Mask-based latent reconstruction for reinforcement learning. Advances in Neural Information Processing Systems, 35:25117 25131, 2022. Hongyu Zang, Xin Li, and Mingzhong Wang. Simsr: Simple distance-based state representations for deep reinforcement learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pp. 89979005, 2022. Amy Zhang, Harsh Satija, and Joelle Pineau. Decoupling dynamics and reward for transfer learning. arXiv preprint arXiv:1804.10689, 2018. Amy Zhang, Rowan Thomas McAllister, Roberto Calandra, Yarin Gal, and Sergey Levine. Learning invariant representations for reinforcement learning without reconstruction. In International Conference on Learning Representations, 2020. Tianjun Zhang, Tongzheng Ren, Mengjiao Yang, Joseph Gonzalez, Dale Schuurmans, and Bo Dai. Making linear mdps practical via contrastive representation learning. In International Conference on Machine Learning, pp. 2644726466. PMLR, 2022. Yi Zhao, Wenshuai Zhao, Rinu Boney, Juho Kannala, and Joni Pajarinen. Simplified temporal consistency reinforcement learning. In International Conference on Machine Learning, pp. 42227 42246. PMLR, 2023. Published as conference paper at ICLR 2025 Ruijie Zheng, Xiyao Wang, Yanchao Sun, Shuang Ma, Jieyu Zhao, Huazhe Xu, Hal Daume III, and Furong Huang. Taco: Temporal latent action-driven contrastive loss for visual reinforcement learning. Advances in Neural Information Processing Systems, 36, 2024. Luisa Zintgraf, Sebastian Schulze, Cong Lu, Leo Feng, Maximilian Igl, Kyriacos Shiarlis, Yarin Gal, Katja Hofmann, and Shimon Whiteson. Varibad: Variational bayes-adaptive deep rl via meta-learning. Journal of Machine Learning Research, 22(289):139, 2021. 18 Published as conference paper at ICLR"
        },
        {
            "title": "B Experimental Details",
            "content": "19 23 B.1 Hyperparameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 B.2 Network Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 B.3 Environments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.4 Baselines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28 B.5 Software Versions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "C Complete Main Results",
            "content": "29 C.1 Gym . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 C.2 DMC - Proprioceptive . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30 C.3 DMC - Visual . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.4 Atari . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "D Complete Ablation Results",
            "content": "36 D.1 Gym . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36 D.2 DMC - Proprioceptive . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37 D.3 DMC - Visual . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.4 Atari . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "A PROOFS",
            "content": "Theorem 1. The fixed point of the model-free approach (Equation 4) and the solution of the modelbased approach (Equation 5) are the same. Proof. Let be matrix containing state-action embeddings zsa for each state-action pair (s, a) A. Let be the corresponding matrix of next state-action embeddings zsa. Let be the vector of the corresponding rewards r(s, a). The linear semi-gradient TD update: wt+1 = wt Î±Z (Zwt (R + Î³Z wt)) = wt Î±Z Zwt + Î±Z + Î±Î³Z wt = (I Î±(Z Î³Z ))wt + Î±Z = (I Î±A)wt + Î±B, where = Î³Z and = R. The fixed point of the system: wmf = (I Î±A)wmf + Î±B wmf (I Î±A)wmf = Î±B Î±Awmf = Î±B wmf = A1B. 19 (21) (22) (23) (24) (25) (26) (27) (28) Published as conference paper at ICLR"
        },
        {
            "title": "The least squares solution to Wp and wr",
            "content": "Wp = (Z Z)1 wr = (Z Z)1 R By rolling out Wp and wr, we arrive at model-based solution: = Zwmb = t= Î³tW pwr. Simplify wmb: wmb = t=0 Î³tW pwr wmb = (I Î³Wp)1 wr wmb = (I Î³ (Z Z)1 1 ) (Z Z)1 Z (I Î³ (Z Z)1 ) wmb = (Z Î³Z ) wmb = wmb = A1B wmb = wmf. (29) (30) (31) (32) (33) (34) (35) (36) (37) (38) Theorem 2. The value error of the solution described by Theorem 1 is bounded by the accuracy of the estimated dynamics and reward: VE(s, a) 1 1 Î³ max (s,a)SA (z sawr Ers,a[r] + max wi saWp Es,as,a[zsa]) . (39) Proof. Let be the solution described in Theorem 1, i.e. = wmb = wmf. Let pÏ(s, a) be the discounted state-action visitation distribution according to the policy Ï starting from the state-action pair (s, a). Firstly from Theorem 1, we can show that = (I Î³Wp)1wr (I Î³Wp)w = wr Î³Wpw = wr. 20 (40) (41) (42) (43) (44) (45) (46) (47) (48) (49) (50) (51) (52) (53) (54) (55) (56) (57) (58) (59) Published as conference paper at ICLR 2025 Simplify VE(s, a): VE(s, a) = Q(s, a) QÏ(s, a) = Q(s, a) QÏ(s, a) = Q(s, a) Er,s,a [r + Î³QÏ(s, a)] = Q(s, a) Er,s,a [r + Î³ (Q(s, a) VE(s, a))] = Q(s, a) Er,s,a [r + Î³Q(s, a)] + Î³Es,a [VE(s, a)] sawr + = Q(s, a) Er,s,a [r = Q(s, a) Er,s,a [r sawr + + Î³Es,a [VE(s, a)] saw Er,s,a [r + Î³Es,a [VE(s, a)] saw Er [r + Î³Es,a [VE(s, a)] saw sawr Î³z + Î³Es,a [VE(s, a)] saWpw Er [r sawr] Î³Es,a [z sawr + Î³ (z sa saw saw sawr + sawr + = = = sawr + Î³Q(s, a)] + Î³Es,a [VE(s, a)] sawr + Î³ (z saWpw)] saWpw + saWpw + saWpw)] saWpw + saWpw] sawr] Î³Es,a [z sa saWpw] (w Î³Wpw wr) Er [r = sa + Î³Es,a [VE(s, a)] sawr] Î³Es,a [z saw saWpw] = Er [r = (z sawr Er [r]) + Î³ (z sawr] Î³Es,a [z sa saWp Es,a [z saWpw] + Î³Es,a [VE(s, a)] sa]) + Î³Es,a [VE(s, a)] . Then given the recursive relationship, akin to the Bellman equation (Sutton & Barto, 1998), the value error VE recursively expands to the discounted state-action visitation distribution pÏ. For (Ës, Ëa) A: VE(Ës, Ëa) = 1 1 Î³ E(s,a)pÏ(Ës,Ëa) [(z sawr Ers,a [r]) + Î³ (z saWp Es,as,a [z sa ]) w] . VE(Ës, Ëa) Taking the absolute value: VE(Ës, Ëa) = 1 1 Î³ 1 1 Î³ 1 1 Î³ 1 1 Î³ = max (s,a)SA max (s,a)SA E(s,a)pÏ(Ës,Ëa) [(z sawr Ers,a [r]) + Î³ (z saWp Es,as,a [z sa]) w] E(s,a)pÏ(Ës,Ëa) [z sawr Ers,a [r] + Î³ (z saWp Es,as,a [z sa]) w] (z sawr Ers,a [r] + Î³ (z saWp Es,as,a [z sa]) w) (z sawr Ers,a [r] + max wi saWp Es,as,a [zsa ]) .(60) Theorem 3. Given functions (s) = zs and g(zs, a) = zsa, then if there exists functions Ëp and ËR such that for all (s, a) A: ËR [ ËR(zsa)] = ER [R(s, a)] , Ëp(zszsa) = p(Ëss, a), (61) ËszËs=zs then for any policy Ï where there exists corresponding policy ËÏ(azs) = Ï(as), there exists function ËQ equal to the true value function QÏ over all possible state-action pairs (s, a) A: Furthermore, Equation 61 guarantees the existence of an optimal policy ËÏ(azs) = Ï(as). ËQ(zsa) = QÏ(s, a). (62) Published as conference paper at ICLR 2025 Proof. Let"
        },
        {
            "title": "Then",
            "content": "QÏ (s, a) = ËQh(zsa) = t=0 t=0 Î³tEÏ[R(st, at)s0 = s, a0 = a] Î³tEÏ[ ËR(zstat )s0 = s, a0 = a] QÏ 0 (s, a) = ER[R(s, a)] [ ËR(zsa)] = ËR = ËQ0(zsa). (63) (64) (65) (66) (67) Assuming QÏ (s) = zs. n1 (s, a) = ËQn1(zsa) then noting that Ëp(zzsa) = 0 if that is not in the image of QÏ (s, a) = ER[R(s, a)] + Î³Es,a[QÏ n1 (s, a)] [ ËR(s, a)] + Î³Es,a[ ËQn1(zsa )] [ ËR(s, a)] + Î³ s [ ËR(s, a)] + Î³ zs = ËR = ËR = ËR p(ss, a)Ï(as) ËQn1(zsa) Ëp(zszsa)ËÏ(azs) ËQn1(zsa ) = ËQn(zsa). (68) (69) (70) (71) (72) Thus ËQ(zsa) = limn ËQn(zsa) exists, as ËQn can be defined as function of Ëp, ËR, and ËÏ for all n. Similarly, let Ï be an optimal policy. Repeating the same arguments we see that (s, a) = ER[R(s, a)] + Î³Es,a [QÏ = ER[R(s, a)] + Î³ [ ËR(s, a)] + Î³ zs n1 p(ss, a) max (s, a)] QÏ Ëp(zszsa) max ËQn1(zsa) (s, a) = ËR QÏ (73) (74) (75) n1 a = ËQn(zsa). (76) Thus there exists function ËQ(g(zs, a)) = Q(s, a), consequently, there exists an optimal policy ËÏ(azs) = argmaxa ËQ(s, a). 22 Published as conference paper at ICLR"
        },
        {
            "title": "B EXPERIMENTAL DETAILS",
            "content": "B.1 HYPERPARAMETERS Table 3: MR.Q Hyperparameters. Hyperparameters values are kept fixed across all benchmarks."
        },
        {
            "title": "Hyperparameter",
            "content": "Dynamics loss weight Î»Dynamics Reward loss weight Î»Reward Terminal loss weight Î»Terminal Pre-activation loss weight Î»pre-activ Encoder horizon HEnc Multi-step returns horizon HQ TD3 (Fujimoto et al., 2018) Target policy noise Ï Target policy noise clipping LAP (Fujimoto et al., 2020) Probability smoothing Î± Minimum priority"
        },
        {
            "title": "Value",
            "content": "1 0.1 0.1 1e 5 5 3 (0, 0.22) (0.3, 0.3) 0."
        },
        {
            "title": "Initial random exploration time steps\nExploration noise",
            "content": "10k (0, 0.22)"
        },
        {
            "title": "Common",
            "content": "Discount factor Î³ Replay buffer capacity Mini-batch size Target update frequency Ttarget Replay ratio 0.99 1M 256"
        },
        {
            "title": "Policy Network",
            "content": "Optimizer Learning rate Weight decay zs dim zsa dim za dim (only used within architecture) Hidden dim Activation function Weight initialization Bias initialization Reward bins Reward range AdamW (Loshchilov & Hutter, 2019) 1e 4 1e 4 512 512 256 512 ELU (Clevert et al., 2015) Xavier uniform (Glorot & Bengio, 2010) 0 65 [10, 10] (effective: [22k, 22k])"
        },
        {
            "title": "Optimizer\nLearning rate\nHidden dim\nActivation function\nWeight initialization\nBias initialization\nGradient clip norm",
            "content": "Optimizer Learning rate Hidden dim Activation function Weight initialization Bias initialization Gumbel-Softmax Ï (Jang et al., 2017) AdamW 3e 4 512 ELU Xavier uniform 0 20 AdamW 3e 4 512 ReLU Xavier uniform 0 10 23 Published as conference paper at ICLR 2025 B.2 NETWORK ARCHITECTURE This section describes the networks used in our method using PyTorch code blocks (Paszke et al., 2019). The state encoder and state-action encoder are described as separate networks for clarity but are trained end-to-end as single network. The value and policy networks are trained independently from the encoders."
        },
        {
            "title": "Preamble",
            "content": "1 import torch 2 import torch.nn as nn 3 import torch.nn.functional as 4 from functools import partial 5 6 zs_dim = 512 7 za_dim = 256 8 zsa_dim = 512 9 10 def ln_activ(self, x): 11 12 = F.layer_norm(x, (x.shape[-1],)) return self.activ(x)"
        },
        {
            "title": "State Encoder f Network",
            "content": "For image inputs, four convolutional layers are used, each with 32 output channels, kernel size of 3, strides of (2, 2, 2, 1), and ELU activations (Clevert et al., 2015). The convolutional layers are followed by linear layer taking in the flattened output followed by LayerNorm (Ba et al., 2016) and final ELU activation. For vector inputs, three layer multilayer perceptron (MLP) is used, with hidden dimension 512 and LayerNorm followed by ELU activations after each layer. The resulting state embedding zs is trained end-to-end with the state-action encoder. It is also used downstream by the policy network (without propagating gradients). self.zs_cnn1 = nn.Conv2d(state_channels, 32, 3, stride=2) self.zs_cnn2 = nn.Conv2d(32, 32, 3, stride=2) self.zs_cnn3 = nn.Conv2d(32, 32, 3, stride=2) self.zs_cnn4 = nn.Conv2d(32, 32, 3, stride=1) # Assumes 84 84 input self.zs_lin = nn.Linear(1568, zs_dim) self.zs_mlp1 = nn.Linear(state_dim, 512) self.zs_mlp2 = nn.Linear(512, 512) self.zs_mlp3 = nn.Linear(512, zs_dim) 1 if image_observation_space: 2 3 4 5 6 7 8 else: 9 10 11 12 13 self.activ = F.elu 14 15 def cnn_forward(self, state): 16 17 18 19 20 21 22 23 24 def mlp_forward(self, state): 25 26 state = state/255. - 0.5 zs = self.activ(self.zs_cnn1(state)) zs = self.activ(self.zs_cnn2(zs)) zs = self.activ(self.zs_cnn3(zs)) zs = self.activ(self.zs_cnn4(zs)) zs = zs.reshape(batch_size, 1568) return ln_activ(self.zs_lin(zs)) zs = self.ln_activ(self.zs_mlp1(state)) zs = self.ln_activ(self.zs_mlp2(zs)) return self.ln_activ(self.zs_mlp3(zs)) 24 Published as conference paper at ICLR 2025 State-Action Encoder Network Action input is processed by linear layer followed by an ELU activation. Afterwards, the processed action is concatenated with the state embedding and processed by three layer MLP with hidden dimension 512, and LayerNorm followed by ELU activations after the first two layers. The resulting state-action embedding zsa is used by linear layer to make predictions about reward, the next state embedding, and the terminal signal. It is also used downstream by the value network (without propagating gradients). 1 self.za = nn.Linear(action_dim, za_dim) 2 self.zsa1 = nn.Linear(zs_dim + za_dim, 512) 3 self.zsa2 = nn.Linear(512, 512) 4 self.zsa3 = nn.Linear(512, zsa_dim) 5 self.model = nn.Linear(zsa_dim, output_dim) 6 self.activ = F.elu 7 8 def forward(self, zs, action): 9 10 11 12 13 14 za = self.activ(self.za(action)) zsa = torch.cat([zs, za], 1) zsa = self.ln_activ(self.zsa1(zsa)) zsa = self.ln_activ(self.zsa2(zsa)) zsa = self.zsa3(zsa) return self.model(zsa), zsa"
        },
        {
            "title": "Value Q Networks",
            "content": "The value network is four layer MLP with hidden dimension 512, and LayerNorm followed by ELU activations after the first three layers. Two value networks are used with the same network and forward pass. 1 self.l1 = nn.Linear(zsa_dim, 512) 2 self.l2 = nn.Linear(512, 512) 3 self.l3 = nn.Linear(512, 512) 4 self.l4 = nn.Linear(512, 1) 5 self.activ = F.elu 6 7 def forward(self, zsa): 8 9 10 11 = self.ln_activ(self.l1(zsa)) = self.ln_activ(self.l2(q)) = self.ln_activ(self.l3(q)) return self.l4(q) Policy Ï Network The policy network is three layer MLP with hidden dimension 512, and LayerNorm followed by ReLU activations after the first two layers. For discrete actions, the final activation is the Gumbel Softmax with Ï = 10. For continous actions, the final activation is tanh function. 1 self.l1 = nn.Linear(zs_dim, 512) 2 self.l2 = nn.Linear(hdim, 512) 3 self.l3 = nn.Linear(512, action_dim) 4 self.activ = F.relu 5 6 if discrete_action_space: 7 8 else: 9 10 11 def forward(self, zs): 12 13 14 = self.ln_activ(self.l1(zs)) = self.ln_activ(self.l2(a)) return self.final_activ(self.l3(a)) self.final_activ = torch.tanh self.final_activ = partial(F.gumbel_softmax, tau=10) 25 Published as conference paper at ICLR B.3 ENVIRONMENTS All main experiments were run for 10 seeds (the design study is based on 5 seeds). Evaluations are based on the average performance over 10 episodes, measured every 5k time steps for Gym and DM control and every 100k time steps for Atari. Gym - Locomotion. For the gym locomotion tasks (Todorov et al., 2012; Brockman et al., 2016; Towers et al., 2024), we choose the five most common environments that appear in prior work (Fujimoto et al., 2018; 2024; Haarnoja et al., 2018; Kuznetsov et al., 2020). We use the -v4 version. No preprocessing is applied. When aggregating scores, we use normalize with the TD3 scores obtained from TD7 (Fujimoto et al., 2024): TD3-Normalized(x) = random score TD3 score random score . (77)"
        },
        {
            "title": "Random",
            "content": "TD3 Ant-v4 HalfCheetah-v4 Hopper-v4 Humanoid-v4 Walker2d-v4 -70.288 -289.415 18.791 120.423 2.791 3942 10574 3226 5165 3946 DM Control Suite. For the DM control suite (Tassa et al., 2018), we choose the 28 default environments that appear either in the evaluation of TD-MPC2 or DreamerV3. We omit any custom environments included by the TD-MPC2 authors. The same subset of tasks are used in the evaluation of proprioceptive and visual control. Like prior work, for both observation spaces, we use an action repeat of 2 (Hansen et al., 2024). For visual control, the state (network input) is composed of the previous 3 observations which are resized to 84 84 pixels in RGB format (Tassa et al., 2018). Atari. For the Atari games (Bellemare et al., 2013; Brockman et al., 2016; Towers et al., 2024), we use the 57 games in the Atari-57 benchmark that appears in prior work (Hessel et al., 2018; Schrittwieser et al., 2020; Badia et al., 2020; Hafner et al., 2023). For DQN and Rainbow, two games (Defender and Surround) are missing from the Dopamine framework (Castro et al., 2018) and are omitted. We use the -v5 version. For MR.Q, we use the common preprocessing steps (Mnih et al., 2015; Machado et al., 2018; Castro et al., 2018), where an action repeat of 4 is used and the observations are grayscaled, resized to 84 84 pixels and set to the max between the 3rd and 4th frame. The state (network input) is composed of the previous 4 observations. Consider the 16 frame sequence used by single state, where fi is the ith grayscaled and resized frame and oj is the jth observation set to the max of two frames action a0 action a1 action a2 action a3 f0, f1, f2, f3 o0=max(f2,f3) , f4, f5, f6, f7 o1=max(f6,f7) , f8, f9, f10, f11 o2=max(f10,f11) , f12, f13, f14, f15 o3=max(f14,f15) , (78) then the state is defined as follows: = o0 = max(f2, f3) o1 = max(f6, f7) o2 = max(f10, f11) o3 = max(f14, f15) . When aggregating scores, we normalize with Human scores obtained from (Wang et al., 2016): Human-Normalized(x) = random score Human score random score . (79) (80) 26 Published as conference paper at ICLR"
        },
        {
            "title": "Human",
            "content": "Alien Amidar Assault Asterix Asteroids Atlantis BankHeist BattleZone BeamRider Berzerk Bowling Boxing Breakout Centipede ChopperCommand CrazyClimber Defender (not used) DemonAttack DoubleDunk Enduro FishingDerby Freeway Frostbite Gopher Gravitar Hero IceHockey Jamesbond Kangaroo Krull KungFuMaster MontezumaRevenge MsPacman NameThisGame Phoenix Pitfall Pong PrivateEye Qbert Riverraid RoadRunner Robotank Seaquest Skiing Solaris SpaceInvaders StarGunner Surround (not used) Tennis TimePilot Tutankham UpNDown Venture VideoPinball WizardOfWor YarsRevenge Zaxxon 227.8 5.8 222.4 210.0 719.1 12850.0 14.2 2360.0 363.9 123.7 23.1 0.1 1.7 2090.9 811.0 10780.5 2874.5 152.1 -18.6 0.0 -91.7 0.0 65.2 257.6 173.0 1027.0 -11.2 29.0 52.0 1598.0 258.5 0.0 307.3 2292.3 761.4 -229.4 -20.7 24.9 163.9 1338.5 11.5 2.2 68.4 -17098.1 1236.3 148.0 664.0 -10.0 -23.8 3568.0 11.4 533.4 0.0 16256.9 563.5 3092.9 32.5 7127.7 1719.5 742.0 8503.3 47388.7 29028.1 753.1 37187.5 16926.5 2630.4 160.7 12.1 30.5 12017.0 7387.8 35829.4 18688.9 1971.0 -16.4 860.5 -38.7 29.6 4334.7 2412.5 3351.4 30826.4 0.9 302.8 3035.0 2665.5 22736.3 4753.3 6951.6 8049.0 7242.6 6463.7 14.6 69571.3 13455.0 17118.0 7845.0 11.9 42054.7 -4336.9 12326.7 1668.7 10250.0 6.5 -8.3 5229.2 167.6 11693.2 1187.5 17667.9 4756.5 54576.9 9173.3 27 Published as conference paper at ICLR 2025 B.4 BASELINES DreamerV3. (Hafner et al., 2023). Results for Gym and DMC were obtained by re-running the authors code (https://github.com/danijar/dreamerv3 - Commit 251910d04c9f38dd9dc385775bb0d6efa0e57a95) over 10 seeds, using the author-suggested hyperparameters from the DMC benchmark. Code was modified slightly to match our evaluation protocol. Atari results are based on the authors reported results. DrQ-v2. (Yarats et al., 2022). We use the authors reported results whenever possible. For missing any results, we re-ran the authors code (https://github.com/facebookresearch/drqv2 - Commit c0c650b76c6e5d22a7eb5f2edffd1440fe94f8ef) for 10 seeds. DQN. (Mnih et al., 2015). Results were obtained from the Dopamine framework (Castro et al., 2018). PPO. (Schulman et al., 2017). Results were gathered using Stable Baselines 3 (Raffin et al., 2021) and default hyperparameters. The default MLP policy was used for Gym and DMC-proprioceptive and the default CNN policy was used for DMC-visual and Atari. Rainbow. (Hessel et al., 2018). Results were obtained from the Dopamine framework (Castro et al., 2018). (Hansen et al., 2024). TD-MPC2. Results for DMC were obtained by re-running the authors code on their main branch (https://github.com/nicklashansen/tdmpc2 - Commit 5f6fadec0fec78304b4b53e8171d348b58cac486). As the Gym environments include termination signal, results for Gym were obtained by running their episodic branch (https://github.com/ nicklashansen/tdmpc2/tree/episodic-rl - Commit 3789fcd5b872079ad610fa3299ff47c3a427a04a). All experiments were run for 10 seeds and use the default author-suggested hyperparameters for all tasks. TD7. (Fujimoto et al., 2024). Results for Gym were obtained from the authors. Results for DMC were obtained by re-running the authors code (https://github.com/sfujim/TD7 - Commit c1c280de1513f474488061b4cf39642b75dd84bd) using our setup for DMC. All experiments use 10 seeds and use the default author-suggested hyperparameters from the Gym benchmark. B.5 SOFTWARE VERSIONS Gymnasium 0.29.1 (Towers et al., 2024) MuJoCo 3.2.2 (Todorov et al., 2012) NumPy 2.1.1 (Harris et al., 2020) Python 3.11.8 (Van Rossum & Drake Jr, 1995) PyTorch 2.4.1 (Paszke et al., 2019) 28 Published as conference paper at ICLR"
        },
        {
            "title": "C COMPLETE MAIN RESULTS",
            "content": "C.1 GYM Table 4: Gym - Locomotion final results. Final average performance at 1M time steps over 10 seeds. The [bracketed values] represent 95% bootstrap confidence interval. The aggregate mean, median and interquartile mean (IQM) are computed over the TD3-normalized score (see Appendix B.3). Task Ant HalfCheetah Hopper Humanoid Walker2d Mean Median IQM TD PPO TD-MPC2 DreamerV3 MR.Q 8509 [8164, 8852] 17433 [17284, 17550] 3511 [3245, 3746] 7428 [7300, 7555] 6096 [5535, 6521] 1584 [1355, 1802] 1744 [1525, 2120] 3022 [2587, 3356] 477 [431, 522] 2487 [1875, 3067] 4751 [3012, 6261] 15078 [14050, 16012] 2081 [1233, 2916] 6071 [5767, 6327] 3008 [1659, 4220] 1947 [1121, 2751] 5502 [3887, 7117] 2666 [2071, 3201] 4217 [2791, 5481] 4519 [3746, 5190] 6901 [6261, 7482] 12939 [11663, 13762] 2692 [2131, 3309] 10223 [9929, 10498] 6039 [5644, 6386] 1.57 [1.54, 1.60] 1.55 [1.45, 1.63] 1.54 [1.49, 1.58] 0.45 [0.41, 0.48] 0.41 [0.36, 0.47] 0.41 [0.35, 0.46] 1.04 [0.90, 1.16] 1.18 [0.80, 1.23] 1.05 [0.87, 1.19] 0.76 [0.67, 0.85] 0.81 [0.56, 0.90] 0.72 [0.62, 0.85] 1.46 [1.41, 1.52] 1.53 [1.43, 1.61] 1.50 [1.44, 1.55] MR.Q DreamerV3 TD-MPC"
        },
        {
            "title": "PPO",
            "content": "TD7 Figure 3: Gym - Locomotion learning curves. Results are over 10 seeds. The shaded area captures 95% boostrap confidence interval. 29 Published as conference paper at ICLR 2025 C.2 DMC - PROPRIOCEPTIVE Table 5: DMC - Proprioceptive final results. Final average performance at 500k time steps (1M time steps in the original environment due to action repeat) over 10 seeds. The [bracketed values] represent 95% bootstrap confidence interval. The aggregate mean, median and interquartile mean (IQM) are computed over the default reward. Task TD7 PPO TD-MPC2 DreamerV3 MR.Q 58 [38, 75] acrobot-swingup 983 [981, 985] ball in cup-catch cartpole-balance 999 [998, 1000] cartpole-balance sparse 1000 [1000, 1000] cartpole-swingup cartpole-swingup sparse cheetah-run dog-run dog-stand dog-trot dog-walk finger-spin finger-turn easy finger-turn hard fish-swim hopper-hop hopper-stand humanoid-run humanoid-stand humanoid-walk pendulum-swingup quadruped-run quadruped-walk reacher-easy reacher-hard walker-run walker-stand walker-walk 869 [866, 873] 573 [333, 806] 821 [642, 913] 69 [36, 101] 582 [432, 741] 21 [13, 30] 52 [19, 116] 335 [99, 596] 912 [774, 983] 470 [199, 727] 86 [64, 120] 87 [25, 160] 670 [466, 829] 57 [23, 92] 317 [117, 516] 176 [42, 320] 500 [251, 743] 645 [567, 713] 949 [939, 957] 970 [951, 982] 898 [861, 936] 804 [783, 825] 983 [974, 989] 977 [975, 980] Mean Median IQM 566 [544, 590] 613 [548, 718] 612 [569, 657] 39 [33, 45] 769 [689, 841] 999 [1000, 1000] 1000 [1000, 1000] 776 [661, 853] 391 [159, 625] 269 [247, 295] 26 [26, 28] 129 [122, 139] 31 [30, 34] 40 [37, 43] 459 [420, 497] 182 [153, 211] 58 [35, 79] 103 [84, 128] 10 [0, 23] 128 [56, 216] 0 [1, 1] 5 [5, 6] 1 [1, 2] 115 [70, 164] 144 [122, 170] 122 [103, 142] 367 [188, 558] 125 [40, 234] 97 [91, 104] 431 [363, 495] 283 [253, 312] 254 [241, 267] 127 [112, 145] 154 [135, 167] 584 [551, 615] 984 [982, 986] 996 [995, 998] 1000 [1000, 1000] 875 [870, 880] 845 [839, 849] 917 [915, 920] 265 [166, 342] 506 [266, 715] 407 [265, 530] 486 [240, 704] 986 [986, 988] 979 [975, 983] 947 [916, 977] 659 [615, 706] 425 [368, 500] 952 [944, 958] 181 [121, 231] 658 [506, 745] 754 [725, 791] 846 [830, 862] 942 [938, 947] 963 [959, 967] 983 [980, 986] 960 [936, 979] 854 [851, 859] 991 [990, 994] 981 [979, 984] 783 [769, 797] 896 [893, 899] 868 [860, 880] 230 [193, 266] 968 [965, 973] 998 [997, 1000] 999 [1000, 1000] 736 [591, 838] 702 [560, 792] 699 [655, 744] 4 [4, 5] 22 [20, 27] 10 [6, 17] 17 [15, 21] 666 [577, 763] 906 [883, 927] 864 [812, 900] 813 [808, 819] 116 [66, 165] 747 [669, 806] 0 [1, 1] 5 [5, 6] 1 [1, 2] 774 [740, 802] 130 [92, 169] 193 [137, 243] 966 [964, 970] 919 [864, 955] 510 [430, 588] 941 [934, 948] 898 [875, 919] 530 [520, 539] 700 [644, 741] 577 [557, 594] 567 [523, 616] 981 [979, 984] 999 [999, 1000] 1000 [1000, 1000] 866 [866, 866] 798 [780, 818] 914 [911, 917] 569 [547, 595] 967 [960, 975] 877 [845, 898] 916 [908, 924] 937 [917, 956] 953 [931, 974] 950 [910, 974] 792 [773, 810] 251 [195, 301] 951 [948, 955] 200 [170, 236] 868 [822, 903] 662 [610, 724] 748 [597, 829] 947 [940, 954] 963 [959, 967] 983 [983, 985] 977 [975, 980] 793 [765, 815] 988 [987, 990] 978 [978, 980] 835 [829, 842] 927 [914, 934] 907 [903, 914] 30 Published as conference paper at ICLR 2025 MR.Q DreamerV3 TD-MPC"
        },
        {
            "title": "PPO",
            "content": "TD7 Figure 4: DMC - Proprioceptive learning curves. Time steps consider the number of environment interactions, where 500k time steps equals 1M frames in the original environment. Results are over 10 seeds. The shaded area captures 95% boostrap confidence interval. 31 Published as conference paper at ICLR 2025 C.3 DMC - VISUAL Table 6: DMC - Visual final results. Final average performance at 500k time steps (1M time steps in the original environment due to action repeat) over 10 seeds. The [bracketed values] represent 95% bootstrap confidence interval. The aggregate mean, median and interquartile mean (IQM) are computed over the default reward. Task DrQ-v2 PPO TD-MPC2 DreamerV3 MR.Q 168 [127, 219] acrobot-swingup 909 [821, 973] ball in cup-catch cartpole-balance 993 [990, 996] cartpole-balance sparse 962 [887, 1000] cartpole-swingup 864 [854, 873] cartpole-swingup sparse 774 [741, 805] 728 [701, 753] cheetah-run 10 [9, 12] dog-run 43 [37, 49] dog-stand 14 [11, 18] dog-trot 22 [18, 29] dog-walk 860 [787, 922] finger-spin 503 [399, 615] finger-turn easy 223 [121, 340] finger-turn hard 84 [65, 107] fish-swim 224 [170, 278] hopper-hop hopper-stand 917 [903, 931] humanoid-run humanoid-stand humanoid-walk pendulum-swingup quadruped-run quadruped-walk reacher-easy reacher-hard walker-run walker-stand walker-walk 1 [1, 1] 6 [7, 7] 2 [2, 2] 838 [813, 861] 459 [412, 507] 750 [699, 796] 938 [903, 973] 705 [580, 831] 546 [475, 612] 980 [977, 984] 766 [489, 957] Mean Median IQM 510 [497, 523] 626 [528, 665] 545 [519, 564] 2 [1, 4] 105 [5, 282] 353 [231, 485] 487 [233, 751] 596 [437, 723] 0 [0, 0] 155 [110, 210] 11 [9, 14] 51 [48, 56] 13 [12, 15] 16 [14, 18] 241 [107, 377] 189 [144, 233] 60 [1, 120] 77 [64, 92] 0 [0, 0] 1 [0, 2] 1 [1, 1] 6 [6, 7] 1 [1, 1] 0 [0, 1] 118 [98, 139] 149 [113, 184] 113 [55, 192] 10 [0, 30] 39 [35, 44] 253 [210, 310] 47 [40, 56] 110 [98, 125] 49 [32, 53] 58 [46, 67] 197 [179, 217] 932 [899, 961] 972 [948, 991] 1000 [1000, 1000] 690 [521, 813] 636 [404, 804] 431 [267, 556] 14 [10, 18] 117 [72, 148] 20 [14, 25] 22 [17, 28] 786 [492, 984] 562 [317, 779] 903 [870, 940] 43 [21, 64] 187 [119, 238] 582 [321, 794] 0 [1, 1] 5 [5, 7] 1 [1, 2] 748 [574, 850] 262 [184, 330] 246 [179, 310] 956 [932, 978] 911 [867, 946] 665 [566, 719] 937 [907, 962] 958 [952, 965] 492 [471, 512] 572 [419, 654] 501 [458, 537] 121 [106, 145] 971 [969, 973] 998 [997, 1000] 999 [999, 1000] 725 [603, 807] 547 [351, 726] 618 [576, 661] 9 [6, 14] 61 [30, 92] 14 [13, 16] 11 [11, 12] 656 [544, 765] 491 [447, 542] 494 [401, 571] 90 [84, 96] 205 [125, 287] 888 [875, 900] 1 [1, 1] 5 [5, 7] 1 [2, 2] 761 [709, 807] 328 [255, 397] 316 [260, 379] 735 [678, 796] 338 [227, 461] 669 [615, 708] 969 [966, 973] 942 [936, 949] 463 [452, 475] 493 [420, 532] 452 [430, 473] 287 [254, 316] 977 [975, 980] 999 [999, 999] 1000 [1000, 1000] 868 [860, 875] 797 [777, 816] 775 [752, 807] 60 [44, 80] 216 [201, 232] 65 [55, 79] 77 [71, 83] 965 [938, 982] 953 [927, 974] 932 [905, 957] 79 [68, 93] 270 [230, 315] 852 [703, 930] 1 [1, 2] 7 [7, 8] 2 [2, 3] 829 [816, 842] 498 [476, 522] 833 [797, 867] 979 [978, 982] 965 [945, 977] 615 [571, 655] 980 [977, 985] 970 [968, 973] 602 [595, 608] 813 [779, 822] 692 [678, 703] 32 Published as conference paper at ICLR MR.Q DreamerV3 TD-MPC"
        },
        {
            "title": "PPO",
            "content": "DrQ-v2 Figure 5: DMC - Visual learning curves. Time steps consider the number of environment interactions, where 500k time steps equals 1M frames in the original environment. Results are over 10 seeds. The shaded area captures 95% boostrap confidence interval. 33 Published as conference paper at ICLR 2025 C.4 ATARI Table 7: Atari final results. Final average performance at 2.5M time steps (10M time steps in the original environment due to action repeat) over 10 seeds. The [bracketed values] represent 95% bootstrap confidence interval. The aggregate mean, median and interquartile mean (IQM) are computed over the human-normalized score. Task DQN Rainbow PPO DreamerV3 MR.Q Alien Amidar Assault Asterix Asteroids Atlantis BankHeist BattleZone BeamRider Berzerk Bowling Boxing Breakout Centipede ChopperCommand CrazyClimber Defender DemonAttack DoubleDunk Enduro FishingDerby Freeway Frostbite Gopher Gravitar Hero IceHockey Jamesbond Kangaroo Krull KungFuMaster MontezumaRevenge MsPacman NameThisGame Phoenix Pitfall Pong PrivateEye Qbert Riverraid RoadRunner Robotank Seaquest Skiing Solaris SpaceInvaders StarGunner Surround Tennis TimePilot Tutankham UpNDown Venture VideoPinball WizardOfWor YarsRevenge Zaxxon Mean Median IQM 925 [879, 968] 178 [169, 186] 988 [957, 1011] 2381 [2313, 2469] 423 [408, 436] 7365 [6893, 7742] 474 [448, 493] 3598 [3235, 3878] 869 [728, 1065] 488 [466, 508] 29 [27, 32] 37 [31, 44] 21 [19, 25] 2832 [2418, 3215] 997 [971, 1022] 64611 [46203, 78709] 116954 [111371, 122032] 1503 [1282, 1690] -18 [-20, -18] 589 [567, 617] -42 [-62, -17] 8 [0, 19] 269 [238, 294] 1470 [1316, 1590] 167 [153, 183] 2679 [2404, 2945] -9 [-10, -9] 47 [42, 52] 539 [525, 553] 4229 [3942, 4490] 15997 [13182, 18813] 0 [0, 0] 2187 [2121, 2247] 4000 [3814, 4187] 4948 [4236, 5627] -60 [-89, -35] -4 [-14, 3] 118 [78, 181] 1658 [1246, 2139] 3198 [3167, 3222] 27980 [27269, 28692] 4 [4, 5] 299 [277, 318] -19568 [-19793, -19362] 1645 [1480, 1804] 663 [651, 675] 692 [662, 719] 3488 [1032, 8241] -21 [-24, -19] 1539 [1479, 1613] 112 [97, 123] 7669 [7116, 8147] 25 [6, 45] 5129 [4611, 5649] 481 [396, 542] 9426 [9177, 9656] 112 [15, 230] 0.25 [0.24, 0.26] 0.12 [0.10, 0.12] 0.17 [0.16, 0.17] 1220 [1191, 1268] 301 [280, 330] 1430 [1392, 1475] 2699 [2598, 2783] 754 [711, 816] 80837 [51139, 126780] 895 [889, 901] 20209 [17157, 22375] 5982 [5664, 6268] 443 [413, 484] 44 [36, 52] 68 [66, 71] 41 [40, 44] 4992 [4784, 5138] 2265 [2160, 2357] 103539 [99749, 106850] 116954 [111371, 122032] 2477 [2269, 2678] -18 [-19, -19] 1601 [1555, 1635] 10 [5, 15] 32 [32, 32] 2510 [2040, 2823] 4279 [4139, 4425] 202 [184, 218] 9323 [7914, 10863] -5 [-6, -5] 514 [509, 520] 5501 [3853, 7151] 5972 [5903, 6047] 18074 [16041, 20864] 0 [0, 0] 2347 [2292, 2403] 8604 [8252, 8931] 4830 [4707, 4968] -14 [-29, -6] 15 [14, 16] 111 [78, 166] 5353 [4363, 6783] 4272 [4060, 4440] 33412 [32459, 34435] 19 [18, 20] 1641 [1621, 1661] -24070 [-25305, -22667] 1289 [1143, 1451] 743 [721, 764] 1488 [1470, 1506] 3488 [1032, 8241] -1 [-2, 0] 2703 [2627, 2787] 179 [165, 191] 12397 [11489, 13312] 19 [14, 25] 26245 [23075, 29067] 2213 [1827, 2617] 10708 [10405, 11071] 3661 [3131, 4192] 320 [251, 383] 126 [90, 167] 423 [271, 581] 296 [216, 403] 206 [180, 232] 2000 [2000, 2000] 187 [41, 421] 2200 [1460, 3100] 479 [348, 581] 384 [310, 469] 51 [38, 60] -3 [-6, 0] 9 [8, 11] 4239 [2222, 6622] 688 [501, 878] 896 [174, 1727] 1333 [705, 2094] 139 [116, 165] -1 [-3, 0] 13 [9, 17] -89 [-91, -87] 15 [11, 18] 245 [231, 259] 126 [80, 174] 63 [31, 98] 1741 [1062, 2302] -8 [-10, -8] 85 [62, 106] 402 [280, 520] 421 [136, 735] 52 [18, 95] 0 [0, 0] 457 [352, 578] 1084 [663, 1501] 101 [81, 120] -16 [-38, -2] -5 [-8, -3] -17 [-592, 762] 484 [393, 570] 1045 [833, 1241] 723 [454, 940] 4 [2, 6] 250 [214, 282] -27901 [-30000, -23704] 0 [0, 2] 294 [235, 354] 415 [316, 499] -9 [-10, -10] -20 [-22, -19] 548 [450, 690] 29 [17, 43] 595 [428, 737] 2 [0, 6] 1005 [0, 2485] 225 [185, 264] 1891 [925, 2964] 0 [0, 0] 4838 [3863, 5813] 470 [419, 524] 3518 [2969, 4179] 7319 [6251, 8354] 1359 [1243, 1482] 664529 [197588, 973362] 801 [691, 1002] 22599 [21055, 24669] 5635 [3161, 7962] 758 [681, 823] 101 [69, 138] 97 [97, 99] 137 [110, 162] 20067 [17410, 22758] 15172 [12940, 17219] 132811 [128446, 135930] 34187 [29814, 39261] 4836 [3443, 6231] 21 [20, 22] 476 [175, 782] 40 [32, 47] 19 [6, 32] 5183 [2151, 8291] 38711 [26066, 48187] 831 [768, 900] 20582 [19845, 21583] 14 [13, 16] 836 [568, 1119] 8825 [5234, 12418] 23092 [14679, 28172] 70703 [50114, 94578] 1310 [598, 2180] 4484 [3539, 5511] 15742 [14542, 17103] 15827 [14903, 16429] 0 [0, 0] 16 [16, 17] 3046 [975, 5118] 16807 [16073, 17564] 9160 [8177, 10077] 66453 [40606, 104163] 51 [47, 55] 3416 [2665, 4426] -30043 [-30394, -29764] 2340 [1882, 2799] 1433 [1039, 1943] 2090 [1678, 2649] 5 [4, 7] -3 [-11, 0] 2834 [2241, 3388] 595 [525, 657] 1296 [1254, 1343] 3358 [3004, 3797] 715 [638, 796] 556845 [469425, 660043] 809 [639, 960] 19880 [13450, 26060] 2299 [1921, 2813] 523 [456, 588] 59 [45, 72] 96 [95, 97] 34 [28, 42] 17835 [16161, 19817] 5748 [4822, 6651] 116954 [111371, 122032] 40457 [36892, 43638] 5924 [4491, 7289] -10 [-15, -9] 1845 [1758, 1938] 10 [2, 18] 32 [32, 32] 4561 [3299, 5740] 19174 [14932, 23587] 397 [320, 490] 13450 [11915, 14781] 0 [-1, 2] 624 [588, 662] 9807 [7851, 11591] 9309 [8646, 9953] 29369 [26954, 31595] 50 [0, 140] 4922 [4191, 5843] 8693 [8071, 9199] 5173 [5025, 5322] -20 [-60, 0] 17 [16, 19] 100 [100, 100] 3938 [3210, 4327] 10791 [9307, 12511] 49579 [47425, 51426] 13 [12, 15] 3522 [2401, 4850] -30000 [-30000, -30000] 1103 [799, 1430] 701 [626, 768] 3488 [1032, 8241] -2 [-4, -2] 0 [0, 0] 7779 [3128, 13016] 253 [240, 269] 4382 [4208, 4528] 164 [145, 185] 284807 [178615, 391388] 73095 [40836, 108810] 0 [0, 0] 22345 [20669, 23955] 7086 [6518, 7730] 62209 [57783, 67113] 17347 [15320, 19385] 112 [0, 304] 53826 [40600, 67972] 2599 [2259, 2942] 34861 [29734, 40020] 8850 [8045, 9740] 1.08 [1.02, 1.14] 0.40 [0.40, 0.47] 0.61 [0.60, 0.62] -0.09 [-0.10, -0.07] 0.01 [0.00, 0.01] 0.02 [0.01, 0.02] 3.74 [3.29, 4.13] 1.25 [1.11, 1.47] 1.46 [1.34, 1.51] 2.54 [2.34, 2.75] 0.96 [0.78, 0.98] 0.90 [0.88, 0.94] 34 Published as conference paper at ICLR 2025 MR.Q DreamerV"
        },
        {
            "title": "DQN",
            "content": "Figure 6: Atari learning curves. Time steps consider the number of environment interactions, where 2.5M time steps equals 10M frames in the original environment. Results are over 10 seeds. The shaded area captures 95% boostrap confidence interval. 35 Published as conference paper at ICLR"
        },
        {
            "title": "D COMPLETE ABLATION RESULTS",
            "content": "In this section, we show per-environment breakdown of each variation in the design study in Section 5.2. Each table reports the raw score for each environment. The [bracketed values] represent 95% bootstrap confidence interval. The aggregate mean, median and interquartile mean (IQM) are computed over the the difference in the normalized score. We use TD3 to normalize for Gym, raw scores divided by 1000 for DMC and human scores to normalize for Atari (see Appendix B.3). Highlighting is used to designate the scale of the difference in normalized score: ( 0.5) [0.2, 0.5) [0.01, 0.2) [0.01, 0.2) [0.2, 0.5) ( 0.5) D.1 GYM Task MR.Q Linear value function Dynamics target No target encoder Ant HalfCheetah Hopper Humanoid Walker2d 6901 [6261, 7482] 12939 [11663, 13762] 2692 [2131, 3309] 10223 [9929, 10498] 6039 [5644, 6386] Mean Median IQM - - - 1844 [1663, 2018] 3383 [3054, 3732] 968 [720, 1210] 461 [395, 532] 1117 [999, 1238] -1.17 [-1.19, -1.15] -1.25 [-1.28, -1.21] -1.13 [-1.15, -1.11] 5867 [5543, 6289] 14019 [13746, 14285] 2890 [2030, 3747] 8370 [7651, 8988] 5844 [5146, 6477] -0.10 [-0.17, -0.04] -0.05 [-0.23, 0.09] -0.08 [-0.19, 0.01] 3970 [2468, 5509] 12838 [12459, 13266] 3007 [2164, 3852] 305 [272, 356] 5944 [5570, 6323] -0.53 [-0.60, -0.46] -0.02 [-0.16, 0.02] -0.25 [-0.37, -0.16] Task MR.Q Revert Non-linear model MSE reward loss Ant HalfCheetah Hopper Humanoid Walker2d 6901 [6261, 7482] 12939 [11663, 13762] 2692 [2131, 3309] 10223 [9929, 10498] 6039 [5644, 6386] Mean Median IQM - - - -422 [-1770, 846] -658 [-750, -604] 103 [39, 189] 189 [104, 277] 260 [-5, 638] -1.47 [-1.54, -1.39] -1.47 [-1.53, -1.37] -1.51 [-1.58, -1.39] 7215 [6971, 7466] 13370 [12649, 14053] 2492 [1835, 3424] 10257 [9612, 10688] 5548 [4980, 6117] -0.01 [-0.07, 0.03] 0.01 [-0.03, 0.08] -0.01 [-0.05, 0.04] 7153 [5991, 7815] 14413 [14096, 14710] 2869 [2090, 3689] 10592 [10017, 10983] 6626 [5256, 7984] 0.10 [-0.02, 0.19] 0.07 [0.00, 0.17] 0.09 [-0.01, 0.18] Task MR.Q No reward scaling No min No LAP Ant HalfCheetah Hopper Humanoid Walker2d 6901 [6261, 7482] 12939 [11663, 13762] 2692 [2131, 3309] 10223 [9929, 10498] 6039 [5644, 6386] Mean Median IQM - - - 6866 [6227, 7547] 13502 [13333, 13673] 2551 [2090, 3064] 9515 [8520, 10245] 5743 [5362, 6102] -0.04 [-0.09, 0.02] -0.04 [-0.11, 0.04] -0.04 [-0.10, 0.03] 6936 [6582, 7329] 14143 [13819, 14515] 2113 [1728, 2626] 10528 [10202, 10837] 4293 [3547, 5107] -0.09 [-0.16, -0.01] 0.01 [-0.08, 0.07] -0.04 [-0.10, 0.02] 6817 [6616, 7039] 13185 [13085, 13299] 2681 [1883, 3465] 8441 [6206, 9738] 5463 [4134, 6376] -0.10 [-0.24, -0.00] -0.02 [-0.12, 0.02] -0.06 [-0.18, 0.00] Task MR.Q No MR 1-step return No unroll Ant HalfCheetah Hopper Humanoid Walker2d 6901 [6261, 7482] 12939 [11663, 13762] 2692 [2131, 3309] 10223 [9929, 10498] 6039 [5644, 6386] Mean Median IQM - - - 4195 [2573, 5819] 11249 [9238, 12495] 1877 [1524, 2153] 3942 [3262, 4624] 4155 [3251, 4897] -0.56 [-0.69, -0.43] -0.48 [-0.71, -0.27] -0.47 [-0.66, -0.28] 7757 [7729, 7799] 13123 [10691, 14653] 2737 [2131, 3343] 2328 [1491, 3337] 4747 [3197, 6229] -0.33 [-0.46, -0.21] 0.01 [-0.21, 0.16] -0.10 [-0.32, 0.12] 7528 [7224, 7830] 14409 [13817, 15002] 2578 [1857, 3414] 10617 [10504, 10731] 6077 [5752, 6355] 0.07 [0.01, 0.14] 0.08 [0.06, 0.16] 0.07 [0.04, 0.15] 36 Published as conference paper at ICLR 2025 D.2 DMC - PROPRIOCEPTIVE Task MR.Q Linear value function Dynamics target No target encoder acrobot-swingup ball in cup-catch cartpole-balance cartpole-balance sparse cartpole-swingup cartpole-swingup sparse cheetah-run dog-run dog-stand dog-trot dog-walk finger-spin finger-turn easy finger-turn hard fish-swim hopper-hop hopper-stand humanoid-run humanoid-stand humanoid-walk pendulum-swingup quadruped-run quadruped-walk reacher-easy reacher-hard walker-run walker-stand walker-walk Mean Median IQM Task 567 [517, 621] 981 [979, 983] 999 [999, 1000] 1000 [1000, 1000] 866 [866, 866] 798 [779, 818] 914 [911, 917] 569 [546, 595] 967 [959, 975] 877 [845, 898] 916 [908, 924] 937 [917, 958] 953 [928, 975] 950 [908, 974] 792 [772, 811] 251 [201, 295] 951 [948, 955] 200 [169, 236] 868 [823, 907] 662 [609, 721] 748 [594, 830] 947 [940, 954] 963 [959, 968] 983 [983, 985] 977 [975, 979] 793 [766, 816] 988 [987, 990] 978 [978, 980] - - - 30 [15, 46] 820 [658, 922] 449 [380, 520] 183 [142, 224] 267 [225, 310] 12 [5, 22] 394 [376, 411] 11 [5, 18] 22 [17, 29] 15 [11, 20] 13 [11, 18] 736 [670, 825] 238 [157, 319] 23 [1, 63] 83 [65, 102] 10 [4, 17] 66 [30, 102] 1 [1, 1] 6 [5, 7] 1 [1, 2] 357 [114, 617] 172 [93, 252] 91 [52, 141] 802 [722, 877] 853 [778, 914] 238 [207, 274] 859 [780, 921] 504 [397, 613] 626 [578, 684] 980 [978, 983] 999 [999, 1000] 1000 [1000, 1000] 869 [866, 876] 817 [809, 832] 919 [919, 921] 254 [202, 305] 672 [520, 830] 319 [279, 365] 312 [247, 396] 942 [916, 971] 947 [900, 979] 923 [878, 966] 410 [307, 493] 199 [131, 265] 639 [413, 866] 1 [1, 1] 7 [6, 8] 2 [2, 3] 826 [812, 840] 942 [930, 951] 939 [935, 943] 984 [983, 986] 970 [965, 976] 730 [585, 814] 988 [985, 991] 975 [975, 977] 16 [9, 25] 569 [436, 719] 992 [986, 997] 1000 [1000, 1000] 852 [840, 861] 0 [0, 0] 904 [899, 910] 11 [8, 16] 36 [27, 51] 12 [8, 19] 10 [7, 15] 869 [698, 963] 624 [509, 756] 431 [301, 563] 97 [65, 129] 0 [0, 1] 4 [3, 7] 1 [1, 1] 7 [6, 9] 2 [2, 3] 784 [706, 834] 829 [757, 895] 952 [946, 960] 983 [981, 986] 975 [972, 979] 776 [757, 792] 988 [986, 991] 974 [973, 976] -0.58 [-0.59, -0.56] -0.58 [-0.64, -0.57] -0.62 [-0.64, -0.60] -0.15 [-0.15, -0.15] -0.01 [-0.02, -0.00] -0.05 [-0.06, -0.03] -0.35 [-0.35, -0.34] -0.22 [-0.23, -0.21] -0.27 [-0.29, -0.25] MR.Q Revert Non-linear model MSE reward loss 553 [478, 629] 982 [982, 984] 999 [999, 1000] 1000 [1000, 1000] 866 [866, 867] 824 [809, 839] 909 [906, 913] 588 [516, 646] 962 [938, 982] 868 [816, 914] 920 [915, 925] 868 [767, 951] 972 [968, 978] 931 [887, 975] 790 [754, 824] 288 [222, 332] 848 [664, 945] 205 [191, 221] 811 [712, 878] 668 [590, 734] 819 [802, 836] 944 [934, 954] 963 [961, 967] 983 [982, 984] 953 [914, 976] 795 [784, 811] 983 [971, 991] 974 [972, 978] -0.00 [-0.02, 0.01] -0.00 [-0.00, 0.00] -0.00 [-0.01, 0.00] 577 [547, 612] 983 [982, 985] 999 [998, 1000] 1000 [1000, 1000] 865 [865, 866] 812 [786, 834] 910 [907, 915] 527 [513, 545] 964 [958, 971] 861 [831, 886] 724 [473, 882] 907 [875, 947] 935 [894, 977] 947 [910, 969] 793 [766, 821] 174 [119, 230] 854 [707, 936] 91 [45, 121] 214 [14, 566] 77 [3, 224] 827 [811, 843] 949 [941, 956] 966 [961, 971] 964 [927, 984] 976 [974, 978] 778 [709, 821] 988 [987, 990] 967 [948, 979] -0.06 [-0.08, -0.05] -0.00 [-0.01, 0.00] -0.01 [-0.02, -0.00] acrobot-swingup ball in cup-catch cartpole-balance cartpole-balance sparse cartpole-swingup cartpole-swingup sparse cheetah-run dog-run dog-stand dog-trot dog-walk finger-spin finger-turn easy finger-turn hard fish-swim hopper-hop hopper-stand humanoid-run humanoid-stand humanoid-walk pendulum-swingup quadruped-run quadruped-walk reacher-easy reacher-hard walker-run walker-stand walker-walk Mean Median IQM 567 [517, 621] 981 [979, 983] 999 [999, 1000] 1000 [1000, 1000] 866 [866, 866] 798 [779, 818] 914 [911, 917] 569 [546, 595] 967 [959, 975] 877 [845, 898] 916 [908, 924] 937 [917, 958] 953 [928, 975] 950 [908, 974] 792 [772, 811] 251 [201, 295] 951 [948, 955] 200 [169, 236] 868 [823, 907] 662 [609, 721] 748 [594, 830] 947 [940, 954] 963 [959, 968] 983 [983, 985] 977 [975, 979] 793 [766, 816] 988 [987, 990] 978 [978, 980] - - - 11 [8, 18] 301 [233, 365] 272 [206, 332] 197 [177, 214] 191 [99, 263] 0 [0, 0] 74 [31, 123] 3 [4, 4] 22 [18, 29] 4 [3, 5] 4 [4, 6] 0 [0, 1] 159 [60, 280] 60 [20, 100] 71 [53, 90] 0 [0, 1] 5 [3, 8] 1 [1, 1] 7 [7, 8] 1 [2, 2] 61 [20, 103] 87 [40, 145] 81 [36, 130] 789 [727, 856] 526 [361, 695] 25 [22, 31] 221 [179, 264] 33 [23, 46] -0.72 [-0.73, -0.72] -0.78 [-0.80, -0.76] -0.78 [-0.78, -0.76] 37 Published as conference paper at ICLR 2025 Task MR.Q No reward scaling No min No LAP acrobot-swingup ball in cup-catch cartpole-balance cartpole-balance sparse cartpole-swingup cartpole-swingup sparse cheetah-run dog-run dog-stand dog-trot dog-walk finger-spin finger-turn easy finger-turn hard fish-swim hopper-hop hopper-stand humanoid-run humanoid-stand humanoid-walk pendulum-swingup quadruped-run quadruped-walk reacher-easy reacher-hard walker-run walker-stand walker-walk Mean Median IQM Task 567 [517, 621] 981 [979, 983] 999 [999, 1000] 1000 [1000, 1000] 866 [866, 866] 798 [779, 818] 914 [911, 917] 569 [546, 595] 967 [959, 975] 877 [845, 898] 916 [908, 924] 937 [917, 958] 953 [928, 975] 950 [908, 974] 792 [772, 811] 251 [201, 295] 951 [948, 955] 200 [169, 236] 868 [823, 907] 662 [609, 721] 748 [594, 830] 947 [940, 954] 963 [959, 968] 983 [983, 985] 977 [975, 979] 793 [766, 816] 988 [987, 990] 978 [978, 980] - - - 593 [532, 672] 983 [982, 984] 998 [999, 999] 1000 [1000, 1000] 865 [864, 866] 647 [318, 822] 911 [909, 914] 586 [546, 613] 959 [940, 979] 817 [713, 903] 901 [890, 917] 873 [768, 947] 977 [973, 982] 946 [905, 969] 745 [663, 809] 343 [263, 477] 934 [912, 948] 184 [149, 214] 810 [655, 899] 665 [589, 765] 816 [790, 838] 951 [944, 958] 966 [961, 971] 964 [926, 984] 971 [968, 975] 804 [783, 820] 989 [988, 990] 979 [978, 980] -0.01 [-0.02, 0.00] -0.00 [-0.00, 0.00] -0.00 [-0.00, 0.00] 623 [572, 673] 982 [980, 984] 999 [1000, 1000] 1000 [1000, 1000] 868 [866, 874] 799 [780, 812] 910 [893, 921] 577 [540, 610] 946 [917, 969] 846 [767, 906] 747 [447, 908] 926 [907, 950] 976 [972, 980] 894 [833, 953] 785 [763, 810] 336 [322, 352] 935 [926, 947] 198 [175, 225] 833 [793, 871] 597 [292, 808] 825 [811, 839] 946 [941, 951] 959 [942, 972] 983 [981, 986] 978 [974, 982] 806 [779, 821] 989 [986, 992] 978 [976, 980] -0.01 [-0.02, 0.01] -0.00 [-0.00, 0.00] -0.00 [-0.00, 0.00] 566 [520, 612] 981 [980, 984] 999 [998, 1000] 992 [982, 1000] 865 [865, 866] 796 [779, 810] 908 [905, 913] 536 [499, 573] 971 [966, 976] 842 [764, 897] 899 [886, 914] 915 [892, 948] 975 [967, 983] 949 [909, 972] 788 [754, 826] 347 [265, 431] 941 [935, 948] 202 [191, 212] 880 [856, 900] 697 [561, 828] 815 [792, 836] 937 [927, 947] 955 [942, 967] 983 [982, 986] 974 [969, 981] 812 [803, 822] 986 [985, 987] 977 [974, 980] 0.00 [-0.00, 0.01] -0.00 [-0.00, 0.00] -0.00 [-0.00, 0.00] MR.Q No MR 1-step return No unroll acrobot-swingup ball in cup-catch cartpole-balance cartpole-balance sparse cartpole-swingup cartpole-swingup sparse cheetah-run dog-run dog-stand dog-trot dog-walk finger-spin finger-turn easy finger-turn hard fish-swim hopper-hop hopper-stand humanoid-run humanoid-stand humanoid-walk pendulum-swingup quadruped-run quadruped-walk reacher-easy reacher-hard walker-run walker-stand walker-walk Mean Median IQM 567 [517, 621] 981 [979, 983] 999 [999, 1000] 1000 [1000, 1000] 866 [866, 866] 798 [779, 818] 914 [911, 917] 569 [546, 595] 967 [959, 975] 877 [845, 898] 916 [908, 924] 937 [917, 958] 953 [928, 975] 950 [908, 974] 792 [772, 811] 251 [201, 295] 951 [948, 955] 200 [169, 236] 868 [823, 907] 662 [609, 721] 748 [594, 830] 947 [940, 954] 963 [959, 968] 983 [983, 985] 977 [975, 979] 793 [766, 816] 988 [987, 990] 978 [978, 980] - - - 576 [483, 665] 981 [980, 984] 994 [991, 999] 1000 [1000, 1000] 870 [864, 878] 684 [528, 814] 871 [823, 907] 68 [63, 75] 494 [452, 530] 65 [49, 80] 102 [81, 122] 888 [731, 975] 947 [913, 974] 846 [756, 926] 706 [683, 727] 85 [33, 142] 365 [233, 491] 1 [1, 2] 201 [9, 517] 84 [3, 247] 827 [812, 842] 871 [793, 933] 951 [943, 962] 980 [979, 983] 949 [909, 974] 780 [769, 790] 983 [980, 988] 976 [975, 977] 440 [360, 528] 984 [983, 985] 999 [1000, 1000] 961 [886, 1000] 881 [879, 882] 845 [845, 847] 922 [921, 924] 299 [196, 360] 606 [344, 865] 725 [679, 756] 788 [739, 832] 983 [977, 988] 980 [979, 982] 968 [958, 976] 498 [323, 651] 364 [336, 394] 952 [947, 959] 190 [124, 241] 753 [665, 838] 761 [689, 827] 823 [807, 841] 945 [940, 950] 962 [958, 968] 984 [984, 986] 980 [979, 983] 835 [827, 843] 990 [990, 992] 979 [977, 982] 515 [455, 598] 982 [981, 984] 999 [999, 1000] 1000 [1000, 1000] 864 [861, 867] 818 [811, 831] 909 [908, 911] 514 [473, 554] 955 [944, 971] 857 [833, 883] 920 [905, 934] 880 [781, 940] 950 [917, 976] 947 [907, 971] 709 [618, 783] 297 [169, 442] 949 [944, 955] 192 [172, 214] 858 [806, 913] 675 [593, 772] 819 [793, 843] 950 [944, 955] 962 [955, 969] 981 [976, 986] 954 [913, 978] 780 [702, 825] 988 [988, 990] 975 [969, 981] -0.19 [-0.19, -0.18] -0.05 [-0.08, -0.01] -0.06 [-0.08, -0.05] -0.04 [-0.05, -0.02] 0.00 [0.00, 0.00] 0.00 [-0.00, 0.01] -0.01 [-0.01, -0.00] -0.00 [-0.00, 0.00] -0.00 [-0.01, -0.00] 38 Published as conference paper at ICLR 2025 D.3 DMC - VISUAL Task MR.Q Linear value function Dynamics target No target encoder acrobot-swingup ball in cup-catch cartpole-balance cartpole-balance sparse cartpole-swingup cartpole-swingup sparse cheetah-run dog-run dog-stand dog-trot dog-walk finger-spin finger-turn easy finger-turn hard fish-swim hopper-hop hopper-stand humanoid-run humanoid-stand humanoid-walk pendulum-swingup quadruped-run quadruped-walk reacher-easy reacher-hard walker-run walker-stand walker-walk Mean Median IQM Task 287 [253, 317] 977 [975, 980] 999 [999, 999] 1000 [1000, 1000] 868 [861, 875] 797 [777, 818] 775 [752, 805] 60 [44, 80] 216 [201, 233] 65 [54, 79] 77 [70, 83] 965 [938, 982] 953 [925, 974] 932 [905, 957] 79 [67, 93] 270 [229, 317] 852 [705, 932] 1 [1, 2] 7 [7, 8] 2 [2, 3] 829 [815, 842] 498 [474, 523] 833 [796, 868] 979 [978, 982] 965 [945, 977] 615 [571, 655] 980 [977, 984] 970 [968, 973] - - - 15 [5, 22] 644 [328, 904] 306 [254, 349] 243 [183, 327] 229 [181, 294] 4 [0, 14] 230 [159, 294] 19 [17, 22] 76 [70, 82] 19 [16, 24] 30 [26, 33] 789 [598, 923] 132 [98, 200] 66 [0, 100] 69 [45, 109] 1 [0, 2] 7 [3, 11] 1 [1, 1] 5 [4, 8] 1 [1, 2] 97 [0, 192] 131 [77, 187] 105 [57, 155] 605 [398, 868] 288 [195, 473] 158 [133, 193] 707 [601, 881] 350 [228, 529] 296 [281, 323] 972 [965, 976] 998 [998, 999] 1000 [1000, 1000] 861 [859, 865] 267 [0, 801] 831 [761, 875] 36 [34, 39] 191 [155, 247] 46 [42, 53] 62 [57, 72] 786 [672, 931] 876 [691, 969] 859 [777, 963] 71 [38, 106] 184 [165, 204] 911 [900, 922] 1 [1, 2] 6 [5, 8] 2 [1, 3] 749 [632, 840] 488 [468, 517] 717 [445, 895] 977 [973, 981] 975 [971, 978] 531 [463, 577] 982 [981, 984] 904 [850, 951] 16 [0, 38] 605 [496, 726] 978 [947, 997] 1000 [1000, 1000] 689 [487, 808] 0 [0, 0] 745 [723, 784] 10 [6, 20] 60 [44, 89] 9 [9, 10] 16 [10, 21] 929 [893, 981] 898 [855, 969] 492 [385, 577] 65 [49, 84] 2 [0, 6] 5 [3, 9] 1 [1, 1] 7 [6, 8] 1 [1, 2] 191 [93, 287] 575 [566, 594] 817 [790, 868] 979 [970, 986] 970 [963, 975] 611 [590, 631] 984 [979, 988] 965 [957, 972] -0.41 [-0.42, -0.39] -0.37 [-0.44, -0.37] -0.42 [-0.43, -0.38] -0.05 [-0.05, -0.04] -0.01 [-0.01, -0.00] -0.02 [-0.02, -0.01] -0.15 [-0.15, -0.15] -0.03 [-0.05, -0.03] -0.05 [-0.06, -0.04] MR.Q Revert Non-linear model MSE reward loss acrobot-swingup ball in cup-catch cartpole-balance cartpole-balance sparse cartpole-swingup cartpole-swingup sparse cheetah-run dog-run dog-stand dog-trot dog-walk finger-spin finger-turn easy finger-turn hard fish-swim hopper-hop hopper-stand humanoid-run humanoid-stand humanoid-walk pendulum-swingup quadruped-run quadruped-walk reacher-easy reacher-hard walker-run walker-stand walker-walk Mean Median IQM 287 [253, 317] 977 [975, 980] 999 [999, 999] 1000 [1000, 1000] 868 [861, 875] 797 [777, 818] 775 [752, 805] 60 [44, 80] 216 [201, 233] 65 [54, 79] 77 [70, 83] 965 [938, 982] 953 [925, 974] 932 [905, 957] 79 [67, 93] 270 [229, 317] 852 [705, 932] 1 [1, 2] 7 [7, 8] 2 [2, 3] 829 [815, 842] 498 [474, 523] 833 [796, 868] 979 [978, 982] 965 [945, 977] 615 [571, 655] 980 [977, 984] 970 [968, 973] - - - 19 [13, 23] 195 [91, 297] 190 [163, 212] 346 [193, 639] 115 [82, 175] 0 [0, 0] 69 [36, 129] 3 [3, 4] 17 [16, 19] 5 [4, 6] 6 [6, 8] 1 [0, 2] 133 [99, 200] 66 [0, 100] 53 [47, 60] 0 [0, 2] 4 [3, 8] 1 [1, 1] 6 [5, 7] 1 [1, 2] 66 [0, 100] 86 [69, 120] 76 [34, 100] 583 [395, 684] 33 [0, 100] 35 [29, 39] 280 [269, 294] 22 [19, 25] 279 [235, 314] 973 [970, 977] 999 [999, 999] 1000 [1000, 1000] 849 [819, 873] 768 [684, 824] 763 [757, 770] 37 [36, 40] 200 [193, 207] 51 [47, 56] 69 [62, 78] 907 [841, 971] 932 [889, 977] 938 [903, 967] 67 [55, 79] 308 [257, 368] 935 [929, 942] 1 [1, 1] 6 [6, 8] 2 [2, 3] 820 [795, 844] 555 [514, 578] 762 [727, 788] 939 [900, 979] 868 [753, 956] 612 [593, 633] 982 [977, 987] 951 [917, 972] 265 [242, 294] 974 [969, 978] 998 [998, 1000] 1000 [1000, 1000] 876 [875, 878] 33 [0, 60] 732 [712, 757] 36 [33, 38] 195 [187, 208] 47 [44, 50] 63 [58, 71] 924 [884, 980] 844 [786, 881] 900 [867, 964] 75 [59, 105] 102 [14, 151] 919 [914, 925] 1 [1, 1] 7 [7, 8] 1 [1, 2] 581 [94, 842] 516 [478, 544] 835 [751, 880] 979 [976, 984] 941 [879, 976] 596 [505, 643] 983 [984, 984] 969 [961, 976] -0.52 [-0.52, -0.51] -0.68 [-0.72, -0.62] -0.57 [-0.58, -0.56] -0.01 [-0.02, -0.00] -0.01 [-0.01, -0.00] -0.01 [-0.01, -0.00] -0.05 [-0.07, -0.04] -0.01 [-0.01, 0.00] -0.01 [-0.01, -0.00] Published as conference paper at ICLR 2025 Task MR.Q No reward scaling No min No LAP acrobot-swingup ball in cup-catch cartpole-balance cartpole-balance sparse cartpole-swingup cartpole-swingup sparse cheetah-run dog-run dog-stand dog-trot dog-walk finger-spin finger-turn easy finger-turn hard fish-swim hopper-hop hopper-stand humanoid-run humanoid-stand humanoid-walk pendulum-swingup quadruped-run quadruped-walk reacher-easy reacher-hard walker-run walker-stand walker-walk Mean Median IQM Task 287 [253, 317] 977 [975, 980] 999 [999, 999] 1000 [1000, 1000] 868 [861, 875] 797 [777, 818] 775 [752, 805] 60 [44, 80] 216 [201, 233] 65 [54, 79] 77 [70, 83] 965 [938, 982] 953 [925, 974] 932 [905, 957] 79 [67, 93] 270 [229, 317] 852 [705, 932] 1 [1, 2] 7 [7, 8] 2 [2, 3] 829 [815, 842] 498 [474, 523] 833 [796, 868] 979 [978, 982] 965 [945, 977] 615 [571, 655] 980 [977, 984] 970 [968, 973] - - - 323 [275, 368] 973 [971, 977] 999 [999, 999] 1000 [1000, 1000] 860 [829, 877] 813 [805, 823] 720 [678, 758] 61 [49, 73] 317 [239, 387] 65 [55, 79] 89 [83, 96] 903 [776, 975] 873 [775, 954] 923 [885, 962] 73 [59, 87] 244 [204, 298] 911 [888, 926] 1 [1, 1] 7 [6, 8] 2 [2, 4] 823 [798, 846] 505 [471, 545] 823 [781, 867] 962 [924, 983] 972 [970, 975] 600 [544, 632] 986 [984, 989] 970 [968, 974] -0.00 [-0.01, 0.01] -0.00 [-0.00, 0.00] -0.00 [-0.00, 0.00] 332 [301, 391] 975 [974, 976] 998 [998, 999] 1000 [1000, 1000] 879 [879, 880] 805 [763, 829] 751 [734, 762] 42 [37, 51] 228 [224, 232] 50 [48, 53] 86 [69, 106] 870 [709, 982] 963 [952, 975] 933 [874, 976] 54 [49, 61] 255 [244, 275] 923 [902, 945] 1 [1, 1] 6 [5, 8] 2 [2, 4] 831 [809, 843] 539 [500, 578] 849 [745, 909] 953 [897, 981] 936 [872, 975] 666 [643, 682] 982 [975, 988] 970 [968, 972] 0.00 [-0.01, 0.01] 0.00 [-0.00, 0.00] 0.00 [-0.00, 0.00] 280 [235, 354] 971 [966, 978] 998 [997, 999] 1000 [1000, 1000] 798 [785, 821] 764 [736, 799] 706 [670, 741] 62 [45, 91] 279 [229, 315] 58 [56, 61] 91 [85, 101] 940 [864, 979] 844 [785, 879] 932 [858, 974] 63 [49, 89] 186 [152, 204] 884 [877, 896] 1 [1, 2] 10 [6, 18] 2 [2, 3] 829 [808, 841] 463 [428, 485] 799 [713, 905] 948 [885, 980] 973 [973, 974] 662 [629, 725] 984 [984, 985] 972 [964, 979] -0.01 [-0.02, -0.01] -0.00 [-0.01, 0.00] -0.01 [-0.02, 0.00] MR.Q No MR 1-step return No unroll acrobot-swingup ball in cup-catch cartpole-balance cartpole-balance sparse cartpole-swingup cartpole-swingup sparse cheetah-run dog-run dog-stand dog-trot dog-walk finger-spin finger-turn easy finger-turn hard fish-swim hopper-hop hopper-stand humanoid-run humanoid-stand humanoid-walk pendulum-swingup quadruped-run quadruped-walk reacher-easy reacher-hard walker-run walker-stand walker-walk Mean Median IQM 287 [253, 317] 977 [975, 980] 999 [999, 999] 1000 [1000, 1000] 868 [861, 875] 797 [777, 818] 775 [752, 805] 60 [44, 80] 216 [201, 233] 65 [54, 79] 77 [70, 83] 965 [938, 982] 953 [925, 974] 932 [905, 957] 79 [67, 93] 270 [229, 317] 852 [705, 932] 1 [1, 2] 7 [7, 8] 2 [2, 3] 829 [815, 842] 498 [474, 523] 833 [796, 868] 979 [978, 982] 965 [945, 977] 615 [571, 655] 980 [977, 984] 970 [968, 973] - - - 362 [305, 421] 898 [746, 977] 998 [998, 999] 1000 [1000, 1000] 871 [864, 878] 459 [139, 780] 782 [765, 805] 22 [21, 24] 137 [127, 148] 32 [29, 36] 42 [36, 50] 887 [757, 965] 694 [539, 805] 622 [436, 825] 72 [60, 93] 192 [166, 216] 918 [897, 935] 1 [1, 2] 7 [7, 8] 2 [2, 3] 819 [787, 844] 478 [432, 515] 701 [663, 731] 978 [976, 981] 545 [214, 858] 568 [538, 599] 974 [962, 986] 955 [948, 964] 91 [76, 112] 980 [979, 982] 998 [998, 1000] 1000 [1000, 1000] 858 [835, 875] 712 [685, 759] 675 [674, 677] 30 [21, 43] 160 [143, 191] 29 [25, 32] 55 [33, 67] 984 [978, 989] 942 [874, 979] 908 [872, 974] 64 [58, 72] 248 [231, 280] 877 [820, 915] 1 [1, 2] 7 [6, 9] 2 [2, 3] 828 [811, 839] 456 [424, 476] 666 [627, 720] 972 [948, 985] 978 [972, 982] 672 [639, 730] 986 [982, 991] 971 [967, 978] 126 [77, 159] 976 [973, 980] 998 [999, 999] 1000 [1000, 1000] 872 [863, 879] 529 [0, 816] 753 [679, 845] 45 [36, 56] 209 [195, 217] 47 [46, 49] 67 [63, 76] 738 [588, 960] 869 [766, 962] 902 [780, 973] 67 [55, 90] 242 [219, 270] 925 [907, 940] 1 [1, 1] 7 [5, 9] 2 [2, 3] 665 [382, 811] 398 [326, 465] 730 [663, 769] 978 [977, 980] 893 [776, 967] 656 [619, 696] 983 [981, 987] 971 [971, 972] -0.07 [-0.09, -0.03] -0.02 [-0.03, -0.01] -0.03 [-0.03, -0.01] -0.03 [-0.03, -0.02] -0.01 [-0.01, -0.00] -0.01 [-0.02, -0.01] -0.04 [-0.06, -0.01] -0.01 [-0.01, -0.00] -0.02 [-0.02, -0.01] 40 Published as conference paper at ICLR 2025 D.4 ATARI Task MR.Q Linear value function Dynamics target No target encoder Alien Amidar Assault Asterix Asteroids Atlantis BankHeist BattleZone BeamRider Berzerk Bowling Boxing Breakout Centipede ChopperCommand CrazyClimber Defender DemonAttack DoubleDunk Enduro FishingDerby Freeway Frostbite Gopher Gravitar Hero IceHockey Jamesbond Kangaroo Krull KungFuMaster MontezumaRevenge MsPacman NameThisGame Phoenix Pitfall Pong PrivateEye Qbert Riverraid RoadRunner Robotank Seaquest Skiing Solaris SpaceInvaders StarGunner Surround Tennis TimePilot Tutankham UpNDown Venture VideoPinball WizardOfWor YarsRevenge Zaxxon Mean Median IQM 2471 [1848, 3155] 443 [376, 499] 1125 [1094, 1160] 2216 [2081, 2346] 602 [493, 689] 445022 [282338, 630730] 542 [348, 749] 16520 [10560, 22760] 2007 [1855, 2194] 430 [383, 472] 50 [37, 65] 95 [94, 97] 25 [21, 32] 14954 [13541, 16508] 4348 [3756, 5002] 104766 [99290, 109629] 25962 [23406, 29182] 4660 [4072, 5241] -9 [-11, -9] 1480 [1378, 1592] -34 [-43, -27] 31 [31, 32] 4003 [2871, 5163] 4936 [3923, 5730] 275 [232, 322] 8391 [6845, 10060] -2 [-3, -1] 551 [534, 573] 4833 [2716, 7064] 8660 [8198, 9147] 26150 [21973, 30490] 12 [0, 34] 4395 [3799, 5002] 7511 [7085, 7911] 4843 [4635, 5033] -8 [-19, -1] 15 [13, 17] 100 [100, 100] 3600 [2554, 4366] 7362 [7062, 7630] 27152 [19731, 34480] 10 [9, 13] 2660 [2055, 3579] -30000 [-30000, -30000] 1262 [863, 1686] 478 [429, 524] 1146 [996, 1437] -6 [-7, -5] 0 [-1, 0] 3101 [2772, 3482] 130 [124, 139] 26477 [11956, 43260] 0 [0, 0] 18826 [15048, 23233] 1918 [1706, 2154] 27299 [23434, 30493] 3820 [2577, 4854] - - - 596 [561, 631] 48 [38, 61] 366 [359, 374] 810 [585, 1035] 609 [595, 623] 17683 [13080, 20330] 93 [76, 111] 11300 [11100, 11500] 584 [528, 642] 315 [216, 415] 31 [31, 33] 39 [37, 42] 3 [2, 4] 6709 [6207, 7213] 890 [780, 1000] 23016 [21610, 25010] 7825 [5640, 10010] 1608 [1365, 1852] -18 [-24, -13] 343 [319, 368] -90 [-96, -86] 20 [19, 23] 198 [198, 198] 598 [524, 672] 190 [130, 250] 615 [112, 1118] -15 [-16, -15] 46 [40, 50] 555 [520, 590] 6078 [5777, 6379] 10400 [9320, 11480] 0 [0, 0] 826 [757, 896] 2339 [2258, 2436] 570 [286, 854] -25 [-50, 0] -20 [-21, -20] 45 [0, 90] 256 [228, 285] 1997 [1829, 2165] 3245 [2410, 4080] 7 [3, 11] 305 [214, 400] -30000 [-30000, -30000] 480 [0, 960] 242 [230, 255] 1060 [880, 1240] -9 [-9, -9] -24 [-24, -24] 2525 [1430, 3620] 90 [86, 95] 3180 [2321, 4040] 65 [0, 130] 12994 [10570, 15419] 635 [480, 790] 6404 [6391, 6417] 0 [0, 0] -1.35 [-1.41, -1.29] -0.42 [-0.55, -0.42] -0.56 [-0.60, -0.55] 1176 [1138, 1215] 214 [182, 247] 911 [906, 917] 1940 [1865, 2015] 776 [716, 837] 529745 [145750, 913740] 646 [326, 966] 8250 [2100, 14400] 1201 [1015, 1387] 381 [359, 403] 81 [81, 82] 90 [88, 93] 9 [8, 11] 7853 [7680, 8026] 3055 [2870, 3240] 92455 [84000, 100910] 17592 [9290, 25895] 281 [242, 322] -15 [-22, -10] 622 [621, 623] -64 [-66, -62] 32 [33, 33] 268 [267, 269] 853 [832, 874] 352 [250, 455] 7560 [7560, 7560] -6 [-10, -4] 412 [310, 515] 6830 [600, 13060] 7460 [6961, 7959] 17020 [7520, 26520] 0 [0, 0] 2950 [2490, 3410] 6660 [6568, 6752] 3996 [3843, 4150] 0 [0, 0] 14 [14, 16] 100 [100, 100] 493 [488, 500] 6860 [6391, 7330] 21835 [20960, 22710] 7 [4, 10] 895 [834, 956] -30000 [-30000, -30000] 710 [378, 1042] 303 [263, 344] 960 [920, 1000] -7 [-8, -7] -5 [-10, 0] 2525 [2040, 3010] 164 [150, 179] 3045 [2667, 3424] 0 [0, 0] 9170 [7880, 10460] 1260 [670, 1850] 23613 [16984, 30244] 690 [0, 1380] -0.38 [-0.81, 0.05] -0.16 [-0.16, -0.11] -0.20 [-0.22, -0.15] 2040 [1585, 2495] 249 [232, 268] 1057 [880, 1234] 2407 [1860, 2955] 765 [591, 939] 76930 [76090, 77770] 40 [38, 43] 4600 [2500, 6700] 1468 [1298, 1639] 359 [275, 444] 40 [33, 48] 92 [89, 96] 1 [1, 2] 5167 [3661, 6674] 1385 [1060, 1710] 40240 [29150, 51330] 11627 [5745, 17510] 278 [204, 352] -20 [-21, -19] 690 [667, 713] -81 [-87, -75] 11 [0, 22] 824 [258, 1390] 4371 [3794, 4948] 60 [40, 80] 2200 [1377, 3024] -8 [-10, -6] 102 [80, 125] 685 [590, 780] 9088 [8624, 9552] 12130 [11280, 12980] 0 [0, 0] 3171 [1873, 4469] 7015 [6396, 7634] 4260 [3944, 4577] -66 [-122, -12] -10 [-19, -2] 90 [80, 100] 747 [615, 880] 6342 [5450, 7234] 35120 [33050, 37190] 6 [3, 9] 1227 [378, 2076] -30000 [-30000, -30000] 1219 [1198, 1240] 296 [269, 323] 970 [960, 980] -7 [-9, -6] 0 [0, 0] 1710 [1020, 2400] 78 [2, 155] 4523 [4422, 4625] 0 [0, 0] 19734 [14479, 24989] 1440 [930, 1950] 21163 [21047, 21280] 0 [0, 0] -0.86 [-0.89, -0.83] -0.18 [-0.19, -0.12] -0.26 [-0.27, -0.25] Published as conference paper at ICLR 2025 Task MR.Q Revert Non-linear model MSE reward loss Alien Amidar Assault Asterix Asteroids Atlantis BankHeist BattleZone BeamRider Berzerk Bowling Boxing Breakout Centipede ChopperCommand CrazyClimber Defender DemonAttack DoubleDunk Enduro FishingDerby Freeway Frostbite Gopher Gravitar Hero IceHockey Jamesbond Kangaroo Krull KungFuMaster MontezumaRevenge MsPacman NameThisGame Phoenix Pitfall Pong PrivateEye Qbert Riverraid RoadRunner Robotank Seaquest Skiing Solaris SpaceInvaders StarGunner Surround Tennis TimePilot Tutankham UpNDown Venture VideoPinball WizardOfWor YarsRevenge Zaxxon Mean Median IQM 2471 [1848, 3155] 443 [376, 499] 1125 [1094, 1160] 2216 [2081, 2346] 602 [493, 689] 445022 [282338, 630730] 542 [348, 749] 16520 [10560, 22760] 2007 [1855, 2194] 430 [383, 472] 50 [37, 65] 95 [94, 97] 25 [21, 32] 14954 [13541, 16508] 4348 [3756, 5002] 104766 [99290, 109629] 25962 [23406, 29182] 4660 [4072, 5241] -9 [-11, -9] 1480 [1378, 1592] -34 [-43, -27] 31 [31, 32] 4003 [2871, 5163] 4936 [3923, 5730] 275 [232, 322] 8391 [6845, 10060] -2 [-3, -1] 551 [534, 573] 4833 [2716, 7064] 8660 [8198, 9147] 26150 [21973, 30490] 12 [0, 34] 4395 [3799, 5002] 7511 [7085, 7911] 4843 [4635, 5033] -8 [-19, -1] 15 [13, 17] 100 [100, 100] 3600 [2554, 4366] 7362 [7062, 7630] 27152 [19731, 34480] 10 [9, 13] 2660 [2055, 3579] -30000 [-30000, -30000] 1262 [863, 1686] 478 [429, 524] 1146 [996, 1437] -6 [-7, -5] 0 [-1, 0] 3101 [2772, 3482] 130 [124, 139] 26477 [11956, 43260] 0 [0, 0] 18826 [15048, 23233] 1918 [1706, 2154] 27299 [23434, 30493] 3820 [2577, 4854] - - - 66 [32, 100] 39 [19, 60] 366 [359, 374] 492 [425, 560] 249 [239, 259] 7310 [3140, 11480] 14 [0, 28] 3550 [3300, 3800] 546 [510, 582] 315 [275, 355] 41 [27, 55] 29 [21, 38] 2 [1, 3] 2877 [2821, 2933] 530 [420, 640] 1700 [0, 3400] 1917 [1780, 2055] 149 [147, 151] -23 [-24, -23] 3 [0, 6] -86 [-91, -82] 0 [0, 0] 170 [151, 190] 385 [280, 490] 82 [80, 85] 0 [0, 0] -14 [-17, -11] 60 [45, 75] 80 [0, 160] 10 [0, 20] 1815 [130, 3500] 0 [0, 0] 283 [182, 385] 2675 [2609, 2742] 728 [517, 939] -1012 [-2000, -24] -20 [-21, -21] 50 [20, 80] 137 [125, 150] 1660 [1131, 2190] 0 [0, 0] 2 [3, 3] 134 [20, 248] -30000 [-30000, -30000] 878 [8, 1748] 207 [198, 216] 930 [710, 1150] -9 [-9, -9] -12 [-24, -2] 2195 [2040, 2350] 14 [0, 30] 1692 [1526, 1859] 0 [0, 0] 6961 [6299, 7623] 575 [550, 600] 148 [0, 297] 0 [0, 0] -1.69 [-1.70, -1.67] -0.63 [-0.64, -0.63] -0.67 [-0.69, -0.65] 2167 [1426, 3169] 466 [364, 570] 1033 [998, 1068] 1987 [1560, 2414] 563 [424, 740] 734 [617, 856] 157 [140, 177] 923 [873, 987] 2503 [2050, 3040] 765 [624, 952] 444370 [241216, 647524] 87410 [31610, 153510] 1006 [961, 1042] 23820 [20300, 26200] 1904 [1777, 2046] 527 [498, 579] 69 [58, 82] 95 [91, 98] 17 [17, 18] 10053 [6514, 13594] 2918 [2006, 3830] 103950 [98066, 110282] 24283 [22936, 25425] 2467 [1370, 3548] -10 [-14, -8] 1117 [1059, 1173] -33 [-36, -30] 31 [31, 32] 2693 [834, 4491] 7216 [3645, 11049] 309 [156, 419] 7635 [7577, 7693] -1 [-2, -1] 495 [462, 538] 5732 [3148, 7954] 8396 [8178, 8593] 24644 [19158, 30310] 240 [80, 400] 3721 [3169, 4499] 6162 [5941, 6450] 4611 [4300, 4800] -3 [-9, 0] 16 [15, 19] 100 [100, 100] 4295 [4006, 4586] 6679 [5053, 7770] 26678 [19418, 32016] 10 [8, 13] 2344 [1541, 3450] -30000 [-30000, -30000] 1280 [498, 2062] 536 [405, 667] 1014 [994, 1048] -5 [-7, -5] 0 [-1, 0] 3822 [3282, 4418] 138 [127, 151] 34574 [9812, 71080] 74 [0, 210] 14689 [10497, 17911] 1852 [1650, 2062] 29495 [25242, 32299] 3144 [1128, 5118] -0.07 [-0.32, 0.18] -0.01 [-0.02, 0.00] -0.02 [-0.05, 0.00] 245 [195, 309] 4566 [3900, 5200] 1489 [1446, 1543] 334 [295, 398] 30 [27, 35] 93 [89, 98] 11 [7, 18] 11624 [8061, 16184] 2806 [1610, 3570] 107220 [104990, 109150] 15231 [7875, 21485] 311 [288, 331] -10 [-14, -9] 800 [694, 899] -71 [-75, -66] 30 [30, 31] 3954 [266, 7285] 2484 [886, 3514] 140 [50, 310] 933 [0, 2799] -8 [-9, -7] 355 [280, 455] 2793 [540, 7060] 8886 [7771, 9458] 19536 [12710, 31840] 0 [0, 0] 1457 [1382, 1549] 6091 [5656, 6475] 3638 [3317, 3959] -22 [-68, 0] 13 [8, 17] 33 [0, 100] 861 [785, 968] 3418 [482, 6556] 24583 [15870, 35950] 8 [2, 13] 1676 [368, 2336] -30000 [-30000, -30000] 870 [370, 1736] 253 [235, 278] 976 [960, 1000] -8 [-9, -8] -2 [-5, 0] 2376 [2070, 2880] 37 [0, 112] 4568 [4174, 4972] 0 [0, 0] 11244 [9717, 12780] 1190 [1000, 1430] 14267 [10884, 16770] 0 [0, 0] -0.79 [-0.86, -0.73] -0.24 [-0.24, -0.17] -0.23 [-0.24, -0.20] 42 Published as conference paper at ICLR 2025 Task MR.Q No reward scaling No min No LAP Alien Amidar Assault Asterix Asteroids Atlantis BankHeist BattleZone BeamRider Berzerk Bowling Boxing Breakout Centipede ChopperCommand CrazyClimber Defender DemonAttack DoubleDunk Enduro FishingDerby Freeway Frostbite Gopher Gravitar Hero IceHockey Jamesbond Kangaroo Krull KungFuMaster MontezumaRevenge MsPacman NameThisGame Phoenix Pitfall Pong PrivateEye Qbert Riverraid RoadRunner Robotank Seaquest Skiing Solaris SpaceInvaders StarGunner Surround Tennis TimePilot Tutankham UpNDown Venture VideoPinball WizardOfWor YarsRevenge Zaxxon Mean Median IQM 2471 [1848, 3155] 443 [376, 499] 1125 [1094, 1160] 2216 [2081, 2346] 602 [493, 689] 2074 [1402, 2821] 402 [345, 454] 1235 [1140, 1330] 2476 [2051, 2901] 614 [520, 711] 2375 [1921, 3052] 567 [465, 686] 1154 [967, 1254] 2881 [2600, 3045] 769 [713, 878] 2265 [1784, 2768] 361 [291, 428] 1095 [1027, 1170] 2503 [2250, 2800] 509 [447, 568] 445022 [282338, 630730] 608658 [262742, 946168] 386213 [77480, 941750] 329941 [196522, 506576] 542 [348, 749] 16520 [10560, 22760] 2007 [1855, 2194] 430 [383, 472] 50 [37, 65] 95 [94, 97] 25 [21, 32] 14954 [13541, 16508] 4348 [3756, 5002] 104766 [99290, 109629] 25962 [23406, 29182] 4660 [4072, 5241] -9 [-11, -9] 1480 [1378, 1592] -34 [-43, -27] 31 [31, 32] 4003 [2871, 5163] 4936 [3923, 5730] 275 [232, 322] 8391 [6845, 10060] -2 [-3, -1] 551 [534, 573] 4833 [2716, 7064] 8660 [8198, 9147] 26150 [21973, 30490] 12 [0, 34] 4395 [3799, 5002] 7511 [7085, 7911] 4843 [4635, 5033] -8 [-19, -1] 15 [13, 17] 100 [100, 100] 3600 [2554, 4366] 7362 [7062, 7630] 27152 [19731, 34480] 10 [9, 13] 2660 [2055, 3579] -30000 [-30000, -30000] 1262 [863, 1686] 478 [429, 524] 1146 [996, 1437] -6 [-7, -5] 0 [-1, 0] 3101 [2772, 3482] 130 [124, 139] 26477 [11956, 43260] 0 [0, 0] 18826 [15048, 23233] 1918 [1706, 2154] 27299 [23434, 30493] 3820 [2577, 4854] - - - 490 [176, 806] 13660 [8040, 18780] 1989 [1947, 2031] 427 [361, 524] 78 [66, 88] 91 [87, 96] 22 [20, 26] 15952 [12806, 19014] 2796 [2228, 3378] 105014 [94550, 112412] 30912 [27871, 33695] 4893 [4282, 5345] -11 [-14, -8] 1450 [1311, 1593] -22 [-25, -20] 25 [13, 32] 3247 [1532, 4771] 5802 [1467, 13322] 256 [215, 305] 8775 [7575, 11125] -3 [-5, -2] 543 [460, 610] 6148 [3412, 8600] 8878 [7898, 9491] 24292 [18954, 29568] 0 [0, 0] 4086 [3847, 4413] 8323 [7308, 9338] 4940 [4541, 5255] 0 [0, 0] 15 [14, 18] 40 [0, 80] 2848 [1410, 4287] 6669 [5104, 7779] 37306 [32906, 40582] 15 [13, 17] 1998 [1141, 2626] -30000 [-30000, -30000] 1175 [660, 1770] 458 [431, 493] 1074 [964, 1266] -5 [-7, -5] 0 [0, 0] 3118 [2620, 3618] 144 [87, 184] 13859 [6662, 21316] 0 [0, 0] 18345 [15176, 21920] 1906 [1070, 2984] 27358 [21924, 31727] 1336 [0, 3300] 0.18 [-0.25, 0.56] -0.00 [-0.01, 0.00] -0.01 [-0.02, 0.03] 304 [205, 436] 21300 [20100, 22200] 1989 [1948, 2016] 601 [398, 708] 38 [28, 50] 95 [92, 97] 27 [24, 30] 11288 [8365, 16843] 3283 [2330, 4770] 110046 [107390, 113610] 32336 [27280, 38695] 2468 [712, 3513] -9 [-13, -7] 967 [0, 1461] -17 [-28, -13] 32 [31, 34] 2401 [267, 4457] 11774 [7552, 18634] 393 [185, 570] 7594 [7525, 7670] -2 [-3, -1] 546 [470, 650] 8033 [5540, 9390] 8430 [6464, 9987] 25553 [23960, 27040] 190 [100, 370] 3860 [3394, 4470] 7992 [7194, 8458] 4780 [4605, 5008] 0 [0, 0] 18 [19, 19] 125 [100, 177] 6567 [4638, 7722] 7464 [7213, 7688] 38703 [35530, 45050] 15 [10, 19] 2005 [1596, 2348] -30000 [-30000, -30000] 772 [216, 1606] 515 [488, 532] 11993 [1240, 22010] -5 [-6, -4] 6 [-4, 20] 4433 [4190, 4830] 156 [154, 160] 37177 [17986, 54020] 0 [0, 0] 22721 [18131, 28516] 1790 [1560, 2010] 26211 [21118, 33567] 6746 [4940, 7760] 0.13 [-0.10, 0.58] 0.01 [0.00, 0.04] 0.03 [0.03, 0.06] 235 [136, 388] 13650 [9270, 17290] 1543 [1194, 1840] 531 [464, 593] 68 [58, 78] 95 [94, 97] 21 [18, 25] 12236 [9602, 14654] 3490 [2724, 4341] 88805 [77424, 100499] 32385 [29809, 34752] 4280 [2797, 6098] -11 [-14, -9] 1117 [883, 1287] -35 [-46, -27] 32 [32, 32] 1595 [644, 2511] 11483 [4836, 23229] 329 [286, 382] 7519 [7379, 7640] -5 [-7, -4] 471 [406, 528] 4616 [2392, 6998] 8535 [8189, 8924] 23422 [21084, 25671] 20 [0, 50] 3602 [3051, 4116] 7681 [6795, 8351] 4717 [4388, 5016] -1 [-3, 0] 13 [10, 17] 50 [20, 80] 3365 [2590, 4045] 6191 [5522, 6889] 33950 [27807, 39559] 10 [7, 13] 1421 [986, 1782] -30000 [-30000, -30000] 886 [637, 1106] 494 [459, 530] 991 [976, 1005] -7 [-9, -5] 0 [0, 0] 3616 [2880, 4259] 125 [99, 150] 17527 [8653, 28247] 0 [0, 0] 20490 [14091, 27791] 1549 [1375, 1708] 24249 [20860, 27018] 1488 [369, 2733] -0.13 [-0.38, 0.14] -0.03 [-0.07, -0.00] -0.04 [-0.08, -0.01] Published as conference paper at ICLR 2025 Task MR.Q No MR 1-step return No unroll Alien Amidar Assault Asterix Asteroids Atlantis BankHeist BattleZone BeamRider Berzerk Bowling Boxing Breakout Centipede ChopperCommand CrazyClimber Defender DemonAttack DoubleDunk Enduro FishingDerby Freeway Frostbite Gopher Gravitar Hero IceHockey Jamesbond Kangaroo Krull KungFuMaster MontezumaRevenge MsPacman NameThisGame Phoenix Pitfall Pong PrivateEye Qbert Riverraid RoadRunner Robotank Seaquest Skiing Solaris SpaceInvaders StarGunner Surround Tennis TimePilot Tutankham UpNDown Venture VideoPinball WizardOfWor YarsRevenge Zaxxon Mean Median IQM 2471 [1848, 3155] 443 [376, 499] 1125 [1094, 1160] 2216 [2081, 2346] 602 [493, 689] 445022 [282338, 630730] 542 [348, 749] 16520 [10560, 22760] 2007 [1855, 2194] 430 [383, 472] 50 [37, 65] 95 [94, 97] 25 [21, 32] 14954 [13541, 16508] 4348 [3756, 5002] 104766 [99290, 109629] 25962 [23406, 29182] 4660 [4072, 5241] -9 [-11, -9] 1480 [1378, 1592] -34 [-43, -27] 31 [31, 32] 4003 [2871, 5163] 4936 [3923, 5730] 275 [232, 322] 8391 [6845, 10060] -2 [-3, -1] 551 [534, 573] 4833 [2716, 7064] 8660 [8198, 9147] 26150 [21973, 30490] 12 [0, 34] 4395 [3799, 5002] 7511 [7085, 7911] 4843 [4635, 5033] -8 [-19, -1] 15 [13, 17] 100 [100, 100] 3600 [2554, 4366] 7362 [7062, 7630] 27152 [19731, 34480] 10 [9, 13] 2886 [2458, 3315] 312 [204, 421] 1105 [1075, 1142] 2298 [1920, 2731] 485 [387, 584] 12834 [9038, 17186] 404 [73, 726] 22000 [12100, 29080] 1849 [1726, 2004] 437 [333, 537] 84 [73, 95] 92 [89, 95] 15 [14, 18] 10517 [9157, 11949] 3394 [3140, 3764] 83734 [71466, 93070] 14469 [9247, 18763] 746 [527, 994] -14 [-18, -11] 897 [784, 1075] -21 [-30, -8] 26 [13, 33] 3098 [1645, 4358] 1833 [1552, 2114] 89 [0, 267] 7584 [7548, 7644] -5 [-8, -4] 433 [406, 462] 7508 [5608, 9500] 8403 [7433, 9087] 19066 [15450, 22028] 0 [0, 0] 3297 [2679, 3908] 3638 [3207, 3999] 4101 [3752, 4503] -27 [-67, -5] 12 [8, 16] 3068 [24, 9080] 4491 [3362, 5870] 7479 [6818, 8239] 29182 [23250, 35114] 11 [6, 16] 1342 [1226, 1454] 240 [213, 287] 1045 [904, 1134] 2708 [2110, 3710] 723 [689, 783] 60986 [25200, 87460] 123 [77, 160] 4700 [3600, 5800] 1438 [1213, 1604] 361 [290, 400] 44 [30, 67] 93 [92, 95] 14 [7, 23] 8927 [7098, 10732] 2520 [1980, 2900] 66980 [65140, 70490] 9658 [3245, 17870] 1861 [1675, 1990] -7 [-10, -6] 1064 [1055, 1069] -82 [-95, -64] 32 [32, 33] 1231 [254, 3182] 5597 [4242, 6512] 166 [70, 310] 7594 [7544, 7676] -6 [-8, -5] 518 [495, 545] 6940 [1520, 10460] 7386 [6611, 7881] 15300 [14760, 16350] 0 [0, 0] 2282 [1956, 2592] 5590 [5026, 5941] 3825 [3435, 4306] 0 [0, 0] 14 [9, 20] 100 [100, 100] 2517 [2032, 3105] 6733 [5928, 7759] 36145 [33590, 38700] 6 [6, 7] 2660 [2055, 3579] -30000 [-30000, -30000] 1556 [1248, 1866] -30000 [-30000, -30000] 2166 [2154, 2178] -30000 [-30000, -30000] 1262 [863, 1686] 478 [429, 524] 1146 [996, 1437] -6 [-7, -5] 0 [-1, 0] 3101 [2772, 3482] 130 [124, 139] 26477 [11956, 43260] 0 [0, 0] 18826 [15048, 23233] 1918 [1706, 2154] 27299 [23434, 30493] 3820 [2577, 4854] - - - 552 [319, 881] 550 [476, 632] 1272 [1020, 1730] -8 [-9, -7] 0 [-2, 0] 2440 [1540, 3178] 112 [65, 148] 25451 [17297, 34036] 0 [0, 0] 8524 [3899, 12519] 2058 [1632, 2640] 30666 [28659, 33225] 5758 [4712, 6962] -0.78 [-0.88, -0.69] -0.06 [-0.10, -0.01] -0.09 [-0.14, -0.05] 1107 [662, 1552] 389 [382, 396] 1290 [1000, 1580] -5 [-6, -5] 0 [-1, 0] 2535 [2030, 3040] 151 [120, 182] 4477 [2993, 5962] 0 [0, 0] 13506 [7955, 19058] 1545 [1490, 1600] 18513 [16940, 20088] 330 [0, 660] -0.70 [-0.81, -0.59] -0.12 [-0.12, -0.07] -0.15 [-0.16, -0.13] 3056 [2300, 3614] 385 [335, 471] 1079 [1003, 1140] 2388 [2190, 2740] 581 [406, 766] 57426 [40850, 88730] 783 [231, 1080] 23733 [19600, 26100] 1389 [1274, 1535] 443 [399, 489] 72 [60, 88] 95 [94, 96] 27 [17, 44] 11984 [11005, 13691] 2866 [1670, 3990] 104076 [82650, 114820] 34718 [29120, 44180] 2840 [2064, 3618] -11 [-14, -10] 1075 [1018, 1131] -54 [-63, -45] 32 [32, 33] 4182 [3473, 5451] 4746 [2378, 6822] 270 [145, 415] 6879 [5384, 7694] -2 [-4, -1] 546 [460, 625] 9166 [8520, 9500] 7785 [7176, 8271] 29020 [27150, 30700] 0 [0, 0] 2839 [2612, 2954] 6529 [6069, 6983] 4941 [4613, 5193] 0 [0, 0] 12 [8, 16] 100 [100, 100] 4220 [3918, 4508] 5856 [3607, 7855] 37636 [33610, 40160] 12 [11, 17] 2194 [2112, 2284] -30000 [-30000, -30000] 1088 [638, 1386] 551 [502, 596] 1423 [990, 2220] -5 [-10, 1] 0 [-2, 0] 2236 [1170, 3810] 148 [127, 181] 31342 [3170, 84771] 0 [0, 0] 27525 [22242, 35815] 1573 [1430, 1700] 19082 [11442, 24565] 0 [0, 0] -0.33 [-0.41, -0.28] -0.00 [-0.02, 0.00] -0.01 [-0.04, -0.00]"
        }
    ],
    "affiliations": [
        "Meta FAIR"
    ]
}