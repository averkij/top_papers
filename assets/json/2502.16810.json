{
    "paper_title": "Grounded Persuasive Language Generation for Automated Marketing",
    "authors": [
        "Jibang Wu",
        "Chenghao Yang",
        "Simon Mahns",
        "Chaoqi Wang",
        "Hao Zhu",
        "Fei Fang",
        "Haifeng Xu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This paper develops an agentic framework that employs large language models (LLMs) to automate the generation of persuasive and grounded marketing content, using real estate listing descriptions as our focal application domain. Our method is designed to align the generated content with user preferences while highlighting useful factual attributes. This agent consists of three key modules: (1) Grounding Module, mimicking expert human behavior to predict marketable features; (2) Personalization Module, aligning content with user preferences; (3) Marketing Module, ensuring factual accuracy and the inclusion of localized features. We conduct systematic human-subject experiments in the domain of real estate marketing, with a focus group of potential house buyers. The results demonstrate that marketing descriptions generated by our approach are preferred over those written by human experts by a clear margin. Our findings suggest a promising LLM-based agentic framework to automate large-scale targeted marketing while ensuring responsible generation using only facts."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 2 ] . [ 1 0 1 8 6 1 . 2 0 5 2 : r Grounded Persuasive Language Generation for Automated Marketing Jibang Wu Chenghao Yang Simon Mahns Chaoqi Wang Hao Zhu Fei Fang Haifeng Xu"
        },
        {
            "title": "Abstract",
            "content": "This paper develops an agentic framework that employs large language models (LLMs) to automate the generation of persuasive and grounded marketing content, using real estate listing descriptions as our focal application domain. Our method is designed to align the generated content with user preferences while highlighting useful factual attributes. This agent consists of three key modules: (1) Grounding Module, mimicking expert human behavior to predict marketable features; (2) Personalization Module, aligning content with user preferences; (3) Marketing Module, ensuring factual accuracy and the inclusion of localized features. We conduct systematic human-subject experiments in the domain of real estate marketing, with focus group of potential house buyers. The results demonstrate that marketing descriptions generated by our approach are preferred over those written by human experts by clear margin. Our findings suggest promising LLM-based agentic framework to automate large-scale targeted marketing while ensuring responsible generation using only facts."
        },
        {
            "title": "Introduction",
            "content": "While large language models (LLMs) have made significant strides across various tasks, their ability to persuade remains an underexplored frontier (see discussion of related work in Section 6). This however is particularly important capability since persuasion-related economic activities common thread in almost all voluntary transactions from advertising and lobbying to litigation and negotiation underpin roughly 30% of the US GDP (Antioch, 2013; McCloskey and Klamer, 1995), hence gives rise to tremendous opportunity for applying LLMs across wide range of sectors. Meanwhile, this same potential introduces serious trustworthiness concerns. If LLMs can generate persuasive content at scale, their influence on human opinions raises risks of misinformation, manipulation and misuse, especially in sensitive domains such as political campaigns (Voelkel et al., 2023; Goldstein et al., 2024). In addition to these profound economic and societal applications, the relationship between the nature of intelligence and persuasion has been fundamental research question since the time of early Greek philosophy. Aristotle viewed persuasion as both an art and an expression of intelligence, rooted in the ability to reason and communicate effectively. Yet, the Greeks also cautioned against the sophistry that overly relies on rhetorical and emotional techniques divorced from the truth. This tension becomes especially relevant today with the rise of generative AI. Notable figures, such as This work is supported by the AI2050 program at Schmidt Sciences (Grant G-24-66104) and NSF Award CCF-2303372. The first two authors contribute equally. University of Chicago, corresponding email: {wujibang,chenghao}@uchicago.edu Stanford University Carnegie Mellon University Preprint. Under review. Sam Altman, have predicted that AI systems could achieve superhuman persuasion without superhuman general intelligence (Altman, 2023). This dichotomy raises several crucial questions for LLM research: How can we reliably and consistently measure fact-based persuasiveness? Does greater intelligence inherently lead to stronger persuasive capabilities? And if not, what specific abilities must LLMs develop to truly master persuasion? Surprisingly little is known about these questions, and this is what we embark on in this paper. The faculty of observing, in any given case, the available means of persuasion. Aristotle, Rhetoric. In this paper, we study language generation for grounded persuasion particular form of persuasion, inspired by Aristotles philosophy, that is grounded in fact, tailored to the audience, and adapted to contextual factors. Grounded persuasion is crucial for applications in marketing and advertising, and its effectiveness can be linked directly to measurable behavioral changes (e.g., in terms of engagement and conversions) while constrained by factual accuracy. We choose real estate marketing as our testbed for grounded persuasion and construct realistic evaluation environment to include the process of preference elicitation and measure the persuasiveness of preference-based generation. To enable grounded persuasion in this context, we design an LLM-based agent with three key modules: Grounding Module, which mimics human expertise in signaling critical and credible selling points; Personalization Module, which tailors content to user preferences; and Marketing Module, which ensures factual accuracy and integrates localized features. We use this model-based approach to back up LLMs generation and are able to achieve superhuman persuasion in real estate marketing. Our findings lay the groundwork for leveraging LLMs in large-scale, targeted marketing and beyond, offering scalable solution to complex persuasion tasks in real-world applications. Our Contribution Our primary research objective is to demonstrate path towards effective design of persuasive language agents with backbone of more principled theory. Towards this end, we choose to focus on an application of automated marketing, where we employ the economic theory of strategic communication games to guide the agentic process ranging from processing products (factual) raw attributes, to selecting features to highlight, and ultimately to generate human-like marketing texts. We start from developing micro-economic framework for automated marketing, and then operationalize this framework by leveraging the capabilities of LLMs. In addition, we set up realistic evaluation process of the persuasiveness capability. This includes building large real estate dataset from Zillow, designing dedicated survey website to mimic the house search process and test with focused group of potential home buyers. Our experiments suggest marketing descriptions generated by our methods have clear winning edge of 70% over those written by expert human realtors."
        },
        {
            "title": "2 A Benchmark for Grounded Persuasion",
            "content": "Motivations and Challenges The first task of our study is to build consistent and effective evaluation benchmark. However, this effort faces several key challenges. fundamental challenge is the inherent subjectivity of persuasion, which depends heavily on human feedback. Unlike many other LLM capabilities, such as reasoning and planning, which have objective criteria for evaluation, persuasiveness lacks standardized metrics. The persuasiveness of message is determined by its recipient and can vary significantly with individual preferences and context. Another challenge arises from the multifaceted nature of persuasion process broadly studied in fields such as psychology, economics, and communication. Each field offers distinct models of In the influence. Thus, effective persuasion techniques can differ significantly across domains. realm of LLMs, most existing research has focused on political or opinion-based contexts, where persuasion often takes on an adversarial nature. Such contexts complicate evaluation due to the strong influence of subjective beliefs and cognitive biases. For instance, studies like those conducted by Hackenburg and Margetts (2024) and Matz et al. (2024) reached differing conclusions regarding the effectiveness of LLMs in personalized persuasion, despite using similar experimental setups. Moreover, Durmus et al. (2024) noted that opinion-based persuasion is susceptible to the anchoring effect, where participants initial beliefs strongly influence them, making opinion shifts difficult to measure accurately. Additionally, persuasion in these settings often resorts to deception, as fact2 checking is hard even for experienced humans. Durmus et al. (2024) found that prompting models to fabricate information was the most effective strategy under current evaluation. These challenges underscore the critical need to go beyond existing studies and develop new benchmarks that evaluate persuasion in more controlled, fact-based contexts. Thus, we develop an evaluation framework customized for the task of grounded persuasion. Real Estate Marketing as Testbed crucial aspect of our design is to identify domain that aligns well with grounded persuasion. We select real estate marketing as the primary testbed for several reasons. First, the real estate sector is characterized by high-stakes economic decisions, where potential buyers tend to hold more fact-based, rational beliefs compared to political or emotionally charged domains. In this environment, persuasive language must not only resonate with potential buyers but also remain truthful and contextually relevant. This makes it an ideal setting for testing the principles of grounded persuasion. Second, real estate marketing involves complex decision-making processes, where strong persuasive capabilities can significantly influence outcomes. Experienced realtors earn commissions reflecting the economic value of their persuasion skills. In addition, the potential of generative AI in this domain has been highlighted by notable anecdote (User, 2023) claiming an LLM successfully facilitated home sale without an agent. Third, the availability of extensive datasets in the real estate sector enables us to extract valuable domain-specific knowledge for training LLMs and conducting thorough empirical evaluations. Leveraging these resources, our benchmark provides robust and consistent means to assess LLMs grounded persuasion abilities, offering scalable solution for evaluating persuasion in high-stakes scenarios. Realistic Evaluation Interface We design the framework to ensure realistic evaluation interface based on two key criteria. First, we aim to create an immersive experience to gather authentic human feedback on marketing content persuasiveness. Second, we need to naturally elicit human preferences to properly test dynamic personalized content generation. As such, we collect real data from more than 50k real estate listings on the market and design web interface that mimics online platforms, allowing the model to observe buyers general profiles and behaviors (e.g., recently browsed or liked listing). See Appendix and for full description of the web interface and dataset. Measuring the Fact-based Persuasiveness To evaluate model performance in grounded persuasion, we are particularly interested in how buyer behaviors are influenced by the generated content and whether it is factually accurate. Hence, we set an interface to present to the potential buyers each time single listing along with two descriptions generated by two distinct models, then ask them to choose which description makes them more interested. We use the Elo score (Elo, 1967) to measure the relative persuasiveness of text generated by different models. Additionally, we assess whether the generated content is factually accurate. We defer the detailed experiment setup to Section 5."
        },
        {
            "title": "3 A Micro-foundation of Marketing",
            "content": "Marketing fundamentally is about communicating product information, often selectively, to shape potential buyers perceptions and influence their purchasing decisions. This process of information signaling, also known as persuasion, has been extensively studied in decision theory and information economics (Spence, 1978; Arrow, 1996; Kamenica and Gentzkow, 2011; Connelly et al., 2011), though typically within stylized mathematical models. However, to enable practical applications of automated marketing in natural language, we need to conceptualize previous mathematical models/findings to develop framework that is ready to be plugged into todays language generation technology; this is what we embark on next. Formally, we represent generic product (e.g., house on sale or an item on Amazon) as an n-dimensional vector = (X1, X2, . . . , Xn). Each Xi is called raw attribute (or simply attribute). Attributes capture the factual and measurable characteristics of the product, such as the square footage of house, floor number, or distance to metro stations. specific product instance is denoted by vector = (x1, , xn) where xi Xi is the realized value of attribute Xi. Let = ΠiXi be the domain of x. Marketers often choose to emphasize certain attractive properties of product, which are grounded in its underlying raw attributes. We refer to these attractive properties as signaling features (or simply features). For example, common features in real estate marketing include spacious layout, 3 bright room, prime location. Importantly, features differ from attributes: while some attributes may directly serve as features, features generally capture the more abstract (and sometimes ambiguous) properties. We denote the feature set as = (S1, , Sm), with feature vector = (s1, , sm), where each value si [0, 1] is normalized to capture the intensity of feature Si for this product (alternatively, si can be interpreted as the probability of having feature Si). For example, Si could be bright room and correspondingly si denotes the extent to which rooms of the house are bright. In practice, the value of xi and sj can be assessed by domain experts. Signaling via the Attribute-Feature Mapping In our model, signaling features convey partial information to influence potential buyers beliefs, leveraging the inherent cognitive mapping in human natural language. For example, feature bright room may imply multiple facts to potential buyers: high floor, modern light system, large room size, sun-side facing, etc., though all in probabilistic sense. Such partial information carried by the signaling features would shape the buyers perceptions of the product and influence their behaviors (e.g., to visit the open house). We denote such mapping as π : [0, 1]m that maps any product represented by raw attribute values to its feature values [0, 1]m. That is, = π(x). Sometime, we use s(x) to emphasize the dependence of on the underlying attributes x, and sj(x) is its j-th entry. In practice, this attribute-feature mapping π describes the common sense that, given attribute values about the product, to an extent of sj that we can say the product has feature Sj. This type of attribute-feature mapping π is widely used in both machine learning and economics. In the Bayesian statistics literature, Xi is called the observable variable whereas Sj is the latent variable and the mapping π is the probabilistic model that captures the dependence between random variables and s. In information economics, Xi is state, Sj is signal of the states, and π is known as signaling scheme. Generally, signals can be strategically designed to carry certain partial information about the states, and economists have made significant progress in their optimal design to influence the equilibrium outcomes (Kamenica and Gentzkow, 2011; Bergemann et al., 2015; Bergemann and Morris, 2019). This work extends beyond the economic modeling and analysis of prior studies, which are largely rooted in Bayesian theory, to address the nuanced role of natural language that has previously been abstracted away, and to focus on uncovering the signaling mapping of common sense rather than designing new signaling schemes. Marketing Design under Information Asymmetry The essence of marketing is to take advantage of the information asymmetry between sellers and buyers (Grossman, 1981; Lewis, 2011; Dimoka et al., 2012; Kurlat and Scheuer, 2021). This important insight, along with its broader implications in general economic markets, was notably recognized by the 2002 Nobel Economics Prize (Akerlof, 1978; Spence, 1978; Stiglitz, 1975; Lofgren et al., 2002). In our case, the seller or seller agent may perfectly know the product attribute values and its feature values s(x) but buyer enters the market with only prior belief µ on the distribution of product attributes in . Hence, lacking concrete details of the particular product x, the buyer begins with an expected belief on the distribution of features as follows, Initial belief of features: s(µ) = (cid:90) xX s(x)dµ(x). (1) Given the asymmetric feature beliefs between the buyer and seller, the purpose of marketing can be described as revealing features, subject to communication constraints, to shift the buyers belief from s(µ) towards s(x) with the goal of increasing the products attractiveness to the buyer. Grounded Persuasion in Natural Language The remaining part of our model is to optimize the persuasiveness of marketing content. The typical approach in economic theory is to develop models to capture buyers belief updates and decision-making processes. However, these models are challenging to implement in practice due to the lack of concrete buyer utility functions and behavior modeling. Instead, our idea is to harness the power of LLM in persuasive language generation using instructions and heuristics tailored for grounded persuasion. At high level, our approach is to use the attribute-feature mapping π to propose and design heuristics to select subset of features to emphasize the generated content. We also elicit and provide the user preference to use in an LLM prompt for personalization. We hypothesize that the LLM could approximately solve an implicit optimization problem for persuasive language generation: = arg max LL Pr(LI , , r) arg max LL(x) r(L). (2) That is, the language L, output by an LLM provided carefully designed prompts , selected features and user preferences r, could approximately maximize users preference-adjusted persuasiveness function r. Moreover, the generated language will obey product facts (i.e., is grounded), or concretely, be drawn from set L(x) that includes all languages consistent with the product attribute x. Our later agent implementation and its empirical performance validates this hypothesis to very good extent. Based on this hypothesis expressed in Equation (2), our design boils down to better assist an LLM in solving this optimization problem by providing effective prompts , good selection of features and representation of preferences r. In the sequel, we discuss implementation details developed upon this model."
        },
        {
            "title": "4 The Agentic Design of AI Realtor",
            "content": "This section describes our core design ideas of AI Realtor, an AI agent that can automatically process multiple levels of marketing information to compose persuasive descriptions for real estate listings and actively learn to adapt the generated language towards the personal preference of each buyer. At high level, our approach seeks to operationalize previous micro-economic models by practically implementing the following three key ingredients: Grounding Module: identify the attribute-feature mapping π in accordance with the conventions from the domain-specific marketing problem; Personalization Module: elicit and represent buyer preferences r; Marketing Module: select useful yet factual marketing features based on π, above to ensure genuine generation of the marketing content. ingredients, we Armed with these the arg maxLL Pr(LI , , r) problem in Equation (2) with proper prompt . The entire pipeline of our approach can be found in Figure 1. Below we highlight novel ingredients in the above three modules (blue-colored in Figure 1) and defer the full implementation description (including the prompting engineering details as the green-colored in Figure 1) to Appendix C. then employ prompt engineering to solve Figure 1: Illustration of the Design Pipeline of AI Realtor. 5 4.1 Grounding Module: Predicting Credible Features for Marketing Our model assumes the existence of attribute-feature mappings in the marketing problem, which can be used by marketers to influence potential buyers beliefs and behaviors. However, key challenge lies in determining how to accurately obtain such mappings. Specifically, we must identify under what conditions it is valid to market house as possessing particular feature (e.g., convenient transportation) in order to signal its attractiveness. The traditional approach of acquiring this knowledge from human experts for each house listing is labor-intensive, hence is costly to scale up and difficult to personalize. Instead, we take machine learning approach to uncover the mapping from our experiment dataset. While our raw dataset contains no annotation of any signaling feature, we employ LLMs to construct high-quality feature schema and label the dataset accordingly in preparation for learning the attribute-feature mapping. This approach presents novel unsupervised learning paradigm that harnesses the broad knowledge of LLMs to distill expert-level insights from unlabeled data with minimal human supervision. Figure 2: Illustration of the inductive feature schema construction pipeline. Our dataset only contains the raw attributes of each listing appearing on the Zillow platform. To learn high-quality attribute-feature mapping, key challenge we face is that there are too many possible tokens that can serve as the signaling features in the natural language space, and many of these tokens might have duplicate or similar meanings. Without structured representation of features, the resulting label classes may be too sparse for effective learning. Indeed, we discover that the features obtained by directly prompting an LLM include many similar features while missing some important ones. This would hinder the learning process of attribute-feature mapping. To overcome the above challenge of sparse label class, the first step of our approach is to obtain good representation of feature schema S. Specifically, we design more sophisticated prompting strategy to inductively improve the quality and representation of the feature schema. high-level sketch of our constructed pipeline can be found in Figure 2, but the key idea here is to provide LLMs with sufficiently many feature candidates from the dataset and condense them into hierarchical schema. Finally, we only need to employ small number of human annotators to evaluate the quality of the generated feature schema to monitor potential hallucinations and make additional refinements based on their feedback. With the constructed feature schema above, we then guide the LLM to annotate for each house listing, described by attributes x, whether each feature si is described in the human-written text. With LLM assistance, we are able to obtain curated labeled data, after few standard cleaning procedures (e.g., removing low-quality texts, normalizing and embedding listing attributes). We then train neural network to learn the attribute-feature mapping. With random train-test split of 4 : 1 ratio in our dataset, we achieved testing accuracy 69.39% and F1 score 67.43%. This accuracy is already high, given the large amount of available features and stochastic nature of the signaling process. To guarantee the grounded use of signaling features, we implement deterministic feature selection strategy to only use feature Sj with value sj (recall its interpretation as the feature intensity) above some threshold α. As simple heuristics in our implementation, we set the threshold α = 1/2 and we will refer to this set of features as, Marketable Features: S1(x) = {Sj : sj(x) α}. (3) 4.2 Personalization Module: Aligning with Preferences This stage seeks to steer the persuasive language generation toward the buyers preference, which is another crucial objective of grounded persuasion. Our solution has two steps. The first step is to effectively elicit useful information to infer users preference and structure it in good representation. On real-world platforms such as Zillow and Redfin, this could be done with mature ML techniques by analyzing each potential house hunters browsing behaviors on their website. However, we did not have access to these platforms. Fortunately, since the validation of our methods is through systematic human subject experiments, we thus designed preference elicitation questions to elicit human subjects preferences. Specifically, our web interface programs an LLM to act like human realtor and to effectively narrow down the features that each buyer cares about. We then survey human subjects to provide rating rj on the importance of each feature Sj to them, before they start evaluation tasks. While it certainly could be more sophisticated, this preference elicitation procedure already suffices to build strong AI Realtor that very nicely adapts to human preferences, as shown in our experiments. The second step is to select subset of features based on user preference in order to positively shift the users belief. However, we cannot simply rely on data-driven machine learning approach for personalization because real-world marketing texts are not optimized for individual users hence Instead, we use scoring approach to produce score for each cannot supervise the learning. feature that balances its validity and user preference. Specifically, we adjust the population-level feature scores s(x) with the users rating over each feature and only select those features whose scores are above some threshold α: Personalized Features: S2(x) = {sjsj(x) + c(rj r0) α}, where the constant reflects the intensity of personal preference, r0 is the basis rating of each attribute. We feed these features to LLMs, allowing them to decide which personalized features to emphasize in the final generation. 4.3 Marketing Module: Capturing Surprisal via RAG The last stage is designed to better ground persuasive language generation in factual evidence, problem contexts and localized information in automated marketing. Our design here is inspired by rich marketing strategy research (Lindgreen and Vanhamme, 2005; Ludden et al., 2008; Ely et al., 2015), which have shown that buyers would derive entertainment utility from surprising effects/features and have deeper impression. In our setting of real estate marketing, such surprising features are those that are relatively rare compared to their surrounding area. Formally, we determine set of surprising features based on their percentile in the feature distribution as follows, Surprising Features: S3(x) = {Sj S1 : sj(x) is within β-quantile of distribution sj(µ)}. This gives the LLMs localized feature information at different levels of granularity obtained through Retrieval Augmented Generation (RAG) (Lewis et al., 2020). Such behavioral economics-driven design proves to be highly effective; citing one of the human subjects in our experiment (see the full description in Appendix A.1), who was asked about why they liked listing description (without knowing it was AI-generated): ...Description specifically points out the rarity of the ample storage and built-in cabinetry in similarly priced listings, making the property stand out."
        },
        {
            "title": "5 Evaluations",
            "content": "5.1 Evaluation by Human Feedback To evaluate the effectiveness of listing descriptions generated by different models, we draw inspiration from Chatbot Arena (Zheng et al., 2023) and conduct an online survey to collect pairwise human feedback comparing different models outputs. In summary, systematic evaluation by human feedback shows that our AI Realtor clearly outperforms human experts and other model variants, measured by standard metric of Elo ratings (Elo, 1967). Below, we detail the design of our user survey platform, baseline setup, and evaluation metrics, followed by report on the human evaluation results. Quality Assurance We focus on the major US city Chicago with highly active housing market. We recruit about 100 participants from the popular Prolific platform for human-subject experiments, selecting in-state residents familiar with Chicagos housing market and curating approximately 1,000 listings of varied sizes and price ranges. Each human subject is tasked with comparing 7 (a) Elo Ratings (b) Win Rates Figure 3: Comparison of model performance using Elo ratings and win rates. Elo ratings represent overall persuasiveness, and win rates reflect relative persuasiveness. Both metrics are based on evaluations by human subjects. 10 pairs of house descriptions. During each comparison, the human subject sees pictures and all basic information about house, and then faces two listing descriptions without knowing what methods (human realtor or AI agents) generate them, and is asked to choose which description is preferred, and by how much (see Appendix B.3 for details). Notably, AI Realtor generates personalized descriptions on the fly for each human subject, based on their preferences elicited while they join the survey (see Appendix B.2 for details). To ensure feedback quality, we implement several measures: (1) Screening tests to confirm participants can extract information from listings and follow specific home search motives (See Appendix B.1 for details); (2) Attention checks using pairs of nearly identical descriptions to ensure participants carefully compare and identify differences; (3) Control experiments where participants compare human-written, engaging descriptions against LLM-generated descriptions intentionally prompted to be plain and unappealing, verifying their ability to favor high-quality descriptions; and (4) Incentives on the platform, including bonus payments and requests for written reasoning behind choices, to encourage consistent, well-justified feedback. Metrics We adopt the Elo rating score as our main metric. We use typical choice of the initial Elo rating as 1000, scaling parameter = 400, and learning rate = 32, which corresponds to model win rate: (cid:2)1 + 10(e1e0)/c(cid:3)1 , where e0 and e1 are the Elo ratings of two models being compared. In addition to our primary persuasion model AI Realtor, we evaluate several Baseline Models baseline models, including: Vanilla, an LLM prompted with all attributes of the listing; SFT, an LLM fine-tuned with supervised training and prompted with all features of the listing; Human, listing descriptions sourced from Zillow, written by professional realtors; Control, the model used in the control experiment described earlier. We also include two ablation models based on AI Realtor: one that only uses the marketable feature from the Grounding module, the other excludes surprisal features from the Marketing module. Additionally, we experiment with two LLM variants, GPT-4o and GPT-4o-mini, while keeping the prompt instructions consistent across models. Results We plot the Elo ratings of different models in Figure 3a. The results reflect clear trend: while vanilla GPT-4o performs on par with humans (1052 vs 947), each of our designed module enhancement progressively improves the persuasiveness of the generation, ultimately surpassing human performance with clear margin (1318 vs 947). Also observe that using GPT-4o to generate listing description does have clear edge compared to that of GPT-4o-mini. Moreover, we plot empirical win rates among three major competitors (Vanilla, Human and AI Realtor)) in Figure 3b, which directly illustrates how much AI Realtor outperforms the other two.1 Please see Appendix for case studies of our model-generated descriptions with more nuanced observations. 1Human subjects in our experiments are also asked about how much they prefer one description with 1-5 rating. 8 5.2 Evaluation through AI Feedback Human feedback can be costly, especially as we scale the training and evaluation of our task. In this section, we report our empirical evaluation by using AIs to simulate human feedback based on our data collected from the above human-subject experiments. Simulation Setup We employ an LLM to simulate the responses of buyers in the previous experiment. We use the first pairwise comparison results as K-shot in-context learning samples and prompt the LLM to predict the same buyers selections for the remaining samples. We also adopt the chain-of-thought prompting format (Wei et al., 2022) and provide the buyers rationale comments as the information for in-context learning (see Appendix F.7 for the exact prompt). We use the Sotopia framework (Zhou et al., 2024) to configure this simulation agent with GPT-4o-mini (OpenAI, 2024b) as the base model. Metrics We use two metrics to evaluate the reliability of AI feedback compared to human feedback: 1) Shot-wise Simulation Accuracy (SSA): the prediction accuracy averaged across users for each shot; 2) User-wise Simulation Accuracy (USA): the prediction accuracy for each user, averaged across #(shots). The first metric measures overall simulation accuracy across the entire population, while the second one measures simulation accuracy for each user. Effectiveness of AI Feedback The simulation results under both metrics are shown in Figure 4a and 4b. The model achieves 61.6% accuracy across users and exhibits non-trivial (> 50%) performance for 79.2% of users, suggesting potential for leveraging AI feedback. However, the accuracy remains unsatisfactory for reliable evaluation. Additionally, the variance in the USA metric is high and increases with more provided shots, underscoring the challenges of personality simulation, as highlighted in (Wang et al., 2024). While the upward trend in variance is expected due to fewer data points, it highlights the difficulty of predicting user preferences dynamically. To further understand the limitations of AI-simulated feedback, we conduct manual analysis of simulation errors. Excluding the 56.1% error cases that lack clearly explainable patterns, we attribute the rest of them to several key error sources in Figure 4c: 1) Length Bias: Similar to the observation in Chatbot Arena (Zheng et al., 2023), the model overly favors longer responses; 2) Tie Comments: Buyers consider the influence from descriptions as indifferent yet still cast confident votes in one of the choices; 3) Emergent Preference: While the model only has access to buyers pre-established preference, buyers selections in some cases reflect some unspecified preferences or ones in contradiction; 4) Only Until Late: Correct predictions about buyers selection only emerge after sufficient in-context samples; 5) Model Confusion: The models prediction appears random, which indicates that the model may not have sufficient information to simulate such buyer. Some of these errors can be mitigated by collecting more selection data from each buyer or improving the preference elicitation process in future work. (a) Shot-wise Accuracy (b) User-wise Accuracy (c) Error Case Attribution Figure 4: Analyses of Simulating Human Feedback with AI Feedback. 5.3 Hallucination Checks For grounded persuasion, it is important to ensure minimal risks of hallucination. Hence, we evaluate the amount of misinformation in the marketing content through the fine-grained fact-checking test (Min et al., 2023), where we use GPT-4o to assist our hallucination check and set the listing attributes in the dataset as atomic facts. Specifically, we consider two types of factual attributes to check, Xhard and Xsoft. For attributes in Xhard, we require the attribute description to be completely 9 Figure 5: Faithfulness Scores for Hallucination Checks. accurate (e.g., the number of bathrooms), whereas we allow attributes in Xsoft to be roughly accurate (e.g., the home address). For given attribute set and description L, we ask the model to perform the following functions (with the exact prompts in Appendix E): supp(L, X) outputs the set of attributes in that are mentioned in the description L; evalhard(L, x) outputs the binary value whether attribute is accurately described; evalsoft(L, x) outputs how accurate is described with value on 0-10 scale. We then compute the faithfulness score for attributes in Xhard as follows, Faithfulhard(L) = (cid:80) xsupp(L,Xhard) evalhard(L, x) supp(L, Xhard) , and for attributes in Xsoft as follows, Faithfulsoft(L) = (cid:80) xsupp(L,Xsoft) evalsoft(L, x)/10 supp(L, Xsoft) . As shown in Figure 5, the model-generated descriptions are mostly faithful to listing information with minimal hallucination under both metrics. In contrast, the descriptions from human realtors or SFT model show an even higher level of hallucination. After digging into details, we found that this is due to human realtors (also SFTs) vague description of attributes in Xhard such as the following example, This 4 bedroom, 3.5 bathroom home offers nearly 2,000 (1,828) sqft of living space.... Our AI Realtor, however, tends to accurately describe factual attributes whenever mentioned, likely due to its preference to copy from context interestingly, this preference seems to be forgotten by the model after supervised fine-tuning on human-written descriptions. That said, it is debatable whether such vague descriptions of attributes is true kind of hallucination, though some buyers did complain about this kind of language in the comments of their responses."
        },
        {
            "title": "6 Related Work",
            "content": "While this paper embarks on the task of grounded persuasion for language agents, there have been few recent work in evaluating the persuasive capabilities of LLMs (Durmus et al., 2024) with key focus on the risks of LLM-generated propaganda in politically sensitive domains (Voelkel et al., 2023; Goldstein et al., 2024; Hackenburg et al., 2024; Luciano, 2024). In addition, several work focus on the power of LLM in personalized persuasion (Hackenburg and Margetts, 2024; Salvi et al., 2024; Matz et al., 2024). Breum et al. (2024) study the potential impact of LLM in the opinion dynamics of multi-round persuasion. Meanwhile, several LLM capabilities related to persuasion have been investigated including negotiation (Bianchi et al., 2024), debate (Khan et al., 2024) and sycophancy (Sharma et al., 2023; Denison et al., 2024). The persuasion capability also relates to the rationality of LLMs in strategic games, as assessed in Chen et al. (2023) and Raman et al. (2024). Similar to our work, Angelopoulos et al. (2024) also study the potential of LLMs in marketing applications. With marketing email generated by fine-tuned LLM, they report 33% improvement in email click-through rates compared to human expert baselines. In comparision, our work develops full agentic solution for automated marketing from learning domain expert knowledge to crafting localized features, which significantly outperforms the model with supervised fine-tuning in our human-subject experiments."
        },
        {
            "title": "Impact Statement",
            "content": "Our research contributes to the principled design of strategic language agents, underpinned by rigorous theoretical foundations. This work has implications for both the development of AI-driven persuasive agents and the broader study of language-based strategic interactions. From an ethical standpoint, we recognize the potential risks of deploying persuasive language agents, particularly regarding LLM hallucinations and misinformation. To address this, we conduct fine-grained fact-checking analysis (see Section 5.3) and find no substantial hallucination risks in our designed agents. However, we acknowledge that this remains an open challenge and encourage further investigations into mitigating potential unintended consequences. To promote transparency and reproducibility, we have obtained IRB approval (exempt) for our data collection and annotation. The Zillow-based source data used in our study are publicly available and processed to remove identifiable information. Additionally, we will release our codes and annotation data (subject to IRB requirements and annotator agreements) to foster continued research in this area. Given the growing use of LLMs in high-stakes applications, we encourage further research into the ethical and societal implications of persuasive AI, including potential risks of manipulation and misinformation. We hope this work lays foundation for the responsible development and deployment of strategic language models."
        },
        {
            "title": "References",
            "content": "George Akerlof. The market for lemons: Quality uncertainty and the market mechanism. In Uncertainty in economics, pages 235251. Elsevier, 1978. Sam Altman. Twitter post. https://x.com/sama/status/1716972815960961174, 2023. [Online; accessed September 17, 2024]. Panagiotis Angelopoulos, Kevin Lee, and Sanjog Misra. Causal alignment: Augmenting language models with a/b tests. Available at SSRN, 2024. Anthropic. Claude 3.5 sonnet, 2024. URL https://www.anthropic.com/news/ claude-3-5-sonnet. AI language model. Gerry Antioch. Persuasion is now 30 per cent of us gdp: Revisiting mccloskey and klamer after quarter of century. Economic Round-up, (1):110, 2013. Kenneth Arrow. The economics of information: An exposition. Empirica, 23(2):119128, 1996. Dirk Bergemann and Stephen Morris. Information design: unified perspective. Journal of Economic Literature, 57(1):4495, 2019. Dirk Bergemann, Benjamin Brooks, and Stephen Morris. The limits of price discrimination. American Economic Review, 105(3):921957, 2015. Federico Bianchi, Patrick John Chia, Mert Yuksekgonul, Jacopo Tagliabue, Dan Jurafsky, and James Zou. How well can llms negotiate? negotiationarena platform and analysis. arXiv preprint arXiv:2402.05863, 2024. Steven Bird, Ewan Klein, and Edward Loper. Natural language processing with Python: analyzing text with the natural language toolkit. OReilly Media, Inc., 2009. Simon Martin Breum, Daniel Vædele Egdal, Victor Gram Mortensen, Anders Giovanni Møller, and Luca Maria Aiello. The persuasive power of large language models. In Proceedings of the International AAAI Conference on Web and Social Media, volume 18, pages 152163, 2024. Yiting Chen, Tracy Xiao Liu, You Shan, and Songfa Zhong. The emergence of economic rationality of gpt. Proceedings of the National Academy of Sciences, 120(51):e2316205120, 2023. Brian Connelly, Trevis Certo, Duane Ireland, and Christopher Reutzel. Signaling theory: review and assessment. Journal of management, 37(1):3967, 2011. 11 Carson Denison, Monte MacDiarmid, Fazl Barez, David Duvenaud, Shauna Kravec, Samuel Marks, Nicholas Schiefer, Ryan Soklaski, Alex Tamkin, Jared Kaplan, et al. Sycophancy to subterfuge: Investigating reward-tampering in large language models. arXiv preprint arXiv:2406.10162, 2024. Angelika Dimoka, Yili Hong, and Paul Pavlou. On product uncertainty in online markets: Theory and evidence. MIS quarterly, pages 395426, 2012. Esin Durmus, Liane Lovitt, Alex Tamkin, Stuart Ritchie, Jack Clark, and Deep Ganguli. Measuring the persuasiveness of language models, 2024. Arpad Elo. The proposed uscf rating system, its development, theory, and applications. Chess life, 22(8):242247, 1967. Jeffrey Ely, Alexander Frankel, and Emir Kamenica. Suspense and surprise. Journal of Political Economy, 123(1):215260, 2015. Josh Goldstein, Jason Chao, Shelby Grossman, Alex Stamos, and Michael Tomz. How persuasive is ai-generated propaganda? PNAS nexus, 3(2):pgae034, 2024. Sanford Grossman. The informational role of warranties and private disclosure about product quality. The Journal of law and Economics, 24(3):461483, 1981. Kobi Hackenburg and Helen Margetts. Evaluating the persuasive influence of political microtargeting with large language models. Proceedings of the National Academy of Sciences, 121(24): e2403116121, 2024. Kobi Hackenburg, Ben Tappin, Paul Rottger, Scott Hale, Jonathan Bright, and Helen Margetts. Evidence of log scaling law for political persuasion with large language models. arXiv preprint arXiv:2406.14508, 2024. Albert Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024. Emir Kamenica and Matthew Gentzkow. Bayesian persuasion. American Economic Review, 101 (6):25902615, 2011. Akbir Khan, John Hughes, Dan Valentine, Laura Ruis, Kshitij Sachan, Ansh Radhakrishnan, Edward Grefenstette, Samuel Bowman, Tim Rocktaschel, and Ethan Perez. Debating with more persuasive llms leads to more truthful answers. arXiv preprint arXiv:2402.06782, 2024. Pablo Kurlat and Florian Scheuer. Signalling to experts. The Review of Economic Studies, 88(2): 800850, 2021. Gregory Lewis. Asymmetric information, adverse selection and online disclosure: The case of ebay motors. American Economic Review, 101(4):15351546, 2011. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Kuttler, Mike Lewis, Wen-tau Yih, Tim Rocktaschel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems, 33: 94599474, 2020. Adam Lindgreen and Joelle Vanhamme. Viral marketing: The use of surprise. Advances in electronic marketing, pages 122138, 2005. Karl-Gustaf Lofgren, Torsten Persson, and Jorgen Weibull. Markets with asymmetric information: the contributions of george akerlof, michael spence and joseph stiglitz. The Scandinavian Journal of Economics, pages 195211, 2002. Floridi Luciano. Hypersuasionon ais persuasive power and how to deal with it. Philosophy & Technology, 37(2):110, 2024. Geke DS Ludden, Hendrik NJ Schifferstein, and Paul Hekkert. Surprise as design strategy. Design Issues, 24(2):2838, 2008. 12 SC Matz, JD Teeny, Sumer Vaid, Peters, GM Harari, and Cerf. The potential of generative ai for personalized persuasion at scale. Scientific Reports, 14(1):4692, 2024. Donald McCloskey and Arjo Klamer. One quarter of gdp is persuasion. The American Economic Review, 85(2):191195, 1995. Rui Meng, Ye Liu, Shafiq Rayhan Joty, Caiming Xiong, Yingbo Zhou, and Semih Yavuz. Sfrembedding-mistral: enhance text retrieval with transfer learning. Salesforce AI Research Blog, 3, 2024. Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Koh, Mohit Iyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi. FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 1207612100, 2023. OpenAI. Gpt-4o, 2024a. Available at: https://openai.com/index/hello-gpt-4o/. intellicost-efficient OpenAI. https://openai.com/index/ gence, gpt-4o-mini-advancing-cost-efficient-intelligence/. Accessed: 202409-19. Gpt-4o 2024b."
        },
        {
            "title": "Advancing",
            "content": "mini:"
        },
        {
            "title": "July",
            "content": "Narun Krishnamurthi Raman, Taylor Lundy, Samuel Joseph Amouyal, Yoav Levine, Kevin LeytonBrown, and Moshe Tennenholtz. Steer: Assessing the economic rationality of large language models. In Forty-first International Conference on Machine Learning, 2024. Francesco Salvi, Manoel Horta Ribeiro, Riccardo Gallotti, and Robert West. On the conversational persuasiveness of large language models: randomized controlled trial. arXiv preprint arXiv:2403.14380, 2024. Mrinank Sharma, Meg Tong, Tomasz Korbak, David Duvenaud, Amanda Askell, Samuel Bowman, Newton Cheng, Esin Durmus, Zac Hatfield-Dodds, Scott Johnston, et al. Towards understanding sycophancy in language models. arXiv preprint arXiv:2310.13548, 2023. Michael Spence. Job market signaling. In Uncertainty in economics, pages 281306. Elsevier, 1978. Joseph Stiglitz. The theory of screening, education, and the distribution of income. The American economic review, 65(3):283300, 1975. Reddit User. Chatgpt helped me save $50k buying/selling house. reddit.com/r/ChatGPT/comments/12z8g3l/chatgpt_helped_me_save_ 50k_buyingselling_a_house/, 2023. [Online; posted April 27, 2023]. https://www. Jan Voelkel, Robb Willer, et al. Artificial intelligence can persuade humans on political issues. 2023. Danqing Wang, Kevin Yang, Hanlin Zhu, Xiaomeng Yang, Andrew Cohen, Lei Li, and Yuandong Tian. Learning personalized alignment for evaluating open-ended text generation. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 13274 13292, 2024. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36:4659546623, 2023. Xuhui Zhou, Hao Zhu, Leena Mathur, Ruohong Zhang, Zhengyang Qi, Haofei Yu, Louis-Philippe Morency, Yonatan Bisk, Daniel Fried, Graham Neubig, and Maarten Sap. Sotopia: Interactive evaluation for social intelligence in language agents. 2024. URL https://openreview. net/forum?id=mM7VurbA4r."
        },
        {
            "title": "A Case Studies",
            "content": "A.1 User Feedback on Generated Descriptions with Surprisal Features Preferred Description (AI Realtor, Desc-B): Discover the splendid charm of urban living at 547 Clark St APT 405, chic one-bedroom, one-bathroom condo in the heart of Chicagos desirable Printers Row. Priced at an enticing $297,500, this condo harmonizes affordability with luxury, featuring an expansive 876 sqft layout adorned with elegant hardwood floors throughout and cozy gas fireplace. Enjoy culinary delights with new stainless steel appliances and eyecatching granite countertops in the kitchen. The huge bedroom suite boasts walk-thru closet area, offering ample built-in cabinet space and additional storage an rarity in similarly priced listings. Revel in the tranquility of your spacious private balcony, perfect for unwinding with views of the bustling cityscape. Keep convenience at your fingertips with in-unit side-by-side laundry and parking in secure garage. Built in 1998, this condo is stones throw away from Millennium Park and surrounded by public transportation, grocery stores, and cultural amenities, amplifying its urban attraction. With manageable monthly HOA fee of $389 and strong investment potential, this condo ensures financial peace of mind alongside lifestyle enrichment. Dont miss out on this prime opportunity in vibrant Cook County, where your ideal location meets impeccable structure and style. Disfavored Description (AI Realtor [w/o Grounding], Desc-A): Welcome to your dream condo in the vibrant Printers Row neighborhood! This stunning one-bedroom, one-bathroom home is not only affordable but packed with modern amenities and stylish features, all for $297,500. Nestled in the heart of Chicago, 547 Clark St APT 405 offers unparalleled convenience with easy access to public transportation, shopping, dining, and the iconic Millennium Park. Built in 1998, this condo boasts huge bedroom suite, hardwood flooring throughout, and an inviting gas fireplace. The newly upgraded stainless steel appliances and eye-catching granite countertops make the kitchen chefs delight. Enjoy your morning coffee on the spacious private balcony, while additional storage and walk-thru closet area cater to all your organizational needs. The property is equipped with central air and in-unit laundry for ultimate comfort and convenience. With $389 monthly HOA fee, this condo promises both affordability and secure investment in the bustling heart of Chicago. Dont miss the opportunity to embrace lifestyle of ease and style in this perfect one-bedroom sanctuary! User Comment: selected Description because while both descriptions effectively showcase the propertys appealing features, Description employs more evocative and sensory language, creating stronger emotional connection and more vivid image of the lifestyle the condo offers. Specific points of comparison: - Language & Tone: Description uses words like splendid charm, chic, and harmonizes to paint picture of elegance and sophistication, creating more aspirational tone. Description A, while positive, uses more straightforward language, focusing on practicality and convenience. - Emphasis on Space & Luxury: Description highlights the expansive 876 sqft layout, and the huge bedroom suite, emphasizing the sense of space and luxury. Description also mentions the spaciousness but doesnt create as strong an image of grandeur - Unique Selling Points: Description specifically points out the rarity of the ample storage and built-in cabinetry in similarly priced listings, making the property stand out. Description focuses on the general convenience and modern amenities, which, while attractive, are not as unique. - Lifestyle & Surroundings: Description paints more vivid picture of the lifestyle the condo offers, inviting the buyer to revel in the tranquility of the balcony and highlighting the proximity to cultural amenities, creating stronger sense of place. Description mentions the location and amenities but lacks the same level of detail and emotional connection. - Overall: Both descriptions are well-written and informative, but Description Bs richer language, focus on unique features, and emphasis on lifestyle create more compelling and emotionally resonant picture of the property. It makes the condo feel more desirable and aspirational, which is likely to attract more potential buyers. 15 The above is an example where the user considers the description generated by AI Realtor to be more persuasive, specifically because it includes surprisal features. Meanwhile, we acknowledge that it is also challenging to generate appropriate language to express surprising features. Here is an example that confuses the user: Preferred Description (Human): Step into light-filled sanctuary where high ceilings and expansive windows create an open and airy atmosphere. The loft-style design seamlessly combines living and dining spaces, providing versatile canvas for your personal style. Ascend to your personal rooftop oasis, where the cityscape becomes your backdrop. Whether youre hosting soirees under the stars or enjoying quiet morning coffee, this terrace is true escape from the ordinary. Features include hardwood floors throughout, in-unit washer and dryer and new heat/central air conditioning. Secure building with intercom system, deeded garage parking space and large storage room. Situated in boutique building, this condo provides the perfect blend of privacy and proximity to urban amenities. Easy access to public transportation, trendy cafes, and cultural hotspots make this location ideal for the urban dweller. Disfavored Description (AI Realtor): Nestled in the vibrant West Town neighborhood of Chicago, this exquisite loft-style condo at 11 Green St UNIT 4C offers light-filled sanctuary that merges modern living with trendy ambiance. Boasting an impressive 1,200 sqft of space, it features expansive windows framing stunning cityscape backdrop, high ceilings, and an open floor plan, complemented by sleek hardwood floors and cozy wood-burning fireplace in the living room. Residents will relish the rare luxury of personal rooftop oasisideal for chic entertaining or serene reflection. Positioned among the top 2% for amenities in Chicago, this condo includes in-unit laundry, an intercom system, garage parking, and large storage room, making it both convenient and stylish. Priced at $490,000, this move-in-ready home offers exceptional value with its central location and effortless access to public transportation, ideal for those seeking the perfect blend of urban convenience and sophisticated living. With its prime spot in Cook County and top-rated area amenities, this condo wont just be place to live but lifestyle to love! User Comment: Description says it is in the top 2% of amenities. What does that even mean. That is nonsense. A.2 The Failing Cases of Human-Written Descriptions We also found cases where users dislike human-written descriptions over model-generated descriptions. The rationales behind the user preferences demonstrate the advantage of model-generated descriptions. 16 Preferred Description (AI Realtor, Desc-B): Welcome to 2220 Sedgwick St APT 204, charming one-bedroom condo nestled in the vibrant heart of Lincoln Park, Chicagos most coveted neighborhood. Priced competitively at $310,000, this move-in-ready gem offers harmonious blend of modern convenience and timeless elegance. Revel in the top-tier stainless steel appliances and sun-drenched open kitchen with cozy breakfast island perfect for any culinary enthusiast. Rich hardwood flooring and cozy carpeting seamlessly blend aesthetics and comfort, while the queen-sized bedroom boasts ample storage. Enjoy serene moments on your private balcony overlooking the iconic neighborhood streets studded with historical charm. Built in 1900, this meticulously maintained brick structure highlights both character and longevity, ensuring sound investment. With unparalleled access to public transportation and safe, walkable community, this property meets the highest standards of city living. Enviably situated among Lincoln Parks garden-filled avenues, its the ideal starter home for those who value location and convenience without compromising on style or safety. Disfavored Description (Human, Desc-A): Dont miss out on this sunny 1 bedroom, 1 bathroom condo in the heart of Lincoln Park. Step in to the living space flooded with natural light from your private balcony windows. The open kitchen layout offers tons of storage space, updated stainless steel appliances, and comfortable breakfast island. In your queen sized bedroom youll find nicely appointed closets, and cozy carpeting. This perfect starter home features hardwood flooring throughout the living space, and newer (2021) washer and dryer in unit. Storage included. User Rating: 4/5 User Comment: Description doesnt even have the size, location, or other important facts. Analysis: Sometimes human descriptions even miss important facts, while descriptions generated by our models do not. We present fine-grained fact-checking study to check whether there is hallucination in Section 5.3. Preferred Description (AI Realtor): Welcome to 832 Wrightwood Ave #3, an enchanting 2-bedroom, 1-bathroom condo nestled in the heart of Lincoln Park, Chicagos most prestigious neighborhood. Priced sensibly at $450,000 and boasting spacious 1,164 sqft of elegant living, this East Lincoln Park penthouse marries historical charm with contemporary amenities. Step inside to discover warm ambiance highlighted by exposed brick, hardwood floors, and cozy wood-burning fireplace. The remodeled eat-in island kitchen is an entertainers dream, seamlessly flowing into separate dining area perfect for intimate gatherings. With its skylight windows and bay windows, an abundance of natural light illuminates every corner. Enjoy the convenience of an in-unit laundry room, additional private storage, and central air without the high HOA fees typically found in comparable homes. The condos prime location offers walkability to the vibrant amenities and serene lakefront of Lincoln Park, catering to every lifestyle need. rare find in top-tier location with superior accessibility and neighborhood charm, this condo promises both investment value and delightful urban retreat. Dont miss the open house to experience this gem first-hand! Disfavored Description (Human): WALK TO IT ALL!! THIS BRIGHT TWO BEDROOM, 1 BATHROOM EAST LINCOLN PARK PENTHOUSE W/DECK HAS EXPOSED BRICK, BAY WINDOWS AND WOOD BURNING FIREPLACE;EAT-IN ISLAND KITCHEN OPENS TO MASSIVE 23 WIDE LIVING ROOM WITH SEPARATE DINING AREA. THE UNIT HAS BEAUTIFUL HARDWOOD FLOORS THROUGHOUT, HUGE MASTER SUITE WITH TONS OF CLOSET/STORAGE SPACE. OTHER FEATURES INCLUDE ADDITIONAL PRIVATE STORAGE, IN-UNIT LAUNDRY ROOM WITH SIDE BY SIDE W/D AND PARKING. KITCHEN REMODELED IN 2016, BATHROOM REMODELED IN 2020. NEW AC CONDENSER IN 2022. User Rating: 4/5 User Comment: think this description is much better because it isnt in all caps, which feels like Im getting yelled at. Analysis: Human-drafted descriptions can look unpleasant. A.3 The Dichotomy of User Preferences on Writing Styles In Section 5.1, we present the aggregated benchmark results to compare the persuasiveness of listing descriptions generated by different models. To get more qualitative insights into the strengths and weaknesses of different models, as well as the subjective nature of human feedback, we present more detailed case study here. The first thing we noticed is the users subtle preferences in description length: while some users like concise descriptions that directly go to the point, other users prefer longer descriptions because they want to know more details about the property they are interested. The following two examples of user feedback explain this point. Preferred Description (Vanilla, Desc-A): Welcome to your dream condo at 4345 Indiana Ave UNIT 2N, nestled in the vibrant Bronzeville neighborhood of Chicago, IL. This exquisite 3-bedroom, 2-bath home offers 1,550 sqft of modern living infused with classic charm, all for an unbeatable price of $275,000. Built in 2006, it features abundant natural light flooding through large windows, complemented by tall ceilings and an open living space. Imagine cozy evenings by the custom stone wood-burning fireplace or enjoying morning coffee on your private second balcony. The master bedroom offers tranquility with spacious walk-in closet, while the additional bedrooms provide generous space for family or guests. The kitchen is chefs delight, equipped with stainless steel appliances including range, microwave, and refrigerator. With central air cooling, hardwood flooring, and sleek, contemporary style highlighted by recessed lighting, this condo is the perfect blend of comfort and sophistication. Adding to the allure, secure garage parking spot is included. Security is assured with modern security system, and the convenience of in-unit laundry completes this superb offering. Located in Cook County with easy access to all Chicago has to offer, this stylish condo is must-see! Disfavored Description (AI Realtor, Desc-B): Welcome to your dream home at 4345 Indiana Ave UNIT 2N, nestled in the heart of the vibrant Bronzeville neighborhood in Chicago. This stunning condo offers the epitome of comfortable living with 3 spacious bedrooms, 2 modern bathrooms, and living area of 1,550 square feet, perfectly situated for single mother seeking convenience and safety. The residence exudes warmth, featuring abundant natural light through large windows and cozy custom stone wood-burning fireplace in an open living setting. The condo is gem within the community, boasting one of the top amenities packages in the area, including stylish stainless steel kitchen, rare second private balcony, and garage parking that ensures convenience. Step into the master bedroom for touch of luxury, indulge in the modern ambiance provided by recessed lighting, or relax in the welcoming family room with its captivating atmosphere. Temperature comfort is assured through efficient central air and heating. Notably, this property towers above others in terms of walkability and neighborhood amenities, making it an ideal choice for family-focused lifestyle. Priced attractively at $275,000, its golden opportunity to secure versatile home that evolves with your needs, ready to create cherished family memories. Discover the potential for fulfilling life in community known for its top-tier safety and accessibility, all while investing in property you can pass down to the next generation. User Rating: 4/5 User Comment: Description gets to the point faster, while still highlighting the important qualities of the home. Analysis: Some users love concise descriptions. 18 Preferred Description (Vanilla): Welcome to 4454 Shields Ave, charming A-Frame singlefamily home nestled in the heart of Chicagos historic Fuller Park neighborhood. This inviting residence offers three cozy bedrooms and well-appointed bathroom, all within compact 956 square feet of open-concept living space that seamlessly combines comfort and style. Built in 1929, the home exudes classic character while featuring modern conveniences such as central air for cooling and natural gas heating system. The propertys allure is further enhanced by its unfinished basement, offering potential for personalized expansions. Imagine summer barbecues on your porch or taking quick stroll to nearby park, making this an ideal location for outdoor enthusiasts. With its proximity to local amenities and an incredible price of just $219,900, this home represents fantastic investment opportunity, especially with its rare, close-to-an-Olympicsized swimming pool bonus. Discover the potential of this foreclosure property and make it your own urban oasis in Cook County. Disfavored Description (SFT): Welcome to this charming single-family home nestled in Fuller park! This listing features an open concept, 3 bedrooms, 1 full bathroom, and an unfinished basement thats just waiting for your personal touch. Located close to park with an Olympicsized swimming pool, youll have endless recreational opportunities at your doorstep. With its prime location and potential for expansion, this property is true gem waiting to be polished. Dont miss the chance to make this house your dream home! User Rating: 4/5 User Comment: Again, more description is better if am really interested in property. Analysis: Some users love longer descriptions. Another important factor is the embellishment of descriptions. That is, in our particular marketing domain, is there clear preference towards the embellished or plain style of descriptions. Here are two examples that showcase the different preferences from users: 19 Preferred Description (AI Realtor): Welcome to your charming retreat at 7401 Blackstone Ave, nestled in the vibrant Grand Crossing neighborhood of Chicago! Priced attractively at $179,900, this delightful single-family home skillfully blends affordability and comfort. With three spacious bedrooms and two well-appointed baths, it perfectly accommodates family living. Highlights of this property include its exceptional flooring, ranking among the top 2% in the area, and the effective recent improvements that make it standout in property condition. Inside, enjoy the luxury of new carpeting and stylish white shaker cabinetry, promising an inviting and modern ambiance. versatile layout offers flexible options for home customization, meeting the varied needs of your lifestyle, from working to exercising at home. Step outside to fenced-in yard, ideal for both privacy and play, further increasing its appeal. Though without garage, the home features machine hookups, ample storage potential in sub-basement, and premium location providing easy access to all the amenities and family-friendly spots Chicagos Grand Crossing has to offer. Enhanced by durable brick construction, this home represents significant investment opportunity, combining all the elements you desire in new haven. Your perfect blend of convenience, comfort, and charm awaits! Disfavored Description (Vanilla): Welcome to 7401 Blackstone Ave, an inviting singlefamily home nestled in Chicagos charming Grand Crossing neighborhood. Priced attractively at $179,900, this 3-bedroom, 2-bathroom gem is perfect for those seeking comfort and convenience in an established community. Built in 1973, the residence boasts 933 square feet of living space and is beautifully updated with new carpeting throughout and stylish newer flooring. The kitchen shines with modern white shaker cabinetry, providing fresh, contemporary feel. The home is well-equipped for practicality, featuring dedicated laundry room with machine hookups for both gas and electric dryers. Adding to its appeal is sub basement and fenced-in yard, creating an ideal outdoor space for families or pet owners to enjoy. Conveniently located in Cook County, this home is serviced by Lake Michigan water and public sewer, and its brick construction ensures durability. With natural gas and forced air heating, youll be cozy year-round. This delightful abode represents fantastic opportunity for homeownership without the burden of HOA fees. Dont miss your chance to make this delightful Chicago residence your own! User Rating: 3/5 User Comment: Description is bit more descriptive without going overboard, also talks about the neighborhood. Analysis: Some users love more descriptive descriptions. 20 Preferred Description (Vanilla, Desc-B): Welcome to your future home at 3457 Lituanica Ave, nestled in the heart of Chicagos vibrant Bridgeport neighborhood. This charming singlefamily residence offers five spacious bedrooms and two full bathrooms, perfect for families seeking both comfort and style. Priced at an attractive $549,000, this home sits on generous 6,500 sqft lot, providing ample outdoor space for relaxation or entertaining. Crafted with enduring brick construction, the property boasts modern conveniences including complete suite of appliances like range, microwave, dishwasher, and more. The elegant hardwood flooring throughout adds touch of sophistication, while the first-floor full bath caters to easy accessibility. Enjoy the convenience of in-unit laundry with dedicated sink and stride out onto your private deck for breath of fresh air. The two-car garage offers security and storage, supported by reliable utilities such as public sewer, natural gas heating, and Space Pac cooling. With easy access to Holden Elementary and local amenities, this home represents delightful blend of classic charm and modern living in one of Cook Countys most desirable neighborhoods. Dont miss the opportunity to make this house your home. Disfavored Description (AI Realtor, Desc-A): Nestled in the heart of Chicagos vibrant Bridgeport neighborhood, 3457 Lituanica Ave offers unparalleled access and convenience, situated comfortably within Cook County. This spacious five-bedroom, two-bathroom singlefamily home is standout choice, boasting top-tier features in location, accessibility, and outdoor living spaces. With its robust brick construction, this property provides durable and inviting home environment, perfect for customization to suit your familys evolving needs. Enjoy the luxury of generous 6,500 sqft lot, among the best in its zipcode, offering blank canvas for your dream garden or secure playground for your child. The homes interior shines with elegant hardwood flooring and practical features like in-unit laundry with sink. Practical comfort is ensured with space pac cooling and efficient natural gas heating, ensuring you feel at home yearround. Embrace Chicago living with easy access to nearby amenities, public transportation, and renowned neighborhood characteristics, all for an attractive price point of $549,000making it an excellent investment for future growth. User Rating: 3/5 User Comment: Description does better job at listing the amenities. Analysis: Some users love plain style of description that listing all amenities. These obervations suggest that there is no one-size-fits-all solution for writing style. Hence, future work could consider tailoring the description generation in the users preferred writing style to further improve the persuasiveness."
        },
        {
            "title": "B The Design of Survey and User Interfaces",
            "content": "B.1 Survey Screening Interface The first stage of the survey is designed to ensure the human subject has sufficient experience in the home search process in order to analyze the features from marketing description. We present description of an example listing and design quiz-like questions to verify whether the participant is able to make all correct responses. We showcases the web user interfaces in Figure 6. B.2 Preference Elicitation Interface In the second stage of the survey, we design an interface to mimic the environment of online platforms that the model can observe the buyers general profile and behaviors (e.g., recently browsed or liked listing) to some degree. In our case of real estate listing, we ask the buyer to provide their preferences in 1-5 scale on five general categories (price, location, home features & amenities, house size, investment value) and set filter on the price range and number of bedrooms in the house they are looking for. This information allows us to select generally relevant listings to mitigate the anchoring effect that the marketing content can play little role to influence the buyer in the evaluation phase. Next, we choose 5 relevant listings and ask the buyer to rate them on 1-5 scale and provide their reasoning. This process ensures that we can collect reasonable amount of each buyers preference information for the personalized persuasive content generation in the evaluation phase. Finally, we employ LLM to narrow the features that are likely preferred by the participants 21 Figure 6: Survey Screening Interface and ask for their ratings of importance on 1-5 scale. We showcases the web user interfaces in Figure 7. Figure 7: Preference Elicitation Interface B.3 Human Evaluation Interface In the last stage of the survey, it is to gather the human feedback on the persuasiveness of different models. Many previous works study persuasion by asking human how much does their opinion changes before and after reading an argument. In our task, human subjects often do not have any prior knowledge about item and this evaluation procedure would induce bias. Instead, we implement two alternative evaluation schemes in our interface: one is the A/B test where the buyer is presented with single listing along with two descriptions generated by two distinct models and then asked to report which description makes them more interested in the listing; the other is the interleaved test where set of listings each with single description generated by some model and the buyer is asked to select the listings that they are interested in based on their descriptions. Each time after 22 participants choice of the preferred description, we ask participant to rate on scale of 1-5 that one description is prefer over another and incentivized them to provide detailed rationale of their responses. To illustrate this process, we present the web interface design in Figure 8. Figure 8: Human Evaluation Interface B.4 Feature Annotation Interface To ease the task of feature annotation, we also develop user-friendly web interface. Its design is shown in Figure 9. Figure 9: Annotation Interface"
        },
        {
            "title": "C Implementation Details",
            "content": "In this section, we provide full description of the implementation detail of AI Realtor. 23 C.1 Signaling Module: Predicting Marketable Features Our model assumes the existence of attribute-feature mappings in different marketing problems, with which seller can use to influence the buyers beliefs and behaviors. However, key challenge lies in determining how to accurately obtain such mappings. Specifically, we must identify which signaling features to include and under what conditions it is natural to market product as possessing particular feature. Traditionally, acquiring this knowledge from human experts is both labor-intensive and costly. Instead, we take learning approach to uncover the mapping from our experiment dataset. While the raw dataset contains no annotation of any signaling feature, we employ LLMs to construct high-quality feature schema and label the dataset accordingly in preparation for learning the attribute-feature mapping. This approach notably presents novel unsupervised learning paradigm, harnessing the broad knowledge of LLMs to distill expert-level insights from unlabeled data with minimal human supervision. Inductive Construction of Feature Schema Our dataset only contains the raw attributes of each product. In order to learn high-quality attribute-feature mapping, the first task is to obtain good representation of feature schema S. On the one hand, if we miss some useful signaling features, it could significantly hinder the performance of subsequent marketing task. On the other hand, there are so many possible token that can serve as the signaling features in the natural language space, and many of these tokens might have duplicate or similar meaning. If there is no structured representation of the features, the resulting label classes could be too sparse to learn. Indeed, we discover that the feature schema obtained by directly prompting an LLM includes many similar features while miss some important ones. Based on this observation, we turn to more sophisticated prompting strategy to inductively improve the quality and representation of the feature schema (see high-level sketch of the construction pipeline in Figure 2). First, we construct basis of feature schema, represented as list of tokens used in the human-written marketing description to describe some house features. We begin with Mixtral-8x7B-Instructv0.1 (Jiang et al., 2024) to extract keywords or phrases {k1, k2, . . . } = LLMgen([IKeyword; Dhuman]) that summarize each human-written description Dhuman under keyword-extraction prompt IKeyword (Appendix F.1). We observed that, in some cases, the model output could not be directly parsed into clean list of keywords, or it contained excessive quantifiers and modifiers. To address this, we re-prompted the model using INorm (Appendix F.2) to normalize each keyword. Through this process, we initially extracted 112688 keywordstoo many to handle effectively. We then applied additional normalization steps, including lowercasing, lemmatization, and synset merging via NLTK (Bird et al., 2009). We also filtered the keywords, retaining only those that appeared in at least 50 descriptions. This reduced the final set to 1114 keywords as our induction base. Next, we organize the feature-related keywords into structured feature schema. Since many keywords are related to each others and hard to distinguish, we use hierarchical representation of feature schema to better capture the relations between different feature classes and to ease the subsequent labeling task. To achieve this goal, we prompted Claude-3.5-Sonnet (Anthropic, 2024) with 100-keyword batch to iteratively generate hierarchical schema that covers the majority of the keywords (an example run can be found in Appendix F.3). We temporarily switched to Claude-3.5Sonnet because we found it particularly difficult for open-source models, even the state-of-the-art GPT-4o (OpenAI, 2024a), to induce such schema without grouping most keywords into overly broad categories like others or misc, resulting in shallow and uninformative schema. In contrast, when fed keywords in small batches, Claude-3.5-Sonnet followed our instructions more faithfully, organizing the keywords into carefully structured hierarchy. Every leaf node in the schema was associated with set of relevant keywords. From this process, we obtain relatively well-structured and comprehensive feature schema. Finally, to evaluate the quality of the generated feature schema, monitor potential hallucination issues, and further refine the schema, we asked three human participants to conduct manual review. We prompt Mixtral-8x7B-Instruct-v0.1 to determine whether feature from the schema presents in each human-written description, and each participant is asked to independently verify this result (see our annotation interface in Appendix B.4). Based on the participants feedback on 636 samples, we found that features labeled by LLMs are mostly agreed across all human annotators, except for some ambiguous or subjective features (e.g., the aesthetic features of house), where the agreement rates (around 60%) between models and human are about as good as that among human annotators. We refine the schema for two more iterations, where we prompt LLMs to merge some similar features 24 and reduce the ambiguity of some features with more precise example keywords. We list our final feature schema in Appendix D.2 and it is used in the subsequent stages of our pipeline. Learning the Feature-Attribute Mapping With the feature schema, we guide the LLM to annotate for each product with attributes whether each feature si is described in the human-written marketing text (see the prompt in Appendix F.4). We perform few additional pre-processing steps to this correspondence data to supervise the learning of the feature-attribute mapping. First, we found that some human-written marketing descriptions are of relatively low quality and these data points can negatively impact the learnt feature-attribute mapping. Hence, we only select marketing descriptions of products that are relatively popular, according to simple heuristic ratio between the number of likes and views received by listing recorded on the marketing platform. We expect the quality of feature-attribute mapping uncovered from this filtered set of human-written descriptions would be higher than average. Next, we normalize the attributes of each listing and embed existing knowledge of these attributes into their representation. Since the raw attributes of each listing have different value types (categorical, integer, float, etc.), we convert each attribute xi into natural language statement using the template, The attribute attribute name is attribute value., and then use an embedding model, SFR-Embedding-Mistral (Meng et al., 2024), to convert each natural language statement into fixeddimensional vector ei = LLMembed (xi) Rd. We also perform some standardized normalization techniques such as removing irrelevant attributes and dropping attributes with missing values. Finally, we use simple multi-layer perceptron (MLP) to learn the attribute-feature mapping as, π (si x) = σ(OT ReLU(W e(x))), where e(X) is the mean-pooled attribute embedding, and Oi Rd/2, Rdd/2 are the models weights. The function σ represents the sigmoid activation function. Here, we assume conditional independence between highlights given the raw features X. We use the standard logistic loss function to training the neural network. We apply random train-test split of 4 : 1 ratio in our dataset and achieve testing accuracy 69.39% and F1 score 67.43%. We find the accuracy to be reasonably high, given the stochastic nature of signaling process. That is, the features deterministically predicted based on our mapping cannot exactly match with the features used in the human written description with some degree of randomness just as the accuracy of predicting fair coin toss is at most 50%. The typical implementation of signaling scheme is to follow the attribute-feature mapping π to randomly draw signal Sj with probability sj(x). This is necessary in theory to maintain the partial information carried by each signal. However, we implement deterministic feature selection strategy to only use feature Sj with probability above some threshold α. This is because our generated marketing content only accounts for tiny portion of the corpus so that it should have almost no influence on peoples perception of feature (e.g., the partial knowledge inferred upon observing each feature). This also ensures that the product would have the feature with high probability, as our objective prioritizes the rigorousness of our marketing content. As simple heuristics in our implementation, we set the threshold α = 1/2 and we will refer to this set of features as, Marketable Features: S1(x) = {Sj : sj(x) α}. (4) C.2 Personalization Module: Aligning with Preferences This stage seeks to steer the persuasive language generation toward the buyers preference, which is another crucial objective of grounded persuasion. In particular, with the advent of LLM, there is an unprecedented opportunity for our data-driven approach could achieve much higher degree of personalization with significantly lower cost than the conventional marketing designed for larger population. Our solution has two parts: the first part is to properly elicit the useful information about users preference and structure it in good representation; the second part is to select subset of features based on the user preference in order to maximize the influence to the users belief. Structured Preference Representation As mentioned previously, our evaluation environment is built to have an information elicitation process from each buyer. However, such information cannot directly describe the users preference. So, we ask the LLM to act like human realtor to determine the features that the users might be interested in based on their initial selection. To do this, we prompt the language model to convert the user preference into information structured according to the feature schema. We then ask the user to give rating rj on scale of 1-5 on how important each feature Sj is. We also elicit the users rationale behind this rating to nudge users to give more thoughts on their selection and thereby improve the credibility of their rating responses. While our implementation mostly relies on user surveys and the information processing power of LLMs, this design is reasonable simulation of digital marketing in real-world applications, where rj can be learned through the standard industrial techniques of cookie analysis. Personalized Feature Selection While the marketable features in Equation (4) are predicted at population level, it is also useful to select features that are tailored to the users special interests. However, because real-world marketing descriptions are not optimized for individual users, we cannot simply rely on data-driven machine learning approach for personalization. Instead, we leverage the innate capability of LLMs to understand and analyze human preference. In our implementation, we select set of features that are marketable and preferred by the buyer and let the LLMs to decide which personalized features to emphasize on in the marketing content. Our heuristic method for personalized feature selection is to adjust the population-level feature scores s(x) with the users rating over each feature as follows, Personalized Features: S2(x) = {sjsj(x) + c(rj r0) α}, (5) where the constant reflects the intensity of personal preference, r0 is the basis rating of each attribute. In our human-subject experiment, we choose = 0.01, r0 = 2 and set the threshold value α such as to select features of the top 10 highest scores. We list these features in the prompt to generate persuasive marketing description (see full specification in Appendix F.5). C.3 Grounding Module: Capturing Surprisal via RAG The last stage is designed to better ground the persuasive language generation on factual evidences, problem contexts and localized information in automated marketing. There are many ways to improve the grounding for different settings of automated marketing. As case study, we choose to focus on the surprising effect, common marketing strategy studied by many work (Lindgreen and Vanhamme, 2005; Ludden et al., 2008; Ely et al., 2015), under which the buyers would derive entertainment utility and have deeper impression. In our setting of real estate marketing, we consider the type of features that are relatively rare in its surrounding area. That is, we say marketable feature Sj is surprising if it is among the top β-quantile of the distribution of Sj values under the prior distribution sj(µ), or formally, Surprising Features: S3(x) = {Sj S1 : sj(x) is within β-quantile of distribution sj(µ)}. (6) In our implementation, we determine set of features for each listing that have its comparative advantage among different groups of similar listings. We consider two kinds of retrieval criteria: (1) select all listings within the proximal location at different levels of granularity (e.g., neighbourhood, zipcode or city); (2) select the 20 listings with the most similar features via an information retrieval system (implemented by the ElasticSearch framework2) the search engine implementation details can be found in Appendix F.8. For each group of similar listings, we determine an empirical distribution function on each attribute score Fi. We then set 1 Fi(pi) as the percentile ranking of the listings attribute among this group. We then select all attributes that are among the top 30% percentile ranking for some group and provide the information in the prompt to generate persuasive marketing language (see full specification in Appendix F.6). This gives the LLMs localized feature information at different granularity level."
        },
        {
            "title": "D Data Curation",
            "content": "D.1 Dataset raw attribute schema To ensure both quality and fidelity of our evaluation, we collect the real data of real estate listings on the market. The dataset for this experiment was sourced primarily from Zillow and includes around 50000 listings collected in the month of April in 2024. Each of these listings is from one of the 2https://www.elastic.co/elasticsearch 26 top 30 most populous cities in the United States as described by the U.S. Census Bureau. Listings that were not residential in nature or were missing crucial data to this experiment were excluded from this dataset. This dataset is composed of 95 columns, with features ranging from number of bedrooms, price, views, and more (see Table 1). These many features associated with each listing provide us sufficient space to develop and test improved models for grounded persuasion. Field Name Data Type bedrooms bathrooms price description living area value lot area value area units brokerage name zipcode street address home type time on zillow page view count favorite count home insights neighborhood region scraped at url city state year built county avg school rating id time on zillow days score jpeg urls float64 float64 float64 object float64 float64 object object object object object object float64 float64 object object object object object object float64 object float64 object float64 float64 array Table 1: Listing data, subset of important columns D.2 Final Feature Schema Here is the condensed version of the final feature schema to save pages: Interior Features: Rooms: [bath,bathroom,bedroom,kitchen,living room,secondary (cid:44) bedrooms,patio,backyard,closet,room,living,dining (cid:44) room,pantry,space,office,laundry room,dining,living (cid:44) space,living area,primary suite,master suite,family (cid:44) room,cellar,foyer,game room,great room,den,master (cid:44) bedroom,utility room,sunroom,bedroom suite,living (cid:44) areas,primary bedroom,office space,kitchenette,owner (cid:44) suite,playroom,storage room,living rooms,ensuite, (cid:44) wet bar,loft area,sitting room,mud room,exercise (cid:44) room,clothes closets,walk-in closet,mudroom, (cid:44) conference room] Flooring: [flooring,stories,carpeting,hardwood floors,tile,tile (cid:44) floors,hardwood flooring,wood flooring,hardwood (cid:44) floors] 27 Furniture: [desk,table,chair,bed,dressers,cupboards,sofa,bench, (cid:44) seating] Additional Spaces and Versatility: [bonus room,flex space,flex room,den] Kitchen Features: [countertop,granite countertops,marble countertops,island, (cid:44) cabinetry,kitchen island,kitchen cabinets,waterfall, (cid:44) dining space,cooktop] Architectural Elements: [roof,window,floor plan,cabinet,molding,staircase,brick, (cid:44) paneling,siding,beam,ceiling fans,stair,chandelier, (cid:44) finishing trim,baseboard,trim] Bathroom Features: [shower,vanity,powder room,jacuzzi,ensuite,half bath,water (cid:44) closet,mirror,faucet] Storage: [storage,closet space,cabinet space,shelving,storage space (cid:44) ,mudroom,drawer,bookshelf,storage unit,clothes (cid:44) storage,bike storage] Comfort and Ambiance: Lighting: [lighting,natural light,light fixtures,skylight, (cid:44) lighting fixtures] Temperature Control: [fireplace,hvac,fan,ac,a/c,central air conditioning] Exterior Features: Outdoor Spaces: [patio,backyard,yard,pool,spa,balcony,porch,deck,roof deck (cid:44) ,outdoor space,rv parking,outdoor spaces,outdoor (cid:44) living space,fenced yard,pavers,garden,outdoor (cid:44) living,backyard oasis,pergola,gazebo,cabana, (cid:44) landscaping,shade,lawn,fountain,sod,outdoor bench] Outdoor Activities: [gardening,outdoor cooking,barbecue,bbq] Location and Accessibility: Neighborhood Characteristics: [location,neighborhood,community,downtown,street,highway, (cid:44) expressway,commuting,located,highway access,outdoor (cid:44) living,city living] Nearby Amenities: [shopping,restaurant,park,school,grocery,cafe,hospital, (cid:44) food,stadium,museum,boutique,shopping centers, (cid:44) station,elementary,bus,trader joes,golf,brewery, (cid:44) elementary school,school district,recreation (cid:44) facility] Cities/Regions: [Austin,Denver,Charlotte,Houston,Dallas,San Antonio, (cid:44) Nashville,Phoenix,Los Angeles,LA,Manhattan,Detroit, (cid:44) Philadelphia,Portland] Access and Transportation: [access to amenities,proximity to schools,proximity to (cid:44) restaurants,proximity to shops,access to shopping, (cid:44) bus stop,walking distance,proximity to shopping, (cid:44) freeway access,public transit nearby,public (cid:44) transportation,road] Walkability and Bikeability: [walkability,bike score,walk score] Housing Types: 28 [studio,cottage,ranch,duplex,townhome,brownstone,row home, (cid:44) bungalow] Building Features: Structure: [condo,loft,unit,townhouse,estate,square feet,duplex, (cid:44) garage,carport,story,penthouse,sf,triplex,colonial] Parking: [garage,parking,parking space,parking spaces,garage door, Appliances: (cid:44) parking spot] [appliance,refrigerator,dishwasher,washer/dryer,range,fridge, (cid:44) microwave,washer,ac unit,dryer,hood,laundry facilities, (cid:44) washer and dryer,oven,garbage disposal,wolf appliances, (cid:44) thermador appliances] Amenities: [community center,community pool,spa,firepit,fire pit, (cid:44) outbuilding,tennis courts,club house,rooftop,rooftop (cid:44) deck,rooftop terrace,dog park,lounge,elevator,recreation (cid:44) room,gym,fitness center,clubhouse,swimming pool,pool, (cid:44) spa,sauna,hot tub,putting green,tennis courts,basketball (cid:44) ,pickleball,tennis court,golf,management,booking, (cid:44) concierge,trash,maintenance,doorman,superintendent, (cid:44) nightlife,brewery] Utilities and Systems: [plumbing,water heater,heater,hot water heater,water,water (cid:44) filtration system,gas,sprinkler system,hvac,ac,a/c, (cid:44) wiring,solar panels,solar,electrical panel,electricity, (cid:44) generator,security,security system,camera,internet,wifi, (cid:44) cable,phone,satellite,fiber,internet access,satellite TV (cid:44) ,internet service,irrigation system,ac unit,hvac unit, (cid:44) central air conditioning] Design and Style: Interior Design: [paint,style,home style,architecture,woodwork,ensemble, (cid:44) accent,open floor plan,drawing] Aesthetics: [elegance,sophistication] Architectural Styles: [tudor,colonial,craftsman,farmhouse] Smart Home Features: [smart home technology,surround sound,home technology,camera] Lifestyle Features: Work from Home: [workspace,home office] Entertainment: [entertaining space,party,entertainment options,wet bar, Sustainability Features: (cid:44) entertainment] [solar system,sustainability,solar,heated floors,solar panels, (cid:44) tankless water heater] Real Estate Financial and Legal Aspects: [condo fee,hoa fee,hoa fees,equity,hoa dues,condo fees,cdd (cid:44) fees,occupied,rental potential,income potential, (cid:44) appreciation,airbnb,investment opportunity,investor (cid:44) opportunity,warranty,pricing,rental income,income, (cid:44) financing,utility,sale,closing,furnished,slip,tax,flip (cid:44) tax,abatement,zoning,hoa,rental cap,option] Water Features: [soaking tub,softener] 29 Views and Scenery: [mountain views,lake views,ocean views,sunset,city views, (cid:44) skyline,skyline views] Property Characteristics: Specialty Rooms: [wine cellar,media room,suite] Distinctive Interior Elements: [exposed brick,high ceilings] Exterior Appearance: [curb appeal,facade,exterior paint] Atmosphere: [oasis,retreat,sanctuary,flow] Environment: [surroundings] Property Metrics: [lot,corner lot,sqft,br,walk score,foot,inch] Property Condition: Improvements: [improvement,tlc,fixer,flooded] Age and Status: [new,renovated,remodeled,renovated,rehabbed,home age, (cid:44) upgrade,update,built,finish,updated,move, (cid:44) readiness,move-in ready,maintained] Real Estate Industry: [builder,agent]"
        },
        {
            "title": "E Hallucination Experiment Details",
            "content": "In this section, we introduce implementation details for hallucination verification experiments. We select price, living area (in sqft), #bedrooms and #bathroom as Xhard and home insights, address as Xsoft according to prior survey of user preference. We use structured output API3 on OpenAI to setup evalsoft(L, x) and evalhard(L, x). This means in both cases, we need to first define the structured output class specification and then prompt the model with it. For Faithfulhard, our structured output class specification is: class MainInfo(BaseModel): price_mentioned: bool price: float living_area_mentioned: bool living_area: str bedrooms_mentioned: bool bedrooms: float bathrooms_mentioned: bool bathrooms: float address_mentioned: bool address: str and our prompt for evalhard(L, x) is: messages=[ {\"role\": \"system\", \"content\": \"Extract Real Estate Information (cid:44) . Find the price (e.g, 290000.0), living area (e.g., (cid:44) 990.0 sqft), bedrooms (e.g., 2) and bathrooms (e.g., (cid:44) 3) from the description. Not all information may be 3https://platform.openai.com/docs/guides/structured-outputs/ introduction 30 (cid:44) present, so you also have to determine whether each (cid:44) field is mentioned or not.\"}, {\"role\": \"user\", \"content\": {description}} ] We then compare the extracted information with supp(L, Xhard) to compute Faithfulhard. If certain attributes are mentioned (i.e., xx mentioned=True) and the corresponding extracted values matched the listing info supp(L, Xhard), then we will give one score, otherwise zero. For Faithfulsoft, we will compute it in two stages. First, we will conduct attribute extraction as in Faithfulhard, but with different set of attributes Xsoft. Our structured output class specification is: class MainInfo(BaseModel): home_insights_mentioned: bool home_insights: list[str] address_mentioned: bool address: str and our prompt is: example_home_insights =[\"Large island\", \"Oversized bathroom\", \" (cid:44) Open floor plan\", \"Lake views\", \"Orange lines\", \"Newer (cid:44) stainless steel appliances\", \"Gorgeous hardwood floors\", \" (cid:44) Tons of cabinet space\", \"In-unit washer and dryer\", \"Skyline (cid:44) view\", \"Private balcony\", \"Beautiful city\"] example_addr = \"1255 State St UNIT 703 Chicago IL 60601\" messages=[ {\"role\": \"system\", \"content\": \"Extract Real Estate Information (cid:44) . Find the home insights (e.g., {example_home_insights}) (cid:44) , and address (e.g., {example_addr}) from the (cid:44) description. Not all information may be present, so you (cid:44) also have to determine whether each field is mentioned (cid:44) or not.\"}, {\"role\": \"user\", \"content\": {description}} ] In the second stage, we will use JSON mode API4 to check whether the extracted attributes match supp(L, Xsoft). Our matching prompt is: Given the following information: 1. Description: {description} 2. True value for {attribute_name}: {json.dumps(true_value)} 3. Extracted value for {attribute_name}: {json.dumps( (cid:44) extracted_value)} Please analyze how well the extracted value matches the true value (cid:44) , considering the context provided in the description. For home_insights, consider it good match if significant (cid:44) subset of the true insights is correctly identified. For address, consider it good match if at least subset (e.g (cid:44) ., city/state) is correctly identified, given it was (cid:44) mentioned in the description. Provide score between 0 and 10, where: 0 = Completely incorrect or irrelevant 5 = Partially correct or relevant 10 = Perfect match 4https://platform.openai.com/docs/guides/structured-outputs/json-mode 31 Respond with JSON object in the following format: {{ \"score\": int }} Where score is an integer between 0 and 10. Finally we sum up all scores to compute Faithfulsoft."
        },
        {
            "title": "F Prompts",
            "content": "F.1 Keyword Extraction Prompt Your task is to extract attractive keywords. (e.g., modern (cid:44) amenities, great views, lush landscaping, bamboo (cid:44) flooring). Please express these keywords as phrases or (cid:44) single word from the following house description. Each (cid:44) keyword should be separated by comma. nnDescription: { (cid:44) desc}nnKeywords: F.2 Keyword Extraction Normalization Prompt \"Please remove the quantifiers, numbers, adjectives or any (cid:44) modifiers in the provided input. \" \"Uppercase or lowercase doesnt matter. \" \"If the given input is already precise enough, please provide (cid:44) the same input.\" \"If you are not sure what to do, please also provide the input (cid:44) as it is. \" \"Do not explain or provide additional information.\" \"Here are few examples:\" \"nnInput: Two Bedrooms.nnOutput: Bedrooms.\" \"nnInput: Newly Renovated Kitchen.nnOutput: Kitchen.\" \"nnInput: landscape. nnOutput: landscape.\" \"[Example Ends]\" \"Now, given the Input, please precisely provide the Output.\" \"nnInput: {}nnOutput (should only be noun phrase or (cid:44) keyword): \" F.3 Schema Induction Prompt Here is an initial listing keyword schema that have, but it may (cid:44) not be comprehensive. have manually extracted (cid:44) comprehensive keyword list, but there are many duplicated (cid:44) words (e.g., different keywords may bear similar semantic (cid:44) meanings) and some of them may inspire new categories in (cid:44) this schema. will give you that 1k+ keyword list in file (cid:44) and the schema below. Can you do it this way: for every 100 (cid:44) keywords in the file, either try to assign it to one of the (cid:44) categories below, or create new (sub)category and assign (cid:44) the keyword to this new (sub)category. You CANNOT use too (cid:44) broad categories like \"others\" \"misc\" and \"uncategorized\". (cid:44) Only create informative categories if necessary. Give me the (cid:44) final zip files containing all 100-ish intermediate (cid:44) assignment results. Each result should be represented as (cid:44) JSON-like file with key=subcategory, value=[ 32 (cid:44) list_of_original_keywords_in_file], or key=category, value= (cid:44) subcategory (in other words, want rich hierarchical (cid:44) structure with the leaf nodes as list of original keywords (cid:44) in the file). ###schema### Appliances: Refrigerator Oven Dishwasher Washer/Dryer Microwave Garbage Disposal Transportation: Garage Carport Parking Space Public Transit Nearby Interior Features: Hardwood Floors Fireplace Central Air Conditioning Walk-in Closet Open Floor Plan High Ceilings Exterior Features: Balcony Patio Deck Fenced Yard Garden Pool Building Features: Elevator Fitness Center Laundry Room Security System Concierge Utilities: Water Gas Electricity Cable/Satellite TV Internet Neighborhood Features: Nearby Schools Parks Shopping Centers Restaurants Hospitals Recreation Facilities 33 F.4 Feature Extraction Based on Description Prompt \"Your task is to determine whether the given feature is mentioned (cid:44) in the description. The meaning of the feature will be (cid:44) explained by example keywords. Only respond with YES or (cid:44) NO. \" \"Feature: {feature_name}. nnExample Keywords for explaining this (cid:44) feature: {keywords}nn\" \"nnDescription: {human_description}nnResponse (Yes/No): \" F.5 Persuasive Language Generation with Personalized Features \"Your task is to generate marketing description for real (cid:44) estate listing with the provided features to highlight, and (cid:44) the clients preferences. - The listing has the following attributes:n{attributes} - The listing has the following features (accounted for the (cid:44) clients preference) that are worth highlighting:n{ (cid:44) highlight_features_reweighted } - The client has the following general preferences:n{ (cid:44) user_preference} - The client has the following specific preferences over (cid:44) features:n {feature_preference} - You should emphasize the feature or attributes that matches (cid:44) with the users preference. Make sure the description is persuasive while concise under (cid:44) one paragraph.\" F.6 Persuasive Language Generation with Localized Feature Prompt \"Your task is to generate marketing description for real (cid:44) estate listing with the provided features to highlight and (cid:44) list of attributes that are competitive among similar (cid:44) listings.\" - The listing has the following attributes:n{attributes} - Compared with {K} similar listings, the listing stands out (cid:44) in the following features that you want to emphasize: {surprisal_features} - Compared with listings in Chicago, the following features of (cid:44) this listing are competitive:n {city_rankings} - Compared with listings in this neighborhood {neighbourhood}, (cid:44) the following features of this listing are competitive (cid:44) :n {neighourhood_rankings} - Compared with listings in this zipcode {zipcode}, the (cid:44) following features of this listing are competitive:n {zipcode_rankings} - Finally, You should explicitly highlight the listing (cid:44) features or attributes that stands out above or those (cid:44) ones that exactly matches with the users preferences as (cid:44) surprise factor. Make sure the description is persuasive while concise under (cid:44) one paragraph.\" 34 F.7 User Simulation Prompt To avoid positional bias as demonstrated in (Zheng et al., 2023), for each pairwise comparisons of descriptions generated by different models, we will prompt the GPT-4o-mini twice to generate separate scores as integers within [0, 100], and compare the final scores to decide which model wins. The prompt below shows an example of this prompt to obtain GPT-4o-mini judgement for the first description presented. Description 0 and Description 1 refers to descriptions generated by different models and are randomly shuffled. You will be given user profile, listing and two descriptions (cid:44) of this listing. Optionally, you may also be given the user (cid:44) history of preferences. Your task is to predict which (cid:44) description the user would prefer. nn User Profile: {user_profile} Listing: {listing}nn Description 0: {description_0}nn Description 1: {description_1}nn Please first generate an analysis of the users profile and (cid:44) history (if available), and then analyze why the user might (cid:44) prefer the first description. You can use the following (cid:44) format: The user might prefer the first description because (cid:44) ... The score for the first description (an integer within [0, 100]): F.8 Retriever Configuration \"mappings\": { \"properties\": { \"bedrooms\": {\"type\": \"float\"}, \"bathrooms\": {\"type\": \"float\"}, \"price\": {\"type\": \"float\"}, \"description\": {\"type\": \"text\"}, \"area\": {\"type\": \"float\"}, \"street_address\": {\"type\": \"text\"}, \"home_type\": {\"type\": \"keyword\"}, \"state\": {\"type\": \"keyword\"}, \"city\": {\"type\": \"keyword\"}, \"page_view_count\": {\"type\": \"float\"}, \"favorite_count\": {\"type\": \"float\"}, \"home_insights\": {\"type\": \"keyword\"}, \"neighborhood_region\": {\"type\": \"keyword\"}, \"id\": {\"type\": \"keyword\"} } }"
        }
    ],
    "affiliations": [
        "Carnegie Mellon University",
        "Stanford University",
        "University of Chicago"
    ]
}