{
    "paper_title": "PrismLayers: Open Data for High-Quality Multi-Layer Transparent Image Generative Models",
    "authors": [
        "Junwen Chen",
        "Heyang Jiang",
        "Yanbin Wang",
        "Keming Wu",
        "Ji Li",
        "Chao Zhang",
        "Keiji Yanai",
        "Dong Chen",
        "Yuhui Yuan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Generating high-quality, multi-layer transparent images from text prompts can unlock a new level of creative control, allowing users to edit each layer as effortlessly as editing text outputs from LLMs. However, the development of multi-layer generative models lags behind that of conventional text-to-image models due to the absence of a large, high-quality corpus of multi-layer transparent data. In this paper, we address this fundamental challenge by: (i) releasing the first open, ultra-high-fidelity PrismLayers (PrismLayersPro) dataset of 200K (20K) multilayer transparent images with accurate alpha mattes, (ii) introducing a trainingfree synthesis pipeline that generates such data on demand using off-the-shelf diffusion models, and (iii) delivering a strong, open-source multi-layer generation model, ART+, which matches the aesthetics of modern text-to-image generation models. The key technical contributions include: LayerFLUX, which excels at generating high-quality single transparent layers with accurate alpha mattes, and MultiLayerFLUX, which composes multiple LayerFLUX outputs into complete images, guided by human-annotated semantic layout. To ensure higher quality, we apply a rigorous filtering stage to remove artifacts and semantic mismatches, followed by human selection. Fine-tuning the state-of-the-art ART model on our synthetic PrismLayersPro yields ART+, which outperforms the original ART in 60% of head-to-head user study comparisons and even matches the visual quality of images generated by the FLUX.1-[dev] model. We anticipate that our work will establish a solid dataset foundation for the multi-layer transparent image generation task, enabling research and applications that require precise, editable, and visually compelling layered imagery."
        },
        {
            "title": "Start",
            "content": "PrismLayers: Open Data for High-Quality Multi-Layer Transparent Image Generative Models Junwen Chen Heyang Jiang Yanbin Wang Keming Wu Ji Li Chao Zhang Keĳi Yanai Dong Chen Yuhui Yuan Microsoft Research Asia https://prism-layers.github.io 5 2 0 2 8 2 ] . [ 1 3 2 5 2 2 . 5 0 5 2 : r Figure 1: Illustration of key statistics from PrismLayers (number of layers) and PrismLayersPro (different of styles), along with representative high-quality synthetic multi-layer transparent images from PrismLayersPro. Research intern at Microsoft. Corresponding author: yuhui.yuan@microsoft.com Preprint. Under review. Figure 2: User study results on the effectiveness of PrismLayersPro. Left: ART+ v.s. ART. Right: ART+ v.s. MultiLayerFLUX. With fine-tuning on PrismLayersPro, ART+ achieves the best performance."
        },
        {
            "title": "Abstract",
            "content": "Generating high-quality, multi-layer transparent images from text prompts can unlock new level of creative control, allowing users to edit each layer as effortlessly as editing text outputs from LLMs. However, the development of multi-layer generative models lags behind that of conventional text-to-image models due to the absence of large, high-quality corpus of multi-layer transparent data. In this paper, we address this fundamental challenge by: (i) releasing the first open, ultrahigh-fidelity PrismLayers (PrismLayersPro) dataset of 200K (20K) multilayer transparent images with accurate alpha mattes, (ii) introducing trainingfree synthesis pipeline that generates such data on demand using off-the-shelf diffusion models, and (iii) delivering strong, open-source multi-layer generation model, ART+, which matches the aesthetics of modern text-to-image generation models. The key technical contributions include: LayerFLUX, which excels at generating high-quality single transparent layers with accurate alpha mattes, and MultiLayerFLUX, which composes multiple LayerFLUX outputs into complete images, guided by human-annotated semantic layout. To ensure higher quality, we apply rigorous filtering stage to remove artifacts and semantic mismatches, followed by human selection. Fine-tuning the state-of-the-art ART model on our synthetic PrismLayersPro yields ART+, which outperforms the original ART in 60% of head-to-head user study comparisons and even matches the visual quality of images generated by the FLUX.1-[dev] model. We anticipate that our work will establish solid dataset foundation for the multi-layer transparent image generation task, enabling research and applications that require precise, editable, and visually compelling layered imagery. Dataset: https://huggingface.co/datasets/artplus/PrismLayersPro"
        },
        {
            "title": "Introduction",
            "content": "Despite remarkable advances in text-to-image diffusion models, users still face significant challenges in refining outputs to achieve satisfactory results. The difficulty lies in the fact that users cannot precisely articulate their visual requirements before seeing generated images, leading to laborious post-processing workflows. The fundamental issue here is that existing diffusion models are designed to produce single-layer images, lacking the transparent layers and precise alpha mattes required for flexible, layer-wise editing. Modern image editing workflows rely on multi-layered structures for the smooth adjustment of individual elements without causing disruption to the entire composition. In this paper, we argue for paradigm shiftfrom text-to-image generation to text-to-layered-image generation. Such an evolution would empower models to support flexible, layer-wise editing operations that align closely with professional design workflows. The fundamental challenge hindering progress in this area is the lack of high-quality multi-layer image datasets featuring both visually appealing transparency and accurate alpha mattes. Bridging this gap is essential to unlocking the full potential of layered image generation with diffusion models. Nevertheless, existing literature still relies on the conventional pipeline of fine-tuning generative models on limited, low-quality crawled multi-layer datasets. These datasets have two major drawbacks: (i) aesthetic quality: our empirical analysis shows that the aesthetic scores of crawled multi-layer images are significantly lower than those of RGB images generated by state-of-the-art diffusion models like FLUX.1-[dev]. As result, we empirically find that fine-tuning on less visually appealing data can degrade the overall aesthetics; (ii) dataset size: the scale of these crawled multi-layer datasets is much smaller than that of conventional RGB image datasets. Consequently, fine-tuning on such datasets becomes less effective as the foundational generative models become increasingly powerful. 2 This paper leverages off-the-shelf powerful diffusion models to generate high-quality multi-layer transparent images, thereby bypassing the need for fine-tuning on specific datasets. To achieve this goal, this paper makes three key contributions: (i) LayerFLUX: We propose training-free, singlelayer transparent image generation system that utilizes generate-then-matting scheme. Specifically, our approach leverages diffusion models to generate images with solid-colored backgrounds and uses state-of-the-art image matting model to extract high-quality alpha masks for salient objects. We have named this system LayerFLUX, as it builds upon the latest diffusion transformer model, FLUX.1-[dev]. (ii) MultiLayerFLUX: We introduce layout-then-layer scheme that composes multiple high-quality transparent layers generated by LayerFLUX according to given layout, which can be obtained either from reference image or generated using an LLM. This modular approach enables precise control over spatial composition while preserving the visual quality and alpha matte of each layer, resulting in our MultiLayerFLUX system. (iii) Transparent Image Preference Scoring Model: We develop dedicated preference scoring model to assess the visual aesthetics of the generated transparent images. Figure 1 shows the high-quality synthetic multi-layer transparent images generated using MultiLayerFLUX. To demonstrate the effectiveness of above designs, we first compare LayerFLUX against previous state-of-the-art transparent image generation methods such as LayerDiffuse [25]. Figure 14 shows the user-study results on comprehensive benchmark (Layer-Bench) that includes prompts describing natural object layers, sticker/text sticker layers, and creative object layers. Second, we leverage MultiLayerFLUX to construct large-scale high-quality multi-layer dataset (PrismLayers) comprising approximately 200K multi-layer transparent images and perform rigid filtering to construct smaller set of 20K samples with the best quality, forming PrismLayersPro. We validate the benefits of PrismLayersPro by fine-tuning the latest multi-layer generation model, ART [19], and present corresponding user-study results in Figure 2, comparing our models performance with that of the original ART. We find that ART+ is preferred in approximately 57% to 60% of cases across prompt alignment, global harmonization, and layer quality. We empirically find the composed multi-layer images generated with ART+ even match the quality of the holistic single-layer images generated with FLUX.1-[dev] to some extent. These results demonstrate the fundamental role of high-quality multi-layer transparent dataset in developing the next generation of multi-layer transparent image generation models. We anticipate that our open-source dataset will serve as solid foundation for future efforts in this direction. We also plan to increase the scale of PrismLayersPro from 20K to 200K-covering more diverse aspect ratios-in the coming months."
        },
        {
            "title": "2 Related work",
            "content": "Transparent image generation for interactive content can be divided into two paths: single-layer methods (LayerDiffuse [25], Text2Layer [26], LayeringDiff [11]) and multi-layer methods (LayerDiff [8], ART [19]). Unlike top-down schemes such as MULAN [21], our bottom-up pipeline generates high-fidelity transparent layers before composition, achieving superior aesthetics on PrismLayers. Meanwhile, graphic design generation has shifted toward business-driven layouts: COLE/OpenCOLE [10, 9] iteratively assemble elements via LLMs and diffusion, while Graphist [6] employs hierarchical layout planning. In this paper, we focus on building an open, high-quality, multi-layer transparent image dataset to facilitate future work on closing the gap between multi-layer generation and conventional single-layer text-to-image models, such as the latest FLUX series. We also discuss the connections and differences between our benchmark and previous multi-layer transparent image generation datasets in Table 1."
        },
        {
            "title": "3 PrismLayers: A High-Quality Multi-Layer Transparent Image Dataset",
            "content": "We introduce PrismLayers, synthetic dataset consisting of approximately 200,000 multi-layer transparent images. Each sample is accompanied by global image caption, layer-wise captions, corresponding layer-wise RGB images, and precise alpha mattes. All samples have undergone rigorous aesthetic evaluation and filtering based on our proposed Transparent Image Preference Score (TIPS) model. Furthermore, we curate high-quality subset of 20,000 images from PrismLayers, termed PrismLayersPro, representing the top aesthetic tier of the dataset. We will first present 3 Figure 3: Illustrating the key dataset statistics on PrismLayers and PrismLayersPro # Layers Dataset Multi-layer Dataset [25] LAION-L2I [26] MLCID [8] MLTD [19] MAGICK [5] MuLAn [21] Crello [24] PrismLayers PrismLayersPro # Samples 1 57 2 1 150 44 20 200 20 Alpha Quality good normal poor good good poor normal good good Table 1: Comparison with previous multi-layer transparent image datasets. Source Data commercial, generated LAION LAION Graphic design website Synthetic COCO, LAION Graphic design website Synthetic Synthetic Open Source 2 2 [2,3,4] 2 50 1 2 6 2 50 2 50 2 50 Aesthetic good normal poor normal good poor poor good excellent detailed statistical characteristics and the curation pipeline of the PrismLayers dataset. Subsequently, we will present our key technical contributions: LayerFLUX and MultiLayerFLUX. 3.1 PrismLayers Statistics Statistics on the number of layers. We analyze the distribution of transparent layer counts in PrismLayers. Each image contains an average of 7 layers (median: 6), with 85% of samples containing between 3 and 14 layers. This indicates that PrismLayers effectively captures wide range of visual complexity. Figure 3 (a) provides more detailed illustration of the transparent layer count distribution. Statistics on the aesthetics of layers. key contribution of this open-source dataset is the provision of aesthetically pleasing transparent layers, addressing the limited visual quality found in existing multilayer datasets. As shown in Figure 3 (f), quantitative evaluations using our Transparent Image Aesthetic Scoring (TIPS) model illustrate the aesthetic distributions of PrismLayers, MULAN [21], and MLTD [19]. Figure 4 visualizes qualitative comparisons between PrismLayers and PrismLayersPro. Our results show that PrismLayers consistently provides higher-quality layers, with the open-source subset PrismLayersPro achieving the best overall aesthetic quality. Statistics of visual text layers. High-quality visual text rendering is essential for multi-layer transparent image generation, as textual elements play central role in many business-centric visual designs [16, 17]. PrismLayers contains large number of accurately rendered text layers, each isolated in separate transparent channel. Figure 3 (c), (d), and (e) present statistics on the number of text layers per image, the number of characters per instance, and the area ratio of text layers. Statistics of different visual styles. In the middle of Figure 1, we illustrate the distribution of transparent layers across different styles in PrismLayersPro, which contains 21 distinct styles. The top five most frequent styles are toy, melting silver, line draw, ink, and doodle art Comparison with existing transparent datasets. Table 1 presents comparison with previously existing multi-layer transparent image datasets. We position PrismLayersPro as the first open, high-quality synthetic dataset that supports diverse range of layers, high-quality alpha mattes, and excellent aesthetic quality. We believe PrismLayersPro can serve as solid foundation for future efforts in building better multi-layer transparent image generation models. 3.2 PrismLayers Dataset Curation Process We illustrate the entire dataset curation pipeline in Figure 5. To ensure clarity, we mark all dataset states with blue-colored markers, including , , , , , and . For the different algorithm operations, we use black-colored markers, including 1 , 2 , 3 , 4 , 5 , and 6 . Further details are explained as follows: 4 Figure 4: Illustrating the aesthetic quality of the crawled data (columns 1 and 4), synthetic data (columns 2 and 5), and high-quality synthetic data generated with style prompt (columns 3 and 6). Multi-layer prompts and semantic layout from crawled data. 1 We begin by collecting an internal dataset of 800K multi-layer graphic designs sourced from various commercial websites. Each design instance consists of multiple transparent layers, including background elements, decorations, text, and icons. To enrich the semantic understanding of each instance, we employ an off-the-shelf LLMLlava 1.6 [15]to generate captions for both individual transparent layers and the fully composed images. This process yields annotations comprising 800K multi-layer prompts and their corresponding semantic layouts, effectively capturing both the visual composition and the intended design semantics. We also extract the original metadata specifying the layer ordering for each graphic. For the filtered PrismLayersPro set (after 4 ), we further enhance semantic richness by using GPT-4o to generate high-quality layer-wise captions. Synthetic multi-layer transparent images with MultiLayerFLUX. 2 With the constructed 800K multi-layer prompts and corresponding semantic layout information, we apply novel model, MultiLayerFLUX, to transform the layer-wise prompts into multiple transparent layers, each generated separately using single-layer transparent image generation engine such as LayerFLUX, as illustrated in Sec. 3.3. We then composite these transparent layers onto shared canvas, preserving the correct stacking order and ensuring seamless integration across layers. key challenge is that the transparent layers within multi-layer image often have varying resolutions and aspect ratios. We observe that simply applying LayerFLUX to generate each layer within fixed square canvas tends to produce objects with an unnatural square shape. To remedy this issue, we instead apply LayerFLUX to generate transparent layers on canvases that match their original aspect ratios and resolutions. Artifact multi-layer transparent image filter. 3 As MultiLayerFLUX generates each transparent layer separately and then combines them following the layer order, we observe severe artifacts in some synthetic multi-layer images. These artifacts include duplicate or similar layers positioned in conflicting spatial arrangements or exhibiting substantial and unreasonable overlap, as shown in Figure 7. To address this issue, we construct reliable artifact classifier to further filter out flawed multi-layer transparent images. We begin by manually annotating severe artifacts in subset of 8K synthetic multi-layer images with high aesthetic scores. Then, we train an artifact classifier by fine-tuning BLIP-2 [13] to predict confidence scores indicating whether composed multi-layer transparent image contains such artifactse.g., conflicting layer placements or unreasonable overlap. To ensure the quality of the final dataset, we apply the trained classifier to select subset of 200K synthetic multi-layer transparent images, forming PrismLayers. High-quality reference layout pool. The aforementioned Artifact Classifier performs image-level structural assessment. Next, we perform visual quality filtering using an aesthetic predictor [1]. We rank images with different numbers of layers based on their aesthetic scores, then select fixed proportion of the highest-scoring images from each group to form an 80K-image high-quality reference layout pool. Layer-wise quality filter, styled prompt rewrite, and human selection. 5 6 + 4 We take three steps to further improve the layer quality: 1) applying transparent image preference score (TIPS) to evaluate the quality of the transparent layers, 2) rewriting style prompt to enhance the diversity and visual appealing of these transparent layers, and 3) human selection to find the samples with the best quality. We train the TIPS model on collection of our PrismLayers, single-layer images generated by LayerDiffuse [25], and our reproduction of LayerDiffuse based on FLUX.1-[dev]. We define 20 distinct style keywords, and for each style, we randomly sample 2,000 layouts from the 80K reference layout pool. Each sampled layouts individual layers are pasted onto gray background and fed to GPT-4o, which rewrites the layer captions to include the target style directives. Next, 5 Figure 5: Dataset Curation Pipeline of PrismLayers and PrismLayersPro. We first extract semantic layouts from database of 800K crawled multi-layer graphic design images. Then, we apply MultiLayerFLUX to generate high-quality multi-layer transparent images. An Artifact Classifier is used to evaluate the quality of each composed image, discarding low-quality results to construct PrismLayers. We also apply the Transparent Image Preference Score (TIPS) model to assess the quality of individual transparent layers. By filtering for aesthetic quality and balancing the number of layers, we collect an 80K-image reference layout pool. From this pool, we sample 20K of the highest-quality layouts and regenerate them with style prompts, followed by manual selectionforming our released open-source, high-quality multi-layer dataset, PrismLayersPro. we employ MultiLayerFLUX to regenerate each transparent layer according to its new, style-aware caption. The resulting styled layers are manually reviewed to remove obvious failures with reference to the scores by our transparent image preference score (TIPS) predictor. Finally, we discard all low-scoring layers or artifact-prone images, producing our final 20K refined high-quality synthetic multi-layer dataset, PrismLayersPro. Discussion. natural question is whether the generated multi-layer images exhibit cross-layer coherence. We acknowledge that the synthetic multi-layer transparent images generated by MultiLayerFLUX cannot fully guarantee inter-layer consistency. This remains known limitation of our current scheme, which we mitigate through human selection. Nonetheless, we empirically observe that the recent ART model [19], when trained on our filtered high-quality dataset, produces multi-layer images with noticeably improved coherencehighlighting the value of high-quality supervision in addressing this challenge. 3.3 LayerFLUX and MultiLayerFLUX In this section, we present the mathematical formulation of the multi-layer transparent image generation task, followed by key insights and implementation details of our LayerFLUX and MultiLayerFLUX models. Formulation. The transparent image generation task aims to train generative model that transform the input global text prompt Tglobal and the optional regional text prompts into an output consisting {Ti that can form high-quality multi-layer image Iglobal, and of set of transparent layers {Ii each layer is with accurate alpha channels {Ii . This task degrades to single-layer transparent i=1 image generation task when = 1. Following the latest ART [19], we apply flow matching model to model the multi-layer transparent image generation task by performing the latent denoising on the concatenation of both the global visual tokens and the regional visual tokens. Figure 6: LayerFLUX and MultiLayerFLUX Framework. RGBA}N region}N alpha}N i=1 i=1 LayerFLUX. As shown in Figure 6, we build the LayerFLUX with two key designs, including the suffix prompt scheme and the additional salient object matting to predict the accurate alpha mattes. 6 Figure 7: Illustrating the artifact multi-layer transparent images that our classifier can identify and filter out. Figure 8: Attention maps between the suffix text token and visual tokens. We observe clearly higher attention response in the background area with accurate boundary patterns. Inspired by MAGICK [5], we design series of tailored suffix prompts to guide diffusion models in generating images with single-colored, uniform backgrounds. These controlled conditions ensure that the foreground elements are clearly delineated, thereby simplifying the isolation process. Our implementation involves simply appending the suffix prompt isolated on gray background to the original text prompt. We also compare the results of using alternative suffix prompts by replacing the word gray with other colors, such as green, blue, white, black, half green and half red, half red and half blue, and others. Figure 8 visualizes the attention maps between the suffix tokens and the visual tokens. We observe that appropriately chosen suffix prompts can guide the diffusion transformer to produce isolated background regions that are more amenable to segmentation. detailed analysis of different suffix prompt effects is provided in the supplementary material. To extract accurate alpha mattes, we explore and evaluate multiple state-of-the-art image matting techniques, including SAM2 [20], BiRefNet [27], and RMBG-2.0 [4], to achieve the separation of the foreground from the background. By leveraging these advanced matting algorithms, we aim for precise alpha matte extraction, ensuring that the edges of the isolated objects are smooth and accurately defined. This step is critical for producing high-quality, transparent images that can be seamlessly integrated into multi-layer compositions. We empirically find that RMBG-2.0 achieves the best matting quality, and we choose it as our default method. MultiLayerFLUX. We construct the MultiLayerFLUX framework by stacking the outputs from the above-mentioned LayerFLUX according to the given layer-wise prompts and semantic layout. Unlike the original FLUX.1-[dev], which directly predicts transparent layers within square canvas of size 1024 1024, we preserve the original aspect ratio of each transparent layer and use FLUX.1-[dev] to generate images at varying resolutions, fixing the longer side to 1024. Each generated transparent layer is then resized to fit the corresponding bounding boxes based on the semantic layout information, and the layers are composited according to the layer-order annotations, resulting in the final synthetic multi-layer transparent images. 3.4 Transparent Image Quality Assessment Existing image quality assessment models [12, 22, 23] are primarily trained to predict human preferences for conventional RGB images, and thus are not well suited for evaluating transparent images with alpha mattes. To address this gap, we propose dedicated quality scoring model tailored for transparent layer images. The core idea is to distill ensembled preference signalsaggregated from multiple RGB-oriented modelsinto model specialized for transparent image quality, thereby mitigating model-specific biases. Furthermore, given that our LayerFLUX framework reliably produces high-quality alpha mattes, we exclude explicit transparency-related factors when constructing the preference dataset. Transparent image preference dataset. We first collect transparent image preference (TIP) dataset of more than 100K win-lose pairs by gathering three types of data resources, including those generated with LayerFLUX and LayerDiffuse. We use multiple image quality scoring models to rate the quality of each transparent layer, including Aesthetic Predictor V2.5 [1], Image Reward [23], LAION Aesthetic Predictor [3], HPSV2 [22], and VQA Score [14]. Then, we compare each pair of transparent layers based on the weighted sum of the scores predicted by the aforementioned quality scoring models. Here, we assume that the alpha mask quality of most transparent layers generated with our LayerFLUX and LayerDiffuse methods is satisfactory. 7 Transparent image preference score. We train the transparent image preference scoring model by fine-tuning CLIP on the TIP dataset. For each pair of transparent images with preference labels, we choose loss function Lpref = (log 1 log pw), where pw is the probability of the win image being the preferred one, and we compute the pw as: pw = exp (τ fCLIP-V(Iw)fCLIP-T(T)) exp (τ fCLIP-V(Iw)fCLIP-T(T))+exp (τ fCLIP-V(Il)fCLIP-T(T)) , (1) where fCLIP-V() and fCLIP-T() represent the CLIP visual encoder and text encoder separately. Iw and Il represent the prefered and disprefered transparent image. During the evaluation, we compute the transparent image preference score as follows: = fCLIP-V(I) fCLIP-T(T), where we directly use the dot product between the normalized CLIP visual embedding and the CLIP text embedding as the transparent image preference score, abbreviated as TIPS for convenience. (2)"
        },
        {
            "title": "4 Experiment",
            "content": "4.1 Setting Implementation details. We conduct all the experiments with the latest FLUX.1[dev] [2] model. For the fine-tuning of ART [19] on our MultiLayerFLUX datasets, we use 20K training iterations, global batch size of 4, an image resolution of 512512, and learning rate of 1.0 with the Prodigy optimizer, followed by fine-tuning at larger resolution of 10241024 with 10K training iterations. Instead of assessing the models performance solely on crawled multi-layer graphic designs [19]most of which follow similar flat stylewe propose evaluating it on more diverse and creative set generated by the state-of-the-art diffusion model FLUX.1-[dev]. This benchmark is chosen to quantify the gap between generated multi-layer graphic designs and the holistic single-layer image designs produced by the latest text-to-image generation models. 4.2 ART+: Improving ART with PrismLayersPro User Study Evaluation. To assess the effectiveness of our dataset and fine-tuning strategy, we conduct user study comparing the fine-tuned ART model (denoted as ART+) with the original ART [19], PrismLayers, and PrismLayersPro. Unlike the original ART, which relies on private multi-layer dataset, we first train ART from FLUX.1-[dev] using the 200K-sample synthetic PrismLayers, and then fine-tune it on the 20K extremely high-quality subset, PrismLayersPro, following the quality-tuning paradigm [7]. The study involves 40 representative samples from FLUX-MultiLayer-Bench, with over 20 participants evaluating three key dimensions: (i) Layer Quality (visual aesthetics and alpha fidelity), (ii) Global Harmonization (inter-layer coherence), and (iii) Prompt Following (alignment with input prompts). As shown in Figure 2, ART+ outperforms the original ART with average win rates of 57.9% in layer quality and 59.3% in prompt following. It also surpasses MultiLayerFLUX in global harmonization (45.1% win rate), validating the impact of combining high-quality supervision with task-specific tuning. Quantitative Results. Table 2 presents the layer-wise TIPS scores and the FIDmerged scores, comparing the predicted merged images with ground-truth images obtained either from the design test set (DesignMulti-Layer-Bench) or directly from the FLUX image set generated with FLUX.1-[dev]. Our ART+ significantly outperforms ART on the FLUX-Multi-Layer-Bench, and we also provide additional qualitative comparison results below. One interesting observation is that ART achieves better FIDmerged scores on Design-Multi-Layer-Bench, as ART is trained on design images with style distribution similar to that of the test set, whereas our ART+ is trained with much more diverse styles. Qualitative MultiLayer Results. Figure 10 presents qualitative results comparing our MultiLayerFLUX with the fine-tuned ART+, while Figure 9 shows qualitative comparisons between ART and the fine-tuned ART+. We observe that ART+ achieves significantly better global harmonization than MultiLayerFLUX and better layer quality than ART, separately. These comparisons reveal that the fine-tuned ART+ achieves an excellent balance between layer quality and global harmonization. 8 Figure 9: Qualitative comparison results between ART (top row) and ART+ (bottom row). Figure 10: Qualitative comparison results between MultiLayerFLUX (top row) and ART+ (bottom row). Figure 11: Qualitative comparison results between FLUX.1-[dev] (1st row), MultiLayerFLUX (2nd row), ART (3rd row), and ART+ (4th row) across 7 cases (columns). The rightmost columns show composed multi-layer images. Comparison to FLUX. Figure 11 compares the merged multi-layer image generation results with the reference ideal images generated directly with FLUX.1-[dev]. We can see that our ART+ significantly outperforms ART and MultiLayerFLUX, achieving aesthetics very close to those of the original modern text-to-image generation models. More Experiments. We provide more experimental results of LayerFLUX and qualitative comparison results in the supplementary materials."
        },
        {
            "title": "5 Conclusion",
            "content": "This paper has tackled the critical gap in multi-layer transparent image generation by assembling and releasing two large-scale datasetsPrismLayers (200K samples) and its ultra-high-fidelity subset 9 Method ART [19] MultiLayerFLUX ART+ Design-Multi-Layer-Bench TIPS FIDmerged 16.84 18.34 19.90 21.29 18.91 26.53 FLUX-Multi-Layer-Bench FIDmerged 30.04 29.64 26.07 TIPS 16.64 20.65 19.42 Table 2: Comparison with state-of-the-art ART. PrismLayersPro (20K samples)each annotated with precise alpha mattes. To produce this data on demand, we devised training-free synthesis pipeline that harnesses off-the-shelf diffusion models, and we built two complementary methods: LayerFLUX and MultiLayerFLUX. After rigorous artifact filtering and human validation, we fine-tuned the ART model on PrismLayersPro to obtain ART+, which outperforms the original ART in 60% of head-to-head user studies and matches the visual quality of top text-to-image models. By establishing this open dataset, synthesis pipeline, and strong baseline, we lay solid foundation for future research and applications in precise, editable, and visually compelling multi-layer transparent image generation. Limitations & Future Work. We raise several important questions for future work. How can we generate high-quality multi-layer prompts and semantic layouts without relying on reference data from designers? We observe that even the latest LLMs, including OpenAI o3, still lag behind human-designed layouts and are therefore not yet suitable for multi-layer transparent image generation. How can we generate photorealistic multi-layer transparent images? While PrismLayersPro focuses on the domain of graphic design, photorealistic images involve more complex inter-layer relationships due to lighting and occlusion effects. We leave these fundamental challenges for future exploration. References [1] Aesthetic score v2.5. https://github.com/discus0434/aesthetic-predictor-v2-5. [2] Flux. https://github.com/black-forest-labs/flux/. [3] Laion aesthetic. https://github.com/LAION-AI/aesthetic-predictor. [4] Rmbg-2.0. https://huggingface.co/briaai/RMBG-2.0. [5] R. D. Burgert, B. L. Price, J. Kuen, Y. Li, and M. S. Ryoo. Magick: large-scale captioned dataset from matting generated images using chroma keying. In CVPR, pages 2259522604, 2024. [6] Y. Cheng, Z. Zhang, M. Yang, H. Nie, C. Li, X. Wu, and J. Shao. Graphic design with large multimodal model. arXiv preprint arXiv:2404.14368, 2024. [7] X. Dai, J. Hou, C.-Y. Ma, S. Tsai, J. Wang, R. Wang, P. Zhang, S. Vandenhende, X. Wang, A. Dubey, et al. Emu: Enhancing image generation models using photogenic needles in haystack. arXiv preprint arXiv:2309.15807, 2023. [8] R. Huang, K. Cai, J. Han, X. Liang, R. Pei, G. Lu, S. Xu, W. Zhang, and H. Xu. LayerDiff: Exploring text-guided multi-layered composable image synthesis via layer-collaborative diffusion model. In ECCV, 2024. [9] N. Inoue, K. Masui, W. Shimoda, and K. Yamaguchi. Opencole: Towards reproducible automatic graphic design generation. In CVPR, pages 81318135, 2024. [10] P. Jia, C. Li, Y. Yuan, Z. Liu, Y. Shen, B. Chen, X. Chen, Y. Zheng, D. Chen, J. Li, et al. Cole: hierarchical generation framework for multi-layered and editable graphic design. arXiv preprint arXiv:2311.16974, 2023. [11] K. Kang, G. Sim, G. Kim, D. Kim, S. Nam, and S. Cho. Layeringdiff: Layered image synthesis via generation, then disassembly with generative knowledge. arXiv preprint arXiv:2501.01197, 2025. [12] Y. Kirstain, A. Polyak, U. Singer, S. Matiana, J. Penna, and O. Levy. Pick-a-pic: An open dataset of user preferences for text-to-image generation. NeurIPS, 36, 2024. 10 [13] J. Li, D. Li, S. Savarese, and S. Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International conference on machine learning, pages 1973019742. PMLR, 2023. [14] Z. Lin, D. Pathak, B. Li, J. Li, X. Xia, G. Neubig, P. Zhang, and D. Ramanan. Evaluating text-to-visual generation with image-to-text generation. In ECCV, pages 366384. Springer, 2024. [15] H. Liu, C. Li, Y. Li, B. Li, Y. Zhang, S. Shen, and Y. J. Lee. Llava-next: Improved reasoning, ocr, and world knowledge, January 2024. [16] Z. Liu, W. Liang, Z. Liang, C. Luo, J. Li, G. Huang, and Y. Yuan. Glyph-byt5: customized text encoder for accurate visual text rendering. In European Conference on Computer Vision, pages 361377. Springer, 2024. [17] Z. Liu, W. Liang, Y. Zhao, B. Chen, L. Liang, L. Wang, J. Li, and Y. Yuan. Glyph-byt5-v2: strong aesthetic baseline for accurate multilingual visual text rendering. arXiv preprint arXiv:2406.10208, 2024. [18] D. Podell, Z. English, K. Lacey, A. Blattmann, T. Dockhorn, J. Müller, J. Penna, and R. Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. [19] Y. Pu, Y. Zhao, Z. Tang, R. Yin, H. Ye, Y. Yuan, D. Chen, J. Bao, S. Zhang, Y. Wang, L. Liang, L. Wang, J. Li, X. Li, Z. Lian, G. Huang, and B. Guo. Art: Anonymous region transformer for variable multi-layer transparent image generation. In CVPR, 2025. [20] N. Ravi, V. Gabeur, Y.-T. Hu, R. Hu, C. Ryali, T. Ma, H. Khedr, R. Rädle, C. Rolland, L. Gustafson, et al. Sam 2: Segment anything in images and videos. arXiv preprint arXiv:2408.00714, 2024. [21] P.-D. Tudosiu, Y. Yang, S. Zhang, F. Chen, S. McDonagh, G. Lampouras, I. Iacobacci, and S. Parisot. Mulan: multi layer annotated dataset for controllable text-to-image generation. In CVPR, pages 2241322422, 2024. [22] X. Wu, Y. Hao, K. Sun, Y. Chen, F. Zhu, R. Zhao, and H. Li. Human preference score v2: solid benchmark for evaluating human preferences of text-to-image synthesis. arXiv preprint arXiv:2306.09341, 2023. [23] J. Xu, X. Liu, Y. Wu, Y. Tong, Q. Li, M. Ding, J. Tang, and Y. Dong. Imagereward: Learning and evaluating human preferences for text-to-image generation. NeurIPS, 36, 2024. [24] K. Yamaguchi. Canvasvae: Learning to generate vector graphic documents. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 54815489, 2021. [25] L. Zhang and M. Agrawala. Transparent image layer diffusion using latent transparency. arXiv preprint arXiv:2402.17113, 2024. [26] X. Zhang, W. Zhao, X. Lu, and J. Chien. Text2Layer: Layered image generation using latent diffusion model. arXiv:2307.09781, 2023. [27] P. Zheng, D. Gao, D.-P. Fan, L. Liu, J. Laaksonen, W. Ouyang, and N. Sebe. Bilateral reference for high-resolution dichotomous image segmentation. CAAI Artificial Intelligence Research, 3:9150038, 2024. 11 A. Details of Suffix Prompt Templates Table 3 illustrates the detailed suffix prompt templates we adopted for LayerFLUX. Method detailed prompt SuffixPrompt SuffixPrompt SuffixPrompt SuffixPrompt SuffixPrompt SuffixPrompt SuffixPrompt SuffixPrompt Table 3: Effect of choosing different suffix prompt templates. on solid plain gray background. with clear, solid gray background. on solid single gray background. floating with background that is solid gray. cut-out on solid gray background. standing on background that is fully solid gray without any surrounding details isolated on solid gray background B. Generating Multi-Page and Multi-Layer Transparent Slides. We plan to extend our approach to generate multi-page, multi-layer transparent slides. Our framework not only produces single-layer transparent images but also assembles them into coherent slide decks with multiple pages. Each slide is constructed from several transparent layers, with each layer corresponding to different design elements. This modular, bottom-up strategy enables precise control over both the spatial layout and stylistic attributes of each slide, ensuring consistency across pages while preserving the flexibility to customize individual layers. C. Side Effect of Suffix Prompt. We admit that adding the suffix prompt is not free lunch and report the results of adding the suffix prompt on the GenEval benchmark in Table 4. We can see that the prompt-following capability of the original text-to-image generation model slightly drops, while the visual aesthetics are maintained. Model Overall Single Two Counting Colors Position Color FLUX.1-[dev] FLUX.1-[dev] + suffix prompt 0.657 0.591 0.978 0.906 0.816 0.609 0.716 0.628 0.801 0.723 0.228 0. 0.405 0.370 Table 4: Comparison results on GenEval. D. Technical Details of LayerDiffuse with FLUX. Our implementation of Layerdiffuse with FLUX is built on FLUX.1-[Dev] with LoRA. Specifically, we convert the image in the MAGICK dataset to grayscale according to the alpha channel mask. After training, the model is capable of generating grayscale background images without the need for additional conditional inputs. Then, we train transparency VAE decoder to enable the prediction of alpha channels. The decoder is trained on both the MAGICK dataset and an internal dataset, thereby enhancing its robustness and generalization. For the text sticker, we collect 5k dataset and use GPT-4o to reception of the image. E. Experiment Results of LayerFLUX. We construct Layer-Bench to evaluate the quality of the single-layer transparent images generated by our LayerFLUX. The Layer-Bench consists of 1,500 prompts divided into three types of prompt sets: (i) one that primarily focuses on natural objects sampled from the MAGICK [5] set, where each prompt describes photorealistic object; (ii) one centers on stickers and text stickers, where the text stickers contains visual text designed in creative typography and style to make the words stand out as part of the visual design; and (iii) one is about creative and stylistic objects. We construct the test set of stickers and text stickers by recaptioning sticker images crawled from the internet. We compare our approach to the latest state-of-the-art transparent image generation LayerDiffuse [25] by involving more than 20 participants from diverse backgrounds in AI, graphic design, art, and marketing. We present system level comparison in Table 6 and the user study results and visual comparisons in Figure 14 and Figure 13. We can see that our LayerFLUX achieves better results across the three types of prompt sets, especially in the creative, stylistic, or text sticker prompt sets. For example, our LayerFLUX achieves better layer quality and prompt following than LayerDiffuse, with win-rates of 63.1% and 61.2% when evaluated on our Layer-Bench. One possible concern might be that LayerDiffuse is built on SDXL [18] rather than FLUX.1-[dev]. We also fine-tune LayerDiffuse on existing transparent image datasets based on FLUX, but we find that the performance is even worse # samples Baseline (ART) 10 100 1000 TIPS (Layer Quality) 0.1140.077 0.1100.076 0.1300.086 0.1350.080 Composed Image Quality 4.6740.373 4.6840.543 4.9380.418 4.9360.415 Table 5: Effect of the high-quality data scale. Method 26.28 LayerDiffuse [25] LayerDiffuse w/ FLUX 24.33 Ours 26. Natural Object Layer Quality Creative Object Layer Quality Sticker Layer Quality HPSv2 AE-V2.5 TIPS HPSv2 AE-V2.5 TIPS HPSv2 AE-V2.5 TIPS 32.53 3.640 29.09 4.376 36.25 4.735 29.37 27.65 30.19 19.11 25.16 25.69 5.451 5.374 5.617 5.057 4.974 5.551 29.13 25.25 29. 21.51 25.79 26.14 Table 6: Comparison with LayerDiffuse on Layer-Bench. than that of the original LayerDiffuse based on SDXL. We infer that key reason is that the quality of data generated by these powerful models (like FLUX.1-[dev]) significantly outperforms that of existing transparent images available on the internet or predicted by existing models. This widening quality gap makes it risky to fine-tune them directly. In summary, our training-free LayerFLUX can better maintain the original capabilities of the off-the-shelf text-to-image generation model, providing solid foundation for wide range of applications. F. Effect of salient object matting model choice. How to extract high-quality alpha channels is critical for constructing high-quality single-layer transparent images. We study the influence of different salient object matting models, such as SAM2, BiRefNet, and RMBG-2.0, and summarize the comparison results on Layer-Bench in Table 7. We primarily consider the visual aesthetics of the transparent layers after matting and report the quantitative results. Additionally, we visualize the qualitative comparison results in Figure 12. We empirically find that RMBG-2.0 achieves the best results and adopt it as the default model. G. Prompt of the Creative Caption Generation Compared to the common images in the MAGICK dataset, creative images reflect the models ability to generate less frequent and more novel visual content. To evaluate this capability of our method, we constructed test set consisting of 500 creative prompts generated by GPT-4o, ensuring diversity and originality in the evaluation dataset. We mainly focus on single objective description generation H. Prompt of Multi-layer Style-align Recaption Instruction Given reference layer of multi-layer image, we leverage the visual recognition capabilities of GPT-4o and style-align reception instruction to transfer the original layer caption to specific style caption. Specifically, we paste the original layer to the center of gray background image while keeping the aspect ratio. Then, the style-specific instruction and the gray background layer image are fed to GPT-4o. Also, for the generation of ART, we use similar instruction prompt to transfer the overall writing and style of the global caption. I. How to Choose the Suffix Prompt? To understand how the suffix prompt helps the transparent layer generation task, we analyze the attention maps between the background regions and the color text tokens within the suffix prompt in Table 8, where we observe that the gray\" token achieves the best attention map response. We further conducted series of experiments to compute mIoUFG and mIoUBG by calculating the mean IoU between the binary attention mask and the mask predicted by an image matting model to demonstrate the effect of choosing different suffix prompts quantitatively. In addition, we compute the mean square error between the attention map and the matting mask using MSEBG and MSEFGLeak, where the latter metric reflects the degree of information leakage from the background to the foreground regions. We compute these metrics as follows: IoUBG = (1 M) (1 M) IoUFG = (1 A) (1 A) , , MSEBG = 1 MSEFGLeak = (cid:88) ((1 Mi) Ai)2, i=1 1 (cid:88) i=1 (Mi Mi Ai)2, (3) (4) 13 Method 26.24 SAM2 BiRefNet 26.03 RMBG-2.0 26.58 Creative Object Layer Quality Sticker Layer Quality Natural Object Layer Quality HPSv2 AE-V2.5 TIPS HPSv2 AE-V2.5 TIPS HPSv2 AE-V2.5 TIPS 36.76 4.556 35.24 4.719 36.25 4.735 5.251 5.503 5.551 Table 7: Effect of choosing different salient object matting models. 30.03 29.26 30.19 24.49 25.62 25.69 5.374 5.548 5.617 30.01 29.09 29.55 26.04 26.08 26.14 where denotes the binary foreground mask predicted by state-of-the-art image matting model, and denotes the binarized version of the attention mask computed between the suffix prompt tokens and the visual tokens extracted from the self-attention blocks within the diffusion transformer. denotes the number of pixels. In addition, we also use trajectory magnitude to analyze whether the diffusion model is able to control the background region pixels across all timesteps throughout the entire denoising trajectory. Refer to the Appendix for more details. Figure 8 visualizes the attention maps between the suffix tokens and the visual tokens. We can see that by choosing suitable suffix prompt, we can elicit the potential of the diffusion transformer to generate isolated background regions that are easy to segment. Suffix Prompt - original (w/o background prompt) 0.7863 half green and half red background half red and half blue background 0.7318 half gray and half black background 0.7902 half gray and half white background 0.7787 0.8282 solid red background 0.8554 solid green background 0.8379 solid blue background 0.7318 solid black background 0.8070 solid white background 0.5801 solid transparent background 0.8642 solid gray background Attention between Suffix text token and visual token Trajectory Magnitude dBG mIoUBG mIoUFG MSEBG 6.198 - 6.427 0.5943 6.420 0.5403 6.062 0.5692 6.266 0.5540 7.814 0.6398 6.624 0.6646 6.818 0.6493 8.317 0.5179 9.083 0.6495 7.872 0.3302 5.591 0.6809 MSEFGLeak - 0.2488 0.2413 0.2468 0.2275 0.2503 0.2401 0.2416 0.2409 0.2365 0.2262 0.2564 dFG dBG 0.041 -0.202 -0.200 0.243 0.093 -1.412 -0.376 -0.485 -1.749 -2.503 -1.413 0. - 0.4717 0.4868 0.4478 0.4701 0.4414 0.4706 0.4714 0.4255 0.3992 0.4410 0.4181 Table 8: Attention-map analysis of different suffix prompts. J. Effect of suffix prompt templates. As shown in Table 8, the design of the suffix prompt is important for guiding the text-to-image generation models to generate images consisting of objects that can be easily isolated from the background by ensuring an approximately single-colored background. Here, we further compare the matting results of nine different suffix prompt designs in Table 9. We empirically find that choosing isolated on solid gray background (SuffixPrompt H) achieves slightly better results. We provide the detailed suffix prompts in the appendix. K. Effect of color within suffix prompt. One natural question is which color is better for transparent layer generation. We investigate the influence of using different color words within the suffix prompt and summarize the results in Table 10. Accordingly, we find that using the color gray achieves the best results. This differs from the observation in previous work [5], which stated that using the color green performs best because green is the least common hue. L. More PrismLayersPro Mult-Layer Transparent Image Samples We visualize more PrismLayersPro mult-layer transparent image samples in Figure 15, Figure 16, and Figure 17. Figure 12: Qualitative comparison of different salient object matting models. From left to right, we show the matted results with RMBG-2.0, BiRefNet, and SAM2. Method Natural Object Layer Quality Creative Object Layer Quality Sticker Layer Quality HPSv2 AE-V2.5 TIPS HPSv2 AE-V2.5 TIPS HPSv2 AE-V2.5 TIPS 36.25 4.758 36.32 4.726 36.42 4.758 36.12 4.745 34.84 4.739 35.70 4.755 36.14 4.654 36.25 4.735 29.83 29.95 30.06 29.65 29.35 29.43 30.07 30.19 25.67 25.45 25.77 25.93 25.76 25.75 25.30 25.69 5.572 5.529 5.566 5.539 5.497 5.518 5.397 5. 5.609 5.587 5.625 5.631 5.493 5.607 5.468 5.617 26.07 25.98 26.14 26.23 26.12 26.10 25.72 26.14 29.12 29.28 29.35 29.38 28.78 29.28 29.87 29.55 SuffixPrompt 26.13 SuffixPrompt 26.29 SuffixPrompt 26.32 SuffixPrompt 25.95 SuffixPrompt 26.07 SuffixPrompt 26.01 SuffixPrompt 26.45 SuffixPrompt 26.58 Table 9: Effect of choosing different suffix prompt templates. Method 26.58 Gray 25.59 Green 26.29 Blue 25.70 Red 24.71 White 26.16 Black Transparent 26.26 Half green and half red 25.91 25.83 Half red and half blue Natural Object Layer Quality Creative Object Layer Quality Sticker Layer Quality HPSv2 AE-V2.5 TIPS HPSv2 AE-V2.5 TIPS HPSv2 AE-V2.5 TIPS 36.25 4.735 34.52 4.605 35.55 4.690 34.46 4.618 34.73 4.399 34.48 4.655 36.50 4.569 35.79 4.699 35.89 4.691 30.19 28.72 29.53 28.40 27.34 29.38 29.36 29.03 29.10 25.69 25.02 25.63 25.49 24.26 24.96 24.94 26.08 26.05 5.617 5.304 5.434 5.267 4.975 5.500 5.274 5.344 5.418 5.551 5.342 5.456 5.400 5.362 5.430 5.453 5.399 5. 29.55 28.78 29.29 28.72 27.97 28.78 29.64 29.72 29.75 26.14 25.62 25.83 25.68 25.28 25.34 25.47 25.93 25.99 Table 10: Effect of choosing different color within suffix prompt. Figure 13: Qualitative comparison of results with SOTA on Layer-Bench. The first row shows the results generated with LayerDiffuse, while the second row shows the results generated with our LayerFLUX. Figure 14: Illustrating the win-rate on single-layer transparent image generation benchmark Layer-Bench. Text Sticker Recaption Prompt for GPT-4o You are given the key word of text sticker and its corresponding image. Your task is to generate an accurate and descriptive caption for the sticker, following these guidelines: 1. The caption begins with \"The text sticker describes/contains/\" and ends with \"isolated on solid transparent background.\" 2. Clearly describe the text in the sticker, including the font color, font style, and any visual effects (e.g., shadows, gradients) observed in the image. 3. Keywords usually refer to the text in the sticker, and you may include other relevant descriptive elements. Be explicit about these in your caption. 4. Refer to the examples provided for clarity on how to construct your caption. Aim for creativity while adhering to the required structure. Here are some examples for reference: - \"The text sticker presents the word Focus in sharp, modern font, filled with gradient of charcoal gray to bright red. The letters are outlined in bright white, and stylized targets surround the text, conveying determination and clarity, isolated on solid transparent background.\" - \"The text sticker showcases the word Celebrate in festive, curly font, filled with vibrant confetti gradient of rainbow colors. Each letter is dotted with tiny sparkles, and balloons and streamers float around, enhancing the joyful spirit of celebration, isolated on solid transparent background.\" Please ensure to generate caption that fits this style and adheres to the guidelines. ************************************************** Response 1: {response 1} *************************************************** Please strictly follow the following format requirements when outputting, and dont have any other unnecessary words. Output Format: response 1 or response 2. Creative Object Layer Prompt for GPT-4o You are tasked with generating imaginative and creative image descriptions based on given object word. The generated description should follow these specific guidelines: ### **1. Input:** - You will receive single object word (e.g., \"penguin\", \"teapot\", \"robot\", etc.). - Use this object as the central focus of the description. ### **2. Output Requirements:** - The description should be **creative and unexpected**, modifying the object or adding elements that make it unusual, humorous, or visually striking. - The description **must not include details about the background**focus only on the main object and any additional elements that make it more interesting. - Aim for **concise but vivid** description, ideally **within 20 to 30 words**. - Use **strong visual language** to create mental image. - Avoid generic descriptionsmake it **fun, unique, and imaginative**. ### **3. Examples for Reference:** Given Object Generated Description - Kangaroo kangaroo holding beer, wearing ski goggles and passionately singing silly songs. Car car made out of vegetables. Raccoon cyberpunk-styled raccoon wearing neon glasses and futuristic jacket, holding laser gun in one paw. Teapot giant teapot with robotic arms, serving tea while wearing tiny monocle and top hat. Penguin punk-styled penguin with mohawk, leather jacket, and electric guitar, rocking out on an ice stage. ### **4. Constraints & Guidelines:** - Do **not** include the background in the description. - Feel free to **modify the objects appearance, abilities, or accessories** to make it more interesting. - If necessary, **add related objects** (e.g., robot might have futuristic gadgets, dog might have sunglasses and skateboard). - Keep the tone fun, artistic, and engaging. ### **5. Additional Notes:** Please directly respond to the prompt with the creative description. 16 Multi-layer Style Recaption Instruction for GPT-4o You will receive an RGBA image placed on gray background. Your task is to generate highly detailed description of the images content while adhering to given stylistic (STYLEPROMPT) requirement. **Key Guidelines:** 1. **Ignore the Gray Background:** - Do not mention or describe the gray background in any way. Focus solely on the foreground content. 2. **Handling Text in the Image:** - If the image contains any textual elements, the description **must** begin with **\"Text:\"** followed by precise transcription of all visible text. - Transcribe every word, symbol, punctuation mark, and character **without omission or modification**. - The description of text must be brief and the style description should be limited to 5 words. 3. **Handling Non-Text Elements:** - If the image contains **non-text elements**, generate an **detailed** description, capturing all visible aspects. - Ensure that the provided style, STYLEPROMPT, is seamlessly **integrated into the description**, maintaining coherence and natural flow. 4. **Output Format:** - Provide only the description of the image. Do **not** include any additional explanations, comments, or meta-information about the task itself. - The description **must explicitly state** that the image is in **STYLEPROMPT style**, starting with **\"This is STYLEPROMPT style image.\"** (VERY IMPORTANT) - Limited to 70 words!!! The image is shown below: 17 Figure 15: Visualizing High-Quality Transparent Image Samples of PrismLayersPro (1/3). 18 Figure 16: Visualizing High-Quality Transparent Image Samples of PrismLayersPro (2/3). Figure 17: Visualizing High-Quality Transparent Image Samples of PrismLayersPro (3/3)."
        }
    ],
    "affiliations": [
        "Microsoft Research Asia"
    ]
}