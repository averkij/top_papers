{
    "paper_title": "Explaining Sources of Uncertainty in Automated Fact-Checking",
    "authors": [
        "Jingyi Sun",
        "Greta Warren",
        "Irina Shklovski",
        "Isabelle Augenstein"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Understanding sources of a model's uncertainty regarding its predictions is crucial for effective human-AI collaboration. Prior work proposes using numerical uncertainty or hedges (\"I'm not sure, but ...\"), which do not explain uncertainty that arises from conflicting evidence, leaving users unable to resolve disagreements or rely on the output. We introduce CLUE (Conflict-and-Agreement-aware Language-model Uncertainty Explanations), the first framework to generate natural language explanations of model uncertainty by (i) identifying relationships between spans of text that expose claim-evidence or inter-evidence conflicts and agreements that drive the model's predictive uncertainty in an unsupervised way, and (ii) generating explanations via prompting and attention steering that verbalize these critical interactions. Across three language models and two fact-checking datasets, we show that CLUE produces explanations that are more faithful to the model's uncertainty and more consistent with fact-checking decisions than prompting for uncertainty explanations without span-interaction guidance. Human evaluators judge our explanations to be more helpful, more informative, less redundant, and more logically consistent with the input than this baseline. CLUE requires no fine-tuning or architectural changes, making it plug-and-play for any white-box language model. By explicitly linking uncertainty to evidence conflicts, it offers practical support for fact-checking and generalises readily to other tasks that require reasoning over complex information."
        },
        {
            "title": "Start",
            "content": "Explaining Sources of Uncertainty in Automated Fact-Checking Jingyi Sun* Greta Warren*"
        },
        {
            "title": "Irina Shklovski Isabelle Augenstein",
            "content": "University of Copenhagen {jisu, grwa, ias, augenstein}@di.ku.dk 5 2 0 2 3 2 ] . [ 1 5 5 8 7 1 . 5 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Understanding sources of models uncertainty regarding its predictions is crucial for effective human-AI collaboration. Prior work proposes to use numerical uncertainty or hedges (Im not sure, but. . . ), which do not explain uncertainty arising from conflicting evidence, leaving users unable to resolve disagreements or rely on the output. We introduce CLUE (Conflict-&Agreement-aware Language-model Uncertainty Explanations), the first framework to generate natural language explanations of (i) identifying relamodel uncertainty by: tionships between spans of text that expose claim-evidence or inter-evidence conflicts/agreements driving the models predictive uncertainty in an unsupervised way; and (ii) generating explanations via prompting and attention steering to verbalize these critical interactions. Across three language models and two fact-checking datasets, we demonstrate that CLUE generates explanations that are more faithful to model uncertainty and more consistent with fact-checking decisions than prompting for explanation of uncertainty without spaninteraction guidance. Human evaluators find our explanations more helpful, more informative, less redundant, and more logically consistent with the input than this prompting baseline. CLUE requires no fine-tuning or architectural changes, making it plug-and-play for any white-box language model. By explicitly linking uncertainty to evidence conflicts, it offers practical support for fact-checking and readily generalizes to other tasks that require reasoning over complex information."
        },
        {
            "title": "Introduction",
            "content": "Large Language Models (LLMs) are increasingly prevalent in high-stakes tasks that involve reasoning about information reliability, such as factchecking (Wang et al., 2024; Fontana et al., 2025). *Equal contribution. Figure 1: Example of claim and evidence documents, alongside span interactions for uncertainty and generated natural language explanations. To foster effective use of such models in factchecking tasks, these models must explain the rationale for their predictions (Atanasova et al., 2020; Kotonya and Toni, 2020). However, current methods in automated fact-checking have been criticised for their failure to address practical explainability needs of fact-checkers (Warren et al., 2025) and for their disconnect from the tasks typically performed by fact-checkers (Schlichtkrull et al., 2023). For example, although fact-checking involves complex reasoning about the reliability of (often conflicting) evidence, existing automatic fact-checking techniques focus only on justifying the verdict (Atanasova et al., 2020; Stammbach and Ash, 2020; Zeng and Gao, 2024). Such methods do not explain the uncertainty associated with their predictions, which is crucial for their users to determine whether some of the uncertainty is resolvable, and if so, which aspects of this uncertainty within the evidence to address (e.g., by retrieving additional information) (Warren et al., 2025). Uncertainty in model predictions is often communicated through numerical scores (e.g., am 73% confident), however, such metrics can be hard to contextualize and lack actionable insights for end-users (Zimmer, 1983; Wallsten et al., 1993; van der Waa et al., 2020; Liu et al., 2020). Recent efforts have instead used natural language expressions (e.g., Im not sure) to convey uncertainty (Steyvers et al., 2025; Yona et al., 2024; Kim et al., 2024), but such expressions often fail to faithfully reflect model uncertainty (Yona et al., 2024), and users may overestimate model confidence (Steyvers et al., 2025). Existing explainable fact-checking systems exhibit two critical limitations: they focus solely on justifying veracity predictions through generic reasoning summaries of the input sequence (see Figure 2), while neglecting to (1) communicate model uncertainty or (2) explicitly surface evidentiary conflicts and agreements that relate to it. This constitutes fundamental methodological gap, as effective fact-checking requires precisely identifying the sources of uncertainty, for example from conflicting evidence, to guide targeted verification (Graves, 2017; Micallef et al., 2022). We propose CLUE, pipeline that generates natural language explanations (NLEs) of model uncertainty by explicitly capturing conflicts and agreements in the input (e.g., claim and its supporting or refuting evidence). The pipeline first identifies the salient span-level interactions that matter to the prediction of the model through an unsupervised approach, providing an input-feature explanation that highlights key relationships between separate input segments (e.g., claim and evidence) (Ray Choudhury et al., 2023). These interactions have been shown to be both faithful to the model and plausible to humans (Sun et al., 2025). CLUE then converts these signals into uncertainty-aware explanations by explicitly discussing the interactions, the conflict/agreement relations they express and how they contribute to uncertainty regarding the verdict. CLUE does not require gold-label explanations, avoids fine-tuning, and operates entirely at inference time. Across three language models (4.2) and two fact-checking datasets (4.1), we evaluate two variants of CLUE. Automatic metrics show that both variants generate explanations that are more faithful to each models uncertainty and agree more closely with the gold fact-checking labels than prompting baseline that lacks conflict-/agreement-span guidance (5.5). Human study participants likewise judge CLUE explanations as more helpful, more informative, less redundant, and more logically consistent with the input. We also observe tradeoff between two variants of our CLUE framework, one attains higher faithfulness, the other higher plausibility, highlighting promising avenue for future work to achieve both simultaneously (5.5)."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 Uncertainty Quantification in LLMs Recent work on LLM uncertainty quantification primarily relies on logit-based methods such as answer distribution entropy (Kadavath et al., 2022), summing predictive entropies across generations (Malinin and Gales, 2021), and applying predictive entropy to multi-answer question-answering (Yang et al., 2025). Estimating uncertainty in long-form tasks involves measuring semantic similarity between responses (Duan et al., 2024; Kuhn et al., 2023; Nikitin et al., 2024). Given that logit-based uncertainty quantification is infeasible for closedsource black-box models, alternative approaches have depended on verbalizing confidence directly (Lin et al., 2022; Mielke et al., 2022), though these measures are overconfident and unreliable (Yona et al., 2024; Tanneru et al., 2024). Other approaches measure output diversity across paraphrased prompts (Zhang et al., 2024a; Chen and Mueller, 2024), but this technique can introduce significant computational overhead and conflate model uncertainty with prompt-induced noise, obscuring interpretability. Accordingly, in this work, we focus on the uncertainty of open-source models, which are readily accessible and widely used. We adopt predictive entropy, straightforward whitebox metric computed from the models answer logits, as our uncertainty measure for fact-checking tasks. This choice balances interpretability and computational efficiency while avoiding potential noise introduced by multiple prompts."
        },
        {
            "title": "2.2 Linguistic Expressions of Uncertainty",
            "content": "Numerical uncertainty estimates do not address the sources of uncertainty, and are therefore difficult for end-users, such as fact-checkers, to interpret and act upon (Warren et al., 2025). Linguistic exFigure 2: Explanations produced by earlier systems, e-FEVER (Stammbach and Ash, 2020), Explain-MT (Atanasova et al., 2020), and JustiLM (Zeng and Gao, 2024), compared with those from our CLUE framework. CLUE is the only approach that explicitly traces model uncertainty to the conflicts and agreements between the claim and multiple evidence passages. pressions of uncertainty may be more intuitive for people to understand than numerical ones, (Zimmer, 1983; Wallsten et al., 1993; Windschitl and Wells, 1996), and recent work has proposed models that communicate uncertainty through hedging phrases such as am sure or doubt (Mielke et al., 2022; Lin et al., 2022; Zhou et al., 2023; Tian et al., 2023; Xiong et al., 2023; Ji et al., 2025; Zheng et al., 2023; Farquhar et al., 2024). However, these expressions are not necessarily faithful reflections of the models uncertainty (Yona et al., 2024) and tend to overestimate the models confidence (Tanneru et al., 2024), risking misleading users (Steyvers et al., 2025). Moreover, they do not explain why the model is uncertain. In this paper, we propose method that explains sources of model uncertainty by referring to specific conflicting or concordant parts of the input that contribute to the models confidence in the output. This approach ensures more faithful reflection of model uncertainty and provides users with more intuitive and actionable understanding of model confidence."
        },
        {
            "title": "2.3 Generating Natural Language",
            "content": "Explanations for Fact-Checking Natural language explanations provide justifications for model predictions designed to be understood by laypeople (Wei Jie et al., 2024). NLEs have typically been evaluated by measuring the similarity between generated NLEs and humanwritten reference explanations using surface-level metrics such as ROUGE-1 (Lin, 2004) and BLEU (Papineni et al., 2002). In fact-checking, supervised methods have been proposed that involve extracting key sentences from existing fact-checking articles and using them as explanations (Atanasova et al., 2020). Later work proposed post-editing mechanism to enhance the coherence and fluency of explanations, (Jolly et al., 2022), while others have finetuned models on data collected from fact-checking websites to generate explanations (Feher et al., 2025; Raffel et al., 2020; Beltagy et al., 2020). Recent work has shifted towards few-shot methods requiring no fine-tuning, for example, using few-shot prompting with GPT-3 (Brown et al., 2020) to produce evidence summaries as explanations (Stammbach and Ash, 2020) incorporating planning step before explanation generation (Zhao et al., 2024) to outperform standard prompting approaches, and generating fact-checking justifications based on retrieval-augmented language models (Zeng and Gao, 2024). However, existing methods are often not faithful to model reasoning (Atanasova et al., 2023; Siegel et al., 2024, 2025), have limited utility in fact-checking (Schmitt et al., 2024), and fail to address model uncertainty, which has been identified as key criterion for fact-checking (Warren et al., 2025). To this end, we introduce the first framework designed for the task of explaining sources of uncertainty in multi-evidence fact-checking. Our method analyzes span-level agreements and conflicts correlated with uncertainty scores. Unlike conventional approaches that aim to replicate human NLEs (prioritising fluency or plausibility over faithfulness to model reasoning), our method generates explanations that are both faithful to model uncertainty and helpful to people in fact-checking context. answers = {SUPPORTS, REFUTES, NEUTRAL}. For each candidate label yi Y:"
        },
        {
            "title": "3 Method",
            "content": "3.1 Preliminaries and Overall Framework Our objective is to explain why LLM is uncertain about multi-evidence fact-checking instance by grounding that uncertainty in specific agreements or conflicts within the input. Problem setup. Each input instance is triple = (C, E1, E2) consisting of claim and two evidence pieces E1, E2. Note that, in this work, we set the number of evidence pieces to two for simplicity. For clarity, we denote their concatenation as = [x1, . . . , xC+E1+E2]. The task label comes from the set = {SUPPORTS, REFUTES, NEUTRAL}. Pipeline overview. Our framework comprises three stages: 1. Uncertainty scoring. We compute predictive entropy from the models answer logits to obtain scalar uncertainty score u(X) ( 3.2). This logit-based measure is model-agnostic. 2. Conflicts/Agreement extraction. We capture the agreements and conflicts most relevant to the models reasoning by identifying the textspan interactions between C, E1, and E2 that embody these relations ( 3.3). 3. Explanation generation. The model receives the extracted spans as soft constraints and produces natural-language rationale YR = [y r] along with its predicted label ˆy to the identified interactions ( 3.4). 1, . . . , Outputs. For each instance X, the framework returns the predicted task label ˆy Y; the numeric uncertainty score u(X); and the textual explanation YR = [y r] that grounds the source of uncertainty in the specific agreements or conflicts between C, E1, E2. 1, . . . , y"
        },
        {
            "title": "3.2 Predictive Uncertainty Score Generation",
            "content": "To quantify model uncertainty for generating an answer label on specific input sequence, we follow previous work and calculate predictive uncertainty with entropy theory, which does not require multiple runs and is widely used in open-source models. Specifically, we define the numeric uncertainty score as the entropy of the softmax distribution over the models output logits for set of candidate (yi X) = exp(logit(yi)) j=1 exp(logit(yj)) (cid:80)Y (1) where logit(yi) is the models output logit towards candidate answer yi given input X. (yi X) is the confidence score of model for selecting yi as the final answer across all candidate answers within Y. Finally, the models uncertainty towards the input sequence is: u(X) = (cid:88) yiY (yi X) log (yi X) (2) 3.3 Conflict and Agreement Span Interaction Identification for Answer Uncertainty To surface the conflicts and agreements that drive models uncertainty, we extract and then label salient span interactions among the claim and two evidence passages, E1 and E2. interaction extraction. For Span (F, ) each ordered input part pair {(C, E1), (C, E2), (E1, E2)}, we follow previous work (Ray Choudhury et al., 2023; Sun et al., 2025) to extract the important span interactions and their importance score to models answer by (i) identifying the most important attention head to the models answer prediction from its final (ii) obtaining its attention matrix R(F +T )(F +T ), and (iii) symmetrizing the cross-part scores: layer, p,q = 1 2 (cid:0)Ap,q + Aq,p (cid:1), xp F, xq T. Treating p,q as edge weights yields bipartite token graph, which we partition into contiguous spans with the Louvain algorithm (Blondel et al., 2008). Given spanw and spanv , their interaction importance is awv = 1 spanw spanv (cid:88) (cid:88) p,q. (3) xpspanw xqspanv The scored interactions for (S, ) form S(S,T ) = {((spanw, spanv), awv)}. Relation labeling. To tag each span pair as an agreement, disagreement, or unrelated, we prompt GPT-4o (OpenAI Team, 2024)1 to assign label 1https://openai.com/index/hello-gpt-4o/ rwv {agree, disagree, unrelated},balancing scalability and accuracy (See templates in App. H.6). After labeling all three pairs, the complete interaction set for instance is for example, SR = SR(C, E1) SR(C, E2) SR(E1, E2), (4) = where, {((spanw, spanv), awv, rwv)}. Each element links two spans with an importance score and relation label, thereby supplying the conflictor agreement-span interactions used in later stages. SR(C, E1) 3.4 Uncertainty Natural Language Explanation Generation To convert the extracted conflictand agreement spans to rationales for model uncertainty, we rely on two complementary mechanisms. (i) Instruction-driven prompting embeds the spans directly in the input so the model is instructed (ii) Intrinsic atwhich segments to reference. tention steering guides the models own attention toward those same segments while it is generating the rationale. Both mechanisms use selfrationalization: the model first states its verdict ˆy and then explains YR, sequencing shown to improve faithfulness over pipeline approaches (Wiegreffe et al., 2021; Marasovic et al., 2022; Siegel et al., 2025). Instruction-based NLE. For each instance X, we rank all labelled interactions by importance and keep the top = 3, denoted S(K) , to avoid overly long explanations. These three span pairs are slotted into three-shot prompt (See App.F.1), which instructs the model to explain how the highlighted agreements or conflicts influence its confidence. Finally, the standard transformer decoding process outputs both the predicted label ˆy and the accompanying explanation YR. Attention steering. Instead of explicit instructions, we can guide generation by modifying attention on the fly with PASTA (Zhang et al., 2024b). Starting from the same S(K) , we collect all token indices that fall inside any selected span, = (cid:8)p : (spanw, spanv) S(K) , spanwspanv (cid:9). (5) down-weight non-target tokens by β: Aij = Zi = Aij Zi (cid:88) jI (cid:40) 1 if I, β otherwise, Aij + β Aij. (cid:88) /I (6) (7) All other heads remain unchanged. Following Zhang et al. (2024b), we steer = 100 heads and set β = 0.01 to balance steering efficacy and prevent degeneration; see App. for the headselection procedure. With the steered attention in place, the transformer generates ˆy followed by the rationale YR, now naturally centered on the conflictor agreement spans that drive its uncertainty."
        },
        {
            "title": "4.1 Datasets",
            "content": "We select two fact-checking datasets, one specific to the health domain, HealthVer (Sarrouti et al., 2021), and one closer to real-world factchecking scenario, DRUID (Hagström et al., 2024). These datasets were chosen because they provide multiple evidence pieces per claim, making them well-suited to our goal of explaining model uncertainty arising from the inter-evidence conflicts and agreements. For experiments, we select six hundred instances that consist of claim and multiple pieces of evidence, and golden label {SUPPORTS, REFUTES, NEUTRAL} from each dataset."
        },
        {
            "title": "4.2 Models",
            "content": "We compare three generation strategies for NLEs towards model uncertainty: PromptBaseline: three-shot prompt baseline extending prior few-shot NLE work (Stammbach and Ash, 2020; Zeng and Gao, 2024; Zhao et al., 2024) by explicitly asking the model to highlight conflicting or supporting spans that shape its uncertainty (See prompt template in App. F.1). CLUE-Span: The instruction-based variant of our CLUE method where the extracted span interactions are filled into three-shot prompt to guide the explanation generation ( 3.4; prompt template in App.F.2). For each attention head (ℓ, h) deemed relevant to model uncertainty, let be its attention matrix. We 2While DRUID has six fine-grained fact-checking labels, we merge the labels into the above three categories to balance the label categories. CLUE-Span+Steering: The attention steering variant of our CLUE method in which the same prompt as CLUE-Span is used. Additional attention steering is applied to instinctively guide the models explanation generation toward the identified spans ( 3.4; prompt template in App. F.2). Experiments are run on three recent, openweight, instruction-tuned LLMs of comparable size: Qwen2.5-14B-Instruct3 (Qwen Team, 2024), Gemma-2 9B-IT4 (Gemma Team, 2024), and OLMo-2-1124-13B-Instruct5 (Team OLMo et al., 2024). Each backbone is used consistently across our pipeline for span-interaction extraction, answer prediction, and NLE generation on four NVIDIA A100-SXMS-40GB GPUs. We selected these models to balance capability (reasoning and instructionfollowing quality) with practical constraints on inference latency and GPU memory."
        },
        {
            "title": "5.1 Faithfulness",
            "content": "To assess whether the NLEs produced by CLUE are faithful to the models uncertainty, we adapt the Correlational Counterfactual Test (CCT) (Siegel et al., 2024) and propose an Entropy-CCT metric. Following Siegel et al. (2024), we begin by inserting random adjective or noun into the original instance to obtain perturbed input (See App. for details). Let u(X) denote the models uncertainty score defined by Eq. 2, unlike CCT(See details of original CCT in App.E), we measure the impact of the perturbation on the models uncertainty with Absolute Entropy Change (AEC): u(X) = (cid:12) (cid:12)u(X) u(X )(cid:12) (cid:12) (8) For each perturbation, we record whether the inserted word appears in the generated NLE, using its presence as proxy for importance. This yields binary mention flag {0, 1}, following Siegel et al. (2024); Atanasova et al. (2023). Let Dm denote the set of perturbed examples where the NLE mentions the inserted word and Dm is the complementary set where it does not, we correlate the continuous variable with the 3https://huggingface.co/Qwen/Qwen2. 5-14B-Instruct 4https://huggingface.co/google/gemma-2-9b-it 5https://huggingface.co/allenai/ OLMo-2-1124-13B-Instruct binary mention flag via the point-biserial correlation rpb (Tate, 1954). The Entropy-CCT statistic is: CCTentropy=rpb= Em[u]Em[u] Std(u) (cid:114) DmDm (Dm+Dm)2 (9) where Em[u] and Em[u] are the mean absolute entropy changes for these two groups, respectively. Std(u) is the standard deviation of absolute entropy changes across the full dataset. Ultimately, this metric quantifies the alignment between changes in model uncertainty and explanatory references to input perturbations, thereby measuring how faithfully the NLEs reflect the models uncertainty. 5.2 Span-Coverage An uncertainty explanation should surface all information conveyed by the selected span interactions. We therefore compute Span-Coverage: the fraction of reference interactions that are explicitly mentioned in the generated NLE. Let SNLE be the set of span interactions extracted from the explanation, and let SR(k) be the reference set supplied in the prompt (see 3.4). Then Span-Coverage = SNLE SR(k) SR(k) . (10) higher value indicates the NLE covers higher proportion of the information supplied by the extracted span interactions."
        },
        {
            "title": "5.3 Span-Extraneous",
            "content": "Ideally, the explanation should mention only the provided interactions and avoid introducing extraneous information. We measure the proportion of mentioned interactions that do not belong to the reference set, denoted Span-Extraneous: Span-Extraneous = SNLE SR(k) SNLE . (11) lower value indicates closer alignment with the intended span interactions."
        },
        {
            "title": "5.4 Label-Explanation Entailment",
            "content": "We evaluate the extent to which the uncertainty explanation agrees with the models predicted label by formulating the task as natural-language inference (NLI) problem. First, we convert the predicted label into hypothesis using the template The claim is supported by / refuted by / neutral to the evidence. The generated explanation serves as the premise. The resulting premisehypothesis pair is fed to widely used off-the-shelf languageinference model, DeBERTa-v36 (He et al., 2023). The Label-Explanation Entailment (LEE) score is the proportion of examples for which the NLI model predicts ENTAILMENT. 5.5 Results Here, we present the results of our automatic evaluation. For brevity, we refer to Qwen2.5-14BInstruct, OLMo-2-1124-13B-Instruct, and Gemma2-9B-it simply as Qwen, OLMo, and Gemma, respectively. Faithfulness. We use Entropy-CCT, point biserial correlation bounded by 1 rpb 1 (Eq. 9), to measure the faithfulness of NLEs to the models uncertainty (5.1). When rpb = 0, the explanation mentions highand low-impact perturbation words equally often; every +0.01 adds roughly one percentage point (pp) to the chance that the explanation names token that is truly influential for the models predictive uncertainty (App. G). Table 1 shows that PromptBaseline is nonfaithful in all six settings with rpb are all negative values ranging from 0.03 to 0.13. Thus its NLEs mention truly influential tokens 313 pp less often than uninfluential onesthe opposite of faithful behaviour. Both variants of our CLUE reverse this trend. Presenting span interactions in the prompt (CLUE-Span) raises every correlation to non-negative values and peaks at rpb = 0.089 on the DRUIDQwen setting. This means the explanation now mentions about 17 pp more often than PromptBaseline(rpb = 0.080). Adding attention steering (CLUE-Span+Steering) lifts the rbp scores to 0.033 on HEALTHVER and 0.102 on DRUID with Qwen model, i.e., net gains of +6 pp and +18 pp over PromptBaseline. Moreover, four of the six positive correlations produced by CLUE-Span+Steering are significant at < 0.01 (Table 4 in App. G.3), confirming that the improvements are both substantial and statistically reliable. Particularly large jumps of OLMo on Druid dataset (up to rpb = +0.23 +23 pp) suggest that span-interaction guidance from our CLUE framework is most beneficial for models that initially struggle to align explanations with predictive 6https://huggingface.co/MoritzLaurer/ DeBERTa-v3-large-mnli-fever-anli-ling-wanli uncertainty. (5.4). Other Properties We evaluate three futher properties of the generated NLEs: (i) Span-Coverage of extracted conflict-/agreementspan interac- (ii) Span-Extraneous: mention tions (5.2), of non-extracted spans (5.3), and (iii) LabelExplanation Entailment with the generated factAs Table 1 shows, checking label CLUE-Span+Steering outperforms CLUE-Span in both Span-Coverage and Span-extraneous, consistent with the attention steering methods effectiveness in directing the model to focus on provided spans during generation (Zhang et al., 2024b). Absolute numbers, however, remain modest (peak Span-Coverage: .44, Span-Extraneous: .20 with Qwen). Span-Coverage of 1 means the NLE cites every extracted interaction, while Span-Extraneous score of 0 means it adds none beyond them. This gap highlights considerable headroom for better integrating critical span interactions into the explanations. Among the three backbones, Qwen attains the highest Span-Coverage and the lowest Span-Extraneous scores, trend that likely reflects its stronger instruction-following ability (see benchmark scores in App. A), and thus larger or more capable models might further narrow the gap. Both variants of our framework achieve stronger label-explanation entailment scores than the baseline, yielding explanations logically consistent with the predicted labels while remaining faithful to the models uncertainty patterns (as demonstrated in our faithfulness analysis)."
        },
        {
            "title": "6.1 Method",
            "content": "We recruited N=12 participants from Prolific (https://www.prolific.com/) to rank explanations generated by PromptBaseline, CLUE-Span, CLUE-Span+Steering for 40 instances (20 from DRUID, 20 from HealthVer) (see details about participants and set-up in App. H.1). Adapting Atanasova et al. (2020), participants ranked explanations in descending order (1st, 2nd, 3rd) according to five criteria, complementary to our automatic evaluation metrics: Helpfulness. The explanation offers information that aids readers to judge the claim and factcheck. Coverage. The explanation captures all salient information in the input that matters for the Approach Faith. () Span-Cov. () Span-Ext. () LEE () Faith. () Span-Cov. () Span-Ext. () LEE () HealthVer DRUID PromptBaseline CLUE-Span CLUE-Span+Steering PromptBaseline CLUE-Span CLUE-Span+Steering PromptBaseline CLUE-Span CLUE-Span+Steering -0.028 0.006 0.033 -0.10 0.005 0. -0.105 0.007 0.021 0.33 0.44 0.10 0.23 0.34 0.39 Qwen2.5-14B-Instruct OLMo-2-1124-13B-Instruct 0.68 0.53 0.83 0.77 0.59 0.50 0.74 0.75 0.80 0.55 0.61 0.68 0.66 0.82 0. -0.08 0.089 0.102 -0.13 0.014 0.099 -0.12 0.043 0.098 0.20 0.28 0.08 0.15 0.23 0. 0.38 0.20 0.79 0.70 0.43 0.47 0.60 0.78 0.77 0.53 0.65 0.69 0.57 0.76 0. Gemma-2-9B-It Table 1: Uncertainty NLE evaluation results across the HealthVer and DRUID datasets (4.1). For each model (4.2) we compare PromptBaseline, CLUE-Span, and CLUE-Span+Steering on four metrics: Faith. (5.1), Span-Cov. (5.2), Span-Ext. (5.3), and LEE (5.4). Bold values mark the best result per metric for each datasetmodel pair; indicates inapplicable metrics for PromptBaseline , as it is not supplied with extracted span interactions. fact check, distinct from Span-Coverage (5.2), which counts overlap with pre-extracted spans. Non-redundancy. The explanation does not offer irrelevant or repetitive information to the input, distinct from Span-Extraneous (5.3) which counts mentions outside the extracted spans. Consistency. The explanation contains logically consistent statements to the input, disticnt from Label-Explanation Entailment (5.4), which measures label-explanation alignment. Overall Quality. Ranking of explanations by their overall quality, considering all criteria above."
        },
        {
            "title": "6.2 Results",
            "content": "The results of our evaluation results are depicted in Table 2. Annotator agreement was moderate to low (see App. H.2.1), which we attribute to the relative complexity of the task and individual differences in how the information was perceived. The explanations generated by CLUE were preferred by our participants to those generated using PromptBaseline: the explanations generated by CLUE-Span+Steering were rated as most helpful, highest coverage, and containing the least amount of redundant information, while those from CLUE-Span were judged to have the highest consistency and overall quality. Although CLUE-Span+Steering achieves the highest faithfulness (see 5.5), our participants judged its overall quality slightly lower than that of CLUE-Span. possible reason for this is that although CLUE-Span+Steering adheres closely to the top-K=3 extracted span interactions (as reflected in its higher Span-Coverage and lower SpanExtraneous scores), it may produce explanations that are slightly less internally consistent or fluent. In contrast, CLUE-Span is less faithful to those extracted spans, but may capture additional points that study participants deemed important, likely because the spans identified as important for model do not fully overlap with those identified by humans (Ray Choudhury et al., 2023), highlighting the well-documented trade-off between faithfulness and plausibility (Agarwal et al., 2024). Future work on improving the plausibility of the span interactions while retaining their faithfulness may therefore improve the human evaluation scores for CLUE-Span+Steering. Finally, we observed slight variation between datasets: CLUE-Span+Steering tended to be rated higher than CLUE-Span for DRUID, and vice versa for HealthVer. This may arise from differences in length and complexity of the input: DRUID evidence documents, retrieved from heterogeneous online sources and often consisting of longer form new articles, may have benefited from attention steering more than HealthVer evidence documents which consist of focused, shorter extracts from scientific abstracts."
        },
        {
            "title": "7 Conclusion",
            "content": "We present the first framework, CLUE, for generating NLEs of model uncertainty by referring to the conflicts and agreements between claims and multiple pieces of evidence in fact-checking task. Our method, evaluated across three language models and two datasets, demonstrates significant improvements in both faithfulness to model uncertainty and PromptBase CLUE-S CLUE-SS Helpfulness Overall DRUID HealthVer Consistency Overall DRUID HealthVer 2.025 1.9 2.15 1.875 1.717 2.033 Non-redundancy Overall DRUID HealthVer 2.05 1.983 2.117 Coverage Overall DRUID HealthVer 1.967 1.767 2. Overall Quality Overall DRUID HealthVer 1.967 1.9 2.033 1.892 1.917 1.867 1.783 1.75 1.817 1.908 1.983 1.833 1.775 1.75 1. 1.908 1.9 1.917 1.867 1.767 1.967 1.817 1.617 2.017 1.833 1.683 1.983 1.758 1.617 1.9 1.925 1.817 2. Table 2: Mean Average Rank (MAR) for the five human-evaluation criteria applied to explanations from Qwen2.5-14B-Instruct on the HEALTHVER and DRUID datasets (chosen for its high faithfulness; see 5.5). PromptBaseline, CLUE-Span (CLUE-S), and CLUE-Span+Steering (CLUE-SS) are compared. Lower MAR means better (higher) average rank; the best score in each row is boldfaced. label consistency compared to standard prompting. Evaluations by human participants further demonstrate that the explanations generated by CLUE are more helpful, more informative, less redundant, and more logically consistent with the input. This work establishes foundation for explainable fact-checking systems, providing end users (e.g., fact-checkers) with grounded, faithful explanations that reflect the models uncertainty."
        },
        {
            "title": "Limitations",
            "content": "Our paper proposes novel framework for generating NLEs towards the models uncertainty by explicitly pointing to the conflicts or agreements within the claim and multi-evidence interactions. While our framework demonstrates improved explanation quality through rigorous evaluation across three language models and two datasets, we acknowledge several limitations that present opportunities for future research. Our experiments are constrained to mediumsized models (Qwen2.5-14B-Instruct, Gemma29B-it, and OLMo2-13B-Instruct) which were selected based on computational limitations. Although these models show significant improvements over baseline performance, our results suggest that larger models (e.g., 70B parameter scale) with enhanced instruction-following and reasoning capabilities might further improve explanation quality particularly for coverage and redundancy metrics. Our frameworks modular design readily accommodates such scaling. In this study we focus on the HealthVer and DRUID datasets, in which claims are paired with discrete pieces of evidence, ideal for studying evidence-conflict scenarios. Future work could investigate more complex evidence structures (e.g., long-form documents), diverse fact-checking sources, and scenarios with more than two pieces of evidence per claim to better reflect real-world fact-checking challenges. While our evaluation with laypeople confirms that our framework produces explanations of higher quality than prompting, expert evaluations (e.g., with professional fact-checkers) are needed to assess practical utility in high-stakes settings. Our work is limited to the scope of explaining model uncertainty arising from evidence conflicts. While this captures critical subset of cases, real-world uncertainty may also stem from other sources, including insufficient evidence, knowledge gaps in the model, and context-memory conflicts. We view this work as foundational step toward broader research on model uncertainty explanation."
        },
        {
            "title": "Ethical Considerations",
            "content": "This work concerns automated fact-checking, which aims to reduce the harm and spread of misinformation, but nevertheless has the potential for harm or misuse through model inaccuracy, hallucination, or deployment for censorship. Our current work aims to provide explanation that allow users to examine the outputs of these systems more critically, and so we do not see any immediate risks associated with it. Our work is limited to examining claims, evidence, and explanations in English, and so our results may not be generalisable to other languages. As the task involved complex reasoning about technical subjects, we screened our participants to be native English speakers to ensure that they could fully understand the material and increase the chances of high-quality responses (see H.1 for details). However, this criteria may also introduce or reinforce existing biases and limit the generalisability of our findings. Participants were informed about the study and its aims before agreeing to provide informed consent. No personal data was collected from participants and they received fair payment for their work (approximately 9 GBP/hour)."
        },
        {
            "title": "Acknowledgments",
            "content": "This research was co-funded by the European Union (ERC, ExplainYourself, 101077481), by the Pioneer Centre for AI, DNRF grant number P1, as well as by The Villum Synergy Programme. Views and opinions expressed are however those of the author(s) only and do not necessarily reflect those of the European Union or the European Research Council. Neither the European Union nor the granting authority can be held responsible for them."
        },
        {
            "title": "References",
            "content": "Chirag Agarwal, Sree Harsha Tanneru, and Himabindu Lakkaraju. 2024. Faithfulness vs. plausibility: On the (un)reliability of explanations from large language models. Preprint, arXiv:2402.04614. Pepa Atanasova, Oana-Maria Camburu, Christina Lioma, Thomas Lukasiewicz, Jakob Grue Simonsen, and Isabelle Augenstein. 2023. Faithfulness tests for natural language explanations. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 283294, Toronto, Canada. Association for Computational Linguistics. Pepa Atanasova, Jakob Grue Simonsen, Christina Lioma, and Isabelle Augenstein. 2020. Generating fact checking explanations. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 73527364, Online. Association for Computational Linguistics. Iz Beltagy, Matthew E. Peters, and Arman Cohan. 2020. Longformer: The long-document transformer. Vincent D. Blondel, Jean-Loup Guillaume, Renaud Lambiotte, and Etienne Lefebvre. 2008. Fast Unfolding of Communities in Large Networks. Journal of statistical mechanics: theory and experiment, 2008(10):P10008. Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. In AdLanguage models are few-shot learners. vances in Neural Information Processing Systems, volume 33, pages 18771901. Curran Associates, Inc. Jiuhai Chen and Jonas Mueller. 2024. Quantifying uncertainty in answers from any language model and enhancing their trustworthiness. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 51865200, Bangkok, Thailand. Association for Computational Linguistics. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168. Jinhao Duan, Hao Cheng, Shiqi Wang, Alex Zavalny, Chenan Wang, Renjing Xu, Bhavya Kailkhura, and Kaidi Xu. 2024. Shifting attention to relevance: Towards the predictive uncertainty quantification of In Proceedings free-form large language models. of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 50505063, Bangkok, Thailand. Association for Computational Linguistics. Sebastian Farquhar, Jannik Kossen, Lorenz Kuhn, and Yarin Gal. 2024. Detecting Hallucinations in Large Language Models using Semantic Entropy. Nature, 630(8017):625630. Darius Feher, Abdullah Khered, Hao Zhang, Riza Batista-Navarro, and Viktor Schlegel. 2025. Learning to Generate and Evaluate Fact-Checking Explanations with Transformers. Engineering Applications of Artificial Intelligence, 139:109492. Nicolo Fontana, Francesco Corso, Enrico Zuccolotto, and Francesco Pierri. 2025. Evaluating open-source large language models for automated fact-checking. Preprint, arXiv:2503.05565. Gemma Team. 2024. Gemma: Open models based on gemini research and technology. Lucas Graves. 2017. Anatomy of fact check: Objective practice and the contested epistemology of fact checking. Communication, culture & critique, 10(3):518537. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Lovisa Hagström, Sara Vera Marjanovic, Haeun Yu, Arnav Arora, Christina Lioma, Maria Maistro, Pepa Atanasova, and Isabelle Augenstein. 2024. Reality Check on Context Utilisation for RetrievalAugmented Generation. Preprint, arXiv:2412.17031. Pengcheng He, Jianfeng Gao, and Weizhu Chen. 2023. Debertav3: Improving deberta using electra-style pretraining with gradient-disentangled embedding sharing. Preprint, arXiv:2111.09543. Andrey Malinin and Mark J. F. Gales. 2021. Uncertainty Estimation in Autoregressive Structured Prediction. In Proceedings of the 9th International Conference on Learning Representations (ICLR 2021). Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021. Measuring massive multitask language understanding. In International Conference on Learning Representations. Ziwei Ji, Lei Yu, Yeskendir Koishekenov, Yejin Bang, Anthony Hartshorn, Alan Schelten, Cheng Zhang, Pascale Fung, and Nicola Cancedda. 2025. Calibrating Verbal Uncertainty as Linear Feature to Reduce Hallucinations. arXiv preprint arXiv:2503.14477. Shailza Jolly, Pepa Atanasova, and Isabelle Augenstein. 2022. Generating fluent fact checking explanations with unsupervised post-editing. Information, 13(10). Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli Language Models Tran-Johnson, et al. 2022. (Mostly) Know What They Know. arXiv preprint arXiv:2207.05221. Maurice Kendall and B. Babington Smith. 1939. The problem of rankings. The annals of mathematical statistics, 10(3):275287. Sunnie S. Y. Kim, Q. Vera Liao, Mihaela Vorvoreanu, Stephanie Ballard, and Jennifer Wortman Vaughan. 2024. \"im not sure, but...\": Examining the impact of large language models uncertainty expression on user reliance and trust. In Proceedings of the 2024 ACM Conference on Fairness, Accountability, and Transparency, FAccT 24, page 822835, New York, NY, USA. Association for Computing Machinery. Neema Kotonya and Francesca Toni. 2020. Explainable Automated Fact-Checking: Survey. arXiv preprint. ArXiv:2011.03870 [cs]. Lorenz Kuhn, Yarin Gal, and Sebastian Farquhar. 2023. Semantic Uncertainty: Linguistic Invariances for Uncertainty Estimation in Natural Language Generation. arXiv preprint arXiv:2302.09664. Chin-Yew Lin. 2004. ROUGE: package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 7481, Barcelona, Spain. Association for Computational Linguistics. Stephanie C. Lin, Jacob Hilton, and Owain Evans. 2022. Teaching Models to Express Their Uncertainty in Words. Transactions on Machine Learning Research. https://openreview.net/forum? id=8s8K2UZGTZ. Dawn Liu, Marie Juanchich, Miroslav Sirota, and Sheina Orbell. 2020. The Intuitive Use of Contextual Information in Decisions Made with Verbal and Numerical Quantifiers. Quarterly Journal of Experimental Psychology, 73(4):481494. Ana Marasovic, Iz Beltagy, Doug Downey, and Matthew Peters. 2022. Few-shot self-rationalization with natural language prompts. In Findings of the Association for Computational Linguistics: NAACL 2022, pages 410424, Seattle, United States. Association for Computational Linguistics. Nicholas Micallef, Vivienne Armacost, Nasir Memon, and Sameer Patil. 2022. True or false: Studying the work practices of professional fact-checkers. Proc. ACM Hum.-Comput. Interact., 6(CSCW1). Sabrina J. Mielke, Arthur Szlam, Emily Dinan, and YLan Boureau. 2022. Reducing conversational agents overconfidence through linguistic calibration. Transactions of the Association for Computational Linguistics, 10:857872. Alexander Nikitin, Jannik Kossen, Yarin Gal, and Pekka Marttinen. 2024. Kernel language entropy: Finegrained uncertainty quantification for llms from semantic similarities. 37:89018929. OpenAI Team. 2024. Gpt-4o system card. Preprint, arXiv:2410.21276. Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL 02, page 311318, USA. Association for Computational Linguistics. Qwen Team. 2024. Qwen2.5: party of foundation models. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. 2020. Exploring the Limits of Transfer Learning with Unified Text-to-Text Transformer. Journal of machine learning research, 21(140):167. Sagnik Ray Choudhury, Pepa Atanasova, and Isabelle Augenstein. 2023. Explaining interactions between text spans. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 1270912730, Singapore. Association for Computational Linguistics. Mourad Sarrouti, Asma Ben Abacha, Yassine Mrabet, and Dina Demner-Fushman. 2021. Evidence-based fact-checking of health-related claims. In Findings of the Association for Computational Linguistics: EMNLP 2021, pages 34993512, Punta Cana, Dominican Republic. Association for Computational Linguistics. Michael Schlichtkrull, Nedjma Ousidhoum, and Andreas Vlachos. 2023. The intended uses of automated fact-checking artefacts: Why, how and who. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 86188642, Singapore. Association for Computational Linguistics. Vera Schmitt, Luis-Felipe Villa-Arenas, Nils Feldhus, Joachim Meyer, Robert P. Spang, and Sebastian Möller. 2024. The role of explainability in collaborative human-ai disinformation detection. In Proceedings of the 2024 ACM Conference on Fairness, Accountability, and Transparency, FAccT 24, page 21572174, New York, NY, USA. Association for Computing Machinery. Noah Siegel, Oana-Maria Camburu, Nicolas Heess, and Maria Perez-Ortiz. 2024. The probabilities also matter: more faithful metric for faithfulness of freetext explanations in large language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 530546, Bangkok, Thailand. Association for Computational Linguistics. Noah Siegel, Nicolas Heess, Maria Perez-Ortiz, and Oana-Maria Camburu. 2025. Faithfulness of LLM Self-Explanations for Commonsense Tasks: Larger Is Better, and Instruction-Tuning Allows TradeOffs but Not Pareto Dominance. arXiv preprint arXiv:2503.13445. Dominik Stammbach and Elliott Ash. 2020. e-FEVER: Explanations and Summaries for Automated Fact Checking. Proceedings of the 2020 Truth and Trust Online (TTO 2020), pages 3243. Mark Steyvers, Heliodoro Tejeda, Aakriti Kumar, Catarina Belem, Sheer Karny, Xinyue Hu, Lukas Mayer, and Padhraic Smyth. 2025. What large language models know and what people think they know. Nature Machine Intelligence, pages 111. Jingyi Sun, Pepa Atanasova, and Isabelle Augenstein. 2025. Evaluating input feature explanations through unified diagnostic evaluation framework. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 1055910577, Albuquerque, New Mexico. Association for Computational Linguistics. Sree Harsha Tanneru, Chirag Agarwal, and Himabindu Lakkaraju. 2024. Quantifying Uncertainty in Natural Language Explanations of Large Language Models. In Proceedings of The 27th International Conference on Artificial Intelligence and Statistics, volume 238 of Proceedings of Machine Learning Research, pages 10721080. PMLR. Robert Tate. 1954. Correlation between Discrete and Continuous Variable. Point-Biserial Correlation. The Annals of mathematical statistics, 25(3):603 607. Team OLMo, Pete Walsh, Luca Soldaini, Dirk Groeneveld, Kyle Lo, Shane Arora, Akshita Bhagia, Yuling Gu, Shengyi Huang, Matt Jordan, Nathan Lambert, Dustin Schwenk, Oyvind Tafjord, Taira Anderson, David Atkinson, Faeze Brahman, Christopher Clark, Pradeep Dasigi, Nouha Dziri, Michal Guerquin, Hamish Ivison, Pang Wei Koh, Jiacheng Liu, Saumya Malik, William Merrill, Lester James V. Miranda, Jacob Morrison, Tyler Murray, Crystal Nam, Valentina Pyatkin, Aman Rangapur, Michael Schmitz, Sam Skjonsberg, David Wadden, Christopher Wilhelm, Michael Wilson, Luke Zettlemoyer, Ali Farhadi, Noah A. Smith, and Hannaneh Hajishirzi. 2024. 2 olmo 2 furious. Katherine Tian, Eric Mitchell, Allan Zhou, Archit Sharma, Rafael Rafailov, Huaxiu Yao, Chelsea Finn, and Christopher Manning. 2023. Just ask for calibration: Strategies for eliciting calibrated confidence scores from language models fine-tuned with human feedback. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 54335442, Singapore. Association for Computational Linguistics. Jasper van der Waa, Tjeerd Schoonderwoerd, JurriInaan van Diggelen, and Mark Neerincx. 2020. terpretable confidence measures for decision support systems. International Journal of Human-Computer Studies, 144:102493. Thomas S. Wallsten, David V. Budescu, Rami Zwick, and Steven M. Kemp. 1993. Preferences and Reasons for Communicating Probabilistic Information in Verbal or Numerical Terms. Bulletin of the Psychonomic Society, 31(2):135138. Yuxia Wang, Revanth Gangi Reddy, Zain Muhammad Mujahid, Arnav Arora, Aleksandr Rubashevskii, Jiahui Geng, Osama Mohammed Afzal, Liangming Pan, Nadav Borenstein, Aditya Pillai, Isabelle Augenstein, Iryna Gurevych, and Preslav Nakov. 2024. Factcheck-bench: Fine-grained evaluation benchmark for automatic fact-checkers. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 1419914230, Miami, Florida, USA. Association for Computational Linguistics. Greta Warren, Irina Shklovski, and Isabelle Augenstein. 2025. Show me the work: Fact-checkers requirements for explainable automated fact-checking. In Proceedings of the CHI Conference on Human Factors in Computing Systems, CHI 25, New York, NY, USA. Association for Computing Machinery. Yeo Wei Jie, Ranjan Satapathy, Rick Goh, and Erik Cambria. 2024. How interpretable are reasoning explanations from prompting large language models? In Findings of the Association for Computational Linguistics: NAACL 2024, pages 21482164, Mexico City, Mexico. Association for Computational Linguistics. Sarah Wiegreffe, Ana Marasovic, and Noah A. Smith. 2021. Measuring association between labels and free-text rationales. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1026610284, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics. Paul Windschitl and Gary Wells. 1996. Measuring Psychological Uncertainty: Verbal versus Numeric Methods. Journal of Experimental Psychology: Applied, 2(4):343. Miao Xiong, Zhiyuan Hu, Xinyang Lu, Yifei Li, Jie Fu, Junxian He, and Bryan Hooi. 2023. Can LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs. arXiv preprint arXiv:2306.13063. Yongjin Yang, Haneul Yoo, and Hwaran Lee. 2025. MAQA: Evaluating uncertainty quantification in LLMs regarding data uncertainty. In Findings of the Association for Computational Linguistics: NAACL 2025, pages 58465863, Albuquerque, New Mexico. Association for Computational Linguistics. Gal Yona, Roee Aharoni, and Mor Geva. 2024. Can large language models faithfully express their intrinsic uncertainty in words? In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 77527764, Miami, Florida, USA. Association for Computational Linguistics. Fengzhu Zeng and Wei Gao. 2024. JustiLM: Few-shot justification generation for explainable fact-checking of real-world claims. Transactions of the Association for Computational Linguistics, 12:334354. Caiqi Zhang, Fangyu Liu, Marco Basaldella, and Nigel Collier. 2024a. LUQ: Long-text uncertainty quantification for LLMs. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 52445262, Miami, Florida, USA. Association for Computational Linguistics. Qingru Zhang, Chandan Singh, Liyuan Liu, Xiaodong Liu, Bin Yu, Jianfeng Gao, and Tuo Zhao. 2024b. Tell your model where to attend: Post-hoc attention steering for LLMs. In Proceedings of the Twelfth International Conference on Learning Representations (ICLR 2024). Xiaoyan Zhao, Lingzhi Wang, Zhanghao Wang, Hong Cheng, Rui Zhang, and Kam-Fai Wong. 2024. PACAR: Automated Fact-Checking with Planning and Customized Action Reasoning Using Large LanIn Proceedings of the 2024 Joint guage Models. International Conference on Computational Linguistics, Language Resources and Evaluation (LRECCOLING 2024), pages 1256412573. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. 2023. Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena. Advances in Neural Information Processing Systems, 36:4659546623. Kaitlyn Zhou, Dan Jurafsky, and Tatsunori Hashimoto. 2023. Navigating the Grey Area: How Expressions of Uncertainty and Overconfidence Affect Language Models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 55065524, Singapore. Association for Computational Linguistics. Alf Zimmer. 1983. Verbal vs. Numerical Processing of Subjective Probabilities. In Advances in psychology, volume 16, pages 159182. Elsevier."
        },
        {
            "title": "A Backbone model performance on",
            "content": "public benchmarks Table 3 summarises the publicly reported five-shot results on two standard reasoning benchmarks. All figures are taken verbatim from the official model cards or accompanying technical reports. Figures are copied from the official model cards. These numbers corroborate our claim that Qwen2.5-14B-Instruct is the strongest of the three for instruction-following and reasoning. Method: Selecting attention heads to steer Following Zhang et al. (2024b), we steer only selected subset of attention heads rather than all of them, because targeted steering yields larger gains in output quality. Our selection criterion, however, differs from theirs: instead of ranking heads by their impact on task accuracy, we rank them by how strongly they affect the models predictive uncertainty during fact-checking. Concretely, for each fact-checking dataset chosen in this work(see details in 4.1), D, we draw validation subset Dd with Dd = 300 examples. For every input Dd, we compute the models baseline uncertainty score u(X) when it predicts the fact-checking label as stated in 3.2. Then, for each attention head identified by layer ℓ and index h, we zero out that head, re-run the model, and measure the absolute change in uncertainty u(X, ℓ, h) = (cid:12) (cid:12) u(X) u/o(l,h)(X)(cid:12) (cid:12). Averaging u(X, l, h) over all Dd yields single importance score for head (ℓ, h). We rank the heads by this score and keep the top heads for each dataset and each model. Note that we set = 100 in line with the recommendation of Zhang et al. (2024b) and to balance steering effectiveness against the risk of degeneration. Model Params MMLU GSM8K Qwen2.5-14B-Instruct (Qwen Team, 2024) Gemma-2-9B-IT (Gemma Team, 2024) OLMo-2-1124-13B-Instruct (Team OLMo et al., 2024) 14.7 9.0 13 79.7 71.3 67.5 90.2 68.6 54.2 Table 3: Benchmark scores on MMLU (Hendrycks et al., 2021) and GSM8K (Cobbe et al., 2021) are used to characterize instruction-following and reasoning strength. You are helpful assistant . Your task :"
        },
        {
            "title": "D Perturbation details for faithfulness",
            "content": "1. Read the claim and its two evidence passages (E1 , E2 ). 2. For each supplied span interaction , decide whether the two spans AGREE , DISAGREE , or are UNRELATED , taking the full context into account . 3. Output the span pairs exactly as given , followed by \" relation : agree disagree unrelated \". Return format : 1. \" SPAN A\" - \" SPAN B\" relation : < agree disagree unrelated > 2. ... 3. ... ( annotated example ) ### SHOT 1 Claim : [...] Evidence 1: [...] Evidence 2: [...] Span interactions ( to be labelled ): 1. \"[...]\" - \"[...]\" 2. \"[...]\" - \"[...]\" 3. \"[...]\" - \"[...]\" Expected output : 1. \"[...]\" - \"[...]\" 2. \"[...]\" - \"[...]\" 3. \"[...]\" - \"[...]\" relation : ... relation : ... relation : ... ### SHOT 2 ### SHOT 3 % omitted for brevity % omitted for brevity measurement To evaluate how faithfully each NLE reflects model uncertainty, we generate multiple counterfactuals per instance, following Atanasova et al. (2020) and Siegel et al. (2024) (see 5.1). For every input, comprising one claim and two evidence passages, we first tag part-of-speech with spaCy, then choose seven random insertion sites. At each site we insert either (i) random adjective before noun or (ii) random adverb before verb. The candidate modifiers are drawn uniformly from the full WordNet lists of adjectives and adverbs. Because we sample three random candidates for each of the four positions, this procedure yields 4 3 = 12 perturbations per instance, providing sufficient set for the subsequent Entropy-CCT evaluation, in which we check whether the NLE mentions the inserted word and correlate that mention with the uncertainty change induced by each perturbation. ( pre - filled for each new example ) Differences Between Entropy-CCT and ### NEW INSTANCE Claim : { CLAIM } Evidence 1: { E1 } Evidence 2: { E2 } Span interactions : 1. \"{ SPAN1 -A }\" - \"{ SPAN1 -B }\" 2. \"{ SPAN2 -A }\" - \"{ SPAN2 -B }\" 3. \"{ SPAN3 -A }\" - \"{ SPAN3 -B }\" Figure 3: Prompt template for span interaction relation labelling."
        },
        {
            "title": "Relation Labels to Captured Span\nInteractions",
            "content": "To identify agreements and conflicts between the claim and the two evidence passages, we use the prompt in Figure 3 to label each extracted span interaction (see 3.3)."
        },
        {
            "title": "CCT",
            "content": "In CCT test, Total Variation Distance (TVD) is computed between two probability distributions and as TVD(P, Q) = 1 Pi Qi, mea2 suring the absolute change in class-wise probabilities. We instead operate on the entropies of those distributions, yielding single-valued measure of uncertainty shift. (cid:80) Prompt template for PromptBaseline, CLUE-Span and CLUE-Span+Steering on Healthver and Druid dataset We designed two prompt templates for our experiments. The baseline prompt (Figure 4) gives the model no span interactions; instead, it must first identify the relevant agreements or conflicts and then discuss them in its explanation. In contrast, the prompt used by our CLUE framework (Figure 5) supplies the three pre-extracted span interactions (3.3). The model is explicitly instructed to base You are helpful assistant . Your tasks : 1. Determine the relationship between the claim and You are helpful assistant . Your tasks : 1. Determine the relationship between the claim and the two evidence passages . the two evidence passages . 2. Explain your prediction uncertainty by 2. Explain your prediction uncertainty by identifying the three most referring to the three span influential span interactions from Claim - Evidence interactions provided below ( Claim - Evidence 1, 1, Claim - Evidence 2, Claim - Evidence 2, and Evidence 1Evidence 2, and describing how Evidence 1 - Evidence 2) and describing how each each interaction relation interaction relation ( agree , disagree , or unrelated ) affects your ( agree , disagree , or unrelated ) affects your overall confidence . overall confidence . Return format : [ Prediction ] [ Explanation ] Return format : [ Prediction ] [ Explanation ] ### SHOT 1 Input Claim : [...] Evidence 1: [...] Evidence 2: [...] Output ### SHOT 1 Input : Claim : [...] Evidence 1: [...] Evidence 2: [...] Span interactions : [ Prediction : ...] [ Explanation : ...] 1. [...] - [...] (C - E1 ) relation : ### SHOT 2 ### SHOT 3 % omitted for brevity % omitted for brevity ### NEW INSTANCE Claim : { CLAIM } Evidence 1: { E1 } Evidence 2: { E2 } Your answer : Figure 4: Three-shot prompt for PromptBaseline (Shots 23 omitted) on the HealthVer and DRuiD datasets. its explanation on these spans, ensuring that the rationale remains grounded in the provided evidence. F.1 Prompt template for PromptBaseline To generate NLEs about model uncertainty without span-interaction guidance, we craft three-shot prompt that instructs the model to identify the interactions most likely to affect its uncertainty and to explain how these relations they represent affect it. (See Figure 4). F.2 Prompt template for CLUE-Span and CLUE-Span+Steering To generate NLEs about model uncertainty with the span-interaction guidance, we craft three-shot prompt that instructs the model to discuss how these interactions, along with the relations they represent, affect its uncertainty. (See Figure 5)."
        },
        {
            "title": "Faithfulness Scores",
            "content": "This section elaborates on the statistical evaluation of faithfulness regarding (i) recalling the definition and intuitive interpretation of the pointbiserial coefficient rpb(E.q. 9), (ii) outlining the t-test used to assess significance, (iii) reporting the faithfulness results (5.1) along with statistical results. Note that, each dataset is evaluated on = 600 12 = 7,200 perturbations with 600 instances with 12 per- [...] 2. [...] - [...] (C - E2 ) relation : [...] 3. [...] - [...] (E1 - E2 ) relation : [...] Output : [ Prediction : ...] [ Explanation : ...] ### SHOT 2 ### SHOT % omitted for brevity % omitted for brevity ### NEW INSTANCE Claim : { CLAIM } Evidence 1: { E1 } Evidence 2: { E2 } Span interactions ( pre - filled ): 1. { SPAN1 -A} - { SPAN1 -B } (C - E1 ) relation : { REL1 } 2. { SPAN2 -A} - { SPAN2 -B } (C - E2 ) relation : { REL2 } 3. { SPAN3 -A} - { SPAN3 -B } ( E1 - E2 ) relation : { REL3 } Your answer : Figure 5: Three-shot prompt for CLUE-Span and CLUE-Span+Steering (Shots 23 omitted) on the HEALTHVER and DRUID datasets. turbations each (see App. D). and (iv) demonstrating through concise numerical summaries that both CLUE-Span and CLUE-Span+Steering are significantly more faithful than the PromptBaseline. G.1 Interpreting rpb and rpb The Entropy-CCT score is the point-biserial correlation (Tate, 1954) between the absolute entropy change and the binary mention flag m. Because it is mathematically identical to Pearson computed between one continuous and one binary variable, it obeys 1 rpb 1. When rpb = 0, it means the highand low-impact perturbations are mentioned equally often. If the two strata are roughly balanced, every +0.01 in rpb increases the probability that truly uncertainty-influential token is mentioned by about one percentage point (pp). gain rpb therefore translates to an absolute improvement of rpb 100,pp in mention rate. For instance, moving from 0.08 to +0.06 is swing of 0.14, corresponding to, 14,pp. G.2 Significance testing Because the point-biserial is Pearson correlation, the familiar ttest applies: = rpb (cid:115) 2 1 r2 pb , (12) t(n2) under H0 : rpb = 0. (13) With = 7, 200 we have df = 7, 198; the critical two-sided values are > 1.96 for < 0.05 and > 2.58 for < 0.01. G.3 Faithfulness with significance results Table 4 shows the point-biserial coefficients rpb, which is our faithfulness measurement for model uncertainty(See, E.q.9), the associated statistics, and two-sided values for every modelmethod pair. Values that meet the stricter < 0.01 criterion are highlighted in bold. Across both datasets and all three backbones, the PromptBaseline exhibits negative correlations, implying an non-faithful tendency to highlight lowimpact tokens within the generation NLEs, with mean = 0.094. The prompt-only variant of our CLUE framework CLUE-Span neutralises this bias and turns the average into +0.027; three of its six coefficients are clear < 0.01, indicating modest but significant improvement regarding faithfulness. The full CLUE-Span+Steering variant pushes the mean to +0.062 and achieves < 0.01 in four of six settings. Interpreting these numbers via G.1, the switch from 0.094 to +0.062 yields absolute increase of (0.062 (0.094)) 100! !16, pp in the probability that truly influential token of uncertainty is named in the NLE, which is easily noticeable in qualitative inspection. The consistently positive, statistically significant gains therefore substantiate the claim made in the main text: CLUE produces markedly more faithful NLEs towards model uncertainty than the PromptBaseline, and the steer variant is particularly beneficial for models that initially struggle with uncertainty attribution."
        },
        {
            "title": "H Human Evaluation Details",
            "content": "H.1 Participants and Materials Participants We recruited N=12 participants from Prolific, screened to be native English speakers from Australia, Canada, Ireland, New Zealand, the United Kingdom, and the United States. The study was approved by our institutions Research Ethics Committee (reference number 504-0516/245000). Materials Forty instances (20 from DRUID, 20 from HealthVer) were selected at random for evaluation. For each instance, participants were provided with claim, two evidence documents, model verdict, model numerical certainty, and three alternative explanations (see Figure 6 in H.6). The explanations presented to participants were those generated using Qwen2.5-14b-instruct (Qwen Team, 2024) based on its automatic evaluation performance. Each participant evaluated explanations for 10 instances (5 labelled True, 5 labelled False), in addition to two attention check instances which were used to screen responses for quality. Procedure Participants read information about the study (see H.3) and provided informed consent (see H.4) before reading detailed task instructions and completing practice example of the task (see H.5). Participants then progressed through the study at their own pace. The task took approximately 20 minutues, and participants were paid 3 for their work. H.2 Human Evaluation Results H.2."
        },
        {
            "title": "Interrater agreement",
            "content": "In line with similar NLE evaluations carried out by previous studies (e.g., (Atanasova et al., 2020)), interrater agreement (Kendalls (Kendall and Smith, 1939)) was moderate to low (see Table 5). We attribute this to the relative complexity of the task and individual differences in how the information was perceived. H.3 Human Evaluation Information Screen Thank you for volunteering to participate in this study! Before you decide whether you wish to take part, please read this information screen carefully. 1. What is the project about? Our goal is to make sure that AI fact-checking systems can explain the decisions they produce in ways that are understandable and useful to people. This survey is part of project to help us understand what kinds of explanations are helpful and why. 2. What does participation entail? You are invited to help us explore what kinds of explanations work better in fact-checking. In this task you will see claims, an AI systems prediction Model Method rpb Qwen2.5-14B-Instruct OLMo-2-1124-13B-Instruct Gemma-2-9B-IT HealthVer PromptBaseline 0.028 CLUE-Span +0.006 CLUE-Span+Steering +0.033 PromptBaseline 0.100 CLUE-Span +0.005 CLUE-Span+Steering +0.020 PromptBaseline 0.105 CLUE-Span +0.007 CLUE-Span+Steering +0.021 DRUID 2.38 +0.51 +2.80 8.53 +0.42 +1.70 8.96 +0.59 +1.78 1.7 102 6.1 101 5.1 103 < 1015 6.7 101 9.0 102 < 1015 5.5 101 7.5 102 Qwen2.5-14B-Instruct OLMo-2-1124-13B-Instruct Gemma-2-9B-IT PromptBaseline 6.81 0.080 CLUE-Span +7.58 +0.089 CLUE-Span+Steering +0.102 +8.70 PromptBaseline 0.130 11.12 CLUE-Span +1.19 +0.014 CLUE-Span+Steering +0.099 +8.44 PromptBaseline 0.120 10.26 CLUE-Span +3.65 +0.043 CLUE-Span+Steering +0.098 +8.35 9.8 1012 3.4 1014 < 1015 < 1015 2.3 101 < 1015 < 1015 2.6 104 < 1015 Table 4: Detailed faithfulness evaluation results for baseline method PromptBaseline, and two variants of our CLUE framework CLUE-Span and CLUE-Span+Steering on Healthver and Druid dataset based on Qwen2.5-14BInstruct(Qwen Team (2024)), OLMo-2-1124-13B-Instruct(Team OLMo et al. (2024))and Gemma-2-9B-IT(Gemma Team (2024)). Point-biserial correlation rpb is our Entropy-CCT measurement(5.1), along with statistic and two-sided p-value for each modelmethod pair (n = 7,200, df = 7,198). Entries with < 0.01 are bold. Helpfulness Consistency Non-redundancy Coverage Overall Quality"
        },
        {
            "title": "HealthVer",
            "content": "Set Set Set Set .013 .016 .019 .027 .002 .016 .44 .005 .494 .005 .003 .017 .005 .018 .01 .079 .058 .084 .113 .158 Table 5: Interrater agreement (Kendalls W) for human evaluation about whether this claim is true or false and corresponding evidence used to make the prediction. You will also see an explanation for why the AI system is certain or uncertain about its prediction to help you decide how to interpret the true/false prediction. We ask you to evaluate the explanations along 5 different dimensions (the detailed explanation of the task is on the next page). All participants who complete the survey will receive payment of 3. There is no cost to you for participating. You may refuse to participate or discontinue your involvement at any time without penalty. 3. Source of funding This project has received funding from the ERC (European Research Council) Starting Grant on Explainable and Robust Fact Checking under grant agreement ID no. 101077481. 4. Consenting to participate in the project and withdrawing from the research You can consent to participating in this study by ticking the box on the next page of the study. Participation in the study is completely voluntary. Your decision not to consent will have no adverse consequences. Should you wish to withdraw during the experiment you can simply quit the webpage. All incomplete responses will be deleted. After you have completed the study and submitted your responses, it will no longer be possible to withdraw from the study, as your data will not be identifiable and able to linked to you. 5. Possible benefits and risks to participants By participating in this study you will be contributing to research related to understanding what kinds of explanations are useful to people who use or who are impacted by automated fact checking systems. This is long-term research project, so the benefits of the research may not be seen for several years. It is not expected that taking part will cause any risk, inconvenience or discomfort to you or others. 6. What personal data does the project process? The project does not process any personal data. in research project, you the GDPR. rights under rights are specified in the University https: 7. Participants rights under the General Data Protection Regulation (GDPR) As participant have number of Your of Copenhagens privacy policy. //informationssikkerhed.ku.dk/english/ protection-of-information-privacy/ privacy-policy/ 8. Person responsible for storing and processing of data University of Copenhagen, CVR no. 29979812, is the data controller responsible for processing data in the research project. The research project is headed by Prof. Isabelle Augenstein who can be contacted via email: augenstein@di.ku.dk, phone: <>, address: Øster Voldgade 3 1350 Copenhagen, Denmark. Greta Warren is the contact point for this project and can be contacted via email: grwa@di.ku.dk, phone: <>, address: Øster Voldgade 3, 1350 Copenhagen, Denmark. Please click Next to read more about consenting to participate in the study. H.4 Human Evaluation Consent Form We hereby request your consent for processing your data. We do so in compliance with the General Data Protection Regulation (GDPR). See the information sheet on the previous screen for more details about the project and the processing of your data. confirm that have read the information sheet and that this forms the basis on which consent to the processing of my data by the project. hereby give my consent that the University of Copenhagen may register and process my data as part of the Human-Centred Explainable Fact Checking project. understand that any data provide will be anonymous and not identifiable to me. understand that my anonymous response data will be retained by the study team. understand that after submit my responses at the end of the study, they cannot be destroyed, withdrawn, or recalled, because they cannot be linked with me. understand that anonymous data shared through publications or presentations will be accessible to researchers and members of the public anywhere in the world, not just the EU. give my consent that the anonymous data provided may be stored in database for new research projects after the end of this project. give permission for my anonymous data to be stored for possible future research related to the current study without further consent being required. understand will not be paid for any future use of my data or products derived from it. By checking this box, confirm that agree to the above and consent to take part in this study. (cid:50) consent H.5 Evaluation Task Instructions What do have to do? In this study you will see claims, an AI systems prediction about whether this claim is true or false, how certain the system is about its label, and the corresponding evidence used to make the prediction. You will also see three different explanations for why the AI system is certain or uncertain about its prediction. These explanations are intended help you decide how to interpret the true/false prediction. Your task is to evaluate the quality of the explanations provided, not the credibility of the claims and evidence. What information will be shown? You will be shown examples of claims, evidence document, verdicts and explanations. claim is some statement about the world. It may be true, false, or somewhere in between. Additional information is typically necessary to verify the truthfulness of claim - this is referred to as evidence or evidence document. An evidence document consists of one or several sentences extracted from an external source for the particular claim. In this study, you will see two evidence documents that have been retrieved for claim. These evidence documents may or may not agree with each other. understand that there are no direct benefits to me from participating in this study Based on the available evidence, verdict is reached regarding whether claim is true or false. produces: if you were to see this claim for the first time, would you find the explanation provided by the AI useful? On the next page, you will see an example of the task. H.6 Example of human evaluation set-up Here is an example of what you will see during the study. First, you will see Claim, and two pieces of Evidence, along with an AI systems predicted Verdict and the systems Certainty that its prediction is correct. The parts of the claim and evidence that are most important to the AI systems certainty are highlighted. Parts of the Claim are Red, parts of Evidence 1 are Blue, and parts of Evidence 2 are Green. Underneath, you will see three alternative explanations for the AI systems certainty, Explanation A, Explanation B, and Explanation C. The parts of each explanation that refer to the claim and evidence are colour coded in the same way (Claim = Red, Evidence 1 = Blue, Evidence 3 = Green). Your task is to read the claim, evidence, and explanations, and rank each explanation based on five properties. Now, you can try this example below! Uncertainty often arises when evaluating the claim and evidence to reach verdict. Each verdict is accompanied by numerical uncertainty score which represents the AI systems confidence that its predicted verdict is correct. You will see 3 alternative explanations for where uncertainty arises with regard to the verdict. Note that these explanations focus on the AI systems uncertainty, not the verdict itself. You are asked to evaluate the explanations according to 5 different properties. The properties are as follows: Helpfulness. The explanation contains information that is helpful for evaluating the claim and the fact check. Coverage. The explanation contains important, salient information and does not miss any important points that contribute to the fact check. Non-redundancy. The explanation does not contain any information that is redundant/repeated/not relevant to the claim and the fact check. Consistency. The explanation does not contain any pieces of information that are contradictory to the claim and the fact check. Overall Quality. Rank the explanations by their overall quality. Please rank the explanations in descending order. For example, you should rank the explanation that you think is most helpful as 1, and the explanation that you think is least helpful as 3. If two explanations appear almost identical, you can assign them the same ranking, but as general rule, you should try rank them in hierarchical order. The three explanations, Explanation A, Explanation B, and Explanation C, will appear in different order throughout the study, so you may need to pay some attention to which is which. Important: Please only consider the provided information (claim, evidence documents, and explanations) when evaluating explanations. Sometimes you will be familiar with the claim, but we ask you to approach each claim as new, whether or not you have seen it before. It doesnt matter whether you personally agree or disagree with the claim or evidence we are asking you to evaluate what the AI Figure 6: Example of human evaluation set-up. Explanation was generated using PromptBaseline, Explanation by CLUE-Span, and Explanation by CLUESpan+Steering"
        }
    ],
    "affiliations": [
        "University of Copenhagen"
    ]
}