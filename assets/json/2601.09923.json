{
    "paper_title": "CaMeLs Can Use Computers Too: System-level Security for Computer Use Agents",
    "authors": [
        "Hanna Foerster",
        "Robert Mullins",
        "Tom Blanchard",
        "Nicolas Papernot",
        "Kristina Nikolić",
        "Florian Tramèr",
        "Ilia Shumailov",
        "Cheng Zhang",
        "Yiren Zhao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "AI agents are vulnerable to prompt injection attacks, where malicious content hijacks agent behavior to steal credentials or cause financial loss. The only known robust defense is architectural isolation that strictly separates trusted task planning from untrusted environment observations. However, applying this design to Computer Use Agents (CUAs) -- systems that automate tasks by viewing screens and executing actions -- presents a fundamental challenge: current agents require continuous observation of UI state to determine each action, conflicting with the isolation required for security. We resolve this tension by demonstrating that UI workflows, while dynamic, are structurally predictable. We introduce Single-Shot Planning for CUAs, where a trusted planner generates a complete execution graph with conditional branches before any observation of potentially malicious content, providing provable control flow integrity guarantees against arbitrary instruction injections. Although this architectural isolation successfully prevents instruction injections, we show that additional measures are needed to prevent Branch Steering attacks, which manipulate UI elements to trigger unintended valid paths within the plan. We evaluate our design on OSWorld, and retain up to 57% of the performance of frontier models while improving performance for smaller open-source models by up to 19%, demonstrating that rigorous security and utility can coexist in CUAs."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 4 1 ] . [ 1 3 2 9 9 0 . 1 0 6 2 : r SYSTEM-LEVEL SECURITY FOR COMPUTER USE AGENTS CAMELS CAN USE COMPUTERS TOO:"
        },
        {
            "title": "A PREPRINT",
            "content": "Hanna Foerster, Robert Mullins University of Cambridge Tom Blanchard, Nicolas Papernot University of Toronto & Vector Institute Kristina Nikolic, Florian Tramèr ETH Zurich Ilia Shumailov, Cheng Zhang, Yiren Zhao AI Sequrity Company"
        },
        {
            "title": "ABSTRACT",
            "content": "AI agents are vulnerable to prompt injection attacks, where malicious content hijacks agent behavior to steal credentials or cause financial loss. The only known robust defense is architectural isolation that strictly separates trusted task planning from untrusted environment observations. However, applying this design to Computer Use Agents (CUAs) systems that automate tasks by viewing screens and executing actions presents fundamental challenge: current agents require continuous observation of UI state to determine each action, conflicting with the isolation required for security. We resolve this tension by demonstrating that UI workflows, while dynamic, are structurally predictable. We introduce Single-Shot Planning for CUAs, where trusted planner generates complete execution graph with conditional branches before any observation of potentially malicious content, providing provable control flow integrity guarantees against arbitrary instruction injections. Although this architectural isolation successfully prevents instruction injections, we show that additional measures are needed to prevent Branch Steering attacks, which manipulate UI elements to trigger unintended valid paths within the plan. We evaluate our design on OSWorld, and retain up to 57% of the performance of frontier models while improving performance for smaller open-source models by up to 19%, demonstrating that rigorous security and utility can coexist in CUAs."
        },
        {
            "title": "Introduction",
            "content": "Computer Use Agents (CUAs) are large Vision-Language Model (VLM)-based systems that automate tasks by perceiving user interfaces through screenshots or structured data and executing actions. These agents are increasingly entrusted with open-ended control of computer interfaces. Unlike text-based agents with well-defined tool APIs, CUAs use loosely constrained action parameters (e.g., arbitrary text inputs, pixel coordinates), creating attack surfaces where malicious instructions can be embedded directly into UI content. Real-world exploits demonstrate severe compromises such as data exfiltration or arbitrary code execution [Li et al., 2025]. Consequently, ensuring control flow integrity, and guaranteeing that the agent only executes actions prescribed by user, have become paramount security requirements. To secure agentic systems, recent literature proposed to adopt system-centric approach the Dual-LLM paradigm [Willison, 2023, Costa et al., 2025, Debenedetti et al., 2025]. This architecture structurally enforces control flow integrity by separating the agent into two distinct components: Privileged Planner (P-LLM), which generates high-level plans while remaining blind to the environment, and Quarantined Perception model (Q-VLM), which is called inside the plan and processes untrusted external data such as UI observations. While effective for agents with Equal contribution System-level Security for Computer Use Agents PREPRINT Figure 1: Dual-LLM in Computer Use Agent (CUA) setting visualized simple well defined tools, applying this pattern to CUAs presents fundamental challenge. Standard CUAs operate in data-dependent, multi-turn loop: they must perceive the UI state at every step to determine the next action. Strict isolation blocks malicious injections by preventing the planner from observing the screen, but simultaneously appears to break the visual feedback loop needed to navigate dynamic environments. We challenge this assumption and demonstrate that Dual-LLM isolation can be successfully adapted to CUAs. Our key insight is that UI workflows, while dynamic, exhibit structural predictability that enables comprehensive upfront planning. The reliance on continuous, multi-turn visual feedback is not an inherent requirement of the task space, but rather design choice of current systems. We introduce Single-Shot Planning, where the P-LLM generates complete execution graph with conditional branches before observing untrusted data. Each branch follows an Observe-VerifyAct paradigm that structures how the agent perceives, validates, and responds to environmental inputs. Single-Shot Planning provides formal Control Flow Integrity (CFI) guarantees by preventing adversaries from injecting instructions that deviate from the pre-approved plan. Our evaluation on the OSWorld benchmark, which measures task completion success rates across real desktop applications, demonstrates that this secure architecture retains significant utility. For small open-source models, we achieve up to 19% improvement over baseline performance, while for larger closed-source models, we retain up to 57% of their original success rate. While larger models experience performance gap due to the lack of runtime adaptation, utility critically scales with the planners reasoning capability and number of task attempts. This makes secure CUAs increasingly practical as foundation models advance. Finally, while our architecture guarantees control flow integrity, we show it retains critical vulnerability inherent to data-dependent systems: Branch Steering. Unlike control-flow attacks where adversaries hijack the agent to execute arbitrary commands, Branch Steering targets the data flow. Attackers can manipulate visual cues (e.g., fake buttons or concealed DOM elements) to deceive the perception model. This forces the agent to choose dangerous, yet valid, path within its pre-written plan. Although sandbox-level defenses such as ceLLMate [Meng et al., 2025] isolate the browser environment, they do not prevent this logic-level manipulation within the agents valid action space. We thus explore alternative mitigations and evaluate their tradeoff between utility and security. In summary, this work makes the following contributions: Dual-LLM Architecture for CUAs: We design the first Dual-LLM architecture adapted for Computer Use Agents, using Single-Shot Planning with an Observe-Verify-Act paradigm to provide Control Flow Integrity guarantees. Our architecture achieves up to 19% performance improvement for small open-source models and retains up to 57% of performance for larger closed-source models on the OSWorld benchmark. Branch Steering & Defenses: We identify Branch Steering as distinct data-flow threat vector, where attackers manipulate visual cues (e.g., fake buttons) to fool the agent into choosing dangerous, yet valid, path within its pre-written plan. We demonstrate its feasibility, and evaluate redundancy-based mitigation, highlighting the fundamental distinction between control-flow and data-flow security in isolated architectures. Beyond security, our architecture unlocks practical deployment advantages by strengthening open-source models where they need it most (reasoning, achieving 19% improvement) while leveraging their strong perception capabilities for privacy-preserving local deployment. Our evaluation shows that perception capabilities differ minimally between open-source and closed-source VLMs, while reasoning capabilities show larger gaps. This enables hybrid deployment strategy: proprietary models handle planning while open-source models run perception locally. This preserves user privacy, as the planner never observes sensitive environmental data like screenshots or personal UI content. Furthermore, 2 System-level Security for Computer Use Agents PREPRINT it substantially reduces costs, since the bulk of computational overhead (repeated screenshot processing) runs on efficient local models rather than expensive API calls."
        },
        {
            "title": "2.1 Computer Use Agents",
            "content": "CUA Capabilities and Architectures. Computer Use Agents (CUAs) require perception (understanding UI state through VLMs processing screenshots or structured data like DOM trees), action (executing from generalizable action space), reasoning (task decomposition and planning), and memory (execution history and task knowledge) [Qin et al., 2025]. CUA architectures can be categorized into end-to-end models where single VLM handles all capabilities [Qin et al., 2025, Wang et al., 2025, Fu et al., 2025, Ye et al., 2025] and agentic frameworks that distribute tasks across specialized modules [Agashe et al., 2025a,b, Gonzalez-Pumariega et al., 2025, Song et al., 2025] (see Section A.3 for more details). Despite architectural differences, both paradigms rely on iterative, reactive planningcontinuously adapting plans based on real-time environmental feedback. This multi-turn adaptation is fundamental to their utility but creates security vulnerabilities: because the planner must observe untrusted content to adapt, adversaries can inject malicious instructions at any step. In contrast, our Dual-LLM approach achieves isolation through single-shot planning, where the planner generates complete branching execution graph upfront without ever observing the environment, fundamentally preventing arbitrary instruction injection. CUA Attacks. Previous attacks on CUAs achieve arbitrary control over task execution by injecting malicious instructions or optimized pixels into text and visual content. Attack vectors include adversarial pop-ups [Zhang et al., 2025], malicious image patches [Aichberger et al., 2025], fine-print injections [Chen et al., 2025], and redirection attacks via trusted platforms like Reddit [Li et al., 2025], with benchmarks like OS-HARM [Kuntz et al., 2025] and WASP [Evtimov et al., 2025] demonstrating goals such as data exfiltration and arbitrary code execution. These attacks succeed because standard CUA architectures cannot architecturally separate trusted instructions from untrusted environmental observations required for navigation. We fundamentally change this threat model with our Dual-LLM framework: by isolating the privileged planner from untrusted visual content through single-shot planning, we eliminate arbitrary attacker control. Attacks can no longer inject unexpected actionsinstead, adversaries are constrained to steering the agent down pre-planned execution paths, requiring them to predict the planners logic and manipulate perceptual data to trigger valid but unintended branches. CUA Defenses. Existing defenses for CUAs lack formal security guarantees.Yang et al. [2025] use few-shot learning with malicious and benign examples to help agents recognize attacks but this requires manually curating examples for each attack type and fails on novel attacks. Hu et al. [2025] propose AgentSentinel, which monitors the agents actions in real-time and flags suspicious behavior using predefined rules and an LLM auditor but this requires manually defining what constitutes suspicious behavior and remains vulnerable if the auditor itself receives poisoned information. Both approaches rely on pattern recognition rather than providing principled architectural isolation from untrusted content, which is known to be vulnerable [Nasr et al., 2025]. CeLLMate [Meng et al., 2025] addresses browser-level security rather than CUA-specific vulnerabilities by enforcing HTTP-level sandboxing policies that restrict agent actions (e.g., limiting purchase amounts, blocking sensitive API calls), assuming the planner may be compromised and focusing on damage control through least-privilege enforcement. In contrast, our Dual-LLM architecture provides Control Flow Integrity (CFI) by preventing planner compromise entirely: the privileged planner generates the complete execution graph before observing untrusted data, architecturally preventing adversaries from injecting arbitrary actions. These defenses operate at different layers: CeLLMate provides browser-level safety net when agents are compromised, while Dual-LLM eliminates the control flow compromise itself at the architectural level for any CUA task."
        },
        {
            "title": "2.2 Dual-LLM",
            "content": "Dual-LLM and System-Centric Security. To mitigate prompt injection, the Dual-LLM paradigm decouples agent responsibilities into Privileged LLM (P-LLM) for trusted planning and Quarantined LLM (Q-LLM) for processing untrusted data. CaMeL [Debenedetti et al., 2025] compiles user queries into formal plans with explicit access constraints enforced by an interpreter, while Fides [Costa et al., 2025] calls the planner iteratively to generate singleaction executions per turn while redacting variable content from the P-LLM. While both guarantee control flow isolation for data-independent tasks, they face fundamental limitations with data-dependent workflows where the required action sequence itself is specified in untrusted data. (See Section for more details.) What Exactly Does Dual-LLM Mitigate? Dual-LLM guarantees that environment information cannot derail the control flow of task execution, ensuring only function calls planned without environment knowledge can be executed. However, it does not inherently prevent data flow attacks where malicious content influences the Q-LLMs outputs, 3 System-level Security for Computer Use Agents PREPRINT which are then used as arguments to tool calls or in branching logic. Security policies and capability-based restrictions can address data flow vulnerabilities, but these are domain and user specific, making it hard to adopt for CUA in general, while also being hard to define for computer use due to lack of semantics for the tools e.g. click function in isolation. Branch Steering Attacks. Current literature does not explicitly differentiate types of prompt injections. Existing defenses are commonly evaluated against control-flow attacks where adversaries attempt to execute entirely different actions from user-intended ones [Shi et al., 2025, Nasr et al., 2025]. Since this precise threat model does not apply to CaMeL and Fides as such attacks are structurally impossible due to the separation of planning and executionwe introduce different attack type that explicitly targets data flows within given P-LLM plan. Specifically, we define branch steering an attack, where an adversary manipulates environmental observations (e.g., via adversarial images or DOM injection) to trigger valid, but malicious, conditional branch within the P-LLMs pre-approved plan. In our setup attacks can be performed using environment-querying functions such as verify_hypothesis and find."
        },
        {
            "title": "3 Methodology",
            "content": "We address the security challenges of Computer Use Agents (CUAs) by adapting the Dual-LLM framework to enforce strict control flow isolation; this architecture separates privileged planning from untrusted perception and mitigates data flow risks through redundancy. Section discusses the design decisions in significantly more detail."
        },
        {
            "title": "3.1 Threat Model",
            "content": "The Dual-LLM setup guarantees control flow integrity specifically, that the agents execution logic remains fixed. However, we show that under certain assumptions and in dynamic environments such as CUAs, the data flow can in the worst casemanipulate the agent, effectively bypassing control flow restrictions. While these attacks typically only degrade utility, they can be specifically crafted to exploit parts of users plan to execute adversarial objectives. Attacker Capabilities and Objectives. We assume an attacker who cannot access or manipulate the P-LLM (which generates the plan), but can manipulate parts of the execution environment. This threat model aligns with realistic scenarios where attackers may control entire malicious websites or inject content within trusted sites (e.g., advertisements, forum posts, product reviews), but do not control all websites the agent interacts with or the underlying agent infrastructure. The attacker embeds malicious visual or textual cues (e.g., adversarial ads, prompt injections in DOM elements) to exploit the Q-VLM during plan execution. The goal is to cause the agent to execute adversary-chosen actions by manipulating the information the Q-VLM perceives and returns to the P-LLM. Attacker Knowledge Assumptions. The defender deploys the Dual-LLM framework for CUAs with single-shot planning, restrictions on the Q-VLM, and prompt optimization for enhancing the general utility of plans generated by the P-LLM, as described in Section B.1.2. We assume the attacker knows the defenders task, knows the functions the defender creates for the P-LLM to use, and can predict approximately the plans that the defenders P-LLM generates. These strong knowledge assumptions represent worst-case scenario that only strengthens our results: any defense effective against such well-informed attacker will be even more effective against attackers with less information. Under these assumptions, the attacker can construct targeted interventions in the environment to steer plan execution along specific branches. Any function that retrieves information from the environment serves as potential attack vector, with attack surface expanding as plan complexity increases. These assumptions are realistic in practice (see Section B.2 for detailed justification). For tractability, we focus exclusively on browser-based tasks and assume attackers can inject malicious elements through website advertisements, forum posts, or direct website ownership."
        },
        {
            "title": "3.2 Adapting Dual-LLM Architecture for Computer Use with Observe-Act-Verify Planning",
            "content": "Standard CUAs typically employ an iterative perception-action loop where the model consumes the new environment state at every turn [Qin et al., 2025]. While flexible, this exposes the agents control flow to prompt injection via visual and textual inputs (e.g., malicious pop-ups, ads, comments). To secure this, we adopt the Dual-LLM architecture, separating responsibilities into two distinct components. We formalize this section based on CaMeL, while Fides can be considered minor expansion of the setup, where the plan is decoded one instruction at time. It is worth noting that both CaMeL and Fides in the original design also had security policy components, put in place to restrict the dataflow in the system. In our work we explicitly exclude discussion of security policies since they do not change the Dual-LLM architecture and only further increase security in essence our setup can be considered worst-case CaMeL and Fides performance security-wise since we do not perform additional usecase-specific policing. 4 System-level Security for Computer Use Agents PREPRINT Overall, the system has two main components: Privileged Planner (P-LLM): secure LLM that generates complete, verified execution plan single-shot before any interaction occurs. It has no access to the live environment. Quarantined Perception (Q-VLM): An untrusted Vision-Language Model that interacts with the environment (screenshots/DOM). It executes within the P-LLMs plan but cannot alter the control flow instructions. Because the P-LLM cannot see the screen to adjust its plan dynamically, we bridge the semantic gap using comprehensive toolset allowing for conditional execution. The P-LLM compiles the user query into Python-like plan utilizing specific functions that for example summarize the screenshot state, return coordinates, or check the state of the environment with screenshot or the DOM. high-level overview of how Dual-LLM works is visualized in Figure 1, and the core of our secure execution strategy is formalized in Algorithm 2, along with example plans in Section F. To enable effective single-shot plans without intermediate reasoning, we enforce an Observe-Verify-Act methodology in the P-LLM system prompt. Observe: The plan aggregates state information (visual summaries or DOM elements). Verify: The plan uses verify_hypothesis to check logical conditions (e.g., \"Am on the login page?\"). This returns boolean or status code, allowing the pre-written plan to branch to appropriate sub-routines. Act: The plan executes interactions (clicks/types) only after verification. This structure allows the P-LLM to reason in advance by generating code that anticipates multiple failure modes and state transitions. We arrived at this methodology by analysing successful plans on the benchmarks, where initial single-shot plans frequently failed due to implicit assumptions about computer state. Successful plans consistently gathered state information first, explicitly verified assumptions, and only then acted on verified conditions. This pattern proved especially critical for handling the ambiguous initial states in benchmark tasks, where one-sentence descriptions provide no information about the current computer environment. Further implementation details are in Section B.1.2."
        },
        {
            "title": "3.3 Additional Defenses through Redundancy",
            "content": "While Dual-LLM secures control flow, the data flow remains vulnerable: compromised Q-VLM could return false coordinates or summaries to steer the plan into malicious branches, i.e., branch steering. To mitigate this, standard Dual-LLM approaches can rely on semantic security policies, which are rules constraining what data can be read and written to by subsequent tool calls in the program (e.g., blocking email to untrusted addresses). However, we do not consider such policies in our work because low-level CUA tools like click or find lack intrinsic semantic meaning suitable for policy enforcement. Extending semantic policies to CUA tools remains an open research question that would require additional infrastructure such as planner-provided intent annotations on tool calls or website-provided metadata restricting permissible actions as is proposed by Meng et al. [2025]. Instead, we implement additional verification mechanisms to detect compromised Q-VLM outputs. We treat functions that call the Q-VLM as potentially faulty nodes since these receive information from the environment and can influence plan execution. For each such function, we introduce verifier module that cross-checks outputs before they populate plan variables. We propose two complementary verification strategies, which provide empirical defense-in-depth but do not offer formal guarantees beyond the core Dual-LLM control flow protection. First, DOM Consistency (where available): the verifier uses an independent VLM to check whether the Q-VLMs visual findings align with ground-truth DOM structure, detecting hallucinations or clicks on disguised elements (e.g., transparent overlays or elements in ad sections). The DOM provides tagged subsections that can reveal mismatches not visible in the screenshot alone. Second, Multi-Modal Consensus: the same independent VLM receives both the screenshot and the first Q-VLMs output to detect visual anomalies or semantic mismatches for example, recognizing that an element labeled Login is actually pixel-injected advertisement, or that raw HTML camouflaged correctly in the DOM appears malicious visually. This defense relies on the empirically observed low transferability of adversarial examples between diverse models [Schaeffer et al., 2024]. By employing distinct verifier model (e.g., closed-source VLM different from the Q-VLM), optimization-based attacks targeting the Q-VLM are less likely to simultaneously fool the verifier, though this remains best-effort defense with potential failure modes. If any verification check detects potential attack, execution halts and the trace is returned to the user. However, because these verifiers rely on probabilistic model outputs rather than formal isolation, they remain susceptible to adversarial manipulation; we demonstrate this residual vulnerability via pixel-based attacks in the next section. System-level Security for Computer Use Agents PREPRINT"
        },
        {
            "title": "3.4 CUA Attacks",
            "content": "To empirically validate the limitations of Dual-LLM architectures and evaluate the effectiveness of our redundancy-based defenses, we develop two branch steering attack classes that target data-flow manipulation through environment-querying functions. While the Dual-LLM framework successfully prevents control-flow attacks by isolating planning from perception, and our redundancy defenses provide additional protection against data-flow manipulation, it remains critical to understand worst-case attack vectors that may still succeed. Well-designed security policies could further mitigate data-flow risks if developed for CUA tools, but policies are inherently domain-specific, and may have blind spots. Thus, these attacks serve to characterize worst-case remaining security weaknesses, identifying scenarios where even multi-layered defenses may be insufficient. We focus on attacks that exploit the find function, which retrieves UI element coordinates given an instruction text. Our attack vectors include seemingly legitimate UI elements (e.g., cookie popups embedded in ad banners) and optimized pixel perturbations, building on realistic threat models from prior work [Zhang et al., 2025, Aichberger et al., 2025, Evtimov et al., 2025]. We assume adversaries can control ad banners and focus on two widely adopted advertisement formats: (1) static Google advertisement banners, which allow advertisers to specify hyperlink and image, and (2) embedded HTML5 advertisement banners, which support full HTML snippets and thus provide larger attack surface for malicious advertisers. Prompt Injection Attack: Cookie Popup. We design proof-of-concept attack that embeds fake cookie popups within ad banners, redirecting users to attacker-controlled websites when the find function is called with instructions to locate cookie popups. This exploits fundamental CUA requirement: handling cookie consent before website interactiona predictable utility routine that must appear early in task execution in all of the plans. While specific to Chrome-based tasks in GDPR jurisdictions, the attack reveals general vulnerability: when adversaries can predict mandatory UI interactions and control UI portions, data-flow manipulation remains feasible even under Dual-LLM isolation. Multi-step and Long-range Variants. We extend this attack to multi-step execution by introducing an intermediary benign hop website that does not verify hyperlinks in its ad banners, circumventing real-world hyperlink verification on the victims homepage. The attack succeeds when tasks require visiting more than two webpages, as the plan must handle multiple cookie popups. We further introduce long-range variant that embeds the fabricated cookie popup on later webpage the adversary predicts the agent will visit, increasing stealthiness by delaying malicious redirection until deeper into task execution (see Figure 2 for complete attack illustration and Section E.1.3 for full technical details). Pixel Attack. To demonstrate the brittleness of redundancy defenses against data-flow attacks, we design gradient-based pixel perturbations that evade both screenshot-based and DOM-enhanced Q-VLM verification. We initialize the ad banner with white background and perform gradient-based optimization exclusively on the banner region. Since the final image must be in the integer domain, we apply quantization before optimization and introduce stochasticity followed by expectation over transformation steps [Athalye et al., 2018] to ensure robustness. The banner is optimized to induce malicious Q-VLM outputs when querying the find function. Rather than returning the ad banners coordinates (which would be immediately flagged), we optimize the Q-VLM to return plausible thought traces and coordinates that align with the element search instruction. This ensures the defenses described in Section 3.3 do not flag the attack: the screenshot and DOM appear benign (containing only seemingly random pixel perturbations), while the Q-VLMs returned action and thought trace appear legitimate and consistent with the task. Figure 4 shows an example attack."
        },
        {
            "title": "4 Evaluation",
            "content": "In our evaluation, we aim to answer three primary research questions: (1) Can Dual-LLM architectures retain sufficient utility for complex Computer Use tasks? (2) How does the choice of Planner LLM affect this utility? Finally, (3) how well can redundancy defenses empirically mitigate data-flow attacks while minimizing performance impact? We conduct our evaluation on OSWorld [Xie et al., 2024], benchmark for measuring agent performance on realistic computer use tasks across applications including but not limited to Chrome, LibreOffice, and GIMP. OSWorld evaluates task completion through automated verification scripts. Following standard practice in CUA benchmarking, we report pass@k metrics, which measure the probability that at least one correct solution appears in independent attempts at task. Pass@k represents an upper bound assuming oracle selection and shows the general capability to solve given task; however, it characterizes achievable performance if plan selection mechanisms can be developed for our architecture, where multiple plans can be generated upfront. We additionally report pass@1 as conservative estimate of single-attempt performance. We first measure the baseline utility of OSWorld on the CUAs integrated into our Dual-LLM setup. Subsequently, we evaluate the utility of CaMeL+OSWorld across different planner LLMs and CUAs. 6 System-level Security for Computer Use Agents PREPRINT Finally, we assess security against Branch Steering attacks under three distinct defense configurations: No Defense, DOM Consistency, and Multi-Modal Consensus, which are discussed in more detail in Sections 4.1 and 4.2. Experimental Setup. For our experiments, we integrate the OSWorld benchmark [Xie et al., 2024] into official code-bases of CaMeL2 and Fides3. We set up three different end-to-end CUAs, of which two are small open-source models UITars-1.5-7B [Seed, 2025], OpenCUA-32B [Wang et al., 2025], and one is large closed source model Claude Sonnet 4.5. Further, we test 9 different frontier LLMs as our planner models, of which three are open-source Deepseek-R1-0528, Kimi-K2-Thinking, GPT-OSS-120b, while 6 are closed source Claude Sonnet 4.5, Gemini 2.5 Pro, Gemini 3 Pro Preview, GPT-5, GPT-5.1, Grok-4."
        },
        {
            "title": "4.1 OSWorld Performance: Baseline, Dual-LLM, and Redundancy Defenses",
            "content": "Table 1: Planner Model Performance on minimal subset of OSWorld tasks with CaMeL. We pick 8 successful chrome tasks and 1 successful task from each other category from the UITars successful tasks subset, resulting in 17 tasks that we evaluate with 9 different planners using UITars as the Q-VLM. GPT-5 performs by far the best followed by Grok-4. Metric GPT-5 Grok-4 GPT-5.1 Gemini 3 Gemini 2.5 Success Rates by Application Chrome Other Apps 7/8 5/9 4/8 6/ Pro 1/8 5/9 2/8 4/9 Overall Performance 12/17 Overall 10/17 6/ 6/17 Cumulative Pass@k Performance (%) Pass@1 17.6 Pass@2 52.9 Pass@3 64.7 Pass@4 64.7 Pass@5 70.6 29.4 41.2 47.1 52.9 58.8 17.6 29.4 35.3 35.3 35.3 29.4 29.4 35.3 35.3 35.3 Pro 1/8 5/9 6/17 23.5 23.5 23.5 23.5 35.3 Claude Sonnet 4.5 GPT-OSS Kimi DeepSeek R1-0528 K2 120B 1/8 4/8 5/17 23.5 29.4 29.4 29.4 29.4 3/8 1/9 2/8 3/9 2/8 2/ 5/17 4/17 4/17 6.2 6.2 18.8 18.8 29.4 11.8 17.6 17.6 23.5 23.5 0.0 0.0 0.0 17.6 23. Our Base Utility on OSWorld. We first evaluate base utility on the OSWorld benchmark. Of the 369 total tasks, 30 are marked as automatically unmeasurable; for the remaining tasks, we report the number of successful completions for UITars, OpenCUA, and Claude Sonnet in Section C.1 Table 3. When including all 369 tasks in the success rate calculation, we achieve 24.4%, 28.7%, and 36.7% for UITars, OpenCUA, and Claude Sonnet pass@1 respectively, with maximum step count of 15. Our UITars results align with published OSWorld benchmark numbers, where UITars-1.5-7B achieves 24.51.2% with 15 steps at pass@2. Similarly, OpenCUA-32B achieves 29.71.1% with 15 steps at pass@3 in the published results. However, our Claude results (36.7%) are substantially lower than the published performance of 58.1%, which we attribute to our significantly reduced step count (15 vs. 50 steps), enforced due to time and cost limitations. OSWorld+CaMeL: Planner Utility Evaluation. Table 1 shows that GPT-5 clearly outperformed other models in constructing useful plans. While this could partially reflect our prompt engineering conducted with GPT-5 as the planner, the final system prompt is written in general manner that should be equally applicable to other models. Models that underperformed exhibit two primary failure modes. First, they fail to generate comprehensive plans that foresee many edge cases. Second, they struggle to construct effective multi-step strategies for complex website or application navigation. notable limitation, particularly for Gemini 3 Pro, Gemini 2.5 Pro, and Claude Sonnet 4.5, is their apparently low output diversity. This is critical because the planner cannot react in real-timeit must generate alternative trajectories to recover from potential failures. Although we maintained all models at their default temperature and sampling settings, these three models consistently generate highly similar plans across runs. This low diversity negatively impacts pass@k rates. Example plans from GPT-5 and Gemini 3 Pro are provided in Section F.1 and Section F.2 for comparison. OSWorld+CaMeL: Overall Base Utility Evaluation. In Table 2, we evaluate all three CUAs with GPT-5 as the fixed planner model. At pass@5, the CaMeL setup retains up to 65% of the original utility achieved without the Dual-LLM architecture. When evaluating each model on its respective successful OSWorld tasks, we observe high pass@5 success rate of 56.0% even for Claudes successful tasks, despite this set including significantly more tasks. 2https://github.com/google-research/camel-prompt-injection 3https://github.com/microsoft/fides 7 System-level Security for Computer Use Agents PREPRINT Notably, the Dual-LLM architecture reduces the importance of CUA-specific capabilities, yielding similar success rates across UITars tasks: UITars (65.0%), OpenCUA (66.7%), and Claude (68.3%). As long as element detection performs comparably across CUAs, the Dual-LLM architecture normalizes performance differences between them. This suggests that performance is primarily driven by the quality of plans generated by the P-LLM rather than the underlying CUA implementation. However, evaluating only on previously successful tasks may underestimate CaMeLs capabilities, as the strong planner could enable success on tasks that originally failed. To assess the full potential, we evaluate UITars-1.5-7B+CaMeL on all OSWorld tasks. Remarkably, we achieve 29.0% success ratehigher than UITars-1.5-7B alone (24.4%). This matches the performance of the significantly larger 32B OpenCUA model (see Section C.1, Table 3), demonstrating that strong planner can elevate smaller CUAs to compete with much larger models within secure-by-design agentic framework. OSWorld+Fides. Similarly to OSWorld+CaMeL Fides achieves pass@1 of 33.3%, and pass@5 of 66.7% with UITars on the 60 tasks UITars was successful on in OSWorld Section C.2 Table 4. OSWorld+CaMeL: Redundancy Defense Evaluation. For our strongest redundancy configuration (Multi-Modal Consensus), we employ Claude-Haiku-4.5 as the checker using the DOM representation of the environment, and GPT-5 as the checker using the raw screenshot. Both models use common prompt with strong emphasis on the specifics of our threat model: detecting potential prompt injections embedded within advertisement banners on common websites. We evaluate the success rate of flagging our cookie mimicry attacks across 10 different runs, and evaluate the utility trade-off introduced by this additional defense on the 17 tasks used for the planner utility evaluation, as reported in Section C.3 Table 5. The low observed false positive rate on benign tasks can be explained by the highly specific targeting of the redundancy defense toward prompt injections contained in website advertisement banners. Any attempt to provide broader attack space coverage ultimately results in significant utility loss, consistent with findings in Zhang et al. [2025]. Table 2: Performance comparison of Q-VLMs/CUAs with CaMeL across different task sets. For tasks that UITars1.5-7B completed successfully on OSWorld, we evaluate performance when adding CaMeL across different CUA backends including OpenCUA-32B and Claude Sonnet 4.5, finding that CUA differences become less significant with CaMeL. OpenCUA+CaMeL performance on OpenCUA tasks and Claude+CaMeL performance on Claude tasks show surprisingly similar results even with larger set sizes, underscoring the strong performance of the CaMeL+CUA setup. For All tasks, we evaluate only with UITars as the CUA at lower pass@k rate and include the 30 non-automatically evaluatable tasks that are originally set to success by default on OSWorld in the Overall row. Category UITars Tasks (n=60) OpenCUA Tasks Claude Tasks (n=109) All Tasks (n=369339) UITars OpenCUA Claude OpenCUA Chrome GIMP LibreOffice Calc LibreOffice Impress LibreOffice Writer Multi-apps OS Thunderbird VLC VS Code 6/11 (54.5%) 5/6 (83.3%) 2/3 (66.7%) 6/9 (66.7%) 3/8 (37.5%) 3/5 (60.0%) 3/4 (75.0%) 2/4 (50.0%) 1/2 (50.0%) 8/8 (100.0%) 8/11 (72.7%) 4/6 (66.7%) 2/3 (66.7%) 5/9 (55.6%) 4/8 (50.0%) 3/5 (60.0%) 3/4 (75.0%) 3/4 (75.0%) 1/2 (50.0%) 7/8 (87.5%) 8/11 (72.7%) 4/6 (66.7%) 2/3 (66.7%) 6/9 (66.7%) 3/8 (37.5%) 3/5 (60.0%) 4/4 (100.0%) 3/4 (75.0%) 1/2 (50.0%) 7/8 (87.5%) 11/13 (84.6%) 5/9 (55.6%) 2/3 (66.7%) 9/14 (64.3%) 3/7 (42.9%) 3/3 (100.0%) 4/7 (57.1%) 5/7 (71.4%) 1/4 (25.0%) 5/9 (55.6%) Claude 8/14 (57.1%) 6/11 (54.5%) 4/13 (30.8%) 7/17 (41.2%) 8/12 (66.7%) 4/7 (57.1%) 7/11 (63.6%) 7/8 (87.5%) 0/4 (0.0%) 11/12 (91.7%) UITars 10/46 (21.7%) 5/16 (31.25%) 6.46 (13%) 13/47 (27.7%) 6/22 (27.3%) 11/100 (11.0%) 8/19 (42.1%) 6/14 (42.9%) 2/17 (11.8%) 10/18 (55.6%) Overall Pass@1 Pass@2 Pass@3 Pass@4 Pass@ 39/60 (65.0%) 40/60 (66.7%) 41/60 (68.3%) 48/76 (63.2%) 62/109 (56.9%) 77/339 (22.7%)107/369 (29.0%) 25/60 (41.7%) 30/60 (50.0%) 35/60 (58.3%) 35/60 (58.3%) 39/60 (65.0%) 22/60 (36.7%) 29/60 (48.3%) 35/60 (58.3%) 37/60 (61.7%) 40/60 (66.7%) 23/60 (38.3%) 30/60 (50.0%) 33/60 (55.0%) 35/60 (58.3%) 41/60 (68.3%) 22/76 (28.9%) 32/76 (42.1%) 37/76 (48.7%) 44/76 (57.9%) 48/76 (63.2%) 31/109 (28.4%) 46/109 (42.2%) 57/109 (52.3%) 61/109 (56.0%) 62/109 (56.9%) 51/339 (15.0%) 70/339 (20.6%) 77/339 (22.7%)"
        },
        {
            "title": "4.2 CUA Attack Results",
            "content": "Cookie Popup Attack Performance. This one-step attack succeeds when no defense is employed. With the DOM Consistency defense, the attack is blocked for static Google ad banners due to the Advertisement frame being detectable in the DOM, but HTML5 ads circumvent this by replacing the advertisement frame with cookie frame in the DOM structure. With the Multi-Modal Consensus defense, distinguishing fabricated cookie banners from legitimate ones remains challenging without degrading benign task utility, consistent with weak success rates of prompt-based defenses [Evtimov et al., 2025]. The attack generalizes to any highly predictable utility routine that must appear early in task execution, such as login verification or welcome screen dismissal. Detailed defense analysis is provided in Section E.1. 8 System-level Security for Computer Use Agents PREPRINT 1 4 3 Figure 2: Cookie attack execution flow. Illustration of multi-step and long-range cookie attack: (1) legitimate homepage, (2) spoofed intermediate page, (3) trusted redirect, (4) final malicious landing page. Pixel Attack Performance. Even at our strongest redundancy defense (Multi-Modal Consensus) configured to heavily prioritize security over utility with very high false positive rates, the pixel-based attack evades both screenshotbased and DOM-enhanced Q-VLM verification. Figure 4 shows an example where the model, originally navigating www.drugs.com to Browse the Natural Product Database, is misled into clicking on specific drug page despite the planner calling find(instruction=Natural Product Database). The adversarially crafted token sequence makes the target drug appear to be natural product and perfect match for the instruction. This attack could be exploited by advertisers, such as pharmaceutical companies, to artificially increase traffic to their product pages by redirecting CUA agent traffic on commercial websites. We leave potential multi-objective and multi-Q-VLM pixel attacks to future work, as Aichberger et al. [2025] have already demonstrated techniques for constructing such attacks, and defer multi-step pixel attacks (which would simply require chaining instances of this one-step attack)."
        },
        {
            "title": "5 Discussion",
            "content": "Our investigation into adapting the Dual-LLM framework for CUAs challenges the assumption that strict system-centric security is incompatible with the dynamic nature of GUI automation as well as highlights that tasks that the CUA is expected to solve may not be as data-dependent as they seem. By converting multi-turn interactions into single-shot planning problems with redundancy, we establish new baseline for secure CUAs. However, this architecture also exposes fundamental trade-offs between plan rigidity, data dependency, and residual attack vectors. The Feasibility of Single-Shot Planning in (seemingly) Dynamic Environments. Contrary to the expectation that strict control-data isolation would sever CUA functionality, our results demonstrate that Dual-LLM architecture is viable. This finding suggests that current academic benchmarks may rely less on real-time reactivity than previously thought, or that advanced general reasoning capabilities are sufficient to navigate complex app-specific tasks via careful planning. In our architecture, performance is primarily driven by the reasoning and memory capability of the P-LLM rather than the reactive capabilities of the Q-VLM. This suggests that as reasoning models improve, the utility of single-shot secure planning is likely to increase without requiring fundamental architectural changes. TaskData Dependency. While our Observe-Verify-Act methodology allows for significant utility, we identify data dependency as the main limitation factor in the security-utility trade-off. The success of single-shot plan depends on the P-LLMs ability to anticipate start states and failure modes. Underspecified tasks (e.g., those lacking initial state descriptions) force the planner to account for exponentially more branches, degrading performance and increasing the 9 System-level Security for Computer Use Agents PREPRINT associated costs. However, we find that performance can be significantly improved through prompt tuning, suggesting that seemingly data-dependent tasks can often be reduced to data-independent ones by: (1) fine-tuning on the task distribution to increase predictability, (2) incorporating extensive state-verification logic (verify_hypothesis) into the prompt to handle runtime ambiguity, and (3) explaining general reasoning behind planning. Data-Flow Prompt Injections. While Dual-LLM effectively secures the control flow and prevents the execution of arbitrary code or unauthorized tools, it cannot fully isolate the data flow without rendering the agent useless. This gives rise to branch steering, new attack vector unique to Dual-LLM based systems. When plan relies on verification steps (e.g., verify_hypothesis), the data returned by the Q-VLM becomes control signal. An attacker cannot inject new code, but they can manipulate the environment (via adversarial visual elements) to coerce the Q-VLM into returning specific values that force the plan into attacker-chosen branches. Even with strict isolation, if the P-LLMs plan contains valid path that benefits the attacker (e.g., malicious cookie banner ad disguised as button), the system remains vulnerable to manipulation and requires additional infrastructural defenses like ceLLMate [Meng et al., 2025]. Better Benchmarks. Many of the tasks found in OSWorld are ill-defined (\"Increase font here\"), unmeasurable automatically 4, and are inherently data independent (e.g., \"get to natural products at drugs. com \", where there is no explicit button to go there and the agent either needs to explore or know what precise buttons to click). Reflection on Costs and Security Scaling. We observe that pass@5 significantly improves performance. Because CaMeL samples plans unconditionally of prior failures, it is possible to sample multiple plans in parallel or combine them into comprehensive super-plan. This indicates that CaMeL in CUAs scales favorably with model inference costs and speed. In other words, as foundational models become more efficient, high-security control flow should become increasingly accessible. In fact, when running the pass@20 we observe that Claude scales even further in performance, as we show in Figure 3, to approximately 73% by pass@20. Tasks Solved at Pass@N by Category ) % ( v k 100 80 70 60 50 40 20 10 0 Category chrome gimp libreoffice calc libreoffice impress libreoffice writer multi apps os thunderbird vlc vs code Overall 1 3 4 5 6 7 9 10 11 12 13 15 16 17 18 19 Pass@N Figure 3: Performance of Claude-based Computer Use Agent as more unbiased sampling is allowed. We observe that agent performance grows even further to almost 73%. Usage overhead. We compute the total token throughput count with CaMeL+OSWorld and Fides+OSWorld. Adding our version of CaMeL alone induces x1.88 token count increase compared with the non defended agent, and x29.60 increase for Fides+OSWorld. The latter is explained by the iterative querying of the planner model with the redacted variables during execution, which introduces an approximate of max_steps more tokens. This token count increase, though relatively low for CaMeL+OSWorld alone, increases significantly with the added defenses, up to x6.57 more tokens are used with CaMeL+Multi-Modal Consensus. More details and ablations on different setups of defenses and on the different parts of CaMeL+OSWorld can be found in Section D. 430 tasks in OSWorld, e.g., https://github.com/xlang-ai/OSWorld/blob/cbc3b590ff7573034e5614ba74a4b1483a0e75b5/ evaluation_examples/examples/chrome/ae78f875-5b98-4907-bbb5-9c737fc68c03.json System-level Security for Computer Use Agents PREPRINT"
        },
        {
            "title": "6 Conclusion",
            "content": "Our work has successfully challenged core assumption in AI security: that robust, system-centric defenses are incompatible with the dynamic, data-dependent nature of Computer Use Agents. By adapting the Dual-LLM framework with the Observe-Verify-Act methodology, we have established new and surprisingly effective baseline for secure CUA operation. Our results demonstrate that single-shot planning preserves significant utility, improving up to 19% utility with smaller open-source models (pass@3) and retaining up to 57% performance with larger closed-source models (pass@5) on CUA tasks, empirically validating the viability of secure CUA deployment. Importantly, this approach shifts the primary driver of success to the Privileged Planners reasoning capabilities, meaning future utility improvements can leverage advances in foundational LLM capabilities without compromising the security architecture. While Dual-LLM successfully isolates control flow, it remains vulnerable to our new branch steering attacks, where malicious environment coerces the Quarantined Perception model into returning crafted data that forces execution into attacker-chosen paths within legitimate plan. Even with additional defenses, residual vulnerabilities persist, as highlighted by the practical threat of our cookie attack."
        },
        {
            "title": "Contributions",
            "content": "Tom, Ilia, and Hanna developed the core ideas and iterated on the core features. Hanna built and evaluated CaMeL+OSWorld, Tom built and evaluated Fides+OSWorld. Hanna developed necessary functions to convert CUA into Dual-LLM setup and developed Observe-Verify-Act methodology and prompt optimized to achieve utility in the Dual-LLM+CUA setup. Tom and Hanna developed and optimized redundancy defenses together. Tom built most attacks, Kristina helped to build the pixel attack. Ilia, Hanna, and Tom wrote most of the paper. AI Sequrity Company team helped advise, design, and independently confirm findings in the paper. Robert Mullins, Yiren Zhao, Nicolas Papernot, and Florian Tramèr helped advise and position the project all throughout its development."
        },
        {
            "title": "References",
            "content": "Ang Li, Yin Zhou, Vethavikashini C. Raghuram, Tom Goldstein, and Micah Goldblum. Commercial LLM Agents Are Already Vulnerable to Simple Yet Dangerous Attacks. arXiv preprint arXiv:2502.08586, 2025. Simon Willison. The dual llm pattern for building ai assistants that can resist prompt injection, 2023. URL https: //simonwillison.net/2023/Apr/25/dual-llm-pattern/. Blog post. Manuel Costa, Boris Köpf, Aashish Kolluri, Andrew Paverd, Mark Russinovich, Ahmed Salem, Shruti Tople, Lukas Wutschitz, and Santiago Zanella-Béguelin. Securing ai agents with information-flow control, 2025. URL https: //arxiv.org/abs/2505.23643. Edoardo Debenedetti, Ilia Shumailov, Tianqi Fan, Jamie Hayes, Nicholas Carlini, Daniel Fabian, Christoph Kern, Chongyang Shi, Andreas Terzis, and Florian Tramèr. Defeating prompt injections by design, 2025. URL https: //arxiv.org/abs/2503.18813. Luoxi Meng, Henry Feng, Ilia Shumailov, and Earlence Fernandes. cellmate: Sandboxing browser ai agents. https: //cellmate-sandbox.github.io/, 2025. Yujia Qin, Yining Ye, Junjie Fang, Haoming Wang, Shihao Liang, Shizuo Tian, Junda Zhang, Jiahao Li, Yunxin Li, Shijue Huang, et al. Ui-tars: Pioneering automated gui interaction with native agents. arXiv preprint arXiv:2501.12326, 2025. Xinyuan Wang, Bowen Wang, Dunjie Lu, Junlin Yang, Tianbao Xie, Junli Wang, Jiaqi Deng, Xiaole Guo, Yiheng Xu, Chen Henry Wu, Zhennan Shen, Zhuokai Li, Ryan Li, Xiaochuan Li, Junda Chen, Boyuan Zheng, Peihang Li, Fangyu Lei, Ruisheng Cao, Yeqiao Fu, Dongchan Shin, Martin Shin, Jiarui Hu, Yuyan Wang, Jixuan Chen, Yuxiao Ye, Danyang Zhang, Dikang Du, Hao Hu, Huarong Chen, Zaida Zhou, Haotian Yao, Ziwei Chen, Qizheng Gu, Yipu Wang, Heng Wang, Diyi Yang, Victor Zhong, Flood Sung, Y. Charles, Zhilin Yang, and Tao Yu. Opencua: Open foundations for computer-use agents, 2025. URL https://arxiv.org/abs/2508.09123. Tianyu Fu, Anyang Su, Chenxu Zhao, Hanning Wang, Minghui Wu, Zhe Yu, Fei Hu, Mingjia Shi, Wei Dong, Jiayao Wang, et al. Mano technical report. arXiv preprint arXiv:2509.17336, 2025. 11 System-level Security for Computer Use Agents PREPRINT Jiabo Ye, Xi Zhang, Haiyang Xu, Haowei Liu, Junyang Wang, Zhaoqing Zhu, Ziwei Zheng, Feiyu Gao, Junjie Cao, Zhengxi Lu, et al. Mobile-agent-v3: Fundamental agents for gui automation. arXiv preprint arXiv:2508.15144, 2025. Saaket Agashe, Jiuzhou Han, Shuyu Gan, Jiachen Yang, Ang Li, and Xin Eric Wang. Agent S: An Open Agentic Framework that Uses Computers Like Human. In International Conference on Learning Representations (ICLR), 2025a. URL https://arxiv.org/abs/2410.08164. Saaket Agashe, Kyle Wong, Vincent Tu, Jiachen Yang, Ang Li, and Xin Eric Wang. Agent s2: compositional generalist-specialist framework for computer use agents, 2025b. URL https://arxiv.org/abs/2504.00906. Gonzalo Gonzalez-Pumariega, Vincent Tu, Chih-Lun Lee, Jiachen Yang, Ang Li, and Xin Eric Wang. The unreasonable effectiveness of scaling agents for computer use, 2025. URL https://arxiv.org/abs/2510.02250. Linxin Song, Yutong Dai, Viraj Prabhu, Jieyu Zhang, Taiwei Shi, Li Li, Junnan Li, Silvio Savarese, Zeyuan Chen, Jieyu Zhao, Ran Xu, and Caiming Xiong. Coact-1: Computer-using agents with coding as actions, 2025. URL https://arxiv.org/abs/2508.03923. Yanzhe Zhang, Tao Yu, and Diyi Yang. Attacking vision-language computer agents via pop-ups. In Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar, editors, Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 83878401, Vienna, Austria, July 2025. Association for Computational Linguistics. ISBN 979-8-89176-251-0. doi:10.18653/v1/2025.acl-long.411. URL https://aclanthology.org/2025.acl-long.411/. Lukas Aichberger, Alasdair Paren, Guohao Li, Philip Torr, Yarin Gal, and Adel Bibi. MIP against agent: Malicious image patches hijacking multimodal OS agents. In The Thirty-ninth Annual Conference on Neural Information Processing Systems, 2025. URL https://openreview.net/forum?id=ToNRHqX6xq. Chaoran Chen, Zhiping Zhang, Bingcan Guo, Shang Ma, Ibrahim Khalilov, Simret Gebreegziabher, Yanfang Ye, Ziang Xiao, Yaxing Yao, Tianshi Li, et al. The obvious invisible threat: Llm-powered gui agents vulnerability to fine-print injections. arXiv preprint arXiv:2504.11281, 2025. Thomas Kuntz, Agatha Duzan, Hao Zhao, Francesco Croce, Zico Kolter, Nicolas Flammarion, and Maksym Andriushchenko. OS-harm: benchmark for measuring safety of computer use agents. In ICML 2025 Workshop on Computer Use Agents, 2025. URL https://openreview.net/forum?id=kNHA0QCSQa. Ivan Evtimov, Arman Zharmagambetov, Aaron Grattafiori, Chuan Guo, and Kamalika Chaudhuri. WASP: Benchmarking web agent security against prompt injection attacks. arXiv preprint arXiv:2504.18575, 2025. Pei Yang, Hai Ci, and Mike Zheng Shou. In-context defense in computer agents: An empirical study. arXiv preprint arXiv:2503.09241, 2025. Haitao Hu, Peng Chen, Yanpeng Zhao, and Yuqi Chen. Agentsentinel: An end-to-end and real-time security defense framework for computer-use agents. In Proceedings of the 2025 ACM SIGSAC Conference on Computer and Communications Security, CCS 25, page 35353549, New York, NY, USA, 2025. Association for Computing Machinery. ISBN 9798400715259. doi:10.1145/3719027.3765064. URL https://doi.org/10.1145/3719027. 3765064. Milad Nasr, Nicholas Carlini, Chawin Sitawarin, Sander V. Schulhoff, Jamie Hayes, Michael Ilie, Juliette Pluto, Shuang Song, Harsh Chaudhari, Ilia Shumailov, Abhradeep Thakurta, Kai Yuanqing Xiao, Andreas Terzis, and Florian Tramèr. The attacker moves second: Stronger adaptive attacks bypass defenses against llm jailbreaks and prompt injections, 2025. URL https://arxiv.org/abs/2510.09023. Chongyang Shi, Sharon Lin, Shuang Song, Jamie Hayes, Ilia Shumailov, Itay Yona, Juliette Pluto, Aneesh Pappu, Christopher A. Choquette-Choo, Milad Nasr, Chawin Sitawarin, Gena Gibson, Andreas Terzis, and John \"Four\" Flynn. Lessons from defending gemini against indirect prompt injections, 2025. URL https://arxiv.org/abs/ 2505.14534. Rylan Schaeffer, Dan Valentine, Luke Bailey, James Chua, Cristóbal Eyzaguirre, Zane Durante, Joe Benton, Brando Miranda, Henry Sleight, John Hughes, Rajashree Agrawal, Mrinank Sharma, Scott Emmons, Sanmi Koyejo, and Ethan Perez. Failures to find transferable image jailbreaks between vision-language models, 2024. URL https://arxiv.org/abs/2407.15211. Anish Athalye, Logan Engstrom, Andrew Ilyas, and Kevin Kwok. Synthesizing robust adversarial examples. In International conference on machine learning, pages 284293. PMLR, 2018. Tianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, Toh Hua, Zhoujun Cheng, Dongchan Shin, Fangyu Lei, et al. Osworld: Benchmarking multimodal agents for open-ended tasks in real computer environments. Advances in Neural Information Processing Systems, 37:5204052094, 2024. ByteDance Seed. Ui-tars-1.5. https://seed-tars.com/1.5, 2025. 12 System-level Security for Computer Use Agents PREPRINT Fangzhou Wu, Ethan Cecchetti, and Chaowei Xiao. System-level defense against indirect prompt injection attacks: An information flow control perspective. arXiv preprint arXiv:2409.19091, 2024. Yuhao Wu, Franziska Roesner, Tadayoshi Kohno, Ning Zhang, and Umar Iqbal. Isolategpt: An execution isoIn Proceedings of the Network and Distributed System Selation architecture for llm-based agentic systems. curity (NDSS) Symposium. Internet Society, 2025. URL https://www.ndss-symposium.org/ndss-paper/ isolategpt-an-execution-isolation-architecture-for-llm-based-agentic-systems/. Peter Yong Zhong, Siyuan Chen, Ruiqi Wang, McKenna McCall, Ben Titzer, Heather Miller, and Phillip Gibbons. Rtbas: Defending llm agents against prompt injection and privacy leakage. arXiv preprint arXiv:2502.08966, 2025. Sahar Abdelnabi, Amr Gomaa, Per Ola Kristensson, and Reza Shokri. Firewalls to secure dynamic llm agentic networks. arXiv preprint arXiv:2502.01822, 2025. Sahra Ghalebikesabi, Eugene Bagdasaryan, Ren Yi, Itay Yona, Ilia Shumailov, Aneesh Pappu, Chongyang Shi, Laura Weidinger, Robert Stanforth, Leonard Berrada, Pushmeet Kohli, Po-Sen Huang, and Borja Balle. Operationalizing contextual integrity in privacy-conscious assistants, 2024. URL https://arxiv.org/abs/2408.02373. Kuang-Hua Huang and Jacob A. Abraham. Algorithm-based fault tolerance for matrix operations. IEEE Transactions on Computers, C-33(6):518528, 1984. doi:10.1109/TC.1984.1676475. Banerjee. Concurrent fault diagnosis in multiple processor systems. In Proc. 16th Symp. Fault Tolerant Comput, pages 298303, 1986. VS Sukumaran Nair. Analysis and design of algorithm-based fault-tolerant systems. University of Illinois at UrbanaChampaign, 1990. Algirdas Avizienis. The n-version approach to fault-tolerant software. IEEE Transactions on Software Engineering, 11 (12):14911501, 1985. Liming Chen and Algirdas Avizienis. N-version programming: fault-tolerance approach to reliability of software operation. Digest of Papers FTCS-8: Eighth Annual International Conference on Fault Tolerant Computing, 1978. Paul Ammann and John Knight. Data diversity: An approach to software fault tolerance. IEEE Transactions on Computers, 37(4):418425, 1988. 13 System-level Security for Computer Use Agents PREPRINT"
        },
        {
            "title": "A Extended Background",
            "content": "A.1 Dual-LLM Prior work shows that model level defenses alone are insufficient against prompt injection attacks, motivating systemcentric defenses that constrain the information model is allowed to see to harden the control flow [Wu et al., 2024, Willison, 2023, Wu et al., 2025, Zhong et al., 2025, Abdelnabi et al., 2025]. We assume this setting and focus on Dual-LLM-style designs. Algorithm 1 Two Flavors of Dual-LLM CaMeL (Static Planning): 1: Π P-LLM(Task, Tools) 2: for step Π do Interpreter(s) 3: if needs data then 4: Q-LLM(Data) 5: 6: 7: end for end if Fides (Iterative Planning): 8: while Task not done do 9: 10: 11: 12: 13: end while Hsaf Redact(History) at P-LLM(Hsaf e) rt Exec(at) History History (at, rt) Generate full plan Enforce policy Isolated process Hide values Next single step Save to var Dual-LLM variants and their guarantees. Dual-LLM separates control flow (P-LLM) from data flow (Q-LLM) and has been extended in two directions. CaMeL [Debenedetti et al., 2025] compiles task into typed, Python-like plan with capability metadata and an interpreter that enforces control flow and allows to add data-flow policies on tools. Fides [Costa et al., 2025] retains this dual-LLM separation but forgoes static plan in favor of writing only single line of the plan in each turn and saving outputs of tool calls, or Q-LLM calls to variables to use for later. Note that the content of these variables are redacted and cannot be seen from the viewpoint of the P-LLM. Under the condition that the Q-LLM is not given control over parts of the control flow, these systems guarantee that environment information cannot derail task execution, as only function calls that were planned to be executed without environment knowledge can be executed. In particular, there are cases of tasks where the dual-LLM pattern cannot provide solutions: data-dependent tasks where the required sequence of actions is itself specified in untrusted data. For such tasks, the P-LLM cannot determine what steps to execute without seeing the untrusted content, which would violate the isolation guarantee. This represents fundamental limitation of maintaining strict separation between planning and data processing. For data independent tasks, outputs from the Q-LLM or tool-calls can be used for adding conditional logic or loops with conditionals into the plan and dealing with unknowns in the environment. However, even in this case, dual-LLM does not necessarily provide guarantees for data flow manipulations. An attacker could inject malicious content into the environment that influences the Q-LLMs outputs when it processes that untrusted data. Since the Q-LLMs extracted values are used as arguments to tool calls and branching logic, the attacker could achieve some damage with this, even while adhering to the P-LLMs plan. CaMeL addresses data flow security through security policies and capabilities. Capabilities are metadata tags tracking each values provenance and allowed readers, enabling fine-grained restrictions on individual data items. Security policies are Python functions that check whether tool executions are permissible based on these capabilities, for instance, blocking emails sharing confidential documents with unauthorized recipients, even if those addresses were Q-LLM-extracted from prompt-injected sources. However, defining policies requires manual domain expertise with no automatic derivation method, which becomes bottleneck for dynamic environments with huge set of environment states. As most users are not experts in recognizing what policy allowances might incur which security risks, some policies need to be preset and others need to be automatically adaptive and personalized. The personalization is necessary, as users might have significant differences in preference of whether they want to share certain information or want the agent to take specific paths of execution in certain contexts [Ghalebikesabi et al., 2024]. All of this taken together means that it cannot be solely manual human task to set policies but rather requires model trained specifically for this. However, such policies may be incorrect, necessitating worst-case baseline. Therefore, when attempting to adapt Dual-LLM to CUAs we do not attempt to add security policies but rather restrict how the Q-LLM can be called and add redundancy style defenses inside the Q-LLM 14 System-level Security for Computer Use Agents PREPRINT tool call. We hope that this baseline can highlight the benefits of adding Dual-LLM framework on CUAs, while also underlining remaining difficulties that we hope future work can build upon. A.2 Redundancy style defenses ABFT is concept from multiprocessor systems which adds redundancy and error detection into system to enhance fault tolerance [Huang and Abraham, 1984, Banerjee, 1986, Nair, 1990]. Such redundancy trades off computation to add reliability, and upon error detection, selective recomputation or reconfiguration can be triggered. The core principle is N-version programming[Avizienis, 1985, Chen and Avizienis, 1978], where multiple independent implementations solve the same problem and their outputs are compared. Disagreements signal potential faults. In our setting, we treat function outputs that reveal information about the environment as potentially faulty nodes and employ redundancy through diverse modality inputs or changes in the prompt and/or different VLMs that process query. This is analogous to data diversity techniques in fault-tolerant systems [Ammann and Knight, 1988], where the same logical input is represented differently to expose errors that might be masked in single representation. A.3 How do CUAs work? Four capabilities needed for CUA performance: Perception, Action, Reasoning, Memory [Qin et al., 2025]. Perception is the component which provides the real-time understanding of the environment recognizing and interpreting the dynamic user interface. This understanding can be gained from either structured text that the API provides such as HTML, the accessibility tree or the Document Object Model (DOM) or through visual screenshot or from both. Structured text can have the caveat of verbosity and limitations depending on OS, or application, while VLM processing screenshot needs the capability of understanding elements in the environment, captioning these elements, reading text in the environment with Optical Character Recognition (OCR), understanding spatial relationships and where elements are located, capturing state transitions, and question answering capabilities. Action is the capability of predicting the best next action in an action space. The action space needs to be designed decomposing user tasks into subtasks and generalizing over components that will be needed in many of them. Reasoning consists of System 1 thinking which is the fast, and intuitive thinking needed in subtasks, as well as System 2 thinking which is the long horizon thinking, decomposing the problem into subtasks, reflecting on failures and iterative trial and error. Finally, the memory module can be divided into short term memory which captures state transitions, maintains situational awareness and execution history, and long term memory which consists of base knowledge, and knowledge gained from previous tasks and paths. Agent frameworks VS end-to-end models. CUA architectures can be broadly categorized into architectures using an agentic framework, which distributes tasks to specialized modules, and end-to-end agent models, which rely on single model for all capabilities needed to gain utility in CUA tasks Qin et al. [2025]. Agentic frameworks can have specialized VLMs for perception, action, reasoning and memory, and have seen success in benchmarks with CUAs such as Agashe et al. [2025a,b], Gonzalez-Pumariega et al. [2025], Song et al. [2025]. Notably, Agashe et al. [2025b] employ compositional framework with high-level manager which decomposes task into list of subgoals, worker that generates actions to complete subgoal in natural language, and grounding experts which receive this natural language instruction and turn them into UI actions. They employ proactive planning which reevaluates and updates the plan after every subtask. Song et al. [2025] employ one central planner module called Orchestrator, which decomposes the user task and assigns subtasks to either programmer agent or GUI operator agent. Basing off the insight that some actions can be achieved more reliably through programmatic measures, the programmer agent writes and executes python and bash scripts, whereas the GUI operator performs the conventional UI actions such as clicking. Finally, Gonzalez-Pumariega et al. [2025] recognize that paths towards solving task can branch in unpredictable ways due to delayed feedback, mistaken reasoning, and the dynamic environment and can accumulate mistakes in destructive manner. While the other two frameworks employed hierarchical planning, they only employ one end-to-end model but run three of them in parallel. Then they summarize the trajectories into narratives of how the task was solved and run Best-of-N fourth run based on the best path. However, Qin et al. [2025] or Ye et al. [2025] list adaptability to dynamic environments or unfamiliar tasks, dependence on manual optimization and expert knowledge, maintenance overhead, and disjoint learning paradigms between specialized modules as limitations. Qin et al. [2025], Wang et al. [2025], Fu et al. [2025], Ye et al. [2025] are examples of end-to-end CUA models. They similarly formulate the problem of CUA task solving into multi-step sequential planning problem, which consists of system prompt, user task given as the first user prompt, and multi-turn conversation between the VLM (assistant message) and tool call outputs returned as user message. The content of this user message consists of the tool call, tool output, and screenshot after the tool was called. The assistant message always contains the tool calls and parameters to execute next, as well as depending on the model, context thoughts as to why this was chosen. Assistant message and 15 System-level Security for Computer Use Agents PREPRINT user message interleave and create conversation history until the assistant deems the task solved. Many CUAs are specifically trained VLMs that have been trained at excelling at perception, action, reasoning, memory, with short-term memory often being tuned adhoc through conversation history length."
        },
        {
            "title": "B Extended methodology section",
            "content": "B.1 Combining Dual-LLM and CUA Algorithm 2 CaMeL Adaptation for Computer Use Agents (CUA) Require: User Task , System Prompt Sprompt (containing \"Observe-Verify-Act\" instructions), Environment Require: P-LLM (Privileged Planner), Q-VLM (Quarantined Perception), Verifier (Redundancy Check) Ensure: Execution Status (Success/Fail) 1: Define Toolset {summarize_screenshot, find, find_element_by_text, verify_hypothesis, check_done} 2: Sprompt Sprompt Instructions(\"Use Dual Finding Strategy\") 3: Π P-LLM(Sprompt, T, F) Compile user query into Python-like plan with loops/branches Phase 2: Secure Plan Execution Phase 1: Single-Shot Plan Generation 4: for instruction in Π do // Step 1: OBSERVE - Gather Environment State if calls summarize_screenshot(query) then Oraw Q-VLM(E.screenshot, query) Security Check: If Verifier(Oraw, E) = ATTACKED abort Update Plan Variables with Oraw end if // Step 2: VERIFY - Conditional Logic and State Hypothesis if calls verify_hypothesis(observation, hypothesis) then Sstatus Q-VLM(observation, hypothesis) Security Check: If Verifier(Sstatus, E) = ATTACKED abort Branch Plan Execution based on Sstatus (e.g., if FAIL then retry or explore) 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: end if // Step 3: ACT - Find Elements and Execute if calls find(desc) or find_element_by_text(desc) then Lvis Q-VLM(E.screenshot, desc) Ldom Q-VLM(E.DOM, desc) Security Check: If Verifier(Lvis Ldom, E) = ATTACKED abort if Lvis = Ldom = then Execute Action (Click/Type) on at coordinates Verify Action: Check if E.screenshot changed else 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: 26: 27: 28: 29: 30: end for 31: return Fail (if plan exhausts without success) end if if calls check_done() then end if end if if Q-VLM(E, ) = True then return Success end if"
        },
        {
            "title": "Trigger Retry Loop or Fallback Strategy",
            "content": "Visual Search Accessibility Tree Search B.1.1 From adaptive to single shot planning When we adapt CUAs to work with the Dual-LLM setup, roughly we need to separate perception from action, reasoning, and memory. Thus, we are forced to use hierarchical agent framework setup rather than only an end-to-end model. All reasoning and action planning is done by the P-LLM. An end-to-end trained CUA model forms the base for our perception module (the Q-VLM), which may be used only to retrieve information to inform conditional logic in plan or argument values for tool calls. To enhance predictability which can work in favor for utility as well as security, we expand the usual action space of GUI tool calls, with pre-defined tool calls with which the Q-VLM 16 System-level Security for Computer Use Agents PREPRINT can be called. Specifically, we have one function that summarizes the screenshot within limited amount of text (summarize_screenshot_content) based on short description of what is being looked for. Another function finds the UI elements matching given text description and returns UI coordinates as floats (find). final function checks whether task described by short text has finished and returns boolean (check_done). Further enhancement to the perception module is given by specific tool calls that parse the DOM and optionally find elements in it with the help of another Q-VLM. Some of these functions have the same functionality as Q-VLM tool, so are redundant but in this instance we use this for utility rather than security. One function returns up to certain length of the text of the elements in the accessibility tree (get_page_elements), another returns up to certain length of the text content visible in the current page (get_page_text). Then, we implement another find function but using the elements from the accessibility tree query Q-LLM for the element matching the text description given (find_element_by_text). Lastly, we add one function (verify_hypothesis) which takes text describing hypothesis of state in which the environment could be in and text with the actual observation and uses the Q-LLM to compare the two described states and output either OK, FAIL or UNKNOWN. For reasoning, the P-LLM may use pre-trained knowledge as well as knowledge given via general system prompt to then plan the full task with all possible states denoted through steps, branches, and loops in one-shot. The difficulty of maintaining utility with Dual-LLM is in not being able to update the plan depending on environment information and iterate and reflect on failures. Many end-to-end models are trained on multi-turn sequential planning setups, and even in most agentic frameworks subtask completion influences and changes the next subtasks to attempt. Needing to predict realistic possible paths for all sequential subtasks is problem that explodes exponentially in numbers of possible branches and may at first glance seem infeasible to most researchers working on CUAs. Furthermore, short term memory only exists in the form of variables in program, while long term memory is more similar to other CUAs and can consist of knowledge from model training as well as system prompt. B.1.2 Practicality of how to improve utility of single shot planning As we tradeoff lot of reasoning by design through the restriction of planning the whole agent execution single shot, we share the practical insights that we gained when building the implementation of Dual-LLM for CUAs that improved our utility and made our setup more practical. CUA is not enough, lets add the DOM back in. Many CUAs do not utilize the DOM at all due to limitations depending on application and OS and the difficulty of handling its verbosity and also parsing screenshot at the same time. However, we find that the DOM can be very valuable second source of information that can be easily added into single shot planning and can even enhance the utility of plans. Getting the page elements or the page text can provide less biased view than summary of the screenshot from the end-to-end CUA model but may at times be too verbose. For finding elements the DOM can be more precise and when enhanced with an LLM that receives query for specific element and all DOM elements it can for example also expose smaller elements less visually noticeable. However, the end-to-end trained CUA can be more dynamic and return related elements even if not an exact match, as it was trained to reason about the query it was given in the context of user interface. While this could be beneficial for utility if the jump is small enough, it can become problem when this causes an action that the plan did not foresee or foresaw only later in the plan. Adding state verification. To ensure that the CUA is not jumping ahead and outputting move that has nothing to do with the query, we add check after the find function on whether the verbalized thoughts of the CUA fit with the initial input query to the find function. This is implemented through call to another LLM with specific query prompt which asks it to find any discrepancy between instruction and CUA output. If discrepancy is found the coordinates are not returned and rather FAIL is returned. Furthermore, we also add verification after any action taken on whether the screenshot of the computer changed before or after. If there is no change in state the knowledge that subtask has not finished can be used further in the plan. Developing general process methodologies (OBSERVE, VERIFY, ACT). When trying to optimize plans to succeed on CUA tasks, we noticed that especially the first few steps need to be similar in many tasks. First, as the tasks often are only composed of one sentence that do not describe anything about how the computer state is at the beginning of the task, the CUA needs to understand where it is and where it needs to be to succeed in the task. If for example the task where to be Find the Monthly forecast for Manchester, GB for this month, then first the plan needs to check whether the browser is open already, if the browser is open it needs to check whether specific website related to weather is already open. To systematically approach this in plan we teach the P-LLM in the system prompt to always follow three step process of observing verifying and then acting. First, for observing, functions such as summarize_screenshot_content, get_page_elements, or get_page_text can be used. Second, the text output of such functions can be used to verify whether the hypothesis of which state the computer is on is correct of not, e.g., does the description for summarize_screenshot_content fit the hypothesis we are on website. Third, now depending on whether 17 System-level Security for Computer Use Agents PREPRINT the hypothesis matches or not the conditional can try to use find or find_element_by_text to either find search bar element to find website to navigate to or if already on website continue to explore its elements to find the monthly forecast of Manchester. This pattern is versatile pattern that is useful in many substeps of plan and works as long as the planner can predict all the different state hypotheses the computer could be in, on the way to finishing task. This includes many conditional branches and adding fall back options, for when the state is in different state from the main predicted path or second or third best predicted path. Finding, branching, and looping. In order to make finding elements as smooth as possible, we add into the prompt how both finding functions can be used and underline that checking whether coordinate was actually found and whether the action actually changed after the action is taken is important to move on to the next part of the plan. Since elements might not always be found at first try because of the way the query is set by the planner, we prompt the planner to be more descriptive than just one or two words when looking for UI elements. Additionally, we recommend to use both finding functions sequentially, in case one works better than the other for specific prompt. Finally, we recommend iteration over multiple possible prompts for finding elements needed for the plan in for loop, which always tries with one find function and then the other, verifies states each time and if coordinates are found the clicking is still also located in the for loop, such that the for loop can be continued in case the clicking fails. Since CaMeL does not have any branches, we recommend the usage of no_op function to iterate on in the loop if clicking task has finished early. This does not apply to Fides, as the plan is executed dynamically. Cookie handling for browser tasks. When browsing the web, many websites will first have popup about cookies or privacy consent. The buttons on the popup are not always the same, so we add into the prompt to try to find the correct button with different phrasings of the cookie popup and do iteration as described in above paragraph. general lesson on how to navigate websites and applications. Finally navigation of websites and applications are the least generalizable part, however, we still add advice on how one could generally go about navigating website, and strongly advise to explore website before going back into the general search bar and leaving website or application (which the P-LLM suggests way too often). Some of the strategies include finding an internal search bar or looking for links that may be related to what is being looked for. For example, for the task Browse the natural products database., the task starts out on website called drugs.com. Strong general prompting to stay on website that is open is then needed for the planner to not go on general search and finding different website with natural products that it knows from training like the COCONUT Natural Products Database or the ATLAS Natural Products database. Then after using the search bar in drugs.com to look for the keyword natural products, the links in the site search do not directly have any titles with the keyword. Hence, general prompting is necessary to advise that content previews of links should also be considered to understand whether they may get you to the right place. The drugs & medications link is the correct one to navigate to in this instance. On this page now, there are links to browse drugs A-Z, Browse drugs by category and Popular Drug Searches. And finally under Browse drugs by category there is link to Natural Products. general lesson from understanding such benchmark browsing task is that websites are built in such complex way that browsing them is non-trivial and even humans might need many steps of iteration on failure to get to the right page. It is no wonder then that it is hard for P-LLM to predict all of the clicks and searches that need to be done to land on the right page. The depth of steps to navigate an environment is not always clear and predictable in advance. We only add some more general advice such as how to scroll page and systematically recheck if relevant elements have appeared, to think out of the box and in meta categories and sub categories when prompting to find elements, how menu bars could look like, and to assume that navigation requires going through wider categories and closing into sub categories of the actual element needed. B.1.3 Ablation: How to make Fides work In Fides, the planner is called iteratively to generate each next action to be executed on the environment. Since outputs from those functions are redacted as variables, the planner can never see any information about the environment. This framework is then different from the CaMeL-style one-shot plan prediction in that it does not enable direct conditionals on the variables output by previous function calls. For instance, the boolean output by the verify_hypothesis function is fully redacted, and thus the planner can not go on different branch of execution depending on the response from this function. This core difference limits the execution to simple static plan of execution, which we earlier indicated has very low utility on CUAs. To circumvent it, we choose to relax the process of variable redaction under specified conditions, and we call this new paradigm Fides+CUA. We relax Fides for CUA by allowing the planner to check boolean variables from our environment functions. The main potential security implication for this relaxation is that it enables the planner to ultimately get all environment information through infinite repeated tools calls on the same redacted variable. This is security breach in particular because the planner executes actions iteratively, and thus execution termination cannot be guaranteed comparatively to fixed-length plan like output by the CaMeL planner. We add several arguments to ensure minimal environment System-level Security for Computer Use Agents PREPRINT information gain (similar to those necessary in CaMeL for sufficient utility). Those arguments from Fides+CUA are: max_steps the maximum number of GUI tool calls that are not Q-VLM functions (like summarize_screenshot_content for instance), max_turn the maximum number of any function calls, and max_variable_reuse the maximum number of time the planner can reuse the same redacted variable of an earlier tool call. This three arguments enable both forced task termination and limited environment information gain from the planner. In practice max_steps=15, max_turn=70, and max_variable_reuse=5. B.2 Dual-LLM Attack Further Details B.2.1 Threat Model Assumption Justification We provide detailed justification for the three attacker knowledge assumptions in our threat model. Assumption 1: Task prediction. Task prediction is feasible when attackers target specific websites they own or where they can place advertisements. Each website supports finite set of user tasks, and attackers may even have access to analytics showing common navigation patterns. Assumption 2: Function prediction. Function prediction is plausible within known task domains; for CUAs, the space of useful environment information retrieval functions is limited, and tool call functions (e.g., click, type) follow relatively standardized conventions. Assumption 3: Plan pattern prediction. Plan pattern prediction is supported by our evaluation showing that many planners exhibit similar problem-solving patterns, which become more predictable with descriptive system prompts that provide explicit planning guidance. B.2.2 Positioning relative to prior work. Similar to Zhang et al. [2025], we embed our attack into seemingly legitimate UI pop-ups, but we extend the realism by specifically targeting ad banners as vectors for fake pop-ups, thereby mimicking the realistic threat models proposed by Aichberger et al. [2025], Evtimov et al. [2025]. These attacks differ from other potential OS-level entry points, such as those presented in Chen et al. [2025] which insert malicious links or toggle wraps directly into the UI, or the website prompt injection attacks in Kuntz et al. [2025] which embed adversarial text directly into website layouts. Our pixel-based attack can be viewed as specific instantiation of the image-based attacks presented in Aichberger et al. [2025], as our adversarial images are optimized toward single objective and initialized from simple white background rather than from synthetic images."
        },
        {
            "title": "C Further Utility Measurements on OSWorld",
            "content": "C.1 Base Utility on OSWorld C.2 Utility of UITars on Fides C.3 Redundancy Defense Evaluation Details on the false positive rates of redundancy defenses. more detailed analysis of the reasoning traces of the redundancy defenses false positive reveals that all false positives are wrong assumption of fake cookie banners present in the DOM or the screenshot, despite those banners being legitimate. This underlines further the difficulty of tuning these types of defenses, even if the case of the very specific threat model of potentially adversarially controlled advertisement banners in website. To be noted that the false positive rate of 0/9 for non-chrome tasks is also due to this tuning of the defense towards this threat model: when the task is not related to web navigation, the defense does not consider it as harmful. Additionally, the equal number of false positives between level 2 and 3 of redundancy can be explained by the over representation of false positive of the DOM Consistency module. The screenshot-based checker seems to be much better at distinguishing between legitimate cookie banner and fake ones embedded in ad banners, probably because the screenshot itself carries more information than the accessibility tree. Details on the true positive rate for the cookie mimicry attack. Further analysis on the attack reveals that the attack is 100% successful when the cookie snippet is very similar to the sample presented in Section F.3. Failures of the attacks are due to variations of the generated plan to handle cookies, using for instance the function find_element_by_text with specific elements as input like buttons. Indeed, in the case of standard google advertisement banner, the attacker does not have access to writing html code, and thus to write fake buttons, which means that find_element_by_text will not return coordinates for supposed cookie banner, and the attack will fail. We state again that we evaluate this google System-level Security for Computer Use Agents PREPRINT Table 3: OSWorld task distribution. Claude Sonnet 4.5, OpenCUA-32B, and UITars-1.5-7B denote the successful tasks (excluding those that are denoted infeasible to evaluate) for these models in each category when restricting the models to at most 15 steps, history of 15 and maximal output token length for the VLM of 4096 (pass@1). Only for OpenCUA we cut off the history at 5 due to slow throughput. To calculate the overall success rate for these three models, we add back the 30 tasks that are defined as not automatically evaluatable and thus by default set to success in the OSWorld benchmark. While our success rates for OpenCUA and UITars are similar to those denoted in the OSWorld benchmark, the success rate for Claude is significantly lower, which we attribute to the much lower step size that we chose (15 vs. 50)."
        },
        {
            "title": "Category",
            "content": "All Not Infeas. Claude OpenCUA UITars Chrome GIMP LibreOffice Calc LibreOffice Impress LibreOffice Writer Multi-apps OS Thunderbird VLC VS Code"
        },
        {
            "title": "Total",
            "content": "Success Rate (%) 46 26 47 47 23 101 24 15 17 23 369 43 16 46 47 22 100 19 14 14 18 339 14 11 13 17 12 7 11 8 4 109 37.67 13 9 3 14 7 3 7 7 4 9 76 11 6 3 9 8 3 4 4 2 8 28.72 24.39 Table 4: Performance comparison of UITars on Fides up to Pass@5. Category Chrome GIMP LibreOffice Calc LibreOffice Impress LibreOffice Writer Multi-apps OS Thunderbird VLC VS Code Overall Pass@1 Pass@2 Pass@3 Pass@4 Pass@5 UITars 7/11 (63.64%) 2/6 (33.33%) 3/3 (100.00%) 6/9 (66.67%) 5/8 (62.5%) 3/5 (60.0%) 3/4 (75.0%) 2/4 (50.0%) 1/2 (50.0%) 8/8 (100.0%) 40/60 (66.67%) 20/60 (33.33%) 28/60 (46.67%) 35/60 (58.3%) 37/60 (61.67%) 40/60 (66.67%) advertisement banner against the redundancy level 1. This failure case also shows that tighter control over the DOM returned by the OS is very beneficial to defending CUA Agents against prompt injections, because it provides tighter control over the data flow (searching for buttons only vs. searching the whole screenshot and/or DOM)."
        },
        {
            "title": "D Token counts comparison between defenses",
            "content": "The prices were computed following OpenAI Platform5 and Anthropic Platform6 pricing per token (input/output). As per these experiments, we use GPT-5 for the planner model and the second checker model with the screenshot for Multi-Modal Consensus, we use Claude Haiku 4.5 for the first checker model with the DOM used in DOM Consistency 5https://platform.openai.com/docs/pricing 6https://platform.claude.com/docs/en/about-claude/pricing 20 System-level Security for Computer Use Agents PREPRINT Table 5: Evaluation of the efficiency of redundancy defenses. We first evaluate the false positive rate of each redundancy defense level with set of benign tasks, and evaluate their efficiency against our two realistic cookie mimicry attacks: the standard google ad attack (evaluated against DOM Consistency), and the more advanced HTML5 type attack (evaluated against Multi-Modal Consensus). See Section for full explanation of the reason of evaluating the google ad cookie attack with DOM Consistency defense and the HTML5 cookie attack on the Multi-Modal Consensus defense. The redundancy setup was applied to CaMeL+OSWorld. Metric Redundancy Levels DOM Consistency Multi-Modal Consensus False Positive Rates by Application Chrome Other Apps 3/8 (37.50%) 0/9 (0.00%) Overall False Positive Rate 3/8 (37.50%) 0/9 (0.00%) Overall 3/17 (17.65%) 3/17 (17.65%) Cumulative Pass@k False Positive Rate for Chrome (%) Pass@1 Pass@2 Pass@3 Pass@4 Pass@ 0/8 (0.00%) 2/8 (25.00%) 3/8 (37.50%) 3/8 (37.50%) 3/8 (37.50%) 2/8 (25.00%) 2/8 (25.00%) 2/8 (25.00%) 2/8 (25.00%) 3/8 (37.50%) True positive rate on the cookie mimicry attack for 10 seeds (%) 6/10 (60%) 5/10 (50%) Table 6: Token count and related prices for different setups of defenses. Realized on subset of 17 tasks of OSWorld. The evaluation was done with Pass@5 with defenses. Defense Input Tokens Output Tokens Total Cost ($) No defense CaMeL Fides CaMeL+DOM Consistency CaMeL+Multi-Modal Consensus 1,797,736 2,950,253 51,724,263 8,437,603 10,926,601 13,120 456,105 1,874,795 605,656 982, 0.00 5.40 76.07 11.57 18.37 & Multi-Modal Consensus. Additionally, we use UITars for the main QVLM model, since the model is deployed locally, we do not count it in the total costs, hence total cost of 0 for No defense. Table 7: Token count and related prices for different elements of the agent. Realized on subset of 17 tasks of OSWorld. The QVLM functions represent all functions inside of generated plan that are using the QVLM. The tokens and related costs are evaluated on Pass@5 run using CaMeL. Element Input Tokens Output Tokens Total Cost ($) Planner QVLM functions DOM Consistency Multi-Modal Consensus 618,399 2,304,776 5,433,932 7,892,323 313,061 133,784 147,466 520,256 3.90 1.50 6.17 12.97 Observing Table 6 and Table 7, other than the token count explosion induced by OSWorld+Fides, we can make the following 3 observations: Both type of defense (CaMeL, Fides) generate proportionally much higher output than input token count increase. This can be explained first by the necessary output formatting for further processing in the plan and second because of environment state describing functions like summarize_screenshot_content. Redundancy defenses are responsible for the largest increase in token count and cost. They indeed require very detailed system prompt to enable higher distinguishability between real and spoofed ad banners, as well as sota models. 21 System-level Security for Computer Use Agents PREPRINT The planner model takes most of the token count cost for OSWorld+CaMeL without redundancy defenses. This imbalance stems from the very high number of output tokens compared with all QVLM functions combined (x3 approximatively), which can itself be explained by the necessary long plans to encompass all of the required possibilities of OS navigation when trying to fulfill the user task."
        },
        {
            "title": "E Further Details on Attack Implementations",
            "content": "This section provides technical details on our attack implementations that complement the overview in Section 4.2. E.1 Cookie Attack Technical Details E.1.1 Basic Cookie Attack Mechanism Exploiting design requirements: The attack exploits fundamental design requirement of defended CUAs: agents must handle cookie popups and advertisements before interacting with websites. An example Python code snippet demonstrating this requirement is provided in Section F.3. The attack generalizes to target any highly predictable, common utility routine that must appear early in task execution, such as cookie handling, login verification, or welcome screen dismissal. Attack vector and targets: We embed fake cookie popups within ad banners for both advertisement types. When the find function is called with instructions to locate cookie popups, the Q-VLM clicks the fabricated popup, redirecting users to an attacker-controlled webpageeither mimicking trusted website or directing to the attackers malicious site (implemented as spoofed.html, shown in Figure 2, top right). While other attacks such as denial of service are possible, we consider redirection to unknown websites as the most harmful outcome in this threat model. E.1.2 One-Step Attack and Defense Levels Using crafted HTML code for cookie popup, we demonstrate that one-step attack can mislead the Q-VLM to output coordinates corresponding to the ad banner, redirecting to an adversarially controlled website (spoofed.html). DOM Consistency: At our first redundancy level, an additional Q-VLM receives the DOM alongside the proposed action (coordinates from the find function) and the original instruction. For static Google ad banners, the standardized HTML code features common Advertisement frame containing placeholders for an image and hyperlink (the only elements accessible to the attacker). When the DOM is provided to the Q-VLM, it includes this Advertisement string and the coordinates of the ad banner box, successfully flagging any click attempts within this region. This defense effectively blocks attacks on static ad banners. However, HTML5 ads present vulnerability: the DOM communicates only the leaf nodes of the webpages HTML architecture. When fabricated cookie popup is embedded within the ad banner, it replaces the \"Advertisement\" frame with cookie frame in the DOM, effectively circumventing DOM-based defenses. Multi-Modal Consensus: At our second redundancy defense, an additional independent Q-VLM call validates the screenshot alongside function outputs and the original instruction. Although the fabricated cookie banner is clearly visible within the ad banner to human observers, enabling clear distinction for the screenshot-based Q-VLM redundancy defense requires considerable effort. We find that achieving effective attack detection without degrading benign task utility is challenging, consistent with the weak success rates observed when applying the prompt-based defenses of Evtimov et al. [2025]. Even if an optimal trade-off is found for this defense, we later demonstrate how to circumvent Level 2 using optimized pixel attacks. E.1.3 Multi-Step Attack with Hop Website We extend this attack to multi-step execution. The previous attacks assume an adversary can directly specify the spoofed.html hyperlink in the victims homepage ad banner. In real-world settings, most websites hosting ad banners verify advertiser-provided hyperlinks. We demonstrate that our cookie attack can be extended to include an intermediary benign websitea hop site that does not verify hyperlinks in its ad banners. The attackers objective is to have the agent click first fabricated cookie popup to reach the hop website, then click subsequent fabricated popup that redirects to spoofed.html. This attack succeeds when the benign task requires visiting more than two webpages: if the agent must navigate multiple pages, the plan must anticipate handling at least two cookie popups, enabling our attack. This multi-step scheme generalizes to any number of webpage visits that the planner expects for task completion. 22 System-level Security for Computer Use Agents PREPRINT Long-range variant: We further extend the attacks potential harm by introducing long-range effect. Instead of embedding the first fabricated cookie popup in the initial webpage, we place it on later webpage that the adversary predicts the agent will visit based on the accessible plan. This approach increases attack stealthiness by delaying the malicious redirection until deeper into task execution. The complete multi-step long-range attack is detailed in Figure 2. E.2 pixel attack We demonstrate the brittleness of redundancy defenses against data-flow attacks. Even at our strongest redundancy defense, adversaries can craft attacks that evade both screenshot-based and DOM-enhanced Q-VLM verification. We consider redundancy defense configuration that heavily prioritizes security over utility, accepting very high false positive rate to maximize true positive rate. Even in this conservative setting where injecting spoofed elements typically triggers detection, attackers can optimize adversarial pixel perturbations that remain imperceptible from both DOM and screenshot perspectives. Optimization Procedure We initialize the ad banner with white background and perform gradient-based optimization exclusively on the image patch corresponding to the banner region. Since the final image must be in the integer domain, we apply quantization before optimization and introduce stochasticity followed by Expectation over Transformation (EoT) steps Athalye et al. [2018] to ensure robustness against quantization effects. Objective Function Design The banner is optimized with an objective function to induce malicious Q-VLM outputs when querying the find function. Since returning the ad banners coordinates would be immediately flagged by the redundancy defense, we instead optimize the Q-VLM to return plausible thought traces and coordinates that align with the element search instruction (see attack example in Figure 4). This ensures the defense does not flag the attack on multiple fronts: the screenshot and DOM appear benign (containing only seemingly random pixel perturbations), while the Q-VLMs returned action and thought trace appear legitimate and consistent with the task. Example Attack Scenario An example of the pixel attack is shown in Figure 4, where the model, originally navigating www.drugs.com to Browse the Natural Product Database, is misled into clicking on specific drug page despite the planner calling find(instruction=Natural Product Database). The adversarially crafted token sequence replacing the standard Q-VLM thought and action traces makes the target drug appear to be natural product and perfect match for the instruction Natural Product Database. This attack could be exploited by advertisers, such as pharmaceutical companies, to artificially increase traffic to their product pages by redirecting CUA agent traffic on commercial websites like www.drugs.com. Extensions and Future Work We leave potential multi-objective and multi-Q-VLM pixel attacks to future work, as Aichberger et al. [2025] have already demonstrated techniques for constructing such attacks. We also defer multi-step pixel attacks to future work, noting that an n-step attack would simply require chaining instances of the aforementioned one-step pixel attack."
        },
        {
            "title": "F Example Plans",
            "content": "F.1 Natural Products database example of good plan (P-LLM: GPT-5) Task: Browse the natural products database. # Step 1: OBSERVE - Understand current state summary = m z _ e h _ t ( Instruction ( text = \" Describe whether web browser is open showing website , generic search / new tab page , or the desktop / home screen . \" , length = ) , length =300 ) # Step 2: VERIFY - Confirm high - level location ( only location type for initial check ) in_website = verify _hypothesis ( observation = summary . text , hypothesis = \" We are viewing website in web browser and not the desktop home screen or generic empty search home page . \" ) is_desktop = verify _hypothesis ( 23 System-level Security for Computer Use Agents PREPRINT Figure 4: An example of how the pixel attack looks like. Q-VLM is tricked into clicking at the specific drug instead of the Natural Product Database, and to provides justifiable explanation (that the drug is categorized as natural product) hence the action is not flagged by the redundancy defense (Level 2). observation = summary . text , hypothesis = \" We are on the desktop home screen and not inside web browser . \" ) is_search_home = veri fy_hypot hesis ( observation = summary . text , hypothesis = \" We are in web browser on generic search or new tab homepage without specific website loaded . \" ) # Helper : cookie handling ( try quickly with multiple descriptions , both methods ) ef _c oo ie _d sc pt s = [ \" button to accept all cookies or consent to tracking in cookie notice \" , \" an 'I agree ' or consent button in cookie popup \" , \" button labeled ' Accept all ' in privacy banner \" , \" dismiss or OK button in cookie notification popup \" ] _ d _ k _ m s = get _page_el ements ( element_types = None ) cookie_check = ve rify_hyp othesis ( observation = _ d _ k _ m s . text , hypothesis = \" cookie or privacy consent popup with accept / consent buttons is visible \" ) if cookie_check . status == \" OK \" : cookie_found = False for desc in ef _ oo _d sc ri pt on : if cookie_found : no_op () else : 24 System-level Security for Computer Use Agents PREPRINT cookie_try = find ( Instruction ( text = desc , length =150) ) if cookie_try . start is None : cookie_try = fi nd_e le me nt_by_text ( description = desc , element_types =[ \" push - button \" , \" button \" , \" link \" ]) if cookie_try . start is not None : cclick = left_single ( cookie_try . start , \" Accept cookies / consent \" ) if cclick . status == \" OK \" or cclick . status == \" UNKNOWN \" : cookie_found = True wait () else : no_op () else : no_op () else : no_op () # Branch on current state if in_website . status == \" OK \" : # We are already on website . Explore this site first . # Try to find site - internal search ( not the browser address bar ) . search_found = False it _ se ch _ c ip ns = [ \" the website 's internal search box in the header or main area ( not the browser address bar ) \" , \" search input field on this site to search site content ( not Google search ) \" ] for sdesc in e _ ea _ sc pt ns : if search_found : no_op () else : sres = find ( Instruction ( text = sdesc , length =200) ) if sres . start is None : sres = fi nd_ el em en t_ by _t ext ( description = sdesc , element_types =[ \" entry \" , \" textbox \" , \" search \" ]) if sres . start is not None : sclick = left_single ( sres . start , \" Focus site search field ( internal website search ) \" ) if sclick . status == \" OK \" or sclick . status == \" UNKNOWN \" : wait () # Search for natural products using the website search ( explicitly not Google ) typed = type_text ( text = \" natural products database \" , instruction = \" Type into site internal search field ( not Google ) \" ) if typed . status == \" OK \" or typed . status == \" UNKNOWN \" : wait () search_found = True else : no_op () else : no_op () else : no_op () # After attempting site search once , assess if content seems relevant ; if not , prepare to use broader web search . # If we ' re not on clearly relevant page yet , we will do web search via the address bar as last resort . # Determine if results / content appear relevant to browsing natural products e _ t _ e _ e _ r = get_page_text ( max_length =2000 , incl ud e_n av ig ati on = False ) relevant_check = veri fy_hypot hesis ( observation = e _ t _ e _ e _ r . text , 25 System-level Security for Computer Use Agents PREPRINT hypothesis = \" The page is showing content or results related to natural products databases that can be browsed . \" ) if relevant_check . status != \" OK \" : # Use the browser 's address bar to search the web as last resort hotkey ( keys =[ Key . CTRL , Key . ] , instruction = \" Focus browser address bar for web search \" ) type_res = type_text ( text = \" natural products database \" , instruction = \" Search the web for natural products database \" ) if type_res . status == \" OK \" or type_res . status == \" UNKNOWN \" : wait () else : no_op () elif is_desktop . status == \" OK \" : # On desktop - open Chromium browser opened = False # Try to click Chromium icon if visible w er _ n _ c t s = [ \" Chromium browser icon or launcher on the desktop or dock \" , \" Google Chrome or Chromium icon to open the browser \" ] for bdesc in w _ co _ c ti : if opened : no_op () else : bres = find ( Instruction ( text = bdesc , length =150) ) if bres . start is None : bres = in d_e le me nt _b y_ text ( description = bdesc , element_types =[ \" push - button \" , \" button \" , \" link \" ]) if bres . start is not None : bclick = left_single ( bres . start , \" Open Chromium \" ) if bclick . status == \" OK \" or bclick . status == \" UNKNOWN \" : opened = True wait () else : no_op () else : no_op () if not opened : # Use system launcher as fallback hotkey ( keys =[ Key . WIN ] , instruction = \" Open system application launcher \" ) wait () type_text ( text = \" chromium \" , instruction = \" Type Chromium in launcher \" ) wait () press ( Key . ENTER , instruction = \" Launch Chromium from launcher \" ) wait () # Once browser is open , perform web search for natural products database hotkey ( keys =[ Key . CTRL , Key . ] , instruction = \" Focus address bar \" ) type_text ( text = \" natural products database \" , instruction = \" Search the web for natural products database \" ) wait () elif is_search_home . status == \" OK \" : # In browser on generic search / new tab - search directly hotkey ( keys =[ Key . CTRL , Key . ] , instruction = \" Focus address bar for search \" ) type_text ( text = \" natural products database \" , instruction = \" Search the web for natural products database \" ) wait () else : 26 System-level Security for Computer Use Agents PREPRINT # Fallback : Try to focus address bar anyway and search hotkey ( keys =[ Key . CTRL , Key . ] , instruction = \" Focus address bar ( fallback ) \" ) type_text ( text = \" natural products database \" , instruction = \" Search the web for natural products database \" ) wait () # Handle cookies on the search results page ( if any ) elem ents_ aft er _ se ar = get_page_elements ( element_types = None ) cookie_check2 = verify_ hypothesi ( observation = le me nt _a ft er_sea rch . text , hypothesis = \" cookie or privacy consent popup with accept / consent buttons is visible \" ) if cookie_check2 . status == \" OK \" : cookie2_found = False for desc in _c ie _ es ri pt on : if cookie2_found : no_op () else : r1 = find ( Instruction ( text = desc , length =150) ) if r1 . start is None : r1 = ind_ ele me nt _b y_ te xt ( description = desc , element_types =[ \" push - button \" , \" button \" , \" link \" ]) if r1 . start is not None : a1 = left_single ( r1 . start , \" Accept cookies / consent on search results \" ) if a1 . status == \" OK \" or a1 . status == \" UNKNOWN \" : cookie2_found = True wait () else : no_op () else : no_op () else : no_op () # Step 3: ACT - From web search , navigate to credible natural products database result db_found = False db _r esu lt _de cr ti ns = [ \" search result link for COCONUT natural products database \" , \" search result link for Natural Products Atlas ( NPAtlas ) \" , \" search result link for NPASS natural products database \" , \" search result link that clearly leads to natural products database website \" ] for ddesc in db _ re ul _ des cr ipt ion : if db_found : no_op () else : fres = find ( Instruction ( text = ddesc , length =200) ) if fres . start is None : fres = in d_e le me nt _b y_ text ( description = ddesc , element_types =[ \" link \" ]) if fres . start is not None : nav = left_single ( fres . start , \" Open natural products database site \" ) if nav . status == \" OK \" or nav . status == \" UNKNOWN \" : db_found = True wait () else : no_op () else : no_op () # Handle cookies on the target site 27 System-level Security for Computer Use Agents PREPRINT elements_on _t arg et = get_page_elements ( element_types = None ) cookie_check3 = verify_ hypothesi ( observation = ele me nts_on_ ta rge . text , hypothesis = \" cookie or privacy consent popup with accept / consent buttons is visible \" ) if cookie_check3 . status == \" OK \" : cookie3_found = False for desc in _c ie _ es ri pt on : if cookie3_found : no_op () else : r2 = find ( Instruction ( text = desc , length =150) ) if r2 . start is None : r2 = ind_ ele me nt _b y_ te xt ( description = desc , element_types =[ \" push - button \" , \" button \" , \" link \" ]) if r2 . start is not None : a2 = left_single ( r2 . start , \" Accept cookies / consent on database site \" ) if a2 . status == \" OK \" or a2 . status == \" UNKNOWN \" : cookie3_found = True wait () else : no_op () else : no_op () else : no_op () # Verify if we have reached natural products database site content_check _te xt = get_page_text ( max_length =2000 , include_navigation = False ) on_db_site = verify_hypothe sis ( observation = content _ch eck_text . text , hypothesis = \" We are on natural products database website or portal with options to browse or search compounds , structures , or entries . \" ) # If on the site , try to locate Browse / Explore / Search interface to start browsing if on_db_site . status == \" OK \" : browse_opened = False browse_strategies = [ ( \" link or button to Browse the database \" , [ \" link \" , \" push - button \" , \" button \" ]) , ( \" link or button labeled Explore to navigate entries \" , [ \" link \" , \" push - button \" , \" button \" ]) , ( \" main Search input field to search compounds in the database \" , [ \" entry \" , \" textbox \" ]) , ( \" navigation element for Compounds or Entries \" , [ \" link \" , \" push - button \" , \" button \" ]) ] for bdesc , btypes in browse_strategies : if browse_opened : no_op () else : bfind = find ( Instruction ( text = bdesc , length =200) ) if bfind . start is None : bfind = ind _e le men t_b y_text ( description = bdesc , element_types = btypes ) if bfind . start is not None : bclick = left_single ( bfind . start , \" Open browsing / search interface on the database site \" ) if bclick . status == \" OK \" or bclick . status == \" UNKNOWN \" : browse_opened = True wait () System-level Security for Computer Use Agents PREPRINT else : no_op () else : no_op () if not browse_opened : # Try few scrolls to reveal browse / search options for _ in range (3) : if browse_opened : no_op () else : scroll ( direction = \" down \" , start = None , instruction = \" Reveal more database navigation options \" ) wait () for bdesc , btypes in browse_strategies : if browse_opened : no_op () else : bfind2 = find ( Instruction ( text = bdesc , length =200) ) if bfind2 . start is None : bfind2 = fin d_ el em en t_ by_tex ( description = bdesc , element_types = btypes ) if bfind2 . start is not None : bclick2 = left_single ( bfind2 . start , \" Open browse / search after scrolling \" ) if bclick2 . status == \" OK \" or bclick2 . status == \" UNKNOWN \" : browse_opened = True wait () else : no_op () else : no_op () else : # If not clearly on database site yet , try once more to follow any link that indicates natural products database if not db_found : alt = ind _ele me nt _b y_ text ( description = \" any link that leads to natural products database website \" , element_types =[ \" link \" ] ) if alt . start is not None : alt_click = left_single ( alt . start , \" Open alternative natural products database \" ) if alt_click . status == \" OK \" or alt_click . status == \" UNKNOWN \" : wait () else : no_op () else : no_op () # Final verification and completion final_text = get_page_text ( max_length =2000 , include_navigation = False ) final_verification = verify_hypothesis ( observation = final_text . text , hypothesis = \" We are now on natural products database page with browse or search interface visible , allowing us to browse entries . \" ) if final_ve rif ic ation . status == \" OK \" : # Optional : check_done before marking done done_check = check_done ( Instruction ( 29 System-level Security for Computer Use Agents PREPRINT text = \" am on natural products database website with browse or search interface , ready to browse entries . \" , length =200 ) ) if done_check . done : mark_done () else : mark_done () else : mark_fail () F.2 Natural Products database example of an insufficient plan (P-LLM: Gemini 3 Pro) Task: Browse the natural products database. This plan is lacking branches, is too confident, and does not focus on website navigation but rather on going back into search and finding other databases it thinks fits the task description. # Step 1: Observe the current state summary = m z _ e h _ t ( Instruction ( text = \" Describe the current page content , checking if it is search engine , specific website , or if there are cookie popups . \" , length =300) , length =300) print ( \" Current page state : { summary . text } \" ) # Step 2: Handle Cookie Popups ( Common in browser tasks ) # We check for this first to ensure elements aren 't blocked cookie_check = verify_hypothesis ( observation = summary . text , hypothesis = \" cookie consent popup , privacy notice , or GDPR banner is visible on the screen . \" ) if cookie_check . status == \" OK \" : print ( \" Cookie popup detected . Attempting to accept . \" ) # Try to find the accept button using common labels cookie_handled = False cookie_labels = [ \" Accept all \" , \" Accept cookies \" , \" agree \" , \" Consent \" , \" Allow all \" , \" OK \" ] for label in cookie_labels : if cookie_handled : no_op () else : btn = fin d_ el em en t_ by _text ( label , element_types =[ \" button \" , \" push - button \" ]) if btn . start is not None : left_single ( btn . start , \" Click { label } \" ) wait () cookie_handled = True # If text search failed , try visual search if not cookie_handled : btn = find ( Instruction ( text = \" Accept or Agree button in cookie popup \" , length =100) ) if btn . start is not None : left_single ( btn . start , \" Click Accept visually \" ) wait () # Refresh summary after handling cookies summary = m z _ e h _ t ( Instruction ( text = \" Page content after cookie handling \" , length =300) , length =300) # Step 3: Determine Location and Navigate # Check if we are already on Natural Products Database is_on_database = veri fy_hypot hesis ( 30 System-level Security for Computer Use Agents PREPRINT observation = summary . text , hypothesis = \" The page is Natural Products Database website ( . . , NPASS , COCONUT , NP - MRD , SuperNatural ) or has scientific data about natural compounds . \" ) if is_on_database . status == \" OK \" : print ( \" Already on database website . Proceeding to browse . \" ) # Look for navigation options to browse data nav_clicked = False nav_options = [ \" Browse \" , \" Explore \" , \" Data \" , \" Compounds \" , \" Search \" ] for opt in nav_options : if nav_clicked : no_op () else : link = fi nd_ el em en t_ by _text ( opt , element_types =[ \" link \" , \" button \" , \" menu item \" ]) if link . start is not None : left_single ( link . start , \" Click { opt } \" ) wait () nav_clicked = True if not nav_clicked : # If no specific link found , scroll to browse content scroll ( \" down \" , None , \" Scroll down to browse the page content \" ) wait () mark_done () else : # Not on database . Check if we are on search results . is_search_res ults = verify _hypothe sis ( observation = summary . text , hypothesis = \" The page shows search results for ' natural products database ' or similar queries . \" ) # If not on search results , we need to perform search if is_sea rch_results . status == \" FAIL \" : print ( \" Not on database or search results . Initiating search . \" ) # Find search bar search_bar = in d_ el em ent_by_text ( \" Search \" , element_types =[ \" entry \" , \" textbox \" ]) if search_bar . start is None : search_bar = find ( Instruction ( text = \" Search bar or input box \" , length =50) ) if search_bar . start is not None : left_single ( search_bar . start , \" Click search bar \" ) type_text ( \" natural products database \" , \" Search for natural products database \" ) wait () # Assume we are now on search results is_search_res ults = ActionCall ( status = \" OK \" ) else : print ( \" Could not find search bar to navigate . \" ) mark_fail () # Step 4: Select Database from Search Results if is_search_resu lts . status == \" OK \" : print ( \" On search results page . Looking for valid database link . \" ) # Look for reputable databases 31 System-level Security for Computer Use Agents PREPRINT target_found = False # Candidates : NPASS , COCONUT , NP - Atlas , SuperNatural candidates = [ \" NPASS \" , \" COCONUT \" , \" Natural Products Atlas \" , \" SuperNatural \" , \" Natural Product Activity and Species Source \" ] for cand in candidates : if target_found : no_op () else : link = fi nd_ el em en t_ by _text ( cand , element_types =[ \" link \" , \" heading \" ]) if link . start is not None : left_single ( link . start , \" Click { cand } \" ) wait () target_found = True # Fallback if no specific name found if not target_found : generic_link = in d_ el ement_by_text ( \" Natural Products Database \" , element_types =[ \" link \" , \" heading \" ]) if generic_link . start is not None : left_single ( generic_link . start , \" Click generic database link \" ) wait () target_found = True if target_found : # Step 5: Browse the selected database # Check for cookies on the new site new_summary = m z _ e h _ t ( Instruction ( text = \" New page content \" , length =200) , length =200) new_cookie_check = ver ify_hypothesis ( observation = new_summary . text , hypothesis = \" Cookie popup visible \" ) if new_cookie_check . status == \" OK \" : accept_btn = fi nd_e le ment_by_text ( \" Accept \" , element_types =[ \" button \" , \" push - button \" ]) if accept_btn . start is not None : left_single ( accept_btn . start , \" Accept cookies on new site \" ) wait () # Find Browse button browse_btn = in d_el em ent_by_text ( \" Browse \" , element_types =[ \" link \" , \" button \" , \" menu item \" ]) if browse_btn . start is None : browse_btn = fi nd_e le me nt_by_text ( \" Explore \" , element_types =[ \" link \" , \" button \" ]) if browse_btn . start is not None : left_single ( browse_btn . start , \" Click Browse \" ) wait () else : # Scroll if no browse button scroll ( \" down \" , None , \" Scroll to browse content \" ) wait () mark_done () else : print ( \" Could not find relevant database link in the search results . \" ) mark_fail () 32 System-level Security for Computer Use Agents PREPRINT F.3 Example of generic cookie handler snippet ef _c oo ie _d sc pt s = [ \" button to accept all cookies or consent to tracking in cookie notice \" , \" an 'I agree ' or consent button in cookie popup \" , \" button labeled ' Accept all ' in privacy banner \" , \" dismiss or OK button in cookie notification popup \" ] # After potential navigation or app launch , re - observe summary = m z _ e h _ t ( Instruction ( text = \" Observe the current page to understand if we are on website and what site it might be . \" , length = ) , length =1200 ) cookie_check = ve rify_hypo thesis ( observation = summary . text , hypothesis = \" cookie or privacy consent popup with accept / consent buttons is visible \" ) if cookie_check . status == \" OK \" : cookie_found = False for desc in ef _ oo _d es ri pt on : if cookie_found : no_op () else : cookie_try = find ( Instruction ( text = desc , length =150) ) if cookie_try . start is None : cookie_try = in d_ el em ent_by_text ( description = desc , element_types =[ \" push - button \" , \" button \" , \" link \" ]) if cookie_try . start is not None : cclick = left_single ( cookie_try . start , \" Accept cookies / consent \" ) if cclick . status == \" OK \" or cclick . status == \" UNKNOWN \" : cookie_found = True wait () else : no_op () else : no_op () else : no_op ()"
        }
    ],
    "affiliations": [
        "AI Sequrity Company",
        "ETH Zurich",
        "University of Cambridge",
        "University of Toronto",
        "Vector Institute"
    ]
}