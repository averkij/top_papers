{
    "paper_title": "FABLE: Forest-Based Adaptive Bi-Path LLM-Enhanced Retrieval for Multi-Document Reasoning",
    "authors": [
        "Lin Sun",
        "Linglin Zhang",
        "Jingang Huang",
        "Change Jia",
        "Zhengwei Cheng",
        "Xiangzheng Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The rapid expansion of long-context Large Language Models (LLMs) has reignited debate on whether Retrieval-Augmented Generation (RAG) remains necessary. However, empirical evidence reveals persistent limitations of long-context inference, including the lost-in-the-middle phenomenon, high computational cost, and poor scalability for multi-document reasoning. Conversely, traditional RAG systems, while efficient, are constrained by flat chunk-level retrieval that introduces semantic noise and fails to support structured cross-document synthesis. We present \\textbf{FABLE}, a \\textbf{F}orest-based \\textbf{A}daptive \\textbf{B}i-path \\textbf{L}LM-\\textbf{E}nhanced retrieval framework that integrates LLMs into both knowledge organization and retrieval. FABLE constructs LLM-enhanced hierarchical forest indexes with multi-granularity semantic structures, then employs a bi-path strategy combining LLM-guided hierarchical traversal with structure-aware propagation for fine-grained evidence acquisition, with explicit budget control for adaptive efficiency trade-offs. Extensive experiments demonstrate that FABLE consistently outperforms SOTA RAG methods and achieves comparable accuracy to full-context LLM inference with up to 94\\% token reduction, showing that long-context LLMs amplify rather than fully replace the need for structured retrieval."
        },
        {
            "title": "Start",
            "content": "FABLE: Forest-Based Adaptive Bi-Path LLM-Enhanced Retrieval for Multi-Document Reasoning Linglin Zhang Qiyuan Tech Beijing, China Jingang Huang Qiyuan Tech Beijing, China Lin Sun Qiyuan Tech Beijing, China 6 2 0 2 6 ] . [ 1 6 1 1 8 1 . 1 0 6 2 : r Change Jia Qiyuan Tech Beijing, China Zhengwei Cheng Qiyuan Tech Beijing, China Xiangzheng Zhang Qiyuan Tech Beijing, China Abstract The rapid expansion of long-context Large Language Models (LLMs) has reignited debate on whether Retrieval-Augmented Generation (RAG) remains necessary. However, empirical evidence reveals persistent limitations of long-context inference, including the lostin-the-middle phenomenon, high computational cost, and poor scalability for multi-document reasoning. Conversely, traditional RAG systems, while efficient, are constrained by flat chunk-level retrieval that introduces semantic noise and fails to support structured cross-document synthesis. We present FABLE, Forest-based Adaptive Bi-path LLM-Enhanced retrieval framework that integrates LLMs into both knowledge organization and retrieval. FABLE constructs LLM-enhanced hierarchical forest indexes with multi-granularity semantic structures, then employs bi-path strategy combining LLM-guided hierarchical traversal with structure-aware propagation for fine-grained evidence acquisition, with explicit budget control for adaptive efficiency trade-offs. Extensive experiments demonstrate that FABLE consistently outperforms SOTA RAG methods and achieves comparable accuracy to full-context LLM inference with up to 94% token reduction, showing that long-context LLMs amplify rather than fully replace the need for structured retrieval."
        },
        {
            "title": "1 Introduction\nThe rapid advancement of Large Language Models (LLMs) with ex-\ntended context windows has fundamentally reshaped the landscape\nof knowledge-intensive question answering. Models like GPT-4\nTurbo with 128K tokens [24], Claude 3 with 200K tokens [1], and\nGemini 1.5 Pro with up to 1M tokens [28] have led to claims in\nindustry discussions and technical commentaries that Retrieval-\nAugmented Generation (RAG) [19] may become obsolete: a senti-\nment echoed across industry blogs and technical media. The under-\nlying assumption is straightforward: if an LLM can process entire\ndocument collections within its context window, why bother with\nthe complexity of retrieval systems?",
            "content": "However, this narrative oversimplifies multifaceted challenge. Recent research has exposed critical limitations of long-context LLMs: the \"lost in-the-middle\" phenomenon [21] where relevant information buried in long contexts is often overlooked, with performance varying significantly based on information placement [17], quadratic computational complexity of attention mechanisms [36] Corresponding author. Email:lincharliesun@gmail.com 1 making large-scale deployment prohibitively expensive, and insufficient capacity for repositories requiring synthesis across hundreds of documents [32, 35]. Traditional RAG systems [9, 19], while more scalable, suffer from semantic noise, which retrieved passages may exhibit high surface-level similarity to queries without containing actual answers [27, 31]. This disconnect between surface-level semantic matching performed by embedding models [18, 22] and deep answer relevance becomes particularly evident in multi-document benchmarks. Additionally, most RAG systems operate at individual document level, struggling with cross-document reasoning tasks such as comparative analysis, trend synthesis, or contradiction detection [13, 37]. Recent structured approaches have sought middle ground: graph-based methods like GraphRAG [7], LightRAG [10], and HippoRAG [11] organize information around entity relations and leverage techniques like personalized PageRank for multi-hop reasoning, while hierarchical methods like RAPTOR [29], TreeRAG [33], and HiRAG [15] construct tree structures over document collections. Yet these advances remain fundamentally constrained by paradigmatic divide. Structural RAG approaches whether organizing information around entity-centric relations and community summaries [7, 10] or employing predefined hierarchical schemas [15, 29, 33], construct static knowledge representations with passive similarity-matching retrieval decoupled from LLM reasoning. Conversely, long-context optimization approaches remain fundamentally reactive, providing LLMs with pre-assembled, largely flat information pools rather than enabling active, structured navigation through hierarchical knowledge spaces. Neither paradigm endows LLMs with the cognitive control humans naturally exercise: dynamically deciding whether to \"zoom in\" for granular details or \"zoom out\" for holistic synthesis based on query demands. Our Approach. We argue that overcoming the limitations of prior RAG systems requires fundamentally rethinking the relationship between retrieval and reasoning. Rather than treating them as separate stages(first retrieve, then reason), we propose FABLE (Forest-Based Adaptive Bi-Path LLM-Enhanced Retrieval), unified framework where LLMs actively participate in both organizing knowledge structures and dynamically navigating them according to query-specific cognitive demands. Unlike prior structured RAG methods, FABLE treats retrieval as query-conditioned navigation problem over LLM-constructed semantic hierarchies, rather than static similarity matching over pre-defined structures. Our key insight is twofold: (1) LLMs should not merely consume retrieved Figure 1: Overview of FABLE, forest-based adaptive bi-path LLM-enhanced retrieval framework information but actively construct semantically meaningful, multigranularity knowledge hierarchies during indexing; (2) retrieval should not follow one-size-fits-all logic but adaptively employ different traversal strategies based on query characteristics. FABLE implements this vision through three core innovations: Hierarchical Knowledge Forests: LLMs dynamically organize information into multi-level semantic hierarchies, enabling structured reasoning across both fine-grained details and high-level summaries. Importantly, indexing is one-time offline process, amortized across all queries, and can be performed incrementally. In contrast, long-context inference incurs per-query quadratic costs. Query-Conditioned Traversal: Retrieval is treated as an active navigation process, allowing the model to decide when to \"zoom in\" for granular content or \"zoom out\" for holistic synthesis, directly operationalizing the cognitive control humans exercise during reasoning. Bi-Path Retrieval: Unlike prior graphor tree-based RAG methods that rely on single static traversal, FABLE simultaneously employs semantic navigation path and structural aggregation path. This bi-path mechanism allows per-query adaptive exploration, ensuring that retrieval both captures deep semantic relevance and preserves document-level structural integrity capability absent in existing structured or long-context RAG approaches. These innovations unify retrieval and reasoning in FABLE, overcoming the static and decoupled limitations of prior methods, and enabling query-adaptive, multi-granularity access for complex multidocument tasks. We evaluate FABLE on synthetic and real-world multi-hop QA and large-scale agent tasks, demonstrating significant gains in effectiveness and efficiency. Superior reasoning with fewer hallucinations. FABLE achieves 93.65% completeness with low hallucination (5.37%) and irrelevance (2.52%), gaining +7.0 EM on HotpotQA and +8.0 EM on 2Wiki over structured RAG baselines. Dramatic token efficiency. FABLE matches full-context LLM performance (517K tokens) using only 31K tokens( 21.5K for LLM to select docs, 1.5K for LLM to select nodes, and 8K as the input for generator) up to 94% reduction, while achieving 92.07% completeness vs. Gemini-2.5-Pros 91.05% with full context. Complementary bi-path strengths. Bi-path fusion improves completeness by +1.6 points at 4K tokens; node-level navigation achieves 81.6% at 1K tokens (+30 points over flat retrieval). Scalability to large-scale collections. On BrowseCompplus (100K+ docs), FABLE boosts agent accuracy to 66.60% (+22.14) and recall to 76.60% (+14.28) while reducing search calls to 21.74, without changing the agent LLM. Contributions. This work makes the following contributions: Unified hierarchical retrieval-reasoning framework: FABLE integrates LLM-driven semantic structuring with budget-adaptive bi-path navigation over multi-granularity semantic forests, enabling dynamic balance between coverage breadth and detail depth. Bi-path mechanisms at multiple granularities: We introduce complementary bi-path retrieval combining depthadaptive LLM selection with vector retrieval at document level, and hierarchical navigation with TreeExpansions structural propagation at node level, capturing both symbolic understanding and embedding similarity while exploiting structural relationships. Budget-adaptive routing: FABLE dynamically adjusts retrieval granularity based on budget constraints with structureaware fusion and position-preserving ordering, avoiding unnecessary fine-grained retrieval when coarse results suffice. Comprehensive evaluation: Results across diverse benchmarks show FABLE consistently achieves superior balance between completeness and faithfulness, outperforming structured RAG and full-context LLMs across multiple real-world QA and agent tasks."
        },
        {
            "title": "2 Related Work\n2.1 Long-Context Large Language Models\nRecent years have seen rapid expansion of LLM context windows,\nfrom 4K tokens in GPT-3 to 128K in GPT-4 Turbo and beyond. These\nadvances are enabled by architectural techniques such as efficient",
            "content": "2 attention variants and improved positional encodings (e.g., RoPE). However, longer context does not necessarily imply better performance on knowledge-intensive tasks. Liu et al. [21] systematically demonstrate the lost-in-the-middle phenomenon, showing substantial accuracy degradation when relevant information appears in the middle of long contexts rather than near the boundaries. Subsequent analyses further confirm strong positional biases in practical attention utilization. Moreover, long-context inference incurs significant computational and economic costs due to the quadratic complexity of attention, making it expensive for large-scale or production use. As result, several studies argue that retrieval-based methods remain more efficient and reliable for tasks requiring precise information access or multi-document reasoning, motivating hybrid designs that combine retrieval with selective long-context modeling."
        },
        {
            "title": "2.2 Retrieval-Augmented Generation\nRetrieval-Augmented Generation (RAG), introduced by Lewis et\nal. [19], has become a standard paradigm for knowledge-intensive\nNLP. A typical RAG pipeline consists of chunking and indexing\ndocuments, retrieving relevant passages using dense or hybrid\nretrievers, and conditioning an LLM on the retrieved context. Ad-\nvances in dense retrieval, including bi-encoder models trained with\ncontrastive objectives, have substantially improved recall, while\ncross-encoder rerankers further enhance precision at higher cost.\nDespite these improvements, traditional chunk-based RAG ex-\nhibits fundamental limitations. Retrieved passages may be semanti-\ncally similar to the query yet fail to contain the required evidence,\nespecially in multi-document or research-oriented settings. Recent\nbenchmarks show that vanilla RAG performs poorly on complex\nmulti-hop queries [5]. These issues stem from flat chunking, limited\nglobal context modeling, and the lack of structured mechanisms\nfor cross-document synthesis.",
            "content": "Various extensions have been proposed, including query rewriting and decomposition (e.g., HyDE [8]) and iterative retrieval with self-reflection (Self-RAG [2]). While effective in some cases, these methods largely remain within the passage-level retrieval paradigm."
        },
        {
            "title": "2.3 Structured Representations for RAG\nTo overcome the limitations of flat retrieval, recent work explores\nstructured knowledge representations. Graph-based approaches or-\nganize information around entities and their relations. For example,\nGraphRAG constructs entity-centric graphs with community-level\nsummarization to support global queries [7]. HippoRAG further\nintegrates knowledge graphs with Personalized PageRank for multi-\nhop retrieval [11]. While effective for relational reasoning, such\nmethods remain inherently entity-centric and rely on graph diffu-\nsion over predefined relations, often missing higher-level semantic\nabstractions spanning full documents.",
            "content": "Hierarchical approaches instead organize information into multilevel abstractions. RAPTOR recursively clusters and summarizes text to form tree structures, enabling multi-granularity retrieval [29]. However, its bottom-up construction produces static hierarchies that may not align with discourse-level semantics and enforces fixed-level retrieval without query-conditioned traversal; crossdocument semantic connections also remain limited. We compare 3 against HippoRAG2 as the SOTA structured RAG baseline, which has been shown to outperform RAPTOR, GraphRAG, and LightRAG in recent evaluations. Overall, existing structured RAG methods primarily emphasize either entity-centric graph diffusion or document-internal hierarchies, but rarely construct document-spanning semantic hierarchies or support query-adaptive retrieval that jointly preserves structural integrity and semantic precision across documents."
        },
        {
            "title": "3.1 LLM-Enhanced Hierarchical Forest\nGiven a document collection D = {ğ‘‘1, ğ‘‘2, . . . , ğ‘‘ğ‘ }, FABLE con-\nstructs a semantic forest F = {ğ‘‡1,ğ‘‡2, . . . ,ğ‘‡ğ‘ }, where each tree ğ‘‡ğ‘–\nrepresents a hierarchical, multi-granularity semantic abstraction\nof document ğ‘‘ğ‘– . Each ğ‘‡ğ‘– is independently built and serves as the\nfundamental indexing unit for downstream retrieval and reasoning.\nSemantic-Aware Document Chunking. Instead of fixed-length\nor heuristic-based segmentation, we adopt LLM-guided semantic\nchunking to preserve discourse coherence. For each document ğ‘‘ğ‘– ,\nwe obtain a sequence of semantically self-contained chunks:",
            "content": "Cğ‘– = LLMsegment (ğ‘‘ğ‘– ) = {ğ‘1, ğ‘2, . . . , ğ‘ğ‘šğ‘– } (1) where each chunk ğ‘ ğ‘— is defined as ğ‘ ğ‘— = (chunk_idğ‘—, contentğ‘— ). Chunk boundaries are aligned with semantic units (e.g., paragraphs or topical shifts), avoiding sentence fragmentation and semantic overlap. Tree Structure Definition. For document ğ‘‘ğ‘– , its semantic structure is formalized as rooted tree ğ‘‡ğ‘– = (ğ‘‰ğ‘–, ğ¸ğ‘– ), subject to the following structural constraints: Bounded Depth: depth(ğ‘‡ğ‘– ) ğ·, where ğ· is configurable hyperparameter. Figure 2: Semantic tree construction and multi-granularity vector indexing: (1) semantic-aware chunking, (2) LLM-based hierarchical tree generation, (3) vector indexing of internal (toc+summary) and leaf (chunk) nodes for hierarchical retrieval. Typed Nodes: Each node ğ‘£ ğ‘‰ğ‘– is associated with semantic level, such that ğ‘‰ğ‘– = ğ‘‰root ğ‘‰section ğ‘‰subsection ğ‘‰leaf. Node Types and Attributes. Each node is represented as structured record with type-specific attributes: Tğ‘– = TreeMerge(cid:0)T (1) , . . . , (ğ¾ ) ğ‘– (cid:1) ğ‘– Multi-Granularity Vector Indexing. For non-leaf node ğ‘£, (4) let toc_path(ğ‘£) be the path from the root to ğ‘£. Its embedding is: Leaf Nodes (ğ‘£ ğ‘‰leaf): Correspond to original semantic chunks. Each leaf node is defined as ğ‘£ = (node_id, chunk_id), where chunk_id indexes an element in Cğ‘– . Internal Nodes (ğ‘£ ğ‘‰ğ‘– ğ‘‰leaf): Represent higher-level semantic abstractions. Each internal node is defined as ğ‘£ = (node_id, title, summary, children), where title is concise topic descriptor (e.g., ToC-style heading), summary is summary of all descendant nodes, and children denotes an ordered set of child nodes. Tree Construction. Given set of semantic chunks Cğ‘– , the semantic tree ğ‘‡ğ‘– is constructed by an LLM-based structuring module: ğ‘‡ğ‘– = LLMstructure (Cğ‘– ğ‘‘ğ‘– ) (2) which jointly generates hierarchical table of contents and nodelevel summaries. The construction process combines bottom-up semantic aggregation (from chunks to section-level summaries) with top-down structural constraints, including maximum tree depth and predefined node types. Progressive Construction for Long Documents. For documents exceeding the LLM context limit, we employ batch-wise tree construction strategy by partitioning the document into sequential parts. Each part is independently processed to build partial tree. The final document tree is obtained by merging all partial trees. This approach enables scalable long-document processing while maintaining cross-part semantic coherence. (ğ‘˜ ) ğ‘– = TreeBuild(partğ‘˜ ), ğ‘˜ = 1, . . . , ğ¾ (3) 4 eğ‘£ = Embed(toc_path(ğ‘£) summary(ğ‘£)) For leaf node ğ‘ (an original semantic chunk), eğ‘ = Embed(content(ğ‘)) (5) (6)"
        },
        {
            "title": "3.2 Budget-Adaptive Bi-Path Retrieval\nFABLE adopts a hierarchical retrieval architecture that dynamically\nadapts to context budget constraints while ensuring comprehensive\nrecall through bi-path mechanisms at each granularity level (Fig-\nure 1). The complete retrieval process is formalized in Algorithm 1.\nDocument-Level Bi-Path Recall. The retrieval begins with\nparallel document selection through two complementary paths.\nThe first path employs depth-adaptive LLM-guided selection, a novel\napproach that constrains LLM reasoning to non-leaf nodes within\ndepth threshold ğ¿. Unlike prior work that feeds entire documents or\nflat chunk lists to LLMs, this depth-adaptive strategy exploits the hi-\nerarchical structure of semantic trees: by analyzing only high-level\ntable-of-contents and summaries, the LLM performs structured\nreasoning over document semantics with bounded context length,\navoiding token budget explosion while maintaining global docu-\nment understanding. Concurrently, the second path performs multi-\ngranularity vector-based retrieval using FAISS indexing over all node\nembeddings, capturing semantically similar documents through\ndense retrieval that complements LLMâ€™s reasoning-based selec-\ntion. The fusion of these paths produces a deduplicated candidate\nset Dfusion that balances symbolic understanding with embedding-\nspace similarity.",
            "content": "Algorithm 1 Budget-Adaptive Bi-Path Retrieval Input: Query ğ‘, Semantic Forest F, Hierarchy Threshold ğ¿, Budget ğµmax Output: Retrieved Content 1: // Doc-Level Retrieval 2: //Path 1: Depth-Adaptive LLM-Guided Docs Selection: 3: Initialize candidate set: ğ‘‰ğ¿ = {all non-leaf nodes with depth ğ¿} 4: Context = { (toc(ğ‘£), summary(ğ‘£) ) ğ‘£ ğ‘‰ğ¿ } 5: Dllm = LLMselect (ğ‘, Context) 6: //Path 2: Vector-Based Docs Selection: 7: Dvector TopKFAISS (ğ‘, ğ¾doc, index = nodes_vector) 8: 9: // Doc-Level Fusion 10: Dfusion Deduplicate( Dllm Dvector ) 11: 12: // Budget-Adaptive Routing 13: if (cid:205)ğ‘‘ Dfusion content(ğ‘‘ ) ğµmax then return {content(ğ‘‘ ) ğ‘‘ Dfusion } 14: 15: end if 16: 17: // Node-Level Retrieval 18: // Path 1: LLM-Guided Navigation 19: Context (cid:208)ğ‘‘ Dfusion { (toc(ğ‘£), summary(ğ‘£) ) ğ‘£ NonLeaf(ğ‘‘ ) } 20: Nllm LLMnavigate (ğ‘, Context) 21: // Path 2: Tree-Based Expansion with Budget-Adaptive 22: Ntreexp TreeExpansion(ğ‘, Dfusion, ğµmax ) 23: 24: // Node-Level Fusion 25: Cordered NodeFusion( Nllm, Ntreexp ) 26: 27: // Budget Control 28: Cfinal BudgetControl( Cordered ) 29: return Cfinal Budget-Adaptive Routing. key innovation of FABLE is its dynamic granularity control: if the total content size of Dfusion falls within budget constraint ğµmax, the framework terminates at document-level and directly returns full document contents. This adaptive mechanism avoids unnecessary fine-grained retrieval overhead when coarse-grained results suffice, significantly improving efficiency for queries with limited relevant scope. Node-Level Bi-Path Recall. When budget constraints necessitate fine-grained retrieval, FABLE activates node-level bi-path selection within the fused document set. The LLM-guided path performs hierarchical navigation through hierarchical non-leaf nodes, progressively narrowing down to relevant semantic nodes. In parallel, we employ Structure-Aware Propagation TreeExpansion over the semantic tree to supplement vector retrieval. Traditional dense retrieval operates in flat embedding space, missing valuable parent-child relationships encoded in our hierarchical structure. We propagate relevance through tree edges by combining three signals: (1) direct query-node similarity, (2) ancestor-inherited relevance (capturing topic continuity), and (3) child-aggregated relevance (capturing subtopic importance). For node ğ‘£, composite score is: ğ‘† (ğ‘£) = 1 3 (cid:16) ğ‘†sim (ğ‘£) + ğ‘†inh (ğ‘£) + ğ‘†child (ğ‘£) (cid:17) (7) where ğ‘†sim (ğ‘£) = cos(eğ‘£, eğ‘)/depth(ğ‘£) applies depth decay to favor high-level abstractions, ğ‘†inh (ğ‘£) = maxğ‘¢ Anc(ğ‘£) ğ‘†sim (ğ‘¢) inherits the maximum ancestor score, and ğ‘†child (ğ‘£) = avgğ‘ Children(ğ‘£) ğ‘† (ğ‘) 5 Algorithm 2 NodeFusion Input: Nllm, Ntreexp Output: Ordered chunks Cordered 1: // Ancestor-descendant deduplication 2: Ndedup RemoveDescendants( Nllm Ntreexp ) 3: 4: // Document-level sorting 5: Dllm Docs( Ndedup Nllm ) 6: Dtreexp Docs( Ndedup Ntreexp ) Dllm 7: 8: // Intra-document sorting 9: Nordered [ ] 10: for ğ‘‘ [ Dllm, Dtreexp ] do 11: 12: 13: end for 14: 15: // Extract chunks 16: Cordered GetChunks( Nordered ) 17: 18: return Cordered Nğ‘‘ sort_by_position_order_in_d( Ndedup Tree(ğ‘‘ ) ) Append Nğ‘‘ to Nordered aggregates child scores. We assign uniform weights (1/3 each) to the three relevance signals. No special tuning or optimization was performed for these weights; despite their simplicity, this scheme works robustly across diverse document structures, preserves interpretability, and enables efficient local graph diffusion over the semantic trees. Node-Level Fusion Strategy. The final fusion operation (Algorithm 2) integrates results through three steps: (1) Structure-aware deduplication removes redundant ancestor-descendant pairs by retaining ancestor subtrees that encompass descendant content, (2) Priority-based partitioning separates LLM-selected nodes llm from tree-expanded nodes treexp, and (3) Position-preserving ordering sorts nodes within each document by their original positions, concatenating LLM results before TreeExpansion results. This strategy ensures that explicitly identified relevant content precedes structurally inferred context while maintaining document-native reading order, providing generation models with optimally arranged retrieved evidence. TreeExpansion Details. TreeExpansion traverses each tree in Dfusion, computing scores via Eq. 4 recursively (bottom-up for ğ‘†child, top-down for ğ‘†inh). Nodes are ranked by ğ‘† (ğ‘£) and greedily selected until budget ğµmax is exhausted, with ancestor nodes taking priority to avoid redundancy. Complexity is ğ‘‚ (ğ‘‰ğ‘– ğ·) per tree where ğ· is depth. Budget Control. Given the maximum input length ğµmax of the generation model, dynamically adjust the retrieval results through greedy algorithms or knapsack problem solvers: Cfinal = {ğ‘1, . . . , ğ‘ğ‘˜ } ğ‘˜ = max (cid:40) ğ‘˜ (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) ğ‘˜ ğ‘–=1 (cid:41) â„“ (ğ‘ğ‘– ) ğµmax (8) The bi-path design at both granularities ensures robust recall: LLM-guided paths capture semantically precise matches through structured reasoning over hierarchical abstractions, while multigranularity vector or tree-based paths provide coverage over diverse Table 1: Results on synthetic and real-world multi-hop QA benchmarks. FABLE variants: \"llm-docs/nodes\" use LLM-guided selection only, \"docs/nodes\" use full bi-path fusion. HippoRAG2 represents the current SOTA for structured RAG. Best results in bold. TreeRAG results unavailable in original paper. Method Dragonball/DragBalance Dragonball/DragSingleZh HotpotQA 2WikiMultiHopQA Recall(%) EIR(%) Comp.(%) Hall.(%) Irr.(%) Recall(%) EIR(%) EM(%) F1(%) EM(%) F1(%) Synthetic Knowledge Real-World Knowledge Traditional RAG BM25 BGE-M3 66.14 64.02 Structure-Enhanced RAG TreeRAG LongRefiner HippoRAG 24.72 39.16 5.58 5.43 1.53 5.77 not reported 41.14 62.19 LLM-in-context (Full-context Inference) Qwen3-32B Gemini-2.5-Flash Gemini-2.5-Pro 100 100 100 Our Proposed Method FABLE(llm-docs) FABLE(llm-nodes) FABLE(docs) FABLE(nodes) 85.55 77.30 85.80 84.55 0.02 0.02 0.02 3.46 18.60 0.76 1. 65.73 88.37 91.05 92.01 84.78 92.07 89.39 67.85 67.16 16.97 17.83 15.18 15.01 not reported not reported 25.00 37.50 36.40 51.50 15.00 1.00 15.98 1.85 33.77 26.72 25.13 5.93 5. 4.88 10.98 5.37 5.98 25.09 11.00 9.02 5.58 3.45 3.00 4.22 2.52 4.61 57.31 15.79 28.08 100 100 74.61 66.07 74.99 72.97 18.38 1.30 5.48 0.01 0.01 0.01 5.19 21.90 0.4 3.33 31.50 41.00 0.00 31.00 42. 31.00 29.00 46.50 48.00 not reported 43.18 58.87 8.00 44.50 5.56 47.44 55.97 42.22 38.34 62.22 63. 0.00 38.50 52.02 30.5 30.50 52.50 50.00 11.27 59.67 7.80 54.94 63.68 38.46 38.43 63.87 60.26 similarity patterns through dense retrieval and structural propagation, collectively addressing the complementary strengths of neural retrieval and structured navigation."
        },
        {
            "title": "4 Experimental Setup\n4.1 Evaluation Datasets\nWe evaluate FABLE on diverse datasets covering synthetic reason-\ning, multi-hop QA, and agent-based retrieval.",
            "content": "Synthetic Knowledge QA: DragonBall [40] contains LLMgenerated documents and queries following predefined schemas, enabling controlled evaluation without real-world confounds. Real-World Knowledge Multi-hop QA: We use two subsets from LongBench [3]: HotpotQA [37] requiring reasoning across Wikipedia documents and 2Wiki [14] emphasizing multi-step entity reasoning focusing on logical reasoning over structured and unstructured sources. We adapt these two datasets from long-context inference benchmark to RAG setting by treating the candidate documents as retrieval corpus. Agent-based Downstream Application: BrowseComp-plus [5] evaluates the integration of retrieval modules within autonomous research agents. In this setting, FABLE replaces the original retrieval component used in DeepResearch-style agents, and performance is measured by the agents ability to conduct multi-turn navigation and selective information acquisition over large document collections."
        },
        {
            "title": "4.2 Experimental Configuration\nTo ensure fair and reproducible comparisons, we strictly align our\nexperimental settings with those of the selected baselines and only",
            "content": "6 modify components that are directly related to the proposed method. We intentionally align backbone LLMs with dataset language and official evaluation protocols, and relative comparisons are always performed under identical LLM settings. Therefore, observed performance differences can be attributed to retrieval architecture rather than generator capacity. Agent Baselines and Retrieval Strategy. For agent baselines, we use Qwen3-Embed-8B [39] as in the original implementations, with results from official leaderboards. We replace the original retrieval with our proposed mechanism using GPT-OSS-120B [25] and Qwen3-Embed-8B, while keeping the agent policy model unchanged. Embedding and Reranking Models. For non-agent methods with dense retrieval, we use BGE-M3 [4] for embedding and BGEReranker-v2-M3 [20] for reranking when required. Backbone Language Models. For DragonBall (which includes Chinese subsets), we use DeepSeek-V3.2 [6] throughout. For HotpotQA and 2Wiki (English benchmarks), we use GPT-OSS-120B. Document Chunking and Hyperparameters. Document chunking uses LLM-based segmentation: DeepSeek-V3.2 for DragonBall, GPT-OSS-120B for HotpotQA, 2Wiki, and BrowseComp-plus. Other hyperparameters follow baseline defaults unless specified. We set hierarchy depth ğ· = 4, and budget ğµmax ranging from 1K to 128K tokens depending on experimental conditions. We will release code, prompts, and constructed semantic forests upon acceptance. Overall, this setup ensures that performance differences primarily arise from the proposed retrieval strategy rather than changes in embedding models, agent policies, or hyperparameter tuning. Figure 3: Performance of retrieval and context construction strategies across varying input lengths. Long-context models use fixed 517K windows, while retrieval methods are tested from 1K to 128K tokens. Chunk-based retrieval shows limited gains and degrades at large contexts, whereas FABLE consistently improves completeness while reducing hallucination and irrelevance. FABLE (docs/llm-docs) achieves the best performance at moderate budgets. Table 2: Performance on BrowseComp-plus. Top five rows: official leaderboard results with rankings. Bottom two rows: our methods with FABLE retriever. TDR-30B-A3B and Q3-E8B denote Tongyi-DeepResearch-30B-A3B and Qwen3Embed-8B. FABLE(ln): LLM-based node selection; FABLE: bi-path selection. Best in bold; second-best underlined. Method (LLM/Retriever) Rank BrowseComp-plus Acc(%) Recall(%) SearchCalls GPT5/MixedbreadSearch GPT5/Q3-E8B o3/Q3-E8B GPT5/BM25 TDR-30B-A3B/Q3-E8B TDR-30B-A3B/FABLE(ln) TDR-30B-A3B/FABLE 1st 3rd 4rd 5rd 11th 78.41 71.69 65.90 57.59 44.46 64.92 66.60 48.85 78.98 73.24 61.70 62.32 68.07 76. 44.67 21.74 23.97 23.23 30.37 46.99 21."
        },
        {
            "title": "4.3 Evaluation Metrics\nWe use dataset-specific metrics for faithful assessment across tasks.\nDragonBall. Following [40], we report retrieval metrics (Recall\nand EIR) and end-to-end metrics (Completeness, Hallucination,\nIrrelevance). Recall measures whether gold supporting sentences\nare retrieved; EIR evaluates the proportion of relevant retrieved\ninformation. End-to-end metrics assess answer completeness, un-\nsupported content, and irrelevant information, evaluated using\nDeepSeek-V3.2.",
            "content": "Multi-hop QA Datasets. Following [3], we report Exact Match (EM) and F1 for answer correctness. EM measures exact answer matching; F1 evaluates token-level overlap. We use official HippoRAG2 [12] evaluation scripts. BrowseComp-plus. Following [5], we report Accuracy, Recall, and Search-Calls. Accuracy measures answer correctness, Recall evaluates retrieval of supporting information, and Search-Calls quantify search actions for efficiency assessment."
        },
        {
            "title": "5 Evaluation\n5.1 Effectiveness on Multi-Document Reasoning\nTo comprehensively evaluate the effectiveness of our method, we\ncompare against a diverse set of baselines, including the tradi-\ntional sparse retrieval approach BM25, the modern dense retriever\nBGE-M3 [4], as well as several structured or hierarchical retrieval-\naugmented generation methods, namely TreeRAG [33], LongRe-\nfiner [16], and HippoRAG2 [12] which is the current SOTA that has\nbeen shown to outperform RAPTOR, GraphRAG, and LightRAG.",
            "content": "We additionally evaluate several strong large language models, including Qwen3-32B [34], Gemini-2.5-Flash, and Gemini-2.5-Pro. For these evaluations, all documents from the benchmark are concatenated and fed to the LLMs to leverage their long-context capabilities. When the total content exceeds the models context window, we process the documents in parallel batches and aggregate the outputs to produce the final answer. For TreeRAG, we report results on DragSingleZh as provided in the original paper [33]. For all other methods, we conduct experiments under consistent experimental settings as described. The results are reported in Table 1. Across synthetic knowledge benchmarks, traditional RAG baselines (BM25 and BGE-M3) exhibit reasonable recall but suffer from low EIR and relatively high hallucination and irrelevance, indicating limited ability to aggregate and reason over multiple retrieved documents. Structure-enhanced baselines partially alleviate this issue: while TreeRAG improves recall and EIR on DragonBall/DragSingleZh, its applicability is limited to specific settings, and methods such as LongRefiner and HippoRAG2 still show clear trade-off between completeness and faithfulness, with increased hallucination under complex multi-hop reasoning. In contrast, LLM-in-context inference with full documents, despite achieving perfect recall by construction, does not consistently translate retrieval coverage into correct reasoning. Models such as Qwen3-32B exhibit substantial hallucination and low completeness, highlighting that long-context capacity alone is insufficient for structured multi-document reasoning. Even strong proprietary models (Gemini-2.5-Flash and Gemini-2.5-Pro), while significantly reducing hallucination, still lag behind our method in terms of overall reasoning faithfulness and controllability, particularly on synthetic benchmarks with explicitly structured dependencies. Our proposed FABLE framework consistently outperforms all baselines across both synthetic and real-world datasets. On DragonBall/DragBalance, FABLE (docs) achieves the highest completeness (92.07%) while simultaneously minimizing hallucination (5.37%) and irrelevance (2.52%), demonstrating strong ability to preserve global document semantics while suppressing spurious generation. Notably, FABLE variants with node-level representations yield substantially higher EIR, indicating more effective evidence aggregation and utilization during reasoning, even when recall is slightly lower than document-level variants. On real-world multi-hop QA benchmarks (HotpotQA and 2Wiki), FABLE further demonstrates robust generalization. Both FABLE (docs) and FABLE (nodes) achieve the strongest EM and F1 scores among retrieval-augmented methods, substantially outperforming traditional and structure-enhanced RAG baselines. These results suggest that explicitly modeling hierarchical knowledge structures enables more reliable cross-document reasoning in open-domain settings, where evidence is noisy and distributed. Overall, the results confirm that FABLE effectively bridges the gap between retrieval coverage and reasoning faithfulness. By enforcing structured knowledge organization and controlled evidence aggregation, our method delivers consistently higher completeness with lower hallucination across diverse knowledge regimes, outperforming both conventional RAG pipelines and full-context LLM inference."
        },
        {
            "title": "5.2 Impact of Retriever Choice on Agent\nTable 2 presents the performance of different LLM and Retriever\ncombinations on the BrowseComp-plus benchmark. The top five\nrows correspond to official leaderboard results, with their respec-\ntive rankings indicated in the â€œRankâ€ column. These results show\nthat both the choice of LLM and the retriever significantly af-\nfect agent performance: higher-ranked methods generally leverage\nstronger LLM backbones or more effective retrievers. For exam-\nple, GPT5 with Mixedbread Search achieves the 1st rank, while\nTongyiDeepResearch-30B-A3B with Qwen3-Embed-8B ranks 11th,\ndemonstrating that even a strong LLM can be limited by the re-\ntriever quality.",
            "content": "The bottom two rows report our results using FABLE as the retriever while keeping TongyiDeepResearch-30B-A3B as the LLM backbone. Compared to the official leaderboard entries using Qwen3Embed-8B, replacing the retriever with FABLE substantially improves accuracy and recall, moving the method closer to higherranked leaderboard entries despite using the same LLM. This indicates that FABLE effectively enhances retrieval quality, which in turn improves downstream agent reasoning. Overall, these findings highlight that agent performance is jointly determined by the LLM and the retriever, and that improving the retriever alone without changing the LLM, can lead to meaningful 8 performance gains. FABLE demonstrates strong alternative to existing retrievers, particularly in scenarios where upgrading the LLM is costly or impractical."
        },
        {
            "title": "5.3 Ablation Study\nWe conduct ablation studies to validate each componentâ€™s contri-\nbution in FABLE. All experiments use the DragonBall benchmark\nwith context budgets from 1k to 128k tokens and Qwen3-32B for\nLLM generation.",
            "content": "Impact of LLM-Enhanced Indexing. We compare two in5.3.1 dexing strategies: (1) fixlength-chunks: Fixed-length chunking (128 tokens); (2) llm-chunks: LLM-guided semantic chunking without hierarchy. Results. Figure 3 (left panel) shows llm-chunks substantially outperforms fixlength-chunks across all context lengths. At 64k tokens, llm-chunks achieves 66.6% completeness vs. 61.2% for fixlengthchunks, approaching the 65.7% upper bound of qwen3-32B with full document access. More critically, llm-chunks reaches near-optimal performance at 4k tokens, while fixlength-chunks requires 128k tokens for comparable results. At 8k tokens, llm-chunks achieves 98.2% of the full-document upper bound (64.5% vs. 65.7%), demonstrating 64 efficiency gain. This validates that semantic-aware chunking preserves discourse coherence, enabling smaller context windows without sacrificing quality."
        },
        {
            "title": "5.3.2 Document-Level Bi-Path Ablation. We evaluate FABLE(docs)\nagainst single-path document selection variants: (1) FABLE(llm-\ndocs): Depth-adaptive LLM-guided selection only; (2) FABLE(docs):\nBi-path fusion at document level (includes vector retrieval)",
            "content": "Results. Figure 3 shows FABLE(docs) with bi-path mechanism consistently achieves higher completeness than single-path variants. Interestingly, FABLE(llm-docs) shows slightly higher completeness at 1k-2k tokens (56.0% vs. 52.1% at 1k), suggesting that pure LLM reasoning is more effective for extremely limited budgets where vector retrieval may introduce noise. However, at 4k+ tokens, FABLE(docs) consistently outperforms the single-path variant (+1.6 points at 4k, +1.3 points at 8k). Notably, FABLE(docs) at 4k tokens (92.5%) already surpasses gemini-2.5-pro with full document access (91.1%), demonstrating that bi-path hierarchical retrieval enables smaller context budgets to exceed the performance of significantly larger models with unrestricted access. This validates that intelligent retrieval architecture is more impactful than simply scaling context or model size. The hallucination and irrelevance metrics show similar patterns: FABLE(docs) achieves superior precision at longer contexts (5.37% hallucination and 2.52% irrelevance at 8k vs. 5.5% and 3.45% for gemini-2.5-pro), validating that vector-based retrieval supplements LLM reasoning with embedding-space similarity signals that reduce false positives."
        },
        {
            "title": "5.3.3 Node-Level Bi-Path Ablation. We compare three fine-\ngrained retrieval strategies: (1) FABLE(llm-nodes): LLM-guided\nhierarchical navigation only; (2) FABLE(treexp-nodes): TreeExpan-\nsion with structural propagation only; (3) FABLE(nodes): Bi-path\nfusion at node level.",
            "content": "Results. Figure 3 reveals distinct strengths across context budgets. Short context (1k-2k): At 1k tokens, FABLE (llm-nodes) reaches 81.6% completeness, significantly surpassing llm-chunks (57.2%) and fixlength-chunks (51.4%), with improvements of 24.4 and 30.2 points, respectively. This highlights the importance of LLM-guided hierarchical navigation for resource-constrained scenarios, directly reasoning over ToC to identify relevant sections. In contrast, vectorbased TreeExpansion may retrieve semantically similar but irrelevant passages under tight budgets. The bi-path FABLE(nodes) (61.9%) lies between FABLE(llm-nodes) and FABLE(treexp) (52.5%), indicating TreeExpansion introduces noise at extreme budget limits. Convergence at longer contexts (4k-8k): The gap rapidly closes as context increases. At 4k tokens, FABLE(nodes) achieves 89.4% completeness, significantly outperforming flat retrieval baselines (llmchunks: 62.3%, fixlength-chunks: 58.2%) by 27+ points. By 8k tokens, FABLE(treexp) (88.9%) and FABLE(nodes) (89.1%) approach the performance of gemini-2.5-pro with full document access (91.1%), while using only 8k tokens compared to 517k. Hallucination and irrelevance control: FABLE(nodes) consistently shows the lowest hallucination rates at 4k+ tokens (6.0% at 4k, 7.0% at 8k), substantially better than fixlength-chunks and llmchunks. For irrelevance, FABLE(nodes) achieves 4.6% at 4k and 3.9% at 8k, compared to 10.6% and 8.1% for the flat retrieval baselines, validating that fine-grained hierarchical retrieval enables more precise evidence localization."
        },
        {
            "title": "5.3.4 Bi-path Component Analysis. Our results validate the\ncore hypothesis that combining global and local information pro-\ncessing yields complementary advantages. As shown in the task-\nspecific results (Figure 5), FABLE(llm-nodes) excels at tasks re-\nquiring broad information synthesis (e.g., Summarization: 91.3%),\nwhile TreeExpansion performs better on tasks demanding local\ndetail extraction (e.g., Temporal Sequence and Factual Question).\nFABLE(Nodes) effectively integrates both strengths, achieving su-\nperior performance across all task types (average 97.7%).",
            "content": "The cross-domain experiments as shown in Figure 4 further confirm this bi-path advantage. In the Medical domain, where FABLE and TreeExpansion show opposing performance patterns (En: 91.7% vs. 86.2%; Zh: 81.2% vs. 72.3%), FABLE(Nodes) successfully reconciles both approaches (97.6% and 94.3%). This pattern holds consistently across Finance and Law domains in both English and Chinese, demonstrating that the architectural benefits are both domainand language-agnostic. The most substantial improvements appear in tasks requiring complex reasoning: Multi-hop Reasoning (+22.8% over baseline) and Summarization Questions (+35.5%), where neither global context alone nor local details alone suffice. This confirms that the bi-path design addresses fundamental limitation of single-path approaches in handling the full spectrum of question-answering challenges."
        },
        {
            "title": "6 Conclusion\nLimitations. FABLE requires upfront indexing and benefits most\nfrom semantically structured documents. Its advantages diminish\non highly unstructured corpora or queries relying solely on key-\nword matching. Code and constructed forests will be released upon\nacceptance.",
            "content": "Figure 4: bi-path performance across domains and languages. Figure 5: bi-path performance across different query types. This work revisits whether long-context LLMs can replace retrievalaugmented systems for knowledge-intensive reasoning. We show that increasing context length alone does not resolve core challenges such as semantic distraction, hallucination, and inefficient multi-document evidence utilization. We introduced FABLE, forest-based adaptive bi-path retrieval framework that positions LLMs as knowledge organizers rather than passive consumers. By constructing LLM-enhanced hierarchical representations and enabling query-adaptive bi-path retrieval over global abstractions and local evidence, FABLE balances retrieval coverage with reasoning faithfulness. With fixed LLM backbones, FABLE consistently outperforms strong RAG and structured retrieval baselines, achieving performance comparable to full-context inference with lower token budgets. Our results indicate that gains stem from retrieval-side architecture rather than simply longer context, emphasizing the importance of structured knowledge organization and controlled traversal for efficient, faithful LLM reasoning. References [1] Anthropic. 2024. Introducing the Claude 3 Model Family. https://www.anthropic. com/news/claude-3-family. Accessed: 2024-01-16. [2] Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. 2023. Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection. arXiv:2310.11511 [cs.CL] https://arxiv.org/abs/2310.11511 9 [3] Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, et al. 2024. Longbench: bilingual, multitask benchmark for long context understanding. In Proceedings of the 62nd annual meeting of the association for computational linguistics (volume 1: Long papers). 31193137. [4] Jianlyu Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng Liu. 2024. M3-Embedding: Multi-Linguality, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation. In Findings of the Association for Computational Linguistics: ACL 2024, Lun-Wei Ku, Andre Martins, and Vivek Srikumar (Eds.). Association for Computational Linguistics, Bangkok, Thailand, 23182335. doi:10.18653/v1/2024.findings-acl. [5] Zijian Chen, Xueguang Ma, Shengyao Zhuang, Ping Nie, Kai Zou, Andrew Liu, Joshua Green, Kshama Patel, Ruoxi Meng, Mingyi Su, Sahel Sharifymoghaddam, Yanxi Li, Haoran Hong, Xinyu Shi, Xuye Liu, Nandan Thakur, Crystina Zhang, Luyu Gao, Wenhu Chen, and Jimmy Lin. 2025. BrowseComp-Plus: More Fair and Transparent Evaluation Benchmark of Deep-Research Agent. arXiv:2508.06600 [cs.CL] https://arxiv.org/abs/2508.06600 [6] DeepSeek-AI. 2025. DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models. [7] Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva Mody, Steven Truitt, and Jonathan Larson. 2024. From Local to Global: Graph RAG Approach to Query-Focused Summarization. arXiv preprint arXiv:2404.16130 (2024). [8] Luyu Gao, Xueguang Ma, Jimmy Lin, and Jamie Callan. 2022. Precise ZeroShot Dense Retrieval without Relevance Labels. arXiv:2212.10496 [cs.IR] https: //arxiv.org/abs/2212.10496 [9] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Meng Wang, and Haofen Wang. 2024. Retrieval-Augmented Generation for Large Language Models: Survey. arXiv:2312.10997 [cs.CL] https://arxiv.org/abs/2312. [10] Zirui Guo, Xiaohua Lian, Yanhua Yang, Hanzhi Huang, Shuwen Liu, Yixuan Feng, Yiding Liu, and Jinhao Li. 2024. LightRAG: Simple and Fast Retrieval-Augmented Generation. arXiv preprint arXiv:2410.05779 (2024). [11] Bernal JimÃ©nez GutiÃ©rrez, Yiheng Shu, Yu Gu, Michihiro Yasunaga, and Yu Su. 2024. HippoRAG: Neurobiologically Inspired Long-Term Memory for Large Language Models. In Advances in Neural Information Processing Systems, Vol. 37. [12] Bernal JimÃ©nez GutiÃ©rrez, Yiheng Shu, Weijian Qi, Sizhe Zhou, and Yu Su. 2025. From RAG to Memory: Non-Parametric Continual Learning for Large Language Models. arXiv:2502.14802 [cs.CL] https://arxiv.org/abs/2502.14802 [13] Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. 2020. Constructing Multi-hop QA Dataset for Comprehensive Evaluation of Reasoning Steps. In Proceedings of the 28th International Conference on Computational Linguistics. 66096625. [14] Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. 2020. Constructing Multi-hop QA Dataset for Comprehensive Evaluation of Reasoning Steps. In Proceedings of the 28th International Conference on Computational Linguistics, Donia Scott, Nuria Bel, and Chengqing Zong (Eds.). International Committee on Computational Linguistics, Barcelona, Spain (Online), 66096625. doi:10.18653/v1/2020.coling-main.580 [15] Haoyu Huang, Yongfeng Huang, Junjie Yang, Zhenyu Pan, Yongqiang Chen, Kaili Ma, Hongzhi Chen, and James Cheng. 2025. Retrieval-Augmented Generation with Hierarchical Knowledge. arXiv preprint arXiv:2503.10150 (2025). [16] Jiajie Jin, Xiaoxi Li, Guanting Dong, Yuyao Zhang, Yutao Zhu, Yongkang Wu, Zhonghua Li, Ye Qi, and Zhicheng Dou. 2025. Hierarchical Document Refinement for Long-context Retrieval-augmented Generation. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar (Eds.). Association for Computational Linguistics, Vienna, Austria, 3502 3520. doi:10.18653/v1/2025.acl-long. [17] Greg Kamradt. 2023. Needle In Haystack - Pressure Testing LLMs. https: //github.com/gkamradt/LLMTest_NeedleInAHaystack. Accessed: 2024-01-16. [18] Vladimir Karpukhin, Barlas OÄŸuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense Passage Retrieval for OpenDomain Question Answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). 67696781. [19] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich KÃ¼ttler, Mike Lewis, Wen-tau Yih, Tim RocktÃ¤schel, Sebastian Riedel, and Douwe Kiela. 2020. Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks. In Advances in Neural Information Processing Systems, Vol. 33. 94599474. [20] Chaofan Li, Zheng Liu, Shitao Xiao, and Yingxia Shao. 2023. Making Large Language Models Better Foundation For Dense Retrieval. arXiv:2312.15503 [cs.CL] [21] Nelson Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2023. Lost in the Middle: How Language Models Use Long Contexts. In Advances in Neural Information Processing Systems, Vol. 36. [22] Niklas Muennighoff, Nouamane Tazi, LoÃ¯c Magne, and Nils Reimers. 2023. MTEB: Massive Text Embedding Benchmark. In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics. 20142037. [23] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, and John Schulman. 2022. WebGPT: Browser-assisted question-answering with human feedback. arXiv:2112.09332 [cs.CL] https: //arxiv.org/abs/2112. [24] OpenAI. 2023. GPT-4 Turbo: Announcing New Models and Developer Products. https://openai.com/blog/new-models-and-developer-products-announcedat-devday. Accessed: 2024-01-16. [25] OpenAI. 2025. gpt-oss-120b & gpt-oss-20b Model Card. arXiv:2508.10925 [cs.CL] https://arxiv.org/abs/2508.10925 [26] Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A. Smith, and Mike Lewis. 2023. Measuring and Narrowing the Compositionality Gap in Language Models. arXiv:2210.03350 [cs.CL] https://arxiv.org/abs/2210.03350 [27] Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham. 2023. In-Context Retrieval-Augmented Language Models. In Transactions of the Association for Computational Linguistics, Vol. 11. 13161331. [28] Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al. 2024. Gemini 1.5: Unlocking Multimodal Understanding Across Millions of Tokens of Context. arXiv preprint arXiv:2403.05530 (2024). [29] Parth Sarthi, Salman Abdullah, Aditi Tuli, Shubh Khanna, Anna Goldie, and Christopher D. Manning. 2024. RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval. In International Conference on Learning Representations. [30] Timo Schick, Jane Dwivedi-Yu, Roberto DessÃ¬, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2023. Toolformer: Language Models Can Teach Themselves to Use Tools. arXiv:2302.04761 [cs.CL] https://arxiv.org/abs/2302. [31] Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. 2023. REPLUG: Retrieval-Augmented Black-Box Language Models. arXiv preprint arXiv:2301.12652 (2023). [32] Yixuan Tang and Yi Yang. 2024. MultiHop-RAG: Benchmarking RetrievalAugmented Generation for Multi-Hop Queries. arXiv:2401.15391 [cs.CL] https: //arxiv.org/abs/2401.15391 [33] Wenyu Tao, Xiaofen Xing, Yirong Chen, Linyi Huang, and Xiangmin Xu. 2025. TreeRAG: Unleashing the Power of Hierarchical Storage for Enhanced Knowledge Retrieval in Long Documents. In Findings of the Association for Computational Linguistics: ACL 2025. Association for Computational Linguistics, Vienna, Austria, 356371. doi:10.18653/v1/2025.findings-acl.20 [34] Qwen Team. 2025. Qwen3 Technical Report. arXiv:2505.09388 [cs.CL] https: //arxiv.org/abs/2505.09388 [35] Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. 2023. Interleaving Retrieval with Chain-of-Thought Reasoning for KnowledgeIntensive Multi-Step Questions. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. 1001410037. [36] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Åukasz Kaiser, and Illia Polosukhin. 2017. Attention is All You Need. In Advances in Neural Information Processing Systems. 59986008. [37] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher Manning. 2018. HotpotQA: Dataset for Diverse, Explainable Multi-hop Question Answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. 23692380. [38] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2023. ReAct: Synergizing Reasoning and Acting in Language Models. arXiv:2210.03629 [cs.CL] https://arxiv.org/abs/2210.03629 [39] Yanzhao Zhang, Mingxin Li, Dingkun Long, Xin Zhang, Huan Lin, Baosong Yang, Pengjun Xie, An Yang, Dayiheng Liu, Junyang Lin, Fei Huang, and Jingren Zhou. 2025. Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models. arXiv preprint arXiv:2506.05176 (2025). [40] Kunlun Zhu, Yifan Luo, Dingling Xu, Yukun Yan, Zhenghao Liu, Shi Yu, Ruobing Wang, Shuo Wang, Yishan Li, Nan Zhang, Xu Han, Zhiyuan Liu, and Maosong Sun. 2025. RAGEval: Scenario Specific RAG Evaluation Dataset Generation Framework. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar (Eds.). Association for Computational Linguistics, Vienna, Austria, 85208544. doi:10.18653/v1/2025.acl-long."
        }
    ],
    "affiliations": [
        "Qiyuan Tech Beijing, China"
    ]
}