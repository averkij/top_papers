{
    "paper_title": "From Word to World: Can Large Language Models be Implicit Text-based World Models?",
    "authors": [
        "Yixia Li",
        "Hongru Wang",
        "Jiahao Qiu",
        "Zhenfei Yin",
        "Dongdong Zhang",
        "Cheng Qian",
        "Zeping Li",
        "Pony Ma",
        "Guanhua Chen",
        "Heng Ji",
        "Mengdi Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Agentic reinforcement learning increasingly relies on experience-driven scaling, yet real-world environments remain non-adaptive, limited in coverage, and difficult to scale. World models offer a potential way to improve learning efficiency through simulated experience, but it remains unclear whether large language models can reliably serve this role and under what conditions they meaningfully benefit agents. We study these questions in text-based environments, which provide a controlled setting to reinterpret language modeling as next-state prediction under interaction. We introduce a three-level framework for evaluating LLM-based world models: (i) fidelity and consistency, (ii) scalability and robustness, and (iii) agent utility. Across five representative environments, we find that sufficiently trained world models maintain coherent latent state, scale predictably with data and model size, and improve agent performance via action verification, synthetic trajectory generation, and warm-starting reinforcement learning. Meanwhile, these gains depend critically on behavioral coverage and environment complexity, delineating clear boundry on when world modeling effectively supports agent learning."
        },
        {
            "title": "Start",
            "content": "From Word to World: Can Large Language Models be Implicit Text-based World Models? Yixia Li1,2, Hongru Wang3, (cid:66), Jiahao Qiu4, Zhenfei Yin5, Dongdong Zhang2, Cheng Qian6, Zeping Li7, Pony Ma8, Guanhua Chen1, (cid:66), Heng Ji6, Mengdi Wang4 1Southern University of Science and Technology 2Microsoft Research 3University of Edinburgh 4Princeton University 5Oxford University 6University of Illinois Urbana-Champaign 7Fudan University 8Mind Lab (cid:66)Corresponding Authors Code: https://github.com/X1AOX1A/Word2World Agentic reinforcement learning increasingly relies on experience-driven scaling, yet real-world environments remain non-adaptive, limited in coverage, and difficult to scale. World models offer potential way to improve learning efficiency through simulated experience, but it remains unclear whether large language models can reliably serve this role and under what conditions they meaningfully benefit agents. We study these questions in text-based environments, which provide controlled setting to reinterpret language modeling as next-state prediction under interaction. We introduce three-level framework for evaluating LLM-based world models: (i) fidelity and consistency, (ii) scalability and robustness, and (iii) agent utility. Across five representative environments, we find that sufficiently trained world models maintain coherent latent state, scale predictably with data and model size, and improve agent performance via action verification, synthetic trajectory generation, and warm-starting reinforcement learning. Meanwhile, these gains depend critically on behavioral coverage and environment complexity, delineating clear boundry on when world modeling effectively supports agent learning. 5 2 0 2 1 ] . [ 1 2 3 8 8 1 . 2 1 5 2 : r Figure 1 LLMs as text-based world models for agent learning. (A) We formulate world modeling as next-state prediction under fixed text-based interaction protocol. (B) Assess world-model capability along three axes: fidelity/consistency, scalability/robustness, and agent utility. (C) World model exhibits high fidelity and consistency in both single-step predictions and long-horizon rollouts. (D) Performance scales predictably with increased training data across text environments. (E) Faithful world models enhance agents via verification, synthetic data generation, and improved reinforcement learning through stronger initialization. From Word to World: Can Large Language Models be Implicit Text-based World Models? 1. Introduction Recent progress in agentic reinforcement learning (RL) increasingly hinges on experience-driven scaling: as agents improve, further gains demand larger, more diverse, and more challenging environments (Tong et al., 2025; Zeng et al., 2025; Zhang et al., 2025a). Unlike static pretraining corpora, however, experience must be collected through interaction. As agents scale, this reliance exposes fundamental experience bottleneck: realistic environments are non-adaptive, difficult to scale, and limited in coverage, which become key limiting factors for progress in agentic RL (Guo et al., 2025; Jiang et al., 2025; Wei et al., 2025). key lever for alleviating this bottleneck is world modeling (Hafner et al., 2024, 2025; Hu et al., 2025a; Zhao et al., 2025). Effective learning from interaction requires agents to maintain internal models of the environment that track latent state, predict action outcomes, and learning beyond immediate observations. By internalizing environment dynamics and enabling learning from imagined interaction, world models help close the interaction loop and enable more efficient and effective learning. Meanwhile, LLM trained at massive scale via next-token prediction, exhibit strong generalization and encode rich world knowledge (Grattafiori et al., 2024; Hu et al., 2025b; Qwen et al., 2025). This naturally raises the question: Can large language models serve as effective world models, thereby improving agents to learn from experience? While prior work has explored LLMs as simulators, experience generators, or planning interfaces (Chen et al., 2025; Gu et al., 2025; He et al., 2025; Li et al., 2025b; Wang et al., 2025; Wu et al., 2025), it remains unclear how to learn world model and when it is reliable enough to improve downstream agents. useful world model must go beyond locally plausible text, maintaining coherent state over time, remaining robust to distribution shift, and providing measurable utility. To address these questions in controlled setting, we focus on text-based environments as unifying interface between language modeling and world modeling. This abstraction preserves core challenges of agentic-environment interaction while reframing the objective from next-token prediction to next-state prediction under fixed interaction protocol. Through this lens, we formalize three-level framework for characterizing world modeling capabilities in agent learning. Fidelity and Consistency: Examine whether world model maintains coherent latent states over short and long horizons, forming the foundation for reliable world modeling. Scalability and Robustness: Characterize how world modeling capabilities scale and generalize under varying conditions, providing practical guidance for world model development. Agent Utility: Evaluate whether high-fidelity world models translate into measurable improvements for downstream agents. Studying five representative text-based environments, our analysis yields three main findings: (i) LLMs can function as reliable world models: they exhibit internal latent dynamics that support in-context world modeling, and supervised fine-tuning substantially improves short-term predictive fidelity and enables consistent long-horizon rollouts in well-structured domains. (ii) The development of reliable world models requires systematic scaling of model capacity and data volume with environment complexity, and maintaining robustness to distribution shift through broad behavioral coverage and diverse environment exposure. (iii) Fidelity world models provide practical benefits to agents by enabling verification of high-stakes actions to prevent irreversible failures, and by generating synthetic trajectories and warm-starting reinforcement learning to improve learning efficiency and effectiveness. 2 From Word to World: Can Large Language Models be Implicit Text-based World Models? Taken together, these findings illuminate both the promise and the limits of LLM-based world models in text environments. From words to worlds, from next-token to next-state prediction, we provide an empirical foundation for treating LLMs as general-purpose world models for agentic learning and chart path toward domains beyond text. 2. Related Works Large language models have recently been explored as world models across variety of text-based and structured settings. Prior efforts in world modeling largely focus on predicting environment dynamics through structured or discrete state representations. Patch-based approaches prompt LLMs to estimate state deltas in ByteSized32 (Wang et al., 2024; Yang et al., 2024), while in web navigation, systems such as WMA (Chae et al., 2025) and RLVR-World (Wu et al., 2025) reason over updates to the Accessibility Tree. Other lines of work adopt closed-form prediction schemes where the model outputs predefined symbolic labels or categories, including preconditions and effects in cooking environments (Xie et al., 2024), disaster impact ratings (Li et al., 2025a), or classifier-head predictions trained on LLM embeddings (Yang et al., 2025). Although these methods illustrate the utility of structured prediction for specific settings, they generally depend on environment-specific abstractions and fixed output space tailored to particular domains. In contrast, we formulate world modeling as multi-turn natural language simulation task, where the LLM generates next-state transitions in free text, enabling more general and compositional interaction patterns. Regarding model adaptation, much prior work employs zero-shot or few-shot prompting (Li et al., 2025a; Wang et al., 2024; Yang et al., 2024; Zuo et al., 2025) or attaches lightweight classifier heads for closed-form prediction (Yang et al., 2025). While such settings highlight the latent capabilities of LLMs, they often yield limited accuracy and constrains their applicability in downstream tasks. Moving beyond prompting-based adaptations, we finetune LLMs on large-scale multi-turn interaction trajectories to better internalize environment dynamics over extended horizons. Prior evaluation efforts largely center on single-step prediction accuracy in limited environments and domains (Chae et al., 2025; Li et al., 2025a; Wang et al., 2024; Xie et al., 2024) , and rarely examines long-horizon consistency or compounding errorsfactors that are critical for using world models as reliable simulators. Consequently, it remains an open question whether LLM-based world models can produce coherent multi-step trajectories that are executable in real environments. To address this gap, we conduct systematic evaluation across five representative environments, measuring not only one-step fidelity but also rollout stability, WM-to-Real transfer, and generalization across agents, environments, and scales. 3. LLM as Text-based World Models 3.1. Formalization of World Models We formalize the interaction between an agent and text-based world model as multi-turn language-based decision process, where both perception and action are represented in natural language. Agent text-based agent operates in ReAct style (Yao et al., 2023b), yielding simple, unified interface where each step involves internal reasoning and external action. Formally, the agent is defined as: : {0, (, 答, ) =1 } (, 答), (1) 3 From Word to World: Can Large Language Models be Implicit Text-based World Models? where  denotes the textual observation (or environment response) at step ,  represents the agents internal reasoning trace, and 答 denotes the explicit action expressed in natural language. World Model The environment or its surrogate world model defines the complementary mapping: : {0, ( 答,   )1 =1 , 答} ( ,  ), (2)  denotes the next state predicted by the world model, and  where   {0, 1} is binary reward indicating task success or termination. value of   = 1 corresponds to successful completion, while   = 0 denotes either an unfinished or failure state (e.g., triggering validation at the wrong time). Through these textual transitions, the world model functions as an implicit next-state predictor of environment dynamics. This capability can be realized through in-context learning, where the model leverages few-shot examples of state transitions in its prompt, or through supervised fine-tuning on trajectory data to learn the underlying dynamics. Note that in practice, text-based environments are inherently POMDPs (Partially Observable Markov Decision Processes): the true environment state is richer than what is described to the agent in text. For example, in ALFWorld room may contain objects and spatial details that are never mentioned (e.g., what is inside closed drawer), yet these hidden factors matter for predicting how the world evolves. Thus, although the agent only receives partial view of the initial state 0, the world model can be initialized with more complete context such as full environment configurations or randomized setups, allowing it to better approximate the latent dynamics of the environment. Interactive Process Together, the agent and world model form an iterative process: ,    = (cid:0)A (0, (, 答,  =1 )(cid:1),  )1 which unrolls into multi-turn textual trajectory generated within the world model: Correspondingly, the real environment produces the trajectory wm = {0, 1, 1,  1 , . . . ,  , 答 ,   }. real = {0, 1, 1, 1, . . . ,  , 答 ,  }, (3) (4) (5) which serves as the reference for evaluating the fidelity and consistency of W. By formulating text-based environments as multi-turn interactive processes, the world model can be prompted with few-shot examplars or trained on real trajectories real to predict next-state transitions. This formulation enables to capture long-horizon dependencies and cumulative effects across interaction steps. While prior works (Wang et al., 2024; Xie et al., 2024; Yang et al., 2025) primarily focus on next-state prediction accuracy, we explicitly model, train and evaluate the world models long-horizon consistency, which is critical for applications such as data synthesis, test-time simulator, and model-based reinforcement learning. 3.2. Text-based Environments To examine the range of knowledge and dynamics required for text-based world modeling in broad way, we adopt five representative environments spanning both structured and open-ended settings. The structured environments ALFWorld (Shridhar et al., 2021), SciWorld (Wang et al., 4 From Word to World: Can Large Language Models be Implicit Text-based World Models? 2022), and TextWorld (C么t茅 et al., 2018) feature bounded state spaces. They provide deterministic or rule-governed transitions grounded in embodied, scientific, or narrative regularities. In contrast, the open-ended environments WebShop (Yao et al., 2023a) and StableToolBench (Guo et al., 2025) exhibit broad, compositional, and open-world dynamics, with diverse entities and flexible task formulations that require stronger generalization beyond fixed schemas. Table 5 in Appendix A.1 summarizes these environments and their key characteristics, with examples in Appendix B. Together, these settings provide comprehensive and diversified testbed for evaluating language models as text-based world simulators. 4. World Model Training and Evaluation We summarize the world model training and evaluation setup and defer full implementation details to Appendix A.1. Unless otherwise specified, all experiments follow the default settings described in this section. Data We collect interaction trajectories using GPT-4o as the behavior policy. To match environment complexity (see Section 6.1), we gather 40K trajectories each for ALFWorld, SciWorld, and TextWorld, and 70K for WebShop. We retain both successful and failed episodes to broaden behavioral coverage for world model training.1 For StableToolBench, we use the public single-turn dataset with 160K samples. Initialization Context For ALFWorld and SciWorld, the world model is provided with full initial state descriptions (see Figures 8 and 9 in Appendix A.4). In contrast, TextWorld does not expose complete initial states, and WebShop/StableToolBench are inherently partially observable. This setting places greater demands on history-based state tracking and prior knowledge to infer unobserved state variables. Finetuning Models We use Qwen2.5-7B (base) and Llama-3.1-8B (base) as backbone models for text-based world modeling. comparison across different model sizes is provided in Section 6.2. Training Each trajectory is formatted as multi-turn dialogue of alternating agent actions and environment responses (see Eq. 5). During supervised fine-tuning, the world model predicts the next environment response conditioned on the dialogue history and the current action. , 答} and predicting the next state and reward ( Metrics We evaluate world models along two dimensions: one-step prediction fidelity and multi-step rollout consistency. Fidelity. We compute exact-match (EM) accuracy by conditioning on real trajectory prefix {0, ( 答, )1 ). prediction =1 is correct if ( ,  ) matches the ground-truth (, ). For TextWorld, EM is conservative lower bound since multiple surface forms can describe the same underlying state. For StableToolBench, whose outputs are highly open-ended, we additionally report word-level F1. Consistency. We report: (1) Real: success rate in the real environment; (2) WM: success rate inside the world model; (3) W2R: success rate when replaying WM actions in the real environment; and (4) Consistency Ratio: CR = W2R/Real, where higher values indicate better long-horizon transfer (CR may exceed 1 when world model rollouts are more successful than real world). ,  1The success/failure mixture is induced by GPT-4os native success rate without additional filtering. 5 From Word to World: Can Large Language Models be Implicit Text-based World Models?"
        },
        {
            "title": "Environment",
            "content": "AW SW TW WS"
        },
        {
            "title": "STB",
            "content": "STBF1 Zero-shot"
        },
        {
            "title": "45.20\nGPT-4o-mini\n44.45\nGPT-4o\n42.64\nGPT-4-turbo\n43.56\nGPT-4.1\n35.09\nGPT-5\nGemini-2.5-flash\n50.00\nClaude-sonnet-4.5 64.73",
            "content": "40.68 45.78 34.14 35.65 13.06 44.81 56.83 0.36 7.86 0.00 0.00 9.20 3.51 17.70 Few-shot (3 shot)"
        },
        {
            "title": "63.79\nGPT-4o-mini\n56.88\nGPT-4o\n62.56\nGPT-4-turbo\n63.37\nGPT-4.1\n67.13\nGPT-5\nGemini-2.5-flash\n61.85\nClaude-sonnet-4.5 77.04",
            "content": "56.26 48.98 50.08 51.56 49.44 61.20 73.08 11.43 14.11 11.66 13.39 44.27 40.35 49.12 56.59 58.20 52.45 58.07 46.12 57.64 58.80 61.93 64.62 62.76 64.23 65.90 66.09 56."
        },
        {
            "title": "SFT",
            "content": "0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 13.94 11.88 12.64 12.83 8.02 8.74 11.36 13.44 11.08 10.72 10.33 6.28 8.47 13.11 Qwen2.5-7B Llama3.1-8B 99.87 99. 98.60 98.64 70.60 70.45 79.05 77.24 48.90 49.25 79.15 78.97 Table 1 Next-state prediction EM accuracy (%) of prompt-based and finetuned models across five environments. AW, SW, TW, WS and STB denote ALFWorld, SciWorld, TextWorld, WebShop and StableToolBench, respectively. STBF1 denotes the word-level F1 score for StableToolBench, given its open-ended output space. 5. Fidelity & Consistency 5.1. Next-state Prediction Fidelity Table 1 demonstrates that pretrained LLMs exhibit meaningful in-context world modeling ability. Models such as Gemini-2.5-flash and Claude-sonnet-4.5 achieve strong next-state prediction in structured environments like ALFWorld and SciWorld, where handful of demonstrations provides substantial improvements (e.g., Claude rises from 56.83 to 73.08 accuracy on SciWorld with only three examples). This suggests that contemporary LLMs encode latent knowledge of environment dynamics and can rapidly adapt their transition rules with minimal supervision. However, these capabilities do not fully transfer to open-ended settings such as WebShop, where few-shot prompting plateaus around mid-50s, indicating that implicit world knowledge alone is insufficient for generating unconstrained, context-dependent state updates. Supervised fine-tuning yields substantial improvements. Open-source models trained directly on transition trajectories achieve 99%/98% accuracy on ALFWorld and SciWorld and reach 49% F1 on StableToolBench. These results indicate that robust world modeling requires dynamics-aligned training: prompting alone cannot capture the full diversity of transition patterns, whereas supervised fine-tuning enables even relatively small models to internalize them effectively. Findings 1: [Short-term Fidelity] LLMs exhibit internal latent dynamics that support in-context world modeling, while high-fidelity transition modeling depends on dynamics-aligned fine-tuning. From Word to World: Can Large Language Models be Implicit Text-based World Models?"
        },
        {
            "title": "SciWorld",
            "content": "Real WM W2R CR Real WM W2R CR"
        },
        {
            "title": "Real",
            "content": "TextWorld WM W2R"
        },
        {
            "title": "WebShop",
            "content": "CR Real WM W2R CR Qwen2.5-7B WorldModel 7.69 7.69 1.00 12.64 12.04 7.69 GPT-4o-mini 55.90 57.44 0.99 34.97 37.63 58.00 GPT-4o 74.21 62.56 64.62 0.87 36.79 50.00 GPT-4-turbo 68.56 69.59 1.04 43.41 45.79 67.20 GPT-4.1 84.62 86.67 0.95 68.21 64.10 91.00 GPT-5 51.79 52.31 1.04 56.00 39.49 Gemini-2.5-flash 50.50 76.00 76.00 0.93 66.00 45.64 Claude-sonnet-4.5 82.00 58.16 59.19 0.96 45.43 42.10 61.51 Average 4.85 5.99 97.44 100.00 69.36 0.71 0.97 0.16 8.90 0.70 98.84 100.00 96.53 0.98 29.36 17.43 16.51 0.56 31.44 0.90 36.60 0.99 100.00 99.42 98.84 0.99 17.73 14.89 11.70 0.66 46.32 1.07 100.00 100.00 100.00 1.00 21.14 12.22 12.22 0.58 61.03 0.89 100.00 100.00 100.00 1.00 51.00 33.03 31.19 0.61 45.64 0.82 100.00 100.00 76.30 0.76 25.00 21.10 18.35 0.73 57.95 0.88 100.00 100.00 100.00 1.00 61.00 49.00 50.00 0.82 99.92 91.58 0.92 30.17 21.79 20.13 0.67 41.13 0.91 99.47 Llama3.1-8B WorldModel 9. 9.74 1.27 12.64 10.78 7.69 GPT-4o-mini 58.00 58.46 56.92 0.98 34.97 37.63 GPT-4o 74.21 67.53 67.01 0.90 36.79 52.31 GPT-4-turbo 68.72 68.21 1.02 43.41 45.13 67.20 GPT-4.1 82.56 81.54 0.90 68.21 63.07 91.00 GPT-5 53.33 53.33 1.06 56.00 57.95 Gemini-2.5-flash 50.50 84.00 84.00 1.02 66.00 58.46 Claude-sonnet-4.5 82.00 60.62 60.11 0.98 45.43 46.48 61.51 Average 97.44 8.21 0.65 32.99 0.94 98.84 44.10 1.20 100.00 35.38 0.82 100.00 57.44 0.84 100.00 52.31 0.93 100.00 53.33 0.81 100.00 99.47 30.52 0.89 2.75 92.48 57.80 0.59 0.00 0.00 5.99 97.11 90.17 0.91 29.36 23.81 22.62 0.77 97.69 93.06 0.93 17.73 25.47 17.92 1.01 98.27 94.22 0.94 21.14 19.27 17.43 0.82 98.84 94.80 0.95 51.00 31.19 30.28 0.59 99.42 93.06 0.93 25.00 22.02 17.43 0.70 93.33 90.00 0.90 61.00 60.00 55.00 0.90 96.73 87.59 0.88 30.17 26.36 22.95 0.76 Table 2 Task success rate (%) of different agents across four environments. Real, WM, and W2R denote the success rate under real environment, world model, and world model-to-real execution. The last column reports the consistency ratio (CR=W2R/Real), with higher values (darker green color) indicating better rollout fidelity. 5.2. Rollout Consistency reliable world model requires not only high single-step prediction accuracy, but more critically, the ability to maintain consistency during extended interactions with agents. We examine two key dimensions: (1) whether small local errors compound into significant failures over long-horizon rollouts, and (2) whether the world model generalizes across different agent behaviors beyond its training distribution. Table 2 reports consistency metrics across four environments and multiple agents; StableToolBench is omitted due to its single-turn nature. Consistency Across Environments World models largely preserve single-step fidelity in longhorizon rollouts, especially in structured environments. In ALFWorld, SciWorld, and TextWorld, the fine-tuned Qwen2.5 world model attains high consistency ratios of 96%, 91%, and 92%, indicating that multi-step trajectories generated within the world model remain executable when transferred to the real environment. WebShop, however, exhibits lower consistency (typically below 80%), primarily due to its open-ended nature and diverse search results that the world model struggles to simulate accurately. This error can be substantially mitigated by grounding model rollouts with real observations. When the rollout is initialized with real search results, the consistency with GPT-4o agent increases dramatically from 56% to nearly 100%, demonstrating that partial real-environment anchoring effectively reduces simulation drift. Findings 2: [Long-term Consistency] World models maintain consistent long-horizon rollouts in well-structured domains, but tend to drift in open-ended environments due to high diversity, necessitating anchoring to real-world signals. How does behavior shift affect consistency? Beyond environment-specific factors, world model consistency also depends on how well agent behaviors match the training distribution. Lower-capacity agents such as GPT-4o-mini yield consistency ratios frequently below 70%, whereas stronger agents 7 From Word to World: Can Large Language Models be Implicit Text-based World Models? Figure 2 Next-state prediction accuracy under varying training data sizes on Qwen2.5-7B. Structured settings saturate with modest data (20K), whereas open-ended settings continue to benefit from larger datasets. Note. We apply nonlinear y-axis transform  ( ) = 100 20 log10 (max(100 , 0.01) + 1) to better reveal growth trends. Figure 3 Next-state prediction accuracy on Qwen2.5 family. Smaller models (1.5B) capture structured dynamics effectively, whereas more complex settings benefit markedly from increased model capacity. like GPT-4.1, GPT-5, and Claude reliably exceed 90%. This disparity stems from weaker agents taking actions misaligned with task objectives, causing their trajectories to drift outside the training distribution. In contrast, higher-capacity agents preserve goal-directed behavior that aligns with the expert policy (GPT-4o) used for trajectories sampling, enabling higher consistency. These results highlight the importance of diversifying training trajectories rather than relying solely on single strong agent, as further discussed in Section 6.5. 6. Scalability & Robustness 6.1. Data Scaling Laws for World Models To investigate how world model performance scales with data, we vary training trajectories from 1K to 160K and evaluate single-step accuracy. As shown in Figure 2, structured environments (ALFWorld, SciWorld, TextWorld) improve rapidly and saturate around 20K trajectories, consistent with their low-entropy, rule-driven dynamics. In contrast, open-ended environments scale more gradually: WebShop benefits from additional data up to roughly 70K trajectories, while StableToolBench shows no saturation at 160K samples due to long-tail linguistic variation and highly compositional API behaviors. These results indicate that world modeling exhibits environment-dependent scaling: structured environments are highly data-efficient, whereas open-ended domains require substantially larger datasets. 6.2. Model Size Effects We next analyze how model capacity shapes world model performance (Figure 3). Mirroring datascaling trends, model size interacts strongly with environment complexity. In structured environments, performance saturates quickly: 1.5B models already capture core transition dynamics, with further 8 From Word to World: Can Large Language Models be Implicit Text-based World Models? Figure 4 Task success rate (%) in ALFWorld under different OOD settings. Success rate averaged over different agents, with full results provided in Table 10 of Appendix C. World models maintain strong performance even when layouts or room types change. scaling yielding only marginal improvements. In open-ended environments, however, capacity matters substantially. Smaller models struggle to represent rich linguistic variability and compositional tool usage, whereas larger models offer steady accuracy gains. Together with the data-scaling results, these findings indicate that success in open-ended world modeling requires both extensive trajectories and sufficient model capacity to internalize long-tailed, high-entropy dynamics. Findings 3: [LLMs are Scalable World Models] World modeling performance scales systematically with data volume, model size and environment complexity. 6.3. Beyond Memorized Environments central question in world model design is how well they generalize across unseen settings. Using ALFWorld as representative case, we analyze two out-of-distribution test splits following the original environment settings (Shridhar et al., 2021): OOD-Seen, which keeps the room type but alters the layout, and OOD-Unseen, which introduces entirely new room types or unseen layout configurations. As shown in Figure 4, the world models maintain success rates closely aligned with the real environment across both OOD settings even when the spatial configuration shifts or novel room types appear. These results indicate that the LLM world model captures transferable transition dynamics rather than memorizing specific layouts, demonstrating strong robustness to structural variations in environment state space. 6.4. Cross-Env Transfer via Joint Training Training world models in isolation often limits their ability to generalize beyond single environment, motivating us to investigate whether jointly training on multiple environments can yield transferable gains. We therefore evaluate three mixed-training configurations: Mix3 (ALFWorld, SciWorld, TextWorld), Mix4 (with WebShop), and Mix5 (with StableToolBench), allocating 1K trajectories per environment to match the data budget of individually trained models. As shown in Figure 5, mixed training consistently accelerates learning and improves final accuracy, with particularly strong gains in TextWorld and WebShop, suggesting that the model effectively internalizes and reuses shared physical, procedural, and narrative dynamics across tasks. The exception is StableToolBench, whose schema-centric, single-turn structure is underrepresented in the mixture, causing separately trained model to outperform. Overall, these results show that mixed data provides stable positive gains and, importantly, enables practical deployments where single world model can robustly serve multiple environments. From Word to World: Can Large Language Models be Implicit Text-based World Models? Figure 5 Next-state prediction accuracy under mixed and separate training on Qwen2.5-7B, with 1K samples per environment. We begin by mixing structured environments (ALFWorld, SciWorld, TextWorld) and then progressively incorporate open-ended environments (WebShop, StableToolBench), yielding the Mix3, Mix4, and Mix5 settings."
        },
        {
            "title": "Agent",
            "content": "Single Agent Traj Mix Agent Traj WM W2R CR WM W2R CR 24.00 24.00 18.00 0.75 26.00 18.00 0.75 Qwen3-235B 34.97 32.31 26.67 0.76 32.31 26.67 0.76 GPT-4o 68.21 55.38 59.49 0.87 69.74 60.00 0.88 GPT-5 66.00 40.51 57.44 0.87 52.31 49.74 0.75 Claude-sonnet-4.5 48.30 38.05 40.40 0.81 45.09 38.60 0.79 ID Agent Average 12.64 5.64 6.15 0.49 13.39 10.26 0.81 GPT-4o-mini 36.79 32.31 38.97 1.06 51.28 42.56 1.16 GPT-4-turbo 43.41 28.72 36.41 0.84 52.31 36.41 0.84 GPT-4.1 Gemini-2.5-flash 56.00 36.92 51.79 0.92 56.92 45.64 0.82 OOD Agent Average 37.21 25.90 33.33 0.83 43.48 33.72 0.91 Table 3 Task success rate (%) in SciWorld under different training data compositions. Single Agent Traj uses only 4K GPT-4o trajectories for training, whereas Mix Agent Traj combines trajectories from ID agents, with 1K trajectories from each. 6.5. Behavioral Coverage for Robust World Modeling As behavior shifts reduce consistency, we ask whether broader behavioral coverage improves generalization. We train world model on mixed-agent trajectories and compare it to GPT-4o-only baseline. Table 3 shows marked OOD gains for weaker agents: GPT-4o-minis consistency ratio rises from 0.49 to 0.81, and GPT-4-turbo also improves. This indicates that expert-only trajectories are insufficient under distribution shift; incorporating diverse agent behaviors is crucial for improving generalization and rollout stability. Findings 4: [World Models Learn Generalizable Dynamics] World models generalize beyond memorized configurations and transfer across environments and agent policies, where coverage of environments and diversity of agent behaviors play critical roles. 7. Agent Utility 7.1. Can World Models Prevent Irreversible Mistakes? In real-world decision-making, some actions are irreversible and costly, creating safety bottleneck: single mistaken commitment can end an episode or cause unrecoverable loss. This motivates using world models as rewindable imagined world to evaluate high-stakes actions before execution. WebShop exemplifies this setting: once the agent checks out, the episode ends and errors cannot be 10 From Word to World: Can Large Language Models be Implicit Text-based World Models?"
        },
        {
            "title": "Agent",
            "content": "0 2 4 10 50 7.55 (+1.56) 7.50 (+1.51) 5.99 7.59 (+1.60) GPT-4o-mini 29.36 32.41 (+3.05) 33.94 (+4.58) 34.86 (+5.50) 36.70 (+7.34) GPT-4o 17.73 33.33 (+15.60) 27.05 (+9.32) 29.37 (+11.64) 25.60 (+7.87) GPT-4-turbo 21.14 23.59 (+2.45) 23.59 (+2.45) 23.08 (+1.94) 25.13 (+3.99) GPT-4.1 51.00 53.27 (+2.27) 53.77 (+2.77) 53.27 (+2.77) 51.50 (+0.50) GPT-5 25.00 31.00 (+6.00) 29.50 (+4.50) 28.00 (+3.00) 27.50 (+1.50) Gemini-2.5-flash Claude-sonnet-4.5 61.00 62.00 (+1.00) 65.00 (+4.00) 64.00 (+3.00) 62.00 (+1.00) 7.59 (+1.60) Table 4 Task success rate (%) of different agents in WebShop with varying numbers of max preexecution verification attempts using the world model. The numbers in parentheses indicate the improvement over the baseline without verification. undone. We therefore use the world model as lightweight pre-execution verifier. Before committing to checkout, the agent simulates the outcome; it executes the action only when the prediction indicates success, otherwise it continues interacting with the environment. We vary the verification budget (0, 2, 4, 10, 50). As shown in Table 4, verification improves success rates for all agents, with the largest gains for medium-capacity models. However, returns are not monotonic, since repeated verification changes the trajectory context and shifts the agents action distribution, inducing distribution shift that can weaken alignment between imagined and real outcomes. In practice, moderate budgets (e.g., 210 checks) provide the best trade-off, reducing irreversible failures without destabilizing behavior. Findings 5: [As Rewindable Safety Verifier] By simulating irreversible actions and gating execution on predicted success, world models help agents avoid costly commitments and achieve higher task success. 7.2. Synthetic Data Competes with Real When real interaction is expensive, slow, or constrained, agents face an experience bottleneck. world model can potentially alleviate this bottleneck by synthesizing trajectories that substitute for portion of real experience. To examine this, we collect 1,000 successful trajectories from either the real environment or the world model2, and construct four SFT datasets for Qwen2.5-7B-Instruct: Real 1K, Syn 1K, 0.5K+0.5K, and 1K+1K mixture, all trained under identical procedures. Figure 6 shows that world modelgenerated trajectories are highly competitive with real data. In SciWorld, Syn 1K matches Real 1K, while the 1K+1K mixture outperforms either source alone. In WebShop, synthetic data remains similarly effective, and mixed regimes yield the most stable gains. Overall, these results suggest that synthetic experience can reduce reliance on real-environment interaction, providing an alternative pathway for scaling agent learning when real experience is limited. 7.3. Early Experience for Policy Learning Recent work (Zhang et al., 2025b) suggests that exposing model to environment dynamics before explicit policy learning can provide useful inductive bias: anticipating consequences may reduce unguided exploration and stabilize early RL. To study this in our text-based decision environments, we compare (1) standard Agent-SFT RL baseline; and (2) world-model warmup pipeline (WM-SFT Agent-SFT RL), where the agent is first exposed to environment dynamics with same objective 2Implementation details in Appendix A. 11 From Word to World: Can Large Language Models be Implicit Text-based World Models? Figure 6 Task success rate (%) of Qwen2.5-7BInstruct SFT trained agents with different data synthesis strategies in SciWorld and WebShop. Figure 7 Task success rate (%) of Qwen2.5-7BInstruct RL trained agents with and without early experience in ALFWorld and SciWorld. as world model training3. Figure 7 indicates that early experience delivers consistent gains on both ALFWorld and SciWorld. By exposing the agent to environment dynamics before policy learning, early experience stabilizes RL training, reducing failures driven by incorrect commonsense priors, and ultimately yields higher final success rates than the baseline. Overall, early experience provides promising direction for improving learning effectiveness. Findings 6: [Learning from World-Model Dynamics] By providing both scalable supervision and early dynamics exposure, world models reduce reliance on real interaction and improve learning efficiency, enabling agents to learn more effectively from experience. 8. Conclusion This work set out to investigate simple yet far-reaching question: can the paradigm that enables large language models to model words also enable them to model worlds, and in turn support more effective agent learning from experience? Using text-based environments as controlled testbed, we recast world modeling as multi-turn next-state prediction under interaction, and introduce systematic framework for evaluating fidelity, scalability, and agent utility. Our results provide strong evidence that LLMs can serve as implicit text-based world models. When trained with dynamics-aligned supervision at sufficient scale and coverage, they maintain coherent latent state over extended horizons and yield tangible benefits for downstream agents, including safer decision making, scalable experience generation, and improved learning efficiency. At the same time, these gains are not universal: robustness depends critically on behavioral coverage, distributional alignment, and environment complexity, delineating concrete regimes in which world modeling meaningfully supports agent learning. Taken together, these findings establish an empirical foundation for treating LLMs not merely as sequence predictors, but as learned simulators of interactive worlds. By bridging next-token prediction with next-state modeling, this work points toward unifying view of language models as world models for agentsand opens the door to extending these ideas beyond text to richer, multimodal, and embodied domains. 3Implementation details in Appendix A. 12 From Word to World: Can Large Language Models be Implicit Text-based World Models?"
        },
        {
            "title": "References",
            "content": "H. Chae, N. Kim, K. T. iunn Ong, M. Gwak, G. Song, J. Kim, S. Kim, D. Lee, and J. Yeo. Web agents with world models: Learning and leveraging environment dynamics in web navigation, 2025. URL https://arxiv.org/abs/2410.13232. Z. Chen, Z. Zhao, K. Zhang, B. Liu, Q. Qi, Y. Wu, T. Kalluri, S. Cao, Y. Xiong, H. Tong, H. Yao, H. Li, J. Zhu, X. Li, D. Song, B. Li, J. Weston, and D. Huynh. Scaling agent learning via experience synthesis, 2025. URL https://arxiv.org/abs/2511.03773. M.-A. C么t茅, A. K谩d谩r, X. Yuan, B. Kybartas, T. Barnes, E. Fine, J. Moore, R. Y. Tao, M. Hausknecht, L. E. Asri, M. Adada, W. Tay, and A. Trischler. Textworld: learning environment for text-based games. CoRR, abs/1806.11532, 2018. A. Grattafiori, A. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-Dahle, A. Letman, A. Mathur, A. Schelten, A. Vaughan, A. Yang, A. Fan, A. Goyal, A. Hartshorn, A. Yang, A. Mitra, A. Sravankumar, A. Korenev, A. Hinsvark, A. Rao, A. Zhang, A. Rodriguez, A. Gregerson, A. Spataru, B. Roziere, B. Biron, B. Tang, B. Chern, C. Caucheteux, C. Nayak, and C. B. et al. The llama 3 herd of models, 2024. URL https://arxiv.org/abs/2407.21783. Y. Gu, K. Zhang, Y. Ning, B. Zheng, B. Gou, T. Xue, C. Chang, S. Srivastava, Y. Xie, P. Qi, H. Sun, and Y. Su. Is your llm secretly world model of the internet? model-based planning for web agents, 2025. URL https://arxiv.org/abs/2411.06559. Z. Guo, S. Cheng, H. Wang, S. Liang, Y. Qin, P. Li, Z. Liu, M. Sun, and Y. Liu. Stabletoolbench: Towards stable large-scale benchmarking on tool learning of large language models, 2025. URL https://arxiv.org/abs/2403.07714. D. Hafner, J. Pasukonis, J. Ba, and T. Lillicrap. Mastering diverse domains through world models, 2024. URL https://arxiv.org/abs/2301.04104. D. Hafner, W. Yan, and T. Lillicrap. Training agents inside of scalable world models, 2025. URL https://arxiv.org/abs/2509.24527. H. He, Y. Zhang, L. Lin, Z. Xu, and L. Pan. Pre-trained video generative models as world simulators. In ICLR 2025 Workshop on World Models: Understanding, Modelling and Scaling, 2025. URL https://openreview.net/forum?id=oTYF8WUadL. M. Hu, T. Chen, Y. Zou, Y. Lei, Q. Chen, M. Li, Q. Liang, Y. Mu, H. Zhang, W. Shao, and P. Luo. Text2world: Benchmarking large language models for symbolic world model generation. In ICLR 2025 Workshop on World Models: Understanding, Modelling and Scaling, 2025a. URL https: //openreview.net/forum?id=dIQNOxuBay. Z. Hu, J. Lian, Z. Xiao, S. Zhang, T. Wang, N. J. Yuan, X. Xie, and H. Xiong. Unveiling the learning mind of language models: cognitive framework and empirical study. arXiv preprint arXiv:2506.13464, 2025b. D. Jiang, Y. Lu, Z. Li, Z. Lyu, P. Nie, H. Wang, A. Su, H. Chen, K. Zou, C. Du, T. Pang, and W. Chen. Verltool: Towards holistic agentic reinforcement learning with tool use, 2025. URL https:// arxiv.org/abs/2509.01055. L. Li, D. Li, Z. Ou, X. Xu, J. Liu, Z. Ma, R. Yu, and M. Deng. Llms as world models: Data-driven and human-centered pre-event simulation for disaster impact assessment, 2025a. URL https: //arxiv.org/abs/2506.06355. 13 From Word to World: Can Large Language Models be Implicit Text-based World Models? Y. Li, H. A. Inan, X. Yue, W.-N. Chen, L. Wutschitz, J. Kulkarni, R. Poovendran, R. Sim, and S. Rajmohan. Simulating environments with reasoning models for agent training, 2025b. URL https://arxiv. org/abs/2511.01824. Qwen, :, A. Yang, B. Yang, B. Zhang, B. Hui, B. Zheng, B. Yu, C. Li, D. Liu, F. Huang, H. Wei, H. Lin, J. Yang, J. Tu, J. Zhang, J. Yang, J. Yang, J. Zhou, J. Lin, K. Dang, K. Lu, K. Bao, K. Yang, L. Yu, M. Li, M. Xue, P. Zhang, Q. Zhu, R. Men, R. Lin, T. Li, T. Tang, T. Xia, X. Ren, X. Ren, Y. Fan, Y. Su, Y. Zhang, Y. Wan, Y. Liu, Z. Cui, Z. Zhang, and Z. Qiu. Qwen2.5 technical report, 2025. URL https://arxiv.org/abs/2412.15115. M. Shridhar, X. Yuan, M.-A. C么t茅, Y. Bisk, A. Trischler, and M. Hausknecht. Alfworld: Aligning text and embodied environments for interactive learning, 2021. URL https://arxiv.org/abs/2010. 03768. J. Tong, J. Tang, H. Li, Y. Mou, M. Zhang, J. Zhao, Y. Wen, F. Song, J. Zhan, Y. Lu, C. Tao, Z. Guo, J. Yu, T. Cheng, Z. Xi, C. Jiang, Z. Yin, Y. Zheng, W. Ge, G. Chen, T. Gui, X. Qiu, Q. Zhang, and X. Huang. Game-rl: Synthesizing multimodal verifiable game data to boost vlms general reasoning, 2025. URL https://arxiv.org/abs/2505.13886. R. Wang, P. Jansen, M.-A. C么t茅, and P. Ammanabrolu. Scienceworld: Is your agent smarter than 5th grader?, 2022. URL https://arxiv.org/abs/2203.07540. R. Wang, G. Todd, Z. Xiao, X. Yuan, M.-A. C么t茅, P. Clark, and P. Jansen. Can language models serve as text-based world simulators? In L.-W. Ku, A. Martins, and V. Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 117, Bangkok, Thailand, Aug. 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-short.1. URL https://aclanthology.org/2024.acl-short.1/. S. Wang, Z. Fei, Q. Cheng, S. Zhang, P. Cai, J. Fu, and X. Qiu. World modeling makes better planner: Dual preference optimization for embodied task planning. In ICLR 2025 Workshop on World Models: Understanding, Modelling and Scaling, 2025. URL https://openreview.net/ forum?id=HCXshVxxdg. Z. Wei, W. Yao, Y. Liu, W. Zhang, Q. Lu, L. Qiu, C. Yu, P. Xu, C. Zhang, B. Yin, H. Yun, and L. Li. Webagent-r1: Training web agents via end-to-end multi-turn reinforcement learning, 2025. URL https://arxiv.org/abs/2505.16421. J. Wu, S. Yin, N. Feng, and M. Long. Rlvr-world: Training world models with reinforcement learning, 2025. URL https://arxiv.org/abs/2505.13934. Z. Xi, Y. Ding, W. Chen, B. Hong, H. Guo, J. Wang, D. Yang, C. Liao, X. Guo, W. He, S. Gao, L. Chen, R. Zheng, Y. Zou, T. Gui, Q. Zhang, X. Qiu, X. Huang, Z. Wu, and Y.-G. Jiang. Agentgym: Evolving large language model-based agents across diverse environments, 2024. Z. Xi, J. Huang, C. Liao, B. Huang, H. Guo, J. Liu, R. Zheng, J. Ye, J. Zhang, W. Chen, W. He, Y. Ding, G. Li, Z. Chen, Z. Du, X. Yao, Y. Xu, J. Chen, T. Gui, Z. Wu, Q. Zhang, X. Huang, and Y.-G. Jiang. Agentgym-rl: Training llm agents for long-horizon decision making through multi-turn reinforcement learning, 2025. URL https://arxiv.org/abs/2509.08755. K. Xie, I. Yang, J. Gunerli, and M. Riedl. Making large language models into world models with precondition and effect knowledge, 2024. C. Yang, X. Wang, J. Jiang, Q. Zhang, and X. Huang. Evaluating world models with llm for decision making, 2024. URL https://arxiv.org/abs/2411.08794. 14 From Word to World: Can Large Language Models be Implicit Text-based World Models? C. Yang, X. Wang, Q. Zhang, Q. Jiang, and X. Huang. Efficient integration of external knowledge to LLM-based world models via retrieval-augmented generation and reinforcement learning. In C. Christodoulopoulos, T. Chakraborty, C. Rose, and V. Peng, editors, Findings of the Association for Computational Linguistics: EMNLP 2025, pages 94849501, Suzhou, China, Nov. 2025. Association for Computational Linguistics. ISBN 979-8-89176-335-7. doi: 10.18653/v1/2025.findings-emnlp. 504. URL https://aclanthology.org/2025.findings-emnlp.504/. S. Yao, H. Chen, J. Yang, and K. Narasimhan. Webshop: Towards scalable real-world web interaction with grounded language agents, 2023a. URL https://arxiv.org/abs/2207.01206. S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. Narasimhan, and Y. Cao. React: Synergizing reasoning and acting in language models, 2023b. URL https://arxiv.org/abs/2210.03629. Z. Zeng, H. Ivison, Y. Wang, L. Yuan, S. S. Li, Z. Ye, S. Li, J. He, R. Zhou, T. Chen, C. Zhao, Y. Tsvetkov, S. S. Du, N. Jaques, H. Peng, P. W. Koh, and H. Hajishirzi. Rlve: Scaling up reinforcement learning for language models with adaptive verifiable environments, 2025. URL https://arxiv.org/ abs/2511.07317. J. Zhang, Y. Peng, F. Kong, C. Yang, Y. Wu, Z. Yu, J. Xiang, J. Ruan, J. Wang, M. Song, H. Liu, X. Tang, B. Liu, C. Wu, and Y. Luo. Autoenv: Automated environments for measuring cross-environment agent learning, 2025a. URL https://arxiv.org/abs/2511.19304. K. Zhang, X. Chen, B. Liu, T. Xue, Z. Liao, Z. Liu, X. Wang, Y. Ning, Z. Chen, X. Fu, J. Xie, Y. Sun, B. Gou, Q. Qi, Z. Meng, J. Yang, N. Zhang, X. Li, A. Shah, D. Huynh, H. Li, Z. Yang, S. Cao, L. Jang, S. Zhou, J. Zhu, H. Sun, J. Weston, Y. Su, and Y. Wu. Agent learning via early experience, 2025b. URL https://arxiv.org/abs/2510.08558. Y. Zhao, A. Scannell, Y. Hou, T. Cui, L. Chen, D. B眉chler, A. Solin, J. Kannala, and J. Pajarinen. Generalist world model pre-training for efficient reinforcement learning. In ICLR 2025 Workshop on World Models: Understanding, Modelling and Scaling, 2025. URL https://openreview.net/ forum?id=WtJnrr4BGO. Y. Zheng, R. Zhang, J. Zhang, Y. Ye, Z. Luo, Z. Feng, and Y. Ma. Llamafactory: Unified efficient finetuning of 100+ language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), Bangkok, Thailand, 2024. Association for Computational Linguistics. URL http://arxiv.org/abs/2403.13372. D. Zuo, Z. CHEN, C. Zhou, Y. Guo, X. He, and M. Gong. RADI: LLMs as world models for robotic action decomposition and imagination. In ICLR 2025 Workshop on World Models: Understanding, Modelling and Scaling, 2025. URL https://openreview.net/forum?id=cPo2iS6lwP. From Word to World: Can Large Language Models be Implicit Text-based World Models?"
        },
        {
            "title": "A Implementation Details",
            "content": "17 A.1 World Model Training and Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 A.2 Synthetic Data Competes with Real . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 A.3 Early Experience for Policy Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 A.4 World Model Initialization Context . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "D System Prompts for Agent Trajectory Collection",
            "content": "27 32 33 16 From Word to World: Can Large Language Models be Implicit Text-based World Models? A. Implementation Details A.1. World Model Training and Evaluation Environments We evaluate five text-based environments, including ALFWorld (Shridhar et al., 2021), SciWorld (Wang et al., 2022), TextWorld (C么t茅 et al., 2018), WebShop (Yao et al., 2023a), and StableToolBench (Guo et al., 2025). Table 5 summarizes these environments along four dimensions: the nature of the environment, the abilities required of an agent, the form of the underlying world state, and the modeling capabilities demanded of world model."
        },
        {
            "title": "Description",
            "content": "Required Agent Ability"
        },
        {
            "title": "World Model State",
            "content": "ALFWorld (Shridhar et al., 2021) Embodied environment where agents accomplish household tasks by issuing textbased commands. Spatial and physical commonsense, reasoning about containers and locations, and multi-step executions. SciWorld (Wang et al., 2022) TextWorld (C么t茅 et 2018) al., interacText-based laboratory tivate environment involving simplified physics & chemistry experiments. concepts, Scientific causal reasoning, experiment planning, hypothesis testing with outcome evaluation. openText-based environment world explosupporting ration, interaction, and diverse quest-like tasks. Environment understanding, open-ended temtask planning, tracking, and poral structured exploration. of Room layout with conhundreds comtainerobject binations, agent inventory, and task progression. interconnected Ten labs with 200 materials, intermediate substance states, and experiment progress."
        },
        {
            "title": "World",
            "content": "Track physical configurations, maintain object relations, and predict stable multi-step state transitions. Scientific modeling, reasoning, simulation, iment estimation. dynamics physical chemical experprogress Multiple connected rooms with 10 objects, exploration and discovery status, and task advancement. Long-horizon state prediction, symbolic transition feedback, and exploration progress estimation. WebShop (Yao et al., 2023a) Simulated shopping website where agents search, browse, and shop through multistep interactions. Goal decomposition, product evaluation, and robust reasoning over semidiverse structured attributes. Metadata for over 1M attributes, product search-query items surface, item details, and cart states. Simulation of search engines, multi-step web navigation, product attributes, and constraint satisfaction. StableToolBench (Guo et 2025) al., API-based tool-use environment requiring schema adherence and structured output generation. understanding, Doc symbolic reasoning, and executing schemaaction compliant sequences. Over 10K API tools, input/output schemas, intermediate tool-call states, and execution context. Symbolic world state simulation, doc understanding, schema constraint satisfaction, structured generation. Table 5 Summary of the five text-based environments used in our paper, highlighting the knowledge demands placed on both agents and world models. Task examples are provided in Figures 1418 in Appendix B. Data Sources and Sizes For ALFWorld, SciWorld and WebShop, we follow the data splits provided in AgentGym4 (Xi et al., 2024). For TextWorld, we follow the official TextWorld repository5 to generate game files and randomly split into 2.5K training games and 200 test games. For StableToolBench, 4https://github.com/WooooDyy/AgentGym 5https://github.com/microsoft/TextWorld 17 From Word to World: Can Large Language Models be Implicit Text-based World Models? we filtered the StableToolBench MirrorAPI dataset6 and remove samples with errors or incomplete information, and use 160K API pairs for training and 2K pairs for testing. The data sizes for different environments are summarized in Table 6. Trajectories Collection We utilize the AgentGym (Xi et al., 2024) framework to collect long-horizon interaction trajectories using GPT-4o as the agent across four interactive environments: ALFWorld, SciWorld, TextWorld, and WebShop. We maintain consistent system prompts (Appendix D), interaction protocols, environment configurations as in AgentGym. The sampling temperature is set to 1.0 with Top-p of 1.0, and maximum of 50 interaction turns per trajectory. System prompts used for trajectory collection are provided in Figure 19 to 23. Ultimately, we collect 40K trajectories each for ALFWorld, SciWorld, and TextWorld, and 70K trajectories for WebShop on their respective training sets, as summarized in Table 6."
        },
        {
            "title": "ALFWorld\nSciWorld\nTextWorld\nWebShop\nStableToolBench",
            "content": "2420 2120 2500 3930 160K 200 200 200 200 2000 40K 40K 40K 70K None Table 6 Training data sizes for different environments. StableToolBench only contains single-turn training data without interactive trajectories. World Model Training Hyper-parameters We utilize LLaMa-Facotry7 (Zheng et al., 2024) for SFT training of LLM-based world models. The training parameters are summarized in Table 7. Parameters unspecified in the table follow the default settings of LLaMA-Factory. Training data size varies across different environments, as detailed in Table 6 if not otherwise specified. Experiments are conducted on 4xH100-80GB GPUs."
        },
        {
            "title": "Value",
            "content": "Global Train Batch Size Learning Rate Number of Training Epochs LR Scheduler Type Warmup Steps BF16 Max Gradient Norm 128 1.0e-5 5 Constant with Warmup 10 True 100 Table 7 SFT hyper-parameters for training LLM-based world models. World Model Backbones We utilize Qwen2.5-7B (Qwen et al., 2025) and Llama3.1-8B (Grattafiori et al., 2024) as the primary backbone for LLM-based world models. To study the impact of model scale, we train Qwen2.5 models of four sizes: 0.5B, 1.5B, 3B, and 7B parameters. The specific model checkpoints used are as follows: 6https://huggingface.co/datasets/stabletoolbench/MirrorAPI-Training 7https://github.com/hiyouga/LLaMA-Factory From Word to World: Can Large Language Models be Implicit Text-based World Models?"
        },
        {
            "title": "Checkpoint URL",
            "content": "https://huggingface.co/Qwen/Qwen2.5-7B https://huggingface.co/Qwen/Qwen2.5-3B Qwen2.5-7B Qwen2.5-3B Qwen2.5-1.5B https://huggingface.co/Qwen/Qwen2.5-1.5B Qwen2.5-0.5B https://huggingface.co/Qwen/Qwen2.5-0.5B Llama3.1-8B https://huggingface.co/meta-llama/Llama-3.1-8B Table 8 Model checkpoints used for world model training. API Models We list the API models and their versions used in paper in Table 9."
        },
        {
            "title": "Version",
            "content": "GPT-4o-mini GPT-4o GPT-4-turbo GPT-4.1 GPT-5 Gemini-2.5-flash Claude-sonnet-4.5 claude-sonnet-4-5-20250929 Qwen3-235B gpt-4o-mini-2024-07-18 gpt-4o-2024-11-20 gpt-4-turbo-2024-04-09 gpt-4.1-2025-04-14 gpt-5-2025-08-07 gemini-2.5-flash qwen3-235b-a22b-instruct-2507 Table 9 API models and versions used for evaluations. A.2. Synthetic Data Competes with Real To compare the quality of world-modelsynthesized trajectories with those from the real environment, we construct matched SFT datasets using 1,000 successful trajectories collected from (i) the real environment and (ii) the world model. To control for the behavior policy used during data collection, both datasets are generated by the same agent: Qwen2.5-7B-Instruct policy trained via direct RL (i.e., without any SFT). This design avoids reusing the world-model training policy (GPT-4o) as the collector, thereby reducing the risk that the world model self-replays trajectories through the behavior policy. For world-model rollouts, trajectory success is determined by the models own predicted outcome. A.3. Early Experience for Policy Learning Early Experience (WM-SFT) To provide early dynamics exposure before policy learning, we warmstart the agent with world-model style supervised objective (Eq. 2): predicting the next environment response and termination signal conditioned on the dialogue history and the current action. We use the same data sources described in Appendix A.1 and sample 1,000 trajectories to construct the WM-SFT dataset. Training follows the same SFT hyper-parameters as world model training  (Table 7)  . For the baseline without early experience, this stage is skipped. Agent Warmup (Agent-SFT) After WM-SFT, we perform standard policy warmup stage by supervised fine-tuning on real-environment trajectories collected in Appendix A.1. Specifically, we sample 1,000 trajectories and fine-tune the agent to generate its next turn (reasoning trace and action; Eq. 1) from the interaction history. We use the same SFT hyper-parameters as in Table 7. 19 From Word to World: Can Large Language Models be Implicit Text-based World Models? Reinforcement Learning (Agent-RL) We utilize the AgentGymRL framework8 (Xi et al., 2025) to run GRPO training for agent policy training, with the suggested hyper-parameters as suggested in the paper. The command is as follows: python3 -m verl.agent_trainer.main_ppo algorithm.adv_estimator=grpo algorithm.rounds_ctrl.type=fixed algorithm.rounds_ctrl.rounds=20 data.train_file=${DATA_FILE} data.train_batch_size=16 data.max_prompt_length=1024 data.max_response_length=4096 actor_rollout_ref.agentgym.task_name=${TASK_NAME} actor_rollout_ref.agentgym.env_addr=${ENV_ADDR} actor_rollout_ref.agentgym.timeout=600 actor_rollout_ref.model.path=${MODEL_PATH} actor_rollout_ref.actor.use_kl_loss=True actor_rollout_ref.actor.kl_loss_coef=0.001 actor_rollout_ref.actor.kl_loss_type=low_var_kl actor_rollout_ref.rollout.gpu_memory_utilization=0.6 actor_rollout_ref.rollout.n=8 actor_rollout_ref.rollout.max_model_len=32768 actor_rollout_ref.rollout.max_tokens=200 actor_rollout_ref.rollout.tensor_model_parallel_size=1 actor_rollout_ref.actor.ppo_epochs=1 actor_rollout_ref.actor.optim.lr=1e-6 actor_rollout_ref.actor.ppo_mini_batch_size=8 actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu=1 algorithm.kl_ctrl.kl_coef=0.001 trainer.default_local_dir=outputs/ckpt/${trainer.experiment_name} trainer.project_name=\"agentgym\" trainer.experiment_name=\"${EXPERIMENT_NAME}\" trainer.save_freq=10 trainer.total_epochs=10 trainer.n_gpus_per_node=4 A.4. World Model Initialization Context In ALFWorld and SciWorld, each game instance involves random initialization of the environment. For example, in ALFWorld, the positions and contents of objects within rooms vary, while in SciWorld, the connectivity of houses changes with each initialization. Consequently, even for humans, accurately predicting the next state of the environment based solely on task descriptions is challenging. Similar to RAWM (Yang et al., 2025), we include the initial state information of the environment for the world models predictions. This design aligns with practical applications where the world model is used with knowledge of the initial environment state. In data synthesis scenarios, such random states can be sampled through similar random generation methods, enhancing the diversity and generalization capabilities of the world model. Examples of initial state information are provided in Figure 8 and 9. While TextWorld lacks full initial state information due to environment limits. WebShop and StableToolBench are inherently open environments where comprehensive initial state information cannot be provided, so they also do not include initial state information. 8https://github.com/WooooDyy/AgentGym-RL From Word to World: Can Large Language Models be Implicit Text-based World Models? # Environment Information (Only visible to Assistant) === Objects on Receptacles === - cabinet 1 is closed, if opened, in it, you see nothing. - cabinet 2 is closed, if opened, on the cabinet 2, you see dishsponge 1 - cabinet 3 is closed, if opened, on the cabinet 3, you see mug 1 - cabinet 4 is closed, if opened, in it, you see nothing. - cabinet 5 is closed, if opened, on the cabinet 5, you see soapbottle 3 - cabinet 6 is closed, if opened, on the cabinet 6, you see plate 1 - On the cabinet 9, you see mug 3 - On the countertop 1, you see bread 1, dishsponge 3, egg 2, fork 2, mug 2, pen 2, peppershaker 1, potato 1, potato 2, winebottle 2, winebottle 3 - On the diningtable 1, you see apple 1, bowl 1, bowl 2, butterknife 1, butterknife 2, butterknife 3, fork 1, glassbottle 1, glassbottle 2, knife 1, knife 2, lettuce 1, pen 1, pot 1, saltshaker 1, saltshaker 2, soapbottle 1, soapbottle 2, spatula 1, spatula 2, winebottle 1 - drawer 2 is closed, if opened, on the drawer 2, you see dishsponge 2, pencil 1 - fridge 1 is closed, if opened, on the fridge 1, you see bowl 3, cup 1, lettuce 2, tomato 1 - On the garbagecan 1, you see apple 2, egg 1 - microwave 1 is closed, if opened, on the microwave 1, you see apple 3 - On the sinkbasin 1, you see glassbottle 3, spoon 1 - On the stoveburner 1, you see pan 1 - On the stoveburner 3, you see pan 2 - On the stoveburner 4, you see pan 2 # User Environment Information (Displayed to User) You are in the middle of room. Looking quickly around you, you see cabinet 10, cabinet 9, cabinet 8, cabinet 7, cabinet 6, cabinet 5, cabinet 4, cabinet 3, cabinet 2, cabinet 1, coffeemachine 1, countertop 1, diningtable 1, drawer 2, drawer 1, fridge 1, garbagecan 1, microwave 1, sinkbasin 1, stoveburner 4, stoveburner 3, stoveburner 2, stoveburner 1, and toaster 1. Your task is to: cool some mug and put it in coffeemachine. AVAILABLE ACTIONS: go to cabinet 1,go to cabinet 10,go to cabinet 2,go to cabinet 3,go to cabinet 4,go to cabinet 5,go to cabinet 6,go to cabinet 7,go to cabinet 8,go to cabinet 9,go to coffeemachine 1,go to countertop 1,go to diningtable 1,go to drawer 1,go to drawer 2,go to fridge 1,go to garbagecan 1,go to microwave 1,go to sinkbasin 1,go to stoveburner 1,go to stoveburner 2,go to stoveburner 3,go to stoveburner 4,go to toaster 1,help,inventory,look Figure 8 Initialization Context Example of ALFWorld 21 From Word to World: Can Large Language Models be Implicit Text-based World Models? # Environment Information (Only visible to Assistant) === Goal Progress === Completed keys: - Sequential Subgoals: - 0 false GoalFind focus on thermometer 1 false GoalFind focus on substance 2 false GoalFindAnswerBox focus on correct answer box - Unordered and Optional Subgoals: - 0 false GoalInRoomWithObject be in same location as thermometer 1 false GoalSpecificObjectInDirectContainer have thermometer in inventory 2 false GoalMoveToNewLocation move to new location 3 false GoalMoveToLocation move to the location asked by the task (substance location) 4 false GoalMoveToLocation move to the location asked by the task (answer box location) 5 false GoalMoveToLocation move to location with heating device (kitchen) 6 false GoalMoveToLocation move to location with heating device (outside) 7 false GoalMoveToLocation move to location with heating device (foundry) 8 false GoalSpecificObjectInDirectContainer have task object in inventory 9 false GoalPastActionUseObjectOnObject use thermometer on substance 10 false GoalPastActionUseObjectOnObject use thermometer on substance (after it has been heated) 11 false GoalObjectsInSingleContainer have substance alone in single container 12 false GoalActivateDeviceWithName activate heater (stove) 13 false GoalActivateDeviceWithName activate heater (blast furnace) 14 false GoalActivateDeviceWithName activate heater (oven) 15 false GoalActivateDeviceWithName activate heater (hot plate) 16 false GoalSpecificObjectInDirectContainer have lighter in inventory 17 false GoalSpecificObjectInDirectContainer move wood into fire pit 18 false GoalTemperatureOnFire ignite wood 19 false GoalObjectInContainer have substance on heater (stove) 20 false GoalObjectInContainer have substance on heater (blast furnace) 21 false GoalObjectInContainer have substance on heater (oven) 22 false GoalObjectInContainer have substance on heater (hot plate) 23 false GoalObjectInContainer have substance on heater (fire pit) 24 false GoalTemperatureIncrease heat substance by at least 20C - === Possible Actions === - activate OBJ - close OBJ - connect OBJ to OBJ - deactivate OBJ - disconnect OBJ - dunk OBJ in OBJ - eat OBJ - flush OBJ - focus on OBJ - go OBJ - inventory - look around Figure 9 Initialization Context Example of SciWorld 22 From Word to World: Can Large Language Models be Implicit Text-based World Models? - look at OBJ - look in OBJ - mix OBJ - move OBJ to OBJ - open OBJ - pick up OBJ - pour OBJ in OBJ - put down OBJ - read OBJ - reset task - task - teleport OBJ - use OBJ on OBJ - wait - wait1 === Per-Room Observations === == Room: terminal 2 == This room is called the bathroom. In it, you see: the agent substance called air bathtub, which is turned off. In the bathtub is: nothing. glass cup (containing nothing) picture sink, which is turned off. In the sink is: nothing. toilet. In the toilet is: drain, which is open, substance called water. You also see: door to the kitchen (that is closed) Possible Objects: agent, air, bathroom, bathtub, cup, door, kitchen, picture, sink, substance in toilet, toilet == Room: kitchen == This room is called the bathroom. In it, you see: the agent substance called air bathtub, which is turned off. In the bathtub is: nothing. glass cup (containing nothing) picture sink, which is turned off. In the sink is: nothing. toilet. In the toilet is: drain, which is closed, substance called water. You also see: door to the kitchen (that is closed) Possible Objects: agent, air, bathroom, bathtub, cup, door, kitchen, picture, sink, substance in toilet, toilet == Room: sewer == This room is called the bathroom. In it, you see: the agent substance called air bathtub, which is turned off. In the bathtub is: nothing. glass cup (containing nothing) picture sink, which is turned off. In the sink is: nothing. toilet. In the toilet is: drain, which is closed, substance called water. You also see: door to the kitchen (that is closed) Possible Objects: agent, air, bathroom, bathtub, cup, door, kitchen, picture, sink, substance in toilet, toilet Figure 10 Initialization Context Example of SciWorld (Continued) 23 From Word to World: Can Large Language Models be Implicit Text-based World Models? == Room: bedroom == This room is called the bedroom. In it, you see: the agent substance called air bed. On the bed is: mattress. On the mattress is: white pillow.. book shelf (containing nothing) closet. The closet door is closed. drawing table. On the table is: nothing. You also see: door to the hallway (that is closed) Possible Objects: agent, air, bed, bedroom, book shelf, closet, cloth sittable, door, drawing, hallway, object, table == Room: art studio == This room is called the art studio. In it, you see: the agent substance called air large cupboard. The large cupboard door is closed. table. On the table is: jug (containing nothing). wood cup (containing yellow paint) wood cup (containing blue paint) wood cup (containing red paint) You also see: door to the hallway (that is closed) Possible Objects: agent, air, art studio, blue paint, cup containing blue paint, cup containing red paint, cup containing yellow paint, cupboard, door, hallway, jug, paint in cup containing red paint, paint in cup containing yellow paint, table == Room: foundry == This room is called the foundry. In it, you see: the agent substance called air blast furnace, which is turned off. The blast furnace door is closed. sink, which is turned off. In the sink is: nothing. table. On the table is: nothing. You also see: door to the outside (that is closed) Possible Objects: agent, air, blast furnace, door, foundry, outside, sink, steel table == Room: workshop == This room is called the workshop. In it, you see: the agent substance called air table. On the table is: battery, black wire, blue light bulb, which is off, green light bulb, which is off, red wire, switch, which is off, violet light bulb, which is off, yellow wire. ultra low temperature freezer. The ultra low temperature freezer door is closed. You also see: door to the hallway (that is closed) Possible Objects: agent, air, anode in battery, anode in blue light bulb, anode in component, anode in green light bulb, anode in violet light bulb, battery, battery cathode, black wire, black wire terminal 1, black wire terminal 2, blue light bulb, blue light bulb cathode, cathode in component, cathode in green light bulb, cathode in violet light bulb, door, freezer, green light bulb, hallway, red wire, red wire terminal 1, red wire terminal 2, switch, table, terminal 1 in yellow wire, terminal 2 in yellow wire, violet light bulb, workshop, yellow wire Figure 11 Initialization Context Example of SciWorld (Continued) 24 From Word to World: Can Large Language Models be Implicit Text-based World Models? == Room: hallway == This room is called the hallway. In it, you see: the agent substance called air drawing You also see: door to the art studio (that is closed) door to the bedroom (that is closed) door to the greenhouse (that is closed) door to the kitchen (that is closed) door to the living room (that is closed) door to the workshop (that is closed) Possible Objects: agent, air, art studio, art studio door, bedroom, bedroom door, door to greenhouse, door to kitchen, door to living room, door to workshop, drawing, greenhouse, hallway, kitchen, living room, workshop == Room: bathroom == This room is called the bathroom. In it, you see: the agent substance called air bathtub, which is turned off. In the bathtub is: nothing. glass cup (containing nothing) picture sink, which is turned off. In the sink is: nothing. toilet. In the toilet is: drain, which is closed, substance called water. You also see: door to the kitchen (that is closed) Possible Objects: agent, air, bathroom, bathtub, cup, door, kitchen, picture, sink, substance in toilet, toilet == Room: terminal 1 == This room is called the bathroom. In it, you see: the agent substance called air bathtub, which is turned off. In the bathtub is: nothing. glass cup (containing nothing) picture sink, which is turned off. In the sink is: nothing. toilet. In the toilet is: drain, which is closed, substance called water. You also see: door to the kitchen (that is closed) Possible Objects: agent, air, bathroom, bathtub, cup, door, kitchen, picture, sink, substance in toilet, toilet == Room: living room == This room is called the bathroom. In it, you see: the agent substance called air bathtub, which is turned off. In the bathtub is: nothing. glass cup (containing nothing) picture sink, which is turned off. In the sink is: nothing. toilet. In the toilet is: drain, which is closed, substance called water. You also see: door to the kitchen (that is closed) Possible Objects: agent, air, bathroom, bathtub, cup, door, kitchen, picture, sink, substance in toilet, toilet Figure 12 Initialization Context Example of SciWorld (Continued) 25 From Word to World: Can Large Language Models be Implicit Text-based World Models? == Room: outside == This outside location is called the outside. Here you see: the agent substance called air an axe fire pit (containing nothing) fountain (containing substance called water) the ground shovel substance called wood You also see: door to the foundry (that is closed) door to the greenhouse (that is closed) door to the kitchen (that is closed) Possible Objects: agent, air, axe, door to foundry, door to greenhouse, door to kitchen, fire pit, foundry, fountain, greenhouse, ground, kitchen, outside, shovel, substance in fountain, wood == Room: greenhouse == This room is called the greenhouse. In it, you see: the agent substance called air bee hive. The bee hive door is closed. jug (containing nothing) sink, which is turned off. In the sink is: nothing. You also see: door to the hallway (that is closed) door to the outside (that is closed) Possible Objects: agent, air, bee hive, door to hallway, door to outside, greenhouse, hallway, jug, outside, sink # User Environment Information (Displayed to User) Your task is to measure the melting point of tin, which is located around the kitchen. First, focus on the thermometer. Next, focus on the tin. If the melting point of tin is above 200.0 degrees celsius, focus on the blue box. If the melting point of tin is below 200.0 degrees celsius, focus on the orange box. The boxes are located around the kitchen. This room is called the bathroom. In it, you see: the agent substance called air bathtub, which is turned off. In the bathtub is: nothing. glass cup (containing nothing) picture sink, which is turned off. In the sink is: nothing. toilet. In the toilet is: drain, which is open, substance called water. You also see: door to the kitchen (that is closed) Figure 13 Initialization Context Example of SciWorld (Continued) From Word to World: Can Large Language Models be Implicit Text-based World Models? B. Task Examples and Case Studies In this section, we provide task examples and case studies on world model across five environments. Instruction: API doc: { api_name: SWIFT codes by country, api_description: Returns list of SWIFT codes for country (use two letter ISO country code), required_parameters: [ { name: country_code, type: string, description: , default: AU } ], optional_parameters: [ ], tool_description: Lookup country banks and their SWIFT codes, tool_name: Bank SWIFT codes, tool_category: Financial } Request: { category: Financial, tool_name: Bank SWIFT codes, api_name: SWIFT codes by country, tool_input: {country_code: JP}, strip: filter }"
        },
        {
            "title": "World Model Trajectory",
            "content": "{ error: , response: { data: [ { country: JP, code: FCTOJPJ1XXX, bank: ABN AMRO CLEARING TOKYO CO., LTD. }, { country: JP, code: ACSRJPJ1XXX, bank: ACE SECURITIES CO., LTD. }, { country: JP, code: ADCTJPJJXXX, bank: ADEKA CORPORATION }, { country: JP, code: AEONJPJ1XXX, bank: AEON BANK, LTD. }, { country: JP, code: AEONJPJTXXX, bank: AEON BANK, LTD. }... ] } } { error: , response: { data: [ { country: JP, code: 01234567, bank: MUKAMAL BANK }, { country: JP, code: 09876543, bank: NIPPON BANK }, { country: JP, code: 12345678, bank: YAMATO BANK }, { country: JP, code: 87654321, bank: TOHOKU BANK } ] } } Figure 14 Task Example and Case Study of StableToolBench 27 From Word to World: Can Large Language Models be Implicit Text-based World Models? Instruction: You are in the middle of room. Looking quickly around you, you see armchair 1, bed 1, diningtable 1, drawer 2, drawer 1, garbagecan 1, sidetable 2, and sidetable 1. Your task is to: put some pillow on armchair. AVAILABLE ACTIONS: go to armchair 1,go to bed 1,go to diningtable 1,go to drawer 1,go to drawer 2,go to garbagecan 1,go to sidetable 1,go to sidetable 2,help,inventory,look"
        },
        {
            "title": "World Model Trajectory",
            "content": "Thought 1: My task is to put pillow on the armchair. Before proceeding, should first find out where the pillows are located. likely place could be the bed or the sidetables. Starting with checking the bed might be good idea as pillows are commonly found there. Thought 1: In order to complete the task of putting pillow on the armchair, need to first find pillow. Pillows are most likely to be found on bed, in drawer, or on sidetable. should explore these areas to locate pillow and then proceed to the armchair to place it there. Action 1: go to bed 1 Action 1: go to bed 1 Observation 1: You arrive at bed 1. On the bed 1, you see book 2, laptop 1, pillow 2, and pillow 1. Observation 1: You arrive at bed 1. On the bed 1, you see book 2, laptop 1, pillow 2, and pillow 1. Thought 2: have found two pillows on the bed. My task requires putting pillow on the armchair. need to pick up one of the pillows and then go to the armchair. Thought 2: have found pillows on the bed, which is helpful in achieving the task. need to pick up one of the pillows so can then place it on the armchair. Action 2: inventory Action 2: inventory Observation 2: You are not carrying anything. Observation 2: You are not carrying anything. Thought 3: need to pick up pillow from the bed to carry it to the armchair. Once have the pillow, Ill proceed to go to the armchair to complete the task. Thought 3: Its confirmed that am not carrying anything. To complete the task, should pick up one of the pillows from the bed and then make my way to the armchair. Action 3: pick up pillow 1 Observation 3: Nothing happens. Action 3: Take pillow 1 Observation 3: Nothing happens. Thought 4: The action to pick up the pillow didnt work, suggesting perhaps its not valid action at this moment, or need to utilize another approach to be able to pick up items. should consult the help to see if theres any guidance on how to interact with objects in the environment. Action 4: help Observation 4: Available commands: look: look around your current location. inventory: check your current inventory. go to (receptacle): move to receptacle. open (receptacle): open receptacle... Thought 5: Based on the available commands, should use the \"take (object) from (receptacle)\" command to pick up pillow from the bed. will proceed with the correct format to take pillow. Action 5: take pillow 1 from bed 1 Observation 5: You pick up the pillow 1 from the bed 1. Thought 6: successfully picked up pillow. Now, need to go to the armchair to place the pillow on it, completing the task. Thought 4: It appears that \"Take pillow 1\" is not valid action, or perhaps it is not the correct format for the given scenario. will revise my approach and check if there is specific action for picking up items or if need to use different method. Action 4: help Observation 4: Available commands: look: look around your current location. inventory: check your current inventory. go to (receptacle): move to receptacle. open (receptacle): open receptacle... Thought 5: Now that have the list of available commands, see that need to use the \"take (object) from (receptacle)\" command to pick up pillow from the bed. will proceed to take one of the pillows. Action 5: take pillow 1 from bed Observation 5: You pick up the pillow 1 from the bed 1. Thought 6: now have pillow 1 in my possession. The next step is to go to the armchair so that can place the pillow on it to achieve the task goal. Action 6: go to armchair 1 Action 6: go to armchair 1 Observation 6: You arrive at armchair 1. On the armchair 1, you see nothing. Observation 6: You arrive at armchair 1. On the armchair 1, you see nothing. Thought 7: Im at the armchair where need to place the pillow. will use the move (object) to (receptacle)command to put the pillow on the armchair. Thought 7: am now at the armchair with pillow 1 in my inventory. The final step is to place the pillow onto the armchair to complete the task. Action 7: move pillow 1 to armchair 1 Action 7: move pillow 1 to armchair 1 Observation 7: You move the pillow 1 to the armchair 1. Observation 7: You move the pillow 1 to the armchair Success: True Success: True Figure 15 Task Example and Case Study of ALFWorld 28 From Word to World: Can Large Language Models be Implicit Text-based World Models? Instruction: Your task is to measure the temperature of unknown substance B, which is located around the bathroom. First, focus on the thermometer. Next, focus on the unknown substance B. If the unknown substance temperature is above 0.0 degrees celsius, place it in the yellow box. If the unknown substance temperature is below 0.0 degrees celsius, place it in the purple box. The boxes are located around the bathroom. This room is called the workshop. In it, you see: the agent substance called air table. On the table is: battery, blue light bulb, which is off, blue wire, orange wire, red wire, switch, which is off, violet light bulb, which is off, yellow light bulb, which is off. ultra low temperature freezer. The ultra low temperature freezer door is closed. You also see: door to the hallway (that is closed)"
        },
        {
            "title": "World Model Trajectory",
            "content": "Thought 1: To start this task, first need to focus on the thermometer, as it is crucial for measuring the temperature of the unknown substance B. Action 1: focus on thermometer Observation 1: No known action matches that input. Thought 2: It seems need to revise my approach. Let me start by looking around to gather more information about my surroundings and locate the thermometer. Action 2: look around Observation 2: This room is called the workshop. In it, you see: ... Thought 3: dont see any mention of thermometer in my surroundings. should explore further to find where the thermometer might be located or check the areas close to the bathroom for their contents. Action 3: go to the hallway Observation 3: The door is not open. Thought 4: need to open the door to the hallway so can proceed to search for the thermometer and unknown substance near the bathroom. Action 4: open door to the hallway Observation 4: The door is now open. Thought 5: Now that the door to the hallway is open, can proceed to the hallway and then look for the bathroom where the thermometer and unknown substance might be. Action 5: go to hallway Observation 5: You move to the hallway. Thought 6: am now in the hallway. need to locate the bathroom to look for the thermometer and the unknown substance B. Action 6: look around Observation 6: This room is called the hallway. In it, you see: the agent substance called air finger painting You also see: door to the art studio (that is closed)... [More steps omitted for brevity] Thought 24: Ive focused on the unknown substance and previously focused on the thermometer. Now ... Action 24: move unknown substance to yellow box Observation 24: You move the unknown substance to the yellow box. Success: True Thought 1: need to find thermometer to measure the temperature of the unknown substance B. Currently, dont see thermometer in the workshop, so Ill need to look around or go to different room to find it. Action 1: look around Observation 1: This room is called the workshop. In it, you see: ... Thought 2: still dont see thermometer. need to search more thoroughly or consider moving to another location. The door to the hallway could lead to another room where the thermometer or the unknown substance might be found. Action 2: open the door to the hallway Observation 2: The door is now open. Thought 3: With the door to the hallway open, can explore further to potentially locate the thermometer or the unknown substance needed for the task. Action 3: go to hallway Observation 3: You move to the hallway. Thought 4: Now that Im in the hallway, should look around to see if there are any signs of the thermometer or the unknown substance B. Action 4: look around Observation 4: This room is called the hallway. In it, you see: the agent substance called air finger painting You also see: door to the art studio (that is closed)... Thought 5: There are several doors leading to different rooms. should choose room that is likely to ... Action 5: go to the kitchen Observation 5: The door is not open. Thought 6: need to open the door to the kitchen to proceed and see if the thermometer is there. Action 6: open the door to the kitchen Observation 6: The door is now open. [More steps omitted for brevity] Thought 20: It seems Im unable to use the actions incorrectly to check the temperature of the unknown ... Action 20: move unknown substance to yellow box Observation 20: You move the unknown substance to the yellow box. Success: True Figure 16 Task Example and Case Study of SciWorld From Word to World: Can Large Language Models be Implicit Text-based World Models? Instruction: Hey, thanks for coming over to the TextWorld today, there is something need you to do for me. First of all, you could, like, look and see that the antique trunk inside the bedroom is opened. Then, recover the old key from the antique trunk. Then, make absolutely sure that the wooden door inside the bedroom is unlocked. After unlocking the wooden door, open the wooden door in the bedroom. Then, try to head east. After that, try to travel south. Once you get through with that, take the milk from the couch within the living room. Having taken the milk, attempt to travel north. That done, rest the milk on the stove inside the kitchen. And if you do that, youre the winner! -= Bedroom =- You make grand eccentric entrance into bedroom.You see closed normal looking chest drawer right there by you. You can see an antique trunk. You see king-size bed. But the thing is empty.There is closed wooden door leading east. -= Bedroom =-0/1 AVAILABLE ACTIONS: examine antique trunk, examine chest drawer, examine king-size bed, examine wooden door, inventory, look, open antique trunk, open chest drawer"
        },
        {
            "title": "World Model Trajectory",
            "content": "Thought 1: The description mentioned an antique trunk inside the bedroom that needs to be opened first. Since one of the admissible actions is to open the antique trunk, this should be the first step to proceed with the tasks. Action 1: open antique trunk Observation 1: You open the antique trunk, revealing an old key. -= Bedroom =-0/2 Thought 2: Now that the antique trunk is open and the old key is revealed, the next step is to recover the old key from the antique trunk. Action 2: take old key Observation 2: You take the old key from the antique trunk. -= Bedroom =-0/ Thought 3: With the old key in hand, the next step is to ensure that the wooden door inside the bedroom is unlocked. Examining the wooden door might be necessary to unlock it. Action 3: examine wooden door Observation 3: it is what it is, wooden door You cant see inside it because the lids in your way. -= Bedroom =-0/4 Thought 4: The door needs to be unlocked using the old key we obtained. The next logical step would be to try to unlock the door. Action 4: unlock wooden door with old key Observation 4: You unlock wooden door. -= Bedroom =-0/ Thought 5: The wooden door has been unlocked. The next step is to open the door so that you can proceed to head east. Action 5: open wooden door Observation 5: You open wooden door. -= Bedroom =-0/6 Thought 6: With the wooden door now open, the next step is to proceed through the door to the east as mentioned in the instructions. Action 6: go east Observation 6: -= Kitchen =- Well how about that, you are in place were calling kitchen. The room is well lit. You make out closed refrigerator nearby. You can see counter. The counter is normal. But the thing hasnt got anything on it. You rest your hand against wall, but you miss the wall and fall onto stove. Looks like ... [More steps omitted for brevity] Success: True Thought 1: The first step is to open the antique trunk to check if it contains the old key. Action 1: open antique trunk Observation 1: You open the antique trunk, revealing an old key. -= Bedroom =-0/2 Thought 2: Now that the antique trunk is open and the old key is visible, the next step is to take the old key from the trunk. Action 2: take old key Observation 2: You take the old key from the antique trunk. -= Bedroom =-0/3 Thought 3: With the old key in hand, the next task is to unlock the wooden door in the bedroom. Action 3: examine wooden door Observation 3: The wooden door looks well-built. You cant see inside it because the lids in your way. -= Bedroom =-0/4 Thought 4: attempted to examine the wooden door, but need to unlock it now using the old key to proceed with the tasks. Action 4: unlock wooden door with old key Observation 4: You unlock wooden door. -= Bedroom =-0/5 Thought 5: The wooden door is now unlocked, and the next step is to open it so that can move to the next room. Action 5: open wooden door Observation 5: You open wooden door. -= Bedroom =-0/6 Thought 6: The wooden doorf is open, so the next step is to go through it by heading east. Action 6: go east Observation 6: -= Kitchen =- You are in kitchen. standard one. You can see refrigerator. You can make out counter. The counter is normal. But the thing is empty. You can make out stove. The stove is usual. But the thing is empty, unfortunately. You can make out kitchen island. The kitchen island is normal. On the kitchen island you can make out note. There is closed screen door leading east. There is an open wooden door leading west. You need an unblocked exit? You should try going north. You need an unguarded exit? You should try going south. Your score has just gone up by one point. -= Kitchen =-1/7 [More steps omitted for brevity] Success: True Figure 17 Task Example and Case Study of TextWorld. From Word to World: Can Large Language Models be Implicit Text-based World Models? Instruction: Find me mens sleep & lounge with long sleeve, elastic waistband for daily wear with color: multi 7, and size: x-large, and price lower than 80.00 dollars [SEP] Search Your admissible actions of the current situation are: [search[<your query>],click[search]]."
        },
        {
            "title": "World Model Trajectory",
            "content": "Thought 1: need to search for mens sleep & lounge wear that has long sleeves, an elastic waistband, is designated for daily wear, colored multi 7, size x-large, and priced under $80. Thought 1: need to find mens sleep & lounge wear with specific attributes. Its best to search using the keywords related to the product, including category, features, and price. Action 1: search[mens sleep lounge long sleeve elastic waistband multi 7 x-large under 80] Action 1: search[mens sleep lounge long sleeve elastic waistband multi 7 x-large under 80] Observation 1: Back to Search [SEP] Page 1 (Total results: 50) [SEP] Next > [SEP] B09ND8P2QR [SEP] InterestPrint Deer and Stars Mens Loungewear Pajama Sets - Long Sleeve Tee and Jogger Pant [SEP] $43.59 [SEP]... Your admissible actions of the current situation are: [click[back to search],click[next >],click[b09nd8p2qr],click[b09nd9dp7j],click[b09qw2... Observation 1: Back to Search [SEP] Page 1 (Total results: 50) [SEP] Next > [SEP] B09ND8P2QR [SEP] InterestPrint Deer and Stars Mens Loungewear Pajama Sets - Long Sleeve Tee and Jogger Pant [SEP] $43.59 [SEP] B09ND9DP7J [SEP] InterestPrint Gold Horse Pattern Mens 2-Piece Sleepwear Set, Long Sleeve Shirt with... Thought 2: found mens loungewear set, B09ND8P2QR, that has long sleeves and is within the price range of under $80. It seems to be potential match and worth investigating further. Thought 2: Based on the search result list, need to identify an appropriate item that matches the criteria: mens sleep & lounge wear with long sleeves, an elastic waistband, multi 7 color, size x-large, and priced under $80. Action 2: click[b09nd8p2qr] Action 2: click[B09ND8P2QR] Observation 2: Back to Search [SEP] < Prev [SEP] size [SEP] small [SEP] medium [SEP] large [SEP] x-large [SEP] xx-large [SEP] color [SEP] multi 1 [SEP] multi 10 [SEP] multi 2 [SEP]... Your admissible actions of the current situation are: [click[back to search],click[< prev],click[description],click[features],click[reviews]... Observation 2: Back to Search [SEP] < Prev [SEP] size [SEP] small [SEP] medium [SEP] large [SEP] x-large [SEP] xx-large [SEP] color [SEP] multi 1 [SEP] multi 10 [SEP] multi 2... Your admissible actions of the current situation are: [click[back to search], click[< prev], click[description], click[features], click[reviews]... Thought 3: need to select the correct size and color for the loungewear to match the requirements. The color should be multi 7, and the size should be x-large. Thought 3: First, need to ensure the selected item is available in the correct size (x-large) and color (multi 7) to match the criteria. Action 3: click[x-large] Action 3: click[x-large] Observation 3: Back to Search [SEP] < Prev [SEP] size [SEP] small [SEP] medium [SEP] large [SEP] x-large [SEP] xx-large [SEP] color [SEP] multi 1 [SEP] multi 10 [SEP] multi 2 [SEP] multi 3... Your admissible actions of the current situation are: [click[back to search],click[< prev],click[description],click[features],click[reviews]... Observation 3: Back to Search [SEP] < Prev [SEP] size [SEP] small [SEP] medium [SEP] large [SEP] x-large [SEP] xx-large [SEP] color [SEP] multi 1 [SEP] multi 10 [SEP] multi 2 [SEP]... Your admissible actions of the current situation are: [click[back to search], click[< prev], click[description], click[features], click[reviews]... Thought 4: need to select the color multi 7 to fully match the criteria provided. Thought 4: Now, need to confirm the color option multi 7 to ensure it matches the criteria. Action 4: click[multi 7] Action 4: click[multi 7] Observation 4: Back to Search [SEP] < Prev [SEP] size [SEP] small [SEP] medium [SEP] large [SEP] xlarge [SEP] xx-large [SEP] color [SEP] multi 1 [SEP] multi 10 [SEP] multi 2... Your admissible actions of the current situation are: [click[back to search],click[< prev],click[description],click[features],click[reviews]... Observation 4: Back to Search [SEP] < Prev [SEP] size [SEP] small [SEP] medium [SEP] large [SEP] x-large [SEP] xx-large [SEP] color [SEP] multi 1 [SEP] multi 10 [SEP] multi 2... Your admissible actions of the current situation are: [ click[back to search], click[< prev], click[description], click[features], click[reviews]... [More steps omitted for brevity] [More steps omitted for brevity] Thought 7: have selected the correct size (x-large) and color (multi 7) for the loungewear. Now, it is time to proceed with purchasing the item. Action 7: click[buy now] Observation 7: Thank you for shopping with us! [SEP] Your code: [SEP] None [SEP] (Paste it in your MTurk interface.) [SEP] Purchased [SEP] asin [SEP] B09ND8P2QR [SEP] options [SEP] color: multi 7, size: x-large [SEP] attrs [SEP] None [SEP] category [SEP] None [SEP] query [SEP] None [SEP] product category [SEP] None [SEP] Target [SEP] asin [SEP] options [SEP] attrs [SEP] price upper [SEP] instuction text... Success: True Thought 7: Since both the desired size and color have been confirmed, the next step is to proceed with purchasing the item. Action 7: click[buy now] Observation 7: Thank you for shopping with us! [SEP] Your code: [SEP] None [SEP] (Paste it in your MTurk interface.) [SEP] Purchased [SEP] asin [SEP] B09ND8P2QR [SEP] options [SEP] color: multi 7, size: x-large [SEP] attrs [SEP] None [SEP] category [SEP] None [SEP] query [SEP] None [SEP] product category [SEP] None [SEP] Target [SEP] asin [SEP] options [SEP] attrs [SEP] price upper [SEP] instuction text... Success: True Figure 18 Task Example and Case Study of WebShop From Word to World: Can Large Language Models be Implicit Text-based World Models? C. Detailed Results We provide detailed results in Table 10 for the OOD generalization of world models."
        },
        {
            "title": "Agent",
            "content": "OOD - Seen Real WM W2R CR OOD - Unseen Real WM W2R CR Qwen2.5-7B WorldModel 6.75 7.10 7.10 1. 4.03 4.67 5.33 1.32 GPT-4o-mini 52.10 43.79 45.56 0.87 52.00 44.67 44.67 0.86 GPT-4o 65.00 51.23 52.47 0.81 74.50 62.42 62.42 0.84 GPT-4-turbo 53.37 56.80 57.40 1.08 64.19 65.33 64.67 1.01 GPT-4.1 71.60 69.23 71.01 0.99 74.00 76.00 76.67 1.04 GPT-5 Gemini-2.5-flash 39.05 40.83 41.42 1.06 51.35 48.67 49.33 0.96 Claude-sonnet-4.5 87.00 72.00 79.00 0.91 76.04 76.00 79.00 1.04 53.55 48.71 50.57 0.94 56.59 53.97 54.58 0.96 Average Llama3.1-8B WorldModel 6.75 8.88 8.88 1.32 4.03 2.67 2.67 0.66 GPT-4o-mini 52.10 48.52 47.93 0.92 52.00 49.33 49.33 0.95 GPT-4o 65.00 56.52 55.90 0.86 74.50 62.16 62.16 0.83 GPT-4-turbo 53.37 56.21 55.62 1.04 64.19 60.67 60.00 0.93 GPT-4.1 71.60 69.82 69.23 0.97 74.00 74.00 73.33 0.99 GPT-5 39.05 42.60 42.60 1.09 51.35 46.00 45.33 0.88 Gemini-2.5-flash Claude-sonnet-4.5 87.00 78.00 84.00 0.97 76.04 81.00 78.00 1.03 53.55 51.51 52.02 0.97 56.59 53.69 52.97 0.94 Average Table 10 Task success rate (%) in ALFWorld under different OOD settings. OOD-Seen indicates the same room with different layout as training. OOD-Unseen indicates tasks containing room types or environment layouts never seen during training. From Word to World: Can Large Language Models be Implicit Text-based World Models? D. System Prompts for Agent Trajectory Collection Interact with household to solve task. Imagine you are an intelligent agent in household environment and your target is to perform actions to complete the task goal. At the beginning of your interactions, you will be given the detailed description of the current environment and your goal to accomplish. For each of your turn, you will be given list of actions which you can choose one to perform in this turn. Now its your turn to take an action. You should first think about the current condition and plan for your future actions, and then output your action in this turn. Your output must strictly follow this format: Thought: your thoughts. Action: your next action After your each turn, the environment will give you immediate feedback based on which you plan your next few steps. if the envrionment output Nothing happened, that means the previous action is invalid and you should try more options. Reminder: the action must be chosen from the given available actions. Any actions except provided available actions will be regarded as illegal. Figure 19 Agent System Prompt for ALFWorld Trajectory Collection You are web shopping. will give you instructions about what to do. You have to follow the instructions. Every round will give you an observation and list of available actions, you have to respond an action based on the state and instruction. You can use search action if search is available. You can click one of the buttons in clickables. An action should be of the following structure: search[keywords] click[value] If the action is not valid, perform nothing. Keywords in search are up to you, but the value in click must be value in the list of available actions. Remember that your keywords in search should be carefully designed. Your response should use the following format: Thought: think ... Action: click[something] Figure 20 Agent System Prompt for WebShop Trajectory Collection 33 From Word to World: Can Large Language Models be Implicit Text-based World Models? You are an agent for science world. Every round will give you an observation, you have to respond an action based on the observation to finish the given task. Here are the actions you may take: [ {action: open/close OBJ, description: open/close container}, {action: de/activate OBJ, description: activate/deactivate device}, {action: connect OBJ to OBJ, description: connect electrical components}, {action: disconnect OBJ, description: disconnect electrical components}, {action: use OBJ [on OBJ], description: use device/item}, {action: look around, description: describe the current room}, {action: look at OBJ, description: describe an object in detail}, {action: look in OBJ, description: describe containers contents}, {action: read OBJ, description: read note or book}, {action: move OBJ to OBJ, description: move an object to container}, {action: pick up OBJ, description: move an object to the inventory}, {action: put down OBJ, description: drop an inventory item}, {action: pour OBJ into OBJ, description: pour liquid into container}, {action: dunk OBJ into OBJ, description: dunk container into liquid}, {action: mix OBJ, description: chemically mix container}, {action: go to LOC, description: move to new location}, {action: eat OBJ, description: eat food}, {action: flush OBJ, description: flush toilet}, {action: focus on OBJ, description: signal intent on task object}, {action: wait, description: take no action for 10 iterations}, {action: wait1, description: take no action for 1 iteration}, {action: examine OBJ, description: provides description of the objects present on or in receptacle.}, {action: task, description: describe current task}, {action: inventory, description: list your inventory} ] Your response should use the following format: Thought: your thoughts. Action: your next action Figure 21 Agent System Prompt for SciWorld Trajectory Collection You are playing text-based interactive fiction game (TextWorld). You will receive observations describing the current state. When available, list of admissible actions may be provided. Always output strictly in the following format: Thought: <your reasoning> Action: <the single action to take> Guidelines: - Prefer actions from admissible commands when provided. - If no list is provided, issue valid single command (e.g., look, inventory, open door, go north, take key). - Avoid invalid or multiple actions in one step. Figure 22 Agent System Prompt for TextWorld Trajectory Collection 34 From Word to World: Can Large Language Models be Implicit Text-based World Models? Imagine you are an API Server operating within specialized tool, which contains collection of distinct APIs. Your role is to deeply understand the function of each API based on their descriptions in the API documentation. As you receive specific inputs for individual API calls within this tool, analyze these inputs to determine their intended purpose. Your task is to craft JSON formatted response that aligns with the expected output of the API. The JSON scheme is: { \"error\": \"\", \"response\": \"\" } The error field should remain empty, indicating no errors in processing. The response field should contain the content you formulate based on the APIs functionality and the input provided. Ensure that your responses are meaningful, directly addressing the APIs intended functionality. The key is to maintain the JSON formats integrity while ensuring that your response is an accurate reflection of the APIs intended output within the tool. Please note that your answer should not contain anything other than json format object, which should be parsable directly to json. Note that: - your response should contain rich information given the api input parameters. - your response must be effective and have practical content. API calls may fail for various reasons, such as invalid input parameters, authentication issues, or server errors. Your goal is to generate response that accurately reflects the APIs intended functionality, even if the input parameters are incorrect. Your response should be informative and relevant to the APIs purpose, providing clear and concise explanation of the expected output based on the input provided. Here is an example: API doc: { \"api_name\": \"List Languages\", \"api_description\": \"Get list of currently supported languages. We are constantly adding more every few weeks.\", \"required_parameters\": [], \"optional_parameters\": [], \"tool_description\": \"Introducing our cutting-edge text to speech service, designed to provide you with the most realistic human-sounding voices at an affordable price. Our service is fast and reliable, delivering high-quality audio output in matter of seconds. Additionally, we offer wide range of languages and variety of voice choices, so you can find the perfect fit for your project. Whether you need voiceover for video, an audiobook, or any other project, our text to speech service has you covered. Ex...\", \"tool_name\": \"TTSKraken\", \"tool_category\": \"Artificial_Intelligence_Machine_Learning\" } Request: data = { \"category\": \"Artificial_Intelligence_Machine_Learning\", \"tool_name\": \"TTSKraken\", \"api_name\": \"List Languages\", \"tool_input\": \"{}\", \"strip\": \"filter\", } Response: { \"error\": \"\", \"response\": \"{\"status\":0,\"msg\":\"Success\",\"languages\":[\"en\",\"fr-fr\",\"pt-br\"]}\" } Figure 23 Agent System Prompt for StableToolBench Trajectory Collection"
        }
    ],
    "affiliations": [
        "Fudan University",
        "Microsoft Research",
        "Mind Lab",
        "Oxford University",
        "Princeton University",
        "Southern University of Science and Technology",
        "University of Edinburgh",
        "University of Illinois Urbana-Champaign"
    ]
}