{
    "paper_title": "Atlas-Chat: Adapting Large Language Models for Low-Resource Moroccan Arabic Dialect",
    "authors": [
        "Guokan Shang",
        "Hadi Abdine",
        "Yousef Khoubrane",
        "Amr Mohamed",
        "Yassine Abbahaddou",
        "Sofiane Ennadir",
        "Imane Momayiz",
        "Xuguang Ren",
        "Eric Moulines",
        "Preslav Nakov",
        "Michalis Vazirgiannis",
        "Eric Xing"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce Atlas-Chat, the first-ever collection of LLMs specifically developed for dialectal Arabic. Focusing on Moroccan Arabic, also known as Darija, we construct our instruction dataset by consolidating existing Darija language resources, creating novel datasets both manually and synthetically, and translating English instructions with stringent quality control. Atlas-Chat-2B, 9B, and 27B models, fine-tuned on the dataset, exhibit superior ability in following Darija instructions and performing standard NLP tasks. Notably, our models outperform both state-of-the-art and Arabic-specialized LLMs like LLaMa, Jais, and AceGPT, e.g., our 9B model gains a 13% performance boost over a larger 13B model on DarijaMMLU, in our newly introduced evaluation suite for Darija covering both discriminative and generative tasks. Furthermore, we perform an experimental analysis of various fine-tuning strategies and base model choices to determine optimal configurations. All our resources are publicly accessible, and we believe our work offers comprehensive design methodologies of instruction-tuning for low-resource languages, which are often neglected in favor of data-rich languages by contemporary LLMs."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 1 1 ] . [ 2 2 1 9 7 1 . 9 0 4 2 : r Atlas-Chat: Adapting Large Language Models for Low-Resource Moroccan Arabic Dialect Guokan Shang1, Hadi Abdine1, Yousef Khoubrane2,3, Amr Mohamed1, Yassine Abbahaddou6, Sofiane Ennadir4, Imane Momayiz5, Xuguang Ren1, Eric Moulines1,6, Preslav Nakov1, Michalis Vazirgiannis1,6, Eric Xing 1MBZUAI, 2EMINES-UM6P, 3LINAGORA, 4KTH, 5AtlasIA, 6Ecole Polytechnique"
        },
        {
            "title": "Abstract",
            "content": "We introduce Atlas-Chat, the first-ever collection of LLMs specifically developed for dialectal Arabic. Focusing on Moroccan Arabic, also known as Darija, we construct our instruction dataset by consolidating existing Darija language resources, creating novel datasets both manually and synthetically, and translating English instructions with stringent quality control. Atlas-Chat-2B, 9B1, and 27B models, fine-tuned on the dataset, exhibit superior ability in following Darija instructions and performing standard NLP tasks. Notably, our models outperform both state-of-the-art and Arabic-specialized LLMs like LLaMa, Jais, and AceGPT, e.g., our 9B model gains 13% performance boost over larger 13B model on DarijaMMLU, in our newly introduced evaluation suite for Darija covering both discriminative and generative tasks. Furthermore, we perform an experimental analysis of various fine-tuning strategies and base model choices to determine optimal configurations. All our resources are publicly accessible, and we believe our work offers comprehensive design methodologies of instruction-tuning for low-resource languages, which are often neglected in favor of data-rich languages by contemporary LLMs."
        },
        {
            "title": "Introduction",
            "content": "Transformer-based Large Language Models have revolutionized NLP research and beyond, demonstrating exceptional performance in both natural and formal language generation (Gunasekar et al., 2023), and exhibiting advanced reasoning capabilities in arithmetic, symbolic, and logical tasks (Hendrycks et al., 2020). Despite their success and the frequent release of new, superior open models exemplified by LlaMa (Dubey et al., 2024) and Mistral (Jiang et al., 2023), these breakthroughs have These authors contributed equally. Correspondence: guokan.shang@mbzuai.ac.ae 1https://hf.co/MBZUAI-Paris/Atlas-Chat-9B 1 been concentrated in few data-rich languages (Üstün et al., 2024), assuming access to hundreds of billions or even dozen trillions of tokens for training, often neglecting underrepresented languages. In this work, we explore the challenges of introducing LLMs for low-resource Dialectal Arabic (DA). The Arabic language has rich history and profound cultural significance, featuring an intricate script, extensive lexicon, and complex grammar, making it unique linguistic entity. Although interest in developing Arabic-specialized models has recently been growing, notably led by models like Jais (Sengupta et al., 2023), AceGPT (Huang et al., 2024), and ALLaM (Bari et al., 2024), these efforts primarily focus on bilingualism by balancing English and Modern Standard Arabic (MSA), while often neglecting or excluding DA. However, MSA differs significantly from DA in terms of morphology, syntax, and other linguistic features. Moreover, various Arabic dialects also differ considerably from one another. In fact, Arabic dialects collectively have more native speakers than MSA, as DA serves as the primary mode of communication in daily life across various Arabic-speaking regions (Zaidan and Callison-Burch, 2014). This asymmetry is due in large part to the fact that DA poses challenges not encountered with MSA. Some are related to the lack of essential components for model developmentnamely, training data, benchmarks, and suitable evaluation metricsbut others stem from the very nature of the linguistic characteristics involved in DA itself more generally. We take Moroccan Arabic, also known as Darija, as the focus of our work. Despite being spoken by 40 million people2, Darija remains low-resource. This is because MSA is used in official domains in Morocco, while Darija, blend of MSA, Amazigh, French, and Spanish, is the vernacular widely spoken in daily life. Although Darija, previously only 2https://en.wikipedia.org/wiki/Moroccan_Arabic an oral language, has recently developed written form through the proliferation of social networks and increased access to technology, it still lacks standardization and established grammatical or syntactic rules due to its recent emergence (Gaanoun et al., 2024). Moreover, Darija can be represented in two forms: Arabic script or Latin script (also known as Arabizi). For example, the Darija translation of How are you? can be written as: kidayr? or (cid:63)(cid:81)(cid:75)(cid:10)(cid:64)(cid:89)(cid:74)(cid:10)(cid:187). These challenges underscore the need for models tailored to this linguistic context. To the best of our knowledge, we are the first to introduce modern LLMs specifically developed for Moroccan Arabic, as well as for DA in general. We first constructed the Darija-SFT-Mixture3 dataset, consisting of 458K instruction samples, by consolidating existing Darija language resources, creating novel datasets both manually and synthetically, and translating English instructions under strict quality control. We then developed comprehensive evaluation suite including benchmarks: DarijaMMLU, DarijaHellaSwag, DarijaAlpacaEval, and DarijaBench, to assess LLM capabilities in real-world knowledge, following Darija instructions, and performing traditional NLP tasks such as translation, summarization, and sentiment analysis. In the end, Atlas-Chat models4, fine-tuned from the Gemma 2 models (Team et al., 2024) on our instruction dataset, exhibit superior ability in Darija, surpassing both state-of-the-art and Arabic-specialized LLMs like LLaMa, Jais, and AceGPT, according to automatic metrics and simulated win rates. Additionally, we conduct an experimental analysis of various fine-tuning strategies and base model choices to determine final configurations. We provide some examples by chatting with our models in Appendix D. All our resources are publicly accessible, and we believe our work offers comprehensive design methodologies of instruction-tuning for low-resource languages."
        },
        {
            "title": "2 Related Work",
            "content": "In this section, we begin by reviewing LLMs and benchmarks developed for Arabic, followed by an exploration of recent trends in expanding LLMs to low-resource languages. 3https://hf.co/datasets/MBZUAI-Paris/ Darija-SFT-Mixture 4Inspired by the naming of the Jais models, UAEs highest mountain peak. We chose Atlas to reflect the cultural and geographical significance of the Atlas Mountains that traverse Morocco. Arabic-specialized LLMs. Recent efforts in Arabic-specialized LLMs mainly focus on MSA, the formal written standard across Arabic regions. Jais (Sengupta et al., 2023), 13B-parameter model trained on 395B tokens of Arabic, English, and code data. Containing 116B Arabic tokens 25% of which were translated from EnglishJais was designed to enhance performance in both Arabic and English tasks, trained on mixture of the two languages in 1:2 ratio. However, this approach may suffer from localization issues. AceGPT (Huang et al., 2024) aims to address localization issues by pre-training LLaMA 2 (Touvron et al., 2023) 7B and 13B models on 30B and 10B token mixtures, respectively, of Arabic and English data, with the Arabic portion dominating the dataset. The models were then fine-tuned on Arabic instructions and aligned with Arabic values and culture using RLAIF (Lee et al., 2023). They further introduced the Arabic Cultural and Value Alignment dataset, comprising 8,000 yesno questions. ALLaM (Bari et al., 2024) demonstrated that second-language acquisition can steer the model towards new language without catastrophic forgetting, even with random initialization of weights. They hypothesize that low-resource languages are diluted in large volumes of highresource languages, and pre-train 7B model from scratch on 4T English tokens, followed by training on 1.2T mixture of Arabic and English. Regarding Darija, DarijaBERT (Gaanoun et al., 2024) is currently the only LLM dedicated to the Moroccan Arabic dialect. The model was trained on 100M tokens. However, DarijaBERT is encoder-only, and no decoder-only models have been developed for Darija. Arabic benchmarks for LLMs. Several benchmarks have been created for various tasks and domains to evaluate the Arabic capabilities of LLMs. ArabicMMLU (Koto et al., 2024) is an Arabic adaptation of the original MMLU benchmark (Hendrycks et al., 2020), consisting of 14K multiple-choice questions across 40 tasks in MSA. The benchmark covers wide range of subjects, including history, mathematics, science, and linguistics, reflecting educational levels from eight different countries. LAraBench (Abdelali et al., 2024), benchmark designed for evaluating MSA LLMs on several practical NLP tasks, such as sentiment analysis, named entity recognition, and machine translation, spanning 33 tasks across 61 datasets en2 compassing 296 data points. The Open Arabic LLM Leaderboard (OALL)5 aggregates various native and translated Arabic benchmarks to evaluate models performance across tasks such as reading comprehension, reasoning, and more. LLMs for Low-resource languages. Very recently, the LLM development community has begun to focus on low-resource languages. Multilingual Aya model (Üstün et al., 2024) was developed by instruction-tuning mT5 (Xue, 2020), 13B encoder-decoder model pre-trained on 1T tokens across 101 languages. Of these, 51 are low-resource languages, including Hausa, Icelandic, Luxembourgish, and etc. Other efforts include InkubaLM (Tonja et al., 2024), 0.4B model pre-trained from scratch on 2.4B tokens from five low-resource African languagesHausa, Yoruba, Swahili, isiZulu, and isiXhosaalong with English and French, then fine-tuned to follow instructions on several tasks. Similarly, Tao et al. (2024) explored two language adaptation strategies: continual pre-training followed by fine-tuning and model merging. Their experiments focused on seven lowresource languagesTamil, Telugu, Odia, Bengali, Tibetan, Uyghur, and Mongolianusing datasets ranging from 1 to 20B tokens per language. Another line of research targets subcategory of main languages with limited resources, such as the Claire model (Hunter et al., 2023; Louradour et al., 2024), dedicated to spontaneous French dialogue. Despite advancements, little attention has been given to developing LLMs and benchmarks for DA."
        },
        {
            "title": "3 Data Overview",
            "content": "In developing Atlas-Chat, we chose to use instruction-tuning on base model rather than training from scratch. This decision was primarily driven by the fact that training an LLM from the ground up requires extensive data, which is not readily available for Darija, low-resource dialect. For the same reason, our training process does not include the additional continual pre-training phase typically seen in many language adaptation efforts. However, to mitigate this limitation, we designed synthetic instruction dataset (see Section 5.3) that, to some extent, mimics the next-word prediction task over relatively longer context, typically performed during (continual) pre-training. Moreover, recent studies show that multilingual LLMs often exhibit bias toward internally solv5https://hf.co/blog/leaderboard-arabic ing tasks in English, even when trained on multiple languages (Zhao et al., 2024), and perform best with English prompts, followed by mixed prompts, while non-English prompts significantly underperform (Kmainasi et al., 2024). This observation led us to limit the scope of our work to monolingual LLM, making Atlas-Chat Darija-centric. We focus on developing model that accurately understands prompts written in Darija, generates Darija content, respects its cultural context, and remains accessible and adaptable for native speakers. Therefore, we directed our efforts towards creating an extensive and diverse Darija dataset for instruction-tuning. Table 1 summarizes the composition of our Darija-SFT-Mixture dataset. We employed multifaceted approach to data preparation. First, we reviewed previous research in Darija NLP and collected the majority of available native Darija datasets that met our quality standards. The data selection rule established by native speakers was as follows: if the data is mix of Darija with some MSA, it is acceptable; if it is mixed with other dialects, it is not. In total, ten datasets covering tasks such as translation, summarization, and sentiment analysis were selected. Second, we synthesized high-quality instruction data using advanced proprietary models, drawing on sources such as Wikipedia pages, social media posts, and stories written in Darija. We then converted the native and synthetic datasets into training instructions using templates, with 80% formatted as zero-shot, 10% as few-shot (Longpre et al., 2023), and 10% as multi-turn samples. Third, we translated highquality English instruction datasets into Darija with stringent quality control to expand the range of scenarios, domains, and tasks covered by our dataset. By combining these different sources, we aimed to enhance the models ability to understand and generate Darija across various contexts."
        },
        {
            "title": "4.1 Machine Translation",
            "content": "We collected four datasets containing sentence translations between Darija, MSA, English, and French. These datasets were then converted into training instructions using the templates provided in Appendix A.1. Since our model is Darija-centric, we consider six translation directions: Darija to English, French, MSA, and vice versa. All instructions are written in Darija for each case. 3 Subset 4.1 Translation # Samples Source 85,662 4.1 Transliteration 4.2 Sentiment Analysis 16,920 86, 4.3 Summarization 5.1 MW-QA 5.2 MSM-MG 16,756 30,555 11,808 5.3 Story Completion 48,983 FLORES+, DODa-10K, MADAR, NLLB-Seed DODa-10K MSAC, MSDA, MAC ElecMorocco2016, MYC MArSum Wikipedia Twitter/X, YouTube Comments 9esa.com 6 TÜLU-Darija C.1 Hard Coded 161,259 130 TÜLU-V2-Mix Manual Annotation Description Darja to English, French, MSA and vice-versa Darija in Arabic Script Latin Script Sentences labeled as Positive, Negative, and Neutral"
        },
        {
            "title": "Article titles as summaries\nSynthetic dataset from Moroccan Wikipedia pages\nSynthetic dataset from Tweets and YouTube comments",
            "content": "Stories converted to dataset with part of the story as prompt and the continuation as response Translated TÜLU-V2-Mix after filtering Prompts ensuring the model correctly answers identity/creator-related questions Table 1: Composition of our Darija-SFT-Mixture instruction-tuning dataset. DODa-10K6. The Darija Open Dataset (DODa) (Outchakoucht and Es-Samaali, 2021, 2024)7 is an open-source collaborative project for collecting Darija language resource, including lexicons in semantic and syntactic categories, Darija-English parallel corpus, and etc. Darija is represented in Latin script, as well as in automatically converted Arabic script. We augmented the first 10K examples of the parallel corpus, with MSA and French translated from the English text, by leveraging GPT-4. The final DODa-10K dataset includes translation quintuples between Darija (in both Arabic and Latin scripts), MSA, English, and French. The dataset was then extensively reviewed by groups of native speakers to ensure the quality. In addition to translation, to enhance the models ability to convert between Darija in Arabic and Latin scripts (also known as the transliteration task), we transformed 10K parallel forms into instructions using templates found in Appendix A.2. MADAR (Bouamor et al., 2018)8. The Multi-Arabic Dialect Applications and Resources (MADAR) corpus is collection of parallel sentences covering the dialects of 25 Arab cities, built upon the Basic Traveling Expression Corpus (Takezawa et al., 2007). We select the dialect of Rabat city as Darija translation, along with MSA, resulting in 12K sentence pairs. The split corpus-6test-corpus-26-test is reserved for the evaluation. NLLB-Seed (Maillard et al., 2023)9. The Seed machine translation dataset contains 6K sentences 6https://hf.co/datasets/MBZUAI-Paris/DoDa-10K 7https://github.com/darija-open-dataset 8https://sites.google.com/nyu.edu/madar 9https://github.com/openlanguagedata/seed sampled from English Wikipedia and translated into 39 low-resource languages. We extract the Darija and English pairs. FLORES+10. Built upon FLORES-200 (Costajussà et al., 2022), this corpus is specifically designed to support multilingual research and evaluation. The English sentences were sampled in equal amounts from Wikinews, Wikijunior (a collection of age-appropriate non-fiction books), and Wikivoyage. These were then translated into other languages. For each language, the dataset has 997 sentences for the dev split and 1012 sentences for the devtest split. We selected those in Darija, MSA, English, and French. Dev is severed as training, while devtest for the evaluation."
        },
        {
            "title": "4.2 Sentiment Analysis",
            "content": "We collected five datasets for sentiment analysis, whose content is primarily sourced from social networks. Two datasets come with three labels (positive, negative, and neutral), while the other three have two labels (positive and negative). These datasets were then transformed into training instructions using templates from Appendix A.3. MSDA (Boujou et al., 2021)11. It is an open dataset for sentiment analysis, designed to support research in NLP for Arabic dialects and social media. The dataset includes 52K tweets in Darija, categorized into three labels: positive, neural, or negative. The tweets are preprocessed, and emojis are retained because they play significant role in expressing sentiment. Labels are annotated semi-automatically and bootstrapped with human intervention. 10https://github.com/openlanguagedata/flores 11https://cc.um6p.ma/cc_datasets 4 MSAC (Oussous et al., 2018, 2020)12. The Moroccan Sentiment Analysis Corpus (MSAC) is manually prepared dataset consisting of reviewers opinions for Hespress13 articles, and collection of Arabic comments from Facebook, Twitter and YouTube. It includes content in both MSA and Darija, consisting of 2K sentences labeled as positive or negative in equal proportions. ElecMorocco2016 (Elouardighi et al., 2017)14. The 2016 Moroccan elections (ElecMorocco2016) is sentiment analysis dataset comprising 10K Facebook comments about Moroccans legislative elections held on October 7, 2016. Each comment is labeled as either positive or negative. The comments are written in Darija and MSA. MYC (Jbel et al., 2024)15. The Moroccan Youtube Corpus (MYC) is sentiment analysis dataset of YouTube comments collected from Moroccan channels covering various topics. The dataset prioritizes variety over size, with 20K manually labeled samples, evenly divided between positive and negative. Notably, the 20K comments are equally balanced between Arabic script and Latin script. MAC (Garouani and Kharroubi, 2021)16. The Moroccan Arabic Corpus (MAC) is free, large-scale Darija corpus for sentiment analysis, consisting of 18K manually labeled tweets categorized as positive, neutral, negative, or mixed. Only 643 tweets are labeled as mixed, so we filtered them out."
        },
        {
            "title": "4.3 Automatic Summarization",
            "content": "We found only one dataset for summarization. The documents and summaries were converted into instructions using the template in Appendix A.4. MArSum (Gaanoun et al., 2022)17. The Moroccan Articles Summarization dataset (MArSum) contains 19K news articles written in Darija, along with their titles. The articles were crawled from Goud.ma18. While some content includes MSA, all titles are written in Darija. Since the articles are relatively concise and the titles are sufficiently informative, the titles are considered as summaries. The average length of the titles is 14.6 words. 12https://github.com/ososs/ Arabic-Sentiment-Analysis-corpus 13https://www.hespress.com 14https://github.com/sentiprojects/ ElecMorocco2016 15https://github.com/MouadJb/MYC 16https://github.com/LeMGarouani/MAC 17https://github.com/KamelGaanoun/"
        },
        {
            "title": "MoroccanSummarization",
            "content": "18http://www.goud.ma/"
        },
        {
            "title": "5.1 MoroccanWikipedia-QA",
            "content": "MW-QA19 is dataset derived from Moroccan Wikipedia dump20, developed in our work to enhance the models question-answering (QA) capability. The dataset is divided into four tasks: Open QA (8%), Multiple-Choice QA (40%) (MMLUalike), Extractive QA (10%), and Multiple-Choice Extractive QA (42%) (Belebele-alike), with each percentage reflecting the proportion of Wikipedia pages used for the respective task. The latter two tasks provide context along with the questions, whereas the former two do not. In Open QA and Extractive QA, answers are provided in sentence form. In the multiple-choice tasks, four answer options are presented, with the index of the correct option serving as the answer. The distribution of correct answers (e.g., A, B, C, D) are balanced. The QAs were converted into instructions with the template in Appendix A.5. The dataset generation involved providing each Wikipedia page to Claude 3.5 Sonnet21 and prompting it to generate QA pairs tailored to the four task categories. The prompts followed one-shot or two-shot format to ensure that output adhered to the desired structure. For the extractive tasks, rather than splitting the page into paragraphsan approach that risked losing contextual meaningwe opted to present the entire page to Claude. The model was instructed to first extract meaningful passage from the page and then generate QA pair based on the content of that passage. Also, the model was directed to ensure that the extracted passages were long, self-contained, and did not lose meaning when removed from their original context. total of 8,730 pages were collected and preprocessed by removing scraping errors. Among these pages, some followed uniform structure, typically consisting of brief description of village or community followed by statistical data (e.g., literacy rates and unemployment figures). Given that these statistical sections could become meaningless when extracted from their context, they were allocated to non-extractive tasks, which could still utilize the statistical information to enrich the fine-tuned models knowledge base. 19https://hf.co/datasets/MBZUAI-Paris/ MoroccanWikipedia-QA 20https://dumps.wikimedia.org/arywiki/latest/ 21https://www.anthropic.com/news/ claude-3-5-sonnet 5 The final distribution of QA pairs is as follows: 15.7% Open QA, 43.1% Multiple-Choice QA, 6.9% Extractive QA, and 34.3% Multiple-Choice Extractive QA. These percentages differ from the initial page distribution because Claude generated varying numbers of samples for each task. For example, the average number of samples generated for Open QA is 7.73, while for Extractive QA, it is 2.72."
        },
        {
            "title": "5.3 DarijaStory-Completion",
            "content": "To mitigate the limitation of performing only instruction-tuning for language adaptation without the typical continual pre-training phasedue to the lack of sufficient amount of Darija pre-training datawe designed synthetic story completion dataset, aiming to enhance the next-word prediction capability in Darija for our models over relatively longer context. First, we collected 4,392 long stories from 9esa25, website featuring rich collection of various stories entirely written in Darija. We denote this dataset as DarijaStory26. The scraped stories were then divided into segments of approximately 2,048 tokens, adhering to the base model tokenizers vocabulary. The segments were further divided into two parts of varying lengths: the beginning part and the ending part to be completed. For the two segmentation steps above, the split point is preferably placed at line breaks. Finally, the pairs were converted into instructions using the template provided in Appendix A.7."
        },
        {
            "title": "6 Translated English Instruction Datasets",
            "content": "Finally, we broadened our instruction-tuning data by translating English datasets into Darija, to cover wider array of scenarios, domains, and tasks. We began by reviewing the most widely used datasets for fine-tuning state-of-the-art models to ensure that our translation efforts would lead to meaningful improvements. After careful consideration, we decided to focus on the TÜLU-V2-mix (Ivison et al., 2023)27 dataset for several reasons. It offers comprehensive dataset composition, including samples from some of the most widely used datasets, such as FLAN and ShareGPT, for fine-tuning state-of-the-art models. Appendix B.1 presents descriptions of each of these datasets and describes how the subset was sampled. The dataset mixture was meticulously designed based on ablation studies of both human-annotated and AIgenerated data, with focus on complexity and diversity. Models fine-tuned on it showed significant improvements in overall performance on key benchmarks compared to those trained on individual datasets. We adopted the user-assistant message format from TÜLU-V2-mix (see Appendix B.2) to structure our entire Darija-SFT-Mixture dataset. 22https://hf.co/datasets/MBZUAI-Paris/ MoroccanSocialMedia-MultiGen 23https://github.com/qcri/QADI 24https://github.com/kabilessefar/ 25https://www.9esa.com 26https://hf.co/datasets/MBZUAI-Paris/"
        },
        {
            "title": "DarijaStory",
            "content": "27https://hf.co/datasets/allenai/ OMCD-Offensive-Moroccan-Comments-Dataset tulu-v2-sft-mixture 6 To ensure quality, we first filtered out instructions from TÜLU-V2-mix that are either inappropriate for typical Darija speakers or could lose meaning or coherence when translated, such as scientific content, translation tasks, and non-English samples. We then experimented with several opensource and closed-source models for English-toDarija translation, including NLLB (Costa-jussà et al., 2022), GPT, and others. Our results showed that closed-source models consistently outperformed open-source alternatives, with Claude 3.5 Sonnet emerging as our final choice. Finally, we implemented several post-processing measures to correct errors introduced by the automatic translation. All details are provided in Appendix B.3."
        },
        {
            "title": "7 Training Details",
            "content": "In this section, we outline the training details and present the experimental analysis of various finetuning strategies and base model choices that informed our final settings. Base model selection. Initially, we considered the two Arabic models: Jais and AceGPT (as ALLaM is not open-weights). Later, we included Gemma 2 based on positive feedback from Arabic LLM community, as it can serve as strong starting point for Arabic fine-tuning tasks. We also compared the performance differences between fine-tuning on an instruction-tuned model and base model. Our results indicate that continual fine-tuning of instruction-tuned Gemma 2 models (Gemma-2-2BIt, 9B-It28, and 27B-It) yields significantly higher scores than other settings on our dataset. Training framework. We also investigated the performance differences between full fine-tuning and parameter-efficient approaches (Han et al., 2024). Results indicate that the latter, with Low-Rank Adaptation (LoRA) (Hu et al., 2021), proved to be more effective, whereas full fine-tuning resulted in catastrophic forgetting (French, 1999). This is supported by the recent work of Biderman et al. (2024), that shows LoRA exhibits desirable form of regularization: it better maintains the base models performance on tasks outside the target domain, and it also helps maintain more diverse generations. Hyperparameters. LoRA was set with rank 256 and alpha 128. We run the training for 3 epochs, and set the learning rate to 5e-5 with warmup ratio of 3%, and per_device_train_batch_size to 4, 28https://hf.co/google/gemma-2-9b-it with gradients accumulated over 4 steps.The maximum input context length was configured to 2048. We used bfloat16 to optimize training speed. The loss is computed only on the responses, not on the prompts of instructions. The Atlas-Chat models were trained on 8 Nvidia A100 80 GB GPUs in parallel, utilizing FSDP strategy on AWS SageMaker."
        },
        {
            "title": "8 Evaluation Benchmarks",
            "content": "To evaluate LLM performance in Darija, we developed comprehensive suite that includes benchmarks such as DarijaMMLU, DarijaHellaSwag, DarijaAlpacaEval, and DarijaBench. Additionally, we evaluated using an existing benchmark, Belebele. All our custom benchmarks are integrated into fork29 of the LM-Evaluation-Harness repository (Gao et al., 2024) to ensure reproducibility and foster future model comparison. DarijaMMLU30. It is constructed by translating two major benchmarks into Darija from English and MSA: Massive Multitask Language Understanding (MMLU) (Hendrycks et al., 2020)31 and ArabicMMLU (Koto et al., 2024)32, whose subsets that were either too technical (beyond typical user needs) or culturally inappropriate for the Moroccan context were excluded. The remaining samples were translated into Darija using Claude 3.5 Sonnet. The benchmark consists of 22,027 multiple-choice questions, with the number of choices ranging from 2 to 5. The subsets we selected are listed in C.4. DarijaHellaSwag33. HellaSwag34 (Zellers et al., 2019) is multiple-choice dataset designed to evaluate machine reading comprehension and commonIt presents complex scenarios sense reasoning. where models must select the most plausible continuation of passage from four options, challenging nuanced language understanding and contextual inference. Using Claude 3.5 Sonnet, We translated the HellaSwag validation set into Darija. Belebele_Ary. Belebele (Bandarkar et al., 2024)35 is multiple-choice machine reading comprehension dataset designed to evaluate both monolingual and multilingual models across 122 languages. 29https://github.com/MBZUAI-Paris/ lm-evaluation-harness-atlas-chat 30https://hf.co/datasets/MBZUAI-Paris/"
        },
        {
            "title": "DarijaMMLU",
            "content": "31https://hf.co/datasets/cais/mmlu 32https://hf.co/datasets/MBZUAI/ArabicMMLU 33https://hf.co/datasets/MBZUAI-Paris/"
        },
        {
            "title": "DarijaHellaSwag",
            "content": "34https://hf.co/datasets/Rowan/hellaswag 35https://hf.co/datasets/facebook/belebele 7 Each question is paired with brief passage and offers four multiple-choice answers. For our work, we specifically used the Ary_Arab (indicating Moroccan Arabic) subset of Belebele. DarijaAlpacaEval36. Claude 3.5 Sonnet was prompted to translate and culturally adapt the AlpacaEval dataset (Li et al., 2023) into Darija, to evaluate the instruction-following capabilities and cultural alignment of LLMs in Darija. The dataset consists of 805 instructions, focusing on culturally relevant content tailored to the Moroccan context. More details about the dataset creation and evaluation method can be found in Appendix C.3. DarijaBench37. In addition to the above benchmarks, we evaluated with the test sets from the native Darija datasets (see Section 4). Typically, 10% of each subset is reserved for testing, unless the original source provides pre-defined separate test set. The combined test sets, referred to as DarijaBench, encompass three tasks: Translation, Sentiment Analysis, and Summarization."
        },
        {
            "title": "9 Results",
            "content": "Evaluation measures. We employed Accuracy to evaluate models on multiple-choice benchmarks, including DarijaMMLU, DarijaHellaSwag, Belebele_Ary, and the discriminative sentiment analysis task within DarijaBench. For translation and summarization tasks, we adopted the conventional BLEU (Papineni et al., 2002) and ROUGE-1/L (Lin, 2004), respectively. However, since these metrics are based on n-grams, they are not well-suited for assessing Darija. For example, the same word in Darija can be written in multiple ways (\"How are you?\" = \"(cid:81)(cid:75)(cid:10)(cid:89)(cid:74)(cid:10)(cid:187)\" = \"(cid:81)(cid:75)(cid:10)(cid:64)(cid:89)(cid:74)(cid:10)(cid:187)\" = \"(cid:81)(cid:75)(cid:10)(cid:64)(cid:88) (cid:250)(cid:10)(cid:187)\") due to the lack of standardization (e.g., diacritics, agglutinations, borrowings), making them overly rigid in cases where slight variations still convey the same meaning. To gain more fine-grained insight, we also included chrF (Popovic, 2015), operating at the level of character n-grams. In addition, to capture higher-level semantic similarity, we also used BERTScore (Zhang et al., 2019), with DarijaBERT as the reference model for summarization, and multilingual BERT38 for translation. These evaluations were conducted in zero-shot setting using greedy 36https://hf.co/datasets/MBZUAI-Paris/"
        },
        {
            "title": "DarijaAlpacaEval",
            "content": "37https://hf.co/datasets/MBZUAI-Paris/"
        },
        {
            "title": "DarijaBench",
            "content": "38https://hf.co/google-bert/ bert-base-multilingual-cased decoding, and some in few-shot setting. The number of few-shot examples was chosen based on relevant literature and standard practices. For summarization evaluation, we also employ the LLM-as-a-Judge approach (Zheng et al., 2023), where model judges the preferred summary between reference and generated one, based on predefined criteria. We report the win-rate, defined as the percentage of instances where the generated summary is chosen over the reference. Detailed information on the judge model, prompt, bias mitigation, and selection criteria is in Appendix C.5. DarijaAlpacaEval employs the same approach as LLM-as-a-Judge, where we choose Jais-13B-Chat, the first Arabic-specialized LLM, as the reference. For these two evaluations, we applied the default sampling-based decoding. Baseline models. We compared Atlas-Chat with instruction-tuned models from new Jais series (including the -family models trained from scratch and the -adapted ones based on LLaMA 2), along with AceGPT, LLaMA 3.1, 3.2, and Gemma 2 (our base model). Given that Atlas-Chat features 2B, 9B, and 27B sizes, we extended our comparison to the closest larger-sized model above 27B when available, while included all smaller-sized ones. Zero-shot performance. The evaluation results in Table 2 demonstrate the exceptional performance of Atlas-Chat models across all Darija benchmarks. Compared to baseline models with 7B or fewer parameters, Atlas-Chat-2B shows significantly superior zero-shot performance. Atlas-Chat-2B surpassed its closest competitor, Jais-family-6.7Bchat, by performance gaps of 5.05% on DarijaMMLU, 2.40% on DarijaHellaSwag, 2.11% on Belebele_Ary, 27.13% on DarijaAlpacaEval, and 17.08% on sentiment analysis. In translation and summarization tasks, Atlas-Chat-2B outperformed other models across all evaluation metrics. The strong zero-shot performance of Atlas-Chat is further enhanced by the larger-sized Atlas-Chat9B, which consistently outperforms other baseline models with parameters less than or equal to 13B, achieving the highest scores in 14 out of 16 metrics. Its strength is especially evident in translation as it leads in all three metrics, chrF, BLEU, and BERTScore, by significant margin. Moreover, the model excels in DarijaMMLU, DarijaHellaSwag, Belebele_Ary, DarijaAlpacaEval, and sentiment analysis, surpassing larger models like AceGPT13B-chat and Jais-family-13B-Chat. 8 Base Model DarijaMMLU DarijaHellaSwag Belebele_Ary Darija AlpacaEval Sentiment Analysis Translation (DODa-10K) Summarization (MArSum) 0-shot 3-shot 0-shot 10-shot 0-shot 5-shot chrF BLEU BERTScore chrF ROUGE-1 ROUGE-L BERTScore LLM Judge Llama-3.2-1B-Instruct Jais-family-1.3B-chat Gemma-2-2B-It Jais-family-2.7B-chat Llama-3.2-3B-Instruct Jais-family-6.7B-chat Jais-Adapted-7B-chat AceGPT-7B-chat Atlas-Chat-2B 27.66 30.79 26.88 35.39 31.24 27.71 28.59 38.22 27.72 37.58 31.76 29.10 32.60 31.17 28.33 39.96 33.42 32.64 39.30 39.07 29.55 36.00 29.31 30.33 45.01 44.43 35. Gemma-2-9B-It Jais-family-13B-Chat Llama-3.1-8B-Instruct Jais-Adapted-13B-chat 44.14 44.75 31.40 35.96 56.38 33.61 45.08 41.91 33.98 45.31 46.92 32.84 41.05 36.55 32.19 58.32 59.31 43.65 jais-family-30B-8k-chat 51.88 49.27 35.61 36.47 59.80 37.04 AceGPT-13B-chat gemma-2-27b-it Atlas-Chat-9B 27.03 27.25 27.65 28.32 28.26 32.64 29.97 30.83 34.55 31.94 35.06 33.93 33.25 33.05 44. 36.77 39.38 28.89 24.00 38.89 37.44 25.22 40.67 45.00 38.67 38.00 40.77 51.22 46.67 43.56 30.67 30.33 25.67 53.33 56.67 47.22 28.56 31.33 69.22 58.56 48.56 50.11 47.33 33.11 36.78 74.33 79.44 65.67 22.89 35.78 75.56 Atlas-Chat-27B 61.95 63.30 48.37 48.72 75.67 80.67 23.57 35.56 58.67 52.97 47.62 65.18 61.84 47.31 92.31 78.08 90.86 69.93 77.52 52. 95.62 56.73 95.07 96.58 46.27 44.82 53.38 51.67 49.20 56.93 52.96 40.18 74.01 44.17 59.93 41.79 66.85 59. 81.85 24.64 57.59 73.00 5.95 6.01 3.58 7.51 13.67 11.81 9.36 11.34 0.07 0.12 0.07 0.26 0.62 0.71 0.60 0.45 44.86 22. 13.82 15.04 11.73 10.48 14.22 0.84 0.85 0.93 0.88 0.69 50.44 27.98 14.40 13.04 1.10 0.67 51.74 29. 37.45 39.17 35.31 39.80 43.78 45.80 45.03 45.36 73.72 44.62 48.28 45.90 47.85 47.97 76.30 47.22 48.17 77. 27.78 20.56 0.48 20.63 27.56 22.12 23.20 27.18 28.80 28.66 25.49 22.53 23.80 26.83 32.07 22.31 9.64 32. 7.35 6.85 0.49 7.74 8.16 7.98 7.82 7.60 9.00 10.20 9.84 7.99 8.86 7.92 9.50 8.15 5. 7.18 6.72 0.48 7.60 8.09 7.82 7.63 7.55 8.88 9.93 9.93 9.64 7.84 8.63 9.45 7.97 5. 10.53 10.42 38.32 35.77 24.44 36.38 38.56 37.10 36.89 37.29 44.71 39.37 39.37 38.00 37.13 37.67 47. 37.17 37.22 47.82 8.23 0.50 6.79 0.89 8.23 3.02 2.82 2.28 55.22 16.14 13.81 1.77 1.92 2.80 59. 0.46 11.10 60.70 Table 2: Performance comparison of Atlas-Chat and state-of-the-art models on the evaluation suite with prompts written in Darija. The highest scores are indicated in bold, the second-highest are underlined, and the third-highest are in italic. Figure 1 shows the average score over all the benchmarks and measures for each model. Our largest model, Atlas-Chat-27B, consistently outperforms competitors, including Jaisfamily-30B-8k-chat and Gemma-2-27B-It. In DarijaMMLU, DarijaHellaSwag, Belebele_Ary, and DarijaAlpacaEval, it achieves zero-shot performance gaps of 10.07%, 12.76%, 1.51%, and 10.00%, respectively, over the highest-performing competitor. Similarly, in translation and summarization tasks, Atlas-Chat-27B demonstrates significant zero-shot performance advantages over its closest competitor, with substantial performance improvements over all evaluation metrics. Few-shot performance. Atlas-Chat demonstrated further improvements when moving from the zeroshot to the few-shot setting, with the effect being particularly pronounced for the 9B and 27B models, especially on the Belebele_Ary benchmark. However, this enhancement in few-shot performance is not observed for the Atlas-Chat-2B model, despite consistently outperforming competitors. Further analysis. Although Atlas-Chat-27B showed the best overall performance, it was outperformed in the sentiment analysis task by smaller counterparts like Atlas-Chat-9B. We hypothesize that this discrepancy might be inherited from our base models, where Gemma-2-9B-it similarly outperformed Gemma-2-27B-it in the same task. Additionally, in the summarization task measured by ROUGE, Atlas-Chat models did not achieve significant leading advantage as seen with other metrics. This discrepancy could stem from the inability of these n-gram-based metrics to fully Figure 1: Average model scores over all benchmarks. capture Darijas nuances. Moreover, summarization, as less constrained generation task, often yields equally valid summaries that vary in formulation. However, when the models summarization capability was evaluated using the LLM-as-a-judge framework, the judge model selected Atlas-Chats responses 60.70% of the time over reference summaries surpassing its closest competitor, Llama3.1-8B-Instruct, by approximately 45%. Similarly, in the translation task measured by BLEU, baseline models demonstrated unexpectedly low performance. Quality analysis indicated that the low performance was due to their inability to consistently produce Darija. For example, in English-to-Darija translation, these models produced outputs consisting solely of MSA or mix of MSA and Darija, resulting in notable lack of overlapping n-grams with the reference text."
        },
        {
            "title": "10 Conclusion",
            "content": "In this work, we presented Atlas-Chat, the first collection of large language models specifically developed for dialectal Arabic, with primary focus on Moroccan Darija. We constructed comprehensive instruction dataset by consolidating existing Darija resources, creating novel datasets both manually and synthetically, and translating English instructions with rigorous control measures. To evaluate LLM performance in Darija, we also introduced several benchmarks including both discriminative and generative tasks. Atlas-Chat models showed superior performance in following Darija instructions and executing standard NLP tasks, outperforming both state-of-the-art and Arabic-specialized LLMs. Our work highlights the potential of targeted LLM development for underrepresented languages and offers comprehensive design methodologies of instruction-tuning that can be applied to similar language adaptation challenges."
        },
        {
            "title": "Limitations",
            "content": "Despite the promising results, our work has some limitations. First, the model occasionally generates hallucinations. Second, the dataset may contain inherent biases that could affect the models fairness and representation. Additionally, we relied heavily on Claude for translating English instructions into Darija. However, because Claude is primarily trained on English and reflects Western cultural values, it may not fully capture the unique nuances of Darija. Moreover, our models lack preferencetuning to better align with Darija speakers. We intend to address these limitations in future work."
        },
        {
            "title": "Acknowledgments",
            "content": "The authors would like to thank all the Moroccan Darija speakers who warmly contributed to this work from its inception, assisting with data annotation and selection, and evaluating the quality of model outputs in their language."
        },
        {
            "title": "References",
            "content": "Ahmed Abdelali, Hamdy Mubarak, Shammur Chowdhury, Maram Hasanain, Basel Mousi, Sabri Boughorbel, Samir Abdaljalil, Yassine El Kheir, Daniel Izham, Fahim Dalvi, Majd Hawasly, Nizi Nazar, Youssef Elshahawy, Ahmed Ali, Nadir Durrani, Natasa MilicFrayling, and Firoj Alam. 2024. LAraBench: Benchmarking Arabic AI with large language models. In Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers), pages 487520, St. Julians, Malta. Association for Computational Linguistics. Ahmed Abdelali, Hamdy Mubarak, Younes Samih, Sabit Hassan, and Kareem Darwish. 2021. QADI: Arabic dialect identification in the wild. In Proceedings of the Sixth Arabic Natural Language Processing Workshop, pages 110, Kyiv, Ukraine (Virtual). Association for Computational Linguistics. Lucas Bandarkar, Davis Liang, Benjamin Muller, Mikel Artetxe, Satya Narayan Shukla, Donald Husa, Naman Goyal, Abhinandan Krishnan, Luke Zettlemoyer, and Madian Khabsa. 2024. The belebele benchmark: parallel reading comprehension dataset in 122 language variants. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 749775, Bangkok, Thailand. Association for Computational Linguistics. Saiful Bari, Yazeed Alnumay, Norah Alzahrani, Nouf Alotaibi, Hisham Alyahya, Sultan AlRashed, Faisal Mirza, Shaykhah Alsubaie, Hassan Alahmed, Ghadah Alabduljabbar, et al. 2024. Allam: Large language models for arabic and english. arXiv preprint arXiv:2407.15390. Dan Biderman, Jose Gonzalez Ortiz, Jacob Portes, Mansheej Paul, Philip Greengard, Connor Jennings, Daniel King, Sam Havens, Vitaliy Chiley, Jonathan Frankle, et al. 2024. Lora learns less and forgets less. arXiv preprint arXiv:2405.09673. Houda Bouamor, Nizar Habash, Mohammad Salameh, Wajdi Zaghouani, Owen Rambow, Dana Abdulrahim, Ossama Obeid, Salam Khalifa, Fadhl Eryani, Alexander Erdmann, and Kemal Oflazer. 2018. The MADAR Arabic dialect corpus and lexicon. In Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018), Miyazaki, Japan. European Language Resources Association (ELRA). ElMehdi Boujou, Hamza Chataoui, Abdellah El Mekki, Saad Benjelloun, Ikram Chairi, and Ismail Berrada. 2021. An open access nlp dataset for arabic dialects: Data collection, labeling, and model construction. arXiv preprint arXiv:2102.11000. Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. 2023. Sharegpt4v: Improving large multimodal models with better captions. arXiv preprint arXiv:2311.12793. Marta Costa-jussà, James Cross, Onur Çelebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, et al. 2022. No language left behind: Scaling human-centered machine translation. arXiv preprint arXiv:2207.04672. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783. Abdeljalil Elouardighi, Mohcine Maghfour, and Hafdalla Hammia. 2017. Collecting and processing arabic facebook comments for sentiment analysis. In Model and Data Engineering: 7th International Conference, MEDI 2017, Barcelona, Spain, October 46, 2017, Proceedings 7, pages 262274. Springer. Kabil Essefar, Hassan Ait Baha, Abdelkader El Mahdaouy, Abdellah El Mekki, and Ismail Berrada. 2023. Omcd: Offensive moroccan comments dataset. Language Resources and Evaluation, 57(4):17451765. Alexander R. Fabbri, Wojciech Kryscinski, Bryan McCann, Caiming Xiong, Richard Socher, and Dragomir Radev. 2021. SummEval: Re-evaluating summarization evaluation. Transactions of the Association for Computational Linguistics, 9:391409. Robert French. 1999. Catastrophic forgetting in connectionist networks. Trends in cognitive sciences, 3(4):128135. Kamel Gaanoun, Abdou Mohamed Naira, Anass Allak, and Imade Benelallam. 2022. Automatic text summarization for moroccan arabic dialect using an artificial intelligence approach. In International Conference on Business Intelligence, pages 158177. Springer. Kamel Gaanoun, Abdou Mohamed Naira, Anass Allak, and Imade Benelallam. 2024. Darijabert: step forward in nlp for the written moroccan dialect. International Journal of Data Science and Analytics, pages 113. Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noach, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. 2024. framework for few-shot language model evaluation. Moncef Garouani and Jamal Kharroubi. 2021. Mac: an open and free moroccan arabic corpus for sentiment analysis. In The Proceedings of the International Conference on Smart City Applications, pages 849 858. Springer. Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio César Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, et al. 2023. Textbooks are all you need. arXiv preprint arXiv:2306.11644. Zeyu Han, Chao Gao, Jinyang Liu, Sai Qian Zhang, et al. 2024. Parameter-efficient fine-tuning for large models: comprehensive survey. arXiv preprint arXiv:2403.14608. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685. Huang Huang, Fei Yu, Jianqing Zhu, Xuening Sun, Hao Cheng, Song Dingjie, Zhihong Chen, Mosen Alharthi, Bang An, Juncai He, Ziche Liu, Junying Chen, Jianquan Li, Benyou Wang, Lian Zhang, Ruoyu Sun, Xiang Wan, Haizhou Li, and Jinchao Xu. 2024. AceGPT, localizing large language models in Arabic. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 81398163, Mexico City, Mexico. Association for Computational Linguistics. Julie Hunter, Jérôme Louradour, Virgile Rennard, Ismaïl Harrando, Guokan Shang, and Jean-Pierre Lorré. 2023. The claire french dialogue dataset. arXiv preprint arXiv:2311.16840. Hamish Ivison, Yizhong Wang, Valentina Pyatkin, Nathan Lambert, Matthew Peters, Pradeep Dasigi, Joel Jang, David Wadden, Noah Smith, Iz Beltagy, et al. 2023. Camels in changing climate: Enhancing lm adaptation with tulu 2. arXiv preprint arXiv:2311.10702. Mouad Jbel, Mourad Jabrane, Imad Hafidi, and Abdulmutallib Metrane. 2024. Sentiment analysis dataset in moroccan dialect: bridging the gap between arabic and latin scripted dialect. Language Resources and Evaluation, pages 130. Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023. Mistral 7b. arXiv preprint arXiv:2310.06825. Mohamed Bayan Kmainasi, Rakif Khan, Ali Ezzat Shahroor, Boushra Bendou, Maram Hasanain, and Firoj Alam. 2024. Native vs non-native language prompting: comparative analysis. arXiv preprint arXiv:2409.07054. Andreas Köpf, Yannic Kilcher, Dimitri von Rütte, Sotiris Anagnostidis, Zhi Rui Tam, Keith Stevens, Abdullah Barhoum, Duc Nguyen, Oliver Stanley, Richárd Nagyfi, et al. 2024. Openassistant conversations-democratizing large language model alignment. Advances in Neural Information Processing Systems, 36. Fajri Koto, Haonan Li, Sara Shatnawi, Jad Doughman, Abdelrahman Sadallah, Aisha Alraeesi, Khalid Almubarak, Zaid Alyafeai, Neha Sengupta, Shady Shehata, Nizar Habash, Preslav Nakov, and Timothy Baldwin. 2024. ArabicMMLU: Assessing massive multitask language understanding in Arabic. In Findings of the Association for Computational Linguistics ACL 2024, pages 56225640, Bangkok, Thailand and virtual meeting. Association for Computational Linguistics. Harrison Lee, Samrat Phatale, Hassan Mansoor, Thomas Mesnard, Johan Ferret, Kellie Lu, Colton Bishop, Ethan Hall, Victor Carbune, Abhinav Rastogi, et al. 2023. Rlaif: Scaling reinforcement learning from human feedback with ai feedback. arXiv preprint arXiv:2309.00267. 11 Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Alpacaeval: An automatic evaluator of instruction-following models. https://github.com/tatsu-lab/alpaca_eval. Chin-Yew Lin. 2004. ROUGE: package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 7481, Barcelona, Spain. Association for Computational Linguistics. Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc Le, Barret Zoph, Jason Wei, et al. 2023. The flan collection: Designing data and methods for effective instruction tuning. In International Conference on Machine Learning, pages 2263122648. PMLR. Jérôme Louradour, Julie Hunter, Ismaïl Harrando, Guokan Shang, Virgile Rennard, and Jean-Pierre Lorré. 2024. Claire: Large language models for spontaneous French dialogue. In Actes de la 31ème Conférence sur le Traitement Automatique des Langues Naturelles, volume 1 : articles longs et prises de position, pages 530548, Toulouse, France. ATALA and AFPC. Jean Maillard, Cynthia Gao, Elahe Kalbassi, Kaushik Ram Sadagopan, Vedanuj Goswami, Philipp Koehn, Angela Fan, and Francisco Guzman. 2023. Small data, big impact: Leveraging minimal data for effective machine translation. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 27402756, Toronto, Canada. Association for Computational Linguistics. Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid Palangi, and Ahmed Awadallah. 2023. Orca: Progressive learning from complex explanation traces of gpt-4. arXiv preprint arXiv:2306.02707. Ahmed Oussous, Fatima-Zahra Benjelloun, Ayoub Ait Lahcen, and Samir Belfkih. 2020. Asa: framework for arabic sentiment analysis. Journal of Information Science, 46(4):544559. Ahmed Oussous, Ayoub Ait Lahcen, and Samir Belfkih. 2018. Improving sentiment analysis of moroccan tweets using ensemble learning. In Big Data, Cloud and Applications: Third International Conference, BDCA 2018, Kenitra, Morocco, April 45, 2018, Revised Selected Papers 3, pages 91104. Springer. Aissam Outchakoucht and Hamza Es-Samaali. 2021. Moroccan dialect-darija-open dataset. arXiv preprint arXiv:2103.09687. Aissam Outchakoucht and Hamza Es-Samaali. 2024. The evolution of darija open dataset: Introducing version 2. arXiv preprint arXiv:2405.13016. Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311318, Philadelphia, Pennsylvania, USA. Association for Computational Linguistics. Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. 2023. Instruction tuning with gpt-4. arXiv preprint arXiv:2304.03277. Maja Popovic. 2015. chrF: character n-gram F-score for automatic MT evaluation. In Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 392395, Lisbon, Portugal. Association for Computational Linguistics. Neha Sengupta, Sunil Kumar Sahu, Bokang Jia, Satheesh Katipomu, Haonan Li, Fajri Koto, William Marshall, Gurpreet Gosal, Cynthia Liu, Zhiming Chen, et al. 2023. Jais and jais-chat: Arabiccentric foundation and instruction-tuned open genarXiv preprint erative large language models. arXiv:2308.16149. Toshiyuki Takezawa, Genichiro Kikui, Masahide Mizushima, and Eiichiro Sumita. 2007. Multilingual spoken language corpus development for communication research. In International Journal of Computational Linguistics & Chinese Language Processing, Volume 12, Number 3, September 2007: Special Issue on Invited Papers from ISCSLP 2006, pages 303324. Mingxu Tao, Chen Zhang, Quzhe Huang, Tianyao Ma, Songfang Huang, Dongyan Zhao, and Yansong Feng. 2024. Unlocking the potential of model merging for low-resource languages. arXiv preprint arXiv:2407.03994. Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Léonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ramé, et al. 2024. Gemma 2: Improving open language models at practical size. arXiv preprint arXiv:2408.00118. Atnafu Lambebo Tonja, Bonaventure FP Dossou, Jessica Ojo, Jenalea Rajab, Fadel Thior, Eric Peter Wairagala, Aremu Anuoluwapo, Pelonomi Moiloa, Jade Abbott, Vukosi Marivate, et al. 2024. Inkubalm: small language model for low-resource african languages. arXiv preprint arXiv:2408.17024. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288. Ahmet Üstün, Viraat Aryabumi, Zheng Yong, Wei-Yin Ko, Daniel Dsouza, Gbemileke Onilude, Neel Bhandari, Shivalika Singh, Hui-Lee Ooi, Amr Kayid, Freddie Vargus, Phil Blunsom, Shayne Longpre, Niklas Muennighoff, Marzieh Fadaee, Julia Kreutzer, and Sara Hooker. 2024. Aya model: An instruction finetuned open-access multilingual language model. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1589415939, Bangkok, Thailand. Association for Computational Linguistics. Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew Dai, and Quoc Le. 2021. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652. 12 Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. 2023. Wizardlm: Empowering large language models to follow complex instructions. arXiv preprint arXiv:2304.12244. Xue. 2020. mt5: massively multilingual prearXiv preprint trained text-to-text transformer. arXiv:2010.11934. Omar F. Zaidan and Chris Callison-Burch. 2014. Arabic dialect identification. Computational Linguistics, 40(1):171202. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019. HellaSwag: Can machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 47914800, Florence, Italy. Association for Computational Linguistics. Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Weinberger, and Yoav Artzi. 2019. Bertscore: Evaluating text generation with bert. arXiv preprint arXiv:1904.09675. Yiran Zhao, Wenxuan Zhang, Guizhen Chen, Kenji Kawaguchi, and Lidong Bing. 2024. How do large arXiv language models handle multilingualism? preprint arXiv:2402.18815. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. 2023. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36:4659546623. Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al. 2024. Lima: Less is more for alignment. Advances in Neural Information Processing Systems, 36."
        },
        {
            "title": "A Instruction Data Templates",
            "content": "In this section, we list the instruction templates used for constructing our Darija-SFT-Mixture dataset. A.1 Machine Translation n[source language text]n :[target language] (cid:201)(cid:203) [source language] (cid:9)(cid:225)(cid:211) (cid:209)(cid:107)(cid:46) (cid:81)(cid:16)(cid:75) user: assistant: [target language text] A.2 Transliteration user: assistant: [target language text] n[source language text]n:[source language] (cid:200)(cid:65)(cid:75)(cid:10)(cid:88) (cid:9)(cid:172)(cid:240)(cid:81)(cid:109)(cid:204)(cid:39)(cid:65)(cid:75)(cid:46) (cid:250)(cid:10)(cid:230)(cid:17)(cid:133)(cid:88)(cid:65)(cid:235) (cid:73)(cid:46) (cid:16)(cid:74)(cid:187) A.3 Sentiment Analysis user: assistant: [target] A.4 Automatic Summarization user: assistant: [summary] A.5 MoroccanWikipedia-QA Template 1: n[source text]: n(cid:63) (cid:16)(cid:233)(cid:202)(cid:210)(cid:109)(cid:46)(cid:204)(cid:39)(cid:64) (cid:88)(cid:65)(cid:235) (cid:200)(cid:65)(cid:75)(cid:10)(cid:88) (cid:128)(cid:65)(cid:130)(cid:107)(cid:66)(cid:13) (cid:64) (cid:241)(cid:235) (cid:241)(cid:9)(cid:74) (cid:17)(cid:131) (cid:16)(cid:232)(cid:80)(cid:65)(cid:74)(cid:46)(cid:170)(cid:203)(cid:64) (cid:16)(cid:72)(cid:66)(cid:65)(cid:210)(cid:16)(cid:74)(cid:107)(cid:66)(cid:13) (cid:64) n(cid:250)(cid:10)(cid:230)(cid:46)(cid:202)(cid:131)- n(cid:250)(cid:10)(cid:71)(cid:46) (cid:65)(cid:109)(cid:46)(cid:26)(cid:39)(cid:10)(cid:64) - n: n:(cid:169)(cid:162)(cid:16)(cid:174)(cid:214)(cid:207)(cid:64) (cid:88)(cid:65)(cid:235) (cid:145)(cid:9)(cid:109)(cid:204) [passage] user: nn:(cid:200)(cid:64) (cid:13)(cid:241)(cid:130)(cid:203)(cid:64) (cid:250)(cid:206)(cid:171) (cid:72)(cid:46) (cid:240)(cid:65)(cid:103)(cid:46) (cid:240) (cid:145)(cid:9)(cid:74)(cid:203)(cid:64) (cid:88)(cid:65)(cid:235) (cid:64)(cid:81)(cid:16)(cid:175) nn [passage] nn [question] assistant: [answer] Template 2: Template 3: nn [question]n :(cid:200)(cid:64) (cid:13)(cid:241)(cid:130)(cid:203)(cid:64) (cid:88)(cid:65)(cid:235) (cid:250)(cid:206)(cid:171) (cid:72)(cid:46) (cid:240)(cid:65)(cid:103)(cid:46) user: assistant: [answer] (cid:65)(cid:75)(cid:46) (cid:64)(cid:88)(cid:240) nn[passage] nn:(cid:145)(cid:9)(cid:74)(cid:203)(cid:64) (cid:88)(cid:65)(cid:235) (cid:64)(cid:81)(cid:16)(cid:175) user: nn[passage] nn(cid:145)(cid:9)(cid:74)(cid:203)(cid:65)(cid:9)(cid:175) (cid:250)(cid:10)(cid:205) (cid:250)(cid:10)(cid:230)(cid:17)(cid:132)(cid:187) (cid:64)(cid:88) (cid:9)(cid:225)(cid:211) (cid:65)(cid:16)(cid:175)(cid:67)(cid:162)(cid:9)(cid:29)(cid:64) (cid:200)(cid:64) (cid:13)(cid:241)(cid:130)(cid:203)(cid:64) (cid:88)(cid:65)(cid:235) (cid:250)(cid:206)(cid:171) (cid:72)(cid:46) (cid:240)(cid:65)(cid:103)(cid:46) nn [question] assistant: [answer] A.6 MoroccanSocialMedia-MultiGen"
        },
        {
            "title": "Continuation",
            "content": "user: assistant: [completion] [source sentence] (cid:16)(cid:233)(cid:202)(cid:210)(cid:109)(cid:46)(cid:204)(cid:39)(cid:64) (cid:88)(cid:65)(cid:235) (cid:201)(cid:210)(cid:187)(cid:58)"
        },
        {
            "title": "Safe Response",
            "content": "user: assistant: [reply] user: assistant: [summary] [message]n :(cid:104)(cid:46) (cid:65)(cid:130)(cid:28)(cid:10)(cid:214)(cid:207)(cid:64) (cid:88)(cid:65)(cid:235) (cid:250)(cid:206)(cid:171) (cid:72)(cid:46) (cid:240)(cid:65)(cid:103)(cid:46) [passage]n :(cid:145)(cid:9)(cid:74)(cid:203)(cid:64) (cid:88)(cid:65)(cid:235) (cid:145)(cid:9)(cid:109)(cid:204) user: [source sentence]n :(cid:248)(cid:81) (cid:9)(cid:107)(cid:64) assistant: [resphrased sentence] (cid:16)(cid:233)(cid:16)(cid:174)(cid:75)(cid:10)(cid:81)(cid:163) (cid:250)(cid:10)(cid:230)(cid:17)(cid:132)(cid:29)(cid:46) (cid:16)(cid:233)(cid:202)(cid:210)(cid:109)(cid:46)(cid:204)(cid:39)(cid:64) (cid:88)(cid:65)(cid:235) (cid:73)(cid:46) (cid:16)(cid:74)(cid:187) user: assistant: [explanation] [source sentence]n : (cid:16)(cid:233)(cid:202)(cid:210)(cid:109)(cid:46)(cid:204)(cid:39)(cid:64) (cid:88)(cid:65)(cid:235) (cid:65)(cid:74)(cid:10)(cid:203) (cid:104)(cid:81)(cid:229)(cid:17)(cid:133) user: [source sentence]n : assistant: [safe response] (cid:16)(cid:233)(cid:75)(cid:46) (cid:88) (cid:13) (cid:65)(cid:211) (cid:16)(cid:233)(cid:16)(cid:174)(cid:75)(cid:10)(cid:81)(cid:162)(cid:29)(cid:46) (cid:250)(cid:10)(cid:230)(cid:17)(cid:133)(cid:88)(cid:65)(cid:235) (cid:250)(cid:206)(cid:171) (cid:72)(cid:46) (cid:240)(cid:65)(cid:103)(cid:46) A.7 DarijaStory-Completion user: assistant: [completion] [story]n : (cid:16)(cid:233)(cid:146)(cid:16)(cid:174)(cid:203) (cid:88)(cid:65)(cid:235) (cid:201)(cid:210)(cid:187) TÜLU-V2-mix and Translation In this section, we provide detailed overview of the TÜLU-V2-mix dataset and its translation process into Darija, including the datasets it incorporates and the sampling strategies employed. We also describe the datasets format and the steps involved in translating the dataset to Moroccan Darija. B.1 Composition of TÜLU-V2-mix TÜLU-V2-mix incorporates subsets from the following datasets: FLAN (Wei et al., 2021)39, Open Assistant 1 (Köpf et al., 2024)40, ShareGPT (Chen et al., 2023)41, GPT4-Alpaca (Peng et al., 2023)42, Code-Alpaca43, LIMA (Zhou et al., 2024)44, WizardLM Evol Instruct (Xu et al., 2023)45, and Open-Orca (Mukherjee et al., 2023)46. The mixture also incorporates hard-coded instructions and set of sciencerelated questions derived from scientific documents. Table 3 presents descriptions of each of these datasets and describes how the subset in TÜLU-V2-mix was sampled. B.2 Dataset Format TÜLU-V2-mix is structured in \"messages\" format commonly used for conversational datasets. Each interaction consists of sequence of messages, where each message is represented as JSON object with at least two key-value pairs: 39https://github.com/google-research/FLAN/tree/main 40https://hf.co/datasets/OpenAssistant/oasst1 41https://hf.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered 42https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM#data-release 43https://github.com/sahil280114/codealpaca 44https://hf.co/datasets/GAIR/lima 45https://hf.co/datasets/WizardLM/WizardLM_evol_instruct_V2_196k 46https://hf.co/datasets/Open-Orca/OpenOrca"
        },
        {
            "title": "Sampling Strategy",
            "content": "A collection of datasets with tasks such as question answering, summarization, translation, and more. 100,000 examples from FLAN v2, split equally between general tasks and the CoT subset. Open Assistant 1 human-annotated assistant-style conversation corpus. Top-ranked paths in conversation trees."
        },
        {
            "title": "ShareGPT",
            "content": "User-shared conversations with ChatGPT and GPT-4. 7,708 examples. 114,046 samples ShareGPT dataset. from processed GPT4-Alpaca GPT-4 generated responses to prompts from Alpaca. 20,000 samples. Code-Alpaca Coding instruction-tuning data generated by text-davinci003. All 20,022 examples."
        },
        {
            "title": "LIMA",
            "content": "Carefully selected data with special focus on quality. All 1,030 examples."
        },
        {
            "title": "WizardLM Evol\nInstruct",
            "content": "Automatic evolution of instruction datasets, enhancing the complexity and diversity of instructions. 30,000 examples. Open-Orca"
        },
        {
            "title": "Hardcoded",
            "content": "Augmented FLAN data with additional generated explanations. 30,000 samples generated by GPT-4. Prompts ensuring the model correctly answers questions about its identity or creators. 14 samples each repeated 10 times = 140 total samples."
        },
        {
            "title": "Science",
            "content": "Scientific documents understanding tasks. 7,544 examples. Table 3: Subsets of TÜLU-V2-mix. \"role\": Specifies the role of the participant in the conversation. Typically, this is either \"user\" (the person asking questions or giving prompts) or \"assistant\" (the models response). \"content\": Contains the actual text of the message. This is where the question, instruction, or response is written. Figure 2 shows how samples from TÜLU-V2-mix are formatted. [ { }, { } ] \" role \": \" user \", \" content \": \" What is the latest release version of the Lua programming language ?\" \" role \": \" assistant \", \" content \": \" The latest release of the Lua programming language is Lua 5.4.4 , released in January 2022. The latest major version , Lua 5.4 , was released in June generational mode for garbage collection , const variables , and to -be - closed variables .\" 2020 and introduced new features such as new Figure 2: Sample from TÜLU-V2-mix. The \"messages\" format is particularly useful for training conversational models as it simulates multiturn conversations by incorporating alternating roles between user and assistant messages. This format ensures clear distinction between user inputs and the models responses. Additionally, during fine-tuning, the loss function is applied specifically to messages with the role \"assistant,\" to focus optimization on improving response generation. We applied this format to structure the whole training dataset. 16 B.3 Translation to Darija B.3.1 Preprocessing Before translating the dataset into Darija, we applied several filters to ensure that the translation meets our quality requirements: Excluding the Science subset: We removed this part because the questions often involved parts or entire sections from research articles, which could lose meaning or coherence when translated, particularly into Darija. Additionally, we considered that typical Darija-speaking user is unlikely to ask the model about research papers in Darija, as they would more commonly use English for such inquiries. Filtering out empty messages: Based on reported issue47, we discovered that some examples contained turns where the message role was defined, but the content was empty. To ensure data quality, we removed all such samples from the dataset. Removing translation tasks: We decided to omit translation instructions because translating both the source and target sentences into Darija would result in redundant outputs. Even if we specify that only the target sentence should be translated, it would be challenging to consistently ensure that the model performing the Darija translation adheres to the instruction across all examples. Additionally, verifying the quality of the translations would be challenging, particularly when the original meaning could be distorted. Furthermore, we already possess high-quality translation datasets, so including lower-quality translations would only degrade the overall dataset quality. To filter out translation tasks, we removed all samples containing either the strings \"translate \" or \" translation \". We recognize that this method might exclude some instances where translation is mentioned without being the core task, for example, the user might be asking about the definition of the word \"translation\". However, given the large size of TÜLU-V2-mix, we believe such cases are rare, and the potential loss of few samples would not impact the datasets overall quality. Excluding non-English samples: We filtered out non-English examples to ensure higher translation quality, as translating from English to Darija tends to yield more accurate results compared to translations from other languages, especially those with low resources. To implement this filter, we used one of the best language identification tools: the fastText Language Identification model48. We set k=2, meaning the model predicts the two most likely languages for each input text and provides probability score for each. We excluded any samples where the most likely language was not English, as well as those labeled as English with confidence score below 80%. Through multiple experiments, we found that purely English texts typically score close to 100%, while lower scores often indicate the presence of other languages mixed with English. B.3.2 Translation We experimented with several open-source and closed-source Darija translation models, including NLLB200-3.3B49 (No Language Left Behind50), Terjman-Ultra51, GPT-4o52, Claude 3 Opus53, and Claude 3.5 Sonnet54. Our results showed that closed-source models consistently outperformed open-source alternatives, with GPT-4o and Claude 3.5 Sonnet taking the lead. We ultimately chose Claude 3.5 Sonnet, as it slightly outperformed GPT-4o and offered compatibility with Amazon Bedrock. Table 4 shows comparison of an instruction translated to Darija using each of the models we tested. We observed that open-source models, namely NLLB-200-3.3B and Terjman-Ultra, tend to use more MSA, while closed-source models produce translations closer to Moroccan Darija. They also retain key 47https://github.com/allenai/open-instruct/issues/161 48https://hf.co/facebook/fasttext-language-identification 49https://hf.co/facebook/nllb-200-3.3B 50https://ai.meta.com/research/no-language-left-behind 51https://hf.co/atlasia/Terjman-Ultra 52https://openai.com/index/hello-gpt-4o 53https://www.anthropic.com/news/claude-3-family 54https://www.anthropic.com/news/claude-3-5-sonnet 17 formatting elements like line breaks (n) and tags (###), which are crucial for preserving the structure of the instructions."
        },
        {
            "title": "Original\nSentence",
            "content": "NLLB-2003.3B TerjmanUltra GPT-4o"
        },
        {
            "title": "Claude\nOpus",
            "content": "3 Claude 3.5 Sonnet (cid:9)(cid:224) the that (cid:13) (cid:65)(cid:16)(cid:74)(cid:203)(cid:64) (cid:9)(cid:225)(cid:186)(cid:214)(cid:223)(cid:10) (cid:90)(cid:65)(cid:191) (cid:9)(cid:89)(cid:203) (cid:16)(cid:233)(cid:170)(cid:75)(cid:46) (cid:80) response (cid:16)(cid:72)(cid:64)(cid:81)(cid:30)(cid:10)(cid:17)(cid:75) (cid:16)(cid:72)(cid:64)(cid:81)(cid:30)(cid:10)(cid:17)(cid:75) completes (cid:9)(cid:224)(cid:241)(cid:186)(cid:75)(cid:10) (cid:16)(cid:233)(cid:75)(cid:10)(cid:65)(cid:171)(cid:81)(cid:203)(cid:64) (cid:16)(cid:233)(cid:74)(cid:10)(cid:75)(cid:46) (cid:65)(cid:109)(cid:46)(cid:26)(cid:39)(cid:10)(cid:64)(cid:13) appropriately (cid:16)(cid:233)(cid:74)(cid:10)(cid:75)(cid:46) (cid:65)(cid:109)(cid:46)(cid:26)(cid:39)(cid:10)(cid:66)(cid:13) (cid:64) (cid:16)(cid:233)(cid:171)(cid:65)(cid:9)(cid:74)(cid:147) (cid:250)(cid:206)(cid:171) (cid:65)(cid:234)(cid:203) (cid:58) (cid:88)(cid:80) (cid:35)(cid:35)(cid:35) (cid:16)(cid:233)(cid:74)(cid:10)(cid:106)(cid:146)(cid:203)(cid:64) (cid:13) (cid:64) (cid:250)(cid:10)(cid:171)(cid:65)(cid:9)(cid:74)(cid:162)(cid:147)(cid:64) (cid:90)(cid:65)(cid:191) (cid:9)(cid:89)(cid:203) Write request.nn### Instruction:nIdentify four positive impacts that artificial intelligence can have on the healthcare industrynn### Response: (cid:13) (cid:9)(cid:225)(cid:211) (cid:16)(cid:233)(cid:170)(cid:75)(cid:46) (cid:80) (cid:64) (cid:88)(cid:89)(cid:103) (cid:58) (cid:16)(cid:72)(cid:65)(cid:210)(cid:74)(cid:10)(cid:202)(cid:170)(cid:16)(cid:74)(cid:203)(cid:64) (cid:35)(cid:35)(cid:35) (cid:46)(cid:73)(cid:46) (cid:131)(cid:65)(cid:9)(cid:74)(cid:211) (cid:201)(cid:190) (cid:17)(cid:130)(cid:29)(cid:46) (cid:73)(cid:46) (cid:202)(cid:162)(cid:203)(cid:64) (cid:201)(cid:210)(cid:186)(cid:75)(cid:10) (cid:88)(cid:80) (cid:73)(cid:46) (cid:16)(cid:74)(cid:187) (cid:16)(cid:230)(cid:203)(cid:64) (cid:250)(cid:10) (cid:58)(cid:88)(cid:81)(cid:203)(cid:64) (cid:35)(cid:35)(cid:35) (cid:16)(cid:233)(cid:74)(cid:10)(cid:106)(cid:146)(cid:203)(cid:64) (cid:9)(cid:225)(cid:186)(cid:214)(cid:223)(cid:10) (cid:13) (cid:13) (cid:64) (cid:88)(cid:89)(cid:103) (cid:58) (cid:16)(cid:72)(cid:65)(cid:210)(cid:74)(cid:10)(cid:202)(cid:170)(cid:16)(cid:75) (cid:35)(cid:35)(cid:35) (cid:46)(cid:73)(cid:46) (cid:131)(cid:65)(cid:9)(cid:74)(cid:211) (cid:201)(cid:190) (cid:17)(cid:130)(cid:29)(cid:46) (cid:73)(cid:46) (cid:202)(cid:162)(cid:203)(cid:64) (cid:201)(cid:210)(cid:187) (cid:65)(cid:75)(cid:10) (cid:88)(cid:80) (cid:250)(cid:10)(cid:230)(cid:46) (cid:16)(cid:74)(cid:187) (cid:65)(cid:16)(cid:75) (cid:13) (cid:64) (cid:250)(cid:10)(cid:171)(cid:65)(cid:9)(cid:74)(cid:162)(cid:147)(cid:64) (cid:13) (cid:64) (cid:88) (cid:15)(cid:89)(cid:103) (cid:58) (cid:16)(cid:72)(cid:65)(cid:210)(cid:74)(cid:10)(cid:202)(cid:170)(cid:16)(cid:74)(cid:203)(cid:64) (cid:35)(cid:35)(cid:35) n (cid:46)(cid:73)(cid:46) (cid:131)(cid:65)(cid:9)(cid:74)(cid:211) (cid:201)(cid:190) (cid:17)(cid:130)(cid:29)(cid:46) (cid:73)(cid:46) (cid:202) (cid:15)(cid:162)(cid:203)(cid:64) (cid:201)(cid:15)(cid:210)(cid:186)(cid:74)(cid:10)(cid:187) (cid:88)(cid:80) (cid:73)(cid:46) (cid:16)(cid:74)(cid:187) (cid:9)(cid:225)(cid:186)(cid:214)(cid:223)(cid:10) (cid:250)(cid:10)(cid:171)(cid:65)(cid:9)(cid:74)(cid:162)(cid:147)(cid:66)(cid:64) (cid:90)(cid:65)(cid:191) (cid:9)(cid:89)(cid:203)(cid:64) (cid:250)(cid:10)(cid:206)(cid:203)(cid:64) (cid:16)(cid:233)(cid:75)(cid:10)(cid:65)(cid:171)(cid:81)(cid:203)(cid:64) (cid:168)(cid:65)(cid:162)(cid:16)(cid:175) (cid:250)(cid:206)(cid:171) (cid:240)(cid:89)(cid:9)(cid:74)(cid:171) (cid:9)(cid:224)(cid:241)(cid:186)(cid:75)(cid:10) (cid:13) (cid:16)(cid:74)(cid:187) (cid:64) (cid:64)(cid:241)(cid:107)(cid:46) (cid:73)(cid:46) (cid:9)(cid:225)(cid:186)(cid:214)(cid:223)(cid:10) (cid:250)(cid:10)(cid:205) (cid:16)(cid:233)(cid:74)(cid:10)(cid:75)(cid:46) (cid:65)(cid:109)(cid:46)(cid:26)(cid:39)(cid:10)(cid:64)(cid:13) (cid:16)(cid:74)(cid:187)(cid:64) (cid:64)(cid:241)(cid:107)(cid:46) (cid:73)(cid:46) (cid:16)(cid:233)(cid:74)(cid:10)(cid:75)(cid:46) (cid:65)(cid:109)(cid:46)(cid:26)(cid:39)(cid:10)(cid:64)(cid:13) (cid:13) (cid:13) (cid:9)(cid:172)(cid:81)(cid:171) (cid:58) (cid:16)(cid:72)(cid:65)(cid:210)(cid:74)(cid:10)(cid:202)(cid:170)(cid:16)(cid:74)(cid:203)(cid:64) (cid:35)(cid:35)(cid:35) n (cid:46)(cid:73)(cid:46) (cid:131)(cid:65)(cid:9)(cid:74)(cid:211) (cid:201)(cid:190) (cid:17)(cid:130)(cid:29)(cid:46) (cid:73)(cid:46) (cid:202)(cid:162)(cid:203)(cid:64) (cid:201)(cid:210)(cid:186)(cid:75)(cid:10) (cid:250)(cid:10)(cid:205) (cid:72)(cid:46) (cid:65)(cid:16)(cid:75) (cid:169)(cid:75)(cid:46) (cid:80) (cid:64) (cid:64)(cid:241)(cid:109)(cid:46)(cid:204)(cid:39)(cid:64) (cid:35)(cid:35)(cid:35) n (cid:58)(cid:72)(cid:46) (cid:13) (cid:64) (cid:88)(cid:89)(cid:103) (cid:58) (cid:16)(cid:72)(cid:65)(cid:210)(cid:74)(cid:10)(cid:202)(cid:170)(cid:16)(cid:74)(cid:203)(cid:64) (cid:35)(cid:35)(cid:35) n (cid:46)(cid:73)(cid:46) (cid:131)(cid:65)(cid:9)(cid:74)(cid:211) (cid:201)(cid:190) (cid:17)(cid:130)(cid:29)(cid:46) (cid:73)(cid:46) (cid:202)(cid:162)(cid:203)(cid:64) (cid:201)(cid:210)(cid:186)(cid:75)(cid:10) (cid:250)(cid:10)(cid:206)(cid:203)(cid:64) (cid:72)(cid:46) (cid:9)(cid:225)(cid:186)(cid:214)(cid:223)(cid:10) (cid:250)(cid:10)(cid:206)(cid:203)(cid:64) (cid:58)(cid:72)(cid:46) (cid:16)(cid:233)(cid:106)(cid:146)(cid:203)(cid:64) (cid:168)(cid:65)(cid:162)(cid:16)(cid:175) (cid:250)(cid:206)(cid:171) (cid:240)(cid:89)(cid:9)(cid:74)(cid:171) (cid:9)(cid:224)(cid:241)(cid:186)(cid:75)(cid:10) (cid:250)(cid:10)(cid:171)(cid:65)(cid:9)(cid:74)(cid:162)(cid:147)(cid:66)(cid:64) (cid:90)(cid:65)(cid:191) (cid:9)(cid:89)(cid:202)(cid:203) (cid:16)(cid:233)(cid:106)(cid:146)(cid:203)(cid:64) (cid:168)(cid:65)(cid:162)(cid:16)(cid:175) (cid:250)(cid:206)(cid:171) (cid:65)(cid:235)(cid:81)(cid:75)(cid:10)(cid:89)(cid:75)(cid:10) (cid:250)(cid:10)(cid:171)(cid:65)(cid:9)(cid:74)(cid:162)(cid:147)(cid:66)(cid:64) (cid:90)(cid:65)(cid:191) (cid:9)(cid:89)(cid:202)(cid:203) (cid:13) (cid:65)(cid:16)(cid:75) (cid:169)(cid:75)(cid:46) (cid:80) (cid:58)(cid:88)(cid:81)(cid:203)(cid:64) (cid:35)(cid:35)(cid:35) n (cid:64)(cid:241)(cid:109)(cid:46)(cid:204)(cid:39)(cid:64) (cid:35)(cid:35)(cid:35) n (cid:16)(cid:233)(cid:171)(cid:65)(cid:9)(cid:74)(cid:147) (cid:250)(cid:206)(cid:171) (cid:65)(cid:234)(cid:203) (cid:16)(cid:233)(cid:74)(cid:10)(cid:106)(cid:146)(cid:203)(cid:64) (cid:16)(cid:233)(cid:74)(cid:10)(cid:75)(cid:46) (cid:65)(cid:109)(cid:46)(cid:26)(cid:39)(cid:10)(cid:64)(cid:13) (cid:16)(cid:233)(cid:75)(cid:10)(cid:65)(cid:171)(cid:81)(cid:203)(cid:64) (cid:9)(cid:224)(cid:241)(cid:186)(cid:75)(cid:10) (cid:16)(cid:72)(cid:64)(cid:81)(cid:30)(cid:10)(cid:17)(cid:75) (cid:16)(cid:72)(cid:64)(cid:81)(cid:30)(cid:10)(cid:17)(cid:75) (cid:16)(cid:72)(cid:64)(cid:81)(cid:30)(cid:10)(cid:17)(cid:75) (cid:16)(cid:233)(cid:170)(cid:75)(cid:46) (cid:80) (cid:13) (cid:65)(cid:16)(cid:75) (cid:9)(cid:224) Table 4: Translation example for model comparison. We used Amazon Bedrock55, cloud-based machine learning service from AWS, to translate the dataset into Darija. We provided specific instructions to Claude 3.5 Sonnet for handling the translations, refining the prompt after several rounds of experimentation. The final version of the prompt that produced the best results is shown in Figure 3. We altered this prompt slightly as needed for each subset of the dataset, ensuring that the translation remained consistent with the context and structure of each specific subset. Translate the ' content ' field in the paragraph after [ Source Text ] to Moroccan dialect ( Darija - Arabic alphabet ) while following these guidelines : - Keep the format of the original text ( list of json ). - If word is usually not used in Arabic , use its French equivalent . - Do not include any introduction or explanation after the translation , only the translation . - If there is given context , example or question translate it as well . - Whenever you come across code contexts or technical words , keep them in English . - Whenever you come across literature , or example or question , translate it to Moroccan . - If the text is culturaly not accepted for Morrocans , change it to more acceptable one . - Do not answer the request in the source text . - Write first the original text after the tag [[ Original ]] and then the translation after the tag [[ Translation ]]. [ Source Text ] Figure 3: The prompt given to Claude 3.5 Sonnet for translation. We used asynchronous programming techniques with Pythons asyncio library to make multiple concurrent requests (often 25 at time) to the Bedrock translation service. This parallel approach 55https://aws.amazon.com/bedrock 18 significantly sped up the overall translation process by allowing us to handle many requests simultaneously instead of sequentially. B.3.3 Postprocessing After finishing the translation and cleaning the errors, we post-processed the translations by: Replacing non-translated keywords: put\", were not (cid:9)(cid:175)(cid:65) (cid:46)(cid:250)(cid:10) (cid:9)(cid:147)(cid:64)(cid:13) \"Response\", \"Answer\", translated. (cid:16)(cid:134)(cid:65)(cid:74)(cid:10)(cid:131) (cid:44) (cid:16)(cid:233)(cid:74)(cid:10) (cid:9)(cid:147)(cid:81) (cid:9)(cid:174)(cid:203)(cid:64) (cid:44) (cid:16)(cid:72)(cid:65)(cid:210)(cid:74)(cid:10)(cid:202)(cid:170)(cid:16)(cid:75) (cid:44)(cid:72)(cid:46) \"Instructions\", (cid:64)(cid:241)(cid:109)(cid:46)(cid:204)(cid:39)(cid:64) (cid:44)(cid:72)(cid:46) (cid:64)(cid:241)(cid:109)(cid:46)(cid:204)(cid:39)(cid:64) (cid:44) (cid:16)(cid:233)(cid:106)(cid:46) (cid:74)(cid:10) (cid:16)(cid:28)(cid:9)(cid:74)(cid:203)(cid:64) (cid:44) (cid:16)(cid:72)(cid:65)(cid:74)(cid:10)(cid:162)(cid:170)(cid:214)(cid:207)(cid:64) such \"Outand \"Additional Context\" We replaced these keywords with their Darija equivalents: keywords \"Hypothesis\" \"Input\","
        },
        {
            "title": "Some",
            "content": "as Removing samples with excessive English content: We utilized the fastText Language Identification model to detect samples where the predicted language was not Arabic. Since the model does not differentiate dialects, Darija is recognized as Arabic due to its use of Arabic script. We removed samples where the predicted language was not Arabic or where Arabic was predicted with confidence level below 80%."
        },
        {
            "title": "C Additional Details",
            "content": "C.1 Hard Coded Instruction Samples We manually created 13 instruction samples to ensure that the model responds correctly to identity and creator-related questions, such as Who created you? and What is your name?. Each instruction is repeated 10 times to reinforce the memorization of the answers. Figure 4 presents the full list of hard-coded instruction-answer pairs. Figure 4: Hard coded instruction-answer pairs. 19 C.2 Selected Keywords for Tweet Searching We provide the 26 Darija-specific keywords used for tweet collection through the Twitter API, as referenced in Section 5.2. (cid:44) (cid:16)(cid:233)(cid:211)(cid:241) (cid:17)(cid:130)(cid:107) (cid:44) (cid:113)(cid:202)(cid:190)(cid:211) (cid:44) (cid:16)(cid:233)(cid:146)(cid:74)(cid:10) (cid:44) (cid:16)(cid:233)(cid:146)(cid:16)(cid:174) (cid:9)(cid:174)(cid:203)(cid:64) (cid:44) (cid:9)(cid:225)(cid:30)(cid:10)(cid:9)(cid:75)(cid:65)(cid:75)(cid:10) (cid:9)(cid:81)(cid:211) (cid:44) (cid:17)(cid:187)(cid:241)(cid:74)(cid:46)(cid:16)(cid:75) (cid:17)(cid:129)(cid:9)(cid:28)(cid:74)(cid:10)(cid:187)(cid:65)(cid:211) (cid:44) (cid:16)(cid:233)(cid:75)(cid:10)(cid:81)(cid:229)(cid:133)(cid:65)(cid:210)(cid:130)(cid:203)(cid:64) (cid:44)(cid:200)(cid:65)(cid:75)(cid:10)(cid:88) (cid:44)(cid:65)(cid:75)(cid:46) (cid:64)(cid:88) (cid:44)(cid:240)(cid:89)(cid:75)(cid:10) (cid:9)(cid:81)(cid:30)(cid:10)(cid:187) (cid:44) (cid:9)(cid:172)(cid:241) (cid:17)(cid:130)(cid:16)(cid:28)(cid:187) (cid:44) (cid:9)(cid:172)(cid:241) (cid:17)(cid:130)(cid:28)(cid:10)(cid:187) (cid:44)(cid:250)(cid:10)(cid:230)(cid:17)(cid:132)(cid:187)(cid:64)(cid:88) (cid:44)(cid:250)(cid:10)(cid:190)(cid:74)(cid:46)(cid:16)(cid:74)(cid:187) (cid:44)(cid:189)(cid:106) (cid:9)(cid:146)(cid:74)(cid:10)(cid:187) (cid:44) (cid:44) (cid:232)(cid:241)(cid:162) (cid:9)(cid:174)(cid:74)(cid:10)(cid:146)(cid:29)(cid:10) (cid:44) (cid:232)(cid:241)(cid:162) (cid:9)(cid:174)(cid:74)(cid:10)(cid:130)(cid:29)(cid:10) (cid:44)(cid:248)(cid:10) (cid:240)(cid:89)(cid:9)(cid:75)(cid:65)(cid:191) (cid:44)(cid:248)(cid:10) (cid:240)(cid:89)(cid:75)(cid:10)(cid:65)(cid:191) (cid:44) (cid:16)(cid:135)(cid:74)(cid:10)(cid:75)(cid:46) (cid:81)(cid:9)(cid:109)(cid:26)(cid:16)(cid:39) (cid:44) (cid:16)(cid:233)(cid:74)(cid:10)(cid:74)(cid:46)(cid:235)(cid:67) (cid:17)(cid:131) (cid:44) (cid:9)(cid:172)(cid:241) (cid:17)(cid:130)(cid:16)(cid:29)(cid:65)(cid:191) (cid:17)(cid:128)(cid:240)(cid:65)(cid:16)(cid:174)(cid:74)(cid:46) (cid:9)(cid:74)(cid:211) (cid:44) (cid:16)(cid:233)(cid:9)(cid:75)(cid:65)(cid:210)(cid:74)(cid:10)(cid:131) (cid:44) (cid:9)(cid:225)(cid:30)(cid:10)(cid:9)(cid:74)(cid:75)(cid:10)(cid:240) (cid:9)(cid:80) (cid:46)(cid:248)(cid:10) (cid:80)(cid:64)(cid:80)(cid:89)(cid:203)(cid:64) C.3 DarijaAlpacaEval Dataset Creation and Models Evaluation To create the DarijaAlpacaEval dataset, we employed Claude 3.5 Sonnet to translate and culturally adapt the AlpacaEval dataset (Li et al., 2023) for evaluating models capabilities in instruction following in Moroccan Darija. The prompt used for translation is shown in Figure 5. Given the following question about .S . culture :{ english_question }, translate and adapt it to focus on Moroccan culture ."
        },
        {
            "title": "Ensure that the question retains the same underlying theme but is contextually",
            "content": "suitable for Morocco , taking into account cultural , historical , and societal differences . For example , replace references to American holidays , traditions , or figures with their Moroccan counterparts ."
        },
        {
            "title": "The questions should be precise and should not differ significantly in length from",
            "content": "the original question ."
        },
        {
            "title": "Ensure that the question is unique to Morocco and not applicable to any neighboring",
            "content": "countries . Adjust the language from English to Arabic Moroccan Darija . Return only the question with no additional text . Figure 5: The prompt given to Claude 3.5 Sonnet for translation and cultural adaptation of the AlpacaEval instructions. This process resulted in 805 instructions, all adapted to the Moroccan culture and written in Darija. The models were subsequently evaluated by generating responses to these instructions, with their answers compared to baseline model, jais-13b-chat, one of the earliest state-of-the-art models developed for Arabic NLP tasks. To assess cultural appropriateness, Claude 3.5 Sonnet was prompted to compare two model responses for each instruction, using criteria focused on cultural alignment, fluency, and relevance. The evaluation prompt is show in Figure 6. Each pair of baseline and model answers, with positions swapped, was evaluated twice by Claude to determine the better answer. If the position swap influenced Claudes choice, that particular pair was discarded to ensure the methods rhobustness to possible LLM biases. The models win-rate was then calculated as the proportion of instances where Claude selected the models answer over the baseline. C.4 Selected Topics from MMLU and ArabicMMLU The MMLU subjects included in DarijaMMLU are: Global Facts, High School European History, High School Geography, High School Government and Politics, High School Psychology, High School Statistics, High School World History, Human Aging, International Law, Jurisprudence, Logical Fallacies, Management, Marketing, Moral Disputes, Moral Scenarios, Nutrition, Philosophy, Professional Law, Professional Psychology, Public Relations, Security Studies, Sociology, and World Religions. From ArabicMMLU, the subjects adopted into DarijaMMLU are: Islamic Studies, Driving Test, Natural Science, History, General Knowledge, Law, Physics, Social Science, Management, Arabic Language, Political Science, Philosophy, Accounting, Computer Science, Geography, Mathematics, Biology, Economics, Arabic Language (General), Arabic Language (Grammar), and Civics. C.5 LLM-as-a-Judge Prompt for Summarization Evaluation Following the work of Zheng et al. (2023) and Fabbri et al. (2021), which used advanced LLMs to evaluate responses from other LLMs, we employed Claude 3.5 Sonnet to assess the models summarization 20 You are an expert evaluator tasked with judging the cultural appropriateness and relevance of two answers written in Moroccan Darija for given instruction . Your judgment should focus solely on how well the answers reflect Moroccan cultural norms , values , and context . ### Criteria : 1. Cultural Appropriateness and Relevance : The answer should align well with Moroccan culture , norms , and societal context . Avoid any references , language , or ideas that are not relevant or appropriate for Morocco . 2. Fluency : The answer has to be in clear and precise language in Moroccan Darija . 3. Relevance : The answer should answer the instruction without any divergence from the instruction ' goal . ### Instructions : For each instruction , you will receive two answers , and B. Evaluate them based on the criterion above and decide which one better reflects Moroccan culture . Provide only the letter or as the answer . ### Output format : Better Answer : [A or B] ### Evaluate : ** Instruction **: [ Start of the instruction ] { instruction } [ Text of the instruction ] ** Answer **: [ Start of Answer A] { answer_a } [ Text of Answer A] ** Answer **: [ Start of Answer B] { answer_b } [ Text of Answer B] Your Response ( Only \"A\" or \"B\" with no additional text ): Figure 6: The prompt Given to Claude 3.5 Sonnet for choosing the answer that better follows the instruction and predefined DarijaAlpacaEval criteria between the baseline another LLMs generated answers. capabilities. Summarization is subjective, and traditional text overlap-based methods often struggle to provide accurate evaluations. As shown in Figure 7, we instructed Claude to evaluate model-generated summaries based on three main criteria: wordness, conciseness, and relevance. The objective of the Darija summarization task is to produce concise summary in native Darija using the fewest words possible, without introducing external information. At each evaluation step, two summaries were presented to Claude: one generated by an LLM and the corresponding ground truth summary. To mitigate biases such as verbosity and position bias, identified by Zheng et al. (2023), all models were instructed to generate summaries of no more than 30 words (the average length of title summaries). Additionally, each pair of generated and ground truth summaries was presented to Claude twice, with their positions swapped. Pairs in which position swapping influenced Claudes decision were discarded. The win-rate of models summary was calculated based on how often Claude preferred the models summary over the ground truth. 21 You are an expert evaluator tasked with judging the quality of two summaries written in Moroccan Darija for given passage , also in Moroccan Darija . You are strict regarding any language or dialect that is not Moroccan Darija , such as Modern Standard Arabic ( MSA ) and English . ### Criteria : Choose the better summary based on these criteria : 1. ** Wordness **: Clear and precise language in Moroccan Darija that conveys the passage ' original meaning and doesn ' use any other language or Dialect . 2. ** Conciseness **: Straight to the point , capturing essential information without unnecessary details . 3. ** Relevance **: Directly related to the passage without adding new information . ### Instructions : For each passage , you will receive two summaries , ** ** and ** **. Evaluate them based on the criteria above and decide which one is better . Provide only the letter ** ** or ** ** as the answer . It is strictly forbidden that summary is written in Modern Standard Arabic ( MSA ). summary should not be chosen if it is written in MSA . ### Output format : Better Summary : [A or B] ### Evaluate : ** Passage **: [ Start of the passage ] { passage } [ Text of the passage ] ** Summary **: [ Start of Summary A] { summary_a } [ Text of Summary A] ** Summary **: [ Start of Summary B] { summary_b } [ Text of Summary B] Your Response ( Only or with no additional text ) : Figure 7: The prompt Given to Claude 3.5 Sonnet for choosing the best summary between the baseline and LLM-generated summaries. Examples of Atlas-Chat-9B Responses Figure 8 and 9 present some samples of Atlas-Chat responses on variety of questions. 22 Figure 8: Atlas-Chat-9B response example 1. Figure 9: Atlas-Chat-9B response example 2 (The model can understand English instructions but only responds in Darija)."
        }
    ],
    "affiliations": [
        "AtlasIA",
        "EMINES-UM6P",
        "Ecole Polytechnique",
        "KTH",
        "LINAGORA",
        "MBZUAI"
    ]
}