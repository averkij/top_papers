{
    "paper_title": "CrossWordBench: Evaluating the Reasoning Capabilities of LLMs and LVLMs with Controllable Puzzle Generation",
    "authors": [
        "Jixuan Leng",
        "Chengsong Huang",
        "Langlin Huang",
        "Bill Yuchen Lin",
        "William W. Cohen",
        "Haohan Wang",
        "Jiaxin Huang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Existing reasoning evaluation frameworks for Large Language Models (LLMs) and Large Vision-Language Models (LVLMs) predominantly either assess text-based reasoning or vision-language understanding capabilities, with limited dynamic interplay between textual and visual constraints. To address this limitation, we introduce CrossWordBench, a benchmark designed to evaluate the reasoning capabilities of both LLMs and LVLMs through the medium of crossword puzzles-a task requiring multimodal adherence to semantic constraints from text-based clues and intersectional constraints from visual grid structures. CrossWordBench leverages a controllable puzzle generation framework that produces puzzles in multiple formats (text and image) and offers different evaluation strategies ranging from direct puzzle solving to interactive modes. Our extensive evaluation of over 20 models reveals that reasoning LLMs outperform non-reasoning models substantially by effectively leveraging crossing-letter constraints. We further demonstrate that LVLMs struggle with the task, showing a strong correlation between their puzzle-solving performance and grid-parsing accuracy. Our findings offer insights into the limitations of the reasoning capabilities of current LLMs and LVLMs, and provide an effective approach for creating multimodal constrained tasks for future evaluations."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 3 ] . [ 1 3 4 0 0 0 . 4 0 5 2 : r Preprint. Under review. CrossWordBench: Evaluating the Reasoning Capabilities of LLMs and LVLMs with Controllable Puzzle Generation Jixuan Leng1, Chengsong Huang2, Langlin Huang2, Bill Yuchen Lin3, William W. Cohen1, Haohan Wang4, Jiaxin Huang2 1CMU, 2WUSTL, 3UW, 4UIUC jixuanl@cs.cmu.edu, jiaxinh@wustl.edu Code: https://github.com/SeanLeng1/CrossWordBench Dataset: https://huggingface.co/datasets/HINT-lab/CrossWordBench"
        },
        {
            "title": "Abstract",
            "content": "Existing reasoning evaluation frameworks for Large Language Models (LLMs) and Large Vision-Language Models (LVLMs) predominantly either assess text-based reasoning or vision-language understanding capabilities, with limited dynamic interplay between textual and visual constraints. To address this limitation, we introduce CrossWordBench, benchmark designed to evaluate the reasoning capabilities of both LLMs and LVLMs through the medium of crossword puzzlesa task requiring multimodal adherence to semantic constraints from text-based clues and intersectional constraints from visual grid structures. CrossWordBench leverages controllable puzzle generation framework that produces puzzles in multiple formats (text and image) and offers different evaluation strategies ranging from direct puzzle solving to interactive modes. Our extensive evaluation of over 20 models reveals that reasoning LLMs outperform non-reasoning models substantially by effectively leveraging crossing-letter constraints. We further demonstrate that LVLMs struggle with the task, showing strong correlation between their puzzle-solving performance and gridparsing accuracy. Our findings offer insights into the limitations of the reasoning capabilities of current LLMs and LVLMs, and provide an effective approach for creating multimodal constrained tasks for future evaluations."
        },
        {
            "title": "Introduction",
            "content": "Figure 1: Performance gap between top-performing reasoning LLMs and non-reasoning LLMs/LVLMs on CrossWordBench. Reasoning LLMs achieve better overall performance vs. non-reasoning models, and follow crossing letter constraints significantly better. Large reasoning models (e.g., OpenAI-o1 (Jaech et al., 2024) and DeepSeek-R1 (Guo et al., 2025)) have made exceptional progress on reasoning benchmarks including math problem solving (Veeraboina, 2023), coding (Austin et al., 2021) and commonsense reasoning (Clark et al., 2018). However, existing evaluation benchmarks for Large Language Models (LLMs) and Large Vision-Language Models (LVLMs) (e.g., Visual Question Answering (Yue et al., 2023)) mostly focus on text-based reasoning or vision-language understanding, lacking the dynamic interplay between textual and visual constraints that characterizes real-world problem solving. Consequently, evaluating multimodal reasoning capabilityparticularly for reasoning tasks requiring both textual and visual constraintsremains challenging. 1 Preprint. Under review. Crossword puzzles, classic grid-based task in which horizontal (Across) words and vertical (Down) words must be filled in based on text-based clues, provides unique testbed for such evaluations. They pose two distinct challenges: (1) question answering for each text-based clue, which may admit multiple correct solutions, and (2) visual constraint satisfaction, which requires precise letter alignment at the intersections of Across and Down entries. Prior crossword datasets (Efrat et al., 2021; Rozner et al., 2021; Kulshreshtha et al., 2022; Chen et al., 2025) often suffer from their reliance on static and copyrighted online news sources and adopt text-centric formulation, thereby neglecting the visual structure. In this paper, we introduce CrossWordBench, controllable and scalable benchmark for evaluating the reasoning capabilities of both LLMs and LVLMs. CrossWordBench collects data and generates puzzles from three sources: (1) multilingual word-clue pairs from public repositories, (2) dictionary-based definitions, and (3) adapted questions-answer pairs from existing benchmarks (e.g., CommonsenseQA (Talmor et al., 2018)) where the answers are open-ended or unconstrained. By representing grids and clues in different formats, as shown in Figure 2, CrossWordBench facilitates the evaluation of both model types. It supports two evaluation modes: direct puzzle-solving mode, which generates one-round responses using zero-shot Chain-of-Thought (CoT) prompts, and an interactive mode for step-by-step puzzle-solving, where grid generator is called to provide intermediate visual outputs for follow-ups, thereby serving as foundation for evaluating agents using function calling. We evaluate over 20 state-of-the-art models, including both proprietary models (e.g., GPT4o (Hurst et al., 2024) and Claude 3.7 Sonnet (Anthropic, 2024)) and open-weight models (e.g., DeepSeek-R1 (Guo et al., 2025) and Pixtral-Large-Instruct (Agrawal et al., 2024)). Our evaluation yields several notable findings: (1) LVLMs perform significantly worse than LLMs on CrossWordBench (as shown in Figure. 1), and they struggle in OCR in vertical (Down) word extraction. In fact, their puzzle-solving performance strongly correlates with their grid-parsing accuracy (r = 0.94). (2) Reasoning LLMs outperform non-reasoning models, and benefit from test-time scaling and increased crossing-letter constraints. (3) Even puzzles derived from saturated benchmarks (e.g., CommonsenseQA) remain challenging, highlighting the significance of structural constraints in reasoning evaluation."
        },
        {
            "title": "2 Related Work",
            "content": "LLMs and LVLMs Reasoning. Recent research on the reasoning capabilities of LLMs (Ahn et al., 2024; Huang & Chang, 2022; Kojima et al., 2022; Plaat et al., 2024; Jaech et al., 2024; Guo et al., 2025; OpenAI, 2025; Huang et al., 2022) has led to the development of various approaches, including prompting-based methods (Wei et al., 2022; Yao et al., 2023; Besta et al., 2024; Chen et al., 2022) that guide LLMs to generate intermediate steps for solving complex problems, fine-tuning methods that train LLMs on long reasoning chains (Ye et al., 2025; Muennighoff et al., 2025; Zhao et al., 2025), and test-time scaling via self-refinement or with verifier (Setlur et al., 2024; Feng et al., 2023; Wang et al., 2023; Zhang et al., 2024a; Uesato et al., 2022; Huang et al., 2025a). Moreover, recent study from Deepseek-R1 (Guo et al., 2025) demonstrates that reinforcement learning (RL) with verifiable rewards facilitates the emergence of complex thinking processes in LLMs. Several studies have explored reasoning in the multimodal domain by constructing CoT data for fine-tuning (Xu et al., 2024) and developing verifiable problems for RL (Yang et al., 2025; Huang et al., 2025b). These approaches have demonstrated success, yet the evaluation datasets remain largely restricted to mathematical problems (Wang et al., 2024; Lu et al., 2023; Zhang et al., 2024b). Crossword Puzzles in Language Model Evaluation. Crossword puzzles have long been focus of research in natural language processing (NLP), particularly before the advent of LLMs. Early approaches typically employed constraint satisfaction algorithms augmented by external knowledge bases. Notable examples include systems such as Proverb (Littman et al., 2002), Dr. Fill (Ginsberg, 2011), and specialized models such as the Berkeley Crossword Solver (Wallace et al., 2022), which incorporate fine-tuned BERT (Devlin et al., 2019) and belief propagation. More recent studies have leveraged LLMs to address crossword puzzles through techniques including fine-tuning (Efrat et al., 2021; Rozner et al., 2021; Sadallah et al., 2024), prompting strategies such as Tree-of-Thoughts (Yao et al., 2023), or integration with search algorithms (Saha et al., 2024), demonstrating the potential of LLMs for crosswords. 2 Preprint. Under review. Several datasets for crossword puzzles have been proposed, covering both English (Efrat et al., 2021; Rozner et al., 2021; Kulshreshtha et al., 2022; Chen et al., 2025) and Arabic (Zeinalipour et al., 2025). However, one significant limitation of these datasets and approaches is that they rely on data from online news sources (Efrat et al., 2021; Rozner et al., 2021; Kulshreshtha et al., 2022; Chen et al., 2025) and often formulate crossword solving as question-answering (QA) task (Sadallah et al., 2024; Efrat et al., 2021; Rozner et al., 2021; Yao et al., 2023), thereby overlooking the fundamental constraint-based nature of the problem. Moreover, all of them treat crossword solving as text-based task, despite the inherently visual nature of crossword grids, leaving gap in extending crossword puzzles for evaluating LVLMs. In this work, we address these limitations by introducing CrossWordBench, framework that features controllable puzzle generation and extends evaluation to LVLMs."
        },
        {
            "title": "3 Benchmark Curation",
            "content": "Figure 2: Framework of CrossWordBench. (a) Dataset curation process and input templates for LLMs and LVLMs; (b) Zero-shot CoT evaluation; (c) Interactive Mode Evaluation. Online sources, including The New York Times and The Los Angeles Times, provide complete crossword puzzles. Nevertheless, directly utilizing these puzzles presents several disadvantages. (1) Copyright restrictions may limit their usage, and their online availability increases the probability that they have been incorporated into the pretraining datasets of contemporary models. (2) Online puzzles are typically staticwith predefined words, themes, grid sizes, and strict formatting (e.g., black square symmetry)1which not only limits their adaptability for diverse benchmark generation, but also imposes arbitrary formatting constraints that do not yield meaningful benefits for evaluation. Prior research (Mirzadeh et al., 2024) has shown that even minor modifications to questions can significantly affect model performance, particularly when performances on the original version is near saturation. (3) The static characteristics of these puzzles restricts the range of potential evaluation strategies. To overcome these limitations, we propose two-stage benchmark construction strategy: Word-Clue Pairs Curation. We compile word-clue pairs from three source categories: (1) Public repositories: We selectively extract individual, often cryptic, word-clue pairs from public online repositories with samples in both English2 and Chinese3. (2) Dictionary-Based 1The New York Times Crossword Requirements. 2A Dataset of Cryptic Crossword Clues 3Chinese Crosswords 3 Preprint. Under review. Pairs: We collect standard English words and use their dictionary definitions from NLTK WordNet4 as clues, referring to this category as English Simple. (3) Adapted Benchmark Data: We demonstrate that existing LLM benchmarks, which are typically designed for open-ended or multiple-choice questions without strict formatting constraints, can be transformed into word-clue pairs. In our study, we filter the CommonsenseQA (Talmor et al., 2018) training set for single-word answers and use the associated questions as clues. While the possibility of data contamination may still exist for these word-clue pairs, the variations in grid design can yield distinct crossword puzzles, and as demonstrated in Section 4.5, even when incorporating extensively tuned data such as CommonsenseQA training set, the resulting puzzles remain remarkably challenging for both LLMs and LVLMs. Stats. Total # of puzzles Total # of words Unique words (% total) Unique clues (% total) English Chinese English Simple CommonsenseQA 77 1414 77 100 100 100 1,193 1,327 3,472 83.82% 80.76% 92.92% 100% 100% 100% 7 100 1,139 36.70% 100% 77 50 543 59.85% 100% Aggregated (English 7x7 & 14x14): 200 puzzles, 74.02% unique words, 100% unique clues Words per Puzzle Minimum Maximum Mean Word Length (Letters) Minimum Maximum Mean 11 16 11.93 2 5 3.59 22 44 34.72 3 12 4.31 11 18 13.27 2 5 3. 11 13 11.39 3 5 3.63 9 13 10.86 3 5 3.77 Avg blocked cells (%) 39.51% 45.22% 43.37% 39.12% 38.37% Table 1: Crossword puzzle statistics for different subjects and grid sizes. Statistics are presented separately for each category, as distinct word and clue pairs are used for their construction. Additionally, aggregated statistics for all English puzzles set are included. Puzzle Generation. After collecting word-clue pairs, we implement an automatic pipeline to generate puzzles in both text and image formats. The pipeline enables (1) puzzle difficulty control by adjusting grid sizes and (2) incremental word placement function, which facilitates an interactive evaluation mode described in Section 4.4. Figure 2 illustrates the overall benchmark pipeline. To maintain category integrity, we avoid overlapping clues under each category. Table 1 presents the overall dataset statistics for each category."
        },
        {
            "title": "4 Experiment",
            "content": "In the following sections, we present an extensive empirical evaluation of CrossWordBench across multiple model architectures and examine their performance characteristics. 4.1 Experimental Setup Evaluation Data and Metrics. For main experiments, we evaluate models on the English set with three metrics to assess answer accuracy and adherence to crossword constraints. Word Coverage Rate (WCR): word-level accuracy, percentage of correctly solved words. Letter Coverage Rate (LCR): letter-level accuracy, percentage of correct letter placements. Intersection Consistency Rate (ICR): the internal consistency of the models answers at intersections where across and down words overlap, defined as: ICR = 1 (a,d,j,k)I 1{a[j] = d[k]} (1) 4NLTK Wordnet Preprint. Under review. where denotes the set of all intersections, where each tuple (a, d, j, k) indicates the jth letter of the across word overlaps with the kth letter of the down word d. This metric reflects whether models correctly adhere to the grid structural constraints of puzzle. Evaluated Models. We evaluate collection of proprietary and open-weight models, including both LVLMs and LLMs. For proprietary models, we consider state-of-the-art models such as GPT-4o (Hurst et al., 2024) and Claude-3-7-Sonnet (Anthropic, 2024) (with and without thinking mode). For open-weight models, our selections ranging from 3B to 124B parameters, such as the Qwen Series (Team, 2024; Bai et al., 2025) and Pixtral-LargeInstruct-2411 (Agrawal et al., 2024) for LVLMs. For LLMs, we evaluate both reasoning models such as the Deepseek Series (Liu et al., 2024; Guo et al., 2025) and non-reasoning models such as Llama series (Dubey et al., 2024). The full list of models is shown in Table 2. We set temperature to 0 for non-reasoning models for consistency, 0.6 for reasoning models based on commonly recommended settings, and 1.0 to certain proprietary models (e.g., used by Claude-3-7-Sonnet Thinking). Further generation details are in Appendix B.3. Input Prompt Templates and Output Response Parsing. For the main evaluation, we In adopt the zero-shot Chain-of-Thought (CoT) (Wei et al., 2022) prompting strategy. LVLM evaluation, both the clues and the grid are embedded within an image. In LLM evaluation, the grid is represented as 2D binary array, with 1 indicating blocked cell and 0 representing an unfilled cell, and is preprended to text clues in the prompt. Detailed prompt templates are in Appendix B.4. To extract answers from responses, we leverage the structured output capabilities of o3-mini5 to convert raw model responses into JSON format, by generating dynamic Pydantic models. Implementation details are in Appendix B.2. 4.2 Main Results Reasoning LLMs substantially outperform conventional ones across metrics, with notable improvements in ICR, as shown in Table 2. In particular, among reasoning LLMs, o3mini achieves an ICR of 0.891 on 7x7 grid, demonstrating strong ability to interpret and enforce grid constraints. Although DeepSeek-R1 attains the highest WCR and LCR on 7x7 grid, its ICR is slightly lower than that of o3-mini. Other reasoning models, such as QwQ-32B and R1-Distilled-Llama-70B, show moderate performance, with WCRs of approximately 0.347 and 0.387, respectively. One notable outlier is Open-Reasoner-Zero-32B, which performs poorly across all metrics, indicating that it does not effectively leverage grid constraints for reasoning. This suggests that its trainingprimarily focused on mathematical reasoningdoes not generalize well to tasks requiring spatial and linguistic integration, thereby highlighting key limitation in the training strategies of these reasoning models. Non-reasoning LLMs exhibit limitations, especially in ICR, and larger grids present increased challenges. Among non-reasoning LLMs, Claude-3-7-Sonnet and Gemini 2.0 Pro Exp yield the best results, with WCRs of 0.482 and 0.460 on the 7x7 grid, respectively; however, their relatively lower ICR indicates model limitations on explicit reasoning constraints for crossword puzzles. Notably, thinking mode improves Claude-3-7-Sonnet on all three metrics, highlighting the importance of reasoning and reflection in solving constraints in crossword tasks. Additionally, larger grids lead to decreasing performance, demonstrating the increased complexity of maintaining constraint adherence over larger search space. LVLMs currently lag behind LLMs in performance, with minimal adherence to grid constraints. With image inputs, Claude-3-7-Sonnet achieves the highest performance among LVLMs, but underperforms its own text inputs version. Models such as GPT-4o and Gemini 2.0 Pro Exp exhibit similar trends, with WCRs below 0.35 on larger grids. All LVLMs demonstrate low ICRs, suggesting that they struggle to maintain reasoning consistency. Notably, performance declines when thinking mode is enabled on Claude-3-7-Sonnet with image input, which contradicts the improvements observed with text inputs; we explore this phenomenon in the next section. Among open-weight LVLMs, Pixtral-Large-Instruct achieves the best WCR of 0.297 on 7x7 grid, while still lags behind most proprietary LVLMs. 5https://openai.com/index/openai-o3-mini/ 5 Preprint. Under review. Models 7x7 LCR WCR ICR WCR 14x LCR ICR Proprietary LVLMs Claude-3-7-Sonnet Claude-3-7-Sonnet (cid:17) GPT-4o-2024-11-20 Gemini 2.0 Pro Exp Gemini 2.0 Flash 0.4790.014 0.5280.013 0.3660.016 0.4160.009 0.4490.009 0.2720.010 0.3650.017 0.4480.014 0.3300.015 0.3820.009 0.4280.007 0.2280.008 0.3480.015 0.4030.015 0.2340.017 0.3500.009 0.3930.008 0.1900.008 0.3510.014 0.3680.014 0.3390.015 0.2730.008 0.3030.007 0.2210.007 0.2770.015 0.3000.013 0.2250.013 0.2600.008 0.2840.008 0.1900.007 Open-Weight LVLMs Pixtral-Large-Instruct-2411 0.2970.015 0.3380.014 0.1980.014 0.2510.009 0.2840.007 0.1340.007 InternVL2 5-78B-MPO 0.1210.011 0.1640.009 0.0990.011 0.1190.007 0.1590.006 0.0730.005 NVLM-D-72B 0.1340.010 0.1790.009 0.0760.008 0.0850.006 0.1200.007 0.0530.004 Qwen2.5-VL-72B-Instruct 0.2070.013 0.2450.011 0.1330.011 0.1940.007 0.2270.006 0.1100.006 QVQ-72B-Preview 0.1970.012 0.2180.010 0.0910.008 0.1950.007 0.2150.007 0.1080.006 llava-onevision-72b-ov-chat 0.1410.012 0.1650.010 0.0970.009 0.1120.008 0.1410.007 0.0750.005 gemma-3-27b-it 0.1580.011 0.2180.011 0.1240.010 0.1060.009 0.1600.009 0.0750.005 Aria 0.0610.009 0.1010.007 0.0510.006 0.0350.006 0.0700.006 0.0460.004 MiniCPM-V-2 6 0.0430.007 0.0850.006 0.0640.008 0.0230.004 0.0570.004 0.0400.003 Qwen2.5-VL-3B-Instruct 0.0130.003 0.0400.004 0.0380.006 0.0140.002 0.0340.003 0.0230.003 Proprietary LLMs o3-mini-high (cid:17) Claude-3-7-Sonnet (cid:17) Claude-3-7-Sonnet GPT-4o-2024-11-20 Gemini 2.0 Pro Exp Gemini 2.0 Flash 0.5870.023 0.6840.021 0.8910.018 0.4450.011 0.5200.011 0.5120.007 0.6170.019 0.7120.017 0.7540.021 0.4920.013 0.5420.012 0.4310.014 0.4820.015 0.5740.014 0.4720.019 0.4460.011 0.4850.011 0.3210.011 0.4100.018 0.4720.018 0.2880.019 0.3380.011 0.3690.012 0.1960.010 0.4600.014 0.5250.012 0.3880.016 0.4250.009 0.4570.008 0.2890.010 0.3010.014 0.3180.012 0.2550.014 0.2800.007 0.2980.006 0.1980.006 Open-Weight LLMs Llama-3.1-405B-Instruct 0.1610.013 0.3590.012 0.2430.017 0.3550.013 0.3900.009 0.2220.008 DeepSeek-R1 (cid:17) 0.6460.019 0.7070.017 0.6780.023 0.4720.011 0.5070.011 0.3560.011 DeepSeek-V3 0.3030.014 0.3690.014 0.1860.013 0.2900.009 0.3350.008 0.1450.007 R1-Distill-Llama-70B (cid:17) 0.3870.015 0.4480.015 0.3470.017 0.2850.009 0.3190.009 0.1610.008 Llama-3.3-70B-Instruct 0.3030.013 0.3710.012 0.2060.014 0.2800.011 0.3400.009 0.1730.008 QwQ-32B (cid:17) 0.3470.017 0.4450.018 0.5180.020 0.2540.009 0.3070.009 0.1890.009 Open-Reasoner-Zero-32B (cid:17) 0.1390.010 0.2040.010 0.1840.012 0.1460.007 0.1990.007 0.0950.005 Phi-4 0.1220.010 0.1940.010 0.1130.011 0.1400.007 0.2000.006 0.0850. Table 2: Comparison of various LLMs and LVLMs on CrossWordBench English set across two difficulty levels using zero-shot CoT. We report the mean and standard error over 100 samples for both 7x7 and 14x14 grids. (cid:17) indicates that the model is reasoning model. : We use the Fireworks API for DeepSeek V3 and Llama-3.1-405B, while offical API for R1. 6 Preprint. Under review. 4.3 Positive Correlation: LVLMs Grid Parsing & Puzzle-Solving Performance Model performance on crossword puzzles exhibits strong dependency on grid parsing capabilities, with systematic biases in word orientations. We evaluate grid parsing ability by prompting models to parse completed puzzle grids and associated clues (prompt details in Appendix B.4). Successful grid parsing involves: (1) identifying grid indexing numbers, (2) mapping numbers to clues to determine word orientation, and (3) extracting words with boundary recognition. The grid parsing ability reflects an LVLMs ability to interpret both spatial and textual information. Figure 3: Grid Parsing vs. Puzzle-Solving on 77 English puzzles, measured with WCR. Down Across Models Proprietary VLMs We reveal two significant patterns: (1) As shown in Table 3, LVLMs extract Across words more accurately than Down words, revealing OCR limitations. (2) Fig. 3 demonstrates strong positive correlation between grid parsing and overall puzzlesolving performance (both measured by WCR). Notably, when enabling thinking mode, Claude-3-7-Sonnet exhibits lower grid parsing performance, which aligns with its lower puzzle-solving performance. We hypothesize that the additional token generation in thinking mode diverts attention from image processing. These findings underscore the importance of precise spatialtextual interpretation for future LVLM design. For full results, please refer to Appendix A.2. Claude-3-7-Sonnet 0.9540.009 Claude-3-7-Sonnet (cid:17) 0.9490.010 GPT-4o-2024-11-20 0.8860. Pixtral-Large-Instruct 0.7530.022 QVQ-72B-Preview 0.7170.022 Table 3: WCR on Grid Parsing. 0.7600.018 0.6540.022 0.4480.024 0.3610.022 0.1390.019 Open-Weight VLMs 4.4 Agentic Evaluation Setting: Interactive Mode for VLMs Motivated by recent research on visual-of-thoughts (Wu et al., 2024; Li et al., 2025), we introduce new evaluation setting for LVLMs, referred to as Interactive Mode. This setting leverages the ability of CrossWordBench to automatically generate updated grid images. Interactive Mode requires step-by-step puzzle solving instead of solving the entire puzzle in one pass. Specifically, the implementation of CrossWordBench allows for updating grid images each time model provides an answer. We maintain four-round conversation history due to context limitations. We introduce Interactive Success Step (ISS) to quantify the number of correctly solved words before the models first error. Figure 4 shows the cumulative distribution of ISS for each modelmost models fail at the initial solution step on most puzzles. Figure 4: CDF of ISS on 7x7 English Puzzles. Unlike the visual-of-thoughts approach, which focuses on generating intermediate reasoning plots, we employ external functions to update the grid. This enables evaluating LVLMs in agentic settings, with error feedback to provide cues for the model to refine its responses. 4.5 Beyond English: Evaluations on Multilingual, Dictionary-based, and Adapted Data We extend zero-shot CoT prompting evaluation to Chinese, dictionary-based, and benchmark-adapted wordclue pairs in CrossWordBench, and show results in Table 4. Preprint. Under review. Models Chinese English Simple CommonSenseQA WCR ICR WCR ICR WCR ICR Proprietary LVLMs GPT-4o-2024-11-20 Gemini 2.0 Flash 0.3660.018 0.1700.017 0.3350.015 0.2270.017 0.3920.026 0.2470.026 0.2080.031 0.2330.018 0.2290.014 0.2160.013 0.3270.021 0.2160.018 Pixtral-Large-Instruct-2411 Qwen2.5-VL-72B-Instruct 0.2520.015 0.1010.011 0.2160.016 0.1870.015 0.4390.023 0.2700.023 0.3910.025 0.2820.018 0.2390.016 0.1830.015 0.4180.022 0.2520.023 Open-Weight LVLMs Proprietary LLMs GPT-4o-2024-11-20 o3-mini-high (cid:17) 0.5930.019 0.4480.021 0.4380.020 0.3060.022 0.5240.024 0.3260.029 0.7740.016 0.9530.014 0.7820.021 0.9460.012 0.8120.027 0.9710.011 Open-Weight LLMs DeepSeek-R1 (cid:17) QwQ-32B (cid:17) 0.9070.016 0.8980.018 0.7590.017 0.7870.020 0.7520.031 0.8290.026 0.7010.020 0.6540.022 0.6470.021 0.7340.020 0.6990.026 0.7660.027 Table 4: WCR and ICR for Chinese, English Simple, and CommonSenseQA puzzle sets. Reasoning models outperform conventional ones across metrics, and the performance gap between open-weight and proprietary models is reduced. o3-mini maintains high ICR across all three settings, consistent with main result observations. Furthermore, the performance gap between open-weight and proprietary reasoning models decreases, possibly due to the simpler nature of these tasks and differences in training data. Interestingly, when solving Chinese puzzles, we observe that QwQ-32B consistently reasons in Chinese. Effectiveness of grid construction: puzzles constructed with CommonsenseQA training data remain challenging. Despite the CommonsenseQA having saturated performance on most LLMs, its training set continue to pose significant challenges for reasoning models. For example, o3-mini achieves the highest WCR of 0.812, yet it still falls short of perfection. In contrast, best evaluated non-reasoning LLMs such as GPT-4o and open-weight LVLMs Pixtral-Large-Instruct obtain WCR of 0.524 and 0.439, respectively. These results highlight the effectiveness of this grid construction approach in leveraging existing benchmark data."
        },
        {
            "title": "5 Analysis",
            "content": "In this section, we provide an analysis of model behavior on CrossWordBench through the impact of structural constraints and reasoning mechanisms, and discuss future applications. 5.1 Crossing Letter Count: Impact on Puzzle Performance Average word accuracy on reasoning models increases with crossing letter count. As shown in Figure 5, the average WCR for each range of crossing letter counts, divided into three groups, across five reasoning LLMs and nine non-reasoning LLMs. We observe that reasoning models achieve higher accuracy as the number of letter intersections increases, whereas this trend is not observed for non-reasoning LLMs. Figure 5: Crossing letter counts and average WCR on 7x7 English puzzles across LLMs. This pattern aligns with our main obPreprint. Under review. servations, where reasoning models tend to exhibit higher ICRsindicating that they benefit from effectively utilizing grid constraints for solution space reduction. 5.2 Self-Reflection: Minimal Effect on Puzzle Performance Backtracking and verifying previous answers are essential for solving crossword puzzles. To evaluate the effect of selfreflection, we include manually crafted follow-up query that asks models to revisit their previous zero-shot CoT responses (see Appendix B.4 for details). As shown in Figure 6, there is no observable performance improvement for either reasoning LLMs or non-reasoning LVLMs. This suggests that additional interactions with manual prompting alone are insufficient to enhance reasoning capabilities for puzzle solving. Figure 6: Self-reflection improvements on 7x7 English puzzles. Top row: reasoning LLMs. Bottom row: non-reasoning LVLMs. 5.3 Test-Time Scaling: Diminishing Returns on Puzzle Performance To investigate the impact of test-time scaling on CrossWordBench, we use o3-mini on English 7x7 puzzles and control its reasoning effort, which in turn determines the number of reasoning tokens generated during inference. As shown in Figure 7, medium effort yields significant performance improvement compared to low effort across all three metrics, while further doubling the reasoning tokens does not result in significant additional performance gain, indicating diminishing marginal returns. Figure 7: o3-mini performance on 7x7 English puzzles across three distinct reasoning efforts. 5.4 Discussion: Verifiable Crossword for Future Multimodal RL Training Table 2 shows that reasoning models such as Open-Reasoner-Zero-32B, which are primarily trained on mathematical problems, demonstrate limited generalization to CrossWordBench. This highlights the inadequacy of relying predominantly on math-based problems with verifiable answers for reinforcement learning. As multimodal reasoning evolves, significant challenge remains: the lack of multimodal datasets with verifiable rewards for training. We contend that crossword puzzles, owing to their inherent verifiability and multimodal nature, offer promising alternative, and our framework can serve as foundation to address this gap. We leave exploration of multimodal RL training on crossword puzzles for future work."
        },
        {
            "title": "6 Conclusion",
            "content": "This paper introduces CrossWordBench, benchmark designed to evaluate the multimodal reasoning capabilities of both LLMs and LVLMs using crossword puzzles, which uniquely integrate text-based clues and visual constraints. Our extensive evaluation of over 20 models shows that reasoning models substantially outperform non-reasoning counterparts and can benefit from increased crossing-letter constraints. Additionally, we find strong correlation between puzzle-solving performance and grid-parsing accuracy in LVLMs. Even puzzles derived from saturated benchmarks remain challenging, emphasizing the necessity of structural complexity in rigorous reasoning evaluation. This work paves the way for improving future multimodal RL training where interplay between modalities is essential. 9 Preprint. Under review."
        },
        {
            "title": "References",
            "content": "Marah Abdin, Jyoti Aneja, Harkirat Behl, Sebastien Bubeck, Ronen Eldan, Suriya Gunasekar, Michael Harrison, Russell Hewett, Mojan Javaheripi, Piero Kauffmann, et al. Phi-4 technical report. arXiv preprint arXiv:2412.08905, 2024. Pravesh Agrawal, Szymon Antoniak, Emma Bou Hanna, Baptiste Bout, Devendra Chaplot, Jessica Chudnovsky, Diogo Costa, Baudouin De Monicault, Saurabh Garg, Theophile Gervet, et al. Pixtral 12b. arXiv preprint arXiv:2410.07073, 2024. Janice Ahn, Rishu Verma, Renze Lou, Di Liu, Rui Zhang, and Wenpeng Yin. Large language models for mathematical reasoning: Progresses and challenges. arXiv preprint arXiv:2402.00157, 2024. Anthropic. Claude 3.5 sonnet. https://www.anthropic.com/news/claude-3-5-sonnet, 2024. Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie J. Cai, Michael Terry, Quoc V. Le, and Charles Sutton. Program synthesis with large language models. ArXiv, abs/2108.07732, 2021. URL https://api. semanticscholar.org/CorpusID:237142385. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Hubert Niewiadomski, Piotr Nyczyk, et al. Graph of thoughts: Solving elaborate problems with large language models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pp. 1768217690, 2024. Jianghao Chen, Zhenlin Wei, Zhenjiang Ren, Ziyong Li, and Jiajun Zhang. Lr2bench: Evaluating long-chain reflective reasoning capabilities of large language models via constraint satisfaction problems, 2025. URL https://arxiv.org/abs/2502.17848. Wenhu Chen, Xueguang Ma, Xinyi Wang, and William Cohen. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. arXiv preprint arXiv:2211.12588, 2022. Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2418524198, 2024. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. ArXiv preprint, abs/1803.05457, 2018. Wenliang Dai, Nayeon Lee, Boxin Wang, Zhuolin Yang, Zihan Liu, Jon Barker, Tuomas Rintamaki, Mohammad Shoeybi, Bryan Catanzaro, and Wei Ping. Nvlm: Open frontierclass multimodal llms. arXiv preprint, 2024. Google DeepMind. Gemini 2.0. https://deepmind.google/technologies/gemini, 2024. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies, volume 1 (long and short papers), pp. 41714186, 2019. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. 10 Preprint. Under review. Avia Efrat, Uri Shaham, Dan Kilman, and Omer Levy. Cryptonite: cryptic crossword benchmark for extreme ambiguity in language. arXiv preprint arXiv:2103.01242, 2021. Xidong Feng, Ziyu Wan, Muning Wen, Stephen Marcus McAleer, Ying Wen, Weinan Zhang, and Jun Wang. Alphazero-like tree-search can guide large language model decoding and training. arXiv preprint arXiv:2309.17179, 2023. Matthew Ginsberg. Dr. fill: Crosswords and an implemented solver for singly weighted csps. Journal of Artificial Intelligence Research, 42:851886, 2011. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Chengsong Huang, Langlin Huang, Jixuan Leng, Jiacheng Liu, and Jiaxin Huang. Efficient test-time scaling via self-calibration. arXiv preprint arXiv:2503.00031, 2025a. Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and Jiawei Han. Large language models can self-improve. arXiv preprint arXiv:2210.11610, 2022. Jie Huang and Kevin Chen-Chuan Chang. Towards reasoning in large language models: survey. arXiv preprint arXiv:2212.10403, 2022. Wenxuan Huang, Bohan Jia, Zijie Zhai, Shaosheng Cao, Zheyu Ye, Fei Zhao, Yao Hu, and Shaohui Lin. Vision-r1: Incentivizing reasoning capability in multimodal large language models. arXiv preprint arXiv:2503.06749, 2025b. Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35:2219922213, 2022. Saurabh Kulshreshtha, Olga Kovaleva, Namrata Shivagunde, and Anna Rumshisky. Down and across: Introducing crossword-solving as new nlp benchmark. arXiv preprint arXiv:2205.10442, 2022. Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024a. Chengzu Li, Wenshan Wu, Huanyu Zhang, Yan Xia, Shaoguang Mao, Li Dong, Ivan Vulic, and Furu Wei. Imagine while reasoning in space: Multimodal visualization-of-thought. arXiv preprint arXiv:2501.07542, 2025. Dongxu Li, Yudong Liu, Haoning Wu, Yue Wang, Zhiqi Shen, Bowen Qu, Xinyao Niu, Guoyin Wang, Bei Chen, and Junnan Li. Aria: An open multimodal native mixture-ofexperts model. arXiv preprint arXiv:2410.05993, 2024b. Michael Littman, Greg Keim, and Noam Shazeer. probabilistic approach to solving crossword puzzles. Artificial Intelligence, 134(1-2):2355, 2002. Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. 11 Preprint. Under review. Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255, 2023. Iman Mirzadeh, Keivan Alizadeh, Hooman Shahrokhi, Oncel Tuzel, Samy Bengio, and Mehrdad Farajtabar. Gsm-symbolic: Understanding the limitations of mathematical reasoning in large language models. arXiv preprint arXiv:2410.05229, 2024. Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Cand`es, and Tatsunori Hashimoto. s1: Simple test-time scaling. arXiv preprint arXiv:2501.19393, 2025. OpenAI. Openai o3-mini. https://openai.com/index/openai-o3-mini, 2025. Aske Plaat, Annie Wong, Suzan Verberne, Joost Broekens, Niki van Stein, and Thomas Back. Reasoning with large language models, survey. arXiv preprint arXiv:2407.11511, 2024. Josh Rozner, Christopher Potts, and Kyle Mahowald. Decrypting cryptic crosswords: Semantically complex wordplay puzzles as target for nlp. Advances in Neural Information Processing Systems, 34:1140911421, 2021. Abdelrahman Sadallah, Daria Kotova, and Ekaterina Kochmar. Are llms good cryptic crossword solvers? arXiv preprint arXiv:2403.12094, 2024. Soumadeep Saha, Sutanoya Chakraborty, Saptarshi Saha, and Utpal Garain. Language models are crossword solvers. arXiv preprint arXiv:2406.09043, 2024. Amrith Setlur, Chirag Nagpal, Adam Fisch, Xinyang Geng, Jacob Eisenstein, Rishabh Agarwal, Alekh Agarwal, Jonathan Berant, and Aviral Kumar. Rewarding progress: Scaling automated process verifiers for llm reasoning. arXiv preprint arXiv:2410.08146, 2024. Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. Commonsenseqa: question answering challenge targeting commonsense knowledge. arXiv preprint arXiv:1811.00937, 2018. Gemma Team, Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova, Alexandre Rame, Morgane Rivi`ere, et al. Gemma 3 technical report. arXiv preprint arXiv:2503.19786, 2025. Qwen Team. Qvq: To see the world with wisdom, December 2024. URL https://qwenlm. github.io/blog/qvq-72b-preview/. Qwen Team. Qwq-32b: Embracing the power of reinforcement learning, March 2025. URL https://qwenlm.github.io/blog/qwq-32b/. Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia Creswell, Geoffrey Irving, and Irina Higgins. Solving math word problems with process-and outcome-based feedback. arXiv preprint arXiv:2211.14275, 2022. Hemish Veeraboina. Aime problem set 1983-2024, 2023. URL https://www.kaggle.com/ datasets/hemishveeraboina/aime-problem-set-1983-2024. Eric Wallace, Nicholas Tomlin, Albert Xu, Kevin Yang, Eshaan Pathak, Matthew Ginsberg, and Dan Klein. Automated crossword solving. arXiv preprint arXiv:2205.09665, 2022. Ke Wang, Junting Pan, Weikang Shi, Zimu Lu, Houxing Ren, Aojun Zhou, Mingjie Zhan, and Hongsheng Li. Measuring multimodal mathematical reasoning with math-vision dataset. Advances in Neural Information Processing Systems, 37:9509595169, 2024. Peiyi Wang, Lei Li, Zhihong Shao, RX Xu, Damai Dai, Yifei Li, Deli Chen, Wu, and Zhifang Sui. Math-shepherd: label-free step-by-step verifier for llms in mathematical reasoning. arXiv preprint arXiv:2312.08935, 2023. 12 Preprint. Under review. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. Wenshan Wu, Shaoguang Mao, Yadong Zhang, Yan Xia, Li Dong, Lei Cui, and Furu Wei. Minds eye of llms: Visualization-of-thought elicits spatial reasoning in large language models. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. Guowei Xu, Peng Jin, Li Hao, Yibing Song, Lichao Sun, and Li Yuan. Llava-o1: Let vision language models reason step-by-step. arXiv preprint arXiv:2411.10440, 2024. Yi Yang, Xiaoxuan He, Hongkun Pan, Xiyan Jiang, Yan Deng, Xingtao Yang, Haoyu Lu, Dacheng Yin, Fengyun Rao, Minfeng Zhu, et al. R1-onevision: Advancing generalized multimodal reasoning through cross-modal formalization. arXiv preprint arXiv:2503.10615, 2025. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. Advances in neural information processing systems, 36:1180911822, 2023. Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He, et al. Minicpm-v: gpt-4v level mllm on your phone. arXiv preprint arXiv:2408.01800, 2024. Yixin Ye, Zhen Huang, Yang Xiao, Ethan Chern, Shijie Xia, and Pengfei Liu. Limo: Less is more for reasoning. arXiv preprint arXiv:2502.03387, 2025. Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 95569567, 2023. URL https://api.semanticscholar. org/CorpusID:265466525. Kamyar Zeinalipour, Mohamed Zaky Saad, Marco Maggini, and Marco Gori. From arabic text to puzzles: Llm-driven development of arabic educational crosswords. arXiv preprint arXiv:2501.11035, 2025. Dan Zhang, Sining Zhoubian, Ziniu Hu, Yisong Yue, Yuxiao Dong, and Jie Tang. Rest-mcts*: Llm self-training via process reward guided tree search. Advances in Neural Information Processing Systems, 37:6473564772, 2024a. Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Yu Qiao, et al. Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems? In European Conference on Computer Vision, pp. 169186. Springer, 2024b. Han Zhao, Haotian Wang, Yiping Peng, Sitong Zhao, Xiaoyu Tian, Shuaiting Chen, Yunjie Ji, and Xiangang Li. 1.4 million open-source distilled reasoning dataset to empower large language model training, 2025. URL https://arxiv.org/abs/2503.19633. 13 Preprint. Under review."
        },
        {
            "title": "A More Results and Analysis",
            "content": "A.1 Behavior Analysis To better analyze the errors made by models in puzzle-solving, we define two metrics: global length error and local length error. The global length error metric compares the number of words produced by the model with those in the reference answer list, assessing whether the model supplies an answer for every clue in the puzzle. In contrast, the local length error metric compares the length of each individual word to its corresponding reference, thereby quantifying the models adherence to the grid constraints. Table 5 shows that even the best-performing reasoning models, such as Claude-3-7-Sonnet with thinking mode and DeepSeek R1, exhibit global length errors on two puzzles. Almost all modelswith the exception of o3-mini, which demonstrates the highest ICRcommit significant number of local errors. Moreover, we observe that all global length errors arise from models providing shorter answers, meaning that they fail to address some clues. Furthermore, most models tend to produce answers that are longer than the corresponding reference answers. A.2 Grid Parsing Performance In Section 4.3, we demonstrate positive correlation between LVLMs grid-parsing accuracy and their puzzle-solving performance. We also provide several examples illustrating the limitations of LVLMs in extracting horizontal words. Here, we present complete table of all models shown in Table 3, as summarized in Table 6, where similar trend is observed. A.3 Beyond English: Evaluations on Multilingual, Dictionary-based, and Adapted Data In this section, we present additional results for evaluations on the extended data. As shown in Table 7, LLMs generally perform better across these three categories than on the English puzzles; however, even for puzzles from CommonSenseQA, performance remains far from perfect. In contrast, LVLMs do not exhibit significant performance improvement over English puzzles. Based on our observation of strong positive correlation between gridparsing and puzzle-solving performance (See Figure 3 for more details), we hypothesize that the reasoning capabilities of LVLMs are constrained by their visual processing abilities. A.4 Test-Time Scaling: Diminishing Returns on Puzzle Performance In Section 5.3, we use bar chart to demonstrate the performance differences associated with three distinct reasoning efforts for o3-mini on 7x7 English puzzles. Here, we provide table detailing the specific numerical values for each metric as supplement to Figure 7. A.5 Token Usage In this section, we report the token usage for all evaluated models on both the 7x7 and 14x14 English puzzles. Notably, we include non-reasoning models in this analysis, defining token usage as the total number of completion tokens. For reasoning models, token usage is calculated as the sum of reasoning tokens and response tokens. As shown in Figure 8, token usage increases with grid size across all models, with reasoning models generating more. A.6 Input Format Ablations In our main evaluations, both the clues and the grid are embedded within single image as inputs for LVLMs. In this section, we investigate the effect of isolating the clues and providing them to the LVLMs as separate text inputs, while the image contains only an empty grid. The prompt used for this experimental setting is shown in Figure 11 in Appendix B. The results, shown in Figure 9 and measured by WCR on 7x7 English puzzles, reveal no significant performance differences between the two input formats. This indicates that the input format is not the primary cause of the suboptimal performance observed for LVLMs on CrossWordBench, suggesting that LVLMs are robust to slight variations in input format. 14 Preprint. Under review. Models Claude-3-7-Sonnet Claude-3-7-Sonnet (cid:17) GPT-4o-2024-11-20 Gemini 2.0 Pro Exp Gemini 2.0 Flash Pixtral-Large-Instruct-2411 InternVL2 5-78B-MPO NVLM-D-72B Qwen2.5-VL-72B-Instruct QVQ-72B-Preview llava-onevision-72b-ov-chat gemma-3-27b-it Aria MiniCPM-V-2 6 Qwen2.5-VL-3B-Instruct o3-mini (cid:17) Claude-3-7-Sonnet (cid:17) Claude-3-7-Sonnet GPT-4o-2024-11-20 Gemini 2.0 Pro Exp Gemini 2.0 Flash Llama-3.1-405B-Instruct DeepSeek-R1 (cid:17) DeepSeek-V3 R1-Distill-Llama-70B (cid:17) Llama-3.3-70B-Instruct QwQ-32B (cid:17) Open-Reasoner-Zero-32B (cid:17) Phi-4 Global Length Error Local Length Error Tot. Long Short Tot. Long Short Proprietary LVLMs 0 0 10 1 0 Open-Weight LVLMs 3 0 8 2 20 3 18 16 16 31 0 2 1 17 0 0 Proprietary LLMs Open-Weight LLMs 8 2 11 9 22 9 10 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 10 1 3 0 8 2 20 3 18 16 16 31 0 2 1 17 0 0 8 2 11 9 22 9 10 2 363 454 581 565 665 205 237 326 467 568 481 623 682 834 620 791 600 744 525 765 715 829 645 781 720 894 918 688 1034 6 124 274 399 378 633 835 25 513 203 598 65 697 709 4 44 74 183 254 545 741 14 206 110 326 34 473 447 158 217 255 98 97 142 152 171 144 240 114 136 174 230 2 80 200 216 124 88 94 11 307 93 272 31 224 262 Table 5: Global and Local Length Errors across models on 7x7 English puzzles. Long and Short indicate words that are longer or shorter than the corresponding reference answer. 15 Preprint. Under review. Models Size Across Down All Claude-3-7-Sonnet Claude-3-7-Sonnet (cid:17) GPT-4o-2024-11-20 Gemini 2.0 Pro Exp Gemini 2.0 Flash Pixtral-Large-Instruct-2411 NVLM-D-72B InternVL2 5-78B-MPO Qwen2.5-VL-72B-Instruct QVQ-72B-Preview llava-onevision-qwen2-72b-ov-chat gemma-3-27b-it Aria MiniCPM-V-2 6 Qwen2.5-VL-3B-Instruct Proprietary LVLMs - - - - - 0.9540.009 0.9490.010 0.8860.014 0.9620.008 0.9540.009 0.7600.018 0.6540.022 0.4480.024 0.6930.018 0.3810.024 0.8550.010 0.8000.012 0.6680.015 0.8260.011 0.6670.013 Open-Weight LVLMs 124B 78.4B 78.4B 73.4B 73.4B 73.2B 27.4B 25.3B 8.1B 3.75B 0.7530.022 0.4290.024 0.7440.019 0.7300.017 0.7170.022 0.3820.021 0.7460.021 0.2580.022 0.0910.015 0.0230.007 0.3610.022 0.0990.015 0.2580.021 0.3780.023 0.1390.019 0.1850.020 0.2500.021 0.0740.014 0.0180.006 0.0030.002 0.5560.016 0.2610.013 0.5010.014 0.5540.015 0.4280.017 0.2810.014 0.4990.015 0.1650.013 0.0540.009 0.0130.004 Table 6: WCR of Grid Parsing for all models. Models Chinese English Simple CommonSenseQA WCR ICR WCR ICR WCR ICR Proprietary LVLMs GPT-4o-2024-11-20 Claude-3-7-sonnet Gemini 2.0 Flash 0.3660.018 0.1700.017 0.3350.015 0.2270.017 0.3920.026 0.2470.026 0.3390.023 0.2670.018 0.4080.018 0.2880.018 0.5400.022 0.3860.028 0.2080.031 0.2330.018 0.2290.014 0.2160.013 0.3270.021 0.2160. Pixtral-Large-Instruct-2411 Qwen2.5-VL-72B-Instruct 0.2520.015 0.1010.011 0.2160.016 0.1870.015 0.4390.023 0.2700.023 0.3910.025 0.2820.018 0.2390.016 0.1830.015 0.4180.022 0.2520.023 Open-Weight LVLMs Proprietary LLMs GPT-4o-2024-11-20 Claude-3-7-sonnet o3-mini-high (cid:17) 0.5930.019 0.4480.021 0.4380.020 0.3060.022 0.5240.024 0.3260.029 0.4780.019 0.4700.019 0.5390.021 0.4870.021 0.5830.024 0.5180.025 0.7740.016 0.9530.014 0.7820.021 0.9460.012 0.8120.027 0.9710. Open-Weight LLMs DeepSeek-R1 (cid:17) QwQ-32B (cid:17) 0.9070.016 0.8980.018 0.7590.017 0.7870.020 0.7520.031 0.8290.026 0.7010.020 0.6540.022 0.6470.021 0.7340.020 0.6990.026 0.7660.027 Table 7: WCR and ICR for Chinese, English Simple, and CommonSenseQA puzzle sets. Reasoning Effort WCR LCR ICR Avg. Tokens High Medium Low 0.5870.023 0.5340.022 0.2950.018 0.6840.021 0.6340.022 0.3920. 0.8910.018 0.7770.025 0.3630.026 49865 23723 4770 Table 8: o3-mini performance on 7x7 English puzzles across three distinct reasoning efforts. 16 Preprint. Under review. (a) Token Usage of VLMs. (b) Token Usage of LLMs. Figure 8: Token usage for LVLMs and LLMs on 7x7 and 14x14 English puzzles. Reasoning Models are highlighted in red. 17 Preprint. Under review. Figure 9: WCR difference on two inputs formats for LVLMs."
        },
        {
            "title": "B Implementation Details",
            "content": "B.1 Evaluation Metrics Here, we provide more complete and formal description of the three metrics used. Word Coverage Rate (WCR): WCR measures word-level accuracy by calculating the percentage of correctly solved words in the crossword puzzle, defined as: WCR = 1 WA + WD (cid:32) wWA 1 {rw = mw} + wWD (cid:33) 1 {rw = mw} . (2) where WA and WD denote the set of across and down words, respectively. For each word w, rw represents the reference answer, while mw represents the model answer. Letter Coverage Rate (LCR): LCR evaluates letter-level accuracy, providing partial credit for correct letter placements. For each word w, Let: Cw = min(rw,mw) j= 1 {rw[j] = mw[j]} and Lw = max (rw , mw) where Cw counts the correctly matched letters and Lw is the total number of positions considered, the overall letter accuracy is defined as: wW Cw wW Lw LCR = (3) Intersection Consistency Rate (ICR): the internal consistency of the models answers at intersections where across and down words overlap, defined as: ICR = 1 (a,d,j,k)I 1{a[j] = d[k]} (4) where denotes the set of all intersections, where each tuple (a, d, j, k) indicates the jth letter of the across word overlaps with the kth letter of the down word d. This metric reflects whether models correctly adhere to the grid structural constraints of puzzle. B.2 Parsing Details Answers extracted from model responses are converted into JSON format by leveraging the structured output capabilities of o3-mini and dynamic Pydantic models that adhere to the reference answer structure. Algorithm 1 provides the pseudocode for creating these models. 18 Preprint. Under review. key answer[\"direction\"] if key does not match pattern then fields {} pattern ˆ(acrossdown)s+d+$ for all answer reference answers do Algorithm 1 Create Dynamic Pydantic Model 1: function CREATE DYNAMIC PYDANTIC MODEL(reference answers) 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: end function end for Dynamic Pydantic Model create model(\"Dynamic Pydantic Model\", **fields) return Dynamic Pydantic Model raise ValueError(\"Reference key does not match expected format\") end if fields[key] (Optional[ClueAnswer], Field with description Answer for clue at key) Initialize an empty dictionary B.3 Generation Configuration Table 9 lists all the models evaluated along with their corresponding generation configs. Model Name Max Tokens (77) Max Tokens (1414) Temperature Claude-3-7-Sonnet (Anthropic, 2024) Claude-3-7-Sonnet (Thinking) (Anthropic, 2024) GPT-4o-2024-11-20 (Hurst et al., 2024) Gemini 2.0 Pro Exp (DeepMind, 2024) Gemini 2.0 Flash (DeepMind, 2024) o3-mini-high (OpenAI, 2025) Pixtral-Large-Instruct-2411 (Agrawal et al., 2024) NVLM-D-72B (Dai et al., 2024) InternVL2 5-78B-MPO (Chen et al., 2024) Qwen2.5-VL-72B-Instruct (Bai et al., 2025) QVQ-72B-Preview (Team, 2024) llava-onevision-72b-ov-chat (Li et al., 2024a) gemma-3-27b-it (Team et al., 2025) Aria (Li et al., 2024b) MiniCPM-V-2 6 (Yao et al., 2024) Qwen2.5-VL-3B-Instruct (Bai et al., 2025) Llama-3.1-405B-Instruct (Dubey et al., 2024) DeepSeek-R1 (Guo et al., 2025) DeepSeek-V3 (Liu et al., 2024) R1-Distill-Llama-70B (Guo et al., 2025) Llama-3.3-70B-Instruct (Dubey et al., 2024) QwQ-32B (Team, 2025) Phi-4 (Abdin et al., 2024) 8192 64000 16384 20480 20480 100000 20480 20480 20480 100000 20480 8192 20480 20480 20480 100000 100000 100000 100000 20480 100000 10000 8192 64000 16384 20480 20480 100000 20480 20480 20480 20480 100000 20480 8192 20480 20480 20480 100000 100000 100000 100000 20480 100000 10000 0.0 1.0 0.0 0.0 0.0 0.6 0. 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.6 0.0 0.6 0.0 0.6 0.0 Table 9: Generation Configurations for CrossWordBench. B.4 Prompts Figures 10, 11, 12, 13, 14, 15, 17, and 18 present all the prompts employed in this studycomprising image prompts for LVLMs and text prompts for LLMs. We observe that Claude-3-7-Sonnet sometimes produce partial outputs and request confirmation to continue. To mitigate this issue, we incorporate an additional system prompt (see Figure. 18 for the system prompt); note that this modification applies only to the Claude-3-7-Sonnet. 19 Preprint. Under review. Image Zero-Shot CoT Prompt You are given crossword puzzle image containing clues and grid. Your task is to solve the puzzle accurately, ensuring that all answers fit both the given clues and the grid structure, including (cid:44) intersecting words. (cid:44) sequential (e.g., Across clues might be numbered 1, 4, 7, and 9, while Down clues might be 2, 3, 5, 6, and 8). For each clue, provide step-by-step explanation: - Identify the clue by its EXACT NUMBER AND DIRECTION as shown in the image. The numbers may not be (cid:44) (cid:44) - Determine word length from available grid spaces. - Check for any pre-filled letters from intersecting words that have already been solved and explain how (cid:44) - Analyze the clue (definition, wordplay, cryptic hint). - Explain your reasoning process. - Confirm alignment with crossing letters. they constrain possible answers. Solving tips: - Answers must be single word with no spaces (combine phrases if needed). - Abbreviations in clues typically indicate abbreviated answers. - Match the clue's tense, singular/plural form, and part of speech. - Look for wordplay signals, such as question marks (?) for puns or cryptic hints. - Down words are filled from top to bottom, Across words from left to right. - Always confirm that intersecting words remain valid after placing each answer. Present your final solution as: Across: [Number as shown in image]: [Answer] Down: [Number as shown in image]: [Answer] IMPORTANT: - DO NOT list clues in sequential numerical order. You MUST match the exact numbering pattern from the (cid:44) - DO NOT ask for confirmation or stop midway. Always provide complete solution for all clues. image. Figure 10: Image Zero-Shot CoT Prompt. Preprint. Under review. Image Zero-Shot CoT with Grid Only Prompt You are given crossword puzzle image containing only the grid, while the clues are provided separately in text form. Your task is to solve the puzzle accurately, ensuring that all answers fit both the (cid:44) given clues and the grid structure, including intersecting words. (cid:44) Clues: Each clue contains: - Clue Direction and Number (e.g., \"Across 1\", \"Down 2\"). - Start Position (row, column) for the first letter of the answer. - The actual clue text. <clues> For each clue, provide step-by-step explanation: - Identify the clue by its EXACT NUMBER AND DIRECTION as shown in the clue description. The numbers may not be sequential (e.g., Across clues might be numbered 1, 4, 7, and 9, while Down clues might be 2, (cid:44) 3, 5, 6, and 8). (cid:44) - Determine word length from available grid spaces. - Check for any pre-filled letters from intersecting words that have already been solved and explain how (cid:44) - Analyze the clue (definition, wordplay, cryptic hint). - Explain your reasoning process. - Confirm alignment with crossing letters. they constrain possible answers. Solving tips: - Answers must be single word with no spaces (combine phrases if needed). - Abbreviations in clues typically indicate abbreviated answers. - Match the clue's tense, singular/plural form, and part of speech. - Look for wordplay signals, such as question marks (?) for puns or cryptic hints. - Down words are filled from top to bottom, Across words from left to right. - Always confirm that intersecting words remain valid after placing each answer. Present your final solution as: Across: [Number as shown in clues]: [Answer] Down: [Number as shown in clues]: [Answer] IMPORTANT: - DO NOT list clues in sequential numerical order. You MUST match the exact numbering pattern from the (cid:44) - DO NOT ask for confirmation or stop midway. Always provide complete solution for all clues. given clues. Figure 11: Image Zero-Shot CoT with Grid Only Prompt. 21 Preprint. Under review. Text Zero-Shot CoT Prompt You are given crossword puzzle grid and set of clues. Your task is to solve the puzzle accurately, ensuring that all answers fit both the given clues and the grid structure, including intersecting (cid:44) words. (cid:44) Grid Representation: The crossword grid is represented as 2D array where: - `1` represents black (blocked) cell - `0` represents an empty (unfilled) cell <grid> Clues: Each clue contains: - Clue Direction and Number (e.g., \"Across 1\", \"Down 2\"). - Start Position (row, column) for the first letter of the answer. - The actual clue text. <clues> not be sequential (e.g., Across clues might be numbered 1, 4, 7, and 9, while Down clues might be 2, 3, 5, 6, and 8). For each clue, provide step-by-step explanation: - Identify the clue by its EXACT NUMBER AND DIRECTION as shown in the clue description. The numbers may (cid:44) (cid:44) - Determine word length from available grid spaces. - Check for any pre-filled letters from intersecting words that have already been solved and explain how (cid:44) - Analyze the clue (definition, wordplay, cryptic hint). - Explain your reasoning process. - Confirm alignment with crossing letters. they constrain possible answers. Solving tips: - Answers must be single word with no spaces (combine phrases if needed). - Abbreviations in clues typically indicate abbreviated answers. - Match the clue's tense, singular/plural form, and part of speech. - Look for wordplay signals, such as question marks (?) for puns or cryptic hints. - Down words are filled from top to bottom, Across words from left to right. - Always confirm that intersecting words remain valid after placing each answer. Present your final solution as: Across: [Number as shown in clues]: [Answer] Down: [Number as shown in clues]: [Answer] IMPORTANT: - DO NOT list clues in sequential numerical order. You MUST match the exact numbering pattern from the (cid:44) - DO NOT ask for confirmation or stop midway. Always provide complete solution for all clues. given clues. Figure 12: Text Zero-Shot CoT Prompt. 22 Preprint. Under review. Interactive Mode Prompt You are given crossword puzzle image containing clues and grid. Your task is to solve the puzzle accurately, ensuring that all answers fit both the given clues and the grid structure, including (cid:44) intersecting words. (cid:44) sequential (e.g., Across clues might be numbered 1, 4, 7, and 9, while Down clues might be 2, 3, 5, 6, and 8). Pick ONE clue, provide step-by-step explanation: - Identify the clue by its EXACT NUMBER AND DIRECTION as shown in the image. The numbers may not be (cid:44) (cid:44) - Determine word length from available grid spaces. - Check for any pre-filled letters from intersecting words that have already been solved and explain how (cid:44) - Analyze the clue (definition, wordplay, cryptic hint). - Explain your reasoning process. - Confirm alignment with crossing letters. they constrain possible answers. Solving tips: - Answers must be single word with no spaces (combine phrases if needed). - Abbreviations in clues typically indicate abbreviated answers. - Match the clue's tense, singular/plural form, and part of speech. - Look for wordplay signals, such as question marks (?) for puns or cryptic hints. - Down words are filled from top to bottom, Across words from left to right. - Always confirm that intersecting words remain valid after placing each answer. Only solve ONE clue at time and wait for confirmation before proceeding to the next round. Figure 13: Interactive Mode Prompt. Interactive Mode Follow-up Prompt For the following round: Using the confirmed answers so far: sequential (e.g., Across clues might be numbered 1, 4, 7, and 9, while Down clues might be 2, 3, 5, 6, and 8). Pick another clue, provide step-by-step explanation: - Identify the clue by its EXACT NUMBER AND DIRECTION as shown in the image. The numbers may not be (cid:44) (cid:44) - Determine word length from available grid spaces. - Check for any pre-filled letters from intersecting words that have already been solved and explain how (cid:44) - Analyze the clue (definition, wordplay, cryptic hint). - Explain your reasoning process. - Confirm alignment with crossing letters. they constrain possible answers. Solving tips: - Prioritize clues that intersect with confirmed answers. - Answers must be single word with no spaces (combine phrases if needed). - Abbreviations in clues typically indicate abbreviated answers. - Match the clue's tense, singular/plural form, and part of speech. - Look for wordplay signals, such as question marks (?) for puns or cryptic hints. - Down words are filled from top to bottom, Across words from left to right. - Always confirm that intersecting words remain valid after placing each answer. Only solve ONE clue at time and wait for confirmation before proceeding to the next round and do not (cid:44) repeat previously solved clues. Figure 14: Interactive Mode Follow-up Prompt. 23 Preprint. Under review. Grid-Parsing Prompt image consists of two sections: Your task is to extract and match all words from crossword puzzle grid with their respective clues. The (cid:44) 1. The Clue Section: - Contains two lists: \"Across\" and \"Down.\" - Each clue is numbered and corresponds to starting position in the grid. 2. The Grid Section: - crossword grid containing letters, empty cells, and numbered starting positions for words. - Words extend either across (left to right) or down (top to bottom). Step 1: Extract Clues and Grid Structure - Identify all clues under the \"Across\" and \"Down\" sections, preserving their numbers. - Identify all numbered cells in the grid. Step 2: Extract Words from the Grid For each numbered cell: - If the word extends ACROSS: - Start at the numbered cell and read consecutive letters left to right until reaching an empty cell (cid:44) or grid boundary. - If the word extends DOWN: - Start at the numbered cell and read consecutive letters top to bottom until reaching an empty cell (cid:44) or grid boundary. Step 3: Match Words to Clues - Match each numbered word in the grid to its corresponding clue in the Across or Down section. - Ensure extracted words are correctly assigned to their respective clues. Output Format: ACROSS: [Number as shown in image]: [Clue Text] Extracted Word: [Word from Grid] DOWN: [Number as shown in image]: [Clue Text] Extracted Word: [Word from Grid] Ensure accuracy in matching words to their clues, and extract all words fully without omitting any. Figure 15: Grid-Parsing Prompt. Self-Reflection Prompt Your previous solution contains incorrect answers. Take step back, carefully re-examine your entries, and systematically verify each word to ensure complete consistency and correctness within the (cid:44) crossword puzzle. (cid:44) clues meet. Provide step-by-step verification: 1. Cross-Check Letters: List every intersection explicitly, noting the letters where Across and Down (cid:44) 2. Consistency Check: Verify that each intersection matches perfectly. Identify and highlight any (cid:44) 3. Clue Validation: Revisit each clue thoroughly, confirming that each answer fully aligns with its clue (cid:44) 4. Grid Integrity: Confirm that your corrected entries maintain the integrity of the puzzle grid, leaving (cid:44) description and adheres strictly to length constraints. no unresolved conflicts or empty cells. conflicting letters immediately. After completing these steps, present your revised and verified solutions in the following format: Across: [Clue Number]: [Corrected Answer] Down: [Clue Number]: [Corrected Answer] IMPORTANT: - DO NOT list clues in sequential numerical order. You MUST match the exact numbering pattern. - Do NOT restate previous incorrect answers. Provide only fully corrected solutions after reflection. Figure 16: Self-Reflection Prompt. 24 Preprint. Under review. Answer-Parsing System Prompt You are crossword puzzle answer extractor. Extract only valid answers from text response containing (cid:44) crossword solutions. Requirements: - If an answer contains spaces or multiple words, combine them into single word. - Do not shift or reorder answers. For example, if the expected keys are \"Down 12\" and \"Down 13\" and only the answer for \"Down 13\" is provided in the text, then \"Down 12\" should be null and \"Down 13\" should (cid:44) contain the answer. (cid:44) - Do not invent or infer answers not explicitly stated in the text Output your response in the given structure. Figure 17: Answer-Parsing Prompt. Claude System Prompt You are helpful assistant who completes tasks fully without seeking confirmation. Your role is to (cid:44) (cid:44) deliver comprehensive responses in one go. Never ask if the user wants you to continue or show more - you must provide the complete response. Figure 18: Claude System Prompt."
        }
    ],
    "affiliations": [
        "CMU",
        "UIUC",
        "UW",
        "WUSTL"
    ]
}