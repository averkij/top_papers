{
    "paper_title": "Scaling Behavior Cloning Improves Causal Reasoning: An Open Model for Real-Time Video Game Playing",
    "authors": [
        "Yuguang Yue",
        "Irakli Salia",
        "Samuel Hunt",
        "Chris Green",
        "Wenzhe Shi",
        "Jonathan J Hunt"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Behavior cloning is enjoying a resurgence in popularity as scaling both model and data sizes proves to provide a strong starting point for many tasks of interest. In this work, we introduce an open recipe for training a video game playing foundation model designed for inference in realtime on a consumer GPU. We release all data (8300+ hours of high quality human gameplay), training and inference code, and pretrained checkpoints under an open license. We show that our best model is capable of playing a variety of 3D video games at a level competitive with human play. We use this recipe to systematically examine the scaling laws of behavior cloning to understand how the model's performance and causal reasoning varies with model and data scale. We first show in a simple toy problem that, for some types of causal reasoning, increasing both the amount of training data and the depth of the network results in the model learning a more causal policy. We then systematically study how causality varies with the number of parameters (and depth) and training steps in scaled models of up to 1.2 billion parameters, and we find similar scaling results to what we observe in the toy problem."
        },
        {
            "title": "Start",
            "content": "Scaling Behavior Cloning Improves Causal Reasoning: An Open Model for Real-Time Video Game Playing Yuguang Yue Irakli Salia Samuel Hunt Chris Green Wenzhe Shi Jonathan Hunt Abstract Behavior cloning is enjoying resurgence in popularity as scaling both model and data sizes proves to provide strong starting point for many tasks of interest. In this work, we introduce an open recipe for training video game playing foundation model designed for inference in realtime on consumer GPU. We release all data (8300+ hours of high quality human gameplay), training and inference code, and pretrained checkpoints under an open license. We show that our best model is capable of playing variety of 3D video games at level competitive with human play. We use this recipe to systematically examine the scaling laws of behavior cloning to understand how the models performance and causal reasoning varies with model and data scale. We first show in simple toy problem that, for some types of causal reasoning, increasing both the amount of training data and the depth of the network results in the model learning more causal policy. We then systematically study how causality varies with the number of parameters (and depth) and training steps in scaled models of up to 1.2 billion parameters, and we find similar scaling results to what we observe in the toy problem. 6 2 0 2 J 8 ] . [ 1 5 7 5 4 0 . 1 0 6 2 : r 1. Introduction Artificial intelligence (AI) has been applied to game playing since its inception (Turing, 1953). When mod1This paper is based, in part, on prior paper Yue et al. (2025). 2All data, code, model checkpoints and videos of game playing are available at the accompanying website https: //elefant-ai.github.io/open-p2p/. 1Player2, USA. Correspondence to: Yuguang Yue <yuguang@elefant.gg>. Preprint. January 9, 2026. 1 els are allowed to interact directly with an environment, reinforcement learning has achieved remarkable success, including superhuman performance in complex games (Berner et al., 2019; Vinyals et al., 2019). However, such systems are typically tailored to single game, as they rely on carefully engineered training environments and substantial manual design of reward functions, limiting their generality and scalability. Behavior cloning (BC), by contrast, is simple and long-standing approach to policy learning that formulates control as supervised learning from stateaction pairs (Pomerleau, 1989; Bain & Sammut, 1995). Because it learns solely from collected datasets, behavior cloning has the potential to generalize across diverse game environments without requiring environmentNevertheless, BC is specific reward engineering. known to suffer from two fundamental challenges: distributional shift (Ross et al., 2011) and causal confusion (De Haan et al., 2019), both of which can severely degrade performance. In parallel, recent advances in large language models (LLMs), such as ChatGPT (Brown et al., 2020), have brought general-purpose AI into everyday use. Most commercial LLM systems have since evolved into multimodal visual language models (VLMs) that accept image inputs. Despite this progress, deploying large multimodal models for real-time control remains challenging on consumer-grade hardware. Even ignoring latency and cost constraints, current VLMs perform poorly at game control; for example, none are able to complete the first level of the 1996 shooter Quake (Zhang et al., 2025). In this paper, we train single model capable of playing range of 3D video games using raw image observations and producing keyboard and mouse actions in real time on consumer-grade GPU. Our approach leverages behavior cloning with large-scale dataset collected across diverse set of games. We release all training and inference code, as well as the dataset (an example is shown in Figure 1), under open licenses. We demonstrate that the model can play simple games that do not require high level of planning with high An Open Model for Real-Time Video Game Playing Figure 1. Example gameplay sequence with aligned action and text annotations. For visual clarity, we only show the frames where text annotation is initialized, keyboard actions are simplified to WASD inputs, and mouse clicks are omitted; The highlighted key means the key is pressed, and the arrow indicates mouse movement in the and directions. level of competence. We study the scaling laws of BC models under dataconstrained regimes by training four model sizes ranging from 150M to 1.2B parameters across five dataset size ranges of 6%, 12%, 25%, 50%, and 100% of the full training data. We observe clear power-law relationship between test loss and dataset size. Causal confusion occurs due to non-causal correlations in the data that can result in the policy learning to predict the action using these non-causal correlates. An example, taken from De Haan et al. (2019), is that the policy may learn to apply the brakes when it sees the brake lights, since these are highly correlated. Obviously, such non-causal policy performs poorly. We show, both in simple toy example and in testing our model at different data and model sizes, that, in practice, in our setting, scaling both the model and dataset size results in model that more reliably focuses on causal signals. This suggests that one practical solution to issues of causality in behavior cloning is simply scaling up both the model size and data size and diversity. 2. Gameplay Dataset 2.1. Annotated data We collect large-scale, high-quality dataset of human gameplay spanning diverse set of popular 3D video games. The complete list of games is provided in Appendix A.1. Gameplay is recorded by experienced players who are instructed to capture only active gameplay segments (e.g., excluding lobby or waiting periods). Annotators use variety of hardware, monitor sizes and resolutions, mouse sensitivities, and gameplay styles, which provide diverse set of gameplay videos. All gameplay videos are recorded at 20 frames per second (FPS), following prior work (Baker et al., 2022). For each frame, we capture the raw screen pixels oi together with the corresponding keyboard and mouse actions ai. gameplay trajectory is represented as sequence [o1, a1, o2, a2, . . . , oT , aT ], After filtering for quality (see Appendix A.3 for details), the dataset comprises over 8,300 hours of high-quality human gameplay, corresponding to more than 650 million imageaction pairs. The distribution of recording hours across games is shown in Appendix A. 2.1.1. Text-Annotated Data To enable text-conditioned policy learning, we augment the gameplay data with text annotations. These annotations provide text instructions that the model is trained to follow over temporal windows ranging from several seconds to few minutes. Text annotations were generated retrospectively using commercial VLM. The VLM was prompted to review gameplay segments and infer plausible instructions that could have guided the human player at specific timestamps. While VLMs are not capable of playing games at high level (Zhang et al., 2025), we found that they produce high-quality instructional descriptions when analyzing gameplay videos offline. key challenge in this process is that commercial 2 An Open Model for Real-Time Video Game Playing VLMs typically operate on temporally downsampled video (often around 1 Hz), which is insuﬀicient for text annotation in fast-paced games. To address this limitation, we deliberately prompt the VLM to generate temporally extended, goal-oriented instructions (e.g., move toward the skull gate) rather than instantaneous commands (e.g., turn left). We further design the prompt to suppress repetitive, high-frequency events, such as continuous shooting, by annotating such behaviors only once per contiguous segment. single unified prompt is used across all games to ensure scalability and avoid per-game customization. The full prompt is provided in Appendix A.4. 2.1.2. Correction Data common challenge of behavior cloning is distributional shift, where the state distribution encountered during online deployment deviates from that of the training data. Inspired by DAgger (Ross et al., 2011), we mitigated this issue by collecting human correction data. To collect correction trajectories, we deploy trained policy to interact with the game environment while human annotator monitors its behavior. When the policy encounters out-of-distribution situations (e.g., becoming stuck or exhibiting degenerate behavior), the annotator temporarily took control to guide the agent back to valid state. Control is then returned to the policy. For behavior cloning we used only the human actions and masked out the loss for actions taken by the agent. These correction trajectories are mixed with the original annotated data during training and constitute less than 1% of the total annotated dataset. 2.1.3. Simple Benchmark Environments In addition to commercial games, we collected data from two lightweight 3D environments we designed for automated evaluation: Hovercraft and Simple-FPS (see Appendix A.5). These environments are fully programmatic, allowing precise control over game state and diﬀiculty, which provide fair and controlled benchmark for comparing model performance (see Section 4). 2.2. Unlabeled Data In addition to annotated gameplay data, we curated large corpus of unlabeled gameplay videos from public sources (Fan et al., 2022; Baker et al., 2022). Details of the unlabeled dataset and its usage are provided in Appendix B. 3. Policy Model We present multimodal action policy model, which we refer to as Pixels2Play (P2P). P2P is textconditioned policy that takes visual observations and optional textual instructions as input, and outputs lowlevel keyboard and mouse actions. The model builds upon the transformer-based policy architecture introduced in Yue et al. (2025), but introduces substantial extensions to support text conditioning and to improve online performance by explicitly conditioning the backbone transformer on ground-truth action tokens during training. primary design constraint is that the model must operate in real time (20 Hz) on high-end consumer GPUs (e.g. NVIDIA RTX 5090), enabling deployment on end-user hardware. To satisfy this requirement, we trained lightweight, decoder-only transformer backbone from scratch rather than fine-tuning large pretrained VLM, as explored in prior work (Kim et al., 2024). This design offers two advantages: (i) it enables custom image tokenization pipeline that produces small number of image tokens, allowing the model to attend to longer temporal histories, which is important for games that require long-term memorization; and (ii) the reduced model size and custom architecture ensures fast inference and compatibility with compilation and optimization techniques. image encoders, We evaluated several including MagVitV2 (Luo et al., 2024), DINOv2 (Oquab et al., 2023), IBQ (Shi et al., 2025), and CosmosTokenizer (Agarwal et al., 2025). We found that EﬀicientNet (Tan & Le, 2019), following Pearce & Zhu (2022), provides the best trade-off between representation quality and computational eﬀiciency. Specifically, we use the first six layers of EﬀicientNet, followed by linear projection into small set of visual tokens (typically Ni {1, . . . , 4}). Unfreezing the image encoder during training consistently improves performance compared to freezing it (Appendix C.1). The model operates directly on the resized raw pixel inputs (192 pixels 192 pixels). Much prior work focused on single-game agents and adopts reduced action spaces that can be modeled as single categorical variable (Baker et al., 2022; Pearce & Zhu, 2022). In contrast, our setting requires unified policy capable of operating across many games, necessitating much larger action space comprising the full keyboard and mouse input domain. We allowed up to four simultaneous key presses and two concurrent mouse actions, making treating the action as single one-hot combinatorial impractical. 3 An Open Model for Real-Time Video Game Playing (a) (b) Figure 2. (a) Architecture of P2P. The core policy transformer and action decoder are both decoder-only transformers. Each timestep begins with text token ti. Since many frames do not contain text annotation there is default text token tnull used on these frames. This is followed by image token(s) from video frame oi followed by learnable reasoning token ki that grants the model extra computation. The policy transformer then outputs single action prediction token ain. smaller transformer, the action decoder, then auto-regressively transforms and samples the single action prediction token into the full action space. Then the true action tokens ai are input so that ain at time + 1 can attend to the true action tokens from time i. (b) Attention mask used in our transformer policy (green denotes 1 and gray 0). This custom mask ensures the action prediction token ain at time cannot attend to the ground truth action at time i. Note that no other tokens attend to ain to stabilize the training process. To address this, we modeled actions autoregressively. We used Na = 8 tokens for each action: 4 keyboard tokens, 2 mouse tokens (x, movement), and 2 mouse button tokens. To avoid increasing the token count of the main transformer, we introduced lightweight action decoder. The backbone policy transformer outputs single latent action token ain, which is then decoded autoregressively by the action decoder into the full action specification (Figure 2a). This design allows the policy transformer to perform single forward pass per timestep during inference, yielding approximately 5 speedup in real-time execution compared to directly predicting all action tokens. At each timestep, the policy transformer consumes image tokens (oi), text-conditioning token (ti), groundtruth action tokens (ai), and single action prediction token (ain). We further introduce an optional thinking token ki, which provides the model with an additional reasoning step prior to action prediction. The resulting token count per timestep is Ni +Na +2. Each token is augmented with learned type embedding indicating its role (image, text, reasoning, ground-truth action, or action prediction). Rotary positional embeddings (Su et al., 2024) are applied at every transformer layer. During inference, we employ keyvalue caching with sliding-window attention to bound memory usage. Because ground-truth action tokens are provided as input during training, we design custom causal attention mask to prevent information leakage. The action prediction token ain is prohibited from attending to ground-truth action tokens at the same timestep, ensuring causality. Other tokens (image, text, reasoning, and ground-truth actions) may attend to each other within the same timestep, but are restricted from attending to ain tokens from previous timesteps to avoid training instability (Figure 2b). Behavior cloning often suffers from causal confusion (De Haan et al., 2019). One particular case of this, which becomes worse at higher frequencies, is the model learns to copy previous actions instead of attending to visual inputs (Wen et al., 2020). Although this issue can be avoided by excluding ground-truth actions from the policy inputs, we find that conditioning the policy on ground-truth action tokens produces more human-like behavior than conditioning on visual observations alone. In particular, the policy learns to sustain actions over multiple frames before switching, closely resembling human gameplay dynamics. Qualitative differences in behavior are illustrated in the demonstration videos3. We observe pronounced causal confusion when directly predicting action tokens without an action decoder and when we only had small amount of training data. In 3Under Action Conditioning section on the website. 4 An Open Model for Real-Time Video Game Playing contrast, introducing an action decoder and scaling the dataset size substantially mitigates this issue. detailed analysis is provided in Section 5.1. 3.0.1. Leveraging unlabeled data Since unlabeled data from publicly available resources are far more abundant than annotated data, it is desirable to leverage such unlabeled data for training. Details on leveraging unlabeled data, including the training procedure and some preliminary experimental results, are provided in Appendix D.6. (a) (b) Figure 3. (a) Gap induced by video compression without data augmentation. We measured this gap by comparing model outputs on raw frames (inference) and resized frames (training). An irreducible gap arises from lossy video compression during data collection. The gap was smaller with RGB than YUV encoding and increases as compression quality degrades (lower file size). The x-axis qp denotes the quantization parameter, where larger values indicate lower quality. Data augmentation (b) mitigates this gap when reasonable compression quality is used. 3.1. Mitigating the TrainingInference Gap Early experiments revealed substantial performance gap between offline evaluation and online deployment. We traced this gap to discrepancies between train5 ing and inference inputs arising from video recording, compression, and resizing. For practical latency and storage considerations, videos undergo two lossy processing steps during data collection: (1) compression and upload on the annotator side, and (2) resizing to 192 192 for model processing. During inference resizing occurs but no compression takes place. We observed substantial divergence in model outputs when using uncompressed raw frames versus resized training frames, which leads to deceptively strong offline metrics but poor online performance. We measured this gap by collecting small number of uncompressed videos. We then compressed the videos using differing compression options and compared the trained model probabilities on the uncompressed versus compressed output. Although compression at the annotation stage is unavoidable (which causes the irreducible gap), we find that the choice of color space during resizing plays critical role: RGB encoding yields smaller traininginference gap than YUV encoding (Figure 3b). Unfortunately, NVIDIA hardware encoders support only the YUV color space; therefore, we adopt mixture of QP values to balance encoding speed and encoding quality. We also found that two different resizing functions were used between inference and training code paths that, while visually indistinguishable, contributed to the traininginference gap. The training code used PyTorch function, while inference used Rust function. We modified the code to ensure bitwise identical resizing function was used for both training data and inference, and this mitigated the issue. Data augmentation also substantially reduces the traininginference gap. We applied mild spatial transformations, color perturbations, Planckian jitter, ISO noise, random gaussian or motion blur, sharpening, and translation during training. As shown in Figure 3b, these augmentations significantly reduced the discrepancy and improved online performance. Consequently, all experiments in this work employ data augmentation. We believe further improvements to reduce the training-inference gap via targeted data augmentation are an interesting area for future work. Finally, discretization of the mouse action space can lead to overly aggressive or overly conservative mouse Following Pearce & Zhu (2022), we movements. discretized mouse actions using quantile-based bins, which provide fine resolution near zero but coarse resolution in the tails. To improve robustness, we fit truncated normal distributions to the and axes of the mouse action space using undiscretized training data. At inference time, the policys predicted discrete An Open Model for Real-Time Video Game Playing mouse action defines the upper and lower bounds, from which we sample the final mouse action using the fitted truncated normal distribution. Empirically, this approach yields smoother control and better online performance. Model Size Hovercraft Simple-FPS FPS 150M 300M 600M 1.2B 42 37 44 37 24 21 28 37 80 64 62 40 4. Evaluation All models were trained using automatic mixedprecision training (Micikevicius et al., 2017), with model parameters stored in float32 and activations in bfloat16, except for RMSNorm layers, which were computed in float32. Common techniques such as z-loss and K, norm (Rybakov et al., 2024) were applied to stabilize the training. Training was performed on 8NVIDIA H100 GPUs. All inference experiments are conducted on single NVIDIA RTX 5090 GPU; an additional NVIDIA RTX 5080 GPU was used for game rendering to ensure inference ran at consistent speed. Unless otherwise specified, all experiments share the same set of hyperparameters, which are reported in Appendix D.2. Data augmentation, as described in Section 3.1, was applied to all training data. We trained policy models at four parameter scales (150M, 300M, 600M, and 1.2B) and compared their performances. We evaluated our models using (i) the scores from controlled programmatic environments, (ii) human preference evaluations on real games, and (iii) quantitative analyzes of scaling behavior on test loss4 and causality. Although several existing game benchmarks exist (Zhang et al., 2025; VIDEO, 2025; Xu et al., 2025; Tomilin et al., 2023), they primarily focus on textheavy 2D games or sandbox-style 3D environments, which differ substantially from the real-time, firstperson gameplay data used in our training setup. Textual input is used exclusively for the instructionfollowing evaluation and is not provided in any other evaluation setting. Camera settings can significantly impact performance when playing games in real-time, as overly high sensitivity makes it diﬀicult for the model to adjust to small movements. Detailed camera configurations for DOOM, Quake, and Roblox are provided in Appendix D.1. 4.1. Simple Programmatic Environment Table 1. Performance on Godot-based programmatic environments across model sizes. The Hovercraft value is the average time (in seconds) that each model takes to finish loop; the Simple-FPS value is the number of hits of enemy minus the number of hits received by model. FPS is how many frames per second of inference was feasible on RTX 5090 GPU. ronments were implemented by us in Godot (Holfeld, 2023), allowing full control over map layouts and difficulty settings, which enables fair and reproducible comparisons across model variants. For Hovercraft, we measure the time (in seconds) required for the agent to complete full loop. For Simple-FPS, we report the number of hits on the enemy minus the number of hits received. We also report end-to-end inference latency measured on single RTX 5090 GPU. We compared models of varying sizes trained on the full labeled dataset. Each model was evaluated three times in the same environment, and we report the mean performance. Results are summarized in Table 1. In general, the larger model has worse latency but better capacity. 4.2. Human Evaluation in Real Environments Since our policy is designed for real-time interaction in real video games, we conducted human evaluation on gameplay videos generated by the models. We evaluated performance on four games: two single-player titles (DOOM and Quake from the Steam platform) and two multiplayer games (Roblox Be-a-Shark and Roblox Hypershot). For DOOM and Quake, we manually divided the games into multiple checkpoints. The model was run from each checkpoint for approximately 90 seconds, or until it reached the subsequent checkpoint. Specifically, we split the first level of DOOM into three checkpoints and the first level of Quake into four checkpoints. Together with Be-a-Shark and Hypershot, this yielded total of nine evaluation tasks. For each checkpoint, we ran the model three times and used the best trajectory for human evaluation. We first evaluate our models in two programmatic environments: Hovercraft and Simple-FPS. These enviHuman evaluators5 assessed (i) the extent to which the agents behavior resembles that of human player and 4Perplexity of the keyboard was used as test loss because some games do not require mouse actions so mouse perplexity was more noisy. 5The evaluators were authors of this paper. Evaluation was performed blinded to the model identity. The videos used for evaluation are available on the Evaluation section from the website. 6 An Open Model for Real-Time Video Game Playing (ii) how effectively it progressed toward checkpointspecific objectives. Quantitative preference comparisons across model sizes are shown in Figure 4a. As we can see, the 1.2B model outperforms the rest, which is aligned with the numerical score from the Section 4.1. (a) (b) Each model Figure 4. (a) Human preference comparisons across model sizes (ties excluded). is evaluated from nine game checkpoints, and its gameplay trajectories are recorded. Human evaluators then compare model pairs and judge which performs better based on (i) how closely the behavior resembles that of human player and (ii) how effectively the model progresses toward the next checkpoint, when applicable. (b) Instruction-following comparison. Each model is evaluated from the same maze checkpoint with and without an instruction (press the red button), with five runs per condition. We report the success rate of completing the maze. 4.2.1. Instruction-following We evaluated the models ability to follow text instructions. We performed this evaluation in the Quake environment, where experiments can reliably start from the same checkpoint. We selected maze scenario in which the player must press three red buttons on the wall to unlock door (see Appendix D.3 for demonstration). Without textual input, the model consistently fails to press all three buttons. This failure mode is expected, as the policy primarily imitates expert trajectories, and the action of not pressing button introduces only subtle deviations in behavior unless explicitly emphasized through instruction. We evaluated four candidate models on this maze over five independent runs and compared their success rates. As shown in Figure 4b, providing the text instruction press the red button substantially increases the success rate compared to the no-text baseline, demonstrating that the Figure 5. Lowest test loss versus dataset size for the 1.2B model. As might be expected, we find the test loss fits power-law curve closely. model actively conditions its behavior on text input6. However, we also observe mild performance degradation when textual instructions are provided, such as camera shaking during gameplay. This issue primarily arises from increased inference latency introduced by the text encoder, which reduces the control frequency to slightly above 20 Hz. This could be mitigated by caching the text encoding between frames which we have not yet implemented. The resulting delay can cause instability when deploying the model in real-time interactive settings. 5. Causality and scaling laws We used our working recipe for model training and architecture to investigate the relationship between model and dataset size and both the test loss and the causal behavior of the model. We focused on the data constrained regime, which is becoming an increasing focus in other modalities (Muennighoff et al., 2025). In this regime, we train each model/data size for multiple epochs until overfitting or lack of further improvement is observed. We used static subset of approximately 500M frames (about 7,000 hours of gameplay). Modof this dataset els (100%, 50%, 25%, 12.5%, 6%) paramefour across ter scales (1.2B, 600M, 300M, 150M). trained on fractions are Following Kaplan et al. (2020), we fitted the empirical scaling relationship L(D) = + , ( )α Dc where denotes the irreducible loss, is the number 6The videos are available on the Instruction Following section from the website. 7 An Open Model for Real-Time Video Game Playing of training frames, and Dc and α are fitted constants. For the 1.2B model, we estimate = 1.111, α = 0.2336, and Dc = 17, as shown in Figure 5. Scaling curves for all model sizes are reported in Appendix D.4. In Appendix D.4 Figure 14 we provide the full test loss trajectories as function of training steps for all combinations of model scale and dataset size. These results show that, in general, larger models achieve lower test loss, and increasing the amount of training data consistently reduces test loss. Moreover, larger models benefit disproportionately from additional data in data-abundant regimes (e.g., when using 50% or 100% of the training data). (a) (b) Figure 6. (a) toy environment we used to investigate causality in behavior cloning. The observation contains both causally informative feature (is an obstacle present) and correlated but non-causal feature (is the brake light on from the previous frame). (b) We find that increasing the depth of the network improves the speed of learning causally correct solution. We also find that for all nonlinear networks, an approximately causality correct solution is found using SGD. Despite the fact that optimal linear policy exists, we find that SGD makes no progress towards learning solution with randomly initialized linear network. 5.1. Causality Analysis 5.1.1. Toy problem Before presenting the results on the large behavior cloning models, we first construct simple environment to better understand the relationship between causality and network depth. As we will show, the results demonstrate surprising commonality with the scaled up models. In the toy environment, the observation consists of two binary features (Figure 6a) and three distractor features of random noise. The action is single binary choice (brake, dont brake). The binary features indicate when an obstacle is present and whether the brake was applied in the previous step (i.e. brake light). The optimal policy is to brake whenever an obstacle is present and to ignore all other components of the observation. However, because obstacles persist for 8 multiple frames, there is strong correlation between the brake light feature and braking. We train simple neural networks consisting of linear (with bias) network and multilayer perceptrons (MLPs) of differing depths with ReLU non-linearity. All networks have final sigmoid, so that the output can be interpreted as p(as). All networks are trained using stochastic gradient descent (SGD), batch size of 1, and fixed learning rate. The training data consists only of optimal behavior. We evaluate the causality of the learned policies as = log p(a = 1so) log p(a = 1sb) where so is an observation with an obstacle and no brake light, and sb is an observation with no obstacle but the brake light on. The optimal causal policy should brake for the obstacle but not for the brake light so coptimal = . Note that this measure is logarithmic. The causally optimal policy can be trivially represented with linear policy. Despite this, we find that training randomly initialized linear policy using SGD results in no progress towards learning causal policy. Adding non-linearity is necessary for the SGD optimization to make progress towards causally correct solution. We find that increasing the number of layers improves the speed of learning causally optimal solution (Figure 6b). 5.2. Scaled Causality In practical settings, such as video game playing, it is much more diﬀicult to have an explicit measure of the causal correctness of model. We therefore focus on one particular pain-point: in action-conditioned models, predictions can be influenced by two sources: the visual input (frame sequence) and the ground-truth action history. While effective decision-making should largely be causally driven by image observations analogous to human perceptionmodels often exhibit lazy behavior, relying disproportionately on the statistical priors of the action sequence rather than the visual evidence. To quantify this tendency, we propose causality score that measures the models reliance on the input frame sequence. higher score indicates stronger causal dependence of the models output on frame input. We analyze this score across different model sizes and dataset scales as function of the number of training frames. Our results show that, as in the toy example, larger, deeper models generally achieve higher causality scores, and that causality increases with additional training. Furthermore, increasing the number of unique training frames leads to higher causality scores, An Open Model for Real-Time Video Game Playing except in extremely data-limited regimes. We compute the causality score by measuring changes in the models output distribution using the KL divergence between predictions produced from the original input sequence and from perturbed version of the same sequence. Formally, ( ) score(f ) = KL (ob,c, ab,c), (ob,c, ab,c) , bbatch c= where denotes the model, is the original image sequence, is the original action sequence, is the perturbed image sequence, and is the number of evenly sized temporal chunks. Given an input sequence, we partition it into equallength chunks and randomly perturb individual image frames with probability p. The model output at the end of each chunk is then compared between the original and perturbed inputs. Only image frames are perturbed; the action sequence remains unchanged. To ensure that perturbations remain semantically meaningful rather than degenerate noise, each perturbed frame is replaced with frame drawn from different scene within the same game. This is implemented by swapping frames within the batch, preserving gamelevel context while inducing perturbations. For fair comparison across models, we compute causality scores using the checkpoint with the lowest test loss for each model. As shown in Figure 7, causality scores increase with both model size and dataset size, except in the extremely data-limited regime (e.g., 30M frames). The standard deviation of the score is on the order of 104 and does not affect the relative ranking of models. We additionally report causality scores as function of training steps in Appendix D.5, where we observe consistent increase in causality throughout training. Notably, causality scores continue to rise even in overfitting regimes, suggesting that causal reliance on image input and generalization performance are not always aligned. 6. Related Work large body of prior work studied vision language action (VLA) models for robotic control. Many approaches fine-tune pretrained visionlanguage models (VLMs) to generate robot actions (Driess et al., 2025; Kim et al., 2024; Pertsch et al., 2025; Intelligence et al., 2025; Zitkovich et al., 2023; Wang et al., 2025; Chen et al., 2025; Zhou et al., 2025; Shukor et al., 2025; Bjorck et al., 2025), while others train VLA models Figure 7. Causality score as function of dataset size and model size. Except in the low-data regime (30M), the causality score generally increases with larger models and larger training datasets. from scratch using robot-centric video and instruction data (Brohan et al., 2022; Zheng et al., 2025). These methods have demonstrated strong performance in both simulated and real-world robotic tasks. Our setting differs from robotic VLA models in several fundamental ways. First, robotic action spaces are typically continuous, whereas our action space is discrete (keyboard and mouse inputs), which leads to our choice of autoregressive rather than flow-matching for the action output. Second, robotic VLA policies generally operate at low control frequencies (typically below 5 Hz), which is insuﬀicient for real-time video game interaction that requires substantially higher temporal resolution. parallel line of work focuses on learning action policies in video game environments. Many studies trained agents within single environment or testbed, such as Minecraft (Li et al., 2025; Hafner et al., 2025; Fan et al., 2022; Baker et al., 2022; Lifshitz et al., 2023; Wang et al., 2024) or Counter-Strike: Global Offensive (Pearce & Zhu, 2022; Durst et al., 2024). Other approaches leverage large language models (LLMs) or vision language models (VLMs) to guide or improve gameplay through planning, code generation, or tool use (Yang et al., 2024; Li et al., 2025; Wang et al., 2023). In multi-game settings, Reed et al. (2022) demonstrated generalist agent capable of playing variety of Atari games, while Wiedemer et al. (2025) leveraged VLMs for zero-shot understanding and control. More recent work train general action models across diverse video game environments (Raad et al., 2024; Bolton et al., 2025). However, these systems do not release code or datasets, making reproduction and further research challenging for the broader community. An Open Model for Real-Time Video Game Playing contemporaneous piece of work (Magne et al., 2025) also explored training single model across multiple video games, primarily focusing on controller-based console games (e.g., Xbox). In contrast, our work targets keyboardmouse based PC games, which involve different interaction modalities and higher-frequency real-time control, making our setting complementary to prior efforts. 7. Conclusion In this work, we present large-scale, high-quality video dataset annotated with both actions and text, and introduce real-time policy model designed to leverage this data for online gameplay in real-world environments. We discuss key technical considerations for effectively training such models and evaluate their performance through both qualitative and quantitative analyses. Furthermore, we study how model performance scales with dataset size and model capacity, examining both scaling laws and causal behavior. Our results show that larger models achieve lower test loss and higher causality scores in data-abundant regimes. This suggests that one approach to issues of causality in behavior cloning may be simply scaling both model size and dataset size and diversity."
        },
        {
            "title": "Acknowledgment",
            "content": "We thank João Victor for his significant contributions to the collection of the labeled dataset."
        },
        {
            "title": "References",
            "content": "Agarwal, N., Ali, A., Bala, M., Balaji, Y., Barker, E., Cai, T., Chattopadhyay, P., Chen, Y., Cui, Y., Ding, Y., et al. Cosmos world foundation model platform arXiv preprint arXiv:2501.03575, for physical ai. 2025. Bain, M. and Sammut, C. framework for behavioural cloning. In Furukawa, K., Michie, D., and Muggleton, S. (eds.), Machine Intelligence 15. Oxford University Press, 1995. Baker, B., Akkaya, I., Zhokov, P., Huizinga, J., Tang, J., Ecoffet, A., Houghton, B., Sampedro, R., and Clune, J. Video pretraining (vpt): Learning to act by watching unlabeled online videos. Advances in Neural Information Processing Systems, 35:24639 24654, 2022. Berner, C., Brockman, G., Chan, B., Cheung, V., Dębiak, P., Dennison, C., Farhi, D., Fischer, Q., Hashme, S., Hesse, C., et al. Dota 2 with large scale deep reinforcement learning. arXiv preprint arXiv:1912.06680, 2019. Bjorck, J., Castañeda, F., Cherniadev, N., Da, X., Ding, R., Fan, L., Fang, Y., Fox, D., Hu, F., Huang, S., et al. Gr00t n1: An open foundation model for generalist humanoid robots. arXiv preprint arXiv:2503.14734, 2025. Bolton, A., Lerchner, A., Cordell, A., Moufarek, A., Bolt, A., Lampinen, A., Mitenkova, A., Hallingstad, A. O., Vujatovic, B., Li, B., et al. Sima 2: generalist embodied agent for virtual worlds. arXiv preprint arXiv:2512.04797, 2025. Brohan, A., Brown, N., Carbajal, J., Chebotar, Y., Dabis, J., Finn, C., Gopalakrishnan, K., Hausman, K., Herzog, A., Hsu, J., et al. Rt-1: Robotics transformer for real-world control at scale. arXiv preprint arXiv:2212.06817, 2022. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. Chen, P., Bu, P., Wang, Y., Wang, X., Wang, Z., Guo, J., Zhao, Y., Zhu, Q., Song, J., Yang, S., et al. Combatvla: An eﬀicient vision-language-action model for combat tasks in 3d action role-playing games. arXiv preprint arXiv:2503.09527, 2025. De Haan, P., Jayaraman, D., and Levine, S. Causal confusion in imitation learning. Advances in neural information processing systems, 32, 2019. Driess, D., Springenberg, J. T., Ichter, B., Yu, L., LiBell, A., Pertsch, K., Ren, A. Z., Walke, H., Vuong, Q., Shi, L. X., et al. Knowledge insulating visionlanguage-action models: Train fast, run fast, generalize better. arXiv preprint arXiv:2505.23705, 2025. Durst, D., Xie, F., Sarukkai, V., Shacklett, B., Frosio, I., Tessler, C., Kim, J., Taylor, C., Bernstein, G., Choudhury, S., et al. Learning to move like professional counter-strike players. In Computer Graphics Forum, volume 43, pp. e15173. Wiley Online Library, 2024. Fan, L., Wang, G., Jiang, Y., Mandlekar, A., Yang, Y., Zhu, H., Tang, A., Huang, D.-A., Zhu, Y., and Anandkumar, A. Minedojo: Building open-ended embodied agents with internet-scale knowledge. Advances in Neural Information Processing Systems, 35:1834318362, 2022. 10 An Open Model for Real-Time Video Game Playing Hafner, D., Yan, W., and Lillicrap, T. Training agents arXiv preprint inside of scalable world models. arXiv:2509.24527, 2025. Journal of Machine Learning Research, 26(53):1 66, 2025. URL http://jmlr.org/papers/v26/24-1000. html. Holfeld, J. On the relevance of the godot engine in the indie game development industry. arXiv preprint arXiv:2401.01909, 2023. Intelligence, P., Black, K., Brown, N., Darpinian, J., Dhabalia, K., Driess, D., Esmail, A., Equi, M., Finn, C., Fusai, N., et al. π0. 5: vision-language-action model with open-world generalization. arxiv 2025. arXiv preprint arXiv:2504.16054, 2025. Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J., and Amodei, D. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. Kim, M. J., Pertsch, K., Karamcheti, S., Xiao, T., Balakrishna, A., Nair, S., Rafailov, R., Foster, E., Lam, G., Sanketi, P., et al. Openvla: An opensource vision-language-action model. arXiv preprint arXiv:2406.09246, 2024. Li, Z., Xie, Y., Shao, R., Chen, G., Jiang, D., and Nie, L. Optimus-2: Multimodal minecraft agent with goal-observation-action conditioned policy. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 90399049, 2025. Lifshitz, S., Paster, K., Chan, H., Ba, J., and McIlraith, S. Steve-1: generative model for text-tobehavior in minecraft. Advances in Neural Information Processing Systems, 36:6990069929, 2023. Luo, Z., Shi, F., Ge, Y., Yang, Y., Wang, L., and Shan, Y. Open-magvit2: An open-source project toward democratizing auto-regressive visual generation. arXiv preprint arXiv:2409.04410, 2024. Magne, L., Awadalla, A., Wang, G., Xu, Y., Belofsky, J., Hu, F., Kim, J., Schmidt, L., Gkioxari, G., Kautz, J., Yue, Y., Choi, Y., Zhu, Y., and Fan, L. J. Nitrogen: An open foundation model for generalist gaming agents. Preprint, NVIDIA / MineDojo, December 2025. URL https://nitrogen.minedojo.org/ assets/documents/nitrogen.pdf. Micikevicius, P., Narang, S., Alben, J., Diamos, G., Elsen, E., Garcia, D., Ginsburg, B., Houston, M., Kuchaiev, O., Venkatesh, G., et al. Mixed precision training. arXiv preprint arXiv:1710.03740, 2017. Muennighoff, N., Rush, A. M., Barak, B., Scao, T. L., Piktus, A., Tazi, N., Pyysalo, S., Wolf, T., and Raffel, C. Scaling data-constrained language models. Nvidia, J. B., Castaneda, F., Cherniadev, N., Da, X., Ding, R., Fan, L., Fang, Y., Fox, D., Hu, F., Huang, S., et al. Gr00t n1: An open foundation model for generalist humanoid robots. ArXiv, abs/2503.14734, 2025. Oquab, M., Darcet, T., Moutakanni, T., Vo, H., Szafraniec, M., Khalidov, V., Fernandez, P., Haziza, D., Massa, F., El-Nouby, A., et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. Pearce, T. and Zhu, J. Counter-strike deathmatch with large-scale behavioural cloning. In 2022 IEEE Conference on Games (CoG), pp. 104111. IEEE, 2022. Pertsch, K., Stachowicz, K., Ichter, B., Driess, D., Nair, S., Vuong, Q., Mees, O., Finn, C., and Levine, S. Fast: Eﬀicient action tokenization for vision-language-action models. arXiv preprint arXiv:2501.09747, 2025. Pomerleau, D. A. Alvinn: An autonomous land vehicle in neural network. In Advances in Neural Information Processing Systems, volume 1. Morgan Kaufmann, 1989. Raad, M. A., Ahuja, A., Barros, C., Besse, F., Bolt, A., Bolton, A., Brownfield, B., Buttimore, G., Cant, M., Chakera, S., et al. Scaling instructable agents across many simulated worlds. arXiv preprint arXiv:2404.10179, 2024. Reed, S., Zolna, K., Parisotto, E., Colmenarejo, S. G., Novikov, A., Barth-Maron, G., Gimenez, M., Sulsky, Y., Kay, J., Springenberg, J. T., et al. generalist agent. arXiv preprint arXiv:2205.06175, 2022. Ross, S., Gordon, G., and Bagnell, D. reduction of imitation learning and structured prediction to noregret online learning. In Proceedings of the fourteenth international conference on artificial intelligence and statistics, pp. 627635. JMLR Workshop and Conference Proceedings, 2011. Rybakov, O., Chrzanowski, M., Dykas, P., Xue, J., and Lanir, B. Methods of improving llm training stability. arXiv preprint arXiv:2410.16682, 2024. Schmidt, D. and Jiang, M. Learning to act without actions. arXiv preprint arXiv:2312.10812, 2023. 11 An Open Model for Real-Time Video Game Playing Shi, F., Luo, Z., Ge, Y., Yang, Y., Shan, Y., and Wang, L. Scalable image tokenization with index backpropagation quantization. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 1603716046, 2025. Shukor, M., Aubakirova, D., Capuano, F., Kooijmans, P., Palma, S., Zouitine, A., Aractingi, M., Pascal, C., Russi, M., Marafioti, A., et al. Smolvla: visionlanguage-action model for affordable and eﬀicient robotics. arXiv preprint arXiv:2506.01844, 2025. Su, J., Ahmed, M., Lu, Y., Pan, S., Bo, W., and Liu, Y. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. Tan, M. and Le, Q. Eﬀicientnet: Rethinking model scaling for convolutional neural networks. In International conference on machine learning, pp. 6105 6114. PMLR, 2019. Tomilin, T., Fang, M., Zhang, Y., and Pechenizkiy, M. Coom: game benchmark for continual reinforcement learning. Advances in Neural Information Processing Systems, 36:6779467832, 2023. Turing, A. M. Digital computers applied to games. Faster than thought, 1953. VIDEO, D. Orak: Afoundational benchmark for training and evaluating llm agents on diverse video games. arXiv preprint arXiv:2506.03610, 2025. Vinyals, O., Babuschkin, I., Czarnecki, W. M., Mathieu, M., Dudzik, A., Chung, J., Choi, D. H., Powell, R., Ewalds, T., Georgiev, P., et al. Grandmaster level in starcraft ii using multi-agent reinforcement learning. nature, 575(7782):350354, 2019. Wang, G., Xie, Y., Jiang, Y., Mandlekar, A., Xiao, C., Zhu, Y., Fan, L., and Anandkumar, A. Voyager: An open-ended embodied agent with large language models. arXiv preprint arXiv:2305.16291, 2023. Wang, Y., Ding, P., Li, L., Cui, C., Ge, Z., Tong, X., Song, W., Zhao, H., Zhao, W., Hou, P., et al. Vla-adapter: An effective paradigm for tinyscale vision-language-action model. arXiv preprint arXiv:2509.09372, 2025. Wang, Z., Cai, S., Mu, Z., Lin, H., Zhang, C., Liu, X., Li, Q., Liu, A., Ma, X. S., and Liang, Y. Omnijarvis: Unified vision-language-action tokenization enables open-world instruction following agents. Advances in Neural Information Processing Systems, 37:7327873308, 2024. Wen, C., Lin, J., Darrell, T., Jayaraman, D., and Gao, Y. Fighting copycat agents in behavioral cloning from observation histories. Advances in Neural Information Processing Systems, 33:25642575, 2020. Wiedemer, T., Li, Y., Vicol, P., Gu, S. S., Matarese, N., Swersky, K., Kim, B., Jaini, P., and Geirhos, R. Video models are zero-shot learners and reasoners. arXiv preprint arXiv:2509.20328, 2025. Xu, S., Deng, W., Zhou, Y., and Zhong, F. Probe by gaming: game-based benchmark for assessing conceptual knowledge in llms. arXiv preprint arXiv:2505.17512, 2025. Yang, J., Dong, Y., Liu, S., Li, B., Wang, Z., Tan, H., Jiang, C., Kang, J., Zhang, Y., Zhou, K., et al. Octopus: Embodied vision-language programmer from environmental feedback. In European conference on computer vision, pp. 2038. Springer, 2024. Ye, S., Jang, J., Jeon, B., Joo, S., Yang, J., Peng, B., Mandlekar, A., Tan, R., Chao, Y.-W., Lin, B. Y., et al. Latent action pretraining from videos. arXiv preprint arXiv:2410.11758, 2024. Yue, Y., Green, C., Hunt, S., Salia, I., Shi, W., and Hunt, J. J. Pixels to play: foundation model for 3d gameplay. In 2025 IEEE Conference on Games (CoG), pp. 14. IEEE, 2025. Zhang, A. L., Griﬀiths, T. L., Narasimhan, K. R., and Press, O. Videogamebench: Can vision-language models complete popular video games? arXiv preprint arXiv:2505.18134, 2025. Zheng, R., Wang, J., Reed, S., Bjorck, J., Fang, Y., Hu, F., Jang, J., Kundalia, K., Lin, Z., Magne, L., et al. Flare: Robot learning with implicit world modeling. arXiv preprint arXiv:2505.15659, 2025. Zhou, Z., Zhu, Y., Zhu, M., Wen, J., Liu, N., Xu, Z., Meng, W., Peng, Y., Shen, C., Feng, F., et al. Chatvla: Unified multimodal understanding and robot control with vision-language-action model. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pp. 5377 5395, 2025. Zitkovich, B., Yu, T., Xu, S., Xu, P., Xiao, T., Xia, F., Wu, J., Wohlhart, P., Welker, S., Wahid, A., et al. Rt-2: Vision-language-action models transfer web knowledge to robotic control. In Conference on Robot Learning, pp. 21652183. PMLR, 2023. An Open Model for Real-Time Video Game Playing A. Dataset A.1. Game List We list all the games we collected in the annotated dataset: Roblox: Blade Ball Msdos:Quake Goat Simulator 3 Warhammer: VerRoblox: Death Ball Msdos: Need for Speed Helldivers 2 Roblox: Be Shark Msdos: DOOM Need for Speed: Hot Roblox: Be Snake Msdos: DOOM II Pursuit mintide Warhammer: mintide VerRoblox: Be Tornado DOOM Eternal Roblox: Natural DisasEval: Hovercraft ter Survival Roblox: Rivals Roblox:Slap Battle Roblox:A Dusty Trip Eval: Basic FPS Left 4 Dead 2 ground Call of Duty Mobile Fortnite Call of Duty: Black Ops Roblox: Hypershot II Zombies Roblox: Evade Call of Duty: Black Ops House Flipper House Flipper 2 Roblox: Murderers vs. III Zombies PowerWash Need for Speed: Most Saints Row: The Third Wanted Need for Speed: Carbon Saints Row IV Resident Evil 5: MerceNeed for Speed: Undernaries Resident Evil Revelations 2: Raid Grand Theft Auto Grand Theft Auto: San Andreas Sheriffs Goat Simulator Euro Truck Simulator 2 Minecraft A.2. Labeled Data The labeled data distribution regarding game types can be found at Figure 8 13 An Open Model for Real-Time Video Game Playing Figure 8. Distribution of games in the annotated dataset. A.3. Quality filter Each recorded video undergoes two-stage quality assurance process. First, we apply automated filtering based on the following criteria: (1) no single key is held for more than 60% of the total frames to avoid hardware or recording artifacts; (2) no more than six keys are pressed simultaneously at any time; (3) minimum level of interaction is maintained, ensuring that actions are taken throughout the sequence; (4) lightweight action policy model (described in Section 3) is evaluated on the full trajectory, and videos with likelihoods below predefined threshold are flagged. All videos flagged by these checks are reviewed by human inspectors. In addition, we randomly sampled videos for manual inspection using an adaptive strategy based on each annotators prior contributions to given game: annotators with fewer accepted, recorded hours for that game were sampled with higher probability, while those with more extensive prior submissions were sampled less frequently. In this approach, we ensured that the recorded videos were of high quality, expert gameplay. 14 An Open Model for Real-Time Video Game Playing A.4. Text Annotation"
        },
        {
            "title": "Here we show the prompt we used for commercial VLM to do text annotations",
            "content": "The prompt used for video text annotation is You are an expert gaming tutor and video analyst. I. MACRO INSTRUCTIONS A. Definition MACRO INSTRUCTION is an event that is explicitly listed in the GAME-FOCUS RULE table (Section II) or selects the players next objective or area and passes **all three** checks: 1. Changes the long-term goal or destination ( 1 new room, zone, target, phase, or tactic). 2. Without it the player would wander or be unsure what to pursue next. 3. When applicable, cites **distinct visual landmark** beginners can recognise (door colour, sign, glowing platform, giant tree, scoreboard icon, billboard, unique prey, ball aura, etc.). B. Start & finish timestamps start = first frame where the objective becomes clear. end = frame where the objective is satisfied; use null if unfinished when the video ends. II. GAME-FOCUS RULE When deciding whether an event is *strategic enough* to log as macro, follow the table below. Skip events that dont match the focus for that game family. Game family ----------------------------------------------------------------------------------------------------------------------------------- **Growth-Arena** (Roblox *Be Snake*, *Be Shark*, *Be Tornado*) Switching strategy to avoid larger Focused macro events (examples) snake/shark/tornado or ambush smaller snake/shark/tornado (**Eat the small snake**; **avoid the big shark **). Use the numbers on top of each snake/shark/tornados head to indicate the size. Activating or chasing power-up landmark (Dash to the **gold lightning-bolt boost**). **FPS** (*Doom*, *Doom2*, *Quake*, *Call of Duty Series*, *Left 4 dead*, etc) Reaching or unlocking key areas (Collect the **red skull key** from the altar, press switch on the wall to open the door). Navigating path choices (Enter the **bronze-framed doorway with skull carving**, **move to the entry with red mark **). Securing major gear required to finish the level (Take the **rocket launcher on the blood altar**). Use switch to open the door (Press the switch on the wall to open the door). Switch weapons or obtain weapons (Use axe, Use gun). Pay attention to what the weapon the player is holding, if it was switched then note it down (switch to gun) **Arena Ball / Dodgeball** (Roblox *Blade Ball*, *Death Ball*, etc) Choosing safe position or platform ( Jump onto the **neon cube just outside the danger zone**). Engaging the ball intentionally (Charge the ** glowing ball with spin-slash**). Triggering or acquiring special ability landmark (Grab the **purple deflect power-up circle**). **Racing** (*Need for Speed*, etc.) Major route decision or shortcut (Drift through the **billboard shortcut into the alley**). Scheduled pit stop or repair landmark (Pit in at the **blue-neon service lane**). Switching race tactics (Switch to **draft-and-slingshot strategy behind the lead car**). **Survival** (*Natural Disaster Survival*, etc.) Major route decision to escape or survive (Run to the ** elevator shaft**). If game isnt listed, map it to the closest family. III. GAME-IGNORE RULE When deciding whether an event is **NOT** *strategic enough* to log as macro, follow the table below. Game family ----------------------------------------------------------------------------------------------------------------------------------- **Growth-Arena** (Roblox *Be Snake*, *Be Shark*, *Be Tornado*) Eating small prey from the map (e.g., SKIP macro events (examples) Eat the small fog) **Survival** (*Natural Disaster Survival*, etc.) IGNORE the DISASTER WARNING caption. 15 An Open Model for Real-Time Video Game Playing If game isnt listed, map it to the closest family. IV. EVENT-FREQUENCY RULES A. Global cool-down 8 between macros (unless genuine strategy shift appears sooner). B. Continuous-action filter (Growth-Arena, racers) Record macro only when the agent changes *overall tactic* or *target landmark* (giant pellet, boss prey, pit stop). Ignore repetitive minor events. V. DIVERSITY RULE After accuracy is guaranteed, vary verbs, landmark phrases, sentence forms. Try to avoid using the same instruction, use diverse description to describe the same event. Never invent events just to add varietyaccuracy outranks diversity. VI. AVOID RULE Avoid using the same instruction to describe the same event, try to be as diverse in wording as possible. Avoid using directions in the instruction, such as turn right to xxx, turn left to xxx, etc. VII. ON-SCREEN TEXT RULE Capture any explicit text hint/objective that appears on screen. Quote it exactly once and log it as macro if it meets the four checks. VIII. WORKFLOW 1. Watch the video once; identify the game and its win condition(s). 2. **Narrative** third-person past tense, focus on strategy and playstyle. 3. **Macro list** array of objects: { start: MM:SS, end: MM:SS null, instruction: <imperative sentence> } IX. OUTPUT (JSON only no markdown) { narrative: <string>, macro_instructions: [ { start: 00:12, end: 00:25, instruction: Enter the bronze-framed doorway with skull carving }, { start: 00:34, end: 00:57, instruction: Take the corridor lit by flickering red lights }, { start: 00:01, end: 00:02, instruction: Switch to axe }, { start: 00:52, 16 An Open Model for Real-Time Video Game Playing end: instruction: Run through without engaging with enemy 00:54, }, { start: 01:01, end: 01:02, instruction: Change the weapon from gun to fist }, ] } A.5. Simple Environment We constructed some simple games for automated testing with Godot. (a) Hovercraft environment: the car is looping in fixed racing road. We evaluate the model by measuring how much time it takes to finish one loop (b) Simple-FPS environment: simple fps in static map. We evaluate the model by counting the number of hit enemies minus the hits taken. B. Unlabeled dataset B.1. Unlabeled data distribution In addition to annotated gameplay data, we curate large corpus of unlabeled gameplay videos from public sources (Fan et al., 2022; Baker et al., 2022). While such data are abundant, they present several challenges: highly variable quality, interleaving with non-game content, different resolutions and frame rates, and the absence of corresponding action labels. We curate unlabeled gameplay trajectories using two-stage filtering pipeline based on commercial VLMs. In the first stage, videos are filtered using metadata signals such as titles, descriptions, topics, and thumbnails (when available). VLM is queried to assess relevance to predefined set of game-related queries. In the second stage, the full video content is processed to segment and remove non-gameplay intervals. To balance cost and filtering accuracy, this step is performed on temporally downsampled video. We run queries covering broad range of popular game titles to collect diverse gameplay footage. The distribution of video hours in the resulting unlabeled dataset is shown in Figure 10. Note that we did not release this dataset as we do not hold the copyright. 17 An Open Model for Real-Time Video Game Playing Figure 10. Distribution of games in the unlabeled dataset. C. Policy Model C.1. Image Tokenizer Figure 11 compares training and validation perplexity when the image tokenizer is either frozen or trained jointly with the policy model. Jointly training the image tokenizer consistently yields lower perplexity on both training and validation sets, indicating that adapting the visual representation is critical for achieving strong downstream policy performance. 18 An Open Model for Real-Time Video Game Playing (a) Training perplexity with frozen vs. unfrozen tokenizer (b) Validation perplexity with frozen vs. unfrozen tokenizer Figure 11. Effect of training the image tokenizer jointly with the policy model. As might expected, training the image tokenizer during model training significantly reduce the training and validation perplexity. D. Evaluation D.1. Camera setup We list the in-game camera setup below: Roblox: Camera sensitivity set to 0.52. Quake: Mouse sensitivity set to 3.5; smoothing disabled. DOOM: Smoothing set to 2; look sensitivity 22%, move sensitivity 22%. D.2. Hyperparameters Table 2 summarizes the hyperparameters used for training the policy model. Batch sizes are chosen to maximize GPU utilization for each model scale. We perform limited sweep over learning rates {1104, 3104, 3105} on the full dataset and select the best-performing value for each model size, which is then fixed for all remaining experiments. Parameter Value Batch size History length Frame resolution Text tokenizer Image tokenizer Number of image tokens Transformer Backbone Number of layers Hidden dimension Query heads Keyvalue heads Optimizer (AdamW) Learning rate Weight decay β1 β2 5 (1.2B), 8 (600M), 8 (300M), 10 (150M) 200 192 192 Gemma EﬀicientNet 1 10 (150M), 20 (300M), 9 (600M), 22 (1.2B) 1024 (150M, 300M), 2048 (600M, 1.2B) 16 1 104 (150M, 300M, 600M), 3 105 (1.2B) 1 104 0.9 0.999 Table 2. Architecture and training hyperparameters across model scales. 19 An Open Model for Real-Time Video Game Playing D.3. Text Instruction Checkpoints Figure 12 shows the checkpoints used for evaluating text-conditioned behavior in Quake. In the maze, there are three red buttons and the player needs to press all of them to open the door. Figure 12. Selected frames from the Quake maze. Three buttons appear along the path, all of which must be pressed to open the door. D.4. Scaling Laws Figure 13 presents scaling-law fits for all four model sizes (150M, 300M, 600M, and 1.2B). Across all configurations, test loss closely follows power-law relationship with respect to the number of training frames. Larger models consistently achieve lower test loss across dataset sizes when dataset size is relatively large. Figure 13. Scaling-law curves relating test loss to the number of training frames. All four models are fitted power-law curve between the data size and test loss, the data exhibits strong fit to the power-law curve. Figure 14 further breaks down test loss as function of training frames across different dataset fractions and model sizes. An Open Model for Real-Time Video Game Playing (a) 100% of the dataset (b) 50% of the dataset (c) 25% of the dataset (d) 12% of the dataset Figure 14. Test loss as function of training frames across dataset sizes and model scales. The larger model can leverage the data better when the dataset size is larger, and in general the larger models achieve lower test loss when trained with similar number of frames. (e) 6% of the dataset 21 An Open Model for Real-Time Video Game Playing D.5. Causality Analysis Figure 15 shows causality scores as function of training steps for different model and dataset sizes. Because causality scores consistently increase over the course of training, the 600M model trained on the 30M dataset can exhibit higher causality score than the same model trained on the 500M dataset at certain training steps. However, when we instead select model checkpoints based on the lowest test loss and then evaluate causality, the resulting trends align with those discussed in Section 5.1. Figure 15. Causality scores as function of training steps for different model and dataset sizes. Because causality scores generally increase during training, 600M model trained on 30M samples can exhibit higher causality than the same model trained on 500M samples at intermediate checkpoints. When selecting checkpoints based on lowest test loss, however, the resulting causality trends are consistent with those reported in Section 5.1. D.6. Pretraining with Unlabeled Data Unlabeled gameplay videos are orders of magnitude more abundant than human annotated demonstrations. We therefore leverage an inverse dynamics model (IDM) to convert unlabeled videos into additional training data. D.6.1. Inverse Dynamics Model Two classes of IDMs have been explored in prior work: real-action models that directly predict keyboard and mouse actions (Baker et al., 2022), and latent-action models that infer abstract action codes subsequently mapped to real actions (Schmidt & Jiang, 2023; Ye et al., 2024; Nvidia et al., 2025). For simplicity, we adopt the realaction formulation. Formally, the IDM predicts the action at time conditioned on the surrounding image sequence: ( at pIDM at o1, o2, . . . , oT ) . The IDM shares the same architecture as the policy model (Figure 2a), with two key differences. First, it does not condition on text or ground-truth action tokens, since environment dynamics are independent of text inputs An Open Model for Real-Time Video Game Playing and the IDM must predict all action labels in single forward pass rather than autoregressively. Second, the IDM is non-causal and is allowed to attend to future frames, which improves action imputation accuracy. This architectural alignment ensures that improvements to the policy model transfer directly to the IDM. The IDM was trained using cross-entropy loss on the labeled dataset. To ensure robustness to the diversity of unlabeled data, we applied extensive data augmentation during training. Once trained, the IDM was used to impute actions for the unlabeled gameplay dataset. D.6.2. Evaluation of pretrained model We trained 600M model that leverages unlabeled data to study the benefits of large-scale pretraining. We refer to this model as pretrained-600M, which is obtained through three-stage procedure: (1) training an inverse dynamics model (IDM) on the full labeled dataset; (2) using the trained IDM to generate pseudo-labels for an unlabeled dataset that is approximately 10 larger than the annotated dataset, followed by pretraining the 600M policy model on this pseudo-labeled data for one epoch; and (3) fine-tuning the model on the full labeled dataset. Evaluating the quality of the IDM presents nontrivial challenge. Since ground-truth action labels are only available for the annotated dataset, quantitative evaluation of the IDM can only be performed on this data, on which the IDM is also trained. As result, standard evaluation metrics tend to overestimate performance and do not fully reflect the IDMs effectiveness in its intended deployment setting. In practice, the IDM is primarily used to generate pseudo-labels for unlabeled gameplay videos, whose distribution differs substantially from that of the labeled data. To account for this distributional gap, we complement quantitative evaluation with manual inspection. Specifically, we sample unlabeled videos and assess the plausibility and temporal consistency of the generated pseudolabels using human judgment. This qualitative evaluation provides an additional sanity check on whether the IDM produces reasonable action annotations when applied to out-of-distribution data. D.6.3. Simple Programmatic Environment Similar to the evaluation of other models, we show the evaluation on Godot environment of the 600M pretrained model at Table 3. There is incremental changes in the scores when compared to its 600M counterpart. Model Size Hovercraft Simple-FPS FPS Pretrained 600M 38 24 61 Table 3. Performance on Godot-based programmatic environments across model sizes for pretrained 600M model. There is only incremental change when comparing with the 600M scores from Table 1. D.6.4. Quantitative Eval Here, we compare the test loss of the pretrained 600M model with that of 600M model trained solely on labeled data. As shown in Figure 16, the pretrained 600M model achieves substantially lower test loss than the model trained only on labeled data when both are trained on the same number of frames. 23 An Open Model for Real-Time Video Game Playing Figure 16. Comparison of test loss between pretrained and label-only 600M models. The pretrained 600M model achieves substantially lower test loss than model trained solely on labeled data when trained on the same number of frames. D.6.5. Human Evaluation in Real Environment We compare gameplay videos generated by the pretrained 600M model and the 600M model trained using labeled data only. Although the pretrained model achieves lower test loss, its online performance is not significantly preferred over the non-pretrained model. We hypothesize that this gap arises because pretraining incorporates large amount of unrelated video data, which may introduce atypical or environment-specific movements when deployed in particular game. Such behaviors can appear less human-like, which is critical factor in human preference during evaluation."
        }
    ],
    "affiliations": [
        "Player2, USA"
    ]
}