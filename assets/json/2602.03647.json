{
    "paper_title": "Search-R2: Enhancing Search-Integrated Reasoning via Actor-Refiner Collaboration",
    "authors": [
        "Bowei He",
        "Minda Hu",
        "Zenan Xu",
        "Hongru Wang",
        "Licheng Zong",
        "Yankai Chen",
        "Chen Ma",
        "Xue Liu",
        "Pluto Zhou",
        "Irwin King"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Search-integrated reasoning enables language agents to transcend static parametric knowledge by actively querying external sources. However, training these agents via reinforcement learning is hindered by the multi-scale credit assignment problem: existing methods typically rely on sparse, trajectory-level rewards that fail to distinguish between high-quality reasoning and fortuitous guesses, leading to redundant or misleading search behaviors. To address this, we propose Search-R2, a novel Actor-Refiner collaboration framework that enhances reasoning through targeted intervention, with both components jointly optimized during training. Our approach decomposes the generation process into an Actor, which produces initial reasoning trajectories, and a Meta-Refiner, which selectively diagnoses and repairs flawed steps via a 'cut-and-regenerate' mechanism. To provide fine-grained supervision, we introduce a hybrid reward design that couples outcome correctness with a dense process reward quantifying the information density of retrieved evidence. Theoretically, we formalize the Actor-Refiner interaction as a smoothed mixture policy, proving that selective correction yields strict performance gains over strong baselines. Extensive experiments across various general and multi-hop QA datasets demonstrate that Search-R2 consistently outperforms strong RAG and RL-based baselines across model scales, achieving superior reasoning accuracy with minimal overhead."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 3 ] . [ 1 7 4 6 3 0 . 2 0 6 2 : r 2026-02Search-R2: Enhancing Search-Integrated Reasoning via Actor-Refiner Collaboration Bowei He, Minda Hu, Zenan Xu, Hongru Wang, Licheng Zong, Yankai Chen Chen Ma, Xue Liu, Pluto Zhou, Irwin King The Chinese University of Hong Kong LLM Department, Tencent Mohamed bin Zayed University of Artificial Intelligence McGill University City University of Hong Kong The University of Edinburgh"
        },
        {
            "title": "Abstract",
            "content": "Search-integrated reasoning enables language agents to transcend static parametric knowledge by actively querying external sources. However, training these agents via reinforcement learning is hindered by the multi-scale credit assignment problem: existing methods typically rely on sparse, trajectory-level rewards that fail to distinguish between high-quality reasoning and fortuitous guesses, leading to redundant or misleading search behaviors. To address this, we propose Search-R2, novel ActorRefiner collaboration framework that enhances reasoning through targeted intervention, with both components jointly optimized during training. Our approach decomposes the generation process into an Actor, which produces initial reasoning trajectories, and Meta-Refiner, which selectively diagnoses and repairs flawed steps via cut-and-regenerate mechanism. To provide fine-grained supervision, we introduce hybrid reward design that couples outcome correctness with dense process reward quantifying the information density of retrieved evidence. Theoretically, we formalize the ActorRefiner interaction as smoothed mixture policy, proving that selective correction yields strict performance gains over strong baselines. Extensive experiments across various general and multi-hop QA datasets demonstrate that SearchR2 consistently outperforms strong RAG and RL-based baselines across model scales, achieving superior reasoning accuracy with minimal overhead."
        },
        {
            "title": "Introduction",
            "content": "Large language models are rapidly evolving from static knowledge repositories into dynamic, search-integrated agents that interact with external environments (Trivedi et al., 2023; Li et al., 2025). By combining iterative reasoning with active retrieval, these agents tackle knowledge-intensive tasks such as open-domain and multi-hop question answering that were previously intractable due to limited parametric knowledge and hallucinations. Consequently, this field has turned to Reinforcement Learning (RL) to optimize these systems (Jin et al., 2025; Chen et al., 2025), grounding agent behavior in task-specific performance objectives rather than imitation of human demonstrations. However, training search-integrated agents with RL faces key challenge: multi-scale credit assignment. In practice, agent behavior is sequence of decisions, including query formulation, information filtering, and logical deduction, yet standard methods optimize policies with trajectory-level rewards such as final-answer correctness (Jin et al., 2025; Wang et al., 2025). Since this outcome-only signal provides no supervision over intermediate reasoning or the timing and necessity of retrieval, it induces credit misattribution across both retrieval and reasoning decisions (Zhang et al., 2025a). Consequently, efficient, logically coherent trajectories receive similar credit to trajectories that succeed only after redundant, costly, or poorly timed retrieval, reducing sample efficiency and yielding brittle reasoning chains. This limitation highlights critical gap in current methodologies: the inability to diagnose and repair error propagation. As shown in Figure 1, single irrelevant search query early in trajectory can misguide the entire subsequent reasoning chain. Existing rejection sampling techniques (Ahn et al., 2024) are inefficient here, as they discard the entire trajectory rather than addressing the specific root cause of the deviation. To build robust agents, we must move beyond outcome-based filtering toward paradigm that enforces both global reasoning coherence and local search quality. To this end, we propose Search-R2, novel ActorRefiner collaboration framework designed to enhance searchintegrated reasoning through targeted intervention. Unlike standard generation approaches, Search-R2 decomposes the reasoning process into two distinct roles: an Actor that generates initial reasoning trajectories with tool calls, and Meta-Refiner that identifies localized failures, such as uninformative retrieval or logical gaps, and performs cut-and-regenerate operation. This mechanism preserves valid reasoning prefixes while surgically repairing flawed *The first three authors have equal contributions. Correspondence to: Hongru Wang <hrwang@ed.ac.uk>, Irwin King <king@cse.cuhk.edu.hk>, Pluto Zhou <plutozhou096@foxmail.com> 2026-02-04 Figure 1: Demonstration of Search-R1 and Search-R2. While Search-R1 (Left) is disrupted by retrieval noise and falls into an error propagation loop, Search-R2 (Right) utilizes an Actor-Refiner collaboration. The Meta-Refiner identifies the deviation and applies \"cut-and-regenerate\" mechanism to surgically repair the reasoning chain at the point of error, successfully redirecting focus from the incorrect entity (Aguinaldo) to the correct one (Quezon). suffixes, significantly enhancing learning efficiency. The Actor and Meta-Refiner are jointly optimized during training, enabling mutual feedback between trajectory generation and selective refinement. To further provide dense supervision, we introduce hybrid reward that combines outcome correctness with process reward that quantifies the density of evidence information. We theoretically prove that our ActorRefiner interaction, which is modeled as smoothed mixture policy, strictly exceeds the performance of baselines like rejection sampling under satisfiable conditions. Experiments on seven benchmarks show consistent gains of the proposed Search-R2 over strong RAG and RL-based baselines across model sizes ranging from 7B to 32B, with minimal overhead. In summary, our contributions are as follows: 1.Problem Identification: We formalize the multi-scale credit assignment problem in search-integrated reasoning, highlighting the inadequacy of trajectory-level rewards for optimizing intermediate search behaviors. 2.Framework: We propose Search-R2, an ActorRefiner framework that integrates step-level process rewards with trajectory-level cut-and-regenerate refinement mechanism, and jointly optimizes both the Actor and the Refiner. 3. Theoretical Analysis: We characterize the Meta-Refiner as mixture policy and derive the theoretical conditions under which selective correction guarantees performance improvement over baseline sampling. 4. Empirical Success: We demonstrate state-of-the-art performance on seven across different-size models, showing that Search-R2 improves both the accuracy of final answers and the quality of the underlying search process."
        },
        {
            "title": "2 Related Works",
            "content": "This section reviews prior work on search-integrated reasoning and multi-turn reinforcement learning. 2.1 Search-Integrated Reasoning Search-integrated language agents augment large language models with the ability to actively query external information sources during problem solving, enabling them to overcome the limitations of static parametric knowledge (Jin et al., 2025; Chen et al., 2025). Prior work has explored search-augmented reasoning for tasks such as multi-hop question answering (Sun et al., 2025; Wu et al., 2025), deep research (Team et al., 2025; Hu et al., 2024), and web-based decision making (Zhou et al.; Hu et al., 2025), demonstrating that iterative search can substantially improve factual accuracy and coverage. More recent approaches integrate search into reinforcement learning frameworks (Chen et al., 2025; Qian & Liu, 2025; Song et al., 2025), allowing agents to learn when and how to issue search queries based on task-level feedback. However, existing methods typically optimize search behavior only through delayed, trajectory-level rewards, without explicitly assessing the quality of individual search decisions (Wen et al., 2026). As result, agents often issue redundant, mistimed, or weakly informative queries, especially in long-horizon interactions (Gao et al., 2025), where suboptimal search decisions compound over time and degrade both task performance and learning efficiency. 2 2026-02-04 Figure 2: Overview of the Search-R2 framework. The Actor generates initial reasoning trajectories with search queries. The Meta-Refiner employs Discriminator to detect errors and Trimmer to identify the exact step of failure. Upon rejection, the trajectory is truncated and regenerated from the error point. The system is jointly optimized via GRPO using hybrid reward. 2.2 Credit Assignment in Multi-Turn RL Learning effective policies for multi-turn decision making remains central challenge in reinforcement learning and agent research due to sparse rewards and difficult credit assignment (Devidze et al., 2022; Wang & Ammanabrolu, 2025). This challenge is particularly pronounced in search-integrated agents, where intermediate decisions such as query formulation and timing are evaluated only through final task outcomes (Zhang et al., 2025a). Prior work has proposed dense reward shaping (Zeng et al., 2025; Zhang et al., 2025b) and learned reward models (Zou et al., 2025), including Large Language Models (LLM)-based judges (Zha et al., 2025), to provide richer feedback signals. While these techniques improve optimization stability in some settings, they are most commonly applied to evaluate final responses or aggregate trajectory quality, leaving the quality of intermediate decisions underspecified. Consequently, policy optimization often suffers from low sampling efficiency, as many rollouts contain low-quality intermediate actions that contribute little to learning. These limitations motivate approaches that provide fine-grained supervision over intermediate decisions while remaining compatible with multi-turn optimization."
        },
        {
            "title": "3 Methodology",
            "content": "We propose Search-R2, novel Actor-Refiner collaboration framework designed to address the multi-scale credit assignment challenge. Rather than treating search-integrated reasoning as monolithic generation task, our approach decouples the process into two distinct phases: an Actor generating initial reasoning chains, and Meta-Refiner performing trajectory-level assessment and causal correction. This decomposition allows us to optimize both global reasoning coherence and local search quality simultaneously. 3.1 The Search-Integrated Reasoning Actor The foundation of our system is an Actor policy, denoted as πl(x), responsible for generating the initial reasoning trajectory ˆy. Given the search engine Λ, the πl is trained to invoke Λ autonomously following standard tool-use paradigm (Algorithm 2), to enable dynamic information acquisition. The model generates chain of thought and, when necessary, emits query within <search>...</search> tags. The system halts generation, executes the query against Λ, appends the top-k results within <information>...</information> tags, and resumes generation. This cycle repeats until the model outputs the final answer or reaches step limit. To initialize πl, we utilize structural template  (Table 1)  that enforces strict format: Reasoning Search Call Answer. This acts as soft constraint, ensuring adherence to the systems operational logic without imposing content-specific biases. 3.2 The Meta-Refiner for Hierarchical Correction core premise of our work is that suboptimal search decisions often occur in intermediate steps and silently misguide subsequent reasoning. Standard rejection sampling is inefficient for repairing such cascading errors. 3 2026-02-04 Table 1: Template for Search-Integrated Reasoning following the implementation of Search-R1 Jin et al. (2025). Answer the given question. You must conduct reasoning inside <think> and </think> first... if you lack knowledge, call search engine via <search> query </search>... return results in <information>... Final answer in <answer>... Question: question. To address this, we introduce the Meta-Refiner, which performs targeted causal intervention rather than blind regeneration. The Meta-Refiner shares the underlying LLM with the Actor but is steered by control prompts to perform two sub-objectives. 1) Discriminator for global coherence checking. The Discriminator, denoted πd( ˆy x) [0, 1], serves as gate that enforces trajectory-level reasoning coherence. Given reasoning trajectory ˆy, it estimates the probability that the reasoning remains globally coherent with the problem specified by x. We accept ˆy when πd( ˆy x) τ; otherwise, we flag it for refinement. Accordingly, the acceptance probability is Bernoulli distribution α( ˆyx) = P(πd( ˆy x) τ). 2) Trimmer for local error localization. To address the issue of error propagation, the Trimmer πh(k ˆy, x) identifies the specific search step + 1 where the reasoning or search query first deviated (the \"root cause\"). The system preserves the valid prefix ˆy1:k, truncates the flawed suffix, and regenerates new suffix using the base policy πl. This cut-and-regenerate strategy preserves valuable partial reasoning, significantly improving sample efficiency compared to discarding the entire trajectory. Together, the discriminator and trimmer implement an iterative accept-or-repair procedure. For each candidate trajectory, the discriminator first decides whether it is globally coherent. If it is rejected, the trimmer localizes the earliest deviation and triggers cut-and-regenerate editing to produce revised trajectory. This collaborative process induces smoothed mixture policy q(y x), formalized in Algorithm 1. Repeating this procedure up to budget Nmax yields progressively improved trajectories and accumulates correction history, which strengthens the Meta-Refiners ability to localize errors over time. if πd( ˆyx) τ then return ˆy {Accept} Algorithm 1 Meta-Refiner Execution Flow 1: Input: Context x, Policy πl, Discriminator πd, Trimmer πh. 2: Generate initial trajectory ˆy πl(x) 3: while < Nmax do 4: 5: 6: 7: 8: 9: 10: 11: 12: end while 13: return = ˆy end if Sample cut-point πh( ˆy, x) yprefix ˆy1:k Regenerate ysuffix πl(x, yprefix) ˆy [yprefix, ysuffix] + 1 3.3 Hybrid Reward Modeling for Multi-Scale Supervision To tackle the credit assignment issue where local search actions are conflated with global outcomes, we introduce hybrid reward R(y) that provides supervision at both scales. Global Outcome Reward. We use Exact Match (EM) between the predicted answer apred and ground truth agold: routcome(y) = I(apred = agold). This ensures the final output satisfies the users intent. Local Process Reward. To distinguish between trajectories that are correct by chance versus those supported by high-quality evidence, we quantify the utility of retrieved context. For set of retrieved chunks = {c1, . . . , cM}, an external judge evaluates the utility ui {0, 1} of each chunk. The process reward is the density of useful information: rprocess(y) = 1 Overall reward. To prevent reward hacking (maximizing retrieval without solving the task), the process reward is gated by outcome i=1 ui. Implementation specifics are outlined in Appendix K. R(y) = routcome(y) (1 + rprocess(y)). This formulation explicitly reinforces the principle that high-quality search is necessary condition for robust reasoning. (1) 3.4 Joint Optimizing the Actor and Meta-Refiner 2026-02-04 We leverage Group Relative Policy Optimization (GRPO) to optimize the shared weight θ of Actor and Meta-Refiner jointly (Shao et al., 2024). For each input x, we sample group of trajectories {y1, . . . , yG} from the mixture distribution q(x). Crucially, we treat each yi as an augmented execution trace comprising both the reasoning path from πl and refinement actions sampled from the discriminator πd(y) and trimmer πh(k ˆy). The objective is to maximize: LGRPO(θ) = Lt(yi, θ) = (cid:104) (cid:34) 1 x,{yi}G i=1 rt(θ) ˆAi,t, clip(rt(θ), 1 ϵ, 1 + ϵ) ˆAi,t Lt(yi, θ) 1 Li t=1 Li i=1 (cid:35) (cid:105) βDKL[πlπref], (2) where the advantage ˆAi is computed via group normalization of the hybrid rewards, and rt(θ) denotes the probability ratio πθ (atst) (atst) , which measures the deviation of the current policy from the old policy. This allows the model to πθold learn the optimal balance between generation and correction solely from the interaction outcome, effectively solving the multi-scale credit assignment problem end-to-end. 3.5 Mechanisms of Performance Gain Unlike prior work that optimizes only the Actor, we jointly optimize both the Actor and the Meta-Refiner. To rigorously justify the necessity of optimizing the Meta-Refiner, as opposed to relying on static prompting or standard rejection sampling, we decompose the total expected sample reward improvement J, into three governing mechanisms. As formally derived in Appendix D, the net performance gain is not byproduct of mere sampling volume, but strictly depends on the agents ability to satisfy specific covariance conditions. We characterize the gain decomposition as: = Aprec (cid:124) (cid:123)(cid:122) (cid:125) Selection Precision + Vinter (cid:124) (cid:123)(cid:122) (cid:125) Intervention Volume Strim (cid:124)(cid:123)(cid:122)(cid:125) Trimming Skill . (3) We next describe each term in Eq. 3 and explain how it contributes to the overall improvement: Selection Precision Aprec. This term represents the systems capacity for global evaluation. Mathematically defined as Covπl (α(y), R(y) Jtrim(y)), it measures the alignment between the discriminators acceptance probability and the trajectorys relative quality. positive Aprec implies the discriminator successfully distinguishes which trajectories are worth preserving while exposing chains requiring correction. (e.g., those containing hallucinations or redundant steps) to the refinement process. By treating the entire interaction trace, such as reasoning, decisionto-accept, and decision-to-cut, as single unified trajectory, GRPO naturally maximizes this covariance without requiring separate supervision signals for the Meta-Refiner. Trimming Skill Strim. This term quantifies the effectiveness of the cut-and-regenerate mechanism. Defined as Cov(πh(ky), Gk(y)), it measures the correlation between the selected cut-point and the expected gain Gk(y) from regenerating at that specific step. Therefore, positive Strim indicates that the Trimmer precisely locates the specific low-quality search action that caused the reasoning collapse, such as failed search query or logic error, where the trajectory first deviated. This behavior is reinforced by propagating the outcome reward back to the specific cut-point selection k, encouraging the agent to target pivotal moments of failure. Intervention Volume Vinter. Defined as 1 E[α(y)], this term represents the volume of trajectories subjected to correction. It acts as multiplier in the Eq. 3. Even highly skilled trimmer (Strim > 0) contributes little if the discriminator is overly conservative (accepting flawed answers, Vinter 0). Conversely, if the discriminator flags valid answers (Vinter 1) while the trimmer is unskilled, the computational budget is wasted. The system must find balance between exploration and exploitation, ensuring that it neither overlooks errors nor wastes resources. The joint optimization seeks an equilibrium where Vinter is sufficiently large to correct errors but constrained enough to preserve sample efficiency. Under joint optimization with meta-refiner, if the agent accepts low-quality trajectory, the resulting low group-relative advantage penalizes the discriminator, directly driving Aprec upward. Summary of Success Conditions. Unlike standard RAG or Rejection Sampling, which rely solely on the actor policys generation probability, Search-R2 achieves net positive gain (J > 0) for each rollout if and only if three conditions are met simultaneously. Formally, these correspond to Aprec > 0, Strim > 0, and calibrated Vinter that exposes sufficient samples for refinement without suppressing high-quality outputs. Furthermore, the Meta-Refiner supports iterative execution within Nmax, where the posterior q(x) from iteration serves as the base policy for + 1. The conditions for improvement remain valid in recursive settings. 2026-02-"
        },
        {
            "title": "4 Formalization",
            "content": "In this section, we present theoretical framework for analyzing the mechanisms that drive the performance improvements of Search-R2. While the previous section detailed the algorithmic implementation of the ActorRefiner collaboration, this section aims to mathematically quantify the specific contributions of the discrimination and refinement phases. We formalize the collaborative process as smoothed mixture policy and derive decomposition of the expected reward gain. summary of the mathematical notation is provided in Appendix 6. 4.1 Performance Analysis Our primary theoretical objective is to quantify the performance advantage of the Meta-Refiner over the base actor policy. We analyze the expected performance gain, = Jmeta Jbase, where Jbase = Eyπl [R(y)] represents the standard actors performance, and Jmeta = Eyq[R(y)] represents the performance under the Meta-Refiner distribution q. Analyzing this difference is crucial because it allows us to mathematically disentangle two sources of improvement, namely the discriminative ability to identify poor samples and the trimming ability to correct them. Proposition 4.1 (Performance Decomposition of Meta-Refiner). Let the induced trajectory distribution q(y x) of the Meta-Refiner be formalized as mixture policy: q(y x) = πl(y x)α(y) + (cid:90) ˆy πl( ˆy x)(1 α( ˆy))T(y x, ˆy) ˆy, (4) where πl is the base policy, α(y) [0, 1] is the acceptance probability, and T(y x, ˆy) is the normalized transition distribution of the trimmer for cutting and regenerating rejected sample ˆy. Note that is self-normalized (see Proof in Appendix B). The expected reward Jmeta decomposes relative to the base performance Jbase as: Jmeta = Jbase + Covπl (a(y), R(y) Jtrim(y)) (cid:125) (cid:124) (cid:123)(cid:122) Selection Precision + (1 Zacc)( Jtrim Jbase) (cid:123)(cid:122) (cid:125) Correction Volume Gain (cid:124) . (5) Here, Cov(X, Y) = E[XY] E[X]E[Y] denotes the covariance. Jtrim( ˆy) = reward after correcting ˆy, Jtrim = Eπl [Jtrim( ˆy)], and Zacc = Eπl [a(y)] is the global acceptance rate. yT( ˆy)[R(y)] is the expected This derivation characterizes as smoothed mixture policy. The performance gain is driven by the discriminators precision in identifying low-quality samples (Selection Precision) and the trimmers ability to improve those samples (Correction Volume Gain). 4.2 Decomposing the Correction Volume Gain We further analyze the term Jtrim = Jtrim Jbase, which represents the performance improvement provided by the trimming strategy. We aim to decompose this gain into the baseline gain and the attribution ability of the trimmer. Preliminaries. Let ˆy be draft sequence of length from πl rejected by the discriminator. We define the set of possible cut-points as = {1, . . . , T}. Let πh(k ˆy) be the trimmer policy (probability of cutting at index + 1) and let Vπl ( ˆy1:k) be the value of regenerating the suffix from k. Proposition 4.2 (Decomposition of Trimming Strategy). Let Gk( ˆy) = Vπl ( ˆy1:k) R( ˆy) denote the regeneration gain at step k. The total correction gain Jtrim decomposes into covariance term representing the agents skill and mean term: Jtrim = Cov ˆy(πh(k ˆy), Gk( ˆy)) + G( ˆy) (cid:124) (cid:123)(cid:122) (cid:125) Baseline Gain , (6) k=1 (cid:124) where G( ˆy) = (cid:123)(cid:122) Trimming Skill E[πh(k ˆy)]E[Gk( ˆy)] denotes the baseline gain (see proof in Appendix C). (cid:125) This formulation isolates two drivers of performance: Trimming Skill: positive covariance indicates that πh concentrates probability mass on cut-points + 1 where the regeneration gain Gk is highest. This measures the agents ability to identify the \"root cause\" of bad generation. positive covariance implies that the trimmer possesses the capacity for concentrating probability mass on the critical turning points that yield the greatest regeneration gain (Gk) rather than performing random trimming. Baseline Gain: In high-dimensional reasoning tasks, arbitrarily truncating and regenerating trajectory rarely improves the outcome (i.e., E[Gk( ˆy)] 0 for random k). Consequently, 0, implying that maximizing the correction gain Jtrim relies almost entirely on the trimmers skill in selecting precise cut-points. 6 Table 2: The main results on seven datasets. / represents in-domain/out-of-domain datasets. All baselines except Search-R1 are conducted on the Qwen2.5-7B model. The best and second best performances are set as bold and underlined, respectively. 2026-02-04 Methods General QA Multi-Hop QA NQ TriviaQA PopQA HotpotQA 2WikiMultiHopQA Musique Bamboogle Average 13.4 Direct Inference 4.8 CoT 22.4 IRCoT 15.1 Search-o1 34.9 RAG 31.8 SFT 29.7 R1-base 27.0 R1-instruct 36.0 Rejection Sampling 39.5 Search-R1(Qwen2.5-7B) Search-R1(Qwen3-8B) 44.0 Search-R1(Qwen2.5-32B) 47.6 39.9 Search-R2(Qwen2.5-7B) 47.7 Search-R2(Qwen3-8B) Search-R2(Qwen2.5-32B) 50. 40.8 18.5 47.8 44.3 58.5 35.4 53.9 53.7 59.2 56.0 63.1 68.0 65.9 67.6 70.9 14.0 5.4 30.1 13.1 39.2 12.1 20.2 19.9 38.0 38.8 41.8 47.0 41.0 46.6 50.1 18.3 9.2 13.3 18.7 29.9 21.7 24.2 23.7 33.1 32.6 37.2 43.3 39.0 41.2 49.9 25.0 11.1 14.9 17.6 23.5 25.9 27.3 29.2 29.6 29.7 35.5 46.2 35.8 40.5 51.7 3.1 2.2 7.2 5.8 5.8 6.6 8.3 7.2 12.3 12.5 15.7 22.1 15.1 17.2 25.4 12.0 23.2 22.4 29.6 20.8 11.2 29.6 29.3 35.5 36.0 43.0 45.0 46.2 51.2 56. 18.1 10.6 23.9 20.6 30.4 20.7 27.6 27.1 34.8 35.0 40.0 45.6 40.4 44.6 50.8 Table 3: Ablation results for Search-R2 on general and multi-hop question answering. Method General QA Multi-Hop QA Average Qwen2.5-7B Search-R1 Search-R1 + Meta-Refiner Search-R1 + Meta-Refiner + Process Reward Search-R2 (Full Version) Qwen3-8B Search-R1 Search-R1 + Meta-Refiner Search-R1 + Meta-Refiner + Process Reward Search-R2 (Full Version) Qwen2.5-32B-Instruct Search-R1 Search-R1 + Meta-Refiner Search-R1 + Meta-Refiner + Process Reward Search-R2 (Full Version) 41.7 45.3 45.6 46.5 46.5 49.4 49.9 50.8 51.5 54.2 54.3 55.5 26.1 30.4 31.6 32.4 31.4 35.3 36.1 36. 37.8 42.7 43.3 44.5 35.0 38.9 39.6 40.4 40.0 43.4 44.0 44.6 45.6 49.3 49.5 50."
        },
        {
            "title": "5 Experiments",
            "content": "5.1 Experiment Setup Datasets: We evaluate search-integrated reasoning methods on two categories of datasets. For general question answering, we use NQ (Kwiatkowski et al., 2019), TriviaQA (Joshi et al., 2017), and PopQA (Mallen et al., 2022). For multi-hop question answering, we use HotpotQA (Yang et al., 2018), 2WikiMultiHopQA (Ho et al., 2020), Musique (Trivedi et al., 2022), and Bamboogle (Press et al., 2023). We train on the union of the NQ and HotpotQA training splits. Evaluation is performed on the validation or test splits of all seven datasets, which allows us to measure in-domain performance on the training distributions as well as out-of-domain generalization to held-out datasets. Methods: We compare Search-R2 against three baseline families and strong reference model. (i) Inference without retrieval: direct inference and Chain-of-Thought (CoT) reasoning (Wei et al., 2022). (ii) Inference with retrieval: Retrieval-Augmented Generation (RAG) (Lewis et al., 2020), IRCoT (Trivedi et al., 2023), and Search-o1 (Li et al., 2025). (iii) Fine-tuning based methods: supervised fine-tuning (SFT) (Chung et al., 2024), RL-based fine-tuning without search (R1) (Guo et al., 2025), and rejection sampling with search engine (Ahn et al., 2024). (iv) Reference: Search-R1 (Jin et al., 2025), the backbone of our approach. We run experiments on three model backbones spanning multiple generations and scales, namely Qwen2.5-32B, Qwen2.5-7B, and Qwen3-8B (Yang et al., 2024; 2025). Retriever: We use E5 (Wang et al., 2022) as the retriever and the 2018 Wikipedia dump (Karpukhin et al., 2020) as the knowledge source. For fairness, we directly utilize the available index file provided by (Jin et al., 2025) and set the number of retrieved passages to 3. 7 2026-02Table 4: The hyperparameter sensitivity experiment results with increasing maximum revision times (from 1 to 4) for each initial rollout trajectory. We conduct these experiments on the Qwen2.5-32B-Instruct model. The best performance is set as bold. Max Revision NQ TriviaQA PopQA HotpotQA 2WikiMultiHopQA Musique Bamboogle Average 1 2 3 4 50.8 50.9 51.4 51.6 69.4 71.0 71.2 71.2 49.0 50.6 50.4 50. 47.6 48.7 49.3 49.3 49.4 50.7 51.4 51.6 24.2 25.5 25.7 26.0 54.4 54.4 54.4 55.6 49.3 50.2 50.6 50.9 Implementation Details: To ensure consistency with prior work (Jin et al., 2025), we use Exact Match (EM) as the evaluation metric and train all models with GRPO (Shao et al., 2024) for 300 steps. At each step, 512 prompts are randomly sampled, and = 5 rollouts are generated for each prompt. Our training framework is based on the verl framework (Sheng et al., 2025) and sets the max assistant turns as 4. The max revision number per rollout is set as 1 by default. We use learning rate of 1e-6 with warmup ratio of 0.285. We provide more details in Appendix E. 5.2 Performance Comparison Table 2 details the performance of Search-R2 against strong baselines on seven benchmarks. We observe that Search-R2 establishes consistent performance lead. Notably, Search-R2 built on the Qwen2.5-7B backbone achieves 16.1% EM gain over the Search-R1 rejection-sampling baseline, even when Search-R1 employs the stronger Qwen3-8B backbone. This confirms that the Actor-Refiner framework effectively compensates for reduced model scale by optimizing reasoning quality. When scaling the backbone from 7B to 32B, we observe further performance gain, with average EM rising from 40.4 to 50.8. This consistent gain under model size scaling further highlights the effectiveness of our approach. Moreover, the performance gains are more pronounced on complex reasoning tasks. For instance, Search-R2 achieves 5.5-point improvement on 2WikiMultiHopQA and an 11.4-point improvement on Bamboogle (+25.3% relative gain). These tasks typically require multi-step retrieval and reasoning, where early mistakes and noisy intermediate search results can cascade and derail the remaining trajectory. By using the Meta-Refiner to detect deviations and sample high-quality traces, Search-R2 mitigates such error propagation and yields larger gains across different benchmarks. Finally, to further verify that these gains stem from targeted refinement rather than additional computation, we compare Search-R2 against the Search-R1 baseline trained using doubled rollout budget (n = 10). As reported in Appendix G, Search-R2 (n = 5, max revision = 1) still performs better, indicating that surgical correction is substantially more sample-efficient than brute-force sampling. 5.3 Ablation Study To rigorously disentangle the sources of improvement in Search-R2, we perform component-wise analysis by sequentially integrating the Meta-Refiner, Process Reward, and Joint Optimization modules into the Search-R1 baseline. For the intermediate configurations (Search-R1 + Meta-Refiner and + Process Reward), we optimize the policy solely on reasoning traces, excluding intervention refinement from the Meta-Refiner. As can be seen in Table 3, each module contributes positively to overall performance. Firstly, the integration of the Meta-Refiner drives the largest performance leap (+11.1% on Qwen2.5-7B), suggesting that the Meta-Refiner acts as crucial scaffold for reasoning coherence. Secondly, integrating the process reward yields consistent performance gains by explicitly valuing high-information-density retrieval. It guides the Actor under sparse feedback in complex reasoning settings. Finally, the full Search-R2 setup with joint optimization achieves the highest accuracy. These results support our strategy: unlike static methods, it enables the Actor and Meta-Refiner to co-adapt, allowing the policy to precisely localize errors and internalize the cut-and-regenerate mechanism for higher sample efficiency. Limited by the space, further details can be found in Appendix  (Table 8)  . 5.4 Sensitivity to the Maximum Revision Limit We evaluate sensitivity to the maximum revision limit by varying the max revision value from 1 to 4 using Qwen2.532B as the backbone. In these experiments, we disable process reward modeling and joint optimization to focus on the effect of allowing additional revisions. As shown in Table 4, increasing max revision yields consistent gains. Notably, max revision = 4 reaches an average score of 50.9, essentially matching the fully optimized Search-R2 with single revision (50.8). This comparison highlights an efficiency trade-off that our proposed joint optimization strategy can successfully distill the benefits of larger revision budget into more efficient policy that achieves comparable accuracy with one correction step. We also observe rapidly diminishing gains as the revision limit increases. The absolute EM gain drops from 0.9 points when increasing revisions from 1 to 2 to 0.3 points from 3 to 4. This pattern suggests that early revisions primarily correct errors that are relatively easy to fix, such as retrieval noise or shallow hallucinations, whereas 8 2026-02-04 Figure 3: The total rollout numbers after revision (initial rollout numbers + refined rollout numbers) corresponding to different max revision time settings. the remaining failures are less responsive to repeated refinement. Figure 3 corroborates this trend, showing that most trajectories trigger at most one revision. Given higher max revision limits, harder cases rarely activate further refinement. Consequently, we set max revision = 1 as the default operating point, which captures most of the benefit at low revision cost. Table 5: Average time cost for each training step (seconds/step). Model Search-R1 Search-R2 Qwen2.5-7B Qwen3-8B Qwen2.5-32B 177.8 141.5 458. 193.2 147.3 469.5 Relative Change + 8.66% + 4.10% + 2.43% EM (%) Time (%) 1.78 2.80 4.69 5.5 Efficiency Analysis We now examine whether the Search-R2 pipeline introduces substantial computational overhead in practice. Surprisingly, Table 5 shows that Search-R2 increases training time by only 5.06% on average relative to the Search-R1 baseline. This modest overhead is largely due to the cut-and-regenerate mechanism, which preserves valid prefixes rather than discarding entire trajectories, thereby reducing wasted computation. Moreover, the relative overhead decreases with model scale and drops to 2.43% for the 32B model, suggesting that the marginal refinement cost becomes less significant as distributed training overhead grows. At inference time, Search-R2 introduces no additional latency because the Meta-Refiner is decoupled at deployment. To quantify training cost-effectiveness, we report the ratio EM(%)/Time(%), which measures accuracy improvement per unit increase in training time. As shown in Table 2, this ratio exceeds 1 for all models, indicating that accuracy gains consistently outpace the added compute. Moreover, the ratio further improves with scale, increasing from 1.78 at 7B to 4.69 at 32B, which suggests that Search-R2 becomes more cost-effective for larger backbones. 5.6 Trajectory Quality Comparison To better understand trajectory quality, we compare Search-R2 against Search-R1 using GPT-5.1 as an automated judge. The evaluation covers six dimensions: evidence groundedness, information density, non-redundancy efficiency, query timing quality, trajectory coherence, and uncertainty handling. For each of the seven test datasets, we randomly sample 100 paired trajectories, evaluating Search-R1 and Search-R2 on the same prompt, for total of 700 pairs. The judge assigns each trajectory an independent three-level score, with 0 indicating poor quality, 1 acceptable, and 2 strong. We then compare the paired scores and record win when Search-R2 scores higher, fail when it scores lower, and tie otherwise1. As shown in Figure 4, Search-R2 outperforms Search-R1 across all dimensions, indicating more grounded, efficient, and coherent search and reasoning behavior. Detailed rubrics, full results, and evaluation prompts are provided in Appendix I. 1We omit ties in Figure 4 to improve readability. 9 2026-02Figure 4: Average counts of Search-R2 winning and failing against Search-R1 across all seven datasets for each rubric. 6 Conclusions In this work, we introduced Search-R2, search-integrated reasoning framework designed to mitigate the LLM fragility when facing retrieval noise. Experiments show that while standard approaches like Search-R1 are susceptible to error propagation loops caused by misleading initial context, Search-R2s Actor-Refiner collaboration with joint optimization effectively interrupts these failures. By employing dynamic cut-and-regenerate mechanism, Search-R2 enables models to correct reasoning trajectories in real-time. These findings highlight the critical importance of integrating active refinement into search-integrated reasoning, offering path toward more reliable agent behavior."
        },
        {
            "title": "References",
            "content": "Janice Ahn, Rishu Verma, Renze Lou, Di Liu, Rui Zhang, and Wenpeng Yin. Large language models for mathematical reasoning: Progresses and challenges. In Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics: Student Research Workshop, pp. 225237, 2024. Mingyang Chen, Linzhuang Sun, Tianpeng Li, Haoze Sun, Yijie Zhou, Chenzheng Zhu, Haofen Wang, Jeff Pan, Wen Zhang, Huajun Chen, et al. Learning to reason with search for llms via reinforcement learning. arXiv preprint arXiv:2503.19470, 2025. Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. Journal of Machine Learning Research, 25(70):153, 2024. Rati Devidze, Parameswaran Kamalaruban, and Adish Singla. Exploration-guided reward shaping for reinforcement learning under sparse rewards. Advances in Neural Information Processing Systems, 35:58295842, 2022. Jiaxuan Gao, Wei Fu, Minyang Xie, Shusheng Xu, Chuyi He, Zhiyu Mei, Banghua Zhu, and Yi Wu. Beyond ten turns: Unlocking long-horizon agentic search with large-scale asynchronous rl. arXiv preprint arXiv:2508.07976, 2025. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. Constructing multi-hop qa dataset In Proceedings of the 28th International Conference on for comprehensive evaluation of reasoning steps. Computational Linguistics, pp. 66096625, 2020. Minda Hu, Licheng Zong, Hongru Wang, Jingyan Zhou, Jingjing Li, Yichen Gao, Kam-Fai Wong, Yu Li, and Irwin King. SeRTS: Self-rewarding tree search for biomedical retrieval-augmented generation. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (eds.), Findings of the Association for Computational Linguistics: EMNLP 2024, pp. 13211335, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-emnlp.71. URL https://aclanthology.org/2024.findings-emnlp. 71/. 10 2026-02-04 Minda Hu, Tianqing Fang, Jianshu Zhang, Jun-Yu Ma, Zhisong Zhang, Jingyan Zhou, Hongming Zhang, Haitao Mi, Dong Yu, and Irwin King. WebCoT: Enhancing web agent reasoning by reconstructing chain-of-thought in reflection, branching, and rollback. In Christos Christodoulopoulos, Tanmoy Chakraborty, Carolyn Rose, and Violet Peng (eds.), Findings of the Association for Computational Linguistics: EMNLP 2025, pp. 51555173, Suzhou, China, November 2025. Association for Computational Linguistics. ISBN 979-8-89176-335-7. doi: 10.18653/v1/2025.findings-emnlp.276. URL https://aclanthology.org/2025.findings-emnlp. 276/. Bowen Jin, Hansi Zeng, Zhenrui Yue, Jinsung Yoon, Sercan Arik, Dong Wang, Hamed Zamani, and Jiawei Han. Search-r1: Training llms to reason and leverage search engines with reinforcement learning. arXiv preprint arXiv:2503.09516, 2025. Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. Triviaqa: large scale distantly supervised challenge dataset for reading comprehension. arXiv preprint arXiv:1705.03551, 2017. Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 67696781, 2020. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. Natural questions: benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:453466, 2019. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in neural information processing systems, 33:94599474, 2020. Xiaoxi Li, Guanting Dong, Jiajie Jin, Yuyao Zhang, Yujia Zhou, Yutao Zhu, Peitian Zhang, and Zhicheng Dou. Search-o1: Agentic search-enhanced large reasoning models. arXiv preprint arXiv:2501.05366, 2025. Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Hannaneh Hajishirzi, and Daniel Khashabi. When not to trust language models: Investigating effectiveness and limitations of parametric and non-parametric memories. arXiv preprint arXiv:2212.10511, 7, 2022. Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah Smith, and Mike Lewis. Measuring and narrowing the compositionality gap in language models. In Findings of the Association for Computational Linguistics: EMNLP 2023, pp. 56875711, 2023. Hongjin Qian and Zheng Liu. Scent of knowledge: Optimizing search-enhanced reasoning with information foraging. arXiv preprint arXiv:2505.09316, 2025. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. In Proceedings of the Twentieth European Conference on Computer Systems, pp. 12791297, 2025. Huatong Song, Jinhao Jiang, Yingqian Min, Jie Chen, Zhipeng Chen, Wayne Xin Zhao, Lei Fang, and Ji-Rong Wen. R1-searcher: Incentivizing the search capability in llms via reinforcement learning. arXiv preprint arXiv:2503.05592, 2025. Shuang Sun, Huatong Song, Yuhao Wang, Ruiyang Ren, Jinhao Jiang, Junjie Zhang, Fei Bai, Jia Deng, Wayne Xin Zhao, Zheng Liu, et al. Simpledeepsearcher: Deep information seeking via web-powered reasoning trajectory synthesis. arXiv preprint arXiv:2505.16834, 2025. Tongyi DeepResearch Team, Baixuan Li, Bo Zhang, Dingchu Zhang, Fei Huang, Guangyu Li, Guoxin Chen, Huifeng Yin, Jialong Wu, Jingren Zhou, et al. Tongyi deepresearch technical report. arXiv preprint arXiv:2510.24701, 2025. Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. Musique: Multihop questions via single-hop question composition. Transactions of the Association for Computational Linguistics, 10:539554, 2022. Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. Interleaving retrieval with chain-ofthought reasoning for knowledge-intensive multi-step questions. In Proceedings of the 61st annual meeting of the association for computational linguistics (volume 1: long papers), pp. 1001410037, 2023. 11 2026-02-04 Hongru Wang, Cheng Qian, Wanjun Zhong, Xiusi Chen, Jiahao Qiu, Shijue Huang, Bowen Jin, Mengdi Wang, Kam-Fai Wong, and Heng Ji. Acting less is reasoning more! teaching model to act efficiently, 2025. URL https://arxiv.org/abs/2504.14870. Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, and Furu Wei. Text embeddings by weakly-supervised contrastive pre-training. arXiv preprint arXiv:2212.03533, 2022. Ruiyi Wang and Prithviraj Ammanabrolu. practitioners guide to multi-turn agentic reinforcement learning. arXiv preprint arXiv:2510.01132, 2025. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chainof-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. Tongyu Wen, Guanting Dong, and Zhicheng Dou. Smartsearch: Process reward-guided query refinement for search agents. arXiv preprint arXiv:2601.04888, 2026. Jinming Wu, Zihao Deng, Wei Li, Yiding Liu, Bo You, Bo Li, Zejun Ma, and Ziwei Liu. Mmsearch-r1: Incentivizing lmms to search. arXiv preprint arXiv:2506.20670, 2025. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2024. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher Manning. Hotpotqa: dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018 conference on empirical methods in natural language processing, pp. 23692380, 2018. Siliang Zeng, Quan Wei, William Brown, Oana Frunza, Yuriy Nevmyvaka, Yang Katie Zhao, and Mingyi Hong. Reinforcing multi-turn reasoning in llm agents via turn-level credit assignment. In ICML 2025 Workshop on Computer Use Agents, 2025. Kaiwen Zha, Zhengqi Gao, Maohao Shen, Zhang-Wei Hong, Duane Boning, and Dina Katabi. Rl tango: Reinforcing generator and verifier together for language reasoning. arXiv preprint arXiv:2505.15034, 2025. Wenlin Zhang, Xiangyang Li, Kuicai Dong, Yichao Wang, Pengyue Jia, Xiaopeng Li, Yingyi Zhang, Derong Xu, Zhaocheng Du, Huifeng Guo, et al. Process vs. outcome reward: Which is better for agentic rag reinforcement learning. arXiv preprint arXiv:2505.14069, 2025a. Zijing Zhang, Ziyang Chen, Mingxiao Li, Zhaopeng Tu, and Xiaolong Li. Rlvmr: Reinforcement learning with verifiable meta-reasoning rewards for robust long-horizon agents. arXiv preprint arXiv:2507.22844, 2025b. Shuyan Zhou, Frank Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, et al. Webarena: realistic web environment for building autonomous agents. In The Twelfth International Conference on Learning Representations. Jiaru Zou, Ling Yang, Jingwen Gu, Jiahao Qiu, Ke Shen, Jingrui He, and Mengdi Wang. Reasonflux-prm: Trajectory-aware prms for long chain-of-thought reasoning in llms. arXiv preprint arXiv:2506.18896, 2025. 2026-02-"
        },
        {
            "title": "A Notation Table",
            "content": "Table 6 summarizes the mathematical notations and symbols used throughout the formalization and analysis of the Search-R2 framework. Table 6: Summary of Notations and Symbols Symbol Description Policies and Models πl(yx) πd( ˆyx) πh(k ˆy, x) q(yx) Λ The Base Policy (Actor) responsible for generating reasoning trajectories and search queries. The Discriminator (part of Meta-Refiner) that estimates the probability of trajectorys global coherence. The Trimmer (part of Meta-Refiner) that identifies the specific step (cut-point) where an error occurred. The smoothed mixture policy induced by the interaction between the Actor and the Meta-Refiner realized by Algorithm 1. The external search engine used for retrieval. ˆy apred agold ˆy1:k Trajectory and Search The input context or question. The initial reasoning trajectory generated by the Actor πl. The final trajectory output after potential refinement from q(yx). The predicted final answer extracted from the trajectory. The ground truth answer. The index of step in the trajectory (specifically used as the cut-point). The valid prefix of trajectory ˆy up to step + 1. set of retrieved chunks {c1, . . . , cM}. Rewards and Optimization R(y) routcome(y) rprocess(y) LGRPO(θ) ˆAt The total hybrid reward described in Section 3.3, combining outcome and process signals. The outcome reward (Binary Exact Match) indicating if apred = agold. The process reward quantifying the information density of retrieved evidence. The objective function for Group Relative Policy Optimization. The advantage estimate computed via group normalization. Theoretical Analysis Cov(X, Y) The covariance between variables and Y, defined as E[XY] E[X]E[Y]. α( ˆyx) τ Zacc Jbase Jmeta Aprec Strim Vinter Jtrim( ˆy) Gk( ˆy) Vπl ( ˆy1:k) The Discriminator πds acceptance probability of trajectory, defined as P(πd( ˆyx) τ). The predefined threshold for the Discriminator πd to accept trajectory. The global acceptance rate, defined as Eπl [α(y)]. The expected performance of the base policy: Eyπl [R(y)]. The expected performance of the meta-refiner policy: Eyq[R(y)]. The net performance gain: Jmeta Jbase. Selection Precision: Covariance measuring the Discriminators ability to identify low-quality samples. Trimming Skill: Covariance measuring the Trimmers ability to locate the root cause of errors. Intervention Volume: The probability mass allocated to the trimming process (1 Zacc). The expected reward after correcting specific rejected trajectory ˆy. The regeneration gain at step k: Vπl ( ˆy1:k) R( ˆy). The value of regenerating the suffix starting from step using the base policy πl. 13 Proof for Performance Decomposition of Meta-Refiner Proof. 1. Normalization Check. We first verify that q(yx) integrates to 1. (cid:90) (cid:90) q(yx)dy = πl(y)α(y) dy + (cid:90) (cid:20)(cid:90) = Eπl [α(y)] + (cid:90) πl( ˆy)(1 α( ˆy)) = Zacc + Eπl [1 α( ˆy)] = Zacc + (1 Zacc) = 1. πl( ˆy)(1 α( ˆy))T(y ˆy) ˆy (cid:21) (cid:20)(cid:90) T(y ˆy) dy ˆy (cid:125) (cid:124) (cid:123)(cid:122) =1 (cid:21) dy 2026-02-04 (7) 2. Expected Reward Derivation. The expected reward Jmeta is the integral of R(y) over the mixture components: Jmeta = (cid:90) (cid:124) R(y)πl(y)α(y) dy (cid:125) (cid:123)(cid:122) Term (Accepted) + (cid:90) (cid:124) (cid:20)(cid:90) R(y) πl( ˆy) α( ˆy)T(y ˆy) ˆy (cid:123)(cid:122) Term (Rejected) (cid:21) dy , (cid:125) where α( ˆy) = 1 a( ˆy). Analyzing Term A: Using the covariance identity E[XY] = E[X]E[Y] + Cov(X, Y): = Eyπl [R(y)α(y)] = JbaseZacc + Covπl (α, R). Analyzing Term B: By Fubinis Theorem, we swap the order of integration: (cid:90) = πl( ˆy)(1 α( ˆy)) (cid:20)(cid:90) R(y)T(y ˆy) dy (cid:21) ˆy (cid:90) = = ˆyπl [(1 α( ˆy))Jtrim( ˆy)] . Let Jtrim = Eπl [Jtrim( ˆy)]. Applying the covariance identity again: πl( ˆy)(1 α( ˆy))Jtrim( ˆy) ˆy = (1 Zacc) Jtrim Covπl (α, Jtrim). Synthesis: Combining and B, and grouping the covariance terms: Jmeta = [JbaseZacc + (1 Zacc) Jtrim] + (cid:2)Covπl (α, R) Covπl (α, Jtrim)(cid:3) = [JbaseZacc + (1 Zacc) Jtrim] + Covπl (α, Jtrim). Subtracting Jbase from both sides yields the final gain: = Covπl (α, Jtrim) + (1 Zacc)( Jtrim Jbase)."
        },
        {
            "title": "C Proof for Decomposition of Trimming Strategy",
            "content": "Proof. The total expected gain is the difference between the expected return after trimming and the baseline: (cid:35) πh(k ˆy)Vπl ( ˆy1:k) ˆy[R( ˆy)] πh(k ˆy) (Vπl ( ˆy1:k) R( ˆy)) (cid:35) (cid:34) k=1 (cid:34) k=1 ˆy [πh(k ˆy)Gk( ˆy)] . Jtrim = ˆy = ˆy = k= (8) (9) (10) (11) Applying the covariance identity E[XY] = Cov(X, Y) + E[X]E[Y] to each term in the summation yields the proposition. 2026-02-"
        },
        {
            "title": "D Drivers of Performance Gain",
            "content": "Building upon Propositions 4.1 and 4.2, we decompose the total system improvement, J, into three governing factors. These components isolate the specific contributions of the discriminators judgment, the Meta-Refiners localization capability, and the overall frequency of intervention. Definition D.1 (Selection Precision). Let Aprec quantify the covariance between the acceptance probability α(y) and the samples relative advantage (current reward minus potential correction value): Aprec Covπl (α(y), R(y) Jtrim(y)) . positive Aprec indicates that the discriminator functions as an effective filter, preferentially preserving samples where the existing reward R(y) outweighs the expected value of correction Jtrim(y). Definition D.2 (Trimming Skill). Let Strim quantify the alignment between the cut-point policy πh and the regeneration gain Gk( ˆy) across all possible cut-points k: (12) Strim k=1 Covπl (πh(k ˆy), Gk( ˆy)) . (13) positive Strim implies the Meta-Refiner correctly identifies cut-points that yield higher regeneration gains. Definition D.3 (Intervention Volume). Let Vinter represent the total probability mass allocated to the trimming process (the rejection rate): This term dictates the magnitude of the opportunity space available for the Trimmer to act. Vinter 1 Zacc = Eπl [1 α(y)]. Substituting these definitions into the total gain equation yields the following decomposition: = Aprec + Vinter (Strim + ). (14) (15) We leverage GRPO with meta-actions to jointly optimize the Actor and the Meta-Refiner. By treating each trajectory yi as an augmented execution trace, comprising both the reasoning tokens from πl and the meta-actions sampled from the Discriminator πd( ˆy) and Trimmer πh(k ˆy). GRPO inherently maximizes J. This formulation ensures that the policy gradient updates align with the maximization of Aprec and Strim."
        },
        {
            "title": "E Supplementary Implementation Details",
            "content": "Hardware All experiments were conducted on multiple 8-node GPU clusters. Each node features dual-socket AMD EPYC 9K84 processors, providing total of 192 physical cores and 384 threads per node, organized into two NUMA nodes. Storage infrastructure includes 480 GB SATA SSD for the OS and environment, alongside two enterprise-grade 7.68 TB NVMe SSDs for high-throughput local data caching. Nodes are linked via high-speed interconnect and share distributed file system for dataset storage and checkpoint synchronization. Configurations The model is trained on unified search-integrated reasoning dataset stored in Parquet format. Data & Rollout: We set the maximum prompt and response lengths to 4096 and 3000 tokens, respectively. To prevent information loss, truncation is disabled; prompts exceeding the limit are filtered out. We utilize SGLang as the rollout engine to facilitate efficient multi-turn generation with tool calls, maintaining the raw chat format. Each prompt samples = 5 rollout trajectories per GRPO step, with maximum of 4 assistant turns per trajectory. The context length during rollout is capped at 15,000 tokens to accommodate interleaved reasoning and retrieved evidence. For validation, we employ greedy decoding (sampling disabled). Optimization: The Actor is trained via PPO-style updates using GRPO advantages. We utilize learning rate of 1e-6 with warmup ratio of 0.285. The global PPO mini-batch size is 512, with per-GPU micro-batch size of 4. To stabilize training, we apply low-variance KL penalty (coefficient 0.001) rather than incorporating it into the reward; entropy regularization is disabled. Training utilizes Fully Sharded Data Parallel (FSDP) with full state offloading. Tensor model parallelism is set to 8 for the 32B model and 2 for the 7B/8B models. Meta-Refiner: The Meta-Refiner functions as an internal agent sharing weights with the Actor but utilizing distinct prompts. It is trained jointly with the Actor and remains active during rollout, performing at most one revision per trajectory. Intervention decisions are determined by comparing log-probabilities of candidate actions (revision vs. no-revision); revision is triggered only if its log-probability exceeds that of the no-revision decision (margin 0.0). Resource Links We provide the necessary resource links of models, retrievers, and software, to help reproduce our implementation and experiments as follows: Models: Qwen2.5-32B-Instruct 2, Qwen2.5-7B 3, Qwen3-8B 4, 2https://huggingface.co/Qwen/Qwen2.5-32B-Instruct 3https://huggingface.co/Qwen/Qwen2.5-7B 4https://huggingface.co/Qwen/Qwen3-8B 15 2026-02Figure 5: Detailed training dynamics of Search-R2 with different base models across all seven datasets. and DeepSeek-R1-Distill-Qwen-7B 5; Retriever: E5 6, 2018 Wikipedia dump 7, and index file 8; Softwares: verl 9, FSDP 10, and SGlang 11."
        },
        {
            "title": "F Training Dynamics",
            "content": "To investigate the training dynamics of our agentic RL framework, Figure 5 visualizes EM scores across the seven experimental datasets, plotted from 0 to 300 steps at 50-step intervals. We observe consistent trends across all three models and datasets, with performance converging as training approaches 300 steps. Extending training beyond this point yields negligible performance gains and increases the risk of model collapse due to instabilities such as traininference mismatch and automatic mixed-precision overflowchallenges inherent to the current RL training infrastructure. Furthermore, while performance gaps persist between models of different sizesconfirming that parameter scale remains critical factor in tool-use and reasoningSearch-R2 enables smaller models (e.g., Qwen2.5-7B and Qwen3-8B) to approach the performance of substantially larger models like Qwen2.5-32B-Instruct on tasks such as NQ and TriviaQA. This underscores the frameworks efficacy in enhancing search-integrated reasoning for compact models, facilitating their adoption in practical scenarios. Comparison against Search-R1 with Double Rollout Numbers To verify that the performance gains of Search-R2 are not merely an artifact of increased rollout volume, we trained the Search-R1 agent with doubled rollouts (n = 10, compared to the default = 5). This setting serves as proxy for naive refinement strategy where every trajectory is regenerated from scratch, in contrast to Search-R2s targeted refinement of intermediate turns. As shown in Table 7, Search-R2 (n = 5, max revision = 1) consistently outperforms Search-R1 (n = 10) throughout the training process. At the final step 300, Search-R2 achieves score of 50.8, surpassing Search-R1 by 6.28%. While increasing to 10 improves Search-R1, it fails to match the performance of Search-R2. This confirms that our gains stem from the Meta-Refiners ability to identify and correct specific flaws, rather than simple sample scaling. Furthermore, Search-R2 is significantly more efficient: while Search-R1 (n = 10) requires generating 5,120 trajectories per step, Search-R2 generates approximately 3,300 on average, as the Meta-Refiner selects only 30% of trajectories for revision. This reduction lowers the computational overhead from 803.2 seconds/step (Search-R1) to 469.5 seconds/step (Search-R2), demonstrating the efficiency of the Meta-Refiner module. 5https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B 6https://huggingface.co/intfloat/e5-base-v2 7https://huggingface.co/datasets/PeterJinGo/wiki-18-corpus 8PeterJinGo/wiki-18-e5-index 9https://github.com/volcengine/verl/tree/main 10https://docs.pytorch.org/docs/stable/fsdp.html 11https://github.com/sgl-project/sglang 16 Table 7: Performance comparison between Search-R2 (initial rollout number as 5 per prompt and max revision as 1) and Search-R1 with double rollout numbers (10 per prompt instead of default 5 per prompt). Here, Qwen2.5-32BInstruct is taken as the base model. Training Steps NQ TriviaQA PopQA HotpotQA 2WikiMultiHopQA Musique Bamboogle Average 2026-02-04 Search-R1 (n = 10) 50 100 150 200 250 300 33.9 38.3 43.8 46.6 49.0 49.7 63.3 65.3 67.6 68.0 68.3 68.6 Search-R2 (n = 5, max revision = 1) 50 100 150 200 250 63.3 66.3 68.0 68.3 68.7 70.9 34.5 40.9 46.5 49.1 49.7 50.9 38.2 40.3 45.2 47.3 49.1 49.1 38.3 41.6 46.1 48.0 48.8 50.1 38.7 41.4 43.7 44.3 45.2 45.9 39.1 42.7 44.1 45.6 48.6 49. 36.6 40.2 43.9 44.5 46.6 47.8 37.4 41.8 43.9 48.0 50.2 51.7 15.7 17.4 18.9 21.1 23.2 24.0 16.1 18.3 19.9 22.3 24.9 25.4 46.4 45.6 49.6 48.8 50.4 49.6 48.8 48.8 49.8 51.2 54.4 56. 39.0 41.2 44.7 45.8 47.4 47.8 39.7 42.9 45.5 47.5 49.3 50.8 Table 8: The detailed ablation study results of Search-R2 with different base LLMs on seven datasets. The MetaRefiner, process reward, and joint optimization modules are incorporated into the original Search-R1 framework in an incremental manner. Method NQ TriviaQA PopQA HotpotQA 2WikiMultiHopQA Musique Bamboogle Average Qwen2.5-7B Search-R1 Search-R1 + Meta-Refiner Search-R1 + Meta-Refiner + Process Reward Search-R2 (Full Version) Qwen3-8B Search-R1 Search-R1 + Meta-Refiner Search-R1 + Meta-Refiner + Process Reward Search-R2 (Full Version) Qwen2.5-32B-Instruct Search-R1 Search-R1 + Meta-Refiner Search-R1 + Meta-Refiner + Process Reward Search-R2 (Full Version) 39.5 39.3 39.6 39.9 44.0 46.2 46.7 47.7 47.6 50.8 50.1 50.9 56.0 64.4 64.9 65. 63.1 65.9 66.4 67.6 68.0 69.4 70.0 70.9 38.8 40.3 40.5 41.0 41.8 45.1 45.6 46.6 47.0 49.0 49.4 50.1 32.6 37.0 37.4 39. 37.2 40.2 40.7 41.2 43.3 47.6 47.6 49.9 29.7 34.6 34.9 35.8 35.5 40.2 40.6 40.5 46.2 49.4 49.9 51.7 12.5 12.7 14.2 15. 15.7 16.2 16.7 17.2 22.1 24.2 24.3 25.4 36.0 44.0 45.6 46.2 43.0 49.6 51.0 51.2 45.0 54.4 55.6 56.4 35.0 38.9 39.6 40. 40.0 43.4 44.0 44.6 45.6 49.3 49.5 50."
        },
        {
            "title": "H Detailed Ablation Study Results",
            "content": "As supplement to Section 5.3, we provide the detailed ablation study results on each dataset in Table 8."
        },
        {
            "title": "I Supplementary Introduction to Trajectory Quality Analysis",
            "content": "I.1 Rubric Explanation We evaluate trajectory quality using six rubric dimensions that capture complementary aspects of search-integrated reasoning beyond final answer correctness. Evidence Groundedness measures whether key claims and intermediate conclusions in the trajectory are explicitly supported by retrieved information. high score indicates that reasoning steps consistently reference or rely on evidence obtained through search, while low score reflects unsupported claims or hallucinated content. Information Density assesses the usefulness of retrieved information relative to the total search results. Trajectories with high information density primarily retrieve content that directly contributes to solving the task, whereas low scores indicate noisy, weakly relevant, or distracting retrievals. Non-Redundancy Efficiency evaluates how effectively the trajectory uses its search budget. High-scoring trajectories avoid repeated or unnecessary queries and demonstrate efficient progression toward task-relevant information, while low scores reflect redundant searches or inefficient exploration. 2026-02-04 Table 9: The trajectory quality comparison results among six rubric dimensions on seven datasets. / represents in-domain/out-of-domain datasets. All experiments are conducted on the Qwen2.5-32B-Instruct model. In each block of X/Y, indicates the pair amounts of Search-R2 outperforms Search-R1, while indicates the pair amounts of Search-R1 outperforms Search-R2. Methods General QA Multi-Hop QA NQ TriviaQA PopQA HotpotQA 2WikiMultiHopQA Musique Bamboogle Average Evidence Groundedness Information Density Non-Redundancy Efficiency Query Timing Quality Trajectory Coherence Uncertainty Handling 24/1 37/4 35/3 16/0 35/3 10/0 27/0 28/1 28/0 17/0 29/1 20/1 20/1 35/4 32/3 9/1 32/4 8/ 24/4 40/9 36/7 9/2 34/7 10/2 8/6 37/10 34/7 5/1 30/6 0/5 7/2 32/10 20/8 30/0 23/6 3/2 25/3 46/6 39/5 13/2 36/4 11/1 19.3/2.4 36.4/6.3 32.0/4.7 14.1/0.9 31.3/4.4 8.9/1.7 Query Timing Quality captures whether searches are issued at appropriate moments and whether the queries are well-formed. High scores correspond to timely searches with precise, informative queries, whereas low scores indicate poorly timed searches or vague and uninformative query formulations. Trajectory Coherence measures the global consistency of the reasoning process. coherent trajectory maintains alignment between early hypotheses, retrieved evidence, and final conclusions, while incoherent trajectories exhibit logical drift, contradictions, or premature commitment to incorrect assumptions. Uncertainty Handling evaluates how the model responds to incomplete or ambiguous information. High-scoring trajectories appropriately acknowledge uncertainty, seek additional evidence, or hedge conclusions when warranted, whereas low scores indicate overconfident conclusions unsupported by sufficient evidence. I.2 Detailed Results Complementing the analysis in Section 5.6, Table 9 presents the full trajectory quality comparison across seven datasets. Across the six evaluation rubrics, the frequency with which Search-R2 outperforms Search-R1 is significantly higher than the reverse on most datasets. This confirms that our Actor-Refiner collaboration mechanism effectively facilitates the generation of higher-quality search-integrated reasoning trajectories. I.3 Evaluation Prompt To enhance the reproducibility, we provide the prompt for trajectory quality comparison in Table 10. Pseudocode for LLM Response Rollout with Multi-Turn Search We provide the pseudocode for the standard search-integrated reasoning (original Search-R1) in Algorithm 2. Generate ˆyb until </search>, </answer>, or EOS. ˆy ˆy + ˆyb if <search> in ˆyb then Algorithm 2 LLM Response Rollout with Multi-Turn Search Require: Input x, policy πθ, search engine Λ, budget B. Ensure: Final response ˆy. ˆy , 0 1: 2: while < do 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: end while 13: return ˆy Extract query λ; Retrieve = Λ(λ) ˆy ˆy + <information>I</information> else if <answer> in ˆyb then end if + 1 return ˆy"
        },
        {
            "title": "K Local Process Reward Implementation Details",
            "content": "Local Process Reward (rprocess) quantifies the information density of the retrieved evidence, ensuring that the model is incentivized to perform efficient, non-redundant, and relevant searches. We compute the process reward using 18 2026-02-04 SYSTEM: You are STRICT evaluator for trajectory quality comparison in search -integrated reasoning. You will compare two trajectories (A and B) for the SAME question. Rules: - Do NOT use outside knowledge. Judge only from what the trajectories show. - Score EACH trajectory independently on each rubric dimension using 0/1/2: 0=poor, 1=acceptable/mixed, 2=strong. - If is slightly better than on dimension, assign higher score even if both are acceptable. - Avoid giving identical scores unless the two trajectories are truly indistinguishable on that dimension. - After scoring, choose the overall winner (A or B). Output 'Tie' ONLY if nearly identical in outcome AND process. - A/B are just labels. Return SINGLE valid JSON object only (no markdown), following the schema exactly. USER: QUESTION: {question} TRAJECTORY (JSON): {traj_a} TRAJECTORY (JSON): {traj_b} Rubric dimensions: evidence_groundedness, information_density, non_redundancy_efficiency, query_timing_quality, trajectory_coherence, uncertainty_handling Output JSON schema (MUST be valid JSON): { \"winner\": \"A\" \"B\" \"Tie\", \"confidence\": 0-100, \"scores\": { \"A\": { \"evidence_groundedness\": 012, \"information_density\": 012, \"non_redundancy_efficiency\": 012, \"query_timing_quality\": 012, \"trajectory_coherence\": 012, \"uncertainty_handling\": 01 }, \"B\": { \"evidence_groundedness\": 012, \"information_density\": 012, \"non_redundancy_efficiency\": 012, \"query_timing_quality\": 012, \"trajectory_coherence\": 012, \"uncertainty_handling\": 012 } }, \"reasons\": [ \"reason1\", \"reason2\", \"reason3\" ] } Decision procedure: 1) Score and independently on all rubric dimensions. 2) Decide winner primarily by outcome quality, if similar, decide by process quality. 3) Use 'Tie' only if truly indistinguishable. Notes: - reasons: at most 3 short bullet strings. Table 10: Prompt for trajectory quality comparison. 19 2026-02-04 SYSTEM: You are evaluating numbered collections of retrieved documents to determine their usefulness in answering given question, where each collection is enclosed within <collection_x> and </collection_x> tags containing documents that belong to that collection. Given the question and its correct answer, mark each collection as \"useful\" (yes) or \"not useful\" (no ) based on these criteria: (1) collection is useful if it contains information or clues that help identify the correct answer, even partially. (2) collection is not useful if it's completely irrelevant to the question. (3) collection is not useful if it merely duplicates information from previous collections without adding new insights, even if that information would otherwise be relevant. After evaluating all collections strictly according to these criteria and the provided information, report the total count of collections marked as useful. USER: Question: {question} Answer: {answer} {M} collections of the tool responses: <collection_1> Doc 1: {retrieved_info_1_top_1} Doc 2: {retrieved_info_1_top_2} ...... Doc k: {retrieved_info_1_top_k} </collection_1> ...... <collection_{M}> ...... Doc k: {retrieved_info_{M}_top_k} </collection_{M}> Provide your answer strictly in the format: Final Answer: number Table 11: LLM Judge Prompt for Chunk Utility ui. The system prompt enforces strict criteria for relevance and non-redundancy. density-based approach evaluated by an external LLM judge (DeepSeek-R1-Distill-Qwen-7B in our experiments). Inference is performed via the vLLM framework with greedy decoding parameters: temperature set to 0.0, top-p at 0.95, repetition penalty of 1.0, and maximum token limit of 3, 000. The evaluation procedure proceeds as follows: 1. Collection Grouping. For given reasoning trajectory y, we identify all search tool invocations. The top-k documents returned by single search query are grouped into single collection, denoted as ci. If trajectory contains search actions, we have set of collections = {c1, . . . , cM}. 2. Judge Evaluation. We construct prompt provided in Table 11 containing the user question, the ground truth answer, and the chronological list of collections. The judge evaluates each collection ci against three strict criteria: Useful (ui = 1): The collection contains information or clues that help identify the correct answer, even partially. Not Useful (ui = 0): The collection is completely irrelevant. Redundant (ui = 0): The collection merely duplicates information from previous collections (c1...i1) without adding new insights, even if the information is relevant. 3. Density Computation. The judge outputs the total count of useful collections. The process reward is calculated as the ratio of useful collections to total search actions: rprocess(y) = 1 i=1 ui. 2026-02- (16) 4. Outcome Gating. To prevent \"reward hacking\" where an agent maximizes retrieval scores without solving the task, the process reward is applied only when the final answer is correct. The total reward R(y) is defined as: R(y) = routcome(y) (1 + rprocess(y)), (17) where routcome(y) is the binary Exact Match (EM) score. Meta-Refiner Prompt You are meticulous meta-thinker. Review the numbered ASSISTANT_STEP entries and identify the earliest flawed step. Return single integer between 0 and {max_steps} where 0 means all steps are acceptable. ASSISTANT_CONTEXT: <assistant turns before current rollout> USER: <user message> ASSISTANT_STEP_1: <assistant turn 1> TOOL: <tool output triggered by step 1 (if any)> ASSISTANT_STEP_2: <assistant turn 2> TOOL: <tool output triggered by step 2 (if any)> ... Problematic step index (0 = no issue): We provide the prompt for Meta-Refiner in Table 12. Table 12: Prompt for Meta-Refiner."
        }
    ],
    "affiliations": [
        "City University of Hong Kong",
        "LLM Department, Tencent",
        "McGill University",
        "Mohamed bin Zayed University of Artificial Intelligence",
        "The Chinese University of Hong Kong",
        "The University of Edinburgh"
    ]
}