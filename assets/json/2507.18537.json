{
    "paper_title": "TTS-VAR: A Test-Time Scaling Framework for Visual Auto-Regressive Generation",
    "authors": [
        "Zhekai Chen",
        "Ruihang Chu",
        "Yukang Chen",
        "Shiwei Zhang",
        "Yujie Wei",
        "Yingya Zhang",
        "Xihui Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Scaling visual generation models is essential for real-world content creation, yet requires substantial training and computational expenses. Alternatively, test-time scaling has garnered growing attention due to resource efficiency and promising performance. In this work, we present TTS-VAR, the first general test-time scaling framework for visual auto-regressive (VAR) models, modeling the generation process as a path searching problem. To dynamically balance computational efficiency with exploration capacity, we first introduce an adaptive descending batch size schedule throughout the causal generation process. Besides, inspired by VAR's hierarchical coarse-to-fine multi-scale generation, our framework integrates two key components: (i) At coarse scales, we observe that generated tokens are hard for evaluation, possibly leading to erroneous acceptance of inferior samples or rejection of superior samples. Noticing that the coarse scales contain sufficient structural information, we propose clustering-based diversity search. It preserves structural variety through semantic feature clustering, enabling later selection on samples with higher potential. (ii) In fine scales, resampling-based potential selection prioritizes promising candidates using potential scores, which are defined as reward functions incorporating multi-scale generation history. Experiments on the powerful VAR model Infinity show a notable 8.7% GenEval score improvement (from 0.69 to 0.75). Key insights reveal that early-stage structural features effectively influence final quality, and resampling efficacy varies across generation scales. Code is available at https://github.com/ali-vilab/TTS-VAR."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 2 ] . [ 1 7 3 5 8 1 . 7 0 5 2 : r TTS-VAR: Test-Time Scaling Framework for Visual Auto-Regressive Generation Zhekai Chen1 Ruihang Chu2 Yukang Chen3 Shiwei Zhang2 Yujie Wei2 Yingya Zhang2 Xihui Liu1 1 HKU MMLab 2 Tongyi Lab, Alibaba Group 3 CUHK zkchen66@outlook.com"
        },
        {
            "title": "Abstract",
            "content": "Scaling visual generation models is essential for real-world content creation, yet requires substantial training and computational expenses. Alternatively, test-time scaling has garnered growing attention due to resource efficiency and promising performance. In this work, we present TTS-VAR, the first general test-time scaling framework for visual auto-regressive (VAR) models, modeling the generation process as path searching problem. To dynamically balance computational efficiency with exploration capacity, we first introduce an adaptive descending batch size schedule throughout the causal generation process. Besides, inspired by VARs hierarchical coarse-to-fine multi-scale generation, our framework integrates two key components: (i) At coarse scales, we observe that generated tokens are hard for evaluation, possibly leading to erroneous acceptance of inferior samples or rejection of superior samples. Noticing that the coarse scales contain sufficient structural information, we propose clustering-based diversity search. It preserves structural variety through semantic feature clustering, enabling later selection on samples with higher potential. (ii) In fine scales, resampling-based potential selection prioritizes promising candidates using potential scores, which are defined as reward functions incorporating multi-scale generation history. Experiments on the powerful VAR model Infinity show notable 8.7% GenEval score improvement (0.690.75). Key insights reveal that early-stage structural features effectively influence final quality, and resampling efficacy varies across generation scales. Code is available at https://github.com/ali-vilab/TTS-VAR."
        },
        {
            "title": "Introduction",
            "content": "Recent years have witnessed significant progress in image generative models [14]. Previous text-toimage generative models primarily rely on diffusion models [5, 6], which iteratively denoise the latent to generate high-quality images from random noise. Yet, advancements in large language models (LLMs) [79] have spurred interest in Auto-Regressive (AR) architectures for image generation [10 12], leveraging sequential modeling to capture visual patterns. Among these, Visual Auto-Regressive Modeling (VAR) [13, 14] has emerged as groundbreaking paradigm. It encodes images into multi-scale coarse-to-fine representations and progressively predicts the \"next scale\" to synthesize images through hierarchical aggregation. Due to its superior efficiency and the potential for unified integration with LLMs, VAR is fast emerging as key research frontier. * Corresponding Authors Preprint. Under review. Figure 1: TTS-VAR generates several samples concurrently like Best-of-N (BoN). In TTS-VAR, we adopt an adaptive descending batch size schedule to make the most of AR efficiency, with feature clustering at early scales to ensure diversity, and resampling according to potentials at late scales for more valuable samples. (1-3) are overviews showing the difference between raw inference, BoN, and TTS-VAR. (a) is detailed example of the generation process of our method. Meanwhile, following the success of test-time scaling in LLMs [1519] , researchers have begun exploring this methodology to image generative models for better results. In auto-regressive models, previous works typically formulate image generation as an image-level [20] or token-level [21] multi-stage process, treating the sequence of stages as the Chain-of-Thought (CoT). However, these methods require additional training to achieve effective scaling. Alternatively, diffusion-based approaches [2226] regard scaling as path searching problem, which scores different intermediate states and selects the most promising noise to denoise for higher-quality images. There are two main strategies to achieve great improvement. One [22, 23] introduces additional denoising steps to obtain the clean latents for image decoding and select intermediate latent states based on the final decoded results. The other [24], instead, directly scores decoded images from intermediate noisy latents to guide selection by reward functions [27]. Inspired by this perspective, we explore whether VAR models can also benefit from path searching. However, directly adopting the two strategies from diffusion models is non-trivial. For the former, the extra inference steps incur prohibitively high computation for VAR models with exponential complexity growth. They also disrupt the KV Cache mechanism [28, 29], which is crucial for retaining efficiency in AR inference. The latter also fails to reach the expectation. We observe that the rewards of images at early scales struggle to accurately represent the quality of final images, leading to incorrectly ruling out certain early-scale tokens which could be promising for later-scale generation, as demonstrated in Sec. 5.3. We attribute this to the difference between VAR and diffusion models. In VAR, unlike diffusion process that can refine generated noise through iterative denoising, all tokens remain fixed once generated. Each token not only contributes to decoding the final image 2 but also directly affects all subsequent token generation, resulting in much lower tolerance for poor early-stage tokens. In this paper, we introduce the first Test-Time Scaling framework for VAR, abbreviated as TTSVAR. Different from simple selection according to reward functions, we design scaling strategies aligned to the causal coarse-to-fine generation process of VAR. Firstly, noticing the progressively increased consumption of FLOPs and RAM in VAR, we implement our framework under an adaptive descending batch size schedule, reduced from larger batch sizes at coarse scales to smaller ones in fine scales. This promotes the expression of more possibilities with little additional consumption. Secondly, though early scales are hard to evaluate by reward functions, we observe that structural information, which has great impact on image contents, can be captured since early scales, as shown in Fig. 1 (b) and Sec. 5.4. This motivates splitting the generation process into two key components: clustering-based diversity search for early scales and resampling-based potential selection for late scales. At early scales, while results of intermediate states can hardly be estimated, we aim to keep the diversity as batch size decreases, thus enabling later selections on samples with higher potential. We employ clustering on semantic features extracted by pre-trained extractors like DINOv2 [30], and pick from each category for dissimilar samples to ensure sampling diversity. At late scales, while scores of intermediate images share high consistency with those of final images, we calculate potential scores to directly resample preferred samples. The potential scores are specifically defined reward functions based on the generation history of all scales, instead of only the current one. To summarize, we propose TTS-VAR, the first general test-time scaling framework for VAR models. By integrating clustering-based diversity search and resampling-based potential selection tailored to VARs causal generation process, TTS-VAR consistently delivers stable performance improvements. We conduct comprehensive experiments and analysis on Infinity [14], scaled-up text-to-image VAR model, revealing why resampling methods exhibit scale-dependent limitations and demonstrating the benefits of structural feature clustering. Notably, TTS-VAR significantly improves the GenEval score from 0.69 to 0.75, along with consistent improvements in other metrics."
        },
        {
            "title": "2 Related Works",
            "content": "2.1 Test-time Scaling in Diffusion Models Diffusion models [6, 3135] create high-resolution images by denoising Gaussian distribution into an image distribution. Initial research efforts [36, 5, 37] concentrated on scaling up the number of denoising steps to enhance the quality. However, it has been observed that as the number of inference steps increased, performance plateaued, and sampling additional steps is ineffective. Consequently, early research [3840] mainly aims to reduce inference steps while maintaining image quality. Ma et al. [22] address the scaling issue in diffusion models as path searching problem, achieving significant improvements by applying several search strategies within the latent space, with reward functions serving as verifiers. Building on this problem definition, subsequent studies [23, 24] investigate the effectiveness of various search strategies and methods to accurately verify intermediate states for choice. Oshima et al. [23], for instance, employ few-step sampling instead of one-step sampling for denoised images that are clearer and more suitable for verification. 2.2 Test-time Scaling in Autoregressive Models In autoregressive Large Language Models [9, 7, 8], test-time scaling is widely employed technique to enhance performance. Since Wei et al. [15] proposed Chain-of-Thought and enabled LLMs to benefit from structured thinking process, various studies [1619, 41] have explored tree search, graph search, and other methodologies to improve outcomes further. All these strategies leverage the reasoning capabilities of models and utilize the properties inherent in natural language for scaling. However, within autoregressive image generative models, characterized by deterministic process with steady token length, it is unnatural to directly increase the image token sequence as \"thinking\". Instead, Guo et al. [20] conceptualize generation CoT as an image-level problem. By employing unified understanding and generation model [42] that first generates and then evaluates, it self-corrects results to align with expectations. Nevertheless, this approach relies solely on evaluating results, neglecting the generation process itself. Jiang et al. [21], instead, propose splitting the task into 3 semantic-level and token-level phases, enabling multi-stage generation as the thinking process. However, this method necessitates additional reinforcement learning for fine-tuning."
        },
        {
            "title": "3 Preliminary: Visual Auto-Regressive Modeling",
            "content": "Unlike traditional next-token prediction auto-regressive models such as LLama-Gen [10], Visual AutoRegressive Modeling (VAR) [13] tokenizes an input image into feature map Rhwd. With quantizer Q, it quantizes the feature map into sequence of multi-scale discrete residual feature maps [43] {ri}K i=1, where represents the number of residual features across varying resolutions. For each residual feature map rk, the resolution is hk wk, which progressively increases from = 1 to = K. Specifically, when = 1, hk = wk = 1, and when = K, hk = h, wk = w. From the sequence of residual features, at each scale k, gradually refined feature map fk can be computed as: fk = (cid:88) i=1 up(ri, (h, w)), (1) where up(, ) denotes upsampling the single-scale feature map to the target resolution, and fk is the aggregated sum of features {ri}k i=1. During inference, the downsampled accumulated feature map fk = down(fk, (hk+1, wk+1)) is appended as initial tokens for the prediction of the next scale and scale-wise causal mask is employed to facilitate local bi-directional information modeling. The transformer is trained to predict the next-scale residual feature map. In Infinity [14], VAR-based model scaled up for text-to-image generation, the quantizer is advanced from VQ [44] to BSQ [45]. Additionally, Flan-T5 [46] text encoder Ψ is harnessed for prompt embeddings. With text prompt as the condition, the overall likelihood is: p(r1, r2, . . . , rK) = (cid:89) (rkr1, r2, . . . , rk1; Ψ(c)). (2) k="
        },
        {
            "title": "4 Method",
            "content": "In TTS-VAR, we conceptualize the generation of high-quality and human-preferred images as path searching problem, and identify two primary subproblems: (i) how to search for more possibilities, and (ii) how to select intermediate states for superior final results. Besides applying an adaptive batch size schedule as illustrated in Sec. 4.1 to enlarge the search scope, we introduce clusteringbased diversity search in Sec. 4.2 and resampling-based potential selection in Sec. 4.3 to solve these problems. The complete method is illustrated in Fig. 1 (c). 4.1 Adaptive Batch Sampling Figure 2: Different Batch Size Schedules. We visualize the memory usage in (a) and computation complexity in (b) for 13 scales during the generation of Infinity, with fixed batch size 1 and adaptive batch size. Specifically, the adaptive batch size here is [8,8,6,6,6,4,2,2,2,1,1,1,1]. This batch size schedule enables more possibilities with little additional consumption. Influenced by the causal attention mechanism, in VAR models, both RAM memory consumption and computational expense increase along with the length of the token sequence. As illustrated by the blue lines in Fig. 2 (a) and (b), during the inference process of early scales, memory requirements and computational costs are minimal. However, at later scales, when the pre-existing sequence extends 4 significantly and the current prediction scale encompasses large number of tokens, the resource consumption becomes substantial. Therefore, we implement adaptive batch sizes during inference, capitalizing on the efficiency of lower consumption at early scales. This descending adaptive batch size schedule {b0, b1, . . . , bK}, where represents the number of scales, generates more samples in earlier stages and fewer samples in the later stages. For typical VAR model encompassing 13 scales, the batch size schedule is {8N , 8N , 6N , 6N , 6N , 4N , 2N , 2N , 2N , 1N , 1N , 1N , 1N } unless otherwise specified. At early scales, clustering in Sec. 4.2 filters several categories. At late scales, resampling in Sec. 4.3 chooses superior states. As demonstrated in Fig. 2, while the increased number of batches amplifies memory and computation costs at early scales, these additions are relatively minor in the overall expenditure. 4.2 Clustering-Based Diversity Search As sequence length increases, maintaining large batch size becomes cost-prohibitive, necessitating filtering method for the desired samples. straightforward approach involves calculating the reward function for intermediate results and selecting those with higher scores. However, our findings in Sec. 5.3 indicate that rating models struggle to evaluate early intermediate images for reward scores consistent with the final images, as also observed by Guo et al. [20]. To avoid erroneous elimination of certain initial samples that may hold potential for later-scale generation, we explore ways to keep the diversity of samples. We observe that during the generation process, unlike the details appearing in late scales, the structural information, which significantly influences the quality of final images, is conveyed from the early phases. We analyze this phenomenon in Sec. 5.4 and find extractors like DINOv2 [30] can effectively capture features strongly connected with structures. Given this, we create clusters based on the semantic features to filter samples from each category and ensure structural diversity, thereby maximally enhancing possibilities for valuable results. Specifically, from the current batch size bi, we need to select bi+1 samples as next states. Firstly, for bi intermediate images {Ij}bi j=1, each image is embedded in high-dimensional semantic embedding space via feature extractor , creating set of embeddings = (cid:83) (Ij). Subsequently, we apply the K-Means++ [47] algorithm to cluster these embeddings into bi+1 cluster centers and select samples with the shortest L2 distance to cluster centers as new batches. bi To extract structural information for diversity, we primarily employ the self-supervised DINOv2 [30] as the extractor, generating the feature map R(hw)d. To obtain one-dimensional features for clustering, we apply PCA reduction on feature patches for R(hw). We also consider pooling the second dimension for Rd, and supervised features from InceptionV3 [48] without the final fully connected layer. We discuss these choices in Sec. 5.4. 4.3 Resampling-Based Potential Selection In contrast to early-stage diversity preservation through clustering, when intermediate images show high consistency with final results, reward functions can directly guide the generation toward higher quality and alignment with human preferences at late scales. Typically, reward function rϕ(x) is derived from reward model ϕ, which includes specially trained rating models and vision-language models. For scores conditioned on text prompt c, the reward function can be expressed as rϕ(x, c). In the context of generative model based on the generation distribution pθ(x), we aim to steer the distribution to align with reward preferences [49, 24], as follows: pθ(x) = 1 pθ(x) exp(λ rϕ(x, c)) (3) where pθ(x) is the target distribution, is normalization constant, and λ is hyperparameter to control the temperature in selection. To obtain high-quality samples, we evaluate the reward score for each intermediate state at current scale and replace them with ones sampled from multinomial distribution based on the potential score Pk. Considering that the generation of VAR is path with historical states and merely rating the image xk = D(fk), decoded by the image decoder from the accumulated feature fk, may not adequately reflect the potential of final results, we therefore contemplate several potential scores. 5 Methods # Params GenEval Two Obj. Counting Color Attri. Overall Diffusion Models SDXL [2] +FK (N = 8) [24] PixArt-Alpha [1] DALL-E 3 [3] FLUX [4] SD3 [61] AR Models LlamaGen [10] Chameleon [62] Show-o [42] +PARM (N = 20) [20] Emu3 [11] Infinity Infinity+IS (N = 8) Infinity+BoN (N = 8) Infinity+Ours (N = 2) Infinity+Ours (N = 8) 2.6B 2.6B 0.6B - 12B 8B 0.8B 7B 1.3B 1.3B 8.5B 2B 2B 2B 2B 2B 0.74 - 0.50 0.87 0.81 0.94 0.34 - 0.52 0.77 0.71 0.39 - 0.44 0.47 0.74 0. 0.21 - 0.49 0.68 0.34 0.23 - 0.07 0.45 0.45 0.60 0.04 - 0.28 0.45 0.21 0.55 0.65 0.48 0.67 0.66 0.74 0.32 0.39 0.53 0.67 0.54 0.8351 0.8969 0.9201 0.9278 0. 0.5923 0.6220 0.6756 0.7113 0.7411 0.6150 0.6550 0.6700 0.6775 0.6800 0.6946 0.7181 0.7364 0.7403 0.7530 Table 1: Quantitative evaluation on GenEval. Figure 3: Score Curves over Sample Number . Potential Score Pk. We denote Pk(x0, x1, . . . , xk) as the potential score of sample at scale k, with x0, x1, . . . , xk representing the generation history of this sample. Pk(x0, x1, . . . , xk) = exp(λ rϕ(xk, c)): This directly utilizes the reward score as the potential score, referred to as Value. It is also known as importance sampling (IS) [50, 51]. Pk(x0, x1, . . . , xk) = exp(λ (rϕ(xk, c) rϕ(xk1, c))): This computes the difference between two consecutive scales as the potential score, termed DIFF. Pk(x0, x1, . . . , xk) = exp(λ maxk i=0{rϕ(xi, c)}): This selects the highest score in the generation path as the current potential score, designated MAX. Pk(x0, x1, . . . , xk) = exp(λ(cid:80)k i=0 rϕ(xi, c)): This accumulates all scores from the history to determine the current potential score, labeled SUM. Different potential scores benefit distinct attributes of the generation history. For instance, DIFF favors samples with higher growth rates, while MAX favors those with higher ceilings. In our setting, VALUE performs well. We will explore these choices in Sec. 5.3."
        },
        {
            "title": "5 Experiments",
            "content": "In this section, we demonstrate the effectiveness of our TTS-VAR on the powerful VAR model Infinity2B [14] with resampling temperature λ = 10. We present the comparisons in Sec. 5.1 and Sec. 5.2, and precisely analyze design details in Sec. 5.3 and Sec. 5.4. Following previous work [24, 22, 52], we utilize ImageReward [53] as the reward function for guidance. We evaluate results using the main metric GenEval [54] and T2I-CompBench [55], with relevant indicators ImageReward [53], HPSv2.1 [56, 57], Aesthetic V2.5 [58], and CLIP-Score [59, 60], based on prompts offered by GenEval. 5.1 Overall Performance As shown in Table 1, our proposed method demonstrates significant improvements over existing state-of-the-art models and conventional test-time scaling strategies (Importance Sampling and Bestof-N). With model size of 2B parameters, TTS-VAR achieves an overall GenEval score of 0.7530 at = 8, surpassing the record 0.74 of Stable Diffusion 3 (8B parameters) while utilizing 60% fewer parameters. Our framework also exhibits substantial gains across single items, like two objects. Notably, even with minimal computational overhead (N = 2), our approach attains the competitive performance of score 0.7403, outperforming Best-of-N (N = 8) with only 25% sample number. 6 Model Avg. Color Shape Texture 2D Spatial 3D Spatial Numeracy Stable v2 [31] Stable XL [2] Pixart-α-ft [1] DALLE 3 [3] FLUX.1 [4] Infinity [14] Infinity+IS (N = 8) Infinity+BoN (N = 8) Infinity+Ours (N = 2) Infinity+Ours (N = 8) B-VQA B-VQA B-VQA UniDet UniDet UniDet 0.4839 0.5255 0.5583 0.6168 0.6087 0.5688 0.5965 0.6115 0.6151 0. 0.5065 0.5879 0.6690 0.7785 0.7407 0.7421 0.7746 0.7950 0.7887 0.8073 0.4221 0.4687 0.4927 0.6205 0.5718 0.4557 0.5078 0.5439 0.5578 0.5914 0.4922 0.5299 0.6477 0.7036 0.6922 0.6034 0.6501 0.6886 0.6858 0. 0.1342 0.2133 0.2064 0.2865 0.2863 0.2279 0.2462 0.2545 0.2697 0.2644 0.3230 0.3566 0.3901 0.3744 0.3866 0.4023 0.4194 0.4205 0.4286 0.4302 0.4582 0.4988 0.5032 0.5926 0.6185 0.5479 0.6002 0.6090 0.6112 0. Nonspatial S-CoT 0.7567 0.7673 0.7747 0.7853 0.7809 0.7820 0.7803 0.7870 0.7853 0.7880 Complex S-CoT 0.7783 0.7817 0.7823 0.7927 0.7927 0.7890 0.7937 0.7937 0.7936 0.7963 Table 2: Quantitative evaluation on T2I-CompBench. From the perspective of different , TTS-VAR maintains consistent performance gains across varying sample sizes in both GenEval and ImageReward metrics, as illustrated in Figure 3. While Best-of-N sampling benefits more from larger N, its performance remains distinctly inferior to ours, even failing to match our results at = 2 with = 8. Detailed evaluations across different values are provided in the appendix for comprehensive analysis. We further evaluated performance on T2I-CompBench [55]. As presented, TTS-VAR exhibits remarkable improvement across every indicator compared to the base model. Consistent with the consequences on GenEval, our method achieves superior results at = 2 compared to Best-of-N at = 8, and secures the highest scores on most individual items and the overall average. 5.2 Qualitative Comparison We present images generated by different methods here for reference, to further explain the improvement in image quality and text alignment. As displayed in Fig. 4, our method correctly generated objects with the required number, for example, the \"kayaks\" in the first case and the \"birds\" in the second case, which is hard for base models and other scaling strategies to achieve. In the complex scenes with several pairs of numbers and colors, TTS-VAR ensures the color attribute like the \"golden apples\" in the fourth case, avoiding the problem of attribute forgetting. 5.3 Resampling for Superior Samples Analysis. In path searching, resampling intermediate states with high potentials is straightforward yet effective approach for superior results with minimal consumption. However, in VAR, this may not be advantageous and can even be detrimental at certain scales. In Fig. 5 (a), we illustrate the score distinctions between solely employing Best-of-N (N = 2, 4) and concurrently applying potential (VALUE) resampling at specific scales. Although Best-of-N selection ensures comparatively high results, it is evident that resampling at early scales (e.g., scale 3) leads to noticeable decline in the final results. Instead, resampling at later scales yields certain degree of improvement. Resampling Scale GenEval 0.6946 1 0.7133 2 0.7130 2 0.7114 2 0.7276 4 0.7247 4 0.7210 4 - [6, 9] [6, 8, 10] [6, 7, 8, 9, 10, 11] [6, 9] [6, 8, 10] [6, 7, 8, 9, 10, 11] ImageReward 1.132 1.2572 1.2591 1.2497 1.3534 1.3592 1. HPS 0.3042 0.3066 0.3066 0.3067 0.3082 0.3085 0.3083 CLIP 0.3366 0.3379 0.3381 0.3378 0.3398 0.3397 0.3398 Aesthetic 0.5811 0.5801 0.5809 0.5810 0.5817 0.5822 0.5830 Table 3: Resampling Scale Difference. This table shows results with different resampling scales, indicating the influence of resampling frequency. We analyze this phenomenon from the perspective of consistency between intermediate states and final images, as shown in Fig. 5 (b). We compute potential scores at each scale and select the one with the highest potential accordingly. We then assess whether the selected best intermediate state aligns with the best final image (the optimal state at scale 12), resulting in sequence of scores between 0 Figure 4: Qualitative Comparison. Each line shows results generated by Stable Diffusion 3 (SD3) [61], Infinity, and Infinity with test-time scaling strategies, with objects marked blue and relationships marked green. and 1, termed consistency. Low consistency of early scales in this evolving curve indicates that those scores rarely accurately reflect the quality of final results. The scores become valuable from certain late scale, like scale 6, with comparatively high consistency. This explains why resampling efficacy varies and should be applied selectively on later scales. Figure 5: Resample choices. The left graph shows the variation in ImageReward Score when executing resampling-based potential selection at different scales (0-11). The right graph shows the consistency between scores of intermediate states and those of final results at each scale. It demonstrates that in VAR, not all scales are suitable for selection, and some may lead to degradation. 8 Resampling Scales. Based on the aforementioned observation, we further investigate whether increasing the resampling frequency at late scales is beneficial. As illustrated in Table 3, compared with raw inference, resampling greatly enhances the results. However, increasing the frequency has negligible impact. For instance, there is modest improvement in ImageReward and HPS for scales [6,8,10] compared to scales [6,9], but at the cost of Geneval. Considering that executing resampling incurs additional computational expenses related to image decoding and score calculation, we opt to resample only at scales 6 and 9. Figure 6: Consistency of Different Potentials. We visualize the consistency between different pairs of potential scores and final results. Accordingly, VALUE and MAX can better indicate the potentials. Potential Scores. As mentioned in Sec. 4.3, we have developed distinct computational modes to explore those with higher potentials. Initially, we seek better options through theoretical approach by visualizing the consistency. As exhibited, DIFF continuously yields low consistency levels and fails to predict the desired outcomes. SUM demonstrates stable increment but with comparatively low values. In contrast, VALUE and MAX exhibit similar characteristics, maintaining relatively high scores since scale 6 and showing steady increase. Experiments in Table 4 with = 2, 4 present coherent results. Among these potentials, DIFF lags in all indicators. Although SUM achieves some acceptable outcomes, the overall score remains low. As forecasted by consistency, VALUE and MAX achieve the highest scores in text-related metrics such as GenEval, ImageReward, and HPS, indicating higher likelihood of selecting for superior final results. Considering that MAX requires score calculations at each scale and leads to an additional computational cost, we utilize VALUE as the potential score. 5.4 Clustering for Structural Diversity Figure 7: Visualization of Generation Process. The text prompt here is \"a photo of bottle and bicycle\". The left is one final generated image. The right is the corresponding generation process and visualized DINOv2 features extracted from different scales. It demonstrates that features captured in early scales can indicate the structural information. Potential GenEval 0.7133 2 0.7150 2 0.7130 2 0.7006 2 0.7276 4 0.7285 4 0.7244 4 0.7030 4 VALUE MAX SUM DIFF VALUE MAX SUM DIFF ImageReward 1.2572 1.2510 1.2364 1.1725 1.3534 1.3495 1.3326 1.2412 HPS 0.3066 0.3065 0.3064 0.3042 0.3082 0.3082 0.3082 0.3051 CLIP 0.3379 0.3379 0.3379 0.3365 0.3398 0.3398 0.3394 0.3378 Aesthetic 0.5801 0.5803 0.5801 0.5798 0.5817 0.5815 0.5830 0.5808 Table 4: Potential Score Difference. This table shows results using different potential calculating means as scores for resampling. Analysis. In reference to Sec. 5.3, resampling is not universally applicable to all scales. Nevertheless, in VAR, the effectiveness and low cost associated with early scales present an invaluable and unmissable opportunity to search for more samples, thereby unlocking greater potential for the final outcomes. We notice that, given the same prompt, the structure of images significantly influences the scores. Moreover, unlike details that emerge later, structural information can be captured from the early scales. The right side of Fig. 7 demonstrates that the generation process follows structureto-detail progression, with rough outlines becoming discernible from scale 2. When using DINOv2 to extract intermediate images and visualizing them through PCA, as seen in the bottom line, these features exhibit characteristics akin to the original images. Consequently, we leverage structural information and conduct clustering-based diversity search to sample dissimilar structures, thus scaling for more possibilities, especially when resampling may not suffice. Clustering Scale GenEval 0.7087 2 0.7089 2 0.7099 2 0.7184 2 0.7244 4 0.7300 4 0.7293 4 0.7337 4 - [2] [5] [2, 5] - [2] [5] [2, 5] ImageReward 1.2545 1.2513 1.2558 1.2682 1.3471 1.3502 1.3558 1.3610 Table 5: Clustering Scale Difference. This table shows results with and without clustering at certain scales. Extractor GenEval 0.7184 2 0.7127 2 0.7073 2 0.7337 4 0.7296 4 0.7207 PCA Pool Inception PCA Pool Inception ImageReward 1.2682 1.2720 1.2727 1.3610 1.3629 1.3664 Table 6: Extractor Difference. This table shows results when adopting different feature extraction methods. PCA and Pool here are both transformed from 2-dimensional features extracted by DINOv2. Clustering Scales. According to Fig. 7, features at scale 2 display coarse structure, while those at scale 5 reveal refined structure similar to final outcomes. Therefore, we specifically apply clustering on these scales. The results of = 2, 4, with and without clustering, are presented in Table 5. The first lines of each block denote the results without clustering (Best-of-N), with subsequent lines showing outcomes under different clustering scales. Evidently, each clustering increases the likelihood of yielding better results, and there is an obvious growth when employing both scales. Extractor. We tested various extractors for clustering features outlined in Table 6. Both PCA and Pool are transformations of features extracted by DINOv2 [30], as detailed in Sec. 4.2. While supervised InceptionV3 [48] features perform optimally in ImageReward, they notably underperform in GenEval. PCA delivers superior results on average and is employed. We attribute this to that the patch-level features from PCA align more closely with the observed structural traits."
        },
        {
            "title": "6 Conclusion",
            "content": "In this work, we introduce the first general test-time scaling framework for VAR models. Through analysis on different scales, we demonstrate that TTS-VAR, which incorporates adaptive batch sampling, clustering-based diversity search, and resampling-based potential selection, aligns with distinct stages of VAR generation process. This dual-strategy approach enhances final result quality with minimal additional computational cost while maintaining algorithmic efficiency. We notice the limitation and potential societal impact on privacy and copyright, and discuss these in the appendix."
        },
        {
            "title": "Appendix",
            "content": "A Algorithm of TTS-VAR We describe the algorithm of TTS-VAR in Alg. 1. Following the generation process of VAR [13] (Infinity [14]), TTS-VAR first predicts the residual tokens at the current scale and adds them to the accumulated feature maps. At scales that require clustering, TTS-VAR uses the extractor to gather features from bi intermediate images decoded from the feature maps. It then clusters the samples based on these features and selects bi+1 ones as the next batch. At scales that require resampling, TTS-VAR employs the potential function to calculate scores for each image and samples bi+1 indexes from the multinomial distribution for superior intermediate states. Algorithm 1 TTS-VAR Require: Scales = {s1, s2, ..., sK}, Descending batch sizes = {b1, b2, ..., bK}, Clustering scales set Sc, Resampling scales set Sr, Generative model θ, Reward model rϕ Extractor , Potential Score function , Text prompt c. ri Generate(θ, bi, si, fi1, c) fi fi1 + ri if si Sc then 1: Initialize accumulated feature map f0 with zeros. 2: for {1, 2, ..., K} do 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: end for Decode(fi) eat (x) index KMeans++(f eat, bi+1) fi fi[index] else if scale Sr then Decode(fi) rw rϕ(x) (rw) index Multinomial(p, bi+1) fi fi[index] end if Iterate through scales Clustering phase Resampling phase return Final generated images Decode(fK)"
        },
        {
            "title": "B Detailed Main Results",
            "content": "N 1 1 2 2 2 4 4 4 8 8 8 Strategy Raw Inference Ours Importance Sampling Best-of-N Ours Importance Sampling Best-of-N Ours Importance Sampling Best-of-N Ours GenEval 0.6946 0.7253 0.7022 0.7087 0.7403 0.7116 0.7244 0.7437 0.7181 0.7364 0.7530 ImageReward 1.1320 1.3226 1.1941 1.2545 1.4136 1.2883 1.3471 1.4605 1.3657 1.4144 1.4995 HPS 0.3042 0.3084 0.3051 0.3069 0.3106 0.3067 0.3083 0.3112 0.3085 0.3103 0.3122 CLIP 0.3366 0.3395 0.3374 0.3384 0.3411 0.3387 0.3397 0.3414 0.3395 0.3406 0. Aesthetic 0.5811 0.5822 0.5807 0.5813 0.5821 0.5815 0.5820 0.5821 0.5810 0.5820 0.5810 Table 6: Scores over Different Strategies. This table shows results with different scaling strategies. We present detailed results of variant curves in Table 6. As evident, TTS-VAR demonstrates clear advantages across all indicators [54, 53, 57, 58, 60] compared to the baselines. In Table 7, we list each item of the GenEval [54] metric. Generally, our method significantly improves performance on handling two objects and counting tasks. We attribute this to the importance of structural accuracy in multi-character scenes, particularly when two objects and multiple identical objects (counting) are involved. For instance, when provided with prompt for three objects, there is possibility that 11 the model may incorrectly generate layout with four objects. Once this error occurs, following the structure-to-detail generation process in VAR, it becomes challenging for subsequent scales to rectify. However, TTS-VAR facilitates structure diversity, thereby enabling the selection of layout with the correct configuration and avoiding the irreversible wrong generation process for inferior samples. 1 1 2 2 2 4 4 4 8 8 Strategy Raw Inference Ours Importance Sampling Best-of-N Ours Importance Sampling Best-of-N Ours Importance Sampling Best-of-N Ours Overall 0.6946 0.7253 0.7022 0.7087 0.7403 0.7116 0.7244 0.7437 0.7181 0.7364 0.7530 Single Obj. Two Obj. Counting Colors 0.9293 0.9192 0.9268 0.9242 0.9318 0.9318 0.9242 0.9293 0.9318 0.9444 0.9318 0.8351 0.9072 0.8497 0.8789 0.9278 0.8840 0.8969 0.9510 0.8969 0.9201 0.9501 0.5923 0.6518 0.6071 0.6339 0.7113 0.6339 0.6756 0.6994 0.6220 0.6756 0.7411 0.9938 0.9938 0.9969 0.9906 0.9936 0.9906 1.0000 0.9906 0.9906 0.9938 0. Position Color Attri. 0.2020 0.2096 0.1869 0.1944 0.1995 0.1970 0.1944 0.2045 0.2121 0.2146 0.2172 0.6150 0.6700 0.6475 0.6300 0.6775 0.6325 0.6550 0.6875 0.6550 0.6700 0.6800 Table 7: GenEval Details. This table shows each item of the GenEval benchmark, \"Object\" is short for \"Obj.\", and \"Attribute\" is short for \"Attri.\"."
        },
        {
            "title": "C Performance over Computational Consumption",
            "content": "We here display the changing curves of GenEval, ImageReward, and HPSv2 over the increment of computation in Fig. 8, along with the increment of sample number . As shown, our method TTS-VAR has higher computational efficiency and surpasses Importance Sampling and Best-of-N with less than half TFLOPs. Figure 8: Performance over Flops. This figure shows the variant curves of different methods with computational consumption as the x-axis, demonstrating the efficiency of our method."
        },
        {
            "title": "D Ablation Study",
            "content": "D.1 Pipeline Ablation We present the ablation study of different design components in the overall pipeline in Table 8. As adaptive batch sampling alone (without integrated sample selection mechanisms) cannot directly enhance generation performance, these cases are denoted by \"-\". Excluding these baseline cases, both clustering-based diversity search and resampling-based potential selection demonstrate performance improvements, with statistically significant gains observed in reward and related evaluation indicators. Notably, the clustering approach yields relatively moderate improvements, which can be attributed to its primary function of maintaining structural diversity rather than actively identifying superior samples for subsequent generation. The combination of diversity maintenance through clustering and quality-based selection via resampling synergistically enhances the effectiveness of the pipeline. This dual-mechanism framework ultimately achieves substantial performance gains over the baseline system, with the resampling component playing the pivotal role in selecting high-quality candidates for iterative refinement. 12 Method Infinity 2 +BoN +Adaptive Batch Sampling +Clustering-Based Diversity Search +Resampling-Based Potential Selection 4 Infinity +BoN +Adaptive Batch Sampling +Clustering-Based Diversity Search +Resampling-Based Potential Selection GenEval 0.6946 0.7087 - 0.7220 0.7403 0.6946 0.7244 - 0.7294 0.7437 ImageReward 1.1320 1.2545 - 1.2591 1.4136 1.1320 1.3471 - 1.3608 1.4605 HPS 0.3042 0.3069 - 0.3072 0.3106 0.3042 0.3083 - 0.3095 0. CLIP 0.3366 0.3384 - 0.3385 0.3411 0.3366 0.3397 - 0.3403 0.3414 Aesthetic 0.5811 0.5813 - 0.5816 0.5821 0.5811 0.5820 - 0.5824 0.5821 Table 8: Pipeline Ablation. This table shows gains from each design. D.2 Reward Models We implement comparisons on using different reward models to rate the intermediate images and calculate the potential scores (VALUE), including Aesthetic [58], ImageReward [53], HPSv2 [57], and HPS+ImageReward. Owing to different value ranges of HPS and ImageReward, for HPS+ImageReward, we first calculate scores using the two models separately, then softmax the values of each model into the range [0, 1], and finally take the average as the potential scores. As shown in Table 9, generally, each reward model motivates an increase in the corresponding metric. For instance, with = 4, the Aesthetic model, the ImageReward model, and the HPS model achieve the highest scores in the associated indicators, respectively. Among different models, ImageReward promotes improvements more. Especially with = 2, ImageReward demonstrates clear lead in GenEval and even defeats HPS in HPS score. We attribute this to the ability to clearly distinguish between superior and inferior samples, along with scoring that better aligns with human preferences. Reward Model - Aesthetic ImageReward HPS 2 2 2 2 2 HPS+ImageReward 4 4 4 4 4 HPS+ImageReward - Aesthetic ImageReward HPS GenEval 0.7087 0.6966 0.7403 0.7135 0.7238 0.7244 0.6842 0.7437 0.7255 0.7413 ImageReward 1.2545 1.123 1.4136 1.2246 1.3522 1.3471 1.1172 1.4605 1.2812 1.4128 HPS 0.3069 0.3054 0.3106 0.3102 0.3088 0.3083 0.3056 0.3112 0.3154 0. CLIP 0.3384 0.3366 0.3411 0.3391 0.3402 0.3397 0.3363 0.3414 0.3402 0.3406 Aesthetic 0.5813 0.6004 0.5821 0.583 0.5824 0.5820 0.6114 0.5821 0.5843 0.5818 Table 9: Reward Model Ablation. This table shows results using different models for the potential. D.3 λ Setting In Table 10, we exhibit the results using different temperature λ in the resampling process with fixed clustering operations. Intuitively, higher temperature promotes the expression of intermediate states with higher potential scores and prevents superior samples. However, excessively high temperatures can also widen the gap between intermediate states with the highest scores and those with scores that are only slightly lower. This can directly inhibit the generation of these slightly lagging intermediate states, which may ultimately become the optimal results. As shown, though there is steady increase in ImageReward, λ = 10.0 falls behind λ = 5.0 with = 4 in GenEval."
        },
        {
            "title": "E More Visualization Results",
            "content": "We present pairs of results on GenEval in Fig. 9 to compare the quality of Infinity, Infinity-IS, Infinity-BoN, and Infinity-TTS-VAR. These cases are sampled from text prompts in GenEval, which include the single object, two objects, counting, colors, position, and color attributes. As shown, our method produces higher-quality samples for the single object, such as the airplane in the left second line, effectively avoiding the generation of artifacts. In two-object settings, TTS-VAR successfully distinguishes references to different objects and generates accurate outputs based on prompts. For example, in the right second line, it eliminates conceptual mixtures and object disappearances. As illustrated in lines 3-10 on the right side, TTS-VAR also excels at determining counting numbers, positional relationships, and color attributes. Notably, in the last right line, our method generates 13 Lambda GenEval 0.7087 2 0.7065 2 0.7210 2 0.7222 2 0.7361 2 0.7403 2 0.7244 4 0.7308 4 0.7347 4 0.7418 4 0.7465 4 0.7437 4 - 0.1 0.5 1.0 5.0 10.0 - 0.1 0.5 1.0 5.0 10.0 ImageReward 1.2545 1.2458 1.3167 1.3459 1.4010 1.4136 1.3471 1.3576 1.3918 1.4097 1.4500 1.4605 HPS 0.3069 0.3065 0.3080 0.3087 0.3101 0.3106 0.3083 0.3089 0.3094 0.3099 0.3108 0. CLIP 0.3384 0.3387 0.3400 0.3403 0.3410 0.3411 0.3397 0.3400 0.3406 0.3407 0.3412 0.3414 Aesthetic 0.5813 0.5811 0.5819 0.5821 0.5821 0.5821 0.5820 0.5816 0.5819 0.5820 0.5820 0.5821 Table 10: λ Ablation. This table shows results with different lambda values. counterintuitive \"green carrot,\" demonstrating its ability to separate objects from their natural attributes."
        },
        {
            "title": "F Societal Impact",
            "content": "When applied to VAR models, TTS-VAR enhances the alignment of generated images with textual descriptions, making the generation process more controllable and better suited to meet creative and production demands. However, we also acknowledge the potential for misuse of this method, which could lead to privacy and copyright concerns. Nonetheless, we believe that our in-depth research into the VAR generation process will help researchers gain clearer understanding, advance studies on controllability and safety in generation, and ultimately ensure that image generation models become safe and manageable tools."
        },
        {
            "title": "G Limitation and Future Work",
            "content": "Though TTS-VAR shows significant improvement over the baseline and sets new record, it still has two main limitations. First, TTS-VAR does not completely address the misalignment between text prompts and generated images. As indicated by the scores in Table 7, there are still some failure cases, particularly in the Position item. Second, while TTS-VAR is based on general coarse-to-fine process, its potential application to other coarse-to-fine models, such as autoregressive models that use 1-D tokenizers, remains unexplored. In the future, we will investigate the generation process more thoroughly, examining the reasons for failure and designing solutions to unlock further scaling potential. Additionally, we plan to assess the compatibility of TTS-VAR with other autoregressive coarse-to-fine models, including those utilizing 1-D tokenizers and hybrid architectures that combine diffusion models. These efforts aim to create more robust scaling framework for text-to-image synthesis while enhancing the methodological transferability of coarse-to-fine paradigms. 14 Figure 9: More Visualization Results. Samples are generated from GenEval prompts, with \"IS\" meaning Importance Sampling, \"BoN\" meaning Best-of-N, and our method TTS-VAR. We display various cases, including the single object, two objects, counting, colors, position, and color attributes."
        },
        {
            "title": "References",
            "content": "[1] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-a: Fast training of diffusion transformer for photorealistic text-to-image synthesis. arXiv preprint arXiv: 2310.00426, 2023. [2] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv: 2307.01952, 2023. [3] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, et al. Improving image generation with better captions. Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf, 2(3):8, 2023. [4] Black Flux. announcing-black-forest-labs/, 2024."
        },
        {
            "title": "Forest",
            "content": "Labs. https://blackforestlabs.ai/ [5] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv: 2010.02502, 2020. [6] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. NEURIPS, 2020. [7] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [8] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. [9] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [10] Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan. Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint arXiv: 2406.06525, 2024. [11] Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, Yingli Zhao, Yulong Ao, Xuebin Min, Tao Li, Boya Wu, Bo Zhao, Bowen Zhang, Liangdong Wang, Guang Liu, Zheqi He, Xi Yang, Jingjing Liu, Yonghua Lin, Tiejun Huang, and Zhongyuan Wang. Emu3: Next-token prediction is all you need. arXiv preprint arXiv: 2409.18869, 2024. [12] Chengyue Wu, Xiaokang Chen, Zhiyu Wu, Yiyang Ma, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, Chong Ruan, and Ping Luo. Janus: Decoupling visual encoding for unified multimodal understanding and generation. arXiv preprint arXiv: 2410.13848, 2024. [13] Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling: Scalable image generation via next-scale prediction. Neural Information Processing Systems, 2024. doi: 10.48550/arXiv.2404.02905. [14] Jian Han, Jinlai Liu, Yi Jiang, Bin Yan, Yuqi Zhang, Zehuan Yuan, Bingyue Peng, and Xiaobing Liu. Infinity: Scaling bitwise autoregressive modeling for high-resolution image synthesis. arXiv preprint arXiv: 2412.04431, 2024. [15] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed H. Chi, F. Xia, Quoc Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. Neural Information Processing Systems, 2022. [16] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V. Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. 16 [17] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. [18] Maciej Besta, Nils Blach, Aleš Kubíˇcek, Robert Gerstenberger, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Michal Podstawski, H. Niewiadomski, P. Nyczyk, and Torsten Hoefler. Graph of thoughts: Solving elaborate problems with large language models. AAAI Conference on Artificial Intelligence, 2023. doi: 10.1609/aaai.v38i16.29720. [19] Ruomeng Ding, Chaoyun Zhang, Lu Wang, Yong Xu, Minghua Ma, Wei Zhang, Si Qin, Saravan Rajmohan, Qingwei Lin, and Dongmei Zhang. Everything of thoughts: Defying In Lun-Wei Ku, Andre Martins, and the law of penrose triangle for thought generation. Vivek Srikumar, editors, Findings of the Association for Computational Linguistics, ACL 2024, Bangkok, Thailand and virtual meeting, August 11-16, 2024, pages 16381662. Association for Computational Linguistics, 2024. doi: 10.18653/V1/2024.FINDINGS-ACL.95. [20] Ziyu Guo, Renrui Zhang, Chengzhuo Tong, Zhizheng Zhao, Peng Gao, Hongsheng Li, and Pheng-Ann Heng. Can we generate images with cot? lets verify and reinforce image generation step by step. arXiv preprint arXiv: 2501.13926, 2025. [21] Dongzhi Jiang, Ziyu Guo, Renrui Zhang, Zhuofan Zong, Hao Li, Le Zhuo, Shilin Yan, PhengAnn Heng, and Hongsheng Li. T2i-r1: Reinforcing image generation with collaborative semantic-level and token-level cot. arXiv preprint arXiv: 2505.00703, 2025. [22] Nanye Ma, Shangyuan Tong, Haolin Jia, Hexiang Hu, Yu-Chuan Su, Mingda Zhang, Xuan Yang, Yandong Li, Tommi Jaakkola, Xuhui Jia, and Saining Xie. Inference-time scaling for diffusion models beyond scaling denoising steps. arXiv preprint arXiv: 2501.09732, 2025. [23] Yuta Oshima, Masahiro Suzuki, Yutaka Matsuo, and Hiroki Furuta. Inference-time text-to-video alignment with diffusion latent beam search. arXiv preprint arXiv: 2501.19252, 2025. [24] Raghav Singhal, Zachary Horvitz, Ryan Teehan, Mengye Ren, Zhou Yu, Kathleen McKeown, and Rajesh Ranganath. general framework for inference-time scaling and steering of diffusion models. arXiv preprint arXiv: 2501.06848, 2025. [25] Masatoshi Uehara, Yulai Zhao, Chenyu Wang, Xiner Li, Aviv Regev, Sergey Levine, and Tommaso Biancalani. Inference-time alignment in diffusion models with reward-guided generation: Tutorial and review. arXiv preprint arXiv: 2501.09685, 2025. [26] Ye Tian, Ling Yang, Xinchen Zhang, Yunhai Tong, Mengdi Wang, and Bin Cui. Diffusionsharpening: Fine-tuning diffusion models with denoising trajectory sharpening. arXiv preprint arXiv: 2502.12146, 2025. [27] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback. arXiv preprint arXiv: 2203.02155, 2022. [28] Yuhan Liu, Hanchen Li, Yihua Cheng, Siddhant Ray, Yuyang Huang, Qizheng Zhang, Kuntai Du, Jiayi Yao, Shan Lu, Ganesh Ananthanarayanan, Michael Maire, Henry Hoffmann, Ari Holtzman, and Junchen Jiang. Cachegen: Kv cache compression and streaming for fast large language model serving. Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication, 2023. doi: 10.1145/3651890.3672274. [29] Luohe Shi, Hongyi Zhang, Yao Yao, Zuchao Li, and Hai Zhao. Keep the cost down: review on methods to optimize llm kv-cache consumption. arXiv preprint arXiv: 2407.18003, 2024. 17 [30] Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Mido Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Hervé Jégou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. Dinov2: Learning robust visual features without supervision. Trans. Mach. Learn. Res., 2024, 2024. [31] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. Highresolution image synthesis with latent diffusion models. arXiv preprint arXiv: 2112.10752, 2021. [32] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv: 2204.06125, 2022. [33] Enze Xie, Junsong Chen, Yuyang Zhao, Jincheng Yu, Ligeng Zhu, Yujun Lin, Zhekai Zhang, Muyang Li, Junyu Chen, Han Cai, Bingchen Liu, Daquan Zhou, and Song Han. Sana 1.5: Efficient scaling of training-time and inference-time compute in linear diffusion transformer. arXiv preprint arXiv: 2501.18427, 2025. [34] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. [35] Haonan Qiu, Shiwei Zhang, Yujie Wei, Ruihang Chu, Hangjie Yuan, Xiang Wang, Yingya Zhang, and Ziwei Liu. Freescale: Unleashing the resolution of diffusion models via tuning-free scale fusion. arXiv preprint arXiv:2412.09626, 2024. [36] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. arXiv preprint arXiv: 2206.00364, 2022. [37] Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv: 2011.13456, 2020. [38] Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. arXiv preprint arXiv: 2202.00512, 2022. [39] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. arXiv preprint arXiv: 2303.01469, 2023. [40] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver++: Fast solver for guided sampling of diffusion probabilistic models. arXiv preprint arXiv: 2211.01095, 2022. [41] Jiankai Sun, Chuanyang Zheng, Enze Xie, Zhengying Liu, Ruihang Chu, Jianing Qiu, Jiaqi Xu, Mingyu Ding, Hongyang Li, Mengzhe Geng, et al. survey of reasoning with foundation models: Concepts, methodologies, and outlook. ACM Computing Surveys, 2023. [42] Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding and generation. arXiv preprint arXiv: 2408.12528, 2024. [43] Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, and Wook-Shin Han. Autoregressive image generation using residual quantization. arXiv preprint arXiv: 2203.01941, 2022. [44] Aäron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. Neural discrete representation learning. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett, editors, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pages 63066315, 2017. [45] Yue Zhao, Yuanjun Xiong, and Philipp Krähenbühl. Image and video tokenization with binary spherical quantization. arXiv preprint arXiv: 2406.07548, 2024. 18 [46] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with unified text-to-text transformer. J. Mach. Learn. Res., 21:140:1140:67, 2020. [47] Davin Choo, C. Grunau, Julian Portmann, and Václav Rozhoˇn. k-means++: few more steps yield constant approximation. International Conference on Machine Learning, 2020. [48] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Z. Wojna. Rethinking the inception architecture for computer vision. Computer Vision and Pattern Recognition, 2015. doi: 10.1109/CVPR.2016.308. [49] Tomasz Korbak, Ethan Perez, and C. Buckley. Rl with kl penalties is better viewed as bayesian inference. Conference on Empirical Methods in Natural Language Processing, 2022. doi: 10.48550/arXiv.2205.11275. [50] V. Elvira and Luca Martino. Advances in importance sampling. Wiley StatsRef: Statistics Reference Online, 2021. doi: 10.1002/9781118445112.stat08284. [51] Art Owen and Yi Zhou Associate and. Safe and effective importance sampling. Journal of the American Statistical Association, 95(449):135143, 2000. doi: 10.1080/01621459.2000. 10473909. [52] Kevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, and S. Levine. Training diffusion models with reinforcement learning. International Conference on Learning Representations, 2023. doi: 10.48550/arXiv.2305.13301. [53] Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagereward: Learning and evaluating human preferences for text-to-image generation. Neural Information Processing Systems, 2023. doi: 10.48550/arXiv.2304.05977. [54] Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating text-to-image alignment. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. [55] Kaiyi Huang, Chengqi Duan, Kaiyue Sun, Enze Xie, Zhenguo Li, and Xihui Liu. T2icompbench++: An enhanced and comprehensive benchmark for compositional text-to-image generation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 47:35633579, 2025. doi: 10.1109/TPAMI.2025.3531907. [56] Xiaoshi Wu, Keqiang Sun, Feng Zhu, Rui Zhao, and Hongsheng Li. Human preference score: Better aligning text-to-image models with human preference. IEEE International Conference on Computer Vision, 2023. doi: 10.1109/ICCV51070.2023.00200. [57] Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng Zhu, Rui Zhao, and Hongsheng Li. Human preference score v2: solid benchmark for evaluating human preferences of text-to-image synthesis. arXiv preprint arXiv: 2306.09341, 2023. [58] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. Laion-5b: An open large-scale dataset for training next generation image-text models. arXiv preprint arXiv: 2210.08402, 2022. [59] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. arXiv preprint arXiv: 2103.00020, 2021. [60] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: reference-free evaluation metric for image captioning. Emnlp, 2021. [61] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, 2024. [62] Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv: 2405.09818, 2024."
        }
    ],
    "affiliations": [
        "CUHK",
        "HKU MMLab",
        "Tongyi Lab, Alibaba Group"
    ]
}