{
    "paper_title": "Tool-integrated Reinforcement Learning for Repo Deep Search",
    "authors": [
        "Zexiong Ma",
        "Chao Peng",
        "Qunhong Zeng",
        "Pengfei Gao",
        "Yanzhen Zou",
        "Bing Xie"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Issue localization, the process of identifying code locations that need modification to resolve software issues, is a critical yet challenging task in software development. The semantic gap between natural language issue descriptions and faulty code requires complex multi-hop reasoning through code dependencies. Existing LLM-based agents attempt to address this by integrating repository retrieval tools. However, this transforms issue localization into a demanding task we call Repo Deep Search, which requires the LLM to effectively utilize various repository retrieval tools throughout a multi-step reasoning and navigation process. To tackle this challenge, we present ToolTrain, a two-stage tool-integrated training framework combining rejection-sampled supervised fine-tuning and tool-integrated reinforcement learning to enhance LLMs' ability to use retrieval tools for issue localization. Experimental results show that ToolTrain-trained models achieve state-of-the-art performance, with our 32B model even surpassing Claude-3.7 on function-level localization. The results also show that improved localization performance translates to better end-to-end issue resolution performance. This further demonstrates that training for issue localization is a viable and effective strategy for improving automated software development."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 ] . [ 1 2 1 0 3 0 . 8 0 5 2 : r ToolTrain: Tool-integrated Reinforcement Learning for Repo Deep Search Zexiong Ma, Chao Peng, Qunhong Zeng, Pengfei Gao, Yanzhen Zou, Bing Xie Peking University, ByteDance, Beijing Institute of Technology {mazexiong@stu., zouyz@, xiebing@}pku.edu.cn, {pengchao.x, gaopengfei.se}@bytedance.com, qunhongzeng@bit.edu.cn"
        },
        {
            "title": "Abstract",
            "content": "Issue localization, the process of identifying code locations that need modification to resolve software issues, is critical yet challenging task in software development. The semantic gap between natural language issue descriptions and faulty code requires complex multi-hop reasoning through code dependencies. Existing LLMbased agents attempt to address this by integrating repository retrieval tools, which poses higher demand for LLMs to effectively utilize various tools during multistep reasoning for issue localization. To tackle this challenge, we present ToolTrain, two-stage tool-integrated training framework combining rejection-sampled supervised fine-tuning and tool-integrated reinforcement learning to enhance LLMs ability to use retrieval tools for issue localization. Experimental results show that ToolTrain-trained models achieve state-of-the-art performance, with our 32B model even surpassing Claude-3.7 on function-level localization. The results also show that improved localization performance translates to better end-to-end issue resolution performance. This further demonstrates that training for issue localization is viable and effective strategy for improving automated software development."
        },
        {
            "title": "Introduction",
            "content": "Issue localization is the process of identifying the exact locations in source code that need to be modified to resolve software issue (Yang et al., 2024). Given an issue description (e.g., bug report in natural language), developers must navigate through the entire repository to locate the specific files, functions, or code segments that require changes (Wen et al., 2016). Issue localization is time-consuming and labor-intensive task, particularly for complex, large-scale repositories (Zhu et al., 2022; Youm et al., 2017). This has motivated growing research into automated approaches, and with LLMs remarkable success in software engineering tasks such as code generation (Jiang et al., 2024; Austin et al., 2021; Li et al., 2022) and test synthesis (Wang et al., 2024b; Fakhoury et al., 2024), LLM-based issue localization has emerged as natural and promising direction. Despite this promise, LLM-based issue localization faces challenges. First, issue descriptions and their corresponding faulty code often lack direct semantic connections. Like human developers, LLMs must perform multi-hop reasoning through code dependencies, tracing function calls and variable usages across multiple files to connect high-level issue descriptions with low-level fault locations. Second, enabling LLMs to directly locate faulty code across the entire repository for such Work done during the internship at ByteDance. Corresponding authors. Preprint. Under review. multi-hop localization remains difficult, as it is impractical to input entire repositories into LLMs due to context limitations (Radford et al., 2019). Recent research has addressed this challenge by integrating repository retrieval tools into LLMbased agents (Chen et al., 2025; Yang et al., 2024), allowing LLMs to use various tools (e.g., searching specific function by keyword, listing functions that call certain function) to dynamically explore repositories. However, this approach places higher demands on LLMs: beyond multi-step reasoning, they require sophisticated tool-calling abilities to explore code effectively rather than walking aimlessly through the repository. Current LLMs often struggle with these high-demand requirements, making incorrect tool calls or failing to maintain coherent reasoning chains throughout the exploration process. In this paper, we propose ToolTrain, tool-integrated training framework to enable LLMs to perform more effective multi-hop reasoning with tool call during issue localization. Firstly, we implement RepoSearcher, lightweight and LLM-friendly issue localization agent. RepoSearcher includes simple and easy-to-use retrieval tools that allow LLMs to retrieve function or class definitions by name. To help the LLMs better leverage these tools for multi-hop reasoning, we construct set of labeled issue localization training data from open-source repositories and train the LLMs in two stages: (1) Rejection-sampled supervised fine-tuning (SFT), where the LLM generates tool-use trajectories, and only those leading to correct answers are used for fine-tuning. This stage serves as warm-up to help the LLM learn the task format and how to call the retrieval tool. (2) Toolintegrated reinforcement learning (RL), where the LLM learns through trial and error by exploring different tool-use strategies. We employ rule-based reward signals that evaluate whether the LLMs exploration successfully locates the correct code elements. This direct feedback teaches the model which tool-calling patterns lead to successful localization and which lead to dead ends. Through this iterative process, the LLM learns to make more strategic tool calls at each step, avoiding redundant explorations and focusing on promising code paths. This two-stage training enables the LLMs to master multi-hop reasoning with retrieval tools, yielding more precise issue localization. We applied ToolTrain to train LLMs and compared RepoSearcher with ToolTrain-model against multiple issue localization frameworks (Jiang et al., 2025; Xia et al., 2025; Chen et al., 2025; Yu et al., 2025). The results demonstrate that our method achieves state-of-the-art (SOTA) performance on the issue localization task among same-size LLMs, and on some function-level localization tasks, it even surpasses leading LLMs like Cluade-3.7. We also experimentally validated that more accurate issue localization results lead to better issue resolution performance. Furthermore, through detailed analysis of the ToolTrain process, we found that reinforcement learning (RL) effectively enhances the LLMs tool-calling and reasoning abilities. This enables the model to explore the repository with greater efficiency and precision, allowing it to accurately localize the faulty position. In summary, this paper makes the following main contributions: We design RepoSearcher, lightweight issue localization agent, to streamline the training of issue localization agents. We propose ToolTrain, two-stage tool-integrated training approach, to enhance the reasoning capabilities of LLMs during the tool-calling process. We use ToolTrain to train open-source LLMs and validate the effectiveness of RepoSearcher with ToolTrain-model."
        },
        {
            "title": "2 Background and Motivation",
            "content": "In this section, we first introduce issue localization task and issue localization agent. Then we present two LLM post-training techniques: rejection-sampled supervised fine-tuning and rule-based reinforcement learning. 2."
        },
        {
            "title": "Issue Localization",
            "content": "Issue localization (Chen et al., 2025) refers to the task of identifying faulty file or code snippets in repository based on natural language issue descriptions (e.g., bug reports). Issue localization is critical step in the automated issue resolution pipeline, as accurate localization can significantly reduce the time and effort developers spend understanding and fixing problems. However, issue 2 Figure 1: RepoSearcher framework. Through multiple rounds of tool calls, RepoSearcher navigates across different functions within to gather relevant information. Once LLMs determine that all necessary information is collected, it calls the Exit tool and provides the final localization results. localization poses several challenges. On one hand, there is often significant semantic gap between the issue description and the actual faulty code. The issue typically describes user-facing symptoms, while the root cause may lie deep in the underlying implementation (Li et al., 2021c). On the other hand, modern software systems are large and structurally complex, making exhaustive code search both inefficient and impractical. To address these challenges, recent studies have explored LLM-based issue localization agents (Chen et al., 2025; Jiang et al., 2025; Yu et al., 2025), which allow LLMs to interact with code retrieval tools and perform multi-turn information gathering and reasoning, enabling more precise issue localization. The core idea behind these approaches is to empower LLMs with the ability to actively issue retrieval commands, dynamically query relevant parts of the repository based on context, and progressively narrow down the search space. However, existing LLMs are generally not trained specifically for tool-augmented reasoning, which limits their efficiency in using tools for issue localization. This problem is particularly pronounced when the issue descriptions are incomplete or ambiguous, the model often struggles to accurately understand the problem and make successful tool calls. Therefore, key research challenge is to enhance the tool call accuracy of LLMs and their ability to reason over retrieved information during issue localization."
        },
        {
            "title": "2.2 LLM Post-Training",
            "content": "LLM post-training refers to the further training LLMs with (question, answer) training data after the pretraining phase. Post-Training aims to enhance the models performance in specific tasks or usage scenarios. In this section, we will introduce two mainstream LLM post-training techniques: rejection sampled supervised fine-tuning and rule-based reinforcement learning."
        },
        {
            "title": "2.2.1 Rejection-Sampled Supervised Fine-tuning",
            "content": "Rejection-sampled supervised fine-tuning (SFT) (Pan et al.) is post-training approach that improves LLMs performance with three-stage pipeline: sampling, filtering, and fine-tuning. First, the LLMs generate multiple candidate responses for given question. Next, filtering mechanism evaluates these responses against ground-truth answer, selecting only the high-quality responses. Finally, this curated set of question-answer pairs is used as the dataset for supervised fine-tuning. In agent-based settings, an agent is used to generate multiple tool call trajectories. High-quality trajectories are selected for fine-tuning based on whether the final result matches the ground-truth answer. This approach could improve the models ability to call tools and complete tasks effectively. However, key limitation of SFT is that it only leverages successful (i.e., high-quality) trajectories during training, while ignoring the negative supervision signals embedded in failed trajectories. This onesided training paradigm can lead the model to memorize superficial patterns from small number of successful examples (Chu et al., 2025), rather than truly learning the reasoning logic behind effective tool use. As result, the fine-tuned model may suffer from limited generalization, especially when faced with out-of-distribution tasks, complex reasoning paths, or novel tool combinations (Schick et al., 2023)."
        },
        {
            "title": "2.2.2 Rule-based Reinforcement Learning",
            "content": "Rule-based reinforcement learning (RL) (Guo et al., 2025) involves three-stage process: sampling, scoring, and training. Initially, the LLM samples set of responses to prompt. These responses 3 Table 1: Repository search tools for RepoSearcher. API name GetRepoStructure ( ) GetImportOfFile (file) SearchClass (file, class) SearchFunction (file, function) SearchClassMethod (file, class, method) Exit ( ) Output Description The repository file structure. Get the repository file structure. The imports of file. Get the imports of given file. Code content of the searched class. Search for the content of class in the file. Search for the content of function in the file. Code content of the searched function. Search for the content of method in the class of file. Code content of the searched method. Exit if LLM have found all the information needed. - are then evaluated against ground-truth answer to produce reward score. This score is subsequently used as the supervision signal to fine-tune the model via reinforcement learning. Rule-based reinforcement learning evaluates the quality of reasoning processes solely based on the correctness of the final answer, using reinforcement learning to encourage reasoning paths that lead to correct answers while discouraging those that produce incorrect ones. This approach has proven effective in enhancing LLMs reasoning abilities in complex tasks (Ma et al., 2025). Both OpenAI-o1 (Jaech et al., 2024) and DeepSeek-R1 (Guo et al., 2025) have employed RL to improve LLMs reasoning capabilities. In agent-oriented scenarios, rule-based reinforcement learning enables the agent to sample multiple reasoning trajectories (including tool invocations) and optimizes the entire trajectory through reinforcement learning based on scoring results. Several existing works have explored RL to improve LLM-based agents, such as OpenAIs Deep Research (dee, 2025) and Operator (ope, 2025), Anthropics Claude Code (cla, 2025c), and open-source efforts like Search-R1 (Jin et al., 2025). Compared to fine-tuning methods that only utilize high-quality trajectories, RL not only improves the models ability to generate correct trajectories by reinforcing high-scoring paths, but also reduces the likelihood of low-quality outputs by penalizing incorrect trajectories through low scores. By leveraging both positive and negative samples, RL makes more efficient use of the LLMs sampling outputs, significantly enhancing the models generalization capabilities (Xie et al., 2025; Chu et al., 2025)."
        },
        {
            "title": "2.3 Motivation",
            "content": "Issue localization is critical task in software engineering that requires LLM-based agents to make complex reasoning, and calling various search tools to navigate repositories. However, current LLMs exhibit poor performance on this task due to the lack of tool-integrated training. Their ability to reason throughout the tool-calling process remains significant bottleneck. To address this gap, we explore how to effectively enhance the tool-calling capabilities of LLMs through post-training. straightforward approach is rejection-sampled supervised fine-tuning (SFT), which can directly and simply improve an LLMs performance on issue localization. However, we contend that excessive fine-tuning risks compromising the models generalization capabilities, potentially harming the broad reasoning skills that make LLMs powerful in the first place. On the other hand, Rule-based Reinforcement Learning (RL) holds great potential for enhancing an agents capabilities in issue localization. However, major challenge lies in constructing suitable training data for an issue localization agent. Specifically, ensuring the reliability of ground-truth answers is difficult and underexplored problem. In this paper, we aim to tackle these challenges head-on. First, we will investigate methods for constructing high-quality training data specifically designed for the issue localization task. Second, we will explore how SFT and RL can be effectively combined. Our goal is to develop hybrid training methodology that leverages the strengths of both approaches to significantly enhance the issue localization capabilities of LLM-based agents."
        },
        {
            "title": "3 Approach",
            "content": "In this section, we will introduce our approach, which consists of three components. (1) Issue Localization Agent: We designed RepoSearcher, which contains set of simple yet efficient tools for the issue localization task to reduce the difficulty for LLMs in acquiring repository context. (2) Training Data Construction: We constructed training data for issue localization based on (issue, pull request) data from open-source communities. (3) Tool-integrated Training: We propose ToolTrain, leverages our constructed data to perform tool-integrated training for LLMs. This training process enables the models to efficiently use tools for reasoning, further improves their accuracy in issue localization. 4 Figure 2: Tool-integrated training framework. It consists of two stages: (1) rejection-sampled supervised fine-tuning (SFT) and (2) rule-based reinforcement learning (RL). 3."
        },
        {
            "title": "Issue Localization Agent",
            "content": "An LLM-based agent capable of excelling at issue localization and amenable to tool-integrated training should be designed with the following characteristics: (1) Well-Defined and Usable Tools: The tools must be clearly defined, simple to use, and enable the LLM to accurately retrieve content from the target repository. (2) Concise Trajectories: The localization should not require an excessive number of tool calls. This is critical to prevent the generation of overly long trajectories, which cannot be fully utilized as complete samples for training the LLM. As shown in Table 1, we designed set of simple and easy-to-use search tools for retrieving repository content. This suite includes six tools: GetRepoStructure, GetImportOfFile, SearchClass, SearchFunction, SearchClassMethod, and Exit. The GetRepoStructure and GetImportOfFile tools are designed to quickly and accurately provide the repositorys structural information and dependencies. This helps the LLM understand the overall repository architecture and perform coarse-grained localization. On the other hand, SearchClass, SearchFunction, and SearchClassMethod provide the model with specific code snippets from the repository. This allows the LLM to inspect the content of specific functions and perform fine-grained localization. Once the LLM determines that it has gathered sufficient information, it invokes the Exit tool to conclude the search process. As shown in Figure 1, we propose RepoSearcher, lightweight issue localization agent. Initially, RepoSearcher generates preliminary thought based on the input issue description and invokes retrieval tool to fetch content from the repository. Based on the retrieved results, it proceeds with the next cycle of reasoning and tool invocation. Through multiple rounds of tool calls, RepoSearcher navigates across different functions within the project to gather relevant information. Once the LLM determines that all necessary information has been collected, it calls the Exit tool and provides the final issue localization result based on the accumulated context."
        },
        {
            "title": "3.2 Training Data Construction",
            "content": "In this paper, we construct training data for the issue localization task based on (issue, pull request) pairs from high-quality projects on GitHub. We select 600 high-quality repositories from GitHub according to the following criteria: (1) at least 1,000 issues; (2) at least 1,000 pull requests; (3) at least 100 stars; (4) inclusion of an appropriate license. To prevent data leakage, we excluded repositories and issues that appeared in SWE-Bench-Verified (our evaluation dataset, see Section 4.1 for details). From these repositories, we construct issue localization training data by pairing resolved issues with their corresponding pull requests. To ensure data quality, we apply several filtering criteria: First, we discard issues with fewer than 100 characters to ensure sufficient descriptive content. Second, we filter out pull requests that only contain documentation updates, configuration changes, or other non-code modifications, focusing on modifications to the actual source code. Through this selection process, we constructed around 28k high-quality localization examples. For each example, we form the question from the issue description and use the source code files and functions modified in the corresponding pull request as the ground-truth answer Aq (excluding any non-source code elements). 5 By leveraging authentic, high-quality, and successfully resolved issues as the foundation of our training data, we ensure both the relevance and reliability of our dataset, which in turn drives the effectiveness and stability of our training pipeline."
        },
        {
            "title": "3.3 Tool-integrated Training",
            "content": "We propose ToolTrain, two-stage tool-integrated training framework, to enhance the LLMs ability to reason and interact with external tools effectively. It contains two training stages: (1) Rejection-sampled supervised fine-tuning (SFT). We fine-tune the LLM exclusively on high-quality trajectories selected via rejection sampling. This process equips the model with foundational understanding of task formats, tool invocation methods, and core reasoning strategies. (2) Rule-based reinforcement learning (RL), we score the trajectories sampled by the LLM to increase the probability of it generating correct trajectories while simultaneously discouraging it from producing incorrect ones. This further improves the LLMs multi-hop reasoning capabilities during the tool-calling process."
        },
        {
            "title": "3.3.1 Rejection-Sampled Supervised Fine-Tuning",
            "content": "We use RepoSearcher to sample agent trajectories for issue localization task, and then select highquality trajectories to fine-tune the LLM. More specifically, given the task description of issue, LLM invokes search tools to retrieve relevant context and generates localization result. We then filter out low-quality trajectories that completely fail to hit the ground-truth answer, and use the remaining high-quality trajectories to fine-tune LLM. The main goal of this stage is to warm up LLM for tool use, enabling it to understand the task format of project retrieval, how to use the tools, and the basic reasoning strategies for issue localization."
        },
        {
            "title": "3.3.2 Tool-integrated Reinforcement Learning",
            "content": "Although SFT can quickly teach LLM the strategy for issue localization, the fine-tuned LLM may simply memorize the correct reasoning paths, leading to poor generalization when faced with new issues. We employ tool-integrated reinforcement learning to robustly enhance the LLMs reasoning capabilities during tool interaction. More specifically, the LLM samples tool-use trajectories during the issue localization process, and each trajectorys predicted answer is scored based on its alignment with the ground-truth answer. These scores serve as reward signal, guiding the models update through reinforcement learning. By incentivizing high-quality trajectories and penalizing low-quality ones, this RL framework not only polishes the accuracy of tool interactions but also teaches the model to avoid erroneous or illogical tool calls. Consequently, it simultaneously enhances both the reliability and efficiency of the entire tool-use process. We utilize nDCG@k (Normalized Discounted Cumulative Gain at rank k) (Wang et al., 2013) to measure trajectory quality, as it holistically assesses both the recall of correct functions and their ranking position. In the issue localization training process, the Agent is expected to output ranked list of functions that are potentially related to given issue. We are not only concerned with whether the Agent can cover all correct target functions, but also with whether it can rank these functions as high as possible in the list. This is crucial because, in practical scenarios, downstream systems (e.g., automated program repair agents (Wang et al., 2024a; Yang et al., 2024; Chen et al., 2024)) typically prioritize only the top-ranked candidate functions. If the correct functions are ranked too low, they may be ignored by repair agents. Therefore, we use nDCG@k as the reward function to evaluate the ranking quality of the predicted list relative to the ground-truth set. nDCG@k is calculated by Equation 1, it assigns different gains based on the positions of correctly predicted functions in the output list, functions ranked higher contribute more to the final score. By incorporating nDCG@k as reward signal in the reinforcement learning process, we effectively encourage the Agent not only to find the correct functions but also to rank the correct functions higher, which aligns more closely with the practical requirements of real-world issue localization scenarios. nDCG@k(q) = DCG@k(q) IDCG@k(q) (1) DCG@k(q) = (cid:88) i=1 I(Lq[i] Aq) log2(i + 1) IDCG@k(q) = min(k,Aq) (cid:88) i=1 1 log2(i + 1) (2) (3) where Lq[i] is the item at rank in the predicted list for query q, Aq is the set of ground-truth relevant items for query q, I() is the indicator function that returns 1 if the condition holds, and 0 otherwise."
        },
        {
            "title": "4 Experimental Setup",
            "content": "In this section, we will introduce our experimental setup, including: benchmark, evaluation metrics, baselines and implementation details."
        },
        {
            "title": "4.1 Benchmark",
            "content": "In this paper, we construct our evaluation dataset based on SWE-Bench-Verified3. SWE-BenchVerified is benchmark designed to evaluate the issue resolution capabilities of LLM-based Agents, built from real issues collected from GitHub. To ensure accurate and reliable evaluation of LLMbased Agents on issue resolution, OpenAI invited 93 professional software developers to analyze and manually verify 2,000 real issues from GitHub. Based on this effort, curated set of 500 verifiably solvable issues was constructed to form the SWE-Bench-Verified dataset. It has been adopted by leading large language model providers (e.g., OpenAI (gpt, 2023), Qwen (Hui et al., 2024), and Anthropic (cla, 2025b)) to assess the performance of cutting-edge models in automatic software development tasks. In this paper, we use the functions and files modified by the golden patches in SWE-Bench-Verified as the ground-truth answers for the issue localization task, to evaluate the performance of our RepoSearcher with ToolTrain-model."
        },
        {
            "title": "4.2 Metrics",
            "content": "To systematically evaluate the effectiveness of RepoSearcher with ToolTrain-model, we conducted multi-dimensional evaluation using multiple metrics. Recall@k Recall@k measures the proportion of ground-truth answers that are included in the top-k localization results. Previous study have shown that 73.58% of developers only examine the top-5 localization results (Kochhar et al., 2016). Therefore, following previous work (Jiang et al., 2025), we set the maximum value of to 5 and evaluate the performance at Recall@1, Recall@3, and Recall@5. MAP MAP (Mean Average Precision) evaluates the overall ranking quality by averaging the Average Precision (AP) across all issues. For each issue, AP measures the average of the precision values at the ranks where relevant (ground-truth) items occur. This metric reflects both the presence and the ranking of relevant results, and is calculated as Equation 5. Then, MAP is computed as Equation 4. MAP = 1 (cid:88) qQ AP(q), AP(q) = 1 Aq Lq (cid:88) k= Precision@k I[Lq[k] Aq], (4) (5) where is the set of all queries. For query Q, let Aq be the set of ground-truth relevant items. AP(q) refers to the Average Precision for query q, Lq refers to the predicted ranked list for query q, Precision@k is the precision at rank k, I[Lq[k] Aq] is an indicator function that is 1 if the item at rank is relevant, and 0 otherwise. 3https://openai.com/index/introducing-swe-bench-verified/ MRR MRR (Mean Reciprocal Rank) evaluates how early the ground-truth item appears in the ranked list. It is the average of the reciprocal ranks of the ground-truth results across all queries. The higher the MRR, the better the system is at placing the correct result near the top. MRR = 1 (cid:88) qQ 1 rankq , (6) where rankq is the rank position of the ground-truth result for query q. nDCG@k The nDCG@k (Normalized Discounted Cumulative Gain at rank k) score is computed as the mean nDCG@k(q) over all queries: nDCG@k = 1 (cid:88) qQ nDCG@k(q). (7) %Resolved The %Resolved is used to evaluate the accuracy of generated patch in RQ3. %resolved calculates the proportion of generated patches that pass all unit tests."
        },
        {
            "title": "4.3 Baselines",
            "content": "To evaluate the effectiveness of RepoSearcher with the ToolTrain-model on issue localization, we conduct comparisons against multiple issue localization frameworks across various models."
        },
        {
            "title": "4.3.1 Frameworks",
            "content": "We selected four state-of-the-art issue localization frameworks with different design philosophies as baselines: Agentless(Xia et al., 2025), CrcaLoca(Yu et al., 2025), CoSIL(Jiang et al., 2025), LocAgent(Chen et al., 2025). See Appendix for detailed description of baseline frameworks."
        },
        {
            "title": "4.3.2 Models",
            "content": "We applied ToolTrain to the Qwen2.5-Coder-7B-Instruct (hereafter Qwen-7B) and Qwen2.5-Coder32B-Instruct (hereafter Qwen-32B) models (Hui et al., 2024) and compared them against several baselines. For frameworks without their own fine-tuned versions (Agentless, Orcaloca, and CoSIL), we used the off-the-shelf Qwen-7B and Qwen-32B models. For LocAgent, we used its specialized CL7B and CL-32B models. Our evaluation also included comparison between our ToolTrain-models and state-of-the-art proprietary models like GPT-4o and Claude-3.7-Sonnet(cla, 2025a). 4."
        },
        {
            "title": "Implementation Details",
            "content": "Among 28k issue localization examples, 10k examples were used to sample RepoSearcher trajectories with Claude-3.7-Sonnet (cla, 2025a). Low-quality trajectories were filtered based on ground-truth data, resulting in 5k high-quality trajectories for SFT. The remaining 18k examples were used for RL training. We use verl (Sheng et al., 2025) for training in both the SFT and RL stages. In the SFT stage, we set the train_batch_size to 128 and train for 3 epochs on the 5k training examples. For the RL stage, we adopt the SGLang4 inference framework to accelerate the sampling process and set the temperature to 1.0 to encourage response diversity. We set train_batch_size to 128, with max_prompt_length set to 12k and max_response_length to 20k, and train for one epoch on 18k examples."
        },
        {
            "title": "5 Results and Analysis",
            "content": "In this section, we will present the experimental results for the three research questions, and experimental analysis. 4https://github.com/sgl-project/sglang 8 Table 2: Issue localization results on SWE-Bench-Verified. Blue background indicates the results of ToolTrain-model; bold numbers denote the best performance among same-size models, while underlined numbers indicate the best performance across all models. Framework Model File-level Function-level Recall@1 Recall@3 Recall@5 MAP MRR nDCG@5 Recall@1 Recall@3 Recall@5 MAP MRR nDCG@5 Agentless Agentless CoSIL LocAgent GPT-4o Claude-3.7-Sonnet Claude-3.7-Sonnet Claude-3.7-Sonnet RepoSearcher Claude-3.7-Sonnet Agentless OrcaLoca LocAgent LocAgent CoSIL CoSIL RepoSearcher RepoSearcher Agentless OrcaLoca LocAgent LocAgent CoSIL CoSIL RepoSearcher RepoSearcher Qwen-7B Qwen-7B Qwen-7B CL-7B Qwen-7B ToolTrain-7B Qwen-7B ToolTrain-7B Qwen-32B Qwen-32B Qwen-32B CL-32B Qwen-32B ToolTrain-32B Qwen-32B ToolTrain-32B 62.28 66.84 70.69 68.58 69. 47.81 47.69 46.78 56.11 51.11 53.13 38.33 59.78 58.56 56.78 64.15 67.99 59.34 65.61 58.79 68. 79.41 82.48 86.07 84.36 87.09 61.59 50.29 58.98 72.11 65.28 67.35 53.23 77.89 74.67 61.21 77.12 84.29 78.73 83. 76.33 85.31 Proprietary Models 73.02 76.47 79.73 77.64 79.39 75.72 79.76 82.87 79.57 82.31 76.83 79.97 82.96 79.95 82.73 Open-source 7B Models 55.09 49.09 53.11 65.68 59.43 61.54 46.36 70.43 57.85 52.40 56.20 68.61 62.74 64.33 49.20 73. 58.01 50.14 55.76 68.87 62.95 64.95 49.95 74.44 Open-source 32B Models 68.03 59.16 71.44 73.04 69.88 76. 68.77 78.20 71.14 63.43 74.13 76.09 73.02 79.54 71.72 80.86 71.75 60.63 73.79 75.31 73.92 80. 72.67 81.60 85.01 86.76 89.13 88.53 89.24 63.76 50.29 60.40 75.18 69.93 72.09 57.73 83.11 79.50 61.21 78.02 87. 82.68 88.07 81.03 88.59 28.50 36.09 50.01 41.75 48.55 19.73 16.44 10.91 24.82 23.37 34.86 18.12 43. 26.30 19.94 20.97 39.41 38.62 47.42 32.38 49.94 43.56 51.21 61.56 53.67 61.57 24.12 27.23 16.72 30.29 29.48 46. 24.54 57.32 38.85 38.91 40.00 52.49 50.27 60.85 44.35 64.26 46.55 53.83 66.38 60.59 66.08 25.10 28.57 17.72 34. 30.47 50.87 25.82 62.38 40.70 40.94 45.34 59.69 54.41 64.22 48.19 68.55 37.17 45.12 58.43 47.25 57. 22.29 22.51 13.93 27.81 26.95 42.49 21.60 53.28 33.32 30.20 31.61 46.07 45.96 56.29 39.45 59. 43.32 52.54 67.28 51.36 65.94 27.16 26.41 16.50 33.56 32.44 48.44 24.91 60.44 39.44 35.57 36.47 50.22 53.94 64. 45.89 67.35 41.45 49.49 63.17 51.85 62.53 24.23 25.27 15.59 32.99 29.27 46.50 23.53 57.91 37.03 34.69 36.61 50. 50.34 60.80 43.50 64."
        },
        {
            "title": "5.1 Effectiveness",
            "content": "Setup To evaluate the performance of RepoSearcher with the ToolTrain-model on the issue localization task, we compared it with four different issue localization frameworks (Agentless, Orcaloca, CoSIL, and LocAgent) using different models (Qwen-7B, Qwen-32B, and Claude-3.7-Sonnet). We conducted multi-dimensional assessment of the various methods in terms of file localization and function localization, including multiple evaluation metrics: recall@1, recall@3, recall@5, MAP, MRR, and nDCG@5. Results As shown in Table 2, RepoSearcher with ToolTrain-model achieves state-of-the-art (SOTA) performance among same-size models and even surpasses leading commercial models on some function-level metrics. Although the RepoSearcher underperformed other agent frameworks when using the original Qwen models due to its simpler agent design, after applying ToolTrain, it achieves SOTA performance among same-size models. For example, RepoSearcher with ToolTrain-32B achieves 68.55 at function-level Recall@5. Furthermore, RepoSearcher with ToolTrain-model also demonstrates superior performance compared to CL-7B and CL-32B, which were trained specifically for the LocAgent. This demonstrates that ToolTrain can effectively enhance the issue localization capability of LLMs and offers advantages over previous training approaches. On function-level localization, RepoSearcher with ToolTrain-7B even outperforms other frameworks that use 32B models. This indicates that training with ToolTrain effectively enhances an LLMs toolcalling capabilities, allowing model with smaller parameter count to achieve better performance than much larger one. It is also noteworthy that RepoSearcher with ToolTrain-32B achieves performance on issue localization comparable to Claude-3.7-Sonnet. It even surpasses the commercial model on several fine-grained, function-level metrics (e.g., 68.55 v.s. 66.38 on function-level Recall@5). This detailed level of accuracy is potentially more impactful for downstream issue resolution tasks. This suggests that RepoSearcher with ToolTrain-model has the potential to serve as lightweight alternative to proprietary models like Claude (cla, 2025a). To validate the generalization ability of the ToolTrain-model, we also evaluated its performance on the CoSIL benchmark. As shown in Table 2, the ToolTrain-model achieves better performance on CoSIL compared to the original Qwen-model (e.g., 50.87 v.s. 30.47 on function-level Recall@5). This indicates that the ToolTrain-model, trained on RepoSearcher, can successfully generalize its issue localization capabilities to other agents. 9 (a) Training strategy ablation on 7B models. (b) Training strategy ablation on 32B models. Figure 3: Training strategy ablation. Table 3: Issue resolution performance of different issue localization frameworks. Patch Model Framework Func Recall@5 %Resolved Qwen-7B Agentless Qwen-7B CoSIL Qwen-7B RepoSearcher Qwen-32B Agentless Qwen-32B CoSIL Qwen-32B RepoSearcher Loc Model Qwen-7B Qwen-7B ToolTrain-7B Qwen-32B Qwen-32B ToolTrain-32B 7.60 10.80 14.00 25.80 26.40 31.60 25.10 30.47 62.38 40.70 54.41 68."
        },
        {
            "title": "5.2 Training Strategy Ablation",
            "content": "Setup To evaluate the effectiveness of ToolTrain, we compared it with two other training strategies: (1) SFT: Directly performing Supervised Fine-Tuning on the LLM using the trajectories sampled in the first stage of ToolTrain. (2) RL: Directly applying tool-integrated reinforcement learning to the LLM without any prior SFT. We applied these three training strategies to both 7B and 32B models. We then evaluated the issue localization performance of the different models on the RepoSearcher framework. Results As shown in Figure 3, ToolTrain model outperforms both SFT and RL models of the same parameter size in issue localization. Although training an LLM with either RL or SFT alone effectively improves issue localization performance, ToolTrains combination of the two further boosts the LLMs performance across all issue localization metrics. For example, the base 7B models file-level recall@5 is 57.73. This increases to 73.18 for RL-7B and 79.31 for SFT-7B, while ToolTrain-7B pushes it even further to 83.11. Figure 3 also indicates that standalone SFT yields greater improvement than standalone RL. This can be attributed to the learning dynamics of each method. The RL process requires the model to first discover how to properly invoke tools through extensive exploration, leaving less capacity to refine its reasoning policies. SFT, on the other hand, explicitly provides examples of both correct tool usage and effective reasoning strategies, allowing the model to learn more efficiently. Consequently, standalone SFT delivers larger improvement in issue localization. 5."
        },
        {
            "title": "Influence on issue resolution",
            "content": "Setup To evaluate the impact of different issue localization results on the final issue resolution performance, we use the localization results from various methods on SWE-Bench-Verified as input. We then use the patch generation script of agentless to generate patches. For localization methods employing 7B models, we use Qwen-7B as the patch generation model, and for those using 32B models, we use Qwen-32B. Following the standard evaluation method of SWE-Bench-Verified, patch is considered to have successfully resolved the corresponding issue if it passes all unit tests. Based on this, we assess the impact of different localization methods on issue resolution using the issue resolution rate (%Resolved). 10 Results As shown in Table 3, more precise issue localization leads to better issue resolution results. RepoSearcher with ToolTrain-32B achieves function-level localization recall@5 of 68.55, and when using Qwen-32B as the patch generation model, it attains an issue resolution rate of 31.60, the best among all methods. Similarly, RepoSearcher with ToolTrain-7B achieves function-level localization recall@5 of 62.38, and with Qwen-7B as the patch generation model, its issue resolution rate is 14.00, marking the best performance among the 7B models. The table also reveals that despite the comparable localization results between RepoSearcher with ToolTrain-32B and RepoSearcher with ToolTrain-7B, the use of different patch generation models results in significant disparity in their resolution outcomes (14.00 vs. 31.60). This indicates that the capability of the patch generation model is also crucial for issue resolution. Therefore, future work should focus on further enhancing the patch generation capabilities of models."
        },
        {
            "title": "6.1 Fault Localization",
            "content": "Fault localization refers to locating faulty code snippet within repository based on failed test (Raselimo & Fischer, 2019; Abreu et al., 2009; Qin et al., 2025; Wen et al., 2019) or an issue description (Jimenez et al., 2024). DeepFL (Li et al., 2019) utilizes multiple fault-diagnosis dimensions, and combines them with deep neural network to localize faulty code. DeepRL4FL (Li et al., 2021a) works by encoding the test coverage matrix into feature matrix. It then integrates this with inter-statement data dependencies and static representations of the code, using Convolutional Neural Network (CNN) (Li et al., 2021b) to automatically identify and locate buggy code statements or methods. With the advancement of large language models in language understanding and reasoning, an increasing number of studies have started focusing on fault localization based on issue description. These range from lightweight pipelines to autonomous agents. Agentless (Xia et al., 2025) represents the pipeline approach, using hierarchical strategy that combines LLMs and retrievers to progressively narrow down from files to fine-grained code locations. In parallel, agent-based systems have gained traction. key strategy is representing code as graph to guide the agent. LocAgent (Chen et al., 2025) preprocesses the repository into comprehensive static graph to enable efficient multi-hop reasoning. In contrast, CoSIL (Jiang et al., 2025) dynamically constructs module call graphs during its search, enabling iterative exploration. Another line of research, exemplified by OrcaLoca Yu et al. (2025), focuses on enhancing the agents internal mechanics through priority-based action scheduling, action decomposition, and distance-aware context pruning to boost accuracy. In this paper, we introduce RepoSearcher, lightweight issue localization agent. Its lightweight design enables the rapid sampling of numerous trajectories, allowing us to efficiently train an LLM specifically tailored for the agent."
        },
        {
            "title": "6.2 Agentic Training",
            "content": "Although Large Language Models (LLMs) have achieved near-human performance in many general domains, their performance in issue resolution as agents still lags significantly behind that of humans due to lack of agent-specific training. Many works have attempted to post-train LLMs for agentoriented tasks to enhance their ability to use tools, reason, and complete tasks. The most critical challenge in this process is ensuring the quality of the training data. To address this, SWE-Gym (Pan et al.) constructs an environment of real Python repositories, executable environments, and unit tests. It filters for high-quality agent trajectories based on the outcomes of unit test executions and uses these trajectories to supervised fine-tune the LLM, thereby enhancing its agent capabilities. Similarly, SEAlign (Zhang et al., 2025) proposes an alignment training framework for software engineering agents. It gathers high-quality engineering process trajectories, employs Monte Carlo Tree Search (MCTS) ( Swiechowski et al., 2023) for fine-grained scoring during multi-step decision-making, and combines this with preference optimization on key actions to supervise the fine-tuning of the LLM. LocAgent (Chen et al., 2025) also constructs the ground truth for issue localization based on functions modified by golden patches from GitHub. This is used to filter out low-quality trajectories sampled by LocAgent to ensure the quality of the training data. While the training data in prior work was mostly built upon existing issues, SWE-smith (Yang et al., 2025) first establishes an execution environment from any Python repository and then generates large number of task instances that break existing 11 tests. This allows for the efficient synthesis of dataset with hundreds of thousands of tasks, each with execution-based validation. While previous work has primarily used SFT to train LLMs (Chen et al., 2025; Yang et al., 2025), in this paper, we explore how to organically combine SFT and RL to further enhance the LLMs issue localization capabilities. To the best of our knowledge, we are the first work to organically combine SFT and RL for training an issue localization agent."
        },
        {
            "title": "7 Conclusion",
            "content": "In this paper, we improve the issue localization performance of LLMs by introducing ToolTrain, tool-integrated training approach to enhance their tool-use and reasoning. Our framework first utilizes supervised fine-tuning to warm up the model with our lightweight agent, RepoSearcher, and then employs tool-integrated reinforcement learning to teach the model how to effectively navigate code repositories using various tools. We apply ToolTrain to open-source models, and evaluate them on issue localization tasks. The experiment results show that ToolTrain trained model achieve state-of-the-art performance among same-size models, and even surpassing leading models like Claude-3.7 on specific tasks."
        },
        {
            "title": "Acknowledgments",
            "content": "This work is supported by National Key Research and Development Program of China (Grant No. 2023YFB4503803)."
        },
        {
            "title": "References",
            "content": "Gpt-4. https://openai.com/index/gpt-4-research/, 2023. Claude-3.7. https://www.anthropic.com/news/claude-3-7-sonnet, 2025a. Claude-4. https://www.anthropic.com/news/claude-4, 2025b. Claude code: Deep coding at terminal velocity. https://www.anthropic.com/claude-code, 2025c. Introducing deep research. https://openai.com/index/introducing-deep-research/, 2025. Introducing operator. https://openai.com/index/introducing-operator/, 2025. Rui Abreu, Peter Zoeteweij, and Arjan JC Van Gemund. Spectrum-based multiple fault localization. In 2009 IEEE/ACM International Conference on Automated Software Engineering, pp. 8899. IEEE, 2009. Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021. Dong Chen, Shaoxin Lin, Muhan Zeng, Daoguang Zan, Jian-Gang Wang, Anton Cheshkov, Jun Sun, Hao Yu, Guoliang Dong, Artem Aliev, et al. Coder: Issue resolving with multi-agent and task graphs. arXiv preprint arXiv:2406.01304, 2024. Zhaoling Chen, Xiangru Tang, Gangda Deng, Fang Wu, Jialong Wu, Zhiwei Jiang, Viktor Prasanna, Arman Cohan, and Xingyao Wang. Locagent: Graph-guided llm agents for code localization. arXiv preprint arXiv:2503.09089, 2025. Tianzhe Chu, Yuexiang Zhai, Jihan Yang, Shengbang Tong, Saining Xie, Dale Schuurmans, Quoc Le, Sergey Levine, and Yi Ma. Sft memorizes, rl generalizes: comparative study of foundation model post-training. arXiv preprint arXiv:2501.17161, 2025. Sarah Fakhoury, Aaditya Naik, Georgios Sakkas, Saikat Chakraborty, and Shuvendu Lahiri. Llm-based test-driven interactive code generation: User study and empirical evaluation. IEEE Transactions on Software Engineering, 2024. 12 Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Keming Lu, et al. Qwen2. 5-coder technical report. arXiv preprint arXiv:2409.12186, 2024. Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. Juyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim. survey on large language models for code generation. arXiv preprint arXiv:2406.00515, 2024. Zhonghao Jiang, Xiaoxue Ren, Meng Yan, Wei Jiang, Yong Li, and Zhongxin Liu. Cosil: Software issue localization via llm-driven code repository graph searching, 2025. URL https://arxiv. org/abs/2503.22424. Carlos Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. SWE-bench: Can language models resolve real-world github issues? In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/ forum?id=VTF8yNQM66. Bowen Jin, Hansi Zeng, Zhenrui Yue, Jinsung Yoon, Sercan Arik, Dong Wang, Hamed Zamani, and Jiawei Han. Search-r1: Training llms to reason and leverage search engines with reinforcement learning. arXiv preprint arXiv:2503.09516, 2025. Pavneet Singh Kochhar, Xin Xia, David Lo, and Shanping Li. Practitioners expectations on automated fault localization. In Proceedings of the 25th international symposium on software testing and analysis, pp. 165176, 2016. Xia Li, Wei Li, Yuqun Zhang, and Lingming Zhang. Deepfl: Integrating multiple fault diagnosis dimensions for deep fault localization. In Proceedings of the 28th ACM SIGSOFT international symposium on software testing and analysis, pp. 169180, 2019. Yi Li, Shaohua Wang, and Tien Nguyen. Fault localization with code coverage representation learning. In 2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE), pp. 661673. IEEE, 2021a. Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rmi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, et al. Competition-level code generation with alphacode. Science, 378(6624):10921097, 2022. Zewen Li, Fan Liu, Wenjie Yang, Shouheng Peng, and Jun Zhou. survey of convolutional neural IEEE transactions on neural networks and networks: analysis, applications, and prospects. learning systems, 33(12):69997019, 2021b. Zeyan Li, Junjie Chen, Rui Jiao, Nengwen Zhao, Zhijun Wang, Shuwei Zhang, Yanjun Wu, Long Jiang, Leiqin Yan, Zikai Wang, et al. Practical root cause localization for microservice systems via trace analysis. In 2021 IEEE/ACM 29th International Symposium on Quality of Service (IWQOS), pp. 110. IEEE, 2021c. Zexiong Ma, Chao Peng, Pengfei Gao, Xiangxin Meng, Yanzhen Zou, and Bing Xie. Sorft: Issue resolving with subtask-oriented reinforced fine-tuning. arXiv preprint arXiv:2502.20127, 2025. Jiayi Pan, Xingyao Wang, Graham Neubig, Navdeep Jaitly, Heng Ji, Alane Suhr, and Yizhe Zhang. Training software engineering agents and verifiers with swe-gym. In Forty-second International Conference on Machine Learning. Yihao Qin, Shangwen Wang, Yiling Lou, Jinhao Dong, Kaixin Wang, Xiaoling Li, and Xiaoguang Mao. oap fl: standard operating procedure for llm-based method-level fault localization. IEEE Transactions on Software Engineering, 2025. 13 Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. Moeketsi Raselimo and Bernd Fischer. Spectrum-based fault localization for context-free grammars. In Proceedings of the 12th ACM SIGPLAN International Conference on Software Language Engineering, pp. 1528, 2019. Timo Schick, Jane Dwivedi-Yu, Roberto Dess, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. Advances in Neural Information Processing Systems, 36:6853968551, 2023. Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. In Proceedings of the Twentieth European Conference on Computer Systems, pp. 12791297, 2025. Maciej Swiechowski, Konrad Godlewski, Bartosz Sawicki, and Jacek Mandziuk. Monte carlo tree search: review of recent modifications and applications. Artificial Intelligence Review, 56(3): 24972562, 2023. Xingyao Wang, Boxuan Li, Yufan Song, Frank Xu, Xiangru Tang, Mingchen Zhuge, Jiayi Pan, Yueqi Song, Bowen Li, Jaskirat Singh, et al. Opendevin: An open platform for ai software developers as generalist agents. arXiv preprint arXiv:2407.16741, 2024a. Yining Wang, Liwei Wang, Yuanzhi Li, Di He, and Tie-Yan Liu. theoretical analysis of ndcg type ranking measures. In Conference on learning theory, pp. 2554. PMLR, 2013. Zejun Wang, Kaibo Liu, Ge Li, and Zhi Jin. Hits: High-coverage llm-based unit test generation via method slicing. In Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering, pp. 12581268, 2024b. Ming Wen, Rongxin Wu, and Shing-Chi Cheung. Locus: Locating bugs from software changes. In Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering, pp. 262273, 2016. Ming Wen, Junjie Chen, Yongqiang Tian, Rongxin Wu, Dan Hao, Shi Han, and Shing-Chi Cheung. Historical spectrum based fault localization. IEEE Transactions on Software Engineering, 47(11): 23482368, 2019. Chunqiu Steven Xia, Yinlin Deng, Soren Dunn, and Lingming Zhang. Demystifying llm-based software engineering agents. Proc. ACM Softw. Eng., 2(FSE), June 2025. doi: 10.1145/3715754. URL https://doi.org/10.1145/3715754. Tian Xie, Zitian Gao, Qingnan Ren, Haoming Luo, Yuqian Hong, Bryan Dai, Joey Zhou, Kai Qiu, Zhirong Wu, and Chong Luo. Logic-rl: Unleashing llm reasoning with rule-based reinforcement learning. arXiv preprint arXiv:2502.14768, 2025. John Yang, Carlos Jimenez, Alexander Wettig, Kilian Lieret, Shunyu Yao, Karthik Narasimhan, and Ofir Press. Swe-agent: Agent-computer interfaces enable automated software engineering. arXiv preprint arXiv:2405.15793, 2024. John Yang, Kilian Leret, Carlos Jimenez, Alexander Wettig, Kabir Khandpur, Yanzhe Zhang, Binyuan Hui, Ofir Press, Ludwig Schmidt, and Diyi Yang. Swe-smith: Scaling data for software engineering agents. arXiv preprint arXiv:2504.21798, 2025. Klaus Changsun Youm, June Ahn, and Eunseok Lee. Improved bug localization based on code change histories and bug reports. Information and Software Technology, 82:177192, 2017. Zhongming Yu, Hejia Zhang, Yujie Zhao, Hanxian Huang, Matrix Yao, Ke Ding, and Jishen Zhao. Orcaloca: An llm agent framework for software issue localization, 2025. URL https://arxiv. org/abs/2502.00350. Kechi Zhang, Huangzhao Zhang, Ge Li, Jinliang You, Jia Li, Yunfei Zhao, and Zhi Jin. Sealign: Alignment training for software engineering agent. arXiv preprint arXiv:2503.18455, 2025. Ziye Zhu, Hanghang Tong, Yu Wang, and Yun Li. Enhancing bug localization with bug report decomposition and code hierarchical network. Knowledge-Based Systems, 248:108741, 2022."
        },
        {
            "title": "A Tool Call Analysis",
            "content": "Tool-integrated reinforcement learning can effectively improve the tool call success rate of LLMs. We analyzed the progression of the LLMs tool call success rate during the reinforcement learning (RL) training process on 7B model. tool call is deemed successful when it invokes an existing tool with valid parameters (such as filenames and function names that are present in the repository) and successfully retrieves result from it. As shown in Figure 4, the LLMs tool call success rate exhibits steady increase throughout the training, eventually stabilizing at over 95%. This indicates that in tool-integrated reinforcement learning, even when the reward is calculated solely based on the correctness of the final localization result, the models proficiency in tool calling can be effectively enhanced. plausible explanation is that during the RL process, the LLM learns to call tools with greater efficiency and accuracy in order to more effectively acquire the necessary issue-related project context. Figure 4: Tool call success rate during RL training."
        },
        {
            "title": "B Case study",
            "content": "The case in Figure 5 demonstrates that ToolTrain can effectively enhance the tool-calling and reasoning abilities of LLMs in the issue localization process. Figure 5: Case study.5 For the issue presented in Figure 5(1), we conducted detailed analysis of the reasoning and toolcalling performance of ToolTrain-32B and Qwen-32B during the issue localization process. As 5Due to space limitations, the issue content and think content have been simplified. The original issue content can be found at: https://code.djangoproject.com/ticket/32332 shown in Figure 5(2), ToolTrain-32B demonstrates the ability to reason and call tools accurately during localization. It first identifies that the ForeignKey field is directly related to the bug and progressively traces the issue to key methods such as ForeignKeyDeferredAttribute.__set__ and ForwardManyToOneDescriptor.__set__. Upon realizing that the functions along this path are not the root cause of the bug, it precisely pivots its search direction, ultimately locating the Model._prepare_related_fields_for_save function in base.py. Throughout this process, ToolTrain-32B not only shows strong ability to comprehend the problem but also exhibits excellent inter-module dependency analysis skills. It proactively inspects methods related to primary key updates and foreign key assignments, reasonably utilizes search tools to uncover the evidence path, and ultimately succeeds in locating the bugs source. In contrast, Qwen-32B, as shown in Figure 5(3), exhibits significant shortcomings. Although it also identified ForeignKey-related classes and methods in the initial stage, its search scope remained confined to few explicitly mentioned fields and classes. It failed to break out of this initial path to perform broader reasoning about the code structure, ultimately leading it to incorrectly pinpoint ForeignObject.get_local_related_value and causing the task to fail. Furthermore, Qwen-32B made an incorrect tool call by attempting to retrieve non-existent method ForeignKey.get_local_related_value. Ultimately, this example validates the effectiveness of our ToolTrain framework in cultivating the deep and flexible reasoning, which is necessary to navigate complex codebases and overcome challenging localization tasks."
        },
        {
            "title": "C Threats to Validity",
            "content": "A limitation of our evaluation lies in the construction of the ground truth. For the issue localization task, we define the ground truth as the files and functions modified by the provided golden patch. However, the solution to given issue is not always unique, meaning other code locations could potentially serve as an equally valid fix. Consequently, our evaluation methodology may not credit these alternative, correct solutions. Nevertheless, we maintain that using the golden patch as benchmark is an effective means of evaluating and comparing different methods. The primary goal of our task is to identify suspicious files and functions. model that successfully pinpoints location known to be correct (i.e., the one in the golden patch) is demonstrably more effective than one that does not. Therefore, this approach provides consistent and practical standard for assessing the performance of issue localization frameworks. The evaluation was conducted on Python repositories, and the performance on other languages remains to be validated. This is mainly because the evaluation set was constructed based on SWEBench-Verified, which is focused on the Python language. In our future work, we plan to apply RepoSearcher and ToolTrain to more languages. This will allow us to further verify the effectiveness of our method and contribute to the advancement of automated issue localization."
        },
        {
            "title": "D Details of Baseline Frameworks",
            "content": "Agentless Agentless (Xia et al., 2025) is pipeline-based issue resolution framework. In the localization stage, it adopts hierarchical localization strategy: it first identifies suspicious files in the repository using LLMs, then narrows down to relevant classes or functions, and finally pinpoints fine-grained edit locations by LLMs for precise issue localization. OrcaLoca Orcaloca (Yu et al., 2025) is an agent-based issue localization framework that integrates priority-based LLM-guided action scheduling, action decomposition with relevance scoring, and distance-aware context pruning to enhance the agents issue localization accuracy. CoSIL CoSIL (Jiang et al., 2025) is function-level software issue localization Agent that utilizes LLMs to dynamically construct module call graphs during the repository search process, iteratively explores relevant contexts, and employs context pruning to effectively narrow the search space. LocAgent LocAgent (Chen et al., 2025) is an issue localization agent that transforms repositories into directed heterogeneous graphs, capturing structures like files, classes, and functions, along with their dependencies such as imports and invocations. This representation allows LLM agents to perform efficient multi-hop reasoning, precisely identifying relevant code from natural language descriptions."
        }
    ],
    "affiliations": [
        "Beijing Institute of Technology",
        "ByteDance",
        "Peking University"
    ]
}