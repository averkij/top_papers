{
    "paper_title": "Audio Jailbreak: An Open Comprehensive Benchmark for Jailbreaking Large Audio-Language Models",
    "authors": [
        "Zirui Song",
        "Qian Jiang",
        "Mingxuan Cui",
        "Mingzhe Li",
        "Lang Gao",
        "Zeyu Zhang",
        "Zixiang Xu",
        "Yanbo Wang",
        "Chenxi Wang",
        "Guangxian Ouyang",
        "Zhenhao Chen",
        "Xiuying Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The rise of Large Audio Language Models (LAMs) brings both potential and risks, as their audio outputs may contain harmful or unethical content. However, current research lacks a systematic, quantitative evaluation of LAM safety especially against jailbreak attacks, which are challenging due to the temporal and semantic nature of speech. To bridge this gap, we introduce AJailBench, the first benchmark specifically designed to evaluate jailbreak vulnerabilities in LAMs. We begin by constructing AJailBench-Base, a dataset of 1,495 adversarial audio prompts spanning 10 policy-violating categories, converted from textual jailbreak attacks using realistic text to speech synthesis. Using this dataset, we evaluate several state-of-the-art LAMs and reveal that none exhibit consistent robustness across attacks. To further strengthen jailbreak testing and simulate more realistic attack conditions, we propose a method to generate dynamic adversarial variants. Our Audio Perturbation Toolkit (APT) applies targeted distortions across time, frequency, and amplitude domains. To preserve the original jailbreak intent, we enforce a semantic consistency constraint and employ Bayesian optimization to efficiently search for perturbations that are both subtle and highly effective. This results in AJailBench-APT, an extended dataset of optimized adversarial audio samples. Our findings demonstrate that even small, semantically preserved perturbations can significantly reduce the safety performance of leading LAMs, underscoring the need for more robust and semantically aware defense mechanisms."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 2 ] . [ 1 6 0 4 5 1 . 5 0 5 2 : r Audio Jailbreak: An Open Comprehensive Benchmark for Jailbreaking Large Audio-Language Models Zirui Song1 Qian Jiang 1 Mingxuan Cui1 Mingzhe Li2 Lang Gao 1 Zeyu Zhang 3 Zixiang Xu 1 Yanbo Wang 1 Chenxi Wang 1 Guangxian Ouyang 1 Zhenhao Chen 1 Xiuying Chen 1 1 Mohamed bin Zayed University of Artificial Intelligence 2 ByteDance 3 Australia National University"
        },
        {
            "title": "Abstract",
            "content": "The rise of Large Audio-Language Models (LAMs) brings both potential and risks, as their audio outputs may contain harmful or unethical content. However, current research lacks systematic, quantitative evaluation of LAM safetyespecially against jailbreak attacks, which are challenging due to the temporal and semantic nature of speech. To bridge this gap, we introduce AJailBench, the first benchmark specifically designed to evaluate jailbreak vulnerabilities in LAMs. We begin by constructing AJailBench-Base, dataset of 1,495 adversarial audio prompts spanning 10 policy-violating categories, converted from textual jailbreak attacks using realistic text-to-speech synthesis. Using this dataset, we evaluate several state-of-the-art LAMs and reveal that none exhibit consistent robustness across attacks. To further strengthen jailbreak testing and simulate more realistic attack conditions, we propose method to generate dynamic adversarial variants. Our Audio Perturbation Toolkit (APT) applies targeted distortions across time, frequency, and amplitude domains. To preserve the original jailbreak intent, we enforce semantic consistency constraint, and employ Bayesian optimization to efficiently search for perturbations that are both subtle and highly effective. This results in AJailBench-APT+, an extended dataset of optimized adversarial audio samples. Our findings demonstrate that even small, semantically preserved perturbations can significantly reduce the safety performance of leading LAMs, underscoring the need for more robust and semantically aware defense mechanisms. We release AJailBench, including both static and optimized adversarial data, to facilitate future research: https://github.com/mbzuai-nlp/AudioJailbreak Warning: This paper contains examples of harmful language. Reader discretion is recommended."
        },
        {
            "title": "Introduction",
            "content": "The concept of artificial assistants, long-standing staple of science fiction, is increasingly becoming reality in the field of artificial intelligence. Recently, the development of Large Language Models (LLMs) has seen their deployment across various domains, including virtual agents (Li et al., 2024; Song et al., 2024a; Liu et al., 2024b), embodied robots (Song et al., 2024b), and medical diagnosis (Han et al., 2024; Xie et al., 2025). Extending this progress, LAMs are further narrowing the gap between fiction and reality (Deshmukh et al., 2023; Nachmani et al., 2023; Wang et al., 2023; Ghosh et al., 2024; SpeechTeam, 2024; Gong et al., 2023; Tang et al., 2023; Wu et al., 2023b; Zhang et al., 2023; Chu et al., 2023; Fang et al., 2024; Xie, Wu, 2024). With OpenAIs GPT-4 enabling Equal Contribution Corresponding Author. Preprint. Under review. Figure 1: (a) Illustration of the audio jailbreak pipeline. benign audio prompt yields safe response, while an adversarially perturbed version may trigger harmful output from an LAM. Perturbations span time, frequency, and mixing domains. (b) The AJailBench taxonomy with 3 core aspects and 10 policy-violating subcategories covering diverse misuse scenarios. scheduled tasks, voice-interactive AI assistants such as those imagined in science fiction are becoming possible, allowing users to perform actions like making phone calls, sending emails, and setting reminders through voice. It is therefore critical to ensure that LAMs are aligned with safety standards to prevent the generation of harmful or unethical responses. However, most existing research focuses on the vulnerabilities of LLMs and Large Vision Models (LVMs) under jailbreak attacks, while studies targeting LAMs remain significantly limited. Some prior works (Ying et al., 2024; Shen et al., 2024) merely convert textual jailbreak benchmarks like AdvBench into speech form and manually test the jailbreak capabilities of GPT-4os audio modality. These approaches are relatively naive, primarily focusing on semantic-level attacks while overlooking the unique acoustic characteristics and perturbation space of the audio modality. As result, they fall short in comprehensively evaluating the safety robustness of LAMs. To address this gap, as shown in Figure 1, we propose AJailBenchto the best of our knowledge, the first open-source benchmark for automated and systematic evaluation of jailbreak vulnerabilities in LAMs. We begin by constructing AJailBench-Base, dataset of 1,495 adversarial audio prompts spanning 10 policy-violating categories, converted from textual jailbreak attacks using realistic text-to-speech synthesis. Using this dataset, we evaluate seven leading openand closed-source LAMs, offering unified comparison of their safety performance. Our analysis reveals that no single model is robust across all safety dimensions; LAMs adopt varied safety strategies, from strict denial to permissiveness, each reflecting different trade-offs between robustness and usability. To further probe model robustness under more realistic adversarial conditions, we introduce the Audio Perturbation Toolkit (APT), which consists of three categories of perturbationstime-domain, frequency-domain, and mixing-basedcovering seven methods for generating diverse adversarial audio variants. To ensure that perturbed audio retains its original jailbreak intent, we propose Semantic Consistency Constraint, enabling the generation of adversarial examples with strong semantic fidelity and transferability. By leveraging GPTScore (Fu et al., 2024) as an intermediate metric between human judgment and heterogeneous perturbation parameters, our approach supports semantic consistency across attack types. We further apply Bayesian optimization to automatically search for the most effective perturbation configurations that remain semantically consistent. This results in AJailBench-APT+, an extended benchmark dataset containing optimized adversarial audio. The addition of these perturbations leads to further degradation in LAM performance, demonstrating the effectiveness of the attacks and offering deeper insights into the cross-modal robustness transferability of LAMs between text and audio modalities. Our contributions can be summarized as three key points: we propose AJailBench, the most comprehensive open-source benchmark for evaluating jailbreak vulnerabilities in LAMs, which includes static dataset (AJailBench-Base) with 1,495 adversarial audio prompts across 10 policy-violating categories; we introduce the Audio Perturbation Toolkit (APT) to generate dynamic adversarial variants using time-, frequency-, and mixing-based perturbations, and further present AJailBench-APT+, an extended dataset constructed using semantic consistency constraints and Bayesian optimization; 2 Aspect Data Source Perturbation Type Semantic Preservation Combinable Attacks Yang et al. (2024) Hughes et al. (2024) Xiao et al. (2025) AJailBench 350 samples (7 categories) 159 samples 520 samples (7 categories) 1,495 samples (10 categories) Spelled-letter audio Time-domain (Partial) TTS edits (tone,speed,etc) No constraint No constraint No constraint Random Sample Time-domain Frequency-domain Hybrid perturbations GPTScore + Human examination Bayesian optimization Benchmark + tool Open-source Close-source Tool only Close-source Table 1: Comparison of AJailBench with recent audio jailbreak studies. AJailBench uniquely offers signal-level audio perturbation benchmark with semantic consistency constraints, combinable attacks, and open-source release. finally, we conduct comprehensive evaluations on seven leading openand closed-source LAMs, revealing that no single model is robust across all dimensions, thereby highlighting key safety vulnerabilities and enabling fair comparison under adversarial scenarios."
        },
        {
            "title": "2 Related Work",
            "content": "Large Audio-Language Models. In the domain of audio-based language models, initial systems (Lakhotia et al., 2021; Radford et al., 2023; Borsos et al., 2022; Song et al., 2025a) employed either acoustic or semantic tokens to facilitate generation from audio inputs to text or audio outputs. Recent advancements in LLMs have spurred the development of multimodal models. These models often use LLMs as backbones and incorporate additional encoders that transform input audio waveforms into text representations. Decoders then convert these representations back to output, enhancing the interaction between different modalities (Tang et al., 2023; Chen et al., 2023; Wu et al., 2023a; Fathullah et al., 2024; Cai et al., 2025; Song et al., 2025b; Huang et al., 2025b,a; Wang et al., 2025b,a; Chen et al., 2025). For example, SpeechGPT (Zhang et al., 2023) adopts cross-modal architecture to synchronize speech and text, facilitating tasks like instruction following and spoken dialogue. DiVA (Held et al., 2024) revolutionizes the training of speech-based LLMs by leveraging the responses of text-only LLM to transcribe speech as form of self-supervision. SALMONN (Tang et al., 2023) introduces dual encoders for processing diverse audio inputs, excelling in tasks such as speech recognition and audio storytelling. Innovations continue with Qwen2-Audio (Chu et al., 2024), LLama-Omni (Fang et al., 2024), and Gemini-1.5-pro (Reid et al., 2024), which offer unique capabilities from voice chatting and low-latency interactions to managing complex multimodal data. Furthermore, GPT-4o (Achiam et al., 2023) extends these capabilities, ensuring robust performance in audio-text interactions within noisy environments. Jailbreak Attack on LAMs. There are limited papers focused on the audio Jailbreak. Early paper (Shen et al., 2024) only naively transfers the text jailbreak data like AdvBench (Zou et al., 2023) to audio by Text-to-Speech models like OPENAI TTS-1 (OpenAI, 2024). However, they overlook the potential impact of other audio characteristics, such as pitch and frequency, on the audio encoder. Based on AdvBench after Text-To-Speech (TTS), ADVWAVE (Kang et al., 2024) introduces white-box attack approach based on dual-phase optimization, specifically designed for open-source models but lacking broader applicability. The most related works to ours include Yang et al. (2024); Hughes et al. (2024); Xiao et al. (2025), as summarized in Table 1. Yang et al. (2024) focus on TTS-generated audio with spelled-letter prompts but lack semantic constraint to ensure meaning preservation. Hughes et al. (2024) propose BoN sampling to augment prompts, but their method operates purely at the text level and does not explore audio-domain perturbations. Xiao et al. (2025) explore TTS-based audio editing (e.g., tone, speed), but do not support composable attacks or quantify semantic distortion. All of the above methods rely on outdated AdvBench samples with limited coverage and decreasing effectiveness against modern models. In contrast, our work introduces AJailBench, the first benchmark targeting signal-level audio perturbation attacks on LAMs."
        },
        {
            "title": "3 AJailBench",
            "content": "3.1 AJailBench-base Text Jailbreak Collection. We collect jailbreak text samples from two main sources. The first includes manually designed prompts curated from published research papers and real user-shared examples on online platforms such as Reddit (Chao et al., 2024; Shen et al., 2023). The second consists of automatically generated samples, produced using open-source jailbreak generation tools released by prior work (Chao et al., 2024). Since many known jailbreak prompts (e.g., grandmother reciting Windows activation codes) are already blocked by ChatGPT-3.5/4, we retain only those verified to bypass safety filters on these models. This ensures that our benchmark remains challenging and practically relevant. After collection and filtering, we annotate each sample with its violation type with DeekSeek-V3 (Liu et al., 2024a), following the categories defined in OpenAIs usage policies. In total, we construct dataset of 1,495 jailbreak text samples spanning 10 violation categories, including disinformation, economic harm, etc. Audio Generation. To avoid bias that individual voices could introduce, we use the state-of-the-art Google Cloud TTS models to convert text to natural-sounding spoken audio. Additionally, we have configured 118 distinct timbres across four English accents (UK, AU, US, India) to maximize audio diversity. It is worth noting that in automatically generated jailbreak samples, there are instances of disordered vocabulary similar to typos, which the TTS model spells out rather than reads. 3.2 AJailBench-APT+ While AJailBench-Base evaluates robustness against clean audio, it may underestimate model vulnerability to stronger, more realistic attacks. To this end, we introduce AJailBench-APT+, motivated by the need for (1) stronger attacks that can challenge even well-aligned models, and (2) audio-specific perturbations that exploit the unique characteristics of speech, such as temporal variation and acoustic ambiguity; and (3) exploring the combinatorial effects of multiple perturbation types, which may enhance attack diversity and effectiveness. Although minor perturbations may sound like mere audio quality changes to humans, they can cause representation shifts in LAMs, leading to semantic misinterpretation and allowing the model to bypass its refusal mechanisms. Concretely, we apply 7 audio perturbation methods across time, frequency, and mixing domains. To preserve the original jailbreak intent, we enforce semantic consistency and use Bayesian optimization to find effective perturbations within safe bounds. 3.2.1 Audio Perturbation Toolkit We propose unified mathematical framework for audio perturbation. Let the original audio sample be represented by which denotes the entire time-domain waveform. x(t) denotes the value of waveform at the specific time t. Perturbation is defined as parameterized transformation (x; θ), yielding the audio after perturbation x. To preserve the jailbreak intent, we enforce semantic consistency constraint: S(x, x) τ , where measures Similarity and τ is threshold. This defines the semantically valid perturbation space: Θ = {θ S(x, (x; θ)) τ } . To realize in practice, we introduce the Audio Perturbation Toolkit (APT)a suite of parameterized editing operations grouped into waveform-domain, frequency-domain, and hybrid perturbations. Each operation modulates the signal in controlled and interpretable manner. Waveform-domain Perturbation: gain, windowing, or local deletion. operations that act directly on the waveform x(t) via point-wise = Twave(x; θwave). (1) Energy Distribution Perturbation modifies the overall energy = (cid:80) x(t)2 of the signal without changing the time-frequency structure of speech content. Specifically, the time-domain waveform is scaled linearly using scalar θEDP: TEDP(x; θEDP) = θEDP x(t), θEDP [θEDPmin, θEDPmax], (2) 4 where θEDP > 1 amplifies the signal and θEDP < 1 attenuates it. Trimming applies an inverse rectangular window to remove signals within the interval [t0, t0 + θTrim], where t0 is the interval starting time. Trimming introduces discontinuities in the time domain, disrupting the context, thereby affecting how the audio is perceived without changing the overall content outside the specified interval: TTrim(x; t0; θTrim) = x(t) I(t / [t0, t0 + θTrim]), θTrim 0.1s. (3) Fade In/Out applies linear gain ramps to the beginning and end of the signal. Let be the total duration of the audio x. The transition duration γ is sampled from uniform distribution γ (0, θFade]. This smooths the onset and offset of the audio by gradually increasing and then decreasing its amplitude, while leaving the central portion unaffected. TFade(x; γ) = x(t) 0 < γ t/γ γ γ 1 (T t)/γ γ < (4) Frequency-domain Perturbation: modify the signal by manipulating its frequency components, typically accessed via the Short-Time Fourier Transform (STFT). Although the core manipulation Tf req operates in the frequency domain, the overall transformation can be viewed as function directly mapping the input time-domain signal to the output time-domain signal x. This implicitly involves transforming to the frequency domain, applying the modification Tf req, and transforming back to the time domain (iSTFT): = iSTFT(Tfreq(STFT(x); θfreq)). (5) Pitch Shifting modifies the perceived pitch (fundamental frequency and its harmonics) of the signal without changing its duration. This is achieved by scaling the frequency components in the STFT domain. We use Phase Vocoding(PV) (Dolson, 1986) that adjusts phase information accordingly to maintain temporal coherence: TPS(X(t, ); θPS) = PV(X(t, ), θPS), θPS [θPSmin , θPSmax ]. (6) Temporal Scaling stretches or compresses the audio to speed up or slow down without altering its perceived pitch. It is implemented via Phase Vocoder that adjusts the phase increments between STFT frames to achieve time expansion or compression. When θTS < 1, the playback is slowed down; when θTS > 1, it is sped up. Importantly, the fundamental frequency F0 remains unaffected: TTS(X(t, ); θTS) = PV(X(t, ), θTS), θTS [θTSmin, θTSmax], (7) Hybrid Perturbation: combines the original signal with external signals, such as natural noise or inaudible components. These methods affect both time and frequency characteristics of the signal: = Tmix(x; n; θhybrid). (8) Extra-auditory Priming adds single sinusoidal signal to the audio signal, either in the infrasound (fa < 20Hz) or ultrasound (fa > 20kHz) ranges. This is intended to simulate specific types of realworld tonal noise or interference, such as low-frequency electrical hum or high-frequency electronic whine. The frequency of sinusoidal signal is controlled by parameter θSTA {ultrasound, infrasound}. The amplitudes, where A0 = 0.1 is the peak amplitude of the sinusoidal perturbation term. Semantic integrity is maintained through: TEP(x; θep) = x(t) + A0 sin(2πfθSTAt). Natural Noise Injection overlays randomly selected natural acoustic event signal x(t, θe) onto the original signal x(t) . The event θe is chosen from predefined set [Rain, Cry, Horn, Music], and nθe(t)represents corresponding noise waveform instance. : TNI(x; θe) = x(t) + nθe (t). 3.3 AJailBench-APT+ via Bayesian Optimization Building on these tools, we explore how to effectively leverage or combine them to maximize attack effectiveness. We adapt classic Bayesian Optimization (BO) (Frazier, 2018) to efficiently identify 5 impactful audio perturbations, parameterized by low-dimensional vector = (p1, p2) within normalized search space = [0, 1]2. Specifically, the perturbation process is controlled by two parameters: p1 represents configuration of perturbation types, allowing activation of set of perturbation methods (e.g., trimming + noise injection + pitch shifting), and p2 controls the intensity or key characteristics of each activated perturbation (e.g., segment duration, frequency shift, or noise amplitude). The function E(aorig; p) then maps the original audio aorig to its perturbed version apert. The goal of the attack optimization is to identify perturbation parameters that steer the model away from producing standard refusal responses. To quantify the degree of refusal in given output, we define reference set of refusal phrases = {r(1) ref } and measure the semantic similarity between the models response and this set. Given perturbed input apert, the model produces textual output = M(apert), which is evaluated by: ref , . . . , r(N ) S(r) = maxrrefR cos(emb(r), emb(rref)), where emb() denotes SentenceBERT (Reimers, Gurevych, 2019) embeddings and cos(, ) computes cosine similarity. Our objective is to minimize this refusal score: = arg minx[0,1]2 S(M(apert)). Minimizing this objective helps identify audio perturbations that reduce the models tendency to produce refusal responses, thereby exposing potential jailbreaks or unintended behaviors. Detailed implementation of BO could be seen in the appendix A. 3.4 Semantic Consistency Constrain To ensure the effectiveness and realism of adversarial audio attacks, it is essential that the perturbed input retains the core semantics of the original query. Without such constraints, perturbations may unintentionally alter or obscure the intended meaning, making it unclear whether model responses are due to true vulnerabilities or simply semantic degradation. Moreover, maintaining semantic consistency promotes the generalizability and transferability of adversarial examples, enabling successful attacks across different voice styles, accents, or speaking ratesclosely resembling real-world black-box scenarios. To address these challenges, we introduce Semantic Consistency Constraint, which ensures that perturbed audio remains semantically faithful to the original intent while preserving adversarial effectiveness. Figure 2: Workflow of Semantic Consistency Constraint. Perturbed audio is transcribed, scored with GPTScore, and filtered via threshold to ensure semantic preservation. Each parameter corresponds to different perturbation type. Specifically, each perturbation method is controlled by parameter that adjusts the degree of distortion, as introduced in 3.2.1. We sweep through the parameter range to generate perturbed audio samples of varying intensity, then transcribe each sample using an automatic speech recognition model. We use GPTScore (Fu et al., 2024) to measure the semantic similarity between the transcribed text and the original jailbreak prompt. We then use human evaluations to identify which samples remain semantically consistent, and determine the corresponding minimum GPTScore. This score is mapped back to the perturbation parameter space to define the maximum semantically safe perturbation threshold for each method. The whole process is shown in Figure 2. Importantly, we use GPTScore as an intermediate bridge between human judgments and the perturbation parameter space (e.g., scaling rates, frequency shifts, time segments), avoiding the issue of incomparability across different perturbation types. Ultimately, we retain only the perturbations below the semantic threshold to ensure both semantic consistency and effective adversarial attacks. Details can be found in Appendix B. 6 Figure 3: Semantic Consistency Constraint Experiments visualization. (a) Energy Distribution Perturbation. (b) Pitch shifting. (c) Temporal Scaling (d) Perturbation Overlay Round."
        },
        {
            "title": "4 Benchmark Results",
            "content": "4.1 Evaluated LAMs We evaluate the performance of several mainstream LAMs, including SpeechGPT(Zhang et al., 2023), SALMONN(Tang et al., 2023), DiVA(Held et al., 2024), Qwen2-Audio(Chu et al., 2024), LLama-Omni(Fang et al., 2024), Gemini-2.0-flash(Reid et al., 2024), and GPT-4o-audio (Achiam et al., 2023). Prompts are adapted as needed to align with model-specific QA formatsfor instance, SALMONN and LLama-Omni are prompted according to their respective instruction formats. For GPT-4o and Gemini, we use their public APIs. All open-source models are evaluated locally using 2 A6000 GPUs (48GB). 4.2 Evaluation Metrics To comprehensively evaluate the models behavior under adversarial attacks, we adopt five metrics: Attack Success Rate (ASR ), Toxicity Score (TS ), Policy Violation (PV ), Relevance (), and Similarity (). ASR indicates the models vulnerability to adversarial prompts. Following Shen et al. (2024), we compute ASR using refusal keyword list (e.g., Sorry, cant); an attack is considered unsuccessful if the response contains any of these keywords. Lower ASR reflects stronger resistance to unauthorized or unsafe generations. To evaluate the quality, safety, and relevance of outputs, we use GPT-4o-based framework (Lin et al., 2023). TS measures harmfulness on scale of 1 to 5. PV is binary indicator of policy violation (1 = violation, 0 = safe). Relevance captures how well the response aligns with the prompt, also rated from 1 to 5. To assess alignment with standard refusal behavior, we compute Similarity between model outputs and 142 GPT-4-generated refusals using GPTScore (Fu et al., 2024) following Gao et al. (2024). Higher similarity scores indicate closer alignment with calibrated refusal strategies. 4.3 Performance on JailBench-base As shown in Table 2 and Figure 4, all evaluated LAMs, including Qwen2 Audio, DiVA, LLaMA Omni, and GPT4o, exhibit highly consistent safety behavior between text and audio inputs. This indicates that incorporating the audio modality does not compromise the safety alignment of their underlying language models. Notably, GPT-4o imposes even stricter safety constraints on its audio modality compared to its text counterpart, as reflected by lower scores in ASR, TS, and PV. We also reveal that SpeechGPT is the least aligned model in terms of safety, showing minimal ability to defend against jailbreak attacks. At the opposite end of the spectrum, SALMONN enforces extremely strict safety constraints, but this comes at the cost of reduced usability, as it frequently fails to provide helpful responses to benign user queries. 7 Figure 4: Performance of existing LAMs across various aspects. Model Metrics Disinf. Economic Decision Making Harm Fraud Gov. Advice Decep. Discr. Har. Malware/ Hacking Physical Harm Privacy Sexual Content Average SpeechGPT SALMONN Qwen2-Audio DiVA LLama-Omni Gemini-flash GPT-4o ASR () TS () PV () Relevance () Similarity () ASR () TS () PV () Relevance () Similarity () ASR () 0.937 3.758 0.787 2.448 0.219 0.276 3.000 0.436 1.850 0.380 0.495 ASR () (cid:239) 0.625 3.195 3.430 0.597 0.722 3.149 0. TS () TS () (cid:239) PV () PV () (cid:239) Relevance () Similarity () ASR () TS () PV () Relevance () Similarity () ASR () 0.748 2.942 0.574 2.954 0.295 0.794 ASR () (cid:239) 0.728 3.264 3.458 0.724 0.611 3.494 0.295 TS () TS () (cid:239) PV () PV () (cid:239) Relevance () Similarity () ASR () 0.483 ASR () (cid:239) 0.722 3.575 3.847 0.735 0.763 3.736 0.319 TS () TS () (cid:239) PV () PV () (cid:239) Relevance () Similarity () ASR () ASR () (cid:181) TS () TS () (cid:181) PV () PV ()(cid:181) Relevance () Similarity () 0.211 0.334 2.250 3.347 0.315 0.458 2.763 0.546 0.929 3.964 0.892 2.000 0.202 0.358 3.214 0.642 1.357 0.346 0.358 0.143 2.500 2.500 0.571 0.571 3.214 0.288 0.358 2.142 0.357 2.142 0.384 0.572 0.286 3.142 1.857 0.714 0.357 3.142 0. 0.286 0.143 3.214 2.785 0.642 0.571 3.000 0.338 0.285 0.143 1.643 2.214 0.142 0.285 2.786 0.511 0.960 2.279 0.648 2.983 0.216 0.151 1.815 0.424 1.776 0.405 0.632 0.775 2.092 2.034 0.548 0.571 3.383 0.335 0.862 1.861 0.548 3.378 0. 0.908 0.885 2.041 2.034 0.560 0.539 3.293 0.306 0.718 0.806 2.204 2.257 0.631 0.622 4.279 0.307 0.388 0.628 1.902 1.963 0.397 0.421 3.176 0.527 0.939 3.858 0.931 1.959 0.220 0.052 2.636 0.620 1.496 0.421 0.526 0.575 3.668 3.311 0.818 0.873 2.607 0. 0.835 2.915 0.814 2.694 0.363 0.851 0.757 3.370 3.392 0.831 0.818 2.698 0.343 0.429 0.672 3.851 3.553 0.938 0.925 3.542 0.379 0.070 0.205 2.780 3.139 0.559 0.490 2.208 0.684 1.000 4.074 0.870 2.000 0.208 0.297 2.962 0.666 1.407 0. 0.519 0.630 2.629 3.148 0.629 0.740 3.111 0.296 0.593 2.000 0.370 2.407 0.339 0.704 0.260 3.148 1.925 0.777 0.222 3.370 0.246 0.444 0.259 3.222 2.555 0.666 0.629 3.481 0.266 0.240 0.297 1.080 2.148 0.120 0.481 2.960 0.477 0.928 4.710 0.948 1.784 0. 0.080 4.163 0.519 1.262 0.405 0.446 0.655 4.435 4.450 0.727 0.845 2.128 0.403 0.629 4.252 0.638 1.995 0.377 0.842 0.600 4.480 4.390 0.767 0.715 2.321 0.325 0.455 0.800 4.662 4.505 0.930 0.893 3.239 0.354 0.045 0.165 3.822 4.26 0.350 0.465 1.573 0. 0.940 4.527 0.939 1.763 0.226 0.176 3.768 0.685 1.435 0.398 0.676 0.563 3.842 3.312 0.740 0.750 2.222 0.295 0.630 3.703 0.648 2.157 0.343 0.829 0.188 3.859 2.375 0.875 0.791 2.609 0.277 0.562 0.062 4.234 2.812 0.937 0.937 3.406 0. 0.175 0.063 2.200 2.437 0.475 0.583 2.475 0.652 0.919 4.691 0.910 2.016 0.219 0.155 4.260 0.528 1.268 0.406 0.513 0.579 4.463 4.520 0.796 0.743 2.439 0.347 0.6017 4.235 0.642 1.788 0.352 0.748 0.455 4.447 4.429 0.845 0.909 2.333 0. 0.407 0.595 4.585 4.310 0.894 0.789 3.203 0.345 0.103 0.182 3.862 4.371 0.436 0.404 1.862 0.699 0.937 2.95 0.650 2.350 0.238 0.400 2.450 0.550 1.750 0.366 0.575 0.500 2.475 2.800 0.451 0.501 3.675 0.363 0.725 2.100 0.450 3.400 0. 0.925 0.534 2.900 2.133 0.550 0.561 4.250 0.290 0.675 0.400 2.450 2.100 0.475 0.466 3.650 0.344 0.265 0.367 1.412 1.800 0.264 0.302 3.147 0.505 0.963 4.379 0.916 1.910 0.223 0.110 3.977 0.695 1.396 0.408 0.523 0.657 3.816 3.901 0.764 0.846 2.563 0. 0.736 3.873 0.758 2.304 0.325 0.880 0.700 4.155 4.067 0.833 0.333 2.275 0.318 0.592 0.785 4.169 4.144 0.883 0.893 3.006 0.352 0.130 0.203 3.104 4.036 0.441 0.699 2.000 0.667 0.945 3.690 0.849 2.242 0.220 0.137 3.015 0.577 1.531 0. 0.552 0.648 3.343 3.264 0.664 0.716 2.784 0.359 0.752 3.038 0.580 2.653 0.339 0.852 0.677 3.344 3.172 0.747 0.658 2.829 0.311 0.548 0.672 3.561 3.355 0.773 0.801 3.612 0.340 0.190 0.317 2.654 3.071 0.350 0.496 2.432 0.624 Table 2: Evaluation results of LAMs on the AJailBench benchmark across 10 jailbreak-related categories and five metrics (Arrow direction indicates higher model safety). Cells represent the performance of the base text-only models when provided with the text version of the audio inputs, serving as reference point for LAMs audio understanding capability. Blue / light blue = best / 2nd-best; red / light red = worst / 2nd-worst. Interestingly, GPT-4o adopts nuanced strategy when handling potential jailbreak prompts. Instead of issuing direct refusals, it often responds with vague yet neutral language that acknowledges the topic while subtly deflecting the request. For example, when asked about medical concerns, it replies: Im here to provide information and promote positive and responsible behavior... its always best to consult licensed healthcare professional. This reflects thoughtful balance between safety and usability, maintaining alignment without being overly restrictive. 4.4 Semantic Safety Thresholds Experiment To evaluate the impact of different perturbation methods on semantic consistency across varying intensity levels, we conducted semantic safety threshold experiments, as shown in Figure 3. Our experiment indicates the following: Energy distribution perturbation leads to relatively gradual decline in Similarity, which drops sharply at high perturbation intensities. Pitch shifting exhibits minor increase in Similarity at moderate frequency offsets, followed by rapid decrease, suggesting that the model possesses some robustness to certain frequency variations. Temporal scaling significantly affects Similarity. When the scaling rate falls below 0.6 or exceeds 1.2, Similarity decreases sharply, indicating low tolerance for semantic fidelity preservation under such transformations. The 8 Model SALMONN Qwen2-Audio DIVA Gemini-flash GPT4o ASR () Base APT+ TS () Base APT+ PV () Base APT+ Relevance () Base APT+ Similarity () Base APT+ 0.356 0.491 0.580 0.611 0. 0.433 0.526 0.674 0.737 0.314 2.759 2.583 2.314 3.084 1.639 2.670 2.595 2.428 3.431 1.734 0.577 0.664 0.580 0.773 0.350 0.471 0.537 0.503 0.753 0.249 1.778 3.140 2.739 3.270 2. 1.657 2.726 2.608 2.881 2.633 0.360 0.348 0.340 0.311 0.550 0.373 0.343 0.299 0.275 0.548 Table 3: Evaluation performance of each model under AJailBench-base and AJailBench-APT+ settings. Arrow direction indicates higher model safety. Numbers in bold indicate statistically significant difference from the original performance (two-tailed paired t-test, < 0.01). maximum number of perturbation overlay rounds is 13, but in our Bayesian Optimization process, we adopted more conservative threshold and selected 10 rounds. Additionally, all perturbed audio samples were manually checked to ensure intelligibility. The decline in Similarity resulting from multi-round superimposed perturbations is the most linear and sustained, with semantic consistency degrading more markedly as the number of perturbations increases. By analyzing the experiment results with the semantic safety threshold established via human evaluation, we determined the maximum permissible perturbation range for perturbation. Subsequently, we ensure that all perturbation methods employed within our proposed AjailBench-APT++ operate strictly within these pre-defined safe thresholds. This constraint is crucial for balancing the preservation of semantic consistency with the effectiveness of the attack. 4.5 Performance on AJailBench-APT+ As shown in Table 3, we report the comparative performance of strong LAM models on the AJailBench-Base and AJailBench-APT+ datasets. Notably, models exhibit significantly degraded safety metrics on AJailBench-APT+, indicating the increased difficulty introduced by our semantically consistent perturbations. These results highlight three key insights. First, jailbreak attacks on LAMs can succeed not only through carefully crafted semantic content but also through subtle manipulations in the audio signal itselfrevealing an underexplored attack vector beyond text-level prompts. Second, the success of adversarial examples in AJailBench-APT+ suggests that current LAM safety mechanisms may overly rely on clean, transcribed speech representations, potentially overlooking non-canonical acoustic patterns that can bypass refusal strategies. Third, APT+ constitutes more stringent benchmark by integrating signallevel variability with semantic preservation, thereby providing more realistic and transferable evaluation of audio-model robustness under adversarial conditions. We show the distribution of the seven APT tools selected via our Bayesian optimization in Figure 5, which shows that time stretch perturbation and fade perturbation are most frequently utilized and have the strongest effect on degrading model robustness across variety of inputs. Figure 5: Sample distribution across 7 APT techniques in AJailBench-APT+, selected via Bayesian optimization. Discussion on Defense Mechanisms for LAMs. To our best knowledge, no prior work has proposed systematic defense mechanisms specifically designed for LAMs, despite the growing awareness of their vulnerability to jailbreak attacks. To address this gap, we propose that future research explore adversarial fine-tuning using semantically preserved perturbations (Fan et al., 2021), consistency regularization across augmented audio views (Lu et al., 2019), and front-end signal filtering techniques to mitigate input-level attacks. Additionally, incorporating acoustic-context-aware refusal calibration and uncertainty-aware decoding strategies (Subedar et al., 2019) may help LAMs detect and abstain from unsafe completions when encountering anomalous or adversarial audio signals."
        },
        {
            "title": "5 Conclusion",
            "content": "In this paper, we introduce AJailBench, the first benchmark for systematically evaluating jailbreak vulnerabilities in LAMs. While LAMs offer new possibilities, their speech-based outputs pose unique safety risks that are difficult to assess. AJailBench includes dataset of adversarial audio prompts and an Audio Perturbation Toolkit (APT) for generating realistic, semantically preserved attack variants. Our experiments reveal that state-of-the-art LAMs are highly vulnerable to both static and perturbed inputs. Overall, AJailBench provides practical testbed for LAM safety and highlights the need for more robust, semantically aware defenses. We provide detailed limitation discussion in Appendix C."
        },
        {
            "title": "References",
            "content": "Achiam Josh, Adler Steven, Agarwal Sandhini, Ahmad Lama, Akkaya Ilge, Aleman Florencia Leoni, Almeida Diogo, Altenschmidt Janko, Altman Sam, Anadkat Shyamal, others . Gpt-4 technical report // arXiv preprint arXiv:2303.08774. 2023. Borsos Zalán, Marinier Raphaël, Vincent Damien, Kharitonov Eugene, Pietquin Olivier, Sharifi Matt, Teboul Olivier, Grangier David, Tagliasacchi Marco, Zeghidour Neil. Audiolm: language modeling approach to audio generation.(2022) // arXiv preprint arXiv:2209.03143. 2022. Cai Rizhao, Song Zirui, Guan Dayan, Chen Zhenhao, Li Yaohang, Luo Xing, Yi Chenyu, Kot Alex. Benchlmm: Benchmarking cross-style visual capability of large multimodal models // European Conference on Computer Vision. 2025. 340358. Chao Patrick, Debenedetti Edoardo, Robey Alexander, Andriushchenko Maksym, Croce Francesco, Sehwag Vikash, Dobriban Edgar, Flammarion Nicolas, Pappas George J, Tramer Florian, others . Jailbreakbench: An open robustness benchmark for jailbreaking large language models // arXiv preprint arXiv:2404.01318. 2024. Chen Feilong, Han Minglun, Zhao Haozhi, Zhang Qingyang, Shi Jing, Xu Shuang, Xu Bo. X-llm: Bootstrapping advanced large language models by treating multi-modalities as foreign languages // arXiv preprint arXiv:2305.04160. 2023. Chen Xiuying, Wang Tairan, Guo Taicheng, Guo Kehan, Zhou Juexiao, Li Haoyang, Song Zirui, Gao Xin, Zhang Xiangliang. Unveiling the power of language models in chemical research question answering // Communications Chemistry. 2025. 8, 1. 4. Chu Yunfei, Xu Jin, Yang Qian, Wei Haojie, Wei Xipin, Guo Zhifang, Leng Yichong, Lv Yuan- // arXiv preprint jun, He Jinzheng, Lin Junyang, others . Qwen2-audio technical report arXiv:2407.10759. 2024. Chu Yunfei, Xu Jin, Zhou Xiaohuan, Yang Qian, Zhang Shiliang, Yan Zhijie, Zhou Chang, Zhou Jingren. Qwen-audio: Advancing universal audio understanding via unified large-scale audiolanguage models // arXiv preprint arXiv:2311.07919. 2023. Deshmukh Soham, Elizalde Benjamin, Singh Rita, Wang Huaming. Pengi: An audio language model for audio tasks // Advances in Neural Information Processing Systems. 2023. 36. 1809018108. Dolson Mark. The phase vocoder: tutorial // Computer Music Journal. 1986. 10, 4. 1427. Fan Lijie, Liu Sijia, Chen Pin-Yu, Zhang Gaoyuan, Gan Chuang. When does contrastive learning preserve adversarial robustness from pretraining to finetuning? // Advances in neural information processing systems. 2021. 34. 2148021492. Fang Qingkai, Guo Shoutao, Zhou Yan, Ma Zhengrui, Zhang Shaolei, Feng Yang. LLaMA-Omni: Seamless Speech Interaction with Large Language Models // arXiv preprint arXiv:2409.06666. 2024. Fathullah Yassir, Wu Chunyang, Lakomkin Egor, Jia Junteng, Shangguan Yuan, Li Ke, Guo Jinxi, Xiong Wenhan, Mahadeokar Jay, Kalinli Ozlem, others . Prompting large language models with speech recognition abilities // ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). 2024. 1335113355. 10 Frazier Peter I. tutorial on Bayesian optimization // arXiv preprint arXiv:1807.02811. 2018. Fu Jinlan, Ng See Kiong, Jiang Zhengbao, Liu Pengfei. GPTScore: Evaluate as You Desire // Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers). 2024. 65566576. Gao Lang, Zhang Xiangliang, Nakov Preslav, Chen Xiuying. Shaping the Safety Boundaries: Understanding and Defending Against Jailbreaks in Large Language Models // arXiv preprint arXiv:2412.17034. 2024. Ghosh Sreyan, Kumar Sonal, Seth Ashish, Evuru Chandra Kiran Reddy, Tyagi Utkarsh, Sakshi S, Nieto Oriol, Duraiswami Ramani, Manocha Dinesh. GAMA: Large Audio-Language Model with Advanced Audio Understanding and Complex Reasoning Abilities // arXiv preprint arXiv:2406.11768. 2024. Gong Yuan, Luo Hongyin, Liu Alexander H, Karlinsky Leonid, Glass James. Listen, think, and understand // arXiv preprint arXiv:2305.10790. 2023. Han Wenhan, Fang Meng, Zhang Zihan, Yin Yu, Song Zirui, Chen Ling, Pechenizkiy Mykola, Chen Qingyu. MedINST: Meta Dataset of Biomedical Instructions // arXiv preprint arXiv:2410.13458. 2024. Held William, Li Ella, Ryan Michael, Shi Weiyan, Zhang Yanzhe, Yang Diyi. Distilling an end-to-end voice assistant without instruction training data // arXiv preprint arXiv:2410.02678. 2024. Huang Yue, Gao Chujie, Wu Siyuan, Wang Haoran, Wang Xiangqi, Zhou Yujun, Wang Yanbo, Ye Jiayi, Shi Jiawen, Zhang Qihui, others . On the trustworthiness of generative foundation models: Guideline, assessment, and perspective // arXiv preprint arXiv:2502.14296. 2025a. Huang Yue, Wang Yanbo, Xu Zixiang, Gao Chujie, Wu Siyuan, Ye Jiayi, Chen Xiuying, Chen Pin-Yu, Zhang Xiangliang. Breaking Focus: Contextual Distraction Curse in Large Language Models // arXiv preprint arXiv:2502.01609. 2025b. Hughes John, Price Sara, Lynch Aengus, Schaeffer Rylan, Barez Fazl, Koyejo Sanmi, Sleight Henry, Jones Erik, Perez Ethan, Sharma Mrinank. Best-of-n jailbreaking // arXiv preprint arXiv:2412.03556. 2024. Kang Mintong, Xu Chejian, Li Bo. AdvWave: Stealthy Adversarial Jailbreak Attack against Large Audio-Language Models // arXiv preprint arXiv:2412.08608. 2024. Lakhotia Kushal, Kharitonov Eugene, Hsu Wei-Ning, Adi Yossi, Polyak Adam, Bolte Benjamin, Nguyen Tu-Anh, Copet Jade, Baevski Alexei, Mohamed Abdelrahman, others . On generative spoken language modeling from raw audio // Transactions of the Association for Computational Linguistics. 2021. 9. 13361354. Li Yanda, Zhang Chi, Yang Wanqi, Fu Bin, Cheng Pei, Chen Xin, Chen Ling, Wei Yunchao. Appagent v2: Advanced agent for flexible mobile interactions // arXiv preprint arXiv:2408.11824. 2024. Lin Bill Yuchen, Ravichander Abhilasha, Lu Ximing, Dziri Nouha, Sclar Melanie, Chandu Khyathi, Bhagavatula Chandra, Choi Yejin. The unlocking spell on base llms: Rethinking alignment via in-context learning // arXiv preprint arXiv:2312.01552. 2023. Liu Aixin, Feng Bei, Xue Bing, Wang Bingxuan, Wu Bochao, Lu Chengda, Zhao Chenggang, Deng Chengqi, Zhang Chenyu, Ruan Chong, others . Deepseek-v3 technical report // arXiv preprint arXiv:2412.19437. 2024a. Liu Yuhan, Song Zirui, Zhang Xiaoqing, Chen Xiuying, Yan Rui. From tiny slip to giant leap: An llm-based simulation for fake news evolution // arXiv preprint arXiv:2410.19064. 2024b. Lu Kangkang, Foo Chuan-Sheng, Teh Kah Kuan, Tran Huy Dat, Chandrasekhar Vijay Ramaseshan. Semi-Supervised Audio Classification with Consistency-Based Regularization. // INTERSPEECH. 1. 2019. 36543658. 11 Nachmani Eliya, Levkovitch Alon, Hirsch Roy, Salazar Julian, Asawaroengchai Chulayuth, Mariooryad Soroosh, Rivlin Ehud, Skerry-Ryan RJ, Ramanovich Michelle Tadmor. Spoken Question Answering and Speech Continuation Using Spectrogram-Powered LLM // arXiv preprint arXiv:2305.15255. 2023. OpenAI . GPT-4o System Card. 2024. https://openai.com/index/gpt-4o. Radford Alec, Kim Jong Wook, Xu Tao, Brockman Greg, McLeavey Christine, Sutskever Ilya. Robust speech recognition via large-scale weak supervision // International conference on machine learning. 2023. 2849228518. Reid Machel, Savinov Nikolay, Teplyashin Denis, Lepikhin Dmitry, Lillicrap Timothy, Alayrac Jeanbaptiste, Soricut Radu, Lazaridou Angeliki, Firat Orhan, Schrittwieser Julian, others . Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context // arXiv preprint arXiv:2403.05530. 2024. Reimers Nils, Gurevych Iryna. Sentence-bert: Sentence embeddings using siamese bert-networks // arXiv preprint arXiv:1908.10084. 2019. Shen Xinyue, Chen Zeyuan, Backes Michael, Shen Yun, Zhang Yang. \" do anything now\": Characterizing and evaluating in-the-wild jailbreak prompts on large language models // arXiv preprint arXiv:2308.03825. 2023. Shen Xinyue, Wu Yixin, Backes Michael, Zhang Yang. Voice Jailbreak Attacks Against GPT-4o // arXiv preprint arXiv:2405.19103. 2024. Song Zirui, Li Yaohang, Fang Meng, Chen Zhenhao, Shi Zecheng, Huang Yuan, Chen Ling. // arXiv preprint Mmac-copilot: Multi-modal agent collaboration operating system copilot arXiv:2404.18074. 2024a. Song Zirui, Ouyang Guangxian, Fang Meng, Na Hongbin, Shi Zijing, Chen Zhenhao, Fu Yujie, Zhang Zeyu, Jiang Shiyu, Fang Miao, others . Hazards in Daily Life? Enabling Robots to Proactively Detect and Resolve Anomalies // arXiv preprint arXiv:2411.00781. 2024b. Song Zirui, Yan Bin, Liu Yuhan, Fang Miao, Li Mingzhe, Yan Rui, Chen Xiuying. Injecting DomainSpecific Knowledge into Large Language Models: Comprehensive Survey // arXiv preprint arXiv:2502.10708. 2025a. Song Zirui, Yang Jingpu, Huang Yuan, Tonglet Jonathan, Zhang Zeyu, Cheng Tao, Fang Meng, Gurevych Iryna, Chen Xiuying. Geolocation with Real Human Gameplay Data: Large-Scale Dataset and Human-Like Reasoning Framework // arXiv preprint arXiv:2502.13759. 2025b. SpeechTeam Tongyi. FunAudioLLM: Voice Understanding and Generation Foundation Models for Natural Interaction Between Humans and LLMs // arXiv preprint arXiv:2407.04051. 2024. Subedar Mahesh, Krishnan Ranganath, Meyer Paulo Lopez, Tickoo Omesh, Huang Jonathan. Uncertainty-aware audiovisual activity recognition using deep bayesian variational inference // Proceedings of the IEEE/CVF international conference on computer vision. 2019. 63016310. Tang Changli, Yu Wenyi, Sun Guangzhi, Chen Xianzhao, Tan Tian, Li Wei, Lu Lu, Ma Zejun, Zhang Chao. Salmonn: Towards generic hearing abilities for large language models // arXiv preprint arXiv:2310.13289. 2023. Wang Chenxi, Gu Tianle, Wei Zhongyu, Gao Lang, Song Zirui, Chen Xiuying. Word Form Matters: LLMs Semantic Reconstruction under Typoglycemia // arXiv preprint arXiv:2503.01714. 2025a. Wang Tianrui, Zhou Long, Zhang Ziqiang, Wu Yu, Liu Shujie, Gaur Yashesh, Chen Zhuo, Li Jinyu, Wei Furu. Viola: Unified codec language models for speech recognition, synthesis, and translation // arXiv preprint arXiv:2305.16107. 2023. Wang Yanbo, Ye Jiayi, Wu Siyuan, Gao Chujie, Huang Yue, Chen Xiuying, Zhao Yue, Zhang Xiangliang. TRUSTEVAL: Dynamic Evaluation Toolkit on Trustworthiness of Generative Foundation Models // Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (System Demonstrations). 2025b. 7084. 12 Watanabe Shuhei. Tree-structured parzen estimator: Understanding its algorithm components and their roles for better empirical performance // arXiv preprint arXiv:2304.11127. 2023. Wu Jian, Gaur Yashesh, Chen Zhuo, Zhou Long, Zhu Yimeng, Wang Tianrui, Li Jinyu, Liu Shujie, Ren Bo, Liu Linquan, others . On decoder-only architecture for speech-to-text and large language model integration // 2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU). 2023a. 18. Wu Shengqiong, Fei Hao, Qu Leigang, Ji Wei, Chua Tat-Seng. Next-gpt: Any-to-any multimodal llm // arXiv preprint arXiv:2309.05519. 2023b. Xiao Erjia, Cheng Hao, Shao Jing, Duan Jinhao, Xu Kaidi, Yang Le, Gu Jindong, Xu Renjing. Tune In, Act Up: Exploring the Impact of Audio Modality-Specific Edits on Large Audio Language Models in Jailbreak // arXiv preprint arXiv:2501.13772. 2025. Xie Yunfei, Zhou Ce, Gao Lang, Wu Juncheng, Li Xianhang, Zhou Hong-Yu, Liu Sheng, Xing Lei, Zou James, Xie Cihang, Zhou Yuyin. MedTrinity-25M: Large-scale Multimodal Dataset with Multigranular Annotations for Medicine. 2025. Xie Zhifei, Wu Changqiao. Mini-Omni: Language Models Can Hear, Talk While Thinking in Streaming // arXiv preprint arXiv:2408.16725. 2024. Yang Hao, Qu Lizhen, Shareghi Ehsan, Haffari Gholamreza. Audio Is the Achilles Heel: Red Teaming Audio Large Multimodal Models // arXiv preprint arXiv:2410.23861. 2024. Ying Zonghao, Liu Aishan, Liu Xianglong, Tao Dacheng. Unveiling the Safety of GPT-4o: An Empirical Study using Jailbreak Attacks // arXiv preprint arXiv:2406.06302. 2024. Zhang Dong, Li Shimin, Zhang Xin, Zhan Jun, Wang Pengyu, Zhou Yaqian, Qiu Xipeng. Speechgpt: Empowering large language models with intrinsic cross-modal conversational abilities // arXiv preprint arXiv:2305.11000. 2023. Zou Andy, Wang Zifan, Kolter Zico, Fredrikson Matt. Universal and transferable adversarial attacks on aligned language models // arXiv preprint arXiv:2307.15043. 2023."
        },
        {
            "title": "A Detail implementation of Bayesian Optimization",
            "content": "The Bayesian Optimization (BO) procedure implemented in this work efficiently searches the twodimensional parameter space = [0, 1]2 for audio perturbations that minimize the refusal similarity score S(M(E(aorig; x))). For the surrogate model guiding the search, we employ the Tree-structured Parzen Estimator (TPE) algorithm Watanabe (2023). TPE does not model the objective function directly, but instead models (xy) and leverages Bayes rule to optimize the inverse probability. Given the history of evaluated points and their scores Dt = {(xi, yi)}t i=1, where yi = S(M(E(aorig; xi))), TPE models the probability distributions of the parameters conditioned on the objective score. It defines threshold based on quantile γ of the best observed scores to split the observations into good set Dg = {(xi, yi)yi < y} and bad set Db = {(xi, yi)yi y}. It then builds two non-parametric density estimators, l(x) which derived from the parameters in the good set Dg, representing (xy < y) and g(x) which derived from the parameters in the bad set Db, representing (xy y). The point selection strategy in TPE involves maximizing the ratio l(x)/g(x). This criterion, which is proportional to the Expected Improvement, effectively guides the search towards regions where parameters are likely to produce low objective scores (low refusal similarity) by leveraging the density estimates from past good and bad observations. The TPE implementation was configured with the following hyperparameters: Initial Random Trials (nstartup): 10 trials were evaluated using quasi-random sampling before TPE modeling began. Quantile (γ): 0.10 was used to distinguish \"good\" from \"bad\" observations for density estimation. 13 EI Candidates (ncandidates): 24 candidate points were sampled from l(x) when optimizing the acquisition criterion at each step. Other TPE settings related to prior weighting and sampling details followed standard practices for the algorithm. The iterative workflow proceeds as follows: 1. Initialization: Conduct nstartup (10) evaluations using quasi-random sampling. 2. Model Fitting: Update the TPE density estimators l(x) and g(x) based on all collected observations Dt. 3. Acquisition Optimization: Select the next parameters xt+1 by sampling ncandidates (24) points from l(x) and choosing the one that maximizes the ratio l(x)/g(x). 4. Objective Evaluation: Evaluate yt+1 = (xt+1) by executing the full pipeline: apert = E(aorig; xt+1), rt+1 = M(apert), yt+1 = S(rt+1). 5. Data Augmentation: Update Dt+1 = Dt {(xt+1, yt+1)}. 6. Iteration: Repeat steps 2-5 until the total evaluation budget is reached. This TPE-driven BO process enables an efficient search over the parameterized audio perturbation space . It identifies transformation parameters that are most effective at inducing specific model behaviors (minimizing refusal similarity) under the constraint of limited function evaluations. The resulting highlights specific sensitivities of the model to combinations of audio transformation types and intensities."
        },
        {
            "title": "B Semantic Safety Threshold Experiment via Human Evaluation",
            "content": "To determine semantic safety threshold, an experiment involving human evaluation was conducted. Three undergraduate students with relevant domain expertise were recruited as evaluators. corpus of 150 audio samples was curated for this evaluation, designed to encompass varying degrees of noise interference. These varying noise levels were achieved by introducing random APT+ perturbations. The parameters for the initial APT+ perturbations were set based on broad, heuristically defined range. The final evaluation set was constructed through an iterative process: noise was randomly added over 15 rounds, and 10 distinct samples were selected from each round, resulting in the 150 samples used for human assessment. An audio intelligibility scale was specifically designed to evaluate the clarity and comprehensibility of the audio samples. This scale consisted of 10 distinct rating levels, each accompanied by qualitative descriptor: The audio intelligibility scoring system is designed to evaluate the clarity and comprehensibility of speech under varying levels of noise intensity. The scoring range is from 0 to 10, where higher scores indicate greater intelligibility of the audio content. The detailed descriptions are as follows: 14 Score 10: The audio content is completely clear, with negligible noise interference. Listeners can easily understand all spoken content without effort. Score 9: Although there is slight background noise, it does not significantly affect the comprehension of the audio content. Listeners can easily grasp the semantic information. Score 8: Noise becomes noticeable but does not hinder the overall understanding of the speech. Listeners can comprehend the content accurately with minimal focus. Score 7: Noise interference is more pronounced, requiring moderate level of concentration to fully understand the speech content. Listeners might experience slight difficulty in certain segments. Score 6: Noise interference is significant, necessitating high degree of focus for comprehension. However, most of the semantic information remains accessible. Score 5: Noise is sufficiently strong to obscure parts of the speech content. Listeners need to exert effort to discern the speech, resulting in noticeable increase in comprehension difficulty. Score 4: Noise intensity is high, substantially impacting the intelligibility of the speech. Some portions of the audio may be entirely incomprehensible. Score 3: Noise is severe, causing the majority of the speech content to become distorted. Only occasional clear segments can be discerned, significantly restricting comprehension. Score 2: The speech content is almost incomprehensible due to intensified noise interference. Only faint traces of the speech might be captured in isolated instances. Score 1: Noise intensity reaches an extreme level, making the speech content virtually inaudible. Only very small number of vague fragments may be distinguishable. Score 0: The audio content is completely masked by noise. The speech loses all intelligibility, and no meaningful information can be identified. For each audio sample, the evaluators were instructed to listen carefully and assign an intelligibility rating based on the provided scale. Analysis of the evaluation results indicated that after 13 rounds of cumulative noise addition, the majority of audio samples were rated as difficult to understand (i.e., low intelligibility), where the score is lower than 4. The inter-rater agreement, measured by Cohens Kappa, is 0.72, denoting substantial agreement among annotators. Based on this finding, we proceeded to apply 13 rounds of noise perturbation to the entire AJailbench-base dataset. Subsequently, we generated transcriptions for both the original, unperturbed audio files and their 13-round perturbed counterparts using the Whisper automatic speech recognition system. The textual similarity between these paired transcriptions (original vs. perturbed) was computed. The resulting average similarity score, 0.638, was established as the semantic safety threshold. To ensure transparency and facilitate reproducibility, all experimental records, including the dataset generation process and evaluation results, have been made available in an open-source repository."
        },
        {
            "title": "C Limitation",
            "content": "Although AJailBench provides systematic framework for evaluating jailbreak vulnerabilities in LAMs under audio-based attacks, there remain several unexplored directions. First, we do not investigate defenses against audio adversarial attacks. This is primarily due to the limited progress in this areathere are currently no well-established or widely adopted defense methods specifically designed for the audio modality. We leave this important direction for future work. Second, our study focuses primarily on English audio inputs. While various accents are included, cross-lingual robustness under adversarial perturbations remains unexplored and may be critical for real-world multilingual deployment scenarios."
        }
    ],
    "affiliations": [
        "Australia National University",
        "ByteDance",
        "Mohamed bin Zayed University of Artificial Intelligence"
    ]
}