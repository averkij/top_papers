{
    "paper_title": "From Scores to Skills: A Cognitive Diagnosis Framework for Evaluating Financial Large Language Models",
    "authors": [
        "Ziyan Kuang",
        "Feiyu Zhu",
        "Maowei Jiang",
        "Yanzhao Lai",
        "Zelin Wang",
        "Zhitong Wang",
        "Meikang Qiu",
        "Jiajia Huang",
        "Min Peng",
        "Qianqian Xie",
        "Sophia Ananiadou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) have shown promise for financial applications, yet their suitability for this high-stakes domain remains largely unproven due to inadequacies in existing benchmarks. Existing benchmarks solely rely on score-level evaluation, summarizing performance with a single score that obscures the nuanced understanding of what models truly know and their precise limitations. They also rely on datasets that cover only a narrow subset of financial concepts, while overlooking other essentials for real-world applications. To address these gaps, we introduce FinCDM, the first cognitive diagnosis evaluation framework tailored for financial LLMs, enabling the evaluation of LLMs at the knowledge-skill level, identifying what financial skills and knowledge they have or lack based on their response patterns across skill-tagged tasks, rather than a single aggregated number. We construct CPA-QKA, the first cognitively informed financial evaluation dataset derived from the Certified Public Accountant (CPA) examination, with comprehensive coverage of real-world accounting and financial skills. It is rigorously annotated by domain experts, who author, validate, and annotate questions with high inter-annotator agreement and fine-grained knowledge labels. Our extensive experiments on 30 proprietary, open-source, and domain-specific LLMs show that FinCDM reveals hidden knowledge gaps, identifies under-tested areas such as tax and regulatory reasoning overlooked by traditional benchmarks, and uncovers behavioral clusters among models. FinCDM introduces a new paradigm for financial LLM evaluation by enabling interpretable, skill-aware diagnosis that supports more trustworthy and targeted model development, and all datasets and evaluation scripts will be publicly released to support further research."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 1 ] . [ 1 1 9 4 3 1 . 8 0 5 2 : r From Scores to Skills: Cognitive Diagnosis Framework for Evaluating Financial Large Language Models Ziyan Kuang1,2, Feiyu Zhu1,2, Maowei Jiang3, Yanzhao Lai4, Zelin Wang1,2, Zhitong Wang5, Meikang Qiu6, Jiajia Huang3, Min Peng1,2, Qianqian Xie1,2*, Sophia Ananiadou7 1School of Artifical Intelligence, Wuhan University 2Center for the Study of Language and Information , Wuhan University 3School of Computer Science, Nanjing Audit University 4Southwest Jiaotong University 5Beijing University of Financial Technology 6Computer and Cyber Sciences, Augusta University 7Computer Science, University of Manchester Abstract Large Language Models (LLMs) have shown promise for financial applications, yet their suitability for this high-stakes domain remains largely unproven due to inadequacies in existing benchmarks. Existing benchmarks solely rely on score-level evaluation, summarizing performance with single score that obscures the nuanced understanding of what models truly know and their precise limitations. They also rely on datasets that cover only narrow subset of financial concepts, while overlooking other essentials for real-world applications. To address these gaps, we introduce FinCDM, the first cognitive diagnosis evaluation framework tailored for financial LLMs, enabling the evaluation of LLMs at the knowledge-skill level, identifying what financial skills and knowledge they have or lack based on their response patterns across skill-tagged tasks, rather than single aggregated number. We construct CPA-QKA, the first cognitively informed financial evaluation dataset derived from the Certified Public Accountant (CPA) examination, with comprehensive coverage of real-world accounting and financial skills. It is rigorously annotated by domain experts, who author, validate, and annotate questions with high inter-annotator agreement and fine-grained knowledge labels. Our extensive experiments on 30 proprietary, open-source, and domain-specific LLMs show that FinCDM reveals hidden knowledge gaps, identifies under-tested areas such as tax and regulatory reasoning overlooked by traditional benchmarks, and uncovers behavioral clusters among models. FinCDM introduces new paradigm for financial LLM evaluation by enabling interpretable, skillaware diagnosis that supports more trustworthy and targeted model development, and all datasets and evaluation scripts will be publicly released to support further research."
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs) are increasingly applied to financial tasks (Nie et al. 2024a), but their suitability for such high-stakes domains remains largely untested (Xie et al. 2024). While numerous open-domain benchmarks such as MMLU (Hendrycks et al. 2021), HELM (Liang et al. 2022), *Corresponding author. Email: xieq@whu.edu.cn 1https://github.com/WHUNextGen/FinCDM and HLB (Duan et al. 2024) have demonstrated the excellent general capabilities of LLMs like GPT-4 (Achiam et al. 2023), DeepSeek (Liu et al. 2024a), and LLaMA (Touvron et al. 2023), their effectiveness in the financial domainspecific tasks remains largely unknown. To address this uncertainty, several benchmarks (Xie et al. 2023, 2024; Li et al. 2024a; Peng et al. 2025b), have been recently proposed to systematically evaluate the capabilities of LLMs specifically within the financial domain. However, existing financial LLM benchmarks (Xie et al. 2024; Li et al. 2024a; Peng et al. 2025b) often fail to reflect what matters for those tasks and applications, namely what the model knows, what it can reliably do, and where it is likely to fail. First, this issue stems from score flattening, where dataset in existing benchmarks is reduced to single number, making it unclear what knowledge the model has actually mastered. For example, in FinQA from MultiFinBen (Peng et al. 2025b), GPT-4o, general-domain LLM, and FinMA (Xie et al. 2023), financial LLM, achieve similar overall accuracy scores. However, GPT-4o tends to perform better on numerical computation tasks such as calculating the net change in cash, while FinMA is stronger at handling finance-specific conceptual questions, such as identifying total equity. Second, many prior datasets also exhibit coverage imbalance, where examples disproportionately rely on certain types of financial knowledge or skills, making it difficult to evaluate how models perform across the full range of real-world requirements. For example, in the accounting questions of FinEval (Guo et al. 2025), most items focus on narrow set of concepts such as total revenue and net income, while overlooking other areas like equity changes or tax-related components. To address these limitations, we introduce FinCDM, the first cognitive diagnosis evaluation framework for financial LLMs. Inspired by cognitive diagnosis model (CDM) in educational assessment, FinCDM evaluates LLMs like human examinees, identifying what financial skills and knowledge they have or lack based on their response patterns across skill-tagged tasks, rather than single aggregated number. First, we apply the Figure 1: The whole framework of FinCDM. non-negative matrix co-factorization based CDM model to estimate LLMs mastery on different financial skills with collected responses of LLMs, addressing the problem of score flattening. CDMs assume that the models answer to task reflects whether the model possesses all the underlying skills required by the question. Therefore, by decomposing the responses to all questions based on knowledge labels annotated for each task, we can assess the models proficiency in different knowledge, revealing where it excels and where it struggles. Second, we introduce the first Chinese dataset named CPA-QKA rigorously annotated by domain experts, specifically constructed to support the knowledgeskill level evaluation of FinCDM, and provide comprehensive evaluation coverage. The dataset is grounded in the Certified Public Accountant (CPA) exam, the most recognized professional certification in accounting and financial reporting. We build the financial knowledge framework covering 70 key concepts from the content and skill specification outline of the CPA exam, ensuring comprehensive coverage of real-world financial concepts. Under the framework, our dataset construction involves rigorous, two-phase annotation process. Domain experts first author multiple questions for per concept, and then perform verification to ensure accuracy, consistency, and high inter-annotator agreement (IAA). These verified questions are annotated with precise financial knowledge and skill tags, also employing IAA-based quality control. By annotating existing benchmarks using our framework, we further highlight their limitations, illustrating their narrow coverage of financial concepts and underscoring the need for the broader and deeper skill assessment that FinCDM provides. Figure 1 illustrates the complete workflow of FinCDM. Based on FinCDM, we evaluate 30 representative LLMs, including proprietary models, open-source general-purpose models of varying sizes, and finance-specific models. Our results support three key findings. First, FinCDM reveals significant differences in models mastery of financial knowledge that are not captured by traditional aggregate metrics. For example, Doubao shows strong performance on Chinese-specific regulations and specialized accounting areas, while Gemini demonstrates superior understanding of Debt Restructuring, Lease, Post-Balance Sheet Events, showcasing robust mastery in general accounting concepts. Despite these differences, both models achieve similar overall scores on FinQA, highlighting how traditional aggregate metrics can obscure underlying capability variation. Second, our evaluation illustrates that prior benchmarks predominantly assess narrow subset of financial concepts, resulting in inadequate coverage. In contrast, our new dataset systematically reveals previously overlooked weaknesses in areas like deferred tax liabilities, lease classification, and regulatory ratios, which are concepts rarely tested in existing benchmarks but critical in real-world applications. Third, by analyzing the knowledge-skill level mastery profiles produced by FinCDM, we identify latent associations between financial concepts and reveal distinct clusters of models exhibiting similar skill acquisition patterns. For example, GPT-3.5 and DeepSeek-VL share strengths in financial reporting and valuation, while FinGPT and FinQwen exhibit aligned capabilities in regulation and macroeconomic reasoning, reflecting different domain specialization strategies. In summary, our major contributions are: 1. We propose FinCDM, the first cognitive diagnosis framework for financial LLM evaluation, which moves beyond aggregate metrics by assessing models proficiency at the knowledge-skill level. 2. We construct new dataset and structured financial knowledge framework derived from the CPA exam, with high-quality human annotations. This supports reliable evaluation of FinCDM and exposes the narrow coverage of existing benchmarks. 3. We apply FinCDM to broad set of 30 proprietary, opensource, and finance-specific models, uncovering their knowledge gaps, strengths, and behavioral patterns, offering actionable insights for model development and deployment."
        },
        {
            "title": "2 Related Work",
            "content": "Financial Benchmark Numerous benchmarks have been developed to evaluate LLMs in financial domains, covering tasks such as extraction, QA, reasoning, simulation, and retrieval. Early efforts like PIXIU (Xie et al. 2023) and FinBen (Xie et al. 2024) offer broad coverage but rely on aggregate metrics. Benchmarks like FinanceBench (Islam et al. 2023), BizBench (Krumdick et al. 2024), and BizFinBench (Lu et al. 2025) focus on QA and business reasoning, while multilingual and low-resource evaluation is addressed by CFinBench (Nie et al. 2024b), CFLUE (Zhu et al. 2024), FinEval (Zhang et al. 2023), Golden Touchstone (Wu et al. 2024), Plutusben (Peng et al. 2025a), and the more recent MultiFinBen (Peng et al. 2025b). FinMTEB (Tang and Yang 2025) cover embedding-based retrieval and classification. Structured reasoning is evaluated in FinDABench (Liu et al. 2024b), Fino1 (Qian et al. 2025), FinChain (Xie et al. 2025), and agentic decision-making in InvestorBench (Li et al. 2024a), AlphaFin (Li et al. 2024b), and AveniBench (Klimaszewski et al. 2025). Multimodal understanding is addressed by M3FinMeeting (Zhu et al. 2025)and FinAudio (Cao et al. 2025).FinTagging (Wang et al. 2025) and FinDER (Choi et al. 2025) focus on fine-grained financial concept extraction and retrieval-augmented QA from 10-K filings and XBRL reports. Despite their breadth, these benchmarks rely on task-level aggregate metrics and lack concept-aware diagnostics. Financial Dataset Design wide range of data sets have been developed to evaluate financial LLM in reasoning, QA, information extraction, summarization, and multimodal tasks. Numerical reasoning datasets include FinQA (Chen et al. 2021), TATQA (Zhu et al. 2021), ConvFinQA (Chen et al. 2022), DocFinQA (Reddy et al. 2024), and FinanceQA (Mateega, Georgescu, and Tang 2025), which require multistep or conversational numerical inference over financial reports. FinTextQA (Chen et al. 2024), FinLLMs (Yuan et al. 2024), and FinTruthQA (Xu et al. 2024) focus on long-form or formula-based QA, while SEC-QA (Lai et al. 2024) and MultiHiertt (Zhao et al. 2022) introduce multidocument and table-text hybrid challenges. Structured extraction is addressed by FinTagging (Wang et al. 2025), FiNERORD (Shah et al. 2023), FiNER-139 (Loukas et al. 2022), FinRED (Sharma et al. 2022), REFinD (Kaur et al. 2023), which allow evaluation of concept and relationship levels aligned with financial taxonomies. For numeral understanding, FinNum (Chen et al. 2019), FinNum-2 (Chen et al. 2020), and FiNCAT (Ghosh and Naskar 2022) annotate finegrained semantic types or numeral claims in finance text. The summarization is covered by ECTSum (Mukherjee et al. 2022), FNS-2020 (El-Haj et al. 2020), focusing on earnings calls and annual reports. Chinese and multilingual resources include FinEval (Zhang et al. 2023), CFLUE (Zhu et al. 2024), and UCFE (Yang et al. 2024), enabling QA and NLU in non-English contexts. Multimodal datasets such as FinMME (Luo et al. 2025) and FinLMM-R1 (Lan et al. 2025) support chart, image, and documenttext alignment. Despite covering diverse formats and tasks, most existing datasets are task-driven with limited concept coverage. Cognitive Diagnosis Model Cognitive diagnosis models (CDMs), rooted in educational assessment, aim to infer mastery of latent knowledge attributes from observed response behaviors. Early interpretable models such as the Deterministic Inputs, Noisy And gate model (DINA)(De La Torre 2009), the Deterministic Inputs, Noisy Or gate model (DINO)(Templin and Henson 2006), and the Generalized DINA (GDINA) model(De La Torre 2011) employ binary latent attributes and probabilistic response functions; while valuable for interpretation, they depend heavily on accurate matrix specification and strict parametric assumptions (Gu and Xu 2019). To mitigate these limitations, matrix factorizationbased approaches, such as MF-DINA and logistic matrix factorization, embed examinees and attributes into low-dimensional spaces, enabling more flexible and robust modeling of item-attribute interactions. Neural networkbased CDMs such as NeuralCD (Wang et al. 2022) and recent graph-based models such as RCD (Gao et al. 2021) use deep architectures and disentangled graph learning to model nonlinear and noise-robust concept, exercise, and student representations. These neural approaches offer superior predictive power on large datasets but can suffer from overfitting and reduced interpretability when evaluated with few entities and many attributes."
        },
        {
            "title": "Preliminaries",
            "content": "In educational psychology, assessment plays central role not only in certifying achievement but also in supporting learning and guiding instruction. Rather than reporting single aggregate score, modern diagnostic assessments aim to provide interpretable and actionable insights into learners mastery of specific knowledge components (Frederiksen, Mislevy, and Bejar 1993; Leighton and Gierl 2007). This fine-grained feedback is foundational to formative assessment, enabling educators to design targeted interventions and fostering trust in the evaluation process (Kuh et al. 2011). Cognitive diagnosis models (CDMs) (Leighton and Gierl 2007) were developed to meet these goals by explicitly modeling how learners observed responses on assessment items reflect their their latent mastery of specific knowledge concepts. In the CDM framework, particular knowledge domain is defined by latent knowledge components (or attributes), and each learner is associated with binary mastery profile α {0, 1}K, where αk = 1 indicates mastery of the k-th skill. Each assessment item {1, . . . , I} is linked to subset of these skills through the expert-defined binary Q-matrix {0, 1}IK, where qik = 1 signifies that answering item correctly requires mastery of skill k. Given learners binary response vector {0, 1}I , where Xi = 1 denotes correct answer to item i, CDMs estimate the probability of correctly answering each item by modeling the relationship among the learners mastery profile α, the items skill requirements qi, and additional model-specific parameters θ: (Xi = 1 α, Q, θ) = fi(α, qi; θ) where, fi denotes the item response function for item i. The primary learning objective in CDMs is to accurately infer each learners skill mastery profile α and estimate the Metric Dataset CPA-KQA Fineval-KQA Krippendorffs Alpha σ KS-based measure 0.937 0.9904 1 - 0.9208 1 Table 1: Inter-annotator agreement metrics for human expert annotations on CPA-KQA and Fineval-KQA. quality items. Annotators were explicitly instructed to craft questions ensuring clarity and precision. To ensure reliability and consistency, we conducted rigorous quality validation. In the first phase, each authored question was independently reviewed by the two other experts (excluding the original author) to assess relevance, clarity, and alignment with the intended financial concept. Discrepancies were resolved through collaborative discussions. Inter-annotator agreement for question-concept mappings in this phase was measured using Krippendorffs alpha. In the second stage, all questions are independently annotated by the same two experts, who assign one or more knowledge labels selected from predefined set of 70 financial concepts. If the annotators assigned knowledge concepts are consistent with those originally provided by the question setter, the question proceeds without modification. In cases where the annotators disagree with each other or with the question setter, three-way discussion is held to resolve the disagreement and determine the final label. Annotation consistency in this stage is measured using the σ score and the KS-based measure (Braylan, Alonso, and Lease 2022). As shown in Table 1, our annotation achieved high consistency, with Krippendorffs alpha of 0.937, σ score of 0.9904, and KS-based measure of 1. These results confirm the high reliability and quality of our CPA-KQA dataset. Further annotation details are provided in the appendix. FinEval-KQA To further reveal the coverage of financial knowledge in existing datasets, we introduce the FinEvalKQA dataset, augmenting the FinEval question set (Zhang et al. 2023), widely used Chinese financial domain knowledge evaluation benchmark, with concept-level annotations based on the well-established framework of CPA-KQA. The FinEval dataset (Zhang et al. 2023) is publicly available resource containing over 2,000 questions designed to evaluate financial models on variety of financial knowledge areas. These questions span across multiple sub-domains, including financial analysis, accounting, and financial forecasting. We apply the CPA-KQA concept taxonomy to annotate subset of FinEval with 101 questions that focuses on accounting-related knowledge. Expert Annotation and Quality Validation: Our three expert annotators was tasked with mapping existing FinEval questions to the most relevant CPA-KQA concepts. guideline document (see appendix for more details) was created to ensure that each question was annotated consistently with respect to the CPA-KQA taxonomy, and the annotators were instructed to focus on clarity, specificity, and relevance. To Figure 2: 70 financial concepts covered by CPA-KQA. model parameters θ from observed response data. By inferring fine-grained knowledge states, CDMs enable transparent and skill-specific evaluation, offering richer insights than traditional test scores. Dataset Curation Existing benchmarks typically rely on aggregate score evaluations and cover only limited subset of financial concepts. To address these limitations, we propose new dataset CPAKQA with domain experts-authored financial questions, to support the knowledge-skill level evaluation of financial LLMs using CDMs, and provide comprehensive knowledge coverage. To construct CPA-KQA, we referenced the CPA examination, highly recognized professional certification in the financial industry, to ensure comprehensive coverage of financial knowledge. It covers core financial areas including accounting, auditing, financial cost management, corporate strategy and risk management, economic law, and tax law. We build the financial knowledge framework covering 70 core concepts, derived from the content and skill specification outline of the CPA exam 2, such as fixed assets, liabilities, and long-term investment decisions, as shown in figure 2. Expert Annotation and Quality Validation: Our data annotation involves two phases including question authoring and financial concepts tagging. The annotation team comprised three domain experts: an undergraduate student, masters student, and an associate professor specializing in finance. For each of the 70 financial concepts, three distinct questions were crafted by three annotators to ensure multiple cognitive perspectives, resulting in total of 210 high2https://www.cicpa.org.cn/xxfb/tzgg/202502/ W020250228535255887805.pdf ensure high-quality and consistent annotations, the interannotator agreement was evaluated using the same metrics as the original CPA-KQA annotations: σ and KS-based measure. We conducted reconciliation process in which annotators discussed any discrepancies in their annotations to ensure alignment. The results showed strong consistency, with σ of 0.9208 and KS-based measure of 1, demonstrating the robustness of our annotation process. Evaluation Framework Building upon our annotated datasets, we now illustrate how we assess the mastery of financial knowledge of different LLMs. Specifically, using our newly constructed CPAKQA, we can obtain the observed response data from LLMs, which forms our response matrix X, and leverage the expert-annotated Q. We employ non-negative matrix co-factorization inspired by SNMCF (Yu et al. 2023) and DINA (De La Torre 2009), modeling the process of evaluating LLM skill mastery explicitly through probabilistic generative model. Our framework assumes the following generative process: 1. Latent skill representation for questions: For each question i, generate latent skill requirement vector ei from Gamma prior: ei Gamma(a, b), ei RT 0. 2. Latent skill proficiency for models: For each LLM j, generate latent skill proficiency vector uj from Gamma prior: uj Gamma(c, d), uj RT 0. 3. Latent skill to financial concept mapping: For each financial concept k, generate latent skill-to-concept association vector vk from Gamma prior: vk Gamma(e, ), vk RT 0. 4. Generation of observed responses X: Each observed binary response xij indicating whether LLM correctly answers question is drawn from Bernoulli distribution parameterized by the alignment between the latent skill vectors: xij Bernoulli (cid:0)σ(e uj)(cid:1) , σ(z) = 1 1 + ez . 5. Generation of question-concept association Q: Similarly, the observed binary association qik, indicating whether question is related to concept k, is drawn from Bernoulli distribution parameterized by the alignment between their latent vectors: qik Bernoulli (cid:0)σ(e vk)(cid:1) , σ(z) = 1 1 + ez . Through this generative formulation, we aim to factorize the matrices and into low-dimensional latent representations with non-negative constraints: EU, EV, E, U, 0 0 where RM represents the relationship between questions and latent skills, capturing how strongly each question is related to each latent skill. RT captures the proficiency of each model in the latent skills. Each row in corresponds to latent skill, and each column represents the proficiency of model in that skill. RT represents the relationship between latent skills and financial concepts, mapping the latent skills to the relevant concepts for each question. We estimate these latent matrices by optimizing the following joint objective: 0 min E,U,V 0 (XEU )2 +βQEV 2 +λE E2 +λU F +λV 2 , The factorized matrices E, , and provide rich, interpretable structure that reveals the relationships between questions, concepts, and models capabilities. See appendix for the optimization process to inference these latent matrices. Finally, we explicitly estimate each LLMs mastery of financial concepts by combining the learned latent matrices and : = V, RN provides detailed, interpretable diagnostics, indicating the proficiency level of each LLM across the financial concepts. 0 , Benchmarking Based on FinCDM, we conduct benchmark study covering both closed-source and open-source models, including those specifically tailored for the financial domain. In total, our evaluation involves over 30 Chinese-capable LLMs. Detailed model information can be found in Appendix. We evaluate models in the following categories: 1. Closed-source general models: These include GPT-4, GPT-4o, and GPT-4o-mini (Achiam et al. 2023); Claude (Anthropic 2024); 3.5 Sonnet, and Claude 3.7 Sonnet Gemini 1.5 Pro, Gemini 1.5 Flash, and Gemini 2.5 Pro Experimental (Team et al. 2024); Grok-3 (xAI 2024); Doubao-1.5-Pro-256k, and Doubao-1.5-Pro-32k (ByteDance 2024). 2. Open-source These general models: include Baichuan2-13B-Chat (Baichuan Inc. 2023), ChatGLM36B (Zhipu AI 2024), Falcon-7B (Almazrouei et al. 2023), GLM-4-32B-0414 and GLM-4-9B-0414 (GLM et al. 2024); Qwen2-72B-Instruct, Qwen2.5-7B-Instruct, Qwen3-0.6B, and Qwen3-235B-A22b (Yang et al. 2025; Team 2024) DeepSeek-Chat and DeepSeek-V3-0324 (Bi et al. 2024; Liu et al. 2024a) and Hunyuan (Tencent Hunyuan Team 2025). 3. Financial domain models: These include Finma-7bFull (Xie et al. 2023), CFGPT2-7B (Li et al. 2023). For the evaluation, we prompt (see appendix for the prompt used in our experiments) each LLM to answer the questions in our datasets using consistent and controlled setup. The decoding configuration is set with temperature of 1.2 to promote diversity in generated responses, allowing models to explore varied responses under the same input. The maximum generation length is capped at 64 tokens to ensure concise answers, and each prompt instructs the model to return relevant and informative outputs. Each model generates 10 responses per question. The final performance scores are computed by averaging over these 10 responses, providing more reliable and robust estimate of model proficiency. All models are evaluated using unified script and configuration pipeline to ensure fairness and comparability across systems. performance is concentrated in narrow subset of concepts. These differences are not reflected in its aggregate score, which suggests uniformly strong performance. FinCDM exposes this imbalance by diagnosing both the breadth and depth of the models conceptual coverage, offering more interpretable and actionable analysis of model capabilities."
        },
        {
            "title": "Main Results",
            "content": "Table 2: Results on CPA-KQA and Fineval-KQA. The Con column means concept, which represents the number of concepts for which the model achieved mastery probability greater than 0.9. The data is sorted in descending order for ease of viewing. CPA-KQA Fineval-KQA Con Model 40/70 GLM4 39/70 Claude3.7 37/70 Qwen3-235b 36/70 Doubao32k 36/70 Qwen-max 34/70 Doubao256k 34/70 Qwen2.5-72b 33/70 Claude3.5 33/70 Gemini2.5pro 31/70 Gemini1.5 29/70 Hunyuan 27/70 Grok3 25/70 Deepseek-v3 25/70 Gpt4 24/70 Deepseek-chat 24/70 Qwen2.5-7b 24/70 Glm4-9b 22/70 Gemini1.5pro 21/70 Gpt4o-mini 20/70 Gpt4o 19/70 Glm4-32b 19/70 CFGPT2 17/70 Chatglm3 17/70 Baichuan2 13/70 DBRX Llama2 13/70 Llama3.1 11/70 10/70 Qwen3-0.6b 9/70 0/ Finma7b Falcon7b Acc 0.63 0.77 0.73 0.82 0.76 0.84 0.76 0.74 0.84 0.64 0.64 0.63 0.65 0.63 0.65 0.62 0.56 0.59 0.50 0.52 0.57 0.38 0.34 0.39 0.38 0.45 0.57 0.24 0.19 0.10 Con Model 13/38 Gemini1.5 13/38 Gemini2.5pro 11/38 Claude3.7 11/38 Doubao1.5pro 11/38 Qwen2.5-72b 10/38 Claude3.5 8/38 8/38 8/38 7/38 7/38 7/38 7/38 7/38 6/38 6/38 6/38 6/38 6/38 6/38 5/38 5/38 5/38 5/38 4/38 4/38 3/38 3/38 2/38 1/ Gpt4 Qwen-max Qwen3-235b DBRX Gpt4o-mini Qwen2.5-7b Glm4 Baichuan2 Doubao256k Gpt4o Llama2-70b Llama3.1 Glm4-32b CFGPT2 Deepseek-chat Deepseek-v3 Glm4-9b chatGlm3 Gemini1.5pro Hunyuan Grok3 Qwen3-0.6b Falcon Finma7b Acc 0.66 0.87 0.76 0.88 0.77 0.69 0.75 0.75 0.79 0.42 0.48 0.68 0.69 0.39 0.85 0.49 0.47 0.48 0.67 0.48 0.74 0.74 0.56 0.37 0.54 0.70 0.60 0.29 0.20 0.19 Table 2 and Figure 3 summarize the knowledge-skill level performance of various LLMs on our CPA-KQA dataset. Our key findings are as follows: The Role of FinCDM. FinCDM provides more informative evaluation than aggregate metrics by uncovering how models overall performance is distributed across individual financial concepts, highlighting both strengths and blind spots that remain hidden under traditional evaluation. As shown in Figure 3, Claude 3.5 Sonnet achieves high average accuracy on CPA-KQA, but FinCDM reveals that this Divergent Knowledge Specialization across HighPerforming Models. While many top-performing models achieve similar overall passing rates, our analysis further reveals substantial variation in the specific financial concepts each model masters. This divergence reflects differing strengths in subdomains of financial knowledge, suggesting that high aggregate accuracy does not necessarily imply uniform competence. For instance, Gemini-2.5-Pro-Exp and Doubao-1.5-Pro-256k both attain high overall passing rates of 0.84, respectively. However, closer inspection at the concept level reveals that Gemini excels in general accounting categories such as contingency and Lease, which are typically aligned with international financial In contrast, Doubao demonstrates reporting standards. stronger performance in financial cost management areas, particularly Long-term Investment Decisions, Long-term Financing Decisions, and Working Capital Management, reflecting its expertise in financial management domains. This pattern is consistent across other model pairs as well. These findings emphasize the importance of fine-grained, concept-level evaluation beyond aggregate metrics, and support the need for modular assessments that can reveal the unique strengths and limitations of each model. Influence of Linguistic Resources on Model Performance. Our evaluation highlights the substantial impact of linguistic resource availability on models performance in domain-specific tasks. Models with limited Chinese language capabilities consistently underperform, both in aggregate accuracy and in concept-level mastery. For example, Falcon-7B, which lacks robust pretraining on Chinese corpora, achieves passing rate of only 0.15 and demonstrates minimal competence across financial concepts. These results underscore the necessity of adequate linguistic grounding for effective domain adaptation. Without sufficient coverage of the target language during pretraining or finetuning, models struggle not only with general comprehension but also with acquiring specialized financial knowledge. Evaluating Datasets on Knowledge Mastery. To investigate how dataset structure shapes model understanding of financial knowledge, we evaluate multiple LLMs also on FINEVAL-KQA, an existing benchmark re-annotated at the knowledge-point level. As illustrated in Figure 4, FINEVALKPA exhibits significant structural imbalance, with majority of questions concentrated on few specific financial concepts, notably Financial Instruments, Fundamentals of Financial Management, Strategic Choices, Civil Law, and Commercial Law, the latter appearing as many as 13 times. This skewed distribution strongly influences model performance, causing mastery rates to be heavily dependent on the frequency and representation of concepts. In contrast, CPAKQA maintains more balanced representation across Figure 3: Model knowledge mastery heatmap on the CPA-KQA dataset. broader range of financial concepts, enabling more robust and generalizable assessments of conceptual understanding. For instance, Gemini1.5 demonstrates mastery across the relatively highest number of concepts but achieves low overall accuracy due to limited mastery of frequently tested concepts such as extitCommercial Law , highlighting how uneven concept distribution can disproportionately impact model performance."
        },
        {
            "title": "Ablation Studies",
            "content": "We further compare the performance of three categories of CDMs: Neural CDMs, Graph-based CDMs, and our employed Matrix Co-Factorization (MCF)-based method. Table 3 presents the results in terms of accuracy (acc), area under the curve (AUC), and root mean square error (RMSE). To compute these metrics, we first use the inferred latent matrices and to reconstruct the predicted response matrix ˆX = EU . We then compute the metrics by comparing these predictions against the observed response matrix X. MCF outperforms both baselines by significant margin, achieving the highest accuracy and AUC, and the lowest RMSE. These results correspond to absolute gains of +0.177 in accuracy and +0.146 in AUC, and reduction of 0.167 in RMSE compared to the strongest baseline, RCD. Table 3: Model performance comparison across architectures. Model Accuracy (acc) AUC RMSE Matrix Co-Factorization (MCF) Neural CDM (CNN-based) (Wang et al. 2022) GNN-based CDM (Gao et al. 2021) 0.9379 0.7140 0.7469 0.9873 0.7789 0.8329 0.2314 0.4394 0."
        },
        {
            "title": "Case Study",
            "content": "To evaluate the reliability of our diagnostic framework, we conduct focused case study on Claude 3.5, examining its mastery of two specific financial concepts: F3 and F5. According to our cognitive diagnosis analysis (Figure 3), Claude 3.5 failed to demonstrate mastery in these two concepts. Upon closer inspection, we confirmed that the model answered all six related questions incorrectly (three questions per concept). This provides concrete evidence that our diagnostic framework effectively identifies specific knowledge gaps in financial LLMs. To further verify the reliability of our framework, we enlisted five certified auditing experts to independently review the six incorrectly answered questions along with Claude 3.5s responses. All five experts hold at least an undergraduate degree in finance and auditing. They were asked to annotate the primary financial concept tested by each question without being informed of the original concept labels. As shown in Table 4, four of the five experts consistently assigned the questions to conknowledge representation. Through evaluations on 30 diverse LLMs, FinCDM reveals that models with similar overall scores often differ markedly in concept-level mastery, exposes coverage gaps in existing benchmarks, and uncovers latent specialization patterns across models. Future directions include multilingual extensions, incorporation of multimodal financial content, and leveraging diagnostic feedback to inform instruction tuning and benchmark design. Introducing the Claude 3 model family. References Achiam, J.; Adler, S.; Agarwal, S.; Ahmad, L.; Akkaya, I.; Aleman, F. L.; Almeida, D.; Altenschmidt, J.; Altman, S.; Anadkat, S.; et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774. Almazrouei, E.; Alobeidli, H.; Alshamsi, A.; Cappelli, A.; Cojocaru, R.; Debbah, M.; Goffinet, E.; Hesslow, D.; Launay, J.; Malartic, Q.; et al. 2023. The falcon series of open language models. arXiv preprint arXiv:2311.16867. Anthropic. 2024. Accessed: 2025-07-30. Baichuan Inc. 2023. Baichuan2: Open-Source Large Language Models. Accessed: 2025-07-30. Bi, X.; Chen, D.; Chen, G.; Chen, S.; Dai, D.; Deng, C.; Ding, H.; Dong, K.; Du, Q.; Fu, Z.; et al. 2024. Deepseek llm: Scaling open-source language models with longtermism. arXiv preprint arXiv:2401.02954. Braylan, A.; Alonso, O.; and Lease, M. 2022. Measuring annotator agreement generally across complex structured, multi-object, and free-text annotation tasks. In Proceedings of the ACM Web Conference 2022, 17201730. ByteDance. 2024. Doubao 1.5 Pro. Accessed: 2025-07-30. Cao, Y.; Li, H.; Yu, Y.; Javaji, S. R.; He, Y.; Huang, J.; Zhu, Z.; Xie, Q.; Liu, X.-y.; Subbalakshmi, K.; et al. 2025. FinAudio: Benchmark for Audio Large Language Models in Financial Applications. arXiv preprint arXiv:2503.20990. Chen, C.-C.; Huang, H.-H.; Takamura, H.; and Chen, H.-H. 2019. Overview of the ntcir-14 finnum task: Fine-grained numeral understanding in financial social media data. In Proceedings of the 14th NTCIR Conference on Evaluation of Information Access Technologies, 1927. Chen, C.-C.; Huang, H.-H.; Takamura, H.; and Chen, H.-H. 2020. Overview of the NTCIR-15 FinNum-2 Task: Numeral attachment in financial tweets. Development, 850(194): 1 044. Chen, J.; Zhou, P.; Hua, Y.; Loh, Y.; Chen, K.; Li, Z.; Fintextqa: dataset for Zhu, B.; and Liang, J. 2024. arXiv preprint long-form financial question answering. arXiv:2405.09980. Chen, Z.; Chen, W.; Smiley, C.; Shah, S.; Borova, I.; Langdon, D.; Moussa, R.; Beane, M.; Huang, T.-H.; Routledge, B.; et al. 2021. Finqa: dataset of numerical reasoning over financial data. arXiv preprint arXiv:2109.00122. Chen, Z.; Li, S.; Smiley, C.; Ma, Z.; Shah, S.; and Wang, W. Y. 2022. Convfinqa: Exploring the chain of numerical reasoning in conversational finance question answering. arXiv preprint arXiv:2210.03849. Figure 4: Structure of FinEval-KQA. The lighter-colored blocks represent concepts covered by FinEval-KQA, while the darker-colored blocks indicate concepts missing from it. The further block extends from the center, the more frequently that concept was assessed. cepts F3 and F5. The fifth expert introduced minor variations, such as labeling one question as F2 or F4. The overall inter-annotator agreement was 0.80, indicating strong consensus among the experts. These results demonstrate that our diagnosis framework aligns closely with expert judgment in identifying these conceptual deficiencies. Table 4: Expert annotations of knowledge points associated with six incorrectly answered questions."
        },
        {
            "title": "Question Index",
            "content": "Expert 1 Expert 2 Expert 3 Expert 4 Expert 5 1 F3 F3 F3 F2 F3 2 F3 F3 F3 F3 F3 F3 F3 F3 F4 F3 4 F5 F5 F5 F5 F5 5 F5 F5 F5 F5 F5 F5 F5 F5 F5 F"
        },
        {
            "title": "5 Conclusion\nThis work presents FinCDM, the first cognitive diagno-\nsis framework for evaluating financial LLMs beyond con-\nventional aggregate metrics. Inspired by educational as-\nsessment, FinCDM diagnoses model proficiency at\nthe\nknowledge-skill level, enabling interpretable insights into\nwhat financial concepts a model has mastered or misun-\nderstood. To support this, we introduce CPA-KQA, a high-\nquality, expert-annotated benchmark grounded in the CPA\nexam, covering 70 core financial concepts with balanced",
            "content": "Choi, C.; Kwon, J.; Ha, J.; Choi, H.; Kim, C.; Lee, Y.; Sohn, J.-y.; and Lopez-Lira, A. 2025. Finder: Financial dataset for question answering and evaluating retrieval-augmented generation. arXiv preprint arXiv:2504.15800. De La Torre, J. 2009. DINA model and parameter estimation: didactic. Journal of educational and behavioral statistics, 34(1): 115130. De La Torre, J. 2011. The generalized DINA model framework. Psychometrika, 76(2): 179199. Duan, X.; Xiao, B.; Tang, X.; and Cai, Z. G. 2024. HLB: Benchmarking LLMs Humanlikeness in Language Use. arXiv preprint arXiv:2409.15890. El-Haj, M.; Litvak, M.; Pittaras, N.; Giannakopoulos, G.; et al. 2020. The financial narrative summarisation shared task (FNS 2020). In Proceedings of the 1st Joint Workshop on Financial Narrative Processing and MultiLing Financial Summarisation, 112. Frederiksen, N.; Mislevy, R. J.; and Bejar, I. I. 1993. Test theory for new generation of tests. L. Erlbaum Associates. Gao, W.; Liu, Q.; Huang, Z.; Yin, Y.; Bi, H.; Wang, M.-C.; Ma, J.; Wang, S.; and Su, Y. 2021. RCD: Relation map driven cognitive diagnosis for intelligent education systems. In Proceedings of the 44th international ACM SIGIR conference on research and development in information retrieval, 501510. Ghosh, S.; and Naskar, S. K. 2022. Fincat: Financial numeral claim analysis tool. In Companion Proceedings of the Web Conference 2022, 583585. GLM, T.; Zeng, A.; Xu, B.; Wang, B.; Zhang, C.; Yin, D.; Zhang, D.; Rojas, D.; Feng, G.; Zhao, H.; et al. 2024. Chatglm: family of large language models from glm-130b to glm-4 all tools. arXiv preprint arXiv:2406.12793. Gu, Y.; and Xu, G. 2019. The sufficient and necessary condition for the identifiability and estimability of the DINA model. Psychometrika, 84(2): 468483. Guo, X.; Xia, H.; Liu, Z.; Cao, H.; Yang, Z.; Liu, Z.; Wang, S.; Niu, J.; Wang, C.; Wang, Y.; et al. 2025. FinEval: Chinese Financial Domain Knowledge Evaluation BenchIn Proceedings of the mark for Large Language Models. 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), 62586292. Hendrycks, D.; Burns, C.; Basart, S.; Zou, A.; Mazeika, M.; Song, D.; and Steinhardt, J. 2021. Measuring Massive Multitask Language Understanding. In International Conference on Learning Representations. Islam, P.; Kannappan, A.; Kiela, D.; Qian, R.; Scherrer, N.; and Vidgen, B. 2023. Financebench: new bencharXiv preprint mark for financial question answering. arXiv:2311.11944. Kaur, S.; Smiley, C.; Gupta, A.; Sain, J.; Wang, D.; Siddagangappa, S.; Aguda, T.; and Shah, S. 2023. REFinD: Relation extraction financial dataset. In Proceedings of the 46th international ACM SIGIR conference on research and development in information retrieval, 30543063. Klimaszewski, M.; Chen, P.; Guillou, L.; Papaioannou, I.; Haddow, B.; and Birch, A. 2025. AVENIBENCH: Accessible and Versatile Evaluation of Finance Intelligence. In Proceedings of the Joint Workshop of the 9th Financial Technology and Natural Language Processing (FinNLP), the 6th Financial Narrative Processing (FNP), and the 1st Workshop on Large Language Models for Finance and Legal (LLMFinLegal), 111117. Krumdick, M.; Koncel-Kedziorski, R.; Lai, V. D.; Reddy, V.; Lovering, C.; and Tanner, C. 2024. Bizbench: quantitative reasoning benchmark for business and finance. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 83098332. Kuh, G. D.; Kinzie, J.; Buckley, J. A.; Bridges, B. K.; and Hayek, J. C. 2011. Piecing together the student success puzzle: Research, propositions, and recommendations: ASHE higher education report, volume 116. John Wiley & Sons. Lai, V. D.; Krumdick, M.; Lovering, C.; Reddy, V.; Schmidt, C.; and Tanner, C. 2024. Sec-qa: systematic evaluation corpus for financial qa. arXiv preprint arXiv:2406.14394. Lan, K.; Zhu, J.; Li, J.; Cheng, D.; Chen, G.; and Jiang, C. 2025. FinLMM-R1: Enhancing Financial Reasoning in LMM through Scalable Data and Reward Design. arXiv preprint arXiv:2506.13066. Leighton, J.; and Gierl, M. 2007. Cognitive diagnostic assessment for education: Theory and applications. Cambridge University Press. Li, H.; Cao, Y.; Yu, Y.; Javaji, S. R.; Deng, Z.; He, Y.; Jiang, Y.; Zhu, Z.; Subbalakshmi, K.; Xiong, G.; et al. 2024a. Investorbench: benchmark for financial decision-making tasks with llm-based agent. arXiv preprint arXiv:2412.18174. Li, J.; Bian, Y.; Wang, G.; Lei, Y.; Cheng, D.; Ding, Z.; and Jiang, C. 2023. Cfgpt: Chinese financial assistant with large language model. arXiv preprint arXiv:2309.10654. Li, X.; Li, Z.; Shi, C.; Xu, Y.; Du, Q.; Tan, M.; Huang, J.; and Lin, W. 2024b. Alphafin: Benchmarking financial analysis with retrieval-augmented stock-chain framework. arXiv preprint arXiv:2403.12582. Liang, P.; Bommasani, R.; Lee, T.; Tsipras, D.; Soylu, D.; Yasunaga, M.; Zhang, Y.; Narayanan, D.; Wu, Y.; Kumar, A.; et al. 2022. Holistic evaluation of language models. arXiv preprint arXiv:2211.09110. Liu, A.; Feng, B.; Xue, B.; Wang, B.; Wu, B.; Lu, C.; Zhao, C.; Deng, C.; Zhang, C.; Ruan, C.; et al. 2024a. Deepseekv3 technical report. arXiv preprint arXiv:2412.19437. Liu, S.; Zhao, S.; Jia, C.; Zhuang, X.; Long, Z.; Zhou, J.; Zhou, A.; Lan, M.; Wu, Q.; and Yang, C. 2024b. Findabench: Benchmarking financial data analysis ability of large language models. arXiv preprint arXiv:2401.02982. Loukas, L.; Fergadiotis, M.; Chalkidis, I.; Spyropoulou, E.; Malakasiotis, P.; Androutsopoulos, I.; and Paliouras, G. 2022. FiNER: Financial numeric entity recognition for XBRL tagging. arXiv preprint arXiv:2203.06482. Lu, G.; Guo, X.; Zhang, R.; Zhu, W.; and Liu, J. 2025. BizFinBench: Business-Driven Real-World Financial Benchmark for Evaluating LLMs. arXiv preprint arXiv:2505.19457. Luo, J.; Kou, Z.; Yang, L.; Luo, X.; Huang, J.; Xiao, Z.; Peng, J.; Liu, C.; Ji, J.; Liu, X.; et al. 2025. FinMME: Benchmark Dataset for Financial Multi-Modal Reasoning Evaluation. arXiv preprint arXiv:2505.24714. Mateega, S.; Georgescu, C.; and Tang, D. 2025. Financeqa: benchmark for evaluating financial analysis capabilities of large language models. arXiv preprint arXiv:2501.18062. Mukherjee, R.; Bohra, A.; Banerjee, A.; Sharma, S.; Hegde, M.; Shaikh, A.; Shrivastava, S.; Dasgupta, K.; Ganguly, N.; Ghosh, S.; et al. 2022. Ectsum: new benchmark dataset for bullet point summarization of long earnings call transcripts. arXiv preprint arXiv:2210.12467. Nie, Y.; Kong, Y.; Dong, X.; Mulvey, J. M.; Poor, H. V.; Wen, Q.; and Zohren, S. 2024a. survey of large language models for financial applications: Progress, prospects and challenges. arXiv preprint arXiv:2406.11903. Nie, Y.; Yan, B.; Guo, T.; Liu, H.; Wang, H.; He, W.; Zheng, B.; Wang, W.; Li, Q.; Sun, W.; et al. 2024b. Cfinbench: comprehensive chinese financial benchmark for large language models. arXiv preprint arXiv:2407.02301. Peng, X.; Papadopoulos, T.; Soufleri, E.; Giannouris, P.; Xiang, R.; Wang, Y.; Qian, L.; Huang, J.; Xie, Q.; and Ananiadou, S. 2025a. Plutus: Benchmarking large language models in low-resource greek finance. arXiv preprint arXiv:2502.18772. Peng, X.; Qian, L.; Wang, Y.; Xiang, R.; He, Y.; Ren, Y.; Jiang, M.; Zhao, J.; He, H.; Han, Y.; et al. 2025b. MultiFinBen: Multilingual, Multimodal, and Difficulty-Aware Benchmark for Financial LLM Evaluation. arXiv preprint arXiv:2506.14028. Qian, L.; Zhou, W.; Wang, Y.; Peng, X.; Huang, J.; and Xie, Q. 2025. Fino1: On the transferability of reasoning enhanced llms to finance. arXiv e-prints, arXiv2502. Reddy, V.; Koncel-Kedziorski, R.; Lai, V. D.; Krumdick, M.; Lovering, C.; and Tanner, C. 2024. Docfinqa: arXiv preprint long-context financial reasoning dataset. arXiv:2401.06915. Shah, A.; Gullapalli, A.; Vithani, R.; Galarnyk, M.; and Chava, S. 2023. FiNER-ORD: Financial Named Entity Recognition Open Research Dataset. arXiv preprint arXiv:2302.11157. Sharma, S.; Nayak, T.; Bose, A.; Meena, A. K.; Dasgupta, K.; Ganguly, N.; and Goyal, P. 2022. FinRED: dataset In Companion for relation extraction in financial domain. Proceedings of the Web Conference 2022, 595597. Tang, Y.; and Yang, Y. 2025. Finmteb: Finance massive text embedding benchmark. arXiv preprint arXiv:2502.10990. Team, G.; Georgiev, P.; Lei, V. I.; Burnell, R.; Bai, L.; Gulati, A.; Tanzer, G.; Vincent, D.; Pan, Z.; Wang, S.; et al. 2024. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530. Team, Q. 2024. Qwen2 technical report. arXiv preprint arXiv:2407.10671. Templin, J. L.; and Henson, R. A. 2006. Measurement of psychological disorders using cognitive diagnosis models. Psychological methods, 11(3): 287. Tencent Hunyuan Team. 2025. Hunyuan-A13B: An 80B-parameter MoE model with only 13B active parameters, 256K context support, and hybrid reasoning modes. Technical report, Tencent Hunyuan Model Team. Accessed: 2025-07-30. Touvron, H.; Lavril, T.; Izacard, G.; Martinet, X.; Lachaux, M.-A.; Lacroix, T.; Rozi`ere, B.; Goyal, N.; Hambro, E.; Azhar, F.; et al. 2023. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971. Wang, F.; Liu, Q.; Chen, E.; Huang, Z.; Yin, Y.; Wang, S.; and Su, Y. 2022. NeuralCD: general framework for cognitive diagnosis. IEEE Transactions on Knowledge and Data Engineering, 35(8): 83128327. Wang, Y.; Ren, Y.; Qian, L.; Peng, X.; Wang, K.; Han, Y.; Feng, D.; Liu, X.-Y.; Huang, J.; and Xie, Q. 2025. FinTagging: An LLM-ready Benchmark for Extracting arXiv preprint and Structuring Financial Information. arXiv:2505.20650. Wu, X.; Liu, J.; Su, H.; Lin, Z.; Qi, Y.; Xu, C.; Su, J.; Zhong, J.; Wang, F.; Wang, S.; et al. 2024. Golden Touchstone: Comprehensive Bilingual Benchmark for Evaluating Financial Large Language Models. arXiv preprint arXiv:2411.06272. xAI. 2024. Introducing Grok-3. Accessed: 2025-07-30. Xie, Q.; Han, W.; Chen, Z.; Xiang, R.; Zhang, X.; He, Y.; Xiao, M.; Li, D.; Dai, Y.; Feng, D.; et al. 2024. Finben: holistic financial benchmark for large language models. Advances in Neural Information Processing Systems, 37: 9571695743. Xie, Q.; Han, W.; Zhang, X.; Lai, Y.; Peng, M.; Lopez-Lira, A.; and Huang, J. 2023. Pixiu: comprehensive benchmark, instruction dataset and large language model for finance. Advances in Neural Information Processing Systems, 36: 3346933484. Xie, Z.; Sahnan, D.; Banerjee, D.; Georgiev, G.; Thareja, R.; Madmoun, H.; Su, J.; Singh, A.; Wang, Y.; Xing, R.; et al. 2025. FinChain: Symbolic Benchmark for Verifiable Chain-of-Thought Financial Reasoning. arXiv preprint arXiv:2506.02515. Xu, Z.; Zhou, P.; Shi, X.; Wu, J.; Jiang, Y.; Chong, D.; Ke, B.; and Yang, J. 2024. Fintruthqa: benchmark dataset for evaluating the quality of financial information disclosure. arXiv preprint arXiv:2406.12009. Yang, A.; Li, A.; Yang, B.; Zhang, B.; Hui, B.; Zheng, B.; Yu, B.; Gao, C.; Huang, C.; Lv, C.; et al. 2025. Qwen3 technical report. arXiv preprint arXiv:2505.09388. Yang, Y.; Zhang, Y.; Hu, Y.; Guo, Y.; Gan, R.; He, Y.; Lei, M.; Zhang, X.; Wang, H.; Xie, Q.; et al. 2024. Ucfe: user-centric financial expertise benchmark for large language models. arXiv preprint arXiv:2410.14059. Required Format for Item Submission Submit each item strictly following this format: Knowledge Point: (Name of the Knowledge Point) Question Stem: Clearly and completely stated Options (only for single-choice items): A. ... B. ... C. ... D. ... Correct Answer: (Option letter or detailed solution) Annotation and Quality Review Procedures Experts will create and annotate items as follows: Each expert independently creates items for their assigned knowledge points. Upon completion, two other experts annotate each item on two dimensions: Quality Annotation: Mark as either Usable or Unusable. Criteria for Unusable include unclear meaning, inaccurate wording, calculation errors, ambiguous answers, or controversial content. Knowledge Point Consistency Annotation: Verify if the item accurately corresponds to the specified knowledge point. Items marked Unusable or Inconsistent by both reviewers are deleted directly. Items marked by only one reviewer require one-time revision and must pass re-review. Deleted items must be replaced until each knowledge point has three approved items. Optimization Process for Non-negative Matrix Co-factorization We estimate these latent matrices by optimizing the following joint objective: min E,U,V (X EU )2 + αQ EV 2 + λU 2 with multiplicative update rules for convergence to local + λV 2 , + λEE2 optimum: (W X)U + αQV (W EU )U + αEV + λEE , , E(W X) E(W EU ) + λU αEQ αEEV + λV , where the symbol denotes element-wise multiplication, and division operations are also element-wise. Yu, S.; Zeng, Y.; Pan, Y.; and Yang, F. 2023. Snmcf: scalable non-negative matrix co-factorization for student cognitive modeling. IEEE Transactions on Knowledge and Data Engineering, 36(7): 34863500. Yuan, Z.; Wang, K.; Zhu, S.; Yuan, Y.; Zhou, J.; Zhu, Y.; and Wei, W. 2024. Finllms: framework for financial reasoning dataset generation with large language models. IEEE Transactions on Big Data. Zhang, L.; Cai, W.; Liu, Z.; Yang, Z.; Dai, W.; Liao, Y.; Qin, Q.; Li, Y.; Liu, X.; Liu, Z.; et al. 2023. Fineval: chinese financial domain knowledge evaluation benchmark for large language models. arXiv preprint arXiv:2308.09975. Zhao, Y.; Li, Y.; Li, C.; and Zhang, R. 2022. MultiHiertt: Numerical reasoning over multi hierarchical tabular and textual data. arXiv preprint arXiv:2206.01347. Zhipu AI. 2024. ChatGLM3: Open Bilingual Chat Models. Accessed: 2025-07-30. Zhu, F.; Lei, W.; Huang, Y.; Wang, C.; Zhang, S.; Lv, J.; Feng, F.; and Chua, T.-S. 2021. TAT-QA: question answering benchmark on hybrid of tabular and textual content in finance. arXiv preprint arXiv:2105.07624. Zhu, J.; Li, J.; Wen, Y.; and Guo, L. 2024. Benchmarking Large Language Models on CFLUEA Chinese Financial Language Understanding Evaluation Dataset. arXiv preprint arXiv:2405.10542. Zhu, J.; Li, J.; Wen, Y.; Li, X.; Guo, L.; and Chen, F. 2025. 3 FinMeeting: Multilingual, Multi-Sector, and MultiTask Financial Meeting Understanding Evaluation Dataset. arXiv preprint arXiv:2506.02510."
        },
        {
            "title": "A Guidelines for Creating and Annotating",
            "content": "CPA-KQA Items Principles and Requirements for Item Creation Consistency with CPA Exam Style Each assessment item must align closely with the CPA exam in terms of wording, question format, and option setting, reflecting the CPA exams rigorous standards for professional knowledge. Originality and Accuracy All items must be original creations. Direct copying or quoting from existing CPA exam items or publicly available materials is strictly prohibited. Ensure accuracy, clarity, and absence of ambiguity in each item. Specificity and Clarity Each item should clearly assess only one specified financial knowledge point. Avoid incorporating content irrelevant to the specified knowledge point to ensure accurate and effective evaluation. Avoidance of Politically Sensitive Content Strictly avoid incorporating politically sensitive topics, events, or individuals in any assessment items. Content that may be viewed as politically controversial or sensitive is explicitly prohibited. Item Formats Create three items per knowledge point, in Single-choice questions (at least one per knowledge point) Provide four clearly distinguishable options (A, B, C, D), with exactly one correct answer."
        },
        {
            "title": "Access Version Date",
            "content": "Open-Source Closed-Source GLM-4-32B-0414 GLM-4-9B-0414 Hunyuan DeepSeek-Chat DeepSeek-V3-0324 DBRX-Instruct Qwen2-72B-Instruct Qwen2.5-7B-Instruct Qwen3-0.6B Qwen3-235B-A22b LLaMA2-70B LLaMA3.1 405B Baichuan2-13B-Chat Falcon-7B ChatGLM3-6B Zhipu AI Zhipu AI Tencent DeepSeek AI DeepSeek AI Databricks Alibaba Cloud Alibaba Cloud Alibaba Cloud Alibaba Cloud Meta Meta Baichuan Inc. TII Zhipu AI"
        },
        {
            "title": "OpenAI\nOpenAI\nOpenAI\nGoogle\nGoogle",
            "content": "GPT-4o GPT-4o-mini GPT-4 Gemini 1.5 Pro Gemini 1.5 Flash Gemini2.5 Pro Experimental 03-25 Claude 3.5 Sonnet Claude 3.7 Sonnet GLM-4 Grok 3 Doubao-1.5-Pro-256k ByteDance ByteDance Doubao-1.5-Pro-32k Alibaba Cloud Qwen-Max Anthropic Anthropic Zhipu AI xAI Google undisclosed undisclosed undisclosed 236B (MoE) 671B (MoE) 132B (MoE) 72B 7B 0.6B 235B(MOE) 70B 405B 13B 7B 6B undisclosed undisclosed undisclosed undisclosed undisclosed undisclosed undisclosed undisclosed undisclosed undisclosed undisclosed undisclosed undisclosed"
        },
        {
            "title": "API\nAPI\nAPI\nAPI\nAPI",
            "content": "API API API API API API API API Financial Finma-7b-Full CFGPT2-7B Finma TongjiFinLab 7B 7B Weights Weights 2025.4 2025.4 2023.9 2024.5 2025.3 2024.3 2024.6 2024.9 2025.4 2025.4 2023.7 2024.7 2023.12 2023.5 2023.1 2024.5 2024.7 2023.3 2024.5 2024.5 2025.3 2024.1 2025.2 2024.1 2025.2 2025.1 2025.1 2025.1 2023.9 2024. Table 5: Models evaluated in this paper. The Access column shows whether we have full access to the model weights or we can only access through API. The Version Date column shows the release date of the corresponding version of the model we evaluated."
        },
        {
            "title": "D prompt",
            "content": "Figure 5: Prompt for multiple-choice questions in Intermediate Financial Accounting. For better readability, the English translation is displayed to the right of the corresponding Chinese text."
        }
    ],
    "affiliations": [
        "Beijing University of Financial Technology",
        "Center for the Study of Language and Information, Wuhan University",
        "Computer Science, University of Manchester",
        "Computer and Cyber Sciences, Augusta University",
        "School of Artificial Intelligence, Wuhan University",
        "School of Computer Science, Nanjing Audit University",
        "Southwest Jiaotong University"
    ]
}