{
    "paper_title": "Learning Eigenstructures of Unstructured Data Manifolds",
    "authors": [
        "Roy Velich",
        "Arkadi Piven",
        "David Bensaïd",
        "Daniel Cremers",
        "Thomas Dagès",
        "Ron Kimmel"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce a novel framework that directly learns a spectral basis for shape and manifold analysis from unstructured data, eliminating the need for traditional operator selection, discretization, and eigensolvers. Grounded in optimal-approximation theory, we train a network to decompose an implicit approximation operator by minimizing the reconstruction error in the learned basis over a chosen distribution of probe functions. For suitable distributions, they can be seen as an approximation of the Laplacian operator and its eigendecomposition, which are fundamental in geometry processing. Furthermore, our method recovers in a unified manner not only the spectral basis, but also the implicit metric's sampling density and the eigenvalues of the underlying operator. Notably, our unsupervised method makes no assumption on the data manifold, such as meshing or manifold dimensionality, allowing it to scale to arbitrary datasets of any dimension. On point clouds lying on surfaces in 3D and high-dimensional image manifolds, our approach yields meaningful spectral bases, that can resemble those of the Laplacian, without explicit construction of an operator. By replacing the traditional operator selection, construction, and eigendecomposition with a learning-based approach, our framework offers a principled, data-driven alternative to conventional pipelines. This opens new possibilities in geometry processing for unstructured data, particularly in high-dimensional spaces."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 3 ] . [ 1 3 0 1 1 0 . 2 1 5 2 : r a"
        },
        {
            "title": "Learning Eigenstructures of Unstructured Data Manifolds",
            "content": "Roy Velich1* Arkadi Piven1 David Bensaıd1 Daniel Cremers2,3 Thomas Dag`es1,2,3 Ron Kimmel1 In memory of Haım Brezis (1944 2024)."
        },
        {
            "title": "Abstract",
            "content": "We introduce novel framework that directly learns spectral basis for shape and manifold analysis from unstructured data, eliminating the need for traditional operator selection, discretization, and eigensolvers. Grounded in optimal-approximation theory, we train network to decompose an implicit approximation operator by minimizing the reconstruction error in the learned basis over chosen distribution of probe functions. For suitable distributions, they can be seen as an approximation of the Laplacian operator and its eigendecomposition, which are fundamental in geometry processing. Furthermore, our method recovers in unified manner not only the spectral basis, but also the implicit metrics sampling density and the eigenvalues of the underlying operator. Notably, our unsupervised method makes no assumption on the data manifold, such as meshing or manifold dimensionality, allowing it to scale to arbitrary datasets of any dimension. On point clouds lying on surfaces in 3D and high-dimensional image manifolds, our approach yields meaningful spectral bases, that can resemble those of the Laplacian, without explicit construction of an operator. By replacing the traditional operator selection, construction, and eigendecomposition with learningbased approach, our framework offers principled, datadriven alternative to conventional pipelines. This opens new possibilities in geometry processing for unstructured data, particularly in high-dimensional spaces. 1. Introduction Differential geometry is at the core of shape and manifold analysis. By defining operators, one can encode the underlying geometry and compute intrinsic and extrinsic quantities, from geodesics and Gaussian curvature to mean curvature and normals. The specific choice of operator depends on the task at hand. For example, the Laplace-Beltrami operator (LBO) captures intrinsic geo- *Corresponding author: royve@campus.technion.ac.il 1Technion - Israel Institute of Technology 2Technical University of Munich 3Munich Center for Machine Learning metric properties of the underlying manifold and governs heat diffusion. Hamiltonian operators, appearing in the famous Schrodinger equation, model wave motion and allow potential-modulated particle dynamics. The biharmonic operator encodes fourth-order behavior and is natural for smooth interpolation and shape-aware distances. In practice, these operators are primarily used for their spectral decompositions, which serve as the computational foundation for geometry processing. For instance, LBO eigenvalues are intrinsic shape signatures [49, 50, 61, 99, 104], while LBO eigenfunctions power heatand wavekernel methods for smoothing and propagation [35, 54], define feature descriptors [7, 110, 120], and support matching via functional maps [90]. As such, these eigendecompositions are routinely required in geometry processing. The standard pipeline for computing operator eigendecompositions on discrete data begins by choosing discrete approximation that preserves key geometric properties such as positive semi-definiteness (PSD), symmetry, consistency, or weak formulations [97, 115, 132]. Then, explicitly construct matrix acting as the discretized linear operator and solve generalized eigenvalue problem using classical numerical solver. Although effective on data sampled from surfaces in 3D, this pipeline designed for intrinsically twodimensional manifolds does not scale gracefully to highdimensional manifolds [115]. Here, we propose different route. Rather than targeting specific operator, explicitly discretizing and eigendecomposing it, we learn the spectral decomposition directly from the data. The learned spectral decomposition implicitly corresponds to an operator that is reconstructible posteriori from the basis and associated eigenvalues. Building on optimal-approximation theory [3, 4, 24], we find the eigenbasis of an optimal-approximation operator, that minimizes the reconstruction error over class of probe functions. Different probe classes induce different metrics and thus different operators. In particular cases, the learned operator approximates the LBO, but the proposed framework generalizes to more operators. While only the eigenbasis is learned, the eigenvalues are given directly as by-product of the worst-case reconstruction error. This yields learning methodology for spectral analysis that is data-driven and 1 avoids the need for explicit operator construction. To summarize, our contributions are as follows: We learn spectral bases directly from point clouds, bypassing the explicit choice of an operator, its construction, and classical numerical eigensolvers. We ground the method in an optimal-basis representation theory and use it to propose unique learning objective. We demonstrate on surfaces in 3D that the proposed approach provides eigenstructures and estimated metrics performing competitively with oracle discrete LBO baselines, while bypassing explicit operator construction. We show that the method scales from surfaces to higherdimensional data manifolds, enabling scalable manifold learning where mesh-based pipelines are inapplicable and common graph-based ones are unreliable. We will release our code as open source upon acceptance. 2. Related Works Our framework lies at the intersection of operator discretization and neural methods for eigenproblems. Discrete operators. In geometry processing, discrete operators supply via spectral decomposition [28] the orthonormal basis that we actually use to process signals on meshes and graphs. For instance, filtering [15, 70], diffusion [35, 116], and convolution [27, 71] are usually implemented in this spectral basis. Notably, the community rarely applies the operator matrix directly. In practice, only the first eigenvectors are kept, which amounts to projecting signal onto the lowest-energy subspace defined by the operator. This acts as low-pass filter where only the (smooth) low-frequency components are retained. Because the operator defines what energy, smoothness, and frequency mean, different operators emphasize different properties of the manifold. This motivates the importance of choosing operators wisely in the field of geometry processing. Perhaps the most important example, the LBO [12, 106] is ubiquitous in geometry processing [2022, 26, 30, 120, 136, 142]. Discretizing the weak form gives the cotangent Laplacian [84, 97] the reference in LBO discretization, but alternatives exist [32, 139]. Cotangent weights need welltriangulated meshes, i.e. Delaunay, to avoid negative edges violating the maximum principle [132]. They cannot be directly applied to unstructured data, like point clouds, requiring first wise meshings, e.g. with the tufted cover of the Robust Laplacian [115] enabling flips to Delaunay triangulations. While it extends to thin 3D volumes, this approach does not scale beyond surfaces. For higher-dimensional manifolds, graph Laplacians [33, 64] are used, but they depend strongly on connectivity, e.g. local density, unlike the targeted smooth LBO [72]. Constructing reliable highdimensional generalization of the LBO is challenge. Learning Laplacians has become popular line of research as the amount of data increases. Some works suggest learning the operator action [100] but lack eigendecompositions. Others learn eigenvalues but not eigenvectors [5]. Most learn Laplacian matrix from data rather than heuristics [92, 138], but they still explicitly approximate the operator by assembling mass and stiffness matrices and then apply eigensolvers sensitive to these approximations. As they rely on triangle-based discretizations, they target surfaces and do not extend to higher-dimensionality. For implicit neural representations, the Laplacian can be built via Rayleigh quotients directly [137]. However, this assumes two-dimensional surface and the Euclidean ambient metric, removing adaptivity to other metrics or dimensionalities. On the theoretical front, Laplacian estimation has progressed [29, 85, 95, 125], yet, translating these insights into practice remains largely unexplored. Additionally, other discrete operators provide different insights. Changing the metric, via scaling (scale-invariant LBO [2, 20, 53, 114]), anisotropy [6, 17, 18, 103, 106], or asymmetry [9, 37, 38, 88, 133], changes the LBO and its approximation. Beyond LBOs, both intrinsic [14, 31, 102] and extrinsic [131] alternatives are understudied. Eigenproblems and neural networks. Recent efforts apply neural networks to operator eigenvalue problems, typically using variational or dynamical formulations. In [108], networks are trained to represent individual Laplacian eigenfunctions as continuous functions on parametrized domains, based on the Rayleigh quotient objective and finding eigenfunctions sequentially via Gram-Schmidt orthogonalization, an idea that can be extended to learning multiple eigenfunctions simultaneously [13]. However, this approach is limited to simplistic impractical domains, e.g. Euclidean. Another approach reformulates the eigenvalue problem as fixed point of the operators semi-group flow, training networks via forward-backward stochastic differential equations to handle high-dimensional problems, up to ten dimensions [55]. These methods fundamentally (1) they require exdiffer from the proposed approach: plicit domain parameterization with global coordinates, (2) they assume flat geometry by taking Euclidean gradients via automatic differentiation, and (3) they must be trained from scratch for each specific domain, with no mechanism for generalization across different geometries. Thus, they cannot directly handle curved manifolds sampled as point clouds without manually specifying metric tensors and managing coordinate singularities. 3. Method 3.1. Foundations Denote Rd as high-dimensional manifold, typically curved low-dimensional surface embedded in Rd, Figure 1. Overview of our neural framework to compute spectral bases directly from unstructured point clouds of any dimensionality, based on optimal-approximation theory, without first explicitly choosing, discretely approximating, and eigendecomposing an operator. with functions F(M, R) and linear operators : F(M, R) F(M, R) acting on them. When the manifold is discretely sampled by points, scalar functions can be represented by vectors Rn and operators as matrices Rnn. Here, we denote by , the Lweighted inner product f, gL = Lg for any symmetric positive definite (SPD) matrix L, with its induced norm L = (cid:112)f, = (cid:112)f Lf . Optimal-approximation theory. Given class of signals on the discrete domain M, fundamental question arises: What is the optimal orthonormal basis for representing functions C? The answer depends on how we characterize the class of signals. key insight [4, 24] is that many practical signal classes can be characterized by constraints of the form L 1, where is an SPD operator encoding prior knowledge about the signals. Remarkably, for any such constraint class, the optimal basis is given by the eigenvectors of itself. Theorem 3.1 (Min-Max Optimality [4, Theorem 2.1]). Given symmetric positive definite operator with eigenvalues 0 < λ1 λn and eigenvectors e1, . . . , en, the min-max approximation error αk = min b=(b1,...,bn) max L1 (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:88) i=1 f, biLbi 2 (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) , where ranges over orthonormal bases, is minimized by the first eigenvectors of L, i.e. bi = ei k, with optimal value λk+1 = 1 . For simple spectrum, λi < λi+1 i, the αk basis that is solution for every is unique up to signs. The key insight is that for any SPD operator Rnn, the optimal orthonormal basis for the progressive k-term approximation, i.e. for every k, is uniquely given by the eigenvectors of ordered by increasing eigenvalues. Crucially, the worst approximation error using the first eigenvectors , meaning that the (k + 1)-th eigenvalue is inversely is proportional to the worst maximum reconstruction error. 1 λk+ The min-max formulation (Theorem 3.1) optimizes over the set CL = {f ; L 1}. Instead of minimizing the worst-case reconstruction error, another natural approach to find optimal representations would be to maximize the captured variance. This alternative problem leads to Principal Component Analysis (PCA) on the same class of signals CL. Remarkably, we obtain the following result when performing PCA on uniformly distributed signals from CL. Theorem 3.2 (Operator-Bounded PCA [4, Section 5]). Given symmetric positive definite operator with eigenvalues 0 < λ1 λn, and eigenvectors e1, . . . , en, the PCA objective min b=(b1, ,bn) U(cid:0)f L1(cid:1) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:88) f, biLbi i= , 2 (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) over orthonormal bases is minimized by the eigenvectors of the covariance matrix RL = U(cid:0){f L1}(cid:1)[f ], which are the first eigenvectors of L, with eigenvalues (variances) λ1 . In other words: RL = L1. 1 λ For reminder why the expectation of the approximation error is PCA, i.e. iterative maximisation of the Rayleigh quotient of RL, see Sec. A.1. Therefore, PCA on CL yields the same eigenvectors and order as the min-max solution. The min-max optimization (Theorem 3.1) and its PCA counterpart (Theorem 3.2) are thus equivalent when applied to the same signal class. These dual formulations have useful practical implications that we exploit in our method. natural prior for signals on manifold is smoothness, implying that the Dirichlet energy 2 is bounded. By Greens identity, 2 = , where is the (discrete) Laplace-Beltrami operator (LBO). Thus, bounding the Dirichlet energy leads to choosing for L. To motivate our method, let us thus focus on the Laplacian operator and its construction. This will enable the derivation of our framework, generalizable both beyond Laplacian operators and to arbitrarily high-dimensional manifolds, from surfaces in R3 to image datasets. Discrete Laplacians. The discrete Laplacian is traditionally constructed from two fundamental matrices, the mass 3 2 SM 1 2 qi) = matrix Rnn, which is positive diagonal matrix encoding the local (metric-dependent) sampling density defining the manifolds Riemannian metric, and the stiffness matrix Rnn encoding geometric relationships that discretize the continuous Laplace-Beltrami operator. For two-dimensional surfaces, represents vertex areas and contains cotangent weights [6, 19, 118]. For higher-dimensionality, the LBO is harder to approximate, thus and are constructed with schemes more sensiIn any tive to the connectivity of the relationship graph. case, the matrices and enable two common formulations for the discretized Laplacian: the unnormalized Laplacian = 1S and the symmetric normalized Laplacian Lnorm = 1 2 . While both operators share identical eigenvalues, their eigenvectors differ. The unnormalized Laplacians eigenvectors vi Rn are -orthogonal, satisfying vi, vjM = δij, where δij = 1 if = and 0 otherwise, whereas the normalized Laplacians eigenvectors qi Rn are Euclidean-orthogonal, with qi, qj = δij. Both sets of eigenvectors are related by vi = 1 2 qi, as L(M 1 2 SM 1 On closed connected manifolds1, these operators nullspaces reveal important differences. While the vector of ones 1 lies in the nullspace of L, the vector 1 2 1 lies in the nullspace of Lnorm. This means the first normalized eigenvector of Lnorm corresponding to eigenvalue zero is proportional to 1 2 1, effectively encoding the square root of the sampling density weights (metric-sensitive). Should we want to learn directly the eigendecomposition of Laplacian, the normalized formulation of Lnorm offers crucial advantage. Indeed, since the first eigenvector directly encodes the sampling weights of the discrete metric of the manifold, trained neural network predicting the eigenbasis of Lnorm simultaneously learns both the spectral decomposition and the manifold metric in unified manner. This eliminates the need for separate modules to predict the mass matrix, simplifying the architecture while maintaining geometric consistency. Inspired by this remarkable property for Laplacians, which generalizes beyond them (see Sec. A.2), we designed our framework to similarly learn basis, where we decided that the first eigenvector encodes the underlying metric and from which we can explicitly compute the metric-dependent area weights. 2 qi = λiM 1 2 1 2 qi. Exploiting these insights, we train neural newtork that predicts basis for geometry processing directly from unstructured data like point clouds in arbitrary dimension Rd. This basis (and associated extracted scalars) yields the eigendecomposition of an implicit operator, which can resemble common operators like the Laplacian, bypassing the need to choose an operator, discretely approximate it via mass and stiffness matrices, and then call eigensolvers. 3.2. Neural Framework for Direct Spectral Bases Our goal is to learn directly basis for geometry processing on discretely sampled data of arbitrary dimensions, bypassing the need to choose specific operator, how to discretise it, and calls to sensitive eigensolvers on its disrete approximation. Should we choose an operator, like the Laplacian, sub-goal would be to compute its eigendecomposition directly without first discretising it (by combining mass and stiffness matrices) and post hoc eigendecomposition. Given point cloud discretely sampling manifold with points pi Rd for 1 n, we design neural network Φθ : Rnd RnK to predict K-dimensional feature vector for each point, where is the number of basis vectors we wish to predict. These per-point predictions form matrix Φθ (P) RnK, on which we do QR decomposition, Φθ (P) = QR, where RnK has orthonormal columns and RKK is upper triangular. By analogy with Laplacians, we can interpret = [q1, q2, . . . , qK], with i-th column qi Rn, as the first eigenvectors of normalized operator, like Lnorm, corresponding to eigenvalues λ = [λ1, λ2, . . . , λK], and ordered such that 0 = λ1 λ2 . . . λK. For any K, we denote by Qk = [q1, . . . , qk] Rnk the matrix containing the first columns of Q. This approach radically differs from and short-circuits traditional methods, which require first to choose an operator (e.g. ), then its approximate discretization (e.g. Lnorm), and then classically eigendecompose it. In contrast, our pipeline completely bypasses this altogether by computing spectral basis directly without first choosing, computing, and eigendecomposing an operator. Remarkably, we need not learn the metric separately. Indeed, with our interpretation, the first eigenvector q1 directly encodes the mass matrix diagonal, by taking = diag(q1 q1). We also need not learn separately the eigenvalues by exploiting the min-max theorem (Theorem 3.1) linking the eigenvalues to the maximum approximation error. During the forward pass, we generate random probe functions for the input point cloud and project them onto our predicted truncated bases Qk for all k. The maximum reconstruction error across these projections provides an estimate αk, from which we compute λk+1 1 . This gives us eigenvalue αk estimates without requiring additional network parameters. 3.3. Learning by Optimal Approximations 1This setting is standard. It is both common in practice for meshes, and is systematic for unstructured data like pointclouds, which is what we focus on, where potential manifold boundaries are both ill-defined and not provided. The optimal approximation theory generalizes to this case by focusing on the orthogonal of the nullspace. Our framework  (Fig. 1)  operates on batch of point clouds. Both at train or inference time, we progressively reconstruct probe functions by -orthogonal projections onto the Euclidean-orthogonal estimated bases Qk (see how in 4 Algorithm 1 Progressive Reconstruction Forward Pass Input: Point cloud with points, number of probe functions m, number of eigenvectors Output: Reconstruction loss Lrec and maximum-error emax RK 1: Generate independently and uniformly probe functions (1), (2), . . . , (m) Rn. By default, do this by iteratively applying Gaussian kernels on the k-nearset neighbor graph to independently and uniformly sampled signals of Rn. 2: Compute forward pass on the network Φθ(P) 3: Compute the QR-decomposition Φθ(P) = QR 4: for = 1 to do 5: 6: 7: Qk truncated to the first columns for = 1 to do (i) proj,k -projection of (i) onto Qk (i) (i) e(i) proj,k2 2 end for argmaxi(e(i) imax ) emax.append(e(imax ) ) 8: 9: 10: 11: 12: end for 13: Lrec 1 mK (cid:80)m i=1 (cid:80)K k=1 e(i) Sec. B.1) for increasing values of and for each point cloud in the batch, as summarized in Algorithm 1. Training. We train our model by learning basis to optimally reconstruct probe functions. Depending on the choice of probe function distribution, we will be working implicitly with different operators in the optimal reconstruction theory (Theorems 3.1 and 3.2), leading to different estimated bases each having their own advantages. By default, we take inspiration from the Laplacian, which is the most commonly used operator, yet without computing it. Probe functions for the Laplacian should be smooth, with bounded Dirichlet energy, and uniformly distributed in this bounded set. However, computing the gradient requires the knowledge of the metric, which on unstructured data like high dimensional point clouds is not provided. We relax these constraints by generating probe functions from smoothing random functions with Gaussian kernels on the k-nearest neighbor graph of the data. The resulting distribution loosely resembles that of the constrained Laplacian, leading to similarly behaved bases, yet we neither aim for exact replica of the Laplacian nor or are we constrained to this operator or this choice of distribution. Our training loss is based on optimal-reconstruction theory. Instead of the min-max formulation (Theorem 3.1), where for each only one probe function in the batch (the worst approximated one) would be used to optimize the model, we switch to the equivalent PCA one (Theorem 3.2) averaging out the contribution of all probe functions. This leads to stabler optimization. Our proposed loss is thus the average reconstruction loss Lrec measuring the average quality of these progressive approximations Lrec = 1 mK (cid:88) (cid:88) i=1 k=1 (i) (i) proj,k2 2, (1) where (i), (i) proj,k Rn are the i-th probe function and its projection onto the first estimated basis vectors. We train our model by backpropagating through the entire pipeline using Lrec as the sole training objective, enabling the network to learn the optimal basis through end-to-end gradient descent-based optimization. Note that we switched from the -norm to the unweighted Euclidean norm. This hybrid approach, combining -weighted projection with 2norm error, creates an unsupervised mechanism for learning density-related mass matrix and improves the training stability (see Sec. B.2). Importantly, we never compute the implicit optimal reconstruction operator nor its eigenvalues during training. Nevertheless, they are easy to compute at inference time. Inference. feed-forward pass computes our optimal reconstruction basis for each point cloud in the inference batch. We can then easily compute the associated implicit optimal reconstruction operator or its eigenvalues for downstream geometric tasks. Thanks to the min-max formulation of optimal reconstruction theory (Theorem 3.1) the eigenvalues can be estimated from the worst-case reconstruction over all the probe functions at each spectral resolution {1, . . . , K} using λk+1 = . This pro1 maxi (i)f (i) proj,k2 vides theoretically-grounded eigenvalue estimates directly from the estimated basis rather than in parallel to it. Given the basis and its associated eigenvalues, we can then optionally explicitly reconstruct the implicit normalized symmetric operator by matrix multiplication QKΛKQT K, or it unKM 1 normalized non-symmetric version 1 2 . However, the operator matrix has little use in practice, as even its action is usually computed in its spectral basis. Thus, in our experiments, we never needed to recompute it explicitly. From the estimated normalized basis qi, we can also compute an unnormalized spectral basis vi = 1 2 qi, which can be preferable downstream as it is more sampling invariant. For instance, the first vector v1 is constant regardless of the sampling of the the underlying smooth manifold. 2 QKΛKQT 4. Experiments We demonstrate the versatility of our framework to compute spectral bases on arbitrary data. Starting from sanity check in toy 1D setting, with points sampled in [0, 1] R, we show that we can handle not only traditional 3D point 5 clouds sampled on two-dimensional surfaces in R3, but also high-dimensional data in Rd such as image embeddings. Full details on the data, implementation, and further results are pushed to Sec. C. 4.1. Toy 1D Segment Manifold As sanity check, we illustrate our method on the simplest geometry: the unit interval Ω = [0, 1]. Figure 2. Learned eigenfunctions on [0,1] recover frequencyordered harmonics resembling the Laplacians spectrum. Setting. We grid sample 100 points on [0, 1], forming point cloud. As in higher dimensions, random probe functions are first smoothened, without boundary constraints. Our learned extractor Φθ is small MLP, which suffices in this single small point cloud setting. Results. We plot in Fig. 2 the first five learned unnormalized spectral basis vectors vi = 1 2 qi after sign alignment. The network recovers Fourier basis-like family with frequency-increasing harmonics: v1 is constant, while the other vi exhibit half-waves across the interval. This minimal example showcases how we can compute frequencyordered orthonormal basis resembling the Fourier basis, i.e. the Laplacians eigenfunctions, just by optimising the approximation objective on basic probe functions. Importantly, this happens because the metric weights , extracted from the first normalized basis vector q1, are sensible as they are correlated with intuition. 4.2. 2D Surface and 3D Volume Manifolds We here show that our method can learn an approximation of the eigendecomposition of the most prevalent operator in shape analysis: the Laplace-Beltrami operator. By learning on single 3D point clouds (overfitting setting), we obtain highly accurate estimates. By learning on wide collection of 3D point clouds from various datasets (generalization setting), we may obtain foundation model that generalizes to unseen point clouds in R3 without retraining. Datasets. The shapes in the overfitting setting are from [87]. In the generalization setting, we train our model on wide collection of surface datasets to ensure broad generalization, ranging from protein structures to human scans [16, 65, 69, 74, 98]. We evaluate on reference shape analysis benchmarks [40, 47, 68, 75, 83, 87, 96, 119, 124, 143], Figure 3. Unnormalized spectral basis (top) and xyz reconstruction from basis vectors (bottom), using either the oracle cotangent Laplacian or our method (overfitting setting). Scalars are cosine similarities between basis vectors. We get similar if not more detailed reconstructions. More in Sec. C. Figure 4. Eigenvalues of the oracle cotangent Laplacian and our estimated ones (overfitting setting). More in Sec. C. Figure 5. Estimated mass metric from q1 (overfitting setting). spanning many challenges, e.g. deformations, topology variations, and different geometric characteristics. In each 6 Table 1. Average cosine similarity between predicted and oracle eigenfunctions at different truncation levels k, and mean relative eigenvalue discrepancy (overfitting setting). More in Sec. C. Shape Image 10 50 λ Discrepancy Armadillo Bimba Botijo Elephant Fertility Kitten Laurent Hand Lion Pegaso 0.968 0. 0.967 0.945 0.773 0.200 0.126 0.822 0.093 0. 0.972 0.955 0.813 0.153 0.092 0.979 0. 0.993 0.823 0.951 0.932 0.866 0. 0.988 0.696 0.908 0.797 0.687 0.105 0. 0.720 0.981 0.083 0.106 0.088 0.104 0.568 0.066 0. 0.822 0.067 0.086 0.544 0.142 0.140 dataset, we dropped the mesh connectivity to work only with point clouds. Also, all shapes are scaled to fit within unit sphere. We evaluate generalization to new manifold dimensionality by testing on 3D volumetric point clouds, computed by randomly sampling points inside the volume of shapes in [40], the model learned on surface point clouds. Methods. We compare our neural framework (without connectivity) using transformer [127] as learned extractor Φθ against the reference axiomatic oracle: the classical cotangent Laplacian (with oracle mesh connectivity). Results Overfitting setting. We plot the learned unnormalized eigenvectors vi in Fig. 3. Remarkably, our eigenvectors, unsupervisedly learned solely using optimalapproximation theory and without knowledge of the underlying mesh, are almost identical to those computed for the cotangent Laplacian, which relies on the oracle mesh structure, with cosine similarity between them often close to 1 (see Tab. 1). Additionally, the eigenvalues extracted from the worst case errors provide good approximation to those of the oracle (see Fig. 4). These results show that our neural unsupervised method can be used in an overfitting setting to get highly accurate spectral basis estimates that match those of the targeted Laplace-Beltrami oracle. On some shapes though, e.g. Pegaso, higher frequency basis vectors slightly diverge from the oracle. Yet by analyzing shape reconstruction (spectral filtering) results (see Fig. 3), projecting the xyz coordinates to the first spectral vectors, we see that our model captures additional details lost in the oracle, providing better top approximation. Thus not only can we imitate the reference oracle method, we can in some cases provide superior version with better compressed information in the spectral basis. cornerstone of our method is the unsupervised extraction of the metric weights directly from the first estimated normalized vector q1, without Figure 6. Unnormalized spectral basis v1 on unseen shapes, either surfaces (left) or volumes (right), when the model was trained on wide collection of surface point clouds (generalization setting). Our model exhibits foundation-level generalization capabilities. knowledge of the mesh structure. We see in Fig. 5 that these estimated area weights are indeed well-behaved. Results Generalization setting. Here, our model is trained on wide collection of surface point clouds. We plot learned unnormalized eigenvectors vi and filter the xyz coordinates on unseen evaluation shapes, either surfaces or volumes  (Fig. 6)  , which is type of manifold never seen in training. Our model generalizes well to new eclectic types of shapes, yet with smaller precision than in the overfitting setting, demonstrating that our framework could provide unsupervised foundation models to compute spectral decompositions generalizing well beyond the training data. 4.3. High-Dimensional Manifolds To demonstrate the generality of our approach, we experiment, beyond the classical 2D surface setting, on manifolds with high intrinsic dimensionality. Here, each image is single point on the dataset manifold, with distances between images measured in pretrained feature space. Datasets. We use standard image classification datasets: STL10 [34], Imagenette [58], CIFAR100 [66], and Caltech256 [52] having 10, 10, 100, and 256 classes. Here, we do not mix datasets, but follow the train-test splits. As is standard, rather than working on raw pixel images, we use reference feature embeddings, DINOv2 [89] and CLIP [101], as high-dimensional (yet lower dimensional 7 {2, 5, 10, 50} our method against reference manifold learning techniques: PCA [57, 94], Isomap [123], Laplacian Eigenmaps [11], t-SNE [80, 126], and UMAP [82]. Results. Based on our results (see Figs. 7 and 8), our learned spectral basis consistently provides competitive or superior embeddings compared to baselines. These results showcase the strength of our neural spectral decomposition for capturing the intrinsic geometry of manifolds. In particular, our superiority compared to the similar Laplacian Eigenmaps suggests that our spectral basis and associated implicit operator are superior to those of the graph Laplacian, which is the standard for high-dimensional data. 5. Conclusion We propose neural framework to compute directly spectral bases on unstructured data of any dimensionality, bypassing the traditional need to first choose, discretize, and then eigendecompose an operator. Grounded in optimalapproximation theory, we find the orthonormal basis that optimally approximates constrained probe functions, with different probe distributions encoding different implicit operators. From the estimated basis, we can recover directly the manifold metric and compute explicitly the associated eigenvalues of the associated implicit operator. In wide range of experiments covering one to three-dimensional manifolds, we showed that our method can recover the Laplace-Beltrami operator (LBO), which is the most useful operator in 3D geometry. However, our method scales in practice to high-dimensionality, unlike the LBO, and we show that it provides advantageous spectral representations via manifold learning experiments on image datasets. Our data-driven alternative to conventional pipelines paves the way for new avenues in geometry processing for unstructured data, especially in high-dimensions. Limitations and future work. Training our method is computationally expensive, requiring time and GPUs, although this can be improved by optimizing our code with efficient compilation and CUDA implementations. Due to constraints, we focused on few tasks, yet we intend to explore further downstream applications, especially in highdimensions, such as using our spectral basis in graph neural networks, or learning more general foundation model that would apply to data of any dimensionality. At the heart of our method, the choice of probe functions imposes an underlying metric and implicit operator. By default we smoothen random functions on the kNN graph, giving similar results to the Laplacian. Yet, to best approximate it we need to manually tune distribution hyperparameters per shape, as we do in the overfitting setting. In future work, we will learn probe distributions and explore the approximation of more operators beyond the traditional Laplacians. Figure 7. Average clustering performance over 50 runs of manifold learning methods on DINOv2 features of random data subsets (1500 images). Higher is better. More in Sec. C. Figure 8. Manifold learning visualization of random subset of STL10 by 2D embedding DINOv2 features. More in Sec. C. than the raw pixel image) image coordinates in Rd, with = 768 (resp. 512) for DINOv2 (resp. CLIP). Dataset manifolds are thus mapped to submanifolds of Rd, and image distances are measured on embeddings. Methods. Unlike 3D point clouds, high-dimensional data cannot be visualized and analyzed directly. Manifold learning addresses this by finding low-dimensional embeddings in Rk, with d, that preserve pairwise dissimilarities to capture manifold structures. In particular, it enables 2D visualizations (k = 2) of high-dimensional data. For short overview of manifold learning see Fig. 7. For classification data, it is widely accepted that better manifold learning methods provide better clustered embeddings correlating with the class labels. This can be both evaluated visually or with metrics such as the Normalized Mutual Information (NMI) [129] and Adjusted Rand Index (ARI) [59]. Our unnormalized spectral basis vi naturally provides k-lowdimensional embeddings, which is our proposed manifold learning technique that we name Optimal-Approximation Eigenmaps. It is direct generalization of Laplacian Eigenmaps [11], which uses the graph Laplacian spectral basis instead. We compare for various embedding dimensions"
        },
        {
            "title": "References",
            "content": "[1] Yonathan Aflalo and Ron Kimmel. Spectral multidimensional scaling. Proceedings of the National Academy of Sciences, 110(45):1805218057, 2013. 5 [2] Yonathan Aflalo, Ron Kimmel, and Dan Raviv. Scale invariant geometry for nonrigid shapes. SIAM Journal on Imaging Sciences, 6(3):15791597, 2013. 2 [3] Yonathan Aflalo, Haim Brezis, and Ron Kimmel. On the optimality of shape and data representation in the spectral domain. SIAM Journal on Imaging Sciences, 8(2):1141 1160, 2015. 1 [4] Yonathan Aflalo, Haım Brezis, Alfred Bruckstein, Ron Kimmel, and Nir Sochen. Best bases for signal spaces. Comptes Rendus. Mathematique, 354(12):1155 1167, 2016. 1, 3 [5] Yulin An and Enrique del Castillo. An ai approach for learning the spectrum of the laplace-beltrami operator. arXiv preprint arXiv:2507.07073, 2025. 2 [6] Mathieu Andreux, Emanuele Rodola, Mathieu Aubry, and Daniel Cremers. Anisotropic laplace-beltrami operators for shape analysis. In European conference on computer vision, pages 299312. Springer, 2014. 2, [7] Mathieu Aubry, Ulrich Schlickewei, and Daniel Cremers. The wave kernel signature: quantum mechanical approach to shape analysis. 2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops), 1:16261633, 2011. 1 [8] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey Hinton. arXiv preprint arXiv:1607.06450, Layer normalization. 2016. 3 [9] Thomas Barthelme. natural finsler-laplace operator. Israel Journal of Mathematics, 196(1):375412, 2013. 2 [10] Mikhail Belkin and Partha Niyogi. Laplacian eigenmaps and spectral techniques for embedding and clustering. Advances in neural information processing systems, 14, 2001. 5 [11] Mikhail Belkin and Partha Niyogi. Laplacian eigenmaps for dimensionality reduction and data representation. Neural Computation, 15(6):13731396, 2003. 8, 5 [12] Eugenio Beltrami. Saggio di interpretazione della geometria non-euclidea. Stab. Tip. De Angelis, 1868. 2 [13] Ido Ben-Shaul, Leah Bar, Dalia Fishelov, and Nir Sochen. Deep learning solution of the eigenvalue problem for differential operators. Neural Computation, 35(6):11001134, 2023. 2 [14] David Bensaıd, Amit Bracha, and Ron Kimmel. Partial shape similarity by multi-metric hamiltonian spectra In Scale Space and Variational Methods in matching. Computer Vision: 9th International Conference, SSVM 2023, Santa Margherita Di Pula, Italy, May 2125, 2023, Proceedings, page 717729, Berlin, Heidelberg, 2023. Springer-Verlag. 2 [15] David Bensaıd, Noam Rotstein, Nelson Goldenstein, and Ron Kimmel. Partial matching of nonrigid shapes by learning piecewise smooth functions. In Computer Graphics Forum, page e14913. Wiley Online Library, 2023. 2 [16] Federica Bogo, Javier Romero, Matthew Loper, and Michael Black. Faust: Dataset and evaluation for 3d mesh registration. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3794 3801, 2014. 6, 2 [17] Davide Boscaini, Jonathan Masci, Emanuele Rodol`a, and Michael Bronstein. Learning shape correspondence with anisotropic convolutional neural networks. Advances in neural information processing systems, 29, 2016. 2 [18] Davide Boscaini, Jonathan Masci, Emanuele Rodol`a, Michael Bronstein, and Daniel Cremers. Anisotropic diffusion descriptors. In Computer Graphics Forum, pages 431441. Wiley Online Library, 2016. 2 [19] Mario Botsch, Leif Kobbelt, Mark Pauly, Pierre Alliez, and Bruno Levy. Polygon mesh processing. pages 97122, 2010. [20] Amit Bracha, Oshri Halimi, and Ron Kimmel. Shape Correspondence by Aligning Scale-invariant LBO Eigenfunctions. In Eurographics Workshop on 3D Object Retrieval. The Eurographics Association, 2020. 2 [21] Amit Bracha, Thomas Dag`es, and Ron Kimmel. On unIn Proceedings supervised partial shape correspondence. of the Asian Conference on Computer Vision, pages 4488 4504, 2024. [22] Amit Bracha, Thomas Dag`es, and Ron Kimmel. Wormhole loss for partial shape matching. In Advances in Neural Information Processing Systems, pages 131247131277. Curran Associates, Inc., 2024. 2, 5 [23] Matthew Brand. Nonrigid embeddings for dimensionality reduction. In European Conference on Machine Learning, pages 4759. Springer, 2005. 5 [24] Haım Brezis and David Gomez-Castro. Rigidity of optimal bases for signal spaces. Comptes Rendus. Mathematique, 355(7):780785, 2017. 1, 3 [25] Alexander Bronstein, Michael Bronstein, and Ron Kimmel. Generalized multidimensional scaling: framework for isometry-invariant partial surface matching. Proceedings of the National Academy of Sciences, 103(5): 11681172, 2006. [26] Alexander Bronstein, Michael Bronstein, and Ron Numerical geometry of non-rigid shapes. Kimmel. Springer Science & Business Media, 2008. 2 [27] Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. Spectral networks and locally connected networks International Conference on Learning Repreon graphs. sentations (ICLR), 2013. 2 [28] Augustin-Louis Cauchy. Sur lequation `a laide de laquelle on determine les inegalites seculaires des mouvements des plan`etes. Oeuvres Compl`etes (IIeme Serie), 9:174195, 1829. 2 [29] Frederic Chazal, Ilaria Giulini, and Bertrand Michel. Data driven estimation of laplace-beltrami operator. Advances in Neural Information Processing Systems, 29, 2016. 2 [30] Gengxiang Chen, Xu Liu, Qinglu Meng, Lu Chen, Changqing Liu, and Yingguang Li. Learning neural operators on riemannian manifolds. National Science Open, 3 (6):20240001, 2024. 2 [31] Yoni Choukroun, Gautam Pai, and Ron Kimmel. Sparse approximation of 3d meshes using the spectral geometry of the hamiltonian operator. J. Math. Imaging Vis., 60(6): 941952, 2018. [32] Ming Chuang, Linjie Luo, Benedict Brown, Szymon Estimating the Rusinkiewicz, and Michael Kazhdan. laplace-beltrami operator by restricting 3d functions. In Computer graphics forum, pages 14751484. Wiley Online Library, 2009. 2 [33] Fan RK Chung. Spectral graph theory. American Mathematical Soc., 1997. 2 [34] Adam Coates, Andrew Ng, and Honglak Lee. An analysis of single-layer networks in unsupervised feature learning. In Proceedings of the fourteenth international conference on artificial intelligence and statistics, pages 215223. JMLR Workshop and Conference Proceedings, 2011. 7 [35] Ronald R. Coifman and Stephane Lafon. Diffusion maps. Applied and Computational Harmonic Analysis, 21(1):5 30, 2006. Special Issue: Diffusion Maps and Wavelets. 1, 2, 5 [36] Ronald Coifman, Stephane Lafon, Ann Lee, Mauro Maggioni, Boaz Nadler, Frederick Warner, and Steven Zucker. Geometric diffusions as tool for harmonic analysis and structure definition of data: Diffusion maps. Proceedings of the national academy of sciences, 102(21): 74267431, 2005. 5 [37] Thomas Dag`es, Michael Lindenbaum, and Alfred Bruckstein. Metric convolutions: unifying theory to adaptive image convolutions. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 13974 13984, 2025. [38] Thomas Dag`es, Simon Weber, Ya-Wei Eileen Lin, Ronen Talmon, Daniel Cremers, Michael Lindenbaum, Alfred Bruckstein, and Ron Kimmel. Finsler multi-dimensional scaling: Manifold learning for asymmetric dimensionality reduction and embedding. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 25842 25853, 2025. 2, 5 [39] David Donoho and Carrie Grimes. Hessian eigenmaps: Locally linear embedding techniques for high-dimensional data. Proceedings of the National Academy of Sciences, 100 (10):55915596, 2003. 5 [40] Roberto Dyke, Yu-Kun Lai, Paul Rosin, Stefano Zappal`a, Seana Dykes, Daoliang Guo, Kun Li, Riccardo Marin, Simone Melzi, and Jingyu Yang. SHREC20: Shape correspondence with non-isometric deformations. Computers & Graphics, 92:2843, 2020. 6, 7, 2, 3 [41] Yuval Eldar, Michael Lindenbaum, Moshe Porat, and Yehoshua Zeevi. The farthest point strategy for progressive image sampling. IEEE transactions on image processing, 6(9):13051315, 1997. 3 [42] William Falcon. Pytorch lightning. GitHub, 2019. 3 [43] Bruce Fischl, Martin Sereno, and Anders Dale. Cortical surface-based analysis: Ii: inflation, flattening, and surface-based coordinate system. Neuroimage, 9(2):195 207, 1999. 5 American statistical association, 78(383):553569, 1983. [45] Clement Fuji Tsang, Maria Shugrina, Jean Francois Lafleche, Or Perel, Charles Loop, Towaki Takikawa, Vismay Modi, Alexander Zook, Jiehan Wang, Wenzheng Chen, Tianchang Shen, Jun Gao, Krishna Murthy Jatavallabhula, Edward Smith, Artem Rozantsev, Sanja Fidler, Gavriel State, Jason Gorski, Tommy Xiang, Jianing Li, Michael Li, and Rev Lebaredian. Kaolin: pytorch library for accelerating 3d deep learning research. 3 [46] Thorben Funke, Tian Guo, Alen Lancic, and Nino AntulovFantulin. Low-dimensional statistical manifold embedding In 8th International Conference on of directed graphs. Learning Representations (ICLR 2020), pages 20182035. Curran, 2020. 5 [47] Daniela Giorgi, Silvia Biasotti, and Laura Paraboschi. Watertight models track. Shape Retrieval Contest, 2007. 6, 2 [48] Teofilo Gonzalez. Clustering to minimize the maximum intercluster distance. Theoretical computer science, 38: 293306, 1985. 3 [49] Carolyn Gordon and David Webb. You cant hear the shape of drum. American Scientist, 84(1):4655, 1996. 1 [50] Carolyn Gordon, David Webb, and Scott Wolpert. One cannot hear the shape of drum. Bulletin of the American Mathematical Society, 27(1):134138, 1992. [51] Jianping Gou, Xia Yuan, Ya Xue, Lan Du, Jiali Yu, Shuyin Xia, and Yi Zhang. Discriminative and geometrypreserving adaptive graph embedding for dimensionality reduction. Neural Networks, 157:364376, 2023. 5 [52] Gregory Griffin, Alex Holub, Pietro Perona, et al. Caltech256 object category dataset. Technical report, Technical Report 7694, California Institute of Technology Pasadena, 2007. 7 [53] Oshri Halimi and Ron Kimmel. Self functional maps. In 2018 International Conference on 3D Vision (3DV), pages 710718. IEEE, 2018. 2 [54] David K. Hammond, Pierre Vandergheynst, and Remi Gribonval. Wavelets on graphs via spectral graph theory. Applied and Computational Harmonic Analysis, 30(2):129 150, 2011. 1 [55] Jiequn Han, Jianfeng Lu, and Mo Zhou. Solving highdimensional eigenvalue problems using deep neural networks: diffusion monte carlo like approach. Journal of Computational Physics, 423:109792, 2020. 2 [56] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (GELUs). arXiv preprint arXiv:1606.08415, 2016. 3 [57] Harold Hotelling. Analysis of complex of statistical variables into principal components. Journal of educational psychology, 24(6):417, 1933. 8, 5 [58] Jeremy Howard. Imagenette: smaller subset of imagenet, 2019. 7 [59] Lawrence Hubert and Phipps Arabie. Comparing partitions. Journal of classification, 2(1):193218, 1985. 8, 6 [44] Edward Fowlkes and Colin Mallows. method for Journal of the comparing two hierarchical clusterings. [60] Parvaneh Joharinad, Hannaneh Fahimi, Lukas Silvester Isumap: Manifold Barth, Janis Keck, and Jurgen Jost. 10 learning and data visualization leveraging vietoris-rips filtrations. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 1769917706, 2025. 5 [61] Mark Kac. Can one hear the shape of drum? The american mathematical monthly, 73(4P2):123, 1966. 1 [62] Jungeum Kim and Xiao Wang. Inductive global and local manifold approximation and projection. Transactions on Machine Learning Research, 2024. [63] Diederik Kingma. Adam: method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. 2 [64] Gustav Kirchhoff. Ueber die auflosung der gleichungen, auf welche man bei der untersuchung der linearen vertheilung galvanischer strome gefuhrt wird. Annalen der Physik, 148 (12):497508, 1847. 2 [65] Sebastian Koch, Albert Matveev, Zhongshi Jiang, Francis Williams, Alexey Artemov, Evgeny Burnaev, Marc Alexa, Denis Zorin, and Daniele Panozzo. Abc: big cad model dataset for geometric deep learning. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019. 6, 2 [66] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009. 7 [67] Joseph Kruskal. Multidimensional scaling by optimizing goodness of fit to nonmetric hypothesis. Psychometrika, 29(1):127, 1964. 5 [68] Zorah Lahner, Emanuele Rodol`a, Michael Bronstein, Daniel Cremers, Oliver Burghard, Luca Cosmo, Alexander Dieckmann, Reinhard Klein, Sahillioˇglu, et al. SHREC16: Matching of deformable shapes with topologIn Eurographics Workshop on 3D Object Reical noise. trieval, EG 3DOR, pages 5560. Eurographics Association, 2016. 6, [69] Florent Langenfeld, Tunde Aderinwale, Feryal Windal, Mahmoud Melkemi, Ekpo Otu, Reyer Zwiggelaar, David Hunter, Yonghuai Liu, Lea Sirugue, Huu-Nghia H. Nguyen, Tuan-Duy H. Nguyen, Vinh-Thuyen NguyenTruong, Charles Christoffer, Danh Le, Hai-Dang Nguyen, MinhTriet Tran, Matthieu Mont`es, Woong-Hee Shin, Genki Terashi, Xiao Wang, Daisuke Kihara, Halim Benhabiles, Karim Hammoudi, and Adnane Cabani. SHREC 2021: Surface-based Protein Domains Retrieval. In Eurographics Workshop on 3D Object Retrieval. The Eurographics Association, 2021. 6, 2 [70] Thibault Lescoat, Hsueh-Ti Derek Liu, Jean-Marc Thiery, Alec Jacobson, Tamy Boubekeur, and Maks Ovsjanikov. In Computer Graphics FoSpectral mesh simplification. rum, pages 315324. Wiley Online Library, 2020. 2 [71] Ron Levie, Wei Huang, Lorenzo Bucci, Michael Bronstein, and Gitta Kutyniok. Transferability of spectral graph convolutional neural networks. Journal of Machine Learning Research, 22(272):159, 2021. 2 [72] Bruno Levy. Laplace-beltrami eigenfunctions towards an In IEEE Interalgorithm that understands geometry. national Conference on Shape Modeling and Applications 2006 (SMI06), pages 1313. IEEE, 2006. 2 [73] Shen Li, Yanli Zhao, Rohan Varma, Omkar Salpekar, Pieter Noordhuis, Teng Li, Adam Paszke, Jeff Smith, Brian Vaughan, Pritam Damania, and Soumith Chintala. Pytorch distributed: experiences on accelerating data parallel training. Proc. VLDB Endow., 13(12):30053018, 2020. 5 [74] Yang Li, Hikari Takehara, Takafumi Taketomi, Bo Zheng, and Matthias Nießner. 4dcomplete: Non-rigid motion estimation beyond the observable surface. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1270612716, 2021. 6, 2 [75] Z. Lian, J. Zhang, S. Choi, H. ElNaghy, J. El-Sana, T. Furuya, A. Giachetti, R. A. Guler, L. Lai, C. Li, H. Li, F. A. Limberger, R. Martin, R. U. Nakanishi, A. P. Neto, L. G. Nonato, R. Ohbuchi, K. Pevzner, D. Pickup, P. Rosin, A. Sharf, L. Sun, X. Sun, S. Tari, G. Unal, and R. C. Wilson. Non-rigid 3D Shape Retrieval. In Eurographics Workshop on 3D Object Retrieval. The Eurographics Association, 2015. 6, [76] Ya-Wei Eileen Lin, Ronald Coifman, Gal Mishne, and Ronen Talmon. Hyperbolic diffusion embedding and disIn Intertance for hierarchical representation learning. national Conference on Machine Learning, pages 21003 21025. PMLR, 2023. 5 [77] Ya-Wei Eileen Lin, Ronald Coifman, Gal Mishne, and Ronen Talmon. Tree-wasserstein distance for high dimensional data with latent feature hierarchy. arXiv preprint arXiv:2410.21107, 2024. [78] Yongming Liu. Curvature augmented manifold embedding and learning. arXiv preprint arXiv:2403.14813, 2024. 5 [79] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2019. 3 [80] Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine learning research, 9 (Nov):25792605, 2008. 8, [81] Manuel Martın-Merino and Alberto Munoz. Visualizing asymmetric proximities with SOM and MDS models. Neurocomputing, 63:171192, 2005. 5 [82] Leland McInnes, John Healy, and James Melville. UMAP: Uniform manifold approximation and projection for dimension reduction. arXiv, 2018. 8, 5 [83] Simone Melzi, Riccardo Marin, Emanuele Rodol`a, Umberto Castellani, Jing Ren, Adrien Poulenard, Ovsjanikov, et al. SHREC19: matching humans with different connectivity. In Eurographics Workshop on 3D Object Retrieval, pages 18. The Eurographics Association, 2019. 6, 2 [84] Mark Meyer, Mathieu Desbrun, Peter Schroder, and Alan Barr. Visualization and mathematics III. Mathematics and Visualization, pages 3557, 2003. 2 [85] Hrushikesh Mhaskar and Ryan ODowd. Learning on manifolds without manifold learning. Neural Networks, 181:106759, 2025. 2 [86] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, In International and Hao Wu. Mixed precision training. Conference on Learning Representations, 2018. 3 [87] Ashish Myles, Nico Pietroni, and Denis Zorin. Robust field-aligned global parametrization. ACM Transactions on Graphics, 33(4):114, 2014. 6, 11 [88] Shin-ichi Ohta and Karl-Theodor Sturm. Heat flow on finsler manifolds. Communications on Pure and Applied Mathematics: Journal Issued by the Courant Institute of Mathematical Sciences, 62(10):13861433, 2009. 2 [89] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. 7 [90] Maks Ovsjanikov, Mirela Ben-Chen, Justin Solomon, Adrian Butscher, and Leonidas Guibas. Functional maps: flexible representation of maps between shapes. ACM Transactions on Graphics, 31(4):111, 2012. 1 [91] Gautam Pai, Alex Bronstein, Ronen Talmon, and Ron Kimmel. Deep isometric maps. Image and Vision Computing, 123:104461, 2022. 5 [92] Bo Pang, Zhongtian Zheng, Yilong Li, Guoping Wang, and Peng-Shuai Wang. Neural laplacian operator for 3d point clouds. ACM Transactions on Graphics, 43(6):114, 2024. 2 [93] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32, 2019. [94] Karl Pearson. On lines and planes of closest fit to systems of points in space. The London, Edinburgh, and Dublin philosophical magazine and journal of science, 2(11):559 572, 1901. 8, 5 [95] John Wilson Peoples and John Harlim. higher order local mesh method for approximating laplacians on unknown manifolds. arXiv preprint arXiv:2405.15735, 2024. 2 [96] D. Pickup, X. Sun, P. L. Rosin, R. R. Martin, Z. Cheng, Z. Lian, M. Aono, A. Ben Hamza, A. Bronstein, M. Bronstein, S. Bu, U. Castellani, S. Cheng, V. Garro, A. Giachetti, A. Godil, J. Han, H. Johan, L. Lai, B. Li, C. Li, H. Li, R. Litman, X. Liu, Z. Liu, Y. Lu, A. Tatsuma, and J. Ye. SHREC14 track: Shape retrieval of non-rigid 3d human models. In Proceedings of the 7th Eurographics workshop on 3D Object Retrieval. Eurographics Association, 2014. 6, 2 [97] Ulrich Pinkall and Konrad Polthier. Computing discrete minimal surfaces and their conjugates. Experimental Mathematics, 2(1):1536, 1993. 1, 2 [98] Adrien Poulenard, Marie-Julie Rakotosaona, Yann Ponty, and Maks Ovsjanikov. Effective rotation-invariant point cnn with spherical harmonics kernels. In 2019 International Conference on 3D Vision (3DV), pages 4756. IEEE, 2019. 6, 2 [99] Murray Protter. Can one hear the shape of drum? revisited. Siam Review, 29(2):185197, 1987. [100] Quackenbush and PJ Atzberger. Geometric neural operators (gnps) for data-driven deep learning in non-euclidean settings. Machine Learning: Science and Technology, 5(4): 045033, 2024. 2 [101] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language superIn International conference on machine learning, vision. pages 87488763. PmLR, 2021. 7 [102] Arianna Rampini, Irene Tallini, Maks Ovsjanikov, Alex Bronstein, and Emanuele Rodol`a. Correspondence-free region localization for partial shape similarity via hamiltonian spectrum alignment. In 2019 International Conference on 3D Vision (3DV), pages 3746. IEEE, 2019. 2 [103] Dan Raviv and Ron Kimmel. Affine invariant non-rigid shape analysis. Int. J. Comput. Vision, submitted, 2015. [104] Martin Reuter, Franz-Erich Wolter, and Niklas Peinecke. Laplacebeltrami spectra as shape-dna of surfaces and solids. Computer-Aided Design, 38(4):342366, 2006. Symposium on Solid and Physical Modeling 2005. 1 [105] Andrew Rosenberg and Julia Hirschberg. V-measure: conditional entropy-based external cluster evaluation measure. In Proceedings of the 2007 joint conference on empirical methods in natural language processing and computational natural language learning (EMNLP-CoNLL), pages 410420, 2007. 6 [106] Steven Rosenberg. The Laplacian on Riemannian manifold: an introduction to analysis on manifolds. Number 31. Cambridge University Press, 1997. 2 [107] Guy Rosman, Michael Bronstein, Alexander Bronstein, and Ron Kimmel. Nonlinear dimensionality reduction by topologically constrained isometric embedding. International Journal of Computer Vision, 89(1):5668, 2010. 5 [108] Conor Rowan, John Evans, Kurt Maute, and Alireza Doostan. Solving engineering eigenvalue problems with neural networks using the rayleigh quotient. arXiv, 2025. 2 [109] Sam Roweis and Lawrence Saul. Nonlinear dimensionality reduction by locally linear embedding. science, 290(5500):23232326, 2000. 5 [110] Raif M. Rustamov. Laplace-Beltrami Eigenfunctions for Deformation Invariant Shape Representation. In Geometry Processing. The Eurographics Association, 2007. 1 [111] Bernhard Scholkopf, Alexander Smola, and Klaus-Robert Muller. Nonlinear component analysis as kernel eigenvalue problem. Neural computation, 10(5):12991319, 1998. 5 [112] Ariel Schwartz and Ronen Talmon. Intrinsic isometric manifold learning with application to localization. SIAM Journal on Imaging Sciences, 12(3):13471391, 2019. [113] E.L. Schwartz, A. Shaw, and E. Wolfson. numerical solution to the generalized mapmakers problem: Flattening nonconvex polyhedral surfaces. IEEE Transactions on Pattern Analysis and Machine Intelligence, 11(9):10051008, 1989. [114] Matan Sela, Yonathan Aflalo, and Ron Kimmel. Computational caricaturization of surfaces. Computer Vision and Image Understanding, 141:117, 2015. 2 [115] Nicholas Sharp and Keenan Crane. laplacian for nonmanifold triangle meshes. Computer Graphics Forum, 39 (5):6980, 2020. 1, 2 12 [116] Nicholas Sharp, Souhaib Attaiki, Keenan Crane, and Maks Ovsjanikov. Diffusionnet: Discretization agnostic learning on surfaces. ACM Transactions on Graphics (TOG), 41(3): 116, 2022. 2 [117] Roger Shepard. The analysis of proximities: multidimensional scaling with an unknown distance function. i. Psychometrika, 27(2):125140, 1962. 5 [118] Sorkine, Cohen-Or, Lipman, Alexa, Rossl, and Seidel. Laplacian surface editing. Proceedings of the 2004 Eurographics/ACM SIGGRAPH symposium on Geometry processing, pages 175184, 2004. [119] Robert Sumner and Jovan Popovic. Deformation transfer for triangle meshes. ACM Transactions on Graphics, 23(3): 399405, 2004. 6, 2 [120] Jian Sun, Maks Ovsjanikov, and Leonidas Guibas. concise and provably informative multi-scale signature based In Computer graphics forum, pages on heat diffusion. 13831392. Wiley Online Library, 2009. 1, 2 [121] Ryota Suzuki, Ryusuke Takahama, and Shun Onoda. Hyperbolic disk embeddings for directed acyclic graphs. In International Conference on Machine Learning, pages 6066 6075. PMLR, 2019. 5 [122] Jian Tang, Jingzhou Liu, Ming Zhang, and Qiaozhu Mei. Visualizing large-scale and high-dimensional data. In Proceedings of the 25th international conference on world wide web, pages 287297, 2016. 5 [123] Joshua Tenenbaum, Vin de Silva, and John Langford. global geometric framework for nonlinear dimensionality reduction. Science, 290(5500):23192323, 2000. 8, 5 [124] Elia Moscoso Thompson, Silvia Biasotti, Andrea Giachetti, Claudio Tortorici, Naoufel Werghi, Ahmad Shaker Obeid, Stefano Berretti, Hoang-Phuc Nguyen-Dinh, Minh-Quan Le, Hai-Dang Nguyen, Minh-Triet Tran, Leonardo Gigli, Santiago Velasco-Forero, Beatriz Marcotegui, Ivan Sipiran, Benjamin Bustos, Ioannis Romanelis, Vlassis Fotis, Gerasimos Arvanitis, Konstantinos Moustakas, Ekpo Otu, Reyer Zwiggelaar, David Hunter, Yonghuai Liu, Yoko Arteaga, and Ramamoorthy Luxman. SHREC 2020: Retrieval of digital surfaces with similar geometric reliefs. Computers & Graphics, 91:199218, 2020. 6, 2 [125] Nicolas Garcıa Trillos, Chenghui Li, and Raghavendra Venkatraman. Minimax rates for the estimation of eigenpairs of weighted laplace-beltrami operators on manifolds. arXiv preprint arXiv:2506.00171, 2025. [126] Laurens Van Der Maaten. Accelerating t-sne using treebased algorithms. The journal of machine learning research, 15(1):32213245, 2014. 8 [127] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. arXiv, 2017. 7 [128] Jarkko Venna, Jaakko Peltonen, Kristian Nybo, Helena Aidos, and Samuel Kaski. Information retrieval perspective to nonlinear dimensionality reduction for data visualization. Journal of Machine Learning Research, 11(2), 2010. 5 [129] Nguyen Xuan Vinh, Julien Epps, and James Bailey. Inforis mation theoretic measures for clusterings comparison: correction for chance necessary? In Proceedings of the 26th annual international conference on machine learning, pages 10731080, 2009. 8, [130] Brian Wandell, Suelika Chial, and Benjamin Backus. Visualization and measurement of the cortical surface. Journal of cognitive neuroscience, 12(5):739752, 2000. 5 [131] Yu Wang, Mirela Ben-Chen, Iosif Polterovich, and Justin Solomon. Steklov spectral geometry for extrinsic shape analysis. ACM Transactions on Graphics (TOG), 38(1):1 21, 2018. 2 [132] Max Wardetzky, Saurabh Mathur, Felix Kalberer, and Eitan Grinspun. Discrete Laplace operators: no free lunch. In Symposium on Geometry processing, page 37. Aire-laVille, Switzerland, 2007. 1, 2 [133] Simon Weber, Thomas Dag`es, Maolin Gao, and Daniel Cremers. Finsler-Laplace-Beltrami operators with application to shape analysis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 31313140, 2024. 2 [134] Kilian Weinberger, Benjamin Packer, and Lawrence Saul. Nonlinear dimensionality reduction by semidefinite programming and kernel matrix factorization. In International Workshop on Artificial Intelligence and Statistics, pages 381388. PMLR, 2005. 5 [135] Kilian Weinberger and Lawrence Saul. Unsupervised learning of image manifolds by semidefinite programming. International journal of computer vision, 70:7790, 2006. 5 [136] Aaron Wetzler, Yonathan Aflalo, Anastasia Dubrovina, and Ron Kimmel. The Laplace-Beltrami operator: ubiquitous tool for image and shape processing. In International Symposium on Mathematical Morphology and Its Applications to Signal and Image Processing, pages 302316. Springer, 2013. [137] Romy Williamson and Niloy J. Mitra. Neural geometry processing via spherical neural surfaces. Computer Graphics Forum, 44(2):e70021, 2025. 2 [138] Oguzhan Yigit and Richard Wilson. Lbonet: Supervised spectral descriptors for shape analysis. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2025. 2 [139] Ao Zhang, Qing Fang, Peng Zhou, and Xiao-Ming Fu. Topology-controlled laplacebeltrami operator on point clouds based on persistent homology. Graphical Models, 139:101261, 2025. 2 [140] Zhenyue Zhang and Jing Wang. MLLE: Modified locally linear embedding using multiple weights. Advances in neural information processing systems, 19, 2006. 5 [141] Zhenyue Zhang and Hongyuan Zha. Principal manifolds and nonlinear dimensionality reduction via tangent space alignment. SIAM journal on scientific computing, 26(1): 313338, 2004. 5 [142] Hongyu Zhou and Zorah Lahner. Laplace-Beltrami operator for gaussian splatting. arXiv preprint arXiv:2502.17531, 2025. 2 [143] Qingnan Zhou and Alec Jacobson. dataset of 10,000 3d-printing models. arXiv:1605.04797, 2016. 6, 2 Thingi10k: arXiv preprint"
        },
        {
            "title": "Supplementary Material",
            "content": "A. Additional Theory A.1. PCA Alternative Formulations Without loss of generality, assume E(f ) = 0, as is the case for our signals uniformly distributed in CL = {f ; L 1}. Denote = E(f ) as the covariance matrix of distributed functions . In our case for Theorem 3.2, = RL as U(S). The classical iterative optimization formulation for PCA is the following. PCA finds the orthonormal basis that maximizes iteratively over the Rayleigh quotient bk s.t. max bk2= bk{b1,...,bk1} Cbk. (2) The solution to this Rayleigh optimization, and thus to PCA, is given by the eigenvectors of in decreasing order of eigenvalues. Another alternative formulation exists, where the PCA optimization objective is given as the iterative minimization of the expected squared approximation error, or variance, in basis min bk2= bk{b1,...,bk1} bk s.t. (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:88) f, bibi i= (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) 2 . (3) Indeed, let us remind the classical link between the two formulations. Denoting Bk Rdk with columns b1, . . . , bk, with Bk = Ik, the expected squared approximation error can be rewritten as (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:88) f, bibi i=1 (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) 2 = E((I BkB )f 2) )f )) )(I BkB )f )) )f )) = tr(E(f (I BkB = tr(E(f (I BkB = tr(E((I BkB = tr((I BkB )C) = tr(C) tr(BkB = tr(C) tr(B C) CBk) = tr(C) (cid:88) i= Cbi (4) (5) (6) (7) (8) (9) (10) (11) As is constant, minimizing the expected squared approximation error is thus the same as maximizing the cu1 mulative sum of Rayleigh quotients of C. Working iteratively over k, we get that the optimal solution for both formulations is given by the eigenvectors of in decreasing eigenvalue order. A.2. Normalized Operators and Eigenstructures We here generalize the relationship between the normalized symmetric Laplacian compared to the unnormalized one regarding its first eigenvector and how it provides the metric. For any operator A, we can define an analogous normalized operator Anorm = AN 1, where is matrix used for normalization. For instance = 1 2 for the normalized Laplacian Lnorm and = L. Both operators and Anorm share the same eigenvalues but different eigenvectors, related by the normalization matrix . Indeed, if λi, enorm are eigenvalues and eigenvectors of Anorm, then AN 1ei = 1N AN 1ei = 1Anormei = λiN 1ei, meaning that ei = 1enorm are the eigenvectors of the unnormalized operator A. As such, choosing = 1 2 to be given by the metric as for the normalized Laplacian, the eigenvectors of the normalized Laplacian are related to the unnormalized one by the metric enorm 2 ei. Note that Anorm is symmetric if = 2Σ for symmetric Σ and , which is the case for the Laplacians where Σ = and = 1 2 is diagonal, as then Anorm = 1ΣN 1, and so by the spectral theorem enorm is Euclidean-orthogonal whereas ei is -orthogonal. = 1 Of note, as for the Laplacians, if e1 = 1 is in the nullspace of the unnormalized operator A, then enorm 1 = 1 is in the nullspace of Anorm. Remarkably, if is diagonal matrix (like 1 2 ), if the nullspace is of dimension one, and if we learn to find the nullspace by learning the eigenbasis directly, then we can compute directly from it as diag(N ) = enorm . For most operators of interest, the assumptions on hold (for connected relationship graphs). This is because operators of interest are usually linear and differential, i.e. linear combinations with constant coefficients of the derivative operator = (cid:80) km αkk, usually without 0-th order term, and constant functions are zeroed out by derivatives. As such, our methodology learning Euclidean orthogonal basis directly is well-suited to capture eigenstructures of implicit operators with Euclidean orthogonal eigenvectors and single nullspace eigenvector dimension given by diagonally scaled version of the 1 vector. The normalized Laplacian falls into this category, yet it is just one out of the many other interesting operators satisfying these assumptions. B. Additional Details on Our Framework B.1. Projecting onto Optimal Reconstruction Bases To approximate probe signals, we need to compute projections onto Qk. However, Euclidean, that is uniform, orthogonal projection will not account for the non-uniform geometry of the sampled manifold due to non-uniform sampling density. To overcome this issue, we resolve to the -orthogonal projection onto the columns of Qk. As they are the output of QR-decomposition, these vectors are Euclidean orthogonal and not -orthogonal, yet, we want to -orthogonally project onto their span. To do this, the - orthogonally projected functions can be rewritten as fproj,k = (cid:88) i=1 c(k) qi = Qkc(k), (12) 1 , . . . , c(k) where c(k) = [c(k) ] Rk contains the projection coefficients. The coefficients c(k) are determined by requiring that the residual (f fproj,k) is -orthogonal to all basis vectors (f fproj,k)M qi = 0 for = 1, . . . , k. (13)"
        },
        {
            "title": "This orthogonality condition yields the system",
            "content": "q = (cid:88) j=1 c(k) (q qj). (14) In matrix form, this becomes approach, using -weighted projection with 2-norm error, creates an unsupervised mechanism for learning densityrelated mass matrix. Since the 2-norm weights all points equally regardless of sampling density, regions with different sampling densities contribute differently to the loss, implicitly encouraging the network to learn mass elements connected to the ambient spaces Euclidean-induced volume (or area on surface manifolds) of the samples. We also experimented with using the -norm for error computation as prescribed by theory. However, this resulted in unstable training, numerical issues (NaNs), and even collapses of the metric to 0 to naively minimise Lrec. This confirms the necessity of our hybrid approach. C. Additional Experimental Details and Results C.1. Toy 1D Segment Manifold In this toy setting we work on single point cloud. We uniformly grid sample = 100 points xi = n1 for = 0, . . . , 1. Probe signals fi are generated by drawing i.i.d. noise gi Rn and applying few steps of 1D Gaussian smoothing on the kNN graph with 16 neighbors, yielding smooth probes fi. Our lightweight MLP consists in 3 hidden layers with width 64 and ReLU activations. We only compute the first = 5 basis vectors. Wet train over batches of = 500000 probe functions using the Adam [63] optimizer with an initial learning rate of η = 102, step scheduler with parameter γ = 0.1 at 30% and 70%. G(k)c(k) = b(k), (15) C.2. 2D Surface and 3D Volume Manifolds ij = i . Since G(k) = where G(k) Rkk is the Gram matrix with entries G(k) qj, and b(k) Rk is vector with entries bi = M Qk is symmetric positive definite2, we can efficiently solve this linear system using differentiable solver to obtain the coefficients c(k)3. Crucially, G(k) Rk is small, where is the number of eigenvectors used for projection, making the system computationally efficient and independent of the input size n. Note that c(k) is not the truncation of c(K) to its first terms since Qk is not orthogonal for the used -weighted scalar product. These independent systems are batched and solved concurrently for all K. B.2. Replacing the M-norm with the Euclidean norm for Approximation Errors While the theoretical framework calls for the -norm in the error computation (in both Theorems 3.1 and 3.2), we deliberately use the 2-norm in our loss (Eq. (1)). This hybrid 2As the columns of Qk are linearly independent thanks to the QRdecomposition and is positive definite. 3Avoiding the explicit inversion in the formula = (G(k))1b. 2 C.2.1. Datasets Overfitting setting. We use shapes from [87] commonly used for benchmarking geometry processing works. We discard the mesh connectivity to work with point clouds. Generalization setting. In order to train foundation model for estimating spectral bases on 3D pointclouds, we unsupervisedly trained our model on many datasets covering large spectrum of types of shapes: SURREAL [98] for synthetic human bodies, SHREC21 Protein Domains [69] for biological structures, ABC [65] for CAD models, FAUST [16] for real human scans, and DeformingThings4D [74] for dynamic non-rigid surfaces. Evaluation is performed on well-established evaluation benchmarks covering many challenging settings: DefTransfer [119], MPZ14 [87], SHREC07 Watertight [47], SHREC14 Human Models [96], SHREC15 Non-Rigid [75], SHREC16 Topological Noise (TopKids) [68], SHREC19 Humans [83], SHREC20 NonIsometric (SHREC20-NI) [40], SHREC20 Geometric Reliefs (SHREC20-GR) [124], and Thingi10K [143]. All shapes in the datasets, both in train and evaluation, originally come with mesh connectivity. When using our method, we remove all this oracle connectivity information to keep only unstructured point clouds. We also evaluate, but not train, on volumetric point clouds generated from surface meshes in SHREC20 Non-Isometric [40]. To generate this data, we perform volumetric sampling using Kaolins 4 [45] GPU-accelerated approach. Random candidate points are uniformly sampled within the meshs axisaligned bounding box (with 5% padding), then for each point we compute the unsigned distance to the nearest mesh surface and determine inside/outside classification via ray casting5. Points classified as interior are retained and subsampled to the target count, optionally combined with surface vertices to form the final volumetric point cloud. C.2.2. Implementation Generalization Setting The following implementation details describe our generalization model that learns to predict spectral basis resembling Laplacian eigenfunctions across diverse 3D shapes, rather than overfitting to single geometry. The model is trained on large-scale dataset combining multiple datasets to generalize to unseen shapes at test time. Network Architecture. Our 3D model processes point clouds through three-stage pipeline. First, raw 3D coordinates are passed through preprocessing MLP with architecture [3 64 256 1024] using GELU activations [56] and layer normalization [8]. The resulting 1024-dimensional features are then processed by an 8layer Transformer encoder with 32 attention heads, where each point attends to all other points in the point cloud via global self-attention. The Transformer uses dmodel = 1024, feedforward dimension of 1024, GELU activations, and processes sequences in batch-first format. Finally, lightweight output MLP [1024 256 50] with GELU activations and layer normalization predicts the 50 feature vectors for each point (prior to QR decomposition to get the estimated normalized spectral basis). Training Configuration. We optimize with AdamW [79] (learning rate 5 104) and cosine annealing schedule decaying to 1105 over 100 epochs. We apply gradient clipping at 0.01 and accumulate gradients over 2 batches. The training set consists of approximately 200000 3D shapes (file sizes 0.2-7MB) from diverse datasets with batch size of 30. Data Processing. Each mesh is sampled to 1500 points using Farthest Point Sampling (FPS) [41, 48], with random 4https://github.com/NVIDIAGameWorks/kaolin 5Shooting ray and counting mesh intersections, with odd counts indicating interior points. initialization during training and fixed initialization during validation to ensure reproducibility. We generate 500 scalar fields per shape by sampling uniform random values in the range [-1, 1] at each vertex. Before computing the reconstruction loss, scalar fields are smoothed for 20 iterations using Gaussian kernel convolution with σ = 0.1 over k-nearest neighbor graph with = 15. The kNN graph construction uses Euclidean distance (not cosine similarity) and includes self-loops. Oracle cotangent Laplacian eigendecompositions are precomputed and cached to accelerate the validation phase, where we monitor cosine similarity between predicted and oracle eigenvectors. Computational Resources. All experiments were conducted on 8 GPUs (type NVIDIA A100-SXM4-40GB) with automatic mixed precision [86] training enabled through PyTorch Lightning [42, 93]. C.2.3. Implementation Overfitting setting The following implementation details describe the configuration for overfitting the eigendecomposition of specific 3D shape. This setup employs smaller architecture to achieve maximum accuracy for single target geometry. Network Architecture. The architecture for single-shape overfitting uses more compact design compared to the generalization model. Raw 3D coordinates are processed through preprocessing MLP with architecture [3 32 128 256] using GELU activations and layer normalization. The core network consists of an 8-layer Transformer encoder with 8 attention heads (compared to 32 for generalization), dmodel = 256, and feedforward dimension of 256. The output head is simple two-layer MLP [256 50] with GELU activation and layer normalization that directly predicts the 50 Laplacian eigenvectors. Training Configuration. We optimize with AdamW (learning rate 0.001) and cosine annealing schedule decaying to 1 105 over 5000 epochs, training for total of 12,000 epochs. Gradient clipping is applied at 0.01. The training consists of single mesh with batch size 1, allowing the model to specialize completely to the target geometry. Data Processing. The single training mesh is sampled to 4000 points using Farthest Point Sampling (FPS) with random initialization at each iteration to provide variation during training. We use higher point count compared to the generalization setting (4000 vs 1500) since batch size 1 requires significantly less GPU memory, allowing for denser point sampling. We generate 1500 scalar fields per shape by sampling uniform random values in the range [-1, 1] at each vertex. Before computing the reconstruction loss, 3 each scalar field is smoothed for 40 iterations using Gaussian kernel convolution over k-nearest neighbor graph with = 10, where σ is randomly sampled from the range [0.01, 0.2] independently for each scalar field. We employ this range of smoothing scales rather than fixed value because we empirically observed improved results across various shapes. We speculate that sampling different σ values allows the network to sense the shapes manifold at multiple scales, which helps generate statistically sufficient number of smooth functions that are smooth with respect to the surface metric requirement for optimal approximation. Empirically, we found that using range of σ values does not contribute significantly to the generalization case, suggesting this multi-scale sensing is particularly beneficial when overfitting to specific geometry. The kNN graph uses Euclidean distance and does not include self-loops. Validation is performed every 200 epochs, with model checkpoints saved at the same frequency. Computational Resources. Experiments were conducted on single NVIDIA GPU using PyTorch Lightning. The compact architecture and small batch size make this configuration accessible for consumer-grade hardware such as an RTX 3090, requiring significantly fewer computational resources than the generalization model. Eigenvalues at Inference Time. Our model is designed to compute spectral basis. However, we can recover without additional cost as byproduct the associated eigenvalIt is ues of the implicit optimal-reconstruction operator. given by the invert of the worse approximation error. Depending on how we generate probe functions at inference time, we get different errors and thus different eigenvalues. Our methodology to generate probe functions depends on three hyperparameters: the number of nearest neighbors in the kNN graph, the number of smoothing iterations, and the smoothing scale σ. By default, we propose to take at inference time the fixed hyperparameters = 70, 48 iterations, and σ = 0.101. These numbers gave us the minimal mean relative eigenvalue discrepancy with the oracle cotangent Laplacians eigenvalues across all tested shapes. However, we also manually tuned these hyperparameters per shape and provide in Tab. 2 the best ones we found. Unless specified otherwise, presented results used the tuned version rather than the fixed one. In future work we intend to provide an automatic mechanism for finding the best hyperparameters per shape. For instance, as probe functions depend differentiably on σ, it could be learned via descent strategies. C.2.4. Additional Results Table 2. Optimal hyperparameters for generating probe functions in the single-shape overfitting setting when estimating eigenvalues. For each shape, we report the k-nearest neighbors (k), number of smoothing iterations, and Gaussian kernel bandwidth (σ) that yielded the best alignment with the oracle cotangent Laplacians eigenvalues."
        },
        {
            "title": "Armadillo\nBeetle\nBimba\nBotijo\nDavid\nDente\nDragon\nElephant\nEros\nFertility\nHeptoroid\nHorse\nKitten\nKnot\nLaurent Hand\nLion\nMaster Cylinder\nPegaso\nTeddy\nWoodenfish\nWrench",
            "content": "k 45 70 33 27 33 21 70 57 27 63 39 70 27 15 21 39 15 70 45"
        },
        {
            "title": "Iterations",
            "content": "σ 111 120 57 93 48 75 48 75 102 40 40 75 66 66 102 111 102 120 48 120 120 0.194 0.120 0.101 0.120 0.138 0.101 0.269 0.157 0.138 0.176 0.213 0.194 0.120 0.101 0.101 0.176 0.269 0.213 0.138 0.232 0.082 Overfitting setting. In Figs. 9 to 12, we provide additional visualizations of the learned unnormalized spectral basis vi and obtained spectral filtering, generalizing Fig. 3 to other shapes. We also plot on few shapes in Figs. 13 to 17 all the first 50 spectral basis vectors (excluding the first constant one v1), i.e. v2, . . . , v50. In Fig. 19, we plot eigenvalue curves on other shapes than in Fig. 4 in the tuned hyperparameter setting for generating inference probe functions, along with those obtained in the fixed hyperparameter setting in Fig. 20. In Fig. 18, we display more estimated metrics from q1 on many more shapes than in Fig. 5. In Tab. 3, we give further quantitative results between our estimated eigendecomposition and the oracle cotangent one6 for the average cosine similarity and the eigenvalue discrepancy between both bases, extending Tab. 1. These results supplement those in the main manuscript demonstrating how well we are able to recover the oracle Laplacian spectral decomposition, both the basis and associated eigenvalues, with our direct neural method without any knowledge on the underlying mesh structure. While extremely Due to page-length constraints, we here provide additional results to those in the main paper. 6Using the ground-truth mesh information that our point cloud method does not have access to. 4 Table 3. Average cosine similarity between predicted and oracle eigenfunctions at different truncation levels k, and mean relative eigenvalue discrepancy (overfitting setting). Shape Beetle David Dente Dragon Eros Heptoroid Horse Knot Wrench Master Cylinder Teddy Image 10 50 λ Discrepancy 0.855 0.803 0.981 0.879 0. 0.879 0.976 0.827 0.450 0.823 0. 0.529 0.514 0.706 0.098 0.126 0.106 0.150 0.197 0.481 0. 0.891 0.640 0.083 0.156 0.796 0.927 0. 0.938 0.948 0.987 0.757 0.908 0. 0.817 0.923 0.981 0.509 0.718 0. 0.476 0.779 0.912 0.182 0.185 0.107 0.128 0.127 0. 0.694 0.222 0.051 0.045 0.109 0.146 similar at low frequencies, differences emerge at higher frequencies, revealing that our method can account for different details from the oracle, while still preserving the same overall (i.e. low-pass) information of the shape manifold. Unlike the eigenfunctions, recovering the same eigenvalues as those of the oracle Laplacian is trickier and often requires carefully tuning hyperparameters to generate the appropriate probe functions at test time. Generelization setting. In Tab. 5, we provide quantitative results comparing our predicted and oracle eigenfunctions on each evaluation dataset. Our generalization method quantitatively demonstrates potential for designing foundation model to estimate Laplacian spectral bases. proper foundational model would be achieved by greatly scaling up the amount of training data, as is usually done for foundation models in other fields like image or text processing. In our limited training setting, yet still large for initial insights, our model is able to accurately estimate the first Fourier eigenfunctions, which corresponds to low-pass information and thus to the most important global information on the manifold. As expected, performance is naturally lower than in the single-shape overfitting setting. C.3. High-Dimensional Manifolds C.3.1. Short Manifold Learning Overview Manifold learning is classical task [43, 67, 117, 130] It consists in findin data analysis and computer vision. ing low-dimensional representations capturing the manifold structure of high-dimensional data. In practice, methods aim to find very low-dimensional embeddings in Rk, with (where is the original high dimension), by preserving pairwise dissimilarities based on distances. The wide collection of approaches is usually grouped based on whether the targeted preserved structures of the manifold are local [10, 11, 35, 36, 39, 51, 7678, 81, 82, 109, 121, 122, 128, 140, 141], global [23, 46, 134, 135], or combination of both [1, 22, 25, 38, 57, 60, 62, 80, 91, 94, 107, 111 113, 123]. C.3.2. Implementation For manifold learning experiments on image datasets, we work on the embedded feature manifold provided by pretrained vision models. Thus, each image is mapped to high-dimensional7 feature space, and we learn the spectral decomposition of the resulting manifold structure across four benchmark datasets: Caltech256, CIFAR100, Imagenette, and STL-10. Feature Extraction. We experiment with two pretrained vision models as feature extractors: CLIP8 producing 512dimensional embeddings, and DINOv29 producing 384dimensional embeddings. These frozen feature extractors map images to points on learned manifold, where geometric relationships reflect semantic similarities. The extracted embeddings are used directly as spatial coordinates for kNN graph construction with cosine similarity. and Hyperparameters. Network Architecture In the architecture and probe function Tab. 4, we detail generation hyperparameters for each experiment. For CLIP features, we use preprocessing MLP [512 256], followed by 6-layer Transformer encoder with 8 attention heads, dmodel = 256, feedforward dimension of 256, dropout of 0.1, and GELU activations. The output head is [256 64 11] with layer normalization. For DINOv2 features, we use more compact architecture: preprocessing MLP [384 128], 6-layer Transformer with 4-8 attention heads depending on the dataset, dmodel = 128, and simpler output head [128 11]. Training Configuration. We optimize with AdamW (learning rate 0.0005) and accumulate gradients over 16 batches. Gradient clipping is applied at 0.01. Training uses distributed data parallel (DDP) [73] strategy across multiple GPUs. Each dataset samples 5000 images uniformly, with deterministic sampling for validation and random sampling for training. We predict 11 eigenvectors for spectral clustering evaluation at {2, 5, 10}. We generate 300 probe functions per manifold by sampling uniform random values in [-1, 1] at each point. 7Yet lower-dimensional than the original image dimensions. 8https://huggingface.co/openai/clipvitbasepatch32 9https://huggingface.co/facebook/dinov2-small 5 Evaluation Setting. We compare our method against classical manifold learning approaches including PCA, UMAP, t-SNE, Isomap, and Laplacian Eigenmaps. For evaluation, we use class-weighted sampling to create challenging, non-uniform manifold distributions. Specifically, we sample 1500 images using random class weights with entropy in the range [0.01, 0.1], ensuring that the sampled manifolds have imbalanced class distributions. This sampling strategy tests the robustness of each method to realistic, non-uniform data distributions across image classes. Due to the randomness of the sampling and of some of the methods, including ours due to random probe functions, we perform 32 runs for each method on each dataset. Sensitivity to Hyperparameters. Similar to the singleshape overfitting experiments in the 3D case, we observe that results in the image manifold setting are sensitive to the choice of probe function smoothing hyperparameters (k, σ, iterations). The parameters in Table 4 were tuned per dataset to achieve good performance. In future work, these probe function parameters could potentially be learned during training rather than manually tuned. We emphasize that the current results showcase the potential of our method, but more optimized tuning should yield better results. As such, we believe the full potential of our approach for image manifold learning has yet to be realized. Computational Resources. All experiments were conducted on 8 GPUs (type NVIDIA A100-SXM4-40GB) using PyTorch Lightning with DDP training strategy. Table 4. Architecture and probe function generation hyperparameters for image manifold experiments. All experiments use 6 Transformer layers, 11 predicted eigenvectors, 5000 sampled images, and 300 probe functions. The kNN graph always uses cosine similarity with self-loops enabled. Dataset Feature Extractor dmodel Attn. Heads Caltech256 Caltech256 CIFAR100 CIFAR100 Imagenette Imagenette STL-10 STL-10 CLIP DINOv2 CLIP DINOv2 CLIP DINOv2 CLIP DINOv2 256 128 256 128 256 128 256 128 8 8 8 4 8 4 8 10 15 5 30 20 30 35 20 σ Iterations 0.04 0.15 0.04 0.20 0.08 0.10 0.08 0.10 20 40 20 30 20 30 20 C.3.3. Results We evaluate our method on four image datasets (Caltech256, CIFAR100, Imagenette, and STL-10) using both CLIP and DINOv2 embeddings, comparing against classical manifold learning methods: PCA, UMAP, t-SNE, Isomap, and Laplacian Eigenmaps. Results are reported for spectral clustering with {2, 5, 10} clusters across seven standard clustering metrics: Normalized Mutual Information (NMI) [129], Adjusted Rand Index (ARI) [59], 6 Completeness [105], Adjusted Mutual Information (AMI) [129], Homogeneity [105], V-Measure [105], and FowlkesMallows Index (FMI) [44]. Quantitative Results. In Tabs. 6 to 13 we present comprehensive results, averaged over our 32 runs, across all datasets and feature extractors. Note that UMAP and t-SNE often get better clustering scores, which is unsurprising as these methods are designed to handle classification datasets by overclustering their embeddings (better scores yet often artificial separation between clusters). Our method (Optimal Approximation Eigenmaps) demonstrates competitive performance across most settings. Our approach generally outperforms classical methods like PCA, Isomap, and most importantly, shows superior performance to its vanilla analog, Laplacian Eigenmaps, in most cases. On CIFAR100 with CLIP embeddings, our method achieves particularly strong results at = 10 (NMI = 0.641, ARI = 0.257), matching or exceeding Laplacian Eigenmaps while substantially outperforming PCA and Isomap. Similarly, on Caltech256 with DINOv2 features at = 5, we achieve NMI = 0.714 and ARI = 0.246, demonstrating effective learned spectral representations. The method shows consistent improvements as the number of clusters increases from 2 to 10, suggesting that higher-dimensional embeddings better capture the manifold structure. Qualitative Analysis. In Figs. 21 to 28, we plot the 2D (i.e. = 2) clustering results for Imagenette and STL-10 datasets with both CLIP and DINOv2 embeddings. These visualizations demonstrate that our learned spectral basis captures meaningful semantic structure in the image manifolds, with distinct clusters corresponding to different object categories that are better clustered than in traditional methods like PCA and Isomap. In addition to accurate clusters, our embeddings provide smooth transitions between clusters, indicating that the learned basis successfully preserves the underlying smooth manifold geometry, unlike methods artificially overclustering the data like t-SNE and UMAP. This important observation demonstrates that our method is in fact superior to modern references t-SNE and UMAP, which cluster very well as revealed by the quantitative results but poorly preserve the manifold structure. Our visualisations are significantly better than those of Laplacian Eigenmaps, the vanilla analog of our method using the Graph Laplacian eigenfunctions instead of our Optimal Approximation Operators eigenfunctions. Indeed, in Laplacian Eigenmaps clusters are ill-shaped and artificially separated unlike in our method. Figure 9. Unnormalized spectral basis (top) and xyz reconstruction from basis vectors (bottom), using either the oracle cotangent Laplacian or our method (overfitting setting). Scalars are cosine similarities between basis vectors. 7 Figure 10. Unnormalized spectral basis (top) and xyz reconstruction from basis vectors (bottom), using either the oracle cotangent Laplacian or our method (overfitting setting). Scalars are cosine similarities between basis vectors. 8 Figure 11 Unnormalized spectral basis (top) and xyz reconstruction from basis vectors (bottom), using either the oracle cotangent Laplacian or our method (overfitting setting). Scalars are cosine similarities between basis vectors. Figure 12. Unnormalized spectral basis (top) and xyz reconstruction from basis vectors (bottom), using either the oracle cotangent Laplacian or our method (overfitting setting). Scalars are cosine similarities between basis vectors. 10 Figure 13. Unnormalized spectral basis using either the oracle cotangent Laplacian or our method (overfitting setting). Scalars are cosine similarities between basis vectors. 11 Figure 14. Unnormalized spectral basis using either the oracle cotangent Laplacian or our method (overfitting setting). Scalars are cosine similarities between basis vectors. Figure 15. Unnormalized spectral basis using either the oracle cotangent Laplacian or our method (overfitting setting). Scalars are cosine similarities between basis vectors. 13 Figure 16. Unnormalized spectral basis using either the oracle cotangent Laplacian or our method (overfitting setting). Scalars are cosine similarities between basis vectors. 14 Figure 17. Unnormalized spectral basis using either the oracle cotangent Laplacian or our method (overfitting setting). Scalars are cosine similarities between basis vectors. Figure 18. Estimated mass metric from q1 (overfitting setting). Figure 19. Eigenvalues of the oracle cotangent Laplacian and our estimated ones (overfitting setting, fine-tuned hyperparameter configuration for generating probe functions per shape). 16 Figure 20. Eigenvalues of the oracle cotangent Laplacian and our estimated ones (overfitting setting, fixed hyperparameter configuration for generating probe functions across all shapes). Table 5. Average cosine similarity between predicted and oracle eigenfunctions at different truncation levels on each evaluation dataset (generalization setting)."
        },
        {
            "title": "Dataset",
            "content": "#Shapes 5 10 15 20 25 30 35 40 45 50 105 380 700 1200 50 MPZ14 SHREC07 SHREC14 SHREC15 SHREC19 SHREC20-GR 220 SHREC20-NI DefTransfer TopKids 14 278 26 0.735 0.729 0.793 0.693 0.750 0.670 0.598 0.621 0.788 0.612 0.610 0.666 0.570 0.633 0.514 0.536 0.500 0. 0.526 0.535 0.585 0.511 0.556 0.435 0.495 0.425 0.551 0.473 0.480 0.532 0.458 0.495 0.385 0.446 0.386 0.486 0.429 0.438 0.478 0.417 0.448 0.348 0.404 0.352 0.438 0.393 0.403 0.438 0.384 0.409 0.321 0.375 0.326 0.402 0.365 0.374 0.404 0.356 0.381 0.296 0.348 0.303 0.374 0.341 0.349 0.377 0.331 0.357 0.277 0.326 0.285 0. 0.320 0.327 0.352 0.310 0.334 0.261 0.307 0.269 0.331 0.301 0.308 0.329 0.291 0.315 0.247 0.291 0.254 0.312 17 Table 6. Average manifold learning results on STL10 with DINOv2 embeddings. The mean and standard deviation over 32 runs is provided. The best and second best results are in bold and underlined respectively. STL10 - DINOv2 Method Opt. App. Eigenmaps (Ours) Laplacian Eigenmaps PCA UMAP t-SNE Isomap Opt. App. Eigenmaps (Ours) Laplacian Eigenmaps PCA UMAP t-SNE Isomap Opt. App. Eigenmaps (Ours) Laplacian Eigenmaps PCA UMAP t-SNE Isomap 2 2 2 2 2 2 5 5 5 5 5 10 10 10 10 10 10 NMI ARI Comp. AMI Homo. V-Meas. FMI 0.790 0.083 0.594 0.075 0.668 0.046 0.808 0.058 0.789 0.058 0.684 0.052 0.859 0.061 0.815 0.054 0.746 0.053 0.821 0.051 0.796 0.060 0.772 0.047 0.892 0.047 0.828 0.077 0.788 0.045 0.821 0.051 0.810 0.051 0.794 0.050 0.712 0.144 0.382 0.096 0.475 0.073 0.633 0.119 0.595 0.106 0.520 0. 0.812 0.120 0.734 0.130 0.565 0.092 0.666 0.109 0.616 0.112 0.626 0.104 0.863 0.098 0.768 0.124 0.620 0.097 0.665 0.109 0.643 0.108 0.656 0.101 0.752 0.094 0.568 0.058 0.611 0.041 0.741 0.082 0.718 0.080 0.632 0.062 0.823 0.081 0.793 0.063 0.682 0.073 0.757 0.076 0.727 0.083 0.711 0.071 0.862 0.071 0.835 0.068 0.721 0.068 0.758 0.076 0.743 0.077 0.733 0.072 0.787 0.084 0.589 0.076 0.664 0.046 0.806 0.059 0.786 0.058 0.681 0. 0.858 0.061 0.813 0.055 0.743 0.053 0.819 0.051 0.793 0.060 0.769 0.048 0.891 0.048 0.826 0.077 0.785 0.046 0.819 0.051 0.808 0.051 0.791 0.050 0.834 0.080 0.627 0.111 0.740 0.075 0.894 0.038 0.879 0.033 0.751 0.066 0.902 0.047 0.842 0.065 0.828 0.043 0.901 0.031 0.884 0.037 0.849 0.037 0.927 0.029 0.827 0.101 0.873 0.036 0.901 0.031 0.896 0.030 0.870 0.037 0.790 0.083 0.594 0.075 0.668 0.046 0.808 0.058 0.789 0.058 0.684 0. 0.859 0.061 0.815 0.054 0.746 0.053 0.821 0.051 0.796 0.060 0.772 0.047 0.892 0.047 0.828 0.077 0.788 0.045 0.821 0.051 0.810 0.051 0.794 0.050 0.775 0.110 0.510 0.095 0.579 0.076 0.717 0.081 0.686 0.070 0.618 0.074 0.856 0.088 0.792 0.104 0.658 0.061 0.743 0.074 0.704 0.075 0.709 0.069 0.896 0.071 0.829 0.073 0.706 0.065 0.742 0.074 0.726 0.071 0.733 0.068 Table 7. Average manifold learning results on STL10 with CLIP embeddings. The mean and standard deviation over 32 runs is provided. The best and second best results are in bold and underlined respectively. STL10 - CLIP Method Opt. App. Eigenmaps (Ours) Laplacian Eigenmaps PCA UMAP t-SNE Isomap Opt. App. Eigenmaps (Ours) Laplacian Eigenmaps PCA UMAP t-SNE Isomap Opt. App. Eigenmaps (Ours) Laplacian Eigenmaps PCA UMAP t-SNE Isomap 2 2 2 2 2 2 5 5 5 5 5 5 10 10 10 10 10 10 NMI ARI Comp. AMI Homo. V-Meas. FMI 0.691 0.071 0.585 0.071 0.673 0.041 0.835 0.050 0.801 0.056 0.718 0.053 0.820 0.060 0.796 0.042 0.760 0.043 0.838 0.049 0.803 0.064 0.796 0. 0.863 0.047 0.796 0.131 0.788 0.051 0.833 0.049 0.811 0.066 0.814 0.047 0.525 0.108 0.365 0.109 0.515 0.071 0.676 0.111 0.614 0.110 0.590 0.113 0.741 0.120 0.716 0.107 0.593 0.091 0.694 0.103 0.632 0.118 0.672 0.102 0.818 0.086 0.720 0.166 0.628 0.103 0.685 0.103 0.650 0.118 0.697 0.111 0.653 0.071 0.558 0.056 0.615 0.043 0.770 0.078 0.730 0.080 0.667 0.065 0.795 0.072 0.774 0.063 0.694 0.061 0.777 0.074 0.735 0.088 0.741 0. 0.845 0.063 0.850 0.055 0.723 0.072 0.772 0.074 0.745 0.090 0.759 0.069 0.687 0.072 0.580 0.073 0.669 0.042 0.833 0.051 0.798 0.057 0.715 0.053 0.818 0.060 0.794 0.043 0.757 0.044 0.837 0.050 0.801 0.064 0.794 0.042 0.861 0.047 0.794 0.133 0.785 0.051 0.832 0.049 0.809 0.067 0.812 0.048 0.736 0.085 0.621 0.112 0.746 0.063 0.915 0.028 0.892 0.032 0.781 0.060 0.850 0.063 0.823 0.045 0.844 0.041 0.914 0.028 0.890 0.036 0.865 0. 0.884 0.047 0.770 0.161 0.870 0.035 0.910 0.029 0.894 0.038 0.881 0.031 0.691 0.071 0.585 0.071 0.673 0.041 0.835 0.050 0.801 0.056 0.718 0.053 0.820 0.060 0.796 0.042 0.760 0.043 0.838 0.049 0.803 0.064 0.796 0.041 0.863 0.047 0.796 0.131 0.788 0.051 0.833 0.049 0.811 0.066 0.814 0.047 0.622 0.093 0.493 0.110 0.613 0.069 0.752 0.075 0.703 0.072 0.675 0.095 0.797 0.094 0.775 0.095 0.682 0.066 0.765 0.070 0.717 0.080 0.745 0. 0.858 0.067 0.803 0.086 0.711 0.073 0.758 0.070 0.731 0.080 0.765 0.082 18 Table 8. Average manifold learning results on Imagenette with DINOv2 embeddings. The mean and standard deviation over 32 runs is provided. The best and second best results are in bold and underlined respectively. Imagenette - DINOv2 Method Opt. App. Eigenmaps (Ours) Laplacian Eigenmaps PCA UMAP t-SNE Isomap Opt. App. Eigenmaps (Ours) Laplacian Eigenmaps PCA UMAP t-SNE Isomap Opt. App. Eigenmaps (Ours) Laplacian Eigenmaps PCA UMAP t-SNE Isomap 2 2 2 2 2 2 5 5 5 5 5 5 10 10 10 10 10 NMI ARI Comp. AMI Homo. V-Meas. FMI 0.684 0.061 0.642 0.062 0.645 0.075 0.875 0.035 0.829 0.036 0.679 0.045 0.842 0.043 0.800 0.069 0.759 0.048 0.881 0.038 0.825 0.029 0.836 0.039 0.858 0.046 0.752 0.092 0.795 0.047 0.882 0.038 0.820 0.036 0.862 0.043 0.602 0.114 0.456 0.102 0.486 0.126 0.802 0.076 0.716 0.072 0.599 0.090 0.844 0.051 0.703 0.137 0.629 0.080 0.816 0.086 0.725 0.059 0.790 0. 0.856 0.055 0.577 0.145 0.676 0.080 0.818 0.086 0.721 0.068 0.818 0.093 0.655 0.070 0.627 0.048 0.617 0.062 0.836 0.053 0.782 0.045 0.646 0.041 0.818 0.058 0.787 0.071 0.718 0.049 0.845 0.058 0.779 0.040 0.802 0.049 0.832 0.060 0.820 0.061 0.754 0.052 0.846 0.058 0.776 0.047 0.828 0.063 0.680 0.062 0.638 0.063 0.641 0.077 0.873 0.036 0.828 0.036 0.676 0.046 0.840 0.044 0.798 0.070 0.756 0.048 0.880 0.039 0.823 0.029 0.834 0. 0.857 0.046 0.749 0.092 0.793 0.047 0.881 0.039 0.818 0.037 0.860 0.043 0.716 0.053 0.659 0.082 0.677 0.092 0.918 0.023 0.884 0.036 0.717 0.058 0.868 0.032 0.817 0.081 0.806 0.056 0.922 0.021 0.879 0.030 0.875 0.045 0.888 0.037 0.706 0.122 0.841 0.052 0.923 0.021 0.871 0.034 0.901 0.030 0.684 0.061 0.642 0.062 0.645 0.075 0.875 0.035 0.829 0.036 0.679 0.045 0.842 0.043 0.800 0.069 0.759 0.048 0.881 0.038 0.825 0.029 0.836 0. 0.858 0.046 0.752 0.092 0.795 0.047 0.882 0.038 0.820 0.036 0.862 0.043 0.667 0.098 0.550 0.091 0.568 0.113 0.839 0.060 0.767 0.058 0.665 0.080 0.872 0.041 0.754 0.117 0.691 0.070 0.850 0.068 0.775 0.046 0.827 0.065 0.881 0.044 0.685 0.091 0.732 0.067 0.851 0.068 0.771 0.055 0.851 0.073 Table 9. Average manifold learning results on Imagenette with CLIP embeddings. The mean and standard deviation over 32 runs is provided. The best and second best results are in bold and underlined respectively. Imagenette - CLIP Method Opt. App. Eigenmaps (Ours) Laplacian Eigenmaps PCA UMAP t-SNE Isomap Opt. App. Eigenmaps (Ours) Laplacian Eigenmaps PCA UMAP t-SNE Isomap Opt. App. Eigenmaps (Ours) Laplacian Eigenmaps PCA UMAP t-SNE Isomap 2 2 2 2 2 5 5 5 5 5 5 10 10 10 10 10 10 NMI ARI Comp. AMI Homo. V-Meas. FMI 0.746 0.055 0.519 0.109 0.682 0.043 0.895 0.042 0.876 0.041 0.755 0.040 0.882 0.034 0.820 0.058 0.818 0.040 0.898 0.043 0.875 0.042 0.876 0.030 0.915 0.028 0.819 0.167 0.855 0.034 0.899 0.041 0.876 0.039 0.895 0. 0.680 0.083 0.281 0.130 0.551 0.073 0.818 0.095 0.782 0.091 0.683 0.074 0.874 0.055 0.707 0.141 0.746 0.080 0.830 0.094 0.795 0.093 0.844 0.064 0.917 0.045 0.707 0.193 0.780 0.077 0.830 0.091 0.793 0.084 0.852 0.080 0.712 0.061 0.532 0.083 0.642 0.034 0.855 0.065 0.831 0.063 0.721 0.039 0.858 0.046 0.809 0.053 0.774 0.050 0.859 0.066 0.832 0.065 0.842 0.046 0.895 0.042 0.899 0.141 0.811 0.051 0.860 0.064 0.832 0.060 0.860 0. 0.743 0.056 0.513 0.112 0.678 0.044 0.894 0.042 0.875 0.042 0.752 0.040 0.880 0.034 0.818 0.058 0.816 0.040 0.897 0.043 0.874 0.043 0.874 0.030 0.914 0.029 0.817 0.168 0.854 0.034 0.898 0.042 0.874 0.040 0.894 0.038 0.784 0.056 0.515 0.140 0.728 0.060 0.941 0.017 0.929 0.022 0.794 0.054 0.908 0.026 0.833 0.074 0.869 0.039 0.942 0.020 0.925 0.022 0.914 0.027 0.937 0.017 0.761 0.174 0.907 0.025 0.943 0.018 0.926 0.022 0.934 0. 0.746 0.055 0.519 0.109 0.682 0.043 0.895 0.042 0.876 0.041 0.755 0.040 0.882 0.034 0.820 0.058 0.818 0.040 0.898 0.043 0.875 0.042 0.876 0.030 0.915 0.028 0.819 0.167 0.855 0.034 0.899 0.041 0.876 0.039 0.895 0.038 0.733 0.069 0.421 0.104 0.623 0.071 0.853 0.074 0.823 0.071 0.735 0.066 0.896 0.044 0.757 0.119 0.792 0.064 0.862 0.073 0.834 0.072 0.872 0.052 0.932 0.036 0.790 0.108 0.821 0.060 0.863 0.071 0.832 0.064 0.880 0. 19 Table 10. Average manifold learning results on CALTECH256 with DINOv2 embeddings. The mean and standard deviation over 32 runs is provided. The best and second best results are in bold and underlined respectively. CALTECH256 - DINOv2 Method Opt. App. Eigenmaps (Ours) Laplacian Eigenmaps PCA UMAP t-SNE Isomap Opt. App. Eigenmaps (Ours) Laplacian Eigenmaps PCA UMAP t-SNE Isomap Opt. App. Eigenmaps (Ours) Laplacian Eigenmaps PCA UMAP t-SNE Isomap 2 2 2 2 2 2 5 5 5 5 5 5 10 10 10 10 10 10 NMI ARI Comp. AMI Homo. V-Meas. FMI 0.611 0.027 0.650 0.036 0.603 0.029 0.834 0.014 0.840 0.015 0.655 0.021 0.714 0.020 0.724 0.024 0.696 0.019 0.843 0.013 0.832 0.015 0.754 0.017 0.759 0.016 0.758 0.016 0.747 0.015 0.843 0.013 0.823 0.015 0.799 0.015 0.078 0.026 0.121 0.037 0.063 0.020 0.448 0.084 0.419 0.071 0.138 0.045 0.246 0.062 0.209 0.044 0.184 0.039 0.465 0.081 0.394 0.066 0.319 0.067 0.322 0.060 0.257 0.042 0.267 0.049 0.462 0.083 0.379 0.066 0.409 0. 0.594 0.030 0.645 0.038 0.583 0.032 0.810 0.020 0.809 0.021 0.633 0.024 0.696 0.024 0.712 0.026 0.671 0.023 0.820 0.020 0.800 0.020 0.732 0.021 0.743 0.021 0.743 0.021 0.721 0.020 0.819 0.021 0.792 0.021 0.777 0.019 0.196 0.053 0.311 0.061 0.165 0.050 0.654 0.048 0.660 0.036 0.273 0.063 0.411 0.058 0.445 0.053 0.355 0.054 0.675 0.039 0.642 0.039 0.486 0.056 0.508 0.047 0.508 0.045 0.466 0.052 0.673 0.040 0.623 0.039 0.582 0. 0.629 0.024 0.655 0.036 0.626 0.027 0.860 0.013 0.873 0.011 0.678 0.019 0.733 0.019 0.737 0.026 0.722 0.015 0.868 0.011 0.866 0.011 0.778 0.015 0.775 0.014 0.774 0.015 0.776 0.013 0.868 0.011 0.857 0.011 0.823 0.012 0.611 0.027 0.650 0.036 0.603 0.029 0.834 0.014 0.840 0.015 0.655 0.021 0.714 0.020 0.724 0.024 0.696 0.019 0.843 0.013 0.832 0.015 0.754 0.017 0.759 0.016 0.758 0.016 0.747 0.015 0.843 0.013 0.823 0.015 0.799 0. 0.092 0.031 0.134 0.036 0.077 0.026 0.476 0.073 0.457 0.059 0.155 0.051 0.263 0.067 0.223 0.042 0.206 0.043 0.493 0.068 0.432 0.056 0.341 0.067 0.339 0.062 0.272 0.039 0.293 0.050 0.490 0.070 0.416 0.055 0.433 0.068 Table 11. Average manifold learning results on CALTECH256 with CLIP embeddings. The mean and standard deviation over 32 runs is provided. The best and second best results are in bold and underlined respectively. CALTECH256 - CLIP Method Opt. App. Eigenmaps (Ours) Laplacian Eigenmaps PCA UMAP t-SNE Isomap Opt. App. Eigenmaps (Ours) Laplacian Eigenmaps PCA UMAP t-SNE Isomap Opt. App. Eigenmaps (Ours) Laplacian Eigenmaps PCA UMAP t-SNE Isomap 2 2 2 2 2 2 5 5 5 5 5 10 10 10 10 10 10 NMI ARI Comp. AMI Homo. V-Meas. FMI 0.596 0.031 0.600 0.055 0.599 0.031 0.844 0.015 0.862 0.011 0.643 0.020 0.685 0.022 0.688 0.024 0.677 0.020 0.857 0.013 0.853 0.011 0.742 0.017 0.734 0.018 0.735 0.020 0.724 0.014 0.856 0.014 0.844 0.012 0.788 0.013 0.069 0.030 0.075 0.032 0.062 0.022 0.472 0.094 0.465 0.079 0.115 0. 0.226 0.068 0.160 0.029 0.165 0.044 0.501 0.089 0.430 0.070 0.291 0.056 0.309 0.071 0.223 0.038 0.237 0.051 0.494 0.088 0.417 0.070 0.381 0.062 0.578 0.033 0.599 0.055 0.577 0.034 0.819 0.021 0.832 0.019 0.622 0.023 0.666 0.026 0.685 0.026 0.653 0.022 0.835 0.020 0.821 0.019 0.721 0.021 0.718 0.022 0.728 0.020 0.698 0.018 0.833 0.021 0.812 0.019 0.767 0.018 0.159 0.050 0.225 0.076 0.153 0.046 0.673 0.051 0.709 0.031 0.249 0. 0.348 0.055 0.390 0.051 0.315 0.064 0.704 0.040 0.686 0.033 0.463 0.059 0.456 0.050 0.475 0.048 0.414 0.062 0.702 0.040 0.667 0.036 0.560 0.054 0.615 0.029 0.602 0.057 0.622 0.028 0.869 0.013 0.895 0.007 0.666 0.018 0.704 0.020 0.692 0.025 0.704 0.019 0.881 0.010 0.887 0.007 0.765 0.015 0.750 0.015 0.742 0.025 0.752 0.013 0.881 0.011 0.879 0.008 0.810 0.012 0.596 0.031 0.600 0.055 0.599 0.031 0.844 0.015 0.862 0.011 0.643 0. 0.685 0.022 0.688 0.024 0.677 0.020 0.857 0.013 0.853 0.011 0.742 0.017 0.734 0.018 0.735 0.020 0.724 0.014 0.856 0.014 0.844 0.012 0.788 0.013 0.083 0.036 0.088 0.032 0.076 0.028 0.501 0.081 0.503 0.064 0.132 0.041 0.243 0.073 0.173 0.029 0.186 0.052 0.528 0.075 0.469 0.056 0.313 0.056 0.325 0.073 0.236 0.038 0.262 0.056 0.522 0.074 0.456 0.057 0.403 0.059 Table 12. Average manifold learning results on CIFAR100 with DINOv2 embeddings. The mean and standard deviation over 32 runs is provided. The best and second best results are in bold and underlined respectively. CIFAR100 - DINOv2 Method Opt. App. Eigenmaps (Ours) Laplacian Eigenmaps PCA UMAP t-SNE Isomap Opt. App. Eigenmaps (Ours) Laplacian Eigenmaps PCA UMAP t-SNE Isomap Opt. App. Eigenmaps (Ours) Laplacian Eigenmaps PCA UMAP t-SNE Isomap 2 2 2 2 2 2 5 5 5 5 5 5 10 10 10 10 10 10 NMI ARI Comp. AMI Homo. V-Meas. FMI 0.568 0.016 0.590 0.032 0.492 0.017 0.745 0.016 0.740 0.014 0.533 0. 0.583 0.017 0.640 0.019 0.601 0.013 0.750 0.016 0.736 0.015 0.647 0.017 0.613 0.013 0.669 0.018 0.649 0.018 0.751 0.015 0.734 0.019 0.689 0.018 0.164 0.036 0.186 0.037 0.098 0.019 0.433 0.040 0.412 0.038 0.142 0.032 0.176 0.029 0.239 0.036 0.206 0.031 0.447 0.041 0.403 0.036 0.291 0.048 0.214 0.039 0.273 0.038 0.268 0.042 0.448 0.038 0.400 0.044 0.355 0.055 0.576 0.016 0.577 0.032 0.472 0.020 0.716 0.018 0.708 0.016 0.512 0. 0.602 0.023 0.622 0.018 0.576 0.015 0.722 0.018 0.705 0.017 0.622 0.018 0.622 0.016 0.649 0.017 0.622 0.017 0.722 0.017 0.703 0.020 0.664 0.017 0.412 0.046 0.426 0.050 0.273 0.046 0.635 0.040 0.626 0.038 0.332 0.051 0.442 0.034 0.492 0.047 0.428 0.049 0.644 0.039 0.621 0.037 0.495 0.049 0.477 0.036 0.531 0.048 0.495 0.054 0.644 0.036 0.617 0.044 0.556 0.050 0.561 0.020 0.603 0.034 0.515 0.015 0.776 0.021 0.775 0.018 0.556 0. 0.565 0.016 0.659 0.024 0.629 0.015 0.781 0.020 0.771 0.018 0.673 0.020 0.605 0.017 0.690 0.024 0.679 0.023 0.781 0.018 0.768 0.023 0.717 0.023 0.568 0.016 0.590 0.032 0.492 0.017 0.745 0.016 0.740 0.014 0.533 0.018 0.583 0.017 0.640 0.019 0.601 0.013 0.750 0.016 0.736 0.015 0.647 0.017 0.613 0.013 0.669 0.018 0.649 0.018 0.751 0.015 0.734 0.019 0.689 0.018 0.191 0.038 0.206 0.038 0.118 0.024 0.457 0.043 0.439 0.041 0.163 0. 0.208 0.030 0.258 0.039 0.228 0.036 0.470 0.043 0.430 0.039 0.313 0.052 0.238 0.040 0.292 0.041 0.292 0.047 0.472 0.040 0.426 0.047 0.376 0.059 Table 13. Average manifold learning results on CIFAR100 with CLIP embeddings. The mean and standard deviation over 32 runs is provided. The best and second best results are in bold and underlined respectively. CIFAR100 - CLIP Method Opt. App. Eigenmaps (Ours) Laplacian Eigenmaps PCA UMAP t-SNE Isomap Opt. App. Eigenmaps (Ours) Laplacian Eigenmaps PCA UMAP t-SNE Isomap Opt. App. Eigenmaps (Ours) Laplacian Eigenmaps PCA UMAP t-SNE Isomap 2 2 2 2 2 2 5 5 5 5 5 5 10 10 10 10 10 NMI ARI Comp. AMI Homo. V-Meas. FMI 0.499 0.021 0.392 0.064 0.423 0.028 0.656 0.022 0.668 0.021 0.464 0.019 0.555 0.019 0.487 0.051 0.504 0.020 0.667 0.021 0.665 0.021 0.561 0.019 0.566 0.021 0.551 0.031 0.557 0.018 0.667 0.021 0.653 0.027 0.588 0.019 0.111 0.024 0.055 0.028 0.052 0.012 0.320 0.045 0.320 0.038 0.083 0.019 0.159 0.027 0.101 0.031 0.116 0.026 0.342 0.041 0.310 0.039 0.176 0. 0.173 0.037 0.137 0.037 0.172 0.035 0.343 0.045 0.292 0.049 0.211 0.039 0.488 0.024 0.414 0.068 0.406 0.031 0.628 0.024 0.639 0.024 0.448 0.022 0.552 0.022 0.520 0.047 0.481 0.023 0.641 0.023 0.636 0.023 0.541 0.021 0.577 0.023 0.579 0.025 0.532 0.021 0.641 0.024 0.625 0.028 0.567 0.021 0.300 0.040 0.208 0.080 0.177 0.036 0.506 0.049 0.523 0.044 0.240 0.044 0.388 0.040 0.333 0.059 0.286 0.048 0.525 0.044 0.519 0.044 0.375 0. 0.415 0.042 0.407 0.049 0.363 0.047 0.525 0.044 0.501 0.056 0.414 0.051 0.511 0.020 0.373 0.063 0.442 0.025 0.686 0.023 0.701 0.021 0.481 0.017 0.557 0.020 0.459 0.056 0.528 0.019 0.696 0.021 0.698 0.021 0.582 0.020 0.555 0.023 0.526 0.039 0.584 0.017 0.696 0.021 0.685 0.028 0.611 0.021 0.499 0.021 0.392 0.064 0.423 0.028 0.656 0.022 0.668 0.021 0.464 0.019 0.555 0.019 0.487 0.051 0.504 0.020 0.667 0.021 0.665 0.021 0.561 0. 0.566 0.021 0.551 0.031 0.557 0.018 0.667 0.021 0.653 0.027 0.588 0.019 0.132 0.027 0.088 0.035 0.072 0.016 0.345 0.048 0.346 0.041 0.103 0.024 0.180 0.030 0.140 0.032 0.138 0.030 0.365 0.045 0.336 0.042 0.197 0.041 0.200 0.037 0.174 0.036 0.195 0.039 0.366 0.048 0.318 0.053 0.232 0.044 21 Figure 21. 2D visualization of random subset of the Imagenette dataset using CLIP embeddings. Points are colored by ground truth class labels. Our learned spectral basis (Optimal Approximation Eigenmaps) produces meaningful clusters that separate the image manifold according to semantic content, with less overlap than in PCA and Isomap, while accurately preserving the smooth manifold structure without artificially overclustering it as in t-SNE, Umap, and Laplacian Eigenmaps. Figure 22. 2D visualization of another random subset of the Imagenette dataset using CLIP embeddings. 22 Figure 23. 2D visualization of random subset of the Imagenette dataset using DINOv2 embeddings. Figure 24. 2D visualization of another random subset of the Imagenette dataset using DINOv2 embeddings. 23 Figure 25. 2D visualization of random subset of the STL-10 dataset using CLIP embeddings. Figure 26. 2D visualization of another random subset of the STL-10 dataset using CLIP embeddings. 24 Figure 27. 2D visualization of random subset of the STL-10 dataset using DINOv2 embeddings. Figure 28. 2D visualization of another random subset of the STL-10 dataset using DINOv2 embeddings."
        }
    ],
    "affiliations": [
        "Munich Center for Machine Learning",
        "Technical University of Munich",
        "Technion - Israel Institute of Technology"
    ]
}