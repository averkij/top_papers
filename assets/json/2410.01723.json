{
    "paper_title": "HarmoniCa: Harmonizing Training and Inference for Better Feature Cache in Diffusion Transformer Acceleration",
    "authors": [
        "Yushi Huang",
        "Zining Wang",
        "Ruihao Gong",
        "Jing Liu",
        "Xinjie Zhang",
        "Jun Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Diffusion Transformers (DiTs) have gained prominence for outstanding scalability and extraordinary performance in generative tasks. However, their considerable inference costs impede practical deployment. The feature cache mechanism, which involves storing and retrieving redundant computations across timesteps, holds promise for reducing per-step inference time in diffusion models. Most existing caching methods for DiT are manually designed. Although the learning-based approach attempts to optimize strategies adaptively, it suffers from discrepancies between training and inference, which hampers both the performance and acceleration ratio. Upon detailed analysis, we pinpoint that these discrepancies primarily stem from two aspects: (1) Prior Timestep Disregard, where training ignores the effect of cache usage at earlier timesteps, and (2) Objective Mismatch, where the training target (align predicted noise in each timestep) deviates from the goal of inference (generate the high-quality image). To alleviate these discrepancies, we propose HarmoniCa, a novel method that Harmonizes training and inference with a novel learning-based Caching framework built upon Step-Wise Denoising Training (SDT) and Image Error Proxy-Guided Objective (IEPO). Compared to the traditional training paradigm, the newly proposed SDT maintains the continuity of the denoising process, enabling the model to leverage information from prior timesteps during training, similar to the way it operates during inference. Furthermore, we design IEPO, which integrates an efficient proxy mechanism to approximate the final image error caused by reusing the cached feature. Therefore, IEPO helps balance final image quality and cache utilization, resolving the issue of training that only considers the impact of cache usage on the predicted output at each timestep."
        },
        {
            "title": "Start",
            "content": "HARMONICA: HARMONIZING TRAINING AND INFERENCE FOR BETTER FEATURE CACHE IN DIFFUSION TRANSFORMER ACCELERATION Yushi Huang1, Zining Wang1,2, Ruihao Gong1,2, Jing Liu3, Xinjie Zhang4, Jun Zhang4 1SenseTime Research {huangyushi, ziningwang, gongruihao}@sensetime.com,liujing 95@outlook.com, xinjie.zhang@connect.ust.hk, eejzhang@ust.hk 2Beihang University 3Monash University 4HKUST 4 2 0 2 4 ] . [ 2 3 2 7 1 0 . 0 1 4 2 : r (a) PIXART-Σ w/o feature cache (b) HarmoniCa (1.68) Figure 1: High-resolution 2048 2048 images generated using PIXART-Σ (Chen et al., 2024a) with 20-step DPM-Solver++ sampler (Lu et al., 2022b). Our proposed feature cache framework achieves substantial 1.68 speedup. More visualization results can be found in Sec. K."
        },
        {
            "title": "ABSTRACT",
            "content": "Diffusion Transformers (DiTs) have gained prominence for outstanding scalability and extraordinary performance in generative tasks. However, their considerable inference costs impede practical deployment. The feature cache mechanism, which involves storing and retrieving redundant computations across timesteps, holds promise for reducing per-step inference time in diffusion models. Most existing caching methods for DiT are manually designed. Although the learningbased approach attempts to optimize strategies adaptively, it suffers from discrepancies between training and inference, which hampers both the performance and acceleration ratio. Upon detailed analysis, we pinpoint that these discrepancies primarily stem from two aspects: (1) Prior Timestep Disregard, where training ignores the effect of cache usage at earlier timesteps, and (2) Objective Mismatch, where the training target (align predicted noise in each timestep) deviates from the goal of inference (generate the high-quality image). To alleviate these discrepancies, we propose HarmoniCa, novel method that Harmonizes training Equal contribution; work completed during internships at SenseTime Research. Corresponding authors."
        },
        {
            "title": "Preprint",
            "content": "and inference with novel learning-based Caching framework built upon StepWise Denoising Training (SDT) and Image Error Proxy-Guided Objective (IEPO). Compared to the traditional training paradigm, the newly proposed SDT maintains the continuity of the denoising process, enabling the model to leverage information from prior timesteps during training, similar to the way it operates during inference. Furthermore, we design IEPO, which integrates an efficient proxy mechanism to approximate the final image error caused by reusing the cached feature. Therefore, IEPO helps balance final image quality and cache utilization, resolving the issue of training that only considers the impact of cache usage on the predicted output at each timestep. Extensive experiments on class-conditional and text-toimage (T2I) tasks for 7 models and 4 samplers with resolutions ranging from 256 256 to 2048 2048 demonstrate the exceptional performance and speedup capabilities of our HarmoniCa. For example, HarmoniCa is the first feature cache method applied to the 20-step PIXART-α 1024 1024 that achieves over 1.5 speedup in latency with an improved FID compared to the non-accelerated model. Remarkably, HarmoniCa requires no image data during training and reduces about 25% of training time compared to the existing learning-based approach."
        },
        {
            "title": "INTRODUCTION",
            "content": "Diffusion models (Ho et al., 2020; Dhariwal & Nichol, 2021) have recently gained increasing popularity in variety of generative tasks, such as image (Saharia et al., 2022; Esser et al., 2024) and video generation (Blattmann et al., 2023; Ma et al., 2024a), due to their ability to produce diverse and high-quality samples. Among different backbones, Diffusion Transformers (DiTs) (Peebles & Xie, 2023) stand out for offering exceptional scalability. However, the extensive parameter size and multi-round denoising nature of diffusion models bring tremendous computational overhead during inference, limiting their practical applications. For instance, generating one 20482048 resolution image using PixArt-Σ (Chen et al., 2024a) with 0.6B parameters and 20 denoising rounds can take up to 14 seconds on single NVIDIA H800 80GB GPU, which is unacceptable. To accelerate the generation process of diffusion models, previous methods are developed from two perspectives: reducing the number of sampling steps (Liu et al., 2022; Song et al., 2020b) and decreasing the network complexity in noise prediction of each step (Fang et al., 2023; He et al., 2024). Recently, new branch of research (Selvaraju et al., 2024; Yuan et al., 2024; Chen et al., 2024b) has started to focus on accelerating sampling time per step by the feature cache mechanism. This technique takes advantage of the repetitive computations across timesteps in diffusion models, allowing previously computed features to be cached and reused in later steps. Nevertheless, most existing methods are either tailored to the U-Net architecture (Ma et al., 2024c; Wimbauer et al., 2024) or develop their strategy based on empirical observations (Chen et al., 2024b; Selvaraju et al., 2024), and there is lack of adaptive and systematic approaches for DiT models. Learning-toCache (Ma et al., 2024b) introduces learnable router to guide the cache scheme for DiT models. However, this method induces discrepancies between training and inference, which always leads to distortion build-up (Ning et al., 2023; Li et al., 2024; Ning et al., 2024). The discrepancies arise from two main factors: (1) Prior Timestep Disregard: During training, the model directly samples timestep and employs the training images manually added noise akin to DDPM (Hu et al., 2021), ignoring the impact of the feature cache mechanism from earlier steps, which differs from the inference process. (2) Objective Mismatch: The training objective minimizes noise prediction error of each timestep, while the inference goal aims for high-quality final images, causing misalignment in objectives. We believe these inconsistencies hinder effective and efficient router learning. To alleviate the above discrepancies effectively, we present harmonizing training and inference with HarmoniCa, novel cache learning framework featuring unique training paradigm and distinct learning objective. Specifically, to mitigate the first disparity, we design Step-Wise Denoising Training (SDT), which aligns the training process with the full denoising trajectory of inference using student-teacher model setup. The student utilizes the cache while the teacher does not, effectively mimicking the teachers outputs across all continuous timesteps. This approach maintains the reuse and update of the cache at earlier timesteps, similar to inference. Additionally, to address the misalignment in optimization goals, we introduce the Image Error Proxy-Guided Objective (IEPO), which leverages proxy to approximate the final image error and reduces the significant costs of"
        },
        {
            "title": "Preprint",
            "content": "directly utilizing the error to supervise training. This objective helps SDT efficiently balance cache usage and image quality. By combining SDT and IEPO, extensive experiments for text-to-image (T2I) and class-conditioned generation tasks show the promising performance and speedup ratio of HarmoniCa, e.g., 1.51 speedup and even lower FID (Nash et al., 2021) for PIXART-α 1024 1024 (Chen et al., 2023). In addition, HarmoniCa eliminates the requirement of training with large amount of image data and reduces about 25% training time compared to the existing learning-based method (Ma et al., 2024b), further enhancing its applicability. Our contributions are summarized as follows: We uncover two discrepancies between training and inference in the existing learning-based feature cache method: (1) Prior Timestep Disregard, indicating that the training process overlooks the influence of preceding timesteps, which is inconsistent with the inference process. (2) Objective Mismatch, minimizing intermediate outputs error, instead of the final image error. These discrepancies prevent the method from further performance and acceleration improvements. We propose novel framework called HarmoniCa to alleviate the discovered discrepancies by: (1) Step-Wise Denoising Training (SDT), which addresses the first discrepancy by capturing the complete denoising trajectory, ensuring that the model learns to consider the impact of earlier timesteps. (2) Image Error Proxy-Guided Optimization Objective (IEPO), which mitigates the second discrepancy by using proxy for the final image error, and thereby targets aligning the training objective with the inference. Extensive experiments on NVIDIA H800 80GB GPUs for DiT-XL/2, PIXART-α, and PIXARTΣ seriesencompassing 7 models, 4 samplers, and 4 resolutionsproves the substantial efficacy and universality of HarmoniCa. For instance, it outperforms previous state-of-the-art (SOTA) by 6.74 IS increase and 1.24 FID decrease with higher speedup ratio on DiT-XL/2 256 256. Notably, our image-free framework with much lower training costs exhibits superior efficiency and applicability than the current learning-based method."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Diffusion models. Diffusion models, initially conceptualized with the U-Net architecture (Ronneberger et al., 2015), have achieved satisfactory performance in image (Rombach et al., 2022; Podell et al., 2023) and video generation (Ho et al., 2022). Despite their success, U-Net models struggle with modeling long-range dependencies in complex, high-dimensional data. In response, the Diffusion Transformer (DiT) (Peebles & Xie, 2023; Chen et al., 2023; 2024a) is introduced, leveraging the inherent scalability of Transformers to efficiently enhance model capacities and handle more complex tasks with improved performance. Efficent diffusion. Diverse methods have been proposed to tackle the poor real-time performance of diffusion models. These techniques fall into two main categories: reducing the number of sampling steps and decreasing the computational load per denoising step. In the first category, several works utilize distillation (Salimans & Ho, 2022; Luhman & Luhman, 2021) to obtain reduced sampling iterations. Furthermore, this category encompasses advanced techniques such as implicit samplers (Kong & Ping, 2021; Song et al., 2020a; Zhang et al., 2022) and specialized differential equation (DE) solvers. These solvers tackle both stochastic differential equations (SDE) (Song et al., 2020b; Jolicoeur-Martineau et al., 2021) and ordinary differential equations (ODE) (Lu et al., 2022a; Liu et al., 2022; Zhang & Chen, 2022), addressing diverse aspects of diffusion model optimization. In contrast, the second category mainly focuses on model compression. It leverages techniques like pruning (Fang et al., 2023; Zhang et al., 2024; Wang et al., 2024b) and quantization (Shang et al., 2023; Huang et al., 2024; He et al., 2024) to reduce the workload in static way. Additionally, dynamic inference compression is also being explored (Liu et al., 2023; Pan et al., 2024), where different models are employed at varying steps of the process. In this work, we focus on the urgently needed DiT acceleration through feature cache, method distinct from the above-discussed ones. Feature cache. Due to the high similarity between activations (Li et al., 2023b; Wimbauer et al., 2024) across continuous denoising steps in diffusion models, recent studies (Ma et al., 2024c; Wimbauer et al., 2024; Li et al., 2023a) have explored caching these features for reuse in subsequent steps to avoid redundant computations. Notably, their strategies rely heavily on the specialized structure"
        },
        {
            "title": "Preprint",
            "content": "of U-Net, e.g., up-sampling blocks 1 or SpatialTransformer blocks 2. Besides, FORA (Selvaraju et al., 2024) and -DiT (Chen et al., 2024b) further apply the feature cache mechanism to DiT. However, both methods select the cache position and lifespan in handcrafted way. Learningto-Cache (Ma et al., 2024b) introduces learnable cache scheme but fails to harmonize training and inference. In this work, we design new training framework, to alleviate the discrepancies between the training and inference, which further enhances the performance and acceleration ratio for DiT."
        },
        {
            "title": "3 PRELIMILARIES",
            "content": "Cache granularity. The noise estimation network of DiT (Peebles & Xie, 2023) is built on the Transformer block (Vaswani, 2017), which is composed of an Attention block and feed-forward network (FFN). Each Attention block and FFN is wrapped up in residual connection (He et al., 2016). For convenience, we sequentially denote these Attention blocks and FFNs without residual connections as {b0, b1, . . . bN 1}, where is their total amount. Following Ma et al. (2024b), we store the output of bi in cache as ci. The cache, once completely filled, is represented as follows: cache = [c0, c1, . . . , cN 1]. (1) Cache router. The cache scheme for DiT can be formulated with pre-defined threshold τ (0 τ < 1) and customized router matrix: Router = [rt,i]1tT,0iN 1 RT , (2) where 0 < rt,i 1 and is the maximum denoising step. At timestep during inference, the residual corresponding to bi is fused with oi defined as follows: (cid:26)bi(hi, cs), rt,i > τ rt,i τ oi = ci, (3) , where hi is the image feature and cs represents the conditional inputs 3. Specifically, rt,i > τ indicates computing bi(hi, cs) as oi. This computed output also replaces ci in the cache. Otherwise, the model loads ci from cache without computation. Here we present naive example of the cache scheme as depicted in Fig. 2. To be noted, RouterT,: is set to [1]1N by default to pre-fill the empty cache. Cache usage ratio (CUR). In addition, we define cache usage ratio (CUR) formulated as (cid:80)t=T t=1 in this paper to represent the reduced computation by reusing cached features. Irt,iτ (cid:80)N 1 i=0 For instance, CUR is roughly equal to 33.33% in Fig. 2. Figure 2: Generation process from random Gaussian noise x4 to an image x0 using feature cache (T = 4, = 3). We omit the sampler (Ho et al., 2020; Song et al., 2020a) and conditional inputs. 1https://github.com/CompVis/stable-diffusion/blob/main/ldm/modules/ diffusionmodules/openaimodel.py#L626 2https://github.com/CompVis/stable-diffusion/blob/main/ldm/modules/ attention.py#L218 3For example, cs represents the time condition and textual condition for text-to-image (T2I) generation."
        },
        {
            "title": "4 HARMONICA",
            "content": "In this section, we first observe that the existing learning-based feature cache strategy shows discrepancies between the training and inference (Sec. 4.1). Then, we propose framework named HarmoniCa to Harmonize them for better feature Cache (Sec. 4.2). Finally, our HarmoniCa shows higher efficiency and better applicability than the previous training-based method (Sec. 4.3)."
        },
        {
            "title": "4.1 DISCREPANCY BETWEEN TRAINING AND INFERENCE",
            "content": "Revealing previous approaches for DiT, most of them (Selvaraju et al., 2024; Chen et al., 2024b) manually set the value of the Router in heuristic way. To be adaptive, Learning-to-Cache (Ma et al., 2024b) employs learnable Router 4. However, we have identified two discrepancies between its training and inference phases in the following. Prior timestep disregard. As illustrated in Fig. 2, the inference process employing feature cache at timestep is subject to the prior timesteps. For example, at timestep = 1, the input x1 has the error induced by reusing the cached features c0 and c1 at preceding timestep = 2. Furthermore, reusing and updating features at earlier timesteps also shape the contents of the current cache. However, Learning-to-Cache is unaffected by prior denoising steps during training. Specifically, for each training iteration, as depicted in Fig. 3 (a), it first uniformly samples timestep akin to DDPM (Ho et al., 2020). It then pre-fills an empty cache at and proceeds to train Routert1,: at subsequent timestep 1, without being influenced by the feature cache mechanism from timestep to + 1. Training paradigm of Figure 3: Learning-to-Cache. L(t) LT denotes the loss function. In each iteration, this method manually adds noise to images to obtain xt as the input of DiT at t. in DiT () represents the current timestep. Objective mismatch. Moreover, we also find that Learning-to-Cache (Ma et al., 2024b) solely focuses on the predicted noise at each denoising step during training. It leverages the following learning objective at timestep t: LT = L(t) L(t) SE + β (cid:80)N 1 i=0 rt,i, (4) where β is coefficient for the regularization term of the Routert: and L(t) SE represents the Mean Square Error (MSE) between the predicted noise of the DiT with and without reusing cached features at t. In contrast, the target during inference is to generate the high-quality image x0, which also leads to discrepancy of objective."
        },
        {
            "title": "4.2 HARMONIZING TRAINING AND INFERENCE",
            "content": "Existing studies (Ning et al., 2023; Li et al., 2024; Ning et al., 2024) on diffusion models show that discrepancies between training and inference phases can lead to error accumulation (Arora et al., 2022; Schmidt, 2019) and results in performance degradation. Therefore, we Harmonize training and inference with new learning-based Caching framework called HarmoniCa. It is composed of the following two techniques to alleviate the discrepancies mentioned above. Detailed algorithms of HarmoniCa can be found in Sec. A. Step-wise denoising training. To mitigate the first discrepancy, as shown in Fig. 4 (a), we propose new training paradigm named Step-Wise Denoising Traning (SDT), which completes the entire denoising process over timesteps, thereby accounting for the cache usage and update from all prior timesteps. Specifically, at timestep , we randomly sample Gaussian noise xT and perform single denoising step to pre-fill the cache. Over the following 1 timesteps, The student model, which employs the feature cache mechanism, gradually removes noise to generate an image. 4rt,i in the Router is learnable parameter."
        },
        {
            "title": "Preprint",
            "content": "Concurrently, the teacher model executes the same task without utilizing the cache. Requiring the student to mimic the output representation of its teacher, we compute the loss function and perform back-propagation to update Routert,: at each timestep t. To ensure that each rt,i is differentiable during training, distinct from Eq. (3), we proportionally combine the directly computed feature with the cached one to obtain oi following Ma et al. (2024b): oi = rt,ibi(hi, cs) + (1 rt,i)ci. (5) Similar to inference, we also update ci in the cache with bi(hi, cs) when rt,i > τ . To improve training stability (Wimbauer et al., 2024), we fetch the output from the student as the input to the teacher for the next iteration. We repeat the above learning iterations until the end of training. (a) Step-Wise Denoising Training (SDT) mimics the multiFigure 4: Overview of HarmoniCa. timestep inference stage, which integrates the impact of prior timesteps at t. (b) Image-Error ProxyGuided Objective (IEPO) incorporates the final image error into the learning objective by an efficient proxy λ(t), which is updated through gradient-free image generation passes every training iterations. M(t) masks the Router to disable the impact of the cache mechanism at t. denotes the operation of element-wise multiplication. As depicted in Fig. 5, by incorporating prior denoising timesteps during training, SDT significantly reduces error at each timestep and obtains much more accurate image x0, even with lower computation, compared to Learning-to-Cache. Image error proxy-guided objective. For the second discrepancy, straightforward solution to align the target with inference involves using the error of final image x0 caused by cache usage directly with regularization term of Router as our training objective. However, even for DiT-XL/2 256256 (Peebles & Xie, 2023) with small training batch size, this requires approximately 5 GPU memory and 10 time compared to SDT combined with L(t) LT as detailed in Sec. B, making it impractical. Therefore, we have to identify proxy for the error of x0 that can be integrated into the learning objective. Based on the above analysis, we propose an Image Error Proxy-guided Objective (IEPO). It is defined at each timestep as follows: Figure 5: MSE of xt for DiT-XL/2 256 256 (Peebles & Xie, 2023) (T = 20, = 56) induced by different feature cache methods. xt is the noisy image obtained at timestep + 1. LTC denotes Learningto-Cache. For fair comparison, L(t) LT is employed for SDT. We mark the CUR in the brackets. IEP = λ(t)L(t) L(t) SE + β (cid:80)N 1 i=0 rt,i, (6) where λ(t) is our final image error proxy treated as coefficient of L(t) SE. This proxy represents the final image error caused by the cache usage at t. With large λ(t), L(t) SE prioritizes reduction of the output error at t. This tends to decrease the cached feature usage rate at the corresponding timestep, and vice versa. Therefore, our proposed objective considers the trade-off between the error of x0 and the cache usage at certain denoising step."
        },
        {
            "title": "Preprint",
            "content": "Here, we detail the process to obtain λ(t) as follows. For given Router, mask matrix is defined to disable the use of cached features and force updating the entire cache at as: M(t) j,k = (cid:40) 1, 1 rj,k , = = , (7) where (j, k) 5 denotes the index of M(t) RT . As depicted in Fig. 4 (b), x0 and x(t) are 0 final images generated from randomly sampled Gaussian noise xT using feature cache guided by (Upper) Router and (Lower) Router element-wise multiplied by M(t), respectively. Then, we can formulate λ(t) as: λ(t) = x0 x(t) 0 2 , (8) where denotes the Frobenius norm. To adapt to the training dynamics, we periodically update all the coefficients {λ(1), . . . , λ(T )} every iterations 6, instead of employing static ones. (a) DiT w/o feature cache (b) SDT+L(t) LT (1.40) (c) HarmoniCa (1.44) Figure 6: Random samples for DiT-XL/2 256 256 (Peebles & Xie, 2023) w/ and w/o feature cache (T = 20). We mark the speedup ratio in the brackets. Fig. 6 shows that L(t) improves the quality of x0 even at higher speedup ratio than L(t) that employing L(t) IEP helps yield much more accurate objective-level traits and significantly LT C. The study in Sec. justifies LT incurs the optimization deviating from minimizing the error of x0."
        },
        {
            "title": "4.3 EFFICIENCY DISCUSSION",
            "content": "Training efficiency. Our HarmoniCa incurs significantly lower training costs than the previous learning-based method. As shown in Tab. 1, HarmoniCa requires no training images, whereas Learning-to-Cache utilizes original training datasets. Thus, it is challenging to apply Learning-toCache to models like the PIXART-α (Chen et al., 2023) family, which are trained on large datasets, limiting its applicability. Moreover, while dynamic update of λ(t) incurs approximately 10% extra time overhead, HarmoniCa requires only three-quarters of the training hours compared to Learningto-Cache, which needs to pre-fill the cache for each training iteration. Inference efficiency. Fortunately, our method with pre-learned Router has no computational overhead during runtime. Moreover, less than 6% extra memory overhead 7 is induced by cache for DiT-XL/2 256 256 with batch size of 8. Therefore, the introduced inference cost is controlled at small level."
        },
        {
            "title": "5 EXPERIMENTS",
            "content": "Table 1: Training costs of learning-based feature cache methods for DiT-XL/2 256 256 (Peebles & Xie, 2023) (T = 20). We train with all methods for 20K iterations using global batch size 64 on 4 NVIDIA H800 80GB GPUs. For HarmoniCa, we set = 500. As in the original paper, we utilize the full ImageNet training set (Russakovsky et al., 2015) for Learning-to-Cache."
        },
        {
            "title": "Method",
            "content": "#Images Time(h) Memory(GB/GPU) This section begins by outlining the detailed experimental protocols (Sec. 5.1). Subsequently, we provide comprehensive comparisons across different methods to show the superior performance and acceleration ratio of our HarmoniCa (Sec. 5.2). Finally, we provide ablation studies for the key designs of our method (Sec. 5.3). LT HarmoniCa 33.28 1.63 0 1.47 33.28 Learning-to-Cache SDT+L(t) 1.22M 2. 33.33 51 and 0 1. 6C mod = 0. 7The cache occupies 0.49 GB GPU memory and inference without the feature cache mechanism takes"
        },
        {
            "title": "8.18 GB GPU memory.",
            "content": ""
        },
        {
            "title": "Preprint",
            "content": "5."
        },
        {
            "title": "IMPLEMENTATION DETAILS",
            "content": "Models and datasets. We conduct experiments on two different image generation tasks. For classconditional task, we employ DiT-XL/2 (Peebles & Xie, 2023) 256 256 and 512 512 models pre-trained and accessed on ImageNet dataset (Russakovsky et al., 2015). For text-to-image (T2I) task, we utilize PIXART-α (Chen et al., 2023) series, known for its outstanding performance. These models including PIXART-XL/2 at resolutions of 256 256 and 512 512, along with PIXARTXL/2-1024-MS at higher resolution of 1024 1024, are tested on MS-COCO dataset (Lin et al., 2015). We additionally use T5 model (Raffel et al., 2023) as their text encoders. Training settings. Following Ma et al. (2024b), we set the threshold τ as 0.1 for all the models. Each of them is trained for 20K iterations employing the AdamW optimizer (Loshchilov & Hutter, 2019) on 4 NVIDIA H800 80GB GPUs. The learning rate is fixed at 0.01, is set to 500, and global batch sizes of 64, 48, and 32 are utilized for models with increasing resolutions. Additionally, we collect 1000 MS-COCO captions for T2I training. Baselines. For class-conditional experiments, we choose the current state-of-the-art (SOTA) Learning-to-Cache (Ma et al., 2024b) as our baseline. Due to the limits mentioned in Sec. 4.3, we employ FORA (Selvaraju et al., 2024) and -DiT (Chen et al., 2024b), excluding Learning-to-Cache for the T2I task. The results of these methods are obtained either by re-running their open-source code (if available) or by using the data provided in the original papers, all under the same conditions as our experiments. We also report the performance of models with reduced denoising steps. Evaluation. To assess the generation quality, Frechet Inception Distance (FID) (Nash et al., 2021), and sFID (Nash et al., 2021) are applied to all experiments. For DiT/XL-2, we additionally provide Inception Score (IS) (Salimans et al., 2016), Precision, and Recall (Kynkaanniemi et al., 2019) as reference metrics. For PIXART-α, to gauge the compatibility of image-caption pairs, we calculate CLIP score (Hessel et al., 2022) using ViT-B/32 (Dosovitskiy et al., 2020) as the backbone. To evaluate the inference efficiency, we measure the CUR 8 and the inference latency for batch size of 8. In detail, we sample 50K images adopting DDIM (Song et al., 2020a) for DiT-XL/2, and 30K images utilizing IDDPM (Nichol & Dhariwal, 2021), DPM-Solver++ (Lu et al., 2022b), and SA-Solver (Xue et al., 2024) for PIXART-α. All of them use classifier-free guidance (cfg) (Ho & Salimans, 2022). More implementation details can be found in Sec. and the results of PIXART-Σ (Chen et al., 2024a) family are available in Sec. E, including generation with an extremely high-resolution of 2048 2048. In addition, we also present the results of combination with quantization to further accelerate DiT inference in Sec. F."
        },
        {
            "title": "5.2 MAIN RESULTS",
            "content": "Class-conditional generation. We begin our evaluation with DiT-XL/2 on ImageNet and compare it with current SOTA Learning-to-Cache (Ma et al., 2024b) and the approach employing fewer timesteps. The results are presented in Tab. 2, where our HarmoniCa surpasses baseline methods. Notably, with higher speedup ratio for 10-step DiT-XL/2 256 256, HarmoniCa achieves an FID of 13.35 and an IS of 206.57, outperforming Learning-to-Cache by 1.24 and 5.20, respectively. Moreover, the superiority of our HarmoniCa increases as the number of timesteps decreases. We conjecture that it is because the difficulty to learn Router rises as the timestep goes up. Additionally, we further conduct experiments with lower CUR for this task in Sec. H. T2I generation. We also present PixArt-α results in Tab. 3, comparing our HarmoniCa against FORA (Selvaraju et al., 2024) and the method using fewer timesteps. HarmoniCa outperforms these benchmarks across all metrics. For example, with the 20-step DPM-Solver++, PIXART-α 256 256 employing HarmoniCa achieves an FID of 27.61 and speeds up by 1.52, surpassing the non-accelerated models FID of 27.68. In contrast, DPM-Solver++ with 15 steps and FORA only achieves FIDs of 31.68 and 38.20, respectively, with speed increases under 1.32. Notably, HarmoniCa also cuts about 36% off processing time without dropping performance when using the IDDPM sampler, while FORA results in over 20 FID increase and 15.67% CUR decrease. Overall, our method consistently delivers superior performance and speedup improvements across 8Definition can be found in Sec. 3."
        },
        {
            "title": "Preprint",
            "content": "Table 2: Accelerating image generation on ImageNet for the DiT-XL/2. We mark the speedup ratio in the brackets and highlight the best score in bold. Method IS FID sFID Prec. Recall CUR(%) Latency(s) DDIM (Song et al., 2020a) DDIM (Song et al., 2020a) Learning-to-Cache (Ma et al., 2024b) HarmoniCa DDIM (Song et al., 2020a) DDIM (Song et al., 2020a) Learning-to-Cache (Ma et al., 2024b) HarmoniCa DDIM (Song et al., 2020a) DDIM (Song et al., 2020a) Learning-to-Cache (Ma et al., 2024b) HarmoniCa DDIM (Song et al., 2020a) DDIM (Song et al., 2020a) Learning-to-Cache (Ma et al., 2024b) HarmoniCa DiT-XL/2 256 256 (cfg = 1.5) 240.37 237.84 233. 238.74 224.37 201.83 201.37 206.57 159. 140.37 145.09 151.83 2.27 2.37 2. 2.36 3.52 5.77 5.34 4.88 12. 16.54 14.59 13.35 4.25 4.32 4. 4.24 4.96 6.61 6.36 5.91 11. 14.44 11.58 11.13 80.25 80.22 79. 80.57 78.47 75.14 75.04 75.20 67. 62.63 64.03 65.22 59.77 59.31 59. 59.68 58.33 55.08 56.09 58.74 52. 50.08 52.06 52.18 DiT-XL/2 512 512 (cfg = 1.5) 184.47 173. 178.11 179.84 5.10 6.47 6.24 5. 5.79 6.67 7.01 6.61 81.77 81. 81.21 81.33 54.50 51.30 53.30 55. 50 39 50 50 20 20 20 10 9 10 20 16 20 20 - - 23.39 23.68 - - 35.60 37. - - 19.11 22.86 - - 23.57 25.98 1.767 1.379(1.28) 1.419(1.25) 1.361(1.30) 0.658 0.466(1.41) 0.468(1.41) 0.456(1.44) 0.332 0.299(1.11) 0.279(1.19) 0.270(1.23) 3.356 2.688(1.25) 2.633(1.28) 2.574(1.30) different resolutions and samplers, demonstrating its efficacy. HarmoniCa also significantly outperforms -DiT (Chen et al., 2024b), which can be found in Sec. I. Table 3: Accelerating image generation on MS-COCO for the PIXART-α."
        },
        {
            "title": "Method",
            "content": "T CLIP FID sFID CUR(%) Latency(s) PIXART-α 256 256 (cfg = 4.5) DPM-Solver++ (Lu et al., 2022b) DPM-Solver++ (Lu et al., 2022b) FORA (Selvaraju et al., 2024)"
        },
        {
            "title": "HarmoniCa",
            "content": "IDDPM (Nichol & Dhariwal, 2021) IDDPM (Nichol & Dhariwal, 2021) FORA (Selvaraju et al., 2024)"
        },
        {
            "title": "HarmoniCa",
            "content": "SA-Solver (Xue et al., 2024) SA-Solver (Xue et al., 2024)"
        },
        {
            "title": "HarmoniCa",
            "content": "20 15 20 20 100 100 100 25 20 25 30. 30.77 - 30.93 31.25 31.25 - 31.23 31.31 31.28 31.29 27.68 31. 38.20 27.61 24.15 24.17 55.30 23. 23.76 23.96 23.85 36.39 38.92 - 37.48 33.65 33.73 - 32.49 34. 35.63 35.56 PIXART-α 512 512 (cfg = 4.5) DPM-Solver++ (Lu et al., 2022b) DPM-Solver++ (Lu et al., 2022b)"
        },
        {
            "title": "HarmoniCa",
            "content": "20 15 20 31.30 31.29 31. 23.96 25.12 24.99 40.34 40.37 40. PIXART-α 1024 1024 (cfg = 4.5) DPM-Solver++ (Lu et al., 2022b) DPM-Solver++ (Lu et al., 2022b)"
        },
        {
            "title": "HarmoniCa",
            "content": "20 15 20 31.10 31.07 31. 25.01 25.77 24.76 37.80 42.50 41. - - 50.00 65.02 - - 50.00 65.67 - - 54.31 - - 55.01 - - 59.65 0.553 0.418(1.32) 0.424(1.30) 0.364(1.52) 2.572 1.868(1.37) 1.889(1.36) 1.641(1.56) 0.891 0.677(1.31) 0.665(1.34) 1.759 1.291(1.36) 1.168(1.51) 9.470 7.141(1.32) 6.289 (1.51)"
        },
        {
            "title": "5.3 ABLATION STUDIES",
            "content": "In this subsection, we employ 20-step DDIM (Song et al., 2020a) sampler for DiT-XL/2 256 256 and settings in Sec. 5.1 without special claim. Effect of different components. To show the effectiveness of components involved in HarmoniCa, we apply different combinations of training techniques and show the results in Tab. 4. For the training paradigm, equipped with L(t) LT C, our SDT significantly decreases FID by 10 compared to that of Learning-to-Cache. For the learning objective, our IEPO achieves nearly 40 IS improvement and"
        },
        {
            "title": "Preprint",
            "content": "Table 4: Ablation results of different components. The first row denotes the model w/o feature cache. The second and last rows denote Learning-to-Cache and HarmoniCa, respectively. Training Paradigm Learning Objective Learning-to-Cache SDT L(t) LT L(t) IEP IS FID sFID CUR(%) Latency(s) 224.37 115. 203.41 166.65 206.67 3.52 4.96 - 18.57 16.18 5.20 8.01 4.88 6. 7.62 5.91 32.68 36.70 34.20 37. 0.658 0.483(1.36) 0.458(1.44) 0.471(1.40) 0.456(1.44)"
        },
        {
            "title": "3.13 FID reduction for SDT compared with L(t)\nLT C. Moreover, both SDT and IEPO can help signifi-\ncantly enhance performance for the counterparts in the table. For a fair comparison, we modify the\nimplementation of Learning-to-Cache to train the entire Router in Tab. 4. A detailed discussion\nof this can be found in Sec. J.",
            "content": "Effect of iteration interval C. As illustrated in Fig. 7, we carry out experiments to evaluate the impact of varying values on updating λ(t) in Eq. (8). Despite similar speedup ratios, using an extreme value leads to notable performance degradation. Specifically, large means the proxy λ(t) fails to accurately and timely reflect the cache mechanisms effect on the final image. Conversely, small results in overly frequent updates, complicating training convergence. Hence, we choose moderate value of 500 as in this paper based on its superior performance, as demonstrated in the figure. Figure 7: Ablation results of iteration interval C. denotes the model employing L(t) LT as its loss function. Effect of coefficient β. We also explore the tradeoff between inference speed and performance for different values of β in Eq. (6). As shown in Fig. 8, higher β leads to greater acceleration but at the cost of more pronounced performance degradation, and vice versa. Notably, performance declines gradually when β 8e8 and more sharply outside this range. This observation suggests the potential for autonomously finding an optimal β to balance speed and performance, which we aim to address in future research. Figure 8: Ablation results of coefficient β in Eq. (6). denotes the model w/o feature cache. Effect of different metrics for λ(t). In Tab. 5, we conduct experiments to explore the effect of λ(t) with different metrics. Both 2 and DKL() lead to notable performance enhancements compared to using only the output error (i.e., λ(t) = 1) at each time step. Due to the insensitivity to outliers, (cid:80) is generally less effective for image reconstruction and inferior to the others in Tab. 5. Table 5: Ablation results of different metrics for λ(t). The first and second columns represent the model w/o feature cache and SDD+L(t) LT C, respectively. DKL() denotes KullbackLeibler (KL) divergence. λ(t) IS FID sFID + (cid:80) x0 x(t) 0 x0 x(t) 0 2 DKL(x0, x(t) 0 ) 224.37 166. 172.08 3.52 4.96 8.01 7.62 6. 7.79 206.57 4.88 5.91 205.91 5. 5.51 CUR(%) - Latency(s) 0.658 34.20 0.471(1.40) 34.82 0.470(1.40) 37.50 0.456(1.44) 36.79 0.458(1.44)"
        },
        {
            "title": "6 CONCLUSION",
            "content": "In this research, we focus on accelerating Diffusion Transformers (DiTs) through the cache mechanism in learning-based way. We first identify two discrepancies between training and inference of the previous method: (1) Prior Timestep Disregard in which earlier step influences are neglected, leading to inconsistency with inference, and (2) Objective Mismatch, where training focuses on"
        },
        {
            "title": "Preprint",
            "content": "intermediate results, misaligning with the final image quality target. To alleviate these discrepancies, we Harmonize training and inference by introducing novel feature Cache framework dubbed HarmoniCa, which consists of the Step-wise Denoising Training (SDT) and the Image Error-Aware Optimization Objective (IEPO). SDT captures the influence of all timesteps during training, closing the gap with the inference stage, while IEPO introduces an efficient proxy for final image error, ensuring that optimization objectives remain aligned with inference requirements. With the combination of the two components, extensive experiments demonstrate that our framework achieves superior performance and efficiency with significantly lower training costs compared to the existing training-based method."
        },
        {
            "title": "ACKNOWLEDGEMENT",
            "content": "We thank Chengtao Lv and Yuyang Chen for their insights and feedback, and Yifu Ding for her help with the papers diagrams."
        },
        {
            "title": "REFERENCES",
            "content": "Kushal Arora, Layla El Asri, Hareesh Bahuleyan, and Jackie Cheung. Why exposure bias matters: An imitation learning perspective of error accumulation in language generation. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio (eds.), Findings of the Association for Computational Linguistics: ACL 2022, pp. 700710, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.findings-acl.58. URL https://aclanthology. org/2022.findings-acl.58. Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, et al. Pixart-alpha: Fast training of diffusion transformer for photorealistic text-to-image synthesis. arXiv preprint arXiv:2310.00426, 2023. Junsong Chen, Chongjian Ge, Enze Xie, Yue Wu, Lewei Yao, Xiaozhe Ren, Zhongdao Wang, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-sigma: Weak-to-strong training of diffusion transformer for 4k text-to-image generation. arXiv preprint arXiv:2403.04692, 2024a. Pengtao Chen, Mingzhu Shen, Peng Ye, Jianjian Cao, Chongjun Tu, Christos-Savvas Bouganis, Yiren Zhao, and Tao Chen. δ-dit: training-free acceleration method tailored for diffusion transformers, 2024b. URL https://arxiv.org/abs/2406.01125. Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:87808794, 2021. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, 2024. Gongfan Fang, Xinyin Ma, and Xinchao Wang. Structural pruning for diffusion models, 2023. URL https://arxiv.org/abs/2305.10924. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770778, 2016."
        },
        {
            "title": "Preprint",
            "content": "Yefei He, Jing Liu, Weijia Wu, Hong Zhou, and Bohan Zhuang. Efficientdm: Efficient quantizationaware fine-tuning of low-bit diffusion models, 2024. URL https://arxiv.org/abs/ 2310.03270. Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: reference-free evaluation metric for image captioning, 2022. Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium, 2018. URL https://arxiv.org/abs/1706.08500. Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J. Fleet. Video diffusion models, 2022. URL https://arxiv.org/abs/2204.03458. Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models, 2021. URL https: //arxiv.org/abs/2106.09685. Yushi Huang, Ruihao Gong, Jing Liu, Tianlong Chen, and Xianglong Liu. Tfmq-dm: Temporal feature maintenance quantization for diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 73627371, 2024. Alexia Jolicoeur-Martineau, Ke Li, Remi Piche-Taillefer, Tal Kachman, and Ioannis Mitliagkas. Gotta go fast when generating data with score-based models. arXiv preprint arXiv:2105.14080, 2021. Andrew Kerr, Duane Merrill, Julien Demouth, and John Tran. Cutlass: Fast linear algebra in cuda c++. NVIDIA Developer Blog, 2017. Zhifeng Kong and Wei Ping. On fast sampling of diffusion probabilistic models. arXiv preprint arXiv:2106.00132, 2021. Tuomas Kynkaanniemi, Tero Karras, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Improved precision and recall metric for assessing generative models. Advances in neural information processing systems, 32, 2019. Mingxiao Li, Tingyu Qu, Ruicong Yao, Wei Sun, and Marie-Francine Moens. Alleviating exposure bias in diffusion models through sampling with shifted time steps, 2024. URL https: //arxiv.org/abs/2305.15583. Senmao Li, Taihang Hu, Fahad Shahbaz Khan, Linxuan Li, Shiqi Yang, Yaxing Wang, Ming-Ming Cheng, and Jian Yang. Faster diffusion: Rethinking the role of unet encoder in diffusion models. arXiv preprint arXiv:2312.09608, 2023a. Xiuyu Li, Yijiang Liu, Long Lian, Huanrui Yang, Zhen Dong, Daniel Kang, Shanghang Zhang, and Kurt Keutzer. Q-diffusion: Quantizing diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 1753517545, 2023b. Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays, Pietro Perona, Deva Ramanan, C. Lawrence Zitnick, and Piotr Dollar. Microsoft coco: Common objects in context, 2015. URL https://arxiv.org/abs/1405.0312. Enshu Liu, Xuefei Ning, Zinan Lin, Huazhong Yang, and Yu Wang. Oms-dpm: Optimizing the In International Conference on Machine model schedule for diffusion probabilistic models. Learning, pp. 2191521936. PMLR, 2023. Luping Liu, Yi Ren, Zhijie Lin, and Zhou Zhao. Pseudo numerical methods for diffusion models on manifolds. arXiv preprint arXiv:2202.09778, 2022."
        },
        {
            "title": "Preprint",
            "content": "Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization, 2019. URL https: //arxiv.org/abs/1711.05101. Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: fast ode solver for diffusion probabilistic model sampling in around 10 steps. Advances in Neural Information Processing Systems, 35:57755787, 2022a. Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver++: Fast solver for guided sampling of diffusion probabilistic models. arXiv preprint arXiv:2211.01095, 2022b. Eric Luhman and Troy Luhman. Knowledge distillation in iterative generative models for improved sampling speed. arXiv preprint arXiv:2101.02388, 2021. Xin Ma, Yaohui Wang, Gengyun Jia, Xinyuan Chen, Ziwei Liu, Yuan-Fang Li, Cunjian Chen, arXiv preprint Latte: Latent diffusion transformer for video generation. and Yu Qiao. arXiv:2401.03048, 2024a. Xinyin Ma, Gongfan Fang, Michael Bi Mi, and Xinchao Wang. Learning-to-cache: Accelerating diffusion transformer via layer caching, 2024b. URL https://arxiv.org/abs/2406. 01733. Xinyin Ma, Gongfan Fang, and Xinchao Wang. Deepcache: Accelerating diffusion models for free. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1576215772, 2024c. Markus Nagel, Marios Fournarakis, Rana Ali Amjad, Yelysei Bondarenko, Mart van Baalen, and Tijmen Blankevoort. white paper on neural network quantization, 2021. URL https:// arxiv.org/abs/2106.08295. Charlie Nash, Jacob Menick, Sander Dieleman, and Peter Battaglia. Generating images with sparse representations. arXiv preprint arXiv:2103.03841, 2021. Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. In International conference on machine learning, pp. 81628171. PMLR, 2021. Mang Ning, Enver Sangineto, Angelo Porrello, Simone Calderara, and Rita Cucchiara. Input perturbation reduces exposure bias in diffusion models, 2023. URL https://arxiv.org/abs/ 2301.11706. Mang Ning, Mingxiao Li, Jianlin Su, Albert Ali Salah, and Itir Onal Ertugrul. Elucidating the exposure bias in diffusion models, 2024. URL https://arxiv.org/abs/2308.15321. Zizheng Pan, Bohan Zhuang, De-An Huang, Weili Nie, Zhiding Yu, Chaowei Xiao, Jianfei Cai, and Anima Anandkumar. T-stitch: Accelerating sampling in pre-trained diffusion models with trajectory stitching. arXiv preprint arXiv:2402.14167, 2024. William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 41954205, 2023. Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis, 2023. URL https://arxiv.org/abs/2307.01952. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with unified text-to-text transformer, 2023. URL https://arxiv.org/abs/1910.10683. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. Highresolution image synthesis with latent diffusion models. In CVPR, 2022. Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation, 2015."
        },
        {
            "title": "Preprint",
            "content": "Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. International journal of computer vision, 115:211252, 2015. Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems, 35:3647936494, 2022. Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. arXiv preprint arXiv:2202.00512, 2022. Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. Advances in neural information processing systems, 29, 2016. Florian Schmidt. Generalization in generation: closer look at exposure bias. In Alexandra Birch, Andrew Finch, Hiroaki Hayashi, Ioannis Konstas, Thang Luong, Graham Neubig, Yusuke Oda, and Katsuhito Sudoh (eds.), Proceedings of the 3rd Workshop on Neural Generation and Translation, pp. 157167, Hong Kong, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-5616. URL https://aclanthology.org/D19-5616. Pratheba Selvaraju, Tianyu Ding, Tianyi Chen, Ilya Zharkov, and Luming Liang. Fora: Fast-forward caching in diffusion transformer acceleration. arXiv preprint arXiv:2407.01425, 2024. Yuzhang Shang, Zhihang Yuan, Bin Xie, Bingzhe Wu, and Yan Yan. Post-training quantization on diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 19721981, 2023. Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020a. Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020b. Vaswani. Attention is all you need. Advances in Neural Information Processing Systems, 2017. Hongyu Wang, Shuming Ma, Li Dong, Shaohan Huang, Dongdong Zhang, and Furu Wei. Deepnet: Scaling transformers to 1,000 layers. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024a. Kafeng Wang, Jianfei Chen, He Li, Zhenpeng Mi, and Jun Zhu. Sparsedm: Toward sparse efficient diffusion models. arXiv preprint arXiv:2404.10445, 2024b. Felix Wimbauer, Bichen Wu, Edgar Schoenfeld, Xiaoliang Dai, Ji Hou, Zijian He, Artsiom Sanakoyeu, Peizhao Zhang, Sam Tsai, Jonas Kohler, et al. Cache me if you can: AcceleratIn Proceedings of the IEEE/CVF Conference on ing diffusion models through block caching. Computer Vision and Pattern Recognition, pp. 62116220, 2024. Shuchen Xue, Mingyang Yi, Weijian Luo, Shifeng Zhang, Jiacheng Sun, Zhenguo Li, and Zhi-Ming Ma. Sa-solver: Stochastic adams solver for fast sampling of diffusion models. Advances in Neural Information Processing Systems, 36, 2024. Zhihang Yuan, Pu Lu, Hanling Zhang, Xuefei Ning, Linfeng Zhang, Tianchen Zhao, Shengen Yan, Guohao Dai, and Yu Wang. Ditfastattn: Attention compression for diffusion transformer models. arXiv preprint arXiv:2406.08552, 2024. Dingkun Zhang, Sijia Li, Chen Chen, Qingsong Xie, and Haonan Lu. Laptop-diff: Layer pruning and normalized distillation for compressing diffusion models. arXiv preprint arXiv:2404.11098, 2024. Qinsheng Zhang and Yongxin Chen. Fast sampling of diffusion models with exponential integrator. arXiv preprint arXiv:2204.13902, 2022."
        },
        {
            "title": "Preprint",
            "content": "Qinsheng Zhang, Molei Tao, and Yongxin Chen. gddim: Generalized denoising diffusion implicit models. arXiv preprint arXiv:2206.05564, 2022."
        },
        {
            "title": "A ALOGRITHM OF HARMONICA",
            "content": "As described in Alg. 1, we provide detailed algorithm of our HarmoniCa. For clarity, we omit the pre-fill stage (i.e., denoising at ), where RouterT : is forced to be set to {1}1N . The conds for T2I tasks and class-conditional generation are pre-prepared text prompts and class labels, respectively. Algorithm 1 HarmoniCa: the upper snippet describes the full procedure, and the lower side contains the subroutine for computing the proxy of the final image error. func HARMONICA(ϕ, ϵθ, iters, conds, τ, β, T, C) Require: ϕ() diffusion sampler ϵθ() DiT model iters amount of training iterations conds conditional inputs τ threshold β constraint coefficient maximum denoising step iteration interval {λ(1), . . . , λ(T )} = gen proxy(ϕ, ϵθ, xT , conds[i], τ, Router) = 0 then 1 do: end if for in to 1 do: 1: Initialize Router with normal distribution 2: cache = 3: for in 0 to iters xT (0, I) 4: if i% 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: end for 16: return Router ϵ(t) ϵ(t) = ϵθ(xt, t, conds[i]) IEP = λ(t)ϵ(t) L(t) Tune Routert,: by back-propagation xt1 = ϕ(xt, t, ϵ(t) ϵ(t)2 end for ) = ϵθ(xt, t, conds[i], Routert,:, τ, cache) + β (cid:80)N 1 i=0 r(t) func gen proxy(ϕ, ϵθ, xT , cond, τ, Router) 1: cache = 2: Employ feature cache guided by Router to generate x0 3: for in to 1 do: Generate M(t) 4: Employ feature cache guided by Router M(t) to generate x(t) 5: 0 λ(t) = x0 x(t) 6: 7: end for 8: return {λ(1), λ(2), . . . , λ(T )} 0 Initialize cache Fig. 2 Eq. (6) Initialize cache Eq. (7) Eq. (8)"
        },
        {
            "title": "OBJECTIVE",
            "content": "In Tab. A, SDT+L(t) x0 requires 1 additional denoising passes per training iteration at to compute the error of x0. Consequently, this approach consumes about 9.73 GPU hours compared to SDT+L(t) LT C. Due to the extensive intermediate activations stored from timestep to 1 for backpropagation, it also costs 4.90 GPU memory. This estimation is conducted with small batch sizes and limited iterations. Therefore, SDT+L(t) x0 is less feasible for models with larger latent spaces or higher token counts per image, such as DiT-XL/2 512 512, particularly in large-batch, complete training scenarios. Additionally, the network effectively becomes stacked Transformer"
        },
        {
            "title": "Preprint",
            "content": "blocks under this strategy, making it difficult (Wang et al., 2024a) to optimize the Router with even moderate value, such as 50 or 100. Table A: Training costs estimation across different methods for DiT-XL/2 256256 (Peebles & Xie, 2023) (T = 20). We only employ 5K iterations with global batch size of 8 on 4 NVIDIA H800 80G GPUs. L(t) x0 denotes the loss function replacing L(t) SE in Eq. (4) with the final image error."
        },
        {
            "title": "Method",
            "content": "#Images Time(h) Memory(GB/GPU) SDM+L(t) x0 SDM+L(t) LT 0 0 1. 0.15 65.36 13."
        },
        {
            "title": "C OPTIMIZATION DEVIATION",
            "content": "FID: 8.01, CUR(%): 34.20 (a) SDT+L(t) FID: 4.88, CUR(%): 37.50 LT (b) HarmoniCa LT C. Figure A: (Left) Variations of L(t) SE and λ(t) for SDT+L(t) (Right) Router visualization across different methods. The gray grid (t, i) represents using the feature in cache at without computing oi. The white grid indicates computing and updating cache. We also mark their FID (Heusel et al., 2018) and CUR. All the above experiments employ DiT-XL/2 256 256 (T = 20, = 56). To generate high-quality x0 and accelerate the inference phase, we believe only considering the output error at certain timestep can cause deviated optimization due to its gap w.r.t the error of x0. To validate this, we plot the values of L(t) SE in Eq. (4) and λ(t) in Eq. (8) during the training phase of SDT+L(t) SE and λ(t) across different denoising steps, their results present significant discrepancy. For instance, L(t) SE at = 14 is several orders of magnitude smaller than that at = 1 during the entire training process, and the opposite situation happens for λ(t). Intuitively, this indicates that we could increase the cache usage rate at = 1, and vice versa at = 14 for higher performance while keeping the same speedup ratio according to the value of the proxy λ(t). However, only considering the output error at each timestep (i.e., L(t) SE) can optimize towards shifted direction. In practice, the learned Router with the guidance of λ(t) in Fig. (Right) (b) caches less in large timesteps like = 14 and reuses more in small timesteps as = 1 compared to that in Fig. (Right) (a) achieving significant performance enhancement. LT in Fig. (Left). Comparing L(t)"
        },
        {
            "title": "D MORE IMPLEMENTATION DETAILS",
            "content": "In this section, we present more implementation details for our HarmoniCa. First, following Ma et al. (2024b), we also perform sigmoid function 9 to each rt,i before it is passed to the model. 9σ(x) = 1 1+ex"
        },
        {
            "title": "Preprint",
            "content": "Moreover, unless specified otherwise, the hyper-parameter β in Eq. (6) for all experiments is given in Tab. B; any exceptions are noted in the relevant tables. Table B: Hyper-parameter β for training the Router."
        },
        {
            "title": "Model",
            "content": "DiT-XL/2 PIXART-α PIXART-Σ"
        },
        {
            "title": "Resolution",
            "content": "256 256 512 512 256 256 512 512 1024 1024 1024 2048 2048 β 10 20 20 20 100 25 20 7e 8e8 5e8 4e8 1e3 8e4 8e 8e4 20 8e4 20 8e4 8e4 RESULTS FOR PIXART-Σ In this section, we present the results for the PIXART-Σ family, including PIXART-Σ-XL/2-1024MS and PIXART-Σ-XL/2-2K-MS. For the latter one, we test by sampling 10K images. Additionally, we train the Router with batch size of 16 and measure latency using batch size of 1. All other settings are consistent with those described in Sec. 5.1. As shown in Table C, HarmoniCa achieves 1.51 speedup along with improved CLP scores and sFID compared to the non-accelerated model for PIXART-Σ 2048 2048. Notably, this is the first time for the feature cache mechanism to accelerate image generation with such super-high resolution of 2048 2048. Table C: Accelerating image generation on MS-COCO for the PIXART-Σ."
        },
        {
            "title": "Method",
            "content": "T CLIP FID sFID CUR(%) Latency(s) PIXART-Σ 1024 1024 (cfg = 4.5) DPM-Solver++ (Lu et al., 2022b) DPM-Solver++ (Lu et al., 2022b)"
        },
        {
            "title": "HarmoniCa",
            "content": "20 15 20 31.37 31.34 31. 20.98 21.63 20.94 27.47 28.68 27. PIXART-Σ 2048 2048 (cfg = 4.5) DPM-Solver++ (Lu et al., 2022b) DPM-Solver++ (Lu et al., 2022b)"
        },
        {
            "title": "HarmoniCa",
            "content": "20 15 20 31.19 31.26 31. 23.61 24.40 23.88 51.12 53.34 53. - - 59.52 - - 58. 9.467 7.100(1.33) 6.432(1.47) 14.198 9.782(1.45) 9.410(1.51)"
        },
        {
            "title": "F COMBINATION WITH QUANTIZATION",
            "content": "In this section, we conduct experiments to show the high compatibility of our HarmoniCa with the In Tab. D, our method boosts considerable speedup ratio from model quantization technique. 1.18 to 1.77 with only 0.16 FID increase for PIXART-α 256 256. In the future, we will explore combining our HarmoniCa with other acceleration techniques, such as pruning and distillation, to further reduce the computational demands for DiT."
        },
        {
            "title": "G EXPERIMENTAL DETAILS FOR QUANTIZATION",
            "content": "In Sec. F, we employ 8-bit channel-wise weight quantization and 8-bit layer-wise activation quantization for full-precision (FP32) DiT-XL/2 and half-precision (FP16) PIXART-α. The former uses 20-step DDIM sampler (Song et al., 2020a), while the latter employs DPM-Solver++ sampler (Lu et al., 2022b) with the same steps. More specifically, we use MSE initialization (Nagel et al., 2021) for quantization parameters. For the quantization-aware fine-tuning stage, we set the learning rate of LoRA (Hu et al., 2021) and activation quantization parameters to 1e6 and that of weight quantization parameters to 1e5, respectively. Additionally, we employ 3.2K iterations for DiT-XL/2 (Peebles & Xie, 2023) and 9.6K iterations for PIXART-α (Chen et al., 2023) on single NVIDIA H800 80G GPU. Other settings are the same as those from the original paper (He et al., 2024). Leveraging NVIDIA CUTLASS (Kerr et al., 2017) implementation, we evaluate the latency of quantized models employing the 8-bit multiplication for all the linear layers and convolutions."
        },
        {
            "title": "Preprint",
            "content": "Table D: Results of the combination of our framework and an advanced quantization method: EfficientDM (He et al., 2024). IS is for the former and CLIP is for the latter in the table. Experimental details for quantization can be found in Sec. G. We mark the speedup ratio and the compression ratio in the brackets."
        },
        {
            "title": "Method",
            "content": "IS/CLIP FID sFID CUR(%) Latency(s) #Size(GB) DiT-XL/2 256 256 (cfg = 1.5) EfficientDM (He et al., 2024) +HarmoniCa (β = 4e8) 172.70 168.16 6.10 6. 4.55 4.32 - 26.25 0.591(1.11) 0.473(1.40) 0.64(3.93) 0.64(3.93) PIXART-α 256 256 (cfg = 4.5) EfficientDM (He et al., 2024) +HarmoniCa 30.09 30.23 34. 35.00 30.34 31.38 - 53.34 0.469(1.18) 0.301(1.77) 0.59(1.98) 0.59(1.98) PIXART-α 512 512 (cfg = 4.5) EfficientDM (He et al., 2024) +HarmoniCa 30.71 30. 25.82 26.90 41.64 42.82 - 54. 0.461(1.20) 0.296(1.80) 0.59(1.98) 0.59(1.98) COMPARISON BETWEEN LEARNING-TO-CACHE AND HARMONICA WITH LOW CUR(%) In this section, we compare HarmoniCa with Learning-to-Cache (Ma et al., 2024b) at relatively low CUR(%). As shown in Tab. E, both methods achieve similar speedup ratio and even better performance than non-accelerated models. Therefore, we employ higher CUR in Tab. 2 to show our pronounced superiority. Table E: Comparison results between Learning-to-Cache and HarmoniCa for the DiT-XL/2 with low CUR(%)."
        },
        {
            "title": "Method",
            "content": "T IS FID sFID Prec. Recall CUR(%) Latency(s) DDIM (Song et al., 2020a) DDIM (Song et al., 2020a) Learning-to-Cache (Ma et al., 2024b) HarmoniCa (β = 3e8) DDIM (Song et al., 2020a) DDIM (Song et al., 2020a) Learning-to-Cache (Ma et al., 2024b) HarmoniCa (β = 2e8) DiT-XL/2 256 256 (cfg = 1.5) 224.37 214.77 228. 228.79 3.52 4.17 3.49 3.51 4. 5.54 4.66 4.76 78.47 77.43 79. 79.43 DiT-XL/2 512 512 (cfg = 1.5) 184.47 180.06 183.57 183. 5.10 5.62 5.45 5.32 5.79 6. 6.05 5.84 81.77 81.37 82.10 81. 20 15 20 20 20 20 20 58.33 56.30 59.10 59. 54.50 53.90 54.90 55.80 - - 22.05 21.07 - - 14.64 16. 0.658 0.564(1.17) 0.545(1.21) 0.547(1.20) 3.356 3.021(1.11) 2.927(1.15) 2.863(1.17) COMPARISON BETWEEN -DIT AND HARMONICA In this section, we compare HarmoniCa with -DiT (Chen et al., 2024b). Given that the code and implementation details of -DiT 10 are not open source, we report results derived from the original paper. Additionally, we evaluate performance sampling 5000 images as used in that study. As depicted in Tab F, our framework further decreases 20% latency and gains 3.52 IS improvement compared with -DiT for PIXART-α with 20-step DPM-Solver++ sampler (Lu et al., 2022b). 10-DiT presents the speedup ratio based on multiply-accumulate operates (MACs). Here we report the results according to the latency in that study."
        },
        {
            "title": "Preprint",
            "content": "Table F: Comparison results between -DiT and HarmoniCa on on MS-COCO for PIXART-α 1024 1024."
        },
        {
            "title": "Method",
            "content": "T CLIP FID IS CUR(%) Speedup PIXART-α 1024 1024 (cfg = 4.5) DPM-Solver++ (Lu et al., 2022b) DPM-Solver++ (Lu et al., 2022b) -DiT (Chen et al., 2024b) HarmoniCa (β = 1e3) 20 20 20 31.07 31.04 30.40 31. 31.98 33.29 35.88 32.97 41.30 39. 32.22 40.67 - - 37.49 62. - 1.54 1.49 1.63 COMPARISON BETWEEN LEARNING-TO-CACHE WITH DIFFERENT"
        },
        {
            "title": "SAMPLING STRATEGIES",
            "content": "For the implementation details 11, Learning-to-Cache uniformly samples an even timestep during each training iteration 12, as opposed to sampling any timestep from the set {1, . . . , } as mentioned in Alg. 1 of its original paper. Consequently, according to Fig. 3, only rt,i, where is an odd timestep, is learnable, while the remaining values are set to one. We compare Learning-to-Cache under different sampling strategies (i.e., sampling an even timestep or without this constraint for each training iteration) against HarmoniCa. As shown in Tab. G, our frameworkwhether training the entire Router or only parts of it (similar to the Learning-to-Cache implementation)consistently outperforms Learning-to-Cache regardless of the sampling strategy. It should be noted that the experiments in Sec. 5, with the exception of those in Tab. 4, use an implementation that uniformly samples an even timestep during each training iteration. This approach achieves significantly higher performance compared to sampling without constraints. Table G: Comparison results between Learning-to-Cache with different sampling strategies and HarmoniCa for the DiT-XL/2 256 256. denotes that only parts of the Router corresponding to odd timesteps are learnable and the remaining values are set to one (i.e., disable reusing cached features). Method IS FID sFID Prec. Recall CUR(%) Latency(s) DiT-XL/2 256 256 (cfg = 1.5) DDIM (Song et al., 2020a) Learning-to-Cache (Ma et al., 2024b) Learning-to-Cache (Ma et al., 2024b) HarmoniCa (β = 3.5e8) HarmoniCa 20 20 20 20 224.37 115.00 201.37 205. 206.57 3.52 4.96 18.57 16.18 5. 4.86 4.88 6.36 5.92 5.91 78. 60.35 75.04 75.06 75.20 58.33 62. 56.09 57.97 58.74 - 32.68 35. 36.07 37.50 0.658 0.483(1.36) 0.468(1.41) 0.463(1.42) 0.456(1.44)"
        },
        {
            "title": "K VISUALIZATION RESULTS",
            "content": "As demonstrated in Figures to H, we present random samples from both the non-accelerated DiT models and ones equipped with HarmoniCa, using fixed random seed. Our approach not only significantly accelerates inference but also produces results that closely resemble those of the original model. For detailed comparison, zoom in to closely examine the relevant images. 11Let be an even number here. 12https://github.com/horseee/learning-to-cache/blob/main/DiT/train_ router.py#L244-L"
        },
        {
            "title": "Preprint",
            "content": "(a) DiT-XL/2 w/o feature cache (b) HarmoniCa (1.44) Figure B: Random samples from (a) non-accelerated and (b) accelerated DiT-XL/2 256 256 (Chen et al., 2023) with 20-step DDIM sampler (Song et al., 2020a). The resolution of each sample is 256 256. We mark the speedup ratio in the brackets. (a) DiT-XL/2 w/o feature cache (b) HarmoniCa (1.30) Figure C: Random samples from (a) non-accelerated and (b) accelerated DiT-XL/2 512 512 (Chen et al., 2023) with 20-step DDIM sampler (Song et al., 2020a). The resolution of each sample is 512 512. (a) PIXART-α w/o feature cache (b) HarmoniCa (1.52) Figure D: Random samples from (a) non-accelerated and (b) accelerated PIXART-α 256256 (Chen et al., 2023) with 20-step DPM-Solver++ sampler (Lu et al., 2022b). The resolution of each sample is 256 256. Text prompts are exhibited above the corresponding images"
        },
        {
            "title": "Preprint",
            "content": "(a) PIXART-α w/o feature cache (b) HarmoniCa (1.51) Figure E: Random samples from (a) non-accelerated and (b) accelerated PIXART-α 512512 (Chen et al., 2023) with 20-step DPM-Solver++ sampler (Lu et al., 2022b). The resolution of each sample is 512 512."
        },
        {
            "title": "Preprint",
            "content": "(a) PIXART-α w/o feature cache (b) HarmoniCa (1.51) Figure F: Random samples from (a) non-accelerated and (b) accelerated PIXART-α 1024 1024 (Chen et al., 2023) with 20-step DPM-Solver++ sampler (Lu et al., 2022b). The resolution of each sample is 1024 1024."
        },
        {
            "title": "Preprint",
            "content": "(a) PIXART-Σ w/o feature cache (b) HarmoniCa (1.47) Figure G: Random samples from (a) non-accelerated and (b) accelerated PIXART-Σ 1024 1024 (Chen et al., 2024a) with 20-step DPM-Solver++ sampler (Lu et al., 2022b). The resolution of each sample is 1024 1024."
        },
        {
            "title": "Preprint",
            "content": "(a) PIXART-Σ w/o feature cache (b) HarmoniCa (1.51) Figure H: Random samples from (Left) non-accelerated and (Right) accelerated PIXART-Σ2K (Chen et al., 2024a) with 20-step DPM-Solver++ sampler (Lu et al., 2022b). The resolution of each sample is 2048 2048."
        }
    ],
    "affiliations": [
        "Beihang University",
        "HKUST",
        "Monash University",
        "SenseTime Research"
    ]
}