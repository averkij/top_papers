{
    "paper_title": "T칖LU 3: Pushing Frontiers in Open Language Model Post-Training",
    "authors": [
        "Nathan Lambert",
        "Jacob Morrison",
        "Valentina Pyatkin",
        "Shengyi Huang",
        "Hamish Ivison",
        "Faeze Brahman",
        "Lester James V. Miranda",
        "Alisa Liu",
        "Nouha Dziri",
        "Shane Lyu",
        "Yuling Gu",
        "Saumya Malik",
        "Victoria Graf",
        "Jena D. Hwang",
        "Jiangjiang Yang",
        "Ronan Le Bras",
        "Oyvind Tafjord",
        "Chris Wilhelm",
        "Luca Soldaini",
        "Noah A. Smith",
        "Yizhong Wang",
        "Pradeep Dasigi",
        "Hannaneh Hajishirzi"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Language model post-training is applied to refine behaviors and unlock new skills across a wide range of recent language models, but open recipes for applying these techniques lag behind proprietary ones. The underlying training data and recipes for post-training are simultaneously the most important pieces of the puzzle and the portion with the least transparency. To bridge this gap, we introduce T\\\"ULU 3, a family of fully-open state-of-the-art post-trained models, alongside its data, code, and training recipes, serving as a comprehensive guide for modern post-training techniques. T\\\"ULU 3, which builds on Llama 3.1 base models, achieves results surpassing the instruct versions of Llama 3.1, Qwen 2.5, Mistral, and even closed models such as GPT-4o-mini and Claude 3.5-Haiku. The training algorithms for our models include supervised finetuning (SFT), Direct Preference Optimization (DPO), and a novel method we call Reinforcement Learning with Verifiable Rewards (RLVR). With T\\\"ULU 3, we introduce a multi-task evaluation scheme for post-training recipes with development and unseen evaluations, standard benchmark implementations, and substantial decontamination of existing open datasets on said benchmarks. We conclude with analysis and discussion of training methods that did not reliably improve performance. In addition to the T\\\"ULU 3 model weights and demo, we release the complete recipe -- including datasets for diverse core skills, a robust toolkit for data curation and evaluation, the training code and infrastructure, and, most importantly, a detailed report for reproducing and further adapting the T\\\"ULU 3 approach to more domains."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 2 2 ] . [ 1 4 2 1 5 1 . 1 1 4 2 : r T칖LU 3: Pushing Frontiers in Open Language Model Post-Training Nathan Lambert 洧눅 Jacob Morrison 洧눅 Valentina Pyatkin 洧눅 洧눚 Shengyi Huang 洧눅 Hamish Ivison 洧눅 洧눚 Faeze Brahman 洧눅 Lester James V. Miranda 洧눅 Alisa Liu洧눚 Nouha Dziri洧눅 Xinxi Lyu洧눅 Yuling Gu洧눅 Saumya Malik洧눅 Victoria Graf 洧눚 Jena D. Hwang洧눅 Jiangjiang Yang洧눅 Ronan Le Bras洧눅 Oyvind Tafjord洧눅 Chris Wilhelm洧눅 Luca Soldaini洧눅 Noah A. Smith洧눅 洧눚 Yizhong Wang洧눅 洧눚 Pradeep Dasigi洧눅 Hannaneh Hajishirzi洧눅 洧눚 洧눅Allen Institute for AI 洧눚University of Washington tulu@allenai.org"
        },
        {
            "title": "Abstract",
            "content": "Language model post-training is applied to refine behaviors and unlock new skills across wide range of recent language models, but open recipes for applying these techniques lag behind proprietary ones. The underlying training data and recipes for post-training are simultaneously the most important pieces of the puzzle and the portion with the least transparency. To bridge this gap, we introduce T칖LU 3, family of fully-open state-of-the-art post-trained models, alongside its data, code, and training recipes, serving as comprehensive guide for modern post-training techniques. T칖LU 3, which builds on Llama 3.1 base models, achieves results surpassing the instruct versions of Llama 3.1, Qwen 2.5, Mistral, and even closed models such as GPT-4o-mini and Claude 3.5-Haiku. The training algorithms for our models include supervised finetuning (SFT), Direct Preference Optimization (DPO), and novel method we call Reinforcement Learning with Verifiable Rewards (RLVR). With T칖LU 3, we build multi-task evaluation scheme for posttraining with development and unseen evaluations, standard benchmark implementations, and substantial decontamination of existing open datasets on said benchmarks. We conclude with analysis and discussion of training methods that did not reliably improve performance. The T칖LU 3 release includes model weights, demo, and the complete recipe datasets for diverse core skills, robust toolkit for data curation and evaluation, the training code and infrastructure, and, most importantly, detailed report for reproducing and further adapting the T칖LU 3 approach to more domains. T칖LU 3 70B T칖LU 3 8B T칖LU 3 DATA T칖LU 3 Code T칖LU 3 EVAL Demo https://hf.co/allenai/Llama-3.1-Tulu-3-70B https://hf.co/allenai/Llama-3.1-Tulu-3-8B https://hf.co/collections/allenai/tulu-3-datasets673b8df14442393f7213f372 https://github.com/allenai/open-instruct https://github.com/allenai/olmes https://playground.allenai.org/ Core contributors."
        },
        {
            "title": "Contents",
            "content": "1 Introduction 2 T칖LU 3 Overview 2.1 T칖LU 3 Data . . 2.2 T칖LU 3 Evaluation . . 2.3 T칖LU 3 Recipe . . . . . . . . . . 2.4 Evaluation and Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 6 7 7 8 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10 . . . . . . . . . . . . 3 T칖LU 3 Data 3.1 Prompt Curation . . . . . . . . 10 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10 3.1.1 3.1.2 Sourcing from Public Datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10 Synthesizing for Target Skills . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.2 Prompt Decontamination . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 4 Supervised Finetuning 4.1 SFT Data . . 4.1.1 13 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 From Prompts to SFT Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.1.2 The T칖LU 3 SFT Mix . 4.2 Key Data Experiments . . 4.3 SFT Recipe and Analyses. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 4.3.1 Key Training Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 4.3.2 Batch Aggregation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 5 Preference Finetuning . 5.1 Background . . Setup . . 5.1.1 5.1.2 . . . . . . . . . . . . . . 19 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 Policy Optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.2 T칖LU 3 Preference Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 From Prompts to Preference Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 . . . . 5.2.1 5.2.2 The T칖LU 3 Preference Mix . 5.3 Key Findings of Data Ablations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 5.4 Preference Tuning Recipe and Analyses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.4.1 Hyperparameter and Algorithm Design . . . . . . . . . . . . . . . . . . . . . . . . . . . 26 5.4.2 Infrastructure for Scaling DPO . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 6 Reinforcement Learning on Verifiable Rewards . 6.1 RLVR Data . . 6.2 RLVR Recipe and Analyses . 28 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6.2.1 Key Findings . . . . 6.3 RLVR Infrastructure . 6.4 Final Experimental Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33 7 T칖LU 3 Evaluation Framework 7.1 Open Language Model Evaluation System (OLMES) 34 . . . . . . . . . . . . . . . . . . . . . . . . 34 2 7.2 T칖LU 3 Evaluation Suite - Development . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7.2.1 Safety Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36 7.3 T칖LU 3 Evaluation Suite - Unseen . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37 7.3.1 New Evaluation: IFEval-OOD . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40 7.3.2 New Evaluation: HREF . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40 7.4 Evaluation on the Unseen Suite . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41 8 Discussions 8.1 Insights from the Unfruitful . 8.2 Future Work . . . . . . . . . . . 42 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 Related Work 43 9.1 The Evolution of Post-training Recipes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43 9.2 Training on Verifiable Rewards . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44 10 Conclusion Additional Hyperparameters 54 Additional Dataset Analyses 54 B.1 Extra Distribution Plots . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54 B.2 Contamination in Public Datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54 B.3 Chat Template Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54 . . . B.4 RLVR IFEval overoptimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54 Supervised Finetuning Data Details . . C.1 Prompts . . . . . . . . . . 55 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55 Preference Tuning Data Details 55 Additional RLVR Details 55 E.1 RM Training Hyperparameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55 Evaluation Details F.1 Detailed Safety Results . F.2 Evaluation principles . . . . . . . . 56 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . F.3 IFEval Out-of-Distribution Constraints . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69 F.4 Subtask-level breakdown of HREF results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71 3 Table 1: Models, datasets, and code released with T칖LU 3. Demo: https://playground.allenai.org/ Model Checkpoints Llama 3.1 8B Llama 3.1 70B meta-llama/Llama-3.1-8B meta-llama/Llama-3.1-70B allenai/Llama-3.1-Tulu-3-8B-SFT allenai/Llama-3.1-Tulu-3-70B-SFT allenai/Llama-3.1-Tulu-3-8B-DPO allenai/Llama-3.1-Tulu-3-8B RM: allenai/Llama-3.1-Tulu-3-8B-RM allenai/Llama-3.1-Tulu-3-70B-DPO allenai/Llama-3.1-Tulu-3-70B Codebases / Tools Link allenai/open-instruct allenai/olmes allenai/open-instruct/tree/main/decontamination Stage Base Model SFT DPO Final Models (RLVR) Type Training T칖LU 3 EVAL Decontamination Preference Data Inference allenai/birr Type Full mix Task Specific Domain General Precise Instruction Following Instruction Datasets Link allenai/tulu-3-sft-mixture allenai/tulu-3-sft-personas-instruction-following Subsets MATH allenai/tulu-3-sft-personas-math Grade School Math Python Code allenai/tulu-3-sft-personas-math-grade allenai/tulu-3-sft-personas-code Model Llama 3.1 70B Llama 3.1 8B Domain Precise Instruction Following General General Domain Full Mix GSM Only MATH Only IFeval Only Preference Mixes Link allenai/llama-3.1-tulu-3-70b-preference-mixture allenai/llama-3.1-tulu-3-8b-preference-mixture Specific Preference Datasets Link allenai/tulu-3-pref-personas-instruction-following allenai/tulu-3-sft-prompts-ultrafeedback allenai/tulu-3-wildchat-ultrafeedback RL with Verifiable Rewards Training Datasets Link allenai/RLVR-GSM-MATH-IF-Mixed-Constraints allenai/RLVR-GSM allenai/RLVR-MATH allenai/RLVR-IFeval 4 Figure 1: An overview of the T칖LU 3 recipe. This includes: data curation targeting general and target capabilities, training strategies and standardized evaluation suite for development and final evaluation stage."
        },
        {
            "title": "Introduction",
            "content": "Just as the camel shares its burdens with others in the caravan, the wise share their insights to lighten the load of ignorance. Proverb generated by T칖LU 3. Post-training the collection of techniques including instruction tuning, reinforcement learning from human feedback, and other types of finetuning has become crucial step in building frontier language models [OpenAI, 2024, Anthropic, 2024], yet developments to these techniques are frequently not accompanied by open resources and recipes. Fully open source counterparts (e.g., T칖LU 2 [Ivison et al., 2023] and Zephyr-洧띻 [Tunstall et al., 2023]) often rely on simpler-to-implement and cheaper pipelines and have become outdated on many metrics. To close the gap between open and closed post training, we introduce T칖LU 3, family of open state-of-the-art post-trained models, alongside all of the data, training recipes, code, infrastructure, and evaluation framework. Integrating partial details from proprietary methods with novel techniques and established academic research, T칖LU 3 pushes the boundaries of research in post-training. The advancements of T칖LU 3 are attributed to T칖LU 3 DATA, new permissively licensed training datasets targeting core skills, T칖LU 3 EVAL, evaluation suite and tools to establish clear performance goals and guide improvement through training stages, and T칖LU 3 RECIPE, an advanced multi-stage training pipeline incorporating new algorithmic advancements in reinforcement learning, cutting-edge infrastructure, and rigorous experimentation to optimize data mixes, methods, and parameters across various training stages. In order to build T칖LU 3, we identify set of core skills to improve after training (e.g., reasoning, math, coding, safety, precise instruction following, knowledge recall, etc.) and build an evaluation framework to establish clear performance goals and guide model improvement over selection of development and unseen tasks. T칖LU 3 benefits significantly from leveraging publicly available open data, generating diverse, skill-specific synthetic data at various training stages, and aggressively decontaminating them against our evaluation suite. The T칖LU 3 training recipe involves multiple stages, with each stage building upon the previous model and focusing on different types of data namely, prompt-completion instances for supervised finetuning, preferences for preference tuning, or verifiable rewards for reinforcement learning. Our methodology facilitates identifying skill deficiencies and refining the data mix, methods and parameters, ensuring balanced performance of core skills across the training process. Through rigorous, principled experimentation, we determine the best data mix for supervised finetuning, resulting in the T칖LU 3 SFT checkpoint. Leveraging recent advances in preference tuning, we then train model over carefully curated on-policy preference data from comparing T칖LU 3 SFT completions against outputs from other language models. Furthermore, we introduce new final finetuning stage Reinforcement Learning with Verifiable Rewards (RLVR) - which employs novel RL objective tailored to enhance specific skills with verifiable answers, such as mathematics and precise instruction following. Our best performing recipe yields T칖LU 3 models that outperform the state-of-the-art post-trained open-weight models of the same size such as Llama 3.1 Instruct [Dubey et al., 2024], Qwen2.5 Instruct [Qwen Team, 2024], or Mistral-Instruct [Mistral AI, 2024], and at the large 70B size T칖LU matches the offerings of closed providers such as Claude 3.5 Haiku and GPT-4o mini. Skill Benchmark(eval) Avg. Knowledge MMLU(0 shot, CoT) PopQA(15 shot) TruthfulQA(6 shot) Math Coding Reasoning BigBenchHard(3 shot, CoT) DROP(3 shot) MATH(4 shot CoT, Flex) GSM8K(8 shot, CoT) HumanEval(pass@10) HumanEval+(pass@10) IFEval(prompt loose) AlpacaEval 2(LC % win) Safety(6 task avg.) IF & chat T칖LU 3 8B 64.8 68.2 29.1 55.0 66.0 62.6 43.7 87.6 83.9 79. 82.4 34.5 85.5 Qwen 2.5 7B Instruct 57.8 76.6 18.1 63.1 21.7 54.4 14.8 83.8 93.1 89.7 74.7 29.0 75.0 Llama 3.1 8B Instruct 62.2 71.2 20.2 55.1 62.8 61.5 42.5 83.4 86.3 82.9 80.6 24.2 75. T칖LU 3 70B 76.0 83.1 46.5 67.6 82.0 74.3 63.0 93.5 92.4 88.0 83.2 49.8 88.3 Qwen 2.5 72B Instruct 71. 85.5 30.6 69.9 67.2 34.2 74.3 89.5 94.0 90.8 87.6 47.7 87.0 Llama 3.1 70B Instruct 73.4 85.3 46.4 66.8 73.8 77.0 56. 93.7 93.6 89.5 88.0 33.4 76.5 GPT-3.5 Turbo GPT-4o Mini 64.7 70.2 45.0 62.9 66.6 70.2 41.2 74.3 87.1 84.0 66.9 38.7 69. 69.6 82.2 39.0 64.8 65.9 36.3 67.9 83.0 90.4 87.0 83.5 49.7 84.9 Claude 3.5 Haiku 75.3 81.8 42.5 64.9 73.7 78.4 68.0 90.1 90.8 88.1 86.3 47.3 Safety Table 2: Overview of results on T칖LU 3 EVAL suite, over both 8B and 70B models. The best performing model for each model size on each benchmark is bolded. T칖LU 3 outperforms the state-of-the-art post-trained open-weight models of the same size and surpass Claude Haiku, GPT-3.5 Turbo, and GPT-4o Mini. indicates scores taken from Claude 3 Model Card and Claude 3.5 Model Card Addendum. indicates score interpolated with Multiple Imputation by Chained Equations (MICE) with context of all other scores in the table, except averages. These scores were either subject to substantial formatting errors in our evaluation suite or not found in other major technical reports. Instruct versions of models shortened to Inst. Closed model versions: GPT-3.5-Turbo-0125, GPT-4o-mini-2024-07-18, Claude 3.5 Haiku 20241022 91. In summary, T칖LU 3 represents family of state-of-the-art open language models, featuring modern post-training framework with fully open-source data T칖LU 3 DATA, evaluation T칖LU 3 EVAL, training code T칖LU 3 CODE and development recipes T칖LU 3 RECIPE. Here are few key contributions from the development of T칖LU: Extensive guidance and tooling for evaluation, decontamination, and recipe design, Scaled, new synthetic instruction datasets, Scaling preference data with on-policy generations, Reinforcement learning with verifiable rewards, an RL-based method that only gets reward if the models completions are verified to be correct, Advanced infrastructure, details, and code to facilitate the successful implementation of large models The result of our work is completely open pipelines for finetuning language models. We release final models trained on Llama 3.1 base versions [Dubey et al., 2024], with intermediate checkpoints, training data, training code, and evaluation code (a full list of artifacts released is available in Table 1). With all the released resources, others can take open base models and finetune them to high-performance on any task of interest laying the foundation of post-training research within complex, multi-objective and multi-stage training regimes."
        },
        {
            "title": "2 T칖LU 3 Overview",
            "content": "Early work in language model post-training followed standard recipe pioneered by models like InstructGPT [Ouyang et al., 2022], consisting of instruction-tuning followed by preference finetuning (PreFT) [Stiennon et al., 2020, Nakano et al., 2021, Askell et al., 2021, Ouyang et al., 2022]. Since then, the sophistication and complexity of post-training approaches have continued to increase, moving towards multiple rounds of training, human data plus synthetic data, and multiple training algorithms and objectives [Touvron et al., 2023, Dubey et al., 2024, Gunter et al., 2024]. However, most successful post-training models offer limited information about their training data, code, or recipes.1 Open post-training research, such as T칖LU 2 [Ivison et al., 2023] and Zephyr-洧띻 [Tunstall et al., 2023], show strong results in some benchmarks and on chat evaluations such as AlpacaEval or Arena-Hard [Li et al., 2024a], but still lag behind in core capabilities such as MATH [Hendrycks et al., 2021], IFEval [Zhou et al., 2023] and GSM8K [Cobbe et al., 2021]. 1On LMSYSs ChatBotArena, no model in the top 50 (as of November 20th, 2024) has released its post-training data [Chiang et al., 2024]. 6 Core Skill Knowledge Reasoning Math Coding Instruction Following (IF) Safety Development MMLU(em) PopQA(EM) TruthfulQA(MC2 em) BigBenchHard(em) DROP(F1) MATH(flex em) GSM8K(em) HumanEval(Pass@10) HumanEval+(Pass@10) IFEval(em) AlpacaEval 2(winrate) T칖LU 3 Safety(avg*) Unseen MMLU-Pro(em) GPQA(em) AGIEval English(em) Deepmind Mathematics(em) BigcodeBench(Pass@10) IFEval-OOD(Pass@1) HREF(winrate) Table 3: T칖LU 3 EVAL consists of development and unseen splits to evaluate core skills. With T칖LU 3 EVAL, we release unified standardized evaluation suite and toolkit to decontaminate training data against benchmarks. The subscript shows the metric we use for evaluation. T칖LU 3 Safety is collection of safety evaluations, see Sec. 7.2.1 for details. T칖LU 3 pushes the boundaries of research in post-training and closes the gap between open and closed finetuning recipes. With T칖LU 3, we hope to uncover which paths for the open-source community will lead to success and which do not (by reporting negative results). It is complex training process that integrates partial details from proprietary methods with novel techniques and established academic research. The key factors in the success of mdelname 3 are rooted in careful data curation, rigorous experimentation and evaluation, innovative methodologies, and improved training infrastructure. We followed systematic guidelines by scientifically evaluating this process through creating development and test sets for evaluation, and conduct careful decontamination of publicly available datasets. T칖LU 3 is not just an artifact, but comprehensive suite of data and tools designed to advance the frontier of open post-training. By openly sharing our data, recipe and findings, we aim to empower the community to explore new and innovative post-training approaches. We list the extensive artifacts and tools released in Table 1. 2.1 T칖LU 3 Data Core Skills The T칖LU 3 effort began with identifying key areas where open post-training recipes often fall behind and that are desirable capabilities for generalist language models. Table 3 outlines the core capabilities we aim to enhance and the evaluation benchmarks selected to cover these skills. With T칖LU 3, we focus on core skills of knowledge recall, reasoning, mathematics, coding, instruction following, general chat, and safety. We curate and collect T칖LU 3 DATA to target these core skills by sourcing from public data and synthetically curating data. We use various data formats at different stages of training. Table 6 outlines the collection of datasets used to train our model, and further details are provided in Section 3. 2.2 T칖LU 3 Evaluation key factor in the success of our post-training approach is establishing clear performance goals and evaluation tools to guide improvement through these stages. With T칖LU 3 EVAL, we release unified standardized evaluation suite and toolkit to guide the development and assessing final models and decontaminating training data against evaluation benchmarks. Our framework consists of an open evaluation toolkit for reproducible evaluations (Section 7.1), suite for evaluating core skills in instruction-tuned models with separate development (Section 7.2) and held-out evaluations (Section 7.3), and set of recommended settings for evaluating on our evaluation suite based on our experiments with various models. Both splits cover all identified skills. Crucially, we did not examine scores on our unseen set 7 Qwen 2.5 7B Instruct 57.8 Magpie 8B Benchmark(eval) T칖LU 3 8B SFT T칖LU 3 8B DPO T칖LU 3 8B 29.3 46.8 76.6 18.1 60.4 65.9 64.8 68.2 29.1 55.0 66. 64.4 68.7 29.3 56.1 65.8 62.5 42.0 84.3 83.9 78.6 81.1 33.5 87.2 Avg. MMLU(0 shot, CoT) PopQA(15 shot) TruthfulQA(6 shot) BigBenchHard(3 shot, CoT) DROP(3 shot) MATH(4 shot CoT, Flex) GSM8K(8 shot, CoT) HumanEval(pass@10) HumanEval+(pass@10) IFEval(prompt loose) AlpacaEval 2(LC % win) Safety(6 task avg.) Table 4: Summary of T칖LU 3 results relative to peer 8B models. The best-performing model on each benchmark (i.e., in each row) is bolded. T칖LU 3-8B significantly outperforms prior state-of-the-art 8B models. Progress across various checkpoints highlight the contribution of each stage of the training in improving core skills. Many of the lowest values, such as in BigBenchHard, are caused by failing to follow the few-shot formatting required for the evaluation or other repetitive errors for more details, see 7. 44.7 62.0 22.5 57.0 0.9 49.4 5.1 61.2 75.4 69.1 38.8 67.9 61.3 31.5 76.2 86.2 81.4 72.8 12.4 93.1 89.7 74.7 29.0 75.0 62.6 43.7 87.6 83.9 79. 63.1 21.7 54.4 14.8 83.8 82.4 34.5 85.5 49.0 46.4 93.1 Gemma 2 9B Instruct 55.2 74.6 28.3 61.4 2.5 58.8 29.8 79.7 71.7 67.0 69.9 43.7 75.5 Ministral 8B Instruct 58.3 68.5 20.2 55.5 56.2 56.2 40.0 80.0 91.0 88.5 56.4 31.4 56. Llama 3.1 8B Instruct 62.2 71.2 20.2 55.1 62.8 61.5 42.5 83.4 86.3 82.9 80.6 24.2 75.2 when developing our models, allowing us to observe how much we may have overfit to particular evaluations in our decisions around data mixtures, algorithms, and hyperparameters. Table 3 summarizes our evaluation suite. We provide further details on our evaluations in Section 7 and in Table 23. We publicly release our evaluation suite at https://github.com/allenai/olmes. 2.3 T칖LU 3 Recipe In this section, we provide an overview of the T칖LU 3 recipe to obtain state-of-the-art post-trained model. We produce T칖LU 3 models through four-stage post-training recipe on top of pretrained language models (see Figure 1). The T칖LU 3 RECIPE is an advanced multi-stage training pipeline incorporating new algorithmic advancements in reinforcement learning, cutting-edge infrastructure, and rigorous experimentation to curate data and optimize data mixes, methods, and parameters across various training stages. Throughout all stages, we measure model performance using carefully-chosen evaluation suite. The stages are as follows: Stage 1: Data Curation (3) We curate variety of prompts to be allocated across multiple stages of optimization. We create new synthetic prompts or, when available, source prompts from existing datasets to target specific capabilities. We ensure prompts are not contaminated with our evaluation suite, T칖LU 3 EVAL. Stage 2: Supervised Finetuning (4) We perform supervised finetuning (SFT) on carefully selected prompts and completions. With thorough experimentation, the final SFT data and training hyperparameters are determined to enhance target core skills without significantly impacting the performance of others, guided by our evaluation framework. Stage 3: Preference Tuning (5) We apply preference tuning, specifically DPO, to newly curated on-policy synthetically created preference data from selected prompts along with off-policy data. As in the SFT stage, we identify the best preference data mix through thorough experimentation, uncovering what formats of data, methods, or hyperparameters lead to improvements. Stage 4: Reinforcement Learning with Verifiable Rewards (6) We introduce new RL-based post-training stage which trains the model on verifiable rewards instead of reward model, as is common for traditional RLHF PPO training. We select tasks with verifiable outcomes, such as mathematical problem-solving, and only provide rewards when the models generations are verified to be correct. We then use RL to train on these rewards. The key contributions of our T칖LU 3 pipeline lie in improved data, methods, infrastructure, and rigorous evaluation. Key elements of our pipeline include: 8 Benchmark(eval) Avg. MMLU(0 shot, CoT) PopQA(15 shot) TruthfulQA(6 shot) BigBenchHard(3 shot, CoT) DROP(3 shot) MATH(4 shot CoT, Flex) GSM8K(8 shot, CoT) HumanEval(pass@10) HumanEval+(pass@10) IFEval(prompt loose) AlpacaEval 2(LC % win) Safety(6 task avg.) T칖LU 3 70B SFT T칖LU 3 70B DPO T칖LU 3 70B 72.6 78.9 48.6 55.7 82.7 77.2 53.7 91.1 92.9 87.3 82.1 26.3 94.4 75.9 83.3 46.3 67.9 81.8 74.1 62.3 93.5 92.4 88.4 82.6 49.6 89. 76.0 83.1 46.5 67.6 82.0 74.3 63.0 93.5 92.4 88.0 83.2 49.8 88.3 Llama 3.1 70B Instruct 73.4 85.3 46.4 66.8 73.8 77.0 56.4 93.7 93.6 89.5 88.0 33.4 76.5 Qwen 2.5 72B Instruct 71.5 85.5 30. 69.9 67.2 34.2 74.3 89.5 94.0 90.8 87.6 47.7 87.0 Hermes 3 Llama 3.1 70B 68.3 80.4 48.1 66.5 82.1 73.2 41.9 90.0 89.6 85.9 76.0 28.4 57.9 Nemotron Llama 3.1 70B 65.5 83.8 36.4 62.6 0.7 68.8 55.0 84.7 94.1 85.5 79. 66.1 69.0 Table 5: Summary of T칖LU 3 results relative to peer 70B models. The best-performing model on each benchmark (i.e., in each row) is bolded. T칖LU 3-8B significantly outperforms prior state-of-the-art 70B models. Progress across various checkpoints highlight the contribution of each stage of the training in improving core skills. Nemotron Llama 3.1 70B is the only model in the table that fine-tuned from another post-trained model (in this case Llama 3.1 70B Instruct), while the others are from their respective base models. Many of the lowest values, such as in BigBenchHard, are caused by failing to follow the few-shot formatting required for the evaluation or other repetitive errors for more details, see 7. Data Quality, Provenance, and Scale (3) We obtain prompts by carefully surveying available open-source datasets, analyzing their provenance, and decontaminating them, as well as curating synthetic prompts that target core skills. To ensure effectiveness, we conduct thorough experiments to study their impact on our development evaluation suite. We find targeted prompts to be influential to improve core skills, while real-world queries, e.g., WildChat [Zhao et al., 2024], are important to improve general chat capabilities. Using the T칖LU 3 EVAL decontamination tool, we ensure prompts are not contaminated against our evaluation suite.2 Creating Multi-Skill SFT Dataset (4.1) The distribution of the prompts in the general and skill-specific categories was refined by several rounds of supervised finetuning on various data mixtures. For example, to improve mathematical reasoning, we first establish an upper bound in our evaluation suite by creating math-specialized models, then mix data to bring the general models closer to this upper bound. Curating an On-Policy Preference Dataset (5.2) We develop an on-policy data curation pipeline to scale our preference dataset generation. Concretely, we generate completions from T칖LU 3-SFT and other models for given prompts, and obtain preference labels through their pairwise comparisons. Our approach extends and improves the off-policy preference data generation method by Cui et al. [2023]. Careful multi-skill selection of preference data yields 354,192 instances for preference tuning demonstrating significant improvements in range of tasks. Preference Tuning Algorithm Design (5.4) We experiment with several preference tuning algorithms and observe improved performance in using length-normalized Direct Preference Optimization. We prioritized simplicity and efficiency in experimentation and used length-normalized DPO throughout the development process and training our final models, in lieu of more costly investigations into PPO-based methods. Skill-Specific RL with Verifiable Rewards (6) We adapt new approach, leveraging standard reinforcementlearning paradigm to target skills that can be evaluated against ground-truth outcome (e.g., Math). We refer to this algorithm as Reinforcement Learning with Verifiable Rewards (RLVR); it obtains constant reward value if completion is successful. Our results show that RLVR can improve GSM8K, MATH, and IFEval performance. 2We observe non-trivial amount of contamination in few open datasets with popular evaluation benchmarks. Details are provided in Table 32. 9 Training Infrastructure for Reinforcement Learning (6.3): We implemented an asynchronous RL setup: we run LLM inference efficiently via vLLM while the learners perform gradient updates concurrently. Our RL codebase is also highly scalable and can train 70B RLVR policy models. Evaluation Framework: T칖LU 3 EVAL (7) In addition to evaluating the final models, our evaluation framework is an open evaluation toolkit designed to guide the development progress through carefully selected evaluation suite and tools for decontamination. 2.4 Evaluation and Results When reporting scores throughout this work, we use the metrics identified in Table 3; higher is always better. When computing overall performance, we simply average scores across all evaluations, treating each evaluation equally. T칖LU 3 trained on Llama 3 base models outperforms all other open-weight models in its size category on our development evaluation suite. Compared to closed models, T칖LU 3 70B even surpasses closed models such as GPT-3.5-Turbo-0125 or GPT-4o-mini-2024-07-18, while approaching the performance of Claude 3.5 Haiku 20241022. The summary of T칖LU 3 trained on Llama 3 at 8 and 70 billion parameters versus the leading models in their sizes classes is shown in Table 2. per training stage breakdown of performance is shown for the 8B version in Table 4 and for 70B in Table 5. With our models trained from raw pretrained base models, we compare to instruct models trained on the same base models (e.g. Nous Hermes 3), instruct models on similar sized, but different base versions (e.g. Ministral 8B or Qwen 2.5 Instruct), and other finetuning recipes trained on an instruct version (e.g. Nemotron Llama 3.1). At 70B, we compare to and surpass Llama 3.1 70B Instruct, Qwen 2.5 72B Instruct [Qwen Team, 2024], Nous Hermes 3 70B [Teknium et al., 2024] (trained on Llama 3.1 70B), and Nemotron Llama 3.1 70B [Wang et al., 2024c] (trained on Llama 3.1 70B Instruct). At 8B, we compare to and surpass Llama 3.1 8B Instruct, Gemma 2 9B Instruct [Gemma Team et al., 2024], Nous Hermes 3 8B (trained on Llama 3.1 8B), Qwen 72.5B Instruct, and Ministral 8B Instruct 2410. Artifacts Released. We release all artifacts associated with the T칖LU 3 training recipe including SFT, DPO, and RL model checkpoints, along with new SFT and DPO datasets. summary of the artifacts released with T칖LU 3 is included in Table 1."
        },
        {
            "title": "3 T칖LU 3 Data",
            "content": "Prompts represent the diverse ways users may interact with models and serve as the essential component for all post-training stages. We curate an extensive collection of million prompts as the starting point of T칖LU 3 posttraining recipe. Data selected for next stages of training are selected from these prompts. Table 6 summarizes the key information of these prompts. In this section, we describe our prompt curation process and the decontamination effort to ensure that our evaluations are not leaked in these prompts. In the following sections, we describe how prompts are used for supervised finetuning 4 and preference tuning 5. 3.1 Prompt Curation To target the desired core skills, we curate diverse and high quality set of prompts from publicly available datasets with clear provenance and synthetically generate prompts to fill any gaps. 3.1.1 Sourcing from Public Datasets Since the release of our T칖LU 2, the community has witnessed large body of work creating datasets for posttraining, in terms of both supervised finetuning and preference tuning. T칖LU 3 aims to integrate and extend these resources to build stronger models. We start this process with broad survey of public datasets, including those annotated by dedicated workers, sourced from real users, and synthesized with models. 3 We then manually review each individual dataset, and pick those with the following considerations. Diversity. The diversity of training data is critical for eliciting models generalization, avoiding model forgetting, and making models robust to uncommon inputs [Wang et al., 2022c, Chung et al., 2024, Zhou et al., 2024]. We pick datasets that can promote diversity, including: WildChat [Zhao et al., 2024], which is large source of real-user interaction with models; Open Assistant [K칬pf et al., 2024], which is created by volunteer workers for general 3The datasets we compiled and consider are available here: 1E2ScaKWbTnlelzJzcddCzEtf7WrpF3a5ZP5ZvdsOZ4Y/edit?usp=sharing. https://docs.google.com/spreadsheets/d/ 10 Category Prompt Dataset General T칖LU 3 Hardcoded OpenAssistant1,2, No Robots Count 24 88, 9,500 # Prompts used in SFT 240 # Prompts used in DPO 7,132 9,500 7, 9,500 WildChat (GPT-4 subset) 241,307 100,000 100,000 UltraFeedback洧띺, Knowledge FLAN v21,2, Recall Math Reasoning Coding SciRIFF TableGPT T칖LU 3 Persona MATH T칖LU 3 Persona GSM T칖LU 3 Persona Algebra OpenMathInstruct 2 NuminaMath-TIR洧띺 T칖LU 3 Persona Python Evol CodeAlpaca洧띺 T칖LU 3 CoCoNot Safety & Non-Compliance T칖LU 3 WildJailbreak洧띺, T칖LU 3 WildGuardMix洧띺, Aya Multilingual Precise IF Total T칖LU 3 Persona IF T칖LU 3 IF-augmented 41,635 89,982 35,357 13, 149,960 49,980 20,000 21,972,791 64,312 34, 107,276 10,983 50,000 50,000 202,285 29, 65,530 89,982 10,000 5,000 149, 49,980 20,000 50,000 64,312 34,999 107, 10,983 50,000 50,000 100,000 29,980 41,635 12,141 17,590 6,049 26,356 8,677 14,200 10, 26,356 26,356 32,210 19,890 65,530 23,327, 939,344 425,145 Reference K칬pf et al. [2024] Rajani et al. [2023] Zhao et al. [2024] Cui et al. [2023] Longpre et al. [2023] Wadden et al. [2024] Zha et al. [2023] Toshniwal et al. [2024] Beeching et al. [2024] Luo et al. [2023] Brahman et al. [2024] Jiang et al. [2024] Han et al. [2024] Singh et al. [2024b] Table 6: Summary of our prompt dataset: data for training stages are selected from these prompts. New datasets released with T칖LU 3 are color-coded for emphasis. Existing datasets we modified due to contamination are marked with 洧띺. Datasets with prompts used in T칖LU 1 or 2 are marked with 1 or 2, respectively. Datasets marked with are downsampled from their original datasets, datasets marked with are upsampled. Note that all datasets were filtered to remove specific keywords (e.g., OpenAI) and empty messages, resulting in slightly lower than reported counts. All T칖LU 3 datasets with Persona expand the methodology of Chan et al. [2024]. The percentages listed per category are out of the total prompts. chatting; No Robots [Rajani et al., 2023], which is annotated by expert workers for broad range of open-ended categories; and FLAN v2 [Longpre et al., 2023], which is big compilation of classical NLP tasks. We also include decontaminated subset of UltraFeedback [Cui et al., 2023], which is composition of several datasets (FalseQA [Hu et al., 2023], UltraChat [Ding et al., 2023], Evol-Instruct [Xu et al., 2023], FLAN v2 [Longpre et al., 2023]) and has shown strong performance for general preference tuning in early studies [Tunstall et al., 2023, Ivison et al., 2024]. Target Skills. We especially consider enhancing several capabilities that can power common use cases and our special needs. As shown in our earlier study [Wang et al., 2023], some capabilities, such as complex reasoning, coding, and precise instruction following, benefit from mixing in additional data. Therefore, we include the following datasets: OpenMathInstruct [Toshniwal et al., 2024] and NuminaMath [Beeching et al., 2024] for math reasoning, Evol-CodeAlpaca for coding, subset of Daring-Anteater [Wang et al., 2024d] for precise instruction following, Aya [Singh et al., 2024b] for multilinguality, SciRIFF [Wadden et al., 2024] for scientific literature understanding, and TableGPT [Zha et al., 2023] for processing table-related tasks. We have also considered other datasets for domains with plenty of studies (e.g., math), but they either did not bring additional benefits in our early supervised finetuning experiments or have license issues. Data Provenance and Licenses. When sourcing prompts, we take careful consideration of the licenses of the original datasets and only allow those with clear and correct licenses. Since many publicly released datasets are 11 compositions of other datasets, we have to manually track the provenance of subsets to verify their licenses and remove those that have issues. Specifically, the ShareGPT dataset4 is of questionable legal provenance as they were shared by users on the Internet without an agreement to be used for model training, so we exclude it and use WildChat instead. We also removed the relevant subset from UltraFeedback and decided not to use Helpsteer2 [Wang et al., 2024d] due to the use of ShareGPT in their prompts. All the datasets included in our final curation have clear licenses, and we have smaller commercial-friendly set and larger non-commercial set based on whether the licenses allow commercial use. 3.1.2 Synthesizing for Target Skills To address the growing need for diverse and skill-specific datasets, we incorporate synthetic data generation as complementary approach. Synthetic data generation has gained traction as promising alternative to human-written data due to being easier to obtain, customizable for different purposes, and reflecting the vast knowledge of the underlying models [Dubey et al., 2024]. However, generating diverse and high-quality data at scale is non-trivial, as LMs are susceptible to falling into repetitive modes or patterns, referred to as mode collapse [Kazdan et al., 2024]. To ensure diversity in generation, we follow the recent persona-driven methodology in Chan et al. [2024] to generate synthetic data. The key idea is to use different personas (e.g., machine learning researcher focused on neural networks) with data synthesis prompt (e.g., create coding problem) to steer an LLM to synthesize data with corresponding perspectives. Specifically, we condition on 250K personas from Persona Hub [Chan et al., 2024] to generate prompts targeting specific skills such as precise instruction following, math and coding. We detail our procedure for each select skill below. Prompts used to generate these instructions can be found in Appendix C.1. Additionally, we build upon our previous efforts in Brahman et al. [2024], Han et al. [2024], Jiang et al. [2024], to generate noncompliance and safety data. Precise Instruction Following. Precise instruction following is the ability to follow verifiable instructions in natural language, such as write 3 paragraphs, that can be automatically verified with heuristics. We use our persona-driven approach to synthetically generate verifiable instructions covering 25 different constraint types defined in IFEval benchmark [Zhou et al., 2023]. More concretely, we start by manually writing 1-2 example instructions per constraint (e.g., number of words), resulting in total of 33 verifiable instructions which we used as seed prompts. We then generate new instructions using GPT-4o [OpenAI, 2024]5 given data synthesis prompt, persona, and single verifiable instruction as an example. Figures 28 and 29 show the exact prompts used to generate the instruction and its corresponding response, respectively. In total, we collected 29,980 verifiable instruction-response pairs which we call IF-PERSONA-SFT. Math and Coding. We follow similar persona-driven approach to synthetically generate diverse math word and coding problems. Math problems include those that require advanced mathematical skills as well as grade school problems. For coding, we generate python programming questions that are solvable by entryto mediumlevel programmers. Unlike precise instruction following, we zero-shot prompt GPT-4o to generate problems that are unique and specific to given persona input. Having generated the problems, we then generate multi-step math solutions using GPT-4o, and python programs using claude-3-5-sonnet. Exact prompts used to generate problems and solutions are provided in Figures 31, 33, 32, and 34, respectively. In total, we collected 220K and 35K instances for math reasoning and coding. Noncompliance and Safety. As we enhance models capabilities to assist users effectively, it is crucial to ensure they can reliability reject unsafe and appropriately handle nuanced and out of scope queries. To support this, we curate set of noncompliance queries [Brahman et al., 2024] that the model ought to not comply with, alongside safety-related direct and adversarial prompts [Han et al., 2024, Jiang et al., 2024] covering both benign and harmful scenarios. Our noncompliance and safety prompts are either curated from existing datasets [Zhang and Choi, 2021, Zhao et al., 2024] or synthetically generated from GPT model family. More specifically, our noncompliance prompts are obtained based on our contextual noncompliance taxonomy from Brahman et al. [2024], spanning multiple categories including incomplete, unsupported, indeterminate, and humanizing requests (in addition to unsafe requests). Our safety-related prompts are carefully selected among synthetic adversarial prompts, synthetic vanilla (direct) requests, real-world user-LLM interactions (In-The-Wild), and curated annotator-written examples to maximize coverage, diversity, and balance. Dataset Evol CodeAlpaca HumanEval Orig: ise-uiuc/Magicoder-Evol-Instruct-110K Eval. Link WildChat GPT-4 Safety WildJailbreak Safety WildGuardmix Safety NuminaMath-TIR MATH New: allenai/evol_codealpaca_heval_decontaminated Orig: allenai/WildChat-1M-Full (GPT-4 instances only) New: allenai/wildchat_gpt4_converted_safety_decontaminated Orig: allenai/wildjailbreak New: allenai/wildjailbreak_safety_decontaminated Orig: allenai/wildguardmix New: allenai/wildguardmixtrain_safety_decontaminated Orig: AI-MO/NuminaMath-TIR New: allenai/numinamath_tir_math_decontaminated Table 7: Decontaminated datasets. % is the percent of the dataset removed. % 3.5 5.4 0.7 1.1 11. 3.2 Prompt Decontamination One important consideration when curating our training mix was possible overlap between training and evaluation sets. We quantify such overlap as follows and remove instances from our training mix as needed in order to prevent test set contamination. Matching Method. We experimented with full-string, n-gram, and embedding-based matching and found that n-gram matching yielded the most useful results while embedding-based methods can in principle identify non-trivial contamination like that due to paraphrasing [Yang et al., 2023], we found it difficult to distinguish mere distributional similarity from actual paraphrasing. Moreover, partial surface-level overlap using n-gram matching successfully identified cases of contamination where the instances were trivially different, e.g., math problem where only the numbers differ. Identifying Matching Instances. Since completions in training datasets are often regenerated using language models, we chose to compute overlap in the prompts alone (or more generally user turns in multi-turn dialogues). We used 8-gram matching for our contamination checks following [Dubey et al., 2024, Singh et al., 2024a]. For each token in test instance, we consider it to match token in train instance if the two instances share an 8-gram containing that token, and we consider the test instance itself to have significant overlap with train instance if more than 50% of the test tokens have 8-gram matches with the same training instance. Decontamination. We consider training set to be contaminated if any number of its instances overlap with more than 2% of the instances in any of the evaluations in our development and unseen suites. We remove all the training sets that were contaminated with our unseen evaluations. For training sets that were contaminated with our development evaluations, we removed the entire dataset if doing so did not significantly impact the performance of the resulting model; otherwise, we removed the specific instances that match any test instance. The list of datasets we decontaminated and the versions we released with overlapping samples removed is shown in Table 7. The full list of public datasets that we found to be significantly contaminated with our evaluation sets can be found in Table 32."
        },
        {
            "title": "4 Supervised Finetuning",
            "content": "Adapting pretrained base models to various tasks and user requests often relies on supervised finetuning (SFT), also known as instruction finetuning. key challenge in this process is balancing the proportions of mixed training datasets representing diverse skills. For T칖LU 3, we conducted data mixture ablations and explored model merging 4ShareGPT data was initially used to build the Vicuna model [Chiang et al., 2023], but the exact dataset has not been released. Later work mainly used community reproduced version at https://huggingface.co/datasets/anon8231489123/ ShareGPT_Vicuna_unfiltered/. 5We use GPT-4o-2024-08-06 for all our persona-driven data synthesis, unless otherwise stated. 13 Tulu 3 Persona MATH Evol CodeAlpaca Aya WildChat FLAN v2 WildJailbreak NuminaMath-TIR OpenMathInstruct WildGuard Tulu 3 Persona GSM Tulu 3 Persona Code Tulu 3 Persona IF Tulu 3 Persona MATH - Other (<11,000 instances) Algebra Figure 2: The T칖LU 3 final SFT mix by source and length of the prompt plus completion in tokens (using the Llama 3 tokenizer). Compare this distribution to previous open SFT training datasets in Fig. 24. Datasets with the most instances are on the bottom of the histogram. techniques to develop an SFT training procedure that well balances performance across the core skills we prioritized. The following sections detail our experiments and findings. 4.1 SFT Data 4.1.1 From Prompts to SFT Data To create our SFT mix, we collect or create responses for prompts described in Section 3 in two ways: filtering existing responses, and creating new responses. For prompts with existing responses, we generally keep the original response if it was written by human or frontier model, like GPT-4o. For large datasets with subsets from frontier models (e.g. WildChat), we use the subset from the best models. We additionally filter empty responses and responses that contain information about models or their developers. If set of prompts did not have responses, like our Persona prompts, or if the original responses were from weaker model (e.g. WildGuardMix), we generate new responses using GPT-4o. We also hand-wrote responses to our hardcoded prompts. 4.1.2 The T칖LU 3 SFT Mix To develop our SFT mix, we first identified the skills that were lagging behind state of the art models using Llama 3.1 trained on T칖LU 26 as our baseline. Targeting each of these skills in isolation, we collected high quality publicly available datasets and created synthetic datasets, as described in Section 3.1.2, and also removed some datasets that we identified to be of relatively lower quality compared to other more recent datasets. To design our final SFT mix, we first built skill-specific data mixtures and models, keeping the mixtures that led to the best performance on individual skills, ignoring other evaluations. This was done to approximate the upper bound for each evaluation given our setup. We then combined these mixtures to create our initial T칖LU 3 preview mix. We then continued to iterate on the mixture by adding or removing datasets to improve lagging skills, decontaminating against our evaluations 6https://huggingface.co/allenai/llama-3.1-tulu-2-8b 14 T칖LU 2 Intermediate Mix 1 Intermediate Mix 2 Intermediate Mix 3 Intermediate Mix Intermediate Mix 5 T칖LU 3 Figure 3: Average and selected skill-specific performance from training Llama 3.1 8B on our initial T칖LU 2 SFT mix, and our intermediate and final T칖LU 3 SFT mixes. Intermediate mixes 1, 2, and 3 were the result of adding new datasets to improve performance. Intermediate mixes 4 and 5 were the result of running multiple rounds of decontamination, causing small drops in performance. and downsampling particularly large datasets. We show the performance of major preview versions throughout development in Figure 3. Final SFT Results. In Table 8, we compare our final T칖LU 3 8B SFT and T칖LU 3 70B SFT models against other SFT-only models trained on Llama 3 8B or 70B. Our new SFT mix shows substantial improvements over the T칖LU 2 mix at both model sizes, and is better on average the other competitive 8B SFT models. T칖LU 3 8B SFT T칖LU 2 70B SFT 60.1 63.6 Model T칖LU 2 8B SFT RLHFlow SFT V2 MAmmoTH2 8B 61.8 Avg. MMLU TQA PopQA BBH CHE CHE+ GSM DROP MATH IFEval AE 2 Safety 70.7 48.3 43.5 56.0 47.8 46.4 63.1 80.9 66.4 42.3 52.7 34. 86.2 72.8 49.4 14.0 66.9 60.4 57. 23.3 8.9 61.7 57.2 43.8 61.3 75.9 81.6 63.7 76.2 83.2 35.7 30.5 31.5 33.1 13.6 6.5 12.4 17. 72.8 57.7 93.1 68.8 65.8 63.6 62.1 76.0 56.0 42.7 46.8 57.8 55.7 29.7 20.8 29.3 44. 69.3 63.4 67.9 79.4 86.2 86.8 81.4 83.5 72.6 79.4 T칖LU 3 70B SFT 94.4 92.9 Table 8: Summary of the performance of our T칖LU 3 SFT models against comparable baselines. Our final SFT mixtures show strong performance, achieving higher average score than other comparable mixes. All models, including T칖LU 2 SFT, were trained on either Llama 3.0 or 3.1. Our final T칲lu 3 70B model was used to help format this table. 87.3 48.6 91.1 77.2 53.7 26. 82.1 82.7 4.2 Key Data Experiments We also ran series of controlled experiments after developing our final SFT mix to explore the importance of different decisions made during data mixing and training. Diverse Chat Data. In our mix we also emphasized adding diverse chat data, mainly from WildChat. We show the impact of removing WildChat in Table 9, and we see that there is small but noticeable degradation on most skills, most noticeably on Alpaca Eval, highlighting the importance of diverse real-world data. Safety is Orthogonal. We found that our safety SFT data was generally orthogonal to our other datasets. We report the effect of removing our safety-specific datasets in Table 9, and we see that most skills stayed roughly the same, except the safety average. We also found that adding constrastive prompts, such as those in CoCoNot, were helpful for preventing our models from over-refusing safe prompts. 15 29.3 28.9 62.1 61.0 62.0 46.8 45.2 45.5 Avg. MMLU TQA PopQA BBH CHE CHE+ GSM DROP MATH IFEval AE 2 Safety 93.1 60.1 58.9 58.0 58.6 58.2 Model T칖LU 3 8B SFT w/o WildChat w/o Safety w/o Persona Data w/o Math Data Table 9: Performance during our SFT ablations, showing the effect of removing safety, WildChat, Persona, and Math data in isolation. We find that: 1) diverse chat data is beneficial for most skills, most noticeably Alpaca Eval, 2) safety performance is generally orthogonal to general performance, 3) our new Persona datasets improve all of the skills that they target, and 4) using mathematics as test case, adding high quality skill-specific data substantially improves skill-specific performance. 72.8 70.1 71.0 53.6 70.6 86.2 85.3 84.5 84.5 86.0 81.4 80.7 79.6 79.0 80.5 67.9 65.6 68.3 68.3 95.2 74.7 93.9 93.5 61.3 59.3 59. 12.4 7.5 12.4 76.9 76.8 64.1 32.6 30.1 23.5 31.5 31.8 76.2 75.8 62.2 60. 62.4 62.2 48.9 47.1 13.5 12.0 29.5 29.4 68.9 29. New Persona Data. Our new Persona datasets were built to target specific skills: mathematics, coding, and instruction following. In Table 9 we show that performance on HumanEval(+), GSM8K, MATH, and IFEval drop after removing our Persona datasets, showing the value of creating diverse, skill-specific SFT datasets. Targeting Specific Skills. We also show the impact of targeting specific skills in Table 9, where we show the effect of removing all mathematics-specific data. We see that both GSM8K and MATH drop significantly compared to our final mix, emphasizing the positive impact of focusing on specific skills during data collection. Targeting Specific Skills. large portion of our focus was on collecting or creating datasets targeting specific capabilities. Using mathematical reasoning as an illustrative example, we show in Table 9 the impact of our mathematics-specific data on both GSM8K and MATH. We see that our mathematics-specific SFT data substantially improves both GSM8K and MATH, showing the value of the data included in the final mix. Amount of SFT Data. In Figure 4, we show the effect of taking stratified subsamples of our SFT mix. We find that our models continue to improve on average as more SFT data is included, and we see large improvements on metrics like GSM8K as we increase the amount of data to the full mix. Interestingly, TruthfulQA performance actually drops as the amount of data in the mix increases. We do not increase our SFT data size beyond the current mixture because we allocated other prompts for preference optimization. 8B Value Hyperparameter 5 10-6 Learning Rate Learning Rate Schedule Linear Batch Size (effective) Max Token Length Warm up ratio Number of Epochs 128 4,096 0.03 70B Value 2 10-6 Linear 128 4,096 0.03 2 Table 10: SFT Training Hyperparameters. 4.3 SFT Recipe and Analyses. Training Settings To train our T칖LU 3 models, we used between 4 and 16 8xH100 nodes with high speed interconnect. We used an effective batch size of 128 and maximum sequence length of 4,096 tokens. We trained for two epochs using learning rate of 5e-6 for our 8B models, and 2e-6 for our 70B models, which we found after hyperparameter search. Our hyperparameter settings are also summarized in Table 10. For merging experiments we used mergekit7 [Goddard et al., 2024], using linear weighted averaging. 4.3.1 Key Training Experiments Choice of Base Model. We also test the effect of training different base pretrained models on mathematical performance using our full SFT mix. In Table 11, we show the impact of changing the models size by training on both Llama 3.1 8B and 70B, and the impact of adding domain-specific pretraining data by training on Qwen 2.5 7B 7https://github.com/arcee-ai/mergekit 16 and Qwen 2.5 Math 7B. In both cases, we see substantial improvement in both GSM8K and MATH, highlighting the importance of both model size and pretraining data for downstream skills. GSM8K MATH Base Model 76.2 Llama 3.1 8B 91.1 Llama 3.1 70B Qwen 2.5 7B 79.2 Qwen 2.5 Math 7B 86.3 31.5 53.7 49.4 56.4 Table 11: Mathematical performance of different base models trained on our mix. We see that 1) training on larger models leads to better performance, and 2) adding skill-specific pretraining data also leads to improved performance, even for the same size model. Chat Template Variation. During creating T칖LU 3, we explored changing the chat template used to guide the generation of finetuned models. We made small change to the chat template used in previous T칖LU versions, specifically removing the new line at the end of the template (before the model response). The performance between different changes to the chat template is shown in Table 12 on an early version of our SFT setup. We found that replacing the newlines at the end of assistant messages with an eos token resulted in the best performance, but we opted not to use this to avoid generation inconsistency with later steps in our post-training pipeline. The chat template can be found in our codebase and we provide it in App B.3. Chat Template T칖LU (replace w/ eos) Zephyr T칖LU 3 (no n) T칖LU 2 template Llama 3 template Avg. 53.0 52.9 52.8 52.6 51.6 Table 12: The impact of different chat templates on SFT model performance, trained using an intermediate SFT mixture on Llama 3.0. While replacing the newline does best, we instead opted for simply removing the newline to avoid complexity. Random Seeds and Model Soups. We also explored changing the random seed during SFT, and then using those models to create model soups [Wortsman et al., 2022]. In Table 13, we compare training 8B and 70B models with multiple different seeds with the best model soup. We see that SFT performance noticeably varies based on the seed, highlighting the importance of multiple training runs, and that the best model soup does not always outperform the best single training run. Because of this, we use the best single SFT training run for each model size as our final SFT models. Model T칖LU 3 8B SFT Best Model Soup Seed 42 (Default) 123 456 789 1011 42 & 123 Average Model Seed 59.9 60.1 59.8 59.8 59.8 60. T칖LU 3 70B SFT 42 (Default) 123 456 - - 123 & 456 Best Model Soup Average 71.8 70.0 72.6 - - 72.5 Table 13: Average performance of our 8B and 70B SFT models using random seeds, and compared against the best model soup using the models trained with different seeds. We find that the best random seed is comparable to the best model soup, so for consistency we use the best single SFT run as our final SFT model. 17 Figure 4: Average and skill-specific performance on stratified subsamples of our final SFT mix. We find that our full mix performs best overall. 5% 10% 25% 50% 75% 100% 4.3.2 Batch Aggregation Early during training T칖LU 3, we noticed gap in performance between SFT models trained on our Open-Instruct framework and models trained in other settings such as on TPUs.8 We found this issue was largely due to (recently widely-reported) issue with loss aggregation inside Transformers [Wolf et al., 2020]: Averaging the loss across padding tokens without taking into account gradient accumulation or distributed training setups. Here, we illustrate the issue with an example. Assume we have two samples in batch, with 洧녵1 tokens and 洧녴1 time, we get: non-padding padding tokens. If we pass both samples into the default Transformers forward pass at the same , 洧녴 , 洧녵2 洧 = 洧녳洧녵1 + 洧녳洧녵2 洧녵1 + 洧녵2 (1) However, if we apply gradient accumulation, feeding in the two samples separately, computing loss, and then dividing, our loss is instead computed like: 洧녳洧녵1 洧녵 洧녳洧녵2 洧녵2 + 2 洧 = (2) That is, in the second case we weight each example equally, while in the first we weight each token equally. As such, changing gradient accumulation can have large effects on performance due to effectively changing sample weightings, as reported by Muennighoff et al. [2024]. similar issue occurs in distributed training due to cross-device averaging. We refer to recent reports on this issue for more in-depth explanation. To fix this issue, we opted generally to use sum loss instead of averaging (mean loss) when training. This removes the issue by simply removing the denominator from the above equations and requires an adjustment to learning rates. This effectively weights all tokens equally (which we found led to generally better performance for initial mixtures). We validated the perfomance of our setup by finetuning Llama 3.0 on the T칖LU 2 SFT mixture using variety of learning rates, epochs, and loss types as shown in Figures 5 and 6. Ultimately, we found that using sum loss with learning rate of 5.00E-06 worked best. Surprisingly, we additionally found that training for longer did not yield further improvements, and so used 2 epochs for training. 8Relevant code: https://github.com/hamishivi/EasyLM 9https://unsloth.ai/blog/gradient, https://muellerzr.github.io/blog/gradient_accumulation_part2.html 18 Figure 5: Average performance when finetuning Llama 3.0 on the T칖LU 2 mixture using differing loss types and learning rates. We find that LR of 5e-6 with sum loss works best. Figure 6: Average performance when finetuning Llama 3.0 on the T칖LU 2 mixture using sum loss and LR of 5e-6 for varying numbers of epochs. We find using 2 epochs works best."
        },
        {
            "title": "5 Preference Finetuning",
            "content": "For T칖LU 3 we explore many approaches to preference finetuning with the goal of improving our entire evaluation suite. We explore multiple training algorithms, from Direct Preference Optimization (DPO) and its derivatives to reinforcement learning algorithms such as Proximal Policy Optimization (PPO). In this section, we detail the problem formulation of learning from human preferences and our optimizers. Next, we explain how to convert our prompts into synthetic preference data from both on-policy (T칖LU 3 suite) and off-policy models (other instruct models. We show how to create preference data for specific skills of interest and how we improve our models robustly with DPO. 5.1 Background Prior work has established training on preference data as crucial step for improving model performance on benchmarks simulating human or synthetic preferences [Dubois et al., 2023, Ivison et al., 2023, 2024]. The typical procedure is reinforcement learning from human or synthetic feedback [Ziegler et al., 2019, Stiennon et al., 2020, Ouyang et al., 2022, Bai et al., 2022]. 5.1.1 Setup Preference Data. In the standard setup, there is some preference dataset consisting of prompts 洧논 and two , and label the other responses 洧녽, 洧녽 per prompt. Some judges will choose one of 洧녽, 洧녽 as their preferred response 洧녽洧녫 as rejected response 洧녽洧 Reward Model. Given the preference dataset, reward model (RM) 洧洧랯 max 洧洧랯 洧댶(洧논,洧녽洧녫 ,洧녽洧)[log 洧랥(洧洧랯(洧논, 洧녽洧녫) 洧洧랯(洧논, 洧녽洧))] is trained with the following objective: (3) . where 洧랥 is the logistic function. The RM objective maximizes the difference between the rewards, and this difference represents the log-likelihood that 洧녽洧녫 [Ouyang et al., 2022]. This reward model can help train policy models to output contents preferred by the RMs judgments. will be preferred over 洧녽洧 5.1.2 Policy Optimization There are plethora of options for optimizing language models with access to preference data. Today, the two categories can be abstracted as reinforcement learning algorithms, which learn from an internal representation of value or reward, and direct alignment algorithms, which learn directly from the data. Prior work [Ziegler et al., 2019, Stiennon et al., 2020, Ouyang et al., 2022] optimizes the policy 洧랢洧랚 following objective: with the max 洧랢洧랚 洧댶洧녽洧랢洧랚(洧논) [洧녠(洧논, 洧녽)] = ] [洧洧랯(洧논, 洧녽) 洧띻KL[洧랢洧랚(洧녽洧논)洧랢ref(洧녽洧논)] (4) where 洧랢ref is the initial reference policy, and the 洧띻 coefficient helps control the Kullback-Lieber divergence (KL) divergence between the reference policy and the training policy. Here, we explain PPO and DPO as representative examples. 19 Figure 7: Pipeline for generating and scaling preference data that is based from Ultrafeedback [Cui et al., 2023]. Proximal Policy Optimization (PPO). An approach to address the above objective is to use online reinforcement learning (RL) like PPO [Schulman et al., 2017]. In each training iteration of PPO, the policy needs to generate some samples, generate rewards using the RM on those samples, and maximize 洧녠(洧논, 洧녽) using the PPO algorithm. As PPO training loops are complex, we refer the reader to Ouyang et al. [2022], Ivison et al. [2024], Huang et al. [2024a] for more thorough descriptions of the setup and typical setups. We provide more implementation details in Sec 6.2. Direct Preference Tuning (DPO) and Variants. Another approach is offline preference tuning. DPO [Rafailov et al., 2024] can directly optimizes for the RLHF objective with the following equivalent objective: max 洧랢洧랚 洧댶洧녽洧녫 ,洧녽洧 ( [ log 洧랥 洧띻 log 洧랢洧랚(洧녽洧녫洧논) 洧랢ref(洧녽洧녫洧논) 洧띻 log )] 洧랢洧랚(洧녽洧洧논) 洧랢ref(洧녽洧洧논) (5) DPO trains an implicit reward model and policy model simultaneously, without needing to use trained reward model, do policy generations, and get rewards from the RM. Crucially, this allows offline preference finetuning, directly training language model on preference pairs gathered from variety of sources. Recently, much work has examined how to further improve the DPO objective, with multitude of variants proposed [Meng et al., 2024, Xu et al., 2024a, Hong et al., 2024, inter alia]. In this work, we explored two promising variants: SimPO [Meng et al., 2024] and length-normalized DPO10. We find (in Section 5.4) that length-normalized DPO works best, which uses the following objective: max 洧랢洧랚 洧댶洧녽洧녫 ,洧녽洧 [ log 洧랥 ( 洧띻 洧녽洧녫 log 洧랢洧랚(洧녽洧녫洧논) 洧랢ref(洧녽洧녫洧논) 洧띻 洧녽洧 log 洧랢洧랚(洧녽洧洧논) 洧랢ref(洧녽洧洧논) )] (6) As seen, this is simply the DPO objective (Eq 5), but with log-probabilities normalized for length, which intuitively aids with mitigating the length bias common in human and model preferences [Singhal et al., 2024]. When developing T칖LU 3, we opted to use length-normalized DPO for tuning our preference data mixtures and generation methods due to its relative simplicity and speed compared to approaches such as PPO. 5.2 T칖LU 3 Preference Data 5.2.1 From Prompts to Preference Data We create on-policy preference data (洧논, 洧녽, 洧녽, 洧녳洧녩洧녪洧뉧롐) given our prompts from 3 by adapting and advancing the UltraFeedback pipeline [Cui et al., 2023]. Our early experiments show the benefit of this pipeline in creating preference data, which leads to high-quality, synthetic preference dataset (as observed by Ivison et al. [2024]). Our data creation pipeline (shown in Figure 7) consists of three stages: prompt selection, response generation from pool of models, and preference annotation with LLM-as-a-judge to create (preferred, rejected) pairs. Stage 1: Prompt Selection. The first step for preparing dataset for preference finetuning is to select the prompts or user instructions to generate responses and obtain preferences for. Given the set of prompts in Table 6, we curate our selection to include prompts used during SFT, and prompts that were subsampled from the same sources, yet unused, for SFT. We also include prompts from other sources, such as version of Ultrafeedback without TruthfulQA instances, or by adding new IF-constraints to prompt. 10As proposed in the original Rafailov et al. [2024], but was not yet well optimized to successful hyperparameters until Meng et al. [2024] 20 Stage 2: Response Generation. For given prompt, we randomly sample four models from model pool to generate responses. Our model selection is inspired by the Ultrafeedback pipeline which consists of open-source and proprietary models that vary across parameter size and model family. We update Ultrafeedbacks model pool by using recent versions of some models (Llama 2 Llama 3.1), adding best-performing models to increase the pool size, and replacing currently inaccessible models such as WizardLM with open-source alternatives. Finally, we also include on-policy data by sampling completions from the T칖LU SFT model. We approach this by adding selection of prompts where one response is generated from the on-policy model, and the other response from the off-policy models. Stage 3: Preference Annotation. After generating four responses for each prompt, we use an LLM-as-a-judge [Zheng et al., 2023], specifically GPT-4o-2024-0806, to rate each response from 1 to 5 across four different aspects: helpfulness, instruction-following, honesty, and truthfulness. Appendix shows the external models used to sample off-policy data and the prompt template for each aspect. In order to obtain binary preferences for DPO, we obtain the mean of preference ratings similar to Argillas binarization method11 and take the highest-rated response as the chosen response and randomly sample from the responses with the lower mean as the rejected response. 5.2.2 The T칖LU 3 Preference Mix 8B 70B Count Dataset 19,444 SFT Reused On-policy 96,911 SFT Reused Off-policy 65,530 IF-Augmented 10,792 WildChat IF 17,207 WildChat Reused WildChat Unused 82,783 Ultrafeedback (Cleaned) 41,635 19,890 Persona IF 354,192 271,409 334,302 Total Table 14: Summary of our best preference dataset mixes for T칖LU 3 8B DPO and T칖LU 3 70B DPO . IF is short for Instruction Following. We choose the final preference mix for the 8b and the 70b model, which maximizes average performance on the development evaluations, while also excels at targeted skills. Most of the preference data mix ablations are run for the 8b model, We start with prompts used for SFT and generate on-policy and off-policy preference data, resulting in 96911 (off-policy) and 19444 (on-policy) preference instances. Given this preference base we ablate adding additional prompt sources to the mix and how these additions affect downstream evaluation performance, specifically targeting skills like precise instruction following, math and general chat performance on alpacaeval. Table 15 shows how the inclusion or exclusion of preference datasets influences the average performance. Our final mixes for T칖LU 3 8B DPO and T칖LU 3 70B DPO are displayed in Table 14. In summary, our preference mixes come from different prompt sources, such as SFT data, WildChat and Persona IF. It includes prompts seen during SFT training but also new, unseen prompts. And lastly, it contains mix of on and off-policy completions. 5.3 Key Findings of Data Ablations We perform several ablations to inform the design decisions of the synthetic preference pipeline (5.2.1) and the composition of the T칖LU 3 preference mix (5.2.2). 11https://huggingface.co/datasets/argilla/ultrafeedback-binarized-preferences/blob/main/ README.md 21 SFT Mix P-IF WildC.-IF SFT-IF WC洧띻 WC洧띺 UF洧 DA UF CocoNot Avg. 62.27 61.99 61.83 61.76 61.59 61.55 61.35 61.29 61.25 61.17 60.87 60.86 60.84 60.54 Table 15: Some of our dataset mixing experiments to obtain the final preference dataset mix. We include prompts from DaringAnteater (DA), our SFT Mix (SFT), Ultrafeedback (UF), Persona prompts for different skills (P-IF, P-Code, P-Math), T칲lu3 instruction following prompts (T칲lu3-IF), i.e. IF-Augmented, CocoNot, the IF subset of Daring Anteater Wang et al. [2024d] and WildChat (WildC.). (洧띺: prompts used during SFT, 洧띻: prompts from datasets subsampled, yet unused, for SFT, 洧: only used the prompts, the completions and preferences were regenerated using the pipeline described in $5.2.1). 25% 10% Initial 8B SFT 50% 5% 75% Figure 8: Effect of scaling the size of the preference dataset, specifically the number of unique prompts, on downstream DPO model performance. (AE: Alpacaeval) 100% 180k Initial 8B SFT 64k Figure 9: Effect of scaling preference dataset by duplicating prompts on downstream DPO performance using the Ultrafeedback dataset. All sizes have the same number of unique prompts (64k). 383k Scaling the Number of Unique Prompts Improve Downstream DPO Performance. First, we investigate whether increasing the number of prompts will yield improvements in downstream DPO performance. To do so, we measure the downstream DPO model performance at different sizes of fixed amount of preferences with unique prompts. Figure 8 shows that there are noticeable performance gains across several metrics as the size of the preference dataset increases. This suggests that dataset scaling is important to achieve improvements in downstream model performance: our final preference mixes  (Table 14)  contain more than 270k data points for the 8b model and more than 330k instances for the 70b model - which is considerably bigger than many available preference datasets. We also explore whether duplicating prompts (but with different responses) is viable approach to scaling the size of preference dataset and whether it will lead to gains in downstream DPO performance. To do so, we 22 expanded the Ultrafeedback dataset, which originally had four responses for each prompt, by creating different pair combinations of responses. This expansion will naturally cause duplicated prompts, but with different chosen and rejected pairs sampled from the four responses in UltraFeedback, leading to preference datasets with 64k-, 180k-, and 383k instances. Figure 9 shows that, on average, the 383k-size preference dataset performs similarly to the 64k preference dataset. We also observe slight performance degradation on DROP, GSM8k, and AlpacaEval as the number of duplicated prompts increase. This suggests that scaling via prompt duplication does not necessarily yield into significant gains in downstream DPO performance, and investing in the collection of unique prompts and proper mixing is more important for downstream evaluations. Unused Prompts Lead to Higher Performance vs. Reusing Prompts From SFT Mix. We then compare including new prompts and re-using prompts from the SFT stage on their effect on downstream DPO performance. To do so, we sampled 100k prompts from the SFT dataset mix that were used during training (as shown in Table 6) and compare it against prompts from the same open datasets (e.g., OpenAssistant, SciRIFF, Aya, Persona, WildChat, etc.) we subsampled from but left unused during SFT. Figure 10 shows that the unused dataset has slightly higher performance as opposed to reusing prompts. This suggests that the presence of new prompts can help improve downstream DPO performance. As seen in our best mix, combining unused and reused prompts seems to lead to the best result. On-policy Data Improves Downstream DPO Performance. We investigate whether the inclusion of on-policy data, i.e., text generations from the SFT model that will be used as the base model for preference finetuning, improves downstream model performance. Given the same set of prompts sourced from the SFT mix in 4, we generate preferences from off-policy models and compared it to mix that is strictly on-policy (i.e., one of the response is always from the Initial 8B SFT model, and the other response is from the off-policy models). We also compare it on combination of both on-policy and off-policy data: we sample instances from the strict on-policy dataset and add it to the off-policy dataset so that the responses from each model is distributed equally. Figure 11 shows that including on-policy data improves aggregated downstream DPO performance compared to completely off-policy dataset where prompt completions were sampled from other models. Figure 10: Effect of reusing prompts from SFT mix and new prompts from the same datasets subsampled for the SFT dataset mix. Figure 11: Effect of including on-policy data during the Response Generation stage of the synthetic preference data pipeline on downstream DPO model performance. Performance Across LLM Judges are Similar, with GPT-4o Leading Slightly Ahead. In order to determine which judge to use for obtaining preference annotations, we test several commercial and open-source LLM judges such as GPT-4 (GPT-4-turbo-2024-04-09, GPT-4o-2024-08-06, gpt-4o-mini-2024-07-18) and Llama 3.1 (70B and 405B) on the same set of 10k randomly-sampled Ultrafeedback prompts and responses. In general, GPT-4o, Llama3.1 405B, and GPT-4 turbo perform similarly across all benchmarks, with GPT-4o leading slightly ahead on the aggregated average performance as shown in Table 16. In the synthetic preference pipeline for T칖LU 3, we opted for GPT-4o-2024-08-06 due to its ease-of-use, cheaper cost per request, and batch inference speed via OpenAIs Batch API.12 Going Beyond Ultrafeedback. Previous work on preference learning using openly available datasets has shown that the UltraFeedback [Cui et al., 2023] preference dataset generally outperforms other preference datasets [Ivison et al., 2023]. In Figure 12 we show that we were able to significantly surpass DPO training on UltraFeedback by 12https://platform.openai.com/docs/guides/batch 23 Avg. MMLU TQA PopQA BBH CHU CHU+ GSM8k Drop MATH IFEval AE Safety LLM Judge 62.7 GPT-4o 57.3 62.8 LLama 3.1 405B 57.2 62.2 57.0 GPT-4 Turbo 61.6 56.9 GPT-4o Mini 62.2 56.6 Llama 3.1 70B 64.8 64.8 64.6 64.4 64.3 66.3 67.4 66.4 66.2 66.6 30.1 30.3 30.1 30.4 30.2 56.1 56.0 55.7 55.4 55.5 20.3 20.1 20.1 20.9 20.1 87.0 86.2 86.6 86.6 85. 80.7 80.8 79.4 79.8 81.4 20.6 21.5 20.6 21.4 18.6 62.7 62.0 62.6 60.7 62.1 60.4 59.0 59.9 60.1 58.2 75.3 75.1 75.5 74.8 74.8 Table 16: Performance of DPO models trained on preference annotations by different LLM judges Figure 12: Effect of different DPO mix on 8B and 70B models: UltraFeedback, Helpsteer2, and our best preference mix. training on our best mix. The improvement is greater for the 70B model (+3.3 vs. +1.8), we hypothesize that this is because UltraFeedbacks completions are mainly sourced from models that are less capable than the 70B model we are starting with. Helpsteer2 Wang et al. [2024d], another high-quality preference dataset, also performs lower than our best mix on the 8B model. Persona Preference Data. From the three persona preference datasets targeting instruction following, coding and math skills, only T칖LU 3 Persona IF improves the average eval score and the targeted IFEval score (see Figure 13). Neither T칖LU 3 Persona Math nor T칖LU 3 Persona Code improve their respective targeted evaluations and slightly harm the average score. We therefore only include the T칖LU 3 Persona IF preferences in our final mix. Targeting IF. We created preference data targeted to improve models precise instruction following skills. 1. Persona IF: We take subset of our collected instruction following SFT dataset, IF-PERSONA-SFT and convert it into preference dataset. Each example in IF-PERSONA-SFT dataset contains (prompt, constraints, response) tuple. We start by rewriting each prompt in the subset to relax one of the given constraints. More specifically, we prompt GPT-4o to generate rewrites such that the new response to the modified prompt is no longer valid response for the original prompt (does not satisfy all the constraints). We then use the response to the new modified prompt as the rejected response, and create (chosen, rejected) pairs to form our IF-PERSONA-PREF dataset containing close to 20K examples. 2. IF-augmented: We randomly sample instructions from the T칖LU 2 SFT mix and combine them with constraints from the taxonomy in Zhou et al. [2023]. The chosen and rejected completions are obtained through the synthetic pipeline in 5.2.1. 3. WildChat IF: We sample instructions from WildChat [Zhao et al., 2024] which contain constraints. For this purpose we asked GPT-4 to extract whether or not prompt includes constraint. For IF-augmented, we run two analyses. We generate an additional set of more than 66k instances and we then run the chosen completions through constraint verifier functions, and only add those instances to the final set which actually fulfilled the constraint(s). This leaves us with cleaned set of about 26k preferences, which we call IF-augmented-verified. In Figure 14 we show that the IF-persona preferences significantly improve IFEval scores beyond the baseline mix, while minimally harming average performance. The IF-augmented-verified dataset improves IFEval performance only by 1 point, while also slightly harming the average performance. Combining IF-persona with IF-augmented-verified leads to the best IFEval performance, but to slightly lower average. We 24 Average IFEval MATH CHU Figure 13: Adding persona preference data to the SFT Reused mix for DPO. Average IFEval Original dataset Regen. using synthetic pipeline Figure 14: Performance of different IF-targeted preference mixes, average and IFEval. Best here consists of our final best mix for the 8B model (minus Persona-IF). Figure 15: Comparing the use of the original completions to regenerating completions using our synthetic preference pipeline. therefore choose to include IF-augmented (not verified) and Persona IF in the final 8B DPO mix, which leads to both satisfiying average and IFEval score. Wildchat. Our ablations show that adding preference data consisting of WildChat prompts and chosen/rejected pairs obtained using our Synthetic Preference Data Pipeline, generally improves DPO performance. Ablations in Figure 5.2.2 reveal that adding WildChat prompts seen during SFT training to the DPO mix leads to better average performance than combining the unused with the reused WildChat prompts. Comparing original preference datasets and their regenerated counterparts. We also investigate whether the preference dataset generated by the synthetic pipeline in 5.2.1 can yield to gains in downstream DPO performance on existing datasets. To do so, we take the prompts from open-source datasets such as Helpsteer2, Ultrafeedback, and MultiPref [Miranda et al., 2024], then regenerate their completions and preference annotations using the synthetic data pipeline. Figure 15 shows that the downstream DPO performance of the regenerated dataset is better than the original dataset, suggesting that the synthetic pipeline itself can yield to performance gains. LR Algorithm - SFT Base 5.00E-07 SimPO 5.00E-07 SimPO 5.00E-07 DPO 1.00E-06 PPO PPO 1.00E-06 DPO-norm 1.00E-07 DPO-norm 5.00E-07 DPO-norm 5.00E-07 DPO-norm 5.00E-07 DPO-norm 5.00E-07 DPO-norm 5.00E-07 洧 洧띻 ratio - 0.5 0.3 - - - - - - - - - 洧띻 - 2 10 0.1 0.0325 0.05 5 10 15 2 5 5 Epochs Batch Size Average Score - 1 1 3 1 1 3 3 3 3 3 1 - 128 128 32 64 64 32 32 32 32 32 55.7 51.8 52.9 55.2 54.5 55.5 56.1 55.2 55.7 46.8 53.4 57.3 Table 17: Hyperparameters and algorithms examined for DPO tuning. We use UltraFeedback as the training dataset in all cases, and train on top of an early T칖LU 3 version. DPO-norm refers to the length-normalized DPO variant proposed in Meng et al. [2024]. We explore hyperparameters suggested by prior work [Meng et al., 2024, Ivison et al., 2023]. For PPO, we train reward models on UltraFeedback and reuse prompts during online training, following the hyperparameters in Ivison et al. [2024]. We find that length-normalized DPO performs best overall. LR 5.0 10-7 2.0 10-7 1.5 10-7 1.0 10-7 5.0 10-7 2.0 10-7 Avg. Performance 72.74 71.17 71.12 71.06 71.14 74.35 Table 18: Learning rate ablations for the 70B DPO model, for two different preference mixes: Mix 1: T칲lu-3-PersonaIF, Tulu-3-Helpsteer2, Ultrafeedback, Tulu-3-SFT-reused (On-policy), Mix 2: Best 70B Mix (both trained on an older SFT base). 5.4 Preference Tuning Recipe and Analyses 5.4.1 Hyperparameter and Algorithm Design Hyperparameter Learning Rate Learning Rate Schedule Batch Size (effective) Max Token Length KL penalty coefficient 洧띻 Warm up ratio Number of Epochs 8B Value 5 10-7 Linear 32 2,048 5 0.1 1 70B Value 2.0e-7 Linear 128 2,048 5 0.1 1 Table 19: Final DPO Training Hyperparameters. We use the length-normalized variant of DPO proposed in Meng et al. [2024]. In light of the significant amount of work on improving DPO and related algorithms since the release of T칖LU 2, we revisited our hyperparameter and algorithm choices alongside our preference datasets. We ablated both algorithm and hyperparameter choices using an early SFT checkpoint and the UltraFeedback dataset. We explored using DPO, SimPO [Meng et al., 2024], and length-normalized DPO. Our results are shown in Table 17. We found that only length-normalized DPO outperformed our base checkpoint overall, and so further tuned it, resulting in the final hyperparameters shown in Table 19. We lowered the learning rate and increased the batch size for the 70B training based on the fact that it is common to lower the learning rate and increase batch size when doing SFT with larger models [Touvron et al., 2023]. Learning Rate Ablations for 70B. We ran small hyperparameter search over set of leraning rates using generally well performing preference data mix13 and our final best mix. Table 18 shows that either learning rate of 2.0 10-7 or 5.0 10-7, depending on data mix, performs better than lower learning rate. For our final DPO models we decided on using learning rate of 2.0 10-7. 13T칲lu-3-Persona-IF, Tulu-3-Helpsteer2, Ultrafeedback, Tulu-3-SFT-Used (On-policy) 26 Hyperparameters Learning Rate Discount Factor 洧 General Advantage Estimation 洧랝 Mini-batches 洧녜mb PPO Update Iterations 洧 PPOs Clipping Coefficient 洧 Value Function Coefficient 洧녫1 Gradient Norm Threshold Learning Rate Schedule Generation Temperature Batch Size (effective) Max Token Length Max Prompt Token Length Penalty Reward Value for Responses without an EOS Token Response Length Total Episodes KL penalty coefficient (洧띻) Warm up ratio (洧랪) for optimizing RM for optimizing against RLVR 3 10-7 3 10-7 1.0 1.0 0.95 0.95 1 1 4 1 0.2 0.2 0.1 0.1 1.0 1.0 Linear Linear 1.0 1.0 512 512 2,048 2,048 2,048 2,048 -10.0 1,024 300,000 [0.05, 0.03, 0.02, 0.01] [0.1, 0.0] -10.0 1,024 (2,048 for MATH) 100,000 [0.1, 0.05, 0.03, 0.01] 0.0 Table 20: The hyperparameters of PPO used for 1) optimizing against general RM and 2) optimizing against the verifiable reward function. The differences between the hyperparameters are highlighted. Comparison Between PPO and DPO We also conducted more in depth ablation study comparing PPO and DPO later in development. We anchored DPO preference mix in the development history to train an RM. We use the same setup as Stiennon et al. [2020], Ouyang et al. [2022], Huang et al. [2024a], we only extract the RMs logits at the end-of-sequence (EOS) token as the reward model. Also, the linear head to output reward scalars is initialized with weights according to (0, 1 (洧녬model + 1)). We use the same prompts in the DPO preference mix to make controlled comparison between DPO and PPO. The reward model was trained only once and we did not attempt to tune the RMs performance. Evaluating RMs performance can be tricky because strong RM performance on RM-specific benchmarks does not necessarily translate to better downstream performance for PPO [Ivison et al., 2024, Chen et al., 2024]. Furthermore, iterating with RM and PPO is more expensive then iterating with DPO, so we decided to do most of our preference tuning experiments via DPO. The hyperparameters for the RM and PPO can be found in Table 31 and Table 20. The results can be found in Figure 16. Here are our findings: 1. PPO Gets Similar Average Scores with DPO in this Non-Tuned Setup Overall, we found that PPO could reach comparable level of performance to DPO (albeit slightly lower) in this controlled setup. 2. PPO is More Computationally Expensive The PPO runtime is roughly 28 hours using two nodes, whereas the DPO runtime is about 4 hours using single node. If we use more computational budget or do more tuning, it is entirely possible that we can push up the PPOs performance even higher. However, given limited resources and the subtlety in RM evaluation, using DPO for preference tuning seems more economical. We decide to use PPO primarily for RLVR, to be introduced in Section 6. 5.4.2 Infrastructure for Scaling DPO To run the 70B DPO training, we found it useful to implement two key optimizations for reducing the GPU footprint of DPO training: 27 Figure 16: The average scores of PPO runs with different learning rate warm-up ratios 洧랪, KL penalty coefficient 洧띻. PPO can get similar (though slightly lower) average scores as DPO. Figure 17: The peak GPU memory allocated can be reduced by caching the reference policys logprobs on the preference dataset and doing forward passes separately for the chosen and rejected pairs. Prompt Dataset Count Verification GSM8K Train MATH Train IF verifiable Total 7,473 7,500 14,973 29,946 Reference Exact match against extracted answer Cobbe et al. [2021] Exact match against extracted answer Hendrycks et al. [2021] Prompt-specific verifiers - Table 21: Summary of our verifiable prompt dataset. New datasets released with T칖LU 3 are color-coded for emphasis. 1. Caching DPO Log Probs To reduce GPU memory usage, we pre-compute and cache log probabilities across the dataset using the initial model, rather than keeping reference DPO model in memory during training like the canonical implementation [von Werra et al., 2020, Rafailov et al., 2024]. This optimization eliminates the need to allocate GPU memory for the reference model. 2. Separate Forward Passes for Chosen and Rejected Sequences The canonical DPO implementation [von Werra et al., 2020, Rafailov et al., 2024] also concatenates the chosen and rejected sequences during the forward pass, effectively doubling the batch size and increasing GPU memory requirements. To save GPU memory, we simply perform the forward passes separately on the chosen and rejected. We empirically validated these two techniques on the Llama 3.1 model and found they resulted in near identical training losses. As expected, the model uses less GPU memory when using the two techniques on an 8xH100, as shown in Figure 17."
        },
        {
            "title": "6 Reinforcement Learning on Verifiable Rewards",
            "content": "In T칖LU 3, we introduce Reinforcement Learning with Verifiable Rewards (RLVR), novel method for training language models on tasks with verifiable outcomes such as mathematical problem-solving and instruction following. RLVR leverages the existing RLHF objective but replaces the reward model with verification function, as shown conceptually in Figure 18. When applied to domains with verifiable answers, such as mathematics and verifiable instruction following tasks [Zhou et al., 2023], RLVR demonstrates targeted improvements on benchmarks like GSM8K while maintaining performance across other tasks. RLVR can be seen as simplified form of existing approaches for bootstrapping LM reasoning [Zelikman et al., 2022, 2024, Hoffman et al., 2023] or simpler form of RL with execution feedback [Gehring et al., 2024], in which we simply use answer matching or constraint verification as binary signal to train the model. While this has been done for improving math skills alone in prior work [Kazemnejad et al., 2024], we further extend RLVR to cover multiple evaluations and test how it can improve overall model performance, integrating it as component of generalist training pipeline. 28 Figure 18: An overview of how Reinforcement Learning with Verifiable Rewards (RLVR) works. We sample completions from policy model given set of prompts, and verify their correctness using deterministic function. If the answer is verifiably correct, we provide reward of 洧띺, otherwise 0. We then train against this reward using PPO. RLVR is based on simple principle, common in RL literature, applied to language models: the policy only receives reward when its generated responses are verifiably correct. More specifically, RLVR optimizes the following objective:14 max 洧랢洧랚 洧댶洧녽洧랢洧랚(洧논) [洧녠RLVR(洧논, 洧녽) ] = ] [洧녺(洧논, 洧녽) 洧띻KL[洧랢洧랚(洧녽洧논)洧랢ref(洧녽洧논)] (7) where 洧녺 is the verifiable reward function. 洧녺 takes in prompt and completion pair (洧논, 洧녽), and checks if the answer is correct within the generated text: 洧녺(洧논, 洧녽) = { 洧띺 if correct, otherwise. 0 (8) We train models with RLVR following preference finetuning, and we use the PPO [Schulman et al., 2017] algorithm to optimize for the RLVR objective. We set 洧띺 = 10 based on pilot experiments and did not tune it further. 6.1 RLVR Data Creating data for RLVR entails obtaining prompts with an accompanying binary verifier (i.e., constructing set of inputs 洧논 with accompanying verifier functions 洧녺). We focus on two domains (mathematics, exact instruction following) and three evaluations (GSM8K, MATH, IFEval) with relatively straightfoward methods for verification, and leave more complex verifiers to future work.15 In practice, the answer extraction and verification method is domain-dependent. We use three sources of training prompts and verifiers: GSM8K. We use the GSM8k training set. We augment each sample with the standard 8-shot prompt using during evaluation to encourage the model to use chain-of-thought, and then extract the final number produced and compare to the ground-truth label to determine correctness. MATH. We use the MATH training set. Similar to GSM8k, we augment each sample with the standard 3-shot CoT prompt used to encourage the model to generate chains of thought during evaluation, and then extract the answer and determine correctness following the flex MATH evaluation logic. IFEval. We randomly sample instructions from the T칖LU 2 SFT mix and combine them with constraints from the taxonomy in Zhou et al. [2023]. We have verification function for each of the constraint templates that is able to verify whether completion satisfies constraint. Given these prompts and verification functions, we then train the models via Proximal Policy Optimization (PPO) [Schulman et al., 2017] on these verifiable rewards. We combine all prompts together, results in mixture of roughly 30,000 prompts with ground truth labels. We summarize our verifiable prompt mixture in Table 21. 14This is very similar to the standard KL-constrained RLHF objective, shown in Eq. 4, with reward function instead of learned reward model. 15For example, recent work has found success in using code execution feedback to train models with RL [Gehring et al., 2024, Xu et al., 2024b]. 29 6.2 RLVR Recipe and Analyses Implementation Details. RL and RLHF have many subtle implementation details that can significantly impact training stability [Engstrom et al., 2020, Huang et al., 2022]. As we use PPO to train our models against our verifiers, we adapt these effective implementation details from Huang et al. [2024a]: 1. Initialize the Value model from General RM. We initialize the value model from the reward model for the RLVR setup (following the standard setup in Ziegler et al. [2019], Ouyang et al. [2022]). 2. Disable Dropout. We set the dropout probability to be 0 during RM and RL training (similar to Ziegler et al. [2019]). This ensures the token log probabilities can be computed deterministically during the forward passes of the policy model and reference model, allowing more accurate estimation of the KL penalty. Furthermore, PPO calculates the token log probabilities in two phases: during the rollout phase and the learning phase. It is important to ensure the token log probabilities match up during these two phases: they produce probability ratio of 1 during the first PPO epoch, so PPO can clip the ratio and apply the surrogate objective properly. If the log probabilities differ drastically due to dropout, all ratios could potentially be clipped, resulting in zero gradient. 3. Train with the SFT Dataset and Shuffle Between Epochs. As pointed out in in Huang et al. [2024a], PPO can train for more episodes than the total available prompts, effectively training for multiple epochs. In our RLVR ablation experiments, we train for roughly 1000007473 13 epochs. We shuffle the prompts in between epochs. For our final runs, we examine model checkpoints every 40-100 steps and choose the best checkpoint on our development evaluation set. 4. Non End-of-Sequence (EOS) Penalty. During training, PPO typically sample fixed amount of maximum tokens. If the sampled response does not end with an EOS token, we give -10 penalty to encourage the model to always complete its responses. 5. Advantage Whitening / Normalization. Like done in standard PPO implementation details literature [Engstrom et al., 2020, Huang et al., 2022, 2024a], we normalize the advantages by subtracting its mean followed by dividing its standard deviation. We start by anchoring DPO model in the development history as the initial model for conducting experiments for RLVR. We conducted several sets of experiments: 1. Individual Tasks. we applied the RLVR recipe on GSM8K, MATH, and IFEval, respectively with sweep of beta values [0.1, 0.05, 0.03, 0.01]. For evaluation, we look at the verifiable rewards, the KL divergence, and the response length. 2. Value Model Initialization Ablation. We experimented with initializing PPOs value model from 1) general reward model and the anchored DPO model, and ran sweep of beta values [0.1, 0.05, 0.03, 0.01, 0.005, 0.001] on the GSM8K task. The general RM is trained with the UltraFeedback dataset [Cui et al., 2023]. For evaluation, we examine the GSM8K test evaluation score and the average scores across all evaluation. 3. Scores from RM Ablations. One possible implementation for RLVR is to add verifiable rewards on top of the scores from the reward model. We launch experiments also using sweep of beta values [0.1, 0.05, 0.03, 0.01, 0.005, 0.001] on the GSM8K task. 4. Starting from Weaker Model. The models base capabilities are also confounding factor. We launch another set of experiments using an SFT model with lower average scores (from which the anchored DPO checkpoints were trained, so they share the same linearage) using beta values [0.1, 0.05, 0.03, 0.01]. Unless otherwise specified, we use the following hyperparameters for PPO training in Table 20. To train reward models, we use the hyperparameters in Table 31. 6.2.1 Key Findings Below are our main findings: RLVR Can Improve Performance in Targeted Domains. Figure 19 shows training with RLVR results in improved test performance for all three settings. In all cases, we achieve models that outperform the initial model in that particular evaluation. We also see that the verifiable rewards (i.e., correctness on the train set) improves consistently for all three settings. Interestingly, in GSM8K and MATH, we found that incurring more KL budget does not necessarily lead to improvements in verifiable rewards. 30 洧띻 = 0. 洧띻 = 0.03 洧띻 = 0.05 洧띻 = 0.1 Figure 19: The top three rows show RLVRs verifiable rewards, KL divergence, and response lengths on the train dataset of GSM8K, MATH, and prompts with constraints, when starting from DPO checkpoint. The bottom row shows the corresponding downstream test performance. RLVR can lead to higher verifiable rewards in the train datasets. Importantly, RLVR can also lead to higher scores in the corresponding test dataset, however, an increase in the average score across all evaluations is not guaranteed. Initializing RLVRs Value Function from General RM Works Best. Figure 21 shows that initializing the value from general RM obtains the highest GSM8K test score and also higher average scores. This suggests the value function plays an important role in RLVRs training. Do Not Use the Scores from RM. Figure 22 shows that using only the verifiable rewards outperforms using scores from the reward model. Training with verifiable rewards with the scores from RM seems to introduce more noise, especially in the average scores. Starting from Weaker Model Can Converge to the Same Verifiable Rewards. Figure 20 shows that starting from both SFT and DPO can lead to the same level of verifiable rewards, but starting from the SFT model would incur larger KL compared to starting from the DPO model. This makes sense because the SFT model is further away from good at GSM8K than the DPO model. However, we find that starting from stronger model usually results in better test set performance. 31 start = DPO, 洧띻 = 0.03 start = SFT, 洧띻 = 0.03 start = DPO, 洧띻 = 0.05 start = SFT, 洧띻 = 0.05 start = DPO, 洧띻 = 0.01 start = SFT, 洧띻 = 0.01 start = DPO, 洧띻 = 0.1 start = SFT, 洧띻 = 0. Figure 20: The comparison of RLVRs performance on GSM8K between starting from DPO checkpoint and starting from weaker SFT checkpoint. We see that starting from both SFT and DPO can lead to the same level of verifiable rewards, but starting from SFT would incur larger KL compared to starting from DPO when using the same 洧띻. Figure 21: The performance of RLVR vs KL Divergence under different value model initialization. Both initializations could improve the models compared to an initial DPO baseline model. However, as the model diverges more from the initial model, overoptimization happens as the average scores drop significantly. Figure 22: Similar to Figure 21, but this is comparison of 1) using scores from on top of the verifiable rewards and 2) using only the verifiable rewards. We found using the verifiable rewards performs better in GSM8K, and using scores and verifiable rewards to be more noisy. Overestimation Happens. As we lower the KL penalty 洧띻, the trained model incurs more KL from the initial model. We observed that more KL divergence typically results in lower average scores, as shown in Figure 21. The exception is Figure 22, where the largest KL corresponds to the highest average score. Furthermore, we showcase overoptimization of prompts with constraints cases in Appendix B.4. 6.3 RLVR Infrastructure Our PPO setup follows best practices on implementation details [Huang et al., 2024a]. To enable our implementation to scale to models up to and above 70B parameters, we also adapted model allocation techniques from existing distributed RLHF frameworks [Hu et al., 2024]. Furthermore, we accelerate throughput by making RL training asynchronous [Noukhovitch et al., 2024]. Distributed Setup. Out PPO infrastructure leverages Zero Stage 3 [Rajbhandari et al., 2020] to fit the models and applicable optimizer states into the memory. In RLVR, we have 3 models: the policy model, the reference policy model, and the value model. The policy and value models need to be trained, but the reference policy model only performs inference. Often inference time is bottleneck in RLHF infrastructure, so we allocate dedicated GPUs to do inference like done in OpenRLHF [Hu et al., 2024]. In particular, we use Ray [Moritz et al., 2018] to allocate dedicated GPUs to run PagedAttention via vLLM [Kwon et al., 2023]. PagedAttention helps reduce GPU memory fragmentation and redundant duplication leveraging virtual memory and paging techniques. As result, it helps our 32 Model Size 8B 70B Category Benchmark(Eval Setting) Llama 3.1 Inst. T칖LU 3 DPO T칖LU 3 RLVR Llama 3.1 Inst. T칖LU 3 DPO T칖LU 3 RLVR Avg. Knowledge MMLU(0 shot, CoT) PopQA(15 shot) TruthfulQA(6 shot) BigBenchHard(3 shot, CoT) DROP(3 shot) MATH(4 shot CoT, Flex) GSM8K(8 shot, CoT) HumanEval(pass@10) HumanEval+(pass@10) IFEval(Strict) AlpacaEval 2(LC % win) Safety6 task avg. Reasoning Math Code IF & Chat Safety 62.2 71.2 20.2 55.1 62.8 61.5 42.5 83.4 86.3 82.9 80.6 24.2 75.2 64.4 68. 29.3 56.1 65.8 62.5 42.0 84.3 83.9 78.6 81.1 33.5 87.2 64.8 68.2 29.1 55.0 66.0 62.6 43.7 87.6 83.9 79.2 82.4 34.5 85.5 73. 85.3 46.4 66.8 73.8 77.0 56.4 93.7 93.6 89.5 88.0 33.4 76.5 75.9 83.3 46.3 67.9 81.8 74.1 62.3 93.5 92.4 88.4 82.6 49.6 89. 76.0 83.1 46.5 67.6 82.0 74.3 63.0 93.5 92.4 88.0 83.2 49.8 88.3 Table 22: Final performance of RLVR-trained T칖LU 3 models compared to Llama 3.1 and DPO starting points. The best-performing model on each benchmark (i.e., in each row) and of each size is bolded. PPO infrastructure run LLM inference using much larger batch size and speed up inference. This setup allowed us to scale PPO policy training to the 70B scale. Asynchronous RL Training. Furthermore, our PPO setup uses asynchronous RL training to improve training efficiency [Noukhovitch et al., 2024]. The existing PPO frameworks [Hu et al., 2024, Shen et al., 2024a] are typically synchronous: the inference GPUs would first collect policy rollout data, and then the learner GPUs would train on that rollout data. This setup would utilize GPU relatively well under typical setup with Atari [Mnih et al., 2015]. However, under the RLHF setup, inference computation typically requires different optimizations (e.g., PagedAttention), thus requiring different GPU memory allocation strategies. As result, synchronous RL training means inference GPUs could be idling while the learner GPUs run, and vice versa. An alternative implementation is to use the same set of GPUs for training and inference, but it could incur additional overhead such as an additional copy of the policy parameters and compiling inference engine in the training GPUs [Shen et al., 2024a]. Our setup allocates GPUs specifically for inference and training, alleviating the need to compile an inference engine and save copy of the policy parameters in the training GPUs. Furthermore, we run the inference computation concurrently with the training computation, thus reducing the GPU idle time. However, asynchronous RL training can introduce stale data, in cases where the inference can generate data much faster than training consumes [Espeholt et al., 2018], which can introduce reproducibility problems [Huang et al., 2023]. To help make training more reproducible, our setup always trains the policy using the second latest inference data [Huang et al., 2023, Noukhovitch et al., 2024]. 6.4 Final Experimental Results Based on the above results, we ran our final RLVR runs using the combined verifiable prompt set, and used the best DPO models from the prior section as starting points. Specifically, at 8B scale, we tested the best overall DPO model and the best model with IFEval persona data mixed in during training, and at 70B scale we use the best overall DPO model. For hyperparameters, for 8B models, we used the hyperparameters from Table 20, but tested higher KL penalty coefficients (up to 0.15) based on previous 8B RL development runs. For 70B models, we used the hyperparameters from Table 20, but with 1 10-7 learning rate, 0.1 warmup ratio, 2048 response length, 400,000 episodes, 640 effective batch size, and 洧띻 = 0.7 based on previous 70B RL development runs. We initialize our value model from reward model trained on the same dataset as the best DPO model (the T칖LU 3 8B preference mixture) starting from T칖LU 3 SFT, using the same hyperparameters as in our ablation experiments  (Table 31)  . We evaluated our models every 100 training steps (40 for 70B), and picked as our final 8B model the checkpoints with best overall performance on MATH and IFEval. We show the logs from RLVR training in Figure 23, and 33 Figure 23: Rewards, KL divergence, and average response length on GSM8k test set over episodes for our final RLVR runs. We mark the point we choose the 8B and 70B checkpoints with green and purple star respectively. Final 8B run Final 70B run compare the final performance against their DPO starting points and Llama 3.1 in Table 22. RLVR results in non-trivial improvements at the 8B scale, improving all three of MATH, GSM8k, and IFEval. In fact, we observed that some 8B runs were able to achieve GSM8k scores of up to 89.4% and IFEval scores of up to 84.8% (although such models tended to perform worse in other metrics, dragging down their overall average). At the 70B scale, we observe more modest improvements in IFEval and MATH, and no improvement in GSM8k, likely due to the fact that it is already close to saturation (93.5%). Surprisingly, we find that our 70B run displays extremely low KL divergence, remaining well below 1 over the duration of run, probably due to the lower learning rate16."
        },
        {
            "title": "7 T칖LU 3 Evaluation Framework",
            "content": "We designed our framework for evaluating T칖LU 3 and the other models we compare against with the following goals: 1) Our evaluations should be reproducible. 2) We should evaluate models generalization to unseen tasks, not just the specific benchmarks we use for development. 3) Our evaluation setup (e.g., templates and strategies for prompting) should be fair to wide range of models. Accordingly, our framework consists of an open evaluation toolkit for reproducible evaluations (Section 7.1), suite for evaluating core skills in instruction-tuned models with separate development (Section 7.2) and held-out evaluations (Section 7.3), and set of recommended settings for evaluating on our evaluation suite that based on our experiments with various models, which we refer to as the T칖LU 3 Evaluation Regime, summarized in Table 23. As described in Section 2.1, we split our evaluation suite into development set and an unseen set, the former used for developing models, and the latter only for evaluating final models. This setup, along with our training data decontamination efforts (see Section 3.2) provide fairer evaluation of our models generalization capabilities for each of the core skills we focus during development. However, in comparisons between our models and other models, we cannot rule out that any closed model has not trained on our evaluation suite, and hence cannot make clear judgments on models that have not publicly released finetuning data. 7.1 Open Language Model Evaluation System (OLMES) In an effort to make evaluations more standardized and reproducible, we are sharing the code base used to produce the evaluations in this work.17 The OLMES evaluation system supports: wide range of models and tasks, leveraging existing work in the Eleuther AI LM Evaluation Harness [Gao et al., 2024] Flexible configuration options for each task Direct access to the specific task formulations used in this work (as well as in earlier work such as OLMo [Groeneveld et al., 2024] and the OLMES standard [Gu et al., 2024]) Detailed instance-level output data for analysis of model predictions, confidences, etc E.g., to reproduce our Llama-3.1-8B-Instruct numbers for MMLU-Pro, one would simply run something like olmes task mmlu_pro::tulu3 model llama3.1-8b-instruct. 16We had attempted using higher learning rate during the initial exploration but found that KL could explode initially and cause non-trivial drop in average scores. 17See http://github.com/allenai/olmes 34 Category Benchmark CoT # Shots Chat Knowledge Recall Reasoning m e Math Coding MMLU PopQA TruthfulQA BigBenchHard DROP GSM8K MATH HumanEval HumanEval+ Instruction Following IFEval Safety AlpacaEval 2 T칖LU 3 Safety Knowledge Recall MMLU-Pro s Reasoning Math Coding GPQA AGIEval English Deepmind Mathematics BigCodeBench Instruction Following IFEval-OOD HREF 0 6 3 3 8 4 0 0 0 0 0 0 0 0 0 0 Multiturn ICL N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A Metric EM EM MC2 EM F1 EM Flex EM Pass@10 Pass@10 Pass@1 (prompt; loose) LC Winrate Average EM EM EM EM (Sympy) Pass@10 Pass@1 (prompt; loose) Winrate Table 23: The T칖LU 3 Evaluation Regime: settings for development (top) and unseen (bottom) portions of the evaluation suite. CoT are evaluations run with chain of thought prompting [Wei et al., 2022b]. #Shots is the number of in-context examples in the evaluation template. Chat refers to whether we use chat template while prompting the model. Multiturn ICL refers to setting where we present each in-context example as separate turn in conversation (applicable only when chat template is used and # Shots is not 0). Average over multiple sub-evaluations full details of the safety evaluation are included in Section 7.2.1. 7.2 T칖LU 3 Evaluation Suite - Development We design the evaluation setup for the development partition of T칖LU 3 EVAL based on existing practices in current literature as well as insights during development. Where appropriate, we adapt the evaluation setup based on the nature of the task and take additional care to robustify our answer extraction and comparison approaches as described below. MMLU [Hendrycks et al., 2020] is heterogeneous with regard to the type of reasoning skills required to answer the questions, containing instances that require basic factual recall as well as those that demand logical reasoning and problem-solving skills. We design zero-shot CoT setting that asks the models to summarize its reasoning before answering the questions (see Table 39 for the prompt used). We find that among the various CoT settings, including prompting the model to think step-by-step and using no CoT, the prompt yields systematic performance improvement over standard 5-shot multiple choice setting across tested models Table 41 and maximizes on the number of subject (knowledge categories) that the CoT benefits. This indicates that our summarize prompt is an effective strategy for dealing with the heterogeneity of the benchmark. See appendix F.2 for details. We compute macro average over all the subjects in MMLU as the final task metric. PopQA [Mallen et al., 2022] is an entity-centric question-answering benchmark that evaluates language models tendency to forget information about long-tail entities. We prompt the models in 15-shot setting (as recommended in the dataset paper) without any additional instructions, with each QA demonstration presented in different conversation turn, setting that we refer to as Multiturn ICL henceforth, and use greedy sampling to obtain model predictions. TruthfulQA [Lin et al., 2021] contains questions that humans tend to answer incorrectly due to common misconceptions. We use the multiple-choice version of this benchmark where the models being evaluated are presented with questions and options containing multiple correct answers (the MC2 setting). 35 HumanEval [Chen et al., 2021] and HumanEval+ [Liu et al., 2023] evaluate models ability to complete Python code given docstrings. HumanEval+ uses more rigorous evaluation procedure than the original HumanEval benchmark with additional tests. We use sample from the models at temperature of 0.8 and use pass@10 as the evaluation metric. GSM8K [Cobbe et al., 2021] contains grade school math word problems. We use the 8-shot chain-of-thought prompt from Wei et al. [2022b], formatted for multiturn ICL evaluation. We obtain model responses using greedy sampling and extract the last number in the model response as the predicted answer. MATH [Hendrycks et al., 2021] contains problems from mathematics competitions spanning various categories such as algebra and calculus. We use 4-shot multi-turn setup with CoT from Lewkowycz et al. [2022], formatted for multiturn ICL, and greedy sampling for model completions. For determining the correctness of predictions, we use flex scheme that attempts to extract the answer in three different ways: (1) following the minerva format [Lewkowycz et al., 2022]; (2) finding the last instance of < 洧녩洧녵洧 > ; (3) taking the text between the last two $ tags. This is due to issues we found during development wherein models would often not follow the correct output format despite the few-shot examples, necessitating varied answer extraction strategy. We find that moving from the minerva format alone to our flex strategy can sometimes improve reported scores by up to 10 points, highlighting the need for this flexible strategy. We compute macro average across the subsections to obtain the final task metric. BigbenchHard [Suzgun et al., 2022] contains challenging reasoning problems for which models benefit from step-by-step reasoning. We follow the setup described in the original paper and use 3-shot CoT prompts, formatted for multiturn ICL. We use greedy sampling for obtaining model predictions. DROP [Dua et al., 2019] is reading comprehension task that requires discrete reasoning. We draw 3 random few-shot examples from the train split following the setup used for Llama 3 [Dubey et al., 2024], and use greedy sampling to get model predictions. IFEval [Zhou et al., 2023] evaluates instruction following ability of models in setting where each instruction corresponds to constraints such that the it can be programmatically verified whether the outputs satisfy those constraints. We use greedy decoding to generate model outputs give the instructions in the dataset, and measure the prompt-level accuracy of satisfying constraints in the loose evaluation setting. AlpacaEval 2 [Dubois et al., 2024] contains set of prompts sourced that reflect real human usages of LMs, and compares model outputs to GPT-4 turbo responses with an additional length control to avoid longer answers being unfairly favoured. We generate responses using greedy decoding up to 8,192 tokens in length, following Ivison et al. [2023]. 7.2.1 Safety Evaluation We follow Han et al. [2024] and Jiang et al. [2024] to define our safety evaluation suite using the following benchmarks18. Each of these benchmarks evaluates whether models refuse to respond to unsafe requests, and in the case of XSTest and WildJailbreak, additionally evaluate whether they comply with benign requests. We use greedy sampling to get model responses for the prompts in each of the benchmarks, and compute the accuracy (at refusal or compliance as appropriate). We report the macro average of the scores over all the benchmarks as our final safety evaluation metric. XSTest [R칬ttger et al., 2023] consists of 200 unsafe prompts and 250 prompts which are safe but superficially resemble unsafe prompts: these prompts use vocabulary similar to that of unsafe prompts. Categories include homonyms, figurative language, safe targets, safe contexts, definitions, real discrimination/nonsense group, nonsense discrimination/real group, historical events, public privacy, and fictional privacy. We report the overall accuracy score based on whether WildGuard [Han et al., 2024] classifies the response as refusal or compliance. HarmBench [Mazeika et al., 2024]. We evaluated on subset of harmful prompts which consists of 321 harmful prompts19 categorized into Functional and Semantic categories. Functional category includes two types of behavior: Standard behaviors, which are modeled after existing datasets of harmful behaviors such as AdvBench and the TDC 2023 Red Teaming Track dataset, and Copyright behaviors, which test the handling of copyrighted content. The semantic category comprises seven types of harmful behaviors: cybercrime, unauthorized intrusion, 18Built on Ai2 Safety Tool, forked to focus only on safety evals: https://github.com/nouhadziri/safety-eval-fork. Note that while WildGuard and WildJailbreak are popular training datasets, we use generations over the test set prompts as test of compliance with response filter such as the WildGuard model. 19https://github.com/centerforaisafety/HarmBench/blob/main/data/behavior_datasets/harmbench_ behaviors_text_test.csv 36 Benchmarks T칖LU 3 8B T칖LU 3 8B T칖LU 3 8B Llama 3.1 8B Ministral 8B Qwen 2.5 7B SFT 98.4 90.4 Instruct 82.8 HarmBench XSTest WildGuardTest Jailbreaktrigger DoAnythingNow WildjailbreakTest Overall 94.7 93.3 98.5 85.5 62.0 78.8 85.5 Table 24: Breakdown of safety scores by benchmark of T칖LU 3 8B models compared with similarly sized open weight models. 92.7 86.2 78.8 45.0 65.6 75.2 99.2 95.8 88.3 86.7 93.1 Instruct 53.4 85.6 68.1 63.3 16.0 50.7 56.2 Instruct 84.1 91.8 85.0 71.0 61.7 56.2 75.0 DPO 94.4 92.4 98.9 87.0 69.7 81.1 87. Benchmarks HarmBench XSTest WildGuardTest Jailbreaktrigger DoAnythingNow WildjailbreakTest Overall 97.8 SFT DPO 98.8 91.1 99. T칖LU 3 70B T칖LU 3 70B T칖LU 3 70B Llama 3.1 Qwen 2.5 Hermes 3 Nemotron Llama 3.1 70B 84.4 92.0 84.9 60.5 36.3 56.05 69.0 Llama 3.1 70B 54.7 89.3 66.6 56.3 26.7 53.8 57.9 70B Instruct 80.6 87.1 81.3 71.0 80.0 59.2 76.5 72B Instruct 86.3 93.6 93.1 89.8 93.3 66.0 87.0 97.8 92.4 98.9 86.7 67.7 86.2 88.3 94.9 99.2 87.0 69.0 86.3 89. 95.3 93.7 88.6 94.4 Table 25: Breakdown of safety scores by benchmark of T칖LU 3 70B models compared with similarly sized open weight models. chemical/biological weapons or drugs, copyright violations, misinformation/disinformation, harassment/bullying, illegal activities, and general harm. We use WildGuard to evaluate the models refusal to assist with harmful prompts. Do-Anything-Now [Shen et al., 2024b] consists of jailbreak prompts that were created by combining the jailbreak templates from DAN with harmful behaviors from HarmBench, and subsample 300 of them for testing. We report accuracy using the WildGuard classifier. JailbreakTrigger [Huang et al., 2024b] incorporates prompts based on 13 distinct jailbreak attack methods. In total, the dataset20 consists of 400 examples, split evenly between two categories: \"Questions about Bad Behaviors\" and \"Instructions to generate Toxic Content\". This dataset serves to evaluate the effectiveness of LLMs defenses and measures the toxicity of responses under jailbreak scenarios. The reported metric is RTA measured by WildGuard. WildJailbreakTest [Jiang et al., 2024] is an adversarial evaluation set which contains subset for adversarial benign queries (210 examples) and subset for adversarial harmful queries (2000 examples). The adversarial benign queries are used to measure models exaggerated safety behaviors and the adversarial harmful queries are used to measure models safeguards regarding adversarial attacks. We measure RTA using WildGuard for both categories. For benign queries, RTA is expected to be () and for harmful queries, the RTA is expected to be (). WildGuardTest [Han et al., 2024] contains 1725 items for prompt harm, response harm, and response refusal classification tasks. 55% are vanilla prompts, and 45% are adversarial. The prompts are collected based on adversarial synthetic data and in-the-wild user-LLM (In-the-wild) interactions. We report RTA using WildGuard. 7.3 T칖LU 3 Evaluation Suite - Unseen For the unseen evaluation suite, the task formulations were decided through an independent design process from that of development suite. One goal of the unseen suite is to evaluate instruction-tuned models in ways that are closely aligned to realistic usage. Specifically, we follow these general principles: 20https://huggingface.co/datasets/TrustLLM/TrustLLM-dataset 37 Model Gemma 2 9B Inst Gemma 2 9B Inst-SimPO Llama 3.1 8B Inst Llama 3.2 3B Inst Ministral 2410 8B Inst OLMo 0724 7B Inst OLMoE 0924 1B 7B Inst Qwen 2.5 7B Inst Tulu 2 DPO 7B Llama 3.1 prompt 51.6 52.6 49.2 39.1 43.8 26.1 20.7 56.2 25. Ours 52.6 51.8 48.7 39.7 44.3 22.9 20.3 54.2 22.2 Table 26: Comparing evaluating on MMLU-Pro using our 0-shot CoT prompt and the 5-shot prompt used in Llama3.1 evaluations, using macro average over tasks in both cases (the Llama3.1 evaluation used micro average). Model Gemma 2 9B Inst Gemma 2 9B Inst-SimPO Llama 3.1 8B Inst Llama 3.1 70B Inst Llama 3.2 3B Inst Ministral 2410 8B Inst OLMo 0724 7B Inst OLMoE 0924 1B 7B Inst Qwen 2.5 7B Inst Tulu 2 DPO 7B Llama 3.1 prompt 35.7 35.0 29.5 46.2 33.5 31.0 27.2 24.6 32.1 28.4 Ours 35.5 35.7 29.5 44.0 27.7 31.5 27.9 24.8 36.8 27.5 Table 27: Comparing evaluating on GPQA using our 0-shot CoT prompt and the prompt in Llama3.1. Formulate tasks similar to how humans interact with the models. E.g., avoid few-shot examples presented as dialog, or precise chain-of-thought (CoT) examples for how the model is supposed to think. Prompt models with clear instructions that set the context, encourage concise reasoning, and specify how the final answer should be formatted. Apply reasonable heuristics for answer extraction and comparison to gold answers, to avoid penalizing models that do not exactly follow syntax implied by the instructions (based on examining outputs from wide set of existing baseline models). We first apply these principles to some of the tasks in the development suite using set of exploratory models (instruction-tuned models predating T칖LU 3). On the exploratory models, we find that following the above principles, in particular being more aligned to how human usage, generally does not degrade performance and often times allow most models to perform better on tasks (despite, e.g., removing few-shot examples). We did not update the formulation of the development tasks based on this, but carried the principles over to formulating the unseen tasks. More detailed analysis can be found in Appendix F.2. The task formulations in the T칖LU 3 unseen suite are as follows. For all benchmarks which include multiple sub tasks, we compute the average over the sub tasks (i.e., the macro average) to be consistent. AGIEval English [Zhong et al., 2024] includes the English language subset of the AGIEval benchmark, specifically these multiple-choice tasks: aqua-rat, logiqa-en, lsat-ar, lsat-lr, lsat-rc, sat-en, sat-math, and gaokao-english. We do not include the sat-en-without-passage task as we find these questions are typically severely underspecified without access to the passage. We formulate the task using the simple zero-shot CoT prompt which encourages concise reasoning ending with clearly stated answer choice. In Appendix F.2, Figure 41, we provide the 0-shot reasoning prompt, used for the multiple-choice tasks in the unseen evaluation suite, including AGIEval English. 38 Model Gemma 2 9B Inst Gemma 2 9B Inst-SimPO Llama 3.1 8B Inst Llama 3.2 1B Inst Llama 3.2 3B Inst Ministral 2410 8B Inst OLMo 0724 7B Inst OLMoE 0924 1B 7B Inst Qwen 2.5 7B Inst Tulu 2 DPO 7B Llama 3.1 Tulu 2 8B Llama 3.1 Tulu 2 DPO 8B base-adapted, in context examples 18.0 19.3 20.0 11.6 19.2 18.8 3.2 9.0 21.2 9.6 21.7 18.6 CoT prompt 45.9 45.3 39.4 13.1 32.6 36.7 5.8 4. 54.7 6.0 13.6 14.5 Table 28: Comparing evaluating on DeepMind Mathematics using different prompts designed evaluation practices for instruction-tuned models, with minor variants. Bolded numbers indicate cases where applying principles more aligned with real usage leads to better performance on models. In our evaluation suite, the chain of thought (CoT) prompt is referred to as chat-v3. The models answer choice is extracted by first matching to the requested format, with fallback patterns if the format was not followed precisely. Specifically, we first look for the exact phrase indicated in the prompt (Therefore, the answer is [ANSWER]) and grab the last such match. If that fails, we look for sequence of softer variants, like answer is [ANSWER] or answer: [ANSWER] before falling back to the last letter in parenthesis found, and if that fails, the last stand-alone capital letter. MMLU-Pro [Wang et al., 2024b] is 10-way multiple-choice extended version of the MMLU dataset. We use essentially the same prompt and answer extraction as used for our AGIEval setup, just adjusting for the number of answer choices. We generally find our formulation to be as effective as the traditional 5-shot CoT prompt (e.g., used in Llama 3.1 evaluations [Dubey et al., 2024]) despite being much shorter, more realistic, and easier for users to create (see Table 26 for comparison across exploratory models). GPQA [Rein et al., 2023] is set of very challenging multiple-choice questions written by domain experts in biology, physics, and chemistry. We use the same zero-shot prompt and answer extraction as for AGIEval. This is similar to the approach used in Llama 3.1 evaluations, but with less prescription on how the reasoning should be structured. Figure 27 shows that across exploratory models, that our approach leads to comparable scores. Deepmind Mathematics [Saxton et al., 2019] is dataset of 56 categories of math questions, testing mathematical and algebraic reasoning skills. We devised zero-shot CoT prompt that sets the context of the task, explains the format expected for the answers (e.g., x**2 for powers), and for each category included three example answers to specify the answer format. The prompt used and answer extraction heuristics were refined by examining example outputs from our exploratory models. As seen from Table 28, applying evaluation principles aligned with real usage (chat versions), often performs better than using setup involving presenting few-shot examples adapted from base model evaluations (base-adapted). We provide the zero-shot reasoning prompt for the Deepmind Mathematics task in Appendix F.2, Figure 42. Aligned with the instructions for formatting in the prompt, when extracting the answer, we first look for an answer in the format Therefore, the final answer is [answer]. hope it is correct. We look for this in case-insensitive way, process the [answer] by stripping away any trailing period, and known math delimiters surrounding the answer (e.g., $ ) based on outputs from our exploratory models. For answer comparison, we first compare the raw strings of the processed generated answer and gold answer to check if they are equal. In addition, we parse both using the SymPy [Meurer et al., 2017] package and compare the equivalence of the parsed outputs to check their mathematical equivalent. We also consider the generated answer to be correct if it is equal to the parsed gold answer after this parsing. BigCodeBench [Zhuo et al., 2024] is set of coding challenges. We focus on the hard subset of 148 (out of the total 1140) instances, using the instruct formulation of each task and the calibrated score. We follow the setup used for the original leaderboard for our implementation. 39 Instruction Group count Instruction person_names format ratio sentence words custom emoji stop_words keyword alphabet csv_special_character Description Mention at least {N} different person names in the response. Please use an emoji at the end of every sentence. Ensure that stop words constitute no more than {percentage}% of the total words in your response. The response must include keyword {keyword} in the {N}-th sentence. Each word in your response must start with the next letter of the alphabet, looping back to after Z. Generate CSV data: The column names are [\"ProductID\", \"Category\", \"Brand\", \"Price\", \"Stock\"], the data should be comma delimited. Please generate 14 rows. Add one field which contains special character and enclose it in double quotes. Table 29: Examples of IFEval out-of-distribution constraints. Constraints are added to an unseen WildChat prompt to form the final prompt except for in the \"custom\" instruction group. complete list of constraints is provided in Appendix F.3. 7.3.1 New Evaluation: IFEval-OOD In order to test precise instruction following abilities of LLMs and whether they are able to follow constraints that go beyond the 25 constraints included in IFEval [Zhou et al., 2023], we developed IFEval-OOD (IFEval Out-of-Distribution). IFEval-OOD consists of 52 constraints across six broad categories such as the examples in Table 29. complete list of the constraints can be found in Appendix F.3. portion of the constraints were sourced by asking group of researchers for constraint ideas and others were written by authors of this paper. One of the six categories (custom) comprises manually written verifiable prompts to cover specific sub-skills, such as CSV generation. For the remaining five categories, the verifiable constraint was combined with 10 unseen prompts from WildChat. To select the final prompts, these constrained prompts were human annotated for quality and compatibility with the constraint (e.g. prompt to paraphrase one-sentence reference text that doesnt contain any names to begin with would not be compatible with constraint to mention at least 23 different person names in the response). To ensure constraint coverage, additional unseen WildChat prompts were manually paired with constraints that did not have at least five selected prompts. Our evaluation dataset emphasizes broad coverage of constraint types to differentiate constraint-following abilities from overfitting on the existing IFEval constraints. 7.3.2 New Evaluation: HREF We constructed an automatic evaluation of instructability of language models, called Human Reference-guided Evaluation of instruction Following (HREF). HREF focuses on 11 instruction following tasks that language models are typically trained on, namely, Brainstorming, Open QA, Closed QA, Extraction, Generation, Rewriting, Summarization, Classification, Numerical Reasoning, Multi-document Synthesis, and Fact Checking. We obtained high quality human-written prompts and responses in these categories from professional instruction-tuning data creators, and developed reliable automatic evaluation procedure. Following AlpacaFarm [Dubois et al., 2023], we use win-rate against fixed baseline model as the evaluation metric. However, since we have larger number of tasks in our benchmark than AlpacaFarm, and also because we have access to human-written references, we hypothesized that the setup used for win-rate computation may not be directly applicable to our new evaluation. So we experimented with various win-rate computation methods, and their details along multiple dimensions: Choice of LM Judge. We tried GPT-4, GPT-4 turbo, and Llama3.1-Instruct models at 7B and 70B sizes as LM judges. LM-as-a-Judge vs. Direct Comparison with Human References. Using large language model e.g., GPT-4 Turbo for AlpacaFarm, may not be appropriate for tasks where the responses are expected to be short and factual. We compared this setup with directly comparing the similarity of the responses from the target and the baseline models with the human-written references, according to simpler embedding model (RoBERTa-Large [Liu, 2019]). 40 Skill Benchmark(eval) Avg. Knowledge Recall GPQA(0 shot, CoT) Reasoning Math Coding Instruct Following MMLU Pro(0 shot, CoT) AGIEval English(0 shot, CoT) DeepMind Math(0 shot, CoT) BigCodeBench-Hard(Pass@10) IFEval OOD(Prompt loose) HREF(Winrate) Llama 3.1 8B Instruct 37.5 29.2 49.1 64.2 39.3 15.5 26.4 38.5 Hermes 3 Llama 3.1 8B 30.5 32.8 40.9 58.1 28.3 9.5 18.0 26. T칖LU 3 8B 33.1 32.8 44.3 59.3 35.4 7.4 19.7 32.7 Llama 3.1 70B Instruct 51.4 46.9 68.3 77.8 62.4 26.4 32.7 45.6 Hermes 3 Llama 3.1 70B 42.8 42.4 60.3 73.3 50.0 14.2 22.5 36.8 T칖LU 3 70B 46.4 45.1 65.8 75.0 49.8 21.6 25.4 42. Table 30: Evaluation of selection of open-weight and T칖LU 3 models on our unseen evaluation suite. It is important to note that without open training data for any of the other models that we cannot verify that they are not training on any of the unseen benchmarks. Use of Human-Written References When Evaluating with LM-as-a-Judge. Since we have access to humanwritten references, we experimented with including those as additional context while prompting the LM judge. To make decisions about the evaluation setup, we collected human judgments comparing responses from diverse set of 16 models, when prompted using the instructions taken from subset of our evaluation dataset. We collected four human judgments per each model response pair, and compared the agreement of each evaluation setup with majority human judgments. We repeated this procedure for each task category to identify the best setup for that category. Summary of the Final Evaluation Setup. We used Llama 3.1 70B Instruct as our LM judge since its agreement with human judges was the highest of the pool of models we considered. Our baseline model we used is Llama 3.1 405B Instruct. We found that using LM as judge results in higher human agreement (compared to the embedding-based method) in all subtasks except Open QA and Fact Checking. We use embedding similarity with human-written references as the way of computing win-rates in these two subtasks. Of the remaining 9 subtasks, evaluation in Brainstorming and Summarization did not benefit from using human-written references while prompting the LM judge, and hence prompt LM judges with human references only for the remaining 7 subtasks. On the subset for which we collected human judgments, our composite evaluation procedure resulted in an agreement of 69.4% with humans, which is comparable to the inter-human agreement of 67%. 7.4 Evaluation on the Unseen Suite To evaluate how much and in what ways we over-fit to our development evaluations, we measure the performance of our models on unseen tasks that still correspond to the same set of core skills. Table 30 shows comparison between T칖LU 3 models, Llama 3.1 Instruct models, and Hermes 3 Llama 3.1 models at 8B and 70B sizes, and Table 43 shows subtask-level breakdown of the performance of these models on HREF. It is important to note that while all these evaluations are unseen for the T칖LU 3 models, we do not know if GPQA, MMLU-Pro, AGIEval, DeepMind Math, and BigCodeBench were used for developing the two other models. We summarize below our key takeaways from this comparison and from our qualitative analysis of the outputs of T칖LU 3 models on these datasets: T칖LU 3 generalizes well to unseen evaluations. In almost all the evaluations, T칖LU 3s performance is generally comparable to that of the two other models we evaluate, often falling between the performance numbers of the two models. This suggests that our recipe of choosing representative evaluations for each core-skill and curating training datasets targeting those evaluations can lead to models that generalize well to other tasks that require the same skills. Training for MATH does not transfer well to Deepmind Math. One difference between MATH, which is part of our development evaluations and DeepMind Math, which is in our unseen evaluations is that the former often requires solutions and answers to be output in LaTeX format, while the latter does not. We found that our trained models have the tendency to format the chain-of-thought reasoning and the final answers in LaTeX even for the questions in the Deepmind Math dataset where this is not required. This often interfered with the intermediate reasoning, and also made our answer extraction logic fail. We hypothesize that this is consequence of models over-fitting to the formatting in training datasets that tend to be particularly helpful for MATH. 41 Models generally overfit to IFEval. We find that there is significant difference between performance on IFEval and IFEval-OOD of all the models, even though we created the latter to be structured very similar to the original dataset, just with disjoint set of constraints. We observe that instruction following with verifiable constraints is challenging skill for models to learn effectively, and hypothesize that those models that do well on IFEval are likely overfitting to the specific set of constraints included in the dataset, making it hard for them to generalize to new constraints. Generalization on knowledge recall may be dependent on the post training recipes. As one might expect, the performance of models on MMLU and MMLU-Pro is correlated. We see that their performance on GPQA shows different trend all three models we compare here are post-trained from the same base model, suggesting that the post-training recipe may affect the generalization in knowledge-recall. Instruction following performance varies across categories. We observe that the relative performance of T칖LU 3 models on AlpacaEval is different from that on HREF. This may be explained by the fact that instruction following is highly diverse task, and the distributions of HREF and AlpacaEval may differ, with some categories of instructions not necesarily transferring well to others, leading to the shift in relative performance. We do note that T칖LU 3 70B outperforms Llama 3.1 70B Instruct on 5 out of 11 subtasks, as seen in Table 43 in the Appendix, which shows breakdown of model performance across subtasks in HREF. Future work will explore how different behaviors of instruction following can be measured with more diverse instruction following evaluations."
        },
        {
            "title": "8 Discussions",
            "content": "8.1 Insights from the Unfruitful In this section, we discuss number of methods and approaches we considered for T칖LU 3 but did not ultimately make it into our final recipe for variety of reasons. Online DPO. Standard DPO methods use preference datasets that are usually collected ahead of time, often from distinct language model, and are thus considered as offline. In other words, with DPO, the policy cannot obtain feedback over it own generations during training. This is in contrast to online methods like PPO where the RM provides online feedback to generations from the policy 洧랢洧랚 being trained. To mitigate the distributional shift issue, recent works proposed Online DPO [Guo et al., 2024] following three-step process: (1) sample 2 responses to prompt from the current policy; (2) obtain online feedback over the response pair to create pairwise data, and (3) use this pairwise data update the policy 洧랢洧랚 via standard DPO loss. While the original paper proposed using online AI feedback for step 2, to better scale our experiments, we obtain feedback from trained reward model. We tried online DPO both to enhance general, and target capabilities, i.e., mathematical reasoning. For general capabilities, we train an RM for 1 epoch using the 82K preference data points from the Skywork.21 For targeting mathematical reasoning, we continue training the same RM on our synthetic on-policy math-specific preference data (described in 5.2.1). Training online DPO on top of one of our T칖LU 3 DPO checkpoint for total of 200K episodes on math problems (prompts are taken from the same RM training data), resulted in no or little improvement on GSM8K and degradation on MATH performance (we experimented with various sampling temperatures and KL penalty coefficients). We did not further explore this approach extensively, as our initial results suggested limited gains in both general and targeted domains. Future work could investigate alternative training strategies, such as different sampling methods, or finetuning RM architectures, to better align the optimization process with the desired capabilities. Rejection Sampling. Rejection sampling for large language models is an increasingly popular method for improving post-training performance of frontier language models [Dubey et al., 2024, Adler et al., 2024, Dong et al., 2023]. Using an initial SFT and preference data mix to train an initial model, that model is used to then generate responses to each SFT prompt. These responses, in addition to the original response, are then ranked using reward model or an LLM as judge, and the best response is kept. The other responses can then be used to create chosen/rejected pairs for preference optimization. The full post-training pipeline is then run on these datasets, and the process repeats until performance converges. We tried rejection sampling, but found that for our setup the performance gains were minimal for the amount of compute required, and thus we leave deeper exploration for future work. Qualitatively, we found that strong judges are vital, and publicly available models can struggle to choose the best response out of the candidates. We also found that including the original response as choice for the judge (in other words, choosing the best response 21https://huggingface.co/datasets/Skywork/Skywork-Reward-Preference-80K-v0.1 42 out of the generations in addition to the original response) performed much better than only choosing from the newly generated responses. 8.2 Future Work While we aimed to cover broad set of skills for T칖LU 3, due to various limitations, we left some skills for future iterations of T칖LU. We discuss some of these skills below: Long Context and Multi-turn. Currently, the data collected for T칖LU 3 is relatively short and does not contain long multi-turn data (the average number of turns in our mixture is 2.4 turns and majority of samples are under 2,048 tokens in length). However, long-context has been popular area of focus in recent work [Pawar et al., 2024], as improving the context window of LMs enables new use-cases [Gemini Team, 2024] and more in-context examples, potentially improving performance [Agarwal et al., 2024]. Relatedly, improving multi-turn capabilities can better improve end-user experience, with non-trivial number of real-world user conversations with LMs going over 2 turns [Zhao et al., 2024]. We hope to address both skills in the future with dedicated training and evaluations. Multilinguality. We specifically focus on English data and evaluations for T칖LU 3 (although we do include the multilingual Aya [칖st칲n et al., 2024] dataset due to its high quality). However, this neglects the myriad languages apart from English spoken around the world, speakers of which may benefit from or desire LMs that can process their languages. Future work may examine the current multilingual abilities of T칖LU 3 and how to further improve them. We also note that multilingual post-training can make use of different techniques to monolingual post-training for example, cross-lingual alignment [Wu et al., 2024] or careful data balancing strategies [Li et al., 2024b]. This makes multilingual post-training an interesting and impactful area for future work. Tool Use and Agents. While we evaluate T칖LU 3 on its own, LMs are being increasingly deployed as parts of larger systems, in which they have access to tools [Qu et al., 2024] or are themselves part of larger agent framework.22 Furthermore, training models to use tools is natural way to dramatically improve their reasoning and mathematical skills [Gou et al., 2024], rather than trying to accomplish everything in the weights. Future work involves training or evaluating T칖LU 3 for tool-use either on its own or as part of larger framework."
        },
        {
            "title": "9 Related Work",
            "content": "9.1 The Evolution of Post-training Recipes Modern post-training has its roots in multi-task language model training, in particular instruction tuning [Mishra et al., 2022, Wei et al., 2022a, Sanh et al., 2022, Wang et al., 2022b, Longpre et al., 2023], in which language models are trained on samples including task instructions and their corresponding responses, allowing the models to generalize zero-shot to new tasks. Early instruction-tuning datasets tended to focus on more traditional NLP tasks (e.g., natural language inference) rather than more generic tasks that downstream users might perform [Wang et al., 2022a]. With the rise of ChatGPT and chat-based LMs (Claude, Gemini, etc), post-training techniques evolved beyond instruction tuning to include preference tuning stages, with models undergoing both instruction tuning and then preference tuning (PreFT) or RLHF [Ouyang et al., 2022]. Early work in RLHF originated from experiments on Deep RL for control [Christiano et al., 2017, Ibarz et al., 2018, Leike et al., 2018] and typically involves first learning reward model from human preferences, and then optimizing language model via an RL framework using the learnt reward [Stiennon et al., 2020, Nakano et al., 2021, Askell et al., 2021, Ouyang et al., 2022]. Recently, approaches that allow directly training language model on such preferences have been developed [Rafailov et al., 2024, Zhao et al., 2023], reducing the complexity of incorporating PreFT into training. While early approaches to PreFT were extremely human-centric, using tens or hundreds of thousands of human-written instructions and human preference labels, more recent work uses mixtures of human and synthetically generated preference data, along with multiple rounds of training and varied training algorithms [Touvron et al., 2023, Dubey et al., 2024, Gunter et al., 2024]. During the evolution of RLHF primarily in closed laboratories, open recipes for post-training have lagged somewhat behind. Initial attempts at building open post-training recipes focused on the instruction-tuning stage [Taori et al., 2023, Conover et al., 2023], finetuning openly released language models on synthetically generated or human made datasets. While combining these datasets could yield strong performance [Wang et al., 2023], incorporating PreFT stage proved important for bridging the gap with closed models based on human evaluations [Ivison et al., 2023]. Today, most popular adapted models with open recipes for PreFT use DPO (or one of its variants), and 22For example, the OpenHands platform [Wang et al., 2024a]. AI feedback data including T칖LU 2 [Ivison et al., 2023], Zephyr-洧띻 [Tunstall et al., 2023], and Starling [Zhu et al., 2023]. However, many of these models are outdated relative to closed post-training recipes both in terms of data and in performance: no open-recipe models exist in the top 50 of LMSYSs ChatBotArena (as of November 20th, 2024) have released their post-training data [Chiang et al., 2024], and most open recipes use relatively little data and few rounds of training compared to closed post-training setups, which can involve multiple rounds of training with varied objectives, and millions of datapoints [Touvron et al., 2023, Dubey et al., 2024]. For instance, Llama 3.1 trained on generated outputs from the previous model for multiple rounds with extensive human feedback data, and used strong models to write synthetic instructions [Dubey et al., 2024]. Other recent developments includes rejection sampling for synthetic data and advanced reward modeling for step-wise assistant responses. Whilst we do not entirely reach the size of these closed recipes, in this work we hope to push the state of open post-training recipes forward by building recipe that matches or beats strong closed recipes, and releasing all related artefacts (code, models, data, etc) for further scientific study and use. Our pipeline is significantly larger and more complex than prior work, comprising of almost million instruction tuning samples, hundreds of thousands of preference pairs, and novel online RL training phase. 9.2 Training on Verifiable Rewards The RLVR approach proposed in this work relates to variety of recent work on improving LM reasoning through RL-related techniques. Closely related is the self-taught reasoner (STaR) line of work [Zelikman et al., 2022, 2024] and TRICE [Hoffman et al., 2023], both of which examine using existing ground-truth answers as signals to generate better model rationales (or chains-of-thought). STaR can be seen as an approximation of policy gradient algorithm, with Quiet-STaR extending the approach of training the model to use additional generations to improve generic language modelling (thinking before speaking). TRICE [Hoffman et al., 2023] also aims to improve the likelihood of correct answers by training over multiple reasoning traces, using custom MCMC-based EM algorithm. More recently, VinePPO [Kazemnejad et al., 2024] uses binary rewards from GSM8k and MATH correctness to test novel PPO-based algorithm, and other recent work has explored using code feedback as signal for training [Gehring et al., 2024, Xu et al., 2024b]. In contrast, our proposed approach, RLVR simply uses an existing RL framework (PPO) for training, and runs entirely online with binary rewards (compared to the iterative approach of STaR or the log-likelihood rewards of Quiet-STaR). Additionally, we expand beyond the math domain, also finding that this approach can yield improvements in precise instruction following. Finally, we also carefully ablate number of core components of RLVR, including value model initialization and using general reward model with verifiable rewards. We hope to further develop and expand this technique in future work."
        },
        {
            "title": "10 Conclusion",
            "content": "We introduce T칖LU 3, family of fully open state-of-the-art language models, featuring modern post-training framework with fully open-source data T칖LU 3 DATA, evaluation T칖LU 3 EVAL, training code T칖LU 3 CODE and development recipes T칖LU 3 RECIPE. We release final models trained on Llama 3.1 base versions, with intermediate checkpoints, training data, training code, and evaluation code. T칖LU 3 bridges the gap between open and closed post-training methods, marking new milestone in open posttraining research. With the resources provided, others can build on open base models, finetune them for high performance across diverse tasks. This paves the way for advancing post-training research within multi-objective, and multi-stage training frameworks."
        },
        {
            "title": "Acknowledgments",
            "content": "We thank John Schulman for extremely useful advice. We acknowledge the National Artificial Intelligence Research Resource (NAIRR) Pilot and Microsoft Azure for contributing to the results in this work. We thank Niklas Muennighoff for helping with some experimentation on OLMoE. We thank countless members of Ai2 and UW NLP communities for useful feedback throughout this project. Research supported with Cloud TPUs from Googles TPU Research Cloud (TRC). We thank Huy Tran, Jesse Dodge, Jiacheng Liu, Sruthi Sreeram, Taylor Blanton, Aaron Sarnat, Arnavi Chheda, Byron Bischoff, Chris Newell, Michael Schmitz, Sam Skjonsberg, Eric Marsh, Karen Farley, and YenSung Chen for building the Ai2 Playground for model demos. We also thank these others at Ai2 for many indirect contributions to the project: Taira Anderson, Jen Dumas, Crystal Nam, Sophie Lebrecht, Brooke Vlahos, Chris Wilhelm, Jenna James, Alex Buraczynski, Will Smith, Caitlin Wittlif, Carissa Schoenick, and Ali Farhadi. 44 References B. Adler, N. Agarwal, A. Aithal, D. H. Anh, P. Bhattacharya, A. Brundyn, J. Casper, B. Catanzaro, S. Clay, J. Cohen, et al. Nemotron-4 340b technical report. arXiv preprint arXiv:2406.11704, 2024. R. Agarwal, A. Singh, L. M. Zhang, B. Bohnet, L. Rosias, S. C. Chan, B. Zhang, A. Anand, Z. Abbas, A. Nova, J. D. Co-Reyes, E. Chu, F. Behbahani, A. Faust, and H. Larochelle. Many-shot in-context learning. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https://openreview. net/forum?id=AB6XpMzvqH. M. AI. Mistral introduces nemo, 2024. URL https://mistral.ai/news/mistral-nemo/. Accessed: 202411-21. E. Almazrouei, H. Alobeidli, A. Alshamsi, A. Cappelli, R. Cojocaru, M. Debbah, E. Goffinet, D. Heslow, J. Launay, Q. Malartic, B. Noune, B. Pannier, and G. Penedo. Falcon-40B: an open large language model with state-of-the-art performance. 2023. Anthropic. Introducing computer use, new claude 3.5 sonnet, and claude 3.5 haiku. Anthropic, 2024. URL https://www.anthropic.com/news/3-5-models-and-computer-use. Accessed: 2024-10-22. A. Askell, Y. Bai, A. Chen, D. Drain, D. Ganguli, T. Henighan, A. Jones, N. Joseph, B. Mann, N. DasSarma, et al. general language assistant as laboratory for alignment. arXiv preprint arXiv:2112.00861, 2021. Y. Bai, S. Kadavath, S. Kundu, A. Askell, J. Kernion, A. Jones, A. Chen, A. Goldie, A. Mirhoseini, C. McKinnon, et al. Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073, 2022. E. Beeching, S. C. Huang, A. Jiang, J. Li, B. Lipkin, Z. Qina, K. Rasul, Z. Shen, R. Soletskyi, and L. Tunstall. Numinamath 7b tir. https://huggingface.co/AI-MO/NuminaMath-7B-TIR, 2024. F. Brahman, S. Kumar, V. Balachandran, P. Dasigi, V. Pyatkin, A. Ravichander, S. Wiegreffe, N. Dziri, K. Chandu, J. Hessel, et al. The art of saying no: Contextual noncompliance in language models. arXiv preprint arXiv:2407.12043, 2024. Z. Cai, M. Cao, H. Chen, K. Chen, K. Chen, X. Chen, X. Chen, Z. Chen, Z. Chen, P. Chu, X. Dong, H. Duan, Q. Fan, Z. Fei, Y. Gao, J. Ge, C. Gu, Y. Gu, T. Gui, A. Guo, Q. Guo, C. He, Y. Hu, T. Huang, T. Jiang, P. Jiao, Z. Jin, Z. Lei, J. Li, J. Li, L. Li, S. Li, W. Li, Y. Li, H. Liu, J. Liu, J. Hong, K. Liu, K. Liu, X. Liu, C. Lv, H. Lv, K. Lv, L. Ma, R. Ma, Z. Ma, W. Ning, L. Ouyang, J. Qiu, Y. Qu, F. Shang, Y. Shao, D. Song, Z. Song, Z. Sui, P. Sun, Y. Sun, H. Tang, B. Wang, G. Wang, J. Wang, J. Wang, R. Wang, Y. Wang, Z. Wang, X. Wei, Q. Weng, F. Wu, Y. Xiong, C. Xu, R. Xu, H. Yan, Y. Yan, X. Yang, H. Ye, H. Ying, J. Yu, J. Yu, Y. Zang, C. Zhang, L. Zhang, P. Zhang, P. Zhang, R. Zhang, S. Zhang, S. Zhang, W. Zhang, W. Zhang, X. Zhang, X. Zhang, H. Zhao, Q. Zhao, X. Zhao, F. Zhou, Z. Zhou, J. Zhuo, Y. Zou, X. Qiu, Y. Qiao, and D. Lin. Internlm2 technical report, 2024. X. Chan, X. Wang, D. Yu, H. Mi, and D. Yu. Scaling synthetic data creation with 1,000,000,000 personas. arXiv preprint arXiv:2406.20094, 2024. M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. de Oliveira Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman, A. Ray, R. Puri, G. Krueger, M. Petrov, H. Khlaaf, G. Sastry, P. Mishkin, B. Chan, S. Gray, N. Ryder, M. Pavlov, A. Power, L. Kaiser, M. Bavarian, C. Winter, P. Tillet, F. P. Such, D. Cummings, M. Plappert, F. Chantzis, E. Barnes, A. Herbert-Voss, W. H. Guss, A. Nichol, A. Paino, N. Tezak, J. Tang, I. Babuschkin, S. Balaji, S. Jain, W. Saunders, C. Hesse, A. N. Carr, J. Leike, J. Achiam, V. Misra, E. Morikawa, A. Radford, M. Knight, M. Brundage, M. Murati, K. Mayer, P. Welinder, B. McGrew, D. Amodei, S. McCandlish, I. Sutskever, and W. Zaremba. Evaluating large language models trained on code. 2021. Y. Chen, D. Zhu, Y. Sun, X. Chen, W. Zhang, and X. Shen. The accuracy paradox in rlhf: When better reward models dont yield better language models. arXiv preprint arXiv:2410.06554, 2024. W.-L. Chiang, Z. Li, Z. Lin, Y. Sheng, Z. Wu, H. Zhang, L. Zheng, S. Zhuang, Y. Zhuang, J. E. Gonzalez, I. Stoica, and E. P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. URL https://lmsys.org/blog/2023-03-30-vicuna/. W.-L. Chiang, L. Zheng, Y. Sheng, A. N. Angelopoulos, T. Li, D. Li, H. Zhang, B. Zhu, M. Jordan, J. E. Gonzalez, et al. Chatbot arena: An open platform for evaluating llms by human preference. arXiv preprint arXiv:2403.04132, 2024. 45 P. F. Christiano, J. Leike, T. Brown, M. Martic, S. Legg, and D. Amodei. Deep reinforcement learning from human preferences. Advances in neural information processing systems, 30, 2017. H. W. Chung, L. Hou, S. Longpre, B. Zoph, Y. Tay, W. Fedus, Y. Li, X. Wang, M. Dehghani, S. Brahma, et al. Scaling instruction-finetuned language models. Journal of Machine Learning Research, 25(70):153, 2024. K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano, C. Hesse, and J. Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. M. Conover, M. Hayes, A. Mathur, J. Xie, J. Wan, S. Shah, A. Ghodsi, P. Wendell, M. Zaharia, and R. Xin. Free dolly: Introducing the worlds first truly open instruction-tuned llm, 2023. URL https://www.databricks. com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm. G. Cui, L. Yuan, N. Ding, G. Yao, W. Zhu, Y. Ni, G. Xie, Z. Liu, and M. Sun. Ultrafeedback: Boosting language models with high-quality feedback. arXiv preprint arXiv:2310.01377, 2023. N. Ding, Y. Chen, B. Xu, Y. Qin, Z. Zheng, S. Hu, Z. Liu, M. Sun, and B. Zhou. Enhancing chat language models by scaling high-quality instructional conversations. arXiv preprint arXiv:2305.14233, 2023. H. Dong, W. Xiong, D. Goyal, Y. Zhang, W. Chow, R. Pan, S. Diao, J. Zhang, K. Shum, and T. Zhang. Raft: Reward ranked finetuning for generative foundation model alignment, 2023. URL https://arxiv.org/abs/ 2304.06767. D. Dua, Y. Wang, P. Dasigi, G. Stanovsky, S. Singh, and M. Gardner. DROP: reading comprehension benchmark requiring discrete reasoning over paragraphs. In J. Burstein, C. Doran, and T. Solorio, editors, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 23682378, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1246. URL https://aclanthology. org/N19-1246. A. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-Dahle, A. Letman, A. Mathur, A. Schelten, A. Yang, A. Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Y. Dubois, X. Li, R. Taori, T. Zhang, I. Gulrajani, J. Ba, C. Guestrin, P. Liang, and T. Hashimoto. Alpacafarm: simulation framework for methods that learn from human feedback. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id=4hturzLcKX. Y. Dubois, B. Galambosi, P. Liang, and T. B. Hashimoto. Length-controlled alpacaeval: simple way to debias automatic evaluators. arXiv preprint arXiv:2404.04475, 2024. L. Engstrom, A. Ilyas, S. Santurkar, D. Tsipras, F. Janoos, L. Rudolph, and A. Madry. Implementation matters in deep rl: case study on ppo and trpo. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=r1etN1rtPB. L. Espeholt, H. Soyer, R. Munos, K. Simonyan, V. Mnih, T. Ward, Y. Doron, V. Firoiu, T. Harley, I. Dunning, et al. Impala: Scalable distributed deep-rl with importance weighted actor-learner architectures. In International conference on machine learning, pages 14071416. PMLR, 2018. L. Gao, J. Tow, B. Abbasi, S. Biderman, S. Black, A. DiPofi, C. Foster, L. Golding, J. Hsu, A. Le Noach, H. Li, K. McDonell, N. Muennighoff, C. Ociepa, J. Phang, L. Reynolds, H. Schoelkopf, A. Skowron, L. Sutawika, E. Tang, A. Thite, B. Wang, K. Wang, and A. Zou. framework for few-shot language model evaluation, 07 2024. URL https://zenodo.org/records/12608602. J. Gehring, K. Zheng, J. Copet, V. Mella, T. Cohen, and G. Synnaeve. Rlef: Grounding code llms in execution feedback with reinforcement learning, 2024. URL https://arxiv.org/abs/2410.02089. Gemini Team. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context, 2024. URL https://arxiv.org/abs/2403.05530. Gemma Team, M. Riviere, S. Pathak, P. G. Sessa, C. Hardin, S. Bhupatiraju, L. Hussenot, T. Mesnard, B. Shahriari, A. Ram칠, et al. Gemma 2: Improving open language models at practical size. arXiv preprint arXiv:2408.00118, 2024. 46 T. M. Gemma Team, C. Hardin, R. Dadashi, S. Bhupatiraju, L. Sifre, M. Rivi칟re, M. S. Kale, J. Love, P. Tafti, L. Hussenot, and et al. Gemma. 2024. doi: 10.34740/KAGGLE/M/3301. URL https://www.kaggle.com/ m/3301. C. Goddard, S. Siriwardhana, M. Ehghaghi, L. Meyers, V. Karpukhin, B. Benedict, M. McQuade, and J. Solawetz. Arcees mergekit: toolkit for merging large language models. arXiv preprint arXiv:2403.13257, 2024. Z. Gou, Z. Shao, Y. Gong, yelong shen, Y. Yang, M. Huang, N. Duan, and W. Chen. ToRA: tool-integrated In The Twelfth International Conference on Learning reasoning agent for mathematical problem solving. Representations, 2024. URL https://openreview.net/forum?id=Ep0TtjVoap. D. Groeneveld, I. Beltagy, P. Walsh, A. Bhagia, R. Kinney, O. Tafjord, A. Jha, H. Ivison, I. Magnusson, Y. Wang, S. Arora, D. Atkinson, R. Authur, K. R. Chandu, A. Cohan, J. Dumas, Y. Elazar, Y. Gu, J. Hessel, T. Khot, W. Merrill, J. D. Morrison, N. Muennighoff, A. Naik, C. Nam, M. E. Peters, V. Pyatkin, A. Ravichander, D. Schwenk, S. Shah, W. Smith, E. Strubell, N. Subramani, M. Wortsman, P. Dasigi, N. Lambert, K. Richardson, L. S. Zettlemoyer, J. Dodge, K. Lo, L. Soldaini, N. A. Smith, and H. Hajishirzi. Olmo: Accelerating the science of language models. ArXiv, abs/2402.00838, 2024. URL https://api.semanticscholar.org/CorpusID: 267365485. Y. Gu, O. Tafjord, B. Kuehl, D. Haddad, J. Dodge, and H. Hajishirzi. Olmes: standard for language model evaluations, 2024. URL https://arxiv.org/abs/2406.08446. T. Gunter, Z. Wang, C. Wang, R. Pang, A. Narayanan, A. Zhang, B. Zhang, C. Chen, C.-C. Chiu, D. Qiu, et al. Apple intelligence foundation language models. arXiv preprint arXiv:2407.21075, 2024. S. Guo, B. Zhang, T. Liu, T. Liu, M. Khalman, F. Llinares, A. Rame, T. Mesnard, Y. Zhao, B. Piot, J. Ferret, and M. Blondel. Direct language model alignment from online ai feedback, 2024. URL https://arxiv.org/abs/ 2402.04792. S. Han, K. Rao, A. Ettinger, L. Jiang, B. Y. Lin, N. Lambert, Y. Choi, and N. Dziri. Wildguard: Open one-stop moderation tools for safety risks, jailbreaks, and refusals of llms. arXiv preprint arXiv:2406.18495, 2024. D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020. D. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, D. Song, and J. Steinhardt. Measuring mathematical problem solving with the math dataset. NeurIPS, 2021. M. D. Hoffman, D. Phan, david dohan, S. Douglas, T. A. Le, A. T. Parisi, P. Sountsov, C. Sutton, S. Vikram, and R. A. Saurous. Training chain-of-thought via latent-variable inference. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id=a147pIS2Co. J. Hong, N. Lee, and J. Thorne. ORPO: Monolithic preference optimization without reference model. In Y. AlOnaizan, M. Bansal, and Y.-N. Chen, editors, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 1117011189, Miami, Florida, USA, Nov. 2024. Association for Computational Linguistics. URL https://aclanthology.org/2024.emnlp-main.626. J. Hu, X. Wu, W. Wang, D. Zhang, Y. Cao, et al. Openrlhf: An easy-to-use, scalable and high-performance rlhf framework. arXiv preprint arXiv:2405.11143, 2024. S. Hu, Y. Luo, H. Wang, X. Cheng, Z. Liu, and M. Sun. Wont get fooled again: Answering questions with false premises. arXiv preprint arXiv:2307.02394, 2023. S. Huang, R. F. J. Dossa, A. Raffin, A. Kanervisto, and W. Wang. The 37 implementation details of proximal policy optimization. In ICLR Blog Track, 2022. URL https://iclr-blog-track.github.io/2022/03/25/ ppo-implementation-details/. https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/. S. Huang, J. Weng, R. Charakorn, M. Lin, Z. Xu, and S. Onta침칩n. Cleanba: reproducible and efficient distributed reinforcement learning platform. In The Twelfth International Conference on Learning Representations, 2023. S. Huang, M. Noukhovitch, A. Hosseini, K. Rasul, W. Wang, and L. Tunstall. The n+ implementation details of RLHF with PPO: case study on TL;DR summarization. In First Conference on Language Modeling, 2024a. URL https://openreview.net/forum?id=kHO2ZTa8e3. 47 Y. Huang, L. Sun, H. Wang, S. Wu, Q. Zhang, Y. Li, C. Gao, Y. Huang, W. Lyu, Y. Zhang, et al. Trustllm: Trustworthiness in large language models. arXiv preprint arXiv:2401.05561, 2024b. A. Hurst, A. Lerer, A. P. Goucher, A. Perelman, A. Ramesh, A. Clark, A. Ostrow, A. Welihinda, A. Hayes, A. Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. B. Ibarz, J. Leike, T. Pohlen, G. Irving, S. Legg, and D. Amodei. Reward learning from human preferences and demonstrations in atari. Advances in neural information processing systems, 31, 2018. H. Ivison, Y. Wang, V. Pyatkin, N. Lambert, M. Peters, P. Dasigi, J. Jang, D. Wadden, N. A. Smith, I. Beltagy, et al. Camels in changing climate: Enhancing lm adaptation with tulu 2. arXiv preprint arXiv:2311.10702, 2023. H. Ivison, Y. Wang, J. Liu, Z. Wu, V. Pyatkin, N. Lambert, N. A. Smith, Y. Choi, and H. Hajishirzi. Unpacking dpo and ppo: Disentangling best practices for learning from preference feedback. arXiv preprint arXiv:2406.09279, 2024. A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. d. l. Casas, F. Bressand, G. Lengyel, G. Lample, L. Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. L. Jiang, K. Rao, S. Han, A. Ettinger, F. Brahman, S. Kumar, N. Mireshghallah, X. Lu, M. Sap, Y. Choi, and N. Dziri. Wildteaming at scale: From in-the-wild jailbreaks to (adversarially) safer language models, 2024. URL https://arxiv.org/abs/2406.18510. J. Kazdan, R. Schaeffer, A. Dey, M. Gerstgrasser, R. Rafailov, D. L. Donoho, and S. Koyejo. Collapse or thrive? perils and promises of synthetic data in self-generating world, 2024. URL https://arxiv.org/abs/2410.16713. A. Kazemnejad, M. Aghajohari, E. Portelance, A. Sordoni, S. Reddy, A. Courville, and N. L. Roux. Vineppo: Unlocking rl potential for llm reasoning through refined credit assignment, 2024. URL https://arxiv.org/ abs/2410.01679. A. K칬pf, Y. Kilcher, D. von R칲tte, S. Anagnostidis, Z. R. Tam, K. Stevens, A. Barhoum, D. Nguyen, O. Stanley, R. Nagyfi, et al. Openassistant conversations-democratizing large language model alignment. Advances in Neural Information Processing Systems, 36, 2024. W. Kwon, Z. Li, S. Zhuang, Y. Sheng, L. Zheng, C. H. Yu, J. E. Gonzalez, H. Zhang, and I. Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. J. Leike, D. Krueger, T. Everitt, M. Martic, V. Maini, and S. Legg. Scalable agent alignment via reward modeling: research direction. arXiv preprint arXiv:1811.07871, 2018. A. Lewkowycz, A. J. Andreassen, D. Dohan, E. Dyer, H. Michalewski, V. V. Ramasesh, A. Slone, C. Anil, I. Schlag, T. Gutman-Solo, Y. Wu, B. Neyshabur, G. Gur-Ari, and V. Misra. Solving quantitative reasoning problems with language models. In A. H. Oh, A. Agarwal, D. Belgrave, and K. Cho, editors, Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/forum?id=IFXTZERXdM7. T. Li, W.-L. Chiang, E. Frick, L. Dunlap, B. Zhu, J. E. Gonzalez, and I. Stoica. From live data to high-quality benchmarks: The arena-hard pipeline, April 2024a. URL https://lmsys.org/blog/2024-04-19-arena-hard/. T. Li, H. Xu, W. Tan, K. Murray, and D. Khashabi. Upsample or upweight? balanced training on heavily imbalanced datasets, 2024b. URL https://arxiv.org/abs/2410.04579. S. Lin, J. Hilton, and O. Evans. Truthfulqa: Measuring how models mimic human falsehoods. arXiv preprint arXiv:2109.07958, 2021. J. Liu, C. S. Xia, Y. Wang, and L. Zhang. Is your code generated by chatGPT really correct? rigorous evaluation of large language models for code generation. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id=1qvx610Cu7. Y. Liu. Roberta: robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 364, 2019. S. Longpre, L. Hou, T. Vu, A. Webson, H. W. Chung, Y. Tay, D. Zhou, Q. V. Le, B. Zoph, J. Wei, et al. The flan collection: Designing data and methods for effective instruction tuning. arXiv preprint arXiv:2301.13688, 2023. 48 Z. Luo, C. Xu, P. Zhao, Q. Sun, X. Geng, W. Hu, C. Tao, J. Ma, Q. Lin, and D. Jiang. Wizardcoder: Empowering code large language models with evol-instruct, 2023. A. Mallen, A. Asai, V. Zhong, R. Das, H. Hajishirzi, and D. Khashabi. When not to trust language models: Investigating effectiveness and limitations of parametric and non-parametric memories. arXiv preprint, 2022. M. Mazeika, L. Phan, X. Yin, A. Zou, Z. Wang, N. Mu, E. Sakhaee, N. Li, S. Basart, B. Li, et al. Harmbench: standardized evaluation framework for automated red teaming and robust refusal. arXiv preprint arXiv:2402.04249, 2024. Y. Meng, M. Xia, and D. Chen. Simpo: Simple preference optimization with reference-free reward. arXiv preprint arXiv:2405.14734, 2024. A. Meurer, C. P. Smith, M. Paprocki, O. 캛ert칤k, S. B. Kirpichev, M. Rocklin, A. Kumar, S. Ivanov, J. K. Moore, S. Singh, T. Rathnayake, S. Vig, B. E. Granger, R. P. Muller, F. Bonazzi, H. Gupta, S. Vats, F. Johansson, F. Pedregosa, M. J. Curry, A. R. Terrel, v. Rou캜ka, A. Saboo, I. Fernando, S. Kulal, R. Cimrman, and A. Scopatz. Sympy: symbolic computing in python. PeerJ Computer Science, 3:e103, Jan. 2017. ISSN 2376-5992. doi: 10.7717/peerj-cs.103. URL https://doi.org/10.7717/peerj-cs.103. L. J. V. Miranda, Y. Wang, Y. Elazar, S. Kumar, V. Pyatkin, F. Brahman, N. A. Smith, H. Hajishirzi, and P. Dasigi. Hybrid preferences: Learning to route instances for human vs. ai feedback. arXiv preprint arXiv:2410.19133, 2024. S. Mishra, D. Khashabi, C. Baral, and H. Hajishirzi. Cross-task generalization via natural language crowdsourcing instructions. In S. Muresan, P. Nakov, and A. Villavicencio, editors, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 34703487, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.244. URL https: //aclanthology.org/2022.acl-long.244. Mistral AI. Ministraux: Pushing the boundaries of efficient transformer design, 2024. URL https://mistral. ai/news/ministraux/. Accessed: 2024-11-17. V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, et al. Human-level control through deep reinforcement learning. nature, 518(7540): 529533, 2015. P. Moritz, R. Nishihara, S. Wang, A. Tumanov, R. Liaw, E. Liang, M. Elibol, Z. Yang, W. Paul, M. I. Jordan, et al. Ray: distributed framework for emerging {AI} applications. In 13th USENIX symposium on operating systems design and implementation (OSDI 18), pages 561577, 2018. MosaicML NLP Team. Introducing mpt-30b: Raising the bar for open-source foundation models, 2023. URL www.mosaicml.com/blog/mpt-30b. Accessed: 2023-06-22. N. Muennighoff, H. Su, L. Wang, N. Yang, F. Wei, T. Yu, A. Singh, and D. Kiela. Generative representational instruction tuning, 2024. R. Nakano, J. Hilton, S. Balaji, J. Wu, L. Ouyang, C. Kim, C. Hesse, S. Jain, V. Kosaraju, W. Saunders, et al. Webgpt: Browser-assisted question-answering with human feedback. arXiv preprint arXiv:2112.09332, 2021. M. Noukhovitch, S. Huang, S. Xhonneux, A. Hosseini, R. Agarwal, and A. Courville. Asynchronous rlhf: Faster and more efficient off-policy rl for language models, 2024. URL https://arxiv.org/abs/2410.18252. OpenAI. Learning to reason with llms. OpenAI, 2024. URL https://openai.com/index/ learning-to-reason-with-llms/. Accessed: 2024-09-12. OpenAI. Gpt-4o, 2024. https://openai.com. L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. S. Pawar, S. M. T. I. Tonmoy, S. M. M. Zaman, V. Jain, A. Chadha, and A. Das. The what, why, and how of context length extension techniques in large language models detailed survey, 2024. URL https: //arxiv.org/abs/2401.07872. C. Qu, S. Dai, X. Wei, H. Cai, S. Wang, D. Yin, J. Xu, and J.-R. Wen. Tool learning with large language models: survey. arXiv preprint arXiv:2405.17935, 2024. Qwen Team. Qwen2.5: party of foundation models, September 2024. URL https://qwenlm.github.io/ blog/qwen2.5/. R. Rafailov, A. Sharma, E. Mitchell, C. D. Manning, S. Ermon, and C. Finn. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36, 2024. N. Rajani, L. Tunstall, E. Beeching, N. Lambert, A. M. Rush, and T. Wolf. No robots. https://huggingface. co/datasets/HuggingFaceH4/no_robots, 2023. S. Rajbhandari, J. Rasley, O. Ruwase, and Y. He. Zero: Memory optimizations toward training trillion parameter models. In SC20: International Conference for High Performance Computing, Networking, Storage and Analysis, pages 116. IEEE, 2020. D. Rein, B. L. Hou, A. C. Stickland, J. Petty, R. Y. Pang, J. Dirani, J. Michael, and S. R. Bowman. Gpqa: graduate-level google-proof q&a benchmark, 2023. P. R칬ttger, H. R. Kirk, B. Vidgen, G. Attanasio, F. Bianchi, and D. Hovy. Xstest: test suite for identifying exaggerated safety behaviours in large language models. arXiv preprint arXiv:2308.01263, 2023. V. Sanh, A. Webson, C. Raffel, S. Bach, L. Sutawika, Z. Alyafeai, A. Chaffin, A. Stiegler, A. Raja, M. Dey, M. S. Bari, C. Xu, U. Thakker, S. S. Sharma, E. Szczechla, T. Kim, G. Chhablani, N. Nayak, D. Datta, J. Chang, M. T.-J. Jiang, H. Wang, M. Manica, S. Shen, Z. X. Yong, H. Pandey, R. Bawden, T. Wang, T. Neeraj, J. Rozen, A. Sharma, A. Santilli, T. Fevry, J. A. Fries, R. Teehan, T. L. Scao, S. Biderman, L. Gao, T. Wolf, and A. M. Rush. Multitask Prompted Training Enables Zero-Shot Task Generalization. In International Conference on Learning Representations (ICLR), 2022. D. Saxton, E. Grefenstette, F. Hill, and P. Kohli. Analysing mathematical reasoning abilities of neural models. In International Conference on Learning Representations, 2019. J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. G. Shen, Z. Wang, O. Delalleau, J. Zeng, Y. Dong, D. Egert, S. Sun, J. J. Zhang, S. Jain, A. Taghibakhshi, M. S. Ausin, A. Aithal, and O. Kuchaiev. Nemo-aligner: Scalable toolkit for efficient model alignment. In First Conference on Language Modeling, 2024a. URL https://openreview.net/forum?id=yK2eGE8QVW. X. Shen, Z. Chen, M. Backes, Y. Shen, and Y. Zhang. Do Anything Now: Characterizing and Evaluating In-The-Wild Jailbreak Prompts on Large Language Models. In ACM SIGSAC Conference on Computer and Communications Security (CCS). ACM, 2024b. A. K. Singh, M. Y. Kocyigit, A. Poulton, D. Esiobu, M. Lomeli, G. Szilvasy, and D. Hupkes. Evaluation data contamination in llms: how do we measure it and (when) does it matter? arXiv preprint arXiv:2411.03923, 2024a. S. Singh, F. Vargus, D. Dsouza, B. F. Karlsson, A. Mahendiran, W.-Y. Ko, H. Shandilya, J. Patel, D. Mataciunas, L. OMahony, et al. Aya dataset: An open-access collection for multilingual instruction tuning. arXiv preprint arXiv:2402.06619, 2024b. P. Singhal, T. Goyal, J. Xu, and G. Durrett. long way to go: Investigating length correlations in RLHF, 2024. URL https://openreview.net/forum?id=sNtDKdcI1f. N. Stiennon, L. Ouyang, J. Wu, D. Ziegler, R. Lowe, C. Voss, A. Radford, D. Amodei, and P. F. Christiano. Learning to summarize with human feedback. Advances in Neural Information Processing Systems, 33:30083021, 2020. M. Suzgun, N. Scales, N. Sch칛rli, S. Gehrmann, Y. Tay, H. W. Chung, A. Chowdhery, Q. V. Le, E. H. Chi, D. Zhou, , and J. Wei. Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022. R. Taori, I. Gulrajani, T. Zhang, Y. Dubois, X. Li, C. Guestrin, P. Liang, and T. B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca, 2023. R. Teknium, J. Quesnelle, and C. Guang. Hermes 3 technical report. arXiv preprint arXiv:2408.11857, 2024. 50 S. Toshniwal, W. Du, I. Moshkov, B. Kisacanin, A. Ayrapetyan, and I. Gitman. Openmathinstruct-2: Accelerating ai for math with massive open-source instruction data. arXiv preprint arXiv:2410.01560, 2024. H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. L. Tunstall, E. Beeching, N. Lambert, N. Rajani, K. Rasul, Y. Belkada, S. Huang, L. von Werra, C. Fourrier, N. Habib, et al. Zephyr: Direct distillation of lm alignment. arXiv preprint arXiv:2310.16944, 2023. A. 칖st칲n, V. Aryabumi, Z. Yong, W.-Y. Ko, D. Dsouza, G. Onilude, N. Bhandari, S. Singh, H.-L. Ooi, A. Kayid, F. Vargus, P. Blunsom, S. Longpre, N. Muennighoff, M. Fadaee, J. Kreutzer, and S. Hooker. Aya model: An instruction finetuned open-access multilingual language model. In L.-W. Ku, A. Martins, and V. Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1589415939, Bangkok, Thailand, Aug. 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.845. URL https://aclanthology.org/2024.acl-long.845. L. von Werra, Y. Belkada, L. Tunstall, E. Beeching, T. Thrush, N. Lambert, S. Huang, K. Rasul, and Q. Gallou칠dec. Trl: Transformer reinforcement learning. https://github.com/huggingface/trl, 2020. D. Wadden, K. Shi, J. Morrison, A. Naik, S. Singh, N. Barzilay, K. Lo, T. Hope, L. Soldaini, S. Z. Shen, et al. Sciriff: resource to enhance language model instruction-following over scientific literature. arXiv preprint arXiv:2406.07835, 2024. X. Wang, B. Li, Y. Song, F. F. Xu, X. Tang, M. Zhuge, J. Pan, Y. Song, B. Li, J. Singh, H. H. Tran, F. Li, R. Ma, M. Zheng, B. Qian, Y. Shao, N. Muennighoff, Y. Zhang, B. Hui, J. Lin, R. Brennan, H. Peng, H. Ji, and G. Neubig. OpenHands: An Open Platform for AI Software Developers as Generalist Agents, 2024a. URL https://arxiv.org/abs/2407.16741. Y. Wang, Y. Kordi, S. Mishra, A. Liu, N. A. Smith, D. Khashabi, and H. Hajishirzi. Self-instruct: Aligning language models with self-generated instructions. arXiv preprint arXiv:2212.10560, 2022a. Y. Wang, S. Mishra, P. Alipoormolabashi, Y. Kordi, A. Mirzaei, A. Naik, A. Ashok, A. S. Dhanasekaran, A. Arunkumar, D. Stap, E. Pathak, G. Karamanolakis, H. Lai, I. Purohit, I. Mondal, J. Anderson, K. Kuznia, K. Doshi, K. K. Pal, M. Patel, M. Moradshahi, M. Parmar, M. Purohit, N. Varshney, P. R. Kaza, P. Verma, R. S. Puri, R. Karia, S. Doshi, S. K. Sampat, S. Mishra, S. Reddy A, S. Patro, T. Dixit, and X. Shen. Super-NaturalInstructions: Generalization via declarative instructions on 1600+ NLP tasks. In Y. Goldberg, Z. Kozareva, and Y. Zhang, editors, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 50855109, Abu Dhabi, United Arab Emirates, Dec. 2022b. Association for Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main.340. URL https://aclanthology.org/2022.emnlp-main.340. Y. Wang, S. Mishra, P. Alipoormolabashi, Y. Kordi, A. Mirzaei, A. Naik, A. Ashok, A. S. Dhanasekaran, A. Arunkumar, D. Stap, et al. Super-naturalinstructions: Generalization via declarative instructions on 1600+ nlp tasks. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 50855109, 2022c. Y. Wang, H. Ivison, P. Dasigi, J. Hessel, T. Khot, K. Chandu, D. Wadden, K. MacMillan, N. A. Smith, I. Beltagy, et al. How far can camels go? exploring the state of instruction tuning on open resources. Advances in Neural Information Processing Systems, 36:7476474786, 2023. Y. Wang, X. Ma, G. Zhang, Y. Ni, A. Chandra, S. Guo, W. Ren, A. Arulraj, X. He, Z. Jiang, et al. Mmlu-pro: more robust and challenging multi-task language understanding benchmark. arXiv preprint arXiv:2406.01574, 2024b. Z. Wang, A. Bukharin, O. Delalleau, D. Egert, G. Shen, J. Zeng, O. Kuchaiev, and Y. Dong. Helpsteer2-preference: Complementing ratings with preferences. arXiv preprint arXiv:2410.01257, 2024c. Z. Wang, Y. Dong, O. Delalleau, J. Zeng, G. Shen, D. Egert, J. J. Zhang, M. N. Sreedhar, and O. Kuchaiev. Helpsteer2: Open-source dataset for training top-performing reward models. arXiv preprint arXiv:2406.08673, 2024d. J. Wei, M. Bosma, V. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du, A. M. Dai, and Q. V. Le. Finetuned Language Models are Zero-Shot Learners. In International Conference on Learning Representations (ICLR), 2022a. 51 J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le, D. Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022b. T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac, T. Rault, R. Louf, M. Funtowicz, J. Davison, S. Shleifer, P. von Platen, C. Ma, Y. Jernite, J. Plu, C. Xu, T. Le Scao, S. Gugger, M. Drame, Q. Lhoest, and A. Rush. Transformers: State-of-the-art natural language processing. In Q. Liu and D. Schlangen, editors, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 3845, Online, Oct. 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-demos.6. URL https://aclanthology.org/2020.emnlp-demos.6. M. Wortsman, G. Ilharco, S. Y. Gadre, R. Roelofs, R. Gontijo-Lopes, A. S. Morcos, H. Namkoong, A. Farhadi, Y. Carmon, S. Kornblith, and L. Schmidt. Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time. In K. Chaudhuri, S. Jegelka, L. Song, C. Szepesvari, G. Niu, and S. Sabato, editors, Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 2396523998. PMLR, 1723 Jul 2022. URL https: //proceedings.mlr.press/v162/wortsman22a.html. Z. Wu, A. Balashankar, Y. Kim, J. Eisenstein, and A. Beirami. Reuse your rewards: Reward model transfer for zero-shot cross-lingual alignment. In Y. Al-Onaizan, M. Bansal, and Y.-N. Chen, editors, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 13321353, Miami, Florida, USA, Nov. 2024. Association for Computational Linguistics. URL https://aclanthology.org/2024.emnlp-main. 79. C. Xu, Q. Sun, K. Zheng, X. Geng, P. Zhao, J. Feng, C. Tao, and D. Jiang. Wizardlm: Empowering large language models to follow complex instructions. arXiv preprint arXiv:2304.12244, 2023. H. Xu, A. Sharaf, Y. Chen, W. Tan, L. Shen, B. V. Durme, K. Murray, and Y. J. Kim. Contrastive preference optimization: Pushing the boundaries of llm performance in machine translation, 2024a. S. Xu, W. Fu, J. Gao, W. Ye, W. Liu, Z. Mei, G. Wang, C. Yu, and Y. Wu. Is dpo superior to ppo for llm alignment? comprehensive study. In ICML, 2024b. URL https://openreview.net/forum?id=6XH8R7YrSk. S. Yang, W.-L. Chiang, L. Zheng, J. E. Gonzalez, and I. Stoica. Rethinking benchmark and contamination for language models with rephrased samples, 2023. A. Young, B. Chen, C. Li, C. Huang, G. Zhang, G. Zhang, H. Li, J. Zhu, J. Chen, J. Chang, et al. Yi: Open foundation models by 01. ai. arXiv preprint arXiv:2403.04652, 2024. E. Zelikman, Y. Wu, J. Mu, and N. Goodman. STar: Bootstrapping reasoning with reasoning. In A. H. Oh, A. Agarwal, D. Belgrave, and K. Cho, editors, Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/forum?id=_3ELRdg2sgI. E. Zelikman, G. Harik, Y. Shao, V. Jayasiri, N. Haber, and N. D. Goodman. Quiet-star: Language models can teach themselves to think before speaking. COLM, abs/2403.09629, 2024. L. Zha, J. Zhou, L. Li, R. Wang, Q. Huang, S. Yang, J. Yuan, C. Su, X. Li, A. Su, et al. Tablegpt: Towards unifying tables, nature language and commands into one gpt. arXiv preprint arXiv:2307.08674, 2023. M. Zhang and E. Choi. SituatedQA: Incorporating extra-linguistic contexts into QA. In M.-F. Moens, X. Huang, L. Specia, and S. W.-t. Yih, editors, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 73717387, Online and Punta Cana, Dominican Republic, Nov. 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.586. URL https://aclanthology.org/ 2021.emnlp-main.586. W. Zhao, X. Ren, J. Hessel, C. Cardie, Y. Choi, and Y. Deng. Wildchat: 1m chatgpt interaction logs in the wild. arXiv preprint arXiv:2405.01470, 2024. Y. Zhao, R. Joshi, T. Liu, M. Khalman, M. Saleh, and P. J. Liu. Slic-hf: Sequence likelihood calibration with human feedback, 2023. URL https://arxiv.org/abs/2305.10425. L. Zheng, W.-L. Chiang, Y. Sheng, S. Zhuang, Z. Wu, Y. Zhuang, Z. Lin, Z. Li, D. Li, E. P. Xing, H. Zhang, J. E. Gonzalez, and I. Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena. In NeurIPS Datasets and Benchmarks Track, 2023. 52 W. Zhong, R. Cui, Y. Guo, Y. Liang, S. Lu, Y. Wang, A. Saied, W. Chen, and N. Duan. AGIEval: humancentric benchmark for evaluating foundation models. In K. Duh, H. Gomez, and S. Bethard, editors, Findings of the Association for Computational Linguistics: NAACL 2024, pages 22992314, Mexico City, Mexico, June 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-naacl.149. URL https://aclanthology.org/2024.findings-naacl.149. C. Zhou, P. Liu, P. Xu, S. Iyer, J. Sun, Y. Mao, X. Ma, A. Efrat, P. Yu, L. Yu, et al. Lima: Less is more for alignment. Advances in Neural Information Processing Systems, 36, 2024. J. Zhou, T. Lu, S. Mishra, S. Brahma, S. Basu, Y. Luan, D. Zhou, and L. Hou. Instruction-following evaluation for large language models, 2023. URL https://arxiv.org/abs/2311.07911. B. Zhu, E. Frick, T. Wu, H. Zhu, and J. Jiao. Starling-7b: Improving llm helpfulness & harmlessness with rlaif, November 2023. T. Y. Zhuo, M. C. Vu, J. Chim, H. Hu, W. Yu, R. Widyasari, I. N. B. Yusuf, H. Zhan, J. He, I. Paul, et al. Bigcodebench: Benchmarking code generation with diverse function calls and complex instructions. arXiv preprint arXiv:2406.15877, 2024. D. M. Ziegler, N. Stiennon, J. Wu, T. B. Brown, A. Radford, D. Amodei, P. Christiano, and G. Irving. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2019. 53 Hyperparameter Learning Rate Gradient Norm Threshold Learning Rate Schedule Batch Size (effective) Max Token Length Number of Epochs Value 3 10-6 1.0 Linear 256 2,048 Table 31: Hyperparameters used for reward model training. (a) Tulu 2 Distribution. (b) OpenHermes 2.5 Distribution. Figure 24: SFT mix distribution by length of the prompt plus completion in tokens (using the Llama 3 tokenizer) for other open training mixes."
        },
        {
            "title": "B Additional Dataset Analyses",
            "content": "B.1 Extra Distribution Plots The token length distribution of two other, popular SFT training datasets that are available publicly, T칖LU 2 SFT Mix and OpenHermes 2.5, are shown in Figure 24. B.2 Contamination in Public Datasets Table 32 shows list of some publicly available datasets that we found to be contaminated with evaluations our suite. One general takeaway from these results is that datasets that contain realistic uses of API models like ShareGPT, WildChat, and LMSys Chat are likely to overlap with test sets of existing benchmarks and practitioners should make efforts to decontaminate them before using them as training data. B.3 Chat Template Implementation We provide the implementation of our chat template in Figure 25. B.4 RLVR IFEval overoptimization We checked the output of our RLVR IFEval experiments and found some higher KL runs to have interesting overoptimized outputs. We show some non-overoptimized samples in Figure 27 and overoptimized samples in Figure 26. 54 Dataset Link Evol CodeAlpaca ise-uiuc/Magicoder-Evol-Instruct-110K WildChat GPT-4 allenai/WildChat-1M-Full (GPT-4 instances only) WildJailbreak allenai/wildjailbreak WildGuardmix allenai/wildguardmix Eval. HumanEval JailbreakTrigger Do-Anything-Now WildGuardTest HarmBench JailbreakTrigger Do-Anything-Now MATH MATH NuminaMath-TIR AI-MO/NuminaMath-TIR DaringAnteater ShareGPT LMSys Chat 1M lmsys/lmsys-chat-1m nvidia/Daring-Anteater anon8231489123/ShareGPT_Vicuna_unfiltered AlpacaEval TruthfulQA MMLU HumanEval GSM8K AlpacaEval BBH TruthfulQA JailbreakTrigger HarmbenchEval Do-Anything-Now AGIEval English AlpacaEval OpenAssistant/oasst2 (English only) % eval overlap 70.7 9.0 54.0 8.2 6.3 19.0 39.7 18.2 30.7 19.2 19.1 10.3 17.7 8.9 46.5 10.6 9.2 75.0 9.4 90.3 18.7 18.3 OpenAssistant 2 Table 32: Public datasets where we found significant (>5% eval overlap) contamination with our evaluation suite. % eval overlap is the percentage of instances in the evaluation set that overlap (as per to the heuristics in Section 3.2) with instances in the dataset. We included decontaminated versions of the first five datasets in our training sets, and did not include the last four datasets."
        },
        {
            "title": "C Supervised Finetuning Data Details",
            "content": "C.1 Prompts We provide the prompts used for creating persona data in Figures 28, 29, 30 (IF data); Figures 31, 32 (math data); Figures 33, 34 (code data)."
        },
        {
            "title": "D Preference Tuning Data Details",
            "content": "We provide the system prompt used for LLM-as-a-judge in Figure 35. The template used for rating model responses is given in Figure 36. Additional variants for rating model responses for particular aspects are given in Figures 37, 38, 39 and 40."
        },
        {
            "title": "E Additional RLVR Details",
            "content": "E.1 RM Training Hyperparameters We detail the hyperparameters used to train T칖LU 3 8B RM (used for initializing the value model for RLVR training) in Table 31. 55 Exact implementation of our T칖LU 3 chat template. \"{% \"{% \" {{ < t > + s [ t ] + }} \" \"{% \" {{ < r > + s [ t ] + }} \" \"{% \"{% \" {{ < i n > + s [ t ] + _ e + }} \" \"{% \" {{ < i n > + s [ t ] + _ e }} \" \"{% \"{% \"{% \" {{ < i n > }} \" \"{% \"{% Figure 25: Exact implementation of the chat template used for T칖LU 3. See Sec. 4.3.1 for further details on chat templates we considered."
        },
        {
            "title": "F Evaluation Details",
            "content": "F.1 Detailed Safety Results We provide detailed breakdowns of the safety scores of T칖LU 3 and peer 8B models by risk type on the HarmBench  (Table 34)  and WildGuardTest  (Table 35)  benchmarks. F.2 Evaluation principles Experimenting Principles for unseen suite on development tasks We compare evaluating instruction-tuned models using an evaluation setup adapted from base model evaluation with our designed evaluation practices that align more with real usage. See Tables 36,37, 38 for example results. Unlike the case for base models 4-shot (MATH) or 8-shot CoT (GSM8K), few-shot in-context examples goes into issues like few-shot as multi-turn could be seen as putting words in the mouth of the model, sticking all examples in the prompt together could confuse models into answering all questions. We observe based on the exploratory models used for testing our setups, that natural instruction is more important when dealing with instruction-tuned models. In the tables chat-v1 refers to using zero-shot CoT prompt and chat-v2 refers to using 1-shot CoT prompt. Given that providing an additional example does not always lead to better scores, we stick to zero-shot CoT, keeping the prompt simple and avoid unintentionally steering the model to favor any answer due to the single example. similar story holds for DROP where 1-turn zero-shot CoT prompt (more similar to users would prompt an LM) performs better than the setup where in-context examples are stitched together in the 1-turn (as adopted from Llama 3s base model evaluation setup). Prompts used for unseen evaluation tasks In Figure 41 we provide the 0-shot reasoning prompt used for the multiple-choice tasks in the unseen evaluation suite, which includes AGIEval English, MMLU-Pro, and GPQA. We provide the 0-shot reasoning prompt for the Deepmind Mathematics task in Figure 42. MMLU Chain-of-Thought Prompting 56 Conversation 1 User: Measure the length of the given object in cm. Pen. Include keywords timberlike, unconnectedly in your response. Assistant: timberlike, unconnectedly Conversation User: In your response, the word craspedodromous should appear 25 times. Measure the length of the given object in cm. Pen Assistant: craspedodromous, craspedodromous, craspedodromous, craspedodromous, craspedodromous, craspedodromous, craspedodromous, craspedodromous, craspedodromous, craspedodromous, craspedodromous, craspedodromous, craspedodromous, craspedodromous, craspedodromous, craspedodromous, craspedodromous, craspedodromous, craspedodromous, craspedodromous, craspedodromous, craspedodromous, craspedodromous, craspedodromous, craspedodromous Conversation 3 User: Do not include keywords loath, aesculapius, cardiocentesis in the response. Measure the length of the given object in cm. Pen Assistant: 30 Conversation User: Measure the length of the given object in cm. Pen. In your response, the letter should appear 14 times. Assistant: e, e, e, e, e, e, e, e, e, e, e, e, e, Conversation 5 User: Measure the length of the given object in cm. Pen. Your response should contain 5 paragraphs. You separate paragraphs using the markdown divider: * * * Assistant: Paragraph 1 * * * Paragraph 2 * * * Paragraph 3 * * * Paragraph 4 * * * Paragraph 5 Figure 26: The example conversations of the RLVR models on the Prompts with Constraints with 洧띻 = 0.01. RLVR overoptimizes against the verifiable reward function and does not output meaningful content. For MMLU, we experiment with various CoT settings that prompts models to provide reasoning before answering the question. We experiment with four CoT settings (see Table 39): two formulations (Explicit CoT) that explicitly prompts the models to provide step-by-step reasoning before answering the question and one implicit CoT variant (Implicit CoT) that asks model to summarize its reasoning before returning an answer prefixed by Therefore, the answer is. All variants also include in the prompt instructions for answer format, which is used for answer extraction. At answer extraction, we also use set of heuristics for extracting the model answers to provide us with the flexibility of capturing responses that do not follow the exact requested answer format but are nevertheless correct. We use exact match to gold answers to determine accuracy. We choose the setting, Implicit CoT, as our primary prompting strategy as it leads to consistent improvement for T칖LU 3 and its peer 8B models  (Table 40)  over the traditionally employed no-CoT 5-shot setting. CoT setting is effective for increasing performance across the majority of the models both in the 8B and 70B scale  (Table 41)  . 57 Conversation 1 User: Measure the length of the given object in cm. Pen. Include keywords timberlike, unconnectedly in your response. Assistant: Measuring the length of pen would typically be done using ruler or measuring tape. Since pen is generally about the size of pencil, one can estimate its length to be around 15-20 centimeters. The material of pen might be timberlike due to its resemblance to wooden materials, though typically pens are made from materials like plastic, metal, or glass-fiber. However, the comparison might not be entirely accurate because timber itself is type of wood. The adjective \"unconnectedly\" seems to imply lack of connection or disconnection in context, which doesnt directly align with measuring the length of pen. It might be used in different types of discussions or analyses where unconnected elements or aspects need to be mentioned. Conversation 2 User: In your response, the word craspedodromous should appear 25 times. Measure the length of the given object in cm. Pen Assistant: To solve this problem, will explain the process and not directly output the word \"craspedodromous\" 25 times for brevity, as it would be inefficient and tedious. First, lets identify and clarify the requirements: - Ensure the word \"craspedodromous\" appears 25 times. - Measure the length of pen in centimeters. [Content truncated for brevity - continues with detailed measurement process and examples...] Conversation 3 User: Do not include keywords loath, aesculapius, cardiocentesis in the response. Measure the length of the given object in cm. Pen Assistant: The length of typical pen can vary depending on its size, but standard pen is typically around 14 to 15.5 cm in length. Conversation 4 User: Measure the length of the given object in cm. Pen. In your response, the letter should appear 14 times. Assistant: The length of typical pen is approximately 15 centimeters. Here, the letter appears 14 times. Conversation 5 User: Measure the length of the given object in cm. Pen. Your response should contain 5 paragraphs. You separate paragraphs using the markdown divider: * * * Assistant: Paragraph 1 * * * Paragraph 2 * * * Paragraph 3 * * * Paragraph 4 * * * Paragraph Figure 27: The example conversations of the RLVR models on the Prompts with Constraints with 洧띻 = 0.1 Additionally, implicit CoT is most effective at enabling reasoning capabilities without penalizing academic subjects that do not require explicit step-by-step reasoning. As shown in Table 40, an average of 78% of the subjects see improvement with the implicit CoT for T칖LU 3 models, while explicit CoT caps at 53% benefited subjects. It is also worth pointing out that the difference between the three CoT prompt formulations are reasonable wording changes that preserve the overall meaning of the instructions. Despite this, however, the performance observed is variable. This suggests that careful attention to wording or style in prompting language is warranted when it comes to model evaluation. We expect adv to be especially relevant for heterogenous evaluations like MMLU, which contain questions that require varying types of reasoning to answer. 58 Precise Instruction Following (prompt) Create verifiable instruction that the following persona might ask you to do: {persona} An example of verifiable instruction could be: {example} Note: 1. The above example is not tied to any particular persona, but you should create one that is unique and specific to the given persona. 2. The instruction should contain all the following verifiable constraint(s): {constraints} 3. Your output should start with \"User instruction:\". Your output should not include an answer to the instruction. Figure 28: Prompt used to generate precise instruction following instances. {persona} are borrowed from Chan et al. [2024]. We use the set of {constraints} defined in Zhou et al. [2023]. Example seeds are manually written by authors for each constraint. Precise Instruction Following (response) Provide response to the given instruction while satisfying the constraints. Instruction: {generated_instruction} Note that you should follow the instruction precisely and satisfy all the constraints. Figure 29: Prompt used to generate response for precise instruction following instance. Rewriting the Instruction Following Instance (Preference Data Construction) Rewrite the given instruction to remove one of the constraints. {Instruction} Note: 1. You should rewrite the instruction coherently while relaxing one of the following constraint categories: {constraints} 2. Remember to entirely relax one of the constraint category 3. Your output should start with \"User instruction:\". Your output should not include an answer to the instruction. Figure 30: Prompt used to generate modify an instruction following query minimally such that the answer to the rewritten prompt does not satisfy the original query and thus can be used as rejected response for preference data construction. 59 Hard Math Problems (prompt) Create math problem related to the following persona: {persona} Note: 1. The math problem should be challenging and involve advanced mathematical skills and knowledge. Only top talents can solve it correctly. 2. You should make full use of the persona description to create the math problem to ensure that the math problem is unique and specific to the persona. 3. Your response should always start with \"Math problem:\". Your response should not include solution to the created math problem. 4. Your created math problem should include no more than 2 sub-problems. Figure 31: Prompt used to generate hard math word problems. {persona} are borrowed from Chan et al. [2024]. Hard Math Problems (response) Provide solution to the given math problem. Problem: {generated_math_problem} Note: Provide your solution step-by-step, and end your solution in new line in the following format: Final Answer: The final answer is $final_answer$. hope it is correct. Figure 32: Prompt used to generate solutions for hard math word problems. Code Completion (prompt) {persona} Assume you are the persona described above and you are asking python programming question in stackoverflow. Note: 1. Your question should be solvable by entryto medium-level python programmers. 2. Your question should clearly specify the type of input, expected output and an optional example. 3. Your response should always start with \"Question: Write python function to\" 4. Your response should not include solution to the created coding problem. Figure 33: Prompt used to generate code completion instances. {persona} are borrowed from Chan et al. [2024]. 60 Code Completion (response) Provide solution to the given python programming question. Question: {generated_code_problem} Note: 1. Your response should always start with the function definition and end with the final return statement. 2. Your response should only and only include python function. Figure 34: Prompt used to generate code completion. Model Name Yi-34B-Chat Yi-6B-Chat Tulu 2 7B Tulu 2 13b Google Gemma 2 27B it Google Gemma 2 9B it InternLM2.5 20B InternLM2.5 7BB InternLM2.5 1.8B GPT-4o MPT 30B Chat MPT 7B 8k Chat Llama 3.1 8B Instruct Llama 3.1 70B Instruct Llama 3 8B Instruct Mistral 7B Instruct v0.2 Mistral Nemo Instruct 2407 Qwen2.5 72B Instruct Qwen2.5 32B Instruct Qwen2.5 14B Instruct Qwen 2.5 7B Instruct Falcon 7B Reference [Young et al., 2024] [Young et al., 2024] [Ivison et al., 2023] [Ivison et al., 2023] [Gemma Team et al., 2024] [Gemma Team et al., 2024] [Cai et al., 2024] [Cai et al., 2024] [Cai et al., 2024] [Hurst et al., 2024] [MosaicML NLP Team, 2023] [MosaicML NLP Team, 2023] [Dubey et al., 2024] [Dubey et al., 2024] [Dubey et al., 2024] [Jiang et al., 2023] [AI, 2024] [Qwen Team, 2024] [Qwen Team, 2024] [Qwen Team, 2024] [Qwen Team, 2024] [Almazrouei et al., 2023] Table 33: External models used to sample off-policy data in the synthetic preference pipeline. System prompt for LLM-as-a-judge Your role is to evaluate text quality based on given criteria. Youll receive an instructional description (Instruction) and text outputs (Text). Understand and interpret instructions to evaluate effectively. Provide annotations for each text with rating and rationale. The texts given are independent, and should be evaluated separately. Figure 35: System prompt for LLM-as-a-judge, adapted from Cui et al. [2023]. 61 Formatting preference instance for LLM-as-a-judge { aspect_guideline } ## Format: ### Input Instruction: [Clearly specify the task goal and restrictions] Texts: {% for in range(1, completionslength + 1) %} <text {{ }}> [Text {{ }}] {% endfor %} ### Output {% for in range(1, completionslength + 1) %} #### Output for Text {{ }} {% if identifier is defined %} Type: [List of numeric identifiers (or \"None\"), separatedby commas] Rationale: [Rationale for identification in short sentences] {% endif %} Rating: [Rating for text {{ }}] Rational: [rational for the rating in short sentences] {% endfor %} ## Annotation ### Input Instruction: {{ instruction }} Texts: {% for completion in completions %} <text {{ loop.index + 1 }}> {{ completion }} {% endfor %} ### Output Figure 36: Jinja2 template used to rate model response given set of aspect-based guidelines, an instruction and list of completions, adapted from Cui et al. [2023]. Categories Chemical & Biological Weapons/Drugs Copyright Violations Cybercrime & Unauthorized Intrusion Harassment & Bullying General Harm Illegal Activities Misinformation & Disinformation T칖LU 3 8B T칖LU 3 8B T칖LU 3 8B Llama 3.1 8B Ministral 8B Qwen 2.5 7B Instruct 97.6 60.0 88.5 100.0 94.4 98.1 81.5 Instruct 97.6 75.0 76.9 100.0 88.9 96.2 66.7 Instruct 57.1 56.3 42.3 90.5 66.7 64.2 27.8 SFT 100.0 100.0 96.2 100.0 94.4 96.2 100. DPO 95.2 100.0 92.3 100.0 83.3 98.1 85.2 97.6 100.0 96.2 100.0 83.3 100.0 79.6 Table 34: Breakdown of model refusal rate by risk categories of the HarmBench benchmark. 62 Instruction Following Aspect (prompt) # Instruction Following Assessment Evaluate alignment between output and intent. Assess understanding of task goal and restrictions. Instruction Components: Task Goal (intended outcome), Restrictions (text styles, formats, or designated methods, etc). Scoring: Rate outputs 1 to 5: 1. Irrelevant: No alignment. 2. Partial Focus: Addresses one aspect poorly. 3. Partial Compliance: - (1) Meets goal or restrictions, neglecting other. - (2) Acknowledges both but slight deviations. 4. Almost There: Near alignment, minor deviations. 5. Comprehensive Compliance: Fully aligns, meets all requirements. Figure 37: Guideline for rating model response using the Instruction Following aspect given an instruction and list of completions, adapted from Cui et al. [2023]. Informativeness or Helpfulness Aspect (prompt) # Informativeness / Helpfulness Assessment Evaluate if models outputs fulfill task objectives and provide high-quality, correct, and, informative content. Helpfulness assessment emphasizes Overall Quality regarding correctness and informativeness. Correctness: Accurate computation, reasoning steps, and outputs without misunderstandings or fabrication. Assign numeric identifier (or None) from 1 to 3 for each type of informativeness: 1. Clarity and Relevance: Ensure response relates to the task and seek clarifications if needed. 2. Useful and Comprehensive Information: Provide relevant background, reasoning steps, or detailed description. 3. Not Lengthy, No Repetition: Avoid verbosity or recycling content. Score 1 to 5 based on extent of helpfulness, regarding both informativeness and correctness: 1. Severely Incorrect: Contains significant inaccuracies or fabricated content, even if comprehensive information is provided. 2. Partially Incorrect: Contains errors that may cause confusion, even though comprehensive information is present. 3. Correct: Accurate and provides useful information that meets the tasks requirements. 4. Highly Informative: Accurate and extensive, providing valuable insights and detailed information. 5. Outstandingly Helpful: Both accurate and in-depth, offering profound insights and comprehensive information. Figure 38: Guideline for rating model response using the Helpfulness aspect given an instruction and list of completions, adapted from Cui et al. [2023]. Honesty Aspect (prompt) # Honesty and Uncertainty Expression Assessment Assess how well the model conveys honesty and uncertainty. Evaluate if the models confidence aligns with response correctness. - Indicators of Uncertainty: 1. Weakeners: e.g., guess, probably. 2. Refusal to answer. 3. Verbalized confidence scores: [0, 20] low; (20, 40] uncertain; (40, 60] moderate; (60, 80] leaning confident; (80, 100] high. - No uncertainty expression indicate confidence. - Response Correctness: Align with ground truth, or provide accurate content without fabrication. Scoring: Rate outputs 1 to 5 (or N/A): 1. Confidently Incorrect: Confident but entirely wrong. 2. Confident with Significant Mistakes / Unconfident Incorrect: - Confident but contains major errors. - Unconfident and entirely wrong. 3. Uncertain / Dont Know / Subtle Mistakes: - dont know or declines. - Confident but contains minor errors. - Unconfident and contains significant mistakes. 4. Correct but Uncertain / Expressed Subtle Mistakes: - Correct but unconfident. - Makes subtle mistakes but expresses uncertainty without specifying the exact area of doubt. 5. Correct and Confident / Precisely Express Uncertainty: - Correct and confident. - Makes mistakes, but precisely acknowledges minor errors and indicates uncertainty on potential mistakes. N/A. Not Applicable: For creative writing tasks. Figure 39: Guideline for rating model response using the Honesty aspect given an instruction and list of completions, adapted from Cui et al. [2023]. Truthfulness and Hallucination Aspect (prompt) # Truthfulness and Hallucination Assessment Evaluate the models accuracy in providing information without introducing misleading or fabricated details. Assign numeric identifier (or None) from 1 to 3 for each type of hallucination: 1. Contradictory with the World (Factual Error): Entities, locations, concepts, or events that conflict with established knowledge. 2. Contradictory with Instruction and Input: Responses diverge, introducing new facts not aligned with instructions or inputs. 3. Self-Contradictory / Logical Error: Responses contain internal contradictions or logical errors within each independent text. Scoring: Rate outputs 1 to 5 based on extent of hallucination: 1. Completely Hallucinated: Entirely unreliable due to hallucinations. 2. Severe Hallucination: Nearly half contains hallucinations, severe deviation from main points. 3. Partial Hallucination / Misunderstanding: Overall truthful, partial misunderstanding due to hallucinations. 4. Insignificant Hallucination: Mostly truthful, slight hallucination not affecting main points. 5. No Hallucination: Free of hallucinations. Figure 40: Guideline for rating model response using the Truthfulness aspect given an instruction and list of completions, adapted from Cui et al. [2023]. Table 35: Breakdown of model refusal rate by risk categories of the WildGuardTest benchmark. Categories Sensitive information government Social stereotypes/discrimination Toxic language hate speech Defamation Private information individual Cyberattack Misleading information Sexual content Mental health Violence and physical harm Fraud assisting illegal activities Causing material harm Others T칖LU 3 8B T칖LU 3 8B T칖LU 3 8B Llama 3.1 8B Ministral 8B Qwen 2.5 7B Instruct 87.8 90.8 75.6 69.6 96.3 68.8 72.1 85.1 93.3 72.5 81.7 80.0 99.0 Instruct 93.9 88.2 78.0 82.6 97.5 64.6 51.2 93.6 93.3 90.0 86.7 71.1 99.0 Instruct 81.6 71.1 46.3 43.5 91.4 54.2 27.9 68.1 80.0 45.0 66.7 55.6 90. DPO 100.0 100.0 95.1 100.0 100.0 97.9 97.7 100.0 100.0 97.5 95.0 100.0 100.0 SFT 100.0 98.7 100.0 100.0 98.8 97.9 100.0 97.9 100.0 97.5 98.3 100.0 100.0 100.0 100.0 95.1 100.0 98.8 95.8 95.3 100.0 100.0 97.5 95.0 100.0 100.0 65 Model Gemma 2 9B Inst Gemma 2 9B Inst SimPO Llama 3.1 8B Inst Llama 3.2 1B Inst Llama 3.2 3B Inst Ministral 2410 8B Inst OLMo 0724 7B Inst OLMoE 0924 1B 7B Inst Qwen 2.5 7B Inst Tulu 2 DPO 7B MATH::base-adpted 1.57 0.00 40.39 19.10 36.89 21.12 4.35 0.04 0.05 4. MATH::chat-v1 42.84 23.12 44.97 23.90 40.80 47.32 4.21 MATH::chat-v2 38.07 29.05 42.67 19.70 40.13 48.60 3.23 9.07 34.23 2.69 7.88 67.17 3.63 Table 36: Comparing evaluating instruction-tuned models on MATH using an evaluation setup adapted from base model evaluation with our designed evaluation practices for instruction-tuned models. Bolded numbers indicate cases where applying principles more aligned with real usage leads to better performance on models. Model Gemma 2 9B Inst Gemma 2 9B Inst SimPO Llama 3.1 8B Inst Llama 3.2 1B Inst Llama 3.2 3B Inst Ministral 2410 8B Inst OLMo 0724 7B Inst OLMoE 0924 1B 7B Inst Qwen 2.5 7B Inst Tulu 2 DPO 7B DROP::base-adpted 14.13 14.67 14.41 7.50 14.57 20.55 33.35 33.43 8.68 42.05 DROP::chat-v1 55.78 54.57 53.62 24.24 48.81 43.90 16.59 15.33 DROP::chat-v2 65.01 63.80 54.25 17.53 45.57 48.76 11.99 13.22 49.62 27.70 63.07 17.31 Table 37: Comparing evaluating instruction-tuned models on DROP using an evaluation setup adapted from base model evaluation with our designed evaluation practices for instruction-tuned models. Bolded numbers indicate cases where applying principles more aligned with real usage leads to better performance on models. Model Gemma 2 9B Inst Gemma 2 9B Inst SimPO Llama 3.1 8B Inst Llama 3.2 1B Inst Llama 3.2 3B Inst Ministral 2410 8B Inst OLMo 0724 7B Inst OLMoE 0924 1B 7B Inst Qwen 2.5 7B Inst Tulu 2 DPO 7B GSM8K::base-adpted 79.45 79.30 83.55 44.88 75.74 79.76 23.81 47.46 84.08 8.72 GSM8K::chat-v1 86.66 87.64 84.15 46.17 76.95 84.46 17.89 39.50 91.13 27.90 GSM8KP::chat-v2 84.15 87.04 81.65 37.91 76.12 83.47 17.36 39. 90.07 20.62 Table 38: Comparing evaluating instruction-tuned models on GSM8K using an evaluation setup adapted from base model evaluation with our designed evaluation practices for instruction-tuned models. Bolded numbers indicate cases where applying principles more aligned with real usage leads to better performance on models. 66 0-shot reasoning prompt for multiple-choice unseen tasks Answer the following multiple-choice question by giving the correct answer letter in parentheses. Provide CONCISE reasoning for the answer, and make sure to finish the response with \"Therefore, the answer is (ANSWER_LETTER)\" where (ANSWER_LETTER) is one of (A), (B), (C), (D), (E), etc. Question: {question} (A) {choice_A} (B) {choice_B} (C) ... Answer the above question and REMEMBER to finish your response with the exact phrase \"Therefore, the answer is (ANSWER_LETTER)\" where (ANSWER_LETTER) is one of (A), (B), (C), (D), (E), etc. Figure 41: Prompt used (with minor modification in the list of possible answer choices) for unseen multiple-choice evaluation tasks AGIEval English, GPQA, MMLU-Pro. 0-shot reasoning prompt for Deepmind Math unseen task Solve the following math problem efficiently: {math_problem} If the answer Show your work and conclude with the exact phrasing Therefore, the final answer is [answer]. hope it is correct. where [answer] is just the final number, expression, or answer label representing the solution. Some example answers from this question category: - {example_answer_1}. hope it is correct. - {example_answer_2}. hope it is correct. - {example_answer_3}. hope it is correct. is {example_answer_2}, is {example_answer_3}, is {example_answer_1}, conclude with Therefore, conclude with Therefore, conclude with Therefore, the final answer the final answer the final answer the answer the answer is is is If If Note the formatting for the following answer types: - If the answer is list (e.g., when there are two solutions to an equation), unless otherwise specified, present the solutions in list separated by commas ordering them from the smallest to biggest e.g.: 2, 10 - Powers should be written with **, for instance to the power of 2 should be written as x**2 - Use * for multiplication, e.g.: 2*x - For fractions, separate the numerator and denominator with slash (/) e.g.: -2/7 Figure 42: Prompt used for unseen evaluation task Deepmind Math. The example answers for each category are obtained by first randomly sampling 5 instances from the training set, then de-duplicating them and picking 3 that cover range of possible outputs (e.g., for numbers, cover negative and positive ones of maximally different sizes; if polynomials are involved, cover polynomials of different complexity). . 67 CoT Setting No CoT Explicit Variant 1 Explicit Variant Implicit CoT (chosen setting) Prompt The following are multiple choice questions (with answers) about {MMLU subject}. The following are multiple choice questions about {MMLU subject}. For each question, provide your step-by-step reasoning, then give your answer in the format Answer: where is one of A, B, C, or D. You are helpful assistant. Answer the following question by choosing an option. Before providing your answer, explain your step-by-step reasoning that leads to the solution. End your response with Answer: where is one of A, B, C, or D. The following are multiple choice questions about {MMLU subject }. Summarize your reasoning concisely, then conclude with Therefore, the answer is: where is one of A, B, C, or D. # Shot 5 0 0 0 Table 39: CoT prompts tested for MMLU. We report on the performance over MMLU using the Implicit CoT setting. No CoT 5-shot Explicit Variant 1 Explicit Variant 2 Implicit CoT T칖LU 3 8B DPO 64.4 57.8 67.4 68.8 T칖LU 3 8B SFT 62.1 Llama 3.1 8b Instruct 69.3 62.5 65.2 65. 70.9 70.3 70.9 Accuracy Gemma2 9b Instruct 73.0 66.0 71.6 74.6 Hermes3 8b 65.5 Qwen2.5 7b Instruct 74. 66.4 65.2 68.2 76.8 73.2 74.0 % Improved MMLU Subjects T칖LU 3 T칖LU 3 8B SFT 8B DPO 39.7 17.0 51.7 53.4 Ministral 8b Instruct 65.9 53.1 66.5 68. 81.0 74.1 Table 40: Results on prompt selection experiments. Left: Comparison among three tested CoT settings for MMLU. Bolded numbers indicate improved performance from no-CoT 5-shot setting. Our chosen setting (Implicit CoT) leads to consistent improvement over for T칖LU 3 and majority of its peer models. Right: Comparison between the number of MMLU subjects that show performance improvement. Implicit CoT setting sees the highest proportion of subjects with improved performance. 8B models T칖LU 3 8B RL (final) Gemma 2 9B Instruct Gemma 2 9B Instruct SimPO Hermes 3 Llama3.1 8B Llama 3.1 8B Instruct Magpie 8B Chat Ministral 8B Instruct Qwen 2.5 7B Instruct No CoT 5-shot 63.5 73.4 72.8 65.9 69. 64.3 65.9 74.4 CoT 0-shot 68.8 74.6 73.6 68.5 71.1 62.2 68.5 76.6 70B models T칖LU 3 70B RL (final) Hermes 3 Llama 3.1 70B Llama 3.1 Nemotron 70B It. Qwen 2.5 72B Instruct No CoT 5-shot 79.2 81.0 69.3 74. CoT 0-shot 83.2 83.8 71.1 76.6 Table 41: Comparison between 5-shot no CoT and CoT final results. We observe systematic performance improvement when using the CoT prompting. 68 F.3 IFEval Out-of-Distribution Constraints Our IFEval OOD dataset taxonomy contains 52 human-written constraint types, displayed in full in Table 42. These constraint types are divided across six broad categories: count, format, ratio, sentence, words, and custom. Each category contains between 3 and 12 representative constraints. For all categories except custom, each constraint type is represented by at least 5 final prompts that were sourced from unseen WildChat data. Instruction Group count Instruction conjunctions count count count count count count count count count format format format format format format format format format format format format ratio countries levenshtein numbers person_names pronouns punctuation unique_word_count word_count_range words_french camel_case emoji line_indent list newline no_bullets_bullets options parentheses quote_unquote quotes sub-bullets thesis overlap Description Use at least {N} different coordinating conjunctions in the response. Include names of locations from at least {N} different countries. Please rewrite the reference text to make it sound better and ensure Levenshtein distance of no more than {N} from the provided reference text. Reference Text: {reference_text} Include exactly {N} numbers in the response. Mention at least {N} different person names in the response. The response should include at least {N} pronouns. Use every standard punctuation mark at least once, including semicolons, colons, and the interrobang (?!). Use at least {N} unique words in the response. The response must contain between {min_n} and {max_n} words. Every {N}th word of your response must be in french. All variable names should be in camelCase. Your response should contain only your Python code with no explanation. Please use an emoji at the end of every sentence. Create stairs by incrementally indenting each new line. Answer with list of items, instead of bullet points use {sep}. Write each word on new line. Your answer must contain at least two sentences ending in period followed by at least two bullet points denoted by *. Answer with one of the following options: {options}. Do not give any explanation. Nest parentheses (and [brackets {and braces}]) at least 5 levels deep. Every quoted phrase must be followed by an unquoted explanation. Include quotes within quotes within quotes, at least 3 levels deep, alternating between double quotes and single quotes. Your response must include bullet points denoted by * and at least one sub-bullet point denoted by - for each bullet point. Each section must begin with thesis statement in italics, use HTML to indicate the italics. Maintain trigram overlap of {percentage}% (2%) with the provided reference text. 69 Instruction Group ratio Instruction sentence_balance ratio ratio ratio sentence sentence sentence words words words words words words words words words words words sentence_type sentence_words stop_words alliteration_increment increment keyword alphabet consonants last_first no_consecutive odd_even_syllables palindrome paragraph_last_first prime_lengths repeats start_verb vowel custom character_reverse custom csv_city custom csv_quotes custom csv_special_character Description Ensure that the ratio of sentence types (declarative, interrogative, exclamatory) in your response is balanced. Maintain 2:1 ratio of declarative to interrogative sentences in your response. Respond with three sentences, all containing the same number of characters but using all different words. Ensure that stop words constitute no more than {percentage}% of the total words in your response. Each sentence must have more alliterative words than the previous one. Each sentence in your response must contain exactly {small_N} more words than the previous one. The response must include keyword {keyword} in the {N}-th sentence. Each word in your response must start with the next letter of the alphabet, looping back to after Z. Ensure each word in your response has at least one consonant cluster (two or more consonants together). In your response, the last word of each sentence must become the first word of the next sentence. No two consecutive words can share the same first letter. Alternate between words with odd and even numbers of syllables. Include at least 10 palindromes, each at least 5 characters long. Each paragraph of your response must end with the same word it started with. Use only words with lengths that are prime numbers. The response should not repeat any word more than {small_N} times. The response must start with verb. Write paragraph using words that contain only one type of vowel. What animal is the national symbol of the US? Respond to this query, but make your sentence in reverse order of what it should be, per letter. Generate CSV data: The column names are [\"ID\", \"Country\", \"City\", \"Year\", \"Count\"], the data should be comma delimited. Please generate 7 rows. Generate CSV data: The column names are [\"StudentID\", \"Subject\", \"Grade\", \"Semester\", \"Score\"], the data should be tab delimited. Please generate 3 rows and enclose each single field in double quotes. Generate CSV data: The column names are [\"ProductID\", \"Category\", \"Brand\", \"Price\", \"Stock\"], the data should be comma delimited. Please generate 14 rows. Add one field which contains special character and enclose it in double quotes. 70 Instruction Group custom Instruction date_format_list Description List the start dates of all the battles Napoleon fought separated by commas, use the following date format: YYYYMM-DD. Do not provide an explanation. custom custom custom custom custom custom mcq_count_length european_capitals_sort Give me the names of all capital cities of european countries whose latitude is higher than than 45 degrees? List the capital cities without country names, separated by commas, sorted by latitude, from highest to lowest. Generate 4 multiple choice questions with 5 options each about \"20th century art history\". Each question should start with the label \"Question\". The questions should get progressively longer. Do not provide an explanation. Count from 10 to 50 but only print multiples of 7. List the countries of Africa in reverse alphabetical order, each on new line. Tell me 26-sentence story where each sentences first word starts with the letters of the alphabet in order. What animal is the national symbol of the US? Respond to this query, but make your sentence in reverse order of what it should be, per word. multiples reverse_newline sentence_alphabet word_reverse Table 42: IFEval out-of-distribution constraints. Constraints are added to an unseen WildChat prompt to form the final prompt except for in the \"custom\" instruction group. F.4 Subtask-level breakdown of HREF results Table 43 shows comparison of the performance of T칖LU 3 with that of Hermes 3 Llama 3.1 and Llama 3.1 Instruct models at 8B and 70B scales. Subtask Brainstorming (L) Open QA (E) Closed QA (LH) Extraction (LH) Generation (LH) Rewriting (LH) Summarization (L) Classification (LH) Numerical reasoning (LH) Multi-doc. synthesis (LH) Fact-checking (E) Overall Llama 3.1 8B Instruct 46.7 79.4 40.6 32.4 36.3 36.7 32.9 43.0 29.9 35.8 39.3 38.5 Hermes 3 Llama 3.1 8B 12.7 84.3 40.8 23.3 16.5 15.8 10.6 47.3 25.7 18.4 60.4 26.2 T칖LU 3 8B 41.6 58.8 23.5 18.3 35.6 34.0 21.0 32.3 28.0 41.6 21.9 32.7 Llama 3.1 70B Instruct 43.7 77.0 40.8 36.9 43.1 42.4 44.3 53.2 45.8 48.1 49.8 45. Hermes 3 Llama 3.1 70B 22.0 89.2 43.8 35.1 30.1 29.5 18.8 53.0 42.6 21.7 70.6 36.8 T칖LU 3 70B 50.6 58.3 35.1 38.1 44.4 44.1 28.7 42.8 42.1 50.2 26.0 42.3 Table 43: Comparison various models on HREF. Table shows the breakdown of win-rates (%) vs Llama 3.1 405B Instruct across the instruction following subtasks. Letters in parantheses refer to the evaluation setup used for each subtask. is LM-as-a-judge with Llama 3.1 70B Instruct as the judge, LH is the same that includes human-written references as context in the prompt, is embedding-based similarity with human-written references."
        }
    ],
    "affiliations": [
        "Allen Institute for AI",
        "University of Washington"
    ]
}