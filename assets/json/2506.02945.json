{
    "paper_title": "Quantitative LLM Judges",
    "authors": [
        "Aishwarya Sahoo",
        "Jeevana Kruthi Karnuthala",
        "Tushar Parmanand Budhwani",
        "Pranchal Agarwal",
        "Sankaran Vaidyanathan",
        "Alexa Siu",
        "Franck Dernoncourt",
        "Jennifer Healey",
        "Nedim Lipka",
        "Ryan Rossi",
        "Uttaran Bhattacharya",
        "Branislav Kveton"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "LLM-as-a-judge is a framework in which a large language model (LLM) automatically evaluates the output of another LLM. We propose quantitative LLM judges, which align evaluation scores of existing LLM judges to human scores in a given domain using regression models. The models are trained to improve the score of the original judge by using the judge's textual evaluation and score. We present four quantitative judges for different types of absolute and relative feedback, which showcases the generality and versatility of our framework. Our framework is more computationally efficient than supervised fine-tuning and can be more statistically efficient when human feedback is limited, which is expected in most applications of our work. We validate these claims empirically on four datasets using two base judges. Our experiments show that quantitative judges can effectively improve the predictive power of existing judges through post-hoc modeling."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 ] . [ 1 5 4 9 2 0 . 6 0 5 2 : r a"
        },
        {
            "title": "Quantitative LLM Judges",
            "content": "Aishwarya Sahoo, Jeevana Kruthi Karnuthala, Tushar Parmanand Budhwani University of Massachusetts Amherst Pranchal Agarwal, Sankaran Vaidyanathan University of Massachusetts Amherst Alexa Siu, Franck Dernoncourt, Jennifer Healey, Nedim Lipka, Ryan Rossi Adobe Research Uttaran Bhattacharya, Branislav Kveton Adobe Research"
        },
        {
            "title": "Abstract",
            "content": "LLM-as-a-judge is framework in which large language model (LLM) automatically evaluates the output of another LLM. We propose quantitative LLM judges, which align evaluation scores of existing LLM judges to human scores in given domain using regression models. The models are trained to improve the score of the original judge by using the judges textual evaluation and score. We present four quantitative judges for different types of absolute and relative feedback, which showcases the generality and versatility of our framework. Our framework is more computationally efficient than supervised fine-tuning and can be more statistically efficient when human feedback is limited, which is expected in most applications of our work. We validate these claims empirically on four datasets using two base judges. Our experiments show that quantitative judges can effectively improve the predictive power of existing judges through post-hoc modeling."
        },
        {
            "title": "Introduction",
            "content": "Measuring the quality of generated natural language presents several challenges due to the diverse range of generation methods and evaluation criteria [10]. The LLM-as-a-judge paradigm has recently emerged as compelling approach to these evaluation challenges. By leveraging the reasoning capabilities of large language models (LLMs), this approach can provide more nuanced assessments that correlate better with human judgments across diverse tasks [13]. Many LLM judges generate both qualitative feedback and numerical scores, thus combining the comprehensiveness of human evaluation with the scalability of automated metrics. Recent studies highlight issues with LLM judges such as low alignment with human scores, miscalibration, score compression, high variance, prompt sensitivity, and leniency bias [39, 46]. To address these issues, various approaches have been proposed to fine-tune LLMs to improve score prediction on specific tasks [9, 28]. However, fine-tuning still requires gradient updates across billions of model parameters, and the associated data and compute costs often make this approach impractical. As result, many users continue to rely on off-the-shelf models like GPT-4 or Claude [2, 15]. Equal contribution. Preprint. Under review. key source of the issues with current LLM judges is that they conflate qualitative reasoning with quantitative assessment. For example, fine-tuned LLM judges like Prometheus [21] are trained with the cross-entropy (CE) loss on gold-standard textual evaluations to generate both qualitative feedback and numerical scores. This is an inherent mismatch in objectives, as LLMs optimized for next-token prediction are tasked with producing accurate numerical scores, fundamentally different statistical problem. While LLMs excel at producing structured textual evaluations and semantic embeddings, they are poorly suited for regressing human scores or preferences [9]. This mismatch raises natural question: can qualitative summarization be decoupled from quantitative score prediction to achieve more accurate judgment? By decoupling these objectives, the LLM can focus on its strengthproducing textual evaluations through reasoningwhile accurate numerical score prediction is handled by classic machine learning models that are robust and loved by practitioners. This perspective is supported by prior works in interpretability and probing, which show that when model representations encode information relevant for downstream tasks, simple linear decoders can recover this information effectively [1, 14, 16, 3]. Building on this insight, we introduce quantitative judges, framework that enhances the original base judge by using its textual evaluation to predict more accurate numerical scores. We formulate four different quantitative judges for absolute rating and relative preference prediction tasks. Each judge has two stages: in the qualitative stage, frozen LLM judge generates textual evaluation and initial score, and in the quantitative stage, these outputs are used to predict better score. Our design is general, efficient, and applies to any base LLM judge. Specifically: 1. General: Our judges predict human scores using the base judges textual evaluation and score. The predictor is generalized linear model (GLM) [47] trained on human scores in the domain. The judges can be applied to absolute rating prediction tasks, such as regression and classification, or relative preference prediction tasks. They can be also applied to any base judge because we treat it as black box. We show the versatility of our framework by proposing four quantitative judges. 2. Statistically efficient: Our judges are based on GLMs, which can be learned from limited feedback. This is expected in most applications of our work. Our judges are designed such that the base judges score or distribution of it serves as bias term in our model. This allows us, at least in principle, to always learn at least as good judge as the base judge. 3. Computationally efficient: Learning of classic machine learning models on the top of LLM embeddings is more computationally efficient, and can also be more statistically efficient, than fine-tuning of LLMs. We report an order of magnitude speedups in Section 5. We comprehensively evaluate all proposed quantitative judges, showing their strengths and weaknesses. We also conduct ablation studies on training set size, regularization strength, and embedding choices. Through experiments on two base judges. Prometheus [21] and Llama 3.1 [12], we demonstrate that our quantitative judges consistently outperform base judges on both absolute rating and relative preference datasets. They can also outperform fine-tuning of the base judges on both quality and computational efficiency simultaneously. In summary, quantitative judges offer practical and effective alternative to fine-tuning for improving LLM-based evaluation."
        },
        {
            "title": "2 Background",
            "content": "LLMs are increasingly used not only to generate outputs but also to evaluate them, paradigm known as the LLM-as-a-judge [21, 41, 13, 53, 39, 56]. This approach uses natural language capabilities of LLMs to simulate human evaluations, offering scalable, cost-effective, and reproducible alternative. The output of the LLM judge can be natural language assessment, quantitative score, or pairwise preference. Both the text assessment and numeric score are often generated [21]. While efficient, the metrics that LLM judges produce have been criticized as not consistently aligned with human assessments [8, 10]. 2.1 LLM-as-a-Judge and Fine-Tuning Early methods, such as Prometheus [21], introduced LLMs fine-tuned for both absolute rating and pairwise ranking. These models tried to replicate human judgment, often outperforming heuristic 2 metrics in aligning with human preferences. Recent works, like LLMEval [53] and others [43, 25], further refined this setup by training on diverse instruction-response datasets, such as Alpaca52k [4], LMSYS-Chat [54], and ToxicChat [27], using feedback from GPT-4 or GPT-3.5. To improve the consistency and depth of judging, CritiqueLLM [19] proposed multi-path prompting that combines pointwise and pairwise strategies and JudgeLM [56] explored reference-free fine-tuning. Self-curation methods have been proposed to improve the alignment of judges. Yuan et al. [50] introduced self-rewarding LMs that generate both responses and reward signals to create preference datasets for direct preference optimization (DPO) [34]. Pang et al. [31] and Trivedi et al. [41] explored rationalization-based preferences by generating multiple CoT explanations from fixed seed model. Other works [49, 52] integrated iterative feedback from stronger models or humans to refine evaluation granularity. While these methods improve alignment, they often rely on aggressive fine-tuning, which can introduce variance and bias. Small changes in the training data can lead to noticeable shifts in outputs, particularly when human feedback is limited. Moreover, strong priors from pre-training may be inadvertently distorted, undermining judgment consistency. 2.2 LLM Benchmarks and Calibration Challenges Evaluating the effectiveness of LLM judges has become an active area of study. JudgeBench [38] and Eval4NLP [51] proposed benchmarks where LLMs evaluate paired responses, aiming to reflect model reasoning and preference ranking. BigGenBench [20] expanded this by covering 77 diverse tasks, although with limited instances per task, affecting statistical robustness. Thakur et al. [39] showed that while large judges align well with humans, smaller models often under-perform. To reduce evaluation variance, some methods compute mean scores by averaging outputs across multiple runs or implement voting schemes. FLEUR [22] integrated logit-based probability weighting to enhance score reliability for multi-modal evaluation. However, evaluations still suffer from dataset limitations and inconsistent generalization, especially since judges are often trained on general world knowledge and not calibrated to the particular domain being evaluated. 2.3 Drawbacks of LLM-as-a-Judge Recent studies have surfaced critical limitations in LLM-as-a-judge systems. Goel et al. [11] found that the judges tend to favor models similar in architecture or training data, creating blind spots due to shared inductive biases. Li et al. [24] revealed the preference leakage problem, where evaluation outcomes are skewed by overlap between judge training data and model-generated responses. The issues such as overconfidence [40, 7, 48] and bias [30] persist even after extensive fine-tuning, which additionally decreases the models generalization [17]. Despite ongoing efforts using reinforcement learning from human feedback (RLHF), robust, unbiased, and well-calibrated evaluation remains elusive [23, 18, 26]. As noted in recent survey [13], LLM evaluators are still unreliable in high-stakes or open-ended judgment scenarios. We propose an alternative framework that sidesteps instability from direct fine-tuning by decomposing LLM-based evaluation into qualitative (rationale-based) and quantitative (score-calibrated) components. Instead of generating new rationales for each training iteration, we fix rationales from base LLM judge and align them with human scores using classic machine learning models. This structured approach is reliable and robust, as we show empirically in Section 5."
        },
        {
            "title": "3 Setting",
            "content": "We study two types of LLM judges: evaluating single response and comparing two responses. Absolute LLM judge: The absolute judge evaluates single LLM response. The evaluation can have various forms: text, score, or both. The score can evaluate various aspects of the response, such as coherence, correctness, factual consistency, relevance, and adherence to task-specific guidelines. In this work, we assume that the judge generates both textual evaluation and its score. Specifically, let (x, y) be prompt-response pair from judged LLM. The absolute judge maps (x, y) to (e, b), where is the textual evaluation of given and is its score, reflecting its absolute value. 3 The primary advantage of an absolute judge is its consistency and standardization in scoring across different responses. However, it may require extensive prompt engineering or fine-tuning to align with human evaluations [18]. Relative LLM judge: The relative judge compares two or more LLM responses, and ranks them or selects the best one. It is commonly used in ranking-based assessments, preference modeling, and pairwise comparisons. Formally, let (x, y1, y2) be prompt-responses tuple from two judged LLMs. The relative judge maps (x, y1, y2) to (e, b), where is the textual evaluation of y1 and y2 given x, and {0, 1} is relative preference score. When = 1, the judge prefers the first response y1; otherwise it prefers the second response y2. We consider pairwise comparisons to simplify exposition and discuss an extension to multiple responses in Section 6. The relative judge mitigates score inconsistencies in absolute evaluation by leveraging direct comparisons but may introduce biases due to contextual dependencies between the responses [17]."
        },
        {
            "title": "4 Quantitative LLM Judges",
            "content": "This work is motivated by the observation that the scores of pre-trained LLM judges (Section 3) are not calibrated to any given domain, simply because they are trained on general world knowledge. To obtain better scores, we learn to predict them from evaluations of existing judges and human scores in the domain. Our predictors are generalized linear models (GLMs) [47], which generalize linear models to non-linear functions while inheriting their efficiency. We obtain more quantitative judge by using the predicted score from the learned model. Therefore, we call our judges quantitative. We introduce four quantitative judges and each has the following high-level structure. The existing judge is called base judge, and we assume that its evaluation comprises both text and score. We denote its textual evaluation by and its vector embedding by ϕ(e) Rd, where is the size of the embedding. We denote the base judges score by b. When the base judge assigns probabilities to its scores, we denote them by p. At inference time, when we judge, human score is predicted from ϕ(e), along with or p. At training time, we use ground-truth human scores to train the predictor. We visualize the architectures of our quantitative judges in Figure 1 and present them next. 4.1 Least-Squares Judge The least-squares (LS) judge is an absolute judge that predicts the score of single response from its evaluation by base judge. The predicted score is (e, b; θ) = (ϕ(e) b)θ + , (1) where ϕ(e) Rd is the embedding of base judges textual evaluation, is its score, is concatenated vector of and v, θ Rd+1 is learned model parameter, and is population bias. The population bias plays the same role as the bias term in linear regression. We introduce so that we can always learn judge that performs at least as well as the base judge. In particular, when θ = 0d 1 and = 0, (1) becomes the base judges score b. We learn the judge by minimizing the squared loss, which lends the judge its name. Specifically, we minimize regularized squared loss L(θ) = (cid:80)n 2 over data points, where et is the textual evaluation, bt is the base judges score, and st is the human score in data point [n]. The regularization strength γ > 0 is set automatically by cross-validation. t=1(f (et, bt; θ) st)2 + γθ2 4.2 Multinomial Judge The multinomial (MN) judge is an absolute judge designed for predicting categorical scores, such as Likert scores. The Likert score can be viewed as an absolute score or as ranking among options [6]. The MN judge is designed for the former interpretation. Our relative judges in Sections 4.3 and 4.4 are designed for the latter. The MN judge predicts the most likely score from set of scores S. The probability of score is computed as π(s e, p; Θ) = exp[(ϕ(e) log ps)θs + cs] sS exp[(ϕ(e) log ps)θs + cs] , (cid:80) (2) 4 Prompt Response Response y1 Prompt Response y2 Response y1 Prompt Response Absolute judge Relative judge Absolute judge Absolute judge Text Score Text Score Text e1 Score b1 Text e2 Score LS / MN judge BTL judge BTL2 judge Absolute score Relative score Relative score Figure 1: Visualization of the architectures of the least-squares (LS), multinomial (MN), BradleyTerry-Luce (BTL), and two-headed BTL (BTL2) judges described in Section 4. where ϕ(e) Rd is the embedding of base judges textual evaluation, ps is the probability that the base judge predicts score s, = (ps)sS is vector of all the probabilities, θs Rd+1 is learned parameter for score s, and cs is population bias towards score s. We denote all learned parameters by Θ = (θs)sS and estimate ps using the next token probability in [13, Section 2.3.3]. Since (2) is equivalent to the probability of outcome in multinomial logistic regression [29, Chapter 8], the population bias cs plays the same role. We introduce so that we can always learn judge that performs at least as well as the base judge. Specifically, when θs = 0d 1 and cs = 0 for all S, the predicted probability becomes the base judges probability because exp[log ps] sS exp[log ps] (cid:80) = ps sS ps (cid:80) = ps . The last equality holds because is probability vector and thus (cid:80) We learn the judge by maximizing the probability of correct score predictions. This is equivalent to minimizing regularized cross-entropy loss L(θ) = (cid:80)n 2, where et is the textual evaluation, pt is distribution over base judges scores, and st is the human score in data point [n]. The regularization strength γ > 0 is set automatically by cross-validation. t=1 log π(st et, pt; Θ) + γθ2 sS ps = 1. We consider both LS and MN judges because they provide different perspectives on predicting an absolute score: regression versus classification. The LS judge treats the scores as real numbers and minimizes the squared error. The MN judge treats the scores as discrete choices and maximizes the accuracy of predicting them. 4.3 Bradley-Terry-Luce Judge The Bradley-Terry-Luce (BTL) judge is relative judge that estimates the preference of one response over another from its evaluation by relative base judge. The judge is motivated by arguably the most popular discrete choice model in human preference modeling [5]. The probability that the first response is preferred is computed as (cid:32)(cid:18) π(e, p; θ) = µ ϕ(e) log (cid:19) 1 (cid:33) θ + , (3) where µ is sigmoid function, ϕ(e) Rd is the embedding of base judges textual evaluation, is the probability that the base judge prefers the first response, θ Rd+1 is learned model parameter, and is population bias. We estimate using the next token probability in [13, Section 2.3.3]. The first response is preferred when π(e, p; θ) > 0.5; otherwise the second one is preferred. The population bias plays the same role as the bias term in logistic regression. We introduce so that we can always learn judge that performs at least as well as the base judge. Specifically, when 5 θ = 0d 1 and = 0, the predicted probability becomes the base judges probability because (cid:18) µ log (cid:19) = 1 (cid:104) 1 log 1p (cid:105) = 1 1 + 1p = . 1 + exp We learn the judge by maximizing the probability of ranking correctly. We pose this as minimizing regularized logistic loss L(θ) = (cid:80)n t=1[st log π(et, pt; θ) + (1 st) log(1 π(et, pt; θ))] + γθ2 2, where et is the textual evaluation, pt is the probability that the base judge prefers the first response, and st {0, 1} is the human score in data point [n]. When st = 1, the human prefers the first response; and when st = 0, the human prefers the second one. The regularization strength γ > 0 is set automatically by cross-validation. 4.4 Two-Headed BTL Judge The two-headed BTL (BTL2) judge is BTL judge that estimates the preferred response from two separate absolute evaluations. This builds on the findings that pointwise evaluators tend to be more robust [17], while pairwise evaluators are more susceptible to superficial cues due to inherent biases in LLMs [42, 8]. Our empirical results in Table 2 strongly support this approach. We instantiate BTL2 within the framework of Section 4.3 as follows. Let ϕ(e1), ϕ(e2) Rd be the embeddings of base judges textual evaluations e1 and e2, respectively. Let b1, b2 > 0 be the base judges scores for e1 and e2, respectively. The probability that the first response is preferred is (3), where ϕ(e) = ϕ(e1) ϕ(e2) and = b1/(b1 + b2). We motivate this representation by noting that the difference of the embeddings ϕ(e) = ϕ(e1) ϕ(e2) captures the difference of word affinities in the two responses. The probability biases the response based on base judges scores. Specifically, when b1 > b2, log 1p > 0 and the first response is preferred; otherwise the second response is preferred. The judge is learned as in Section 4.3. We set pt = bt,1/(bt,1 + bt,2), where bt,1, bt,2 > 0 are the base judges scores for both responses in data point [n]."
        },
        {
            "title": "5 Experiments",
            "content": "To validate the performance of our quantitative judges, we comprehensively evaluated them on four tasks: two for absolute rating and two for relative preference prediction. We compare them to base judges as well as fine-tuned judges. We use 6 metrics in rating tasks and 7 in preference tasks. The metrics evaluate prediction errors, classification accuracy, and correlation. 5.1 Datasets and Metrics Our datasets span both absolute rating and relative preference tasks. Summarize from Feedback [37] is human-annotated dataset with summary responses rated on 7-point Likert scale. We use the axis subset, which contains absolute scores on different axes, such as overall, accuracy, coverage, and coherence. We use the overall score in our experiments, train on their validation set (8.59k data points), and test on their test set (6.31k data points). HelpSteer2 [44, 45] is dataset of absolute ratings for instruction-response pairs with correctness, coherence, complexity, verbosity, and overall helpfulness scores. The helpfulness is score on 5-point Likert scale and we use it in our experiments. We train on their training set (20.3k data points) and test on their validation set (1.04k data points). Offset Bias [32] is synthetic pairwise preference dataset composed of instruction-response triplets (x, y1, y2), where is prompt, y1 is good response, and y2 is high-quality flawed response. This dataset is designed to confuse judges by injecting critical errors into otherwise fluent outputs, and is constructed using GPT-4 and Claude-3 with prompting strategies targeting off-topic and erroneous behavior. We create our own training (6.8k data points) and test (1.7k data points) sets from their publicly available training set. Nectar [55] is large-scale preference dataset where GPT-4 ranks responses from seven different models. We convert each data point into (cid:0)7 (cid:1) pairwise comparisons required by our BTL and BTL2 judges. Our own training (83.9k data points) and test (21k data points) sets are created from their publicly available training set. 2 6 Summarize from Feedback HelpSteer2 Method MSE MAE Acc. ρ τ MSE MAE Acc. ρ τ Prometheus Absolute base 6.346 2.041 0.168 0.317 0.315 0.272 2.232 1.039 0.355 0.197 0.152 0.134 2.626 1.362 0.195 0.323 0.292 0.214 1.431 0.967 0.291 0.295 0.244 0.187 3.237 1.425 0.229 0.318 0.289 0.212 1.657 1.068 0.416 0.171 0.148 0.114 3.622 1.445 0.275 0.370 0.330 0.285 2.133 0.923 0.437 0.336 0.300 0.271 LS MN SFT Llama Absolute base 4.697 1.804 0.158 0.277 0.212 0.184 2.188 1.115 0.303 0.223 0.186 0.160 2.700 1.387 0.189 0.225 0.176 0.128 1.366 0.951 0.297 0.290 0.243 0.184 LS 3.680 1.515 0.203 0.202 0.162 0.118 2.116 1.304 0.419 0.020 0.032 0.024 MN 3.067 1.442 0.191 0.225 0.211 0.182 2.156 0.977 0.397 0.225 0.153 0.138 SFT Naive 2.871 1.391 0.187 1.557 0.985 0.300 Table 1: Evaluation on rating prediction tasks. We report three prediction metrics (MSE, MAE, and accuracy) and three correlation metrics (Pearsons r, Spearmans ρ, and Kendalls τ ). The best result for each dataset and base judge is reported in bold. The underlined numbers are statistically significant gains over the absolute base judge at < 0.05. The naive baselines are the mean and majority Likert ratings. Offset Bias Nectar Method Acc. Pre. Rec. F1 ρ τ Acc. Pre. Rec. r ρ τ Prometheus Absolute base 0.648 0.658 0.648 0.650 0.298 0.298 0.298 0.625 0.625 0.625 0.625 0.250 0.250 0.250 0.535 0.535 0.535 0.535 0.049 0.049 0.049 0.707 0.723 0.707 0.702 0.430 0.430 0.430 0.605 0.613 0.605 0.567 0.206 0.224 0.183 0.691 0.693 0.691 0.690 0.418 0.416 0.340 0.783 0.800 0.657 0.721 0.634 0.628 0.513 0.711 0.721 0.696 0.708 0.504 0.503 0.411 0.788 0.784 0.614 0.688 0.541 0.541 0.541 0.751 0.734 0.721 0.727 0.497 0.497 0.497 Relative base BTL BTL2 SFT Llama Absolute base 0.615 0.624 0.615 0.617 0.229 0.229 0.229 0.642 0.642 0.642 0.642 0.284 0.284 0.284 0.531 0.543 0.531 0.532 0.073 0.073 0.073 0.710 0.723 0.710 0.705 0.433 0.433 0.433 Relative base 0.636 0.633 0.636 0.630 0.311 0.319 0.261 0.694 0.695 0.694 0.694 0.440 0.439 0.358 BTL 0.800 0.802 0.702 0.749 0.657 0.645 0.527 0.635 0.637 0.628 0.632 0.339 0.338 0.276 BTL2 0.723 0.725 0.604 0.659 0.435 0.435 0.435 0.770 0.773 0.764 0.769 0.541 0.541 0.541 SFT Table 2: Evaluation on preference prediction tasks. We report four prediction metrics (accuracy, precision, recall, and F-score) and three correlation metrics (Pearsons r, Spearmans ρ, and Kendalls τ ). The best result for each dataset and base judge is reported in bold. The underlined numbers are statistically significant gains over the best base judge at < 0.05. Summarize from Feedback and HelpSteer2 provide absolute ratings for language quality, covering summarization and instruction-following domains. They are widely adopted in the LLM evaluation literature. Offset Bias and Nectar are synthetic datasets that focus on relative comparison, testing robustness to adversarial scenarios and scalability. Our selection of datasets ensures that we test on both calibrated human judgments and challenging synthetic ground-truth. (cid:17) We consider three types of metrics. For rating prediction tasks, we report the mean squared error (MSE), mean absolute error (MAE), and Likert scale accuracy. For preference prediction tasks, we report accuracy (probability that the judge agrees with the ground-truth preference), precision (cid:16) , and the F1-Score. TP, FP, and FN denote the number of true positives, false positives, and false negatives, respectively. We also report three correlation metrics: Pearson correlation coefficient (r), Spearmans rank correlation coefficient (ρ), and Kendalls τ . The correlation metrics show the utility of the predictive scores. In particular, even if the scores cannot be predicted well, they may be predicted well enough for ranking them, which often suffices in practice. (cid:16) , recall +F +F (cid:17) 5.2 Implementation Details We have two types of baselines: the absolute and relative base judges described in Section 3, and their fine-tuned versions (SFT). We fine-tune the judges to predict human scores from their prompts instantiated with evaluated responses. We experiment with two base judges: Prometheus-7B-V2 [21] (Prometheus) and Llama-3.1-8B-Instruct [12] (Llama). Prometheus is specialized model fine-tuned for evaluation whereas Llama is general instruction-following model. We experiment with Llama to test if our framework generalizes beyond specialized evaluators. 7 Judge Summarize from Feedback HelpSteer2 Offset Bias Nectar LS MN BTL BTL2 SFT 0.320 + 0.455 0.320 + 0.468 - - 14.160 0.880 + 0.450 0.880 + 0.461 - - 27.460 - - 1.569 + 0.457 2.247 + 0.538 19.300 - - 21.483 + 1.249 34.731 + 1.221 276. Table 3: Training times in GPU minutes per dataset with Prometheus base judge. We break the times of our judges into computing embeddings plus training the judge. Our evaluation protocol follows the grading methodology of Prometheus [21], which defines the two types of judges in Section 3. To ensure consistency across all judges, we use standardized grading prompts (Appendix A). All models are queried using vLLM and the same decoding configuration: temperature = 0.1, top_p = 0.9, and top_k = -1 (unrestricted sampling). Embeddings in our judges ϕ(e) are obtained from the base judge using the output of the final layer. Our judges are trained by SGD [35] and we set the regularization strength γ by 5-fold cross-validation. All reported results are on test sets. Unless explicitly stated, we train on 10% of our training sets, to mimic real-world setting where the number of human ratings is from mid hundreds (HelpSteer2) to multiple thousands (Nectar). We show how the performance of our judges varies with training set size in Section 5.5 and Appendix B. All results are averaged over 10 randomly chosen training sets. We use the multiple runs to determine statistical significance. 5.3 Results Our results on absolute rating prediction tasks are reported in Table 1. We start with the LS judge on Summarize from Feedback dataset with the Prometheus base judge. The MSE of the LS judge (2.626) is less than half of that of the base judge (6.346), and the lowest of all methods. This is expected because we optimize the MSE (Section 4.1). The LS judge has also the lowest MAE. The accuracy of the MN judge (0.229) is 36% higher than that of the base judge (0.168). The MN judge is outperformed by SFT, which also maximizes the probability of predicting the correct score but using supervised fine-tuning (Section 5.2). The MN judge is trained in fraction of the time of SFT (Section 5.4) though. Finally, although we optimize for rating prediction, the correlation metrics are comparable to the base judge, except for drop in Kendalls τ . We report confusion matrices of our judges in Appendix C. We observe similar trends for LS and MN judges with the Llama base judge, except that the MN judge has the highest accuracy and all correlation metrics drop. On HelpSteer2 dataset, with both Prometheus and Llama base judges, there are two changes in the trends. First, the LS judge significantly improves over the correlation metrics of the base judges. Second, SFT attains the lowest MAE with the Prometheus base judge. In summary, Table 1 shows that quantitative judges, despite their simplicity, outperform pre-trained judges on optimized metrics, and can outperform more complex and computationally costly alternatives. Our results in Table 1 show that optimization of absolute scores can degrade ranking metrics. We optimize the ranking metrics in Table 2. On Offset Bias dataset with the Prometheus base judge, the BTL judge outperforms the relative base judge in all metrics. While the absolute judge outperforms the BTL judge, it is also bested by the BTL2 judge in all metrics. Notably, the Pearson correlation coefficient and Spearmans rank correlation coefficient ρ double when compared to the base judge. We observe the same major improvements for BTL and BTL2 judges with the Llama base judge. The results on Nectar dataset are less conclusive. While the BTL2 judge performs comparably to SFT with the Prometheus base judge, SFT performs the best with the Llama base judge. We note that the BTL and BTL2 judges can be trained in fraction of the time for SFT (Section 5.4). In summary, quantitative judges can outperform pre-trained judges and more computationally costly alternatives. 5.4 Computation Times Our quantitative judges have both inference and training components (Section 4). The computational overhead at inference time is minimal when implemented correctly, because the base judges embedding ϕ(e) is available for free after the evaluation is generated; and the score or its probability 8 Figure 2: Test MSE of the LS and MN judges, and SFT, as function of the training set size. Figure 3: MSE of the LS judge and accuracy of the MN judge as function of the regularization strength γ. The base judge is Prometheus. can be obtained in O(1) time as is generated. Because of this, we only focus on training times in this discussion. We report the total GPU training times of our quantitative judges and SFT in Table 3. The times are measured on an NVIDIA-A100-SXM4-80GB GPU. We observe two major trends. First, the times of our judges are often dominated by that of computing the base judges embeddings ϕ(e). Second, our times may be an order of magnitude smaller than for SFT. Notably, our training time on Offset Bias dataset is 19.3/(2.247 + 0.538) = 6.93 times lower and yet we outperform SFT in all metrics in Table 2. This is direct benefit of the statistical and computational efficiency of simpler models at smaller sample sizes. 5.5 Ablation Studies Our ablation studies are in Appendix and we only summarize the main trends here. Figure 2 shows that the MSE of the LS judge decreases with more training data. While we observe the same trend for SFT, it cannot attain comparable MSE or attains it only at large sample sizes. We observe the latter for accuracy in about half of our experiments and report these trends in Appendix B.1. Figure 3 shows that the MSE of the LS judge and accuracy of the MN judge depend on the regularization strength γ. We noted this in our earlier experiments and thus set γ automatically by cross-validation (Section 5.2). Finally, Appendix B.3 shows that our quantitative judges can be implemented on other embeddings than those of the base judge. We observe similar performance on rating prediction tasks and small drop on preference prediction tasks."
        },
        {
            "title": "6 Conclusions",
            "content": "We introduce quantitative judges, family of LLM judges that disentangle qualitative reasoning from quantitative score prediction in LLM-as-a-judge. Our approach has two stages: the qualitative stage, where frozen LLM judge generates an evaluation, and the quantitative stage, where these outputs are used by lightweight model to predict human score. This decoupling mitigates the instability and bias of fine-tuning while preserving the interpretability and reasoning abilities of LLMs. We propose four quantitative judges and evaluate them on four datasets. We show that the quantitative judges consistently outperform the base judges, and can even outperform their fine-tuning on both quality and computational efficiency simultaneously. As such, quantitative judges offer promising new direction for quantitative and interpretable LLM evaluation at minimal additional cost. Limitations. When compared to pre-trained LLM judges, the main limitation of our approach is that it requires human data for training. We conduct ablation studies on training set size in Figure 2 9 and Appendix B.1. The quality of our models also depends on how good the embedding of the base judges textual evaluation is. To address this point, we experiment with two base judges and conduct an ablation study on embeddings in Appendix B.3. Future work. Our work can be extended in several directions. For instance, the BTL and BTL2 judges can be extended beyond pairwise comparisons by replacing the Bradley-Terry-Luce model [5] in (3) with the Plackett-Luce model [33]. One assumption in our work is that we consider the LLM embeddings to be frozen. We believe that the CoT process in the LLM judge, together with the resulting embeddings, can be optimized to produce better scores, akin to learning to reason [36]."
        },
        {
            "title": "References",
            "content": "[1] Guillaume Alain and Yoshua Bengio. Understanding intermediate layers using linear classifier probes, 2017. URL https://openreview.net/forum?id=ryF7rTqgl. [2] Anna Bavaresco, Raffaella Bernardi, Leonardo Bertolazzi, Desmond Elliott, Raquel Fernández, Albert Gatt, Esam Ghaleb, Mario Giulianelli, Michael Hanna, Alexander Koller, André F. T. Martins, Philipp Mondorf, Vera Neplenbroek, Sandro Pezzelle, Barbara Plank, David Schlangen, Alessandro Suglia, Aditya Surikuchi, Ece Takmaz, and Alberto Testoni. Llms instead of human judges? large scale empirical study across 20 nlp evaluation tasks, 2024. URL https://arxiv.org/abs/2406.18403. [3] Yonatan Belinkov. Probing classifiers: Promises, shortcomings, and advances. Computational Linguistics, 48(1):207219, March 2022. doi: 10.1162/coli_a_00422. URL https://aclanthology.org/2022.cl-1.7/. [4] Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, et al. On the opportunities and risks of foundation models. ArXiv, 2021. URL https://crfm.stanford.edu/assets/report. pdf. [5] Ralph Allan Bradley and Milton Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3-4):324345, 1952. [6] James Carifio and Rocco Perla. Resolving the 50-year debate around using and misusing likert scales. Medical Education, 42:11501152, 2008. [7] Lihu Chen, Alexandre Perez-Lebel, Fabian M. Suchanek, and Gaël Varoquaux. Reconfidencing LLMs from the grouping loss perspective, 2024. URL https://arxiv.org/abs/2402. 04957. [8] Cheng-Han Chiang and Hung-yi Lee. Can large language models be an alternative to human evaluations? In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1560715631, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.870. URL https://aclanthology.org/2023. acl-long.870/. [9] Cheng-Han Chiang, Hung yi Lee, and Michal Lukasik. Tract: Regression-aware fine-tuning meets chain-of-thought reasoning for llm-as-a-judge, 2025. URL https://arxiv.org/abs/ 2503.04381. [10] Sebastian Gehrmann, Elizabeth Clark, and Thibault Sellam. Repairing the cracked foundation: survey of obstacles in evaluation practices for generated text. Journal of Artificial Intelligence Research, 77:103166, 2023. [11] Shashwat Goel, Joschka Struber, Ilze Amanda Auzina, Karuna Chandra, Ponnurangam Kumaraguru, Douwe Kiela, Ameya Prabhu, Matthias Bethge, and Jonas Geiping. Great models think alike and this undermines ai oversight, 2025. URL https://arxiv.org/abs/2502. 04313. [12] Aaron Grattafiori, Abhimanyu Dubey, et al. The llama 3 herd of models, 2024. URL https: //arxiv.org/abs/2407.21783. [13] Jiawei Gu, Xuhui Jiang, Zhichao Shi, Hexiang Tan, Xuehao Zhai, Chengjin Xu, Wei Li, Yinghan Shen, Shengjie Ma, Honghao Liu, Saizhuo Wang, Kun Zhang, Yuanzhuo Wang, Wen Gao, Lionel Ni, and Jian Guo. survey on llm-as-a-judge, 2025. URL https://arxiv.org/abs/ 2411.15594. [14] John Hewitt and Christopher D. Manning. structural probe for finding syntax in word representations. In Jill Burstein, Christy Doran, and Thamar Solorio, editors, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4129 4138, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1419. URL https://aclanthology.org/N19-1419/. [15] Hui Huang, Yingqi Qu, Xingyuan Bu, Hongli Zhou, Jing Liu, Muyun Yang, Bing Xu, and Tiejun Zhao. An empirical study of llm-as-a-judge for llm evaluation: Fine-tuned judge model is not general substitute for gpt-4, 2024. URL https://arxiv.org/abs/2403.02839. [16] Dieuwke Hupkes, Sara Veldhoen, and Willem Zuidema. Visualisation and diagnostic classifiers reveal how recurrent and recursive neural networks process hierarchical structure. J. Artif. Int. Res., 61(1):907926, January 2018. ISSN 1076-9757. [17] Hawon Jeong, ChaeHun Park, Jimin Hong, Hojoon Lee, and Jaegul Choo. The Comparative Trap: Pairwise Comparisons Amplifies Biased Preferences of LLM Evaluators. arXiv e-prints, art. arXiv:2406.12319, June 2024. doi: 10.48550/arXiv.2406.12319. [18] Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli Tran-Johnson, Scott Johnston, Sheer El-Showk, Andy Jones, Nelson Elhage, Tristan Hume, Anna Chen, Yuntao Bai, Sam Bowman, Stanislav Fort, Deep Ganguli, Danny Hernandez, Josh Jacobson, Jackson Kernion, Shauna Kravec, Liane Lovitt, Kamal Ndousse, Catherine Olsson, Sam Ringer, Dario Amodei, Tom Brown, Jack Clark, Nicholas Joseph, Ben Mann, Sam McCandlish, Chris Olah, and Jared Kaplan. Language models (mostly) know what they know, 2022. URL https://arxiv.org/ abs/2207.05221. [19] Pei Ke, Bosi Wen, Zhuoer Feng, Xiao Liu, Xuanyu Lei, Jiale Cheng, Shengyuan Wang, Aohan Zeng, Yuxiao Dong, Hongning Wang, Jie Tang, and Minlie Huang. CritiqueLLM: Towards an informative critique generation model for evaluation of large language model generation, 2024. URL https://arxiv.org/abs/2311.18702. [20] Seungone Kim, Juyoung Suk, Ji Yong Cho, Shayne Longpre, Chaeeun Kim, Dongkeun Yoon, Guijin Son, Yejin Cho, Sheikh Shafayat, Jinheon Baek, Sue Hyun Park, Hyeonbin Hwang, Jinkyung Jo, Hyowon Cho, Haebin Shin, Seongyun Lee, Hanseok Oh, Noah Lee, Namgyu Ho, Se June Joo, Miyoung Ko, Yoonjoo Lee, Hyungjoo Chae, Jamin Shin, Joel Jang, Seonghyeon Ye, Bill Yuchen Lin, Sean Welleck, Graham Neubig, Moontae Lee, Kyungjae Lee, and Minjoon Seo. The biggen bench: principled benchmark for fine-grained evaluation of language models with language models, 2024. URL https://arxiv.org/abs/2406.05761. [21] Seungone Kim, Juyoung Suk, Shayne Longpre, Bill Yuchen Lin, Jamin Shin, Sean Welleck, Graham Neubig, Moontae Lee, Kyungjae Lee, and Minjoon Seo. Prometheus 2: An open source language model specialized in evaluating other language models. arXiv preprint arXiv:2405.01535, 2024. [22] Yebin Lee, Imseong Park, and Myungjoo Kang. Fleur: An explainable reference-free evaluation metric for image captioning using large multimodal model, 2024. URL https://arxiv. org/abs/2406.06004. [23] Jixuan Leng, Chengsong Huang, Banghua Zhu, and Jiaxin Huang. Taming overconfidence in llms: Reward calibration in rlhf, 2025. URL https://arxiv.org/abs/2410.09724. [24] Dawei Li, Renliang Sun, Yue Huang, Ming Zhong, Bohan Jiang, Jiawei Han, Xiangliang Zhang, Wei Wang, and Huan Liu. Preference leakage: contamination problem in llm-as-a-judge, 2025. URL https://arxiv.org/abs/2502.01534. [25] Lijun Li, Bowen Dong, Ruohui Wang, Xuhao Hu, Wangmeng Zuo, Dahua Lin, Yu Qiao, and Jing Shao. Salad-bench: hierarchical and comprehensive safety benchmark for large language models, 2024. URL https://arxiv.org/abs/2402.05044. [26] Miaomiao Li, Hao Chen, Yang Wang, Tingyuan Zhu, Weijia Zhang, Kaijie Zhu, Kam-Fai Wong, and Jindong Wang. Understanding and mitigating the bias inheritance in llm-based data augmentation on downstream tasks, 2025. URL https://arxiv.org/abs/2502.04419. [27] Zi Lin, Zihan Wang, Yongqi Tong, Yangkun Wang, Yuxin Guo, Yujia Wang, and Jingbo Shang. ToxicChat: Unveiling hidden challenges of toxicity detection in real-world user-ai conversation, 2023. URL https://arxiv.org/abs/2310.17389. [28] Michal Lukasik, Zhao Meng, Harikrishna Narasimhan, Yin-Wen Chang, Aditya Krishna Menon, Felix Yu, and Sanjiv Kumar. Better autoregressive regression with LLMs via regression-aware fine-tuning. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=xGs7Ch3Vyo. [29] Kevin Murphy. Machine Learning: Probabilistic Perspective. MIT Press, 2012. ISBN 9780262018029. [30] OpenAI. Gpt-4 technical report, 2023. URL https://arxiv.org/abs/2303.08774. [31] Richard Yuanzhe Pang, Weizhe Yuan, Kyunghyun Cho, He He, Sainbayar Sukhbaatar, and Jason Weston. Iterative reasoning preference optimization, 2024. URL https://arxiv.org/ abs/2404.19733. [32] Junsoo Park, Seungyeon Jwa, Meiying Ren, Daeyoung Kim, and Sanghyuk Choi. Offsetbias: Leveraging debiased data for tuning evaluators, 2024. URL https://arxiv.org/abs/2407. 06551. [33] Robin Lewis Plackett. The analysis of permutations. Journal of the Royal Statistical Society: Series (Applied Statistics), 24(2):193202, 1975. [34] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. In Advances in Neural Information Processing Systems 36, 2023. [35] Herbert Robbins and Sutton Monro. stochastic approximation method. The Annals of Mathematical Statistics, 22(3):400407, 1951. doi: 10.1214/aoms/1177729586. [36] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. DeepSeekMath: Pushing the limits of mathematical reasoning in open language models. CoRR, abs/2402.03300, 2024. URL https://arxiv.org/abs/2402.03300. [37] Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel M. Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul Christiano. Learning to summarize from human feedback. In NeurIPS, 2020. [38] Sijun Tan, Siyuan Zhuang, Kyle Montgomery, William Y. Tang, Alejandro Cuadron, Chenguang Wang, Raluca Ada Popa, and Ion Stoica. Judgebench: benchmark for evaluating llm-based judges, 2024. URL https://arxiv.org/abs/2410.12784. [39] Aman Singh Thakur, Kartik Choudhary, Venkat Srinik Ramayapally, Sankaran Vaidyanathan, and Dieuwke Hupkes. Judging the judges: Evaluating alignment and vulnerabilities in llms-asjudges, 2025. URL https://arxiv.org/abs/2406.12624. [40] Katherine Tian, Eric Mitchell, Allan Zhou, Archit Sharma, Rafael Rafailov, Huaxiu Yao, Chelsea Finn, and Christopher D. Manning. Just ask for calibration: Strategies for eliciting calibrated confidence scores from language models fine-tuned with human feedback, 2023. URL https://arxiv.org/abs/2305.14975. [41] Prapti Trivedi, Aditya Gulati, Oliver Molenschot, Meghana Arakkal Rajeev, Rajkumar Ramamurthy, Keith Stevens, Tanveesh Singh Chaudhery, Jahnavi Jambholkar, James Zou, and Nazneen Rajani. Self-rationalization improves llm as fine-grained judge, 2024. URL https://arxiv.org/abs/2410.05495. [42] Peiyi Wang, Lei Li, Liang Chen, Zefan Cai, Dawei Zhu, Binghuai Lin, Yunbo Cao, Lingpeng Kong, Qi Liu, Tianyu Liu, and Zhifang Sui. Large language models are not fair evaluators. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 9440 9450, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10. 18653/v1/2024.acl-long.511. URL https://aclanthology.org/2024.acl-long.511/. [43] Yidong Wang, Zhuohao Yu, Zhengran Zeng, Linyi Yang, Cunxiang Wang, Hao Chen, Chaoya Jiang, Rui Xie, Jindong Wang, Xing Xie, Wei Ye, Shikun Zhang, and Yue Zhang. Pandalm: An automatic evaluation benchmark for llm instruction tuning optimization, 2024. URL https://arxiv.org/abs/2306.05087. [44] Zhilin Wang, Alexander Bukharin, Olivier Delalleau, Daniel Egert, Gerald Shen, Jiaqi Zeng, Oleksii Kuchaiev, and Yi Dong. Helpsteer2-preference: Complementing ratings with preferences, 2024. URL https://arxiv.org/abs/2410.01257. 12 [45] Zhilin Wang, Yi Dong, Olivier Delalleau, Jiaqi Zeng, Gerald Shen, Daniel Egert, Jimmy J. Zhang, Makesh Narsimhan Sreedhar, and Oleksii Kuchaiev. Helpsteer2: Open-source dataset for training top-performing reward models, 2024. [46] Hui Wei, Shenghua He, Tian Xia, Fei Liu, Andy Wong, Jingyang Lin, and Mei Han. Systematic evaluation of llm-as-a-judge in llm alignment tasks: Explainable metrics and diverse prompt templates, 2025. URL https://arxiv.org/abs/2408.13006. [47] R. Wolke and H. Schwetlick. Iteratively reweighted least squares: Algorithms, convergence analysis, and numerical comparisons. SIAM Journal on Scientific and Statistical Computing, 9 (5):907921, 1988. [48] Miao Xiong, Zhiyuan Hu, Xinyang Lu, Yifei Li, Jie Fu, Junxian He, and Bryan Hooi. Can llms express their uncertainty? an empirical evaluation of confidence elicitation in llms, 2024. URL https://arxiv.org/abs/2306.13063. [49] Wenda Xu, Danqing Wang, Liangming Pan, Zhenqiao Song, Markus Freitag, William Yang Instructscore: Explainable text generation evaluation with finegrained Wang, and Lei Li. feedback, 2023. URL https://arxiv.org/abs/2305.14282. [50] Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Xian Li, Sainbayar Sukhbaatar, Jing Xu, and Jason Weston. Self-rewarding language models, 2024. URL https://arxiv.org/ abs/2401.10020. [51] Zhiyuan Zeng, Jiatong Yu, Tianyu Gao, Yu Meng, Tanya Goyal, and Danqi Chen. Evaluating large language models at evaluating instruction following, 2024. URL https://arxiv.org/ abs/2310.07641. [52] Mi Zhang, Xudong Pan, and Min Yang. JADE: linguistics-based safety evaluation platform for large language models, 2023. URL https://arxiv.org/abs/2311.00286. [53] Yue Zhang, Ming Zhang, Haipeng Yuan, Shichun Liu, Yongyao Shi, Tao Gui, Qi Zhang, and Xuanjing Huang. Llmeval: preliminary study on how to evaluate large language models. Proceedings of the AAAI Conference on Artificial Intelligence, 38(17):1961519622, Mar. 2024. doi: 10.1609/aaai.v38i17.29934. [54] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Tianle Li, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zhuohan Li, Zi Lin, Eric P. Xing, Joseph E. Gonzalez, Ion Stoica, and Hao Zhang. Lmsys-chat-1m: large-scale real-world llm conversation dataset, 2024. URL https://arxiv.org/abs/2309.11998. [55] Banghua Zhu, Evan Frick, Tianhao Wu, Hanlin Zhu, and Jiantao Jiao. Starling-7b: Improving llm helpfulness & harmlessness with rlaif, November 2023. [56] Lianghui Zhu, Xinggang Wang, and Xinlong Wang. JudgeLM: Fine-tuned large language models are scalable judges, 2025. URL https://arxiv.org/abs/2310.17631. 13 Figure 4: Test accuracy of the LS and MN judges, and SFT, as function of the training set size."
        },
        {
            "title": "A Prompts",
            "content": "Absolute LLM judge prompt You are fair judge assistant tasked with providing clear, objective feedback based on specific criteria, ensuring each assessment reflects the absolute standards set for performance. Task Description: An instruction (might include an Input inside it), response to evaluate, and score rubric representing evaluation criteria are given. 1. Write detailed feedback that assess the quality of the response strictly based on the given score rubric, not evaluating in general. 2. After writing feedback, write score that is an integer between {min_score} and {max_score}. You should refer to the score rubric. 3. The output format should look as follows: \"(write feedback for criteria) [RESULT] (an integer number between {min_score} and {max_score})\" 4. Please do not generate any other opening, closing, and explanations. Instruction: {instruction} Response: {response} Score Rubrics: {rubrics} Feedback: Relative LLM judge prompt You are fair judge assistant assigned to deliver insightful feedback that compares individual performances, highlighting how each stands relative to others within the same cohort. Task Description: An instruction (might include an Input inside it), two responses to evaluate (denoted as Response and Response B), and an evaluation criteria are given. 1. Write detailed feedback that assess the quality of the two responses strictly based on the given evaluation criteria, not evaluating in general. 2. Make comparisons between Response and Response B. Instead of examining Response and Response separately, go straight to the point and mention the commonalities and differences. 3. After writing the feedback, indicate the better response, either \"A\" or \"B\". 4. The output format should look as follows: \"Feedback: (write feedback for criteria) [RESULT] (Either \"A\" or \"B\")\" 5. Please do not generate any other opening, closing, and explanations. Instruction: {instruction} Response A: {response_a} Response B: {response_b} Score Rubrics: {rubrics} Feedback: The score rubrics in Table 4 mimic the original annotation guidelines, for humans in Summarize from Feedback and HelpSteer2 datasets, and GPT-4 in Nectar dataset. This ensures good performance of the base judges and informative reasoning for our quantitative judges. We use the same rubric text for both base judges to ensure consistency."
        },
        {
            "title": "B Ablation Studies",
            "content": "We conduct additional ablation studies on key components of our quantitative judges to gain deeper insights into their behavior. 14 Dataset Rubric text Summarize from Feedback HelpSteer [How good is the summary overall at representing the post? If its hard to find ways to make the summary better, give the summary high score. If there are lots of different ways the summary can be made better, give the summary low score. Judge on the following criteria while giving the feedback: Essence: is the summary good representation of the post?, Clarity: is the summary reader-friendly? Does it express ideas clearly? Accuracy: does the summary contain the same information as the longer post? Purpose: does the summary serve the same purpose as the original post? Concise: is the summary short and to-the-point? Style: is the summary written in the same style as the original post? While giving score, you can refer the following scoring rubrics. Try to interpolate to scores of 2, 3, 5 and 6 as those are not mentioned. You can only give single value for overall score. Score of 1: The summary is terrible. Score of 4: The summary is an okay representation of the post, but could be significantly improved. Score of 7: The summary is an excellent representation of the post. [Helpfulness can be measured by how useful and helpful the overall response is. While giving score, you can refer the following scoring rubrics. You can only give single value for the resulting score.] Score of 0: The response is not useful or helpful at all. The response completely missed the essence of what the user wanted. Score of 1: The response is borderline unhelpful and mostly does not capture what the user was looking for, but is still usable and helpful in small way. Score of 2: The response is partially helpful but misses the overall goal of the users query/input in some way. The response did not fully satisfy what the user was looking for. Score of 3: The response is mostly helpful and mainly aligned with what the user was looking for, but there is still some room for improvement. Score of 4: The response is extremely helpful and completely aligned with the spirit of what the prompt was asking for. Offset Bias Nectar Does the model provide relevant and useful responses to the users needs or questions? We have 2 rubrics, one is the helpfulness rubric, one is the harmlessness rubric. The helpfulness rubric is as follows: 1. The completion must be relevant and should represent the best possible assistant response to the given prompt. 2. The completion must be accurate and fulfill the prompts request with correctness. 3. If the prompt calls for creativity, the completion should be appropriately creative. If the prompt is analytical, the completion should be factual and objectively correct. 4. The completion should be detailed and meet the full level of explanation or elaboration expected by the prompt. The harmlessness rubric is as follows: 1. The completion must adhere to moral and ethical standards. 2. The content must be truthful and not misleading. 3. The tone of the response should be polite and respectful. 4. In cases where the request is not good natured, the completion should decline to respond while providing brief, polite explanation of why the request cannot be fulfilled. Completions should always maintain harmlessness in the face of adversarial requests. We may see conversations that are good natured and some that are not good natured. All conversations should be completed in manner that aligns with the harmlessness rubric. Good natured conversations additionally should align with the helpfulness rubric. Table 4: Rubric scores for base judges. Figure 5: Test accuracy of the LS and MN judges, and SFT, as function of the training set size. B.1 Training Set Size We start with varying the training set size. In Figure 2, we report the MSE on rating prediction tasks. We observe that the LS judge has lower MSE than SFT in all settings except for Summarize from Feedback dataset with 100% data. The MN judge has lower MSE than SFT at smaller sample sizes in 3 plots out of 4. In Figure 4, we report accuracy on rating prediction tasks. Our results are mixed. We observe that SFT has higher accuracy than our judges with the Prometheus base judge. On the other hand, with the Llama base judge, the MN judge has higher accuracy than SFT up to 20% data. 15 Figure 6: MSE of the LS judge and accuracy of the MN judge as function of the regularization strength γ. The base judge is Llama. Figure 7: Accuracy on Offset Bias and Nectar datasets with Prometheus and Llama base judges as function of the regularization strength γ. Figure 8: MSE and accuracy of LS and MN judges for Prometheus and all-MiniLM-L6-v2 embeddings on rating prediction tasks. In Figure 5, we report accuracy on preference prediction tasks. Our results are mixed. We observe that SFT has higher accuracy than our judges on Nectar dataset. On the other hand, on Offset Bias dataset, the BTL2 judge has higher accuracy than SFT up to 10% data. These results highlight that while our models are sample efficient and therefore perform well with limited data, SFT can outperform them when ample supervision is available. This happens because our models have O(d) parameters, where = 4096 is the embedding size. Fine-tuning of an LLM optimizes billions of parameters. B.2 Regularization Strength The impact of the regularization strength γ on our judges is investigated in Figures 3, 6 and 7. We observe that moderate regularization improves generalization, with performance degrading at both extremes (under-regularization and over-regularization). This points to the importance of tuning γ. To avoid putting this burden on human, we suggest setting the regularization strength γ automatically based on k-fold cross-validation. We use = 5 in our experiments. B.3 Embeddings The impact of the embedding choice on our judges is investigated in Figures 8 and 9. Specifically, we reduce the dimensionality of Prometheus embeddings from 4096 dimensions to 384 using PCA and compare them to those of all-MiniLM-L6-v2, which also have 384 dimensions. We start with the discussion of Figure 8, which reports metrics on rating prediction tasks. For the MN judge and Summarize from Feedback dataset, the new embedding has both lower MSE and higher accuracy. For the MN judge and HelpSteer2 dataset, the new embedding has lower MSE. In all other cases, 16 Figure 9: Accuracy and F1 score of BTL and BTL2 judges for Prometheus and all-MiniLM-L6-v2 embeddings on preference prediction tasks. Figure 10: Confusion matrices of base, LS, and MN judges on Summarize from Feedback dataset. Figure 11: Confusion matrices of base, LS, and MN judges on HelpSteer2 dataset. the new embedding is either comparable or worse. Overall, we do not see any trend or added benefit of using the original judges embeddings for rating prediction tasks. For preference prediction tasks (Figure 9), we observe that Prometheus embeddings consistently outperform the new embeddings in all metrics. This could be attributed to the discriminative nature of preference prediction tasks, for which the the original judges embeddings may be better suited."
        },
        {
            "title": "C Confusion Matrices",
            "content": "The confusion matrices of base, LS, and MN judges on Summarize from Feedback and HelpSteer2 datasets are reported in Figures 10 and 11, respectively. These matrices provide deeper insight on how our judges improve over the base judge in rating prediction tasks. Specifically, we observe that the base judge is poorly calibrated because it is unaware of the score distribution in the domain. For instance, it never predicts Likert scores 5 and 6 in Figure 10, and barely predicts Likert scores 3 and 2 in Figures 10 and 11, respectively. On the other hand, predictions of the LS judge concentrate around the mean prediction because it minimizes the MSE. The mean prediction is around 6 in Figure 10 and 4 in Figure 11. Finally, the MN judge addresses this limitation by treating each Likert score as separate category. Therefore, the proximity of the Likert scores does not influence the optimized loss and the predictions of the judge are more evenly distributed across the full score range."
        }
    ],
    "affiliations": [
        "Adobe Research",
        "University of Massachusetts Amherst"
    ]
}