{
    "paper_title": "IMA++: ISIC Archive Multi-Annotator Dermoscopic Skin Lesion Segmentation Dataset",
    "authors": [
        "Kumar Abhishek",
        "Jeremy Kawahara",
        "Ghassan Hamarneh"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Multi-annotator medical image segmentation is an important research problem, but requires annotated datasets that are expensive to collect. Dermoscopic skin lesion imaging allows human experts and AI systems to observe morphological structures otherwise not discernable from regular clinical photographs. However, currently there are no large-scale publicly available multi-annotator skin lesion segmentation (SLS) datasets with annotator-labels for dermoscopic skin lesion imaging. We introduce ISIC MultiAnnot++, a large public multi-annotator skin lesion segmentation dataset for images from the ISIC Archive. The final dataset contains 17,684 segmentation masks spanning 14,967 dermoscopic images, where 2,394 dermoscopic images have 2-5 segmentations per image, making it the largest publicly available SLS dataset. Further, metadata about the segmentation, including the annotators' skill level and segmentation tool, is included, enabling research on topics such as annotator-specific preference modeling for segmentation and annotator metadata analysis. We provide an analysis on the characteristics of this dataset, curated data partitions, and consensus segmentation masks."
        },
        {
            "title": "Start",
            "content": "IMA++: ISIC Archive Multi-Annotator Dermoscopic Skin Lesion Segmentation Dataset Kumar Abhishek, Jeremy Kawahara, and Ghassan Hamarneh Medical Image Analysis Lab, School of Computing Science, Simon Fraser University, Canada AIP Labs, Hungary 1 5 2 0 2 5 2 ] . [ 1 2 7 4 1 2 . 2 1 5 2 : r AbstractMulti-annotator medical image segmentation is an important research problem, but requires annotated datasets that are expensive to collect. Dermoscopic skin lesion imaging allows human experts and AI systems to observe morphological structures otherwise not discernable from regular clinical photographs. However, currently there are no large-scale publicly available multi-annotator skin lesion segmentation (SLS) datasets with annotator-labels for dermoscopic skin lesion imaging. We introduce ISIC MultiAnnot++, large public multi-annotator skin lesion segmentation dataset for images from the ISIC Archive. The final dataset contains 17,684 segmentation masks spanning 14,967 dermoscopic images, where 2,394 dermoscopic images have 2-5 segmentations per image, making it the largest publicly available SLS dataset. Further, metadata about the segmentation, including the annotators skill level and segmentation tool, is included, enabling research on topics such as annotator-specific preference modeling for segmentation and annotator metadata analysis. We provide an analysis on the characteristics of this dataset, curated data partitions, and consensus segmentation masks. IEEE SocietyEngineering Medicine & Biology Society (EMBS) Data RepositoryÄ±10.5281/zenodo. Code Repositorysfu-mial/IMAplusplus Data Typesegmentation masks; medical image segmentation Index Termsannotator agreement, dermatology, dermoscopy, machine learning, melanoma, multiple annotators, segmentation, skin lesion I. BACKGROUND Skin cancer is the most common form of cancer diagnosed globally, with an estimated 1.5 million new diagnoses in 2022 alone, according to the World Health Organization [1]. Melanoma, the most aggressive form of skin cancer, alone accounted for 330,000 new diagnoses and 60,000 deaths that same year. With the incidence rate of skin cancers rising driven by factors including but not limited to: ageing populations, rising life expectancy, increased sun exposure, and environmental factors [2], [3], the incidence rate of melanoma is projected to double every 10-20 years [4]. This burgeoning global burden of skin diseases, estimated to be over 42 million DALYs (disability-adjusted life years) in 2019 [5], coupled with projections of declining dermatologist-to-population ratio [6], strongly motivates the research and development of automated methods for dermatological image analysis. Corresponding author: kabhishe@sfu.ca (Kumar Abhishek) TABLE COMPARING PUBLIC SINGLEAND MULTI-ANNOTATOR SKIN LESION IMAGE SEGMENTATION DATASETS. IMA++ SPANS THE LARGEST NUMBER OF IMAGES AND HAS THE LARGEST NUMBER OF SEGMENTATIONS. Dataset Year Modality Total Images (train/val/test) Multi-Annotator Segmentations? Total Segmentations SCD [7] 2013 Clinical DermoFit [8] 2013 Clinical PH2 [9] 2013 Dermoscopy ISIC 2016 [10] 2016 Dermoscopy ISIC 2017 [11] 2017 Dermoscopy ISIC 2018 [12] 2018 Dermoscopy HAM10000 [13], [14] 2020 Dermoscopy ISIC 2019-Seg [15] 2023 Dermoscopy IMA++ 2025 Dermoscopy 206 (N/A) 1,300 (N/A) 200 (N/A) 1,279 (900/-/379) 2,750 (2,000/150/600) 3,694 (2,594/100/1000) 10,015 (N/A) 100 (N/A) 14,967 (X/Y/Z) 206 1,300 200 1,279 2, 3,694 10,015 300 17,"
        },
        {
            "title": "Segmentation of skin lesions is a crucial",
            "content": "Dermoscopy is widely-used non-invasive imaging technique for the examination of pigmented skin lesions, allowing clinicians to visualize both morphological surface features and subsurface structures otherwise obscured to the naked eye [16][18]. Studies have shown that when used by trained experts, dermoscopy significantly improves both the sensitivity [19], [20] and specificity [21], [22] of melanoma diagnosis. Dermoscopy has also been the target modality for automated skin image analysis. For example, almost 3 decades ago, Binder et al. [23] used artificial neural networks for detecting malignant melanomas from dermoscopic skin lesion images. task in the automated skin lesion image analysis pipeline. Rule-based diagnosis clinical prediction rules, including the most widely used [24], [25] ABCD (Asymmetry, Border, Color, Differential structure) [26], rely on an accurate delineation of the skin lesion boundary. In recent years, deep learning (DL)-based skin image analysis methods rely on segmentation either as an end-goal, an intermediate task (e.g., analyzing wide-field images, tracking the evolution of skin lesions, removing imaging artifacts, and enhancing the interpretability of DL models), or as benchmark for evaluating massive foundation models [27], [28]. We direct the interested reader to comprehensive surveys on automated skin image analysis in general [29] and DLbased skin lesion segmentation (SLS) in particular [30]. However, although crucial, SLS remains challenging task due 2025. This manuscript version is made available under the arXiv.org perpetual, non-exclusive license 1.0. to the presence of imaging artifacts (e.g., hair, gel bubbles, dark corners), lesion size and shape variability, varying skin tones, variable contrast and illumination, and ambiguous lesion boundaries [30], all of which affect the annotation of true ground truth segmentation. This variability in medical image segmentation, including in skin lesion images, is an active area of research, spanning several related yet distinct goals: studying variability in expert segmentations [31][35], aggregating multiple segmentations to model single gold standard segmentation [36][41], learning to model individual annotator-specific segmentation preferences [15], [42], [43], modeling the underlying distribution of segmentations [44] [47] and discovering the underlying segmentation styles [48], among others. Table lists all the publicly available SLS datasets, spanning the two popular skin imaging modalities: dermoscopic and clinical. Existing datasets sizes vary from little as 200 images (PH2 [9]) to just over 10,000 images (HAM100000 [14]), but despite the inherit ambiguity in segmentation, all these datasets contain only one segmentation mask per image. There is only one SLS dataset that contains multiple annotations per image: ISIC 2019-Seg [15], but with only 3 segmentations for each of its 100 images, it is quite small for effectively modeling annotator-specific tasks, especially compared to multiannotator datasets from other medical imaging modalities such as CT (e.g., LIDC-IDRI [49]) and fundus photography (e.g., RIGA [50]). The ISIC Archive, maintained by the International Skin Imaging Collaboration (ISIC), hosts the worlds largest collection of digital skin images. At the time of writing this article, the ISIC Archive contains over 1.2 million images, of which more than 120,000 are publicly available dermoscopic images. The Archive also contains subsets that were released as part of ISICs Skin Lesion Analysis Towards Melanoma Detection Segmentation Challenges over the years (20162018). In fact, recent survey by Mirikharaji et al. [30] found that of the 177 papers on SLS that they reviewed, 168 (95%) of the papers used at least one ISIC dataset, underlining the importance of the ISIC Archive to the skin image analysis community. Motivated by the lack of large multi-annotator SLS datasets and the popularity of ISIC Archive, we collect and publicly release ISIC MultiAnnot++ (IMA++ hereafter). With 14,967 images segmented by 16 annotators, the IMA++ contains total of 17,684 segmentations. Of these 14,967 images, 2,394 have at least 2 segmentations per image. To the best of our knowledge, IMA++ is the largest publicly available skin lesion segmentation dataset, multi-annotator or otherwise. Additionally, to establish segmentation consensus for images that have multiple segmentations, we also include segmentation masks using two consensus algorithms, increasing the number of segmentations to 22,472. The IMA++ dataset presents the following meritorious properties: Inter-annotator variability: IMA++ captures wide range of segmentation styles, reflecting differences in annotator preferences, tools used, and manual review process (varying skill levels). Realistic multi-annotator scenario: Most multiimage segmentation datasets [49], annotator medical 2 Fig. 1. breakdown of the IMA++ dataset: (a) distribution of number of segmentations per image and annotation factor-wise segmentation counts: (b) annotator, (c) tool, and (d) skill level. every image segmented by every [51][55] have thus forming complete bipartite graph annotator, between the set of images and the set of annotators. IMA++, on the other hand, real-world annotation scenarios where multiple annotators contribute images, and therefore features an to subset of incomplete bipartite graph. This means that every image in IMA++ is segmented by at least one annotator, but not all images are segmented by all annotators. simulates Tool-specific segmentation styles: Because of the availability of tool and skill level information for the segmentations, IMA++ allows for the exploration of how variations in these often-overlooked factors affect segmentation variability. These features make IMA++ an extremely valuable dataset for researchers working on variety of open problems including: (1) skin lesion image classification; (2) skin lesion segmentation, e.g., multi-annotator segmentation preference modeling, multi-expert segmentation consensus modeling, learning the distribution of segmentations and discovering the underlying segmentation styles from multi-annotator masks, and studying inter-annotator agreement among experts; and, (3) multi-modal (dermoscopic images and rich metadata) and multi-task (diagnosis, segmentation, IAA prediction) skin image analysis, and other tasks. Subsets of IMA++ have been used in two recent papers: (a) to evaluate the discovery of unique annotation segmentation styles in the absence of annotator-segmentation correspondence [48] and (b) to examine statistical association between the inter-annotator segmentation agreement levels (IAA hereafter) and the malignancy of skin lesions and to evaluate the feasibility of predicting IAA from skin lesion images directly without requiring any segmentations [56]. II. COLLECTION METHODS AND DESIGN A. Data Collection The ISIC Archive exposes public API for automated fetching of its contents. The images and raw segmentations in IMA++ are originally obtained from the ISIC Archive using the now-deprecated ISIC application programming interface (API) v1 [57]. The new ISIC API v2 [58] and its associated command line interface (CLI) [59], however, do not have endpoints for fetching segmentations. This leads to situation where although the images are available for download on the ISIC Archive, the corresponding segmentations or the segmentation-associated metadata are not currently available through the API, CLI, or web-interface. To address this gap, we gathered the complete list of segmentations and metadata by combining previously downloaded segmentations and by contacting sources to obtain the metadata for these masks, followed by anonymization and appropriate organization to build the IMA++ dataset. The scripts used for all the data collection, processing, and analysis are available on GitHub [60] at https://github.com/sfu-mial/IMAplusplus. B. Data Filtering As an initial quality control for the segmentation masks, we subject the masks to the following quality checks: empty masks (59), masks covering the entire image (3), and masks touching the image border (1,129). Of these checks, only empty masks i.e., masks that do not have any object (lesion) pixels, affect the utility, and therefore we remove these masks from our dataset. Similarly, we checked for images with missing metadata and found 59 images to not have any associated metadata. For IMA++ to be truly useful as multimodal dataset, all images contained therein should have metadata (Table II). Therefore, we remove these images, and finally, we are left with 14,967 images that have between 2 and 5 segmentations per image, resulting in 17,684 segmentation masks in total. Figure 1 shows the distribution of the number of segmentations per image in the IMA++ dataset. Of the 14,967 images, 12,573 have one segmentation mask per image, whereas the remaining 2,394 have multiple segmentations, leading to total of (12, 573 1) + (2130 2) + (209 3) + (51 4) + (4 5)=17,684 segmentations. C. Data Processing Next, we process the segmentation masks and their metadata (Table III). First, we assign unique identifiers to the three 3 annotation factors that determine the variability in the segmentation masks: annotator, tool, and skill level of the manual reviewer. Annotator mapping: We sort the annotators in decreasing order of the number of segmentation masks produced, and assign them unique identifiers accordingly: {A00, ..., A15}. Tool mapping: IMA++ contains three different tools that were used to draw the segmentation masks, and we assign them the following tool IDs: T1: manual polygon tracing by human expert, T2: semi-automated flood-fill with expert-defined parameters, and T3: fully-automated segmentation reviewed and accepted by human expert. Skill level mapping: Next, we have two skill levels of the manual reviewer: S1: expert and S2: novice. Finally, to avoid file corruption issues during data handling, we compute the MD5 hashes of the segmentation masks and add them to the metadata, so that users can verify data integrity. The counts of segmentations in IMA++ broken down by these three annotation factors is presented in Figure 1 (b, c, d). Similar to the number of segmentations per image, the annotator-segmentation counts also exhibit skewed distribution: six annotators (A00 A05) contribute approximately 78% of the segmentations (13,748 out of 17,684). Tools T1 and T3 account for 81% of the segmentations (14,247 out of 17,684), and 70% of the segmentations were manually reviewed by an S1 skill level reviewer (12,454 out of 17,684). D. Computing Segmentation Consensus Of the 14,967 images in the dataset, 2,394 images have multiple (2-5) annotations of the lesion segmentation per image. To compute consensus among these segmentations, as is standard for multi-annotator medical image segmentation challenges and datasets [54], [55], [61], [62], we employ two popular consensus algorithms: Simultaneous Truth and Performance Level Estimation (STAPLE) [36] and majority voting using SimpleITKs STAPLEImageFilter and LabelVotingImageFilter, respectively. This allows us to study the agreement between the original annotations and the consensus masks w.r.t. the annotation factors (discussed later) and also allows usage of IMA++ for the training and evaluation of SLS models without multi-annotator set-ups. Figure 2 shows few representative samples from the IMA++ dataset, with two rows each for images with (from top to bottom) five, four, three, and two segmentations per image, respectively. For each of these sets of multi-annotator masks, the two consensus algorithms outputs are also shown: majority voting (MV) and STAPLE (ST). E. Data Splits While the availability of consensus masks allows IMA++ to be used for lesion diagnosis, single-segmentation-per-image SLS, and multi-modal multi-task set ups, the data preparation for multi-annotator segmentations is nuanced and requires Fig. 2. consensus segmentation masks computed using majority voting (MV) and STAPLE (ST). Sample image-segmentation pairs from IMA++: 2 rows each for images with {5, 4, 3, 2} segmentations per image along with the corresponding careful consideration while splitting the data into training, validation, and testing partitions. Therefore, we split the 2,394 images that have two or more segmentation masks per image stratified by 2 criteria: segmentation count per image: The proportions of images with {2, 3, 4, 5} segmentations per image are similar across the three partitions, yielding (1493, 144, 37, 1) with (2, 3, 4, 5) annotations in the training partition, with corresponding numbers being (214, 19, 6, 1) and (423, 46, 8, 2) in validation and testing partitions, respectively. inter-annotator agreement: We quantify the interannotator agreement (IAA) for each image by calculating the per-image averaged pairwise Dice coefficient between segmentation masks, and then categorize the images as having low IAA (i.e., Dice [0, 0.5)), medium IAA (i.e., Dice [0.5, 0.8]), or high IAA (i.e., Dice (0.8, 1.0]). We then use these IAA levels to stratify the splits, ensure that the proportions of these levels are similar across partitions. yielding (165, 403, 1107) images with (low, medium, high) IAA in the training partition, with corresponding numbers being (24, 58, 158) and (47, 115, 317) in validation and testing partitions, respectively. Based on these two criteria, we split the 2,394 images 5 Fig. 3. UpSet plot showing the distribution of segmentations across the 16 annotators (A00 A15). The distribution is long-tailed, with the top 6 annotators (37% of the annotators) contributing 91% of the segmentations. Best viewed online. Fig. 4. Quantifying the inter-annotator agreement for IMA++ based on the three factors: (a) annotator, (b) tool, and (c) skill level. For each factor, we report the mean Dice coefficient (left) and 95th percentile of Hausdorff distance (right). Combinations that do not exist in the dataset are grayed out. Best viewed online. Fig. 5. UpSet plot comparing the proposed IMA++ with eight popular public skin lesion image (both dermoscopic and clinical) segmentation datasets and images shared amongst them. With 14,967, IMA++ is the largest dataset, and although it shares some images with other ISIC Challenge Datasets, over 74% of its images (11,081) are unique from past ISIC challenges. Please also see Table for more details about these datasets. Best viewed online. into (training, validation, testing) partitions in the ratio of 70:10:20, resulting in (1,675, 240, 479) images across the three partitions. These standardized data partitions for the multi-annotator segmentations of IMA++ allow researchers to systematically report and compare results across methods. TABLE II METADATA COLUMNS FOR THE IMAGES IN IMA++. Column Description isic_id copyright_license age_approx anatom_site_general benign_malignant concomitant_biopsy dermoscopic_type diagnosis_{1, 2, ..., 5} diagnosis_confirm_type lesion_id mel_class mel_thick_mm melanocytic nevus_type patient_id pixels_{x, y} sex The unique ID for the image on the ISIC Archive. Example: ISIC 0010183 License. Either CC-0 or CC-BY-NC. Approximate age of the patient. The general anatomical location of the skin lesion. The malignancy status of the skin lesion. Whether biopsy was taken at the same time as imaging. Type of dermoscopic imaging used. Hierarchical diagnosis labels, wherever applicable. How the diagnosis was confirmed. Example: single image expert consensensus, histopathology, etc. Unique ID of the skin lesion. Melanoma class, wherever applicable. Melanoma thickness in mm. Whether the lesion is melanocytic. Type of nevus, wherever applicable. Unique ID of the patient. Spatial dimensions of the image. Sex of the patient. TABLE III METADATA COLUMNS FOR THE SEGMENTATION MASKS IN IMA++."
        },
        {
            "title": "Description",
            "content": "ISIC_id img_filename seg_filename annotator tool skill_level mskObjectID mask_md5 The unique ID for the image on the ISIC Archive. Example: ISIC 0010183 The filename of the image. Example: ISIC 0010183.JPG. The filename of the segmentation mask. Example: ISIC 0010183 A04 T3 S2 55a9384a9fc3c156bd715c1b.png. The ID of the annotator. Example: A04. The ID of the tool used. Example: T3. The ID of the skill level. Example: S2. The objectID of the segmentation mask. Unique for each mask. Example: 55a9384a9fc3c156bd715c1b. The MD5 hash of the segmentation mask. Example: f6dae23fab650ba0aa441569a84a7624. 6 III. VALIDATION AND QUALITY A. Visualizing annotator overlap We visualize the distribution of the segmentations across the 16 annotators using an UpSet plot [63] in Figure 3. We choose an UpSet plot because relationships among our 16 sets (16 annotators) are too complex to represent with Venn diagrams, which do not scale well beyond 3 sets. Co-occurence matrices are also insufficient, since they only describe pairwise interactions and do not reveal higher-order interactions across the rows correspond to multiple sets. In this UpSet plot, the number of segmentations generated by each of the 16 annotators (A00 A15). For each row, the cells (denoted by dots) that are part of set are filled in, and their counts are denoted by the respective bars along the columns on the top. If column has multiple cells that are filled in, they are connected with line and the column count denotes the size of the intersection of the corresponding sets. For example, the bottom row corresponds to the annotator A00. Traversing the bottom row shows that there are nine unique sets that A00s segmentation masks appear in: {A00}: 2,319 masks (2nd vertical bar at the top), {A00 A03}: 726 masks (7th vertical bar), {A00 A03 A08}: 91 masks, {A00 A09}: 68 masks, {A00 A08}: 39 masks, {A00 A06}: 35 masks, {A00 A08 A09}: 5 masks, {A00 A03 A09}: 2 masks, and {A00 A03 A06}: 1 mask. Since these are the segmentations that A00 contributed, their total 2, 319 + 726 + 91 + 68 + 39 + 35 + 5 + 2 + 1 = 3, 286 is indicated in the horizontal bar corresponding to A00 (along the bottom row). With 16 annotators, 216 1 unique non-empty intersections of annotators are possible. On the other hand, if the images were annotated in complete bipartite manner, only 1 unique intersection would be possible, i.e., all images would be segmented by all the annotators. We observe that with an incomplete bipartite annotation set-up, IMA++ has 57 unique annotator intersections, yielding rich variety of multiannotator interactions. B. Analyzing interand intra-factor agreement in IMA++ Studying the extent of (dis)agreement between annotators is commonly explored area with multi-annotator set-ups. For instance-level labels, this inter-rater agreement (IAA) is often measured using widely used statistics such as Cohens kappa, Krippendorffs alpha, and Fleiss kappa. For segmentation masks, however, IAA is measured by computing the similarity between the masks, generally using overlap-based (e.g., Dice similarity coefficient, Jaccard index) or boundary-based (e.g., Hausdorff distance, boundary F1 score) measures. For our analysis of IMA++, we choose the most popular measure of each of these categories: Dice coefficient (DC) and 95th percentile of the Hausdorff distance (HD95). We extend our analysis beyond just annotators, and visualize these 7 Fig. 7. All images (n = 23) in IMA++ that have entirely non-overlapping segmentations (black and magenta contours) from multiple annotators. Best viewed online. similar pattern emerges when analyzing tools and skill levels. Notably, tools T1 and T3 show much higher agreement with each other than with themselves or T2, whereas T2 exhibits the opposite pattern: high intra-tool agreement but low inter-tool agreement. C. IMA++ versus other datasets Table shows that of these, ISIC 2019-Seg [15] is the only dataset with multiple annotations per image, however, its small size (100 images, 300 segmentations) makes it challenging to both conduct multi-annotator analysis as well as model the variability among the segmentations. On the other hand, while HAM10000s scale (10,015 images with single segmentation per image) is appealing, the lack of multi-expert labels limits its utility beyond traditional single segmentation modeling. Among all the single-annotator and multi-annotator publicly available SLS datasets covering both dermoscopic and clinical skin imaging modalities, IMA++ is the largest with 14,967 images and 17,684 total segmentations (Table I), allowing researchers to leverage the data for wide variety of tasks. Another desirable attribute of IMA++ is the number of new skin lesion images for which the segmentation masks are unique from past ISIC challenges. The official ISIC Segmentation Challenges dataset from 2016 through 2018 exhibit considerable overlap among each other, with 706 images being common across the three datasets [66]. To conduct thorough analysis of the overlap among all the public SLS datasets in Table I, we visualize their image identifiers in an UpSet plot 5. As expected, we find varying degrees of overlap between ISIC {2016, 2017, 2018, and 2019-Seg} datasets. Unsurprisingly, we also discover that small number of images in IMA++ also appear in the previously released ISIC datasets, albeit with Inter-annotator agreement distribution, as measured by Dice and Fig. 6. HD95, for all combinations of annotators that segmented at least 50 images. measures (DC and HD95) computed between pairs of segmentations in Figure 4 for all the factors: {annotator, tool, skill level}, and also include the consensus masks (MV and ST) in our analysis. Note that high DC and low HD95 correspond to higher levels of agreement. Since both DC and HD95 are symmetric, lower-triangular matrix-based visualization allows us to study both intra-factor (diagonal entries) and inter-factor (off-diagonal entries) agreements. Combinations of factors that are not present in the dataset have been grayed out. There are some key observations from the analysis of annotators: (i) some annotators (A01, A03) have lower levels of intra-annotator agreement, (ii) one annotator (A08) has generally low agreement with all the other annotators as well as the consensus masks, (iii) surprising pairs of low (e.g., {A00, A06}, {A03, A12}, {A04, A11}) and high (e.g., {A03, A05}, {A06, A07}) agreements emerge, which has also been observed in previous study [64], and (iv) even majority voting and STAPLE do not exhibit very high degree of agreement, emphasizing the value of multiple consensus algorithms. TABLE IV LIST OF ALL THE FILES IN THE ZENODO DATA REPOSITORY [65]. Filename Description segs.zip seg_metadata.csv img_metadata.csv seg_metadata_multi annotator_subset.csv iaa_metrics_pairwise.csv iaa_metrics_image.csv splits/{train,val, test}.csv ZIP archive of all the 22,472 segmentation masks in IMA++. CSV file containing the metadata for all the 22,472 segmentation masks. CSV file containing the metadata for all the 14,967 skin lesion images. CSV file containing the metadata for only the multi-annotator subset of IMA++ (i.e., 2,394 images with multiple segmentations per image). CSV file with the IAA metrics calculated for all mask pairs for all the images in the multi-annotator subset of IMA++. CSV file with the pairwise IAA metrics averaged per image. Standardized {training, validation, testing} partitions for the multiannotator subset of IMA++. only single segmentation masks per image for three of these four datasets. However, the majority of the images in IMA++ (11,081 images out of total 14,967; 74%) are new, and their segmentation masks, singleor multi-annotator, differ from all the previous ISIC challenge datasets. IV. RECORDS AND STORAGE The list of all files available in the Zenodo repository [65] is presented in Table IV. single ZIP file contains all the segmentation masks in flat directory structure, which includes all the 17,684 masks obtained from the 16 annotators and the consensus masks (majority voting and STAPLE) for all the images with multiple segmentations, leading to total of 22,472 segmentation masks. Since the corresponding skin lesion images are already publicly available and accessible on the ISIC Archive, they are not included in this repository, and can be downloaded using the ISIC API v2 [58], [67]. The IMA++ contains rich metadata for both the images and the segmentations, whose fields are listed in Tables II and III, respectively. The images metadata contains patient (e.g., age, gender, lesion location) and clinical (e.g., type of dermoscopy, how the diagnosis was confirmed, malignancy status, hierarchical diagnosis labels, melanoma thickness, whether concomitant biopsy was taken) information. The segmentations metadata, on the other hand, contains information previously discussed: unique identifiers about the annotator, tool used, and skill level of the manual reviewer, as well as object identifiers (mskObjectID) and the MD5 hash of the segmentation file. The segmentation mask files have been richly named to contain all this information, so that even in the absence of dedicated metadata file, all the necessary information can be fetched directly from the filename. Both these metadata CSV files share column used to store the unique ISIC identifier of the skin lesion images, and this column can be used as the primary key for merging the two metadata files. V. INSIGHTS AND NOTES A. Understanding Zero Overlap Scenarios notable insight from calculating the IAA metrics (DC and HD95) was the wide variability in agreement. Focusing on the multi-annotator intersections from Figure 3 with at least 50 images segmented by two or more annotators, Figure 6 shows the distribution of Dice and HD95 for these 8 annotator sets. We observe skewed distribution of agreement levels, with high density at high agreement values and long tail extending towards low agreements, particularly evident in DC values. This is observed throughout the entire dataset as well: although most images exhibited reasonably high degree of agreement (DC: 0.866 0.187; inter-quartile range of 0.161), substantial number of images showed poor agreement. Specifically, 236 images had mean DC below 0.5, 43 below 0.1, and 23 images had segmentations with zero overlap. These 23 images and their corresponding segmentations are shown in Figure 7. Understanding what leads annotators to completely disagree on their skin lesions localizations for these 23 images might be worthwhile direction to explore. B. Inter-annotator variability, malignancy, and ABCD Multiple studies have shown, for example, that lesion segmentation variability exists in the clinic, with expert dermatologists favoring tighter segmentations [32], [34]. Recently, Abhishek et al. [56] demonstrated statistical association between IAA, measured by Dice, and the malignancy of the underlying skin lesion, showing that lower IAA values are linked to malignant lesions. This raises an important question: if clinical prediction criteria such as the ABCD rule rely on an accurate segmentation to compute the features, how does variability in segmentation impact ABCD calculations and, consequently, the diagnosis derived from these rules? Investigating the effect of segmentation variability on clinical decision-making could therefore be an interesting and important direction for future research. C. Beyond Pairwise Multi-Annotator IAA limitation of all existing measures for quantifying interannotator agreement in medical image segmentation is that they are restricted to pairwise (dis)agreement. Consequently, when reporting image-level agreement values, the (cid:0)N (cid:1) pairwise agreements computed from segmentations must be aggregated (e.g., using mean or median), which inevitably loses information about the full distribution of agreements across the segmentations. truly groupwise IAA measure is its development currently missing from the literature, yet could be highly valuable. The incomplete bipartite graph of annotators in IMA++ provides an ideal testbed for evaluating the robustness of such measure. 2 VI. SOURCE CODE AND SCRIPTS All data collection, processing, validation, and analconducted on an Ubuntu 22.04 workstaysis were i9-14900K, 64 GB RAM, NVIDIA tion with Intel In alphabetical orRTX 4090, with Python 3.10.19. der, the following Python packages with (version numbers) were used: isic-cli (12.4.0), matplotlib (3.10.7), medpy (0.5.2), numpy (2.2.6), opencv-python (4.12.0), pandas (2.3.3), pillow (12.0.0), requests (2.32.5), scikit-image (0.25.2), scikit-learn (1.7.2), scipy (1.15.3), seaborn (0.13.2), simpleitk (2.5.2), torchvision (0.24.0), torch (2.9.0), and upsetplot (0.9.0). All scripts used in this work for processing, validation, and analysis are publicly available on GitHub [60]."
        },
        {
            "title": "Special",
            "content": "thanks to Jochen Weber from Memorial Sloan Kettering Cancer Center for sharing the segmentations and the corresponding metadata. CONTRIBUTIONS K.A. collected, processed, and analyzed the data with inputs and feedback from J.K. and G.H, and wrote the first draft of the manuscript. J.K. and G.H. provided input on the visualization and data presentation, and edited the manuscript. All authors reviewed the manuscript."
        },
        {
            "title": "CONFLICTS OF INTEREST",
            "content": "The authors have no competing interests to declare."
        },
        {
            "title": "REFERENCES",
            "content": "[1] Skin cancer IARC, [Accessed November 14, 2025]. [Online]. Available: https://www.iarc.who.int/cancer-type/skin-cancer/ [2] R. Wang, Y. Chen, X. Shao, T. Chen, J. Zhong, Y. Ou, and J. Chen, Burden of from 1990 to 2021 and modelled projection to 2050, JAMA Dermatology, [Online]. Available: https: Jul. 2025. vol. 161, no. 7, p. 715, //dx.doi.org/10.1001/jamadermatol.2025.1276 in older adults skin cancer [3] L. Zhou, Y. Zhong, L. Han, Y. Xie, and M. Wan, Global, regional, and national trends in the burden of melanoma and non-melanoma skin cancer: Insights from the global burden of disease study 19902021, Scientific Reports, vol. 15, no. 1, Feb. 2025. [Online]. Available: https://dx.doi.org/10.1038/s41598-025-90485- [4] U. Leiter, T. Eigentler, and C. Garbe, Epidemiology of Skin Cancer. Springer New York, 2014, p. 120140. [Online]. Available: https://dx.doi.org/10.1007/978-1-4939-0437-2 7 [5] A. Yakupu, R. Aimaier, B. Yuan, B. Chen, J. Cheng, Y. Zhao, Y. Peng, J. Dong, and S. Lu, The burden of skin and subcutaneous diseases: findings from the global burden of disease study 2019, Frontiers in Public Health, vol. 11, Apr. 2023. [Online]. Available: https://dx.doi.org/10.3389/fpubh.2023.1145513 [6] H. W. Lim, S. A. Collins, J. S. Resneck, J. L. Bolognia, J. A. Hodge, T. A. Rohrer, M. J. Van Beek, D. J. Margolis, A. J. Sober, M. A. Weinstock, D. R. Nerenz, W. Smith Begolka, and J. V. Moyano, The burden of skin disease in the united states, Journal of the American Academy of Dermatology, vol. 76, no. 5, pp. 958972.e2, May 2017. [Online]. Available: https://dx.doi.org/10.1016/j.jaad.2016.12.043 [7] J. L. Glaister, Automatic segmentation of skin lesions from dermatological photographs, https://uwaterloo.ca/vision-image-processing-lab/ research-demos/skin-cancer-detection, 2013, cited: 2022-1-31. [8] L. Ballerini, R. B. Fisher, B. Aldridge, and J. Rees, color and texture based hierarchical K-NN approach to the classification of nonmelanoma skin lesions, in Color Medical Image Analysis, M. E. Celebi and G. Schaefer, Eds., vol. 6. Springer Netherlands, 2013, pp. 6386. [Online]. Available: https://dx.doi.org/10.1007/978-94-007-5389-1 4 9 [9] T. Mendonca, P. M. Ferreira, J. S. Marques, A. R. S. Marcal, and J. Rozeira, PH2 - dermoscopic image database for research and benchmarking, in IEEE Engineering in Medicine and Biology Society, [Online]. Available: https: Jul. 2013, pp. 54375440. //dx.doi.org/10.1109/embc.2013. [10] D. Gutman, N. C. F. Codella, E. Celebi, B. Helba, M. Marchetti, N. Mishra, and A. Halpern, Skin Lesion Analysis toward Melanoma Detection: Challenge at the International Symposium on Biomedical Imaging (ISBI) 2016, hosted by the International Skin Imaging Collaboration (ISIC), May 2016. [Online]. Available: https://arxiv.org/ abs/1605.01397 [11] N. C. F. Codella, D. Gutman, M. E. Celebi, B. Helba, M. A. Marchetti, S. W. Dusza, A. Kalloo, K. Liopyris, N. Mishra, H. Kittler, and A. Halpern, Skin lesion analysis toward melanoma detection: the 2017 International symposium on biomedical challenge at imaging (ISBI), hosted by the international skin imaging collaboration (ISIC), in 2018 IEEE 15th International Symposium on Biomedical Imaging (ISBI 2018). Washington, DC: IEEE, Apr. 2018, pp. 168 172, https://ieeexplore.ieee.org/document/8363547/. [Online]. Available: https://dx.doi.org/10.1109/ISBI.2018.8363547 [12] N. Codella, V. Rotemberg, P. Tschandl, M. E. Celebi, S. Dusza, D. Gutman, B. Helba, A. Kalloo, K. Liopyris, M. Marchetti, H. Kittler, and A. Halpern, Skin Lesion Analysis Toward Melanoma Detection 2018: Challenge Hosted by the International Skin Imaging Collaboration (ISIC), Mar. 2019. [Online]. Available: https://arxiv.org/abs/1902.03368 [13] P. Tschandl, C. Rinner, Z. Apalla, G. Argenziano, N. Codella, A. Halpern, M. Janda, A. Lallas, C. Longo, J. Malvehy, J. Paoli, S. Puig, C. Rosendahl, H. P. Soyer, I. Zalaudek, and H. Kittler, Humancomputer collaboration for skin cancer recognition, Nature Medicine, vol. 26, no. 8, pp. 12291234, Jun. 2020. [Online]. Available: https://doi.org/10.1038/s41591-020-0942-0 [14] ViDIR Dataverse, HAM10000 Binary Lesion Segmentations, https: //doi.org/10.7910/DVN/DBW86T, 2020, [Online. Accessed January 9, 2023]. [15] K. Zepf, E. Petersen, J. Frellsen, and A. Feragen, That labels got style: Handling label style bias for uncertain image segmentation, in The Eleventh International Conference on Learning Representations, 2023. [Online]. Available: https://openreview.net/forum?id=wZ2SVhOTzBX [16] M. Binder, Epiluminescence microscopy: useful the diagnosis of pigmented skin lesions for formally trained dermatologists, Archives of Dermatology, vol. 131, no. 3, p. 286, Mar. 1995. [Online]. Available: https://dx.doi.org/10.1001/archderm.1995.01690150050011 tool for [17] Z. B. Argenyi, Dermoscopy (epiluminescence microscopy) of pigmented skin lesions, Dermatologic Clinics, vol. 15, no. 1, p. 7995, Jan. 1997. [Online]. Available: https://dx.doi.org/10.1016/ S0733-8635(05)70417- [18] H. Kittler, H. Pehamberger, K. Wolff, and M. Binder, Diagnostic accuracy of dermoscopy, The Lancet Oncology, vol. 3, no. 3, p. 159165, Mar. 2002. [Online]. Available: https://dx.doi.org/10.1016/ s1470-2045(02)00679-4 [19] M. Vestergaard, P. Macaskill, P. Holt, and S. Menzies, Dermoscopy compared with naked eye examination for the diagnosis of primary melanoma: meta-analysis of studies performed in clinical setting, British Journal of Dermatology, pp. 669676, Jun. 2008. [Online]. Available: https://dx.doi.org/10.1111/j.1365-2133.2008.08713.x [20] C. Rosendahl, G. Williams, D. Eley, T. Wilson, G. Canning, J. Keir, I. McColl, and D. Wilkinson, The impact of subspecialization and dermatoscopy use on accuracy of melanoma diagnosis among primary care doctors in australia, Journal of the American Academy of Dermatology, vol. 67, no. 5, p. 846852, Nov. 2012. [Online]. Available: https://dx.doi.org/10.1016/j.jaad.2011.12.030 [21] P. Carli, V. De Giorgi, E. Crocetti, F. Mannone, D. Massi, A. Chiarugi, and B. Giannotti, Improvement of malignant/benign ratio in excised melanocytic lesions in the dermoscopy era: retrospective study 1997-2001, British Journal of Dermatology, vol. 150, no. 4, p. 687692, Apr. 2004. [Online]. Available: https://dx.doi.org/10.1111/j.0007-0963.2004.05860.x [22] J. van der Rhee, W. Bergman, and N. Kukutsch, Impact of dermoscopy on the management of high-risk patients from melanoma families: prospective study, Acta Dermato Venereologica, vol. 91, no. 4, p. 428431, 2011. [Online]. Available: https://dx.doi.org/10.2340/ 00015555-1100 [23] M. Binder, A. Steiner, M. Schwarz, S. Knollmayer, K. Wolff, and H. Pehamberger, Application of an artificial neural network in epiluminescence microscopy pattern analysis of pigmented skin study, British Journal of Dermatology, vol. lesions: 130, no. 4, p. 460465, Apr. 1994. [Online]. Available: https: //dx.doi.org/10.1111/j.1365-2133.1994.tb03378.x pilot [24] A. Forsea, P. Tschandl, I. Zalaudek, V. del Marmol, H. Soyer, Eurodermoscopy Working Group, G. Argenziano, and A. Geller, impact of dermoscopy on melanoma detection in the The practice of dermatologists in Europe: Results of pan-European of Dermatology survey, and Jul. 11481156, 31, 2017, https://onlinelibrary.wiley.com/doi/10.1111/jdv.14129. [Online]. Available: https://dx.doi.org/10.1111/jdv.14129 the European Academy no. Journal Venereology, vol. pp. of 7, ambulatory [25] E. Harrington, B. Clyne, N. Wesseling, H. Sandhu, L. Armstrong, Diagnosing malignant melanoma clinical systematic e014096, 7, vol. https://bmjopen.bmj.com/lookup/doi/10.1136/bmjopenhttps://dx.doi.org/10.1136/ [Online]. and T. Fahey, care: rules, BMJ Open, H. Bennett, in prediction Mar. 2017, 2016-014096. bmjopen-2016-014096 review 3, no. Available: of p. [26] F. Nachbar, W. Stolz, T. Merkle, A. B. Cognetta, T. Vogt, M. Landthaler, P. Bilek, O. Braun-Falco, and G. Plewig, The ABCD rule of dermatoscopy, Journal of the American Academy of Dermatology, vol. 30, no. 4, p. 551559, Apr. 1994. [Online]. Available: https://dx.doi.org/10.1016/s0190-9622(94)70061- [27] S. Yan, Z. Yu, C. Primiero, C. Vico-Alonso, Z. Wang, L. Yang, P. Tschandl, M. Hu, L. Ju, G. Tan, V. Tang, A. B. Ng, D. Powell, P. Bonnington, S. See, E. Magnaterra, P. Ferguson, J. Nguyen, P. Guitera, J. Banuls, M. Janda, V. Mar, H. Kittler, H. P. Soyer, and Z. Ge, multimodal vision foundation model for clinical dermatology, Nature Medicine, vol. 31, no. 8, p. 26912702, Jun. 2025. [Online]. Available: https://dx.doi.org/10.1038/s41591-025-03747-y [28] J. Xu, D. Cheng, X. Zhao, J. Yang, Z. Wang, X. Jiang, X. Luo, L. Chen, X. Ning, C. Li et al., DermINO: Hybrid pretraining for versatile dermatology foundation model, arXiv preprint arXiv:2508.12190, 2025. [Online]. Available: https://arxiv.org/abs/2508.12190 [29] K. Korotkov and R. Garcia, Computerized analysis of pigmented skin lesions: review, Artificial Intelligence in Medicine, vol. 56, no. 2, p. 6990, Oct. 2012. [Online]. Available: https://dx.doi.org/10.1016/j. artmed.2012.08.002 [30] Z. Mirikharaji, K. Abhishek, A. Bissoto, C. Barata, S. Avila, E. Valle, M. E. Celebi, and G. Hamarneh, survey on deep learning for skin lesion segmentation, Medical Image Analysis, vol. 88, p. 102863, Aug. 2023. [Online]. Available: https://dx.doi.org/10.1016/j.media.2023. 102863 [31] M. P. Sampat, Z. Wang, M. K. Markey, G. J. Whitman, T. W. Stephens, and A. C. Bovik, Measuring intra-and inter-observer agreement in images, in 2006 identifying and localizing structures in medical International Conference on Image Processing. IEEE, Oct. 2006, pp. 8184. [Online]. Available: https://dx.doi.org/10.1109/ICIP.2006.312367 [32] A. Silletti, E. Peserico, A. Mantovan, E. Zattra, A. Peserico, and automatic segmentation of melanocytic lesions, in 2009 Annual International the IEEE Engineering in Medicine and Biology Conference of Society. Minneapolis, MN: IEEE, Sep. 2009, pp. 57895792, https://ieeexplore.ieee.org/document/5332543/. [Online]. Available: https://dx.doi.org/10.1109/IEMBS.2009.5332543 Variability Fortina, human and A. in [33] X. Li, B. Aldridge, J. Rees, and R. Fisher, Estimating the ground truth from multiple individual segmentations with application to skin lesion segmentation, in Proc. Medical Image Understanding and Analysis Conference, UK, vol. 1, 2010, pp. 101106. [34] A. B. Fortina, E. Peserico, A. Silletti, and E. Zattra, Wheres the naevus? inter-operator variability in the localization of melanocytic lesion border, Skin Research and Technology, vol. 18, no. 3, pp. 311315, Oct. 2012. [Online]. Available: https://dx.doi.org/10.1111/j. 1600-0846.2011.00572.x [35] V. Ribeiro, S. Avila, Handling inter-annotator and E. Valle, agreement for automated skin lesion segmentation, arXiv preprint arXiv:1906.02415, 2019. [Online]. Available: https://arxiv.org/abs/1906. 02415 [36] S. K. Warfield, K. H. Zou, and W. M. Wells, Simultaneous truth and performance level estimation (STAPLE): An algorithm for the validation of image segmentation, IEEE Transactions on Medical 10 Imaging, vol. 23, no. 7, pp. 903921, Jul. 2004. [Online]. Available: https://dx.doi.org/10.1109/TMI.2004.828354 III 22. [37] E. Kats, J. Goldberger, and H. Greenspan, soft STAPLE algorithm combined with anatomical knowledge, in Medical Image Computing and Computer Assisted InterventionMICCAI 2019: 22nd International Conference, Shenzhen, China, October 1317, 2019, Proceedings, Part [Online]. Available: Springer, 2019, pp. 510517. https://dx.doi.org/10.1007/978-3-030-32248-9 57 Less is more: Sample and E. Valle, selection and label conditioning improve skin lesion segmentation, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) ISIC Skin Image Analysis Workshop (ISIC). [Online]. Available: https://dx.doi.org/10.1109/CVPRW50498.2020.00377 [38] V. Ribeiro, S. Avila, Jun. 2020, p. 31823191. IEEE, [39] Z. Mirikharaji, K. Abhishek, S. Izadi, and G. Hamarneh, D-LEMA: Deep learning ensembles from multiple annotations-application to skin lesion segmentation, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) ISIC Skin Image Analysis Workshop (ISIC). IEEE, Jun. 2021, pp. 18371846. [Online]. Available: https://dx.doi.org/10.1109/CVPRW53098.2021.00203 [40] T. Amit, S. Shichrur, T. Shaharabany, and L. Wolf, Annotator consensus prediction for medical image segmentation with diffusion models, in International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer, 2023, pp. 544554. [Online]. Available: https://dx.doi.org/10.1007/978-3-031-43901-8 [41] D. Hamzaoui, S. Montagne, R. Renard-Penna, N. Ayache, computa- (MACCHIatO), no. 2, [Online]. Available: and H. Delingette, tion via Machine UNSURE2022, https://dx.doi.org/10.59275/j.melba.2023-219c heuristics-based Learning for p. iterative Biomedical Sep. optimization Imaging, Morphologically-aware consensus 361389, 2023. vol. [42] W. Ji, S. Yu, J. Wu, K. Ma, C. Bian, Q. Bi, J. Li, H. Liu, L. Cheng, and Y. Zheng, Learning calibrated medical image segmentation via multi-rater agreement modeling, in Proceedings the IEEE/CVF Conference on Computer Vision and Pattern of Recognition, [Online]. Available: https://dx.doi.org/10.1109/CVPR46437.2021.01216 Jun. 2021, pp. 12 34112 351. J. De Fauw, [43] Z. Liao, S. Hu, Y. Xie, and Y. Xia, Modeling annotator preference and stochastic annotation error for medical image segmentation, Medical Image Analysis, vol. 92, p. 103028, Feb. 2024. [Online]. Available: https://dx.doi.org/10.1016/j.media.2023.103028 [44] S. Kohl, B. Romera-Paredes, C. Meyer, J. R. Jimenez Rezende, and Ledsam, K. Maier-Hein, S. Eslami, D. O. Ronneberger, probabilistic U-Net for segmentation of ambiguous images, Advances in Neural Information Processing Systems, vol. 31, 2018. [Online]. Available: https://proceedings.neurips.cc/paper files/ paper/2018/file/473447ac58e1cd7e96172575f48dca3b-Paper.pdf [45] C. F. Baumgartner, K. C. Tezcan, K. Chaitanya, A. M. Hotker, U. J. Muehlematter, K. Schawkat, A. S. Becker, O. Donati, and E. Konukoglu, PHiSeg: Capturing uncertainty in medical image segmentation, in International Conference on Medical Image Computing and ComputerAssisted Intervention. Springer, 2019, pp. 119127. [Online]. Available: https://dx.doi.org/10.1007/978-3-030-32245-8 14 [46] A. Rahman, J. M. J. Valanarasu, I. Hacihaliloglu, and V. M. Patel, Ambiguous medical image segmentation using diffusion models, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, Jun. 2023, pp. 11 53611 546. [Online]. Available: https://dx.doi.org/10.1109/CVPR52729.2023.01110 [47] A. Schmidt, P. Morales-Alvarez, and R. Molina, Probabilistic modeling of inter-and intra-observer variability in medical image segmentation, in Proceedings of IEEE/CVF International Conference on Computer Vision, Oct. 2023, pp. 21 04021 049. [Online]. Available: https://dx.doi.org/10.1109/ICCV51070.2023.01929 the [48] K. Abhishek, J. Kawahara, and G. Hamarneh, Segmentation style discovery: Application to skin lesion images, in International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI) ISIC Skin Image Analysis Workshop (ISIC). Springer Nature Switzerland, 2025, pp. 2434. [Online]. Available: https://dx.doi.org/10.1007/978-3-031-77610-6 3 [49] S. G. Armato III, G. McLennan, L. Bidaut, M. F. McNitt-Gray, C. R. Meyer, A. P. Reeves, B. Zhao, D. R. Aberle, C. I. Henschke, E. A. Hoffman et al., The lung image database consortium (LIDC) and image database resource initiative (IDRI): completed reference database of lung nodules on CT scans, Medical Physics, vol. 38, no. 2, pp. 915931, Jan. 2011. [Online]. Available: https://dx.doi.org/10.1118/1.3528204 [50] A. Almazroa, S. Alodhayb, E. Osman, E. Ramadan, M. Hummadi, M. Dlaim, M. Alkatee, K. Raahemifar, and V. Lakshminarayanan, in marking the optic disc Agreement among ophthalmologists images, International Ophthalmology, and optic cup in fundus vol. 37, no. 3, pp. 701717, Aug. 2017. [Online]. Available: https://dx.doi.org/10.1007/s10792-016-0329-x [51] M. M. Fraz, P. Remagnino, A. Hoppe, B. Uyyanonvara, A. R. Rudnicka, C. G. Owen, and S. A. Barman, An ensemble classificationbased approach applied to retinal blood vessel segmentation, IEEE Transactions on Biomedical Engineering, vol. 59, no. 9, p. 25382548, Sep. 2012. [Online]. Available: https://dx.doi.org/10.1109/TBME.2012. 2205687 [52] A. Carass, S. Roy, A. Jog, J. L. Cuzzocreo, E. Magrath, A. Gherman, J. Button, J. Nguyen, F. Prados, C. H. Sudre, M. Jorge Cardoso, N. Cawley, O. Ciccarelli, C. A. Wheeler-Kingshott, S. Ourselin, L. Catanese, H. Deshpande, P. Maurel, O. Commowick, C. Barillot, X. Tomas-Fernandez, S. K. Warfield, S. Vaidya, A. Chunduru, R. Muthuganapathy, G. Krishnamurthi, A. Jesson, T. Arbel, O. Maier, H. Handels, L. O. Jain, D. M. Sima, Iheme, D. Unay, S. D. Smeets, M. Ghafoorian, B. Platel, A. Birenbaum, H. Greenspan, P.-L. Bazin, P. A. Calabresi, C. M. Crainiceanu, L. M. Ellingsen, D. S. Reich, Longitudinal and D. L. Pham, multiple sclerosis lesion segmentation: Resource and challenge, NeuroImage, vol. 148, p. 77102, Mar. 2017. [Online]. Available: https://dx.doi.org/10.1016/j.neuroimage.2016.12.064 J. L. Prince, [53] O. Commowick, A. Istace, M. Kain, B. Laurent, F. Leray, M. Simon, S. C. Pop, P. Girard, R. Ameli, J.-C. Ferre, A. Kerbrat, T. Tourdias, F. Cervenansky, T. Glatard, J. Beaumont, S. Doyle, F. Forbes, J. Knight, A. Khademi, A. Mahbod, C. Wang, R. McKinley, F. Wagner, J. Muschelli, E. Sweeney, E. Roura, X. Llado, M. M. Santos, W. P. Santos, A. G. Silva-Filho, X. Tomas-Fernandez, H. Urien, I. Bloch, S. Valverde, M. Cabezas, F. J. Vera-Olmos, N. Malpica, C. Guttmann, S. Vukusic, G. Edan, M. Dojat, M. Styner, S. K. Warfield, F. Cotton, and C. Barillot, Objective evaluation of multiple sclerosis lesion segmentation using data management and processing infrastructure, Scientific Reports, vol. 8, no. 1, Sep. 2018. [Online]. Available: https://dx.doi.org/10.1038/s41598-018-31911-7 [54] H. B. Li, F. Navarro, I. Ezhov, A. Bayat, D. Das, F. Kofler, S. Shit, D. Waldmannstetter, J. C. Paetzold, X. Hu, B. Wiestler, L. Zimmer, T. Amiranashvili, C. Prabhakar, C. Berger, J. Weidner, M. Alonso-Basant, A. Rashid, U. Baid, W. Adel, D. Ali, B. Baheti, Y. Bai, I. Bhatt, S. C. Cetindag, W. Chen, L. Cheng, P. Dutand, L. Dular, M. A. Elattar, M. Feng, S. Gao, H. Huisman, W. Hu, S. Innani, W. Jiat, D. Karimi, H. J. Kuijf, J. T. Kwak, H. L. Le, X. Lia, H. Lin, T. Liu, J. Ma, K. Ma, T. Ma, I. Oksuz, R. Holland, A. L. Oliveira, J. B. Pal, X. Pei, M. Qiao, A. Saha, R. Selvan, L. Shen, J. L. Silva, Z. Spiclin, S. Talbar, D. Wang, W. Wang, X. Wang, Y. Wang, R. Xia, K. Xu, Y. Yan, M. Yergin, S. Yu, L. Zeng, Y. Zhang, J. Zhao, Y. Zheng, M. Zukovec, R. Do, A. Becker, A. Simpson, E. Konukoglu, A. Jakab, S. Bakas, L. Joskowicz, and B. Menze, QUBIQ: Uncertainty quantification for biomedical image segmentation challenge, arXiv preprint arXiv:2405.18435, pp. 113, 2024. [Online]. Available: https://arxiv.org/abs/2405.18435 [55] D. S. Carmo, A. A. Pezzulo, R. A. Villacreses, M. L. Eisenbeisz, R. L. Anderson, S. E. V. Dorin, L. Rittner, R. A. Lotufo, S. E. Gerard, J. M. 11 Reinhardt, and A. P. Comellas, Manual segmentation of opacities and consolidations on CT of long COVID patients from multiple annotators, Scientific Data, vol. 12, no. 1, Mar. 2025. [Online]. Available: https://dx.doi.org/10.1038/s41597-025-04709- [56] K. Abhishek, J. Kawahara, and G. Hamarneh, What can we learn from inter-annotator variability in skin lesion segmentation? in International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI) ISIC Skin Image Analysis Workshop (ISIC). Springer Nature Switzerland, Sep. 2025, pp. 2333. [Online]. Available: https://dx.doi.org/10.1007/978-3-032-05825-6 3 [57] ISIC, ISIC Archive REST API Documentation, https: //web.archive.org/web/20220802194512/https://isic-archive.com/api/v1, the deprecated. Original URL: is https://isic-archive.com/api/v1 (no longer accessible). Archived on August 2, 2022. ISIC REST API v1 [58] , ISIC Archive v2 OAS 3.1, https://api.isic-archive.com/api/docs/ swagger/, [Online. Accessed November 01, 2025]. [59] , isic-cli GitHub, https://github.com/ImageMarkup/isic-cli/, [Online. Accessed November 01, 2025]. [60] SFU MIAL, IMA++ GitHub, https://github.com/sfu-mial/ IMAplusplus/, [Online. Accessed November 21, 2025]. [61] K. A. Wahid, D. Lin, O. Sahin, M. Cislo, B. E. Nelms, R. He, M. A. Naser, S. Duke, M. V. Sherer, J. P. Christodouleas, A. S. R. Mohamed, J. D. Murphy, C. D. Fuller, and E. F. Gillespie, Large scale crowdsourced radiotherapy segmentations across variety of cancer anatomic sites, Scientific Data, vol. 10, no. 1, Mar. 2023. [Online]. Available: https://dx.doi.org/10.1038/s41597-023-02062-w [62] K. A. Wahid, C. Dede, D. M. El-Habashy, S. Kamel, M. K. Rooney, Y. Khamis, M. R. A. Abdelaal, S. Ahmed, K. L. Corrigan, E. Chang, S. O. Dudzinski, T. C. Salzillo, B. A. McDonald, S. L. Mulder, L. McCullum, Q. Alakayleh, C. Sjogreen, R. He, A. S. R. Mohamed, S. Y. Lai, J. P. Christodouleas, A. J. Schaefer, M. A. Naser, and C. D. Fuller, Overview of the Head and Neck Tumor Segmentation for Magnetic Resonance Guided Applications (HNTS-MRG) 2024 Challenge. Springer Nature Switzerland, 2025, p. 135. [Online]. Available: https://dx.doi.org/10.1007/978-3-031-83274-1 [63] A. Lex, N. Gehlenborg, H. Strobelt, R. Vuillemot, and H. Pfister, UpSet: Visualization of intersecting sets, IEEE Transactions on Visualization and Computer Graphics, vol. 20, no. 12, p. 19831992, Dec. 2014. [Online]. Available: https://dx.doi.org/10.1109/TVCG.2014.2346248 [64] M. Zhao, J. Kawahara, K. Abhishek, S. Shamanian, and G. Hamarneh, Skin3D: Detection and longitudinal tracking of pigmented skin lesions in 3D total-body textured meshes, Medical Image Analysis, vol. 77, p. 102329, Apr. 2022. [Online]. Available: https://dx.doi.org/10.1016/j. media.2021.102329 [65] K. Abhishek, J. Kawahara, and G. Hamarneh, IMA++: ISIC archive multi-annotator dermoscopic skin lesion segmentation dataset, 2024. [Online]. Available: https://zenodo.org/doi/10.5281/zenodo.14201692 [66] K. Abhishek, Input space augmentation for skin lesion segmentation in dermoscopic images, Masters thesis, Applied Sciences: School of Computing Science, Simon Fraser University, 2020, https://summit.sfu. ca/item/20247. [67] , ISIC API Image Downloader GitHub, https://github.com/ [Online. Accessed kakumarabhishek/ISIC-API-Image-Downloader/, November 21, 2025]."
        }
    ],
    "affiliations": [
        "AIP Labs, Hungary",
        "Medical Image Analysis Lab, School of Computing Science, Simon Fraser University, Canada"
    ]
}