{
    "paper_title": "Open Materials 2024 (OMat24) Inorganic Materials Dataset and Models",
    "authors": [
        "Luis Barroso-Luque",
        "Muhammed Shuaibi",
        "Xiang Fu",
        "Brandon M. Wood",
        "Misko Dzamba",
        "Meng Gao",
        "Ammar Rizvi",
        "C. Lawrence Zitnick",
        "Zachary W. Ulissi"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The ability to discover new materials with desirable properties is critical for numerous applications from helping mitigate climate change to advances in next generation computing hardware. AI has the potential to accelerate materials discovery and design by more effectively exploring the chemical space compared to other computational methods or by trial-and-error. While substantial progress has been made on AI for materials data, benchmarks, and models, a barrier that has emerged is the lack of publicly available training data and open pre-trained models. To address this, we present a Meta FAIR release of the Open Materials 2024 (OMat24) large-scale open dataset and an accompanying set of pre-trained models. OMat24 contains over 110 million density functional theory (DFT) calculations focused on structural and compositional diversity. Our EquiformerV2 models achieve state-of-the-art performance on the Matbench Discovery leaderboard and are capable of predicting ground-state stability and formation energies to an F1 score above 0.9 and an accuracy of 20 meV/atom, respectively. We explore the impact of model size, auxiliary denoising objectives, and fine-tuning on performance across a range of datasets including OMat24, MPtraj, and Alexandria. The open release of the OMat24 dataset and models enables the research community to build upon our efforts and drive further advancements in AI-assisted materials science."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 6 1 ] - m . - c [ 1 1 7 7 2 1 . 0 1 4 2 : r Open Materials 2024 (OMat24) Inorganic Materials Dataset and Models Luis Barroso-Luque, Muhammed Shuaibi, Xiang Fu, Brandon M. Wood, Misko Dzamba, Meng Gao, Ammar Rizvi, C. Lawrence Zitnick, Zachary W. Ulissi Fundamental AI Research (FAIR) at Meta The ability to discover new materials with desirable properties is critical for numerous applications from helping mitigate climate change to advances in next generation computing hardware. AI has the potential to accelerate materials discovery and design by more effectively exploring the chemical space compared to other computational methods or by trial-and-error. While substantial progress has been made on AI for materials data, benchmarks, and models, barrier that has emerged is the lack of publicly available training data and open pre-trained models. To address this, we present Meta FAIR release of the Open Materials 2024 (OMat24) large-scale open dataset and an accompanying set of pre-trained models. OMat24 contains over 110 million density functional theory (DFT) calculations focused on structural and compositional diversity. Our EquiformerV2 models achieve state-of-the-art performance on the Matbench Discovery leaderboard and are capable of predicting ground-state stability and formation energies to an F1 score above 0.9 and an accuracy of 20 meV/atom, respectively. We explore the impact of model size, auxiliary denoising objectives, and fine-tuning on performance across range of datasets including OMat24, MPtraj, and Alexandria. The open release of the OMat24 dataset and models enables the research community to build upon our efforts and drive further advancements in AI-assisted materials science. Date: October 18, 2024 Correspondence: L. Barroso-Luque (lbluque@meta.com), C.L. Zitnick (zitnick@meta.com), Z. Ulissi (zulissi@meta.com) Code: https://github.com/FAIR-Chem/fairchem, Permissive open source license Dataset: https://huggingface.co/datasets/fairchem/OMAT24, Creative Commons 4.0 License Checkpoints: https://huggingface.co/fairchem/OMAT24, Permissive open source license Blogpost: https://ai.meta.com/blog/fair-news-segment-anything-2-1-meta-spirit-lm-layer-skip-salsa-sona"
        },
        {
            "title": "1 Introduction",
            "content": "The discovery of new materials lies at the foundation of many pressing global problems. This includes finding new catalysts materials for renewable energy storage or for producing carbon neutral fuels 13 and the design of direct air capture sorbents 4, among many others 511. The search space of possible materials is enormous, and remains significant challenge for both computational and experimental approaches to material science. Identifying promising candidates through computational screening with machine learning models offers the potential to dramatically increase the search space and the rate of experimental discovery. To aid in the discovery of new materials, computational approaches are typically used as filters for identifying promising materials for synthesis in the lab. This is done by computing the formation energy of candidate material and any materials in its local neighborhood of composition space. An indication that candidate material may be stable experimentally is whether its formation energy is on or below the convex hull of the energies of its neighbors. The challenge computationally is that the formation energy calculations are typically performed using Density Functional Theory (DFT), which is computationally very expensive and limits its utility in exploring the combinatorial search space of new materials. Recently, significant advancements have been made in the training of machine learning interatomic potentials to replace costly DFT calculations. Most of these approaches use graph neural network architectures (among 1 many others 1223) with large (>1M configurations) training datasets for molecules 24,25, catalysts 1,26, and metal organic frameworks 4. Open datasets and models have led to rapid progress in each of these areas. For inorganic materials, Matbench 27 was one of the first community benchmarks and had datasets with up to 130k examples. As model development quickly advanced to architectures that could leverage more data, the training data was expanded to most of the relaxation data (1.6M examples) from the Materials Project 28 to create the MPtrj dataset 29. At the same time, the Matbench Discovery Benchmark 30 was introduced and remains the primary method for evaluating new models and approaches. Recently, further advances and state-of-the-art models on Matbench Discovery have been realized using even larger datasets (>10M) 22,23,31. However, most of these datasets and the procedures used to train these models have remained proprietary (as of 10/2024) making it challenging for the research community to build upon these advances. In this paper, we introduce the Open Materials 2024 (OMat24) dataset and models (Figure 1) from Meta FAIR to further the rapid advancement of AI and material science. The OMat24 dataset contains over 100 million single point DFT calculations sampled from diverse set of non-equilibrium atomic configurations and elemental compositions for inorganic bulk materials. The dataset builds upon other public datasets such as MPtrj 29, the Materials Project 28 and Alexandria 32 which contain equilibrium or near-equilibrium configurations 33. We pre-train several variants of the EquiformerV2 model 20,34 on the OMat24 dataset, and demonstrate state-of-the-art results on the MatBench Discovery leaderboard 30 after fine-tuning on the MPtrj dataset 29 and subset of the Alexandria dataset 32. Further, we show that the EquiformerV2 model trained with DeNS 34 on just the MPtrj dataset is also competitive with some of the largest proprietary models. The dataset, models and code are all open sourced to ensure the reproducibility of the results and to allow the community to build upon and further improve our results."
        },
        {
            "title": "2 OMat24 Dataset",
            "content": "The OMat24 dataset consists of combination of DFT single-point calculations, structural relaxations, and molecular dynamic trajectories over diverse set of inorganic bulk materials. In total, 118 million structures labeled with total energy, forces and cell stress were calculated with 400M+ core hours of compute. OMat24 includes physically-important non-equilibrium structures with wide energy, forces and stress distributions, as well as significant compositional diversity. Given the focus on non-equilibrium structures, we expect that models trained on OMat24 will be better at far-from-equilibrium and dynamic properties than models trained solely on relaxations. We also demostrate that pre-training on the diverse OMat24 data substantially improves fine-tuning performance on MPtrj. Figure 1 Overview of the OMat24 dataset generation, application areas, and sampling strategies. Inset images are random sample across the different sampling strategies."
        },
        {
            "title": "2.1 OMat24 summary and statistics",
            "content": "The OMat24 dataset includes total of 118 million structures labeled with energy, forces and cell stress. The number of atoms per structure ranges from 1 to 100 atoms per structure, with the majority of structures having 20 atoms or less. histogram of the number of atoms per structure is shown in Figure 2a. The majority of OMat24 structures have less than 20 atoms per structure as direct result of starting from structures in the Alexandria dataset, which are predominantly structures with 16 or less atoms per structure. The OMat24 structures with more than 50 atoms per structure are the larger input structures that were used for AIMD. OMat24 was designed to enable property predictions from equilibrium (see sampling strategies below), and this is captured in the label distributions. The distributions of energy, forces and stress labels along with those of the MPtrj and Alexandria datasets are shown in Figure 2(a). We observe that the energy distribution is slightly higher than the Alexandria dataset, which was used for input structures, and significantly higher than the MPtrj dataset. As expected, the distribution of forces of the OMat24 dataset is notably wider than that of both MPtrj and Alexandria. The distribution of maximum cell stress values of OMat24 is also much wider than that of the MPtrj and Alexandria datasets. The elemental distribution of OMat24 largely covers the periodic table as illustrated in Figure 2(b). As expected, the elemental distribution is similar to that of the Alexandria and MPtrj datasets in Figures 6 and 7. The dataset covers most of the elements relevant to inorganic materials discovery. Oxides are somewhat over-represented compared to other elements due to their abundance in most open datasets."
        },
        {
            "title": "2.2 Dataset generation",
            "content": "2.2.1 Crystal structure generation Input structure generation for the OMat24 dataset consists of three different processes intended to obtain diverse non-equilibrium structures: Boltzmann sampling of rattled (random Gaussian perturbations of atomic positions) structures, ab initio molecular dynamics (AIMD), and relaxations of rattled structures. These methods were used to increase the diversity of sampled configurations, similar to prior large dataset efforts 22,31. In all three approaches, initial structures were obtained by randomly sampling the relaxed structures in the Alexandria PBE bulk materials dataset (3D compounds) 32. The Alexandria dataset was chosen as starting point because it was the largest openly available DFT dataset of equilibrium and near equilibrium structures (4.5 million materials). By randomly sampling relaxed structures from the Alexandria dataset we are able to cover wide elemental composition diversity. Additionally, using Alexandria relaxed structures as starting point prevents generating structures too far from equilibrium that could result in DFT convergence errors, or unphysical starting configurations. The details for the three processes are as follows: Rattled Boltzmann sampling: For each randomly sampled Alexandria structure we generated 500 candidate non-equilibrium structures by scaling the unit cell to contain at least 10 atoms. Atomic positions were then rattled with displacements sampled from Gaussian distribution of µ = 0Å and σ = 0.5Å. Unit cells were also deformed isotropically and anisotropically from Gaussian distribution of µ = 1% and σ = 5%. For each set of 500 candidate structures we selected 5 of them from Boltzmann-like distribution based on total energies predicted by an EquiformerV2 model trained on the MPtrj dataset. The sampling procedure was done using three different sampling temperatures: 300K, 500K and 1000K. Ab-Initio Molecular Dynamics (AIMD): Short-length (50 ionic steps) ab-initio molecular dynamics were carried out starting from randomly sampled relaxed structures in the Alexandria dataset. Structures were recorded from constant temperature and volume (NVT), and constant temperature and pressure (NPT) AIMD trajectories at temperatures of 1000K and 3000K. Unit cells were scaled to contain at least 50 atoms for structures sampled at 3000K. Rattled relaxation: Relaxed structures from Alexandria were selected at random, rattled (both atomic positions and unit cell), and re-relaxed. Atomic displacements were sampled from Gaussian distribution of µ = 0Å and σ = 0.2Å. Similarly, isotropic and anisotropic cell deformations were sampled with µ = 1% and σ = 4%. All structures along the relaxation trajectory were included in the dataset. 3 Figure 2 (a) Energy per atom, forces norm and max absolute stress element distributions for MPtrj, Alexandria and OMat24 datasets. (b) Distribution of elements in the OMat24 dataset. We note that these strategies were chosen to increase diversity, but there are many possible sampling strategies that could be used to maximize information content 35,36, and we expect these strategies to be useful for future sampling efforts. Active learning 22,31 sampling strategies have the potential to further enhance these approaches but it remains unclear how they compare to random baselines when considering large scale dataset sizes. 2.2.2 DFT calculation settings and details DFT calculations generally followed Material Project default settings 33 with some important exceptions. The calculations in this work have been performed using the ab-initio total-energy and molecular-dynamics package VASP (Vienna ab-initio simulation package) developed at the Institut für Materialphysik of the Universiät Wien 37,38 with periodic boundary conditions and the projector augmented wave (PAW) pseudopotentials. Exchange and correlation effects were calculated using the generalized gradient approximation and the Perdew-Burke-Ernzerhof (PBE) with Hubbard corrections for oxide and fluoride materials containing Co, Cr, Fe, Mn, Mo, Ni, V, or W, following Materials Project defaults 33. VASP input sets were generated using the MPRelaxSet class defined in the pymatgen 39 library with the following modifications to account for recent updates and changes in the underlying algorithms and pseudopotentials: 4 Figure 3 Formation energy taken directly from the WBM dataset 46 and formation energy calculated from DFT calculations with OMat DFT settings. Outliers are primarily elements with updated psuedopotentials. 1. Version 54 of pseudopoentials provided by VASP were used, rather than the legacy PBE MPRelaxSet defaults 33. The Yb_3 and W_sv pseudopotentials were used for Yb and to account for changes between version 52 and 54 of VASP PBE pseudopotentials 40,41. 2. All calculations were done with the ALGO flag set to Normal. Relaxations were conducted with the MPRelax set defaults for IBRION/etc. All AIMD calculations were carried out for 50 steps at time-step of 2 femtoseconds. We note that 2 fs is large time-step for typical AIMD simulations, especially for hydrogen-containing materials, but was chosen as the goal was to sample diverse configurations rather than perfectly integrate the trajectories. Finally, for static calculations resulting from inputs generated using rattling and Boltzmann sampling, only the NSW and IBRION flags where updated as appropriate for single point calculations."
        },
        {
            "title": "2.3 Dataset limitations",
            "content": "The OMat24 dataset is the largest open dataset of its kind for training DFT surrogate models for materials. However, the dataset has limitations similar to many high-throughput datasets that impact the predictions of models trained using the dataset. OMat24 is calculated with PBE and PBE+U levels of DFT, which includes inherent errors in their approximation and resulting calculations 42 that are addressed to some extent in other functionals such as PBEsol 43, SCAN 44, R2SCAN 45 or hybrid functionals. The OMat24 dataset includes only periodic bulk structures and excludes important effects from point defects, surfaces, non-stoichiometry, and lower dimensional structures. Finally, the OMat24 dataset includes small fraction of structural relaxations (45,000 total relaxations) starting from distorted relaxed structures in the Alexandria dataset, and does not provide additional or novel information about stable structures. Note the calculations in OMat24 differ from those found in the Materials Project PBE and PBE+U calculations. Care must be taken when mixing calculations for analysis or training models. Although the difference in settings is small (the pseudopotential in version 5.4 and the choice of pseudopotential for Yb and W), predictions of total and formation energies differ. To illustrate this, we compare calculated energies and formation energies for MP settings and OMat24 settings using calculations in the WBM dataset 46. To understand the impact of these changes, we used the original WBM calculations, which were done with parameters fully compatible with MP calculations 46. They are compared to single-point calculations of the same relaxed structures with the OMat24 DFT settings. In order to compute formation energies, we computed elemental references and fit anion and PBE/PBE +U corrections following the methodology used in the Materials2020Compatibility class in pymatgen. The mean absolute error between WBM calculations and those we computed with OMat24 DFT settings is 52.25 meV/atom. Figure 3 shows parity plots for DFT calculated total energy and predicted formation energies for 240,000 compounds from the WBM dataset."
        },
        {
            "title": "2.4 OMat24 train, validation, and test splits",
            "content": "OMat24 is divided into several splits to ensure consistent training and evaluation by the community. Training and validation splits are released to allow for model development and iteration. The test set is divided into four different splits. The first split (WBM Test) is to ensure that the training dataset does not overlap with the Matbench Discovery leaderboard 30 created from the WBM dataset 46. The other three splits measure the accuracy of the models on in-domain training data (ID) and the ability of the models to generalize to out-of-distribution compositions (OOD-Composition) and elemental compositions (OOD-Element). The WBM Test split was created using the AFLOW structure prototype labels 47 in the aviary package 48. The prototype label of structure is standardized way to classify crystal structures by elemental stoichiometry, space group, Pearson symbol, and Wyckoff positions 47. The split includes all OMat24 structures that were generated starting from an Alexandria relaxed structure with prototype label matching any of prototype labels from the initial or relaxed structures included in the WBM dataset. Additionally, all OMat24 structures with prototype label matching an initial or relaxed WBM structure are also included in the WBM Test split. The WBM Test split includes total of 5.3 million structures. Note, filtering the dataset and creating this test split is important to ensure that there was no inadvertant data leakage from the training data to the final Matbench Discovery results, as there is overlap in materials between the Alexandria and WBM datasets. The OOD-Composition split is constructed by picking approximately 5,000 unique elemental compositions and adding all structures with matching compositions to the split. This OOD-Composition test split includes 573,000 total structures. The OOD-Elemental split is made by picking 3,000 unique element combinations and retrieving all structures matching these element combinations. This resulted in 619,000 total structures included in the OOD-Elemental split. The training, validation and ID test splits includes all remaining OMat24 structures after creating the 3 test splits described above, and includes total of 111 million structures. We randomly split this dataset into training, validation and ID test split containing 100 million, 5 million and 5 million structures respectively for the model training in this work, included in Table 1. Table 1 Size of the OMat24 train, validation and test dataset splits. Split Size Fraction % Train Validation WBM Test ID Test OOD Composition Test OOD Element Test 100,824,585 5,320,549 5,373,339 5,453,320 573,301 619,021 85.3 4.5 4.5 4.6 0.5 0."
        },
        {
            "title": "3 OMat24 models and training strategies",
            "content": "Progress in artificial intelligence and deep learning has led to the development of models that can efficiently and accurately predict and simulate materials properties 12,22,29,4851. Recently, Graph Neural Network (GNN) machine learning potentials have surpassed the accuracy of other ML models when predicting and simulating mechanical properties 30,52,53. All of the top models on the OC20 leaderboard 54, dataset with similar size and diversity to OMat24, are GNNs. Similarly, for the task of predicting ground-state formation energy and energy above the convex hullas proxy for materials discoveryGNN interatomic potentials have surpassed all other methodologies 30 and have set new standard in terms of scale (number of different materials) and accuracy (predicted energy errors) 2123,50,51. We leverage the OMat24 dataset along with the MPtrj 29 and Alexandria 32 datasets to train GNNs. Since similar structures exist in the Alexandria and the WBM dataset used for testing, we subsampled the Alexandria dataset for training to ensure there was no leakage between the training and testing datasets. The new subset of Alexandria (sAlexandria) was created by removing all trajectories in which any structure matched 6 structure in the WBM initial and relaxed structures using structure prototype labels 47. Next, we reduced the size of the dataset by removing all structures with energies > 0 eV, forces norm > 50 eV/Å, and stress > 80 GPa. Finally, we only sampled structures in the remaining trajectories that had energy difference greater than 10 meV/atom. The resulting datasets used for training and validation had 10 million and 500 thousand structures respectively. For model architectures, we focused solely on EquiformerV2 20 since it is currently the top performing model on the OC20 1, OC22 26 and ODAC23 4 leaderboards among the many contributions from the broader scientific community. The models are trained to predict energy, forces and stress given an input structure. The models are optimized for relaxed energy prediction for aiding materials discovery by directly predicting forces and stress, instead of relying on energy derivatives. For model training, we explored three strategies: 1. EquiformerV2 models trained solely on the OMat24 dataset, with and without denoising augmentation objectives. These are the models with the most physical meaning as they are fit solely on datasets containing important updates to the underlying psuedopotentials relative to the legacy Materials Project settings. 2. EquiformerV2 models trained solely on the MPtrj dataset, with and without denoising augmentation objectives, useful for direct comparison on the Matbench Discovery leaderboard (denoted compliant models). 3. EquiformerV2 models from (1) or OC20 checkpoints further fine-tuned on either the MPtrj or sAlexandria datasets, leading to the highest performing models for the Matbench Discovery leaderboard (denoted non-compliant). In each case, several model sizes were chosen. Table 2 lists the total number of parameters for the models trained. Additional model specifications, training information and parameters are given in Section B, Table 7 and Table 8. Similar to many of the leading OC20 models and the recently released ORB models 23, we use non-conservative model here with separate energy, force, and stress heads for GPU-efficient training. Table 2 Total number of parameters and their inference throughput in the EquiformerV2 models in this work. Throughput evaluated on Nvidia A100 GPUs with batch size 1 and no inference-time optimization with samples from the MPtrj dataset. Model # of Parameters eqV2-S (small) eqV2-M (medium) eqV2-L (large) 31,207,434 86,589,068 153,7698,68 Throughput (Samples / GPU sec. (MPtrj) 9.4 7.4 4.9 We also explored the role of including auxiliary denoising objectives for training models with different dataset types and sizes. We use the Denoising Non-equilibrium Structures (DeNS) protocol 55 to train EquiformerV2 models on the OMat24 dataset and the MPtrj relaxation dataset. In addition to predicting energy, forces, and stress given input structures, DeNS introduces data augmentation by letting the model predict the energy and added noise from perturbed input structures and unperturbed forces."
        },
        {
            "title": "4 Results",
            "content": "We include results for each of the three training strategies described above (training models from scratch on MPtrj, training models from scratch on OMat24, and fine-tuning OMat24 or OC20-trained models on MPtrj and Alexandria). OMat24 pre-training is performed using only the 100M training split to avoid dataset contamination with WBM. We provide all of the OMat24 training data 56 with Creative Commons 4.0 license, and the necessary source code 57 and model weights 58 with permissive open source license (with some geographic and acceptable use restrictions) necessary to reproduce our results. 7 We evaluate the models on the popular Matbench-Discovery benchmark 30. The benchmark provides useful set of metrics to measure the impact and progress that machine learning potentials have in accelerating materials discovery. Specifically it evaluates the task of predicting ground-state (0 K) thermodynamic stability, which examines materials decomposition energy (stability) with respect to all possible competing phases 59. Predicting the energy above the convex hull with DFT level accuracy is reliable proxy for thermodynamic stability predictions 52,59,60. When evaluated on Matbench-Discovery, we observe the EquiformerV2 model achieves state-of-the-art performance on the leaderboard (with F1 being the primary metric) for both compliant (trained using MPtrj only) and non-compliant (trained with additional data) benchmarks. When training on MPtrj only, our best model achieves substantial performance improvement from DeNS 34. Because MPtrj is relatively small dataset with relaxation trajectories only, we interpret the improved performance as result of the effective data augmentation through the noisy input structures that DeNS provides. When training on both the OMat24 and MPtrj datasets, we observe that pre-training on OMat24 (a dataset with high composition diversity and predominantly non-equilibrium structures) and fine-tuning on MPtrj and sAlexandria substantially improves model performance to an F1 score of 0.916 and an energy MAE of 20 meV/atom - the top scores for non-compliant models."
        },
        {
            "title": "4.1 Models trained solely on OMat24",
            "content": "We trained three models of different sizes listed in Table 2 using the OMat24 training data for total of two epochs. Tables 3 and 4 list validation and test mean absolute error (MAE) for predicted energy, forces and stress. The ID and OOD test splits result in similiar results demonstrating the ability of the models to generalize to new compositions and elemental combinations. Interestingly, the WBM split performs worse, and is likely due the materials in the split having greater material diversity than the OOD splits. When training eqV2-S with DeNS, the results are slightly worse with energy MAE of 11.3 meV/atom and force MAE of 52.0 meV/Å, suggesting that due to the scale and diversity in the OMat24 dataset, denoising does not provide additional training improvements. Table 3 Validation mean absolute error metrics of equiformerV2 models trained on the OMat24 dataset. Energy errors are in units meV/atom, forces errors are in meV/Å and stress errors are in meV/Å3. Model Energy Forces Stress Forces cos eqV2-S eqV2-M eqV2-L 11 10 9.6 49.2 44.8 43.1 2.4 2.3 2.3 0.985 0.986 0.987 Table 4 Test mean absolute error metrics of equiformerV2 models trained on OMat24 dataset. Energy errors are in units meV/atom, forces errors are in meV/Å and stress errors are in meV/Å3. WBM test ID test OOD composition test OOD element test Model energy forces stress energy forces stress energy forces stress energy forces stress 15.96 50.68 3.69 eqV2-S eqV2-M 14.87 46.26 3.61 eqV2-L 14.57 44.72 3.57 12.09 51.14 2.73 11.09 46.62 2.64 10.70 44.88 2.60 11.05 48.85 2.53 44.51 2.45 9.98 43.00 2.42 9.70 9.34 8.83 8.38 49.14 2.16 44.71 2.06 43.08 2."
        },
        {
            "title": "4.2 Models trained from scratch on MPtrj as “compliant” Matbench-Discovery models",
            "content": "We use models trained on the MPtrj dataset as baseline for performance. By training only on the smaller MPtrj dataset of relaxation trajectories, we can determine how much pre-training with the OMat24 and OC20 datasets can improve model performance. These baseline models also fall into the compliant model category in the Matbench-Discovery benchmark, which allows disciplined comparison of our EquiformerV2 models to other architectures. 8 We trained models using the original MPtrj dataset 29, removing all structures in which all the atoms are farther than 12 Å apart. We used the uncorrected total energy, forces and stress as labels. For model size, we trained small variant (eqV2-S in Table 2). We also trained small, medium and large (eqV2-S, eqV2-M, and eqV2-L) models with DeNS 34. More details on model and training hyper-parameters are summarized in Table 7 and Table 8. Table 5 shows the results of the compliant models on the Matbench-Discovery benchmark. The EquiformerV2S model outperforms prior models with an F1 score of 0.77. With the addition of DeNS, the F1 scores significantly improve to 0.815 with the smaller model. For the larger model, the regularization provided by DeNS enables effective training even with the smaller MPtrj dataset to achieve the highest F1 score of 0.823 for compliant models. The energy MAE does not vary significantly (35-36 meV/atom) for different model sizes, and indicates that the smaller model may be the most useful in practice. Table 5 Matbench-Discovery benchmark results of compliant models trained only on MPTraj with results on the unique prototype split. Mean absolute error (MAE) and Root mean squared error (RMSE) are in units of eV/atom. Model eqV2-L-DeNS eqV2-M-DeNS eqV2-S-DeNS eqV2-S ORB MPtrj SevenNet MACE F1 DAF Precision Recall Accuracy TPR FPR TNR FNR MAE RMSE R2 0.823 5.184 0.792 0.856 0.944 0.856 0.041 0.959 0.144 35 82 0.802 0.818 5.109 0.781 0.858 0. 0.858 0.044 0.956 0.142 35 82 0.803 0.815 5.042 0.771 0.864 0.941 0.864 0.047 0.953 0.136 36 85 0.788 0.77 4.64 0.709 0.841 0. 0.841 0.063 0.937 0.159 42 87 0.778 0.765 4.702 0.719 0.817 0.922 0.817 0.059 0.941 0.183 45 91 0.756 0.724 4.252 0.65 0.818 0. 0.818 0.081 0.919 0.182 48 92 0.75 0.669 3.777 0.577 0.796 0.878 0.796 0.107 0.893 0.204 57 101 0."
        },
        {
            "title": "4.3 Models pre-trained on OMat24 or OC20, and fine-tuned for Matbench-Discovery",
            "content": "Table 6 shows the resulting metrics for the models pre-trained with the OMat24 and OC20 datasets and fine-tuned on MPTrj or sAlex and MPTrj jointly, and other non-compliant models on the Matbench-Discovery leaderboard. Additional results for the total and 10K most stable predictions splits are listed in Supplementary section D. Several trends can be observed. Pre-training on the OMat24 leads to significantly better results on almost all metrics, and significantly outperforms previous approaches on the F1 (0.916) and energy MAE (20 meV / atom) metrics. Fine-tuning on both sAlexandria and MPtrj outperforms just fine-tuning on MPtrj. Pre-training on OC20, dataset not intended for training models for materials, provides surprisingly strong results once fine-tuned with MPtrj. Larger models (eqV2-M) do outperform the smaller models (eqV2-S), but depending on the applications the accuracy improvements may not be worth the additional compute cost. Finally, we did train models using DeNS for both pre-training and fine-tuning, but did not see an improvement in accuracy. We hypothesize this was due to the diversity of the larger datasets not requiring the additional regularization provided by DeNS."
        },
        {
            "title": "5 Conclusions and future directions",
            "content": "In summary, the EquiformerV2 model trained from scratch on MPtrj is state-of-the-art for compliant models on MatBench Discovery with remarkable 35 meV/atom MAE. When models are pre-trained on OMat24, the results improve substantially with an energy above hull MAE of 20 meV/atom on MatBench Discovery and are the first models of any kind to reach the F1=0.9 threshold. In practice, the models trained from scratch on OMat24 are likely to be even more useful as they have energy MAE of only 10 meV/atom on the OMat24 test splits despite most of the structures being at elevated temperatures. We are encouraged by the results of applying denoising objectives for improved model accuracy and data 9 Table 6 Matbench-Discovery benchmark results of non-compliant models on the unique protoype split. Mean absolute error (MAE) and Root mean squared error (RMSE) are in units of eV/atom. Model Pre-train Dataset Fine-tune Dataset MPtrj-sAlex eqV2-M OMat eqV2-M OMat MPtrj eqV2-S OMat eqV2-S OMat MPtrj-sAlex MPtrj eqV2-L OC20 MPtrj eqV2-S OC20 MPtrj ORB MatterSim GNoME F1 DAF Precision Recall Accuracy TPR FPR TNR FNR MAE RMSE R2 0.916 6.040 0.923 0.91 0.974 0.91 0.014 0.986 0.09 20 72 0. 0.909 5.948 0.909 0.909 0.973 0.909 0.017 0.983 0.091 21 72 0.849 0.901 5.902 0.902 0.9 0.97 0.9 0.018 0.982 0.1 24 80 0. 0.89 5.752 0.879 0.901 0.966 0.901 0.023 0.977 0.099 26 81 0.807 0.86 5.639 0.862 0.858 0.957 0.858 0.025 0.975 0.142 29 78 0. 0.837 5.392 0.824 0.849 0.951 0.849 0.033 0.967 0.151 33 80 0.81 0.88 6.041 0.924 0.841 0.965 0.841 0.013 0.987 0.159 28 77 0. 0.859 5.646 0.863 0.856 0.957 0.856 0.025 0.975 0.144 26 80 0.812 0.829 5.523 0.844 0.814 0.955 0.814 0.028 0.972 0.186 35 85 0. efficiency. Applying strategies developed and honed on the OC20 dataset 55, we obtain strong results when trained solely on the MPtrj dataset. Perhaps unsurprisingly these denoising objectives become less important in the limit of very large well sampled datasets, with no improvements when training on OMat24. We believe that the observed transferability of model performance from OC20 to OMat24 is yet another strong indication that model training strategies developed for very large simulation datasets are likely transferable to other large dataset challenges (similar to correlations seen between OC20, OC22, and ODAC23). Perhaps most importantly, the release of the OMat24 dataset, one of the largest open DFT datasets, and the models trained in this work, will provide valuable resource for future model and dataset development. We hope the community builds upon our efforts, driving further advancements in the capabilities of ML models in material science. This work builds on number of open science datasets such as the Materials Project 33 and the Alexandria datasets 32. As the MatBench Discovery benchmark approaches saturation (the best models are now F1>0.9), it is essential to develop newer and improved benchmarks that address its inherent limitations. In particular, discrepancies between the PBE functional and experimental results limit the effective value of predictions with respect to experiments. Developing training sets and benchmarks using more accurate DFT functionals such as SCAN and R2SCAN could help overcome these challenges 61. As of release v2022.10.28 62, the Materials Project now includes R2SCAN calculations for subset of its data. Large scale datasets similar to OMat24 but with SCAN, R2SCAN, or other functionals are an obvious next step. We hope that the release of OMat24 will enable these efforts to proceed in an even more data-efficient manner. While our work has focused on materials discovery, there are still many opportunities for exploration beyond this application that are unlocked by OMat24 but have not yet been thoroughly explored. For instance, molecular dynamics (MD) and Monte Carlo (MC) simulations are critical methodologies needed to predict and study properties beyond formation energy at zero Kelvin. Substantial progress and work has been published on the reliability and shortcomings of MD with GNN interatomic potential 6365. Future research could investigate the applicability of models trained using the OMat24 dataset to these areas, where non-equilibrium training data may be even more important."
        },
        {
            "title": "6 Acknowledgements",
            "content": "We acknowledge helpful discussions with Miguel Marques (Ruhr University Bochum), Janosh Riebesall (Radical AI), Shyue Ping Ong (UCSD), Yi-Lun Liao (MIT), Tess Smidt (MIT) and Matt Uyttendaele. The authors acknowledge valuable assistance from Kyle Michel and Lowik Chanussot (FAIR at Meta) in enabling and scaling the simulation datasets on Meta compute resources. We generated many of our figures using crystal-toolkit 66 and pymatviz 67."
        },
        {
            "title": "References",
            "content": "[1] Lowik Chanussot, Abhishek Das, Siddharth Goyal, Thibaut Lavril, Muhammed Shuaibi, Morgane Riviere, Kevin Tran, Javier Heras-Domingo, Caleb Ho, Weihua Hu, Aini Palizhati, Anuroop Sriram, Brandon Wood, Junwoong Yoon, Devi Parikh, C. Lawrence Zitnick, and Zachary Ulissi. Open catalyst 2020 (oc20) dataset and community challenges. ACS Catalysis, 11(10):60596072, 2021. [2] Lawrence Zitnick, Lowik Chanussot, Abhishek Das, Siddharth Goyal, Javier Heras-Domingo, Caleb Ho, Weihua Hu, Thibaut Lavril, Aini Palizhati, Morgane Riviere, et al. An introduction to electrocatalyst design using machine learning for renewable energy storage. arXiv preprint arXiv:2010.09435, 2020. [3] Arunima Singh, Joseph Montoya, John Gregoire, and Kristin Persson. Robust and synthesizable photocatalysts for co2 reduction: data-driven materials discovery. Nature Communications, 10(1):443, 2019. [4] Anuroop Sriram, Sihoon Choi, Xiaohan Yu, Logan M. Brabson, Abhishek Das, Zachary Ulissi, Matt Uyttendaele, Andrew J. Medford, and David S. Sholl. The open dac 2023 dataset and challenges for sorbent discovery in direct air capture. ACS Central Science, 10(5):923941, 2024. [5] Kamal Choudhary, Brian DeCost, Chi Chen, Anubhav Jain, Francesca Tavazza, Ryan Cohn, Cheol Woo Park, Alok Choudhary, Ankit Agrawal, Simon JL Billinge, et al. Recent advances and applications of deep learning methods in materials science. npj Computational Materials, 8(1):59, 2022. [6] G. Ceder, Y. M. Chiang, D. R. Sadoway, M. K. Aydinol, Y. I. Jang, and B. Huang. Identification of cathode materials for lithium batteries guided by first-principles calculations. Nature, 392(6677):694696, 1998. [7] Mark W. Tibbitt, Christopher B. Rodell, Jason A. Burdick, and Kristi S. Anseth. Progress in material design for biomedical applications. Proceedings of the National Academy of Sciences, 112(47):1444414451, 2015. [8] George Crabtree, Sharon Glotzer, Bill McCurdy, and Jim Roberto. Computational materials science and chemistry: Accelerating discovery and innovation through simulation-based engineering and science. Technical report, United States, 2010. [9] Timothy Erps, Michael Foshey, Mina Konaković Luković, Wan Shou, Hanns Hagen Goetzke, Herve Dietsch, Klaus Stoll, Bernhard von Vacano, and Wojciech Matusik. Accelerated discovery of 3d printing materials using data-driven multiobjective optimization. Science advances, 7(42):eabf7435, 2021. [10] Shulin Luo, Tianshu Li, Xinjiang Wang, Muhammad Faizan, and Lijun Zhang. High-throughput computational materials screening and discovery of optoelectronic semiconductors. WIREs Computational Molecular Science, 11(1):e1489, 2021. [11] Elizabeth A. Pogue, Alexander New, Kyle McElroy, Nam Q. Le, Michael J. Pekala, Ian McCue, Eddie Gienger, Janna Domenico, Elizabeth Hedrick, Tyrel M. McQueen, Brandon Wilfong, Christine D. Piatko, Christopher R. Ratto, Andrew Lennon, Christine Chung, Timothy Montalbano, Gregory Bassen, and Christopher D. Stiles. Closed-loop superconducting materials discovery. npj Computational Materials, 9(1):181, 2023. [12] Chi Chen and Shyue Ping Ong. universal graph deep learning interatomic potential for the periodic table. Nature Computational Science, 2(11):718728, 2022. [13] Kamal Choudhary and Brian DeCost. Atomistic line graph neural network for improved materials property predictions. npj Computational Materials, 7(1):185, 2021. [14] Ilyes Batatia, David Peter Kovacs, Gregor N. C. Simm, Christoph Ortner, and Gabor Csanyi. MACE: Higher order equivariant message passing neural networks for fast and accurate force fields. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022. 11 [15] Ilyes Batatia, Simon Batzner, Dávid Péter Kovács, Albert Musaelian, Gregor NC Simm, Ralf Drautz, Christoph Ortner, Boris Kozinsky, and Gábor Csányi. The design space of (3)-equivariant atom-centered interatomic potentials. arXiv preprint arXiv:2205.06643, 2022. [16] Simon Batzner, Albert Musaelian, Lixin Sun, Mario Geiger, Jonathan P. Mailoa, Mordechai Kornbluth, Nicola Molinari, Tess E. Smidt, and Boris Kozinsky. E(3)-equivariant graph neural networks for dataefficient and accurate interatomic potentials. Nature Communications, 13(1):2453, 2022. [17] Alvaro Sanchez-Gonzalez, Jonathan Godwin, Tobias Pfaff, Rex Ying, Jure Leskovec, and Peter W. In Proceedings of the 37th Battaglia. Learning to simulate complex physics with graph networks. International Conference on Machine Learning, ICML20. JMLR.org, 2020. [18] Johannes Gasteiger, Muhammed Shuaibi, Anuroop Sriram, Stephan Günnemann, Zachary Ward Ulissi, C. Lawrence Zitnick, and Abhishek Das. Gemnet-OC: Developing graph neural networks for large and diverse molecular simulation datasets. Transactions on Machine Learning Research, 2022. [19] Saro Passaro and Lawrence Zitnick. Reducing so (3) convolutions to so (2) for efficient equivariant gnns. In International Conference on Machine Learning, pages 2742027438. Proceedings of Machine Learning Research, 2023. [20] Yi-Lun Liao, Brandon Wood, Abhishek Das, and Tess Smidt. Equiformerv2: Improved equivariant transformer for scaling to higher-degree representations. arXiv preprint arXiv:2306.12059, 2023. [21] Yutack Park, Jaesun Kim, Seungwoo Hwang, and Seungwu Han. Scalable parallel algorithm for graph neural network interatomic potentials in molecular dynamics simulations. Journal of Chemical Theory and Computation, 2024. [22] Han Yang, Chenxi Hu, Yichi Zhou, Xixian Liu, Yu Shi, Jielan Li, Guanzhi Li, Zekun Chen, Shuizhou Chen, Claudio Zeni, et al. Mattersim: deep learning atomistic model across elements, temperatures and pressures. arXiv preprint arXiv:2405.04967, 2024. [23] Orbital Materials. orb-models, 2024. orb-models github repository. https:// github.com/ orbital-materials/ [24] Peter Eastman, Pavan Kumar Behara, David Dotson, Raimondas Galvelis, John Herr, Josh Horton, Yuezhi Mao, John Chodera, Benjamin Pritchard, Yuanqing Wang, Gianni De Fabritiis, and Thomas E. Markland. Spice, dataset of drug-like molecules and peptides for training machine learning potentials. Scientific Data, 10(1):11, 2023. [25] Justin Smith, Roman Zubatyuk, Benjamin Nebgen, Nicholas Lubbers, Kipton Barros, Adrian Roitberg, Olexandr Isayev, and Sergei Tretiak. The ani-1ccx and ani-1x data sets, coupled-cluster and density functional theory properties for molecules. Scientific Data, 7(1):134, 2020. [26] Richard Tran, Janice Lan, Muhammed Shuaibi, Brandon Wood, Siddharth Goyal, Abhishek Das, Javier Heras-Domingo, Adeesh Kolluru, Ammar Rizvi, Nima Shoghi, et al. The open catalyst 2022 (oc22) dataset and challenges for oxide electrocatalysts. ACS Catalysis, 13(5):30663084, 2023. [27] Alexander Dunn, Qi Wang, Alex Ganose, Daniel Dopp, and Anubhav Jain. Benchmarking materials property prediction methods: the matbench test set and automatminer reference algorithm. npj Computational Materials, 6(1):138, 2020. [28] A. Jain, S. P. Ong, G. Hautier, W. Chen, W. D. Richards, S. Dacek, S. Cholia, D. Gunter, D. Skinner, G. Ceder, and K. A. Persson. The Materials Project: materials genome approach to accelerating materials innovation. APL Materials, 1(1):011002, 2013. [29] Bowen Deng, Peichen Zhong, KyuJung Jun, Janosh Riebesell, Kevin Han, Christopher Bartel, and Gerbrand Ceder. Chgnet as pretrained universal neural network potential for charge-informed atomistic modelling. Nature Machine Intelligence, 5(9):10311041, 2023. [30] Janosh Riebesell, Rhys EA Goodall, Anubhav Jain, Philipp Benner, Kristin Persson, and Alpha Lee. Matbench discoveryan evaluation framework for machine learning crystal stability prediction. arXiv preprint arXiv:2308.14920, 2023. 12 [31] Amil Merchant, Simon Batzner, Samuel Schoenholz, Muratahan Aykol, Gowoon Cheon, and Ekin Dogus Cubuk. Scaling deep learning for materials discovery. Nature, 624(7990):8085, 2023. [32] Jonathan Schmidt, Tiago FT Cerqueira, Aldo Romero, Antoine Loew, Fabian Jäger, Hai-Chen Wang, Silvana Botti, and Miguel AL Marques. Improving machine-learning models in materials science through large datasets. Materials Today Physics, page 101560, 2024. [33] A. Jain, S. P. Ong, G. Hautier, W. Chen, W. D. Richards, S. Dacek, S. Cholia, D. Gunter, D. Skinner, G. Ceder, et al. Commentary: The materials project: materials genome approach to accelerating materials innovation. APL Materials, 1(1), 2013. [34] Yi-Lun Liao, Tess Smidt, and Abhishek Das. Generalizing denoising to non-equilibrium structures improves equivariant force fields. arXiv preprint arXiv:2403.09549, 2024. [35] Daniel Schwalbe-Koda, Aik Rui Tan, and Rafael Gómez-Bombarelli. Differentiable sampling of molecular geometries with uncertainty-based adversarial attacks. Nature Communications, 12(1):5104, 2021. [36] Kangming Li, Daniel Persaud, Kamal Choudhary, Brian DeCost, Michael Greenwood, and Jason HattrickSimpers. Exploiting redundancy in large materials datasets for efficient machine learning with less data. Nature Communications, 14(1):7283, 2023. [37] Georg Kresse and Jürgen Hafner. Ab initio molecular-dynamics simulation of the liquid-metalamorphoussemiconductor transition in germanium. Physical Review B, 49(20):1425114269, 1994. [38] Georg Kresse and Jürgen Furthmüller. Efficient iterative schemes for ab initio total-energy calculations using plane-wave basis set. Physical Review B, 54(16):1116911186, 1996. [39] Shyue Ping Ong, William Davidson Richards, Anubhav Jain, Geoffroy Hautier, Michael Kocher, Shreyas Cholia, Dan Gunter, Vincent Chevrier, Kristin Persson, and Gerbrand Ceder. Python materials genomics (pymatgen): robust, open-source python library for materials analysis. Computational Materials Science, 68:314319, 2013. [40] Materials Project. Issue #2968: Add support for new crystal structure prediction method. https: //github.com/materialsproject/pymatgen/issues/2968, 2023. [41] Materials Project. Issue #3016: Add support for new crystal structure prediction method. https: //github.com/materialsproject/pymatgen/issues/3016, 2023. [42] John Perdew, Kieron Burke, and Matthias Ernzerhof. Generalized gradient approximation made simple. Physical Review Letters, 77(18):3865, 1996. [43] John Perdew, Adrienn Ruzsinszky, Gábor Csonka, Oleg Vydrov, Gustavo Scuseria, Lucian Constantin, Xiaolan Zhou, and Kieron Burke. Restoring the density-gradient expansion for exchange in solids and surfaces. Physical review letters, 100(13):136406, 2008. [44] Jianwei Sun, Adrienn Ruzsinszky, and John Perdew. Strongly constrained and appropriately normed semilocal density functional. Physical Review Letters, 115(3):036402, 2015. [45] James Furness, Aaron Kaplan, Jinliang Ning, John Perdew, and Jianwei Sun. Accurate and numerically efficient r2scan meta-generalized gradient approximation. The Journal of Physical Chemistry Letters, 11(19):82088215, 2020. [46] Hai-Chen Wang, Silvana Botti, and Miguel AL Marques. Predicting stable crystalline compounds using chemical similarity. npj Computational Materials, 7(1):12, 2021. [47] Michael Mehl, David Hicks, Cormac Toher, Ohad Levy, Robert Hanson, Gus Hart, and Stefano Curtarolo. The aflow library of crystallographic prototypes: part 1. Computational Materials Science, 136:S1S828, 2017. [48] Rhys EA Goodall, Abhijith Parackal, Felix Faber, Rickard Armiento, and Alpha Lee. Rapid discovery of stable materials by coordinate-free coarse graining. Science Advances, 8(30):eabn4117, 2022. [49] Chi Chen, Weike Ye, Yunxing Zuo, Chen Zheng, and Shyue Ping Ong. Graph networks as universal machine learning framework for molecules and crystals. Chemistry of Materials, 31(9):35643572, 2019. 13 [50] Ilyes Batatia, Philipp Benner, Yuan Chiang, Alin Elena, Dávid Kovács, Janosh Riebesell, Xavier Advincula, Mark Asta, William Baldwin, Noam Bernstein, et al. foundation model for atomistic materials chemistry. arXiv preprint arXiv:2401.00096, 2023. [51] Amil Merchant, Simon Batzner, Samuel Schoenholz, Muratahan Aykol, Gowoon Cheon, and Ekin Dogus Cubuk. Scaling deep learning for materials discovery. Nature, 624(7990):8085, 2023. [52] Christopher Bartel, Amalie Trewartha, Qi Wang, Alexander Dunn, Anubhav Jain, and Gerbrand Ceder. critical examination of compound stability predictions from machine-learned formation energies. npj Computational Materials, 6(1):97, 2020. [53] Alexandre Duval, Simon Mathis, Chaitanya Joshi, Victor Schmidt, Santiago Miret, Fragkiskos Malliaros, Taco Cohen, Pietro Lio, Yoshua Bengio, and Michael Bronstein. hitchhikers guide to geometric gnns for 3d atomic systems. arXiv preprint arXiv:2312.07511, 2023. [54] Meta Fundamental AI Research and collaborators. opencatalystproject.org/ leaderboard.html, 2024. The oc20 leaderboard. https:// [55] Yi-Lun Liao, Tess Smidt, and Abhishek Das. Generalizing denoising to non-equilibrium structures improves equivariant force fields. arXiv preprint arXiv:2403.09549, 2024. [56] Meta Fundamental AI Research. The omat24 dataset. https:// huggingface.co/ datasets/ fairchem/ OMAT24 , 2024. [57] Meta Fundamental AI Research and Collaborators. The fair chemistry (fairchem) model repository. https:// github.com/ FAIR-Chem/ fairchem, 2024. [58] Meta Fundamental AI Research. The omat24 trained model checkpoints. https:// huggingface.co/ fairchem/ OMAT24 , 2024. [59] Christopher Bartel. Review of computational approaches to predict the thermodynamic stability of inorganic solids. Journal of Materials Science, 57(23):1047510498, 2022. [60] Wenhao Sun, Stephen Dacek, Shyue Ping Ong, Geoffroy Hautier, Anubhav Jain, William Richards, Anthony Gamst, Kristin Persson, and Gerbrand Ceder. The thermodynamic scale of inorganic crystalline metastability. Science Advances, 2(11):e1600225, 2016. [61] Ryan Kingsbury, Ayush Gupta, Christopher Bartel, Jason Munro, Shyam Dwaraknath, Matthew Horton, and Kristin Persson. Performance comparison of r2-scan and scan metagga density functionals for solid materials via an automated, high-throughput computational workflow. Physical Review Materials, 6(1):013801, 2022. [62] The Materials Project. Materials project database versions. https:// docs.materialsproject.org/ changes/ database-versions, 2024. [63] Sina Stocker, Johannes Gasteiger, Florian Becker, Stephan Günnemann, and Johannes Margraf. How robust are modern graph neural network potentials in long and hot molecular dynamics simulations? Machine Learning: Science and Technology, 3(4):045010, 2022. [64] Xiang Fu, Zhenghao Wu, Wujie Wang, Tian Xie, Sinan Keten, Rafael Gomez-Bombarelli, and Tommi Jaakkola. Forces are not enough: Benchmark and critical evaluation for machine learning force fields with molecular simulations. arXiv preprint arXiv:2210.07237, 2022. [65] S. Raja, I. Amin, F. Pedregosa, and A. S. Krishnapriyan. Stability-aware training of neural network interatomic potentials with differentiable boltzmann estimators. arXiv preprint arXiv:2402.13984, 2024. [66] Matthew Horton, Jimmy-Xuan Shen, Jordan Burns, Orion Cohen, François Chabbey, Alex Ganose, Rishabh Guha, Patrick Huck, Hamming Howard Li, Matthew McDermott, et al. Crystal toolkit: web app framework to improve usability and accessibility of materials science research algorithms. arXiv preprint arXiv:2302.06147, 2023. [67] Janosh Riebesell, Haoyu Yang, Rhys Goodall, and Sterling G. Baird. Pymatviz: visualization toolkit for materials informatics, 2022. 10.5281/zenodo.7486816 - https://github.com/janosh/pymatviz."
        },
        {
            "title": "A Dataset statistics",
            "content": "Figures 4 shows histograms for the number of atoms per structure in the sub-datasets that make up the OMat24 dataset. Similarly figure 5 shows the distributions of energy, forces and stress in the different sub-datasets. The number of atoms per system is shifted to largers sizes, since these structures were generated by tiling the unit cell of relaxed Alexandria structures. The rest of the distributions are concentrated at smaller sizes, as direct result of the distribution in the Alexandria dataset shown in Figure 2. From Figure 5 we can see that the AIMD portion of the dataset has much narrower force and stress distribution compared to that of structures generated from rattled relaxations and rattled Boltzmann sampling. We also observe shift to lower energies for structures generated using AIMD. Figure 4 Histogram of number of atoms per structure per sub-dataset in OMat-24 dataset. Figure 5 Energy, forces norm and maximum absolute stress densities for all sub-datasets in OMat-24. Figures 6 and 7 show per element histograms of the number of occurences in each dataset. The histograms are created by randomly sampling 1% and 10% of each dataset respectively, as result elements that appear very rarely may have been missed. By comparing Figures 6 and 7 with the corresponding histogram for the OMat24 dataset in Figure 2b we observe that the distributions between OMat24 and Alex are very similar and more uniformly distributed that that of MPtrj. Figure 6 Element distribution in Alexandria bulk PBE dataset 32. Figure 7 Element distribution in MPtrj bulk dataset 29. 16 Model training hyper-parameters and configuration All models trained in this work use the EquiformerV2 architecture 20 with separate heads for energy, forces and stress prediction. We use per-atom MAE loss for energy and an l2 norm loss for forces. For stress prediction, we decompose the 3 3 symmetric stress tensor to 1-dimensional isotropic component (L = 0) and 5-dimensional anisotropic component (L = 2). We use prediction head that outputs scalar and irreps of = 2, then use an MAE loss for the isotropic and the anisotropic component separately. At test time, we recover the stress tensor by combining the isotropic and anisotropic components. For models trained with DeNS, an additional head is added to predict the noise added to perturbed structure. For DeNS forward pass, we input the noisy structure and predict the unperturbed energy as well as the noise vectors, then compute per-atom MAE loss for energy and an MSE loss for the noise vectors. We refer to the readers to the original EquiformerV2 and DeNS papers for more details on model architectures 20,34. The hyper-parameters for our models are summarized in Table table 7. The hyper-parameters for training are summarized in Table table 8. We use the same mixed precision strategy as the original EquiformerV paper 20 to speed up the training process. Models were trained on 64 NVIDIA A100 GPUs for pre-training and 32 NVIDIA A100 GPUs for fine-tuning with distributed data parallel. Table 7 Hyper-parameters for the EquiformerV2 models of different sizes. All hyper-parameters for given model size is used for all dataset settings. We denote the dimension of irreps features as (Lmax, C) where Lmax is the maximum degree and is the number of channels. Hyper-parameters eqV2-S eqV2-M eqV2-L Maximum degree Lmax Maximum order Mmax Number of Transformer blocks Cutoff radius (Å) Maximum number of neighbors Number of radial bases Dimension of hidden scalar features in radial functions dedge Embedding dimension dembed (L) ij dimension dattn_hidden Number of attention heads (0) ij dimension dattn_alpha Value dimension dattn_value Hidden dimension in feed forward networks df Resolution of point samples 4 2 8 12 20 600 (0, 128) (4, 128) (4, 64) 8 (0, 64) (4, 16) (4, 128) 18 6 4 10 12 20 600 (0, 128) (6, 128) (6, 64) 8 (0, 64) (6, 16) (6, 128) 6 3 20 12 20 600 (0, 128) (6, 128) (6, 64) 8 (0, 64) (6, 16) (6, 128) 18 Table 8 Hyper-parameters for EquiformerV2 model training for different dataset setting. All model sizes use the same set of hyper-parameters for given dataset setting. Hyper-parameters MPTrj training OMat training MPTrj Fine-tuning MPTrj+sAlex Fine-tuning Optimizer Learning rate scheduling Warmup epochs Warmup factor Maximum learning rate Minimum learning rate factor Batch size Number of epochs Gradient clipping norm threshold Model EMA decay Weight decay Dropout rate Stochastic depth Energy loss coefficient Force loss coefficient Stress loss coefficient DeNS settings AdamW Cosine 0.1 0.2 2 104 0.01 512 150 100 0.999 1 103 0.1 0.1 20 20 5 Probability of optimizing DeNS Standard deviation of Gaussian noise DeNS loss coefficient 0.5 0.1 10 AdamW Cosine 0.1 0.2 2 104 0.01 256 32 100 0.999 1 103 0.1 0.1 20 10 1 - - - AdamW Cosine 0.1 0.2 2 104 0.01 256 8 100 0.999 1 103 0.1 0.1 20 10 - - - AdamW Cosine 0.01 0.2 2 104 0.01 512 2 100 0.999 1 103 0.1 0.1 20 20 5 0.25 0.1 10 17 Validation metrics for models trained and fine-tuned with MPtrj Tables 9 and 10 list the validation metrics for the modles trained or fine-tuned solely on MPTrj. Table 9 Validation mean absolute error metrics for models trained on the MPtrj dataset. Energy errors are in units meV/atom, forces errors are in meV/Å and stress errors are in meV/Å3. model energy forces stress forces cos eqV2-S eqV2-S-DeNS eqV2-M-DeNS eqV2-L-DeNS 12.4 11.43 11.17 10.58 32.22 31.67 31.46 30.48 1.55 1.44 1.48 1.47 0.72 0.72 0.728 0.738 Table 10 Validation metrics for finetuning OMat pretrained models on MPtrj model energy (meV/atom) forces (meV/A) stress (meV/A3) forces cos eqV2-S-OMat-MP eqV2-L-OMat-MP 8.52 7.99 23.86 22.63 1.3 1.28 0.764 0.777 Matbench-Discovery metrics for full and 10k most stable splits We report the performance of our compliant EquiformerV2 models on the full WBM test set and the 10k materials predicted by each model to be most stable in Table 11 and Table 12. We report the performance of our non-compilant EquiformerV2 models on the full and 10k most stable splits in Table 13 and Table 13. Notably the best modelsthose fine-tuned jointly with MPtrj and sAlexperform worse on the 10,000 materials predicted to be most stable. We hypothesize that this is result of undertraining and/or the sAlex data overwhelming that in MPtrj. Table 11 Matbench-Discovery benchmark results of compliant models trained only on MPTraj with results on all of the WBM test set. Mean absolute error (MAE) and Root mean squared error (RMSE) are in units of eV/atom. Model eqV2-L-DeNS eqV2-M-DeNS eqV2-S-DeNS eqV2-S F1 DAF Precision Recall Accuracy TPR FPR TNR FNR MAE RMSE R2 0.806 4.497 0.772 0.844 0.931 0.844 0.052 0.948 0. 34 81 0.798 0.798 4.362 0.748 0.855 0.927 0.855 0.059 0.941 0.145 35 84 0.785 0.758 4.053 0.696 0.833 0.912 0.833 0.076 0.924 0. 41 85 0.777 0.8 4.414 0.757 0.847 0.929 0.847 0.056 0.944 0.153 34 81 0.8 18 Table 12 Matbench-Discovery benchmark results of compliant models trained only on MPTraj with results on the 10K materials predicted to be most stable. Mean absolute error (MAE) and Root mean squared error (RMSE) are in units of eV/atom. Model eqV2-L-DeNS eqV2-M-DeNS eqV2-S-DeNS eqV2-S F1 DAF Precision Accuracy MAE RMSE R2 0.985 6.347 0.97 0.97 30 91 0.821 0.984 6.33 0.968 0.968 28 79 0.865 0.983 6.326 0.967 0.967 31 91 0.823 0.974 6.21 0.949 0.949 37 94 0.812 Table 13 Matbench-Discovery benchmark results of non-compliant models on all of the WBM test set. Mean absolute error (MAE) and Root mean squared error (RMSE) are in units of eV/atom. eqV2-L eqV2-S Model Pre-train Dataset OC20 OC20 Fine-tune Dataset MPtrj-sAlex MPtrj MPtrj-sAlex MPtrj MPtrj MPtrj eqV2-M OMat eqV2-M OMat eqV2-S OMat eqV2-S OMat F1 DAF Precision Recall Accuracy TPR FPR TNR FNR MAE RMSE R2 0.895 5.24 0.899 0.892 0.964 0.892 0.021 0.979 0.108 0.02 0.071 0.843 0.887 5.143 0.882 0.892 0.962 0.892 0.025 0.975 0. 0.021 0.071 0.843 0.88 5.106 0.876 0.884 0.959 0.884 0.026 0.974 0.116 0.024 0.079 0.809 0.868 4.942 0.848 0.888 0.954 0.888 0.033 0.967 0. 0.025 0.08 0.804 0.84 4.874 0.836 0.843 0.946 0.843 0.034 0.966 0.157 0.028 0.076 0.819 0.817 4.661 0.8 0.835 0.938 0.835 0.043 0.957 0. 0.032 0.079 0.807 Table 14 Matbench-Discovery benchmark results of non-compliant models on the 10K materials predicted to be most stable. Mean absolute error (MAE) and Root mean squared error (RMSE) are in units of eV/atom. eqV2-L eqV2-S Model Pre-train Dataset OC20 OC20 Fine-tune Dataset MPtrj-sAlex MPtrj MPtrj-sAlex MPtrj MPtrj MPtrj eqV2-M OMat eqV2-M OMat eqV2-S OMat eqV2-S OMat F1 DAF Precision Accuracy MAE RMSE R2 0.987 6.368 0.974 0.974 17 72 0.887 0.987 6.37 0.974 0.974 17 71 0.889 0.99 6.413 0.98 0.98 16 61 0.917 0.991 6.424 0.982 0.982 17 63 0. 0.989 6.399 0.978 0.978 26 95 0.806 0.987 6.368 0.974 0.974 28 91 0."
        }
    ],
    "affiliations": [
        "Fundamental AI Research (FAIR) at Meta"
    ]
}