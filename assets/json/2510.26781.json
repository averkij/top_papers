{
    "paper_title": "ChartAB: A Benchmark for Chart Grounding & Dense Alignment",
    "authors": [
        "Aniruddh Bansal",
        "Davit Soselia",
        "Dang Nguyen",
        "Tianyi Zhou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Charts play an important role in visualization, reasoning, data analysis, and the exchange of ideas among humans. However, existing vision-language models (VLMs) still lack accurate perception of details and struggle to extract fine-grained structures from charts. Such limitations in chart grounding also hinder their ability to compare multiple charts and reason over them. In this paper, we introduce a novel \"ChartAlign Benchmark (ChartAB)\" to provide a comprehensive evaluation of VLMs in chart grounding tasks, i.e., extracting tabular data, localizing visualization elements, and recognizing various attributes from charts of diverse types and complexities. We design a JSON template to facilitate the calculation of evaluation metrics specifically tailored for each grounding task. By incorporating a novel two-stage inference workflow, the benchmark can further evaluate VLMs' capability to align and compare elements/attributes across two charts. Our analysis of evaluations on several recent VLMs reveals new insights into their perception biases, weaknesses, robustness, and hallucinations in chart understanding. These findings highlight the fine-grained discrepancies among VLMs in chart understanding tasks and point to specific skills that need to be strengthened in current models."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 3 ] . [ 1 1 8 7 6 2 . 0 1 5 2 : r a"
        },
        {
            "title": "Preprint",
            "content": "ChartAB: BENCHMARK FOR CHART GROUNDING & DENSE ALIGNMENT Aniruddh Bansal, Davit Soselia, Dang Nguyen, Tianyi Zhou University of Maryland, College Park {ani01, dsoselia, dangmn}@umd.edu Project: https://github.com/tianyi-lab/ChartAlignBench"
        },
        {
            "title": "ABSTRACT",
            "content": "Charts play an important role in visualization, reasoning, data analysis, and the exchange of ideas among humans. However, existing vision-language models (VLMs) still lack accurate perception of details and struggle to extract fine-grained structures from charts. Such limitations in chart grounding also hinder their ability to compare multiple charts and reason over them. In this paper, we introduce novel ChartAlign Benchmark (ChartAB) to provide comprehensive evaluation of VLMs in chart grounding tasks, i.e., extracting tabular data, localizing visualization elements, and recognizing various attributes from charts of diverse types and complexities. We design JSON template to facilitate the calculation of evaluation metrics specifically tailored for each grounding task. By incorporating novel twostage inference workflow, the benchmark can further evaluate VLMs capability to align and compare elements/attributes across two charts. Our analysis of evaluations on several recent VLMs reveals new insights into their perception biases, weaknesses, robustness, and hallucinations in chart understanding. These findings highlight the fine-grained discrepancies among VLMs in chart understanding tasks and point to specific skills that need to be strengthened in current models."
        },
        {
            "title": "INTRODUCTION",
            "content": "Recent large multimodal models (LMMs), such as vision-language models (VLMs), have achieved remarkable breakthroughs in aligning the visual modality with language models, enabling challenging language-level reasoning on visual input signals and opening the door to wide range of applications that naturally rely on interactions between the two modalities (Alayrac et al., 2022; Li et al., 2023; Liu et al., 2023b). One critical class of applications is chart understanding and reasoning, which has broad use in finance, data science, mass media, biology, and other scientific domains where ideas and information are communicated through visualizations. In these applications, measuring numerical values in charts, comparing visual elements (e.g., bars or curves), mapping correspondences between colors, numbers, names, or markers, and recognizing attributes are essential skills for downstream tasks. Most of these tasks require accurate grounding of the structured details in charts. Moreover, dense alignment of elements across multiple charts is also widely needed skill in practical scenarios. These challenges present new open problems for VLMs. Instead of focusing on charts, existing VLMs have primarily been pretrained and finetuned on natural images and common questions/instructions, which are not fully compatible with chart understanding tasks (Yao et al., 2024; Lauren√ßon et al., 2024). Unlike perceiving objects shapes, poses, and semantic meanings in natural images, accurate measurement and comparison of geometric/graphic components, understanding of their structure and layout, and manipulation of their positions and rich textual content are more critical for perception and reasoning with chart images. However, it remains challenging for VLMs to acquire these capabilities, often leading to hallucinations and misinterpretations in chart-centric tasks (Masry et al., 2022; Xia et al., 2024). Despite the recent growing interest in chart-related tasks, existing VLMs and benchmarks specifically designed for charts usually focus on simple QA tasks (Masry et al., 2022; 2025; Wang et al., 2024b; Li & Tajbakhsh, 2023), which cannot comprehensively assess the capabilities of VLMs in grounding and understanding chart components for more general-purpose tasks. Moreover, the alignment of"
        },
        {
            "title": "Preprint",
            "content": "layouts and components across multiple charts has not been explored in previous work. Hence, there remains lack of benchmarks dedicated to evaluating these critical skills. In this paper, we take the first step toward systematically evaluating and analyzing general-purpose VLMs on chart grounding and multi-chart dense alignment. We formally categorize the information to be grounded in chart into two dimensions: (1) data, and (2) attributes (e.g., colors, styles, legends, sizes, positions) that define the visualization design, components, and layout. We define the chart grounding task as extracting both the underlying data table and the associated attributes from chart image, and the dense alignment task as identifying correspondences and differences between two charts. Together, these tasks represent fundamental capabilities and critical subroutines required for wide range of chart-centric applications. To this end, we develop comprehensive benchmark using pairs of similar charts to evaluate model performance on the two tasks with respect to each type of information in the two categories. To create pair of similar charts, we perturb an existing chart by randomly modifying (1) one or few data cells in the data table and/or (2) an attribute in the script used to generate the original chart. To maximize the potential of VLMs and evaluate their full capabilities, we propose multi-stage information extraction and query pipeline. In this pipeline, VLMs are first queried with grounding task targeting specified information in each chart, followed by comparison of the grounding results between the two charts. The pipeline leverages structured JSON templates to guide the grounding and alignment of different types of information. In addition, we introduce several novel evaluation metrics that account for the symmetry and ambiguity inherent in various types of information, thereby enabling more reliable quantitative comparisons across different VLMs. Our analysis reveals the weaknesses of existing VLMs in chart perception and understanding for dense grounding and alignment. The observed errors highlight their biases and hallucinations regarding certain chart components, offering critical insights for improving VLMs. The evaluation results further show how differences across models, chart types, and queried data/attributes influence benchmarking performance. In addition, we assess the robustness of VLMs in data grounding and alignment under different attribute variations, such as changes in chart type or color schemes. Our contributions and novelties are summarized as follows: We introduce the first comprehensive benchmark, ChartAB to systematically evaluate VLMs capabilities in dense grounding and alignment of data and attributes in multiple chart images. We propose holistic evaluation suite, including multi-stage pipeline converting charts into JSON files with specific templates for data/attributes grounding, and rating scheme of the grounding/alignment performance based on VLMs answers. Our evaluation and analysis of existing VLMs reveal their weaknesses in fine-grained chart understanding, highlight hallucinations, and expose biases in their vision encoders when perceiving critical chart features and structures. We evaluate VLMs robustness on data grounding and alignment under perturbations of attributes. It provides novel insights for the design of high-quality charts."
        },
        {
            "title": "2 RELATED WORK",
            "content": "VLMs for Charts. Vision-language models have shown significant advancements in chart understanding tasks. They can be broadly classified into (1) general-purpose multimodal models and (2) chart-specialized models. General-purpose models include proprietary ones (Hurst et al., 2024) and open-source ones (Abdin et al., 2024; Chen et al., 2024; Liu et al., 2023a; Bai et al., 2025). Chart-specialized models (Zhang et al., 2024b; Masry et al., 2024; Xia et al., 2024; Meng et al., 2024) demonstrate strong performance on chart benchmarks; however, they are limited by instruction tuning on specific tasks, which restricts dense-level understanding, and are further hindered by incompatible pipelines that often rely on predefined routines to handle task requirements. Chart Understanding Benchmarks. Current chart benchmarks evaluate VLMs on specific tasks including question answering (Methani et al., 2020; Masry et al., 2022), summarization (Kantharaj et al., 2022b), explanation-generation (Kantharaj et al., 2022a). Multi-task benchmarks including ChartLlama Han et al. (2023), ChartX Xia et al. (2024) perform agglomeration of various modalities"
        },
        {
            "title": "Preprint",
            "content": "(like chart data, description, summary) for the downstream tasks. Recent works specifically focus on expanding QA scope to overcome increased saturation by VLMs, for example CharXiv Wang et al. (2024b) focuses on charts in research papers, SciGraphQA Li & Tajbakhsh (2023) evaluates multiturn QA, MultiChartQA Zhu et al. (2024) evaluates multi-hop reasoning on multiple related charts, ChartQAPro Masry et al. (2025) includes diverse visualizations such as dashboards, infographs, and flexible questions (hypothetical, unanswerable). Visual Grounding. The dense-level understanding abilities of VLMs have been extensively enhanced through visual grounding. DePlot Liu et al. (2022) trained transformer for image-to-CSV generation, introducing novel table comparison method for evaluation. StructChart Xia et al. (2023) proposed module-based augmentation for efficient grounding of chart data and plot code in downstream applications. Beyond charts, the Grounded-SAM model (Ren et al., 2024) leverages Grounding-DINO (Liu et al., 2024) for improved dense-level open-set object tracking. BLIP-2 Li et al. (2023) has been widely integrated into VLMs for VQA-related tasks. LLaVA-Grounded Zhang et al. (2024a) enables detailed text descriptions of multi-object natural images by leveraging imagetext grounding for instruction tuning. Multi-Image Reasoning. Multiple benchmarks have been developed to evaluate VLMs on multiimage reasoning. MMMU Yue et al. (2024) includes interleaved examples with multiple images from medical, cartoon, art, and technical domains. MUIRBench Wang et al. (2024a) focuses on multi-chart diagram QA but is limited to coarse-level understanding. MMIR Zhao et al. (2024) addresses chart understanding through cross-modal alignment, i.e., plotting-code correctness relative to the chart image. MileBench Song et al. (2024) introduces semantic understanding tasks involving text-rich images, emphasizing text extraction and comprehension in OCR, documents, and slides. 3 ChartAB: CHART GROUNDING AND ALIGNMENT BENCHMARK Figure 1: Examples of paired charts for ChartAB tasks. ChartAB evaluates dense grounding and alignment capabilities of VLMs on chart images. (1) Paired charts in each Data Grounding & Alignment task differ in few visualized data values. (2) Paired charts in each Attribute Grounding & Alignment task differ in visualization attribute, e.g., color, legend position, or text style. (3) Each Robustness task contains multiple variants of the same chart-pair for Data Alignment, with different attributes (e.g., colors) across the variants."
        },
        {
            "title": "Preprint",
            "content": "We introduce ChartAB, the first benchmark designed to evaluate vision-language models (VLMs) on dense level chart understanding. The benchmark focuses on three core capabilities essential to chart reasoning: (1) grounding: extracting structured information from single chart image, (2) alignment: identifying fine-grained differences between pair of similar charts, and (3) robustness: assessing the stability of alignment performance under variations in chart appearance. These capabilities serve as cornerstones for wide range of downstream applications. We develop novel two-stage pipeline that can isolate and rigorously evaluate them. Thereby, ChartAB offers deeper diagnostic suite of VLMs perceptual accuracy, reasoning limits, and alignment behavior in structured visual domains."
        },
        {
            "title": "3.1 DATASET TAXONOMY AND CONSTRUCTION",
            "content": "Table 1: Task Taxonomy in ChartAB, which is composed of three types of tasks defined on different data cells and attributes. We construct ChartAB from ChartX Xia et al. (2024) as the source dataset. It encompasses diverse chart types from various domains, including commerce, industry, lifestyle, society, and culture, and provides both CSV data and plotting code for each chart. We list the taxonomy of ChartAB in Table 1. For each chart, we extract dense annotations of two types of fine-grained information: (1) Data: The underlying data table that the chart visualizes. (2) Attributes: The visual attributes that defines the appearance of the chart, e.g., color, legend, and text Style. In particular, color refers to the colors of the visual elements as bars, lines, or boxes in charts. Legend refers to the position of the chart legend. Text Style captures the textual characteristics in four chart regions: title, legend, axis labels, and axis ticks. These characteristics include textual size, weight (lightness/boldness), and font family (e.g., Times New Roman). Text Style Size Weight Font Family Grounding Alignment Robustness 1-Cell 2-Cell 3-Cell Color Legend Attributes Task Type Data Section 3.2 introduces three types of tasks built upon the dense annotations. Grounding tasks aim to extract these dense labels, while robustness tasks evaluate grounding performance under perturbations of attributes. Alignment tasks introduced aim to identify the differences between two similar charts. To create pairs of similar charts, we draw an image from the ChartX, apply controlled modifications in the plotting code, and execute the code to render an variant of the original chart. Each charts source data (CSV file) and plotting script are provided in ChartX, ensuring precise ground-truths. Figure 1 provides several examples of different tasks, while Figure 2 reports the statistics of these tasks. ChartAB covers nine diverse chart types with different data and attribute perturbations: (1) simple charts: bar chart, bar-numbered chart, line chart, and line-numbered chart; (2) complex charts: 3D chart, box chart, radar chart, rose chart, and multi-axes chart. More details about chart data curation are provided in A.3. 3.2 EVALUATION TASKS Figure 2: Statistics of ChartAB. ChartAB includes 9,000 paired chart images curated for tasks below: (1) Paired charts for Data Grounding & Alignment differ in one to three data cells; (2) Paired charts for Attribute Grounding & Alignment differ in color, legend position, or text style; (3) Robustness task includes multiple pairs that share identical differences in data but differ in certain attributes. Grounding of Single Charts Dense grounding of chart elements requires the extraction of precise semantic information from chart images. However, general-purpose VLMs are trained to mainly focus on global visual features or major objects in scenes. When applied to charts, they often fall short of perceiving the details (Xu et al., 2023), which are crucial for chart reasoning. Prior works primarily evaluate VLMs chart understanding capabilities via QA tasks, which do not fully capture their semantic grounding or reflect their cross-modal inconsistencies (Huang et al., 2024). To ensure"
        },
        {
            "title": "Preprint",
            "content": "interpretable and compositional reasoning, we need to examine whether VLMs can ground the chart information in textual form. We formalize Grounding as the conversion of chart image into structured textual representation of data or attributes. As shown in Table 1, we assess this capability through the following tasks: (1) Data Grounding, (2) Color Grounding, (3) Legend Grounding, (4) Text Style Grounding (subtasks: Size, Weight, Font Family). Data Grounding requires the VLM to generate standard CSV representation of the data table. We provide JSON template for tasks requiring Attribute Grounding (Color/Legend/Text Style) and prompt the model to generate JSON representation. Grounding the chart image into textual form isolates the models perceptual ability from downstream prompt variation or instruction complexity. This helps build foundation for the subsequent dense alignment and QA tasks, while also enabling failure analysis of VLM in perceiving chart components. Dense Alignment between Two Charts While single chart grounding evaluates models perception of details in given chart, multi-chart reasoning in practice often requires comparing similar charts to detect and analyze the differences among them. To evaluate this capability, we define dense alignment task where the model identifies fine-grained discrepancies between two charts. Crucially, this task builds on grounded representations, allowing us to isolate and evaluate comparative reasoning for given chart pairs. As shown in our ablation studies (A.6.2), direct alignment without grounding yields significantly weaker performance, highlighting the necessity of grounding for subsequent dense alignment. We formalize Dense Alignment as comparison of two chart images that differ in local details of data or attributes. As shown in Table 1, we assess this capability via the following tasks: (1) Data Alignment, (2) Color Alignment, (3) Legend Alignment, (4) Text Style Alignment. Data Alignment task is further divided into subtasks: 1-cell, 2-cell, and 3-cell, which perform dense alignment of data for chart images that differ in 1, 2, and 3 data points, respectively. Each alignment task challenges the model to identify the set of divergent content and produce structured JSON listing these differences. Robustness of Data Alignment to Attribute Variation Using VLMs for real-world understanding of charts requires analyzing charts in diverse visual forms, i.e., diverse attributes (color/text style/legends) presence for similar types of data, often due to differing plotting tools. Moreover, past work shows the sensitivity of VLMs chart understanding under attribute changes (Guo et al., 2024). Hence, it motivates the evaluation of VLMs chart understanding consistency across noise, style shifts, and design variations due to variations in attributes. We thus formalize Robustness of Data Alignment to variation in Attributes (Color/Legend/Text Style). To perform the task, each instance contains five pairs of chart variants created from the same pair of charts. Each pair visualizes the same source data and maintains identical data differences as the other four pairs, but their attributes (e.g., color of bars) vary across the five pairs. Effects of Dense Grounding & Alignment on Downstream QA Tasks Practical applications of VLMs on chart-related tasks often require complex reasoning, in which dense grounding & alignment usually serve as foundational building blocks and the cornerstone of various downstream tasks. On the other hand, grounding/alignment errors are common reasons for many reasoning failures of VLMs on charts. To demonstrate the importance of dense grounding/alignment skills, we evaluate VLMs on QA tasks, the most widely applied category of downstream tasks, and investigate the correlation between QA performance and the grounding/alignment quality scores. To this end, our study is conducted on QA tasks from ChartX (Xia et al., 2024) that have single-word answers derived from the grounded CSV tables. 3.3 TWO-STAGE EVALUATION PIPELINE We propose two-stage evaluation pipeline inspired by the multi-step approach of SOTA reasoning models, for example, color alignment by o4-mini OpenAI (2025) in Figure 3. The models reasoning takes two steps: grounding the box colors in each chart, followed by dense alignment (comparison) of their grounded colors. This two-stage strategy performs complex, finer-level reasoning by groundthen-compare subtasks with efficient element-wise comparisons. It thus mitigates hallucinations and outperforms the one-stage strategy of GPT-4o, validating the importance of dense grounding for other tasks."
        },
        {
            "title": "Preprint",
            "content": "In our evaluation pipeline, the prompt in each stage consists of natural language instructions with task-specific JSON template defining the output format. This enables better inswtruction following and flexible output parsing and evaluation. As shown in Figure 4, The first-stage performs grounding of data or certain attributes in the given charts. Such wellformatted element-wise representation facilitates subsequent dense alignment and QA tasks. The second-stage compares the grounding results of the two charts from the first stage and produces JSON file to list the dense alignment results. Figure 3: Two-stage color alignment by o4-mini. The o4-mini model automatically decomposes the task into grounding step for the colors in each chart, followed by an output prediction of the alignment. This two-stage reasoning yields more accurate result than GPT-4o, which performs alignment directly without intermediate grounding. The second stage is critical to evaluating end-to-end alignment as it requires VLMs to perform semantic comparison over grounded outputs, beyond surface-level extraction. Compared to one-stage approaches, it mitigates grounding ambiguities and collects additional context, offering more human-like assessment of alignment ability. More details of the pipeline are discussed in A.4. 3.4 EVALUATION METRICS Dense Grounding performance is evaluated by the precision of the detected semantic elements in given chart, e.g., values of visualized data, color of bars, legend position, font size. In the experiments, we report (1) Legend position groundings confusion matrix in Figure 8; (2) Text-style grounding accuracy in Figure 6; (3) Color groundings L2 error of RGB values in Figure 7; and (4) Data grounding performance in Figure 9b is evaluated by the precision of predicted CSV using the SCRM metric introduced in StructChart (Xia et al., 2023). Dense Alignment performance is evaluated across four task categories: data alignment (subtasks: 1-cell/2-cell/3-cell), color alignment, text style alignment, and legend alignment. For each chart pair, the model is prompted to output JSON file that lists the differences on possible attributes and their own values. The performance on the first three tasks is evaluated by key-value alignment score, which assess the capability to identify the different elements (keys) between two charts and their associated values. In contrast, legend alignment score mainly focuses on comparing the different spatial positions of legends in two charts (values only) because the key (i.e., the position) is unique and fixed. More details of the keys and values are provided in Table 2, while the concrete definitions of the metrics are introduced in A.5.1. Robustness of data alignment to the variations of different visualization attributes, e.g., colors, legend positions, text style, is evaluated by the standard deviation of data alignment scores over multiple variations of the original chart pairs. We evaluate the robustness score under the variation of each attribute, and report the averaged scores over chart pairs. More details of the robustness score are provided in A.5.2. Grounding/Alignment affects QA Performance To further analyze the impact of grounding/alignment quality on downstream QA tasks, we evaluate QA accuracy by following the protocols in ChartX (Xia et al., 2024): string-based answers require an exact match, while numerical values are considered correct if they fall within 5% error margin; and investigate its correlation with the grounding/alignment performance. To this end, we adopt two-stage QA that firstly extracts CSV (table) file from chart (data grounding), and then answers the question given the grounding result."
        },
        {
            "title": "Preprint",
            "content": "Figure 4: Two-Stage Evaluation Pipeline for Data Grounding & Alignment in ChartAB. The first stage focuses on grounding the data visualized by each chart in CSV table, while the second stage focuses on alignment, which aims to allocate the difference between the two tables and output JSON file listing the different cells. The other two categories of tasks in ChartAB also adopt similar multi-stage pipelines, detailed in Figures 15, 16, 17 of the Appendix. We analyze how this two-stage QAs accuracy and its difference to the ordinary one-stage QAs accuracy vary with grounding/alignment quality, which results are reported in Figure 9."
        },
        {
            "title": "4 EXPERIMENTS & ANALYSIS",
            "content": "We evaluated GPT-4o (Hurst et al., 2024) and four open-source VLM families: Phi-3.5 vision-instruct (Abdin et al., 2024), InternVL-2.5 (Chen et al., 2024), LLaVA-1.6 (Liu et al., 2023a), QWEN-2.5 VL (Bai et al., 2025). We also evaluated chart-specialized VLMs, including TinyChart (Zhang et al., 2024b) and ChartGemmap Masry et al. (2024). However, as discussed in Section 2, their task-specific training leads to collapse of general instruction following capabilities and fails to output the JSON format required by evaluation. Further discussion and ablation study are provided in A.6.1 and A.6.2. Finding 1 VLMs dense grounding and alignment of data/color are not satisfying on complex charts."
        },
        {
            "title": "Preprint",
            "content": "Compared to simpler and more common charts, e.g., bar/line charts and numbered bar/line charts, dense grounding/alignment on complex charts such as 3D/box/radar/rose/multi-axes charts with more components and irregular layouts is more challenging to most VLMs. Despite the similar alignment performance for legend (Figure 12a) and text-style (Figure 12b) between simple vs. complex charts, the color and data alignment (Figure 5) on complex charts are much poorer than those on simple charts. The color grounding requires identifying each constituents visual encoding and corresponding color, while the data grounding needs to find the mapping from visual encoding to numeric values. Hence, complex layouts with more components make these tasks more difficult. In contrast, identifying the position of legends and text styles (which both have limited options) is easier and less affected by the chart complexity. Figure 5: Left: Comparing VLMs on Data Alignment tasks on paried charts with one-cell difference. Llava-1.6 performs worse than most other VLMs, while QWEN-2.5-VL outperforms GPT4o on most chart types. Right: Color alignment on fine-grained visual elements (e.g., bars, lines, sectors) between two charts. Most VLMs perform better on simpler and more common charts, e.g., line/bar charts. Related discussion beneath Finding 1. Finding 2 VLMs text-style grounding and alignment performance is poor in general, and it varies across text size, weight, and font family. Figure 6 shows that most VLMs fail to detect the correct text size and font family, suffering from <20% accuracy (except GPT-4os performance on font family grounding). These indicate lack of knowledge on these two text attributes. VLMs performance on text weight ((light/normal/bold)) is much better (60%) and close to each other, but still not satisfying. Although LLMs can select reasonable text sizes in code generation for plots, they tend to rely on the default sizes in their priors or relative sizes to other chart components. They still lack sufficient capability to identify exact text sizes in chart images. Finding VLMs weak color recognition ability. As shown in Figure 7, all models color grounding error (L2 distance in RGB space) has median exceeding 50. This implies their inability to understand color shades beyond common ones, e.g., red, blue, green, etc., which exposes their weaknesses in color recognition. The lack of color understanding affects the perception of detailed differences in charts and leads to misalignment in color-conditioned reasoning tasks. Consequently, the VLMs performance in color alignment tasks (Figure 5) is consistent with that on color grounding. These results suggest to improve the color understanding capability by adding more color-sensitive data to VLM training. 8 Figure 6: Text-style grounding on size, weight, and font family. The low accuracy of most VLMs highlights the lack of style knowledge (Finding 4). Figure 7: Color groundings L2 error in the RGB space, which median over VLMs >50 implies their weaknesses in color recognition (Finding 3)."
        },
        {
            "title": "Preprint",
            "content": "Figure 8: Confusion matrix of legend position grounding. The dark non-diagonal entries show the fail patterns and imply the biases of incorrectly identifying position-i as position-j. Phi-3.5 exhibits severe bias towards the upper-left position while GPT-4o shows the minimal bias. More discussion is provided below Finding 2. Finding 4 Spatial reasoning bias: Most VLMs suffer from biases when allocating the position of legends. The grounding of the legends position (Figure 8) suffers from strong bias of pretrained VLMs. The Phi-3.5 model shows the strongest prior towards the upper-left position. The 7-8B scale VLMs, e.g., LlaVa-1.6, Inten-VL-2.5, QWEN-2.5-VL, all show similar level of bias but towards the upper-right position instead. The GPT-4o model exhibits the minimal bias among all evaluated VLMs. The grounding bias strongly affects the legend alignment (Figure 12a) where Phi-3.5 performs the worst, GPT-4o has the best performance, while the other 3 models performance is between them. (a) Data Alignment correlates with QA performance. (b) Data Groundings impact on QA Performance. Figure 9: (a) shows that the failed (successful) QA tasks decrease (increase) with the data alignment score, underscoring the importance of data alignment capability of VLMs on downstream chart reasoning tasks. (b) shows that precise (poor) data grounding leads to positive (negative) gain on QA tasks, indicating the importance of data grounding on downstram tasks. More discussion can be found beneath Finding 6. Finding 5 Poor (precise) grounding and alignment degrade (improve) downstream QA performance. Figure 9b demonstrates that precise (poor) grounding of chart-visualized data boosts (degrades) QA performance. It validates grounding as gateway to extract structured data from charts for reliable downstream reasoning. Notably, the greatest gains are achieved on simple chart types (bar/line charts and numbered bar/line charts) due to better numeric understanding of these charts visualized data, as discussed in Finding 1. Figure 9a shows steady rise of QA accuracy (predicted) with the data alignment score, demonstrating the importance of dense chart understanding to QA reasoning. These findings position grounding and alignment as essential prerequisites for chart reasoning."
        },
        {
            "title": "Preprint",
            "content": "Finding 6 VLMs follow the scaling law on most dense alignment tasks. As shown in Figure 10, we observed consistent scaling law across most dense alignment subtasks, except for Text-Style Alignment. The deviation arises from the relatively greater complexity of the JSON template in this task, which led to significantly higher number of failures where InternVL-2.5 produced incorrect JSON formats."
        },
        {
            "title": "5 CONCLUSION",
            "content": "We introduce ChartAB, the first benchmark for fine-grained chart grounding and multi-chart dense alignment in visionlanguage models (VLMs). Our evaluations across diverse chart types reveal persistent challenges, including perFigure 10: Alignment performance of VLMs with ceptual bias, weak attribute understanding, and different sizes from the InternVL-2.5 family. Relimited spatial reasoning especially on complex sults of other VLMs are reported in Appendix 11. visual representations. Experiments with our novel two-stage pipeline show effectiveness of intermediate grounding in improving dense alignment, and the impact of grounding and alignment accuracy for enhance downstream question answering, establishing these capabilities as essential foundations for robust chart understanding."
        },
        {
            "title": "REFERENCES",
            "content": "Marah Abdin, Jyoti Aneja, Hany Awadalla, Ahmed Awadallah, Ammar Ahmad Awan, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Jianmin Bao, Harkirat Behl, et al. Phi-3 technical report: highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219, 2024. Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob L. Menick, Sebastian Borgeaud, Andy Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and Kar√©n Simonyan. Flamingo: visual language model for few-shot learning. In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh (eds.), Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022. URL http://papers.nips.cc/paper_files/ paper/2022/hash/960a172bc7fbf0177ccccbb411a7d800-Abstract-Conference.html. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271, 2024. William Cohen, Pradeep Ravikumar, Stephen Fienberg, et al. comparison of string distance metrics for name-matching tasks. In IIWeb, volume 3, pp. 7378, 2003. Grace Guo, Jenna Jiayi Kang, Raj Sanjay Shah, Hanspeter Pfister, and Sashank Varma. Understanding graphical perception in data visualization through zero-shot prompting of vision-language models. arXiv preprint arXiv:2411.00257, 2024. Yucheng Han, Chi Zhang, Xin Chen, Xu Yang, Zhibin Wang, Gang Yu, Bin Fu, and Hanwang Zhang. Chartllama: multimodal llm for chart understanding and generation. arXiv preprint arXiv:2311.16483, 2023. Wen Huang, Hongbin Liu, Minxin Guo, and Neil Zhenqiang Gong. Visual hallucinations of multimodal large language models. arXiv preprint arXiv:2402.14683, 2024."
        },
        {
            "title": "Preprint",
            "content": "Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. Shankar Kantharaj, Xuan Long Do, Rixie Tiffany Ko Leong, Jia Qing Tan, Enamul Hoque, and Shafiq Joty. Opencqa: Open-ended question answering with charts. arXiv preprint arXiv:2210.06628, 2022a. Shankar Kantharaj, Rixie Tiffany Ko Leong, Xiang Lin, Ahmed Masry, Megh Thakkar, Enamul Hoque, and Shafiq Joty. Chart-to-text: large-scale benchmark for chart summarization. arXiv preprint arXiv:2203.06486, 2022b. Hugo Lauren√ßon, L√©o Tronchon, Matthieu Cord, and Victor Sanh. What matters when building vision-language models?, 2024. URL https://arxiv.org/abs/2405.02246. Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International conference on machine learning, pp. 1973019742. PMLR, 2023. Shengzhi Li and Nima Tajbakhsh. Scigraphqa: large-scale synthetic multi-turn question-answering dataset for scientific graphs. arXiv preprint arXiv:2308.03349, 2023. Fangyu Liu, Julian Martin Eisenschlos, Francesco Piccinno, Syrine Krichene, Chenxi Pang, Kenton Lee, Mandar Joshi, Wenhu Chen, Nigel Collier, and Yasemin Altun. Deplot: One-shot visual language reasoning by plot-to-table translation. arXiv preprint arXiv:2212.10505, 2022. Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning, 2023a. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine (eds.), Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023b. URL http://papers.nips.cc/paper_files/paper/2023/hash/ 6dcf277ea32ce3288914faf369fe6de0-Abstract-Conference.html. Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Qing Jiang, Chunyuan Li, Jianwei Yang, Hang Su, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. In European Conference on Computer Vision, pp. 3855. Springer, 2024. Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartqa: benchmark for question answering about charts with visual and logical reasoning. arXiv preprint arXiv:2203.10244, 2022. Ahmed Masry, Megh Thakkar, Aayush Bajaj, Aaryaman Kartha, Enamul Hoque, and Shafiq Joty. Chartgemma: Visual instruction-tuning for chart reasoning in the wild. arXiv preprint arXiv:2407.04172, 2024. Ahmed Masry, Mohammed Saidul Islam, Mahir Ahmed, Aayush Bajaj, Firoz Kabir, Aaryaman Kartha, Md Tahmid Rahman Laskar, Mizanur Rahman, Shadikur Rahman, Mehrad Shahmohammadi, et al. Chartqapro: more diverse and challenging benchmark for chart question answering. arXiv preprint arXiv:2504.05506, 2025. Fanqing Meng, Wenqi Shao, Quanfeng Lu, Peng Gao, Kaipeng Zhang, Yu Qiao, and Ping Luo. Chartassisstant: universal chart multimodal language model via chart-to-table pre-training and multitask instruction tuning. arXiv preprint arXiv:2401.02384, 2024. Nitesh Methani, Pritha Ganguly, Mitesh Khapra, and Pratyush Kumar. Plotqa: Reasoning over scientific plots. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 15271536, 2020. OpenAI. Openai o3 and o4-mini system card. Technical report, April 2025. System card covering multimodal and reasoning capabilities, safety evaluation, tool use, and performance benchmarks."
        },
        {
            "title": "Preprint",
            "content": "Tianhe Ren, Shilong Liu, Ailing Zeng, Jing Lin, Kunchang Li, He Cao, Jiayu Chen, Xinyu Huang, Yukang Chen, Feng Yan, et al. Grounded sam: Assembling open-world models for diverse visual tasks. arXiv preprint arXiv:2401.14159, 2024. Dingjie Song, Shunian Chen, Guiming Hardy Chen, Fei Yu, Xiang Wan, and Benyou Wang. Milebench: Benchmarking mllms in long context. arXiv preprint arXiv:2404.18532, 2024. Fei Wang, Xingyu Fu, James Huang, Zekun Li, Qin Liu, Xiaogeng Liu, Mingyu Derek Ma, Nan Xu, Wenxuan Zhou, Kai Zhang, et al. Muirbench: comprehensive benchmark for robust multi-image understanding. arXiv preprint arXiv:2406.09411, 2024a. Zirui Wang, Mengzhou Xia, Luxi He, Howard Chen, Yitao Liu, Richard Zhu, Kaiqu Liang, Xindi Wu, Haotian Liu, Sadhika Malladi, et al. Charxiv: Charting gaps in realistic chart understanding in multimodal llms. Advances in Neural Information Processing Systems, 37:113569113697, 2024b. Renqiu Xia, Bo Zhang, Haoyang Peng, Hancheng Ye, Xiangchao Yan, Peng Ye, Botian Shi, Yu Qiao, and Junchi Yan. Structchart: Perception, structuring, reasoning for visual chart understanding. arXiv preprint arXiv:2309.11268, 2023. Renqiu Xia, Bo Zhang, Hancheng Ye, Xiangchao Yan, Qi Liu, Hongbin Zhou, Zijun Chen, Peng Ye, Min Dou, Botian Shi, et al. Chartx & chartvlm: versatile benchmark and foundation model for complicated chart reasoning. arXiv preprint arXiv:2402.12185, 2024. Zhengzhuo Xu, Sinan Du, Yiyan Qi, Chengjin Xu, Chun Yuan, and Jian Guo. Chartbench: benchmark for complex visual reasoning in charts. arXiv preprint arXiv:2312.15915, 2023. Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He, Qianyu Chen, Huarong Zhou, Zhensheng Zou, Haoye Zhang, Shengding Hu, Zhi Zheng, Jie Zhou, Jie Cai, Xu Han, Guoyang Zeng, Dahai Li, Zhiyuan Liu, and Maosong Sun. Minicpm-v: gpt-4v level mllm on your phone, 2024. URL https://arxiv.org/abs/ 2408.01800. Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 95569567, 2024. Hao Zhang, Hongyang Li, Feng Li, Tianhe Ren, Xueyan Zou, Shilong Liu, Shijia Huang, Jianfeng Gao, Leizhang, Chunyuan Li, et al. Llava-grounding: Grounded visual chat with large multimodal models. In European Conference on Computer Vision, pp. 1935. Springer, 2024a. Li Zhang, Shuo Zhang, and Krisztian Balog. Table2vec: Neural word and entity embeddings for table population and retrieval. In Proceedings of the 42nd international ACM SIGIR conference on research and development in information retrieval, pp. 10291032, 2019. Liang Zhang, Anwen Hu, Haiyang Xu, Ming Yan, Yichen Xu, Qin Jin, Ji Zhang, and Fei Huang. Tinychart: Efficient chart understanding with visual token merging and program-of-thoughts learning. arXiv preprint arXiv:2404.16635, 2024b. Bingchen Zhao, Yongshuo Zong, Letian Zhang, and Timothy Hospedales. Benchmarking multi-image understanding in vision and language models: Perception, knowledge, reasoning, and multi-hop reasoning. arXiv preprint arXiv:2406.12742, 2024. Zifeng Zhu, Mengzhao Jia, Zhihan Zhang, Lang Li, and Meng Jiang. Multichartqa: Benchmarking vision-language models on multi-chart problems. arXiv preprint arXiv:2410.14179, 2024."
        },
        {
            "title": "A APPENDIX",
            "content": "A.1 LLM USAGE STATEMENT LLMs were used in the work as general purpose writing aid (e.g. to polish grammar and phrasing) and to assist with literature search. All substantive research ideation, experiments and analysis has been conducted by the authors. A.2 LIMITATIONS Our work focuses on VLM evaluations and do not assess model fine-tuning. While such approaches might yield stronger results, they diverge from our goal of studying general purpose VLMs for dense level understanding. For dataset construction despite availability of chart datasets with more sophisticated real-world chart examples, we selected the ChartX Xia et al. (2024) dataset because it provides precise chart information in form of csv data and plotting code which is essential for generating precise ground truth values for the evaluation of dense grounding and alignment. A.3 DATASET CONSTRUCTION Algorithm 1: ChartAB Dataset Construction: Data Grounding and Alignment Subset Input: Source dataset DChartX = {(Ti, Si)}N i=1 from ChartX (Xia et al., 2024), where Ti is CSV table and Si is the corresponding plotting script; Number of cells to modify {1, 2, 3}; Scaling range [Œ±min, Œ±max]. ChartAB = {(xi, i=1, where xi, Output: Constructed dataset D(data) are chart images, yg i, yg )}M is the alignment label. , ya is the grounding label, and ya foreach (T, S) DChartX do Parse table to obtain set of all cells = {(r, c, vr,c)}, where and denote cells row label and column label respectively, and vr,c the corresponding cell value; Identify candidate cells with unique values; if < then skip this chart; Sample cells {(ri, ci, vri,ci)}k Sample scaling factors {Œ±i}k Initialize and S; foreach (r, c, vr,c) do i=1 from ; i=1 from scaling range [Œ±min, Œ±max]; Compute modified value if not (unique match of vr,c in S) then r,c = Œ±i ¬µc, where ¬µc is the mean of cells in column c; skip this chart; Replace vr,c with r,c in and ; Execute and to generate chart images and x; if and generation succeed then Create instance (x, x, yg, ya) where yg = (T, ) and ya = {(ri, ci, vri,ci, Append (x, x, yg, ya) to D(data) ChartAB; ri,ci )}k i=1; We used ChartX dataset Xia et al. (2024) as source dataset for our ChartAlignBench curation. ChartX contains plotting-code and csv data-table for the chart with extremely high level of precision thus offering the flexibility for performing finer-level changes along with ground-truth generation capabilities. It contains diverse chart types of varying complexities, and chart data from multiple domains. Hence enabling analysis across charts of varying difficulties. We utilize perturbations for generating fine-grained variations for given chart thus helping build densealignment pairs. Charts plotting-code is perturbed for precise data or attribute changes based on rigorous formatting check using regex-based search and replace, resulting in chart image generation from code execution."
        },
        {
            "title": "Preprint",
            "content": "Algorithm 2: ChartAB Dataset Construction: Attribute Grounding and Alignment Subset Input: Source dataset DChartX = {(Ti, Si)}N i=1 from ChartX (Xia et al., 2024), where Si is the Output: Constructed dataset D(attribute) plotting script; Set of attribute types = {color, legend, text style}. ChartAB = {(xi, i, ati, yg i=1, where xi, i )}M , ya images, ati A: attribute type, yg is the grounding label, ya is the alignment label. are chart foreach (T, S) DChartX do Parse plotting script using regex to detect plot attributes; color_list locate unique color array in S, corresponding to visual encodings (e.g., bars/lines/boxes); legend_position extract position parameter from legend(..., loc=) in S; text_style parse rcParams for size, weight, and font family for regions (title, legend, axes labels, axes ticks); Collect detected attributes {color_list, legend_position, text_style}; if any attribute value is undefined or ambiguous then skip this chart; // Generate modified versions for each attribute type foreach attribute type at do Initialize S, yg , and ya ; if at = color then Sample new color list color_list by randomly replacing subset of colors; Replace color array in with color_list; yg (color_list, color_list); changed_colors {(cold, cnew) cold = cnew}; ya {type: color, changed: changed_colors}; else if at = legend then Sample new legend position legend_position {upper left, upper right, . . . }; Replace loc parameter in with legend_position; yg (legend_position, legend_position); ya {type: legend, changed: legend_position}; else if at = text style then Sample new text style parameters text_style (font size, weight, or family); Update rcParams in with text_style; yg (text_style, text_style); changed_fields {(k, vold, vnew) text_style[k] = text_style[k]}; ya {type: text style, changed: changed_fields}; Execute to generate modified chart image x; if generation succeeds then Create instance (x, x, at, yg, ya); Append (x, x, at, yg, ya) to D(attribute) ChartAB ; The csv availability and attribute information enable accurate ground-truth generation. Generated pairs for data alignment and attribute alignment include randomly assigned changes, and robustness sets include diverse attribute values for meticulous and unbiased evaluation. The algorithmic description for generating chart pairs for Data Grounding & Alignment 1, Attribute Grounding & Alignment 2, Robustness 3 describe the process in detail. A.4 TWO-STAGE EVALUATION PIPELINE DETAILS We utilize natural-language based instructions for zero-shot inference to enable simple execution with minimal task specific nuances for strong generalization across various models. VLM outputs follow JSON based formatting due to precise nature of the key-value structure which is essential for element specific information serialization for finer-analysis, along with flexibility for"
        },
        {
            "title": "Preprint",
            "content": "Algorithm 3: ChartAB Dataset Construction: Robustness Set Generation Input: Source dataset DChartX = {(Ti, Si)}N i=1; Data modification params: k, [Œ±min, Œ±max]; Visual variations per instance: = 5; Attribute types: = {color, legend, text style}. , ati}M are chart images for is the )}d variation j, ati A: attribute type being varied, yg alignment label. is the grounding label, ya ChartAB = {{(x(j) i=1 where x(j) j=1, yg , x(j) , ya (j) Output: D(robust) , i foreach at do foreach (T, S) DChartX do // Apply data modification (Algorithm 1) Parse to extract cells {(r, c, vr,c)}; identify unique-value cells if >= k; Sample cells {(ri, ci)}k Create modified table and script by replacing vri,ci with if any vri,ci has non-unique match in then i=1 from and scaling factors {Œ±i}k skip this chart i=1 from [Œ±min, Œ±max]; = Œ±i ¬µci; ri,ci Set yg (T, ) and ya {(ri, ci, vri,ci , i=1; // Generate base pair and visual variations Execute and to generate base charts x(0) and x(0); if generation fails then skip this chart )}k ri,ci Initialize ; for = 1 to do Sample variation for attribute at (color/legend/text style); Apply to both and to create Sj and j; to generate x(j) and x(j); Execute Sj and if generation succeeds then Add (x(j), x(j)) to if = then Append {{(x(j) , x(j) )}d j=1, yg , ya , ati} to D(robust) ChartAB; variations in completion of grounding and fine grained analysis. The alignment JSON contains finer level attributes for which the charts differ, and the values for corresponding attribute in the two charts. E.g. for data alignment (as shown in Fig. 4) the finer level attributes changed between the charts i.e. cells are identified by their row & column header, along with its values in the chart pairs, i.e. value in chart 1 & value in chart 2 respectively. Evaluation of attribute alignment tasks follow the same pipeline, as illustrated in Figure 15 for color alignment, Figure 16 for text-style alignment, Figure 17 for legend alignment. A.5 EVALUATION METRICS A.5.1 DENSE ALIGNMENT We evaluate dense alignment performance across four task categories: data alignment (subtasks: 1-cell/2-cell/3-cell), color alignment, text style alignment, and legend alignment. Performance on the first three tasks is evaluated by key-value alignment score, which assess the capability to identify the different elements (keys) between two charts and their associated values. In contrast, legend alignment score mainly focuses on identifying the different positions of legends in two charts (values only) because the key is unique and fixed. Table 2 summarizes the keys and values of each type of elements as well as the notations of their dense alignment scores. Key-Value Alignment Score. For data, color, and text style alignment tasks, we define elements as the atomic units that may differ across chart pairs. Each element is characterized by two components:"
        },
        {
            "title": "Preprint",
            "content": "Key: textual identifier that uniquely specifies the element within the chart. Value: The content or attribute value of the element in each chart of the pair. The key serves to locate and identify different elements, while the values capture their corresponding data or content. We define the alignment score salign on chart pair (x, x) as: salign(x, x) = skey + svalue (1) where skey [0, 1] measures the key identification and svalue [0, 1] measures the precision of predicted values. We rescale salign(x, x) to [0, 10] for better interpretability. We will apply superscripts, e.g., s(data) align (x, x), to distinguish different task categorie, as shown in Table 2. Key Identification Score skey evaluates whether the model correctly identifies different elements between two charts. Let Kgt = {k1, . . . , kn} be the set of ground truth keys and Kpred = {ÀÜk1, . . . , ÀÜkm} be the set of predicted keys. We perform key matching between Kgt and Kpred using task-specific criteria: (1) for data and color alignment, we use Levenshtein distance with threshold œÑ = 0.5 to account for the high lexical diversity of real-world named entities (Cohen et al., 2003) and tabular headers (Zhang et al., 2019); (2) for text style alignment, we require exact matches since the keys are predefined and region-characteristic. Let Kvalid = Kpred œÑ Kgt denote the set of valid predicted keys, where œÑ represents the fuzzy intersection operator. We compute the following F1 score as skey: pkey = Kvalid Kpred , rkey = Kvalid Kgt , skey = 2 pkey rkey pkey + rkey (2) Precision of Predicted Values svalue. For each valid predicted element Kvalid, we measure the precision of its predicted values in both charts. Let (vk, k) denote the ground truth and predicted values in charts and respectively. The precision of predicted values is defined as (cid:88) k) and (ÀÜvk, ÀÜv (œÅ(vk, ÀÜvk) + œÅ(v k, ÀÜv k)) (3) svalue = 1 2Kvalid kKvalid where œÅ(, ) [0, 1] is task-specific value matching function: exact match for categorical values (e.g., color hex codes, text styles), and œÅ(v, ÀÜv) = 1 min(v ÀÜv/v, 1) for numerical values (e.g., data points, font sizes). Task Data Alignment Score s(data) align (x, x) Color Alignment s(color) align (x, x) Key Value Row and column labels (e.g., John, Salary) Numerical value (float/int) Series/category label (e.g., Product A) Hex color code (e.g., #FF5733) Text Style Alignment Alignment s(textstyle) align (x, x) Region-characteristic pair (e.g., title-size) Style attribute value (size: int, weight/family: categorical) Legend Alignment s(legend) align (x, x) Position (implicit) 3X3 grid (center, upper, ...) Table 2: Chart elements keys, values, and scores in the four categories of dense alignment tasks. For data, color, and text style alignment, fuzzy matching (Levenshtein distance œÑ = 0.5) or exact matching is used to evaluate the key identification, while the precision of associated values are evaluated using œÅ(, ). Legend alignment score is defined by spatial distance between the values of legend positions. Legend Alignment Score. Unlike the above three alignment tasks, legend alignment only focuses on one unique key, i.e., the legend position, so the legend alignment score is defined as the spatial proximity between the ground truth and model-detected positions. We discretize the chart into 3 3 grid and measure the Manhattan distance between predicted and ground truth legend positions. The legend alignment score is defined by 1 10 (dManhattan(pos, ÀÜpos) + dManhattan(pos, ÀÜpos)) (x, x) = 1 s(legend) align (4)"
        },
        {
            "title": "Preprint",
            "content": "where ÀÜpos and pos are the predicted and ground truth positions, and dManhattan(, ) [0, 5] is the Manhattan distance. We normalize s(legend) (x, x) to [0, 10] for better interpretability. align For each chart type, we report the averaged alignment scores over all the chart pairs belonging to that chart type. A.5.2 ROBUSTNESS We evaluate the robustness of data alignment performance to the variations of visual attributes. For chart pair (x, x) differing in 1-3 data cells, we define robustness r(x, x) as the reciprocal of the standard deviation œÉ() of alignment scores across visual variations: r(x, x) = œÉ (cid:18)(cid:110) 1 s(data) align (x(j), x(j)) (cid:111)d i=1 (cid:19) (5) where (x(j), x(j)), . . . , (x(d), x(d)) are the visually-varied versions of the same chart pair (x, x), and s(data) align denotes the data alignment score. Higher r(x, x) indicates more consistent data alignment performance across different visual variations. We compute robustness separately for each attribute {color, legend, text style}. For each chart type, we report the robustness score averaged over all the chart pairs belonging to that chart type. A.6 ADDITIONAL EXPERIMENTAL DETAILS A.6.1 VLM SELECTION We evaluate diverse suite of open-source VLMs from following families: Phi-3.5 vision-instruct Abdin et al. (2024), InternVL-2.5 (8B) Chen et al. (2024), LLaVA-1.6 Mistral (7B) Liu et al. (2023a), QWEN-2.5 VL (8B) Bai et al. (2025). These models constitute among most widely used VLMs, and have long timeline of continuous evolution with each released version. The set encompasses the top-performed VLMs in various chart benchmarks (CharXiv Wang et al. (2024b), ChartQAPro Masry et al. (2025), SCI-CQA Li & Tajbakhsh (2023), MultiChartQA Zhu et al. (2024), discussed in 2). Our choice of proprietary VLM is based on CharXiv Wang et al. (2024b) leaderboard as its tasks/questions require dense-level grounding. For example, CharXiv tasks need to identify axes ticks by positions and their value enumerartion, grid-lines count and intersections, integral (area comparison of regions) and slope (rate of increase/decrease) in line charts. And GPT-4o Hurst et al. (2024) is the best performing proprietary in the CharXiv paper. Among chart-specialized VLMs, we evaluate TinyChart Zhang et al. (2024b) & ChartGemma Masry et al. (2024) models. However, due to their task-specific training (discussed in 2), these models show collapse of instruction following capabilities and fail to output required JSON format needed for evaluation. Below are few examples of the outputs. JSON output: Data alignment (1 cell) by ChartGemma and TinyChart models using 1-stage stitchedcharts (i.e chart pair stacked as single image) evaluation. REQUIRED FORMAT (specified in prompt instructions):- {\"row name\": <row name of the cell>, \"column name\": <column name of the cell>, \"value in chart 1\": <value in first chart of the pair>, \"value in chart 2\": <value in second chart of the pair>} EXAMPLE:- {\"row name\": \"Production (million units)\", \"column name\": \"2021\", \"value in chart 1\": 35, \"value in chart 2\": 30} CHARTGEMMA OUTPUT (abnormal valued JSON which is inconsistent with required format):- {\"row name\": \"sample row\", \"column name\": \"sample column\", \"value in chart 1\": Infinity, \"value in chart 2\": Infinity} TINYCHART OUTPUT (abnormal list instead of JSON):-"
        },
        {
            "title": "Preprint",
            "content": "[\"Production (million units)\", \"Production (million units)\", \"Production (million units)\" ..... \"Production (million units)\"] A.6.2 ABLATIONS Type Approach Bar Bar # 3D Bar Line Line # Radar Rose Box Multi-Axes 1-stage Multi-chart Stitched-chart 2-stage Ours 5.1 5.8 6. 6.6 6.6 7.5 3.7 4.2 3.3 3.8 4.0 5. 3.7 2.6 6.6 2.6 1.1 2.5 0.7 1.0 2. 2.3 1.6 1.4 2.6 2.2 2.1 Table 3: Ablation study of 1-stage vs. 2-stage evaluations on data alignment (one cell change) task. Mean scores across nine chart types show that our 2-stage evaluation reflects VLMs greatest potential on chart alignment. We performed ablation experiments to vigorously compare differing approaches to our 2-stage approach. The ablation experiments aimed to thoroughly compare single-stage based alignment approaches for performing multi-image reasoning vis-a-vis our two-stage approach. The ablation techniques:- (1) stitched-charts inference: The chart-pair images are vertically concatenated resulting in single image of stitched chart-pairs which undergo single-stage inference. (2) multi-image inference: The VLM inputs multiple images, and contextualizes output based on the input images with aim of better understanding across of finer-level alignment in multi-image reasoning. The ablation experiments analyzed Phi-3.5 models performance on data alignment task. As shown in table. 3, the single-stage approach fared poorly compared to out two-stage approach reaffirming the two-stage approach. Multi-image inference showed the weakest performance. Despite increasing training efforts towards improved VLM training, the models still face issues in reasoning ability on fine-grained tasks. Stitched-charts approach showed better results than multi-image, however they too underperformed vis-a-vis our two-stage approach. The comparatively stronger image selfattention capabilities seem to augment multi-image by utilzing the stitched connection. However the better prevailing capabilities of two-stage approach capture the gain of grounding generation. The VLMs multi-modal understanding though improving still suffers from finer-level nuances missed by information loss in image-encoding and cross-attention mechanisms. A.7 ADDITIONAL FINDING & INSIGHTS Figure 11: Task performances for different sizes of Qwen-2.5-VL and LlaVa-Vicuna-1.6. Finding 7 VLMs data grounding and alignment are more robust to color variations than changes in legend positions and text styles. Fig. 13 shows that robustness is the worst under text-style variations and the best under color variations. In the visualizations of data, colors are used to discretize, categorize, and measure chart"
        },
        {
            "title": "Preprint",
            "content": "(a) Legend Alignment (b) Text-Style Alignment Figure 12: (a) Legend alignment of legend positions. Phi-3.5 performs the worst while GPT-4o is best. Related discussion in Finding 1&2. (b) Text-style alignment (size, weight, font). Worst: QWEN-2.5-VL, Best: GPT-4o. Discussion in Finding 1&4. constituents. As long as their colors are distinguishable, color variations will not affect the data grounding. In contrast, the text styles and legends provide critical information about the data via ticks, labels, and legend items. Moreover, changing legend position may lead to position changes and occlusion of other chart elements. Hence, their variations have greater impact on the data grounding/alignment performance. Figure 13: VLMs Robustness of data alignment (3-cell change) to variations in color, legend, and text-style. VLMs show better robustness to color changes than text-style changes. QWEN-2.5VL outperforms the other four VLMs on robustness. More discussion can be found below Finding 6. Finding VLMs spatial understanding capability affects several important chart understanding skills. Chart understanding usually requires an accurate mapping between spatial relationships and the corresponding numerical values to be visualized."
        },
        {
            "title": "Preprint",
            "content": "(a) Depth estimation in 3D bar charts (b) Text vs. non-text cues for value scaling in rose charts. Figure 14: VLMs spatial understanding is poor on complex charts. More discussion is provided below Finding 7. Depth understanding: Despite the high-level similarity between 3D bar charts and (2D) bar charts, as shown in Fig 5, the data alignment performance is much poorer on 3D bar charts due to the lack of depth understanding, which affects the measurement of scales and values along axes in the 3D space. Text vs non-text cues: Rose charts are extended from bar charts by allowing more polar coordinates with scale differences in radial forms. However, Fig. 14b reveals great difference between the two on data alignment performance. This is due to fewer text cues (e.g., axes ticks) in rose charts, where non-text cues such as grid lines cannot be fully leveraged. Better performance on numbered charts: numbered bar and line charts explicitly place the data values in the charts, hence facilitating VLMs to extract the data easily without precise measurements of the visual elements. Hence, as shown in Fig. 5, numbered bar/line charts usually enjoy better performance."
        },
        {
            "title": "Preprint",
            "content": "Figure 15: Two-Stage Evaluation Pipeline for Color Grounding & Alignment in ChartAB. The first stage focuses on grounding the color for visual encodings in each chart, while the second stage focuses on alignment, which aims to evaluate the colors for visual encodings and output JSON file listing the visual encodings which differ in color values between the chart pair."
        },
        {
            "title": "Preprint",
            "content": "Figure 16: Two-Stage Evaluation Pipeline for Text Style Grounding & Alignment in ChartAB. The first stage focuses on grounding the text characteristics for the four chart regions: title, legend, axes labels, axes ticks. These characteristics are textual size, weight (lightness/boldness), and font family (e.g., Times New Roman). The second stage focuses on alignment, which aims to evaluate the grounded text characteristics and output JSON file listing the characteristics for each region which differ between the chart pair."
        },
        {
            "title": "Preprint",
            "content": "Figure 17: Two-Stage Evaluation Pipeline for Legend Grounding & Alignment in ChartAB. The first stage focuses on grounding the legend position in each chart, while the second stage focuses on alignment, which aims to determine the difference in the position and output the JSON file listing the difference."
        }
    ],
    "affiliations": [
        "University of Maryland, College Park"
    ]
}