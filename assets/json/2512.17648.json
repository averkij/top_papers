{
    "paper_title": "Simulstream: Open-Source Toolkit for Evaluation and Demonstration of Streaming Speech-to-Text Translation Systems",
    "authors": [
        "Marco Gaido",
        "Sara Papi",
        "Mauro Cettolo",
        "Matteo Negri",
        "Luisa Bentivogli"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Streaming Speech-to-Text Translation (StreamST) requires producing translations concurrently with incoming speech, imposing strict latency constraints and demanding models that balance partial-information decision-making with high translation quality. Research efforts on the topic have so far relied on the SimulEval repository, which is no longer maintained and does not support systems that revise their outputs. In addition, it has been designed for simulating the processing of short segments, rather than long-form audio streams, and it does not provide an easy method to showcase systems in a demo. As a solution, we introduce simulstream, the first open-source framework dedicated to unified evaluation and demonstration of StreamST systems. Designed for long-form speech processing, it supports not only incremental decoding approaches, but also re-translation methods, enabling for their comparison within the same framework both in terms of quality and latency. In addition, it also offers an interactive web interface to demo any system built within the tool."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 1 ] . [ 1 8 4 6 7 1 . 2 1 5 2 : r Simulstream: Open-Source Toolkit for Evaluation and Demonstration of Streaming Speech-to-Text Translation Systems Marco Gaido, Sara Papi, Mauro Cettolo, Matteo Negri, Luisa Bentivogli Fondazione Bruno Kessler, Trento, Italy {mgaido, spapi, cettolo, negri, bentivo}@fbk.eu"
        },
        {
            "title": "Abstract",
            "content": "that balance Streaming Translation Speech-to-Text (StreamST) requires producing translations concurrently with incoming speech, imposing latency constraints and demanding strict models partial-information decision-making with high translation quality. Research efforts on the topic have so far relied on the SimulEval repository, which is no longer maintained and does not support systems that revise their outputs. In addition, it has been designed for simulating the processing of short segments, rather than long-form audio streams, and it does not provide an easy method to showcase systems in demo. As solution, we introduce simulstream, the first open-source framework dedicated to unified evaluation and demonstration of StreamST systems. Designed for long-form speech processing, it supports not only incremental decoding approaches, but also re-translation methods, enabling for their comparison within the same framework both in terms of quality and latency. In addition, it also offers an interactive web interface to demo any system built within the tool. github.com/hlt-mt/simulstream"
        },
        {
            "title": "Introduction",
            "content": "The task of Streaming Speech-to-Text Translation (StreamST) requires translating continuous, longform speech input concurrently with the speakers speech (Ren et al., 2020), posing significant challenge at the intersection of automatic speech recognition and machine translation (Fügen et al., 2008). Fundamentally distinct from offline translation where the entire utterance is available before processingStreamST systems must adhere to specific latency constraints, defined as the time elapsed between when segment of speech is spoken and when its translation is produced. Specifically, translation output must be generated simultaneously with, or shortly after, the source speech is received 1 to remain aligned with the pace of the speaker. This process necessitates making decisions with only partial information availableoften called policy (Grissom II et al., 2014), demanding models that can balance low latency with high-quality translation. Despite these inherent challenges, considerable efforts in both academia and industry have led to rapid progress in the field, as evidenced by the growing number of recent publications on the topic (Papi et al., 2025). In the literature, two main paradigms emerged for delivering StreamST outputs: re-translation and incremental decoding. Re-translation entails continuously re-translating the entire target sentence (or part of it) whenever new source speech is received. This strategy utilizes non-monotonic decoding approach, enabling dynamic self-correction and revision of previous outputs to maximize translation quality, but it often leads to visible output instability, known as flickering (Arivazhagan et al., 2020b). Incremental decoding, instead, does not allow the revision or deletion of previously generated outputs (reflecting an append-only approach) at the benefit of more stable user experience (Dalvi et al., 2018). Re-translation is prevalent in industrial applications1 as it is easy to apply, requiring no specific modification or re-training to the underlying offline speech translation model. Research focuses more on incremental approaches, primarily models trained ad-hoc for the task (Schneider and Waibel, 2020; Iranzo-Sánchez et al., 2021; Ouyang et al., 2025), but few recent works have successfully employed training-free approaches (Polák and Bojar, 2024; Papi et al., 2024). Both paradigms are critical to study and advance the state of the art in simultaneous communication systems (Arivazhagan et al., 2020a). However, comparison between these two crit1https://research.google/blog/stabilizing-live-speechtranslation-in-google-translate/ ical paradigms is not currently possible with the current community-standard SimulEval (Ma et al., 2020), tool that provides simultaneous inference infrastructure and scoring. SimulEval was designed for incremental decoding only, and does not allow for token deletion and, therefore, for assessing re-translation solutions. Furthermore, it has been created for the simplified setting of processing short speech segments, without considering the more realistic scenario in which the input audio is long stream, which is the focus of more recent research works (Abdulmumin et al., 2025; Polák et al., 2025). Lastly, there is no facility supporting the creation of live demonstrations featuring the systems built on SimulEval, and, critically, the project is not maintained anymore (it has been archived on Sep 18th, 2025), leaving the community without an active open source framework to rely on. Beyond SimulEval, only limited number of alternative tools have been proposed, all with scarce adoption: SLTev (Ansari et al., 2021), which supports the evaluation of StreamST systems under both re-translation and incremental settings, but does not provide demonstration interface, and Lecture Translator (Huber et al., 2023), which offers web interface, but the evaluation framework is unavailable due to an inaccessible codebase. As result, the community lacks maintained and practical framework for jointly evaluating and showcasing StreamST systems. To address this gap, we propose simulstream, the first open-source tool for both evaluating and demonstrating StreamST systems. simulstream is released open source under the Apache 2.0 License and natively supports: Both re-translation and incremental decoding paradigms, including accurate tracking of emitted and deleted tokens; seamless porting of existing methods developed on SimulEval, by means of dedicated proxy processor supporting any existing SimulEval agent; Long-form speech evaluation and metrics computation, both in terms of translation quality (including BLEU and COMET), and latency (including computationally aware and unaware measurements); Integrated demonstration with an interactive web interface for real-time visualization and system comparison."
        },
        {
            "title": "2 The simulstream Tool",
            "content": "simulstream provides WebSocket server and utilities for running streaming speech processing experiments and demos. It supports real-time transcription and translation through streaming audio input. By streaming, we mean that the library by default assumes that the input is an unbounded speech signal, rather than sequence of short speech segments as in simultaneous speech processing. The simultaneous setting can be easily addressed by feeding small audio segments to simulstream. Figure 1: Architecture of the simulstream tool."
        },
        {
            "title": "2.1 Architecture",
            "content": "The package is based on full-duplex WebSocket client-server interaction, shown in Figure 1. The server waits for connections from the clients. The clients can send configuration messages in JSON to specify the input/output desired format (e.g. languages) and audio chunks, encoded as 16-bit integers. The server processes the input audio and sends to the client JSONs that contain the transcript/translation generated by the configured speech processor, which implements the logic to generate outputs from incoming audio, as in SimulEval agents. In addition, if configured to do so, the server writes metric logs into JSONL file, reporting generated (and deleted) text at each step together with computational costs and total audio processed, which can be used to compute metrics. The codebase is organized into 3 main blocks: the server, the clients, and the evaluation (see 2.3). Server. The Websocket server can be run with command (simulstream_server) that takes two YAML configuration files as arguments. One configuration file specifies generic properties of the WebSocket server (e.g., the IP/DNS address and port on which to listen), while the other specifies the speech processor to use (see 2.2). The server creates pool with fixed (configurable) number 2 Figure 2: Screenshot of the web interface. of speech processors, which enables parallel serving of multiple clients. If the number of connecting clients exceeds the amount of available speech processors, newer client connections are refused by the server. In this way, the server enables controlling the amount of concurrent operations so that out-ofmemory issues due to excessive parallel requests are avoided. Clients. The tool comes with two clients, an HTTP web server and command-line WAV client, meant, respectively, for demo and evaluation of speech processor. The HTTP web server provides an HTML/CSS/Javascript web interface interacting with the WebSocket server (see Figure 2). The command-line WAV client, instead, gets as argument file containing list of WAV files to stream to the server and it can be used to produce the metric logs necessary for evaluating speech processor. To further ease experimenting and evaluating speech processors, simulstream includes dedicated command that processes list of audios without setting up client-server interaction and writes the metric logs into specified file."
        },
        {
            "title": "2.2 Speech Processors",
            "content": "simulstream also comes with several streaming speech processors, which can also be used as examples to build custom ones. Sliding window (Sen et al., 2022): speech processor that applies fixed-length sliding window retranslation with deduplication to mitigate overlapping outputs when processing unsegmented audio streams. The approach 3 relies on detecting the longest common subsequence between the current window and the previous one, in order to prevent repeating tokens caused by overlapping audio windows. This approach is implemented for Canary v2 (Sekoyan et al., 2025), Seamless (Communication et al., 2023), and any speech-to-text HuggingFace model. StreamAtt (Papi et al., 2024): speech processor that leverages audio-textual alignments based on cross-attention scores. This mechi) identifying anism serves two purposes: which part of the generated hypothesis should be emitted (i.e., applying the AlignAtt policy; Papi et al. 2023), and ii) determining which part of the audio history to retain in memory, based on the textual history (i.e., the previously generated outputs), thereby enabling long-form processing. This approach is implemented for Seamless. SimulEval Agent Wrapper: wrapper processor that calls configured SimulEval agent.2 This enables seamless porting of any existing system developed using the legacy SimulEval tool. VAD Wrapper: speech processor that integrates Voice Activity Detection (VAD), specifically the Silero VAD (Team, 2024). Its function is to split continuous audio streams into speech chunks by filtering out parts that do not 2It supports agents implemented for SimulEval>=1.1.0. contain speech before passing the remaining segments to an underlying speech processor. add The repository is structured to be easily extended with custom speech processors. To this aim, users should create subclass of simulstream.server.speech_processors. SpeechProcessor, the class the to PYTHONPATH, and reference it in the YAML file configuration of the speech processor. Subclasses must implement methods to load models, process audio chunks, set source/target languages, and clear internal states. Speech chunks are fed as 1D NumPy arrays containing PCM audio normalized to the range [1.0, 1.0] sampled at 16 kHz. Their size (in seconds) is determined by the speech processor itself through dedicated method and the speech processor has to return incremental outputs, containing the tokens to be deleted from previously generated outputs (if any) and the new tokens to be emitted."
        },
        {
            "title": "2.3 Evaluation",
            "content": "The evaluation of streaming systems is typically performed by looking at the trade-off between latency and quality. simulstream not only offers metrics for each of them, but also an easy interface to integrate custom/additional metrics. Besides, since the evaluation input is the fine-grained metric logs, which include all the information about what speech processor has generated, newer metrics do not require logging additional information and can evaluate any past run. Quality. Most simultaneous/streaming ST works have relied solely on BLEU (Papineni et al., 2002), even though the community has largely discouraged its usage in favor of more recent neural metrics with demonstrated higher correlation with human judgments (Freitag et al., 2022). For this reason, simulstream implements not only BLEU score computed with sacreBLEU (Post, 2018) but also COMET3 (Rei et al., 2020). The scores are computed after re-segmentation of the full generated text for an audio to match the segmentation of the references, which are assumed to be segmented at the sentence level. This operation is performed through mweralign (Post and Hoang, 2025), which is Python porting of the widespread mwerSegmenter binary executable (Matusov et al., 3The default model is Unbabel/wmt22-comet-da but different models can be specified with command line parameter. 2005) used in ST evaluation campaigns (Abdulmumin et al., 2025) and previous work on StreamST (Papi et al., 2024). In the case of streaming processors that delete previously generated outputs, such as the sliding window processor, the final output is used in the evaluation and intermediate outputs are not considered. contains simulstream Latency. reimplementation of the StreamLAAL (Papi et al., 2024) metric for assessing latency. The main difference from the original StreamLAAL implementation is the adoption of mweralign instead of mwerSegmenter for the automatic resegmentation of the generated text with respect to the sentence-level references. StreamLAAL assumes that each generated and reference word at the same position are aligned and computes the latency as the difference between the time in which word has been generated and the expected time of the corresponding word in the reference (the words in the reference are considered to be equally distributed ie., if the reference is made of 10 words for 5 second utterance, each word is assumed to be uttered every 0.5 seconds). The time of generated word can be defined as either the ideal time in the audio (i.e., indicating how much audio has been processed when the word is emitted) or the computational-aware (CA) time, which incorporates the computational cost required to produce that output. Typically, StreamLAAL indicates the first case, while StreamLAAL CA refers to the latter. Since StreamLAAL was originally introduced for the evaluation of streaming agents that do not overwrite generated content, we extended its definition to retranslation processors by considering the final output for the evaluation, in accordance with what is done for the quality metrics. This means that, for each word, we consider the last time step in which it was updated as the emission time to compute latency. This choice implies that the computed latency may be higher than what experienced by users for retranslation systems, but also the quality reported may be higher since it does not account for intermediate (possibly wrong) outputs. simulstream additionally comOther statistics. putes statistics that complement the evaluation metrics defined above. Specifically, its current version enables computing normalized erasure (NE) and the real time factor (RTF). NE (Arivazhagan et al., 2020a) measures flickering in retranslation. It is 4 defined as the ratio between the number of tokens that have been deleted and the number of final generated tokens. RTF, instead, is measured as seconds spent in computation for each input audio second. RTF values greater than 1 indicate that the system is not able to process the input in time before the next input arrives. As for the metrics above, the system modularity enables easy integration of additional/customized statistics."
        },
        {
            "title": "3.1 Setup",
            "content": "We demonstrate the utility of our tool by comparing the released speech processors (2.2). Specifically, we include i) the sliding window with both Canary v2 and Seamless medium v1,4 ii) the VAD wrapper with sliding window as underlying processor (which we include only for Seamless medium v1 for brevity), and iii) StreamAtt with Seamless medium v1. For the sliding window and VAD wrapper, we slide the window by 2s.5 For the sliding window approach, we vary the window length (8, 10, 12, 14 seconds) to obtain different operating points in terms of quality and latency. For the VAD wrapper, instead, we use 14s as window lenght and we vary the VAD probability threshold (from 0.3 to 0.6), which controls whether to be more conservative (lower values) or more aggressive (higher values) in considering audio as non-speech and filtering it out. Lastly, for StreamAtt, we use speech chunks of 1s, and we vary the cutoff frame with 2, 4, 6, and 8 to get different latency-quality tradeoffs. As data, we leverage the 8 language pairs (ende,es,fr,it,nl,pt,ro,ru) of the widespread MuST-C dataset (Di Gangi et al., 2019) and we process the entire talks in the test set (each lasting 10 minutes). Popular ST corpora, such as CoVoST2 (Wang et al., 2020) and FLEURS (Conneau et al., 2023), are not employed in the experiments as they contain sentence-level audios (lasting few seconds), which are not representative of real use cases with long audio streams. For quality and latency metrics (2.3), we use as references the sentence-level texts. 4We also experimented with Seamless large v2 but obtained inferior results (see Appendix A). 5We tested 1s, 2s, and 3s on the dev set and chose 2s as it gave the best quality/latency trade-off."
        },
        {
            "title": "3.2 Results",
            "content": "Table 1 reports the scores obtained by the different speech processors averaged over all the 8 language pairs. The detailed scores for each language pair are reported in Appendix B. By comparing the retranslation approaches with the two models (Canary and SeamlessM4T), Canary emerges as the best performing one by large margin with analogous trends both with and without VAD. Although their latency (StreamLAAL) is similar, the quality (COMET, SacreBLEU) of Canarys translations is significantly better than that of SeamlessM4T. Canary also displays lower amount of flickering (NE) and computational costs (RTF). Moving to the comparison between the sliding window approach and the processor made by the VAD wrapper on top of it, the behavior is consistent for both models. The flickering and computational costs are dramatically reduced (flickering is roughly 8 smaller, and RTF 2-3 lower). On the downside, there is reduced possibility of controlling the latency-quality tradeoff (with the VAD wrapper, all the operational points are close to each other, with latency differences spanning 0.2s and quality less than 0.015 COMET) and, most importantly, the translation quality suffers huge drop. 0.78 0.76 O 0. 0.72 2 2.5 3.5 3 StreamLAAL (s) 4.5 Re-translation Incremental Latency (StreamLAAL) Figure 3: - Quality (COMET) curves of Sliding-window re-translation and StreamAtt incremental methods on SeamlessM4T v1 medium. Dashed lines indicate computationally aware latency, while solid lines computationally unaware. Lastly, we turn to looking at the differences between the incremental StreamAtt and the retranslation sliding window methods with the SeamlessM4T model. Such comparison is the first between an incremental and retranslation approach, as this has been hindered so far by the lack of tool supporting both of them. Recalling that the Speech P. Model w/f/t COMET BLEU StreamLAAL StreamLAALCA NE RTF Canary SeamlessM4T Canary SeamlessM4T SeamlessM4T 8 10 12 8 10 12 14 0.6 0.5 0.4 0.3 0.6 0.5 0.4 0.3 2 4 6 8 0.7853 0.7883 0.7957 0.7986 0.7174 0.7359 0.7439 0. 0.7375 0.7426 0.7464 0.7523 0.6949 0.6972 0.7008 0.7058 0.7589 0.7653 0.7695 0.7696 28.71 28.48 28.92 29.20 24.51 25.39 25.70 25.65 24.70 25.01 25.26 25. 21.97 22.16 22.49 22.98 26.42 27.25 27.80 27.94 n n l d g i + t a S i s t a m n 2.47 3.01 3.40 3.93 2.42 2.92 3.53 4.26 2.54 2.57 2.57 2.61 2.53 2.56 2.63 2. 1.95 2.25 2.56 2.84 2.80 3.43 3.93 4.47 2.80 3.38 4.07 4.90 2.78 2.82 2.83 2.87 2.81 2.84 2.92 3.04 2.23 2.53 2.85 3. 0.5771 0.8049 0.9475 1.1277 0.8586 1.0215 1.1778 1.3401 0.1304 0.1393 0.1488 0.1631 0.1631 0.1709 0.1842 0.2060 0.0000 0.0000 0.0000 0.0000 0.1438 0.1865 0.2315 0. 0.1770 0.2101 0.2432 0.2878 0.0822 0.0834 0.0854 0.0866 0.0939 0.0949 0.0973 0.1008 0.2196 0.2328 0.2447 0.2581 Table 1: Quality (COMET, BLUE), latency (StreamLAAL, StreamLAAL_CA), flickering (NE), and computational cost (RTF) of the supported re-translation and incremental speech processors (Speech P.) with Canary and SeamlessM4T v1 medium on the MuST-C test set (averaged over all language pairs). The column w/f/t refers to varying, the window length (w) for sliding window, the number of frames (f) for StreamAtt, and the VAD probability threshold (t) for VAD-based sliding window. metrics have been designed in way that penalizes the latency but favors the quality scores for retranslation methods (see 2.3), StreamAtt surprisingly outperforms sliding window not only on latency but also in terms of quality. Its superiority is clear also looking at Figure 3, which reports the quality-latency tradeoff for both methods. In addition, StreamAtt has no flickering. However, the computational cost (RTF) of StreamAtt turns out to be higher, especially at lower latency regimes. Overall, the incremental StreamAtt strategy emerges as the most promising approach in terms of optimizing the quality-latency tradeoff, even though it is the most computationally expensive. We have also shown that the choice of the underlying model has huge impact on quality and computational costs, while latency depends less on this. Although these findings should be corroborated by extensive human evaluations and should be validated on broader range on datasets and language pairs, they provide first insight on assessing StreamST solutions and strong baselines for further research on the topic. Besides shedding new light on the effectiveness of different approaches to the StreamST problem, these contributions demonstrate the utility of tool like simulstream for filling the gap left by the discontinuation of SimulEval, while also offering new functionalities."
        },
        {
            "title": "4 Conclusions",
            "content": "We presented simulstream, the first open-source framework enabling unified evaluation and demonstration of Streaming Speech-to-Text Translation (StreamST) systems. Unlike previous tools, simulstream supports both incremental decoding and re-translation, tracks token emissions and deletions, and accommodates long-form audio, providing comprehensive platform for assessing translation quality, latency, and computational costs. In addition, its interactive web interface allows real-time visualization and comparison of different StreamST systems, bridging the gap between research and practical deployment. By offering paradigm-agnostic, flexible, and extendable toolkit, simulstream empowers the community to systematically study, benchmark, and showcase modern StreamST approaches, facilitating the advancement of simultaneous ST research."
        },
        {
            "title": "Acknowledgments",
            "content": "lInnovazione Tecnologica the project We acknowledge the support of Network Italiano dei CenInnovAction: (CUP per tri B47H2200437000), funded by MIMIT with NPRR - NextGenerationEU funds, in collaboration with Piazza Copernico S.r.l. We also acknowledge the support of the PNRR project FAIR - Future AI Research (PE00000013), under the NRRP MUR program funded by the NextGenerationEU."
        },
        {
            "title": "References",
            "content": "Idris Abdulmumin, Victor Agostinelli, Tanel Alumäe, Antonios Anastasopoulos, Luisa Bentivogli, Ondˇrej Bojar, Claudia Borg, Fethi Bougares, Roldano Cattoni, Mauro Cettolo, Lizhong Chen, William Chen, Raj Dabre, Yannick Estève, Marcello Federico, Mark Fishel, Marco Gaido, Dávid Javorský, Marek Kasztelnik, and 33 others. 2025. Findings of the IWSLT 2025 evaluation campaign. In Proceedings of the 22nd International Conference on Spoken Language Translation (IWSLT 2025), pages 412481, Vienna, Austria (in-person and online). Association for Computational Linguistics. Ebrahim Ansari, Ondˇrej Bojar, Barry Haddow, and Mohammad Mahmoudi. 2021. SLTEV: Comprehensive evaluation of spoken language translation. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations, pages 7179, Online. Association for Computational Linguistics. Naveen Arivazhagan, Colin Cherry, Wolfgang Macherey, and George Foster. 2020a. Re-translation versus streaming for simultaneous translation. In Proceedings of the 17th International Conference on Spoken Language Translation, pages 220227, Online. Association for Computational Linguistics. Naveen Arivazhagan, Colin Cherry, Te, Wolfgang Macherey, Pallavi Baljekar, and George Foster. 2020b. Re-translation strategies for long form, simultaneous, spoken language translation. In ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 79197923. Seamless Communication, Loïc Barrault, Yu-An Chung, Mariano Cora Meglioli, David Dale, Ning Dong, Paul-Ambroise Duquenne, Hady Elsahar, Hongyu Gong, Kevin Heffernan, John Hoffman, Christopher Klaiber, Pengwei Li, Daniel Licht, Jean Maillard, Alice Rakotoarison, Kaushik Ram Sadagopan, Guillaume Wenzek, Ethan Ye, and 49 others. 2023. Seamlessm4t: Massively multilingual & multimodal machine translation. Preprint, arXiv:2308.11596. Alexis Conneau, Min Ma, Simran Khanuja, Yu Zhang, Vera Axelrod, Siddharth Dalmia, Jason Riesa, Clara Rivera, and Ankur Bapna. 2023. Fleurs: Few-shot learning evaluation of universal representations of speech. In 2022 IEEE Spoken Language Technology Workshop (SLT), pages 798805. IEEE. Fahim Dalvi, Nadir Durrani, Hassan Sajjad, and Stephan Vogel. 2018. Incremental decoding and training methods for simultaneous translation in neural machine translation. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 493499, New Orleans, Louisiana. Association for Computational Linguistics. Mattia A. Di Gangi, Roldano Cattoni, Luisa Bentivogli, Matteo Negri, and Marco Turchi. 2019. MuST-C: Multilingual Speech Translation Corpus. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 20122017, Minneapolis, Minnesota. Association for Computational Linguistics. Markus Freitag, Ricardo Rei, Nitika Mathur, Chi-kiu Lo, Craig Stewart, Eleftherios Avramidis, Tom Kocmi, George Foster, Alon Lavie, and André F. T. Martins. 2022. Results of WMT22 metrics shared task: Stop using BLEU neural metrics are better and more robust. In Proceedings of the Seventh Conference on Machine Translation (WMT), pages 4668, Abu Dhabi, United Arab Emirates (Hybrid). Association for Computational Linguistics. Christian Fügen, Alex Waibel, and Muntsin Kolss. 2008. Simultaneous translation of lectures and speeches. Machine Translation, 21(4):209252. Alvin Grissom II, He He, Jordan Boyd-Graber, John Morgan, and Hal Daumé III. 2014. Dont until the final verb wait: Reinforcement learning for simultaneous machine translation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 13421352, Doha, Qatar. Association for Computational Linguistics. Christian Huber, Tu Anh Dinh, Carlos Mullov, NgocQuan Pham, Thai Binh Nguyen, Fabian Retkowski, Stefan Constantin, Enes Ugan, Danni Liu, Zhaolin Li, Sai Koneru, Jan Niehues, and Alexander Waibel. 2023. End-to-end evaluation for low-latency simultaneous speech translation. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 1220, Singapore. Association for Computational Linguistics. Javier Iranzo-Sánchez, Javier Jorge, Pau Baquero-Arnal, Joan Albert Silvestre-Cerdà, Adrià Giménez, Jorge Civera, Albert Sanchis, and Alfons Juan. 2021. Streaming cascade-based speech translation leveraged by direct segmentation model. Neural Networks, 142:303315. 7 Matt Post and Hieu Hoang. 2025. Effects of automatic alignment on speech translation metrics. In Proceedings of the 22nd International Conference on Spoken Language Translation (IWSLT 2025), pages 8492, Vienna, Austria (in-person and online). Association for Computational Linguistics. Ricardo Rei, Craig Stewart, Ana Farinha, and Alon Lavie. 2020. COMET: neural framework for MT evaluation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 26852702, Online. Association for Computational Linguistics. Yi Ren, Jinglin Liu, Xu Tan, Chen Zhang, Tao Qin, Zhou Zhao, and Tie-Yan Liu. 2020. SimulSpeech: End-to-end simultaneous speech to text translation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3787 3796, Online. Association for Computational Linguistics. Felix Schneider and Alexander Waibel. 2020. Towards stream translation: Adaptive computation time for simultaneous machine translation. In Proceedings of the 17th International Conference on Spoken Language Translation, pages 228236, Online. Association for Computational Linguistics. Monica Sekoyan, Nithin Rao Koluguri, Nune Tadevosyan, Piotr Zelasko, Travis Bartley, Nikolay Karpov, Jagadeesh Balam, and Boris Ginsburg. 2025. Canary-1b-v2 & parakeet-tdt-0.6b-v3: Efficient and high-performance models for multilingual asr and ast. Preprint, arXiv:2509.14128. Sukanta Sen, Ondˇrej Bojar, and Barry Haddow. 2022. Simultaneous translation for unsegmented Preprint, input: sliding window approach. arXiv:2210.09754. Silero Team. 2024. Silero vad: pre-trained enterprisegrade voice activity detector (vad), number detector and language classifier. https://github.com/ snakers4/silero-vad. Changhan Wang, Anne Wu, and Juan Pino. 2020. Covost 2 and massively multilingual speech-to-text translation. arXiv preprint arXiv:2007.10310. Xutai Ma, Mohammad Javad Dousti, Changhan Wang, Jiatao Gu, and Juan Pino. 2020. SIMULEVAL: An evaluation toolkit for simultaneous translation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 144150, Online. Association for Computational Linguistics. Evgeny Matusov, Gregor Leusch, Oliver Bender, and Hermann Ney. 2005. Evaluating machine translation output with automatic sentence segmentation. In Proceedings of the Second International Workshop on Spoken Language Translation, Pittsburgh, Pennsylvania, USA. Siqi Ouyang, Xi Xu, and Lei Li. 2025. InfiniSST: Simultaneous translation of unbounded speech with large language model. In Findings of the Association for Computational Linguistics: ACL 2025, pages 30323046, Vienna, Austria. Association for Computational Linguistics. Sara Papi, Marco Gaido, Matteo Negri, and Luisa Bentivogli. 2024. StreamAtt: Direct streaming speech-totext translation with attention-based audio history selection. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 36923707, Bangkok, Thailand. Association for Computational Linguistics. Sara Papi, Peter Polák, Dominik Macháˇcek, and Ondˇrej Bojar. 2025. How real is your real-time simultaneous speech-to-text translation system? Transactions of the Association for Computational Linguistics, 13:281313. Sara Papi, Marco Turchi, and Matteo Negri. 2023. Alignatt: Using attention-based audio-translation alignments as guide for simultaneous speech translation. In Interspeech 2023, pages 39743978. Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311318, Philadelphia, Pennsylvania, USA. Association for Computational Linguistics. Peter Polák, Sara Papi, Luisa Bentivogli, and Ondˇrej Bojar. 2025. Better late than never: Evaluation of latency metrics for simultaneous speech-to-text translation. arXiv preprint arXiv:2509.17349. Peter Polák and Ondˇrej Bojar. 2024. Long-form end-toend speech translation via latent alignment segmentation. In 2024 IEEE Spoken Language Technology Workshop (SLT), pages 10761082. Matt Post. 2018. call for clarity in reporting BLEU scores. In Proceedings of the Third Conference on Machine Translation: Research Papers, pages 186 191, Brussels, Belgium. Association for Computational Linguistics. 8 Comparison with SeamlessM4T v"
        },
        {
            "title": "Large",
            "content": "Table 2 reports the scores for SeamlessM4T v2 large and SeamlessM4T v1 medium. Interestingly, SeamlessM4T v1 medium outperforms by large margin the v2 large version on all metrics including those related to output quality but normalized erasure. The latency is higher not only due to the (expected) higher computational cost, evident from the high RTF, but also the ideal StreamLAAL is notably worse. For this reason, the results in the main paper are reported using the medium v1 version."
        },
        {
            "title": "Pair",
            "content": "Tables 3, 4, 5, 6, 7, 8, 9, 10 contain the detailed scores for each language pair, i.e., respectively, ende, en-es, en-fr, en-it, en-nl, en-pt, en-ro, en-ru. 9 Model w/f COMET BLEU StreamLAAL StreamLAALCA NE RTF SeamlessM4T v2 large SeamlessM4T v1 medium 8 10 12 14 8 10 12 14 0.7016 0.7089 0.7089 0.7005 0.7174 0.7359 0.7439 0.7469 21.72 23.31 23.31 20.02 24.51 25.39 25.70 25. 3.29 3.57 3.57 6.35 2.42 2.92 3.53 4.26 4.14 4.55 4.55 7.58 2.80 3.38 4.07 4.90 0.7062 0.8455 0.8455 1.0316 0.8586 1.0215 1.1778 1. 0.4417 0.4505 0.4505 0.6101 0.1770 0.2101 0.2432 0.2878 Table 2: Quality (COMET, BLUE), latency (StreamLAAL, StreamLAAL_CA), flickering (NE), and computational cost (RTF) of the sliding window processor using SeamlessM4T v2 large and SeamlessM4T v1 medium, averaged across the 8 language pairs of MuST-C. Speech P. Model w/f/t COMET BLEU StreamLAAL StreamLAALCA NE RTF Canary SeamlessM4T Canary SeamlessM4T SeamlessM4T 8 10 12 14 8 10 12 14 0.6 0.5 0.4 0.3 0.6 0.5 0.4 0. 2 4 6 8 0.7591 0.7656 0.7768 0.7814 0.6793 0.6982 0.7104 0.7108 0.7147 0.7211 0.7259 0.7325 0.6700 0.6730 0.6772 0.6792 0.7305 0.7382 0.7420 0. 28.12 27.79 28.41 29.01 22.34 23.07 23.63 23.26 24.02 24.45 24.79 25.11 20.45 20.56 20.86 21.27 25.15 26.12 26.55 26.62 n n l w i i S + t a S t n e t e i 2.56 3.09 3.48 4.07 2.39 3.09 3.69 4.38 2.58 2.60 2.61 2.66 2.55 2.59 2.72 2.76 1.95 2.23 2.53 2.80 2.89 3.48 4.14 4. 2.77 3.56 4.21 5.13 2.82 2.85 2.87 2.94 2.83 2.87 3.01 3.07 2.23 2.51 2.83 3.09 0.6416 0.8750 1.0082 1.1995 0.8799 1.0688 1.2380 1. 0.1387 0.1485 0.1567 0.1726 0.1797 0.1857 0.2028 0.2226 0.0000 0.0000 0.0000 0.0000 0.1444 0.1743 0.2961 0.2361 0.1753 0.2115 0.2405 0.3357 0.0816 0.0831 0.0861 0. 0.0949 0.0965 0.0977 0.1033 0.2226 0.2401 0.2562 0.2649 Table 3: Quality (COMET, BLUE), latency (StreamLAAL, StreamLAAL_CA), flickering (NE), and computational cost (RTF) of the supported re-translation and incremental speech processors (Speech P.) with Canary and SeamlessM4T v1 medium on the en-de MuST-C test set. The column w/f/t refers to varying, the window length (w) for sliding window, the number of frames (f) for StreamAtt, and the VAD probability threshold (t) for VAD-based sliding window. 10 Speech P. Model w/f/t COMET BLEU StreamLAAL StreamLAALCA NE RTF Canary SeamlessM4T Canary SeamlessM4T SeamlessM4T 8 10 12 14 8 10 12 14 0.6 0.5 0.4 0. 0.6 0.5 0.4 0.3 2 4 6 8 0.7925 0.7938 0.7985 0.8016 0.7304 0.7495 0.7604 0.7652 0.7437 0.7489 0.7529 0.7580 0.7069 0.7099 0.7127 0. 0.7655 0.7741 0.7793 0.7787 32.39 31.99 32.16 32.61 26.97 27.92 28.53 28.51 27.94 28.34 28.72 29.12 24.40 24.55 24.97 25.72 28.63 29.70 30.40 30. d g i o W d + t a S t n e t e i 2.35 2.96 3.35 3.92 2.29 2.69 3.29 3.98 2.51 2.55 2.60 2.58 2.45 2.51 2.54 2.61 1.74 2.10 2.43 2.75 2.67 3.52 3.99 4. 2.72 3.19 3.87 4.67 2.75 2.79 2.85 2.84 2.72 2.78 2.82 2.91 1.99 2.36 2.69 3.01 0.5375 0.7666 0.9083 1.1009 0.8658 0.9758 1.1476 1. 0.1169 0.1240 0.1408 0.1550 0.1586 0.1664 0.1745 0.1990 0.0000 0.0000 0.0000 0.0000 0.1373 0.2466 0.2862 0.2199 0.2007 0.2301 0.2674 0.3144 0.0794 0.0807 0.0803 0. 0.0931 0.0937 0.0947 0.0990 0.2068 0.2211 0.2314 0.2443 Table 4: Quality (COMET, BLUE), latency (StreamLAAL, StreamLAAL_CA), flickering (NE), and computational cost (RTF) of the supported re-translation and incremental speech processors (Speech P.) with Canary and SeamlessM4T v1 medium on the en-es MuST-C test set. The column w/f/t refers to varying, the window length (w) for sliding window, the number of frames (f) for StreamAtt, and the VAD probability threshold (t) for VAD-based sliding window. 11 Speech P. Model w/f/t COMET BLEU StreamLAAL StreamLAALCA NE RTF Canary SeamlessM4T Canary SeamlessM4T SeamlessM4T 8 10 12 14 8 10 12 14 0.6 0.5 0.4 0. 0.6 0.5 0.4 0.3 2 4 6 8 0.7920 0.7929 0.7986 0.8025 0.7226 0.7427 0.7500 0.7560 0.7378 0.7438 0.7482 0.7539 0.6941 0.6968 0.6980 0. 0.7634 0.7720 0.7744 0.7743 38.19 38.46 39.13 39.34 32.98 34.26 34.58 34.77 33.73 34.02 34.42 34.85 28.70 28.99 29.27 29.88 34.76 35.77 36.38 36. d g i o W d + t a S t n e t e i 2.42 2.92 3.30 3.83 2.43 2.91 3.46 4.07 2.52 2.55 2.60 2.63 2.55 2.59 2.67 2.72 1.95 2.24 2.53 2.84 2.78 3.34 3.81 4. 2.81 3.39 4.00 4.70 2.77 2.82 2.88 2.90 2.84 2.87 2.97 3.03 2.23 2.53 2.82 3.15 0.5547 0.7711 0.9097 1.0840 0.8330 0.9821 1.1069 1. 0.1224 0.1341 0.1439 0.1588 0.1531 0.1610 0.1747 0.1978 0.0000 0.0000 0.0000 0.0000 0.1531 0.1846 0.2249 0.3314 0.1776 0.2156 0.2452 0.2840 0.0856 0.0880 0.0924 0. 0.0978 0.0988 0.1014 0.1038 0.2290 0.2422 0.2522 0.2728 Table 5: Quality (COMET, BLUE), latency (StreamLAAL, StreamLAAL_CA), flickering (NE), and computational cost (RTF) of the supported re-translation and incremental speech processors (Speech P.) with Canary and SeamlessM4T v1 medium on the en-fr MuST-C test set. The column w/f/t refers to varying, the window length (w) for sliding window, the number of frames (f) for StreamAtt, and the VAD probability threshold (t) for VAD-based sliding window. 12 Speech P. Model w/f/t COMET BLEU StreamLAAL StreamLAALCA NE RTF Canary SeamlessM4T Canary SeamlessM4T SeamlessM4T 8 10 12 14 8 10 12 14 0.6 0.5 0.4 0. 0.6 0.5 0.4 0.3 2 4 6 8 0.7887 0.7921 0.7973 0.8007 0.7214 0.7353 0.7392 0.7425 0.7352 0.7409 0.7443 0.7502 0.6892 0.6924 0.6972 0. 0.7531 0.7576 0.7646 0.7641 27.33 27.24 27.38 28.02 22.14 22.99 23.18 23.53 23.31 23.58 23.67 23.86 19.92 20.14 20.39 20.96 24.26 24.95 25.66 25. d g i o W d + t a S t n e t e i 2.48 3.05 3.42 3.99 2.37 2.87 3.53 4.19 2.50 2.54 2.56 2.60 2.54 2.57 2.59 2.77 1.97 2.26 2.54 2.83 2.80 3.43 3.88 4. 2.74 3.31 4.06 4.82 2.74 2.80 2.82 2.86 2.82 2.85 2.88 3.08 2.24 2.53 2.83 3.12 0.5773 0.8284 0.9569 1.1579 0.8585 1.0321 1.1914 1. 0.1307 0.1373 0.1460 0.1616 0.1613 0.1704 0.1845 0.2039 0.0000 0.0000 0.0000 0.0000 0.1404 0.1654 0.1986 0.2299 0.1707 0.2011 0.2371 0.2852 0.0812 0.0828 0.0845 0. 0.0924 0.0928 0.0954 0.0989 0.2151 0.2270 0.2430 0.2527 Table 6: Quality (COMET, BLUE), latency (StreamLAAL, StreamLAAL_CA), flickering (NE), and computational cost (RTF) of the supported re-translation and incremental speech processors (Speech P.) with Canary and SeamlessM4T v1 medium on the en-it MuST-C test set. The column w/f/t refers to varying, the window length (w) for sliding window, the number of frames (f) for StreamAtt, and the VAD probability threshold (t) for VAD-based sliding window. 13 Speech P. Model w/f/t COMET BLEU StreamLAAL StreamLAALCA NE RTF Canary SeamlessM4T Canary SeamlessM4T SeamlessM4T 8 10 12 14 8 10 12 14 0.6 0.5 0.4 0. 0.6 0.5 0.4 0.3 2 4 6 8 0.7898 0.7946 0.8055 0.8089 0.7318 0.7564 0.7631 0.7657 0.7522 0.7555 0.7605 0.7644 0.7148 0.7172 0.7206 0. 0.7775 0.7835 0.7859 0.7849 30.78 30.13 30.54 30.82 26.90 27.97 27.99 27.97 26.63 26.94 27.16 27.22 24.0094 24.3518 24.7816 25.2648 28.70 29.31 29.81 29. d g i o W d + t a S t n e t e i 2.56 3.06 3.53 4.04 2.48 2.86 3.50 4.09 2.56 2.57 2.59 2.61 2.48 2.55 2.61 2.70 1.86 2.16 2.43 2.74 2.88 3.45 4.00 4. 2.86 3.29 4.00 4.67 2.80 2.81 2.84 2.88 2.74 2.83 2.89 3.00 2.12 2.43 2.69 3.02 0.6302 0.8527 1.0112 1.1992 0.8490 1.0153 1.1656 1. 0.1401 0.1509 0.1602 0.1705 0.1599 0.1673 0.1811 0.2038 0.0000 0.0000 0.0000 0.0000 0.1384 0.1719 0.2035 0.2153 0.1747 0.1963 0.2283 0.2669 0.0814 0.0800 0.0813 0. 0.0907 0.0921 0.0945 0.0975 0.2142 0.2270 0.2366 0.2547 Table 7: Quality (COMET, BLUE), latency (StreamLAAL, StreamLAAL_CA), flickering (NE), and computational cost (RTF) of the supported re-translation and incremental speech processors (Speech P.) with Canary and SeamlessM4T v1 medium on the en-nl MuST-C test set. The column w/f/t refers to varying the window length (w) for sliding window, the number of frames (f) for StreamAtt, and the VAD probability threshold (t) for VAD-based sliding window. 14 Speech P. Model w/f/t COMET BLEU StreamLAAL StreamLAALCA NE RTF Canary SeamlessM4T Canary SeamlessM4T SeamlessM4T 8 10 12 14 8 10 12 14 0.6 0.5 0.4 0. 0.6 0.5 0.4 0.3 2 4 6 8 0.8048 0.8069 0.8098 0.8117 0.7547 0.7716 0.7784 0.7828 0.7576 0.7625 0.7657 0.7711 0.7270 0.7282 0.7314 0. 0.7867 0.7915 0.7949 0.7952 28.87 28.71 29.02 29.18 28.37 29.43 29.75 29.68 24.52 24.81 25.07 25.54 25.30 25.62 25.99 26.35 29.57 30.68 31.20 31. d g i o W d + t a S t n e t e i 2.42 2.93 3.22 3.71 2.45 2.90 3.56 4.28 2.60 2.62 2.64 2.67 2.46 2.52 2.58 2.69 1.93 2.23 2.52 2.79 2.74 3.38 3.75 4. 2.82 3.35 4.07 4.88 2.84 2.87 2.89 2.94 2.72 2.79 2.86 2.99 2.19 2.49 2.79 3.07 0.5113 0.7146 0.8592 1.0196 0.8896 1.0518 1.1951 1. 0.1210 0.1278 0.1391 0.1531 0.1581 0.1677 0.1794 0.2031 0.0000 0.0000 0.0000 0.0000 0.1373 0.2001 0.2281 0.2162 0.1687 0.2029 0.2335 0.2653 0.0825 0.0819 0.0841 0. 0.0905 0.0916 0.0951 0.0989 0.2114 0.2221 0.2327 0.2468 Table 8: Quality (COMET, BLEU), latency (StreamLAAL, StreamLAAL_CA), flickering (NE), and computational cost (RTF) of the supported re-translation and incremental speech processors (Speech P.) with Canary and SeamlessM4T v1 medium on the en-pt MuST-C test set. The column w/f/t refers to varying the window length (w) for sliding window, the number of frames (f) for StreamAtt, and the VAD probability threshold (t) for VAD-based sliding window. 15 Speech P. Model w/f/t COMET BLEU StreamLAAL StreamLAALCA NE RTF Canary SeamlessM4T Canary SeamlessM4T SeamlessM4T 8 10 12 14 8 10 12 14 0.6 0.5 0.4 0. 0.6 0.5 0.4 0.3 2 4 6 8 0.8071 0.8092 0.8162 0.8192 0.7300 0.7470 0.7542 0.7534 0.7531 0.7581 0.7622 0.7699 0.7056 0.7063 0.7113 0. 0.7702 0.7774 0.7807 0.7827 25.14 24.87 25.48 25.56 21.45 22.16 22.21 21.84 21.51 21.87 21.94 22.37 19.67 19.61 19.89 20.24 23.47 24.17 24.75 24. d g i o W d + t a S t n e t e i 2.47 2.98 3.39 3.91 2.47 3.08 3.72 4.54 2.59 2.58 2.51 2.62 2.58 2.60 2.67 2.75 1.95 2.28 2.60 2.84 2.82 3.38 3.85 4. 2.85 3.55 4.26 5.17 2.83 2.87 2.76 2.87 2.87 2.88 2.97 3.06 2.23 2.57 2.90 3.14 0.5655 0.8125 0.9404 1.1203 0.8613 1.0327 1.2135 1. 0.1312 0.1411 0.1484 0.16 0.1662 0.1718 0.1856 0.2038 0.0000 0.0000 0.0000 0.0000 0.1459 0.1714 0.2029 0.2322 0.1760 0.2108 0.2500 0.2833 0.0818 0.0824 0.0830 0. 0.0973 0.0967 0.1007 0.1024 0.2167 0.2358 0.2472 0.2578 Table 9: Quality (COMET, BLEU), latency (StreamLAAL, StreamLAAL_CA), flickering (NE), and computational cost (RTF) of the supported re-translation and incremental speech processors (Speech P.) with Canary and SeamlessM4T v1 medium on the en-ro MuST-C test set. The column w/f/t refers to varying the window length (w) for sliding window, the number of frames (f) for StreamAtt, and the VAD probability threshold (t) for VAD-based sliding window. 16 Speech P. Model w/f/t COMET BLEU StreamLAAL StreamLAALCA NE RTF Canary SeamlessM4T Canary SeamlessM4T SeamlessM4T 8 10 12 14 8 10 12 14 0.6 0.5 0.4 0. 0.6 0.5 0.4 0.3 2 4 6 8 0.7481 0.7513 0.7625 0.7631 0.6689 0.6863 0.6959 0.6986 0.7054 0.7100 0.7116 0.7183 0.6514 0.6535 0.6579 0. 0.7243 0.7282 0.7340 0.7354 18.82 18.63 19.22 19.09 14.88 15.35 15.70 15.60 15.93 16.10 16.28 16.52 13.32 13.46 13.75 14.12 16.83 17.31 17.64 17. d g i o W d + t a S t n e t e i 2.48 3.06 3.52 3.98 2.45 2.97 3.51 4.54 2.46 2.50 2.43 2.47 2.64 2.58 2.64 2.89 2.28 2.50 2.91 3.12 2.82 3.47 3.99 4. 2.82 3.42 4.04 5.15 2.71 2.77 2.70 2.75 2.91 2.86 2.93 3.19 2.58 2.81 3.24 3.45 0.5988 0.8182 0.9862 1.1400 0.8318 1.0134 1.1643 1. 0.1425 0.1507 0.1551 0.1714 0.1682 0.1769 0.1907 0.2139 0.0000 0.0000 0.0000 0.0000 0.1536 0.1778 0.2117 0.2427 0.1725 0.2126 0.2434 0.2677 0.0845 0.0880 0.0915 0. 0.0943 0.0968 0.0991 0.1023 0.2409 0.2468 0.2580 0.2708 Table 10: Quality (COMET, BLEU), latency (StreamLAAL, StreamLAAL_CA), flickering (NE), and computational cost (RTF) of the supported re-translation and incremental speech processors (Speech P.) with Canary and SeamlessM4T v1 medium on the en-ru MuST-C test set. The column w/f/t refers to varying the window length (w) for sliding window, the number of frames (f) for StreamAtt, and the VAD probability threshold (t) for VAD-based sliding window."
        }
    ],
    "affiliations": [
        "Fondazione Bruno Kessler, Trento, Italy"
    ]
}