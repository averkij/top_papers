{
    "paper_title": "Learning Humanoid End-Effector Control for Open-Vocabulary Visual Loco-Manipulation",
    "authors": [
        "Runpei Dong",
        "Ziyan Li",
        "Xialin He",
        "Saurabh Gupta"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Visual loco-manipulation of arbitrary objects in the wild with humanoid robots requires accurate end-effector (EE) control and a generalizable understanding of the scene via visual inputs (e.g., RGB-D images). Existing approaches are based on real-world imitation learning and exhibit limited generalization due to the difficulty in collecting large-scale training datasets. This paper presents a new paradigm, HERO, for object loco-manipulation with humanoid robots that combines the strong generalization and open-vocabulary understanding of large vision models with strong control performance from simulated training. We achieve this by designing an accurate residual-aware EE tracking policy. This EE tracking policy combines classical robotics with machine learning. It uses a) inverse kinematics to convert residual end-effector targets into reference trajectories, b) a learned neural forward model for accurate forward kinematics, c) goal adjustment, and d) replanning. Together, these innovations help us cut down the end-effector tracking error by 3.2x. We use this accurate end-effector tracker to build a modular system for loco-manipulation, where we use open-vocabulary large vision models for strong visual generalization. Our system is able to operate in diverse real-world environments, from offices to coffee shops, where the robot is able to reliably manipulate various everyday objects (e.g., mugs, apples, toys) on surfaces ranging from 43cm to 92cm in height. Systematic modular and end-to-end tests in simulation and the real world demonstrate the effectiveness of our proposed design. We believe the advances in this paper can open up new ways of training humanoid robots to interact with daily objects."
        },
        {
            "title": "Start",
            "content": "Learning Humanoid End-Effector Control for Open-Vocabulary Visual Loco-Manipulation Runpei Dong Ziyan Li"
        },
        {
            "title": "Saurabh Gupta",
            "content": "University of Illinois Urbana-Champaign hero-humanoid.github.io 6 2 0 2 8 1 ] . [ 1 5 0 7 6 1 . 2 0 6 2 : r Fig. 1: We build capability for humanoid to autonomously loco-manipulate novel objects in novel scenes using onboard sensors. We achieve this through modular system powered by large vision models for visual generalization and an accurate end-effector tracking policy. Our system achieves an 83 .8 % average success rate at reaching and picking up novel objects in novel scenes in the real world in challenging scenarios that involve whole-body control via bending, squatting, and twisting. AbstractVisual loco-manipulation of arbitrary objects in the wild with humanoid robots requires accurate end-effector (EE) control and generalizable understanding of the scene via visual inputs (e.g., RGB-D images). Existing approaches are based on real-world imitation learning and exhibit limited generalization due to the difficulty in collecting large-scale training datasets. This paper presents new paradigm, HERO, for object locomanipulation with humanoid robots that combines the strong generalization and open-vocabulary understanding of large vision models with strong control performance from simulated training. We achieve this by designing an accurate residual-aware EE tracking policy. This EE tracking policy combines classical robotics with machine learning. It uses a) inverse kinematics to convert Equal contributions. residual end-effector targets into reference trajectories, b) learned neural forward model for accurate forward kinematics, c) goal adjustment, and d) replanning. Together, these innovations help us cut down the end-effector tracking error by 3.2. We use this accurate end-effector tracker to build modular system for loco-manipulation, where we use open-vocabulary large vision models for strong visual generalization. Our system is able to operate in diverse real-world environments, from offices to coffee shops, where the robot is able to reliably manipulate various everyday objects (e.g., mugs, apples, toys) on surfaces ranging from 43cm to 92cm in height. Systematic modular and end-to-end tests in simulation and the real world demonstrate the effectiveness of our proposed design. We believe the advances in this paper can open up new ways of training humanoid robots to interact with daily objects. I. INTRODUCTION Think about reaching to pick up the objects placed on the various table tops in Fig. 1. As humans we can reliably and robustly use our whole bodies to execute such pick ups. We can use our back to reach over and across the table, or rotate our torso to pick up objects kept on the side, and use our legs to squat down to pick up objects kept on coffee tables, all while maintaining balance on our two legs. We can pick up seen objects on seen tables, but equally effortlessly also pick up novel objects on novel tables in novel scenes. Once we have glanced at the object and scene, we can even do this with our eyes closed if we want. In this paper, we develop an autonomous system that equips humanoid robot with this fundamental capability: reach over to pick up novel objects in novel everyday environments around us. Humanoids are doing backflips [24, 27, 29, 44, 86], so why would we be writing about such mundane and seemingly unimpressive task? There are two key differences that make our problem of manipulating novel objects harder: the need for processing the high-dimensional RGB-D image observations to infer object locations and scene collision geometry, and the precision necessary for object manipulation. Most existing humanoid control works arent conditioned on perceptual input from RGB-D camera: either they dont need any scene information or scene information is provided as input using environment sensors (e.g. MOCAP). Second, while back flips are impressive, it doesnt really matter where you land, as long as you land safely. In contrast, if robot needs to pick up an object, it needs to get its hand where the object actually is. Object manipulation requires precise goal-directed behavior. Operation in novel environments, sensing using on-board RGB-D sensors, precise EE control, and the complexity of maintaining balance while moving around, make this problem challenging. State-of-the-art for training humanoids for such tasks is end-to-end imitation learning in the real world [2, 4, 18, 73]. However, difficulty in collecting large datasets for learning limits generalization capabilities of learned policies. This causes them to fall short of the goal of manipulating novel objects in novel environments, which requires broad generalization. In this paper, we pursue an alternate approach. We take inspiration from strong results with modular systems for table-top object manipulation problems [12, 47, 62]. Vision foundation models translate high-level instructions into actionable plans (e.g., grasp the red coke can), grasp synthesis models (e.g., AnyGrasp [13]) convert this into grasps, and low-level control module conveys the robot EE to the grasp location. Being able to use large pre-trained models enables broad generalization and even open-world reasoning. In many ways, this is the more direct, obvious, and performant way to build such system. So, why isnt such modular method the go-to method for building humanoid object manipulation system? While it is easy to get Franka Emika robot to where you want, it turns out it is extremely difficult for current methods to accurately control humanoid hand. Leading policies for this task achieve approximately 813cm error for end-effector tracking, an error rate that is simply too large for object manipulation. Our key technical contribution is to develop an accurate end-effector tracking policy that enables object manipulation applications. This unlocks the possibility of developing modular humanoid systems for object manipulation that generalize without large-scale real-world imitation demos. So what are the ingredients of building highly accurate endeffector tracker? Our accurate end-effector control algorithm is based on multiple innovations. First, rather than just directly trying to get the end-effector to the target location, we use motion planner to generate an upper-body reference motion that gets the end-effector to the desired target. Second, the policy receives as input not just the current and target joint angles (output using the motion planner), but also the current and target end-effector position. Third, it is important to obtain high-quality estimate of the current end-effector position, as we found that analytical forward kinematics and odometry on low-cost humanoid robot like Unitree G1 are not accurate. We mitigate this issue by training neural forward models. We train two models. neural forward kinematics model maps that provides an accurate end-effector pose relative to the base. And neural odometry model that provides an accurate base pose relative to the stationary feet. Even after improvements, the tracker would make systematic errors. We further mitigate these errors by adjusting the desired target passed into the tracker based on the current EE pose tracking error, to encourage the tracker to get where we want it to be. Together, these innovations improve the state-of-the-art for end-effector. Our proposed tracker HERO achieves tracking error of 2.5cm compared to 813 cm for previous state-of-the-art [39, 94]. In real-world testing in MOCAP room, our full system achieves an average end-effector tracking error of 2.44cm. Using this performant end-effector tracking policy, we develop modular system for picking up open-vocabulary novel objects in novel everyday environments. This modular system leverages an open-vocabulary perception module to detect and segment the target object using large pre-trained vision models (Grounding DINO 1.5 [48] and SAM-3 [7]). We next use the AnyGrasp model [13] to produce parallel jaw grasps on the candidate object. We retarget them to the Dex3 hand on the Unitree robot. Finally, we use our tracker as low-level controller to achieve the grasp pose. In real-world testing for grasping open-vocabulary object queries in novel environments, our system achieves average success rate of 83.8% across more than 25 daily objects, 10 daily and cluttered scenes kept on diverse table heights. II. RELATED WORKS A. Legged Loco-Manipulation 1) Loco-manipulation via motion tracking: Motion tracking is vital for loco-manipulation, where teleoperation has emerged as one of the primary paradigms for facilitating imitation learning [2, 73, 92]. Inspired by DeepMimic-style tracking [57], He et al. [22, 23], Fu et al. [18], and Cheng et al. [10] pioneer this direction via training motion tracking policy with reinforcement 2 learning (RL) and sim2real transferring, where remarkable locomanipulation results have been obtained on an H1 humanoid platform [79]. Abundant works have been proposed to improve whole-body tracking accuracy [26, 32, 39, 54, 60], agility [15, 24, 30], generalization [9, 42, 51], robustness [41, 94, 96], object interaction [84, 97], and reachability [95]. With generated reference motion, several works have demonstrated success in humanoid loco-manipulation [40, 55]. In the meanwhile, teleoperation systems have been rapidly developed, with which imitation learning has demonstrated remarkable locomanipulation reuslts [5, 45, 50, 59, 67, 83, 91, 93]. 2) Visual loco-manipulation: One line of research focuses on imitation learning, where the visual loco-manipulation data is collected via human teleoperation, and great progress has been made [2, 18, 21, 22, 73]. Another line of research relies on reference state-based policy learning. For example, Liu et al. [46] utilize depth inputs that learn to predict high-level manipulation commands that control low-level policies. Yin et al. [87] propose to generate motions from visual depth inputs, where the robot interacts with objects via tracking generated motions. Our work lies in this category, where our system takes the visual input and outputs the EE pose target for loco-manipulation. More recently, He et al. [25] explores an end-to-end RL-based loco-manipulation visual policy learning and demonstrated great success on cylinder-style objects (e.g., coke can). However, such methods cannot generalize to openvocabulary queries as training assets are rather limited. B. System Identification Real-world robots have intrinsic errors due to hardware inaccuracy. One assumption is that the robots joint has elasticity [16, 71, 72], which makes the joint positions encoded by motors unreliable. Researcher have proposed to conduct system identification to mitigate this sim2real gap [3, 34]. To do this, two mainstream methods are used: online and offline system identification. Online methods aim at learning to compensate for the hardware and dynamic error during inference, where an adaptation model is trained to mitigate possible errors [15, 17, 36, 38, 43, 56, 58, 81, 8890]. The other line of research, instead, utilizes the offline collected data for system identification. For example, ASAP [24] utilizes the offline collected data for dynamic adaptation model training, which enables extremely agile motion tracking. Focusing on the same direction of optimizing motor dynamics, several works have been proposed and achieved promising system identification results [37, 70]. In this work, we propose two residual models that correct the forward kinematics and robot base odometry trained on collected MOCAP data, which can be categorized as an offline system identification method. III. HERO: HUMANOID END-EFFECTOR CONTROL Given desired end-effector pose in the robot frame, the end-effector control policy outputs motor commands for all 29-DOFs of G1 humanoid robot [80] to convey the arm to reach the desired end-effector pose. Note that even the robot doesnt need to take any steps, reaching far away targets it into an upper-body goal. It Fig. 2: HERO is an accurate end-effector control framework. Given an EE goal pose, HERO first uses IK to convert then uses motion planning to generate an upper-body reference trajectory that is tracked via learned tracking policy πt (Sec. III). In addition to reference joints, πt also takes accurate estimates of the residual EE error (obtained via learned neural forward model (Sec. III-B, Sec. III-B2). HERO also employs periodic replanning (Sec. III-C) to adapt to drifts and goal adjustment (Sec. III-D) to mitigate systematic tracking errors. Accurate tracking enables building modular object manipulation systems (Sec. IV, Fig. 3). requires whole-body coordination and balancing (and thus the control of all DOFs): bending at the waist, twisting the torso, or bending down the legs. Our innovation lies in the design of the policy architecture and input/output representations, where we combine classical robotics components with learned ones in novel ways. As our experiments will illustrate, monolithic learning solution, that directly attempts to learn this mapping, fails. The overall design of our tracker is shown in Fig. 2. Given the end-effector target location EE in the robot frame, we first use inverse kinematics to transform it into robot base height R, and upper body joint angles R17 (3 DoF waist plus 2 7 = 14 DoF left and right arm joints). We then use collision-free motion planner [68] that takes the egocentric depth image and robot configurations to compute joint trajectory {qt}T t=1 that conveys the robot arm from the starting configuration q0 to the target configuration q. We then use our proposed whole-body end-effector tracking policy πt to execute this whole-body reaching motion. πt is learned neural network policy (described in Sec. III-A) that outputs joint angles commands for position control, where torque is obtained via PD controller operating at 50Hz. πt itself relies on learned forward model η (described in Sec. III-B) and learned base odometry model ξ (described in Sec. III-B2) to map robot configurations to end-effector poses (rather than forward kinematics, which is significantly less accurate). There is still systematic offset in where the end-effector ends up after πt is executed. We account for this by systematically shifting the desired end-effector pose goal in the opposite direction to further improve tracking accuracy (Sec. III-D). Finally, because πt requires reference 3 Fig. 3: Overall architecture for our proposed modular system for open-vocabulary object grasping. Given free-form natural language text query indicating which object needs to be picked, we use open-vocabulary large vision models (LVMs like Grounding DINO [48] and SAM [7]) to segment out the object of interest and predict parallel jaw grasps (using the AnyGrasp model [13]). We retarget the predicted grasp to Dex-3 hand. We use our proposed whole-body end-effector tracker to convey the robot arm to the predicted grasp before picking up the object. By decomposing action planning (i.e. identifying which object to pick and using what grasp) from action execution (i.e. actual control of the robot), we inherit the strong visual generalization from pre-trained models as well as strong control capabilities for simulated training of the tracking policy. poses {qt}T poses every steps during execution (Sec. III-C). t=0 as input, we replan to recompute the reference A. Whole-body End-Effector Tracking Policy, πt To track the target end-effector pose EE SE(3) defined in the robot frame, our whole-body EE tracking policy πt first obtains reference trajectory {qt}T t=1 and the corresponding reference EE pose trajectory {eet}T t=1 from motion planner. Given the trajectory, the current proprioceptive state st, and other commands, the tracking policy predicts the 29-DoF joint angles that are passed to per-joint PD controllers. 1) Residual-aware end-effector tracking: πt output actions at at time as follows: at = πt (st, ht, qt, Et, vt, st5:t1, at5:t1) , where st is the current proprioception, ht is the reference base height, qt are the reference upper-body joint angles, vt are the linear and angular velocity locomotion commands, and st5:t1, at5:t1 are five time steps of proprioception and action history. The proprioception include the robots joint angles, joint velocities, angular velocity, projected gravity, and roll and pitch encoded from the IMU. We dont use the IMU yaw as it is inaccurate [22]. Et represents the residual pose error between the current and target end-effector pose in the robot frame, i.e., Et = EE(xt) eet, where EE(xt) maps the arm states xt R17 to the endeffector pose EE SE(3), and is the is the inverse pose composition operator.1 (1) 2) Policy architecture: πt is implemented as two threehidden-layer MLPs that decouple whole-body control into upperand lower-body actions. Both networks take the same observation as input and together predict 29-DoF whole-body actions, obtained by combining the upper-body components from one MLP with the lower-body components from the other. 1We use to denote the pose composition operator: T1 T2 = T2 T1 and is the inverse pose composition operator: T1 T2 = 2 T1. 4 3) Training: πt is trained in simulation via reinforcement learning with PPO [65]. We utilize both the AMASS dataset [52] (8K motion sequences) and curated set of typical end-effector targets (8K) that are encountered in everyday object reaching. Specifically, we randomly sample reaching targets with [x, y, z] coordinates defined in robot frame (z axis is computed from the ground better understanding) ranging from [0.1m, 0.5m, 0.65m] to [0.5m, 0.5m, 1.15m], with the yaw orientation randomly sampled from 60 to 60. The reference trajectories are obtained via motion planner that outputs the upper-body and end-effector tracking goals. B. Learned Residual Neural Forward Models 1) Residual neural FK: Our residual neural forward kinematics function, η, learns correction to the output of the analytical forward kinematic function, FK, to output accurate end-effector poses. Specifically, given the current proprioceptive state of one operating arm and waist xt R10 at timestep and output from analytical FK, FK(xt), the final corrected end-effector pose EE(xt) is obtained via pose composition: EE(xt) = FK(xt) η(cid:0)xt, FK(xt)(cid:1). Note that the analytical forward kinematics function FK(xt) uses the robot geometry and coordinate transformations to compute the 6-DoF end-effector pose in the robot base frame. For precise robots, FK is itself quite accurate, however as our experiments will show, FK is inaccurate for our humanoid, necessitating the need for learning correction. 2) Residual Neural Leg Odometry: Different from fixed-base object manipulation tasks, where the robots base is fixed and stable, humanoid robots base floats in the air for whole-body balancing during reaching. This movement makes the original reaching target inaccurate, as the reaching target defined in the robot frame is no longer the same place where the object lies. One might consider using the egocentric visual information for replanning or motion adjustment. However, as shown in Fig. 4, the egocentric view of the robot is too narrow for the robot to see the object when the robots arm and waist movements are Fig. 4: Learned neural forward kinematics model and odometry model. (a) To correct for inaccurate analytical forward kinematics (FK) that maps joint angles and robot geometry to end-effector poses, we design residual neural forward kinematics model (Sec. III-B), η, that predicts corrections tEE to the analytical forward kinematics output. (b) As the humanoids base moves around while reaching the object, the object can go out of view due to large whole-body motions, making closed-loop adjustment from vision infeasible. Thus, it is necessary to accurately estimate base motion. (c) Our residual neural odometry model accurately estimates base odometry from lower body joint states and by assuming that feet remain fixed (Sec. III-B2). & REE large. As result, knowing the robots odometry, which helps in adjusting the reaching goal, is critical. We assume the feet to be static on the ground and use the lower body joint angles to predict the base pose. By assuming the robot ankle joint as the root joint and the robot base as the end-effector, we can compute the base pose using forward kinematics. However, similar to the error in analytical FK for EE, analytical FK to compute the base pose is also inaccurate (see analysis in Sec. V). Similar to our solution for EE, we adopt residual model to mitigate these inaccuracies. The only difference here is that we reason about base pose transformation relative to time step 0, rather than the absolute base pose. Concretely, let yt R6 be the 6DOF state of the left (or right) leg motors. We can get analytical base odometry, i.e. base pose relative to base pose at time step 0, OFK(yt, y0) SE(3), using analytical forward kinematics and SE(3) difference: OFK(yt, y0) = FK(y0) FK(yt). Our residual neural leg odometry model ξ learns the residual: odometry(yt, y0) = OFK(yt, y0) ξ(yt, y0, OFK(yt, y0)). 3) Policy Architecture: η is realized using 3-layer multitransformation layer perceptron and outputs the residual comprising residual rotation and residual translation. One head predicts the residual translation in R3 and another head predicts the first two columns of the residual rotation as vector in R6 following [98]. Similar to the η, ξ is also 3-layer MLP that predicts translation and rotation for the residual transformation between ground truth and analytical FK result OFK (yt, y0). 5 4) Training data and Loss Function: η is trained by collecting data in an MOCAP room using the Optitrack system [1]. Specifically, we deploy tracking policy (it doesnt have to be an accurate tracking policy) to move the robot end-effector across the workspace. We collect the robot state (joint angles) using the motor encoders and the true end-effector pose using the MOCAP system. We employ the Kabsch-Umeyama algorithm [33, 78] to transform the captured MOCAP marker coordinates to the robots end-effector and base pose, which achieves an accurate estimation with < 1.5mm RMSE error (more details can be found in the supplementary). In this fashion, we collect total of 3 hours of data DEE = {T EE t=1 that spans the workspace we want the robot to operate in, we use the first 2 hours of data for training and the last 1 hour of data for offline validation. ξ is also trained on data captured in MOCAP room, following the same protocol: 2 hours of training data and 1 hour of validation data. As ξ needs temporal pairs, here we create training data by sampling pairs of time steps from temporal trajectories. , xt}N Given data DEE, and the residual transformation for η defined as EE = EE FK(xt), we use decoupled representation R3, and the orien- , where the translation is TEE for EE R6 is the first two columns of the orientation tation REE transformation matrix [98]. The residual neural FK model, η (cid:17) (cid:16) η(cid:0)xt, FK(xt)(cid:1), EE is trained using the MSE loss: LMSE . Similarly, ξ utilize the same decoupled continuous representaξ(cid:0)ym, yn, OFK tion and MSE loss LMSE , where 0 are two timestamps randomly sampled from collected temporal sequence. (cid:1), Onm nm (cid:17) (cid:16) TABLE I: Success rate for the end-to-end open-vocabulary grasping task on novel objects on general and short tables in the real world. Our proposed system achieves 90% success rate at this task across objects placed at two different heights. Language Query red coke can emergency stop button red piranha plant orange cube olive oil bottle game cartridge chip can hand soap bottle robot hand red apple Success Rate Short Table General Table Height: 0.74 Height: 0.56m 3/3 3/3 3/3 3/3 2/3 2/3 2/3 3/3 3/3 3/3 3/3 3/3 3/3 3/3 2/3 3/3 3/3 3/3 2/3 2/ Total 27/30 27/30 robot. Finally, we use our proposed HERO tracker to convey the robot end-effector to the predicted location. Specifically, to map parallel gripper grasps to the Dex3 hand, we rotate the AnyGrasp output pose by 45 around the z-axis, enabling the thumb to form one jaw while the other two fingers form the opposing jaw. This configuration provides larger contact areas and better force closure than straight finger extension, improving grasp robustness and pose error tolerance. We also clip the end-effector orientation within 70 to ensure natural full-body motion. Excessive rotations cause twisted upper-body postures from inverse kinematics, degrading EE tracking precision. V. EXPERIMENTS We design experiments to test the performance of our overall system as well as the effectiveness of each individual module. Specifically, we seek answers to the following questions: a) what matters for building an accurate end-effector tracking system for humanoids? b) how does the end-effector tracking performance vary depending on the target reaching location? c) what is the extent of error in forward kinematics and can learnt model successfully mitigate it? d) to what extent can modular system enable humanoid to manipulate novel objects in novel environments in the real world? A. Experimental Setup All our experiments use an unmodified Unitree G1 Humanoid robot with Dex-3 hands. We use the inputs from the headmounted RGB-D camera (Intel D435i) and proprioception (dof angles, velocities), and base-mounted IMU for our system. Experiments involving visual sensing are all conducted in novel environments on novel objects in the real world. Experiments that assess the quality of tracking or the accuracy in forward kinematics are done in MOCAP room equipped with 13 Optitrack cameras (more details can be found in Fig. 5: Novel test environments and novel test objects used for end-to-end testing of our proposed humanoid openvocabulary object grasping system. (a-b) Standard table (0.74m) and short table (0.56m) setups; note that for short table the robot would first squat down by 15cm the motion planner suggested. (c) 10 daily objects with different shapes, physical properties, appearances, etc (details can be found in Sec. B.7). C. Replanning Over the course of execution, the robot may drift sufficiently far from the reference joint trajectory that the tracking policy πt is being asked to track. This could lead to poor tracking performance because of the input being out-of-distribution or the tracking target simply being too far away. We mitigate this by replanning the reference motion every = 300 time steps (6 seconds). We replan using the same motion planner [68] as before (Sec. III). replanning at time updates the remaining {qt}T and takes 20ms. and {eet}T D. Goal Adjustment Finally, due to the sim2real mismatch there may still be systematic gap between where the policy is commanded to go to and where the policy ends up at. To eliminate these systematic errors, we adjust the target in the direction opposite to the current error to encourage the policy to get to where we want it to get to. We only modify the Et that is being input into the policy. We simply scale up the current error by factor of α = 1.6. We only scale up the translation error vector as we found no benefit from also scaling up the rotation error. The goal adjustment starts when the EE tracking error Et 0.15m, and stops when the Et 0.02m, which helps stabilize the robot when sufficiently close. IV. MODULAR SYSTEM FOR HUMANOID OBJECT GRASPING Our overall task of picking up novel open-vocabulary objects in novel environments. Specifically, given humanoid robot that is standing at table, the goal is for it to pick up objects described by free-form natural language query only using its onboard egocentric sensors. We design modular system for this task by building on top of our HERO tracker from Sec. III. Concretely, we first segment out the object corresponding to the query using Grounding DINO [48]. We next use an AnyGrasp model [13] to produce parallel jaw grasps on the candidate object. We filter the predicted grasps based on how parallel to the table they are. We retarget the selected grasp to the Dex3 hand on the Unitree 6 Fig. 6: Success rate for the end-to-end open-vocabulary grasping task on novel objects in (a) broader and (b) cluttered novel scenes in the real world. (a) We test HERO in 10 daily scenes on 10 new daily objects, such as office lounge and coffee shops. HERO achieves an overall 22/30 (73.3%) success rate, demonstrating strong scene generalization capability. Details of tested locations can be found in Sec. B.8. (b) We also test HERO in 5 random cluttered scenes with different layouts. HERO achieves an overall 12/15 (80%) success rate, demonstrating the generalization capability in using language as an accurate proxy for manipulating objects in cluttered scenes. Sec. B.1). Finally, some design choices are validated in simulation using Issac Gym [53] and MoJuCo [75]. B. End-to-end System Testing Our end-to-end system test starts with the humanoid robot standing 10-20cm in front of tables of varying heights (43cm 92cm) and the goal for the robot is to pick up an object described via natural language. Fig. 5 shows the test objects, and Tab. lists the open-vocabulary text queries we experimented with. The robot needs to operate entirely using its onboard sensors: the head-mounted RGB-D camera, proprioception, and base-mounted IMU sensor. During trials, objects are placed randomly in the area reachable by the robot. trial is deemed successful is the robot is able to pick up the correct object and lift it off the table for more than 2 seconds. We conduct 3 trials per object per table height. 10 Daily Objects. Tab. reports the success rate at this endto-end test. Across the different open world queries, two table heights, and objects, our system achieves success rate of 90%, demonstrating the effectiveness of our modular system design. This establishes the effectiveness of modular system for open-vocabulary object grasping with humanoid robot, previously thought impossible for humanoid robot. It also serves as the first demonstration of open-vocabulary grasping capability on humanoid robots operating under whole-body control. 10 Daily Scenes. Fig. 6(a) plots the success rate on broader novel daily scenes and objects for generalization evaluation. We test our system across various scenes like robot lab and classroom, and the results show that HERO achieves an overall 73.3% success rate, demonstrating remarkable scene and object generalization capability. 5 Cluttered Layouts. To further evaluate the open-vocabulary capability and robustness of our system, we test HERO on grasping objects from cluttered layouts, shown in Fig. 6(b). HERO achieves an 80% success rate, which demonstrates that our open-vocabulary system is language-sensitive and can reliably grasp objects in such challenging cluttered scenes without any human demonstration. C. Forward Kinematics vs. Learned Forward Kinematics We assess the accuracy of analytical forward kinematics on the G1 humanoid robot. We conduct this test in MOCAP room. We affix tracking markers to the robot base and the robot end-effector, and use the relative transformation between the robot base and the robot end-effector as the ground truth. We execute reaching policy and record the end-effector positions (from MOCAP, to use as ground truth) and the corresponding joint angles. We use the joint angles either with the analytical forward kinematics or with our learned forward kinematics model to obtain predicted end-effector poses. End-Effector Pose. Tab. II reports the translation and orientation errors between the predicted end-effector poses and the ground truth poses measured using MOCAP. We average over 60 samples captured in diverse poses in the robots workspace. As we can see, analytical forward kinematics is off by as much as 1.76cm. At the same time, our learned model is much better and achieves an error of 0.27cm. The fact that learned 7 TABLE II: Inaccuracies in analytical forward kinematics and how our learned model mitigates them. a) We report mean translation and rotation error in the estimate of endeffector position via different methods. Ground truth comes from MOCAP measurements. Analytical FK (i.e. using robot joint angles along with robot geometry) is inaccurate for our humanoid robot causing 1.76cm translation error in the endeffector pose. Our proposed learned residual forward kinematics model is able to mitigate these errors with residual design being more effective. b) Bottom part of the table reports metrics for base odometry and exhibits the same trends. Method Translation Error (cm) Rotation Error (deg) a) End-effector Pose Analytical FK Learned FK (ours) Learned FK, no residual (ours) b) Base Odometry Analytical FK Learned FK (ours) Learned FK, no residual (ours) 1.76 0.27 0.35 1.10 0.33 0.37 5.87 2.30 2.98 0.49 0.36 0.42 model reduces the error by 6 indicates that analytical forward kinematics model errors are systematic. Tab. II also reports an ablation where we directly try to predict the end-effector pose without using the estimate from analytical FK and note that it does worse. Base Odometry. Tab. II also reports base odometry results. Once again, we see that the residual neural model is better than both analytical FK and non-residual neural model. Execution Curves. We compare analytical and neural forwardmodel errors over time in Fig. 7. While analytical FK exhibits persistent bias above 1.75 cm during reaching, our neural model η remains below 0.25 cm throughout. Similarly, as whole-body balancing causes analytical odometry drift to grow, our neural odometry model ξ reduces this drift by about 3 relative to the analytical baseline. D. End-effector Tracking Accuracy Evaluation We evaluate the performance of different tracking methods in simulation and in the real world under MOCAP. We evaluate different tracking methods on fixed set of 180 reaching goals. The goal distribution is designed to reflect realistic grasping scenarios: we sample from three different table heights, with z-coordinates ranging from 5-15cm above each surface and x-y positions uniformly distributed across the table workspace. This yields 60 poses per table height (180 total), capturing the typical distribution of manipulation targets in everyday environments. Metrics. We measure the translation and rotation errors in the end-effector position as the primary metrics. We report the mean and std of the errors. We use end-effector position as measured under MOCAP to compute these metrics. We also report the joint tracking error for the upper body joints. Here Fig. 7: Impact of neural forward models in the real world. We plot the neural and analytical forward kinematics and base odometry as function of execution time steps. The plot shows 1 minute of execution at 50Hz. TABLE III: End-effector tracking evaluation against prior methods in simulation (Sec. V-D1). We report end-effector translation and rotation errors and the upper body joint tracking error. Across all three table heights, our proposed HERO tracker achieves the lowest end-effector tracking error as compared to recent tracking methods FALCON [94] and AMO [39]. On average, our translation errors are 3.2 lower than the best baseline. Method Translation Error (cm) Orientation Error (deg) Joint Tracking Error (rad) General Table (H: 0.74m) FALCON [94] AMO [39] HERO (ours) 16.07 4.00 11.12 3.58 14.38 5.86 8.32 3.41 2.21 0.58 10.85 3.95 0.02 0.00 0.02 0.00 0.16 0.03 Tall Table (H: 0.88m) FALCON [94] AMO [39] HERO (ours) 6.80 2.51 8.46 4.17 3.30 1. Low Table (H: 0.5m) FALCON [94] AMO [39] HERO (ours) 22.80 7.15 8.10 3.87 1.92 0.96 13.30 3.02 14.04 5.16 8.93 3.06 0.03 0.01 0.02 0.00 0.20 0.04 28.63 8.87 13.12 6.70 13.92 4.91 0.02 0.00 0.02 0.00 0.16 0. we use measurements from the motor encoders. Comparisons. We compare against AMO [39] and FALCON [94], two recent tracking methods that report impressive tracking performance. We retrain both policies on the same end-effector targets that our policy is trained on. This mitigates any bias due to mismatch in training distributions between the different policies. In addition, we also compare to ablated versions of our method. Specifically, we ablate the effect of forward model quality (Sec. III-B and Sec. III-B2), re-planning (Sec. III-C), and goal adjustment (Sec. III-D). Results. We discuss the results and our key takeaways. 1) Comparisons against state-of-the-art.: Tab. III reports the tracking accuracy of HERO against AMO [39] and 8 components are important with the replanning component being more important than the goal adjustment component. Execution Curves. We further analyze how the error evolves as the execution proceeds with and without replanning in Fig. 8. The cyan line that doesnt use replanning plateaus off at much higher error, whereas the blue line with replanning achieves much lower overall error. In addition, the close performance of HERO using MOCAP observations and learned neural forward models demonstrates that our learned forward models are feasible and accurate. This ensures the possibility of transferring HERO to anywhere outside the ideal experimental space using MOCAP, making HERO useful real-world system. TABLE IV: Learned forward models improve end-effector tracking in the real world (Sec. V-D2). We study the impact of using different estimates, MOCAP vs. Analytical Forward Kinematics (FK) vs. our neural model, for end-effector (EE) pose and base pose. Use of our proposed neural forward models improves end-effector tracking accuracy over using analytical forward kinematics and achieves comparable errors to an oracle version that uses estimates from MOCAP (last row). EE Pose FK Ours FK Ours Base Pose FK FK Ours Ours Trans. Error Orient. Error (cm) (deg) Joint Error (rad) 0.20 0.03 14.59 3.99 4.67 1.30 0.19 0.03 14.07 3.93 3.35 0.70 3.89 1.06 0.20 0.04 14.28 4.75 2.56 1.23 12.06 4.38 0.18 0.03 MoCap MoCap 2 .44 0 .86 14 .29 4 .55 0 .21 0 .05 TABLE V: Importance of Replanning (Sec. III-C) and Goal Adjustment (Sec. III-D) in the real world. Both goal adjustment and replanning lead to improvement in end-effector tracking accuracy, with replanning being more important. All methods in this table use accurate end-effector and base pose from MOCAP. Method Trans. Error Orient. Error Joint Error (deg) (rad) (cm) 5.17 2.21 w/o Replan w/o Goal Adjustment 2.71 0.87 HERO (full) 16.13 4.66 0.21 0.03 9.38 2.72 0.20 0.03 2.44 0.86 8.22 3.52 0.21 0.05 FALCON [94] in simulation across 3 different table heights: 0.5m, 0.74m, and 0.88m. HERO obtains much lower endeffector translation and rotation error. Average translation error for HERO is 2.48cm compared to 8.29cm for AMO [39] and 13.57cm for FALCON [94]. Interestingly, HERO has larger joint tracking error but much lower end-effector tracking error. Unlike baseline methods that optimize purely in joint space, HERO leverages end-effector shift observations to directly improve task-space accuracy, demonstrating that minimizing joint error does not guarantee optimal end-effector positioning. 2) Importance of Accurate Forward Kinematics: Tab. IV reports control experiments where we replace the end-effector and base pose to be from analytical forward kinematics or MOCAP rather than our learned model. Using accurate endeffector and base pose from our learned models (denoted by ours) leads to the lowest errors. These errors are to those of an oracle that uses ground truth estimates from MOCAP (last row). Using FK instead of our learned model for either the end-effector or the base leads to worse performance. 3) Importance of Replanning and Goal Adjustment: We next assess the importance of replanning and goal adjustment by removing each of these components one at time. To maximally isolate the effect of these components, we conduct this experiment where all methods have access to end-effector and base pose from MOCAP. As reported in Tab. V, both Fig. 8: Impact of replanning on end-effector tracking error in the real world. We plot the end-effector translation error as function of execution time steps. The plot shows 1 minute of execution at 50Hz. The transparent lines are individual 60 real-world rollouts, and the corresponding solid line indicates the average value. The gray vertical dashlines indicate the replanning every 6 seconds (0.15Hz). Cyan line shows HERO without replanning and while purple line shows HERO with replanning. Re-planning leads to more accurate tracking. Orange line uses end-effector estimates from our neural model which leads to tracking performance very close to the oracle purple line that uses end-effector estimates from MOCAP. E. Failure Mode Analysis Fig. 9 categorizes the observed failures into two distinct modes, both emphasizing the need for precise end-effector control. The first mode, object slipping (Fig. 9(a)), arises when the limited dexterity of the Dex-3 hand fails to secure large, irregularly shaped objects. The second mode, object knocked over (Fig. 9(b)), reveals the importance of feasible grasping orientation and the limitation of the Dex-3 hand that has simply too large fingers. When interacting with unstable items like standing books, the margin for error is minimal; an insufficient hand orientation retargeting during reaching can trigger collision. This highlights stringent accuracy requirement for an appropriate retargeting strategy that helps stable and accurate grasping with the clumsy Dex-3 hand. 9 the waists control to bend and twist the torso, which effectively repositions the shoulder frame and allows the EE to cover farther-forward and lower-height targets that are infeasible with non-actuated waist. 2) Workspace Showcasing: Fig. 10 illustrates HERO retrieving various objects distributed across an expansive tabletop workspace. Since every object is positioned beyond 0.4m to the robot base, the task requires whole-body coordination to maximize reachability and precision. As shown, HERO enables the robot to coordinate expressive whole-body motion while maintaining the necessary precision for successful grasping. Fig. 10: HERO enables humanoid picking up objects from standard table (0.74m) across large workspace with open-vocabulary queries. (a-c) The robot can reach and pick up red apple placed at different heights, poses, and locations. VI. LIMITATIONS This work proposes HERO, which learns to control humanoid end-effectors, with which open-vocabulary visual locomanipulation task is targeted. While HERO achieves remarkable results, we identify the following potential limitations. i) The egocentric field of view (FoV) under current hardware setup is rather limited, making it difficult for the humanoid to see objects farther than 1m or higher than 0.9m. Besides, as stated in the paper, the humanoid may twist the whole body lot, which makes the object disappear from egocentric vision. One potential is to enable neck DoFs that achieves active vision and action [61, 93], which enables the robot to achieve improved closed-loop reasoning from vision. ii) HERO relies on classical motion planner [68], which may lead to extremely twisted motion that is not an optimal or energy-efficient choice for loco-manipulation. One potential is to leverage learning-based prior for trajectory optimization, which may help unleash more energy-efficient locomotion. iii) As HERO is modular system, the limitations of these systems also apply. For example, the failure may come from submodules like the LVMs when the operating environment is too complicated [20, 47]. iv) The current system is limited in dexterity as we use Dex-3 hand. With the rapid progress Fig. 9: Failure mode examples. We identify two main failure modes: a) object slipping out during grasping, which happens when grasping large objects with irregular shapes that are challenging for Dex-3 hand endowed with limited dexterity; b) object knocked over during reaching, which happens when the retargeted hand orientation is not sufficiently large while the object stands unstably (e.g., game cartridge or book that has thin ridge). TABLE VI: Reachable workspace volume across configurations. Configuration Single Arm (m3) Combined (LUR) (m3) Arms-only (14 DoFs) Arms+Waist (17 DoFs) 0.166 0.426 0.248 0.523 F. Whole Body Reaching Space Analysis 1) Importance of Waist Bending: To quantify how enabling torso motion via the waist bending affects end-effector reachability, we estimate the EE workspace under two kinematic settings: i) arms-only, where IK optimizes the 14-DoF arm joints, and ii) arms+waist, where IK additionally optimizes the waist DoFs (17 DoFs total). We define an axis-aligned 3D candidate region in the robot base frame: [0, 1.0] m, [1.0, 1.0] m, [0.5, 1.0] (2) and uniformly discretize it with grid resolution of 0.02 m. For each sampled point p, we test feasibility by solving inverse kinematics with cuRobo [68] under joint-limit constraints. point is marked reachable if the IK solver converges within fixed iteration budget and achieves an EE position residual below preset tolerance; otherwise, it is marked unreachable. The reachable workspace volume is approximated via voxel counting: Nreach (0.02)3, (3) where Nreach denotes the number of reachable grid points. The results are shown in Tab. VI. Quantitatively, enabling waist DoFs substantially increases workspace volume. The combined workspace (LUR) grows from 0.248 m3 in the armsonly setting to 0.523 m3 with arms+waist, corresponding to 2.1 increase. Similarly, the single-arm workspace increases from 0.166 m3 to 0.426 m3. This gain is primarily attributed to 10 in tackling the hand dexterity challenge, there exists great potential to leverage better embodiment for more dexterous whole-body dexterous manipulation, which remains critical challenge in robotics research. VII. DISCUSSIONS We designed system that enables humanoid robot to grasp open-vocabulary novel objects in novel environments. This is challenging task as it requires strong visual generalization (grounding open-vocabulary queries into robots egocentric perception) as well as strong control performance (squatting, bending, twisting to reach for objects while maintaining stability). Our system achieves 90% success rate in the real world on this challenging task. This was made possible by adopting decomposed design that separated action planning (ingesting RGB-D images to select specified objects and predict grasps for them) from action execution (actually conveying the robot to the predicted grasps). The separation enabled the use of large pre-trained models for action planning, leading to strong generalization and open-vocabulary capabilities. At the same time, action execution benefited from large-scale simulated training. Overall, we demonstrated that there is more scalable alternative to the current practice of real world imitation learning for getting humanoid robots to rearrange objects. Along the way, we encountered number of surprising findings. It turned out, forward kinematics on humanoid robot isnt accurate and leads to the end-effector pose estimates that are off by 1.76cm on average. However, the error are systematic. This enabled the design of residual neural forward model that reduced the error to 0.27cm. Second, even though many recent works produce wholebody humanoid trackers, it turns out they incur large (8 13cm) end-effector tracking errors. This precludes the use of such trackers for manipulation applications. However, as we demonstrated, by careful design, particularly one that combines classical robotics (forward kinematics, inverse kinematics, and motion planning) with learning, it is possible to build accurate end-effector tracking policies. Our tracking policies achieve an end-effector tracking error rate of 2.5cm in the real world. Finally, we believe that this successful demonstration of modular system that enables humanoid robot to manipulate objects will unlock the unification of the work on manipulation and humanoid control. Being able to accurately control humanoid end-effectors would mean that manipulation policies trained for other platforms (e.g., via UMI [11, 21]) can be readily deployed on versatile humanoids. ACKNOWLEDGMENTS We are grateful to the Coordinated Science Laboratory and the Center for Autonomy for their help with experimental space and the MOCAP room, and for lending us the G1 humanoid robot used in this work. This work was supported by an NSF CAREER Award (IIS-2143873). REFERENCES [1] Optitrack. URL https://optitrack.com/. 5, 19 [2] Helix: vision-language-action model for generalist humanoid control, 2025. URL https://www.figure.ai/news/ helix. 2, 3 [3] Karl Johan Astrom and Peter Eykhoff. System identificationa survey. Automatica, 7(2):123162, 1971. 3 [4] Jose Barreiros, Andrew Beaulieu, Aditya Bhat, Rick Cory, Eric Cousineau, Hongkai Dai, Ching-Hsin Fang, Kunimatsu Hashimoto, Muhammad Zubair Irshad, Masha Itkina, et al. careful examination of large behavior arXiv models for multitask dexterous manipulation. preprint arXiv:2507.05331, 2025. 2 [5] Qingwei Ben, Feiyu Jia, Jia Zeng, Junting Dong, Dahua Lin, and Jiangmiao Pang. HOMIE: Humanoid LocoManipulation with Isomorphic Exoskeleton Cockpit. In Proceedings of Robotics: Science and Systems, LosAngeles, CA, USA, June 2025. 3 [6] Gary Bradski. The opencv library. Dr. Dobbs Journal: Software Tools for the Professional Programmer, 25(11): 120123, 2000. 19 [7] Nicolas Carion, Laura Gustafson, Yuan-Ting Hu, Shoubhik Debnath, Ronghang Hu, Didac Suris Coll-Vinent, Chaitanya Ryali, Kalyan Vasudev Alwala, Haitham Khedr, Andrew Huang, Jie Lei, Tengyu Ma, Baishan Guo, Arpit Kalla, Markus Marks, Joseph Greer, Meng Wang, Peize Sun, Roman Radle, Triantafyllos Afouras, Effrosyni Mavroudi, Katherine Xu, Tsung-Han Wu, Yu Zhou, Liliane Momeni, RISHI HAZRA, Shuangrui Ding, Sagar Vaze, Francois Porcher, Feng Li, Siyuan Li, Aishwarya Kamath, Ho Kei Cheng, Piotr Dollar, Nikhila Ravi, Kate Saenko, Pengchuan Zhang, and Christoph Feichtenhofer. SAM 3: Segment anything with concepts. In The Fourteenth International Conference on Learning Representations, 2026. 2, 4, 18, [8] Yiming Chen, Zekun Qi, Wenyao Zhang, Xin Jin, Li Zhang, and Peidong Liu. Reasoning in space via grounding in the world. In The Fourteenth International Conference on Learning Representations, 2026. 17 [9] Zixuan Chen, Mazeyu Ji, Xuxin Cheng, Xuanbin Peng, Xue Bin Peng, and Xiaolong Wang. Gmt: General motion tracking for humanoid whole-body control. arXiv preprint arXiv:2506.14770, 2025. 3 [10] Xuxin Cheng, Yandong Ji, Junming Chen, Ruihan Yang, Ge Yang, and Xiaolong Wang. Expressive Whole-Body Control for Humanoid Robots. In Proceedings of Robotics: Science and Systems, Delft, Netherlands, July 2024. 2 [11] Cheng Chi, Zhenjia Xu, Chuer Pan, Eric Cousineau, Benjamin Burchfiel, Siyuan Feng, Russ Tedrake, and Shuran Song. Universal Manipulation Interface: In-TheWild Robot Teaching Without In-The-Wild Robots. In Proceedings of Robotics: Science and Systems, Delft, Netherlands, July 2024. 11 [12] Murtaza Dalal, Min Liu, Walter Talbott, Chen Chen, Deepak Pathak, Jian Zhang, and Ruslan Salakhutdinov. 11 Local policies enable zero-shot long-horizon manipulation. In 2025 IEEE International Conference on Robotics and Automation (ICRA), pages 1387513882. IEEE, 2025. 2 [13] Hao-Shu Fang, Chenxi Wang, Hongjie Fang, Minghao Gou, Jirong Liu, Hengxu Yan, Wenhai Liu, Yichen Xie, and Cewu Lu. Anygrasp: Robust and efficient grasp perception in spatial and temporal domains. IEEE Transactions on Robotics, 39(5):39293945, 2023. 2, 4, 6, [14] Hao-Shu Fang, Hengxu Yan, Zhenyu Tang, Hongjie Fang, Chenxi Wang, and Cewu Lu. Anydexgrasp: General dexterous grasping for different hands with human-level learning efficiency. arXiv preprint arXiv:2502.16420, 2025. 18 [15] Nolan Fey, Gabriel B. Margolis, Martin Peticco, and Pulkit Agrawal. Bridging the Sim-to-Real Gap for Athletic LocoManipulation. In Proceedings of Robotics: Science and Systems, LosAngeles, CA, USA, June 2025. 3 [16] Mirjana Filipovic, Veljko Potkonjak, and Miomir Vukobratovic. Elasticity in humanoid robotics. Scientific Technical Review, Military Technical Institute, Belgrade, 1:2433, 2007. 3 [17] Zipeng Fu, Xuxin Cheng, and Deepak Pathak. Deep whole-body control: Learning unified policy for manipulation and locomotion. In Karen Liu and Dana Kulic andD Jeffrey Ichnowski, editors, Conference on Robot Learning, CoRL 2022, 14-18 December 2022, Auckland, New Zealand, volume 205 of Proceedings of Machine Learning Research, pages 138149. PMLR, 2022. 3 [18] Zipeng Fu, Qingqing Zhao, Qi Wu, Gordon Wetzstein, and Chelsea Finn. Humanplus: Humanoid shadowing and imitation from humans. In 8th Annual Conference on Robot Learning, 2024. 2, 3 [19] Sergio Garrido-Jurado, Rafael Munoz-Salinas, Francisco Jose Madrid-Cuevas, and Manuel Jesus MarınJimenez. Automatic generation and detection of highly reliable fiducial markers under occlusion. Pattern Recognition, 47(6):22802292, 2014. 19 [20] Arjun Gupta, Michelle Zhang, Rishik Sathua, and Saurabh Gupta. Demonstrating MOSART: Opening Articulated Structures in the Real World. In Proceedings of Robotics: Science and Systems, LosAngeles, CA, USA, June 2025. 10, [21] Huy Ha, Yihuai Gao, Zipeng Fu, Jie Tan, and Shuran Song. Umi-on-legs: Making manipulation policies mobile with manipulation-centric whole-body controllers. In Conference on Robot Learning, pages 52545270. PMLR, 2025. 3, 11 [22] Tairan He, Zhengyi Luo, Xialin He, Wenli Xiao, Chong Zhang, Weinan Zhang, Kris M. Kitani, Changliu Liu, and Guanya Shi. Omnih2o: Universal and dexterous humanto-humanoid whole-body teleoperation and learning. In 8th Annual Conference on Robot Learning, 2024. 2, 3, 4, 20 [23] Tairan He, Zhengyi Luo, Wenli Xiao, Chong Zhang, Kris Kitani, Changliu Liu, and Guanya Shi. Learning human-to-humanoid real-time whole-body teleoperation. In IEEE/RSJ International Conference on Intelligent Robots and Systems, IROS 2024, Abu Dhabi, United Arab Emirates, October 14-18, 2024, pages 89448951. IEEE, 2024. 2 [24] Tairan He, Jiawei Gao, Wenli Xiao, Yuanhang Zhang, Zi Wang, Jiashun Wang, Zhengyi Luo, Guanqi He, Nikhil Sobanbabu, Chaoyi Pan, Zeji Yi, Guannan Qu, Kris Kitani, Jessica K. Hodgins, Linxi Fan, Yuke Zhu, Changliu Liu, and Guanya Shi. ASAP: Aligning Simulation and RealWorld Physics for Learning Agile Humanoid Whole-Body Skills. In Proceedings of Robotics: Science and Systems, LosAngeles, CA, USA, June 2025. 2, 3, 20 [25] Tairan He, Zi Wang, Haoru Xue, Qingwei Ben, Zhengyi Luo, Wenli Xiao, Ye Yuan, Xingye Da, Fernando Castaneda, Shankar Sastry, et al. Viral: Visual sim-to-real at scale for humanoid loco-manipulation. arXiv preprint arXiv:2511.15200, 2025. [26] Tairan He, Wenli Xiao, Toru Lin, Zhengyi Luo, Zhenjia Xu, Zhenyu Jiang, Jan Kautz, Changliu Liu, Guanya Shi, Xiaolong Wang, et al. Hover: Versatile neural wholeIn 2025 IEEE body controller for humanoid robots. International Conference on Robotics and Automation (ICRA), pages 99899996. IEEE, 2025. 3 [27] Xialin He, Runpei Dong, Zixuan Chen, and Saurabh Gupta. Learning Getting-Up Policies for Real-World Humanoid Robots. In Proceedings of Robotics: Science and Systems, LosAngeles, CA, USA, June 2025. 2, 20 [28] Haoxu Huang, Fanqi Lin, Yingdong Hu, Shengjie Wang, and Yang Gao. Copa: General robotic manipulation through spatial constraints of parts with foundation models. In 2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 94889495. IEEE, 2024. 17 [29] Tao Huang, Junli Ren, Huayi Wang, Zirui Wang, Qingwei Ben, Muning Wen, Xiao Chen, Jianan Li, and Jiangmiao Pang. Learning Humanoid Standing-up Control across Diverse Postures. In Proceedings of Robotics: Science and Systems, LosAngeles, CA, USA, June 2025. 2 [30] Tao Huang, Huayi Wang, Junli Ren, Kangning Yin, Zirui Wang, Xiao Chen, Feiyu Jia, Wentao Zhang, Junfeng Long, Jingbo Wang, and Jiangmiao Pang. Towards adaptable humanoid control via adaptive motion tracking. In 2026 IEEE International Conference on Robotics and Automation (ICRA), 2026. 3 [31] Wenlong Huang, Chen Wang, Yunzhu Li, Ruohan Zhang, and Li Fei-Fei. Rekep: Spatio-temporal reasoning of relational keypoint constraints for robotic manipulation. In Conference on Robot Learning, pages 45734602. PMLR, 2025. 17 [32] Mazeyu Ji, Xuanbin Peng, Fangchen Liu, Jialong Li, Ge Yang, Xuxin Cheng, and Xiaolong Wang. Exbody2: Advanced expressive humanoid whole-body control. In RSS 2025 Workshop on Whole-body Control and Bimanual Manipulation: Applications in Humanoids and Beyond. 3 [33] Wolfgang Kabsch. solution for the best rotation to 12 relate two sets of vectors. Foundations of Crystallography, 32(5):922923, 1976. 5, 19 humanoid control via guided diffusion. arXiv preprint arXiv:2508.08241, 2025. 2 [34] Kozin and HG Natke. System identification techniques. Structural safety, 3(3-4):269316, 1986. 3 [35] Eric Krotkov, Douglas Hackett, Larry Jackel, Michael Perschbacher, James Pippine, Jesse Strauss, Gill Pratt, and Christopher Orlowski. The darpa robotics challenge finals: Results and perspectives. In The DARPA robotics challenge finals: Humanoid robots to the rescue, pages 126. Springer, 2018. [36] Ashish Kumar, Zipeng Fu, Deepak Pathak, and Jitendra Malik. RMA: rapid motor adaptation for legged robots. In Robotics: Science and Systems XVII, Virtual Event, July 12-16, 2021, 2021. 3 [37] Joonho Lee, Alexey Dosovitskiy, Dario Bellicoso, Vassilios Tsounis, Vladlen Koltun, and Marco Hutter. Learning agile and dynamic motor skills for legged robots. Sci. Robotics, 4(26), 2019. 3 [38] Kuang-Huei Lee, Ofir Nachum, Tingnan Zhang, Sergio Guadarrama, Jie Tan, and Wenhao Yu. PI-ARS: accelerating evolution-learned visual-locomotion with predictive information representations. In IEEE/RSJ International Conference on Intelligent Robots and Systems, IROS 2022, Kyoto, Japan, October 23-27, 2022, pages 14471454. IEEE, 2022. 3 [39] Jialong Li, Xuxin Cheng, Tianshu Huang, Shiqi Yang, Ri-Zhao Qiu, and Xiaolong Wang. AMO: Adaptive Motion Optimization for Hyper-Dexterous Humanoid Whole-Body Control. In Proceedings of Robotics: Science and Systems, LosAngeles, CA, USA, June 2025. 2, 3, 8, 9, 16 [40] Jinhan Li, Yifeng Zhu, Yuqi Xie, Zhenyu Jiang, Mingyo Seo, Georgios Pavlakos, and Yuke Zhu. Okami: Teaching humanoid robots manipulation skills through single video imitation. In Conference on Robot Learning, pages 299 317. PMLR, 2025. 3 [41] Yitang Li, Yuanhang Zhang, Wenli Xiao, Chaoyi Pan, Haoyang Weng, Guanqi He, Tairan He, and Guanya Shi. Hold my beer: Learning gentle humanoid locomotion and end-effector stabilization control. In Conference on Robot Learning, pages 45064523. PMLR, 2025. [42] Yitang Li, Zhengyi Luo, Tonghe Zhang, Cunxi Dai, Anssi Kanervisto, Andrea Tirinzoni, Haoyang Weng, Kris Kitani, Mateusz Guzek, Ahmed Touati, Alessandro Lazaric, Matteo Pirotta, and Guanya Shi. BFM-zero: promptable behavioral foundation model for humanoid control using unsupervised reinforcement learning. In The Fourteenth International Conference on Learning Representations, 2026. 3 [43] Zhongyu Li, Xue Bin Peng, Pieter Abbeel, Sergey Levine, Glen Berseth, and Koushil Sreenath. Reinforcement learning for versatile, dynamic, and robust bipedal locomotion control. Int. J. Robotics Res., 44(5):840888, 2025. 3 [44] Qiayuan Liao, Takara Truong, Xiaoyu Huang, Yuman Gao, Guy Tevet, Koushil Sreenath, and Karen Liu. Beyondmimic: From motion tracking to versatile [45] Fukang Liu, Zhaoyuan Gu, Yilin Cai, Ziyi Zhou, Hyunyoung Jung, Jaehwi Jang, Shijie Zhao, Sehoon Ha, Yue Chen, Danfei Xu, et al. Opt2skill: Imitating dynamicallyfeasible whole-body trajectories for versatile humanoid loco-manipulation. IEEE Robotics and Automation Letters, 2025. 3 [46] Minghuan Liu, Zixuan Chen, Xuxin Cheng, Yandong Ji, Ri-Zhao Qiu, Ruihan Yang, and Xiaolong Wang. Visual In whole-body control for legged loco-manipulation. Conference on Robot Learning, pages 234257. PMLR, 2025. 3 [47] Peiqi Liu, Yaswanth Orru, Jay Vakil, Chris Paxton, Nur Muhammad (Mahi) Shafiullah, and Lerrel Pinto. Demonstrating ok-robot: What really matters in integrating openknowledge models for robotics. In Dana Kulic, Gentiane Venture, Kostas E. Bekris, and Enrique Coronado, editors, Robotics: Science and Systems XX, Delft, The Netherlands, July 15-19, 2024, 2024. 2, 10, [48] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Qing Jiang, Chunyuan Li, Jianwei Yang, Hang Su, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. In European conference on computer vision, pages 3855. Springer, 2024. 2, 4, 6, 20 [49] Ilya Loshchilov and Frank Hutter. Decoupled weight In International Conference on decay regularization. Learning Representations, 2019. 20 [50] Chenhao Lu, Xuxin Cheng, Jialong Li, Shiqi Yang, Mazeyu Ji, Chengjing Yuan, Ge Yang, Sha Yi, and Xiaolong Wang. Mobile-television: Predictive motion priors for humanoid whole-body control. In 2025 IEEE International Conference on Robotics and Automation (ICRA), pages 53645371. IEEE, 2025. 3 [51] Zhengyi Luo, Ye Yuan, Tingwu Wang, Chenran Li, Sirui Chen, Fernando Castaneda, Zi-Ang Cao, Jiefeng Li, David Minor, Qingwei Ben, et al. Sonic: Supersizing motion tracking for natural humanoid whole-body control. arXiv preprint arXiv:2511.07820, 2025. 3 [52] Naureen Mahmood, Nima Ghorbani, Nikolaus Troje, Gerard Pons-Moll, and Michael Black. Amass: Archive of motion capture as surface shapes. In Proceedings of the IEEE/CVF international conference on computer vision, pages 54425451, 2019. [53] Viktor Makoviychuk, Lukasz Wawrzyniak, Yunrong Guo, Michelle Lu, Kier Storey, Miles Macklin, David Hoeller, Nikita Rudin, Arthur Allshire, Ankur Handa, and Gavriel State. Isaac gym: High performance GPU based physics simulation for robot learning. In Joaquin Vanschoren and Sai-Kit Yeung, editors, Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual, 2021. 7, 20 [54] Jiageng Mao, Siheng Zhao, Siqi Song, Chuye Hong, Tianheng Shi, Junjie Ye, Mingtong Zhang, Haoran Geng, 13 Jitendra Malik, Vitor Guizilini, and Yue Wang. Universal humanoid robot pose learning from internet human videos. In 2025 IEEE-RAS 24th International Conference on Humanoid Robots (Humanoids), pages 18, 2025. 3 [55] James Ni, Zekai Wang, Wei Lin, Amir Bar, Yann LeCun, Trevor Darrell, Jitendra Malik, and Roei Herzig. From generated human videos to physically plausible robot trajectories. arXiv preprint arXiv:2512.05094, 2025. 3 [56] Michael OConnell, Guanya Shi, Xichen Shi, Kamyar Azizzadenesheli, Anima Anandkumar, Yisong Yue, and Soon-Jo Chung. Neural-fly enables rapid learning for agile flight in strong winds. Sci. Robotics, 7(66), 2022. 3 [57] Xue Bin Peng, Pieter Abbeel, Sergey Levine, and Michiel van de Panne. Deepmimic: example-guided deep reinforcement learning of physics-based character skills. ACM Trans. Graph., 37(4):143, 2018. 2 [58] Xue Bin Peng, Erwin Coumans, Tingnan Zhang, TsangWei Edward Lee, Jie Tan, and Sergey Levine. Learning agile robotic locomotion skills by imitating animals. In Marc Toussaint, Antonio Bicchi, and Tucker Hermans, editors, Robotics: Science and Systems XVI, Virtual Event / Corvalis, Oregon, USA, July 12-16, 2020, 2020. 3 [59] Oliver Porges, Mathilde Connan, Bernd Henze, Andrea Gigli, Claudio Castellini, and Maximo Alejandro Roa Garzon. wearable, ultralight interface for bimanual teleoperation of compliant, whole-body-controlled humanoid robot. In 2019 International Conference on Robotics and Automation, ICRA 2019. IEEE, 2019. [60] Tifanny Portela, Andrei Cramariuc, Mayank Mittal, and Marco Hutter. Whole-body end-effector pose tracking. In 2025 IEEE International Conference on Robotics and Automation (ICRA), pages 1120511211. IEEE, 2025. 3 [61] Haozhi Qi, Yen-Jen Wang, Toru Lin, Brent Yi, Yi Ma, Koushil Sreenath, and Jitendra Malik. Coordinated arXiv humanoid manipulation with choice policies. preprint arXiv:2512.25072, 2025. 10 [62] Zekun Qi, Wenyao Zhang, Yufei Ding, Runpei Dong, XinQiang Yu, Jingwen Li, Lingyun Xu, Baoyu Li, Xialin He, Guofan Fan, Jiazhao Zhang, Jiawei He, Jiayuan Gu, Xin Jin, Kaisheng Ma, Zhizheng Zhang, He Wang, and Li Yi. Sofar: Language-grounded orientation bridges In The spatial reasoning and object manipulation. Thirty-ninth Annual Conference on Neural Information Processing Systems, 2025. 2, 17 [63] Tianhe Ren, Qing Jiang, Shilong Liu, Zhaoyang Zeng, Wenlong Liu, Han Gao, Hongjie Huang, Zhengyu Ma, Xiaoke Jiang, Yihao Chen, et al. Grounding dino 1.5: Advance the edge of open-set object detection. arXiv preprint arXiv:2405.10300, 2024. 18, 20 [64] Andreas Schmid, Nicolas Gorges, Dirk Goger, and Heinz Worn. Opening door with humanoid robot using multiIn 2008 IEEE international sensory tactile feedback. conference on robotics and automation, pages 285291. IEEE, 2008. 16 [65] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. [66] Mili Shah, Roger Eastman, and Tsai Hong. An overview of robot-sensor calibration methods for evaluation of In Proceedings of the Workshop perception systems. on Performance Metrics for Intelligent Systems, pages 1520, 2012. 19 [67] Christopher Stanton, Anton Bogdanovych, and Edward Ratanasena. Teleoperation of humanoid robot using fullbody motion capture, example movements, and machine learning. In Proc. Australasian Conference on Robotics and Automation, volume 8, page 51, 2012. 3 [68] Balakumar Sundaralingam, Siva Kumar Sastry Hari, Adam Fishman, Caelan Garrett, Karl Van Wyk, Valts Blukis, Alexander Millane, Helen Oleynikova, Ankur Handa, Fabio Ramos, et al. Curobo: Parallelized collisionfree robot motion generation. In 2023 IEEE International Conference on Robotics and Automation (ICRA), pages 81128119. IEEE, 2023. 3, 6, 10, 20 [69] Amy Tabb and Khalil Ahmad Yousef. Solving the robot-world hand-eye (s) calibration problem with iterative methods. Machine Vision and Applications, 28 (5):569590, 2017. 19 [70] Jie Tan, Tingnan Zhang, Erwin Coumans, Atil Iscen, Yunfei Bai, Danijar Hafner, Steven Bohez, and Vincent Vanhoucke. Sim-to-real: Learning agile locomotion for quadruped robots. In Hadas Kress-Gazit, Siddhartha S. Srinivasa, Tom Howard, and Nikolay Atanasov, editors, Robotics: Science and Systems XIV, Carnegie Mellon University, Pittsburgh, Pennsylvania, USA, June 26-30, 2018, 2018. [71] Johannes Tenhumberg and Berthold Bauml. Calibration of an elastic humanoid upper body and efficient compensation for motion planning. In 20th IEEE-RAS International Conference on Humanoid Robots, Humanoids 2021, Munich, Germany, July 19-21, 2021, pages 98103. IEEE, 2021. 3 [72] Johannes Tenhumberg, Dominik Winkelbauer, Darius Burschka, and Berthold Bauml. Self-contained calibration of an elastic humanoid upper body using only headmounted RGB camera. In 21st IEEE-RAS International Conference on Humanoid Robots, Humanoids 2022, Ginowan, Japan, November 28-30, 2022, pages 702707. IEEE, 2022. 3 [73] Tesla. Artificial intelligence & autopilot, 2021. URL https://www.tesla.com/AI. 2, 3 [74] Josh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Wojciech Zaremba, and Pieter Abbeel. Domain randomization for transferring deep neural networks from simulation In 2017 IEEE/RSJ International to the real world. Conference on Intelligent Robots and Systems (IROS), pages 2330, 2017. 20 [75] Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: In 2012 physics engine for model-based control. IEEE/RSJ International Conference on Intelligent Robots and Systems, IROS 2012, Vilamoura, Algarve, Portugal, October 7-12, 2012, pages 50265033. IEEE, 2012. 7, 14 20 [76] Roger Tsai. Efficient and accurate camera calibration technique for 3d machine vision. In IEEE conference on computer vision and pattern recognition, 1985. 19 [77] Roger Tsai, Reimar Lenz, et al. new technique for fully autonomous and efficient 3 robotics hand/eye calibration. IEEE Transactions on robotics and automation, 5(3):345358, 1989. 19 [78] Shinji Umeyama. Least-squares estimation of transforIEEE mation parameters between two point patterns. Transactions on pattern analysis and machine intelligence, 13(4):376380, 2002. 5, 19 [79] Unitree. Unitree H1 / H1-2: Unitrees first universal humanoid robot. 2023. URL https://www.unitree.com/h1. 3 [80] Unitree. Unitree G1: Humanoid Agent AI Avatar. 2024. URL https://www.unitree.com/g1. 3 [81] Hongxi Wang, Haoxiang Luo, Wei Zhang, and Hua Chen. CTS: concurrent teacher-student reinforcement learning for legged locomotion. IEEE Robotics Autom. Lett., 9 (11):91919198, 2024. 3 [82] Jianyuan Wang, Minghao Chen, Nikita Karaev, Andrea Vedaldi, Christian Rupprecht, and David Novotny. Vggt: Visual geometry grounded transformer. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 52945306, 2025. 17 [83] Lai Wei, Xuanbin Peng, Ri-Zhao Qiu, Xuxin Cheng, and Xiaolong Wang. Hmc: Learning heterogeneous meta-control for contact-rich loco-manipulation. In RSS 2025 Workshop on Whole-body Control and Bimanual Manipulation: Applications in Humanoids and Beyond. 3 [84] Haoyang Weng, Yitang Li, Nikhil Sobanbabu, Zihan Wang, Zhengyi Luo, Tairan He, Deva Ramanan, and Guanya Shi. Hdmi: Learning interactive humanoid whole-body control from human videos. arXiv preprint arXiv:2509.16757, 2025. 3 [85] Haoru Xue, Tairan He, Zi Wang, Qingwei Ben, Wenli Xiao, Zhengyi Luo, Xingye Da, Fernando Castaneda, Guanya Shi, Shankar Sastry, et al. Opening the sim-toreal door for humanoid pixel-to-action policy transfer. arXiv preprint arXiv:2512.01061, 2025. 16 [86] Lujie Yang, Xiaoyu Huang, Zhen Wu, Angjoo Kanazawa, Pieter Abbeel, Carmelo Sferrazza, Karen Liu, Rocky Duan, and Guanya Shi. Omniretarget: Interactionpreserving data generation for humanoid whole-body loco-manipulation and scene interaction. arXiv preprint arXiv:2509.26633, 2025. [87] Shaofeng Yin, Yanjie Ze, Hong-Xing Yu, Karen Liu, and Jiajun Wu. Visualmimic: Visual humanoid locomanipulation via motion tracking and generation. arXiv preprint arXiv:2509.20322, 2025. 3 [88] Wenhao Yu, Jie Tan, C. Karen Liu, and Greg Turk. Preparing for the unknown: Learning universal policy with online system identification. In Nancy M. Amato, Siddhartha S. Srinivasa, Nora Ayanian, and Scott Kuindersma, editors, Robotics: Science and Systems XIII, Massachusetts Institute of Technology, Cambridge, Massachusetts, USA, July 12-16, 2017, 2017. 3 [89] Wenhao Yu, C. Karen Liu, and Greg Turk. Policy transfer with strategy optimization. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. [90] Wenhao Yu, Jie Tan, Yunfei Bai, Erwin Coumans, and Sehoon Ha. Learning fast adaptation with meta strategy IEEE Robotics Autom. Lett., 5(2):2950 optimization. 2957, 2020. 3 [91] Yanjie Ze, Zixuan Chen, Joao Pedro Araujo, Zi-ang Cao, Xue Bin Peng, Jiajun Wu, and Karen Liu. Twist: Teleoperated whole-body imitation system. In Joseph Lim, Shuran Song, and Hae-Won Park, editors, Proceedings of The 9th Conference on Robot Learning, volume 305 of Proceedings of Machine Learning Research, pages 21432154. PMLR, 2730 Sep 2025. 3 [92] Yanjie Ze, Zixuan Chen, Wenhao Wang, Tianyi Chen, Xialin He, Ying Yuan, Xue Bin Peng, and Jiajun Wu. Generalizable humanoid manipulation with 3d diffusion policies. In 2025 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 28732880. IEEE, 2025. [93] Yanjie Ze, Siheng Zhao, Weizhuo Wang, Angjoo Kanazawa, Rocky Duan, Pieter Abbeel, Guanya Shi, Jiajun Wu, and Karen Liu. Twist2: Scalable, portable, and holistic humanoid data collection system. In 2026 IEEE International Conference on Robotics and Automation (ICRA), 2026. 3, 10 [94] Yuanhang Zhang, Yifu Yuan, Prajwal Gurunath, Ishita Gupta, Shayegan Omidshafiei, Ali-akbar Aghamohammadi, Marcell Vazquez-Chanlatte, Liam Pedersen, Tairan He, and Guanya Shi. Falcon: Learning forceadaptive humanoid loco-manipulation. In 8th Learning for Dynamics & Control Conference, 2026. 2, 3, 8, 9, 16 [95] Zhikai Zhang, Chao Chen, Han Xue, Jilong Wang, Sikai Liang, Yun Liu, Zongzhang Zhang, He Wang, and Li Yi. Unleashing humanoid reaching potential via real-worldready skill space. IEEE Robotics and Automation Letters, 11(2):20822089, 2025. 3 [96] Zhikai Zhang, Jun Guo, Chao Chen, Jilong Wang, Chenghuai Lin, Yunrui Lian, Han Xue, Zhenrong Wang, Maoqi Liu, Jiangran Lyu, et al. Track any motions under any disturbances. arXiv preprint arXiv:2509.13833, 2025. 3 [97] Siheng Zhao, Yanjie Ze, Yue Wang, Karen Liu, Pieter Abbeel, Guanya Shi, and Rocky Duan. Resmimic: From general motion tracking to humanoid whole-body loco-manipulation via residual learning. arXiv preprint arXiv:2510.05070, 2025. 3 [98] Yi Zhou, Connelly Barnes, Jingwan Lu, Jimei Yang, and Hao Li. On the continuity of rotation representations In Proceedings of the IEEE/CVF in neural networks. conference on computer vision and pattern recognition, pages 57455753, 2019. 5,"
        },
        {
            "title": "APPENDIX",
            "content": ". . . . . . Additional Experimental Analysis . 16 16 A.1 Language Sensitivity . . 16 A.2 Tracking Error Distribution Analysis A.3 Moving Object Grasping with Visual Replanning 16 A.4 Extending HERO to Other Tasks Like Door . . . . . . . . . . . . . . . . . . . . . A.5 Field of View Analysis . . . . A.6 Visual Perception Illustration . A.7 Analytical FK Error Visualization . . . . . . . . . Opening . 16 17 17 . . . . . . . . . . . . . . . . . . . . . . . . . Additional Implementation Details . . . . . . . . . . . . B.1 MOCAP Setup . B.2 Onboard Egocentric RGB-D Camera . . . B.3 Hyper-parameters . . . . B.4 Rewards . . . . . B.5 Policy Training . . B.6 Deployment Hardware . . B.7 Testing Assets Details . . B.8 Testing Scenes Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 19 19 19 20 20 20 20 20 ADDITIONAL EXPERIMENTAL ANALYSIS A.1 Language Sensitivity Fig. 11 shows that our system can correctly interpret language to pick up the correct object among relevant distractors. It picks up the red apple (and not the green one) when told to pick up the red apple in the top row, and vice versa in the bottom row. Fig. 11: HERO is able to distinguish the target object via language queries. (a) Picking up red apple; (b) Picking up green apple instead of red apple. A.2 Tracking Error Distribution Analysis We visualize the CDFs of EE tracking errors in Fig. 12. Top row. HERO dominates all baselines: at 80%, HERO achieves 3.9 cm / 6.7 (pos/rot), versus 20.9 cm / 25.3 for FALCON and 9.8 cm / 19.5 for AMO. At 90%, HERO remains below 4.6 cm and 8.2, indicating strong tail robustness. Bottom row. Replanning (Sec. III-C) is critical: at 80%, HERO achieves 3.1 cm vs. 6.3 cm without replanning (2.1 worse), and at 90% 3.4 cm vs. 7.1 cm. Rotation gains are smaller but consistent (median 13.6 vs. 16.1). 16 Fig. 12: CDF analysis of end-effector tracking errors. Top row: Comparison of translation (left) and rotation (right) error distributions across HERO, FALCON [94], and AMO [39] for all table heights. The steeper curves of HERO indicate consistently lower errors and tighter distributions. Bottom row: Ablation study showing translation (left) and rotation (right) error distributions with and without replanning (Sec. III-C). The steeper CDF curves with replanning demonstrate their significant contribution to tracking accuracy. A.3 Moving Object Grasping with Visual Replanning HERO derives target grasping poses from vision and language queries, enabling the robot to capture moving objects through closed-loop replanning. Fig. 13(a-b) illustrates this dynamic adaptation: while the system initially generates an EE trajectory based on the first vision perception, it re-estimates the pose as the object moves. This visual feedback triggers the update of the target grasp, allowing the robot to seamlessly adjust its trajectory and successfully secure the moving object. Note that, in these two trials, the robot successfully sees the object after moving, but the object can only be seen at the corner due to the rather limited field of view. A.4 Extending HERO to Other Tasks Like Door Opening As HERO constructed modular system that coordinates high-level planning and low-level end-effector control, it reveals possibility of extending HERO to broader tasks like door opening, which is challenging loco-manipulation task [35, 64, 85]. In Fig. 14, we directly employ HERO to identify the target grasping pose for fridge door handle, followed by the same pipeline as object grasping, enabling the humanoid to grasp the fridge handle and finally open it when returning to the default position. Note that the door requires large force to open because of magnetic attraction; we leave the Fig. 13: HERO enables humanoid to grasp moving object via visual closed-loop replanning. (a-b) Two examples of visual closed-loop replanning. The goal is to grasp black can. Fig. 14: Door opening with HERO. (a) The egocentric RGB-D visual inputs. (b) Given language query for the door handle (e.g., fridge handle prompted here), our modular system obtains the grasping pose, same as the pipeline for picking up objects. (c) The robot executes the reaching trajectory and closes the hand when reaching the target poses closely, and then returns to the default pose with the door opened. Note that while the door is heavy, HERO successfully manages to open the door with smooth and stable door motion. door unlatched beforehand. This result shows HEROs modular potential, and it is also possible to extend our system to broader loco-manipulation tasks by incorporating off-the-shelf trajectory generation frameworks [20, 28, 31, 62]. A.5 Field of View Analysis Our system uses the onboard camera for visual perception, which, however, has limited field of view. As shown in Fig. 15, the robot first stands randomly at distance of 1.28m from the object, while the target object (stapler) is not visible at this distance; after walking forward under consistent velocity command to distance of 0.6m and continuously detecting the object, the egocentric view successfully captures the target object, which makes the robot stops at about 0.5m from the object. Then the robot coordinates the whole-body reaching motion and successfully grasps the object. This visualization indicates that the robots onboard egocentric view is limited, and the robot can only see the object within short distance (< 0.6m), which makes the availability of the 3D spatial understanding beforehand [8, 47, 82] critical for searching objects, which could be future exploration. A.6 Visual Perception Illustration Fig. 15: Filed of view visualization using HERO. We let the robot stand at random distance from the object (e.g., 1.28m), and the robot keeps walking forward until the target object (stapler) is detected. After successfully detecting the target object, the robot stops walking and grasps the object via whole-body coordination. The robot can only see the object within 0.6m, which makes it hard for the robot to search for the object in random room. Fig. 16 illustrates how HERO leverages LVMs to obtain the targeted EE grasping pose, following modular perception-toaction design that is similar to prior systems [47, 62]. Object of Interest from Language. Given an ego-centric RGBD observation and natural-language query specifying the target object, HERO first applies GroundingDINO to produce Fig. 16: HERO visual perception pipeline illustration. (a-d) Examples of ego-centric visual perception using LVMs, including GroundingDINO [63], SAM-3 [7], and AnyGrasp. Given the language query, GroundingDINO outputs the detection box, which is input to SAM for the segmentation mask. The mask is used to filter out jaw grasps predicted by AnyGrasp, which is finally retargeted to the 6-DoF end-effector pose for dexterous grasping with Dex-3 hand. Fig. 17: Visualization of analytical forward kinematics error. We plot the 60 data points collected in the MOCAP room. The error is indicated in the color bar on the left side of the figure, and the size of the scatter also increases with the error. language-conditioned detection box [63]. The detected box is then used to prompt SAM-3 for segmentation of the object of interest [7]. AnyDexGrasp [14] can also be used here, but we find that the Dex-3 hand lacks dexterity, and the difference between these two methods is limited. Grasp Proposals. This mask serves as spatial constraint for grasp proposal generation: HERO runs AnyGrasp [13] to produce set of candidate grasps, and then filters out proposals outside the segmented object region. Note that Grasp Selection. To select the best jaw grasp, we first filter out the grasp poses that lie on the opposite side of the object relative to the robots hand (e.g., for an object to the right side of the hand, the left approaching grasps are abandoned). 18 Then we filter out grasps that are too high or too low based on gravity-aligned height estimation of objects using depth. Afterward, we select the grasp that lies most parallel to the ground with the highest confidence as the final grasp. Grasp Retargeting. The selected grasp is retargeted to 6DoF end-effector pose for dexterous grasping with the Dex-3 hand. We first rotate the gripper pose by 45 degrees around the z-axis to improve the grasp robustness and pose error tolerance. After that, we clip the yaw angle within 70 degrees to ensure the orientation is not too large. A.7 Analytical FK Error Visualization In Fig. 17, we visualize the translation error of analytical forward kinematic results. We plot the error via the collected 60 samples in the MOCAP room, where the error is recorded when time is 1 minute. From the figure, we can observe that the error generally increases when the EE location becomes larger along the and axes, which may form pattern that can be learned from neural model. ADDITIONAL IMPLEMENTATION DETAILS B.1 MOCAP Setup MOCAP System. We use the modern MOCAP system Optitrack [1] with 13 cameras which provides 0.2mm measure accuracy. Robot Link Pose. To obtain the end-effector pose in the robot frame, we put several markers onto both links, and we show markers on the hand in Fig. 18(a). Although the MOCAP system provides constructed asset poses via selected marker groups, there exists misalignment between the MOCAP asset frame and the robot link frame. To address this, we carefully measure each markers relative offset to the links origin, followed by the Kabsch-Umeyama (KU) algorithm [33, 78] that transforms individual marker coordinates into 6-DoF link pose in the MOCAP frame within < 1.5mm RMSE error. The relative transformation of EE and the robot base is thus obtained as they are all in the MOCAP frame. This approach ensures an accurate measurement of both the end-effector and the robot base, setting solid ground for our evaluation and camera calibration, introduced next. B.2 Onboard Egocentric RGB-D Camera Setup. We use the onboard RGB-D camera D435i mounted on the humanoids head, as shown in Fig. 18(b). The humanoids neck features pitch degree of freedom enabling head rotation within limited range, necessitating precise camera calibration for accurate 3D perception. Calibration with MOCAP. Standard hand-eye calibration [66, 69] typically relies on analytical forward kinematics to obtain end-effector poses. However, as demonstrated in the main paper (Sec. V-C), analytical forward kinematics exhibits systematic errors of approximately 1.8cm due to hardware inaccuraciesunsuitable for precise camera calibration. We instead leverage the MOCAP system for ground-truth pose measurement. Following the marker-based approach described previously, we attach reflective markers to an ArUco 19 Fig. 18: MOCAP markers, camera, and calibration setups. (a) We put several MOCAP markers on both the robots endeffector and robots base (similar to the EE), and each markers relative location to the links base is measured carefully. By employing the Kabsch-Umeyama algorithm [33, 78], we are able to accurately obtain the robots links coordinate in the MOCAP frame from each markers individual coordinates in the MOCAP frame with < 1.5mm RMSE error. (b) The onboard D435i camera mounted on the Unitree G1 humanoid robots head. (c) While no motor is set, there is neck pitch DoF that allows the head to rotate along the axis via external physical force, making the manufacturer-provided camera parameters far from the real setup. (d) Similar to EE and base, we put several MOCAP markers on standard ArUco calibration board [19] to obtain an accurate relative transformation of the calibration board to the robot base. (e) Our calibration requires one person to hold the board in front of the camera to collect different board poses in the robot frame. calibration board [19] and apply the KU algorithm [33, 78] for 6-DoF pose estimation. During data collection, we manually move the board through 60-70 diverse poses in front of the camera. For each pose i, we record: 1) the robot base pose in MOCAP frame base MOCAP, 2) the board pose in MOCAP frame MOCAP, and 3) the board pose in camera frame board,i board,i camera via ArUco detection using the OpenCV library [6]. To compute the camera-to-base transformation camera base solve the eye-to-hand calibration problem: base board,i camera camera = base MOCAP board,i MOCAP , we (4) using the Tsai-Lenz method [76, 77]. This MOCAP-assisted calibration achieves reprojection error within 2.5mm, ensuring accurate egocentric 3D perception. Image Resolution & FPS. We use the RGB-D images with resolution of 640 480 in 60Hz FPS. B.3 Hyper-parameters Motion planning For motion planning, we use cuRobo and set the planning dt to 7.25e-6. Grasping Threshold When the robot approaches the object, it autonomously close the hand when the hand distance to the target grasp Et δ where δ > 0 is threshold. At the moment when this threshold is reached, we pass the same local waypoint of the planned motion trajectory to the policy to ensure stability, and the hand is immediately closed for grasping. In this paper, we utilize threshold of δ = 1.5cm, which we find most effective across tested objects. B.4 Rewards Tab. VII summarizes reward components and weights used for RL training of πt, which is structured into four categories: tracking task, penalties, regularization, and locomotion task. To ensure precise manipulation, the tracking rewards weigh the alignment of the end-effector based on our newly proposed residual Et. Note that EE orientation is represented with the continuous 6D parameterization (first two columns of the rotation matrix) [98]. To encourage the planned upper-body posture (e.g., waist bending or torso twisting), we also add joint-space tracking term. Penalties strictly enforce safety constraints (e.g., joint limits, termination), while regularization termssuch as costs on torque, acceleration, and stance symmetryare essential for generating smooth, stable motions capable of robust and natural Sim2Real transfer. To train the robot to follow locomotion commands, we also use flag variable to control the standing and waking mode switching. B.5 Policy Training Simulation & Training Setup. We train our end-effector tracking policy πt with the IsaacGym simulator [53], and transfer this policy to the MuJoco simulator [75] for Sim2Sim evaluations before deploying it in the real world. We train our policy with 4,096 environments for overall 20K iterations in parallel, with learning rate of 1e-4 for both the actor and critic models. AdamW optimizer [49] is used with weight decay of 1e-2. We use high simulation frequency of 500Hz, with the low-level PD controller running at 50Hz. All the policy training is conducted on single NVIDIA RTX 4090 or an L40S GPU. Sim2Real Domain Randomization. Following previous works [22, 24, 27], we employ standard domain and dynamics randomization to facilitate Sim2Real transfer [74], including variations in link center of mass (CoM) and control delay. Notably, we identify that randomizing the end-effector mass is essential; without this specific randomization, the policy exhibits end-effector instability, leading to high-frequency hand oscillations that compromise tracking accuracy. B.6 Deployment Hardware We run all modules (e.g., πt and SAM-3 [7]) off-theshelf on 32-GB RAM laptop equipped with NVIDIA RTX 5070Ti GPU and Intel Core Ultra 9 275HX CPU processor (24 CPU cores / 24 threads). We run cuRobo with CUDA graph acceleration, which largely improve the efficiency on the edge [68]. For the detection module, we have tested both Grounding DINO base [48] and Grounding DINO 1.5 [63], where the base version can be deployed on the laptop, and Grounding DINO 1.5 only provides access through online APIs. However, we find that Grounding DINO base is sufficient for most scenes and objects. 20 TABLE VII: Reward components and weights. Penalty rewards prevent unreasonable behaviors for sim2real transfer, regularization helps improve motion smoothness and stability, and task rewards ensure successful and precise end-effector and upper-body tracking. TERM EXPRESSION WEIGHT Tracking Task Rewards: End-effector exp Upper-body DoF exp Base height exp Penalty: DoF position limits DoF velocity limits Termination Regularization: End-effector linear velocity End-effector angular velocity DoF acceleration DoF velocity Action rate Torque Angular velocity Base velocity Base orientation Torso orientation Stance symmetry Ankle roll Feet contact Feet orientation Negative knee DoFs Feet spread distance Walkings Task Rewards: Linear Velocity vx Linear Velocity vy Angular Velocity exp(Et2) exp(qupper (Ref) 2) exp(hbase hbase (Ref)2) qupper 1(qt / [qmin, qmax]) 1( qt / [qmin, qmax]) 1termination v2 EE ω2 EE qt2 qt2 2 at2 2 τ ω2 v2 1 cos θbase cos θbase = gbasegtarget 1 cos θtorso cos θtorso = gtorsogtarget gtorsogtarget (cid:12) (cid:12) (cid:80) (cid:16)(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)qleft (cid:12)qleft (cid:12) + s: sagittal joints, a: anti-sagittal joints (cid:80) (cid:16) ankle,roll + qright qleft 1(ncontact < 2) + 1(ncontact = 2 ncontact = 0) qright + qright gbasegtarget ankle,roll (cid:12) (cid:17) (cid:12) (cid:12) (cid:17) + gright foot gleft foot xy xy (cid:80) 1(qknee < qknee,min) pright foot xy 1(pleft foot xy < dthresh) exp((vcmd exp((vcmd exp((ωcmd vbase vbase ωbase )2/σ) )2/σ) )2/σ) 2.0 4.0 4.0 -5.0 -5.0 -250 -0.2 -0.02 -2.5e-7 -1e-3 -0.1 -1e-5 -0.05 -2.0 -1. -1.0 -0.5 -2.0 -4.0 -2.0 -1.0 -10.0 2.0 1.5 4. B.7 Testing Assets Details In the paper, we have tested HERO with 20 daily objects; these objects have different sizes and weights, while being made with different materials, making it challenging to grasp with Dex-3 hand. We list the detailed sizes, weights, materials, and language queries of all objects tested in Tab. VIII. Note that the size is roughly measured as the shape is irregular and cannot be easily described. B.8 Testing Scenes Details Tab. IX lists the details of the novel scenes tested in this paper, which are mainly chosen from the Coordinated Science Laboratory Studio (CSL Studio) and the Thomas M. Siebel Center for Computer Science at the University of Illinois Urbana-Champaign, Urbana, IL. The snapshot of these testing scenes can be found in Fig. 6. TABLE VIII: Testing objects, sizes, weights, materials, and language queries. Sizes are roughly measured due to irregular shapes. Weights are measured with an accurate food scale. Object Size Weight Material Language Query 10 daily object evaluation. 4.94.94.9 cm 58.06 Wood orange cube 1266 cm 14.97 Aluminum 8.57.57.5 cm 15.88 Plastic coke can red apple 955 cm 137.89 Plastic & Metal emergency stop button 1565 cm 239.95 Plastic & Metal robot hand 16.510.53.3 cm 185.07 Plastic game cartridge 27.59.59.5 cm 79.83 Plastic olive oil bottle 168.58.5 cm 392.81 Plastic & Liquid hand soap 237.97.9 cm 43.09 Paperboard & Plastic chip can 171111 cm 73.94 Plush red piranha plant 10 daily objects used in 10 daily scenes evaluation. 211243 cm 215.91 Plastic 16.51121 cm 213.19 Plush kettle toy dog 17.299 cm 24.95 Paperboard & Plastic Starbucks coffee 11.59.49.4 cm 526.17 Ceramic orange mug 18.56.36.3 cm 286.22 Plastic & Metal water bottle 77.77.7 cm 14.97 Plastic green apple Continued on next page 21 Object Size Weight Material Language Query 18.82.413.3 cm 301.19 Paper 14839 cm 234.96 Plastic & Metal 85.610 cm 367.41 Metal & Spam purple book helicopter spam 1955 cm 86.18 Plastic & Liquid cleaner bottle Additional objects. 24.586.5 cm 135.17 Plush 2388 cm 307.08 Plush 7.57.27.5 cm 19.05 Plastic 212.313.8 cm 376.03 Paper carrot broccoli orange book 15.36.56.5 cm 18.14 Aluminum black can 22 TABLE IX: Testing scenes, table heights, language queries. Here we list the novel scenes chosen in this paper for evaluation, and the corresponding table height. The snapshot of these scenes can be found in Fig. 6. Scene Location corridor CSL Studio @ UIUC office lounge CSL Studio @ UIUC building cafe Thomas M. Siebel Center for Computer Science @ UIUC CSL Studio @ UIUC Thomas M. Siebel Center for Computer Science @ UIUC CSL Studio @ UIUC office building lounge office kitchenette building den Thomas M. Siebel Center for Computer Science (RM 3333) @ UIUC robotics lab CSL Studio @ UIUC office kitchen CSL Studio @ UIUC classroom Thomas M. Siebel Center for Computer Science (RM 1302) @ UIUC Table Height Language Query 0.43m 0.48m 0.72m 0.74m 0.74m kettle toy dog Starbucks coffee orange mug water bottle 0.74m green apple 0.74m purple book 0.86m 0.87m 0.92m helicopter spam cleaner bottle"
        }
    ],
    "affiliations": [
        "University of Illinois Urbana-Champaign"
    ]
}