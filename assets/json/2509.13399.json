{
    "paper_title": "EdiVal-Agent: An Object-Centric Framework for Automated, Scalable, Fine-Grained Evaluation of Multi-Turn Editing",
    "authors": [
        "Tianyu Chen",
        "Yasi Zhang",
        "Zhi Zhang",
        "Peiyu Yu",
        "Shu Wang",
        "Zhendong Wang",
        "Kevin Lin",
        "Xiaofei Wang",
        "Zhengyuan Yang",
        "Linjie Li",
        "Chung-Ching Lin",
        "Jianwen Xie",
        "Oscar Leong",
        "Lijuan Wang",
        "Ying Nian Wu",
        "Mingyuan Zhou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Instruction-based image editing has advanced rapidly, yet reliable and interpretable evaluation remains a bottleneck. Current protocols either (i) depend on paired reference images -- resulting in limited coverage and inheriting biases from prior generative models -- or (ii) rely solely on zero-shot vision-language models (VLMs), whose prompt-based assessments of instruction following, content consistency, and visual quality are often imprecise. To address this, we introduce EdiVal-Agent, an automated, scalable, and fine-grained evaluation framework for multi-turn instruction-based editing from an object-centric perspective, supported by a suite of expert tools. Given an image, EdiVal-Agent first decomposes it into semantically meaningful objects, then synthesizes diverse, context-aware editing instructions. For evaluation, it integrates VLMs with open-vocabulary object detectors to assess instruction following, uses semantic-level feature extractors to evaluate content consistency, and leverages human preference models to judge visual quality. We show that combining VLMs with object detectors yields stronger agreement with human judgments in instruction-following evaluation compared to using VLMs alone and CLIP-based metrics. Furthermore, the pipeline's modular design allows future tools to be seamlessly integrated, enhancing evaluation accuracy over time. Instantiating this pipeline, we build EdiVal-Bench, a multi-turn editing benchmark covering 9 instruction types and 11 state-of-the-art editing models spanning autoregressive (AR) (including Nano Banana, GPT-Image-1), flow-matching, and diffusion paradigms. We demonstrate that EdiVal-Agent can be used to identify existing failure modes, thereby informing the development of the next generation of editing models. Project page: https://tianyucodings.github.io/EdiVAL-page/."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 1 ] . [ 1 9 9 3 3 1 . 9 0 5 2 : r a"
        },
        {
            "title": "Preprint",
            "content": "EDIVAL-AGENT: AN OBJECT-CENTRIC FRAMEWORK FOR AUTOMATED, SCALABLE, FINE-GRAINED EVALUATION OF MULTI-TURN EDITING Tianyu Chen1,, Yasi Zhang2,, Zhi Zhang2, Peiyu Yu2, Shu Wang2, Zhendong Wang3, Kevin Lin3, Xiaofei Wang3, Zhengyuan Yang3, Linjie Li3, Chung-Ching Lin3, Jianwen Xie4 Oscar Leong2,, Lijuan Wang3,, Ying Nian Wu2,, Mingyuan Zhou1,3, 1University of Texas at Austin 2University of California, Los Angeles 3Microsoft 4Lambda, Inc"
        },
        {
            "title": "ABSTRACT",
            "content": "Instruction-based image editing has advanced rapidly, yet reliable and interpretable evaluation remains bottleneck. Current protocols either (i) depend on paired reference imagesresulting in limited coverage and inheriting biases from prior generative modelsor (ii) rely solely on zero-shot visionlanguage models (VLMs), whose prompt-based assessments of instruction following, content consistency, and visual quality are often imprecise. To address this, we introduce EdiVal-Agent , an automated, scalable, and finegrained evaluation framework for multi-turn instruction-based editing from an object-centric perspective, supported by suite of expert tools. Given an image, EdiVal-Agent first decomposes it into semantically meaningful objects, then synthesizes diverse, context-aware editing instructions. For evaluation, it integrates VLMs with open-vocabulary object detectors to assess instruction following, uses semantic-level feature extractors to evaluate content consistency, and leverages human preference models to judge visual quality. We show that combining VLMs with object detectors yields stronger agreement with human judgments in instruction-following evaluation compared to using VLMs alone and CLIPbased metrics. Furthermore, the pipelines modular design allows future tools to be seamlessly integrated, enhancing evaluation accuracy over time. Instantiating this pipeline, we build EdiVal-Bench, multi-turn editing benchmark covering 9 instruction types and 11 state-of-the-art editing models spanning autoregressive (AR), flow-matching, and diffusion paradigms. Our analysis shows that Nano Banana achieves the best overall balance of instruction following and content consistency while maintaining surprisingly low latency; GPT-Image-1 exhibits the strongest instruction following across turns but ranks among the lowest in content consistency and has high latency; AR models substantially outperform non-AR models in multi-turn editing by better preserving contextual coherence; and Qwen-Image-Edit is strong on the first turn but degrades in later turns due to exposure bias. We demonstrate that EdiVal-Agent can be used to identify existing failure modes, thereby informing the development of the next generation of editing models. Project page: https://tianyucodings.github.io/ EdiVAL-page/"
        },
        {
            "title": "INTRODUCTION",
            "content": "What truly defines the success of an instruction-based image editor? At its core, editing requires making targeted, instruction-driven changes while preserving contextual consistency and perceptual realismoften across multiple refinement turns. Yet current evaluation practice struggles to capture this multifaceted objective. Equal contribution. Correspondonce to tianyuchen@utexas.edu and yasminzhang@ucla.edu. Equal advising."
        },
        {
            "title": "Preprint",
            "content": "Figure 1: Overview of our workflow and representative models performance. For visualization, we adopt two thresholds: consistency score of at least 90 and visual quality score of at least 6. Details of the automated evaluation pipeline are provided in Figure 2 and Section 3. In multi-turn editing, models exhibit distinct weaknesses: GPT-Image-1 struggles with content consistency, Qwen-ImageEdit underperforms in both visual quality and content consistency, and FLUX.1-Kontext-dev lags in instruction following, whereas Nano Banana shows no single dominant weakness. comprehensive analysis is presented in Sec. 5 and Tab. 2. When ground-truth edited images are available, common strategy is to compare model outputs against these references Zhang et al. (2023); Zhao et al. (2024); Yu et al. (2025); Sheynin et al. (2024). Typical metrics include pixel-level distances (e.g., L1/L2) and semantic similarities (e.g., DINO Caron et al. (2021) and CLIP Radford et al. (2021)). While informative, such metrics suffer from two structural issues: (i) the space of acceptable edits is inherently large, whereas single reference provides only one realization; and (ii) references are frequently synthesized by existing editing models (e.g., Prompt-to-Prompt Hertz et al. (2023), SDXL Podell et al. (2024), DALLE2 Ramesh et al. (2022)), thereby importing their biases and limitations into the evaluation itself. Consequently, high reference similarity does not necessarily imply faithful instruction following, preservation of irrelevant content, or aesthetically plausible outcomes. complementary line of work employs VLMs as interpretable evaluatorsfor example, VIEScore Ku et al. (2023), HQ-Edit Hui et al. (2024), and Complex-Edit Yang et al. (2025)and queries VLMs about specific aspects of an edit. While VLMs offer holistic, language-mediated judgments, they remain insufficient for precise editing assessment for several reasons. First, for instructionfollowing evaluation, they are notoriously poor at spatial reasoning Zhang et al. (2025b); Chen et al. (2024); Chang et al. (2025) and are prone to object hallucinations in existence, category, attributes, and relations Bai et al. (2024). These issues together undermine their ability to assess common object-related edit instructions. Second, they have limited sensitivity to pixel-level changes and frequently miss subtle, localized modifications Vo et al. (2025) (e.g., fine structures, small attribute shifts, etc.), which are crucial for evaluating content consistency. Third, since they are predominantly pretrained on natural images rather than synthetic generations, their priors are miscalibrated for artifacts and aesthetics, leading to failures in detecting common generative defects (e.g., extra fingers) and in modeling perceptual naturalness Liang et al. (2024); Xu et al. (2023); Ma et al. (2025a), which humans are sensitive to. Consequently, VLM-only scoring lacks the precision and reliability required for fine-grained evaluation across instruction following, content consistency, and visual quality. To address these challenges, we introduce EdiVal-Agent: an automated, scalable, and fine-grained evaluation agent for multi-turn instruction-based image editing from an object-centric perspective, supported by suite of expert tools. As shown in Fig. 2, EdiVal-Agent first decomposes images into semantically meaningful components and identifies their contextual relationships. It then generates diverse and proper editing scenarios at scale, which are based on the initial analysis. Finally, it systematically evaluates editing model outputs from multiple axes, specifically by integrating VLMs with open-vocabulary object detectors to assess instruction following, using semantic-level feature"
        },
        {
            "title": "Preprint",
            "content": "Figure 2: Framework of EdiVal-Agent. It first decomposes images into semantically meaningful objects, such as metal yellow sign and metal brown pole, and identifies their contextual relationships, e.g., they are both in foreground. It then generates diverse and proper editing scenarios at scale which are based on the initial analysis, e.g., Change the color of metal brown pole to gray. Finally, it systematically evaluates editing model outputs from multiple axes, including instruction following, content consistency, and visual quality, specifically by integrating VLMs (Qwen2.5-VL) with openvocabulary object detectors (Grouding-DINO) to assess instruction following, using semantic-level feature extractors (DINOv3) and pixel-level metrics (L1 distance) to evaluate content consistency, and leveraging human preference models (HPSv3) to judge visual quality. Our agentic pipeline is agnostic to the expert tools used and can be readily enhanced with more advanced tools in the future. extractors to evaluate content consistency, and leveraging human preference models to judge visual quality. We show that combining VLMs with object detectors yields stronger agreement with human judgments in instruction-following evaluation compared to thresholded CLIP directional (CLIP dir) scores Gal et al. (2022) and using VLMs alone, as evidenced in Sec. 4. Note that the full evaluation process, from evaluation instruction creation to evaluation metrics, is fully automated, without human intervention. This results in scalable pipeline that can be easily expanded. Instantiating the agentic pipeline, we curate new multi-turn image editing benchmark, EdiValBench, featuring 9 instruction types and 11 existing editing modelsspanning autoregressive (AR), flow-matching, and diffusion paradigms, across both closedand open-source systemsconduct fine-grained analyses, and draw actionable insights. Empirically, as demonstrated in Fig. 1 and Tab. 2, GPT-Image-1 excels at instruction following yet ranks near the bottom in content consistency, whereas Nano Banana performs strongly on both axes. Besides, recently released open-sourced models like Qwen-Image-Edit significantly degrade in visual quality when editing turns increase, and fails to maintain consistency with the base image, while FLUX.1-Kontext-dev lags in instruction following. We further study editing behavior differences between AR and non-AR models in the multi-turn setting, and contrast multi-turn editing with complex single-shot prompts Yang et al. (2025), highlighting complementary strengths and failure modes. We hope that our agent pipeline, benchmark, and analyses accelerate the transition of multi-turn editing toward practical applications. Key contributions. 1) Agent: EdiVal-Agent is fully automated, scalable evaluator that performs object-centric decomposition, generates multi-turn editing instructions, and integrates VLM reasoning with symbolic pixeland semantic-level metrics via grounding tools; 2) Benchmark: using EdiVal-Agent, we build EdiVal-Bench with 1,716 instructions across 9 types and 3 turns on 572 real-world images; 3) Human agreement: EdiVal-Agent achieves 81.3% agreement with human ratings in instruction following, outperforming zero-shot VLMs and CLIP-based methods; 4) Evaluation: we assess 11 editors (diffusion, flow-matching, autoregressive) along instruction following, content consistency, and visual quality; 5) Insights: AR models better preserve contextual coherence"
        },
        {
            "title": "Preprint",
            "content": "and instruction accuracy across turns, while non-AR models exhibit exposure bias and cumulative degradation; 6) Artifacts & settings: we reveal luminance drift across turns, and contrast multi-turn against complex single-shot editing to delineate strengths and weaknesses across model families."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Instruction-based editing models. InstructPix2Pix (IP2P) Brooks et al. (2023) introduced twostage recipe that converts text-to-image diffusion model Rombach et al. (2022); Zhang et al. (2025a) into an editor: (i) synthesize paired editing data using Stable Diffusion Rombach et al. (2022) and training-free techniques such as Prompt-to-Prompt Hertz et al. (2023); (ii) fine-tune the diffusion model on these pairs. Subsequent systemsMagicBrush Zhang et al. (2023), UltraEdit Zhao et al. (2024), and AnyEdit Yu et al. (2025)scale this paradigm to large, fine-grained realimage editing. More recent work (e.g., OmniGen Xiao et al. (2025); Wu et al. (2025b), Step1X-Edit Liu et al. (2025), FLUX.1 Kontext Labs et al. (2025), and Qwen-Image-Edit Wu et al. (2025a)) adopts task-aware architectures and increasingly leverages flow matching Liu et al. (2022); Lipman et al. (2022); Zhang et al. (2024). complementary line explores autoregressive (AR) editors such as Gemini 2.0 Flash Image Gemini2 (2025), Gemini 2.5 Flash Image (Nano Banana) Deepmind (2025), and GPT-Image-1 OpenAI (2025). These models enable in-context multi-turn editing: users iteratively refine an image within conversational interface, with the model maintaining coherent editing history. To our knowledge, we provide the first systematic comparison of in-context multi-turn AR editing versus context-free multi-turn editing with non-AR models across instruction following, content consistency, and visual quality. Editing evaluation. Early evaluations (e.g., Brooks et al. (2023)) rely on CLIP-based similarity Radford et al. (2021), including directional variants Gal et al. (2022), to approximate editing quality. However, CLIP emphasizes semantic alignment and is less sensitive to fine, pixel-level changes. When ground-truth edited images exist, it is natural to compare model outputs against references using pixel distances (L1) and semantic similarities (DINO Caron et al. (2021), CLIP Radford et al. (2021)) Zhang et al. (2023); Zhao et al. (2024); Yu et al. (2025); Sheynin et al. (2024). Yet references are imperfect: the space of valid edits is inherently multimodal, while single reference captures only one realization; moreover, many references are themselves synthesized by prior editors (e.g., Prompt-to-Prompt Hertz et al. (2023), SDXL Podell et al. (2024), DALLE-2 Ramesh et al. (2022)), importing their biases into evaluation. Recent work relies exclusively on VLMs as interpretable judgese.g., VIEScore Ku et al. (2023), HQ-Edit Hui et al. (2024), and Complex-Edit Yang et al. (2025)by querying models such as GPT4o OpenAI (2025) about specific aspects of an edit. While VLMs offer holistic, language-mediated assessments, they are insufficient on their own: they are notoriously poor at spatial reasoning Zhang et al. (2025b); Cheng et al. (2024); Chen et al. (2024); Qharabagh et al. (2024); Chang et al. (2025) and are prone to object hallucinations in existence, category, attributes, and relations Bai et al. (2024); they have limited sensitivity to pixel-level changes and frequently miss subtle, localized modifications Vo et al. (2025) (e.g., fine structures, small attribute shifts, etc.), which are crucial for evaluating content consistency; they are miscalibrated for artifacts and aesthetics Liang et al. (2024); Xu et al. (2023); Ma et al. (2025a), which humans are sensitive to. Our approach, EdiValAgent, addresses these gaps by integrating VLM-based reasoning with grounding tools, symbolic, object-centric pixeland semantic-level tools, and human preference models, yielding precise and interpretable evaluation of instruction-based editing. Editing tasks. We consider three settings: (i) Single-turn vs. multi-turn. Multi-turn editing Zhang et al. (2023); Zhao et al. (2024) is more demanding than single-turn, as the model must maintain consistency across sequential instructions. In contrast to context-free multi-turn pipelines (each turn consumes the previous image and the next instruction), AR models Gemini2 (2025); Deepmind (2025); OpenAI (2025) support in-context multi-turn editing by conditioning on the full (ii) Complex single-shot vs. multi-turn. Following Yang et al. (2025), conversational history. sequence of edits can be concatenated into single complex prompt and executed in one pass; we compare this setting to genuine multi-turn editing. (iii) Other tasks. We focus on instructionbased editing, the most common regime; other scenarios (e.g., prompt-to-prompt/caption-to-caption"
        },
        {
            "title": "Preprint",
            "content": "Hertz et al. (2023)) are outside our scope. To the best of our knowledge, this paper offers the first comprehensive comparison covering single-turn, multi-turn, and complex single-shot editing within unified framework. 3 EDIVAL-AGENT"
        },
        {
            "title": "3.1 OVERVIEW",
            "content": "EdiVal-Agent is fully automated, scalable framework for evaluating instruction-based image editing from symbolic, object-centric perspective, as illustrated in Fig. 2. The pipeline comprises three stages: (1) Decomposition uses VLM (e.g., GPT-4o; other VLMs are viable alternatives) to extract structured, object-level descriptionsobjects, attributes, and relationsenabling symbolic reasoning; (2) Instruction Generation produces multi-turn, diverse, compositional prompts by maintaining an explicit object pool and sampling from nine instruction types spanning subject-, attribute-, relational-, text-, count-, and global-level edits; (3) Evaluation scores edited images along Instruction Following, Content Consistency, and Visual Quality, combining Grounding-DINO Liu et al. (2024a), Qwen-VL 2.5 Yang et al. (2024), DINOv3 Simeoni et al. (2025), pixel-wise L1, and the HPSv3 quality scorer Ma et al. (2025a). Our agentic pipeline is agnostic to the agent VLM/expert tools used and can be readily enhanced with more advanced tools in the future. 3.2 STEP 1: DECOMPOSITION Given an image, VLM-based agent parses clearly visible foreground objects and returns per-object JSON with fields object, color, material, text, count, and boolean foreground. Names follow \"{material} {color} {object}\"; unknown fields are omitted; person identity is never recorded (only wearables/accessories). Example: {\"metal yellow sign\": {\"object\":\"sign\",\"color\":\"yellow\",\"material\":\"metal\",\"text\":\"SCHOO L\",\"count\":1,\"foreground\":true}}. An aggregated all objects string concisely lists objects (e.g., metal yellow sign . metal brown pole). We apply this stage to GEdit-Bench Liu et al. (2025) (606 images), exclude 34 images with sensitive personal content, and retain 572 images. After extraction, Grounding-DINO validates objects and detects bounding boxes; only reliable detections are kept to seed instruction generation and evaluation. The filtered objects are stored in the All Objects Pool and later used to initialize three distinct object pools that dynamically track the evolving state of instruction generation. 3.3 STEP 2: INSTRUCTION GENERATION From the decomposed scene, the agent generates multi-turn edits that are grounded in the current object state. The instruction taxonomy (nine types; six categories) appears in Table 1. We (all objects ever present), unch maintain three evolving pools at turn t: all (original objects not edited up to t), and avail (objects currently editable). With turn budget MAX TURNS, at each turn the agent (i) selects typedefaulting to subject add if avail = , otherwise sampling type not yet used in the chain; (ii) selects object(s) from avail ; (iii) emits naturallanguage instruction via GPT-4o referencing those objects and the scene state; and (iv) updates all t+1 according to the intended edit. When background change edit applies at turn t, background-consistency scoring is disabled since this turn, and we append make {objects in foreground} unchanged to the instruction to preserve object-level comparability, where objects in foreground = { avail : o.foreground = true }. The loop is adaptive by expanding/contracting avail and naturally compositional. Our default sets MAX TURNS= 3 (In our implementation, each turn is assigned distinct instruction type.), though longer chains are easily obtained by allowing repetition or adding types. t+1 , and unch t+1, avail t 3.4 STEP 3: EVALUATION Instruction Following verifiable by typessubject add, subject remove, subject replace, position change, count changeare solely checked with Grounding-DINO Liu et al. (2024b) using prompts derived from the instruction and object descriptors; compliance uses detection confidence, Symbolically evaluability. separates types"
        },
        {
            "title": "Preprint",
            "content": "Table 1: Instruction types in EdiVal-Bench created by EdiVal-Agent, grouped by semantic category. Counts are shown per turn (T1T3). Category Instruction Type Example Instruction Subject-centric Attribute-centric subject add subject remove subject replace color alter material alter Text-related text change Relational position change Add bench on the left of metal red fire hydrant. Remove wooden brown door. Replace stone gray railing with wooden fence. Change the color of metal white airplane to blue. Change the material of plastic black pen to metal. Replace the text BEARS CONTROL on cotton black cap with WILD PATH. Change the position of ceramic white cup to right of plastic white laptop. Counting Global count change Change the count of fur brown bear to 3. background change Change the background to forest, remain the brown fur bear unchanged. T1 T2 T3 Total 67 75 54 56 66 64 77 69 57 73 50 93 61 55 57 72 54 237 205 166 186 188 52 63 48 163 73 58 55 60 72 191 counts, and geometry (existence and relative positions), following GENEVAL Ghosh et al. (2023) and T2I-CompBench Huang et al. (2023). Semantically verifiable typescolor alter, material alter, text change, and background changeare evaluated with Qwen2.5VL Yang et al. (2024) using instruction-specific templates (See details in Appendix), applied to detector-guided (i.e., Grounding DINO) object crops to focus on the edited regions. The multiturn instruction following accuracy aggregates per-instruction pass/fail along the chain (see details in Appendix). obj and Ωt bg. For objects, we crop each element of unch Content Consistency assesses whether non-target content is preserved at semantic and pixel levels. Let Bt denote the Grounding-DINO detections at turn t; define object and background regions as from 0 and and compute DINOv3 Ωt cosine similarity sobj = cos(cid:0)ϕ(I 0 obj obj1/Z, averaged over unchanged objects (larger is better). Here 1 sums over all pixels and channels within the crop, and normalizes the pixelwise L1 distance to [0, 1]; for region with channels, Ω pixels, and per-channel dynamic range (e.g., = 255 for 8-bit RGB or = 1 for [0, 1]-scaled floats), we set = Ω . For background, we mask Bt and compute the same two measures over Ω0 bg; if background change occurs at turn t, background scoring is disabled thereafter and omitted from the combined score. obj)(cid:1) and normalized pixel metric qobj = 1 0 bg versus Ωt obj), ϕ(I Visual Quality is measured using Human Preference Score v3 (HPSv3) Ma et al. (2025b), which yields scores aligned with human preference and captures both aesthetics and visual plausibility for image synthesis. We avoid general-purpose VLMs for aesthetic judgments because they are biased toward real-image semantics and less sensitive to synthesis artifacts. We also observe systematic overexposure in some models under multiturn editing; to quantify this effect, we also compute lowlevel exposure statistics (e.g., luminance percentiles). Design Scope and Limitations. We omit style change from the current taxonomy. Openvocabulary detectors optimized for natural images (e.g., Grounding-DINO) generalize poorly to strongly stylized domains, undermining reliable symbolic verification. Extending EdiVal-Agent with style-aware recognition is promising future work. After language-based extraction, we validate objects with Grounding-DINO, pruning low-confidence or ambiguous detections, and canonicalizing names to uniform schema. This stabilizes the object pools, reduces error propagation during instruction generation, and tightens the coupling between symbolic references and detector-visible entities. The pipeline is fully automated and scales linearly with dataset size; privacy is enforced by excluding sensitive images at ingestion and never recording identitiesonly attributes of visible clothing and accessories."
        },
        {
            "title": "4 MEASURING HUMAN AGREEMENT",
            "content": "Setup. We conduct human study on edits made by four exemplary models, Step1X-Edit, AnyEdit, Gemini-Flash 2.0 and Flux.1-Kontext-dev, on EdiVal-Bench, generated by EdiVal-Agent as described in Section 3.3. For each edit, we collect two human ratings for each edit, yielding total"
        },
        {
            "title": "Preprint",
            "content": "Figure 3: Results of human agreement. Dashed lines represent the average accuracy of each method. EdiVal-Agent achieves 81.3% human agreement accuracy, significantly outperforming the VLM (Qwen2.5-VL) at 75.2% and thresholded CLIP dir at 65.4%. Note that the CLIP dir threshold is tuned separately for each task. The inner-annotation baselines indicate the ceiling performance achievable by the tools. of 572 4 2 = 4,576 annotations. Raters were recruited online, each holding at least bachelors degree. During evaluation, they were shown the original image, the edited image, and the corresponding instruction. They were asked binary question: Evaluate whether the edited image successfully follows the given instruction. Results. Figure 3 summarizes the findings. EdiVal-Agent achieves human agreement accuracy of 81.3%, significantly higher than VLM-only (Qwen-2.5-VL, 75.2%), CLIP dir (65.4%), and other zero-shot VLMs. These results verify that integrating VLMs reasoning with object detection leads to better alignment with human judgment compared to existing methods. The inter-annotators agreement rate (85.5%) indicates the best performance any evaluation tool can reach. We attribute the improvement in instruction-following evaluation to two factors. First, for symbolically verifiable instruction typessubject add, subject remove, subject replace, position change, and count changeEdiVal-Agent relies solely on Grounding-DINO. It determines the success of an edit by checking object presence/absence, the positions of object centers, and the number of bounding boxes. Results for position change and subject remove show that these fixed rules, combined with Grounding-DINO, can significantly outperform Qwen2.5-VL in edit evaluation. We hypothesize that errors in position change stem from poor spatial reasoning, while failures in subject remove are due to hallucinations regarding object existence. Second, semantically verifiable typescolor alter, material alter, text change, and background changeare evaluated using Qwen2.5-VL combined with Grounding-DINO. The decomposition stage in EdiVal-Agent can supports evaluation by localizing text regions, enabling the LLM to reason more precisely about text edits. These findings demonstrate that EdiVal-Agent not only enhances interpretability but also improves the practical applicability of evaluation pipelines in real-world scenarios requiring human-like understanding."
        },
        {
            "title": "5 BENCHMARKING MULTI-TURN EDITING",
            "content": "Editing Models We evaluate three families of image editors: (i) diffusion-based models (InstructPix2Pix (IP2P), MagicBrush, UltraEdit, AnyEdit), (ii) flow-matching models (FLUX.1-Kontextdev, Qwen-Image-Edit, Step1X-Edit, OmniGen), and (iii) autoregressive/regeneration models (Gemini 2.0 Flash, Nano Banana, and GPT-Image-1)1. EdiVal-Bench We evaluate all models on EdiVal-Bench, generated by EdiVal-Agent as described in Section 3.3. The source images are from GEdit-Bench Liu et al. (2025), which contains 1These models are closed-source and lack public technical reports. We label them Autoregressive because they are integrated into an autoregressive language model in the web UI."
        },
        {
            "title": "Preprint",
            "content": "606 real-world images. For safety, we exclude 34 images with sensitive personal content, yielding 572 images successfully decomposed by EdiVal-Agent. During instruction generation we set MAX TURNS=3, producing 1,716 editing instructions spanning 9 types  (Table 1)  . In short, every model is evaluated on the same images and the same instructions, using the same metrics. Metrics As described in Section 3.4, we evaluate three complementary dimensions. First, instruction following αi is the image-success rate at turn the fraction of images for which all edits up to and including turn are successful. Second, content consistency captures the stability of unchanged objects and background across turns. Consistency is computed primarily using DINOv3 feature similarity for the object and background regions; we also report pixel-level L1 distances as diagnostic statistics. Let κi denote the content-consistency score at turn i, defined as the mean of the object and background DINOv3 similarities. To summarize overall performance on these two dimensions, we combine with geometric mean: Overall = αi κi, which favors models that achieve balanced gains in both instruction following and content preservation. Perceptual quality (e.g., aesthetics and luminance-based statistics) is reported separately and is not folded into Overall. We evaluate multi-turn editing across mix of closedand open-source models. Unless noted, higher is better; latency is measured in seconds per image (lower is better). We first summarize overall trends and quantitative results (Section 5.1), then analyze instruction following (Section 5.2) and consistency (Section 5.3). We also assess perceptual quality using both learned human-preference scores and low-level statistics (Section 5.4). Finally, because multi-turn instructions can often be compressed into single prompt, we compare multi-turn versus single-shot complex editing (Section 5.5). 5.1 OVERALL RESULTS Table 2: Results of multi-turn editing. Instruction following, content consistency, and overall across three sequential editing turns. Overall is the geometric mean of instruction following and content consistency. Best per column in dark red; second-best in lighter red. Latency is seconds per image (lower is better). Latency Instruction Following Content Consistency Overall Technique Model Date (s/img) T1 T2 T3 T2 T3 T1 T2 T3 Autoregressive Flow Matching Diffusion Nano Banana GPT-Image-1 Gemini 2.0 Flash Qwen-Image-Edit Step1X-Edit FLUX.1-Kontext-dev OmniGen AnyEdit UltraEdit MagicBrush IP2P 25.08.26 25.07.16 25.02. 25.08.04 25.04.25 25.06.25 24.09.11 24.11.24 24.07.07 23.06.16 23.12.15 9.70 71.30 8.20 115.08 20.42 29.21 19.70 3.93 3.15 4.08 4.09 70.70 73.12 68. 72.90 61.89 59.97 54.72 41.07 51.37 42.31 37.41 50.66 54.89 45.96 44.06 34.97 32.69 24.48 16.32 17.70 15.73 10.66 35.35 38.35 28. 22.55 17.83 16.61 10.66 7.22 6.36 4.90 2.80 93.91 81.00 90.58 84.22 92.76 95.32 93.00 86.42 86.80 86.96 76.85 90.48 77.78 85. 80.52 88.52 92.24 88.42 78.91 84.50 81.26 68.36 89.48 75.50 80.88 77.98 85.21 90.22 83.92 70.10 82.40 76.86 60.30 81.48 76.96 78. 78.36 75.77 75.61 71.34 59.58 66.78 60.66 53.62 67.70 65.34 62.54 59.56 55.64 54.91 46.52 35.89 38.67 35.75 26.99 56.24 53.81 47. 41.93 38.98 38.71 29.91 22.50 22.89 19.41 12.99 Summary. Based on the data in Table 2, across three turns, Nano Banana offers the best speedquality trade-offhighest Overall at T1/T2/T3 (81.48/67.70/56.24) with 9.7 s/img. GPTImage-1 delivers the strongest instruction following across turns, but its latency (71.3 s/img)2 and weaker consistency leave it second in Overall; Nano Banana trails GPT-4o by only 4.2 (T2) and 3.0 (T3) points on instruction following. For consistency, FLUX.1-Kontext-dev leads across turns with Nano Banana close behind, whereas GPT-Image-1 ranks second-to-lastconsistent with more regenerative/restyling behavior that can erode pixel/feature stability despite aesthetic gains. Gemini 2.0 Flash is competitive at T1 (second-best Overall) but exhibits steeper decline by T3. Among open-source systems, Qwen-Image-Edit is strongest at T1 (Overall 78.36) yet degrades rapidly with additional turns, likely due to exposure bias from single-turn training on real images and short edit-history window that forces the model to operate on its own outputs, more evidence in Section 5.2 and 5.5. 2Closed-source latencies are measured in the providers hosted web UI; open-source latencies on single NVIDIA A100 GPU with default settings."
        },
        {
            "title": "Preprint",
            "content": "5."
        },
        {
            "title": "INSTRUCTION FOLLOWING",
            "content": "Marginal Task Success Rate. We analyze instruction-following ability across nine edit tasks. For given turn, the marginal task success rate is the proportion of prompts for which the requested edit in that turn is successfully In contrast, the instruction folimplemented. lowing score reported in Table 2 corresponds to the multi-turn task success rate at turn ithe fraction of images for which all edits up to turn are successful. Figure 4 shows the evolution of per-turn performance. Models that are AR (autoregressive) and condition on the full editing history are relatively stable across turns: their marginal task success rates change only slightly between turns. By contrast, non-AR models with very short history window (effectively conditioning only on the previous output) suffer substantial degradation in marginal task success, particularly for flow-matching style models. Figure 4: Marginal instruction-following across turns. striking example is Qwen-Image-Edit. It is the strongest open-source system at turn 1 (Overall 78.36 vs. 81.48 for Nano Banana) but degrades much faster over subsequent turns. We hypothesize that this is primarily an exposure bias issue: many single-turn edit models are trained to operate on real images and ground-truth inputs, not on their own earlier outputs. When model must operate on its own previous edits, small distribution mismatches compound across turns and reduce stabilitythis effect is exacerbated when the model can attend only to short slice of the history. This phenomenon suggests that naively applying single-turn edit models to multi-turn editing will amplify exposure bias and lead to rapid failure. To mitigate this, multi-turn training and inference strategies should be considered, for example: training with model-generated rollouts (teacherstudent or scheduled sampling) to reduce distribution mismatch, or increasing the history window / using full-history conditioning when feasible. In summary, autoregressive/full-history architectures exhibit greater stability in multi-turn editing, whereas short-window non-AR modelsespecially those trained for single-turn flow matchingare more vulnerable to exposure bias. Future work should evaluate the mitigation strategies above and quantify their impact on multi-turn robustness. Marginal Success Rate Across Instruction Types. We analyze marginal subtask success rates across turns for different instruction types. The results for GPT-Image-1 are shown in Fig. 5. Other editing models exhibit similar behavior. GPT-Image-1 performs relatively well on attribute-centric tasks such as color alter and material alter, but poorly on position change and count change, indicating weaknesses in spatial and numerical reasoning, respectively. Among all subtasks, count change is the most challenging. Even the best-performing model (GPTImage-1) achieves success rate below 25% at turn 1, while most models remain under 5%. We also provide illustrative examples in Figure 6. Notably, all non-AR models fail on this prompt. 5.3 CONTENT CONSISTENCY Beyond successfully implementing editing instructions, multi-turn editing requires attention to consistency. We evaluate two aspects: (i) unchanged-object consistency, which measures whether objects that are not edited up to turn remain unchanged, and (ii) background consistency, which assesses whether the background remains stable when it is not explicitly modified. We can evaluate consistency in two ways: 1) extract object features with DINOv3 and compute cosine similarity; 2) measure pixel-level consistency using the L1 distance and report one minus this value. Note that when calculating consistency, the grounding box is extracted from the raw input image and applied to all edited images. We therefore choose to report DINOv3 for consistency"
        },
        {
            "title": "Preprint",
            "content": "Figure 5: Marginal task success rate grouped by task types for GPT-Image-1. It performs relatively well on attribute-centric tasks such as color alter and material alter, but poorly on position change and count change, indicating weaknesses in spatial and numerical reasoning, respectively. We note that other editing models exhibit similar behavior. (a) Base Image (b) GPT-Image-1 () (c) Nano Banana () (d) Gemini 2.0 Flash () (e) Qwen-Image-Edit () Figure 6: Example of the count change task: changing the number of paper cups to five. computation because even small shifts in object location can lead to large variations in pixel-wise L1 loss, even when unchanged objects are well preserved. By relying on DINO features, we ensure that consistency is measured semantically, capturing attributes such as object identity, attributes, and texture, etc. Nevertheless, the consistency scores from DINOv3 remain highly correlated with those computed using pixel-wise L1 loss (See results in the Appendix). Based on the results, the most consistent editing model is FLUX.1-Kontext-dev, followed by Nano Banana. In contrast, GPT-Image-1 ranks near the bottom, showing notably poor consistency across turns. Representative qualitative examples are shown in Figure 7 and Figure 8. (a) Base Image (b) GPT-Image-1 (95.19) (c) Nano Banana (98.05) (d) Qwen-Image-Edit (94.96) Figure 7: Illustration of object consistency. Instruction: Remove brick beige house. The grounding box, extracted from the raw input image, highlights the localized region used to compute unchanged-object consistency. The corresponding consistency score is shown in brackets. 5.4 VISUAL QUALITY Beyond instruction following and content consistency, the perceptual quality of the edited image is key dimension. We therefore report (i) learned aesthetic score and (ii) several low-level image statistics that can surface systematic artifacts and drift in multi-turn editing pipelines. 5.4.1 AESTHETICS"
        },
        {
            "title": "Preprint",
            "content": "We quantify aesthetics with HPSv3 Ma et al. (2025a), which we found to generalize reliably to our generated images, whereas alternatives (e.g., RAHF Liang et al. (2024)) underperform in this setting. We do not fold these quality metrics into the aggregate Overall score, as preferences differ on whether an edited image should strictly preserve the input style or pursue beautification. (a) Base Image (cid:12)HPSturn HPSbase To disentangle these preferences, we report the absolute change in aesthetic score relative to the (cid:12) base image: = (cid:12) (cid:12). Smaller indicates stronger style fidelity to the base image; larger reflects greater beautification or stylistic drift. As summarized in Table 3, GPT-Image-1 achieves the highest aesthetic scores across turns and remains stable. Qwen-Image-Edit is the next strongest on absolute HPS. For preserving the base images look (small ), Gemini 2.0 Flash shows the least drift, with Nano Banana also performing well. Illustration of background consisFigure 8: tency. Instruction: Remove brick beige house. The grounding box is derived from the union of the raw and edited images, excluding the removed object region. This box is used to compute background consistency. (b) Qwen-Image-Edit Table 3: Human Preference Scores (HPS), absolute change , and techniques across three refinement dark red denotes the best value in the colturns. umn (i.e., the least difference in human preference score); lighter red denotes the second-best. For the HPS columns higher values are better (stronger perceived aesthetics). For the columns smaller values are better (stronger fidelity to the base image). Technique Model Human Preference Score Autoregressive Flow Matching Diffusion Nano Banana GPT-Image-1 Gemini 2.0 Flash Qwen-Image-Edit Step1X-Edit FLUX.1-Kontext-dev OmniGen AnyEdit UltraEdit MagicBrush IP2P T1 4.94 6.65 4. 5.86 4.06 5.12 4.61 3.66 4.79 3.85 3.20 T2 5.12 6.59 4.23 5.72 3.34 5.07 4.07 2.80 4.68 3.08 2. T3 5.26 6.56 4.07 5.15 2.76 5.04 3.50 1.95 4.36 2.36 1.44 0.73 2.21 0.15 1.34 1.04 0.69 0.31 1.58 0.30 1.30 2.01 T1 0.56 2.27 0.05 1.47 0.33 0.73 0. 0.72 0.41 0.54 1.18 T3 0.88 2.18 0.32 0.77 1.63 0.65 0.89 2.44 0.02 2.02 2.94 Figure 9: Per-image 99.9% luminance quantile across turns. Higher values indicate more extreme bright pixels and greater risk of over-exposure."
        },
        {
            "title": "5.4.2 LOW-LEVEL IMAGE STATISTICS",
            "content": "In addition to learned aesthetic scores, we compute several low-level image statistics that help reveal systematic, multi-turn editing artifacts. Concretely, we convert RGB pixels to luminance using the Rec. 709 luma coefficients: = 0.2126 + 0.7152 + 0.0722 B, and for each edited image we extract the 99.9% luminance quantile (the per-image pixel value below which 99.9% of pixels fall). The 99.9% quantile is sensitive to high-exposure pixels and therefore highlights over-exposure and bright streaks while being robust to single-pixel outliers. In Figure 9 we plot the trend of this statistic across turns. The measured trend shows clear pattern: Qwen-Image-Edit and several other flow-matching models (with the notable exception of FLUX.1-Kontext-dev) exhibit pronounced increase in the 99.9% luminance quantile over turns, indicating progressive brightening and increased risk of overexposure. By contrast, regeneration-style editors such as GPT-Image-1 tend to produce lower luminance values than the input (reflecting darker, more conservative reconstructions), and several models remain stable across turns. Figure 10 provides qualitative examples from Qwen-Image-Edit. The edited images exhibit elevated luminance and noticeable high-frequency bright artifacts (e.g., white streaks or line textures) that degrade perceptual quality, with luminance quintiles increasing substantially. Correspondingly, HPS"
        },
        {
            "title": "Preprint",
            "content": "drops from 6.19 to 4.19 and 3.34, suggesting that HPS is sensitive to over-exposure to some extent. In contrast, when querying VLMs about the visual quality of these images, the returned scores do not change in the first two turns and remain consistently above 50, reflecting positive evaluation under the [0, 100] scale, while the T2/T3 edited images show significant artifacts. (a) HPS: 4.25 VLM: 85 Luminance:0.7 (b) HPS: 6.19 VLM: 85 Luminance: 0.60 (c) HPS: 4.19 VLM: 85 Luminance: 0.97 (d) HPS: 3.34 VLM: 60 Luminance: 1.00 Figure 10: Representative Qwen-Image-Edit examples illustrating over-exposure and bright artifact formation across turns. Although editing instructions are often satisfied, the images show elevated luminance and high-frequency bright streaks that accompany the edits (visible especially in T2/T3). Editing instructions: [Remove polyester white skirt, Change the count of tennis ball to 4, Change the color of tank top to blue]. Note that VLM gives positive score to all the images. 5.5 MULTI-TURN EDITING VS. COMPLEX EDITING We compare two strategies for applying multiple edits. In multi-turn editing, instructions are executed sequentially: apply instruction 1, then apply instruction 2 to the result, and so on. In complex editing, we concatenate instructions into single prompt and perform one edit (complex level C, {1, 2, 3}). We evaluate two metrics: (i) the Turn-3 instruction-following rate (all edits up to turn 3 succeed), and (ii) the marginal success of the final instruction in C-instruction complex prompt. Empirically (Table 4; Figure 11), flow-matching editors (short-history condition) behave differently from AR editors when comparing multi-turn and complex modes. AR models tend to achieve higher final-task success in the multi-turn setting, demonstrating the benefit of step-by-step chain of edits (analogous to chain-of-thought for reasoning). By contrast, many flow-matching models obtain better final-task performance under complex (single-shot) editing: sequential edits amplify failure modes (most notably exposure-related drift), which degrades multi-turn performance. Figure 11 shows that marginal success for the final instruction remains largely stable as complex prompt length increases. Together with the multi-turn drops seen in Figure 4, this pattern supports an exposure-bias explanation: performance degradation primarily stems from error accumulation across sequential edits rather than an intrinsic inability to handle multiple instructions in single prompt. Practical recommendation: For true step-by-step workflows, we recommend using autoregressive architectures. For flow-matching editors, it is preferable to either (i) pack multiple edits into single complex prompt when the instruction sequence is known in advance, or (ii) increase the models history window and/or add inter-turn regularization (e.g., histogram matching, exposure penalties, or periodic reconstruction passes) to mitigate cumulative drift."
        },
        {
            "title": "6 LIMITATION AND DISCUSSION",
            "content": "Given the object-centric evaluations conducted in this study, several limitations warrant consideration. First, our instruction types are limited to object-centric prompts, which may not capture the full range of creative editing requests typical in real-world scenarios. Future research should explore broader spectrum of instructions, including those involving stylistic changes or complex narrative elements."
        },
        {
            "title": "Preprint",
            "content": "Table 4: Turn-3 instruction following: Multi-turn vs. single-shot complex prompts, grouped by technique. Bold indicates which setting is higher for each model. Technique Model Multi-turn (T3) Complex (C3) Autoregressive Flow Matching Diffusion Nano Banana GPT-Image-1 Gemini 2.0 Flash Qwen-Image-Edit Step1X-Edit FLUX.1-Kontext-dev OmniGen AnyEdit UltraEdit MagicBrush IP2P 35.35 38.35 28.42 22.55 17.83 16.61 10. 7.22 6.36 4.90 2.80 28.14 28.78 21.89 27.62 15.73 19.58 11.01 2.80 8.22 4.55 2.80 Figure 11: Marginal task-success rate of the last instruction as function of complex prompt length (levels = 1, 2, 3). Additionally, while our work provides reliable and comprehensive evaluation framework for multiturn editing, it does not apply the evaluation results to improve the editing models themselves. straightforward extension would be to use evaluation scores for Best-of-N selection to improve inference-time performance. Future work could also explore post-training methods such as reinforcement learning, treating the evaluation scores as reward signals."
        },
        {
            "title": "7 CONCLUSION",
            "content": "We introduced EdiVal-Agent, scalable, automated, and interpretable framework for evaluating instruction-based image editing. By leveraging symbolic object decomposition, structured instruction generation, and hybrid evaluation pipeline integrating both specialist tools and vision-language reasoning models, EdiVal-Agent enables fine-grained, object-centric assessment of modern editing systems. Our design emphasizes transparency, extensibility, and real-world applicabilityproviding new standard for evaluating multi-turn, compositional visual editing. We hope EdiVal-Agent will serve as valuable resource for benchmarking, diagnosing, and advancing the next generation of instruction-based editing models."
        },
        {
            "title": "REFERENCES",
            "content": "Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. Zechen Bai, Pichao Wang, Tianjun Xiao, Tong He, Zongbo Han, Zheng Zhang, and Mike Zheng arXiv preprint large language models: survey. Shou. Hallucination of multimodal arXiv:2404.18930, 2024. Tim Brooks, Aleksander Holynski, and Alexei Efros. Instructpix2pix: Learning to follow image editing instructions. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1839218402, 2023. Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the International Conference on Computer Vision (ICCV), 2021. Yingshan Chang, Yasi Zhang, Zhiyuan Fang, Ying Nian Wu, Yonatan Bisk, and Feng Gao. Skews in the phenomenon space hinder generalization in text-to-image generation. In Aleˇs Leonardis, Elisa Ricci, Stefan Roth, Olga Russakovsky, Torsten Sattler, and Gul Varol (eds.), Computer Vision ECCV 2024, pp. 422439, Cham, 2025. Springer Nature Switzerland. ISBN 978-3-031-73021-4. Boyuan Chen, Zhuo Xu, Sean Kirmani, Brain Ichter, Dorsa Sadigh, Leonidas Guibas, and Fei Xia. Spatialvlm: Endowing vision-language models with spatial reasoning capabilities. In Proceedings"
        },
        {
            "title": "Preprint",
            "content": "of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1445514465, 2024. An-Chieh Cheng, Hongxu Yin, Yang Fu, Qiushan Guo, Ruihan Yang, Jan Kautz, Xiaolong Wang, and Sifei Liu. Spatialrgpt: Grounded spatial reasoning in vision-language models. Advances in Neural Information Processing Systems, 37:135062135093, 2024. Google Deepmind. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities, 2025. URL https://arxiv.org/abs/ 2507.06261. Rinon Gal, Or Patashnik, Haggai Maron, Amit Bermano, Gal Chechik, and Daniel CohenOr. Stylegan-nada: Clip-guided domain adaptation of image generators. ACM Transactions on Graphics (TOG), 41(4):113, 2022. Gemini2. generation. Google age experiment-with-gemini-20-flash-native-image-generation/, Accessed: 2025-06-22. im2.0 https://developers.googleblog.com/en/ 2025."
        },
        {
            "title": "Experiment",
            "content": "gemini native flash with Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating text-to-image alignment. Advances in Neural Information Processing Systems, 36: 5213252152, 2023. Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt image editing with cross-attention control. In ICLR, 2023. Kaiyi Huang, Kaiyue Sun, Enze Xie, Zhenguo Li, and Xihui Liu. T2i-compbench: comprehensive benchmark for open-world compositional text-to-image generation. Advances in Neural Information Processing Systems, 36:7872378747, 2023. Mude Hui, Siwei Yang, Bingchen Zhao, Yichun Shi, Heng Wang, Peng Wang, Yuyin Zhou, and Cihang Xie. Hq-edit: high-quality dataset for instruction-based image editing. arXiv preprint arXiv:2404.09990, 2024. Max Ku, Dongfu Jiang, Cong Wei, Xiang Yue, and Wenhu Chen. Viescore: Towards explainable metrics for conditional image synthesis evaluation. arXiv preprint arXiv:2312.14867, 2023. Black Forest Labs, Stephen Batifol, Andreas Blattmann, Frederic Boesel, Saksham Consul, Cyril Diagne, Tim Dockhorn, Jack English, Zion English, Patrick Esser, Sumith Kulal, Kyle Lacey, Yam Levi, Cheng Li, Dominik Lorenz, Jonas Muller, Dustin Podell, Robin Rombach, Harry Saini, Axel Sauer, and Luke Smith. Flux.1 kontext: Flow matching for in-context image generation and editing in latent space, 2025. URL https://arxiv.org/abs/2506.15742. Youwei Liang, Junfeng He, Gang Li, Peizhao Li, Arseniy Klimovskiy, Nicholas Carolan, Jiao Sun, Jordi Pont-Tuset, Sarah Young, Feng Yang, Junjie Ke, Krishnamurthy Dj Dvijotham, Katie Collins, Yiwen Luo, Yang Li, Kai Kohlhoff, Deepak Ramachandran, and Vidhya Navalpakkam. Rich human feedback for text-to-image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024. Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Qing Jiang, Chunyuan Li, Jianwei Yang, Hang Su, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. In European conference on computer vision, pp. 3855. Springer, 2024a. Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Qing Jiang, Chunyuan Li, Jianwei Yang, Hang Su, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. In European Conference on Computer Vision, pp. 3855. Springer, 2024b."
        },
        {
            "title": "Preprint",
            "content": "Shiyu Liu, Yucheng Han, Peng Xing, Fukun Yin, Rui Wang, Wei Cheng, Jiaqi Liao, Yingming Wang, Honghao Fu, Chunrui Han, et al. Step1x-edit: practical framework for general image editing. arXiv preprint arXiv:2504.17761, 2025. Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. Yuhang Ma, Xiaoshi Wu, Keqiang Sun, and Hongsheng Li. Hpsv3: Towards wide-spectrum human preference score, 2025a. URL https://arxiv.org/abs/2508.03789. Yuhang Ma, Xiaoshi Wu, Keqiang Sun, and Hongsheng Li. Hpsv3: Towards wide-spectrum human preference score. arXiv preprint arXiv:2508.03789, 2025b. OpenAI."
        },
        {
            "title": "Introducing",
            "content": "4o image generation. https://openai.com/index/ introducing-4o-image-generation/, 2025. Accessed: 2025-06-22. Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. SDXL: Improving latent diffusion models for high-resolution image In The Twelfth International Conference on Learning Representations, 2024. URL synthesis. https://openreview.net/forum?id=di52zR8xgf. Muhammad Fetrat Qharabagh, Mohammadreza Ghofrani, and Kimon Fountoulakis. count: Enhancing the counting ability of large vision-language models. arXiv:2412.00686, 2024. LvlmarXiv preprint Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pp. 87488763. PmLR, 2021. Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical textconditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1(2):3, 2022. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1068410695, 2022. Shelly Sheynin, Adam Polyak, Uriel Singer, Yuval Kirstain, Amit Zohar, Oron Ashual, Devi Parikh, and Yaniv Taigman. Emu edit: Precise image editing via recognition and generation tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 8871 8879, 2024. Oriane Simeoni, Huy Vo, Maximilian Seitzer, Federico Baldassarre, Maxime Oquab, Cijo Jose, Vasil Khalidov, Marc Szafraniec, Seungeun Yi, Michael Ramamonjisoa, et al. Dinov3. arXiv preprint arXiv:2508.10104, 2025. An Vo, Khai-Nguyen Nguyen, Mohammad Reza Taesiri, Vy Tuong Dang, Anh Totti Nguyen, and Daeyoung Kim. Vision language models are biased. arXiv preprint arXiv:2505.23941, 2025. Chenfei Wu, Jiahao Li, Jingren Zhou, Junyang Lin, Kaiyuan Gao, Kun Yan, Sheng ming Yin, Shuai Bai, Xiao Xu, Yilei Chen, Yuxiang Chen, Zecheng Tang, Zekai Zhang, Zhengyi Wang, An Yang, Bowen Yu, Chen Cheng, Dayiheng Liu, Deqing Li, Hang Zhang, Hao Meng, Hu Wei, Jingyuan Ni, Kai Chen, Kuan Cao, Liang Peng, Lin Qu, Minggang Wu, Peng Wang, Shuting Yu, Tingkun Wen, Wensen Feng, Xiaoxiao Xu, Yi Wang, Yichang Zhang, Yongqiang Zhu, Yujia Wu, Yuxuan Cai, and Zenan Liu. Qwen-image technical report, 2025a. URL https://arxiv.org/abs/ 2508.02324. Chenyuan Wu, Pengfei Zheng, Ruiran Yan, Shitao Xiao, Xin Luo, Yueze Wang, Wanli Li, Xiyan Jiang, Yexin Liu, Junjie Zhou, Ze Liu, Ziyi Xia, Chaofan Li, Haoge Deng, Jiahao Wang, Kun Luo, Bo Zhang, Defu Lian, Xinlong Wang, Zhongyuan Wang, Tiejun Huang, and Zheng Liu. Omnigen2: Exploration to advanced multimodal generation. arXiv preprint arXiv:2506.18871, 2025b."
        },
        {
            "title": "Preprint",
            "content": "Shitao Xiao, Yueze Wang, Junjie Zhou, Huaying Yuan, Xingrun Xing, Ruiran Yan, Chaofan Li, Shuting Wang, Tiejun Huang, and Zheng Liu. Omnigen: Unified image generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1329413304, 2025. Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagereward: learning and evaluating human preferences for text-to-image generation. In Proceedings of the 37th International Conference on Neural Information Processing Systems, pp. 1590315935, 2023. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2024. Siwei Yang, Mude Hui, Bingchen Zhao, Yuyin Zhou, Nataniel Ruiz, and Cihang Xie. Complexedit: Cot-like instruction generation for complexity-controllable image editing benchmark. arXiv preprint arXiv:2504.13143, 2025. Qifan Yu, Wei Chow, Zhongqi Yue, Kaihang Pan, Yang Wu, Xiaoyang Wan, Juncheng Li, Siliang Tang, Hanwang Zhang, and Yueting Zhuang. Anyedit: Mastering unified high-quality image editing for any idea. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 2612526135, 2025. Kai Zhang, Lingbo Mo, Wenhu Chen, Huan Sun, and Yu Su. Magicbrush: manually annotated dataset for instruction-guided image editing. Advances in Neural Information Processing Systems, 36:3142831449, 2023. Yasi Zhang, Peiyu Yu, Yaxuan Zhu, Yingshan Chang, Feng Gao, Ying Nian Wu, and Oscar Leong. Flow priors for linear inverse problems via iterative corrupted trajectory matching. In The Thirtyeighth Annual Conference on Neural Information Processing Systems, 2024. URL https:// openreview.net/forum?id=1H2e7USI09. Yasi Zhang, Peiyu Yu, and Ying Nian Wu. Object-conditioned energy-based attention map alignment in text-to-image diffusion models. In Aleˇs Leonardis, Elisa Ricci, Stefan Roth, Olga Russakovsky, Torsten Sattler, and Gul Varol (eds.), Computer Vision ECCV 2024, pp. 5571, Cham, 2025a. Springer Nature Switzerland. ISBN 978-3-031-72946-1. Zheyuan Zhang, Fengyuan Hu, Jayjun Lee, Freda Shi, Parisa Kordjamshidi, Joyce Chai, and Ziqiao Ma. Do vision-language models represent space and how? evaluating spatial frame of reference In The Thirteenth International Conference on Learning Representations, under ambiguities. 2025b. URL https://openreview.net/forum?id=84pDoCD4lH. Haozhe Zhao, Xiaojian Shawn Ma, Liang Chen, Shuzheng Si, Rujie Wu, Kaikai An, Peiyu Yu, Minjia Zhang, Qing Li, and Baobao Chang. Ultraedit: Instruction-based fine-grained image editing at scale. Advances in Neural Information Processing Systems, 37:30583093, 2024."
        },
        {
            "title": "Preprint",
            "content": "Algorithm 1 Object Listing and Grounding Filter Require: image Ensure: object pool with grounding metadata 1: LISTOBJECTS(I) 2: 3: for all (name, attrs) excluding key All Objects do (boxes, phrases, scores) GROUND(I, name) 4: if boxes = and each box has w, < 0.9 and area 0.4 then 5: 6: 7: 8: end for 9: O[Filtered All Objects] JOIN(KEYS(O), . ) and then append . O[name] attrs; attach grounding metadata (count, boxes, phrases, scores) end if see Sec. A.2.1 thresholds 0.30."
        },
        {
            "title": "A ALGORITHMIC DETAILS",
            "content": "This appendix provides the algorithmic details of our pipeline: object discovery and groundingbased filtering (decomposition), instruction generation for multi-turn editing, and evaluation (instruction following, consistency, and perceptual quality). We also list the exact prompts and implementation specifics needed for reproducibility, and summarize the model-generation configurations. A.1 DECOMPOSITION We first enumerate visible objects in an input image using vision-language model (VLM) prompt, then filter these objects via visual grounding. Object listing: We use GPT-4o with the prompt in Section A.2.1. The model returns JSON with one entry per object and terminal aggregated string key All Objects. Grounding filter: We use GroundingDINO SwinT-OGC Liu et al. (2024a) to retain only objects that can be visually grounded. We resize images to 512 512. We keep detections meeting text/box thresholds (0.35) and reject oversized boxes by checking width/height in normalized coordinates; we use max box size=0.9 and filter large regions if area > 0.4. The output augments each kept object with grounding counts, phrases, boxes, and scores, and creates Filtered All Objects string listing retained objects. A. INSTRUCTION GENERATION We generate multi-turn editing instructions from the grounded object pool. We support local edits {subject replace, subject remove, material alter, nine task types: color alter, subject add, text change, position change, count change} and the global edit {background change}. We set MAX TURNS=3. At each turn, we sample new task type without repetition where feasible. Feasibility is checked against the current object pool (e.g., position change requires at least two objects). If sampled task is infeasible, we fall back to subject add. We maintain an available-objects pool that is updated after each instruction according to its semantics (adds, removes, or modifies attributes). If background change occurs, we mark bg consistency=false for subsequent turns and restrict the pool to foreground objects for the remainder of the episode. Prompts (Full Text) Below we reproduce the prompts used by our generators, reformatted for readability in print (content preserved). A.2.1 OBJECT LISTING PROMPT You will be given an image. Your task is to identify and describe all clearly visible objects in the image in structured JSON format. Output rules:"
        },
        {
            "title": "Preprint",
            "content": "Algorithm 2 Multi-Turn Instruction Generation Require: grounded pool O0, turns =3 Ensure: tasks {τt}, instructions {It}, formats {Ft}, flag has bg, set all objects ever 1: used ; O0; has bg false 2: all edited ; all objects ever keys(O0) 3: for = 1 to do 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: end for 18: return {τt}, {It}, {Ft}, has bg, all objects ever cand {all task types} used τt sample(cand); if not feasible(τt, O) then τt subject add end if Ft format instruction(τt, O) It render instruction(Ft, τt, O) used used {τt}; append Ft, It Update all object pool by adding any objects introduced in instruction It. Update available object pool by adding or removing objects as specified in It. Update unchanged objects pool by removing any objects affected by It. if τt = background change then has bg true; filter foreground(O) end if Query VLM by prompts in Section A.2.2 strip brackets; add unchanged list for background 1. Each object must be listed as key in the JSON, using the format: {material} {color} {object name}. If the material or color is unknown, omit that part. Do not include any visible text in the key. Do not use person as an object name; instead, describe wearable items (e.g., blue cotton shirt). 2. For each object, the value is dictionary with fields: object (type, e.g., shirt, cup), color (dominant color, use null if unknown), material (likely material, use null if unknown), text (visible text, null if none), count (number of instances), and foreground (boolean). 3. Do not include objects that are too small to describe, mostly occluded/incomplete, or only background scenery (e.g., distant sky, wall, floor). 4. Add final key All Objects whose value is single string listing all object names, formatted as: {material} {color} {object name}. {color} {object name}. {material} {object name}. {object name}. Exclude null/None and separate entries by . (period + space). Do not include any text content in this list. Example output (abridged JSON): cotton blue shirt: {object: shirt, color: blue, material: cotton, text: null, count: 1, foreground: true} ceramic white cup: {object: cup, color: white, material: ceramic, text: GOOD DAY, count: 1, foreground: false} leather bag: {object: bag, color: null, material: leather, text: null, count: 2, foreground: true} red scarf: {object: scarf, color: red, material: null, text: null, count: 1, foreground: true} All Objects: cotton blue shirt. ceramic white cup. leather bag. red scarf. A.2.2 TASK PROMPTS Subject Replace You are given an image and asked to suggest replacement object for specific object in the scene. Given object to replace: object name Your task: 1. Understand the scene context. 2. Suggest new object that naturally replaces object name."
        },
        {
            "title": "Preprint",
            "content": "3. Ensure the suggestion is realistic for the scene. 4. Respond with only the object name (e.g., chair, lamp, book). Examples: In kitchen: bowl, mug; on street: bus, truck; in an office: stool, bench. Answer format: New object name: Material Alter You are given an image and asked to suggest new material for specific object. Object: object name Current material: current material Your task: 1. Identify the object. 2. Suggest realistic alternative material that is easy to distinguish from the current one. 3. Respond with only the material name (e.g., wood, metal, plastic, leather). Examples: cup: ceramic, glass, metal, plastic; chair: wood, metal, plastic, fabric; bag: leather, canvas, nylon, fabric. Answer format: New material: Position Change You are given an image and asked to create position change instruction. Available objects: available objects Positions: left, right, above, below Your task: 1. Select target object to move and reference object. 2. Choose relative position (left, right, above, below). 3. Ensure the instruction is physically reasonable. 4. Format: Change the position of [target object] to [position] of [reference object]. Examples: Change the position of [cup] to [right] of [book]; Change the position of [lamp] to [above] of [table]. Answer format: Position change instruction: Count Change You are given an image and asked to create count change instruction. Available objects: available objects Target count: target count Your task: 1. Identify suitable object for the requested count. 2. Ensure the target count is realistic for the scene. 3. Format: Change the count of [object name] to [target count]. Examples: Change the count of [cup] to [3]; Change the count of [book] to [2]. Answer format: Count change instruction: Text Change You are given an image and asked to generate new text content. Context: text situation Your task: 1. Generate text that fits the scene. 2. Keep text short: max 2 words in English or 4 Chinese characters. 3. Respond with only the text content (no quotes or extra words). Examples: coffee shop: COFFEE, OPEN; book: NOVEL, GUIDE; sign: EXIT, STOP; Chinese: 咖啡 , 出口 . Answer format: New text: Color Alter"
        },
        {
            "title": "Preprint",
            "content": "You are given an image and asked to suggest new color for specific object. Object: object name Current color: current color Your task: 1. Suggest simple, common color that fits the object. 2. Use only basic colors: red, blue, green, yellow, black, white, brown, gray, orange, purple, pink. 3. Choose color different from the current color and answer with the color name only. Answer format: New color: Subject Add You are given an image and asked to suggest new object to add to the scene. Reference object: reference object Position: position Your task: 1. Propose an object that would naturally fit at the specified position relative to the reference object. 2. Ensure the suggestion is realistic and contextually appropriate. 3. Respond with only the object name (e.g., lamp, book, cup). Examples: next to desk: chair, lamp, computer; near kitchen counter: bowl, plate, mug; by window: plant, curtain, book. Answer format: New object: Background Change You are given an image and asked to suggest new background for the scene. The existing objects should remain unchanged. Your task: 1. Propose new background that works with the current setting. 2. Keep it simple and realistic; use 12 words (e.g., kitchen, office, garden, beach, forest). 3. Respond with only the background name. Answer format: New background: A.3 EVALUATION We evaluate in two modes: (i) Multi-turn (each turn edits the output of the previous turn), and (ii) Complex Editing (compress all instructions to single prompt). Instruction Following. We compute binary success per instruction with detector combining GroundingDINO Liu et al. (2024a) and VLM (Qwen2-VL-7B) Bai et al. (2025). Representative details: Detector thresholds. Unless noted per task, GroundingDINO thresholds are 0.30.4; detections return normalized boxes [x1, y1, x2, y2]. Cropping and small objects. For object-level checks we crop by detected boxes; very small boxes (< 0.05 in width/height) can be enlarged before VLM queries. Replace. Detect old and new objects in source/target; success if both are detected and any IoU between source box (old) and target box (new) is > 0. VLM pre-check rejects obvious non-replacements. See details in Alg 3. Remove. Detect the object in the source; success if the object is absent in the target. See details in Alg 4. Position change. Detect target and reference objects and verify the requested spatial relation using object centers; also ensure the object count did not increase spuriously. See details in Alg 6. Count change. Use the detector to locate instances of the target object and take the number of validated detections as the count. See details in Alg 7."
        },
        {
            "title": "Preprint",
            "content": "Algorithm 3 Evaluate Subject Replace Tn DETECT(T, n, τ ) Require: base B, target , old object name o, new object name Ensure: success flag succ 1: DETECT(B, o, τ ); 2: if = Tn = then 3: 4: else 5: 6: end if 7: return succ succ maxbS, tTn IOU(b, t) > 0 succ false Algorithm 4 Evaluate Subject Remove Require: base B, target , object name Ensure: success flag succ 1: DETECT(B, o, τ ); 2: succ (S = To = ) 3: return succ To DETECT(T, o, τ ) Color/material. Crop the object in the target and ask the VLM yes/no question about the new color/material. See details in Alg 8 and Alg 9. Text change. If the instruction adds text anywhere, run the VLM on the whole image; if it replaces text on specific object, first crop that objects box, ask the VLM to extract the text, and compare it to the requested text. See details in Alg 10. Background change. Ask the VLM yes/no whether the requested background category is present. See details in Alg 11. Consistency. We measure object and background stability as follows: Object consistency (unchanged objects): DINOv3 ViT-B/16 Simeoni et al. (2025) feature similarity between crops of unchanged objects in base vs. target; we also report pixel L1 consistency and average across objects per image. Background consistency: in base/target (Grounddetect objects in all objects pool ingDINO), mask them to isolate background, then compute masked L1 between backgrounds (optionally DINOv3 masked similarity). Background consistency is evaluated only when no background change occurred earlier (bg consistency=true). Perceptual Quality. We report HPSv3 Ma et al. (2025a) plausibility and aesthetics, plus luminance metrics. Quality is not folded into the Overall score. A.4 OVERALL SCORE AND AGGREGATION DETAILS Let αt be the image success rate at turn t: the fraction of images for which all edits up to and including turn are successful (aggregated per task type, then averaged). Let κ denote the average content-consistency score combining object and background DINOv3 similarities when applicable. Overall score. We report Overall = (cid:2)meant(αt) mean(κ)(cid:3)1/2 . Missing outputs across turns. For summary tables, we include only images that produce all required outputs for the evaluated mode. If model fails to generate later turn, that image is omitted from later-turn aggregates for that mode. Some edits will be rejected by some models since the sensitive content flag. No unchanged objects. If the unchanged-object list is empty, object consistency is recorded as None and excluded from averages; background consistency is still computed when bg consistency=true."
        },
        {
            "title": "Preprint",
            "content": "Algorithm 5 Evaluate Subject Add Require: base B, target , new object name n, optional reference object name r, optional position {left, right, above, below} Tn DETECT(T, n, τ ) Tr DETECT(T, r, τ ) end if Choose max logits boxes Tn, Tr (xt, yt) CENTER(t); if = left xt < xu εx then (xu, yu) CENTER(u) return false Ensure: success flag 1: Bn DETECT(B, n, τ ); 2: if Tn = Bn = then 3: 4: end if 5: if and are provided then Br DETECT(B, r, τ ); 6: if Tr = then 7: return false 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: else 26: 27: end if end if return false return true return true return true return true return true end if if = right xt > xu + εx then end if if = above yt < yu εy then end if if = below yt > yu + εy then Turn-level reporting. We also report per-turn (T1, T2, T3) instruction-following and consistency, and per-task-type success rates αt,type. Quality metrics are reported separately and are not folded into Overall. A.5 MODEL GENERATIONS We evaluate mix of closedand open-source editors using each models default settings (no hyperparameter tuning): GPT-Image-1, Nano Banana, and Gemini 2.0 Flash: called via their APIs with default parameters. QWEN Image Edit: default settings from https://huggingface.co/Qwen/ Qwen-Image-Edit. InstructPix2Pix (IP2P): settings from https://github.com/timothybrooks/ instruct-pix2pix. Magicbrush: same settings as IP2P; model weights from https://huggingface. co/vinesmsuic/magicbrush-jul7. UltraEdit: settings from https://github.com/HaozheZhao/UltraEdit; we apply black mask since no explicit mask is provided. AnyEdit: repository at https://github.com/weichow23/AnySD/tree/ to 9e7d36ef88e237b527695efc90b1abc18fa51218 with edit type set general. Step1X-Edit: repository at https://github.com/stepfun-ai/Step1X-Edit; weights at https://huggingface.co/stepfun-ai/Step1X-Edit."
        },
        {
            "title": "Preprint",
            "content": "Algorithm 6 Evaluate Position Change return false return false Ta DETECT(T, a, τ ) Tr DETECT(T, r, τ ) Require: base B, target , target object name a, reference object r, position Ensure: success flag 1: Ba DETECT(B, a, τ ); 2: Br DETECT(B, r, τ ); 3: if Ta = Tr = then 4: 5: end if 6: if Ta > Ba then 7: 8: end if 9: Select max logits boxes Ta, Tr 10: (xt, yt) CENTER(t); 11: if = left then 12: 13: end if 14: if = right then 15: 16: end if 17: if = above then 18: 19: end if 20: if = below then 21: 22: end if 23: return false (xu, yu) CENTER(u) return xt < xu εx return xt > xu + εx return yt < yu εy return yt > yu + εy No count inflation Algorithm 7 Evaluate Count Change Require: target , name o, requested count Ensure: success flag 1: ˆc DETECT(T, o) 2: return (ˆc = c) OmniGen: repository at https://github.com/VectorSpaceLab/OmniGen. FLUX: default settings from https://huggingface.co/black-forest-labs/ FLUX.1-Kontext-dev. Modes. For clarity in the paper: we report both Multipass and Complex Editing (renamed from singlepass for consistency with the rest of the paper). Reproducibility Notes. Prompts are provided in full (Section A.2); thresholds are specified above. Grounding uses SwinT-OGC weights; consistency uses DINOv3 ViT-B/16; the quality head follows our RAHF implementation, and HPSv3 is included when available. All other parameters are left at defaults."
        },
        {
            "title": "B ADDITIONAL EVALUATION RESULTS",
            "content": "In this section, we provide extended evaluation results. We separate the analysis into two modes: multi-turn editing and complex editing. Each mode is evaluated across three aspects: instruction following, consistency, and quality. For the multi-turn editing mode, the overall instruction-following success rate is reported in Table 5, while success rates for individual instruction types appear in Tables 6 and 7. Consistency results are summarized in Table 11. We also observed that some input images are non-square after resizing, which can leave black padding on the top/bottom or left/right edges. Certain editing models, such"
        },
        {
            "title": "Preprint",
            "content": "Algorithm 8 Evaluate Color Alter Require: target image , object name o, color 1: return VLMYESNO(T, Is the k?) Algorithm 9 Evaluate Material Alter Require: target image , object name o, material Ensure: success flag 1: return VLMYESNO(T, Is the made of m?) as GPT-Image-1 and Qwen-Image-Edit, attempt to fill these areas, whereas others preserve them. To account for this, we separately report consistency for square  (Table 12)  and non-square inputs  (Table 13)  . The conclusions remain consistent with the overall evaluation. Quality results for multiturn editing are presented in Table 15. For the complex editing mode, the overall instruction-following success rate is shown in Table 8, and per-instruction-type results are in Tables 9 and 10. Consistency and quality results are reported in Tables 14 and 16, respectively. In consistency table, p99 means 99% quantile of luminance value, and p999 means 99.9% quantile of luminance value."
        },
        {
            "title": "C HUMAN AGREEMENT",
            "content": "The human study was conducted online through Gradio3. Annotators were asked to answer 2-way multiple-choice problem (Yes/No) about an editing instruction, an original image, and an edited image. There were very limited potential participant risks, if they were to be exposed to an image that was disturbing or not safe for work (NSFW). It is because the source images we used were from GEit-Bench Liu et al. (2025), which were not in themselves offensive. Also, our agent already filtered out unsafe images during the first decomposition stage. Furthermore, all edited images from the models were passed through its own NSFW filters which blacked out any potentially unsafe content. We conducted human study on edits made by four exemplary modelsStep1X-Edit, AnyEdit, Gemini-Flash 2.0, and Flux.1-Kontext-devon EdiVal-Bench, generated by EdiVal-Agent as described in Section A.2. For each edit, we collected two human ratings, yielding total of 572 4 2 = 4,576 annotations. Depending on the prompt (which affected the editing instruction), each annotation took about 12 minutes. Raters were recruited online, each holding at least bachelors degree. They were shown the original image, the edited image, and the corresponding instruction, and were asked binary question: Evaluate whether the edited image successfully follows the given instruction."
        },
        {
            "title": "D VLMS FAILING TO JUDGE VISUAL QUALITY",
            "content": "The following is the zero-shot prompt for visual quality with VLMs. The example results are shown in Fig. 12. You are an expert at evaluating image visual quality and naturalness. will show you an image. Please analyze whether the image is visually pleasing and natural. Consider: 1. Is the image visually pleasing? 2. Is the image natural? 3. Does the image look natural and coherent? 3https://www.gradio.app/"
        },
        {
            "title": "Preprint",
            "content": "Algorithm 10 Evaluate Text Change Require: target , desired text (optionally object name) Ensure: success flag 1: VLMTEXT(T ) 2: Normalize and (case, punctuation, whitespace) 3: return TEXT-MATCH(t, t) Algorithm 11 Evaluate Background Change Require: target , category Ensure: success flag 1: return VLMYESNO(T, Does the background show g?) Respond only with score between 0 and 100, where 100 is the highest score. 100 means the image is visually pleasing and natural. 0 means the image is not visually pleasing and natural. 50 means the image is neutral."
        },
        {
            "title": "Preprint",
            "content": "Table 5: Image success rates and overall task success rates across turns. (Multi-turn model) Model GPT-4o Nano Gemini QWEN StepEdit FLUX OmniGen AnyEdit UltraEdit MagicBrush IP2P Image Success Rate Overall Task Rate T2 T3 T1 T2 T3 73.12 70.70 68.07 72.90 61.89 59.97 54.72 41.07 51.37 42.31 37. 54.89 50.66 45.96 44.06 34.97 32.69 24.48 16.32 17.70 15.73 10.66 38.35 35.35 28.42 22.55 17.83 16.61 10.66 7.22 6.36 4.90 2.80 73.12 70.70 68.07 72.90 61.89 59.97 54.72 40.03 50.52 42.31 37.41 74.44 72.59 67.72 62.94 59.09 56.29 48.60 39.34 36.54 40.73 32.87 73.12 68.24 68.42 56.12 53.32 51.40 42.48 40.56 31.47 41.26 34.27 Table 6: Task success rates (%) across five instruction types and three turns (multi-turn mode). Model Subject Replace Subject Remove Material Alter Color Alter Subject Add T1 T2 T3 T1 T2 T1 T2 T3 T1 T2 T1 T2 T3 GPT-Image-1 Nano Banana Gemini 2.0 Flash QWEN-Image-Edit Step1X-Edit FLUX.1-Kontext-dev OmniGen AnyEdit UltraEdit MagicBrush IP2P 84.31 91.84 83.33 87.04 90.74 85.19 88.89 74.07 88.89 83.33 75.93 94.64 92.31 92.98 82.46 96.49 80.70 84.21 66.67 63.16 75.44 66. 85.71 75.47 78.18 70.91 67.27 72.73 58.18 61.82 38.18 63.64 65.45 70.77 64.18 58.67 70.67 53.33 54.67 46.67 37.33 26.67 28.00 25.33 55.93 51.61 53.62 31.88 30.43 42.03 21.74 39.13 5.80 18.84 8.70 47.37 40.35 50.82 37.70 21.31 32.79 19.67 36.07 6.56 18.03 18.03 96.83 91.94 90.91 93.94 95.45 84.85 84.85 78.79 87.88 83.33 74.24 95.65 89.36 82.00 90.00 80.00 74.00 72.00 68.00 66.00 86.00 70. 87.88 87.50 83.33 79.17 87.50 73.61 70.83 68.06 63.89 80.56 65.28 100.00 100.00 100.00 100.00 100.00 100.00 100.00 78.57 98.21 94.64 87.50 97.06 97.18 89.04 97.26 100.00 98.63 90.41 68.49 80.82 87.67 82.19 100.00 98.11 98.21 94.74 91.23 94.74 91.23 78.95 78.95 91.23 75.44 80.95 73.02 77.61 77.61 64.18 67.16 53.73 22.39 38.81 37.31 23.88 72.46 77.94 72.73 55.84 57.14 61.04 51.95 38.96 23.38 41.56 28. 72.41 72.41 75.27 39.78 45.16 39.78 37.63 25.81 9.68 37.63 19.35 Table 7: Task success rates (%) across four instrcution types and three turns (multi-turn mode). Model Text Change Position Change Count Change Background Change T1 T2 T3 T1 T3 T1 T2 T3 T1 T3 GPT-Image-1 Nano Banana Gemini 2.0 Flash Qwen-Image-Edit Step1X-Edit FLUX.1-Kontext-dev OmniGen AnyEdit UltraEdit MagicBrush IP2P 88.33 83.33 90.32 98.44 60.94 50.00 29.69 3.12 28.12 7.81 1.56 97.01 86.36 94.29 92.86 51.43 41.43 35.71 10.00 15.71 12.86 8.57 97.96 81.25 96.30 72.22 44.44 27.78 18.52 11.11 7.41 3.70 5.56 31.11 20.00 21.15 21.15 13.46 15.38 17.31 21.15 21.15 19.23 13. 40.68 47.37 28.57 34.92 31.75 26.98 22.22 25.40 36.51 15.87 15.87 50.00 45.24 31.91 33.33 27.08 33.33 20.83 27.08 35.42 20.83 25.00 11.27 23.19 10.96 12.33 0.00 0.00 5.48 0.00 5.48 0.00 1.37 18.52 9.62 7.14 1.72 1.72 1.72 5.17 1.72 6.90 0.00 0.00 18.18 10.91 10.00 0.00 1.67 5.00 0.00 1.67 5.00 3.33 5.00 98.39 96.67 86.15 98.46 87.69 90.77 76.92 56.92 75.38 43.08 47. 94.44 94.44 83.64 80.00 87.27 80.00 56.36 40.00 38.18 34.55 20.00 91.30 90.00 80.56 77.78 83.33 77.78 56.94 52.78 43.06 43.06 31.94 Table 8: Image rates, overall task rates, and marginal means across three turns (complex mode). Model Image Success Rate Overall Task Rate Marginal Task Rate T1 T2 T3 T1 T3 T1 T2 T3 GPT-Image-1 Nano Banana Gemini 2.0 Flash Qwen-Image-Edit Step1X-Edit FLUX.1-Kontext-dev OmniGen AnyEdit UltraEdit MagicBrush IP2P 73.08 71.46 67.43 72.90 61.36 60.14 54.55 39.86 50.70 42.13 37. 48.45 46.56 40.63 46.15 32.34 33.74 23.43 10.31 22.03 14.86 12.41 28.78 28.14 21.89 27.62 15.73 19.58 11.01 2.80 8.22 4.55 2.80 73.08 71.46 67.43 72.90 61.36 60.14 54.55 39.86 50.70 42.13 37.24 69.77 68.83 64.54 69.23 57.69 59.53 50.96 34.79 48.34 38.46 37.76 68.25 67.27 61.94 68.07 55.01 57.87 49.83 34.27 46.62 38.81 35.14 73.08 71.46 67.43 72.90 61.36 60.14 54.55 39.86 50.70 42.13 37. 69.98 69.03 64.80 67.83 55.77 57.52 48.95 31.47 47.73 38.64 38.46 70.19 67.21 62.17 67.66 54.02 56.99 47.90 31.64 47.38 39.34 34."
        },
        {
            "title": "Preprint",
            "content": "Table 9: Success rates (%) for five instruction types across three turns (complex mode). Model Subject Replace Subject Remove Material Alter Color Alter Subject Add T1 T2 T3 T1 T3 T1 T2 T3 T1 T3 T1 T2 T3 GPT-Image-1 Nano Banana Gemini 2.0 Flash Qwen-Image-Edit Step1X-Edit FLUX.1-Kontext-dev OmniGen AnyEdit UltraEdit MagicBrush IP2P 82.22 91.67 85.19 87.04 90.74 85.19 88.89 66.67 88.89 83.33 75. 80.65 88.89 82.88 86.49 84.68 82.88 82.88 58.56 87.39 74.77 73.87 78.99 79.33 75.90 80.72 76.51 74.10 75.90 49.40 78.31 69.28 61.45 70.31 67.16 57.33 70.67 52.00 54.67 46.67 33.33 26.67 28.00 25.33 63.87 55.83 55.56 58.33 41.67 47.92 39.58 20.14 30.56 27.78 31.25 58.54 59.88 54.15 54.63 42.93 40.00 41.95 22.44 31.71 29.27 24.88 96.49 92.59 93.94 93.94 93.94 86.36 86.36 77.27 87.88 83.33 71. 90.20 85.15 75.00 86.21 82.76 75.00 73.28 76.72 79.31 69.83 63.79 84.66 83.33 74.87 85.64 79.26 76.06 72.34 70.21 77.66 73.94 59.57 100.00 97.87 100.00 100.00 100.00 100.00 100.00 78.57 98.21 92.86 87.50 97.27 98.15 96.90 99.22 93.02 99.22 96.12 68.99 93.02 84.50 82.95 91.88 94.19 93.01 98.39 90.86 98.39 93.01 72.04 90.32 83.87 79.03 81.67 69.84 70.15 77.61 64.18 67.16 53.73 28.36 38.81 37.31 23. 70.73 71.21 65.97 75.69 59.03 68.75 48.61 22.92 46.53 35.42 31.94 69.00 67.77 66.67 73.84 54.01 63.29 49.37 21.10 43.46 32.91 29.11 Table 10: Success rates (%) for four instruction types across three turns (complex mode). Model Text Change Position Change Count Change Background Change T1 T2 T3 T2 T3 T1 T2 T3 T2 T3 GPT-Image-1 Nano Banana Gemini 2.0 Flash Qwen-Image-Edit Step1X-Edit FLUX.1-Kontext-dev OmniGen AnyEdit UltraEdit MagicBrush IP2P 87.50 84.21 85.94 98.44 57.81 50.00 28.12 3.12 29.69 7.81 3.12 92.24 85.34 86.57 97.01 59.70 48.51 32.84 5.22 16.42 4.48 5.22 93.75 84.24 84.04 93.62 52.13 47.87 27.13 7.98 11.70 6.38 6. 30.95 24.44 17.31 21.15 15.38 15.38 15.38 25.00 21.15 19.23 13.46 22.11 26.00 23.68 22.61 21.74 26.96 21.74 26.96 26.09 22.61 20.00 25.36 22.54 16.67 24.54 25.15 27.61 17.18 29.45 22.70 17.18 20.25 13.11 20.34 11.11 12.33 1.37 0.00 6.85 1.37 5.48 0.00 1.37 10.38 14.15 10.77 3.05 3.05 1.53 1.53 1.53 2.29 0.00 2.29 13.12 13.75 6.84 3.66 1.57 3.66 2.09 1.57 3.14 5.24 1. 98.15 98.15 90.77 98.46 86.15 90.77 75.38 56.92 75.38 43.08 47.69 96.08 93.40 84.17 95.83 80.00 90.00 70.00 44.17 65.00 36.67 37.50 93.37 93.71 80.73 93.75 73.44 88.54 69.79 40.62 64.06 35.42 38.54 Table 11: Consistency scores (%) across DINOv3-based and L1-based object/background metrics. (multi-turn mode) Model Object DINOv3 Consistency Background DINOv3 Consistency Object 1 L1 Consistency Background 1 L1 Consistency T1 T2 GPT-Image-1 Nano Banana Gemini 2.0 Flash Qwen-Image-Edit Step1X-Edit FLUX.1-Kontext-dev OmniGen AnyEdit UltraEdit MagicBrush IP2P 73.26 90.17 85.53 77.12 88.17 92.66 88.34 82.02 78.81 79.70 68.24 68.80 85.25 77.12 71.56 81.65 87.92 80.77 73.41 75.11 70.71 56.83 67.21 84.38 72.02 68.51 77.33 85.29 73.64 63.04 72.24 65.46 48.01 T1 T2 88.74 97.65 95.63 91.31 97.34 97.97 97.66 90.82 94.80 94.22 85.47 86.76 95.70 93.07 89.47 95.40 96.55 96.08 84.42 93.89 91.81 79.89 83.78 94.58 89.74 87.45 93.09 95.14 94.21 77.17 92.57 88.27 72.59 T1 T2 79.65 92.39 90.72 83.48 93.92 94.39 93.87 92.52 91.86 91.13 84.44 78.32 90.00 86.32 79.15 90.64 91.59 91.02 88.96 90.47 87.13 79.74 77.60 89.10 84.11 76.32 88.80 89.91 89.43 84.97 89.65 85.56 77.21 T1 T2 78.07 95.95 95.04 84.57 98.24 96.36 97.44 93.72 97.12 96.31 91.31 76.53 94.70 93.15 81.18 97.10 95.06 97.00 89.98 96.62 94.52 87.21 75.79 93.88 91.73 78.43 95.73 94.13 96.34 86.05 96.19 92.87 83.51 Table 12: Consistency and 1 L1 metrics across three turns (multi-turn mode for square image). Model GPT4o nano Gemini QWEN StepEdit FLUX OmniGen AnyEdit UltraEdit MagicBrush IP2P Object DINOv3 Consistency (Mean) Background DINOv3 Consistency Object L1 Consistency (Mean) Background L1 Consistency T2 73.62 88.68 84.16 75.36 87.40 92.55 89.41 82.25 79.43 79.02 76.38 71.27 85.19 79.91 72.68 84.03 88.92 84.51 72.40 76.51 75.60 66.12 T3 67.27 83.31 71.13 68.85 78.27 84.04 77.77 53.59 71.54 70.07 54.94 T2 90.00 96.88 91.50 88.50 97.43 96.49 97.34 86.12 92.83 92.07 85.29 85.72 93.79 91.83 88.98 92.42 93.57 93.13 78.60 89.88 87.33 81.46 T3 82.16 92.41 87.07 82.71 88.15 92.19 86.64 70.09 86.54 81.08 65.50 T1 T2 80.33 89.73 88.98 81.66 93.14 92.83 93.26 92.33 91.74 89.67 86.90 78.80 87.64 87.07 78.47 91.57 90.61 91.48 88.00 90.16 87.53 82.25 T3 79.88 88.48 85.13 77.04 89.40 88.34 89.53 82.10 89.24 86.90 77. T1 T2 85.69 94.21 95.19 91.16 98.10 96.91 95.79 94.36 95.87 95.29 92.45 81.86 93.77 93.35 87.85 97.12 95.74 96.44 91.68 95.14 93.88 88.01 T3 80.85 93.36 93.34 85.78 96.01 94.73 95.36 87.55 94.63 92.38 84."
        },
        {
            "title": "Preprint",
            "content": "Table 13: Consistency and L1 metrics across three turns (multi-turn model for unsquared image). Model GPT4o nano Gemini QWEN StepEdit FLUX OmniGen AnyEdit UltraEdit MagicBrush IP2P Object DINOv3 Consistency (Mean) Background DINOv3 Consistency Object 1 L1 Consistency (Mean) Background 1 L1 Consistency T1 73.22 90.34 85.69 77.32 88.26 92.67 88.22 82.00 78.74 79.78 67.32 68.53 85.26 76.79 71.43 81.37 87.80 80.33 73.53 74.95 70.14 55.75 T3 67.20 84.50 72.12 68.47 77.22 85.44 73.14 64.18 72.32 64.90 47.18 T1 88.60 97.74 96.14 91.65 97.33 98.15 97.70 91.39 95.04 94.48 85.50 86.88 95.94 93.23 89.53 95.77 96.92 96.45 85.14 94.39 92.37 79.69 T3 83.99 94.88 90.10 88.10 93.77 95.54 95.25 78.14 93.40 89.27 73.57 T1 79.57 92.69 90.91 83.68 94.01 94.57 93.94 92.54 91.87 91.30 84.17 78.27 90.29 86.23 79.23 90.53 91.70 90.97 89.07 90.51 87.08 79.45 T3 77.35 89.17 83.99 76.23 88.73 90.10 89.42 85.31 89.70 85.40 77.14 T1 77.20 96.16 95.02 83.78 98.26 96.30 97.64 93.64 97.26 96.44 91.18 75.91 94.81 93.12 80.37 97.09 94.98 97.07 89.78 96.80 94.59 87.12 T3 75.14 93.95 91.51 77.43 95.70 94.05 96.47 85.84 96.40 92.94 83.38 Table 14: Consistency scores (%) across object/background DINOv3 and L1 metrics (complex mode). Model Object DINOv3 T1 T2 T3 Object 1 L1 T2 T3 GPT-Image-1 Nano Banana Gemini 2.0 Flash Qwen-Image-Edit Step1X-Edit FLUX.1-Kontext-dev OmniGen AnyEdit UltraEdit MagicBrush IP2P 73.23 89.23 85.41 77.12 88.14 92.66 88.37 81.90 78.81 79.70 68.24 70.02 87.20 80.37 76.09 85.31 90.30 85.15 82.94 72.75 75.64 67.31 67.57 86.46 77.38 76.69 84.38 89.19 83.14 84.92 71.67 75.53 69.49 79.52 92.15 90.60 83.48 93.93 94.39 93.88 92.34 91.86 91.13 84. 78.03 91.08 88.45 83.08 92.34 92.79 92.46 92.43 89.51 89.23 82.93 77.29 90.40 86.53 83.11 92.11 91.61 91.06 93.72 88.99 88.69 83.72 Background DINOv3 T1 T2 Background 1 L1 T3 T2 T1 88.72 97.39 96.03 91.31 97.34 97.97 97.62 90.97 94.80 94.22 85.47 86.77 96.69 94.18 91.32 96.37 96.74 97.10 92.63 93.01 94.34 85.88 84.79 95.38 92.83 90.51 95.44 95.40 96.07 93.78 92.03 93.13 86.45 77.96 96.39 95.03 84.57 98.24 96.36 97.45 94.15 97.12 96.31 91.31 77.38 95.75 94.77 84.93 98.02 95.57 97.58 95.11 96.35 96.14 89. 76.51 95.33 93.46 85.35 98.04 94.04 97.40 95.87 96.02 95.83 90.19 Table 15: Human preference scores, p999, and p99 across three turns (multi-turn mode). Model Human Preference Score T1 T3 GPT-Image-1 Nano Banana Gemini 2.0 Flash Qwen-Image-Edit Step1X-Edit FLUX.1-Kontext-dev OmniGen AnyEdit UltraEdit MagicBrush IP2P 6.6519 4.9431 4.4386 5.8591 4.0577 5.1192 4.6099 3.6609 4.7934 3.8465 3.2020 6.5898 5.1179 4.2332 5.7198 3.3443 5.0701 4.0743 2.8017 4.6806 3.0805 2.3779 6.5609 5.2638 4.0677 5.1502 2.7569 5.0354 3.4958 1.9457 4.3598 2.3606 1.4418 T2 82.96 90.27 92.21 97.20 95.39 91.94 95.24 86.70 94.19 93.33 91.59 T1 84.73 89.67 90.95 89.16 92.81 90.76 93.15 86.70 92.71 91.49 89.61 T3 82.50 90.56 92.95 99.04 97.27 93.21 96.71 86.71 95.82 94.20 92.44 74.38 81.31 83.08 79.60 85.10 82.05 85.55 77.54 85.42 83.42 81.79 p99 T2 71.91 82.01 84.79 90.51 88.46 83.03 88.24 76.53 86.76 84.70 83.78 70.54 82.09 86.24 95.28 91.21 84.58 90.50 75.82 88.34 85.32 84.77 Table 16: Updated human preference scores, p999 scores, and p99 scores across three turns (complex mode). Model Human Preference Score T1 T3 GPT-Image-1 Nano Banana Gemini 2.0 Flash Qwen-Image-Edit Step1X-Edit FLUX.1-Kontext-dev OmniGen AnyEdit UltraEdit MagicBrush IP2P 6.6328 4.9444 4.4511 5.8591 4.0534 5.1192 4.5976 3.7020 4.7934 3.8465 3.2020 6.8428 5.1700 4.5428 5.8769 3.9063 5.2446 4.3070 3.7601 4.7647 3.6029 3.3552 6.9655 5.3632 4.5732 5.9155 3.8648 5.4645 3.8122 3.8382 4.8117 3.5523 3.5640 T2 84.14 90.67 92.66 90.92 93.55 91.01 93.74 87.16 93.06 91.52 90.46 T1 85.33 89.65 91.27 89.16 92.82 90.76 93.15 86.47 92.71 91.49 89.61 T3 84.44 91.79 93.48 92.23 94.05 91.29 95.65 87.43 93.24 91.64 90.86 74.73 81.02 83.85 79.60 85.11 82.05 85.56 77.82 85.42 83.42 81.79 p99 T2 73.92 81.93 85.69 81.36 85.49 81.53 86.28 78.60 86.09 83.31 82.57 73.07 82.75 86.75 82.62 86.01 81.55 88.57 79.12 86.47 83.07 82."
        },
        {
            "title": "Preprint",
            "content": "(a) Visual quality (Qwen2.5-VL: 60, HPSv3: 1.61). (b) Visual quality (Qwen2.5-VL: 60, HPSv3: 3.35). Figure 12: Examples of visual quality evaluation. Scores are reported from both VLM Qwen2.5VL and HPSv3 for comparison. The VLM score ranges from 0 to 100, so the VLM gives positive score to both images with significant artifacts. 29 Input T2 T"
        },
        {
            "title": "Nano Banana",
            "content": "GPT-Image-1 Gemini 2.0 Flash Qwen-Image-Edit Figure 13: T1: Change the color of pumpkin to purple; T2: Change the background to forest; T3: Remove fabric orange bow. Row-wise quality examples for the first four models: Nano Banana, GPT-Image-1, Gemini 2.0 Flash, and Qwen-Image-Edit. Each row shows generations for Input and three editing turns."
        },
        {
            "title": "E MORE QUALITY EXAMPLES",
            "content": "30 Input T1 T2 T"
        },
        {
            "title": "Preprint",
            "content": "Step1X-Edit FLUX.1-kontext-dev OmniGen AnyEdit UltraEdit MagicBrush IP2P Figure 14: T1: Change the color of pumpkin to purple; T2: Change the background to forest; T3: Remove fabric orange bow. Row-wise quality examples for the remaining models: Step1XEdit, FLUX.1-kontext-dev, OmniGen, AnyEdit, UltraEdit, MagicBrush, and IP2P."
        }
    ],
    "affiliations": [
        "Lambda, Inc",
        "Microsoft",
        "University of California, Los Angeles",
        "University of Texas at Austin"
    ]
}