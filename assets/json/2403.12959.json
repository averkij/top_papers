{
    "paper_title": "WHAC: World-grounded Humans and Cameras",
    "authors": [
        "Wanqi Yin",
        "Zhongang Cai",
        "Ruisi Wang",
        "Fanzhou Wang",
        "Chen Wei",
        "Haiyi Mei",
        "Weiye Xiao",
        "Zhitao Yang",
        "Qingping Sun",
        "Atsushi Yamashita",
        "Ziwei Liu",
        "Lei Yang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Estimating human and camera trajectories with accurate scale in the world coordinate system from a monocular video is a highly desirable yet challenging and ill-posed problem. In this study, we aim to recover expressive parametric human models (i.e., SMPL-X) and corresponding camera poses jointly, by leveraging the synergy between three critical players: the world, the human, and the camera. Our approach is founded on two key observations. Firstly, camera-frame SMPL-X estimation methods readily recover absolute human depth. Secondly, human motions inherently provide absolute spatial cues. By integrating these insights, we introduce a novel framework, referred to as WHAC, to facilitate world-grounded expressive human pose and shape estimation (EHPS) alongside camera pose estimation, without relying on traditional optimization techniques. Additionally, we present a new synthetic dataset, WHAC-A-Mole, which includes accurately annotated humans and cameras, and features diverse interactive human motions as well as realistic camera trajectories. Extensive experiments on both standard and newly established benchmarks highlight the superiority and efficacy of our framework. We will make the code and dataset publicly available."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 9 1 ] . [ 1 9 5 9 2 1 . 3 0 4 2 : r WHAC: World-grounded Humans and Cameras Wanqi Yin,1,2, Zhongang Cai,1,3,4, Ruisi Wang1, Fanzhou Wang1, Chen Wei1, Haiyi Mei1, Weiye Xiao1, Zhitao Yang1, Qingping Sun1, Atsushi Yamashita2, Ziwei Liu3, Lei Yang1,4 1 SenseTime Research, 2 The University of Tokyo, 3 S-Lab, Nanyang Technological University, 4 Shanghai AI Laboratory"
        },
        {
            "title": "Abstract",
            "content": "Estimating human and camera trajectories with accurate scale in the world coordinate system from monocular video is highly desirable yet challenging and ill-posed problem. In this study, we aim to recover expressive parametric human models (i.e., SMPL-X) and corresponding camera poses jointly, by leveraging the synergy between three critical players: the world, the human, and the camera. Our approach is founded on two key observations. Firstly, camera-frame SMPL-X estimation methods readily recover absolute human depth. Secondly, human motions inherently provide absolute spatial cues. By integrating these insights, we introduce novel framework, referred to as WHAC, to facilitate world-grounded expressive human pose and shape estimation (EHPS) alongside camera pose estimation, without relying on traditional optimization techniques. Additionally, we present new synthetic dataset, WHAC-A-Mole, which includes accurately annotated humans and cameras, and features diverse interactive human motions as well as realistic camera trajectories. Extensive experiments on both standard and newly established benchmarks highlight the superiority and efficacy of our framework. We will make the code and dataset publicly available."
        },
        {
            "title": "Introduction",
            "content": "Expressive human pose and shape estimation (EHPS) has garnered considerable research attention due to its wide applications across the entertainment, fashion, and healthcare industries. Despite remarkable advancements in recent years, the majority of EHPS methods primarily focus on estimating parametric human models (i.e., SMPL-X [30]) in the camera coordinate system. This approach falls short in dynamic situations where the camera and subject move concurrently. Estimating 3D trajectories in the world coordinate system (world-grounded) from 2D camera footage is challenging as the 3D-to-2D projection results in loss of critical spatial information. Consequently, camera trajectories deduced are thus inherently scaleless\", and the depth of humans directly estimated from the camera perspective lacks validity. In this work, we demonstrate the synergy between humans, cameras, and the world. First, existing camera-frame EHPS methods, although not specifically supervised to estimate human depth directly, can still accurately deduce the true depth. This only requires reasonably accurate focal length that can be obtained from the video capture devices or estimated [20]. Second, root translation is critical component of human motions, allowing the latter to serve as strong prior after an association is learned. Hence, by analyzing human poses, one can make an informed estimation of the velocity of human movement. Building upon these insights, we present WHAC, novel framework designed to jointly estimate expressive human models and camera movements using monocular video. For any given input video, camera-frame SMPL-X parameters and preliminary camera trajectory are Equal contributions. Preprint. Under review. Figure 1: WHAC synergizes human-camera (camera-frame SMPL-X estimation), camera-world (visual odometry), and human-world (our proposed MotionVelocimeter) modeling for constructing world-grounded human and camera trajectories. first estimated using plug-and-play EHPS [6] and visual odometry [39] models. The human-camera relative positions are first deduced. These estimations are then utilized with VO estimations to canonicalize the sequences of human poses for accurate velocity estimation. Consequently, the scale of the camera trajectory can be recovered. It is noteworthy that WHAC pioneers whole-body, optimization-free estimation in world-grounded context to recover human and camera trajectories jointly. Moreover, the development of new dataset becomes essential to more accurately assess model performance on world-grounded human motions and camera trajectories across broader spectrum of scenarios. Recent studies have underscored the surprising efficacy of synthetic data [6, 3, 42, 7], thanks to its diversity and controllability. Inspired by these findings, we introduce WHAC-A-Mole, comprehensive synthetic dataset for World-grounded Humans And Cameras with rich collection of Animated subjects under MOving viewpoints in muLtiple Environments. WHAC-A-Mole features comprehensive motion sequences that include 1) interactive human activities from DLP-MoCap [4], 2) partner dances from DD100 [36], in addition to 3) the standard AMASS [27] motion repository. Notably, WHAC-A-Mole includes automatically generated camera trajectories that mimic cinematic filming techniques, such as tracking shots and arc shots, thereby offering high level of realism. We validate our WHAC on standard benchmarks and WHAC-A-Mole to obtain consistent performance gains compared to the state-of-the-art (SoTA) methods under both camera-frame and worldgrounded settings. WHAC even demonstrates surprising capability to handle corner cases when motion-based and camera-based observations contradict, paving the way for potential applications. In summary, our contributions are three-fold. First, we propose WHAC, novel regression-based framework that capitalizes on human priors for the pioneering world-grounded EHPS method. Second, we contribute WHAC-A-Mole, comprehensive benchmark with accurate human and camera annotations of diverse human activities. Third, our empirical evaluations underscore the superior performance of WHAC across multiple benchmarks."
        },
        {
            "title": "2 Related Works",
            "content": "2.1 Expressive Human Pose and Shape Estimation (EHPS) EHPS captures body, face, and hands from monocular images or videos, typically through parametric human models (e.g., SMPL-X [30]). Early optimization-based method [30] fits SMPL-X models on 2D keypoints, and was soon outperformed by regression-based methods that were trained on large amount of paired data. Two-stage methods estimate body parameters first, then hand/face parameters from crop-out image patches [10, 33, 49, 12, 28, 22, 46, 29]. Recently, OSX [24] proposes the one-stage paradigm that estimates body, hand, and face with shared features. This paradigm shift simplifies the pipeline and led to the first foundation model SMPLer-X [6] that achieved unprecedented generalization ability across key benchmarks. However, despite their success, these 2 Figure 2: Overview of WHAC. SMPL-X estimator extracts camera-frame SMPL-X with dummy depth, which is recovered in Sec. 3.2. The scaleless camera trajectory estimated by VO is then used to canonicalize the human trajectory to estimate its velocity and thus scale in Sec. 3.3. camera trajectory is then derived for alignment and scale recovery, which subsequently updates the human trajectory in Sec. 3.4. methods estimate parametric humans in the camera coordinate, lacking information on the global trajectory especially when the camera is moving. 2.2 World-grounded Recovery of Humans and Cameras Estimation of human trajectory in world coordinate system typically requires multi-camera setup [14, 18, 48, 31, 15, 5, 8] or additional wearable devices (e.g., IMU [41, 13] or electromagnetic sensors [19]). Methods that require only single camera often rely on other assumptions: Yu et al. [44] needs the scene to be provided by the user and Luvizon et al. [26] assumes static camera. D&D [23], GLAMR [45], and TRACE [37] estimate global human trajectories from single-frame poses or image features. However, camera and human rotation have coupled effect on camera-frame global orientation estimation, which leads to ambiguity. Liu et al. [25] leverages Structure-from-Motion (SfM) [34] to reconstruct both camera and human trajectories and adjust humans scale to match the cameras, which may not reflect the absolute scale. Recently, SLAHMR leverages SLAM [38] and human motion prior [32] in the optimization to recover humans and the camera. However, the process is computationally expensive and takes excessively long to complete. PACE [21] also leverages visual odometry [39] for camera pose estimation and faster human motion to significantly accelerate the optimization process but is still time-consuming. WHAM [35] is the first regression-based work in the domain that features real-time performance. It takes camera estimation (angular velocity) as the input and estimates human parameters in the camera frame and human trajectory in the world frame through separate branches, while the camera trajectory is not recovered. Our WHAC aims to recover both human and camera trajectories in the world coordinate with accurate scales."
        },
        {
            "title": "3 Methodology",
            "content": "Recovering accurate 3D dimensions from 2D observations is an ill-posed problem: small object at close range may appear the same as large object at far range. In this section, we aim to address two ambiguities with priors that are surprisingly effective, but not thoroughly utilized in existing EHPS works: the parametric humans themselves. 3.1 Preliminaries Problem Formulation. We aim to estimate human and camera pose sequences in the world coordinate system. The humans are represented by SMPL-X parameters: global orientation θw go R13, body pose θb R213, left hand pose θlh R153, right hand pose R13, translation tw θrh R153, jaw pose θj R13, body shape β R10 and facial expression ϕ R10. The cameras R13. In this work, the are represented by world-frame rotation Rw superscript indicates the coordinate system (i.e., for world and for camera). R13 and translation tw 3 Figure 3: a) Human trajectories derived from camera trajectories of different scales can be vastly different in both shape and direction, despite that the same camera-frame human root depth dt and translations tc are used. b) Different pairs of focal length and tz can correspond to the same image. Camera-frame SMPL-X Estimation typically omits absolute depth estimation. Hence, primary metrics (e.g., PA-MPJPE, MPJPE, and PVE) all perform root alignment. We employ SMPLer-X [6], strong foundation model that demonstrates accurate estimation of human pose and shapes. We add additional GRUs before the prediction heads and finetune the model to better capture the temporal cues. Visual Odometry (VO) is scaleless but accurate in shape. We also follow the standard to define the first camera frame of the input video as the world coordinate system. typically provides high-quality Rw ; the trajectory formed by tw 3.2 Recovering Camera-space Human Root Translation Mainstream EHPS methods [28, 24, 6] recover parametric humans in the camera space and adopt weak perspective camera model, which considers all points to be at the same depth away from the camera. (cid:34)f 0 0 0 0 (cid:35) (cid:35) (cid:34)tx + δx 0 ty + δy 0 tz + δz 1 (cid:34)f 0 0 0 (cid:35) (cid:35) (cid:34)tx + δx 0 ty + δy 0 tz 1 (1) where indicates the focal length in NDC (Normalized Device Coordinate) space (image pixel coordinates are normalized into [1, 1]). tc = (tx, ty, tz) is the root translation in the camera frame, (δx, δy, δz) is the relative translation of point relative to the root. Hence, the projected 2D point (NDC space) is written as: (cid:21) (cid:20)u = (cid:20)f (tx + δx)/tz (ty + δy)/tz (cid:21) (cid:20)s(tx + δx) s(ty + δy) (cid:21) = (2) where is the scale parameter. The SMPL-X estimator also predicts camera parameters (s, tx, ty) to reproject SMPL-X joints on the image plane. Hence, we obtain the relationships between focal length, depth, and s: tz where is the focal length in pixels. is the resolution (side pixel length) of the input crop to the SMPL-X estimator. tz = = 2 2 (3) = tz We point out that although these camera-frame methods do not supervise the human root depth tz, by training the model to produce scale that overlays SMPL-X accurately back on the image plane, the model implicitly learns human root depth tz that is coupled with focal length as illustrated in Fig. 3b). dummy focal length of 5,000 is often used [28, 24, 6], however, this leads to unrealistic human root depth tz. We highlight that accurate intrinsic parameters are accessible from many devices, and our empirical results show using the diagonal pixel length [20] also yields satisfactory results. 3.3 Estimating World-frame Human Motions for Scale Extraction In recent years, there has been plethora of high-quality optical motion capture datasets that become available, covering wide range of human activities. Previous art [35] estimates human global 4 trajectory from 2D keypoint observations, which may not capture subtle 3D information. Hence, we propose to learn absolute scale from 3D human motions. First, for SMPL-X sequence of frames, estimated in the camera coordinate system, we compute the 3D joint coordinates. Specifically, for each frame: = (θc go, tc, θb, θlh, θrh, θj, β, ϕ), RK153 (4) where is the SMPL-X parametric model. We select 15 joints (14 LSP [17] joints and the pelvis) from the original 55 joints. We then compute the joints in the world coordinate system: = vo c, vo = [Rw tw ] (5) where vo is the visual odometrys estimation (camera-to-world transformation). Note that tw not have valid scale. does To facilitate the training, we standardize the input: we define canonical frame where the human is root-aligned with zero global orientation. We then compute the canonical transformation cano by using the first (0th) frames rotation and offset the translation to zero: go,0)1 = (θc cano = [Rcanotcano], Rcano = (Rw go,0)1, tcano = pw c,0 θc (6) c,0 is 0th camera rotation in the world frame estimated from visual odometry, θc go,0 is 0th where Rw global orientation estimated in the camera space, pw c,0 is an identity matrix I3 as the 0th camera frame is defined as the world frame. All joints are then canonicalized as cano = cano w. 0 is the pelvis joint of 0 . Note Rw Our MotionVelocimeter then estimates per-frame velocity in the canonical space: where the velocity is then de-canonicalized back to the world frame: cano = MotionVelocimeter(J cano) world = (T cano)1 cano (7) (8) with world, we can reconstruct the human trajectory with scale in the world coordinate system. MotionVelocimeter only requires simple architecture that we include in the Supplementary Material. 3.4 Recovering Scaled Human and Camera Trajectories As we obtain human trajectory tw with absolute scale, one possible way is to align the human trajectory derived from the VO-estimated camera trajectory using camera-frame human root translation to tw . However, Fig. 3a) shows that such alignment is problematic as the human trajectory derived from scaleless camera trajectory may be invalid. Hence, we propose to transfer the scale to the camera trajectory in two steps. First, we derive camera trajectory from the human trajectory: c,derived = (T cano)1 cano (T h)1 (9) c,derived = [Rw (10) This derived camera trajectory already has an accurate scale with good shape. However, we find that the camera trajectory estimated by VO has better, more robust shape because it can leverage visual cues that are much denser than human motion cues. In this light, we perform Umeyamas method [40] (shown as U) to align the VO-estimated camera trajectory with the human-derived camera trajectory tw c,derived and keeping the camera rotation Rw : c,derived while discard Rw c,derived] c,derivedtw Hence, we then update human trajectory by deriving it from the aligned camera trajectory tw c,f inal: c,f inal = tw tw tw c,derived (11) h,f inal = w c,f inal h, c,f inal = [Rw tw h,f inal = [Rw c,f inal], h,f inal and camera trajectory tw h,f inal] h,f inaltw c,f inal, both in the world (12) As result, we obtain human trajectory tw coordinate system and with absolute scales. 5 Table 1: Dataset Comparison. #Inst.: number of human instances (crops). #Seq.: number of video sequences. R/S: Real or Synthetic. Multi.: multiperson scenes. Track.: track ID labels. HHI: human-human interaction motions. : EgoSet. : unknown as the data is not released when this paper is written. : typically short (<100 frames) clips."
        },
        {
            "title": "Dataset",
            "content": "#Inst. #Seq. R/S Multi. Track. Contact HHI Camera Human 3DPW [41] RICH [16] HCM [21] EMDB [19] EgoBody [47] BEDLAM [3] SynBody [42] 60 141 25 81 125 74.6K 476K 109K 175K 951K 10.4K 2.7M 27K WHAC-A-Mole 1.46M"
        },
        {
            "title": "R\nR\nS\nR\nR\nS\nS",
            "content": "S N.A. Moving SMPL Static* SMPL Moving SMPL N.A. Moving SMPL Moving SMPL-X SMPL-X SMPL-X Moving SMPL-X"
        },
        {
            "title": "4 WHAC-A-Mole Dataset",
            "content": "We highlight that WHAC-A-Mole combines fine-crafted automatic camera movements with varied characters animated with diverse, high-quality motion sequences to generate dataset with accurate camera and SMPL-X annotations. The dataset is constructed with the advanced human data synthesis toolbox XRFeitoria [11]. It leverages SMPL-XL (a layered extension of SMPL-X) to create virtual humans with diverse body shapes, clothing, and accessories. We follow SynBody [42] in the scene setup, subject creation, and placement. We further improve the data synthesized in two ways: diverse motion sources (Sec. 4.1) and camera trajectory generation (Sec. 4.2). In Tab. 1, we compare WHAC-A-Mole with popular video-based benchmarks with both camera and human annotations. WHAC-A-Mole features competitive scale of training instances and video sequences, multiperson scenes with track IDs, contact labels, accurate camera pose and SMPL-X annotations. We split WHAC-A-Mole by motion sequence into 80%:20% for training and testing. Examples of WHAC-AMole are visualized in Fig. 4. 4.1 Interactive Human Motions AMASS [27] is popular motion repository, widely used by existing synthetic datasets [21, 3, 42]. However, AMASS only contains single-person motions. As result, synthetic data is captured in virtual scenes populated with unrelated single-person motions, typically scattered sparsely to avoid collision. However, close human interactions are common in daily life, and difficult to solve. In this light, we select two latest motion datasets that contain comprehensive interactive human motions. First, DD100 [36], duet dance motion capture dataset that includes near two hours of partner dances of 10 different genres. Second, DLP-MoCap [4], motion capture dataset containing daily interactions between two subjects. Since SMPL-XL models are fully compatible with SMPL-X body pose sequences, we animate virtual characters with combination of AMASS, DD100, and DLP-MoCap. 4.2 Camera Trajectory Generation To better model the camera movement, we adopt the representation in Rao et al. [1] to define the camera in human-centric spherical coordinate system (rc, θc, ϕc), in which the rc represents the distance from the camera to the character, while the polar angle θc and the azimuthal angle ϕc define the angle between the cameras looking direction and the characters facing direction. Therefore, given characters location (xch, ych, zch) and facing direction (θch, ϕch), the cameras location in the world space is (xc, yc, zc) = (xch, ych, zch) + rc(sin(θ) cos(ϕ), sin(θ) sin(ϕ), cos(θ)) (13) where the θ = (θc + θch) mod 2π, the ϕ = (ϕc + ϕch) mod 2π, and the cameras rotation is thereby calculated by restricting the camera look at the (xch, ych, zch). In WHAC-A-Mole, we design two types of shot scales including the medium shot and the full shot, which respectively use the location of the neck and the pelvis as the characters location (xch, ych, zch). For the motion sequences that consist of multiple characters, the (xch, ych, zch) and the (θch, ϕch) are derived from 6 Figure 4: Visualization of WHAC-A-Mole sample sequences, animated with a) AMASS, b-c) DLPMoCap, and d-e) DD100. In each sample, the first row depicts the overview (note the camera trajectory shown in bright rays), and the second and the third rows show the camera view and overlaid SMPL-X annotations. the average of the locations and the facing directions of all the characters. Based on the human-centric spherical coordinate system (rc, θc, ϕc), we design different keyframe-setting strategies to simulate five common camera movements below. - Arc shot adds equally-spaced keyframes to rotate the camera around the character horizontally by increasing or decreasing ϕc [ϕmin, ϕmax] or vertically by increasing or decreasing θc [θmin, θmax]. Moreover, the angular velocity of the arc shots can be controlled by adjusting the ϕc or the θc between two adjacent keyframes. - Push shot also adds equally-spaced keyframes and moves the camera towards the character by decreasing the rc. More specifically, the fraction of the character in the view, which is more intuitive when filming, is used to indirectly decrease the rc by (cid:40) rc = hbbox ractan(f ov/2) hbboxas ractan(f ov/2) as 1.0 as > 1. (14) where the hbbox is the height of the characters bounding box in the camera space, the rac is the desired fraction of the character in the view, the as is the aspect ratio of the camera frame, and the ov is the field of view of the camera. Similar to the arc shots, the speed of the camera movement can be adjusted by setting the rac between two adjacent keyframes. - Pull shot is opposite to the push shot and moves the camera further away from the character by increasing the rc under the control of the rac. Randomly sampling rac in range [f racmin, racmax] at different keyframes derives continuous pushing and pulling, which is commonly used when filming dances. - Tracking shot follows the character and maintains the relative position between the camera and the character, i.e., maintain the (rc, θc, ϕc) of the camera in the human-centric spherical coordinate system. new keyframe of the tracking shot is added when the overlap ratio of the characters bounding box in the current frame and in the last keyframe is greater than threshold λoverlap. - Pan shot rotates the camera horizontally to keep the camera looking at the character, therefore it is another way to make the camera follow the character, and it shares the same rule with the tracking shot to add new keyframe. Rather than assigning specific camera movement to an entire human motion sequence, our pipeline automatically combines several types of camera movements into one motion sequence to increase the variety of camera movements. For example, when capturing static motions (whose longest edge of the bounding box formed by the (xch, ych, zch) across all frames is less than threshold λbbox) or interactive motions, we combine the horizontal and the vertical arc shots with the random pull or push shots to rotate the camera around the characters as well as transiting smoothly between different shot angles, such as high-angle, low-angle or eye-level, and pushing in or pulling out the distance between the camera and the characters to increase the rhythm of the camera movement. For the motions with 7 long-distance movements, we combine the tracking shots and the pan shots to follow the character. If the characters facing direction is stable (i.e., that the rotation angle from the characters facing direction in the last keyframe to the current keyframe is less than threshold λangle), we use the tracking shot. Otherwise, we use the pan shot. This rule effectively smooths the cameras movement, especially when the character turns dramatically."
        },
        {
            "title": "5 Experiments",
            "content": "We evaluate WHAC on both camera-frame and world-grounded benchmarks to compare its parametric human recovery abilities with existing SoTA methods. Due to space constraints, we include inference speed comparison, more visualizations on trajectory reconstruction, and more qualitative results in the Supplementary Material. 5.1 Implementation Details We finetune SMPLer-X-B [6] with EgoBody, 3DPW, and EMDB for camera-frame estimation of SMPL-X parameters. WHAC-A-Mole (with motions from AMASS, DD100, and DLP-MoCap), 3DPW, EMDB, and RICH are used to train the MotionVelocimeter. More details are in the Supplementary Material. 5.2 Datasets In addition to our proposed WHAC-A-Mole, mainstream benchmarks for human pose and shape estimation with parametric human labels are used. EgoBody [47] includes 125 sequences of 36 subjects in 15 indoor scenes, featuring 3D human motions interacting with scenes. We study the EgoSet that is captured by head-mounted camera; 2) 3DPW [41], popular dataset with 60 sequences captured by an iPhone, featuring diverse human activities in outdoor scenes; 3) EMDB [19] provides 58 minutes of motion data of 10 subjects in 81 indoor and outdoor scenes. Notably, it contains subset, EMDB 2, that contains global trajectories of humans and cameras. 4) RICH [16] consists of 142 multi-view videos with 22 subjects and 5 scenes with 6-8 fixed cameras. RICH is not used for evaluation as the cameras are static. 5.3 Evaluation Metrics For camera-frame human recovery, we use the standard Mean Per Joint Position Error (MPJPE), Procrustes-aligned MPJPE (PA-MPJPE), Per Vertex Error (PVE) in millimeters (mm), and Acceleration error (Accl.) in m/s2. Note that these metrics are evaluated after root alignment between estimated and ground truth parametric humans, thus not considering discrepancy in translation estimation. In this light, we also report T-MPJPE [2] and similarly T-PVE, which are variants of MPJPE and PVE that includes translation estimation to reflect the accuracy of depth estimation in the camera space. For world-frame human/camera recovery, we follow previous works [43, 21, 35] to split human motion sequences with global trajectory into 100-frame segments. The segments are Procrustesaligned to the ground truth for MPJPE computation: W-MPJPE100 if the first two frames are used in the alignment or WA-MPJPE100 if the entire segment. To evaluate the quality of trajectory, we extend Average Trajectory Error (ATE) [21] to C-ATE and H-ATE for camera and human respectively, which are computed after Procrustes-alignment of estimated and ground truth trajectories. All metrics are in millimeters (mm). We also report respective Alignment Scales (AS) used in the alignment for the camera (C-AS) and human (H-AS) and values closer to 1.0 indicate more accurate scale estimation. 5.4 World-grounded Benchmarks In Tab. 3, we evaluate on WHAC-A-Mole in Tab. 2. WHAC-A-Mole provides expressive human (i.e., SMPL-X), with accurately annotated camera motions. Since no existing EHPS methods produce SMPL-X in the world coordinate system and strictly fair comparison is not plausible, we build the first benchmark by making two adaptations to the SoTA methods (OSX [24] and SMPLer-X [6]): Table 2: World-frame evaluation on WHAC-A-Mole. *: adapted to world-grounded evaluation. H-AS and C-AS: the closer to 1.0, the better. PA-MPJPE W-MPJPE WA-MPJPE H-ATE H-AS C-ATE C-AS OSX* [24] + DPVO [39] SMPLer-X-B* [6] + DPVO [39] WHAC (GT Gyro) WHAC 90.1 76.7 76.5 76.5 1036.1 842.3 343.8 343.3 390.7 335.4 182.0 182. 180.5 138.3 103.5 103.5 0.5 0.5 0.9 0.9 0.5 0.5 0.5 0.5 7.3 7.3 1.3 1.3 Table 3: World-frame evaluation on EMDB2. *: adapted to world-grounded evaluation. H-AS and C-AS: the closer to 1.0, the better. PA-MPJPE W-MPJPE WA-MPJPE H-ATE H-AS C-ATE C-AS GLAMR [45] SLAHMR [43] WHAM [35] (GT Gyro) OSX-L* [24] + DPVO [39] SMPLer-X-B* [6] + DPVO [39] WHAC (GT Gyro) WHAC 56.0 61.5 41.9 99.9 42.5 39.4 39.4 756.1 807.4 436.4 1186.2 930.1 392.5 389. 286.2 336.9 165.9 458.8 375.8 143.1 142.2 - 207.8 83.2 235.4 200.6 75.8 76.7 - 1.9 1.5 2.3 2.0 1.1 1. - - - 14.8 14.8 14.8 14.8 - - - 5.1 5.1 1.5 1.4 camera-frame translation estimation and visual odometry. Implementation details are included in the Supplementary Material. It is noted that methods that achieve good results on EMDB still struggle on WHAC-A-Mole, which can be attributed to the more challenging scenarios of WHAC-A-Mole (involving hard poses, diverse interactions, occlusions, and complicated camera movements). We hope WHAC-A-Mole can serve as useful foundation for future world-grounded EHPS research. Moreover, we compare WHAC with both body-only methods (GLAMR [45], SLAHMR [43], and WHAM [35]) and whole-body methods (OSX-L [24] and SMPLer-X [6]) on EMDB2, where WHAC achieves best performance, even surpassing body-only methods that are native to EMDBs SMPL annotations. 5.5 Camera-space Benchmarks In Tab. 4, it is shown that WHAC outperforms existing SoTAs. We highlight that 1) WHAC archives immense T-PVE-all improvement, which captures absolute depth estimation from humans to cameras. This is because WHAC formulates the subject distance to the camera. 2) With temporal information embedded in the EHPS module, WHAC attains substantial reductions in acceleration error (Accl.) compared to previous single-frame SoTAs. Moreover, temporal cues also lead to significant performance gains in hand and face estimation. In Tab. 5, we further evaluate WHAC on EMDB and 3DPW, where the plausibility of camera-frame human translation estimation and the significance of temporal modeling are validated again. Albeit WHAC-A-Mole is mainly designed for world-grounded evaluation of human and camera pose sequences, we evaluate WHACs performance under the camera-frame setting on WHAC-A-Mole in Tab. 6. Similar to previous experiments, it is observed that WHAC is on par with SMPLer-X with better performance on the acceleration error. Table 4: Results of camera-frame methods on EgoBody (EgoSet) with SMPL-X ground truths. PVE variants are measured for whole-body (SMPL-X) methods only. PA-MPJPE PA-PVE-all PVE-all PVE-hand PVE-face Accl. GLAMR [45] SLAHMR [43] Hand4Whole [28] OSX-L [24] SMPLer-X-B [6] WHAC 114.3 79.1 71.0 66.5 47.1 46.9 - - 127.6 115.7 72.7 64.7 - - 48.0 50.5 43.7 41.0 - - 173.5 25.8 41.2 41.0 32.4 26.3 27.2 24.7 18.9 11.6 - - 59.8 54.6 40.7 39.0 9 Table 5: More camera-frame evaluations on EMDB1 and 3DPW. Compared to existing mainstream EHPS methods, WHAC recovers meaningful human depths (T-PVE) and achieves lower acceleration errors (Accl.)."
        },
        {
            "title": "Method",
            "content": "EMDB1 [19] 3DPW [41] PA-PVE PVE T-PVE Accl. PA-PVE PVE T-PVE Accl. Hand4Whole [28] OSX-L [24] SMPLer-X-B [6] WHAC 99.5 143.1 36851.8 93.3 134.0 45526.0 99.3 41298.0 68.2 140.2 91.2 61.0 34.2 30.3 24.4 18. 81.7 124.7 30279.0 76.9 117.8 38472.2 62.6 95.6 32532.0 260.8 91.9 62.8 31.0 24.9 24.8 20.3 Table 6: Results of camera-frame methods on WHAC-A-Mole. WHAC is on par with SMPLer-X but produces lower acceleration error. PA-MPJPE PA-PVE-all PVE-all PVE-hand PVE-face Accl. OSX-L [24] SMPLer-X-B [6] WHAC 90.1 76.7 76. 88.1 74.8 74.8 155.7 116.2 117.8 83.3 70.6 77.7 85.0 63.1 63.2 38.9 44.0 31.2 5.6 Ablation Study We evaluate the necessity of the key components in Tab. 7. It is observed that using visual odometry alone (body trajectory depends on estimated camera trajectory) leads to accurate camera trajectory shape (lowest camera trajectory error) but lacks accurate scale (alignment scale is far from 1.0). Using MotionVelocimeter alone (camera trajectory depends on estimated body trajectory), however, results in very accurate scale recovery and better body trajectory error. WHAC leverages the scale recovery ability of MotionVelocimeter and visual odometer, achieving high-quality body and camera trajectories with only slight decline in scale accuracy. 5.7 Visualization We highlight that WHAC is the first regression-based, whole-body method that simultaneously predicts camera and human trajectories. We highlight that the camera provides supplementary cues to human motions. In Fig. 5 a) and b), we test two corner cases where the human motion itself can be misleading: when human pose appears stationary but there is root movement in the world coordinates (e.g., skateboarding), and when human pose clearly indicates motion but there is no root movement in the world coordinates (e.g., running on treadmill). Our formulation considers both motion and camera cues to predict the correct trajectories whereas WHAM fails, which leverages foot contact and locking but no camera information in humans global trajectory estimation. We also show complicated scenarios on c) in-the-wild video from TikTok, which features fast-moving object. Our MotionVelocimeter can estimate reasonable root movement, whereas WHAMs contact estimation and foot-locking results in floating subject. More visualizations are included in the Supplementary Material. Figure 5: Visualization on in-the-wild hard cases. WHAC leverages human-camera-scene collaboration to resolve cases where motion prior alone would fail: a) Skateboarding and b) Treadmill. c) WHAC can also handle fast cases. 10 Table 7: Ablation on key components. DPVO represents visual odometry, MV represents MotionVelocimeter. Method WA-MPJPE H-ATE C-ATE C-AS Table 8: Ablation on intrinsic sources. reasonable intrinsic drastically improve human root translation estiamtion. T-MPJPE W-MPJPE WA-MPJPE DPVO MV MV + DPVO 376.0 233.2 142.2 177.8 129.9 76. 14.8 134.1 14.8 5.10 1.10 1.40 Dummy(5,000) Assumed [20] GT 36020.4 179.7 100.3 6239.9 391.2 389.4 604.6 144.0 142."
        },
        {
            "title": "6 Conclusion",
            "content": "In conclusion, we present WHAC, the pioneering regression-based EHPS method that jointly recovers human motions and camera trajectories in the world coordinate system. Moreover, our WHAC-A-Mole serves as useful benchmark for the evaluation of world-grounded EHPS methods. WHAC achieves SoTA performance on both standard benchmarks and our proposed WHAC-A-Mole, demonstrating strong potentials for downstream applications. Limitations. WHAC-A-Mole includes rich collection of multiperson scenarios that may require special algorithm designs to tackle close interaction and occlusions, which WHAC lacks. We leave this to the future work. Potential negative societal impact. WHAC may be used for unwarranted surveillance as it recovers human trajectories in the world frame."
        },
        {
            "title": "References",
            "content": "[1] Rao Anyi, Jiang Xuekun, Guo Yuwei, Xu Linning, Yang Lei, Jin Libiao, Lin Dahua, and Dai Bo. Dynamic storyboard generation in an engine-based virtual environment for video production. arXiv preprint arXiv:2301.12688, 2023. [2] Eduard Gabriel Bazavan, Andrei Zanfir, Mihai Zanfir, William Freeman, Rahul Sukthankar, and Cristian Sminchisescu. Hspace: Synthetic parametric humans animated in complex environments. arXiv preprint arXiv:2112.12867, 2021. [3] Michael Black, Priyanka Patel, Joachim Tesch, and Jinlong Yang. Bedlam: synthetic dataset of bodies exhibiting detailed lifelike animated motion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 87268737, 2023. [4] Zhongang Cai, Jianping Jiang, Zhongfei Qing, Xinying Guo, Mingyuan Zhang, Zhengyu Lin, Haiyi Mei, Chen Wei, Ruisi Wang, Wanqi Yin, et al. Digital life project: Autonomous 3d characters with social intelligence. arXiv preprint arXiv:2312.04547, 2023. [5] Zhongang Cai, Daxuan Ren, Ailing Zeng, Zhengyu Lin, Tao Yu, Wenjia Wang, Xiangyu Fan, Yang Gao, Yifan Yu, Liang Pan, et al. Humman: Multi-modal 4d human dataset for versatile sensing and modeling. In European Conference on Computer Vision, pages 557577. Springer, 2022. [6] Zhongang Cai, Wanqi Yin, Ailing Zeng, Chen Wei, Qingping Sun, Wang Yanjun, Hui En Pang, Haiyi Mei, Mingyuan Zhang, Lei Zhang, Chen Change Loy, Lei Yang, and Ziwei Liu. Smpler-x: Scaling up expressive human pose and shape estimation. In A. Oh, T. Neumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems, volume 36, pages 1145411468. Curran Associates, Inc., 2023. [7] Zhongang Cai, Mingyuan Zhang, Jiawei Ren, Chen Wei, Daxuan Ren, Zhengyu Lin, Haiyu Zhao, Lei Yang, Chen Change Loy, and Ziwei Liu. Playing for 3d human recovery. arXiv preprint arXiv:2110.07588, 2021. [8] Wei Cheng, Ruixiang Chen, Siming Fan, Wanqi Yin, Keyu Chen, Zhongang Cai, Jingbo Wang, Yang Gao, Zhengming Yu, Zhengyu Lin, et al. Dna-rendering: diverse neural actor repository In Proceedings of the IEEE/CVF International for high-fidelity human-centric rendering. Conference on Computer Vision, pages 1998219993, 2023. [9] Kyunghyun Cho, Bart Van Merriënboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoderdecoder for statistical machine translation. arXiv preprint arXiv:1406.1078, 2014. [10] Vasileios Choutas, Georgios Pavlakos, Timo Bolkart, Dimitrios Tzionas, and Michael Black. Monocular expressive body regression through body-driven attention. In Computer Vision ECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part 16, pages 2040. Springer, 2020. [11] XRFeitoria Contributors. Openxrlab synthetic data rendering toolbox. https://github.com/ openxrlab/xrfeitoria, 2023. [12] Yao Feng, Vasileios Choutas, Timo Bolkart, Dimitrios Tzionas, and Michael Black. Collaborative regression of expressive bodies using moderation. In 2021 International Conference on 3D Vision (3DV), pages 792804. IEEE, 2021. [13] Vladimir Guzov, Aymen Mir, Torsten Sattler, and Gerard Pons-Moll. Human poseitioning system (hps): 3d human pose estimation and self-localization in large scenes from bodymounted sensors. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 43184329, 2021. [14] Nils Hasler, Bodo Rosenhahn, Thorsten Thormahlen, Michael Wand, Jürgen Gall, and HansPeter Seidel. Markerless motion capture with unsynchronized moving cameras. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, pages 224231. IEEE, 2009. [15] Buzhen Huang, Yuan Shu, Tianshu Zhang, and Yangang Wang. Dynamic multi-person mesh recovery from uncalibrated multi-view cameras. In 2021 International Conference on 3D Vision (3DV), pages 710720. IEEE, 2021. [16] Chun-Hao Huang, Hongwei Yi, Markus Höschle, Matvey Safroshkin, Tsvetelina Alexiadis, Senya Polikovsky, Daniel Scharstein, and Michael Black. Capturing and inferring dense 12 full-body human-scene contact. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1327413285, 2022. [17] Sam Johnson and Mark Everingham. Clustered pose and nonlinear appearance models for human pose estimation. In BMVC, pages 111. British Machine Vision Association, 2010. [18] Hanbyul Joo, Hao Liu, Lei Tan, Lin Gui, Bart Nabbe, Iain Matthews, Takeo Kanade, Shohei Nobuhara, and Yaser Sheikh. Panoptic studio: massively multiview system for social motion capture. In Proceedings of the IEEE International Conference on Computer Vision, pages 33343342, 2015. [19] Manuel Kaufmann, Jie Song, Chen Guo, Kaiyue Shen, Tianjian Jiang, Chengcheng Tang, Juan José Zárate, and Otmar Hilliges. Emdb: The electromagnetic database of global 3d human pose and shape in the wild. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1463214643, 2023. [20] Imry Kissos, Lior Fritz, Matan Goldman, Omer Meir, Eduard Oks, and Mark Kliger. Beyond In European Conference on weak perspective for monocular 3d human pose estimation. Computer Vision, pages 541554. Springer, 2020. [21] Muhammed Kocabas, Ye Yuan, Pavlo Molchanov, Yunrong Guo, Michael Black, Otmar Hilliges, Jan Kautz, and Umar Iqbal. Pace: Human and camera motion estimation from in-the-wild videos. arXiv preprint arXiv:2310.13768, 2023. [22] Jiefeng Li, Siyuan Bian, Chao Xu, Zhicun Chen, Lixin Yang, and Cewu Lu. Hybrik-x: Hybrid analytical-neural inverse kinematics for whole-body mesh recovery. arXiv preprint arXiv:2304.05690, 2023. [23] Jiefeng Li, Siyuan Bian, Chao Xu, Gang Liu, Gang Yu, and Cewu Lu. &d: Learning human dynamics from dynamic camera. In European Conference on Computer Vision, pages 479496. Springer, 2022. [24] Jing Lin, Ailing Zeng, Haoqian Wang, Lei Zhang, and Yu Li. One-stage 3d whole-body mesh recovery with component aware transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2115921168, 2023. [25] Miao Liu, Dexin Yang, Yan Zhang, Zhaopeng Cui, James Rehg, and Siyu Tang. 4d human body capture from egocentric video via 3d scene grounding. In 2021 international conference on 3D vision (3DV), pages 930939. IEEE, 2021. [26] Diogo Luvizon, Marc Habermann, Vladislav Golyanik, Adam Kortylewski, and Christian Theobalt. Scene-aware 3d multi-human motion capture from single camera. In Computer Graphics Forum, pages 371383. Wiley Online Library, 2023. [27] Naureen Mahmood, Nima Ghorbani, Nikolaus Troje, Gerard Pons-Moll, and Michael Black. Amass: Archive of motion capture as surface shapes. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 54425451, 2019. [28] Gyeongsik Moon, Hongsuk Choi, and Kyoung Mu Lee. Accurate 3d hand pose estimation for whole-body 3d human mesh estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 23082317, 2022. [29] Hui En Pang, Zhongang Cai, Lei Yang, Qingyi Tao, Zhonghua Wu, Tianwei Zhang, and Ziwei Liu. Towards robust and expressive whole-body human pose and shape estimation. In A. Oh, T. Neumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems, volume 36, pages 1733017344. Curran Associates, Inc., 2023. [30] Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani, Timo Bolkart, Ahmed AA Osman, Dimitrios Tzionas, and Michael Black. Expressive body capture: 3d hands, face, and body from single image. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1097510985, 2019. [31] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR, 2021. [32] Davis Rempe, Tolga Birdal, Aaron Hertzmann, Jimei Yang, Srinath Sridhar, and Leonidas Guibas. Humor: 3d human motion model for robust pose estimation. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1148811499, 2021. 13 [33] Yu Rong, Takaaki Shiratori, and Hanbyul Joo. Frankmocap: monocular 3d whole-body pose estimation system via regression and integration. In Proceedings of the International Conference on Computer Vision, pages 17491759, 2021. [34] Johannes Schonberger and Jan-Michael Frahm. Structure-from-motion revisited. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 41044113, 2016. [35] Soyong Shin, Juyong Kim, Eni Halilaj, and Michael Black. Wham: Reconstructing worldgrounded humans with accurate 3d motion. arXiv preprint arXiv:2312.07531, 2023. [36] Li Siyao, Tianpei Gu, Zhitao Yang, Zhengyu Lin, Ziwei Liu, Henghui Ding, Lei Yang, and Chen Change Loy. Duolando: Follower gpt with off-policy reinforcement learning for dance accompaniment. In The Twelfth International Conference on Learning Representations, 2023. [37] Yu Sun, Qian Bao, Wu Liu, Tao Mei, and Michael Black. Trace: 5d temporal regression of avatars with dynamic cameras in 3d environments. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 88568866, 2023. [38] Zachary Teed and Jia Deng. Droid-slam: Deep visual slam for monocular, stereo, and rgb-d cameras. Advances in neural information processing systems, 34:1655816569, 2021. [39] Zachary Teed, Lahav Lipson, and Jia Deng. Deep patch visual odometry. Advances in Neural Information Processing Systems, 36, 2024. [40] Shinji Umeyama. Least-squares estimation of transformation parameters between two point patterns. IEEE Transactions on Pattern Analysis & Machine Intelligence, 13(04):376380, 1991. [41] Timo von Marcard, Roberto Henschel, Michael Black, Bodo Rosenhahn, and Gerard PonsMoll. Recovering accurate 3d human pose in the wild using imus and moving camera. In Proceedings of the European Conference on Computer Vision (ECCV), pages 601617, 2018. [42] Zhitao Yang, Zhongang Cai, Haiyi Mei, Shuai Liu, Zhaoxi Chen, Weiye Xiao, Yukun Wei, Zhongfei Qing, Chen Wei, Bo Dai, Wayne Wu, Chen Qian, Dahua Lin, Ziwei Liu, and Lei Yang. Synbody: Synthetic dataset with layered human models for 3d human perception and modeling. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 2028220292, October 2023. [43] Vickie Ye, Georgios Pavlakos, Jitendra Malik, and Angjoo Kanazawa. Decoupling human and camera motion from videos in the wild. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2122221232, 2023. [44] Ri Yu, Hwangpil Park, and Jehee Lee. Human dynamics from monocular video with dynamic camera movements. ACM Transactions on Graphics (TOG), 40(6):114, 2021. [45] Ye Yuan, Umar Iqbal, Pavlo Molchanov, Kris Kitani, and Jan Kautz. Glamr: Global occlusionaware human mesh recovery with dynamic cameras. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1103811049, 2022. [46] Hongwen Zhang, Yating Tian, Yuxiang Zhang, Mengcheng Li, Liang An, Zhenan Sun, and Yebin Liu. Pymaf-x: Towards well-aligned full-body model regression from monocular images. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023. [47] Siwei Zhang, Qianli Ma, Yan Zhang, Zhiyin Qian, Taein Kwon, Marc Pollefeys, Federica Bogo, and Siyu Tang. Egobody: Human body shape and motion of interacting people from head-mounted devices. In Computer VisionECCV 2022: 17th European Conference, Tel Aviv, Israel, October 2327, 2022, Proceedings, Part VI, pages 180200. Springer, 2022. [48] Yuxiang Zhang, Liang An, Tao Yu, Xiu Li, Kun Li, and Yebin Liu. 4d association graph for realtime multi-person motion capture using multiple video cameras. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 13241333, 2020. [49] Yuxiao Zhou, Marc Habermann, Ikhsanul Habibie, Ayush Tewari, Christian Theobalt, and Feng Xu. Monocular real-time full body capture with inter-part correlations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 48114822, 2021."
        },
        {
            "title": "A Overview",
            "content": "Given the space constraints in the main paper, we provide additional information and details in this supplementary material: more results visualization on the EMDB dataset and WHAC-A-Mole dataset in Sec. B; further elaboration on the MotionVelocimeter in Sec. C; the average inference speed in Sec. D; detailed training procedures in Sec. E, explanations of the adaptations applied to camera space EHPS methods during evaluations in Sec. F, and the step-by-step comprehensive formulations for trajectory transformations in Sec. G."
        },
        {
            "title": "B Results Visualization",
            "content": "In Fig. 6, we visualize the camera and human trajectories of two sequences from EMDB2 dataset. The sequences consist of 2.7k frames for and 1.1k frames respectively. Notably, WHAC demonstrates its capability to accurately recover trajectory and scale in the world space, even for lengthy sequences. Moreover, in Fig. 6b1) and Fig. 6b2), WHAC effectively captures the downward trajectory as the depicted human descends stairs. Besides human and camera trajectory estimation in the world space, we present the visualization of the camera space results in Fig. 7. This visualization includes various scenarios such as severe occlusions, Figure 6: Visualization of world space results on the EMDB dataset. a1) and b1) depict camera trajectories, while a2) and b2) illustrate human trajectories. Notably, in sequence b, the human is descending stairs, and WHAC effectively captures the global trajectory, indicating downward direction besides recovering the absolute trajectory scale in the world space. The grid size in the plots is 2m. 15 Figure 7: Visualization of camera space results on WHAC-A-Mole dataset. Each sample comprises two rows: the first row displays the original input frames from the sequence, while the second row overlays the SMPL-X results. This visualization showcases WHACs performance on challenging scenes, including sequences with severe occlusions, intricate human interactions, and dynamic dancing poses. close interactions, body contact between subjects, and challenging dancing poses, which serve as representative cases for the WHAC-A-Mole dataset. Even without specific training or finetuning on WHAC-A-Mole dataset, WHAC demonstrates strong abilities in handling pose estimation and depth recovery in camera space for various scenarios. However, challenges persist in the recovery of multi-human interactions and contact between body parts in the world space."
        },
        {
            "title": "C MotionVelocimeter",
            "content": "We present the architecture of MotionVelocimeter in Fig. 8. The inputs to MotionVelocimeter consist of canonicalized 3D joints, which are derived from SMPL-X meshes and positioned within the canonical space of the sequences initial frame. The models outputs are root velocities corresponding to the previous frame within the canonical space. In contrast to the motion encoder and trajectory decoder that takes 2D keypoints as input in WHAM[35], 3D joints retain spatial information that is critical to velocity estimation in the world-frame for absolute scale recovery. Utilizing 3D joints enhances the models ability to capture and interpret complex movement patterns, offering more comprehensive representation of spatial and temporal dynamics within the world-grounded environment. 16 Figure 8: Illustration of MotionVelocimeter module. The inputs are canonicalized 3D joints regressed from SMPL-X meshes, and the outputs are root velocities in the canonical space. Table 9: Inference speed of core modules in frame per second (FPS). *Denotes the inference speed without the replaceable, off-the-shelf modules. Method GLAMR[45] SLAHMR[43] PACE[21] WHAM[35] WHAC WHAC* FPS 2. 0.04 2.1 200"
        },
        {
            "title": "D Inference speed",
            "content": "As indicated in Table 9, we assess the inference speed of both the core modules (excluding real-time human detection and visual odometry) following the protocol applied in WHAM[35], as well as the inference speed without off-the-shelf modules, which only includes MotionVelocimeter and the scale recovery of human and camera trajectories. As regression-based method, the inference speed with WHAC has notably faster inference speeds compared to optimization-based methods. Its efficiency enables it to meet the real-time speed requirement."
        },
        {
            "title": "E Training Details",
            "content": "To enhance temporal consistency and smoothness in camera frame results on temporal datasets, we finetune SMPLer-X-B[6]. This involves incorporating GRUs[9] between the ViT-B backbone and the regression heads of the SMPLer-X-B model. We finetune the model on 4 V100 GPUs using EgoBody[47], 3DPW[41], and EMDB[19] datasets. We set the minimum learning rate to 1 106 for 10 epochs, with sequence length of 32 frames per batch. For training MotionVelocimeter, we freeze the finetuned SMPLer-X-B model and initialize the learning rate to 1 103. We employ step decay learning rate scheduler, reducing the learning rate by γ = 0.1 every 2 epochs. The MotionVelocimeter is trained on 4 V100 GPUs over 8 epochs."
        },
        {
            "title": "F Adaptations for camera frame EHPS methods",
            "content": "In Table 2 and Table 3 of the main paper, we employ world-grounded adaptations to camera frame methods such as OSX[24] and SMPLer-X[6] using the approach described in Sec. 3.2. This method enables the recovery of camera-space root depth, ensuring fair comparison with other worldgrounded methods. Without the adaptations, camera frame methods face limitations when compared to world-grounded methods, due to the high T-MPJPE observed in the camera frame."
        },
        {
            "title": "G Comprehensive Formulations",
            "content": "Given the space constraints, we present only the finalized and general formulations in Sec. 3.3 and Sec. 3.4 of the main paper. Here, we include the comprehensive step-by-step formulations of the trajectory transformations for clarity. 17 G.1 Canonicalization We briefly explain the transformation for canonicalization in Eq. 6 in the main paper. Here we provide the comprehensive formulation: cano = cano w,i = [Rcanotcano] = [Rcano w,i tcano w,i ], (15) where cano is generalized symbol for the transformation from world coordinate system to canonical coordinate system, denotes the ith frame in the sequence. For the first (0th) frame in the sequence:"
        },
        {
            "title": "Rcano",
            "content": "w,0 = (Rw c,0 θc go,0)1 = (θc go,0)1, tcano w,0 = pw 0 , (16) where Rw global orientation estimated in the camera space, pw c,0 is 0th camera rotation in the world frame estimated from visual odometry, θc 0 is the pelvis joint of 0 for the first frame. go,0 is 0th For every frame in the sequence:"
        },
        {
            "title": "Rcano",
            "content": "w,i = (Rw c,0 θc go,0)1 = (θc go,0)1, tcano w,i = pw . (17) We use the pelvis translation pw for the corresponding frame while using the camera rotation and global orientation of the first frame for the entire sequence in canonicalization. This is to retain the human rotation in the world coordinate system between frames for reliable trajectory estimation. G.2 Derive Camera Trajectories from Human Trajectories In Sec. 3.4 in the main paper, we briefly explain the process of deriving camera trajectories from human trajectories in Eq. 9. We append the full formulation for this process: c,derived c,derived = (T cano)1 cano h (T h)1 = cano cano = [Rw h tw ], = h , (18) (19) go,itc ] = [θc = [Rh th h,i]1, th = (θc (20) is the output human trajectories in the world coordinate system from MotionVelocimeter, is the ith where is the inverted human root transformation in the camera coordinate system, θc global orientation and pelvis in camera space respectively. To further process the camera rotations Rw we re-write the transformations mentioned above in the form of = [Rt]: c,derived and camera moving trajectories tw c,derived separately, go,i and pc go,i)1 pc , c,derived = [Rw c,derivedtw c,derived] = [Rw tw ] [Rh th ] = [Rw Rh Rw th + tw ], (21) (22) c,derived is the human-derived camera trajectory in the world coordinate system with absolute where tw scale. The scale recovery is explained in Eq. 11 in the main paper. c,derived = Rw th c,derived = Rw Rh + tw , , tw Rw G.3 Derive Human Trajectories from Camera Trajectories In Sec. 3.4 and Eq. 12, we explain the process of deriving the human trajectories scale-recovered camera trajectories h,f inal from c,f inal: c,f inal = [Rw tw c,f inal], (23) (24) h,f inal = [Rw h,f inaltw h,f inal] = c,f inal h, 18 where estimated with visual odometry is accurate and can be used in the final camera trajectory The scale recovery process via Umeyama alignment[40] for tw main paper. c,f inal is the scale-recovered VO-estimated camera trajectories. We empirically find that Rw c,f inal. c,f inal is explained in Eq. 11 in the is the human root transformation in the camera coordinate system. By first deriving camera trajectories from MotionVelocimeter-estimated human trajectories, and followed by deriving human trajectories from scaled VO-estimated camera trajectories, we obtain human trajectory c,f inal, both in the world coordinate system and with absolute scales. h,f inal and camera trajectory w"
        }
    ],
    "affiliations": [
        "S-Lab, Nanyang Technological University",
        "SenseTime Research",
        "Shanghai AI Laboratory",
        "The University of Tokyo"
    ]
}