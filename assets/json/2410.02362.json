{
    "paper_title": "A Comprehensive Survey of Mamba Architectures for Medical Image Analysis: Classification, Segmentation, Restoration and Beyond",
    "authors": [
        "Shubhi Bansal",
        "Sreeharish A",
        "Madhava Prasath J",
        "Manikandan S",
        "Sreekanth Madisetty",
        "Mohammad Zia Ur Rehman",
        "Chandravardhan Singh Raghaw",
        "Gaurav Duggal",
        "Nagendra Kumar"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Mamba, a special case of the State Space Model, is gaining popularity as an alternative to template-based deep learning approaches in medical image analysis. While transformers are powerful architectures, they have drawbacks, including quadratic computational complexity and an inability to address long-range dependencies efficiently. This limitation affects the analysis of large and complex datasets in medical imaging, where there are many spatial and temporal relationships. In contrast, Mamba offers benefits that make it well-suited for medical image analysis. It has linear time complexity, which is a significant improvement over transformers. Mamba processes longer sequences without attention mechanisms, enabling faster inference and requiring less memory. Mamba also demonstrates strong performance in merging multimodal data, improving diagnosis accuracy and patient outcomes. The organization of this paper allows readers to appreciate the capabilities of Mamba in medical imaging step by step. We begin by defining core concepts of SSMs and models, including S4, S5, and S6, followed by an exploration of Mamba architectures such as pure Mamba, U-Net variants, and hybrid models with convolutional neural networks, transformers, and Graph Neural Networks. We also cover Mamba optimizations, techniques and adaptations, scanning, datasets, applications, experimental results, and conclude with its challenges and future directions in medical imaging. This review aims to demonstrate the transformative potential of Mamba in overcoming existing barriers within medical imaging while paving the way for innovative advancements in the field. A comprehensive list of Mamba architectures applied in the medical field, reviewed in this work, is available at Github."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 3 ] . [ 1 2 6 3 2 0 . 0 1 4 2 : r Comprehensive Survey of Mamba Architectures for Medical Image Analysis: Classification, Segmentation, Restoration and Beyond SHUBHI BANSAL, Indian Institute of Technology Indore, India SREEHARISH A, R.M.D. Engineering College, Kavaraipettai, India MADHAVA PRASATH J, R.M.D. Engineering College, Kavaraipettai, India MANIKANDAN S, R.M.D. Engineering College, Kavaraipettai, India SREEKANTH MADISETTY, Jio Platforms Limited, India MOHAMMAD ZIA UR REHMAN, Indian Institute of Technology Indore, India CHANDRAVARDHAN SINGH RAGHAW, Indian Institute of Technology Indore, India GAURAV DUGGAL, Birla Institute of Technology & Science Pilani, India NAGENDRA KUMAR, Indian Institute of Technology Indore, India Mamba, special case of the State Space Model, is gaining popularity as an alternative to template-based deep learning approaches in medical image analysis. While transformers are powerful architectures, they have drawbacks, including quadratic computational complexity and an inability to efficiently address long-range dependencies. This limitation affects the analysis of large and complex datasets in medical imaging, where there are many spatial and temporal relationships. In contrast, Mamba offers benefits that make it well-suited for medical image analysis. It has linear time complexity, which is significant improvement over transformers. In sequence modeling tasks, computational complexity grows linearly with the length of the input sequence. Mamba processes longer sequences without attention mechanisms, enabling faster inference and requiring less memory. Mamba also demonstrates strong performance in merging multimodal data, improving diagnosis accuracy and patient outcomes. The papers organization allows readers to appreciate Mambas capabilities in medical imaging step by step. We begin with clear definitions of relevant concepts regarding SSMs and concept models, including S4, S5, and S6. We then explore Mamba architectures, including pure Mamba, U-Net variants, and hybrid models that combine Mamba with convolutional networks, transformers, and Graph Neural Networks. Subsequent sections cover Mamba optimizations, techniques such as weakly supervised and self-supervised learning, scanning mechanisms, and detailed analysis of applications across various tasks. We provide an overview of available datasets and several experimental results regarding Mambas efficacy in different domains. Furthermore, we detail the challenges and limitations of Mamba, along with other interesting aspects and possible future directions. The final subsection explains the importance of Mamba in medical imaging and provides an analysis and conclusions regarding its usage and enhancement measures. This review aims to demonstrate the transformative potential of Mamba in overcoming existing barriers within medical imaging while paving the way for innovative advancements in the field. comprehensive list of Mamba architectures applied in medical field, reviewed in this work is available on Github. Corresponding author Authors contributed equally to this research. https://github.com/Madhavaprasath23/Awesome-Mamba-Papers-On-Medical-Domain Authors Contact Information: Shubhi Bansal, phd2001201007@iiti.ac.in, Indian Institute of Technology Indore, Indore, Madhya Pradesh, India; Sreeharish A, 21204048@rmd.ac.in, R.M.D. Engineering College, Kavaraipettai, Chennai, Tamil Nadu, India; Madhava Prasath J, 21204028@rmd.ac.in, R.M.D. Engineering College, Kavaraipettai, Chennai, Tamil Nadu, India; Manikandan S, 21204029@rmd.ac.in, R.M.D. Engineering College, Kavaraipettai, Chennai, Tamil Nadu, India; Sreekanth Madisetty, sreekanth2.m@ril.com, Jio Platforms Limited, India; Mohammad Zia Ur Rehman, phd2101201005@iiti.ac.in, Indian Institute of Technology Indore, Indore, Madhya Pradesh, India; Chandravardhan Singh Raghaw, phd2201101016@iiti.ac.in, Indian Institute of Technology Indore, Indore, Madhya Pradesh, India; Gaurav Duggal, p20230302@pilani.bits-pilani.ac.in, Birla Institute of Technology & Science Pilani, Pilani, India; Nagendra Kumar, nagendra@iiti.ac.in, Indian Institute of Technology Indore, Indore, Madhya Pradesh, India."
        },
        {
            "title": "Contents",
            "content": "S. Bansal et al. Abstract Contents 1 2 2.1 2.2 2.3 2.4 3 3.1 3.1.1 3.1.2 3.1.3 3.2 3.3 3.3.1 3.4 3.4.1 3.4.2 3.4.3 3.4.4 3.5 3.5.1 3.5.2 3.5.3 3.5.4 3.5.5 4 5 5.1 5.2 5.3 6 6.1 6.2 7 7.1 7.2 7.3 References Introduction Core Concepts of SSM State Space Models Structured State Space Sequence Models (S4) Simplified State Space Layers for Sequence Modeling (S5) Selective Structured State Space Models (S6)"
        },
        {
            "title": "Mamba Architectures",
            "content": "Pure Mamba Variants of U-Net Hybrid Architectures"
        },
        {
            "title": "Techniques and Adaptations",
            "content": "Weakly Supervised Learning Semi-Supervised Learning Self Supervised Learning Multimodal Learning Applications in Various Medical Domains Medical Image Segmentation Medical Image Classification Medical Image Restoration/ Reconstruction Medical Image Registration Miscellaneous"
        },
        {
            "title": "Significance for the Field\nSummary of Key Findings\nFuture Directions",
            "content": "1 2 3 5 6 6 7 7 8 8 9 12 15 18 23 23 25 25 25 25 26 28 28 34 36 38 40 43 45 45 47 47 48 48 48 50 50 50 50 51 Comprehensive Survey of Mamba Architectures for Medical Image Analysis"
        },
        {
            "title": "1 Introduction\nIn recent few decades, there has been a remarkable improvement in the field of medicine through\nthe application of machine learning [111] as well as deep learning [114]. The initial architectures\nof neural networks like Convolutional Neural Networks (CNNs) [82] played a pivotal role in better\nimage segmentation [74], classification [79, 106], and object detection [81]. Medical images are\ncomplex, but CNN’s were able to analyze 3D structures in a 2D plane and so proved useful in\nbiomedical image computing especially into image segmentation [108], tumor detection [17], organ\nsegmentation [160], and disease diagnosis imaging [16]. CNNs have been applied extensively to\nmedical imaging tasks, namely segmentation, classification and reconstruction. One weakness,\nhowever, is that they can lack when sequencing data, or multitasking which requires long-range\ndependencies. For example, in the area of medical image segmentation, the use of CNNs may\nnot perform well as one would expect since they may not be able to model super-resolution\ninter-dependence of an image and its parts.",
            "content": "Some of the drawbacks of CNNs have been addressed by transformers [107, 124] as advancements of technologies that have better sequential data processing and long-range dependencies. Still, there are some disadvantages as well. The main problem is the scaling of computed attention which grows quadratically with the sequence length, thus makes the use of such attention costly and hard on sequences that are very long. Moreover, many additional resources and data are usually needed, which is quite problem if one has to work in resource-restricted environment such as in the medical domain. In regard to the shortcomings of classical CNNs and transformers, there has been noticeable progress in research on different types of models which could efficiently represent the long sequences and their intricate dependencies. Of late, State Space Models (SSMs) [47] have gained much attention as one of such alternatives as the Mamba [45] model. Fig. 1. Evolution of Mamba from State Space Models (SSMs) Mamba, aims to address the problems related to modern deep learning techniques. Selective state spaces are employed to quickly assimilate vast lengths of sequences, combine various modes and S. Bansal et al. command extensive yet practical resolutions. The architecture of Mamba incorporates selective scan mechanism and hardware aware algorithm which support high efficiency in storage and computation of the intermediate results. This helps Mamba perform very well in some tasks such as medical image segmentation [128, 134, 138], classification [44, 99, 150], synthesis, registration and reconstruction [62, 83, 162] where long range dependency and high complexity is involved. Mamba has performed promisingly well in the biomedical field, especially in the fields of biomedical imaging, genomics and processing clinical notes. Thus, the model comes in handy in capturing long range and multi-modal data oriented tasks which involve subtle relationships and dependencies between units of information. Figure 1 shows the timeline of Mambas evolution over time, starting from HiPPO [46] and SSMs such as Linear State Space Layer (LSSL) [49], S4 [47], Diagonal State Space (DSS) [51] , S4D [48], S5 [113], S4ND [100], Hungry Hungry Hippos (H3) [42] to Mamba [45]. It also includes variants of Mamba that were created as the model evolved. 12.2% 22.4% 8.2% 4.1%"
        },
        {
            "title": "Miscellaneous\nImage Segmentation",
            "content": "53.1% Fig. 2. Distribution of Research Papers Utilizing Mamba in Medical Domain The pie chart, as depicted in Figure 2 illustrates the distribution of research papers utilizing the Mamba framework across various tasks in the medical domain. The chart is divided into five segments, each representing specific task and its corresponding percentage contribution to the total number of papers. Moreover, Figure 3 illustrates the fluctuation in the number of publications related to Mamba in medical domain over the period from December 2023 to September 2024. notable surge in research activity is evident in March and April 2024. There exist several survey papers on Mamba. However, these might do either of the following works [101, 104] cover the framework broadly or are restricted to its use only in the vision domain [87, 140, 153]. It is worth noting that only [56] provided review of Mamba which was focused on its uses in medical domain. However, our survey paper is more extensive and detailed than those of [56]. In particular, the present work emphasizes the analysis of public resources such as medical datasets, and presents some empirical data on the applicability of Mamba within medical practice, including various resources and interventions to be utilized within Mamba in healthcare context. In addition to this, we include the latest research and developments of Mamba architectures for medical image analysis. Moreover, we have structured our work in such way that readers will Comprehensive Survey of Mamba Architectures for Medical Image Analysis Fig. 3. Publication Trend of Mamba Research in the Medical Domain appreciate the organizational spans of Mamba including its strengths, weaknesses and prospects within the medical arena. In this survey, we focus on use, methods and problems of Mamba state space models within the medical domain. We provide complete overview of the current state of development of this direction, focusing on determining the advantages and disadvantages of the Mamba models, as well as their future prospects. The rest of the paper is organized as follows. Section 2 discusses the key terms related to SSM, different Mamba architectures are explained in Section 3.1. Several Mamba optimizations are discussed in Section 3.3. Several techniques such as weakly supervised, semi-supervised, self supervised, contrastive learning, and multimodal learning are explained in Section 3.4. Different scanning mechanisms in Mamba are discussed in Section 3.2, several applications in different domains are explained in Section 3.5. Datasets are summarized in Section 4. Experimental results showing Mamba performance across different tasks are discussed in Section 5. Limitations and emerging areas are explained in Section 6, finally we conclude the work by giving future directions in Section 7."
        },
        {
            "title": "2 Core Concepts of SSM\nIn the realm of deep learning, Transformers have consistently dominated in both Computer Vision\n(CV) and Natural Language Processing (NLP) tasks. The self-attention [124] mechanism within\nTransformers has greatly improved the understanding of these modalities by generating an attention\nmatrix from the query, key, and value vectors. While the attention matrix is beneficial, it suffers\nfrom quadratic time complexity. The recent advancements, such as FlashAttention by Dao et al.\n[20, 21] and linear attention [72], have addressed this issue by reducing the time complexity. For\ninstance, in linear attention, the key is multiplied by the value instead of the query, and the softmax\nfunction is replaced with a similarity function. Mamba developed by Gu et al. [45] further mitigates\nthis problem by transforming the quadratic time complexity into linear time complexity in a\nrecurrent manner. Mamba is the first model without attention to match the performance of a very\nstrong Transformer. The core concepts of Mamba and its derivation from SSM are explained in the\nfollowing sections.",
            "content": "S. Bansal et al."
        },
        {
            "title": "2.1 State Space Models\nState Space Models (SSMs) uses an approach similar to Kalman filter [71]. SSMs convert a one-\ndimensional input sequence 𝑢 (𝑡) into an N-dimensional continuous latent state 𝑥 (𝑡), which is then\nprojected into a 1D output signal 𝑦 (𝑡). The entire process of a state space model can be represented\nas shown in Equation 1 and Equation 2:",
            "content": "𝑥 (𝑡) = A𝑥 (𝑡) + B𝑢 (𝑡) (1) 𝑦 (𝑡) = C𝑥 (𝑡) + D𝑢 (𝑡) (2) Parameters A, B, C, are initialized differently in SSM models such as S4, S5 and S6. To apply discrete input sequence 𝑢 (𝑢0, 𝑢1, ...), instead of continuous function 𝑢 (𝑡) , the sequence should be discretized using parameter Δ, known as the step size. This process also involves discretizing parameters (A, B, C, D). In the following sections, we discuss each model and its specific discretization steps."
        },
        {
            "title": "2.2 Structured State Space Sequence Models (S4)\nS4 proposed by Gu et al. [47] demonstrates how to efficiently compute all forms of the SSM: the\nrecurrent representation, and the convolutional representation. Additionally, S4 employs a bilinear\nmethod for discretizing the parameters of the SSM, converting the state space parameter A into an\napproximation A. In S4, the parameter 𝐷 from the original state space model is either set to 0 or\nused as a residual connection.",
            "content": "The discrete SSM can be expressed in its recurrent form as shown in Equation 3 and Equation 4. 𝑥𝑘 = A𝑥𝑘 1 + B𝑢𝑘 = (I Δ/2 A) 1(I + Δ/2 A) 𝑦𝑘 = C𝑥𝑘 = (I Δ/2 A) 1(ΔB) = (3) (4) Unrolling the equation above leads us to the convolutional aspect of S4 in Equation 5 & Equation 6. 𝑥0 = B𝑢0 𝑥1 = AB𝑢0 + B𝑢1 𝑥2 = B𝑢0 + AB𝑢1 + B𝑢2 𝑦0 = CB𝑢0 𝑦1 = CAB𝑢0 + CB𝑢1 2 𝑦2 = CA B𝑢0 + CAB𝑢1 + CB𝑢 (5) (6) This can be vectorized into convolution as shown in Equation 7 & Equation 8 with an explicit formula for the convolution kernel as shown in Equation 9. 𝑘 𝑦𝑘 = CA B𝑢0 + CA 𝑘 B𝑢1 + ...... + CAB𝑢𝑘 1 + CB𝑢𝑘 𝑦 = 𝑢. (7) (8) 𝑖 := 𝜅𝐿 (A, B, C) := (CA R𝐿 (9) Equation 7 & Equation 8 represents single convolution, and 𝐾 is referred as the SSM convolution kernels or filters. The parameters of S4 are initialized randomly, except for 𝐴. The 𝐴 parameter is initialized as HiPPO Matrix, which is defined in Equation 10: B)𝑖 [𝐿] = (CB, CAB, ..., CA B). 𝐿1 (HiPPO Matrix) S4 addresses the limitations of transformers by implementing these strategies. This empowers SSMs to excel in tasks requiring long-range dependencies such as Path-X [118] . In contrast, Transformers (2𝑛 + 1)1/2(2𝑘 + 1)1/2 𝑖 𝑓 𝑛 > 𝑘 𝑖 𝑓 𝑛 = 𝑘 𝑛 + 1 𝑖 𝑓 𝑛 < 𝑘 0 A𝑛𝑘 = (10) Comprehensive Survey of Mamba Architectures for Medical Image Analysis [33, 73] in Path-X exhibit accuracy below 50% (worse than random guessing), whereas S4 achieves approximately 80% accuracy."
        },
        {
            "title": "2.3 Simplified State Space Layers for Sequence Modeling (S5)\nS5 proposed by Smith et al. [113], extends from S4 with similar initialization conditions, but enhances\nthe architecture by adopting a Multiple Input Multiple Output (MIMO) approach. Notably, S5\nintroduces a learnable time scale parameter (Δ), replacing the fixed parameter used in S4. Parameters\nin S5 are discretized using the Zero Order Hold (ZOH) method as mentioned in Equation 11,\nproviding a refined parameter system compared to S4.",
            "content": "Λ = 𝑒ΛΔ, = Λ1(Λ I) B, = C, = D. (11) In terms of computation, S5 employs fully recurrent connection with parallel scanning , as detailed in Equation 3 and Equation 4. The authors highlight that smaller latent space employs SSM to do parallel scanning where an associative operation is used in between during offline setting. This characteristic positions S5 for both online and offline processing, highlighting its utility in recurrent tasks within the time domain."
        },
        {
            "title": "2.4 Selective Structured State Space Models (S6)\nExtended from S5, S6 introduced by Gu et al. [45] incorporates SSMs within the Mamba Architec-\nture. S6 builds upon the foundational assumptions of previous SSMs by leveraging a projection\nof the input function (using a linear layer) for initializing parameters 𝐵 and 𝐶. Notably, S6 also\napplies this projection to the step size parameter i.e., Δ. To enhance computational efficiency, S6\nimplements faster recurrence connections using operations on the Static Random Access Memory\n(SRAM) of the GPU, with storage on the High Bandwidth Memory (HBM) similar to the principles\noutlined in FlashAttention mechanism by Dao et al. [21] and FlashAttention-2 mechanism by Dao\net al. [20]. The parameters in S6 are discretized using the Zero Order Hold method, following the\napproach set by S5. The Mamba architecture integrates components from both H3[42] and Gated\nMLP, incorporating an additional SSM layer and connections resembling the green parallelograms\nfound in H3. Algorithm 1 & Algorithm 2 outlined below shows the differences between S4 and S6.",
            "content": "Algorithm 1 SSM (S4) Input : 𝑥 : (𝐵, 𝐿, 𝐷) Output : 𝑦 : (𝐵, 𝐿, 𝐷) 1: : (𝐷, 𝑁 ) Parameter Algorithm 2 SSM + Selection (S6) Input : 𝑥 : (𝐵, 𝐿, 𝐷) Output : 𝑦 : (𝐵, 𝐿, 𝐷) 1: : (𝐷, 𝑁 ) Parameter Represents structured 𝑁 𝑁 matrix Represents structured 𝑁 𝑁 matrix 2: : (𝐷, 𝑁 ) Parameter 3: : (𝐷, 𝑁 ) Parameter 4: : (𝐷) 𝜏Δ (Parameter) 5: A, : (𝐷, 𝑁 ) discretize(Δ, A, B) 6: 𝑆𝑆𝑀 (A, B, C)(𝑥) 2: : (𝐵, 𝐿, 𝑁 ) 𝑠B(𝑥) 3: : (𝐵, 𝐿, 𝑁 ) 𝑠C (𝑥) 4: : (𝐵, 𝐿, 𝐷) 𝜏Δ (Parameter + 𝑠 (𝑥)) 5: A, : (𝐵, 𝐿, 𝐷, 𝑁 ) discretize(Δ, A, B) 6: 𝑆𝑆𝑀 (A, B, C)(𝑥) Time-invariant: recurrence or Time-varying: recurrence (scan) convolution 7: return only 7: return Mamba integrates selective mechanism into its state space models (S6) to prioritize important content dynamically during training. SSM + Selection (S6) outlines computational process S. Bansal et al. for handling input sequences 𝑥 and generating corresponding output sequences 𝑦. It begins by initializing structured matrix and projecting the input sequence 𝑥 into tensors and using specific functions 𝑠B and 𝑠C. The parameter , serves as time step parameter which is used in discretization. It is computed based on function 𝜏 incorporating additional parameters and 𝑠. Subsequently, and undergo discretization alongside , resulting in transformed tensors and as mentioned below in Equation 12. = 𝑒𝑥𝑝 (ΔA) = (ΔA) 1(𝑒𝑥𝑝 (ΔA) I) ΔB (12) The core of the algorithm involves applying state space model (SSM) function to 𝑥 using (A, B, C) facilitating time-varying recurrence process (scan). This approach ensures that the output sequence 𝑦 reflects the transformations and interactions specified by SSM and selection mechanisms integrated within the architecture of S6. Figure 4 illustrates how selective components from H3 and Gated MLP are combined to construct the Mamba. Fig. 4. Architecture of Mamba Block [45] combining H3 [42] and Gated MLP"
        },
        {
            "title": "3.1 Mamba Architectures\nIn this section, we explore and discuss the architectural landscape of Mamba, beginning with an\nexploration of the foundational pure Mamba design and its evolution through variants of U-Net.\nWe then transition into the realm of hybrid architectures, where Mamba is ingeniously combined\nwith other powerful techniques to achieve enhanced performance and tackle complex tasks.",
            "content": "A Comprehensive Survey of Mamba Architectures for Medical Image Analysis"
        },
        {
            "title": "3.1.1 Pure Mamba.",
            "content": "Vision Mamba (ViM) proposed by Zhu et al. [164] incorporates bidirectional SSM by combining convolution with S6 in both forward and backward direction. Moreover, softplus [159] function is applied over the selective scan parameter Δ0 to make sure the parameter stays positive. The parameters of the S6 are discretized with Δ0 parameter mentioned above. ViM employs training strategy similar to Vision Transformers (ViT) [24] where patches of inputs are tokenized by separating them into non-overlapping patches and applying convolution layer on each patch with dimension 𝑑. The tokenized patches are then concatenated with class labels and learnable position encoding are added to the class label and tokenzied patches. Concatenating the class label in the middle has proven to work well as it exploits the recurrent nature of networks. Overall, ViM shows comparable differences in memory and performance to parametric heavy models such as DeiT-Ti (Data-efficient image Transformers-Tiny), DeiT-S (Data-efficient image Transformers-Small) proposed by Touvron et al. [119] on tasks such as object detection, classification and segmentation. Particularly on higher dimensional images such as 1248 1248, ViM consumes 73.2% less memory than DeiT and 2.8 faster than DeiT. Figure 5(a) depicts the architecture of the ViM block, illustrating its key elements and their role in improving the models performance in vision tasks. (a) Vision Mamba (b) Visual State Space Fig. 5. Architectures of 5(a) Vision Mamba (ViM) [164] and 5(b) Visual State Space (VSS) Block [88] VMamba proposed by Liu et al. [88] employs an architecture similar to transformers, replacing the traditional multihead attention block [124] with novel approach. It utilizes Visual State Space (VSS) block, which differentiates it from the standard Mamba architecture by incorporating depthwise convolution and SS2D (2D selective scan). This new implementation features nonmultiplicative branching method and replaces S6 used in Mamba with SS2D. While S6 performs well for NLP tasks, extending it to 2D vision data presents challenges in making S6 modules scan-independent. SS2D addresses this by implementing gating mechanism, eliminating the need for branched multiplication as proposed in Mamba and ViM. In VMamba, patches are initially partitioned within the stem module, resulting in feature map of size 𝐻 /4 𝑊 /4. As the data progresses through the layers, the feature map dimensions change sequentially to 𝐻 /8 𝑊 /8, 𝐻 /16 𝑊 /16, and finally 𝐻 /32 𝑊 /32, with representing the networks arbitrary dimensionality. Each stage, except the first, includes downsampling block alongside VSS. Ultimately, prediction head is employed to generate outputs for the designated task. SS2D consists of two stages: 1) cross scan module 2) cross merge module. The cross scan module in VMamba scans patches in four different directions: top to bottom, bottom to top, left to right and right to left. For each direction, an independent SSM is utilized, and the representations from these SSMs are combined using cross merging. On the performance perspective, VMamba achieves superior metrics with S. Bansal et al. minimal number of parameters and memory usage. Its performance is comparable to ViM-S, DeiT-S, and DeiT-B across various tasks including semantic segmentation, classification, and object detection. Figure 5(b) depicts the architecture of the Visual State Space (VSS) block, showcasing its key components like SS2D scanning and depthwise convolution. Plain Mamba proposed by Yang et al. [141] is non-hierarchical SSM, similar to Vision Transformers (ViT). Drawing inspiration from ViT, Plain Mamba begins with patch embedding combined with position embedding, followed by plain Mamba layer. In contrast to VSS, the plain Mamba layer utilizes gated multiplication of features, similar to ViM, but instead of single SSM block, it employs four SSMs with continuous 2D scanning. The scan orders are the same as in VMamba, ensuring no positional bias and promoting uniform image understanding. Plain Mamba as shown in Figure 6(a), also incorporates direction-aware updating by embedding 2D relative position information into flattened 1D layer within the SSM. Unlike ViM, it uses global pooling, followed by classification head on top. (a) Plain Mamba (b) Efficient Visual State Space Fig. 6. Architectures of 6(a) Plain Mamba [141] and 6(b) Efficient Visual State Space (EVSS) Block [102] Extension of VMamba: Pei et al. [102] proposed the EVSS block, which stands for Efficient Visual State Space, combines local and global features effectively. EVSS block shown in Figure 6(b), employs the ES2D module to extract the global feature map and depth-wise convolutional branch to obtain the local feature map. It also incorporates squeeze excitation block similar to SqueezeNet which is proposed by Iandola et al. [64]. The global and local features are then combined. ES2D introduces novel method of patch-wise scanning. Initially, the input image is divided into patches belonging to different groups. These patches undergo forward and backward 2D scanning, similar to the VSS method, and are then passed to S6 and merged. In the overall architecture, EVSS blocks are utilized in the initial stages, while inverted residual blocks are employed in later stages. In conclusion, EfficientVMamba achieves better performance with less number of parameters compared to transformer-based and convolution-based models. Li et al. [80] introduced the Mamba ND Block, which evaluates different scanning methods for 2D and 3D image representations. They found that the most effective approach involves scanning forward and then backward across height, width, and volume/time (if applicable). Separate Mamba blocks are designated for each scanning direction. The Mamba ND models outperformed transformer-based models while utilizing fewer parameters, resulting in enhanced performance in tasks such as video classification. The architecture of Mamba ND block is detailed in Figure 7, showcasing its design and structural components. Comprehensive Survey of Mamba Architectures for Medical Image Analysis Fig. 7. Architecture of Mamba ND Block [80] MHS-VM proposed by Ji et al. [68], introduced an organized approach to construct visual features within 2D image spaces using Multi-Head Scan (MHS) module. This module projects embeddings from the previous layer into multiple lower-dimensional subspaces, where selective scan is performed along distinct scan routes. The resulting sub-embeddings undergo integration and projection back into the high-dimensional space. Additionally, the module incorporates Scan Route Attention (SRA) mechanism to improve its ability to discern complex structures. To validate its efficiency, the SS2D block replaces the original block in VM-UNet, resulting in significant performance improvements while reducing parameters. Figure 8(a) illustrates the architecture of MHS-VM, detailing its use of components such as depthwise convolution, layer normalization, and multihead scanning. Extension of ViM: Huang et al. [63] proposed Local Mamba, which implements novel scanning mechanism where tokens are partitioned into distinct windows, and scanning is performed continuously within each patch of these windows. Local Mamba consists of four SSM blocks with different scanning patterns: 7 7 local scan, 2 2 local scan, vertical scan (both forward and backward), and horizontal scan (both forward and backward). Figure 8(b) depicts the architecture of Local Mamba, which incorporates four SSM blocks. Mamba with ViM: Wang et al. [130] proposed Weak-Mamba-UNet, hybrid model which integrates CNN, ViT, and ViM-based Mamba models to achieve medical image segmentation using scribble-based annotations. The model assigns specific weighting factors to each network, denoted by parameters 𝛼, 𝛽 and 𝛾 for respective models. Mamba, proposed by Hao et al. [53], introduced modified architecture based on ViM. This architecture includes an additional branched input representation, which undergoes weighted fourier transformation and band-pass filtering, followed by an inverse fourier transformation. Using gated selection unit, these transformed representations are then projected into single unified representation. Mamba with VSS: Wu et al. [134] proposed H-vmunet, which presents modified architecture based on the VSS block that enables selective scan 2D. The paper introduces Higher Order Visual State Space (H-VSS), employing both first order and third order operations. Each order is mathematical multiplication of its own projection with local-SS2D block. The local-SS2D block S. Bansal et al. (a) Local Mamba (b) MHS-VM Fig. 8. Architectures of 8(a) Local Mamba Block [63] and 8(b) MHS Block [68] consists of LayerNorm [3], followed by 𝐶/2 convolutional layer and 𝐶/2 SS2D layer. These are concatenated, and another LayerNorm is applied at the end. Wu et al. [135] proposed UltraLight VM-UNet which employs parallel vision mamba layer. Initially, the input is normalized and split into the desired number of channel-wise parts. Each split part undergoes Mamba layer with residual connection using an adjustment factor. The results are then concatenated and projected out with LayerNorm. For skip connections, UltraLight VM-UNet uses Spatial Attention Bridge and Channel Attention Bridge, similar to the traditional approach. Light M-UNet proposed by Liao et al. [85] employs the RVM (Residual Vision Mamba) layer in the encoder block. This involves applying LayerNorm, followed by VSS module, and then performing scaled residual connection. The decoder block utilizes depthwise convolution with scaled residual connection. Others: SegMamba proposed by Xing et al. [138] has the TOM (Tri-orientated Spatial Mamba Block) module which improves the sequential modeling of 3D features from three different directions, after which the spatial features are modeled by gated spatial convolution module. The model architecture includes gated spatial convolution followed by LayerNorm and TOM block, which is residually connected with the input, creating an architecture similar to transformers. The Tri-Oriented Mamba block first flattens the 3D representation and applies Mamba layer to rows, columns, and features respectively. Following this, the encoder output is downsampled using Conv3D networks, and then CNN-based decoder is utilized. Wang et al. [128] proposed LKM-UNet where the image is partitioned into pixel-level segments and bidirectional Mamba is applied to each segment. After pooling the outputs, bidirectional Mamba is applied again in the network. Finally, the resulting representation is added residually to the pixel-level representation. mechanism that plays crucial role in Mamba architectures is scanning, which is detailed in the upcoming subsection 3.2."
        },
        {
            "title": "3.1.2 Variants of U-Net.",
            "content": "The integration of Mamba blocks into the U-Net [108] architecture has resulted in several variants which are designed to improve its performance. For instance, they can be added before the first encoder layer, after an encoder layer, within the skip connections or can even replace the whole encoder block in UNet architecture. Figure 9 presents comparision of traditional U-Net and Mamba U-Net [132] architecture. Both networks share standard UNet architecture having encoder and decoder blocks connected by skip connections but Mamba U-Net incorporates mamba blocks. Ruan et al. [109] proposed VM-UNet where images are converted into tokens using patch embedding block. The encoder of the network contains two VSS blocks with patch merging block and skip connection in each layer of the encoder. The decoder contains patch expanding block Comprehensive Survey of Mamba Architectures for Medical Image Analysis Fig. 9. Architectural Comparison of Traditional U-Net and Mamba U-Net followed by VSS block with skip connections added from the encoder. Finally, the representations from the decoder are then passed on to the final projection layer which reconstructs the image back into its original size i.e., the number of classes. Figure 10 illustrates the UNet-based architecture of VM-UNet. Fig. 10. Architecture of VM-UNet [109] Archit et al. [2] introduced ViM-UNet which uses ViM as its base Mamba. ViM-UNet features ViM encoder with bidirectional SSM layers, while its decoder copies the design of UNEt TRansformers (UNETR) [54] using convolution and transposed convolution layers. To enhance efficiency, ViMUNet introduces flexibility in its encoder sizes, such as tiny and small. In contrast, the traditional S. Bansal et al. UNet uses standard CNN for biomedical segmentation. UNet relies heavily on skip connections between corresponding encoder and decoder blocks to capture better features. However, ViM-UNet takes different approach by excluding these skip connections and using ViM as the encoder. This key architectural change allows ViM-UNet to achieve efficient global feature extraction and improved segmentation performance, without the need for the skip connections that are important in the traditional UNet. Inspired by VM-UNet [109], Zhang et al. [156] proposed the second version called VM-UNet-V2. The model differs from VM-UNet by not using skip connections between subsequent encoder to decoder layers as shown in ??. Instead, features from each block of encoder are fused using Semantics and Details Infusion block (SDI block). VSS captures contextual information within images, while SDI module improves the integration of both low-level and high-level features, leading to comprehensive understanding of image features. By combining these elements, VM-UNetV2 maximizes the potential of SSMs within the UNet, thus offering more efficient and powerful solution for segmentation tasks. Fig. 11. Architecture of VM-UNet V2[156] Wang et al. [128] proposed LKM-UNet, novel architecture designed for efficient 2D and 3D medical image segmentation. LKM-UNet uses strengths of Mamba to achieve superior performance in both local and global modeling with linear time complexity. The use of large windows within SSM improves the receptive field compared to CNNs and Transformers. The network architecture includes hierarchical and bidirectional Mamba block, enhancing spatial modeling capabilities by integrating Pixel-level (PiM) and Patch-level (PaM) SSMs. LKM-UNet excels in capturing both fine-grained details and long-range dependencies from data. The model achieves efficient feature extraction and segmentation through U-shaped network with encoder-decoder blocks, layers for downsampling and upsampling along with skip connections. Wu et al. [134] proposed High-order Vision Mamba UNet (H-vmunet) which enhances the 2Dselective-scan (SS2D) mechanism and also introduced higher-order interactions to reduce redundant Comprehensive Survey of Mamba Architectures for Medical Image Analysis Fig. 12. Architecture of H-vmunet [134] information and improve extraction of local features. Traditional architecture like CNN and ViT face limitations in processing long sequences and capturing local features respectively. The proposed architecture features U-shaped structure with six layers which incorporates High-order Visual State Space (H-VSS) module and it applies higher-order SS2D from layers three to six. The inclusion of Spatial Attention Bridge (SAB) and Channel Attention Bridge (CAB) modules in the architecture as shown in Figure 12 further enhances multilevel and multiscale information fusion which is crucial for capturing detailed medical image features."
        },
        {
            "title": "3.1.3 Hybrid Architectures.",
            "content": "In this section, we explore hybrid architectures that combine Mamba with other powerful techniques. We cover Mambas integration with convolutions for enhanced feature extraction, attention mechanisms and transformers for capturing contextual relationships, recurrence for modeling sequential data and GNNs for graph structured data. Additionally, we touch upon other miscellaneous hybrid approaches that demonstrate Mambas versatility. Mamba with Convolution: CNNs often face challenges in capturing long-range dependencies due to their inherent focus on local features and computational complexity. SSMs have the ability to handle long sequences of data. The combination of Mamba with convolution plays huge role in capturing local spatial information along with capturing long range dependencies from medical images. Some of the papers that demonstrate these hybrid approaches are as follows: Ma et al. [93] introduced U-Mamba, hybrid CNN-SSM architecture integrating Mamba blocks within the encoder of U-Net, demonstrating superior performance over traditional CNN-based and transformer-based segmentation networks across various modalities and segmentation targets. Wang et al. [132] proposed Mamba-U-Net which integrates pure Vision mamba in the U-Net along with linear embedding and VSS block in the model. Xu et al. [139] developed HC-Mamba, which uses dilated convolution followed by depth-wise separable convolution along with Mamba. It improves the receptive field and reduces the parameters of the model. Another innovative model, SegMamba S. Bansal et al. designed by Xing et al. [138], mimics Transformer-based U-Net architecture, utilizing series of TS-Mamba block and downsample block with residual connections inspired by He et al. [55]. TS-Mamba employs gated spatial convolution to analyze spatial relationships between 3D features, followed by the Tri-oriented Spatial Mamba (TOM) block, which replaces the traditional multiheaded attention layers in transformers. Gong et al. [44] proposed nnMamba, which features the Mamba-In-Convolution with Channel-Spatial Siamese (MICCSS) block. The backbone architecture of nnMamba leverages the MICCSS block to maintain computational efficiency. This helps in improving the models representational capacity and improving the performance on tasks that require deep understanding of visual data. Fig. 13. Architecture of Swin-UMamba [86] Liu et al. [86] proposed Swin-UMamba which is hybrid architecture that combines VSS with UNet for medical image segmentation tasks. Swin-UMamba detailed in Figure 13 integrates Mamba-based encoder pretrained on ImageNet with series of VSS blocks and upsample blocks. The architecture of Swin-UMamba includes patch embedding, VSS blocks, patch merging, upsample blocks, residual blocks, and 1x1 convolution for segmentation output which combines Mamba with traditional segmentation network components. By combining the strengths of Mambabased models with segmentation network structures, Swin-UMamba achieves superior results in medical image segmentation tasks, highlighting its effectiveness in improving pretrained models for improved segmentation accuracy. Figure 13 details the architecture of Swin-UMamba. Mamba with Attention and Transformers: Wang et al. [130] proposed Weak-Mamba-UNet which uses CNN-based UNet for extracting local features from data, Swin Transformer-based SwinUNet for understanding global context and VMamba based Mamba-UNet for modeling longrange dependencies. The results of Weak-Mamba-UNet shows that it solves inaccurate predictions caused by other approaches. Kirillov et al. proposed Segment Anything Model (SAM) [76] which is Comprehensive Survey of Mamba Architectures for Medical Image Analysis originally done for 2D images. Inspired by this, Wang et al. [127] proposed Efficiently Adapting Segment Anything Model (SAM) which shows superior performance on 3D medical images. The authors have modified the encoder of SAM by adding Tri-Plane Mamba (TP-Mamba) block on top of each ViT and the decoder of the model uses 3D convolution with instance normalization [123] and GELU activation function [57]. The authors have also used LoRA [60] to adapt weights of MultiHead self Attention module. TP-Mamba shows superior performance compared to transformer based models, U-Mamba [93] and other adapter-based algorithms for SAM such as SA-Med [154], MA-SAM [11]. Figure 14 gives an overview of pretrained SAM with adapters on 3D segmentation, ViT blocks from pretrained SAM and the architecture of TP-Mamba. Fig. 14. Architecture of Tri-Plane Mamba [127] Zhang et al. [155] proposed HMT-UNet which combines MambaVision Mixer with the transformers self attention block for segmentation of medical images. MambaVision mixer is modification of the Mamba layer, where the first casual convolution is replaced with regular convolution and passed on to SSM. Initially, the encoder of UNet consists of four layers where the initial two layers uses two 3 3 convolution with stride of two and downsampling uses batch normalized 3 3 convolution with stride of two which scales down the resolution of the spatial dimension by half and doubles the channel. The layer 3 and layer 4 contains MambaVision Mixer and transformers self attention block. In the decoder block, the spatial dimensions are doubled and channel size is halved. The final stages of decoder use the same configuration as initial stages of encoder but instead of downsampling the image as in encoder, it linearly upsamples the image. Mamba with Recurrence: VMRNN proposed by Tang et al. [117] is recurrent cell that incorporates VSS block within an Long Short Term Memory (LSTM) [59] network. Mambas integration into spatial-temporal forecasting based on vision allows robust sequential modeling. The paper introduces two novel architectures: VMRNN-B and VMRNN-D. These architectures excel at extracting spatiotemporal features, establishing new strong foundation for spatiotemporal forecasting. Spatiotemporal predictive learning differs from traditional image-level vision tasks by predicting S. Bansal et al. future video frames based on past sequences. VMRNN tackles this challenge by processing individual frames, segmenting them into smaller patches. These patches gets flattened before feeding them into patch embedding layer for initial processing. The VMRNN layer then leverages these transformed patches alongside past states to capture the crucial spatiotemporal features necessary for predicting the next frame. The hybrid architecture of VSS and LSTM is attributed to the models capability to learn and leverage global spatial dependencies with linear complexity, enabling more refined understanding of spatiotemporal dynamics. Mamba with GNN:. Ding et al. [23] combines Mamba with Graph Neural Network (GNN) to capture global and local tissue spatial relationships respectively in Whole Slide Images (WSI). The model leverages message-passing GNN, specifically Graph Attention Network (GAT) proposed by Velickovic et al. [125], to process hierarchical graph constructed from both cell-level and tile-level graphs. Graph convolution operation is performed using GAT which assigns importance scores to neighboring nodes, allowing the model to focus on the most relevant information for the current node during the convolution process. Miscellaneous: Nasiri et al. [99] proposed Vim4Path which used ViM architecture within the DINO framework proposed by Caron et al. [10], for learning representations in computational pathology. ViM is modified to accept arbitrary input image sizes using positional embedding interpolation, making it adaptable within DINO for Self-Supervised Learning (SSL). The study benchmarks the Camelyon16 dataset [5], extracting image patches from Whole Slide Images (WSIs) without labels for training the ViM encoder using the DINO framework. Yang et al. [145] introduced the MambaMIL framework, which combines the Mamba framework with Multiple Instance Learning (MIL) to enhance long sequence modeling in computational pathology. The core component of the MambaMIL framework is the Sequence Reordering Mamba (SR-Mamba), which is designed to be aware of the order and distribution of instances within long sequences. Mamba is used to enable each instance through compressed hidden state to interact with previously scanned instances, facilitating effective modeling of long sequences while reducing computational complexity. The MambaMIL framework partitions tissue regions into sequence of patches, maps these patches into instance features, reduces feature dimension through linear projection, utilizes stacked SRMamba modules for handling long sequences, and finally employs an aggregation module to obtain bag-level representations for downstream tasks."
        },
        {
            "title": "3.2 Scanning\nAttention mechanisms, especially self-attention have a quadratic time complexity causing computa-\ntional costs to grow quadratically with sequence length. In contrast, scanning operations generally\nhave linear time complexity, making them more efficient for long sequences. The scan operation\ninvolves calculating an array, like the prefix sum, where each value is determined by using the\npreviously calculated value and the current input. Similarly, the recurrent form of SSM can be\nviewed as a scan operation. Scanning is a crucial component in mamba, especially when handling\nmultidimensional inputs. The selection of the scanning mechanism in Mamba models is crucial\nas it enhances efficiency and provides important information. Figure 15 provides visualization of\nvarious scanning mechanisms employed in mamba-based architectures. This visualization high-\nlights the diversity of scanning approaches integrated into mamba models. Table 1 summarizes\nthe various scanning mechanisms and the models associated with each mechanism. The scanning\nmethodologies used in mamba models are detailed as follows:",
            "content": "(1) Bidirectional Scan : In bidirectional scan (forward and backward scan) [164], after tokenizing image patches, they are processed through the forward SSM. Simultaneously, the same Comprehensive Survey of Mamba Architectures for Medical Image Analysis (a) Depicts (i) BiDirectional Scan [164], (ii) Continuous 2D Scan [141], (iii) Local Scan [63], (iv) Cross Scan [88] and (v) Multi-Head Scan [68] (b) Demonstrates (vi) Selective 2D Scan [88], (vii) Efficient 2D Scan [102], (viii) Omnidirectional Selective Scan [112], (ix) 3D BiDirectional Scan [78] tokenized representations of images are independently processed through the backward SSM. This scanning mechanism primarily used in ViM-based models, enables the model to S. Bansal et al. (c) Shows Visual Representation of (x) Hierarchical Scan [158], (xi) Zigzag Scan [61], (xii) Spatiotemporal Scan [146], (xiii) Multi-Path Scan [13] (d) Depicts (xiv) Three Directional Scan [138], (xv) BSS Scan [40] and (xvi) Pixelwise and Patchwise Scan [128] Fig. 15. Illustrations in 15(a), 15(b), 15(c) & 15(d) present Different Scanning Mechanisms used in Mambabased Architectures Comprehensive Survey of Mamba Architectures for Medical Image Analysis capture contextual information from both directions, improving its ability to understand and represent the image data effectively. (2) Selective Scan 2D : SS2D [88] performs scanning operations in three directions: top to bottom, left to right, and in reverse direction. Each mamba block is placed to work independently within these directions. SS2D mirrors the self-attention process seen in transformers. It overcomes the limitations of bidirectional scan in ViM, but it also leads to loss of patch continuity. To address this, SS2D incorporates scan merge step, where representations from each scan direction are combined into unified output. (3) Continuous 2D Scan : Continuous 2D scan [141] resolves the issue which is experienced in SS2D. It involves integrating direction-aware parameters into cross-scan mechanism and organizing patches accordingly. This approach ensures the preservation of patch continuity and maintains the contextual understanding of images. The continuous 2D scanning adds direction-aware parameter into data dependent parameter of SSM (B) which is expressed in Equation 13 and Equation 14. ℎ 𝑘,𝑖 = A𝑖ℎ𝑘,𝑖 1 + (B𝑖 + Θ𝑘,𝑖 )𝑥𝑖 𝑦 𝑖 = 4 k=1 (C𝑖ℎ 𝑘,𝑖 + D𝑥𝑖 ), 𝑦𝑖 = 𝑦 𝑖 𝑧𝑖 (13) (14) (4) Zigzag Scan : The extension of continuous 2D Scan is zigzag scan [61] where the images are scanned with continuous scanning mechanism in both forward and backward direction. Zigzag scan is developed to enhance the continuity of patches in the images which are used for diffusion models such as ZigMa [61]. (5) Spatiotemporal Selective Scan : Spatiotemporal selective scan [146] is used to scan on videos where the patches are unfolded on each frame along rows and columns and then concatenated with the frame sequence of ℎ𝑡 𝑖 𝜖 R𝐶𝑖 𝑇 (𝐻𝑊 ) . In this setup, scanning is done bidirectionally to know about temporal dependency. Parallelly, scanned patches are stacked around temporal axis to construct the spatial sequence in the form of ℎ𝑠 𝑖 𝜖 R𝐶𝑖 (𝐻𝑊 )𝑇 , to integrate information of each pixel from all frames. In short, one scan focuses on scanning with time dependency along frames and the other focuses on scanning each pixel along the time axis. (6) Local Scan : Local Scanning [63] overcomes limitations of scanning methods in ViM and VMamba by preserving local dependencies in images through distinct local windows. This technique maintains the global context of the image without compromise. The authors suggest using 7 7 and 2 2 local windows to capture the local context while alternating the scan direction. Vertical and horizontal scans with direction flipping are used to grasp the global context of image tokens. (7) Efficient 2D Scan : Efficient Scan 2D (ES2D) [102] emphasizes efficient image scanning by skipping scan patches with step size 𝑝. It partitions selected spatial dimension features into 𝑚 and 𝑛 using sine and cosine functions to determine the patch location. The entire operation is mathematically expressed in Equation 15. 𝑠𝑐𝑎𝑛 [:, 𝑚 :: 𝑝, 𝑛 :: 𝑝], O𝑖 (15) (cid:8) O𝑖 (cid:9)4 𝑖=1 𝑆𝑆2𝐷 ((cid:8)O𝑖 (cid:9)4 𝑖=1 𝑚𝑒𝑟𝑔𝑒 O𝑖, ), [:, 𝑚 :: 𝑝, 𝑛 :: 𝑝] Table 1. Summary of Scanning Mechanisms and Associated Models"
        },
        {
            "title": "Models",
            "content": "S. Bansal et al."
        },
        {
            "title": "BiDirectional Scan",
            "content": "Selective Scan 2D(SS2D) Spatiotemporal Selective Scan Zigzag Scan Local Scan Efficient 2D Scan Continuous 2D Scan Tri-orientated Spatial Mamba (TOM) - Three Directional Scan Pixelwise and Patchwise Scan Omnidirectional Selective Scan Hierarchical Scan Multi-Head Scan Multi-Path Scan 3D BiDirectional Scan Bidirectional Slice Scan (BSS) Vision Mamba [164], MamMIL [41], Motion-Guided Dual-Camera Tracker [157] CAMS-Net [75] VMamba [88], MedMamba [150], P-Mamba [148], Weak-Mamba-UNet [130], VM-UNET-V2 [156], LightM-UNet [85], HC-Mamba [139], H-vmunet [134], Mamba-HUNet [110], UltraLight VM-UNet [135], VM-UNet [109] VMambaMorph [131], Mamba-UNet [132], Swin-UMamba [86], Semi-Mamba-UNet [92], MambaMIR [62], VM-DDPM [70] Vivim [146] ZigMa [61] LocalMamba [63] , FreqMamba [161] EfficientVMamba [102], FusionMamba [103] PlainMamba [141] SegMamba [138] LKM-UNet [128] VmambaIR [112] Motion Mamba[158] MHS-VM [68] RSMamba[13] VideoMamba[78] SliceMamba [40] 𝑤𝑖𝑡ℎ (𝑚, 𝑛) = ( (cid:22) 1 + 1 2 𝑠𝑖𝑛( 𝜋 2 (𝑖 2)) (cid:23) , (cid:22) 1 2 + 1 2 𝑐𝑜𝑠 ( 𝜋 2 (𝑖 2)) (cid:23) ) (8) Multi-Path Scan : Multi-path scanning mechanism [13] incorporates reverse, forward and random shuffling paths. simple approach to combine the information flow from different paths would be averaging. However, the objective is to selectively activate the information from each path. Consequently, gating mechanism is designed to manage the information flow from various paths. (9) Omnidirectional Selective Scan : There are two information streams 01, 02 R𝐵𝐶 𝐻 𝑊 , serving as inputs for the Omnidirectional Selective Scan (OSS) [112]. In the first stream, bidirectional scanning is performed both longitudinally and transversely on 01 to capture planar two-dimensional feature information. The second stream is refined by depthwise convolution and SiLU activation [25], capturing detailed patterns. These two streams are then fused within OSS, merging refined features with complementary information. After passing through 1 1 convolution, the output of OSS, F𝑂𝑆𝑆 R𝐵𝐶 𝐻 𝑊 provides detailed input representation, improving feature extraction and modeling capabilities. Comprehensive Survey of Mamba Architectures for Medical Image Analysis (10) Hierarchical Scan : Hierarchical Temporal Mamba (HTM) [158] block processes compressed latent representations 𝑧 of dimensions (T , B, C) using hierarchical scanning methodology. Initially, 𝑧 undergoes linear projection to produce representations 𝑥 and 𝑧 of dimension 𝐸. set of scans 𝐾 and memory matrices are applied, where each scan involves 1𝐷 convolution and linear projections to derive transformed outputs. These outputs are combined through linear projection to produce the final transformed representations zHTM , efficiently capturing diverse motion densities and minimizing computational overhead. (11) Multi-Head Scan : Multi-headed scanning [68] processes an input embedding map X𝑙 1 with shape (B, , , C) to produce an output embedding map X𝑙 of the same shape through linear transformations and concatenations. Initially, the input is reshaped to (B, C, , ) and then processed with 𝑛 scan heads. Each scan head projects the input onto subspace and routes it through 𝐾 scan routes, involving specific transformations, activation, and rearrangement. Scan route outputs are concatenated and fused using the coefficient of variation and summation, followed by ReLU. The combined results from all scan heads are concatenated along the channel axis and optionally projected back to the original number of channels. Finally, the output is reshaped to (B, , , C) and returned. (12) Other Scanning Mechanisms: Segmamba [138] uses TOM, which stands for Tri-Orientated Spatial Mamba Block. This block employs scanning along three dimensions: height, width and channels and three directions: forward direction, reverse direction, and inter-slice direction. LKM-UNet [128] applies two-scan strategy where the first scan involves pixel-level scanning. Each pixel is scanned unidirectionally (forward) and max-pooled into single image. Subsequently, another round of unidirectional scanning (forward) is performed on these pooled images. Bidirectional Slice Scan (BSS) [40] proposed in Slice Mamba plays crucial role in its architecture. Firstly the spatial dimension height and width are split into 𝑚 and 𝑛 𝑖 R𝑚𝑊 𝐶 𝑖 = 1, 2, ... 𝐻 windows separately resulting in the sequence of shape 𝐹 ℎ = {fℎ 𝑚 } horizontally and 𝐹 𝑣 = {f 𝑣 𝑛 } sequence of shape vertically. Scanning is applied both in horizontal (both forward and backward) and vertical (both forward and backward) direction. The optimal size of 𝑚 and 𝑛 are chosen through adaptive slice search which uses Neural Architecture Search (NAS) for each layer of mamba blocks using single path one shot. 𝑗 R𝐻 𝑛𝐶 𝑗 = 1, 2, ...𝑊"
        },
        {
            "title": "3.3.1 Lightweight and Efficient.",
            "content": "Lightweight and efficient models are designed to be smaller, quicker, and use fewer resources, while maintaining good performance. Table 2 compares the above mentioned lightweight models based on Giga Floating-point Operations Per Second (GFLOPs), number of parameters, and FPS Frames Per Second (FPS). These metrics provide detailed evaluation of each models computational efficiency, complexity and speed respectively. Light Mamba UNet (LightM UNet) proposed by Liao et al. [85] combines Mamba and UNet architectures in lightweight framework which aims to tackle computational challenges in real world medical environment. The Residual Vision Mamba (RVM) layer is proposed to improve SSM for deep semantic feature extraction from images in pure Mamba-based manner. S. Bansal et al. LightM-UNet overcomes the existing state-of-the-art methods with only 1.09 parameters and 267.19 GFLOPs. UltraLight Vision Mamba UNet (UltraLight VM-UNet) introduced by Wu et al. [135] is lightweight vision Mamba model. An excellent performance is achieved by Parallel vision Mamba (PVM) method that is used for efficiently processing deep features with lowest computational complexity, while maintaining the overall number of processing channels constant. PVM is primarily composed using Mamba combined with residual connections and adjustment factors. This combination allows traditional Mamba to capture remote spatial relations without introducing additional parameters and computational complexity. Several comparisons have been done with the state-of-the-art lightweight models across three public skin lesion datasets. In this analysis, the UltraLight VM-UNet shows the competitive performance with only 0.049M parameters and 0.060 GFLOPs. UltraLight VM-UNet parameters are 87.84% lower than the parameters of LightM-UNet. Table 2. Comparison of Lightweight Models based on Parameters, GFLOPs, and FPS"
        },
        {
            "title": "Models",
            "content": "Params(M) GFLOPs FPS LightM UNet [85] UltraLight VM-UNet [135] MUCM-Net [149] LightCF-Net [69] MiM-ISTD [15] 1.09 0.049 0.071 to 0.139 1.52 4.76 267.19 0.060 0.055 to 0.064 3.25 3. - - - 33 - GFLOPS = (Number of Floating-Point Operations) / (Elapsed Time in Seconds) / (109) - denotes lower is better, - denotes higher is better MUCM-Net proposed by Yuan et al. [149] is an efficient model which combines Mamba StateSpace Models with UCM-Net architecture to improve segmentation and feature learning. In this model, Mamba-UCM is optimized for mobile deployment, providing high accuracy with minimal computational requirements (approximately 0.055 0.064𝐺𝐹 𝐿𝑂𝑃𝑠 and 0.071 0.139𝑀 parameters). LightCF-Net proposed by Ji et al. [69] is novel and efficient lightweight architecture used as long-range context fusion network for real-time polyp segmentation. new FAEncoder module has been developed, merging Large Kernel Attention (LKA) with channel attention mechanisms to extract deep representational features of polyps and uncover long-range relationships. Furthermore, novel Visual Attention Mamba Module (VAM) module has been integrated into skip connections to capture extensive contextual dependencies from encoder-extracted features, prioritizing crucial information and mitigating background noise interference using attention mechanisms. Evaluation is done on four polyp segmentation datasets which showcases its operational efficiency and segmentation accuracy compared to leading lightweight polyp segmentation networks. While this method achieves good results in segmentation task, the complexity of the medical environment and limitations of labeled data during training pose challenges, preventing it from fully meeting the demands of medical applications. The model features 1.52 parameters, operates at 3.25 GFLOPs, and achieves frame rate of 33 FPS. Chen et al. [15] proposed MiM-ISTD for Infrared Small Target Detection (ISTD). It utilizes Mamba to effectively capture both local and global information from the given data. This approach ensures higher efficiency with very less computational costs. Experiments conducted on NUAA-ISTD and IRSTD-1k datasets, where MiM-ISTD demonstrated superior performance in terms of both accuracy and efficiency compared to other related methods. MiM-ISTD is ten times faster than the state-of-the-art method and it reduced GPU memory usage by 73.4% during high resolution image testing. Comprehensive Survey of Mamba Architectures for Medical Image Analysis"
        },
        {
            "title": "3.4 Techniques and Adaptations",
            "content": "In this section, we explore techniques and adaptations for Mamba architectures such as weakly supervised, semi-supervised ands self-supervised approaches. These approaches are used in scenarios where data annotations are absent, partially present or inconsistent, and used to improve the models ability to learn from unstructured or incomplete or semi-structured data."
        },
        {
            "title": "3.4.1 Weakly Supervised Learning.",
            "content": "Weakly Supervised Learning (WSL) uses small amount of correctly labeled data along with large amount of data with incomplete labels. Instead of having detailed labels for each data, this approach works with data that has noisy and partial labels. Wang et al. [130] proposes WeakMamba-UNet, WSL strategy which incorporates three different architectures but with the same symmetrical encoder-decoder networks. The three networks consist of CNN based U-Net, known for capturing local features; Swin Transformer-based SwinUNet, which excels in understanding global context and VMamba-based Mamba-UNet, for efficiently capturing long-range dependency. The proposed WSL framework employs multi-view cross-supervised learning approach for scribble-based supervised medical image segmentation. The work introduced partial cross-entropy to leverage only the scribble annotations during the training of the network. The overall loss is composed of the scribble-based partial cross-entropy loss and the dense-signal pseudo label dice-coefficient loss. The network demonstrates promising results in segmentation tasks achieving an accuracy of 99.63% on MRI Cardiac [9] dataset."
        },
        {
            "title": "3.4.2 Semi-Supervised Learning.",
            "content": "Semi-supervised learning uses fewer amount of labeled data and larger amount of unlabeled data during training. Ma et al. [92] proposes Semi-Mamba-UNet, semi-supervised learning framework integrated with mamba-based segmentation network. It supports the complementary strengths of Mamba-UNet and UNet which uses labeled and large amount of unlabeled data respectively. Pixel-Level Contrastive learning strategy is proposed to increase feature learning from pair of projectors. network via pseudo labeling is used to train other network using pixel-level cross-supervised learning strategy. The overall loss is the sum of three components: supervision loss, self-supervised contrastive loss, and semi-supervised loss. Semi-Mamba-UNet was tested on ACDC MRI Cardiac Dataset [9] using 5% and 10% labeled data, along with remaining unlabeled data. With 5% labeled data, it achieved dice coefficient of 0.8386, accuracy rate of 0.9936, sensitivity of 0.7992, and specificity of 0.9483. Hausdorff Distance (HD) was 6.2139, and Average Surface Distance (ASD) was 1.6406. When trained with 10% labeled data, the models performance improved, achieving dice coefficient of 0.9114, accuracy rate of 0.9964, sensitivity of 0.9146, specificity of 0.9821, HD of 3.9124, and ASD of 1.1698."
        },
        {
            "title": "3.4.3 Self Supervised Learning.",
            "content": "Self-supervised learning is method where the model learns from unlabeled data by creating its own labels. Instead of depending on external manually annotated labels, the model generates these labels internally based on data itself. Nasiri et al. [99] proposes Vim4Path which uses Vision Mamba within DINO by Caron et al. [10] for representation learning. The research aims to explore and adapt ViM to use in SSL. DINO, well known self-supervised learning framework employs self-distillation in teacher-student setup, where both networks share identical architectures but have different parameters. The study compares two architectures in both slide-level and patch-level classification tasks using Camelyon16 dataset for benchmarking purposes. CLAM framework by Lu et al. [90] is employed to compare various architectures for slide-level classification. It utilizes attention-based multiple-instance learning, enabling the identification of sub-regions within slides that are most indicative of the slide-level label. This approach allows the model to focus on the S. Bansal et al. most relavent features without need for detailed annotation. Zhou et al. [163] proposes MGI, new multimodal model which uses both genetic and image data. self-supervised contrastive learning strategy is used during pre-training to align visual encoder and gene encoder on paired genetic and image data, allowing visual encoder to learn relevant features from genetic perspective. This process is followed by lightweight multimodal attention fusion decoder that integrates image and gene data. Using mamba to extract features from gene data addresses the issues faced by previous encoders in capturing long-range dependencies in long gene sequences. Tang et al. [116] proposed MambaMIM, 3D-UNet-based architecture for self-supervised learning. MambaMIM integrates 3D sparse convolution with Mamba blocks in the encoder. It introduces Selective Structured State Space Sequence Token-interpolation (S6T), which generates interpolated vectors between consecutive input vectors, 𝑦𝑖 and 𝑦𝑖+1. The interpolated sequence is processed through linear layer before the decoder block. In the decoder, the unmasked sequence from the encoder is also interpolated using S6T, while sparse features are filled with learnable tokens, transforming them into dense features for upsampling. Contrastive learning is technique within self-supervised learning that focuses on learning representations by comparing pairs of data samples. Yang et al. [142] proposes Contrastive Masked Vim autoencoder (CMViM) which is the efficient representation learning method for 3D multimodal data. To reconstruct 3D masked multi-modal data, it incorporates ViM into mask encoder so that it can effectively capture long range dependencies in 3D medical data. To align multi-modal representations, it introduces intramodal and intermodal contrastive learning mechanisms. Intramodal contrastive learning module is introduced to capture discriminative features within the same modality. Inter-modal contrastive learning mechanism is introduced to align cross-modality representations from different modalities. CMViM outperforms other state-of-the-art methods in Alzheimers disease diagnosis. Ma et al. [92] introduces pixel level contrastive learning strategy for feature learning maximization from representations that are projected in pair of projectors."
        },
        {
            "title": "3.4.4 Multimodal Learning.",
            "content": "Xie et al. [137] introduced U-Net-like architecture called Fusion Mamba which is designed for encoding multimodal images and then decoding them. Fusion Mamba shown in Figure 16 fuses features from two different source images. The encoder part incorporates Dynamic Vision State Space (DVSS) block which integrates Efficient State Space Module. ESSM uses an Efficient 2D Selective Scan (ES2D) and Efficient Channel Attention (ECA) [129]. ECA involves adaptive average pooling followed by 1D convolution layer and sigmoid activation function. ECA enables SSM to learn channel representations effectively. The output from DVSS is residually added to the input through Learnable Descriptive Convolution (LDC). LDC uses learnable matrix (𝑚) combined with an identity matrix. dot product is performed between the modified learnable matrix and the input map. The result is multiplied in the next step to produce the output map which allows SSM to capture the textural features present in each modality. Dynamic Feature Fusion Module (DFFM) is used to fuse features between different modalities. The DFFM includes Dynamic Feature Enhancement Module (DFEM) for each modality which takes two different feature modes as input which is followed by performing coarse-grained fusion and then passing output through the Cross Modality Fusion Mamba Module (CMFM). The Cross Modality Fusion Mamba (CMFM) block starts by applying layer normalization, followed by linear layer and depthwise convolution for each modality. The resulting representations from each modality are multiplied and then residually connected with outputs of depth-wise convolutions to resemble gating mechanism. These representations are further processed through an Efficient Selective 2D Scan (ES2D). The outputs from layer normalization are then passed through linear layer where it is multiplied with representations from ES2D and residually added. Finally, merged features are passed through Comprehensive Survey of Mamba Architectures for Medical Image Analysis Fig. 16. Visualization of Fusion Mamba Framework [137] and its Main Components another linear layer, followed by the ECA. The decoder contains patch-expanding block followed by two DVSS blocks. The combined features from DFFM act as skip connections for the decoder. The patch-expanding block can either use transposed convolution or bilinear upsampling of features. Finally, the fused image is obtained from each image modality. Zhou et al. [163] proposed MGI which is multimodal approach used for aligning images and gene modalities using pre-training approach similar to CLIP [105]. Mamba-based encoder is employed for both images and genes. Contrastive loss is applied between modality embeddings to generate matrix similar to CLIP. For task-specific learning, the model integrates an attention integration module and mask output module. In the attention integration module, alignment between modules is achieved through self-attention block on the gene modality, gene-to-image attention block, MLP layer and then an image-to-gene attention block to combine modalities. Similarly, in the mask output module gene-to-image attention block is followed by MLP and dot product is applied between image and gene modalities to generate mask image which is trained using Dice loss. Fang et al. introduced GFE-Mamba [39], model employing multi-stage training strategy. First, they train 3D GAN to convert MRI images into PET images. Afterward, latent representations from MRI and PET are concatenated with tabular data, where continuous features pass through linear layer, and discrete features pass through an embedding layer, before being fed into Mamba classifier. The representations from Mamba classifier and MRI and PET latents from 3D GAN undergo pixel-level bi-cross attention operation, applying attention operation between Mamba classifiers representation and MRI/PET latents respectively. GFE-Mamba integrates U-Net-based architecture, with the encoder in the 3D U-Net utilizing three downsample blocks, each composed of convolution block, ReLU activation, max pooling, and group normalization, with encoder channel sizes of 64, 128, and 256. The middle layer features ViT blocks to enhance representation capture, outperforming traditional residual layers used in conventional 3D GANs. The decoder consists of upsample blocks, each with transposed convolution and ReLU activation, with channel sizes of S. Bansal et al. 256, 128, and 64. The 3D GAN is trained using discriminator, similar to the standard GAN training method with real and fake images. The pixel-level bi-cross attention combines cross-attention from MRI and PET latent spaces with Mamba representations. First, attention between MRI latents and Mamba is calculated with RMS normalization, followed by attention from the PET block, again with RMS normalization, and the final output is classified into binary classes."
        },
        {
            "title": "3.5.1 Medical Image Segmentation.",
            "content": "Medical image segmentation is technique used to identify and extract specific Regions Of Interest (ROI) from medical images such as tumors, lesions, tissues or organs. The objective is to divide the image into areas that share similar features including color, texture, brightness, and contrast. Table 3 outlines the overview of segmentation models, parameters, descriptions and the code availability. Figure 17 shows the workflow of Mamba-based models in medical image segmentation task. Some notable research works on mamba-based models for medical image segmentation includes: Fig. 17. Work Flow of Segmentation Task Xing et al. [138] proposed an architecture called SegMamba along with new dataset comprising 500 3D computed tomography scans with expert annotations. The proposed model features an architecture similar to the transformer-based U-Net. Initially, it employs depthwise convolution with kernel size of 7 7 7, padding of 3 3 3, and stride of 2 2 2. On the encoder side, the model incorporates series of TS-Mamba blocks and downsample blocks with residual connections Comprehensive Survey of Mamba Architectures for Medical Image Analysis mirrored on the upsample side. TS-Mamba block has transformer-like architecture that uses gated spatial convolution to capture spatial relationships between 3D features. This is followed by the Tri-orientated Spatial Mamba (TOM) block, which replaces the traditional multi-headed attention layers found in transformers. TOM applies independent Mamba models to forward, backward, and inter-slice feature interactions respectively. The computed values are then processed by multilayer perceptron to enrich feature representation. Each TS-Mamba block ends with downsampling, doubling the number of channels while halving the height, width, and depth of the features. These outputs are then residually connected to the decoder side of the network. The decoder uses convolution transpose networks to upsample the image. SegMamba was evaluated on three publicly available datasets: the BraTS2023 dataset, AIIB2023 dataset, and CRC-500. It demonstrated superior performance across all metrics compared to other transformer-based U-Net models. Table 3. Overview of Segmentation Models"
        },
        {
            "title": "Models",
            "content": "Params(M) Core Mamba Description"
        },
        {
            "title": "Code",
            "content": "SegMamba [138] H-Vmunet [134] LKM-UNet [128] Mamba-UNet [132] Weak Mamba UNet [130] LightM-UNet [85] UltraLight-VM UNet [135] T-Mamba [53] HC-Mamba [139] Semi-Mamba-UNet [92] ViM-UNet [2] U-Mamba [93] Mamba-HUNet [110] UU-Mamba [121] CAMS-Net [75] MUCM-Net [149] - 8.97 - - - 1.87 0.049 1.04 13.88 - 18 - - - - 0.047 H-VSS"
        },
        {
            "title": "TSMamba Multiple Tumor analysis",
            "content": "Bi-Mamba 2D & 3D Multiple organ analysis VMamba Multiple organ analysis VMamba Analysis of heart with WSL framework"
        },
        {
            "title": "RVM\nMamba\nTim\nMamba\nVMamba Analysis of heart with SSL framework",
            "content": "(cid:33) (cid:33) (cid:33) (cid:33) (cid:33) (cid:33) Tumor analysis in chest X-ray images Skin lesion analysis with light weight networks (cid:33) (cid:33) 2D & 3D tooth analysis (cid:37) Skin lesion analysis (cid:33) (cid:33) ViM (cid:33) Mamba VMamba Analysis of Multiple Sclerosis Legion in brain (cid:37) Mamba Heart Analysis integrated with SAM optimizer (cid:37) (cid:37) (cid:33) Analysis of microscopic organs(cell) 2D & 3D Multiple organ analysis NC-Mamba Heart region segmentation"
        },
        {
            "title": "Mamba",
            "content": "Wu et al. [134] introduced High-order Vision Mamba UNet (H-Vmunet), which is also similar to transformer-based U-Net architecture. This work addresses challenges in skin lesion analysis (ISIC2017), spleen cancer detection, and polyp detection (CVC-ClinicDB dataset). The authors introduced H-VSS module, modified version of VSS, replacing it with the H-SS2D module. In H-SS2D module, the input is projected into 2𝐶 channels. The split of these channels depends on the order of the H-SS2D module. H1-SS2D splits channels by half, while H3-SS2D splits them into 𝐶/4 and 7𝐶/4 for SS2D and Local-SS2D, respectively. Local-SS2D, submodule of H-VSS, applies convolution 2D and SS2D operations to each half of channels. Each step in H-VSS involves multiplication from the projection to Local-SS2D and scanning with SS2D layer. The number of scans increases with the order of the selective scan. Additionally, SS2D employs scan expansion, performed in four different scan order directions: top-left to bottom-right, top-right to bottom-left, bottom-left to top-right and bottom-right to top-left. SS2D uses S6 SSM as described in Mamba. The overall model architecture is based on U-Net, starting with two convolutional layers, followed by H-VSS modules of order 2, 3, 4, and 5 from layers 3 to 6, each followed by convolutional layer with kernel size of 3. The residual layers of this U-Net uses shared weights of Channel Attention Bridge (CAB) and Spatial Attention Bridge (SAB) for information fusion between the encoder and S. Bansal et al. decoder. On the decoder side, the model starts with H-VSS blocks of order 5, 4, 3, and 2 from layers 1 to 3, with the addition of features from SAB and CAB. The features are upsampled using bilinear interpolation. H-Vmunet demonstrates superior performance across metrics in the ISIC2017, Spleen, and CVC-ClinicDB datasets. Wang et al. [128] devised Large Kernel Vision Mamba UNet (LKM-UNet) for both 2D and 3D segmentation tasks. The encoder of LKM-UNet uses depthwise convolution which is followed by LM block and downsampling block. LM block consists of pixel-wise SSM and patch-level SSM, each followed by bidirectional mamba block at each SSM level. The bidirectional Mamba block includes forward and backward SSMs which is similar to ViM. The representations from pixel-level SSMs are processed by bidirectional SSM, which is then pooled, and passed to patch-level SSM, that employs another bidirectional Mamba block. This allows features from each sub-window to interact with patch-level SSMs, followed by bidirectional Mamba. The pixel-level SSM representations are then residually added to unpooled representations from patch-level SSMs. After LM block, downsampling occurs, doubling the feature space while halving the height, width, and depth. In the decoder, LKM-UNet uses convolution-based upsampling with residual connections from corresponding encoder layers. Figure 18 shows the architecture of LKM-UNet and its components LM Block and bidirectional Mamba. LKM-UNet outperforms traditional UNet and mamba-based methods on 3D multi-organ segmentation with Abdomen CT dataset and 2D segmentation with 110 MRI cases in Abdomen MR dataset. Fig. 18. Architecture of LKM-UNet [128] Wang et al. [132] presented Mamba-UNet, model that adapts purely ViM-based U-Net architecture with linear embedding and VSS blocks. This network includes encoder, decoder, and bottleneck layers. Initially, medical images, defined by height and width, are converted into 1D sequence. An embedding layer transforms them into size denoted by C, followed by VSS blocks. During downsampling, the feature space is doubled while height and width are halved. The bottleneck layer consists of two VSS blocks without downsampling. In decoder, features are Comprehensive Survey of Mamba Architectures for Medical Image Analysis reconstructed by upsampling, reducing feature space by half and doubling the height and width. Skip connections are employed between corresponding encoder and decoder layers. Mamba-UNet demonstrates superior performance compared to attention-based and transformer-based U-Net models on datasets such as the Automated Cardiac Diagnosis Challenge, which includes MRI scans of 100 patients, and the Synapse Multi-Organ Segmentation Challenge, which contains 30 abdominal CT scans with expert annotations. Wang et al. [130] proposed Weak Mamba UNet, which combines CNNs, ViTs, and Visual Mamba for WSL. This model uses weighted mixture of the three architectures, each having symmetric architecture with identical input and output sizes. For CNN-based model, the authors utilized classical U-Net with convolution layers for downsampling. ViT-based UNet and Visual mamba-based UNet follow similar symmetric architecture. Weak Mamba UNet uses scribble annotations from images, and all networks are evaluated using pseudo-label loss function and scribble loss. The authors also experimented with using three instances of mamba based UNet, ViT-based UNet, and CNN-based UNet separately. However, the combined approach using CNN, ViT, and mamba-based UNet outperformed the other configurations. Weak Mamba UNet showed superior performance compared to other ViT-based UNet models, such as Swin-UNet, in WSL settings, particularly excelling on ACDC dataset. Liao et al. [85] introduced LightM-UNet, which is an extension of VSS module by incorporating Residual Vision Mamba (RVM) Layer. RVM includes layer normalization, followed by another layer normalization, and then VSS module, with residual connections and scaling. This layer addresses long-term spatial dependencies. Each encoder block in LightM-UNet contains RVM and downsampling layers. The encoder block representations are used for residual connections in corresponding decoder blocks. The decoder blocks consist of depthwise convolution with residual connections scaled similar to RVM layer, and bilinear transformation is used to restore predictions to original resolution. LightM-UNet achieves segmentation with significantly lower number of parameters compared to transformer-based models, reducing computation cost and parameter count by 21 and 116, respectively. It surpasses state-of-the-art performance on two publicly available datasets, LiTs and Montgomery&Shenzhen. Additionally, the authors demonstrated that LightM-UNet can be extended to 3D, evaluated on the LiTs dataset, and still surpass state-of-the-art performance. Wu et al. [135] devised UltraLight-VM UNet, which employs Parallel Vision Mamba (PVM). Each PVM consists of 𝑁 independent Mamba layers, with input representations divided into 𝑁 parts along channel dimension (𝐶/𝑁 ). The authors provided four configurations with 1, 2, and 4 Mamba layers, respectively. Similar to LightM-UNet, Mamba layers in UltraLight-VM UNet use scaled residual connections. All representations from Mamba layers are concatenated, followed by layer normalization and projection layer, which allows for customizable downsampling of feature space. Unlike LightM-UNet, which uses VSS, UltraLight-VM UNet employs classical Mamba layers, reducing number of parameters and enhancing performance. The encoder layers consist of convolution layers for stages 1-3 and PVM layers for stages 4-6. Residual connections incorporate Spatial Attention Bridge (SAB) and Channel Attention Bridge (CAB) layers, differing from LightMUNet. The decoder blocks mirror the encoder structure and use bilinear transformation to restore predictions to the original resolution. UltraLight-VM UNet achieves state-of-the-art performance compared to transformer-based methods for skin lesion segmentation on the ISIC2017, ISIC2018, and PH datasets. Hao et al. [53] presented an architecture called T-Mamba. It is modification of the DenseVNet architecture, incorporating sequential layers of convolutional networks, batch normalization, and the Tim layer. In T-Mamba, Tim layer flattens the input, applies shared position embedding (similar to sinusoidal position embedding), normalizes representations using layer norm, and passes them S. Bansal et al. to modified ViM block, which includes forward and backward SSM similar to Vision Mamba. Additionally, Tim has branch that converts input into Fourier domain, extracts features using learnable weight parameters, implements band-pass filtering, and then applies inverse Fourier transform. The Fourier domain representations are multiplied with max-pooled non-SSM part of Mamba representation. Tim gated selection network is used to select features from forward SSM, backward SSM, and Fourier domain, respectively. The encoder part, inspired by DenseVNet, connects and stacks each layer repeatedly, followed by downsampling up to three layers. Subsequent encoder layers are upsampled by 2, 2, and 4, respectively, leading to prediction head for segmentation. T-Mamba achieves state-of-the-art performance on 3D Tooth CBCT dataset, outperforming both convolution-based U-Net and transformer-based U-Net models. Xu et al. [139] proposed HC-Mamba, which uses HC-SSM block composed of series of HC-Conv blocks and modified Mamba block using HC-Conv. The input is split into two parts: the first part undergoes series of HC-Conv blocks, where HC-Conv involves two-step convolution process with dilated convolution followed by depth-wise separable convolution. The second part uses HC-Convolution block instead of the traditional convolution block in Mamba. The representations from both parts are concatenated and shuffled randomly. HC-Mamba employs patch embedding layer similar to ViT and applies series of HC-SSM blocks with patch merging. Patch merging doubles feature space and halves height and width of representations at each layer iteration in the encoder. Residual connections are applied in the decoder, originating from patch embeddings. The decoder consists of HC-Mamba blocks with patch merging until the final projection. HC-Mamba outperforms convolution-based U-Net, transformer-based U-Net, and mamba based U-Net models in skin lesion segmentation on ISIC2017 and ISIC2018 datasets, as well as in multi-organ segmentation on Synapse dataset. Ma et al. [92] introduced Semi-Mamba-UNet, which leverages self-supervised learning methodology by implementing two networks: Mamba-UNet and UNet. Mamba-UNet incorporates Visual Mamba blocks similar to UNet. In semi-supervised learning setting, Semi-Mamba-UNet employs three combinations of loss functions: contrastive Loss, semi-supervised loss, and supervised loss. Pixel-level contrastive loss is calculated using projection layer in the networks final representations. The loss is determined by taking the 𝐿2 norm of representations on both labelled and unlabeled data, normalized by the number of input data (𝑁 ). Semi-supervised loss is applied to the networks final layers using cross-entropy loss and dice loss, with pseudo labels predicted by the network. Supervised loss uses same loss functions as semi-supervised loss but only with labelled data. The authors experimented with different combinations of U-Net and transformer-based models, finding that mamba-based models use fewer parameters and offer better performance. SemiMamba-UNet achieves state-of-the-art performance on ACDC and MICCAI datasets. Compared to other attention-based SSL frameworks, they found that using 10% of labeled data is optimal. Archit et al. [2] utilized Vision Mamba for cell structure segmentation, achieving superior performance compared to both transformer-based and Convolution-based approaches on U-Net framework. However, the ViM-UNet is not modified version of existing Vision Mamba architectures but rather implements Vision Mamba along with corresponding downsample and upsample layers to create U-Net architecture. The authors demonstrated superior performance on datasets such as LIVECell and CREMI. The paper proposed two variations of ViM-UNet: one with smaller size, termed Tiny with 18 million parameters, and another with larger size, termed Large with 39 million parameters. Ma et al. [93] proposes network called U-Mamba for 2D and 3D biomedical image segmentation. It features an novel hybrid CNN-SSM architecture that effectively captures localized fine-grained features and long-range dependencies in images. UMamba encoder is composed of building blocks, consisting of two successive residual blocks, followed by SSM Mamba, while decoder is composed Comprehensive Survey of Mamba Architectures for Medical Image Analysis of residual blocks with skip connections. U-Mamba incorporates self-configuring feature from nnUNet, and number of network blocks is automatically determined based on the datasets. Additionally, the authors propose two variants of UMamba, namely, U-Mamba_Bot, which integrates U-Mamba block only in the bottleneck, and U-Mamba_Enc, which utilizes Mamba blocks across all encoder layers. These architectures have been evaluated across four diverse datasets: Abdomen CT, Abdomen MRI, Endoscopy Images, and Microscopy Images. Figure 19 illustrates the architecture of U-mamba and its components. Fig. 19. Architecture of U-Mamba [93] Mamba-HUNet proposed by Sanjid et al. [110] is novel architecture that has been designed for reliable and effective medical picture segmentation. Combining the long-range dependency modeling capabilities of SSMs with local feature extraction skills of CNNs, this model combines the best aspects of both Mamba-UNet and simplified version of the Hierarchical Upsampling Network (HUNet). The design greatly improves processing efficiency by mutating HUNet into Mamba-HUNet and optimizing it into more efficient variety without sacrificing performance. To enable efficient processing, input grayscale pictures are divided into patches and converted into 1𝐷 sequences, method influenced by ViT and Mamba. Mamba-HUNet preserves spatial integrity while extracting hierarchical features by using patch merging layers and VSS blocks. The effectiveness of Mamba-HUNet is evaluated through experiments on MRI scans, particularly in multiple sclerosis lesion segmentation. Tsai et al. [121] introduced UU-Mamba, segmentation model that incorporates an uncertaintyaware loss function. This loss function is combination of three components: dice coefficient loss (region-based), cross-entropy loss (distribution-based), and focal loss (pixel-based), all combined with learnable parameter sigma. To address the issue of narrow minima in the uncertainty loss, SAM optimization introduces hyperparameter epsilon to ensure the minima are flatter, thus preventing the network from overly fitting complicated representations. U-Mamba utilizes both UNet-like architecture and Mamba blocks, enabling parameter-efficient computations and achieving S. Bansal et al. superior performance. Each U-Mamba block includes two residual connections: one with Mamba block followed by convolution block and Instance Normalization (IN). Skip connections link features from encoder to decoder. UU-Mamba demonstrates superior performance on ACDC dataset compared to transformer-based UNet models. Khan et al. [75] proposed convolution and attention-free segmentation mamba based model named CAMS-Net. The authors proposed NC-Mamba block, which differs from normal Mamba block by not using 1𝐷 convolution. Instead, representations are directly projected into SSM with SiLU activation function. The linearly interconnected factorized mamba block manipulates the input representation using linear layer with SiLU. When using LIFM block, representations of height and width are combined into single dimension and later reshaped to their original form. Therefore, LIFM block operates on two levels: Mamba Channel Aggregator (MCA), which works on channel level, and Mamba Spatial Aggregator (MSA), which operates on the combined height and width dimension. CS-IF blocks employs combination of MCA and MSA blocks in each unit. Forward and backward scans are subsequently applied with shared weights. The initial part of segmentation network includes an MCA block combined with sinusoidal position embedding. Downsampling in encoder is performed using 2 2 average pooling. CS-IF blocks are employed in bottleneck and last encoder part. The decoder also uses MCA blocks, with upsampling performed by 2 2 bi-linear transformation. Overall, this paper aimed to present convolution and attention-free methodology rather than achieving state-of-the-art results. However, CAMS-Net demonstrates superior or second-best performance compared to convolution and transformer-based methods on CMRRecon MICCAI-2023 dataset. Yuan et al. [149] presented U-Net-based segmentation network tailored for skin lesion segmentation. This architecture differs from other mamba based U-Nets by omitting SAB and CAB blocks in skip connections. It features six encoder layers followed by downsampling with convolution layers, and six decoder layers with upsampling through convolution. The initial and final layers of both encoder and decoder utilize convolution layers. Aside from these initial and final layers, MUCMNet integrates UCM-Net blocks and Mamba blocks by adding their representations together. There are two MUCM-Net configurations: (1) MUCM-Net(1-patch pipeline): This version has single Mamba block where the feature passes directly through the Mamba block and is then added to the UCM-Net block. (2) Another configuration splits channels into parts, each processed by separate Mamba blocks, which are then concatenated and passed to the UCM-Net block, with being 2, 4, or 8. MUCM-Net was evaluated on ISIC-2017 and ISIC-2018 datasets, demonstrating performance that matched or exceeded that of UCM-Net and other mamba-based architectures such as LightM-UNet by Liao et al. [85] and VM-UNet by Ruan et al. [109]."
        },
        {
            "title": "3.5.2 Medical Image Classification.",
            "content": "Classification in medical imaging refers to categorizing images into different classes, such as distinguishing between benign and malignant lesions, or identifying different types of diseases. Figure 20 shows the workflow of Mamba-based models in medical image classification task. Table 4 summarizes classification models and their parameters, along with descriptions and information regarding code availability. Some mamba-based architectures applied in this domain include: Gong et al. [44] introduced nnMamba method for medical image classification that integrates strengths of CNNs and SSMs. Traditional CNNs lack in capturing long-range dependencies due to their local receptive fields, while transformers, though capable of modeling global context, are computationally intensive for 3D medical images. nnMamba addresses these challenges by incorporating Mamba-In-Convolution with Channel-Spatial Siamese (MICCSS) block, which effectively Comprehensive Survey of Mamba Architectures for Medical Image Analysis Fig. 20. Work Flow of Classification Task combines long-range dependency modeling and local feature extraction . nnMamba introduces channel-scaling and channel-sequential learning. Channel-scaling adjusts the importance of different feature channels, while channel-sequential learning processes features in sequential manner to capture complex dependencies. nnMambas backbone architecture leverages MICCSS block to maintain computational efficiency while achieving superior performance in 3D medical imaging tasks. Table 4. Overview of Classification Models"
        },
        {
            "title": "Models",
            "content": "Params(M) Core Mamba Description"
        },
        {
            "title": "Code",
            "content": "nnMamba [44] MedMamba [150] Vim4Path [99] Microscopic-Mamba [165] 15.55 15.2 7 1.59 Res-Mamba Alzheimer prediction and Landmark detection (cid:33) (cid:33) VMamba Multiple disease classification (cid:33) (cid:33)"
        },
        {
            "title": "ViM\nPEVM",
            "content": "Yue et al. [150] propose MedMamba, novel method for medical image classification that leverages modern SSM, particularly inspired by VMamba. MedMamba addresses limitations of CNN and transformers by combining local feature extraction capability of convolutional layers with long-range dependency modeling of SSMs, thereby achieving efficient and effective classification of medical images. The architecture of MedMamba includes patch embedding layer, SS-ConvSSM blocks, and patch merging layers. Figure 21 shows the architecture of MedMamba where it incorporates SS-Conv-SSM blocks. These blocks use dual-branch approach to separately process and merge features from convolutional and SSM pathways, incorporating 2D-selective scan (SS2D) for comprehensive feature extraction. Extensive experimental analysis on multiple datasets demonstrate that MedMamba surpasses existing methods in terms of accuracy, establishing new baseline for medical image classification. The study demonstrates the potential of SSM-based models in medical applications along with the outline of future research directions which includes optimization and integration of explainable AI. S. Bansal et al. Fig. 21. Architecture of MedMamba [150] Nasiri et al. [99] used Vision Mamba (ViM) in its architecture for learning representations in histopathology images. Vim4path performs self supervised learning (SSL) within DINO framework [10]. It extracts image patches from Whole Slide Images (WSIs) and trains ViM encoder using DINO without the availability of labels. ViM is compared with ViT in slide-level and patch-level classification tasks in Camelyon16 dataset. It demonstrates superior performance of ViM at smaller scales and competitive performance at larger scales. The analysis shows that ViM mimics the pathologists diagnostic workflow better than ViTs. The paper highlights potential of ViM in practical diagnostic applications and its effectiveness in less computation scenarios which establishes it as promising tool for computational pathology. Microscopic-Mamba proposed by Zou et al. [165] consists of following components in sequence: patch embedding, four blocks of Hybrid-Conv-SSM each followed by patch merging block, Global pooling, 1 1 convolution (PW convolution) and finally fully connected layer for classification. Hybrid-Conv-SSM block splits the representation channel wise which is then passed through two different batches. First branch is the Conv branch which contains depth wise and point wise convolution. Second branch is SSM branch which contains Parallel Efficient Vision Mamba (PEVM) block. Microscopic-Mamba has three variants in terms of parameters such as tiny, small, base. Tiny has 4.32M parameters, small has 4.97M parameters and base has 8.37M parameters. MicroscopicMamba achieves state-of-the-art results on five different classification datasets outperforming transformer-based methods, mamba-based methods and other hybrid approaches."
        },
        {
            "title": "3.5.3 Medical Image Restoration/ Reconstruction.",
            "content": "Restoration is an application in medical imaging which is used to improve the quality of images that may be corrupted or distorted due to factors such as noise, low resolution and blurring. Reconstruction is mathematical process that converts raw medical data into target image. Table 5 Comprehensive Survey of Mamba Architectures for Medical Image Analysis Fig. 22. Work Flow of Restoration/Reconstruction Task provides an overview of Mamba based models applied in medical image restoration/reconstruction along with parameters, descriptions and the availability of code. Figure 22 shows the workflow of Mamba-based models in medical image restoration/reconstruction task. Key Mamba-based architectures used in this application are explained as follows. Table 5. Overview of Restoration Models"
        },
        {
            "title": "Models",
            "content": "Params(M) Core Mamba Description"
        },
        {
            "title": "Code",
            "content": "MambaMIR [62] FDVM-Net [162] MambaDFuse [83] FusionMamba [137] - - - - Mamba MRI-CT image reconstruction Mamba Mamba DVSS (cid:33) (cid:33) Endoscopic image reconstruction Fused reconstruction with MRI,CT,PET images (cid:33) Dynamic Feature Enhancement and Image fusion (cid:33) Zheng et al. [162] presents FDVM-Net which is designed for endoscopic exposure correction and it leverages frequency-domain reconstruction to achieve high-quality image restoration. To capture spatial features and global dependencies, FDVM-Net combines Mamba and convolutional blocks. This combination serves as the foundation for dual-path network architecture, which separates processing of the images phase and amplitude information. Furthermore, FDVM-Net applies frequency domain cross-attention module to improve the performance of network. Huang et al. [62] introduced MambaMIR for medical image reconstruction and uncertainty estimation. Figure 23 shows MambaMIR which incorporates an Arbitrary-Masked State Space (AMSS) block with Monte Carlo dropout. AMSS is inspired by Mamba which comprises of AMS6 block, gating linear layer, depth-wise convolution layer, and SiLU activation function. Residual connections used in MambaMIR ensures efficient and stable training. This helps MambaMIR to achieve robust performance in medical image reconstruction tasks while providing reliability in reconstruction using uncertainty estimation. S. Bansal et al. Fig. 23. Architecture of MambaMIR [62] MambaDFuse introduced by Li et al. [83] incorporates Multimodal Mamba blocks which effectively merges features from different modality into unified representation. This helps to obtain an informative fused image as output. MambaDFuse is dual-phase model that integrates complementary information from diverse imaging modalities into single image. MambaDFuse utilizes the combined strengths of convolutional layers and Mamba blocks to capture spectrum of features. It ranges from low-level details to high-level semantic information. Xie et al. [137] proposed FusionMamba to address limitations of channel redundancy and limited local enhancement capabilities in existing image fusion methods. FusionMamba uses dynamic convolution and channel attention mechanisms to enhance the models ability to capture global context and local features within the images. FusionMamba leverages strengths of Mamba, dynamic feature enhancement and cross-modal fusion techniques. FusionMamba explores internal features and relationships between different image modalities. This results in dynamically enhanced texture details, better differences between modalities and improved ability to capture correlations while simultaneously reducing redundant information."
        },
        {
            "title": "3.5.4 Medical Image Registration.",
            "content": "Image registration is the process involves aligning two or more images of the same scene captured at different times, from different viewpoints, or by different sensors. In medical image registration, this typically involves aligning fixed volume, such as an intraoperative CT scan with moving volume, such as preoperative MRI to accurately overlay anatomical structures from different imaging modalities. This step is crucial for diagnosis, treatment planning, and monitoring in medical applications. There are two main types of image registration based on transformation models: linear registration and deformable (non-rigid) registration. Linear registration includes methods such as rigid and affine registration. Rigid registration aligns images using only translations and rotations. It assumes that shapes and sizes of objects in images remain constant. Affine registration includes translation, rotation, scaling, and shearing. This method can handle uniform changes in Comprehensive Survey of Mamba Architectures for Medical Image Analysis scale and non-orthogonal angles. Deformable registration refers to the alignment of images using transformation models that allow for local deformations. Fig. 24. Work Flow of Registration Task Deformable registration can handle complicated transformations which makes it appropriate for aligning MR and CT scans when the patients anatomy may have changed between scans. Table 6 describes registration models and their parameters along with information on their descriptions and code availability. Figure 24 shows the workflow of Mamba-based models in medical image registration. Some mamba-based models relevant to this area include: Table 6. Overview of Registration Models"
        },
        {
            "title": "Models",
            "content": "Params(M) Core Mamba MambaMorph [50] VMambaMorph [131] 7.59 9."
        },
        {
            "title": "Description",
            "content": "Code 3D MRI-CT registration (cid:33) 3D MRI-CT registration (cid:33) Guo et al. [50] proposed multi-modality deformable registration framework designed to process the medical MR-CT deformable registration dataset. Inspired by Chen et al. [12], which utilizes Swin Transformers in encoder of registration module, MambaMorph instead uses Mamba blocks to capture long-range spatial relationships while optimizing memory utilization. Figure 25 shows that before registration module, simple fine-grained feature extractor (U-Net with one downsampling step) is utilized to maximize retention of local information. Wang et al. [131] transformed the 2D VSS of VMamba into 3D volumetric feature processing framework. This hybrid architecture combined VMamba with CNN (U-Net), to accurately estimate the deformation field. To address the challenges posed by complex motion and diverse structures in image registration, VMambaMorph introduces recursive registration framework. Furthermore, it employs weight-sharing fine-grained feature extractor to extract features across divergent S. Bansal et al. Fig. 25. Architecture of MambaMorph [50] volumes. MambaMorphs inadequacy in fully exploiting visual features from complex motion and diverse structures of images or volumes has been surpassed by VMambaMorph."
        },
        {
            "title": "3.5.5 Miscellaneous.",
            "content": "Beyond the primary tasks discussed above, Mamba-based models have been employed in range of other medical imaging applications. Table 7 outlines models and their parameters along with their descriptions and code availability. Notable mamba-based research papers in this field include: Xie et al. [136] introduced Prompt-Mamba for polyp segmentation in colonoscopy images. Polyp segmentation is crucial for early cancer detection but it is challenging due to variations in size, shape, and color. Existing models struggle with irregular shapes, small samples and generalization to unseen data. Prompt-Mamba addresses these limitations by combining Vision-Mamba which is known for its robust image feature extraction with prompt technology for improved generalization. The lightweight architecture includes an image encoder with ViM layers, prompt encoder and mask decoder. It also uses combination of Focalloss and Diceloss functions to effectively handle imbalanced data and measure segmentation quality. Prompt-Mamba surpasses existing state-ofthe-art methods by an average of 5% across six datasets. It demonstrates exceptional performance on both validation and unseen datasets. Yang et al. [147] proposed ClinicalMamba, specialized language model based on Mamba. ClinicalMamba is pre-trained on longitudinal clinical notes from MIMIC-III to achieve notable speed and performance benchmarks. It surpasses established clinical language models as well as large language models such as GPT-4 in longitudinal clinical tasks. The researchers pre-trained ClinicalMamba using causal language modeling objective on dataset of de-identified clinical notes from MIMIC-III. This dataset encompassed 82,178 hospital visits with all associated notes aggregated longitudinally. The model leverages selective SSM with context window of 16k tokens, effectively covering over 98% of the visits within the dataset. By specializing in medical domain and pre-training on longitudinal data, ClinicalMamba captures unique characteristics of Comprehensive Survey of Mamba Architectures for Medical Image Analysis Table 7. Overview of Miscellaneous Models"
        },
        {
            "title": "Models",
            "content": "Params(M) Core Module Description"
        },
        {
            "title": "Code",
            "content": "P-Mamba [148] Prompt-Mamba [136] ClinicalMamba [147] Vivim [146] VM-DDPM [70] MD-Dose [43] Motion-guided dualcamera tracker [157] 52.77 102 - - 15.2 - -"
        },
        {
            "title": "ViM\nViM\nMamba",
            "content": "(cid:37) Pediatric cardiac imaging (cid:37) Polyp analysis with prompt technologies Language model with prompt based fine-tuning (cid:33) (cid:33) (cid:37) Synthesis of MRI-X ray images Predicts dosage distribution for tumor patients (cid:33) (cid:33) ST-Mamba Video analysis of multiple tumor VMamba Mamba BiMamba Real-time endoscope tip tracking clinical narratives. The paper highlights potential applications in tasks such as cohort selection for clinical trials and ICD coding, where the model can interpret clinical narratives and extract relevant information. Ye et al. [148] devised P-Mamba, novel deep learning model to address pediatric echocardiographic left ventricular segmentation. It tackles two key issues: computational efficiency and noise interference. The model employs dual-branch architecture. ViM encoder branch focuses on efficiency by capturing global dependencies in the image. DWT-based PMD encoder branch, leveraging technique originally used for image de-noising (DWT-based PMD) block, specifically targets noise suppression while preserving local features crucial for segmentation. This branch utilizes an anisotropic diffusion equation to achieve this balance. Finally, decoders within the model (SegHead and FCNHead) upsample features from both branches to generate segmentation masks. Fu et al. [43] introduced MD-Dose for predicting radiation therapy dose distributions using Mamba-based diffusion model. MD-Dose applies Mamba within diffusion model for both denoising and encoding tasks. The forward process of MD-Dose involves adding Gaussian noise to dose distribution maps, while the reverse process uses noise predictor to reconstruct the original maps from noise. Mamba encoder in MD-Dose is used to extract structural information from CT images and then integrating it with noise prediction to enhance the localization of dose regions in Planning Target Volume (PTV) and Organs At Risk (OAR). The experiments conducted on dataset of 300 thoracic tumor patients demonstrate that MD-Dose outperforms existing models in both accuracy and inference speed. The efficiency of architecture highlights its reduced parameter count and faster inference time compared to other methods which makes it significant advancement in automated radiotherapy planning. Yang et al. [146] proposed Vivim, novel deep learning framework shown in Figure 26 for medical video object segmentation. Vivim addresses challenges of capturing spatiotemporal information and handling long-range dependencies in medical videos. The core of Vivim lies in its Video Vision Mamba (ST-Mamba) module, which leverages transformer architecture with selective scan mechanism inspired by SSMs. ST-Mamba efficiently captures long-range temporal dependencies within video sequences. The hierarchical encoder utilizes temporal Mamba blocks to extract multiscale feature sequences, while lightweight CNN-based decoder head fuses these features and predicts segmentation masks. Vivim offers several advantages. Firstly, it integrates SSMs into medical video object segmentation for the first time, achieving superior performance and speed compared to existing methods. Secondly, the boundary-aware constraint improves the models ability to discriminate between ambiguous tissues, frequent challenge in medical videos. The paper also introduces new benchmark dataset VTUS which includes annotated thyroid ultrasound videos. Extensive evaluations demonstrate that Vivim outperforms state-of-the-art segmentation models on both VTUS and polyp colonoscopy datasets. It highlights its effectiveness in medical video object segmentation. S. Bansal et al. Fig. 26. Architecture of Vivim [146] Ju et al. [70] proposed VM-DDPM, U-Net based architecture which performs forward noising and reverse denoising process as proposed in Denoising Diffusion Probabilistic Models (DDPM) [58]. The authors introduced SSLayer in this architecture which contains residual block and modification of VSS block called Multi-level State Space (MSS) block. MSS block incorporates time step embedding for diffusion process and Cross-Scan Module (CSM) which is modification of S6 scan. To adapt the model for synthesis of new images, the image patches are randomly shuffled and then scanned, similar to SS2D in VSS. This scanning process requires zero parameters. Zhang et al. [157] introduced low-cost dual-camera tracker for endoscopy skill evaluation in mechanical simulators. The tracker solves problems faced by existing methods such as lack of consistency and high cost. It achieves accurate and reliable 3D endoscope tip position feedback. The core of the system lies in cross-Camera Mutual Template (CMT) strategy that utilizes information from both cameras to maintain tracking consistency. Motion-Guided Prediction Head (MMH) based on Mamba integrates historical motion data with visual tracking. It enhances its performance in scenarios with occlusions and distortions. Furthermore, the tracker includes Vision-Motion integrator that combines motion information with visual features for improved 3D localization. This integration is done by mechanism named Multi-KV cross-attention. The paper also introduces Selective Scan SSMs within the tracking which leads to superior performance compared to traditional SSMs. To evaluate the trackers effectiveness, researchers developed customized mechanical gastric simulator with calibrated cameras and an annotated dataset. The combination of CMT, MMH, Vision-Motion, and SSMs demonstrates the trackers potential for improving endoscopy training in mechanical simulators. Comprehensive Survey of Mamba Architectures for Medical Image Analysis"
        },
        {
            "title": "4 Datasets\nApplication in medical domain depends on a wide variety of datasets to improve research and\ninnovation in the field of medical imaging, diagnostics and treatment planning. These datasets\ncover multiple medical specialties and imaging modalities. In this section, we offer a comprehensive\noverview of some of the most significant datasets utilized in Mamba-based models across various\nmedical fields.",
            "content": "i. BraTS2023 Dataset [4, 27, 96] consists of diverse collection of structural MRI sequences from pediatric patients diagnosed with high-grade glioma, gathered across multiple institutions. ii. AIIB2023 [77, 98] establishes new benchmark for AI-based solutions in airway modeling, specifically targeting lung fibrosis disease. It includes 120 high-resolution CT (HRCT) scans, each with rare expert annotations. iii. CRC-500 Dataset [138] is comprehensive and large-scale resource designed for 3D colorectal cancer segmentation, comprising 500 meticulously annotated 3D Computed Tomography (CT) scans. iv. ISIC2017 [19] challenge offers comprehensive dataset for lesion image analysis, including approximately 2,000 training images to facilitate development across three key components of the task. v. ISIC2018 [18, 122] comprises over 12,500 images distributed across three tasks, each designed to address distinct aspects of dermoscopic image analysis. vi. Spleen Dataset [28, 36] comprises 61 portal-venous phase CT scans from patients undergoing chemotherapy for liver metastases, focusing on the spleen as the target ROI. vii. CVC-ClinicDB Dataset [8] features 612 high-resolution images extracted from 31 colonoscopy sequences, each with resolution of 384 288 pixels. viii. 3D Abdomen CT Dataset [32] comprises of 2,300 CT scans and is part of the MICCAI 2022 FLARE Challenge, focused on segmentation of 13 abdominal organs including pancreas,liver, kidneys, spleen, and more. ix. 2D Abdomen MRI Dataset [67] also known as AMOS, is large-scale dataset designed for abdominal organ segmentation, offering 100 MRI scans and 500 CT scans. x. Endoscopy Dataset [35] or ROBUST-MIS, originates from Robotic Instrument Segmentation Sub-Challenge of the MICCAI 2017 EndoVis Challenge. xi. Microscopy Dataset [94] was part of the NeurIPS 2022 Cell Segmentation Challenge, focusing on cell segmentation across various microscopy images sourced from over 20 biology laboratories and covering more than 50 distinct biological experiments. xii. ACDC MRI Cardiac Dataset [9] comprises 150 exams divided into five groups, including four pathological and one healthy group, with each patients data including weight, height, and diastolic and systolic phase information. xiii. Synapse Multi-Organ Abdominal CT Dataset includes 50 abdominal CT scans collected under Institutional Review Board (IRB) supervision from ventral hernia and colorectal cancer studies during the portal venous contrast phase, with detailed labeling of thirteen abdominal organs verified by radiologist. xiv. PH2 Dataset [95] includes 200 dermoscopic images of melanocytic lesions, with 80 common nevi, 40 melanomas, and 80 atypical nevi acquired at the Dermatology Service of Hospital Pedro Hispano in Portugal. Synapse Multi-Organ Abdominal CT Dataset S. Bansal et al. xv. The LiTS Dataset [37], organized in conjunction with IEEE ISBI 2017 and MICCAI 20172018, features diverse image dataset that includes both primary and secondary liver tumors of varying sizes and appearances. xvi. Montgomery and Shenzhen Dataset [66] consists of 138 frontal chest X-rays from Montgomery County, including 80 normal and 58 tuberculosis (TB) cases. Shenzhen dataset, compiled with Shenzhen No.3 Peoples Hospital and Guangdong Medical College, includes 336 cases with TB manifestations and 326 normal cases. xvii. The Brain MRI Multiple Sclerosis Dataset [91] of 2D FLAIR MRI has scans from 60 patients represents valuable resource for studying Multiple Sclerosis (MS) in clinical setting. CMRRecon MICCAI-2023 [29] challenge data has cardiac Magnetic Resonance Imaging (MRI) data from 300 subjects, widely used for both reconstruction and segmentation tasks. xviii. Alzheimers Disease Neuroimaging Initiative (ADNI) [65, 84] is an ongoing, multisite observational study involving healthy older adults, individuals with mild cognitive impairment (MCI), and those with Alzheimers disease. xix. Otoscopy Dataset [152] comprises images from 41,056 patients diagnosed in the Department of Otolaryngology at the Peoples Hospital of Shenzhen Baoan District, captured with standard endoscopes and digital endovision camera systems. xx. PathMNIST [143, 144] consists of 100,000 non-overlapping image patches from hematoxylin and eosin-stained histological images and an additional test set of 7,180 patches from clinical center for predicting survival outcomes in colorectal cancer. xxi. CAMELYON16 [5, 7] includes 270 Whole-Slide Images (WSIs) for training (159 normal and 111 with tumors) and 129 WSIs for testing, aimed at detecting metastases in lymph node sections using eosin (H&E) staining and hematoxylin. xxii. The Colorectal Cancer Histopathology Dataset [89] developed by International Collaboration on Cancer Reporting (ICCR), provides pathology reports on colorectal cancer surgical specimens. xxiii. SynthRAD [50] evaluates methods across different MRI sequences and acquisition settings with 60 MRI, 20 CT and 10 CBCT scans for train, test, and validation respectively, along with 540 paired MRI-CT and 540 CBCT-CT sets, and includes mask for evaluation with target information for validation and test sets kept confidential. xxiv. The FastMRI Dataset [151], consisting of 584 3D knee MRI scans, was utilized for the experiments. To ensure data quality, 20 central coronal slices were selected from each case and cropped to 320320 pixels. The train, test and validation split is in 7:2:1 ratio. The provided emulated single-coil data served as the ground truth for single-coil complex-value reconstruction. xxv. Low-Dose CT Image and Projection Dataset [97] were divided into chest and abdomen subsets. The chest subset consisted of non-contrast scans for pulmonary nodule screening, while the abdomen subset included contrast-enhanced scans for metastatic liver lesion detection. Each subset contained 40 cases, split into training and testing sets. Sparse-view sinograms were created using torch-radon in fan-beam CT geometry with 736 detectors and 60 projection views. The reconstructed image resolution was 512512 pixels. Table 8 provides detailed summary of various datasets used in medical imaging, specifying tasks performed and targeted areas, which includes both anatomical organs and surgical instruments. It highlights diversity of dataset in medical applications, offering insight into how different challenges in medical diagnostics and treatment are addressed through targeted imaging and analysis. Comprehensive Survey of Mamba Architectures for Medical Image Analysis Table 8. Overview of Datasets, Tasks Performed, and Targeted Areas in Medical Imaging and Analysis"
        },
        {
            "title": "Targeted Area",
            "content": "BraTS2023 [4, 27, 96] AIIB2023 [77, 98] CRC-500 Dataset [138] ISIC2017 [19] ISIC2018 [18, 122] MICCAI-2023 [31] Spleen [28, 36] CVC-ClinicDB [8] 3D Abdomen CT dataset [32] 2D Abdomen MRI Dataset [67]"
        },
        {
            "title": "Segmentation\nSegmentation\nSegmentation\nSegmentation",
            "content": "Endoscopy Dataset [35] Microscopy Dataset [94] ACDC MRI Cardiac Dataset [9] Synapse Multi-Organ Abdominal CT Dataset 𝑃𝐻 2 Dataset [95] LiTS Dataset [37] Montgomery and Shenzhen Dataset[66] Brain MRI Multiple Sclerosis Dataset[91] Alzheimers Disease Neuroimaging Classification (ADNI) Dataset [65, 84] Otoscopy [152] PathMNIST [143, 144] Camelyon16 [5, 7] Colorectal Cancer Histopathology Dataset [89] SR-Reg (SynthRAD Registration) dataset [50] FastMRI Knee Dataset [151] Low-Dose CT Image and Projection Datasets [97]"
        },
        {
            "title": "Registration",
            "content": "Brain Lungs Colon, Rectum Skin Skin Heart Spleen Intestine Liver, Kidney, Spleen, etc. Kidney, Gallbladder, Esophagus, Liver, Pancreas, Adrenal gland, etc. Surgical Instruments Cell Heart Pancreas, Spleen , Liver"
        },
        {
            "title": "Brain",
            "content": "Ear Colon Pathology Lymph node Colon, Rectum"
        },
        {
            "title": "Brain",
            "content": "Restoration/ Reconstruction Knee Restoration/ Reconstruction Head, Chest, Abdomen"
        },
        {
            "title": "5.1 Segmentation",
            "content": "In this section, we present Table 9 which provides comparison of segmentation models tested on various datasets, focusing on key performance metrics such as Hausdorff Distance (HD95, HD), Intersection over Union (IoU), Mean Intersection over Union (mIoU), Dice Similarity Coefficient (DSC), Sensitivity (SE), Specificity (SP), Accuracy (ACC), Normalized Surface Distance (NSD), F1 Score (F1), Average Surface Distance (ASD), and Precision (Pre). This comparison highlights the strengths and weaknesses of each mamba-based models which helps to evaluate their effectiveness in specific segmentation tasks. Table 9. Quantitative Comparison of Various Segmentation Models across Different Datasets Model Datasets HD95 IoU DSC SE SP ACC mIOU NSD F1 HD ASD Pre S. Bansal et al. 91.32 - 48.02 90.68 94.03 89. 91.72 95.71 90.87 90.91 89.40 92.65 88.18 89.25 81.56 - - - - - - - - - 88.97 98.23 96.42 93.30 99.92 99.82 87.68 99.21 98.13 90.56 98.31 96.80 96.42 99.92 99.87 88.03 99.40 98.33 90.53 97.90 96.46 86.80 97.81 95.58 93.45 96.06 95.21 - - - - - - - - - - - - 95.17 97.47 86.99 79.27 87.90 97.08 94.84 - - - - - SegMamba [138] BraTS2023 [4, 27, 96] AIIB202 [77, 98] CRC-500 dataset [138] 3.56 - 30.89 - 88.59 - - - - - - - - - - - - 26.32 - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - H-vmunet(VSS) [134] H-vmunet(H-VSS) [134] ISIC2017 [19] Spleen [28, 36] CVC-ClinicDB [8] ISIC2017 [19] Spleen [28, 36] CVC-ClinicDB [8] UltraLight VM-UNet [135] ISIC2017 [19] HC-Mamba [139] U-Mamba_Bot [93] U-Mamba_Enc [93] LKM-UNet [128] Swin-UMamba [86] Swin-UMamba [86] (Using Decoder) ISIC2018 [18, 122] 𝑃𝐻 2 Dataset [95] ISIC2017 [19] ISIC2018 [18, 122] Synapse Multi-Organ Abdominal CT Dataset 3D Abdomen CT dataset [32] 2D Abdomen MRI dataset [67] Endoscopy dataset [35] Microscopy dataset [94] 3D Abdomen CT dataset [32] 2D Abdomen MRI dataset [67] Endoscopy dataset [35] Microscopy dataset [94] 3D Abdomen CT dataset [32] 2D Abdomen MRI dataset [67] 2D Abdomen MRI Dataset [67] Endoscopy Dataset [35] Microscopy Dataset [94] 2D Abdomen MRI Dataset [67] Endoscopy Dataset [35] Microscopy Dataset [94] LightM-UNet [85] LiTS Dataset [37] Montgomery&Shenzhen Dataset [66] UU-Mamba [121] ACDC MRI Cardiac Dataset[9] Mamba-HUNet [110] Vm-Unet [109] Mamba-UNet [132] Brain MRI Multiple Sclerosis Dataset [91] Brain MRI Multiple Sclerosis Dataset [91] ACDC MRI Cardiac Dataset[9] Synapse Multi-Organ Abdominal CT Dataset Weak-Mamba-Unet [130] ACDC MRI Cardiac Dataset[9] Semi-Mamba-UNet [92] ACDC MRI Cardiac Dataset[9] 5% Labeled Data 10%Labeled Data CAMS-Net [75] MICCAI-2023 [31] MUCM-Net [149] ISIC2017 [19] ISIC2018 [18, 122] - denotes lower is better, - denotes higher is better 86.8308.08 75.8810.51 65.4030.08 - 86.3809.08 76.2510.82 63.0330.67 - 86.82 77.35 77.60 67.67 - 77.05 67.83 - 84.58 96.17 92.79 - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 2.25 85.36 92.87 92.94 98.65 2.9 80. 87.49 91.35 95.31 - - - - - - - - 86.98 54.05 92.81 - 92.89 98.59 99.72 66.03 98.90 99.75 - - - - - - 91.71 93.09 99.20 99.63 83.86 91.14 79.92 94.83 99.36 91.46 98.21 99. 84.84 - - - 91.85 90.95 90.14 98.57 96.97 90.46 97.72 96. - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 90.4908.21 82.8510.74 66.9230.50 - - - - 53.8928.17 89.8009.21 83.2710.87 64.5131.04 - - - - 56.0727.84 90.02 83.80 84.21 69.22 - 83.76 69.33 - - - - - 58.06 - - 59.82 - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 2.46 0.76 92.75 24.47 6.47 64.52 3.95 0.88 90. 6.21 3.91 6.51 - - 1.64 88.61 1.16 90.88 - - - - - - - - - - - - - - - - - - - - - 77.48 92.74 - - - - - - - - - - - Comprehensive Survey of Mamba Architectures for Medical Image Analysis"
        },
        {
            "title": "5.2 Classification",
            "content": "In this section, we present Table 10 which shows detailed comparison of Mamba-based models that perform classification task across different datasets. The metrics containes Accuracy (Acc), F1-score and Area Under the Curve (AUC), where higher values indicate better performance. This table helps to assess the effectiveness of each Mamba models in achieving accurate and reliable classification results in medical imaging. Table 10. Quantitative Comparison of Mamba-based Models on Classification Tasks"
        },
        {
            "title": "Model",
            "content": "nnMamba"
        },
        {
            "title": "MedMamba",
            "content": "Vim4Path Microscopic-Mamba"
        },
        {
            "title": "Datasets",
            "content": "ADNI (NS VS AD) ADNI (sMCI VS pMCI)"
        },
        {
            "title": "Otoscopy\nPathMNIST",
            "content": "Camelyon16 RPE Data [34] MHIST [133] SARS [52] Tissue MNIST [144] Med FM Colon [126] Acc 89.41 75.79 89.45 0.951 94. - - - - - F1 88.68 56.55 85.15 - 92.47 - - - - - AUC 95.81 76.84 0.9889 0.997 98.85 98.17 94.17 99.48 93.50 99.64 - denotes lower is better, - denotes higher is better"
        },
        {
            "title": "5.3 Registration",
            "content": "In this section, we provide Table 11 that presents quantitative comparison of Mamba-based image registration methods on the SR-Reg testing set, highlighting the superior performance of VMambaMorph over MambaMorph. Table 11. Quantitative Comparison of Mamba-based Image Registration Models on Testing set of SR-Reg (SynthRAD Registration) [50] Dataset"
        },
        {
            "title": "Methods",
            "content": "Dice(%) HD95(mm) 𝑃 𝐽𝜙 0(%) Time Memory Parameter MambaMorph[50] 82.71 1.45 VMambaMorph[131] 82.94 2.01 2.00 0.22 1.35 0.18 0.34 0.02 1.04 0. - denotes lower is better, - denotes higher is better (s) 0.27 0.10 (Gb) 7.60 3.25 (Mb) 7.59 9.64 The percentage of mean Dice coefficients (Dice) and the 95th percentile Hausdorff distance (HD95) are used to measure registration accuracy. Dice coefficient, represented as percentage (Dice % ), evaluates the overlap between two sets, with higher values indicating better alignment. HD95 (mm) measures the 95th percentile of distances between boundaries of two objects, with lower values indicating closer alignment. To evaluate the diffeomorphic property of the deformation field, the percentage of non-positive Jacobian determinants 𝑃 𝐽𝜙 0(%) is used. This metric indicates the percentage of points where the Jacobian determinant of transformation is non-positive which S. Bansal et al. reflects folding or overlap in the deformation. Additionally, computation evaluation includes time taken for the registration process in seconds, memory used in gigabytes and size of model parameters in megabytes. These metrics provide comprehensive overview of performance, efficiency, and resource utilization of Mamba-based registration methods."
        },
        {
            "title": "6.1 Limitations",
            "content": "(1) Spatial Information Loss: 1D scanning mechanism of Mamba when used with 2D or 3D data can sometimes lose spatial information. Further research is required across multiple dimensions to improve handling of spatial information. (2) Model Understanding: There are some explanations on how Mamba and Mamba-based models work well in NLP but it is still unclear why it performs well in visual tasks. More research in this field is needed to understand its learning process. (3) Causality Issues: Adapting Mambas scanning mechanism is difficult for non-causal visual data. Bidirectional scanning mechanism helps to an extent but there are still problems due to scanning in just one direction. (4) Parameter Initialization: Finding the best way to initialize parameters of Mamba to avoid instability during training remains challenge, especially when models parameter increases."
        },
        {
            "title": "6.2 Emerging Areas\nMamba 2:",
            "content": "Fig. 27. (a) Relationship between SSMs and Attention through Structured Matrices , (b) Structured State Space Duality [22] Comprehensive Survey of Mamba Architectures for Medical Image Analysis Mamba 2 proposed by Dao et al. [22] reduces the gap between recurrent nature of SSM and parallel nature of attention in transformers. Figure 27 shows Structured State Space Duality which explains the relationship between SSM and attention using structures matrices to describe their connection. The recurrent state space equations are converted into matrix form by expanding recurrence equations commonly used in SSM. This approach resembles the kernel representation utilized in Gu et al. [47]. The conversion of the recurrent equations of Gu et al. [45] involves matrix representations that have been discretized using the Zero Order Hold (ZOH) method, as previously discussed in this work. Both the linear (recurrent) and quadratic (attention) forms are unified under the framework known as State Space Dual (SSD) Layer. In the recurrent form, the parameter 𝐴 from Mamba is simplified from diagonal to scalar times identity structure, while larger head dimensions 𝑃 are employed for this model. The authors introduce 1SS (a) mask, derived from unrolling recurrence equations of SSM, which simplifies the parameter 𝐴 when attention equations are rearranged into linear attention forms. This reduced attention equation is termed Structured Masked Attention (SMA). Using SMA, the authors propose architectures equivalent to Multi-Head Attention (MHA), termed Multi-Head SSM, and Multi-Query Attention, known as Multi-Contact SSM. Additionally, Multi-Contact SSM can be extended to include Group Query Attention. Fig. 28. Comparison of Mamba and Mamba-2 Architecture [22] With the established relationship between transformers and SSM, models now leverage parallelism and equivalent implementations similar to ViT. This approach enables the creation of more expressive models using minimal parameters. Figure 28 compares the sequential block of Mamba and parallel block of Mamba-2 architecture. xLSTM: Extended Long Short-Term Memory (xLSTM) proposed by Beck et al. [6] incorporates sLSTM and mLSTM, sLSTM have proposed new normalized gating mechanism compared to original LSTM and also changed gating mechanism from sigmoid to exponential gating. mLSTM has learnable projection matrix for query, key, value similar to transformer [124], and transforms scalar memory states into matrix-based representations for parallel training. Vision-xLSTM proposed by Alkin et al. [1] employs stacked mLSTM blocks similar to ViT, demonstrating superior performance S. Bansal et al. over its Mamba counterpart (ViM) while maintaining minimal parameter complexity. xLSTM-Unet proposed by Chen et al. [14] utilizes U-Net-based model for segmenting 2D MRI, endoscopy, and microscopy images, outperforming both Mamba-based and Transformer-based models. The emergence of xLSTM highlights its ability to leverage recurrent capabilities of SSM-based architectures for improved performance with reduced computational complexity. Since xLSTM literatures are insufficient we cannot collectively conclude that xLSTM performs better than mambabased models but xLSTMs are emerging from these recurrent-based models."
        },
        {
            "title": "7.3 Future Directions",
            "content": "Fig. 29. Future Directions of Mamba Comprehensive Survey of Mamba Architectures for Medical Image Analysis Mambas computational efficiency, similar to that of CNNs, enables it to perform well even without large-scale datasets, making it strong candidate for downstream tasks, multi-tasks, and pre-trained model adaptations. Figure 29 describes future directions of Mamba and Mamba-based architectures in medical applications. Its design, including the use of SSMs, reduces computational complexity, making mamba particularly suited for processing high-resolution data such as remote sensing images, whole slide images, and extended video sequences. In multimodal settings, however, Mamba faces challenges in uniformly learning features across different data types. Similar to transformers which handle both text and images, Mambas capacity for processing extended sequences offers potential for multimodal learning. To fully leverage this potential, an efficient approach is needed, and Mamba2 aims to address these issues. In-context learning has also become more sophisticated, and Mambas ability to model long-range dependencies makes it promising for improving performance across NLP, CV, and multimodal domains. Mambas selective scanning mechanism requires adaptation for non-causal characteristics of visual data. New scanning techniques are necessary to fully exploit higher-dimensional non-causal visual data. In Mamba 2, the scanning process is made parallelly causal using Semiseparable SMA, which models autoregressively in way similar to decoder-only transformers such as GPT [38, 115] and Llama [26, 30, 120]."
        },
        {
            "title": "References",
            "content": "[1] Benedikt Alkin, Maximilian Beck, Korbinian Pöppel, Sepp Hochreiter, and Johannes Brandstetter. 2024. Vision-LSTM: xLSTM as Generic Vision Backbone. arXiv:2406.04303 [cs.CV] https://arxiv.org/abs/2406.04303 [2] Anwai Archit and Constantin Pape. 2024. ViM-UNet: Vision Mamba for Biomedical Segmentation. arXiv:2404.07705 [cs.CV] [3] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. 2016. Layer Normalization. arXiv:1607.06450 [stat.ML] https://arxiv.org/abs/1607.06450 [4] Spyridon Bakas, Hamed Akbari, Aristeidis Sotiras, Michel Bilello, Martin Rozycki, Justin S. Kirby, John B. Freymann, Keyvan Farahani, and Christos Davatzikos. 2017. Advancing The Cancer Genome Atlas glioma MRI collections with expert segmentation labels and radiomic features. Scientific Data 4 (2017). https://api.semanticscholar.org/CorpusID: 3697707 [5] Peter Bandi. [n. d.]. CAMELYON16 dataset. ([n. d.]). https://camelyon16.grand-challenge.org/data [6] Maximilian Beck, Korbinian Pöppel, Markus Spanring, Andreas Auer, Oleksandra Prudnikova, Michael Kopp, Günter Klambauer, Johannes Brandstetter, and Sepp Hochreiter. 2024. xLSTM: Extended Long Short-Term Memory. arXiv:2405.04517 [cs.LG] https://arxiv.org/abs/2405.04517 [7] Babak Ehteshami Bejnordi et al. 2017. Diagnostic assessment of deep learning algorithms for detection of lymph node metastases in women with breast cancer. JAMA Neurology 318, 22 (12 Dec. 2017), 21992210. https://doi.org/ 10.1001/jama.2017.14585 [8] Jorge Bernal, Javier Sánchez, Gloria Fernández-Esparrach, Debora Gil, Cristina Rodríguez, and Fernando Vilariño. 2015. WM-DOVA maps for accurate polyp highlighting in colonoscopy: Validation vs. saliency maps from physicians. Computerized medical imaging and graphics : the official journal of the Computerized Medical Imaging Society 43 (July 2015), 99111. https://doi.org/10.1016/j.compmedimag.2015.02.007 [9] Olivier Bernard et al. 2018. Deep Learning Techniques for Automatic MRI Cardiac Multi-Structures Segmentation and Diagnosis: Is the Problem Solved? IEEE Transactions on Medical Imaging 37, 11 (2018), 25142525. https: //doi.org/10.1109/TMI.2018. [10] Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. 2021. Emerging Properties in Self-Supervised Vision Transformers. arXiv:2104.14294 [cs.CV] [11] Cheng Chen, Juzheng Miao, Dufan Wu, Zhiling Yan, Sekeun Kim, Jiang Hu, Aoxiao Zhong, Zhengliang Liu, Lichao Sun, Xiang Li, Tianming Liu, Pheng-Ann Heng, and Quanzheng Li. 2023. MA-SAM: Modality-agnostic SAM Adaptation for 3D Medical Image Segmentation. arXiv:2309.08842 [cs.CV] https://arxiv.org/abs/2309.08842 [12] Junyu Chen, Eric C. Frey, Yufan He, William P. Segars, Ye Li, and Yong Du. 2022. TransMorph: Transformer for unsupervised medical image registration. Medical Image Analysis 82 (Nov. 2022), 102615. https://doi.org/10.1016/j. media.2022.102615 [13] Keyan Chen, Bowen Chen, Chenyang Liu, Wenyuan Li, Zhengxia Zou, and Zhenwei Shi. 2024. RSMamba: Remote Sensing Image Classification with State Space Model. arXiv:2403.19654 [cs.CV] https://arxiv.org/abs/2403. [14] Tianrun Chen, Chaotao Ding, Lanyun Zhu, Tao Xu, Deyi Ji, Yan Wang, Ying Zang, and Zejian Li. 2024. xLSTM-UNet can be an Effective 2D & 3D Medical Image Segmentation Backbone with Vision-LSTM (ViL) better than its Mamba S. Bansal et al. Counterpart. arXiv:2407.01530 [eess.IV] https://arxiv.org/abs/2407.01530 [15] Tianxiang Chen, Zhentao Tan, Tao Gong, Qi Chu, Yue Wu, Bin Liu, Jieping Ye, and Nenghai Yu. 2024. MiMISTD: Mamba-in-Mamba for Efficient Infrared Small Target Detection. ArXiv abs/2403.02148 (2024). https://api. semanticscholar.org/CorpusID:268247869 [16] Yu-Cheng Chen, Derek Jin-Ki Hong, Chia-Wei Wu, and Muralidhar Mupparapu. 2019. The use of deep convolutional neural networks in biomedical imaging: review. Journal of Orofacial Sciences 11, 1 (2019), 310. [17] Chirodip Lodh Choudhury, Chandrakanta Mahanty, Raghvendra Kumar, and Brojo Kishore Mishra. 2020. Brain tumor detection and classification using convolutional neural network and deep neural network. In 2020 international conference on computer science, engineering and applications (ICCSEA). IEEE, 14. [18] Noel Codella, Veronica Rotemberg, Philipp Tschandl, M. Emre Celebi, Stephen Dusza, David Gutman, Brian Helba, Aadi Kalloo, Konstantinos Liopyris, Michael Marchetti, Harald Kittler, and Allan Halpern. 2019. Skin Lesion Analysis Toward Melanoma Detection 2018: Challenge Hosted by the International Skin Imaging Collaboration (ISIC). arXiv:1902.03368 [cs.CV] https://arxiv.org/abs/1902.03368 [19] Noel C. F. Codella et al. 2018. Skin lesion analysis toward melanoma detection: challenge at the 2017 International symposium on biomedical imaging (ISBI), hosted by the international skin imaging collaboration (ISIC). In 2018 IEEE 15th International Symposium on Biomedical Imaging (ISBI 2018). 168172. https://doi.org/10.1109/ISBI.2018.8363547 FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning. [20] Tri Dao. 2023. arXiv:2307.08691 [cs.LG] https://arxiv.org/abs/2307.08691 [21] Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. 2022. FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness. arXiv:2205.14135 [cs.LG] https://arxiv.org/abs/2205.14135 [22] Tri Dao and Albert Gu. 2024. Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality. arXiv:2405.21060 [cs.LG] https://arxiv.org/abs/2405.21060 [23] Ruiwen Ding, Kha-Dinh Luong, Erika Rodriguez, Ana Cristina Araujo Lemos Da Silva, and William Hsu. 2024. Combining Graph Neural Network and Mamba to Capture Local and Global Tissue Spatial Relationships in Whole Slide Images. https://api.semanticscholar.org/CorpusID:270357797 [24] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. 2021. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. arXiv:2010.11929 [cs.CV] https: //arxiv.org/abs/2010.11929 [25] Stefan Elfwing, Eiji Uchibe, and Kenji Doya. 2017. Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning. arXiv:1702.03118 [cs.LG] https://arxiv.org/abs/1702.03118 [26] Abhimanyu Dubey et al. 2024. The Llama 3 Herd of Models. ArXiv abs/2407.21783 (2024). https://api.semanticscholar. org/CorpusID:271571434 [27] Anahita Fathi Kazerooni et al. 2024. The Brain Tumor Segmentation in Pediatrics (BraTS-PEDs) Challenge: Focus on Pediatrics (CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs). arXiv:2404.15009 [cs.CV] https://arxiv.org/abs/ 2404.15009 [28] Amber L. Simpson et al. 2019. large annotated medical image dataset for the development and evaluation of segmentation algorithms. arXiv:1902.09063 [cs.CV] https://arxiv.org/abs/1902. [29] Chengyan Wang et al. 2023. CMRxRecon: An open cardiac MRI dataset for the competition of accelerated image reconstruction. arXiv:2309.10836 [cs.CV] https://arxiv.org/abs/2309.10836 [30] Hugo Touvron et al. 2023. Llama 2: Open Foundation and Fine-Tuned Chat Models. arXiv:2307.09288 [cs.CL] https://arxiv.org/abs/2307.09288 [31] Jun Lyu et al. 2024. The state-of-the-art in Cardiac MRI Reconstruction: Results of the CMRxRecon Challenge in MICCAI 2023. arXiv:2404.01082 [eess.IV] https://arxiv.org/abs/2404. [32] Jun Ma et al. 2023. Unleashing the Strengths of Unlabeled Data in Pan-cancer Abdominal Organ Quantification: the FLARE22 Challenge. arXiv preprint arXiv:2308.05862 (2023). [33] Krzysztof Marcin Choromanski et al. 2021. Rethinking Attention with Performers. In International Conference on Learning Representations. https://openreview.net/forum?id=Ua6zuk0WRH [34] Loris Nanni et al. 2016. Texture Descriptors Ensembles Enable Image-Based Classification of Maturation of Human Stem Cell-Derived Retinal Pigmented Epithelium. PLoS ONE 11 (2016). https://api.semanticscholar.org/CorpusID: 18481057 [35] Max Allan et al. 2019. 2017 Robotic Instrument Segmentation Challenge. arXiv:1902.06426 [cs.CV] https://arxiv.org/ abs/1902.06426 [36] Michela Antonelli et al. 2022. The Medical Segmentation Decathlon. Nature Communications 13, 1 (July 2022). https://doi.org/10.1038/s41467-022-30695-9 [37] Patrick Bilic et al. 2023. The Liver Tumor Segmentation Benchmark (LiTS). Medical Image Analysis 84 (2023), 102680. https://doi.org/10.1016/j.media.2022.102680 Comprehensive Survey of Mamba Architectures for Medical Image Analysis [38] Tom B. Brown et al. 2020. Language Models are Few-Shot Learners. arXiv:2005.14165 [cs.CL] https://arxiv.org/abs/ 2005.14165 [39] Zhaojie Fang et al. 2024. GFE-Mamba: Mamba-based AD Multi-modal Progression Assessment via Generative Feature Extraction from MCI. arXiv:2407.15719 [cs.CV] https://arxiv.org/abs/2407.15719 [40] Chao Fan, Hongyuan Yu, Yan Huang, Liang Wang, Zhenghan Yang, and Xibin Jia. 2024. SliceMamba with Neural Architecture Search for Medical Image Segmentation. arXiv:2407.08481 [eess.IV] https://arxiv.org/abs/2407.08481 [41] Zijie Fang, Yifeng Wang, Zhi Wang, Jian Zhang, Xiangyang Ji, and Yongbing Zhang. 2024. MamMIL: Multiple Instance Learning for Whole Slide Images with State Space Models. arXiv:2403.05160 [cs.CV] https://arxiv.org/abs/2403.05160 [42] Daniel Y. Fu, Tri Dao, Khaled K. Saab, Armin W. Thomas, Atri Rudra, and Christopher Ré. 2023. Hungry Hungry Hippos: Towards Language Modeling with State Space Models. arXiv:2212.14052 [cs.LG] https://arxiv.org/abs/2212.14052 [43] Linjie Fu, Xia Li, Xiuding Cai, Yingkai Wang, Xueyao Wang, Yali Shen, and Yu Yao. 2024. MD-Dose: Diffusion Model based on the Mamba for Radiotherapy Dose Prediction. arXiv:2403.08479 [eess.IV] [44] Haifan Gong, Luoyao Kang, Yitao Wang, Xiang Wan, and Haofeng Li. 2024. nnMamba: 3D Biomedical Image Segmentation, Classification and Landmark Detection with State Space Model. arXiv:2402.03526 [cs.CV] [45] Albert Gu and Tri Dao. 2024. Mamba: Linear-Time Sequence Modeling with Selective State Spaces. arXiv:2312.00752 [cs.LG] https://arxiv.org/abs/2312. [46] Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher Ré. 2020. HiPPO: Recurrent Memory with Optimal Polynomial Projections. In Advances in Neural Information Processing Systems, H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (Eds.), Vol. 33. Curran Associates, Inc., 14741487. https://proceedings.neurips.cc/paper_ files/paper/2020/file/102f0bb6efb3a6128a3c750dd16729be-Paper.pdf [47] Albert Gu, Karan Goel, and Christopher Ré. 2021. Efficiently modeling long sequences with structured state spaces. arXiv preprint arXiv:2111.00396 (2021). [48] Albert Gu, Ankit Gupta, Karan Goel, and Christopher Ré. 2022. On the Parameterization and Initialization of Diagonal State Space Models. arXiv:2206.11893 [cs.LG] https://arxiv.org/abs/2206.11893 [49] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher Ré. 2021. Combining Recurrent, Convolutional, and Continuous-time Models with Linear State-Space Layers. arXiv:2110.13985 [cs.LG] https://arxiv.org/abs/2110. [50] Tao Guo, Yinuo Wang, Shihao Shu, Diansheng Chen, Zhouping Tang, Cai Meng, and Xiangzhi Bai. 2024. MambaMorph: Mamba-based Framework for Medical MR-CT Deformable Registration. https://api.semanticscholar.org/CorpusID: 268041636 [51] Ankit Gupta, Albert Gu, and Jonathan Berant. 2022. Diagonal State Spaces are as Effective as Structured State Spaces. arXiv:2203.14343 [cs.LG] https://arxiv.org/abs/2203.14343 [52] Yu H, Mohammed FO, Abdel Hamid M, Yang F, Kassim YM, Mohamed AO, Maude RJ, Ding XC, Owusu EDA, Yerlikaya S, Dittrich S, and Jaeger S. 2023. Patient-level performance evaluation of smartphone-based malaria diagnostic application. Malaria journal 22 (2023). https://doi.org/10.1186/s12936-023-04446-0 [53] Jing Hao, Lei He, and Kuo Feng Hung. 2024. T-Mamba: Frequency-Enhanced Gated Long-Range Dependency for Tooth 3D CBCT Segmentation. arXiv:2404.01065 [cs.CV] [54] Ali Hatamizadeh, Yucheng Tang, Vishwesh Nath, Dong Yang, Andriy Myronenko, Bennett Landman, Holger Roth, and Daguang Xu. 2021. UNETR: Transformers for 3D Medical Image Segmentation. arXiv:2103.10504 [eess.IV] https://arxiv.org/abs/2103.10504 [55] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2015. Deep Residual Learning for Image Recognition. arXiv:1512.03385 [cs.CV] [56] Moein Heidari, Sina Ghorbani Kolahi, Sanaz Karimijafarbigloo, Bobby Azad, Afshin Bozorgpour, Soheila Hatami, Reza Azad, Ali Diba, Ulas Bagci, Dorit Merhof, et al. 2024. Computation-Efficient Era: Comprehensive Survey of State Space Models in Medical Image Analysis. arXiv preprint arXiv:2406.03430 (2024). [57] Dan Hendrycks and Kevin Gimpel. 2023. Gaussian Error Linear Units (GELUs). arXiv:1606.08415 [cs.LG] https: //arxiv.org/abs/1606. [58] Jonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. Denoising Diffusion Probabilistic Models. arXiv:2006.11239 [cs.LG] https://arxiv.org/abs/2006.11239 [59] Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long Short-term Memory. Neural computation 9 (12 1997), 173580. https://doi.org/10.1162/neco.1997.9.8.1735 [60] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022. LoRA: Low-Rank Adaptation of Large Language Models. In International Conference on Learning Representations. https://openreview.net/forum?id=nZeVKeeFYf9 [61] Vincent Tao Hu, Stefan Andreas Baumann, Ming Gui, Olga Grebenkova, Pingchuan Ma, Johannes Fischer, and Björn Ommer. 2024. ZigMa: DiT-style Zigzag Mamba Diffusion Model. arXiv:2403.13802 [cs.CV] https://arxiv.org/abs/ 2403. S. Bansal et al. [62] Jiahao Huang, Liutao Yang, Fanwen Wang, Yang Nan, Angelica I. Aviles-Rivero, Carola-Bibiane Schönlieb, Daoqiang Zhang, and Guang Yang. 2024. MambaMIR: An Arbitrary-Masked Mamba for Joint Medical Image Reconstruction and Uncertainty Estimation. arXiv:2402.18451 [eess.IV] https://arxiv.org/abs/2402.18451 [63] Tao Huang, Xiaohuan Pei, Shan You, Fei Wang, Chen Qian, and Chang Xu. 2024. LocalMamba: Visual State Space Model with Windowed Selective Scan. arXiv:2403.09338 [cs.CV] https://arxiv.org/abs/2403.09338 [64] Forrest N. Iandola, Matthew W. Moskewicz, Khalid Ashraf, Song Han, William J. Dally, and Kurt Keutzer. 2016. SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <1MB model size. ArXiv abs/1602.07360 (2016). https://api.semanticscholar.org/CorpusID:14136028 [65] Clifford R. Jack et al. 2008. The Alzheimers Disease Neuroimaging Initiative (ADNI): MRI methods. Journal of Magnetic Resonance Imaging 27, 4 (April 2008), 685691. https://doi.org/10.1002/jmri.21049 [66] Stefan Jaeger, Sema Candemir, Sameer Antani, Yì-Xiáng Wáng, Pu-Xuan Lu, and George Thoma. 2014. Two public chest X-ray datasets for computer-aided screening of pulmonary diseases. Quantitative imaging in medicine and surgery 4 (12 2014), 4757. https://doi.org/10.3978/j.issn.2223-4292.2014.11.20 [67] Yuanfeng Ji, Haotian Bai, Jie Yang, Chongjian Ge, Ye Zhu, Ruimao Zhang, Zhen Li, Lingyan Zhang, Wanling Ma, Xiang Wan, et al. 2022. AMOS: Large-Scale Abdominal Multi-Organ Benchmark for Versatile Medical Image Segmentation. arXiv preprint arXiv:2206.08023 (2022). [68] Zhongping Ji. 2024. MHS-VM: Multi-Head Scanning in Parallel Subspaces for Vision Mamba. arXiv:2406.05992 [eess.IV] https://arxiv.org/abs/2406.05992 [69] Zhanlin Ji, Xiaoyu Li, Jianuo Liu, Rui Chen, Qinping Liao, Tao Lyu, and Li Zhao. 2024. LightCF-Net: Lightweight Long-Range Context Fusion Network for Real-Time Polyp Segmentation. Bioengineering 11, 6 (2024). https: //doi.org/10.3390/bioengineering11060545 [70] Zhihan Ju and Wanting Zhou. 2024. VM-DDPM: Vision Mamba Diffusion for Medical Image Synthesis. arXiv:2405.05667 [eess.IV] https://arxiv.org/abs/2405.05667 [71] Rudolph Emil Kalman. 1960. New Approach to Linear Filtering and Prediction Problems. Transactions of the ASMEJournal of Basic Engineering 82, Series (1960), 3545. [72] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. 2020. Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention. arXiv:2006.16236 [cs.LG] https://arxiv.org/abs/2006.16236 [73] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. 2020. Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention. In Proceedings of the 37th International Conference on Machine Learning (Proceedings of Machine Learning Research, Vol. 119), Hal Daumé III and Aarti Singh (Eds.). PMLR, 51565165. https://proceedings.mlr.press/v119/katharopoulos20a.html [74] Baris Kayalibay, Grady Jensen, and Patrick van der Smagt. 2017. CNN-based segmentation of medical imaging data. arXiv preprint arXiv:1701.03056 (2017). [75] Abbas Khan, Muhammad Asad, Martin Benning, Caroline Roney, and Gregory Slabaugh. 2024. CAMS: Convolution and Attention-Free Mamba-based Cardiac Image Segmentation. arXiv:2406.05786 [cs.CV] https://arxiv.org/abs/2406.05786 [76] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Segment Anything. Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollár, and Ross Girshick. 2023. arXiv:2304.02643 [cs.CV] https://arxiv.org/abs/2304.02643 [77] Hao Li, Zeyu Tang, Yang Nan, and Guang Yang. 2022. Human Treelike Tubular Structure Segmentation: Comprehensive Review and Future Perspectives. Computers in biology and medicine 151 Pt (2022), 106241. https://api.semanticscholar.org/CorpusID: [78] Kunchang Li, Xinhao Li, Yi Wang, Yinan He, Yali Wang, Limin Wang, and Yu Qiao. 2024. VideoMamba: State Space Model for Efficient Video Understanding. arXiv:2403.06977 [cs.CV] https://arxiv.org/abs/2403.06977 [79] Qing Li, Weidong Cai, Xiaogang Wang, Yun Zhou, David Dagan Feng, and Mei Chen. 2014. Medical image classification with convolutional neural network. In 2014 13th international conference on control automation robotics & vision (ICARCV). IEEE, 844848. [80] Shufan Li, Harkanwar Singh, and Aditya Grover. 2024. Mamba-ND: Selective State Space Modeling for MultiDimensional Data. ArXiv abs/2402.05892 (2024). https://api.semanticscholar.org/CorpusID:267547860 [81] Zhuoling Li, Minghui Dong, Shiping Wen, Xiang Hu, Pan Zhou, and Zhigang Zeng. 2019. CLU-CNNs: Object detection for medical images. Neurocomputing 350 (2019), 5359. [82] Zewen Li, Fan Liu, Wenjie Yang, Shouheng Peng, and Jun Zhou. 2021. survey of convolutional neural networks: IEEE transactions on neural networks and learning systems 33, 12 (2021), analysis, applications, and prospects. 69997019. [83] Zhe Li, Haiwei Pan, Kejia Zhang, Yuhua Wang, and Fengming Yu. 2024. Mambadfuse: mamba-based dual-phase model for multi-modality image fusion. arXiv:2404.08406 [cs.CV] https://arxiv.org/abs/2404.08406 [84] Chunfeng Lian, Mingxia Liu, Jun Zhang, and Dinggang Shen. 2020. Hierarchical Fully Convolutional Network for Joint Atrophy Localization and Alzheimers Disease Diagnosis Using Structural MRI. IEEE Transactions on Pattern Comprehensive Survey of Mamba Architectures for Medical Image Analysis Analysis and Machine Intelligence 42 (2020), 880893. https://api.semanticscholar.org/CorpusID:58558019 [85] Weibin Liao, Yinghao Zhu, Xinyuan Wang, Chengwei Pan, Yasha Wang, and Liantao Ma. 2024. LightM-UNet: Mamba Assists in Lightweight UNet for Medical Image Segmentation. arXiv:2403.05246 [eess.IV] https://arxiv.org/abs/2403. 05246 [86] Jiarun Liu, Hao Yang, Hong-Yu Zhou, Yan Xi, Lequan Yu, Yizhou Yu, Yong Liang, Guangming Shi, Shaoting Zhang, Hairong Zheng, and Shanshan Wang. 2024. Swin-UMamba: Mamba-based UNet with ImageNet-based pretraining. arXiv:2402.03302 [eess.IV] https://arxiv.org/abs/2402.03302 [87] Xiao Liu, Chenxu Zhang, and Lei Zhang. 2024. Vision mamba: comprehensive survey and taxonomy. arXiv preprint arXiv:2405.04404 (2024). [88] Yue Liu, Yunjie Tian, Yuzhong Zhao, Hongtian Yu, Lingxi Xie, Yaowei Wang, Qixiang Ye, and Yunfan Liu. 2024. VMamba: Visual State Space Model. arXiv:2401.10166 [cs.CV] [89] M. B. Loughrey, M. Arends, I. Brown, L. J. Burgart, C. Cunningham, J. F. Flejou, S. Kakar, R. Kirsch, M. Kojima, A. Lugli, C. Rosty, K. Sheahan, N. P. West, R. Wilson, and I. D. Nagtegaal. 2020. Colorectal Cancer Histopathology Reporting Guide (1st ed.). International Collaboration on Cancer Reporting, Sydney, Australia. [90] Ming Y. Lu, Drew F. K. Williamson, Tiffany Y. Chen, Richard J. Chen, Matteo Barbieri, and Faisal Mahmood. 2020. Data Efficient and Weakly Supervised Computational Pathology on Whole Slide Images. arXiv:2004.09666 [eess.IV] [91] Ali M. Muslim al. 2022. Brain MRI Dataset of Multiple Sclerosis with Consensus Manual Lesion Segmentation and Patient Meta Information. Data in Brief 42 (04 2022), 108139. https://doi.org/10.1016/j.dib.2022.108139 [92] Chao Ma and Ziyang Wang. 2024. Semi-Mamba-UNet: Pixel-Level Contrastive and Pixel-Level Cross-Supervised Visual Mamba-based UNet for Semi-Supervised Medical Image Segmentation. arXiv:2402.07245 [eess.IV] https: //arxiv.org/abs/2402. [93] Jun Ma, Feifei Li, and Bo Wang. 2024. U-Mamba: Enhancing Long-range Dependency for Biomedical Image Segmentation. ArXiv abs/2401.04722 (2024). https://api.semanticscholar.org/CorpusID:266899624 [94] Jun Ma et al. 2024. The multimodality cell segmentation challenge: toward universal solutions. Nature Methods 21, 6 (March 2024), 11031113. https://doi.org/10.1038/s41592-024-02233-6 [95] Teresa Mendonça, Pedro M. Ferreira, Jorge S. Marques, André R. S. Marcal, and Jorge Rozeira. 2013. PH2 - dermoscopic image database for research and benchmarking. In 2013 35th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC). 54375440. https://doi.org/10.1109/EMBC.2013.6610779 [96] Bjoern H. Menze et al. 2015. The Multimodal Brain Tumor Image Segmentation Benchmark (BRATS). Transactions on Medical Imaging 34, 10 (2015), 1993 2024. https://doi.org/10.1109/TMI.2014. E [97] Taylor Moen, Baiyu Chen, David Holmes III, Xinhui Duan, Zhicong Yu, Lifeng Yu, Shuai Leng, Joel Fletcher, and Cynthia McCollough. 2021. Low-dose CT image and projection dataset. Medical physics 48, 2 (2021), 902911. [98] Yang Nan, Javier Del Ser, Zeyu Tang, Peng Tang, Xiaodan Xing, Yingying Fang, Francisco Herrera, Witold Pedrycz, Simon Walsh, and Guang Yang. 2022. Fuzzy Attention Neural Network to Tackle Discontinuity in Airway Segmentation. arXiv:2209.02048 [eess.IV] https://arxiv.org/abs/2209.02048 [99] Ali Nasiri-Sarvi, Vincent Quoc-Huy Trinh, Hassan Rivaz, and Mahdi Hosseini. 2024. Vim4Path: Self-Supervised Vision Mamba for Histopathology Images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 68946903. [100] Eric Nguyen, Karan Goel, Albert Gu, Gordon W. Downs, Preey Shah, Tri Dao, Stephen A. Baccus, and Christopher Ré. 2022. S4ND: Modeling Images and Videos as Multidimensional Signals Using State Spaces. arXiv:2210.06583 [cs.CV] https://arxiv.org/abs/2210.06583 [101] Badri Narayana Patro and Vijay Srinivas Agneeswaran. 2024. Mamba-360: Survey of state space models as transformer alternative for long sequence modelling: Methods, applications, and challenges. arXiv preprint arXiv:2404.16112 (2024). [102] Xiaohuan Pei, Tao Huang, and Chang Xu. 2024. EfficientVMamba: Atrous Selective Scan for Light Weight Visual Mamba. ArXiv abs/2403.09977 (2024). https://api.semanticscholar.org/CorpusID:268510293 [103] Siran Peng, Xiangyu Zhu, Haoyu Deng, Zhen Lei, and Liang-Jian Deng. 2024. FusionMamba: Efficient Image Fusion with State Space Model. arXiv:2404.07932 [cs.CV] https://arxiv.org/abs/2404.07932 [104] Haohao Qu, Liangbo Ning, Rui An, Wenqi Fan, Tyler Derr, Xin Xu, and Qing Li. 2024. Survey of Mamba. arXiv preprint arXiv:2408.01129 (2024). [105] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. 2021. Learning Transferable Visual Models From Natural Language Supervision. arXiv:2103.00020 [cs.CV] https://arxiv.org/abs/2103. [106] Chandravardhan Singh Raghaw, Parth Shirish Bhore, Mohammad Zia Ur Rehman, and Nagendra Kumar. 2024. An Explainable Contrastive-based Dilated Convolutional Network with Transformer for pediatric pneumonia detection. Applied Soft Computing (2024), 112258. S. Bansal et al. [107] Chandravardhan Singh Raghaw, Arnav Sharma, Shubhi Bansal, Mohammad Zia Ur Rehman, and Nagendra Kumar. 2024. CoTCoNet: An optimized coupled transformer-convolutional network with an adaptive graph reconstruction for leukemia detection. Computers in Biology and Medicine 179 (2024), 108821. [108] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. 2015. U-net: Convolutional networks for biomedical image segmentation. In Medical image computing and computer-assisted interventionMICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18. Springer, 234241. [109] Jiacheng Ruan and Suncheng Xiang. 2024. VM-UNet: Vision Mamba UNet for Medical Image Segmentation. arXiv:2402.02491 [eess.IV] https://arxiv.org/abs/2402. [110] Kazi Shahriar Sanjid, Md. Tanzim Hossain, Md. Shakib Shahariar Junayed, and Dr. Mohammad Monir Uddin. 2024. Integrating Mamba Sequence Model and Hierarchical Upsampling Network for Accurate Semantic Segmentation of Multiple Sclerosis Legion. arXiv:2403.17432 [eess.IV] https://arxiv.org/abs/2403.17432 [111] Mohammad Shehab, Laith Abualigah, Qusai Shambour, Muhannad Abu-Hashem, Mohd Khaled Yousef Shambour, Ahmed Izzat Alsalibi, and Amir Gandomi. 2022. Machine learning in medical applications: review of state-ofthe-art methods. Computers in Biology and Medicine 145 (2022), 105458. [112] Yuan Shi, Bin Xia, Xiaoyu Jin, Xing Wang, Tianyu Zhao, Xin Xia, Xuefeng Xiao, and Wenming Yang. 2024. VmambaIR: Visual State Space Model for Image Restoration. arXiv:2403.11423 [cs.CV] https://arxiv.org/abs/2403.11423 [113] Jimmy T. H. Smith, Andrew Warrington, and Scott W. Linderman. 2023. Simplified State Space Layers for Sequence Modeling. arXiv:2208.04933 [cs.LG] https://arxiv.org/abs/2208.04933 [114] Kenji Suzuki. 2017. Overview of deep learning in medical imaging. Radiological physics and technology 10, 3 (2017), 257273. [115] Bowen Tan, Zichao Yang, Maruan AI-Shedivat, Eric P. Xing, and Zhiting Hu. 2021. Progressive Generation of Long Text with Pretrained Language Models. arXiv:2006.15720 [cs.CL] https://arxiv.org/abs/2006.15720 [116] Fenghe Tang, Bingkun Nian, Yingtai Li, Jie Yang, Liu Wei, and S. Kevin Zhou. 2024. MambaMIM: Pre-training Mamba with State Space Token-interpolation. arXiv:2408.08070 [cs.CV] https://arxiv.org/abs/2408.08070 [117] Yujin Tang, Peijie Dong, Zhenheng Tang, Xiaowen Chu, and Junwei Liang. 2024. VMRNN: Integrating Vision Mamba and LSTM for Efficient and Accurate Spatiotemporal Forecasting. ArXiv abs/2403.16536 (2024). https: //api.semanticscholar.org/CorpusID:268681179 [118] Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. 2021. Long Range Arena : Benchmark for Efficient Transformers. In International Conference on Learning Representations. https://openreview.net/forum?id=qVyeW-grC2k [119] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Hervé Jégou. 2021. arXiv:2012.12877 [cs.CV] https: Training data-efficient image transformers & distillation through attention. //arxiv.org/abs/2012.12877 [120] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023. LLaMA: Open and Efficient Foundation Language Models. arXiv:2302.13971 [cs.CL] https://arxiv.org/ abs/2302.13971 [121] Ting Yu Tsai, Li Lin, Shu Hu, Ming-Ching, Hongtu Zhu, and Xin Wang. 2024. UU-Mamba: Uncertainty-aware U-Mamba for Cardiac Image Segmentation. arXiv:2405.17496 [eess.IV] [122] Philipp Tschandl, Cliff Rosendahl, and Harald Kittler. 2018. The HAM10000 dataset, large collection of multi-source dermatoscopic images of common pigmented skin lesions. Scientific Data 5, 1 (Aug. 2018). https://doi.org/10.1038/ sdata.2018. [123] Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. 2017. Instance Normalization: The Missing Ingredient for Fast Stylization. arXiv:1607.08022 [cs.CV] https://arxiv.org/abs/1607.08022 [124] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention Is All You Need. CoRR abs/1706.03762 (2017). arXiv:1706.03762 http://arxiv.org/abs/ 1706.03762 [125] Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. 2017. Graph Attention Networks. ArXiv abs/1710.10903 (2017). https://api.semanticscholar.org/CorpusID:3292002 [126] Dequan Wang, Xiaosong Wang, Lilong Wang, Mengzhang Li, Qian Da, Xiaoqiang Liu, Xiangyu Gao, Jun Shen, He Junjun, Tian Shen, Qi Duan, Jie Zhao, Kang Li, Yu Qiao, and Shaoting Zhang. 2023. MedFMC: Real-world Dataset and Benchmark For Foundation Model Adaptation in Medical Image Classification. https://doi.org/10.48550/arXiv. 2306. [127] Hualiang Wang, Yiqun Lin, Xinpeng Ding, and Xiaomeng Li. 2024. Tri-Plane Mamba: Efficiently Adapting Segment Anything Model for 3D Medical Images. arXiv:2409.08492 [eess.IV] https://arxiv.org/abs/2409.08492 [128] Jinhong Wang, Jintai Chen, Danny Chen, and Jian Wu. 2024. LKM-UNet: Large Kernel Vision Mamba UNet for Medical Image Segmentation. arXiv:2403.07332 [cs.CV] https://arxiv.org/abs/2403.07332 Comprehensive Survey of Mamba Architectures for Medical Image Analysis [129] Qilong Wang, Banggu Wu, Pengfei Zhu, Peihua Li, Wangmeng Zuo, and Qinghua Hu. 2020. ECA-Net: Efficient Channel Attention for Deep Convolutional Neural Networks. arXiv:1910.03151 [cs.CV] https://arxiv.org/abs/1910.03151 [130] Ziyang Wang and Chao Ma. 2024. Weak-Mamba-UNet: Visual Mamba Makes CNN and ViT Work Better for Scribble-based Medical Image Segmentation. arXiv:2402.10887 [eess.IV] https://arxiv.org/abs/2402.10887 [131] Ziyang Wang, Jian-Qing Zheng, Chao Ma, and Tao Guo. 2024. VMambaMorph: Multi-Modality Deformable Image Registration Framework based on Visual State Space Model with Cross-Scan Module. ArXiv abs/2404.05105 (2024). https://api.semanticscholar.org/CorpusID:269004567 [132] Ziyang Wang, Jian-Qing Zheng, Yichi Zhang, Ge Cui, and Lei Li. 2024. Mamba-UNet: UNet-Like Pure Visual Mamba for Medical Image Segmentation. arXiv:2402.05079 [eess.IV] https://arxiv.org/abs/2402. [133] Jerry Wei, Arief Suriawinata, Bing Ren, Xiaoying Liu, Mikhail Lisovsky, Louis Vaickus, Charles Brown, Michael Baker, Naofumi Tomita, Lorenzo Torresani, Jason Wei, and Saeed Hassanpour. 2021. Petri Dish for Histopathology Image Analysis. arXiv:2101.12355 [eess.IV] https://arxiv.org/abs/2101.12355 [134] Renkai Wu, Yinghao Liu, Pengchen Liang, and Qing Chang. 2024. H-vmunet: High-order Vision Mamba UNet for Medical Image Segmentation. arXiv:2403.13642 [cs.CV] https://arxiv.org/abs/2403.13642 [135] Renkai Wu, Yinghao Liu, Pengchen Liang, and Qing Chang. 2024. UltraLight VM-UNet: Parallel Vision Mamba Significantly Reduces Parameters for Skin Lesion Segmentation. arXiv:2403.20035 [eess.IV] https://arxiv.org/abs/ 2403.20035 [136] Jianhao Xie, Ruofan Liao, Ziang Zhang, Sida Yi, Yuesheng Zhu, and Guibo Luo. 2024. Promamba: Prompt-mamba for polyp segmentation. arXiv preprint arXiv:2403.13660 (2024). [137] Xinyu Xie, Yawen Cui, Chio-In Ieong, Tao Tan, Xiaozhi Zhang, Xubin Zheng, and Zitong Yu. 2024. FusionMamba: arXiv:2404.09498 [cs.CV] https: Dynamic Feature Enhancement for Multimodal Image Fusion with Mamba. //arxiv.org/abs/2404.09498 [138] Zhaohu Xing, Tian Ye, Yijun Yang, Guang Liu, and Lei Zhu. 2024. SegMamba: Long-range Sequential Modeling Mamba For 3D Medical Image Segmentation. arXiv:2401.13560 [cs.CV] https://arxiv.org/abs/2401.13560 [139] Jiashu Xu. 2024. HC-Mamba: Vision MAMBA with Hybrid Convolutional Techniques for Medical Image Segmentation. arXiv:2405.05007 [eess.IV] https://arxiv.org/abs/2405. [140] Rui Xu, Shu Yang, Yihui Wang, Bo Du, and Hao Chen. 2024. survey on vision mamba: Models, applications and challenges. arXiv preprint arXiv:2404.18861 (2024). [141] Chenhongyi Yang, Zehui Chen, Miguel Espinosa, Linus Ericsson, Zhenyu Wang, Jiaming Liu, and Elliot J. Crowley. 2024. PlainMamba: Improving Non-Hierarchical Mamba in Visual Recognition. arXiv:2403.17695 [cs.CV] https: //arxiv.org/abs/2403.17695 [142] Guangqian Yang, Kangrui Du, Zhihan Yang, Ye Du, Yongping Zheng, and Shujun Wang. 2024. CMViM: Contrastive Masked Vim Autoencoder for 3D Multi-modal Representation Learning for AD classification. arXiv:2403.16520 [cs.CV] [143] Jiancheng Yang, Rui Shi, and Bingbing Ni. 2021. MedMNIST Classification Decathlon: Lightweight AutoML Benchmark for Medical Image Analysis. In IEEE 18th International Symposium on Biomedical Imaging (ISBI). 191195. [144] Jiancheng Yang, Rui Shi, Donglai Wei, Zequan Liu, Lin Zhao, Bilian Ke, Hanspeter Pfister, and Bingbing Ni. 2023. MedMNIST v2-A large-scale lightweight benchmark for 2D and 3D biomedical image classification. Scientific Data 10, 1 (2023), 41. [145] Shu Yang, Yihui Wang, and Hao Chen. 2024. Mambamil: Enhancing long sequence modeling with sequence reordering in computational pathology. arXiv preprint arXiv:2403.06800 (2024). [146] Yijun Yang, Zhaohu Xing, Lequan Yu, Chunwang Huang, Huazhu Fu, and Lei Zhu. 2024. Vivim: Video Vision Mamba for Medical Video Segmentation. arXiv:2401.14168 [cs.CV] https://arxiv.org/abs/2401.14168 [147] Zhichao Yang, Avijit Mitra, Sunjae Kwon, and Hong Yu. 2024. Clinicalmamba: generative clinical language model on longitudinal clinical notes. arXiv preprint arXiv:2403.05795 (2024). [148] Zi Ye, Tianxiang Chen, Fangyijie Wang, Hanwei Zhang, and Lijun Zhang. 2024. P-Mamba: Marrying Perona Malik Diffusion with Mamba for Efficient Pediatric Echocardiographic Left Ventricular Segmentation. arXiv:2402.08506 [cs.CV] https://arxiv.org/abs/2402.08506 [149] Chunyu Yuan, Dongfang Zhao, and Sos S. Agaian. 2024. MUCM-Net: Mamba Powered UCM-Net for Skin Lesion Segmentation. arXiv:2405.15925 [eess.IV] https://arxiv.org/abs/2405.15925 [150] Yubiao Yue and Zhenzhang Li. 2024. MedMamba: Vision Mamba for Medical Image Classification. arXiv:2403.03849 [eess.IV] https://arxiv.org/abs/2403.03849 [151] Jure Zbontar, Florian Knoll, Anuroop Sriram, Tullie Murrell, Zhengnan Huang, Matthew Muckley, Aaron Defazio, Ruben Stern, Patricia Johnson, Mary Bruno, et al. 2018. fastMRI: An open dataset and benchmarks for accelerated MRI. arXiv preprint arXiv:1811.08839 (2018). [152] Xinyu Zeng, Zifan Jiang, Wen Luo, Honggui Li, Hongye Li, Guo Li, Jingyong Shi, Kangjie Wu, Tong Liu, Xing Lin, et al. 2021. Efficient and accurate identification of ear diseases using an ensemble deep learning model. Scientific Reports 11, 1 (2021), 10839. S. Bansal et al. [153] Hanwei Zhang, Ying Zhu, Dan Wang, Lijun Zhang, Tianxiang Chen, Ziyang Wang, and Zi Ye. 2024. survey on visual mamba. Applied Sciences 14, 13 (2024), 5683. [154] Kaidong Zhang and Dong Liu. 2023. Customized Segment Anything Model for Medical Image Segmentation. arXiv:2304.13785 [cs.CV] https://arxiv.org/abs/2304. [155] Mingya Zhang, Zhihao Chen, Yiyuan Ge, and Xianping Tao. 2024. HMT-UNet: hybird Mamba-Transformer Vision UNet for Medical Image Segmentation. arXiv:2408.11289 [eess.IV] https://arxiv.org/abs/2408.11289 [156] Mingya Zhang, Yue Yu, Limei Gu, Tingsheng Lin, and Xianping Tao. 2024. VM-UNET-V2 Rethinking Vision Mamba UNet for Medical Image Segmentation. arXiv:2403.09157 [eess.IV] https://arxiv.org/abs/2403.09157 [157] Yuelin Zhang, Wanquan Yan, Kim Yan, Chun Ping Lam, Yufu Qiu, Pengyu Zheng, Raymond Shing-Yan Tang, and Shing Shin Cheng. 2024. Motion-Guided Dual-Camera Tracker for Low-Cost Skill Evaluation of Gastric Endoscopy. arXiv:2403.05146 [cs.CV] https://arxiv.org/abs/2403.05146 [158] Zeyu Zhang, Akide Liu, Ian Reid, Richard Hartley, Bohan Zhuang, and Hao Tang. 2024. Motion Mamba: Efficient and Long Sequence Motion Generation. arXiv:2403.07487 [cs.CV] https://arxiv.org/abs/2403.07487 [159] Huizhen Zhao, Fuxian Liu, Longyue Li, and Chang Luo. 2018. novel softplus linear unit for deep convolutional neural networks. Applied Intelligence 48, 7 (July 2018), 17071720. https://doi.org/10.1007/s10489-017-1028-7 [160] Yu Zhao, Hongwei Li, Shaohua Wan, Anjany Sekuboyina, Xiaobin Hu, Giles Tetteh, Marie Piraud, and Bjoern Menze. 2019. Knowledge-aided convolutional neural network for small organ segmentation. IEEE journal of biomedical and health informatics 23, 4 (2019), 13631373. [161] Zou Zhen, Yu Hu, and Zhao Feng. 2024. FreqMamba: Viewing Mamba from Frequency Perspective for Image Deraining. arXiv:2404.09476 [cs.CV] https://arxiv.org/abs/2404.09476 [162] Zheng and Zhang. 2024. FD-Vision Mamba for Endoscopic Exposure Correction. arXiv:2402.06378 [cs.CV] https://arxiv.org/abs/2402. [163] Jiaying Zhou, Mingzhou Jiang, Junde Wu, Jiayuan Zhu, Ziyue Wang, and Yueming Jin. 2024. MGI: Multimodal Contrastive pre-training of Genomic and Medical Imaging. arXiv:2406.00631 [cs.CV] https://arxiv.org/abs/2406.00631 [164] Lianghui Zhu, Bencheng Liao, Qian Zhang, Xinlong Wang, Wenyu Liu, and Xinggang Wang. 2024. Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model. arXiv:2401.09417 [cs.CV] [165] Shun Zou, Zhuo Zhang, Yi Zou, and Guangwei Gao. 2024. Microscopic-Mamba: Revealing the Secrets of Microscopic Images with Just 4M Parameters. arXiv:2409.07896 [cs.CV] https://arxiv.org/abs/2409."
        }
    ],
    "affiliations": [
        "Birla Institute of Technology & Science Pilani, India",
        "Indian Institute of Technology Indore, India",
        "Jio Platforms Limited, India",
        "R.M.D. Engineering College, Kavaraipettai, India"
    ]
}