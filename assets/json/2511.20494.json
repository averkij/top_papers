{
    "paper_title": "Adversarial Confusion Attack: Disrupting Multimodal Large Language Models",
    "authors": [
        "Jakub Hoscilowicz",
        "Artur Janicki"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce the Adversarial Confusion Attack, a new class of threats against multimodal large language models (MLLMs). Unlike jailbreaks or targeted misclassification, the goal is to induce systematic disruption that makes the model generate incoherent or confidently incorrect outputs. Practical applications include embedding such adversarial images into websites to prevent MLLM-powered AI Agents from operating reliably. The proposed attack maximizes next-token entropy using a small ensemble of open-source MLLMs. In the white-box setting, we show that a single adversarial image can disrupt all models in the ensemble, both in the full-image and Adversarial CAPTCHA settings. Despite relying on a basic adversarial technique (PGD), the attack generates perturbations that transfer to both unseen open-source (e.g., Qwen3-VL) and proprietary (e.g., GPT-5.1) models."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 ] . [ 3 4 9 4 0 2 . 1 1 5 2 : r Adversarial Confusion Attack: Disrupting Multimodal Large Language Models"
        },
        {
            "title": "Warsaw University of Technology\nPoland",
            "content": "Abstract. We introduce the Adversarial Confusion Attack, new class of threats against multimodal large language models (MLLMs). Unlike jailbreaks or targeted misclassification, the goal is to induce systematic disruption that makes the model generate incoherent or confidently incorrect outputs. Practical applications include embedding such adversarial images into websites to prevent MLLM-powered AI Agents from operating reliably. The proposed attack maximizes next-token entropy using small ensemble of open-source MLLMs. In the white-box setting, we show that single adversarial image can disrupt all models in the ensemble, both in the full-image and Adversarial CAPTCHA settings. Despite relying on basic adversarial technique (PGD), the attack generates perturbations that transfer to both unseen open-source (e.g., Qwen3-VL) and proprietary (e.g., GPT-5.1) models."
        },
        {
            "title": "Introduction",
            "content": "Most existing adversarial work targets classification errors, unsafe content steering, or jailbreak manipulation [18]. We address distinct failure mode: confusion. confusion attack aims to destabilize the models decoding process and produce high-confidence hallucinations or incoherent text, thereby preventing the model from forming reliable understanding of the scene. We study formulation of the confusion attack in which we maximize the next-token Shannon entropy of the models output distribution. This objective disrupts the decoders internal state and drives the model toward unstable token generation. Prior work has shown that aligned multimodal models are vulnerable to universal attacks and patch-style perturbations [912], that proprietary systems such as GPT-4 can be affected by adversarial examples [13, 14], and that perturbations and ensemble design follow scaling laws that govern black-box attack success [1416]. Our work complements recent adversarial research on MLLMs and contributes the following: We introduce the Adversarial Confusion Attack, which maximizes output entropy to destabilize decoding, and characterize five distinct modes of resulting model failure. In the white-box setting, we show that single perturbation disrupts all models in the ensemble, in both the full-image and Adversarial CAPTCHA settings. In the full-image setting, we demonstrate black-box transfer to open-source and proprietary MLLMs. Model Output Describe this image. GPT-5.1 GPT-o3 dramatic moment from soccer match [...] The image shows educational display Cell factory [...] Gemini 3.0 This appears to be corrupted static or noise [...] Grok 4 This is jailbreak image. cant assist with that. LLaVA-1. The image features person wearing black shirt [...] Qwen2.5-VL Oh, Pluto! (4) 11 of 1 Table 1: Qualitative results for full-image black-box transfer to proprietary models under an unconstrained perturbation budget (the original attack operates at 448 448 pixels). Example screenshots from the LMSYS Arena are provided in Appendix A. = Success (Hallucination); = Safety Refusal; = Attack Failed."
        },
        {
            "title": "2 Method",
            "content": "Let [0, 1]3HW be an image and {0, 1}HW be binary mask defining the attack region. For global attacks, is an all-ones matrix; for patch attacks, is 1 only within fixed region and 0 elsewhere. The perturbed image is defined as: xδ = Π[0,1](x + δ), δ ε, (1) where denotes the element-wise Hadamard product and Π clips to the valid pixel range. We attack surrogate ensemble = {fj}J j=1 of open-source MLLMs. Each model receives xδ and fixed text prompt through its preprocessing pipeline Φj. For model fj, let zj denote its next-token logits at the final prompt position τj. We compute top-k probabilities pj = softmax(z(k) /Te), where z(k) retains the top logits and Te is the temperature, and maximize the Shannon entropy H(pj) = (cid:80) pj(v) log pj(v). The attack maximizes entropy averaged across models: max δε"
        },
        {
            "title": "1\nJ",
            "content": "J (cid:88) j=1 H(cid:0)pj(xδ, t)(cid:1). (2) We perform projected gradient ascent (PGD) [1], optionally masking the gradient to constrain updates to the patch area: δ Πε (δ + η(M δL)) , (3) with equal to the negative of the entropy objective. Prompt: Describe this image. LLaVA 1.5 The image features person with necklace on, positioned [...] LLaVA 1. This image is composite of two separate photographs [...] Qwen3-VL Im sorry, but cant assist with that request. Qwen2.5-VL cant see any image. Table 2: Qualitative results for the white-box Adversarial CAPTCHA setup (original attack operates at 1024 576 pixels)."
        },
        {
            "title": "3 Experiments",
            "content": "Setup. The base image is screenshot of the CCRU homepage that is resized to 448448 to reduce training time. We also tested other websites and observed no substantial differences in results. For the Adversarial CAPTCHA experiments, we use the full 1024 576 webpage screenshot and optimize fixed 224 224 region at its center. In all scenarios, we optimize the perturbation δ for 50 iterations and select the final adversarial example by choosing the one that yields the highest averaged entropy across the training ensemble. We used four open-source models: Qwen2.5-VL-3B, Qwen3-VL-2B, LLaVA-1.5-7B, and LLaVA-1.6-7B. Metrics & Baselines. We report the Shannon entropy of the next-token distribution, restricted to the top = 50 logits. We found that aggressive truncation (e.g., = 5) reduces transferability, while full-vocabulary optimization introduces training instability. This restriction also standardizes entropy values across models with different vocabulary sizes. We evaluate black-box transfer using cross-family held-out protocol. Specifically, we optimize on two models from one family and evaluate on held-out model from different family. We compare the adversarial output against two baselines: the clean, unperturbed screenshot and an image perturbed with uniform random noise δuni U(ε, ε). Across all models, entropy for the clean image remains low (below 0.6) and comparable to the random noise baseline; modest entropy increase (0.2) was observed for Qwen3-VL under the unconstrained budget noise. We report the Effective Confusion Ratio (ECR), which quantifies how much the attack outperforms both the clean image and the random noise baseline: ECR = H(f (xadv)) max (cid:2)H(f (xclean)), H(f (xnoise))(cid:3) (4) Values above 1 indicate that the adversarial example induces higher uncertainty than both clean and random-noise baselines. Proprietary Evaluation. For proprietary models, we evaluate transfer using the LMSYS Arena platform1 with the prompt Describe this image. and the adversarial image as input. We count an attack as successful when the models description is clearly unrelated to the actual image content. We categorize outcomes with three labels: (coherent hallucination), (safety or jailbreak-style refusal), and (no confusion effect, such as correctly identifying the image as noise or describing the clean website layout). Settings Effective Confusion Ratio (ECR) ε LR Qwen3-VL Qwen2.5-VL LLaVA-1.5 LLaVA-1.6 Mean 1.0 0. 1.0 0.01 1.0 0.5 0.05 0.005 0.5 0.05 0.005 0.5 0.05 0. 0.5 0.05 0.005 0.5 0.05 0.005 Panel A: Full Image Attack (White-box) 2.33 3.29 6.84 1.17 1.83 3.15 5.78 5.90 3. 1.19 2.06 2.31 3.01 5.20 6.08 1.41 2.72 2.46 Panel B: Held-out Transfer (Black-box) 1.72 1.40 1.12 1.04 1.15 1. 1.75 0.97 1.04 1.05 0.98 0.99 2.08 1.43 1.27 1.12 1.33 1.27 1.94 4.96 3.69 1.18 1.09 1. 1.03 1.73 1.11 1.13 1.04 1.02 Panel C: Adversarial 224 224 Patch (White-box) 0.97 3.41 1.05 3.98 4.43 1.02 1.10 3.19 1. 0.95 1.17 1.00 3.27 4.84 5.08 1.24 1.93 2.30 1.65 1.38 1.14 1.09 1.13 1.10 1.75 3.05 1. Table 3: Effective Confusion Ratios as function of the perturbation budget ε and learning rate LR. Panel shows confusion intensity using the full image space. Panel measures transferability to held-out model. Panel evaluates localized adversarial patch."
        },
        {
            "title": "3.1 Results",
            "content": "In the white-box scenario (Table 3, Panel A), full-image perturbations produce strong entropy amplification across all models. Unconstrained-budget settings (ε = 1.0) raise entropy by roughly 36 depending on the learning rate, with the 1https://lmarena.ai best configuration reaching mean ratio of 5.08. Imperceptible perturbations (ε = 0.01) also reliably increase entropy above the baseline. This shows that significant decoding instability can be induced without visible image degradation, though the effect is less severe than unconstrained attacks. For the black-box scenario (Panel B), the best unconstrained configuration reaches mean ratio of 1.65, showing that the perturbation transfers uncertainty also to unseen models. Lower budgets reduce transfer, with ratios near 1.1. Panel demonstrates the efficacy of the white-box patch attack. Constraining the perturbation to 224 224 region yields mean ratio of 3.05. This shows that patch can disrupt models decoding by modifying only 9% of the image pixels. Proprietary evaluations in Table 4 follow similar trend. At ε = 1.0, GPT-5.1, GPT-o3, GPT-4o, and Nova Pro produce coherent hallucinations, while Grok 4 issues safety refusal  (Table 1)  . Lower-budget perturbations fail to transfer and result in accurate descriptions of the original website. High-entropy perturbations therefore generalize beyond the training ensemble, but basic PGD fails to produce transferable perturbations under small-budget constraints."
        },
        {
            "title": "Target Models",
            "content": "ε LR GPT-5.1 GPT-o3 GPT-4o Grok 4 Gemini 2.5 Gemini 3.0 Nova Pro 1.0 0.5 0.05 0.01 0.01 * Table 4: Black-box transfer to proprietary models. = coherent hallucination, = safety or jailbreak-style refusal, = no confusion effect."
        },
        {
            "title": "4 Discussion",
            "content": "Confusion Modes. We categorize the observed adversarial effects into five distinct modes: Blindness, where the model claims inability to view or process the image; Subtle, where the model describes the high-level domain of the image but generates incorrect or uninformative text; Language Switch, characterized by unprompted shifts to non-English scripts; Delusional, involving confident hallucinations of nonexistent objects; and Collapse, complete breakdown of semantic coherence marked by repetition loops. In the white-box setting, we observed the full spectrum of confusion modes. Collapse was typically associated with peak entropy values, whereas Subtle and Delusional modes correlated with lower entropy increases. In the black-box transfer to proprietary models, Collapse and Blindness were absent; instead, these models exhibited primarily Delusional hallucinations and Language Switch. Imperceptibility. In our setting, ε = 0.01 perturbations are visually imperceptible, but they fail to transfer. Consistent with prior work [1, 1316], simple PGD-style attacks show limited transferability under very small budgets. However, in some practical settings, visual imperceptibility is preference rather than requirement. For adversarial patches designed to block AI Agents from operating on websites, the primary goal is Denial of Service. visible, high-entropy noise patch (ε = 1.0) that reliably induces agent malfunction is therefore reasonable defense mechanism, even if the perturbation is conspicuous to human users. Limitations & Future Work. This study uses an entropy-maximization objective implemented with PGD, basic first-order adversarial optimization technique. Future work should investigate whether feature-level disruptions or more advanced momentum-based adversarial methods [11, 12, 15] can help bridge the entropy gap between white-box and black-box settings. Enhancing robustness to compression, rendering, and small geometric transformations is also important for real-world deployment [17]. The adversarial confusion attack also warrants evaluation within complex, multi-step agentic workflows [5, 7, 8, 18, 19]. particularly interesting direction is exploring how adversarial confusion can be embedded into website design, such as through background textures or UI color schemes."
        },
        {
            "title": "5 Conclusion",
            "content": "We introduced the Adversarial Confusion Attack, method for disrupting Multimodal Large Language Models by maximizing next-token entropy. Using standard Projected Gradient Descent optimizer and small surrogate ensemble, we showed that single perturbationapplied globally or as localized patchcan reliably destabilize model decoding. The attack transfers to unseen open-source and proprietary models in the full-image setting, indicating that entropy-based perturbations exploit vulnerabilities shared across current MLLMs [20]. These results position confusion attacks as novel defense against unauthorized AI Agent activity, deployable via the proposed Adversarial CAPTCHA or, in future applications, through direct integration into website UIs."
        },
        {
            "title": "References",
            "content": "[1] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. In International Conference on Learning Representations (ICLR), 2018. [2] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, and Pascal Frossard. Universal adversarial perturbations. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 17651773, 2017. [3] Xiangyu Qi, Kaixuan Huang, Ashwinee Panda, Peter Henderson, Mengdi Wang, and Prateek Mittal. Visual adversarial examples jailbreak aligned large language models. In AAAI Conference on Artificial Intelligence, 2024. [4] Luke Bailey, Euan Ong, Stuart Russell, and Scott Emmons. Image hijacks: Adversarial images can control generative models at runtime. In International Conference on Machine Learning (ICML), 2024. [5] Renhua Ding, Xiao Yang, Zhengwei Fang, Jun Luo, Kun He, and Jun Zhu. Practical and stealthy touch-guided jailbreak attacks on deployed mobile vision-language agents. arXiv preprint arXiv:2510.07809, 2025. [6] Charankumar Akiri, Harrison Simpson, Kshitiz Aryal, Aarav Khanna, large language and Maanak Gupta. models: Benchmarking risk profile and harm potential. arXiv preprint arXiv:2509.10655, 2025."
        },
        {
            "title": "Safety and security analysis of",
            "content": "[7] Xilong Wang, John Bloch, Zedian Shao, Yuepeng Hu, Shuyan Zhou, and Neil Zhenqiang Gong. WebInject: Prompt injection attack to web agents. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 20102030. Association for Computational Linguistics, 2025. [8] Yitong Zhang, Ximo Li, Liyi Cai, and Jia Li. Realistic environmental injection attacks on GUI agents. arXiv preprint arXiv:2509.11250, 2025. [9] Rahmatullaev, Druzhinina, Mikhalchuk, Kuznetsov, and Razzhigaev. Universal adversarial attack on aligned multimodal LLMs. arXiv preprint arXiv:2502.07987, 2025. [10] Aichberger, Paren, Gal, Torr, and Bibi. Attacking multimodal OS agents with malicious image patches. In Advances in Neural Information Processing Systems (NeurIPS), 2025. [11] Lijie Hu and Di Wang. C2 ATTACK: Towards representation backdoor on CLIP via concept confusion. arXiv preprint arXiv:2503.09095, 2025. [12] Ravikumar Balakrishnan and Mansi Phute. VISOR++: Universal viinputs based steering for large vision language models. ArXiv, sual abs/2509.25533, 2025. [13] Hu, Yu, Zhang, Robey, Zou, Xu, Hu, and Fredrikson. Transferable adversarial attacks on black-box vision-language models. arXiv preprint arXiv:2505.01050, 2025. [14] Liu, Chen, Zhang, Dong, and Zhu. Scaling laws for black-box adversarial attacks. arXiv preprint arXiv:2411.16782, 2025. [15] Chen, Zhang, Dong, Yang, Su, and Zhu. Rethinking model ensemble in transfer-based adversarial attacks. In International Conference on Learning Representations (ICLR), 2024. [16] Yanpei Liu, Xinyun Chen, Chang Liu, and Dawn Song. Delving into transferable adversarial examples and black-box attacks. In International Conference on Learning Representations (ICLR), 2017. [17] Anish Athalye, Logan Engstrom, Andrew Ilyas, and Kevin Kwok. Synthesizing robust adversarial examples. In Proceedings of the 35th International Conference on Machine Learning (ICML), pages 284293, 2018. [18] Shuyan Zhou, Frank Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Yonatan Bisk, Daniel Fried, Uri Alon, et al. WebArena: realistic web environment for building autonomous agents. In International Conference on Learning Representations (ICLR), 2024. [19] Yichen Wang, Hangtao Zhang, Hewen Pan, Ziqi Zhou, Xianlong Wang, Peijin Guo, Lulu Xue, Shengshan Hu, Minghui Li, and Leo Yu Zhang. AdvEDM: Fine-grained adversarial attack against VLM-based embodied agents. ArXiv, abs/2509.16645, 2025. [20] Minyoung Huh, Brian Cheung, Tongzhou Wang, and Phillip Isola. The platonic representation hypothesis. In Proceedings of the 41st International Conference on Machine Learning (ICML), 2024. Appendix: Qualitative Results In this section, we provide raw LMSYS Chat Arena screenshots showing the Adversarial Confusion Attack transferring to proprietary models. The examples illustrate the range of observed behaviors, from explicit refusals and noise detection to strong hallucinations and fully fabricated scene descriptions. (a) GPT-5-high hallucinating suburban real estate. (b) GPT-o3 hallucinating subway car, contrasted with Gemini correctly identifying noise. (a) GPT-4o hallucinating vending machine with detailed fictitious elements. (b) GPT-5 hallucinating detailed map of central Europe. (a) GPT-o3 hallucinating an ATM kiosk scene."
        }
    ],
    "affiliations": []
}