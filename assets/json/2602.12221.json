{
    "paper_title": "Best of Both Worlds: Multimodal Reasoning and Generation via Unified Discrete Flow Matching",
    "authors": [
        "Onkar Susladkar",
        "Tushar Prakash",
        "Gayatri Deshmukh",
        "Kiet A. Nguyen",
        "Jiaxun Zhang",
        "Adheesh Juvekar",
        "Tianshu Bao",
        "Lin Chai",
        "Sparsh Mittal",
        "Inderjit S Dhillon",
        "Ismini Lourentzou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We propose UniDFlow, a unified discrete flow-matching framework for multimodal understanding, generation, and editing. It decouples understanding and generation via task-specific low-rank adapters, avoiding objective interference and representation entanglement, while a novel reference-based multimodal preference alignment optimizes relative outcomes under identical conditioning, improving faithfulness and controllability without large-scale retraining. UniDFlpw achieves SOTA performance across eight benchmarks and exhibits strong zero-shot generalization to tasks including inpainting, in-context image generation, reference-based editing, and compositional generation, despite no explicit task-specific training."
        },
        {
            "title": "Start",
            "content": "Best of Both Worlds: Multimodal Reasoning and Generation via Unified Discrete Flow Matching Onkar Susladkar Tushar Prakash Gayatri Deshmukh Kiet A. Nguyen Jiaxun Zhang Adheesh Juvekar Tianshu Bao Lin Chai Sparsh Mittal Inderjit Dhillon Ismini Lourentzou Indian Institute of Technology Roorkee University of Illinois Urbana-Champaign Independent Researcher University of Texas at Austin Google 6 2 0 2 2 1 ] . [ 1 1 2 2 2 1 . 2 0 6 2 : r Figure 1: We propose UniDFlow unified multimodal diffusion framework that supports image understanding, generation, and thinking-based editing. The model performs visual reasoning for question answering, produces high-quality text-to-image generations across diverse scenes and subjects, and enables instructiondriven, multi-step image editing through structured reasoning. Abstract. We propose UniDFlow, unified discrete flow-matching framework for multimodal understanding, generation, and editing. It decouples understanding and generation via task-specific low-rank adapters, avoiding objective interference and representation entanglement, while novel reference-based multimodal preference alignment optimizes relative outcomes under identical conditioning, improving faithfulness and controllability without large-scale retraining. UniDFlow achieves SOTA performance across eight benchmarks and exhibits strong zero-shot generalization to tasks including inpainting, in-context image generation, reference-based editing, and compositional generation, despite no explicit task-specific training. https://plan-lab.github.io/unidflow 1. Introduction Multimodal generative systems have become central to everyday productivity, with large language models (LLMs) such as ChatGPT [39] and Gemini [58] enabling strong reasoning and instruction following. Similarly, diffusion-based models such as Stable Diffusion [15, 45] and DALLE [6, 44] excel at high-fidelity image and video generation. However, these models remain largely disjoint as LLM-centric models excel at understanding but lack native generative mechanisms, while diffusion models provide powerful generation with limited semantic grounding and reasoning. This separation motivates unified multimodal models that integrate LLM-level understanding with diffusion-level generation within single architecture [63, 68]. Early approaches in this direction, such as Emu [13] and Chameleon [57], represent images as visual tokens and model both text and vision using single auto-regressive (AR) transformer [61]. While simple, AR-based generation is highly inefficient for high-dimensional visual outputs. Hybrid models, including EMMA [24], OmniGen2 [65], MammothModa2 [48], and BAGEL [14], combine AR modeling for text with diffusion-style objectives for images to retain language understanding while improving generation. Moreover, UniDisc [55] and Muddit [50] employ fully discrete diffusion with unified denoising objective for text and images, but performance lags behind hybrid models. Despite recent progress, existing unified models still face several fundamental limitations. (1) Large-scale ARdiffusion frameworks couple cross-entropy decoding with diffusion-style regression [48, 65], creating mismatched objectives that lead to un- (2) Even with strong stable joint optimization. pretrained initialization, many approaches rely on full-model updates over hundreds of millions of samples [14, 24], incurring substantial compute while often degrading general-purpose reasoning ability. (3) Current unified diffusion approaches entangle understanding and generation within shared parameters, thus improving one capability can inadvertently erode the other [50, 78]. (4) Generation and editing are often improved through additional alignment stages, such as multimodal reflection [65] or reinforcement learning with scalar rewards [48]. However, these approaches optimize outputs in isolation, encouraging higher scores or improved reasoning trajectories without modeling relative preference under identical conditioning. As result, they fail to learn explicit decision boundaries between faithful and subtly incorrect edits. To address the aforementioned limitations, we introduce UniDFlow, unified discrete diffusion framework for efficient multimodal understanding and generation. UniDFlow leverages strong pretrained visionlanguage model as prior, avoiding redundant"
        },
        {
            "title": "1 INTRODUCTION",
            "content": "Figure 2: Instruction-guided editing attention maps showing UniDFlow more precisely focuses on relevant regions than prior models. pretraining and enabling parameter-efficient adaptation through lightweight adapters. We perform largescale three-stage training: (i) an understandingfocused stage, (ii) generation-focused stage, and (iii) joint understandinggeneration stage with reference-based multimodal preference optimization to improve editing fidelity and controllability. To prevent parameter entangle, UniDFlow trains separate adapters for understanding and generation, while the final stage trains only lightweight router to combine them dynamically Fig. 2 visualizes the instruction-guided activation maps during editing. UniDFlow consistently attends more precisely to instruction-relevant regions, whether modifying coarse objects (e.g., adding T-shirt) or finer details (e.g., changing the swoosh color). Our main contributions are: We introduce UniDFlow, unified discrete diffusion model that repurposes pretrained vision language backbone as generator over multimodal tokens, enabling understanding, text generation, image synthesis, and editing within one probabilistic interface. We unify text and image generation under single discrete flow-matching objective for all tasks and incorporate stable time-conditioning mechanism that preserves the backbones reasoning priors. Compared to prior multi-objectives, UniDFlow achieves efficient training and inference, requiring only 20 denoising steps while preserving high generation quality. We propose mRef DPO, reference-guided multimodal preference alignment that optimizes relative preferences conditioned on both the instruction and the visual reference, leading to more faithful and controllable editing. UniDFlow achieves state-of-the-art performance on 8 benchmarks spanning understanding, generation, and editing, with up to 13% improvement over larger unified models with more than 3 parameters, and up to 24% gains over popular models such as Qwen 3 [5] and DeepSeek-VL2 [67]. 2. Related Work Diffusion for Visual Generation. Diffusion probabilistic models (DPMs) [25, 38, 46] outperform GANs [22] in stability and quality but are costly in pixel space. Latent diffusion models (LDMs) [45] mitigate this via compressed latent representations, enabling strong text-to-image generation [9, 41, 77]. Discrete diffusion [2] extends diffusion to categorical spaces using masking-based corruption, motivating parallel mask-and-predict generators that improve fidelity and efficiency [8, 23]. Unified Models for Understanding and Generation. To unify understanding and generation, early works such as Emu [13, 52] and Chameleon [57] adopt fully autoregressive modeling over text and visual tokens, but scale poorly for high-resolution images. Hybrid frameworks, including EMMA [24], OmniGen2 [65], MammothModa2 [48], and BAGEL [14], combine autoregressive text modeling with diffusionbased image generation, yet still face modality and objective mismatches. Fully discrete diffusion models such as UniDisc [55] and Muddit [50] further unify modeling but lag behind large-scale hybrids. Our work introduces UniDFlow, unified discrete flow-matching model with stable time-conditioning that preserves reasoning priors and enables efficient, high-fidelity multimodal generation and editing. LLM and Diffusion Preference Alignment. LLMs [31, 60] provide strong reasoning with autoregressive Transformers, and VLMs [3] extend them to images by projecting visual features (e.g., SigLIP [76]) into the language token space. Models"
        },
        {
            "title": "3 METHOD",
            "content": "such as Qwen [5], LLaVA [32], BLIP-2 [29], and Flamingo [1] excel at multimodal understanding but typically rely on separate diffusion backbones for image generation and editing. Preference learning has also been adapted to diffusion models, including Diffusion-DPO [62], score-space alignment (DSPO) [79], and stabilized variants such as DGPO [37] and discrete-diffusion extensions [7]. Prior work further improves controllability via additional alignment stages (e.g., multimodal reflection [65] or scalar-reward RL [48]). In contrast, UniDFlow performs reference-based multimodal preference alignment, optimizing pairwise log-likelihood margin against frozen reference model for stable, comparative supervision, improving faithfulness and controllable editing. 3. Method 3.1. Preliminaries: Discrete Flow Matching We use Discrete Flow Matching (DFM) [19] as the common objective across all training stages. DFM learns transport field in discrete spaces by mapping samples from noise to data. Let x0 qdata denote clean discrete sample (e.g., text or visual tokens), and its corrupted version at time step {0, . . . , T} genxt erated by fixed forward noising process q(xt x0, t). Given xt , flow network fθ(xt, t, c) conditioned on time and context predicts the transport toward the clean state as fθ(xt, t, c) q(x0 xt, t, c). The model is trained by minimizing token-wise categorical negative log-likelihood: x0,t,xt [ log fθ(x0 xt, t, c)] . (1) ℒDFM(θ; x0 xt, t, c) = At inference, sampling starts from xT qnoise and applies the learned flow to recover x0 . By directly estimating transport directions, DFM enables efficient few-step sampling, with conditioning via context supporting unified language modeling, visual generation, and editing. 3.2. UniDFlow We cast multimodal understanding, conditional generation, and instruction-based image editing as single discrete denoising process. Starting"
        },
        {
            "title": "3 METHOD",
            "content": "reasoning capabilities, and Stage III performs reference-based multimodal preference alignment to improve fidelity and controllability. We first describe the time-conditioned normalization used throughout the model, followed by the three training stages. 3.2.1. Time-Step Guided RMSNorm Conditioning pretrained transformer on diffusion time by directly adding time embeddings to attention or MLP activations can destabilize training by perturbing learned feature distributions. We address this with Time-Step Guided RMSNorm (TSG-RMSNorm), which injects time information by modulating the RMSNorm scale parameters rather than altering the activations themselves. This preserves pretrained representations by keeping the direction of hidden states unchanged while only applying controlled, time-dependent rescaling. Let hℓ Rd denote the input hidden state (activation vector) to the RMSNorm layer at transformer layer ℓ. Standard RMSNorm is RMSNorm(hℓ) = γℓ hℓ , RMS(hℓ) where RMS(hℓ) = + ε. Given time emj=1 h2 ℓ,j bedding e(t), we predict time-dependent modu- (s) lation for each layer, i.e., sℓ(t) = ℓ e(t), bℓ(t) = (b) e(t). We apply these to the pretrained RMSNorm ℓ parameters via 1 TSG-RMSNorm(hℓ, t) = RMSNorm(hℓ) (γℓ (1 + sℓ(t))) + bℓ(t), (2) where γℓ is the pretrained RMSNorm scale and denotes element-wise multiplication. All timemodulation parameters are zero-initialized so that sℓ(t) = 0 and bℓ(t) = 0 at initialization, exactly recovering the pretrained model. 3.2.2. Stage I: Text Alignment Unified multimodal models often entangle understanding and generation objectives, leading to representational interference and degraded reasoning. We first adapt the pretrained backbone to diffusion-style understanding through text alignment in isolation, preserving languagevisual reasoning before introducing generative training. Figure 3: Overview of Stage (understanding via text alignment) and Stage II (generation via vision alignment) of UniDFlow. from pretrained visionlanguage transformer with parameters θ0 , UniDFlow learns to recover clean token sequence from corrupted one under appropriate conditioning. For understanding, the denoised sequence corresponds to answer text tokens conditioned on an instruction and an input image x; for generation and editing, it corresponds to visual tokens conditioned on and reference image xref . To enable discrete diffusion over images, we map images to sequences of discrete visual tokens using pretrained tokenizer, and we use bidirectional self-attention to support full-context denoising. All task-specific adaptation is implemented with remains frozen. low-rank adapters (LoRA), while θ0 Our training follows three-stage pipeline (illustrated in Figs. 3 and 4): Stage aligns the pretrained visionlanguage backbone for diffusion-based multimodal understanding, Stage II adapts the model for discrete visual generation while preserving"
        },
        {
            "title": "3 METHOD",
            "content": "Figure 4: Stage III of UniDFlow: reference-based multimodal preference alignment for improved faithfulness, controllability, and editing. Given an instruction p, visual tokens x, and fully , the model premasked text token sequence ytxt,t dicts the clean answer tokens ytxt,0 using discrete flow matching. The training objective follows Eq. 1: ℒunder = ℒDFM(θu ; ytxt,0 ytxt,t, p, x), (3) are LoRAtext denotes frozen pretrained VLM paramewhere θ0 ters and θu adapters specialized for understanding. To prevent semantic drift from the pretrained language behavior, we additionally regularize the diffusion-predicted distribution with KL divergence against the autoregressive answer distribution produced by the original VLM: ℒKL = KL(pDFM(ytxt,0 ytxt,t, t, p, x) pAR(ytxt p, x)). (4) Given an instruction and corrupted visual tokens , the model predicts clean visual tokens yvis,0 yvis,t using discrete flow matching: ℒStage II = ℒDFM(θg ; yvis,0 yvis,t, t, p), (5) ( LoRAimg) is trainable, while θ0 and where only θg the understanding adapters θu are kept frozen. The diffusion process operates entirely in discrete latent space, enabling efficient sampling and seamless integration with the backbones token-based architecture. By isolating generation-specific parameters, Stage II establishes strong conditional image generation capabilities without interfering with the language and reasoning behavior learned during Stage I. This constraint anchors diffusion decoding to the pretrained linguistic manifold while allowing bidirectional attention and time-conditioned normalization to support non-autoregressive reasoning. The total Stage objective is ℒStage = ℒunder +λKLℒKL . 3.2.3. Stage II: Vision Alignment This stage adapts the same frozen backbone for conditional generation in discrete visual token space, while preserving the understanding behavior learned in the previous training stage. We keep θ0 and θu frozen and introduce separate set of LoRA adapters θg specialized for generation. 5 3.2.4. Stage III: Reference-Based Multimodal Preference Alignment While the previous stages endow UniDFlow with strong multimodal understanding and generation capabilities, token-level likelihood training cannot reliably distinguish between multiple plausible outputs that differ in instruction fidelity, visual grounding, or reasoning consistency. Stage III therefore introduces reference-based multimodal preference alignment objective that explicitly optimizes relative preferences across text, vision, and reflection, grounded in reference images. Each preference instance specifies an instruction with paired preferred(w)/rejected(l) outcomes: reference image (xw txt), ref visual tokens (yw vis), and reflection sequences (rw, rl ). This formulation allows the model to learn which multimodal outcomes are preferred, conditioned on both the instruction and the reference. ), text responses (yw ref, xl vis, yl txt, yl Mixture-of-LoRA Routing (MoRA). Since this stage optimizes preferences for both understanding and generation, naively sharing parameters can introduce objective interference, while static routing restricts adaptability. Therefore, we learn lightweight router rϕ with parameters ϕ that dynamically composes task-specific adapters based on the hidden state at diffusion step t: θu θ(t) = αt αt = rϕ(ht). + (1 αt)θg, Multimodal Preference Learning. We adopt reference-anchored Direct Preference Optimization (DPO) objective with frozen reference policy πref . For text, the loss is ℒtRefDPO = log σ (βtxt θ ) and preference margin is (6) . . III (8) (7) txt θ vis θ ref) ref) = log log = log log πθ(yl πref(yl πθ(yw πref(yw txtp,xl txtp,xl πθ( yl πref( yl text πθ( yw πref( yw jointly visp,xl ref) visp,xl ref) and txtp,xw ref) txtp,xw ref) For vision, we concatenate reflection and image tokens as yvis = (r, yvis), with the loss defined as ℒvRefDPO = log σ (βvis θ ) and preference margin visp,xw ref) visp,xw ref) aligns vision Stage objective: through + λvℒvRef-DPO, promotℒmRef-DPO = λtℒtRef-DPO ing faithful instruction following, grounded visual editing, and consistent multimodal behavior. We optimize unified objective that combines discrete flow-matching (DFM) likelihood training for three output streams: (1) text generation, ℒtext = ℒDFM(ϕ; yw txt,0 yw ref, p, t) , (2) visual editing vis,t, xw ℒedit = ℒDFM(ϕ; yw ref, p, t) , and (3) reflecref, yw , xw tion ℒrefl = ℒDFM(ϕ; rw txt,t, p, pedit, t). The final objective for stage III is: preference-augmented txt,t, xw txt,0 yw vis,t, yw 0 rw ℒStage-III = ℒtext + ℒedit + ℒrefl + ℒmRef-DPO The DFM terms maximize time-conditioned token likelihood along the discrete diffusion trajectory (9)"
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "under their respective conditionings (instruction, reference image, or edit prompt), enforcing token-level consistency. The ℒmRef-DPO term introduces comparative alignment by increasing the log-likelihood margin of preferred over rejected outputs relative to frozen reference policy πref , stabilizing training and improving cross-modal faithfulness. 4. Experiments We conduct extensive experiments to evaluate the performance of UniDFlow across six benchmarks, covering multimodal understanding, generation, In Stage I, we train using MMInand editing. struct [34] to establish strong multimodal understanding. Stage II focuses on generative capability by training on Text-to-Image-4M [27, 47, 51] . Stage III performs reference-based multimodal preference alignment with 3.5M curated preference samples under identical inputs and reference images. Dataset curation for preference alignment, training, and implementation details are provided in Appendices A-B. 4.1. Multi-Modal Understanding Table 1 reports results on the EvalVLM benchmark. Compared to strong unified hybrid baselines such as BAGEL (7B MoT), UniDFlow achieves +6.9% improvement on MME-P and +7.0% on MME-S, indicating stronger perceptual and reasoning consistency. Against EMMA (4B), UniDFlow further improves MMBench by +6.3% and MathVista by +13.3%, demonstrating superior mathematical and multi-step reasoning despite comparable model scale. Moreover, compared to the unified diffusion baseline Muddit, UniDFlow achieves an overall improvement of 12% across different understanding tasks. Finally, when compared with leading understanding-only models such as Qwen2.5-VL (7B) UniDFlow attains 20.4% higher overall performance. Additional results on OCRBenchV2 [18] can be found in Appendix D. Fig. 5 shows reasoning-based text generation examples, where UniDFlow accurately extracts information from images to respond to user queries."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "Model Params MME-P MME-S MMBench MMMU MM-Vet MathVista MMVP Qwen2.5-VL [4] BLIP-3 [69] DeepSeek-VL2 [67] Qwen3-VL [5] 3B 4B 4B 4B 7B VILA-U [66] 7B Chameleon [57] 7B Janus-Pro [10] 13B TokenFlow-XL [20] 7B BAGEL [14] 8B OmniGen-v2 [65] EMMA [24] 4B MammothModa-2 [48] 4B 4B Muddit [50] 1336 1567 1546 1687 1753 1700 2157 2388 1998 1832 UniDFlow 4B 2555 79.1 76.8 51.1 85.1 66.6 35.7 79.2 68.9 85.0 53.1 85.8 86.6 82.8 91.2 53.1 41.1 60.0 64.1 32.2 28.4 41.0 38.7 55.3 61.5 65.1 71.23 66. 74.3 61.8 62.8 72.5 27.7 8.3 50.0 40.7 67.2 73.0 79.4 76.2 82.7 62.3 39.6 73.1 75.8 81.8 79. 85.9 22.0 0.0 69.3 77.5 74.1 80.2 Table 1: Comparison of multimodal understanding performance on EvalVLMBench [17, 35, 36, 59, 74, 75] across diverse reasoning tasks. Model Params GenEval DPGBench DALL-E 3 [6] SD3-Medium [15] Qwen-Image(-RL) [16] TokenFlow-XL [20] Janus-Pro-7B [10] Bagel [14] OmniGen2/V2 [65] MammothModa-2 [48] EMMA [24] MUDDIT [50] 2B 7B+20B 14B 7B 7B+7B 3B+4B 8B+3B+2B 4B 8B UniDFlow 4B 0.67 0.74 0.91 0.55 0.80 0.88 0.78 0.87 0.93 0.90 0.95 83.50 80.43 88.32 84.19 87.74 83.57 87.20 85.63 86.37 91.19 Table 2: Overall generation performance on GenEval [21] and DPGBench [26]. Appendix provides full benchmark-wise breakdowns. surpasses generation-focused models such as QwenImage (7B+20B) by 4.0% on GenEval and 3.2% on DPGBench, despite using substantially fewer parameters. Fig. 6 (top two rows) further demonstrates that UniDFlow produces visually faithful and prompt-consistent images, accurately rendering finegrained details and background structures, which reflect strong global semantics and local visual fidelity. Subject-driven generation. Furthermore, UniDFlow supports in-context subject-driven image generation from multiple reference images, as shown in Fig. 7, without any explicit task-specific training. Given reference images and textual instruction, UniDFlow Figure 5: Multimodal reasoning from UniDFlow 4.2. Text-to-Image Generation Table 2 summarizes the performance of UniDFlow on GenEval and DPGBench for multimodal generation. On GenEval, which evaluates compositional text-to-image generation across object counting, attribute binding, and spatial reasoning, UniDFlow achieves an overall score of 0.95, outperforming strong unified baselines such as EMMA and MammothModa2 by +2.2% and +9.2%, respectively, highlighting its stronger ability to associate attributes with the correct objects under compositional constraints. similar trend is observed on DPGBench, which evaluates fine-grained prompt grounding across global understanding, attribute binding, and relational reasoning, where UniDFlow outperforms EMMA and MammothModa2 by +6.5% and +4.6%, respectively. Notably, UniDFlow also"
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "Figure 6: Qualitative comparison of compositional text-to-image generation and editing. Prompts require precise grounding of attributes and spatial relations (red text). UniDFlow consistently adheres to these constraints while maintaining realistic structure and visual fidelity, outperforming prior unified baselines in fine-grained prompt alignment. More results can be found in Appendix F. synthesizes coherent output while preserving finegrained visual details from the references. This behavior emerges from its unified multimodal optimization, which enables joint reasoning over object identity, attributes, and spatial relations. 4.3. Text-to-Image Editing Table 3 summarizes the image editing performance of UniDFlow on ImgEdit Bench [71], EmuEdit [49], and GEdit-Bench-EN [33]. On EmuEdit, UniDFlow outperforms EMMA and MammothModa2 by approximately +3.5% and +4.1%, respectively, indicating stronger semantic alignment 8 Figure 7: Subject-driven image generation with attribute editing and multi-object composition."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "Model ImgEdit Emu-Edit GEdit-Bench-EN Add Extract Remove Overall CLIP-I CLIP-Out DINO SC PQ Overall FLUX.1 Kontext-Pro [28] Bagel [14] UniWorld-v1 [30] OmniGen2 [65] Emma [24] MammothModa2 [48] 4.25 3.56 3.82 3.57 4.52 4.57 2.35 1.70 2.27 1.77 3.54 3.38 UniDFlow 4.66 4.01 3.57 2.62 3.24 3.20 4.21 3. 4.24 4.00 3.20 3.26 3.44 4.01 4.06 4.24 0.88 0.839 0.876 0.911 0.891 - 0.307 0.309 0.311 0.322 0.808 0.753 0.822 0.834 0. 7.77 7.12 7.36 6.83 4.93 7.43 7.16 6.77 7.33 7.54 7.77 7.32 6.95 6.52 4.85 6.41 6.52 6.82 0.921 0.362 0.862 8.01 7.82 7. Table 3: Text-to-image editing results. ImgEdit metric is category-wise scores, while Emu-Edit CLIPI/DINO is used for source consistency and CLIP-Out for caption alignment. GEdit-Bench-EN evaluates SC (instruction following) and PQ (perceptual quality). Editing with reasoning. Fig. 8 compares models on editing tasks requiring temporal, geometric, and physical reasoning. UniDFlow generates outputs that better reflect the intended transformations while preserving object identity, benefiting from the strong reasoning priors inherited from the pretrained VLM backbone. Fig. 6 (bottom two rows) presents additional qualitative examples, where UniDFlow produces both accurate, large-scale semantic edits (e.g., style transfer) and fine-grained object-level modifications, exhibiting strong instruction fidelity and precise edit localization. 4.4. Ablations Table 4 presents comprehensive ablation study analyzing the key design choices of UniDFlow. Model sizes. Performance improves consistently as model size increases across all benchmarks. Larger backbones provide stronger multimodal priors and improved capacity for modeling long-range dependencies, which benefits both reasoning and diffusionbased generation. Notably, even the 4B model achieves competitive performance, validating the parameter-efficient design of UniDFlow. Visual tokenizer. UniDFlow uses PyraTok [54], which performs text-guided multi-scale quantization, enabling coarse-to-fine visual representations aligned with language. In contrast, 3D-MBQ-VAE [53] and MAGVIT-v2 [72] use single-scale, visually trained tokenizers, limiting hierarchical modeling and text alignment. SweetTok [56] incorporates text semanFigure 8: Reasoning-driven image editing, highlighting temporal, geometric, and physical transformations handled by UniDFlow. between the input image, editing instruction, and edited output. On GEdit-Bench-EN, which emphasizes perceptual quality and instruction satisfaction, UniDFlow improves the averaged score by +3.7% over EMMA and +2.9% over MammothModa2. Further, on ImageEdit Bench, which evaluates diverse editing scenarios including object manipulation, background changes, style transfer, and hybrid edits, UniDFlow achieves an overall score of 4.24, surpassing EMMA (4.01) and MammothModa2 (4.06) by +5.7% and +4.4%, respectively. Notably, the largest gains are observed in Extract and Remove operations, demonstrating more precise target isolation and reduced collateral degradation. These improvements are driven by reference-based preference alignment, which encourages UniDFlow to select higher-quality edits that better satisfy user intent. 9 EvalVLM GenEval DPGBench ImgEdit UniDFlow 82.85 0.95 91.91 4.24 1. Model Size Ablation Qwen3-0.6B Qwen3-4B Qwen3-8B Qwen3-14B 3D-MBQ-VAE MAGVIT-v2 SweetTok 79.48 82.85 84.02 89.24 0.93 0.95 0.96 0.98 2. Visual tokenizer 81.27 81.19 80.76 0.92 0.91 0. 3. Architectural Ablations w/o LoRAtext w/o LoRAimg w/o MoRA Single LoRA (Und+Gen) 80.11 81.23 80.67 79.92 0.92 0.93 0.93 0.90 4. Loss Function Ablations w/o ℒvRefDPO w/o ℒtRefDPO w/o ℒmRefDPO w/o Reflection 80.45 79.45 77.34 81.23 0.91 0.91 0.86 0.89 88.32 91.91 92.56 95.44 91.43 90.34 90.44 89.33 90.05 89.88 89.12 88.44 90.32 86.23 87. 5. Stage3-Alignment Traning DPO uni-GRPO 80.12 80.88 0.92 0.93 88.82 90.07 4.19 4.24 4.26 4. 4.19 4.16 4.12 4.01 4.08 4.11 4.08 4.09 4.18 4.05 4.14 4.14 4.18 Table 4: Ablations on key UniDFlow components. tics but lacks multi-scale quantization, reducing its ability to capture coarse-to-fine structure. Removing either understandingComponents. specific or generation-specific LoRA adapters leads to noticeable degradation, confirming that separating task-specific adaptations is critical to avoid objective interference. Performance drops further when the router is removed, indicating that dynamic composition of adapters is necessary for balancing understanding and generation. Using single shared LoRA fails entirely, demonstrating that naive parameter sharing causes severe entanglement between tasks. Losses. Removing visual or text alignment losses degrades performance on corresponding benchmarks. Excluding reflection-based preference learning reduces editing and faithfulness metrics, showing that reasoning behind generation helps in precise instruction following and multimodal editing (refer to Appendix for visual results). Alignment Training. We align UniDFlow with DPO [43], uni-GRPO [70], and our mRef-DPO. Vanilla DPO can hurt when textimage tokens are weakly aligned, yielding noisy preference signals that degrade reasoning-grounded generation and edits. Uni-GRPO gives small gains but its group normaliza-"
        },
        {
            "title": "5 CONCLUSION",
            "content": "tion is unstable (especially on short prompts), reducing fine-grained edit reliability. mRef-DPO performs best by using modality-aware preference learning to stabilize cross-modal credit assignment between textual reasoning and diffusion steps, improving alignment and edit precision across metrics. 5. Conclusion We introduce UniDFlow, unified visionlanguage diffusion model that performs understanding, textto-image generation, and instruction-guided editing within single discrete flow-matching framework. We further propose mRef-DPO, reference-anchored multimodal preference objective that jointly aligns text and image outputs against frozen reference policy, improving faithfulness and controllability. Extensive results across six benchmarks show consistent gains, underscoring modality-aware preference alignment as critical for robust reasoning-grounded generation and precise visual edits."
        },
        {
            "title": "Impact Statement",
            "content": "This work presents unified multimodal generative system that combines high-level understanding with high-fidelity visual generation. Such systems can enhance accessibility, creativity, and productivity by enabling natural multimodal interaction, supporting educational and design workflows, and improving humancomputer interfaces. Our parameter-efficient training approach can also reduce computational cost compared to large-scale end-to-end retraining, potentially lowering environmental impact. At the same time, improved generation and editing capabilities introduce risks. High-quality multimodal synthesis can be misused for deceptive media manipulation, and precise editing may enable subtle alterations that are difficult to detect. Biases in pretrained visionlanguage backbones may propagate into generated outputs, leading to stereotypical or harmful representations. Our reference-based multimodal preference alignment aims to improve faithfulness and controllability by learning relative preferences under shared conditioning. This may help reduce spurious correlations and limit amplification of datasetspecific artifacts when preference data is balanced. However, alignment quality depends on the diversity and representativeness of supervision signals, and misuse risks remain. Responsible deployment should therefore include safeguards such as content moderation, bias evaluation, and transparency mechanisms (e.g., watermarking or provenance tracking)."
        },
        {
            "title": "References",
            "content": "[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: visual language model for few-shot learning. Advances in Neural Information Processing Systems (NeurIPS), 2022. [2] Jacob Austin, Daniel Johnson, Jonathan Ho, Daniel Tarlow, and Rianne Van Den Berg. Structured denoising diffusion models in discrete state-spaces. Advances in Neural Information Processing Systems (NeurIPS), 2021. [3] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023. [4] Shuai Bai, Yunfei Chu, Jinze Ding, Kai Du, Xuancheng Fan, Xingzhou Fu, Wenbin Gan, Rui Ge, Zejun Han, Guohao Huang, et al. Qwen2.5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [5] Shuai Bai, Yunfei Chu, Jinze Ding, et al. arXiv preprint Qwen3-vl technical report. arXiv:2511.21631, 2025. [6] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, et al. Improving image generation with better captions. Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf, 2023."
        },
        {
            "title": "REFERENCES",
            "content": "ment of discrete diffusion models. In ICLR 2025 Workshop on Bidirectional Human-AI Alignment, 2025. [8] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William Freeman. Maskgit: Masked generative image transformer. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022. [9] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, et al. Pixart-alpha: Fast training of diffusion transformer for photorealistic text-to-image synthesis. In International Conference on Learning Representations (ICLR), 2024. [10] Xiaokang Chen et al. Janus-pro: Unified multimodal understanding and generation with data and model scaling. arXiv preprint arXiv:2501.12921, 2025. [11] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2024. [12] Zhihong Chen, Xuehai Bai, Yang Shi, Chaoyou Fu, Huanyu Zhang, Haotian Wang, Xiaoyan Sun, Zhang Zhang, Liang Wang, Yuanxing Zhang, Pengfei Wan, and Yi-Fan Zhang. Opengpt-4o-image: comprehensive dataset for advanced image generation and editing. arXiv preprint arXiv:2509.24900, 2025. [13] Xiaoliang Dai, Ji Hou, Chih-Yao Ma, Sam Tsai, Jialiang Wang, Rui Wang, Peizhao Zhang, Simon Vandenhende, Xiaofang Wang, Abhimanyu Dubey, et al. Emu: Enhancing image generation models using photogenic needles in haystack. arXiv preprint arXiv:2309.15807, 2023. [7] Umberto Borso, Davide Paglieri, Jude Wells, and Tim Rocktäschel. Preference-based align- [14] Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Wei11 hao Yu, Xiaonan Nie, Ziang Song, et al. Emerging properties in unified multimodal pretraining. arXiv preprint arXiv:2505.14683, 2025. [15] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In International Conference on Machine Learning (ICML), 2024. [16] Chenfei Wu et al. Qwen-image technical report, 2025. [17] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, et al. Mme: comprehensive evaluation benchmark for multimodal large language models. Advances in Neural Information Processing Systems (NeurIPS), 2023. [18] Ling Fu, Zhebin Kuang, Jiajun Song, Mingxin Huang, Biao Yang, Yuzhe Li, Linghao Zhu, Qidi Luo, Xinyu Wang, Hao Lu, et al. Ocrbench v2: An improved benchmark for evaluating large multimodal models on visual text localization and reasoning. arXiv preprint arXiv:2501.00321, 2024. [19] Itai Gat, Tal Remez, Neta Shaul, Felix Kreuk, Ricky TQ Chen, Gabriel Synnaeve, Yossi Adi, and Yaron Lipman. Discrete flow matching. Advances in Neural Information Processing Systems (NeurIPS), 2024. [20] Michal Geyer, Omer Bar-Tal, Shai Bagon, and Tali Dekel. Tokenflow: Consistent diffusion features for consistent video editing. In International Conference on Learning Representations (ICLR), 2023. [21] Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating text-to-image alignment. Advances in Neural Information Processing Systems (NeurIPS), 2023."
        },
        {
            "title": "REFERENCES",
            "content": "[22] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in Neural Information Processing Systems (NeurIPS), 2014. [23] Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo Zhang, Dongdong Chen, Lu Yuan, and Baining Guo. Vector quantized diffusion model for text-to-image synthesis. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022. [24] Xin He, Longhui Wei, Jianbo Ouyang, Lingxi Xie, and Qi Tian. Emma: Efficient multimodal understanding, generation, and editing with unified architecture. arXiv preprint arXiv:2512.04810, 2025. [25] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems (NeurIPS), 2020. [26] Xiwei Hu, Rui Wang, Yixiao Fang, Bin Fu, Pei Cheng, and Gang Yu. Ella: Equip diffusion models with llm for enhanced semantic alignment. arXiv preprint arXiv:2403.05135, 2024. [27] Jackyhate. text-to-image-2m dataset (hugging face: jackyhate/text-to-image-2m), 2024. [28] Black Forest Labs, Stephen Batifol, Andreas Blattmann, Frederic Boesel, Saksham Consul, Cyril Diagne, Tim Dockhorn, Jack English, et al. Flux.1 kontext: Flow matching for in-context image generation and editing in latent space. arXiv preprint arXiv:2506.15742, 2025. [29] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping languageimage pre-training with frozen image encoders and large language models. In International Conference on Machine Learning (ICML), 2023. [30] Bin Lin, Zongjian Li, Xinhua Cheng, Yuwei Niu, Yang Ye, Xianyi He, Shenghai Yuan, Wangbo Yu, Shaodong Wang, Yunyang Ge, Yatian Pang, and Li Yuan. Uniworld-v1: Highresolution semantic encoders for unified visual understanding and generation. arXiv preprint arXiv:2506.03147, 2025. [31] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. [32] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in Neural Information Processing Systems (NeurIPS), 2023. [33] Shiyu Liu, Yucheng Han, Peng Xing, Fukun Yin, Rui Wang, Wei Cheng, Jiaqi Liao, Yingming Wang, Honghao Fu, Chunrui Han, et al. Step1x-edit: practical framework for general image editing. arXiv preprint arXiv:2504.17761, 2025. [34] Yangzhou Liu, Yue Cao, Zhangwei Gao, Weiyun Wang, Zhe Chen, Wenhai Wang, Hao Tian, Lewei Lu, Xizhou Zhu, Tong Lu, Yu Qiao, and Jifeng Dai. Mminstruct: high-quality multimodal instruction tuning dataset with extensive diversity. Science China Information Sciences, 2024. [35] Yuanzhan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an allaround player? In European Conference on Computer Vision (ECCV), 2024. [36] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyue Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. In International Conference on Learning Representations (ICLR), 2024. [37] Yihong Luo, Tianyang Hu, and Jing Tang. Reinforcing diffusion models by direct group"
        },
        {
            "title": "REFERENCES",
            "content": "preference optimization. arXiv:2510.08425, 2025. arXiv preprint [38] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. In International Conference on Machine Learning (ICML), 2021. [39] OpenAI. Chatgpt, 2022. URL https://ch at.openai.com/. Large language model, accessed 2 Jan 2026. [40] OpenAI. Gpt-4o mini: Advancing cost-efficient intelligence, 2024. [41] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. In International Conference on Learning Representations (ICLR), 2023. [42] Yusu Qian, Eli Bocek-Rivele, Liangchen Song, Jialing Tong, Yinfei Yang, Jiasen Lu, Wenze Hu, and Zhe Gan. Pico-banana-400k: large-scale dataset for text-guided image editing. arXiv preprint arXiv:2510.19808, 2025. [43] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems (NeurIPS), 2023. [44] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-toimage generation. In International Conference on Machine Learning (ICML), 2021. [45] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. Highresolution image synthesis with latent diffusion models. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022. [46] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in Neural Information Processing Systems (NeurIPS), 2022. [47] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in Neural Information Processing Systems (NeurIPS), 2022. [48] Tao Shen, Xin Wan, Taicai Chen, Rui Zhang, Junwen Pan, Dawei Lu, Fanding Lei, Zhilin Lu, Yunfei Yang, Chen Cheng, et al. Mammothmoda2: unified ar-diffusion framework for multimodal understanding and generation. arXiv preprint arXiv:2511.18262, 2025. [49] Shelly Sheynin, Adam Polyak, Uriel Singer, Yuval Kirstain, Amit Zohar, Oron Ashual, Devi Parikh, and Yaniv Taigman. Emu edit: Precise image editing via recognition and generation tasks. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2024. [50] Qingyu Shi, Jinbin Bai, Zhuoran Zhao, Wenhao Chai, Kaidong Yu, Jianzong Wu, Shuangyong Song, Yunhai Tong, Xiangtai Li, Xuelong Li, et al. Muddit: Liberating generation beyond text-to-image with unified discrete diffusion model. arXiv preprint arXiv:2505.23606, 2025. [51] Keqiang Sun, Junting Pan, Yuying Ge, Hao Li, Haodong Duan, Xiaoshi Wu, Renrui Zhang, Aojun Zhou, Zipeng Qin, Yi Wang, Jifeng Dai, Yu Qiao, Limin Wang, and Hongsheng Li. Journeydb: benchmark for generative image understanding. Advances in Neural Information Processing Systems (NeurIPS), 2023."
        },
        {
            "title": "REFERENCES",
            "content": "[52] Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Yueze Wang, Yongming Rao, Jingjing Liu, Tiejun Huang, and Xinlong Wang. Generative multimodal models are in-context learners. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2024. [53] Onkar Susladkar, Jishu Sen Gupta, Chirag Sehgal, Sparsh Mittal, and Rekha Singhal. Motionaura: Generating high-quality and motion consistent videos using discrete diffusion. In International Conference on Learning Representations (ICLR), 2024. [54] Onkar Susladkar, Tushar Prakash, Adheesh Juvekar, Kiet Nguyen, Dong-Hwan Jang, Inderjit Dhillon, and Ismini Lourentzou. Pyratok: Language-aligned pyramidal tokenizer for video understanding and generation. arXiv preprint arXiv:2601.16210, 2026. [55] Alexander Swerdlow, Mihir Prabhudesai, Siddharth Gandhi, Deepak Pathak, and Katerina Fragkiadaki. Unified multimodal discrete diffusion. arXiv preprint arXiv:2503.20853, 2025. [56] Zhentao Tan, Ben Xue, Jian Jia, Junhao Wang, Wencai Ye, Shaoyun Shi, Mingjie Sun, Wenjin Wu, Quan Chen, and Peng Jiang. Sweettok: Semantic-aware spatial-temporal tokenizer for compact video discretization. In International Conference on Computer Vision (ICCV), 2025. [57] Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. [58] Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models, 2023. [59] Shengbang Tong, Zhuang Liu, Yuexian Zhai, Yi Ma, Yann LeCun, and Saining Xie. Eyes wide shut? exploring the visual shortcomings of multimodal llms. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2024. [60] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. [61] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in Neural Information Processing Systems (NeurIPS), 2017. [62] Bram Wallace, Meihua Dang, Rafael Rafailov, Linqi Zhou, Aaron Lou, Senthil Purushwalkam, Stefano Ermon, Caiming Xiong, Shafiq Joty, and Nikhil Naik. Diffusion model alignment using direct preference optimization. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2024. [63] Xin Wang, Yuwei Zhou, Bin Huang, Hong Chen, and Wenwu Zhu. Multi-modal generative ai: Multi-modal llms, diffusions and the unification. IEEE Transactions on Circuits and Systems for Video Technology, 2024. [64] WINDop. Opengpt-4o-image dataset (hugging face: Windop/opengpt-4o-image), 2025. URL https://huggingface.co/datasets/WI NDop/OpenGPT-4o-Image. [65] Chenyuan Wu, Pengfei Zheng, Ruiran Yan, Shitao Xiao, Xin Luo, Yueze Wang, Wanli Li, Xiyan Jiang, Yexin Liu, Junjie Zhou, et al. Omnigen2: Exploration to advanced multimodal generation. arXiv preprint arXiv:2506.18871, 2025. [66] Yixiao Wu, Haotian Lin, et al. Vila-u: unified foundation model integrating visual understanding and generation. arXiv preprint arXiv:2409.04429, 2024. [67] Zhiyu Wu, Xiaokang Liu, Huaiyuan Lin, et al. Deepseek-vl2: Mixture-of-experts visionlanguage models for advanced multimodal understanding. arXiv preprint arXiv:2412.10302, 2024."
        },
        {
            "title": "REFERENCES",
            "content": "Junhao [68] Jinheng Xie, Weijia Mao, Zechen Bai, Zhang, Weihao Wang, David Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding and generation. In International Conference on Learning Representations (ICLR), 2024. [69] Le Xue, Manli Shu, Evan Shelhamer, Haotian He, Wang Wild, Ran Xu, et al. xgen-mm (blip3): family of open large multimodal models. arXiv preprint arXiv:2408.08872, 2024. [70] Ling Yang, Ye Tian, Bowen Li, Xinchen Zhang, Ke Shen, Yunhai Tong, and Mengdi Wang. Mmada: Multimodal large diffusion language models. arXiv preprint arXiv:2505.15809, 2025. [71] Yang Ye, Xianyi He, Zongjian Li, Bin Lin, Shenghai Yuan, Zhiyuan Yan, Bohan Hou, and Li Yuan. Imgedit: unified image editing dataset and benchmark. arXiv preprint arXiv:2505.20275, 2025. [72] Lijun Yu, Jose Lezama, Nitesh Bharadwaj Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng, Agrim Gupta, Xiuye Gu, Alexander Hauptmann, et al. Language model beats diffusion-tokenizer is key to visual generation. In International Conference on Learning Representations (ICLR), 2024. [73] Qifan Yu, Wei Chow, Zhongqi Yue, Kaihang Pan, Yang Wu, Xiaoyang Wan, Juncheng Li, Siliang Tang, Hanwang Zhang, and Yueting Zhuang. Anyedit: Mastering unified high-quality image In IEEE Conference on editing for any idea. Computer Vision and Pattern Recognition (CVPR), 2024. [74] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities. In International Conference on Machine Learning (ICML), 2024."
        },
        {
            "title": "REFERENCES",
            "content": "[75] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2024. [76] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2023. [77] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-toimage diffusion models. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2023. [78] Yu Zhong, Tianwei Lin, Ruike Zhu, Yuqian Yuan, Haoyu Zheng, Liang Liang, Wenqiao Zhang, Feifei Shao, Haoyuan Li, Wanggui He, et al. Unified personalized understanding, generating and editing. arXiv preprint arXiv:2601.06965, 2026. [79] Huaisheng Zhu, Teng Xiao, and Vasant Honavar. Dspo: Direct score preference optimization for diffusion model alignment. In International Conference on Learning Representations (ICLR), 2025."
        },
        {
            "title": "REFERENCES",
            "content": "Figure 9: Image Generation with UniDFlow 17 Hyperparam Setting Stage 1 Stage 2 Stage GPUs Batch / GPU Optimizer Init LR LR schedule Warmup steps Train steps Grad accumulation Max grad norm Weight decay Diffusion steps Classifier-free guidance Resolution Aspect ratios Max seq length Precision GPU-Hours 32A100 (80GB) 8 AdamW 5 110 Cosine 200 10K 6 2.0 0 40 8 2241024 1:1, 4:3, 3:4 2048 BF16 256 32H100 (80GB) 6 AdamW 5 510 Linear 1000 25K 4 1.0 110 50 8 256/512/768/1024 1:1, 16:9, 9:16 2048 BF16 320 2 48H100 (80GB) 4 AdamW 5 210 Cosine 1200 30K 4 2.0 110 50 12 2241280 1:1, 4:3, 3:4, 16:9, 9:16 4096 BF16 528 Table 5: Training setup by stage. Stages 13 correspond to instruction tuning, visual generation, and joint understanding/alignment. A. Implementation Details We employ three-stage training pipeline  (Table 5)  that progressively builds (i) visual instructionfollowing capability, (ii) high-fidelity visual generation, and (iii) joint multimodal understanding and alignment. Across all stages, we use AdamW optimization with mixed-precision training and gradient clipping to stabilize training at scale. Stage 1: Text Alignment We first perform supervised fine-tuning to teach the model to follow visual instructions and ground text responses in images. To improve robustness to real-world inputs, we train with variable aspect ratios and variable image resolutions, enabling the model to generalize across diverse image formats. The learning-rate schedule uses warmup phase followed by cosine annealing for stable convergence. Stage II: Visual Alignment Next, we train the model for visual generation using diffusion-based objective. We train at multiple resolutions (with variable aspect ratios) to encourage both global structure and fine detail, and use linear learning-rate schedule with longer warmup to support stable optimization under the generative objective. Regularization is applied via weight decay to improve generalization. Reference-Based Multimodal Preference Alignment Finally, we jointly optimize understanding and alignment, combining multimodal comprehension with aligned outputs. We expand the image-"
        },
        {
            "title": "B TRAINING DATA",
            "content": "Figure 10: Inference throughput versus parameter count (in billions) for representative baselines and our model family. Higher throughput (right) is better, while fewer parameters (down) are more compact. resolution range further and increase the maximum sequence length to support longer-context reasoning over visual content. This stage uses cosine-annealed schedule with warmup and moderate regularization, aiming to consolidate gains from the first two stages while maintaining training stability at scale. Throughputsize trade-off. Figure 10 summarizes the empirical efficiency landscape by plotting inference throughput against model size for set of representative systems (Janus-Pro [10], OmnigenV2 [65], Bagel [14], MammothModa2 [48], EMMA [24], and MUDDIT [50]) and our variants at 0.7B, 4B, 8B, and 14B parameters. Rather than exhibiting strictly monotonic dependence on parameter count, the scatter shows substantial dispersion across independently implemented models, indicating that architectural choices and inference stacks materially affect end-toend throughput beyond raw scale. In the large-model regime (14B), UniDFlow-14B attains the highest throughput among the compared methods, outperforming other models of similar size (e.g., Bagel and Janus-Pro), suggesting improved runtime efficiency at scale. At intermediate sizes (79B), UniDFlow8B is competitive with contemporaneous baselines, while the smaller UniDFlow-0.7B and UniDFlow-4B provide lightweight operating points that prioritize compactness with correspondingly lower throughput. B. Training Data We employ three-stage data curriculum that progressively transitions from supervised multimodal inSt. Obj. # Tok. 1 2 3 SFT (MMInstruct) 1.0M 0.6T 4.5M 1.2T T2I (refined) 3.5M 1.8T Pref align. Total 3.6T Table 6: Stage-wise data (image+text tokens). struction learning to large-scale image-text pretraining and finally preference-based alignment for unified understanding, generation, and editing. Table 6 provides summary of the data used in training. Token accounting. Throughout this work, the reported token counts include both text tokens and discretized/embedded image tokens as consumed by the multimodal sequence interface (i.e., the effective sequence length seen by the transformer). We report aggregate tokens per stage. In Stage 1, we Stage 1: Text Alignment. initialize instruction-following behavior using MMInstruct [34], high-quality multimodal instruction tuning dataset spanning diverse domains and instruction types. We use approximately 1.0M imagepromptanswer (MMInstruct reports 973K instructions [34]) and train for 0.6T total (image+text) tokens. examples Stage 2: Visual Alignment. Stage 2 focuses on textto-image generative pretraining to improve prompt adherence, compositional generalization, and broad visual coverage. We sample total of 4.5M images (with associated text prompts/captions) from three sources: (i) 1.5M from LAION-5B [47], (ii) 1.0M from JourneyDB [51], and (iii) 2.0M from the jackyhate/text-to-image-2M collection on Hugging Face [27]. We refine and normalize the paired text using proprietary LLM-based caption/prompt rewriting pipeline to reduce noise and increase instruction clarity, and train for 1.2T total (image+text) tokens. Stage III: Reference-Based Multimodal Preference Alignment. Stage 3 aligns the model to high-quality, instruction-faithful outputs in our unified data format for (a) multimodal understanding, (b) image generation, and (c) image editing. We curate 3.5M base tasks by aggregating: OpenGPT-4o-Image"
        },
        {
            "title": "C EXTENDED RELATED WORK",
            "content": "(80K) [12, 64], AnyEdit-derived edits (3.0M; AnyEdit reports 2.5M editing pairs) [73], and Pico-Banana-400K (400K) [42]. We then convert these tasks into high-quality preference dataset via rejection-sampling style annotation using proprietary multimodal LLMs. For each edit instance, we generate and store (i) reflection traces with positive:negative ratio of 3 6, and (ii) paired instruction/response candidates with positive:negative ratio 4 10 (stored as accepted vs. rejected candidates in our format). Stage 3 consumes 1.8T total tokens. C. Extended Related Work Diffusion for Visual Generation. Diffusion probabilistic models (DPMs) [25, 38, 46] surpass GANs [22] in stability and generation quality, but are computationally expensive due to pixel-space diffusion. Latent diffusion models (LDMs) [45] mitigate this cost by operating in compressed latent space and achieve strong text-to-image performance [9, 41, 77]. However, continuous Gaussian diffusion is well-suited for images but less natural for discrete modalities such as text. Discrete diffusion [2] addresses this by using categorical corruption (e.g., masking), motivating image generators that replace autoregressive decoding with parallel mask-and-predict refinement, improving both fidelity and latency [8, 23]. LLMs and VLMs for Understanding Large language models (LLMs) [31, 60] have achieved strong zero-shot reasoning and instruction-following by autoregressively generating tokens with decoder-only Transformers [61]. Inspired by their success, visionlanguage models (VLMs) [3, 4] extend LLMs to visual inputs by coupling vision encoder (e.g., SigLIP [76]) with language model via lightweight projection layers, treating images as sequences of visual tokens. Models such as Qwen [5], LLaVA [32], BLIP-2 [29], and Flamingo [1] enable strong visual understanding (e.g., captioning, VQA), but treat vision as read-only and rely on separate diffusion models for image generation. Beyond likelihood training, preference alignment has been extended from LLMs to diffusion models using"
        },
        {
            "title": "D SCENE TEXT REASONING AND RECOGNITION",
            "content": "Table 7: Evaluation of existing VLMs/MLLMs on English tasks of OCRBench v2 [18] public data. Recognition, Referring, Spotting, Extraction, Parsing, Calculation, Understanding, and Reasoning refer to text recognition, text referring, text spotting, relation extraction, element parsing, mathematical calculation, visual text understanding, and knowledge reasoning, respectively. Higher values indicate better performance."
        },
        {
            "title": "Method",
            "content": "Recog. Ref. Spot. Extr. Pars. Calc. Und. Reas. Avg. 68.8 25.7 1.2 80.2 30.4 38.2 73.2 56.2 46.7 Qwen2.5-VL-7B [4] 67.3 36.9 11.2 89.0 38.4 38.4 79.2 60.5 52.6 InternVL3-14B [11] 77.5 36.3 43.4 71.1 55.5 46.5 61.2 26.7 0.0 GPT-4o [39] 57.9 23.3 0.6 70.8 31.5 38.8 65.9 55.1 43.0 GPT-4o-mini [40] 61.2 39.5 13.5 79.3 39.2 47.7 75.5 59.3 51.9 Gemini-pro [58] 64.4 38.2 5.7 91.03 37.8 44.2 76.8 62.6 55.7 Qwen3-VL-8B [5] 61.3 36.5 2.4 87.23 33.4 40.7 72.7 65.7 48.7 OmniGenv2 [65] 65.8 37.1 3.3 90.45 38.5 41.3 75.2 66.4 52.2 Begal [14] 66.7 36.5 6.7 91.3 37.5 44.5 76.7 67.2 53.8 Emma [24] 64.9 38.4 13.7 92.6 34.5 49.4 78.3 66.1 54.7 Muddit [50] MammothModa2 [48] 68.2 39.5 11.4 92.2 39.1 50.2 80.2 68.1 56.1 UniDFlow-4B UniDFlow-8B UniDFlow-14B 69.9 41.2 12.9 94.1 42.2 53.4 83.1 70.8 58.4 72.2 43.8 14.9 95.0 45.7 55.1 85.9 73.5 60.7 76.7 47.1 16.5 96.9 48.4 58.1 88.7 77.1 63.8 DPO-style objectives [43]. Diffusion-DPO [62] directly fine-tunes text-to-image models on pairwise human judgments via likelihood-based preference loss, while DSPO [79] instead aligns the diffusion score function in score space, staying closer to the original training objective. Subsequent variants such as DGPO [37] improve stability through group-wise preference optimization, and recent work further generalizes DPO-style alignment to discrete diffusion processes [7], bridging continuous and categorical diffusion frameworks. Unified Models for Understanding and Generation Diffusion models and visionlanguage models excel at generation and semantic understanding, respectively, motivating unified architectures. To improve generation and editing, recent models add additional alignment stages. OmniGen2 [65] employs multimodal reflection for self-correction, while MammothModa2 [48] applies reinforcement learning with scalar rewards (e.g., OCR and aesthetic scores). In contrast, UniDFlow introduces reference-based preference alignment across text and vision with reflection, enabling stable and faithful generation and editing. D. Scene Text Reasoning and Recognition Table 7 reports accuracy on eight visual reasoning subtasks, Recognition, Referring, Spotting, Extraction, Parsing, Calculation, Understanding, and Reasoning, together with their macro Average on OCRBenchV2 [18]. The compared systems include strong understanding-focused VLMs (e.g., Qwen2.5-VL [4], InternVL [11]), unified understandinggeneration models (e.g., EMMA [24], BEGAL [14], Muddit [50], MammothModa2 [48]), and proprietary multimodal assistants (e.g., GPT-4o [39], Gemini-Pro [58]). Fig. 18 shows that robustness UniDFlow on complex understanding. This evaluation is particularly diagnostic because it separates perceptual grounding (Recognition/Spotting/Extraction), structured interpretation (Parsing/Calculation), and holistic inference (Understanding/Reasoning). Across model scales, our unified model family consistently dominates the subtask profile, with performance improving monotonically from Ours-4B Ours-8B Ours-14B. Concretely, Ours-14B achieves the best overall Average = 63.8, improving over the strongest baseline (MammothModa2, 56.1) by +7."
        },
        {
            "title": "E FULL QUANTITATIVE RESULTS ON GENEVAL AND DPGBENCH",
            "content": "Model Params Single Obj Two Obj Counting Colors Position Color Attr DALL-E 3 [6] SD3-Medium [15] Qwen-Image [16] TokenFlow-XL [20] Janus-Pro-7B [10] Bagel [14] OmniGen2/V2 [65] MammothModa-2 [48] EMMA [24] MUDDIT [50] UniDFlow 2B 7B+20B 14B 7B 7B+7B 3B+4B 8B+3B+2B 4B 7B 4B 0.96 0.99 1.00 0.95 0.99 0.98 0.95 1.00 1.00 0.95 1.00 0.87 0.94 0.95 0.60 0.89 0.95 0.93 0.97 0.99 0.93 0. 0.47 0.72 0.93 0.41 0.59 0.84 0.64 0.63 0.87 0.85 0.89 0.83 0.89 0.92 0.81 0.90 0.95 0.81 0.89 0.98 0.96 0.98 0.43 0.33 0.87 0.16 0.79 0.78 0.73 0.90 0.86 0.82 0. 0.45 0.60 0.83 0.24 0.66 0.77 0.74 0.82 0.87 0.84 0.93 Table 8: Evaluation of text-to-image generation ability on GenEval benchmark. Model Params Global Entity Attribute Relation Other DALL-E 3 [6] SD3-Medium [15] Qwen-Image [16] TokenFlow-XL [20] Janus-Pro-7B [10] Bagel [14] OmniGen2/V2 [65] MammothModa-2 [48] EMMA [24] MUDDIT [50] 2B 7B + 20B 14B 7B 7B + 7B 3B + 4B 8B + 3B + 2B 4B 7B 90.97 87.92 91.32 87.33 86.91 89.42 81.16 91.24 89.42 89.61 91.01 91.56 88.54 88.95 91.43 92.99 91.71 90.47 89.39 88.48 92.02 89.01 89.43 90.42 86.43 90.16 90.59 89. 90.58 80.72 94.31 85.09 90.02 92.34 91.23 94.35 92.23 90.72 89.83 86.81 92.73 86.55 89.48 88.78 84.81 90.02 88.63 UniDFlow 4B 93.42 94. 95.34 95.03 93.86 Table 9: Quantitative evaluations of the text-to-image generation capacity on DPG-Bench. points. Gains are broad rather than concentrated in single capability: relative to the best previous work in every column, Ours-14B improves Recognition (76.7; +7.9), Referring (47.1; +7.6), Spotting (16.5; +2.8), Extraction (96.9; +4.3), Parsing (48.4; +9.2), Calculation (58.1; +7.9), Understanding (88.7; +8.5), and Reasoning (77.1; +9.0). The largest deltas occur in Parsing and Reasoning, suggesting that the proposed approach strengthens compositional/structured visual reasoning beyond raw perception. second takeaway is that even the compact variant (Ours-4B) is competitive with or better than substantially larger baselines: it reaches 58.4 Avg., exceeding MammothModa2 (56.1) and Muddit (54.7) while also improving the hardest reasoning-heavy columns (Calc. = 53.4, Reas. = 70.8). This aligns with the papers core design choice: rather than entangling understanding and generation in shared parameters, the method trains separate lightweight adapters for understanding vs. generation and combines them with learned router, reducing objective interference and preserving specialization. E. Full Quantitative Results on GenEval and DPGBench The main paper reports the overall performance of UniDFlow on GenEval and DPGBench. Here, we provide the complete attribute-wise breakdown used by both benchmarks. Tables 8 and 9 show that UniDFlow achieves the best global score and consistently improves across fine-grained categories (e.g., entity, attribute, relation) as well as compositional criteria (single/two-object, counting, color, position, and color-attribute). These results indicate that the gains"
        },
        {
            "title": "F ADDITIONAL RESULTS",
            "content": "are not driven by single subset of prompts; instead, UniDFlow improves performance uniformly across evaluation dimensions, reflecting stronger textimage alignment and more reliable adherence to structured constraints. F. Additional Results Ablation on training tokens and LoRA rank. We study the effect of (i) the total number of pre-training tokens (image+text) and (ii) the LoRA rank used for adaptation. Figure 12 shows consistent improvement as we scale the training budget from 0.5T to 3T tokens, yielding substantial gains across TextGen, GenEval [21], DPGBench [26], and ImgEditBench [71]. We also ablate the LoRA rank and observe that increasing the rank from 8 to 32 produces the largest marginal improvement across all benchmarks, indicating that low ranks under-parameterize the adaptation. Beyond rank 32, performance improvements diminish and largely saturate up to rank 128, suggesting the adaptation becomes capacitysufficient. Based on this accuracy-efficiency trade-off, we use default LoRA rank of 64 in all experiments. Ablation on Stage-III losses. Fig 14 presents qualitative examples for image editing without StageIII alignment losses. Removing any single loss term degrades instruction-following and visual realism: in text-to-image generation, w/o variants show noticeable drift in composition and reduced coherence, while in image editing, they yield weaker target edits (e.g., less natural object replacement and less consistent candle ignition) and poorer photometric integration with the original scene. In contrast, UniDFlow produces the most faithful and visually consistent results across all three prompts, indicating that StageIII losses are complementary and jointly necessary for robust alignment."
        },
        {
            "title": "F ADDITIONAL RESULTS",
            "content": "Figure 11: Image editing examples on complex scenarios. Figure 12: Analysis based on the number of training tokens and LoRA rank used during training."
        },
        {
            "title": "F ADDITIONAL RESULTS",
            "content": "Figure 13: Image editing on complex scenarios."
        },
        {
            "title": "F ADDITIONAL RESULTS",
            "content": "Figure 14: Visual ablation based on Stage-III alignment losses."
        },
        {
            "title": "F ADDITIONAL RESULTS",
            "content": "Figure 15: Zero-shot multi-subject reasoning-based editing."
        },
        {
            "title": "F ADDITIONAL RESULTS",
            "content": "Figure 16: Zero-shot multi-subject reasoning-based editing."
        },
        {
            "title": "F ADDITIONAL RESULTS",
            "content": "Figure 17: Text-to-Image generation comparison with baselines."
        },
        {
            "title": "F ADDITIONAL RESULTS",
            "content": "Figure 18: Image understanding and reasoning with complex scenes."
        },
        {
            "title": "F ADDITIONAL RESULTS",
            "content": "Figure 19: Image-to-text generated results with UniDFlow."
        },
        {
            "title": "F ADDITIONAL RESULTS",
            "content": "Figure 20: Image-to-text generated results with UniDFlow."
        },
        {
            "title": "F ADDITIONAL RESULTS",
            "content": "Figure 21: Additional complex reasoning tasks with UniDFlow. Figure 22: Additional complex reasoning tasks with UniDFlow."
        },
        {
            "title": "F ADDITIONAL RESULTS",
            "content": "Dataset curation prompt used in our pipeline You are an expert data curator for multimodal image-editing instruction datasets. Inputs: 1) SOURCE_IMAGE: the original image 2) RAW_INSTRUCTION: the original instruction text 3) EDITED_IMAGE: the ground-truth edited image Tasks: 1) Produce clean training fields. 2) Create preference pairs by writing one \"chosen\" output and >=3 \"rejected\" alternatives (plausible but worse). Rules: - Preserve intent; if ambiguous, pick the interpretation that matches EDITED_IMAGE. - Be consistent with what is visible in SOURCE_IMAGE and EDITED_IMAGE; do not invent unseen details. - Be explicit: what changes, where, how much/style constraints, and what must remain unchanged if relevant. - Reflection describes the final edited image relative to the original; only observable outcomes. Rejected alternatives (>=3): - Each must be realistic mistake (not nonsense) and include: - worse tuned instruction - worse edit directive - worse reflection of the incorrect outcome - \"why_rejected\" explaining what is wrong vs the chosen - \"negative_type\" from: under-edit, over-edit, wrong-attribute, wrong-region, unwanted-add/remove - Include at least one near-miss (almost correct but subtly wrong: intensity, shade, lighting, missed constraint). Output JSON (return ONLY valid JSON; exactly these keys): { \"prompt\": string, \"image\": \"SOURCE_IMAGE\", \"answer_instruction_tuned\": string, \"edit_instruction_for_image\": string, \"edited_image\": \"EDITED_IMAGE\", \"reflection_of_edited_image\": string, \"preference_data\": { \"prompt\": string, \"chosen\": { \"answer_instruction_tuned\": string, \"edit_instruction_for_image\": string, \"reflection_of_edited_image\": string }, \"rejected\": [ { \"answer_instruction_tuned\": string, \"edit_instruction_for_image\": string, \"reflection_of_edited_image\": string, \"why_rejected\": string, \"negative_type\": string } ] } } Now process: SOURCE_IMAGE: <<IMAGE>> RAW_INSTRUCTION: <<INSTRUCTION_TEXT>> EDITED_IMAGE: <<EDITED_IMAGE>>"
        }
    ],
    "affiliations": [
        "Google",
        "Indian Institute of Technology Roorkee",
        "University of Illinois Urbana-Champaign",
        "University of Texas at Austin"
    ]
}