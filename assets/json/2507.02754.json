{
    "paper_title": "Fast and Simplex: 2-Simplicial Attention in Triton",
    "authors": [
        "Aurko Roy",
        "Timothy Chou",
        "Sai Surya Duvvuri",
        "Sijia Chen",
        "Jiecao Yu",
        "Xiaodong Wang",
        "Manzil Zaheer",
        "Rohan Anil"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent work has shown that training loss scales as a power law with both model size and the number of tokens, and that achieving compute-optimal models requires scaling model size and token count together. However, these scaling laws assume an infinite supply of data and apply primarily in compute-bound settings. As modern large language models increasingly rely on massive internet-scale datasets, the assumption that they are compute-bound is becoming less valid. This shift highlights the need for architectures that prioritize token efficiency. In this work, we investigate the use of the 2-simplicial Transformer, an architecture that generalizes standard dot-product attention to trilinear functions through an efficient Triton kernel implementation. We demonstrate that the 2-simplicial Transformer achieves better token efficiency than standard Transformers: for a fixed token budget, similarly sized models outperform their dot-product counterparts on tasks involving mathematics, coding, reasoning, and logic. We quantify these gains by demonstrating that $2$-simplicial attention changes the exponent in the scaling laws for knowledge and reasoning tasks compared to dot product attention."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 ] . [ 1 4 5 7 2 0 . 7 0 5 2 : r FAST AND SIMPLEX: 2-SIMPLICIAL ATTENTION IN TRITON Aurko Roy Meta Menlo Park, CA roy.aurko@gmail.com Timothy Chou Meta Menlo Park, CA timchou@meta.com Sai Surya Duvvuri Department of Computer Science University of Texas at Austin saisurya@cs.utexas.edu Sijia Chen Meta Menlo Park, CA sijiac@meta.com Jiecao Yu Meta Menlo Park, CA jiecaoyu@meta.com Xiaodong Wang Meta Menlo Park, CA xdwang@meta.com Manzil Zaheer Meta Menlo Park, CA manzilzaheer@meta.com Rohan Anil San Francisco, CA rohan.anil@gmail.com"
        },
        {
            "title": "ABSTRACT",
            "content": "Recent work has shown that training loss scales as power law with both model size and the number of tokens, and that achieving compute-optimal models requires scaling model size and token count together. However, these scaling laws assume an infinite supply of data and apply primarily in compute-bound settings. As modern large language models increasingly rely on massive internet-scale datasets, the assumption that they are compute-bound is becoming less valid. This shift highlights the need for architectures that prioritize token efficiency. In this work, we investigate the use of the 2-simplicial Transformer, an architecture that generalizes standard dot-product attention to trilinear functions through an efficient Triton kernel implementation. We demonstrate that the 2-simplicial Transformer achieves better token efficiency than standard Transformers: for fixed token budget, similarly sized models outperform their dot-product counterparts on tasks involving mathematics, coding, reasoning, and logic. We quantify these gains by demonstrating that 2-simplicial attention changes the exponent in the scaling laws for knowledge and reasoning tasks compared to dot product attention."
        },
        {
            "title": "INTRODUCTION",
            "content": "Large language models (LLMs) based on the Transformer architecture (Vaswani et al., 2017) have become foundational to many state-of-the-art artificial intelligence systems, including GPT-3 (Brown et al., 2020), GPT-4 (Achiam et al., 2023), Gemini (Team et al., 2023), and Llama (Touvron et al., 2023). The remarkable progress in scaling these models has been guided by neural scaling laws (Hestness et al., 2017; Kaplan et al., 2020; Hoffmann et al., 2022), which empirically establish power-law relationship between training loss, the number of model parameters, and the volume of training data. key insight from this body of work is that optimal model performance is achieved not simply by increasing model size, but by scaling both the number of parameters and the amount of training data in tandem. Notably, Hoffmann et al. (2022) demonstrate that compute-optimal models require balanced scaling approach. Their findings show that the Chinchilla model, with 70 billion parameters, outperforms the much larger Gopher model (280 billion parameters) by being trained on four times Work done during an internship at Meta Work done while at Meta 1 as much data. This result underscores the importance of data scaling alongside model scaling for achieving superior performance in large language models. As artificial intelligence (AI) continues to advance, significant emerging challenge is the availability of sufficiently high-quality tokens. As we approach this critical juncture, it becomes imperative to explore novel methods and architectures that can scale more efficiently than traditional Transformers under limited token budget. However, most architectural and optimizer improvements merely shift the error but do not meaningfully change the exponent of the power law (Everett, 2025). The work of Kaplan et al. (2020); Shen et al. (2024) showed that most architectural modifications do not change the exponent, while Hestness et al. (2017) show similar result for optimizers. The only positive result has been on data due to the works of Sorscher et al. (2022); Bahri et al. (2024); Brandfonbrener et al. (2024) who show that changing the data distribution can affect the exponent in the scaling laws. In this context we revisit an old work Clift et al. (2019) which generalizes the dot product attention of Transformers to trilinear forms as the 2-simplicial Transformer. We explore generalizations of RoPE (Su et al., 2024) to trilinear functions and present rotation invariant trilinear form that we prove is as expressive as 2-simplicial attention. We further show that the 2-simplicial Transformer scales better than the Transformer under limited token budget: for fixed number of tokens, similar sized 2-simplicial Transformer out-performs the Transformer on math, coding and reasoning tasks. Furthermore, our experiments also reveal that the 2-simplicial Transformer has more favorable scaling exponent corresponding to the number of parameters than the Transformer (Vaswani et al., 2017). This suggests that, unlike Chinchilla scaling (Hoffmann et al., 2022), it is possible to increase tokens at slower rate than the parameters for the 2-simplicial Transformer. Our findings imply that, when operating under token constraints, the 2-simplicial Transformer can more effectively approach the irreducible entropy of natural language compared to dot product attention Transformers."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Several generalizations of attention have been proposed since the seminal work of Vaswani et al. (2017). line of work that started immediately after was to reduce the quadratic complexity of attention with sequence length. In particular, the work of Parmar et al. (2018) proposed local attention in the context of image generation and several other works subsequently used it in conjunction with other methods for language modeling (Zaheer et al., 2020; Roy et al., 2021). Other work has proposed doing away with softmax attention altogether - e.g., Katharopoulos et al. (2020) show that replacing the softmax with an exponential without normalization leads to linear time Transformers using the associativity of matrix multiplication. Other linear time attention work are state space models such as Mamba (Gu & Dao, 2023); however these linear time attention methods have received less widespread adoption due to their worse quality compared to Transformers in practice. According to Allen (2025), the key factor contributing to Mambas success in practical applications is the utilization of the conv1d operator; see also So et al. (2021) and Roy et al. (2022) for similar proposals to the Transformer architecture. The other end of the spectrum is going from quadratic to higher order attention. The first work in this direction to the best of our knowledge was 2-simplicial attention proposed by Clift et al. (2019) which showed that it is good proxy for logical problems in the context of deep reinforcement learning. similar generalization of Transformers was proposed in Bergen et al. (2021) which proposed the Edge Transformer where the authors proposed triangular attention. The AlphaFold (Jumper et al., 2021) paper also used an attention mechanism similar to the Edge Transformer which the authors called triangle self-attention induced by the 2D geometry of proteins. Higher order interactions were also explored in Wang et al. (2021) in the context of recommender systems. Recent work by Sanford et al. (2023) shows that the class of problems solved by an n-layer 2-simplicial Transformer is strictly larger than the class of problems solved by dot product attention Transformers. In particular, the authors define class of problems referred to as Match3 and show that dot product attention requires exponentially many layers in the sequence length to solve this task. Follow up work by Kozachinskiy et al. (2025) propose scalable approximation to 2-simplicial attention and prove lowerbounds between Strassen attention and dot product attention on tasks that require more complex reasoning using VC dimension (Vapnik, 1968) arguments. Also related is work on looping Transformer layers (Dehghani et al., 2018) as in Universal Transformers; see also Yang et al. (2023); Saunshi et al. (2025) for more recent treatment of the same idea. 2 Both higher order attention and looping serve similar purpose: compute more expressive function per parameter. It has been established in these works that looped Transformers are better at logical reasoning tasks. key challenge in scaling looped Transformers to larger models is their trainability. Specifically, looping times increases the model depth by factor of k, which can significantly exacerbate the difficulties associated with training deeper models. As result, it remains unclear how well large looped Transformers can be trained, and further research is needed to address this concern. Notation. We use small and bold letters to denote vectors, capital letters to denote matrices and tensors and small letters to denote scalars. We denote a, to denote dot product between two vectors and b. Similarly, the trilinear dot product is denoted as follows:a, b, = (cid:80)d i=1ai, bi, ci. We use @ to highlight matrix multiplication, for e.g., (AB)@C, for matrices A, B, C. To denote array slicing, we use a[l : + m] = (al, . . . , al+m1) with zero-based indexing. Some tensor operations are described using Einstein summation notation as used in the Numpy library (Harris et al., 2020). We use LOP to denote floating point operations. Column stacking of arrays are denoted by [a, b, c]. We use det to denote determinant of square matrix."
        },
        {
            "title": "3 OVERVIEW OF NEURAL SCALING LAWS",
            "content": "In this section we provide brief overview of neural scaling laws as introduced in Kaplan et al. (2020). We will adopt the approach outlined by Hoffmann et al. (2022), which proposes that the loss L(N, D) decays as power law in the total number of model parameters and the number of tokens D: L(N, D) = + α + Dβ . (1) The first term is often described as the irreducible loss which corresponds to the entropy of natural text. The second term captures the fact that model with parameters underperforms this ideal generative process. The third term corresponds to the fact that we train on only finite sample of the data and do not train the model to convergence. Theoretically, as and large language model should approach the irreducible loss of the underlying text distribution. For given compute budget where LOP s(N, D) = C, one can express the optimal number of parameters as Nopt and the optimal dataset size as Dopt b. The authors of Hoffmann et al. (2022) perform several experiments and fit parametric functions to the loss to estimate the exponents and b: multiple different approaches confirm that roughly 0.49 while 0.5. This leads to the central thesis of Hoffmann et al. (2022): one must scale the number of tokens proportionally to the model size. However, as discussed in Section 1, the quantity of sufficiently high-quality tokens is an emerging bottleneck in pre-training scaling, necessitating an exploration of alternative training algorithms and architectures. On the other hand recent studies have shown that most modeling and optimization techniques proposed in the literature merely shift the error (offset E) and do not fundamentally change the exponent in the power law. We refer the readers to this excellent discussion in Everett (2025)."
        },
        {
            "title": "4 THE 2-SIMPLICIAL TRANSFORMER",
            "content": "The 2-simplicial Transformer was introduced in Clift et al. (2019) where the authors extended the dot product attention from bilinear to trilinear forms, or equivalently from the 1-simplex to the 2-simplex. Let us recall the attention mechanism in standard Transformer (Vaswani et al., 2017). Given sequence Rnd we have three projection matrices WQ, WK, WV Rdd which we refer to as the query, key and value projections respectively. These projection matrices are used to infer the query = XWQ, key = XWK and value = XWV respectively. This is then used to construct the attention logits: Rnn, = QK / (2) which are both entries in Rd . The attention where each entry is dot product Aij = qi, kj/ scores (logits) are then transformed into probability weights by using row-wise softmax operation: Sij = exp (Aij)/ (cid:88) exp (Aij). (3) j=1 3 i (a) 1-simplex between two nodes i, (b) 2-simplex between three nodes i, j, Figure 1: Geometry of dot product attention and 2-simplical attention. The final output of the attention layer is then linear combination of the values according to these attention scores: (cid:88) vi ="
        },
        {
            "title": "Aijvj",
            "content": "(4) j=1 The 2-simplicial Transformer paper Clift et al. (2019) generalizes this to trilinear products where we have two additional key and value projection matrices WK and WV , which give us = XWK and = XWV . The attention logits for 2-simplicial Transformer are then given by the trilinear product between Q, and , resulting in the following third-order tensor: A(2s) ijk = qi, kj, k = 1 (cid:88) l= QilKjlK kl, so that the attention tensor becomes: ijk = exp(A(2s) S(2s) ijk )/ (cid:88) exp(A(2s) ijk ), with the final output of the attention operation being defined as j,k v(2s)(i) = (cid:88) j,k= S(2s) ijk (vj k) , (5) (6) (7) represents the element wise Hadamard product between two vectors in Rd. The where vj pseudo-code for 2-simplicial attention is depicted in Algorithm 1. Note that Equation 5 does not incorporate any position encoding such as RoPE (Su et al., 2024); we discuss this in the next section. Algorithm 1 Pseudocode for the forward pass of 2-simplicial attention 1: procedure 2-SIMPLICIAL ATTENTION(Q, K, , , ) 2: 3: 4: 5: 6: end procedure logits einsum(btnh, bsnh, brnh bntsr, Q, K, K) attention softmax(logits + causal-mask, axis = [1, 2]) output einsum(bntsr, bsnh, brnh btnh, attention, V, V) return output"
        },
        {
            "title": "5 DETERMINANT BASED TRILINEAR FORMS",
            "content": "RoPE (Su et al., 2024) was proposed as way to capture the positional information in sequence for Transformer language models. RoPE applies position dependent rotation to the queries qi and the key kj so that the dot product qi, kj is function of the relative distance j. In particular, note that the dot product is invariant to orthogonal transformations Rdd: qi, kj = Rqi, Rkj. 4 This is important for RoPE to work as for query qi and key ki at the same position i, we expect its dot product to be unchanged by the application of position based rotations: qi, ki = Rqi, Rki. Note that the trilinear form defined in Equation 5 is not invariant to rotation and the application of the same rotation to qi, ki and il = Rqi, Rki, Rk i. Therefore, to generalize RoPE to 2-simplicial attention, it is important to explore alternative bilinear and trilinear forms that are rotation invariant. no longer preserves the inner product: qi, ki, l=1 qilkilk = (cid:80)d We note that the following functions are also invariant to rotations: ˆf2(a, b) = det = a1b2 a2b1, (cid:19) (cid:18)a1 a2 b2 b1 (cid:32)a1 a2 a3 b3 b2 c3 c2 b1 c1 (cid:33) , ˆf3(a, b, c) = det = a1b2c3 + a2b3c1 + a3b1c2 a1b3c2 a2b1c3 a3b2c1 = (a1, a2, a3), (b2, b3, b1), (c3, c1, c2) (a1, a2, a3), (b3, b1, b2), (c2, c3, c1), (8) the rearrangement in the last equality is popularly called Sarrus rule (Strang, 2022). Here, ˆf2 is bilinear form in = (a1, a2) and = (b1, b2) and ˆf3 is trilinear form in = (a1, a2, a3), = (b1, b2, b3), = (c1, c2, c3). Geometrically, ˆf2(a, b) measures the area of the parallelogram spanned by and b, and similarly, ˆf2(a, b, c) measures the volume of the parallelotope spanned by a, and c. We use the signed determinant operation ˆf3 to compute A(det) Rnnn. For any vector q, let q(l) = = q[3(l 1) : 3l] be its lth chunk of size 3. The logits are defined as: A(det) ij1j2 = (cid:88) l=1 det([q(l) , k(l) j1 , j2 (l)]). (9) Since Equation 8 has 2 dot product terms due to Sarrus rule, it would modify Algorithm 1 to use 2 einsums instead of 1 in line 2. The final attention weights are computed by applying softmax function on the logits above, similar to Equation 6. The output for token is then the weighted sum of value vectors as in Equation 7. Theorem 5.1. For any input size and input range = nO(1), there exists transformer architecture with single head of attention with logits computed as in (9), with attention head dimension = 7, such that for all [M ]N , the transformers output for element xi is 1 if j1, j2 s.t. xi + xj1 + xj2 = 0 (mod ), and 0 otherwise. We provide the proof in Appendix A. Since the sum-of-determinants trilinear function of Equation 9 involves 6 terms compared to the simpler trilinear form of Equation 5, in the following sections where we compute the backwards function for 2-simplicial attention, we will use the simpler trilinear form of Equation 5 without loss of generality."
        },
        {
            "title": "6 MODEL DESIGN",
            "content": "Since 2-simplicial attention scales as O(n3) in the sequence length n, it is impractical to apply it over the entire sequence. Instead, we parametrize it as O(n w1 w2), where w1 and w2 define the dimensions of sliding window over the sequence. Each query vector Qi attends to localized region of w1 keys and w2 keys, thereby reducing the computational burden. We systematically evaluate various configurations of w1 and w2 to identify optimal trade-offs between computational efficiency and model performance (see Table 1). For causal dot product attention, the complexity for sequence of length is given by: O(A) = 1 2 2 2n2 = 2n2, where is the sequence length. This involves two matrix multiplications: one for Q@K, one for @V , each requiring two floating-point operations per element. The causal mask allows us to skip 1 2 of these computations. 5 In contrast, the complexity of 2-simplical attention, parameterized by w1 and w2, is expressed as: O(A(2s)) = 3 2nw1w2 = 6nw1w2 This increase in complexity arises from the trilinear einsum operation, which necessitates an additional multiplication compared to standard dot product attention. w1 w2 32k 32k 16k 16k 16k 16k 8k 1024 512 128 256 512 1024 256 w2 32 64 128 64 32 16 32 Latency (ms) 104.1 ms 110.7 ms 59.2 ms 55.8 ms 55.1 ms 55.1 ms 28.3 ms Table 1: Latency for different combinations of w1, w2 We choose window size of (512, 32), balancing latency and quality. With this configuration, the computational complexity of 2-simplical attention is comparable to dot product attention at 48k context length. naive sliding window 2-simplicial attention implementation has each Qi vector attending to w1 + w2 1 different KK vectors, as illustrated in Figure 2. Thus, tiling queries like in flash attention leads to poor compute throughput. Inspired by Native Sparse Attention (Yuan et al., 2025), we adopt model architecture leveraging high Grouped Query Attention GQA (Ainslie et al., 2023) ratio of 64 . This approach enabled efficient tiling along query heads, ensuring dense computation and eliminating the need for costly element-wise masking."
        },
        {
            "title": "7 KERNEL OPTIMIZATION",
            "content": "We introduce series of kernel optimizatins tailored for 2-simplical attention, building off of Flash Attention (Dao et al., 2022) using online softmax. For the trilinear operations, we perform 2d tiling by merging one of the inputs via elementwise multiplication and executing matmul on the product as illustrated in Figure 2. This allows us to overlap both QK and on CUDA Core with (QK)@K and @(V ) on Tensor Core. Implementing this in Triton, we achieve 520 TFLOPS, rivaling the fastest FAv3 Triton implementations. Further optimization could be achieved with lower-level language like CUTLASS for finer grained tuning and optimizations. Despite this, we achieve competitive performance compared to CUTLASS FAv3 for large sequence lengths, as shown in Figure 3. For the backwards pass, we have (cid:88) i,k (cid:88) dVjd = dV kd = (Aijk dOid kd) (Aijk dOid Vjd) dPijk = i,j (cid:88) (dOid Vjd kd) dS = dsof tmaxjk(dP ) (Qid dSijk (cid:88) dKjd = kd) dK kd = i,k (cid:88) i,k (Qid dSijk Kjd) 6 (10) (11) (12) (13) (14) (15) Figure 2: Left: Visualization of sliding window 2-simplical attention. Each Qi attends to [w1, w2] shaped rectangle of K, . Right: Tiling to reduce 2-simplicial einsum QKK to elementwise mul QK on CUDA core and tiled matmul (QK )@K on tensor core. Figure 3: FLOPs and Latencies of FAv3 vs 2-simplical attention 7 dQid = (cid:88) (dSijk Kjd kd) (16) j,k For the backwards pass, aggregations across three different dimension orderings introduces significant overhead from atomic operations. To mitigate this, we decompose the backward pass into two distinct kernels: one for computing dK and dV , and another for dK , dV , and dQ. Although this approach incurs additional overhead from recomputing and dS, we find it is better than the extra overhead from atomics needed for single fused kernel. We note this may be limitation of Tritons coarser grained pipeline control making it difficult to hide the overhead from atomics. For small w2, we employ two-stage approach to compute dQ jointly with dK , dV without atomics as detailed in Algorithm 2. We divide along the sequence dimension into sized tiles. First we iterate over even tiles, storing dQ, dK, dK , and dV , dV . Then we iterate over odd tiles, storing dQ, and adding to dK, dK and dV , dV . [w2, dim] for stage in [0, 1] do for q_start in range(stage * w2, seq_len, w2 * 2) do q_end q_start + w2 for kv1_start in range(q_start - w1, q_end) do q_tile Q[q_start : q_end] ... k2_tile K[kv1_start : q_end] dQ += dQ(q_tile, k2_tile, ...) dV + = dV (q_tile, k2_tile, ...) dK + = dK (q_tile, k2_tile, ...) Algorithm 2 Backward pass for 2-simplicial attention 1: procedure 2-SIMPLICIAL FLASH ATTENTION BWD(Q, K, , , , w1, w2) 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: end procedure dK += load dK dV += load dV end for if stage == 1 then end if store dQ, ..., dK end for end for"
        },
        {
            "title": "8 EXPERIMENTS & RESULTS",
            "content": "We train series of MoE models (Jordan & Jacobs, 1994; Shazeer et al., 2017) ranging from 1 billion active parameters and 57 billion total parameters to 3.5 billion active parameters and 176 billion total parameters. We use interleaved sliding-window 2-simplicial attention, where every fourth layer is 2-simplicial attention layer. The choice of this particular ordering is to distribute the load in attention computation when using pipeline parallelism (Huang et al., 2019; Narayanan et al., 2019), since 2-simplicial attention and global attention are the most compute intensive operations in single pipeline stage and have comparable FLOPs. We use the AdamW optimizer (Loshchilov et al., 2017) with peak learning rate of 4 103 and weight decay of 0.0125. We use warmup of 4000 steps and use cosine decay learning schedule decreasing the learning rate to 0.01 of the peak learning rate. We report the negative log-likelihood on GSM8k (Cobbe et al., 2021), MMLU (Hendrycks et al., 2020), MMLU-pro (Wang et al., 2024) and MBPP (Austin et al., 2021), since these benchmarks most strongly test math, reasoning and coding skills in pre-training."
        },
        {
            "title": "Model",
            "content": "Active Params Total Params GSM8k MMLU MMLU-pro MBPP Transformer 2-simplicial (%) Transformer 2-simplicial (%) Transformer 2-simplicial (%) 1B 1B 2B 2B 3.5B 3.5B 57B 57B 100B 100B 176B 176B 0.6411 0.3277 0.3302 0.6423 +0.79% +0.19% 0.2987 0.2942 0.5932 0.5862 -1.51% -1.19% 0.2781 0. 0.5543 0.5484 -2.27% -1.06% 0.8718 0.8718 -0.01% 0.8193 0.8135 -0.71% 0.7858 0.7689 -2.15% 0.2690 0.2714 +0.88% 0.2435 0.2411 -1% 0.2203 0. -0.45% Table 2: Negative log-likelihood of Transformer (Vaswani et al., 2017) versus 2-simplicial attention. For MMLU (Hendrycks et al., 2020) and MMLU-pro (Wang et al., 2024) we measure the negative log-likelihood of the choice together with the entire answer. For GSM8k (Cobbe et al., 2021) we use 5-shots for the results. We see that the decrease () in negative log-likelihood scaling from 1.0 billion (active) parameter model increases going to 3.5 billion (active) parameter model. Furthermore, on models smaller than 2.0 billion (active) parameters, we see no gains from using 2-simplicial attention. From Table 2 we can estimate how the power law coefficients for the 2-simplicial attention differ from dot product attention. Recall from Section 3 that the loss can be expressed as: L(N, D) = + α + Dβ . (17) Since we train both the models on the same fixed number of tokens, we may ignore the third term and simply write the loss as: L(N ) = + α , log L(N ) log + log α log (18) log L(N ) = α log + β, (19) (20) where β = log logA and is an approximation to since is small. Note that here we used log(a + b) = log(1 + a/b) + log(b) to separate out the two terms, with the 1 + a/b term hidden in E. Therefore we can estimate α, β for both sets of models from the losses in Table 2 where we use for the active parameters in each model. We estimate the slope α and the intercept β for both the Transformer as well as the 2-simplicial Transformer in Table 3. We see that 2-simplicial attention has steeper slope α, i.e. higher exponent in its scaling law compared to dot product attention Transformer (Vaswani et al., 2017)."
        },
        {
            "title": "Model",
            "content": "GSM8k α β"
        },
        {
            "title": "MMLU",
            "content": "α β MMLU-pro β α"
        },
        {
            "title": "MBPP",
            "content": "α β Transformer 2-simplicial (%) 0.1420 0.1683 18.5% -1.8280 -2.3939 0.1256 0.1364 8.5% -2.1606 -2.3960 0.0901 0.1083 20.2% -1.7289 -2.1181 0.1720 0.1837 6.8% -2.2569 -2.5201 Table 3: Estimates of the power law coefficients α and β for the Transformer (Vaswani et al., 2017) and 2-simplicial attention."
        },
        {
            "title": "Model",
            "content": "GSM8k"
        },
        {
            "title": "MMLU",
            "content": "MMLU-pro"
        },
        {
            "title": "MBPP",
            "content": "R2 residual R2 residual R2 residual R2 residual Transformer 2-simplicial 0.9998 0.9974 2.8 106 4.9 105 0.9995 0. 4.7 106 1.3 105 0.9972 0.9999 1.5 105 4.6 108 0.9962 0.9999 7.5 105 1.5 106 Table 4: R2 and residuals measuring goodness of fit for Table 3."
        },
        {
            "title": "9 DISCUSSION",
            "content": "While 2-simplicial attention improves the exponent in the scaling laws, we should caveat that the technique maybe more useful when we are in the regime when token efficiency becomes more important. Our Triton kernel while efficient for prototyping is still far away from being used in production. More work in co-designing the implementation of 2-simplicial attention tailored to the specific hardware accelerator is needed in the future."
        },
        {
            "title": "10 CONCLUSION",
            "content": "We show that similar sized 2-simplicial attention (Clift et al., 2019) improves on dot product attention of Vaswani et al. (2017) by improving the negative log likelihood on reasoning, math and coding problems (see Table 2). We quantify this explicitly in Table 3 by demonstrating that 2-simplicial attention changes the exponent corresponding to parameters in the scaling law of Equation 18: in particular it has higher α for reasoning and coding tasks compared to the Transformer (Vaswani et al., 2017) which leads to more favorable scaling under token constraints. Furthermore, the percentage increase in the scaling exponent α is higher for less saturated and more challenging benchmarks such as MMLU-pro and GSM8k. We hope that scaling 2-simplicial Transformers could unlock significant improvements in downstream performance on reasoning-heavy tasks, helping to overcome the current limitations of pretraining scalability. Furthermore, we believe that developing specialized and efficient implementation is key to fully unlocking the potential of this architecture."
        },
        {
            "title": "11 ACKNOWLEDGMENTS",
            "content": "The authors gratefully acknowledge the invaluable support and feedback from Chuanhao Zhuge, Tony Liu, Ying Zhang, Ajit Mathews, Afroz Mohiuddin, Vinay Rao and Dhruv Choudhary."
        },
        {
            "title": "REFERENCES",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Joshua Ainslie, James Lee-Thorp, Michiel De Jong, Yury Zemlyanskiy, Federico Lebrón, and Sumit Sanghai. Gqa: Training generalized multi-query transformer models from multi-head checkpoints. arXiv preprint arXiv:2305.13245, 2023. Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021. Yasaman Bahri, Ethan Dyer, Jared Kaplan, Jaehoon Lee, and Utkarsh Sharma. Explaining neural scaling laws. Proceedings of the National Academy of Sciences, 121(27):e2311878121, 2024. Leon Bergen, Timothy ODonnell, and Dzmitry Bahdanau. Systematic generalization with edge transformers. Advances in Neural Information Processing Systems, 34:13901402, 2021. David Brandfonbrener, Nikhil Anand, Nikhil Vyas, Eran Malach, and Sham Kakade. Loss-to-loss prediction: Scaling laws for all datasets. arXiv preprint arXiv:2411.12925, 2024. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. James Clift, Dmitry Doryn, Daniel Murfet, and James Wallbridge. Logic and the 2-simplicial transformer. arXiv preprint arXiv:1909.00668, 2019. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. Flashattention: Fast and memoryefficient exact attention with io-awareness. Advances in neural information processing systems, 35: 1634416359, 2022. Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Łukasz Kaiser. Universal transformers. arXiv preprint arXiv:1807.03819, 2018. Katie Everett. Observation on scaling laws, May 2025. URL https://x.com/ _katieeverett/status/1925665335727808651. [Tweet]. Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023. Charles Harris, Jarrod Millman, Stéfan Van Der Walt, Ralf Gommers, Pauli Virtanen, David Cournapeau, Eric Wieser, Julian Taylor, Sebastian Berg, Nathaniel Smith, et al. Array programming with numpy. Nature, 585(7825):357362, 2020. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and arXiv preprint Jacob Steinhardt. Measuring massive multitask language understanding. arXiv:2009.03300, 2020. Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun, Hassan Kianinejad, Md Mostofa Ali Patwary, Yang Yang, and Yanqi Zhou. Deep learning scaling is predictable, empirically. arXiv preprint arXiv:1712.00409, 2017. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022. Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Dehao Chen, Mia Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc Le, Yonghui Wu, et al. Gpipe: Efficient training of giant neural networks using pipeline parallelism. Advances in neural information processing systems, 32, 2019. Michael Jordan and Robert Jacobs. Hierarchical mixtures of experts and the em algorithm. Neural computation, 6(2):181214, 1994. John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin Žídek, Anna Potapenko, et al. Highly accurate protein structure prediction with alphafold. nature, 596(7873):583589, 2021. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Francois Fleuret. Transformers are rnns: fast autoregressive transformers with linear attention. In Proceedings of the 37th International Conference on Machine Learning, ICML20. JMLR.org, 2020. Alexander Kozachinskiy, Felipe Urrutia, Hector Jimenez, Tomasz Steifer, Germán Pizarro, Matías Fuentes, Francisco Meza, Cristian Calderon, and Cristóbal Rojas. Strassen attention: Unlocking compositional abilities in transformers based on new lower bound method. arXiv preprint arXiv:2501.19215, 2025. Ilya Loshchilov, Frank Hutter, et al. Fixing weight decay regularization in adam. arXiv preprint arXiv:1711.05101, 5:5, 2017. 11 Deepak Narayanan, Aaron Harlap, Amar Phanishayee, Vivek Seshadri, Nikhil R. Devanur, Gregory R. Ganger, Phillip B. Gibbons, and Matei Zaharia. Pipedream: generalized pipeline parallelism for dnn training. In Proceedings of the 27th ACM Symposium on Operating Systems Principles, SOSP 19, pp. 115, New York, NY, USA, 2019. Association for ComISBN 9781450368735. doi: 10.1145/3341301.3359646. URL https: puting Machinery. //doi.org/10.1145/3341301.3359646. Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, and Dustin Tran. Image transformer. In International conference on machine learning, pp. 40554064. PMLR, 2018. Aurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier. Efficient content-based sparse attention with routing transformers. Transactions of the Association for Computational Linguistics, 9:5368, 2021. Aurko Roy, Rohan Anil, Guangda Lai, Benjamin Lee, Jeffrey Zhao, Shuyuan Zhang, Shibo Wang, Ye Zhang, Shen Wu, Rigel Swavely, et al. N-grammer: Augmenting transformers with latent n-grams. arXiv preprint arXiv:2207.06366, 2022. Clayton Sanford, Daniel Hsu, and Matus Telgarsky. Representational strengths and limitations of transformers. Advances in Neural Information Processing Systems, 36:3667736707, 2023. Nikunj Saunshi, Nishanth Dikkala, Zhiyuan Li, Sanjiv Kumar, and Sashank Reddi. Reasoning with latent thoughts: On the power of looped transformers. arXiv preprint arXiv:2502.17416, 2025. Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538, 2017. Xuyang Shen, Dong Li, Ruitao Leng, Zhen Qin, Weigao Sun, and Yiran Zhong. Scaling laws for linear complexity language models. arXiv preprint arXiv:2406.16690, 2024. David So, Wojciech Manke, Hanxiao Liu, Zihang Dai, Noam Shazeer, and Quoc Le. Searching for efficient transformers for language modeling. Advances in neural information processing systems, 34:60106022, 2021. Ben Sorscher, Robert Geirhos, Shashank Shekhar, Surya Ganguli, and Ari Morcos. Beyond neural scaling laws: beating power law scaling via data pruning. Advances in Neural Information Processing Systems, 35:1952319536, 2022. Gilbert Strang. Introduction to linear algebra. SIAM, 2022. Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. Vladimir Vapnik. On the uniform convergence of relative frequencies of events to their probabilities. In Doklady Akademii Nauk USSR, volume 181, pp. 781787, 1968. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. Ruoxi Wang, Rakesh Shivanna, Derek Cheng, Sagar Jain, Dong Lin, Lichan Hong, and Ed Chi. Dcn v2: Improved deep & cross network and practical lessons for web-scale learning to rank systems. In Proceedings of the web conference 2021, pp. 17851797, 2021. 12 Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, et al. Mmlu-pro: more robust and challenging multitask language understanding benchmark. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024. Liu Yang, Kangwook Lee, Robert Nowak, and Dimitris Papailiopoulos. Looped transformers are better at learning learning algorithms. arXiv preprint arXiv:2311.12424, 2023. Jingyang Yuan, Huazuo Gao, Damai Dai, Junyu Luo, Liang Zhao, Zhengyan Zhang, Zhenda Xie, YX Wei, Lean Wang, Zhiping Xiao, et al. Native sparse attention: Hardware-aligned and natively trainable sparse attention. arXiv preprint arXiv:2502.11089, 2025. Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. Big bird: Transformers for longer sequences. Advances in neural information processing systems, 33:1728317297, 2020."
        },
        {
            "title": "A ROTATION INVARIANT TRILINEAR FORMS",
            "content": "A.1 PROOF FOR THEOREM 5.1 We define the embedding functions for the Query and Key vectors such that their interaction within the Sum-of-Determinants attention mechanism computes the Match3 function. To handle cases where no match exists, we use 7-dimensional embedding where the 7th dimension acts as selector for \"blank pair\" option, technique adapted from Match2 construction in Sanford et al. (2023). The construction for regular token pairs is based on the mathematical identity: cos(θ1 + θ2 + θ3) = det(M1) + det(M2), (21) where the matrices M1, M2 R33 are defined as: M1 = (cid:32)cos(θ1) sin(θ2) 0 sin(θ1) cos(θ2) 0 (cid:33) 0 0 cos(θ3) , M2 = (cid:32) sin(θ1) cos(θ1) sin(θ2) cos(θ2) 0 0 (cid:33) 0 0 sin(θ3) Let θk = 2πxk MLP ϕ and matrices Q, K, . Let be large scaling constant. . We define the 7-dimensional query vector qi and key vectors kj1, j2 via an input The 7-dimensional query vector qi = Qϕ(xi) is defined as: qi = (c cos(θi), sin(θi), 0, sin(θi), cos(θi), 0, c) The key vectors kj1 = Kϕ(xj1 ) and j = ϕ(xj2) for regular tokens are defined as: kj1 = (sin(θj1), cos(θj1), 0, sin(θj1), cos(θj1 ), 0, 0) j2 = (0, 0, cos(θj2 ), 0, 0, sin(θj2), 0) The attention score is computed via hybrid mechanism: 1. For regular pairs (j1, j2), the score is the sum of determinants of two 3D chunks formed from the first 6 dimensions of the vectors. The 7th dimension of the keys is 0, so it is ignored in this term. Ai,j1,j2 = det(qi[: 3], kj1[: 3], j2 [: 3]) + det(qi[3 : 6], kj1[3 : 6], j2 [3 : 6]) = (det(M1) + det(M2)) (cid:18) 2π(xi + xj1 + xj2) = cos (from (21)) (cid:19) (since θi = 2πxk/M ), where qi[l : + m] = {(qi)l, . . . , (qi)l+m1}, denotes array slicing. 2. For the blank pair, the score is computed using the 7th dimension. It is the dot product of the query vector qi and fixed key vector kblank = (0, 0, 0, 0, 0, 0, 1): Ai,blank = qi kblank = As result, the attention score is maximized to value of if and only if xi + xj1 + xj2 = 0 (mod ). The blank pair also receives score of c. For any non-matching triple, the score is strictly less than c. The value vectors are defined by matrices and . For any regular token xj, we set its value embeddings to be ϕ(xj) = 1 and ϕ(xj) = 1. The resulting value for the pair (j1, j2) in the final value matrix is their Kronecker product, which is 1. For the blank pair, the corresponding value is 0. Let βi be the number of pairs (j1, j2) that form match with xi. The softmax function distributes the attention weight almost exclusively among the entries with score of c. If no match exists (βi = 0), the blank pair receives all the attention, and the output is 0 since its value is 0. 14 If at least one match exists (βi 1), the attention is distributed among the βi matching pairs and the 1 blank pair. The output of the attention layer will be approximately βi(1)+1(0) = βi βi+1 . βi+1 The final step is to design an output MLP ψ such that ψ(z) = 1 if 1/2 and ψ(z) = 0 otherwise, which is straightforward to implement. TRITON KERNEL: FORWARD PASS FOR 2-SIMPLICIAL ATTENTION 1 @triton.autotune( configs=[ 3 4 5 6 8 9 10 11 17 19 20 21 22 23 25 26 27 28 29 31 32 33 34 35 37 38 39 40 41 43 44 45 46 47 49 50 51 52 53 Config( { \"BLOCK_SIZE_Q\": 64, \"BLOCK_SIZE_KV\": 32, \"num_stages\": 1, }, num_warps=4, ) ], key=[\"HEAD_DIM\"], 12 13 ) 14 @triton.jit 15 def two_simplicial_attn_fwd_kernel( # [b, s, k, h] # [b, s, k, h] # [b, s, k, h] # [b, s, k, h] # [b, s, k, h] # [b, s, k, h] # [b, k, s] Q_ptr, K1_ptr, K2_ptr, V1_ptr, V2_ptr, O_ptr, M_ptr, bs, seq_len, num_heads, head_dim, w1: tl.constexpr, w2: tl.constexpr, q_stride_b, q_stride_s, q_stride_k, q_stride_h, k1_stride_b, k1_stride_s, k1_stride_k, k1_stride_h, k2_stride_b, k2_stride_s, k2_stride_k, k2_stride_h, v1_stride_b, v1_stride_s, v1_stride_k, v1_stride_h, v2_stride_b, v2_stride_s, v2_stride_k, v2_stride_h, out_stride_b, out_stride_s, out_stride_k, out_stride_h, m_stride_b, 15 55 56 57 58 59 61 62 63 64 ): 65 66 67 69 70 71 72 73 75 76 77 78 79 81 82 83 84 85 87 88 89 90 91 93 94 95 96 97 99 100 101 102 103 105 106 107 108 109 111 112 113 114 m_stride_k, m_stride_s, BLOCK_SIZE_Q: tl.constexpr, BLOCK_SIZE_KV: tl.constexpr, HEAD_DIM: tl.constexpr, INPUT_PRECISION: tl.constexpr, SM_SCALE: tl.constexpr, K2_BIAS: tl.constexpr, V2_BIAS: tl.constexpr, num_stages: tl.constexpr, data_dtype = tl.bfloat16 compute_dtype = tl.float32 gemm_dtype = tl.bfloat q_start = tl.program_id(0) * BLOCK_SIZE_Q q_end = q_start + BLOCK_SIZE_Q bk = tl.program_id(1) offs_b = bk // num_heads offs_k = bk % num_heads qkv_offs_bk = offs_b * q_stride_b + offs_k * q_stride_k Q_ptr += qkv_offs_bk K1_ptr += qkv_offs_bk K2_ptr += qkv_offs_bk V1_ptr += qkv_offs_bk V2_ptr += qkv_offs_bk O_ptr += qkv_offs_bk M_ptr += offs_b * m_stride_b + offs_k * m_stride_k m_i = tl.zeros((BLOCK_SIZE_Q,), dtype=compute_dtype) - float(\"inf\") l_i = tl.zeros((BLOCK_SIZE_Q,), dtype=compute_dtype) acc = tl.zeros((BLOCK_SIZE_Q, HEAD_DIM), dtype=compute_dtype) q_offs_s = q_start + tl.arange(0, BLOCK_SIZE_Q) qkv_offs_h = tl.arange(0, HEAD_DIM) q_mask_s = q_offs_s < seq_len qkv_mask_h = qkv_offs_h < head_dim q_offs = q_offs_s[:, None] * q_stride_s + qkv_offs_h[None, :] * q_stride_h q_mask = q_mask_s[:, None] & (qkv_mask_h[None, :]) q_tile = tl.load(Q_ptr + q_offs, mask=q_mask).to( compute_dtype # [BLOCK_SIZE_Q, HEAD_DIM] ) softmax_scale = tl.cast(SM_SCALE, gemm_dtype) for kv1_idx in tl.range(tl.maximum(0, q_start - w1), tl.minimum( seq_len, q_end)): k1_offs = kv1_idx * k1_stride_s + qkv_offs_h * k1_stride_h k1_tile = (tl.load(K1_ptr + k1_offs, mask=qkv_mask_h).to( compute_dtype))[ None, : # [1, HEAD_DIM] ] qk1 = q_tile * k1_tile qk1 = qk1.to(gemm_dtype) # [BLOCK_SIZE_Q, HEAD_DIM] v1_offs = kv1_idx * v1_stride_s + qkv_offs_h * v1_stride_h v1_tile = (tl.load(V1_ptr + v1_offs, mask=qkv_mask_h).to( compute_dtype))[ None, : ] # [1, HEAD_DIM] for kv2_idx in tl.range( 16 115 117 118 119 120 121 123 124 125 126 127 129 130 131 132 133 135 136 137 138 139 141 142 143 144 145 147 148 149 150 151 153 154 155 156 157 159 160 161 162 163 165 166 167 168 169 171 172 173 174 175 tl.maximum(0, q_start - w2), tl.minimum(seq_len, q_end), BLOCK_SIZE_KV, num_stages=num_stages, ): kv2_offs_s = kv2_idx + tl.arange(0, BLOCK_SIZE_KV) kv2_mask_s = kv2_offs_s < seq_len k2t_mask = kv2_mask_s[None, :] & qkv_mask_h[:, None] v2_mask = kv2_mask_s[:, None] & qkv_mask_h[None, :] k2_offs = ( kv2_offs_s[None, :] * k2_stride_s + qkv_offs_h[:, None] * k2_stride_h ) v2_offs = ( kv2_offs_s[:, None] * v2_stride_s + qkv_offs_h[None, :] * v2_stride_h ) k2t_tile = tl.load(K2_ptr + k2_offs, mask=k2t_mask).to( compute_dtype # [HEAD_DIM, BLOCK_SIZE_KV] ) v2_tile = tl.load(V2_ptr + v2_offs, mask=v2_mask).to( compute_dtype # [BLOCK_SIZE_KV, HEAD_DIM] ) k2t_tile += K2_BIAS v2_tile += V2_BIAS k2t_tile = k2t_tile.to(gemm_dtype) v2_tile = v2_tile.to(compute_dtype) qk = tl.dot( qk1 * softmax_scale, k2t_tile, input_precision=\"tf32\", out_dtype=tl.float32, # INPUT_PRECISION, ) # [BLOCK_SIZE_Q, BLOCK_SIZE_KV] qk_mask = q_mask_s[:, None] & kv2_mask_s[None, :] # Mask for q_idx - w1 < kv1_idx <= q_idx # and q_idx - w2 < kv2_offs_s <= q_idx kv1_local_mask = ((q_offs_s[:, None] - w1) < kv1_idx) & ( kv1_idx <= q_offs_s[:, None] ) kv2_local_mask = ((q_offs_s[:, None] - w2) < kv2_offs_s[None, :]) & ( kv2_offs_s[None, :] <= q_offs_s[:, None] ) qk_mask &= kv1_local_mask & kv2_local_mask qk += tl.where(qk_mask, 0, -1.0e38) m_ij = tl.maximum(m_i, tl.max(qk, 1)) = tl.math.exp(qk - m_ij[:, None]) l_ij = tl.sum(p, 1) alpha = tl.math.exp(m_i - m_ij) l_i = l_i * alpha + l_ij acc = acc * alpha[:, None] v12_tile = v1_tile * v2_tile acc += tl.dot( p.to(gemm_dtype), v12_tile.to(gemm_dtype), input_precision=\"ieee\", out_dtype=tl.float32, ) # [BLOCK_SIZE_KV, HEAD_DIM] # INPUT_PRECISION, m_i = m_ij acc = acc / l_i[:, None] 17 177 179 180 181 182 183 185 186 187 acc = tl.where(q_mask, acc, 0.0) acc = acc.to(data_dtype) out_offs = q_offs_s[:, None] * out_stride_s + qkv_offs_h[None, :] * out_stride_h tl.store(O_ptr + out_offs, acc, mask=q_mask) = m_i + tl.log(l_i) m_offs = q_offs_s * m_stride_s m_mask = q_offs_s < seq_len tl.store(M_ptr + m_offs, m, mask=m_mask) Listing 1: Forward pass for 2-simplicial attention. TRITON KERNEL: BACKWARD PASS FOR 2-SIMPLICIAL ATTENTION 1 @triton.jit 2 def two_simplicial_attn_bwd_kv1_kernel( # [b, s, k, h] 3 5 6 7 8 9 11 12 13 14 15 17 18 19 20 21 23 24 25 26 27 29 30 31 32 33 35 36 37 38 39 41 42 43 44 45 # [b, s, k, h] # [b, k, s] # [b, k, s] # [b, s, k, h] # [b, s, k, h] # Q[i]: KV1(i-w1,i] # Q[i]: KV2(i-w2,i] # [b, s, k, h] # [b, s, k, h] # [b, s, k, h] # [b, s, k, h] # [b, s, k, h] Q_ptr, K1_ptr, K2_ptr, V1_ptr, V2_ptr, dO_ptr, M_ptr, D_ptr, dQ_ptr, dK1_ptr, dV1_ptr, # Skip writing dk2, dv2 for now. bs, seq_len, num_heads, head_dim, w1, w2, q_stride_b, q_stride_s, q_stride_k, q_stride_h, k1_stride_b, k1_stride_s, k1_stride_k, k1_stride_h, k2_stride_b, k2_stride_s, k2_stride_k, k2_stride_h, v1_stride_b, v1_stride_s, v1_stride_k, v1_stride_h, v2_stride_b, v2_stride_s, v2_stride_k, v2_stride_h, dO_stride_b, dO_stride_s, dO_stride_k, dO_stride_h, m_stride_b, m_stride_k, 18 47 48 49 50 52 53 54 55 56 58 59 60 61 62 64 65 66 67 68 70 71 72 ): 73 74 75 76 78 79 80 81 82 84 85 86 87 88 90 91 92 93 94 96 97 98 99 100 102 103 104 105 106 108 109 m_stride_s, d_stride_b, d_stride_k, d_stride_s, dq_stride_b, dq_stride_s, dq_stride_k, dq_stride_h, dk1_stride_b, dk1_stride_s, dk1_stride_k, dk1_stride_h, dv1_stride_b, dv1_stride_s, dv1_stride_k, dv1_stride_h, BLOCK_SIZE_Q: tl.constexpr, BLOCK_SIZE_KV: tl.constexpr, HEAD_DIM: tl.constexpr, SM_SCALE: tl.constexpr, K2_BIAS: tl.constexpr, V2_BIAS: tl.constexpr, COMPUTE_DQ: tl.constexpr, num_stages: tl.constexpr, is_flipped: tl.constexpr, data_dtype = tl.bfloat16 compute_dtype = tl.float32 gemm_dtype = tl.bfloat16 kv1_start = tl.program_id(0) * BLOCK_SIZE_KV kv1_end = kv1_start + BLOCK_SIZE_KV bk = tl.program_id(1) offs_b = bk // num_heads offs_k = bk % num_heads qkv_offs_bk = offs_b * q_stride_b + offs_k * q_stride_k Q_ptr += qkv_offs_bk K1_ptr += qkv_offs_bk K2_ptr += qkv_offs_bk V1_ptr += qkv_offs_bk V2_ptr += qkv_offs_bk dO_ptr += offs_b * dO_stride_b + offs_k * dO_stride_k M_ptr += offs_b * m_stride_b + offs_k * m_stride_k D_ptr += offs_b * d_stride_b + offs_k * d_stride_k dK1_ptr += offs_b * dk1_stride_b + offs_k * dk1_stride_k dV1_ptr += offs_b * dv1_stride_b + offs_k * dv1_stride_k if COMPUTE_DQ: dQ_ptr += offs_b * dq_stride_b + offs_k * dq_stride_k softmax_scale = tl.cast(SM_SCALE, gemm_dtype) qkv_offs_h = tl.arange(0, HEAD_DIM) qkv_mask_h = qkv_offs_h < head_dim kv1_offs_s = kv1_start + tl.arange(0, BLOCK_SIZE_KV) k1_offs = kv1_offs_s[:, None] * k1_stride_s + qkv_offs_h[None, :] * k1_stride_h kv1_mask_s = kv1_offs_s < seq_len kv1_mask = kv1_mask_s[:, None] & qkv_mask_h[None, :] k1_tile = tl.load(K1_ptr + k1_offs, mask=kv1_mask).to( compute_dtype ) # [BLOCK_SIZE_KV, HEAD_DIM] 19 111 112 113 114 115 117 118 119 120 121 123 124 125 126 127 129 130 131 132 133 135 136 137 138 139 141 142 143 144 145 147 148 149 150 151 153 154 155 156 157 159 160 161 162 163 165 v1_offs = kv1_offs_s[:, None] * v1_stride_s + qkv_offs_h[None, :] * v1_stride_h v1_tile = tl.load(V1_ptr + v1_offs, mask=kv1_mask).to( compute_dtype # [BLOCK_SIZE_KV, HEAD_DIM] ) if is_flipped: k1_tile += K2_BIAS v1_tile += V2_BIAS dv1 = tl.zeros((BLOCK_SIZE_KV, HEAD_DIM), compute_dtype) dk1 = tl.zeros((BLOCK_SIZE_KV, HEAD_DIM), compute_dtype) # for kv2_idx in tl.range(0, seq_len): # kv1 - w2 < kv2 <= kv1 + w1 for kv2_idx in tl.range( tl.maximum(0, kv1_start - w2), tl.minimum(seq_len, kv1_end + w1) ): k2_offs = kv2_idx * k2_stride_s + qkv_offs_h * k2_stride_h k2_tile = (tl.load(K2_ptr + k2_offs, mask=qkv_mask_h).to( compute_dtype))[ None, : # [1, HEAD_DIM] ] v2_offs = kv2_idx * v2_stride_s + qkv_offs_h * v2_stride_h v2_tile = (tl.load(V2_ptr + v2_offs, mask=qkv_mask_h).to( compute_dtype))[ None, : # [1, HEAD_DIM] ] if not is_flipped: k2_tile += K2_BIAS v2_tile += V2_BIAS # [BLOCK_SIZE_KV, HEAD_DIM] # [BLOCK_SIZE_KV, HEAD_DIM] k1k2 = k1_tile * k2_tile v1v2 = v1_tile * v2_tile k1k2 = k1k2.to(gemm_dtype) v1v2 = v1v2.to(gemm_dtype) # kv1 <= < kv1 + w1 # kv2 <= < kv2 + w2 q_start = tl.maximum(kv1_start, kv2_idx) q_end = tl.minimum(seq_len, tl.minimum(kv1_end + w1, kv2_idx + w2 )) for q_idx in tl.range(q_start, q_end, BLOCK_SIZE_Q): # Load qt, m, d, dO q_offs_s = q_idx + tl.arange(0, BLOCK_SIZE_Q) q_offs = q_offs_s[None, :] * q_stride_s + qkv_offs_h[:, None] * q_stride_h q_mask_s = q_offs_s < seq_len qt_mask = q_mask_s[None, :] & qkv_mask_h[:, None] qt_tile = tl.load(Q_ptr + q_offs, mask=qt_mask).to( gemm_dtype # [HEAD_DIM, BLOCK_SIZE_Q] ) m_offs = q_offs_s * m_stride_s m_tile = tl.load(M_ptr + m_offs, mask=q_mask_s).to( compute_dtype)[ None, : # [1, BLOCK_SIZE_Q] ] d_offs = q_offs_s * d_stride_s d_tile = tl.load(D_ptr + d_offs, mask=q_mask_s).to( compute_dtype)[ None, : # [1, BLOCK_SIZE_Q] ] dO_offs = ( q_offs_s[:, None] * dO_stride_s + qkv_offs_h[None, :] * dO_stride_h ) dO_tile = tl.load( dO_ptr + dO_offs, mask=q_mask_s[:, None] & qkv_mask_h[ None, :] ).to(compute_dtype) # [BLOCK_SIZE_Q, HEAD_DIM] 166 167 168 169 170 172 173 174 175 176 178 179 180 181 182 184 185 186 187 188 190 191 192 193 194 196 197 198 199 200 202 203 204 205 206 208 209 210 211 212 214 215 216 217 if COMPUTE_DQ: dq = tl.zeros((BLOCK_SIZE_Q, HEAD_DIM), tl.float32) # Compute dv1. # [KV, D] @ [D, Q] => [KV, Q] qkkT = tl.dot( k1k2, qt_tile * softmax_scale, out_dtype=tl.float32 # [BLOCK_SIZE_KV, BLOCK_SIZE_Q] ) # Mask qkkT to -inf. kv1_local_mask = ((q_offs_s[None, :] - w1) < kv1_offs_s[:, None]) & ( kv1_offs_s[:, None] <= q_offs_s[None, :] ) kv2_local_mask = ((q_offs_s - w2) < kv2_idx) & (kv2_idx <= q_offs_s) local_mask = ( kv1_local_mask & kv2_local_mask[None, :] # [BLOCK_SIZE_KV, BLOCK_SIZE_Q] ) qkkT = tl.where(local_mask, qkkT, -1.0e38) pT = tl.exp(qkkT - m_tile) pT = tl.where(local_mask, pT, 0.0) dOv2 = dO_tile * v2_tile dv1 += tl.dot( # [BLOCK_SIZE_KV, BLOCK_SIZE_Q] # [BLOCK_SIZE_Q, HEAD_DIM] pT.to(gemm_dtype), dOv2.to(gemm_dtype), out_dtype=tl. float32 ) # [BLOCK_SIZE_KV, HEAD_DIM] dpT = tl.dot( v1v2, tl.trans(dO_tile.to(gemm_dtype)), out_dtype=tl. float32 # [BLOCK_SIZE_KV, BLOCK_SIZE_Q] ) dsT = pT * (dpT - d_tile) dsT = tl.where(local_mask, dsT, 0.0) dsT = dsT.to(gemm_dtype) # [BLOCK_SIZE_KV, BLOCK_SIZE_Q] dk1 += ( tl.dot(dsT, tl.trans(qt_tile), out_dtype=tl.float32) * k2_tile.to(tl.float32) * softmax_scale ) if COMPUTE_DQ: # dq[q, d] = dsT.T[q, kv1] @ k1k2[kv1, d] dq += ( tl.dot(tl.trans(dsT), k1k2, out_dtype=tl.float32) * softmax_scale # [BLOCK_SIZE_Q, HEAD_DIM] ) dq_offs = ( q_offs_s[:, None] * dq_stride_s + qkv_offs_h[None, :] * dq_stride_h ) tl.atomic_add( dQ_ptr + dq_offs, dq, mask=q_mask_s[:, None] & qkv_mask_h[None, :] ) dv1_offs = kv1_offs_s[:, None] * dv1_stride_s + qkv_offs_h[None, :] * dv1_stride_h dk1_offs = kv1_offs_s[:, None] * dk1_stride_s + qkv_offs_h[None, :] * dk1_stride_h tl.store(dV1_ptr + dv1_offs, dv1.to(data_dtype), mask=kv1_mask) tl.store(dK1_ptr + dk1_offs, dk1.to(data_dtype), mask=kv1_mask) Listing 2: Backward pass for 2-simplicial attention. 21 3 5 6 7 8 9 11 17 18 19 20 22 23 24 25 26 28 29 30 31 32 34 35 36 37 38 40 41 42 43 44 46 47 48 49 50 52 53 54 55 56 58 59 60 61 62 64 65 1 @triton.autotune( 2 configs=[ Config( { \"BLOCK_SIZE_Q\": 32, \"BLOCK_SIZE_KV2\": 64, \"num_stages\": 1, }, num_warps=4, ) ], key=[\"HEAD_DIM\"], 12 13 ) 14 @triton.jit 15 def two_simplicial_attn_bwd_kv2q_kernel( # [b, s, k, h] 16 # [b, s, k, h] # [b, s, k, h] # [b, s, k, h] # [b, s, k, h] # [b, s, k, h] # [b, k, s] # [b, k, s] # [b, s, k, h] # [b, s, k, h] # [b, s, k, h] # Q[i]: KV1(i-w1,i] # Q[i]: KV2(i-w2,i] Q_ptr, K1_ptr, K2_ptr, V1_ptr, V2_ptr, dO_ptr, M_ptr, D_ptr, dQ_ptr, dK2_ptr, dV2_ptr, bs, seq_len, num_heads, head_dim, w1, w2, q_stride_b, q_stride_s, q_stride_k, q_stride_h, k1_stride_b, k1_stride_s, k1_stride_k, k1_stride_h, k2_stride_b, k2_stride_s, k2_stride_k, k2_stride_h, v1_stride_b, v1_stride_s, v1_stride_k, v1_stride_h, v2_stride_b, v2_stride_s, v2_stride_k, v2_stride_h, dO_stride_b, dO_stride_s, dO_stride_k, dO_stride_h, m_stride_b, m_stride_k, m_stride_s, d_stride_b, d_stride_k, d_stride_s, dq_stride_b, dq_stride_s, dq_stride_k, 66 67 68 69 70 72 73 74 75 76 78 79 80 81 82 83 ): 84 86 87 88 89 90 92 93 94 95 96 98 99 100 101 102 104 105 106 107 108 110 111 112 113 114 116 117 118 119 120 122 123 124 125 126 dq_stride_h, dk2_stride_b, dk2_stride_s, dk2_stride_k, dk2_stride_h, dv2_stride_b, dv2_stride_s, dv2_stride_k, dv2_stride_h, BLOCK_SIZE_Q: tl.constexpr, BLOCK_SIZE_KV2: tl.constexpr, HEAD_DIM: tl.constexpr, SM_SCALE: tl.constexpr, K2_BIAS: tl.constexpr, V2_BIAS: tl.constexpr, num_stages: tl.constexpr, IS_SECOND_PASS: tl.constexpr, assert BLOCK_SIZE_KV2 == BLOCK_SIZE_Q + w2 data_dtype = tl.bfloat16 compute_dtype = tl.float32 gemm_dtype = tl.bfloat16 # First pass does even tiles, second pass does odd tiles. q_start = tl.program_id(0) * BLOCK_SIZE_KV2 if IS_SECOND_PASS: q_start += BLOCK_SIZE_Q q_end = q_start + BLOCK_SIZE_Q kv2_start = q_start - w2 bk = tl.program_id(1) offs_b = bk // num_heads offs_k = bk % num_heads qkv_offs_bk = offs_b * q_stride_b + offs_k * q_stride_k Q_ptr += qkv_offs_bk K1_ptr += qkv_offs_bk K2_ptr += qkv_offs_bk V1_ptr += qkv_offs_bk V2_ptr += qkv_offs_bk dO_ptr += offs_b * dO_stride_b + offs_k * dO_stride_k M_ptr += offs_b * m_stride_b + offs_k * m_stride_k D_ptr += offs_b * d_stride_b + offs_k * d_stride_k dQ_ptr += offs_b * dq_stride_b + offs_k * dq_stride_k dK2_ptr += offs_b * dk2_stride_b + offs_k * dk2_stride_k dV2_ptr += offs_b * dv2_stride_b + offs_k * dv2_stride_k softmax_scale = tl.cast(SM_SCALE, gemm_dtype) qkv_offs_h = tl.arange(0, HEAD_DIM) qkv_mask_h = qkv_offs_h < head_dim q_offs_s = q_start + tl.arange(0, BLOCK_SIZE_Q) kv2_offs_s = kv2_start + tl.arange(0, BLOCK_SIZE_KV2) q_offs = q_offs_s[:, None] * q_stride_s + qkv_offs_h[None, :] * q_stride_h kv2_offs = kv2_offs_s[:, None] * k2_stride_s + qkv_offs_h[None, :] * k2_stride_h m_offs = q_offs_s * m_stride_s d_offs = q_offs_s * d_stride_s dO_offs = q_offs_s[:, None] * dO_stride_s + qkv_offs_h[None, :] * dO_stride_h q_mask_s = q_offs_s < seq_len q_mask = q_mask_s[:, None] & qkv_mask_h[None, :] kv2_mask_s = 0 <= kv2_offs_s and kv2_offs_s < seq_len 23 128 130 131 132 133 134 136 137 138 139 140 142 143 144 145 146 148 149 150 151 152 154 155 156 157 158 160 161 162 163 164 166 167 168 169 170 172 173 174 175 176 178 179 180 181 182 184 185 186 187 kv2_mask = kv2_mask_s[:, None] & qkv_mask_h[None, :] q_tile = tl.load(Q_ptr + q_offs, mask=q_mask).to( compute_dtype # [BLOCK_SIZE_Q, HEAD_DIM] ) k2_tile = tl.load(K2_ptr + kv2_offs, mask=kv2_mask).to(gemm_dtype) # [KV2, HEAD_DIM] v2_tile = tl.load(V2_ptr + kv2_offs, mask=kv2_mask).to(gemm_dtype) # [KV2, HEAD_DIM] m_tile = tl.load(M_ptr + m_offs, mask=q_mask_s).to(compute_dtype) # [ BLOCK_SIZE_Q] d_tile = tl.load(D_ptr + d_offs, mask=q_mask_s).to(compute_dtype) # [ BLOCK_SIZE_Q] dO_tile = tl.load(dO_ptr + dO_offs, mask=q_mask).to( gemm_dtype ) # [BLOCK_SIZE_Q, HEAD_DIM] # Apply KV2 norm. k2_tile += K2_BIAS v2_tile += V2_BIAS k2_tile = k2_tile.to(gemm_dtype) v2_tile = v2_tile.to(gemm_dtype) dq = tl.zeros((BLOCK_SIZE_Q, HEAD_DIM), tl.float32) dk2 = tl.zeros((BLOCK_SIZE_KV2, HEAD_DIM), tl.float32) dv2 = tl.zeros((BLOCK_SIZE_KV2, HEAD_DIM), tl.float32) kv1_start = tl.maximum(0, q_start - w1) kv1_end = tl.minimum(seq_len, q_end) for kv1_idx in tl.range(kv1_start, kv1_end, num_stages=num_stages): k1_offs = kv1_idx * k1_stride_s + qkv_offs_h * k1_stride_h v1_offs = kv1_idx * v1_stride_s + qkv_offs_h * v1_stride_h k1_tile = tl.load(K1_ptr + k1_offs, mask=qkv_mask_h).to( compute_dtype ) # [HEAD_DIM] v1_tile = tl.load(V1_ptr + v1_offs, mask=qkv_mask_h).to( compute_dtype ) # [HEAD_DIM] qk1_s = q_tile * (k1_tile[None, :] * softmax_scale) # [Q, D] qk1_s = qk1_s.to(gemm_dtype) # k2[KV, Q] @ qk1_s.T[Q, D] => [KV2, Q] qkkT = tl.dot(k2_tile, qk1_s.T, out_dtype=tl.float32) # [KV2, Q] qkT_mask = kv2_mask_s[:, None] & q_mask_s[None, :] kv1_local_mask = ((q_offs_s[None, :] - w1) < kv1_idx) & ( kv1_idx <= q_offs_s[None, :] # [KV2, Q] ) kv2_local_mask = ((q_offs_s[None, :] - w2) < kv2_offs_s[:, None]) & ( kv2_offs_s[:, None] <= q_offs_s[None, :] # [KV2, Q] ) local_mask = ( kv1_local_mask & kv2_local_mask # [BLOCK_SIZE_KV, BLOCK_SIZE_Q] ) qkT_mask &= kv1_local_mask & kv2_local_mask pT = tl.exp(qkkT - m_tile[None, :]) # [KV2, Q] pT = tl.where(qkT_mask, pT, 0.0) qkkT = tl.where(local_mask, qkkT, -1.0e38) dOv1 = dO_tile * v1_tile[None, :] # [Q, D] 24 188 189 191 192 193 194 195 197 198 199 200 201 203 204 205 206 207 209 210 211 212 213 215 216 217 dOv1 = dOv1.to(gemm_dtype) # pT[KV2, Q] @ dOv1[Q, D] => [KV2, D] dv2 += tl.dot(pT.to(gemm_dtype), dOv1, out_dtype=tl.float32) # v2[KV2, D] @ dOv1.T[D, Q] => dpT[KV2, Q] dpT = tl.dot(v2_tile, dOv1.T, out_dtype=tl.float32) dsT = pT * (dpT - d_tile[None, :]) # [KV2, Q] dsT = tl.where(qkT_mask, dsT, 0.0) dsT = dsT.to(gemm_dtype) # [KV2, Q] # dsT[KV2, Q] @ qk1[Q, D] => dk2[KV2, D] dk2 += tl.dot(dsT, qk1_s, out_dtype=tl.float32) k1k2 = k1_tile[None, :] * k2_tile # [KV2, D] k1k2 = k1k2.to(gemm_dtype) dq += tl.dot(dsT.T, k1k2) # * softmax scale at the end. # End. update derivatives. if IS_SECOND_PASS: #load, add. prev_dk2 = tl.load(dK2_ptr + kv2_offs, kv2_mask) prev_dv2 = tl.load(dV2_ptr + kv2_offs, kv2_mask) dk2 += prev_dk2 dv2 += prev_dv2 dq *= softmax_scale tl.store(dK2_ptr + kv2_offs, dk2, kv2_mask) tl.store(dV2_ptr + kv2_offs, dv2, kv2_mask) tl.store(dQ_ptr + q_offs, dq, q_mask) Listing 3: Backward pass for 2-simplicial attention optimized for small w2 avoiding atomic adds."
        }
    ],
    "affiliations": [
        "Department of Computer Science University of Texas at Austin",
        "Meta Menlo Park, CA"
    ]
}