{
    "paper_title": "Panoramic Interests: Stylistic-Content Aware Personalized Headline Generation",
    "authors": [
        "Junhong Lian",
        "Xiang Ao",
        "Xinyu Liu",
        "Yang Liu",
        "Qing He"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Personalized news headline generation aims to provide users with attention-grabbing headlines that are tailored to their preferences. Prevailing methods focus on user-oriented content preferences, but most of them overlook the fact that diverse stylistic preferences are integral to users' panoramic interests, leading to suboptimal personalization. In view of this, we propose a novel Stylistic-Content Aware Personalized Headline Generation (SCAPE) framework. SCAPE extracts both content and stylistic features from headlines with the aid of large language model (LLM) collaboration. It further adaptively integrates users' long- and short-term interests through a contrastive learning-based hierarchical fusion network. By incorporating the panoramic interests into the headline generator, SCAPE reflects users' stylistic-content preferences during the generation process. Extensive experiments on the real-world dataset PENS demonstrate the superiority of SCAPE over baselines."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 2 ] . [ 1 0 0 9 1 1 . 1 0 5 2 : r Panoramic Interests: Stylistic-Content Aware Personalized Headline Generation Junhong Lian Institute of Computing Technology, Chinese Academy of Sciences Beijing, China lianjunhong23s@ict.ac.cn Xiang Ao Institute of Computing Technology, Chinese Academy of Sciences Beijing, China aoxiang@ict.ac.cn Xinyu Liu Institute of Computing Technology, Chinese Academy of Sciences Beijing, China liuxinyu@ict.ac.cn Yang Liu Institute of Computing Technology, Chinese Academy of Sciences Beijing, China liuyang2023@ict.ac.cn Qing He Institute of Computing Technology, Chinese Academy of Sciences Beijing, China heqing@ict.ac.cn Abstract Personalized news headline generation aims to provide users with attention-grabbing headlines that are tailored to their preferences. Prevailing methods focus on user-oriented content preferences, but most of them overlook the fact that diverse stylistic preferences are integral to users panoramic interests, leading to suboptimal personalization. In view of this, we propose novel Stylistic-Content Aware Personalized Headline Generation (SCAPE) framework. SCAPE extracts both content and stylistic features from headlines with the aid of large language model (LLM) collaboration. It further adaptively integrates users longand short-term interests through contrastive learning-based hierarchical fusion network. By incorporating the panoramic interests into the headline generator, SCAPE reflects users stylistic-content preferences during the generation process. Extensive experiments on the real-world dataset PENS demonstrate the superiority of SCAPE over baselines. CCS Concepts Computing methodologies Natural language generation; Information systems Personalization; Data mining. Keywords Personalized Headline Generation, Stylistic-Content Awareness Fusion, User Preference Modeling, Large Language Models Corresponding authors. Key Lab of Intelligent Information Processing of Chinese Academy of Sciences (CAS). Institute of Intelligent Computing Technology, CAS, Suzhou, China. High Performance Computer Research Center, ICT, CAS. The authors are also with the University of Chinese Academy of Sciences, CAS. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or fee. Request permissions from permissions@acm.org. WWW 25 Companion, 28 April - 2 May, 2025, Sydney, Australia 2025 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-XXXX-X/18/06 https://doi.org/XXXXXXX.XXXXXXX ACM Reference Format: Junhong Lian, Xiang Ao, Xinyu Liu, Yang Liu, and Qing He. 2025. Panoramic Interests: Stylistic-Content Aware Personalized Headline Generation. In Companion Proceedings of the ACM Web Conference 2025 (WWW 25 Companion), 28 April - 2 May, 2025, Sydney, Australia. ACM, New York, NY, USA, 4 pages. https://doi.org/XXXXXXX.XXXXXXX"
        },
        {
            "title": "1 Introduction\nGenerating attractive news headlines has been firmly established\nas a special form of text summarization [7, 12]. Numerous efforts\nhave focused on generating attention-grabbing headlines through\npersonalized approaches, recognizing that readers with diverse\npreferences may find different focal points in the same news [1,\n2]. These methods aimed to build engaging headlines tailored to\nindividual users’ reading interests by utilizing auxiliary information,\nsuch as user profiles and historical clicks [15, 16, 19, 21]. In spite of\nthat, existing personalized approaches primarily emphasize user-\noriented content-driven headlines, while largely overlooking the\nstylistic features of news headlines.",
            "content": "The stylistic features of news headlines are pivotal in journalism communication [3, 5]. Boosting headline engagement through text style transfer has become widely-used approach [9, 20]. Style transfer-based methods incorporate specific stylistic elements or employ interrogative forms in news headlines to capture readers attention [9, 14]. Nevertheless, these style transfer-based methods rely on single, uniform stylistic strategy to boost engagement risks crossing into clickbait [1, 19]. To improve content consistency, recent works [10, 20] have emphasized the style-content duality in headline generation, focusing on transferring to prototype headline styles while maintaining content decoupling. Despite these efforts, style transfer-based methods are unsatisfactory because they have yet to achieve user-oriented personalization. In this work, we focus on encompassing both content interests and stylistic preferences to fully meet users needs for personalized headline generation. The reason is that even readers with similar content preferences may exhibit distinct style preferences for headlines due to different personal reading habits. For example, consider the case illustrated in Figure 1, wherein users content focus and stylistic preferences jointly affect the personalization of headlines. WWW 25 Companion, 28 April - 2 May, 2025, Sydney, Australia Lian et al."
        },
        {
            "title": "2.2.1 Headline Inference. Extracting the inherent features from\nheadlines without ground-truth labels is challenging, but LLMs\nwith extensive world knowledge offer a promising solution for\ninferring high-level latent concepts. Building on this, the headline\ninference module in SCAPE employs an instruction-tuned LLM,\ndenoted as 𝐿𝐿𝑀𝑖𝑛𝑠𝑡 , to infer text style and content interests from\nheadlines 𝑡𝑖 in news database D as follows:",
            "content": "𝑅𝑠 = 𝐿𝐿𝑀𝑖𝑛𝑠𝑡 (𝑡𝑖, Pstyle ) 𝑅𝑐 = 𝐿𝐿𝑀𝑖𝑛𝑠𝑡 (𝑡𝑖, Pcontent ) where Pstyle and Pcontent are the prompts designed to instruct the LLM to infer stylistic features and content interests, respectively, with 𝑅𝑠 and 𝑅𝑐 as corresponding responses. (1) Inspired by instruction-following text embedding [13], we design task-specific instructions 𝐼𝑠 and 𝐼𝑐 for embedding style and interest. These instructions are concatenated with news headlines and their corresponding responses, which are then encoded by an embedding-based LLM, denoted as 𝐿𝐿𝑀𝑒𝑚𝑏 . The offline stored headline embedding table 𝐸 is constructed as follows: 𝐸𝑠 = 𝐿𝐿𝑀𝑒𝑚𝑏 ( [𝑡𝑖, 𝐼𝑠, 𝑅𝑠 ] ) 𝐸𝑐 = 𝐿𝐿𝑀𝑒𝑚𝑏 ( [𝑡𝑖, 𝐼𝑐, 𝑅𝑐 ] ) (2) 𝐸 = { (𝐸𝑠 (𝑡𝑖 ), 𝐸𝑐 (𝑡𝑖 ) ) 𝑛𝑖 = (𝑡𝑖, 𝑏𝑖 ) } (3) where 𝐸𝑠 (𝑡𝑖 ) and 𝐸𝑐 (𝑡𝑖 ) represent the style and content embeddings of headline 𝑡𝑖 , respectively, and 𝐸 is the set of style-content embedding pairs for all headlines in D."
        },
        {
            "title": "2.2.2 Hierarchical Stylistic-Content Awareness Fusion. Given that\nhistorical clicks reflect both stable personal traits and recent short-\nterm characteristics, we employ an attention mechanism to aggre-\ngate all clicks and a GRU network for the 𝐾 recent clicks. This\nyields the user’s long-term content interests representation 𝑢𝑙\n𝑐 and\nshort-term content interests representation 𝑢𝑠",
            "content": "𝑐 as follows: 𝐸𝑐 (𝑡ℎ 𝑗 ) = MLP(𝐸𝑐 (𝑡ℎ 𝑗 ) ) 𝑗 {1, . . . , 𝐿} 𝑢𝑙 𝑐 = Attn( 𝐸𝑐 (𝑡ℎ1 ), . . . , 𝐸𝑐 (𝑡ℎ𝐿 ) ) (4) (5) 𝑢𝑠 𝑐 = GRU( [ 𝐸𝑐 (𝑡ℎ𝐿𝐾 + (6) where 𝐸𝑐 (𝑡ℎ 𝑗 ) is the projected content embedding for the 𝑗-th historical click with 𝐿 > 𝐾. Similarly, we obtain the users long-term and short-term stylistic preferences, denoted as 𝑢𝑙 𝑠 and 𝑢𝑠 𝑠 . ), . . . , 𝐸𝑐 (𝑡ℎ𝐿 ) ] ) Figure 1: Illustration of the Joint Influence of Content Interests and Stylistic Preferences on Headline Personalization. We mark their content interests and stylistic patterns with different colors and text highlighting. Base on User As historical click patterns, we can infer that User prefers the subject-subordinate title format and interrogative forms to increase engagement, while User favors numbered list headlines and short exclamatory sentences. Hence, we contend that both content interests and stylistic preferences are interwoven to form users panoramic interests in personalization and must be considered simultaneously. However, simultaneously addressing this issue is challenging. primary obstacle is how to extract the inherent content and stylistic features encapsulated in headlines without ground-truth labels. Moreover, another challenge is to decouple implicit user panoramic interests and integrate them into headlines without explicit supervision. To remedy the above challenges, we propose novel framework named Stylistic-Content Aware Personalized Headline Generation (SCAPE). We first designed headline inference module that uses LLM to infer content and stylistic attributes for each news headline individually. It subsequently produces an offline embedding table that encapsulates these attributes at the headline-level. Furthermore, hierarchical gated fusion network is devised to adaptively integrate users longand short-term content interests and stylistic preferences. In addition, self-supervised strategy is adopted to decouple the intertwined user preferences. Lastly, personalized injection module leverages the fused user representation to guide lightweight generator to produce final outputs. This ensures that the generated headlines reflect users panoramic interests. Our main contributions are summarized as follows: To the best of our knowledge, this work represents the first attempt to incorporate both content and style preferences from user profiles for personalized news headline generation. We introduce novel framework, SCAPE, which extracts the inherent features of headlines through LLM collaboration and facilitates personalized generation via hierarchical fusion of both content interests and stylistic preferences. Extensive evaluations conducted on the real-world personalized news headline generation benchmark PENS [2] demonstrate the superior performance of our method. Panoramic Interests: Stylistic-Content Aware Personalized Headline Generation WWW 25 Companion, 28 April - 2 May, 2025, Sydney, Australia 𝑢𝑠 and 𝑝𝑠 𝑢𝑐 and 𝑝𝑠 interest representations. Specifically, we aggregate users historical LS-term clicks using mean aggregation to derive proxies for stylistic preferences (𝑝𝑙 𝑢𝑠 ) and content interests (𝑝𝑙 We employ contrastive learning between user LS-term representations and proxies, ensuring that the learned representations of LS-term style or content are more similar to their respective proxies than to opposite proxies. For content interests modeling, users long-term content interest 𝑢𝑙 𝑐 should be more similar to the long-term content proxy 𝑝𝑙 𝑢𝑐 than to the short-term content proxy 𝑢𝑐 or the long-term style proxy 𝑝𝑙 𝑝𝑠 𝑢𝑠 . The triplet loss function is used to achieve contrastive learning, 𝑢𝑐 ). and the final contrastive learning loss can computed as follows: 𝑅 𝑠, 𝑢𝑠 LCL = 1 1 N𝑅 𝑁 N𝑅 max (0, 𝑑 (𝑅, 𝑃𝑅 ) 𝑑 (𝑅, 𝑁 ) + 𝑚) (10) where = {𝑢𝑙 𝑐 } is user representations set, and N𝑅 is negative proxies set. 𝑃𝑅 is the positive proxy corresponding to 𝑅. 𝑑 denotes the Euclidean distance, and 𝑚 is positive margin value. 𝑐, 𝑢𝑠 𝑠 , 𝑢𝑙 Figure 2: Architecture of the Proposed SCAPE Framework. We then obtain the fusion of longand short-term representations corresponding to different types of interests, as follows: (7) 𝛼 = 𝜎 (𝑊𝑔𝑐 [𝑢𝑙 𝑐, 𝑢𝑠 𝑐 ] + 𝑏𝑔𝑐 ) 𝑐 + (1 𝛼 ) 𝑢𝑠 𝑐 𝛽 = 𝜎 (𝑊𝑔𝑠 [𝑢𝑙 𝑠 ] + 𝑏𝑔𝑠 ) 𝑠, 𝑢𝑠 𝑠 + (1 𝛽 ) 𝑢𝑠 𝑠 𝑢𝑠 = 𝛽 𝑢𝑙 𝑢𝑐 = 𝛼 𝑢𝑙 (8) where 𝜎 denotes the sigmoid activation function, and 𝛼 is the gating weight for content interests. 𝑊𝑔𝑐 and 𝑏𝑔𝑐 are learnable parameters. Similarly, we derive the gating weight 𝛽 and representation 𝑢𝑠 for style preferences through the same process. Subsequently, we integrate the 𝑢𝑐 and 𝑢𝑠 based on the attention weights derived from the candidate article representation. This stylistic-content awareness fusion mechanism enables the stepwise integration of diverse representations of the user."
        },
        {
            "title": "2.2.3 Personalized Injection Module Guided Generator. Once the\nuser representation 𝑈 is obtained, SCAPE integrates it into a de-\ncoder of the lightweight headline generator through the person-\nalized injection module. Specifically, the user representation 𝑈 is\nadded to the input embeddings X of each token in the decoder. This\nuser-specific vector is subsequently propagated through the resid-\nual flow, thereby influencing the generated headlines. By integrat-\ning user preferences at the token level, the headline generator can\nbetter align its output with the user’s interests, producing stylistic-\ncontent-aware personalized headlines that effectively reflect their\npanoramic interests, formulated as follows:",
            "content": "X = + (1𝑛 𝑈 ) (9) where R𝑛𝑑 , 𝑈 R𝑑 , 𝑛 is the sequence length, 𝑑 is the embedding dimension, and 1𝑛 R𝑛 is an all-ones vector."
        },
        {
            "title": "3 Experiments\n3.1 Experimental Setup\n3.1.1 Datasets and Baselines. We use the publicly available person-\nalized news headline generation dataset PENS [2] as benchmark.\nThe dataset comprises 500, 000 anonymized impressions from over\n445, 000 users and 113, 762 news articles, capturing detailed his-\ntorical click data to reflect nuanced personalized preferences. The\ntest set includes 3, 940 news items annotated by 103 users, each\nproviding 200 unique parallel headlines as the gold standard for\npersonalized headline evaluation.",
            "content": "We compare SCAPE with SOTA personalized headline generation methods on the PENS benchmark, including pointer-networkbased frameworks [1, 2, 21] and methods based on pre-trained language models [15, 19]. We also expanded our evaluation by employing prompt-based method to comprehensively investigate the performance of LLMs against strong personalized baselines, evaluating both open-source LLMs [4, 18] of various sizes and closed-source LLMs [6, 8, 11] via API services."
        },
        {
            "title": "3.1.2 Evaluation Metrics. We use ROUGE metrics to measure lexi-\ncal similarity, with ROUGE-1 and ROUGE-2 for informativeness,\nand ROUGE-L for fluency. To evaluate the factual consistency of\ngenerated headlines, we follow previous work [19] that reported\nFact Scores. As for personalization, due to the lack of a widely\naccepted evaluation method, we design a pairwise comparison\ntask [17], where strong LLMs evaluate candidates against original\nheadlines based on the user’s historical clicks. To minimize bias, we\nswap the contextual order of candidates and perform two indepen-\ndent assessments, marking inconsistencies as ties. Personalization\nperformance is reported as \"win/tie/lose\" outcomes between the\ncandidates and original headlines across baseline methods.",
            "content": "Implementation Details. We use FlanT5-base as the backbone 3.1.3 for the headline generator. The model is pre-trained on general headline generation, as [15, 19], for 2 epochs with peak learning rate (LR) of 1𝑒 4 and cosine decay. Early stopping is applied within 5 epochs for subsequent steps, with peak LRs set to 1𝑒 3, 1𝑒 6, and 1𝑒 5, respectively. Other details are consistent with WWW 25 Companion, 28 April - 2 May, 2025, Sydney, Australia Lian et al. Table 1: Performance of the Compared Baseline Methods. Types Methods ROUGE-1 ROUGE-2 ROUGE-L Fact Scores Open-source LLMs LLMs API services FlanT5-large FlanT5-XL Qwen2.5-1.5B Qwen2.5-7B GLM-4-Flash GLM-4-Air DeepSeek-V2.5 GPT-4o 24.91 27.73 27.45 29.54 26.91 27.45 28.27 29.58 8.48 10.08 9.45 10.47 8.40 8.89 9.55 11.03 21.13 23.23 22.24 24.04 21.47 22.03 22.60 24. 80.83 84.16 83.34 88.67 85.88 88.02 86.90 90.11 Personalized models PENS-NRMS PENS-NAML PNG EUI-PENS FPG GTP 26.15 28.01 28.78 32.34 33.06 33.84 34.26 The symbol * denotes the significance level with 𝑝 0.05. Bold font indicates the best-performing method. Underline indicates the second-best results in the group. 50.73 50.16 51.23 NA 89.55 NA 92.36 9.37 10.72 11.27 13.93 13.76 14.23 14.79 21.03 22.24 22.39 26.90 26.78 27.85 28.36 SCAPEours Figure 3: Win Rates in Personalization Evaluation. prior work [19]. The collaborative LLMs are Qwen2.5-72B-Instruct and GTE-Qwen2-7B-Instruct. We employ the Huggingface ROUGE pipeline as prior work [15] and re-report results for FPG [19]. In the pairwise comparison evaluation of personalization performance, we use Qwen2.5-72B-Instruct as the judge to obtain the comprehensive assessment. We use NVIDIA A800 80GB GPU for our experiments."
        },
        {
            "title": "3.2 Results and Analysis\nTable 1 compares SCAPE with baseline methods, demonstrating\nits superior performance and setting a new benchmark for person-\nalized headline generation in SOTA results. SCAPE significantly\noutperforms other methods in informativeness and fluency. This\nresult highlights that SCAPE better improves content coverage\nthrough its stylistic-content-aware user modeling via LLM collabo-\nration. Furthermore, SCAPE emphasizes the importance of balanc-\ning long- and short-term content interests and stylistic preferences\nto improve the factual consistency of personalized headlines.",
            "content": "Figure 3 shows the win rates of personalization evaluation using LLMs with customized prompts as judges. While GPT-4o shows higher win rates than the personalized model FPG, LLMs still struggle to generate personalized headlines based on prompt engineering. This underscores the challenge posed by the complexity and diversity of users clicks preferences. Our SCAPE framework takes crucial step forward in personalized headlines generation by considering both user-oriented content interests and modeling linguistic style preferences in users historical clicks."
        },
        {
            "title": "4 Conclusion\nIn this paper, we propose SCAPE, a novel framework to tackle\nthe challenges of insufficiently user preference modeling in per-\nsonalized headline generation. SCAPE introduces a headline in-\nference module that extracts the inherent stylistic features and",
            "content": "content interests from news headlines without explicit supervision. hierarchical gated fusion mechanism is further introduced to dynamically combine both longand short-term interests for panoramic user modeling, which then guides the headline generator to produce stylistic-content aware personalized headlines. Extensive experiments on the PENS dataset show that SCAPE sets new state-of-the-art benchmark for personalized headline generation. Acknowledgments The research work supported by National Key R&D Plan No.2022YFC3303303, the National Natural Science Foundation of China under Grant (No. U2436209, 62476263). Xiang Ao is also supported by the Project of Youth Innovation Promotion Association CAS, Beijing Nova Program 20230484430, the Innovation Funding of ICT, CAS under Grant No. E461060. References [1] Xiang Ao, Ling Luo, Xiting Wang, et al. 2023. Put Your Voice on Stage: Personalized Headline Generation for News Articles. ACM TKDD (2023). [2] Xiang Ao, Xiting Wang, Ling Luo, Ying Qiao, Qing He, and Xing Xie. 2021. PENS: Dataset and Generic Framework for Personalized News Headline Generation. In ACL. [3] Allan Bell. 1991. The language of news media. Blackwell Oxford. [4] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2024. Scaling instruction-finetuned language models. JMLR (2024). [5] John Fiske. 2010. Introduction to communication studies. Routledge. [6] Team GLM, Aohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, Da Yin, Dan Zhang, Diego Rojas, Guanyu Feng, Hanlin Zhao, et al. 2024. Chatglm: family of large language models from glm-130b to glm-4 all tools. arXiv preprint arXiv:2406.12793 (2024). [7] Xiaotao Gu, Yuning Mao, Jiawei Han, et al. 2020. Generating Representative Headlines for News Stories. In The Web Conference. [8] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. 2024. Gpt-4o system card. arXiv preprint arXiv:2410.21276 (2024). [9] Di Jin, Zhijing Jin, Joey Tianyi Zhou, et al. 2020. Hooks in the Headline: Learning to Generate Headlines with Controlled Styles. In ACL. [10] Mingzhe Li, Xiuying Chen, Min Yang, et al. 2021. Learning to Write Eye-Catching Headlines via Disentanglement. In AAAI. [11] Aixin Liu, Bei Feng, Bin Wang, Bingxuan Wang, Bo Liu, Chenggang Zhao, Chengqi Dengr, Chong Ruan, Damai Dai, Daya Guo, et al. 2024. Deepseekv2: strong, economical, and efficient mixture-of-experts language model. arXiv preprint arXiv:2405.04434 (2024). [12] Ling Luo, Xiang Ao, Yan Song, Feiyang Pan, Min Yang, and Qing He. 2019. Reading like HER: Human Reading Inspired Extractive Summarization. In EMNLP-IJCNLP. [13] Letian Peng, Yuwei Zhang, Zilong Wang, Jayanth Srinivasa, Gaowen Liu, Zihan Wang, and Jingbo Shang. 2024. Answer is All You Need: Instruction-following Text Embedding via Answering the Question. In ACL. [14] Kai Shu, Suhang Wang, Thai Le, et al. 2018. Deep Headline Generation for Clickbait Detection. In ICDM. [15] Yun-Zhu Song, Yi-Syuan Chen, Lu Wang, and Hong-Han Shuai. 2023. General then Personal: Decoupling and Pre-training for Personalized Headline Generation. TACL (2023). [16] Xiaoyu Tan, Leijun Cheng, Xihe Qiu, et al. 2024. Enhancing Personalized Headline Generation via Offline Goal-Conditioned Reinforcement Learning with Large Language Models. In KDD. [17] Peiyi Wang, Lei Li, Liang Chen, Zefan Cai, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui. 2024. Large language models are not fair evaluators. In ACL. [18] An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, et al. 2024. Qwen2 technical report. arXiv preprint arXiv:2407.10671 (2024). [19] Zhao Yang, Junhong Lian, and Xiang Ao. 2023. Fact-Preserved Personalized News Headline Generation. In ICDM. [20] Boning Zhang and Yang Yang. 2023. MediaHG: Rethinking Eye-catchy Features in Social Media Headline Generation. In EMNLP. [21] Kui Zhang, Guangquan Lu, Guixian Zhang, Zhi Lei, and Lijuan Wu. 2022. Personalized Headline Generation with Enhanced User Interest Perception. In ICANN. [22] Yu Zheng, Chen Gao, Jianxin Chang, Yanan Niu, Yang Song, Depeng Jin, and Yong Li. 2022. Disentangling long and short-term interests for recommendation. In The Web Conference."
        }
    ],
    "affiliations": [
        "Institute of Computing Technology, Chinese Academy of Sciences",
        "Key Lab of Intelligent Information Processing of Chinese Academy of Sciences (CAS)"
    ]
}