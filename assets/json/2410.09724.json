{
    "paper_title": "Taming Overconfidence in LLMs: Reward Calibration in RLHF",
    "authors": [
        "Jixuan Leng",
        "Chengsong Huang",
        "Banghua Zhu",
        "Jiaxin Huang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Language model calibration refers to the alignment between the confidence of the model and the actual performance of its responses. While previous studies point out the overconfidence phenomenon in Large Language Models (LLMs) and show that LLMs trained with Reinforcement Learning from Human Feedback (RLHF) are overconfident with a more sharpened output probability, in this study, we reveal that RLHF tends to lead models to express verbalized overconfidence in their own responses. We investigate the underlying cause of this overconfidence and demonstrate that reward models used for Proximal Policy Optimization (PPO) exhibit inherent biases towards high-confidence scores regardless of the actual quality of responses. Building upon this insight, we propose two PPO variants: PPO-M: PPO with Calibrated Reward Modeling and PPO-C: PPO with Calibrated Reward Calculation. PPO-M integrates explicit confidence scores in reward model training, which calibrates reward models to better capture the alignment between response quality and verbalized confidence. PPO-C adjusts the reward score during PPO based on the difference between the current reward and the moving average of past rewards. Both PPO-M and PPO-C can be seamlessly integrated into the current PPO pipeline and do not require additional golden labels. We evaluate our methods on both Llama3-8B and Mistral-7B across six diverse datasets including multiple-choice and open-ended generation. Experiment results demonstrate that both of our methods can reduce calibration error and maintain performance comparable to standard PPO. We further show that they do not compromise model capabilities in open-ended conversation settings."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 3 1 ] . [ 1 4 2 7 9 0 . 0 1 4 2 : r TAMING OVERCONFIDENCE IN LLMS: REWARD CALIBRATION IN RLHF Jixuan Leng1, Chengsong Huang2, Banghua Zhu3, Jiaxin Huang2 1Carnegie Mellon University, 2Washington University in St. Louis, 3UC Berkeley 1jixuanl@cs.cmu.edu, 2{chengsong, jiaxinh}@wustl.edu, 3banghua@berkeley.edu"
        },
        {
            "title": "ABSTRACT",
            "content": "Language model calibration refers to the alignment between the confidence of the model and the actual performance of its responses. While previous studies point out the overconfidence phenomenon in Large Language Models (LLMs) and show that LLMs trained with Reinforcement Learning from Human Feedback (RLHF) are overconfident with more sharpened output probability, in this study, we reveal that RLHF tends to lead models to express verbalized overconfidence in their own responses. We investigate the underlying cause of this overconfidence and demonstrate that reward models used for Proximal Policy Optimization (PPO) exhibit inherent biases towards high-confidence scores regardless of the actual quality of responses. Building upon this insight, we propose two PPO variants: PPO-M: PPO with Calibrated Reward Modeling and PPO-C: PPO with Calibrated Reward Calculation. PPO-M integrates explicit confidence scores in reward model training, which calibrates reward models to better capture the alignment between response quality and verbalized confidence. PPO-C adjusts the reward score during PPO based on the difference between the current reward and the moving average of past rewards. Both PPO-M and PPO-C can be seamlessly integrated into the current PPO pipeline and do not require additional golden labels. We evaluate our methods on both Llama3-8B and Mistral-7B across six diverse datasets including multiple-choice and open-ended generation. Experiment results demonstrate that both of our methods can reduce calibration error and maintain performance comparable to standard PPO. We further show that they do not compromise model capabilities in open-ended conversation settings."
        },
        {
            "title": "INTRODUCTION",
            "content": "As Large Language Models (LLMs) significantly expand their functionality to wide range of applications from complex problem solving (Wei et al., 2022; Song et al., 2023a) to science discovery (Imani et al., 2023; OpenAI, 2023), the importance of their reliability becomes increasingly critical. key aspect of this reliability is language model calibration the alignment between model confidence and its actual performance. LLM confidence can be assessed through two primary methods: logit-based approaches, derived from output token probability distributions, and verbalized expressions, where the model explicitly states its confidence level. In this paper, we focus on verbalized confidence, where we prompt LLMs to express confidence score for their responses (Figure 1, Top). Reinforcement Learning from Human Feedback (RLHF) has significantly improved LLM performance through two key components: reward modeling, which learns to predict human preferences from ranking datasets, and policy optimization guided by reward models, typically implemented with Proximal Policy Optimization (PPO) (Schulman et al., 2017). However, recent studies (Kadavath et al., 2022; OpenAI, 2023) show that RLHF-trained LLMs tend to exhibit overconfidence, potentially due to sharpened output distributions. Previous works address LLM confidence through various methods. Scaling-based approaches (Guo et al., 2017; Zhang et al., 2020) adjust model logits with decoding temperature, while verbalized confidence is enhanced via prompting strategies (Tian et al., 1Our code is released at https://github.com/SeanLeng1/Reward-Calibration. 1 Figure 1: (Top): Illustration of verbalized confidence generation. An LLM incorrectly answers question with high confidence. (Bottom): Comparison between reward scores from vanilla-trained reward model Llama-3-8b-rm-mixture and our calibrated reward model Llama-3-8b-crm. The vanilla model shows bias towards high confidence though the answer is incorrect. Our calibrated reward model correctly assigns higher reward to the low confidence for the incorrect answer. 2023) and supervised fine-tuning (Lin et al., 2022) using ground truth accuracy. More recently, RLHF-based calibration methods (Xu et al., 2024; Tao et al., 2024) are proposed. Our study investigates the underlying causes of overconfidence introduced by RLHF. We demonstrate empirical evidence that RLHF-LLMs elicit verbalized overconfidence compared to their pre-RLHF counterparts. We then reveal system bias in reward models favoring responses with high confidence scores, regardless of the actual response quality, which potentially causes the poor calibration in RLHF-LLMs. We propose two novel solutions to be seamlessly integrated into the RLHF process without additional golden labels. PPO with Calibrated Reward Modeling (PPO-M) calibrates the reward modeling process. We augment the binary pairwise ranking dataset with explicit confidence scores, and encourages the reward model to align confidence levels with response quality (Figure 1, Bottom). PPO with Calibrated Reward Calculation (PPO-C) adjusts standard reward model scores during PPO training. It maintains running average of past reward scores as dynamic threshold to classify responses, and adjusts the reward scores based on model expressed verbalized confidence. We conduct experiments on Llama3-8B and Mistral-7B across six datasets, and demonstrate that both PPO-M and PPO-C consistently outperform vanilla PPO, achieving lower Expected Calibration Error (ECE) while maintaining comparable or higher accuracy (PPO-M on Llama3-8B reduces ECE by 6.44 points and increases accuracy by 2.73 points on GSM8K (Cobbe et al., 2021)). Moreover, evaluation on MT-Bench (Zheng et al., 2024) shows that PPO-M and PPO-C preserve model capabilities in general open-ended conversation settings. We further demonstrate that PPO-M generalizes well to Direct Preference Optimization (DPO) models (Rafailov et al., 2024), which are implicit reward models. Experiments show that our extension, denoted as CDPO, reduces ECE without sacrificing accuracy compared to standard DPO."
        },
        {
            "title": "2 EXPLORING SYSTEMATIC BIASES AND OVERCONFIDENCE IN RLHF-LLMS",
            "content": "In this section, we demonstrate the preliminary experiments that reveal overconfidence in RLHFLLMs and systematic biases in Reward Models, which motivated the development of our methods."
        },
        {
            "title": "2.1 RLHF-LLMS EXHIBIT OVERCONFIDENCE IN THEIR VERBALIZED CONFIDENCE",
            "content": "Previous studies have shown that LLMs tend to exhibit overconfidence when verbalizing their confidence scores (Tian et al., 2023; Chen et al., 2024a; Xiong et al., 2023). However, there is lack of systematic comparisons between RLHF-LLMs and their pre-RLHF counterparts. To address this gap, we conduct preliminary experiments here to further investigate this phenomenon. Llama3-8B-SFT and Llama3-8B-PPO; Tulu-2-7B and Tulu-2-DPO-7B Figure 2: Confidence distributions and corresponding accuracy of two models on CommonsenseQA before and after RLHF. Darker color means more samples fall in that confidence bin. The red dashed line indicates perfect calibration. Setup. We show results on multiple-choice question answering dataset, CommonsenseQA (Talmor et al., 2019). We use four off-the-shelf models 2 for our analysis. We compare RLHF models (trained with PPO and DPO) with their pre-RLHF versions. For each question in the dataset, we explicitly prompt the model to verbalize its confidence score on scale from 0 to 10 after providing its answer to the question. We report the distribution of these confidence scores in Figure 2. Evaluations on other datasets and detailed information on the experimental setup, including prompts and parsing details, are provided in Appendix and E.1. Observations. As illustrated in Figure 2, there is clear trend on both datasets that the RLHF models, whether trained using PPO or DPO, exhibit greater overconfidence compared to their SFT counterparts. Specifically, the SFT models display more diverse confidence distribution, while RLHF models show confidence scores predominantly at the higher levels. This confirms the phenomenon that RLHF models tend to be more confident when verbalizing their confidence scores."
        },
        {
            "title": "2.2 REWARD MODELS ARE BIASED TOWARD HIGH CONFIDENCE SCORES",
            "content": "In this section, we hypothesize that the observed overconfidence in RLHF-LLMs stems from an inherent bias in reward models favoring higher confidence score appended after responses. We conduct experiments to demonstrate this preference. Setup. We employ the RewardBench Dataset (Lambert et al., 2024), following its experimental configuration with certain adjustments to examine how reward models process explicit confidence scores in responses. We evaluate RLHFlow/ArmoRM-Llama3-8B-v0.1 (Wang et al., 2024c) and allenai/tulu-2-dpo-7b (Ivison et al., 2023). Specifically, we prepend confidence-query 2OpenRLHF/Llama-3-8b-sft-mixture OpenRLHF/Llama-3-8b-rlhf-100k allenai/tulu-2-7b allenai/tulu-2-dpo-7b 3 Figure 3: Preference distributions for ArmoRM-Llama3-8B-v0.1, reward model for PPO training (left) and Tulu-2-DPO-7B, DPO model (right) on the modified RewardBench dataset across four modes. From top to bottom: CONFIDENCE REVERSED, CHOSEN WITH CONF, REJECTED WITH CONF, ANSWER ONLY. Red bar indicates the preference for rejected or highconfidence response, and blue bar indicates the preference for chosen or low-confidence response. System Prompt For the following question, provide your best response first, followed by your confidence in the accuracy or helpfulness of your response. Rate your confidence on scale from 0 to 10. Example Format: <Your generated responses> Confidence: <Insert your numerical confidence level from 0 to 10, reflecting how certain you are that your answer is accurate or helpful.> Ensure that your response strictly adheres to this format. Explicitly include the word Confidence: in your response. Figure 4: Confidence-Query System Prompt for verbalizing confidence scores. system prompt as illustrated in Figure 4; if the reward model does not support system prompts, we prepend it into the user prompt instead. This helps the model interpret the scale of confidence scores. Subsequently, we append random confidence score, Confidence:{random score}, to each model response. For comprehensive comparison, we explore four modes: 1) ANSWER ONLY: applying the original RewardBench dataset without modifications; 2) CONFIDENCE REVERSED: the system prompt is prepended. high confidence score (random integer from 7 to 10) is appended to the rejected response, and low confidence score (random integer from 0 to 3) to the chosen response; 3) CHOSEN WITH CONF: the system prompt is prepended, but we compare identical chosen responses with high versus low confidence scores; 4) REJECTED WITH CONF: similar to CHOSEN WITH CONF, but comparing high and low confidence scores for identical rejected responses. We report the preference count for each model. Given that DPO models are implicit reward models (Rafailov et al., 2024), we also include evaluation on DPO models. We provide more details on the modified data and evaluations on other reward models in Appendix C.1 and E.2 Observations. According to Figure 3, when evaluated on the original RewardBench dataset (ANSWER ONLY), both models effectively discriminate between chosen and rejected responses by assigning higher reward scores to chosen responses. It is important to note that in typical pairwise preference datasets, distinctions between the chosen and rejected responses such as length, tone, and correctness are usually pronounced. However, even accounting for these differences, simply modifying the query prompt and assigning low confidence score to the chosen response and high confidence score to the rejected response can significantly alter model behavior. As depicted in CONFIDENCE REVERSED, the number of preferred rejected responses with high confidence largely increases, indicating that the models ability to distinguish between chosen and rejected response is distorted. In CHOSEN WITH CONF and REJECTED WITH CONF where identical responses are compared with different confidence scores, we observe that reward models clearly prefers responses with higher confidence scores, regardless of whether the response is originally chosen or rejected. 4 These observations suggest that reward models exhibit systematic bias towards responses with high confidence scores, and potentially explain the overconfidence of RLHF-LLMs."
        },
        {
            "title": "3 CALIBRATED REWARD MODELING AND CALCULATION",
            "content": "Drawing from observations in previous sections, we propose two methods here to address the bias in reward scores: calibrated reward modeling (PPO-M) and calibrated reward calculation (PPO-C). Background: Reward Modeling. Typical reward model training uses pairwise human preference data with binary ranking labels (chosen and rejected). Let = {(xi, yi r)}n i=1 be the training dataset for the reward model, where xi is the prompt, and yi is the chosen response preferred over the rejected response yi r. binary preference ranking loss (Ouyang et al., 2022) is applied to enforce that the chosen responses receives higher score than the rejected one, as illustrated in Eq. 1. c, yi Lpreference = E(x,yc,yr)D [log σ (Rθ(x, yc) Rθ(x, yr))] where the reward model Rθ is typically initialized from the SFT model. The LM head on top of the last layer is replaced with linear layer to yield single scalar reward prediction Rθ (x, y) for given prompt and response y. Here, yc and yr denote the chosen and rejected responses respectively. (1) PPO-M: PPO with Calibrated Reward Modeling. Existing reward model training datasets typically do not include prompts requesting verbalized confidence scores or responses including explicit confidence levels. To address this gap, we propose straightforward modification to the existing binary pairwise ranking dataset by incorporating confidence-query system prompt (as shown in Fig. 4) and appending random confidence scores to model responses, consistent with the format in our preliminary experiments. This results in modified training dataset for the reward (cid:1) , (cid:0)yi model, denoted as ˆD = (cid:8)(cid:0)ˆxi, (cid:0)yi i=1, where ˆxi represents the prompt with confidence-query system prompt prepended, and represent random high and low confidence scores, respectively. We propose the calibrated reward modeling loss as follows: (cid:1) , (cid:0)yi (cid:1) , (cid:0)yi r, hi c, hi r, li (cid:1)(cid:1)(cid:9)n c, li LPPO-M = (ˆx,(yc,hc),(yc,lc),(yr,hr),(yr,lr))) ˆD (cid:104) log σ (Rθ (ˆx, (yc, hc)) Rθ (ˆx, (yc, lc))) + log σ (Rθ (ˆx, (yr, lr)) Rθ (ˆx, (yr, hr))) (cid:105) (2) This encourages the reward model to prefer high over low verbalized confidence for chosen responses and prefer low over high verbalized confidence for rejected responses. Note that the calibration dataset is not intended for training reward models from scratch. Instead, considering the availability of various off-the-shelf reward models, we assume reward models for calibration are already trained beforehand and generally perform well. Then in PPO training, we replace the pre-calibrated reward model with this calibrated one to generate reward scores. Figure 5: Framework for PPO-C. PPO-C: PPO with Calibrated Reward Calculation. While PPO-M addresses bias in reward model training, we propose an alternative approach, PPO-C, to directly improve PPO training by adjusting the reward calculation process. PPO-C can be seamlessly integrated into the original PPO procedure without modifying the reward model. We modify the original PPO training set by replacing certain amount of prompts with the confidencequery system prompt (shown in Fig. 4) to ask for both the answer and verbalized confidence. This results in mixed dataset with each sample denoted as (xi, yi, si) where xi and yi are the prompt and corresponding model response, and si is an optional verbalized confidence generated by the model if xi asks for confidence score. For samples without confidence querying, we simply use their original reward ri = R(xi, yi) for model updating. For samples with confidence querying, we propose calibrated reward calculation procedure to mitigate the bias in the reward score ri = R(xi, yi, si). We first parse and remove the confidence score from the model response to obtain an unbiased response (xi, yi). This allows us to obtain an unbiased reward score ˆri = R(xi, yi). We maintain moving average of the reward scores, defined by rt = α rt + (1 α) rt1, where α is set to 0.1, to serve as dynamic threshold for classifying the current model response as positive or negative. We then adjust the reward score as follows: ri = (cid:26)ˆri + γ ˆri γ if ˆri rt if ˆri < rt (3) where γ is the reward adjustment factor, defined as γ = ˆri (si 0.5). is scaling coefficient set to 0.5. This coefficient adjusts the extent of the adjustment applied to the unbiased reward ˆr, based on the rescaled confidence score si, which is normalized from 0 to 1.0. We handle missing confidence scores by assigning default confidence of 0.5, to keep the reward score unchanged. The overall framework for PPO-C is illustrated in Fig. 5."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "We evaluate PPO-M and PPO-C on two model families: Llama3-8B and Mistral-7B. We use their supervised fine-tuned versions3 (i.e., OpenRLHF/Llama-3-8b-sft-mixture, teknium/OpenHermes-2.5-Mistral-7B) as the starting point for reward model and RLHF training. We explore two prompting stategies: Direct Answers (DA) and Zero-Shot Chain-of-Thought (CoT) (Kojima et al., 2022). For Direct Answers, we utilize regex parsing to extract model responses and confidence scores. For Zero-Shot CoT, we use gpt-4o-2024-08-06 (Achiam et al., 2023) to parse confidence scores and compare model responses with golden answers. Detailed descriptions of prompts, implementation, and parsing methods are available in Appendix D.5. We consider two evaluation metrics: Expected Calibrated Error (ECE) (Guo et al., 2017) and Area Under the Receiver Operating Characteristic Curve (AUC) (Hendrycks & Gimpel, 2016). ECE bins model predictions by confidence and measures the prediction accuracy within each category. AUC measures the ability of the model to distinguish between correct and incorrect responses."
        },
        {
            "title": "4.1 EXPERIMENTAL SETUP",
            "content": "We employ OpenRLHF4 (Hu et al., 2024) for reward model and RLHF training. All training experiments are conducted on 4A100 GPUs, and evaluations are carried out on one A100 GPU. RM Checkpoints. For Llama3-8B, we employ the readily available reward model OpenRLHF/Llama-3-8b-rm-mixture (Hu et al., 2024), trained from the corresponding SFT checkpoint. For Mistral-7B, we train reward model using logsigmoid loss, as shown in Eq. 1, from scratch on Skywork/Skywork-Reward-Preference-80K-v0.1 (Liu & Zeng, 2024). For additional training details, please refer to Appendix D.1. RM Calibration Dataset. We employ mixture of open-source datasets, and filter samples to ensure high distinction between scores of chosen and rejected responses. Subsequently, we prepend the confidence-query system prompt shown in Fig 4 to each response. We then randomly assign high and low confidence scores to create four response types: chosen with high/low confidence 3These models are instruction-tuned and do not undergo the RLHF process. 4https://github.com/OpenRLHF/OpenRLHF 6 and rejected with high/low confidence. For additional information on dataset compositions, the predetermined threshold, and other details, please refer to Appendix C.3. Evaluation Datasets. We use six diverse datasets for model evaluation: GSM8K (Cobbe et al., 2021) for Arithmetic Reasoning, CommonsenseQA (Talmor et al., 2019) for Commonsense Knowledge, SciQ (Welbl et al., 2017) for Science Knowledge, ObjectCounting from BigBench (Srivastava et al., 2022) for Symbolic Reasoning, four Professional Knowledge datasets (Professional Medicine, Professional Law, Professional Accounting and Professional Psychology) in MMLU (Hendrycks et al., 2020), and TruthfulQA (Lin et al., 2021) for Truthful Reasoning. The datasets include both open-ended generation and multiple-choice questions. RLHF Dataset. We use subset of RLHFlow/prompt-collection-v0.1 (Dong et al., 2024) considering computational resources. We randomly select 20,480 prompts and integrate confidence-query system prompt into 25% of single-turn prompts to elicit verbalized confidence from the model, as exemplified in Figure 4. For clarity, we refer the original 20,480 prompts as the clean version and those with the confidence-query system prompts added as the modified version. Compared Methods. We compare our PPO-M and PPO-C with the following methods: (1) the SFT model, which is the initial checkpoint before RLHF training; (2) the PPO model, which uses vanilla reward model in standard PPO training on the clean version dataset without confidence querying in system prompt; (3) PPO, an ablation of our PPO-M, which includes confidence-query system prompts (modified version) in PPO training, but still uses the vanilla reward model."
        },
        {
            "title": "4.2 MAIN RESULTS",
            "content": "Both PPO-M and PPO-C consistently outperform other baselines across Llama3-8B and Mistral-7B. In Table 1 we show the results of all five methods on six datasets. If we compare vanilla PPO and SFT, vanilla PPO indeed shows degradation in calibration (higher ECE and lower AUC) though generally improves in accuracy. Among all methods, PPO-M and PPO-C consistently demonstrate lower ECE and higher AUC than all the baselines across both models and prompting strategies, indicating their improved calibration ability. Meanwhile, PPO-M and PPO-C preserve comparable and even higher accuracy, showing that their calibration ability is not compromised by model performance. Compared to PPO, an ablation of PPO-M, PPO-M and PPO-C demonstrate better calibration ability. This is because while PPO includes confidence-query system prompts in PPO training, it still uses the vanilla reward model instead of the calibrated reward model introduced in Sec. 3. This further indicates that properly calibrating the reward score could mitigate the bias towards high-confidence responses. Figure 6: Preference distributions of our calibrated reward model Llama-3-8b-crm and the precalibrated version Llama-3-8b-rm-mixture on REJECTED WITH CONF. Calibrated Reward Models. Figure 6 presents the preference distributions of the calibrated reward model compared to the pre-calibrated version. When evaluated on rejected responses with high and low confidence scores, unlike pre-calibrated version that consistently prefers the high-confidence responses, the calibrated reward model tends to favor those with low confidence behavior we aim to achieve. Further results for CHOSEN WITH CONF are detailed in Appendix E.3."
        },
        {
            "title": "5 ANALYSIS",
            "content": "In this section, we explore how our proposed methods affect language model abilities in instructionfollowing and engagement in conversational settings. Additionally, we also present an extension of our approach to Direct Preference Optimization (DPO) models. 7 Methods GSM8K SciQ CommonsenseQA ECE AUC ACC ECE AUC ACC ECE AUC ACC Llama3-8B SFT 0. 0.5184 0.1221 0.0931 0.6067 DA 0.8843 PPO PPO 0.8954 PPO-M 0.8393 PPO-C 0. 0.5021 0.5 0.57 0.516 0.1099 0.1046 0.119 0.1031 0.0683 0.0958 0.0267 0.0282 0.6507 0.5047 0.6115 0.6513 SFT 0. 0.5138 0.5481 0.0944 0.65 CoT 0.2566 PPO PPO 0.2553 PPO-M 0.1909 PPO-C 0. 0.5229 0.5044 0.5499 0.5662 0.7392 0.743 0.7703 0.7422 0.0862 0.1265 0.0392 0.0403 0.6763 0.5452 0.6635 0.6383 Mistral-7B SFT 0.8628 0.5747 0.0902 0.0952 0.5877 DA 0.8675 PPO PPO 0.8851 PPO-M 0.7963 PPO-C 0.8702 0.583 0.5464 0.5055 0.634 0.097 0.0877 0.1016 0.0811 0.0973 0.1117 0.0108 0.0865 0.5497 0.5439 0.5090 0.5758 SFT 0.4124 0.5277 0.5785 0.1124 0.6238 CoT 0.4146 PPO PPO 0.3932 PPO-M 0.3379 PPO-C 0.3653 0.5228 0.5096 0.5974 0.5312 0.58 0.6035 0.5982 0.624 0.1126 0.1044 0.0388 0.1006 0.5794 0.5693 0.6584 0.6425 0. 0.911 0.904 0.898 0.904 0.856 0.879 0.868 0.877 0.889 0.882 0.89 0.885 0.888 0.891 0. 0.877 0.885 0.886 0.892 0.2075 0.5889 0.7183 0.1729 0.2222 0.1206 0.1286 0.5815 0.5113 0.5568 0. 0.7641 0.7748 0.7707 0.7756 0.1928 0.6155 0.7101 0.1767 0.2654 0.1555 0.1581 0.6287 0.5615 0.579 0. 0.7363 0.7191 0.7346 0.7396 0.1634 0.1772 0.1848 0.1163 0.1608 0.56 0.774 0.5594 0.5674 0.5303 0. 0.7748 0.7756 0.7625 0.7723 0.1908 0.6205 0.7518 0.1867 0.2056 0.1157 0.1767 0.6238 0.6135 0.6118 0. 0.7699 0.7518 0.7666 0.7633 Methods TruthfulQA Object Counting Professional Knowledge ECE AUC ACC ECE AUC ACC ECE AUC ACC Llama3-8B SFT 0.4613 0. 0.4113 0.5054 0.5212 DA 0.425 PPO PPO 0.5477 PPO-M 0.3991 PPO-C 0.4426 0.5443 0.5246 0.5813 0. 0.4651 0.4406 0.47 0.4431 0.508 0.497 0.4789 0.4839 0.4988 0.5 0.5227 0.5178 SFT 0.4436 0. 0.4174 0.4545 0.5102 CoT 0.4726 PPO PPO 0.5535 PPO-M 0.4283 PPO-C 0.4306 0.5851 0.5921 0.5674 0. 0.4113 0.4076 0.437 0.448 0.3651 0.337 0.2863 0.2491 0.5023 0.5 0.5341 0.5273 Mistral-7B SFT 0. 0.5755 0.5704 0.5083 0.4989 DA 0.3335 PPO PPO 0.3233 PPO-M 0.245 PPO-C 0. 0.5567 0.5651 0.5568 0.5933 0.5826 0.601 0.6071 0.5912 0.5008 0.5119 0.4248 0.5114 0.5 0.499 0.5067 0.5019 SFT 0. 0.6067 0.5398 0.4862 0.5072 CoT 0.3677 PPO PPO 0.3657 PPO-M 0.3142 PPO-C 0. 0.5911 0.6089 0.6399 0.5851 0.5581 0.5594 0.541 0.5459 0.4599 0.455 0.4134 0.4431 0.4991 0.5022 0.5496 0.5014 0.483 0.491 0.503 0.505 0. 0.54 0.634 0.663 0.703 0.738 0.491 0.499 0.488 0.483 0.488 0.512 0.54 0.543 0.56 0. 0.4308 0.5175 0.4798 0.4078 0.4951 0.3848 0.3949 0.4944 0.4975 0.4926 0.4902 0.5046 0.5009 0.502 0. 0.4644 0.5571 0.4242 0.4309 0.5496 0.4329 0.4423 0.5606 0.5219 0.5422 0.5403 0.4635 0.4316 0.4424 0. 0.4134 0.5018 0.5031 0.4303 0.4571 0.3716 0.4244 0.4889 0.4919 0.489 0.4881 0.4994 0.4872 0.502 0. 0.4863 0.5369 0.4554 0.4783 0.4735 0.4090 0.4634 0.5275 0.5215 0.5526 0.5504 0.4761 0.4865 0.4579 0. Table 1: Performance comparison across various methods on six datasets. SFT: Supervised FineTuned checkpoints, serving as the starting points for all methods. PPO: an ablation of our PPO-M method which uses vanilla reward model in PPO training but on our modified dataset (with confidencequery system prompts). 8 5.1 INSTRUCTION-FOLLOWING CAPABILITIES Dataset. To assess whether PPO-M and PPO-C compromise the instruction-following abilities of LLMs gained through PPO, we evaluate their performance on two benchmarks: MT-Bench (Zheng et al., 2024) and Arena-Hard (Li et al., 2024). MT-Bench consists of 80 high-quality, multi-turn questions designed to evaluate LLMs across various aspects, while Arena-Hard contains 500 technical problem-solving queries and demonstrates higher agreement with human preference rankings. Method MT-Bench Arena-Hard SFT 7.34 Model Llama3-8B PPO-M and PPO-C do not compromise LLM instruction-following abilities. Table 2 presents the average MT-Bench and Arena-Hard scores. It is observed that PPO enhances model performance compared to SFT, as expected. Furthermore, PPO-M and PPO-C trained model also exhibit comparable to or even slightly better scores than PPO, indicating that our calibration methods do not compromise instructionIn contrast, PPO shows following abilities. inferior performance relative to our proposed methods and PPO. We hypothesize that this is due to the reduced prompt diversity brought by confidence-query system prompt. We verified that increasing the proportion of the same system prompt correlates with decreased MT-Bench score, as detailed in Appendix E.4. PPO PPO PPO-M PPO-C PPO PPO PPO-M PPO-C 7.84 7.83 7.95 7.98 8.00 7.81 8.05 8.05 Mistral-7B 7.65 SFT 10.0 14.6 13.4 14.1 14.1 9.2 10.5 11.7 9.9 11.8 Table 2: Results on MT-Bench and Arena-Hard."
        },
        {
            "title": "5.2 EXTENSION TO DPO",
            "content": "Setup. As PPO-M calibrates the reward model using an augmented binary pairwise dataset, we can naturally extend Eq. 2 to Direct Preference Optimization (DPO) training. This is because DPO models are implicit reward models (Rafailov et al., 2024). We denote our extension as Calibrated DPO (CDPO), and illustrate the loss function in Eq. 4. LCDPO(πθ; πref) = E(x,yc,yr,ˆx,(yc,h),(yc,l),(yr,h),(yr,l)))D [log σ(r(x, yc) r(x, yr)) +w(log σ(r(ˆx, (yc, h)) r(ˆx, (yc, l))) + log σ(r(ˆx, (yr, l)) r(ˆx, (yr, h))))] (4) where r(x, y) = β log πθ(yx) πref(yx) represents the implicit reward defined by model πθ and its reference model πref. (yc, h) and (yr, l) denote the model responses paired with high and low confidence respectively, and the subscripts indicate whether it is chosen or rejected response. ˆx represents the prompt with confidence-query system prompt prepended. is the scaling coefficient. The first term in Eq. 4 maintains the original DPO objective to prevent forgetting, since DPO models rely on subtle probability differences to distinguish between chosen and rejected responses. We use the Mistral-7B DPO version (i.e., teknium/OpenHermes-2.5-Mistral-7B) as reference model and the DPO model NousResearch/Nous-Hermes-2-Mistral-7B-DPO for the experiment, and fine-tune the DPO models on our RM calibration Dataset using Eq. 4. Model Method MT-Bench Arena-Hard Results. As shown in Table 4, CDPO significantly improves model calibration across all six datasets by achieving consistent lower ECE, and higher AUC compared to other methods. Notably, CDPO reduces ECE by more than 50% in TruthfulQA, CommonsenseQA, and Professional Knowledge. Although slight decline in performance is observed between CDPO and DPO, CDPO still exhibits peformance comparable to the DPO checkpoint, affirming that calibration does not compromise overall model performance. Results on MT-Bench and Arena-Hard are reported in Table 3. For Mistral-7B, training on additional data yields improvements in both Table 3: Comparison of MT-Bench And ArenaHard Performance for Mistral-7B. DPO CDPO Mistral-7B SFT DPO 7.65 7. 9.2 13.4 7.83 7.85 14.3 15.9 9 MT-Bench and Arena-Hard scores; CDPO further amplifies these enhancements compared to standard DPO on the calibration dataset (DPO). Results for Llama3-8B are included in Appendix E.7. Methods SFT DPO DPO CDPO SFT DPO DPO CDPO Methods SFT DPO DPO CDPO SFT DPO DPO CDPO DA CoT DA CoT GSM8K SciQ CommonsenseQA ECE AUC ACC ECE AUC ACC ECE AUC ACC 0.8628 0.8704 0.8057 0.6767 0.4124 0.4184 0.3456 0.1889 0.5747 0. 0.5409 0.6163 0.5277 0.5253 0.5953 0.7178 0.0902 0.0887 0.0826 0.0781 0.5785 0. 0.5989 0.6164 0.0952 0.0845 0.0149 0.0967 0.1124 0.094 0.0214 0.0553 0.5877 0. 0.5215 0.7236 0.6238 0.5837 0.6687 0.7623 0.882 0.892 0.884 0.89 0.872 0. 0.898 0.883 0.1634 0.177 0.1157 0.0513 0.1908 0.1849 0.0916 0.0676 0.56 0. 0.5491 0.6165 0.6205 0.6145 0.6553 0.6498 0.774 0.7682 0.7772 0.7666 0.7518 0. 0.7764 0.7633 TruthfulQA Object Counting Professional Knowledge ECE AUC ACC ECE AUC ACC ECE AUC ACC 0.3307 0.2912 0.2124 0.104 0.3657 0.3251 0.2169 0.1756 0.5755 0. 0.5674 0.6225 0.6067 0.629 0.6176 0.685 0.5704 0.6181 0.6487 0.661 0.5398 0. 0.6377 0.6193 0.5083 0.5149 0.4336 0.3955 0.4862 0.4581 0.4037 0.322 0.4989 0. 0.5436 0.5304 0.5072 0.5003 0.5585 0.5139 0.491 0.485 0.485 0.491 0.5120 0. 0.539 0.553 0.4134 0.4321 0.3649 0.2574 0.4863 0.4950 0.3679 0.2917 0.5018 0. 0.5208 0.5451 0.5369 0.5314 0.5587 0.614 0.5031 0.4913 0.5091 0.4972 0.4554 0. 0.4961 0.4817 Table 4: Performance comparison of SFT, DPO, DPO, and CDPO across six datasets using Mistral-7B. SFT and DPO denote the reference and trained DPO models, respectively. DPO and CDPO initiate from the trained DPO checkpoint; DPO applies standard DPO on the calibration dataset, focusing on chosen and rejected pairs to assess the impact of training with additional data."
        },
        {
            "title": "6 RELATED WORKS",
            "content": "LLM Calibration. Model Calibration aims to align models confidence with its accuracy. Recent studies show that LLMs often exhibit overconfidence (Tian et al., 2023; Chen et al., 2024a; Xiong et al., 2023; Achiam et al., 2023). Previous studies have explored methods such as scaling-based (Deng et al., 2023; Guo et al., 2017; Zhang et al., 2020) approaches and nonparametric methods such as binning (Zadrozny & Elkan, 2001). Recent work has introduced verbalized confidence (Lin et al., 2022), where models are prompted to directly output confidence scores. Most studies focus on pre-trained and instruction-tuned LLMs (Lin et al., 2022; Han et al., 2024), while other studies examine RLHF-trained LLMs, proposing calibration through prompting strategies (Xiong et al., 2023; Tian et al., 2023). More recent work leverages Reinforcement Learning for calibration (Xu et al., 2024; Tao et al., 2024), which aligns closely with our study. Our study contributes by identifying the potential cause for overconfidence in RLHF-LLMs and proposing calibration of the reward models or reward score calculations to be seamlessly integrated into the existing PPO framework. In addition, our approach does not compromise the models generalization capabilities in open-ended generation. LLM Alignment And Reward Modeling. Reinforcement Learning from Human Feedback (RLHF) (Ouyang et al., 2022; Christiano et al., 2017; Bai et al., 2022) has been widely applied to align LLMs with human preferences. This pipeline typically involves Supervised Fine-Tuning (SFT), reward modeling, and policy optimization using Proximal Policy Optimization (PPO) (Schulman et al., 2017). Recent works have explored variations of this pipeline to address the challenge of noisy human preferences (Hong et al., 2022; Wang et al., 2024a) and to improve computational efficiency with Direct Preference Optimization (DPO) (Rafailov et al., 2024; Dubey et al., 2024) to eliminate the need of separate reward model. We leave comprehensive discussion of related works in Appendix. A."
        },
        {
            "title": "7 CONCLUSION",
            "content": "This paper addresses overconfidence in RLHF-LLMs by identifying systematic bias in reward models that favors high-confidence responses regardless of their actual quality. We propose two solutions, PPO-M, which calibrates reward modeling by aligning confidence levels with response quality, and PPO-C, which adjusts standard reward model scores during PPO training. Both methods can be seamlessly integrated into the RLHF process. Extensive experiments across various benchmarks demonstrate the effectiveness of our methods in reducing expected calibration error without compromising accuracy and the instruction following ability in open-ended generation. Future research directions include extending our study to logit-based confidence, as well as exploring confidence calibration methods for open-ended questions."
        },
        {
            "title": "REFERENCES",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Mohammad Gheshlaghi Azar, Mark Rowland, Bilal Piot, Daniel Guo, Daniele Calandriello, Michal Valko, and Remi Munos. general theoretical paradigm to understand learning from human preferences, 2023. Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022. Alvaro Bartolome, Gabriel Martin, and Daniel Vila. Notus. https://github.com/ argilla-io/notus, 2023. Marco Bellagente, Jonathan Tow, Dakota Mahan, Duy Phung, Maksym Zhuravinskyi, Reshinth Adithyan, James Baicoianu, Ben Brooks, Nathan Cooper, Ashish Datta, et al. Stable lm 2 1.6 technical report. arXiv preprint arXiv:2402.17834, 2024. Ralph Allan Bradley and Milton Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3/4):324345, 1952. Souradip Chakraborty, Jiahao Qiu, Hui Yuan, Alec Koppel, Furong Huang, Dinesh Manocha, Amrit Singh Bedi, and Mengdi Wang. Maxmin-rlhf: Towards equitable alignment of large language models with diverse human preferences. arXiv preprint arXiv:2402.08925, 2024. Lihu Chen, Alexandre Perez-Lebel, Fabian Suchanek, and Gael Varoquaux. Reconfidencing llms from the grouping loss perspective. arXiv preprint arXiv:2402.04957, 2024a. Zhaorun Chen, Zhuokai Zhao, Zhihong Zhu, Ruiqi Zhang, Xiang Li, Bhiksha Raj, and Huaxiu Yao. Autoprm: Automating procedural supervision for multi-step reasoning via controllable question decomposition. North American Chapter of the Association for Computational Linguistics (NAACL), 2024b. Paul Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. Advances in neural information processing systems, 30, 2017. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Wei Zhu, Yuan Ni, Guotong Xie, Zhiyuan Liu, and Maosong Sun. Ultrafeedback: Boosting language models with high-quality feedback. arXiv preprint arXiv:2310.01377, 2023. 11 Luigi Daniele and Suphavadeeprasit. Amplify-instruct: Synthetically generated diverse multiturn conversations for efficient llm training. arXiv preprint arXiv:(coming soon), 2023. URL https://huggingface.co/datasets/LDJnr/Capybara. Ailin Deng, Miao Xiong, and Bryan Hooi. Great models think alike: Improving model reliability via inter-model latent agreement. arXiv preprint arXiv:2305.01481, 2023. Hanze Dong, Wei Xiong, Deepanshu Goyal, Rui Pan, Shizhe Diao, Jipeng Zhang, Kashun Shum, and Tong Zhang. Raft: Reward ranked finetuning for generative foundation model alignment. ArXiv preprint, abs/2304.06767, 2023a. URL https://arxiv.org/abs/2304.06767. Hanze Dong, Wei Xiong, Bo Pang, Haoxiang Wang, Han Zhao, Yingbo Zhou, Nan Jiang, Doyen Sahoo, Caiming Xiong, and Tong Zhang. Rlhf workflow: From reward modeling to online rlhf. arXiv preprint arXiv:2405.07863, 2024. Yi Dong, Zhilin Wang, Makesh Sreedhar, Xianchao Wu, and Oleksii Kuchaiev. Steerlm: Attribute In Findings of the Association for conditioned sft as an (user-steerable) alternative to rlhf. Computational Linguistics: EMNLP 2023, pp. 1127511288, 2023b. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Kawin Ethayarajh, Yejin Choi, and Swabha Swayamdipta. Understanding dataset difficulty with V-usable information. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato (eds.), Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pp. 59886008. PMLR, 1723 Jul 2022. Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan Jurafsky, and Douwe Kiela. Kto: Model alignment as prospect theoretic optimization, 2024. Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Weinberger. On calibration of modern neural networks. In International conference on machine learning, pp. 13211330. PMLR, 2017. Haixia Han, Tingyun Li, Shisong Chen, Jie Shi, Chengyu Du, Yanghua Xiao, Jiaqing Liang, and Xin Lin. Enhancing confidence expression in large language models through learning from past experience. arXiv preprint arXiv:2404.10315, 2024. Dan Hendrycks and Kevin Gimpel. baseline for detecting misclassified and out-of-distribution examples in neural networks. arXiv preprint arXiv:1610.02136, 2016. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and arXiv preprint Jacob Steinhardt. Measuring massive multitask language understanding. arXiv:2009.03300, 2020. Jiwoo Hong, Noah Lee, and James Thorne. Orpo: Monolithic preference optimization without reference model. arXiv preprint arXiv:2403.07691, 2(4):5, 2024. Joey Hong, Kush Bhatia, and Anca Dragan. On the sensitivity of reward inference to misspecified human models. arXiv preprint arXiv:2212.04717, 2022. Jian Hu, Xibin Wu, Weixun Wang, Xianyu, Dehao Zhang, and Yu Cao. Openrlhf: An easy-to-use, scalable and high-performance rlhf framework. arXiv preprint arXiv:2405.11143, 2024. Shima Imani, Liang Du, and Harsh Shrivastava. Mathprompter: Mathematical reasoning using large language models. ArXiv preprint, abs/2303.05398, 2023. URL https://arxiv.org/abs/ 2303.05398. Hamish Ivison, Yizhong Wang, Valentina Pyatkin, Nathan Lambert, Matthew Peters, Pradeep Dasigi, Joel Jang, David Wadden, Noah A. Smith, Iz Beltagy, and Hannaneh Hajishirzi. Camels in changing climate: Enhancing lm adaptation with tulu 2, 2023. 12 Jiaming Ji, Mickel Liu, Josef Dai, Xuehai Pan, Chi Zhang, Ce Bian, Boyuan Chen, Ruiyang Sun, Yizhou Wang, and Yaodong Yang. Beavertails: Towards improved safety alignment of llm via human-preference dataset. Advances in Neural Information Processing Systems, 36, 2024. Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli Tran-Johnson, et al. Language models (mostly) know what they know. arXiv preprint arXiv:2207.05221, 2022. Bradley Knox, Stephane Hatgis-Kessell, Serena Booth, Scott Niekum, Peter Stone, and AlessanarXiv preprint dro Allievi. Models of human preference for learning reward functions. arXiv:2206.02231, 2022. Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35: 2219922213, 2022. Lorenz Kuhn, Yarin Gal, and Sebastian Farquhar. Semantic uncertainty: Linguistic invariances for uncertainty estimation in natural language generation. arXiv preprint arXiv:2302.09664, 2023. Nathan Lambert, Valentina Pyatkin, Jacob Morrison, LJ Miranda, Bill Yuchen Lin, Khyathi Chandu, Nouha Dziri, Sachin Kumar, Tom Zick, Yejin Choi, Noah A. Smith, and Hannaneh Hajishirzi. Rewardbench: Evaluating reward models for language modeling, 2024. Tianle Li, Wei-Lin Chiang, Evan Frick, Lisa Dunlap, Tianhao Wu, Banghua Zhu, Joseph Gonzalez, and Ion Stoica. From crowdsourced data to high-quality benchmarks: Arena-hard and benchbuilder pipeline. arXiv preprint arXiv:2406.11939, 2024. Wing Lian, Bleys Goodson, Eugene Pentland, Austin Cook, Chanvichet Vong, and Teknium. Openorca: An open dataset of gpt augmented flan reasoning traces. https://https:// huggingface.co/Open-Orca/OpenOrca, 2023. Stephanie Lin, Jacob Hilton, and Owain Evans. Truthfulqa: Measuring how models mimic human falsehoods. arXiv preprint arXiv:2109.07958, 2021. Stephanie Lin, Jacob Hilton, and Owain Evans. Teaching models to express their uncertainty in words. arXiv preprint arXiv:2205.14334, 2022. Chris Yuhao Liu and Liang Zeng. Skywork reward model series. https://huggingface.co/ Skywork, September 2024. URL https://huggingface.co/Skywork. Zhihan Liu, Miao Lu, Shenao Zhang, Boyi Liu, Hongyi Guo, Yingxiang Yang, Jose Blanchet, and Zhaoran Wang. Provably mitigating overoptimization in rlhf: Your sft loss is implicitly an adversarial regularizer. arXiv preprint arXiv:2405.16436, 2024. Yu Meng, Mengzhou Xia, and Danqi Chen. SimPO: Simple preference optimization with referencefree reward. arXiv preprint arXiv:2405.14734, 2024. Matthias Minderer, Josip Djolonga, Rob Romijnders, Frances Hubis, Xiaohua Zhai, Neil Houlsby, Dustin Tran, and Mario Lucic. Revisiting the calibration of modern neural networks. Advances in Neural Information Processing Systems, 34:1568215694, 2021. Jishnu Mukhoti, Viveka Kulharia, Amartya Sanyal, Stuart Golodetz, Philip Torr, and Puneet Dokania. Calibrating deep neural networks using focal loss. Advances in Neural Information Processing Systems, 33:1528815299, 2020. Xavier Murias. Simple-math: 2+2=4 4-1=3. https://huggingface.co/datasets/ fblgit/simple-math, 2024. OpenAI. Gpt-4 technical report, 2023. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:27730 27744, 2022. 13 Junsoo Park, Seungyeon Jwa, Meiying Ren, Daeyoung Kim, and Sanghyuk Choi. Offsetbias: Leveraging debiased data for tuning evaluators, 2024. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36, 2024. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Chan Hee Song, Jiaman Wu, Clayton Washington, Brian Sadler, Wei-Lun Chao, and Yu Su. Llm-planner: Few-shot grounded planning for embodied agents with large language models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 29983009, 2023a. Feifan Song, Bowen Yu, Minghao Li, Haiyang Yu, Fei Huang, Yongbin Li, and Houfeng Wang. Preference ranking optimization for human alignment. ArXiv preprint, abs/2306.17492, 2023b. URL https://arxiv.org/abs/2306.17492. Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam Brown, Adam Santoro, Aditya Gupta, Adri`a Garriga-Alonso, et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615, 2022. Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. CommonsenseQA: question answering challenge targeting commonsense knowledge. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 41494158, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1421. URL https: //aclanthology.org/N19-1421. Shuchang Tao, Liuyi Yao, Hanxing Ding, Yuexiang Xie, Qi Cao, Fei Sun, Jinyang Gao, Huawei Shen, and Bolin Ding. When to trust llms: Aligning confidence with response quality. arXiv preprint arXiv:2404.17287, 2024. Katherine Tian, Eric Mitchell, Allan Zhou, Archit Sharma, Rafael Rafailov, Huaxiu Yao, Chelsea Finn, and Christopher Manning. Just ask for calibration: Strategies for eliciting calibrated confidence scores from language models fine-tuned with human feedback. arXiv preprint arXiv:2305.14975, 2023. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. Binghai Wang, Rui Zheng, Lu Chen, Yan Liu, Shihan Dou, Caishuang Huang, Wei Shen, Senjie Jin, Enyu Zhou, Chenyu Shi, et al. Secrets of rlhf in large language models part ii: Reward modeling. arXiv preprint arXiv:2401.06080, 2024a. Haoxiang Wang, Yong Lin, Wei Xiong, Rui Yang, Shizhe Diao, Shuang Qiu, Han Zhao, and Tong Zhang. Arithmetic control of llms for diverse user preferences: Directional preference alignment with multi-objective rewards. arXiv preprint arXiv:2402.18571, 2024b. Haoxiang Wang, Wei Xiong, Tengyang Xie, Han Zhao, and Tong Zhang. Interpretable preferences via multi-objective reward modeling and mixture-of-experts. arXiv preprint arXiv:2406.12845, 2024c. Zhilin Wang, Yi Dong, Jiaqi Zeng, Virginia Adams, Makesh Narsimhan Sreedhar, Daniel Egert, Olivier Delalleau, Jane Polak Scowcroft, Neel Kant, Aidan Swope, and Oleksii Kuchaiev. Helpsteer: Multi-attribute helpfulness dataset for steerlm, 2023. Zhilin Wang, Yi Dong, Olivier Delalleau, Jiaqi Zeng, Gerald Shen, Daniel Egert, Jimmy J. Zhang, Makesh Narsimhan Sreedhar, and Oleksii Kuchaiev. Helpsteer2: Open-source dataset for training top-performing reward models, 2024d. 14 Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. Johannes Welbl, Nelson Liu, and Matt Gardner. Crowdsourcing multiple choice science questions. arXiv preprint arXiv:1707.06209, 2017. Martin Weyssow, Aton Kamanda, and Houari Sahraoui. Codeultrafeedback: An llm-as-a-judge dataset for aligning large language models to coding preferences, 2024. Yuxin Xiao, Paul Pu Liang, Umang Bhatt, Willie Neiswanger, Ruslan Salakhutdinov, and LouisPhilippe Morency. Uncertainty quantification with pre-trained language models: large-scale empirical analysis. arXiv preprint arXiv:2210.04714, 2022. Miao Xiong, Zhiyuan Hu, Xinyang Lu, Yifei Li, Jie Fu, Junxian He, and Bryan Hooi. Can llms express their uncertainty? an empirical evaluation of confidence elicitation in llms. arXiv preprint arXiv:2306.13063, 2023. Tianyang Xu, Shujin Wu, Shizhe Diao, Xiaoze Liu, Xingyao Wang, Yangyi Chen, and Jing Gao. Sayself: Teaching llms to express confidence with self-reflective rationales. arXiv preprint arXiv:2405.20974, 2024. Lifan Yuan, Ganqu Cui, Hanbin Wang, Ning Ding, Xingyao Wang, Jia Deng, Boji Shan, Huimin Chen, Ruobing Xie, Yankai Lin, Zhenghao Liu, Bowen Zhou, Hao Peng, Zhiyuan Liu, and Maosong Sun. Advancing llm reasoning generalists with preference trees, 2024. Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, Songfang Huang, and Fei Huang. Rrhf: Rank responses to align language models with human feedback without tears. ArXiv preprint, abs/2304.05302, 2023. URL https://arxiv.org/abs/2304.05302. Bianca Zadrozny and Charles Elkan. Obtaining calibrated probability estimates from decision trees and naive bayesian classifiers. In Icml, volume 1, pp. 609616, 2001. Jize Zhang, Bhavya Kailkhura, and Yong-Jin Han. Mix-n-match: Ensemble and compositional methods for uncertainty calibration in deep learning. In International conference on machine learning, pp. 1111711128. PMLR, 2020. Yao Zhao, Rishabh Joshi, Tianqi Liu, Misha Khalman, Mohammad Saleh, and Peter Liu. Slic-hf: Sequence likelihood calibration with human feedback. ArXiv preprint, abs/2305.10425, 2023. URL https://arxiv.org/abs/2305.10425. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36, 2024. Zhanhui Zhou, Jie Liu, Chao Yang, Jing Shao, Yu Liu, Xiangyu Yue, Wanli Ouyang, and Yu Qiao. Beyond one-preference-for-all: Multi-objective direct preference optimization. arXiv preprint arXiv:2310.03708, 2023."
        },
        {
            "title": "A RELATED WORKS",
            "content": "LLM Calibration. Model Calibration aims to align models confidence with its accuracy. It has been observed that modern neural network, including Large Language Models (LLMs), often exhibit overconfidence, suggesting poor calibration (Tian et al., 2023; Chen et al., 2024a; Xiong et al., 2023; Achiam et al., 2023). Previous studies have explored methods such as scaling-based (Deng et al., 2023; Guo et al., 2017; Zhang et al., 2020) approaches and nonparametric methods such as binning (Zadrozny & Elkan, 2001). Among these, temperature scaling (Guo et al., 2017; Zhang et al., 2020) has been proved to be effective when combined with large pre-trained LLMs (Kadavath et al., 2022; Xiao et al., 2022; Kuhn et al., 2023). However, previous evaluations often focus on probabilities derived from model logits (Hendrycks et al., 2020; Mukhoti et al., 2020; Guo et al., 2017; Minderer et al., 2021), which can be inaccessible in proprietary models and are unintuitive to human users. Recently, verbalized confidence has been introduced (Lin et al., 2022), prompting models to directly output confidence scores. While most studies focus on pre-trained LLMs and calibrate them through supervised fine-tuning (Lin et al., 2022; Han et al., 2024), which typically involves sampling responses and calculating average accuracy as the ground truth confidence score, other studies have examined verbalized confidence in instruction-tuned and RLHF-trained LLMs, proposing calibration through prompting strategies (Xiong et al., 2023; Tian et al., 2023). More recent work leverages Reinforcement Learning for calibration (Xu et al., 2024; Tao et al., 2024), which aligns closely with our study. Our study contributes by identifying the potential cause for overconfidence in RLHF-LLMs and proposing calibration of the reward models or reward score calculations to reduce this issue. The proposed methods can be seamlessly integrated into the existing PPO framework and, unlike supervised fine-tuning (SFT) methods that require dataset with ground truth labels for accuracy calculationlimiting their use in open-ended generationour approach does not compromise the models generalization capabilities in such settings. LLM Alignment And Reward Modeling. Reinforcement Learning from Human Feedback (RLHF) (Ouyang et al., 2022; Christiano et al., 2017; Bai et al., 2022) has been widely applied to align LLMs with human preferences. This pipeline typically involves three steps: Supervised Fine-Tuning (SFT), the collection of pairwise ranking data and the development of reward model, and optimization of the policy model obtained from the first step using Proximal Policy Optimization (PPO) (Schulman et al., 2017). The effectiveness of PPO relies on the accuracy and robustness of the reward model. Following traditional Bradley-Terry reward models (Bradley & Terry, 1952), training typically utilizes binary pairwise dataset. However, human-labeled preferences can be noisy or present conflicting indications (Hong et al., 2022; Knox et al., 2022; Wang et al., 2024a). Several methods have been proposed to address this challenge, such as introducing margin to guide the reward model in assigning greater discrepancy to more separable comparison pairs (Touvron et al., 2023; Wang et al., 2024a), and empolying multi-objective reward modeling that considers joint preference such as helpfulness, correctness, coherence, etc. (Dong et al., 2023b; Zhou et al., 2023; Wang et al., 2024b; Chen et al., 2024b; Chakraborty et al., 2024; Wang et al., 2024c). While RLHF pipeline has been proven effective in aligning LLMs with human preferences, PPO involves several challenges, including reward hacking, sensitivity to hyperparameters, and substantial computational resource requirements, which complicate its implementation and usage. Several variants have been proposed (Dong et al., 2023a; Yuan et al., 2023; Zhao et al., 2023; Rafailov et al., 2024; Song et al., 2023b; Azar et al., 2023; Ethayarajh et al., 2024; Hong et al., 2024; Liu et al., 2024; Meng et al., 2024), and among these, Direct Preference Optimization (DPO) has been widely adopted (Rafailov et al., 2024; Dubey et al., 2024). DPO defines the preference loss as function of the policy model directly, thereby eliminating the need for separate reward model. Despite these advancements, there remains lack of research on how reward models contribute to the confidence calibration of LLMs. In this study, we bridge this gap by demonstrating the vulnerability of reward models trained using various methods, which can be biased by simple confidence score. Additionally, we explore methods to calibrate these models to reduce overconfidence in RLHF-LLMs."
        },
        {
            "title": "B LIMITATION AND BROADER IMPACT",
            "content": "B.1 LIMITATION Although we demonstrate that directly applying PPO-M style training loss to DPO training can reduce ECE and increase AUC, thereby enhancing calibration, this method is not specifically tailored for DPO. We observe some performance degradation, suggesting that future work could explore specific hyperparameter tuning or more specially designed dataset. B.2 BROADER IMPACT Since our work focuses on model calibration, it can be applied to various fields in need of wellcalibrated language models to improve reliability for general AI systems."
        },
        {
            "title": "C DATASETS",
            "content": "In this section, we provide detailed descriptions of the datasets used in this study. C.1 PRELIMINARY EXPERIMENTS DATASET In Section 2.2, we demonstrate the inherent bias in reward models toward responses with high confidence scores by modifying allenai/reward-bench. This modification involves prepending confidence-query system prompt and appending random confidence scores to the original model responses. We assigned scores ranging from 0 to 3 for low confidence and 7 to 10 for high confidence. Figure 7 displays an example from the modified dataset on REJECTED WITH CONF, featuring rejected response with low confidence scores appended. Moified Reward Bench Example System Prompts: For the following question, provide your best response first, followed by your confidence in the accuracy or helpfulness of your response. Rate your confidence on scale from 0 to 10. Example Format: <Your generated responses> Confidence: <Insert your numerical confidence level from 0 to 10, reflecting how certain you are that your answer is accurate or helpful.> Ensure that your response strictly adheres to this format. Explicitly include the word Confidence: in your response. User Prompt: How do detail car? Model Response: Detailing car involves washing the exterior and interior of the car, as well as polishing and waxing the exterior. Interior detailing typically involves vacuuming, cleaning the upholstery and air vents, polishing the dashboard and console, and dusting. Polishing and waxing the exterior will depend on the condition of the paint, but typically involves applying polish and wax to make it shine. Confidence: 2. Figure 7: An example from the Modified RewardBench in mode: REJECTED WITH CONF C.2 REWARD MODEL TRAINING DATASETS For Mistral-7B, we employ Skywork/Skywork-Reward-Preference-80K-v0.1 (Liu & Zeng, 2024) to train the reward model from scratch. C.3 REWARD MODEL CALIBRATION DATASETS. To compile the dataset for calibrating reward models, we filtered samples from open-source datasets. Below, we list the datasets utilized and the thresholds set for each in Table 5. Initially, we filter out samples that are multi-turn or have tokenized length exceeding 8192, as multi-turn formats are unsuitable for assigning confidence scores and we aim to prevent truncation. The threshold indicates the preference strength (Wang et al., 2024a), defined as the difference between chosen and rejected scores. Notably, in datasets such as RLHFlow/Argilla-Math-DPO-standard, preference strength less than 1 often means that both chosen and rejected responses yield the same answer through different reasoning paths. The goal is to calibrate the reward model to assign higher scores to to high-confidence chosen responses and lower scores to high-confidence rejected responses, while doing the opposite for low-confidence responses. However, when both responses yield the same mathematical solution through different reasoning, it is inappropriate for low-confidence rejected responses to get higher scores. Consequently, we exclude these samples and retain those with significant discrepancy between chosen and rejected responses. We set the threshold to retain about 2,500 samples per dataset considering computational resources. For datasets without specific chosen and rejected scores, we randomly select 2,500 samples."
        },
        {
            "title": "Threhold",
            "content": "argilla/distilabel-capybara-dpo-7k-binarized (Daniele & Suphavadeeprasit, 2023) RLHFlow/CodeUltraFeedback-standard (Weyssow et al., 2024) argilla/ultrafeedback-binarized-preferences-cleaned (Bartolome et al., 2023) RLHFlow/Helpsteer-preference-standard (Wang et al., 2023) RLHFlow/Helpsteer2-standard (Wang et al., 2024d) RLHFlow/Orca-distibalel-standard (Lian et al., 2023) RLHFlow/SHP-standard (Ethayarajh et al., 2022) RLHFlow/HH-RLHF-Helpful-standard (Bai et al., 2022) RLHFlow/Argilla-Math-DPO-standard RLHFlow/PKU-SafeRLHF-30K-standard (Ji et al., 2024) CyberNative/Code Vulnerability Security DPO fblgit/simple-math-DPO (Murias, 2024) Table 5: Datset compositions. 1 3 3.5 2.5 2.0 50 NA 1 NA NA NA C.4 PPO DATASETS For PPO training, we filter out prompts with tokenized length exceeding 8192 to prevent truncation and randomly select 20,480 prompts from RLHFlow/prompt-collection-v0.1 (Dong et al., 2024). We integrate confidence-query system prompt into single-turn prompts to elicit verbalized confidence from the model. The system prompt is used in 25% of the single-turn prompts for main results. Figure 8 illustrates an example from the dataset that incorporates this system prompt. C.5 EVALUATION DATASETS. We examine six datasets encompassing six distinct categories: Arithmetic Reasoning, Commonsense Knowledge, Symbolic Reasoning, Truthful Reasoning, and Professional Knowledge Collectively, these datasets include mix of open-ended generation tasks and multiple-choice questions. The specific datasets are detailed below: GSM8K (Cobbe et al., 2021): This dataset contains high-quality, linguistically diverse grade school math word problems. We utilize the test split, which contains 1319 samples. 18 PPO Prompts Example System Prompts: For the following question, provide your best response first, followed by your confidence in the accuracy or helpfulness of your response. Rate your confidence on scale from 0 to 10. Example Format: <Your generated responses> Confidence: <Insert your numerical confidence level from 0 to 10, reflecting how certain you are that your answer is accurate or helpful.> Ensure that your response strictly adheres to this format. Explicitly include the word Confidence: in your response. User Prompt: Write me an excel function to sum up the values in the cells in column to the left if the cell is full Figure 8: PPO Prompt Example. SciQ (Welbl et al., 2017) : This dataset contains crowdsourced science exams. We use the test split for evaluation, which includes 1000 example. It is multiple-choice dataset, each question offering four answer options. Similar to TruthfulQA and CommonsenseQA, we assign letter to each answer option and request the model to output the corresponding answer letter. CommonsenseQA (Talmor et al., 2019): This dataset comprises multiple-choice questionanswering format that requires commonsense knowledge. We utilize the test split, which contains 1221 samples. TruthfulQA (Lin et al., 2021):5 This dataset contains 817 questions designed to test whether the model can generate truthful responses without failing to recognize false beliefs and misconceptions. We utilize the multiple-choice format of the dataset and consider one single target answer. We randomly shuffle the answer options and corresponding true labels to ensure that the correct label is not predictably the first one. We format the questions as lettered multiple-choices and instruct the model to select the best answer from the options provided. Object Counting in BigBench (Srivastava et al., 2022): BigBench is collaborative benchmark encompassing over 200 tasks. For Symbolic Reasoning, we focus solely on one subset, Object Counting, which includes exactly 1000 samples. This open-ended generation task evaluates whether models can correctly identify the number of objects mentioned in the question prompt. Professional Knowledge in MMLU (Hendrycks et al., 2020): MMLU is multitask benchmark that includes multiple-choice questions from diverse knowledge domains. For the Professional Knowledge category, we combine the test sets from four subsets: Professional Accounting, Professional Law, Professional Medicine, and Professional Teaching."
        },
        {
            "title": "D IMPLEMENTATION DETAILS",
            "content": "In this section, we describe the implementation details for all experiments. D.1 REWARD MODEL TRAINING This study utilizes two reward models. For Llama3-8B, we use an off-the-shelf checkpoint from OpenRLHF/Llama3-8b-rm-mixture . For Mistral-7B, the reward model is trained from scratch using teknium/OpenHermes-2.5-Mistral-7B as the starting point, referred to as Mistral-7B-RM. 5https://huggingface.co/datasets/truthfulqa/truthful_qa/viewer/ multiple_choice D.1.1 HYPERPARAMETERS We list the hyperparameters used for training Mistral-7B-RM in Table 6. Parameter Mistral-7B Train BS Micro Train BS Learning Rate Max Length LR Scheduler Warmup Ratio Optimizer Weight Decay Epoch 512 1 2e-6 8192 cosine with min lr 0.03 AdamW 0.01 Table 6: Hyperparameters for training Mistral-7B-RM. D.2 REWARD MODEL CALIBRATION As stated in Section 3, we asssume reward models for calirbation are already trained beforehand and generally perform well. Therefore, we utilize trained RM checkpoints, namely OpenRLHF/Llama3-8b-rm-mixture and Mistral-7B-RM for calibration. The calibrated versions are referred to as Llama3-8b-crm and Mistral-7B-crm. D.2.1 HYPERPARAMETERS We list the hyperparameters used for calibrating OpenRLHF/Llama3-8b-rm-mixture and Mistral-7B-RM in PPO-M in Table 7."
        },
        {
            "title": "Parameter",
            "content": "Llama3-8b-crm Mistral-7B-crm"
        },
        {
            "title": "Train BS\nMicro Train BS\nLearning Rate\nMax Length\nLR Scheduler\nWarmup Ratio\nOptimizer\nEpoch",
            "content": "256 1 9e-6 8192 cosine with min lr 0.03 Adam 1 256 1 5e-6 8192 cosine with min lr 0.03 Adam 2 Table 7: Hyperparameters for Training Llama3-8B-crm and Mistral-7B-crm. D.3 PPO TRAINING Following the standard RLHF pipeline, we initialize the policy model using supervised fine-tuning checkpoints: OpenRLHF/Llama3-8b-sft-mixture for Llama3-8B, and teknium/OpenHermes-2.5-Mistral-7B for Mistal-7B. For standard PPO and PPO-C, we utilize the pre-calibrated reward models, namely OpenRLHF/Llama3-8b-rm-mixture and Mistral-7B-RM. For standard PPO, we input the entire sequence into the reward model to retrieve the score. For PPO-C, we perform our proposed calibrated reward calculation (see Section 3 for method descriptions). For PPO-M, we use the calibrated reward models, Llama3-8b-crm and Mistral-7B-crm, to calculate reward scores. 20 D.3.1 HYPERPARAMETERS For each model (Llama3-8B and Mistral-7B), we use consistent set of hyperparameters across PPO, PPO-M, and PPO-C as shown in Table 8."
        },
        {
            "title": "Parameter",
            "content": "Llama3-8B Mistral-7B"
        },
        {
            "title": "Train BS\nMicro Train BS\nMicro Rollout BS\nRollout BS\nPrompt max len\nGenerate max len\nActor Learning Rate\nCritic Learning Rate\nActor Weight Decay\nCritic Weight Decay\nInit KL Conf\nLR Scheduler\nWarmup Ratio\nOptimizer\nEpoch",
            "content": "64 2 4 512 1024 1024 5e-7 9e-6 0.0 0.0 0.01 cosine with min lr 0.03 Adam 1 64 2 4 512 1024 1024 1e-7 1e-6 0.01 0.0 0.05 cosine with min lr 0.03 Adam 1 Table 8: Hyperparameters for PPO Training. D.4 DPO TRAINING In Section 5.2, we extend calibrated reward modeling (PPO-M) to DPO training using Eq. 4. Similar to the calibration of reward models, we utilize trained DPO checkpoints. For Llama3-8B, we use Llama-3-Base-8B-SFT-DPO as DPO checkpoint, and use princeton-nlp/Llama-3-8B-Base-SFT as reference model. For Mistral-7B, we use NousResearch/Nous-Hermes-2-Mistral-7B-DPO as DPO checkpoint, with teknium/OpenHermes-2.5-Mistral-7B serving as reference model. D.4.1 HYPERPARAMETERS We list the hyperparameters used for DPO training Nous-Hermes-2-Mistral-7B-DPO and Llama-3-Base-8B-SFT-DPO in Table 9. The same set of hyperparameters is applied to both DPO and CDPO; however, the scaling coefficient is not utilized in DPO."
        },
        {
            "title": "Parameter",
            "content": "Llama3-8B Mistral-7B Train BS Micro Train BS Max Length Learning Rate Beta Weight Decay LR Scheduler Warmup Ratio Optimizer Epoch Zero Stage Adam Offload (scaling coefficient) 128 1 4096 3e-7 0.01 0.0 cosine with min lr 0.03 Adam 1 3 True 1.0 128 1 4096 3e-7 0.01 0.0 cosine with min lr 0.03 Adam 1 2 False 0.5 Table 9: Hyperparameters for DPO and CDPO Training. 21 D.5 EVALUATION AND PARSING We detail the generation configuration, prompting strategies, and parsing strategies utilized in our evaluations. D.5.1 GENERATION CONFIGURATION We maintain consistent configuration settings across both preliminary and main experiments: temperature is set at 1.0, top-p at 1.0, and top-k at 50. The maximum token count is set at 16 for direct answers and 256 for zero-shot CoT. D.5.2 EVALUATION HYPERPARAMETERS. Evaluations are conducted on single A100 80GB GPU with batch size of 8. D.6 EVALUATION PROMPTS Following the format described in Tian et al. (2023), we modify the prompt to enhance clarity and simplify result interpretation. We consider two prompting stratgies for evaluation: Direct Answer and Zero-Shot CoT (Kojima et al., 2022). The exact prompt is shown in Fig 9 and Fig 10, which also include model response from GSM8K. For answer_type: we use option letter for multiple-choice questions and number for open-ended math problems. For demo: we use (A) for multiple-choice questions and 1 for open-ended math problems. The prompt formatting utilizes the chat template in the tokenizer. Instructions are placed in the system prompt, and the question is placed in user prompt. For models like Tulu-2 (Ivison et al., 2023) that lacks system prompt section in the tokenizer chat template, we append the question after the instruction as the user prompt. Prompt for Direct Answers System Prompts: For the following question, provide your answer including only the {answer type} first, followed by your confidence in the accuracy or helpfulness of your response. Rate your confidence on scale from 0 to 10. Please respond only with your answer and numerical confidence score. Do not include any additional text, characters, or explanations. Use the format demonstrated below for your response. Example Format: Answer: <Insert only the {answer type} here (e.g., {demo})> Confidence: <Insert your numerical confidence level from 0 to 10, reflecting how certain you are that your answer is correct.> Ensure that your response strictly adheres to this format and contain only the {answer type} and the confidence score. Explicitly include the words Answer: and Confidence: in your response. User Prompt: Question: James decides to run 3 sprints 3 times week. He runs 60 meters each sprint. How many total meters does he run week? Model Output: Answer: 540 Confidence: 10 Figure 9: Direct Answers Prompt. 22 Prompt for Zero-Shot CoT System Prompts: For the following question, provide step-by-step explanation of your thought process first, then offer your best answer and rate your confidence in the accuracy or helpfulness of each response on scale from 0 to 10. Use the format demonstrated below for your response. Example Format: Explanation: <Your detailed explanation here, outlining how you arrived at your answer.> Answer: <Insert your concise answer here, which should include {answer type} (e.g., {demo})> Confidence: <Insert your numerical confidence level from 0 to 10, reflecting how certain you are that your answer is correct.> Ensure that your response strictly adheres to this format. Explicitly include the words Explanation:, Answer:, and Confidence: in your response. User Prompt: Question: James decides to run 3 sprints 3 times week. He runs 60 meters each sprint. How many total meters does he run week? Model Output: Explanation: James runs 3 sprints of 60 meters each, and he does this 3 times week. Therefore, he runs 3 sprints * 60 meters per sprint = 180 meters per session. Since he does this 3 times week, he runs total of 180 meters/session * 3 sessions = 540 meters in week. Answer: 540 meters Confidence: 10 Figure 10: Zero-Shot CoT Prompt. 23 D.7 PARSING DETAILS Regex Parsing Details. To parse the confidence score from model generation, we implement stopping criterion that halts only when digits follow the phrase Confidence: . For responses that are initially unparseable, we set retry limit of ten attempts. If parsing failures persist, we manually append Confidence: to the model response and resubmit it to the model for completion of the sentence and score generation. This helps us achieve nearly 100% success in parsing all responses. If parsing fails, which is very rare, we use an empty string as the default answer. Instead of assigning an arbitrary confidence of 5.0, which could introduce bias and decrease ECE, we use the most frequently observed confidence scores from successfully parsed responses as the default value for these rare cases, since they are the most representative confidence scores of the model. GPT-4o Evaluation Prompt System Prompt: You are specialized evaluator designed to assess model responses against golden answers for various tasks and extract model confidence. Output your evaluation in JSON format. User Prompt: Evaluate the semantic equivalence between the given model response and the provided golden answer. Determine if they convey the same meaning. If the model response accurately matches the golden answer (i.e., the model response is correct), assign score of 1. If the model response does not match the golden answer, assign score of 0. Additionally, extract the confidence score from the model response. If the model response does not explicitly state confidence score, return -100. Provide your answer in the following JSON format: {correctness: 1 or 0, confidence: X.X} Figure 11: Prompts for GPT4-o Evaluation. GPT-4o Evaluation Details. We use gpt-4o-2024-08-06 to evaluate zero-shot CoT results. Utilizing the structured output feature of GPT, we configure the model to output results in JSON format for easy parsing. The prompt is shown in Figure 11."
        },
        {
            "title": "E MORE RESULTS AND ANALYSIS",
            "content": "E.1 OVERCONFIDENCE IN RLHF-LLMS In this section, we provide additional results for our preminlinary experiments, which demonstate overconfidence in RLHF-LLMs on five more datasets, as shown in Figure 12, 13, 14, 15, and 16. The results indicate that RLHF-LLMs generally express verbalized overconfidence on various datasets. E.2 REWARD MODELS ARE BIASED TOWARD HIGH CONFIDENCE SCORES Following Section 2.2, we present additional results from various reward models to further substantiate the observed phenomenon. concern arises that the reward model may be biased by the confidence-query system prompt, which is included to ensure that the model verbalizes its confidence level. To further investigate the impact of this system prompt, we conduct additional experiments both with and without the system prompt. As shown in Figure 17, 18, and 19, the plots on the left follow the setting outlined in preliminary experiments, where confidence-query system prompt is prepended and random confidence scores are appended to model responses. These plots demonstrate that every tested reward model exhibits biased preference towards high-confidence responses to varying degrees. On the right, we still consider four modes, but this time without the confidence-query system prompts, and only random confidence scores are appended to the model responses. For instance, in REJECTED WITH CONF, the comparison would involve the same chosen responses with high confidence score against that with low confidence score. The results reveal similar, albeit more subtle, phenomenon. Llama3-8B-SFT and Llama3-8B-PPO; Tulu-2-7B and Tulu-2-DPO-7B Figure 12: Confidence distributions of two models on BBH Object Counting before and after RLHF. Llama3-8B-SFT and Llama3-8B-PPO; Tulu-2-7B and Tulu-2-DPO-7B Figure 13: Confidence distributions of two models on GSM8K before and after RLHF. Llama3-8B-SFT and Llama3-8B-PPO; Tulu-2-7B and Tulu-2-DPO-7B Figure 14: Confidence distributions of two models on Professional Knowledge before and after RLHF. 25 Llama3-8B-SFT and Llama3-8B-PPO; Tulu-2-7B and Tulu-2-DPO-7B Figure 15: Confidence distributions of two models on SciQ before and after RLHF. Llama3-8B-SFT and Llama3-8B-PPO; Tulu-2-7B and Tulu-2-DPO-7B Figure 16: Confidence distributions of two models on TruthfulQA before and after RLHF. 26 (a) RLHFlow/ArmoRM-Llama3-8B-v0.1 (Wang et al., 2024c) (b) CIR-AMS/BTRM Qwen2 7b 0613 (c) openbmb/Eurus-RM-7b (Yuan et al., 2024) (d) sfairXC/FsfairX-LLaMA3-RM-v0.1 (Dong et al., 2023a) (e) OpenRLHF/Llama-3-8b-rm-700k (Hu et al., 2024) Figure 17: Preference Distributions for various reward models across four modes (Part 1). Left follows the same setting in preliminary experiments. Right represents the setting where all confidencequery system prompts are removed, and only random confidence scores are appended. (a) Skywork/Skywork-Reward-Llama-3.1-8B (Liu & Zeng, 2024) (b) stabilityai/stablelm-2-12b-chat (Bellagente et al., 2024) (c) allenai/tulu-2-dpo-7b (Ivison et al., 2023) (d) openbmb/UltraRM-13b (Cui et al., 2023) (e) LxzGordon/URM-LLaMa-3.1-8B (Cui et al., 2023) Figure 18: Preference Distributions for various reward models across four modes (Part 2). Left follows the same setting in preliminary experiments. Right represents the setting where all confidencequery system prompts are removed, and only random confidence scores are appended. 28 (a) OpenRLHF/Llama-3-8b-rm-mixture (Hu et al., 2024) (b) NCSOFT/Llama-3-OffsetBias-RM-8B (Park et al., 2024) (c) OpenAssistant/oasst-rm-2.1-pythia-1.4b-epoch-2.5 (d) IDEA-CCNL/Ziya-LLaMA-7B-Reward (Cui et al., 2023) (e) internlm/internlm2-20b-reward Figure 19: Preference Distributions for various reward models across four modes (Part 3). Left follows the same setting in preliminary experiments. Right represents the setting where all confidencequery system prompts are removed, and only random confidence scores are appended. 29 E.3 CALIBRATED REWARD MODELS Section 4.2 demonstrates the preference distributions of our calibrated reward model compared to the pre-calibrated version for Llama3-8B on REJECTED WITH CONF. Here we present complete result and also extend to the Mistral-7B model. CHOSEN WITH CONF REJECTED WITH CONF Figure 20: Comparison of preference distributions between the calibrated reward model Llama-3-8b-crm and the pre-calibrated version Llama-3-8b-rm-mixture on two modes: CHOSEN WITH CONF and REJECTED WITH CONF. CHOSEN WITH CONF REJECTED WITH CONF Figure 21: Comparison of preference distributions between the calibrated reward model Mistral-7B-crm and the pre-calibrated version Mistral-7B-RM on two modes: CHOSEN WITH CONF and REJECTED WITH CONF. As shown in Figure 20 and 21, both calibrated models exhibit similar trend. When evaluated on chosen responses with high and low confidence scores, the calibrated reward models do not exhibit the same level of certainty as their pre-calibrated counterparts. Additionally, when evaluated on rejected responses with high and low confidence scores, both calibrated models show preference for the low-confidence responses, suggesting that our calibrated reward models are more capable at identifying overconfident statements. E.4 PARAMETER SENSITIVITY In Eq. 3, we introduce reward adjustment factor γ, defined as γ = ˆri (si 0.5). Here represents scaling coefficient set at 0.5 for our main results. We conduct hyperparameter study in this section to investigate the impact of w. As shown in Table 10, when is increased to 1.0, the model demonstrates lower ECE across all six datasets. However, it exhibits reduced AUC on SciQ and CommonsenseQA, possibly because the model becomes more conservative in its confidence yet lacks predictive power. Additionally, the model scores lower on the MT-Bench benchmark, indicating compromise in general capabilities. 30 MT-Bench GSM8K SciQ CommonsenseQA ECE AUC ACC ECE AUC ACC ECE AUC ACC 0.5 1.0 8.05 7.76 0.8638 0. 0.516 0.501 0.1031 0.1092 0.0282 0.0075 0.6513 0.5641 0.904 0.903 0.1286 0. 0.5621 0.5076 0.7756 0.7805 MT-Bench TruthfulQA Object Counting Professional Knowledge ECE AUC ACC ECE AUC ACC ECE AUC ACC 0.5 1.0 8.05 7.76 0.4426 0. 0.5303 0.5207 0.4431 0.4345 0.4839 0.4709 0.5178 0.5318 0.503 0.505 0.3949 0. 0.4902 0.5069 0.502 0.4883 Table 10: Performance of PPO-C with different coefficient. E.5 IMPACT OF CONFIDENCE-QUERY SYSTEM PROMPTS. For the main experiments, we select 25% of the single-turn prompts to prepend confidence-query system prompt. Here, we present our study on the effect of varying the percentage of single-turn prompts with this system prompt. As shown in Table 11, the impact on calibration does not show consistent trend; however, we observe decrease in MT-Bench scores as the percentage increases. Given our objective to maintain model capability while improving calibration, we opt for 25%. Percentage MT-Bench GSM8K SciQ CommonsenseQA ECE AUC ACC ECE AUC ACC ECE AUC ACC 0.25 0.5 1.0 8.05 7.88 7.74 0.57 0.0267 0.6115 0.8393 0.86 0.5185 0.1031 0.0389 0.5829 0.8608 0.5065 0.1243 0.0471 0.7165 0.119 0.898 0.896 0.898 0.1206 0.5568 0.7707 0.5399 0.7682 0.134 0.6341 0.7658 0. Percentage MT-Bench TruthfulQA Object Counting Professional Knowledge ECE AUC ACC ECE AUC ACC ECE AUC ACC 0.25 0.5 1.0 8.05 7.88 7.74 0.3991 0.5813 0.4789 0.5227 0.47 0.4453 0.5283 0.4357 0.5119 0.5413 0.3438 0.5737 0.4786 0.5087 0.5052 0.505 0.473 0.487 0.3848 0.4926 0.502 0.3988 0.5221 0.4935 0.502 0.3501 0.5184 Table 11: Performance of PPO-M on Prompt Dataset with various percentage of single-turn prompts prepending confidence-query system prompts. E.6 CAN PPO-M AND PPO-C BE COMBINED? As PPO-M and PPO-C function independently, we aim to explore whether these methods can be combined. Specifically, we apply the calibrated reward models from PPO-M while simultaneously using the calibrated reward calculation from PPO-C to generate scores. Table 12 demonstrates the performance of this combined method. We observe no improvement over individual methods, and sometimes even decline in performance. We hypothesize that this is due to calibrated reward model being trained specifically on responses with confidence scores, which are trained to produce an unbiased reward. Thus, removing these confidence scores to estimate rewards based on comparison with threshold might not be appropriate. E.7 EXTENSION TO DPO In Section 5.2, we present the results of extending PPO-M to DPO training on Mistral-7B, we include results for Llama3-8B here. As shown in Table 14 and 13, CDPO effectively reduces the ECE and increases AUC, following similar trend observed in Mistral-7B, and maintain performance on MT-Bench as well. Model Method MT-Bench Arena-Hard Llama3-8B SFT DPO DPO CDPO 6.44 (6.6) 7.67 (7.7) 3.1 (3.3) 15.9 (15.9) 7.52 7. 15.2 14.7 Table 13: Comparison over MT-Bench and ArenaHard for Llama3-8B. The numbers in parenthesis are sourced from Meng et al. (2024). 31 MT-Bench Arena-Hard DA 7.82 CoT 7. 12.0 12.0 MT-Bench Arena-Hard GSM8K SciQ CommonsenseQA ECE AUC ACC ECE AUC ACC ECE AUC ACC 0.8948 0.5168 0.0978 0.1026 0.5094 0.896 0.2134 0.5415 0.774 0.2441 0.5108 0.7544 0.1434 0. 0.847 0.2553 0.631 0.7027 TruthfulQA Object Counting Professional Knowledge ECE AUC ACC ECE AUC ACC ECE AUC ACC DA 7.82 CoT 7.82 12.0 12. 0.533 0.575 0.4235 0.469 0.5203 0.6263 0.4137 0. 0.5 0.5 0.531 0.5256 0.5158 0.4535 0.691 0. 0.5573 0.4324 Table 12: Performance of PPO-Combine on Llama3-8B across six datasets. Methods SFT DPO DPO CDPO SFT DPO DPO CDPO Methods SFT DPO DPO CDPO SFT DPO DPO CDPO DA CoT DA CoT GSM8K SciQ CommonsenseQA ECE AUC ACC ECE AUC ACC ECE AUC ACC 0.8783 0.904 0.8861 0. 0.6473 0.4159 0.452 0.3313 0.5292 0.5381 0.5203 0.5389 0.5508 0.5452 0.5456 0. 0.0773 0.0834 0.097 0.1024 0.326 0.577 0.539 0.5277 0.1681 0.1085 0.1103 0. 0.1699 0.113 0.0964 0.0386 0.5253 0.561 0.5626 0.6497 0.5816 0.6376 0.6614 0. 0.801 0.886 0.881 0.877 0.803 0.858 0.876 0.86 0.3913 0.3011 0.3004 0. 0.3293 0.2621 0.235 0.1269 0.5294 0.535 0.5409 0.5815 0.588 0.6295 0.5973 0. 0.5528 0.6871 0.683 0.6912 0.579 0.6593 0.6749 0.6798 TruthfulQA Object Counting Professional Knowledge ECE AUC ACC ECE AUC ACC ECE AUC ACC 0.592 0.6126 0.5647 0. 0.5259 0.5188 0.4931 0.3651 0.5388 0.5581 0.5886 0.6194 0.5698 0.5822 0.6111 0. 0.3256 0.3525 0.3856 0.3929 0.3782 0.4088 0.4113 0.4345 0.5964 0.5848 0.5999 0. 0.5388 0.3520 0.3783 0.3488 0.4938 0.4996 0.5008 0.5262 0.5126 0.5000 0.5018 0. 0.395 0.415 0.4 0.422 0.45 0.6480 0.621 0.567 0.5109 0.4764 0.467 0. 0.5091 0.4289 0.4312 0.3349 0.5189 0.4992 0.5153 0.5581 0.5457 0.5700 0.562 0. 0.4127 0.495 0.4939 0.4898 0.4068 0.4831 0.4694 0.4609 Table 14: Performance comparison of SFT, DPO, DPO, and CDPO across six datasets using Llama3-8B. SFT and DPO denote the reference and trained DPO models, respectively. DPO and CDPO initiate from the trained DPO checkpoint; DPO applies standard DPO on the calibration dataset, focusing on chosen and rejected pairs to assess the impact of training with additional data. However, we observe slight performance degradation on Arena-Hard using either DPO or CDPO. This may stem from inadequate hyperparameter tuning or inherent limitations in the structure of calibration dataset, which we leave for future research."
        }
    ],
    "affiliations": [
        "Carnegie Mellon University",
        "UC Berkeley",
        "Washington University in St. Louis"
    ]
}