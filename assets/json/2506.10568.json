{
    "paper_title": "DreamActor-H1: High-Fidelity Human-Product Demonstration Video Generation via Motion-designed Diffusion Transformers",
    "authors": [
        "Lizhen Wang",
        "Zhurong Xia",
        "Tianshu Hu",
        "Pengrui Wang",
        "Pengfei Wang",
        "Zerong Zheng",
        "Ming Zhou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In e-commerce and digital marketing, generating high-fidelity human-product demonstration videos is important for effective product presentation. However, most existing frameworks either fail to preserve the identities of both humans and products or lack an understanding of human-product spatial relationships, leading to unrealistic representations and unnatural interactions. To address these challenges, we propose a Diffusion Transformer (DiT)-based framework. Our method simultaneously preserves human identities and product-specific details, such as logos and textures, by injecting paired human-product reference information and utilizing an additional masked cross-attention mechanism. We employ a 3D body mesh template and product bounding boxes to provide precise motion guidance, enabling intuitive alignment of hand gestures with product placements. Additionally, structured text encoding is used to incorporate category-level semantics, enhancing 3D consistency during small rotational changes across frames. Trained on a hybrid dataset with extensive data augmentation strategies, our approach outperforms state-of-the-art techniques in maintaining the identity integrity of both humans and products and generating realistic demonstration motions. Project page: https://submit2025-dream.github.io/DreamActor-H1/."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 1 ] . [ 1 8 6 5 0 1 . 6 0 5 2 : r DreamActor-H1: High-Fidelity Human-Product Demonstration Video Generation via Motion-designed Diffusion Transformers Lizhen Wang1, Zhurong Xia1*, Tianshu Hu1, Pengrui Wang1, Pengfei Wei1, Zerong Zheng1, Ming Zhou1 1 ByteDance Intelligent Creation {wanglizhen.2024, zhaogang.666, tianshu.hu, wangpengrui.chj, pengfei.wei, zerong, zhouming.9527}@bytedance.com Figure 1. DreamActor-H1 can generate high-fidelity and photo-realistic human-product demonstration videos from human and product reference images."
        },
        {
            "title": "Abstract",
            "content": "In e-commerce and digital marketing, generating highfidelity human-product demonstration videos is important for effective product presentation. However, most existing frameworks either fail to preserve the identities of both humans and products or lack an understanding of humanproduct spatial relationships, leading to unrealistic representations and unnatural interactions. To address these challenges, we propose Diffusion Transformer (DiT)- based framework. Our method simultaneously preserves human identities and product-specific details, such as logos and textures, by injecting paired human-product reference information and utilizing an additional masked crossattention mechanism. We employ 3D body mesh template and product bounding boxes to provide precise motion guidance, enabling intuitive alignment of hand gestures with product placements. Additionally, structured text encoding is used to incorporate category-level semantics, enhancing 3D consistency during small rotational changes across frames. Trained on hybrid dataset with extensive data augmentation strategies, our approach outperforms stateof-the-art techniques in maintaining the identity integrity of both humans and products and generating realistic demonstration motions. Project page: https://submit2025dream.github.io/DreamActor-H1/. 1. Introduction * Corresponding author & project leader With the advancement of image-to-video generation technologies, it has become possible to produce human-product demonstration videos from just product image and person image. In the era of e-commerce, intuitively conveying product features through natural human motions would significantly enhance product marketing. However, despite the technological progress and growing demand, existing methods still struggle to simultaneously preserve both human identity and product details while ensuring correct producthuman interactions with natural motions, which limits their practical applications. Despite advancements in pose-driven animation and object manipulation, existing human-product interaction video generation methods fall short of practical needs. First, traditional hand-object interaction (HOI)-focused approaches like HOI-Swap [68] are limited to partial handobject scenarios, neglecting full-body dynamics. Recent HOI-oriented video generation methods such as AnchorCrafter [66] and Re-HOLD [10] incorporate object and motion information into diffusion models, facilitating object or human replacement in videos. However, they still suffer from low-quality outputs and reliance on predefined motions, which limits them to size-constrained product replacement. Second, pose-guided frameworks (e.g., MimicMotion [77]) do not focus on modeling human-product relationships. When generating videos from human-product images, they often produce severe object deformation as handheld items follow body movements. Finally, while text-to-image diffusion-based methods like InteractDiffusion [19] and PersonaHOI [25] generate HOI images from text prompts, they suffer from vague object details due to semantic prompt limitations. Multi-subject customization methods like Phantom [42] have shown promising potential in generating multi-subject customized videos, but they also rely on mapping subject images to anchored text prompts, introducing semantic ambiguity that fundamentally limits their capacity to model human-product spatial relationships. To tackle these challenges, we introduce DreamActorH1, novel Diffusion Transformer (DiT) [51]-based framework that generates high-quality human-product demonstration videos from paired human and product images. By integrating masked cross-attention to fuse appearance features from both inputs, our method preserves fine-grained details such as human identities, product logos, textures, and contours with high fidelity. For motion realism, we employ 3D body mesh template alongside product bounding boxes to guide hand gestures and object placements, ensuring natural alignment of human interactions with products of varying sizes and shapes. Additionally, structured text encoding injects category-level semantics, enabling the model to learn shared visual features across product classes and enhance the material visual quality and 3D consistency during small rotations. Trained on large-scale hybrid dataset with multi-class augmentation, DreamActor-H1 outperforms state-of-the-art methods in preserving human-product identity integrity and generating physically plausible demonstration motions, making it suitable for personalized e-commerce advertising and interactive media. To conclude, Our key contributions include: We propose DiT-based framework to generate highquality human-product demonstration videos from paired images, preserving good human and object identities via our appearance guidance module. We use 3D body templates and product bounding boxes as motion guidance with an automatic matching algorithm, enhancing user-friendliness and practicality. Our method employs structured text to enhance material visual quality and maintain 3D consistency during subtle product rotations. 2. Related Work 2.1. Human-Object Interaction The generation of humans interacting with objects has been fundamental research goal. While extensive methods [8, 13, 16, 17, 27, 37, 44, 53, 64, 65, 69, 72, 75, 76] have focused on predicting human motions during interactions with objects based on 3D representations, recent advances [3, 10, 19, 20, 25, 41, 66, 68, 70, 74, 78] have begun to address the challenge of human-object interactions (HOI) in image and video generation tasks. HOGAN [20] first explores hand-object interaction image generation via splitand-combine approach considering inter-occlusion, while HOI-Swap [68] proposes two-stage hand-centric video editing framework for object swapping with HOI awareness but limited to single-hand grasping. To generate fullbody human-object interactions, InteractDiffusion [19] provides interaction controllability while PersonaHOI [25] enables facial personalization, both building upon pre-trained text-to-image diffusion models. VirtualModel [3] generates HOI images by leveraging input objects and human poses. To facilitate human-object interactions in video generation, HOIGen-1M [41] introduces large-scale dataset for HOI generation with over one million high-quality videos. AnchorCrafter [66] integrates HOI into pose-guided human video generation models, while ignoring the compatibility of pose sequences with objects of various sizes and shapes. Re-HOLD [10] uses specialized hand-object layout representation to disentangle hand modeling and object adaptation for motion sequences, but its focus on video object replacement limits generative freedom. 2.2. Pose-Guided Video Generation Given human image and pose sequence, pose-guided human video generation [2, 11, 15, 21, 22, 30, 33, 36, 45 47, 52, 58, 61, 62, 67, 77, 81] aims to synthesize human animations following the pose sequence. To improve pose guidance accuracy and flexibility, researchers have explored various pose representations such as 2D skeleton map [2, 46, 61, 62], 3D human parametric model [30, 81], and dense correspondences [33, 67]. In addition, AnimateAnyone [21] proposes ReferenceNet to effectively preserve intricate visual consistency. Building upon video diffusion model [1], MimicMotion [77] designs confidence-aware mechanism to address model instability from keypoint uncertainty while StableAnimator [58] develops specialized modules for identity consistency. DisPose [36] and ControlNeXT [52] introduce plug-and-play modules to seamlessly integrate into existing models, maintaining training efficiency. TALK-Act [15] enhances textural awareness with explicit motion guidance to tackle the difficulty of synthesizing stable hand movements. Building upon diffusion transformer [51], HumanDiT [11] facilitates learning for long-form video generation. DreamActor-M1 [45] proposes the carefully designed hybrid guidance to achieve fine-grained holistic controllability, multi-scale adaptability, and long-term temporal coherence. Notably absent from prior work is explicit modeling of human-object interactions, creating substantial gap for real-world physical scenarios. Apart from human pose, MIMO [47] and Animate Anyone 2 [22] take into account environmental representations as additional input conditions, aiming to effectively characterize the relationship between character and environment. However, they introduce visual artifacts when dealing with complex hand-object interactions that occupy relatively small pixel region, leading to suboptimal performance. 2.3. Customized Video Generation Customized video generation [5, 7, 18, 23, 26, 28, 31, 32, 34, 38, 42, 63, 71, 79] aims to produce videos featuring specific subjects and maintaining identity consistency across different contexts. Some previous works [18, 71, 79] focus on maintaining facial identity. For instance, IDAnimator [18] introduces face adapter to encode the IDrelevant embeddings from learnable facial latent queries and ConsisID [71] designs tuning-free pipeline equipped with frequency-aware identity-control scheme. Besides, DreamVideo [63] learns to customize both subject identity and motion upon finetuning on few images and videos. VideoBooth [31] proposes feed-forward framework by embedding image prompts in coarse-to-fine manner. Through decoupling the subject-specific learning from temporal dynamics, Kim et al. [34] propose recipe to train the customized video generation model without annotated videos. Recent advances such as VideoAlchemist [5], MovieWeaver [38], and Phantom [42] have demonstrated potential in multi-subject video generation. However, these methods mainly rely on mapping subject images to text prompts, introducing semantic ambiguity that limits intersubject relationship modeling. To tackle this, CINEMA [7] and HunyuanCustom [23] use multimodal large language models (MLLMs) to enhance interactive integration of subject images and text, improving cross-modal alignment. However, no prior methods have been specifically designed for human-product demonstrations, resulting in significant challenges in preserving fine-grained product details such as text and logos. 3. Method 3.1. Overview DreamActor-H1 can generate high-fidelity human-product demonstration videos from human reference image and product reference image. As depicted in Fig. 2, our framework is built upon DiT architecture, specifically leveraging Seaweed-7B [56], foundational model for video generation with around 7 billion (7B) parameters. In the dataset preparation phase, we use Vision-Language Model (VLM) to describe the product and human images. Subsequently, pose estimation and bounding box detection are applied to the training product-human demonstration video. During the training stage, we combine the human pose and product bounding box with the input video noise to serve as motion guidance, as described in Sec. 3.3. Additionally, we encode the input human and product images using variational autoencoder (VAE) [35] to serve as appearance guidance. The descriptions of the human and product are utilized as supplementary information, enhancing the material visual quality and 3D consistency during small rotational changes across frames, as explained in Sec. 3.3. Regarding the DiT model, we implement stacks of full attention, reference attention, and object attention, as described in Sec. 3.2. Notably, object attention incorporates the product latent as an extra input. As shown in Fig. 3, during the inference stage, we implement automatic pose template selection based on human and product information. Overall, our approach can overcome the challenges of identity preservation, motion realism, and spatial relationship modeling, and produce high-quality human-product demonstration videos given human and product image as inputs. 3.2. Human-Product Appearance Guidance In video generation, we need to first inject product and human appearance information while maintaining consistency. Unlike image-to-video tasks and pose-based human video driving tasks, our approach involves integrating product image information into human reference image. And in practical applications, maintaining the consistency of the product appearance is of great importance. In the initial injection stage, we adopt reference attention for appearance injection, following [39, 45]. This approach avoids the need for standalone ReferenceNet [21] by directly integrating reference features into the DiT with Figure 2. The pipeline of DreamActor-H1 leverages DiT architecture, starting with dataset preparation where VLM describes product and human images, followed by pose estimation and bounding box detection on training videos. During training, human poses and product boxes integrate with video noise for motion guidance, while VAE encodes input images for appearance guidance; human-product descriptions are fed into the model via text encoder. The model incorporates full attention, reference attention, and object attention (with product latents as inputs), with the reference and object attention mechanisms detailed at the top of the figure. minimal additional parameters (mostly shared with the base model). By concatenating reference features with selfattention mechanisms, the network extracts appearance information from input reference images. However, we found that as the network deepens, some detailed information of the product (e.g. text labels, micro-geometric structures) will be lost during the process of gradual self-update in MMDiT [9]. Therefore, we additionally use masked object attention, which injects the object latents obtained by encoding the product image through VAE. By concatenating and combining with self-attention, we inject the product information in the form of residual, which can improve the consistency of product details in the generated video. in the the attention, Specifically, ref, and input full Fvid R(thw)c, Fref R(1hw)c, and Ftxt Rlc are concatenated along the first dimension (which includes the flattened temporal dimension t, spatial dimensions w, or the compressed token count of text) and then processed through full self-attention to produce outputs vid, txt of the same dimensions. In the reference attention, the inputs are reshaped as Fvid Rt(hw)c, Fref R1(hw)c, followed by separate attention operations along the second dimension (spatial dimensions h, per frame) to generate corresponding outputs. During the attention computation for vid, queries (Q) are derived from Fvid, while keys and values (KV ) are sourced from Fvid, Fref, and Ftxt. For ref, comes from Fref and KV from Fref and Ftxt. For and Ftxt R1lc, txt, is from Ftxt and KV from Fvid and Ftxt. This design ensures Fvid integrates information from both Fref and Ftxt, while Fref focuses solely on text correlations. the input Fobj R1(hw)c, In the object attention, fixed VAE-encoded feature that does not self-update, is concatenated with Fvid Rt(hw)c along the second dimension and processed via self-attention. To selectively refine only product-containing regions in Fvid, the resulting features are scaled by product region mask and added back to Fvid as residual update. As demonstrated in Sec. 4.3, this attention design can significantly enhance the consistency of product appearance preservation. 3.3. Human-Product Motion Guidance On one hand, substantial research has been conducted on human-object interaction (HOI), yet most efforts focus on 3D domains, typically requiring 3D models of objects. This poses significant limitations for practical applications, as the diversity of products makes it challenging to obtain realistic 3D models through image-to-3D techniques. On the other hand, common challenges include interactions that fail to align with text prompt instructions and notable inconsistencies in object sizes, which decrease the realism of visual outputs. To address these challenges, we implement motion guidance method that combines 3D human templates with object bounding boxes to direct video generation. This strategy allows for more precise control over object positioning and scale adjustment. dimension can be adjusted according to the aspect ratio of the input product. The motion templates cover products of various sizes from 1 to 40 centimeters and some common demonstration actions. In practical applications, we first obtain the shape, translation, rotation information of the target human image and the product size information (predicted by VLM or manually assigned), then match the most suitable hand motion template for holding the object based on the product size and body orientation, and also match suitable template for facial expressions and minor movements of the head and trunk according to the body orientation. If the object-holding hand only involves single hand, we will match demonstration action template for the other hand. We retain the humans position, shape, rotation and other information for retargeting according to the motion template, and the product bounding box will be readjusted to the corresponding position and size following the retargeted If it involves two-hand actions, the object-holding hand. product bounding box size needs to be readjusted according to the distance between the two hands. Finally, we also adjust the final product bounding box according to the freely expandable direction of the product marked in the motion template and the aspect ratio of the input product. After rendering, the motion guidance video for inference can be obtained. 3.4. Product Semantics Guidance After the above operations, we have successfully injected product image information into human images to generate demonstration videos with good product preservation. However, there are still issues in representing minor product rotations and material properties (such as reflections from transparent materials, plastics, or metals), and text on products may occasionally fail to be preserved. We speculate that this arises because the network lacks common-sense knowledge about products of the same category or material, making it difficult to proactively determine product categories and materials from single input. Therefore, we used VLM to generate text captions for product categories, sizes, colors, materials, and text, as well as annotations for human image attributes like human information, environment, and lighting. Here, we utilize Seed1.5-VL [57] as our annotation tool. Specifically, to highlight key attributes, we represent input text in dictionary-like format that contains only essential keyword information, as shown in Fig. 2. During training and inference, these texts are converted into text embeddings by the text encoder in Seaweed model to assist video generation. This allows the network to better learn common-sense information corresponding to text from same-category or same-material products in the dataset, thereby improving representation of material properties. Notably, this input also enhances stability during small product rotations, likely Figure 3. During inference, our framework retrieves optimal motion templates from pre-defined pools and adapts object box scaling via joint analysis of reference human/product images, enabling pose-coherent animations. In our pose estimation, we leverage the 3D body template SMPL-X [49] and face model FaceVerse [60] to capture human movements, with 3D parameter fitting guided by frameworks including 4D-Humans [14], HaMeR [50], and FaceVerse [60]. Note that we have manually colored SMPL-X mesh to enhance the visual differences between different regions and assist the network learning. For product localization, we employ two-stage approach: initial bounding box detection using GroundingDINO [43] followed by refined segmentation via SAM2 [55]. The final object bounding box is derived as the minimum rotated rectangle from the segmentation masks, enabling more accurate product size and representation of z-axis rotations compared to boxes from GroundingDINO. Our pose encoder, inspired by [21], uses lightweight CNN architecture. During the training stage, our motion guidance is concatenated with input noise along the channel dimension, providing explicit spatial cues for video generation. The entire pose encoding module is trained end-to-end to optimize motion consistency. During the inference stage, as shown in Fig. 3, we design an automated logic for selecting and adjusting motion templates to generate the final video from given single human and product image. To this end, we have constructed motion template pool from the dataset, which includes movements such as two-hand and head motions, small-scale body movements during speaking, and object-related actions like single-hand grasping, lifting, picking up, raising, two-hand holding, and picking up products from tables, along with the corresponding changes in product bounding boxes, expandable directions of the bounding boxes, and fixed dimensions (e.g., fixing the width when the expansion direction is up or fixing the height for left and right), where another free leveraging geometric patterns from same-category products in the training set. 3.5. Dataset and Training Details To enhance the networks capabilities in human-product identity preservation, generating dynamic motion changes, and maintaining 3D consistency of products, we adopt hybrid dataset for model training. Our core self-collected data focuses on training motion and appearance guidance modules, while supplementary datasets address tasks like product rotation, background naturalness, and identity preservation. Specifically, we collected 15,000 pairs of product-human images and corresponding demonstration videos (approximately 80 hours), featuring over 5,000 structurally simple everyday products with actions covering standing/sitting and tabletop pickup poses. This primary dataset contributes 80% of the training samples. Additionally, we captured rotation videos for each product to assist 3D structure prediction from frontal images. To enrich human identity diversity and dynamic backgrounds (e.g., ocean waves), we collected 50 hours of e-commerce livestream data and incorporated 100 hours of generalized categories from the Seaweed dataset. These supplementary datasets, lacking paired images, contribute 20% of samples as simple image-to-video tasks. For broader applicability across different human formats (full-body, half-body, selfies), we applied random crop augmentation and color augmentation to adjust lighting, contrast and saturation. We further utilized IC-Light [73] to simulate diverse lighting conditions, ensuring robustness to varying product photography environments. During training, we employ Flow Matching [40] as the training objective with region-specific weighting for faces, hands and products. 4. Experiments 4.1. Implement Details Our training weights are initialized from the pretrained image-to-video Seaweed-7B [56] model, and the training process is conducted using 24 NVIDIA H20 GPUs over two weeks, totaling 100,000 steps. The model operates at resolution of 720 1280, with each inference clip containing 65 frames. To ensure full-video consistency, we adopt sequential generation strategy: the last latent code from the current video segment serves as the initial latent for the subsequent segment. For the appearance guidance module, we set the classifier-free guidance (CFG) parameter to 2.5. Additionally, We use AMO Sampler [24] for the CFG, which can improve the preservation of fine-grained product textures and text details. 4.2. Comparison To mitigate legal risks, all human and product images in this paper, except for three products from AnchorCrafter (we edited their logos), are generated by Seedream 3.0 [12]. To the best of our knowledge, no existing method fully aligns with our input-output pipeline. Thus, we compare our approach with AnchorCrafter [66], which requires multi-view product images and depth maps but is functionally closest, alongside subject-to-video frameworks Phantom [42] and VACE [32], which better match our input-output design. For evaluating our improvements in dynamic product preservation, we also compare with UniAnimate-DiT [62]. As UniAnimate-DiT supports only single human image input with pose sequence, we only use its self-driving function with our generated first frame and pose sequence. Since it cannot natively generate first frames or perform 3D body retargeting, we denote this variant as UniAnimate-DiT. Due to AnchorCrafter only releases trial demo, we can only produce 3 videos using the three objects provided in their demo, and we generated 15 videos for other methods. In our comparisons, Phantom is trained based on WAN2.1-1.3B [59], while both VACE and UniAnimate are based on WAN2.1-14B. According to the report of Seaweed [56], WAN2.1-14B is comparable image-to-video generation model with Seaweed-7B adopted in this work. Notably, Phantom and VACE only support landscape video generation, which fails to maintain resolution consistency with other methods. To ensure fairness, we carefully designed our evaluation metrics to minimize the impact of background and human body variations caused by resolution differences among compared methods in subsequent qualitative and quantitative evaluations. As illustrated in Fig. 4, our method surpasses others in preserving human and product identities. AnchorCrafter [66] demonstrates effective product preservation but requires additional multi-view product images and depth maps. Subject-to-video approaches like Phantom [42] and VACE [32] occasionally fail to respond to prompts (we used the prompt This person confidently holds/graps/lifts this product name in hand, ...; refer to the supplementary video for more details), and products often appear oversized. By providing UniAnimate with the first frame and retargeted pose, it yields good results but fails to preserve product details, validating the effectiveness of our product preservation module. As shown in Tab. 1, we employ CLIPI [54] and DINO-I [48] to measure product preservation, with our method achieving the best results. Specifically, we detect masks from videos, sample 10 frames per video, crop products using detected masks, and compute cosine similarity with product images via CLIP and DINO. Notably, Phantom and VACE show promising results when ignoring product size. Since full-body representations vary across methods, we use ArcFace [6] to calculate cosine similarFigure 4. Comparisons with AnchorCrafter [66], Phantom [42], VACE [32] and UniAnimate-DiT [62]. Note that we only generate 3 videos for AnchorCrafter, and UniAnimate-DiT uses our first frames and pose sequences as inputs. Methods AnchorCrafter Phantom-WAN2.1-1.3B VACE-WAN2.1-14B UniAnimate-DiT Ours baseline Ours w/o text Ours CLIP-I DINO-I FaceSim-Arc Motion Smoothness Aesthetic Quality 0.855 0.875 0.853 0.849 0.826 0.867 0.902 0.621 0.795 0.749 0.742 0.731 0.782 0.818 0.594 0.640 0.639 0.741 0.759 0.779 0.769 0.994 0.992 0.993 0.995 0.993 0.994 0.994 0.476 0.493 0.485 0.510 0.518 0.513 0.512 Imaging Quality 0.706 0.730 0.737 0.738 0.734 0.736 0. Table 1. Quantitative comparisons with AnchorCrafter [66], Phantom [42], VACE [32], UniAnimate-DiT [62], and our ablation studies (Ours baseline and Ours w/o text). Note that we us our first frames and pose sequences as inputs for UniAnimate-DiT. ity (for images with face), where our method also outperforms others, confirming superior human identity preservation. We use VBench [29] to evaluate motion smoothness, aesthetic quality, and imaging quality. As only the object-holding hand and product move slightly in the videos, all methods exhibit high motion smoothness. Additionally, methods using DiT as foundation model all perform well in aesthetic quality and imaging quality, indicating that DiT-based video generation models can generally ensure the video quality. We also conducted user study with 50 participants (AnchorCrafter included only 3 videos). The system randomly presented pairs of videos generated from the same input, and participants evaluated them across three dimensions: Method AnchorCrafter Phantom-WAN2.1-1.3B VACE-WAN2.1-14B UniAnimate-DiT Ours Human 12.5 20.8 54.2 92.9 100. Product Overall 77.1 35.7 35.1 62.5 97.0 58.3 32.1 37.5 64.9 97.0 Table 2. Good or same rate of various methods in terms of identity preservation for human and product, as well as overall video quality. It should be noted that our first frame and our pose were used as inputs for UniAnimate-DiT. Figure 5. Ablation studies with Ours baseline (w/o object attention and text input) and Ours w/o text. product identity preservation, human identity preservation, and overall video quality. For each dimension, they chose good, same, or bad. Assessments of product and human identity focused on alignment with reference images. Results were reported as win rates based on good or same ratings. As shown in Tab. 2, our model outperformed all baselines in all evaluation dimensions. 4.3. Ablation Study To validate the contributions of our core modules, we conducted ablation experiments comparing three configurations: baseline trained without object attention or text input (denoted as Ours baseline), variant trained with object attention but without text input (Ours w/o text), and our full model. The results demonstrate that each component plays critical role in enhancing product preservation and video consistency. As shown in Fig. 5, removing both object attention and text guidance (baseline) degrades product detail preservation. Introducing object attention alone enhances visual fidelity by anchoring product features, but the model still fails to capture material properties (e.g., transparent surfaces or dial textures), text consistency, and geometric coherencelikely from missing semantic context. In contrast, our full model integrating object attention and text-derived embeddings outperforms both Ours baseline and Ours w/o text. Text guidance lets the network leverage productcategory common-sense to improve text preservation and 3D rotation stability. Tab. 4 also confirms object attention preserves visual features while text guidance ensures semantic consistency, enabling robust human-product video generation. Please refer to Fig. 6, Fig. 7 and our supplementary video for more results. 5. Discussion In this work, we present DreamActor-H1, Diffusion Transformer-based framework that addresses the challenges of generating high-fidelity human-product demonstration videos by integrating masked cross-attention, 3D motion guidance, and semantic-aware text encoding. Our method effectively preserves fine-grained human and product identities while ensuring natural spatial alignment between human gestures and product placements. Extensive experiments demonstrate that DreamActor-H1 outperforms stateof-the-art approaches in maintaining identity integrity and generating physically plausible interactions, making it solution for e-commerce and digital marketing scenarios. Limitation. DreamActor-H1 has several limitations: it currently handles only relatively small-sized products and may produce unnatural interactions due to relying on pre-defined motions unrelated to specific products (e.g. pose template for picking up products on table requires that the human reference image includes table in front of the person, with arms positioned properly relative to the table); the visual-language model (VLM) may inaccurately judge product sizes, especially for non-standard shapes; and it struggles with highly complex product structures, where fine-grained geometry and texture modeling is challenging. Due to the temporal compression of the VAE, text and textures on products may flicker during fast movements. These challenges still need to be addressed in future research. Ethics considerations. Human-product image animation has potential social risks, such as misuse for fake product demos or counterfeit promotions. Detection tools [4, 80] can identify manipulated media. Mitigating risks requires clear ethical guidelines and AI-generated content labeling. Acknowledgement. We extend our sincere gratitude to Lu Jiang, Yuxuan Luo, Zhengkun Rong, Jiaqi Yang, Tongchun Zuo, Jianwen Jiang, and Youjiang Xu for their invaluable contributions and supports to this research work. Figure 6. Our video results generated from human images and product images. Figure 7. Our results generated from more human images and product images."
        },
        {
            "title": "References",
            "content": "[1] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video arXiv preprint diffusion models to large datasets. arXiv:2311.15127, 2023. 3 [2] Di Chang, Yichun Shi, Quankai Gao, Jessica Fu, Hongyi Xu, Guoxian Song, Qing Yan, Yizhe Zhu, Xiao Yang, and Mohammad Soleymani. Magicpose: Realistic human poses and facial expressions retargeting with identity-aware diffusion. arXiv preprint arXiv:2311.12052, 2023. 2, 3 [3] Binghui Chen, Chongyang Zhong, Wangmeng Xiang, Yifeng Geng, and Xuansong Xie. Virtualmodel: Generating object-id-retentive human-object interaction image by diffusion model for e-commerce marketing. arXiv preprint arXiv:2405.09985, 2024. 2 [4] Haoxing Chen, Yan Hong, Zizheng Huang, Zhuoer Xu, Zhangxuan Gu, Yaohui Li, Jun Lan, Huijia Zhu, Jianfu Zhang, Weiqiang Wang, et al. Demamba: Aigenerated video detection on million-scale genvideo benchmark. arXiv preprint arXiv:2405.19707, 2024. 8 [5] Tsai-Shien Chen, Aliaksandr Siarohin, Willi Menapace, Yuwei Fang, Kwot Sin Lee, Ivan Skorokhodov, Kfir Aberman, Jun-Yan Zhu, Ming-Hsuan Yang, and Sergey Tulyakov. Multi-subject open-set perarXiv preprint sonalization in video generation. arXiv:2501.06187, 2025. 3 [6] Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou. Arcface: Additive angular margin loss In Proceedings of the for deep face recognition. IEEE/CVF conference on computer vision and pattern recognition, pages 46904699, 2019. 6 [7] Yufan Deng, Xun Guo, Yizhi Wang, Jacob Zhiyuan Fang, Angtian Wang, Shenghai Yuan, Yiding Yang, Bo Liu, Haibin Huang, and Chongyang Ma. Cinema: Coherent multi-subject video generation via mllmarXiv preprint arXiv:2503.10391, based guidance. 2025. [8] Christian Diller and Angela Dai. Cg-hoi: Contactguided 3d human-object interaction generation. In Proc. Computer Vision and Pattern Recognition (CVPR), IEEE, 2024. 2 [9] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution In Forty-first international conferimage synthesis. ence on machine learning, 2024. 4 [10] Yingying Fan, Quanwei Yang, Kaisiyuan Wang, Hang Zhou, Yingying Li, Haocheng Feng, Errui Ding, Yu Wu, and Jingdong Wang. Re-hold: Video hand object interaction reenactment via adaptive layout-instructed diffusion model. CVPR, 2025. 2 [11] Qijun Gan, Yi Ren, Chen Zhang, Zhenhui Ye, Pan Xie, Xiang Yin, Zehuan Yuan, Bingyue Peng, and Jianke Zhu. Humandit: Pose-guided diffusion transformer for long-form human motion video generation. arXiv preprint arXiv:2502.04847, 2025. 2, 3 [12] Yu Gao, Lixue Gong, Qiushan Guo, Xiaoxia Hou, Zhichao Lai, Fanshi Li, Liang Li, Xiaochen Lian, Chao Liao, Liyang Liu, et al. Seedream 3.0 technical report. arXiv preprint arXiv:2504.11346, 2025. 6 [13] Anindita Ghosh, Rishabh Dabral, Vladislav Golyanik, Christian Theobalt, and Philipp Slusallek. Imos: Intent-driven full-body motion synthesis for humanobject interactions. In Eurographics, 2023. [14] Shubham Goel, Georgios Pavlakos, Jathushan Rajasegaran, Angjoo Kanazawa, and Jitendra Malik. Humans in 4d: Reconstructing and tracking humans with In Proceedings of the IEEE/CVF Intransformers. ternational Conference on Computer Vision, pages 1478314794, 2023. 5 [15] Jiazhi Guan, Quanwei Yang, Kaisiyuan Wang, Hang Zhou, Shengyi He, Zhiliang Xu, Haocheng Feng, Errui Ding, Jingdong Wang, Hongtao Xie, et al. Talk-act: Enhance textural-awareness for 2d speaking avatar In SIGGRAPH reenactment with diffusion model. Asia 2024 Conference Papers, pages 111, 2024. 2, 3 [16] Mohamed Hassan, Partha Ghosh, Joachim Tesch, Dimitrios Tzionas, and Michael J. Black. Populating 3D scenes by learning human-scene interaction. In Proceedings IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR), 2021. 2 [17] Mohamed Hassan, Yunrong Guo, Tingwu Wang, Michael Black, Sanja Fidler, and Xue Bin Peng. Synthesizing physical character-scene interactions. In ACM SIGGRAPH 2023 Conference Proceedings, pages 19, 2023. 2 [18] Xuanhua He, Quande Liu, Shengju Qian, Xin Wang, Tao Hu, Ke Cao, Keyu Yan, and Jie Zhang. Idanimator: Zero-shot identity-preserving human video generation. arXiv preprint arXiv:2404.15275, 2024. 3 [19] Jiun Tian Hoe, Xudong Jiang, Chee Seng Chan, YapPeng Tan, and Weipeng Hu. Interactdiffusion: InterIn action control in text-to-image diffusion models. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 61806189, 2024. 2 [20] Hezhen Hu, Weilun Wang, Wengang Zhou, and Houqiang Li. Hand-object interaction image generation. In NeurIPS, 2022. [21] Li Hu. Animate anyone: Consistent and controllable image-to-video synthesis for character animation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8153 8163, 2024. 2, 3, 5 [22] Li Hu, Guangyuan Wang, Zhen Shen, Xin Gao, Dechao Meng, Lian Zhuo, Peng Zhang, Bang Zhang, and Liefeng Bo. Animate anyone 2: High-fidelity character image animation with environment affordance. arXiv preprint arXiv:2502.06145, 2025. 2, 3 [23] Teng Hu, Zhentao Yu, Zhengguang Zhou, Sen Liang, Yuan Zhou, Qin Lin, and Qinglin Lu. Hunyuancustom: multimodal-driven architecture for customized video generation. arXiv preprint arXiv:2505.04512, 2025. 3 [24] Xixi Hu, Keyang Xu, Bo Liu, Qiang Liu, and Amo sampler: Enhancing text arXiv preprint Hongliang Fei. rendering with overshooting. arXiv:2411.19415, 2024. 6 [25] Xinting Hu, Haoran Wang, Jan Eric Lenssen, and Bernt Schiele. Personahoi: Effortlessly improving personalized face with human-object interaction generation. arXiv preprint arXiv:2501.05823, 2025. 2 [26] Chi-Pin Huang, Yen-Siang Wu, Hung-Kai Chung, Kai-Po Chang, Fu-En Yang, and Yu-Chiang Frank Wang. Videomage: Multi-subject and motion customization of text-to-video diffusion models. arXiv preprint arXiv:2503.21781, 2025. [27] Mingzhen Huang, Fu-Jen Chu, Bugra Tekin, Kevin Liang, Haoyu Ma, Weiyao Wang, Xingyu Chen, Pierre Gleize, Hongfei Xue, Siwei Lyu, Kris Kitani, Matt Feiszli, and Hao Tang. Hoigpt: Learning long sequence hand-object interaction with language models. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Nashville, USA, 2025. 2 [28] Yuzhou Huang, Ziyang Yuan, Quande Liu, Qiulin Wang, Xintao Wang, Ruimao Zhang, Pengfei Wan, Di Zhang, and Kun Gai. Conceptmaster: Multiconcept video customization on diffusion transformer arXiv preprint models without test-time tuning. arXiv:2501.04698, 2025. 3 [29] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2180721818, 2024. 7 [30] Ziyao Huang, Fan Tang, Yong Zhang, Xiaodong Cun, Juan Cao, Jintao Li, and Tong-Yee Lee. Make-youranchor: diffusion-based 2d avatar generation frameIn Proceedings of the IEEE/CVF Conference work. on Computer Vision and Pattern Recognition, pages 69977006, 2024. 2, 3 [31] Yuming Jiang, Tianxing Wu, Shuai Yang, Chenyang Si, Dahua Lin, Yu Qiao, Chen Change Loy, and Ziwei Liu. Videobooth: Diffusion-based video generation with image prompts. 2023. 3 [32] Zeyinzi Jiang, Zhen Han, Chaojie Mao, Jingfeng Vace: All-inarXiv preprint Zhang, Yulin Pan, and Yu Liu. one video creation and editing. arXiv:2503.07598, 2025. 3, 6, 7 [33] Johanna Karras, Aleksander Holynski, Ting-Chun Wang, and Ira Kemelmacher-Shlizerman. Dreampose: Fashion image-to-video synthesis via stable diffusion. In 2023 IEEE/CVF International Conference on Computer Vision (ICCV), pages 2262322633. IEEE, 2023. 2, 3 [34] Daneul Kim, Jingxu Zhang, Wonjoon Jin, Sunghyun Cho, Qi Dai, Jaesik Park, and Chong Luo. Subjectdriven video generation via disentangled identity and motion. arXiv preprint arXiv:2504.17816, 2025. 3 [35] Diederik Kingma, Max Welling, et al. Autoencoding variational bayes, 2013. 3 [36] Hongxiang Li, Yaowei Li, Yuhang Yang, Junjie Cao, Zhihong Zhu, Xuxin Cheng, and Long Chen. Dispose: Disentangling pose guidance for controllable human image animation. arXiv preprint arXiv:2412.09349, 2024. 2, 3 [37] Jiaman Li, Jiajun Wu, and Karen Liu. Object motion guided human motion synthesis. ACM Trans. Graph., 42(6), 2023. [38] Feng Liang, Haoyu Ma, Zecheng He, Tingbo Hou, Ji Hou, Kunpeng Li, Xiaoliang Dai, Felix Juefei-Xu, Samaneh Azadi, Animesh Sinha, et al. Movie weaver: Tuning-free multi-concept video personalization with anchored prompts. arXiv preprint arXiv:2502.07802, 2025. 3 [39] Gaojie Lin, Jianwen Jiang, Jiaqi Yang, Zerong Zheng, and Chao Liang. Omnihuman-1: Rethinking the scaling-up of one-stage conditioned human animation models. arXiv preprint arXiv:2502.01061, 2025. 3 [40] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Flow matcharXiv preprint Maximilian Nickel, and Matt Le. ing for generative modeling. arXiv:2210.02747, 2022. 6 [41] Kun Liu, Qi Liu, Xinchen Liu, Jie Li, Yongdong Zhang, Jiebo Luo, Xiaodong He, and Wu Liu. Hoigen1m: large-scale dataset for human-object interaction video generation. In CVPR, 2025. 2 [42] Lijie Liu, Tianxaing Ma, Bingchuan Li, Zhuowei Chen, Jiawei Liu, Qian He, and Xinglong Wu. Phantom: Subject-consistent video generation via crossmodal alignment. arXiv preprint arXiv:2502.11079, 2025. 2, 3, 6, 7 [43] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Qing Jiang, Chunyuan Li, Jianwei Yang, Hang Su, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. In European Conference on Computer Vision, pages 3855. Springer, 2024. [44] Yumeng Liu, Xiaoxiao Long, Zemin Yang, Yuan Liu, Marc Habermann, Christian Theobalt, Yuexin Ma, and Wenping Wang. Easyhoi: Unleashing the power of large models for reconstructing hand-object interactions in the wild. arXiv preprint arXiv:2411.14280, 2024. 2 [45] Yuxuan Luo, Zhengkun Rong, Lizhen Wang, Longhao Zhang, Tianshu Hu, and Yongming Zhu. Dreamactorm1: Holistic, expressive and robust human image arXiv preprint animation with hybrid guidance. arXiv:2504.01724, 2025. 2, 3 [46] Yue Ma, Yingqing He, Xiaodong Cun, Xintao Wang, Siran Chen, Xiu Li, and Qifeng Chen. Follow your pose: Pose-guided text-to-video generation using In Proceedings of the AAAI Conpose-free videos. ference on Artificial Intelligence, pages 41174125, 2024. 3 [47] Yifang Men, Yuan Yao, Miaomiao Cui, and Liefeng Bo. Mimo: Controllable character video synthesis with spatial decomposed modeling. arXiv preprint arXiv:2409.16160, 2024. 2, 3 [48] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Russell Howes, Po-Yao Huang, Hu Xu, Vasu Sharma, Shang-Wen Li, Wojciech Galuba, Mike Rabbat, Mido Assran, Nicolas Ballas, Gabriel Synnaeve, Ishan Misra, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. Dinov2: Learning robust visual features without supervision, 2023. 6 [49] Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani, Timo Bolkart, Ahmed A. A. Osman, Dimitrios Tzionas, and Michael J. Black. Expressive body capture: 3d hands, face, and body from single image. In Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2019. [50] Georgios Pavlakos, Dandan Shan, Ilija Radosavovic, Angjoo Kanazawa, David Fouhey, and Jitendra Malik. Reconstructing hands in 3D with transformers. In CVPR, 2024. 5 [51] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 41954205, 2023. 2, 3 [52] Bohao Peng, Jian Wang, Yuechen Zhang, Wenbo Li, Ming-Chang Yang, and Jiaya Jia. Controlnext: Powerful and efficient control for image and video generation. arXiv preprint arXiv:2408.06070, 2024. 2, 3 [53] Xiaogang Peng, Yiming Xie, Zizhao Wu, Varun Jampani, Deqing Sun, and Huaizu Jiang. Hoi-diff: Textdriven synthesis of 3d human-object interactions using diffusion models. arXiv preprint arXiv:2312.06553, 2023. 2 [54] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR, 2021. 6 [55] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Radle, Chloe Rolland, Laura Gustafson, et al. Sam 2: Segment anything in images and videos. arXiv preprint arXiv:2408.00714, 2024. [56] Team Seawead, Ceyuan Yang, Zhijie Lin, Yang Zhao, Shanchuan Lin, Zhibei Ma, Haoyuan Guo, Hao Chen, Lu Qi, Sen Wang, et al. Seaweed-7b: Cost-effective training of video generation foundation model. arXiv preprint arXiv:2504.08685, 2025. 3, 6 [57] ByteDance Seed Team. Seed1.5-vl technical report. arXiv preprint arXiv:2505.07062, 2025. 5 [58] Shuyuan Tu, Zhen Xing, Xintong Han, Zhi-Qi Cheng, Qi Dai, Chong Luo, and Zuxuan Wu. Stableanimator: High-quality identity-preserving human image animation. arXiv preprint arXiv:2411.17697, 2024. 2, 3 [59] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, Jianyuan Zeng, Jiayu Wang, Jingfeng Zhang, Jingren Zhou, Jinkai Wang, Jixuan Chen, Kai Zhu, Kang Zhao, Keyu Yan, Lianghua Huang, Mengyang Feng, Ningyi Zhang, Pandeng Li, Pingyu Wu, Ruihang Chu, Ruili Feng, Shiwei Zhang, Siyang Sun, Tao Fang, Tianxing Wang, Tianyi Gui, Tingyu Weng, Tong Shen, Wei Lin, Wei Wang, Wei Wang, Wenmeng Zhou, Wente Wang, Wenting Shen, Wenyuan Yu, Xianzhong Shi, Xiaoming Huang, Xin Xu, Yan Kou, Yangyu Lv, Yifei Li, Yijing Liu, Yiming Wang, Yingya Zhang, Yitong Huang, Yong Li, You Wu, Yu Liu, Yulin Pan, Yun Zheng, Yuntao Hong, Yupeng Shi, Yutong Feng, Zeyinzi Jiang, Zhen Han, Zhi-Fan Wu, and Ziyu Liu. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. 6 [60] Lizhen Wang, Zhiyuan Chen, Tao Yu, Chenguang Ma, Liang Li, and Yebin Liu. Faceverse: fine-grained and detail-controllable 3d face morphable model from hybrid dataset. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2033320342, 2022. siani, and Sifei Liu. Affordance diffusion: Synthesizing hand-object interactions. In CVPR, 2023. 2 [61] Tan Wang, Linjie Li, Kevin Lin, Yuanhao Zhai, Chung-Ching Lin, Zhengyuan Yang, Hanwang Zhang, Zicheng Liu, and Lijuan Wang. Disco: Disentangled control for realistic human dance generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9326 9336, 2024. 2, 3 [62] Xiang Wang, Shiwei Zhang, Changxin Gao, Jiayu Wang, Xiaoqiang Zhou, Yingya Zhang, Luxin Yan, and Nong Sang. Unianimate: Taming unified video diffusion models for consistent human image animation. arXiv preprint arXiv:2406.01188, 2024. 2, 3, 6, 7 [63] Yujie Wei, Shiwei Zhang, Zhiwu Qing, Hangjie Yuan, Zhiheng Liu, Yu Liu, Yingya Zhang, Jingren Zhou, and Hongming Shan. Dreamvideo: Composing your dream videos with customized subject and motion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6537 6549, 2024. 3 [64] Sirui Xu, Zhengyuan Li, Yu-Xiong Wang, and LiangYan Gui. InterDiff: Generating 3d human-object interactions with physics-informed diffusion. In ICCV, 2023. 2 [65] Sirui Xu, Hung Yu Ling, Yu-Xiong Wang, and Liangyan Gui. Intermimic: Towards universal wholebody control for physics-based human-object interactions. In CVPR, 2025. [66] Ziyi Xu, Ziyao Huang, Juan Cao, Yong Zhang, Xiaodong Cun, Qing Shuai, Yuchen Wang, Linchao Bao, Jintao Li, and Fan Tang. Anchorcrafter: Animate cyber-anchors selling your products via humanobject interacting video generation. arXiv preprint arXiv:2411.17383, 2024. 2, 6, 7 [67] Zhongcong Xu, Jianfeng Zhang, Jun Hao Liew, Hanshu Yan, Jia-Wei Liu, Chenxu Zhang, Jiashi Feng, and Mike Zheng Shou. Magicanimate: Temporally consistent human image animation using diffusion model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1481 1490, 2024. 2, 3 [68] Zihui Xue, Mi Luo, Chen Changan, and Kristen Grauman. Hoi-swap: Swapping objects in videos with arXiv preprint hand-object interaction awareness. arXiv:2406.07754, 2024. 2 [69] Yufei Ye, Abhinav Hebbar, Poorvi Gupta, and Shubham Tulsiani. Diffusion-guided reconstruction of everyday hand-object interaction clips. In ICCV, 2023. 2 [70] Yufei Ye, Xueting Li, Abhinav Gupta, Shalini De Mello, Stan Birchfield, Jiaming Song, Shubham Tul- [71] Shenghai Yuan, Jinfa Huang, Xianyi He, Yunyuan Ge, Yujun Shi, Liuhan Chen, Jiebo Luo, and Identity-preserving text-to-video generaLi Yuan. arXiv preprint tion by frequency decomposition. arXiv:2411.17440, 2024. 3 [72] Hui Zhang, Sammy Christen, Zicong Fan, Otmar Hilliges, and Jie Song. GraspXL: Generating grasping motions for diverse objects at scale. In European Conference on Computer Vision (ECCV), 2024. 2 [73] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Scaling in-the-wild training for diffusion-based illumination harmonization and editing by imposing consistent light transport. In The Thirteenth International Conference on Learning Representations, 2025. 6 [74] Mengqi Zhang, Yang Fu, Zheng Ding, Sifei Liu, Zhuowen Tu, and Xiaolong Wang. Hoidiffusion: Generating realistic 3d hand-object interaction data. arXiv preprint arXiv:2403.12011, 2024. 2 [75] Siwei Zhang, Yan Zhang, Qianli Ma, Michael Black, and Siyu Tang. Place: Proximity learning of articulaIn 2020 Intertion and contact in 3d environments. national Conference on 3D Vision (3DV), pages 642 651. IEEE, 2020. 2 [76] Yan Zhang, Mohamed Hassan, Heiko Neumann, Michael Black, and Siyu Tang. Generating 3d peoIn Proceedings of the ple in scenes without people. IEEE/CVF conference on computer vision and pattern recognition, pages 61946204, 2020. 2 [77] Yuang Zhang, Jiaxi Gu, Li-Wen Wang, Han Wang, Junqi Cheng, Yuefeng Zhu, and Fangyuan Zou. Mimicmotion: High-quality human motion video generarXiv ation with confidence-aware pose guidance. preprint arXiv:2406.19680, 2024. 2, 3 [78] Hongxiang Zhao, Xingchen Liu, Mutian Xu, Yiming Hao, Weikai Chen, and Xiaoguang Han. Taste-rob: Advancing video generation of task-oriented handobject interaction for generalizable robotic manipulation. In CVPR, 2025. [79] Yong Zhong, Zhuoyi Yang, Jiayan Teng, Xiaotao Gu, and Chongxuan Li. Concat-id: Towards universal identity-preserving video synthesis. arXiv preprint arXiv:2503.14151, 2025. 3 [80] Jiachen Zhou, Mingsi Wang, Tianlin Li, Guozhu Meng, and Kai Chen. Dormant: Defending against pose-driven human image animation. arXiv preprint arXiv:2409.14424, 2024. 8 [81] Shenhao Zhu, Junming Leo Chen, Zuozhuo Dai, Zilong Dong, Yinghui Xu, Xun Cao, Yao Yao, Hao Zhu, and Siyu Zhu. Champ: Controllable and consistent human image animation with 3d parametric guidance. In European Conference on Computer Vision, pages 145162. Springer, 2024. 2,"
        }
    ],
    "affiliations": [
        "ByteDance Intelligent Creation"
    ]
}