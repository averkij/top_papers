{
    "paper_title": "Scaling Code-Assisted Chain-of-Thoughts and Instructions for Model Reasoning",
    "authors": [
        "Honglin Lin",
        "Qizhi Pei",
        "Xin Gao",
        "Zhuoshi Pan",
        "Yu Li",
        "Juntao Li",
        "Conghui He",
        "Lijun Wu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reasoning capability is pivotal for Large Language Models (LLMs) to solve complex tasks, yet achieving reliable and scalable reasoning remains challenging. While Chain-of-Thought (CoT) prompting has become a mainstream approach, existing methods often suffer from uncontrolled generation, insufficient quality, and limited diversity in reasoning paths. Recent efforts leverage code to enhance CoT by grounding reasoning in executable steps, but such methods are typically constrained to predefined mathematical problems, hindering scalability and generalizability. In this work, we propose Caco (Code-Assisted Chain-of-ThOught), a novel framework that automates the synthesis of high-quality, verifiable, and diverse instruction-CoT reasoning data through code-driven augmentation. Unlike prior work, Caco first fine-tunes a code-based CoT generator on existing math and programming solutions in a unified code format, then scales the data generation to a large amount of diverse reasoning traces. Crucially, we introduce automated validation via code execution and rule-based filtering to ensure logical correctness and structural diversity, followed by reverse-engineering filtered outputs into natural language instructions and language CoTs to enrich task adaptability. This closed-loop process enables fully automated, scalable synthesis of reasoning data with guaranteed executability. Experiments on our created Caco-1.3M dataset demonstrate that Caco-trained models achieve strong competitive performance on mathematical reasoning benchmarks, outperforming existing strong baselines. Further analysis reveals that Caco's code-anchored verification and instruction diversity contribute to superior generalization across unseen tasks. Our work establishes a paradigm for building self-sustaining, trustworthy reasoning systems without human intervention."
        },
        {
            "title": "Start",
            "content": "Scaling Code-Assisted Chain-of-Thoughts and Instructions for Model Reasoning Honglin Lin1,2, Qizhi Pei1, Xin Gao1,2, Zhuoshi Pan1, Yu Li1, Juntao Li3, Conghui He1, Lijun Wu1 1OpenDataLab, Shanghai Artificial Intelligence Laboratory, 2Shanghai Jiao Tong University, 3Soochow University Reasoning capability is pivotal for Large Language Models (LLMs) to solve complex tasks, yet achieving reliable and scalable reasoning remains challenging. While Chain-of-Thought (CoT) prompting has become mainstream approach, existing methods often suffer from uncontrolled generation, insufficient quality, and limited diversity in reasoning paths. Recent efforts leverage code to enhance CoT by grounding reasoning in executable steps, but such methods are typically constrained to predefined mathematical problems, hindering scalability and generalizability. In this work, we propose Caco (Code-Assisted Chain-of-ThOught), novel framework that automates the synthesis of high-quality, verifiable, and diverse instruction-CoT reasoning data through code-driven augmentation. Unlike prior work, Caco first fine-tunes code-based CoT generator on existing math and programming solutions in unified code format, then scales the data generation to large amount of diverse reasoning traces. Crucially, we introduce automated validation via code execution and rule-based filtering to ensure logical correctness and structural diversity, followed by reverse-engineering filtered outputs into natural language instructions and language CoTs to enrich task adaptability. This closedloop process enables fully automated, scalable synthesis of reasoning data with guaranteed executability. Experiments on our created Caco-1.3M dataset demonstrate that Caco-trained models achieve strong competitive performance on mathematical reasoning benchmarks, outperforming existing strong baselines. Further analysis reveals that Cacos code-anchored verification and instruction diversity contribute to superior generalization across unseen tasks. Our work establishes paradigm for building self-sustaining, trustworthy reasoning systems without human intervention. Date: October 7, 2025 Correspondence: Lijun Wu, wulijun@pjlab.org.cn Code: https://github.com/LHL3341/Caco Equal contribution: Honglin Lin, Qizhi Pei 5 2 0 2 5 ] . [ 1 1 8 0 4 0 . 0 1 5 2 : r Figure 1: Overview of Caco results. Caco shows superior performance on Olympiad Bench and on average than baseline methods. 1 Scaling Code-Assisted Chain-of-Thoughts and Instructions for Model Reasoning"
        },
        {
            "title": "1 Introduction",
            "content": "The advent of Large Language Models (LLMs) [44, 32, 35] has revolutionized domains requiring complex reasoning, such as mathematics, code, and algorithmic problem-solving [42, 15, 1]. Recent LLMs demonstrate remarkable capabilities in generating step-by-step solutions through Chain-ofThought (CoT) [42, 25, 45] prompting, where intermediate reasoning steps are explicitly articulated before final answers. This paradigm has become instrumental in tasks like mathematical problem solving and program synthesis, where systematic logic decomposition is critical. prevalent strategy involves generating long CoT sequences [29, 12, 36, 37] to mimic human-like deliberation. However, these CoT approaches predominantly rely on natural language reasoning traces, which suffer from several limitations. (1) Unverifiability, since natural language reasoning is not executable, errors in intermediate steps may propagate and lead to incorrect conclusions; (2) Scalability constraints, high-quality CoT data typically requires manual annotation, making it difficult to scale to diverse problem domains. To address these issues, recent works have explored code-assisted reasoning [11, 41, 23, 10], where reasoning steps are grounded in executable code snippets (e.g., Python codes or algorithm sketches). By translating natural language logic into formal code, these methods enable automatic verification through code execution. Preliminary studies [11] demonstrate that code-verified CoT can reduce hallucination and improve answer accuracy. However, existing implementations struggle to generalize beyond predefined mathematical problems, limiting their adaptability and scalability [39, 41, 10]. In this work, we introduce Caco, scalable code-assisted CoT and instruction generation framework designed to automate the production of high-quality reasoning training data through code-anchored refinement. core innovation of Caco lies in its fine-tuning of base LLM on compact set of structured code CoT demonstrations, enabling the model to learn systematic code reasoning solutions. Leveraging this fine-tuned LLM, we generate large-scale candidate code-based CoT solutions, which are subsequently refined via an automated verification engine. This engine executes code snippets, verifies logical consistency, and enforces diversity in reasoning patterns. Finally, the validated code solutions are translated back into natural language instructions and the corresponding language CoTs, yielding instruction-aligned data pairs that establish bidirectional alignment between code and textual reasoning paths. The Caco generated natural language CoT offers several advantages. (1) Scalability: Through these model-generated synthetic code CoTs, we eliminate reliance on manual annotation of the aligned language CoTs, enabling the creation of millions of high-quality reasoning traces (e.g., our Caco-1.3M dataset); (2) Verifiability: Not only are the answers guaranteed to be correct for the augmented instructions, but the executable and automatic validation of intermediate steps of Code CoTs also ensures the aligned language CoTs to be correct solutions. (3) Diversity: By harnessing the fine-tuned LLMs generative capacity and sampling mechanism, Caco produces varied reasoning paths as well as the instructions, enhancing generalization across different problem types. We evaluate Caco through extensive experiments on standard mathematical reasoning benchmarks. Models fine-tuned using our Caco-1.3M dataset achieve strong competitive performance; for example, attaining 92.6% accuracy on GSM8K and 82.4% on MATH, significantly outperforming prior approaches. Caco also exhibits strong generalization, the trained model maintains 67.7% accuracy on average over multiple benchmarks, surpassing comparable methods by margin exceeding 7.9%. Further analysis confirms that Caco-generated CoT data preserves high diversity and scalability. Beyond advancing superior performance in mathematical reasoning, our work establishes generalizable framework for developing self-improving and verifiable LLMs across algorithmic domains. 2 Scaling Code-Assisted Chain-of-Thoughts and Instructions for Model Reasoning"
        },
        {
            "title": "2.1 Data Augmentation for Mathematical Reasoning",
            "content": "A wide range of recent efforts have explored different strategies for constructing instruction-tuning datasets tailored to mathematical reasoning [41, 20, 46]. For example, WizardMath [25], MetaMath [45], Orca-Math [28], MMIQC [21], and MathFusion [30] enhance answers and rationales for seed problems through prompt engineering and reinforcement learning techniques. KPMath [16], MathScale [34], and ScaleQuest [8] generate new problems from scratch by extracting mathematical concepts and topical structures. MAmooTH2 [47] and Numina-Math [18] construct instruction-tuning datasets by collecting and curating large-scale data from the web. DART-Math [38] applies rejection sampling based on problem difficulty to ensure the quality of generated solutions. Caco also falls within this scope and uses code as scalable medium to generate diverse mathematical problems."
        },
        {
            "title": "2.2 Code Integration for Enhanced Reasoning",
            "content": "LLMs often make calculation errors in complex mathematical reasoning (e.g., computing eigenvalues) when using CoT prompting [3, 9]. To address this, methods such as Program of Thoughts (PoT) [3], Program-Aided Language models (PAL) [9], and Code-based Self-Verification (CSV) [50] are proposed to prompt LLMs to generate executable code, leveraging external code interpreters for accurate computation. As open-source models improve, code-integrated data for post-training has gained attention. OpenMathInstruct-1 [39], TORA [10], MathCoder [41, 24], MegaMath [51], and DotaMath [17] embed code within natural language, enabling more robust reasoning. MAmmoTH [46] introduces MathInstruct, hybrid of CoT and PoT datasets, allowing for different reasoning strategies for different problems. rStar-Math [11] generates paired natural language rationales and Python code, keeping only verified executable steps. CodeI/O [19] distills diverse reasoning patterns embedded in code by transforming it into code input-output prediction format. MathGenie [23] synthesizes math problems and code-integrated solutions through solution augmentation, question back-translation, and verification-based filtering. Unlike previous works, our Caco leverages code generation model to ensure both scalability and verifiability, while additionally introducing algorithmic problem types to promote greater diversity in problem coverage."
        },
        {
            "title": "3 Method",
            "content": "Overview. Figure 2 presents the overall framework of Caco. We begin by abstracting each problems solution into an executable code template. Based on this, we fine-tune problem generation model (CodeGen) to learn diverse reasoning strategies by extending these templates. Sampling from the trained model yields large number of new programs, each representing unique solution pattern for particular family of problems. Each code pattern is back-translated into concrete mathematical problems and corresponding step-by-step solutions. Only the instances where the natural language answer matches the code output are retained. Details regarding the prompt formulations, model specifications, and the criteria for filtering Code CoTs are provided in Appendix A."
        },
        {
            "title": "3.1 Unifying Code CoT",
            "content": "To improve the quality, consistency, and verifiability of CoT reasoning for math problems, we explore unified Code CoT representation. Motivated by prior findings that code data can enhance mathematical reasoning in language models [4, 46], we collect and standardize Code CoTs from both mathematical and algorithmic domains. Specially, we use general LLM Gpc : to map each problem to code and an executor that returns the correct answer upon running c. We retain only verified traces; namely, the seed set is Cseed = { Gpc(p) F(c) = }. This unified representation not 3 Scaling Code-Assisted Chain-of-Thoughts and Instructions for Model Reasoning Figure 2: An overview framework of Caco data generation, including unifying Code CoT, scaling Code CoT with CodeGen, and instruction reversal and language CoT generation. only improves interpretability and execution fidelity but also lays the groundwork for scalable data generation and model training. Mathematical Problems. We collected broad set of mathematical problems from multiple sources to ensure diversity, such as the MATH dataset [14] (7.5K), DeepScaleR [26] (40K), and BigMath [2] (251K). These problems vary in complexity and format; some are accompanied by natural language CoT explanations, while others are not. To unify their representation, we convert each solution into structured Python program following generic template (See Prompt 1). This template encodes problem inputs as dictionaries and defines problem-solving logic through explicit function calls. It supports wide range of reasoning typesarithmetic, algebraic, geometric, probabilisticwhile enabling direct execution for correctness verification. For example, consider the problem: George has an unfair six-sided die. The probability that it rolls 6 is 1 any other number is 1 10 . What is the expected value of the number shown when this die is rolled? 2 , and the probability that it rolls We transform its solution into the following code representation: def expected_value ( probabilities , values ): return sum (p * for , in zip ( probabilities , values )) probabilities = [1/10 , 1/10 , 1/10 , 1/10 , 1/10 , 1/2] # Probabilities for 1, 2, 3, 4, 5, 6 values = [1 , 2, 3, 4, 5, 6] # Values on the die input = {\" probabilities \": probabilities , \" values \": values } output = expected_value (** input ) print ( output ) This standardized representation ensures structural consistency across different problem types and facilitates easier interpretation by both models and humans. Algorithmic Problems. In parallel, we incorporate algorithmic problems as an additional source of structured reasoning. We sample 40K problems from the Kodcode [43] dataset, covering key Scaling Code-Assisted Chain-of-Thoughts and Instructions for Model Reasoning Figure 3: case of one problem with its Code CoT. We demonstrate two augmentations, where problem-level augmentation refers to the original Code CoT can be back-translated into multiple question variants, and pattern-level augmentation means our CodeGen is capable of generating novel Code CoTs that generalize beyond the original seed patterns. algorithmic domains such as sorting, searching, and dynamic programming. These problems typically come with code-level solutions and brief natural language comments, providing native form of Code CoT. To ensure consistency across data sources, we normalize all algorithmic solutions into the same Python-based template used for mathematical problems. This standardization enables joint training and evaluation under single format. The conversion prompts are described in Prompt 2. Unified Seed Code CoTs. After Code CoT generation, we perform rigorous post-processing to ensure quality. Following the procedure described in Section A.4, we validate each code sample through execution: only programs that run successfully, produce correct outputs, and conform to the standardized format are retained. This filtering yields curated seed corpus of 146K high-quality Code CoT instances (122K Math + 24K Code). Among these, 109K problems originally had solutions, which we refer to as Seed109K in the experiments. The resulting dataset provides robust foundation for training models to enable effective generation of verifiable and scalable CoT reasoning in executable form in Section 3.2."
        },
        {
            "title": "3.2 Scaling Code CoT with CodeGen",
            "content": "To scale the generation of high-quality code-based reasoning chains, we leverage the seed Code CoT dataset introduced previously to train dedicated Code CoT generation model, CodeGen, so as to enable automated synthesis of executable, diverse, and logically coherent Code CoTs at scale. By training model to internalize the structure and logic of our unified format, we facilitate the creation of new reasoning traces without relying on costly human annotations or handcrafted solutions. Training CodeGen on Unified Code CoTs. We fine-tune unconditional CodeGen Uθ on Cseed to model the distribution of valid reasoning programs. min θ L(θ) = cCseed t=1 log pθ (cid:0)ct (cid:12) (cid:12) c<t (cid:1). (1) The resulting model, CodeGen, is designed to generalize the reasoning patterns embedded in our dataset and produce structurally consistent code-based CoTs for both mathematical and algorithmic problems. Fine-tuning is conducted using pretty simple prompt described in Appendix Table 3. Notably, 5 Scaling Code-Assisted Chain-of-Thoughts and Instructions for Model Reasoning training uses only Code CoTs, without problem contexts or requirements, focusing on internalizing the reasoning trace space rather than specific problem-solution pairs. In this way, we aim to largely explore the diverse Code CoTs in the generation phase. Large-Scale CoT Generation via Sampling. After fine-tuning, we employ CodeGen to generate large number of new Code CoT samples Csamp = {c Uθ}. Using temperature sampling, we generate multiple candidate programs use prompt in Appendix Table 4 (same as training CodeGen). This sampling-based approach introduces stochasticity into the decoding process, allowing the model to explore diverse set of reasoning paths and solution strategies. The result is scalable and flexible pipeline for synthesizing varied Code CoTs. As illustrated in Figure 3, even for problem types the model has seen during trainingsuch as calculating the expected value of biased diethe model is capable of restructuring the logic, e.g., by decomposing the problem into multiple rolls and aggregating expected outcomes. This demonstrates that CodeGen supports two complementary modes of augmentation: Problem-level Augmentation arises when natural language problems are synthesized from code by varying the situational context or rephrasing the same underlying logic in different stylistic forms. This introduces diversity in surface formulations (implemented in Section 3.3). Pattern-level Augmentation arises when the CodeGen explores novel reasoning structuressuch as problem decompositions or alternative solution strategiesthereby enriching the pool of underlying logic templates. Together, these modes yield both surface-level diversity and deeper structural variability in the synthesized dataset. Additional representative samples along with training settings and sampling configurations are provided in Appendix A. Code CoT Filtration. To ensure the quality of generated Code CoTs, we apply the execution-based filtering criteria similar to the unified seed Code CoTs. The only difference is that at this stage, we do not enforce output matching with known answers, as consistency verification is deferred to the later back-translation stage. In total, we synthesize approximately 5.3M Code CoT samples. After filtering, we retain high-quality subset of around 4.6M executable and structurally valid programs. This large-scale dataset forms the basis for the subsequent stage of problem synthesis, enabling us to bootstrap new questionsolution pairs and further expand the reach of code-based reasoning. 3."
        },
        {
            "title": "Instruction Reversal and Language CoT Generation",
            "content": "Following the generation of substantial set of executable code templates, we distill their underlying logic to synthesize natural language problems alongside their corresponding solutions, derived from the combined set Cseed Csamp. As shown in Figure 3, this process significantly expands our dataset by producing diverse and high-quality problem-answer pairs. Two-Stage QA Generation. For quality control, we adopt two-stage method to generate problem and language CoT instead of one-step generation for both. In our preliminary experiments, we find that jointly generating the instruction and language CoT together based on the Code CoT is easy to lead to low quality or incorrect language solutions, perhaps due to the lazy mode [39] by LLMs since it sees the correct Code CoT as guidance. Therefore, each code snippet is paired with representative input-output examples (code-instruction pair) and provided as input to the LLM (see Prompt 5), which generates natural language problem at the first stage. Secondly, we prompt the generated problem to the LLM (see Prompt 6) for natural language CoT synthesis, which largely forces the LLM to think and generate correct language CoTs. Dual Verification. Two filtration and verification ways are processed to ensure the correctness of the instruction and natural language CoT. 6 Scaling Code-Assisted Chain-of-Thoughts and Instructions for Model Reasoning (1) Answer Consistency: We execute the code and compare its output to the answer inferred from the LLMs CoT reasoning. Any mismatches are discarded to maintain high precision. (2) CoT Consistency: We remove samples where the language CoT and Code CoT for the same problem are not aligned, based on the consistency judgment in Prompt 7. This process ensures the correctness of the reasoning steps in the language CoT. Only tuples (p, s, c) that simultaneously satisfy both conditions are retained. This filtering process can be formally expressed as: Dfinal = {(p, s, c) = Gcp(c), = Gps(p), (Ans(s) = Exec(c)) Con(s, c)}, (2) where denotes general-purpose LLM used for instruction reversal and answer generation; Ans(s) extracts the final answer from the solution and compares it with the execution result of the code Exec(c); and Con(s, c) represents the CoT consistency check between the natural language solution and the code. After this pipeline, we obtain approximately 1.3M validated instruction-answer pairs in Dfinal, which significantly enhance the diversity and reliability of the training data and serve as valuable resource for downstream reasoning tasks."
        },
        {
            "title": "4.1 Experimental Setup\nBaselines. We compare our Caco-generated dataset against several mainstream synthesized instruction-\ntuning datasets for math reasoning, including data-centric methods such as MetaMath [45], MMIQC [21],\nNuminaMath [18], MathFusion [34], RFT [38], and DART-Math [38], which all demonstrate strong\nreasoning enhancement. Besides, we also include well-known open-source instruction-tuned or\nreinforcement learning (RL)-based models as baselines: LLaMA3-7B-Instruct [49], Qwen2.5-Math-\nInstruct [44], and DeepSeekMath-7B-RL [32].\nTraining Configuration. To evaluate the generalizable effectiveness of our Caco produced dataset, our\nexperiments are conducted on two math-specialized LLMs—DeepSeekMath-7B [32] and Qwen2.5-Math-7B [44],\nas well as one general-purpose model, LLaMA3-8B [49]. Unless otherwise specified, all models are\nfine-tuned for 3 epoch using a learning rate of 5 × 10−6, a batch size of 128, and a cosine decay schedule\nwith a warm-up ratio of 0.03. Additional implementation details are provided in Appendix B.",
            "content": "Evaluation Setup. Following the evaluation protocol of DartMath [38], we evaluate on multiple popular benchmarks to show the advantages, including MATH [14], GSM8K [6], CollegeMath [34], DeepMind-Mathematics [31], OlympiadBench-Math [13], and TheoremQA [5]. Solutions are generated using greedy decoding with maximum sequence length of 2048 tokens, and we report Pass@1 accuracy in the zero-shot setting without tool integration. Further evaluation details and benchmark statistics can be found in Appendix B."
        },
        {
            "title": "4.2 Main Results\nTable 1 presents a comprehensive comparison of our Caco against a series of strong baselines across\nthe three different base models (DeepSeekMath-7B, Qwen2.5-Math-7B, and LLaMA3-8B). We report\nresults for two synthesized data sizes: Caco-596K and Caco-1.3M samples. From the results, we can\nsummarize the following findings:\nConsistent improvements across base models. Caco consistently outperforms existing methods across\nall three base models. For instance, on LLaMA3-8B, Caco-1.3M achieves an average score of 57.3,\nsurpassing the previous best of 39.7 from DartMath [38] by a relative improvement of 44.3%.",
            "content": "7 Scaling Code-Assisted Chain-of-Thoughts and Instructions for Model Reasoning Model # Samples MATH GSM8K College DM Olympiad Theorem AVG DeepSeekMath-7B (Math-Specialized Base Model) DeepSeekMath-7B-RL DeepSeekMath-7B-MetaMath DeepSeekMath-7B-MMIQC DeepSeekMath-7B-NuminaMath DeepSeekMath-7B-RFT DeepSeekMath-7B-DartMath DeepSeekMath-7B-MathFusion Caco-Seed109K-DeepSeekMath-7B Caco-596K-DeepSeekMath-7B Caco-1.3M-DeepSeekMath-7B - 400K 2.3M 860K 590K 590K 60K 109K 596K 1.3M 51.1 40.2 45.3 47.7 53.0 53.6 53.4 58.7 63.5 68.2 88.8 80.5 79.0 78.5 88.2 86.8 77.9 82.4 85.2 85.1 34.5 35.7 35.3 38.0 41.9 40.7 39.8 42.9 44.4 46.0 58.2 48.1 52.9 56.2 60.2 61.6 65.8 71.3 78.0 80.2 Qwen2.5-Math-7B (Math-Specialized Base Model) Qwen2.5-Math-7B-Instruct Qwen2.5-Math-7B-MetaMath Qwen2.5-Math-7B-NuminaMath Qwen2.5-Math-7B-DartMath Qwen2.5-Math-7B-MathFusion Caco-Seed109K-Qwen2.5-Math-7B Caco-596K-Qwen2.5-Math-7B Caco-1.3M-Qwen2.5-Math-7B - 400K 860K 590K 60K 109K 596K 1.3M 82.1 51.7 70.6 61.4 75.2 80.6 81.1 82.4 94.1 84.7 90.8 89.7 83.5 92.3 92.4 92.6 50.4 40.0 46.1 42.5 43.0 47.1 50.3 51.4 LLaMA3-8B-Instruct LLaMA3-8B-MetaMath LLaMA3-8B-MMIQC LLaMA3-8B-NuminaMath LLaMA3-8B-RFT LLaMA3-8B-DartMath LLaMA3-8B-MathFusion Caco-Seed109K-Llama3-8B Caco-596K-LLaMA3-8B Caco-1.3M-LLaMA3-8B LLaMA3-8B (General Base Model) - 400K 2.3M 860K 590K 590K 60K 109K 596K 1.3M 44.3 32.5 39.5 43.6 39.7 46.6 46.5 55.3 64.3 70.6 53.4 77.3 77.6 79.7 81.7 81.1 79.2 86.0 88.6 89.1 29.8 20.6 29.5 24.7 23.9 28.8 27.9 42.2 44.8 46.2 72.9 62.6 75.1 72.0 76.0 83.0 86.7 87. 42.0 35.0 41.0 43.1 41.7 48.0 43.4 52.0 66.7 72.5 18.8 11.4 13.0 18.2 19.1 21.7 23.3 22.4 25.8 29.5 41.3 18.2 35.9 25.8 39.5 41.6 43.3 46.5 11.3 5.5 9.6 16.4 9.3 14.5 17.2 19.1 24.7 34.1 30.9 21.8 23.4 22.1 27.2 32.2 24.6 28.9 30.2 33.8 40.8 26.5 37.4 35.5 41.5 45.9 45.5 46. 17.7 13.8 16.2 19.9 14.9 19.4 20.0 25.6 27.6 31.0 47.1 39.6 41.5 43.5 48.3 49.4 47.5 51.1 54.5 57.1 63.6 47.3 59.3 54.5 59.8 65.1 66.6 67.7 33.1 30.8 35.6 37.9 35.2 39.7 39.0 46.7 52.8 57.3 Table 1: Performance comparison on mathematical benchmarks including MATH, GSM8K, CollegeMATH (College), DeepMind-Mathematics (DM), OlympiadBench-Math (Olympiad), and TheoremQA (Theorem). The best results are highlighted in bold. Baseline results labeled with are derived from MathFusion [30]. Scaling Code-Assisted Chain-of-Thoughts and Instructions for Model Reasoning Improvement over scaled synthetic data. Performance improves obviously when increasing the Cacogenerated data from 596K to 1.3M. On Qwen2.5-Math-7B, Caco-1.3M achieves 67.7, outperforming Caco-596K by 1.1 and demonstrating the scalability and effectiveness of our approach under larger supervision. Strong performance on challenging subsets. Notably, Caco shows superior performance on harder benchmarks such as OlympiadBench and TheoremQA, where other baselines struggle. For instance, on LLaMA-8B, Caco-596K improves OlympiadBench from 17.2 to 34.1 and TheoremQA from 20.0 to 31.0 compared to MathFusion, which shows the great potential of our approach. Competitive with strong instruction-tuned and RL-based models. Remarkably, Caco matches or exceeds the performance of strong instruction-tuned or RL-finetuned models. For example, on Qwen2.5-Math-7B, Caco-1.3M achieves 67.7, which is comparable to Qwen2.5-Math-7B-Instruct (63.6). On DeepSeekMath and LLaMA series, Caco-1.3M trained models significantly surpass DeepSeekMath7B-RL (47.1) and LLaMA-8B-Instruct (33.1). This greatly demonstrates the superiority of our method. Effectiveness of Caco Data. Compared to the seed data we used to train CodeGen (Caco-Seed-109K), Caco-596K and Caco-1.3M consistently deliver substantial improvements. For instance, on LLaMA3-8B, Caco-1.3M achieves 57.3, significant increase from Caco-Seed-109Ks score of 46.7. This validates our data scaling strategy, showing that our method yields performance gains by ensuring the training data comprehensively represents diverse and challenging problems."
        },
        {
            "title": "5 Analysis",
            "content": "To further understand the strengths of our proposed approach, we analyze three key aspects that contribute to Cacos effectiveness: the diversity, the scalability, and the verification mechanism in the Caco data construction pipeline. Together, these components form the foundation of Cacos training methodology and help explain its strong performance across models and benchmarks. In the following sections, we provide detailed analysis of each component and its contribution. More experiments and discussion of cost are in Appendix and D."
        },
        {
            "title": "5.1 Analysis on Data Diversity",
            "content": "We conduct comprehensive investigation into the diversity of the dataset to assess the range and variability of the Caco-generated problems. This analysis is crucial for understanding how well the model can generate problems across various domains and ensure broad coverage of mathematical topics. By examining both the distribution of problems and the variety of problem types, we aim to demonstrate that the dataset not only spans wide range of topics but also captures diverse problem-solving scenarios that are representative of real-world mathematical challenges. Problem Diversity. We analyze the distribution of problems in the synthesized Caco dataset to assess its coverage and diversity. Specifically, we randomly sample 5K problems from Caco and compare them with samples from the original seed datasets (MATH, DeepScaleR and BigMath). We encode all problems using the all-MiniLM-L6-v2 sentence embedding model1, and visualize their distributions via t-SNE [40], as shown in Figure 4a. The resulting plot demonstrates that Cacos synthesized data broadly and evenly spans the embedding space, effectively covering the original seed distributions. Notably, we observe distinct region on the left side of the plot where Caco samples diverge from the seed data clusters, suggesting that our generation pipeline introduces novel and diverse problem types beyond the original datasets. This supports the claim that Caco enhances distributional generalization through its diverse synthetic augmentation. 1https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2 9 Scaling Code-Assisted Chain-of-Thoughts and Instructions for Model Reasoning Topic Diversity. To further assess the topical diversity of the Caco dataset, we apply clustering analysis to the problem embeddings. Using the same embedding method as before, we encode all problems and then apply the KMeans algorithm [27] to partition them into 12 distinct clusters. The clustering results are visualized in Figure 4b. The clusters reveal wide range of mathematical and algorithmic topics, including algebra, geometry, applied mathematics, data structures, algorithms, and more. This confirms that Caco spans broad spectrum of problem types, rather than concentrating on narrow domains. Representative samples from each identified topic cluster are provided in Appendix for qualitative reference. Figure 4: Left: Problem distribution of our Caco dataset and the original data sources. Right: KMeans clustering result of the problem types."
        },
        {
            "title": "5.3 Ablation on Verification",
            "content": "Figure 5: Left: Comparison of solvability and correctness between generated samples with and without verification. Middle: Accuracy comparison between models trained on verified and non-verified data. Right: Performance improvements of the Caco model as data size increases. Verification is crucial process in our Caco data generation. To further investigate the impact of verification on data quality and reliability, we compare the data with and without applying the verification filtering. We randomly sample 100K data points for each version and use Qwen3-32B to evaluate both the solvability (i.e., whether the problem can be solved) and the correctness (i.e., whether 10 Scaling Code-Assisted Chain-of-Thoughts and Instructions for Model Reasoning the final answer is accurate) of the generated samples. We further evaluate downstream performance by fine-tuning the LLaMA model on each dataset. Impact on Data Quality. As shown in Figure 5a, the verification mechanism substantially improves the quality of the training data. With verification, the ratio of solvable problems increases from 91K to 97K, and the number of correct answers rises from 88K to 93K. These improvements suggest that the verification processbased on answer validation and consistency checks over reasoning chainseffectively filters out low-quality or incorrect samples, resulting in more reliable supervision. Impact on Model Performance. In addition to improving data quality, verification also yields tangible benefits in downstream performance (Figure 5b). The model trained with verified data achieves an average accuracy of 21.8, compared to 20.8 without verification, reflecting consistent improvement across benchmarks. The performance gain is especially notable on more challenging tasks: for instance, on Olympiad, the verified model scores 18.7, outperforming its non-verified counterpart by 1.1 points. This demonstrates that the enhanced data reliability introduced by verification translates into better generalization and reasoning robustness in trained models."
        },
        {
            "title": "5.4 Generality Beyond Mathematics\nWe first evaluate the generalization of Caco models. Using OpenCompass [7], we assess Caco-1.3M\nmodels across a broad set of reasoning tasks, including mathematics (AIME24), code generation\n(HumanEval+), scientific QA (ARC-c), logic puzzles (BBH, KorBench), and general knowledge/science\n(AGIEval). Caco models demonstrate substantial improvements beyond math, with notable gains in\nlogic puzzles, general reasoning, science reasoning, and code tasks. These results indicate that the\nmodels trained on Caco data generalize effectively across diverse benchmarks.",
            "content": "Model AGIEval AIME24 HumanEval+ ARC-c BBH KorBench Average Qwen2.5-Math-7B-base Caco-1.3M-Qwen2.5-Math-7B LLaMA3-8B-base Caco-1.3M-LLaMA3-8B 42.5 53.3 28.5 46. 20.0 23.3 0.0 10.8 12.8 53.1 32.3 34.2 72.2 81.4 79.0 83. 19.9 65.1 19.8 33.8 39.7 47.1 23.8 44.1 34.5 53.9 30.6 42. Table 2: Performance comparison of base models and Caco-augmented models across diverse out-ofdomain benchmarks."
        },
        {
            "title": "Model",
            "content": "AGIEval ARC-c MMLU-STEM Average LLaMA-MegaScience-Seed5.2K LLaMA-MegaScience-Caco37K 42.8 45.0 78.6 84.8 55.4 60.5 59.0 63. Table 3: Evaluation of LLaMA models trained on MegaScience seed data (5.2K) vs. Caco-augmented expansion (37K). We next discuss the generality of the Caco methodology itself beyond mathematics. Although our primary experiments focused on mathematical reasoning, Caco is fundamentally general-purpose framework for structured, code-based reasoning, and is applicable to domains exhibiting logical, symbolic, or programmatic structure, such as logic puzzles, scientific reasoning, and procedural tasks. In logic puzzles, for instance, many problems share reusable underlying reasoning template (e.g., arithmetic expression puzzles, countdown problems), which can be parameterized in code to generate diverse instances. This aligns with Cacos central principle: code abstracts problem logic more compactly than natural language, enabling systematic sampling and verification. To test cross-domain applicability, we applied Caco to 5.2K science reasoning seeds from MegaScience. The pipeline generated 37K valid QA samples, and fine-tuning LLaMA on these yielded an average 11 Scaling Code-Assisted Chain-of-Thoughts and Instructions for Model Reasoning score improvement from 59.0 to 63.4 across AGIEval, ARC-c, and MMLU-STEM  (Table 3)  . These results confirm that Cacos code-driven design enables effective extension to new domains where logic can be programmatically represented."
        },
        {
            "title": "6 Conclusion",
            "content": "In this work, we present Caco, code-assisted framework for generating high-quality, verifiable, and diverse chain-of-thought reasoning data. By leveraging code execution and automated filtering, Caco enables scalable synthesis of logically grounded instruction data without human supervision. Models trained with Caco outperform strong baselines on both mathematical reasoning benchmarks and out-of-domain benchmarks. Our findings highlight the effectiveness of code-driven verification and instruction diversity in improving reasoning generalization. 12 Scaling Code-Assisted Chain-of-Thoughts and Instructions for Model Reasoning References [1] Janice Ahn, Rishu Verma, Renze Lou, Di Liu, Rui Zhang, and Wenpeng Yin. Large language models for mathematical reasoning: Progresses and challenges. In Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics: Student Research Workshop, pp. 225237, 2024. [2] Alon Albalak, Duy Phung, Nathan Lile, Rafael Rafailov, Kanishk Gandhi, Louis Castricato, Anikait Singh, Chase Blagden, Violet Xiang, Dakota Mahan, et al. Big-math: large-scale, high-quality math dataset for reinforcement learning in language models. arXiv preprint arXiv:2502.17387, 2025. [3] Wenhu Chen, Xueguang Ma, Xinyi Wang, and William Cohen. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. Transactions on Machine Learning Research. [4] Wenhu Chen, Xueguang Ma, Xinyi Wang, and William Cohen. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. arXiv preprint arXiv:2211.12588, 2022. [5] Wenhu Chen, Ming Yin, Max Ku, Pan Lu, Yixin Wan, Xueguang Ma, Jianyu Xu, Xinyi Wang, and Tony Xia. Theoremqa: theorem-driven question answering dataset. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 78897901, 2023. [6] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. [7] OpenCompass Contributors. Opencompass: universal evaluation platform for foundation models. https://github.com/open-compass/opencompass, 2023. [8] Yuyang Ding, Xinyu Shi, Xiaobo Liang, Juntao Li, Qiaoming Zhu, and Min Zhang. Unleashing reasoning capability of llms via scalable question synthesis from scratch. arXiv preprint arXiv:2410.18693, 2024. [9] Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. Pal: Program-aided language models. In International Conference on Machine Learning, pp. 10764 10799. PMLR, 2023. [10] Zhibin Gou, Zhihong Shao, Yeyun Gong, Yujiu Yang, Minlie Huang, Nan Duan, Weizhu Chen, et al. Tora: tool-integrated reasoning agent for mathematical problem solving. In The Twelfth International Conference on Learning Representations. [11] Xinyu Guan, Li Lyna Zhang, Yifei Liu, Ning Shang, Youran Sun, Yi Zhu, Fan Yang, and Mao Yang. rstar-math: Small llms can master math reasoning with self-evolved deep thinking. arXiv preprint arXiv:2501.04519, 2025. [12] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [13] Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, et al. Olympiadbench: challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. arXiv preprint arXiv:2402.14008, 2024. [14] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. Sort, 2(4):06. [15] Jie Huang and Kevin Chen-Chuan Chang. Towards reasoning in large language models: survey. In Findings of the Association for Computational Linguistics: ACL 2023, pp. 10491065, 2023. [16] Yiming Huang, Xiao Liu, Yeyun Gong, Zhibin Gou, Yelong Shen, Nan Duan, and Weizhu Chen. Key-pointdriven data synthesis with its enhancement on mathematical reasoning. arXiv preprint arXiv:2403.02333, 2024. [17] Chengpeng Li, Guanting Dong, Mingfeng Xue, Ru Peng, Xiang Wang, and Dayiheng Liu. Dotamath: Decomposition of thought with code assistance and self-correction for mathematical reasoning. arXiv preprint arXiv:2407.04078, 2024. 13 Scaling Code-Assisted Chain-of-Thoughts and Instructions for Model Reasoning [18] Jia Li, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Shengyi Huang, Kashif Rasul, Longhui Yu, Albert Jiang, Ziju Shen, et al. Numinamath: The largest public dataset in ai4maths with 860k pairs of competition math problems and solutions. 2024. [19] Junlong Li, Daya Guo, Dejian Yang, Runxin Xu, Yu Wu, and Junxian He. Codei/o: Condensing reasoning patterns via code input-output prediction. arXiv preprint arXiv:2502.07316, 2025. [20] Honglin Lin, Zhuoshi Pan, Yu Li, Qizhi Pei, Xin Gao, Mengzhang Cai, Conghui He, and Lijun Wu. Metaladder: Ascending mathematical solution quality via analogical-problem reasoning transfer. arXiv preprint arXiv:2503.14891, 2025. [21] Haoxiong Liu, Yifan Zhang, Yifan Luo, and Andrew Yao. Augmenting math word problems via iterative question composing. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pp. 2460524613, 2025. [22] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. [23] Zimu Lu, Aojun Zhou, Houxing Ren, Ke Wang, Weikang Shi, Junting Pan, Mingjie Zhan, and Hongsheng Li. Mathgenie: Generating synthetic data with question back-translation for enhancing mathematical reasoning of llms. arXiv preprint arXiv:2402.16352, 2024. [24] Zimu Lu, Aojun Zhou, Ke Wang, Houxing Ren, Weikang Shi, Junting Pan, Mingjie Zhan, and Hongsheng Li. Mathcoder2: Better math reasoning from continued pretraining on model-translated mathematical code. arXiv preprint arXiv:2410.08196, 2024. [25] Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jian-Guang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, Yansong Tang, and Dongmei Zhang. Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct. In The Thirteenth International Conference on Learning Representations. [26] Michael Luo, Sijun Tan, Justin Wong, Xiaoxiang Shi, William Tang, Manan Roongta, Colin Cai, Jeffrey Luo, Tianjun Zhang, Li Erran Li, et al. Deepscaler: Surpassing o1-preview with 1.5 model by scaling rl. Notion Blog, 2025. [27] James MacQueen. Some methods for classification and analysis of multivariate observations. In Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics and Probability, Volume 1: Statistics, volume 5, pp. 281298. University of California press, 1967. [28] Arindam Mitra, Hamed Khanpour, Corby Rosset, and Ahmed Awadallah. Orca-math: Unlocking the potential of slms in grade school math. arXiv preprint arXiv:2402.14830, 2024. [29] OpenAI. Learning to reason with llms, 2024. URL https://openai.com/index/ learning-to-reason-with-llms. [30] Qizhi Pei, Lijun Wu, Zhuoshi Pan, Yu Li, Honglin Lin, Chenlin Ming, Xin Gao, Conghui He, and Rui Yan. Mathfusion: Enhancing mathematic problem-solving of llm through instruction fusion. arXiv preprint arXiv:2503.16212, 2025. [31] David Saxton, Edward Grefenstette, Felix Hill, and Pushmeet Kohli. Analysing mathematical reasoning abilities of neural models. In International Conference on Learning Representations. [32] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [33] Weisong Sun, Chunrong Fang, Yun Miao, Yudu You, Mengzhe Yuan, Yuchen Chen, Quanjun Zhang, An Guo, Xiang Chen, Yang Liu, et al. Abstract syntax tree for programming language understanding and representation: How far are we? arXiv preprint arXiv:2312.00413, 2023. [34] Zhengyang Tang, Xingxing Zhang, Benyou Wang, and Furu Wei. Mathscale: Scaling instruction tuning for mathematical reasoning. In International Conference on Machine Learning, pp. 4788547900. PMLR, 2024. [35] Mistral AI team. Learning to reason with llms, 2024. URL https://mistral.ai/en/news/mathstral. 14 Scaling Code-Assisted Chain-of-Thoughts and Instructions for Model Reasoning [36] Qwen Team. Qwq: Reflect deeply on the boundaries of the unknown, November 2024. URL https: //qwenlm.github.io/blog/qwq-32b-preview/. [37] Qwen Team. Qwen3: Think deeper, act faster, April 2025. URL https://qwenlm.github.io/blog/qwen3/. [38] Yuxuan Tong, Xiwen Zhang, Rui Wang, Ruidong Wu, and Junxian He. Dart-math: Difficulty-aware rejection tuning for mathematical problem-solving. Advances in Neural Information Processing Systems, 37:78217846, 2024. [39] Shubham Toshniwal, Ivan Moshkov, Sean Narenthiran, Daria Gitman, Fei Jia, and Igor Gitman. Openmathinstruct-1: 1.8 million math instruction tuning dataset. Advances in Neural Information Processing Systems, 37:3473734774, 2024. [40] Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine learning research, 9(11), 2008. [41] Ke Wang, Houxing Ren, Aojun Zhou, Zimu Lu, Sichun Luo, Weikang Shi, Renrui Zhang, Linqi Song, Mingjie Zhan, and Hongsheng Li. Mathcoder: Seamless code integration in llms for enhanced mathematical reasoning. In The Twelfth International Conference on Learning Representations. [42] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. [43] Zhangchen Xu, Yang Liu, Yueqin Yin, Mingyuan Zhou, and Radha Poovendran. Kodcode: diverse, challenging, and verifiable synthetic dataset for coding. arXiv preprint arXiv:2503.02951, 2025. [44] An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, et al. Qwen2. 5-math technical report: Toward mathematical expert model via self-improvement. arXiv preprint arXiv:2409.12122, 2024. [45] Longhui Yu, Weisen Jiang, Han Shi, YU Jincheng, Zhengying Liu, Yu Zhang, James Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. Metamath: Bootstrap your own mathematical questions for large language models. In The Twelfth International Conference on Learning Representations. [46] Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. Mammoth: Building math generalist models through hybrid instruction tuning. In The Twelfth International Conference on Learning Representations. [47] Xiang Yue, Tuney Zheng, Ge Zhang, and Wenhu Chen. Mammoth2: Scaling instructions from the web. arXiv preprint arXiv:2405.03548, 2024. [48] Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. Star: Bootstrapping reasoning with reasoning. Advances in Neural Information Processing Systems, 35:1547615488, 2022. [49] Di Zhang, Jiatong Li, Xiaoshui Huang, Dongzhan Zhou, Yuqiang Li, and Wanli Ouyang. Accessing gpt4 level mathematical olympiad solutions via monte carlo tree self-refine with llama-3 8b. arXiv preprint arXiv:2406.07394, 2024. [50] Aojun Zhou, Ke Wang, Zimu Lu, Weikang Shi, Sichun Luo, Zipeng Qin, Shaoqing Lu, Anya Jia, Linqi Song, Mingjie Zhan, et al. Solving challenging math word problems using gpt-4 code interpreter with code-based self-verification. In The Twelfth International Conference on Learning Representations. [51] Fan Zhou, Zengzhi Wang, Nikhil Ranjan, Zhoujun Cheng, Liping Tang, Guowei He, Zhengzhong Liu, and Eric Xing. Megamath: Pushing the limits of open math corpora. arXiv preprint arXiv:2504.02807, 2025. 15 Scaling Code-Assisted Chain-of-Thoughts and Instructions for Model Reasoning"
        },
        {
            "title": "A Data Generation",
            "content": "A.1 Prompts We show the prompts used for Code CoT unifying in Prompt 1 and Prompt 2, and CodeGen training and sampling in Prompt 3 and 4. We also provide the back-translation prompt for question generation in Prompt 5 and answer generation prompt in Prompt 6. prompts for problem solvability, answer correctness, the consistency between the answer and the codes chain-of-thought (CoT) evaluations are displayed in Prompt 8, 9, and 7. A.2 Model Usage We detail the models employed at each stage of our pipeline: Unifying Code CoTs: We used Qwen2.5-72B-Instruct to generate unified Code CoTs. CodeGen: The unconditional CodeGen was fine-tuned from Qwen2.5-Coder-7B. Problem Reversal & Solution Generation: Both question back-translation and answer synthesis were performed using Qwen3-8B. Evaluation: All assessments (problem solvability, answer correctness, and CoT consistency) were conducted with Qwen3-32B. A."
        },
        {
            "title": "Implementation Details",
            "content": "Focusing on Challenging Code CoT. To increase the difficulty of the dataset, we applied additional filtering to the largest subset, bigmath, of the Code CoT dataset. Based on the solve rate annotations provided with the dataset, we retained only those Code CoTs with solve rate of less than 0.3. Hyperparameters. During the Unifying Code CoT stage, we deployed Qwen2.5-72B-Instruct on 4 A100 GPUs to generate code from the raw datasets. For each sample, we performed single pass of sampling with temperature of 0.6. For training the CodeGen model, we used the LlamaFactory framework and adopted the same training configuration as in the main experiments. During inference, we sampled with temperature of 0.9 and maximum sequence length of 1024 tokens. For problem and solution generation, we followed the Qwen3-8B best practice [37]. Specifically, we used: Temperature = 0.7, TopP = 0.8, TopK = 20, MinP = 0, and enable_thinking = False. We use Qwen3-32B for evaluating problem solvability, answer correctness, and the CoT consistency between the natural language solution and code CoT. A.4 Filtering Mechanism for Code CoTs As discussed in method section, many stages of our pipeline require rigorous filtering to ensure the quality, correctness, and executability of the generated Code CoTs. Here, we formally describe the filtering criteria used throughout our work. Executability. The code must be syntactically valid and executable without raising runtime errors. This ensures basic correctness and structural integrity. Execution Efficiency. To prevent degenerate or non-terminating programs, we discard any samples that exceed 10-second execution time limit under controlled runtime environment. Minimum Code Length. To avoid trivial or underdeveloped solutions, we require that each code snippet contain at least six non-comment lines of code. This encourages minimal degree of reasoning complexity and explanatory depth. 16 Scaling Code-Assisted Chain-of-Thoughts and Instructions for Model Reasoning AST-Based Semantic Validation. Using abstract syntax tree (AST) analysis [33], we ensure that all variables declared in the input dictionary are functionally utilized in the programs logic. This discourages redundant or templated outputs and promotes semantically meaningful solutions. Output Consistency. When ground-truth answers are available, we verify that the program output exactly matches the expected solution. This check is applied in cases where reference answers are known and consistency can be reliably evaluated."
        },
        {
            "title": "B Train and Evaluation",
            "content": "B.1 Training Setup Model training was conducted using the LLaMA Factory 2 framework on 8 NVIDIA A100 GPUs. All models were trained for 3 epoch with batch size of 128. We used the AdamW optimizer [22] with learning rate of 5 106, cosine learning rate decay, and warm-up ratio of 0.03. The maximum sequence length (cutoff) was set to 4096, and the weight decay was 0.1. The prompt used for training is shown in Prompt 10. B.2 Evaluation Setup All models were evaluated using unified framework 3 under the zero-shot setting. We used greedy decoding with maximum generation length of 2048 tokens. The prompt used for evaluation is shown in Prompt 11. B.3 Evaluation Benchmarks The following datasets are used for evaluation: MATH [14]: benchmark of 12,500 high school math competition problems, with 7,500 for training and 5,000 for testing. Problems are categorized into 7 topics (Prealgebra, Intermediate Algebra, Algebra, Precalculus, Geometry, Counting & Probability, and Number Theory) and 5 difficulty levels. GSM8K [6]: This dataset contains 8,792 high-quality grade school math word problems, with 7,473 for training and 1,319 for testing. Each problem typically requires 2 to 8 reasoning steps to solve. CollegeMath [34]: test set containing 2,818 college-level math problems collected from 9 college textbooks, covering 7 core subjects: Algebra, Precalculus, Calculus, Vector Calculus, Probability, Linear Algebra, and Differential Equations. DeepMind-Mathematics [31]: This test set consists of 1,000 problems covering wide range of mathematical reasoning tasks including algebra, arithmetic, calculus, and probability. It is designed to assess the mathematical reasoning abilities of models. OlympiadBench-Math [13]: benchmark of 675 Olympiad-level math problems. We evaluate only on the English text-only subset of OlympiadBench. TheoremQA [5]: theorem-driven question-answering benchmark containing 800 problems grounded in 350 domain-specific theorems. It evaluates models ability to apply mathematical and scientific theorems across disciplines such as mathematics, physics, electrical engineering, computer science, and finance. 2https://github.com/hiyouga/LLaMA-Factory 3https://github.com/ZubinGou/math-evaluation-harness/tree/main 17 Scaling Code-Assisted Chain-of-Thoughts and Instructions for Model Reasoning"
        },
        {
            "title": "C Additional Experiments",
            "content": "C.1 Distinguishing Caco from Teacher Knowledge Transfer and STaR-style Self-"
        },
        {
            "title": "Improvement",
            "content": "A natural concern is whether Cacos performance gains stem primarily from knowledge transfer from the large teacher model (Qwen-2.5-72B-Instruct) used to generate the seed dataset, rather than from the Caco procedure itself. To isolate this factor, we conducted control experiment in which the same teacher model was used to directly produce natural language Chain-of-Thought (CoT) answers for the same seed questions, resulting in 300K QA dataset (QWEN72B-SEED-DISTILLED). We compared models fine-tuned on this dataset to those trained on 300K subset of Caco (Caco-300K) and the full Caco-1.3M. Even at equal data size, Caco outperformed the distilled baseline (e.g., 66.2 vs. 65.5 AVG for Qwen-7B), and scaling Caco to 1.3M samples yielded further improvements (up to 67.7 AVG). This suggests that prompt and reasoning diversity, enabled by Cacos code-based augmentation, provides benefits beyond direct teacher distillation, and that Caco scales more effectively. Another question is whether the gains could also be achieved by simpler self-improvement methods such as STaR [48]. Conceptually, Caco differs from these in that it does not focus on iteratively refining answers to fixed set of questions; instead, it trains dedicated code generator to produce executable CoTs from scratch, enabling scalable, verifiable creation of new problems. Nevertheless, to provide direct comparison, we implemented single iteration of STaR-style self-improvement on the seed dataset, generating multiple CoTs per seed question, filtering for correctness, and sampling 300K verified solutions (SEED-SELF-IMPROVE). Across both DeepSeek-Math-7B and Qwen-7B backbones, Caco-300K consistently outperformed SEED-SELF-IMPROVE by substantial margins (e.g., 53.2 vs. 48.9 AVG for DS-7B, and 66.2 vs. 52.6 for Qwen-7B). These results reinforce that Cacos improvements derive from its code-driven, diversity-oriented generation process, rather than simply inheriting knowledge from stronger teacher or applying standard self-improvement on seed data. Model #Samples MATH GSM8K College DM Olympiad TheoremQA AVG Qwen-7B-Seed-self-improve Qwen-7B-Qwen72B-Seed-distilled Qwen-7B-Caco-300K Qwen-7B-Caco-1.3M DS-7B-Seed-self-improve DS-7B-Qwen72B-Seed-distilled DS-7B-Caco-300K DS-7B-Caco-1.3M 300K 300K 300K 1.3M 300K 300K 300K 1.3M 70.7 79.0 81.6 82.4 53.1 57.4 61.8 68.2 83.0 91.2 92.4 92.6 86.7 83.0 83.2 85. 47.1 52.1 51.2 51.4 41.6 42.4 43.3 46.0 47.6 84.4 84.8 87.1 62.5 69.0 76.0 80.2 39.0 41.3 42.5 46.5 19.3 23.4 23.9 29. 28.2 45.0 44.9 46.0 30.2 31.4 31.1 33.8 52.6 65.5 66.2 67.7 48.9 51.1 53.2 57.1 Table 4: Control experiment comparing teacher-distilled natural language CoTs vs. Caco-generated data at matched size (300K) and at scale (1.3M). Bold indicates the best within each student block."
        },
        {
            "title": "D Computational Cost and Efficiency",
            "content": "To quantify the efficiency of our method, we report the full computational cost of generating the Caco-1.3M dataset  (Table 5)  . All experiments were conducted on single machine equipped with 8 NVIDIA A100 GPUs. The pipeline consists of four main stages: unifying Code CoTs (339K samples, 2h), scaling Code CoTs (5.3M samples, 8h), question reversal (4.6M samples, 6h), and answer generation (4.6M samples, 38.5h), totaling approximately 55 hours to produce 1.3M verified samples. Importantly, the entire process relies solely on open-source models, avoiding the substantial cost of proprietary API usage. From cost breakdown perspective, the majority of the runtime is consumed by the answer generation stage, which is unavoidable in any instruction tuning or self-improvement setup. For example, prior 18 Scaling Code-Assisted Chain-of-Thoughts and Instructions for Model Reasoning"
        },
        {
            "title": "Stage",
            "content": "#Samples Time (Hours)"
        },
        {
            "title": "Unifying Code CoT\nScaling Code CoT\nQuestion Reversal\nAnswer Generation",
            "content": "Total (for 1.3M valid data) 339K 5.3M 4.6M 4.6M 2h 8h 5h 40h 55h Table 5: Computation time for each stage in generating the CACO-1.3M dataset on single 8A100 machine. works such as DartMath [38] also incur comparable or higher costs in solution generation, particularly when sampling multiple candidate answers per prompt. The additional steps specific to CacoCode CoT generation and question reversalare lightweight (combined 16h), as natural language solutions are substantially longer than questions or Code CoTs. Overall, these results demonstrate that Caco can generate over one million verified, diverse reasoning samples in under three days on single 8-GPU node, highlighting its strong scalability and accessibility. While we acknowledge that data-efficient methods have their merits, Caco is designed with complementary focus: producing large-scale, diverse, and verifiable reasoning data to support cross-domain generalization."
        },
        {
            "title": "E Limitations",
            "content": "Although Caco demonstrates strong capabilities in generating diverse reasoning paths and instructions, its performance is still limited by the predefined problem types used during training. The system may struggle when faced with highly innovative or unconventional problems, particularly those that do not align with the templates or problem categories used during training. As result, generating high-quality code-based CoTs for more complex or uncommon problem types remains challenge, potentially leading to biases in the distribution of generated data. Additionally, while the code can be executed accurately, converting it back into human-readable natural language instructions may result in the loss of some details or require simplification, causing the final output to be less rich or specific than the original reasoning steps. Furthermore, the generated code is primarily used for filtering data and not for final training purposes. It helps ensure the correctness and consistency of the reasoning process, but does not directly contribute to the final training dataset. In future work, it will be essential to explore how the generated code can be used to further improve the quality of the data and enhance the training process. Currently, Cacos application scope is focused mainly on mathematical and algorithmic reasoning tasks. Future work will need to explore extending it to broader domains, such as logical puzzles or STEM problem solving, which will require further effort."
        },
        {
            "title": "F Future Works",
            "content": "We outline three complementary directions: increasing difficulty, expanding diversity, and leveraging Caco in reinforcement learning (RL). 19 Scaling Code-Assisted Chain-of-Thoughts and Instructions for Model Reasoning Raising Difficulty. Since the completion of this work, several math corpora with higher difficulty and quality than DEEPSCALER and BIGMATH have emerged, such as AM-THINKING-V1-DISTILLED and the DAPO dataset. Starting from harder, cleaner seed sets is likely to further amplify the benefits of code-based augmentation. Concretely, we plan to (i) replace/augment the seed pool with highdifficulty problems (e.g., Olympiad-style, exam-grade items) and (ii) adopt hardness-aware sampling and adversarial program mutations during Code CoT generation. Expanding Diversity. As demonstrated in Section 5.4, our method generalizes beyond mathematics and applies naturally to domains with logical, symbolic, or procedural structure. While we discussed science and logic reasoning, broader coverage (e.g., data reasoning, procedural planning, code debugging, diagram/physics problems, proofs) should allow CodeGen to learn richer templates and compose more diverse problems. Furthermore, extending the framework beyond Python to support formal languages (e.g., Lean, Coq, or Wolfram Language) could enhance rigor and verifiability. We will (i) train multi-domain CodeGen with domain tags, (ii) design compositional templates that factor shared subroutines across domains. Applications: Reinforcement Learning with Verifiable Rewards (RLVR). Recent RL-based training has shown strong gains for reasoning models but often depends critically on the correctness of reference answers. Cacos executable traces provide natural, low-noise reward signal. We will integrate Caco with RL by (i) deriving rewards from execution correctness (ii) employing curriculum over program length and control-flow complexity. This combination targets scalable, verifiable RL training without heavy reliance on noisy external references."
        },
        {
            "title": "G More Cases",
            "content": "This section presents samples from the Caco dataset, including the subsequence counting problem (Case 1), geometric sequences problem (Case 2), permutation and combination problem (Case 3), mathematical expression calculation problem (Case 4), and analytical geometry problem (Case 5). 20 Scaling Code-Assisted Chain-of-Thoughts and Instructions for Model Reasoning"
        },
        {
            "title": "Code Generation Prompt",
            "content": "Given the following math problem in natural language, provide the complete code solution that solves the problem. Requirements: The final output of the program **must be the correct numerical or symbolic answer** to the problem. You must actually **compute** the result using Python code (e.g., using arithmetic or libraries like sympy), **not just explain in text or comments**. The code must define an input dictionary, call function using that input, assign the result to variable output, and finally print(output). Please provide complete, standalone executable script. ### Example Math Problem: snail is at the bottom of 20-foot well. Each day, it climbs up 3 feet, but at night, it slips back 2 feet. How many days will it take for the snail to reach the top of the well? ### Example Code Solution: def days_to_reach_top ( well_height , climb_distance , slip_distance ): days = 0 current_height = 0 while current_height < well_height : current_height += climb_distance if current_height >= well_height : break current_height -= slip_distance days += 1 return days + 1 # Represent the input as dictionary named ' input ' input = {{ \" well_height \": 20 , \" climb_distance \": 3, \" slip_distance \": 2}} # Call the function with the input dictionary , assign the result to ' output ' output = days_to_reach_top (** input ) # Print the output print ( output ) Now, please provide the code solution for the following math problem directly. Make sure your code solution defines the input as dictionary named input, calls the solution function using this dictionary, stores the result in variable named output, and prints output. ### Math Problem: {problem} ### Solution (Optional): {solution} ### Code Solution: Prompt 1: Code Generation Prompt for solving math problem using Python code. 21 Scaling Code-Assisted Chain-of-Thoughts and Instructions for Model Reasoning"
        },
        {
            "title": "Code Unifying Prompt",
            "content": "Given the following code and test function, please refactor the solution into the required format: ### Example Output Format: def add (a , b): return + # Represent the input as dictionary named ' input ' input = {{ \"a\": 3, \"b\": 5}} # Call the function with the input dictionary , assign the result to ' output ' output = add (** input ) # Print the output print ( output ) <answer>8</answer> Code: {code} Test Function: {test_code} Please refactor the code to follow the required format. The code must define an input dictionary, call function using that input, assign the result to variable output, and finally print(output). If there are multiple test cases in test function, just select one of them. Please provide complete, standalone executable script. ### Output: Prompt 2: Prompt for refactoring code into the required input-output format."
        },
        {
            "title": "CodeGen Training Prompt",
            "content": "<im_start>system You are helpful assistant.<im_end> <im_start>user {code}<im_end> Prompt 3: Prompt for training the CodeGen model."
        },
        {
            "title": "CodeGen Sampling Prompt",
            "content": "<im_start>system You are helpful assistant.<im_end> <im_start>user Prompt 4: Prompt for sampling from the trained CodeGen model. 22 Scaling Code-Assisted Chain-of-Thoughts and Instructions for Model Reasoning Question Back-translation Prompt The code represents solution to math problem, and your task is to generate the original math problem that corresponds to the code. ### Example Code: def change_ref ( amt , coins ): if amt <= 0: return 0 if amt != 0 and not coins : return float ( \" inf \" ) elif coins [0] > amt : return change_ref ( amt , coins [1:]) else : use_it = 1 + change_ref ( amt - coins [0] , coins ) lose_it = change_ref ( amt , coins [1:]) return min ( use_it , lose_it ) # Represent the input as dictionary named ' input ' input = {\" amt \": 13 , \" coins \": [1 , 3, 5, 7]} # Call the function with the input dictionary , assign the result to ' output ' output = change_ref (** input ) # Print the output print ( output ) ### Example Math Problem: What is the minimum number of coins needed to make total of 13 units using the available coin denominations of 1, 3, 5, and 7 units, each in unlimited supply? ### End Problem Please generate **Math Problem** based on the following code. Ensure the generated problem is fully self-contained, solvable, and doesnt miss any necessary conditions or context. You may add concrete scenario or express the problem in different styles for diversity. ### Code: {code} ### Math Problem: Prompt 5: Question Back-translation Prompt. The prompt for generating math problem based on given code solution, where the generated problem should fully capture the conditions and context of the code."
        },
        {
            "title": "Answer Generation Prompt",
            "content": "### Instruction: {problem}. Please reason step by step, and put your final answer within boxed{}. ### Response: Prompt 6: Instructions for generating step-by-step reasoning and the final answer enclosed in boxed format. 23 Scaling Code-Assisted Chain-of-Thoughts and Instructions for Model Reasoning"
        },
        {
            "title": "Consistency Checking Prompt",
            "content": "Solution: {solution} Code: {code} Please determine if the logic of the code and the chain-of-thought in the solution are consistent. Answer with single word: \"Yes\" or \"No\". Answer: Prompt 7: Prompt for checking the consistency between the logic of the code and the chain-of-thought in the solution, where the answer is expected to be either \"Yes\" or \"No\"."
        },
        {
            "title": "Solvability Checking Prompt",
            "content": "Problem: {problem} Please determine if the problem is solvable. Answer with single word: \"Yes\" or \"No\". Answer: Prompt 8: Prompt for determining the solvability of given problem, where the answer is expected to be either \"Yes\" or \"No\"."
        },
        {
            "title": "Correctness Checking Prompt",
            "content": "Problem: {problem} Solution: {solution} Please determine if the chain-of-thought of solution is correct. Answer with single word: \"Yes\" or \"No\". Answer: Prompt 9: Prompt for verifying the correctness of the chain-of-thought in given solution, where the answer is expected to be either \"Yes\" or \"No\"."
        },
        {
            "title": "Training Prompt",
            "content": "### Instruction: {question} ### Response: {solution} Prompt 10: Training prompt of Caco model and baseline. 24 Scaling Code-Assisted Chain-of-Thoughts and Instructions for Model Reasoning"
        },
        {
            "title": "Evaluation Prompt",
            "content": "### Instruction: {question} ### Response: Lets think step by step. Prompt 11: Evaluation prompt of Caco model and baseline with step-by-step reasoning. 25 Scaling Code-Assisted Chain-of-Thoughts and Instructions for Model Reasoning"
        },
        {
            "title": "Subsequence Counting Problem",
            "content": "Problem: How many unique subsequences can be formed from the list of numbers [1, 2, 3], where subsequence is defined as any sequence that can be derived by deleting some or no elements from the list without changing the order of the remaining elements? Code: def days_to_reach_top ( well_height , climb_distance , slip_distance ): days = 0 current_height = 0 while current_height < well_height : current_height += climb_distance if current_height >= well_height : break current_height -= slip_distance days += 1 return days + 1 # Represent the input as dictionary named ' input ' input = {{ \" well_height \": 20 , \" climb_distance \": 3, \" slip_distance \": 2}} # Call the function with the input dictionary , assign the result to ' output ' output = days_to_reach_top (** input ) # Print the output print ( output ) Solution: We are asked to find the number of **unique subsequences** that can be formed from the list of numbers: [1, 2, 3] ### Step 1: Understand what subsequence is **subsequence** is any sequence that can be derived by **deleting some or no elements** from the original list, **without changing the order** of the remaining elements. For example, from [1, 2, 3], valid subsequences include: [1], [2], [3], [1, 2], [1, 3], [2, 3], [1, 2, 3] Also the empty subsequence (which is often considered, depending on the problems context) ### Step 2: Count all possible subsequences Each element in the list can either be **included** or **excluded** in subsequence. So for list of length n, there are 2n total possible subsequences (including the empty subsequence). Here, the list has 3 elements, so the total number of subsequences is: 23 = 8 ### Step 3: List all the subsequences to verify Lets list all 8 possible subsequences of [1, 2, 3]: [] (empty subsequence), [1], [2], [3], [1, 2], [1, 3], [2, 3], [1, 2, 3] These are all the possible subsequences. All of them are **unique** because the elements are all distinct, and the order is preserved. ### Final Answer: There are **8 unique subsequences** that can be formed from the list [1, 2, 3]. 8 Case 1: Subsequence Counting Problem: Counting the number of unique subsequences formed from list of numbers. 26 Scaling Code-Assisted Chain-of-Thoughts and Instructions for Model Reasoning"
        },
        {
            "title": "Geometric Sequence Problem",
            "content": "Problem: If the real numbers 1, a, b, c, 9 form geometric sequence, then = ______. Code: from sympy import symbols , solve def find_b (a , c): = symbols ( 'b ') # Since , , form geometric sequence , ^2 = ac equation = **2 - a*c solutions = solve ( equation , b) # We need to determine the correct sign of if solutions [0] * solutions [1] < 0: # If the product is negative , one of the solutions is negative # and the other is positive . We choose the one that is consistent # with the sequence -1, , , , -9 being decreasing . if solutions [0] < 0: return solutions [0] else : return solutions [1] else : return solutions [0] input = {\"a\": -1, \"c\": -9} output = find_b (** input ) print ( output ) Solution: We are given that the real numbers 1, a, b, c, 9 form **geometric sequence**. We are to find the value of b. ### Step 1: Understand the structure of geometric sequence In geometric sequence, each term is obtained by multiplying the previous term by constant ratio r. So, if the sequence is: Then: - = 1 - = = (1 r) = r2 - = = r2 = r3 - 9 = = r3 = r4 ### Step 2: Use the last term to solve for We are told the last term is 9, so: Divide both sides by 1: Now solve for r: Note that 4 9 = (cid:112) 9 = 3, so: ### Step 3: Find = r2 Recall that: Now compute r2 for both values of r: - If = - If = So in both cases: 3, then r2 = ( 3, then r2 = ( 3)2 = 3 3)2 = 3 ### Final Answer: 1, a, b, c, 9 r4 = 9 r4 = 9 = 4 9 3 = = r2 = r2 ="
        },
        {
            "title": "27\nCase 2: Geometric Sequence Problem: Solving for b in a geometric sequence with given terms.",
            "content": "Scaling Code-Assisted Chain-of-Thoughts and Instructions for Model Reasoning"
        },
        {
            "title": "Counting and Combinatorics",
            "content": "Problem: Using the digits 1 and 3 to form four-digit number, and both digits 1 and 3 must appear at least once, there are total of _______ such four-digit numbers. Code: from itertools import product def count_four_digit_numbers () : count = 0 # Generate all 4digit combinations of 1 and 3 for combo in product ([1 , 3] , repeat =4) : # Ensure both 1 and 3 appear at least once if 1 in combo and 3 in combo : count += return count # Represent the input as dictionary named ' input ' input = {} # Call the function with the input dictionary , assign the result to ' output ' output = count_four_digit_numbers (** input ) # Print the output print ( output ) Solution: We are asked to find how many **four-digit numbers** can be formed using only the digits **1** and **3**, such that **both digits 1 and 3 appear at least once**. ### Step 1: Total number of four-digit numbers using digits 1 and 3 Each of the 4 positions in the number can be filled with either **1** or **3**, so: Total combinations = 24 = 16 So, there are **16** total four-digit numbers that can be formed using only digits 1 and 3. ### Step 2: Subtract the cases where only one digit is used We need to exclude the numbers that **do not contain both digits**. That is, the cases where: - All digits are **1** number: 1111 - All digits are **3** number: 3333 So, there are **2** such numbers that we need to exclude. ### Step 3: Compute the valid numbers Valid numbers = Total combinations Invalid combinations ### Final Answer: = 16 2 = 14 14 There are **14** four-digit numbers that can be formed using the digits 1 and 3, with both digits appearing at least once. Case 3: Counting and Combinatorics problem: Counting the number of valid four-digit numbers formed by the digits 1 and 3 where both digits appear at least once. The table includes the problem description, step-by-step solution, and corresponding Python code to calculate the number of valid combinations. Scaling Code-Assisted Chain-of-Thoughts and Instructions for Model Reasoning"
        },
        {
            "title": "Mathematical Expression Calculation",
            "content": "Problem: Calculate: Code: import math 12 + 1 3 + (π 2023)0 def calculate_expression () : sqrt_12 = math . sqrt (12) abs_value = abs (1 - math . sqrt (3) ) power_0 = ( math . pi - 2023) ** 0 result = sqrt_12 + abs_value + power_0 return result # Represent the input as dictionary named ' input ' input = {} # Call the function with the input dictionary , assign the result to ' output ' output = calculate_expression (** input ) # Print the output print ( output ) Solution: We are asked to calculate the expression: Lets break it down step by step. ### Step 1: Simplify We can simplify the square root: ### Step 2: Simplify 1 We know that 3 3 1.732, so: 12 + 1 3 + (π 2023)0 12 = 4 3 = 4 3 = 2 1 3 1 1.732 = 0.732 Since the result is negative, the absolute value is: 3 = 3 1 1 ### Step 3: Simplify (π 2023)0 Any non-zero number raised to the power of 0 is 1. Since π 3.1416, and π 2023 = 0, we have: ### Step 4: Add all the simplified parts Now, add the three parts together: 12 + 1 Simplify: ### Final Answer: (π 2023)0 = 1 3 + (π 2023)0 = 3 + ( 3 1) + 1 3 + 3 1 + 1 = 3 3 3 3 Case 4: Mathematical Expression Calculation: Solving the expression involving square roots, absolute values, and exponentiation. The table provides detailed breakdown of the problem-solving steps and the corresponding Python code implementation. 29 Scaling Code-Assisted Chain-of-Thoughts and Instructions for Model Reasoning"
        },
        {
            "title": "Tangency Condition of Line and Parabola",
            "content": "Problem: Given the line 1 = 0 and the parabola = ax2 are tangent to each other, find the value of a. Code: import sympy as sp def find_tangent_coefficient () : , , = sp . symbols ( 'xya ') line_eq = - - 1 parabola_eq = - a*x **2 # Substitute from the line equation into the parabola equation substituted_eq = parabola_eq . subs (y , - 1) # Simplify the equation simplified_eq = sp . simplify ( substituted_eq ) # For the line and parabola to be tangent , the quadratic equation must have exactly one solution . discriminant = sp . discriminant ( simplified_eq , ) # Solve for a_value = sp . solve ( discriminant , a) return a_value [0] # Represent the input as dictionary named ' input ' input = {} # Call the function with the input dictionary , assign the result to ' output ' output = find_tangent_coefficient (** input ) # Print the output print ( output ) Solution: We are given: - **line**: 1 = 0 - **parabola**: = ax2 We are told that the **line and the parabola are tangent to each other**, which means they intersect at **exactly one point** (i.e., the system has exactly one solution). ### Step 1: Solve the system of equations We are given: 1. 1 = 0 = 1 2. = ax2 Substitute the expression for from the line into the parabola: Rewriting: This is **quadratic equation** in x: 1 = ax2 ax2 + 1 = ax2 + 1 = 0 ### Step 2: Use the condition for tangency For the line and the parabola to be **tangent**, the quadratic equation must have **exactly one solution**. That means the **discriminant** of the quadratic must be zero. Recall that for quadratic ax2 + bx + = 0, the discriminant is: In our case, the quadratic is: So: = a, = 1, = 1 Now compute the discriminant: = b2 4ac ax2 + 1 = Set the discriminant equal to zero (for tangency): = (1)2 4(a)(1) = 1 4a Solve for a: ### Final Answer: 1 4a = 0 4a = 1 = 1 4 1 4 30 Case 5: Tangency Condition of Line and Parabola: Finding the value of for the line and parabola to be tangent."
        }
    ],
    "affiliations": [
        "OpenDataLab, Shanghai Artificial Intelligence Laboratory",
        "Shanghai Jiao Tong University",
        "Soochow University"
    ]
}