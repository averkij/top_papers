{
    "paper_title": "AdamMeme: Adaptively Probe the Reasoning Capacity of Multimodal Large Language Models on Harmfulness",
    "authors": [
        "Zixin Chen",
        "Hongzhan Lin",
        "Kaixin Li",
        "Ziyang Luo",
        "Zhen Ye",
        "Guang Chen",
        "Zhiyong Huang",
        "Jing Ma"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The proliferation of multimodal memes in the social media era demands that multimodal Large Language Models (mLLMs) effectively understand meme harmfulness. Existing benchmarks for assessing mLLMs on harmful meme understanding rely on accuracy-based, model-agnostic evaluations using static datasets. These benchmarks are limited in their ability to provide up-to-date and thorough assessments, as online memes evolve dynamically. To address this, we propose AdamMeme, a flexible, agent-based evaluation framework that adaptively probes the reasoning capabilities of mLLMs in deciphering meme harmfulness. Through multi-agent collaboration, AdamMeme provides comprehensive evaluations by iteratively updating the meme data with challenging samples, thereby exposing specific limitations in how mLLMs interpret harmfulness. Extensive experiments show that our framework systematically reveals the varying performance of different target mLLMs, offering in-depth, fine-grained analyses of model-specific weaknesses. Our code is available at https://github.com/Lbotirx/AdamMeme."
        },
        {
            "title": "Start",
            "content": "AdamMeme: Adaptively Probe the Reasoning Capacity of Multimodal Large Language Models on Harmfulness Zixin Chen, Hongzhan Lin*, Kaixin Li, Ziyang Luo, Zhen Ye, Guang Chen, Zhiyong Huang, Jing Ma BUPT HKBU NUS HKUST {mailboxforvicky}@bupt.edu.cn, {cshzlin,majing}@comp.hkbu.edu.hk 5 2 0 2 2 ] . [ 1 2 0 7 1 0 . 7 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "The proliferation of multimodal memes in the social media era demands that multimodal Large Language Models (mLLMs) effectively understand meme harmfulness. Existing benchmarks for assessing mLLMs on harmful meme understanding rely on accuracybased, model-agnostic evaluations using static datasets. These benchmarks are limited in their ability to provide up-to-date and thorough assessments, as online memes evolve dynamically. To address this, we propose AdamMeme, flexible, agent-based evaluation framework that adaptively probes the reasoning capabilities of mLLMs in deciphering meme harmfulness. Through multi-agent collaboration, AdamMeme provides comprehensive evaluations by iteratively updating the meme data with challenging samples, thereby exposing specific limitations in how mLLMs interpret harmfulness. Extensive experiments show that our framework systematically reveals the varying performance of different target mLLMs, offering in-depth, fine-grained analyses of modelspecific weaknesses. Our code is available at https://github.com/Lbotirx/AdamMeme."
        },
        {
            "title": "Introduction",
            "content": "The growth of social media has fostered the emergence of new multimodal entity: the meme. Multimodal memes typically combine visual elements with concise text, making them easily shareable and capable of spreading rapidly across diverse online platforms. While often perceived as humorous or sarcastic (Hessel et al., 2023; Chen et al., 2024), memes can also serve as tools of harm when the multimodal nature is strategically employed to exploit political or socio-cultural divisions. widely accepted definition of harmful memes1 is multimodal units consisting of an image and * Corresponding authors. 1Disclaimer: This paper contains content that may be disturbing to some readers. Figure 1: An overview of existing solutions and our proposed AdamMeme in the evaluation of harmful meme understanding for multimodal Large Language Models. embedded text that have the potential to cause harm to an individual, an organization, community, or society in general (Sharma et al., 2022). Considering the rich background knowledge stored in multimodal Large Language Models (mLLMs), prior studies (Lin et al., 2023a; Cao et al., 2023; Kumari et al., 2024; Lin et al., 2024a) have been increasingly assisted by mLLMs to detect meme-based social abuse (Kiela et al., 2020; Pramanick et al., 2021). This growing adoption has driven research towards the systematic evaluation of mLLMs inherent reasoning capacity in the context of meme harmfulness, to facilitate future applications on online safety. Existing solutions (Lin et al., 2024b; Cao et al., 2024) typically collected static meme data, to audit and reveal the reasoning capabilities of mLLMs in discerning meme-based social abuse, with simple binary classification manner. However, as shown in Figure 1, such static evaluations that focus solely on the superficial accuracy performance are constrained by infrequent updates, data leakage, and leaderboard swamping, reducing their effectiveness for comprehensive mLLM assessments. This is especially problematic given the dynamic evolving nature of emerging memes (Huang et al., 2024) conveyed with intentionally obscure harmfulness on social media. To address these challenges in evaluating mLLMs capabilities of harmful meme understanding, in this paper, we aim to design more flexible and comprehensive evaluation framework based on the following two key points: 1) The framework should be capable of conducting the mLLM audit with dynamically refreshed meme data. Due to the ever-changing evolution of memes, continuously annotating and creating new benchmarks can be costly and inefficient. We aim to develop dynamic evaluation method that eliminates the need for additional human annotations for harmful memes, enabling effective model assessments using adaptively updated meme data. 2) The framework should facilitate model-centric evaluation of the mLLMs reasoning capacity for harmful meme understanding. While previous work (Lin et al., 2024b) used detection accuracy as primary metric to assess models, such static benchmark work was typically model-agnostic and insufficient for thoroughly evaluating mLLMs comprehension of harmful memes. Since the mLLMs inherently generate open-form content, we aim to assess target mLLMs based on the model-generated responses. To this end, we introduce novel evaluation framework AdamMeme, which Adaptively probes the reasoning capacity of mLLMs on Meme harmfulness. As illustrated in Figure 1, we resort to model-centric evaluation method, leveraging multimodal autonomous agents for dynamic assessment by iteratively generating hard meme samples specific to the target mLLM. Specifically, our framework includes three stages: 1) Harmfulness Mining: We first employ the agent controller as the miner agents to establish dynamically-updated taxonomy, discerning different types of harmfulness in raw memes into categories. 2) Model Scoring: Then for each harmfulness type, AdamMeme deploys the scoring agent to evaluate the target mLLMs performance in conducting harmfulness analysis for the memes. 3) Iterative Refinement: Based on the performances of the target mLLM after the initial scoring, refinement agent is devised to create more challenging test samples by modifying the textual elements in memes, targeting at exposing model-specific weaknesses in the target mLLMs understanding of harmfulness. The modified memes are then used to repeat the evaluation process for creating an adaptive evaluation loop in the harmfulness understanding of memes. Our contribution can be summarized as follows: To our best knowledge, we are the first to evaluate mLLMs ability to understand harmful memes from model-centric, analytical perspective. We focus on their reasoning abilities to discern nuanced harmfulness across diverse contexts. We present AdamMeme, novel evaluation framework that uses agent-based interaction to dynamically uncover trustworthiness limitations of mLLMs in understanding harmfulness. The framework is adaptable to the evolving, multimodal nature of memes, and promotes diversity in mLLM evaluation beyond binary accuracy. Our experimental results demonstrate that the target mLLMs exhibit varying strengths and weaknesses across different aspects of harmfulness. AdamMeme successfully reveals the vulnerabilities of various target mLLMs, providing insightful, fine-grained analysis of their reasoning capabilities in harmful meme understanding."
        },
        {
            "title": "2 AdamMeme",
            "content": "2.1 Overview Problem statement Harmful meme understanding focuses on deciphering and explaining harmful content in memes. Our goal is to develop an adaptive agent-based evaluation that dynamically explores the capacity of the target mLLM to recognize and interpret the harmfulness of memes. Given an unlabeled meme set without any annotations, our proposed evaluation framework, AdamMeme, is to identify the target mLLMs specific limitations on various aspects related to harmfulness: = AdamMeme(α, M), (1) where α means the target mLLM, and denotes the detailed evaluation analysis indicating αs overall capabilities in harmful meme understanding. Due to the intrinsic complexity of memes, models can be easily influenced by the nuanced expressions in meme contents, making it difficult to recognize the true knowledge boundaries based solely on their performance with static memes. Our core idea is to reveal the models weaknesses by continuously modifying the content of memes according to model performances, creating harder cases to test whether the model can steadily decipher the inherent harmfulness under varying superFigure 2: The pipeline of our framework. In harmfulness mining, we formulate taxonomy that generalizes memes into several harmfulness categories. Then we employ model scoring and iterative refinement separately by categories, to first assess the target mLLMs capability in analyzing memes, and iteratively create challenging samples based on the models historical performance to expose model-specific weaknesses in deeper understanding of harmfulness. ficial expressions. An overview of our proposed AdamMeme framework is shown in Figure 2, including: 1) Harmfulness Mining (2.2), 2) Model Scoring (2.3) and 3) Iterative Refinement (2.4). 2.2 Harmfulness Mining Harmfulness can be conveyed by memes through various forms, making effective interpretation of these multimodal harmful contents dependent on the target mLLMs ability to understand different types of background knowledge (Hee et al., 2024), which can vary significantly across multiple dimensions such as race, gender, religion, etc (Pramanick et al., 2021). To address this challenge, in this section, we focus on the mining harmfulness in the raw meme data by formulating taxonomy that categorizes harmful memes into distinct types of harmfulness, allowing for structured and comprehensive analysis of these diverse aspects. Therefore, we deploy three kinds of agents to perform the harmfulness mining stage as well as ensuring the reliability of the taxonomy: 1) the Miner role to discern harmfulness categories in memes, 2) the Examiner role and the Judge role to confirm the existence and validity of harmfulness categories based on the meme and taxonomy contents, respectively, and 3) the Narrator role to generate explanations of memes on specific harmfulness categories. Formally defined as collection of harmfulness categories, taxonomy is denoted as = {c1, c2, . . . , cn}, where each element indicates harmfulness category. serves as reference for the Miners to recognize harmfulness in each meme according to these categories. To start with, we first initialize with basic yet representative categories, which can be dynamically updated by appending new categories during mining. Specifically, we establish the initial taxonomy by drawing inspiration from previous literature (Cao et al., 2023), which includes the following six classic aspects: Race, Gender, Religion, Nationality, Disability, Animal. Given multimodal meme = (i, t) consisting of meme image embedded with meme text t, Miner agent nr is instructed to assign the meme into one or more harmfulness categories of the current taxonomy . To ensure the reliability of this step, we employ the majority vote strategy with 3 Miners of the same agent role: [c1, . . . ] = Vote(M nr1(m, ), ..., nr3(m, )). (2) Each nr provides list of categories in , where each decision in the list is valid only when more than half of Miners vote for it. If the meme is considered harmless, nr then returns an empty list, indicating no harmfulness in the current meme. Each meme can be analyzed into more than one category, since there could be multiple harmful risks within meme. During the process, Miner agents can raise new categories if the meme contains harmful risk that does not match with any category in the existing taxonomy . In order to retain the taxonomy logical and reasonable, when the Miner discovers new type of harmfulness category cnew, the Examiner and the Judge roles will act to check if the new raised category cnew is properly suggested from the perspectives of the meme and the taxonomy . Specifically, the Examiner agent is responsible for examining the correctness of the newly discovered category, to make sure that such harmfulness indeed exists in m. On the other hand, the Judge agent is tasked with evaluating whether the granularity and content of new categories are suitable for inclusion in the current taxonomy, ensuring that the taxonomy is maintainable during potential updates. If both agents respond positively, the current taxonomy can be updated into the new one : = + Judge(cnew, ) Examiner(cnew, m). (3) Besides mining memes to analyze the inherent harmfulness, we further investigate the underlying reasons behind their harmful nature. To achieve this, we introduce Narrator agent to generate concise misbelief statement, denoted as misb, for each mined meme-category pair (m, c): misb = Narrator(m, c). (4) The misbelief statement misb is natural language sentence that explicitly reveals generalized false belief about what makes the meme intentionally harmful within the harmfulness category c, yet instead of obsession with specific harmful meme. After the harmfulness mining stage, each sample in the mined set is denoted as: (i, t, c, misb). By incorporating harmfulness categories and misbelief statements, we present meme harmfulness from holistic and finer-grained perspectives, with misbeliefs providing detailed information that distinguishes memes within the same category. This approach enables deeper exploration of the target models specific weaknesses, allowing for more systematic and focused analysis of harmfulness. 2.3 Model Scoring After analyzing the harmfulness categorization of memes, we evaluate the target model automatically on harmfulness understanding. We accomplish the model scoring stage via specially designed mLLM-as-a-Judge mechanism, by drawing the practice from previous reference-based scoring work (Zheng et al., 2023). Considering the complexity and subtlety involved in deciphering memes, we propose wisdom-of-crowds strategy, enhancing the reliability of the reference answers by taking multiple candidates into account. Specifically, given pre-processed mined sample (i, t, c, misb) after the harmfulness mining, in this scoring stage, three agents are first prompted to decode the meme with respect to potential harmful risks on c, and generate set of candidate answers (ans1, ..., ans3). We then deploy an agent as the senior role, to summarize the best answer among these candidates based on their quality in analyzing meme on harmfulness class c. If none of the candidates is reasonable, the senior agent for reference generation will sum up the issues and generate justifiable response as the final reference answer: ansref = Summarize(ans1, ..., ans3i, t, c). (5) Meanwhile, the target model will also be evaluated to generate its response anstarget to analyze the meme harmfulness. Scorer agent then grades the target answer with score [1, 10] Z: (6) = Scorer(ansref , anstargeti, t, c). After that, each scored sample is denoted as (i, t, c, misb, s), and the final scored sample set is denoted as Mscored. The collaboration between multiple agents in scoring offers flexible and reliable way for evaluating the target mLLMs comprehension of meme harmfulness. After scoring on different harmfulness categories in the taxonomy, we now have primary understanding of the target mLLMs overall capabilities in deciphering memes. Note that in the subsequent refinement stage, the scoring performances can also be used for further observation to reveal the multimodal knowledge boundaries of the target mLLM about harmfulness. 2.4 Iterative Refinement To further explore the target models capabilities in finer-grained perspective beyond harmfulness categories, the iterative refinement stage focuses on generating diverse and unseen cases that present greater challenges crafted by Refiner agent, for the target mLLM to analyze the safety insights in exhaustive test scenarios. Therefore, it is crucial for Refiner agent to identify the factors that create difficult samples inside given harmful context. Since the misbelief statement is designed to describe the specific harmful content within meme, it can be used as an identifier to retrieve similar memes that convey more related harmful meanings in the same harmfulness category. Specifically, we define seed sample (i, t, c, misb, s) S, where is small set of meme samples randomly selected from Mscored to begin iterative refinement with. Cases belonging to category and similar to (i, t, c, misb, s) are retrieved from the history memory consisting of all the scored samples as follows: Href = Retrieve(misbH, c), (7) where Href means the retrieved set of the scored memes that are Top-3 semantically relevant to the current sample with the similar misbelief statement misb, and denotes the set of all the scored history initialized by Mscored. Based on the target models performance in the current sample and it similar cases, the multimodal content in such harmful context can better reveal how the models capabilities are impacted by nuanced expressions. Then we employ Refiner agent to generate new meme sample by learning from the harmful context, which aims to create more challenging combination of multimodal content, probing the target models ability to understand the implicit harmfulness embedded within the meme. Since textual semantics are generally more directly expressed compared to visual semantics (Akbari et al., 2019), the original meme would be modified with the text while preserving the image as follows: = Refiner(ti, c, misb, s, Href), (8) where is the modified meme text, misb serves as reference for Refiner to ensure that the multimodal content should still retain the same false belief after modification, in case unrelated content is generated to deviate from our original purpose. Here, Href is integrated into Refiners input as incontext examples, sorted in descending order for Refiner to learn from the expressions in memes that contribute to challenges for the target model. As illustrated in Figure 2, the refined sample (i, t, c, misb) is then used to test the target model following the same procedures in 2.3, which results in score s. If < s, where the target model fails to perform the same level of analysis as on m, the target model is considered to exhibit weakness on such content. Next, we further explore the target models vulnerability to similar misbelief in the current harmful context, which is conducted by retrieving new relevant sample, with Algorithm 1 Iterative Refinement 1: Input: Target mLLM α, Scored sample set Mscored, Maximum iteration number , Scored history initialized by Mscored. while step < do Href = Retrieve(misbH, c) = Refiner(ti, c, misb, s, Href) = Scoring(α, i, t, c) + (i, t, c, misb, s) if < then 2: Randomly select from Mscored 3: Sample pool = Mscored 4: for case = (i, t, c, misb, s) do 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: end for 19: Output: History H. case Retrieve(misbP, c) case step step + 1 end if end while break else misbelief most similar to misb, as the sample to be modified for the next round of refinement within category c, from the subset of Mscored (after excluding S), referred to as the sample pool . The retrieved sample is then refined following exactly the same steps above in this section. Combined with the models performance on original meme data and refined samples, at the end of iterative refinement, is the overall performance of the target mLLM, which specifies its weaknesses in deciphering harmfulness in memes. The detailed algorithm of the refinement stage is shown in Algorithm 1. By continuously retrieving samples and updating memes with similar misbeliefs, the weakness in understanding harmful contexts can be probed on finer-grained level, resulting in an adaptive evaluation."
        },
        {
            "title": "3 Experiments and Results",
            "content": "In this section, we present series of experimental results to analyze performances of mLLMs. Specifically, we aim to answer three key questions as: RQ1: How do mLLMs perform in analyzing various types of meme harmfulness? RQ2: How are the specific weaknesses of mLLMs exposed in iterative refinement? RQ3: Do the multiple agents in AdamMeme provide fair and reliable evaluations? 3.1 Experimental Setup Datasets We utilized the raw memes from three publicly available datasets: (1) HarM (Pramanick et al., 2021), (2) FHM (Kiela et al., 2020), and (3) Target mLLM Nationality Gender Religion Race Animal Disability Exploitation Political Avg. Score FR Score FR Score FR Score FR Score FR Score FR Score FR Score FR Score FR LLaVA-v1.6 (7B) 5.28 34.16 5.60 26.25 5.01 35.29 5.00 37.88 4.37 51.53 4.86 39.30 4.81 47.70 5.46 24.57 5.05 37.06 LLaVA-v1.6 (34B) 5.90 21.29 6.13 18.80 6.16 16.06 6.05 18.60 5.89 19.18 6.19 17.37 6.01 21.63 6.05 16.32 6.05 18.66 Qwen-VL-Chat (9.6B) 3.84 65.73 4.06 51.38 4.44 47.58 4.55 45.91 3.50 67.92 4.06 54.86 4.19 52.65 4.33 49.16 4.13 54.14 Qwen2.5-VL (7B) 5.99 25.22 6.53 16.60 6.32 18.34 6.45 20.35 5.69 26.57 6.36 18.34 5.79 29.31 6.52 10.05 6.21 20.59 QwQ (32B) 6.19 14.89 6.26 18.60 6.24 18.43 6.16 17.14 5.41 30.52 6.47 10.34 6.03 20.00 6.28 11.16 6.14 17.53 Qwen-VL-Max 4.96 38.77 5.19 29.74 5.07 32.88 4.79 40.64 4.46 49.77 5.16 33.33 4.74 45.57 4.93 38.67 4.92 38.63 Doubao-Lite 5.25 40.48 5.61 24.46 5.68 25.28 5.64 26.52 5.34 28.50 6.12 16.60 5.61 30.98 5.29 30.83 5.57 28.02 Doubao-Pro 5.10 39.11 4.17 54.92 5.41 33.08 5.16 38.65 4.07 62.01 4.09 58.75 4.80 43.88 4.48 52.23 4.67 47.58 Step-1v 8k 6.93 10.92 6.87 09.79 7.00 04.80 6.89 05.81 6.63 10.23 6.47 17.92 6.86 12.24 6.83 05.83 6.81 09.70 7.40 05.33 7.68 03.40 7.68 04.08 7.38 05.22 7.36 02.87 7.29 07.00 7.46 06.33 7.28 05.22 7.44 04.97 Step-1o-Vision 32k GPT-4o 7.53 00.43 7.43 02.14 7.52 01.24 7.30 03.53 7.15 03.64 7.44 02.54 7.39 03.36 7.26 00.44 7.38 02.18 Table 1: Performances of mLLMs in AdamMeme. Best and second results are highlighted in bold and underlined. MAMI (Fersini et al., 2022), to collect data as the initial unlabeled meme set for evaluation. Metrics To evaluate the target models overall performance on deciphering harmfulness in memes, we adopt two metrics: Average Score and Failure Rate (FR). Average Score is calculated with scores assigned by scoring agents in 2.3. FR (%) is the proportion of samples which the target model fails to perform reasonable response with. In the calculation of FR, if the score on sample is lower than preset threshold (set as 4.0), it is considered to have generated flawed answer. higher FR indicates weaker capability in performing analysis. The Scorer agent is prompted to give score under 4 when the analysis of the target model exhibits factual errors. We set FR as the primary metric. Target mLLMs For comprehensive evaluations, we conduct an assessment on 11 mainstream mLLMs of varying scales from 5 series: 1) LLaVAv1.6 (7B, 34B) (Liu et al., 2024), 2) QwenVL-Chat (9.6B), Qwen2.5-VL (7B), QwQ (32B), Qwen-VL-Max (Bai et al., 2023), 3) Doubao-Lite, Doubao-Pro, 4) Step-1o-Vision-32k, Step-1v-8k, 5) GPT-4o, as the target mLLMs. To facilitate reproducibility, we set the temperature to 0 in experiments. Implementation details are provided in Appendix A. we have the following observations: 1) Among all target mLLMs, GPT-4o and Step series showed leading performance in deciphering all types of harmfulness in memes. QwQ (32B) showed outstanding capabilities, comprehensively excelling other mLLMs except for GPT-4o and Step series, which is notable considering that QwQ is relatively lightweight model. 2) Different target models showed varying levels of capacities and weaknesses in analyzing diverse types of harmfulness in memes. Among all harmful categories, harmfulness related to Disability is most challenging for models from Step series, with Disability FRs higher than corresponding average FRs by 2.03% and 8.22% for Step-1o-Vision 32k and Step-1v 8k respectively, while Doubao-Lite is relatively strong in deciphering the category of Disability, demonstrating comparable results to Step-1v 8k. 3) Larger models do not guarantee better reasoning capacity in deciphering meme harmfulness. Compared to Qwen-VL-Max, an extended version of Qwen-VLChat (9.6B), LLaVA-v1.6 (7B) achieved comparable results, even slightly surpassing Qwen-VL-Max by 0.13 on average score and -1.57% on average FR. We also notice this observation for mLLMs that are affiliated with the same series, for instance, Doubao-Pro is outperformed on all harmfulness categories by its lighter version Doubao-Lite. 3.2 Main Results (RQ1) 3.3 Effect of Refinement (RQ2) Table 1 shows the results of target mLLMs on various harmfulness categories in our proposed AdamMeme. During harmfulness mining, Miner agents discover two additional harmfulness categories of Political and Child Exploitation (abbreviated as Exploitation in the tables) in the memes used in our experiment, resulting in taxonomy of 8 categories: Nationality, Gender, Religion, Race, Animal, Disability, Child Exploitation, Political. From the results of these harmfulness categories, As shown in Table 2, to investigate the adaptive evaluation claimed in our framework, we conduct analysis by removing the iterative refinement stage. 1) The average FRs decrease to varying degrees, indicating that the Refiner agent effectively generates refined memes that present more challenging cases based on the target mLLMs weaknesses by learning patterns that contribute to difficult cases from in-context historical samples during refinement. 2) Among all tested mLLMs, GPT-4o exRace Gender Religion Nationality Animal Target mLLM 32.50 (-1.66) 24.00 (-2.25) 28.50 (-6.79) 30.00 (-7.88) 44.63 (-6.90) 35.00 (-4.30) 45.23 (-2.47) 24.00 (-0.57) 32.80 (-4.26) LLaVA-v1.6 (7B) LLaVA-v1.6 (34B) 17.50 (-3.79) 13.00 (-5.80) 13.50 (-2.56) 12.00 (-6.60) 16.38 (-2.80) 14.00 (-3.37) 16.58 (-5.05) 14.00 (-2.32) 14.59 (-4.07) Qwen-VL-Chat (9.6B) 65.00 (-0.73) 45.00 (-6.38) 42.21 (-5.37) 42.00 (-3.91) 67.80 (-0.12) 50.50 (-4.36) 49.25 (-3.40) 49.00 (-0.16) 51.11 (-3.03) Qwen2.5-VL (7B) 23.81 (-1.41) 12.95 (-3.65) 12.22 (-6.12) 17.11 (-3.24) 25.42 (-1.15) 17.35 (-0.99) 27.92 (-1.39) 10.22 (+0.17) 18.41 (-2.18) QwQ (32B) 14.14 (-0.75) 18.00 (-0.60) 13.00 (-5.43) 14.07 (-3.07) 28.81 (-1.71) 10.00 (-0.34) 18.09 (-1.91) 08.54 (-2.62) 15.39 (-2.14) Qwen-VL-Max 36.13 (-2.64) 26.94 (-2.80) 29.05 (-3.83) 32.09 (-8.55) 47.46 (-2.31) 30.46 (-2.87) 44.67 (-0.90) 36.90 (-1.77) 35.41 (-3.22) 36.00 (-4.48) 21.13 (-3.33) 16.50 (-8.78) 17.09 (-9.43) 26.55 (-1.95) 14.14 (-2.46) 28.14 (-2.84) 27.00 (-3.83) 23.29 (-4.73) Doubao-Lite 37.00 (-2.11) 46.15 (-8.77) 24.62 (-8.46) 33.00 (-5.65) 58.19 (-3.82) 57.79 (-0.96) 42.71 (-1.17) 49.00 (-3.23) 43.34 (-4.24) Doubao-Pro Step-1v 8k 07.07 (-3.85) 06.60 (-3.19) 02.01 (-2.79) 03.03 (-2.78) 09.04 (-1.19) 15.08 (-2.84) 09.60 (-2.64) 06.06 (+0.23) 07.29 (-2.41) Step-1o-Vision 32k 04.55 (-0.78) 01.02 (-2.38) 01.02 (-3.06) 01.02 (-4.20) 01.13 (-1.74) 05.56 (-1.44) 04.52 (-1.81) 04.15 (-1.07) 02.89 (-2.08) GPT-4o 00.50 (+0.07) 00.50 (-1.64) 00.00 (-1.24) 00.00 (-3.53) 02.82 (-0.82) 00.50 (-2.04) 01.51 (-1.85) 00.00 (-0.44) 00.70 (-1.48) Disability Exploitation Political Avg. Table 2: FR performances on the original meme data without the Iterative Refinement stage. Figure 3: Effect of different iterations in Refinement. hibits the highest robustness, showing the least performance variation, consistently providing accurate analysis even in dynamically-updated evolving data. 3) We also noticed that, GPT-4o showed almost perfect performances on analyzing harmfulness of Race and Disability with drops on FR by 3.53% and 2.04%, proving that compared to the original memes, samples created by Refiner help to probe into the mLLMs true capacity of reasoning on meme harmfulness. 4) On the other hand, Doubao-Lite is most affected by refinement, with drop of 4.73% on average FR, showing weaknesses in Nationality, Religion and Political harmfulness. 5) Figure 3 provides more detailed demonstration of the effect of iterative refinement as the iteration number increases. We observe that average scores in all categories decrease as meme data updates, eventually reaching convergence at around the 6 round of iteration. This iterative approach facilitates more in-depth analysis of mLLMs reasoning capacity by adaptively extending cases that models struggle with. 6) We also analyze the models finer-grained weaknesses revealed through iterative refinement. Figure 4 illustrates the distribution of the top 10 misbelief topics within the harmfulness category of Race, where the pink bar represents the distribution of refined data. Our observations indicate that the target models weaknesses are primarily concentrated in areas such as racial stereotypes, anti-Black bias, and dehumanization, with most refined cases aligning with these topics. Since refinement expands the dataset by Figure 4: An example of target models (Doubao-Lite) specific weaknesses exposed in the Iterative Refinement stage within the harmfulness category of Race. Average Score Average FR Agent 06.20 24.00 Human 06.18 19.99 Agreement 0.567 0.738 Table 3: Results of the human subject study. iteratively refining memes that contain similar misbelief statements the target model struggles with, the distribution of misbelief statements in the refined samples provides valuable insight into the models deficiencies regarding such specific topics. 3.4 Reliability Analysis (RQ3) To verify the reliability of our method in performing fair analysis with multiple agents, we further conduct human evaluations on the agent-based scoring and decision-making components. Specifically, for model scoring, we randomly sampled over 600 cases from evaluation results that evenly cover 8 categories and 11 target mLLMs, and asked human experts to score the target models answers with the same instructions given to the Scorer. As shown in Table 3, on model scoring, agents achieved 56.7% and 73.8% intra-class agreement on average score and average FR. We provide more details and results of human evaluation in Appendix D. 3.5 Case Study The core of our framework is to iteratively generate challenging cases for the target model. To better understand how AdamMeme probes the specific weakness of target mLLMs, we conduct case study on GPT-4os performance in our framework. Figure 5: An example of target models (GPT-4o) analysis on the original meme sample and the refined sample. As shown in Figure 5, the original meme perpetuates the harmful idea of engaging the animal goat in abusive behaviors, expressed by the explicit and crude words in the meme. In the refined case, Refiner removes the explicit word referring to abusive engagement, and preserves the original meaning by keeping the tone of the original meme text with more obscure expression. Before refinement, the target model successfully identifies the inherent harmfulness by catching the textual cues. However, in the refined sample, the target model fails to relate to the idea behind the animal goat that this kind of animal often suffers from potential sexual abuse, which is commonly seen in dark jokes in animal memes. By removing superficial cues from this case, Refiner exposes the weakness that the target model is not sensitive enough to such type of harmfulness, which helps us to explore more specific view of GPT-4os reasoning capacity. This reaffirms that Refiner amplifies the target models vulnerability and facilitates the process of uncovering model-specific weaknesses. We provide more cases of detailed analysis in Appendix E."
        },
        {
            "title": "4 Related Work",
            "content": "Evaluation of Harmful Meme Understanding The understanding of harmful memes (Wang et al., 2025a) is one of the rapidly growing fields for combating disinformation on social media (Lin et al., 2021, 2023b; Wang et al., 2025b), supported by large-scale meme benchmarks (Kiela et al., 2019; Pramanick et al., 2021) and initiatives such as the Hateful Memes Challenge (Kiela et al., 2020) by Facebook, aimed at detecting memes related to hate speech (Das et al., 2020; Hee et al., 2023). These efforts have propelled research into harmful meme detection (Pramanick et al., 2021), task made more challenging by the multimodal nature of memes, which often combine both textual and visual elements. To investigate the capability of mLLMs in understanding harmful memes, Lin et al. (2024b) curated new meme benchmark by integrating previous representative datasets (Fersini et al., 2022; Suryawanshi et al., 2020), with the goal of identifying weaknesses in mLLMs safety awareness of meme-based social abuse. However, beyond the inevitable issue of test set leakage, this static evaluation approach primarily relied on expert-designed, task-specific benchmarks, overlooking the dynamic nature of multimodal meme content and lacking the flexibility needed to address the complex and open-ended challenges posed by real-world social media. Different from previous work on static accuracy evaluation for mLLM audit, our work aims to explore the comprehensive evaluation beyond the detection, to dynamically elicit the limitations of harmful meme understanding in the mLLMs. Multi-agent Systems recent trend in research is the development of agent-based systems powered by mLLMs for variety of downstream applications. Park et al. (2023) explored the simulation of human behaviors through multiple agents, emphasizing the phenomenon of information diffusion, where information spreads as agents communicate. Qian and Cong (2023) introduced ChatDev, system that enables multiple agent roles to communicate and collaborate through conversations, facilitating the completion of the software development life cycle. Similarly, several studies have leveraged multi-agent collaboration to enhance task performance (Du et al., 2024; Wang et al., 2024; Zhang et al., 2024). range of multiagent frameworks (Li et al., 2023; Wu et al., 2024; Hong et al., 2024; Lin et al., 2025) have been proposed to support the development of multi-agent systems. Building on these insights, we develop novel multi-agent framework for the comprehensive mLLM evaluation (Fu et al., 2025) of discerning harmfulness in meme-based social abuse."
        },
        {
            "title": "5 Conclusion and Future Work",
            "content": "This paper introduced AdamMeme, flexible, agent-based evaluation framework for assessing the reasoning capabilities of target mLLMs in identifying harmful memes. Through multi-agent collaboration, our framework iteratively refines meme data with challenging samples, effectively exposing the limitations of target mLLMs in this research realm. Experiments revealed varying performance across different target mLLMs, offering detailed, model-specific insights into their weaknesses in understanding meme harmfulness. Future work will focus on expanding the evaluation of the frameworks reliability and exploring its application to broader range of harmful content and model types."
        },
        {
            "title": "Limitations",
            "content": "There are multiple ways to further improve our work: First, in our experiments, we employ GPT-4o, the most advanced and dominant mLLM, as the agent controller due to its strong capabilities. While we implement various measures, including the wisdom-of-crowds strategy and human evaluations, to enhance the reliability and transparency of agent-based assessments, ensuring fair evaluations and mitigating potential bias, the inherent bias introduced by this approach remains unavoidable. This is similar to how humans tend to favor reasoning that aligns with their own knowledge systems and factual logic. Besides, most emerging mLLMs are trained using synthetic data distilled from GPT-4o, so they also tend to generate GPT4o-like content. In future research, we plan to incorporate more advanced agent settings as mLLMs continue to evolve, replacing the current dominant GPT-4o, and integrating human-in-the-loop procedures to create more reliable and robust evaluation framework. This represents key direction for further investigation. Secondly, in this study, we collect raw data from existing benchmarks on harmful meme detection, which provides diverse set of meme samples with various types of harmfulness, allowing us to validate the effectiveness of our method. However, these datasets do not fully represent the real-world distribution of harmful content, as data distributions often shift over time. To address this limitation, we plan to extend our research by incorporating additional datasets, either through newly established benchmarks or by collecting data from online communities, enabling more diverse and up-to-date exploration of meme harmfulness. Lastly, this study focuses on evaluating the reasoning capacity of mLLMs in understanding harmfulness by directly prompting target models with instructions to analyze meme content. However, we are unable to conduct complete evaluation of certain mainstream mLLMs, such as Claude and Gemini, due to their inherent safety mechanisms, which frequently result in refusals to engage with harmful content. This limitation restricts our ability to fully assess their capabilities. In future research, we aim to address this challenge by exploring alternative methods to enhance model responsiveness, enabling more comprehensive evaluation across broader range of emerging models and ultimately improving the robustness of our framework."
        },
        {
            "title": "Ethics Statement",
            "content": "This research involved human subject studies to evaluate the quality and reliability of AdamMeme. The following considerations were adhered to ensure the protection and ethical treatment of participants: 1) Voluntary Participation: All participants were informed about the nature of the research and their role in it. Participation was entirely voluntary, with participants having the right to withdraw at any time without any consequences. 2) Informed Consent: Written informed consent was obtained from all participants. This consent form detailed the purpose of the research, the procedures involved, potential risks, and measures taken to safeguard participant data. 3) Data Anonymity and Confidentiality: All data collected during the study were anonymized. Personal identifiers were removed to maintain confidentiality and data were stored securely to prevent unauthorized access. 4) Minimal Risk: The study involved minimal risk to participants. The tasks performed were similar to everyday activities, and no sensitive personal information was requested or recorded. Research indicates that evaluating harmful like hateful or offensive content can have negative effects. To protect our human evaluators, we establish three guidelines: 1) ensuring their acknowledgment of viewing potentially harmful content, 2) limiting weekly evaluations and encouraging lighter daily workload, and 3) advising them to stop if they feel overwhelmed. Finally, we regularly check in with evaluators to ensure their well-being. The purpose of this work is to prevent the spread of meme harmfulness and to ensure that people are not subjected to prejudice or racial and gender discrimination. Nevertheless, we are aware of the potential for malicious users to reverse-engineer and create harmful memes guided by AdamMeme. This is strongly discouraged and condemned. Intervention with human moderation would be required in order to ensure that this does not occur. Furthermore, all the refined test data generated by the agents does not contain any personal information. Elisabetta Fersini, Francesca Gasparini, Giulia Rizzi, Aurora Saibene, Berta Chulvi, Paolo Rosso, Alyssa Lees, and Jeffrey Sorensen. 2022. Semeval-2022 task 5: Multimedia automatic misogyny identification. In Proceedings of the 16th International Workshop on Semantic Evaluation (SemEval-2022), pages 533 549."
        },
        {
            "title": "Acknowledgments",
            "content": "This work is partially supported by Tencent RhinoBird Focused Research Program (Value-aligned Credible Large Language Model) and RMGS project (Artificial Intelligence and Big Data Analytics for Social Good)."
        },
        {
            "title": "References",
            "content": "Hassan Akbari, Svebor Karaman, Surabhi Bhargava, Brian Chen, Carl Vondrick, and Shih-Fu Chang. 2019. Multi-level multimodal common semantic space for image-phrase grounding. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1247612486. Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. 2023. Qwen-vl: versatile vision-language model for understanding, localizaarXiv preprint tion, arXiv:2308.12966, 1(2):3. text reading, and beyond. Rui Cao, Ming Shan Hee, Adriel Kuek, Wen-Haw Chong, Roy Ka-Wei Lee, and Jing Jiang. 2023. Procap: Leveraging frozen vision-language model for hateful meme detection. In Proceedings of the 31st ACM International Conference on Multimedia, pages 52445252. Rui Cao, Roy Ka-Wei Lee, and Jing Jiang. 2024. Modularized networks for few-shot hateful meme detection. In Proceedings of the ACM on Web Conference 2024, pages 45754584. Zixin Chen, Hongzhan Lin, Ziyang Luo, Mingfei Cheng, Jing Ma, and Guang Chen. 2024. Cofipara: coarseto-fine paradigm for multimodal sarcasm target identification with large multimodal models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 96639687. Abhishek Das, Japsimar Singh Wahi, and Siyao Li. 2020. Detecting hate speech in multi-modal memes. arXiv preprint arXiv:2012.14891. Yilun Du, Shuang Li, Antonio Torralba, Joshua Tenenbaum, and Igor Mordatch. 2024. Improving factuality and reasoning in language models through multiagent debate. In Forty-first International Conference on Machine Learning. Rao Fu, Ziyang Luo, Hongzhan Lin, Zhen Ye, and Jing Ma. 2025. ScratchEval: Are GPT-4o smarter than my child? evaluating large multimodal models with visual programming challenges. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers), pages 689699. Ming Shan Hee, Wen-Haw Chong, and Roy KaWei Lee. 2023. Decoding the underlying meaning of multimodal hateful memes. arXiv preprint arXiv:2305.17678. Ming Shan Hee, Shivam Sharma, Rui Cao, Palash Nandi, Preslav Nakov, Tanmoy Chakraborty, and Roy Ka-Wei Lee. 2024. Recent advances in online hate speech moderation: Multimodality and the role of large models. In EMNLP (Findings). Jack Hessel, Ana Marasovic, Jena Hwang, Lillian Lee, Jeff Da, Rowan Zellers, Robert Mankoff, and Yejin Choi. 2023. Do androids laugh at electric sheep? humor understanding benchmarks from the new yorker caption contest. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 688714. Sirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, et al. 2024. Metagpt: Meta programming for multi-agent In The Twelfth Internacollaborative framework. tional Conference on Learning Representations. Jianzhao Huang, Hongzhan Lin, Liu Ziyan, Ziyang Luo, Guang Chen, and Jing Ma. 2024. Towards lowresource harmful meme detection with lmm agents. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 22692293. Douwe Kiela, Suvrat Bhooshan, Hamed Firooz, Ethan Perez, and Davide Testuggine. 2019. Supervised multimodal bitransformers for classifying images and text. arXiv preprint arXiv:1909.02950. Douwe Kiela, Hamed Firooz, Aravind Mohan, Vedanuj Goswami, Amanpreet Singh, Pratik Ringshia, and Davide Testuggine. 2020. The hateful memes challenge: detecting hate speech in multimodal memes. In Proceedings of the 34th International Conference on Neural Information Processing Systems, pages 26112624. Gitanjali Kumari, Kirtan Jain, and Asif Ekbal. 2024. M3hop-cot: Misogynous meme identification with multimodal multi-hop chain-of-thought. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 2210522138. of human behavior. In Proceedings of the 36th annual acm symposium on user interface software and technology, pages 122. Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. 2023. Camel: communicative agents for\" mind\" exploration of large language model society. In Proceedings of the 37th International Conference on Neural Information Processing Systems, pages 5199152008. Hongzhan Lin, Yang Deng, Yuxuan Gu, Wenxuan Zhang, Jing Ma, See-Kiong Ng, and Tat-Seng Chua. 2025. Fact-audit: An adaptive multi-agent framework for dynamic fact-checking evaluation of large language models. arXiv preprint arXiv:2502.17924. Hongzhan Lin, Ziyang Luo, Wei Gao, Jing Ma, Bo Wang, and Ruichao Yang. 2024a. Towards explainable harmful meme detection through multimodal debate between large language models. In The ACM Web Conference 2024, Singapore. Hongzhan Lin, Ziyang Luo, Jing Ma, and Long Chen. 2023a. Beneath the surface: Unveiling harmful memes with multimodal reasoning distilled from large language models. In The 2023 Conference on Empirical Methods in Natural Language Processing. Hongzhan Lin, Ziyang Luo, Bo Wang, Ruichao Yang, and Jing Ma. 2024b. Goat-bench: Safety insights to large multimodal models through meme-based social abuse. ACM Transactions on Intelligent Systems and Technology. Hongzhan Lin, Jing Ma, Mingfei Cheng, Zhiwei Yang, Liangliang Chen, and Guang Chen. 2021. Rumor detection on twitter with claim-guided hierarchical graph attention networks. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1003510047. Hongzhan Lin, Haiqin Yang, Ziyang Luo, and Jing Ma. 2024c. Unleashing trigger-free event detection: Revealing event correlations via contrastive derangement framework. In ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 1017110175. IEEE. Hongzhan Lin, Pengyao Yi, Jing Ma, Haiyun Jiang, Ziyang Luo, Shuming Shi, and Ruifang Liu. 2023b. Zero-shot rumor detection with propagation structure via prompt learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 52135221. Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. 2024. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2629626306. Joon Sung Park, Joseph OBrien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael Bernstein. 2023. Generative agents: Interactive simulacra Shraman Pramanick, Dimitar Dimitrov, Rituparna Mukherjee, Shivam Sharma, Md Shad Akhtar, Preslav Nakov, and Tanmoy Chakraborty. 2021. DearXiv tecting harmful memes and their targets. preprint arXiv:2110.00413. Chen Qian and Xin Cong. 2023. Communicative agents for software development. arXiv preprint arXiv:2307.07924, 6(3). Shivam Sharma, Firoj Alam, Md Shad Akhtar, Dimitar Dimitrov, Giovanni Da San Martino, Hamed Firooz, Alon Halevy, Fabrizio Silvestri, Preslav Nakov, and Tanmoy Chakraborty. 2022. Detecting and understanding harmful memes: survey. In Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, pages 55975606. Shardul Suryawanshi, Bharathi Raja Chakravarthi, Mihael Arcan, and Paul Buitelaar. 2020. Multimodal meme dataset (multioff) for identifying offensive content in image and text. In Proceedings of the second workshop on trolling, aggression and cyberbullying, pages 3241. Ruofei Wang, Hongzhan Lin, Ziyuan Luo, Ka Chun Cheung, Simon See, Jing Ma, and Renjie Wan. 2025a. Meme trojan: Backdoor attacks against hateful meme detection via cross-modal triggers. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pages 78447852. Shengkang Wang, Hongzhan Lin, Ziyang Luo, Zhen Ye, Guang Chen, and Jing Ma. 2025b. Mfc-bench: Benchmarking multimodal fact-checking with large vision-language models. In ICLR 2025 Workshop on Reasoning and Planning for Large Language Models. Zhenhailong Wang, Shaoguang Mao, Wenshan Wu, Tao Ge, Furu Wei, and Heng Ji. 2024. Unleashing the emergent cognitive synergy in large language models: task-solving agent through multi-persona selfcollaboration. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 257279. Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun Zhang, Jiale Liu, et al. 2024. Autogen: Enabling next-gen llm applications via multi-agent conversation. In ICLR 2024 Workshop on Large Language Model (LLM) Agents. Hongxin Zhang, Weihua Du, Jiaming Shan, Qinhong Zhou, Yilun Du, Joshua Tenenbaum, Tianmin Shu, and Chuang Gan. 2024. Building cooperative embodied agents modularly with large language models. In The Twelfth International Conference on Learning Representations. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. 2023. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36:4659546623."
        },
        {
            "title": "A Implementation Details",
            "content": "For all experiments, we implement GPT-4o as the agent controller. The number of Miners and the number of candidate agents in scoring are set to 3 by following the principle of Occams razor to realize the function while better controlling the cost for broad usability. In iterative refinement, we retrieve the top 3 semantically relevant samples for Href , as 3 cases proves to have the best generation quality in our previous tests. The size of the meme sample set and the maximum iteration number are empirically set to 10. Seed set is fixed during evaluation on different target models. Compared results (p < 0.05 under t-test) are averaged over three random 3 runs. The cost for evaluating one target model is about 30 dollars and 5 hours. In the calculation of FR, the threshold for flawed answers is set as 4.0. With scored dataset, FR is calculated as: = umscore<threshold umtotal , (9) where umtotal denotes the total number of samples, and umscore<threshold indicates number of samples that target model being rated under threshold. We provide the human evaluation in Appendix that further validates the choice for the threshold. To avoid the interference of original texts embedded in memes during iterative refinement, we employ the OCR-SAM2 tool to erase texts. Note that to make sure model scoring is fair on the original and refined samples, we use erased images as visual input, and add meme texts into textual prompts for all model scoring procedures so that the fairness of model scoring is not affected. In the iterative refinement stage, we apply the BM25 algorithm to retrieve Href as well as the new sample for the next round. Specifically, we use misb of the current sample as query to match the misbelief sentences, which serve as identifiers for the samples in and . The retrieved samples are those that correspond to the top matches in the 2https://github.com/yeungchenwa/OCRSAM?tab=readme-ov-file collection of misbelief sentences. We provide more analysis of the quality of retrieved samples and their impact on in-context generation in Appendix E. Miner Agent. The role of the Miner agent is to discern harmfulness categories in memes. In designing the prompt for Miner, we particularly ask the agent to be strict when suggesting new harmfulness category, because once added into the taxonomy, it will serve as standard for following harmfulness mining procedure, and could cause the taxonomy to expand uncontrollably if not properly suggested. To ensure diversity as well as reliability, we set the temperature as 1 for each Miner in the majority vote strategy. In our experiments, we ask 3 Miners separately, and then integrate their answers in majority vote strategy. The specific prompt is shown in Figure 7. Figure 6: Instructions for Judge to check if new harmfulness category is suitable to join the current taxonomy. Examiner Agent. In harmfulness mining, we employ Examiner agents to confirm the existence of harmfulness to ensure the reliability of new harmfulness suggested by the Miner based on the current meme. To ensure reliability and reproducibility, we set the temperature of the Examiner to 0. The instructions for the Examiner are shown in Figure 8. Judge Agent. Similar to Examiner in harmfulness mining, we employ Judge agent to check if the category of new harmfulness suggested by the Miner is reasonable to be added into the current taxonomy. We also set the temperature of the Judge to 0. The instructions for Judge are shown in Figure 6. Figure 8: Instructions for Examiner to reaffirm the existence of harmfulness in given memes. Narrator Agent. To facilitate further steps as well as the investigation into the underlying reasons behind their harmful nature, the Narrator agent is asked to extract the misbelief that lies in the current meme, based on the harmfulness with respect to certain aspects. The temperature of Narrator is set to 0 for reproducibility. Detailed instructions are shown in Figure 9. Figure 7: Instructions for Miner to decipher harmfulness in given memes. Figure 9: Instructions for Narrator generate specific misbelief. Reference Generation. In generating reference answers for scoring, we employ two types of agents: the agent for generating answers, and the senior agent to summarize the final reference answer. For agents that generate answers, we set the temperature as 1 to ensure diversity, offering more comprehensive perspective for final answers. For the senior agent responsible for summarizing the final answer, we set the temperature as 0. The specific instructions are shown in Figure 10. Figure 10: Instructions for agents to generate reference answers. The prompt on the upper side is for agents to generate answers separately, and the prompt at the bottom is for the senior agent to summarize. Scorer Agent. The model scoring process follows reference-based procedure. With the final summarized reference answer, the Scorer agent is instructed as shown in Figure 11. The temperature for the Scorer agent is set to 0. Refiner Agent. In iterative refinement, Refinement is instructed to modify given meme in an incontext manner, by learning from historical scored samples and preserving the meaning of original misbelief in the meme, as shown in Figure 12. The temperature for the Refiner agent is set to 0. Target mLLMs. In our experiments, we conduct experiments on 11 models from 5 series. For LLaVA-v1.6 (7B, 34B) and Qwen-VL-Chat (9.6B), we conduct evaluations using local deployment, while the other models are accessed via API. Among the tested mLLMs, LLaVA-v1.6 (7B, 34B) and Qwen-VL-Chat (9.6B), Qwen2.5-VL (7B), QwQ (32B) are open-sourced models with known parameters. Note that we do not include models from Claude series for their strong security filtering measures, as models refuse to answer most of the tasks related to analyzing harmfulness. We also exclude Gemini series from target mLLMs because we financially do not have enough access to its API. Figure 11: Instructions for Scorer agent for referencebased scoring. Train Test HarM-C HarM-P FHM harmful 1064 1486 3050 harmless 1949 1534 5450 harmful 124 173 250 harmless 230 182 250 Table 4: Statistics of HarM and FHM. Initial Taxonomy. In harmfulness mining, we establish an initial taxonomy with specific explanations to harmfulness categories as shown in Figure 13."
        },
        {
            "title": "B Dataset Statistics",
            "content": "In our experiments, we sampled raw data from three datasets: HarM (Pramanick et al., 2021), FHM (Kiela et al., 2020), and MAMI (Fersini et al., 2022). Statistics of original datasets are listed in Table 4 and Table 5, HarM consists of HarM-C and HarM-P, with meme data related to COVID19 and politics. MAMI is multi-label task that consists of memes annotated by harmfulness of 5 categories: Misogynous, Shaming, Stereotype, Objectification and Violence. We only use raw memes from the test set of MAMI for sampling to balance the ratio of different harmfulness categories. Religion Race Nationality Gender Disability Animal Child Exploitation Political Total Mined Samples 537 662 718 864 242 177 260 1422 4882 Scored Samples 200 200 200 200 200 177 200 200 1577 Table 6: Statistics of meme data. inedSamples refers to meme data after harmfulness mining, and ScoredSamples denotes data selected for scoring."
        },
        {
            "title": "C Discussion of Data",
            "content": "From the data used in our experiments, we first randomly sampled 5000 raw memes from the datasets for harmfulness mining. After harmfulness mining, the 2 new harmfulness categories and corresponding explanations discovered by Miner agents are: 1)Political: \"This category involves harm related to political ideologies, figures, or movements. Memes in this space can contribute to misinformation, promote political extremism, or encourage divisive and harmful rhetoric towards certain political groups or leaders.\", 2) Child Exploitation: \"This category covers content that promotes or trivializes the exploitation, abuse, or inappropriate treatment of minors. Memes in this category can normalize harmful behaviors towards children or create culture of acceptance around illegal or immoral actions against minors.\" The detailed statistics of meme samples after harmfulness mining are listed in Table 6. After harmfulness mining, the memes that are considered harmless by miner agents are filtered. We then randomly sample 200 data points from each category, for those less than 200, we keep all samples in the category. Note that in our previous experiments, we found that for each harmful category, the size of samples should be at least over 150 for effective and stable evaluations. After iterative refinement, the statistics of different models are shown in Table 7. In iterative refinement, meme data is updated by retrieving and modifying samples that the target models exhibit weaknesses on. It can be observed from the table that, the final data volumes of all models are roughly at the same level, and the number of final meme data does not seem to be directly correlated with the target mLLMs capabilities. Figure 12: Instructions for Refiner agent to generate harder sample. Figure 13: Harmfulness categories in the initial taxonomy and the corresponding explanations. Misogynous Shaming Stereotype Objectification Violence 2810 2202 348 1274 146 5000 500 953 153 Train Test Table 5: Statistics of MAMI. Target mLLM LLaVA-v1.6 (7B) LLaVA-v1.6 (34B) Qwen-VL-Chat (9.6B) Qwen2.5-VL (7B) QwQ (32B) Qwen-VL-Max Doubao-Lite Doubao-Pro Step-1v 8k Step-1o-Vision 32k GPT-4o Nationality Gender Religion Race Animal Disability Exploitation Political Total 1962 1951 1959 1820 1909 1858 1986 1990 1896 1903 1895 255 249 249 234 256 224 271 267 251 245 251 266 264 257 233 248 251 264 253 242 253 258 257 236 257 229 232 243 241 240 240 243 236 243 249 248 230 235 227 252 248 238 244 230 219 212 207 213 219 214 230 215 212 220 240 250 253 235 242 232 235 268 235 235 234 232 239 238 220 233 225 253 247 223 230 227 239 245 245 232 250 237 256 237 252 241 238 Table 7: Statistics of meme data in different experiments after refinement. Average Accuracy Agreement Human Evaluators 0.806 0.767 Table 8: The results of harmfulness mining human evaluation. Agreement indicates the average Cohens Kappa between any two expert annotators. Average Score Agreement Conciseness Informativeness Persuasiveness Readability Soundness 2.25 2.91 2.59 2.80 2.92 0.616 0.769 0.550 0.681 0. Table 9: Human evaluation results of the quality of reference answers."
        },
        {
            "title": "D More Discussion of Reliability",
            "content": "Scorer. To discuss the reliability of our agent-based framework that relies upon mLLM judgments, we further conduct three types of analysis: 1) harmfulness mining evaluation, 2) reference scoring evaluation, 3) refinement evaluation. We employ three human experts aged between 24-28 for human evaluation. Detailed instructions and data settings for each evaluation task are as follows: Harmfulness Mining Evaluation. In evaluating the reliability of harmfulness mining, we design multiple-choice task, where human evaluators are asked to select choices from harmfulness categories in the final taxonomy. Specifically, we randomly select 200 memes from the original dataset for the multiple-choice task, evenly covering all of the 8 harmfulness categories, and calculate the average accuracy. Table 8 shows the results between human experts and agent-based majority vote. As shown in the results, human evaluators reached 80.6 % accuracy on annotating memes with agent-based majority vote answers as true labels. The average Cohens Kappa among three evaluators is 0.767, indicating strong intra-class agreement. Reference-based Scoring Evaluation. We also provide specific assessment on model scoring. In evaluating the fairness of the model scoring stage, we randomly selected 616 scored samples, covering all the 8 harmfulness categories and tested 11 mLLMs with 7 samples for each setting, and designed tasks focusing on two procedures: 1) The quality of reference answer; 2) The reliability of To verify the reliability of generated reference answers, we ask human evaluators to rate final answers according to the following criteria: 1) Conciseness: the answer contains less redundant information; 2) Informativeness: the answer provides new information, such as explaining the background and additional context; 3) Persuasiveness: the answer seems convincing; 4) Readability: the answer follows proper grammar and structural rules; 5) Soundness: the answer seems valid and logical. For each criterion we apply three-point scale scoring, where 1 means the poorest quality and 3 means the best. Table 9 shows the average result of human rated samples. As demonstrated in the table, human evaluators give high quality scores on aspects of Informativeness, Readability and Soundness, proving that the reference answers give accurate analysis on meme harmfulness. The reference answers receive relatively low conciseness score, as most answers result in long texts. Human evaluators show high intra-class agreement on Conciseness, Informativeness, Readability and Soundness, while demonstrating moderate agreement on Persuasiveness, given that harmfulness understanding is subject task. For the reliability of Scorer, we ask the human evaluators to score target models answers using the exact same instructions and reference answers as we give to Scorer. The evaluation results and analysis are provided in 3.4, which shows the high intra-class agreement between Scorer and human Average Score Agreement Redundancy Diversity Readability Coverage Fairness Suitability 1.86 1.74 2.71 2.06 1.68 2.06 0.598 0.501 0.574 0.530 0.429 0.705 Table 10: Human evaluation results of the generation quality of refined meme text. evaluators. As FR indicates the proportion of samples scored lower than the threshold, we calculate the agreement of FR by transforming scores into boolean list, with 0 indicating lower than the threshold and 1 indicating higher, then we compare agent and human results to obtain the FR agreement. The high agreement of 73.8% on FR in Table 3 also helps to prove that the threshold setting in our experiments is reasonable. Refinement Evaluation. We conduct human evaluation on the generation quality of refined meme texts. Specifically, we randomly choose 200 samples, 15 from each harmfulness category, for human evaluators, and instruct them to conduct analysis from the following aspects: 1) Redundancy: the repetitiveness or unnecessary duplication within the refined text; 2) Diversity: the variety of refined text; 3) Readability: how easy it is for human beings to read and understand the content; 4) Coverage: how comprehensively the refined sample covers the misbelief; 5) Fairness: whether the data presents information in balanced and unbiased manner; 6) Suitability: the appropriateness of the data for conducting harmful meme understanding evaluation. As shown in Table 10, the low Redundancy indicates that the refined meme text proves to be short sentences, and Refiners generation does not involve extra information, and high Readability and Coverage shows that the after refinement, the modified meme still preserves the original meanings of misbelief sentence."
        },
        {
            "title": "E More Cases",
            "content": "To facilitate more comprehensive understanding of AdamMeme, we provide extra case studies in the following stages in our framework. Harmfulness Mining. Figure 14 illustrates an example of harmfulness mining, where among three miners, all of them vote for the category of Religion, and one votes for Child Exploitation, which was considered invalid. The third miner raises new category of islamophobia, which is rejected by the Judge because it overlaps with category in Figure 14: An example of harmfulness mining. the current taxonomy. Model Scoring. Figure 17 shows an example of model scoring, where three candidate answers are generated, and the senior agent summarizes the best answer. With the final reference answer, the Scorer grades the target model by comparing its answer with the reference answer, as shown in Figure 18. Refinement with Misbelief. In iterative refinement, Refiner generates new challenging case by learning from previous cases. Figure 15 is an example of the generation with in-context examples for similar events (Lin et al., 2024c), where Refiner learns from the expression of previous scored cases. In the figure, the top 3 similar scored samples are retrieved using the misbelief sentence from history. The third sample is the refined version of the second sample, where the text in the meme is modified into more moderate expression. The Refiner learns from the retrieved samples and refines the meme texts into more vague expression, as shown at the bottom of the figure. More Refined Samples. We present more examples of how memes are refined into more challenging samples in Figure 16. In generating new samples, we notice that the words with explicit hostile meanings in the memes are paraphrased into more euphemistic expressions, and require the target model to focus more on visual contents and the intentions expressed by the combination of multimodal elements. Samples of Retrieval. To further verify the quality of our retrieval meme samples using misbelief sentence, we present more cases in Figure 19. In the figure, we use misbeliefs of the query memes Figure 16: More examples of refined samples. to retrieve memes from meme sample set, and in the retrieved memes are the top 3 samples similar to query memes. It can be observed from the figure that misbelief sentences extract the general harmful concepts in meme harmfulness, and similar memes usually share common phrases in their misbeliefs. Figure 15: An example of in-context examples. Figure 17: An example of reference answer generation. Figure 18: An example of model scoring. The reference answer is the final answer in Figure 17. The target model in this sample is LLaVA-v1.6 (34B). Figure 19: More examples of retrieval using misbelief sentences."
        }
    ],
    "affiliations": [
        "BUPT",
        "HKBU",
        "HKUST",
        "NUS"
    ]
}