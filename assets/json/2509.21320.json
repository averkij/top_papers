{
    "paper_title": "SciReasoner: Laying the Scientific Reasoning Ground Across Disciplines",
    "authors": [
        "Yizhou Wang",
        "Chen Tang",
        "Han Deng",
        "Jiabei Xiao",
        "Jiaqi Liu",
        "Jianyu Wu",
        "Jun Yao",
        "Pengze Li",
        "Encheng Su",
        "Lintao Wang",
        "Guohang Zhuang",
        "Yuchen Ren",
        "Ben Fei",
        "Ming Hu",
        "Xin Chen",
        "Dongzhan Zhou",
        "Junjun He",
        "Xiangyu Yue",
        "Zhenfei Yin",
        "Jiamin Wu",
        "Qihao Zheng",
        "Yuhao Zhou",
        "Huihui Xu",
        "Chenglong Ma",
        "Yan Lu",
        "Wenlong Zhang",
        "Chunfeng Song",
        "Philip Torr",
        "Shixiang Tang",
        "Xinzhu Ma",
        "Wanli Ouyang",
        "Lei Bai"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present a scientific reasoning foundation model that aligns natural language with heterogeneous scientific representations. The model is pretrained on a 206B-token corpus spanning scientific text, pure sequences, and sequence-text pairs, then aligned via SFT on 40M instructions, annealed cold-start bootstrapping to elicit long-form chain-of-thought, and reinforcement learning with task-specific reward shaping, which instills deliberate scientific reasoning. It supports four capability families, covering up to 103 tasks across workflows: (i) faithful translation between text and scientific formats, (ii) text/knowledge extraction, (iii) property prediction, (iv) property classification, (v) unconditional and conditional sequence generation and design. Compared with specialist systems, our approach broadens instruction coverage, improves cross-domain generalization, and enhances fidelity. We detail data curation and training and show that cross-discipline learning strengthens transfer and downstream reliability. The model, instruct tuning datasets and the evaluation code are open-sourced at https://huggingface.co/SciReason and https://github.com/open-sciencelab/SciReason."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 2 ] . [ 1 0 2 3 1 2 . 9 0 5 2 : r SciReasoner: Laying the Scientific Reasoning Ground Across Disciplines Yizhou Wang1,2 , Chen Tang1,2 , Han Deng1,2 , Jiabei Xiao1,2 , Jiaqi Liu1 , Jianyu Wu1,7 , Jun Yao1,4 , Pengze Li1,6 , Encheng Su1,4 , Lintao Wang1,3, Guohang Zhuang1, Yuchen Ren1,3, Ben Fei1,2, Ming Hu1, Xin Chen1, Dongzhan Zhou1, Junjun He1, Xiangyu Yue2, Zhenfei Yin8, Jiamin Wu1,2, Qihao Zheng1, Yuhao Zhou1, Huihui Xu1, Chenglong Ma1, Yan Lu1,2, Wenlong Zhang1, Chunfeng Song1, Philip Torr8, Shixiang Tang1,2 , Xinzhu Ma1,5 , Wanli Ouyang1,2 and Lei Bai1 1Shanghai Artificial Intelligence Laboratory, 2The Chinese University of Hong Kong, 3The University of Sydney, 4University of Science and Technology of China, 5Beihang University, 6Fudan University, 7Shanghai Jiao Tong University, 8University of Oxford We present scientific reasoning foundation model that aligns natural language with heterogeneous scientific representations. The model is pretrained on 206B-token corpus spanning scientific text, pure sequences, and sequencetext pairs, then aligned via SFT on 40M instructions, annealed cold-start bootstrapping to elicit long-form chain-of-thought, and reinforcement learning with task-specific reward shaping, which instills deliberate scientific reasoning. It supports four capability families, covering up to 103 tasks across workflows: (i) faithful translation between text and scientific formats, (ii) text/knowledge extraction, (iii) property prediction, (iv) property classification, (v) unconditional and conditional sequence generation and design. Compared with specialist systems, our approach broadens instruction coverage, improves cross-domain generalization, and enhances fidelity. We detail data curation and training and show that cross-discipline learning strengthens transfer and downstream reliability. The model, instruct tuning datasets and the evaluation code are open-sourced at https: //huggingface.co/SciReason and https://github.com/open-sciencelab/SciReason."
        },
        {
            "title": "Highlights",
            "content": "Adaptive Scientific Reasoning. We obverse not all scientific tasks necessitate explicit reasoningfor instance, property prediction tasks often admit direct answers, whereas retrosynthesis and design tasks require elaborate reasoning. Our model can exploit the robustness of answerbased matching to generate reliable CoT supervision only where reasoning is essential, which explicitly differentiates between two task modes: instant tasks, where the original supervised fine-tuning (SFT) data is preserved, and thinking tasks, where the data is fully replaced with CoT-augmented counterparts. This principled separation enables the model to adaptively allocate reasoning capacity, producing coherent CoT rationales for complex scientific tasks while preserving efficiency and accuracy on simpler, non-reasoning tasks. Task Grouped Rewarding. Typical reward functions for reinforcement learning on typical LLMs lack explicit signals for encouraging and correcting scientific reasoning patterns across diverse thinking tasks, e.g., knowledge extraction, property prediction and classification, sequence generation and design. To tackle these diverse scientific tasks, we propose to group various scientific rewards into three categories: distance-based reward functions for scientific predictions, matching-based reward functions for scientific retrieval and extraction, and scientific toolingverified reward functions that are based on professional scientific tools. Such innovative grouping enables our model share the common knowledge among different tasks and simplifies the reinforcement learning framework. Scientific Reward Softening. Scientific tasks typically return True(1)/False(0), in which case we experimentally observe making the reinforcement learning difficult to converge. In this paper, we develop method to continuously scale reward signals. Specifically, we transform task-specific : Co-first Author Please send correspondence regarding this report to tangshixiang@pjlab.org.cn : Student Project Lead : Core Contributor : Corresponding Author Aspect Scientific Reasoning Scientific Task Coverage Scientific Translation Text/knowledge Extraction Property Prediction Property Classification Generation and Design Total SciReasoner NatureLM Bio-T5/T5+ UniGenX 10 6 49 16 103 3 0 10 17 30 0 1 13 5 21 0 0 4 0 18 Pretraining corpus SFT corpus Reasoning corpus 206B diverse tokens: Scientific text Pure scientific sequences Sequencetext leaved pairs interSequencesequence interleaved pairs 40M QA samples 570K answer-aligned longform scientific CoT data and 72K difficulty-filtered RL data 140B sequence-centric tokens: 90% sequence-based data 10% general text to80B biochemical kens: Completely composed of proteins and small molecules data 84.5M structural samples: 99% protein-structure and materials data 1% small molecule data 5M QA samples 2.8M QA samples 24M task-related samples NA NA NA Table 1 Comparison emphasizing reasoning ability and coverage breadth. Our model explicitly elicits scientific reasoning and supports five capability families with up to 103 tasks; While other methods either only emphasize cross-domain sequence-based discovery or support limited disciplines, they do not pay attention to scientific reasoning and end-to-end workflow coverage. metrics, such as language-based scoring metrics (e.g., BERTScore) or quantitative measures (e.g., RMSE) for regression tasks, into normalized [0, 1] range. This continuous reward signal effectively promotes stable training and significantly boosts the models performance on complex scientific reasoning tasks. Comprehensive Scientific Capability. Our model supports broad scientific workflow, spanning five major families and encompassing up to 100 tasks: Scientific Translation: Bidirectional conversion between human text and scientific representations for interoperability. Text & Knowledge Extraction: Literature-aware modeling that turns unstructured text into structured evidence for readreasondesign cycles. Property Prediction: Accurate estimation of continuous scientific endpoints across chemistry, biology, and materials. Property Classification: Robust categorization of molecular, biological, and material properties into discrete classes. Sequence Generation & Design: Constraint-aware, controllable generation that reuses predictive structure for efficient design loops. We obtain state-of-the-art performance on 54 tasks and rank among top-2 on 101 tasks, demonstrating that single backbone integrating natural language with multi-representation scientific data can enable cross-domain generalization beyond fixed task menus and reduce the fragmentation seen in specialist pipelines. We highlight the superiority of our model in Tab."
        },
        {
            "title": "References",
            "content": "1. Introduction 3 6 6 8 9 9 9 10 10 10 11 13 13 13 13 17 19 23 30 36 36 37 37 38 40 Large foundation models (LFMs) have demonstrated tremendous potential in modeling natural language [1, 2, 3], vision [4, 5, 6, 7, 8], audio [9, 10, 11, 12] and multimodal [13, 14, 15, 16] tasks. Recent works such as GPT-5 [17], Qwen-3 [18], LLaMA-4 [19], DeepSeek-v3 [20], and Gemini2.5 [21] have achieved remarkable performance across diverse domains, benefiting from advances in transformer architectures [22, 23, 24], large-scale pretraining [25], and instruction tuning [26, 27, 28]. While these models excel at general-purpose reasoning [29, 30, 31], summarization [32, 33, 34], and dialogue [35, 36, 37], their adaptation to scientific disciplines such as chemistry, biology, healthcare, and materials science remains an emerging frontie [38, 39, 40, 41, 42, 43, 44]. Scientific problems often involve diverse, domain-specific representations [45, 46, 47], ranging from molecular representation (e.g., SMILES [48], SELFIES [49]) and protein sequences (e.g., FASTA [50]) to genomic data, which differ fundamentally from natural language in structure and semantics. Bridging this gap requires foundation models capable of integrating symbolic, numerical, and sequence-based knowledge with natural language understanding, enabling applications such as molecular property prediction [51, 52, 53, 54], biomolecule engineering [55, 56, 57, 58, 59], and the interpretation of complex experimental data [60, 61, 62, 63, 64]. Existing works on scientific large models can be generally classified into two parts: Figure 1 We present unified scientific LLM framework organized into three stages (Pre-training, SFT, and Reasoning). In the pre-training stage, mixed warm-up corpus aligns the base model with scientific syntax and sequences, combining scientific and general text with pure sequences and paired sequence-text / sequence-sequence data spanning DNA/RNA, proteins, small molecules, and materials. The SFT stage aggregates more than 100 curated scientific sub-tasks across these domains to adapt the model to actionable research objectives such as property prediction and sequence-text transformations. The reasoning stage introduces rationale-augmented corpus and supports two usage modesInstant (fast answers) and Thinking (step-by-step deliberation)to solve complex problems. Specialist Large Models. Bio-T5 families [65, 66], ChemLLM [67], ChemVLM [68], ChemMLLM [69], Chem3DLLM [70], ProLLama [71], BioReason [72] adopt pretrained large language models (e.g., T5 [73], LLaMA-2 [74], Qwen-3 [18], InternLM [3]) to solve the discipline-specific tasks [75, 76, 77]. They typically collect curated, domain-specific corpora [78, 79, 80] and task-aligned instruction data [81], e.g., sequenceproperty pairs; protein/DNA/RNA sequences with structural or functional annotations; reaction and synthesis records; and multi-form molecular representations (e.g., SMILES/IUPAC/SELFIES). Multi-discipline Large Models. NatureLM [82] and UniGenX [83] adopt pretrain-thenfinetune scheme to learn shared model for both life science and materials science. Galactica [84] is designed to support scientific knowledge mining from massive academic literatures. Biology-Instructions [85] collects large-scale dataset to comprehend multi-omics biology. 4 Although these methods and other homologous techniques [86, 70, 87, 88, 68, 89, 90, 91] demonstrate the large models can successfully handle scientific tasks, key gaps remain in cross-domain generalization, grounding to physical and experimental constraints, and systematic coverage of heterogeneous scientific modalities. For specialist models, their narrow scope and customized pipelines limit transfer outside the target subfield, fragment the model ecosystem, and, more importantly, require retraining to support unseen tasks or modalities. For multi-discipline large models [82, 83], they typically focus on limited tasks (e.g., NatureLM supports only 30 tasks) and therefore fail to cover the long tail of scientific problems or generalize compositionally beyond pre-specified task list. Furthermore, they still produce shallow pattern-based predictions rather than explicit multi-step reasoning traces that mirror how scientists derive conclusions from data and theory. We therefore aim to close these gaps by unifying multi-representation scientific data with natural language in single backbone, scaling instruction coverage to substantially broader and more diverse task mixture, and grounding reasoning via long-form thinking process to ensure fidelity and verifiability. In this work, we propose the first scientific reasoning large language model that couples multirepresentation pretraining with instruction-driven alignment and reasoning-inducing post-training. Our model ingests natural language alongside scientific sequences (DNA/RNA/protein), molecular strings (e.g., SMILES/IUPAC/SELFIES) and material representation through task-aware tokenization mapped into shared backbone. Our training process involves pretraining, SFT-based post-training and RL-based post-training. Pretraining first aligns heterogeneous corpus (e.g., sequence-structuretext), while instruction tuning abstracts diverse tasks into small set of consistent inputoutput schemas that encourage compositional generalization. Finally, reasoning-inducing phase equips the model with deliberate and verifiable problem solving: we bootstrap long-form rationales via an annealed cold-start procedure, and finish with reinforcement learning using physicsand task-aware rewards (factuality, stoichiometry consistency, constraint satisfaction), yielding reliable chain-ofthought solutions. Our method supports broad range of the following tasks. We organize capabilities into five families that align with the scientific workflow and enable cross-domain generalization beyond prior systems (e.g., limited task menus as in NatureLM), as follows: Scientific Translation. Ensures interoperability between human-readable text and machineoriented scientific representations, enabling faithful bidirectional conversion that preserves semantics and physical meaning. Compared with earlier systems confined to few format mappings, this category scales to many representations and thus supports data augmentation, tool chaining, and transparent humanAI collaboration. Text and Knowledge Extraction. Focuses on natural language understanding and reasoning tasks that operate directly on diverse scientific text. This evaluates the models ability to identify scientific entities, extract structured knowledge, and answer domain-specific questions from literature. By isolating linguistic comprehension from sequence modeling, it probes whether the model can capture the semantics of scientific discourse, resolve complex entity relations, and deliver accurate answers across diverse contexts. Scientific Property Prediction. Provides unified interface for estimating continuous endpoints (e.g., biochemical, materials), producing continuous signals that can be used for screening and prioritization. Unlike prior models tied to small set of benchmarks, our formulation flexibly accommodates heterogeneous continuous targets across disciplines, enhancing transfer and compositional generalization. Scientific Property Classification. Handles categorical endpoints (binary, multi-class, and hierarchical) such as solubility, stability, functional annotations, or phase labels. It delivers high-fidelity and domain-general capabilities over categorical properties, facilitating practical applications such as candidate triage, property screening, and informed design decisions across scientific domains. 5 Scientific Sequence Generation and Design. Closes the loop from inference to actionable design under multi-objective and constraint-aware settings (e.g., synthesizability, stability, safety). This family goes beyond template matching by supporting controllable, cross-domain generation that reuses learned structure from prediction tasks, yielding better sample efficiency than siloed, single-discipline models. The overall illustration of this work is shwon in Fig. 1. In summary, we introduce unified scientific LLM that couples scaled multi-representation pretraining with instruction-aligned I/O schemas and verifiable reasoning stage. Task-aware tokenization aligns heterogeneous sequences and notations; solvecheck protocol with retrieval and domain tools, followed by reinforcement learning with physicsand task-aware rewards, enforces unit/stoichiometry and constraint consistency. The resulting single backbone supports prediction, design, translation, and literature-grounded extraction within readreasondesign loop, delivering strong cross-domain generalization across chemistry, proteins, genomics, and materials; the following sections detail data, training, and evaluation. 2. Pre-Training 2.1. Pretraining Data Types To ensure downstream results on scientific sequence generation, property prediction, and sequencemanipulation tasks, we design our pretraining strategy around four complementary types: 1. Scientific Text. We harvest long and short prose from PubMed [92], PubChem [93], textbooks, and materials databases (e.g., crystal-structure annotations), as well as solution threads from StackExchange for specific discipline. This corpus grounds the models understanding of domain-specific reasoning and terminology needed for scientific Q&A and context-aware property inference. 2. Pure Sequence. To capture the contextual dependencies of biomolecular and chemical sequences, we collect: DNA/RNA: Serving as the fundamental blueprint of genetic information, pure nucleotide sequences allow the model to directly learn base-level patterns and long-range dependencies without being influenced by external structural or functional annotations. Protein: As the direct products of gene expression, amino acid sequences encode the full spectrum of structural and functional diversity in proteins, enabling the model to capture residue-level motifs and evolutionary conservation signals. Small Molecules: We collect Simplified Molecular Input Line Entry System (SMILES) strings from PubChem. Furthermore, to ensure that the model can learn various representations of molecules beyond SMILES, we also adopt International Union of Pure and Applied Chemistry (IUPAC) and Self-Referencing Embedded Strings (SELFIES) names to achieve enhanced molecule representations. For SMILES sequences, we warp them with the <SMILES>...</SMILES> tags. 3. SequenceSequence and SequenceText Pairs. Beyond standard sequence collections, we enrich the training data with cross-discipline examples drawn from life sciences (e.g. protein structure), materials chemistry, and scientific literature. We hope this hybrid strategy preserves broad language fluency while instilling deep domain-specific proficiency. To simulate the diversity of cross-disciplinary queries, we develop: Intra-discipline pairs: e.g. multi-protein homology judgment based on sequence patterns. Such data enhances the models ability to discern subtle evolutionary and structural relation6 Figure 2 Composition of the 206B-token scientific pretraining corpus across five scientific domains: protein data, chemical data, genomic data, transcriptomic data and materials data. ships within single domain, improving performance in specialized sequence comparison and classification. Cross-discipline pairs: e.g. prediction of small-molecule binding to protein segment. By bridging cross-disciplinary representations, these pairs could improve the capacity to integrate different knowledge for tasks such as drug discovery and biomolecular engineering. Multi-representation of the same entity: concurrent IUPAC, SELFIES, and SMILES representations of molecule. This could promote translation and reasoning between complementary notations, enabling robust understanding and cross-format interoperability. Textdata pairs: combined prompts and labels for molecular, nucleic, or protein properties. Pairing natural language descriptions with scientific sequences enables rich bidirectional interaction, empowering downstream natural language understanding (e.g., protein or molecule interpretation) and language-driven generation of scientific sequences. 4. General Text. In order to maintain the language capabilities of the model itself, we adopt C4 dataset as our general text corpus. Compared with the existing methods [82], the pretraining data they use contains only about 10% of natural text, and almost entirely relies on pure sequence data; therefore, they lack coherent conversation capabilities and cross-domain reasoning abilities."
        },
        {
            "title": "Data Preparation Details",
            "content": "DNA. We retrieve about 200 billion bases from NCBI [94], sampling at an average of 1,000 bp fragments per organism to produce 30 billion <dna></dna>-tagged tokens and is represented by the FASTA format. Essential metadata (e.g., organism name and genomic coordinates) is included to anchor the models interpretation of each sequences context. 7 RNA. We collect about 20 billion RNA tokens from RNAcentral [95], reflecting diverse transcriptomic contexts. Then, we warp the collected RNA sequence (FASTA format) with the <rna></rna> tag. The basic information of the RNA sequence (e.g., Family) is also retrieved and is used to guide the models understanding of sequence context. Protein. We collect protein sequences from UniRef50 and UniRef90 (mammalia) and warp the raw sequence with <protein></protein> tag, this yields about 40 billion protein tokens. We also pair sequence fragments with concise natural-language descriptions, and sample pure text abstracts from PubMed for additional context. Small Molecules. To ensure reliable representation and alignment of small molecules, we curate diverse textual resources, construct multiple molecular encodings, and implement rigorous token validation. Text: abstracts from PubChem and long-form chemistry Q&A from StackExchange. Multi-Representations: To ensure comprehensive molecular characterization, we utilize multiple chemical representations. These are obtained either directly from established databases (e.g., SMILES-to-IUPAC from PubChem) or generated via specialized toolkits (e.g., SMILES-to-SELFIES using the SELFIES library [49]). Tagging: We apply multistage filter: regex based on RDKit [96] / SELFIES [49] toolkits, small LLM validation and manual review to eliminate ambiguous tokens (e.g., In, Is) and ensure high-fidelity tagging. Materials. We collect raw material data from publicly available material databases, including Material Projects [97], GNoME [47], SNUMAT [98], etc. The fields of raw data include the crystal structure files (Crystallographic Information File, CIFs), chemical composition, space-group and crystal-system information, material type (e.g., bulk or slab), thermodynamic metrics (formation energy, energy above hull), electronic and mechanical properties (band gap, density, elastic moduli), magnetic properties, per-site coordination details, bond-length descriptions, etc. To convert the raw data into the pretraining corpus, we generate task-specific templates by prompting an LLM to convert structural annotations into natural-language descriptions, then score and select the best templates for large-scale synthetic data generation. Fig. 2 summarizes the composition of our training corpus across five scientific domains and their internal subdivisions. Specifically, protein sequences account for the largest share (55 billion tokens, 33.1%), followed by chemistry data (50 billion tokens, 30.1%), DNA genome fragments (30 billion tokens, 18.1%), RNA transcripts (26 billion tokens, 15.7%), and materials-science texts (5 billion tokens, 3.0%). 2.2. Training Protocol We train the model for one epoch using bfloat16 mixed-precision. The base models are Qwen3 [18] 1.7B and 8B, respectively. Training batches consisted of 16 and 32 sequences per device for 1.7B model and 8B model, respectively. We set the initial learning rate to 2 104 and accumulated gradients over two steps, applying linear warm-up over the first 1% of total steps. We especially arrange the data for the following training stages: At the warm-up stage, we adopt mixed form of scientific text, general text and paired sequence 8 Figure 3 Training loss over train tokens for the 1.7B and 8B models. The model does not exhibit any saturation phenomenon during the training process. text data. This could help the model rapidly adapt to both domain-specific and general linguistic patterns before the main training phase. At the end of the training, we include SFT-style training data to refine the models capability to follow complex, domain-specific instructions and produce task-aligned outputs. This stage serves to align the models generative behavior with SFT-like reasoning and formatting standards, thereby enhancing its utility in real-world scientific applications. For the training system, we use 128 A800 GPUs for 1.7B model and 256 A800 GPUs for 8B model. The training loss curves are shown in Fig. 3. 3. Post-Training 3.1. Supervised Fine-tuning 3.1.1. Data In the supervised fine-tuning (SFT) stage, we integrated about 100 scientific sub-tasks spanning multiple disciplines, including chemistry (Mol-Instructions [81], ChemData [67], USPTO50k [99], SMolInstruct [100], etc.), DNA, RNA, proteins (Biology-Instructions [85], PEER [101], OPI [102], GUE [103], BEACON [104], etc.), materials (LLM4Mat [105], Material Projects [97]) and general instruction texts from OIG [106]. Moreover, cross-sequence interaction tasks and natural languagebased scientific question answering are categorized within the cross-domain corpus. To warp the scientific sequences with discipline-related tags (i.e., <SMILES>...</SMILES>, <dna>...</dna>, <rna>...</rna>, <protein>...</protein>, etc.), the data preparation pipeline involved: Extracting scientific sequences from chemistry, DNA, RNA, protein, and macromolecule datasets via rule-based scripts tailored to each dataset format. Conducting secondary annotation using domain-specific checkpoints from pre-trained singlediscipline models. Filtering and adding scientific tags with the assistance of small LLMs. Converting all data into unified dialogue format for SFT training. We show the data distribution of supervised finetuning stage in Tab. 2."
        },
        {
            "title": "Category",
            "content": "Training samples (M) Proportion Materials General Text DNA&RNA Protein Molecule Cross Domain"
        },
        {
            "title": "Total",
            "content": "8.278 10.873 5.476 2.473 12.556 0.168 20.79% 27.30% 13.75% 6.21% 31.53% 0.42% 39.824 100.00% Table 2 Distribution of supervised fine-tuning (SFT) training samples across five major scientific domains and additional general text data, reported in millions and proportions. Each domain undergoes tailored preprocessing, domain-specific annotation, and tagging (e.g., <SMILES>...</SMILES>, <dna>...</dna>, <protein>...</protein>) before being converted into unified dialogue format for SFT. The corpus comprises 63.35 million samples. 3.1.2. Training Protocol We train the model for five epochs using 16-bit bfloat16 precision. per-device batch size of 32 is used, with gradient accumulation over two steps to achieve larger effective batch size. The initial learning rate is set to 5 105, and we apply linear warm-up over the first 5% of total steps. Inputs are tokenized lazily with right-side truncation to maximum sequence length of 8192 tokens, and the Liger-kernel [107] is enabled for efficiency. The training process is accelerated using DeepSpeed ZeRO Stage 2 [108] and FlashAttention implementation [109]. Based on the SFT dataset, we train four single-discipline models in DNA/RNA, protein, materials science, and chemical molecules. However, during multi-disciplinary joint training, we observe substantial performance degradation in certain sub-tasks due to data imbalance after dataset merging. To address this, we adopt two strategies: (1) we conduct large-scale pre-training followed by augmentation with additional SFT datawith materials science receiving further data expansionand (2) data re-weighting, which adjusts task-level proportions to mitigate the impact of low-quality instructions or underrepresented tasks. This re-weighting approach effectively alleviates performance decline in mixed-dataset training. 3.2. Reasoning-based Reinforcement Learning 3.2.1. Data Cold Start Before reinforcement learning (RL), we construct correct-only chain-of-thought (CoT) corpus as cold-start prior. For each sub-task, we use DeepSeek-R1-Distill-Qwen-32B to generate CoT, answer pairs and target 𝑇=500 correctly answered items under hard evaluation budget of 𝐶=50,000 prompts. We first draw 6,000 random prompts, evaluate correctness, and retain the CoT attached to correct answers. If after 𝑁 evaluated items the tally is 𝑆 < 𝑇, we estimate the additional sample size via the empirical accuracy ˆ𝑝=𝑆/𝑁 as 𝑛add = (𝑇 𝑆)/max( ˆ𝑝, 𝜀) (with small 𝜀>0 for numerical stability), then repeat samplinggenerationadjudication until either 𝑆 𝑇 or 𝑁 𝐶; if the cap is reached, we keep all accumulated correct examples. Correctness is adjudicated by direct comparison of the generators final answer to SFT ground truth for auto-gradable tasks (exact match or task-specific tolerance), and by an external GPT-3.5-turbo judge for semantic equivalence on free-form tasks (e.g., protein descriptions). This adaptive, budgeted, correct-only harvesting mitigates rationale noise and yields reliable initialization for subsequent RL. Reinforcement Learning We curate reinforcement learning (RL) data at the level of each sub-task using an empirical solve-rate filter that targets medium-difficulty instances. Let 𝐷train 𝑘 denote the k-th sub-tasks training set. For every example 𝑥 𝐷train 𝑘 : 1. Stochastic rollouts. We generate 𝑁 = 8 independent rollouts from 𝑀0 on 𝑥 using stochastic decoding with do_sample=True. To balance output diversity against format fidelity, we adopt an adaptive temperature 𝑇 tuned once per sub-task prior to collection: we initialize at 𝑇 = 0.9 (the DAPO default setting) and increase 𝑇 if the eight-way samples show insufficient diversity (e.g., low uniqueness of normalized responses or highly repetitive content); conversely, if outputs become overly diffuse and induce frequent format non-compliance or invalid parses, we decrease 𝑇. During the final eight rollouts for each 𝑥, we hold 𝑇 fixed and vary only the random seed, keeping all other decoding parameters constant. 2. Empirical solve rate. For rollout 𝑖, compute correctness indicator 𝑓𝑖 (𝑥) {0, 1} using the sub-tasks canonical metric (e.g., exact match or thresholded score), and aggregate ˆ𝑝(𝑥) = 1 8 8 𝑖=1 𝑓𝑖 (𝑥). 3. Mid-difficulty filter. Retain examples with partial success: 𝑆 = (cid:8) 𝑥 𝐷train 𝑘 (cid:12) (cid:12) (cid:12) 0.125 < ˆ𝑝(𝑥) < 0.875 (cid:9) . (1) (2) This removes items that are nearly always solved (ˆ𝑝 1) or nearly always failed (ˆ𝑝 0), which typically yield weaker gradients for policy improvement. 4. Sampling. Uniformly sample without replacement 𝐾 = 1000 examples from 𝑆. If 𝑆 < 1000, include all of 𝑆 and top up to 𝐾 = 1000 by uniformly sampling from 𝐷train 𝑆. The resulting 1,000-example set serves as the RL training pool for that sub-task. Focusing RL on neither-trivial-nor-impossible instances yields stronger and more stable improvement: items with ˆ𝑝 (0.125, 0.875) (i) expose the policy to cases where exploration can flip outcomes, (ii) avoid vanishing advantages on saturated positives, and (iii) reduce uninformative gradients from uniformly failed cases. 3.2.2. Training Protocol Cold Start Before reinforcement learning, we apply an annealed cold-start (ACS) adaptation to the model after SFT stage to (i) endow it with the ability to produce long, coherent chains of thought (CoT) on tasks that genuinely require stepwise reasoning, while (ii) preserving concise, direct-answer behavior on tasks that do not. We therefore partition the SFT task suite into thinking and instant categories. For thinking tasks, we replace the original SFT samples with their CoT-augmented counterparts (teacher-forced rationales followed by the final answer). For instant tasks, we retain the original direct-answer SFT data unchanged. Moreover, we have observed that task-level replacement outperforms within-task mixing of CoT and non-CoT targets. Mixing presents inconsistent sequence lengths, styles, and termination signals for near-identical instructions, weakening the models calibration about when to elaborate. Replacement yields cleaner conditional distribution, i.e., inputs from thinking tasks consistently map to CoT-style trajectories, thereby improving both the incidence and quality of generated rationales without confusing non-reasoning behavior. To minimize behavioral drift from SFT while shaping rationale production where appropriate, the model is trained with reduced learning rate and short schedule relative to SFT (i.e., small-step updates). This low-temperature adjustment biases the model toward emitting structured reasoning only when the input distribution indicates reasoning task. Figure 4 Scientific reward during DAPO training progress. Reinforcement Learning. We adopt DAPO [110] as the RL algorithm to optimize the policy on heterogeneous scientific tasks. For each prompt we draw 𝐺 candidates from the old policy 𝜋𝜃old , compute group-standardized advantages: 𝑟𝑖,𝑡 (𝜃) = 𝜋𝜃(𝑜𝑖,𝑡 𝑞, 𝑜𝑖,<𝑡) 𝜋𝜃old (𝑜𝑖,𝑡 𝑞, 𝑜𝑖,<𝑡) , ˆ𝐴𝑖,𝑡 = 𝑅𝑖 mean({𝑅𝑖}𝐺 std({𝑅𝑖}𝐺 𝑖=1) 𝑖=1) . (3) and maximize the PPO-style clipped objective: JDAPO(𝜃) = 𝔼(𝑞,𝑎)D,{𝑜𝑖 }𝐺 𝑖=1𝜋𝜃old ( 𝑞) (cid:34) 𝐺 min (cid:16) 𝑜𝑖 (cid:205)𝐺 𝑟𝑖,𝑡 (𝜃) ˆ𝐴𝑖,𝑡, clip(cid:16) 1 𝑖=1 𝑜𝑖 (cid:12) {𝑜𝑖 is_equivalent(𝑎, 𝑜𝑖)} (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) < 𝐺, 𝑡=1 𝑖=1 s.t. 0 < 𝑟𝑖,𝑡 (𝜃), 1 𝜀low, 1 + 𝜀high (cid:35) (cid:17) (cid:17) ˆ𝐴𝑖,𝑡 (4) with the asymmetric Clip-Higher schedule to preserve exploration, together with Dynamic Sampling that filters degenerate groups with identical rewards and token-level policy-gradient reduction. Unlike the original rule-based binary reward, many of our tasks (free-form semantic judgment, information extraction, and numeric regression) do not admit single exact-match criterion. We therefore replace the binary signal with reward softening scheme that maps the grouped metrics to common [0, 1] scale: 𝑅soft 𝑖 = 𝑔(cid:0)𝑚𝑖 (𝑞, 𝑜𝑖, (𝑞))(cid:1) (5) where 𝑔() is monotone calibration ensuring comparable reward magnitudes across tasks and 𝑚𝑖 is group-specific quality metric. For grouping scheme, we employ distance-based rewards for scientific prediction tasks, matching-based rewards for retrieval and extraction tasks, and tool-verified rewards leveraging professional scientific software. We show the training dynamics of scientific reward in Fig 4. 12 4. Evaluation and Application 4.1. Task Overview Translation. The tasks in this group involve converting information between different scientific formats or representations, for example translating between IUPAC names and SMILES, generating descriptive text from molecular structures, or performing the inverse transformation. These capabilities are essential to ensure interoperability within the scientific data ecosystem and to enable effective human-machine communication. Text and Knowledge Extraction/Question Answering. Applying natural language processing to scientific literature, these tasks include chemical entity recognition, interaction extraction, and various formats of question answering (multiple-choice, open-ended, true/false). They integrate AI methods with literature analysis to facilitate rapid extraction of knowledge from scientific texts. Property Prediction and Classification. These tasks span wide range of predictive challenges, from molecular properties (e.g., solubility, toxicity) to materials characteristics (e.g., MP classification, properties derived from JARVIS-DFT), DNA/RNA predictions (e.g., epigenetic markers, transcription factor binding in human), and protein functions (e.g., catalytic activity, stability). Their scopeencompassing chemistry, materials, nucleic acids, and proteinshighlights the universal demand for accurate predictive models in scientific discovery. Generation and Design. This category focuses on the unconditional generation tasks and the conditional generation tasks. It includes unconditional generation of molecules, proteins, RNA, and materials, as well as targeted design challenges such as rational antibody design (RAbD) and programmable RNA switches. Progress in this domain is critical for accelerating the discovery of new compounds and biomolecules. 4.2. Baseline For general-purpose closed-source models, we compare against Gemini-2.5-pro and GPT-o3; for general-purpose open-source models, we use GPT-oss-120B. As specialist baselines, we consider two categories: (i) non-language models, represented by Transformer encoder-based architectures (denoted as non-LLM Experts); and (ii) language models, represented by the best-performing specialist large language model on this task (denoted as Specialist LLMs). Please refer to the Appendix for details. 4.3. Scientific Translation Tasks Scientific translation evaluates whether model preserves scientific semantics while mapping across modalitiessymbolic encodings and formulae, natural-language text, and biological sequences. The goal is not to invent new facts but to produce an equivalent rendering that remains invertible and respects domain constraints (e.g., stoichiometry, valence, sequencefunction coherence). This suite Gemini-2.5-pro version: 2025-06-17 GPT-o3 version: 2025-04-16. Results are provided in Appendix Table. 10 to Table 15. Since some test tasks involve extremely large-scale test sets, to avoid substantial testing costs, for closed-source models, we randomly sampled 1,000 samples from tasks with test set size exceeding 1,000 for testing. Meanwhile, we also present the performance of our models on these 1,000 samples for comparison. 13 Task Metric Gemini 2.5-pro GPT oss Non-LLM Experts Specialist LLMs SciReasoner-1.7B SciReasoner-8B Molecular Representation Translation SMILES to IUPAC SMILES to Formula IUPAC to SMILES IUPAC to Formula Top1 Split Match Top1 Element Match Top1 Split Match Top1 Element Match 4.70% 0.17% 55.20% 19.88% 38.20% 7.58% 88.50% 33.24% Natural Language Translation of Molecule Molecular Description Molecular Captioning ROUGE-L MENTOR Protein Functional Translation CASPSimilarSeq IDFilterSeq UniProtSeq Mol-Instructions ROUGE-L ROUGE-L ROUGE-L ROUGE-L 0.23 0.42 0.01 0.01 0.01 0.21 0.05 0.15 0.02 0.03 0.03 0.07 0.75 0.68 29.00% 49.52%50.20% 56.63%57.10% 93.20% 95.92%87.1% 96.39%96.00% 70.10% 78.85%86.50% 84.40%90.70% 87.90% 91.81%88.10% 92.65%93.00% 0.29 0. 0.74 0.70 0.71 0.44 0.75 0.590.59 0.83 0.77 0.84 0.830.82 0.78 0.600.58 0.85 0.82 0.88 0.990.98 Table 3 Evaluation results of scientific translation tasks. Bold indicates the best performance, and underline indicates the second best. The names of expert models and specialist LLMs for each task are provided in Appendix Table. 10. denotes that only randomly sampled 1000 test samples were utilized for evaluation in the corresponding task. This measure is adopted because extremely largescale test sets exist for certain tasks, which would lead to substantial testing costs. therefore probes compositional reasoning, ontology grounding, and robustness to notation/paraphrase as modality-agnostic measure of semantic fidelity. The results are shown in Tab. 3. Molecular Representation Translation. Following SMolInstruct [100], we evaluate the models ability to convert faithfully among common chemical representations, i.e., SMILES strings, IUPAC names, and empirical formulae. The tasks cover SMILES IUPAC, SMILES Formula, and IUPAC Formula, with performance judged by exact-match accuracy and structure-level equivalence after canonicalization. SMILES to IUPAC Translation. This task takes molecules SMILES string as input and converts it into its corresponding IUPAC name. This is fundamental challenge in cheminformatics, as it requires the model to interpret simplified linear notation of chemical structure and convert it into the standardized naming convention used by chemists. SMILES to Formula Translation. This task takes molecules SMILES string as input and converts it into its corresponding molecular formula. The model receives SMILES string as input and generates the complete molecular formula, which specifies the number of atoms for each element in the molecule. This task is crucial for computational chemistry, as it bridges the gap between structural representation and the molecules elemental composition. IUPAC to SMILES Translation. This task converts chemical name into its corresponding SMILES string. The model takes molecules IUPAC name as input and generates the SMILES string, which is concise text-based representation of the molecules chemical structure. This task is fundamental for many cheminformatics applications, as it allows for the conversion of human-readable names into format that computers can process. IUPAC to Formula Translation. This task takes molecules IUPAC name as input and converts it into its corresponding molecular formula. The models output is list of all elements and the number of atoms for each element in the molecule. This task is crucial for chemical informatics, as it links molecules systematic name to its elemental composition."
        },
        {
            "title": "Molecular Captioning Case",
            "content": "Input Please provide brief introduction to this molecule. CCCCCC/C=CCCCCCCCC(=O)OC[C@H](COP(=O)([O-])OCC[N+](C)(C)C)OC(C)=O </SMILES> <SMILES>"
        },
        {
            "title": "Reference",
            "content": "The molecule is 1,2-diacyl-sn-glycero-3-phosphocholine in which the acyl groups at positions 1 and 2 are specified as (9Z)-hexadecenoyl and acetyl respectively. It has role as mouse metabolite. It derives from palmitoleic acid. Our model The molecule is 1,2-diacyl-sn-glycero-3-phosphocholine in which the 1and 2-acyl groups are palmitoleoyl and acetyl respectively. It has role as mouse metabolite. It is functionally related to palmitoleic acid. GPT-5 This molecule is an unsaturated analog of platelet-activating factor (PAF), consisting of glycerol backbone esterified at the sn-1 position with hexadec9-enoic acid, acetylated at the sn-2 position, and linked at the sn-3 position to phosphocholine group. The presence of cis double bond in the long acyl chain introduces unsaturation, which may influence its membrane dynamics and biological activity. As member of the PAF family, it is potentially involved in inflammatory signaling and immune modulation. Figure 5 Comparison of molecular captioning outputs. Evaluation Metric. We evaluate performance using two metrics: Top-1 Split Match and Top-1 Element Match. Split Match. This metric assesses correctness by parsing the predicted formula 𝑃 and the ground-truth formula 𝐺 into structured components according to predefined splitting rule. Let judge_string_split_match(𝑃, 𝐺) denote Boolean function that returns true if 𝑃 and 𝐺 are equivalent under this split-based comparison. The metric is computed as 𝑀split = 1 𝑁 𝑁 𝑖=1 𝕀(cid:0) 𝑗 judge_string_split_match(𝑃𝑖, 𝐺𝑖, 𝑗)(cid:1) , where 𝕀() is the indicator function, 𝑃𝑖 the 𝑖-th prediction, and {𝐺𝑖, 𝑗} the corresponding set of references. Element Match. This metric provides more permissive comparison by disregarding element order and only checking whether the predicted and reference formulas contain identical counts of each element. For example, C5H8 and H8C5 are considered equivalent. Define 𝐶(𝐹) as the function that maps formula 𝐹 to its multiset of element counts. The metric is given by 𝑀element = 1 𝑁 𝑁 𝑖=1 𝕀(cid:0) 𝑗 𝐶(𝑃𝑖) = 𝐶(𝐺𝑖, 𝑗)(cid:1) . 15 Figure 6 Case study of ZIF-8 in Metal Organic Framework (MOF). Natural Language Translation of Molecule. To bridge the gap between chemical representations and human-readable knowledge, we evaluate tasks that translate molecular structures into natural language descriptions. Molecular Description. This is generative task designed to describe molecule based on its chemical structure. The model takes molecules SMILES string as input. Its goal is to generate natural language description that includes the molecules properties, functions, and summary of its effects. This task is crucial for making complex chemical information accessible and understandable to broader audience. This task comes from Mol-Instructions [81]. Molecular Captioning. Similar to the Molecular Description task, this task requires the model to output the SMILES strings explanation, using data from SMolInstruct [100]. Evaluation Metric. We use ROUGE-L and Metric for Evaluation of Translation with Explicit ORdering (MENTOR) as metrics. As shown in Fig. 5, our model matches the reference on every core fact while avoiding unsupported inferences. It correctly identifies the lipid class as 1,2-diacyl-sn-glycero-3-phosphocholine, specifies the exact acyl substituents (palmitoleoyl at sn-1 and acetyl at sn-2), reproduces the biological role (mouse metabolite), and links it to palmitoleic acidall with normalized terminology and no embellishment. As shown in Fig. 6, we visualize the generated 2D chemical structure and 3D ball-and-stick model for CC1=NC=C[N-]1.CC1=NC=C[N-]1.[Zn+2], which is called ZIF-8 in Metal Organic Framework (MOF). Since MOF is coordination compound, there is Coordinate bond in MOF. Our method generates the IUPAC name of ZIF-8 successfully, while GPT-5 could not. Moreover, LLaSMol and Gemini2.5-Pro generate IUPAC names without meaningful 2D chemical structures. In addition to Coordinate bond, we also evaluate our model for generating molecules with only covalent bonds. As depicted in Fig. 7, our method is able to successfully translate SMILES to IUPAC name. However, the IUPAC name generated by GPT-5 yields the opposite chiral isomer, and the IUPAC names generated by LLaSMol and Gemini2.5-Pro are not consistent with the SMILES. Protein Functional Translation. For the given protein sequence, we prompt the model to describe the function of the protein in clear and coherent manner within the context of the natural language domain. It thus tests both the models reasoning when inferring functional roles from sequence data 16 Figure 7 Case study of SMILESIUPAC from latest PubChem data. Given the input SMILES (top), our model generates the exact reference IUPAC name (shown in blue), preserving the 3,4dichloro-benzamide core, the pyridin-2-ylmethylamino 1,3-dicarbonyl motif, and the (2R)/(2R) stereochemistry; the reconstructed structure (bottom left) is therefore congruent with the input. In contrast, competing LLMs (GPT-5, LLaSMol, Gemini-2.5-Pro) yield chemically inconsistent names that alter substituents, lose halogens, change chain length, or flip chirality, leading to mismatched reconstructions. and its proficiency at producing accurate, fluent text. These tasks include four different data for testing, (i) CASPSimilarSeq: the sequences that are similar to the sequences released by CASP14. (ii) IDFilterSeq: this version uses ID-filtered sequences to reduce redundancy. (iii) UniProtSeq: this versions protein sequences are from the UniProt database. (iv) Mol-ins: The protein sequence of this version is derived from the Mol-Instructions [81] benchmark. Additionally, this test set requires the model to also output subcellular localization information. We therefore consider the above four data sources are four sub-tasks of Protein Functional Translation. Summary of Evaluation Results. Our models consistently outperform baselines. The SciReasoner8B model achieves near-ceiling accuracy in formula recovery (>93%), markedly improves challenging SMILES-IUPAC translations, and delivers the highest scores in molecular captioning (MENTOR 0.61) and protein functional translation (ROUGE-L up to 0.98). These results highlight strong semantic fidelity and scalability, establishing new frontier for scientific translation tasks. 4.4. Scientific Knowledge Extraction and Question Answering Unlike our sequence-based tasks that require models to process structured biological inputs such as DNA, RNA, protein sequences, or SMILES strings, the tasks grouped under Scientific Knowledge Extraction and QA focus purely on natural language understanding and reasoning. They evaluate our models ability to extract scientific knowledge, recognize scientific entities, and answer domain-specific questions directly from unstructured text. Furthermore, this isolates linguistic comprehension from sequence modeling, allowing us to probe whether models can capture the semantics of scientific literature, resolve complex entity relations, and deliver accurate answers across diverse contexts. The results are listed in Tab. 4. Chemical Entity Recognition. This task entails detecting textual mentions of chemical substances from the inputs and assigning them to specific entities (e.g., chemical compounds). This poses chal17 Task Metric Gemini 2.5-pro GPT oss Non-LLM Experts Specialist LLMs SciReasoner 1.7B SciReasoner 8B Scientific Knowledge Extraction and QA Chemical Entity Recognition ChemicalProtein Interaction Extraction ChemicalDisease Interaction Extraction General Multiple Choice Question True/False Question Open-Ended Question F1 F1 F1 ACC ACC BertScore 0.71 0.12 0.35 0.94 0.62 0.84 0.55 0.05 0.23 0.87 0.67 0.80 0.75 0.22 0.40 0.65 0.55 0.84 0.81 0.46 0.66 0.92 0.64 0. 0.92 0.83 0.97 0.99 0.68 0.87 Table 4 Evaluation results of scientific knowledge extraction and QA tasks. Bold indicates the best performance, and underline indicates the second best. The names of expert models and specialist LLMs for each task are provided in Appendix Table. 11. lenges for models since the heterogeneity of chemical nomenclature. Accordingly, the performance of this task provides sensitive probe of an LLMs domain competence in biomedical text understanding. Chemical-Protein Interaction Extraction This task evaluates language models on chemicalprotein interaction (CPI) annotation: given biomedical text snippets, the model must (i) detect mentions of chemical entities and protein/gene targets, (ii) link candidate pairs, and (iii) assign an interaction label that captures mechanism and polarity (e.g., binding). The task requires grounding each prediction to supporting evidence spans and correctly handling context , thereby probing biochemical and pharmacological knowledge under realistic nomenclature variability. Chemical-Disease Interaction Extraction This task probes extraction of chemical-induced disease (CID) relations from biomedical literature. For the input, the model must (i) detect mentions of chemicals and diseases, (ii) link candidate pairs, and (iii) decide whether the passage asserts causal, adverse relation in which exposure to the chemical induces or exacerbates the disease, citing the supporting span. Accurate CID extraction enables downstream applications in toxicology, pharmacovigilance, and disease-mechanism mapping. General Multiple Choice Question The task is to select the correct option for each question, probing factual knowledge, conceptual understanding, and applied reasoning relevant to biology, chemistry, and other relevant fields. True/False Question Following the practice in Mol-Instructions [81], this task requires the language models to answer research questions with positive/negative/uncertain results, without being given the references from PubMed. Open-Ended Question Open-ended questions require free-form responses to input content related to biomedicine, without following fixed format. Open-ended QA tests genuine biomedical understanding, demands clear self-contained explanations, mirrors real use. Summary of Evaluation Results Scaling from SciReasoner-1.7B to SciReasoner-8B yields consistent gains across scientific knowledge extraction and QA. The SciReasoner-8B model surpasses prior Specialist LLMs on every tasks. The most pronounced improvements appear on relation extraction (chemicalprotein and chemicaldisease), reflecting stronger entity linking, synonym/acronym disambiguation, and evidence grounding; chemical entity recognition also benefits, indicating sharper 18 Metric Gemini 2.5-pro GPT oss Non-LLM Experts Specialist LLMs SciReasoner 1.7B SciReasoner 8B Task Molecular Property Regression ESOL LIPO Physicochemical Prediction Material Property Regression MPRegression SNUMATRegression JARVISDFT JARVISQETB GNoME hMOF Cantor-HEA QMOF OQMD OMDB RMSE RMSE MAE MAD MAE Biology Multi-omics Property Regression Fluorescence StabilityBiology-Instructions Thermostability Enhancer Activity Prediction APA Isoform Prediction Mean Ribosome Loading Prediction Programmable RNA Switches CRISPR On Target Prediction siRNA Efficiency Prediction Spearman Spearman Spearman PCC R2 R2 R2 Spearman Mixed-score 1.28 1.21 0.027 1.32 1.07 1.09 0.53 1.28 0.65 0.77 1.24 1.20 0.82 -0.43 -9.15 5.47 -4.24 0.13 0.00 0.01 -3.64 38.75 3.45 1.65 1.329 0.95 1.00 0.88 0.50 0.73 0.67 0.45 0.86 0.65 0.93 3.20 -7.79 6.29 -1.29 0.01 0.02 0.01 6.21 40.99 5.32 1.83 4.10 59.44 15.60 1.48 8.40 2. 6.02 1.55 69.00 79.00 78.00 68.00 50.82 78.00 55.67 44.10 49.38 1.04 1.01 0.013 0.76 1.16 0.70 1.05 0.50 0.66 0.87 0.93 1.16 1. 2.57 60.25 45.07 57.24 59.01 47.64 26.65 2.87 42.92 1.21 0.94 0.003 1.08 0.82 0.002 4.984.67 2.182.18 5.115.28 138.59148.71 19.0518.46 1.051.04 7.407.38 8.558.76 5.295.50 1.401.41 5.125.21 1.962.22 5.215.59 126.61137.61 22.0321.81 0.990.99 7.187.02 8.077.94 5.435.67 1.391.42 69.3670.82 63.2555.67 56.2553.17 65.0770.20 87.9486.19 50.8254.56 39.0038.1 14.35 62.5664.13 66.2468.49 64.4159.03 58.5554.45 64.3952.08 85.8787.92 60.4464.5 43.5529.20 25.81 59.6761.94 Table 5 Evaluation results of property prediction tasks. Bold indicates the best performance, and underline indicates the second best. The names of expert models and specialist LLMs for each task are provided in Appendix Table. 12. denotes that only randomly sampled 1000 test samples were utilized for evaluation in the corresponding task. This measure is adopted because extremely largescale test sets exist for certain tasks, which would lead to substantial testing costs. domain-specific span detection. In QA, multiple-choice performance approaches ceiling, open-ended questions and true/false questions remain stable yet competitive with strong general-purpose models. Overall, scaling primarily boosts entity-dense, relation-heavy reading while preserving high performance on general scientific QA. 4.5. Property Prediction Tasks We benchmark our single instruction-tuned model on unified suite of composition-to-property and sequence-to-property regressions spanning materials, molecules, and multi-omics. Inputs include chemical formulas, SMILES, and DNA/RNA/protein sequences, with targets covering structural, electronic, thermodynamic, and biochemical properties. The results are listed in Tab. 5. Molecular Property Regression. We consider three molecular regression tasks from SMolInstruct [100] and Mol-Instructions [81]: ① ESOL Prediction, ② LIPO Prediction, and ③ Physicochemical Prediction. These tasks perform single-output regression to quantitatively predict fundamental molecular characteristics directly from biochemical representations. Estimated Solubility (ESOL) Prediction. The ESOL Property Prediction task on property prediction for molecule. The model receives chemical structure in the form of SMILES string. Its goal is to predict the aqueous solubility for that molecule. This is regression task, requiring single continuous value as output. The predicted value represents the logarithm of 19 solubility (logS) in mol/L. Lipophilicity (LIPO) Prediction. The LIPO Property Prediction task predicts the octanol-water distribution coefficient (logD). The model takes molecules SMILES string as input. It calculates the logD value under the specific condition of pH 7.4. The final output is single numerical value for this coefficient. Physicochemical Prediction. Physicochemical prediction leverages molecules structural details to estimate its core physicochemical traits. By inferring unmeasured attributes, it guides the early prioritization of promising candidates and reduces experimental burden. The above task takes an SMILES string as input, and then requires the model to output specific property, as shown in the following example for the Physicochemical Prediction task. Example: Instructions: Could you give me the LUMO energy value of this molecule? <SMILES> CC1=CC2C(CO)C2C1 </SMILES> Response: 0.0191 Evaluation Metric. For ESOL Prediction and LIPO Prediction, we use RMSE as the evaluation metric. For Physicochemical Prediction, we use MAE as the evaluation metric. Material Property Regression Follow LLM4MAT [105], we evaluate the model using data sources from multiple databases, i.e., ① MPregression [111], ② SNUMATregression [98], ③ JARVIS-QETB [112], ④ JARVIS-DFT [113, 114], ⑤ GNoME [47], ⑥ Cantor-HEA [115], ⑦ QMOF [116, 117], ⑧ OQMD [118],⑨ OMDB [118], and ⑩ hMOF [119], organized into structure-to-property task designed to predict various crystal properties based on input chemical formulas. For Materials Project (MP), we predict band gap (eV), unit-cell volume (Å3), formation energy per atom (eV/atom), energy above hull (eV/atom), total energy per atom (eV/atom), thermodynamic stability flag, mass density (g/cm3), atomic density (atoms/Å3) and Fermi energy (eV). For SNUMAT, we predict both HSE and GGA band gaps (eV), the corresponding optical band gaps (eV), spinorbit coupling flag. For Jarvis-QETB, we predict energy per atom (eV/atom), indirect gap (eV), Fermi energy (eV) and final energy (eV). For Jarvis-DFT, our model outputs formation energy per atom (eV/atom), band gap (eV), spectroscopic limited maximum efficiency (SLME), electronic spillage, total energy (eV), static dielectric constant (x-direction) at the meta-GGA TBmBJ level of theory (MEPSX), maximum electric field gradient (V/Å2), average electron mass, maximum piezoelectric coefficient (C/m2 and pC/N), dielectric constant, nand p-type Seebeck coefficients (µV/K), power factors (µW/cmK2), exfoliation energy (meV/atom), bulk and shear moduli (GPa), modified BeckeJohnson band gap (eV) and energy above hull (eV/atom). For GNoME, we predict formation and decomposition energies (eV/atom), band gap (eV), corrected total energy (eV), volume (Å3) and density (g/cm3). For Cantor-HEA, we estimate formation energy per atom (eV/atom), energy above hull (eV/atom), volume per atom (Å3/atom) and energy per atom (eV/atom). For QMOF, we predict total energy (eV), band gap (eV), largest cavity diameter and pore-limiting diameter (both Å). For OMDB and OQMD, we predict band gap (eV) and, we also predict formation energy (eV/atom) for OQMD. 20 (a) Density (b) FEPA Figure 8 GNoME distributions (Density vs FEPA) (a) Ehull (b) EPA Figure 9 Materials Project distributions (Ehull vs EPA) For hMOF, we predict maximum and minimum CO2 adsorption capacities (mmol/g), largest cavity diameter (Å), pore-limiting diameter (Å), void fraction, and both gravimetric (m2/g) and volumetric (m2/cm3) surface areas. We follow the instruction template in LLM4MAT [105]. Example: Instructions: You are material scientist. and predict the requested property. chemical formula: KPrMnNbO6 property name: Response: {volume : 128.62405239223665} Respond in JSON format. volume"
        },
        {
            "title": "Look at the chemical composition",
            "content": "Evaluation Metric. For the above tasks, the metric is: MAD = 1 𝑛 𝑛 𝑖=1 𝑦𝑖 𝑦 MAE = 1 𝑛 𝑛 𝑖=1 ˆ𝑦𝑖 𝑦𝑖"
        },
        {
            "title": "MAD\nMAE",
            "content": "= (cid:205)𝑚 MAD 𝑗 MAE 𝑗 𝑗=1 𝑆 𝑗 (cid:205)𝑚 𝑗=1 𝑆 𝑗 (6) As shown in Fig. 8, and Fig. 9, our model is capable of predicting broad spectrum of structural, electronic and thermodynamic properties across multiple materials databases. Biology Multi-omics Property Regression For multi-omics (i.e., DNA, DNA and protein) tasks, we consider 9 regression tasks in Biology-Instructions [85]: ① Fluorescence Prediction, ② Stability Prediction, ③ Thermostability Prediction, ④ Enhancer Activity Prediction, ⑤ Alternative Aolyadenylation (APA) Isoform Prediction, ⑥ Mean Ribosome Loading Prediction, ⑦ Programmable RNA Switches, ⑧ CRISPR On Target Prediction, and ⑨ siRNA Efficiency Prediction, they perform single-output or multi-output regression to predict diverse biological properties. 21 (a) Enhancer Activity Prediction (b) Mean Ribosome Loading Prediction Figure 10 Biology Multi-omics distributions. Fluorescence Prediction. This task presents regression task to predict the fluorescence of higher-order green fluorescent protein (GFP) mutants. Each protein sequence is mapped to the logarithm of its fluorescence intensity. Stability Prediction. This task presents regression task to predict the intrinsic stability of proteins. The model maps each protein sequence to continuous stability score reflecting its ability to maintain its fold above specific concentration threshold (e.g., the EC50 value). Thermostability Prediction. This task is to predict protein thermostability, defined as proteins stability at elevated temperatures. Enhancer Activity Prediction. This task is multi-output regression task to predict the activity levels of enhancer regions from DNA sequences. The model output two numeric values corresponding to the enhancers housekeeping and developmental activity levels. Alternative Aolyadenylation (APA) Isoform Prediction. This tesk presents regression task to predict APA isoform usage from RNA sequences. The model outputs proportion between 0 and 1 for each isoform, which represents its relative expression and captures the variability in polyadenylation signal processing. Mean Ribosome Loading Prediction. This task presents regression task to predict ribosome loading efficiency from RNA sequences. The model outputs continuous value representing the mean ribosome loading, reported to two decimal places. Accurate prediction of ribosome loading is essential for understanding how cis-regulatory elements, particularly 5 untranslated regions (UTRs), influence translation efficiency. Programmable RNA Switches. This task addresses multi-label regression task to predict the behavior of programmable RNA switches from their sequences. The model outputs three continuous values representing the ON, OFF, and ON/OFF states, each reported to two decimal places. CRISPR On Target Prediction. This task involves regression task to predict the on-target knockout efficacy of single guide RNA (sgRNA) sequences for use in CRISPR systems. siRNA Efficiency Prediction. The model predicts the gene-silencing efficiency of modified small interfering RNA (siRNA). Based on the siRNA and corresponding target sequences, the model outputs numeric value representing the percentage of mRNA remaining after treatment. Evaluation Metric. For Fluorescence Prediction, Stability Prediction, Thermostability, and CRISPR On Target Prediction, the Spearman score is used as the evaluation metric. For APA Isoform Prediction, Mean Ribosome Loading Prediction, and Programmable RNA Switches, the R2 score is adopted as the evaluation metric. For Enhancer Activity Prediction, PCC is used as the evaluation metric: PCC = 𝑖=1(𝑥𝑖 𝑥)( 𝑦𝑖 𝑦) (cid:205)𝑛 𝑖=1(𝑥𝑖 𝑥)2(cid:205)𝑛 𝑖=1( 𝑦𝑖 𝑦)2 (cid:205)𝑛 (7) 22 For siRNA Efficiency Prediction, we consider the metric of Mixed-Score: Mixed-Score = 50% (1 MAE 100 ) + 50% F1 (1 Range_MAE 100 ) where F1 = 2 Precision Recall Precision + Recall and Range_MAE = 1 𝑚 𝑚 𝑗=1 𝑦 𝑗 ˆ𝑦 𝑗 (8) (9) Summary of Evaluation Results Across materials, molecules, and multi-omics, scaling our model from SciReasoner-1.7B to SciReasoner-8B yields consistent accuracy gains, i.e., lower MAE/RMSE for numeric regressions and higher rank-based correlations (e.g., Spearman/PCC) for sequence-toproperty tasks. Relative to baselines, the SciReasoner-8B model matches or surpasses the strongest non-LLM encoder baselines and/or Specialist LLMs on multiple benchmarks in Tab. 5, especially on long-context or multi-output targets. Materials (compositionproperty). The SciReasoner-8B model improves across MP, SNUMAT, JARVIS, GNoME, Cantor-HEA, OQMD/OMDB, QMOF, and hMOF, with notable MAE drops on harder thermodynamic/electronic properties (e.g., energy-above-hull, band gap, density/surfacearea). On subset of properties (see Tab. 5), SciReasoner-8B ties or outperforms encoder-based baselines / Specialist LLMs. Molecules (SMILESproperty). For ESOL and LIPO (RMSE) and physicochemical endpoints (MAE), SciReasoner-8B consistently surpasses SciReasoner-1.7B, with the largest gains on endpoints sensitive to subtle substructure/charge effects. In several cases, SciReasoner-8B attains or exceeds Specialist LLMs, while SciReasoner-1.7B remains competitive but lags at the extremes (very hydrophilic/hydrophobic regimes). Biology multi-omics (DNA/RNA/proteinproperty). The SciReasoner-8B model yields broad lifts in correlation-based metricsFluorescence, Stability/Thermostability, CRISPR on-target, and Mean Ribosome Loadingand improves composite outcomes such as the siRNA Mixed-Score. The largest SciReasoner-1.7BSciReasoner-8B jumps appear in multi-output or context-dependent tasks (e.g., Enhancer Activity with housekeeping/developmental readouts, APA isoform usage). 4.6. Property Classification Tasks Property classification tasks evaluate models capability to assign discrete categorical labels to small molecular, material, genetic, and multi-omics inputs, thereby enabling systematic assessment of structureproperty relationships across various domains. We show the results of property classification in Tab. 6 and Tab. 7. Molecular Classification Molecular classification tasks evaluate models ability to infer discrete biological or pharmacological properties of molecules directly from their structural representations, enabling rapid assessment of drug permeability, toxicity, antiviral potential, and adverse effect risks that are central to drug discovery and safety evaluation. Blood-Brain Barrier Permeability (BBBP) Prediction. The BBBP property prediction task is designed to predict whether given molecule can penetrate the blood-brain barrier. The model uses the SMILES string of molecule as input. It then outputs binary classification result, indicating if the molecule is blood-brain barrier permeable or not. This is crucial property for drug discovery, as it helps determine if potential drug can be effective for central nervous system targets. 23 Task Metric Gemini 2.5-pro GPT oss Non-LLM Expert Models Specialist LLMs SciReasoner-1.7B SciReasoner-8B Molecular Classification BBBP Clintox HIV Sider ACC@1 ACC@1 ACC@1 ACC@1 71.57% 46.70% 26.39% 52.78% 69.86% 26.7% 38.25% 67.7% Material Property Classification AUC AUC MPclassification SNUMATclassification 0.58 0.59 Gene-centric Association Classification gSymbol2Tissue gSymbol2Cancer gName2Cancer Precision Recall F1 score Precision Recall F1 score Precision Recall F1 score 0.01 0.01 0.01 0.00 0.00 0.00 0.00 0.00 0.00 0.53 0. 0.03 0.09 0.04 0.01 0.02 0.01 0.01 0.04 0.01 0.72 0.72 75.10% 93.10% 96.70% 70.70% 79.70% 92.36% 96.79%92.40% 70.00%66.20% 82.74% 91.67% 96.81%95.90% 68.53%68.70% 0.51 0.58 0.39 0.91 0.53 0.36 0.32 0.32 0.27 0.26 0. 0.700.69 0.690.69 0.720.71 0.660.66 0.41 0.82 0.51 0.75 0.72 0.73 0.68 0.67 0.67 0.41 0.83 0.52 0.81 0.81 0.81 0.80 0.79 0.79 Table 6 Evaluation results of molecular classification tasks, material property classification tasks, and gene-centric association classification tasks. Bold indicates the best performance, and underline indicates the second best. The names of expert models and specialist LLMs for each task are provided in Appendix Table. 13. denotes that only randomly sampled 1000 test samples were utilized for evaluation in the corresponding task. This measure is adopted because extremely large-scale test sets exist for certain tasks, which would lead to substantial testing costs. Clinical Toxicity (ClinTox) Prediction. The ClinTox property prediction task is designed to assess the toxicity of molecule. The model takes the molecules SMILES string as input. It then provides binary classification indicating whether the molecule is toxic or not based on its chemical structure. This prediction is vital in the early stages of drug development to filter out potentially harmful compounds. HIV Prediction. The HIV property prediction task determines whether molecule can inhibit the replication of the HIV virus. The model uses molecules SMILES string as input. It then performs binary classification to predict if the molecule is an HIV replication inhibitor or not. This is an essential step in identifying potential antiviral drugs for treating HIV/AIDS. Side Effect Resource (SIDER) Prediction. The SIDER property prediction task predicts whether molecule causes specific side effect. The model takes molecules SMILES string as input. It then provides binary classification indicating if the molecule is associated with given side effect. This task is crucial for drug safety, helping researchers to identify potential adverse drug reactions early in the development process. Material Property Classification We consider two material property classification tasks, MPclassification and SNUMATclassification, which evaluate the ability of model to classify various crystal properties of an input chemical formula. For the Materials Project (MP), it consists of two sub-tasks. One of them is to predict whether the gap of the crystal is formed directly, and the other is to predict whether crystal is stable. Example: Instructions: You are material scientist. composition and predict whether the material is stable. chemical formula: KPrMnNbO6 property name: Response: {is_stable: False} is_stable"
        },
        {
            "title": "Look at the chemical",
            "content": "Respond in JSON. For the SNUMAT dataset, it outputs direct/indirect gap flag. Example: Instructions: You are material scientist. composition, predict whether the band gap is direct (True) or indirect (False). Respond in JSON. chemical formula: is_direct Response: {is_direct: True} PbS property name:"
        },
        {
            "title": "Given the chemical",
            "content": "Evaluation Metric. We use AUC as the evaluation metric for these two tasks. Gene-centric Association Classification. We assess gene-centric knowledge with three multi-label tasks mapping gene identifier (symbol or official name) to tissue expression profiles or curated genecancer associations as in OPI [102]. Tissue Location Prediction from Gene Symbol (gSymbol2Tissue). This task involves predicting the tissue expression profile of protein-coding gene given its gene symbol. Predicting spatial expression patterns aids in understanding protein-specific biological roles and informs biomarker discovery and therapeutic targeting. Cancer Prediction from Gene Symbol (gSymbol2Cancer). Given gene symbol, this task asks the model to identify associated cancer types, using data from the Cancer Gene Census. This task evaluates the models capability to mine gene-cancer associations from existing biomedical knowledge, supporting cancer gene discovery and precision oncology efforts. Cancer Prediction from Gene Name (gName2Cancer). Given gene symbol, this task asks the model to identify associated cancer types, using data from the Cancer Gene Census. This task evaluates the models capability to mine gene-cancer associations from existing biomedical knowledge, supporting cancer gene discovery and precision oncology efforts. Evaluation Metric. We use Precision, Recall, and F1 score as the evaluation metrics for these three tasks. We present the case study of the gSymbol2Cancer task as follows: Biology Multi-omics Property Classification For biology-related (DNA/RNA/protein) classification tasks, we include the following 26 tasks from the Biology-Instructions [85] and Mol-Instructions [81]. These tasks span binary and multi-label classification problems covering molecular interactions, regulatory elements, and functional annotation. Specifically, we first consider 11 binary classification task: Solubility Classification. For given protein sequence, this task addresses binary classification task to predict whether it is soluble or insoluble. Task Metric Gemini 2.5-pro GPT oss Non-LLM Expert Models Specialist LLMs SciReasoner 1.7B SciReasoner 8B Biology Multi-omics Property Classification (binary) SolubilityBiology-Instructions Antibody-Antigen RNA-Protein Epigenetic Marks Prediction TF-Mouse0 TF-Mouse1 TF-Mouse2 TF-Mouse3 TF-Mouse4 Enhancer-Promoter Interaction Promoter-300all Promoter-300notata Promoter-300tata Promoter-coreall Promoter-corenotata Promoter-coretata TF-Human0 TF-Human1 TF-Human2 TF-Human3 TF-Human4 SolubilityPEER StabilityPEER Human PPI Yeast PPI ACC MCC MCC MCC MCC MCC MCC MCC MCC MCC MCC MCC MCC MCC MCC MCC MCC MCC MCC MCC MCC ACC ACC ACC ACC 51.4 5.05 1.20 0.52 0.20 1.24 0.41 2.72 6.61 3.75 24.12 16.65 11.98 12.38 15.64 13.02 -3.46 12.23 2.29 -8.44 7.56 58.40 49.60 52.74 51. Biology Multi-omics Property Classification (multi-label) Protein Function Domain/Motif FunctionEC Non-coding RNA Func. Classif. Modification Prediction Subcellular Localization Fold Type Numerical EC (New-392) Numerical EC (Price-149) Keywords (CASPSimilarSeq) Keywords (IDFilterSeq) Keywords (UniProtSeq) GO Terms (CASPSimilarSeq) GO Terms (IDFilterSeq) GO Terms (UniProtSeq) ROUGE-L ROUGE-L Fmax ACC AUC Accuracy Accuracy Precision Recall F1 Precision Recall F1 Precision Recall F1 Precision Recall F1 Precision Recall F1 Precision Recall F1 Precision Recall F1 Precision Recall F1 0.20 0.19 3.3 17.8 51.19 0.02 0.01 0 0 0 0 0 0 0.04 0.03 0.03 0.03 0.02 0.01 0.01 0 0 0 0 0 0 0 0 0 0 51.22 2.78 5.23 0.84 4.75 0.38 6.99 9.63 2.07 1.69 -3.56 -2.40 13.04 -3.57 -6.37 17.97 3.24 3.85 5.19 -1.27 15.06 10.91 38.21 58.2 44.90 0.05 0.02 22.86 11.54 50.05 0 0 0 0 0 0 0 0 0 0.02 0.01 0 0.02 0.01 0.01 0.03 0.01 0 0 0 0 0 0 0 0 0 77.00 54.90 93.2 58.83 64.23 86.28 81.28 73.49 52.66 53.59 91.01 94.34 79.43 70.92 71.58 78.15 71.99 76.06 66.52 58.54 77.43 74.37 58.30 85.09 66.16 0.57 0.53 88.9 85. 84.00 0.59 0.48 0.49 0.58 0.47 0.49 - - - - - - - - - - - - - - - - - - 63.02 1.06 74.26 3.64 26.50 66.96 63.98 32.02 9.18 4.13 78.97 83.45 38.50 59.98 62.85 39.36 56.56 61.17 46.92 34.16 51.86 71.40 63.50 84.8 60.40 0.43 0.46 19.79 63.09 59.06 0.78 0.61 0.37 0.34 0.35 0.07 0.07 0.07 0.81 0.74 0.76 0.84 0.80 0.82 0.86 0.82 0.83 0.76 0.74 0.75 0.74 0.73 0.72 0.76 0.74 0.74 62.6767.3 41.5945.43 86.4785.97 14.1618.09 41.7939.77 68.9572.07 71.5769.08 44.8145.10 27.3428.45 8.186.75 82.94 88.46 44.38 61.49 61.72 53.67 59.33 62.22 47.80 38.96 56.43 70.1470.8 68.4168.90 76.79 53.81 0.630.63 0.560.56 60.6560.33 80.3988.8 53.06 0.84 0.34 0.40 0.39 0.39 0.10 0.10 0.10 0.83 0.80 0.81 0.86 0.85 0.85 0.85 0.90 0.88 0.81 0.81 0.80 0.78 0.77 0.77 0.84 0.82 0. 71.6169.4 45.548.84 84.9788.8 17.2421.26 43.5240.42 71.0375.98 75.0768.52 52.9853.31 28.6228.20 14.408.12 83.01 87.04 51.37 62.56 63.52 46.94 61.07 63.47 48.02 40.89 56.10 73.9975.4 64.7064.3 73.42 60.41 0.970.96 0.970.90 77.2677.56 86.6593.4 59.94 0.91 0.48 0.55 0.55 0.54 0.21 0.23 0.22 0.86 0.83 0.84 0.89 0.89 0.88 0.92 0.91 0.91 0.86 0.86 0.86 0.83 0.82 0.81 0.88 0.87 0.86 Table 7 Evaluation results of biology multi-omics property classification tasks. Bold: best, underline: the second best. Details of expert models and specialist LLMs are provided in Appendix Table. 14. denotes that only randomly sampled 1000 test samples were utilized for evaluation in the corresponding task to avoid substantial testing costs."
        },
        {
            "title": "Cancer Prediction from Gene Symbol Case",
            "content": "Input Get the cancer name that is related to the given gene symbol: PLAG"
        },
        {
            "title": "Our model",
            "content": "GPT-5 salivary adenoma, lipoblastoma salivary adenoma; lipoblastoma Pleomorphic adenoma (salivary gland). Figure 11 Case study of cancer prediction from gene symbol (PLAG1). The reference lists two associated tumors: salivary adenoma and lipoblastoma. Our model reproduces both exactly (precision = 1.00, recall = 1.00, F1 = 1.00). GPT-5 returns only pleomorphic adenoma (salivary gland), which maps to the salivary adenoma category but misses lipoblastoma (precision = 1.00, recall = 0.50), indicating partial coverage. Antibody-Antigenqing. This is binary classification task to predict interactions between antibody and antigen sequences. The ultimate goal is to leverage this understanding to predict effective antibodies against new viral variants. RNA-Protein Interaction Prediction. This is binary classification task to predict interactions between non-coding RNAs (ncRNAs) and proteins based on their respective sequences. Most ncRNAs must interact with proteins to perform their biological functions. Therefore, identifying these interactions is crucial for elucidating the mechanisms underlying ncRNA-mediated biological activities . Epigenetic Marks Predictions. For given DNA sequence, this binary classification task predicts the presence of chemical modifications in DNA sequence. TF-Mouse. This is binary classification task, to determine whether specific regions with transcription factors binding in the DNA sequences or not. These transcription binding sites (TBS) are critical for controlling the initiation, enhancement, or repression of transcription. Enhancer-Promoter Interaction Prediction. This task addresses binary classification task focused on identifying interactions between enhancer and promoter regions within given pair of DNA sequences. Predicting these enhancer-promoter interactions is crucial for understanding the complex regulatory networks that govern gene activity. TF-Human Prediction. This is binary classification task, to determine whether specific regions with transcription factors binding in the DNA sequences or not. Promoter-300&Promoter-Core. These two tasks are binary classification problems focused on identifying the presence or absence of promoter regions in DNA sequences. The first task, Promoter Detection 300, involves detecting these regions within 300 base pair (bp) window that includes both the core promoter and surrounding regulatory elements. The second task, Promoter Detection Core, focuses on detecting shorter, core sequence (typically 50-100 bp) located directly upstream of the transcription start site. Human PPI. This task presents binary classification task to predict protein-protein interactions (PPI) in humans. Based on pairs of human proteins, the model outputs Positive/Negetive sentences indicating whether the two proteins interact . Yeast PPI. This task presents binary classification task to predict protein-protein interactions (PPI) in yeast. Based on pairs of yeast proteins, the model outputs Positive/Negetive sentences indicating whether the two proteins interact. 27 In summary, the above binary classification task is to evaluate whether the given DNA/RNA/protein sequence is positive or negative for specific task. Using solubility classification as an example: Example: Instructions: <protein>MKALCLLLLPVLGLLVSSKTLCSMEEAINERIQEVAGSLIFRAISSIGLECQSVT SRGDLATCPRGFAVTGCTCGSACGSWDVRAETTCHCQCAGMDWTGARCCRVQPLEHHHHHH</protein> This task predicts if protein is likely to be soluble or insoluble. protein be expected to be soluble? Response: Solubility is not predicted for this protein."
        },
        {
            "title": "Can this",
            "content": "More example please refer to the Appendix. Then, we consider 15 multi-label&multi-class classification tasks: Domain/Motif Classification. This task requires models to pinpoint folded structural domains and conserved motifs within protein sequence. These annotations provide concise information of protein architecture that aids functional inference and downstream analyses. Protein Function classification. Protein function classification maps an input protein sequence onto standardized annotations across the three GO ontologies: cellular component, biological process, and molecular function. This could enable high-throughput, comprehensive annotation of novel proteins. Function-EC Classification. For give protein sequence, this multi-label classification task predicts enzyme function by annotating protein sequences with their corresponding Enzyme Commission (EC). Non-coding RNA Function Classification. This task presents multi-class classification task for assigning non-coding RNA (ncRNA) sequences to one of 13 functional classes (e.g., tRNA, miRNA, riboswitch). Accurate classification is essential for understanding the diverse regulatory roles of ncRNAs in biological processes and disease. Modification Prediction. This task addresses multi-label classification task to predict posttranscriptional modifications in RNA sequences, identifying one or more types from set of 12 common modifications (e.g., m6A, m1A, m5C). The precise identification of these modification sites is essential for understanding RNAs regulatory mechanisms and its roles in diverse biological processes. Fold Type Classification. This task involves assigning protein to one of 1195 fold types based on its amino acid sequence. Fold types reflect fundamental tertiary structural patterns, even across proteins with low sequence similarity. Subcellular Localization Classification. This task requires predicting the subcellular localization (e.g., nucleus, cytoplasm, membrane) of protein in eukaryotic cells based on its sequence. Subcellular location is tightly linked to protein function and disease relevance, making this task vital for biological interpretation and drug discovery. Numerical EC Classification. This task entails predicting the GO terms associated with protein sequence. GO annotations describe protein functions across three dimensions: molecular function, biological process, and cellular component. Unlike the Function-EC Classification task, this task outputs four numerical levels of EC number system. Moreover, since the test data comes from two distinct sets, i.e., New-392 and Price-149, we therefore consider these two datasets are two sub-tasks of Numerical EC Classification. We add extra spaces between the numbers and the periods to make sure the the four numbers are tokenized properly, for example, 28 (a) GO (CASPSimilarSeq) (b) GO (IDFilterSeq) (c) GO (UniProtSeq) Figure 12 Gene Ontology (GO) term classification measured as the number of correctly predicted attributes per sequence (higher is better). Boxwhisker plots compare two model sizes (SciReasoner8B, blue; SciReasoner-1.7B, red) on three evaluation sets: (A) CASPSimilarSeq, (B) IDFilterSeq, and (C) UniProtSeq. Diamonds mark the mean. Across all datasets, the SciReasoner-8B model shows higher medians and upper quartiles than the SciReasoner-1.7B model, with longer upper whiskers (reaching 910 correct terms vs. 58), indicating more high-quality predictions. Gains are most pronounced on IDFilterSeq and UniProtSeq, suggesting stronger generalization beyond CASP-similar sequences. Example: Instructions: What is the input protein sequences EC identifier? <protein>MTAEPATKKIKLELSDPSEPLTQSDVIAFQKEALFRCINRRRVDFEALRKQYELSRRE...</pro tein> Response: 2 . 3 . 2 . 27 Gene Ontology (GO) Terms Classification. This task entails predicting the GO terms associated with protein sequence. GO annotations describe protein functions across three dimensions: molecular function, biological process, and cellular component. This tasks include three different data for testing, (i) CASPSimilarSeq: the sequences whose are similar to the sequences released by CASP14. (ii) IDFilterSeq: this version uses ID-filtered sequences to reduce redundancy. (iii) UniProtSeq: this versions protein sequences are from the UniProt database. We therefore consider the above three data soucres are three sub-tasks of GO Terms Classification. Function Keywords Predictions. This task targets the prediction of UniProtKB keywords associated with protein, spanning 10 categories: biological process, cellular component, coding sequence diversity, developmental stage, disease, domain, ligand, molecular function, posttranslational modification, and technical term. These keywords provide concise yet informative summaries of protein characteristics and functions. Similar to the Terms Classification, we regard the (i) CASPSimilarSeq, (ii) IDFilterSeq, and (iii) UniProtSeq are three sub-tasks of Function Keywords Prediction. In Fig. 12, we compare Gene Ontology (GO) term classification performance between the SciReasoner1.7B and SciReasoner-8B models across three evaluation sets. The boxwhisker plots show that the SciReasoner-8B model consistently achieves higher medians and upper quartiles of correctly predicted attributes per sequence, with upper whiskers extending to 910 correct terms. These improvements are most evident on IDFilterSeq and UniProtSeq, highlighting stronger generalization beyond CASP-similar sequences. We present the case study of the Function Keywords Prediction task in Fig. 13."
        },
        {
            "title": "Function Keywords Prediction Case",
            "content": "Input Please give the functional keywords of the following protein sequences. <protein>MVEAFCATWKLTNSQNFDEYMKALGVGFATRQVGNVTKPTVIISQEGDKVV IRTLSTFKNTEISFQLGEEFDETTADDRNCKSVVSLDGDKLVHIQKWDGKETNFVREIKDGKMVMTLTFGDV VAVRHYEKA</protein>"
        },
        {
            "title": "Reference",
            "content": "3D-structure ; Acetylation ; Alternative splicing ; Cytoplasm ; Direct protein sequencing ; Lipid-binding ; Reference proteome ; Transport Our model Acetylation; Cytoplasm; Direct protein sequencing; Lipid-binding; Reference proteome; Transport GPT-5 Binding; Metal-binding; Oxidoreductase Figure 13 Case study of functional keyword prediction for single protein sequence. The curated reference lists eight UniProtKB keywords. Our model correctly recovers six of them (Acetylation, Cytoplasm, Direct protein sequencing, Lipid-binding, Reference proteome, Transport), yielding 100% precision and 75% recall. By contrast, GPT-5 returns generic terms (Binding, Metal-binding, Oxidoreductase) with no overlap with the reference. Summary of Evaluation Results Across all classification tasks, scaling from SciReasoner-1.7B to SciReasoner-8B consistently improves performance, often surpassing GPT baselines (GPT-o3/oss) and approaching specialist systems. Molecular Classification. On BBBP, ClinTox, HIV, and SIDER, the 8B model achieves higher accuracy and AUC than 1.7B, reducing toxicity false positives and improving antiviral recall, showing stronger capture of structureactivity relationships. Material Property Classification. For MPclassification and SNUMATclassification, 8B improves AUC by about 3% over 1.7B, narrowing the gap with expert material models and outperforming general-purpose baselines. Gene-centric Association Classification. On gSymbol2Tissue, gSymbol2Cancer, and gName2Cancer, the 8B model gains up to 15 F1 over 1.7B, recovering curated genedisease associations more completely than GPT-5 or smaller models. Biology Multi-omics Classification. The largest relative gains appear here: 8B improves MCC/accuracy by 210 points across several tasks, and achieves state-of-the-art in multi-label GO and UniProt keyword prediction, with broader and higher-quality annotation coverage. Overall, 1.7B 8B scaling delivers robust gains across molecules, materials, genes, and multi-omics, positioning the 8B model as best or second-best on most tasks and validating unified cross-domain pretraining for discrete property inference. 4.7. Generation and Design Tasks We leverage the generation and design tasks to assess models ability to create novel scientific entitiesranging from molecules, proteins, RNAs to materialseither in an unconditional manner or under explicit structural and functional constraints, thereby providing rigorous test of generative 30 Metric Gemini 2.5-pro GPT oss Non-LLM Experts Specialist LLMs SciReasoner 1.7B SciReasoner 8B Task Unconditional Generation Uncondi. Molecular Generation Uncondi. Protein Generation Uncondi. RNA Generation Uncondi. Material Generation Conditional Material Generation Composition to Material Bulk to Material Validity Unique Avg. Length Diversity Validity Avg. MFE SMACT SMACT Precision Novelty SMACT Success rate Conditional Small Molecular Generation Forward Synthesis Prediction Forward Reaction Prediction Reagent Prediction RetrosynthesisMol-Instructions RetrosynthesisUSPTO-50K RetrosynthesisSMol-Instruction Molecule Generation Description-Guided Molecule Exact Match Exact Match Exact Match Exact Match Exact Match Exact Match Exact Match Exact Match Conditional Biochemical Generation Function-guided Protein Design Catalytic Activity Max. Normalized SW ROUGE-L 0.568 0.042 128.87 0.62 0.93 N/A 89.70 0.952 0. 318.91 1.00 0.99 -417.58 N/A N/A N/A N/A N/A N/A 0.32 0.55 0.04 0.20 0.23 0.12 0.25 0.06 0.00 0.25 N/A N/A N/A N/A N/A 0.33 0.36 0.00 0.05 0.12 0.08 0.03 0.04 0.00 0.02 0. 0.26 0.64 0.61 0.11 0.97 0.72 0.968 0.966 284.50 0.97 -177.1 66.07 83.36 98.44 97.13 94.75 0.63 0.54 0.13 0.41 0.70 0.33 0.19 0.002 0.864 0.988 280.10 1.00 0.80 -178.85 55.06 86.07 99.89 85.64 88.92 7. 0.971 0.982 284.70 0.91 0.99 -200.14 88.18 85.85 99.96 84.81 87.49 7.05 0.690.81 0.96 0.19 0.72 0.680.72 0.410.43 0.390.34 0.10 0.750.83 0.99 0. 0.83 0.720.74 0.460.47 0.480.46 0.12 0.43 0.52 0.960.90 1.001.00 0.770.78 0.990.95 Table 8 Evaluation results of generation and design tasks. N/A indicates that the model cannot generate samples that meet the required specifications. Bold indicates the best performance, and underline indicates the second best. The names of expert models and specialist LLMs for each task are provided in Appendix Table. 15. denotes that only randomly sampled 1000 test samples were utilized for evaluation in the corresponding task. This measure is adopted because extremely largescale test sets exist for certain tasks, which would lead to substantial testing costs. priors and controllable design capabilities across diverse scientific domains. We show the results of generation and design tasks in Tab. 8. Unconditional Generation Tasks. To probe the models generative prior, we evaluate unconditional generation across four domains: RNA sequences, inorganic material compositions, small molecules, and proteins. In this setting, the model receives no task-specific conditioning beyond minimal format requirements (e.g., valid alphabet/grammar, length or syntax ranges) and must sample plausible candidates directly from its learned distribution. Furthermore, these tasks could provide seed candidates for downstream, property-conditioned design. Unconditional RNA generation. The unconditional RNA generation task aims to produce novel RNA sequences without any input conditions, using the four types of nucleotides. We show the visualization of unconditional RNA generation in Fig. 14. Example: Instructions: Please generate novel RNA sequence of length 50 nt. 31 Figure 14 Visualization of Unconditional Protein Generation. The pLDDT scores of these three RNA sequences are 74.6, 72.3, and 71.5, respectively. These values indicate that the generated structures achieve moderate confidence and highlight the models ability to produce diverse yet structurally plausible candidates. Response: AUGGCUACUGGUAACCGGAAUCCGGAUUCCGGAAUCCGGAAUCCGGAAUCCGGA Unconditional Material Generation. The unconditional material generation task aims to generate novel material compositions without any input constraints. Example: Instructions: Produce material composition with any bulk modulus. Response: {composition: \"Li2OTiO2MnO2\", bulk_modulus:"
        },
        {
            "title": "120.5 GPa}",
            "content": "Unconditional Small Molecular Generation. This task aims to produce novel molecules that are both chemically plausible and structurally diverse, without any guiding input conditions. Example: Instructions: Generate molecule with <SMILES>. Response: <SMILES> CC(=O)CCCCCC(C)C </SMILES> Unconditional Protein Generation. This task is generative task focused on creating new protein sequences or structures without any specific constraints. The purpose of this task is to test our models understanding of the fundamental principles of protein biology and to generate completely new proteins with biological feasibility from scratch. Evaluation Metric. For unconditional molecular generation, 800 sequences are randomly and independently generated, and the proportions of valid sequences and unique sequences is evaluated by using RDKit. For unconditional protein generation, 500 sequences are generated independently at random, and the average length, diversity (non-repetition rate), and validity (whether only containing valid characters) of these sequences are evaluated. For unconditional RNA generation, 5000 sequences are generated independently at random, and the average minimum free energy (MFE) of the valid sequences is evaluated. For unconditional material generation, 5000 materials are generated independently at random, and the percentage of valid materials is evaluated by Semiconducting Materials by Alternative Chemical Toolkit (SMACT). In conditional material generation tasks, the model is Conditional Material Generation Tasks required to generate candidate material compositions that satisfy given constraints, such as specific elemental requirements or target physical properties. These tasks not only assess the models generative capability but also its ability to adhere to underlying chemical and physical principles in material design. Composition to Material Generation. This task asks the model to design materials containing specific elements (e.g., Li, Ti, Mn, Fe, and O). Example: Instructions: Given F, In, Tc, Zr, design material composition. Response: {composition: \"FeInTcZrO4\", stability: \"metastable\"} Bulk to Material Generation. In this task, the model is asked to generate materials with desired bulk modulus. Example: Instructions: Given bulk modulus of 75.321 GPa, propose material composition. Response: {composition: \"Al3Ti\", predicted_bulk_modulus:"
        },
        {
            "title": "74.8 GPa}",
            "content": "Evaluation Metrics. In addition to SMACT, we evaluate model performance using three complementary metrics: Precision, Novelty, and Success Rate. Precision quantifies the proportion of correctly predicted elements relative to the total number of ground-truth elements, averaged across all samples. It reflects how accurately the predicted material matches the specified composition. Novelty measures the proportion of predictions that do not appear in the ground-truth material set, thereby capturing the models ability to generate unseen candidates. Success Rate denotes the fraction of predictions that exactly reproduce all ground-truth elements of sample. It assesses whether the model can fully recover the intended material. Conditional Small Molecular Generation Tasks We evaluate controllable, chemistry-aware generation under explicit conditioning signals, including reactants/reagents, textual descriptions, scaffolds, and target property constraints. These tasks probe whether model can follow chemical instructions, satisfy constraints (e.g., validity, synthesizability, and target properties), and balance novelty with correctness, and we follow the task settings in Mol-Instruction [81] and SMolInstruct [100]. 33 Figure 15 Visualization comparison on molecular generation tasks. Top (Reactant Prediction): given product SMILES (instruction shown), models propose likely reagents. Bottom (Retrosynthesis Prediction): given product SMILES, models suggest probable reactants/precursors. Our model recovers reactants that align with the reference chemistry, whereas baseline models propose off-target species. Forward Synthesis Prediction. The Forward Synthesis task predicts the product of chemical reaction given set of reactants and reagents. The model takes the SMILES strings of the starting materials as input. Its goal is to generate the most probable product molecule, effectively simulating chemical reaction. This task is fundamental to drug discovery and materials science, as it helps chemists predict the outcome of synthesis and explore new reaction pathways. The Molecule Generation task is generative task that creates molecule based on given set of properties and constraints. The model receives natural language description as input. Its goal is to generate valid SMILES string for molecule that satisfies all the specified conditions. This task is highly valuable in drug discovery, as it allows for the design of novel molecules with specific desired characteristics. Description Guided Molecule Design. This is text-conditioned generation task that produces candidate molecules that satisfy explicit, user-specified criteria expressed in natural language (e.g., target activity). By conditioning the generative process on these constraints, the chemical search space is narrowed toward compounds consistent with the desired profile, improving the efficiency of design and optimization in applications such as drug discovery. Forward Reaction Prediction. Forward reaction prediction forecasts the expected products of chemical reaction from specified reactants and reagents. Reagent Prediction. Reagent prediction identifies the optimal catalysts, solvents and auxiliary agents for any given reaction. 34 Retrosynthesis Prediction. Retrosynthesis prediction task is to invert the synthetic processbeginning with the target molecule and working backward to identify viable precursors and reaction sequences. Evaluation Metric. The exact match metric enforces the strictest criterion: prediction is considered correct only if the predicted molecular formula string is identical to one of the ground-truth formulas in both element symbols and their order. Formally, 𝑀exact = 1 𝑁 𝑁 𝑖=1 𝕀(cid:0) 𝑗 𝑃𝑖 = 𝐺𝑖, 𝑗(cid:1) , where 𝑁 denotes the total number of samples, 𝑃𝑖 the predicted formula for the 𝑖-th sample, {𝐺𝑖, 𝑗} the corresponding set of ground-truth formulas, and 𝕀() the indicator function that equals 1 if the condition holds and 0 otherwise. We visualize the outputs in Fig. 15. Specifically, compared with LLaSMol and Gemini-2.5-Pro, our method recovers reagents/reactants that more closely match the reference chemistry in both reactantand retrosynthesis-prediction settings. Conditional Biochemical Generation Tasks Follow Mol-Instructions [81], we assess the models capacity to generate biochemical outputs conditioned on specific inputs, ranging from function-guided protein design to enzyme reaction description from sequences. Function-guided Protein Design. Given functional prompt (e.g., DNA binding), the model outputs corresponding amino-acid sequence wrapped in <protein> tags. This evaluates its ability to map explicit functional requirements to viable sequences. Function-driven protein design speeds up directed evolution, enzyme engineering and drug-target discovery. Catalytic Activity Prediction. Catalytic activity prediction aims to infer the exact chemical transformation an enzyme performs directly from its amino acid sequence. Example: Instructions: Using the protein sequence supplied, identify and describe the enzymatic catalytic activity, with emphasis on the chemical reaction it accelerates: <protein>...</protein> Response: Based on the provided protein sequence, the enzyme appears to facilitate the chemical reaction: (+)-lariciresinol + H(+) + NADPH. (-)-secoisolariciresinol + NADP(+) = Evaluation Metric. For Function-guided protein design, we use Normalized Smith-Waterman score (Normalized SW) as the metric. Summary of Evaluation Results Scaling from SciReasoner-1.7B to SciReasoner-8B strengthens both unconditional and conditional generation across molecules, proteins, RNAs, and materials. Unconditional generation. SciReasoner-8B attains SOTA-level molecular validity while retaining very high uniqueness; it also improves material feasibility (SMACT) beyond prior Specialist LLMs. RNA samples exhibit lower (better) free energies than SciReasoner-1.7B and the LLM baseline. 35 Figure 16 Structural overlay of the proteins: the reference structure is shown in gray and SciReasoner8Bs prediction in green. Conditional material design. With element-set constraints, SciReasoner-8B maintains high chemical validity with near-perfect precision and improved (lower) novelty; under bulk-modulus conditioning, validity is comparable to SciReasoner-1.7B with small trade-off in success rate. Conditional molecular generation. SciReasoner-8B consistently surpasses both SciReasoner1.7B and prior Specialist LLMs across forward synthesis/reaction prediction, reagent selection, retrosynthesis, and text-guided molecule design, with the largest gains in retrosynthesis and top-1 exact matching. Conditional Protein Design. Function-guided protein generation reaches near-ceiling sequence similarity and substantially outperforms prior LLM baselines. As shown in 16, SciReasoner-8B can generate proteins that are very close to the reference protein. More results please refer to the Appendix. Overall, scaling primarily amplifies constraint satisfaction and chemical/structural validity under conditioning, while also strengthening the generative prior in unconditional regimes. 4.8. Reasoning Examples This section provides detailed exposition of the models reasoning capabilities across diverse scientific domains. Each example illustrates the sophisticated chain-of-thought processing employed by the model to arrive at precise predictions and assessments, underscoring its efficacy in tackling complex scientific inquiries. These cases, ranging from materials science to molecular biology, demonstrate the models ability to interpret intricate inputs and generate well-supported conclusions. For each instance, the input prompt, the models detailed thinking content, the final answer, and reference are provided to offer transparent view of the complete reasoning trajectory. 4.8.1. Material Task: SNUMAT Regression As shown in Fig. 17, the SNUMAT Regression example showcases the models capacity to predict material properties based solely on chemical formula. Given the chemical formula LiBa(BS2)3 and request for its Band_gap_GGA, the model initiates coherent reasoning process. It identifies key compositional elements like barium and sulfur, associating them with significant lattice vibrations and semiconducting properties, respectively. The inclusion of boron is further linked to the formation of network covalent structures, which are intrinsically tied to the existence of band gap. This logical progression of linking elemental properties and structural implications to the target property demonstrates robust understanding of fundamental materials science principles, leading to quantitatively precise band gap prediction that closely aligns with the reference value."
        },
        {
            "title": "SNUMAT Classification",
            "content": "Input. You are material scientist. Look at the chemical formula of the given crystalline material and predict its property. The output must be in json format. For example: property_name:predicted_property_value. Answer as precise as possible and in as few words as possible. chemical formula: RbCuCl3 property name: Direct_or_indirect_HSE Thinking Content: <think> The compound RbCuCl3 is analyzed based on its chemical formula. It consists of Rb+, Cu, and Cl ions, likely forming ionic or coordination bonds. The Cu likely has +2 oxidation state, giving it d9 configuration. Considering its probable crystal structure and electronic arrangement, RbCuCl3 is predicted to be an indirect bandgap semiconductor due to the ionic bonding and the potential for indirect electron transitions in its structure. { \"Direct_or_indirect_HSE\": \"indirect\"} </think> Answer: {Direct_or_indirect_HSE : Indirect} Reference: Indirect Figure 17 SNUMAT Classification Example. This figure demonstrates the models reasoning process for predicting the bandgap type of the material RbCuCl3. The thinking content highlights logical chain of deductions: from analyzing the constituent ions and chemical bonding, to identifying the crucial +2 oxidation state and resulting d9 electronic configuration of the Cu ion. 4.8.2. DNA Task: Promoter-300 As shown in Fig. 18, the model is tasked with assessing the promoter potential of 300 bp DNA sequence. The reasoning sequence meticulously evaluates several critical characteristics pertinent to promoter regions. It correctly identifies the sequence length as being within the typical range for promoters and notes the GC-rich content and the presence of CpG islandsboth established indicators of promoter activity. While acknowledging the absence of an explicit TATA box, the model prudently suggests the potential for other transcription factor binding sites. Importantly, it recommends further bioinformatics analysis using specialized tools or databases, demonstrating nuanced understanding of the empirical requirements for definitive promoter identification. This structured, multi-faceted analysis highlights the models ability to synthesize biological cues into well-reasoned assessment, concluding with biologically plausible interpretation. 4.8.3. Protein Task: Solubility As shown in Fig. 19, the Solubility prediction task exemplifies the models analytical prowess in protein biochemistry. Provided with protein sequence, the model systematically dissects its characteristics to predict solubility. The reasoning primarily focuses on the high proportion of hydrophilic and charged amino acids (e.g., Arginine, Lysine, Glutamic acid, Aspartic acid), identifying them as key factors that promote solubility through favorable interactions with water. It further reinforces this conclusion by noting the absence of long hydrophobic regions or transmembrane domains, which are common drivers of aggregation and insolubility. This detailed consideration of amino acid composition and its implications for protein-solvent interactions underscores sophisticated understanding of the 37 Promoter-300 Input. <dna>TTCTTTTTCCGGATTTTCAGCCGGGTCTTCCGGGGATGGAGAGCAAAAGGACTTGG TGCTCTCGGAGAGAGCCTGCAGGGGGCGGGGTGTTGGTAGAAGGAGTGGGAGGGGTTTTACTGTAG CAAGGCGGGGCCTCAGTTGAGTGGGTGGGGCCTGGAGGCAACTAACGCGGCATAGGGCGAGGGCG GGGCCGGCAGGCCGGGGCGGGGCTGGGAAGTAGTCGGGCGGGGTTGTGAGACGCCGCGCTCAGCT TCCATCGCTGGGCGGTCAACAAGTGCGGGCCTGGCTCAGCGCGGGGGG</dna>: Could you assess the potential of this 300 bp DNA as promoter region? Thinking Content: <think> The 300 bp DNA sequence exhibits several features that suggest it might function as promoter region, though definitive confirmation requires further analysis. Heres structured summary of the evaluation: 1. Length and Position: The sequence length of 300 bp is within the typical range for promoter regions, located upstream of the transcription start site. 2. GC Content: The sequence is GC-rich, which is characteristic of GC-rich promoters, potentially indicating strong promoter element. 3. CpG Islands: The presence of multiple CG dinucleotides suggests CpG island, commonly found in promoter regions, supporting the possibility of being promoter. 4. Transcription Factor Binding Sites: While specific motifs like the TATA box are not evident, the sequence may contain other transcription factor binding sites that could drive gene expression. 5. Conservation Analysis: Without checking against known promoters, its difficult to assess conservation, but this could provide additional evidence. 6. Recommendation for Further Analysis: To confirm its potential as promoter, it is recommended to use bioinformatics tools like PromoScanner or compare it with known promoters in databases. In conclusion, while the sequence shows features consistent with promoter region, such as appropriate length, GC content, and CpG islands, further analysis with specialized tools is necessary for confirmation. </think> Answer: Promoter motifs are detectable in the given DNA fragment. Reference: The 300 bp DNA sequence includes recognizable promoter region. Figure 18 DNA Promoter Region Assessment Example. This figure demonstrates the models structured approach to evaluate 300 bp DNA sequence for its potential as promoter region. The thinking content meticulously breaks down the analysis into key biological features and provides recommendation for further experimental validation. biophysical principles governing protein solubility. 4.9. Effectiveness of Pretraining We ablate the contribution of pretraining on ten representative tasks that span gene-regulatory sequence understanding (promoter detection at 300 bp and core promoters at 50100 bp), transcription factor binding (binary classification), small-molecule property prediction (ESOL; regression), naturallanguage protein function generation, two sequencesequence interaction tasks (antibodyantigen and RNAprotein; binary), alternative APA isoform usage (regression), GO prediction, and gene-name to cancer mapping. Metrics are Matthews correlation coefficient (MCC), ROUGE-L, precision, and R2 (higher is better), and RMSE for ESOL (lower is better). Pretraining yields consistent gains on all tasks. Across the nine higher is better tasks, the median absolute improvement is +15.0 percentage points (pp) and the mean improvement is +22.0 pp. The ESOL regression error drops by 98.7% (RMSE 94.541.2194.541.21). The largest"
        },
        {
            "title": "Solubility",
            "content": "Input. <protein>MDAQTIAPGFESVAELFGRFLSEDREYSAQLAAYHRGVKVLDISGGPHRRPDSVTG VFSCSKGVSGLVIALLVQDGFLDLDAEVVKYWPEFGAEGKATITVAQLLSHQAGLLGVEGGLTLAEYNNS ELAAAKLAQMRPLWKPGTAFGYHALTIGVFMEELCRRITGSTLQEIYEQRIRSVTGAHFFLGLPESEEPRY ATLRWAADPSQPWIDPASHFGLSANSAVGDILDLPNLREVRAAGLSSAAGVASAEGMARVYAAALTGLA ANGDRAAVAPLLSEETIQTVTAEQVFGIDRVFGETSCFGTVFMKSHARSPYGSYRAFGHDGASASLGFA DPVYELAFGYVPQQAEPGGAGCRNLELSAAVRKAVTELAQ</protein> [Solubility Prediction Binary Classification] Will the protein generated from this sequence be soluble? Thinking Content: <think> Based on the analysis of the provided protein sequence, the protein is likely soluble. Heres concise summary of the reasoning: 1. Amino Acid Composition: The sequence contains significant number of hydrophilic and charged amino acids such as Arginine (R), Lysine (K), Glutamic acid (E), and Aspartic acid (D). These residues promote solubility by interacting with water. 2. Absence of Long Hydrophobic Regions: While there are some hydrophobic residues, they do not form extensive stretches that could lead to aggregation or membrane insertion, which might reduce solubility. 3. No Obvious Transmembrane Domains: The sequence lacks regions indicative of transmembrane domains, which are typically hydrophobic and contribute to insolubility. 4. Presence of Charged Residues: The abundance of charged residues suggests potential for ionic interactions with water, enhancing solubility. 5. Aromatic Residues: While aromatic residues can sometimes reduce solubility, their distribution in this sequence does not appear to be significant issue. Given these factors, the protein is predicted to be soluble. Conclusion: The protein is likely soluble based on its composition and sequence analysis. </think> Answer: The solubility task forecasts proteins ability to dissolve in solution. This protein is highly soluble. Reference: This task evaluates if protein will dissolve in solution. Yes, this protein is soluble. Figure 19 Protein Solubility Prediction Example. This figure illustrates the models reasoning for predicting protein solubility. The thinking content details an analysis of key factors, such as the high proportion of hydrophilic and charged amino acids and the absence of long hydrophobic regions or transmembrane domains, which collectively lead to the prediction of soluble protein. absolute gains appear on sequencesequence interaction and regulatory tasks (APA, RNAprotein, antibodyantigen), indicating that pretraining most strongly enhances representations that require long-range dependencies, cross-sequence compatibility, and motif composition. Promoter detection at 300 bp benefits more than core-promoter detection at 50100 bp, consistent with the hypothesis that broader context amplifies the value of pretraining. Language-centric tasks (function generation; structured GO terms; gene-namecancer) also improve steadily, showing transfer from pretrained biochemical/biomedical knowledge to textand ontology-grounded outputs. Tab. 9 summarizes per-task results. For proportion-like metrics reported in percent in the raw logs, we rescale to [0,1][0,1]. Absolute deltas are expressed in pp for those metrics and in raw units for RMSE; relative deltas follow the usual convention of (after-before)/before(after-before)/before for higher-is-better metrics and (before-after)/before(before-after)/before for RMSE."
        },
        {
            "title": "Before",
            "content": "After Δ (pp / RMSE) Relative Δ MCC MCC MCC RMSE ROUGE-L Promoter Detection Core Promoter Detection TF-Human ESOL Protein Function Generation AntibodyAntigen Interaction MCC MCC RNAProtein Interaction R2 APA Isoform Usage Precision GO (CASP-SimilarSeq) Precision GeneName Cancer 0.451 0.719 0.515 0.589 0.447 0.529 94.54 1.21 0.755 0.823 0.129 0.416 0.507 0.865 0.311 0.879 0.690 0.840 0.610 0.740 26.8 7.4 8.2 -93.33-93.33 6.7 28.7 35.8 56.8 15.0 13.0 59.4% 14.4% 18.3% 98.7% 8.9% 221.9% 70.6% 182.3% 21.7% 21.3% Table 9 Ablation of pretraining on 10 tasks. Larger is better for MCC/ROUGE-L/Precision/R; lower is better for RMSE. Values are on the [0,1][0,1] scale after rescaling percent-formatted entries; pp denotes percentage points. Notes. (i) Percent-like entries in the raw results (e.g., 12.92) were rescaled to proportion (e.g., 0.12920.1292). (ii) pp =100(after-before)=100(after-before) for non-RMSE metrics. (iii) Relative Δ is computed with respect to the before score; for RMSE we report relative reduction. 5. Conclusion We introduced scientific language foundation model that bridges general-purpose large language modeling with the heterogeneous data and reasoning workflows of the natural sciences. Our framework couples multi-representation pre-training with instruction-driven alignment and reasoning stage that supports both fast responses and step-by-step deliberation. By mapping natural language, DNA/RNA/protein sequences, molecular strings, and materials representations into shared backbone via task-aware tokenization and consistent inputoutput schemas, the model moves beyond narrow, discipline-specific solutions and limited task menus. This design enables single system to support four core capability families central to scientific workproperty prediction/classification, sequence generation and design under constraints, scientific translation across formats, and text/knowledge extractionwhile grounding its outputs through retrieval and tool execution for improved fidelity and verifiability."
        },
        {
            "title": "References",
            "content": "[1] Jason Wei et al. Finetuned language models are zero-shot learners. In: arXiv preprint arXiv:2109.01652 (2021). [2] Tom Brown et al. Language models are few-shot learners. In: Advances in neural information [3] processing systems 33 (2020), pp. 18771901. InternLM Team. Internlm: multilingual language model with progressively enhanced capabilities. 2023. [4] Maxime Oquab et al. Dinov2: Learning robust visual features without supervision. In: arXiv [5] preprint arXiv:2304.07193 (2023). Jing Shao et al. Intern: new learning paradigm towards general vision. In: arXiv preprint arXiv:2111.08687 (2021). 40 [6] Shixiang Tang et al. Humanbench: Towards general human-centric perception with projector assisted pretraining. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023, pp. 2197021982. [7] Yuanzheng Ci et al. Unihcp: unified model for human-centric perceptions. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2023, pp. 1784017852. [8] Yizhou Wang et al. Hulk: universal knowledge translator for human-centric tasks. In: IEEE Transactions on Pattern Analysis and Machine Intelligence (2025). [9] Shansong Liu et al. Mumu-llama: Multi-modal music understanding and generation via large language models. In: arXiv preprint arXiv:2412.06660 3.5 (2024), p. 6. [10] Alexandre Défossez et al. Moshi: speech-text foundation model for real-time dialogue. In: arXiv preprint arXiv:2410.00037 (2024). [11] Qingkai Fang et al. Llama-omni: Seamless speech interaction with large language models, 2024. In: URL https://arxiv. org/abs/2409.06666 (). [12] Zalán Borsos et al. Audiolm: language modeling approach to audio generation. In: IEEE/ACM transactions on audio, speech, and language processing 31 (2023), pp. 25232533. Jean-Baptiste Alayrac et al. Flamingo: visual language model for few-shot learning. In: Advances in neural information processing systems 35 (2022), pp. 2371623736. [13] [14] Peng Wang et al. Ofa: Unifying architectures, tasks, and modalities through simple sequenceto-sequence learning framework. In: International conference on machine learning. PMLR. 2022, pp. 2331823340. [15] Jaemin Cho et al. Unifying vision-and-language tasks via text generation. In: International Conference on Machine Learning. PMLR. 2021, pp. 19311942. [16] Ming Hu et al. Ophclip: Hierarchical retrieval-augmented learning for ophthalmic surgical video-language pretraining. In: arXiv preprint arXiv:2411.15421 (2024). [17] OpenAI. Introducing GPT-5. Accessed: 2025-08-12. 2025. url: https://openai.com/ index/introducing-gpt-5/. [18] An Yang et al. Qwen3 technical report. In: arXiv preprint arXiv:2505.09388 (2025). [19] Meta AI. The Llama 4 herd: The beginning of new era of natively multimodal AI innovation. Accessed: 2025-08-12. 2025. url: https://ai.meta.com/blog/llama-4-multimodalintelligence/. [20] Aixin Liu et al. Deepseek-v3 technical report. In: arXiv preprint arXiv:2412.19437 (2024). [21] Google DeepMind. Gemini 2.5: Our most intelligent AI model. Last updated March 26, 2025. Accessed: 2025-08-12. 2025. url: https : / / blog . google / technology / google - deepmind/gemini-model-thinking-updates-march-2025/. [22] Kai Han et al. survey on vision transformer. In: IEEE transactions on pattern analysis and machine intelligence 45.1 (2022), pp. 87110. [23] Alexey Dosovitskiy et al. An image is worth 16x16 words: Transformers for image recognition at scale. In: arXiv preprint arXiv:2010.11929 (2020). [24] Ashish Vaswani et al. Attention is all you need. In: Advances in neural information processing [25] systems 30 (2017). Jared Kaplan et al. Scaling laws for neural arXiv:2001.08361 (2020). language models. In: arXiv preprint [26] Baolin Peng et al. Instruction tuning with gpt-4. In: arXiv preprint arXiv:2304.03277 (2023). 41 [27] Haotian Liu et al. Visual instruction tuning. In: Advances in neural information processing systems 36 (2023), pp. 3489234916. [28] Wenliang Dai et al. Instructblip: Towards general-purpose vision-language models with instruction tuning. In: Advances in neural information processing systems 36 (2023), pp. 49250 49267. [29] Jason Wei et al. Chain-of-thought prompting elicits reasoning in large language models. In: Advances in neural information processing systems 35 (2022), pp. 2482424837. [30] Shunyu Yao et al. Tree of thoughts: Deliberate problem solving with large language models. In: Advances in neural information processing systems 36 (2023), pp. 1180911822. [31] Maciej Besta et al. Graph of thoughts: Solving elaborate problems with large language models. In: Proceedings of the AAAI conference on artificial intelligence. Vol. 38. 16. 2024, pp. 1768217690. [32] Tianyi Zhang et al. Benchmarking large language models for news summarization. In: Transactions of the Association for Computational Linguistics 12 (2024), pp. 3957. [33] Liyan Tang et al. Evaluating large language models on medical evidence summarization. In: NPJ digital medicine 6.1 (2023), p. 158. [34] Yang Zhang et al. comprehensive survey on process-oriented automatic text summarization with exploration of llm-based methods. In: arXiv preprint arXiv:2403.02901 (2024). [35] Ehsan Hosseini-Asl et al. simple language model for task-oriented dialogue. In: Advances in Neural Information Processing Systems 33 (2020), pp. 2017920191. [36] Hongru Wang et al. survey of the evolution of language model-based dialogue systems. In: arXiv preprint arXiv:2311.16789 (2023). [37] Tao Gong et al. Multimodal-gpt: vision and language model for dialogue with humans. In: arXiv preprint arXiv:2305.04790 (2023). [38] Ming Hu et al. Survey of Scientific Large Language Models: From Data Foundations to [39] Agent Frontiers. In: arXiv preprint arXiv:2508.21148 (2025). Jiaqi Wei et al. From AI for Science to Agentic Science: Survey on Autonomous Scientific Discovery. In: arXiv preprint arXiv:2508.14111 (2025). [40] Lei Bai et al. Intern-s1: scientific multimodal foundation model. In: arXiv preprint arXiv:2508.15763 (2025). [41] Tianbin Li et al. Gmai-vl & gmai-vl-5.5 m: large vision-language model and comprehensive multimodal dataset towards general medical ai. In: arXiv preprint arXiv:2411.14522 (2024). Jin Ye et al. Gmai-mmbench: comprehensive multimodal evaluation benchmark towards general medical ai. In: Advances in Neural Information Processing Systems 37 (2024), pp. 9432794427. [42] [43] Yutao Hu et al. OmniMedVQA: New Large-Scale Comprehensive Evaluation Benchmark for Medical LVLM. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 2024, pp. 2217022183. [44] Chenglong Ma et al. MedITok: Unified Tokenizer for Medical Image Synthesis and Inter- [45] pretation. In: arXiv preprint arXiv:2505.19225 (2025). Josh Abramson et al. Accurate structure prediction of biomolecular interactions with AlphaFold 3. In: Nature 630.8016 (2024), pp. 493500. doi: 10.1038/s41586-024-07487w. 42 [46] Jonathan Stokes et al. deep learning approach to antibiotic discovery. In: Cell 180.4 (2020), pp. 688702. [47] Amil Merchant et al. Scaling deep learning for materials discovery. In: Nature 624.7990 (2023), pp. 8085. [48] David Weininger. SMILES, chemical language and information system. 1. Introduction to methodology and encoding rules. In: Journal of chemical information and computer sciences 28.1 (1988), pp. 3136. [49] Mario Krenn et al. Self-referencing embedded strings (SELFIES): 100% robust molecular string representation. In: Machine Learning: Science and Technology 1.4 (2020), p. 045024. [50] William Pearson and David Lipman. Improved tools for biological sequence comparison. In: Proceedings of the National Academy of Sciences 85.8 (1988), pp. 24442448. [51] Patrick Walters and Regina Barzilay. Applications of deep learning in molecule generation and molecular property prediction. In: Accounts of chemical research 54.2 (2020), pp. 263 270. [52] Jianbo Qiao et al. self-conformation-aware pre-training framework for molecular property prediction with substructure interpretability. In: Nature Communications 16.1 (2025), p. 4382. [53] Longlong Li et al. KolmogorovArnold graph neural networks for molecular property prediction. In: Nature Machine Intelligence (2025), pp. 19. [54] Yizhen Zheng et al. Large language models for scientific discovery in molecular property prediction. In: Nature Machine Intelligence (2025), pp. 111. [55] Kevin Ulmer. Protein engineering. In: Science 219.4585 (1983), pp. 666671. [56] Romas Kazlauskas and Uwe Bornscheuer. Finding better protein engineering strategies. In: Nature chemical biology 5.8 (2009), pp. 526529. [57] Minkyung Baek et al. Accurate prediction of protein structures and interactions using three-track neural network. In: Science 373.6557 (2021), pp. 871876. [58] Mingchen Li et al. Harnessing Unified Multi-modal Sequence Modeling to unveil ProteinDNA Interdependency. In: bioRxiv (2025), pp. 202502. [59] Muhammad Shaban et al. Foundation Model for Spatial Proteomics. In: arXiv preprint arXiv:2506.03373 (2025). [60] Abdenour Hadid, Tanujit Chakraborty, and Daniel Busby. When geoscience meets generative AI and large language models: Foundations, trends, and future challenges. In: Expert Systems 41.10 (2024), e13654. [61] Zhengyang Lin et al. Large language models reveal big disparities in current wildfire research. In: Communications Earth & Environment 5.1 (2024), p. 168. [62] Yuhao Zhou et al. Scientists First Exam: Probing Cognitive Abilities of MLLM via Perception, Understanding, and Reasoning. In: arXiv preprint arXiv:2506.10521 (2025). [63] Xuming He et al. RadarQA: Multi-modal Quality Analysis of Weather Radar Forecasts. In: arXiv preprint arXiv:2508.12291 (2025). [64] Fengxiang Wang et al. OmniEarth-Bench: Towards Holistic Evaluation of Earths Six Spheres and Cross-Spheres Interactions with Multimodal Observational Earth Data. In: arXiv preprint arXiv:2505.23522 (2025). [65] Qizhi Pei et al. Biot5: Enriching cross-modal integration in biology with chemical knowledge and natural language associations. In: arXiv preprint arXiv:2310.07276 (2023). 43 [66] Qizhi Pei et al. Biot5+: Towards generalized biological understanding with iupac integration and multi-task tuning. In: arXiv preprint arXiv:2402.17810 (2024). [67] Di Zhang et al. Chemllm: chemical large language model. In: arXiv preprint [68] arXiv:2402.06852 (2024). Junxian Li et al. Chemvlm: Exploring the power of multimodal large language models in chemistry area. In: Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 39. 1. 2025, pp. 415423. [69] Qian Tan et al. ChemMLLM: Chemical Multimodal Large Language Model. In: arXiv preprint arXiv:2505.16326 (2025). [70] Lei Jiang et al. Chem3DLLM: 3D Multimodal Large Language Models for Chemistry. In: arXiv preprint arXiv:2508.10696 (2025). [71] Liuzhenghao Lv et al. Prollama: protein large language model for multi-task protein language processing. In: IEEE Transactions on Artificial Intelligence (2025). [72] Adibvafa Fallahpour et al. BioReason: Incentivizing Multimodal Biological Reasoning within DNA-LLM Model. In: arXiv preprint arXiv:2505.23579 (2025). [73] Colin Raffel et al. Exploring the limits of transfer learning with unified text-to-text transformer. In: Journal of machine learning research 21.140 (2020), pp. 167. [74] Hugo Touvron et al. Llama 2: Open foundation and fine-tuned chat models. In: arXiv preprint arXiv:2307.09288 (2023). [75] Wei Li et al. Ophora: large-scale data-driven text-guided ophthalmic surgical video generation model. In: arXiv preprint arXiv:2505.07449 (2025). [76] Ying Chen et al. Slidechat: large vision-language assistant for whole-slide pathology image understanding. In: Proceedings of the Computer Vision and Pattern Recognition Conference. 2025, pp. 51345143. [77] Yanzhou Su et al. Gmai-vl-r1: Harnessing reinforcement learning for multimodal medical reasoning. In: arXiv preprint arXiv:2504.01886 (2025). [78] Anubhav Jain et al. Commentary: The Materials Project: materials genome approach to accelerating materials innovation. In: APL materials 1.1 (2013). [79] Helen Berman et al. The protein data bank. In: Nucleic acids research 28.1 (2000), pp. 235242. [80] Kim Pruitt et al. The reference sequence (RefSeq) database. In: The NCBI handbook 2 (2012). [81] Yin Fang et al. Mol-instructions: large-scale biomolecular instruction dataset for large language models. In: arXiv preprint arXiv:2306.08018 (2023). [82] Yingce Xia et al. Naturelm: Deciphering the language of nature for scientific discovery. In: arXiv e-prints (2025), arXiv2502. [83] Gongbo Zhang et al. Unigenx: Unified generation of sequence and structure with autoregressive diffusion. In: arXiv preprint arXiv:2503.06687 (2025). [84] Ross Taylor et al. Galactica: large language model for science. In: arXiv preprint arXiv:2211.09085 (2022). [85] Haonan He et al. Biology Instructions: Dataset and Benchmark for Multi-Omics Sequence Understanding Capability of Large Language Models. In: arXiv preprint arXiv:2412.19191 (2024). 44 [86] Chaoqi Liang et al. Rethinking the bert-like pretraining for dna sequences. In: arXiv preprint arXiv:2310.07644 1 (2023). [87] Peng Ye et al. Genomics-FM: Universal Foundation Model for Versatile and Data-Efficient Functional Genomic Analysis. In: bioRxiv (2024), pp. 202407. [88] Chen Tang et al. UniSTD: Towards Unified Spatio-Temporal Learning across Diverse Disciplines. In: Proceedings of the Computer Vision and Pattern Recognition Conference. 2025, pp. 2921329224. [89] Chaoqi Liang et al. scWGBS-GPT: Foundation Model for Capturing Long-Range CpG Dependencies in Single-Cell Whole-Genome Bisulfite Sequencing to Enhance Epigenetic Analysis. In: bioRxiv (2025), pp. 202502. [90] Lifeng Qiao et al. Model decides how to tokenize: Adaptive dna sequence tokenization with mxdna. In: Advances in Neural Information Processing Systems 37 (2024), pp. 6608066107. [91] Hao Du et al. Split and Merge Proxy: pre-training protein inter-chain contact prediction by mining rich information from monomer data. In: (2023). [92] National Library of Medicine (NLM). PubMed. Launched 1996; accessed: 2025-09-12. 1996. url: https://pubmed.ncbi.nlm.nih.gov/. [93] National Center for Biotechnology Information (NCBI). PubChem. Launched 2004; accessed: 2025-09-12. 2004. url: https://pubchem.ncbi.nlm.nih.gov/. [94] National Center for Biotechnology Information. NCBI Home. Accessed: 2025-08-12. 2025. url: https://www.ncbi.nlm.nih.gov/. [95] RNAcentral Consortium. RNAcentral: comprehensive database of non-coding RNA sequences. Launch announced September 10, 2014; accessed: 2025-08-12. 2014. url: https : / / rnacentral.org/. [96] RDKit Team. RDKit: Open-Source Cheminformatics Software. Accessed: 2025-09-06. url: https://www.rdkit.org/. [97] Materials Project Team. The Materials Project. Launch announced 2011; accessed: 2025-09-06. 2011. url: https://next-gen.materialsproject.org/. [98] SNUMAT Team. SNU material data center. Accessed: 2025-09-06. url: https://www. snumat.com/. [99] Nadine Schneider, Nikolaus Stiefl, and Gregory Landrum. Whats what: The (nearly) definitive guide to reaction role assignment. In: Journal of chemical information and modeling 56.12 (2016), pp. 23362346. [100] Botao Yu et al. Llasmol: Advancing large language models for chemistry with large-scale, comprehensive, high-quality instruction tuning dataset. In: arXiv preprint arXiv:2402.09391 (2024). [101] Minghao Xu et al. Peer: comprehensive and multi-task benchmark for protein sequence understanding. In: Advances in Neural Information Processing Systems 35 (2022), pp. 35156 35173. [102] Hongwang Xiao et al. OPI: An Open Instruction Dataset for Adapting Large Language Models to Protein-Related Tasks. In: Neurips 2024 Workshop Foundation Models for Science: Progress, Opportunities, and Challenges. [103] Zhihan Zhou et al. Dnabert-2: Efficient foundation model and benchmark for multi-species genome. In: arXiv preprint arXiv:2306.15006 (2023). 45 [104] Yuchen Ren et al. Beacon: Benchmark for comprehensive rna tasks and language models. In: Advances in Neural Information Processing Systems 37 (2024), pp. 9289192921. [105] Andre Niyongabo Rubungo et al. LLM4Mat-bench: benchmarking large language models for materials property prediction. In: Machine Learning: Science and Technology 6.2 (2025), p. 020501. [106] Huu Nguyen et al. The Open Instruction Generalist (OIG) Dataset. First published March 10, 2023 (LAION blog); Hugging Face version updated March 20, 2023; accessed August 12, 2025. 2023. url: https://huggingface.co/datasets/laion/OIG. [107] Pin-Lun Hsu et al. Liger kernel: Efficient triton kernels for llm training. In: arXiv preprint [108] arXiv:2410.10989 (2024). Jeff Rasley et al. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In: Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery & data mining. 2020, pp. 35053506. [109] Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. In: arXiv preprint arXiv:2307.08691 (2023). [110] Qiying Yu et al. Dapo: An open-source llm reinforcement learning system at scale. In: arXiv preprint arXiv:2503.14476 (2025). [111] Jain et al. The Materials Project: Materials Genome Approach to Accelerating Materials Innovation. APL Materials, 1 (1): 011002, 2013. 2013. [112] Kevin Garrity and Kamal Choudhary. Fast and accurate prediction of material properties with three-body tight-binding model for the periodic table. In: Physical review materials 7.4 (2023), p. 044603. [113] Kamal Choudhary et al. High-throughput identification and characterization of twodimensional materials using density functional theory. In: Scientific reports 7.1 (2017), p. 5179. [114] Kamal Choudhary et al. Computational screening of high-performance optoelectronic materials using OptB88vdW and TB-mBJ formalisms. In: Scientific data 5.1 (2018), pp. 1 12. [115] Kangming Li et al. Efficient first principles based modeling via machine learning: from simple representations to high entropy materials. In: Journal of Materials Chemistry 12.21 (2024), pp. 1241212422. [116] Andrew Rosen et al. Machine learning the quantum-chemical properties of metalorganic frameworks for accelerated materials discovery. In: Matter 4.5 (2021), pp. 15781597. [117] Andrew Rosen et al. High-throughput predictions of metalorganic framework electronic properties: theoretical challenges, graph neural networks, and data exploration. In: npj Computational Materials 8.1 (2022), p. 112. [118] Scott Kirklin et al. The Open Quantum Materials Database (OQMD): assessing the accuracy of DFT formation energies. In: npj Computational Materials 1.1 (2015), pp. 115. [119] Christopher Wilmer et al. Large-scale screening of hypothetical metalorganic frameworks. In: Nature chemistry 4.2 (2012), pp. 8389. [120] Amir Shanehsazzadeh, David Belanger, and David Dohan. Is transfer learning necessary for protein landscape prediction? In: arXiv preprint arXiv:2011.03443 (2020). [121] Emanuele Rossi et al. ncRNA classification with graph convolutional networks. In: arXiv preprint arXiv:1905.06515 (2019). [122] Joshua Meier et al. Language models enable zero-shot prediction of the effects of mutations on protein function. In: Advances in neural information processing systems 34 (2021), pp. 29287 29303. [123] Bernardo de Almeida et al. DeepSTARR predicts enhancer activity from DNA sequence and enables the de novo design of synthetic enhancers. In: Nature genetics 54.5 (2022), pp. 613624. [124] Nicholas Bogard et al. deep neural network for predicting and engineering alternative polyadenylation. In: Cell 178.1 (2019), pp. 91106. [125] Paul Sample et al. Human 5 UTR design and variant effect prediction from massively parallel translation assay. In: Nature biotechnology 37.7 (2019), pp. 803809. [126] Nicolaas Angenent-Mari et al. deep learning approach to programmable RNA switches. In: Nature communications 11.1 (2020), p. 5057. [127] Han Xu et al. Sequence determinants of improved CRISPR sgRNA design. In: Genome research 25.8 (2015), pp. 11471157. [128] Yuchen Ren et al. COMET: Benchmark for Comprehensive Biological Multi-omics Evaluation Tasks and Language Models. In: arXiv preprint arXiv:2412.10347 (2024). [129] Sameer Khurana et al. DeepSol: deep learning framework for sequence-based protein [130] solubility prediction. In: Bioinformatics 34.15 (2018), pp. 26052613. Jie Zhang et al. Predicting unseen antibodies neutralizability via adaptive graph neural networks. In: Nature Machine Intelligence 4.11 (2022), pp. 964976. [131] Yong Han and Shao-Wu Zhang. ncRPI-LGAT: prediction of ncRNA-protein interactions with line graph attention network framework. In: Computational and Structural Biotechnology Journal 21 (2023), pp. 22862295. [132] Xiaoping Min et al. Predicting enhancer-promoter interactions by deep learning and matching heuristic. In: Briefings in Bioinformatics 22.4 (2021). [133] Liang He et al. Sfm-protein: Integrative co-evolutionary pre-training for advanced protein [134] sequence representation. In: arXiv preprint arXiv:2410.24022 (2024). Jin Su et al. Saprot: Protein language modeling with structure-aware vocabulary. In: BioRxiv (2023), pp. 202310. [135] Zitao Song et al. Attention-based multi-label neural networks for integrated prediction and interpretation of twelve widely occurring RNA modifications. In: Nature communications 12.1 (2021), p. 4011. [136] Tianhao Yu et al. Enzyme function prediction using contrastive learning. In: Science 379. (2023), pp. 13581363. [137] He Cao et al. InstructMol: Multi-Modal Integration for Building Versatile and Reliable Molecular Assistant in Drug Discovery. In: Proceedings of the 31st International Conference on Computational Linguistics. 2025, pp. 354379. [138] Yuqiang Han et al. Retrosynthesis prediction with an iterative string editing model. In: Nature Communications 15.1 (2024), p. 6404. [139] Carl Edwards et al. Translation between Molecules and Natural Language. In: 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022. 2022."
        },
        {
            "title": "Appendix",
            "content": ""
        },
        {
            "title": "Metric",
            "content": "Gemini 2.5-pro GPT o"
        },
        {
            "title": "GPT\noss",
            "content": "Non-LLM Expert Models"
        },
        {
            "title": "Specialist\nLLMs",
            "content": "SciReasoner-1.7B SciReasoner-8B"
        },
        {
            "title": "IUPAC to Formula",
            "content": "Top1 Split Match Top1 Element Match Top1 Split Match Top1 Element Match 4.70% 1.50% 0.17% 55.20% 64.81% 19.88% 38.20% 21.82% 7.58% 88.50% 76.50% 33.24%"
        },
        {
            "title": "Molecular Description\nMolecular Captioning",
            "content": "ROUGE-L MENTOR"
        },
        {
            "title": "Protein Functional Translation",
            "content": "CASPSimilarSeq IDFilterSeq UniProtSeq Mol-Instructions ROUGE-L ROUGE-L ROUGE-L ROUGE-L 0.23 0.42 0.01 0.01 0.01 0.21 0.07 0.28 0.01 0.01 0.02 0.08 0.05 0.15 0.02 0.03 0.03 0.07 29.00%(LlaSMol-7B [100]) 49.52%50.20% 56.63%57.10% 93.20%(LlaSMol-7B [100]) 95.92%87.1% 96.39%96.00% 70.10%(LlaSMol-7B [100]) 78.85%86.50% 84.40%90.70% 87.90%(LlaSMol-7B [100]) 91.81%88.10% 92.65%93.00% 0.75(BioT5+ [66]) 0.29(Mol-Instructions-7B [81]) 0.45(LlaSMol-7B [100]) 0.75 0.590.59 0.68(BioT5+ [66]) 0.74 (OPI-Galactica-6.7B [102]) 0.70 (OPI-Galactica-6.7B [102]) 0.71 (OPI-Galactica-6.7B [102]) 0.44(Mol-Instructions-7B [81]) 0.83 0.77 0.84 0.830.82 0.78 0.600.58 0.85 0.82 0.88 0.990.98 Table 10 Evaluation results of scientific translation tasks. Bold indicates the best performance, and underline indicates the second best. denotes that only randomly sampled 1000 test samples were utilized for evaluation in the corresponding task. This measure is adopted because extremely large-scale test sets exist for certain tasks, which would lead to substantial testing costs. 4 9 Task Metric Gemini 2.5-pro GPT o3 GPT oss Non-LLM Expert Models Specialist LLMs SciReasoner-1.7B SciReasoner-8B Scientific Knowledge Extraction and QA Chemical Entity Recognition ChemicalProtein Interaction Extraction ChemicalDisease Interaction Extraction General Multiple Choice Question True/False Question Open-Ended Question F1 F1 F1 ACC ACC BertScore 0.71 0.12 0.35 0.94 0.67 0.84 0.66 0.06 0.33 0.71 0.62 0. 0.55 0.05 0.23 0.87 0.66 0.80 0.75(Mol-Instructions-7B [81]) 0.22(Mol-Instructions-7B [81]) 0.40(Mol-Instructions-7B [81]) 0.65(Mol-Instructions-7B [81]) 0.55(Mol-Instructions-7B [81]) 0.84(Mol-Instructions-7B [81]) 0.81 0.46 0.66 0.92 0.64 0.83 0.92 0.83 0.97 0.99 0.68 0.87 Table 11 Evaluation results of scientific knowledge extraction and QA tasks. Bold indicates the best performance, and underline indicates the second best. 5 0 Task Molecular Property Regression ESOL LIPO Physicochemical Prediction Material Property Regression MPRegression SNUMATRegression JARVISDFT JARVISQETB GNoME hMOF Cantor-HEA QMOF OQMD OMDB Biology Multi-omics Property Regression Fluorescence StabilityBiology-InstructionsBiology-Instructions Thermostability Enhancer Activity Prediction APA Isoform Prediction Mean Ribosome Loading Prediction Programmable RNA Switches CRISPR On Target Prediction siRNA Efficiency Prediction Spearman Spearman Spearman PCC R2 R2 R2 Spearman Mixed-score Metric Gemini 2.5-pro GPT GPT oss Non-LLM Expert Models Specialist LLMs SciReasoner-1.7B SciReasoner-8B RMSE RMSE MAE MAD MAE 1.28 1.21 0.027 1.32 1.07 1.09 0.53 1.28 0.65 0.77 1.24 1.20 0.82 -0.43 -9.15 5.47 -4.24 0.13 0.00 0.01 -3.64 38.75 1.58 1.44 1.879 3.45 1.65 1. 1.04(LlaSMol-7B [100]) 1.01(LlaSMol-7B [100]) 0.013(Mol-Instructions-7B [81]) 1.21 0.94 0.003 1.08 0.82 0.002 1.22 1.14 1.2 0.53 1.19 0.8 0.91 1.41 1.25 0.89 0.78 -7.96 15.00 0.49 0.24 0.00 0.15 -2.21 34.02 0.95 1.00 0.88 0.50 0.73 0.67 0.45 0.86 0.65 0.93 3.20 -7.79 6.29 -1.29 0.01 0.02 0.01 6.21 40.99 5.32(MatBERT-109M [105]) 1.83(MatBERT-109M [105]) 4.10(MatBERT-109M [105]) 59.44(LLM-Prop-35M [105]) 15.60(LLM-Prop-35M [105]) 1.48(LLM-Prop-35M [105]) 8.40(LLM-Prop-35M [105]) 2.05(MatBERT-109M [105]) 6.02(LLM-Prop-35M [105]) 1.55(MatBERT-109M [105]) 0.76(Gemma 2-9b-it:5S [105]) 1.16(Llama 2-7b-chat:5S [105]) 0.70(Llama 2-7b-chat:5S [105]) 1.05(Llama 2-7b-chat:5S [105]) 0.50(Llama 2-7b-chat:5S [105]) 0.66(Llama 2-7b-chat:5S [105]) 0.87(Llama 2-7b-chat:5S [105]) 0.93(Llama 2-7b-chat:5S [105]) 1.16(Llama 2-7b-chat:5S [105]) 1.01(Llama 2-7b-chat:5S [105]) 4.984.67 2.182.18 5.115.28 138.59148.71 19.0518.46 1.051.04 7.407.38 8.558.76 5.295.50 1.401.41 5.125.21 1.962.22 5.215.59 126.61137.61 22.0321.81 0.990.99 7.187.02 8.077.94 5.435.67 1.391.42 69.00(ShallowCNN [120]) 79.00(Evoformer [121]) 78.00(ESM-1v [122]) 68.00(DeepSTARR [123]) 50.82(APARENT [124]) 78.00(Optimus [125]) 55.67(MLP-O [126]) 44.10(SCC [127]) 49.38(Meta-Biomol [128]) 2.57(ChatMultiOmics-8B [85]) 60.25(ChatMultiOmics-8B [85]) 45.07(ChatMultiOmics-8B [85]) 57.24(ChatMultiOmics-8B [85]) 59.01(ChatMultiOmics-8B [85]) 47.64(ChatMultiOmics-8B [85]) 26.65(ChatMultiOmics-8B [85]) 2.87(ChatMultiOmics-8B [85]) 42.92(ChatMultiOmics-8B [85]) 69.3670.82 63.2555.67 56.2553.17 65.0770.20 87.9486.19 50.8254.56 39.0038.1 14.35 62.5664.13 66.2468.49 64.4159.03 58.5554.45 64.3952.08 85.8787.92 60.4464.5 43.5529.20 25.81 59.6761.94 Table 12 Evaluation results of property prediction tasks. Bold indicates the best performance, and underline indicates the second best. denotes that only randomly sampled 1000 test samples were utilized for evaluation in the corresponding task. This measure is adopted because extremely large-scale test sets exist for certain tasks, which would lead to substantial testing costs. 5 1 Task Metric Gemini 2.5-pro GPT o3 GPT oss Non-LLM Expert Models Specialist LLMs SciReasoner-1.7B SciReasoner-8B Molecular Classification BBBP Clintox HIV Sider ACC@1 ACC@1 ACC@1 ACC@1 71.57% 26.39% 26.7% 67.7% 78.68% 47.92% 77.70% 70.50% 46.70% 52.78% 69.86% 38.25% Material Property Classification AUC AUC MPclassification SNUMATclassification 0.58 0.59 0.56 0.63 Gene-centric Association Classification gSymbol2Tissue gSymbol2Cancer gName2Cancer Precision Recall F1 score Precision Recall F1 score Precision Recall F1 score 0.01 0.01 0.01 0.00 0.00 0.00 0.00 0.00 0.00 0.38 0.68 0.45 0.11 0.10 0.10 0.11 0.10 0.10 0.53 0.60 0.03 0.09 0.04 0.01 0.02 0.01 0.01 0.04 0.01 75.10%(LlaSMol-7B [100]) 93.10%(LlaSMol-7B [100]) 96.70%(LlaSMol-7B [100]) 70.70%(LlaSMol-7B [100]) 79.70% 92.36% 96.79%92.40% 70.00%66.20% 82.74% 91.67% 96.81%95.90% 68.53%68.70% 0.72(MatBERT-109M [105]) 0.72(LLM-Prop-35M [105]) 0.51(Gemma 2-9b-it:5S [105]) 0.58(Gemma 2-9b-it:5S [105]) 0.700.69 0.690. 0.720.71 0.660.66 0.39(OPI-Galactica-6.7B [102]) 0.91(OPI-Galactica-6.7B) [102] 0.53(OPI-Galactica-6.7B) [102] 0.36(OPI-Galactica-6.7B [102]) 0.32(OPI-Galactica-6.7B [102]) 0.32(OPI-Galactica-6.7B [102]) 0.27(OPI-Galactica-6.7B [102]) 0.26(OPI-Galactica-6.7B [102]) 0.25(OPI-Galactica-6.7B [102]) 0.41 0.82 0.51 0.75 0.72 0.73 0.68 0.67 0.67 0.41 0.83 0.52 0.81 0.81 0.81 0.80 0.79 0. Table 13 Evaluation results of molecular classification tasks, material property classification tasks, and gene-centric association classification tasks. Bold indicates the best performance, and underline indicates the second best. denotes that only randomly sampled 1000 test samples were utilized for evaluation in the corresponding task. This measure is adopted because extremely large-scale test sets exist for certain tasks, which would lead to substantial testing costs. 5 2 Task Metric Gemini 2.5-pro GPT GPT oss Non-LLM Expert Models Specialist LLMs SciReasoner 1.7B SciReasoner 8B Biology Multi-omics Property Classification (binary) SolubilityBiology-Instructions Antibody-Antigen RNA-Protein Epigenetic Marks Prediction TF-Mouse0 TF-Mouse1 TF-Mouse2 TF-Mouse3 TF-Mouse4 Enhancer-Promoter Interaction Promoter-300all Promoter-300notata Promoter-300tata Promoter-coreall Promoter-corenotata Promoter-coretata TF-Human0 TF-Human1 TF-Human2 TF-Human3 TF-Human4 SolubilityPEER StabilityPEER Human PPI Yeast PPI ACC MCC MCC MCC MCC MCC MCC MCC MCC MCC MCC MCC MCC MCC MCC MCC MCC MCC MCC MCC ACC ACC ACC ACC 51.4 5.05 1.20 0.52 0.20 1.24 0.41 2.72 6.61 3.75 24.12 16.65 11.98 12.38 15.64 13.02 -3.46 12.23 2.29 -8.44 7.56 58.40 49.60 52.74 51.52 Biology Multi-omics Property Classification (multi-label) Protein Function Domain/Motif FunctionEC Non-coding RNA Func. Classif. Modification Prediction Subcellular Localization Fold Type Numerical EC (New-392) Numerical EC (Price-149) ROUGE-L ROUGE-L Fmax ACC AUC Accuracy Accuracy Precision Recall F1 Precision Recall F1 0.20 0.19 3.3 17.8 51.19 0.02 0.01 0 0 0 0 0 0 50.83 1.32 4.40 2.40 11.93 3.27 5.97 3.00 3.58 3.24 13.04 20.31 15.12 -3.18 18.43 16.07 2.41 11.08 7.20 0.00 15.66 53.0 45.40 57.4 50.00 0.07 0.04 4.49 6.20 50.96 0.08 0.01 0.01 0 0 0 0 0 51.22 2.78 5.23 0.84 4.75 0.38 6.99 9.63 2.07 1.69 -3.56 -2.40 13.04 -3.57 -6.37 17.97 3.24 3.85 5.19 -1.27 15.06 10.91 38.21 58.2 44. 0.05 0.02 22.86 11.54 50.05 0 0 0 0 0 0 0 0 77.00(DeepSol [129]) 54.90(DeepAAI [130]) 93.2(ncRPI-LGAT [131]) 58.83(DNABert-2 [103]) 64.23(DNABert-2 [103]) 86.28(DNABert-2 [103]) 81.28(DNABert-2 [103]) 73.49(DNABert-2 [103]) 52.66(DNABert-2 [103]) 53.59(EPI-DLMH [132]) 91.01(NT-2500M-multi [103]) 94.34(DNABert-2 [103]) 79.43(NT-2500M-multi [103]) 70.92(DNABert 3-mer [103]) 71.58(NT-2500M-multi [103]) 78.15(DNABert 3mer [103]) 71.99(DNABert-2 [103]) 76.06(DNABert-2 [103]) 66.52(DNABert-2 [103]) 58.54(DNABert-2 [103]) 77.43(DNABert-2 [103]) 74.37(BioT5+ [66]) 58.30(SFM-Protein [133]) 85.09(BioT5+ [66]) 66.16(BioT5+ [66]) 63.02(ChatMultiOmics-8B [85]) 1.06(ChatMultiOmics-8B [85]) 74.26(ChatMultiOmics-8B [85]) 3.64(ChatMultiOmics-8B [85]) 26.50 (ChatMultiOmics-8B [85]) 66.96 (ChatMultiOmics-8B [85]) 63.98 (ChatMultiOmics-8B [85]) 32.02 (ChatMultiOmics-8B [85]) 9.18 (ChatMultiOmics-8B [85]) 4.13(ChatMultiOmics-8B [85]) 78.97(ChatMultiOmics-8B [85]) 83.45(ChatMultiOmics-8B [85]) 38.50(ChatMultiOmics-8B [85]) 59.98(ChatMultiOmics-8B [85]) 62.85(ChatMultiOmics-8B [85]) 39.36(ChatMultiOmics-8B [85]) 56.56(ChatMultiOmics-8B [85]) 61.17(ChatMultiOmics-8B [85]) 46.92(ChatMultiOmics-8B [85]) 34.16(ChatMultiOmics-8B [85]) 51.86(ChatMultiOmics-8B [85]) 71.40(NatureLM [82]) 63.50(NatureLM-8B [82]) 84.8(NatureLM-8B [82]) 60.40(NatureLM-8B [82]) 0.57(BioT5+ [66]) 0.53(BioT5+ [66]) 88.9(SaProt-GearNet [134]) 85.73(GCN [121]) 84.00(MultiRM [135]) 0.59(CLEAN [136]) 0.48(CLEAN [136]) 0.49(CLEAN [136]) 0.58(CLEAN [136]) 0.47(CLEAN [136]) 0.49(CLEAN [136]) 0.43(Mol-Instructions-7B [81]) 0.46(Mol-Instructions-7B [81]) 19.79(ChatMultiOmics-8B [85]) 63.09(ChatMultiOmics-8B [85]) 59.06(ChatMultiOmics-8B [85]) 0.78(OPI-Galactica-6.7B [102]) 0.61(OPI-Llama-3.1-8B-Instruct) [102] 0.37(OPI-Llama-3.1-8B-Instruct) [102]) 0.34(OPI-Llama-3.1-8B-Instruct) [102] 0.35(OPI-Llama-3.1-8B-Instruct) [102] 0.07(OPI-Llama-3.1-8B-Instruct) [102] 0.07(OPI-Llama-3.1-8B-Instruct) [102] 0.07(OPI-Llama-3.1-8B-Instruct) [102] 62.6767.3 41.5945.43 86.4785.97 14.1618.09 41.7939.77 68.9572.07 71.5769.08 44.8145.10 27.3428.45 8.186.75 82.94 88.46 44.38 61.49 61.72 53.67 59.33 62.22 47.80 38.96 56.43 70.1470.8 68.4168.90 76.79 53. 0.630.63 0.560.56 60.6560.33 80.3988.8 53.06 0.84 0.34 0.40 0.39 0.39 0.10 0.10 0.10 71.6169.4 45.548.84 84.9788.8 17.2421.26 43.5240.42 71.0375.98 75.0768.52 52.9853.31 28.6228.20 14.408.12 83.01 87.04 51.37 62.56 63.52 46.94 61.07 63.47 48.02 40.89 56.10 73.9975.4 64.7064.3 73.42 60.41 0.970.96 0.970.90 77.2677.56 86.6593.4 59.94 0.91 0.48 0.55 0.55 0.54 0.21 0.23 0.22 5 3 Continued on next page Task Keywords (CASPSimilarSeq) Keywords (IDFilterSeq) Keywords (UniProtSeq) GO Terms (CASPSimilarSeq) GO Terms (IDFilterSeq) GO Terms (UniProtSeq) Metric Precision Recall F1 Precision Recall F1 Precision Recall F1 Precision Recall F1 Precision Recall F1 Precision Recall F1 Gemini 2.5-pro 0.04 0.03 0.03 0.03 0.02 0.01 0.01 0 0 0 0 0 0 0 0 0 0 0 GPT 0.21 0.19 0.19 0.17 0.16 0.16 0.2 0.18 0.18 0 0 0 0 0 0 0 0 0 GPT oss 0 0.02 0.01 0 0.02 0.01 0.01 0.03 0.01 0 0 0 0 0 0 0 0 0 Non-LLM Expert Models Specialist LLMs SciReasoner 1.7B SciReasoner 8B - - - - - - - - - - - - - - - - - - 0.81(OPI-Galactica-6.7B [102]) 0.74(OPI-Galactica-6.7B [102]) 0.76(OPI-Galactica-6.7B [102]) 0.84(OPI-Galactica-6.7B [102]) 0.80(OPI-Galactica-6.7B [102]) 0.82(OPI-Galactica-6.7B [102]) 0.86(OPI-Galactica-6.7B [102]) 0.82(OPI-Galactica-6.7B [102]) 0.83(OPI-Galactica-6.7B [102]) 0.76(OPI-Galactica-6.7B [102]) 0.74(OPI-Galactica-6.7B [102]) 0.75(OPI-Galactica-6.7B [102]) 0.74(OPI-Galactica-6.7B [102]) 0.73(OPI-Galactica-6.7B [102]) 0.72(OPI-Galactica-6.7B [102]) 0.76(OPI-Galactica-6.7B [102]) 0.74(OPI-Galactica-6.7B [102]) 0.74(OPI-Galactica-6.7B [102]) 0.83 0.80 0.81 0.86 0.85 0.85 0.85 0.90 0.88 0.81 0.81 0.80 0.78 0.77 0.77 0.84 0.82 0.81 0.86 0.83 0.84 0.89 0.89 0.88 0.92 0.91 0.91 0.86 0.86 0.86 0.83 0.82 0.81 0.88 0.87 0.86 Table 14 Evaluation results of biology multi-omics property classification tasks. Dashed lines separate tasks that report multiple metrics (Precision/Recall/F1). Bold indicates the best performance, and underline indicates the second best. 5 4 Task Unconditional Generation Unconditional Molecular Generation Unconditional Protein Generation Unconditional RNA Generation Unconditional Material Generation Conditional Material Generation Composition to Material Generation Bulk to Material Generation Conditional Small Molecular Generation Forward Synthesis Prediction Forward Reaction Prediction Reagent Prediction RetrosynthesisMol-Instructions RetrosynthesisUSPTO-50K RetrosynthesisSMol-Instruction Molecule Generation Description-Guided Molecule Design Validity Unique Avg. Length Diversity Validity Avg. MFE SMACT SMACT Precision Novelty SMACT Success rate Exact Match Exact Match Exact Match Exact Match Exact Match Exact Match Exact Match Exact Match Metric Gemini 2.5-pro GPT o3 GPT oss Non-LLM Expert Models Specialist LLMs SciReasoner 1.7B SciReasoner 8B 0.568 0.042 128.87 0.62 0.93 N/A 89. 0.715 0.364 153.56 0.99 0.97 -151.70 N/A 0.952 0.318 318.91 1.00 0.99 -417.58 N/A N/A N/A N/A N/A N/A 0.33 0.36 0.00 0.05 0.12 0.08 0.03 0.04 N/A N/A N/A N/A N/A 0.32 0.55 0.04 0.20 0.23 0.12 0.25 0.06 N/A N/A N/A N/A N/A 0.27 0.58 0.01 0.16 0.07 0.04 0.16 0.04 0.00 0.09 0.968(NatureLM-8B [82]) 0.966(NatureLM-8B [82]) 284.50(NatureLM-8B [82]) 0.97 (NatureLM-8B [82]) -177.1(NatureLM-8x7B [82]) 66.07(NatureLM-8x7B [82]) 83.36(NatureLM-8B [82]) 98.44(NatureLM-8B [82]) 97.13(NatureLM-1B [82]) 94.75(NatureLM-8x7B [82]) 0.864 0.988 280.10 1.00 0.80 -178.85 55.06 86.07 99.89 85.64 88. 7.12 0.86(BioT5+ [66]) 0.26(BioT5+ [66]) 0.64(BioT5+ [66]) 0.61(EditRetro [138]) 0.11(MolT5 [139]) 0.63(LlaSMol-7B [100]) 0.54(InstructMol-GS-6.9B [137]) 0.13(InstructMol-GS-6.9B [137]) 0.41(InstructMol-GS-6.9B [137]) 0.70(NatureLM-8B [82]) 0.33(LlaSMol-7B [100]) 0.19(LlaSMol-7B [100]) 0.002(Mol-Instructions-7B [81]) 0.690.81 0.96 0.19 0.72 0.680.72 0.410.43 0.390.34 0.10 0.971 0.982 284.7 0.91 0.99 -200.14 88. 85.85 99.96 84.81 87.49 7.05 0.750.83 0.99 0.22 0.83 0.720.74 0.460.47 0.480.46 0.12 1.001.00 0.990.95 Conditional Biochemical Generation Function-guided Protein Design Catalytic Activity Max. Normalized SW ROUGE-L 0.00 0.25 0.00 0.02 0.97(BioT5+ [66]) 0.72(BioT5+ [66]) 0.43(Mol-Instructions-7B [81]) 0.52(Mol-Instructions-7B [81]) 0.960.90 0.770.78 Table 15 Evaluation results of generation and design tasks. N/A indicates that the model cannot generate samples that meet the required specifications. Bold indicates the best performance, and underline indicates the second best."
        },
        {
            "title": "Protein Generation",
            "content": "Figure 20 Additional Visualization for Protein Generation task (a). Figure 21 Additional Visualization for Protein Generation task (b)."
        },
        {
            "title": "Translation between SMILES and IUPAC",
            "content": "Figure 22 Additional Visualization for SMILES-to-IUPAC Translation task"
        },
        {
            "title": "Task Examples",
            "content": "Example 1: Prompt: Response: Its log solubility is -1.41 mol/L. How soluble is <SMILES> CC(C)Cl </SMILES> ? ESOL This task has 112 test samples. Predict the octanol/water distribution coefficient logD under the Example 1: Prompt: circumstance of pH 7.4 for <SMILES> NC(=O)C1=CC=CC=C1O </SMILES> . Response: 1.090 LIPO This task has 420 test samples. Is blood-brain barrier permeability (BBBP) property of <SMILES> Example 1: Prompt: CCNC(=O)/C=C/C1=CC=CC(Br)=C1 </SMILES>? Response: Yes BBBP This task has 197 test samples. Example 1: Prompt: Response: No Is <SMILES> COC[C@@H](NC(C)=O)C(=O)NCC1=CC=CC=C1 </SMILES> toxic? Clintox This task has 144 test samples. Can <SMILES> CC1=CN(C2C=CCCC2O)C(=O)NC1=O </SMILES> serve as an Example 1: Prompt: inhibitor of HIV replication? Response: No HIV This task has 4107 test samples. Are there any known side effects of <SMILES> CC1=CC(C)=C(NC(=O)CN(CC(=O Example 1: Prompt: )O)CC(=O)O)C(C)=C1Br </SMILES> affecting the heart? Response: No SIDER This task has 2860 test samples. 57 You are material scientist. Example 1: Prompt: given crystalline material and predict its property. in json format. For example: {property_name:predicted_property_value}. Answer as precise as possible and in as few words as possible.chemical formula: KPrMnNbO6property name: volume Response: {volume : 128.62405239223665}"
        },
        {
            "title": "The output must be",
            "content": "MPregression This task has 82241 test samples. You are material scientist. Example 1: Prompt: given crystalline material and predict its property. in json format. For example: {property_name:predicted_property_value}. Answer as precise as possible and in as few words as possible.chemical formula: Na2Ti3Cl8property name: Band_gap_HSE Response: {Band_gap_HSE : 1.583856}"
        },
        {
            "title": "The output must be",
            "content": "SNUMATregression This task has 4152 test samples. Example 1: Prompt: You are material scientist. given crystalline material and predict its property. in json format. For example: {property_name:predicted_property_value}. Answer as precise as possible and in as few words as possible.chemical formula: YReW2O8property name: formation_energy_peratom Response: {formation_energy_peratom :"
        },
        {
            "title": "The output must be",
            "content": "-2.15925} JARVISDFT This task has 54647 test samples. You are material scientist. Example 1: Prompt: given crystalline material and predict its property. in json format. For example: {property_name:predicted_property_value}. Answer as precise as possible and in as few words as possible.chemical formula: BeMoRhproperty name: final_energy Response: {final_energy : -169.1866374093}"
        },
        {
            "title": "The output must be",
            "content": "JARVISQETB This task has 249596 test samples. 58 You are material scientist. Example 1: Prompt: given crystalline material and predict its property. in json format. For example: {property_name:predicted_property_value}. Answer as precise as possible and in as few words as possible.chemical formula: Yb3Eu7(TlPd2)2property name: Density Response: {Density : 8.2778}"
        },
        {
            "title": "The output must be",
            "content": "GNoME This task has 216455 test samples. You are material scientist. Example 1: Prompt: given crystalline material and predict its property. in json format. For example: {property_name:predicted_property_value}. Answer as precise as possible and in as few words as possible.chemical formula: Zn4C84N16H22O17property name: max_co2_adsp Response: {max_co2_adsp : 8.95639}"
        },
        {
            "title": "The output must be",
            "content": "hMOF This task has 92925 test samples. You are material scientist. Example 1: Prompt: given crystalline material and predict its property. in json format. For example: {property_name:predicted_property_value}. Answer as precise as possible and in as few words as possible.chemical formula: Al2Co1Cr1Mn1Ni2Si1property name: e_above_hull Response: {e_above_hull : 0.4014893125}"
        },
        {
            "title": "The output must be",
            "content": "Cantor-HEA This task has 33608 test samples. You are material scientist. Example 1: Prompt: given crystalline material and predict its property. in json format. For example: {property_name:predicted_property_value}. Answer as precise as possible and in as few words as possible.chemical formula: Zn12C48H88O80property name: lcd Response: {lcd : 3.40559}"
        },
        {
            "title": "The output must be",
            "content": "QMOF This task has 3064 test samples. 59 You are material scientist. Example 1: Prompt: given crystalline material and predict its property. in json format. For example: {property_name:predicted_property_value}. Answer as precise as possible and in as few words as possible.chemical formula: Sm2CuH6property name: e_form Response: {e_form : -0.3328793676}"
        },
        {
            "title": "The output must be",
            "content": "OQMD This task has 192799 test samples. You are material scientist. Example 1: Prompt: given crystalline material and predict its property. in json format. For example: {property_name:predicted_property_value}. Answer as precise as possible and in as few words as possible.chemical formula: C4H5(CH)3(CH2)3C5H5NCH3C3NH4H3(CO)2C4NH4SO2property name: Response: {bandgap : 2.886436}"
        },
        {
            "title": "The output must be",
            "content": "bandgap OMDB This task has 1213 test samples. Example 1: Please evaluate the following protein sequence and provide an Prompt: explanation of the enzymes catalytic activity, including the chemical reaction it facilitates: <protein>MVKKRIAVIGAGISGLGAIKCCLDEDLEPTCFERNDDIGGLWKF QKNASEKMPSIYRSVTINTSKEMMCFSDFPIPDHFPNYMHNSKLMDYFRMYAKRFSLLDYIRFKTTVRSVRKRPDFHIH GQWDVVVETDGKQESLVFDGVLVCSGHHTDPHLPLKSFPGIEKFEGCYFHSREYKSPEDYVGKRIIVVGIGNSGVDIAV ELGRVAKQVFLSTRRGSWILHRVWNNGYPMDSSFFTRFHSFLQKILTTEAVNKYLEKTLNSRFNHAHYGLQPQHRPLSQ HPTISDDLPNHIISGKVQVKPNVKEFTGTDVHFDDGTVEENIDVVIFATGYSISFPFLGDLIAVTDNEVSLYKLMFPPD LEKPTLAVIGLIQPLGIILPIAELQSRWAVRVFKGLSKLPSVKAMKADMDQRKKAMEKRYVKTARHTIQVDHIEYMDEI ASLAGVKPNLLLLFLSDPTLAMEVFFGPCTPYQYRLQGPGKWDGARRAILTQRERIIKPLKTRITSEKSRSAPGLFWIK MALFGLAFLVPSLTYFSYICQ</protein> Response: Upon reviewing the provided protein sequence, the corresponding enzymes catalytic activity is identified as the following chemical reaction: H(+) + hexan-3-one + NADPH + O2 = ethyl butanoate + H2O + NADP(+). Catalytic Activity This task has 1601 test samples."
        },
        {
            "title": "Can you give me the energy difference between the HOMO and LUMO",
            "content": "Example 1: Prompt: orbitals of this molecule? <SMILES> O=C1OCC2CC1C2 </SMILES> Response: 0.2756 Physicochemical Prediction This task has 1999 test samples. 60 <protein>SKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKFICTTGKLPVP Example 1: Prompt: WPTLVTTLSYGVQCFSRYPDHMKQHDFFKSAMPEGYVQERTIFFKDDGNYKTRAEVRFEGDTLVNRIELKGIDFKEDGNI LGHKLEYNYNSHNVYIMADKQKIGIKVNFRIRHNIEDGSVQLADHYQQNTPIGDGPVLLPDNHYLSTQSALSKDPNEKRD HMVLLEFVTAAGITHGMDELYK</protein> [Fluorescence Prediction Regression] What is the anticipated fitness of this green fluorescent protein mutant? Response: The fluorescence intensity for this GFP mutant is calculated as 3.67. Fluorescence This task has 27217 test samples. <protein>TTIKVNGQEYTVPLSPEQFAKAAKKRWPDYEVQIHGNTVKVTR</protein> What Example 1: Prompt: is the expected stability score for this sequence? Response: The stability score determined for this sequence is 0.53. Stability This task has nan test samples. <protein>MNYARFITAASAARNPSPIRTMTDILSRGPKSMISLAGGLPNPNMFPFKTAVITV Example 1: Prompt: ENGKTIQFGEEMMKRALQYSPSAGIPELLSWLKQLQIKLHNPPTIHYPPSQGQMDLCVTSGSQQGLCKVFEMIINPGDNV LLDEPAYSGTLQSLHPLGCNIINVASDESGIVPDSLRDILSRWKPEDAKNPQKNTPKFLYTVPNGNNPTGNSLTSERKKE IYELARKYDFLIIEDDPYYFLQFNKFRVPTFLSMDVDGRVIRADSFSKIISSGLRIGFLTGPKPLIERVILHIQVSTLHP STFNQLMISQLLHEWGEEGFMAHVDRVIDFYSNQKDAILAAADKWLTGLAEWHVPAAGMFLWIKVKGINDVKELIEEKAV KMGVLMLPGNAFYVDSSAPSPYLRASFSSASPEQMDVAFQVLAQLIKESL</protein> The task evaluates protein thermostability using mass spectrometry to measure melting points in human cells. How is the thermostability of this protein assessed? Response: The task uses mass spectrometry to measure protein melting curves and predict thermostability in human cells. recorded for this protein is 56."
        },
        {
            "title": "The thermostability measure",
            "content": "Thermostability This task has 1336 test samples. <dna>TGTCGCTCCCATTTCGTCAAATGTTGCGTGCTAATTCGCTTGCCTTCTGGCGGCTATTT Example 1: Prompt: TTGTTTTGATATAATTTATATTTCTCGCTCTTTACTTATCTTTTTTGCTTTTTTTTTGTATTTTGTTTGCCGCTTACACT TCACATTTGCCTTCGGGGGCCGTCGCTTAAAAATAAACGCACACACACACTCGCGGCATTCACATATTTATTTTTGGTTT CTCCAACTTTGTACGCTCTGCGTTTTCTTC</dna> [Regression: prediction] Please provide an enhancer activity prediction for this sequence. Response: The enhancer activity prediction yields: HK -0.68, Dev -1."
        },
        {
            "title": "Enhancer activity",
            "content": "Enhancer Activity Prediction This task has 41186 test samples. 61 <rna>CTGCTTGGAGTTTACATTGAGGAAGCTGGTAACTGACCTTCAAAGCTAAAATATAAAAC Example 1: Prompt: TATTTGGGAAGTATGAAACGATGTCTCGTGATCTGGTGTACCCTTATCCCTGTGACGTTTGGCCTCTGACAATACTGGTA TAATTGTAAATAATGTCAAACTCCGTTTTCTAGCAAGTATTAAGGGA</rna> [Isoform prediction] What is the likelihood that this sequence ends in proximal isoform? Response: The processing of 3 UTR regions in the sequence results in proximal isoform proportion of 0.33. APA Isoform Prediction This task has 49755 test samples. <rna>TAATCGGTTCTAAATACGATTAGTG</rna> What ribosome loading rate is Example 1: Prompt: expected from the sequence? Response: The sequence impacts ribosome loading during translation initiation with an efficiency of 0.63 Mean Ribosome Loading Prediction This task has 7600 test samples. <rna>GGGCTTGGGGAAAGTGGACAGAGATGATTATTAAACCAAACACACAAACGCACTAATAA Example 1: Prompt: TCATCTCTGTCCACTTTCCCCAAGAACAGAGGAGACTTGGGATGAGTGGACAGAACCTGGCGGC AGCGCAAAAGATGCGTAAAGGAGAA</rna> [Programmable RNA switches prediction] How does the sequence influence the ON and OFF states of the RNA switch? Response: The RNA switch with this sequence is predicted to have an ON state of 0.02, an OFF state of 0.47, and an ON/OFF ratio of 0.06 Programmable RNA Switches This task has 9154 test samples. <rna>TTCACTGCGCAGCCTGGCATTGG</rna> What gene disruption success rate Example 1: Prompt: is associated with the sequence? Response: The predicted gene disruption level for this sequence is 0.26. CRISPR On Target Prediction This task has 416 test samples. <sirna>a Af Uf u Af Gf u c</sirna> Example 1: Prompt: <rna>ACGTACCTGCAGATCTACGGGGTCCTGGAGCACCCTGAAGACTCAAGACCAAAAGACTGTCAGGA AGGCAGAGTGCAGAGCAATCCACTGTCCAAGACCACACGACTTCGAGAACGGGGAATACTGGCCCCGGTCTCCCTACTAC AATGTGAGTGATGAGATCTCTTTCCACTGCTATGACGGTTACACTCTCCGGGGCTCTGCCAATCGCACCTGCCAAGTGAA TGGCCGATGGAGTGGGCAGACAGCGATCTGTGACAACGGAGCGGGGTACTGCTCCAACCCGGGCATCCCCATTGGCACAA 62 GGAAGGTGGGCAGCCAGTACCGCCTTGAAGACAGCGTCACCTACCACTGCAGCCGGGGGCTTACCCTGCGTGGCTCCCAG CGGCGAACGTGTCAGGAAGGTGGCTCTTGGAGCGGGACGGAGCCTTCCTGCCAAGACTCCTTCATGTACGACACCCCTCA AGAGGTGGCCGAAGCTTTCCTGTCTTCCCTGACAGAGACCATAGAAGGAGTCGATGCTGAGGATGGGCACGGCCCAGGGG AACAACAGAAGCGGAAGATCGTCCTGGACCCTTCAGGCTCCATGAACATCTACCTGGTGCTAGATGGATCAGACAGCATT GGGGCCAGCAACTTCACAGGAGCCAAAAAGTGTCTAGTCAACTTAATTGAGAAGGTGGCAAGTTATGGTGTGAAGCCAAG ATATGGTCTAGTGACATATGCCACATACCCCAAAATTTGGGTCAAAGTGTCTGAAGCAGACAGCAGTAATGCAGACTGGG TCACGAAGCAGCTCA</rna> What is the likely mRNA remaining percentage after siRNA treatment? Response: The sequence leads to an mRNA remaining percentage of 28.00 after siRNA treatment. siRNA Efficiency Prediction This task has 6688 test samples. You are material scientist. Example 1: Prompt: given crystalline material and predict its property. in json format. For example: {property_name:predicted_property_value}. Answer as precise as possible and in as few words as possible.chemical formula: KPrMnNbO6property name: is_stable Response: {is_stable : False}"
        },
        {
            "title": "The output must be",
            "content": "MPclassification This task has 20575 test samples. You are material scientist. Example 1: Prompt: given crystalline material and predict its property. in json format. For example: {property_name:predicted_property_value}. Answer as precise as possible and in as few words as possible.chemical formula: PbSproperty name: SOC Response: {SOC : True}"
        },
        {
            "title": "The output must be",
            "content": "SNUMATclassification This task has 4152 test samples. Example 1: Prompt: Please conduct domain/motif search on the given protein sequence and provide your findings. <protein>MGTTTTHKFDRPLRLPPLPCPFPSEVNPYVEQVDKETLEWLIDSEMLDDAETVERYRQAKYGWLSARTYP YAEHHTLRLVSDWCVWLFAFDDAFCESDRRAAEIARALPQLYAVLEDLDVGSEVDDVFAKSLLEIKGRIAAYGDDEQLD RWRNVTKDYLFAQVWEAANREDEVVPSLEDYIFMRRRTGAMLTVFALIDVASGRSLSADEWRHPGMRAITESANDVVVW DNDLISYAKESNSGNSRNNLVNVLAEHRHYSRQEAMEEIGEMRNQAIADMVAVRPSLEALGSDAVLAYVRGLEFWISGSV DYSLTSSRYTDAWRTARQPSIR</protein> Response: Based on computational analysis, the provided sequence potentially contains the following protein domains or motifs:"
        },
        {
            "title": "DDXXXE motif domains",
            "content": "The sequence is: Domain/Motif This task has 1400 test samples. 63 Example 1: Prompt: Analyze the following amino acid sequence, and determine the function of the resulting protein, its subcellular localization, and any biological processes it may be part of: <protein>MANHKSTQKSIRQDQKRNLINKSRKSNVKTFLKRVTLAINAGDKKVASEALSAAHSKLAKAANKG IYKLNTVSRKVSRLSRKIKQLEDKI</protein> Response: The protein with the amino acid sequence is expected to exhibit rRNA binding, structural constituent of ribosome, contributing to the translation. It can be typically found in the ribonucleoprotein complex, ribosome of the cell. Protein Function This task has 3494 test samples. <protein>MPLGLFSSGKAQVLCDEKIPGGKKKEPKQLSENKCKGVTLKLDHTRVTIEKQIAE Example 1: Prompt: GGFAIVYVASDRKNNKFALKRQFTKDNEKQLEACCREHSFLKQCIGHKNIVEFVDSYTNCLGNGIWECMLLTEYHQKNVL QLMNERISQNQYLTNDEILSIFTDLCEAVSFIHNRPQPIIHRDLKVENVLISSHKPPHYVLCDFGSATTQILSVEKYGVE YVKSEVERNTTMCYRSPEMIDFYSGLEIGLKSDIWALGVLLYRLCFFCVPFEESPLAIQSVNYQFPSVPNIPDEIKVLIY MLLDIDVNRRPSIYQTSVLAFEANHRKPLSEEIQNKKCTDAVPSLKSCIQLMRDGSNPRNKRDSSPRNPEAPPIQSSSKM ASLSQQVPSISNISMPSGSGTVETSVAPRLRPKATTVVPNVPSISPVPPVGLPHLRLPSKGSTDETDGSQVRKVPIDFHH RQSFSGEEQLKPAAEADSAGPLSCPLIKPTDLGFTDLDKPALPRDRAQTDGKRRLPHESDIIFQQQHRRNVSDTSQISRS AFKPYSSQQTTSKTSSQVVRSVEDMSQRQNGGSGEWNPFLVAPFSNNSISRKDGQESAFMMDDSHFGMVFDEIRRKEIPA ELDSETSSIDSRDPFGAAPFDQLTVSTSSSAQPVSLPPGSVHLHTSIAPSPLLRHVLTKAPPPVPRRTTSRGFQV</pro tein> Is this sequence associated with soluble protein? Response: The protein predicted from this sequence is not soluble. Solubility This task has 2001 test samples. <protein>MGSSHHHHHHSQDLENLYFQGSMNIFEMLRIDEGLRLKIYKDTEGYYTIGIGHLL Example 1: Prompt: TKSPSLNAAKSELDKAIGRNTNGVITKDEAEKLFNQDVDAAVRGILRNAKLKPVYDSLDAVRRAALINMVFQMGETGVAG FTNSLRMLQQKRWDEAAVNLAKSRWYNQTPNRTKRVITTFRTGTWDAYKNLRKKLEQLYNRYKDPQDENKIGIDGIQQFC DDLALDPASISVLIIAWKFRAATQCEFSKQEFMDGMTELGCDSIEKLKAQIPKMEQELKEPGRFKDFYQFTFNFAKNPGQ KGLDLEMAIAYWNLVLNGRFKFLDLWNKFLLEHHKRSIPKDTWNLLLDFSTMIADDMSNYDEEGAWPVLIDDFVEFARPQ IAGTKSTTV</protein> [EC Number Prediction Multilabel Classification] What are the donor and acceptor molecules in this enzymes reaction? Response: Donor molecules utilized by enzymes are identified by their EC numbers. Donor molecules it utilizes are identified by EC number EC3.2.1.17,EC3.2.1.-. Function-EC This task has 1919 test samples. Example 1: Prompt: <protein>EVQLVESGANVVRPGGSLRLSCKASGFIFENFGFSWVRQAPGKGLQWVAGLNWNGGDTRYADSVKGRFRMS Ab-Ag interaction detection:"
        },
        {
            "title": "Any binding between",
            "content": "64 RDNSRNFVYLDMDKVGVDDTAFYYCARGTDYTIDDAGIHYQGSGTFWYFDLWGRGTLVSVSSXEIVLAQSPGTLSLSPGE RATLSCRASHNVHPKYFAWYQQKPGQSPRLLIYGGSTRAAGIPGKFSGSGSGTDFTLTISRVDPEDFAVYYCQQYGGSPY TFGQGTKVEIKX</protein> and <protein>MRVMGIQKNYPPLWRWGMIIFWIMMICNAKQMWVTVYYGVPVWR DASTTLFCASDAKAYDPEAHNAWATHACVPTDPNPQEMQLKNVTENFNMWKNDMVEQMHVDIISLWDQSLKPCVKLTPLC VTLNCTDVRNRTLNYTYNNSTSNISLVSSDMEGEIKNCSFNMTTELRDKNQKVYALFYRQDVIQNGNNNSSYYRLINCNT SAMTQACPKVSFNPIPIHYCAPAGFAILKCNDKEFNGTGPCKNVSTVQCTHGIKPVISTQLLLNGSIAEEEIVIRSENIT NNAKTIIVQLNEPVEINCTRPNNNTRKSVRIGPGQTFYATGDIIGNIRQAHCNVSRAKWNETLHKVAEQLRTYFKNKTII FTNHSGGDVELKTHMFNCGGEFFYCNTTALFNSTWDNNSTREDNNSTGNDTITLQCRIKQIVRMWQRSGQAMYAPPIQGV IKCKSNITGLLLTRDGGNNESTNETFRPGGGDMRDNWRSELYKYKVVKIEPLGVAPTHAKRRVVQREKRAVGLGAVFFGF LGAAGSTMGAASITLTVQARQLLSGIVQQQSNLLKAIEAQQHLLKLTVWGIKQLQARVLALERYLGDQQLLGIWGCSGKL ICTTNVPWNSSWSNKTYGDIWDNMTWLQWDKEISNYTNIIYALIEESQNQQEKNEQDLLALDKWASLWNWFDISKWLWYI KIFIMIVGGLIGLRIVFTIITIINRVRQGYSPLSFQTLTHHQREPDRPERIEEEGGEQDKDRSIRLVGGFLALAWDDLRN LCLFIYHRLRDFVLIAARAVSLLGHSSLKGLRLGWEALKYLWNLLAYWGKELKNSAVNLLDTVAIAVANWTDRVIEIGQR AGRAILNIPRRIRQGAERALV</protein>? Response: Yes. Based on their primary structures, physical interaction appears plausible. Antibody-Antigen This task has 3301 test samples. need to know if <rna>TGCTTCGTGCTTGCTTGAAGTTAAACCCGGCGGGGGAACGA Example 1: Prompt: TTGTTCCCACTGAACGTTCCCACAGGAGAACCCTTGGAGACACTACCTTTCCCACAGAATTTCATTCAAGGAGCTCAAAG GCCAGGAAATCTTTTGAAAAGGTACCATCTACCTGGAGGGCATCATCCTGAGTGAGAGGACAAGCAAAAAGAGGAATGGA AGACTAGATACACAGCTAAATGGAAGGAGAACTTCTGGCCTAAATGCAATAAGGAAATCTATGTGGCTACCGAGAGCAAC AGGACACTCCTGACATTGTGTAACCCAAAGATAGAAACATGCAATTGTGCTACATGGCCCTCCAGGCTCTGGAAAAACCA CAATGGCAAAGCAGCTAATGTTAGAATGGTCAGAGCGTAAGCAGGCCCAGATTTTCTCATGTGCCTTCTACATCAGCTGC AGGGAAGTAAATAATACTAAACTTTGCACTTTTGCCCACCTCCTCTCCATGGACAATCC</rna> has any functional or structural interaction with <protein>HVLFEHAVGYALLALKEVEEISLLQPQVE ESVLNLGKFHSIVRLVAFCPFASSQVALENANAVSEGVVHEDLRLLLETHLPSKKKKVLLGVGDPKIGAAIQEELGYNCQ TGGVIAEILRGVRLHFHNLVKGLTDLSACKAQLGLGHSYSRAKVKFNVNRVDNMIIQSISLLDQLDKDINTFSMRVREWY GYHFPELVKIINDNATYCRLAQFIGNRRELNEDKLEKLEELTMDGAKAKAILDASRSSMGMDISAIDLINIESFSSRVVS LSEYRQSLHTYLRSKMSQVAPSLSALIGEAVGARLIAHAGSLTNLAKYPASTVQILGAEKALFRALKTRGNTPKYGLIFH STFIGRAAAKNKGRISRYLANKCSIASRIDCFSEVPTSVFGEKLREQVEERLSFYETGEIPRKNLDVMKEAMVQAEEAAA EITRKLEKQEKKRLKKKK</protein>. Response: The interaction between the RNA and protein is predicted to be positive. RNA-Protein This task has 4164 test samples. <dna>CAACGGTACCCAATGGTTCCATCTCTAAGTCGTTAGGGCCAATCGTGGGCGGTATGCTA Example 1: Prompt: TTGTCACCTTCATCTTCGCTGTCTTCCTCACTCAACTTGAAAGTGGAAGAATCATTATTGTGCATTGTGAAACCTCCAGC ATTGGAAAATCTGTGTTTTAAGCTGGTGGACCTTGAAACCAAAAACAAGAAACATTGAAATGCAATGGCGGAGTGTAGTG AAGGAACAGCACCAAACACAATAGGAGACATGTGAAATCCTTTAGTGTTCAAATGAGTACCCAAATGGCTATCTACTCTT ATCAAACCGGCAGCAAACCCTTCCTGGGTGTAATTAACATGTTCGGTGTCATCAATACCGTACAGATGAGTAAACCATGG GGAGGCCATTGGAACAAGCAGATGGGTAAGGACCCCCGCTATATTTTGTAATCCTAGGGCAAAACTAAAGCATTTTAAAG TCCCCGGTGGTTGGAAGACGTATAAGTATACAGCTGTTAAA</dna> [Classification: marks prediction] need help with an Epigenetic Mark Prediction task. epigenetic Heres 65 the sequence: Response: EMP, or Epigenetic Marks Prediction, aims to identify epigenetic changes, successfully confirmed in yeast DNA sequences. Epigenetic Marks Predictions This task has 28741 test samples. <dna>CTGCTGCTGCTGCTGGCTCTGGCTTTTGTTGTTGATGGTGACATCATCATTTAAAAAAA Example 1: Prompt: AACATATGCTGATCACTGTATGAAAGTTCAACAATGTTAGTG</dna>: transcription factor binding sites in this DNA sequence. Response: Upon examination, can confirm that the mouse DNA sequence piece contains TF binding sites, which is crucial for gene expression."
        },
        {
            "title": "I need a detailed review of",
            "content": "TF-Mouse This task has 10005 test samples. <rna>AGCTACGGCCATATCACCCTGAACATGCCTGATCTCGTCTGATCTCAGAAGCTAAGCAC Example 1: Prompt: GGTAGGGCCTGGTTAGTACTTGGATGGAAGACCACCTGGGAATACCAGGTGCTGTAGGGG</rna> How would you classify this sequence within non-coding RNA classes? Response: The RNA sequence is categorized under the 5S_rRNA family based on its structural characteristics. Non-coding RNA Function Classification This task has 2600 test samples. <rna>TAAAGCACTTAACATACATAACTTACTGTGGCTCAGAGTAAGCGCTTAGTAAAAATTGG Example 1: Prompt: AGTTATTTTTCAATAGATTCATTATCTATAATAAAGGAGAAA</rna> What does the sequence indicate about its potential RNA modifications?Please choose from the following options:[Am, Cm, Gm, Um, m1A, m5C, m5U, m6A, m6Am, m7G, Psi, AtoI, none] Response: AtoI.The RNA modifications predicted for the sequence are AtoI. Modification Prediction This task has 1200 test samples. Analyze the interaction potential between <dna>CCTCCTGGGGCTGTTTACCAGAG Example 1: Prompt: CCTTAACTAGGTGCGACCTGTTGATAGCTGGTGTAGTCCCAGGAACCAGGATTTGGATGGAGGGTACAATGAGGCTCTGA GCTGCCTTCTCCCCTTCCAGGGCTGGGGGCAGCAGCCATGCCTACCTGGGGGGCCCGCCCCGCGTCCCCAGATCGCTTTG CGGTGTCTGCGGAGGCTGAGAACAAGGTTCGGGAACAGCAGCCCCATGTGGAGCGCATCTTCAGCGTGGGGGTGAGCGTC CTTCCGAAGGACTGTCCGGACAACCCCCACATCTGGCTGCAGCTGGAGGGCCCCAAGGAAAACGCCAGCAGAGCCAAGGT GAACGCCTTCTCTCCCCCATCCCTCCAGGCACCAAGGACGCTTTCCCCCAGGGCGGAGGAGAGCAGGGCCGAGGAGAGCA 66 GGGAAGAGGAGGGGCCTGGGGAGGATCTCCACTTCTTGGTTGTCTCACTGGCAACCTCAGTGCCCCTGAGTTTTGAG</d na> and <dna>GGTCGTGCAGGTAAAACATCCCAAGCACCACTTCTTTCAGCAGGCGGCAAAGGAGCGGCCAGGGCCG AGGGCACTGGGACTGCAGCAGCCCCGACAAGGAGCCGTTCTCCATGAATTTAGTCACCAGAGCCGGCTTGGGATCTTGGT CCCAGTTCACCTTCTCGATAACCCCTTCTAGGCGCAGCACGAATTCGTTATCCAGACTTGCCATGGCCTTGACCTCCCTG GATATCGCCTTCCTACACTCCAGGAGAGAGCTGGAGTCGCACCGGGGTCGTGGGAAAATCCCTCCCTTCGCCATTCAGGC CCCAGAGCACAGTGGCTCCACCTTTTTGGCCAGATTGCGGCTGGGGTCAACCGGGGTCACTCACGAGTTTACGATCTTGA CCGCCACATCGTAGCCCCACTTCCTATGTTGCGCCCGGAACACTGTGCCGAACCCGCCTTTGCCGACGAGCTCCTGGTTC TCCAGTTCCTCGATGGACACCAAGGGGGCGGGG</dna>. Response: This DNA sequence suggests high probability of promoter-enhancer interaction. Enhancer-Promoter Interaction Prediction This task has 1762 test samples. <dna>AGAAACCCTATGAATGCAGTCTTTGTAGGAAAGCTTTTTTTGAGAAGTCGGAGCTAATTAGACAT Example 1: Prompt: CTGAGAACTCATACAGGAGAAAAACCTTATGAATGC</dna>: transcription factor binding sites in this DNA sequence. Response: My biological analysis indicates that TF binding sites are present in the given DNA fragment."
        },
        {
            "title": "Assist me in locating",
            "content": "TF-Human This task has 28741 test samples. <dna>TGTCGCGCCCTCCCGGGCTCGGCCACTCTGCCCTTCCCTGCCCCAGCGCAGGTGAGCTGGCGCTT Example 1: Prompt: GTCCTTAACCTTCTTCAACGACGAGGCGGGACGGAGGCTCGGACGCCTCCCCAGCCTCTGGGCTGGCGCAAGTCGACTGC GGTTCGTCCTCAGTCCACGATGAAGTCATAGGGCTGTGGGTCCGTGTGTCTGTGACGTCAGAAGATGGGTCCTGGGATGT CACTGCAGGCCCGGGCGGGGACCTGGGCTCACGGAAGGGCCGGGGGGACCCGCAGGGTAGAAAAAGGAAGTTGTC</dna >: Response: The DNA fragment includes identifiable promoter characteristics. Could you determine if this sequence contains promoter? Promoter-300 This task has 5920 test samples. <dna>CTCTTGCCTTCCGGCCCGGCGCCCGATTTCCGCCTTCCGACCCAGCTGTGGGCTGCGCCCCACGC Example 1: Prompt: CAGCC</dna> Does this sequence function as core promoter? Response: Yes, core promoter elements have been found in the given sequence. Promoter-core This task has 5000 test samples. 67 Example 1: Prompt: Based on their primary structures, is an interaction between <protein>MSGEENPASKPTPVQDVQGDGRWMSLHHRFVADSKDKEPEVVFIGDSLVQLMHQCEIWRELFSPLHALNF GIGGDGTQHVLWRLENGELEHIRPKIVVVWVGTNNHGHTAEQVTGGIKAIVQLVNERQPQARVVVLGLLPRGQHPNPLR EKNRQVNELVRAALAGHPRAHFLDADPGFVHSDGTISHHDMYDYLHLSRLGYTPVCRALHSLLLRLLAQDQGQGAPLLE PAP</protein> and <protein>MWSPEREAEAPAGGDPAGLLPPEWEEDEERMSFLFSAFKRSREVNSTDWDSK MGFWAPLVLSHSRRQGVVRLRLRDLQEAFQRKGSVPLGLATVLQDLLRRGELQRESDFMASVDSSWISWGVGVFLLKPL KWTLSNMLGDNKVPAEEVLVAVELLKEKAEEVYRLYQNSPLSSHPVVALSELSTLCANSCPDERTFYLVLLQLQKEKRV TVLEQNGEKIVKFARGPRAKVSPVNDVDVGVYQLMQSEQLLSRKVESLSQEAERCKEEARRACRAGKKQLALRSL KAKQRTEKRIEALHAKLDTVQGILDRIYASQTDQMVFNAYQAGVGALKLSMKDVTVEKAESLVDQIQELCDTQDEVSQT LAGGVTNGLDFDSEELEKELDILLQDTTKEPLDLPDNPRNRHFTNSVPNPRISDAELEAELEKLSLSEGGLVPSSKSPK RQLEPTLKPL</protein> plausible in human cell? is stable physical association between two proteins and No if not. Response: It is likely that these proteins interact. Please answer Yes if there The answer is Yes. Human PPI This task has 237 test samples. Example 1: Prompt: Based on the primary amino acid chains, is there evidence for stable physical association between yeast protein (<protein>MSNYPLHQACMENEFFKVQELLHSKPSLLLQKDQDGRIPLHWSVSFQAHEITSFLLSKMENVNLDDYPDD SGWTPFHIACSVGNLEVVKSLYDRPLKPDLNKITNQGVTCLHLAVGKKWFEVSQFLIENGASVRIKDKFNQIPLHRAASV GSLKLIELLCGLGKSAVNWQDKQGWTPLFHALAEGHGDAAVLLVEKYGAEYDLVDNKGAKAEDVALNEQVKKFFLNNV </protein>) and yeast protein (<protein>METIDIQNRSFVVRWVKCGRGDVINYQIKPLKKSIEVG IYKKLKSSVDDHASAVHIAPDTKTLLDYTTKSLLHKGSSSNIEEHHRRSSQHSHSSSNGSDNKRKERSYSSLSISGIQQQ SQEIPLREKLSASGFTLVKRVGNVSGNTMVQGDLEVKDTDYYYAFILDNSSSKNAKKKILFNASVINGDNQSMISTRSTP PARPTALSRTSTQQDMLFRVGQGRYLQGYLLKKRRKRLQGFKKRFFTLDFRYGTLSYYLNDHNQTCRGEIVISLSSVSAN KKDKIIIIDSGMEVWVLKATTKENWQSWVDALQTCFDDQFEDKDTSTLEENPDILDDDKEVINKSSPQDHDHLTPTATTK SALSHRQHTQKDMDDIYVPLPSESYATFSMNLRLIQQRLEQCKKDSLSYKPTTLHQRSEGLNGTHSSSSVFTNNRVSSFN HSSSGMTSSDSLASEEVPSNKTYIEHALYNQLADLEVFVSRFVTQGEVLFKDHQILCKKAKDTRVSLTSYLSENDEFFDA EEEISRGVIILPDTEDDINNIVEETPLLGKSDQNEFTKEVQLSGSEQIASSSVESYTTNDENHSRKHLKNRHKNRRRGHP HHQKTKSAQSSTETFTSKDLFALSYPKSVTRRNDIPEAAASPPSLLSFLRKNVGKDLSSIAMPVTSNEPISILQLISETF EYAPLLTKATQRPDPITFVSAFAISFLSIYRDKTRTLRKPFNPLLAETFELIREDMGFRLISEKVSHRPPVFAFFAEHLD WECSYTVTPSQKFWGKSIELNNEGILRLKFKTTGELFEWTQPTTILKNLIAGERYMEPVNEFEVHSSKGDKSHILFDKAG MFSGRSEGFKVSIIPPPSSNRKKETLAGKWTQSLANETTHETIWEVGDLVSNPKKKYGFTKFTANLNEITEIEKGNLPPT DSRLRPDIRAYEEGNVDKAEEWKLKLEQLQRERRNKGQDVEPKYFEKVSKNEWKYITGPKSYWERRKKHDWSDISQLW </protein>)? Please answer Yes if there is stable physical association between two proteins and No if not. Response: Interaction: Yes. The answer is Yes. Yeast PPI This task has 394 test samples. : Return the EC number of the protein sequence. Example 1: Prompt: <protein>MSILEWFWNILCGMAQYLTFSKNLTNDNLVNIYVKSNVGGTISVNLDPKSDIKNVKELVAPKLGLEPDDVK IIFAGKELLDSTVIEVLDFFSDILHAVKVNKKIKNVIPDKPLCETLEELHQLNDQKNVESIEESNLKNEGKNKAHFFIYC ANPCKKINTGKLRVCCSECKHGAFTVDTDPQSWADVLDKNKITGVCNNVGCEGLYAKFYFKCASHPSQGENDTAVPLNLI KRNHKKIPCLACTDICDPVLVFSCDNRHVTCLECFKNYCGSRLKDRQFLSHPDFGYTLPCPAGCSNSFIEEVHHFRLLTD 68 AQYEQYHRFATEEFILQAGGVLCPQPGCGQGILIDQNCNRVQCSCGYVFCGKCLEGFHLGECLNPTDVPFLSQNCDYPLD PEKLEKARWDEASSTVIKVLTKPCPKCRTSTERAGGCMHMICTRANCGFHWCWVCQGPWERDCMASHWFG</protein> Response: : 2 . 3 . 2 . 31 Numerical EC (New-392) This task has 382 test samples. Return the EC number of the protein sequence. Example 1: Prompt: <protein>MAIPPYPDFRSAAF LRQHLRATMAFYDPVATDASGGQFHFFLDDGTVYNTHTRHLVSATRFVVTHAMLYRTTGEARYQVGMRHALEFLRTAFL DPATGGYAWLIDWQDGRATVQDTTRHCYGMAFVMLAYARAYEAGVPEARVWLAEAFDTAEQHFWQPAAGLYADEASPDW QLTSYRGQNANMHACEAMISAFRATGERRYIERAEQLAQGICQRQAALSDRTHAPAAEGWVWEHFHADWSVDWDYNRHD RSNIFRPWGYQVGHQTEWAKLLLQLDALLPADWHLPCAQRLFDTAVERGWDAEHGGLYYGMAPDGSICDDGKYHWVQA ESMAAAAVLAVRTGDARYWQWYDRIWAYCWAHFVDHEHGAWFRILHRDNRNTTREKSNAGKVDYHNMGACYDVLLWAL DAPGFSKESRSAALGRP</protein> Response: 5 . 3 . 1 . 7 NumericalEC (Price-149) This task has 149 test samples. Example 1: Prompt: Through sequence analysis, forecast the proteins subcellular positioning in eukaryotic cells. <protein>AGFPEQEPEPKFWNDWAQKTLDKALSLQTLNKNKAQN LILFLGDGMGVPTVTAARILKGQLRGQPGEEGQLEMDKFPFVALSKTYNTNAQVADSAGTATAYLCGVKANEGTVGVSAA AVRSQANTTQGNEVTSILRWAKDAGKSIGIVTTTRVNHATPSAAYAHCVDRDWYSDNEMPADAVEAGCKDIARQLFENIP DIDVIMGGGRKYMYPKNTTDVEYPGQPKHSGTRKDGRNLVKEWVDRNTEKKGHYVWNKKDLLSLNPTKVDYLLGLFEPAD LPYDLERNKETDPSLSEMVEVAIKILRRNPNGFYLLVEGGRIDHGHHEGKDKQAIHEAVEMDRAIGRADLMTSTSDTLTV VTADHSHLFSFGGYTPRGNEIFGLAAFISDVDQKPFTAILYGNGPGYKLVNGARENVSTVDYQDNSYLAQAAVPLSSETH GGEDVAVFAKGPMAHLLHGVHEQNYIPHAMAYAACIGQNR</protein> Response: membrane Subcellular Localization This task has 2772 test samples. Example 1: Prompt: Analyze the provided protein sequence and predict the UniProtKB keywords associated with it. <protein>MAAADGGGPGGASVGTEEDGGGVGHRTVYLFDRREKESELG DRPLQVGERSDYAGFRACVCQTLGISPEEKFVITTTSRKEITCDNFDETVKDGVTLYLLQSVNQLLLTATKERIDFLPHY DTLVKSGMYEYYASEGQNPLPFALAELIDNSLSATSRNIGVRRIQIKLLFDETQGKPAVAVIDNGRGMTSKQLNNWAVYR LSKFTRQGDFESDHSGYVRPVPVPRSLNSDISYFGVGGKQAVFFVGQSARMISKPADSQDVHELVLSKEDFEKKEKNKEA IYSGYIRNRKPSDSVHITNDDERFLHHLIIEEKEKDSFTAVVITGVQPEHIQYLKNYFHLWTRQLAHIYHYYIHGPKGNE IRTSKEVEPFNNIDIEISMFEKGKVPKIVNLREIQDDMQTLYVNTAADSFEFKAHVEGDGVVEGIIRYHPFLYDRETYPD DPCFPSKLKDEDDEDDCFILEKAARGKRPIFECFWNGRLIPYTSVEDFDWCTPPKKRGLAPIECYNRISGALFTNDKFQV STNKLTFMDLELKLKDKNTLFTRILNGQEQRMKIDREFALWLKDCHEKYDKQIKFTLFKGVITRPDLPSKKQGPWATYAA IEWDGKIYKAGQLVKTIKTLPLFYGSIVRFFLYGDHDGEVYATGGEVQIAMEPQALYDEVRTVPIAKLDRTVAEKAVKKY VEDEMARLPDRLSVTWPEGDELLPNEVRPAGTPIGALRIEILNKKGEAMQKLPGTSHGGSKKLLVELKVILHSSSGNKEI ISHISQHGGKWPYWFKKMENIQKLGNYTLKLQVVLNESNADTYAGRPLPSKAIKFSVKEGKPEKFSFGLLDLPFRVGVPF NIPLEFQDEFGHTSQLVTDIQPVLEASGLSLHYEEITKGPNCVIRGVTAKGPVNSCQGKNYNLKVTLPGLKEDSQILKIR 69 LLPGHPRRLKVKPDSEILVIENGTAFPFQVEVLDESDNITAQPKLIVHCKFSGAPNLPVYVVDCSSSGTSILTGSAIQVQ NIKKDQTLKARIEIPSCKDVAPVEKTIKLLPSSHVARLQIFSVEGQKAIQIKHQDEVNWIAGDIMHNLIFQMYDEGEREI NITSALAEKIKVNWTPEINKEHLLQGLLPDVQVPTSVKDMRYCQVSFQDDHVSLESAFTVRPLPDEPKHLKCEMKGGKTV QMGQELQGEVVIIITDQYGNQIQAFSPSSLSSLSIAGVGLDSSNLKTTFQENTQSISVRGIKFIPGPPGNKDLCFTWREF SDFIRVQLISGPPAKLLLIDWPELKESIPVINGRDLQNPIIVQLCDQWDNPAPVQHVKISLTKASNLKLMPSNQQHKTDE KGRANLGVFSVFAPRGEHTLQVKAIYNKSIIEGPIIKLMILPDPEKPVRLNVKYDKDASFLAGGLFTDFMISVISEDDSI IKNINPARISMKMWKLSTSGNRPPANAETFSCNKIKDNDKEDGCFYFRDKVIPNKVGTYCIQFGFMMDKTNILNSEQVIV EVLPNQPVKLVPKIKPPTPAVSNVRSVASRTLVRDLHLSITDDYDNHTGIDLVGTIIATIKGSNEEDTDTPLFIGKVRTL EFPFVNGSAEIMSLVLAESSPGRDSTEYFIVFEPRLPLLSRTLEPYILPFMFYNDVKKQQQMAALTKEKDQLSQSIVMYK SLFEASQQLLNEMKCQVEEARLKEAQLRNELKIHNIDIPTTQQVPHIEALLKRKLSEQEELKKKPRRSCTLPNYTKGSGD VLGKIAHLAQIEDDRAAMVISWHLASDMDCVVTLTTDAARRIYDETQGRQQVLPLDSIYKKTLPDWKRSLPHFRNGKLYF KPIGDPVFARDLLTFPDNVEHCETVFGMLLGDTIILDNLDAANHYRKEVVKITHCPTLLTRDGDRIRSNGKFGGLQNKAP PMDKLRGMVFGAPVPKQCLILGEQIDLLQQYRSAVCKLDSVNKDLNSQLEYLRTPDMRKKKQELDEHEKNLKLIEEKLGM TPIRKCNDSLRHSPKVETTDCPVPPKRMRREATRQNRIITKTDV</protein> Response: 3D-structure ; Acetylation ; Alternative splicing ; Chromatin regulator ; Chromosome ; Direct protein sequencing ; Disease variant ; DNA damage ; DNA repair ; DNA-binding ; Hydrolase ; Hypogonadotropic hypogonadism ; Isopeptide bond ; Kallmann syndrome ; Microphthalmia ; Phosphoprotein ; Reference proteome ; Ubl conjugation Keywords-CASPSimilarSeq This task has 184 test samples. Example 1: Prompt: Please list the UniProtKB keywords associated with the provided protein sequence. <protein>MTSVAKVYYSQTTQTESRPLMGPGIRRRRVLTKDGRSNVRMEHIADKRFLYL KDLWTTFIDMQWRYKLLLFSATFAGTWFLFGVVWYLVAVAHGDLLELDPPANHTPCVVQVHTLTGAFLFSLESQTTIGYG FRYISEECPLAIVLLIAQLVLTTILEIFITGTFLAKIARPKKRAETIRFSQHAVVASHNGKPCLMIRVANMRKSLLIGCQ VTGKLLQTHQTKEGENIRLNQVNVTFQVDTASDSPFLILPLTFYHVVDETSPLKDLPLRSGEGDFELVLILSGTVESTSA TCQVRTSYLPEEILWGYEFTPAISLSASGKYIADFSLFDQVVKVASPSGLRDSTVRYGDPEKLKLEESLREQAEKEGSAL SVRISNV</protein> Response: ATP-binding ; Cell membrane ; Deafness ; Disease variant ; Epilepsy ; Intellectual disability ; Ion channel ; Ion transport ; Membrane ; Nucleotide-binding ; Potassium ; Potassium transport ; Reference proteome ; Transmembrane ; Transmembrane helix ; Transport ; Voltage-gated channel Keywords-IDFilterSeq This task has 1112 test samples."
        },
        {
            "title": "Analyze the given protein sequence and generate the relevant",
            "content": "Example 1: Prompt: UniProtKB keywords. <protein>MWLKLFFLLLYFLVLFVLARFFEAIVWYETGIFATQLVDPVALSFKKLKT ILECRGLGYSGLPEKKDVRELVEKSGDLMEGELYSALKEEEASESVSSTNFSGEMHFYELVEDTKDGIWLVQVIANDRSP LVGKIHWEKMVKKVSRFGIRTGTFNCSSDPRYCRRRGWVRSTLIMSVPQTSTSKGKVMLKEYSGRKIEVEHIFKWITAHA ASRIKTIYNAEHLKEEWNKSDQYWLKIYLFANLDQPPAFFSALSIKFTGRVEFIFVNVENWDNKSYMTDIGIYNMPSYIL RTPEGIYRYGNHTGEFISLQAMDSFLRSLQPEVNDLFVLSLVLVNLMAWMDLFITQGATIKRFVVLISTLGTYNSLLIIS WLPVLGFLQLPYLDSFYEYSLKLLRYSNTTTLASWVRADWMFYSSHPALFLSTYLGHGLLIDYFEKKRRRNNNNDEVNAN NLEWLSSLWDWYTSYLFHPIASFQNFPVESDWDEDPDLFLERLAFPDLWLHPLIPTDYIKNLPMWRFKCLGVQSEEEMSE GSQDTENDSESENTDTLSSEKEVFEDKQSVLHNSPGTASHCDAEACSCANKYCQTSPCERKGRSYGSYNTNEDMEPDWLT WPADMLHCTECVVCLENFENGCLLMGLPCGHVFHQNCIVMWLAGGRHCCPVCRWPSYKKKQPYAQHQPLSNDVPS</pro 70 tein> Response: Endoplasmic reticulum ; Membrane ; Metal-binding ; Reference proteome ; Transferase ; Transmembrane ; Transmembrane helix ; Ubl conjugation pathway ; Zinc ; Zinc-finger Keywords-UniProtSeq This task has 4562 test samples. Please predict the folding type of the protein sequence, the Example 1: Prompt: prediction range is from 0 to 1194. PWTQRFFQHFGNLSSAGAVMNNPKVKAHGKRVLDAFTQGLKHLDDLKGAFAQLSGLHCNKLHVNPQNFRLLGNVLALVV ARNFGGQFTPNVQALFQKVVAGVANALAHKYH</protein> Response: <protein>MLTAEEKAAVTGFWGKVDVDVVGAQALGRLLVVY Fold Type This task has 3243 test samples. Predict the GO term based on the protein sequence. Example 1: Prompt: <protein>MAAADGGGPGGASVGTEEDGGGVGHRTVYLFDRREKESELGDRPLQVGERSDYAGFRACVCQTLGISPEEK FVITTTSRKEITCDNFDETVKDGVTLYLLQSVNQLLLTATKERIDFLPHYDTLVKSGMYEYYASEGQNPLPFALAELIDN SLSATSRNIGVRRIQIKLLFDETQGKPAVAVIDNGRGMTSKQLNNWAVYRLSKFTRQGDFESDHSGYVRPVPVPRSLNSD ISYFGVGGKQAVFFVGQSARMISKPADSQDVHELVLSKEDFEKKEKNKEAIYSGYIRNRKPSDSVHITNDDERFLHHLII EEKEKDSFTAVVITGVQPEHIQYLKNYFHLWTRQLAHIYHYYIHGPKGNEIRTSKEVEPFNNIDIEISMFEKGKVPKIVN LREIQDDMQTLYVNTAADSFEFKAHVEGDGVVEGIIRYHPFLYDRETYPDDPCFPSKLKDEDDEDDCFILEKAARGKRPI FECFWNGRLIPYTSVEDFDWCTPPKKRGLAPIECYNRISGALFTNDKFQVSTNKLTFMDLELKLKDKNTLFTRILNGQEQ RMKIDREFALWLKDCHEKYDKQIKFTLFKGVITRPDLPSKKQGPWATYAAIEWDGKIYKAGQLVKTIKTLPLFYGSIVRF FLYGDHDGEVYATGGEVQIAMEPQALYDEVRTVPIAKLDRTVAEKAVKKYVEDEMARLPDRLSVTWPEGDELLPNEVRPA GTPIGALRIEILNKKGEAMQKLPGTSHGGSKKLLVELKVILHSSSGNKEIISHISQHGGKWPYWFKKMENIQKLGNYTLK LQVVLNESNADTYAGRPLPSKAIKFSVKEGKPEKFSFGLLDLPFRVGVPF... seq uences for readability)</protein> Response: Barr body ; site of double-strand break ; ATP binding ; ATP hydrolysis activity ; DNA binding ; protein homodimerization activity ; chromosome organization ; dosage compensation by inactivation of chromosome ; double-strand break repair ; inactivation of chromosome by heterochromatin formation ; negative regulation of double-strand break repair via homologous recombination ; nose development ; positive regulation of DNA repair ; positive regulation of double-strand break repair via nonhomologous end joining (We hide the remaining GO-CASPSimilarSeq This task has 184 test samples. Example 1: Prompt: term that describes its molecular function, biological process, and cellular component. Given protein sequence, predict the corresponding Gene Ontology <protein>MTSVAKVYYSQTTQTESRPLMGPGIRRRRVLTKDGRSNVRMEHIADKRFLYLKDLWTTF 71 IDMQWRYKLLLFSATFAGTWFLFGVVWYLVAVAHGDLLELDPPANHTPCVVQVHTLTGAFLFSLESQTTIGYGFRYISEE CPLAIVLLIAQLVLTTILEIFITGTFLAKIARPKKRAETIRFSQHAVVASHNGKPCLMIRVANMRKSLLIGCQVTGKLLQ THQTKEGENIRLNQVNVTFQVDTASDSPFLILPLTFYHVVDETSPLKDLPLRSGEGDFELVLILSGTVESTSATCQVRTS YLPEEILWGYEFTPAISLSASGKYIADFSLFDQVVKVASPSGLRDSTVRYGDPEKLKLEESLREQAEKEGSALSVRISNV </protein> Response: astrocyte projection ; basolateral plasma membrane ; cell body ; ciliary base ; plasma membrane ; presynapse ; ATP binding ; ATP-activated inward rectifier potassium channel activity ; inward rectifier potassium channel activity ; adult walking behavior ; cellular response to potassium ion ; central nervous system myelination ; glutamate reuptake ; non-motile cilium assembly ; potassium ion homeostasis ; potassium ion import across plasma membrane ; potassium ion transport ; regulation of long-term neuronal synaptic plasticity ; regulation of monoatomic ion transmembrane transport ; regulation of resting membrane potential ; visual perception GO-IDFilterSeq This task has 1112 test samples. For given protein sequence, identify the appropriate Gene Ontology Example 1: Prompt: term that corresponds to its molecular function, biological process, and cellular component. <protein>MWLKLFFLLLYFLVLFVLARFFEAIVWYETGIFATQLVDPVALSFKKLKT ILECRGLGYSGLPEKKDVRELVEKSGDLMEGELYSALKEEEASESVSSTNFSGEMHFYELVEDTKDGIWLVQVIANDRSP LVGKIHWEKMVKKVSRFGIRTGTFNCSSDPRYCRRRGWVRSTLIMSVPQTSTSKGKVMLKEYSGRKIEVEHIFKWITAHA ASRIKTIYNAEHLKEEWNKSDQYWLKIYLFANLDQPPAFFSALSIKFTGRVEFIFVNVENWDNKSYMTDIGIYNMPSYIL RTPEGIYRYGNHTGEFISLQAMDSFLRSLQPEVNDLFVLSLVLVNLMAWMDLFITQGATIKRFVVLISTLGTYNSLLIIS WLPVLGFLQLPYLDSFYEYSLKLLRYSNTTTLASWVRADWMFYSSHPALFLSTYLGHGLLIDYFEKKRRRNNNNDEVNAN NLEWLSSLWDWYTSYLFHPIASFQNFPVESDWDEDPDLFLERLAFPDLWLHPLIPTDYIKNLPMWRFKCLGVQSEEEMSE GSQDTENDSESENTDTLSSEKEVFEDKQSVLHNSPGTASHCDAEACSCANKYCQTSPCERKGRSYGSYNTNEDMEPDWLT WPADMLHCTECVVCLENFENGCLLMGLPCGHVFHQNCIVMWLAGGRHCCPVCRWPSYKKKQPYAQHQPLSNDVPS</pro tein> Response: endoplasmic reticulum ; endoplasmic reticulum membrane ; endoplasmic reticulum quality control compartment ; metal ion binding ; ubiquitin protein ligase activity ; ubiquitin-protein transferase activity ; central nervous system development ; endoplasmic reticulum mannose trimming ; protein ubiquitination ; ubiquitin-dependent ERAD pathway GO-UniProtSeq This task has 4562 test samples. According to the relevant data reported in the scientific literature, Example 1: Prompt: the Gene symbol of given protein can be used to predict which tissues the protein is expressed in. TMEM198 Response: adrenal gland , appendix , breast , bronchus , cerebral cortex , colon , fallopian tube , gallbladder , kidney , nasopharynx , prostate , rectum , salivary gland , seminal vesicle , skeletal muscle , small intestine , stomach , urinary bladder gSymbol2Tissue This task has 2181 test samples. 72 Get the cancer name that is associated with the given gene symbol. Example 1: Prompt: MLLT1 Response: AL gSymbol2Cancer This task has 148 test samples. Return the cancer name associated with the given gene name. Example 1: Prompt: myeloid/lymphoid or mixed-lineage leukemia (trithorax homolog, Drosophila); translocated to, 1 (ENL) Response: AL gName2Cancer This task has 148 test samples. Translate the given SMILES formula of molecule <SMILES> Example 1: Prompt: CCC(C)C1CNCCCNC1 </SMILES> into its IUPAC name. Response: <IUPAC> 3-butan-2-yl-1,5-diazocane </IUPAC> S2I This task has 2993 test samples. Example 1: Prompt: </IUPAC> ? Response: Of course. Its <SMILES> CCC1(C)COC(=O)C1 </SMILES> . Could you provide the SMILES for <IUPAC> 4-ethyl-4-methyloxolan-2-one I2S This task has 2993 test samples. Example 1: Prompt: </SMILES>, what would be its molecular formula? Response: It is C7H15Cl2N2OPS . Given the SMILES representation <SMILES> S=P1(N(CCCl)CCCl)NCCCO1 S2F This task has 2993 test samples. Example 1: Prompt: <IUPAC> 2,5-diphenyl-1,3-oxazole </IUPAC> ? Response: C15H11NO"
        },
        {
            "title": "What is the molecular formula of the compound with this IUPAC name",
            "content": "I2F This task has 2993 test samples. 73 Could you provide description of this molecule? Example 1: Prompt: C/C=C/C1=CC2=C(C=C1)OC(=C2C)C3=CC=C(C=C3)OC4=C(C=CC(=C4)C5=C(C6=C(O5)C=CC(=C6) /C=C/C)C)O </SMILES> Response: The molecule is natural product found in Piper aequale with data available. <SMILES> Molecular Description This task has 1000 test samples. Describe this molecule: <SMILES> CCOC(=O)C1=CN=CN1[C@H](C)C1=CC=CC=C1 Example 1: Prompt: </SMILES> Response: The molecule is an imidazole derivative with short-acting sedative, hypnotic, and general anesthetic properties. gamma-aminobutyric acid (GABA) like effects, mediated through GABA-A receptor. The action enhances the inhibitory effect of GABA on the central nervous system by causing chloride channel opening events which leads to membrane hyperpolarization."
        },
        {
            "title": "Etomidate appears to have",
            "content": "Molecule Captioning This task has 2538 test samples."
        },
        {
            "title": "Examine the given protein sequence and share a brief overview of its",
            "content": "Example 1: Prompt: attributes: <protein>MKIVLASNNQGKLAELKAMLAPLGVQLLRQAELGIPEAAEPFRTFVENALAKARHA SALSGLPALADDAGLCVEAFGGLPGVDTAFYAVQFGYAKGDANNVKALLAQMAGITDRRAALVSTLVAVRSA EDPEPLIACGRVAGEVALEPMGSNGFGFDPVMFIPEFGQTFAQLPVEVKNANSHRGKATRQMMALMRER WIT </protein> Response: concise description of the protein with the specified amino acid sequence includes: Pyrophosphatase that catalyzes the hydrolysis of nucleoside triphosphates to their monophosphate derivatives, with high preference for the non-canonical purine nucleotides XTP (xanthosine triphosphate), dITP (deoxyinosine triphosphate) and ITP. Seems to function as house-cleaning enzyme that removes non-canonical purine nucleotides from the nucleotide pool, thus preventing their incorporation into DNA/RNA and avoiding chromosomal lesions. Protein Functional Translation-Molins This task has 2633 test samples. Predict the general functional description of the protein sequence. Example 1: Prompt: <protein>MAAADGGGPGGASVGTEEDGGGVGHRTVYLFDRREKESELGDRPLQVGERSDYAGFRACVCQTLGISPEEK FVITTTSRKEITCDNFDETVKDGVTLYLLQSVNQLLLTATKERIDFLPHYDTLVKSGMYEYYASEGQNPLPFALAELIDN SLSATSRNIGVRRIQIKLLFDETQGKPAVAVIDNGRGMTSKQLNNWAVYRLSKFTRQGDFESDHSGYVRPVPVPRSLNSD ISYFGVGGKQAVFFVGQSARMISKPADSQDVHELVLSKEDFEKKEKNKEAIYSGYIRNRKPSDSVHITNDDERFLHHLII EEKEKDSFTAVVITGVQPEHIQYLKNYFHLWTRQLAHIYHYYIHGPKGNEIRTSKEVEPFNNIDIEISMFEKGKVPKIVN 74 LREIQDDMQTLYVNTAADSFEFKAHVEGDGVVEGIIRYHPFLYDRETYPDDPCFPSKLKDEDDEDDCFILEKAARGKRPI FECFWNGRLIPYTSVEDFDWCTPPKKRGLAPIECYNRISGALFTNDKFQVSTNKLTFMDLELKLKDKNTLFTRILNGQEQ RMKIDREFALWLKDCHEKYDKQIKFTLFKGVITRPDLPSKKQGPWATYAAIEWDGKIYKAGQLVKTIKTLPLFYGSIVRF FLYGDHDGEVYATGGEVQIAMEPQALYDEVRTVPIAKLDRTVAEKAVKKYVEDEMARLPDRLSVTWPEGDELLPNEVRPA GTPIGALRIEILNKKGEAMQKLPGTSHGGSKKLLVELKVILHSSSGNKEIISHISQHGGKWPYWFKKMENIQKLGNYTLK LQVVLNESNADTYAGRPLPSKAIKFSVKEGKPEKFSFGLLDLPFRVGVPFNIPLEFQDEFGHTSQLVTDIQPVLEASGLS LHYEEITKGPNCVIRGVTAKGPVNSCQGKNYNLKVTLPGLKEDSQILKIRLLPGHPRRLKVKPDSEILVIENGTAFPFQV EVLDESDNITAQPKLIVHCKFSGAPNLPVYVVDCSSSGTSILTGSAIQVQNIKKDQTLKARIEIPSCKDVAPVEKTIKLL PSSHVARLQIFSVEGQKAIQIKHQDEVNWIAGDIMHNLIFQMYDEGEREINITSALAEKIKVNWTPEINKEHLLQGLLPD VQVPTSVKDMRYCQVSFQDDHVSLESAFTVRPLPDEPKHLKCEMKGGKTVQMGQELQGEVVIIITDQYGNQIQAFSPSSL SSLSIAGVGLDSSNLKTTFQENTQSISVRGIKFIPGPPGNKDLCFTWREFSDFIRVQLISGPPAKLLLIDWPELKESIPV INGRDLQNPIIVQLCDQWDNPAPVQHVKISLTKASNLKLMPSNQQHKTDEKGRANLGVFSVFAPRGEHTLQVKAIYNKSI IEGPIIKLMILPDPEKPVRLNVKYDKDASFLAGGLFTDFMISVISEDDSIIKNINPARISMKMWKLSTSGNRPPANAETF SCNKIKDNDKEDGCFYFRDKVIPNKVGTYCIQFGFMMDKTNILNSEQVIVEVLPNQPVKLVPKIKPPTPAVSNVRSVASR TLVRDLHLSITDDYDNHTGIDLVGTIIATIKGSNEEDTDTPLFIGKVRTLEFPFVNGSAEIMSLVLAESSPGRDSTEYFI VFEPRLPLLSRTLEPYILPFMFYNDVKKQQQMAALTKEKDQLSQSIVMYKSLFEASQQLLNEMKCQVEEARLKEAQLRNE LKIHNIDIPTTQQVPHIEALLKRKLSEQEELKKKPRRSCTLPNYTKGSGDVLGKIAHLAQIEDDRAAMVISWHLASDMDC VVTLTTDAARRIYDETQGRQQVLPLDSIYKKTLPDWKRSLPHFRNGKLYFKPIGDPVFARDLLTFPDNVEHCETVFGMLL GDTIILDNLDAANHYRKEVVKITHCPTLLTRDGDRIRSNGKFGGLQNKAPPMDKLRGMVFGAPVPKQCLILGEQIDLLQQ YRSAVCKLDSVNKDLNSQLEYLRTPDMRKKKQELDEHEKNLKLIEEKLGMTPIRKCNDSLRHSPKVETTDCPVPPKRMRR EATRQNRIITKTDV</protein> Response: Non-canonical member of the structural maintenance of chromosomes (SMC) protein family that plays key role in epigenetic silencing by regulating chromatin architecture (By similarity). formation in both autosomes and chromosome X, probably by mediating the merge of chromatin compartments (By similarity). inactivation in females by promoting the spreading of heterochromatin . Recruited to inactivated chromosome by Xist RNA and acts by mediating the merge of chromatin compartments: promotes random chromatin interactions that span the boundaries of existing structures, leading to create compartment-less architecture typical of inactivated chromosome (By similarity). Required to facilitate Xist RNA spreading (By similarity). required for silencing of subset of clustered autosomal loci in somatic cells, such as the DUX4 locus . Has ATPase activity; may participate in structural manipulation of chromatin in an ATP-dependent manner as part of its role in gene expression regulation . Also plays role in DNA repair: localizes to sites of DNA double-strand breaks in response to DNA damage to promote the repair of DNA double-strand breaks . non-homologous end joining (NHEJ) and inhibiting homologous recombination (HR) repair ."
        },
        {
            "title": "Also",
            "content": "Protein Functional Translation-CASPSimilarSeq This task has 184 test samples. Predict the functional description of the protein sequence below. Example 1: Prompt: <protein>MTSVAKVYYSQTTQTESRPLMGPGIRRRRVLTKDGRSNVRMEHIADKRFLYLKDLWTTFIDMQWRYKLLLF SATFAGTWFLFGVVWYLVAVAHGDLLELDPPANHTPCVVQVHTLTGAFLFSLESQTTIGYGFRYISEECPLAIVLLIAQL VLTTILEIFITGTFLAKIARPKKRAETIRFSQHAVVASHNGKPCLMIRVANMRKSLLIGCQVTGKLLQTHQTKEGENIRL NQVNVTFQVDTASDSPFLILPLTFYHVVDETSPLKDLPLRSGEGDFELVLILSGTVESTSATCQVRTSYLPEEILWGYEF TPAISLSASGKYIADFSLFDQVVKVASPSGLRDSTVRYGDPEKLKLEESLREQAEKEGSALSVRISNV</protein> Response: May be responsible for potassium buffering action of glial cells in"
        },
        {
            "title": "Inward rectifier potassium channels are characterized by a greater",
            "content": "the brain. tendency to allow potassium to flow into the cell rather than out of it. Their voltage dependence is regulated by the concentration of extracellular potassium; as external potassium is raised, the voltage range of the channel opening shifts to more positive voltages. due to the blockage of outward current by internal magnesium. by extracellular barium and cesium (By similarity). with KCNJ16, mediates basolateral K(+) recycling in distal tubules; this process is critical for Na(+) reabsorption at the tubules."
        },
        {
            "title": "The inward rectification is mainly\nCan be blocked",
            "content": "In the kidney, together Protein Functional Translation-IDFilterSeq This task has 1112 test samples. Predict the general functional description of the protein sequence. Example 1: Prompt: <protein>MWLKLFFLLLYFLVLFVLARFFEAIVWYETGIFATQLVDPVALSFKKLKTILECRGLGYSGLPEKKDVREL VEKSGDLMEGELYSALKEEEASESVSSTNFSGEMHFYELVEDTKDGIWLVQVIANDRSPLVGKIHWEKMVKKVSRFGIRT GTFNCSSDPRYCRRRGWVRSTLIMSVPQTSTSKGKVMLKEYSGRKIEVEHIFKWITAHAASRIKTIYNAEHLKEEWNKSD QYWLKIYLFANLDQPPAFFSALSIKFTGRVEFIFVNVENWDNKSYMTDIGIYNMPSYILRTPEGIYRYGNHTGEFISLQA MDSFLRSLQPEVNDLFVLSLVLVNLMAWMDLFITQGATIKRFVVLISTLGTYNSLLIISWLPVLGFLQLPYLDSFYEYSL KLLRYSNTTTLASWVRADWMFYSSHPALFLSTYLGHGLLIDYFEKKRRRNNNNDEVNANNLEWLSSLWDWYTSYLFHPIA SFQNFPVESDWDEDPDLFLERLAFPDLWLHPLIPTDYIKNLPMWRFKCLGVQSEEEMSEGSQDTENDSESENTDTLSSEK EVFEDKQSVLHNSPGTASHCDAEACSCANKYCQTSPCERKGRSYGSYNTNEDMEPDWLTWPADMLHCTECVVCLENFENG CLLMGLPCGHVFHQNCIVMWLAGGRHCCPVCRWPSYKKKQPYAQHQPLSNDVPS</protein> Response: Acts as an E2-dependent E3 ubiquitin-protein ligase, probably involved in the ER-associated protein degradation pathway. Protein Functional Translation-UniProtSeq This task has 4562 test samples. Example 1: Prompt: Response: <SMILES> CCCOC1=CC=C(C=C1)C(=O)OCCN(CC)CC </SMILES> Generate molecule with <SMILES> Unconditional Molecular Generation This task has 800 test samples. Example 1: Prompt: Response: <protein>MASSNVSSLNNTFLGKAARILKQEGEKGRIPATAVTAYAMAVFYTLVTDIPERDTDMLI SNGVLNVVPTIISKVARKSATIRLDVQARKEPTLPTKRASANYEPEE</protein> Generate protein sequence with <protein> </protein>. Unconditional Protein Generation This task has 1000 test samples. Example 1: Prompt: Response: <rna>GGCTGGTCTGATGGTAGTGGGTTATCAGAACTTATTAACATTAGTGTCACTAAAGTTGGTATAC CCCCACCCCCACTGCTAAATTTGACTGGCTTTAA</rna> Please generate novel RNA sequence. <rna> Unconditional RNA Generation This task has 5000 test samples. Example 1: Prompt: Produce material that has any bulk modulus or composition Response: <material>Zr Zr Zr Zr Zr Zr Zr Zr O O O O <sg> <sg61></material> Unconditional Material Generation This task has 5000 test samples. Example 1: Prompt: Given F, In, Tc, Zr, design material composed of them. Response: materialZr Zr Zr Zr Zr Zr In In In In In In Tc Tc F F F F F F F F F F sg sg176/material Composition to Gaterial Generation This task has 10000 test samples. Given bulk modulus of 75.321 GPa, formulate the composition needed Example 1: Prompt: to achieve it. Response: materialLi Li Li Li Li Li Li Li Li Li Li Li Rh Rh Rh Rh N N sg sg141/material Bulk to Material Generation This task has 3000 test samples. Design molecule that meets the criteria outlined in the description. Example 1: Prompt: The molecule is natural product found in Piper aequale with data available. Response: <SMILES> C/C=C/C1=CC2=C(C=C1)OC(=C2C)C3=CC=C(C=C3)OC4=C(C=CC(=C4)C5 =C(C6=C(O5)C=CC(=C6)/C=C/C)C)O </SMILES> Description-Guided Molecule Design This task has 1000 test samples."
        },
        {
            "title": "Synthesize a protein sequence with the appropriate folding and",
            "content": "Example 1: Prompt: stability properties for the desired function. to modulate glycine decarboxylation via glycine cleavage system in way that leads to desirable outcome. Response: The designed proteins amino acid sequence is as follows: <protein>MNIIKTGIKQSLVSGIYSKVGIRAFCTHYSAELEWVKLSDDNKVATVGLSSFGAQRLGKINYVELP KEHRKCRREEKFGVLESSNATAFGLYAPVSGEVLEVNEKLKKSPSLLNEDPANNWMVKFKVSKPDEFKKLMDSNK YKKFVQWYR</protein>"
        },
        {
            "title": "The protein should be able",
            "content": "1. Protein Design This task has 1176 test samples. Example 1: Prompt: <SMILES> CCI.O=C(O)C1=CC=CC=C1Br.CN(C)C=O.O.O=C([O-1])O.[Na+1] </SMILES> Response: <SMILES> CCOC(=O)C1=CC=CC=C1Br </SMILES> With the provided reactants and reagents, propose potential product. Forward Reaction Prediction This task has 1000 test samples. Based on the given chemical reaction, can you propose Example 1: Prompt: some likely reagents that might have been utilized? CCCBr.CCOCCN1C(CN2CCNCC2)=NC3=CC=CC=C31 </SMILES> Response: <SMILES> CCO.O=C([O-1])[O-1].[K+1].[K+1] </SMILES> <SMILES> Reagent Prediction This task has 1000 test samples. Please suggest potential reactants for the given product. Example 1: Prompt: COC1=C(N)C=CC2=C1CCCC(N3CCOCC3)C2 </SMILES> Response: <SMILES> COC1=C([N+1](=O)[O-1])C=CC2=C1CCCC(N3CCOCC3)C2 </SMILES> <SMILES> With the provided product, recommend some probable Example 2: Prompt: reactants that were likely used in its production.<SMILES> [CH3:1][C:2]([CH3:3])([CH3:4])[O:5][C:6](=[O:7])[n:15]1[c:14]2[cH:13][cH:12][c:1 1]([C:9]([CH3:8])=[O:10])[cH:19][c:18]2[cH:17][cH:16]1 </SMILES> Response: <SMILES> CC(C)(C)OC(=O)O[C:6]([O:5][C:2]([CH3:1])([CH3:3])[CH3:4])=[ O:7].[CH3:8][c:9]1[cH:10][cH:11][c:12]([S:13](=[O:14])(=[O:15])[O:16][C@@H:17]2[ CH2:18][NH:19][C@H:20]3[C@@H:21]2[O:22][CH2:23][C@@H:24]3[OH:25])[cH:26][cH:27]1 </SMILES>"
        },
        {
            "title": "Identify possible reactants that could have been used to create the",
            "content": "Example 3: Prompt: specified product. <SMILES> CC1=CC=C(N)N=C1N </SMILES> Response: <SMILES> CC(C#N)CCC#N.N </SMILES> Retrosynthesis Prediction This task has 4156 test samples. <SMILES> NC1=CC=C2OCOC2=C1.O=CO </SMILES> Based on the reactants and Example 1: Prompt: reagents given above, suggest possible product. Response: possible product can be <SMILES> O=CNC1=CC=C2OCOC2=C1 </SMILES> . Forward Synthesis This task has 4062 test samples."
        },
        {
            "title": "Give me a molecule that satisfies the conditions outlined in the",
            "content": "Example 1: Prompt: description: The molecule is member of the class of tripyrroles that is red-coloured pigment with antibiotic properties produced by Serratia marcescens. It has role as an antimicrobial agent, biological pigment, bacterial metabolite, an apoptosis inducer and an antineoplastic agent. tripyrrole, an aromatic ether and ring assembly. Response: Here is potential molecule: =CN3)C=C2OC)=C1 </SMILES> <SMILES> CCCCCC1=C(C)NC(/C=C2=C(C3=CC"
        },
        {
            "title": "It is",
            "content": "Molecule Generation This task has 2493 test samples."
        },
        {
            "title": "Eating disorders and the associated",
            "content": "Example 1: Prompt: You are provided with set of clinical trial summaries. the chemical-disease relations from the summaries and present your findings in the format of (Subject, Object). behavioural problems and drug abuse are uncommon in pregnancy. do occur they are often unrecognized because of denial but when significant may pose risk to both the mother and her fetus. number of problems that may be encountered in women with eating disorders in pregnancy, including prolonged and recurrent metabolic disturbances and diuretic abuse. In particular it illustrates the derangements of thyroid function seen in pregnant women with eating disorders and reminds us that when cause for thyrotoxicosis remains obscure, thyroxine abuse should be considered and explored. Response: (thyroxine, thyrotoxicosis)"
        },
        {
            "title": "When they",
            "content": "Chemical Disease Interaction Extraction This task has 50 test samples. 79 Example 1: Prompt: Hence , Identify the chemical entities mentioned in the given text. to elucidate the cellular mechanisms that can address these divergent reports , we studied intestinal absorption of the dual substrate loperamide in portal vein - cannulated - gp - competent and - gp - deficient mice . Response: loperamide Chemical Entity Recognition This task has 100 test samples."
        },
        {
            "title": "Your task is to extract the subject and object entities from the",
            "content": "Example 1: Prompt: provided text and establish the corresponding relation triples. ORF 17583, histamine format your answers as (Subject, Relation, Object). H2-receptor antagonist, inhibited gastric acid secretion in pylorus-ligated rats (ED50 = 4.9 mg/kg intraduodenal; 3.4 mg/kg p.o.; and 0.21 mg/kg i.p.) and in total gastric fistula or Heidenhain pouch dogs stimulated by betazole (ED50 = 0.12 mg/kg p.o. and 0.08 mg/kg i.v.), histamine, tetragastrin, bethanechol, 2-deoxy-D-glucose or meal (ED50 values ranged from 0.11-0.26 mg/kg p.o.). The nonspecific inhibition of gastric acid by ORF 17583 supports the existence of interdependence between histamine and the gastrin and cholinergic receptors on the parietal cell surface. of ORF 17583 after intraduodenal administration in pylorus-ligated rats was 6.4 times greater than cimetidine, 1.8 times greater than ranitidine, equal to that of omeprazole and 8 times less than that of famotidine. antisecretory potency of ORF 17583 in gastric fistula dogs was 31 times greater than cimetidine, 3.7 times greater than ranitidine and equal to that of omeprazole and famotidine. Studies using equieffective antisecretory doses of ORF 17583 and ranitidine in dogs suggested that ORF 17583 has short duration of antisecretory activity similar to that of ranitidine. Response: (ORF 17583, antagonist, histamine H2-receptor), (ORF 17583, antagonist, histamine H2-receptor)"
        },
        {
            "title": "Oral",
            "content": "Chemical Protein Interaction Extraction This task has 70 test samples."
        },
        {
            "title": "The following is a marker of acute hepatitis B infection What of the",
            "content": "Example 1: Prompt: following is the right choice? (A) DNA polymerase (B) Hepatitis core antigen (C) Anti HBs (D) IgG to core antigen Response: The final answer is (A). General Multiple Choice Question This task has 1001 test samples. Example 1: Prompt: Response: Tricuspid valve bulging into right atrium wave in JVP is due to 80 Open-Ended Question This task has 1004 test samples. Is routine intraoperative frozen-section examination of sentinel Example 1: Prompt: lymph nodes in breast cancer worthwhile? Response: Maybe, in breast cancer patients having SLN biopsy, the failure of routine intraoperative FS is largely the failure to detect micrometastatic disease. The benefit of routine intraoperative FS increases with tumor size. cancers."
        },
        {
            "title": "Routine FS may not be indicated in patients with the smallest invasive",
            "content": "True/False Question This task has 100 test samples."
        },
        {
            "title": "TMScore for Conditional Protein Generation",
            "content": "Figure 23 TMScore for last 1000 samples"
        }
    ],
    "affiliations": [
        "Beihang University",
        "Fudan University",
        "Shanghai Artificial Intelligence Laboratory",
        "Shanghai Jiao Tong University",
        "The Chinese University of Hong Kong",
        "The University of Sydney",
        "University of Oxford",
        "University of Science and Technology of China"
    ]
}