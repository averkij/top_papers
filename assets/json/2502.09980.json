{
    "paper_title": "V2V-LLM: Vehicle-to-Vehicle Cooperative Autonomous Driving with Multi-Modal Large Language Models",
    "authors": [
        "Hsu-kuang Chiu",
        "Ryo Hachiuma",
        "Chien-Yi Wang",
        "Stephen F. Smith",
        "Yu-Chiang Frank Wang",
        "Min-Hung Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Current autonomous driving vehicles rely mainly on their individual sensors to understand surrounding scenes and plan for future trajectories, which can be unreliable when the sensors are malfunctioning or occluded. To address this problem, cooperative perception methods via vehicle-to-vehicle (V2V) communication have been proposed, but they have tended to focus on detection and tracking. How those approaches contribute to overall cooperative planning performance is still under-explored. Inspired by recent progress using Large Language Models (LLMs) to build autonomous driving systems, we propose a novel problem setting that integrates an LLM into cooperative autonomous driving, with the proposed Vehicle-to-Vehicle Question-Answering (V2V-QA) dataset and benchmark. We also propose our baseline method Vehicle-to-Vehicle Large Language Model (V2V-LLM), which uses an LLM to fuse perception information from multiple connected autonomous vehicles (CAVs) and answer driving-related questions: grounding, notable object identification, and planning. Experimental results show that our proposed V2V-LLM can be a promising unified model architecture for performing various tasks in cooperative autonomous driving, and outperforms other baseline methods that use different fusion approaches. Our work also creates a new research direction that can improve the safety of future autonomous driving systems. Our project website: https://eddyhkchiu.github.io/v2vllm.github.io/ ."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 1 ] . [ 1 0 8 9 9 0 . 2 0 5 2 : r V2V-LLM: Vehicle-to-Vehicle Cooperative Autonomous Driving with Multi-Modal Large Language Models Hsu-kuang Chiu1,2 Ryo Hachiuma1 Chien-Yi Wang1 Stephen F. Smith2 Yu-Chiang Frank Wang1 Min-Hung Chen1 1NVIDIA, 2Carnegie Mellon University"
        },
        {
            "title": "Abstract",
            "content": "Current autonomous driving vehicles rely mainly on their individual sensors to understand surrounding scenes and plan for future trajectories, which can be unreliable when the sensors are malfunctioning or occluded. To address this problem, cooperative perception methods via vehicleto-vehicle (V2V) communication have been proposed, but they have tended to focus on detection and tracking. How those approaches contribute to overall cooperative planning performance is still under-explored. Inspired by recent progress using Large Language Models (LLMs) to build autonomous driving systems, we propose novel problem setting that integrates an LLM into cooperative autonomous driving, with the proposed Vehicle-to-Vehicle Question-Answering (V2V-QA) dataset and benchmark. We also propose our baseline method Vehicle-to-Vehicle Large Language Model (V2V-LLM), which uses an LLM to fuse perception information from multiple connected autonomous vehicles (CAVs) and answer driving-related questions: grounding, notable object identification, and planning. Experimental results show that our proposed V2VLLM can be promising unified model architecture for performing various tasks in cooperative autonomous driving, and outperforms other baseline methods that use different fusion approaches. Our work also creates new research direction that can improve the safety of future autonomous driving systems. 1. Introduction Autonomous driving technology has advanced significantly due to the evolution of deep learning algorithms, computing infrastructures, and the release of large-scale real-world driving datasets and benchmarks [3, 13, 38]. However, the perception and planning systems of autonomous vehicles in daily operation rely strictly on their local LiDAR sensors and cameras to detect notable nearby objects and plan Figure 1. Overview of our problem setting of LLM-based cooperative autonomous driving. All CAVs share their perception information with the LLM. Any CAV can ask the LLM question to obtain useful information for driving safety. for future trajectories. This approach may encounter safetycritical problems when the sensors are occluded by nearby large objects. In such situations, autonomous driving vehicles are unable to accurately detect all nearby notable objects, making subsequent trajectory planning results unreliable. To address this safety-critical issue, recent research proposes cooperative perception algorithms [6, 9, 44, 5052] via vehicle-to-vehicle (V2V) communication. In cooperative driving scenarios, multiple Connected Autonomous Vehicles (CAVs) driving nearby to each other share their perception information via V2V communication. The received perception data from multiple CAVs is then fused to generate better overall detection results. To support and stimulate this research, number of cooperative autonomous driving datasets have been released to the public, including simulated ones [10, 24, 51, 52] and real ones [48, 53, 59, 60]. These datasets also establish benchmarks to evaluate the performance of cooperative perception algorithms. However, to date, cooperative driving research and datasets have mostly focused on perception tasks. How these state-of-theart cooperative perception models can be connected with the downstream planning models to generate good cooperative 1 planning results has not yet been well explored. Other recent research has attempted to use LLM-based methods to build end-to-end perception and planning algorithms for an individual autonomous vehicle [5, 34, 37, 39, 40, 43, 46, 54] due to their common-sense reasoning and generalization ability from large-scale pre-trained data. These LLM-based models encode the raw sensor inputs, such as camera images or LiDAR point clouds, into visual features, and then perform visual understanding and answer varieties of driving-related perception and planning questions. These approaches have shown some promise but have not yet explored the benefits of cooperative perception and planning. LLM-based driving algorithms without cooperative perception could also face safety-critical issues when the individual vehicles sensors are occluded. In this paper, we propose and explore novel problem setting wherein LLM-based methods are used to build endto-end perception and planning algorithms for Cooperative Autonomous Driving, as illustrated in Fig. 1. In this problem setting, we assume that there are multiple CAVs and centralized LLM computing node. All CAVs share their individual perception information with the LLM. Any CAV can ask the LLM question in natural language to obtain useful information for driving safety. To enable the study of this problem setting, we first create the Vehicleto-Vehicle Question-Answering (V2V-QA) dataset, built upon the V2V4Real [53] cooperative perception dataset for autonomous driving. Our V2V-QA includes grounding (Figs. 2a to 2c), notable object identification (Fig. 2d), and planning (Fig. 2e) question-answer pairs. Our specially designed grounding QAs focus on the potential occluded regions of each individual CAV. Our notable object identification and planning QAs evaluate the performance of different models in the metrics most relevant to overall cooperative driving safety. Different from other existing LLM-based QA datasets [4, 33, 35, 37, 39, 43], our QA supports multiple CAVs in cooperative driving scenarios, as described in our aforementioned problem setting. The main differences between our V2V-QA and other related datasets are summarized in Tab. 1. To establish benchmark for the V2V-QA dataset, we first propose strong baseline method: Vehicle-to-Vehicle Large Language Model (V2V-LLM) for cooperative autonomous driving, as illustrated in Fig. 3. Each CAV extracts its own perception features and shares them with V2V-LLM. The V2V-LLM fuses the scene-level feature maps and object-level feature vectors, and then performs vision and language understanding to provide the answer to the input driving-related questions in V2V-QA. We also compare V2V-LLM with other baseline methods corresponding to different feature fusion methods: no fusion, early fusion, and intermediate fusion [48, 5053]. The results show that V2V-LLM achieves the best performance in the more important notable object identification and planning tasks and the second-best performance in the grounding tasks, achieving strong performance for the overall autonomous driving system. Our contribution can be summarized as follows: We create and introduce the V2V-QA dataset to support the development and evaluation of LLM-based approaches to end-to-end cooperative autonomous driving. V2V-QA includes grounding, notable object identification, and planning question-answering tasks. We propose baseline method V2V-LLM for cooperative autonomous driving to provide an initial benchmark for V2V-QA. This method fuses scene-level feature maps and object-level feature vectors provided by multiple CAVs, and answers different CAVs driving-related questions. We create benchmark for V2V-QA and show that V2V-LLM outperforms other baseline fusion methods on the notable object identification and planning tasks and achieves competitive results on the grounding tasks, indicating the potential of V2V-LLM to be foundation model for cooperative autonomous driving. 2. Related Work 2.1. Cooperative Perception in Autonomous Driving Cooperative perception [16] algorithms were proposed to address the potential occlusion problem in individual autonomous vehicles. Pioneering work F-Cooper [6] proposes the first intermediate fusion approach that merges feature maps to achieve good cooperative detection performance. V2VNet [44] builds graph neural networks for cooperative perception. DiscoNet [23] adopts knowledge distillation approach. More recent work, AttFuse [52], V2X-ViT [51], and CoBEVT [50] integrate attention-based models to aggregate features. Another group of works [8, 15, 29, 55] focuses on developing efficient communication approaches. From dataset perspective [28, 57], simulation datasets, OPV2V [52], V2X-Sim [24], and V2XSet [51] were first generated for cooperative perception research. More recently, real datasets have been collected. V2V4Real [53] is the first worldwide available real vehicle-to-vehicle cooperative perception dataset with detection and tracking benchmarks. Other datasets [48, 59, 60] further include sensor data from roadside infrastructures. Different from this group of research, our problem setting and proposed V2V-QA dataset include both perception and planning question-answering tasks for multiple CAVs. Our proposed V2V-LLM model also adopts novel LLMbased fusion approach. 2.2. LLM-based Autonomous Driving LLM has been used to build planning algorithms for individual autonomous vehicles in recent research [21, 31, 32]."
        },
        {
            "title": "Dataset",
            "content": "Publication # CAVs Sim/Real # Frames # QA # QA/frame Point Cloud Planning AD NuScenes [3] Waymo [38] Cooperative perception in AD OPV2V [52] V2XSet [51] V2V4Real [53] V2X-Real [48] LLM-based AD NuScenes-QA [35] Lingo-QA [33] MAPLM-QA [4] DriveLM [37] TOKEN [39] OmniDrive-nuScenes [43] V2V-QA (Ours) CVPR 2020 CVPR 2020 ICRA 2022 ECCV 2022 CVPR 2023 ECCV 2024 AAAI 2024 ECCV 2024 CVPR 2024 ECCV 2024 CoRL 2024 arXiv 2024 - - - 2-7 2-5 2 2 - - - - - -"
        },
        {
            "title": "Sim\nSim\nReal\nReal",
            "content": "Real Real Real Sim+Real Real Real Real 400K 200K 11K 11K 20K 33K - - - - - - 34K 460K 28K 420K 14K 61K 69K 2M 28K 434K 34K 450K 18K 577K - - - - - - 13.5 15.3 4.4 29.1 15.5 13.2 31.7 Table 1. Comparison between our V2V-QA and recent related Autonomous Driving (AD) datasets. This number of frames includes the validation split of V2V4Real [53], which is not released to the public. We build our V2V-QA upon the released training and testing splits of V2V4Real [53]. V2X-Real [48] only releases subset of data to the public. Such language-based planning model first transforms the driving scene, object detection results, and ego-vehicles state into text input to the LLM. Then the LLM generates text output including the suggested driving action or the planned future trajectory. However, this approach may miss important detailed visual information from the raw input from LiDAR sensors or cameras. More recent approaches [37, 39, 40, 43, 45, 54, 56] use Multimodal Large Language Models (MLLMs) [1, 2, 17, 22, 25 27, 36, 41, 42] to encode point clouds or images into visual features. Then, the visual features are projected to the language embedding space for LLM to perform visual understanding and question-answering tasks, such as captioning, grounding, and planning for autonomous vehicles. From dataset perspective, several LLM-based autonomous driving datasets have been built on top of existing autonomous driving datasets. For example, Talk2Car [11], NuPrompt [47], NuScenes-QA [35], NuInstruct [49], and Reason2Drive [34] create captioning, perception, prediction, and planning QA pairs based on the NuScenes [3] dataset. BDD-X [18] is extended from BDD100K [58]. DriveLM [37] adopts real data from NuScenes [3] and simulated data from CARLA [12] to have larger-scale and more diverse driving QAs. Other datasets curated independently focus on different types of QA tasks. HAD [19] contains human-to-vehicle advice data. DRAMA [30] introduces joint risk localization and captioning. Lingo-QA [33] proposed counterfactual question-answering tasks. MAPLMQA [4] emphasizes map and traffic scene understanding. ting and proposed V2V-QA dataset are designed for cooperative driving scenarios with multiple CAVs. 3. V2V-QA Dataset To enable the research in our proposed novel problem setting: LLM-based cooperative autonomous driving, we create the Vehicle-to-Vehicle Question-Answering (V2VQA) dataset to benchmark different models performance on fusing perception information and answering safetycritical driving-related questions. 3.1. Problem Setting Our proposed V2V cooperative autonomous driving with LLM problem is illustrated in Fig. 1. In this setting, we assume there are multiple Connected Autonomous Vehicles (CAVs) and centralized LLM computing node. All CAVs share their individual perception information, such as scene-level feature maps and object-level feature vectors with the centralized LLM. Any CAV can ask the LLM question in natural language to obtain information for driving safety. The LLM aggregates the received perception information from multiple CAVs and provides natural language answer to the CAVs question. In this research, the questions and answers include grounding (Q1-3), notable object identification (Q4), and planning (Q5), as illustrated in Fig. 2. 3.2. Dataset Details Different from all LLM-based driving research that only supports individual autonomous vehicles, our problem setOur V2V-QA dataset is built on top of the V2V4Real [53] dataset, which is the first worldwide available real dataset 3 (a) Q1: Grounding at reference location. (b) Q2: Grounding behind reference object at location. (c) Q3: Grounding behind reference object in direction. (d) Q4: Notable object identification. (e) Q5: Planning. Figure 2. Illustration of V2V-QAs 5 types of QA pairs. The arrows pointing at LLM indicate the perception data from CAVs. QA type Q1 Q2 Q3 Q4 Q5 Total Training 354820 35700 14339 12290 12290 Testing 121383 13882 5097 3446 3446 147254 Total 476203 49582 19436 15736 15736 576693 Table 2. Dataset statistics of our V2V-QA. Q1: Grounding at reference location. Q2: Grounding behind reference object at location. Q3: Grounding behind reference object in direction. Q4: Notable object identification. Q5: Planning. with vehicle-to-vehicle cooperative perception benchmark. This base dataset is collected by driving two vehicles with LiDAR sensors simultaneously near to each other. In addition to the raw LiDAR point clouds, the dataset also includes 3D bounding box annotations for all vehicles in the driving scenes. The training split has 32 driving sequences and total of 7105 frames of data per CAV, and the testing split has 9 driving sequences and total of 1993 frames of data per CAV. The frame rate is 10Hz. We follow the same training and testing splits when building our V2V-QA datasets 5 types of QA pairs. Tab. 2 summarizes the numbers of QA pairs in our proposed V2V-QA. We have 577K QA pairs in total and 31.7 QA pairs per frame on average. More details can be found in the supplementary materials. 3.3. Question and Answer Pairs Curation For each frame of V2V4Real [53] dataset, we create 5 different types of QA pairs, including 3 types of grounding questions, 1 type of notable object identification question, and 1 type of planning question. These QAs are designed for cooperative driving scenarios. To generate instances of these QA pairs, we use V2V4Real [53]s ground-truth bounding box annotations, each CAVs ground-truth trajectories, and individual detection results as the source information. Then we use different manually designed rules based on the geometric relationship among the aforementioned entities and text templates to generate our QA pairs. The text template can be seen in Figs. 5 and 6. The generation rule of each QA type is described as follows. Q1. Grounding at reference location (Fig. 2a): In this type of question, we ask the LLM to identify whether there exists an object occupying specific query 2D location. If so, the LLM is expected to provide the center location of the object. Otherwise, the LLM should indicate that there is nothing at the reference location. To generate instances of this type of QA pair, we use the center locations of ground-truth boxes and every CAVs individual detection result boxes as the query locations in the questions. By doing so, we can focus more on evaluating each models cooperative grounding ability on the potential false positive and false negative detection results. Q2. Grounding behind reference object at location (Fig. 2b): When CAVs field of view is occluded by nearby large detected object, this CAV may want to ask the centralized LLM to determine whether there exists any object behind that occluding large object given the fused perception information from all CAVs. If so, the LLM is expected to return the objects location and the asking CAV may need to drive more defensively or adjust its planning. Otherwise, the LLM should indicate that there is nothing behind the reference object. To generate instances of this type of QA pair, we use the center location of each detection result box as the query locations in these questions. We draw sector region based on the relative pose of the asking CAV and the reference object, and select the closest groundtruth object in the region as the answer. Q3. Grounding behind reference object in direction (Fig. 2c): We further challenge the LLM on language and spatial understanding ability by replacing Q2s reference 2D location with reference directional keyword. To generate instances of this type of QA pair, we first get the closest detection result box in each of the 6 directions of CAV as the reference object. Then we follow the same data generation approach in Q2 to get the closest ground-truth box in the corresponding sector region as the answer. Q4. Notable object identification (Fig. 2d): The aforementioned grounding tasks can be seen as intermediate tasks in the autonomous driving pipeline. More critical abilities of autonomous vehicles involve both identifying notable objects near planned future trajectories and adjusting future planning to avoid potential collisions. In the notable object identification questions, we extract 6 waypoints from the ground-truth trajectory in the next 3 seconds as the reference future waypoints in the questions. Then we get, at most, the 3 closest ground-truth objects within 10 meters of the reference future trajectory as the answer. Q5. Planning (Fig. 2e): Planning is the most important output of autonomous driving systems in comparison to the aforementioned QA Types because the ultimate goal of autonomous vehicles is to navigate through complex environments safely and avoid any potential collision in the future. To generate the planning QAs, we extract 6 future waypoints, evenly distributed in the next 3 seconds, from each CAVs ground-truth future trajectory as the answer. Our V2V-QAs planning task is also more challenging than other NuScenes [3]-based LLM-driving related works [39, 43] for couple reasons. First, we support multiple CAVs in cooperative driving scenarios. The LLM model needs to provide different answers depending on which CAV is asking for its suggested future trajectory, while prior works only need to generate planning results for single autonomous vehicle. Second, our V2V-QA is based on V2V4Real [53], which includes both urban and highway driving scenarios. The motion patterns of vehicles vary lot in those two different environments. On the contrary, NuScenes [3]-based LLM-driving research only needs to consider urban driving scenarios. Figure 3. Model diagram of our proposed V2V-LLM for cooperative autonomous driving. 3.4. Evaluation Metrics Following prior works [39, 43], we evaluate the performance of different models on the grounding questions (Q1, Q2, Q3) and the notable object identification question (Q4) with the F1 score, precision, and recall. For the planning question (Q5), the evaluation metrics are L2 errors and collision rates. 4. V2V-LLM In addition to the proposed dataset, we propose competitive baseline model, V2V-LLM, for this LLM-based collaborative driving problem, as shown in Fig. 3. This model is Multi-modal LLM (MLLM) that takes the individual perception features of every CAV as the vision input, question as the language input, and generates an answer as the language output. 4.1. LiDAR-based Input Features For extracting the perception input features, each CAV applies 3D object detection model to its individual LiDAR point cloud: PEGO and P1. We extract the scene-level feature map SEGO and S1 from the 3D object detection model and transform the 3D object detection results as the object-level feature vectors OEGO and O1. Following prior works V2V4Real [53] and V2X-Real [48], we use PointPillars [20] as the 3D object detector for fair comparisons. 4.2. LiDAR-based LLM Model architecture: We utilize LLaVA [25] to develop our MLLM, given its superior performance on visual questionanswering tasks. However, since the perception features of our cooperative driving tasks are LiDAR-based instead of RGB images used by LLaVA [25], we use LiDAR-based 3D object detector as the point cloud feature encoder, as described in the previous section, instead of LLaVA [25]s 5 (a) No fusion (b) Early fusion (c) Intermediate fusion [5052] Figure 4. Feature encoder diagrams of the baseline methods from different fusion approaches. CLIP [36] image feature encoders. We then feed the resulting features to multi-layer perceptron-based projector network for feature alignment from the point cloud embedding space to the language embedding space. The aligned perception features are the input perception tokens digested by the LLM together with the input language tokens from the question. Finally, the LLM aggregates the perception information from all CAVs and returns an answer based on the question. Training: Our V2V-LLM uses LLaVA-v1.5-7b [25]s Vicuna [7] as the LLM backbone. To train our model, we initialize it by loading the pre-trained LLaVA-v1.5-7b [25]s checkpoint. We freeze the LLM and the point cloud feature encoder, and only finetune the projector and the LoRA [14] parts of the model. During training, we use batch size 32. We train our Q1 for 1 epoch, and other QA types for 10 epochs. For all other training settings and hyperparameters, we use the same ones from LLaVA-v1.5-7b [25]. 5. Experiment 5.1. Baseline Methods We follow V2V4Real [53] and V2X-Real [48] to establish benchmark for our proposed V2V-QA dataset with experiments on baseline methods using different fusion approaches: no fusion, early fusion, intermediate fusion, and our proposed baseline, LLM fusion  (Fig. 3)  . Compared with our method, other baseline methods also adopt the same projector and LLM architecture but with different point cloud feature encoders. Their feature encoder diagrams can be seen in Fig. 4. No fusion: Only single CAVs LiDAR point cloud is fed to single 3D object detector to extract the scene-level feature map and the object-level feature vectors, which are then used as the LLMs visual input. The performance is expected to be worse than all other cooperative perception approaches because the other CAVs sensor input is ignored. Early fusion: The LiDAR point cloud from two CAVs is merged first. Then the merged point cloud is used as input to 3D object detector to extract the visual features as the visual input to the LLM. This approach may achieve good performance by using all the raw sensor input but requires much higher communication bandwidth when compared with other fusion approaches. This limitation causes early-fusion approaches to be impractical for deployment on real-world autonomous vehicles. Intermediate fusion: Prior research CoBEVT [50], V2XViT [51], and AttFuse [52] propose different cooperative detection models that can merge feature maps from multiple CAVs via attention mechanisms. Such approaches require less communication bandwidth and can still achieve good performance. In our benchmark, we extract the features from those cooperative detection models as the visual input to the LLM. LLM fusion: We categorize our proposed V2V-LLM as new type of fusion method, LLM fusion, which lets each CAV perform its individual 3D object detection to extract the scene-level feature maps and object-level feature vectors, and uses the LLM to fuse the features from multiple CAVs. This approach is related to the traditional late fusion method that performs individual 3D object detection and aggregates the results by non-maximum suppression (NMS). Instead of applying NMS, our method adopts LLM to perform more tasks than just detection. 5.2. Experimental Results 5.2.1. Grounding Our V2V-LLM and baseline methods performance on V2V-QAs 3 types of grounding questions can be seen in Tab. 3. We can see that all fusion methods outperform the no-fusion approach. Early fusion, V2X-ViT [50] intermediate fusion, and our proposed V2V-LLM achieve best results in Q1, Q2, and Q3 respectively. In average, CoBEVT [50] achieves the best result and V2V-LLM achieves the secondbest result. Such results indicate that our MLLM also has promising capability of fusing scene-level feature maps and object-level feature vectors from multiple CAVs. And its performance is not too far away from the performance of other, specially designed intermediate fusion models. 5.2.2. Notable Object Identification Tab. 3 shows the performance on the notable object identification task (Q4). Our proposed V2V-LLM outperforms other methods. Compared with the aforementioned grounding tasks, this notable object identification task requires more spatial understanding and reasoning ability to identify the objects close to the planned future waypoints provided by the asking CAV. For such task, our V2V-LLM, which 6 Method Q1 Q2 Q3 QGr Q4 Q5 Comm(MB) F1 F1 F1 F1 F1 L2avg (m) CRavg (%) 66.6 77.9 58.2 22.6 29.4 18.4 17.2 17.4 16.9 35.5 47.3 49.2 45.6 73.5 82.2 66.5 23.3 29.1 19.5 20.8 22.7 19.3 39.2 53.9 55.4 52.6 4.57 3.55 6.55 6. No Fusion Early Fusion Intermediate Fusion AttFuse [52] V2X-ViT [51] CoBEVT [50] LLM Fusion V2V-LLM (Ours) 70.0 80.1 62.2 30.8 36.3 26.7 21.2 21.5 20.8 40.7 59.7 61.9 57.6 70.7 79.6 63.6 26.4 31.6 22.7 18.4 19.6 17.4 38.5 56.9 57.2 56.6 70.8 81.1 62.8 28.0 33.9 23.9 22.6 25.2 20.5 40.5 57.6 57.0 58.2 72.2 76.8 68.1 29.3 34.7 25.3 21.3 22.1 20.6 40.9 57.6 57.2 58.1 6.83 7.08 6.72 4.99 4.12 4.33 3.88 3. 0 0.96 0.20 0.20 0.20 0.203 Table 3. V2V-LLMs performance in V2V-QAs testing split in comparison with baseline methods. Q1: Grounding at reference location. Q2: Grounding behind reference object at location. Q3: Grounding behind reference object in direction. QGr: Average of grounding (Q1, Q2, and Q3). Q4: Notable object identification. Q5: Planning. P: Precision. R: Recall. L2: L2 distance error. CR: Collision Rate. Comm: Communication cost. In each column, the best results are in boldface, and the second-best results are in underline. Method No Fusion Early Fusion Intermediate Fusion AttFuse [52] V2X-ViT [51] CoBEVT [50] LLM Fusion V2V-LLM (ours) L2 (m) 3s 9.30 8.74 9.64 9.99 9.47 2s 6.52 6.19 6.78 7.05 6.71 1s 3.84 3. 4.06 4.21 3.97 average 6.55 6.20 6.83 7.08 6.72 CR (%) 1s 1.31 0.96 1.42 1.33 0. 2s 4.76 3.86 4.41 4.82 3.74 3s 7.63 5.83 6.53 6.85 6.96 average 4.57 3.55 4.12 4.33 3. Comm (MB) 0 0.96 0.20 0.20 0.20 2.96 4.97 7. 4.99 0.55 3.19 5.25 3.00 0. Table 4. V2V-LLMs planning performance in V2V-QAs testing split in comparison with baseline methods. L2: L2 distance error. CR: Collision Rate. In each column, the best results are in boldface. and the second-best results are in underline. lets the LLM perform both feature fusion and question answering, achieves the best results. 5.2.3. Planning Tab. 4 shows the detailed performance on the planning task (Q5). Our proposed V2V-LLM outperforms other methods in this most important question which requires more spatial understanding and reasoning ability to generate safe future trajectory that can avoid potential collisions. 5.2.4. Summary Overall, V2V-LLM achieves the best results in the notable object identification and planning tasks, which are more important than the grounding tasks in autonomous driving applications. V2V-LLM also achieves the second-best result in the grounding tasks. In terms of communication costs, V2V-LLM shares both scene-level-feature maps and object-level-feature vectors and only increases communication costs by 1.5% in comparison to other intermediate fusion baseline methods. 5.3. Ablation Study We experiment with variants of our V2V-LLM model that use either only the scene-level feature maps or only the object-level feature vectors as the visual input. The ablation results can be seen in Tab. 5. Overall, we can observe that both scene-level feature maps and object-level feature vectors contribute to final performance in all QA tasks. In Q1 and Q3, the scene-level-only model and object-levelonly model achieve similar performance. For other QA tasks, the object-level-only model outperforms the scenelevel-only model by large margin. This implies that the object-level features are easier for LLM to digest, which is consistent with the results observed in the previous work with the TOKEN model [39]. 5.4. Qualitative Results Fig. 5 shows our V2V-LLMs grounding results and the ground truth with visualization on V2V-QAs testing split. We can observe that our V2V-LLM is able to locate the objects given the provided reference information for each of the 3 types of grounding questions. Fig. 6s left part shows our V2V-LLMs notable object identification results. V2V-LLM demonstrate its capability of identifying multiple objects near the planned future trajectories specified in the questions for each CAV. Fig. 6s right part shows V2VLLMs planning results. Our model is able to suggest future trajectories that avoid potential collisions with nearby Method Q1 Q2 Q3 QGr Q5 Comm (MB) F1 F1 F1 F1 F1 L2avg (m) CRavg (%) Scene-level only 69.9 74.9 65.5 15.4 19.9 12.6 17.9 26.9 13.5 34.4 43.2 40.2 46.7 Object-level only 69.0 80.9 60.1 26.9 34.7 21.9 17.6 18.3 16.9 37.8 52.6 57.3 48.6 V2V-LLM (ours) 70.0 80.1 62.2 30.8 36.3 26.7 21.2 21.5 20.8 40.7 59.7 61.9 57.6 7.21 5.24 4.99 15.55 7.78 3. 0.20 0.003 0.203 Table 5. Ablation study in V2V-QAs testing split. Q1: Grounding at reference location. Q2: Grounding behind reference object at location. Q3: Grounding behind reference object in direction. QGr: Average of grounding (Q1, Q2, and Q3). Q4: Notable object identification. Q5: Planning. P: Precision. R: Recall. L2: L2 distance error. CR: Collision Rate. Comm: Communication cost. Figure 5. V2V-LLMs grounding results on V2V-QAs testing split. Magenta : reference locations in questions. Yellow +: model output locations. Green : ground-truth answers. Figure 6. V2V-LLMs notable object identification and planning results on V2V-QAs testing split. For notable object identification, Magenta curve: planned future trajectories in questions. Green : ground-truth notable object locations. Yellow + and Cyan : model identification outputs corresponding to CAV EGO and CAV 1, respectively. For planning, Green line: future trajectories in ground-truth answers. Yellow curve and Cyan curve: model planning outputs corresponding to CAV EGO and CAV 1, respectively. objects. Overall, the outputs of our model closely align with the ground-truth answers across all question types, indicating its robustness in cooperative autonomous driving tasks. 6. Conclusion In this work, we expand the research scope of cooperative autonomous driving by integrating the use of LLMbased methods, aimed at improving the safety of future autonomous driving systems. We propose new problem setting and create novel V2V-QA dataset and benchmark that includes grounding, notable object identification, and planning question-answering tasks designed for varieties of cooperative driving scenarios. We propose baseline model V2V-LLM that fuses each CAVs individual perception information and performs visual and language understanding to answer driving-related questions from any CAV. 8 In comparison to other baseline methods adopted from state-of-the-art cooperative perception algorithms, our proposed V2V-LLM achieves comparative performance in the grounding tasks and outperforms all other baseline methods in the more important notable object identification and planning tasks. These experimental results indicate that V2VLLM is promising as unified foundation model that can effectively perform perception and planning tasks for cooperative autonomous driving. We believe our V2V-QA dataset will bring the cooperative driving research field to the next stage. 7. Acknowledgement authors The and thank Boyi Li, Boris Marco Pavone for valuable discussions and comments. Ivanovic,"
        },
        {
            "title": "References",
            "content": "[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: visual language model for few-shot learning. In Advances in Neural Information Processing Systems (NeurIPS), 2022. 3 [2] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: versatile vision-language model for understanding, localization, text reading, and beyond. arXiv preprint arXiv:2308.12966, 2023. 3 [3] Holger Caesar, Varun Bankiti, Alex Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom. nuscenes: multimodal dataset for autonomous driving. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020. 1, 3, 5 [4] Xu Cao, Tong Zhou, Yunsheng Ma, Wenqian Ye, Can Cui, Kun Tang, Zhipeng Cao, Kaizhao Liang, Ziran Wang, James Rehg, and Chao Zheng. Maplm: real-world large-scale vision-language dataset for map and traffic scene understanding. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. 2, 3 [5] Long Chen, Oleg Sinavski, Jan Hunermann, Alice Karnsund, Andrew James Willmott, Danny Birch, Daniel Maund, and Jamie Shotton. Driving with llms: Fusing object-level vector modality for explainable autonomous driving. In IEEE International Conference on Robotics and Automation (ICRA), 2024. 2 [6] Qi Chen, Xu Ma, Sihai Tang, Jingda Guo, Qing Yang, and Song Fu. F-cooper: Feature based cooperative perception for autonomous vehicle edge computing system using 3d In ACM/IEEE Symposium on Edge Computpoint clouds. ing (SEC), 2019. 1, 2 [7] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, 2023. 9 [8] Hsu-kuang Chiu and Stephen F. Smith. Selective communication for cooperative perception in end-to-end autonomous driving. In IEEE International Conference on Robotics and Automation (ICRA) Workshop, 2023. 2 [9] Hsu-kuang Chiu, Chien-Yi Wang, Min-Hung Chen, and Stephen F. Smith. Probabilistic 3d multi-object cooperative tracking for autonomous driving via differentiable multisensor kalman filter. In IEEE International Conference on Robotics and Automation (ICRA), 2024. 1 [10] Jiaxun Cui, Hang Qiu, Dian Chen, Peter Stone, and Yuke Zhu. Coopernaut: End-to-end driving with cooperative perception for networked vehicles. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022. 1 [11] Thierry Deruyttere, Simon Vandenhende, Dusan Grujicic, Talk2car: Luc Van Gool, and Marie Francine Moens. In Empirical Taking control of your self-driving car. Methods in Natural Language Processing and International Joint Conference on Natural Language Processing (EMNLPIJCNLP), 2019. 3 [12] Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio Lopez, and Vladlen Koltun. CARLA: An open urban driving simulator. In Conference on Robot Learning (CoRL), 2017. 3 [13] Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we ready for autonomous driving? the kitti vision benchmark In IEEE/CVF Conference on Computer Vision and suite. Pattern Recognition (CVPR), 2012. [14] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations (ICLR), 2022. 6 [15] Yue Hu, Shaoheng Fang, Zixing Lei, Yiqi Zhong, and Siheng Chen. Where2comm: Communication-efficient collaborative perception via spatial confidence maps. In Advances in Neural Information Processing Systems (NeurIPS), 2022. 2 [16] Tao Huang, Jianan Liu, Xi Zhou, Dinh Nguyen, Mostafa Rahimi Azghadi, Yuxuan Xia, Qing-Long Han, and Sumei Sun. V2x cooperative perception for autonomous driving: Recent advances and challenges. arXiv preprint arXiv:2310.03525, 2023. 2 [17] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representaIn International tion learning with noisy text supervision. Conference on Machine Learning (ICML), 2021. 3 [18] Jinkyu Kim, Anna Rohrbach, Trevor Darrell, John Canny, and Zeynep Akata. Textual explanations for self-driving In European Conference on Computer Vision vehicles. (ECCV), 2018. [19] Jinkyu Kim, Teruhisa Misu, Yi-Ting Chen, Ashish Tawari, and John Canny. Grounding human-to-vehicle advice for self-driving vehicles. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019. 3 [20] Alex H. Lang, Sourabh Vora, Holger Caesar, Lubing Zhou, Jiong Yang, and Oscar Beijbom. Pointpillars: Fast encoders for object detection from point clouds. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019. 5 [21] Boyi Li, Yue Wang, Jiageng Mao, Boris Ivanovic, Sushant Veer, Karen Leung, and Marco Pavone. Driving everywhere with large language model policy adaptation. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. 2 [22] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with In Infrozen image encoders and large language models. ternational Conference on Machine Learning (ICML), 2023. 3 [23] Yiming Li, Shunli Ren, Pengxiang Wu, Siheng Chen, Chen Feng, and Wenjun Zhang. Learning distilled collaboration graph for multi-agent perception. In Advances in Neural Information Processing Systems (NeurIPS), 2021. [24] Yiming Li, Dekun Ma, Ziyan An, Zixun Wang, Yiqi Zhong, Siheng Chen, and Chen Feng. V2x-sim: Multi-agent collaborative perception dataset and benchmark for autonomous IEEE Robotics and Automation Letters (RA-L), driving. 2022. 1, 2 [25] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In Advances in Neural Information Processing Systems (NeurIPS), 2023. 3, 5, 6, 1 [26] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. [27] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, 2024. 3 [28] Mingyu Liu, Ekim Yurtsever, Jonathan Fossaert, Xingcheng Zhou, Walter Zimmer, Yuning Cui, Bare Luka Zagar, and Alois Knoll. survey on autonomous driving datasets: IEEE Statistics, annotation quality, and future outlook. Transactions on Intelligent Vehicles (T-IV), 2024. 2 [29] Yen-Cheng Liu, Junjiao Tian, Nathaniel Glaser, and Zsolt Kira. When2com: Multi-agent perception via communication graph grouping. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020. 2 [30] Srikanth Malla, Chiho Choi, Isht Dwivedi, Joon Hee Choi, and Jiachen Li. Drama: Joint risk localization and captioning in driving. In IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), 2023. [31] Jiageng Mao, Yuxi Qian, Junjie Ye, Hang Zhao, and Yue Wang. Gpt-driver: Learning to drive with gpt. In Advances in Neural Information Processing Systems (NeurIPS) Workshop (Foundation Models for Decision Making), 2023. 2 [32] Jiageng Mao, Junjie Ye, Yuxi Qian, Marco Pavone, and Yue Wang. language agent for autonomous driving. In Conference On Language Modeling (COLM), 2024. 2 [33] Ana-Maria Marcu, Long Chen, Jan Hunermann, Alice Karnsund, Benoit Hanotte, Prajwal Chidananda, Saurabh Nair, Vijay Badrinarayanan, Alex Kendall, Jamie Shotton, et al. Lingoqa: Visual question answering for autonomous driving. In European Conference on Computer Vision (ECCV), 2024. 2, 3 [34] Ming Nie, Renyuan Peng, Chunwei Wang, Xinyue Cai, Jianhua Han, Hang Xu, and Li Zhang. Reason2drive: Towards interpretable and chain-based reasoning for auIn European Conference on Computer tonomous driving. Vision (ECCV), 2024. 2, 3 [35] Tianwen Qian, Jingjing Chen, Linhai Zhuo, Yang Jiao, and Yu-Gang Jiang. Nuscenes-qa: multi-modal visual question answering benchmark for autonomous driving scenario. In AAAI Conference on Artificial Intelligence (AAAI), 2024. 2, 3 [36] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning (ICML), 2021. 3, 6 [37] Chonghao Sima, Katrin Renz, Kashyap Chitta, Li Chen, Hanxue Zhang, Chengen Xie, Ping Luo, Andreas Geiger, and Hongyang Li. Drivelm: Driving with graph visual question answering. In Europian Conference on Computer Vision (ECCV), 2024. 2, 3 [38] Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien Chouard, Vijaysai Patnaik, Paul Tsui, James Guo, Yin Zhou, Yuning Chai, Benjamin Caine, et al. Scalability in perception for autonomous driving: Waymo open dataset. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020. 1, [39] Ran Tian, Boyi Li, Xinshuo Weng, Yuxiao Chen, Edward Schmerling, Yue Wang, Boris Ivanovic, and Marco Pavone. Tokenize the world into object-level knowledge to address In Conference on long-tail events in autonomous driving. Robot Learning (CoRL), 2024. 2, 3, 5, 7 [40] Xiaoyu Tian, Junru Gu, Bailin Li, Yicheng Liu, Yang Wang, Zhiyong Zhao, Kun Zhan, Peng Jia, Xianpeng Lang, and Hang Zhao. Drivevlm: The convergence of autonomous driving and large vision-language models. arXiv preprint arXiv:2402.12289, 2024. 2, 3 [41] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint, 2023. 3 [42] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. 3 [43] Shihao Wang, Zhiding Yu, Xiaohui Jiang, Shiyi Lan, Min Shi, Nadine Chang, Jan Kautz, Ying Li, and Jose Alvarez. OmniDrive: holistic llm-agent framework for autonomous driving with 3d perception, reasoning and planning. arXiv:2405.01533, 2024. 2, 3, 5 [44] Tsun-Hsuan Wang, Sivabalan Manivasagam, Ming Liang, Bin Yang, Wenyuan Zeng, James Tu, and Raquel Urtasun. V2vnet: Vehicle-to-vehicle communication for joint perception and prediction. In European Conference on Computer Vision (ECCV), 2020. 1, 10 [56] Senqiao Yang, Jiaming Liu, Ray Zhang, Mingjie Pan, Zoey Guo, Xiaoqi Li, Zehui Chen, Peng Gao, Yandong Guo, and Shanghang Zhang. Lidar-llm: Exploring the potential of large language models for 3d lidar understanding. arXiv preprint, 2023. 3 [57] Melih Yazgan, Mythra Varun Akkanapragada, and Marius Zollner. Collaborative perception datasets in autonomous driving: survey. In IEEE Intelligent Vehicles Symposium (IV), 2024. 2 [58] Fisher Yu, Haofeng Chen, Xin Wang, Wenqi Xian, Yingying Chen, Fangchen Liu, Vashisht Madhavan, and Trevor Darrell. Bdd100k: diverse driving dataset for heterogeneous multitask learning. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020. 3 [59] Haibao Yu, Yizhen Luo, Mao Shu, Yiyi Huo, Zebang Yang, Yifeng Shi, Zhenglong Guo, Hanyu Li, Xing Hu, Jirui Yuan, and Zaiqing Nie. Dair-v2x: large-scale dataset for vehicleinfrastructure cooperative 3d object detection. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022. 1, 2 [60] Walter Zimmer, Gerhard Arya Wardana, Suren Sritharan, Xingcheng Zhou, Rui Song, and Alois Knoll. Tumtraf v2x cooperative perception dataset. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. 1, [45] Tsun-Hsuan Wang, Alaa Maalouf, Wei Xiao, Yutong Ban, Alexander Amini, Guy Rosman, Sertac Karaman, and Daniela Rus. Drive anywhere: Generalizable end-to-end autonomous driving with multi-modal foundation models. In IEEE International Conference on Robotics and Automation (ICRA), 2023. 3 [46] Wenhai Wang, Jiangwei Xie, ChuanYang Hu, Haoming Zou, Jianan Fan, Wenwen Tong, Yang Wen, Silei Wu, Hanming Deng, Zhiqi Li, et al. Drivemlm: Aligning multi-modal large language models with behavioral planning states for autonomous driving. arXiv preprint arXiv:2312.09245, 2023. 2 [47] Dongming Wu, Wencheng Han, Tiancai Wang, Yingfei Liu, Xiangyu Zhang, and Jianbing Shen. Language prompt for autonomous driving. arXiv preprint, 2023. 3 [48] Hoa Xiang, Zhaoliang Zheng, Xin Xia, Runsheng Xu, Letian Gao, Zewei Zhou, Xu Han, Xinkai Ji, Mingxi Li, Zonglin Meng, Li Jin, Mingyue Lei, Zhaoyang Ma, Zihang He, Haoxuan Ma, Yunshuang Yuan, Yingqian Zhao, and Jiaqi Ma. V2x-real: largs-scale dataset for vehicle-to-everything In Europian Conference on Comcooperative perception. puter Vision (ECCV), 2024. 1, 2, 3, 5, 6 [49] Ding Xinpeng, Han Jinahua, Xu Hang, Laing Xiaodan, Hang Xu, Zhang Wei, and Li Xiaomeng. Holistic autonomous driving understanding by birds-eye-view injected multimodal large models. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. 3 [50] Runsheng Xu, Zhengzhong Tu, Hao Xiang, Wei Shao, Bolei Zhou, and Jiaqi Ma. Cobevt: Cooperative birds eye view semantic segmentation with sparse transformers. In Conference on Robot Learning (CoRL), 2022. 1, 2, 6, [51] Runsheng Xu, Hao Xiang, Zhengzhong Tu, Xin Xia, MingHsuan Yang, and Jiaqi Ma. V2x-vit: Vehicle-to-everything cooperative perception with vision transformer. In European Conference on Computer Vision (ECCV), 2022. 1, 2, 3, 6, 7 [52] Runsheng Xu, Hao Xiang, Xin Xia, Xu Han, Jinlong Li, and Jiaqi Ma. Opv2v: An open benchmark dataset and fusion pipeline for perception with vehicle-to-vehicle communication. In IEEE International Conference on Robotics and Automation (ICRA), 2022. 1, 2, 3, 6, 7 [53] Runsheng Xu, Xin Xia, Jinlong Li, Hanzhao Li, Shuo Zhang, Zhengzhong Tu, Zonglin Meng, Hao Xiang, Xiaoyu Dong, Rui Song, Hongkai Yu, Bolei Zhou, and Jiaqi Ma. V2v4real: real-world large-scale dataset for vehicle-to-vehicle cooperative perception. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023. 1, 2, 3, 4, 5, 6 [54] Zhenhua Xu, Yujia Zhang, Enze Xie, Zhen Zhao, Yong Guo, Kwan-Yee K. Wong, Zhenguo Li, and Hengshuang Zhao. Drivegpt4: Interpretable end-to-end autonomous driving via large language model. IEEE Robotics and Automation Letters (RA-L), 2024. 2, 3 [55] Dingkang Yang, Kun Yang, Yuzheng Wang, Jing Liu, Zhi Xu, Rongbin Yin, Peng Zhai, and Lihua Zhang. How2comm: Communication-efficient and collaboration-pragmatic multiagent perception. In Advances in Neural Information Processing Systems (NeurIPS), 2023. 2 11 V2V-LLM: Vehicle-to-Vehicle Cooperative Autonomous Driving with Multi-Modal Large Language Models"
        },
        {
            "title": "Supplementary Material",
            "content": "Method No Fusion Early Fusion Intermediate Fusion AttFuse [52] V2X-ViT [51] CoBEVT [50] LLM Fusion V2V-LLM (ours) 1 input frame 3 input frames L2 (m) CR (%) L2 (m) CR (%) 6.55 6. 6.83 7.08 6.72 4.99 4.57 3.55 4.12 4.33 3.88 3.00 5.94 5. 6.46 5.52 6.02 4.82 3.77 3.04 3.50 3.84 3.40 2.93 Table 6. V2V-LLMs planning performance in V2V-QAs testing split in comparison with baseline methods. L2: L2 distance error. CR: Collision Rate. In each column, the best results are in boldface. and the second-best results are in underline. 8. Additional Planning Results In the main paper, all experiments use point clouds at single frame from each CAV as the visual input to the models. In this section, we experiment with feeding visual features from 3 consecutive frames, the current one and the previous two, as the visual input to the models. Tab. 6 shows the planning results of the new setting together with the original setting from the main paper. In general, using visual inputs from multiple frames improves planning performance. However, the performance gain for V2V-LLM is not as significant as other methods. We conjecture that it is because, even with single visual input frame, the V2V-LLM model is already capable of inferring future movement directions and max possible movement distances in the next 3 seconds using the point cloud feature maps and the locations of nearby detected objects. 9. Evaluation Metric Details For the grounding questions (Q1, Q2, Q3) and the notable object identification question (Q4), the evaluation metrics are F1 score, precision, and recall. The ground-truth answers and model outputs only contain objects center locations. If the center distance between the ground-truth answer and the model output is less than threshold value, this output is considered as true positive result. We set the threshold value to be 4 meters, typical length of vehicle. For the planning question (Q5), the evaluation metrics are L2 errors and collision rates. The ground-truth answers and model outputs only contain 6 future waypoints, so we calculate the L2 errors on those waypoints. When calcuQA type Train-Pos Train-Neg Test-Pos Test-Neg Q1 Q2 Q3 Q4 Total 217403 17859 7197 9911 252370 137417 17841 7142 2379 164779 76522 8391 3082 2517 90512 Total 44861 476203 49582 5491 19436 2015 15736 929 53296 560957 Table 7. Dataset statistics of our V2V-QAs positive and negative cases in the training and testing splits. Q1: Grounding at reference location. Q2: Grounding behind reference object at location. Q3: Grounding behind reference object in direction. Q4: Notable object identification. lating collision rates, we assume each of the CAVs bounding box sizes is 4 meters in length, 2 meters in width, and 1.5 meters in height. We place each CAVs bounding box at the models output future waypoints and calculate the Intersection-over-Union (IOU) between the CAVs bounding box and every ground-truth objects annotation bounding box in those future frames. If the IOU is larger than 0, it is considered as collision. 10. Additional Implementation Details We use 8 NVIDIA A100-80GB GPUs to train our model. We use the same training hyperparameters from LLaVAv1.5-7b [25]. We use batch size 32. Adam optimizer is adopted for training with starting learning rate 2e 5 and cosine learning rate scheduler with 3% warm-up ratio. We train our Q1 for one epoch and other QA types for 10 epochs. Our training job can finish within 4 hours. 11. Additional Dataset Statistics For the grounding questions (Q1, Q2, Q3) and the notable object identification question (Q4), QA pair can be categorized into either positive case or negative case. If at least one object exists that satisfies the condition specified in the questions, the corresponding QA pair is positive case. Otherwise, it is negative case. Tab. 7 summarizes the numbers of QA pairs in each category. This table shows that V2V-QA has sufficient positive and negative data samples in both training and testing splits for each of these QA pairs. The planning task (Q5) is excluded from Tab. 7, as each planning QA pair inherently includes ground-truth future trajectory in its corresponding answer. We also visualize the distribution of ground truth answer locations relative to the asking CAV for the grounding questions (Q1, Q2, Q3) and the notable object identification (a) (meters) (b) (meters) (a) (meters) (b) (meters) (c) distance (meters) (d) angle (degrees) (c) distance (meters) (d) angle (degrees) Figure 7. The distribution of ground-truth answer locations relative to CAV in V2V-QAs Q1: Grounding at reference location. Figure 9. The distribution of ground-truth answer locations relative to CAV in V2V-QAs Q3: Grounding behind reference object in direction. (a) (meters) (b) (meters) (a) (meters) (b) (meters) (c) distance (meters) (d) angle (degrees) Figure 8. The distribution of ground-truth answer locations relative to CAV in V2V-QAs Q2: Grounding behind reference object at location. question (Q4), as shown in Figs. 7 to 10. In our coordinate system, is the CAVs front direction, and is the CAVs right direction. For the planning question (Q5), we show the distribution of the ending waypoints in the ground truth answer future trajectories, as shown in Fig. 11. These figures indicate that our V2V-QA has diverse spatial distributions in the driving scenes. 12. Additional Qualitative Results In this section, we show more qualitative results of our proposed V2V-LLM and other baseline methods in the testing split of V2V-QAs grounding task in Figs. 12 to 15, notable object identification task in Figs. 16 to 17, and planning task in Figs 18 to 19. The baseline methods include nofusion, early-fusion, and intermediate-fusion: AttFuse [52], V2X-ViT [51], and CoBEVT [50]. In general, we can observe that our proposed V2V-LLMs outputs are closer to 2 (c) distance (meters) (d) angle (degrees) Figure 10. The distribution of ground-truth answer locations relative to CAV in V2V-QAs Q4: Notable object identification. (a) (meters) (b) (meters) (c) distance (meters) (d) angle (degrees) Figure 11. The distribution of ground-truth answer locations relative to CAV in V2V-QAs Q5: Planning. the ground-truth answers, in comparison to other baseline methods results. 13. Limitation Fig. 20 shows failure cases of V2V-LLMs planning results on V2V-QAs testing split. In few frames, the model generates future trajectories in the lane of the opposite traffic direction. potential solution and future work is to include HD map information as additional input to the model. Currently, this approach is not feasible because the base dataset V2V4Real [53] has not released its HD map to the public. 3 Figure 12. V2V-LLM and baseline methods grounding results on V2V-QAs testing split. Magenta : reference locations in questions. Yellow +: model output locations. Green : ground-truth answers. 4 Figure 13. V2V-LLM and baseline methods grounding results on V2V-QAs testing split. Magenta : reference locations in questions. Yellow +: model output locations. Green : ground-truth answers. 5 Figure 14. V2V-LLM and baseline methods grounding results on V2V-QAs testing split. Magenta : reference locations in questions. Yellow +: model output locations. Green : ground-truth answers. 6 Figure 15. V2V-LLM and baseline methods grounding results on V2V-QAs testing split. Magenta : reference locations in questions. Yellow +: model output locations. Green : ground-truth answers. 7 Figure 16. V2V-LLM and baseline methods notable object identification results on V2V-QAs testing split. Magenta curve: planned future trajectories in questions. Green : ground-truth notable object locations. Yellow +: model identification outputs. 8 Figure 17. V2V-LLM and baseline methods notable object identification results on V2V-QAs testing split. Magenta curve: planned future trajectories in questions. Green : ground-truth notable object locations. Cyan : model identification outputs. 9 Figure 18. V2V-LLM and baseline methods planning results on V2V-QAs testing split. Green curve: future trajectories in ground-truth answers. Green : ending waypoints in ground-truth answers. Yellow curve: model planning outputs. Yellow : ending waypoints in model outputs. 10 Figure 19. V2V-LLM and baseline methods planning results on V2V-QAs testing split. Green curve: future trajectories in ground-truth answers. Green : ending waypoints in ground-truth answers. Cyan curve: model planning outputs. Cyan : ending waypoints in model outputs. 11 Figure 20. Failure cases of V2V-LLMs planning results on V2V-QAs testing split. Green curve: future trajectories in ground-truth answers. Green : ending waypoints in ground-truth answers. Yellow curve and Cyan curve: model planning outputs corresponding to CAV EGO and CAV 1, respectively. Yellow and Cyan : ending waypoints in model outputs corresponding to CAV EGO and CAV 1, respectively."
        }
    ],
    "affiliations": [
        "Carnegie Mellon University",
        "NVIDIA"
    ]
}