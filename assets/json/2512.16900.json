{
    "paper_title": "FlashPortrait: 6x Faster Infinite Portrait Animation with Adaptive Latent Prediction",
    "authors": [
        "Shuyuan Tu",
        "Yueming Pan",
        "Yinming Huang",
        "Xintong Han",
        "Zhen Xing",
        "Qi Dai",
        "Kai Qiu",
        "Chong Luo",
        "Zuxuan Wu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Current diffusion-based acceleration methods for long-portrait animation struggle to ensure identity (ID) consistency. This paper presents FlashPortrait, an end-to-end video diffusion transformer capable of synthesizing ID-preserving, infinite-length videos while achieving up to 6x acceleration in inference speed. In particular, FlashPortrait begins by computing the identity-agnostic facial expression features with an off-the-shelf extractor. It then introduces a Normalized Facial Expression Block to align facial features with diffusion latents by normalizing them with their respective means and variances, thereby improving identity stability in facial modeling. During inference, FlashPortrait adopts a dynamic sliding-window scheme with weighted blending in overlapping areas, ensuring smooth transitions and ID consistency in long animations. In each context window, based on the latent variation rate at particular timesteps and the derivative magnitude ratio among diffusion layers, FlashPortrait utilizes higher-order latent derivatives at the current timestep to directly predict latents at future timesteps, thereby skipping several denoising steps and achieving 6x speed acceleration. Experiments on benchmarks show the effectiveness of FlashPortrait both qualitatively and quantitatively."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 1 ] . [ 1 0 0 9 6 1 . 2 1 5 2 : r FlashPortrait: 6 Faster Infinite Portrait Animation with Adaptive Latent Prediction Shuyuan Tu1 Yueming Pan3 Yinming Huang1 Xintong Han4 Zhen Xing5 Qi Dai2 Kai Qiu2 Chong Luo2 Zuxuan Wu1 1Fudan University 2Microsoft Research Asia 3Xian Jiaotong University 4Tencent Inc. 5Tongyi Lab, Alibaba Group https://francis-rings.github.io/FlashPortrait Figure 1. Portrait animations generated by FlashPortrait, showing its power to synthesize infinite-length ID-preserving animations. FrameX refers to the X-th frame of the synthesized video. The clock icon denotes inference time. Wan-Animate is the latest animation model."
        },
        {
            "title": "Abstract",
            "content": "Current diffusion-based acceleration methods for longportrait animation struggle to ensure identity (ID) consistency. This paper presents FlashPortrait, an end-toend video diffusion transformer capable of synthesizing IDpreserving, infinite-length videos while achieving up to 6 acceleration in inference speed. In particular, FlashPortrait begins by computing the identity-agnostic facial expression features with an off-the-shelf extractor. It then introduces Normalized Facial Expression Block to align facial features with diffusion latents by normalizing them with their respective means and variances, thereby improving identity stability in facial modeling. During inference, FlashPortrait adopts dynamic sliding-window scheme with weighted blending in overlapping areas, ensuring smooth transitions and ID consistency in long animations. In each context window, based on the latent variation rate at particular timesteps and the derivative magnitude ratio among diffusion layers, FlashPortrait utilizes higher-order latent derivatives at the current timestep to directly predict latents at future timesteps, thereby skipping several denoising steps and achieving 6 speed acceleration. Experiments on benchmarks show the effectiveness of FlashPortrait both qualitatively and quantitatively. 1. Introduction Portrait animation aims to synthesize portrait videos with natural facial movements, given reference image and driven video, with broad applications in film production and virtual assistants. Diffusion models [9, 14, 16, 17, 30, 36, 40, 41, 45, 46, 4850, 58, 59, 64] have significantly inspired research in portrait animation [4, 33, 55, 62, 67]. To further make portrait animation applicable in real-world scenarios, researchers are advancing toward long-length portrait animation, which substantially raises inference latency and thus drives the need for acceleration strategies. However, current acceleration methods fail to sustain coherent portrait animation over extended sequences. Beyond approximately 20 seconds of animation, they commonly suffer from body distortions and identity (ID) inconsistencies, thereby limiting their practical applicability. 1 To address this issue, some methods have explored quality-preserving acceleration strategies (cache-based methods [26, 27, 37, 72, 74, 75] and distillation-based methods [7, 21, 68, 69]) for Image-to-Video (I2V) generation with subtle motion, yet their effectiveness remains limited when applied to long-length portrait animation featuring complex and large-scale facial expressions. Cache-based methods use training-free save-and-reuse approach to skip denoising steps. However, simply reusing previous feature caches for future latents can drift denoising direction, especially in videos with significant motion, as accurately reconstructing future latents with dramatic motion is highly challenging. By contrast, distillation-based methods require substantial computational cost to train 4-step student model and rely on autoregressive sampling for long video synthesis. As the student network cannot fully preserve the teachers priors, small latent mismatches emerge at every generated segment. These mismatches propagate across segments and intensify over time, ultimately manifesting as noticeable distribution shifts and color instability, especially in long sequences with large and complex motions. Thus, preserving stable identity in fast, extendedlength portrait animations remains challenging. Motivated by this, we propose FlashPortrait, framework with tailored inference and training designs for fast, ID-preserving portrait animation over extended durations, as shown in Fig. 2. To ensure both high-speed acceleration and ID-preserving infinite-length animation, FlashPortrait introduces novel Sliding Window-based Adaptive Latent Prediction Acceleration Mechanism, which maintains ID consistency while achieving 6 speedup. Concretely, to improve the smoothness of the clip transition in long video generation, FlashPortrait first proposes weighted slidingwindow denoising strategy that fuses latents with progressive weights over time. In each context window, FlashPortrait introduces the Adaptive Latent Prediction Acceleration Mechanism to speed up the window-wise denoising, which leverages the differences among historical latents from previous timesteps to approximate high-order derivatives of the current latent at the present timestep, and then applies the Taylor series to directly predict latents at future timesteps, thereby skipping several denoising steps. However, due to the complex and large-amplitude facial motions in portrait animation, latent variations across timesteps are substantial, making fixed-order prediction unreliable [27, 42, 72]. To address this, based on the latent variation rate at particular timestep and the derivative magnitude ratio among diffusion layers, we compute two dynamic functions, which adaptively adjust the Taylor expansion, enabling multiple denoising step skipping while ensuring ID stability. Furthermore, we observe that even within the same clip, ID consistency across frames synthesized by previous models is unstable, primarily due to the large distance between the distribution centers of diffusion latents and facial expression features. To tackle this, FlashPortrait plugs Normalized Facial Expression Blocks into Video Diffusion Transformer, which significantly enhances ID stability across frames. In particular, FlashPortrait first utilizes an off-the-shelf extractor [54] to obtain facial expression features, which are then passed through several self-attention blocks to enhance the perception of the overall facial layout. Then, the means and variances of both the processed features and diffusion latents are computed. FlashPortrait normalizes the facial expression features using these statistics, thereby substantially reducing the adverse effects caused by the large distance between these two distribution centers (latents and facial expression), thereby improving ID stability. As shown in Fig. 1 and Table 4, while the latest opensource portrait animation model Wan-Animate [4] suffers from dramatic identity inconsistency, color drift, and timeconsuming inference, FlashPortrait accurately manipulates the reference based on the driven video while remaining identity stable, achieving 6 inference speedup compared with the baseline even when synthesizing extended sequences exceeding 1,800 frames. In conclusion, our contributions are as follows: (1) We propose novel Sliding Window-based Adaptive Latent Prediction Acceleration Mechanism. It is trainingfree and only activated during inference, achieving 6 speedup while maintaining identity consistency in infinitelength portrait animation. To our knowledge, we are the first to explore video diffusion for accelerating ID-preserving infinite-length portrait animation. (2) We propose novel Normalized Facial Expression Block to align the distribution centers of diffusion latents and facial features, thereby enhancing identity stability during denoising. (3) Experimental results on benchmark datasets show the superiority of our model over the SOTA. 2. Related Work Video Generation. The superior diversity and high fidelity in diffusion models [9, 16, 17, 30, 31, 36, 40, 41, 43, 44, 44, 47, 49, 65] has facilitated the advancement of video generation. Early video diffusion works [2, 3, 12, 39, 45, 46, 48, 63] mostly are based on the U-Net architecture for video generation by inserting additional temporal layers to pretrained image diffusion models. Recent works [1, 20, 23, 53] replace the U-Net with the Diffusionin-Transformer (DiT) [32] for scalability and higher resInspired by previous works [4, 55], we utilize olution. Wan2.1 [53] as the backbone. Portrait Animation. It aims to transfer facial motion from given video to reference image. Early works [11, 18, 19, 24, 56, 66, 70, 71] basically apply GANs [10] to model the motion dynamics. Recently, some studies have applied diffusion models to this field. FollowYE [29] uses facial key2 points, and Skyreels-A1 [33] applies 3D Morphable Models to model facial motion. FantasyPortrait [55] supports multicharacter portrait animation, and Wan-Animate [4] unifies portrait animation and character replacement. However, prior DiT-based approaches entail high inference latency and exhibit identity inconsistency and color drift when generating long videos. FlashPortrait addresses these issues and performs ID-preserving infinite-length portrait animation with 6 faster inference speed. Acceleration. Acceleration techniques for diffusion models can be broadly categorized as training-free cachebased methods [26, 37, 72, 74, 75] and training-intensive distillation-based methods [7, 21, 68, 69]. Regarding cachebased methods, FORA [37] reuses historical attention and MLP features. TeaCache [26] decides the caching latents based on the timestep difference estimation. FoCa [72] treats feature caching as an ODE solving problem. In terms of distillation-based methods, Self-Forcing [21] and SelfForcing++ [7] both require extensive GPU resources to train 4-step student model and generate long videos through an autoregressive sampling paradigm. However, the above techniques mainly work for I2V generation tasks with minor motion variations. When applied to long-length portrait animation with large facial motions, the variance of latent changes across timesteps becomes significant, causing the accumulated errors to grow rapidly over time, which eventually leads to ID inconsistency and color drift. By contrast, FlashPortrait attains 6 inference speed-up while preserving identity consistency and avoiding color drift. 3. Method Illustrated in Fig. 2, FlashPortrait builds on Wan2.1 [53] and synthesizes infinite-length ID-preserving animations at fast speed. In particular, the driven video is first fed to PD-FGC [54] to obtain raw facial embeddings (head pose, eyes, emotion, and mouth), which are subsequently refined to reduce the distribution gap between diffusion latents and facial embeddings. More details are described in Sec. 3.1. Following [53], reference image is incorporated via two pathways. First, the reference is processed by an image encoder [34] to gain image embeddings. These embeddings are injected into Facial Expression Blocks, modulating facial attributes. Second, the reference is temporally padded with zero frames and encoded by frozen 3D VAE [53] to obtain latent code. The code is then concatenated channelwise with compressed video frames and binary mask (the first frame is 1 and all subsequent frames are 0). During inference, the original video frames are replaced with random noise, while rest inputs remain unchanged. We further propose novel Sliding Window-based Adaptive Latent Prediction Acceleration Mechanism, which achieves 6 inference speedup for ID-preserving infinite-length portrait animation, as detailed in Sec. 3.2 and Sec. 3.3. 3.1. Normalized Facial Expression Block We observe that identity consistency across frames synthesized by previous models is unstable, even in the same video clip. The primary limitation lies in their facial modeling, where the large gap between the distribution centers of diffusion latents and raw facial embeddings leads to unstable facial modeling. To address this, we propose novel Normalized Facial Expression Block to replace each Image Cross-attention block in denoising DiT. Concretely, the driven video is fed to Face Encoder [54] to obtain headpose/eye/emotion and mouth embeddings embm, and headpose/eye/emotion are concatenated to gain embe. The image embeddings embimg from the CLIP Image Encoder [34], embm, and embe are fed to Facial Expression Block to modulate the synthesized identity. We further apply several self-attention SA() and FFN on embm and embe to enhance the their perception of the overall facial layout, and concatenate the outputs: embm = MLP(Concat(embm, embe), embp = Concat(FFN(SA(embm)), FFN(SA(embe)), embm e), (1) where embp refer to portrait embeddings. The latents zi then perform cross-attention CA() with embimg and embp, respectively: zimg = CA(zi, embimg), zp = CA(zi, embp), (2) µp σp = zp µimg σimg , we ensure zimg To reduce the distance between distribution centers of zimg and zp , where where µimg/p and σimg/p refer to the mean and standard deviation of zimg/p , respectively. If the above equation holds, the distribution centers of these two features are nearly identical, thereby significantly enhancing identity stability across frames. Thus, we further normalize zp and element-wise add it to zimg for facilitating identity consistency: zp = zi = zp zp µp σp + zimg . σimg + µimg, (3) 3.2. Weighted Sliding-Window Strategy To improve the smoothness of video clip transition in longlength video generation, we propose weighted slidingwindow strategy during inference, as illustrated in Fig.2(b). Compared with conventional sliding window schemes [4, 22, 67], we assign relative frame index-aware weights = {wi = i = 0, 1, 2, .., v} on overlapping areas between adjacent windows and fuse overlapping latents (overlapping length between windows) via weighted summation: zoverlapp = Ci + (1 ) Ci1 (4) 3 Figure 2. Architecture of FlashPortrait. (a) and (b) refer to the structure of the Facial Expression Block and long-length video generation pipeline. Embeddings from the Image Encoder and Face Encoder are injected to each block of DiT. To speed up sliding window computation, each window predicts future latents from cached historical states, rather than invoking DiT for denoising. Algorithm 1 Weighted Sliding-Window Strategy the latents at future timesteps, which can be formulated as 1: Input: DiT model ε(), z[0,L] 2: for in range(T, 0, 1) : 3: ,emb[0,L] , , (l < L), : denoising steps starting index = 0, ending index = + z[0,L] previous ending index eprev = : noised latents 4: 5: while do t1 = ε(z[s,e] z[s,e] 6: if = 0 and = : 7: 8: , emb[s,e] , t) = np.linspace(0, 1, num samples=v) t1 = z[s,s+v] z[s,s+v] if < L: t1 + (1 w) z[eprev v,eprev ] t1 It covers the last clip case eprev = e, = + (l v), = min(s + l, L) 9: 10: 11: 12: 13: return z[0,L] else: break 0 where zoverlapp and Ci refer to overlapping latents and overlapping areas at the i-th window. Leveraging an arithmetic weighting function based on relative frame indices introduces smooth blending effect in the transitions between adjacent windows. More details are described in Algorithm 1. and refer to the VAE-compressed total video length and window length. We set = 5 in our experiment. 3.3. Adaptive Latent Prediction Acceleration To accelerate denoising within each context window, we propose an Adaptive Latent Prediction Acceleration Mechanism that adaptively predicts future latents from historical latent differences, guided by the latent variation rate and inter-layer derivative magnitude ratio. Concretely, we first utilize Taylor expansion to predict 4 (t) = (cid:88) i=0 (i)(a) i! (t a)i + Rn+1 (5) (n+1)! (t a)n+1, ξ [t, a]. where Rn+1 = (n+1)(ξ) In our setting, () refers to denoising DiT, and is set to + k, where is timestep interval (set to 5) and {1, ..., 1}. Thus, we further derive the prediction as: (t) = (cid:88) i=0 (i)(t + k) i! (k)i + Rn+1. (6) To mitigate the extra computation and inference delay caused by differentiation, we employ finite differences to approximate derivatives, such as: (t) = (t + K) (t), 2f (t) = (t + K) (t). (7) We then demonstrate the relationship if (t) if (i)(t) using mathematical induction. In particular, we first explore their relationship for = 1: (t) = (t + K) (t), = [f (t) + Kf (t) + 2 2 (t) + ...] (t) (8) = Kf (t) + O(K 2) Kf (t) We further assume that the above formulation holds for the (i 1)-th order difference (i1f (t) Ki1f (i1)(t)). Thus, for the i-th order difference, we have if (t) = (i1f (t)) = i1f (t + K) i1f (t). we further simplify i1f (t + K) as: i1f (t + K) = i1f (t) + i1 (t) dt + O(K 2), i1f (i1)(t) + i1 (i)(t) = i1f (i1)(t) + (i)(t) (9) Thus, we substitute Eq. 9 into if (t) to obtain: if (t) i1f (i1)(t) + (i)(t) i1f (i1)(t), = if (i)(t) (10) Therefore, if (t) if (i)(t) has been fully verified, and we substitute it into Eq. 6 to obtain the converted prediction: (t) = (cid:88) i=0 if (t + k) i!K (k)i + Rn+1. (11) w() addresses prediction errors caused by large derivativemagnitude gaps across layers at the same timestep. For lower layers that capture texture and edges and are more noise-sensitive (f (i)(t, l) ), high-order derivatives fluctuate sharply, causing the finite-difference approximation to be underestimated. Thus, when r() > 1, the scaling factor is reduced to avoid excessive amplification. For higher layers that model stable global structures (f (i)(t, l) ), derivatives vary smoothly and the approximation is overestimated. When r() < 1, the scaling factor is increased to counteract the insufficient finite-difference magnitude. These two dynamic functions can significantly tackle the fluctuations in latent distribution across different timesteps due to complex facial motion patterns. We further refine if (t) if (i)(t) using Eq. 12 and Eq. 13: if (t, l) w(t, l, i) s(t) (i)(t, l), (14) The DiT only needs to fully denoise the latents at {t + (n + 1)K, ..., + 2K, + K} timesteps. Therefore, we substitute Eq. 14 into Eq. 11 to obtain the final prediction formulation as follows: However, as we observe that portrait animation contains intricate and large-amplitude facial motion patterns, it results in dramatic fluctuations in latent distribution across different timesteps. Thus, fixed-formatting prediction methods suffer from ID inconsistency due to their inaccurate predicted latents. To address this issue, we design two dynamic functions that adaptively refine predictions, ensuring ID stability and efficient acceleration. In particular, we first calculate the latent variation at specific timestep (σ(t) = df (t) ) and the average latent variation across dt (cid:82) timesteps (σavg(T σ(t)dt). is the total timestep number. We then define the first dynamic function based on the latent variation rate at each timestep as follows: ) = 1 s(t) = ( σ(t) σavg(t) )α, (12) where α [0.5, 1.5] (set to 1.5 in our experiments). At early timesteps, the latents vary rapidly (if (t) ), necessitating larger to compensate for the pronounced variations. As the diffusion process enters later timesteps, latent updates become more gradual (if (t) ), so is reduced to prevent excessive amplification of if (t). We then define the second function based on the derivative magnitude among diffusion layers: r(t, l, i) = E[f (i)(t, l)] E[f (i)(t, avg)] , w(t, l, i) = 1 (cid:112)r(t, l, i) , (13) where l, (i)(t, l), and E[f (i)(t, avg)] refer to the DiT the l-th DiT the i-th order derivative at layer index, layer, and the average derivative magnitude across all layers. By dynamically adjusting the mapping between finite differences and high-order derivatives across layers, 5 (t, l) = (t + k, l) + (cid:88) i=1 if (t + k, l) (k)i i! w(t + k, l, i) s(t + k) , (15) Notably, we omit Rn+1 for brevity. 3.4. Training FlashPortrait is trained using reconstruction loss, with only the attention modules of the DiT being trainable. To improve face-region fidelity, we apply face and lip masks Mf ace and Mlip, which are obtained from the input video frames via MediaPipe [28]: = Eθ((zgt zε) (1 + Mf ace + Mlip)2) (16) where zgt and zε refer to diffusion latents and denoised latents. This loss facilitates more targeted and effective learning on different facial components. 4. Experiments 4.1. Implementation Details Our training dataset is comprised of three components: Hallo3 [6], Celebv-HQ [73], and collection of internetsourced videos, amounting to total of 2000 hours. We first follow prior work [67] and test FlashPortrait on the Voxceleb2 [5] and Vfhq [61] datasets. As previous works do not open-source their testing datasets, we randomly select 100 videos (5-20 seconds long) from both Voxceleb2 and Vfhq. In addition, we conduct robustness assessments by testing our model on 100 unseen videos (1-3 minutes in length, FPS=30) from the internet, labeled as the Hard100. Our DiT model leverages pre-trained weights from Wan2.1I2V-14B [53]. The training process spans 20 epochs, utilizing 200 NVIDIA H100 80GB GPUs with batch size of 1 per GPU. We set learning rate= 1e 5, = 5, and = 3. In the table elements / b, and refer to the result on the Table 1. Quantitative comparisons on Voxceleb2&Vfhq and Hard100. Voxceleb2&Vfhq and Hard100, respectively. The average video duration of Voxceleb2&Vfhq is 10 seconds, while Hard100 is 1 minute. LMD/APD multiplied by 103 and AED multiplied by 102. Speed refers to the inference latency for 20-second 480832 video. Model FID FVD PSNR SSIM LMD AED APD MAE Speed LivePortrait [11] Skyreels-A1 [33] FollowYE [29] X-Portrait [62] HunyuanPortrait [67] FantasyPortrait [55] Wan-Animate [4] 83.21/143.72 70.48/168.24 76.45/221.36 85.13/164.68 73.62/157.93 65.27/165.71 65.20/143.61 492.46/584.13 386.24/758.25 425.18/845.69 413.53/814.46 366.72/882.54 328.93/723.57 336.12/695.48 31.53/20.48 31.04/15.42 30.65/21.26 30.82/18.38 31.93/16.63 32.48/16.47 32.54/18.13 0.74/0.72 0.76/0.68 0.69/0.73 0.73/0.74 0.78/0.69 0.80/0.71 0.82/0. 8.75/10.02 5.34/10.45 10.28/9.47 9.47/8.82 6.02/8.64 5.24/8.49 5.17/7.98 28.64/50.43 21.26/46.11 23.69/58.62 22.81/52.94 20.75/49.95 19.66/45.34 19.54/42.98 27.57/43.68 20.43/48.35 21.32/38.12 20.98/39.53 20.14/41.08 19.36/36.67 19.15/35.06 10.22/20.17 9.17/21.23 14.81/23.85 9.49/21.04 8.85/20.48 7.64/19.87 7.88/20.08 108s 504s 648s 1636s 1602s 4339s 2298s Ours 65.18/62.33 320.47/340.21 32.36/26.16 0.79/0.82 4.90/5.26 15.19/29. 14.46/24.40 5.93/12.54 720s 4.2. Comparison with State-of-the-Art Methods Quantitative results. Following [55, 67], in the selfreenactment, we utilize FID [15], FVD [51], PSNR, and SSIM [57] to assess the quality of synthesized images and videos. We leverage LMD [28] and MAE [13] to evaluate the expression motion accuracy and eye movement accuracy. In the cross-reenactment, we use AED [38] and APD [38] to assess the accuracy of expression and head movement. We compare with recent portrait animation models, including GAN-based models (LivePortrait [11]) and diffusion-based models (UNet-based: FollowYE [29], X-Portrait [62]; DiT-based: Skyreels-A1 [33], HunyuanPortrait [67], FantasyPortrait [55], Wan-Animater [4]). We perform quantitative comparisons with the above competitors on Voxceleb2 [5] & Vfhq [61] and Hard100, as shown in Table 1. We observe that although all competitors experience notable performance drop in long video generation, FlashPortrait still outperforms them in expression/eye-motion accuracy, video fidelity, and singleframe quality, while maintaining relatively high quality for short video generation. Furthermore, FlashPortrait achieves the fastest inference speed among all DiT-based methods [4, 33, 55, 67] and delivers the best performance on both test sets. Specifically, Wan2.1-14B-based FlashPortrait surpasses the best competitor Wan2.2-14B-based WanAnimate by 30.9%/30.4%/37.5% in AED/APD/MAE on Hard100, while achieving 3 faster inference speed. Qualitative Results. The qualitative results are shown in Fig. 3. Notably, each driven video is filled with intricate expression patterns, while the references include intricate details of appearances. As FantasyPortrait [55] doesnt contain long video generation mechanisms, we adopt the sliding-window strategy from Wan-Animate [4] to it. In particular, LivePortrait [11] and Skyreels-A1 [33] preserve identity, yet struggle to faithfully transfer reference facial expressions from the driving video, particularly in capturing eye dynamics and mouth movements. X-Portrait [62] suffers from significant face distortion. As Wan-Animate relies on explicit poses, body misalignment between the reference and driven videos greatly degrades performance, especially Table 2. Ablation study on Normalized Facial Expression Blocks. zi in the Baseline, Pure Norm, and Centralization refer to zp + zp µp zimg σp µimg σimg , respectively. zp µp σp + zimg + zimg , and , Model AED APD MAE Baseline Pure Norm Centralization 44.78 38.42 33. 36.87 32.64 27.31 19.73 17.25 14.66 Ours 29.68 24.40 12. when its default alignment fails. HunyuanPortrait [67], FantasyPortrait [55], and Wan-Animate [4] exhibit severe color drift, face/body distortion, and stochastic facial expression changes that deviate from the driving video guidance once the video length surpasses 30 seconds, along with substantial inference latency. In contrast, our FlashPortrait accurately animates images based on the given video while preserving reference identities even after generating 3000+ frames, highlighting the superiority of our model in generating vivid, infinite-length videos. It further achieves the most favorable trade-off between inference speed and visual quality among all DiT-based competitors [4, 33, 55, 67]. 4.3. Ablation Study Normalization. We conduct an ablation study to demonstrate the contributions of Normalized Facial Expression Blocks in FlashPortrait, as shown in Table 2 and Fig. 4(a). Notably, all quantitative ablation studies are on the Hard100 dataset. We can see that the Pure Norm and Centralization fail to fully maintain the facial details and identity consistency, as they do not fundamentally narrow the distribution gap between the latent space and facial embeddings. By contrast, our Normalized Facial Expression Blocks can ensure high facial quality by integrating the mean and the standard deviation from both cross-attention features, significantly reducing the distance between distribution centers of latents and raw facial embeddings. Long Video. We conduct an ablation study on expression/eye motion accuracy in the long portrait animation, as shown in Fig. 4(b) and Table 3. Compared with the motion frame [6] and conventional sliding window [4], our 6 Figure 3. Qualitative comparisons with state-of-the-art methods. [Total=X] refers to the total frame number of the video. Figure 4. Ablation study on normalization and long video. Table 3. Ablation study on long portrait animation methods. Model AED APD MAE Motion Frame [6] Sliding Window [4] 37.25 36.44 30.71 28. 17.67 14.90 Ours 29.68 24.40 12.54 Weighted Sliding-Window Strategy dynamically fuses adjacent context windows through weighted aggregation in their overlapping regions, enabling seamless transitions between sub-clips, thereby significantly improving the synthesized long animation quality. Acceleration. To validate the significance of our Adaptive Latent Prediction Acceleration, we conduct an ablaFigure 5. Ablation study on acceleration. Table 4. Ablation study on acceleration methods. The baseline and w/o Dynamic Functions remove our acceleration strategy in FlashPortrait and w(l, i) s(t) during latent prediction. Speed refers to the inference latency for 20-second 480832 video. Model AED APD MAE Speed Baseline TeaCache [26] w/o Dynamic Functions FoCa [72] Self-Forcing [21] 29.12 33.94 42.66 37.47 52. 23.86 27.62 35.98 32.96 39.32 12.37 15.06 19.63 17.88 20.79 4328s 2164s 682s 862s 266s Ours 29.68 24. 12.54 720s tion study on various types of acceleration methods [21, 26, 27, 72], as shown in Table 4 and Fig. 5. w/o DF 7 Table 5. Ablation study on and in our acceleration. Model AED APD MAE Speed n=1 (K=5) n=2 (K=5) n=4 (K=5) K=2 (n=3) K=8 (n=3) 34.63 32.25 29.60 29.20 44. 28.78 26.54 24.18 23.92 36.68 15.80 13.97 12.48 12.41 19.47 483s 569s 1025s 2116s 295s n=3, K=5 29.68 24.40 12.54 720s refers to w/o Dynamic Functions. We have the following observations: (1) While TeaCache [26] enables acceleration with minimal performance loss compared to the baseline, its speed-up is limited to at most 2. (2) w/o Dynamic Functions encounters significant performance deterioration. The plausible reason is that portrait animation involves complex, large-amplitude facial motions, which cause substantial fluctuations in the latent distribution across timesteps. Thus, fixed-pattern prediction approaches encounter identity inconsistency from inaccurate latent predictions. (3) Although FoCa [72] and Self-Forcing [21] achieve extremely high inference acceleration ratios (520), their synthesized videos often suffer from severe artifacts and identity inconsistency, especially when the driven video exhibits large-amplitude facial expressions and eye movements. The fundamental reason is that such dramatic expression dynamics in portrait animation make FoCas calibration unable to capture latent variations across timesteps accurately. Meanwhile, Self-Forcing relies on an autoregressive DMDbased [68, 69] 4-step student model, where the limited 4step inference heavily overlooks complex expression motion modeling. Thus, Self-Forcing is generally suitable only for relatively static videos. (4) Our acceleration strategy outperforms existing alternatives by achieving 6 speed-up with negligible performance loss. Even under large expression and eye movements, it synthesizes highquality videos without noticeable artifacts and preserves strong identity consistency. We further conduct an ablation study on and in our Adaptive Latent Prediction Acceleration, as shown in Table 5. We can observe that larger yields higher acceleration but causes stronger degradation, especially when > 5. Increasing enhances latent prediction accuracy but reduces speed, and the benefit becomes marginal once > 3. Consequently, = 5 and = 3 achieve the best quality-speed trade-off. More ablation studies are depicted in the Sec.D of the Supp. Figure 6. Cartoon portrait animation results. Table 6. User preference of FlashPortrait compared with other competitors. Higher indicates users prefer more to our model. Model L-A A-A B-A I-A LivePortrait [11] HunyuanPortrait [67] FantasyPortrait [55] Wan-Animate [4] 95.4% 97.2% 98.5% 97.9% 94.8% 96.4% 98.2% 97.6% 95.2% 95.8% 97.7% 96.8% 92.8% 93.7% 97.4% 96.5% the presence of large-scale expression motions. Cartoon Portraits. To validate the robustness of our FlashPortrait, we experiment on cartoon portrait animation, as shown in Fig. 6. We can observe that our model can synthesize natural cartoon portrait animation with rich facial expressions based on the driven video. Video Length. To validate the performance of FlashPortrait in long portrait animation, we experiment on an extremely long case (3 minutes+, FPS=30), as shown in Sec. of the Supp. It indicates that the video fidelity, expression/eye motion synchronization, and identity consistency remain stable without significant degradation, even after generating 5400+ frames. Theoretically, FlashPortrait is capable of generating hours of video at high speed while maintaining stable quality without significant degradation. User Study. To assess perceptual quality in subjective way, we conducted user study involving 30 curated samples. The participants, primarily university students and faculty, are first shown the reference image and its driven video. They then view two synthesized resultsone from FlashPortrait and another from competing methodpresented in random order. Participants are then asked to answer the following questions: L-A/AA/B-A/I-A: Which one has better facial expression motion/foreground appearance/background/identity alignment with the driven video/reference. The results in Table 6 demonstrate that FlashPortrait is consistently preferred in all evaluated aspects. 4.4. Applications and User Study 5. Conclusion Full Body Portrait Animations. We conduct qualitative experiment on our FlashPortrait in full-body portrait animation. The results are shown in Sec. of the Supp. We can see that our FlashPortrait can handle full-body portrait animation in high-fidelity while preserving identities even in In this paper, we propose FlashPortrait, equipped with specialized training and inference mechanisms that enable ID-preserving portrait animation, while infinite-length, achieving up to 6 acceleration in inference speed. FlashPortrait first utilized an off-the-shelf model to obtain 8 identity-agnostic facial expression features. To improve ID stability, FlashPortrait introduced Normalized Facial Expression Block to refine expression features. In inference, to ensure the long videos smoothness and ID consistency, FlashPortrait proposed Weighted SlidingIn each context window, FlashPortrait Window Strategy. further introduced an Adaptive Latent Prediction Acceleration Mechanism to skip several denoising steps, thereby achieving 6 speed acceleration. Experimental results across various datasets demonstrated the superiority of in synthesizing infinite-length ID-preserving our model speed. portrait animations with significantly faster"
        },
        {
            "title": "References",
            "content": "[1] Fan Bao, Chendong Xiang, Gang Yue, Guande He, Hongzhou Zhu, Kaiwen Zheng, Min Zhao, Shilong Liu, Yaole Wang, and Jun Zhu. Vidu: highly consistent, dynamic and skilled text-to-video generator with diffusion models. arXiv preprint arXiv:2405.04233, 2024. 2 [2] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. 2 [3] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video generation models as world simulators. 2024. 2 [4] Gang Cheng, Xin Gao, Li Hu, Siqi Hu, Mingyang Huang, Chaonan Ji, Ju Li, Dechao Meng, Jinwei Qi, Penchong Qiao, et al. Wan-animate: Unified character animation and replacement with holistic replication. arXiv preprint arXiv:2509.14055, 2025. 1, 2, 3, 6, 7, 8 [5] Joon Son Chung, Arsha Nagrani, and Andrew Zisserman. arXiv preprint Voxceleb2: Deep speaker recognition. arXiv:1806.05622, 2018. 5, 6, [6] Jiahao Cui, Hui Li, Yun Zhan, Hanlin Shang, Kaihui Cheng, Yuqi Ma, Shan Mu, Hang Zhou, Jingdong Wang, and Siyu Zhu. Hallo3: Highly dynamic and realistic portrait image animation with video diffusion transformer. In CVPR, 2025. 5, 6, 7, 1 [7] Justin Cui, Jie Wu, Ming Li, Tao Yang, Xiaojie Li, Rui Wang, Andrew Bai, Yuanhao Ban, and Cho-Jui Hsieh. Selfforcing++: Towards minute-scale high-quality video generation. arXiv preprint arXiv:2510.02283, 2025. 2, 3 [8] Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou. Arcface: Additive angular margin loss for deep face recognition. In CVPR, 2019. 1 [9] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. In NeurIPS, 2021. 1, 2 [10] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. Communications of the ACM, 2020. 2 [11] Jianzhu Guo, Dingyun Zhang, Xiaoqiang Liu, Zhizhou Zhong, Yuan Zhang, Pengfei Wan, and Di Zhang. Liveportrait: Efficient portrait animation with stitching and retargeting control. arXiv preprint arXiv:2407.03168, 2024. 2, 6, [12] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized text-toIn ICLR, image diffusion models without specific tuning. 2024. 2 [13] Yue Han, Junwei Zhu, Keke He, Xu Chen, Yanhao Ge, Wei Li, Xiangtai Li, Jiangning Zhang, Chengjie Wang, and Yong Liu. Face-adapter for pre-trained diffusion models with finegrained id and attribute control. In ECCV, 2024. 6 [14] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt imarXiv preprint age editing with cross attention control. arXiv:2208.01626, 2022. 1 [15] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. NIPS, 2017. 6 [16] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In NeurIPS, 2020. 1, [17] Jonathan Ho, Chitwan Saharia, William Chan, David Fleet, Mohammad Norouzi, and Tim Salimans. Cascaded diffusion models for high fidelity image generation. JMLR, 2022. 1, 2 [18] Fa-Ting Hong and Dan Xu. Implicit identity representation conditioned memory compensation network for talking head video generation. In ICCV, 2023. 2 [19] Fa-Ting Hong, Longhao Zhang, Li Shen, and Dan Xu. Depth-aware generative adversarial network for talking head video generation. In CVPR, 2022. 2 [20] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. Cogvideo: Large-scale pretraining for text-to-video generation via transformers. arXiv preprint arXiv:2205.15868, 2022. 2 [21] Xun Huang, Zhengqi Li, Guande He, Mingyuan Zhou, and Eli Shechtman. Self forcing: Bridging the traintest gap in autoregressive video diffusion. arXiv preprint arXiv:2506.08009, 2025. 2, 3, 7, 8 [22] Xiaozhong Ji, Xiaobin Hu, Zhihong Xu, Junwei Zhu, Chuming Lin, Qingdong He, Jiangning Zhang, Donghao Luo, Yi Chen, Qin Lin, et al. Sonic: Shifting focus to global audio perception in portrait animation. In CVPR, 2025. 3 [23] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. [24] Hongxiang Li, Meng Cao, Xuxin Cheng, Yaowei Li, Zhihong Zhu, and Yuexian Zou. G2l: Semantically aligned and uniform video grounding via geodesic and game theory. In ICCV, 2023. 2 [25] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. 1 9 [26] Feng Liu, Shiwei Zhang, Xiaofeng Wang, Yujie Wei, Haonan Qiu, Yuzhong Zhao, Yingya Zhang, Qixiang Ye, and Fang Wan. Timestep embedding tells: Its time to cache for video diffusion model. In CVPR, 2025. 2, 3, 7, 8 [27] Jiacheng Liu, Chang Zou, Yuanhuiyi Lyu, Junjie Chen, and Linfeng Zhang. From reusing to forecasting: Accelerating diffusion models with taylorseers. arXiv preprint arXiv:2503.06923, 2025. 2, 7 [28] Camillo Lugaresi, Jiuqiang Tang, Hadon Nash, Chris McClanahan, Esha Uboweja, Michael Hays, Fan Zhang, ChuoLing Chang, Ming Guang Yong, Juhyun Lee, et al. Mediapipe: framework for building perception pipelines. arXiv preprint arXiv:1906.08172, 2019. 5, [29] Yue Ma, Hongyu Liu, Hongfa Wang, Heng Pan, Yingqing He, Junkun Yuan, Ailing Zeng, Chengfei Cai, Heung-Yeung Shum, Wei Liu, et al. Follow-your-emoji: Fine-controllable and expressive freestyle portrait animation. In SIGGRAPH Asia 2024 Conference Papers, pages 112, 2024. 2, 6 [30] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided image synthesis and editing with stochastic differential equations. In ICLR, 2021. 1, 2 [31] Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. In ICML, 2021. 2 [32] William Peebles and Saining Xie. Scalable diffusion models with transformers. In ICCV, 2023. 2 [33] Di Qiu, Zhengcong Fei, Rui Wang, Jialin Bai, Changqian Yu, Mingyuan Fan, Guibin Chen, and Xiang Wen. Skyreels-a1: Expressive portrait animation in video diffusion transformers. arXiv preprint arXiv:2502.10841, 2025. 1, 3, 6 [34] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, 2021. 3 [35] George Retsinas, Panagiotis Filntisis, Radek Danecek, Victoria Abrevaya, Anastasios Roussos, Timo Bolkart, and Petros Maragos. 3d facial expressions through analysis-byneural-synthesis. In CVPR, 2024. [36] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, 2022. 1, 2 [37] Pratheba Selvaraju, Tianyu Ding, Tianyi Chen, Ilya Zharkov, and Luming Liang. Fora: Fast-forward caching in diffusion transformer acceleration. arXiv preprint arXiv:2407.01425, 2024. 2, 3 [38] Aliaksandr Siarohin, Stephane Lathuili`ere, Sergey Tulyakov, Elisa Ricci, and Nicu Sebe. First order motion model for image animation. In NIPS, 2019. 6 [39] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without text-video data. arXiv preprint arXiv:2209.14792, 2022. 2 [40] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In ICLR, 2021. 1, 2 [41] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In ICLR, 2021. 1, [42] Brook Taylor. Methodus incrementorum directa & inversa. Inny, 1717. 2 [43] Shuyuan Tu, Tianzhen Guan, and Li Kuang. Multiple biological granularities network for person re-identification. In ICMR, 2022. 2 [44] Shuyuan Tu, Qi Dai, Zuxuan Wu, Zhi-Qi Cheng, Han Hu, and Yu-Gang Jiang. Implicit temporal modeling with learnable alignment for video recognition. In ICCV, 2023. 2 [45] Shuyuan Tu, Qi Dai, Zhi-Qi Cheng, Han Hu, Xintong Han, Zuxuan Wu, and Yu-Gang Jiang. Motioneditor: Editing video motion via content-aware diffusion. In CVPR, 2024. 1, 2 [46] Shuyuan Tu, Qi Dai, Zihao Zhang, Sicheng Xie, Zhi-Qi Cheng, Chong Luo, Xintong Han, Zuxuan Wu, and Yu-Gang Jiang. Motionfollower: Editing video motion via lightweight score-guided diffusion. arXiv preprint arXiv:2405.20325, 2024. 1, 2 [47] Shuyuan Tu, Yueming Pan, Yinming Huang, Xintong Han, Zhen Xing, Qi Dai, Chong Luo, Zuxuan Wu, and Yu-Gang Jiang. Stableavatar: Infinite-length audio-driven avatar video generation. arXiv preprint arXiv:2508.08248, 2025. 2 [48] Shuyuan Tu, Zhen Xing, Xintong Han, Zhi-Qi Cheng, Qi Dai, Chong Luo, and Zuxuan Wu. Stableanimator: HighIn quality identity-preserving human image animation. CVPR, 2025. 1, [49] Shuyuan Tu, Zhen Xing, Xintong Han, Zhi-Qi Cheng, Qi Dai, Chong Luo, Zuxuan Wu, and Yu-Gang Jiang. Stableanimator++: Overcoming pose misalignment and face arXiv preprint distortion for human image animation. arXiv:2507.15064, 2025. 2 [50] Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali Plug-and-play diffusion features for text-driven Dekel. image-to-image translation. In CVPR, 2023. 1 [51] Thomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. Towards accurate generative models of video: new metric & challenges. arXiv preprint arXiv:1812.01717, 2018. 6 [52] Thomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. Fvd: new metric for video generation. 2019. 1 [53] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, Jianyuan Zeng, Jiayu Wang, Jingfeng Zhang, Jingren Zhou, Jinkai Wang, Jixuan Chen, Kai Zhu, Kang Zhao, Keyu Yan, Lianghua Huang, Mengyang Feng, Ningyi Zhang, Pandeng Li, Pingyu Wu, Ruihang Chu, Ruili Feng, Shiwei Zhang, Siyang Sun, Tao Fang, Tianxing Wang, Tianyi Gui, Tingyu Weng, Tong Shen, Wei Lin, Wei Wang, Wei Wang, Wenmeng Zhou, Wente Wang, Wenting Shen, Wenyuan Yu, Xianzhong Shi, Xiaoming Huang, Xin Xu, Yan Kou, Yangyu Lv, Yifei Li, Yijing Liu, Yiming Wang, Yingya Zhang, Yitong Huang, Yong Li, You Wu, Yu Liu, Yulin Pan, Yun Zheng, Yuntao Hong, Yupeng Shi, Yutong Feng, Zeyinzi [67] Zunnan Xu, Zhentao Yu, Zixiang Zhou, Jun Zhou, Xiaoyu Jin, Fa-Ting Hong, Xiaozhong Ji, Junwei Zhu, Chengfei Cai, Shiyu Tang, et al. Hunyuanportrait: Implicit condition control for enhanced portrait animation. In CVPR, 2025. 1, 3, 5, 6, 8 [68] Tianwei Yin, Michael Gharbi, Taesung Park, Richard Zhang, Eli Shechtman, Fredo Durand, and Bill Freeman. Improved distribution matching distillation for fast image synthesis. Advances in neural information processing systems, 37:4745547487, 2024. 2, 3, 8 [69] Tianwei Yin, Michael Gharbi, Richard Zhang, Eli Shechtman, Fredo Durand, William Freeman, and Taesung Park. One-step diffusion with distribution matching distillation. In CVPR, 2024. 2, 3, 8 [70] Bowen Zhang, Chenyang Qi, Pan Zhang, Bo Zhang, HsiangTao Wu, Dong Chen, Qifeng Chen, Yong Wang, and Fang Wen. Metaportrait: Identity-preserving talking head generation with fast personalized adaptation. In CVPR, 2023. 2 [71] Jian Zhao and Hui Zhang. Thin-plate spline motion model for image animation. In CVPR, 2022. 2 [72] Shikang Zheng, Liang Feng, Xinyu Wang, Qinming Zhou, Peiliang Cai, Chang Zou, Jiacheng Liu, Yuqi Lin, Junjie Chen, Yue Ma, et al. Forecast then calibrate: Feature caching as ode for efficient diffusion transformers. arXiv preprint arXiv:2508.16211, 2025. 2, 3, 7, [73] Hao Zhu, Wayne Wu, Wentao Zhu, Liming Jiang, Siwei Tang, Li Zhang, Ziwei Liu, and Chen Change Loy. CelebVHQ: large-scale video facial attributes dataset. In ECCV, 2022. 5, 1 [74] Chang Zou, Xuyang Liu, Ting Liu, Siteng Huang, and Linfeng Zhang. Accelerating diffusion transformers with tokenarXiv preprint arXiv:2410.05317, wise feature caching. 2024. 2, 3 [75] Chang Zou, Evelyn Zhang, Runlin Guo, Haohang Xu, Conghui He, Xuming Hu, and Linfeng Zhang. Accelerating diffusion transformers with dual feature caching. arXiv preprint arXiv:2412.18911, 2024. 2, 3 Jiang, Zhen Han, Zhi-Fan Wu, and Ziyu Liu. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. 2, 3, 5 [54] Duomin Wang, Yu Deng, Zixin Yin, Heung-Yeung Shum, and Baoyuan Wang. Progressive disentangled representation learning for fine-grained controllable talking head synthesis. In CVPR, 2023. 2, 3 [55] Qiang Wang, Mengchao Wang, Fan Jiang, Yaqi Fan, Yonggang Qi, and Mu Xu. Fantasyportrait: Enhancing multicharacter portrait animation with expression-augmented diffusion transformers. arXiv preprint arXiv:2507.12956, 2025. 1, 2, 3, 6, [56] Suzhen Wang, Lincheng Li, Yu Ding, and Xin Yu. Oneshot talking face generation from single-speaker audio-visual correlation learning. In AAAI, 2022. 2 [57] Zhou Wang, Alan Bovik, Hamid Sheikh, and Eero Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing, 13(4):600612, 2004. 6 [58] Wenming Weng, Ruoyu Feng, Yanhui Wang, Qi Dai, Chunyu Wang, Dacheng Yin, Zhiyuan Zhao, Kai Qiu, Jianmin Bao, Yuhui Yuan, et al. Art-v: Auto-regressive text-tovideo generation with diffusion models. In CVPRW, 2024. 1 [59] Zejia Weng, Xitong Yang, Zhen Xing, Zuxuan Wu, and Genrec: Unifying video generation arXiv preprint Yu-Gang Jiang. and recognition with diffusion models. arXiv:2408.15241, 2024. 1 [60] Haoning Wu, Zicheng Zhang, Weixia Zhang, Chaofeng Chen, Liang Liao, Chunyi Li, Yixuan Gao, Annan Wang, Erli Zhang, Wenxiu Sun, et al. Q-align: Teaching lmms for visual scoring via discrete text-defined levels. arXiv preprint arXiv:2312.17090, 2023. [61] Liangbin Xie, Xintao Wang, Honglun Zhang, Chao Dong, and Ying Shan. Vfhq: high-quality dataset and benchmark for video face super-resolution. In CVPR, 2022. 5, 6, 1 [62] You Xie, Hongyi Xu, Guoxian Song, Chao Wang, Yichun Shi, and Linjie Luo. X-portrait: Expressive portrait animation with hierarchical motion attention. In ACM SIGGRAPH 2024 Conference Papers, 2024. 1, 6 [63] Zhen Xing, Qi Dai, Han Hu, Zuxuan Wu, and Yu-Gang Jiang. Simda: Simple diffusion adapter for efficient video In Proceedings of the IEEE/CVF Conference generation. on Computer Vision and Pattern Recognition, pages 7827 7839, 2024. 2 [64] Zhen Xing, Qi Dai, Zejia Weng, Zuxuan Wu, and YuGang Jiang. Aid: Adapting image2video diffusion models for instruction-guided video prediction. arXiv preprint arXiv:2406.06465, 2024. 1 [65] Zhen Xing, Qijun Feng, Haoran Chen, Qi Dai, Han Hu, Hang Xu, Zuxuan Wu, and Yu-Gang Jiang. survey on video diffusion models. ACM Computing Surveys, 57(2):142, 2024. 2 [66] Sicheng Xu, Guojun Chen, Yu-Xiao Guo, Jiaolong Yang, Chong Li, Zhenyu Zang, Yizhong Zhang, Xin Tong, and Baining Guo. Vasa-1: Lifelike audio-driven talking faces generated in real time. Advances in Neural Information Processing Systems, 37:660684, 2024. 2 A. Supplementary Material A.1. Evaluation Metrics Following previous portrait animation evaluation settings, we implement numerous quantitative evaluation metrics, including FID, FVD, LMD, AED, APD, and MAE, to compare our FlashPortrait with current state-of-the-art portrait animation models. The details of the above metrics are described as follows: (1) FID refers to measure the similarity in feature distribution between synthesized and real images, employing Inception v3 features. (2) FVD refers to evaluate temporal coherence through feaFigure 7. Examples from Hard100. tures extracted from pretrained model [52]. Table 7. Ablation study on different weight assignment. (3) LMD refers to measure the accuracy of synthesized facial expressions. The landmarks are extracted using Mediapipe. It computes the average Euclidean distance between the facial landmarks of the reference and synthesized images. (4) AED refers to calculate the Manhattan distance of expression from SMIRK [35], with lower values indicating better expression. (5) APD calculate the Manhattan distance of pose parameters from SMIRK [35], with lower values indicating better pose similarity. (6) MAE refers to measure the Mean Angular Error on the eye movement accuracy. A.2. Preliminaries Diffusion models function through stochastic process, consisting of two main phases: forward diffusion step and reverse denoising step for controlled noise addition and removal. In the forward process, Gaussian noise is gradually introduced to data sample x0 pdata, where pdata represents the underlying data distribution. This is done as follows, based on the Rectified Flow method [25]: xt = (1 t)x0 + tx1, (17) where [0, 1] denotes the timestep. After diffusion steps, the original data sample x0 is transformed into pure Gaussian noise x1 (0, I). In the reverse denoising process, the diffusion model εθ(xt, t) is trained to predict the velocity (x1 x0) conditioned on the noisy latents xt and the timestep t. To train the model, the Mean Squared Error (MSE) loss is applied: = Ex0,ε,t(εθ(xt, t) (x1 x0)2). (18) This framework ensures accurate denoising, gradually recovering the original data from noisy latents. A.3. Implementation and Dataset Details We train the model using AdamW (β1 = 0.9, β2 = 0.999) and run the entire optimization in bfloat16. Distributed data Model AED APD MAE Speed w/o s(t) w/o w(t, l, i) 34.73 40.48 28.44 33.52 16.12 18.75 Ours 29.68 24.40 12.54 709s 688s 720s parallelism is handled through DeepSpeed-Stage-3, which manages gradient synchronization and memory efficiency during training. In terms of the training dataset, our training dataset consists of three parts, including Hallo3 [6], Celebv-HQ [73], and collected videos from the internet (BilBil, YouTube, and TikTok). We utilize the Q-Align [60] to filter for higherquality videos by assessing the overall video fidelity. We also apply InsightFace [8] to filter out videos with facial confidence score below 0.8. We obtain the final training dataset, containing roughly 2000 hours of videos. Regarding the testing dataset, we first randomly select 100 videos (5-20 seconds long) from Voxceleb2 [5] and Vfhq [61] to construct the first simple testing dataset. To validate the robustness of our FlashPortrait, we further select 100 unseen videos (1-3 minutes long, FPS=30) from the internet to construct the testing dataset Hard100. Some examples are shown in Fig. 7. The sources of Hard100 come from various social media platforms, such as BilBil, YouTube, and TikTok. The selected videos span both indoor and outdoor environments, and the protagonists exhibit substantial demographic diversity, including balanced distributions across gender and ethnicity. The videos contain both upper-body and full-body subjects, with actions ranging from simple standing poses to complex interactions with objects in the scene. Consequently, our curated testing dataset is substantially more challenging than existing open-source testing datasets (Voxceleb2 [5] and Vfhq [61]) in terms of subject diversity, environmental diversity, and pose variability. Moreover, the average duration of our selected videos is approximately two minutes, which is significantly longer than that of existing open-source testing datasets. Thus, it is more suitable for evaluating long-video generation performance. 1 A.4. Additional Ablation on Acceleration We conduct an ablation study on two dynamic functions in our proposed Adaptive Latent Prediction Acceleration Mechanism, as shown in Table 7. We observe that removing s(t) and w(t, l, i) significantly degrades performance. It indicates that s(t) and w(t, l, i) can facilitate the accuracy of predicted latents based on the latent variation rate at particular timesteps and the derivative magnitude ratio among diffusion layers. The underlying reason is that s(t) and w(t, l, i) jointly regulate the approximation between if (t, l) and (i)(t, l), ensuring robustness of latent prediction across diverse scenarios, even when the generated videos exhibit large motion variations. We further conduct an ablation study on different acceleration methods, presenting the results through progressive visualizations, as shown in Fig.8. We observe that as the number of generated frames increases, all competitors become progressively unstable, particularly in terms of facial and background consistency. When the sequence length exceeds 800 frames, all competitors exhibit varying degrees of face and body distortion, as well as color drift. Moreover, the generated portrait no longer strictly follows the driven video, with facial expressions turning stochastic, especially in mouth closure, eye motion, and head rotation. By contrast, our FlashPortrait achieves 6 inference speedup over the baseline while maintaining comparable visual quality and preserving high-fidelity identity consistency. Moreover, the generated facial expressions strictly follow the guidance of the driven video, which demonstrates the superiority of our Adaptive Latent Prediction Acceleration Mechanism over previous acceleration methods in the long-length portrait animation. A.5. Full/Half Body Portrait Animation We perform qualitative experiment in full/half-body portrait animations, as shown in Fig. 9. Each reference image has complex background layout and intricate foreground appearance. The first case even involves interactions with objects from the environment, such as an instrument, making it more challenging to maintain identity consistency and facial expression synchronization with the driven video. We can see that our FlashPortrait has the capacity to synthesize full/half-body portrait animations, even involving interactions with external objects. B. Long Portrait Animation To further validate the performance of our FlashPortrait in long-length portrait animation, we perform qualitative experiment in an extremely long case (4 minutes, FPS=30), as shown in Fig.10. Our FlashPortrait can still maintain identity consistency and ensure expression synchronization with the driven video, even after synthesizing 7000+ frames. 2 From theoretical perspective, FlashPortrait can synthesize infinite-length high-quality identity-preserving animations. B.1. More Portrait Animation Fig. 11, Fig. 12, Fig. 13, Fig. 14, and Fig. 15 presents additional portrait animation result synthesized by our FlashPortrait. Each driven video contains 1800+ frames, and we only select synthesized frames from the last 100 frames for presentation. The reference protagonists exhibit rich diversity, encompassing both male and female subjects across various ethnicities. They also present complex visual characteristics, including intricate hairstyles, richly textured clothing, elaborate tattoo patterns, and wide range of refined accessories. Each driven video contains substantial and dynamic facial expression motions with irregular expression patterns, such as head rotations and rapid blinking. We can observe that our FlashPortrait can accurately animate the reference image based on the driven video while maintaining strong identity consistency even after synthesizing 1800 frames. For example, the third row of Fig. 13 contains dramatic facial expression motions and exaggerated expression patterns, making it challenging for portrait animation model to preserve identity consistency while following the guidance of the driven video. Our FlashPortrait can still accurately manipulate the facial expression of the reference image (lip movement, eye movement, head movement) while maintaining high-quality identity consistency. B.2. Limitation and Future Work Fig. 16 shows one failure case of our FlashPortrait. When the reference protagonist is humanoid character, such as game avatar or mythological figure, its appearance does not strictly conform to real human facial standards. Since our model is primarily trained on real human video data, FlashPortrait tends to synthesize more realistic human face to replace the original reference protagonists face. This adaptation disrupts identity consistency and results in generated faces that deviate substantially from the reference image. One potential solution is to introduce an additional reference network to explicitly capture the face details of the reference images. This reference network needs to be trained from scratch on large-scale diverse video datasets. This part is left as future work. B.3. Ethical Concern Our FlashPortrait can animate the reference image based on the driven video, allowing reference image to be dynamically reenacted according to driven video. This capability presents risk of being exploited for deceptive media synthesis on social platforms. To address this risk, the deployment of robust sensitive-content and misuse-detection mechanisms is necessary to ensure responsible usage. Figure 8. Ablation study on different acceleration methods. w/o DF refers to w/o Dynamic Functions. 3 Figure 9. Full/Half-body portrait animation results. The images with red borders are the reference images. Figure 10. Long portrait animation results. The images with red borders are the reference images. 4 Figure 11. portrait animation results (1/5). The images with red borders are the reference images. 5 Figure 12. portrait animation results (2/5). The images with red borders are the reference images. 6 Figure 13. portrait animation results (3/5). The images with red borders are the reference images. 7 Figure 14. portrait animation results (4/5). The images with red borders are the reference images. 8 Figure 15. portrait animation results (5/5). The images with red borders are the reference images. Figure 16. One failure case of our FlashPortrait. The images with red borders are the reference images."
        }
    ],
    "affiliations": [
        "Fudan University",
        "Microsoft Research Asia",
        "Tencent Inc.",
        "Tongyi Lab, Alibaba Group",
        "Xian Jiaotong University"
    ]
}