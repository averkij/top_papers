{
    "paper_title": "AudioStory: Generating Long-Form Narrative Audio with Large Language Models",
    "authors": [
        "Yuxin Guo",
        "Teng Wang",
        "Yuying Ge",
        "Shijie Ma",
        "Yixiao Ge",
        "Wei Zou",
        "Ying Shan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in text-to-audio (TTA) generation excel at synthesizing short audio clips but struggle with long-form narrative audio, which requires temporal coherence and compositional reasoning. To address this gap, we propose AudioStory, a unified framework that integrates large language models (LLMs) with TTA systems to generate structured, long-form audio narratives. AudioStory possesses strong instruction-following reasoning generation capabilities. It employs LLMs to decompose complex narrative queries into temporally ordered sub-tasks with contextual cues, enabling coherent scene transitions and emotional tone consistency. AudioStory has two appealing features: (1) Decoupled bridging mechanism: AudioStory disentangles LLM-diffuser collaboration into two specialized components, i.e., a bridging query for intra-event semantic alignment and a residual query for cross-event coherence preservation. (2) End-to-end training: By unifying instruction comprehension and audio generation within a single end-to-end framework, AudioStory eliminates the need for modular training pipelines while enhancing synergy between components. Furthermore, we establish a benchmark AudioStory-10K, encompassing diverse domains such as animated soundscapes and natural sound narratives. Extensive experiments show the superiority of AudioStory on both single-audio generation and narrative audio generation, surpassing prior TTA baselines in both instruction-following ability and audio fidelity. Our code is available at https://github.com/TencentARC/AudioStory"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 2 ] . [ 1 8 8 0 0 2 . 8 0 5 2 : r AudioStory: Generating Long-Form Narrative Audio with Large Language Models Yuxin Guo1,2,3, Teng Wang2, Yuying Ge2, Shijie Ma1,2,3, Yixiao Ge2, Wei Zou1,3, Ying Shan2 1School of Artificial Intelligence, University of Chinese Academy of Sciences 3MAIS, Institute of Automation, CAS, Beijing 2ARC Lab, Tencent PCG Project Lead Corresponding Authors https://github.com/TencentARC/AudioStory"
        },
        {
            "title": "Abstract",
            "content": "Recent advances in text-to-audio (TTA) generation excel at synthesizing short audio clips but struggle with long-form narrative audio, which requires temporal coherence and compositional reasoning. To fill this gap, we propose AudioStory, unified framework that integrates large language models (LLMs) with TTA systems to generate structured, long-form audio narratives. AudioStory possesses strong instruction-following reasoning generation capabilities. It employs LLMs to decompose complex narrative queries into temporally ordered sub-tasks with contextual cues, enabling coherent scene transitions and emotional tone consistency. AudioStory has two appealing features: (1) Decoupled bridging mechanism: AudioStory disentangles LLM-diffuser collaboration into two specialized components, i.e., bridging query for intra-event semantic alignment and residual query for inter-event coherence preservation. (2) End-to-end training: By unifying instruction comprehension and audio generation within single end-to-end framework, AudioStory eliminates the need for modular training pipelines while enhancing synergy between components. Furthermore, we establish benchmark AudioStory10K, encompassing diverse domains such as animated soundscapes and natural sound narratives. Extensive experiments show the superiority of AudioStory on both single-audio generation and narrative audio generation, surpassing prior TTA baselines in both instruction-following ability and audio fidelity."
        },
        {
            "title": "Introduction",
            "content": "Audio content plays pivotal role in modern media, from immersive storytelling and podcasts to interactive entertainment and educational applications. Recent advancements in text-to-audio (TTA) generation, exemplified by models such as TangoFlux [1], AudioLDM [2], and Stable Audio [3], have demonstrated remarkable capabilities in synthesizing high-quality, short-form audio clips from textual descriptions. However, critical gap remains in generating long-form narrative audio, i.e., coherent, structured sequences of audio instances that unfold over extended durations, such as audiobooks, podcasts, or dynamic soundscapes for games. Long-form narrative audio generation introduces unique challenges that extend beyond single-prompt synthesis. First, it requires temporal coherence: maintaining consistency in themes, sound effects, and emotional tone across the whole audio. Second, it demands narrative reasoning to decompose complex instruction into logically ordered sub-events, characters, or environmental interactions. For instance, prompt like suspenseful chase through rainstorm: footsteps splash, thunder roars, car skids, and door slams shut necessitates not only generating individual sounds but also orchestrating their timing, intensity, and interplay to build tension. Existing TTA models, while Preprint. Under review. Figure 1: AudioStory decomposes multimodal instructions into sequence of coherent audio segments, capturing scene transitions, emotional tone, and segment timestamps. Unlike prior T5-based diffusion models, which struggle with complex queries, AudioStory empowers LLMs with high-level planning ability for instruction-followed and consistent long audio generation. proficient at capturing isolated events, often struggle with such compositional and temporal reasoning, leading to fragmented or inconsistent outputs. To address these challenges, we propose AudioStory, novel multi-step framework for generating long-form narrative audio by integrating the reasoning capabilities of LLMs with audio generation. As shown in Fig. 1, we propose interleaved reasoning generation following divide-and-conquer manner: reasoning for general narrative plans, decomposing plans into sequential generation actions, and generating interleaved audio events step-by-step. Specifically, AudioStory employs LLMs to decompose narrative query (in language or multimodality) into structured sequence of audiogenerative sub-tasks, each accompanied by contextual cues such as temporal offsets, emotional tone, and character interactions. These reasoning chains are then synthesized into audio events using diffusion backbone, with explicit mechanisms to ensure style consistency, smooth transitions and temporal alignment. We streamline the narrative planning via LLMs and audio synthesis via diffusion models into an end-to-end framework, enabling the generation of rich, multi-scene audio stories that adhere to user intent while preserving coherence over time. AudioStory introduces several technical innovations: First, unlike prior approaches [4, 5] that bridge LLMs with audio diffusers through predefined textual spaces (e.g., T5 [6]), we propose decoupled bridging space consisting of two distinct tokens: (1) semantic tokens, which encode text-oriented audio semantics, and (2) residual tokens, which capture nuanced acoustic cues and cross-event correlations. This design effectively improves both audio fidelity and temporal consistency during generation. Second, unlike zero-shot integration of LLMs and diffusers, our framework supports end-to-end progressive training, enabling joint optimization of instruction understanding and audio synthesis. This synergistic training paradigm enhances both audio understanding and generation performance. Third, we introduce the first narrative audio generation benchmark, providing comprehensive evaluation framework for assessing audio generation quality and consistency. The contributions of the paper are as follows: We introduce AudioStory for narrative audio generation, which integrates LLM-based reasoning and iterative diffusion-based generation in unified framework, with strong multimodal instructionfollowing and audio generation abilities. We propose decoupled bridging tokens for LLM-diffuser collaboration, using semantic tokens (text-oriented audio semantics) and residual tokens (nuanced acoustic cues) to improve audio fidelity and temporal consistency. We introduce synergistic training paradigm, facilitating collaboration and complementarity between LLM and diffusion models. Unlike zero-shot LLM-diffusion integration, our framework enables end-to-end joint training, enhancing both multimodal understanding and generation. Experiments show AudoStory significantly surpasses prior diffusion-based and MLLM-based models by large margin in narrative audio generation. We also uncover some important findings across multiple aspects, including reasoning formulation, bridging mechanism and training recipes."
        },
        {
            "title": "2 Related Works",
            "content": "Text-to-audio generation (TTA). Recent advances in latent diffusion and flow-matching frameworks have significantly advanced text-to-audio generation. Diffusion/flow-based approaches, exemplified by Make-An-Audio [7] and AudioLDM [8, 2], synthesize audio through iterative denoising of textconditioned latent representations. Extensions like Tango [9, 10], Audio Flamingo [11], GenAu [12], Fugatto [13] further enhance design spaces of latent space, data quality and cross-modal alignments. Recently, Stable Audio series [3] employs hierarchical latent diffusion trained on large-scale datasets for high-fidelity output. Beyond diffusion-based priors, flow-matching techniques optimize probability density transport for audio synthesis. VoiceBox [14] enables zero-shot style transfer via continuous normalizing flows, while AudioBox [15] and FlashAudio [16] prioritize computational efficiency through rectified flow architectures. TangoFlux [1] introduces CLAP-ranked preference optimization to iteratively generates and optimizes preference data to enhance text-audio alignment. Existing methods align text and audio semantically but primarily target descriptive queries, limiting interactive control and adaptability to evolving instructions. They are also confined to short audio domains. These limitations demand TTA models to handle complex instructions over long durations. Any-to-any multimodal LLMs. Within the rapidly evolving field of multimodal learning, any-toany generation across vision, language, and audio modalities represents significant frontier [17, 18, 4, 19, 5, 20, 21, 22, 23]. This paradigm aims for models capable of accepting arbitrary input modalities and generating outputs in any desired modality. Pioneering efforts include CoDi [24] and CoDi-2 [18], which leveraged composable diffusion for diverse modality handling. Spider [5] further extended these capabilities by enabling the generation of multiple modalities in single response. NExT-GPT [4] demonstrated the efficacy of lightweight alignment for adapting LLMs to multimodal tasks, while AnyGPT [19] showcased the potential of discrete sequence modeling for unified multimodal processing. Unified-IO2 [25] highlighted the impact of scale and unified architectures in achieving state-of-the-art performance across broad spectrum of modalities and tasks. Despite these advancements, current methods exhibit limitations in long-context generation with complex instructions: First, they primarily focus on speech generation and simple caption-tomusic or caption-to-sound tasks, struggling to comprehend general and intricate human instructions beyond basic caption; Second, their audio generation is typically limited to single, short segments, hindering the generation of longer audio sequences. Compositional audio generation. Agentic workflows employ multiple off-the-shelf expert tools and controller for compositional audio synthesis. Works like WavJourney [26] and MM-StoryAgent [27] decomposed audio generation into text-centric interface and employs separate text-to-speech, audio, and music decoders for audio creation and storytelling. While these agents could generate combinations of audio components, their zero-shot nature suffers from suboptimal planning and limited adaption of nuanced acoustic cues, degrading instruction-following ability and overall audio quality. Instead, we target on end-to-end training to integrate LLM-based chain-like reasoning and flux decoders for long-term, consistent audio generation."
        },
        {
            "title": "3 Narrative Audio Generation",
            "content": "Problem definition. Narrative audio generation aims to generate long-form, structured and temporally coherent audio sequences = {Am}M m=1, given multimodal instruction xins (e.g., language, audio or vision), where is the number of audio segments. The task shares similar formulation with the text-to-audio generation, but is far more challenging due to two distinct capabilities: (1) Temporal coherence, i.e., maintaining consistency in themes, sound effects, and emotional tone i.e., decomposing high-level narrative across extended durations; (2) Compositional reasoning. instructions into logically ordered events (e.g., footsteps splash, then thunder roars) with precise timing and contextual interactions. Existing TTA systems, while effective for short clips, lack explicit mechanisms to model cross-segment dependencies or align audio events with evolving narrative structures, limiting their applicability to real-world scenarios. The AudioStory-10k benchmark. Given the lack of quantitative evaluation, we establish the AudioStory-10k benchmark for the narrative audio generation task. AudioStory-10k comprises 10k annotated audios paired with narrative prompts. We collect videos from two primary sources: 3 Natural sounds: We carefully select 4,723 audio instances from UnAV-100 [28], covering broad spectrum of real-world environmental recordings (e.g., rainstorms, animal calls, rustling leaves) and human activities (e.g., footsteps, door slams, and conversations). This collection ensures sufficient coverage of everyday acoustic events and ambient soundscapes. Animated sounds: We curate 5,332 audio clips from 157 episodes of Tom & Jerry, capturing stylized background music (e.g., orchestral pieces, string sections) and sound effects (e.g., slapstick actions, cartoonish collisions and rapid movements). These animated sounds feature stylized and expressive audio content, distinct from natural sound recordings. The annotation pipeline involves three stages. First, we filter the videos with sequential audio events, ensuring the storyline of the audio is visually-grounded for meaningful activities1. Then, we parse the video into several key audio events by Gemini-2.5-Pro [31], each of which is labeled with its timestamps, audio caption and visual captions. Next, given these text-based timestamped captions, we prompt GPT-4o [32] to generate diverse instructions and chain-like reasoning steps. To be specific, we design diverse format of multimodal instructions, including text-only instructions for narrative audio generation, audio-text ones for audio continuation and video-text ones for video dubbing (shown in Fig. 1). For flexible control of duration and semantic elements of generated audios, we make the intermediate reasoning encompass at least the following steps: storyline summarization for global summarization of general story, event decomposition for inferring the number of audio events, sound reasoning for predicting timestamp and key elements (e.g., emotional tone, scene transition) of each event. All detailed prompts and processing steps are in Appendix E.1. Evaluation metrics. The AudioStory-10k dataset includes 5.3k samples of natural sounds and 4.7k samples of cartoon audios. We randomly divided the dataset into 85% for training and 15% for testing. We propose comprehensive evaluation spanning three aspects: instruction-following, consistency, and generation quality. (1) Instruction-following ability is quantified through multimodal alignment between instructions and generated audio (Instruct), CLAP score for audio-caption similarity, and reasoning text quality for logical decomposition and event planning. (2) Consistency metrics evaluates internal consistency (timbre uniformity, entity persistence) and temporal coherence (acoustic transitions, emotional flow). (3) Generation quality metrics employs FD and FAD [33] against ground-truth audio. Except from CLAP, FD, FAD-based metrics, we employ Gemini-2.0-flash as the evaluator with score range of 0-5. More details could be found in the Appendix E.2."
        },
        {
            "title": "4 AudioStory",
            "content": "Overview. To achieve instruction-followed audio generation, the ability to understand the input instruction and reason about relevant audio sub-events is essential. To this end, AudioStory adopts unified understanding-generation framework  (Fig. 2)  . Specifically, given multimodal instructions, an LLM analyzes and decomposes it into structured audio sub-events with context. Based on the inferred sub-events, the LLM first performs interleaved reasoning generation (Sec. 4.1), sequentially producing captions and bridging tokens between the LLM and the audio generator (Sec. 4.2). Through progressive end-to-end training, AudioStory ultimately achieves both strong instruction comprehension and high-quality audio generation (Sec. 4.3). 4.1 Interleaved Reasoning Generation Directly generating long-form narrative audio that aligns with complex instructions is challenging. We take the spirit of divide-and-conquer and propose decoupling the input instruction into chronological short audio clips, which are then combined to form the complete long-form narrative audio. Single-audio clip generation. The ability to generate individual audio clips from captions is foundational step toward producing sequential audio events. For audio clip generation, the LLM generates bridge tokens from given caption, which serve as conditions for the DiT. While this method works well for short audio generation based on simple captions, it becomes insufficient for complex instructions involving multiple events, temporal relationships, or narrative structures. 1For Tom & Jerry, where episodes typically consist of numerous discontinuous shots with fast transitions, we employ PySceneDetect [29] to detect preliminary shot boundaries. These boundaries are further refined by thresholding the similarity between frame-level DINOv2 [30] features, which retains only high-quality, temporally consistent shots as individual video instances. For UnAV-100, we keep the videos longer than 30s. 4 Figure 2: Overview of AudioStory, with three core components: (a) unified framewrok: The reasoning-capable LLM processes the instruction input, decomposes the long audio into structured generation sub-tasks, and sequentially generates caption, semantic tokens, and residual tokens for each audio clip. (b) Audio Generator: After fusing semantic and residual tokens, they are combined with the duration information as conditioning inputs to the DiT, which then generates each audio clip. (c) Training strategy: Training is conducted in three stages to progressively enhance generation fidelity, semantic understanding, and global coherence. Interleaved reasoning generation for long-audio generation. We propose to decouple complex, long-form audio into multiple audio segments for segment-by-segment generation. This divide-andconquer process consists of two components: (1) Storyline reasoning: LLMs reason through the entire instruction, inferring the number of audio events. Furthermore, LLMs analyze the start and end timestamps of each event, as well as the event description and corresponding audio content that should be included. (2) Interleaved generation: For each event, the LLM infers the caption, duration, and corresponding bridge queries (semantic tokens and residual tokens, as described in Sec. 4.2), enabling interleaved generation. These queries, along with duration information, are then provided as conditional inputs to the DiT-based audio generator. By accurately predicting durations and utilizing semantically rich bridging tokens, the model ensures both coherent audio semantics within each event and consistency across events. The training data is structured as: [BOS][BOT]{#event}{storyline reasoning tokens}[EOT][BOG]{caption}{duration} TsemanticTresidual[EOG] [BOG]{caption}{duration}TsemanticTresidual[EOG][EOS]. The textual tokens in the entire reasoning process is supervised by the next token prediction loss: (1) Lreason = L#event text + Lcontent text + Lcaption text , where Ltext = (cid:89) i= p(xiX<i, Xp,<i). (2) 4.2 Decoupled Bridging Mechanism Once the LLM is capable of effective reasoning, establishing seamless bridge between the LLM and the DiT becomes crucial. However, text alone might not be the optimal bridge. Although it carries rich semantics, it fails to capture diverse low-level details of the audio modality, e.g., timbre, rhythm, and ambience. Consequently, we propose decoupled bridges queries, which could be divided into semantic Tsemantic and residual tokens Tresidual. The semantic tokens represent the audios high-level semantics, while the residual tokens carry low-level audio details. They complement each other, enabling the disentanglement of audio information. In practice, after producing the caption for each audio event, the LLM collectively generates semantic and residual tokens. For semantic tokens, we use the textual tokens from Flan-T5 [34] Tgt semantic as the supervision and apply MSE loss: semantic Tsemantic2 2. Lmse = Tgt (3) 5 The residual tokens are employed to supplement the missing information of the semantic tokens. Then, both types of tokens are merged and fed into as the conditional inputs of DiT. Here, we adopt multi-head cross-attention to merge these two tokens and obtain the resultant bridge queries: Hbridge = Cross-Attn(Tsemantic, Tresidual, Tresidual). (4) For audio generator with Hbridge as condition, we employ flow-matching [35] for generative modeling: Lflow = Ex1,x0,tu(xt, t, c) vt2 2, (5) where is the condition and we choose = Hbridge and is uniformly sampled from [0, 1]. Through the generative supervision, Tresidual can capture detailed information and complement Tsemantic. 4.3 Progressive Training Strategy After establishing an effective bridge between the LLM and DiT, it becomes essential to design an efficient end-to-end training mechanism to build synergy between the understanding and generation tasks. We propose progressive training strategy, following single-to-multi and generation-tounification paradigm. The training could be divided into three stages, where the model (1) learn to generate single audio segments, followed by (2) unified generation and understanding for single audios and (3) long-audio adaptation. Stage-I: Single-audio generation. There are two sub-stages. (1) Stage-I-Warm, AudioStory learns to generate semantic tokens with MSE supervision in Eq. (3). Only the LoRA of the LLM and the projector of Tsemantic are updated. (2) Stage-I-Whole, AudioStory regresses bridge queries based on the input caption, i.e., generating Tsemantic and Tresidual, respectively. They are subsequently merged via Eq. (4) and fed into DiT. Here, the regression of Tsemantic and the prediction of its beginning and end tokens are supervised. We tune LoRA of the LLM, all projectors, the attention layer and the generation model DiT. The learning objectives are shown below: Lwarm s1 = Lmse, Lwhole s1 = Lmse + λ1Ltoken text + λ2Lflow, (6) where Ltoken text possesses strong capability for single-audio generation. is only applied to the start and the end tokens of Tsemantic. After this Stage-I, AudioStory Stage-II: Single-audio unified generation and understanding. Building upon Stage-I, we further introduce audio understanding data to enable unified generation and understanding of single-audio clips. The model takes audio as input for understanding. We freeze the audio encoder while the trainable parameters remain the same as Stage-I-Whole. The learning objectives are in Eq (7). Ls2 = Lmse + λ1Ltext + λ2Lflow. (7) With this unified training, AudioStorys generation abilities can be further enhanced. Stage-III: Long-audio unified generation and understanding. We extend the unified training in Stage-II to long-form audio. We further introduce Interleaved Reasoning Generation (Sec. 4.1) for narrative audio generation. We curate high-quality multi-audio dataset to perform supervised fine-tuning. For the generation task, the model sequentially infers the number of audio events based on the input instruction, analyzes the audio content, and performs interleaved generation of captions, semantic tokens, and residual tokens. For the audio continuation task, given the input audio and instruction, the model comprehends the inputs, reasons the key events with story details, and finally generates several short audio segments in clip-by-clip manner. The audio understanding data incorporates audio Q&A and instruction data. We keep the learnable components the same as Stage-II. The overall learning objectives are: Ls3 = Lmse + λ1Ltext + λ2Lflow + λ3Lreason. (8)"
        },
        {
            "title": "5 Experiments",
            "content": "In this section, we first present the experimental setup (Sec. 5.1). Then, we compare AudioStory with existing TTA and unified models on long-form audio generation (Sec. 5.2). We also study the audio understanding and the audio generation (Sec. 5.3) ability of AudioStory in short audio clips, showing its superior fundamental ability. Finally, in Sec. 5.5, we conduct an in-depth exploration of reasoning forms, bridging query types, joint training strategies, and the synergy between understanding and generation, and provide several key insights. 6 Table 1: Comparative results on long-audio generation. Instruct is short for instruction-following and CLAP denotes CLAP score, gt denotes ground-truth. Consis. and Coher. are short for consistency and coherence. Here, bold and underline indicate the best and the second-best results. Model Instruction-Following Consistency Generation Quality Max. Duration Instruct. CLAP Reasoning Consis. Coher. FD FAD AudioLDM2 [2] TangoFlux [1] Caps (gt)+TangoFlux [1] LLM+TangoFlux [1] LLM+CoDi [24] LLM+NExT-GPT [4] AudioStory 2.8 3.2 4.0 3.5 3.2 3.3 4.1 0.296 0.317 0.348 0.322 0.286 0. 0.392 - - - 3.5 3.5 3.5 4.2 4.6 4.1 2.4 2.1 1.4 1. 4.1 4.4 4.2 2.0 1.9 1.4 1.7 3.9 3.43 2.48 1.79 2.55 3.39 3. 1.43 4.49 3.49 3.59 3.82 4.04 3.99 3.00 10s 30s 30s 30s 10s 10s 150s 5.1 Experimental Setup Implementation details. We choose Qwen-2.5-3B-Instruct [36] as the LLM and employ DiT pretrained from TangoFlux [1] as the initialization. For encoding instruction for the audio continuation task, we employ Whisper-large-v3 [37] as the audio encoder. The projector has two layers with GeLU activations. In Stage-I, AudioStory is trained with lr= 2e4 for 50 epochs with per-device batch size of 32. In Stage-II, we use lr=1e-4 for 10 epochs. The ratio of understanding and generation data is 2:1. In Stage-III, we set different learning rates for LLM and DiT. We set λ1 = 1, λ2 = 0.2, λ3 = 0.4. The tunable parameters three-stage training are LoRAs in LLMs, projectors, the cross-attention fuser for bridging queries, and DiT. More details are shown in the Appendix. Training datasets. The training dataset comprises the understanding dataset, single-audio generation and multi-audio (long-audio) generation datasets. For the understanding dataset, we integrated AudioSetCaps [38], VGGSound [39], MusicCaps [40], and Auto-ACD [41], converting their captions into QA format. Additionally, we incorporated AudioSetCaps-QA and VGGSound-QA datasets, resulting in 1M audio-QA pairs in total. For the single-audio generation dataset, we combined AudioSetCaps, VGGSound, MusicCaps [40], and Auto-ACD, resulting in 700k audio-caption pairs. For the multi-audio generation dataset, we curated the AudioStory-10k dataset, with details provided in Sec. 3. In Stage-I, we train the model on we train the model on single-audio generation datasets. Stage-II further incorporates the audio understanding dataset beyond Stage-I. As for Stage-III, our model is trained using multi-audio generation as well as understanding datasets. Evaluation metrics. For single-audio generation, we employ the AudioLDM-eval toolkit2 to compute Frechet Distance (FD), Frechet Audio Distance (FAD) [33], KL-Divergence (KL), and stable-audiometrics3 for FDopenl3 [42], KLpasst [43], and CLAP score on AudioCaps testset [44]. For audio understanding, we consider the tasks of audio question answering (AQA), and audio captioning on AudioCaps and Clotho dataset [45], reporting SPIDEr, CIDEr, and ACC scores. The evaluation metrics for long-audio generation is presented in Sec. 3. Baseline methods. Prior audio generation models could be divided into two groups: pure TTA models like AudioLDM2 [2] and TangoFlux [1] and LLM-based unified models, including CoDi [24] and NExT-GPT [4]. Both of them could only generate short audio clips. For long-form audio generation, we curated three classes of baselines: (1) Directly generating audios with maximum available durations using the whole textual caption. (2) Incorporating LLMs to reason and generate captions for each short audio clip, which are then fed into baseline models to generate multiple audio clips separately. These clips are then concatenated to constitute the final long-form audio. (3) Directly using the ground truth captions in the benchmark, serving as the oracle setting and upper bound for baseline models. In addition, we also report the performance of AudioStory on the Tom & Jerry and the audio continuation task. 2https://github.com/haoheliu/audioldm_eval 3https://github.com/Stability-AI/stable-audio-metrics 7 Table 2: Single audio understanding performance. Table 3: Single audio generation performance. Model ClothoCaps ClothoAQA AudioCaps Model SPIDEr CIDEr ACC B-ACC SPIDEr CIDEr AudioCaps Test Set FDopenl3 KLpasst FD FAD KL CLAP UIO-2 XXL [25] CoDi [24] NExT-GPT [4] Spider [5] 5.7 6.2 13.8 - 6.5 7.3 20.3 - - - 26.4 - - - 39.5 - AudioStory-Base 24. 37.7 42.8 60.6 - 48.0 53.4 53.7 54.8 48.9 78.9 80.7 81. 83.2 Make-An-Audio [7] stable-audio-open [3] AudioLDM2 [2] TangoFlux [1] CoDi [24] NExT-GPT [4] AudioStory-Base 128.49 103.68 87.74 83.58 121.66 107.18 83.39 1.16 1.12 1.01 0.95 1.17 1. 0.91 1.65 1.63 1.59 1.57 1.69 1.64 1.52 3.16 2.98 2.63 2.34 9.61 5.69 2.29 0.63 0.61 0.57 0.52 0.60 0. 0.51 0.256 0.298 0.252 0.385 0.228 0.265 0.383 5.2 Long-Form Narrative Audio Generation Instruction-following ability. As shown in Table 1, considering the instruction-following aspect, AudioStory demonstrates significant advantage in complex scenarios involving multiple events and sounding objectives. It outperforms the LLM-aided TTA models by 17.85% on the CLAP score, thereby demonstrating the superior instruction-following generation capability of our model. Our method effectively addresses the issue of overlooking sounding entities, which can be attributed to the enhanced understanding and decomposition of the instruction. Generation quality. AudioStory demonstrates strong long-form audio generation performance across both natural scenes and the cartoon domain. Our approach achieves superior FD and FAD scores compared to diffuser-based and LLM+diffuser baselines. This improvement is reasonable: (1) we enhance long-form audio generation by single-audio clip training, effectively extending high-quality short-audio generation to longer sequences; (2) the duration of generated audio is longer and more closely matches the reference long audios than those generated by previous methods. Consistency. Notably, consistency is meaningful only with strong instruction-following. For example, AudioLDM2, despite high consistency scores from short (10s) outputs, performs poorly on instructionfollowing, making it weak baseline. In contrast, our method achieves substantial advantages in both consistency and coherence, reaching scores of 4.0 and 3.7, respectively, as in Table 1. It is worth noting that in the consistency evaluation, AudioStory achieves comparable performance despite generating significantly longer audio with richer narratives compared to TTA models. 5.3 Single-Audio Generation Joint audio generation & understanding. We also evaluate our models performance on short audio generation and understanding tasks, and conduct comparisons with TTA and LLM-based models. For the generation task in Table 3, AudioStory outperforms prior competitors on both suites of evaluation tools, even outperforming the state-of-the-art TTA model, i.e., TangoFlux [1], indicating the effectiveness of the proposed LLM and DiT bridging mechanism. As for the audio understanding task in Table 2, AudioStory outperforms advanced LLM-based models, which means that our method could competently handle both generation and understanding tasks. 5.4 Qualitative Analysis AudioStory exhibits strong reasoning capabilities, it can accurately divide the input instruction into several events based on narrative logic and temporal order, subsequently generating short audio clips segment by segment, and ultimately composing coherent long-form audio. Besides, AudioStory could accurately infer the duration of each audio clip. Here, we provide qualitative case in Fig. 3. More cases are presented in the Appendix B. 5.5 Ablation Studies Does interleaved reasoning generation help narrative audio generation? We progressively investigate which forms of reasoning are effective for long-form narrative audio generation. We ablate two model variants: (a) model that skips instruction analysis and directly generates captions for audio clips, and (b) model that performs instruction decomposition without explicitly generating 8 Table 4: Ablations of reasoning. Variant Cons. Inst. FAD CLAP w/o reasoning w/o interleaved w/ reasoning 3.1 1.6 4. 3.1 1.2 4.1 4.13 16.03 3.06 0.34 0.14 0.39 Figure 3: Qualitative case of long-form audio generation. audio captions. As shown in Table 4, ablating without reasoning leads to simplified audio events with missing objects or actions, resulting in significant drop in both instruction-following performance and CLAP score. When interleaved reasoning is removed, the model can still infer event content, but lacks contextual guidance when generating bridge queries, severely degrading audio quality. We conclude that reasoning is essential for narrative audio generation, with interleaved reasoning being the most critical component. Explicit caption generation for each audio clip is necessary to ensure generation quality. BQ Which type of features are suitable for bridging between the LLM and the DiT? Our analysis suggests that audio features, on one hand, have lower semantic density, and on the other hand, are more difficult for the LLM to fit compared to textual features, especially given the complex temporal structure in Whisper. Therefore, supervising the semantic tokens with textual features is more suitable and efficient. For residual tokens, Table 5 (c)(g) shows that explicit or weak supervision using existing audio features significantly harms generation performance. In summary, text features with rich semantics are well-suited for supervising semantic tokens, while for residual tokens, applying weak supervision through the DiT loss is the most effective way to capture complementary low-level audio information. Table 5: Ablations on bridging mechanism. ID Single Multi Sup. Feat. (a) Semantic AudioMAE [46] (b) Whipser [37] (c) Residual AudioMAE [46] Whisper [37] (d) AudioMAE [46] Whisper [37] (e) Residual (f) +guid. T5 w/o guid. 11.39 12. 10.06 11.21 9.24 11.06 9.55 10.26 4.21 4.39 3.60 3.71 Ours 2.29 3.12 (g) What are the key factors in end-to-end joint training of unified models? For unified models, prior arts typically train the LLM and DiT separately, connecting them via zero-shot bridging mechanism, which results in feature gap. To address this, we propose end-to-end joint training of the LLM and DiT. We begin by focusing on the end-to-end training paradigm, as in Table 6. Notably, when residual tokens are removed, overall performance drops significantly. Analysis reveals that the LLM and DiT focus on different types of information, and directly updating the LLM using the DiT loss severely impairs its performance. In contrast, residual tokens effectively alleviate this issue. Secondly, we explore how to configure DiTs learnable parameters. As shown in (c)(f), fully freezing DiT degrades performance, while fully unfreezing it achieves the best results. Notably, unfreezing MMDiT outperforms Single-DiT, since the latter focuses on low-level features that are more sensitive to noise, thus affecting generation quality. Thus, we can draw the following conclusions: (1) End-to-end joint training of the LLM and DiT is essential. (2) Residual tokens play critical role, as they capture 9 Table 6: Ablations on the end-to-end joint training strategy of DiT. Here S-DiT and M-DiT denote Single-DiT and MM-DiT. Consis. denotes consistency. ID (a) (b) (c) (d) (e) (f) Semantic Tokens Residual Tokens DiT Joint Training Tunable Module Single Audio Multi Audio FD FAD KL Consis. FAD - open all freeze 0.52 2.33 1.57 2.16 0.84 4.66 4.86 11.04 0.89 0.64 5.84 0.67 3.21 0.51 2.29 3.2 3.4 1.3 2.1 3.5 4.3 5.23 4.98 12.97 6.28 3.64 3.00 open S-DiT 2.37 open M-DiT 1.98 1.53 open all Table 7: Ablations on progressive training. Gen., Und. and BQ denote generation, understanding and Bridge Queries. SAG and LAG are short for single and long-form audio generation. ID (a) (b) (c) (d) (e) (f) (g) (h) Order Stage-I Stage-II Stage-III SAG LAG FAD FAD CIDEr SPIDEr Audio Und. Und.Gen. Gen.Und. Und. Und. Und. - BQ BQ BQ BQ BQ BQ DiT joint - Und. Und. - - DiT joint - - DiT joint Und. N/A DiT joint + Und. - 7.42 6.50 2.37 2.35 3.61 2.29 5.70 - 9.53 7.26 5.23 4.98 6.50 3. 8.74 35.7 36.9 38.6 - 31.5 24.6 37.7 27.3 23.1 23.8 24.9 - 19.5 16.4 24. 18.2 low-level complementary information and help mitigate conflicts between DiT and LLM during optimization. (3) Fully unfreezing DiT is necessary. Selectively unfreezing either the Single-DiT or MM-DiT alone leads to suboptimal performance. How to progressively build the synergy between generation and understanding? Both understanding and generation training are essential to our model, making progressive training strategy crucial. Here, we examine the effectiveness of various progressive training approaches. In Table 7, without progressive training, performance on both comprehension and generation drops significantly, even worse than training the tasks independently, which is primarily due to the inherent conflict and task interference between them. In contrast, well-structured progressive strategy enables unified training to outperform isolated approaches, highlighting its necessity. Further exploration into synergizing generation and comprehension reveals key insights: training generation first, then adding comprehension, yields optimal overall performance, with strong comprehension accuracy. In contrast, reversing the order harms generation, and interleaved training similarly undermines overall optimization. Therefore, we conclude that generation and understanding exhibit inherent synergy, and their optimal training order depends on the primary objective. For unified generation-understanding models, training generation first and then introducing understanding is the most effective strategy. In contrast, omitting progressive training or interleaving both tasks impairs overall optimization. 5.6 Human Evaluation Evaluation protocol. Beyond API-based evaluation, we further conducted an anonymous user study on our model and baseline models. We employ 30 participants to manually score total of 150 audio clips, generated from 50 instructions, by our model, Tangoflux, and Next-GPT, respectively. The participants listened to the long-form audio generated by different models based on the same instruction. They scored the audio on four criteria: instruction-following, consistency, generation quality, and reasoning logic. The scores were averaged to compute user consistency. As shown in the Table 8, AudioStory consistently outperforms other competitors in terms of instruction-following, consistency, quality and reasoning logic. Correlation between Gemini-based & human-based evaluation. Qualitatively, human evaluation results show our model performs the best among all three models, with the LLM + TTA model 10 Table 8: Human evaluation of the generated audios for different methods on: instruct-following, consistency, fidelity, and reasoning logic. Table 9: Correlation of Gemini and human scores. Method Instruct-Follow Consist. Fidelity Reason. Logic LLM + TangoFlux LLM + NExT-GPT AudioStory (Ours) 3.52 3.10 4.23 3.22 2.56 4.68 3.58 2.87 4.37 3.19 3.14 4.22 Across model Across model Kappa Coef. 0.91 0.83 Figure 4: Case of naive video dubbing: First, we extract captions from the video, then write the extracted captions as instructions and send them to AudioStory for audio generation. outperforming the LLM + any-to-any model. This aligns with the results from our Gemini evaluation. Quantitatively, we analyze the correlation between the human subjective and Gemini-based objective evaluation. We calculate Cohens kappa coefficient between these two evaluation protocols. Specifically, we compute the correlation across two dimensions, i.e., different comparative methods in Table 8 and different test samples. The results in Table 9 indicate high correlation between the human and Gemini scoring distributions across various models and samples, validating the correctness of the proposed Gemini-based evaluation."
        },
        {
            "title": "6 Extended Applications",
            "content": "Video dubbing. While previous experiments have focused on text-based instructions, we now extend the application of AudioStory to more practical scenario: video dubbing. This enhancement enables the model to thoroughly analyze video content, reason about the sequence of events and their corresponding timestamps, and generate synchronized audio. An initial approach is to employ Gemini-2.5-pro to generate one-sentence caption summarizing the entire video, followed by instruction-based audio generation, as illustrated in Fig. 4. However, this method is not conducive 11 Figure 5: Case of video dubbing: We input both the video and the instruction into the model, which parses the narrative into segments, extracts story details with corresponding audio elements, and sequentially generates aligned audio clips.. Figure 6: Qualitative cases of audio continuation. to producing audio that aligns closely with the visual content. Furthermore, the model is designed to accept both video data and instructions as input. The LLM performs reasoning on the video and produces bridging tokens. During the reasoning phase, the LLM first understands the overall content of the video, then sequentially breaks it down into events based on their temporal order. It infers the specific visual details and corresponding audio information for each event. Technically, we replace the LLM with pretrained video MLLM (i.e., Qwen2.5-VL [47]) and jointly train the LLM and audio generator using LoRA tuning. The training data is from the animated sound partition of AudioStory-10k. We provide the video dubbing results in Fig. 5. Audio continuation. Given an audio segment and an instruction, our model performs audio continuation. AudioStory first reasons about the content of the subsequent audio to be generated, then proceeds with segment-by-segment generation. The concatenated results are shown in Fig. 6."
        },
        {
            "title": "7 Conclusion",
            "content": "In this paper, we tackle the key limitations of existing text-to-audio and unified models in generating long-form narrative audio in complex scenarios. We introduce AudioStory, unified understandinggeneration model endowed with robust multimodal instruction-following and reasoning capabilities. To achieve this, we design an interleaved reasoning generation process, decoupled bridging mechanism, and progressive training strategy that jointly leverage the reasoning power of LLMs and strengthen the synergy between understanding and generation. Additionally, we present AudioStory10k, the first benchmark for long-form narrative audio generation, which includes fine-grained annotations of audio and audio-visual events with timestamps and detailed reasoning trajectories. Our comprehensive analyses cover reasoning forms, bridge query types, end-to-end training strategies for LLM-DiT integration, and the collaborative dynamics between understanding and generation, providing practical insights for future model development. Limitations and Future Work. Since multimodal instruction for long audio generation remains underexplored, future work can incorporate more sophisticated designs, e.g., integrating multiple audio generators to better address the issue of overlapping audio segments. We also consider blending text and audio generation within the same autoregressive multimodal LLM, as well as delve into the relationship and synergy between audio generation and understanding."
        },
        {
            "title": "References",
            "content": "[1] Chia-Yu Hung, Navonil Majumder, Zhifeng Kong, Ambuj Mehrish, Amir Ali Bagherzadeh, Chuan Li, Rafael Valle, Bryan Catanzaro, and Soujanya Poria. Tangoflux: Super fast and faithful text to audio generation with flow matching and clap-ranked preference optimization. arXiv preprint arXiv:2412.21037, 2024. 1, 3, 7, 8 [2] Haohe Liu, Yi Yuan, Xubo Liu, Xinhao Mei, Qiuqiang Kong, Qiao Tian, Yuping Wang, Wenwu Wang, Yuxuan Wang, and Mark D. Plumbley. Audioldm 2: Learning holistic audio generation with self-supervised pretraining, 2024. 1, 3, 7, 8 [3] Zach Evans, Julian D. Parker, CJ Carr, Zack Zukowski, Josiah Taylor, and Jordi Pons. Stable audio open, 2024. 1, 3, 8 [4] Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-Seng Chua. NExt-GPT: Any-to-any multimodal LLM. In Forty-first International Conference on Machine Learning, 2024. 2, 3, 7, [5] Jinxiang Lai, Jie Zhang, Jun Liu, Jian Li, Xiaocheng Lu, and Song Guo. Spider: Any-to-many multimodal llm. arXiv preprint arXiv:2411.09439, 2024. 2, 3, 8 [6] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with unified text-to-text transformer, 2023. 2 [7] Rongjie Huang, Jiawei Huang, Dongchao Yang, Yi Ren, Luping Liu, Mingze Li, Zhenhui Ye, Jinglin Liu, Xiang Yin, and Zhou Zhao. Make-an-audio: Text-to-audio generation with prompt-enhanced diffusion models, 2023. 3, 8 [8] Haohe Liu, Zehua Chen, Yi Yuan, Xinhao Mei, Xubo Liu, Danilo Mandic, Wenwu Wang, and Mark D. Plumbley. Audioldm: Text-to-audio generation with latent diffusion models, 2023. [9] Navonil Majumder, Chia-Yu Hung, Deepanway Ghosal, Wei-Ning Hsu, Rada Mihalcea, and Soujanya Poria. Tango 2: Aligning diffusion-based text-to-audio generations through direct preference optimization, 2024. 3 13 [10] Deepanway Ghosal, Navonil Majumder, Ambuj Mehrish, and Soujanya Poria. Text-to-audio generation using instruction-tuned llm and latent diffusion model, 2023. 3 [11] Zhifeng Kong, Arushi Goel, Rohan Badlani, Wei Ping, Rafael Valle, and Bryan Catanzaro. Audio flamingo: novel audio language model with few-shot learning and dialogue abilities, 2024. [12] Moayed Haji-Ali, Willi Menapace, Aliaksandr Siarohin, Guha Balakrishnan, and Vicente Ordonez. Taming data and transformers for audio generation. arXiv preprint arXiv:2406.19388, 2024. 3 [13] Rafael Valle, Rohan Badlani, Zhifeng Kong, Sang-gil Lee, Arushi Goel, Sungwon Kim, Joao Felipe Santos, Shuqi Dai, Siddharth Gururani, Aya Aljafari, et al. Fugatto 1: Foundational generative audio transformer opus 1. In The Thirteenth International Conference on Learning Representations, 2025. 3 [14] Matthew Le, Apoorv Vyas, Bowen Shi, Brian Karrer, Leda Sari, Rashel Moritz, Mary Williamson, Vimal Manohar, Yossi Adi, Jay Mahadeokar, and Wei-Ning Hsu. Voicebox: Text-guided multilingual universal speech generation at scale, 2023. 3 [15] Apoorv Vyas, Bowen Shi, Matthew Le, Andros Tjandra, Yi-Chiao Wu, Baishan Guo, Jiemin Zhang, Xinyue Zhang, Robert Adkins, William Ngan, Jeff Wang, Ivan Cruz, Bapi Akula, Akinniyi Akinyemi, Brian Ellis, Rashel Moritz, Yael Yungster, Alice Rakotoarison, Liang Tan, Chris Summers, Carleigh Wood, Joshua Lane, Mary Williamson, and Wei-Ning Hsu. Audiobox: Unified audio generation with natural language prompts, 2023. 3 [16] Huadai Liu, Jialei Wang, Rongjie Huang, Yang Liu, Heng Lu, Wei Xue, and Zhou Zhao. Flashaudio: Rectified flows for fast and high-fidelity text-to-audio generation, 2024. 3 [17] Yang Jin, Kun Xu, Liwei Chen, Chao Liao, Jianchao Tan, Bin Chen, Chenyi Lei, An Liu, Chengru Song, Xiaoqiang Lei, et al. Unified language-vision pretraining with dynamic discrete visual tokenization. arXiv preprint arXiv:2309.04669, 2023. 3 [18] Zineng Tang, Ziyi Yang, Mahmoud Khademi, Yang Liu, Chenguang Zhu, and Mohit Bansal. Codi-2: In-context, interleaved, and interactive any-to-any generation. arXiv preprint arXiv:2311.18775, 2023. 3 [19] Jun Zhan, Junqi Dai, Jiasheng Ye, Yunhua Zhou, Dong Zhang, Zhigeng Liu, Xin Zhang, Ruibin Yuan, Ge Zhang, Linyang Li, Hang Yan, Jie Fu, Tao Gui, Tianxiang Sun, Yu-Gang Jiang, and Xipeng Qiu. Anygpt: Unified multimodal llm with discrete sequence modeling. arXiv preprint arXiv:2402.12226, 2024. 3 [20] Yuying Ge, Sijie Zhao, Ziyun Zeng, Yixiao Ge, Chen Li, Xintao Wang, and Ying Shan. Making llama see and draw with seed tokenizer. arXiv preprint arXiv:2310.01218, 2023. [21] Yecheng Wu, Zhuoyang Zhang, Junyu Chen, Haotian Tang, Dacheng Li, Yunhao Fang, Ligeng Zhu, Enze Xie, Hongxu Yin, Li Yi, et al. Vila-u: unified foundation model integrating visual understanding and generation. arXiv preprint arXiv:2409.04429, 2024. 3 [22] Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding and generation. arXiv preprint arXiv:2408.12528, 2024. 3 [23] Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. 3 [24] Zineng Tang, Ziyi Yang, Chenguang Zhu, Michael Zeng, and Mohit Bansal. Any-to-any generation via composable diffusion. arXiv preprint arXiv:2305.11846, 2023. 3, 7, 8 [25] Jiasen Lu, Christopher Clark, Sangho Lee, Zichen Zhang, Savya Khosla, Ryan Marten, Derek Hoiem, and Aniruddha Kembhavi. Unified-io 2: Scaling autoregressive multimodal models with vision, language, audio, and action. arXiv preprint arXiv:2312.17172, 2023. 3, 8 [26] Xubo Liu, Zhongkai Zhu, Haohe Liu, Yi Yuan, Qiushi Huang, Meng Cui, Jinhua Liang, Yin Cao, Qiuqiang Kong, Mark Plumbley, et al. Wavjourney: Compositional audio creation with large language models. IEEE Transactions on Audio, Speech and Language Processing, 2025. 3 [27] Xuenan Xu, Jiahao Mei, Chenliang Li, Yuning Wu, Ming Yan, Shaopeng Lai, Ji Zhang, and Mengyue Wu. Mm-storyagent: Immersive narrated storybook video generation with multi-agent paradigm across text, image and audio. arXiv preprint arXiv:2503.05242, 2025. 3 [28] Tiantian Geng, Teng Wang, Jinming Duan, Runmin Cong, and Feng Zheng. Dense-localizing audio-visual events in untrimmed videos: large-scale benchmark and baseline. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2294222951, 2023. 4 [29] Brandon Castellano. PySceneDetect: Intelligent scene cut detection and video splitting tool, 2018. Accessed: [Insert last accessed date]. 4 [30] Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. 4 [31] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. 4 [32] OpenAI. Addendum to gpt-4o system card: 4o image generation, 2025. Accessed: 2025-04-02. 4 [33] Kevin Kilgour, Mauricio Zuluaga, Dominik Roblek, and Matthew Sharifi. Frechet audio distance: metric for evaluating music enhancement algorithms. arXiv preprint arXiv:1812.08466, 2018. 4, [34] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21(140):167, 2020. 5 [35] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. 6 [36] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. 7 [37] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. Robust speech recognition via large-scale weak supervision. In International conference on machine learning, pages 2849228518. PMLR, 2023. 7, 9 [38] Jort Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, Channing Moore, Manoj Plakal, and Marvin Ritter. Audio set: An ontology and human-labeled dataset for audio events. In 2017 IEEE international conference on acoustics, speech and signal processing (ICASSP), pages 776780. IEEE, 2017. 7 [39] Honglie Chen, Weidi Xie, Andrea Vedaldi, and Andrew Zisserman. Vggsound: large-scale audio-visual dataset. In ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 721725. IEEE, 2020. [40] Andrea Agostinelli, Timo Denk, Zalán Borsos, Jesse Engel, Mauro Verzetti, Antoine Caillon, Qingqing Huang, Aren Jansen, Adam Roberts, Marco Tagliasacchi, et al. Musiclm: Generating music from text. arXiv preprint arXiv:2301.11325, 2023. 7 [41] Luoyi Sun, Xuenan Xu, Mengyue Wu, and Weidi Xie. Auto-acd: large-scale dataset for audio-language representation learning. In Proceedings of the 32nd ACM International Conference on Multimedia, pages 50255034, 2024. 7 [42] Aurora Linh Cramer, Ho-Hsiang Wu, Justin Salamon, and Juan Pablo Bello. Look, listen, and learn more: Design choices for deep audio embeddings. In ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 38523856. IEEE, 2019. 7 [43] Khaled Koutini, Jan Schlüter, Hamid Eghbal-Zadeh, and Gerhard Widmer. Efficient training of audio transformers with patchout. arXiv preprint arXiv:2110.05069, 2021. 7 [44] Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee, and Gunhee Kim. AudioCaps: Generating captions for audios in the wild. In Jill Burstein, Christy Doran, and Thamar Solorio, editors, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 119132, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. [45] Konstantinos Drossos, Samuel Lipping, and Tuomas Virtanen. Clotho: An audio captioning dataset. In ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 736740. IEEE, 2020. 7 [46] Po-Yao Huang, Hu Xu, Juncheng Li, Alexei Baevski, Michael Auli, Wojciech Galuba, Florian Metze, and Christoph Feichtenhofer. Masked autoencoders that listen. Advances in Neural Information Processing Systems, 35:2870828720, 2022. 9 [47] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025."
        },
        {
            "title": "A Implementation Details",
            "content": "We provide detailed hyper-parameters of three training stages in Table 10. In Stage-II and Stage-III, the ratio of generation and understanding samples is 2:1. For LLM, we choose Qwen2.5-VL-3BInstruct and only tune LoRA to avoid overfitting. TangoFlux is employed as the initialization of DiT for audio generation. For the weights of different loss functions, we set the weight of Lmse for T5 regression, Ltext for next-token-prediction and Lflow for DiT as 5, 2 and 1, respectively. Table 10: Detailed hyper-parameters of three training stages. Here, denotes audio, proj. and lr are short for the projector and learning rate. We use 16 GPUs and report the overall batch size. Dimension Task Dataset Stage-I Warm-up Whole Stage-II Stage-III AT5 AT5 with DiT. AT5 with DiT + Und. AT5 with DiT + Und. + Reasoning AudioCaps, WavCaps I+AudioSetCaps (Q&A), VGGSound (Q&A), MusicCaps, Auto-ACD AudioStory-10k Model Training Config Trainable LLM, proj. (Tsemantic) LLM, all proj., DiT LLM, all projectors, DiT LLM, all proj., DiT Frozen Whipser, DiT Whisper Whisper Whisper batch size 512 lr epoch 1e-3 25 25 Gen.: 8, Und.: 16 Gen.: 8, Und.: 16 1e-4 10 LLM (2e-5), DiT (5e-5)"
        },
        {
            "title": "B Qualitative Cases",
            "content": "For all cases, we separately generate multiple single audio clips and concatenate them to constitute the final long-form audio. Instructional long-form audio generation and continuation. First, we present more cases for long-form audio generation. Our model could automatically derive the duration of each audio segment to be generated, as shown in Fig. 7, Fig. 8 and Fig. 9. One could observe that AudioStory could accurately determine the number of events based on the instruction and provide precise descriptions for each audio segment, including both the events themselves and their associated sound effects. Finally, AudioStory can precisely reason out the caption for each segment and generate the corresponding audio clips accordingly. The case of Tom&Jerry. Given either an instruction or video, our model can generate sound that not only aligns with the musical style of the Tom&Jerry cartoon but also vividly reflects the characters emotions and storyline, exhibiting strong generalization capabilities. Video dubbing. Given the video without audio, we first generate the visual captions and convert them into the form of instructional language. These instructions are subsequently fed into our model, i.e., AudioStory, to generate the audio. As whole, we achieve video dubbing in this multi-step process, i.e., videovisual captioninstructionaudio. Here, we provide case of Snoopy. We use our model AudioStory trained for Tom&Jerry. As in Fig. 5, the video is divided into four distinct segments, with the generated audio closely aligning with the Tom&Jerry style, effectively reflecting Snoopys emotions, e.g., the calmness of waking up, the surprise while playing with the water pipes, and the humorous tone at the end. Notably, for any given video, AudioStory could generate soundtracks that match the musical characteristics of Tom&Jerry in zero-shot manner, which is unique and interesting application of our model."
        },
        {
            "title": "C More Explorations of Residual Tokens",
            "content": "For residual tokens, we not only explore their forms and training strategies, but also investigate hyperparameters such as their quantity and fusion methods with semantic tokens. 16 Figure 7: Long-form audio generation case #2. Figure 8: Long-form audio generation case #3. 17 Figure 9: Audio continuation case #2. The number of residual tokens. Here, we study the impact of different numbers of residual tokens, and report both singleand long-form audio generation, as in Table 11. For single-audio generation, too few residual tokens lead to degraded performance. We attribute this to two factors: less low-level complementary information is captured. Additionally, residual tokens help mitigate conflicts between the LLM and the DiT, while too few tokens weaken this effect. Conversely, an excessive number of tokens also degrades performance, because they increase the difficulty for the LLM to regress. Similar patterns could also be observed in the long-form scenario. Overall, 8 residual tokens are most suitable for both single and long audio scenarios. Table 11: Detailed ablations of the number of residual tokens # Tokens"
        },
        {
            "title": "Long Audio",
            "content": "FD FAD KL Consistency 1 4 8 16 4.01 3.64 1.53 3. 5.02 3.95 2.29 3.75 0.93 0.96 0.51 0.94 3.2 3.9 4.1 3.9 Merging mechanism of residual tokens. For the merging mechanism between semantic and residual tokens, we also conduct in-depth explorations. Here, we mainly consider concatenation and cross-attention. The results of long-form audio generation are reported in Fig. 10. From the results, one can observe that compared to concatenation, cross-attention ensures more effective fusion of the two features. Additionally, zero-initializing the final layer of the cross-attention module is necessary to prevent excessive disturbance to the semantic tokens at the beginning of training. 18 Figure 10: Ablations of token merging. Figure 11: Visualizations of residual tokens. What do Residual Tokens Learn? To thoroughly explore the effect of residual tokens, we provide visualizations in Fig. 11 (left). Specifically, the DiT takes only the residual tokens as the input and generates its corresponding audio. We subsequently concatenate all audio clips to constitute the whole long-form audio. The results reveal that for the same audio sample, the residual tokens capture temporally consistent low-level information, primarily reflecting coherence across different audio clips. In contrast, for different samples, the learned residual characteristics vary distinctly. By contrast, semantic tokens learn the underlying global semantics of the input audio and represent the progression of events over time, as illustrated in Fig. 11 (right). AudioStory-10k Benchmark E.1 Dataset construction pipeline The dataset construction pipeline is illustrated as follows. First, we filter videos to select those containing continuous audio events with visually grounded storylines. Next, in the event parsing stage, we use Gemini-2.0-flash to decompose each video into multiple key audio events, each annotated with timestamp, audio caption, and visual caption, as in Fig. 12. Finally, we perform instruction generation: based on fine-grained textual annotations, GPT-4o is used to generate diverse narrative instructions, accompanied by reasoning steps including task decomposition, audio event timeline planning, scene transitions, and emotional tone inference. E.2 Benchmark Construction Dataset prompt. The constructed dataset consists of instructions, reasoning, and audio clips, each with its caption and duration. Specifically, after parsing videos into key audio events using Gemini-2.0-flash as described in Sec. 3, we obtain annotations for each event including timestamps, audio captions, visual captions, and audiovisual event captions. For instruction generation, we use audio-visual event captions as the source input. prompt, shown in Fig. 13, is used to summarize the whole caption of the full audio, which is then incorporated into predefined instruction template to produce the final instruction. For reasoning generation, we provide GPT-4o with the whole caption along with the individual captions for each audio clip. GPT-4o is then prompted to infer the reasoning structure. The reasoning consists of two levels: high-level decomposition indicating how the whole caption can be divided into several parts, followed by detailed descriptions for each part, including the corresponding events and sound-producing content. An example is illustrated in Fig. 14. 19 Figure 12: AudioStory-10k annotation prompts. Benchmark evaluation. Along with the curated dataset, we also construct the long-form narrative audio generation task and its associated benchmark. (1) Evaluation with Gemini-2.0-flash API, assessing consistency, coherence, instruction following, and reasoning logic. (2) Evaluation with traditional metrics to measure audio generation quality, including FD, FAD, and CLAP score, among others. For the Gemini-based evaluation, we design tailored scoring criteria for each metric: (a) Consistency. Timbre and Sonic Cohesion Evaluate whether the primary sound sources maintain generally consistent timbre and unified sonic characteristics. Sound-Producing Entity Consistency Assess whether the implied sound-producing entities remain consistent, or if changes feel natural and logical within the audio. Acoustic Environment Consistency Evaluate the background ambience, reverberation, and spatial impression for overall consistency or reasonable progression. Transition Smoothness Assess whether the transitions between segments are smooth and free of jarring disruptions.. (b) Coherence. Figure 13: The datasets construction prompt. Figure 14: AudioStory-10k cases. Intentional Transitions Check whether transitions between segments are smooth, purposeful, and naturally connected. Dynamic and Emotional Flow Assess if the dynamic and emotional progression feels consistent or evolves logically, without unjustified sudden shifts. 21 Tempo and Textural Compatibility Evaluate whether tempo, rhythm, and sonic textures between segments are compatible and blend cohesively. Transition Smoothness Judge if segment connections are fluid, without abrupt or disjointed (c) Instruction following. Overall Semantic Alignment Evaluate whether the generated audio broadly reflects the intended scene, actions, and atmosphere described in the instruction. Minor differences are acceptable if the main idea remains clear. Key Element Presence Verify whether the important sound-producing entities, actions, and environmental elements mentioned in the instruction are reasonably represented. Missing few non-central elements is acceptable if key parts are present. Additional sounds not specified in the instruction are acceptable if they logically fit the scene and do not disrupt coherence. Event Sequence and Logical Development Assess whether the overall event progression is reasonable according to the instruction. Small deviations in order are acceptable if they do not break the logical flow. Specific Sound Detail Accuracy Evaluate whether important sound features (such as types of sounds, tonal qualities, or intensities) are reasonably reflected. Natural variations are acceptable as long as they do not change the overall character of the audio. (d) Reasoning logic. Overall Reasoning Logic Evaluate whether the model demonstrates coherent, logical process in interpreting the instruction and planning the audio scene. Caption-Instruction Alignment Assess whether the generated audio caption accurately reflects the instructions key content, sound-producing elements, and described environment. Event Coverage Completeness Determine whether the inferred and described audio events fully cover the instructions core elements, with no major omissions. Semantic and Temporal Accuracy Evaluate whether the implied timeline and semantic structure of the generated audio align with the instructions flow and intent. E.3 Single-Audio Evaluation Details To evaluate the audio generation model, four key metrics assess different aspects of performance: Frechet Distance (FD) measures the statistical similarity between log-Mel spectrogram distributions of generated and real audio, quantifying low-level spectral fidelity (e.g., pitch, timbre) through mean and covariance comparisons in the mel-spectral domain. Frechet Audio Distance (FAD) extends FD using high-level embeddings from pre-trained audio encoder (e.g., VGGish), evaluating perceptual and semantic realism by comparing abstract features like instrument timbre, musical structure, and environmental acoustics. CLAP Score calculates the cosine similarity between audio and text embeddings from cross-modal model, assessing how well generated audio aligns with semantic prompts (e.g., textual descriptions of sound content or context). KL-Divergence (KL) measures the distributional dissimilarity between generated and real audio features (spectral, latent, etc.), identifying consistency in probability distributions and helping debug issues like mode collapse or over-dispersion in outputs. Collectively, these metrics ensure comprehensive evaluation of spectral realism, perceptual quality, semantic accuracy, and distributional consistency in generated audio."
        }
    ],
    "affiliations": [
        "ARC Lab, Tencent",
        "MAIS, Institute of Automation, CAS, Beijing",
        "School of Artificial Intelligence, University of Chinese Academy of Sciences"
    ]
}