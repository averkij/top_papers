{
    "paper_title": "MEETING DELEGATE: Benchmarking LLMs on Attending Meetings on Our Behalf",
    "authors": [
        "Lingxiang Hu",
        "Shurun Yuan",
        "Xiaoting Qin",
        "Jue Zhang",
        "Qingwei Lin",
        "Dongmei Zhang",
        "Saravan Rajmohan",
        "Qi Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In contemporary workplaces, meetings are essential for exchanging ideas and ensuring team alignment but often face challenges such as time consumption, scheduling conflicts, and inefficient participation. Recent advancements in Large Language Models (LLMs) have demonstrated their strong capabilities in natural language generation and reasoning, prompting the question: can LLMs effectively delegate participants in meetings? To explore this, we develop a prototype LLM-powered meeting delegate system and create a comprehensive benchmark using real meeting transcripts. Our evaluation reveals that GPT-4/4o maintain balanced performance between active and cautious engagement strategies. In contrast, Gemini 1.5 Pro tends to be more cautious, while Gemini 1.5 Flash and Llama3-8B/70B display more active tendencies. Overall, about 60\\% of responses address at least one key point from the ground-truth. However, improvements are needed to reduce irrelevant or repetitive content and enhance tolerance for transcription errors commonly found in real-world settings. Additionally, we implement the system in practical settings and collect real-world feedback from demos. Our findings underscore the potential and challenges of utilizing LLMs as meeting delegates, offering valuable insights into their practical application for alleviating the burden of meetings."
        },
        {
            "title": "Start",
            "content": "MEETING DELEGATE: Benchmarking LLMs on Attending Meetings on Our Behalf Lingxiang Hu 1* Shurun Yuan 2* Xiaoting Qin 3 Jue Zhang 3 Qingwei Lin 3 Dongmei Zhang 3 Saravan Rajmohan 3 Qi Zhang 1Northeastern University, China 2Peking University, China 3Microsoft"
        },
        {
            "title": "Abstract",
            "content": "In contemporary workplaces, meetings are essential for exchanging ideas and ensuring team alignment but often face challenges such as time consumption, scheduling conflicts, and inefficient participation. Recent advancements in Large Language Models (LLMs) have demonstrated their strong capabilities in natural language generation and reasoning, prompting the question: can LLMs effectively delegate participants in meetings? To explore this, we develop prototype LLM-powered meeting delegate system and create comprehensive benchmark using real meeting transcripts. Our evaluation reveals that GPT-4/4o maintain balanced performance between active and cautious engagement strategies. In contrast, Gemini 1.5 Pro tends to be more cautious, while Gemini 1.5 Flash and Llama3-8B/70B display more active tendencies. Overall, about 60% of responses address at least one key point from the ground-truth. However, improvements are needed to reduce irrelevant or repetitive content and enhance tolerance for transcription errors commonly found in real-world settings. Additionally, we implement the system in practical settings and collect real-world feedback from demos. Our findings underscore the potential and challenges of utilizing LLMs as meeting delegates, offering valuable insights into their practical application for alleviating the burden of meetings."
        },
        {
            "title": "Introduction",
            "content": "Nowadays, the nature of work has increasingly become more collaborative (Mugayar-Baldocchi et al., 2021), with meetings becoming an essential component (Spataro, 2020) to facilitate the exchange of ideas and information, fostering innovation and ensuring alignment among team members. Attending meetings, however, poses notable difficulties. Firstly, the rapid increase in the number *Equal contribution. Work is done during an internship at Microsoft. Corresponding author. of meetings can consume substantial amount of time, diverting attention from core tasks and reducing overall productivity (Perlow et al., 2017; Kost, 2020). Secondly, scheduling conflicts often arise when multiple meetings are double-booked, forcing participants to prioritize or miss valuable discussions altogether. Thirdly, not all meetings require full attendance; participants may only need to contribute to specific topics, leading to inefficiencies when attendees are required for entire duration. In this study, we investigate the feasibility of developing meeting delegate system to represent individuals in meetings. This concept is becoming increasingly viable with the advancement of Large Language Models (LLMs). These LLMs, renowned for their remarkable capabilities in natural language understanding and generation (Ouyang et al., 2022; OpenAI, 2023; Google, 2024a), demonstrate potential to comprehend meeting context, participate in dynamic conversations, and provide informed responses. Developing LLM-powered meeting delegate systems faces several challenges. Firstly, such systems must navigate complex, context-rich conversations involving multiple participants, requiring them to discern opportune moments for engagement and restraint. Secondly, human conversations often contain ambiguities and uncertainties, such as queries directed ambiguously or pronunciation-related ambiguities, which challenge the systems ability to respond effectively. Thirdly, ensuring user privacy is crucial to prevent over-sharing of information and safeguard the users personal image. Finally, these systems must operate in real-time, necessitating low-latency responsiveness. In this work we aim to develop prototype of an LLM-powered meeting delegate system to address the above challenges, focusing initially on the first two while leaving the last two in the future work. To assess its effectiveness across various LLMs, we conduct real-world testing in few demo scenarios 5 2 0 2 5 ] . [ 1 6 7 3 4 0 . 2 0 5 2 : r and construct an evaluation dataset from real meeting transcripts. In contrast to recent studies that emphasize the facilitator role in meeting engagement (Mao et al., 2024), our work concentrates on the participant role, which is more prevalent and distinct from that of the facilitator. Our evaluation reveals that GPT-4/4o maintain balanced performance between active and cautious engagement strategies, while Gemini 1.5 Pro is more cautious, and Gemini 1.5 Flash and Llama38B/70B are more active. Overall, 60% of responses address at least one main point from the ground-truth, showing the promise of adopting LLM-powered meeting delegates, while improvements are needed, such as to enhance tolerance for transcription errors. Our contributions are summarized as follows: We develop prototype of an LLM-powered meeting delegate system designed to participate in meetings on our behalf, with particular focus on the role of the participant. We create comprehensive benchmark based on real meeting transcripts, covering four common scenarios: Explicit Cue, Implicit Cue, Chime In, and Keep Silence. We plan to release the benchmark dataset with the paper. We evaluate the performance of popular LLMs through some demo scenarios and rigorous assessment using the benchmark. This includes an ablation study on the impact of transcription errors commonly encountered in practice."
        },
        {
            "title": "2 Related Work",
            "content": "Language Model Applications in Meetings. Considerable research has been dedicated to the summarization of meetings (Zhong et al., 2021) and other real-life dialogues (Mehdad et al., 2014; Tuggener et al., 2021). In the context of meetings, key tasks include meeting transcript summarization and action item identification (Cohen et al., 2021). MeetingQA (Prasad et al., 2023) investigated Q&A tasks based on meeting transcripts, highlighting the challenges faced by models such as RoBERTa in handling real-world meeting data. Recent advancements in LLMs have opened new avenues for enhancing these tasks. For instance, an LLM-based meeting recap system (Asthana et al., 2023) has demonstrated effectiveness in generating accurate and coherent summaries and action items. Facilitator in Multi-Participant Chat. MUCA (Mao et al., 2024) presents framework that leverages LLMs to facilitate group chats by simulating users, demonstrating notable effectiveness in goaloriented conversations. Similarly, approaches like GPT-4o demo for meetings (OpenAI, 2024a) are designed to serve as facilitators in group discussions. While these studies underscore LLMs capabilities in managing group chats, they primarily focus on LLMs guiding the meeting process rather than representing individuals with different roles. Role-Playing with LLMs: Characters and Digital Twins. Role-play prompting (Kong et al., 2024) has been shown to be more effective trigger for the chain-of-thought process in LLMs. Additionally, efforts to simulate famous personalities (Shao et al., 2023; Sun et al., 2024) have garnered interest, leading to research on maintaining character consistency and studying social interactions within agentbased group chat environments. Recently, Reid Hoffman (Hoffman, 2024) showcased an interview between himself and his digital twin built on GPT-4. Although this demonstration highlighted the potential of digital representations, it was confined to one-on-one interactions, leaving the complexities of group discussions unexplored. Unlike previous work, our work focus on LLMs as meeting participant delegates, delivering targeted engagement tailored to multi-participant, meeting-specific objectives. Our comprehensive evaluation and realworld deployment further demonstrate the systems potential to significantly reduce the burden of meetings on individuals, thereby advancing the application of LLMs in professional environments."
        },
        {
            "title": "3 LLM-based Meeting Delegate System",
            "content": "Figure 1: Architecture of the meeting delegate system. Figure 1 illustrates the architecture of our proposed meeting delegate system, which comprises three main components: Information Gathering: This component shown on the top-left collects meeting-related information to assist LLMs in participating in meetings. Users can manually provide topics of interest, background knowledge, and shareable materials prior to the meeting. Alternatively, if the user has personal knowledge base or an intelligent personal assistant/agent, the system can query them in real-time, provided latency is manageable. Meeting Engagement: The Meeting Engagement module actively monitors the meetings status and uses LLMs to determine the appropriate timing and content for engagement. Engagement evaluation occurs after each participants utterance, using in-context learning methods. The prompt for evaluation includes general instructions, user-provided meeting information, and the ongoing meeting context (see Table 14 in the Appendix for details). While various contextual data (e.g., transcript, screen sharing, audio) can be utilized, this work focuses on transcripts obtained via meeting software or speech-to-text tools. Figure 1 shows the three common response types: leading the discussion, responding to others, and chiming in. This work concentrates on the latter two, emphasizing the participants role. Voice Generation: After deciding on the content to be spoken, the Voice Generation module shown on the right produces voice response mimicking the users voice using text-to-speech (TTS) technology (Qin et al., 2023). To minimize latency, the system employs streaming modes for both LLM API calls and TTS. We implemented prototype of the above system on widely-used meeting platform and conducted several demo case studies.1 Detailed insights and lessons learned from these real-world applications will be presented in Section 6.2. As an illustration, we present real demo case in Figure 2. In this example, Bob uses his Meeting Delegate to participate in meeting with Alice and others. Before the meeting, Bob provides topics of interest and relevant shareable information to the Meeting Delegate 1 . This information, along with instructions, forms the prompt for the Meeting Delegate 2 . The delegate then joins the meeting 3 and determines, based on the ongoing meeting transcript, whether to engage 4 . During the meeting, Alice discusses updates on the voice function, which aligns with Bobs goal to learn about its progress. The Meeting Delegate then chimes in 5 , generating text-based response (converted to speech 6 ), asking for more 1Omitting the platform name for anonymity. details, thus achieving Bobs objectives and engaging in the conversation."
        },
        {
            "title": "4 Benchmark Dataset",
            "content": "While the proposed meeting delegate system demonstrates potential in few sample demonstrations, more systematic and quantitative evaluation in diverse contexts is needed. Our evaluation goals are to determine whether the system can appropriately time its interventions and generate relevant spoken content. No existing benchmark datasets meet these objectives, prompting us to create one."
        },
        {
            "title": "4.1 Dataset Construction",
            "content": "Our dataset construction strategy involves using real meeting transcripts and generating test cases by taking snapshots from these transcripts. snapshot is defined as truncation of the transcript after participants utterance. Then, by comparing the generated response according to this snapshot with the actual responses in the real script, we can determine how well the system performs. An illustration of this process in given in Figure 8. Our base meeting transcripts are taken from the ELITR Minuting Corpus (Nedoluzhko et al., 2022), comprising de-identified project meeting transcripts in English and Czech. 61 English meeting transcripts are used and the test cases are constructed as follows. Motivated by promising results from LLM annotation (Gilardi et al., 2023), we also leverages LLMs for preparing this dataset while conducting manual verification for quality assurance. Specifically, we first employ GPT-4 to progressively analyze each participants utterances by taking sliding window on the original meeting transcript. This is to capture their meeting intents and the information that they can share during the meeting, serving as the critical input to the Meeting Engagement module for response generation. The shareable meeting information contains pairs of <Context> and <Information>, with <Context> specifying under which context the points in <Information> can be shared. Details of this intent and contextual information extraction prompt can be found in Table 21 in the Appendix. Next, we extract suitable snapshots from the transcripts as test cases. For each participant (excluding facilitators), we identify their utterances and use the preceding transcript as the ongoing meeting context. The ground-truth response is determined by considering several subsequent utterances. This Figure 2: Workflow of an LLM-powered meeting delegate system. The process involves user input of meeting intent and shareable information prior to the meeting, real-time participation based on meeting transcripts, and response generation aligned with prompted instructions and meeting objectives. extraction process leverages GPT-4 (prompt in Table 27), which classifies the meeting scenes (Explicit Cue, Implicit Cue, and Chime In) and selects the necessary utterances to form the ground-truth response, recognizing that users response may span multiple subsequent utterances. To ensure accuracy, the extracted cases are manually verified by two authors. As the extracted test cases closely match real transcripts, we refer them as the Matched Dataset. To evaluate the meeting delegates ability to remain silent when inappropriate to speak, we construct Mismatched Dataset from the Matched Dataset. We take Explicit Cue and Implicit Cue test cases and replace the principal who needs to respond with another participant not involved in the current conversation. The intents and shareable meeting information are accordingly replaced, and the ground-truth is set to be empty. The delegate representing the new principal is expected to remain silent when presented with these transcripts. Lastly, we construct Noisy Name Dataset for our ablation study, addressing the fact that meeting transcribing systems often introduce noise affecting the meeting delegates performance. This issue is particularly significant for recognizing names, which are crucial in Explicit Cue cases. For example, the Chinese name Jisen might be transcribed as Jason. In our construction, we modify the Explicit Cue cases by replacing de-identified names with real-world names and substituting the principals name in the final utterance with phonetically similar word to simulate transcription errors."
        },
        {
            "title": "4.2 Evaluation Metric",
            "content": "In our evaluation, we generate responses using LLMs with the same prompt as in our prototype. These responses are assessed using two categories of metrics: Response Rate / Silence Rate, which determines whether response is generated, and quality-related metrics, Recall and Attribution. The Recall metric evaluates if the generated response includes key points present in the groundtruth response. We define two recall rates: loose recall rate, which is 1 if at least one main point from the ground-truth is mentioned and 0 otherwise; and strict recall rate, which measures the percentage of main points from the ground-truth included in the generated response. Attribution assesses the origin of the main points in the generated response, classifying them into four categories: the expected ground-truth response (Expected Response), contextual information not present in the ground-truth (Contextual Information), previous transcript content (Previous Transcript), and hallucinated texts (Hallucination). cases, participants contribute over ten main points. This indicates substantial level of detail and interaction within the meetings, suggesting that the dataset captures rich and multifaceted discussions. We leverage LLMs for main point extraction and their semantic comparison. Specifically, in the Recall phase, GPT-4 is employed to assess how well the LLM-generated responses match key points from the ground-truth response set, using the prompt provided in Table 17. In the Attribution phase, GPT-4 Turbo is used to trace and evaluate the origin of specific points in the responses, with the prompt provided in Table 19. Through manual validation of 30 randomly sampled cases, we found that LLMs achieved an average of 93.3% accuracy on these Recall and Attribution evaluation tasks, supporting their use in our experiments."
        },
        {
            "title": "5 Experiment",
            "content": "Setup. In our experiment, we utilize three promithe GPT series (GPTnent series of LLMs: 3.5-Turbo, GPT-4, GPT-4o) (OpenAI, 2024c), the Gemini series (Gemini 1.5 Flash, Gemini 1.5 Pro) (Google, 2024b) and the Llama series (Llama3-8B, Llama3-70B) (Meta, 2024). For all LLMs2, we set the temperature to 0 and use the default API settings for other parameters. Note that, due to model context window restriction, we remove test cases that exceed the 8K context window for Llama3 models (56.3% kept) and those exceeding the 16K context window for GPT-3.5Turbo (94.3% kept), while keeping all for the other LLMs. Figure 3: Data statistics of the Matched Dataset. From the 61 original meeting transcripts, we extract 846 test cases for Matched Dataset, in which 54.5% belongs to Implicit Cue, followed by 30.9% for Explicit Cue and 14.7% for Chime In. The numbers of test cases for Mismatched Dataset and Noisy Name Dataset are 294 and 122, respectively. For Matched Dataset, we present various data statistics in Figure 3. Over 50% of test cases involve more than four participants and contain transcripts exceeding 50 utterances, highlighting the datasets complexity and the involvement of multiple individuals. Additionally, approximately 40% of test cases include at least two main points in the ground-truth response, and in more than 50% of Figure 4: Response Rate on Matched Dataset vs. Silence Rate on Mismatched Dataset. Response Rate Analysis. The Response and Silence Rates of the studied LLMs are obtained for Matched and Mismatched Datasets, respectively. Summarized results are presented in Figure 4, with further details (e.g., breaking down to different meeting scenes) provided in Tables 2 and 4 in the Appendix. Overall, GPT-4 and GPT-4o demonstrated balanced performance, with Response/Silence Rates between 0.7 and 0.8. Among the Gemini series models, Gemini 1.5 Pro achieved the highest Silence Rate of approximately 0.9, coupled with low Response Rate, indicating cautious engagement strategy. In contrast, the smaller Gemini 1.5 Flash model and the Llama series exhibited 2Exact model versions can be found in Table 13. higher activity levels, suggesting more proactive engagement approach; however, this also led to tendency to engage when they should remain silent. These patterns persisted when all LLMs are tested using the same subset of cases as the Llama series. Figure 5: Solution directions from error analysis of bad cases in Response (Silence) Rate for Matched and Mismatched Datasets. To uncover the underlying causes of failures, we conduct an in-depth analysis of all failure cases in representative models: GPT-4o and Gemini 1.5 Pro for state-of-the-art LLMs, and Gemini 1.5 Flash and Llama3-8B representing more lightweight models. We manually analyze and categorize all error types, proposing corresponding directions for improvement. For instance, in the \"Explicit Cue\" scenario within the Matched Dataset, the meeting delegate may correctly identify the cue but fail to respond, indicating need for enhanced reasoning capabilities in meeting contexts. Detailed analysis can be found in Table 12 and Figure 9 in Appendix. summary of these results is presented in Figure 5. Our findings reveal that: 1) LLMs like GPT-4o and Gemini 1.5 Pro can improve performance or make functional advancements in meeting scenarios by enhancing reasoning in meeting-specific context, and 2) smaller models need to improve general instruction following and reasoning abilities before addressing meetingspecific issues. Recall Analysis. The recall results for both loose and strict metrics are similar; therefore, we only present the loose recall rate for all studied LLMs on Matched Dataset in Figure 6. Detailed results, including the strict recall rate, are available in Table 6 in the Appendix. Figure 6 shows that these LLMs achieve loose recall rate of approximately 60%. This indicates that, for 60% of test cases, the generated response contains at least one key point from the ground-truth response. Such result is promising, as it suggests that LLM-powered meeting deleFigure 6: Loose recall rate on Matched Dataset. gates can typically respond with reasonable content, maintaining the overall meeting flow. Performance differences among the LLMs reveal that GPT-4o achieves the highest performance across almost all categories, followed by GPT-4. The two Gemini models exhibit similar performance, excelling in Explicit Cue but lagging in Chime In. The Llama series models perform comparably to the Gemini models but tend to be better in Chime In scenarios. Figure 7: The attribution rate on matched dataset. Attribution Analysis. For the Attribution metric, we seek high percentage of Expected Response, indicating high accuracy in responding to given cues, while minimizing other categories, particularly Hallucination. As shown in Figure 7, most models, except GPT-3.5-Turbo and Llama38B, have approximately 40% of their responses attributable to the ground-truth response, with Gemini 1.5 Pro achieving the highest performance at around 50%. About 30% of generated responses are attributed to other input context information not directly related to the ground-truth response, indicating room for improvement in reasoning over the provided information. The proportion attributed to the previous transcript varies significantly across models, ranging from 10% to 30%. Higher values suggest repetitive messages in the generated response, potentially detracting from the meeting experience due to verbosity. The portion of hallucinated texts is minimal, at only 5% across all models, indicating that current LLMs maintain good trustworthiness in meeting engagement. Regarding performance differences across models, we observe that models generally considered more capable demonstrate better performance, while models like GPT-3.5-Turbo and Llama38B, viewed as less capable, show inferior performance. This alignment between general model performance and specific scenarios suggests that in future, more capable general LLMs will also benefit meeting delegate scenarios. Correlation Analysis. We correlate the performance of the above metrics with test case metadata (i.e., those shown in Figure 3). Figure 10 in the Appendix presents an example result for GPT-4o. The result indicates that GPT-4o maintained stable performance across different transcript lengths and complexity measures, including meeting size and input diversity. Therefore, no significant relationships between the evaluation metrics and the metadata were observed. Ablation Study. Two scenarios are considered in our ablation studies. First, we examine the impact of erroneous transcription of participant names to phonetically similar words using the Noisy Name Dataset. We measure the response rates of all models on this dataset, observing significant drop in performance (see Table 10 in the Appendix). For instance, GPT-4os response rate declines from 94.3% in the Explicit Cue cases of the Matched Dataset to 68% in the Noisy Name Dataset. This highlights challenges in accurately recognizing participant names. Further model fine-tuning to better handle such transcription errors may be necessary. In our second study, we investigate how model performance is affected by the provision of context information in the input. Currently, context information is structured as pairs of <Context> and <Information>, specifying under which conditions the information in <Information> can be shared. This setup may not reflect real-world scenarios where users might not always anticipate the context for sharing specific information. To assess the impact, we remove <Context> from test cases and use <Information> and <Intents> alone as input to generate responses. We evaluate this on subset of 121 test cases from the first 11 meetings using GPT-4o. Detailed results are provided in Table 11 in the Appendix, showing minimal performance impact across all evaluation metrics when context information is omitted."
        },
        {
            "title": "6.1 Phased Deployment of Meeting Delegate",
            "content": "This study primarily explores the feasibility of using LLMs to represent users by generating meaningful content in meeting scenarios. However, deploying such meeting delegate system in realworld settings requires addressing additional critical factors, such as responsible AI practices and ethical considerations (see further discussion in the Ethics Statement section). Key challenges include implementing strong privacy safeguards, such as secure data handling, consent mechanisms, userdefined boundaries, and audit trails. review (Yan et al., 2024; Anwar et al., 2024) of current privacypreserving methods for LLMs highlights the difficulty of creating fully autonomous and unconstrained meeting delegate at present. Therefore, we propose three-phase approach that incrementally enhances the AIs autonomy and responsibility, as detailed in Table 1. The phases are characterized by the evolution of data boundaries and limitations on the meeting delegates roles in sharing information, collecting data, and making decisions. In Phase (Execute), the delegate operates strictly within user-defined data boundaries, sharing only explicitly approved information and collecting information from other meeting participants based on direct user instructions. There is no autonomous decision-making allowed, ensuring strong user control and minimal privacy risk. In Phase II (Assist), the system can reason over sensitive data while adhering to privacy guidelines. It infers context beyond explicit instructions and can propose actions, though user approval is still required for making decisions. This phase introduces controlled autonomy with dynamic data boundary management. In Phase III (Delegate), the delegate fully autonomously collects and shares information, making real-time decisions based on user-defined goals and preferences. Privacy filters, decisionmaking models, and audit logs ensure transparency Table 1: Progression of Autonomy and Responsibility in Achieving Fully Autonomous Meeting Delegate. Data Boundary Share Information Phase I: Execute User-defined boundaries Only within user-defined boundaries Collect Information Explicit requests only Decision-Making No decision-making Phase II: Assist Privacy-protected boundaries Some reasoning over sensitive data Infer context beyond user instructions Propose and ask for approval Phase III: Delegate Data accessible by user Autonomous based on predefined goals and preferences Autonomously collects and reasons based on meeting context Full autonomous decision-making and accountability, with the system acting independently on behalf of the user. This phased approach enables the delegate to transition from controlled executor to fully autonomous agent, balancing privacy and increasing decision-making capability while ensuring transparency and accountability. While our ultimate goal is to achieve Phase III for significantly reducing meeting-related burdens, implementing meeting delegate system in earlier phases may already benefit certain situations. For instance, Phase delegate system might be employed in daily project update scrums, where delegate would present updates and gather progress from team members for alignment. Although one could argue that such objectives could be accomplished asynchronously via offline progress updates, deploying an early-stage system is still beneficial. It allows us to gain practical experience that will inform future advancements toward the systems full potential. Additionally, phased deployment familiarizes users with the technology, helping to identify overlooked issues and challenges."
        },
        {
            "title": "6.2 Application in Practice",
            "content": "Our current implementation of the meeting delegate system indeed corresponds to Phase I, consistent with available technologies. To assess its practical performance, the system was tested in several demo scenarios. For example, as shown in Figure 2, we simulated demo scenario of daily project update scrum with three human participants and one LLM-powered delegate. All participants were aware of the delegates presence and located in the same room. One participant acted as the moderator, while the others, including the delegate, provided project updates. Each human participant followed script, requesting information from the delegate, which was preloaded with project-related topics via the Information Gathering module. The moderator guided the meeting, with responses cued or initiated by the participants. The demo lasted five minutes and was repeated to assess the delegates consistency using different LLMs. We evaluated three models: GPT-3.5-Turbo, GPT-4, and GPT-4o. GPT-3.5-Turbo underperformed, proving inadequate for meeting delegation tasks, even at Phase I. GPT-4 and GPT-4o generally delivered relevant responses but occasionally repeated information from earlier transcripts. Response latency was another issue, with the fastest model, GPT-4o, taking 5 seconds to respond. To address issues of irrelevant and repetitive responses, future improvements may include utilizing advanced general LLMs or fine-tuning smaller models. Benchmark results indicate that the Llama3-8B model exhibits satisfactory base performance, and fine-tuning smaller models could potentially reduce latency. For instance, recent implementation of Llama3-8B achieved 500 ms latency in real-time communication (Cerebrium, 2024). Other improvements, such as incorporating windowed context management, advanced summarization techniques, or adopting multi-modal language models with direct speech input and output capabilities (OpenAI, 2024b), have the potential to not only further reduce latency but also maintain or improve performance. For example, the added information from speech, such as speed and tone, could lead to enhanced system performance."
        },
        {
            "title": "Limitations",
            "content": "We acknowledge several limitations in our study. First, the evaluation is restricted to set of representative language models. While this provides valuable insights, future work should explore broader range of LLMs, particularly models specifically fine-tuned for meeting-related tasks. Additionally, recent advancements such as OpenAIs Realtime API (OpenAI, 2024b), which supports direct voice input and output, could significantly enhance the relevance of our findings in multimodal contexts. Second, our benchmark is largely based on limited experimental conditions. Future evaluations should incorporate more diverse and dynamic environments to provide more comprehensive understanding of our systems capabilities. Lastly, while our system shows promise in facilitating meeting participation, it represents an initial exploration of the possibility of using LLMs as meeting delegates. Specifically, it does not extensively address other key dimensions such as privacy, security, or user trust. In the following section, we share an initial discussion on responsible AI and ethics consideration to outline potential directions for further investigation."
        },
        {
            "title": "Ethics Statement",
            "content": "This paper explores the potential use of LLMs as meeting delegates, raising several ethical considerations. We propose phased approach to AI autonomy, starting with limited decision-making in earlier phases and building toward greater capabilities with accountability measures. Privacy-bydesign principles should be central to the systems architecture, and educating users about the AIs limitations will ensure responsible use. Below, we outline key ethical dimensions (Bender et al., 2021; Kasneci et al., 2023; Wang et al., 2024; Kirk et al., 2024), including bias, privacy, transparency, human agency, security, and socio-economic impact, alongside suggested safeguards. Bias and Fairness: LLMs may generate biased or inappropriate content, potentially affecting fairness in meeting outcomes. This risk requires bias detection and mitigation strategies, such as training on diverse datasets, bias audits, and user feedback loops. Fine-tuning models for meeting scenarios and ongoing bias monitoring could be crucial for ensuring fairness. Privacy: Personalization is only possible by collecting user data. This applies to any technology that relies on personal information to deliver tailored benefits. The personalization of meeting delegates relies on sensitive user data, which risks oversharing or misusing private information. To address this, we advocate for privacy-enhancing technologies like encryption and differential privacy, as well as user-defined data boundaries. Real-time voice capabilities also heighten the risk of identity misuse, necessitating strict privacy controls to ensure compliance with data protection standards. Transparency: Transparency is essential for responsible deployment. All participants must be informed when an AI is acting as delegate. Clearly stating the AIs capabilities and limitations helps manage expectations, and audit logs should be available for users to track AI actions and decisions during meetings. Human Agency: LLM-based delegates should support, not replace, human decision-making. In the early phases, the AI assists users without autonomy, and even in later phase like Phase III, human oversight must remain integral. Human-in-the-loop HITL systems are crucial for maintaining control and ensuring users can intervene as needed. Security and Fraud Risks: Unauthorized access to meeting delegate could lead to fraud or impersonation. Security measures like multi-factor authentication, identity verification, and anomaly detection are essential. Federated learning could further protect sensitive data by minimizing centralized storage risks. Ethical Governance and Mitigation: Ethical governance frameworks, including guidelines, audits, and interdisciplinary collaboration, must guide the systems development. User consent should be obtained at key stages, and continuous monitoring is essential to identify and address unintended consequences. Socio-Economic Impact: Automating meeting participation could lead to job displacement in roles that rely on meeting facilitation. While this risk is limited by current technology, future developments may amplify these concerns. Its essential to focus on augmenting human labor rather than replacing."
        },
        {
            "title": "References",
            "content": "Usman Anwar, Abulhair Saparov, Javier Rando, Daniel Paleka, Miles Turpin, Peter Hase, Ekdeep Singh Lubana, Erik Jenner, Stephen Casper, Oliver Sourbut, Benjamin L. Edelman, Zhaowei Zhang, Mario Günther, Anton Korinek, Jose Hernandez-Orallo, Lewis Hammond, Eric Bigelow, Alexander Pan, Lauro Langosco, Tomasz Korbak, Heidi Zhang, Ruiqi Zhong, Seán Ó hÉigeartaigh, Gabriel Recchia, Giulio Corsi, Alan Chan, Markus Anderljung, Lilian Edwards, Aleksandar Petrov, Christian Schroeder de Witt, Sumeet Ramesh Motwan, Yoshua Bengio, Danqi Chen, Philip H. S. Torr, Samuel Albanie, Tegan Maharaj, Jakob Foerster, Florian Tramer, He He, Atoosa Kasirzadeh, Yejin Choi, and David Krueger. 2024. Foundational challenges in assuring alignment and safety of large language models. Preprint, arXiv:2404.09932. Sumit Asthana, Sagih Hilleli, Pengcheng He, and Aaron Halfaker. 2023. Summaries, highlights, and action items: Design, implementation and evaluation of an llm-powered meeting recap system. Preprint, arXiv:2307.15793. Emily M. Bender, Timnit Gebru, Angelina McMillanMajor, and Shmargaret Shmitchell. 2021. On the dangers of stochastic parrots: Can language models be too big? In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, FAccT 21, page 610623, New York, NY, USA. Association for Computing Machinery. Cerebrium. 2024. Fast voice agent. Accessed: 2024-0919. Amir Cohen, Amir Kantor, Sagi Hilleli, and Eyal Kolman. 2021. Automatic rephrasing of transcriptsbased action items. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 28622873, Online. Association for Computational Linguistics. Fabrizio Gilardi, Meysam Alizadeh, and Maël Kubli. 2023. Chatgpt outperforms crowd workers for text-annotation tasks. Proceedings of the National Academy of Sciences, 120(30):e2305016120. Google. 2024a. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. Preprint, arXiv:2403.05530. Google. 2024b. Gemini models. Accessed: 2024-0918. Reid Hoffman. 2024. Reid hoffman meets his ai twin - full. Accessed: 2024-09-18. Enkelejda Kasneci, Kathrin Sessler, Stefan Küchemann, Maria Bannert, Daryna Dementieva, Frank Fischer, Urs Gasser, Georg Groh, Stephan Günnemann, Eyke Hüllermeier, Stephan Krusche, Gitta Kutyniok, Tilman Michaeli, Claudia Nerdel, Jürgen Pfeffer, Oleksandra Poquet, Michael Sailer, Albrecht Schmidt, Tina Seidel, Matthias Stadler, Jochen Weller, Jochen Kuhn, and Gjergji Kasneci. 2023. Chatgpt for good? on opportunities and challenges of large language models for education. Learning and Individual Differences, 103:102274. Hannah R. Kirk, Bertie Vidgen, Paul Röttger, et al. 2024. The benefits, risks and bounds of personalizing the alignment of large language models to individuals. Nature Machine Intelligence, 6:383392. Aobo Kong, Shiwan Zhao, Hao Chen, Qicheng Li, Yong Qin, Ruiqi Sun, Xin Zhou, Enzhi Wang, and Xiaohang Dong. 2024. Better zero-shot reasoning with In Proceedings of the 2024 role-play prompting. Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 40994113, Mexico City, Mexico. Association for Computational Linguistics. Danielle Kost. 2020. Youre right! you are working longer and attending more meetings. Accessed: 2024-09-18. Manqing Mao, Paishun Ting, Yijian Xiang, Mingyang Xu, Julia Chen, and Jianzhe Lin. 2024. Multiuser chat assistant (muca): framework using llms to facilitate group conversations. Preprint, arXiv:2401.04883. Yashar Mehdad, Giuseppe Carenini, and Raymond T. Ng. 2014. Abstractive summarization of spoken and written conversations based on phrasal queries. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 12201230, Baltimore, Maryland. Association for Computational Linguistics. Meta. 2024. Meta llama. Accessed: 2024-09-18. Marino Mugayar-Baldocchi, Bill Schaninger, and Kartik Sharma. 2021. The future of the workplace: Embracing change and fostering connectivity. Accessed: 2024-09-18. Anna Nedoluzhko, Muskaan Singh, Marie Hledíková, Tirthankar Ghosal, and Ondrej Bojar. 2022. Elitr minuting corpus: novel dataset for automatic minuting from multi-party meetings in english and In International Conference on Language czech. Resources and Evaluation. OpenAI. 2023. Gpt-4 technical report. Preprint, arXiv:2303.08774. OpenAI. 2024a. Hello gpt-4o. Accessed: 2024-09-18. OpenAI. 2024b. Introducing the realtime api. Accessed: 2024-10-15. OpenAI. 2024c. Models. Accessed: 2024-09-18. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback. In Advances in Neural Information Processing Systems, volume 35, pages 2773027744. Curran Associates, Inc. Leslie A. Perlow, Constance Noonan Hadley, and Eunice Eun. 2017. Stop the meeting madness. Accessed: 2024-09-18. Archiki Prasad, Trung Bui, Seunghyun Yoon, Hanieh Deilamsalehy, Franck Dernoncourt, and Mohit Bansal. 2023. MeetingQA: Extractive questionanswering on meeting transcripts. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1500015025, Toronto, Canada. Association for Computational Linguistics. Zengyi Qin, Wenliang Zhao, Xumin Yu, and Xin Sun. 2023. Openvoice: Versatile instant voice cloning. Preprint, arXiv:2312.01479. Yunfan Shao, Linyang Li, Junqi Dai, and Xipeng Qiu. 2023. Character-LLM: trainable agent for roleplaying. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 1315313187, Singapore. Association for Computational Linguistics. Jared Spataro. 2020. Remote work trend report: meetings. Accessed: 2024-09-18. Chenkai Sun, Ke Yang, Revanth Gangi Reddy, Yi R. Fung, Hou Pong Chan, ChengXiang Zhai, and Heng Ji. 2024. Persona-db: Efficient large language model personalization for response prediction with collaborative data refinement. Preprint, arXiv:2402.11060. Don Tuggener, Margot Mieskes, Jan Deriu, and Mark Cieliebak. 2021. Are we summarizing the right way? In survey of dialogue summarization data sets. Proceedings of the Third Workshop on New Frontiers in Summarization, pages 107118, Online and in Dominican Republic. Association for Computational Linguistics. Angelina Wang, Jamie Morgenstern, and John P. Dickerson. 2024. Large language models should not replace human participants because they can misportray and flatten identity groups. Preprint, arXiv:2402.01908. Biwei Yan, Kun Li, Minghui Xu, Yueyan Dong, Yue Zhang, Zhaochun Ren, and Xiuzhen Cheng. 2024. On protecting the data privacy of large language models (llms): survey. Preprint, arXiv:2403.05156. Ming Zhong, Da Yin, Tao Yu, Ahmad Zaidi, Mutethia Mutuma, Rahul Jha, Ahmed Hassan Awadallah, Asli Celikyilmaz, Yang Liu, Xipeng Qiu, and Dragomir Radev. 2021. QMSum: new benchmark for querybased multi-domain meeting summarization. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 59055921, Online. Association for Computational Linguistics."
        },
        {
            "title": "A Dataset Construction",
            "content": "An example of evaluation dataset construction is shown in Figure 8. In the meeting transcript, participants are represented by different ID numbers and icons. Each utterance is displayed in colored boxes, with each color representing different participant. In this example, we construct test case with Participant 6 as the principal. Based on Participant 6s utterances in the Original Transcript, we extract one piece of Input Context Information: when the meeting discusses expertise in emotion detection, Participant 6 intends to mention related experience from bachelor thesis. The Transcript Snapshot and Ground-Truth Response are extracted from the Original Transcript using GPT-4. During the response generation stage with the meeting delegate, the Transcript Snapshot is provided to the LLMs to produce response. This generated response is subsequently assessed by comparing it to the Ground-Truth Response."
        },
        {
            "title": "We plan to release our constructed benchmark",
            "content": "dataset with the paper."
        },
        {
            "title": "B Additional Experimental Results",
            "content": "In this section, we provide detailed tables and additional plots for the experimental results discussed in Section 5. Response Rate Analysis. Tables 2 and 4 present the Response Rate and Silence Rate of LLMs evaluated using the Matched and Mismatched Datasets, respectively. Additionally, in Tables 3 and 5, we further evaluate the Response Rate and Silence Rate using the intersection subdataset of all models, given that Llama models and GPT-3.5 have smaller context windows. The findings from these experimental results remain consistent. Response Rate Failure Cases Study. The error types distribution for response rate failure cases study in Matched and Mismatched datasets are presented in Figure 9. The mappings between error types and improvement solution direction are summarized in Table 12. Recall Analysis. The loose recall rate and strict recall rate for the Matched Dataset are shown in Table 6. We further evaluate the recall rates using the intersection subdataset of all models, with results presented in Table 7. Although the absolute values of recall rates for all models are higher, the performance differences among the models are similar. Note that we do not include Llama3-8B and Llama3-70B here in the intersection study to Figure 8: Example of evaluation dataset construction. Participants are represented by different ID numbers and icons. Colored boxes indicate utterances from different participants. The process includes extracting Input Context Information, creating Transcript Snapshot, and generating response with the LLM-powered meeting delegate. The Generated Response is evaluated by comparison with the Ground-Truth Response. avoid too few samples. The findings from these experimental results remain consistent. Attribution Analysis. The attribution metrics for LLMs are included in Table 8. We also evaluate the attribution metrics using the intersection subdataset. Note that we do not include Llama3-8B and Llama3-70B here in the intersection study to avoid too few samples. The findings from these experimental results remain consistent. Correlation Study. The correlation of response rate and recall metrics with test case metadata is shown in Figure 10. No significant correlations is found between these metrics and the metadata. Ablation Study. The response rates of LLMs for the Noisy Name Dataset are presented in Table 10, with the response rates from Explicit Cue in Matched Dataset are also shown for reference. significant drop in performance is observed for all models, except for GPT-3.5 where responses rates are already low. This further highlights challenges in accurately recognizing participant names. Further model fine-tuning to better handle such transcription errors may be necessary. For the No- <Context> study, all evaluation metrics for GPT4o in No-<Context> Scenario are shown in Table 11, showing minimal performance impact across all evaluation metrics when context information is omitted."
        },
        {
            "title": "C Model Specifications",
            "content": "In Table 13, we list all LLMs utilized in this paper, along with their detailed model version and usage scenarios."
        },
        {
            "title": "D Prompts",
            "content": "We include all prompts used in the paper. Table 14 provides the prompt for generating the response in the Meeting Engagement module. The prompts used for evaluating and attributing the generated response are given in Tables 17 and 19, respectively. Lastly, the prompts for extracting context information and extracting test cases from meeting transcripts are given in Table 21 and Table 27, respectively. Table 2: Response Rate for Matched Dateset. Type GPT-3.5 GPT-4 GPT-4o Gemini 1.5 Flash Gemini 1.5 Pro Llama3-8B Llama3-70B Chime In Explicit Cue Implicit Cue All 39.3% 53.2% 52.2% 50.6% 37.9% 61.3% 86.7% 94.3% 67.2% 71.9% 68.9% 77.3% 71.8% 89.7% 83.6% 83.8% 41.9% 78.3% 55.9% 60.8% 84.1% 91.2% 90.0% 89.6% 93.8% 99.4% 94.8% 96.2% Table 3: Response Rate for Intersection Subset of Matched Dateset. Type GPT-3.5 GPT-4 GPT-4o Gemini 1.5 Flash Gemini 1.5 Pro Llama3-8B Llama3-70B Chime In Explicit Cue Implicit Cue All 35.2% 58.6% 54.3% 52.9% 42.3% 57.7% 92.0% 92.0% 65.8% 68.3% 71.2% 74.8% 66.2% 87.7% 81.9% 81.5% 43.7% 76.5% 53.5% 59.9% 81.7% 89.5% 89.7% 88.4% 95.8% 98.1% 94.7% 96.0% Table 4: Silence Rate for Mismatched Dataset. Type GPT-3.5 GPT-4 GPT-4o Gemini 1.5 Flash Gemini 1.5 Pro Llama3-8B Llama3-70B Explicit Cue Implicit Cue All 75.0% 70.4% 72.4% 82.8% 84.6% 79.5% 67.9% 73.6% 81.6% 65.0% 52.0% 57.5% 88.1% 77.1% 81.7% 36.0% 35.3% 35.6% 41.6% 33.3% 37.0% Table 5: Silence Rate for Intersection Subset of Mismatched Dataset. Type GPT-3.5 GPT-4 GPT-4o Gemini 1.5 Flash Gemini 1.5 Pro Llama3-8B Llama3-70B Explicit Cue Implicit Cue All 79.5% 69.5% 74.2% 84.9% 90.4% 81.7% 74.4% 81.9% 83.2% 76.7% 58.5% 67.1% 90.4% 81.7% 85.8% 37.0% 35.4% 36.1% 44.6% 31.9% 38.7% Table 6: Recall Rate for Matched Dataset. Model Chime In Explicit Cue Implicit Cue All Loose Strict Loose Strict Loose Strict Loose Strict GPT-3.5 GPT-4 GPT-4o Gemini 1.5 Flash Gemini 1.5 Pro Llama3-8B Llama3-70B 43.5% 29.5% 54.5% 42.5% 47.8% 37.0% 49.5% 38.0% 51.1% 39.9% 72.8% 60.7% 63.0% 49.6% 65.9% 53.1% 53.9% 47.0% 77.8% 64.2% 62.5% 47.9% 67.3% 53.9% 29.2% 22.5% 69.5% 56.5% 55.0% 40.2% 56.6% 43.4% 34.6% 28.8% 72.8% 59.9% 56.0% 43.5% 60.5% 48.6% 46.7% 35.5% 59.6% 48.7% 52.7% 40.5% 54.2% 42.6% 45.8% 34.7% 69.6% 59.4% 55.9% 44.0% 59.1% 47.9% Table 7: Recall Rate for Intersection Subset of Matched Dataset. Note that due to limited statistics for intersecting Llama results, Llama results are not included. The total number of cases in the considered Intersection Subset is 196. Model Chime In Explicit Cue Implicit Cue All Loose Strict Loose Strict Loose Strict Loose Strict GPT-3.5 GPT-4 GPT-4o Gemini 1.5 Flash Gemini 1.5 Pro 55.6% 47.2% 58.4% 46.9% 56.1% 45.2% 57.1% 46.0% 77.8% 52.8% 79.8% 66.7% 70.4% 55.8% 75.0% 60.6% 66.7% 52.8% 85.4% 70.6% 79.6% 59.8% 81.6% 64.4% 44.4% 32.2% 79.8% 64.6% 67.3% 49.3% 71.9% 55.4% 22.2% 19.4% 77.5% 62.6% 60.2% 46.2% 66.3% 52.4% Table 8: Attribution Analysis results for Matched Dataset. For the Expected Response metric, higher values are better, while for the Previous Transcript and Hallucination metrics, lower values are preferable. Metric GPT-3.5 GPT-4 GPT-4o Gemini 1.5 Flash Gemini 1.5 Pro Llama3-8B Llama3-70B Expected Response Input Context Info Previous Transcript Hallucination Expected Response Input Context Info Previous Transcript Hallucination Expected Response Input Context Info Previous Transcript Hallucination Expected Response Input Context Info Previous Transcript Hallucination 18.4% 37.2% 33.7% 10.8% 32.5% 40.4% 20.8% 6.43% 25.2% 39.9% 31.9% 2.96% 26.9% 39.8% 28.4% 4.95% 27.0% 42.2% 43.0% 37.9% 25.1% 15.9% 4.93% 4.05% 50.1% 51.0% 31.6% 38.1% 12.4% 7.28% 5.98% 3.58% 39.9% 38.9% 39.9% 45.9% 15.0% 11.3% 3.8% 5.12% 42.8% 43.9% 36.9% 42.0% 14.8% 10.3% 5.44% 3.74% Chime In 25.3% 39.2% 28.6% 6.93% Explicit Cue 52.4% 27.0% 14.4% 6.24% Implicit Cue 38.0% 34.2% 22.4% 5.48% All 41.5% 32.3% 20.3% 5.91% 35.2% 26.3% 28.1% 10.4% 61.1% 26.3% 9.25% 3.35% 46.8% 32.0% 14.8% 6.32% 51.6% 29.2% 13.8% 5.48% 25.4% 39.6% 31.6% 3.43% 31.9% 36.8% 25.4% 5.82% 28.2% 34.3% 35.7% 1.80% 29.1% 35.8% 31.6% 3.39% 27.4% 31.4% 32.4% 8.75% 50.1% 28.3% 16.8% 4.81% 38.9% 34.1% 22.6% 4.38% 41.2% 31.8% 21.9% 5.09% Table 9: Attribution Analysis results for Intersection Subset of Matched Dataset. For the Expected Response metric, higher values are better, while for the Previous Transcript and Hallucination metrics, lower values are preferable. Note that due to limited statistics for the intersecting Llama results, Llama results are not included. The total number of cases in the considered Intersection Subset is 196. Metric GPT-3.5 GPT-4 GPT-4o Gemini 1.5 Flash Gemini 1.5 Pro Expected Response Input Context Info Previous Transcript Hallucination Expected Response Input Context Info Previous Transcript Hallucination Expected Response Input Context Info Previous Transcript Hallucination Expected Response Input Context Info Previous Transcript Hallucination 22.0% 55.7% 11.1% 11.1% 37.4% 37.9% 19.6% 5.1% 30.6% 42.6% 23.5% 3.3% 33.3% 41.1% 21.2% 4.5% Chime In 35.8 30.9% 58.1% 64.2% 0.0% 5.0% 0.0% 5.9% Explicit Cue 59.1% 56.1% 27.7% 36.4% 3.3% 10.1% 3.1% 5.98% Implicit Cue 47.3% 49.9% 36.4% 38.6% 7.0% 12.2% 4.5% 4.0% All 51.8% 52.1% 33.5% 38.8% 5.0% 10.9% 3.7% 4.1% 29.2% 45.8% 12.5% 12.5% 59.9% 23.3% 11.7% 5.1% 49.4% 31.3% 17.6% 1.7% 53.4% 28.2% 14.7% 3.7% Table 10: Response rate for Noisy Name Dataset. 22.2% 22.2% 44.4% 11.1% 66.9% 19.5% 12.5% 1.2% 51.3% 29.4% 12.1% 7.1% 57.0% 24.5% 13.8% 4.6% Type Dataset GPT-3.5 GPT-4 GPT-4o Gemini 1.5 Flash Gemini 1.5 Pro Llama3-8B Llama3-70B Explicit Cue Explicit Cue Noisy Name Matched 53.2% 52.5% 86.7% 94.3% 53.3% 68.0% 89.7% 60.7% 78.3% 59.8% 91.2% 79.4% 99.4% 87.0% Table 11: All Evaluation Metrics for GPT-4o in No-<Context> Scenario. Metric Chime In Explicit Cue Implicit Cue All Response Rate Loose Recall Strict Recall Expected Response Input Context Info Previous Transcript Hallucination 59.1% 46.2% 37.7% 21.0% 57.2% 14.1% 7.7% 90.4% 82.6% 65.0% 44.1% 36.0% 14.4% 5.4% 78.7% 75.0% 50.2% 44.7% 31.9% 14.0% 9.4% 80.2% 74.7% 55.8% 41.1% 37.3% 14.2% 7.3% Table 12: Mapping between Error Types and Solution Direction for Response Rate Failure Cases Study. Dataset Scenarios Matched Chime In Explicit Cue Mismatched Mismatched Error Type Decision based on wrong latest utterance Identify as cue to others or all participants Missing the need for proactive participation Decision made due to Conversation is still going, cant interrupt Unable to find the related context Other Decision based on wrong latest utterance Correctly recognizes the cue but does not respond Ambiguity due to multiple names in single utterance or long context Fails to recognize the cue Hallucination Other Decision based on wrong latest utterance Latest utterance related to provided information Failure to recognize cues directed to others Hallucination Other Solution Direction Improved Instruction Following Enhanced Reasoning in Meeting Scenario Enhanced Reasoning in Meeting Scenario Enhanced Reasoning in Meeting Scenario Enhanced General Reasoning N/A Improved Instruction Following Enhanced Reasoning in Meeting Scenario Enhanced Reasoning in Meeting Scenario Enhanced General Reasoning Enhanced General Reasoning N/A Improved Instruction Following Enhanced Reasoning in Meeting Scenario Enhanced Reasoning Enhanced General Reasoning N/A Table 13: Details of Model Use Scenarios and Model Version. Model Name GPT-3.5 GPT-4 GPT-4o Model Use Scenarios Generate Response (Table 2 & Table 3 & Table 4 & Table 5, Prompt in Table 14) Generate Response (Table 2 & Table 3 & Table 4 & Table 5, Prompt in Table 14) Evaluation (Table 6 & Table 7, Prompt in Table 17) Attribution (Table 8 & Table 9, Prompt in Table 19) Extract context information (Figure 8, Prompt in Table 21) Extract test cases (Figure 8, Prompt in Table 27) Generate Response (Table 2 & Table 3 & Table 4 & Table 5, Prompt in Table 14) Model Version gpt-3.5-turbo-1106 with 16k context window gpt-4-turbo-20240409 with 128k context window gpt-4-1106-preview with 128k context window gpt-4-turbo-20240409 with 128k context window gpt-4o-20240513-preview with 128k context window (a) Chine In (Matched Dataset) (b) Explicit Cue (Matched Dataset) (c) Mismatched Dataset Figure 9: (a) Error Types Distribution for Response Rate Failure Cases Study in Chine In Matched Dataset. (b) Error Types Distribution for Response Rate Failure Cases Study in Explicit Cue Matched Dataset. (c) Error Types Distribution for Response Rate Failure Cases Study in Mismatched Dataset. Figure 10: The correlation between the performance metrics and test case metadata for GPT-4o. - You are Meeting Delegate Agent that attends meetings on behalf of <Person Name>. - You are provided with the intent of participating in the meeting, specified as <Intents>. - You are provided with the background information that <Person Name> knows, specified as <Background>. - You are provided with the full list of attendees <Attendees> to help identify if someone cues you. - You are provided with the ongoing meeting transcript <Meeting Transcript> to determine if there is need to respond. - Your task is to assess the content of the ongoing meeting transcript <Meeting Transcript> and determine whether you are can speak and what to say. - You are encouraged to respond and ask questions, give comments, or share information without interrupting others in the meeting. ## About the <Person Name> - <Person Name> is the name of the person you represent in the meeting. - People in the <Attendees> list may cue you by using <Person Name> exactly or parts of the name (e.g., first name, initials). ## About the <Attendees> - <Attendees> is list of names of the people attending the meeting. - Each name in the list is full name or nickname. ## About the <Meeting Transcript> - <Meeting Transcript> is series of utterances spoken by the meeting participants. - Each utterance is formatted as \"Name: Content\", where Name is the speakers name and Content is their spoken text. - The utterances are in chronological order and the latest utterance is at the bottom of the transcript. - The utterances may contain typos and grammatical errors. ## About the <Intents> - <Intents> consists of the questions or topics that <Person Name> aims to discuss during the meeting. - You can ask the questions or motivate the discussion of the topics in the <Intents> at the appropriate time without interrupting others. ## About the <Background> - <Background> consists of the background information that <Person Name> knows before the meeting. - <Background> is list of \"Context\" and \"Information\" pairs. You can share the \"Information\" in the \"Context\" at the appropriate time without interrupting others. ## Guidelines to judge whether you can speak and decide what to say - Read the <Meeting Transcript> to understand the context of the meeting. - Focus on the latest several utterances in the <Meeting Transcript> to understand the current discussion. - Remember that you are delegate attending the meeting on behalf of <Person Name>. - You should judge whether you can speak first, then decide what to say, if you can speak. - Judge whether you can speak according to the following instructions: - Figure out what the latest utterance (at the bottom of the <Meeting Transcript>) is about and pay attention to who is being addressed. - If the latest utterance is straightforward question or request or instructions to other participants, you MUST NOT speak to avoid interrupting others, even if the conversation is related to the <Intents> or <Background>. - If the latest utterance is for the <Person Name>, you should respond to it. - If you can speak, consider the following guidelines: - Your speech content should be directly relevant to the current discussion. - You can reference the <Intents> and <Background> to organize your speech. - You should be polite and natural in your speech. - You MUST NOT make up facts. - You MUST NOT repeat what <Person Name> has said in the <Meeting Transcript>. - Chit chat is natural part of conversation. You can engage in chit chat with other attendees if it is appropriate or relevant to the meeting context. For example, you can say good morning, Thank you, Yeah, agree. - Before speaking, you should think twice to ensure that you are not interrupting others and your speech is relevant to the current discussion. ## Notes on judging whether someone is cued - The name may be transcribed as similar-sounding words by the speech recognition system. Especially, the pronunciation of Chinese names may be recognized as similar-sounding English words, for exmaple, \"Si Li\" may be transcribed as \"Celine\" or \"silence\". - When encountering words that seem out of place, it is likely due to errors in speech recognition. Examine the list of attendees to determine if the pronunciation of these words are similar to any English or Chinese names listed. - You should consider the context of the meeting and the names of the attendees to determine if you or someone are cued. CONTINUE ON THE NEXT PAGE Table 14: Prompt used for generating response in the Meeting Engagement module. ## The output of response <Response> - The response should be dictionary with the following format: { } \"thoughts\": \"<thoughts>\", \"speak\": \"<speak>\" - <thoughts>: The reasoning or considerations to judge whether you can speak and decide what to say. At the beginning, you should state who you are representing. Then <thoughts> should explain what the latest utterance is about and then explain why you can or cannot speak. If you can speak, you should also explain how you decide what to say. - <speak>: The content you are going to speak. If you are not allowed to speak or you do not want to speak, the <speak> is empty. ## Example - Example 1: Below is an example of that pronunciations of Chinese names may be recognized as similarsounding English words by the speech recognition system. <Person Name> Sirui Zhao <Attendees> [ - San Zhang, - Si Li, - Sirui Zhao ] <Meeting Transcript> Si Li: Good morning. Sirui Zhao: Hello! San Zhang: Hi! Si Li: OK, Lets start our meeting. There are still some people who havent joined, so let start first. Our topic today is the progress of environmental protection, three, do you have some thing to share on it? <Intents> [ - The extent of plastic misuse ] <Background> [ { \"Context\":\"Discussion about reducing air pollution\", \"Information\":\"The air pollution of our city is becoming serious. The goverment takes extreme measures to control the pollution by closing the factory and limiting the use private car.\" } ] <Response> { - \"thoughts\": \"Im representing Sirui Zhao in the meeting. In the last utterance, the appearance of three is abrupt. Contextually, there is no need for numbers; phonetically, \"Three\" sounds like \"Sirui.\", which closely resembles Sirui from the attendees, specifically <Person Name>. The speaker is most likely asking Sirui Zhao to share something on the progress of environment protection. So need to give response. And based on the background information, can share something about reduing air pollution.\", - \"speak\": \"Yes. The air pollution of our city is becoming serious. The goverment takes extreme measures to control the pollution by closing the factory and limiting the use private car.\" } CONTINUE ON THE NEXT PAGE Table 15: Prompt used for generating response in the Meeting Engagement module (continued). - Example 2: Below is an example of that you should not speak since the latest utterance is straightforward question or request or instructions to other participants. <Person Name> Frank <Attendees> [ - John, - James, - Alice, - Bob - Frank ] <Meeting Transcript> Bob: James, that price is too high, we can not accept it. James: Ok, will contact the supplier again and discuss the price. John: Thank you, James. John: OK, Lets go to the next topic. Alice, what is your progress on the project development? <Intents> [ - Whether Bob fixed the bug reported ] <Background> [ { \"Context\":\"Report on the dataset preparation progress\", \"Information\":The dataset preparation is almost done. We are now working on the data cleaning and normalization. We expect to finish it by the end of the week. } ] <Response> { - \"thoughts\": \"I am representing Frank in the meeting. In the latest utterance, John is explicitly asking Alice about the project development. can not speak.\", - \"speak\": \"\" } ## Note - You are representing <Person Name> in the meeting. You should respond to the cues from the attendees and the context of the meeting. - You should not interrupt others in the meeting. Table 16: Prompt used for generating response in the Meeting Engagement module (continued). - You are an Evaluation Agent responsible for assessing the response generated by meeting AI assistant against the standard answer. - You are provided with the summary of the <StandardAnswer>. - You are provided with the raw <ActualResponse> generated by the meeting AI assistant. - Your task is to summarize the main points in the <ActualResponse>, and evaluate whether the main points in the <ActualResponse> match the main points in the <StandardAnswer>. ## About the meeting AI Assistant - The meeting AI assistant is designed to represent the user to engage in meeting. ## About the <StandardAnswer> - The <StandardAnswer> is list of strings that represents the main points of the ground truth response. ## About the <ActualResponse> - The <ActualResponse> is string that represents the response generated by the meeting AI assistant to the meeting content. - You should reference the <Transcript> to understand the context of the meeting and the information in the <ActualResponse>. ## Guidelines for Evaluation - The evaluation process involves comparing the main points in the <StandardAnswer> and the <ActualResponse>. - Summarize the main points in the <ActualResponse> and keep the same granularity as the <StandardAnswer>. - The uninformative utterance about expressing goodness or politeness should not be considered as main points. - For example, \"If you need more help, please let me know.\" is not informative and should not be considered as main point. - You should calculate list that contains the index of the matching main points in the <ActualResponse> corresponding to the <StandardAnswer>. For example, if the first main point in the <ActualResponse> matches the second main point in the <StandardAnswer>, the first element of the list should be 2. And if the total number of main points in the <ActualResponse> is 1, the list should be [2]. - Count the number of main points in the <ActualResponse> (ActualMainPointsCount). - Count the number of matching main points between the <ActualResponse> and the <StandardAnswer> (MatchingMainPointsCount). - The main points are considered matching if they are semantically similar. ## Output Format - The output MUST be in the JSON format. - You MUST explain the process of evaluation before providing the evaluation results. - The output MUST include the following fields: - Explanation: explanantion of steps involved in the evaluation process. First, you should summarize the main points in the <ActualResponse>. Then, you should explain which main points in the <ActualResponse> match the main points in the <StandardAnswer> and mark the index of the matching main points in the <ActualResponse> corresponding to the <StandardAnswer> main points. - ActualMainPoints: The list of main points in the <ActualResponse>. - ActualMainPointsCount: The number of main points in the <ActualResponse>. - MatchingMainPoints: The list of matching main points between the <ActualResponse> and the <StandardAnswer>. - MatchingIndex: The list of the index of the matching main points in the <ActualResponse> corresponding to the <StandardAnswer> main points. The length of the list should be the same as the ActualMainPointsCount. If ActualMainPointsCount is 5, the format of the list should be [1, 2, -1, -1, 4], which means the first, second, and fifth main points in the <ActualResponse> match the first, second, and fourth main points in the <StandardAnswer>. And the third and fourth main points in the <ActualResponse> do not match any main points in the <StandardAnswer>. - MatchingMainPointsCount: The number of matching main points between the <ActualResponse> and the <StandardAnswer>. - If the <ActualResponse> is empty, the ActualMainPointsCount, MatchingMainPointsCount, RecallRate, and PrecisionRate should be 0. - Note that you must keep the length of the MatchingIndex the same as the ActualMainPointsCount, instead of the length of the <StandardAnswer>. CONTINUE ON THE NEXT PAGE Table 17: Prompt used for evaluating the generated response against the ground-truth one. ## Example1 <StandardAnswer> [\"Calculated word error rate on Czech transcripts\", \"Conducted testing sessions with PERSON11 and PERSON18\", \"Contributed to the PROJECT3 deliverable\", \"Waiting for new tasks \"] <ActualResponse> \"Hi everyone. Over the past week, calculated the word error rate on Czech transcripts using three versions of Czech ASR created by PERSON10. also conducted few testing sessions with PERSON11 and PERSON18, but they were not successful due to issues with the segmenters from ORGANIZATION1. have updated the German transcripts in its corresponding path. also backed up all the systems, including some new ones created today and last week. For the next week, am waiting for new tasks. By the way, do we have the golden transcripts for the English videos?\" <Evaluation> { \"Explanation\": \"First, summarize the main points in the <ActualResponse>. The < ActualResponse> has the following main points: 1. Calculated word error rate on Czech transcripts. 2. Conducted testing sessions with PERSON11 and PERSON18. 3. Updated German transcripts in its corresponding path. 4. Backed up all the systems. 5. Waiting for new tasks. 6. Ask about the golden transcripts for the English videos. So, the number of main points in the <ActualResponse> is 6 and the length of the MatchingIndex is 6. Sencond, compare the main points in the <ActualResponse> with the main points in the < StandardAnswer>. The matching main points between the <ActualResponse> and the < StandardAnswer> are: 1. Calculated word error rate on Czech transcripts. This point maches with the first point in the <StandardAnswer>. 2. Conducted testing sessions with PERSON11 and PERSON18. This point matches with the second point in the <StandardAnswer>. 3. Waiting for new tasks. This point matches with the fourth point in the <StandardAnswer>. The number of matching main points between the <ActualResponse> and the <StandardAnswer> is 3. The other points in the <ActualResponse> do not match points in the <StandardAnswer >.\", \"ActualMainPoints\": [\"Calculated word error rate on Czech transcripts\", \"Conducted testing sessions with PERSON11 and PERSON18\", \"Updated German transcripts in its corresponding path\", \"Backed up all the systems\", \"Waiting for new tasks\", \"Ask about the golden transcripts for the English videos\"], \"ActualMainPointsCount\": 6, \"MatchingMainPoints\": [\"Calculated word error rate on Czech transcripts\", \"Conducted testing sessions with PERSON11 and PERSON18\", \"Waiting for new tasks\"], \"MatchingIndex\": [1, 2, -1, -1, 4, -1], \"MatchingMainPointsCount\": 3 } ## Example2 <StandardAnswer> [\"Confirm the task about writing report about the calculation and share it with others\", \"Synthesize the information other team members have shared\", \"Wait for the next task\"] <ActualResponse> \"Sure, will finish the calculation. will also write report about the calculation.\" <Evaluation> { \"Explanation\": \"First, summarize the main points in the <ActualResponse>. The < ActualResponse> has the following main points: 1. Confirm the task about finishing the calculation and writing report about it. The number of main points in the < ActualResponse> is 1 and the length of the MatchingIndex is 1. Sencond, compare the main points in the <ActualResponse> with the main points in the <StandardAnswer>. The matching main points between the <ActualResponse> and the <StandardAnswer> contains: 1. Confirm the task about finishing the calculation and writing report about it. This point matches with the first point in the <StandardAnswer>. The number of matching main points between the <ActualResponse> and the <StandardAnswer> is 1. The other points in the < ActualResponse> do not match points in the <StandardAnswer>.\", \"ActualMainPoints\": [\"Confirm the task about finishing the calculation and writing report about it\"], \"ActualMainPointsCount\": 1, \"MatchingMainPoints\": [\"Confirm the task about finishing the calculation and writing report about it\"], \"MatchingIndex\": [1], \"MatchingMainPointsCount\": 1, } Table 18: Prompt used for evaluating the generated response against the ground-truth one (continued). - You are an Attribution Agent responsible for assessing the response generated by meeting AI assistant and determining its source. - You are provided with the list of <ActualResponse>. - You are also provided with the transcript of the meeting content (<Transcript>) and the <ContextInfo> used to generate the <ActualResponse>. - Your task is to attribute the <ActualResponse> to the corresponding part of the <Transcript> or the <ContextInfo>. ## About the <ActualResponse> - The <ActualResponse> is list that represents the response generated by the meeting AI assistant. ## About the <StandardResponse> - The <StandardResponse> is list that represents the expected response. - The <StandardResponse> may be the same as <ActualResponse>, or it may not be. ## About the <Transcript> - The transcript are the collection of utterances from the meeting participants. - Each utterance is formatted as \"Name: Content\", where Name is the speakers name and Content is their spoken text. - Utterances are in chronological order and may contain typos and grammatical errors. - The transcript ends at the time stamp when the meeting AI assistant should generate the response. - Example utterances: PERSON1: Hello everyone, Im glad to see you all here today. (id=0) ## About the <ContextInfo> - <ContextInfo> is dictionary that contains <Intents> and <Background>. - <Intents> consists of the questions or topics that can generate the <ActualResponse>. - <Background> is list of \"Context\" and \"Information\" pairs. For each pair, \"Information\" can be shared in the \"Context\" situation to generate the <ActualResponse>. And each pair can be used many times. ## Guidelines for Attribution - You need to decide whether the main points in the <ActualResponse> match the <StandardResponse>. - The number of main points in the <ActualResponse> is not fixed. PointID is used to identify the main points in the <ActualResponse>. - When assessing whether the main points in the <ActualResponse> originate from the <Transcript> or the <ContextInfo>, consider the following: 1. If the main point has similar or the same meaning as the <ContextInfo>. You should consider it as originating from the <ContextInfo>. 2. If the main point explicitly repeats or closely relates to any point already mentioned in the <Transcript>. However, casual interactions such as greetings or small talk are permissible and not regarded as sourced from the <Transcript>.\" - There are four situations for the origin of the main points in the <ActualResponse>: 1. The main point in the <ActualResponse> can originate from the <ContextInfo> but is not present in the <Transcript>. You should append [PointID, 1, 0] to the AttributionList. 2. The main point in the <ActualResponse> does not originate from the <ContextInfo> but originates from the <Transcript>. You should append [PointID, 0, 1] to the AttributionList. 3. The main point in the <ActualResponse> can originate from both the <ContextInfo> and the <Transcript>. You should append [PointID, 1, 1] to the AttributionList. 4. The main point in the <ActualResponse> does not originate from the <ContextInfo> and is not present in the <Transcript>. You should append [PointID, 0, 0] to the AttributionList. ## Output Format - The output MUST be in the JSON format. - You MUST explain the process of attribution for every main point in the <ActualResponse>. - Note that AttributionList should only contain the List of lists and should not contain any additional information or annotations. CONTINUE ON THE NEXT PAGE Table 19: Prompt used for the attribution of the generated response. - The output MUST include the following fields: - Explanation: For every main point in the <ActualResponse>, explain the process of attribution. Especially, explain why the main point matches or does not match the <StandardResponse> and why it originates from the <Transcript> or the <ContextInfo>. - AttributionList: list of lists, where each list contains PointID and the attribution for main point in the <ActualResponse>. - PointsCount: The number of main points in the <ActualResponse>. ## Example - Example 1: <Transcript> PERSON13: Hi. Hello [PERSON6]. Hello [PERSON19]. Thanks for, uhm. (id=0) PERSON6: Hi everyone. (id=1) PERSON19: Hi. (id=2) PERSON13: Yeah, great. Thanks for joining and, uh, yeah okay. So, yeah. Uh, see that people have written up ehm what they did. (id=3) PERSON19: Hi [PERSON13], can hear you. (id=4) PERSON13: Yeah. [PROJECT3] deliverables. So, Ill try to provide the links-. Or those who of you, who are already working on the deliverables, please mention that. And yeah. Lets lets go quickly over what what have done. So [PERSON6] you are the first on the list. Ehm , ehm, so please briefly update what what you have been working on. And what what is the plan for the next week. (id=5) <ContextInfo> { \"Intents\": [ \"What [PERSON6] has been working on and the plan for the next week?\" ], \"Background\": [ { \"Context\": \"Update on recent work and plans for the next week\", \"Information\": \"This week had fewer tasks. calculated the word error rate on Czech transcripts using three versions of Czech ASR created by [PERSON10]. There were significant mismatches between the golden transcript and its corresponding video. conducted testing sessions with [PERSON11] and [PERSON18], which were not successful due to issues with segmenters from [ORGANIZATION1]. also contributed to the [PROJECT3] deliverable for the punctuator and through caser.\" } ] } <StandardResponse> [\"I calculated the word error rate on Czech transcripts\"] <ActualResponse> [\"Calculated word error rate on Czech transcripts\", \"Conducted testing sessions with PERSON11 and PERSON18\"] <Evaluation> { \"Explanation\": \"1. Calculated word error rate on Czech transcripts. This point matches the standard response. \"I calculated the word error rate on Czech transcripts\" is present in the ContextInfo. Therefore, the attribution is [1, 1, 0]. 2. Conducted testing sessions with PERSON11 and PERSON18. The point does not match the standard response. \"I conducted testing sessions with [PERSON11] and [PERSON18]\" is present in the BackgroundKnowledge. Therefore, the attribution is [2, 1, 0].\", \"AttributionList\": [[1, 1, 0], [2, 1, 0]], \"PointsCount\": 2 } Table 20: Prompt used for the attribution of the generated response (continued). - Your task is to update the summary of the utterances of {participant} in meeting transcript. - You are provided with <Transcript Snippet> that contains portion of the meeting transcript. - You are also provided with <Previous Summary> which contains the summary of utterances for {participant} in other parts of the meeting. - You need to update the <Previous Summary> based on the utterances of the {participant} in the <Transcript Snippet>. ## On the provided <Transcript Snippet> - Transcripts are the collection of utterances from the meeting participants. - The transcript data is deidentified. Speakers and other named entities are not identified by names, but rather by IDs in the format ENTITYNUMBER (e.g. PERSON1 or PROJECT3) or just ENTITY (e.g. PATH). - Speaker IDs at the beginning of transcript lines are enclosed in round brackets, all other deidentified entities in square brackets. - Each utterence ends with \"(id=x)\", which is the utterance id, an increasing number from 0 to indicate the serial number of utterance in the whole meeting transcript. - The provided transcript snippet maybe not start from the beginning of the meeting. - Example utterances: (PERSON1) Hello everyone, Im glad to see you all here today. (id=0) (PERSON2) Hi, Im excited to be here. (id=1) (PERSON3) Im looking forward to the discussion. [PERSON1] mentioned that the project is going well. (id=2) ## On the <Previous Summary> - The <Previous Summary> is structured summary of the utterances of {participant} in the meeting transcript. - The <Previous Summary> contains two parts, \"wanted information\" and \"provided information\". - \"wanted information\" is list of questions made by the {participant}. - \"provided information\" is the information provided by the participant to others. It is list of Context and Information pairs, where the \"Context\" is the context in which where the {participant} provides the \"Information\". ## Instructions on updating the <Previous Summary> - Identify the utterances of {participant} in the <Transcript Snippet>. - If {participant} does not speak in the <Transcript Snippet>, do NOT update the <Previous Summary>. - Focus on only the informative utterances and ignore the greetings, appreciation, simple acknowledge and other chit chat. - Extract the \"wanted information\" and \"provided information\" from the <Transcript Snippet>. - You should try to use original utterances as much as possible after removing noise words and polishing them for better readability. - The second or third personal pronoun (you, he, she, they) in the utterances should be properly replaced with the corresponding participants ID to avoid ambiguity. - Use the extracted information to update the <Previous Summary>. - You can modify the existing \"wanted information\" and \"provided information\" or add new information, but do not remove any existing information. - You MUST NOT mix the information provided by {participant} and other participants while updating the <Previous Summary>. - You MUST NOT miss any important information provided by {participant} in the <Transcript Snippet>. ## Requirement on the output format - You MUST explain your thoughts and steps of updating the <Previous Summary> before providing the updated summary. - Output must be in Json format with the \"Thoughts\" and \"Updated Summary\" as the key. - The \"Thoughts\" is your thoughts and steps of updating the <Previous Summary>. - The \"Updated Summary\" contains the updated summary of the utterances for {participant}. CONTINUE ON THE NEXT PAGE Table 21: Prompt used for extracting context information. ## Example 1 - Here is an example of updating the utterances summary for PERSON2. You can refer to this example for better understanding. - Suppose the transcript snippet contains the following utterances: (PERSON3) Good moring. (id=2) (PERSON1) Lets get started with todays meeting on the recent progress of our software development project. Well go through updates from each team and discuss any blockers or issues. [PERSON2], could you start with the development updates?\". (id=3) (PERSON2) Sure, [PERSON1]. Weve made significant progress this sprint. We completed the implementation of the new authentication module and integrated it with our existing systems. (id=4) (PERSON1) Thats great to hear, [PERSON2]. How about the feature for real-time notifications? Is it on track? (id=5) (PERSON2) Yes, it is. Were about 75% done with it. The core functionality is in place, and we are now working on optimizing the delivery speed and ensuring it works seamlessly across different devices. (id=6) - Suppose the previous summary of PERSON2 contains the following information: {{ }} \"wanted information\": [], \"provided information\": [] - The thoughts and updated summary will be: {{ \"Thoughts\":\"In the transcript snippet, PERSON2 responds to PERSON1s questions about the development updates and the progress of the feature for real-time notifications. This information can be added to the \"provided information\" for PERSON2.\" \"Updated Summary\": {{ \"wanted information\": [], \"provided information\": [ {{ \"Context\": \"Respond to other participants question about the development updates\", \"Information\": \"Weve made significant progress this sprint. We completed the implementation of the new authentication module and integrated it with our existing systems.\" }}, {{ \"Context\": \"Respond to other participants question about the progress of the feature for real-time notifications\", \"Information\": \"Were about 75% done with it. The core functionality is in place, and we are now working on optimizing the delivery speed and ensuring it works seamlessly across different devices.\" }} ] }} }} ## Example 2 - Here is another example of updating the utterances summary for PERSON2. You can refer to this example for better understanding. - Suppose the transcript snippet contains the following utterances: (PERSON3) Good moring. (id=2) (PERSON1) Lets get started with todays meeting on the recent progress of our software development project. Well go through updates from each team and discuss any blockers or issues. [PERSON2], could you start with the development updates?\". (id=3) (PERSON2) Sure, [PERSON1]. Weve made significant progress this sprint. We completed the implementation of the new authentication module and integrated it with our existing systems. (id=4) CONTINUE ON THE NEXT PAGE Table 22: Prompt used for extracting context information (continued). (PERSON1) Thats great to hear, [PERSON2]. How about the feature for real-time notifications? Is it on track? (id=5) (PERSON2) Yes, it is. Were about 75% done with it. The core functionality is in place, and we are now working on optimizing the delivery speed and ensuring it works seamlessly across different devices. (id=6) (PERSON3) [PERSON2], have you had chance to address the bug reported last week related to the authentication module? (id=7) (PERSON2) Yes, [PERSON3]. We identified the root cause of the bug, and its been fixed. It was due to conflict with third-party library we were using. (id=8) (PERSON3) Thats good to hear. Thank you, [PERSON2]. (id=9) (PERSON1) [PERSON2], for the next step of the project, Id like you first complete the real-time notifications feature, and then focus on the chatbot development. (id=10) (PERSON2) Understood, will do that. (id=11) (PERSON2) By the way, whats the timeline of our project? (id=12) (PERSON1) We are aiming to finish the project by the end of August. (id=13) (PERSON2) Ok, know. (id=14) (PERSON1) Lets move to the next topic. [PERSON3], could you provide an update on the testing progress? (id=15) (PERSON3) Sure. Certainly. Weve conducted tests on the new authentication module, and everything looks good so far. (id=16) (PERSON1) Mhm. (id=17) (PERSON3) We are now preparing for the testing of the real-time notifications feature. (id=18) (PERSON2) In our development process, we accumulated some test cases which may help you. (id=19) (PERSON3) Thats helpful, thank you. (id=20) - Suppose the previous summary of PERSON2 contains the following information: {{ \"wanted information\": [], \"provided information\": [ {{ \"Context\": \"Respond to other participants question about the development updates\", \"Information\": \"Weve made significant progress this sprint. We completed the implementation of the new authentication module and integrated it with our existing systems.\" }}, {{ \"Context\": \"Respond to other participants question about the progress of the feature for real-time notifications\", \"Information\": \"Were about 75% done with it. The core functionality is in place, and we are now working on optimizing the delivery speed and ensuring it works seamlessly across different devices.\" }} ] }} - The thoughts and updated summary will be: {{ \"Thoughts\":\"In the transcript snippet, the dicussion between PERSON1 and PERSON2 about the progress of the development and the feature for real-time notifications are already included in the previous summary. PERSON2 responds to PERSON3s question about the bug in the authentication module, which can be added to the \"provided information\" for PERSON2. PERSON2 asks about the timeline of the project, which can be added to the \"wanted information\" for PERSON2. PERSON2 also comments on PERSON3s statement about the testing progress, offering to provide some test cases, which can be added to the \"provided information\" for PERSON2.\" \"Updated Summary\": {{ \"wanted information\": [ \"Whats the timeline of the project?\" ], \"provided information\": [ {{ \"Context\": \"Respond to other participants question about the bug in the authentication module\", CONTINUE ON THE NEXT PAGE Table 23: Prompt used for extracting context information (continued). \"Information\": \"We identified the root cause of the bug reported by [ PERSON3], and its been fixed. It was due to conflict with third-party library we were using.\" }}, {{ \"Context\": \"Comment on other participants statement about the testing progress\", which are helpful to testing.\" \"Information\": \"In our development process, we accumulated some test cases }} ] }} }} ## Example 3 - Here is an example of updating the utterances summary for PERSON6. You can refer to this example for better understanding. - Suppose the transcript snippet contains the following utterances: (PERSON13) Hi. Hello [PERSON6]. Hello [PERSON19]. Thanks for, uhm. (id=0) (PERSON6) Hi everyone. (id=1) (PERSON19) Hi. (id=2) (PERSON13) Yeah, great. Thanks for joining and, uh, yeah okay. So, yeah. Uh, see that people have written up ehm what they did. (id=3) (PERSON19) Hi [PERSON13], can hear you. (id=4) (PERSON13) Yep, thats great. Uh, and also you were evaluating-. Yes, so thats thats re re record. What you did. So what have, uh, on my mind now is uh, uh, well, uh, preparations. So, uh, [PERSON13], uh am busy, uh, with the IW SLT, uh, write-up. Uh, that was the, uh, the wra last part that did. Now busy with interviewing people people to uh to replace those who are em moving forward <laugh/> so to say. So there is number of colleagues on projects that am supervising, uh, that who are going for studies abroad and other things. Uh, so, uh, what think we should focus on is the demo for Project Officer. Then we need to focus on the ladder climbing, uh, which is building uh, uh, [PROJECT 3] test set plus, uh, regularly, uh, testing on it. Ehm, and, ehm what else, uh, the deliverables. Yeah. [PROJECT3] deliverables. So, Ill try to provide the links-. Or those who of you, who are already working on the deliverables, please mention that. And yeah. Lets lets go quickly over what what have done. So [PERSON6] you are the first on the list. Ehm, ehm, so please briefly update what what you have been working on. And what what is the plan for the next week. (id=5) (PERSON6) <other_noise/> So, luckily. <laugh/> Not luckily but this week had like quite less tasks to do. So first calculated the word error rate on Czech transcripts using that three versions of, uh, Czech ASR which [PERSON10] created. And so yesterday [PERSON10] told me that they were, uh, and the golden transcript and its corresponding video there were there were huge huge mismatch. And <unintelligible/> and he said to update me. CONTINUE ON THE NEXT PAGE Table 24: Prompt used for extracting context information (continued). And then we conducted few testing sessions with [PERSON11] and [PERSON18]. And they were not quite successful because the segmenters from, uh, uh, [ ORGANIZATION1] they were still down and [PERSON12] today he is he is working on them. (id =6) (PERSON13) Mhm. (id=7) (PERSON6) And lastly yeah think did-. Uh, it was the input in the [PROJECT3] deliverable of for the punctuator and through caser. <unintelligible/> (id=8) (PERSON13) Mhm, yeah. (id=9) (PERSON6) And dont have-. think that apart from the testing sessions to do this week so am waiting for new tasks. (id=10) (PERSON13) Yeah, so. So the word error rate, there is also the English, uh, transcripts? Ehm, and also we should have from [PERSON9] the German one, right? So. (id=11) (PERSON6) Yeah, yeah, yeah. (id=12) (PERSON13) So, so will make it to do-. (id=13) (PERSON6) So have updated the German transcripts in its corresponding path and like do we have the golden transcripts for the English videos? (id=14) (PERSON13) Yes, thats the other part. Because this is the consecutively translated videos. So there is always the English speaker and then the Czech speaker who repeats the same content. And [PERSON7] has split the video and while the English part should be more reliable, uh, the Czech part has been done simply by using the other ends. So the Czech video has been cut using the English time stamps. Like the end of English and the beginning of the next English segment. Uh, so its like like interleave the the the other way round. So thats why Im not surprised that the Czech video is, uh, imprecise in timing. But still, was not expecting it to be that bad. So, uh, that is something that, yeah. [PERSON10], can you maybe tell us more details about that? (id=15) (PERSON10) Yeah, yeah. Well, just like listened to the audio and followed the talk transcript <other_ noise/> and it was completely off. think it is-. There must be some miss-match because-. (id=16) (PERSON13) Mhm. (id=17) (PERSON10) Yeah, yeah. The transcription is for the completely different audio than its in the subdirectory. (id=18) (PERSON6) Mhm. (id=19) (PERSON13) Oh, so then someone must have like messed it up. (id=20) (PERSON10) Yeah, yeah, may maybe its just like uh. Maybe the files are just switched between the subdirectories? (id=21) (PERSON13) Mhm. (id=22) (PERSON10) havent checked but-. Uh, yeah, there is some some serious mismatch there. (id=23) (PERSON13) Yeah, so [PERSON10] can you coul could you do this check? It should not be hard Like try listening to all the files that are within this demo for [PERSON15], uh, and try to locate the correct file, the appropriate files. But we should have, we should have the transcripts ready for all of those. So we should be able to, uh, to evaluate it. And also for the English ones we have the translations. So for the English ones [PERSON6], uh, would like you to evaluate not only the word error rate of the ASR. But also the machine translation quality or at the SLT even. Uh, with the translation quality into German and Czech. Both are available. (id=24) (PERSON6) Okay. (id=25) CONTINUE ON THE NEXT PAGE Table 25: Prompt used for extracting context information (continued). (PERSON13) We have these files ready. (id=26) (PERSON6) And so these, for like German audio and English [PROJECT1]. (id=27) (PERSON13) English, uh, input for English sound. We have the golden English transcript, so you can check the ASR. And we also have the translation into Czech and into German. So you can also evaluate directly the translation quality, uh, of that. (id=28) (PERSON6) Okay, yeah. (id=29) (PERSON13) Yeah, so this is, this is an important, uh, task, uh, to do, uh wr also for German and English audios. And another to do, uh, bleu or SLTF, uh, for, uh, German and Czech translations of, uh, English. (id=30) - Suppose the previous summary of PERSON6 contains the following information: {{ }} \"wanted information\": [], \"provided information\": [] - The thoughts and updated summary will be: {{ \"Thoughts\":\"PERSON6 responds to PERSON13s questions about the work done and the plan for the next week, which can be added to the provided information for PERSON6. PERSON6 responds to PERSON13s questions about the word error rate for the English and German transcripts, which can be added to the provided information for PERSON6. PERSON6 also asks about the golden transcripts for the English videos, which can be added to the wanted information for PERSON6.\" \"Updated Summary\": {{ \"wanted information\": [ \"Do we have the golden transcripts for the English videos?\" ], \"provided information\": \"wanted information\": [ \"Do we have the golden transcripts for the English videos?\" ], \"provided information\": [ {{ \"Context\": \"Respond to other participants question about the work done and the plan for the next week\", \"Information\": \"This week had quite less tasks to do. So first calculated the word error rate on Czech transcripts using three versions of Czech ASR created by [PERSON10]. And so yesterday [PERSON10] told me that there were huge huge mismatch between the golden transcript and its corresponding video. And he said to update me. And then we conducted few testing sessions with [PERSON11] and [PERSON18]. And they were not quite successful because the segmenters from [ORGANIZATION1] were still down and [PERSON12] today is working on them. And lastly, think did the input in the [PROJECT3] deliverable for the punctuator and through caser. Apart from the testing sessions to do this week so am waiting for new tasks.\" }}, {{ \"Context\": \"Respond to other participants question about the word error rate for the English and German transcripts\", path, and dont konw if we have the golden transcripts for the English videos.\" \"Information\": \"I have updated the German transcripts in its corresponding }} ] }} }} ## Note - You MUST follow the instructions and examples provided. - Similar to examples above, you should try to use original utterances as much as possible after removing noise words and polishing them for better readability. - You MUST NOT put the information provided by other participants or questions of other participants in the updated summary of {participant}. - You MUST NOT miss any important information provided by {participant} in the < Transcript Snippet>. - You MUST give the output in the required format. Table 26: Prompt used for extracting context information (continued). - You are an NLP expert agent tasked with generating an evaluation dataset to assess {person_id}s response abilities in the categories of Chime In, Explicit Cue, Implicit Cue, based on the provided transcript. - The conversation may involve multiple speakers, but your focus should solely be on {person_id}. - Given the transcript contains lengthy utterances, selectively include only the highest quality exchanges in the evaluation dataset. - Exclude chit-chat or unmeaningful utterances such as [\"emm,\" \"okay,\" \"mhm,\" \"uh-huh,\" \"yeah,\" \"oh,\" \"right,\" \"hmm\"] from the evaluation dataset. - Ensure that {person_id}s responses are substantive and meaningful. Exclude responses from {person_id} that are simple acknowledgments or confirmations like \"Yeah, yeah, definitely, yeah\" or \"Okay.\" - The transcript data is deidentified. Speakers and other named entities are identified by IDs in the format ENTITYNUMBER (e.g., PERSON1, Speaker1 or PROJECT3) or simply as ENTITY (e.g., PATH). ## Evaluation Type - Chime In: When {person_id} spontaneously contributes to the conversation without being directly prompted. - Usually Chime In is when {person_id} is not already engaged in the conversation but chimes in with relevant comment or question. - Explicit Cue: When {person_id}s name is specifically mentioned by another Speaker in utterance with ID, then {person_id} responds to clear and direct question or prompt towards {person_id}. - Implicit Cue: When {person_id}s name is not specifically mentioned by Speaker in utterance with ID, but {person_id} responds to less direct prompt or follows up on information that suggests response is needed. - Usually Implicit Cue is when {person_id} is already engaged in the conversation and responds to follow-up question from Speaker in utterance with ID. ## Output Format - Output must be in Json format. Here is the skeleton of the output format with explanation: - Explanation: Your reason for selecting the evaluation instance and for categorizing it. - Type: The category of the evaluation instance: Chime In, Explicit Cue, Implicit Cue. - Response IDs: The id or ids of the {person_id}s response from the transcript. Include all Response IDs that are relevant to the evaluation instance. If there are multiple Response IDs, separate them with commas. - ID: The utterance id that {person_id} responds to. - Speaker: The speaker of the utterance with the ID. - Maintain the chronological order of the transcript when generating the evaluation dataset. ID MUST precede Response IDs. - Response IDs must be from {person_id}s responses only and ID must be from the speakers utterance that {person_id} responds to. - Please return all suitable evaluation instances in the transcript. If you dont find any suitable instances for category, you can leave the evaluation dataset empty. Please ensure you have thought through the transcript carefully before leaving the evaluation dataset empty. ## Example: Below are two examples of transcript and the corresponding evaluation datasets generated to assess PERSON18s response abilities. You can refer to these examples when generating {person_id}s evaluation dataset. <Transcript> \"speaker\": \"Speaker 19\", \"content\": \"If you want, can resend it again. (id=71)\" \"speaker\": \"Speaker 13\", \"content\": \"Space tokeniser. <unintelligible/> Yes, so es essentially to answer your question in the email. We have to switch to and we have for the IWSLT. We have to switch to SacreBLEU and SacreBLEU does its own tokenisation before scoring. So there is no-. Lets lets simply forget NLTK bleu score. That is not reliable. (id=72)\" \"speaker\": \"Speaker 18\", \"content\": \"Yes, but-. (id=73)\" \"speaker\": \"Speaker 19\", \"content\": \"Yes, but we can combine our tokeniser with NLTK. (id=74)\" \"speaker\": \"Speaker 13\", \"content\": \"Uf. Lets not do that. Lets just forget it. Lets lets just use SacreBLEU. (id=75)\" \"speaker\": \"Speaker 19\", \"content\": \"Okay. (id=76)\" \"speaker\": \"Speaker 18\", \"content\": \"I have one comment about it. (id=77)\" \"speaker\": \"Speaker 13\", \"content\": \"Mhm. Yeah. (id=78)\" \"speaker\": \"Speaker 18\", \"content\": \"You sh should use tokeniser before enverse segmenter. (id=79)\" \"speaker\": \"Speaker 13\", \"content\": \"Yes, thats it. Yeah. (id=80)\" \"speaker\": \"Speaker 18\", \"content\": \"Because its much better. Because it can rely on the on the dots and commas and question marks and so on. And you can you can check my script which does tokeniser, enverse segmenter and then de-tokeniser. And here is the path in the document.And-. (id=81)\" \"speaker\": \"Speaker 13\", \"content\": \"Yeah. (id=82)\" \"speaker\": \"Speaker 18\", \"content\": \"And its its using the Moses seg tokeniser and detokeniser. And it needs the the language tag as the first argument and then reference. (id=83)\" \"speaker\": \"Speaker 13\", \"content\": \"Yeah. So [PERSON19], do you do you fo-? Do you understand? (id=84)\" \"speaker\": \"Speaker 19\", \"content\": \"Yeah. (id=85)\" CONTINUE ON THE NEXT PAGE Table 27: Prompt used for extracting test cases from meeting transcript. <Evaluation Dataset> [ \"Explanation\": \"PERSON18s utterance is informative and suitable for evaluation dataset. PERSON18 spontaneously contributes to the conversation without being directly prompted. This is Chime In instead of Implicit Cue since [PERSON18] is not already engaged in the conversation.\", \"Type\": \"Chime in\", \"Response ID\": [77, 79, 81, 83] \"ID\": 76, \"Speaker\": \"Speaker 19\", ] ## Example 2 <Transcript> \"speaker\": \"Speaker 13\", \"content\": \"Yes, so need to review these and the internal deadline is in 6 days from now. Uh, so, hopefully will get back to all of you. To each of you independently towards the end of the week if there is anything unclear. So that we meet the internal deadline on the 8th. Yeah, okay. Great. Uh. So [PERSON18], what what is your progress? (id=117)\" \"speaker\": \"Speaker 18\", \"content\": \"Hmhm. Yes, and by reading the papers found an interesting tool. (id=118)\" \"speaker\": \"Speaker 13\", \"content\": \"Mhm. (id=119)\" \"speaker\": \"Speaker 18\", \"content\": \"I found out that its possible to measure out the speech rate by cutting the syllables. And there is one tool. One patent tool, which can detect the gender of speaker and the speech rate. (id=120)\" \"speaker\": \"Speaker 13\", \"content\": \"Mhm. (id=121)\" \"speaker\": \"Speaker 18\", \"content\": \"And some other characteristics. So we can try it and make dashboard out of it. (id=122)\" \"speaker\": \"Speaker 13\", \"content\": \"Mhm. Thats thats useful thing. Uh, and later on we could even create models like-. If we if we recognise that someone is speaking too fast, we could use like harsher summarisation. (id=123)\" \"speaker\": \"Speaker 18\", \"content\": \"Yes. (id=124)\" \"speaker\": \"Speaker 13\", \"content\": \"So we could be reducing reducing their speech mole with different model. (id=125)\" \"speaker\": \"Speaker 18\", \"content\": \"Yes, and there was also speech modes. Like whether it was angry or normal and so on. (id=126)\" \"speaker\": \"Speaker 13\", \"content\": \"Mhm. (id=127)\" \"speaker\": \"Speaker 18\", \"content\": \"But have no idea how the tool works in practice. I saw it only in Gi GitHub and buy it. (id=128)\" \"speaker\": \"Speaker 13\", \"content\": \"Yeah, uh. (id=129)\" \"speaker\": \"Speaker 18\", \"content\": \"So we can try it and make dashboard out of it. (id=130)\" <Evaluation Dataset> [ \"Explanation\": \"PERSON18s utterance is informative and suitable for evaluation dataset. PERSON18 was directly prompted by Speaker 13. This is an Explicit Cue.\", \"Type\": \"Explicit Cue\", \"Response ID\": [118, 120, 122, 124, 126, 128, 130] \"ID\": 117, \"Speaker\": \"Speaker 13\", ] - Please refer to the examples provided to ensure consistency and coherence in generating the evaluation dataset. The evaluation dataset must be in json format. Table 28: Prompt used for extracting test cases from meeting transcript (continued)."
        }
    ],
    "affiliations": [
        "Microsoft",
        "Northeastern University, China",
        "Peking University, China"
    ]
}