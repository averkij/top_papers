{
    "paper_title": "Evaluate Bias without Manual Test Sets: A Concept Representation Perspective for LLMs",
    "authors": [
        "Lang Gao",
        "Kaiyang Wan",
        "Wei Liu",
        "Chenxi Wang",
        "Zirui Song",
        "Zixiang Xu",
        "Yanbo Wang",
        "Veselin Stoyanov",
        "Xiuying Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Bias in Large Language Models (LLMs) significantly undermines their reliability and fairness. We focus on a common form of bias: when two reference concepts in the model's concept space, such as sentiment polarities (e.g., \"positive\" and \"negative\"), are asymmetrically correlated with a third, target concept, such as a reviewing aspect, the model exhibits unintended bias. For instance, the understanding of \"food\" should not skew toward any particular sentiment. Existing bias evaluation methods assess behavioral differences of LLMs by constructing labeled data for different social groups and measuring model responses across them, a process that requires substantial human effort and captures only a limited set of social concepts. To overcome these limitations, we propose BiasLens, a test-set-free bias analysis framework based on the structure of the model's vector space. BiasLens combines Concept Activation Vectors (CAVs) with Sparse Autoencoders (SAEs) to extract interpretable concept representations, and quantifies bias by measuring the variation in representational similarity between the target concept and each of the reference concepts. Even without labeled data, BiasLens shows strong agreement with traditional bias evaluation metrics (Spearman correlation r > 0.85). Moreover, BiasLens reveals forms of bias that are difficult to detect using existing methods. For example, in simulated clinical scenarios, a patient's insurance status can cause the LLM to produce biased diagnostic assessments. Overall, BiasLens offers a scalable, interpretable, and efficient paradigm for bias discovery, paving the way for improving fairness and transparency in LLMs."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 2 ] . [ 1 4 2 5 5 1 . 5 0 5 2 : r Evaluate Bias without Manual Test Sets: Concept Representation Perspective for LLMs Lang Gao12 Kaiyang Wan1 Wei Liu2 Chenxi Wang1 Zirui Song1 Zixiang Xu1 Yanbo Wang1 Veselin Stoyanov1 Xiuying Chen1 1MBZUAI 2Huazhong University of Science and Technology {Lang.Gao, Xiuying.Chen}@mbzuai.ac.ae"
        },
        {
            "title": "Abstract",
            "content": "Bias in Large Language Models (LLMs) significantly undermines their reliability and fairness. We focus on common form of bias: when two reference concepts in the models concept space, such as sentiment polarities (e.g., positive and negative), are asymmetrically correlated with third, target concept, such as reviewing aspect, the model exhibits unintended bias. For instance, the understanding of food should not skew toward any particular sentiment. Existing bias evaluation methods assess behavioral differences of LLMs by constructing labeled data for different social groups and measuring model responses across them, process that requires substantial human effort and captures only limited set of social concepts. To overcome these limitations, we propose BIASLENS , test-set-free bias analysis framework based on the structure of the models vector space. BIASLENS combines Concept Activation Vectors (CAVs) with Sparse Autoencoders (SAEs) to extract interpretable concept representations, and quantifies bias by measuring the variation in representational similarity between the target concept and each of the reference concepts. Even without labeled data, BIASLENS shows strong agreement with traditional bias evaluation metrics (Spearman correlation > 0.85). Moreover, BIASLENS reveals forms of bias that are difficult to detect using existing methods. For example, in simulated clinical scenarios, patients insurance status can cause the LLM to produce biased diagnostic assessments. Overall, BIASLENS offers scalable, interpretable, and efficient paradigm for bias discovery, paving the way for improving fairness and transparency in LLMs: (cid:135)Github."
        },
        {
            "title": "Introduction",
            "content": "LLMs are central to modern NLP for their strong generalization and generation abilities, and are increasingly applied in domains such as education [1] and healthcare [2]. However, they often inherit and amplify social biases from training data, leading to fairness issues. This work focuses on common yet under-measured bias: when an LLM asymmetrically links reference concepts (e.g., female and male) with an unrelated target (e.g., doctor), it reveals biased associations [3, 4]. Existing bias evaluation frameworks like testing with StereoSet [5] and WinoBias [6], SEAT [7], and more recently, BVF [8], and CLIMB [9] evaluate bias by comparing model behavior across predefined concepts using curated datasets. For instance, they assess probability gaps between male doctor and female doctor. These methods are labor-intensive and rely on domain-specific data, limiting their use in under-resourced scenarios (see Figure 1). To overcome these limitations, we shift the focus from behavioral differences to conceptual representations, eliminating the reliance on manual test sets and enabling fully automatic evaluation. This Corresponding author. Preprint. Under review. Figure 1: Comparison between traditional behavior-based and our representation-based bias evaluation paradigms. Our approach enables simple, test-set-free, concept-level analysis using activations and synthetic data, even when no suitable test set exists. approach is inspired by early work on bias analysis in static word embeddings (e.g., WORD2VEC), where bias is typically detected by comparing vector similarities between words, for example, between male and programmer [10, 11]. However, in LLMs, biases are no longer confined to single words. Their representations often span multiple tokens or cannot be expressed by words at all. As result, bias analysis in LLMs must move beyond tokens and focus on higher-level, abstract concepts. Building on this idea, we propose BIASLENS , bias evaluation framework for LLMs that requires no manually constructed test sets. By measuring geometric alignment between concept vectors, BIASLENS acts as lens to uncover bias in the models internal concept space. As shown in Figure 1, it operates without labeled test data and generalizes across diverse concepts. For each of target concept (e.g., doctor) and two references (e.g., male, female), following the Concept Activation Vector (CAV) method [12, 13], we compute direction in the activation space that represents the transition from random representations to those that are concept-relevant. As CAVs are not inherently interpretable [14], we enhance interpretability by extracting final-layer activations before and after CAV steering, then projecting both into high-dimensional sparse space via pre-trained Sparse Autoencoder (SAE) [15, 16]. Their normalized difference forms an interpretable concept shift vector. We repeat this process for the target and reference concepts and compute cosine similarities between their vectors. The absolute difference between the two similarity scores defines directional bias, capturing asymmetric alignment in the models representation space. Our experiments demonstrate that BIASLENS aligns well with traditional behavior-based evaluations in various LLMs, even without access to manual test samples. We also apply it to analyze LLMs in both general and high-risk domains, uncovering previously unreported biases that align with real-world expectations and partially corroborate findings from sociolinguistic studies. In summary, our key contributions are as follows: we propose novel bias formulation based on the geometric alignment between intrinsic concept vectors, which removes the need for behavior-level comparisons; we introduce BIASLENS , test-free and concept-general framework that leverages CAVs and SAEs to extract and compare intrinsic representations; and we provide empirical validation across multiple LLMs and application domains, demonstrating that BIASLENS aligns well with existing bias metrics while uncovering new, plausible biases with real-world implications."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 Bias in LLMs and Its Evaluation Bias in LLMs. In sociology, bias is an irrational or unfair attitude toward group, often rooted in stereotypes or structural inequality [17, 18]. LLMs inherit and amplify such bias in systematic 2 ways [19, 20],such as stereotypical content [6, 21], value-laden comparisons [22, 23], and preferences [24, 25] during generation. For example, an LLM may associate certain professions with specific genders or races [26, 3, 27]. Bias also affects practical tasks. In the LLM-as-a-judge setting, models favor answers which are longer or include citation-style contents [28]. In high-stake domains, the consequences are even severer. For example, in medicine, some LLMs provide inaccurate advice when patient race is mentioned [29, 30, 31]; In finance, LLMs used for credit scoring can generate unfavorable assessments for disadvantaged groups [32, 33]. We view these problems as arising from unintended correlations between intrinsic concept representations, where concepts like gender and occupation become entangled, and BIASLENS explicitly targets this type of representational bias. Bias evaluation methods. Current bias evaluation methods for LLMs are commonly divided into extrinsic and intrinsic behavior methods [20, 34]. While this terminology is widely adopted, the distinction fundamentally reflects how these methods assess behavioral differences across contexts or groups. Extrinsic methods examine output-level variations, such as changes in generated text or classification accuracy across demographic groups. Representative examples include evaluating biases using WINOBIAS [6] and STEREOSET [35]. Intrinsic methods focus on internal representations, analyzing changes in token probabilities [36, 37] or embedding space geometry [38, 39] under controlled conditions. Despite their differences, both types rely on grouped inputs and predefined bias axes, and both ultimately assess how the models behavior, either extrinsically like accuracy, or intrinsically like probabilities, responds to shifts in contextual variables. In contrast, early work on static word embeddings sidestepped test sets entirely by directly measuring semantic geometry [40, 41]. Inspired by this, BIASLENS evaluates bias via directional alignment of intrinsic concept vectors, requiring neither labeled data nor group-specific prompts. 2.2 Mechanistic Interpretability for LLMs Concept Activation Vectors (CAVs). CAVs were first introduced by [12] as tool for interpreting neural representations. One can train linear classifier that separates activations that contain concept from random activations. The classifier normal vectors are then defined as CAVs [12, 42]. Researchers obtain CAVs for LLMs by contrasting text with and without the target concept and training on intermediate activations [43, 44]. For any user-defined concept or feature that can be systematically manifested in dataset, corresponding CAV can be derived [12]. CAVs are widely adopted for activation steering in LLMs [45, 46, 47], where adding or subtracting CAVs from internal activations at inference time can guide model outputs toward or away from the associated concept [46, 43, 13]. While much prior work emphasizes this steering effect, our interest lies in their capacity to characterize internal concept representations and to serve as probe into modelintrinsic properties. Despite their flexibility and expressive power, CAV directions are not inherently interpretable [14], necessitating auxiliary tools to map them to semantically meaningful space. Sparse Autoencoders (SAEs). An SAE is non-linear, symmetric autoencoder that reconstructs inputs through an overcomplete, sparsely activated latent layer [48]. When trained on the intermediate activations of LLMs, SAEs decompose dense, polysemantic representations into sparse features that activate for distinct, human-interpretable concepts [15, 49, 50, 51]. Such sparse units have been used to analyze model behavior [15], identify causal features for prediction [52], and localize intrinsic drivers of preference or reward [52]. Preliminary work uses SAE features to detect specific biases [53, 54]. However, these methods rely on manually selecting small subset of bias-related features from the sparse representations produced by SAEs, which limits the coverage of bias types and hinders systematic evaluation and cross-bias comparison. In contrast, BIASLENS extracts concept vectors without requiring pre-interpreted features, enabling generalized bias analysis."
        },
        {
            "title": "3 Method",
            "content": "3.1 Concept Representation-based Bias Formulation Traditional bias evaluation often defines bias as behavioral differences exhibited by models under different demographic contexts [55, 56], such as different accuracies, probability distributions, and outputs. For example, model may assign different prediction probabilities to the sentences he is doctor and she is doctor. Such behavioral evaluations encompass variety of indicators, 3 Figure 2: Overview of BIASLENS . running example using the concept doctor illustrates the three main steps of our method: (1) CAV derivation: train linear classifiers at each layer using random and doctor-related sentences, and use the classifier weights as CAVs; (2) Concept representation extraction: extract model activations before and after steering with doctor CAVs, project them into SAE space, and subtract the normalized vectors to obtain the concept representation; (3) Bias score calculation: repeat the process for male and female, and compute the asymmetry in similarity between doctor and each of them. including but not limited to perplexity, probability distribution shifts, or task-specific performance metrics. Formally, given target concept and two reference contexts r1 and r2, behavioral bias is defined as the difference in model behaviors over sets of inputs constructed under these contexts: Biasbehavioral(t; r1, r2) = ExXr [f (x, t)] ExXr2 [f (x, t)], (1) where Xr1 and Xr2 are collections of input sentences reflecting contexts r1 and r2, and (x, t) denotes the models behavior related to concept in input x. While effective in specific scenarios, such behavior-level approaches are difficult to scale, and model performance often heavily depends on the design of the test set . To overcome these limitations, we propose concept representation-based definition of bias based on model features, which we term conceptual correlation bias. Instead of relying on input-output behavior, this formulation directly compares how target concept aligns with different reference concepts in the models concept representation space. Formally, given concept vectors t, r1, and r2, we define: Biasconceptual(t; r1, r2) = Diff(cid:0)Align(t, r1), Align(t, r2)(cid:1), (2) where Align(a, b) measures the alignment between two concepts (e.g., via cosine similarity), and Diff(x, y) quantifies the degree of asymmetry. This definition enables bias evaluation that is dataindependent, domain-general, and applicable to wide range of semantic relationships. 3.2 BIASLENS Framework BIASLENS is constructed based on our formulation of bias. Unlike prior methods, it bypasses behavioral observations and therefore requires no manually constructed test data. Given potential bias, we identify target concept and pair of reference concepts. BIASLENS then computes the alignment difference between their representations. As illustrated in Figure 2, BIASLENS consists of three steps: CAV derivation, concept representation extraction, and bias score calculation. 3.2.1 CAV Derivation Following [12], we define the CAV as linear decision boundary that separates activations corresponding to target concept from those corresponding to unrelated content. To compute the CAV, we first construct probing dataset consisting of two balanced sets of sentences: positive examples containing the target concept are generated by GPT-4o [57], while negative examples are sampled from the random corpus OpenWebText[58]. Details on the prompt are in AppendixB.1 and B.2. We feed each sentence into the target LLM and extract the embedding of the last token at each layer l, denoted as the activation vector ak. Following prior work [59, 60], we use the last token embedding as the activation since it is understood to capture how the LLM interprets the semantic meaning of the entire sentence. Each activation ak is associated with binary label yk {0, 1}, where the positive class indicates that the sentence contains the target concept, and the negative class indicates otherwise. We then train logistic regression classifier to predict yk from ak by minimizing the (cid:0)yk, σ(cid:0)w(l)ak + b(l)(cid:1)(cid:1), where σ() is the average cross-entropy loss: minw(l), b(l) sigmoid function. Details are shown in Appendix B.3. Finally, we define the CAV for layer as the normalized weight vector: v(l) = w(l) w(l) . This vector v(l) points from representations of general language towards the representation of the target concept. k=1 LCE (cid:80)N 1 3.2.2 Concept Representation Extraction Algorithm 1 Concept Steering Across Layers After obtaining the CAVs, we steer the model along the directions v(l) to inject the concept, and use an SAE to construct concept representation that is both structured and interpretable. This process consists of two steps: CAV-based steering and SAE-based extraction. Require: LLM, input x, CAVs {v(l)}, classifiers {f (l)}, threshold τ , step size δ while (l)(a(l)) < τ do a(l) a(l) + δ v(l) 1: a(1) LLM.Layer1(x) 2: for = 1 to do 3: 4: 5: 6: 7: end for 8: return a(n) end while a(l+1) LLM.Layerl+1(a(l)) CAV-based steering. As shown in Figure 2, once the CAV is obtained, we only need single sentence to perform concept representation extraction, from which the final bias score can be computed. Considering that the same concept may exhibit different biases across contexts, we design prompts that clearly specify the intended scenario and naturally introduce both the target and reference concepts. For example, to study gender bias associated with the occupation doctor, we use prompts such as This is description of the person in general settings, and This is description of the movie character in movie review contexts. Full prompt examples under different scenarios in this paper are provided at Table 5 in Appendix B.4. To maximize the effect of concept injection, we apply steering at every layer. For each layer l, we iteratively shift the activation vector a(l) in the direction of the CAV v(l) for step δ = 1, increasing the probability of predicting the target concept. This process continues until the prediction confidence exceeds threshold of τ = 0.999. The process is formalized in Algorithm 1. SAE-based extraction. Since the CAV steering direction is not inherently interpretable, we resort to SAE [15], commonly used tool for disentangling and interpreting internal representations, to project the CAV onto semantically meaningful activation subspaces. We denote the model before steering as LLMori, and the concept-steered model as LLMsteer. We extract the final-layer activations from both LLMori and LLMsteer, and denote them as aori and asteer. These activations are then projected into high-dimensional, sparse semantic space using an SAE. An SAE is symmetric linear network consisting of an encoder and decoder [61, 15]. The encoder maps the input to sparse code via linear transformation followed by an activation function ϕ() : = E(a) = ϕ(WSAE + bSAE), (3) where WSAE Rkd, d. This projection is generally believed to reveal set of interpretable and readable concepts, such as the concept doctor. The decoder reconstructs the original activation from the sparse code, ensuring that the learned features retain the semantic information of the input. Here we only utilize the encoder E(). Let zori = E(aori) and zsteer = E(asteer) be the corresponding sparse representations. Each dimension in is designed to reflect an independent semantic concept. Since CAV steering targets only the injected concept, changes between zori and zsteer should mainly occur in concept-relevant dimensions. Therefore, we normalize both vectors and compute their difference as = Norm(zsteer) Norm(zori), and interpret the resulting vector as the concept representation vector. It highlights dimensions most affected by the concept injection. 5 Figure 3: Validation of concept representation extraction. (a) CAV-based steering activates relevant features, which can be captured by the SAE. (b) Normalizing and differencing the SAE representations improve the ranking of concept-relevant features, ensuring the extracted direction is generally controlled by the dimensions of these features. 3.2.3 Bias Score Calculation In 3.1, we define bias as the difference in alignment between target concept and pair of reference concepts. We expect that strongly coupled concepts should have similar concept representations. This implies that their concept vectors should point in similar directions, forming small angles in space. In contrast, loosely related or independent concepts should produce larger angles. Following this intuition, we quantify bias as the difference in alignment between target concept and set of reference concepts. Let Ctarget be the target concepts representation vector, and Cref1, Cref2 be two reference concepts representation vectors, we compute the bias score as: (cid:12) (cid:12)cos ( Ctarget, Cref1) cos ( Ctarget, Cref2) (cid:12) Sbias(target) = (cid:12) (cid:12) (cid:12) . (4) This score captures how unequally the target concept aligns with the reference set. Larger values indicate stronger bias. 3.3 Effectiveness of Concept Representation Extraction in BIASLENS Concept representation extraction is key step in our method, as the resulting vector is directly used for similarity-based bias scoring. To illustrate how each component contributes, we analyze two main stages: CAV-based steering and SAE-based extraction. This analysis is based on single case study. We use Gemma 2 2B and construct CAV for the concept food following 3.2.1. We also utilize the SAE for the last layer, whose configuration details in Appendix C.1. The input prompt is This is comment about an experience at theater:. CAV-Based Steering Effects Can be Interpreted by SAE. We interpret activated sparse features using Neuronpedia [62], repository of natural language descriptions for SAE dimensions. Features with positive activation are classified as food-related or unrelated by GPT-4o-Mini (Appendix B.5). Figure 3(a) shows the results. Without steering, the model generates no food-related content, and 100% of activated features are unrelated to food. After steering, the output includes food-related descriptions, and 14.74% of the activated features are labeled as food-related. This suggests that steering shifts the models understanding towards the input in semantically meaningful way, which is detectable through the SAE. We further show in Appendix B.6 that even when the output remains unchanged, steering still increases the number of food-related activations in the SAE space. This indicates that the steering effect is consistently captured at the representation level, even if not always reflected in generation. SAE-Based Extraction Amplifies Concept-Relevant Dimensions. The extracted concept vector is directly used for similarity computation, where more salient features have stronger influence on the score. To progressively increase the salience of features related to the target concept, we apply 6 Table 1: Comparison between extrinsic and intrinsic bias metrics across models. Values represent Spearman correlation with BIASLENS . Our metric shows positive correlations with most baseline metrics. For each category, the metric with the highest correlation is bolded. Model Extrinsic Metrics Intrinsic Metrics F1-Diff EOD I.F. G.F. SEAT Perplexity 0.9429 Gemma 2 2B 0.9429 Gemma 2 9B Llama 3.1 8B 0.7143 0.1429 0.8857 -0.9429 0.4286 0.7714 -0.7143 0.2571 0.7714 1.0000 0.7893 0.7276 0.4234 0.4897 0.3083 0. three successive operations before forming the final concept representation: (1) extract SAE-encoded activations before and after CAV steering; (2) apply normalization to both; and (3) compute the difference between the two normalized vectors. To examine their effects, we evaluate four variants of the SAE encoding: (i) original zori, (ii) steered zsteer, (iii) differences zsteer zori, and (iv) normalized differences Norm(zsteer) Norm(zori). For each variant, we sort all features by descending value and compute cumulative distribution over the concept-relevant dimensions, based on Neuronpedia annotations. Figure 3(b) shows the resulting cumulative distribution curves. We further quantify feature salience using the area under each curve (AUC). Higher AUC values indicate stronger prominence of concept-relevant features in the ranking. The AUC increases from 0.4457 for the steered encoding, to 0.7825 after subtraction, and reaches 0.7982 after normalization and subtraction. These results show that our extraction process effectively amplifies the semantic signal of the target concept. We also provide discussion on the robustness of BIASLENS to probing data in Appendix B.7."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Experimental Setup We evaluate BIASLENS on three pretrained LLMs of diverse architectures and sizes: Gemma 2 2B [63], Gemma 2 9B, and Llama 3.1 8B [64]. Full models and SAE settings are available in Appendix C.1. We compare BIASLENS with six existing metrics, referred to as either extrinsic behavioral metrics or intrinsic behavioral metrics, following the taxonomy in 2.1 and to emphasize their contrast with BIASLENS . The extrinsic behavior metrics are computed from classification outputs on sentiment classification datasets, while intrinsic methods focus on internal representations, analyzing changes in token probabilities. Extrinsic behavioral metrics. We compare BIASLENS with four widely-used extrinsic behavioral metrics: F1-Diff [6], Equal Opportunity Difference (EOD) [65, 66], Individual Fairness (I.F.) [67, 68], and Group Fairness (G.F.) [67, 68]. The implementation details of these methods are available in Appendix C.2.1. These metrics are computed on model outputs over Yelp [69] and IMDB [70] datasets, using sentiment classification as the downstream task. Following [71], we annotate each sample with one of six concepts (e.g. food, service, etc) as target concepts, and treat sentiment polarities as reference concepts, as detailed in Appendix C.3.1. We then compute bias metrics separately for each concept. For instance, suppose we classify Yelp reviews that mention food versus those that do not. F1-Diff quantifies whether the model performs better sentiment classification on one group than the other; EOD examines whether samples with positive emotion from both groups are equally likely to be correctly classified; I.F. measures how sensitive the models prediction is when the sentiment context (e.g., delicious vs. bland) is changed within the same structural template; and G.F. evaluates whether the overall sentiment prediction distributions differ systematically between the two groups. Together, these metrics reflect different aspects of behavioral bias. 7 Figure 4: Spearman correlation matrices between BIASLENS and four extrinsic behavioral metrics on (a) Gemma 2 2B, (b) Gemma 2 9B, and (c) Llama 3.1 8B. Each matrix shows pairwise correlations computed over 6 target concepts. Intrinsic behavioral metrics. We compare BIASLENS with two intrinsic behavior tests: SEAT [7] and the Perplexity Test [72], both applied to the WinoBias dataset [6]. These tests use occupationrelated prompts (e.g., He is doctor vs. She is doctor) to evaluate gender bias, where occupation is the target concept and gender the reference. SEAT measures differences in cosine similarity between target and attribute sentences. We use gendered occupation sentences as targets and construct attribute sets based on template-filled occupations, following [7]. The reported metric is the effect size. Perplexity Test measures asymmetry in language modeling behavior by comparing conditional perplexities of gendered prompt pairs. Each pair differs only in pronoun and occupation reference. two-sample t-test is applied to the resulting perplexity values, and we use the t-value as the bias score. Only statistically significant comparisons (p 0.05) are retained. The implementation details of the tests are available in Appendix C.2.2. Dataset construction details are in Appendix C.3.2. 4.2 Consistency with Established Bias Measures Consistency with extrinsic behavioral metrics. Figure 4 and Table 1 together show that BIASLENS exhibits strong and consistent agreement with established bias metrics across different models and concept types. 1) BIASLENS exhibits positive correlations with all extrinsic behavioral metrics on Gemma 2 2B and Gemma 2 9B. In table 1, correlations of BIASLENS with F1-Diff reach 0.9429 on both Gemma 2 2B and 9B, alongside moderate positive correlations with I.F. (0.4286 and 0.7714) and G.F. (0.2571 and 0.7714). 2) BIASLENS consistently achieves the highest correlation with F1-Diff across all models. F1-Diff is widely-used output-level bias measure. Its consistently strong correlation with BIASLENS , especially for both Gemma models, whose correlations are close to 1 in Table 1, supporting the validity of BIASLENS as behavioral bias proxy. Even on Llama 3.1 8B, where metric disagreements are more pronounced (right panel of Figure 4), the correlation remains relatively strong at 0.7143. 3) BIASLENS often surpasses F1-Diff in aligning with other metrics. Figure 4(a) shows that BIASLENS correlates with EOD at 0.43 on Gemma 2 2B, compared to only 0.26 for F1-Diff. On Gemma 2 9B (Figure 4b), BIASLENS achieves 0.60 with EOD and 0.77 with both I.F. and G.F., again matching or outperforming F1-Diff. On Llama 3.1 8B (Figure 4c), BIASLENS improves the correlation with G.F. from 0.71 (F1-Diff) to 1.0. Meanwhile, F1-Diff shows strong negative correlations with other metrics like EOD (0.6) and I.F.(0.83), suggesting its sensitivity to task-specific signals. In contrast, BIASLENS yields more moderate correlations, such as 0.71 with I.F., reducing the gap by 0.12. This indicates that BIASLENS balances diverse fairness signals instead of replicating one specific metric. Consistency with intrinsic behavior metrics. For each baseline, we select occupations with statistically significant bias (p 0.05), and compute the correlation between BIASLENS and the corresponding bias strength metric (i.e., SEATs effect size or perplexity t-value); results are shown in the right half of Table 1. BIASLENS achieves strong correlation with SEAT across all models (e.g., 0.79 on Gemma 2 2B, 0.73 on Gemma 2 9B), reinforcing its validity as an association-aware measure. While correlations with perplexity t-value are lower (e.g., 0.49 on Gemma 2 2B), they remain positive and stable across models. This suggests that BIASLENS captures consistent bias signals even when behavioral and representational patterns differ, offering robust alternative. 8 Table 3: Bias scores between reference and target concepts in medical domains, computed using BIASLENS on Gemma 2 2B. For each target concept, the highest bias score across all reference concept pairs is highlighted in mid blue and the second highest in light blue . Target Concepts vs. Reference Concepts illness pain cancer surgery male vs female rich vs poor white vs black public insurance vs private native vs non-native 0.1008 0.0787 0.0124 0.0783 0. 0.1174 0.0634 0.0008 0.0681 0.0478 0.0588 0.0145 0.0132 0.0861 0.0360 0.0846 0.0471 0.0301 0.1134 0.0319 mental illness 0.0823 0.0862 0.0321 0.0438 0.0077 All bias scores used in this analysis are reported in Appendix D, with extrinsic metric results shown in Table 10 and intrinsic metric results in Table 11 Table 13. 4.3 Discovering New Forms of Bias with BIASLENS To demonstrate the extensibility and practical utility of BIASLENS , we apply it to set of underexplored but socially relevant bias types. Building on prior studies [73, 74], we focus on the medical and educational domainstwo areas where model predictions may have real-world consequences, yet concept-level evaluations remain lacking. While existing work has preliminarily examined bias in these domains [75], to the best of our knowledge, no prior effort has constructed concept-level assessments that reveal associations between specific reference groups and domain-specific concepts. In the medical domain, target concepts correspond to illness-related categories (e.g., cancer, mental illness), while reference concepts span demographic contrasts such as gender, race, income, and insurance status. As shown in Table 3, we observe prominent genderand income-related biases in how the model associates concepts like illness and surgery with demographic groups. These trends align with sociological findings [76, 77], where perceptions of chronic or mental health issues often vary across populations. For instance, the model exhibits greater alignment between insured patients and surgery, suggesting biased clinical assumptions. Results in the education domain, as well as findings on other models, are presented in Appendix E. 4.4 Automation and Efficiency BIASLENS is both labor-free and highly efficient. Manual methods for bias evaluation are costly. For example, building the 12,000-example REDDITBIAS [72] dataset required three annotators with university degrees and one PhD student, each labeling around 4,000 samplesincurring substantial annotation, recruitment, and training costs. In contrast, BIASLENS requires only the definition of target concepts and few prompt templates. All other steps are fully automated. This design also brings major efficiency gains. Manual baselines take 1,0002,000 minutes to annotate (assuming 1 minute per example) and 1,0002,000 seconds for inference (at 1 input per second). BIASLENS generates 450 short prompts (under 10 tokens) in 450 seconds. CAV training takes no more than 15 seconds, and classifier training and evaluation take under 30 seconds. Overall, BIASLENS achieves approximately 50 speedup in both dataset construction and testing."
        },
        {
            "title": "5 Conclusion",
            "content": "This work introduces BIASLENS , test-set-free framework for evaluating bias in LLMs. By redefining bias as asymmetric alignment between target concept and reference pair, we extract sparse, interpretable concept vectors and measure their similarity differences to quantify bias. BIASLENS eliminates the need for curated test sets and supports flexible, context-aware analysis across domains. Experiments show that BIASLENS maintains high consistency with both extrinsic behavioral and intrinsic behavioral bias metrics, while alleviating conflicts among them. Moreover, 9 BIASLENS enables the discovery of underexplored and subtle bias patterns in real-world settings. This highlights its potential as practical, extensible tool aligned with the goals of usable XAIleveraging interpretability not only for explanation, but also for building robust, systematic evaluation systems."
        },
        {
            "title": "References",
            "content": "[1] Iain Weissburg, Sathvika Anand, Sharon Levy, and Haewon Jeong. Llms are biased teachers: Evaluating llm bias in personalized education. arXiv preprint arXiv:2410.14012, 2024. [2] Mahmud Omar, Shelly Soffer, Reem Agbareia, Nicola Luigi Bragazzi, Donald Apakama, Carol Horowitz, Alexander Charney, Robert Freeman, Benjamin Kummer, Benjamin Glicksberg, et al. Sociodemographic biases in medical decision making by large language models. Nature Medicine, pages 19, 2025. [3] Hadas Kotek, Rikker Dockum, and David Sun. Gender bias and stereotypes in large language models. In Proceedings of the ACM collective intelligence conference, pages 1224, 2023. [4] Hannah Rose Kirk, Yennie Jun, Filippo Volpin, Haider Iqbal, Elias Benussi, Frederic Dreyer, Aleksandar Shtedritski, and Yuki Asano. Bias out-of-the-box: An empirical analysis of intersectional occupational biases in popular generative language models. Advances in neural information processing systems, 34:26112624, 2021. [5] Moin Nadeem, Anna Bethke, and Siva Reddy. Stereoset: Measuring stereotypical bias in pretrained language models. arXiv preprint arXiv:2004.09456, 2020. [6] Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Ordonez, and Kai-Wei Chang. Gender bias in coreference resolution: Evaluation and debiasing methods. In Marilyn Walker, Heng Ji, and Amanda Stent, editors, Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 1520, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. [7] Chandler May, Alex Wang, Shikha Bordia, Samuel R. Bowman, and Rachel Rudinger. On measuring social biases in sentence encoders. In Jill Burstein, Christy Doran, and Thamar Solorio, editors, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 622628, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. [8] Yiran Liu, Ke Yang, Zehan Qi, Xiao Liu, Yang Yu, and ChengXiang Zhai. Bias and volatility: statistical framework for evaluating large language model's stereotypes and the associated generation inconsistency. In A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang, editors, Advances in Neural Information Processing Systems, volume 37, pages 110131110155. Curran Associates, Inc., 2024. [9] Yubo Zhang, Shudi Hou, Mingyu Derek Ma, Wei Wang, Muhao Chen, and Jieyu Zhao. Climb: benchmark of clinical bias in large language models, 2024. [10] Hossein Azarpanah and Mohsen Farhadloo. Measuring biases of word embeddings: What similarity measures and descriptive statistics to use? In Yada Pruksachatkun, Anil Ramakrishna, Kai-Wei Chang, Satyapriya Krishna, Jwala Dhamala, Tanaya Guha, and Xiang Ren, editors, Proceedings of the First Workshop on Trustworthy Natural Language Processing, pages 814, Online, June 2021. Association for Computational Linguistics. [11] Sunipa Dev, Tao Li, Jeff Phillips, and Vivek Srikumar. On measuring and mitigating biased inferences of word embeddings. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 76597666, 2020. [12] Been Kim, Martin Wattenberg, Justin Gilmer, Carrie J. Cai, James Wexler, Fernanda B. Viégas, and Rory Sayres. Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (tcav). In Proceedings of the 35th International Conference on Machine Learning, 2018. [13] Hanyu Zhang, Xiting Wang, Chengao Li, Xiang Ao, and Qing He. Controlling large language models through concept activation vectors. arXiv preprint arXiv:2501.05764, 2025. [14] Harry Mayne, Yushi Yang, and Adam Mahdi. Can sparse autoencoders be used to decompose and interpret steering vectors? CoRR, abs/2411.08790, 2024. 10 [15] Robert Huben, Hoagy Cunningham, Logan Riggs Smith, Aidan Ewart, and Lee Sharkey. Sparse autoencoders find highly interpretable features in language models. In The Twelfth International Conference on Learning Representations, 2024. [16] Leo Gao, Tom Dupre la Tour, Henk Tillman, Gabriel Goh, Rajan Troll, Alec Radford, Ilya Sutskever, Jan Leike, and Jeffrey Wu. Scaling and evaluating sparse autoencoders. In The Thirteenth International Conference on Learning Representations, 2025. [17] Lu Wang, Max Song, Rezvaneh Rezapour, Bum Chul Kwon, and Jina Huh-Yoo. Peoples perceptions toward bias and related concepts in large language models: systematic review, 2024. [18] Yiran Liu, Ke Yang, Zehan Qi, Xiao Liu, Yang Yu, and Cheng Xiang Zhai. Bias and volatility: statistical framework for evaluating large language models stereotypes and the associated generation inconsistency. Advances in Neural Information Processing Systems, 37:110131110155, 2024. [19] Yue Huang, Lichao Sun, Haoran Wang, Siyuan Wu, Qihui Zhang, Yuan Li, Chujie Gao, Yixin Huang, Wenhan Lyu, Yixuan Zhang, Xiner Li, Hanchi Sun, Zhengliang Liu, Yixin Liu, Yijue Wang, Zhikun Zhang, Bertie Vidgen, Bhavya Kailkhura, Caiming Xiong, Chaowei Xiao, Chunyuan Li, Eric P. Xing, Furong Huang, Hao Liu, Heng Ji, Hongyi Wang, Huan Zhang, Huaxiu Yao, Manolis Kellis, Marinka Zitnik, Meng Jiang, Mohit Bansal, James Zou, Jian Pei, Jian Liu, Jianfeng Gao, Jiawei Han, Jieyu Zhao, Jiliang Tang, Jindong Wang, Joaquin Vanschoren, John Mitchell, Kai Shu, Kaidi Xu, Kai-Wei Chang, Lifang He, Lifu Huang, Michael Backes, Neil Zhenqiang Gong, Philip S. Yu, Pin-Yu Chen, Quanquan Gu, Ran Xu, Rex Ying, Shuiwang Ji, Suman Jana, Tianlong Chen, Tianming Liu, Tianyi Zhou, William Yang Wang, Xiang Li, Xiangliang Zhang, Xiao Wang, Xing Xie, Xun Chen, Xuyu Wang, Yan Liu, Yanfang Ye, Yinzhi Cao, Yong Chen, and Yue Zhao. Trustllm: Trustworthiness in large language models. In Forty-first International Conference on Machine Learning, 2024. [20] Yingji Li, Mengnan Du, Rui Song, Xin Wang, and Ying Wang. survey on fairness in large language models. arXiv preprint arXiv:2308.10149, 2023. [21] Sunipa Dev, Emily Sheng, Jieyu Zhao, Aubrie Amstutz, Jiao Sun, Yu Hou, Mattie Sanseverino, Jiin Kim, Akihiro Nishi, Nanyun Peng, and Kai-Wei Chang. On measures of biases and harms in NLP. In Yulan He, Heng Ji, Sujian Li, Yang Liu, and Chua-Hui Chang, editors, Findings of the Association for Computational Linguistics: AACL-IJCNLP 2022, pages 246267, Online only, November 2022. Association for Computational Linguistics. [22] Jared Moore, Tanvi Deshpande, and Diyi Yang. Are large language models consistent over value-laden questions? In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, Findings of the Association for Computational Linguistics: EMNLP 2024, pages 1518515221, Miami, Florida, USA, November 2024. Association for Computational Linguistics. [23] Sarath Sivaprasad, Pramod Kaushik, Sahar Abdelnabi, and Mario Fritz. Exploring value biases: How llms deviate towards the ideal, 2024. [24] Hongliu Cao. Writing style matters: An examination of bias and fairness in information retrieval systems. In Proceedings of the Eighteenth ACM International Conference on Web Search and Data Mining, WSDM 25, page 336344, New York, NY, USA, 2025. Association for Computing Machinery. [25] Arjun Panickssery, Samuel R. Bowman, and Shi Feng. LLM evaluators recognize and favor their own generations. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. [26] Jiafu An, Difang Huang, Chen Lin, and Mingzhu Tai. Measuring gender and racial biases in large language models. arXiv preprint arXiv:2403.15281, 2024. [27] Huy Nghiem, John Prindle, Jieyu Zhao, and Hal Daumé Iii. you gotta be doctor, lin : An investigation of name-based bias of large language models in employment recommendations. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 72687287, Miami, Florida, USA, November 2024. Association for Computational Linguistics. [28] Jiayi Ye, Yanbo Wang, Yue Huang, Dongping Chen, Qihui Zhang, Nuno Moniz, Tian Gao, Werner Geyer, Chao Huang, Pin-Yu Chen, Nitesh Chawla, and Xiangliang Zhang. Justice or prejudice? quantifying biases in LLM-as-a-judge. In The Thirteenth International Conference on Learning Representations, 2025. [29] Jesutofunmi Omiye, Jenna Lester, Simon Spichak, Veronica Rotemberg, and Roxana Daneshjou. Large language models propagate race-based medicine. NPJ Digital Medicine, 6(1):195, 2023. [30] Yifan Yang, Xiaoyu Liu, Qiao Jin, Furong Huang, and Zhiyong Lu. Unmasking and quantifying racial bias of large language models in medical report generation. Communications Medicine, 4(1):176, 2024. 11 [31] Brototo Deb and Adam Rodman. Racial differences in pain assessment and false beliefs about race in ai models. JAMA Network Open, 7(10):e2437977e2437977, 2024. [32] Donald E. Bowen III, S. McKay Price, Luke C.D. Stein, and Ke Yang. Measuring and mitigating racial disparities in large language model mortgage underwriting. http://dx.doi.org/10.2139/ssrn. 4812158, April 2024. Available at SSRN: https://ssrn.com/abstract=4812158. [33] Rahul Vats, Shekhar Agrawal, and Srinivasa Chippada. Bias detection and fairness in large language models for financial services. International Journal of Scientific Research in Computer Science, Engineering and Information Technology, 11:13291345, 03 2025. [34] Zishan Guo, Renren Jin, Chuang Liu, Yufei Huang, Dan Shi, Supryadi, Linhao Yu, Yan Liu, Jiaxuan Li, Bojian Xiong, and Deyi Xiong. Evaluating large language models: comprehensive survey, 2023. [35] Moin Nadeem, Anna Bethke, and Siva Reddy. StereoSet: Measuring stereotypical bias in pretrained language models. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli, editors, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 53565371, Online, August 2021. Association for Computational Linguistics. [36] Masahiro Kaneko and Danushka Bollegala. Unmasking the maskevaluating social biases in masked language models. In Proceedings of the AAAI conference on artificial intelligence, volume 36, pages 1195411962, 2022. [37] Keita Kurita, Nidhi Vyas, Ayush Pareek, Alan Black, and Yulia Tsvetkov. Measuring bias in contextualized word representations. In Marta R. Costa-jussà, Christian Hardmeier, Will Radford, and Kellie Webster, editors, Proceedings of the First Workshop on Gender Bias in Natural Language Processing, pages 166172, Florence, Italy, August 2019. Association for Computational Linguistics. [38] Chandler May, Alex Wang, Shikha Bordia, Samuel Bowman, and Rachel Rudinger. On measuring social biases in sentence encoders. arXiv preprint arXiv:1903.10561, 2019. [39] Wei Guo and Aylin Caliskan. Detecting emergent intersectional biases: Contextualized word embeddings contain distribution of human-like biases. In Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society, AIES 21, page 122133, New York, NY, USA, 2021. Association for Computing Machinery. [40] Aylin Caliskan, Joanna Bryson, and Arvind Narayanan. Semantics derived automatically from language corpora contain human-like biases. Science, 356(6334):183186, 2017. [41] Marc-Etienne Brunet, Colleen Alkalay-Houlihan, Ashton Anderson, and Richard Zemel. Understanding the origins of bias in word embeddings. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pages 803811. PMLR, 0915 Jun 2019. [42] Angus Nicolson, Lisa Schut, Alison Noble, and Yarin Gal. Explaining explainability: Recommendations for effective use of concept activation vectors. Transactions on Machine Learning Research, 2025. [43] Zhihao Xu, Ruixuan HUANG, Changyu Chen, and Xiting Wang. Uncovering safety risks of large language models through concept activation vector. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. [44] Hanyu Zhang, Xiting Wang, Chengao Li, Xiang Ao, and Qing He. Controlling large language models through concept activation vectors, 2025. [45] Nina Panickssery, Nick Gabrieli, Julian Schulz, Meg Tong, Evan Hubinger, and Alexander Matt Turner. Steering llama 2 via contrastive activation addition, 2024. [46] Ruixuan Huang. Steering llms behavior with concept activation vectors, September 2024. Draft manuscript. Available on LessWrong forum. [47] Atakan Seyitoglu, Aleksei Kuvshinov, Leo Schwinn, and Stephan Günnemann. Extracting unlearned information from llms with activation steering, 2024. [48] Andrew Ng. Sparse autoencoder. https://web.stanford.edu/class/cs294a/ sparseAutoencoder_2011new.pdf, 2011. CS294A Lecture Notes, Stanford University. [49] Davide Ghilardi, Federico Belotti, and Marco Molinari. Efficient training of sparse autoencoders for large language models via layer groups. arXiv preprint arXiv:2410.21508, 2024. 12 [50] Anish Mudide, Joshua Engels, Eric Michaud, Max Tegmark, and Christian Schroeder de Witt. Efficient dictionary learning with switch sparse autoencoders. arXiv preprint arXiv:2410.08201, 2024. [51] Senthooran Rajamanoharan, Tom Lieberum, Nicolas Sonnerat, Arthur Conmy, Vikrant Varma, János Jumping ahead: Improving reconstruction fidelity with jumprelu sparse Kramár, and Neel Nanda. autoencoders. arXiv preprint arXiv:2407.14435, 2024. [52] Luke R. Smith and Jonas Brinkmann. Interpreting preference models with sparse autoencoders. AI Alignment Forum, 2024. [53] Praveen Hegde. Effectiveness of sparse autoencoder for understanding and removing gender bias in LLMs. In NeurIPS 2024 Workshop on Scientific Methods for Understanding Deep Learning, 2024. [54] Adly Templeton, Tom Conerly, Jonathan Marcus, Jack Lindsey, Trenton Bricken, Brian Chen, Adam Pearce, Craig Citro, Emmanuel Ameisen, Andy Jones, Hoagy Cunningham, Nicholas Turner, Callum McDougall, Monte MacDiarmid, C. Daniel Freeman, Theodore R. Sumers, Edward Rees, Joshua Batson, Adam Jermyn, Shan Carter, Chris Olah, and Tom Henighan. Scaling monosemanticity: Extracting interpretable features from claude 3 sonnet. Transformer Circuits Thread, 2024. [55] Tolga Bolukbasi, Kai-Wei Chang, James Zou, Venkatesh Saligrama, and Adam Kalai. Man is to computer programmer as woman is to homemaker? debiasing word embeddings. Advances in neural information processing systems, 29, 2016. [56] Emily Sheng, Kai-Wei Chang, Premkumar Natarajan, and Nanyun Peng. The woman worked as babysitter: On biases in language generation. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan, editors, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 34073412, Hong Kong, China, November 2019. Association for Computational Linguistics. [57] OpenAI, :, Aaron Hurst, Adam Lerer, Adam P. Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, Aleksander adry, Alex Baker-Whitcomb, Alex Beutel, Alex Borzunov, Alex Carney, Alex Chow, Alex Kirillov, Alex Nichol, Alex Paino, Alex Renzin, Alex Tachard Passos, Alexander Kirillov, Alexi Christakis, Alexis Conneau, Ali Kamali, Allan Jabri, Allison Moyer, Allison Tam, Amadou Crookes, Amin Tootoochian, Amin Tootoonchian, Ananya Kumar, Andrea Vallone, Andrej Karpathy, Andrew Braunstein, Andrew Cann, Andrew Codispoti, Andrew Galu, Andrew Kondrich, Andrew Tulloch, Andrey Mishchenko, Angela Baek, Angela Jiang, Antoine Pelisse, Antonia Woodford, Anuj Gosalia, Arka Dhar, Ashley Pantuliano, Avi Nayak, Avital Oliver, Barret Zoph, Behrooz Ghorbani, Ben Leimberger, Ben Rossen, Ben Sokolowsky, Ben Wang, Benjamin Zweig, Beth Hoover, Blake Samic, Bob McGrew, Bobby Spero, Bogo Giertler, Bowen Cheng, Brad Lightcap, Brandon Walkin, Brendan Quinn, Brian Guarraci, Brian Hsu, Bright Kellogg, Brydon Eastman, Camillo Lugaresi, Carroll Wainwright, Cary Bassin, Cary Hudson, Casey Chu, Chad Nelson, Chak Li, Chan Jun Shern, Channing Conger, Charlotte Barette, Chelsea Voss, Chen Ding, Cheng Lu, Chong Zhang, Chris Beaumont, Chris Hallacy, Chris Koch, Christian Gibson, Christina Kim, Christine Choi, Christine McLeavey, Christopher Hesse, Claudia Fischer, Clemens Winter, Coley Czarnecki, Colin Jarvis, Colin Wei, Constantin Koumouzelis, Dane Sherburn, Daniel Kappler, Daniel Levin, Daniel Levy, David Carr, David Farhi, David Mely, David Robinson, David Sasaki, Denny Jin, Dev Valladares, Dimitris Tsipras, Doug Li, Duc Phong Nguyen, Duncan Findlay, Edede Oiwoh, Edmund Wong, Ehsan Asdar, Elizabeth Proehl, Elizabeth Yang, Eric Antonow, Eric Kramer, Eric Peterson, Eric Sigler, Eric Wallace, Eugene Brevdo, Evan Mays, Farzad Khorasani, Felipe Petroski Such, Filippo Raso, Francis Zhang, Fred von Lohmann, Freddie Sulit, Gabriel Goh, Gene Oden, Geoff Salmon, Giulio Starace, Greg Brockman, Hadi Salman, Haiming Bao, Haitang Hu, Hannah Wong, Haoyu Wang, Heather Schmidt, Heather Whitney, Heewoo Jun, Hendrik Kirchner, Henrique Ponde de Oliveira Pinto, Hongyu Ren, Huiwen Chang, Hyung Won Chung, Ian Kivlichan, Ian OConnell, Ian OConnell, Ian Osband, Ian Silber, Ian Sohl, Ibrahim Okuyucu, Ikai Lan, Ilya Kostrikov, Ilya Sutskever, Ingmar Kanitscheider, Ishaan Gulrajani, Jacob Coxon, Jacob Menick, Jakub Pachocki, James Aung, James Betker, James Crooks, James Lennon, Jamie Kiros, Jan Leike, Jane Park, Jason Kwon, Jason Phang, Jason Teplitz, Jason Wei, Jason Wolfe, Jay Chen, Jeff Harris, Jenia Varavva, Jessica Gan Lee, Jessica Shieh, Ji Lin, Jiahui Yu, Jiayi Weng, Jie Tang, Jieqi Yu, Joanne Jang, Joaquin Quinonero Candela, Joe Beutler, Joe Landers, Joel Parish, Johannes Heidecke, John Schulman, Jonathan Lachman, Jonathan McKay, Jonathan Uesato, Jonathan Ward, Jong Wook Kim, Joost Huizinga, Jordan Sitkin, Jos Kraaijeveld, Josh Gross, Josh Kaplan, Josh Snyder, Joshua Achiam, Joy Jiao, Joyce Lee, Juntang Zhuang, Justyn Harriman, Kai Fricke, Kai Hayashi, Karan Singhal, Katy Shi, Kavin Karthik, Kayla Wood, Kendra Rimbach, Kenny Hsu, Kenny Nguyen, Keren Gu-Lemberg, Kevin Button, Kevin Liu, Kiel Howe, Krithika Muthukumar, Kyle Luther, Lama Ahmad, Larry Kai, Lauren Itow, Lauren Workman, Leher Pathak, Leo Chen, Li Jing, Lia Guy, Liam Fedus, Liang Zhou, Lien Mamitsuka, Lilian Weng, Lindsay McCallum, Lindsey Held, Long Ouyang, Louis Feuvrier, Lu Zhang, Lukas Kondraciuk, Lukasz Kaiser, Luke Hewitt, Luke Metz, Lyric Doshi, Mada Aflak, Maddie Simens, Madelaine Boyd, Madeleine Thompson, Marat Dukhan, Mark Chen, Mark Gray, Mark Hudnall, Marvin Zhang, Marwan Aljubeh, Mateusz Litwin, Matthew Zeng, Max Johnson, Maya Shetty, Mayank Gupta, Meghan Shah, Mehmet Yatbaz, Meng Jia Yang, Mengchao Zhong, Mia Glaese, Mianna Chen, Michael Janner, Michael Lampe, Michael Petrov, Michael Wu, Michele Wang, Michelle Fradin, Michelle Pokrass, Miguel Castro, Miguel Oom Temudo de Castro, Mikhail Pavlov, Miles Brundage, Miles Wang, Minal Khan, Mira Murati, Mo Bavarian, Molly Lin, Murat Yesildal, Nacho Soto, Natalia Gimelshein, Natalie Cone, Natalie Staudacher, Natalie Summers, Natan LaFontaine, Neil Chowdhury, Nick Ryder, Nick Stathas, Nick Turley, Nik Tezak, Niko Felix, Nithanth Kudige, Nitish Keskar, Noah Deutsch, Noel Bundick, Nora Puckett, Ofir Nachum, Ola Okelola, Oleg Boiko, Oleg Murk, Oliver Jaffe, Olivia Watkins, Olivier Godement, Owen Campbell-Moore, Patrick Chao, Paul McMillan, Pavel Belov, Peng Su, Peter Bak, Peter Bakkum, Peter Deng, Peter Dolan, Peter Hoeschele, Peter Welinder, Phil Tillet, Philip Pronin, Philippe Tillet, Prafulla Dhariwal, Qiming Yuan, Rachel Dias, Rachel Lim, Rahul Arora, Rajan Troll, Randall Lin, Rapha Gontijo Lopes, Raul Puri, Reah Miyara, Reimar Leike, Renaud Gaubert, Reza Zamani, Ricky Wang, Rob Donnelly, Rob Honsby, Rocky Smith, Rohan Sahai, Rohit Ramchandani, Romain Huet, Rory Carmichael, Rowan Zellers, Roy Chen, Ruby Chen, Ruslan Nigmatullin, Ryan Cheu, Saachi Jain, Sam Altman, Sam Schoenholz, Sam Toizer, Samuel Miserendino, Sandhini Agarwal, Sara Culver, Scott Ethersmith, Scott Gray, Sean Grove, Sean Metzger, Shamez Hermani, Shantanu Jain, Shengjia Zhao, Sherwin Wu, Shino Jomoto, Shirong Wu, Shuaiqi, Xia, Sonia Phene, Spencer Papay, Srinivas Narayanan, Steve Coffey, Steve Lee, Stewart Hall, Suchir Balaji, Tal Broda, Tal Stramer, Tao Xu, Tarun Gogineni, Taya Christianson, Ted Sanders, Tejal Patwardhan, Thomas Cunninghman, Thomas Degry, Thomas Dimson, Thomas Raoux, Thomas Shadwell, Tianhao Zheng, Todd Underwood, Todor Markov, Toki Sherbakov, Tom Rubin, Tom Stasi, Tomer Kaftan, Tristan Heywood, Troy Peterson, Tyce Walters, Tyna Eloundou, Valerie Qi, Veit Moeller, Vinnie Monaco, Vishal Kuo, Vlad Fomenko, Wayne Chang, Weiyi Zheng, Wenda Zhou, Wesam Manassra, Will Sheu, Wojciech Zaremba, Yash Patil, Yilei Qian, Yongjik Kim, Youlong Cheng, Yu Zhang, Yuchen He, Yuchen Zhang, Yujia Jin, Yunxing Dai, and Yury Malkov. Gpt-4o system card, 2024. [58] Aaron Gokaslan and Vanya Cohen. Openwebtext corpus. http://Skylion007.github.io/ OpenWebTextCorpus, 2019. [59] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. [60] Andy Zou, Zifan Wang, J. Zico Kolter, and Matt Fredrikson. Universal and transferable adversarial attacks on aligned language models, 2023. [61] Alireza Makhzani and Brendan Frey. k-sparse autoencoders, 2014. [62] Johnny Lin. Neuronpedia: Interactive reference and tooling for analyzing neural networks, 2023. Software available from neuronpedia.org. [63] Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Léonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ramé, et al. Gemma 2: Improving open language models at practical size. arXiv preprint arXiv:2408.00118, 2024. [64] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad AlDahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [65] Paula Czarnowska, Yogarshi Vyas, and Kashif Shah. Quantifying social biases in NLP: generalization and empirical comparison of extrinsic fairness metrics. Transactions of the Association for Computational Linguistics, 9:12491267, 2021. [66] Max Hort, Jie M. Zhang, Federica Sarro, and Mark Harman. Search-based automatic repair for fairness and accuracy in decision-making software. Empirical Software Engineering, 29(1):36, 2024. [67] Po-Sen Huang, Huan Zhang, Ray Jiang, Robert Stanforth, Johannes Welbl, Jack Rae, Vishal Maini, Dani Yogatama, and Pushmeet Kohli. Reducing sentiment bias in language models via counterfactual evaluation. In Trevor Cohn, Yulan He, and Yang Liu, editors, Findings of the Association for Computational Linguistics: EMNLP 2020, pages 6583, Online, November 2020. Association for Computational Linguistics. [68] Yingji Li, Mengnan Du, Rui Song, Xin Wang, and Ying Wang. survey on fairness in large language models, 2024. [69] Xiang Zhang, Junbo Zhao, and Yann LeCun. Character-level convolutional networks for text classification. Advances in neural information processing systems, 28, 2015. 14 [70] Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. Learning word vectors for sentiment analysis. In Dekang Lin, Yuji Matsumoto, and Rada Mihalcea, editors, Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 142150, Portland, Oregon, USA, June 2011. Association for Computational Linguistics. [71] Yuhang Zhou, Paiheng Xu, Xiaoyu Liu, Bang An, Wei Ai, and Furong Huang. Explore spurious correlations at the concept level in language models for text classification. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 478492, Bangkok, Thailand, August 2024. Association for Computational Linguistics. [72] Soumya Barikeri, Anne Lauscher, Ivan Vulic, and Goran Glavaš. RedditBias: real-world resource for bias evaluation and debiasing of conversational language models. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli, editors, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 19411955, Online, August 2021. Association for Computational Linguistics. [73] Anna Kruspe. Towards detecting unanticipated bias in large language models, 2024. [74] Peiyi Zhang, Yazhou Zhang, Bo Wang, Lu Rong, Prayag Tiwari, and Jing Qin. Edu-values: Towards evaluating the chinese education values of large language models, 2025. [75] Samuel Schmidgall, Carl Harris, Ime Essien, Daniel Olshvang, Tawsifur Rahman, Ji Woong Kim, Rojin Ziaei, Jason Eshraghian, Peter Abadir, and Rama Chellappa. Evaluation and mitigation of cognitive biases in medical language models. npj Digital Medicine, 7(1):295, 2024. [76] Wikipedia contributors. Socioeconomic status and mental health Wikipedia, the free encyclopedia, 2024. [Online; accessed 8-May-2025]. [77] John D. Glover, Diana M. Hetzel, and Sarah K. Tennant. The socioeconomic gradient and chronic illness and associated risk factors in australia. Australia and New Zealand Health Policy, 1(1):8, 2004. PMID: 15679942, PMCID: PMC546403. [78] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:28252830, 2011. [79] Qihan Huang, Jie Song, Mengqi Xue, Haofei Zhang, Bingde Hu, Huiqiong Wang, Hao Jiang, Xingen Wang, and Mingli Song. Lg-cav: Train any concept activation vector with language guidance. In A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang, editors, Advances in Neural Information Processing Systems, volume 37, pages 3952239551. Curran Associates, Inc., 2024. [80] Joseph Bloom, Curt Tigges, Anthony Duong, and David Chanin. Saelens. https://github.com/ jbloomAus/SAELens, 2024."
        },
        {
            "title": "A Limitations",
            "content": "BIASLENS uses single prompt for CAV-based steering, though in practice multiple prompts may satisfy the steering criteria listed in Appendix B.4. This could lead to some variability in results. Future work may mitigate this by averaging bias scores over diverse set of prompts. Additionally, BIASLENS is based solely on cosine similarity, which may fail to capture complex or non-linear relationships in the representation space; future work could consider more expressive metrics, such as geometric distances or kernel-based measures."
        },
        {
            "title": "B Details of BIASLENS",
            "content": "The settings in this section are consistently used in 3.3 and 4. B.1 Probe Datasets For each concept, we construct probe dataset consisting of 150 positive and 150 negative sentences: Negative samples. We sample from OPENWEBTEXT [58], large-scale web corpus commonly used as pretraining data for LLMs. It serves here as concept-unrelated samples due to its high diversity in contents. We segment text by sentence boundaries and filter for samples of 25 tokens. One sample may contain multiple short sentences. We then randomly select 150 filtered entries. Positive samples. We generate 150 concept-relevant sentences using GPT-4o, with total length limited to 25 tokens. The generation process follows structured prompting strategy designed to ensure both semantic relevance and diversity. Details are provided in Appendix B.2 and Figure 5. B.2 Templates of Synthesizing Positive Probe Data We construct all prompts by concatenating sampled sentence components such as verb, an aspect, tone, context, and format, which ensures high diversity in the generated data. Figure 5 shows one illustrative example of this prompting strategy. we prompt GPT-4o[57] using structured templates composed of sampled elements: verb (e.g., describe), an aspect (e.g., personality), tone, context (e.g., in documentary), and format (e.g., brief narrative). These are inserted in the form of template. Then, We append fairness-oriented generation guidelines to reduce stereotypes and enforce diversity. All components are generated or curated using GPT-4o to ensure semantic alignment and lexical variation. Each output is constrained to 10 words. This process yields 150 diverse, high-relevance sentences per concept. We encourage readers to refer to our GitHub repository for the complete set of prompt templates. B.3 Details in Deriving CAVs To obtain CAVs, we fit logistic regression classifier at each transformer layer to distinguish concept-relevant from irrelevant activations. Each classifier is trained using scikit-learn [78]s LogisticRegression with default hyperparameters. The input features are the last-token activations from each transformer block, and the training labels are binary. All classifiers are trained independently per layer, and the classifiers normalized weight vector is used as the CAV. Classifiers are evaluated using an 80%-20% train-test split. B.4 Steering Prompt Examples of BIASLENS The complete set of steering prompts used in this work is summarized in Table 5. When selecting steering prompts, we primarily consider three criteria: (1) suitability to reflect realistic bias-exploration contexts (e.g., simulating review scenarios), (2) grammatical correctness to naturally elicit both target and reference concepts, and (3) sensitivity to steering, meaning the prompt should effectively allow subsequent text generations to be influenced by the corresponding CAV. The prompts used here are all empirically valid. Readers can refer to our GitHub repository for more details. 16 Figure 5: Prompt template for synthesizing positive probe samples using GPT-4o. Gemma 2 2B Exp. type Target Concepts Prompt Sentiment Bias(4.2) Yelp Concepts (food/service/price) IMDB Concepts(music/comedy/acting) From the moment arrived From the moment it started, Gender Bias(4.2) WinoBias Concepts(40 career types) Identified as Potential Bias(4.3) Medical Concepts Educational Concepts am diagnosing patient as doctor. The patient is As teacher, noticed that this student Gemma 2 9B Exp. type Target Concepts Prompt Sentiment Bias(4.2) Yelp Concepts (food/service/price) IMDB Concepts(music/comedy/acting) had something to say after the visit had something to say after the movie Gender Bias(4.2) WinoBias Concepts(40 career types) Commonly seen as Potential Bias(4.3) Medical Concepts Educational Concepts am diagnosing patient as doctor. The patient is As teacher, noticed that this student Llama 3.1 8B Exp. type Target Concepts Prompt Sentiment Bias(4.2) Yelp Concepts (food/service/price) IMDB Concepts(music/comedy/acting) This is how began the review: This is how began my thoughts: Gender Bias(4.2) WinoBias Concepts(40 career types) Theyre often viewed Potential Bias(4.3) Medical Concepts Educational Concepts am diagnosing patient as doctor. The patient is As teacher, noticed that this student Table 5: Bias evaluation prompts and concepts across different models B.5 Templates of Classifying SAE Features In 3.3, we use GPT-4o-mini to classify whether each SAE feature is concept-relevant, based on its Neuronpedia description. The prompt template is shown in Figure 6. B.6 Further Results on CAV-Based Steering Effects We provide additional examples where steering has limited effect on output text but still causes notable changes in conceptual representations. In Figure 7(a), the model already mentions the concept service without steering. After steering, the output remains similar, yet the proportion of 17 Figure 6: Prompt template for classifying SAE features as concept-relevant or not using GPT-4o-mini. Figure 7: Case analysis of failed steering with successful concept extraction. (a) Concept-related content already exists in the original output, making steering effects less visible. (b) Concept-related content never appears in the output. In both cases, the SAE still captures increased activation of relevant features, showing that BIASLENS can extract meaningful concept representations even when steering has limited surface effect. service-related SAE features increases from 4.41% to 15.91%. In Figure 7(b), the model does not mention music before or after steering, but music-related features increase from 0% to 14.74%. These results suggest that CAV-based steering can shift activations toward the intended concept, even when surface-level outputs do not change. B.7 Robustness of BIASLENS to Probing Data key component of BIASLENS is the use of Concept Activation Vectors (CAVs) to capture conceptrelevant directions in model activations. In the main experiments (4), we train 50 CAVs per model, each representing distinct concept. Table 6 reports the classification accuracy of the corresponding linear classifiers across all layers. For all models, the classifiers achieve mean accuracy above 99%, with the best accuracy reaching 100%. These results demonstrate that, despite the diversity of probe data, the classifiers can reliably separate concept-related from unrelated activationsindicating the CAVs are meaningful and consistent, in line with LG-CAV [79]. Importantly, this separation happens only during the CAV training stagethe sole component of BIASLENS that uses probe data (see Figure 2). All downstream evaluations rely exclusively on internal activations and learned concept directions. This shows that BIASLENS , as whole, is robust to the construction and content of the probing dataset. Model Best Accuracy Worst Accuracy Mean Accuracy Gemma 2 2B Gemma 2 9B Llama 3.1 8B 100.00% 100.00% 100.00% 90.00% 93.33% 81.67% 99.87% 99.82% 99.53% Table 6: Classification accuracy of logistic regression classifiers used to generate CAVs."
        },
        {
            "title": "C Experimental Details",
            "content": "All experiments are conducted on two NVIDIA RTX A6000 GPUs. C.1 Model and SAE Settings We evaluate BIASLENS on three publicly available LLMs. Table 7 summarizes their parameter sizes and the corresponding Sparse Autoencoder (SAE) settings used for projecting final-layer activations. Model Params Layers Hidden Size SAE Dim SAE Name SAE ID Gemma 2 2B [63] Gemma 2 9B [63] Llama 3.1 8B [64] 2.6B 9.2B 8.0B 26 41 32 2304 3584 16,384 16,384 32,768 gemma-scope-2b-pt-res-canonical gemma-scope-9b-pt-res-canonical llama_scope_lxr_8x layer_25/width_16k/canonical layer_41/width_16k/canonical l31r_8x Table 7: Model specifications and corresponding SAE configurations. Gemma 2 2B and Gemma 2 9B share the same architecture but differ in parameter scale, while Gemma 2 9B and Llama 3.1 8B have similar sizes but distinct architectures. This setup demonstrates the broad applicability of BIASLENS across models of varying structure and scale. For each model, we utilize its last-layer SAE. All SAEs are based on the same symmetric linear structure with single encoder and decoder, using the JumpReLU activation [51]. To ensure comparability, we select SAEs with similar dimensionality (16k or 32k). All SAEs are available at the SAELens repository [80]. C.2 Baselines We compare BIASLENS against eight widely used bias evaluation metrics, covering both extrinsic and intrinsic behavioral metrics. C.2.1 Extrinsic Behavioral Metrics We include six group-based fairness metrics widely used in behavioral bias evaluations. F1 gap (F1-Diff) [6] Originally proposed to measure gender bias in coreference resolution, this metric quantifies the performance asymmetry between two opposing demographic conditions. Formally, F1-Diff = 1pro 1anti , (5) where 1pro and 1anti denote the models F1 scores on samples aligned and misaligned with common stereotypes, respectively. In our setup, we adapt this metric to sentiment-based bias: for each target concept, we evaluate the models classification performance across positive and negative sentiment groups, treating the positive group as pro-stereotypical. Thus, F1-Diff = 1pos 1neg . (6) We use the absolute value because our goal is to quantify the degree of bias, without considering its polarity. Equal Opportunity Difference (EOD) EOD measures the difference in true positive rates (TPR) between two demographic groups. It evaluates whether model offers equal opportunity for correct classification across groups. Formally, let G1 and G2 denote two groups, and define: EOD = TPRG1 TPRG2 , where TPRG = TPG TPG + FNG . (7) Here, TPG and FNG denote the true positives and false negatives for group G. smaller EOD implies fairer treatment in terms of correct positive predictions. EOD measures the difference in true positive rates between demographic groups. 19 Individual Fairness Metric (I.F.) This metric measures the local sensitivity of model outputs to group-specific conditions. For each template, we construct sentence pairs that differ only in their reference concept (e.g., positive vs. negative sentiment) while keeping the target concept fixed. Let denote the set of reference groups and the number of such templates. For each pair of reference groups (a, ˆa) A, we compute the Wasserstein-1 distance W1 between the sentiment distributions PS(xm) and PS(ˆxm) of their completions. I.F. is defined as the average pairwise distance over all reference group pairs and templates: I.F. ="
        },
        {
            "title": "2\nM |A|(|A| − 1)",
            "content": "M (cid:88) (cid:88) m=1 a,ˆaA W1(PS(xm), PS(ˆxm)). Higher values of I.F. indicate stronger dependence of model behavior on specific group conditions, implying potential bias. Group Fairness Metric (G.F.) This metric assesses global distributional disparity in model behavior. For each reference group A, we compute the sentiment score distribution over all generated samples. Let denote the aggregated sentiment distribution over all groups. G.F. is defined as the average Wasserstein-1 distance between each group-specific distribution and the global distribution: G.F. = 1 (cid:88) aA W1(P S , ). larger G.F. value implies that group-specific outputs diverge significantly from the overall distribution, suggesting systemic group-level bias in model predictions. C.2.2 Intrinsic Behavioral Metrics We also consider two intrinsic behavioral metrics that focus on representational properties of LLMs. SEAT Test(Sentence Encoder Association Test) SEAT [7] adapts the Word Embedding Association Test (WEAT) [40] to sentence-level encoders. It quantifies how strongly model associates target concept with two contrasting attributes. Given two sets of target sentences and , and two sets of attribute sentences and B, SEAT defines the association score of sentence with attribute sets as: s(s, A, B) = 1 (cid:88) aA cos(s, a) 1 (cid:88) bB cos(s,b), where s, a, and are the sentence embeddings extracted from the encoder under test, and cos(, ) denotes cosine similarity. The overall SEAT score is then computed as the difference in association means between the two target sets: SEAT(X, Y, A, B) = meanxX s(x, A, B) meanyY s(y, A, B). larger magnitude implies stronger stereotypical alignment. In our experiments, sentence templates are adapted to match the evaluation concepts, and embeddings are taken from the final hidden layer of the LLM. Perplexity Test Following [72], this test evaluates bias by comparing perplexity-based likelihoods of language model across demographic variants. Given set of minimal prompt pairs 20 Figure 8: Prompt template for sentiment classification task. {(p(1) , p(2) ation {ci}N i=1 that differ only in reference concept (e.g., gender), the model generates continu- )}N i=1 for each prompt. For each continuation, we compute the conditional perplexity: ci (cid:88) PPL(ci pi) = exp log (ci,t ci,<t, pi) , (8) 1 ci t=1 where (ci,t ci,<t, pi) denotes the models token-level probability under the prompt pi. Students t-test is then applied to the perplexity values from the two groups. The test outputs t-value, indicating the magnitude of perplexity asymmetry, and p-value, indicating statistical significance. Higher absolute t-values suggest stronger behavioral disparity, while only results with < 0.05 are considered statistically valid for downstream analysis. C.3 Datasets C.3.1 Sentiment Bias Datasets To evaluate extrinsic behavioral metrics, we construct sentiment classification datasets based on existing corpora. For Yelp and IMDB, we adopt GPT-based labeling in [] to identify which samples express specific target concepts. For each concept, we build binary sentiment classification dataset containing 2,000 samples: 1,000 that are relevant to the concept and 1,000 that do not. Each subset is balanced with respect to sentiment polarity, containing 50% positive and 50% negative examples. Since our evaluated models are not instruction-tuned, we convert sentiment classification into continuation task using prompt formats suitable for autoregressive generation, as shown in 8. C.3.2 Gender Bias Datasets We construct customized subsets of the WinoBias dataset [6] to support the computation of intrinsic behavioral metrics. To calculate PG, for each occupation concept, we generate pair of gender-contrastive datasets using WinoBias templates. Specifically, we replace the placeholder [occupation] with concrete occupation term and adjust gendered pronouns to uniformly express either male or female. Each dataset contains 794 sentence pairs, where each pair differs only in gender. This setting allows us to isolate gender bias in language modeling behavior with respect to occupational descriptions. To compute SEAT scores, we construct two sets of target examples (male and female) using the gendered example sentences provided in WinoBias. We also define two attribute sets: Attr1 (Target occupation): Formed by inserting specific occupation word into several sentence templates (e.g., She is [occupation]). Attr2 (Other occupations): Constructed by randomly sampling alternative occupation words and inserting them into the same templates. Each SEAT test includes 144 male examples, 144 female examples, 14 Attr1 examples, and 546 Attr2 examples. This configuration enables robust association testing between gender categories and individual occupations in contextualized embeddings. C.4 Metrics We quantify the agreement between BIASLENS and baseline bias metrics using Spearman correlation coefficients. Formally, given two bias metrics, we first compute their respective scores for all applicable target and reference concept pairs, i.e. all types of biases, resulting in two corresponding sets of values: {s(BIASLENS ) denote the ranks of these scores in their respective sets. The Spearman correlation coefficient is then computed as follows: }. Let r(BIASLENS ) } and {s(base) and r(base) 21 = 1 6 (cid:80) i(r(BIASLENS ) r(base) )2 n(n2 1) , (9) where is the number of applicable biases. higher Spearman correlation coefficient indicates that the two metrics rank biases similarly. Such strong correlation implies that the metrics provide mutually supportive evidence and thus offer comparable insights regarding the presence and strength of biases."
        },
        {
            "title": "D Full Results for Correlation Evaluation",
            "content": "This section reports the full set of bias scores used for computing Spearman correlation coefficients. Table 10 shows the values of all bias metrics across six sentiment-related concepts and three models. For gender bias in occupations, we report SEAT scores and perplexity-based scores across 40 occupations in Table 11 (Gemma 2 2B), Table 12 (Gemma 2 9B), and Table 13 (Llama 3.1 8B). In Table 10, BIASLENS results are marked with gray . In tables 11 13, SEAT p-values 0.05 are marked with light yellow , and Perplexity p-values 0.05 with light orange . Correlations in 4.2 are computed using only entries meeting the corresponding p-value threshold. Table 8: Bias scores across models and concepts. Model Metrics Concepts Gemma 2 2B Gemma 2 9B Llama 3.1 8B acting comedy music food price service F1-Diff EOD I.F. G.F. BIASLENS F1-Diff EOD I.F. G.F. BIASLENS F1-Diff EOD I.F. G.F. BIASLENS 0.0125 0.0931 0.0220 0.0043 0. 0.0250 0.0238 0.0383 0.0192 0.5295 0.0057 0.0505 0.0100 0.0015 0.1289 0.0283 0.0985 0.0440 0.0079 0.1565 0.0414 0.0377 0.0420 0.0210 0.5444 0.0025 0.0485 0.0090 0.0014 0.0664 0.0167 0.1025 0.0456 0.0085 0. 0.0262 0.0407 0.0295 0.0147 0.5337 0.0027 0.1339 0.0109 0.0008 0.0195 0.0222 0.0200 0.0400 0.0200 0.0601 0.0041 0.0038 0.0225 0.0112 0.1266 0.0098 0.0095 0.0086 0.0043 0.2490 0.0155 0.0056 0.0325 0.0038 0. 0.0127 0.0125 0.0193 0.0097 0.4215 0.0290 0.0292 0.0051 0.0026 0.2422 0.0309 0.0122 0.0320 0.0041 0.0652 0.0153 0.0153 0.0064 0.0032 0.3083 0.0083 0.0076 0.0089 0.0044 0.4395 Table 10: Bias evaluation metrics across different models and conceptual dimensions."
        },
        {
            "title": "E Full Results on Explored Potential Biases",
            "content": "In 4.3, we introduced the potential biases uncovered by BIASLENS in the medical domain for Gemma 2 2B. Here, we present additional findings in the education domain. In education domain, target concepts include academic-related categories such as math and college, while reference concepts cover gender, income level, race, socioeconomic status (SES), and 22 language background. As shown in Table 14, we observe more pronounced patterns. Racial bias emerges in judgments of students academic strengths (e.g., math, science) and perceived learning ability. Income bias appears in evaluations of suitability for advanced programs such as gifted classes or college admissions. High-income backgrounds are more often associated with academic competence, reflecting real-world social stereotypes. Interestingly, we also detect non-obvious associationssuch as local residency influencing perceived subject expertisedemonstrating the utility of BIASLENS in revealing subtle and unexpected biases in LLMs. We conduct the same analysis for Gemma 2 9B, with results shown in Table 15. The overall pattern is similar to Gemma 2 2B. In the medical domain, bias mainly arises from gender and income, and local residency also influences judgments on certain conditions, such as severe pain or cancer. In the education domain, judgments about students are strongly affected by income and SES, indicating that GEMMA 2 9B is more likely to reflect socioeconomic bias when assessing academic potential. Results for Llama 3.1 8B are shown in Table 17. Compared to the Gemma models, Llama 3.1 8B exhibits less pronounced bias scores. In the medical domain, we observe mild income and insurancerelated bias. In the education domain, most target-reference concept pairs do not exhibit significant bias, possibly due to weaker alignment between them. Overall, this model shows subtle traces of gender and local-residency-related bias. Table 11: Full results of occupation bias on Gemma 2 2B. SEAT p-values 0.05 are marked with light yellow , and Perplexity p-values 0.05 with light orange . occupation accountant analyst assistant attendant auditor baker carpenter cashier CEO chief cleaner clerk construction worker cook counselor designer developer driver editor farmer guard hairdresser housekeeper janitor laborer lawyer librarian manager mechanic mover nurse SEAT Test Perplexity Test BIASLENS effect-size p-value -0.0880 0.1420 -0.3886 -0.5377 0.3644 0.3864 0.4234 0.0375 -0.0205 0.0511 -0.1736 -0.0945 -0.0763 0.3636 -0.2329 -0.4188 0.2509 0.2998 0.1309 0.3039 0.3339 -0.6155 -1.0979 0.3794 0.0278 -0.1111 -0.4195 0.2807 0.3963 0.4406 -0.6321 0.7789 0.1100 0.9998 1.0000 0.0010 0.0005 0.0002 0.3802 0.5702 0.3323 0.9300 0. 0.7454 0.0009 0.9779 0.9996 0.0170 0.0066 0.1351 0.0044 0.0022 1.0000 1.0000 0.0005 0.4111 0.8288 1.0000 0.0080 0.0006 0.0001 1.0000 t-value -5.9328 -6.7114 -5.3983 -5.2643 -4.8817 -6.8766 -7.7783 -1.7925 -5.0641 -6.8679 -3.7471 -5.8404 -6.0677 -4.7926 -5.1672 -6.4438 -5.2608 -6.7790 -5.6284 -5.6431 -5.7580 -3.6176 -0.8284 -6.8807 -7.5959 -5.3598 -2.5374 -4.9715 -7.9098 -6.9644 2. p-value 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0733 0.0000 0.0000 0.0002 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0003 0.4076 0.0000 0.0000 0.0000 0.0113 0.0000 0.0000 0.0000 0.0235 0.3034 0.2757 0.2693 0.2790 0.2853 0.3129 0.3176 0.2982 0.3130 0.2204 0.2759 0.2691 0. 0.2869 0.2592 0.2078 0.1601 0.2958 0.1667 0.2753 0.2545 0.2160 0.2524 0.2934 0.3204 0.2885 0.2173 0.2504 0.2994 0.2923 0.2441 Continued on next page 23 Occupation physician receptionist salesperson secretary sheriff supervisor tailor teacher writer SEAT Test Perplexity Test BIASLENS effect-size p-value -0.1930 -1.1931 -0.3085 0.1155 0.6038 0.2181 0.2140 -0.6400 -0.1604 0.9518 1.0000 0.9941 0.1689 0.0001 0.0298 0.0353 1.0000 0. t-value -6.8786 -1.6331 -5.3541 -3.3812 -5.3607 -5.9205 -6.1805 -3.8105 -5.1152 p-value 0.0000 0.1027 0.0000 0.0007 0.0000 0.0000 0.0000 0.0001 0.0000 0.2837 0.2729 0.2541 0.2371 0.2958 0.1807 0.2606 0.1741 0.1493 Table 12: Full results of occupation bias on Gemma 2 9B. SEAT p-values 0.05 are marked with light yellow , and Perplexity p-values 0.05 with light orange . occupation accountant analyst assistant attendant auditor baker carpenter cashier CEO chief cleaner clerk construction worker cook counselor designer developer driver editor farmer guard hairdresser housekeeper janitor laborer lawyer librarian manager mechanic mover nurse physician receptionist salesperson secretary sheriff SEAT Test Perplexity Test"
        },
        {
            "title": "BIASLENS",
            "content": "effect-size p-value 0.2022 0.3116 -0.1879 -0.3242 0.3453 -0.1340 0.7722 -0.3337 0.1077 0.6186 -0.1590 0.0156 0.2657 0.1876 -0.4214 -0.4618 0.4854 0.5271 0.1281 0.3562 0.3989 -0.8032 -0.9934 0.3288 0.2389 0.2101 -0.6493 0.2760 0.6194 0.5942 -1.0880 0.0379 -1.0378 -0.0391 -0.3168 0.5115 0.0445 0.0044 0.9457 0.9963 0.0023 0.8739 0.0001 0.9984 0.1800 0.0001 0.9094 0. 0.0122 0.0523 0.9999 0.9999 0.0001 0.0001 0.1369 0.0011 0.0007 1.0000 1.0000 0.0033 0.0208 0.0345 1.0000 0.0104 0.0001 0.0001 1.0000 0.3731 1.0000 0.6258 0.9963 0.0001 t-value -4.9593 -5.7949 -4.5314 -4.2731 -4.6485 -5.1524 -5.7172 -4.0587 -5.3965 -6.0127 -4.8026 -4.0212 -4.6278 -4.2971 -4.8225 -5.7251 -6.1558 -5.7453 -5.0071 -5.8263 -5.4150 -4.8824 -3.1916 -4.9127 -4.6873 -4.9629 -3.2744 -5.6244 -6.0566 -5.4041 -2.6023 -5.0263 -2.9371 -4.1250 -4.5273 -4. p-value 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0001 0.0000 0.0000 0.0000 0.0001 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0014 0.0000 0.0000 0.0000 0.0011 0.0000 0.0000 0.0000 0.0094 0.0000 0.0034 0.0000 0.0000 0.0000 0.1252 0.1362 0.1138 0.1477 0.1391 0.1280 0.1463 0.1218 0.1188 0.1621 0.1460 0.1426 0. 0.1310 0.1290 0.1534 0.1251 0.1369 0.1501 0.1578 0.1403 0.1312 0.1337 0.1439 0.1351 0.1153 0.1270 0.0977 0.1484 0.1718 0.1216 0.1704 0.1185 0.1369 0.1090 0.1429 Continued on next page 24 Occupation supervisor tailor teacher writer SEAT Test Perplexity Test effect-size 0.3183 0.0608 -0.7175 -0.3566 p-value 0.0027 0.3053 1.0000 0.9993 t-value -5.0396 -4.8152 -3.9066 -4.4976 p-value 0.0000 0.0000 0.0001 0.0000 BIASLENS 0.1098 0.1474 0.1415 0.1274 Table 13: Full results of occupation bias on Llama 3.1 8B. SEAT p-values 0.05 are marked with light yellow , and Perplexity p-values 0.05 with light orange . occupation accountant analyst assistant attendant auditor baker carpenter cashier CEO chief cleaner clerk construction worker cook counselor designer developer driver editor farmer guard hairdresser housekeeper janitor laborer lawyer librarian manager mechanic mover nurse physician receptionist salesperson secretary sheriff supervisor tailor teacher writer SEAT Test Perplexity Test BIASLENS effect-size p-value 0.3044 0.4243 -0.0456 -0.0525 0.4575 -0.3895 0.3093 -0.1902 0.0716 0.3646 -0.0488 0.3128 0.2496 -0.3148 -0.3827 -0.5169 0.4381 0.4875 0.0940 0.0587 0.4309 -0.5615 -0.9255 0.2103 0.2909 0.2482 -0.5312 0.4146 0.4622 0.3515 -0.5222 -0.1136 -1.1453 0.2196 -0.1607 0.5936 0.3434 -0.4553 -0.5047 -0.0333 0.0057 0.0003 0.6482 0.6767 0.0001 0.9998 0.0048 0.9442 0.2697 0.0009 0.6617 0.0040 0. 0.9971 0.9992 1.0000 0.0003 0.0001 0.2092 0.3130 0.0002 1.0000 1.0000 0.0361 0.0058 0.0180 1.0000 0.0004 0.0001 0.0015 1.0000 0.8356 1.0000 0.0319 0.9144 0.0001 0.0015 1.0000 0.9999 0.6163 t-value -2.3953 -2.2573 -1.3581 -1.6580 -3.2553 -3.3322 -6.9947 -0.4658 -3.3299 -4.7936 -1.0442 -1.6978 -5.8644 -1.1274 -0.7571 -1.8682 -3.9107 -4.0157 -2.8225 -5.8479 -4.7161 0.1853 3.9004 -4.4642 -5.3378 -3.6736 1.0328 -3.1768 -5.1976 -3.2649 3.3381 -3.3344 2.8635 -1.7476 0.1386 -5.1663 -2.3280 -4.7115 -0.4495 -2.0936 p-value 0.0167 0.0241 0.1746 0.0975 0.0012 0.0009 0.0000 0.6414 0.0009 0.0000 0.2965 0.0897 0.0000 0.2597 0.4491 0.0619 0.0001 0.0001 0.0048 0.0000 0.0000 0.8530 0.0001 0.0000 0.0000 0.0002 0.3018 0.0015 0.0000 0.0011 0.0009 0.0009 0.0042 0.0807 0.8898 0.0000 0.0200 0.0000 0.6531 0.0365 0.0000 0.0000 0.0000 0.0000 0.0039 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0. 0.0039 0.0078 0.0000 0.0039 0.0039 0.0000 0.0039 0.0000 0.0000 0.0000 0.0039 0.0039 0.0000 0.0000 0.0039 0.0039 0.0039 0.0000 0.0039 0.0000 0.0000 0.0000 0.0039 0.0039 0.0000 0.0000 0.0000 Table 14: Bias scores between reference and target concepts in the educational domain, computed using BIASLENS on Gemma 2 2B. For each target concept, the highest bias score across all reference concept pairs is highlighted in mid blue and the second highest in light blue . Target Concepts vs. Reference Concepts male vs female rich vs poor white vs black native vs ESL low-SES vs high-SES math science 0.0041 0.0509 0.1304 0.2223 0.0437 0.0187 0.0627 0.1472 0.2933 0.0213 gifted program 0.0013 0.1158 0.0964 0.1189 0.0676 college 0.0392 0.1182 0.0533 0.0171 0. remedial class 0.0339 0.0177 0.0987 0.0618 0.0846 Table 15: Bias scores between reference and target concepts in the medical and educational domains, computed using BIASLENS on Gemma 2 9B. For each target concept, the highest bias score is highlighted in mid blue and the second highest in light blue . Potential Biases in Medical Domain Target Concepts vs. Reference Concepts illness pain cancer surgery male vs female rich vs poor white vs black public insurance vs private native vs non-native 0.0733 0.1109 0.0583 0.0047 0.0675 0.0907 0.1073 0.1025 0.0233 0. 0.1186 0.0501 0.0764 0.0098 0.0844 Potential Biases in Educational Domain Target Concepts vs. Reference Concepts male vs female rich vs poor white vs black native vs ESL low-SES vs high-SES math science 0.0483 0.1434 0.0306 0.0120 0.0127 0.0650 0.1086 0.0172 0.0900 0.0961 gifted program 0.0342 0.2300 0.0008 0.0244 0.0344 0.1119 0.0133 0.0062 0.0033 0.0101 college 0.0218 0.1243 0.0285 0.0776 0.0307 mental illness 0.0227 0.2377 0.1618 0.0233 0.1360 remedial class 0.0017 0.1996 0.0164 0.2572 0.0320 Table 17: Bias scores between reference and target concepts in the medical and educational domains, computed using BIASLENS on Llama 3.1 8B. For each target concept, the highest bias score is highlighted in mid blue and the second highest in light blue . Potential Biases in Medical Domain Target Concepts vs. Reference Concepts illness pain cancer surgery male vs female rich vs poor white vs black public insurance vs private native vs non-native 0.0039 0.0195 0.0039 0.0117 0.0039 0.0000 0.0078 0.0078 0.0117 0.0039 0.0039 0.0430 0.0078 0.0117 0.0117 Potential Biases in Educational Domain Target Concepts vs. Reference Concepts male vs female rich vs poor white vs black native vs ESL low-SES vs high-SES math science 0.0195 0.0000 0.0000 0.0430 0.0117 0.0234 0.0078 0.0000 0.0391 0. gifted program 0.0000 0.0039 0.0039 0.0156 0.0078 0.0039 0.0469 0.0039 0.0039 0.0156 college 0.0078 0.0039 0.0039 0.0039 0.0039 mental illness 0.0039 0.0273 0.0000 0.0078 0.0078 remedial class 0.0078 0.0078 0.0000 0.0156 0."
        },
        {
            "title": "NeurIPS Paper Checklist",
            "content": "1. Claims Question: Do the main claims made in the abstract and introduction accurately reflect the papers contributions and scope? Answer: [Yes] Justification: The abstract and introduction clearly state the papers goalto evaluate bias in LLMs without using test setsand summarize the proposed method (BIASLENS ), its motivation, and results. See Abstract and Section 1. Guidelines: The answer NA means that the abstract and introduction do not include the claims made in the paper. The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. No or NA answer to this question will not be perceived well by the reviewers. The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. 2. Limitations Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: Appendix discusses BIASLENS limitations. Guidelines: The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. The authors are encouraged to create separate \"Limitations\" section in their paper. The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on few datasets or with few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. The authors should reflect on the factors that influence the performance of the approach. For example, facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, worse outcome might be that reviewers discover limitations that arent acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. 3. Theory assumptions and proofs Question: For each theoretical result, does the paper provide the full set of assumptions and complete (and correct) proof? Answer: [NA] Justification: The paper does not present any formal theorems or proofs. Guidelines: The answer NA means that the paper does not include theoretical results. All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced. All assumptions should be clearly stated or referenced in the statement of any theorems. The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide short proof sketch to provide intuition. Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. Theorems and Lemmas that the proof relies upon should be properly referenced. 4. Experimental result reproducibility Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: Appendix provides extensive details on BIASLENS settings. Appendix details model and SAE settings, baselines, datasets and evaluation metrics in the experiments. Guidelines: The answer NA means that the paper does not include experiments. If the paper includes experiments, No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. If the contribution is dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is novel architecture, describing the architecture fully might suffice, or if the contribution is specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to hosted model (e.g., in the case of large language model), releasing of model checkpoint, or other means that are appropriate to the research performed. While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is new model (e.g., large language model), then there should either be way to access this model for reproducing the results or way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. 5. Open access to data and code 29 Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: Code and data are available at the provided GitHub repository linked in the Abstract. Guidelines: The answer NA means that paper does not include experiments requiring code. Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details. While we encourage the release of code and data, we understand that this might not be possible, so No is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for new open-source benchmark). The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details. The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only subset of experiments are reproducible, they should state which ones are omitted from the script and why. At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. 6. Experimental setting/details Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: Appendix and C.1 covers model configuration, number of probe samples, training hyperparameters, and all other necessary settings. Guidelines: The answer NA means that the paper does not include experiments. The experimental setting should be presented in the core of the paper to level of detail that is necessary to appreciate the results and make sense of them. The full details can be provided either with the code, in appendix, or as supplemental material. 7. Experiment statistical significance Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: For intrinsic behavioral metrics, the paper reports p-values from SEAT and perplexity-based tests, and explains that only results with < 0.05 are included for correlation computation (Section 4.1 and Appendix D). Guidelines: The answer NA means that the paper does not include experiments. The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. 30 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). The method for calculating the error bars should be explained (closed form formula, call to library function, bootstrap, etc.) The assumptions made should be given (e.g., Normally distributed errors). It should be clear whether the error bar is the standard deviation or the standard error of the mean. It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report 2-sigma error bar than state that they have 96% CI, if the hypothesis of Normality of errors is not verified. For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. 8. Experiments compute resources Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: Appendix mentions that all experiments are conducted on two NVIDIA RTX A6000 GPUs. Guidelines: The answer NA means that the paper does not include experiments. The paper should indicate the type of compute workers CPU or GPU, intrinsic cluster, or cloud provider, including relevant memory and storage. The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didnt make it into the paper). 9. Code of ethics Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: The work does not violate the NeurIPS Code of Ethics. No personally identifiable data or unethical evaluation is used. Guidelines: The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. If the authors answer No, they should explain the special circumstances that require deviation from the Code of Ethics. The authors should make sure to preserve anonymity (e.g., if there is special consideration due to laws or regulations in their jurisdiction). 10. Broader impacts Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: Abstract and Section 1 outlines the methods benefit (bias discovery without labeled data). Appendix reveals the risks. Guidelines: 31 The answer NA means that there is no societal impact of the work performed. If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how system learns from feedback over time, improving the efficiency and accessibility of ML). 11. Safeguards Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: No new model or dataset is released that presents risk for misuse. Guidelines: The answer NA means that the paper poses no such risks. Released models that have high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make best faith effort. 12. Licenses for existing assets Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: All datasets and models used (e.g., WinoBias, Yelp, IMDB, HuggingFace LLMs) are properly cited in the main text and Appendix C. Guidelines: The answer NA means that the paper does not use existing assets. The authors should cite the original paper that produced the code package or dataset. The authors should state which version of the asset is used and, if possible, include URL. The name of the license (e.g., CC-BY 4.0) should be included for each asset. 32 For scraped data from particular source (e.g., website), the copyright and terms of service of that source should be provided. If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of dataset. For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. If this information is not available online, the authors are encouraged to reach out to the assets creators. 13. New assets Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: The prompt templates and probing datasets are released in the GitHub repository with documentation. Guidelines: The answer NA means that the paper does not release new assets. Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. The paper should discuss whether and how consent was obtained from people whose asset is used. At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. 14. Crowdsourcing and research with human subjects Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: No human subjects or crowdsourced data were involved. Guidelines: The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. 15. Institutional review board (IRB) approvals or equivalent for research with human subjects Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: No human-subject research was conducted. Guidelines: The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. 33 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. 16. Declaration of LLM usage Question: Does the paper describe the usage of LLMs if it is an important, original, or non-standard component of the core methods in this research? Note that if the LLM is used only for writing, editing, or formatting purposes and does not impact the core methodology, scientific rigorousness, or originality of the research, declaration is not required. Answer: [Yes] Justification: LLMs (GPT-4o) are used to synthesize probe data and prompts, clearly described in Appendix B. Guidelines: The answer NA means that the core method development in this research does not involve LLMs as any important, original, or non-standard components. Please refer to our LLM policy (https://neurips.cc/Conferences/2025/LLM) for what should or should not be described."
        }
    ],
    "affiliations": [
        "Huazhong University of Science and Technology",
        "MBZUAI"
    ]
}