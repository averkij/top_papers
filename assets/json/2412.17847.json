{
    "paper_title": "Bridging the Data Provenance Gap Across Text, Speech and Video",
    "authors": [
        "Shayne Longpre",
        "Nikhil Singh",
        "Manuel Cherep",
        "Kushagra Tiwary",
        "Joanna Materzynska",
        "William Brannon",
        "Robert Mahari",
        "Manan Dey",
        "Mohammed Hamdy",
        "Nayan Saxena",
        "Ahmad Mustafa Anis",
        "Emad A. Alghamdi",
        "Vu Minh Chien",
        "Naana Obeng-Marnu",
        "Da Yin",
        "Kun Qian",
        "Yizhi Li",
        "Minnie Liang",
        "An Dinh",
        "Shrestha Mohanty",
        "Deividas Mataciunas",
        "Tobin South",
        "Jianguo Zhang",
        "Ariel N. Lee",
        "Campbell S. Lund",
        "Christopher Klamm",
        "Damien Sileo",
        "Diganta Misra",
        "Enrico Shippole",
        "Kevin Klyman",
        "Lester JV Miranda",
        "Niklas Muennighoff",
        "Seonghyeon Ye",
        "Seungone Kim",
        "Vipul Gupta",
        "Vivek Sharma",
        "Xuhui Zhou",
        "Caiming Xiong",
        "Luis Villa",
        "Stella Biderman",
        "Alex Pentland",
        "Sara Hooker",
        "Jad Kabbara"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Progress in AI is driven largely by the scale and quality of training data. Despite this, there is a deficit of empirical analysis examining the attributes of well-established datasets beyond text. In this work we conduct the largest and first-of-its-kind longitudinal audit across modalities--popular text, speech, and video datasets--from their detailed sourcing trends and use restrictions to their geographical and linguistic representation. Our manual analysis covers nearly 4000 public datasets between 1990-2024, spanning 608 languages, 798 sources, 659 organizations, and 67 countries. We find that multimodal machine learning applications have overwhelmingly turned to web-crawled, synthetic, and social media platforms, such as YouTube, for their training sets, eclipsing all other sources since 2019. Secondly, tracing the chain of dataset derivations we find that while less than 33% of datasets are restrictively licensed, over 80% of the source content in widely-used text, speech, and video datasets, carry non-commercial restrictions. Finally, counter to the rising number of languages and geographies represented in public AI training datasets, our audit demonstrates measures of relative geographical and multilingual representation have failed to significantly improve their coverage since 2013. We believe the breadth of our audit enables us to empirically examine trends in data sourcing, restrictions, and Western-centricity at an ecosystem-level, and that visibility into these questions are essential to progress in responsible AI. As a contribution to ongoing improvements in dataset transparency and responsible use, we release our entire multimodal audit, allowing practitioners to trace data provenance across text, speech, and video."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 9 1 ] . [ 1 7 4 8 7 1 . 2 1 4 2 : r The Data Provenance Initiative, 2024 BRIDGING THE DATA PROVENANCE GAP ACROSS TEXT, SPEECH, AND VIDEO Shayne Longpre, Nikhil Singh, Manuel Cherep, Kushagra Tiwary, Joanna Materzynska, William Brannon, Robert Mahari, Manan Dey, Mohammed Hamdy, Nayan Saxena, Ahmad Mustafa Anis, Emad A. Alghamdi, Vu Minh Chien, Naana Obeng-Marnu, Da Yin, Kun Qian, Yizhi Li, Minnie Liang, An Dinh, Shrestha Mohanty, Deividas Mataciunas, Tobin South, Jianguo Zhang, Ariel N. Lee, Campbell S. Lund, Christopher Klamm, Damien Sileo, Diganta Misra, Enrico Shippole, Kevin Klyman, Lester JV Miranda, Niklas Muennighoff, Seonghyeon Ye, Seungone Kim, Vipul Gupta, Vivek Sharma, Xuhui Zhou, Caiming Xiong, Luis Villa, Stella Biderman, Alex Pentland, Sara Hooker, Jad Kabbara The Data Provenance Initiative"
        },
        {
            "title": "ABSTRACT",
            "content": "Progress in AI is driven largely by the scale and quality of training data. Despite this, there is deficit of empirical analysis examining the attributes of well-established datasets beyond text. In this work we conduct the largest and first-of-its-kind longitudinal audit across modalitiespopular text, speech, and video datasets from their detailed sourcing trends and use restrictions to their geographical and linguistic representation. Our manual analysis covers nearly 4000 public datasets between 1990-2024, spanning 608 languages, 798 sources, 659 organizations, and 67 countries. We find that multimodal machine learning applications have overwhelmingly turned to web-crawled, synthetic, and social media platforms, such as YouTube, for their training sets, eclipsing all other sources since 2019. Secondly, tracing the chain of dataset derivations we find that while less than 33% of datasets are restrictively licensed, over 80% of the source content in widelyused text, speech, and video datasets, carry non-commercial restrictions. Finally, counter to the rising number of languages and geographies represented in public AI training datasets, our audit demonstrates measures of relative geographical and multilingual representation have failed to significantly improve their coverage since 2013. We believe the breadth of our audit enables us to empirically examine trends in data sourcing, restrictions, and Western-centricity at an ecosystem-level, and that visibility into these questions are essential to progress in responsible AI. As contribution to ongoing improvements in dataset transparency and responsible use, we release our entire multimodal audit, allowing practitioners to trace data provenance across text, speech, and video."
        },
        {
            "title": "INTRODUCTION",
            "content": "The capabilities and flaws of multimodal foundation models are often directly attributable to their training data [66], [74], [75], [90], [91], [117], [130]. While the importance of data measurement has been widely established by prior work [118], so has prevailing absence of data documentation [10], [39], transparency [73], and detailed understanding [34], [37], [47]especially for modalities other than text. lack of thorough data analysis has led to significant challenges, including privacy issues [107], retracting datasets with harmful content [35], [80], adversarially bypassing safety filters [66], facial recognition bias with respect to gender and skin type [11], gender bias in hiring [77], benchmark contamination from overlapping train and test sets [87], and challenges in copyright [84]. Understanding data provenance can aid mitigation attempts to reduce model bias and toxicity [50], [102] address representation in data [51], contamination [81], and quality [59], [95], as well as practical challenges with identifying copyright-free and permissively licensed sets [96]. 1 The Data Provenance Initiative, 2024 DATASETS SIZE # SOURCES # DOMAINS CREATOR ORGS COUNTRIES # LANGUAGES # FAMILIES TASKS LICENSES TEXT SPEECH VIDEO TOTAL 3717 95 104 3916 713 2.1T 775k 51 1.13M 44 - 798 23 16 24 83 534 124 101 659 60 29 67 502 260 - 608 21 36 - 37 395 18 443 50 19 11 55 Table 1: We quantify the breadth of our audit, including the total number of datasets (#), their size in tokens or hours, the sources, domains, creator organizations, countries, languages, tasks, and licenses. In aggregate, we audited 3916 datasets from 659 organizations in 67 countries, spanning 2.1T tokens, and 1.9M hours. We cataloged nearly 798 unique sources, 443 tasks, and 55 licenses. Despite the urgent need for the provenance and characteristics of widely used datasets, the majority of attention to date has centered on text datasets [81], [123], or single feature such as prevalence of hate content [35], [37]. In contrast, in this work, we will critically examine several provenance features of data across text, speech, and video. We conduct the largest and most comprehensive multimodal audit of AI data, to date, reviewing nearly 4000 datasets between 1990-2024, covering 443 unique tasks, 608 languages, derived from 798 original sources, and constructed by 659 organizations, spanning 67 countries, over 1T tokens of text, and 1.9M hours of speech and video content (see Table 1). There is an unprecedented acceleration in the development of multimodal AI systems, making all the more urgent an understanding of the datasets that underpin these breakthroughs. Our extensive collection of features from unstructured academic papers, websites, and repositories enables us to provide empirical grounding to an ambitious set of research questions surrounding data sourcing trends, intended licenses, and geographical and linguistic representation. Our key findings include: 1. Multimodal data is increasingly sourced from the web, social media platforms, or synthetically generated; rather than more curated sources such as movies, audiobooks or manually collected. These sources comprise the vast majority of text tokens, as well as speech and video hours in public data. However, while social media platforms provide data scale, heterogeneity and freshness by nature, they are also particularly prone to anti-crawling, copyright, privacy, and factuality concerns. 2. Whereas only 25% of text, speech, and video datasets have non-commercial licenses, over 80% of content from each modality carries undocumented restrictions in the datasets sources. Dataset licenses are inconsistent with their sources restrictions for over 55% of content. Our audit provides the tools for multimodal developers to identify dataset restrictions, and apply their own standards. 3. Geographical and linguistic representation have not improved for decade, across the data ecosystem. While the amount of data from under-represented creators and languages increases each year, to over 600 languages and 60 countries in 2024, their relative representation remains consistently western-centric, with no significant improvements from > 0.7 Gini coefficients. While Africa and South America organizations account for < 0.2% of all modality content, North America or European organizations span 93% of text tokens and 60%+ hours of speech and video. Our work provides critical insights into the landscape of available multimodal data. We release the entire audit, collected data, and analysis tools, which we believe will bring immense value for data creators, developers, and researchers interested in promoting the responsible development of AI systems and analysis of the AI data ecosystem."
        },
        {
            "title": "2 METHODOLOGY",
            "content": "While many prior works have surveyed the dataset ecosystem [15], [42], [103], [114], [121], few empirically examine data corpora at scale, and those that do focus present more narrow focus around specific feature like geographic bias or hate content[8], [62], [71] or single modality [36], [37], [81], [123]. The goal of this work is to provide an empirical, ecosystem-level, and multimodal analysis of widely used training datasets [76]. Our audit focuses on text, speech, and video, as prominent data modalities behind modern multimodal systems, such as Sora, Whisper, Gemini, GPT-4o, and others [100], [104], [108], [115], [129], [140]. Since training data for modalities can often be independent, multimodal models tend to interleave training batches with different combinations of one or two 2 The Data Provenance Initiative, 2024 modalities [70]. As such, we focus our analysis on datasets that represent one or pair of these modalities. Annotation Features & Methodology In particular, we analyze data trends for the state of data permissions (licenses and terms), sourcing (the web, human annotation, and synthetic generation), and representation (of tasks, organizations, languages, and countries). We adopt Longpre, Mahari, Chen, et al. [123]s methodology, including the license annotation taxonomy and process, to manually audit these features precisely and rigorously. We go beyond prior work, which considers dataset licenses, by extending the taxonomy to consider the terms of use of the sources of the dataset, either from models used to generate synthetic data (e.g. OpenAIs non-compete clause1 or Metas acceptable use policy for Llama 3.12), or the sources policy on content restrictions, which can be conveyed in the form of license, terms of use, or content policy on website [119]. For each dataset, the source terms are annotated as Unrestricted, Unspecified, Source Closed or Model Closed, as defined in Table 2. For Figure 2 we combine Source Closed and Model Closed into Restricted. As with prior work [123], [124], we engage domain experts for these annotation tasksAI researchers whose work pertains to the modality and topic. Because many datasets are iteratively re-packaged before they appear in their final form and often shared on popular dataset marketplaces like HuggingFace, Papers with Code or Github, prior work has found that relevant licensing terms or sourcing information for AI training data is frequently omitted [123]. To ensure we collect this information, we require full trace of metadata back to their original sources (sometimes chain of github repositories, websites, or academic papers). This search can be onerous, especially for terms and licenses, but ensures rigor in the results. Table 1 enumerates the full statistics of our audit. All annotations and analysis code will be made publicly available on release. Scope & Dataset Selection For each modality, we define the scope of the audit (detailed separately below), then aggregate resources to distill list of relevant datasets. The scope is focused on (a) publicly available datasets, (b) widely used tasks in the context of general-purpose model development, and (c) relevance to generative tasks. However, we do consider classification-based datasets in text, speech, and video that can and are frequently re-purposed for generative uses (e.g. instruction tuning). Within the defined audit scope, we use mix of the HuggingFace Datasets platform, survey papers, survey repositories, workshop proceedings, and expert review to accumulate relevant datasets. More detail about the dataset selection and collection process is given for each modality below. Each modality requires its own independent process, by virtue of their community dataset ecosystems being unique (discussed in Section 4). Note that text has wider heterogeneity of published publicly available datasets than speech or video. Typically those datasets have been aggregated into large, standardized text-to-text collections, and as such we trace both these Text (Collections) and their constituent Text (Datasets). All datasets are described, linked, and attributed in Appendix D. 2.1 TEXT Scope We focus on providing an extensive audit for post-training datasets, used in training language models. We include single and multi-turn formats, encompassing both datasets typically used for instruction finetuning (SFT) and preference alignment [105]. This scope reflects the prominent role of general-purpose language models, which benefit from multi-task training on heterogeneous collections that span variety of linguistic, reasoning, and knowledge intensive tasks like question answering, coding, tool use, translation, and classification [49], [64]. Dataset Selection We expand the study conducted by the Data Provenance Collection [123], from 44 dataset collections (of 1858 supervised text datasets) to superset of 108 collections of 3717 datasets, prioritizing recent, popular publicly available HuggingFace Datasets introduced between 2022 and April 2024. Our collection sourced popular datasets from recent survey papers [114], [121] and tools [122]. We additionally reviewed HuggingFace Datasets most downloaded datasets every month, from April to July 2024, under the Natural Language Processing category, as well as the SFT/DPO datasets associated with popular open model releases. We also drew from major multilingual data repositories, including the SEACrowd Catalogue [126], the Masader Arabic Data Catalogue [52], AI4Bharat [27], and the Aya Collection [134]. Lastly, our list of datasets was reviewed and supplemented by language model experts to fill in notable omissions. In total, we trace 1OpenAI Terms of Use 2Llama 3.1 Acceptable Use Policy The Data Provenance Initiative, 2024 the provenance and features of 3713 text datasets from 108 collections, covering 395 popular tasks, spanning from 1994 to 2024. 2.2 SPEECH Scope We audit speech datasets for which automatic speech recognition (ASR) was noted as primary task. We focus on ASR datasets because: (1) ASR is fundamental to many speech technologies, including dictation tools, voice assistants, and chatbots [32], [68]; (2) large-scale speech datasets are typically designed for ASR [89]; (3) ASR data follows standardized formats, making comparisons easier (e.g., corpus of audio clips paired with text); and (4) ASR data can often be reused for other tasks like text to speech (TTS) [7] or language identification [20]. Dataset Selection To curate representative sample of popular ASR datasets, we relied on combination of survey repositories3, and HuggingFace Datasets using the Automatic Speech Recognition and Text-to-Speech task tags. We expanded coverage to well-documented datasets on the OpenSLR4 platform, even if they were newer or less widely used. We expect this might reflect datasets that could be adopted more widely in the future. Finally, we included datasets related to low-resource languages and other languages not well-covered by our initial searches. Speech recognition models are increasingly highly multilingual [33], [104], [131], and datasets serving different communities of builders and end-users around the world are priority for making speech recognition technologies more inclusive. In total, we trace the provenance and features of 95 speech datasets, covering 18 popular ASR tasks, spanning from 1990 to 2024. 2.3 VIDEO Scope Early video understanding models primarily focused on video classification, detection and action recognition, where short clips were categorized into predefined classes [30], [69]. More advanced tasks such as temporal action segmentation, video question answering, and video captioning were later introduced to build upon these foundational tasks [63], [111]. Recently, following the success in the field of image generation, video generation from text has become new task that has shown promising results [72], [82], [115], [140]. Given the scarcity of datasets for text-to-video and the often undocumented sources of data used in recent video generation models [127], we take broader approach to our collection of video datasets. We focus on annotating popular video tasks and limit our scope to datasets corresponding to video tasks that are either published, highly cited, or have 100+ downloads on HuggingFace. This approach is justified by three key factors: (1) the usefulness of video data to the research community stems from its collection and presentation in peer-reviewed work, (2) datasets can often be repurposed between different tasks, allowing for applicability to new tasks such as video generation from text, and (3) focusing on highly cited datasets ensures that datasets quality and relevance has been validated by the research community. Dataset Selection We include datasets tagged with Video Classification, Text-to-Video, and Video-Text-to-Text from HuggingFace Datasets. We augmented this with datasets tagged by Video Understanding or Video Generation in PapersWithCode, as well as datasets listed in popular Github survey repository. We also consulted the proceedings of recent video workshops: the Large Scale Video Understanding and Egocentric Vision workshops. We separately consulted committee of non-author video experts to supplement the list with relevant datasets published at CVPR, ICCV, ECCV, and IJCV. In total, we trace the provenance and features of 104 video datasets, covering 33 popular video tasks, spanning from 2009 to 2024."
        },
        {
            "title": "3 RESULTS",
            "content": "We discuss three key results related to (1) the rising use of web, social media and synthetic sources, (2) inconsistent and opaque restrictions on data use, and (3) lack of improvement in geographical or linguistic representation. Each of these findings holds across modalities, at the ecosystem level. 3.1 RISING USE OF WEB, SOCIAL MEDIA & SYNTHETIC DATA The need for scale, and heterogeneity have driven rising use of data from web-crawled, social media, and synthetic data sources. Developers have sought out ever larger and conveniently 3The Speech Datasets Collection 4openslr.org: Open Speech and Language Resources. OpenSLR is widely used platform in the speech community, dedicated to hosting resources for speech tasks. 4 The Data Provenance Initiative, Figure 1: The cumulative size of data (log-scale tokens for text, hours for speech/video) from each source category, across modalities. The source categories in the legend are ordered by descending quantity. Speech and video sources are increasingly dominated by internet videos and YouTube. Whereas text is predominantly web or encyclopedia-based (wiki) sources, synthetic text is rising in popularity. accessible sources of training data [24], [57]. While small, human-curated datasets are often sufficient and sometimes preferred due to higher quality, these sources often do not scale to present demands [24], [26]. In Figure 1, we empirically measure the rising use of web crawling and social media (or forum) websites that provide some of the most scalable and fresh content. While web-sourced data was always prominent, the balance of sources becomes much more skewed after 2018note the use of the y-axis log scale. We find for Speech and Video that by far the most prominent source of data has become internet videos, and specifically YouTube. Nearly 1M hours each of Speech and Video data from this source far outstrips the next most common sources, which comprise less than 100K hours. For Speech, the primary data sources used to be Calling Platforms (pre-2017), content manually collected with Human Participation, and Audiobooks, but since 2018 internet videos have supplanted these other sources. For Video, since 2013, YouTube, synthetic, and general web data sources all constitute significantly larger portion of data used in prominent video datasets, outstripping the use of Movies, Flickr, Getty, or human curated sources. Among text post-training datasets, we see similar trend with general or news web-based sources, including encyclopedic sources (mainly Wikipedia), providing the majority of tokens over time. Encyclopedic sources alone now contribute over 1T tokens in total. Synthetic data sources are rising the most rapidly. Within the video modality, the introduction of VidProm [138] in 2024, consisting of nearly 7M synthetically generated videos, offered large shift in the video source distribution. Within the textual modality, from fig. 1, synthetic data represented <0.1% of the quantity of Web Encyclopedia data in 2020, but is now 10% its proportion in 2024, making up the 5th largest source of tokens. The top models used in generating datasets are mainly from OpenAI. The top 5 consist of ChatGPT, version unspecified (15.0% of synthetic datasets), GPT-4 (14.4%), BART (10.1%), GPT-3 (8.3%) and GPT-3.5-Turbo (4.9%). The average synthetic dataset also has notably longer turns (in tokens) than the average natural dataset: 1,756 tokens vs 1,065. The task distribution of textual synthetic datasets is shifted towards longer form, open-generation and creative tasks. For example, 88.1% of natural datasets contain classification tasks, compared to only 66.3% of synthetic datasets. Natural data is also more likely to cover translation than synthetic data (72.4% of datasets vs only 22.9% of synthetic datasets). 3.2 INCONSISTENT USE RESTRICTIONS In the United States, creators of work automatically have copyright interest that gives them exclusive rights to make copies and derivatives of the work (17 U.S.C. 106). Licenses are legal documents through which the owners of work express how others may use their work. By contrast, Terms of Service express contract between platform and its users to spell out how platform and its content may be used [28]. For simplicity, we use Licenses to refer to dataset restrictions, and Terms to refer to restrictions on the sources of datasets. There remain open questions about whether certain data licenses are enforceable, but these licenses signal the intention of data creators and therefore warrant consideration as the data creators may be best positioned to understand the sensitivities of the data (privacy, copyright, representation, etc.), and the most impacted by its downstream use [88], [93], [94], [97]. The extent to which practitioner adheres to dataset licenses or source terms remains 5 The Data Provenance Initiative, 2024 Figure 2: The distribution of restrictions from dataset licenses and their sources terms. We break this down by the count of datasets (top), as well as total tokens or hours (bottom). Each license is categorized as Non-commercial/Academic (NC/Acad), Unspecified, or Commercially licensed. Each dataset may also have terms from the source: Restricted to non-commercial use, Unspecified restrictions, or Unrestricted. Two main findings across modalities emerge: (1) Commercially licensed datasets represent larger set of tokens and hours, relative to number of datasets; however, (2) the vast majority of those commercially licensed tokens/hours bare restrictions from their sources. Tables 3 and 4 in the appendix provide detailed numbers. an open question, and may depend on jurisdiction or the desired models use cases [88]. This work does not propose one standard for all developers. For these reasons we restrict our treatment and discussion here to tracing the lineage and distribution of licenses and terms for given modality. Data source terms are much more restrictive than the datasets documented license restrictions. In Figure 2, we find only 25%, 33%, and 32% of text/speech/video datasets are licensed noncommercially. This value is even lower if we consider the proportion of tokens or hours, with 21%, 26%, and 33% of text/speech/video quantities carrying license restrictions. However, staggering 99.8%, 78%, and 99% of those quantities carry some form of non-commercial restriction on one of their sources. For text, these restrictions are frequently from being generated by OpenAI or other models with non-compete clause, while for speech and videos this is often since the datasets are derived from web or social media sources. Inconsistencies between dataset licenses and their sources restrictions pose challenges to practitioners. large amount of datasets have permissive or unspecified licenses, but some set of their sources carry non-commercial restrictions. This inconsistency is measurablerepresenting 79% of tokens in text datasets, 55% of speech hours, and 65% of video hours. Additionally, 19%, 14%, and 36% of text, speech, and video datasets have no license or intended use documentation (from our audit of the datasets documentation on Hugging Face Datasets, GitHub, and Papers with Code). lack of centralized documentation around these restrictions means it can be misleading to developers who are attempting to source data according to their own legal standards for copyright and privacy. Furthermore, lack of documentation can hamper developers following best practices around data preparation and transparency [39], [73]. Large quantities of commercially licensed text datasets are locked in collections without clear information to separate them from restrictive datasets. In Figure 2 (top and bottom), we see the number of datasets and number of tokens without restrictions is significantly higher for Text (Datasets) than Text (Collections). Specifically, 60% more Datasets (or 75% more tokens) are commercially licensed, than for Collections. This demonstrates that many collections contain significant amounts of commercially licensed data. While our audit traces licenses for all datasets within collection, 6 The Data Provenance Initiative, 2024 most collections do not aggregate or expose this documentation. As result, practitioners may be left without easy access to filter for the subsets appropriate for their sourcing standards. 3.3 GEOGRAPHICAL & LINGUISTIC REPRESENTATION IS NOT IMPROVING AFRICA ASIA EUROPE AMERICA OCEANIA AMERICA TEXT SPEECH VIDEO TEXT SPEECH VIDEO 0.3 3.6 0.0 0.0 0.1 0. By Count 24.0 30.4 24.4 61.5 30.4 48.0 By Tokens or Hours 55.4 18.8 22.0 38.4 42.4 38. 13.4 35.7 25.2 6.1 38.8 23.1 0.7 0.0 0.8 0.1 0.0 16.7 0.2 0.0 1.6 0.0 0.0 0. Figure 3: The geographical distribution of countries (world maps) and continents (table) represented by dataset creators. Despite some differences in European, Russian, and Middle Eastern representation, creators are heavily concentrated in the US, China, and Western Europe, with little to no representation in South America or Africa, across modalities. The current Gini coefficient for (Text, Speech, Video) = (0.92, 0.86, 0.74), where higher values indicate more concentration. The importance and progress of representation in AI training data. Diversity and representation in training datasets, and among their creators, are widely acknowledged as essential to building AI models that are less biased, more useful, and more equitable [6], [18], [25], [31], [61], [101], [112], [113], [134], [137]. Prior work has measured the diversity of languages in data along with cultural, ideological, and geographical imbalances [8], [14], [41], [55], [62]. These studies have exposed significant flaws, often in the form of bias and discrimination, stemming directly from poor representation in data [12], [35]. As this problem has now been widely acknowledged for decades, recent efforts have foregrounded sourcing data multilingually and multi-culturally, from native speakers and creators (e.g. ROOTS [60], the Aya Dataset [134], the SEACrowd Catalogue [126], the Masader Catalogue [52], Common Voice [13], Causal Conversations V2 [101] or Moments in Time [18]). Measuring geographical and linguistic representation. Naturally, we aim to use our audit to measure the progress of these efforts on geographical and linguistic representation in the AI ecosystem. We measure the progress of two forms of representation: (1) language diversity of text and speech data, and (2) geographical diversity of the creators, in all three modalities. For languages, we use the ISO 639-1 and 639-3 language codes and categories of language families from Glottolog 5.0.5 In Figure 4(a, c) we display the cumulative sum of unique languages and countries present across all audited datasets, at each time period since 2013. While these measurements illustrate the absolute rise in diversity, we also hope to measure the relative dispersion, or equality of languages and countries in the distribution. In Figure 4(b, d), we use the Gini Index [1], [2], traditional measure of statistical dispersion, frequently used to quantify inequality. This allows us to understand if the distributions of languages and creators are more representative of the international community over the last decade, or equally concentrated despite apparent efforts at the margins. 5We use top level Glottolog families. 7 The Data Provenance Initiative, Inequality in geographical representation remains very high, with few organizations creating datasets from the Global South. For every dataset, our audit recorded the organizational affiliations of each creator of the dataset.6 These organizations were then manually mapped to the country in which they are headquartered. Occasionally, organizations like BigScience, BigCode, or Masakhane have international or continental representation, and were counted as such. In Figure 3, we measure the current state of diversity among these creator organizationswhere Gini coefficient of 1 indicates highest concentration, and lower values more broad representation. Without taking up the normative question of what truly fair score would be, these values provide useful comparisons across modalities and over time. We find that Text dataset developers are particularly homogeneous, with Gini-coefficient of 0.92; followed by Speech, at 0.86 and Video at 0.74, which remain high, but are meaningfully less concentrated. Figure 3 also illustrates that even this limited diversity is still concentrated in North America, Europe, East Asia, and less so in the Global South. In Figure 3, we also compare the distribution of datasets, and of tokens or hours by continent. Dataset creators affiliated with African or South American organizations account for fewer than 0.2% of all tokens or hours, in each modality. In contrast, Asian affiliated organizations represent large proportions of the data, particularly for speech (39% of hours, attributed predominantly to YODAS [89]). Much of this driven by Chinese, Indian, Russian, and Saudi Arabian creators. Most prominently, the combination of North American and European datasets comprises 93% of text tokens, 61% of speech hours, and 60% of video hours. (a) Total Language Representation (b) Gini-coefficient for Language Representation (c) Total Geographical Representation (d) Gini-coefficient for Geographical Representation Figure 4: The cumulative totals (left) of languages and countries represented in the data over time, and the 95% confidence intervals of the gini-coefficients over time (right) to measure the representativeness of these variables. Gini-coefficients are measure of statistical dispersion, frequently used to quantify inequality. Gini coefficient of 1 indicates highest concentration, and lower values more broad representation. While the number of represented languages and geographies continue to rise (left), the equality of their distribution has in most cases, not significantly changed. Geographical representation has not significantly improved for over decade. In Figure 4(c), we measure the total unique number of countries represented across all dataset creator organizations. While individual creators will have varying ethnic and national affiliation, we treat this as an estimate for the influence of each locale in dataset development. We find that while the number of represented countries has risen steadily each year, for each modality, this represents only an illusion of progress. Empirically, the Gini coefficient for each modality has not significantly changed since the start of the period we examine in 2013. Geographic diversity has increased only among Video datasets, and these increases are not significant at the ð‘ = 0.05 level. Text and Speech geographical representations appear to remain stable over the last decade of AI development. 6A dataset creator, following [123], is defined as an organization associated with the release of the dataset as created for machine learningnot any of the upstream sources. More details in Appendix D. 8 The Data Provenance Initiative, Multilingual representation has not improved by most measures. Similar to geographical representation, we measure the cumulative number of ISO 639-1 languages and language families over time, as well as the per-modality Gini-coefficient. Figure 4(a) shows significant increases in the number of languages available for speech and text, especially in 2019, and 2023, with the introduction of large sets like Flores [56], xP3x [98], Common Voice [13], and the Aya Collection [134]. However, once again, when measuring the cumulative dispersion of these datasets in Figure 4(b), only Text language families demonstrate any improvement from pre-2013 to the present. Improvements in the Gini coefficient appear to be largely driven by individual large-scale projects like xP3x and Common Voice, both introduced in 2019. Subsequently, newer datasets remain predominantly monolingual, causing measures of concentration in text languages, speech languages, and language families to remain consistently high. Figure 5: The distribution of creator organizations by modality. Most public speech and video datasets are developed by academic organizations, whereas text datasets are developed by wide mix of academia, non-profit or industry labs, as well as startups. Academia, research non-profits, and industry labs continue to drive public dataset development. As well as understanding the geographic associations of the organizations creating popular datasets, we manually categorize them into: Academic Organization (e.g., universities), Research Groups (e.g., non-profits such as BigScience, EleutherAI or AI2), Industry Labs (e.g., Cohere For AI, Google DeepMind), Corporations (e.g. Google, Meta), Startups (e.g., OpenAI, Anthropic), Governments, Unspecified (datasets where owner affiliation is not shared), or Other. When dataset is released in collaboration between organizations, we record each organization. In Figure 5, we find that universities and other academic organizations account for 16%, 47%, and 71% of all recorded dataset releases, across Text, Speech, and Video respectively. Research groups, industry labs and even corporations are also significant contributors, especially for Text datasets, where ecosystem contributors are far more distributed. The significant role of academic organizations in Video and Speech may suggest that the risk profile of releasing Text datasets differs somewhat from Video and Speech datasets, which may have more distinct privacy concerns."
        },
        {
            "title": "4 DISCUSSION",
            "content": "The rise of web-based, social media, and synthetic datasets may pose greater risks to privacy, copyright, and bias. Section 3.1 discusses the rise of web-based sources and particularly social media as primary sources for speech and video. Figure 1 shows these sources now exceed more traditional, curated sources such as movies, audiobooks, radio, TV, or content hand-crafted by human participantsby at least one order of magnitude. These websites made of mostly user-generated content are natural choice, given that they scale in the quantity, freshness, and heterogeneity that is best suited to train general-purpose models [70], [92]. However, prior work suggests that crowd-sourced, user-generated web content also introduces more challenges than curated content, particularly for privacy, copyright, bias, harm, and factuality. Web-based and particularly user-generated content is disproportionately likely to include personally identifiable information (PII) [40], [81], [107], and copyrighted content [16], [88]. These can be reproduced in the outputs of AI models [53], [78], creating privacy and copyright concerns [110]. Open datasets being used to train GPAI often attempt to filterbut frequently missPII and copyrighted data [107], [136] (although not all do [99]). Social media, in particular, is also known to have bias, toxicity and factuality issues [19], which can manifest in trained models, even after alignment [85]. Lastly, while synthetic data can help reduce the prevalence of PII, copyright, or bias in data, it comes with its own challenges [86], [120]. 9 The Data Provenance Initiative, 2024 Social Media websites have become one of the most prominent data sources, but their Terms often restrict crawling or commercial use. We find that 71% of Video data and 69% of Speech data is from YouTube which has become prominent source of data, given its scale, freshness, and multimodality (containing videos, speech, images, and text) [4], [9], [22], [79], [89], [109]. However, YouTube is social media platform owned by Google and its Terms of Service7 prohibit third parties from crawling YouTube. While content creators maintain their ownership rights in the material they upload to YouTube, the YouTube Terms of Service also grant Google license to reproduce, modify, display, and use the content for purposes connected to YouTubes business, which may include building machine learning models; even if the copyright holder has selected permissive license, YouTubes Terms disallow external parties from crawling that data. Model developers such as Nvidia and OpenAI have been sued in the U.S. by content creators who allege that they unlawfully trained on YouTube videos [116], [135]. Large social media platforms and forums have also adopted restrictive terms in recent years, including Reddit and StackOverflow.8 As these data sources become critical to scaling AI systems, access has been made exclusive, which may hamper academic, non-profit, or open source model developmentto the extent that social media platforms can enforce their terms against third party developers.9 Ambiguous and poorly documented use restrictions may significantly inhibit model developers adhering to cautious legal and ethical data sourcing standards. In Section 3.2. we find that significant amount of data carry non-commercial restrictions in their sources, rather than on the final dataset, which can contain no license or permissive one. For text and video, these restrictions can equate to 99% of all tokens and hours. These inconsistencies are the result of datasets being iteratively re-packaged and re-licensed, without carrying on documentation [123]. While not every developer will employ the same filtering standards, our work shows that the challenges to separate and identify appropriate datasets remain difficult across these modalities. Without continued audits and documentation, practitioners may be forced to forego large collections of partially viable data, hampering data scaling laws [26], or take on avoidable risk. We hope this released audit will provide greater tools for practitioners to apply their own standards, to make informed decisions on training data use. The limitations of measures of geographical and linguistic representation. It is important to note that measures of geographical and linguistic representation are imperfect. We are limited by partial information about the developers identities (including for privacy reasons), limited transparency into how frequently these datasets are used, and the extent to which proprietary datasets may fill in representation gaps behind closed doors. Nonetheless, we believe the breadth and rigour of the audit make this the best available empirical measure of representation in publicly documented datasets. Further, we propose the goal of measuring representation in AI data as essential to understanding progress, or its absence, towards AI systems that fairly serve the broader community of users. Figure 3 and Figure 4 demonstrate that despite the absolute rise of geographical and linguistic representation, the relative western-centric concentration persists, across thousands of surveyed datasets. We release all audit materials for transparency and replicability, and for further use by the research community. Conducting representative analyses of an ecosystem comes with assumptions. First, an ecosystem for AI is by nature, not centralized or organized. Widely used datasets for Text are often hosted on Hugging Face, but this is frequently not the case for Speech or Video. Similarly, while Text data undergoes frequent dataset re-packaging for general-purpose post-training, this is not true to the same extent for other modalities. As such, the scope and dataset selection process need to be designed for each modality, rather than single, simple protocol, which inevitably will not accurately represent one modality at its ecosystem-level. Similarly, we chose subset of modalities of interest to foundation model development [104], [115], but note there are many other left for future work (e.g., images, 3D representations, tabular, time series, graphs, and geospatial data). ACKNOWLEDGMENTS This research was conducted by the Data Provenance Initiative, collective of independent and academic researchers volunteering their time to data transparency projects. The Data Provenance Initiative is supported by the Mozilla Data Futures Lab Infrastructure Fund. 7YouTube Terms of Service. 8Reddit User Agreement and StackOverflow Terms of Service. 9We treat the enforceability of licenses and terms as an open legal question, beyond the scope of our work. The Data Provenance Initiative,"
        },
        {
            "title": "REFERENCES",
            "content": "[1] E. B. Wilson, Untitled review, The American Economic Review, vol. 4, no. 2, pp. 442 444, 1914, ISSN: 00028282. [Online]. Available: http://www.jstor.org/stable/ 1804762 (visited on 09/26/2024). [2] A. B. Atkinson et al., On the measurement of inequality, Journal of economic theory, vol. 2, no. 3, pp. 244263, 1970. [3] J. M. Chaquet, E. J. Carmona, and A. FernÃ¡ndez-Caballero, survey of video datasets for human action and activity recognition, Computer Vision and Image Understanding, vol. 117, no. 6, pp. 633659, 2013, ISSN: 1077-3142. DOI: 10.1016/j.cviu.2013.01.013. [Online]. Available: http://dx.doi.org/10.1016/j.cviu.2013.01.013. [4] S. Abu-El-Haija, N. Kothari, J. Lee, et al., Youtube-8m: large-scale video classification benchmark, arXiv preprint arXiv:1609.08675, 2016. [5] P. Rajpurkar, J. Zhang, K. Lopyrev, and P. Liang, Squad: 100,000+ questions for machine comprehension of text, arXiv preprint arXiv:1606.05250, 2016. [6] G. A. Sigurdsson, G. Varol, X. Wang, A. Farhadi, I. Laptev, and A. Gupta, Hollywood in homes: Crowdsourcing data collection for activity understanding, 2016. arXiv: 1604. 01753 [cs.CV]. [Online]. Available: https://arxiv.org/abs/1604.01753. [7] K. Ito and L. Johnson, The LJ Speech Dataset, 2017. [Online]. Available: https : / / keithito.com/LJ-Speech-Dataset (visited on 05/01/2024). [8] S. Shankar, Y. Halpern, E. Breck, J. Atwood, J. Wilson, and D. Sculley, No classification without representation: Assessing geodiversity issues in open data sets for the developing world, arXiv preprint arXiv:1711.08536, 2017. [9] Y. Aytar, T. Pfaff, D. Budden, T. Paine, Z. Wang, and N. de Freitas, Playing hard exploration games by watching youtube, in Advances in Neural Information Processing Systems, S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, Eds., vol. 31, Curran Associates, Inc., 2018. [Online]. Available: https : / / proceedings . neurips . cc / paper _ files / paper / 2018 / file / 35309226eb45ec366ca86a4329a2b7c3-Paper.pdf. [11] [10] E. M. Bender and B. Friedman, Data statements for natural language processing: Toward mitigating system bias and enabling better science, Transactions of the Association for Computational Linguistics, vol. 6, pp. 587604, 2018. DOI: 10.1162/tacl_a_00041. [Online]. Available: https://aclanthology.org/Q18-1041. J. Buolamwini and T. Gebru, Gender shades: Intersectional accuracy disparities in commercial gender classification, in Proceedings of the 1st Conference on Fairness, Accountability and Transparency, S. A. Friedler and C. Wilson, Eds., ser. Proceedings of Machine Learning Research, vol. 81, PMLR, 2018, pp. 7791. [Online]. Available: https://proceedings. mlr.press/v81/buolamwini18a.html. J. Buolamwini and T. Gebru, Gender shades: Intersectional accuracy disparities in commercial gender classification, in Proceedings of the 1st Conference on Fairness, Accountability and Transparency, S. A. Friedler and C. Wilson, Eds., ser. Proceedings of Machine Learning Research, vol. 81, PMLR, 2018, pp. 7791. [Online]. Available: https://proceedings. mlr.press/v81/buolamwini18a.html. [12] [13] R. Ardila, M. Branson, K. Davis, et al., Common voice: massively-multilingual speech corpus, arXiv preprint arXiv:1912.06670, 2019. [14] T. De Vries, I. Misra, C. Wang, and L. Van der Maaten, Does object recognition work for everyone? In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops, 2019, pp. 5259. [15] S. Li, Z. Tao, K. Li, and Y. Fu, Visual to text: Survey of image and video captioning, IEEE Transactions on Emerging Topics in Computational Intelligence, vol. 3, no. 4, pp. 297312, 2019. DOI: 10.1109/TETCI.2019.2892755. J. Meese and J. Hagedorn, Mundane content on social media: Creation, circulation, and the copyright problem, Social Media+ Society, vol. 5, no. 2, p. 2 056 305 119 839 190, 2019. [16] 11 The Data Provenance Initiative, 2024 [17] M. Mitchell, S. Wu, A. Zaldivar, et al., Model cards for model reporting, in Proceedings of the conference on fairness, accountability, and transparency, 2019, pp. 220229. [18] M. Monfort, A. Andonian, B. Zhou, et al., Moments in time dataset: One million videos for event understanding, IEEE transactions on pattern analysis and machine intelligence, vol. 42, no. 2, pp. 502508, 2019. [19] A. Olteanu, C. Castillo, F. Diaz, and E. KÄ±cÄ±man, Social data: Biases, methodological pitfalls, and ethical boundaries, Frontiers in big data, vol. 2, p. 13, 2019. [20] R. Ardila, M. Branson, K. Davis, et al., Common voice: massively-multilingual speech corpus, English, in Proceedings of the Twelfth Language Resources and Evaluation Conference, N. Calzolari, F. BÃ©chet, P. Blache, et al., Eds., Marseille, France: European Language Resources Association, 2020, pp. 42184222, ISBN: 979-10-95546-34-4. [Online]. Available: https://aclanthology.org/2020.lrec-1.520. [21] T. Brown, B. Mann, N. Ryder, et al., Language models are few-shot learners, in Advances in Neural Information Processing Systems, H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin, Eds., vol. 33, Curran Associates, Inc., 2020, pp. 18771901. [Online]. Available: https://proceedings.neurips.cc/paper_files/paper/2020/file/ 1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf. [22] M. Chang, A. Gupta, and S. Gupta, Semantic visual navigation by watching youtube videos, in Advances in Neural Information Processing Systems, H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin, Eds., vol. 33, Curran Associates, Inc., 2020, pp. 4283 4294. [Online]. Available: https://proceedings.neurips.cc/paper_files/ paper/2020/file/2cd4e8a2ce081c3d7c32c3cde4312ef7-Paper.pdf. [23] L. Gao, S. Biderman, S. Black, et al., The pile: An 800gb dataset of diverse text for language modeling, arXiv preprint arXiv:2101.00027, 2020. [24] T. Henighan, J. Kaplan, M. Katz, et al., Scaling laws for autoregressive generative modeling, 2020. arXiv: 2010.14701 [cs.LG]. [Online]. Available: https://arxiv.org/ abs/2010.14701. [25] P. Joshi, S. Santy, A. Budhiraja, K. Bali, and M. Choudhury, The state and fate of linguistic [26] diversity and inclusion in the nlp world, arXiv preprint arXiv:2004.09095, 2020. J. Kaplan, S. McCandlish, T. Henighan, et al., Scaling laws for neural language models, arXiv preprint arXiv:2001.08361, 2020. [27] A. Kunchukuttan, D. Kakwani, S. Golla, A. Bhattacharyya, M. M. Khapra, P. Kumar, et al., Ai4bharat-indicnlp corpus: Monolingual corpora and word embeddings for indic languages, arXiv preprint arXiv:2005.00085, 2020. [28] E. P. Robinson and Y. Zhu, Beyond agree: Users understanding of web site terms of service, Social media+ society, vol. 6, no. 1, p. 2 056 305 119 897 321, 2020. [29] M. J. Sag, The new legal landscape for text mining and machine learning, in Journal of the Copyright Society of the USA, 2020. [30] Y. Zhu, X. Li, C. Liu, et al., comprehensive study of deep video action recognition, 2020. arXiv: 2012.06567 [cs.CV]. [Online]. Available: https://arxiv.org/abs/ 2012.06567. [31] D. I. Adelani, J. Abbott, G. Neubig, et al., Masakhaner: Named entity recognition for african languages, Transactions of the Association for Computational Linguistics, vol. 9, pp. 11161131, 2021. [32] A. AksÃ«nova, D. van Esch, J. Flynn, and P. Golik, How might we create better benchmarks for speech recognition? In Proceedings of the 1st Workshop on Benchmarking: Past, Present and Future, K. Church, M. Liberman, and V. Kordoni, Eds., Online: Association for Computational Linguistics, 2021, pp. 2234. DOI: 10.18653/v1/2021.bppf-1.4. [Online]. Available: https://aclanthology.org/2021.bppf-1.4. [33] A. Babu, C. Wang, A. Tjandra, et al., Xls-r: Self-supervised cross-lingual speech representa- [34] tion learning at scale, arXiv preprint arXiv:2111.09296, 2021. J. Bandy and N. Vincent, Addressing documentation debt in machine learning research: retrospective datasheet for bookcorpus, arXiv preprint arXiv:2105.05241, 2021. The Data Provenance Initiative, 2024 [35] A. Birhane, V. U. Prabhu, and E. Kahembwe, Multimodal datasets: Misogyny, pornography, [36] [37] and malignant stereotypes, arXiv preprint arXiv:2110.01963, 2021. I. Caswell, J. Kreutzer, L. Wang, et al., Quality at glance: An audit of web-crawled multilingual datasets, arXiv preprint arXiv:2103.12028, 2021. J. Dodge, M. Sap, A. Marasovic, et al., Documenting large webtext corpora: case study on the colossal clean crawled corpus, in Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, 2021, pp. 12861305. [38] A. Dosovitskiy, L. Beyer, A. Kolesnikov, et al., An image is worth 16x16 words: Transformers for image recognition at scale, 2021. arXiv: 2010.11929 [cs.CV]. [39] T. Gebru, J. Morgenstern, B. Vecchione, et al., Datasheets for datasets, Communications of the ACM, vol. 64, no. 12, pp. 8692, 2021. [40] A. S. Luccioni and J. D. Viviano, Whats in the box? preliminary analysis of undesirable content in the common crawl corpus, 2021. arXiv: 2105.02732 [cs.CL]. [41] R. Mahadev and A. Chakravarti, Understanding gender and racial disparities in image recognition models, arXiv preprint arXiv:2107.09211, 2021. [42] M. Malik, M. K. Malik, K. Mehmood, and I. Makhdoom, Automatic speech recognition: survey, Multimedia Tools and Applications, vol. 80, pp. 94119457, 2021. [43] M. Monfort, S. Jin, A. Liu, et al., Spoken Moments: Learning Joint Audio-Visual Representations from Video Descriptions, arXiv:2105.04489 [cs, eess], 2021. DOI: 10.48550/arXiv. 2105.04489. [Online]. Available: http://arxiv.org/abs/2105.04489 (visited on 05/02/2024). [44] A. Paullada, I. D. Raji, E. M. Bender, E. Denton, and A. Hanna, Data and its (dis) contents: survey of dataset development and use in machine learning research, Patterns, vol. 2, no. 11, 2021. [45] A. Radford, J. W. Kim, C. Hallacy, et al., Learning transferable visual models from natural language supervision, arXiv preprint arXiv:2103.00020, 2021. [46] A. Rogers, Changing the world by changing the data, in Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), Online: Association for Computational Linguistics, 2021, pp. 21822194. DOI: 10.18653/v1/2021.acllong.170. [Online]. Available: https://aclanthology.org/2021.acl-long. 170. [47] N. Sambasivan, S. Kapania, H. Highfill, D. Akrong, P. Paritosh, and L. M. Aroyo, Everyone wants to do the model work, not the data work: Data cascades in high-stakes AI, in CHI, ser. CHI 21, Yokohama, Japan: Association for Computing Machinery, 2021, ISBN: 9781450380966. DOI: 10 . 1145 / 3411764 . 3445518. [Online]. Available: https : //doi.org/10.1145/3411764.3445518. [49] [48] V. Sanh, A. Webson, C. Raffel, et al., Multitask prompted training enables zero-shot task generalization, ICLR 2022, 2021. [Online]. Available: https://arxiv.org/abs/ 2110.08207. J. Wei, M. Bosma, V. Zhao, et al., Finetuned language models are zero-shot learners, in International Conference on Learning Representations, 2021. J. Welbl, A. Glaese, J. Uesato, et al., Challenges in detoxifying language models, in Findings of the Association for Computational Linguistics: EMNLP 2021, 2021, pp. 24472469. [51] A. Xu, E. Pathak, E. Wallace, S. Gururangan, M. Sap, and D. Klein, Detoxifying language models risks marginalizing minority voices, in Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 2021, pp. 23902397. [50] [52] Z. Alyafeai, M. Masoud, M. Ghaleb, and M. S. Al-shaibani, Masader: Metadata sourcing for arabic text and speech data resources, in Proceedings of the Thirteenth Language Resources and Evaluation Conference, 2022, pp. 63406351. [53] N. Carlini, D. Ippolito, M. Jagielski, K. Lee, F. Tramer, and C. Zhang, Quantifying memorization across neural language models, 2022. arXiv: 2202.07646 [cs.LG]. 13 The Data Provenance Initiative, 2024 [54] B. Elizalde, S. Deshmukh, M. A. Ismail, and H. Wang, Clap: Learning audio concepts from natural language supervision, 2022. arXiv: 2206.04769 [cs.SD]. [55] F. Faisal, Y. Wang, and A. Anastasopoulos, Dataset geography: Mapping language data to language users, in Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2022, pp. 33813411. [56] N. Goyal, C. Gao, V. Chaudhary, et al., The flores-101 evaluation benchmark for lowresource and multilingual machine translation, Transactions of the Association for Computational Linguistics, vol. 10, pp. 522538, 2022. J. Hoffmann, S. Borgeaud, A. Mensch, et al., Training compute-optimal large language models, arXiv preprint arXiv:2203.15556, 2022. [57] [58] S. Kapoor and A. Narayanan, Leakage and the reproducibility crisis in ml-based science, [59] arXiv preprint arXiv:2207.07048, 2022. J. Kreutzer, I. Caswell, L. Wang, et al., Quality at glance: An audit of web-crawled multilingual datasets, Transactions of the Association for Computational Linguistics, vol. 10, pp. 5072, 2022. [60] H. LaurenÃ§on, L. Saulnier, T. Wang, et al., The bigscience roots corpus: 1.6tb composite multilingual dataset, in Advances in Neural Information Processing Systems, S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, Eds., vol. 35, Curran Associates, Inc., 2022, pp. 31 80931 826. [Online]. Available: https://proceedings.neurips.cc/ paper_files/paper/2022/file/ce9e92e3de2372a4b93353eb7f3dc0bdPaper-Datasets_and_Benchmarks.pdf. [61] A. McMillan-Major, Z. Alyafeai, S. Biderman, et al., Documenting geographically and contextually diverse data sources: The bigscience catalogue of language data and resources, 2022. arXiv: 2201.10066 [cs.CL]. [Online]. Available: https://arxiv.org/ abs/2201.10066. [62] A. McMillan-Major, Z. Alyafeai, S. Biderman, et al., Documenting geographically and contextually diverse data sources: The bigscience catalogue of language data and resources, arXiv preprint arXiv:2201.10066, 2022. [63] D. Moctezuma, T. RamÃ­rez-delReal, G. Ruiz, and O. GonzÃ¡lez-ChÃ¡vez, Video captioning: comparative review of where we are and which could be the route, 2022. arXiv: 2204. 05976 [cs.CV]. [Online]. Available: https://arxiv.org/abs/2204.05976. [64] L. Ouyang, J. Wu, X. Jiang, et al., Training language models to follow instructions with human feedback, arXiv preprint arXiv:2203.02155, 2022. [Online]. Available: https: //arxiv.org/abs/2203.02155. [65] A. Ramesh, P. Dhariwal, A. Nichol, C. Chu, and M. Chen, Hierarchical Text-Conditional Image Generation with CLIP Latents, arXiv: arXiv:2204.06125, 2022. DOI: 10.48550/ arXiv.2204.06125. arXiv: 2204.06125 [cs]. J. Rando, D. Paleka, D. Lindner, L. Heim, and F. TramÃ¨r, Red-teaming the stable diffusion safety filter, 2022. arXiv: 2210.04610 [cs.AI]. [Online]. Available: https://arxiv. org/abs/2210.04610. [66] [67] U. Singer, A. Polyak, T. Hayes, et al., Make-A-Video: Text-to-Video Generation without Text-Video Data, arXiv: arXiv:2209.14792, 2022. arXiv: 2209.14792 [cs]. [Online]. Available: http://arxiv.org/abs/2209.14792. [68] Y. Zhang, D. S. Park, W. Han, et al., Bigssl: Exploring the frontier of large-scale semisupervised learning for automatic speech recognition, IEEE Journal of Selected Topics in Signal Processing, vol. 16, no. 6, pp. 15191532, 2022, ISSN: 1941-0484. DOI: 10.1109/ jstsp.2022.3182537. [Online]. Available: http://dx.doi.org/10.1109/ JSTSP.2022.3182537. [69] L. Zheng, T. Zhou, R. Jiang, and Y. Peng, Survey of video object detection algorithms based on deep learning, in Proceedings of the 2021 4th International Conference on Algorithms, Computing and Artificial Intelligence, ser. ACAI 21, Sanya, China: Association for Computing Machinery, 2022, ISBN: 9781450385053. DOI: 10.1145/3508546.3508622. [Online]. Available: https://doi.org/10.1145/3508546.3508622. The Data Provenance Initiative, 2024 [70] A. Aghajanyan, L. Yu, A. Conneau, et al., Scaling laws for generative mixed-modal language models, in International Conference on Machine Learning, PMLR, 2023, pp. 265279. [71] A. Birhane, V. Prabhu, S. Han, V. N. Boddeti, and A. S. Luccioni, Into the laions den: Investigating hate in multimodal datasets, arXiv preprint arXiv:2311.03449, 2023. [72] A. Blattmann, T. Dockhorn, S. Kulal, et al., Stable video diffusion: Scaling latent video diffusion models to large datasets, 2023. arXiv: 2311.15127 [cs.CV]. [Online]. Available: https://arxiv.org/abs/2311.15127. [73] R. Bommasani, K. Klyman, S. Longpre, et al., The foundation model transparency index, 2023. arXiv: 2310.12941 [cs.LG]. [74] N. Carlini, D. Ippolito, M. Jagielski, K. Lee, F. TramÃ¨r, and C. Zhang, Quantifying memorization across neural language models, in The Eleventh International Conference on Learning Representations, OpenReview, 2023. [75] N. Carlini, J. Hayes, M. Nasr, et al., Extracting training data from diffusion models, in 32nd USENIX Security Symposium (USENIX Security 23), Anaheim, CA: USENIX Association, 2023, pp. 52535270, ISBN: 978-1-939133-37-3. [Online]. Available: https : / / www . usenix.org/conference/usenixsecurity23/presentation/carlini. [76] S. H. Cen, A. Hopkins, A. Ilyas, A. Madry, I. Struckman, and L. Videgaray Caso, AI Supply Chains, 2023. [Online]. Available: http://dx.doi.org/10.2139/ssrn.4789403. [77] X. Chang, Gender bias in hiring: An analysis of the impact of amazons recruiting algorithm, Advances in Economics, Management and Political Sciences, vol. 23, pp. 134140, 2023. DOI: 10.54254/2754-1169/23/20230367. [78] Y. Chen, E. Mendes, S. Das, W. Xu, and A. Ritter, Can language models be instructed to protect personal information? en, 2023. [79] S. Coats, Dialect corpora from youtube, Language and linguistics in complex world, 2023. [80] E. David, Ai image training dataset found to include child sexual abuse imagery, The Verge, 2023, 7:57 AM PST. [Online]. Available: https://www.theverge.com/2023/12/ 20/24009418/generative-ai-image-laion-csam-google-stabilitystanford. [81] Y. Elazar, A. Bhagia, I. H. Magnusson, et al., Whats in my big data? In The Twelfth International Conference on Learning Representations, 2023. [82] P. Esser, J. Chiu, P. Atighehchian, J. Granskog, and A. Germanidis, Structure and contentguided video synthesis with diffusion models, 2023. arXiv: 2302.03011 [cs.CV]. [Online]. Available: https://arxiv.org/abs/2302.03011. [83] S. Y. Gadre, G. Ilharco, A. Fang, et al., Datacomp: In search of the next generation of multimodal datasets, in Advances in Neural Information Processing Systems, A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, Eds., vol. 36, Curran Associates, Inc., 2023, pp. 27 09227 112. [Online]. Available: https://proceedings.neurips.cc/ paper_files/paper/2023/file/56332d41d55ad7ad8024aac625881be7Paper-Datasets_and_Benchmarks.pdf. [84] P. Henderson, X. Li, D. Jurafsky, T. Hashimoto, M. A. Lemley, and P. Liang, Foundation models and fair use, arXiv preprint arXiv:2303.15715, 2023. [85] S. Kotha, J. M. Springer, and A. Raghunathan, Understanding catastrophic forgetting in language models via implicit inference, arXiv preprint arXiv:2309.10105, 2023. [86] A. Kurakin, N. Ponomareva, U. Syed, L. MacDermed, and A. Terzis, Harnessing largelanguage models to generate private synthetic text, 2023. arXiv: 2306.01684 [cs.LG]. [87] A. N. Lee, C. J. Hunter, and N. Ruiz, Platypus: Quick, cheap, and powerful refinement of llms, NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following, 2023. [88] K. Lee, A. F. Cooper, and J. Grimmelmann, Talkinbout ai generation: Copyright and the generative-ai supply chain, arXiv preprint arXiv:2309.08133, 2023. [89] X. Li, S. Takamichi, T. Saeki, W. Chen, S. Shiota, and S. Watanabe, Yodas: Youtubeoriented dataset for audio and speech, in 2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), IEEE, 2023, pp. 18. 15 The Data Provenance Initiative, 2024 [90] H. Liu, C. Li, Y. Li, and Y. J. Lee, Improved baselines with visual instruction tuning, 2023. [91] H. Liu, C. Li, Q. Wu, and Y. J. Lee, Visual instruction tuning, in NeurIPS, 2023. [92] S. Longpre, G. Yauney, E. Reif, et al., pretrainers guide to training data: Measuring the effects of data age, domain coverage, quality, & toxicity, 2023. arXiv: 2305.13169 [cs.CL]. [93] R. Mahari and S. Longpre, Discit ergo est: Training data provenance and fair use, Robert Mahari and Shayne Longpre, Discit ergo est: Training Data Provenance And Fair Use, Dynamics of Generative AI (ed. Thibault Schrepel & Volker Stocker), Network Law Review, Winter, 2023. [94] R. Mahari, L. Shayne, L. Donewald, A. Polozov, A. . Pentland, and A. Lipsitz, Comment to US copyright office on data provenance and copyright, 2023. [95] M. Marion, A. ÃœstÃ¼n, L. Pozzobon, A. Wang, M. Fadaee, and S. Hooker, When less is more: Investigating data pruning for pretraining llms at scale, 2023. arXiv: 2309.04564 [cs.CL]. [Online]. Available: https://arxiv.org/abs/2309.04564. [96] S. Min, S. Gururangan, E. Wallace, et al., Silo language models: Isolating legal risk in nonparametric datastore, in NeurIPS 2023 Workshop on Distribution Shifts: New Frontiers with Foundation Models, 2023. [97] F. Morton-Park, Licensed to learn: Mitigating copyright infringement liability of generative ai systems through contracts, Notre Dame Journal on Emerging Technology, vol. 5, p. 64, 2023. [98] N. Muennighoff, T. Wang, L. Sutawika, et al., Crosslingual generalization through multitask finetuning, in Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2023, pp. 15 99116 111. [99] G. Penedo, Q. Malartic, D. Hesslow, et al., The RefinedWeb dataset for falcon LLM: Outperforming curated corpora with web data, and web data only, 2023. arXiv: 2306. 01116 [cs.CL]. [100] Y. Peng, J. Tian, B. Yan, et al., Reproducing whisper-style training using an open-source toolkit and publicly available data, in 2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), IEEE, 2023, pp. 18. [101] B. Porgali, V. Albiero, J. Ryda, C. C. Ferrer, and C. Hazirbas, The casual conversations v2 dataset, 2023. arXiv: 2303.04838 [cs.CV]. [Online]. Available: https://arxiv. org/abs/2303.04838. [102] L. Pozzobon, B. Ermis, P. Lewis, and S. Hooker, Goodtriever: Adaptive toxicity mitigation with retrieval-augmented models, 2023. arXiv: 2310 . 07589 [cs.AI]. [Online]. Available: https://arxiv.org/abs/2310.07589. [103] R. Prabhavalkar, T. Hori, T. N. Sainath, R. SchlÃ¼ter, and S. Watanabe, End-to-end speech recognition: survey, IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2023. [104] A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, and I. Sutskever, Robust speech recognition via large-scale weak supervision, in International Conference on Machine Learning, PMLR, 2023, pp. 28 49228 518. [105] R. Rafailov, A. Sharma, E. Mitchell, S. Ermon, C. D. Manning, and C. Finn, Direct preference optimization: Your language model is secretly reward model, arXiv preprint arXiv:2305.18290, 2023. [106] M. C. Schiappa, Y. S. Rawat, and M. Shah, Self-supervised learning for videos: survey, ACM Computing Surveys, vol. 55, no. 13s, pp. 137, 2023, ISSN: 1557-7341. DOI: 10 . 1145/3577925. [Online]. Available: http://dx.doi.org/10.1145/3577925. [107] N. Subramani, S. Luccioni, J. Dodge, and M. Mitchell, Detecting personal information in training corpora: An analysis, in Proceedings of the 3rd Workshop on Trustworthy Natural Language Processing (TrustNLP 2023), Toronto, Canada: Association for Computational Linguistics, 2023. [108] G. Team, R. Anil, S. Borgeaud, et al., Gemini: family of highly capable multimodal models, arXiv preprint arXiv:2312.11805, 2023. 16 The Data Provenance Initiative, [109] D. Uthus, G. Tanzer, and M. Georg, Youtube-asl: large-scale, open-domain american sign language-english parallel corpus, in Advances in Neural Information Processing Systems, A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, Eds., vol. 36, Curran Associates, Inc., 2023, pp. 29 02929 047. [Online]. Available: https : / / proceedings . neurips . cc / paper _ files / paper / 2023 / file / 5c61452daca5f0c260e683b317d13a3f - Paper - Datasets _ and _ Benchmarks.pdf. [110] D. Zhang, B. Xia, Y. Liu, et al., Tag your fish in the broken net: responsible web framework for protecting online privacy and copyright, 2023. arXiv: 2310.07915 [cs.NI]. [111] C. Zhu, Q. Jia, W. Chen, Y. Guo, and Y. Liu, Deep learning for video-text retrieval: review, 2023. arXiv: 2302.12552 [cs.CV]. [Online]. Available: https://arxiv.org/ abs/2302.12552. [112] Aakanksha, A. Ahmadian, B. Ermis, et al., The multilingual alignment prism: Aligning global and local preferences to reduce harm, 2024. arXiv: 2406.18682 [cs.CL]. [Online]. Available: https://arxiv.org/abs/2406.18682. [113] D. I. Adelani, J. Ojo, I. A. Azime, et al., Irokobench: new benchmark for african languages in the age of large language models, 2024. arXiv: 2406 . 03368 [cs.CL]. [Online]. Available: https://arxiv.org/abs/2406.03368. [114] A. Albalak, Y. Elazar, S. M. Xie, et al., survey on data selection for language models, arXiv preprint arXiv:2402.16827, 2024. [115] T. Brooks, B. Peebles, C. Holmes, et al., Video generation models as world simulators, 2024. [Online]. Available: https://openai.com/research/videogenerationmodels-as-world-simulators. [116] S. Cole, Nvidia sued for scraping youtube after 404 media investigation, 404 Media, 2024. [Online]. Available: https : / / www . 404media . co / nvidia - sued - for - scraping-youtube-after-404-media-investigation/. [117] W. Dai, N. Lee, B. Wang, et al., Nvlm: Open frontier-class multimodal llms, arXiv preprint, 2024. [118] S. Y. Gadre, G. Ilharco, A. Fang, et al., Datacomp: In search of the next generation of multimodal datasets, Advances in Neural Information Processing Systems, vol. 36, 2024. [119] K. Klyman, Acceptable use policies for foundation models, 2024. arXiv: 2409.09041 [cs.CY]. [Online]. Available: https://arxiv.org/abs/2409.09041. [120] R. Liu, J. Wei, F. Liu, et al., Best practices and lessons learned on synthetic data, 2024. arXiv: 2404.07503 [cs.CL]. [121] Y. Liu, J. Cao, C. Liu, K. Ding, and L. Jin, Datasets for large language models: comprehensive survey, arXiv preprint arXiv:2402.18041, 2024. [122] S. Longpre, S. Biderman, A. Albalak, et al., The responsible foundation model development cheatsheet: review of tools & resources, arXiv preprint arXiv:2406.16746, 2024. [123] S. Longpre, R. Mahari, A. Chen, et al., large-scale audit of dataset licensing and attribution in AI, Nature Machine Intelligence, vol. 6, no. 8, pp. 975987, 2024. DOI: 10/gt8f5p. arXiv: 2310.16787 [cs]. [124] S. Longpre, R. Mahari, A. Lee, et al., Consent in crisis: The rapid decline of the ai data commons, arXiv preprint arXiv:2407.14933, 2024. [125] S. Longpre, R. Mahari, N. Obeng-Marnu, et al., Data authenticity, consent, & provenance for ai are all broken: What will it take to fix them? arXiv preprint arXiv:2404.12691, 2024. [126] H. Lovenia, R. Mahendra, S. M. Akbar, et al., Seacrowd: multilingual multimodal data hub and benchmark suite for southeast asian languages, arXiv preprint arXiv:2406.10118, 2024. [127] C. Mauran, What was Sora trained on? Creatives demand answers. https://mashable. com / article / openai - sora - ai - video - generator - training - data, [Accessed 28-09-2024], 2024. The Data Provenance Initiative, 2024 [128] R. Movva, S. Balachandar, K. Peng, G. Agostini, N. Garg, and E. Pierson, Topics, authors, and institutions in large language model research: Trends from 17k arxiv papers, in Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), 2024, pp. 12231243. [129] OpenAI, Hello gpt-4o: Were announcing gpt-4o, our new flagship model that can reason across audio, vision, and text in real time. 2024. [Online]. Available: https://openai. com/index/hello-gpt-4o/. J. Parmar, S. Prabhumoye, J. Jennings, et al., Data, data everywhere: guide for pretraining dataset construction, arXiv preprint 2407.06380, 2024. [130] [131] V. Pratap, A. Tjandra, B. Shi, et al., Scaling speech technology to 1,000+ languages, Journal of Machine Learning Research, vol. 25, no. 97, pp. 152, 2024. [132] F. M. Ramirez, L. Chkhetiani, A. Ehrenberg, et al., Anatomy of industrial scale multilingual asr, arXiv preprint arXiv:2404.09841, 2024. [133] A. Romanou, N. Foroutan, A. Sotnikova, et al., Include: Evaluating multilingual language understanding with regional knowledge, 2024. arXiv: 2411.19799 [cs.CL]. [Online]. Available: https://arxiv.org/abs/2411.19799. [134] S. Singh, F. Vargus, D. Dsouza, et al., Aya dataset: An open-access collection for multilingual instruction tuning, 2024. arXiv: 2402.06619 [cs.CL]. [135] S. Skolnik, Openai sued over using youtube videos without creators consent, Bloomberg Law, 2024. [Online]. Available: https : / / news . bloomberglaw . com / litigation / openai - sued - over - using - youtube - videos - without - creators-consent. [136] L. Soldaini, R. Kinney, A. Bhagia, et al., Dolma: An open corpus of three trillion tokens for language model pretraining research, arXiv preprint arXiv:2402.00159, 2024. [137] A. ÃœstÃ¼n, V. Aryabumi, Z.-X. Yong, et al., Aya model: An instruction finetuned open-access multilingual language model, arXiv preprint arXiv:2402.07827, 2024. [138] W. Wang and Y. Yang, Vidprom: million-scale real prompt-gallery dataset for text-to-video diffusion models, arXiv preprint arXiv:2403.06098, 2024. [139] X. Yang, W. Liang, and J. Zou, Navigating dataset documentations in ai: large-scale analysis of dataset cards on hugging face, 2024. arXiv: 2401.13822 [cs.LG]. [Online]. Available: https://arxiv.org/abs/2401.13822. [140] Z. Zheng, X. Peng, T. Yang, et al., Open-sora: Democratizing efficient video production for all, 2024. [Online]. Available: https://github.com/hpcaitech/Open-Sora. 18 The Data Provenance Initiative, 2024 LABEL DEFINITION MODEL CLOSED SOURCE CLOSED UNSPECIFIED UNRESTRICTED model used to generate part or all of the dataset prohibits using its outputs commercially, to develop competing AI model, or in general. The source has license or terms that prohibits use of the data, either commercially, from being crawled, to develop AI, or in general. No information can be found relevant to restrictions, or lack thereof, for this source. The source has commercially permissive license, such as CC BY, or explicitly states the data is open for broad use. Table 2: The taxonomy used to determine use restrictions on each dataset source. Each source in dataset is examined and fit into one of these categories. The dataset Terms are then labelled according to the strictest terms across the sources, with Model Closed and Source Closed considered stricter than Unspecified which is in turn stricter than Unrestricted."
        },
        {
            "title": "A EXTENDED RELATED WORK",
            "content": "Progress in machine learning across modalities from speech [104] to vision [38] to text [21], [49] has benefited from advancements in large pre-training and fine-tuning corpora. The development of multimodal corpora has also been key to several recent advances, as with CLIP in the image/text domain [45], CLAP for audio/text settings [54], and number of other models involving both text and images, audio or video [65], [67], [104], [132]. The datasets powering these advances are not, however, always well-documented, despite the existence of standards and frameworks for recording and annotating dataset metadata that range from data statements [10] to datasheets for datasets [39] and others [17]. The key problem is not deficiency of any particular framework, but rather inconsistent adoption and fragmentation [125]. Much prior work has argued for the need to document and audit these datasets [44], [46], motivated by concerns from reproducibility [58] to interpretability [92] to bias and fairness problems that may stem from problematic content in training data [35]. There have been several attempts to carry out such audits, with prior work examining pretraining data [124], general web corpora [23], [37], instruction fine-tuning datasets [123], and the documentation fields of the HuggingFace Datasets platform in particular [139]. For speech and vision, there has been less work, with many discussions of datasets in the aggregate occurring in survey papers [3], [106], research aimed directly at improving model performance [83] or close examinations of questions like bias in small groups of datasets [12], [133]. Prior work has also examined the identities, affiliations and national origin of paper authors [128] in AI, but an analogous look at the producers of datasets is lacking. We aim to carry out such analyses: replicating those for pretraining and text finetuning datasets in video and audio domains, and surveying provenance and legal status. Finally, there has also been significant recent attention to legal questions in the collection and use of AI training data [29], [84]. The complex process involved in preparing these datasets [88], and the ambiguous licensing of inputs, can make understanding the legal status of the final output quite difficult. DATASET LICENSES & TERMS Detailed taxonomy We code the legal restrictions placed on use of datasets along two axes. First, we identify whether datasets license permits commercial use (Commercial in Table 3), only non-commercial / academic use (NC / Acad), or does not clearly specify what is permitted (Unspecified). The latter category includes datasets for which we were unable to locate license. Datasets which are in the public domain and not subject to license are counted as commercially usable. Second, we annotate the contractual or terms-of-use restrictions placed on dataset use by the source of each dataset. There are four levels, defined in Table 3. Note that the Model Closed status can only apply to datasets that are AI-generated, at least in part. Some datasets can carry both Model Closed and Source Closed status, but we count the Model Closed first for simplicity. 19 The Data Provenance Initiative, 2024 Detailed breakdown Tables 3 and 4 present crosstabs of these two dimensions, according to respectively the total amount of content and the number of datasets. The most notable finding, as discussed in the main text, is the frequency of clashing restriction status between licenses and terms. By amount of content, fully 73.0% of text content, 55.0% of speech content, and 21.6% of video content is subject to license permitting commercial use but also to terms restrictions forbidding it, or the reverse. The absolute level of restrictions is also high, with < 0.1% of text content, 5.4% of speech content, and 0.6% of video content usable for commercial purposes under both licenses and terms. LICENSE / TERMS RESTRICTED UNSPECIFIED UNRESTRICTED"
        },
        {
            "title": "TOTAL",
            "content": "NC/ACAD UNSPECIFIED COMMERCIAL"
        },
        {
            "title": "TOTAL",
            "content": "NC/ACAD UNSPECIFIED COMMERCIAL"
        },
        {
            "title": "TOTAL",
            "content": "NC/ACAD UNSPECIFIED COMMERCIAL"
        },
        {
            "title": "TOTAL",
            "content": "NC/ACAD UNSPECIFIED COMMERCIAL"
        },
        {
            "title": "Text Collections",
            "content": "96.0 2.3 1.5 99."
        },
        {
            "title": "Text Datasets",
            "content": "21.1 5.7 73.0 99.8 0.0 0.1 0.0 0.1 0.0 0.1 0.0 0."
        },
        {
            "title": "Speech Datasets",
            "content": "23.9 0.5 54.2 78.6 1.4 0.0 13.3 14."
        },
        {
            "title": "Video Datasets",
            "content": "33.7 43.9 21.5 99.1 0.0 0.1 0.0 0.1 96.0 2.4 1.6 21.2 5.7 73. 26.2 0.9 73.0 33.8 44.1 22.1 0.0 0.0 0.0 0.1 0.0 0.0 0.0 0. 0.8 0.4 5.4 6.7 0.1 0.1 0.6 0.8 Table 3: breakdown of the percentage of license and terms restrictions across datasets, by total tokens or hours of content. The much higher frequency of restrictions at the collection level is because we consider collections license or terms status to be the most restrictive of those for its datasets. Note that percentages may not add to exactly 100% because of rounding."
        },
        {
            "title": "C ADDITIONAL RESULTS",
            "content": "Figures 6 and 7 report the size distributions of the datasets. We measure size differently for different types of datasets: Text datasets are in tokens, and audio/video in hours of content. The lack of standard tokenization or preprocessing schemes for those modalities makes it simplest to report raw dataset size. Notably, we find quite different size distributions by modality. The distribution of dataset sizes has the thickest right tail for text, followed by speech and then by video. Most video datasets are short in hour terms, with speech datasets tending to be somewhat longer and text datasets having greater prevalence of both very small and very large datasets relative to the mean size. Dataset tasks, meanwhile, reflect traditional approaches and research programs for each modality. Classification is the most common task for both text and video, with the video communitys longstanding interest in captioning also visible in its role as the second most common task for video datasets. Q&A occupies similar role for text, though text datasets have more balanced distribution 20 The Data Provenance Initiative, 2024 LICENSE / TERMS RESTRICTED UNSPECIFIED UNRESTRICTED"
        },
        {
            "title": "TOTAL",
            "content": "NC/ACAD UNSPECIFIED COMMERCIAL"
        },
        {
            "title": "TOTAL",
            "content": "NC/ACAD UNSPECIFIED COMMERCIAL"
        },
        {
            "title": "TOTAL",
            "content": "NC/ACAD UNSPECIFIED COMMERCIAL"
        },
        {
            "title": "TOTAL",
            "content": "NC/ACAD UNSPECIFIED COMMERCIAL"
        },
        {
            "title": "Text Collections",
            "content": "84.5 1.5 1.5 87."
        },
        {
            "title": "Text Datasets",
            "content": "25.0 17.3 45.2 87.5 0.0 7.5 0.2 7.7 0.0 1.2 6.5 7."
        },
        {
            "title": "Speech Datasets",
            "content": "9.5 6.3 7.4 23.2 9.5 0.0 18.9 28."
        },
        {
            "title": "Video Datasets",
            "content": "22.1 23.1 25.0 70.2 0.0 1.0 0.0 1.0 84.8 8.9 6.3 25.3 18.5 56. 32.6 13.7 53.7 31.7 35.6 32.7 0.3 0.0 4.5 4.8 0.3 0.0 4.5 4. 13.7 7.4 27.4 48.4 9.6 11.5 7.7 28.8 Table 4: breakdown of the percentage of license and terms restrictions by dataset count. The much higher frequency of restrictions at the collection level is because we consider collections license or terms status to be the most restrictive of those for its datasets. Note that percentages may not add to exactly 100% because of rounding. Figure 6: The distribution of dataset sizes for each modality. Most text data collections are between 100M-1B tokens. Speech datasets average 100-1k hours, and video datasets are usually the smallest, commonly less than 100 hours. over other, increasingly prominent tasks like generation and reasoning. Given our selection criteria, all datasets for speech are for ASR tasks, but other tasks like speaker identification and translation are also represented."
        },
        {
            "title": "D DATASETS",
            "content": "This section provides detailed overview of the datasets we have collected and analyzed. Table 5 summarizes the text datasets, Table 6 the audio datasets, and Table 7 the video datasets. Each of these tables lists broad collections of data, sorted in chronological order, and provides information about their properties, sizes, sources and permissions. Each collection can include multiple datasets, and 21 The Data Provenance Initiative, 2024 Figure 7: The task distribution of datasets, across modalities. Post-training text and video datasets are predominantly based on classification. For text, generation and reasoning are rising categories. All speech datasets are recognition-based, particularly for speaker, language, or in the process of translation. they generally reflect the ways dataset creators have grouped their datasets (such as in the same paper). Because of the large number of datasets, we provide detailed information about their licenses and original published papers, where applicable, in the supplementary Attribution Card in Appendix F. Annotation Details: Text For post-training text datasets it is common to package many together as collections, such as Flan [49] or P3 [48]. This practice is not common to the same extent for speech or video datasets. For much of the text analysis, where possible, we chose to analyze statistics at the collection-level, since practitioners are more likely to adopt collection for general-purpose posttraining, than an individual dataset within the collection. Also, in dataset-level statistics, metadata for single collection with many datasets can get repeated and overwhelm the statistics unfairly (e.g. the dataset aggregator/creator being repeated hundreds of times). Consequently, our collection-level analysis of the text modality is reflected in Figure 1, Figure 3, Figure 5, Figure 4, Figure 7, and Figure 6. However, for Figure 2 we draw the distinction between collection and dataset metrics, as practitioners may wish to unpack collections to extract only commercially licensed data. In that case Collection inherits the most restrictive license and terms of its constituent datasets. For annotating creator organizations, we follow prior works instructions [123]. For each dataset they record the affiliations listed on the academic paper or GitHub or HuggingFace object in which the dataset was released. This does not include the organizations who created or owned the sources from which the data was derived. For instance, the SQuAD dataset [5] would be associated with Stanford (the authors affiliation), but not Wikipedia, which the data was partially derived from. For dataset that has authors affiliated with multiple organizations, the dataset will be counted towards each organization. Annotation Details: Speech In many cases, multiple versions of dataset exist due to datasets being expanded or updated. In these scenarios, we used the release date from the initial version (since release dates for subsequent versions were not always clear), but used metadata from the most recently released version for which information was available to offer an overview of the current landscape of data. However, if the dataset versions could not be meaningfully aggregated (e.g. different licenses), or did not appear to be cumulatively designed (non-overlapping or otherwise semantically disjoint data), we maintained separate records. We kept only datasets for which ASR was noted as primary task. For example, if dataset was primarily intended for text-to-speech or speaker recognition, we did not keep it even if it could conceivably be repurposed for ASR. When computing hours, we excluded any hours without supervisory transcripts/scripts (unlabeled data), but kept hours with weak supervision (e.g. model-generated transcripts from speech audio). We recognize the difficulty in comprehensively covering all relevant datasets. Annotation Details: Video In video, single dataset can be re-purposed and annotated to address different tasks [18], [43]. We consider these as two different datasets even if they have the same video source since now they can be used for different computer vision tasks. 22 The Data Provenance Initiative, 2024 Table 5: Alignment tuning (text) collections and properties. Collection properties include numbers of datasets, tasks, languages, and text domains. The SOURCE column indicates whether collection ). The USE contains human-generated web text ( column indicates whether collection includes data freely usable even for commercial purposes ( ), ) and data whose license status data usable only for noncommercial purposes or academic research ( is not specified precisely enough to allow us to determine commercial use permissions ( ). Note that each collection may have different datasets with one, two, or all three of these statuses. Finally, the OAI column indicates collections which include OpenAI model generations. Datasets are sorted chronologically to highlight trends over time. ), language model outputs ( ) or both ( COLLECTION PROPERTY COUNTS YEAR DATASETS TASKS LANGS DOMAINS SOURCE USE TYPES PERMISSIONS OAI RiddleSense MathInstr. No Robots Nectar MetaMathQA MegaWika MedInstr. MathDial PII-Masking-200k Pure-Dove LMSYS-Chat-1M PygmalionAI-PIPPA HelpSteer SeaBench Open Asst. v2 Feedback Coll. Glaive Code Asst. EverythingLM Bactrian-X COBRA Frames UltraFeedback Argilla ExpertQA ChatDoctor Capybara UltraChat-200k CollectiveCognition Thai Gen AI Deita 10K SelFee ChatbotArena OpenGPT Healthcare Orca-Math OpenMathInstr.-1 WildChat Magpie-Pro 2021 2023 2023 2023 2023 2023 2023 2023 2023 2023 2023 2023 2023 2023 2023 2023 2023 2023 2023 2023 2023 2023 2023 2023 2023 2023 2023 2023 2023 2023 2023 2024 2024 2024 2024 1 1 1 1 8 50 1 1 1 1 1 1 1 9 19 1 1 1 6 1 9 1 3 11 1 1 9 2 1 1 3 1 2 2 1 1 1 1 2 1 1 1 4 1 1 1 1 1 5 1 1 1 1 1 2 20 1 2 1 2 1 1 3 1 1 1 3 3 1 1 1 1 1 1 1 50 1 1 4 1 5 1 1 9 19 1 2 2 6 1 1 1 1 2 1 1 1 1 1 1 1 1 1 10 1 3 3 8 1 2 1 1 2 2 4 9 3 5 4 4 2 2 8 4 1 16 3 1 17 7 6 11 11 5 4 4 1 3 7 23 Continued on next page The Data Provenance Initiative, 2024 Table 5: Alignment tuning (text) collections and properties. COLLECTION PROPERTY COUNTS YEAR DATASETS TASKS LANGS DOMAINS SOURCE USE TYPES PERMISSIONS OAI 2024 10k Prompt Ranked 2024 Synth.-GSM8K-Refl. LongAlign-10k 2024 Llama2-MedTuned-Instr. 2024 2024 KIWI 2024 Indic-Instr. 2024 Gretel Text-to-SQL 2024 Conifer 2024 Cidar 2024 Aya 2024 Reasoning Mult. AgentInstruct Mult. InstAr Mult. Dynosaur Mult. Medical Meadow Mult. Open-Platypus Mult. PMC-LLaMA Instr. Mult. COIG Mult. DialogStudio 1 1 1 1 1 8 1 1 1 71 1 6 24 1k 8 10 7 18 83 13 3 3 4 1 7 1 8 8 7 4 3 13 21 2 10 1 13 3 1 1 1 1 1 2 3 1 1 71 1 1 1 1 1 36 1 2 4 1 1 1 2 3 1 2 1 1 1 7 9 22 3 8 2 22 3 Table 6: Audio collections and properties. Collection properties include numbers of audio hours (HR), speakers (SPKR), languages (LANG), creator institutions (CREAT), tasks (TASKS), data sources (SRC), and topics (TOPICS). The number of datasets is not listed because all collections include only one dataset, except for M2ASR which has four. The US column indicates datasets from or partly from the United States, the AC column datasets created by academic institutions, and the IND column datasets created by industry. Note that dataset can have all of these, none of them, or any combination of them. The USE column indicates whether collection includes data freely usable even for commercial purposes ( ), data usable only for noncommercial purposes or academic ) and data whose license status is not specified precisely enough to allow us to determine research ( commercial use permissions ( ). Note that each collection may have different datasets with one, two, or all three of these statuses. Datasets are sorted chronologically to highlight trends over time. COLLECTION CATEGORY PERM YEAR HR SPKR LANG CREAT TASKS SRC TOP US AC IND USE PROPERTY COUNTS TIMIT Switchboard African Acc. French CSJ Fisher CSLU 22 Langs. AMI CSLU 1.2 ALLSSTAR 1990 630 5 1992 250 543 232 2003 22 1k 2003 661 12k 2k 2004 - 2005 84 - 2005 100 5k 25 2007 140 86 2010 3 1 1 1 1 1 1 1 1 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 7 70 7 2 36 7 2 1 3 Continued on next page 1 1 1 1 1 21 1 1 27 24 The Data Provenance Initiative, 2024 Table 6: Audio collections and properties. COLLECTION CATEGORY PERM YEAR HR SPKR LANG CREAT TASKS SRC TOP US AC IND USE PROPERTY COUNTS 1k 2018 2018 96 2018 691 2019 20k 4k 2019 2019 1k 2019 755 2019 31k 330k 124 2019 154k 2019 2012 452 2k 2013 540 870 - 2013 500 - 2013 300 - 56 2014 40 35 2015 2k 1k 2015 371 20 2015 110 44 2016 2016 960 1k 2017 520 400 24 2017 56 2017 1k 2018 TED-LIUM3 NST Norwegian NST Danish NST Swedish Vystadial THCHS-30 LibriSpeech THUYG-20 VCTK Spoken Wikipedia AISHELL-1 1 LJSpeech 317 ClarinPL AISHELL-2 2k Regional Af. Am. Lang. 2018 159 222 3k Crowd Sourced Speech 181 Zeroth-Korean - RTVE - OpenSTT 2k MuST-C - M-AILABS MAGICDATA 1k Common Voice 17 CoNASE Nigerian English Norwegian Parl. Speech 2019 140 309 17 120h Spanish Speech 2019 120 2020 800 DiDiSpeech 6k 2020 444 212 Czech Parliament 78k 3k 2020 CoVoST-2 - 2020 332 KSC 132 2020 Basq., Cat. and Gal. 34 2k 2020 969 KsponSpeech 8k 2020 145 Samromur 6k 2020 50k Multiling. LibriSpeech - 2020 160 MaSS 434 2020 FT SPEECH 2k 120 31 2020 Eng. Acc. in Brit. Isles - Highland Puebla Nahuatl 2021 156 11k 2021 QASR 2k - 2021 765 Multiling. TEDx - 25 2021 Minds14 - 1k 2021 Golos - - 1 1 1 1 2 1 1 1 1 3 1 1 1 1 1 5 1 1 1 16 8 1 1 1 1 1 1 1 22 1 3 1 1 8 8 1 1 1 1 9 14 1 2 1 1 1 1 1 1 2 1 1 2 1 1 2 1 1 1 1 2 2 1 1 3 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 3 2 3 1 3 2 1 1 1 1 1 1 2 1 1 2 1 1 2 1 1 1 1 2 2 1 1 3 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 3 2 3 1 3 1 1 7 1 7 1 7 1 3 2 1 1 1 106 3 1 1 1 1 1 11 2 1 1 7 2 8 1 8 1 1 1 7 1 7 1 6 2 4 1 33 1 1 1 1 1 6 1 7 1 7 1 7 1 2 1 7 1 1 2 5 1 2 1 6 1 5 1 33 1 1 1 2 1 4 1 7 1 7 1 7 1 7 2 6 1 Continued on next page 25 The Data Provenance Initiative, 2024 Table 6: Audio collections and properties. COLLECTION CATEGORY PERM YEAR HR SPKR LANG CREAT TASKS SRC TOP US AC IND USE PROPERTY COUNTS 14k 2021 1k MASC - 2021 2k LaboroTVSpeech 27k 2021 2k KeSpeech - 1k 2021 JTUBESPEECH - 2021 10k GigaSpeech 4k 2k 2021 VoxPopuli 50k 2021 5k SPGISpeech - 2021 142 West Afr. Radio 61 2021 120 AISHELL-4 49 2 2021 West Afr. Virt. Asst. - 40 2021 MediaSpeech - 2021 30k Peoples Speech - 2022 108 1111 Hours Hindi - 2022 6k Shrutilipi - 2022 10k WenetSpeech 3k 2022 131 Samromur Children 2022 200 4k SDS-200 2022 200 600 aidatatang - 1k 2022 Fleurs 2022 1k 1k OLKAVS 2022 140 267 Norwegian Parl. 2022 180 663 MagicData-RAMC 1k 2k 2022 Kathbath - 9 2022 Hebrew Kan - 2022 36 Hebrew Coursera - 2022 428 Bloom Speech - 2022 508 English-Vietnamese 2022 119 125 Earnings-22 - 2023 370k YODAS 2k 2023 200 AFRISPEECH-200 449 2023 3k Aalto Finnish Parl. - 2023 35k ReazonSpeech 120 40 2023 EdAcc - 5k 2023 RixVox - Japanese Anime Speech 2023 110 2023 273 Snow Mountain 11 2023 967 17k Samromur Milljon - 2024 500 Bud500 2024 VibraVox 200 18 Mult. 448 655 M2ASR 3 2 1 4 9 1 4 2 4 2 5 7 1 2 4 1 3 1 3 2 2 4 2 1 1 5 1 1 3 14 1 2 1 1 1 2 1 1 1 3 3 2 1 4 9 1 4 2 4 2 5 7 1 2 4 1 3 1 3 2 2 4 2 1 1 5 1 1 3 14 1 2 1 1 1 2 1 1 1 3 1 1 1 1 3 1 1 1 2 1 12 2 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 3 1 1 1 1 1 1 1 1 1 2 1 1 15 7 1 7 24 1 2 3 6 2 1 14 5 1 10 5 2 7 11 14 2 15 3 3 7 8 7 2 1 6 2 1 8 2 7 1 5 4 1 9 1 2 2 2 1 16 1 10 1 3 4 1 1 12 1 1 1 1 102 1 1 1 12 1 1 56 2 1 149 20 1 1 1 1 1 14 1 1 1 26 The Data Provenance Initiative, 2024 Table 7: Video collections and properties. Collection properties include numbers of hours of video, datasets, creator institutions, countries of creator institutions, and data sources. The USE column indicates whether collection includes data freely usable even for commercial purposes ( ), data usable only for noncommercial purposes or academic research ( ) and data whose license status is ). Note that not specified precisely enough to allow us to determine commercial use permissions ( each collection may have different datasets with one, two, or all three of these statuses. Finally, the AVAIL column indicates whether dataset is available online ( ) or has been taken down, usually for legal reasons ( ). Datasets are sorted chronologically to highlight trends over time. COLLECTION PERMISSIONS YEAR HOURS DATASETS COUNTRIES CREATORS SOURCES USE AVAIL PROPERTY COUNTS 20 2009 HOLLYWOOD2 - 2009 Collective 7k 2011 HMDB 26 2012 UCF101 1k 2013 YouCook 40 2013 50 Salads 7 2014 StoryGraphs 9 2014 Hollywood Ext. 2014 77 Breakfast 2014 106k Sports-1M 254 2014 THUMOS 743 2014 VideoStory 1 2014 SumMe 4 2015 TVSum - 2015 Volleyball 849 2015 ActivityNet 381 2015 MovieQA - 2016 Mars 74 2016 NTU RGB+D 41 2016 MSR-VTT 2016 82 Charades 213 2016 VTW 2016 350k Youtube-8M 2016 Narrated Instr. Vid. 2016 TGIF 2017 MultiTHUMOS 2017 ImageNet-Vid PKU-MMD 2017 20BN-SOMETHING 2017 2017 YouCook2 2017 VoxCeleb 2017 Davis 2017 QFVS 2018 DiDeMo 2018 SOA 2018 Charades-Ego 2018 EPIC-KITCHENS 2018 MovieGraphs 2018 How2 7 86 30 9 50 121 176 2k - 20 275 2k 69 100 94 2k 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 2 1 2 1 2 1 1 2 3 1 1 1 2 2 1 2 1 2 1 1 1 1 2 1 1 1 1 1 3 1 1 1 1 3 1 1 1 1 1 2 1 4 1 3 1 1 2 3 4 1 1 4 2 1 4 3 3 1 2 1 2 1 2 2 1 1 1 3 3 1 1 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 Continued on next page 27 The Data Provenance Initiative, 2024 Table 7: Video collections and properties. COLLECTION PERMISSIONS YEAR HOURS DATASETS COUNTRIES CREATORS SOURCES USE AVAIL PROPERTY COUNTS VLOG VaTeX 20BN-jester HowTo100M COIN MMAct HACS CrossTask Moments in Time TRECVid MSA Toyota Smarthome TITAN VIOLIN RareAct TinyVIRAT 100DOH Oops! OmniSource-Web Condensed Movies MovieScenes EEV Movie-Net FineGym HAA500 LEMMA HVU Apes WebVid VideoLT HOMAGE UAV-Human HD-VILA-100M M-MiT Mimetics Spoken Moments QuerYD MAD FERV39k CDAD MVBench VidProm ShareGPT4Video OpenVid-1M FineVideo Disney Vid. Gen. 336 2018 115 2019 2019 13 2019 134k 476 2019 100 2019 833 2019 376 2019 833 2019 1k 2019 516 2019 269 2019 3 2020 582 2020 21 2020 11 2020 5k 2020 50 2020 13k 2020 1k 2020 250 2020 370 2020 3k 2020 708 2020 5 2020 11 2020 96k 2020 36 2021 13k 2021 14k 2021 30 2021 18 2021 372 2021 833 2021 1 2021 417 2021 207 2021 1k 2022 16 2022 215 2022 - 2023 2024 240k 3k 2024 52k 2024 3k 2024 7 2024 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 2 1 2 1 4 1 1 2 1 1 1 3 1 1 1 1 1 2 1 1 1 2 1 3 3 2 2 1 2 1 1 1 1 1 1 1 1 1 2 1 1 1 1 2 1 4 2 2 3 5 1 1 2 1 1 1 5 1 2 1 1 1 2 2 1 1 4 1 5 3 2 4 2 2 1 1 1 3 1 1 1 2 6 2 4 3 1 - 1 1 1 1 1 1 1 1 11 2 1 1 1 1 1 1 1 1 3 1 1 1 1 1 1 2 1 1 1 1 1 1 1 2 1 11 2 1 1 1 12 5 5 5 1 2 Continued on next page 28 The Data Provenance Initiative, 2024 Table 7: Video collections and properties. COLLECTION PERMISSIONS YEAR HOURS DATASETS COUNTRIES CREATORS SOURCES USE AVAIL PROPERTY COUNTS Kinetics Ego4D MPII Project-Aria Ava LSMDC Mult. Mult. Mult. Mult. Mult. Mult. 4k 5k 110 1k 146 3 2 3 2 2 2 1 1 1 1 1 4 1 2 2 1 1 10 2 1 2 1"
        },
        {
            "title": "E CONTRIBUTIONS",
            "content": "Here we break down contributions to this work. Contributors are listed alphabetically, except for team leads who are placed first. Text Datasets Shayne Longpre (lead), Jad Kabbara (lead), Ahmad Anis, Deividas Mataciunas, Diganta Misra, Emad Alghamdi, Enrico Shippole, Jianguo Zhang, Kun Qian, Lester Miranda, Manan Dey, Minnie Liang, Mohammed Hamdy, Nayan Saxena, Niklas Muennighoff, Naana Obeng-Marnu, Robert Mahari, Seonghyeon Ye, Seungone Kim, Shayne Longpre, Shrestha Mohanty, Vipul Gupta, Vivek Sharma, Vu Minh Chien, William Brannon, Xuhui Zhou, Yizhi Li, An Dinh, Caroline Chitongo, Christopher Klamm, Da Yin, Damien Sileo, Ariel Lee Reviewing Text Dataset Metadata Jad Kabbara (lead), Shayne Longpre (lead), Robert Mahari, Damien Sileo, Niklas Muennighoff, William Brannon, Data Explorer Features Shayne Longpre (lead), Christopher Klamm, Vu Minh Chien, Speech Datasets Nikhil Singh (lead), Manuel Cherep (lead), An Dinh, Minnie Liang, Shrestha Mohanty Video Datasets Kush Tiwary (lead), Joanna Materzynska (lead), Vivek Sharma (lead), Shayne Longpre, Robert Mahari, Jad Kabbara, William Brannon, Tobin South, Shrestha Mohanty, Nikhil Singh, Manuel Cherep Data Analysis Shayne Longpre (lead), Nikhil Singh (lead), Manuel Cherep (lead), Kush Tiwary (lead), Joanna Materzynska (lead), Naana Obeng-Marnu (lead), William Brannon (lead), Writing Shayne Longpre (lead), Jad Kabbara (lead), Nikhil Singh, Manuel Cherep, Kush Tiwary, Joanna Materzynska, Robert Mahari Legal Analysis Robert Mahari (lead), Luis Villa Visualizations & Visual Data Analysis Nikhil Singh (lead), Manuel Cherep (lead), Kush Tiwary (lead), Joanna Materzynska (lead), Naana Obeng-Marnu (lead), William Brannon (lead), Shayne Longpre (lead), Ariel Lee, Hamidah Oderinwale, Campbell Lund Senior Advisors Stella Biderman, Sara Hooker, Jad Kabbara, Hanlin Li, Sandy Pentland, Luis Villa, Caiming Xiong"
        },
        {
            "title": "F ATTRIBUTION CARD",
            "content": "Here we provide detailed information about the licenses of each data collection and its constituent datasets, and cite all of the papers (455 in all) which introduced datasets we consider. Text datasets are laid out in Table 8, audio datasets in Table 9, and video datasets in Table 10. Because of the large number of references, we include second bibliography after the tables (named Attribution Card References), with numbered citations in this section referring to that second bibliography. 29 The Data Provenance Initiative, 2024 Table 8: References and licenses for alignment-tuning (text) dataset collections presented in this paper. Collections containing material under more than three distinct licenses are marked as having Various licenses, and we refer readers to our raw data for the full details. Datasets are sorted alphabetically for ease of dataset lookup. Collection Licenses Cite 10k Prompt Ranked AgentInstruct Aya Bactrian-X COBRA Frames COIG Capybara ChatDoctor ChatbotArena Cidar CollectiveCognition Conifer Deita 10K DialogStudio [322], [386], [397], [418], [423] Unspecified Unspecified, CC BY 4.0, MIT License Apache License 2.0 [446] CC BY-SA 3.0, CC BY-NC 4.0 [393] BigScience OpenRAIL-M [429] Various [424], [433] Various Unspecified [395] CC BY 4.0, CC BY-NC 4.0 [427] CC BY-NC 4.0 [432] MIT License [448] Apache License 2.0 Apache License 2.0, CC BY-NC 4.0 [440] Various [1], [22], [37], [63], [69], [70], [77], [86], [93], [99], [105][107], [117], [124], [125], [128], [131], [139], [143], [150], [151], [153], [159], [165], [167], [169], [173], [176], [178], [180], [181], [185], [194] [196], [214], [216], [217], [243], [246], [248], [251], [253], [255], [270], [279], [280], [282], [289], [290], [295], [305], [308], [309], [313], [326], [333], [334], [338], [344], [345], [347], [358], [359], [364], [365], [369], [380], [384] Continued on next page 30 The Data Provenance Initiative, 2024 Table 8: References and licenses for alignment-tuning (text) dataset collections presented in this paper. Collections containing material under more than three distinct licenses are marked as having Various licenses, and we refer readers to our raw data for the full details. Datasets are sorted alphabetically for ease of dataset lookup. Collection Dynosaur Licenses Various EverythingLM ExpertQA Feedback Coll. Glaive Code Asst. Gretel Text-to-SQL HelpSteer Indic-Instr. InstAr KIWI LMSYS-Chat-1M Llama2-MedTuned-Instr. LongAlign-10k Magpie-Pro MathDial MathInstr. MedInstr. Medical Meadow MIT License MIT License MIT License Apache License 2.0 Apache License 2.0 CC BY 4.0 Various Various CC BY-SA 4.0 LMSYS-Chat-1M Dataset License, Anthropic, Llama 2 CC BY-NC 4.0 Anthropic, Apache License 2.0 Meta Llama3 Community License CC BY-SA 4.0, MIT License MIT License Unspecified Various 31 Cite [4], [5], [9], [12], [16], [17], [23], [25], [33], [40], [43], [49], [51], [53], [54], [56], [65], [68], [73], [76], [78], [81], [82], [84], [87], [88], [91], [95], [96], [98], [99], [102], [103], [110], [111], [113][116], [119][122], [127], [129], [132] [136], [138][142], [144], [145], [148], [154][162], [164], [166], [168], [169], [175], [177], [179], [180], [183], [184], [186][189], [191], [192], [195], [197], [198], [200], [204], [209], [212][215], [222], [224], [227], [229], [231] [233], [237][239], [241], [247], [249], [253], [255], [257], [262], [265], [267], [268], [271], [274] [278], [284][287], [291], [297] [301], [313], [314], [317], [318], [323], [328], [335], [336], [338], [342], [343], [346], [349], [352], [353], [355], [356], [361], [362], [370], [374], [381], [382], [400], [405], [408], [411][414], [416], [425], [428] [442] [438] [415] [437] [18], [34], [47], [50], [152], [174], [202], [210], [225], [250], [337], [368], [430], [431], [436], [445] [452] [455] [407] [434] [453] [398] [422] [454] [230], [256], [264], [391] Continued on next page The Data Provenance Initiative, 2024 Table 8: References and licenses for alignment-tuning (text) dataset collections presented in this paper. Collections containing material under more than three distinct licenses are marked as having Various licenses, and we refer readers to our raw data for the full details. Datasets are sorted alphabetically for ease of dataset lookup. Collection MegaWika MetaMathQA Nectar No Robots Open Asst. v2 Open-Platypus OpenGPT Healthcare OpenMathInstr.-1 Orca-Math PII-Masking-200k PMC-LLaMA Instr. Pure-Dove PygmalionAI-PIPPA Reasoning RiddleSense SeaBench SelFee Synth.-GSM8K-Refl. Thai Gen AI UltraChat-200k UltraFeedback Argilla WildChat Licenses CC BY-SA 4.0 MIT License Various CC BY-NC 4.0 Apache License 2.0 Various Cite [383] [421] [392] [269], [363], [385], [387], [396], [410], [451] [449] Unspecified, OGL 3.0 Custom, MIT License, Apache License 2.0 Various Non Commercial Unspecified, Apache License 2.0 Apache License 2.0 Apache License 2.0 Apache License 2.0 MIT License Apache License 2.0 MIT License Meta Llama3 Community License Various [388] CC BY-NC 4.0 Various [426] AI2 ImpACT License - Low Risk [443] [161], [417] [390] [307] [401] [419] 32 The Data Provenance Initiative, 2024 Table 9: References and licenses for audio dataset collections presented in this paper. Collections containing material under more than three distinct licenses are marked as having Various licenses, and we refer readers to our raw data for the full details. Datasets are sorted alphabetically for ease of dataset lookup. Collection Licenses 1111 Hours Hindi 120h Spanish Speech AFRISPEECH-200 AISHELL-1 AISHELL-2 AISHELL-4 ALLSSTAR AMI Aalto Finnish Parl. African Acc. French Basq., Cat. and Gal. Bloom Speech Bud500 CSJ CSLU 1.2 CSLU 22 Langs. ClarinPL CoNASE CoVoST-2 Common Voice 17 Crowd Sourced Speech Czech Parliament DiDiSpeech Earnings-22 EdAcc Eng. Acc. in Brit. Isles English-Vietnamese FT SPEECH Fisher Fleurs GigaSpeech Golos Hebrew Coursera Hebrew Kan Highland Puebla Nahuatl JTUBESPEECH Japanese Anime Speech KSC Kathbath KeSpeech KsponSpeech Custom CC0 1.0 CC BY-NC-SA 4.0 Apache 2.0 Unspecified CC BY-SA 4.0 CC BY 4.0 CC BY 4.0 Custom Apache 2.0 CC BY-SA 4.0 Various Apache 2.0, CC BY-NC-SA 4.0 Custom CSLU Agreement CSLU Agreement CC BY 4.0 Custom CC0 1.0 CC0 1.0 CC BY-SA 4.0 CC BY 4.0 Unspecified Unspecified CC BY-SA 4.0 CC BY-SA 4.0 CC BY-NC-ND 4.0 Custom LDC User Agreement CC BY 4.0 Apache 2.0 Custom Unspecified Unspecified CC BY-NC-SA 3.0 Unspecified CC0 1.0 CC BY 4.0 CC0 1.0 Custom Unspecified 33 Cite [340] [402] [67] [101] [292] [15] [10] [373] [235] [360] [6] [11] [8] [80] [146] [263] [205] [112] [236] [296] [350] [409] [219] [366] [234] [7] [348] [281] [302] [321] [325] [303] [357] [327] [208] Continued on next page The Data Provenance Initiative, Table 9: References and licenses for audio dataset collections presented in this paper. Collections containing material under more than three distinct licenses are marked as having Various licenses, and we refer readers to our raw data for the full details. Datasets are sorted alphabetically for ease of dataset lookup. Collection Licenses Cite LJSpeech LaboroTVSpeech LibriSpeech M-AILABS M2ASR MAGICDATA MASC MaSS MagicData-RAMC MediaSpeech Minds14 MuST-C Multiling. LibriSpeech Multiling. TEDx NST Danish NST Norwegian NST Swedish Nigerian English Norwegian Parl. Norwegian Parl. Speech OLKAVS OpenSTT Peoples Speech QASR RTVE ReazonSpeech Regional Af. Am. Lang. RixVox SDS-200 SPGISpeech Samromur Samromur Children Samromur Milljon Shrutilipi Snow Mountain Spoken Wikipedia Switchboard TED-LIUM3 THCHS-30 THUYG-20 TIMIT Public Domain Custom CC BY 4.0 Custom Unspecified CC BY-NC-ND 4.0 CC BY 4.0 Unspecified CC BY-NC-ND 4.0 CC BY 4.0 CC BY 4.0 CC BY-NC-ND 4.0 CC BY 4.0 CC BY-NC-ND 4.0 CC0 1.0 CC0 1.0 CC0 1.0 CC BY-SA 4.0 CC0 1.0 CC0 1.0 Custom CC BY-NC 4.0 Various Unspecified Custom CDLA-Sharing-1.0 CC BY-NC-SA 4.0 CC BY 4.0 Custom Custom CC BY 4.0 CC BY 4.0 CC BY 4.0 CC0 1.0 CC BY-SA 4.0 CC BY-SA 4.0 LDC User Agreement CC BY-NC-ND 3.0 Apache 2.0 Apache 2.0 LDC User Agreement 34 [75] [273] [36] [447] [83], [90], [332], [399] [389] [211] [378] [304] [294] [149] [252] [320] [371] [371] [404] [203] [293] [312] [420] [367] [315] [245] [354] [341] [406] [137] [2] [109] [42] [39] [3] Continued on next page The Data Provenance Initiative, 2024 Table 9: References and licenses for audio dataset collections presented in this paper. Collections containing material under more than three distinct licenses are marked as having Various licenses, and we refer readers to our raw data for the full details. Datasets are sorted alphabetically for ease of dataset lookup. Collection Licenses VCTK VibraVox VoxPopuli Vystadial WenetSpeech West Afr. Radio West Afr. Virt. Asst. YODAS Zeroth-Korean aidatatang CC BY 4.0 CC BY 4.0 CC0 1.0 CC BY-SA 3.0 CC BY 4.0 CC BY-SA 4.0 CC BY-SA 4.0 CC BY 3.0 CC BY 4.0 CC BY-NC-ND 4.0 Cite [329] [30] [379] [288] [288] [394] The Data Provenance Initiative, 2024 Table 10: References and licenses for video dataset collections presented in this paper. Collections containing material under more than three distinct licenses are marked as having Various licenses, and we refer readers to our raw data for the full details. Datasets are sorted alphabetically for ease of dataset lookup. Collection Licenses 100DOH 20BN-SOMETHING 20BN-jester 50 Salads ActivityNet Apes Ava Breakfast CDAD COIN Charades Charades-Ego Collective Condensed Movies CrossTask Davis DiDeMo Disney Vid. Gen. EEV EPIC-KITCHENS Ego4D FERV39k FineGym FineVideo HAA500 HACS HD-VILA-100M HMDB HOLLYWOOD2 HOMAGE HVU Hollywood Ext. How2 HowTo100M ImageNet-Vid Kinetics LEMMA LSMDC M-MiT MAD MMAct Custom Custom Custom CC BY-NC-SA 4.0 MIT License Unspecified CC BY 4.0 CC BY 4.0 Unspecified Custom Custom Custom Unspecified CC BY 4.0 Unspecified Custom BSD 2-Clause License Apache 2.0 CC BY 4.0 CC BY-NC 4.0 Custom, MIT License CC BY-NC 4.0 CC BY-NC 4.0 CC BY 4.0 Unspecified Custom Custom CC BY 4.0 Unspecified Unspecified Custom MIT License Various Unspecified CC BY-NC 4.0 Unspecified Unspecified Custom, MIT License Unspecified Custom Custom 36 Cite [258] [72] [170] [24] [35] [272] [104], [182] [31] [376] [190] [60] [126] [14] [207] [201] [55] [108] [324] [100] [351] [375] [259] [283] [199] [377] [19] [13] [319] [220] [26] [123] [171] [41] [79], [97], [261] [228] [57], [260] [311] [372] [163] Continued on next page The Data Provenance Initiative, 2024 Table 10: References and licenses for video dataset collections presented in this paper. Collections containing material under more than three distinct licenses are marked as having Various licenses, and we refer readers to our raw data for the full details. Datasets are sorted alphabetically for ease of dataset lookup. Collection MPII MSA MSR-VTT MVBench Mars Mimetics Moments in Time Movie-Net MovieGraphs MovieQA MovieScenes MultiTHUMOS NTU RGB+D Narrated Instr. Vid. OmniSource-Web Oops! OpenVid-1M PKU-MMD Project-Aria QFVS QuerYD RareAct SOA ShareGPT4Video Spoken Moments Sports-1M StoryGraphs SumMe TGIF THUMOS TITAN TRECVid TVSum TinyVIRAT Toyota Smarthome UAV-Human UCF101 VIOLIN VLOG VTW VaTeX Licenses Unspecified, Custom Unspecified Unspecified MIT License Unspecified Unspecified Custom Unspecified Custom Unspecified Unspecified CC BY 4.0 Custom MIT License Apache License 2.0 CC BY-NC-SA 4.0 CC-BY-4.0 Unspecified Apache License 2.0 Unspecified Unspecified Unspecified Unspecified Attribution-NonCommercial 4.0 International Custom CC BY 3.0 Unspecified Unspecified Custom Custom Non Commercial CC BY-NC-SA 4.0 CC BY 3.0 Unspecified Custom Custom Unspecified Unspecified Custom Unspecified CC BY 4.0 Cite [38], [58] [193] [62] [439] [66] [330] [172] [226] [130] [61] [254] [92] [59] [46] [221] [223] [444] [85] [403], [441] [89] [316] [244] [220] [435] [310] [29] [32] [27] [52] [74] [242] [206] [44] [218] [147] [306] [20] [240] [71] [64] [266] Continued on next page 37 The Data Provenance Initiative, 2024 Table 10: References and licenses for video dataset collections presented in this paper. Collections containing material under more than three distinct licenses are marked as having Various licenses, and we refer readers to our raw data for the full details. Datasets are sorted alphabetically for ease of dataset lookup. Collection VidProm VideoLT VideoStory Volleyball VoxCeleb WebVid YouCook YouCook2 Youtube-8M Licenses CC-BY-NC 4.0 Non Commercial Unspecified Unspecified Custom Custom Unspecified MIT License Unspecified Cite [450] [331] [28] [48] [118] [339] [21] [94] [45] ATTRIBUTION CARD REFERENCES [1] C. T. Hemphill, J. J. Godfrey, and G. R. Doddington, The ATIS spoken language systems pilot corpus, in Speech and Natural Language: Proceedings of Workshop Held at Hidden Valley, Pennsylvania, June 24-27,1990, 1990. DOI: 10/cz3442. [Online]. Available: https://aclanthology.org/H90-1021 (visited on 05/01/2024). J. Godfrey, E. Holliman, and J. McDaniel, SWITCHBOARD: Telephone speech corpus for research and development, in [Proceedings] ICASSP-92: 1992 IEEE International Conference on Acoustics, Speech, and Signal Processing, San Francisco, CA, USA: IEEE, 1992, 517520 vol.1, ISBN: 978-0-7803-0532-8. DOI: 10/fp48kw. [Online]. Available: http://ieeexplore.ieee.org/document/225858/ (visited on 05/01/2024). [2] [3] Garofolo, John S., Lamel, Lori F., Fisher, William M., et al., TIMIT acoustic-phonetic continuous speech corpus, Artwork Size: 715776 KB Pages: 715776 KB, 1993. DOI: 10. 35111/17GK-BN40. [Online]. Available: https://catalog.ldc.upenn.edu/ LDC93S1 (visited on 05/01/2024). [4] E. Shriberg, R. Bates, A. Stolcke, et al., Can prosody aid the automatic classification of dialog acts in conversational speech? Language and Speech, vol. 41 ( Pt 3-4), pp. 443492, 1998, ISSN: 0023-8309. DOI: 10.1177/002383099804100410. [5] A. Stolcke, K. Ries, N. Coccaro, et al., Dialogue act modeling for automatic tagging and recognition of conversational speech, Computational Linguistics, vol. 26, no. 3, pp. 339373, 2000, ISSN: 0891-2017, 1530-9312. DOI: 10/dqmv4j. arXiv: cs/0006023. [Online]. Available: http://arxiv.org/abs/cs/0006023 (visited on 10/02/2024). [6] K. Maekawa, Corpus of spontaneous japanese: Its design and evaluation, in Proceedings of the ISCA/IEEE Workshop on Spontaneous Speech Processing and Recognition, 2003, paper MMO2. [Online]. Available: https://www.iscaarchive.org/sspr_2003/ maekawa03_sspr.html (visited on 05/29/2024). [7] C. Cieri, D. Miller, and K. Walker, The fisher corpus: resource for the next generations of speech-to-text, in Proceedings of the Fourth International Conference on Language Resources and Evaluation (LREC04), M. T. Lino, M. F. Xavier, F. Ferreira, R. Costa, and R. Silva, Eds., Lisbon, Portugal: European Language Resources Association (ELRA), 2004. [Online]. Available: http://www.lrec-conf.org/proceedings/lrec2004/ pdf/767.pdf (visited on 05/01/2024). [8] Lander, T, CSLU: 22 languages corpus, 2005. DOI: 10 . 35111 / ZKN2 - 5X88. [Online]. Available: https://catalog.ldc.upenn.edu/LDC2005S26 (visited on 05/29/2024). [9] B. Pang and L. Lee, Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales, 2005. arXiv: cs / 0506075. [Online]. Available: http : //arxiv.org/abs/cs/0506075 (visited on 05/29/2024). 38 The Data Provenance Initiative, [10] J. Carletta, S. Ashby, S. Bourban, et al., The AMI meeting corpus: pre-announcement, in Machine Learning for Multimodal Interaction, S. Renals and S. Bengio, Eds., vol. 3869, Series Title: Lecture Notes in Computer Science, Berlin, Heidelberg: Springer Berlin Heidelberg, 2006, pp. 2839, ISBN: 978-3-540-32549-9. DOI: 10 . 1007 / 11677482 _ 3. [Online]. Available: http://link.springer.com/10.1007/11677482_3 (visited on 05/01/2024). [11] Lander, T, CSLU: Foreign accented english release 1.2, Artwork Size: 1468006 KB Pages: 1468006 KB, 2007. DOI: 10 . 35111 / 0VWP - XN48. [Online]. Available: https : / / catalog.ldc.upenn.edu/LDC2007S08 (visited on 05/01/2024). [12] T. Pedersen, S. V. S. Pakhomov, S. Patwardhan, and C. G. Chute, Measures of semantic similarity and relatedness in the biomedical domain, Journal of Biomedical Informatics, vol. 40, no. 3, pp. 288299, 2007, ISSN: 1532-0464. DOI: 10/fghjwr. [Online]. Available: https: //www.sciencedirect.com/science/article/pii/S1532046406000645 (visited on 10/02/2024). [13] M. Marszalek, I. Laptev, and C. Schmid, Actions in context, in 2009 IEEE Conference on Computer Vision and Pattern Recognition, Miami, FL: IEEE, 2009, pp. 29292936, ISBN: 978-1-4244-3992-8. DOI: 10/d5bs7p. [Online]. Available: https://ieeexplore. ieee.org/document/5206557/ (visited on 05/02/2024). [14] Wongun Choi, K. Shahid, and S. Savarese, What are they doing? : Collective activity classification using spatio-temporal relationship among people, in 2009 IEEE 12th International Conference on Computer Vision Workshops, ICCV Workshops, Kyoto, Japan: IEEE, 2009, pp. 12821289, ISBN: 978-1-4244-4442-7. DOI: 10/fv4tmg. [Online]. Available: http://ieeexplore.ieee.org/document/5457461/ (visited on 05/02/2024). [15] A. Bradlow, ALLSSTAR: Archive of l1 and l2 scripted and spontaneous transcripts and recordings, 2010. [Online]. Available: https : / / speechbox . linguistics . northwestern.edu/#!/?goto=allsstar (visited on 05/01/2024). [16] K. Ganesan, C. Zhai, and J. Han, Opinosis: graph based approach to abstractive summarization of highly redundant opinions, in Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), C.-R. Huang and D. Jurafsky, Eds., Beijing, China: Coling 2010 Organizing Committee, 2010, pp. 340348. [Online]. Available: https: //aclanthology.org/C10-1039 (visited on 10/02/2024). [17] S. Pakhomov, B. McInnes, T. Adam, Y. Liu, T. Pedersen, and G. B. Melton, Semantic similarity and relatedness between clinical terms: An experimental study, AMIA Annual Symposium Proceedings, vol. 2010, pp. 572576, 2010, ISSN: 1942-597X. [Online]. Available: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3041430/ (visited on 10/02/2024). [18] M. Abbas, K. SmaÃ¯li, and D. Berkani, Evaluation of topic identification methods on arabic corpora, Journal of Digital Information Management, vol. 9, no. 5, 2011. [Online]. Available: https://www.dline.info/fpaper/jdim/v9i5/1.pdf (visited on 10/02/2024). [19] H. Kuehne, H. Jhuang, E. Garrote, T. Poggio, and T. Serre, HMDB: large video database for human motion recognition, in 2011 International Conference on Computer Vision, Barcelona, Spain: IEEE, 2011, pp. 25562563, ISBN: 978-1-4577-1102-2. DOI: 10/fxpf8k. [Online]. Available: http : / / ieeexplore . ieee . org / document / 6126543/ (visited on 05/02/2024). [20] K. Soomro, A. R. Zamir, and M. Shah, UCF101: dataset of 101 human actions classes from videos in the wild, 2012. arXiv: 1212.0402[cs]. [Online]. Available: http:// arxiv.org/abs/1212.0402 (visited on 05/01/2024). [21] P. Das, C. Xu, R. F. Doell, and J. J. Corso, thousand frames in just few words: Lingual description of videos through latent topics and sparse object stitching, in 2013 IEEE Conference on Computer Vision and Pattern Recognition, Portland, OR, USA: IEEE, 2013, pp. 26342641, ISBN: 978-0-7695-4989-7. DOI: 10/gtsqzr. [Online]. Available: http: //ieeexplore.ieee.org/document/6619184/ (visited on 05/02/2024). The Data Provenance Initiative, 2024 [22] J. Liu, P. Pasupat, S. Cyphers, and J. Glass, Asgard: portable architecture for multilingual dialogue systems, in 2013 IEEE International Conference on Acoustics, Speech and Signal Processing, Vancouver, BC, Canada: IEEE, 2013, pp. 83868390, ISBN: 978-1-47990356-6. DOI: 10/gtsqxn. [Online]. Available: http://ieeexplore.ieee.org/ document/6639301/ (visited on 05/01/2024). [23] P. Malo, A. Sinha, P. Takala, P. Korhonen, and J. Wallenius, Good debt or bad debt: Detecting semantic orientations in economic texts, 2013. arXiv: 1307.5336[cs,q-fin]. [Online]. Available: http://arxiv.org/abs/1307.5336 (visited on 05/29/2024). [24] S. Stein and S. J. McKenna, Combining embedded accelerometers with computer vision for recognizing food preparation activities, in Proceedings of the 2013 ACM international joint conference on Pervasive and ubiquitous computing, ser. UbiComp 13, New York, NY, USA: Association for Computing Machinery, 2013, pp. 729738, ISBN: 978-1-4503-1770-2. DOI: 10/gtwv9t. [Online]. Available: https://doi.org/10.1145/2493432. 2493482 (visited on 05/29/2024). [25] R. TyleË‡cek and R. Å Ã¡ra, Spatial pattern templates for recognition of objects with regular structure, in Pattern Recognition, J. Weickert, M. Hein, and B. Schiele, Eds., Berlin, Heidelberg: Springer, 2013, pp. 364374, ISBN: 978-3-642-40602-7. DOI: 10/ggwb5g. [26] P. Bojanowski, R. Lajugie, F. Bach, et al., Weakly supervised action labeling in videos under ordering constraints, 2014. arXiv: 1407.1208[cs]. [Online]. Available: http: //arxiv.org/abs/1407.1208 (visited on 05/29/2024). [27] M. Gygli, H. Grabner, H. Riemenschneider, and L. Van Gool, Creating summaries from user videos, in Computer Vision ECCV 2014, D. Fleet, T. Pajdla, B. Schiele, and T. Tuytelaars, Eds., vol. 8695, Series Title: Lecture Notes in Computer Science, Cham: Springer International Publishing, 2014, pp. 505520, ISBN: 978-3-319-10583-3. DOI: 10.1007/ 978-3-319-10584-0_33. [Online]. Available: http://link.springer.com/ 10.1007/978-3-319-10584-0_33 (visited on 05/02/2024). [28] A. Habibian, T. Mensink, and C. G. Snoek, VideoStory: new multimedia embedding for few-example recognition and translation of events, in Proceedings of the 22nd ACM international conference on Multimedia, Orlando Florida USA: ACM, 2014, pp. 1726, ISBN: 978-1-4503-3063-3. DOI: 10/ggs25n. [Online]. Available: https://dl.acm.org/ doi/10.1145/2647868.2654913 (visited on 05/02/2024). [29] A. Karpathy, G. Toderici, S. Shetty, T. Leung, R. Sukthankar, and L. Fei-Fei, Large-scale video classification with convolutional neural networks, in 2014 IEEE Conference on Computer Vision and Pattern Recognition, Columbus, OH, USA: IEEE, 2014, pp. 1725 1732, ISBN: 978-1-4799-5118-5. DOI: 10 / gf4hdn. [Online]. Available: https : / / ieeexplore.ieee.org/document/6909619 (visited on 05/02/2024). [30] M. Korvas, O. PlÃ¡tek, O. DuÅ¡ek, L. Å½ilka, and F. JurË‡cÃ­Ë‡cek, Free english and czech telephone speech corpus shared under the CC-BY-SA 3.0 license, in Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC14), N. Calzolari, K. Choukri, T. Declerck, et al., Eds., Reykjavik, Iceland: European Language Resources Association (ELRA), 2014, pp. 44234428. [Online]. Available: http://www.lrec-conf. org/proceedings/lrec2014/pdf/535_Paper.pdf (visited on 05/01/2024). [31] H. Kuehne, A. Arslan, and T. Serre, The language of actions: Recovering the syntax and semantics of goal-directed human activities, in 2014 IEEE Conference on Computer Vision and Pattern Recognition, Columbus, OH, USA: IEEE, 2014, pp. 780787, ISBN: 978-1-47995118-5. DOI: 10/gqdc3v. [Online]. Available: https://ieeexplore.ieee.org/ document/6909500/ (visited on 05/02/2024). [32] M. Tapaswi, M. Bauml, and R. Stiefelhagen, StoryGraphs: Visualizing character interactions as timeline, in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2014, pp. 827834. [Online]. Available: https://openaccess.thecvf. com/content_cvpr_2014/html/Tapaswi_StoryGraphs_Visualizing_ Character_2014_CVPR_paper.html (visited on 05/29/2024). 40 The Data Provenance Initiative, 2024 [33] A. Bravo, J. Pinero, N. Queralt-Rosinach, M. Rautschka, and L. I. Furlong, Extraction of relations between genes and diseases from text and large-scale data analysis: Implications for translational research, BMC Bioinformatics, vol. 16, no. 1, p. 55, 2015, ISSN: 1471-2105. DOI: 10/f7kn8s. [Online]. Available: https://doi.org/10.1186/s12859015-0472-9 (visited on 10/02/2024). [34] H. ElSahar and S. R. El-Beltagy, Building large arabic multi-domain resources for sentiment analysis, in Computational Linguistics and Intelligent Text Processing, A. Gelbukh, Ed., Cham: Springer International Publishing, 2015, pp. 2334, ISBN: 978-3-319-18117-2. DOI: 10/g6k58r. [35] F. C. Heilbron, V. Escorcia, B. Ghanem, and J. C. Niebles, ActivityNet: large-scale video benchmark for human activity understanding, in 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Boston, MA, USA: IEEE, 2015, pp. 961970, ISBN: 9781-4673-6964-0. DOI: 10/gfsvdw. [Online]. Available: http://ieeexplore.ieee. org/document/7298698/ (visited on 05/02/2024). [36] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, Librispeech: An ASR corpus based on public domain audio books, in 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), South Brisbane, Queensland, Australia: IEEE, 2015, pp. 52065210, ISBN: 978-1-4673-6997-8. DOI: 10/gfv84w. [Online]. Available: http://ieeexplore.ieee.org/document/7178964/ (visited on 05/01/2024). [37] P. Pasupat and P. Liang, Compositional semantic parsing on semi-structured tables, in Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), C. Zong and M. Strube, Eds., Beijing, China: Association for Computational Linguistics, 2015, pp. 14701480. DOI: 10 / gfz98s. [Online]. Available: https : / / aclanthology.org/P15-1142 (visited on 05/01/2024). [38] A. Rohrbach, M. Rohrbach, N. Tandon, and B. Schiele, dataset for movie description, 2015. arXiv: 1501.02530[cs]. [Online]. Available: http://arxiv.org/abs/1501. 02530 (visited on 05/01/2024). [39] A. Rozi, Dong Wang, Zhiyong Zhang, and T. F. Zheng, An open/free database and benchmark for uyghur speaker recognition, in 2015 International Conference Oriental COCOSDA held jointly with 2015 Conference on Asian Spoken Language Research and Evaluation (O-COCOSDA/CASLRE), Shanghai, China: IEEE, 2015, pp. 8185, ISBN: 978-1-46738279-3. DOI: 10/grh5rd. [Online]. Available: http://ieeexplore.ieee.org/ document/7357869/ (visited on 05/01/2024). [40] A. M. Rush, S. Chopra, and J. Weston, neural attention model for abstractive sentence summarization, 2015. arXiv: 1509.00685[cs]. [Online]. Available: http://arxiv. org/abs/1509.00685 (visited on 05/29/2024). [41] O. Russakovsky, J. Deng, H. Su, et al., ImageNet large scale visual recognition challenge, International Journal of Computer Vision, vol. 115, no. 3, pp. 211252, 2015, ISSN: 15731405. DOI: 10/gcgk7w. [Online]. Available: https://doi.org/10.1007/s11263015-0816-y (visited on 05/29/2024). [42] D. Wang and X. Zhang, THCHS-30 : free chinese speech corpus, 2015. arXiv: 1512. 01882[cs]. [Online]. Available: http://arxiv.org/abs/1512.01882 (visited on 05/01/2024). J. Weston, A. Bordes, S. Chopra, et al., Towards AI-complete question answering: set of prerequisite toy tasks, 2015. arXiv: 1502.05698[cs, stat]. [Online]. Available: http://arxiv.org/abs/1502.05698 (visited on 05/29/2024). [43] [44] Yale Song, J. Vallmitjana, A. Stent, and A. Jaimes, TVSum: Summarizing web videos using titles, in 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Boston, MA, USA: IEEE, 2015, pp. 51795187, ISBN: 978-1-4673-6964-0. DOI: 10/gfsj74. [Online]. Available: http://ieeexplore.ieee.org/document/ 7299154/ (visited on 05/02/2024). [45] S. Abu-El-Haija, N. Kothari, J. Lee, et al., YouTube-8m: large-scale video classification benchmark, 2016. arXiv: 1609 . 08675[cs]. [Online]. Available: http : / / arxiv . org/abs/1609.08675 (visited on 05/01/2024). 41 The Data Provenance Initiative, 2024 [46] J.-B. Alayrac, P. Bojanowski, N. Agrawal, J. Sivic, I. Laptev, and S. Lacoste-Julien, Unsupervised learning from narrated instruction videos, 2016. arXiv: 1506.09215[cs]. [Online]. Available: http://arxiv.org/abs/1506.09215 (visited on 05/01/2024). [47] A. Elnagar and O. Einea, BRAD 1.0: Book reviews in arabic dataset, in 2016 IEEE/ACS 13th International Conference of Computer Systems and Applications (AICCSA), Agadir, Morocco: IEEE, 2016, pp. 18, ISBN: 978-1-5090-4320-0. DOI: 10/g6k6jm. [Online]. Available: http://ieeexplore.ieee.org/document/7945800/ (visited on 10/02/2024). [48] M. Ibrahim, S. Muralidharan, Z. Deng, A. Vahdat, and G. Mori, hierarchical deep temporal model for group activity recognition, 2016. arXiv: 1511.06040[cs]. [Online]. Available: http://arxiv.org/abs/1511.06040 (visited on 05/01/2024). [49] T. Jurczyk, M. Zhai, and J. D. Choi, SelQA: new benchmark for selection-based question answering, 2016. arXiv: 1606.08513[cs]. [Online]. Available: http://arxiv.org/ abs/1606.08513 (visited on 10/02/2024). I. A. El-khair, 1.5 billion words arabic corpus, 2016. arXiv: 1611.04033[cs]. [Online]. Available: http://arxiv.org/abs/1611.04033 (visited on 10/02/2024). [50] [51] R. Lebret, D. Grangier, and M. Auli, Neural text generation from structured data with application to the biography domain, 2016. arXiv: 1603.07771[cs]. [Online]. Available: http://arxiv.org/abs/1603.07771 (visited on 05/29/2024). [52] Y. Li, Y. Song, L. Cao, et al., TGIF: new dataset and benchmark on animated GIF description, 2016. arXiv: 1604 . 02748[cs]. [Online]. Available: http : / / arxiv . org/abs/1604.02748 (visited on 05/01/2024). [53] R. Lowe, N. Pow, I. Serban, and J. Pineau, The ubuntu dialogue corpus: large dataset for research in unstructured multi-turn dialogue systems, 2016. arXiv: 1506.08909[cs]. [Online]. Available: http://arxiv.org/abs/1506.08909 (visited on 10/02/2024). [54] S. Merity, C. Xiong, J. Bradbury, and R. Socher, Pointer sentinel mixture models, 2016. arXiv: 1609.07843[cs]. [Online]. Available: http://arxiv.org/abs/1609.07843 (visited on 05/29/2024). [55] F. Perazzi, J. Pont-Tuset, B. McWilliams, L. Van Gool, M. Gross, and A. Sorkine-Hornung, benchmark dataset and evaluation methodology for video object segmentation, in 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), ISSN: 1063-6919, 2016, pp. 724732. DOI: 10/ggdmmw. [Online]. Available: https://ieeexplore. ieee.org/document/7780454 (visited on 05/29/2024). [56] P. Rajpurkar, J. Zhang, K. Lopyrev, and P. Liang, SQuAD: 100,000+ questions for machine comprehension of text, 2016. arXiv: 1606.05250[cs]. [Online]. Available: http:// arxiv.org/abs/1606.05250 (visited on 05/29/2024). [57] A. Rohrbach, A. Torabi, M. Rohrbach, et al., Movie description, 2016. arXiv: 1605 . 03705[cs]. [Online]. Available: http : / / arxiv . org / abs / 1605 . 03705 (visited on 05/29/2024). [58] M. Rohrbach, A. Rohrbach, M. Regneri, et al., Recognizing fine-grained and composite activities using hand-centric features and script data, International Journal of Computer Vision, vol. 119, no. 3, pp. 346373, 2016, ISSN: 0920-5691, 1573-1405. DOI: 10/f8w6kp. arXiv: 1502.06648[cs]. [Online]. Available: http://arxiv.org/abs/1502. 06648 (visited on 05/01/2024). [59] A. Shahroudy, J. Liu, T.-T. Ng, and G. Wang, NTU RGB+d: large scale dataset for 3d human activity analysis, 2016. arXiv: 1604 . 02808[cs]. [Online]. Available: http : //arxiv.org/abs/1604.02808 (visited on 05/02/2024). [60] G. A. Sigurdsson, G. Varol, X. Wang, A. Farhadi, I. Laptev, and A. Gupta, Hollywood in homes: Crowdsourcing data collection for activity understanding, 2016. arXiv: 1604. 01753[cs]. [Online]. Available: http://arxiv.org/abs/1604.01753 (visited on 05/01/2024). [61] M. Tapaswi, Y. Zhu, R. Stiefelhagen, A. Torralba, R. Urtasun, and S. Fidler, MovieQA: Understanding stories in movies through question-answering, 2016. arXiv: 1512.02902[cs]. [Online]. Available: http://arxiv.org/abs/1512.02902 (visited on 05/29/2024). 42 The Data Provenance Initiative, 2024 [62] J. Xu, T. Mei, T. Yao, and Y. Rui, MSR-VTT: large video description dataset for bridging video and language, in 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), ISSN: 1063-6919, 2016, pp. 52885296. DOI: 10/ggv9gj. [Online]. Available: https://ieeexplore.ieee.org/document/7780940 (visited on 05/29/2024). [63] W.-t. Yih, M. Richardson, C. Meek, M.-W. Chang, and J. Suh, The value of semantic parse labeling for knowledge base question answering, in Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), K. Erk and N. A. Smith, Eds., Berlin, Germany: Association for Computational Linguistics, 2016, pp. 201206. DOI: 10/gkz3hq. [Online]. Available: https://aclanthology.org/P16-2033 (visited on 05/01/2024). [64] K.-H. Zeng, T.-H. Chen, J. C. Niebles, and M. Sun, Title generation for user generated videos, 2016. arXiv: 1608.07068[cs]. [Online]. Available: http://arxiv.org/ abs/1608.07068 (visited on 05/01/2024). [65] X. Zhang, J. Zhao, and Y. LeCun, Character-level convolutional networks for text classification, 2016. arXiv: 1509.01626[cs]. [Online]. Available: http://arxiv.org/abs/ 1509.01626 (visited on 05/29/2024). [66] L. Zheng, Z. Bie, Y. Sun, et al., MARS: video benchmark for large-scale person reidentification, in Computer Vision ECCV 2016, B. Leibe, J. Matas, N. Sebe, and M. Welling, Eds., Cham: Springer International Publishing, 2016, pp. 868884, ISBN: 978-3319-46466-4. DOI: 10/gtwv9w. [67] H. Bu, J. Du, X. Na, B. Wu, and H. Zheng, AISHELL-1: An open-source mandarin speech corpus and speech recognition baseline, 2017. arXiv: 1709 . 05522[cs]. [Online]. Available: http://arxiv.org/abs/1709.05522 (visited on 05/01/2024). [68] A. Chouldechova, Fair prediction with disparate impact: study of bias in recidivism prediction instruments, 2017. arXiv: 1703.00056[cs,stat]. [Online]. Available: http: //arxiv.org/abs/1703.00056 (visited on 10/02/2024). [69] L. El Asri, H. Schulz, S. Sharma, et al., Frames: corpus for adding memory to goaloriented dialogue systems, in Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue, K. Jokinen, M. Stede, D. DeVault, and A. Louis, Eds., SaarbrÃ¼cken, Germany: Association for Computational Linguistics, 2017, pp. 207219. DOI: 10/gtsqx4. [Online]. Available: https://aclanthology.org/W17-5526 (visited on 05/01/2024). [70] M. Eric and C. D. Manning, Key-value retrieval networks for task-oriented dialogue, 2017. arXiv: 1705.05414[cs]. [Online]. Available: http://arxiv.org/abs/1705. 05414 (visited on 05/02/2024). [71] D. F. Fouhey, W.-c. Kuo, A. A. Efros, and J. Malik, From lifestyle vlogs to everyday interactions, 2017. arXiv: 1712.02310[cs]. [Online]. Available: http://arxiv.org/ abs/1712.02310 (visited on 05/01/2024). [72] R. Goyal, S. E. Kahou, V. Michalski, et al., The \"something something\" video database for learning and evaluating visual common sense, 2017. arXiv: 1706.04261[cs]. [Online]. Available: http://arxiv.org/abs/1706.04261 (visited on 05/01/2024). [73] D. Ha and D. Eck, neural representation of sketch drawings, 2017. arXiv: 1704 . 03477[cs,stat]. [Online]. Available: http://arxiv.org/abs/1704.03477 (visited on 10/02/2024). [74] H. Idrees, A. R. Zamir, Y.-G. Jiang, et al., The THUMOS challenge on action recognition for videos \"in the wild\", Computer Vision and Image Understanding, vol. 155, pp. 123, 2017, ISSN: 10773142. DOI: 10/f9rwnr. arXiv: 1604.06182[cs]. [Online]. Available: http://arxiv.org/abs/1604.06182 (visited on 05/02/2024). [75] K. Ito and L. Johnson, The LJ Speech Dataset, 2017. [Online]. Available: https : / / keithito.com/LJ-Speech-Dataset (visited on 05/01/2024). [76] S. Iyer, I. Konstas, A. Cheung, J. Krishnamurthy, and L. Zettlemoyer, Learning neural semantic parser from user feedback, 2017. arXiv: 1704.08760[cs]. [Online]. Available: http://arxiv.org/abs/1704.08760 (visited on 10/02/2024). 43 The Data Provenance Initiative, 2024 [77] M. Iyyer, W.-t. Yih, and M.-W. Chang, Search-based neural structured learning for sequential question answering, in Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), R. Barzilay and M.-Y. Kan, Eds., Vancouver, Canada: Association for Computational Linguistics, 2017, pp. 18211831. DOI: 10/gf6nx8. [Online]. Available: https://aclanthology.org/P17-1167 (visited on 05/01/2024). [78] M. Joshi, E. Choi, D. S. Weld, and L. Zettlemoyer, TriviaQA: large scale distantly supervised challenge dataset for reading comprehension, 2017. arXiv: 1705.03551[cs]. [Online]. Available: http://arxiv.org/abs/1705.03551 (visited on 05/29/2024). [79] W. Kay, J. Carreira, K. Simonyan, et al., The kinetics human action video dataset, 2017. arXiv: 1705.06950[cs]. [Online]. Available: http://arxiv.org/abs/1705.06950 (visited on 05/01/2024). [80] D. Korzinek, K. Marasek, L. Brocki, and K. Wolk, Polish read speech corpus for speech tools and services, 2017. arXiv: 1706.00245[cs]. [Online]. Available: http://arxiv. org/abs/1706.00245 (visited on 05/29/2024). [81] G. Lai, Q. Xie, H. Liu, Y. Yang, and E. Hovy, RACE: Large-scale ReAding comprehension dataset from examinations, 2017. arXiv: 1704.04683[cs]. [Online]. Available: http: //arxiv.org/abs/1704.04683 (visited on 05/29/2024). [82] M. Lewis, D. Yarats, Y. N. Dauphin, D. Parikh, and D. Batra, Deal or no deal? end-to-end learning for negotiation dialogues, 2017. arXiv: 1706.05125[cs]. [Online]. Available: http://arxiv.org/abs/1706.05125 (visited on 05/29/2024). [83] G. Li, H. Yu, T. F. Zheng, J. Yan, and S. Xu, Free linguistic and speech resources for tibetan, in 2017 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC), Kuala Lumpur: IEEE, 2017, pp. 733736, ISBN: 978-1-53861542-3. DOI: 10/gtsqzh. [Online]. Available: http://ieeexplore.ieee.org/ document/8282130/ (visited on 05/01/2024). [84] W. Ling, D. Yogatama, C. Dyer, and P. Blunsom, Program induction by rationale generation : Learning to solve and explain algebraic word problems, 2017. arXiv: 1705.04146[cs]. [Online]. Available: http://arxiv.org/abs/1705.04146 (visited on 05/29/2024). [85] C. Liu, Y. Hu, Y. Li, S. Song, and J. Liu, PKU-MMD: large scale benchmark for continuous multi-modal human action understanding, 2017. arXiv: 1703 . 07475[cs]. [Online]. Available: http://arxiv.org/abs/1703.07475 (visited on 05/01/2024). [86] N. Mrksic, D. O. Seaghdha, T.-H. Wen, B. Thomson, and S. Young, Neural belief tracker: Data-driven dialogue state tracking, 2017. arXiv: 1606.03777[cs]. [Online]. Available: http://arxiv.org/abs/1606.03777 (visited on 05/01/2024). J. Novikova, O. DuÅ¡ek, and V. Rieser, The e2e dataset: New challenges for end-to-end generation, 2017. arXiv: 1706 . 09254[cs]. [Online]. Available: http : / / arxiv . org/abs/1706.09254 (visited on 05/29/2024). [87] [88] A. See, P. J. Liu, and C. D. Manning, Get to the point: Summarization with pointer-generator networks, 2017. arXiv: 1704.04368[cs]. [Online]. Available: http://arxiv.org/ abs/1704.04368 (visited on 05/29/2024). [89] A. Sharghi, J. S. Laurel, and B. Gong, Query-focused video summarization: Dataset, evaluation, and memory network based approach, 2017. arXiv: 1707.04960[cs]. [Online]. Available: http://arxiv.org/abs/1707.04960 (visited on 05/01/2024). [90] Y. Shi, A. Hamdullah, Z. Tang, D. Wang, and T. F. Zheng, free kazakh speech database and speech recognition baseline, in 2017 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC), Kuala Lumpur: IEEE, 2017, pp. 745748, ISBN: 978-1-5386-1542-3. DOI: 10/gtsqzf. [Online]. Available: http: //ieeexplore.ieee.org/document/8282133/ (visited on 05/01/2024). J. Welbl, N. F. Liu, and M. Gardner, Crowdsourcing multiple choice science questions, 2017. arXiv: 1707.06209[cs, stat]. [Online]. Available: http://arxiv.org/abs/ 1707.06209 (visited on 05/29/2024). [91] The Data Provenance Initiative, 2024 [92] S. Yeung, O. Russakovsky, N. Jin, M. Andriluka, G. Mori, and L. Fei-Fei, Every moment counts: Dense detailed labeling of actions in complex videos, 2017. arXiv: 1507 . 05738[cs]. [Online]. Available: http://arxiv.org/abs/1507.05738 (visited on 05/02/2024). [93] V. Zhong, C. Xiong, and R. Socher, Seq2sql: Generating structured queries from natural language using reinforcement learning, 2017. arXiv: 1709.00103[cs]. [Online]. Available: http://arxiv.org/abs/1709.00103 (visited on 05/01/2024). [94] L. Zhou, C. Xu, and J. J. Corso, Towards automatic learning of procedures from web instructional videos, 2017. arXiv: 1703.09788[cs]. [Online]. Available: https:// arxiv.org/abs/1703.09788v3 (visited on 05/02/2024). [96] [95] P. Bajaj, D. Campos, N. Craswell, et al., MS MARCO: human generated MAchine reading COmprehension dataset, 2018. arXiv: 1611.09268[cs]. [Online]. Available: http: //arxiv.org/abs/1611.09268 (visited on 05/29/2024). J. A. Botha, M. Faruqui, J. Alex, J. Baldridge, and D. Das, Learning to split and rephrase from wikipedia edit history, 2018. arXiv: 1808.09468[cs]. [Online]. Available: http: //arxiv.org/abs/1808.09468 (visited on 10/02/2024). J. Carreira, E. Noland, A. Banki-Horvath, C. Hillier, and A. Zisserman, short note about kinetics-600, 2018. arXiv: 1808.01340[cs]. [Online]. Available: http://arxiv. org/abs/1808.01340 (visited on 05/01/2024). [97] [98] P. Clark, I. Cowhey, O. Etzioni, et al., Think you have solved question answering? try ARC, the AI2 reasoning challenge, 2018. arXiv: 1803.05457[cs]. [Online]. Available: http://arxiv.org/abs/1803.05457 (visited on 05/29/2024). [99] A. Coucke, A. Saade, A. Ball, et al., Snips voice platform: An embedded spoken language understanding system for private-by-design voice interfaces, 2018. arXiv: 1805.10190[cs]. [Online]. Available: http://arxiv.org/abs/1805.10190 (visited on 05/01/2024). [100] D. Damen, H. Doughty, G. M. Farinella, et al., Scaling egocentric vision: The EPICKITCHENS dataset, 2018. arXiv: 1804 . 02748[cs]. [Online]. Available: http : / / arxiv.org/abs/1804.02748 (visited on 05/01/2024). J. Du, X. Na, X. Liu, and H. Bu, AISHELL-2: Transforming mandarin ASR research into industrial scale, 2018. arXiv: 1808.10583[cs]. [Online]. Available: http://arxiv. org/abs/1808.10583 (visited on 05/31/2024). [101] [102] M. Faruqui and D. Das, Identifying well-formed natural language questions, 2018. arXiv: 1808.09419[cs]. [Online]. Available: http://arxiv.org/abs/1808.09419 (visited on 05/29/2024). [103] G. Gorrell, K. Bontcheva, L. Derczynski, E. Kochkina, M. Liakata, and A. Zubiaga, RumourEval 2019: Determining rumour veracity and support for rumours, 2018. arXiv: 1809. 06683[cs]. [Online]. Available: http://arxiv.org/abs/1809.06683 (visited on 05/29/2024). [104] C. Gu, C. Sun, D. A. Ross, et al., AVA: video dataset of spatio-temporally localized atomic visual actions, 2018. arXiv: 1705.08421[cs]. [Online]. Available: http://arxiv. org/abs/1705.08421 (visited on 05/29/2024). [105] D. Gupta, S. Kumari, A. Ekbal, and P. Bhattacharyya, MMQA: multi-domain multilingual question-answering framework for english and hindi, in Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018), N. Calzolari, K. Choukri, C. Cieri, et al., Eds., Miyazaki, Japan: European Language Resources Association (ELRA), 2018. [Online]. Available: https : / / aclanthology . org / L18 - 1440 (visited on 05/01/2024). [106] S. Gupta, R. Shah, M. Mohit, A. Kumar, and M. Lewis, Semantic parsing for task oriented dialog using hierarchical representations, 2018. arXiv: 1810.07942[cs]. [Online]. Available: http://arxiv.org/abs/1810.07942 (visited on 05/01/2024). [107] H. He, D. Chen, A. Balakrishnan, and P. Liang, Decoupling strategy and generation in negotiation dialogues, 2018. arXiv: 1808.09637[cs]. [Online]. Available: http:// arxiv.org/abs/1808.09637 (visited on 05/01/2024). 45 The Data Provenance Initiative, 2024 [108] L. A. Hendricks, O. Wang, E. Shechtman, J. Sivic, T. Darrell, and B. Russell, Localizing moments in video with temporal language, in Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, E. Riloff, D. Chiang, J. Hockenmaier, and J. Tsujii, Eds., Brussels, Belgium: Association for Computational Linguistics, 2018, pp. 13801390. DOI: 10 / gtsqzq. [Online]. Available: https : / / aclanthology . org/D18-1168 (visited on 05/02/2024). [109] F. Hernandez, V. Nguyen, S. Ghannay, N. Tomashenko, and Y. EstÃ¨ve, TED-LIUM 3: Twice as much data and corpus repartition for experiments on speaker adaptation, in Lecture Notes in Computer Science, vol. 11096, Springer, Cham, 2018, pp. 198208. DOI: 10 . 1007/978331999579- 3_21. arXiv: 1805.04699[cs]. [Online]. Available: http://arxiv.org/abs/1805.04699 (visited on 05/01/2024). [110] T. Khot, A. Sabharwal, and P. Clark, SciTaiL: textual entailment dataset from science question answering, Proceedings of the AAAI Conference on Artificial Intelligence, vol. 32, no. 1, 2018, ISSN: 2374-3468, 2159-5399. DOI: 10/grm22d. [Online]. Available: https: / / ojs . aaai . org / index . php / AAAI / article / view / 12022 (visited on 10/02/2024). [111] S. Kim, I. Kang, and N. Kwak, Semantic sentence matching with densely-connected recurrent and co-attentive information, 2018. arXiv: 1805.11360[cs]. [Online]. Available: http: //arxiv.org/abs/1805.11360 (visited on 10/02/2024). [112] O. Kjartansson, S. Sarin, K. Pipatsrisawat, M. Jansche, and L. Ha, Crowd-sourced speech corpora for javanese, sundanese, sinhala, nepali, and bangladeshi bengali, in 6th Workshop on Spoken Language Technologies for Under-Resourced Languages (SLTU 2018), ISCA, 2018, pp. 5255. DOI: 10/gtwwbs. [Online]. Available: https://www.iscaarchive. org/sltu_2018/kjartansson18_sltu.html (visited on 05/29/2024). [113] B. M. Lake and M. Baroni, Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks, 2018. arXiv: 1711.00350[cs]. [Online]. Available: http://arxiv.org/abs/1711.00350 (visited on 05/29/2024). [114] L. J. Martin, P. Ammanabrolu, X. Wang, et al., Event representations for automated story generation with deep neural nets, Proceedings of the AAAI Conference on Artificial Intelligence, vol. 32, no. 1, 2018, ISSN: 2374-3468, 2159-5399. DOI: 10/g6k72p. arXiv: 1706.01331[cs]. [Online]. Available: http://arxiv.org/abs/1706.01331 (visited on 10/02/2024). [115] T. Mihaylov, P. Clark, T. Khot, and A. Sabharwal, Can suit of armor conduct electricity? new dataset for open book question answering, 2018. arXiv: 1809.02789[cs]. [Online]. Available: http://arxiv.org/abs/1809.02789 (visited on 05/29/2024). [116] N. Moniz and L. Torgo, Multi-source social feedback of online news feeds, 2018. arXiv: 1801.07055[cs]. [Online]. Available: http://arxiv.org/abs/1801.07055 (visited on 10/02/2024). [117] N. MrkÅ¡ic and I. Vulic, Fully statistical neural belief tracking, 2018. arXiv: 1805 . 11350[cs]. [Online]. Available: http : / / arxiv . org / abs / 1805 . 11350 (visited on 05/01/2024). [118] A. Nagrani, J. S. Chung, and A. Zisserman, VoxCeleb: large-scale speaker identification dataset, 2018. DOI: 10 . 21437 / Interspeech . 2017 - 950. arXiv: 1706 . 08612[cs]. [Online]. Available: http://arxiv.org/abs/1706.08612 (visited on 05/29/2024). [119] S. Narayan, S. B. Cohen, and M. Lapata, Dont give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization, 2018. arXiv: 1808. 08745[cs]. [Online]. Available: http://arxiv.org/abs/1808.08745 (visited on 05/29/2024). [120] A. Royer, K. Bousmalis, S. Gouws, et al., XGAN: Unsupervised image-to-image translation for many-to-many mappings, 2018. arXiv: 1711.05139[cs]. [Online]. Available: http: //arxiv.org/abs/1711.05139 (visited on 10/02/2024). [121] M. Saeidi, M. Bartolo, P. Lewis, et al., Interpretation of natural language rules in conversational machine reading, 2018. arXiv: 1809.01494[cs,stat]. [Online]. Available: http://arxiv.org/abs/1809.01494 (visited on 10/02/2024). The Data Provenance Initiative, 2024 [122] A. Saha, R. Aralikatte, M. M. Khapra, and K. Sankaranarayanan, DuoRC: Towards complex language understanding with paraphrased reading comprehension, 2018. arXiv: 1804 . 07927[cs]. [Online]. Available: http://arxiv.org/abs/1804.07927 (visited on 05/29/2024). [123] R. Sanabria, O. Caglayan, S. Palaskar, et al., How2: large-scale dataset for multimodal language understanding, 2018. arXiv: 1811.00347[cs]. [Online]. Available: http: //arxiv.org/abs/1811.00347 (visited on 05/01/2024). [124] P. Shah, D. Hakkani-TÃ¼r, G. TÃ¼r, et al., Building conversational agent overnight with dialogue self-play, 2018. arXiv: 1801.04871[cs]. [Online]. Available: http://arxiv. org/abs/1801.04871 (visited on 05/01/2024). [125] G. Shang, W. Ding, Z. Zhang, et al., Unsupervised abstractive meeting summarization with multi-sentence compression and budgeted submodular maximization, in Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), I. Gurevych and Y. Miyao, Eds., Melbourne, Australia: Association for Computational Linguistics, 2018, pp. 664674. DOI: 10 / gm8jvb. [Online]. Available: https://aclanthology.org/P18-1062 (visited on 05/01/2024). [126] G. A. Sigurdsson, A. Gupta, C. Schmid, A. Farhadi, and K. Alahari, Actor and observer: Joint modeling of first and third-person videos, 2018. arXiv: 1804.09627[cs]. [Online]. Available: http://arxiv.org/abs/1804.09627 (visited on 05/01/2024). [127] O. Tafjord, P. Clark, M. Gardner, W.-t. Yih, and A. Sabharwal, QuaRel: dataset and models for answering questions about qualitative relationships, 2018. arXiv: 1811.08048[cs]. [Online]. Available: http://arxiv.org/abs/1811.08048 (visited on 10/02/2024). [128] A. Talmor and J. Berant, The web as knowledge-base for answering complex questions, in Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), M. Walker, H. Ji, and A. Stent, Eds., New Orleans, Louisiana: Association for Computational Linguistics, 2018, pp. 641651. DOI: 10 / gkz2k6. [Online]. Available: https : //aclanthology.org/N18-1059 (visited on 05/01/2024). J. Thorne, A. Vlachos, C. Christodoulopoulos, and A. Mittal, FEVER: large-scale dataset for fact extraction and VERification, 2018. arXiv: 1803.05355[cs]. [Online]. Available: http://arxiv.org/abs/1803.05355 (visited on 05/29/2024). [129] [130] P. Vicol, M. Tapaswi, L. Castrejon, and S. Fidler, MovieGraphs: Towards understanding human-centric situations from videos, 2018. arXiv: 1712.06761[cs]. [Online]. Available: http://arxiv.org/abs/1712.06761 (visited on 05/29/2024). [131] W. Wei, Q. Le, A. Dai, and J. Li, AirDialogue: An environment for goal-oriented dialogue research, in Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, E. Riloff, D. Chiang, J. Hockenmaier, and J. Tsujii, Eds., Brussels, Belgium: Association for Computational Linguistics, 2018, pp. 38443854. DOI: 10/gf6gq2. [Online]. Available: https://aclanthology.org/D18-1419 (visited on 05/01/2024). J. Welbl, P. Stenetorp, and S. Riedel, Constructing datasets for multi-hop reading comprehension across documents, 2018. arXiv: 1710.06481[cs]. [Online]. Available: http: //arxiv.org/abs/1710.06481 (visited on 10/02/2024). [132] [133] Z. Wu, B. Ramsundar, E. N. Feinberg, et al., MoleculeNet: benchmark for molecular machine learning, 2018. arXiv: 1703.00564[physics, stat]. [Online]. Available: http://arxiv.org/abs/1703.00564 (visited on 10/02/2024). [134] Z. Yang, P. Qi, S. Zhang, et al., HotpotQA: dataset for diverse, explainable multi-hop question answering, 2018. arXiv: 1809 . 09600[cs]. [Online]. Available: http : / / arxiv.org/abs/1809.09600 (visited on 05/29/2024). [135] A. Amini, S. Gabriel, P. Lin, R. Koncel-Kedziorski, Y. Choi, and H. Hajishirzi, MathQA: Towards interpretable math word problem solving with operation-based formalisms, 2019. arXiv: 1905.13319[cs]. [Online]. Available: http://arxiv.org/abs/1905. 13319 (visited on 05/29/2024). 47 The Data Provenance Initiative, 2024 [136] A. Balakrishnan, J. Rao, K. Upasani, M. White, and R. Subba, Constrained decoding for neural NLG from compositional representations in task-oriented dialogue, 2019. arXiv: 1906.07220[cs]. [Online]. Available: http://arxiv.org/abs/1906.07220 (visited on 10/02/2024). [137] T. Baumann, A. KÃ¶hn, and F. Hennig, The spoken wikipedia corpus collection: Harvesting, alignment and an application to hyperlistening, Language Resources and Evaluation, vol. 53, no. 2, pp. 303329, 2019, ISSN: 1574-0218. DOI: 10/gq5xdf. [Online]. Available: https: //doi.org/10.1007/s10579-017-9410-y (visited on 05/29/2024). [138] Y. Bisk, R. Zellers, R. L. Bras, J. Gao, and Y. Choi, PIQA: Reasoning about physical commonsense in natural language, 2019. arXiv: 1911.11641[cs]. [Online]. Available: http://arxiv.org/abs/1911.11641 (visited on 05/29/2024). [139] B. Byrne, K. Krishnamoorthi, C. Sankar, et al., Taskmaster-1: Toward realistic and diverse dialog dataset, 2019. arXiv: 1909.05358[cs]. [Online]. Available: http://arxiv. org/abs/1909.05358 (visited on 05/01/2024). [141] [140] A. Cetoli, M. Akbari, S. Bragaglia, A. D. OHarney, and M. Sloan, Named entity disambiguation using deep learning on graphs, in vol. 11438, 2019, pp. 7886. DOI: 10 . 1007/978303015719- 7_10. arXiv: 1810.09164[cs]. [Online]. Available: http://arxiv.org/abs/1810.09164 (visited on 10/02/2024). I. Chalkidis, I. Androutsopoulos, and N. Aletras, Neural legal judgment prediction in english, 2019. arXiv: 1906.02059[cs]. [Online]. Available: http://arxiv.org/abs/ 1906.02059 (visited on 10/02/2024). I. Chalkidis, M. Fergadiotis, P. Malakasiotis, and I. Androutsopoulos, Large-scale multi-label text classification on EU legislation, 2019. arXiv: 1906.02192[cs]. [Online]. Available: http://arxiv.org/abs/1906.02192 (visited on 05/29/2024). [142] [143] W. Chen, J. Chen, P. Qin, X. Yan, and W. Y. Wang, Semantically conditioned dialog response generation via hierarchical disentangled self-attention, 2019. arXiv: 1905.12866[cs]. [Online]. Available: http://arxiv.org/abs/1905.12866 (visited on 05/01/2024). [144] P. Christmann, R. S. Roy, A. Abujabal, J. Singh, and G. Weikum, Look before you hop: Conversational question answering over knowledge graphs using judicious context expansion, in Proceedings of the 28th ACM International Conference on Information and Knowledge Management, 2019, pp. 729738. DOI: 10/gkz233. arXiv: 1910.03262[cs]. [Online]. Available: http://arxiv.org/abs/1910.03262 (visited on 10/02/2024). [145] C. Clark, K. Lee, M.-W. Chang, T. Kwiatkowski, M. Collins, and K. Toutanova, BoolQ: Exploring the surprising difficulty of natural yes/no questions, 2019. arXiv: 1905 . 10044[cs]. [Online]. Available: http : / / arxiv . org / abs / 1905 . 10044 (visited on 05/29/2024). [146] S. Coats, corpus of regional american language from YouTube, in Proceedings of the Digital Humanities in the Nordic Countries 5th Conference, 2019. [Online]. Available: https : / / www . semanticscholar . org / paper / - Corpus - of - Regional - American - Language - from - YouTube - Coats / bc428db824d261794a7e081a53c4315b8e02f855 (visited on 05/29/2024). [147] S. Das, R. Dai, M. Koperski, et al., Toyota smarthome: Real-world activities of daily living, in 2019 IEEE/CVF International Conference on Computer Vision (ICCV), Seoul, Korea (South): IEEE, 2019, pp. 833842, ISBN: 978-1-72814-803-8. DOI: 10/ghfjc7. [Online]. Available: https://ieeexplore.ieee.org/document/9008135/ (visited on 05/02/2024). [148] P. Dasigi, N. F. Liu, A. Marasovic, N. A. Smith, and M. Gardner, Quoref: reading comprehension dataset with questions requiring coreferential reasoning, 2019. arXiv: 1908. 05803[cs]. [Online]. Available: http://arxiv.org/abs/1908.05803 (visited on 05/29/2024). 48 The Data Provenance Initiative, 2024 [149] M. A. Di Gangi, R. Cattoni, L. Bentivogli, M. Negri, and M. Turchi, MuST-c: multilingual speech translation corpus, in Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), J. Burstein, C. Doran, and T. Solorio, Eds., Minneapolis, Minnesota: Association for Computational Linguistics, 2019, pp. 20122017. DOI: 10/ gtsqzk. [Online]. Available: https://aclanthology.org/N19-1202 (visited on 05/01/2024). [150] E. Dinan, V. Logacheva, V. Malykh, et al., The second conversational intelligence challenge (ConvAI2), 2019. arXiv: 1902.00098[cs]. [Online]. Available: http://arxiv.org/ abs/1902.00098 (visited on 05/01/2024). [151] E. Dinan, S. Roller, K. Shuster, A. Fan, M. Auli, and J. Weston, Wizard of wikipedia: Knowledge-powered conversational agents, 2019. arXiv: 1811.01241[cs]. [Online]. Available: http://arxiv.org/abs/1811.01241 (visited on 05/01/2024). [152] O. Einea, A. Elnagar, and R. Al Debsi, SANAD: Single-label arabic news articles dataset for automatic text categorization, Data in Brief, vol. 25, p. 104 076, 2019, ISSN: 23523409. DOI: 10/g6k6cn. [Online]. Available: https://linkinghub.elsevier.com/ retrieve/pii/S2352340919304305 (visited on 10/02/2024). [153] M. Eric, R. Goel, S. Paul, et al., MultiWOZ 2.1: consolidated multi-domain dialogue dataset with state corrections and state tracking baselines, 2019. arXiv: 1907.01669[cs]. [Online]. Available: http://arxiv.org/abs/1907.01669 (visited on 05/01/2024). [154] A. R. Fabbri, I. Li, T. She, S. Li, and D. R. Radev, Multi-news: large-scale multi-document summarization dataset and abstractive hierarchical model, 2019. arXiv: 1906.01749[cs]. [Online]. Available: http://arxiv.org/abs/1906.01749 (visited on 05/29/2024). [155] Y. Gallina, F. Boudin, and B. Daille, KPTimes: large-scale dataset for keyphrase generation on news documents, in Proceedings of the 12th International Conference on Natural Language Generation, K. van Deemter, C. Lin, and H. Takamura, Eds., Tokyo, Japan: Association for Computational Linguistics, 2019, pp. 130135. DOI: 10/g6k64k. [Online]. Available: https://aclanthology.org/W19-8617 (visited on 10/02/2024). [156] M. G. Gazzola, S. E. Leal, and S. M. AluÃ­sio, PrediÃ§Ã£o da complexidade textual de recursos educacionais abertos em portuguÃªs, Symposium in Information and Human Language Technology - STIL, 2019. [Online]. Available: https : / / repositorio . usp . br / item/002971271 (visited on 10/02/2024). [157] E. J. George and R. Mamidi, Conversational implicatures in english dialogue: Annotated dataset, 2019. arXiv: 1911.10704[cs]. [Online]. Available: http://arxiv.org/ abs/1911.10704 (visited on 05/29/2024). [158] M. Geva, E. Malmi, I. Szpektor, and J. Berant, DiscoFuse: large-scale dataset for discoursebased sentence fusion, 2019. arXiv: 1902.10526[cs]. [Online]. Available: http:// arxiv.org/abs/1902.10526 (visited on 05/29/2024). [159] B. Gliwa, I. Mochol, M. Biesek, and A. Wawer, SAMSum corpus: human-annotated dialogue dataset for abstractive summarization, in Proceedings of the 2nd Workshop on New Frontiers in Summarization, L. Wang, J. C. K. Cheung, G. Carenini, and F. Liu, Eds., Hong Kong, China: Association for Computational Linguistics, 2019, pp. 7079. DOI: 10/ gmjqgr. [Online]. Available: https://aclanthology.org/D19-5409 (visited on 05/01/2024). [160] L. Huang, R. L. Bras, C. Bhagavatula, and Y. Choi, Cosmos QA: Machine reading comprehension with contextual commonsense reasoning, 2019. arXiv: 1909.00277[cs]. [Online]. Available: http://arxiv.org/abs/1909.00277 (visited on 05/29/2024). [161] Q. Jin, B. Dhingra, Z. Liu, W. W. Cohen, and X. Lu, PubMedQA: dataset for biomedical research question answering, 2019. arXiv: 1909.06146[cs,q-bio]. [Online]. Available: http://arxiv.org/abs/1909.06146 (visited on 05/01/2024). J. Juraska, K. K. Bowden, and M. Walker, ViGGO: video game corpus for data-to-text generation in open-domain conversation, 2019. arXiv: 1910.12129[cs]. [Online]. Available: http://arxiv.org/abs/1910.12129 (visited on 10/02/2024). [162] 49 The Data Provenance Initiative, 2024 [163] Q. Kong, Z. Wu, Z. Deng, M. Klinkigt, B. Tong, and T. Murakami, MMAct: large-scale dataset for cross modal human action understanding, in 2019 IEEE/CVF International Conference on Computer Vision (ICCV), ISSN: 2380-7504, 2019, pp. 86578666. DOI: 10/ghfhxx. [Online]. Available: https://ieeexplore.ieee.org/document/ 9009579 (visited on 05/29/2024). [164] A. Kornilova and V. Eidelman, BillSum: corpus for automatic summarization of US legislation, in Proceedings of the 2nd Workshop on New Frontiers in Summarization, 2019, pp. 4856. DOI: 10/gtwwtd. arXiv: 1910.00523[cs]. [Online]. Available: http: //arxiv.org/abs/1910.00523 (visited on 05/29/2024). [165] S. Larson, A. Mahendran, J. J. Peper, et al., An evaluation dataset for intent classification and out-of-scope prediction, in Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), K. Inui, J. Jiang, V. Ng, and X. Wan, Eds., Hong Kong, China: Association for Computational Linguistics, 2019, pp. 13111316. DOI: 10/ gqrfvv. [Online]. Available: https://aclanthology.org/D19-1131 (visited on 05/01/2024). [166] H. Li, S. Kim, and S. Chandra, Neural code search evaluation dataset, 2019. arXiv: 1908. 09804[cs]. [Online]. Available: http://arxiv.org/abs/1908.09804 (visited on 10/02/2024). [167] Y. Li, K. Qian, W. Shi, and Z. Yu, End-to-end trainable non-collaborative dialog system, 2019. arXiv: 1911.10742[cs]. [Online]. Available: http://arxiv.org/abs/ 1911.10742 (visited on 05/01/2024). [168] K. Lin, O. Tafjord, P. Clark, and M. Gardner, Reasoning over paragraph effects in situations, 2019. arXiv: 1908.05852[cs]. [Online]. Available: http://arxiv.org/abs/ 1908.05852 (visited on 05/29/2024). [170] [169] X. Liu, A. Eshghi, P. Swietojanski, and V. Rieser, Benchmarking natural language understanding services for building conversational agents, 2019. arXiv: 1903 . 05566[cs]. [Online]. Available: http://arxiv.org/abs/1903.05566 (visited on 05/01/2024). J. Materzynska, G. Berger, I. Bax, and R. Memisevic, The jester dataset: large-scale video dataset of human gestures, in 2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW), Seoul, Korea (South): IEEE, 2019, pp. 28742882, ISBN: 978-1-72815023-9. DOI: 10/gh5k47. [Online]. Available: https://ieeexplore.ieee.org/ document/9022297/ (visited on 05/02/2024). [171] A. Miech, D. Zhukov, J.-B. Alayrac, M. Tapaswi, I. Laptev, and J. Sivic, HowTo100m: Learning text-video embedding by watching hundred million narrated video clips, 2019. arXiv: 1906.03327[cs]. [Online]. Available: http://arxiv.org/abs/1906. 03327 (visited on 05/01/2024). [172] M. Monfort, A. Andonian, B. Zhou, et al., Moments in time dataset: One million videos for event understanding, 2019. arXiv: 1801. 03150[cs]. [Online]. Available: http: //arxiv.org/abs/1801.03150 (visited on 05/02/2024). [173] S. Moon, P. Shah, A. Kumar, and R. Subba, OpenDialKG: Explainable conversational reasoning with attention-based walks over knowledge graphs, in Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, A. Korhonen, D. Traum, and L. MÃ rquez, Eds., Florence, Italy: Association for Computational Linguistics, 2019, pp. 845854. DOI: 10/ggt4wm. [Online]. Available: https://aclanthology.org/ P19-1081 (visited on 05/01/2024). [174] H. Mozannar, K. E. Hajal, E. Maamary, and H. Hajj, Neural arabic question answering, 2019. arXiv: 1906.05394[cs]. [Online]. Available: http://arxiv.org/abs/1906. 05394 (visited on 10/02/2024). [175] L. Perez-Beltrachini, Y. Liu, and M. Lapata, Generating summaries with topic templates and structured convolutional decoders, 2019. arXiv: 1906.04687[cs]. [Online]. Available: http://arxiv.org/abs/1906.04687 (visited on 10/02/2024). 50 The Data Provenance Initiative, 2024 [176] D. Peskov, N. Clarke, J. Krone, et al., Multi-domain goal-oriented dialogues (MultiDoGO): Strategies toward curating and annotating large scale dialogue data, in Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), K. Inui, J. Jiang, V. Ng, and X. Wan, Eds., Hong Kong, China: Association for Computational Linguistics, 2019, pp. 45264536. DOI: 10 / gkr9hj. [Online]. Available: https : / / aclanthology.org/D19-1460 (visited on 05/01/2024). [177] F. Petroni, T. RocktÃ¤schel, P. Lewis, et al., Language models as knowledge bases? 2019. arXiv: 1909.01066[cs]. [Online]. Available: http://arxiv.org/abs/1909.01066 (visited on 10/02/2024). [178] J. Quan, D. Xiong, B. Webber, and C. Hu, GECOR: An end-to-end generative ellipsis and coreference resolution model for task-oriented dialogue, in Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), K. Inui, J. Jiang, V. Ng, and X. Wan, Eds., Hong Kong, China: Association for Computational Linguistics, 2019, pp. 4547 4557. DOI: 10/gk3btq. [Online]. Available: https://aclanthology.org/D191462 (visited on 05/01/2024). [179] N. F. Rajani, B. McCann, C. Xiong, and R. Socher, Explain yourself! leveraging language models for commonsense reasoning, 2019. arXiv: 1906.02361[cs]. [Online]. Available: http://arxiv.org/abs/1906.02361 (visited on 05/29/2024). [180] H. Rashkin, E. M. Smith, M. Li, and Y.-L. Boureau, Towards empathetic open-domain conversation models: new benchmark and dataset, 2019. arXiv: 1811 . 00207[cs]. [Online]. Available: http://arxiv.org/abs/1811.00207 (visited on 05/01/2024). [181] S. Reddy, D. Chen, and C. D. Manning, CoQA: conversational question answering challenge, 2019. arXiv: 1808.07042[cs]. [Online]. Available: http://arxiv.org/ abs/1808.07042 (visited on 05/01/2024). J. Roth, S. Chaudhuri, O. Klejch, et al., AVA-ActiveSpeaker: An audio-visual dataset for active speaker detection, 2019. arXiv: 1901.01342[cs, eess]. [Online]. Available: http://arxiv.org/abs/1901.01342 (visited on 05/29/2024). [182] [183] K. Sakaguchi, R. L. Bras, C. Bhagavatula, and Y. Choi, WinoGrande: An adversarial winograd schema challenge at scale, 2019. arXiv: 1907.10641[cs]. [Online]. Available: http: //arxiv.org/abs/1907.10641 (visited on 05/29/2024). [185] [184] M. Sap, H. Rashkin, D. Chen, R. LeBras, and Y. Choi, SocialIQA: Commonsense reasoning about social interactions, 2019. arXiv: 1904.09728[cs]. [Online]. Available: http: //arxiv.org/abs/1904.09728 (visited on 05/29/2024). I. Shalyminov, S. Lee, A. Eshghi, and O. Lemon, Few-shot dialogue generation without annotated data: transfer learning approach, 2019. arXiv: 1908.05854[cs]. [Online]. Available: http://arxiv.org/abs/1908.05854 (visited on 05/01/2024). [186] E. Sharma, C. Li, and L. Wang, BIGPATENT: large-scale dataset for abstractive and coherent summarization, 2019. arXiv: 1906.03741[cs]. [Online]. Available: http: //arxiv.org/abs/1906.03741 (visited on 10/02/2024). [187] O. Tafjord, M. Gardner, K. Lin, and P. Clark, QuaRTz: An open-domain dataset of qualitative relationship questions, 2019. arXiv: 1909.03553[cs]. [Online]. Available: http:// arxiv.org/abs/1909.03553 (visited on 05/29/2024). [188] A. Talmor, J. Herzig, N. Lourie, and J. Berant, CommonsenseQA: question answering challenge targeting commonsense knowledge, 2019. arXiv: 1811.00937[cs]. [Online]. Available: http://arxiv.org/abs/1811.00937 (visited on 05/29/2024). [189] N. Tandon, B. D. Mishra, K. Sakaguchi, A. Bosselut, and P. Clark, WIQA: dataset for \"what if...\" reasoning over procedural text, 2019. arXiv: 1909.04739[cs]. [Online]. Available: http://arxiv.org/abs/1909.04739 (visited on 05/29/2024). [190] Y. Tang, D. Ding, Y. Rao, et al., COIN: large-scale dataset for comprehensive instructional video analysis, 2019. arXiv: 1903.02874[cs]. [Online]. Available: http://arxiv. org/abs/1903.02874 (visited on 05/01/2024). 51 The Data Provenance Initiative, 2024 [191] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman, GLUE: multitask benchmark and analysis platform for natural language understanding, 2019. arXiv: 1804.07461[cs]. [Online]. Available: http://arxiv.org/abs/1804.07461 (visited on 05/29/2024). [192] W. Xiong, J. Wu, H. Wang, et al., TWEETQA: social media focused question answering dataset, 2019. arXiv: 1907.06292[cs]. [Online]. Available: http://arxiv.org/ abs/1907.06292 (visited on 05/29/2024). [193] Y. Xiong, Q. Huang, L. Guo, H. Zhou, B. Zhou, and D. Lin, graph-based framework to bridge movies and synopses, 2019. arXiv: 1910 . 11009[cs]. [Online]. Available: http://arxiv.org/abs/1910.11009 (visited on 05/29/2024). [194] T. Yu, R. Zhang, H. Y. Er, et al., CoSQL: conversational text-to-SQL challenge towards cross-domain natural language interfaces to databases, 2019. arXiv: 1909.05378[cs]. [Online]. Available: http://arxiv.org/abs/1909.05378 (visited on 05/01/2024). [195] T. Yu, R. Zhang, K. Yang, et al., Spider: large-scale human-labeled dataset for complex and cross-domain semantic parsing and text-to-SQL task, 2019. arXiv: 1809.08887[cs]. [Online]. Available: http://arxiv.org/abs/1809.08887 (visited on 05/01/2024). [196] T. Yu, R. Zhang, M. Yasunaga, et al., SParC: Cross-domain semantic parsing in context, in Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, A. Korhonen, D. Traum, and L. MÃ rquez, Eds., Florence, Italy: Association for Computational Linguistics, 2019, pp. 45114523. DOI: 10 / gj4fwd. [Online]. Available: https : / / aclanthology.org/P19-1443 (visited on 05/01/2024). [197] R. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, and Y. Choi, HellaSwag: Can machine really finish your sentence? 2019. arXiv: 1905.07830[cs]. [Online]. Available: http: //arxiv.org/abs/1905.07830 (visited on 05/29/2024). [198] Y. Zhang, J. Baldridge, and L. He, PAWS: Paraphrase adversaries from word scrambling, 2019. arXiv: 1904.01130[cs]. [Online]. Available: http://arxiv.org/abs/ 1904.01130 (visited on 05/29/2024). [199] H. Zhao, A. Torralba, L. Torresani, and Z. Yan, HACS: Human action clips and segments dataset for recognition and temporal localization, 2019. arXiv: 1712.09374[cs]. [Online]. Available: http://arxiv.org/abs/1712.09374 (visited on 05/01/2024). [200] Y. Zhou, S. Liu, J. Siow, X. Du, and Y. Liu, Devign: Effective vulnerability identification by learning comprehensive program semantics via graph neural networks, 2019. arXiv: 1909. 03496[cs,stat]. [Online]. Available: http://arxiv.org/abs/1909.03496 (visited on 10/02/2024). [201] D. Zhukov, J.-B. Alayrac, R. G. Cinbis, D. Fouhey, I. Laptev, and J. Sivic, Cross-task weakly supervised learning from instructional videos, 2019. arXiv: 1903.08225[cs]. [Online]. Available: http://arxiv.org/abs/1903.08225 (visited on 05/01/2024). [202] A. Abdelghany, H. Abdelaal, A. Kamr, and P. Elkafrawy, Doc2vec: An approach to identify hadith similarities, Australian Journal of Basic and Applied Sciences, pp. 4653, 2020. DOI: 10.22587/ajbas.2020.14.12.5. [203] A. Andrusenko, A. Laptev, and I. Medennikov, Exploration of end-to-end ASR for OpenSTT russian open speech-to-text dataset, in Lecture Notes in Computer Science, vol. 12335, Springer, Cham, 2020, pp. 3544. DOI: 10.1007/978303060276- 5_4. arXiv: 2006.08274[cs, eess]. [Online]. Available: http://arxiv.org/abs/2006. 08274 (visited on 05/29/2024). [204] M. C. Ardanuy, F. Nanni, K. Beelen, et al., Living machines: study of atypical animacy, 2020. arXiv: 2005.11140[cs]. [Online]. Available: http://arxiv.org/abs/ 2005.11140 (visited on 10/02/2024). [205] R. Ardila, M. Branson, K. Davis, et al., Common voice: massively-multilingual speech corpus, 2020. arXiv: 1912.06670[cs]. [Online]. Available: http://arxiv.org/ abs/1912.06670 (visited on 05/01/2024). [206] G. Awad, A. A. Butt, K. Curtis, et al., TRECVID 2019: An evaluation campaign to benchmark video activity detection, video captioning and matching, and video search & retrieval, 2020. arXiv: 2009.09984[cs]. [Online]. Available: http://arxiv.org/abs/2009. 09984 (visited on 05/29/2024). 52 The Data Provenance Initiative, 2024 [208] [207] M. Bain, A. Nagrani, A. Brown, and A. Zisserman, Condensed movies: Story based retrieval with contextual embeddings, 2020. arXiv: 2005.04208[cs]. [Online]. Available: http: //arxiv.org/abs/2005.04208 (visited on 05/29/2024). J.-U. Bang, S. Yun, S.-H. Kim, et al., KsponSpeech: Korean spontaneous speech corpus for automatic speech recognition, Applied Sciences, vol. 10, no. 19, p. 6936, 2020, Number: 19 Publisher: Multidisciplinary Digital Publishing Institute, ISSN: 2076-3417. DOI: 10/ gtwwck. [Online]. Available: https : // www . mdpi .com / 2076 - 3417 / 10 / 19 / 6936 (visited on 05/29/2024). [209] M. Bartolo, A. Roberts, J. Welbl, S. Riedel, and P. Stenetorp, Beat the AI: Investigating adversarial human annotation for reading comprehension, Transactions of the Association for Computational Linguistics, vol. 8, pp. 662678, 2020, ISSN: 2307-387X. DOI: 10/gjzgwj. arXiv: 2002.00293[cs]. [Online]. Available: http://arxiv.org/abs/2002. 00293 (visited on 05/29/2024). [210] M. Biltawi, A. Awajan, and S. Tedmori, Arabic reading comprehension benchmarks created semiautomatically, in 2020 21st International Arab Conference on Information Technology (ACIT), Giza, Egypt: IEEE, 2020, pp. 16, ISBN: 978-1-72818-855-3. DOI: 10/g6k6b8. [Online]. Available: https://ieeexplore.ieee.org/document/9300111/ (visited on 10/02/2024). [211] M. Z. Boito, W. N. Havard, M. Garnerin, E. L. Ferrand, and L. Besacier, MaSS: large and clean multilingual corpus of sentence-aligned spoken utterances extracted from the bible, 2020. arXiv: 1907.12895[cs]. [Online]. Available: http://arxiv.org/abs/ 1907.12895 (visited on 05/29/2024). [212] M. Boratko, X. L. Li, R. Das, T. OGorman, D. Le, and A. McCallum, ProtoQA: question answering dataset for prototypical common-sense reasoning, 2020. arXiv: 2005.00771[cs]. [Online]. Available: http://arxiv.org/abs/2005.00771 (visited on 05/29/2024). [213] T. B. Brown, B. Mann, N. Ryder, et al., Language models are few-shot learners, 2020. arXiv: 2005.14165[cs]. [Online]. Available: http://arxiv.org/abs/2005.14165 (visited on 10/02/2024). I. Casanueva, T. TemË‡cinas, D. Gerz, M. Henderson, and I. Vulic, Efficient intent detection with dual sentence encoders, in Proceedings of the 2nd Workshop on Natural Language Processing for Conversational AI, T.-H. Wen, A. Celikyilmaz, Z. Yu, et al., Eds., Online: Association for Computational Linguistics, 2020, pp. 3845. DOI: 10 / gjhzzs. arXiv: 2003 . 04807[cs]. [Online]. Available: https : / / aclanthology . org / 2020 . nlp4convai-1.5 (visited on 05/01/2024). [214] [215] H. Chen, Y. Ji, and D. Evans, Finding friends and flipping frenemies: Automatic paraphrase dataset augmentation using graph theory, 2020. arXiv: 2011 . 01856[cs]. [Online]. Available: http://arxiv.org/abs/2011.01856 (visited on 10/02/2024). [216] W. Chen, H. Zha, Z. Chen, W. Xiong, H. Wang, and W. Y. Wang, HybridQA: dataset of multi-hop question answering over tabular and textual data, in Findings of the Association for Computational Linguistics: EMNLP 2020, T. Cohn, Y. He, and Y. Liu, Eds., Online: Association for Computational Linguistics, 2020, pp. 10261036. DOI: 10/gpmd4x. [Online]. Available: https://aclanthology.org/2020.findings-emnlp.91 (visited on 05/01/2024). [217] S. Coope, T. Farghly, D. Gerz, I. Vulic, and M. Henderson, Span-ConveRT: Few-shot span extraction for dialog with pretrained conversational representations, 2020. arXiv: 2005. 08866[cs]. [Online]. Available: http://arxiv.org/abs/2005.08866 (visited on 05/01/2024). [218] U. Demir, Y. S. Rawat, and M. Shah, TinyVIRAT: Low-resolution video action recognition, 2020. arXiv: 2007.07355[cs, eess]. [Online]. Available: http://arxiv.org/ abs/2007.07355 (visited on 05/02/2024). 53 The Data Provenance Initiative, 2024 [219] I. Demirsahin, O. Kjartansson, A. Gutkin, and C. Rivera, Open-source multi-speaker corpora of the english accents in the british isles, in Proceedings of the Twelfth Language Resources and Evaluation Conference, N. Calzolari, F. BÃ©chet, P. Blache, et al., Eds., Marseille, France: European Language Resources Association, 2020, pp. 65326541, ISBN: 979-10-95546-34-4. [Online]. Available: https://aclanthology.org/2020.lrec-1.804 (visited on 05/29/2024). [220] A. Diba, M. Fayyaz, V. Sharma, et al., Large scale holistic video understanding, 2020. arXiv: 1904.11451[cs]. [Online]. Available: http://arxiv.org/abs/1904.11451 (visited on 05/01/2024). [221] H. Duan, Y. Zhao, Y. Xiong, W. Liu, and D. Lin, Omni-sourced webly-supervised learning for video recognition, 2020. arXiv: 2003.13042[cs]. [Online]. Available: http:// arxiv.org/abs/2003.13042 (visited on 05/02/2024). [222] D. Emelin, R. L. Bras, J. D. Hwang, M. Forbes, and Y. Choi, Moral stories: Situated reasoning about norms, intents, actions, and their consequences, 2020. arXiv: 2012.15738[cs]. [Online]. Available: http://arxiv.org/abs/2012.15738 (visited on 05/29/2024). [223] D. Epstein, B. Chen, and C. Vondrick, Oops! predicting unintentional action in video, in 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), Seattle, WA, USA: IEEE, 2020, pp. 916926, ISBN: 978-1-72817-168-5. DOI: 10/ghbckh. [Online]. Available: https://ieeexplore.ieee.org/document/9156404/ (visited on 05/02/2024). [224] H. Haagsma, J. Bos, and M. Nissim, MAGPIE: large corpus of potentially idiomatic expressions, in Proceedings of the Twelfth Language Resources and Evaluation Conference, N. Calzolari, F. BÃ©chet, P. Blache, et al., Eds., Marseille, France: European Language Resources Association, 2020, pp. 279287, ISBN: 979-10-95546-34-4. [Online]. Available: https://aclanthology.org/2020.lrec-1.35 (visited on 10/02/2024). J. Hu, S. Ruder, A. Siddhant, G. Neubig, O. Firat, and M. Johnson, XTREME: massively multilingual multi-task benchmark for evaluating cross-lingual generalization, 2020. arXiv: 2003.11080[cs]. [Online]. Available: http://arxiv.org/abs/2003.11080 (visited on 10/02/2024). [225] [227] [226] Q. Huang, Y. Xiong, A. Rao, J. Wang, and D. Lin, MovieNet: holistic dataset for movie understanding, 2020. arXiv: 2007.10937[cs]. [Online]. Available: http://arxiv. org/abs/2007.10937 (visited on 05/29/2024). J. J. Irwin, K. G. Tang, J. Young, et al., ZINC20a free ultralarge-scale chemical database for ligand discovery, Journal of Chemical Information and Modeling, vol. 60, no. 12, pp. 60656073, 2020, Publisher: American Chemical Society, ISSN: 1549-9596. DOI: 10/ gmjg8b. [Online]. Available: https://doi.org/10.1021/acs.jcim.0c00675 (visited on 10/02/2024). [228] B. Jia, Y. Chen, S. Huang, Y. Zhu, and S.-c. Zhu, LEMMA: multi-view dataset for learning multi-agent multi-task activities, 2020. arXiv: 2007 . 15781[cs]. [Online]. Available: http://arxiv.org/abs/2007.15781 (visited on 05/02/2024). [229] Y. Jiang, S. Bordia, Z. Zhong, C. Dognin, M. Singh, and M. Bansal, HoVer: dataset for many-hop fact extraction and claim verification, 2020. arXiv: 2011.03088[cs]. [Online]. Available: http://arxiv.org/abs/2011.03088 (visited on 10/02/2024). [230] D. Jin, E. Pan, N. Oufattole, W.-H. Weng, H. Fang, and P. Szolovits, What disease does this patient have? large-scale open domain question answering dataset from medical exams, 2020. arXiv: 2009.13081[cs]. [Online]. Available: http://arxiv.org/abs/ 2009.13081 (visited on 05/02/2024). [231] A. Kanade, P. Maniatis, G. Balakrishnan, and K. Shi, Learning and evaluating contextual embedding of source code, 2020. arXiv: 2001.00059[cs]. [Online]. Available: http: //arxiv.org/abs/2001.00059 (visited on 10/02/2024). [232] D. Kaushik, E. Hovy, and Z. C. Lipton, Learning the difference that makes difference with counterfactually-augmented data, 2020. arXiv: 1909.12434[cs,stat]. [Online]. Available: http://arxiv.org/abs/1909.12434 (visited on 10/02/2024). 54 The Data Provenance Initiative, 2024 [233] T. Khot, P. Clark, M. Guerquin, P. Jansen, and A. Sabharwal, QASC: dataset for question answering via sentence composition, 2020. arXiv: 1910.11473[cs]. [Online]. Available: http://arxiv.org/abs/1910.11473 (visited on 05/29/2024). [234] A. Kirkedal, M. Stepanovic, and B. Plank, FT speech: Danish parliament speech corpus, 2020. DOI: 10 . 21437 / Interspeech . 2020 - 3164. arXiv: 2005 . 12368[cs , eess]. [Online]. Available: http://arxiv.org/abs/2005.12368 (visited on 05/29/2024). [235] O. Kjartansson, A. Gutkin, A. Butryna, I. Demirsahin, and C. Rivera, Open-source high quality speech datasets for basque, catalan and galician, in Proceedings of the 1st Joint Workshop on Spoken Language Technologies for Under-resourced languages (SLTU) and Collaboration and Computing for Under-Resourced Languages (CCURL), D. Beermann, L. Besacier, S. Sakti, and C. Soria, Eds., Marseille, France: European Language Resources association, 2020, pp. 2127, ISBN: 979-10-95546-35-1. [Online]. Available: https:// aclanthology.org/2020.sltu-1.3 (visited on 05/29/2024). J. Kratochvil, P. PolÃ¡k, and O. Bojar, Large corpus of czech parliament plenary hearings, in Proceedings of the Twelfth Language Resources and Evaluation Conference, N. Calzolari, F. BÃ©chet, P. Blache, et al., Eds., Marseille, France: European Language Resources Association, 2020, pp. 63636367, ISBN: 979-10-95546-34-4. [Online]. Available: https: //aclanthology.org/2020.lrec-1.781 (visited on 05/29/2024). [236] [237] F. Kury, A. Butler, C. Yuan, et al., Chia, large annotated corpus of clinical trial eligibility criteria, Scientific Data, vol. 7, no. 1, p. 281, 2020, Publisher: Nature Publishing Group, ISSN: 2052-4463. DOI: 10/gr4ftn. [Online]. Available: https://www.nature.com/ articles/s41597-020-00620-0 (visited on 10/02/2024). [238] B. Y. Lin, S. Lee, R. Khanna, and X. Ren, Birds have four legs?! NumerSense: Probing numerical commonsense knowledge of pre-trained language models, 2020. arXiv: 2005. 00683[cs]. [Online]. Available: http://arxiv.org/abs/2005.00683 (visited on 05/29/2024). [239] B. Y. Lin, W. Zhou, M. Shen, et al., CommonGen: constrained text generation challenge for generative commonsense reasoning, 2020. arXiv: 1911.03705[cs]. [Online]. Available: http://arxiv.org/abs/1911.03705 (visited on 05/29/2024). J. Liu, W. Chen, Y. Cheng, et al., VIOLIN: large-scale dataset for video-and-language inference, 2020. arXiv: 2003.11618[cs]. [Online]. Available: http://arxiv.org/ abs/2003.11618 (visited on 05/01/2024). [240] [241] A. Louis, D. Roth, and F. Radlinski, \"id rather just go to bed\": Understanding indirect answers, 2020. arXiv: 2010.03450[cs]. [Online]. Available: http://arxiv.org/ abs/2010.03450 (visited on 05/29/2024). [242] S. Malla, B. Dariush, and C. Choi, TITAN: Future forecast using action priors, in 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), Seattle, WA, USA: IEEE, 2020, pp. 11 18311 193, ISBN: 978-1-72817-168-5. DOI: 10/gg99rg. [Online]. Available: https://ieeexplore.ieee.org/document/9156550/ (visited on 05/02/2024). [243] S. Martin, S. Poddar, and K. Upasani, MuDoCo: Corpus for multidomain coreference resolution and referring expression generation, in Proceedings of the Twelfth Language Resources and Evaluation Conference, N. Calzolari, F. BÃ©chet, P. Blache, et al., Eds., Marseille, France: European Language Resources Association, 2020, pp. 104111, ISBN: 979-10-95546-34-4. [Online]. Available: https://aclanthology.org/2020.lrec-1.13 (visited on 05/01/2024). [244] A. Miech, J.-B. Alayrac, I. Laptev, J. Sivic, and A. Zisserman, RareAct: video dataset of unusual interactions, 2020. arXiv: 2008 . 01018[cs]. [Online]. Available: http : //arxiv.org/abs/2008.01018 (visited on 05/02/2024). [245] D. E. Mollberg, O. H. Jonsson, S. Thorsteinsdottir, S. Steingremsson, E. H. Magnusdottir, and J. Gudnason, SamrÃ³mur: Crowd-sourcing data collection for icelandic speech recognition, in Proceedings of the Twelfth Language Resources and Evaluation Conference, N. Calzolari, F. Bechet, P. Blache, et al., Eds., Marseille, France: European Language Resources Association, 2020, pp. 34633467, ISBN: 979-10-95546-34-4. [Online]. Available: https://aclanthology.org/2020.lrec-1.425 (visited on 05/01/2024). 55 The Data Provenance Initiative, 2024 [246] J. E. M. Mosig, S. Mehri, and T. Kober, STAR: schema-guided dialog dataset for transfer learning, 2020. arXiv: 2010.11853[cs]. [Online]. Available: http://arxiv.org/ abs/2010.11853 (visited on 05/01/2024). [247] N. Mostafazadeh, A. Kalyanpur, L. Moon, et al., GLUCOSE: GeneraLized and COntextualized story explanations, 2020. arXiv: 2009.07758[cs]. [Online]. Available: http: //arxiv.org/abs/2009.07758 (visited on 05/29/2024). [248] W. Myers, T. Etchart, and N. Fulda, Conversational scaffolding: An analogy-based approach to response prioritization in open-domain dialogs: in Proceedings of the 12th International Conference on Agents and Artificial Intelligence, Valletta, Malta: SCITEPRESS - Science and Technology Publications, 2020, pp. 6978, ISBN: 978-989-758-395-7. DOI: 10/gtsq86. [Online]. Available: http://www.scitepress.org/DigitalLibrary/Link. aspx?doi=10.5220/0008939900690078 (visited on 05/02/2024). [249] Y. Nie, A. Williams, E. Dinan, M. Bansal, J. Weston, and D. Kiela, Adversarial NLI: new benchmark for natural language understanding, 2020. arXiv: 1910.14599[cs]. [Online]. Available: http://arxiv.org/abs/1910.14599 (visited on 05/29/2024). [250] M. Orabi, H. E. Rifai, and A. Elnagar, Classical arabic poetry: Classification based on era, in 2020 IEEE/ACS 17th International Conference on Computer Systems and Applications (AICCSA), Antalya, Turkey: IEEE, 2020, pp. 16, ISBN: 978-1-72818-577-4. DOI: 10 / g6k6b2. [Online]. Available: https : / / ieeexplore . ieee . org / document / 9316520/ (visited on 10/02/2024). [251] A. Parikh, X. Wang, S. Gehrmann, et al., ToTTo: controlled table-to-text generation dataset, in Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), B. Webber, T. Cohn, Y. He, and Y. Liu, Eds., Online: Association for Computational Linguistics, 2020, pp. 11731186. DOI: 10/gm3nmg. [Online]. Available: https://aclanthology.org/2020.emnlp-main.89 (visited on 05/01/2024). [252] V. Pratap, Q. Xu, A. Sriram, G. Synnaeve, and R. Collobert, MLS: large-scale multilingual dataset for speech research, in Interspeech 2020, 2020, pp. 27572761. DOI: 10/grk6mp. arXiv: 2012.03411[cs, eess]. [Online]. Available: http://arxiv.org/abs/ 2012.03411 (visited on 05/01/2024). [253] R. Rameshkumar and P. Bailey, Storytelling with dialogue: critical role dungeons and dragons dataset, in Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, D. Jurafsky, J. Chai, N. Schluter, and J. Tetreault, Eds., Online: Association for Computational Linguistics, 2020, pp. 51215134. DOI: 10/gtsqxp. [Online]. Available: https://aclanthology.org/2020.acl-main.459 (visited on 05/01/2024). [254] A. Rao, L. Xu, Y. Xiong, et al., local-to-global approach to multi-modal movie scene segmentation, 2020. arXiv: 2004.02678[cs]. [Online]. Available: http://arxiv. org/abs/2004.02678 (visited on 05/29/2024). [255] A. Rastogi, X. Zang, S. Sunkara, R. Gupta, and P. Khaitan, Towards scalable multidomain conversational agents: The schema-guided dialogue dataset, 2020. arXiv: 1909. 05855[cs]. [Online]. Available: http://arxiv.org/abs/1909.05855 (visited on 05/01/2024). [256] M. Savery, A. B. Abacha, S. Gayen, and D. Demner-Fushman, Question-driven summarization of answers to consumer health questions, 2020. arXiv: 2005.09067[cs]. [Online]. Available: http://arxiv.org/abs/2005.09067 (visited on 05/02/2024). [257] C. Schulz, J. Levy-Kramer, C. Van Assel, M. Kepes, and N. Hammerla, Biomedical concept relatedness large EHR-based benchmark, 2020. arXiv: 2010.16218[cs]. [Online]. Available: http://arxiv.org/abs/2010.16218 (visited on 10/02/2024). [258] D. Shan, J. Geng, M. Shu, and D. F. Fouhey, Understanding human hands in contact at internet scale, 2020. arXiv: 2006.06669[cs]. [Online]. Available: http://arxiv. org/abs/2006.06669 (visited on 05/02/2024). [259] D. Shao, Y. Zhao, B. Dai, and D. Lin, FineGym: hierarchical video dataset for finegrained action understanding, 2020. arXiv: 2004.06704[cs]. [Online]. Available: http: //arxiv.org/abs/2004.06704 (visited on 05/02/2024). 56 The Data Provenance Initiative, [260] V. Sharma, M. Tapaswi, and R. Stiefelhagen, Deep multimodal feature encoding for video ordering, 2020. arXiv: 2004.02205[cs]. [Online]. Available: http://arxiv.org/ abs/2004.02205 (visited on 05/29/2024). [261] L. Smaira, J. Carreira, E. Noland, E. Clancy, A. Wu, and A. Zisserman, short note on the kinetics-700-2020 human action dataset, 2020. arXiv: 2010.10864[cs]. [Online]. Available: http://arxiv.org/abs/2010.10864 (visited on 05/02/2024). [262] R. Tang, R. Nogueira, E. Zhang, et al., Rapidly bootstrapping question answering dataset for COVID-19, 2020. arXiv: 2004.11339[cs]. [Online]. Available: http://arxiv. org/abs/2004.11339 (visited on 10/02/2024). [263] C. Wang, A. Wu, and J. Pino, CoVoST 2 and massively multilingual speech-to-text translation, 2020. arXiv: 2007.10310[cs, eess]. [Online]. Available: http://arxiv.org/ abs/2007.10310 (visited on 05/01/2024). [264] L. L. Wang, K. Lo, Y. Chandrasekhar, et al., CORD-19: The COVID-19 open research dataset, 2020. arXiv: 2004.10706[cs]. [Online]. Available: http://arxiv.org/ abs/2004.10706 (visited on 05/02/2024). [265] W. Wang, G. Li, B. Ma, X. Xia, and Z. Jin, Detecting code clones with graph neural networkand flow-augmented abstract syntax tree, 2020. arXiv: 2002.08653[cs]. [Online]. Available: http://arxiv.org/abs/2002.08653 (visited on 10/02/2024). [266] X. Wang, J. Wu, J. Chen, L. Li, Y.-F. Wang, and W. Y. Wang, VATEX: large-scale, high-quality multilingual dataset for video-and-language research, 2020. arXiv: 1904 . 03493[cs]. [Online]. Available: http://arxiv.org/abs/1904.03493 (visited on 05/01/2024). [267] O. Weller, N. Lourie, M. Gardner, and M. E. Peters, Learning from task descriptions, 2020. arXiv: 2011.08115[cs]. [Online]. Available: http://arxiv.org/abs/2011. 08115 (visited on 05/29/2024). [268] A. Williams, T. Thrush, and D. Kiela, ANLIzing the adversarial natural language inference dataset, 2020. arXiv: 2010.12729[cs]. [Online]. Available: http://arxiv.org/ abs/2010.12729 (visited on 10/02/2024). [269] W. Yu, Z. Jiang, Y. Dong, and J. Feng, ReClor: reading comprehension dataset requiring logical reasoning, 2020. arXiv: 2002.04326[cs]. [Online]. Available: http://arxiv. org/abs/2002.04326 (visited on 05/01/2024). [270] X. Zang, A. Rastogi, S. Sunkara, R. Gupta, J. Zhang, and J. Chen, MultiWOZ 2.2 : dialogue dataset with additional annotation corrections and state tracking baselines, 2020. arXiv: 2007.12720[cs]. [Online]. Available: http://arxiv.org/abs/2007.12720 (visited on 05/01/2024). [271] O. Agarwal, H. Ge, S. Shakeri, and R. Al-Rfou, Knowledge graph based synthetic corpus generation for knowledge-enhanced language model pre-training, 2021. arXiv: 2010. 12688[cs]. [Online]. Available: http://arxiv.org/abs/2010.12688 (visited on 10/02/2024). J. L. Alcazar, L. Mai, F. Perazzi, et al., APES: Audiovisual person search in untrimmed video, 2021. arXiv: 2106.01667[cs]. [Online]. Available: http://arxiv.org/abs/ 2106.01667 (visited on 05/29/2024). [272] [273] S. Ando and H. Fujihara, Construction of large-scale japanese ASR corpus on TV recordings, 2021. arXiv: 2103.14736[cs, eess]. [Online]. Available: http://arxiv.org/ abs/2103.14736 (visited on 05/29/2024). J. Austin, A. Odena, M. Nye, et al., Program synthesis with large language models, 2021. arXiv: 2108.07732[cs]. [Online]. Available: http://arxiv.org/abs/2108. 07732 (visited on 10/02/2024). [274] [275] F. Boudin and Y. Gallina, Redefining absent keyphrases and their effect on retrieval effectiveness, in Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, K. Toutanova, A. Rumshisky, L. Zettlemoyer, et al., Eds., Online: Association for Computational Linguistics, 2021, pp. 41854193. DOI: 10 / g6k64d. [Online]. Available: https : / / aclanthology.org/2021.naacl-main.330 (visited on 10/02/2024). 57 The Data Provenance Initiative, 2024 [276] S. Cao and L. Wang, Controllable open-ended question generation with new question type ontology, 2021. arXiv: 2107.00152[cs]. [Online]. Available: http://arxiv.org/ abs/2107.00152 (visited on 05/29/2024). I. Chalkidis, M. Fergadiotis, N. Manginas, E. Katakalou, and P. Malakasiotis, Regulatory compliance through doc2doc information retrieval: case study in EU/UK legislation where text similarity has limitations, 2021. arXiv: 2101.10726[cs]. [Online]. Available: http://arxiv.org/abs/2101.10726 (visited on 10/02/2024). [277] [278] E. Chapuis, P. Colombo, M. Manica, M. Labeau, and C. Clavel, Hierarchical pre-training for sequence labelling in spoken dialog, 2021. arXiv: 2009.11152[cs]. [Online]. Available: http://arxiv.org/abs/2009.11152 (visited on 10/02/2024). [279] K. Chawla, J. Ramirez, R. Clever, G. Lucas, J. May, and J. Gratch, CaSiNo: corpus of campsite negotiation dialogues for automatic negotiation systems, in Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, K. Toutanova, A. Rumshisky, L. Zettlemoyer, et al., Eds., Online: Association for Computational Linguistics, 2021, pp. 31673185. DOI: 10/gtsqxv. [Online]. Available: https : / / aclanthology . org / 2021 . naacl - main . 254 (visited on 05/01/2024). [280] D. Chen, H. Chen, Y. Yang, A. Lin, and Z. Yu, Action-based conversations dataset: corpus for building more in-depth task-oriented dialogue systems, in Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, K. Toutanova, A. Rumshisky, L. Zettlemoyer, et al., Eds., Online: Association for Computational Linguistics, 2021, pp. 30023017. DOI: 10/gtsqxt. [Online]. Available: https : / / aclanthology . org / 2021 . naacl - main . 239 (visited on 05/01/2024). [281] G. Chen, S. Chai, G. Wang, et al., GigaSpeech: An evolving, multi-domain ASR corpus with 10,000 hours of transcribed audio, 2021. arXiv: 2106.06909[cs,eess]. [Online]. Available: http://arxiv.org/abs/2106.06909 (visited on 05/01/2024). [282] Y. Chen, Y. Liu, L. Chen, and Y. Zhang, DialogSum: real-life scenario dialogue summarization dataset, in Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, C. Zong, F. Xia, W. Li, and R. Navigli, Eds., Online: Association for Computational Linguistics, 2021, pp. 50625074. DOI: 10/gtsqxs. [Online]. Available: https: //aclanthology.org/2021.findings-acl.449 (visited on 05/01/2024). J. Chung, C.-h. Wuu, H.-r. Yang, Y.-W. Tai, and C.-K. Tang, HAA500: Human-centric atomic action dataset with curated videos, 2021. arXiv: 2009 . 05224[cs , eess]. [Online]. Available: http://arxiv.org/abs/2009.05224 (visited on 05/02/2024). [284] K. Cobbe, V. Kosaraju, M. Bavarian, et al., Training verifiers to solve math word problems, 2021. arXiv: 2110.14168[cs]. [Online]. Available: http://arxiv.org/abs/ 2110.14168 (visited on 05/29/2024). [283] [285] A. Devaraj, I. J. Marshall, B. C. Wallace, and J. J. Li, Paragraph-level simplification of medical texts, 2021. arXiv: 2104.05767[cs]. [Online]. Available: http://arxiv. org/abs/2104.05767 (visited on 10/02/2024). J. DeYoung, I. Beltagy, M. van Zuylen, B. Kuehl, and L. L. Wang, MS2: Multi-document summarization of medical studies, 2021. arXiv: 2104.06486[cs]. [Online]. Available: http://arxiv.org/abs/2104.06486 (visited on 10/02/2024). [286] [287] T. Diggelmann, J. Boyd-Graber, J. Bulian, M. Ciaramita, and M. Leippold, CLIMATE-FEVER: dataset for verification of real-world climate claims, 2021. arXiv: 2012.00614[cs]. [Online]. Available: http://arxiv.org/abs/2012.00614 (visited on 10/02/2024). [288] M. Doumbouya, L. Einstein, and C. Piech, Using radio archives for low-resource speech recognition: Towards an intelligent virtual assistant for illiterate users, 2021. arXiv: 2104. 13083[cs]. [Online]. Available: http://arxiv.org/abs/2104.13083 (visited on 05/29/2024). 58 The Data Provenance Initiative, 2024 [289] A. Fabbri, F. Rahman, I. Rizvi, et al., ConvoSumm: Conversation summarization benchmark and improved abstractive summarization with argument mining, in Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), C. Zong, F. Xia, W. Li, and R. Navigli, Eds., Online: Association for Computational Linguistics, 2021, pp. 68666880. DOI: 10 / gmf9qs. [Online]. Available: https : / / aclanthology . org/2021.acl-long.535 (visited on 05/01/2024). [290] G. Feigenblat, C. Gunasekara, B. Sznajder, S. Joshi, D. Konopnicki, and R. Aharonov, TWEETSUMM dialog summarization dataset for customer service, 2021. arXiv: 2111. 11894[cs]. [Online]. Available: http://arxiv.org/abs/2111.11894 (visited on 05/01/2024). [291] S. Feng, S. S. Patel, H. Wan, and S. Joshi, MultiDoc2dial: Modeling dialogues grounded in multiple documents, in Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, 2021, pp. 61626176. DOI: 10/g6k938. arXiv: 2109.12595[cs]. [Online]. Available: http://arxiv.org/abs/2109.12595 (visited on 10/02/2024). [292] Y. Fu, L. Cheng, S. Lv, et al., AISHELL-4: An open source dataset for speech enhancement, separation, recognition and speaker diarization in conference scenario, 2021. arXiv: 2104. 03603[cs,eess]. [Online]. Available: http://arxiv.org/abs/2104.03603 (visited on 05/29/2024). [293] D. Galvez, G. Diamos, J. Ciro, et al., The peoples speech: large-scale diverse english speech recognition dataset for commercial usage, 2021. arXiv: 2111.09344[cs,stat]. [Online]. Available: http://arxiv.org/abs/2111.09344 (visited on 05/01/2024). [294] D. Gerz, P.-H. Su, R. Kusztos, et al., Multilingual and cross-lingual intent detection from spoken data, 2021. arXiv: 2104.08524[cs]. [Online]. Available: http://arxiv. org/abs/2104.08524 (visited on 05/29/2024). [295] Y. Gu, S. Kase, M. Vanni, et al., Beyond i.i.d.: Three levels of generalization for question answering on knowledge bases, in Proceedings of the Web Conference 2021, 2021, pp. 3477 3488. DOI: 10 / gnnfbt. arXiv: 2011 . 07743[cs]. [Online]. Available: http : / / arxiv.org/abs/2011.07743 (visited on 05/01/2024). [296] T. Guo, C. Wen, D. Jiang, et al., DiDiSpeech: large scale mandarin speech corpus, 2021. arXiv: 2010.09275[eess]. [Online]. Available: http://arxiv.org/abs/2010. 09275 (visited on 05/29/2024). [297] A. Gupta, J. Xu, S. Upadhyay, D. Yang, and M. Faruqui, Disfl-QA: benchmark dataset for understanding disfluencies in question answering, 2021. arXiv: 2106.04016[cs]. [Online]. Available: http://arxiv.org/abs/2106.04016 (visited on 05/29/2024). [298] M. Hazoom, V. Malik, and B. Bogin, Text-to-SQL in the wild: naturally-occurring dataset based on stack exchange data, 2021. arXiv: 2106.05006[cs]. [Online]. Available: http: //arxiv.org/abs/2106.05006 (visited on 10/02/2024). [299] D. Hendrycks, C. Burns, A. Chen, and S. Ball, CUAD: An expert-annotated NLP dataset for legal contract review, 2021. arXiv: 2103.06268[cs]. [Online]. Available: http: //arxiv.org/abs/2103.06268 (visited on 05/29/2024). [300] L. Huang, S. Cao, N. Parulian, H. Ji, and L. Wang, Efficient attentions for long document summarization, 2021. arXiv: 2104.02112[cs]. [Online]. Available: http://arxiv. org/abs/2104.02112 (visited on 05/29/2024). [301] C. Jiang, M. Maddela, W. Lan, Y. Zhong, and W. Xu, Neural CRF model for sentence alignment in text simplification, 2021. arXiv: 2005 . 02324[cs]. [Online]. Available: http://arxiv.org/abs/2005.02324 (visited on 05/29/2024). [302] N. Karpov, A. Denisenko, and F. Minkin, Golos: Russian dataset for speech research, 2021. arXiv: 2106.10161[eess]. [Online]. Available: http://arxiv.org/abs/2106. 10161 (visited on 05/01/2024). [303] Y. Khassanov, S. Mussakhojayeva, A. Mirzakhmetov, A. Adiyev, M. Nurpeiissov, and H. A. Varol, crowdsourced open-source kazakh speech corpus and initial speech recognition baseline, 2021. arXiv: 2009.10334[cs,eess]. [Online]. Available: http://arxiv. org/abs/2009.10334 (visited on 05/29/2024). 59 The Data Provenance Initiative, [304] R. Kolobov, O. Okhapkina, O. Omelchishina, et al., MediaSpeech: Multilanguage ASR benchmark and dataset, 2021. arXiv: 2103 . 16193[cs , eess]. [Online]. Available: http://arxiv.org/abs/2103.16193 (visited on 05/29/2024). [305] H. Li, A. Arora, S. Chen, A. Gupta, S. Gupta, and Y. Mehdad, MTOP: comprehensive multilingual task-oriented semantic parsing benchmark, in Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, P. Merlo, J. Tiedemann, and R. Tsarfaty, Eds., Online: Association for Computational Linguistics, 2021, pp. 29502962. DOI: 10/gtsqxq. [Online]. Available: https: //aclanthology.org/2021.eacl-main.257 (visited on 05/01/2024). [306] T. Li, J. Liu, W. Zhang, Y. Ni, W. Wang, and Z. Li, UAV-human: large benchmark for human behavior understanding with unmanned aerial vehicles, 2021. arXiv: 2104.00946[cs]. [Online]. Available: http://arxiv.org/abs/2104.00946 (visited on 05/02/2024). [307] B. Y. Lin, Z. Wu, Y. Yang, D.-H. Lee, and X. Ren, RiddleSense: Reasoning about riddle questions featuring linguistic creativity and commonsense knowledge, 2021. arXiv: 2101. 00376[cs]. [Online]. Available: http://arxiv.org/abs/2101.00376 (visited on 05/01/2024). [308] Z. Lin, A. Madotto, G. I. Winata, et al., BiToD: bilingual multi-domain dataset for taskoriented dialogue modeling, 2021. arXiv: 2106.02787[cs]. [Online]. Available: http: //arxiv.org/abs/2106.02787 (visited on 05/02/2024). [309] Z. Liu, H. Wang, Z.-Y. Niu, H. Wu, and W. Che, DuRecDial 2.0: bilingual parallel corpus for conversational recommendation, in Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, M.-F. Moens, X. Huang, L. Specia, and S. W.-t. Yih, Eds., Online and Punta Cana, Dominican Republic: Association for Computational Linguistics, 2021, pp. 43354347. DOI: 10 / gtsqxr. [Online]. Available: https : / / aclanthology.org/2021.emnlp-main.356 (visited on 05/01/2024). [310] M. Monfort, S. Jin, A. Liu, et al., Spoken Moments: Learning Joint Audio-Visual Representations from Video Descriptions, arXiv:2105.04489 [cs, eess], 2021. DOI: 10.48550/arXiv. 2105.04489. [Online]. Available: http://arxiv.org/abs/2105.04489 (visited on 05/02/2024). [311] M. Monfort, B. Pan, K. Ramakrishnan, et al., Multi-moments in time: Learning and interpreting models for multi-action video understanding, 2021. arXiv: 1911.00232[cs,eess]. [Online]. Available: http://arxiv.org/abs/1911.00232 (visited on 05/02/2024). [312] H. Mubarak, A. Hussein, S. A. Chowdhury, and A. Ali, QASR: QCRI aljazeera speech resource large scale annotated arabic speech corpus, 2021. arXiv: 2106.13000[cs, eess]. [Online]. Available: http://arxiv.org/abs/2106.13000 (visited on 05/29/2024). [313] L. Nan, D. Radev, R. Zhang, et al., DART: Open-domain structured data record to text generation, in Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, K. Toutanova, A. Rumshisky, L. Zettlemoyer, et al., Eds., Online: Association for Computational Linguistics, 2021, pp. 432447. DOI: 10/gnh49f. arXiv: 2007.02871[cs]. [Online]. Available: https://aclanthology.org/2021.naacl-main.37 (visited on 05/01/2024). [314] T.-P. Nguyen, S. Razniewski, and G. Weikum, Advanced semantics for commonsense knowledge extraction, in Proceedings of the Web Conference 2021, 2021, pp. 26362647. DOI: 10/gnnffn. arXiv: 2011.00905[cs]. [Online]. Available: http://arxiv. org/abs/2011.00905 (visited on 10/02/2024). [315] P. K. ONeill, V. Lavrukhin, S. Majumdar, et al., SPGISpeech: 5,000 hours of transcribed financial audio for fully formatted end-to-end speech recognition, 2021. arXiv: 2104 . 02014[cs,eess]. [Online]. Available: http://arxiv.org/abs/2104.02014 (visited on 05/01/2024). [316] A.-M. Oncescu, J. F. Henriques, Y. Liu, A. Zisserman, and S. Albanie, QuerYD: video dataset with high-quality text and audio narrations, 2021. arXiv: 2011 . 11071[cs]. [Online]. Available: http://arxiv.org/abs/2011.11071 (visited on 05/01/2024). The Data Provenance Initiative, 2024 [317] C. Paik, S. Aroca-Ouellette, A. Roncone, and K. Kann, The world of an octopus: How reporting bias influences language models perception of color, 2021. arXiv: 2110 . 08182[cs]. [Online]. Available: http://arxiv.org/abs/2110.08182 (visited on 10/02/2024). [318] N. Pavlichenko, I. Stelmakh, and D. Ustalov, CrowdSpeech and VoxDIY: Benchmark datasets for crowdsourced audio transcription, 2021. arXiv: 2107.01091[cs,eess]. [Online]. Available: http://arxiv.org/abs/2107.01091 (visited on 10/02/2024). [319] N. Rai, H. Chen, J. Ji, et al., Home action genome: Cooperative compositional action understanding, 2021. arXiv: 2105.05226[cs]. [Online]. Available: http://arxiv. org/abs/2105.05226 (visited on 05/02/2024). [321] [320] E. Salesky, M. Wiesner, J. Bremerman, et al., The multilingual TEDx corpus for speech recognition and translation, 2021. arXiv: 2102.01757[cs]. [Online]. Available: http: //arxiv.org/abs/2102.01757 (visited on 05/29/2024). J. Shi, J. D. Amith, X. Chang, S. Dalmia, B. Yan, and S. Watanabe, Highland puebla nahuatl speech translation corpus for endangered language documentation, in Proceedings of the First Workshop on Natural Language Processing for Indigenous Languages of the Americas, M. Mager, A. Oncevay, A. Rios, et al., Eds., Online: Association for Computational Linguistics, 2021, pp. 5363. DOI: 10/gtwwcd. [Online]. Available: https://aclanthology. org/2021.americasnlp-1.7 (visited on 05/29/2024). [322] M. Shridhar, X. Yuan, M.-A. CÃ´tÃ©, Y. Bisk, A. Trischler, and M. Hausknecht, ALFWorld: Aligning text and embodied environments for interactive learning, 2021. arXiv: 2010 . 03768[cs]. [Online]. Available: http://arxiv.org/abs/2010.03768 (visited on 05/01/2024). [324] [323] A. Soleimani, C. Monz, and M. Worring, NLQuAD: non-factoid long question answering data set, in Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, Online: Association for Computational Linguistics, 2021, pp. 12451255. DOI: 10/g6k6th. [Online]. Available: https: //aclanthology.org/2021.eacl-main.106 (visited on 10/02/2024). J. J. Sun, T. Liu, A. S. Cowen, F. Schroff, H. Adam, and G. Prasad, EEV: large-scale dataset for studying evoked expressions from video, 2021. arXiv: 2001.05488[cs]. [Online]. Available: http://arxiv.org/abs/2001.05488 (visited on 05/01/2024). [325] S. Takamichi, L. KÃ¼rzinger, T. Saeki, S. Shiota, and S. Watanabe, JTubeSpeech: Corpus of japanese speech collected from YouTube for speech recognition and speaker verification, 2021. arXiv: 2112.09323[cs, eess]. [Online]. Available: http://arxiv.org/ abs/2112.09323 (visited on 05/29/2024). [326] A. Talmor, O. Yoran, A. Catav, et al., MultiModalQA: Complex question answering over text, tables and images, 2021. arXiv: 2104 . 06039[cs]. [Online]. Available: http : //arxiv.org/abs/2104.06039 (visited on 05/01/2024). [327] Z. Tang, D. Wang, Y. Xu, et al., KeSpeech: An open source speech dataset of mandarin and its eight subdialects, in Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks, vol. 1, 2021. [Online]. Available: https : //datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/ 0336dcbab05b9d5ad24f4333c7658a0e-Abstract-round2.html (visited on 05/29/2024). [328] A. Thawani, J. Pujara, and F. Ilievski, Numeracy enhances the literacy of language models, in Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, M.-F. Moens, X. Huang, L. Specia, and S. W.-t. Yih, Eds., Online and Punta Cana, Dominican Republic: Association for Computational Linguistics, 2021, pp. 69606967. DOI: 10/g6k6w5. [Online]. Available: https://aclanthology.org/2021.emnlpmain.557 (visited on 10/02/2024). [329] C. Wang, M. RiviÃ¨re, A. Lee, et al., VoxPopuli: large-scale multilingual speech corpus for representation learning, semi-supervised learning and interpretation, 2021. arXiv: 2101. 00390[cs,eess]. [Online]. Available: http://arxiv.org/abs/2101.00390 (visited on 05/01/2024). 61 The Data Provenance Initiative, 2024 [330] P. Weinzaepfel and G. Rogez, Mimetics: Towards understanding human actions out of context, 2021. arXiv: 1912.07249[cs]. [Online]. Available: http://arxiv.org/ abs/1912.07249 (visited on 05/01/2024). [331] X. Zhang, Z. Wu, Z. Weng, et al., VideoLT: Large-scale long-tailed video recognition, 2021. arXiv: 2105.02668[cs]. [Online]. Available: http://arxiv.org/abs/2105. 02668 (visited on 05/02/2024). [332] T. Zhi, Y. Shi, W. Du, G. Li, and D. Wang, M2asr-MONGO: free mongolian speech database and accompanied baselines, in 2021 24th Conference of the Oriental COCOSDA International Committee for the Co-ordination and Standardisation of Speech Databases and Assessment Techniques (O-COCOSDA), Singapore, Singapore: IEEE, 2021, pp. 140145, ISBN: 978-1-66540-870-7. DOI: 10/gtsqzg. [Online]. Available: https://ieeexplore. ieee.org/document/9660401/ (visited on 05/01/2024). [333] M. Zhong, D. Yin, T. Yu, et al., QMSum: new benchmark for query-based multi-domain meeting summarization, 2021. arXiv: 2104 . 05938[cs]. [Online]. Available: http : //arxiv.org/abs/2104.05938 (visited on 05/01/2024). [334] C. Zhu, Y. Liu, J. Mei, and M. Zeng, MediaSum: large-scale media interview dataset for dialogue summarization, 2021. arXiv: 2103.06410[cs]. [Online]. Available: http: //arxiv.org/abs/2103.06410 (visited on 05/01/2024). [335] V. Adlakha, S. Dhuliawala, K. Suleman, H. de Vries, and S. Reddy, TopiOCQA: Open-domain conversational question answering with topic switching, 2022. arXiv: 2110.00768[cs]. [Online]. Available: http://arxiv.org/abs/2110.00768 (visited on 10/02/2024). [336] E. AkyÃ¼rek, T. Bolukbasi, F. Liu, et al., Towards tracing factual knowledge in language models back to the training data, 2022. arXiv: 2205.11482[cs]. [Online]. Available: http://arxiv.org/abs/2205.11482 (visited on 10/02/2024). [337] R. Alghamdi, Z. Liang, and X. Zhang, ArMATH: dataset for solving arabic math word problems, in Proceedings of the Thirteenth Language Resources and Evaluation Conference, N. Calzolari, F. BÃ©chet, P. Blache, et al., Eds., Marseille, France: European Language Resources Association, 2022, pp. 351362. [Online]. Available: https://aclanthology. org/2022.lrec-1.37 (visited on 10/02/2024). [338] Y. Bai, A. Jones, K. Ndousse, et al., Training helpful and harmless assistant with reinforcement learning from human feedback, 2022. arXiv: 2204.05862[cs]. [Online]. Available: http://arxiv.org/abs/2204.05862 (visited on 05/01/2024). [339] M. Bain, A. Nagrani, G. Varol, and A. Zisserman, Frozen in time: joint video and image encoder for end-to-end retrieval, 2022. DOI: 10.48550/arXiv.2104.00650. arXiv: 2104.00650[cs]. [Online]. Available: http://arxiv.org/abs/2104.00650 (visited on 05/03/2024). [340] A. Bhanushali, G. Bridgman, D. G, et al., Gram vaani ASR challenge on spontaneous telephone speech recordings in regional variations of hindi, in Interspeech 2022, ISCA, 2022, pp. 35483552. DOI: 10/gtsqzn. [Online]. Available: https://www.iscaarchive . org / interspeech _ 2022 / bhanushali22 _ interspeech . html (visited on 05/01/2024). [341] K. S. Bhogale, A. Raman, T. Javed, et al., Effectiveness of mining audio and text pairs from public data for improving ASR systems for low-resource languages, 2022. arXiv: 2208. 12666[cs,eess]. [Online]. Available: http://arxiv.org/abs/2208.12666 (visited on 05/01/2024). [342] S. Cao, J. Shi, L. Pan, et al., KQA pro: dataset with explicit compositional programs for complex question answering over knowledge base, 2022. arXiv: 2007.03875[cs]. [Online]. Available: http://arxiv.org/abs/2007.03875 (visited on 10/02/2024). J. S. Chan, M. Pieler, J. Jao, J. Scheurer, and E. Perez, Few-shot adaptation works with UnpredicTable data, 2022. arXiv: 2208 . 01009[cs]. [Online]. Available: http : / / arxiv.org/abs/2208.01009 (visited on 10/02/2024). [343] 62 The Data Provenance Initiative, 2024 [344] M. Chen, Z. Chu, S. Wiseman, and K. Gimpel, SummScreen: dataset for abstractive screenplay summarization, in Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), S. Muresan, P. Nakov, and A. Villavicencio, Eds., Dublin, Ireland: Association for Computational Linguistics, 2022, pp. 86028615. DOI: 10/gtsqxx. [Online]. Available: https://aclanthology.org/2022.acllong.589 (visited on 05/01/2024). [345] Z. Chen, B. Liu, S. Moon, C. Sankar, P. Crook, and W. Y. Wang, KETOD: Knowledgeenriched task-oriented dialogue, 2022. arXiv: 2205.05589[cs]. [Online]. Available: http://arxiv.org/abs/2205.05589 (visited on 05/01/2024). [346] Z. Cheng, H. Dong, Z. Wang, et al., HiTab: hierarchical table dataset for question answering and natural language generation, 2022. arXiv: 2108.06712[cs]. [Online]. Available: http://arxiv.org/abs/2108.06712 (visited on 10/02/2024). [347] S. Chiu, M. Li, Y.-T. Lin, and Y.-N. Chen, SalesBot: Transitioning from chit-chat to taskoriented dialogues, in Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), S. Muresan, P. Nakov, and A. Villavicencio, Eds., Dublin, Ireland: Association for Computational Linguistics, 2022, pp. 61436158. DOI: 10/gtsqxw. [Online]. Available: https://aclanthology.org/2022.acllong.425 (visited on 05/01/2024). [348] A. Conneau, M. Ma, S. Khanuja, et al., FLEURS: Few-shot learning evaluation of universal representations of speech, version: 1, 2022. arXiv: 2205.12446[cs,eess]. [Online]. Available: http://arxiv.org/abs/2205.12446 (visited on 05/01/2024). [349] V. Dankers, C. Lucas, and I. Titov, Can transformer be too compositional? analysing idiom processing in neural machine translation, in Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), S. Muresan, P. Nakov, and A. Villavicencio, Eds., Dublin, Ireland: Association for Computational Linguistics, 2022, pp. 36083626. DOI: 10 / g6k6xn. [Online]. Available: https : / / aclanthology . org/2022.acl-long.252 (visited on 10/02/2024). [350] M. Del Rio, P. Ha, Q. McNamara, C. Miller, and S. Chandra, Earnings-22: practical benchmark for accents in the wild, 2022. arXiv: 2203.15591[cs]. [Online]. Available: http://arxiv.org/abs/2203.15591 (visited on 05/29/2024). [351] K. Grauman, A. Westbury, E. Byrne, et al., Ego4d: Around the world in 3,000 hours of egocentric video, 2022. arXiv: 2110.07058[cs]. [Online]. Available: http://arxiv. org/abs/2110.07058 (visited on 05/02/2024). [352] Y. Gu, R. Tinn, H. Cheng, et al., Domain-specific language model pretraining for biomedical natural language processing, ACM Transactions on Computing for Healthcare, vol. 3, no. 1, pp. 123, 2022, ISSN: 2691-1957, 2637-8051. DOI: 10 / gnmkjx. arXiv: 2007 . 15779[cs]. [Online]. Available: http://arxiv.org/abs/2007.15779 (visited on 10/02/2024). [353] P. Henderson, M. S. Krass, L. Zheng, et al., Pile of law: Learning responsible data filtering from the law and 256gb open-source legal dataset, 2022. arXiv: 2207.00220[cs]. [Online]. Available: http://arxiv.org/abs/2207.00220 (visited on 10/02/2024). [354] C. D. Hernandez Mena, D. E. Mollberg, M. Borsky, and J. Gudnason, SamrÃ³mur children: An icelandic speech corpus, in Proceedings of the Thirteenth Language Resources and Evaluation Conference, N. Calzolari, F. Bechet, P. Blache, et al., Eds., Marseille, France: European Language Resources Association, 2022, pp. 9951002. [Online]. Available: https: //aclanthology.org/2022.lrec-1.105 (visited on 05/01/2024). [355] A. Huang, tis but thy name: Semantic question answering evaluation with 11m names for 1m entities, 2022. arXiv: 2202.13581[cs]. [Online]. Available: http://arxiv.org/ abs/2202.13581 (visited on 10/02/2024). [356] M. Ivgi, U. Shaham, and J. Berant, Efficient long-text understanding with short-text models, 2022. arXiv: 2208.00748[cs]. [Online]. Available: http://arxiv.org/abs/ 2208.00748 (visited on 10/02/2024). 63 The Data Provenance Initiative, [357] T. Javed, K. S. Bhogale, A. Raman, A. Kunchukuttan, P. Kumar, and M. M. Khapra, IndicSUPERB: speech processing universal performance benchmark for indian languages, 2022. arXiv: 2208.11761[cs, eess]. [Online]. Available: http://arxiv.org/abs/ 2208.11761 (visited on 05/01/2024). [358] H. Kim, Y. Yu, L. Jiang, et al., ProsocialDialog: prosocial backbone for conversational agents, 2022. arXiv: 2205.12688[cs]. [Online]. Available: http://arxiv.org/ abs/2205.12688 (visited on 05/01/2024). [359] M. Komeili, K. Shuster, and J. Weston, Internet-augmented dialogue generation, in Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), S. Muresan, P. Nakov, and A. Villavicencio, Eds., Dublin, Ireland: Association for Computational Linguistics, 2022, pp. 84608478. DOI: 10/gr75db. [Online]. Available: https://aclanthology.org/2022.acl-long.579 (visited on 05/01/2024). [360] C. Leong, J. Nemecek, J. Mansdorfer, A. Filighera, A. Owodunni, and D. Whitenack, Bloom library: Multimodal datasets in 300+ languages for variety of downstream tasks, 2022. arXiv: 2210.14712[cs]. [Online]. Available: http://arxiv.org/abs/2210. 14712 (visited on 05/29/2024). [361] Y. Li, D. Choi, J. Chung, et al., Competition-level code generation with AlphaCode, Science, vol. 378, no. 6624, pp. 10921097, 2022, ISSN: 0036-8075, 1095-9203. DOI: 10/grggxf. arXiv: 2203.07814[cs]. [Online]. Available: http://arxiv.org/abs/2203. 07814 (visited on 10/02/2024). [362] S. Lin, J. Hilton, and O. Evans, TruthfulQA: Measuring how models mimic human falsehoods, 2022. arXiv: 2109.07958[cs]. [Online]. Available: http://arxiv.org/abs/ 2109.07958 (visited on 05/29/2024). [363] P. Lu, S. Mishra, T. Xia, et al., Learn to explain: Multimodal reasoning via thought chains for science question answering, 2022. arXiv: 2209 . 09513[cs]. [Online]. Available: http://arxiv.org/abs/2209.09513 (visited on 05/01/2024). [364] R. Mukherjee, A. Bohra, A. Banerjee, et al., ECTSum: new benchmark dataset for bullet point summarization of long earnings call transcripts, in Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, Y. Goldberg, Z. Kozareva, and Y. Zhang, Eds., Abu Dhabi, United Arab Emirates: Association for Computational Linguistics, 2022, pp. 10 89310 906. DOI: 10 / gtsqxz. [Online]. Available: https : //aclanthology.org/2022.emnlp-main.748 (visited on 05/01/2024). [365] L. Nan, C. Hsieh, Z. Mao, et al., FeTaQA: Free-form table question answering, Transactions of the Association for Computational Linguistics, vol. 10, B. Roark and A. Nenkova, Eds., pp. 3549, 2022, Place: Cambridge, MA Publisher: MIT Press. DOI: 10/gtsqx3. [Online]. Available: https://aclanthology.org/2022.tacl-1.3 (visited on 05/01/2024). [366] L. T. Nguyen, N. L. Tran, L. Doan, M. Luong, and D. Q. Nguyen, high-quality and largescale dataset for english-vietnamese speech translation, 2022. arXiv: 2208.04243[cs]. [Online]. Available: http://arxiv.org/abs/2208.04243 (visited on 05/29/2024). [367] M. PlÃ¼ss, M. HÃ¼rlimann, M. Cuny, et al., SDS-200: swiss german speech to standard german text corpus, 2022. arXiv: 2205 . 09501[cs]. [Online]. Available: http : / / arxiv.org/abs/2205.09501 (visited on 05/29/2024). [368] A. Pratapa, R. Gupta, and T. Mitamura, Multilingual event linking to wikidata, 2022. arXiv: 2204.06535[cs]. [Online]. Available: http://arxiv.org/abs/2204.06535 (visited on 10/02/2024). [369] K. Qian, S. Kottur, A. Beirami, et al., Database search results disambiguation for taskoriented dialog systems, in Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, M. Carpuat, M.-C. de Marneffe, and I. V. Meza Ruiz, Eds., Seattle, United States: Association for Computational Linguistics, 2022, pp. 11581173. DOI: 10/gtsqx2. [Online]. Available: https://aclanthology.org/2022.naacl-main.85 (visited on 05/01/2024). [370] V. Sanh, A. Webson, C. Raffel, et al., Multitask prompted training enables zero-shot task generalization, 2022. arXiv: 2110.08207[cs]. [Online]. Available: http://arxiv. org/abs/2110.08207 (visited on 10/02/2024). The Data Provenance Initiative, 2024 [371] P. E. Solberg and P. Ortiz, The norwegian parliamentary speech corpus, in Proceedings of the Thirteenth Language Resources and Evaluation Conference, N. Calzolari, F. BÃ©chet, P. Blache, et al., Eds., Marseille, France: European Language Resources Association, 2022, pp. 10031008. [Online]. Available: https://aclanthology.org/2022.lrec1.106 (visited on 05/29/2024). [372] M. Soldan, A. Pardo, J. L. AlcÃ¡zar, et al., MAD: scalable dataset for language grounding in videos from movie audio descriptions, 2022. arXiv: 2112.00431[cs]. [Online]. Available: http://arxiv.org/abs/2112.00431 (visited on 05/02/2024). [373] A. Virkkunen, A. Rouhe, N. Phan, and M. Kurimo, Finnish parliament ASR corpus - analysis, benchmarks and statistics, 2022. arXiv: 2203.14876[cs,eess]. [Online]. Available: http://arxiv.org/abs/2203.14876 (visited on 05/29/2024). [374] B. Wang, C. Xu, S. Wang, et al., Adversarial GLUE: multi-task benchmark for robustness evaluation of language models, 2022. arXiv: 2111 . 02840[cs]. [Online]. Available: http://arxiv.org/abs/2111.02840 (visited on 10/02/2024). [375] Y. Wang, Y. Sun, Y. Huang, et al., FERV39k: large-scale multi-scene dataset for facial expression recognition in videos, 2022. arXiv: 2203.09463[cs]. [Online]. Available: http://arxiv.org/abs/2203.09463 (visited on 05/02/2024). [376] W. Xiang, C. Li, K. Li, B. Wang, X.-S. Hua, and L. Zhang, CDAD: common daily action dataset with collected hard negative samples, in 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), New Orleans, LA, USA: IEEE, 2022, pp. 39203929, ISBN: 978-1-66548-739-9. DOI: 10/gtsqzp. [Online]. Available: https: //ieeexplore.ieee.org/document/9857185/ (visited on 05/02/2024). [377] H. Xue, T. Hang, Y. Zeng, et al., Advancing high-resolution video-language representation with large-scale video transcriptions, 2022. arXiv: 2111.10337[cs]. [Online]. Available: http://arxiv.org/abs/2111.10337 (visited on 05/29/2024). [378] Z. Yang, Y. Chen, L. Luo, et al., Open source MagicData-RAMC: rich annotated mandarin conversational(RAMC) speech dataset, 2022. arXiv: 2203.16844[cs,eess]. [Online]. Available: http://arxiv.org/abs/2203.16844 (visited on 05/29/2024). [379] B. Zhang, H. Lv, P. Guo, et al., WenetSpeech: 10000+ hours multi-domain mandarin corpus for speech recognition, 2022. arXiv: 2110.03370[cs]. [Online]. Available: http: //arxiv.org/abs/2110.03370 (visited on 05/01/2024). J. Zhang, K. Hashimoto, Y. Wan, et al., Are pretrained transformers robust in intent classification? missing ingredient in evaluation of out-of-scope intent detection, 2022. arXiv: 2106.04564[cs]. [Online]. Available: http://arxiv.org/abs/2106.04564 (visited on 05/01/2024). [380] [381] M. Zhu, A. Jain, K. Suresh, R. Ravindran, S. Tipirneni, and C. K. Reddy, XLCoST: benchmark dataset for cross-lingual code intelligence, 2022. arXiv: 2206.08474[cs]. [Online]. Available: http://arxiv.org/abs/2206.08474 (visited on 05/29/2024). [382] Z. Azerbayev, B. Piotrowski, H. Schoelkopf, E. W. Ayers, D. Radev, and J. Avigad, ProofNet: Autoformalizing and formally proving undergraduate-level mathematics, 2023. arXiv: 2302. 12433[cs]. [Online]. Available: http://arxiv.org/abs/2302.12433 (visited on 10/02/2024). [383] S. Barham, O. Weller, M. Yuan, et al., MegaWika: Millions of reports and their sources across 50 diverse languages, 2023. arXiv: 2307.07049[cs]. [Online]. Available: http: //arxiv.org/abs/2307.07049 (visited on 05/01/2024). [384] M. Chen, A. Papangelis, C. Tao, et al., PLACES: Prompting language models for social conversation synthesis, 2023. arXiv: 2302.03269[cs]. [Online]. Available: http:// arxiv.org/abs/2302.03269 (visited on 05/01/2024). [385] W. Chen, M. Yin, M. Ku, et al., TheoremQA: theorem-driven question answering dataset, 2023. arXiv: 2305.12524[cs]. [Online]. Available: http://arxiv.org/abs/ 2305.12524 (visited on 05/01/2024). [386] X. Deng, Y. Gu, B. Zheng, et al., Mind2web: Towards generalist agent for the web, 2023. arXiv: 2306.06070[cs]. [Online]. Available: http://arxiv.org/abs/2306. 06070 (visited on 05/02/2024). 65 The Data Provenance Initiative, 2024 [387] T. Dettmers, A. Pagnoni, A. Holtzman, and L. Zettlemoyer, QLoRA: Efficient finetuning of quantized LLMs, 2023. arXiv: 2305.14314[cs]. [Online]. Available: http://arxiv. org/abs/2305.14314 (visited on 05/01/2024). [388] N. Ding, Y. Chen, B. Xu, et al., Enhancing chat language models by scaling high-quality instructional conversations, 2023. arXiv: 2305.14233[cs]. [Online]. Available: http: //arxiv.org/abs/2305.14233 (visited on 05/02/2024). [389] M. Al-Fetyani, M. Al-Barham, G. Abandah, A. Alsharkawi, and M. Dawas, MASC: Massive arabic speech corpus, in 2022 IEEE Spoken Language Technology Workshop (SLT), Doha, Qatar: IEEE, 2023, pp. 10061013, ISBN: 979-8-3503-9690-4. DOI: 10/gtsqzj. [Online]. Available: https://ieeexplore.ieee.org/document/10022652/ (visited on 05/01/2024). [390] T. Gosling, A. Dale, and Y. Zheng, PIPPA: partially synthetic conversational dataset, 2023. arXiv: 2308.05884[cs]. [Online]. Available: http://arxiv.org/abs/2308. 05884 (visited on 05/01/2024). [391] T. Han, L. C. Adams, J.-M. Papaioannou, et al., MedAlpaca an open-source collection of medical conversational AI models and training data, 2023. arXiv: 2304.08247[cs]. [Online]. Available: http://arxiv.org/abs/2304.08247 (visited on 05/01/2024). [392] A. KÃ¶pf, Y. Kilcher, D. von RÃ¼tte, et al., OpenAssistant conversations democratizing large language model alignment, 2023. arXiv: 2304.07327[cs]. [Online]. Available: http://arxiv.org/abs/2304.07327 (visited on 05/02/2024). [393] H. Li, F. Koto, M. Wu, A. F. Aji, and T. Baldwin, Bactrian-x: Multilingual replicable instruction-following models with low-rank adaptation, 2023. arXiv: 2305.15011[cs]. [Online]. Available: http://arxiv.org/abs/2305.15011 (visited on 05/01/2024). [394] X. Li, S. Takamichi, T. Saeki, W. Chen, S. Shiota, and S. Watanabe, Yodas: Youtubeoriented dataset for audio and speech, in 2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), Taipei, Taiwan: IEEE, 2023, pp. 18, ISBN: 979-8-35030689-7. DOI: 10/gtsqzc. [Online]. Available: https://ieeexplore.ieee.org/ document/10389689/ (visited on 05/01/2024). [395] Y. Li, Z. Li, K. Zhang, R. Dan, S. Jiang, and Y. Zhang, ChatDoctor: medical chat model fine-tuned on large language model meta-AI (LLaMA) using medical domain knowledge, 2023. arXiv: 2303.14070[cs]. [Online]. Available: http://arxiv.org/abs/ 2303.14070 (visited on 05/01/2024). [396] H. Lightman, V. Kosaraju, Y. Burda, et al., Lets verify step by step, 2023. arXiv: 2305. 20050[cs]. [Online]. Available: http://arxiv.org/abs/2305.20050 (visited on 05/01/2024). [397] X. Liu, H. Yu, H. Zhang, et al., AgentBench: Evaluating LLMs as agents, 2023. arXiv: 2308.03688[cs]. [Online]. Available: http://arxiv.org/abs/2308.03688 (visited on 05/02/2024). J. Macina, N. Daheim, S. P. Chowdhury, et al., MathDial: dialogue tutoring dataset with rich pedagogical properties grounded in math reasoning problems, 2023. arXiv: 2305 . 14536[cs]. [Online]. Available: http://arxiv.org/abs/2305.14536 (visited on 05/01/2024). [398] [399] I. Mamtimin, W. Du, and A. Hamdulla, M2asr-KIRGHIZ: free kirghiz speech database and accompanied baselines, Information, vol. 14, no. 1, p. 55, 2023, ISSN: 2078-2489. DOI: 10/gtsqzm. [Online]. Available: https://www.mdpi.com/2078-2489/14/1/ 55 (visited on 05/01/2024). [400] S. Mishra, M. Finlayson, P. Lu, et al., Lila: unified benchmark for mathematical reasoning, 2023. arXiv: 2210.17517[cs]. [Online]. Available: http://arxiv.org/abs/ 2210.17517 (visited on 10/02/2024). [401] X.-P. Nguyen, W. Zhang, X. Li, et al., SeaLLMs large language models for southeast asia, 2023. arXiv: 2312.00738[cs]. [Online]. Available: http://arxiv.org/abs/ 2312.00738 (visited on 05/01/2024). 66 The Data Provenance Initiative, 2024 [402] T. Olatunji, T. Afonja, A. Yadavalli, et al., AfriSpeech-200: Pan-african accented speech dataset for clinical and general domain ASR, 2023. arXiv: 2310.00274[cs]. [Online]. Available: http://arxiv.org/abs/2310.00274 (visited on 05/29/2024). [403] X. Pan, N. Charron, Y. Yang, et al., Aria digital twin: new benchmark dataset for egocentric 3d machine perception, 2023. arXiv: 2306 . 06362[cs]. [Online]. Available: http : //arxiv.org/abs/2306.06362 (visited on 05/29/2024). J. Park, J.-W. Hwang, K. Choi, et al., OLKAVS: An open large-scale korean audio-visual speech dataset, 2023. arXiv: 2301.06375[cs]. [Online]. Available: http://arxiv. org/abs/2301.06375 (visited on 05/29/2024). [404] [405] T. M. Pham, S. Yoon, T. Bui, and A. Nguyen, PiC: phrase-in-context dataset for phrase understanding and semantic search, 2023. arXiv: 2207.09068[cs]. [Online]. Available: http://arxiv.org/abs/2207.09068 (visited on 05/29/2024). [406] K. Raju, A. V, R. Lish, and J. Mathew, Snow mountain: Dataset of audio recordings of the bible in low resource languages, 2023. arXiv: 2206.01205[cs, eess]. [Online]. Available: http://arxiv.org/abs/2206.01205 (visited on 05/29/2024). [407] O. Rohanian, M. Nouriborji, and D. A. Clifton, Exploring the effectiveness of instruction tuning in biomedical language processing, 2023. arXiv: 2401 . 00579[cs]. [Online]. Available: http://arxiv.org/abs/2401.00579 (visited on 05/01/2024). [408] P. Rust, J. F. Lotz, E. Bugliarello, E. Salesky, M. de Lhoneux, and D. Elliott, Language modelling with pixels, 2023. arXiv: 2207.06991[cs]. [Online]. Available: http:// arxiv.org/abs/2207.06991 (visited on 10/02/2024). [409] R. Sanabria, N. Bogoychev, N. Markl, A. Carmantini, O. Klejch, and P. Bell, The edinburgh international accents of english corpus: Towards the democratization of english ASR, 2023. arXiv: 2303.18110[cs, eess]. [Online]. Available: http://arxiv.org/abs/ 2303.18110 (visited on 05/01/2024). [410] T. Sawada, D. Paleka, A. Havrilla, et al., ARB: Advanced reasoning benchmark for large language models, 2023. arXiv: 2307.13692[cs]. [Online]. Available: http://arxiv. org/abs/2307.13692 (visited on 05/01/2024). [411] D. Sileo and M.-F. Moens, Probing neural language models for understanding of words of estimative probability, 2023. arXiv: 2211.03358[cs]. [Online]. Available: http: //arxiv.org/abs/2211.03358 (visited on 05/29/2024). [412] H. Ullrich, J. Drchal, M. RÃ½par, H. VincourovÃ¡, and V. Moravec, CsFEVER and CTKFacts: Acquiring czech data for fact verification, Language Resources and Evaluation, vol. 57, no. 4, pp. 15711605, 2023, ISSN: 1574-020X, 1574-0218. DOI: 10 / g6k95h. arXiv: 2201.11115[cs]. [Online]. Available: http://arxiv.org/abs/2201.11115 (visited on 10/02/2024). [413] A. Ushio, F. Alva-Manchego, and J. Camacho-Collados, Generative language models for paragraph-level question generation, 2023. arXiv: 2210.03992[cs]. [Online]. Available: http://arxiv.org/abs/2210.03992 (visited on 10/02/2024). [414] Y. Wang, Y. Kordi, S. Mishra, et al., Self-instruct: Aligning language models with selfgenerated instructions, 2023. arXiv: 2212.10560[cs]. [Online]. Available: http:// arxiv.org/abs/2212.10560 (visited on 05/29/2024). [415] Z. Wang, Y. Dong, J. Zeng, et al., HelpSteer: Multi-attribute helpfulness dataset for SteerLM, 2023. arXiv: 2311.09528[cs]. [Online]. Available: http://arxiv.org/abs/ 2311.09528 (visited on 05/01/2024). [416] A. Warstadt, A. Parrish, H. Liu, et al., BLiMP: The benchmark of linguistic minimal pairs for english, 2023. arXiv: 1912.00582[cs]. [Online]. Available: http://arxiv.org/ abs/1912.00582 (visited on 05/29/2024). [417] C. Wu, W. Lin, X. Zhang, Y. Zhang, Y. Wang, and W. Xie, PMC-LLaMA: Towards building open-source language models for medicine, 2023. arXiv: 2304.14454[cs]. [Online]. Available: http://arxiv.org/abs/2304.14454 (visited on 05/01/2024). [418] S. Yao, H. Chen, J. Yang, and K. Narasimhan, WebShop: Towards scalable real-world web interaction with grounded language agents, 2023. arXiv: 2207.01206[cs]. [Online]. Available: http://arxiv.org/abs/2207.01206 (visited on 05/02/2024). 67 The Data Provenance Initiative, 2024 [419] S. Ye, Y. Jo, D. Kim, S. Kim, H. Hwang, and M. Seo. SelFee: Iterative self-revising LLM empowered by self-feedback generation, LK Lab. (2023), [Online]. Available: https: //kaistai.github.io/SelFee/ (visited on 05/01/2024). [420] Y. Yin, D. Mori, and S. Fujimoto, ReazonSpeech: free and massive corpus for japanese ASR, 2023. [Online]. Available: https : / / research . reazon . jp / _static / reazonspeech_nlp2023.pdf. [421] L. Yu, W. Jiang, H. Shi, et al., MetaMath: Bootstrap your own mathematical questions for large language models, 2023. arXiv: 2309.12284[cs]. [Online]. Available: http: //arxiv.org/abs/2309.12284 (visited on 05/01/2024). [422] X. Yue, X. Qu, G. Zhang, et al., MAmmoTH: Building math generalist models through hybrid instruction tuning, 2023. arXiv: 2309.05653[cs]. [Online]. Available: http: //arxiv.org/abs/2309.05653 (visited on 05/01/2024). [423] A. Zeng, M. Liu, R. Lu, et al., AgentTuning: Enabling generalized agent abilities for LLMs, 2023. arXiv: 2310.12823[cs]. [Online]. Available: http://arxiv.org/abs/ 2310.12823 (visited on 05/02/2024). [424] G. Zhang, Y. Shi, R. Liu, et al., Chinese open instruction generalist: preliminary release, 2023. arXiv: 2304.07987[cs]. [Online]. Available: http://arxiv.org/abs/ 2304.07987 (visited on 05/01/2024). J. Zhang, R. Gan, J. Wang, et al., Fengshenbang 1.0: Being the foundation of chinese cognitive intelligence, 2023. arXiv: 2209.02970[cs]. [Online]. Available: http:// arxiv.org/abs/2209.02970 (visited on 10/02/2024). [425] [426] W. Zhao, X. Ren, J. Hessel, C. Cardie, Y. Choi, and Y. Deng, WildChat: 1m ChatGPT interaction logs in the wild, in Proceedings of the Twelfth International Conference on Learning Representations, 2023. [Online]. Available: https://openreview.net/ forum?id=Bl8u7ZRlbM (visited on 05/01/2024). [427] L. Zheng, W.-L. Chiang, Y. Sheng, et al., Judging LLM-as-a-judge with MT-bench and chatbot arena, 2023. arXiv: 2306.05685[cs]. [Online]. Available: http://arxiv. org/abs/2306.05685 (visited on 05/01/2024). [428] S. Zhou, U. Alon, F. F. Xu, Z. Wang, Z. Jiang, and G. Neubig, DocPrompting: Generating code by retrieving the docs, 2023. arXiv: 2207.05987[cs]. [Online]. Available: http: //arxiv.org/abs/2207.05987 (visited on 10/02/2024). [429] X. Zhou, H. Zhu, A. Yerukola, et al., COBRA frames: Contextual reasoning about effects and harms of offensive statements, 2023. arXiv: 2306.01985[cs]. [Online]. Available: http://arxiv.org/abs/2306.01985 (visited on 05/01/2024). [430] A. Abdallah, M. Kasem, M. Abdalla, et al., ArabicaQA: comprehensive dataset for arabic question answering, 2024. arXiv: 2403 . 17848[cs]. [Online]. Available: http : / / arxiv.org/abs/2403.17848 (visited on 10/02/2024). [431] M. Aloui, H. Chouikhi, G. Chaabane, H. Kchaou, and C. Dhaouadi, 101 billion arabic words dataset, 2024. arXiv: 2405.01590[cs]. [Online]. Available: http://arxiv.org/ abs/2405.01590 (visited on 10/02/2024). [432] Z. Alyafeai, K. Almubarak, A. Ashraf, et al., CIDAR: Culturally relevant instruction dataset for arabic, 2024. arXiv: 2402.03177[cs]. [Online]. Available: http://arxiv.org/ abs/2402.03177 (visited on 05/01/2024). [433] Y. Bai, X. Du, Y. Liang, et al., COIG-CQIA: Quality is all you need for chinese instruction fine-tuning, version: 1, 2024. arXiv: 2403.18058[cs]. [Online]. Available: http:// arxiv.org/abs/2403.18058 (visited on 05/01/2024). [434] Y. Bai, X. Lv, J. Zhang, et al., LongAlign: recipe for long context alignment of large language models, 2024. arXiv: 2401.18058[cs]. [Online]. Available: http://arxiv. org/abs/2401.18058 (visited on 05/01/2024). [435] L. Chen, X. Wei, J. Li, et al., ShareGPT4video: Improving video understanding and generation with better captions, 2024. arXiv: 2406.04325[cs]. [Online]. Available: http: //arxiv.org/abs/2406.04325 (visited on 10/02/2024). 68 The Data Provenance Initiative, [436] H. Chouikhi, M. Aloui, C. B. Hammou, G. Chaabane, H. Kchaou, and C. Dhaouadi, GemmAr: Enhancing LLMs through arabic instruction-tuning, 2024. arXiv: 2407 . 02147[cs]. [Online]. Available: http://arxiv.org/abs/2407.02147 (visited on 10/02/2024). J. Gala, T. Jayakumar, J. A. Husain, et al., Airavata: Introducing hindi instruction-tuned LLM, 2024. arXiv: 2401.15006[cs]. [Online]. Available: http://arxiv.org/abs/ 2401.15006 (visited on 05/01/2024). [437] [438] S. Kim, J. Shin, Y. Cho, et al., Prometheus: Inducing fine-grained evaluation capability in language models, 2024. arXiv: 2310.08491[cs]. [Online]. Available: http://arxiv. org/abs/2310.08491 (visited on 05/01/2024). [439] K. Li, Y. Wang, Y. He, et al., MVBench: comprehensive multi-modal video understanding benchmark, 2024. arXiv: 2311 . 17005[cs]. [Online]. Available: http : / / arxiv . org/abs/2311.17005 (visited on 10/02/2024). [440] W. Liu, W. Zeng, K. He, Y. Jiang, and J. He, What makes good data for alignment? comprehensive study of automatic data selection in instruction tuning, 2024. arXiv: 2312. 15685[cs]. [Online]. Available: http://arxiv.org/abs/2312.15685 (visited on 05/01/2024). [441] Z. Lv, N. Charron, P. Moulon, et al., Aria everyday activities dataset, 2024. arXiv: 2402. 13349[cs]. [Online]. Available: http://arxiv.org/abs/2402.13349 (visited on 05/29/2024). [442] C. Malaviya, S. Lee, S. Chen, E. Sieber, M. Yatskar, and D. Roth, ExpertQA: Expert-curated questions and attributed answers, 2024. arXiv: 2309.07852[cs]. [Online]. Available: http://arxiv.org/abs/2309.07852 (visited on 05/01/2024). [443] A. Mitra, H. Khanpour, C. Rosset, and A. Awadallah, Orca-math: Unlocking the potential of SLMs in grade school math, 2024. arXiv: 2402 . 14830[cs]. [Online]. Available: http://arxiv.org/abs/2402.14830 (visited on 05/01/2024). [444] K. Nan, R. Xie, P. Zhou, et al., OpenVid-1m: large-scale high-quality dataset for text-tovideo generation, 2024. arXiv: 2407.02371[cs]. [Online]. Available: http://arxiv. org/abs/2407.02371 (visited on 10/02/2024). [445] S. Pieri, S. S. Mullappilly, F. S. Khan, et al., BiMediX: Bilingual medical mixture of experts LLM, 2024. arXiv: 2402.13253[cs]. [Online]. Available: http://arxiv.org/ abs/2402.13253 (visited on 10/02/2024). [446] S. Singh, F. Vargus, D. Dsouza, et al., Aya dataset: An open-access collection for multilingual instruction tuning, 2024. arXiv: 2402.06619[cs]. [Online]. Available: http: //arxiv.org/abs/2402.06619 (visited on 05/01/2024). I. Solak, The m-AILABS speech dataset caito, 2024. [Online]. Available: https : / / www.caito.de/2019/01/03/the-m-ailabs-speech-dataset/ (visited on 05/01/2024). [447] [448] H. Sun, L. Liu, J. Li, et al., Conifer: Improving complex constrained instruction-following ability of large language models, 2024. arXiv: 2404.02823[cs]. [Online]. Available: http://arxiv.org/abs/2404.02823 (visited on 05/01/2024). [449] S. Toshniwal, I. Moshkov, S. Narenthiran, D. Gitman, F. Jia, and I. Gitman, OpenMathInstruct1: 1.8 million math instruction tuning dataset, 2024. arXiv: 2402.10176[cs]. [Online]. Available: http://arxiv.org/abs/2402.10176 (visited on 05/01/2024). [450] W. Wang and Y. Yang, VidProM: million-scale real prompt-gallery dataset for text-tovideo diffusion models, 2024. arXiv: 2403 . 06098[cs]. [Online]. Available: http : //arxiv.org/abs/2403.06098 (visited on 10/02/2024). [451] X. Wang, Z. Hu, P. Lu, et al., SciBench: Evaluating college-level scientific problem-solving abilities of large language models, 2024. arXiv: 2307.10635[cs]. [Online]. Available: http://arxiv.org/abs/2307.10635 (visited on 05/01/2024). [452] F. Xu, K. Lo, L. Soldaini, B. Kuehl, E. Choi, and D. Wadden, KIWI: dataset of knowledge-intensive writing instructions for answering research questions, 2024. arXiv: 2403.03866[cs]. [Online]. Available: http://arxiv.org/abs/2403.03866 (visited on 05/01/2024). 69 The Data Provenance Initiative, [453] Z. Xu, F. Jiang, L. Niu, et al., Magpie: Alignment data synthesis from scratch by prompting aligned LLMs with nothing, 2024. DOI: 10 . 48550 / arXiv . 2406 . 08464. arXiv: 2406.08464[cs]. [Online]. Available: http://arxiv.org/abs/2406.08464 (visited on 10/02/2024). [454] X. Zhang, C. Tian, X. Yang, L. Chen, Z. Li, and L. R. Petzold, AlpaCare:instruction-tuned large language models for medical application, 2024. arXiv: 2310.14558[cs]. [Online]. Available: http://arxiv.org/abs/2310.14558 (visited on 05/01/2024). [455] L. Zheng, W.-L. Chiang, Y. Sheng, et al., LMSYS-chat-1m: large-scale real-world LLM conversation dataset, 2024. arXiv: 2309.11998[cs]. [Online]. Available: http:// arxiv.org/abs/2309.11998 (visited on 05/01/2024)."
        }
    ],
    "affiliations": [
        "The Data Provenance Initiative"
    ]
}