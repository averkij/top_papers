{
    "paper_title": "Temporal In-Context Fine-Tuning for Versatile Control of Video Diffusion Models",
    "authors": [
        "Kinam Kim",
        "Junha Hyung",
        "Jaegul Choo"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in text-to-video diffusion models have enabled high-quality video synthesis, but controllable generation remains challenging, particularly under limited data and compute. Existing fine-tuning methods for conditional generation often rely on external encoders or architectural modifications, which demand large datasets and are typically restricted to spatially aligned conditioning, limiting flexibility and scalability. In this work, we introduce Temporal In-Context Fine-Tuning (TIC-FT), an efficient and versatile approach for adapting pretrained video diffusion models to diverse conditional generation tasks. Our key idea is to concatenate condition and target frames along the temporal axis and insert intermediate buffer frames with progressively increasing noise levels. These buffer frames enable smooth transitions, aligning the fine-tuning process with the pretrained model's temporal dynamics. TIC-FT requires no architectural changes and achieves strong performance with as few as 10-30 training samples. We validate our method across a range of tasks, including image-to-video and video-to-video generation, using large-scale base models such as CogVideoX-5B and Wan-14B. Extensive experiments show that TIC-FT outperforms existing baselines in both condition fidelity and visual quality, while remaining highly efficient in both training and inference. For additional results, visit https://kinam0252.github.io/TIC-FT/"
        },
        {
            "title": "Start",
            "content": "Temporal In-Context Fine-Tuning for Versatile Control of Video Diffusion Models Kinam Kim Junha Hyung KAIST AI {kinamplify, sharpeeee, jchoo}@kaist.ac.kr Jaegul Choo 5 2 0 2 J 1 ] . [ 1 6 9 9 0 0 . 6 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Recent advances in text-to-video diffusion models have enabled high-quality video synthesis, but controllable generation remains challengingparticularly under limited data and compute. Existing fine-tuning methods for conditional generation often rely on external encoders or architectural modifications, which demand large datasets and are typically restricted to spatially aligned conditioning, limiting flexibility and scalability. In this work, we introduce Temporal In-Context Fine-Tuning (TIC-FT), an efficient and versatile approach for adapting pretrained video diffusion models to diverse conditional generation tasks. Our key idea is to concatenate condition and target frames along the temporal axis and insert intermediate buffer frames with progressively increasing noise levels. These buffer frames enable smooth transitions, aligning the fine-tuning process with the pretrained models temporal dynamics. TIC-FT requires no architectural changes and achieves strong performance with as few as 1030 training samples. We validate our method across range of tasksincluding image-to-video and video-to-video generationusing large-scale base models such as CogVideoX-5B and Wan-14B. Extensive experiments show that TIC-FT outperforms existing baselines in both condition fidelity and visual quality, while remaining highly efficient in both training and inference. For additional results, visit https://kinam0252.github.io/TIC-FT/."
        },
        {
            "title": "Introduction",
            "content": "Text-to-video generation models have advanced rapidly, reaching quality levels suitable for professional applications [1, 2, 3, 4, 5, 6]. Beyond basic generation, recent research has increasingly focused on leveraging pretrained models to enable more precise control and conditional guidance, addressing the growing demand for finer adjustments and more nuanced generation capabilities [7, 8, 9, 10, 11, 12, 13, 14, 15, 16]. Despite this progress, current fine-tuning approaches for conditioning video diffusion models face notable limitations. Many methods require large training datasets and introduce additional architectural components, such as ControlNet [7] or other external modules, which impose substantial memory overhead. Moreover, the reliance on external encoders for conditioning often leads to the loss of fine-grained details during the encoding process. ControlNet-style methods [16, 17, 14], in particular, operate within rigid conditioning frameworks: they are primarily designed for spatially aligned conditions and require conditioning signals to match the target video length. For example, when conditioning on single image, common workarounds include replicating the image across the temporal dimension to align with the video frames or embedding it as global feature. These approaches typically necessitate task-specific adaptations of the conditioning pipeline. Alternative * indicates equal contribution. Preprint. Under review. Figure 1: Demonstration of our method across diverse tasks, including character-to-video, virtual try-on, ad-video generation, object-to-motion, toonification, and video style transfer. fine-tuning strategies, such as IP-Adapter [18] and latent concatenation [10], encounter similar challenges regarding flexibility and computational cost, as they modify or expand the pretrained model architectures. In contrast, in-context learning (ICL) [19] offers more efficient and versatile paradigm. ICL is training-free and can be flexibly applied to user-defined tasks by providing examples directly within the input context, eliminating the need for additional parameter updates. While ICL has shown strong success in large language models [19, 20], its application to image and video generation has primarily been explored in autoregressive models [21, 22], with limited adaptation to diffusion models. Efforts to implement ICL in diffusion models [23, 15] have often relied on ControlNet-style training approaches, which contradict the core advantage of ICL: leveraging pretrained distributions without additional training. Departing slightly from the pure ICL paradigm, recent work has introduced in-context LoRA [8], related technique that enables consistent image generation by producing multiple images in single forward pass arranged in grids, thereby facilitating information sharing across images. With minimal fine-tuning, this method achieves high-quality and highly consistent results, benefiting from the inherent in-context generation capabilities of pretrained text-to-image models, which are naturally suited for grid-based generation. In contrast, video generation models possess far less of this capability. Although concurrent research has explored extending in-context LoRA to video generation [24], these models are poorly suited to producing grid-like outputs, making the approach significantly more training-intensive and less effective. Furthermore, these methods are not inherently designed for conditional generation and often depend on training-free inpainting strategies [25, 26], which tend to degrade performance. They also lack flexibility in handling mismatches between the condition length and the number of target frames, as there are no straightforward solutions for general cases. In the simple case of conditioning on single image, the image must be redundantly replicated across all frames, resulting in substantial increases in memory usage and computational overhead. 2 In this paper, we propose highly effective and versatile fine-tuning method for conditional video diffusion models: temporal in-context fine-tuning. Instead of spatially concatenating condition and target inputs, our approach aligns them temporallyconcatenating condition frame(s) and target frame(s) along the time axisand fine-tunes the model using only minimal number of samples. This design leverages the inherent capability of pretrained video diffusion models to process temporally ordered inputs, enabling effective generation when condition and target frames are arranged sequentially. To ensure smooth transition between the condition and target frames, we introduce buffer framesintermediate frames with monotonically increasing noise levels that bridge the gap between the clean condition frames and the fully noised target frames. These buffer frames facilitate smooth, natural fade-out transitions from condition to generated frames, preventing abrupt scene transitions and preserving consistency with the pretrained models distribution. Combined with this design, our method enables fine-tuning with as few as 1030 training samples. Additionally, our method preserves the original model architecture without introducing additional modules, thereby reducing VRAM requirements. The proposed approach also allows the model to leverage condition frames directly through unified 3D attention, avoiding the detail loss typically introduced by external encoders. Furthermore, it enables versatile conditional generation by eliminating the need for spatial alignment and accommodating wide range of condition lengthsfrom single images to full video sequencesthereby supporting diverse video-to-video translations and image-to-video generation tasks. In summary, our main contributions are as follows: We propose temporal in-context fine-tuning, simple yet highly effective method for conditional video diffusion that minimizes the distribution mismatch between pretraining and fine-tuning, without requiring architectural modifications. We demonstrate strong performance with minimal training data (1030 samples), offering highly efficient fine-tuning strategy. Our method enables versatile conditioning, supporting variable-length inputs and unifying diverse imageand video-conditioned generation tasks within single framework. We validate our method across wide range of tasks, including reference-to-video generation, motion transfer, keyframe interpolation, and style transfer with varying condition content and lengths."
        },
        {
            "title": "2 Related work",
            "content": "Conditional Video Diffusion Models. Many conditional video generation methods [7, 8, 9, 10, 11, 12, 13, 14, 15, 16] rely on auxiliary encoders (e.g., ControlNet [7]) or architectural modifications (e.g., IP-Adapter [18]), which prevent full exploitation of the pretrained models capabilities. These approaches typically require larger datasets, longer training, and incur significant memory overhead. Moreover, they are often limited to spatially aligned conditioning, making them less suitable for variable-length or misaligned conditiontarget pairs. In-Context Learning for Diffusion Models. Inspired by its success in language models [19, 20], in-context finetuning (IC-FT) has been explored in visual domains via grid-based generation [8, 24], but its extension to video is limited. Videos rarely follow grid layouts, and inference methods like SDEdit [25] degrade output quality. Moreover, these approaches assume strict conditionoutput alignment, making them unsuitable for flexible conditional video generation. Diffusion with Heterogeneous Noise Levels. Recent works such as FIFO-Diffusion[27] and Diffusion Forcing[28] demonstrate that diffusion models can effectively operate on sequences with varying noise levels across frames or tokenschallenging the conventional assumption of uniform noise and motivating our use of buffer frames with progressively increasing noise. Building on these ideas, we propose Temporal In-Context Fine-Tuning (TIC-FT)a simple yet effective method that temporally concatenates condition and target frames, inserting buffer frames with increasing noise levels to smooth abrupt transitions in both scene content and noise levels. 3 Unlike ControlNet-style methods, TIC-FT requires no architectural changes and naturally supports variable-length, spatially misaligned conditiontarget pairs."
        },
        {
            "title": "3 Method",
            "content": "3.1 Preliminaries We briefly review diffusion-based text-to-video generation. video with Fin RGB frames is x1:Fin RFin3HpixWpix . spatio-temporal encoder ϕ maps it to latents z(0) = z(0) 1:F = ϕ(x1:Fin ) RF CHW with Fin, Hpix, and Wpix, and decoder ψ approximately inverts ϕ. Latent frames are diffused by q(cid:0)z(0), t(cid:1) := z(t) = αtz(0) + σtε for {0, . . . , } and ε (0, I), with predefined schedule (αt, σt). DiT[29] backbone ϵθ predicts the noise and is trained with Ldiff = z(0),c,ε,t (cid:104)(cid:13) (cid:13)ε ϵθ(z(t), t, c)(cid:13) 2 (cid:13) 2 (cid:105) , (1) where latent frames and paired text condition is sampled from the dataset. Generation starts from z(T ) (0, I) and iteratively applies sampler z(t1) = S(cid:0)z(t), t, c; ϵθ (cid:1) until z(0), which ψ decodes to video. 3.2 Temporal concatenation Overview We introduce overall pipeline of the proposed temporal in-context fine-tuning (TIC-FT) in this section. We first detail the temporal concatenation of condition and target latentswith buffer frames that ease the abrupt scene and noise-level transitionfollowed by the inference and training procedures formalized in Algorithms 12. L+1, . . . ˆz(0) Setup. The task is to generate sequence of target frames of length K, denoted as ˆz(0) = [ˆz(0) ]. Our approach concatenates the condtion and target frames along the temporal axis. naïve formulation simply places the clean condition frames directly before the noisy target frames: L+K], conditioned on set of input frame(s): z(0) = [z(0) 1 , . . . z(0) z(t) = z(0) 1:L (cid:124)(cid:123)(cid:122)(cid:125) condition ˆz(t) (cid:124) L+1:L+K (cid:123)(cid:122) (cid:125) target R(L+K)CHW . (2) Here, ˆz(t) L+1:L+K represents the target latent frames at denoising timestep t. At inference time, we initialize with z(0)ˆz(T ) and iteratively denoise the concatenated frames with z(0)ˆz(t1) = S(cid:0)z(0)ˆz(t), t, c; ϵθ until reaching z(0) = z(0)ˆz(0). At each denoising step, only the target frames are denoised, while the condition frames are fixed to enforce consistency. The final output video corresponds to the target slice z(0) The flexibility of varying allows this formulation to generalize across wide range of conditional video generation tasks. When = 1, the problem becomes an image-to-video generation task: producing full video sequence from single reference image together with text description of the sequence. L+1:L+K. (3) (cid:1) 3.3 Buffer frames Unlike conventional image-to-video (I2V) approaches, where the condition acts as the first frame of the output, our setup also allows for discontinuous conditioning, broadening its applicability. For > 1, the method naturally extends to video-to-video generation. reference clip can perform video style transfer by transferring its appearance onto new motion sequence. Likewise, providing an action snippet along with query frame enables in-context action transfer, where the observed motion is adapted to novel scene. Supplying sparsely sampled frames supports keyframe interpolation, allowing the model to smoothly generate intermediate transitions between distant frames. 4 L+B+1:L+B+K Algorithm 1: TIC-FT inference Input: Clean condition latents z(0); buffer noise levels τ1:B; text prompt c; denoiser ϵθ Output: Denoised target latents ˆz(0) = z(0) Generate buffer latents z(τ1:B ) = q(cid:0)z(0), τ1:B Sample target latents ˆz(T ) (0, I); Concatenate z(T ) z(0) z(τ1:B ) ˆz(T ); for = to 1 do T(cid:0)z(t)(cid:1); { ti = }; S(cid:0)z(t), t, c; ϵθ z(t1) return z(0) (cid:1) A; (cid:1); L+B+1:L+B+K // add noise // global time descending // noise-level vector Thus, simple temporal concatenation serves as unified and highly versatile framework for diverse conditional video generation tasks. However, this naïve approach is suboptimal for fully leveraging the capabilities of the pretrained video diffusion model. Aligning the finetuning task as closely as possible with the pretrained models distribution is essential to achieve high efficiencyenabling strong performance with minimal data and computational resources. Thus, it is desirable to design the finetuning process around tasks the model is already proficient at. Direct concatenation violates this principle in two key ways. First, in scenarios where the target frames do not naturally continue from the condition framesi.e., when there is an abrupt scene transition between the last condition frame and the first target framethe model is forced to synthesize highly discontinuous content. Pretrained video diffusion models are typically trained on smoothly evolving sequences and lack the inherent capability to handle such abrupt transitions, as datasets with sudden scene changes are commonly filtered out during data curation. Second, diffusion models are not designed to denoise sequences containing frames with heterogeneous noise levels, as would occur when combining clean condition frames with noisy target frames during the sampling process. We therefore introduce intermediate frames whose noise levels τb linearly bridge 0 and : z(τ1:B ) = (cid:2) z(τ1) , . . . , z(τB ) (cid:3), τb = + 1 T. (4) There can be different design choices for the buffer frames, and we empirically find that using the noised condition frames, z(t) = z(t), yields good performance. Then the full initial latent sequence becomes z(T ) = z(0) 1:L (cid:124)(cid:123)(cid:122)(cid:125) condition z(τ1:B ) L+1:L+B (cid:123)(cid:122) (cid:125) (cid:124) buffer ˆz(T ) (cid:124) L+B+1:L+B+K (cid:123)(cid:122) (cid:125) target . (5) 3.4 Inference Let (z(t)) be noise level list corresponding to the latent sequence z(t): : RF CHW {0, . . . , }F . The initial noise level list at = is T(cid:0)z(T )(cid:1) = (cid:2) 0, τ1, . . . , τB, T, . . . , (cid:3) {0, . . . , }L+B+K. At any global timestep t, we define the noise levels as: T(cid:0)z(t)(cid:1) = (cid:2) 0, τ1(t), . . . , τB(t), t, . . . , t(cid:3), where τb(t) = τb if τb < t, and τb(t) = otherwise. (6) (7) Our inference algorithm proceeds by iteratively identifying the frames currently at the maximal noise level and applying the video diffusion sampler exclusively to those frames. This process continues from = down to = 0. The full inference procedure is detailed in Algorithm 1. 5 Algorithm 2: TIC-FT training Input: Dataset with tuples (z(0), ˆz(0), c); buffer levels τ1:B; noise schedule (αt, σt) Output: Fine-tuned parameters θ foreach minibatch (z(0), ˆz(0), c) do foreach sample in minibatch do Sample U{1, . . . , } and ε (0, I); z(τ1:B (t)) q(cid:0)z(0), τ1:B(t)(cid:1); ˆz(t) αtˆz(0) + σtε; z(t) z(0)z(τ1:B (t))ˆz(t); ˆε ϵθ(z(t), t, c); (cid:80)L+B+K 1 i=L+B+1 εi ˆεi2 2; Update θ using gradients of L; 3.5 Training For each videotext pair (cid:0)z(0), ˆz(0), c(cid:1) D, the training proceeds as follows. First, we randomly sample global timestep U{1, . . . , } and Gaussian noise ε (0, I). Next, we construct the noised model input sequence z(t) with the noise level defined in Eq. 7. The model then predicts the noise ˆε = ϵθ(z(t), t, c) for all frames. However, the loss is computed only over the target frames to avoid enforcing supervision for the buffer frames. Specifically, we minimize the mean squared error between the true noise and the predicted noise over the target (cid:13) (cid:13) 2 frame indices, defined as = 1 (cid:13)εi ˆεi 2. The model parameters θ are updated via (cid:13) gradient step computed from this loss. By excluding the buffer frames from the loss calculation, the network is free to predict whatever is most natural for these frames, thereby preventing spurious gradients that could shift the model away from the pretraining distribution. In practice, we observe that the buffer frames often evolve into smooth fade-out and fade-in transition between the condition and target frames. The full training procedure is summarized in Algorithm 2. (cid:80)L+B+K i=L+B+"
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Overview We evaluate our proposed method on two recent large-scale text-to-video generation models: CogVideoX-5B and Wan-14B. Our experiments span diverse range of conditional generation tasks, including: Image-to-Video (I2V): e.g., character-to-video generation, object-to-motion, virtual try-on, ad-video generation. Video-to-Video (V2V): e.g., video style transfer, action transfer, toonification. key strength of TIC-FT is its ability to operate in the few-shot regime. We fine-tune models with as few as 1030 training samples and fewer than 1,000 training stepsrequiring less than one hour of training time for CogVideoX-5B on single A100 GPU. We use both real and synthetic datasets for evaluation and demonstrations. Real datasets include SSv2 [30] and manually curated paired videos, while synthetic datasets are created using models such as GPT-4o image generation [31] and Sora [32] (e.g., translating real images into stylized videos). Each task is provided with 20 conditiontarget pairs. Additional details are provided in the Appendix. We compare TIC-FT with three representative fine-tuning methods for conditional video generation. While CogVideoX-5B and Wan-14B are among the most recent and powerful text-to-video diffusion models, most existing editing or fine-tuning approaches have not been evaluated on such largescale backbones. To ensure meaningful comparisons, we reimplement the following representative baselines. 6 Figure 2: (a) Zero-shot comparison of our method (last row) with variants: without buffer frames and with an SDEdit-style inpainting strategy (Replace, second row). Buffer frames enable smoother transitions and better condition preservation. (b) Corresponding results after fine-tuning. ControlNet [7, 9]. We include ControlNet as baseline because large number of recent methods are built upon it or extend its core architecture [16, 17, 14]. It is widely adopted framework that introduces an external reference network and zero-convolution layers to inject conditioning signals, enabling the model to preserve fine-grained visual details while integrating external guidance. Fun-pose[10]. simple yet widely adopted strategy is to concatenate the condition and target latents, as seen in many recent methods[33, 34, 35]. However, this approach requires architectural modifications and extensive retraining, which is infeasible in low-data regimes (e.g., 20 samples). Since training such model from scratch yields extremely poor results, direct comparison would be uninformative. Instead, we adopt Fun-posea variant of CogVideoX and Wan that has already been finetuned to accept reference videoseffectively giving it significant advantage. SIC-FT-Replace[8, 24]. This method performs spatial in-context fine-tuning by training the model to predict videos arranged as spatial grids. At inference time, the ground-truth condition is noised and repeatedly injected into the condition grid slot at each denoising step, following an SDEditstyle replacement strategy[25], while the remaining grid elements are progressively denoised. This approach represents recent trend in applying in-context fine-tuning techniques to diffusion models. 4.2 Results We conduct quantitative evaluations using CogVideoX-5B as the base model, focusing on two I2V tasksobject-to-motion and character-to-videoas shown in Table 1. For V2V, we evaluate performance on style transfer task (real videos to animation), summarized in Table 2. All models are fine-tuned using LoRA (rank 128) with 20 training samples over 6,000 steps, batch size of 2, and single NVIDIA H100 80GB GPU. Inference is conducted with 50 denoising steps. To assess video quality comprehensively, we use three categories of evaluation metrics: VBench [36], GPT-4o [31], and Perceptual similarity scores. VBench provides human-aligned assessments of temporal and spatial coherence, including subject consistency, background stability, and motion smoothness. GPT-4o leverages multimodal large language model to rate aesthetic quality, structural fidelity, and semantic alignment with the prompt. Perceptual metrics quantify lowand high-level visual similarity between condition and target frames, including CLIP-I and CLIP-T (for image/text alignment), LPIPS and SSIM (for perceptual similarity), and DINO (for structural consistency). However, we omit Perceptual metrics when evaluating tasks like object-to-motion, where different viewpoints may reduce similarity scores despite correct semantics. Our model achieves strong performance even with limited training, showing competitive results after only 2,000 training stepsunlike other baselines that require significantly more optimization to reach Table 1: Comparison on VBench, GPT-4o, and perceptual similarity metrics for I2V tasks. Method VBench GPT-4o Perceptual similarity subject consistency background consistency motion smoothness aesthetic structural similarity quality semantic similarity CLIP-I CLIP-T LPIPS SSIM DINO ControlNet [7, 9] Fun-pose [10] SIC-FT-Replace [8, 24] TIC-FT-Replace TIC-FT (w/o Buffer) TIC-FT (2K) TIC-FT (6K) 0.9658 0.9508 0.9513 0.9580 0.9474 0.9505 0.9672 0.9600 0.9598 0.9676 0.9702 0.9686 0.9696 0.9729 0.9926 0.9910 0. 0.9926 0.9892 0.9920 0.9930 3.87 4.09 4.10 4.08 4.05 4.03 4.13 2.69 2.87 2.42 2.00 3.05 3.08 3.14 2.69 3.21 2. 2.48 3.53 3.54 3.63 0.7349 0.7714 0.7993 0.7925 0.7573 0.8066 0.8329 0.2903 0.3099 0.3064 0.3127 0.2986 0.3135 0.3143 0.6535 0.6339 0. 0.6165 0.6242 0.6162 0.4332 0.3477 0.3427 0.3575 0.3866 0.4455 0.4246 0.4123 0.4221 0.4058 0.4160 0.4203 0.4240 0.5917 0.5530 Table 2: Comparison on VBench, GPT-4o, and perceptual similarity metrics for V2V tasks. Method VBench GPT-4o Perceptual similarity subject consistency background consistency motion smoothness aesthetic structural similarity quality semantic similarity CLIP-I CLIP-T LPIPS SSIM DINO ControlNet [7, 9] Fun-pose [10] SIC-FT-Replace [8, 24] TIC-FT-Replace TIC-FT (w/o Buffer) TIC-FT (2K) TIC-FT (6k) 0.9553 0.9679 0.9609 0.9584 0.9479 0.9439 0. 0.9545 0.9675 0.9655 0.9696 0.9571 0.9600 0.9743 0.9854 0.9902 0.9853 0.9802 0.9744 0.9865 0.9935 3.44 4.24 3.99 3.93 3.81 3.85 3. 2.23 2.68 2.44 2.33 2.66 3.67 3.90 2.41 3.23 2.94 2.92 3.20 4.37 4.41 0.6221 0.7260 0.7368 0.7305 0.7471 0.8174 0. 0.2727 0.3018 0.3198 0.3015 0.3020 0.3132 0.3118 0.5434 0.5179 0.5998 0.6373 0.4687 0.2970 0.2251 0.3494 0.2839 0.3328 0.4369 0.2192 0.4025 0.2526 0.3673 0.3800 0.4429 0.5546 0.6089 0.6541 0. similar quality. Additional comparisons under this low-data, low-compute regime are presented in the Appendix. Despite being conditioned on reference frames, Fun-pose and ControlNet exhibit poor condition fidelity. While their outputs appear visually plausibleas indicated by favorable VBench and GPT-4o scoresthey consistently underperform in Perceptual similarity metrics, highlighting lack of alignment with the conditioning input. This is especially problematic for ControlNet, which relies on strict spatial alignment and thus struggles in tasks such as character-to-video and object-tomotion, where viewpoint shifts are common. SIC-FT-Replace[8, 24] also performs suboptimally in I2V settings, as it requires replicating single frame across spatial gridleading to high memory usage and inefficient training. Furthermore, its reliance on SDEdit [25]-style sampling during inference degrades generation quality and weakens condition adherence. We supplement quantitative results with qualitative comparisons across I2V and V2V tasks in Figure 3. We also present additional scenariosincluding virtual try-on, ad-video generation, and action transferare illustrated in Figures 1 and 4. Overall, our proposed TIC-FT consistently outperforms prior methods across diverse tasks, with both quantitative metrics and qualitative examples supporting its superior condition alignment and generation quality. More results and task-specific details are provided in the Appendix. 4.3 Ablation study We validate the effectiveness of our temporal concatenation design with buffer frames by assessing its zero-shot performance. If the model successfully leverages the pretrained capabilities of video diffusion models, it should generate plausible outputs even without any additional training. As shown in Figure 2(a), our method with buffer frames (last row) generates target frames that align well with the given conditiondemonstrating strong zero-shot performance. In contrast, removing the buffer frames leads to abrupt noise-level discontinuities between condition and target regions, causing the target frames to degrade and the condition information to be poorly preserved. We also compare with zero-shot inpainting methods similar to SDEdit, denoted as Replace (second row), which similarly fails to propagate condition signals into the generated frames. Furthermore, in Figure 2(b), we observe that strong zero-shot performance correlates with better results after fine-tuning. Our method with buffer frames consistently outperforms other variants: models trained without buffer frames begin with blurry target frames, and the Replace strategy fails to apply condition information effectively even after training. 8 Figure 3: Qualitative comparison between our method and baseline approaches. Figure 4: Demonstration of our method on character-to-video, action transfer, and virtual try-on."
        },
        {
            "title": "5 Conclusion and Limitation",
            "content": "Temporal In-Context Fine-Tuning (TIC-FT) temporally concatenates condition and target frames with intermediate buffer frames to better align with the pretrained model distribution. TIC-FT enables unified framework for diverse conditional video generation tasks and consistently outperforms existing methods in our experiments. While efficient, the method is currently limited to condition inputs shorter than 10 seconds due to memory constraintsan important direction for future work."
        },
        {
            "title": "References",
            "content": "[1] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. [2] Yoav HaCohen, Nisan Chiprut, Benny Brazowski, Daniel Shalem, Dudu Moshe, Eitan Richardson, Eran Levin, Guy Shiran, Nir Zabari, Ori Gordon, Poriya Panet, Sapir Weissbuch, Victor Kulikov, Yaki Bitterman, Zeev Melumian, and Ofir Bibi. Ltx-video: Realtime video latent diffusion. arXiv preprint arXiv:2501.00103, 2024. [3] Genmo Team. Mochi 1. https://github.com/genmoai/models, 2024. [4] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. [5] Xingwu Sun, Yanfeng Chen, Yiqing Huang, Ruobing Xie, Jiaqi Zhu, Kai Zhang, Shuaipeng Li, Zhen Yang, Jonny Han, Xiaobo Shu, et al. Hunyuan-large: An open-source moe model with 52 billion activated parameters by tencent. arXiv preprint arXiv:2411.02265, 2024. [6] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. [7] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF international conference on computer vision, pages 38363847, 2023. [8] Lianghua Huang, Wei Wang, Zhi-Fan Wu, Yupeng Shi, Huanzhang Dou, Chen Liang, Yutong Feng, Yu Liu, and Jingren Zhou. In-context lora for diffusion transformers. arXiv preprint arXiv:2410.23775, 2024. [9] Zhihao Hu and Dong Xu. Videocontrolnet: motion-guided video-to-video translation framework by using diffusion model with controlnet. arXiv preprint arXiv:2307.14073, 2023. [10] aigc apps. VideoX-Fun: unified pipeline for video generation and editing. https://github.com/ aigc-apps/VideoX-Fun, apr 2025. GitHub repository, commit <hash>. [11] Shiwei Zhang, Jiayu Wang, Yingya Zhang, Kang Zhao, Hangjie Yuan, Zhiwu Qin, Xiang Wang, Deli Zhao, and Jingren Zhou. I2vgen-xl: High-quality image-to-video synthesis via cascaded diffusion models. arXiv preprint arXiv:2311.04145, 2023. [12] David Junhao Zhang, Dongxu Li, Hung Le, Mike Zheng Shou, Caiming Xiong, and Doyen Sahoo. Moonshot: Towards controllable video generation and editing with multimodal conditions. arXiv preprint arXiv:2401.01827, 2024. [13] Xiang Wang, Hangjie Yuan, Shiwei Zhang, Dayou Chen, Jiuniu Wang, Yingya Zhang, Yujun Shen, Deli Zhao, and Jingren Zhou. Videocomposer: Compositional video synthesis with motion controllability. Advances in Neural Information Processing Systems, 36:75947611, 2023. [14] Han Lin, Jaemin Cho, Abhay Zala, and Mohit Bansal. Ctrl-adapter: An efficient and versatile framework for adapting diverse controls to any diffusion model. arXiv preprint arXiv:2404.09967, 2024. [15] Zhendong Wang, Yifan Jiang, Yadong Lu, Pengcheng He, Weizhu Chen, Zhangyang Wang, Mingyuan Zhou, et al. In-context learning unlocked for diffusion models. Advances in Neural Information Processing Systems, 36:85428562, 2023. [16] Chong Mou, Mingdeng Cao, Xintao Wang, Zhaoyang Zhang, Ying Shan, and Jian Zhang. Revideo: Remake video with motion and content control. Advances in Neural Information Processing Systems, 37: 1848118505, 2024. 10 [17] Cong Wang, Jiaxi Gu, Panwen Hu, Haoyu Zhao, Yuanfan Guo, Jianhua Han, Hang Xu, and Xiaodan Liang. Easycontrol: Transfer controlnet to video diffusion for controllable generation and interpolation. arXiv preprint arXiv:2408.13005, 2024. [18] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ip-adapter: Text compatible image prompt adapter for text-to-image diffusion models. arXiv preprint arXiv:2308.06721, 2023. [19] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. [20] Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Jingyuan Ma, Rui Li, Heming Xia, Jingjing Xu, Zhiyong Wu, Tianyu Liu, et al. survey on in-context learning. arXiv preprint arXiv:2301.00234, 2022. [21] Wentao Zhang, Junliang Guo, Tianyu He, Li Zhao, Linli Xu, and Jiang Bian. Video in-context learning: Autoregressive transformers are zero-shot video imitators. In The Thirteenth International Conference on Learning Representations, 2025. [22] Wentao Zhang, Junliang Guo, Tianyu He, Li Zhao, Linli Xu, and Jiang Bian. Video in-context learning. arXiv preprint arXiv:2407.07356, 2024. [23] Jianzhi Liu, Junchen Zhu, Lianli Gao, Heng Tao Shen, and Jingkuan Song. Aicl: Action in-context learning for video diffusion model. arXiv preprint arXiv:2403.11535, 2024. [24] Zhengcong Fei, Di Qiu, Debang Li, Changqian Yu, and Mingyuan Fan. Video diffusion transformers are in-context learners. arXiv preprint arXiv:2412.10783, 2024. [25] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided image synthesis and editing with stochastic differential equations. arXiv preprint arXiv:2108.01073, 2021. [26] Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher Yu, Radu Timofte, and Luc Van Gool. Repaint: Inpainting using denoising diffusion probabilistic models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1146111471, 2022. [27] Jihwan Kim, Junoh Kang, Jinyoung Choi, and Bohyung Han. Fifo-diffusion: Generating infinite videos from text without training. arXiv preprint arXiv:2405.11473, 2024. [28] Boyuan Chen, Diego Martí Monsó, Yilun Du, Max Simchowitz, Russ Tedrake, and Vincent Sitzmann. Diffusion forcing: Next-token prediction meets full-sequence diffusion. Advances in Neural Information Processing Systems, 37:2408124125, 2024. [29] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 41954205, 2023. [30] Raghav Goyal, Samira Ebrahimi Kahou, Vincent Michalski, Joanna Materzynska, Susanne Westphal, Heuna Kim, Valentin Haenel, Ingo Fruend, Peter Yianilos, Moritz Mueller-Freitag, et al. The\" something something\" video database for learning and evaluating visual common sense. In Proceedings of the IEEE international conference on computer vision, pages 58425850, 2017. [31] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. [32] Yixin Liu, Kai Zhang, Yuan Li, Zhiling Yan, Chujie Gao, Ruoxi Chen, Zhengqing Yuan, Yue Huang, Hanchi Sun, Jianfeng Gao, et al. Sora: review on background, technology, limitations, and opportunities of large vision models. arXiv preprint arXiv:2402.17177, 2024. [33] Yan Zeng, Guoqiang Wei, Jiani Zheng, Jiaxin Zou, Yang Wei, Yuchen Zhang, and Hang Li. Make pixels dance: High-dynamic video generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 88508860, 2024. [34] Dianbing Xi, Jiepeng Wang, Yuanzhi Liang, Xi Qiu, Yuchi Huo, Rui Wang, Chi Zhang, and Xuelong Li. Omnivdiff: Omni controllable video diffusion for generation and understanding. arXiv preprint arXiv:2504.10825, 2025. [35] Yong Zhong, Zhuoyi Yang, Jiayan Teng, Xiaotao Gu, and Chongxuan Li. Concat-id: Towards universal identity-preserving video synthesis. arXiv preprint arXiv:2503.14151, 2025. [36] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2180721818, 2024. [37] Rasmus Jensen, Anders Dahl, George Vogiatzis, Engin Tola, and Henrik Aanæs. Large scale multi-view stereopsis evaluation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 406413, 2014. [38] Black Forest Labs. Flux. https://github.com/black-forest-labs/flux, 2024."
        },
        {
            "title": "A Technical Appendices and Supplementary Material",
            "content": "A.1 Training Details All models are fine-tuned using an NVIDIA H100 GPU. Our method builds on the CogVideoX5B backbone and is fine-tuned with LoRA (rank 128), resulting in approximately 130M trainable parameters. Training with 49 frames requires roughly 30GB of GPU memory. For ControlNet, we apply LoRA with the same rank, yielding comparable parameter count of around 150M, and requiring approximately 60GB of GPU memory. For Fun-pose, we use the official full fine-tuning setup, which consumes around 75GB of GPU memory. A.2 Training Amount vs. Performance This section demonstrates the training efficiency of our method compared to ControlNet. Figure 5 presents performance curves for various metricsincluding CLIP-T, CLIP-I, SSIM, DINO, and LPIPSplotted against training time. Our method consistently outperforms ControlNet across all metrics at equivalent training durations. Moreover, with the exception of CLIP-T, all metrics show clear upward trend, indicating continued improvement with more training. In contrast, ControlNet exhibits no such trend, suggesting that its training style tends to overfit and struggles to generalize under limited data regimes. A.3 Ablation Study We conduct ablation study on various buffer frame designs. Specifically, we compare our default settingusing uniformly increasing noise schedulewith alternative strategies: (1) constant noise level for all buffer frames (denoted as Constant-t, where = 100), and (2) linear-quadratic schedules with concave or convex profiles. Figure 6 presents both zero-shot and fine-tuned results for these configurations. While all variants produce reasonable target frames, we observe that the convex schedule and the constant-25 baseline exhibit poor condition alignment and noticeable artifacts in the zero-shot setting. After fine-tuning, all methods perform comparably, though our default setting with uniformly increasing noise remains preferred. Quantitative results after training are presented in Table 3 and Table 4 for the I2V and V2V tasks, respectively. We also evaluate the effect of varying the number of buffer frames, ranging from 1 to 5, denoted as Buffer-n in Figure 7. In the zero-shot setting, we observe that all configurations perform comparably overall; however, shorter buffers tend to produce noisier transitions, likely due to abrupt scene changes. Conversely, longer buffers show tendency to weaken the influence of the condition. After fine-tuning, all variants produce similarly high-quality results. A.4 Dataset For the object-to-motion task, we use the DTU dataset [37]. For character-to-video, keyframe interpolation, and ad video generation tasks, we manually collected conditionvideo pairs tailored to each task. For action transfer, we curate videos from SSv2 [30]. In the video style transfer task, we first synthesize starting frames using FLUX.1-dev [38], and then generate paired videos using SoRA [32] and Wan2.1 [6]. Each task is trained on 30 samples. All videos contain 49 frames at 10 frames per second (fps), resized to either 480480 or 848480 while preserving the original aspect ratio. For evaluation and demonstration, we use image and video conditions that are not part of the training set. These include both manually collected images and synthesized ones generated using GPT4o, FLUX, and Sora. For the action transfer task, we use unseen video samples from SSv2 [30]. Quantitative evaluations are conducted on 100 samples. For image-based metrics such as CLIP and LPIPS, scores are computed on per-frame basis and then averaged to obtain the final results. Training and evaluation prompts are generated using GPT-4o. Each prompt is structured to encompass the condition, buffer, and target frames, with condition and buffer frames denoted as [CONDITION] and target frames as [VIDEO]. Below is the full prompt used for the sample in the ablation study: TIC-FT prompt This animated clip demonstrates the transformation of static character illustration into lively and expressive animated figure; [CONDITION] the condition image showcases cheerful cartoon-style young buffalo with thick brown fur, curved yellow horns, and big, friendly smile. The characters wide eyes and upright posture are set against warm orange background, giving it lively and playful presence. [VIDEO] the video animates the buffalo inside grand museum, where it wears red t-shirt and points excitedly at large dinosaur skeleton behind glass. Its eyes are wide with curiosity and its mouth open in awe, while elegant stone columns and soft lighting emphasize the sense of wonder and fascination with history. A.5 Task Descriptions We detail the construction of data and latent sequences for each conditional video generation task used in our experiments. All tasks are configured with total of 13 latent frames, corresponding to 49 video frames. While this number can be adjusted based on application needs, we adopt the 13-frame setting throughout for implementation simplicity and consistency. The initial latent sequence comprises condition frames, intermediate buffer frames, and noised target frames. An exception is the action transfer task, where buffer frames are omitted, as the last condition frame serves as the starting frame of the target sequence. The specific configurations for each task are described below. Image-to-Video This task aims to generate full video conditioned on single image. The video need not begin directly from the images visual content; instead, the image may represent high-level concept such as character profile or an object viewed from the top, with the video depicting novel dynamics (e.g., rotating 360 view). single reference image is replicated to occupy the first 4 latent frames, followed by 9 target frames. Clean condition: 1 frame (from the image) Buffer: 3 frames (noised condition) Target: 9 frames (pure noise) We visualize the initial latent frames and their denoising process in Figure 8. Figure 5: Performance curves for CLIP-T, CLIP-I, SSIM, DINO, and LPIPS metrics plotted against training time. Our method consistently outperforms ControlNet across all metrics at equivalent training durations. Table 3: Ablation study of constant noise scheduling for buffer frames, evaluated on I2V tasks using VBench, GPT-4o, and perceptual/similarity metrics. VBench Perceptual similarity GPT-4o Method subject consistency background consistency motion smoothness aesthetic structural similarity quality semantic similarity CLIP-I CLIP-T LPIPS SSIM DINO Ours Constant-25 Constant-50 Constant-75 0.9672 0.9516 0.9509 0. 0.9729 0.9724 0.9740 0.9722 0.9930 0.9920 0.9915 0.9917 4.13 4.09 4.05 4.02 3.14 2.81 3.01 3.07 3.63 3.45 3.51 3.68 0.8329 0.7734 0.7760 0. 0.3143 0.3062 0.3010 0.3003 0.4332 0.6088 0.6157 0.6148 0.5917 0.5530 0.4240 0.4202 0.4188 0.4228 0.4250 0.4259 Table 4: Ablation study of constant noise scheduling for buffer frames, evaluated on V2V tasks using VBench, GPT-4o, and perceptual/similarity metrics. VBench Perceptual similarity GPT-4o Method subject consistency background consistency motion smoothness aesthetic structural similarity quality semantic similarity CLIP-I CLIP-T LPIPS SSIM DINO Ours Constant-25 Constant-50 Constant-75 0.9736 0.9539 0.9524 0.9327 0.9743 0.9652 0.9652 0.9552 0.9935 0.9873 0.9886 0.9821 3.99 3.90 3.88 3. 3.90 3.55 3.86 3.60 4.41 4.20 4.31 4.25 0.8794 0.8037 0.8460 0.8330 0.3080 0.3103 0.3153 0.3142 0.2298 0.2744 0.2364 0.2797 0.6541 0.6596 0.5785 0.6083 0.6039 0.6528 0.5707 0. Video Style Transfer This video-to-video task transforms the visual style of source video into that of target domain (e.g., converting realistic video into an animated version) while preserving motion and structure. The first 7 latent frames are taken from source video and the remaining 6 from style-transferred version. Clean condition: 4 frames (from the source video) Buffer: 3 frames (noised condition) Target: 6 frames (pure noise) We visualize the initial latent frames and their denoising process in Figure 9. In-Context Action Transfer This task generates video that continues novel scene using motion inferred from source video. Given reference action and the first frame of new environment, the model synthesizes future frames that imitate the observed motion within the new context. The first 6 latent frames are from reference action video, the 7th is the first frame of novel scene, and the rest are the continuation. Clean condition: 6 frames (from the reference action video) Query frame: 1 clean frame (from the novel scene) Target: 6 frames (pure noise) No buffer frames are used in this task, as the first frame of the target video is explicitly provided as part of the condition. We visualize the initial latent frames and their denoising process in Figure 10. Keyframe Interpolation This task fills in intermediate frames between sparse keyframes to produce temporally coherent video. The goal is to ensure smooth transitions between given keyframes. Four keyframes are replicated to fill the first 7 latent frames, and the remaining 6 are interpolated. Clean condition: 4 frames (replicated keyframes) Buffer: 3 frames (noised condition) Target: 6 frames (pure noise) We visualize the initial latent frames and their denoising process in Figure 11. Multiple Image Conditions This task takes two distinct types of image conditionssuch as person and clothing, or person and an objectand generates target video that reflects the combination of 15 both. This setup is useful for applications like virtual try-on (VITON) or ad video synthesis, where two semantic entities must be jointly represented in motion. The first 3 latent frames are derived from the first condition image, and the next 4 from the second condition image. Clean condition: 4 frames (3 from the first image, 1 from the second) Buffer: 3 frames (noised condition) Target: 6 frames (pure noise) Note that the number of condition sources is not limited to two; the framework supports arbitrary multi-condition setups. We visualize the initial latent frames and their denoising process in Figure 12. A.6 Broader Impacts and Misuse Discussion Our TIC-FT method enables efficient adaptation of video diffusion models with minimal data. However, this ease of fine-tuning also introduces risks, particularly the potential misuse for creating deepfakes or misleading synthetic media. Clear usage policies and responsible deployment practices are essential to mitigate societal risks. 16 Figure 6: Qualitative comparison of buffer frame designs in zero-shot and fine-tuned settings. 17 Figure 7: Qualitative comparison of buffer frame designs in zero-shot and fine-tuned settings. 18 Figure 8: Visual results for initial frames and their denoising process on image-to-video generation. Prompt: [Character] clear, high-resolution front-facing close-up of cheerful cartoon-style wolf character, centered against ... Figure 9: Visual results for initial frames and their denoising process on video style transfer task. Prompt: [VIDEO1] woman in tan cloak walks gracefully along forest path. Her hair flows gently with her movement, and the ... 19 Figure 10: Visual results for initial frames and their denoising process on in-context action transfer task. Prompt: [REFERENCE VIDEO] white paper is folded in half by person wearing black sleeves in dark indoor environment. ... Figure 11: Visual results for initial frames and their denoising process on keyframe interpolation task. Prompt: [VIDEO1] cartoon woman with red hair and jeweled headpiece slowly tilts her head and changes facial expressions ... 20 Figure 12: Visual results for initial frames and their denoising process on virtual try-on task. Prompt: [IMAGE] young woman with long black hair, wearing cream blouse, blue jeans, and black sandals, smiles with both ..."
        }
    ],
    "affiliations": [
        "KAIST AI"
    ]
}