{
    "paper_title": "StdGEN: Semantic-Decomposed 3D Character Generation from Single Images",
    "authors": [
        "Yuze He",
        "Yanning Zhou",
        "Wang Zhao",
        "Zhongkai Wu",
        "Kaiwen Xiao",
        "Wei Yang",
        "Yong-Jin Liu",
        "Xiao Han"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present StdGEN, an innovative pipeline for generating semantically decomposed high-quality 3D characters from single images, enabling broad applications in virtual reality, gaming, and filmmaking, etc. Unlike previous methods which struggle with limited decomposability, unsatisfactory quality, and long optimization times, StdGEN features decomposability, effectiveness and efficiency; i.e., it generates intricately detailed 3D characters with separated semantic components such as the body, clothes, and hair, in three minutes. At the core of StdGEN is our proposed Semantic-aware Large Reconstruction Model (S-LRM), a transformer-based generalizable model that jointly reconstructs geometry, color and semantics from multi-view images in a feed-forward manner. A differentiable multi-layer semantic surface extraction scheme is introduced to acquire meshes from hybrid implicit fields reconstructed by our S-LRM. Additionally, a specialized efficient multi-view diffusion model and an iterative multi-layer surface refinement module are integrated into the pipeline to facilitate high-quality, decomposable 3D character generation. Extensive experiments demonstrate our state-of-the-art performance in 3D anime character generation, surpassing existing baselines by a significant margin in geometry, texture and decomposability. StdGEN offers ready-to-use semantic-decomposed 3D characters and enables flexible customization for a wide range of applications. Project page: https://stdgen.github.io"
        },
        {
            "title": "Start",
            "content": "4 2 0 2 8 ] . [ 1 8 3 7 5 0 . 1 1 4 2 : r StdGEN: Semantic-Decomposed 3D Character Generation from Single Images Yuze He1,2 Yanning Zhou1* Wang Zhao2 Zhongkai Wu1,3 Kaiwen Xiao1 Wei Yang1 Yong-Jin Liu2* Xiao Han1 1 Tencent AI Lab 2 Tsinghua University 3 Beihang University Figure 1. Our StdGEN generates high-quality, decomposed 3D characters from single reference image."
        },
        {
            "title": "Abstract",
            "content": "We present StdGEN, an innovative pipeline for generating semantically decomposed high-quality 3D characters from single images, enabling broad applications in virtual reality, gaming, and filmmaking, etc. Unlike previous methods which struggle with limited decomposability, unsatisfactory quality, and long optimization times, StdGEN features decomposability, effectiveness and efficiency; i.e., it generates intricately detailed 3D characters with separated semantic components such as the body, clothes, and hair, in three minutes. At the core of StdGEN is our proposed Semantic-aware Large Reconstruction Model (S-LRM), transformer-based generalizable model that jointly reconstructs geometry, color and semantics from multi-view images in feed-forward manner. differentiable multilayer semantic surface extraction scheme is introduced to *Corresponding Authors: amandayzhou@tencent.com, liuyongjin@tsinghua.edu.cn Work done during an internship at Tencent AI Lab. acquire meshes from hybrid implicit fields reconstructed by our S-LRM. Additionally, specialized efficient multiview diffusion model and an iterative multi-layer surface refinement module are integrated into the pipeline to facilitate high-quality, decomposable 3D character generation. Extensive experiments demonstrate our state-of-theart performance in 3D anime character generation, surpassing existing baselines by significant margin in geometry, texture and decomposability. StdGEN offers ready-touse semantic-decomposed 3D characters and enables flexible customization for wide range of applications. Project page: https://stdgen.github.io 1. Introduction Generating high-quality 3D characters from single images has widespread applications in virtual reality, video games, filmmaking, etc. Beyond automatically creating complete 3D character, there is an increasing demand for the ability to produce decomposable characters, where distinct seman1 tic components like the body, clothes, and hair are disentangled. This decomposition allows for much easier editing, control, and animation of characters, greatly enhancing their usability across various downstream applications. However, creating such decomposable characters from single images is challenging, as each component may face issues such as occlusion, ambiguity, and inconsistencies in their interactions with other components. Existing methods for decomposable avatar generation primarily focus on realistic clothed human models, exploring disentangled 3D parametric [58], explicit [38, 39], or implicit [11, 16, 21, 61] representations alongside various optimization techniques. These optimization approaches often employ score distillation loss [41] to leverage 2D generative priors, which leads to prolonged optimization times and the generation of coarse, high-contrast textures. Additionally, the dependence on parametric human models, such as SMPL-X [33], is inadequate for virtual characters, which often exhibit exaggerated body proportions and complex clothing designs. To address these limitations, CharacterGen [40] was developed to efficiently generate characters from single images using multi-view diffusion model and large reconstruction model [17]. Despite showing impressive generation capabilities in various posed images, CharacterGen can only produce composed avatars in watertight meshes with no decomposability. These meshes require significant manual labor to separate, edit, or animate, limiting their applicability. Moreover, the quality of the generated meshes is often unsatisfactory, particularly in finer details such as the characters face and clothing, as shown in Fig. 5. Therefore, efficiently generating high-quality, decomposable 3D characters remains an open challenge. In this work, we propose StdGEN, an efficient and effective pipeline to generate decomposable, high-quality, Apose 3D characters from single images of any pose. At its core is novel Semantic-aware Large Reconstruction Model (S-LRM), which innovatively introduces semantic attributes to the original Large Reconstruction Model [17], enabling efficient reconstruction of unified geometry, color and semantic fields. Moreover, differentiable multi-layer semantic surface extraction scheme is proposed to extract decomposed semantic 3D surfaces from reconstructed implicit fields, empowering joint end-to-end training with explicit meshes. Together, S-LRM can efficiently generate semantic-decomposed, complete and consistent 3D surfaces from multi-view input images in feed-forward manner. Built upon S-LRM, StdGEN comprises three key stages: multi-view diffusion, feed-forward reconstruction, and mesh refinement, each designed with technical innovations to enable effective decomposition. Specifically, given single reference image with an arbitrary pose, specialized efficient multi-view diffusion model first generates multiple A-pose RGB images and normal maps at different camera views. Next, the generated images are processed by our novel S-LRM, to reconstruct geometry, color, and semantics in feed-forward manner. Once the coarse decomposed surface generation is obtained, we further refine the mesh quality through proposed iterative multi-layer mesh refinement method, using diffusion-generated 2D images and normal maps as guidance. As result, StdGEN can generate high-quality decomposable 3D characters from single reference image within minutes. To facilitate the semantic decomposed training and testing, we further developed Anime3D++ dataset based on [40] , focusing on anime characters due to the wide internet presence, with finely annotated multi-view multi-pose semantic parts. Our experiments demonstrate that StdGEN surpasses all existing baselines, achieving state-of-the-art performance in both arbitrary and A-pose 3D character generation. Moreover, StdGENs decomposable generation capability enables flexible customization, which can benefit numerous downstream applications. In summary, this work makes the following contributions: We introduce novel Semantic-aware Large Reconstruction Model (S-LRM), which jointly models geometry, color, and semantic information with efficient tri-plane feed-forward inference. An effective differentiable surface extraction scheme is proposed to facilitate the explicit multi-layer semantic surface reconstruction. Building upon (1) our S-LRM, (2) an efficient multi-view diffusion model and (3) iterative multi-layer mesh refinement, we present StdGEN, an efficient pipeline for highquality decomposable 3D character generation, which for the first time enables semantic-decomposed 3D character creation from single images with arbitrary poses in just few minutes. Our method demonstrates superior performance over existing baselines, with its decomposed modeling unlocking potential for various downstream applications. 2. Related Works 2.1. 3D Generation To circumvent the need for extensive 3D assets during training, several approaches suggest lifting powerful 2D pre-trained diffusion models [10, 36, 47, 48] for 3D generation. The earliest works [41, 57] incorporate pretrained 2D diffusion model for probability density distillation using Score Distillation Sampling (SDS). These approaches gradually optimize randomly initialized radiance field [1, 4, 54] with volume rendering, making it timeconsuming to generate an object. Later research continues to enhance the aesthetics and accuracy of 3D content generation [6, 28, 50, 56, 63] and further investigate different application scenarios [13, 45, 49, 53]. However, relying solely on 2D priors for 3D generation often leads to 2 Figure 2. The overview of our StdGEN pipeline. Starting from single reference image, our method utilizes diffusion models to generate multi-view RGB and normal maps, followed by S-LRM to obtain the color/density and semantic field for 3D reconstruction. Semantic decomposition and part-wise refinement are then applied to produce the final result. poor geometry representation, e.g., multi-faced Janus problem, due to the challenges in controlling precise viewpoints through text prompts. The large-scale 3D datasets, e.g. Objaverse [9], unlock the possibility of imposing 3D priors to the model. Several works utilize view-consistent images to fine-tune the diffusion model. Zero-1-to-3 [29] integrates 3D priors into 2D stable diffusion by fine-tuning the pre-trained model for novel view synthesis (NVS). To further enhance the multi-view consistency, several recent works [23, 30, 32, 52] propose synchronously generating multi-view images in single generation process and achieving constraints in 3D place through feature interaction in attention mechanism. Besides, the 3D native generation method shows powerful geometric generation ability [27, 34, 71]. However, the ability of these methods to follow instructions is typically moderate; therefore, they face challenges in achieving the desired outcomes in scenarios requiring precise restoration of reference images, e.g., 3D character generation. 2.2. Large Reconstruction Model Large Reconstruction Model (LRM) [17] leverages the transformer-based model to map the single image feature to implicit tri-plane representation. Instant3D [25] extends LRM by feeding multi-view images instead of single image. LGM [55], GRM [67] and GS-LRM [70] replace the 3D representation to 3D Gaussians, embracing its efficiency in rendering and low memory consumption. InstantMesh [66] and CRM [64] explicitly model the geometry by equipping the generative pipeline with FlexiCubes [51], achieving high-quality surface extraction and high rendering speed. The following works further explored applying advanced model architecture [68] or 3D presentation [5, 8], aiming to improve the efficiency, realism, and generalization of reconstruction. Integrating with multi-view diffusion models, these LRMs can achieve textto-3D generation or single image-to-3D generation. Yet all these methods typically produce holistic models. In contrast, our method generates semantically decomposed characters, making downstream processing such as editing and animation much more efficient. 2.3. 3D Character Generation 3D character generation is challenging problem due to its high precision requirements and the scarcity of data. One line of work leverages 3D-aware GANs to model the distribution of digital humans [2, 15, 24, 37, 69]. Recently the SDS-based methods have shown the possibility of generating variety of stylized characters [3, 12, 22, 59], yet it suffers from the long optimization times and the difficulty of meticulous style control. The most related work to us is CharaterGen [40], which calibrates input poses to canonical multi-view images via an image-conditioned multi-view diffusion model, followed by LRM for 3D character reconstruction and multi-view texture back projection; however, it still exhibits limited geometry and texture quality. Furthermore, the methods above treat the parts of the character as whole, requiring extra manual labor to process. Our approach, in contrast, employs semantic-aware, feedforward paradigm that generates high-quality, decomposable characters using only one forward pass, providing significant efficiency and quality improvement. 3. Method The proposed StdGEN begins with multi-view canonical character generation (Sec. 3.1) from reference image. To reconstruct decomposable 3D character from multi-view images, we extend the LRM with semantic field, enabling semantic-based layered generation (Sec. 3.2). Finally, multi-level refinement process is designed to enhance the results, improving the geometric structure and providing 3 more detailed textures (Sec. 3.3). An overview of the StdGEN pipeline is shown in Fig. 2. 3.1. Multi-view Generation and Canonicalization Given reference character image, our objective is to generate set of multi-view images and corresponding normal maps that maintain 3D consistency. This process involves two steps: Canonicalizing an arbitrary reference image into an A-pose character and generating multi-view RGBs and normals from the A-pose representation. Arbitrary Reference to A-pose Conversion. Directly reconstructing 3D character model in arbitrary poses can be affected by self-occlusion from different viewpoints. Therefore, we design to first canonicalize the character in 2D space. We select the A-pose as the target representation, as it is more conducive to subsequent multi-view generation, reconstruction, and applications like rigging. Following previous works [20, 40], we employ the Stable Diffusion [46] model, augmented with ReferenceNet, to translate 2D arbitrary posed reference image to A-pose image. The ReferenceNet helps to preserve the reference identity in the resulting image. We refer to [40] for more details. Multi-view RGBs and Normals Generation. This stage aims to provide more comprehensive information from different camera views for the subsequent semantic-aware large reconstruction model. Additionally, multi-view consistent normals are crucial to capturing rich surface details for further mesh refinement, which cannot be easily achieved by independently predicting normals for each view. Considering these factors, we adapt Era3D [26] to simultaneously generate multi-view high-resolution RGBs and normals. By employing memory-efficient row-wise attention mechanisms between different views and between RGB and normal maps, we ensure fine-grained spatial consistency and unlock the potential for higher output resolutions. Specifically, it accepts single A-pose image as input and generates orthographic projections for six viewpoint RGBs and normals with the elevation of 0 and azimuths of {90, 45, 0, 45, 90, 180}. To better suit our task and enhance the clarity of character details, we increased the resolution through progressive training approach, ultimately reaching 1024. Compared with CharacterGen [40], our choice can simultaneously generate high-resolution, multi-view consistent normal maps for mesh refinement. Besides, the twostep design allows for improved editing in the 2D A-pose space, facilitating the generation of decomposed characters for enhanced 3D editing applications. 3.2. Semantic-aware Large Reconstruction Model Once obtaining multi-view images, transformer-based Model (LRM) to reconstruct holistic 3D mesh. [40, 66] use sparse-view Large Reconstruction In Figure 3. Demonstration of the structure and intermediate outputs of our semantic-aware large reconstruction model (S-LRM). contrast to them, we aim to generate the character with the decomposed components, including the minimal-clothed to produce human model, external clothing, and hair, 3D character models that are not only visually accurate but also functionally versatile for various uses in 3D games and animation pipelines. To achieve this goal, we proposed the Semantic-aware Large Reconstruction Model (S-LRM), which extends NeRF/SDF to simultaneously encode semantics, appearance and geometry information in feed-forward manner. As shown in Fig. 2, the S-LRM takes multiview images into tokens and then feeds into transformer-based image-to-triplane decoder to output triplane representation. The triplane features are subsequently decoded to obtain semantic, color, density/SDF information. To enhance the training efficiency and reconstruction quality, following InstantMesh [66], we first utilize triplane NeRF representation to render 2D images with volume rendering for training. After that, we then switch to FlexiCubes [51] to extract explicit mesh from triplane-decoded SDF grid, and use direct mesh rasterization to render images. However, to obtain semantic-decomposed surface reconstruction, both NeRF and SDF implicit representations must be capable of rendering distinct semantic layers into images or extracting separate semantic surfaces using FlexiCubes in differentiable manner. To achieve that, novel semantic-equivalent NeRF/ SDF is proposed to extract character parts by specific semantics. Semantic-equivalent NeRF and SDF. NeRF represents 3D scene by spatial-variant volume densities with colors*. We extend it with semantic field, and model them as learnable function FΘ that takes sampled point location = (x; y; z) as inputs, and outputs color c, density σ and semantic distribution as: (σ, c, s) = FΘ(x). To render per-pixel color ˆC(r), series of 3D points are sampled along the ray r, and the pixel color is computed by integrating the sampled densities σi and colors ci using the *We ignore the view-dependent effects to simplify the discussion. 4 Where fi, fi,s are original SDF and equivalent SDF of semantic at location i, respectively. Fig. 4 illustrates our methods scheme. For red semantics, only region 3 is selected, as regions 1, 2 (SDF>0) and region 4 (non-red) are discarded. Similarly, when the green is chosen, region 4 is correctly extracted. This formulation ensures correct decomposition by specific semantics and is fully compatible with subsequent FlexiCubes mesh extraction. In this way, we can differentiably extract multi-layer semantic surfaces from S-LRMs outputs, greatly facilitating the LRM training and downstream optimization. Model Structure. Our S-LRMs core structure is derived from InstantMesh [66] with ViT encoder, an image-totriplane transformer, and the feature decoder. Several enhancements are made to this foundation to better suit our objectives, with the semantic decoder being key addition. This component mirrors the structure of the density and color decoders and is designed to generate the semantic field in 3D space. For memory efficient training, we incorporate LoRA [19, 42] structures into all linear layers inside both self-attention and cross-attention blocks. Multiple semantic level supervision. Current LRMs typically rely solely on 2D supervision, which limits their ability to generate information about objects internal structures under occlusion. While 3D supervision would be effective, it is often too resource-intensive. To address this, we propose an effective supervision method that jointly learns semantic and color information, enabling the acquisition of 3D semantic field and internal character information using only 2D supervision. Stage 1: Training on NeRF with Single-layer Semantics. In this initial stage, we train on the triplane NeRF representation. We initialize the model with the pre-trained InstantNeRF, and train the newly added LoRA in all attention blocks linear layers, and the newly introduced semantic decoder. We train it under the image, mask and semantic loss: ˆS(r) = (cid:88) i=1 Tipiαi, Lsem = (cid:88) CE( ˆSk, gt ) (4) L1 = Lmse + λlpipsLlpips + λmaskLmask + λsemLsem (5) ˆS is the semantic map calculated by the probabilities pi from semantic decoders output through softmax layer. ˆSk, gt denotes the k-th view of rendered and ground-truth semantic maps, and CE denotes the cross-entropy function. Stage 2: Training on NeRF with Multi-layer Semantics. Having learned robust surface semantic information in the first stage, we aim to learn the 3D characters internal semantic and color information. We hierarchically supervise from outside to inside according to the spatial relationship of different semantic parts, by masking specific semantics during rendering and supervising with corresponding 2D ground truth. Assuming we aim to preserve set of semanFigure 4. Our semantic-equivalent NeRF and SDF extraction scheme (shown in yellow color). volume rendering equation with: ˆC(r) = (cid:88) i=1 Tiαici, Ti = i1 (cid:89) (1 αj) (1) j=1 where αi = (1 exp(σiδi)), δi = ti+1 ti is the alpha value of samples and distance between adjacent samples. Given the probability ps,i of semantic at location i, the pixel color ˆCs(r) under semantic can be calculated as: ˆCs(r) = (cid:88) i=1 Ts,ips,iαici, Ts,i = i1 (cid:89) (1 αjps,j) (2) j= If the probability of certain semantic at given location is zero, it should be considered completely transparent under the current semantic category. Furthermore, given that position is known to be opaque, the probability of the current semantic should be linear to the final equivalent transparency. Unlike NeRF, SDF does not incorporate the concept of Instead, positive/negative values represent transparency. points outside/inside the surface. Consequently, semantic probabilities cannot be directly applied to SDF for the mesh part extraction. Upon analysis, the extraction of semanticequivalent SDF should adhere to the following principles: 1. The zero value of the original SDF serves as hard constraint. When the original SDF is positive, the equivalent SDF should also be positive; 2. When the original SDF is negative, the equivalent SDF should be zero at the boundaries where the maximum of relevant semantics transits; 3. At locations where the original SDF equals zero, but the probability of the current semantic is not the highest among all semantics, the equivalent SDF should not only maintain its sign but also be greater than zero. Based on these principles, we propose the following formula for constructing the equivalent SDF: fi,s = max(fi, (max r=s pi,r) pi,s), (3) 5 tics {Ps}, we can render the image and semantic map under current conditions as follows: (cid:88) i= (cid:88) ˆCP (r) = ˆSP (r) = TP,iαici TP,iαipi (cid:88) sP (cid:88) sP ps,i, ps,i, where TP,i = i=1 i1 (cid:89) j=1 (1 αj (cid:88) sP ps,i), The loss function is defined as: L2 = Lmse,P + λlpipsLlpips,P + λmaskLmask,P CE( ˆSP,k, gt + λsem (cid:88) P,k) (6) (7) (8) (9) This decomposed training approach enables our S-LRM to simultaneously learn color and semantic information for the surface and the objects interior, thus achieving feedforward 3D content decomposition and reconstruction. Stage 3: Training on Mesh with Multi-layer Semantics. We switch to mesh representation [51] similar to [66] for efficient high-resolution training. We then extract the equivalent SDF via: fi,P = max(fi, (max /P pi,s max sP pi,s)), (10) Subsequently, we input the equivalent SDF into FlexiCubes to obtain the mesh, render the image and semantic map, and supervise using the following loss function: L3 = L2 + λnormal (cid:88) + λdepth gt (cid:88) (cid:16) gt 1 ˆNP,i gt P,i (cid:17) (cid:13) ˆDP,i Dgt (cid:13) (cid:13) P,i (cid:13) (cid:13) (cid:13)1 + λdevLdev (11) P,i and gt P,i, ˆNP,i, gt where ˆDP,i, Dgt denote the rendered depth, ground truth depth, rendered normal, ground truth normal and ground truth mask of the k-th view under semantic set , respectively; Ldev denotes the deviation loss of FlexiCubes. 3.3. Multi-layer Refinement Given the limitations in geometric and texture detail achievable by large reconstruction models, further optimization of the mesh post-reconstruction is necessary. Recent methods [27, 65] utilizing high-resolution normal maps for original mesh optimization have shown promising results, albeit primarily for single-layer meshes. We propose an iterative optimization mechanism for multi-layer mesh refinement. To ensure thorough optimization at each level, we employ staged approach following our S-LRM: Initially, we 6 extract different parts of the mesh by specifying various semantics and optimize only the base human model; upon completion, we overlay the clothing mesh, fixing the base and optimizing solely the clothing component; finally, we add the hair mesh, fixing the previous two layers and optimizing only the hair. The optimization process is guided by the multi-view normal maps generated earlier through diffusion models, each optimization step involves differentiable rendering of the mesh to compute losses and gradients, followed by vertex adjustments based on these gradients, and re-mesh operations including edge collapse, split and flips. The loss function for this process is defined as follows: Lr1 = λ mask (cid:88) ˆMk pred 2 2 + λcolLcol + λ normalM pred ˆNk pred 2 2 (12) (cid:88) where ˆMk, ˆNk are rendered masks and normal maps, pred , pred are diffusion-generated masks and normal maps unk der k-th view, respectively. Lcol is the collision loss modified from [39] to ensure outer-layer mesh in the outer normal direction of inner-layer mesh: Lcol = 1 (cid:88) i=1 max ((vj vi) nj, 0)3 (13) where vi represents the i-th vertex of the outer-layer mesh, vj is its nearest neighbor of vi on the inner-layer mesh, and nj denotes the normal vector associated with vj. Upon completing the optimization process, the mesh undergoes an additional ExplicitTarget Optimization phase, similar to that employed in Unique3D [65]. This stage aims to eliminate multi-view inconsistencies and further refine the geometry. Finally, the optimized meshes are colorized by the back projection of the multi-view images. Preand Post-dilation of Mesh. To better solve the problematic intersections among meshes, we introduce dilation process applied both before and after optimization. This process constructs an approximate flow field based on the original positions of the inner and outer layer meshes. For each vertex on the outside mesh, we utilize kdtree to query its nearest vertex neighbors of the fixed inside mesh. The movement range is then smoothly weighted based on the exponential inverse distance from these neighbors, with distant points remaining stationary. This approach creates dilation effect, ensuring that when inner layers are moved outward to resolve intersections, the outer layers follow suit in natural, gradual manner. A-pose Conditioned Input Arbitrary-pose Conditioned Input SSIM LPIPS FID CLIP Similarity SSIM LPIPS FID CLIP Similarity SyncDreamer [31] Zero-1-to-3 [29] Era3D [26] CharacterGen [40] Ours Magic123 [43] ImageDream [60] OpenLRM [14] LGM [55] InstantMesh [66] Unique3D [65] CharacterGen [40] Ours 0.870 0.865 0.876 0.886 0.958 0.886 0.856 0.889 0.876 0.888 0.889 0.880 0.937 0.183 0.172 0.144 0.119 0. 0.142 0.171 0.151 0.151 0.126 0.136 0.124 0.066 2D Multi-view Comparisons 0.864 0.885 0.908 0.928 0.941 0.845 0.842 0.842 0.871 0.920 3D Character Comparisons 0.887 0.836 0.878 0.902 0.906 0.919 0.905 0. 0.849 0.823 0.863 0.838 0.846 0.856 0.869 0.916 0.223 0.500 0.095 0.063 0.004 0.192 0.846 0.406 0.282 0.107 0.030 0.081 0.010 0.217 0.209 0.195 0.139 0.071 0.197 0.218 0.191 0.203 0.202 0.190 0.134 0.084 0.328 0.481 0.094 0.056 0. 0.256 0.875 0.707 0.480 0.285 0.042 0.119 0.011 0.839 0.878 0.900 0.919 0.935 0.862 0.818 0.844 0.884 0.886 0.903 0.901 0.936 Table 1. Quantitative comparison of A-pose and arbitrary pose inputs for 2D multi-view generation and 3D character generation on the test split of Anime3D++ dataset. 4. Experiments 4.1. Anime3D++ Dataset While the Anime3D Dataset from CharacterGen [40] comprises RGB renderings of 13,746 stylized character subjects, it falls short of high-quality, decomposable character generation requirements. To address this limitation, we have developed the Anime3D++ Dataset. Following previous approaches [7, 40], we initially collected approximately 14,000 3D anime character models from VRoid-Hub. After rigorous quality control process, including removing low-quality data and models unsuitable for proper layering, we refined our dataset to 10,811 character models. All characters were adjusted to an A-pose configuration, with arms rotated 45 degrees downward from the horizontal position. Our rendering process goes beyond standard image generation, incorporating depth, normal, and semantic maps. To facilitate layered reconstruction supervision, we rendered the complete model and two additional configurations: the base human model alone, and the base model with clothing. The rendering includes eight views at 45degree azimuth intervals with zero elevation, supplemented by top-down and bottom-up views. We enriched the dataset with five close-up facial views and 20 random viewpoints. To enhance the training of our diffusion model, we implemented data augmentation on varying arm angles. 4.2. Implementation Details We divide our Anime3D dataset into training and testing set in 99:1 ratio. We first train the canonicalization diffusion model at 512 resolution with learning rate of 5e-5, then reduce it to 1e-5 as we progressively increase the resolution to 768 and 1024. Similarly, the multi-view diffusion model is trained at constant learning rate of 5e-5 while scaling from 512 to 768 and finally to 1024 resolution. We use LoRA with 128-rank for S-LRM, learning rate of 4e-5, and three supervision stages with rendering resolutions of 192, 144, and 512. The loss function parameters are set as λlpips, λmask, λsem, λdepth, λnormal, λdev = 2.0, 1.0, 1.0, 0.5, 0.2, 0.5. For multi-layer refinement, we set λ normal, λcol = 1.0, 1.0, 100.0, and we further extract the coarse hair mask, applying additional normal and mask loss for hair refinement with weight of 1 and 10. mask, λ 4.3. Results and Comparisons Given that current methods cannot generate layered 3D models from single arbitrary character image, we compare our non-layered generation results with other methods on the test split of our Anime3D++ dataset. We further decoupled the pose canonicalization component to ensure fairness, conducting both 2D and 3D comparisons for arbitrary pose and A-pose character reference inputs. When using A-pose inputs, all methods are compared against A-pose ground truth. For arbitrary pose inputs, we follow CharacterGens settings, comparing our method and CharacterGen (both capable of canonicalization) against the A-pose ground truth, while other methods are compared under the original pose ground truth. Generation quality and fidelity are evaluated using SSIM [62], LPIPS [72], and FID. We also compute CLIP [44] cosine similarity between the front ground-truth image and the multi-views or 3D renderings. Quantitative Results. For 2D multi-view generation results, we compare our method with Zero-1-to-3 [29], SyncDreamer [31], Era3D [26], and CharacterGen [40]. For 3D Character Generation results, we compare with Magic123 [43], ImageDream [60] (SDS-based optimiza7 Figure 5. Qualitative comparisons on geometry and appearance of generated 3D characters. tion); OpenLRM [14, 18], LGM [55], InstantMesh [66] (feed-forward methods); Unique3D [65] (direct mesh reconstruction); and CharacterGen. 3D Results are uniformly rendered as eight equidistant images at elevation=0 and aligned using horizontal mask registration. As shown in Tab. 1, Our method performs better in both standard and arbitrary pose settings. Existing 2D multiview generation methods often struggle to preserve adequate geometric and appearance information in generated images. Among 3D methods, SDS-based approaches typically exhibit blurred geometry and suffer from the Janus Problem, and feed-forward methods generally lack geometric and texture precision. Unique3D achieves high metrics due to high-resolution supervision but suffers from unstable mesh initialization, impacting visual quality. CharacterGen demonstrates notable advantage to other methods in arbitrary pose settings but the advantage diminishes in Apose, showing effective pose canonicalization but limited reconstruction ability. In contrast, our method consistently achieves superior results across all cases. Qualitative Results. Fig. 5 reveals several limitations across different methods. InstantMeshs results were constrained by grid resolution, leading to insufficient texture precision. While Unique3D achieved higher resolution, its heavy reliance on depth-based mesh initialization made it susceptible to geometric collapse under inaccurate depth estimations. CharacterGen exhibited low geometric and texture fidelity despite employing multi-view back-projection, and frequently produced visually disruptive black artifacts. In contrast, our method demonstrated superior geometric accuracy and texture fidelity performance. Decomposed Results. Fig. 6 illustrates our methods capa8 Method Overall Quality Fidelity Geometric Quality Texture Quality CharacterGen [40] InstantMesh [66] Unique3D [65] Ours 4.5% 4.0% 2.0% 1.8% 11.8% 18.3% 82.4% 75.2% 6.2% 1.8% 10.5% 81.5% 4.2% 2.5% 17.0% 76.3% Table 2. User study results comparing different methods across four dimensions. The values represent the percentage of times each method was selected as the best in the respective dimension. Figure 6. Decomposed outputs of our method, presented in texture, mesh, and cross-section. bility to reconstruct decomposed characters from single reference images, organized from left to right as follows: the input reference image, the reconstructed 3D character (applied shading for better visualization), the semantically decomposed components and geometries. The reconstructed meshes demonstrate high geometric precision, while the semantic decomposition is also accurate, successfully decoupling the base human model, clothing, and hair. This level of decomposition represents significant advancement in character reconstruction from single images. The rightmost figure presents cross-sectional view of the clothing, revealing that our reconstructed clothing is entirely internally hollow. This substantially enhances the potential for integration with downstream applications like realistic physics simulations and easier rigging. User Study. To comprehensively evaluate our methods performance, we conducted user study involving 28 volunteers. The study utilized 16 randomly selected 3D meshes and their corresponding reference images, drawn from both web-collected images and the Anime3D++ test split. Participants were asked to compare our method against existing approaches across four dimensions: overall quality, fidelity, geometric quality, and texture quality. For each comparison group, participants identified the best result in each dimension. Tab. 2 shows that our method achieved superior performance across all evaluated dimensions, demonstrating its effectiveness in generating high-quality 3D characters. 4.4. Ablation Study Figure 7. Ablation study on character decomposition. tion. The visual comparison in Fig. 7 reveals that without decomposition, the results exhibit fusion of hair, clothing, and the base human model, significantly limiting their potential for downstream applications. In contrast, our method successfully separates these components while maintaining high mesh precision, showcasing the effectiveness of our semantic decomposing approach. Figure 8. Ablation study on multi-layer refinement. Zoom in for better details. Character Decomposition. To demonstrate the decomposition capabilities of our core S-LRM and its impact on the results, we compared our method with direct refinement approach that does not employ semantic decomposiMulti-layer Refinement. We further illustrate the distinction between the direct output of our S-LRM and the results after multi-layer refinement in Fig. 8. The pre-optimization results demonstrate that our S-LRM successfully decom9 Figure 10. Our pipeline enables diverse 3D editing using only text prompts and masks, leveraging in-painting diffusion models in the 2D domain. tomization, including outfit changes and hairstyle modifications. The process requires minimal user input: the original reference image or front-view capture, roughly sketched in-painting mask, and text prompt. Utilizing existing inpainting models (HD-Painter [35] in this case), we achieve the desired 2D edits. Subsequently, these 2D modifications are translated into the 3D domain through our reconstruction and optimization pipeline. During the multi-layer refinement stage, the relevant mesh components are selectively replaced with the edited versions while preserving the unmodified parts. This bridges the gap between 2D image editing and 3D character customization and streamlines the 3D character modification process. 5. Conclusion In this paper, we introduce StdGEN, an innovative pipeline for generating semantic decomposed high-quality 3D characters from single images. novel semantic-aware large reconstruction model is proposed to jointly reconstruct geometry, color and semantics, together with an efficient 2D diffusion model and iterative multi-layer refinement module to enable the high-quality generation StdGEN achieves from arbitrary-posed single image. superior performance over existing baselines in terms of geometry, texture and decomposability, and offers readyto-use, semantic-decomposed, customizable 3D characters, demonstrating significant potential for various applications."
        },
        {
            "title": "References",
            "content": "[1] Jonathan Barron, Ben Mildenhall, Dor Verbin, Pratul Srinivasan, and Peter Hedman. Mip-nerf 360: Unbounded In Proceedings of the anti-aliased neural radiance fields. Figure 9. Rigging and animation comparisons on 3D character generation. Our method demonstrates superior performance in human perception and physical characteristics. poses various mesh components with correct geometry and shape, validating the capabilities of our approach. However, the precision is limited due to the inherent characteristics of FlexiCubes and memory constraints. Post-refinement, we observe substantial enhancement in precision while maintaining the overall structure. This improvement underscores the effectiveness of our multi-layer refinement process in preserving the structure of the decomposed components while significantly elevating the geometric accuracy and overall quality of the reconstructed character. 4.5. Applications Accurate 3D Animation. Compared to other 3D character generation methods, our decomposed generation in A-pose is more suitable for downstream animation and 3D applications. In Fig. 9, we rig the 3D character generated by our approach and by CharacterGen for comparison. Without decomposition, the hair and clothing stick together and are attached to the base human model. In contrast, our approach maintains separated parts, aligning more closely with natural perception. Additionally, the non-decomposed nature leads to inaccurate deformations and physical characteristics during movement, which our method effectively avoids. 3D Editing. Our methods capability to generate characters in A-pose not only facilitates simpler rigging processes but also enables 3D editing through user-specified prompts and in-painting masks in the 2D domain. As illustrated in Fig. 10, our framework allows for effortless character cusIEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 54705479, 2022. 2 [2] Alexander W. Bergman, Petr Kellnhofer, Wang Yifan, Eric R. Chan, David B. Lindell, and Gordon Wetzstein. Generative neural articulated radiance fields, 2023. 3 [3] Yukang Cao, Yan-Pei Cao, Kai Han, Ying Shan, and KwanYee K. Wong. Dreamavatar: Text-and-shape guided 3d human avatar generation via diffusion models, 2023. 3 [4] Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and In European Hao Su. Tensorf: Tensorial radiance fields. Conference on Computer Vision, pages 333350. Springer, 2022. 2 [5] Anpei Chen, Haofei Xu, Stefano Esposito, Siyu Tang, and Andreas Geiger. Lara: Efficient large-baseline radiance fields, 2024. 3 [6] Rui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia. Fantasia3d: Disentangling geometry and appearance for highIn Proceedings of the quality text-to-3d content creation. IEEE/CVF International Conference on Computer Vision (ICCV), pages 2224622256, 2023. [7] Shuhong Chen, Kevin Zhang, Yichun Shi, Heng Wang, Yiheng Zhu, Guoxian Song, Sizhe An, Janus Kristjansson, Xiao Yang, and Matthias Zwicker. Panic-3d: Stylized singleview 3d reconstruction from portraits of anime characters. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2106821077, 2023. 7 [8] Ruikai Cui, Xibin Song, Weixuan Sun, Senbo Wang, Weizhe Liu, Shenzhou Chen, Taizhang Shang, Yang Li, Nick Barnes, Hongdong Li, and Pan Ji. Lam3d: Large image-point-cloud alignment model for 3d reconstruction from single image, 2024. 3 [9] Matt Deitke, Ruoshi Liu, Matthew Wallingford, Huong Ngo, Oscar Michel, Aditya Kusupati, Alan Fan, Christian Laforte, Vikram Voleti, Samir Yitzhak Gadre, Eli VanderBilt, Aniruddha Kembhavi, Carl Vondrick, Georgia Gkioxari, Kiana Ehsani, Ludwig Schmidt, and Ali Farhadi. Objaverse-xl: universe of 10m+ 3d objects, 2023. 3 [10] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:87808794, 2021. 2 [11] Junting Dong, Qi Fang, Zehuan Huang, Xudong Xu, Tela: Text to arXiv preprint Jingbo Wang, Sida Peng, and Bo Dai. layer-wise 3d clothed human generation. arXiv:2404.16748, 2024. 2 [12] Junting Dong, Qi Fang, Zehuan Huang, Xudong Xu, Jingbo Wang, Sida Peng, and Bo Dai. Tela: Text to layer-wise 3d clothed human generation, 2024. [13] Ayaan Haque, Matthew Tancik, Alexei Efros, Aleksander Holynski, and Angjoo Kanazawa. Instruct-nerf2nerf: arXiv preprint Editing 3d scenes with instructions. arXiv:2303.12789, 2023. 2 [14] Zexin He and Tengfei Wang. Openlrm: Open-source large reconstruction models. https://github.com/ 3DTopia/OpenLRM, 2023. 7, 8 [15] Fangzhou Hong, Zhaoxi Chen, Yushi Lan, Liang Pan, and Ziwei Liu. Eva3d: Compositional 3d human generation from arXiv preprint arXiv:2210.04888, 2d image collections. 2022. 3 [16] Fangzhou Hong, Zhaoxi Chen, Yushi Lan, Liang Pan, and Ziwei Liu. Eva3d: Compositional 3d human generation from arXiv preprint arXiv:2210.04888, 2d image collections. 2022. 2 [17] Yicong Hong, Kai Zhang, Jiuxiang Gu, Sai Bi, Yang Zhou, Difan Liu, Feng Liu, Kalyan Sunkavalli, Trung Bui, and Hao Tan. Lrm: Large reconstruction model for single image to In The Twelfth International Conference on Learning 3d. Representations. 2, 3 [18] Yicong Hong, Kai Zhang, Jiuxiang Gu, Sai Bi, Yang Zhou, Difan Liu, Feng Liu, Kalyan Sunkavalli, Trung Bui, and Hao Tan. Lrm: Large reconstruction model for single image to 3d. arXiv preprint arXiv:2311.04400, 2023. [19] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. 5 [20] Li Hu. Animate anyone: Consistent and controllable imageto-video synthesis for character animation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 81538163, 2024. 4 [21] Shuo Huang, Zongxin Yang, Liangting Li, Yi Yang, and Jia Jia. Avatarfusion: Zero-shot generation of clothingIn Proceedings decoupled 3d avatars using 2d diffusion. of the 31st ACM International Conference on Multimedia, pages 57345745, 2023. 2 [22] Yukun Huang, Jianan Wang, Ailing Zeng, He Cao, Xianbiao Qi, Yukai Shi, Zheng-Jun Zha, and Lei Zhang. Dreamwaltz: Make scene with complex 3d animatable avatars, 2023. 3 [23] Zehuan Huang, Hao Wen, Junting Dong, Yaohui Wang, Yangguang Li, Xinyuan Chen, Yan-Pei Cao, Ding Liang, Yu Qiao, Bo Dai, et al. Epidiff: Enhancing multi-view synthesis via localized epipolar-constrained diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 97849794, 2024. 3 [24] Suyi Jiang, Haoran Jiang, Ziyu Wang, Haimin Luo, Wenzheng Chen, and Lan Xu. Humangen: Generating human radiance fields with explicit priors, 2022. 3 [25] Jiahao Li, Hao Tan, Kai Zhang, Zexiang Xu, Fujun Luan, Yinghao Xu, Yicong Hong, Kalyan Sunkavalli, Greg Shakhnarovich, and Sai Bi. Instant3d: Fast text-to-3d with sparse-view generation and large reconstruction model. In The Twelfth International Conference on Learning Representations. [26] Peng Li, Yuan Liu, Xiaoxiao Long, Feihu Zhang, Cheng Lin, Mengfei Li, Xingqun Qi, Shanghang Zhang, Wenhan Luo, Ping Tan, et al. Era3d: High-resolution multiview diffusion using efficient row-wise attention. arXiv preprint arXiv:2405.11616, 2024. 4, 7 [27] Weiyu Li, Jiarui Liu, Rui Chen, Yixun Liang, Xuelin Chen, Ping Tan, and Xiaoxiao Long. Craftsman: High-fidelity mesh generation with 3d native generation and interactive geometry refiner. arXiv preprint arXiv:2405.14979, 2024. 3, 6 [28] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution 11 In IEEE Conference on Comtext-to-3d content creation. puter Vision and Pattern Recognition (CVPR), 2023. 2 [29] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-toIn Proceedings of 3: Zero-shot one image to 3d object. the IEEE/CVF international conference on computer vision, pages 92989309, 2023. 3, [30] Yuan Liu, Cheng Lin, Zijiao Zeng, Xiaoxiao Long, Lingjie Liu, Taku Komura, and Wenping Wang. Syncdreamer: Generating multiview-consistent images from single-view image. In The Twelfth International Conference on Learning Representations. 3 [31] Yuan Liu, Cheng Lin, Zijiao Zeng, Xiaoxiao Long, Lingjie Liu, Taku Komura, and Wenping Wang. Syncdreamer: Generating multiview-consistent images from single-view image. arXiv preprint arXiv:2309.03453, 2023. 7 [32] Xiaoxiao Long, Yuan-Chen Guo, Cheng Lin, Yuan Liu, Zhiyang Dou, Lingjie Liu, Yuexin Ma, Song-Hai Zhang, Marc Habermann, Christian Theobalt, et al. Wonder3d: Single image to 3d using cross-domain diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 99709980, 2024. 3 [33] Matthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Moll, and Michael Black. Smpl: skinned multiperson linear model. In Seminal Graphics Papers: Pushing the Boundaries, Volume 2, pages 851866. 2023. 2 [34] Yuanxun Lu, Jingyang Zhang, Shiwei Li, Tian Fang, David McKinnon, Yanghai Tsin, Long Quan, Xun Cao, and Yao Yao. Direct2. 5: Diverse text-to-3d generation via multi-view 2.5 diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8744 8753, 2024. 3 [35] Hayk Manukyan, Andranik Sargsyan, Barsegh Atanyan, Zhangyang Wang, Shant Navasardyan, and Humphrey Shi. Hd-painter: high-resolution and prompt-faithful text-guided arXiv preprint image inpainting with diffusion models. arXiv:2312.14091, 2023. [36] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021. 2 [37] Atsuhiro Noguchi, Xiao Sun, Stephen Lin, and Tatsuya Harada. Unsupervised learning of efficient geometry-aware neural articulated representations, 2022. 3 [38] Panwang Pan, Zhuo Su, Chenguo Lin, Zhen Fan, Yongjie Zhang, Zeming Li, Tingting Shen, Yadong Mu, and Yebin Humansplat: Generalizable single-image human Liu. arXiv preprint gaussian splatting with structure priors. arXiv:2406.12459, 2024. 2 [39] Bo Peng, Yunfan Tao, Haoyu Zhan, Yudong Guo, and Juyong Zhang. Pica: Physics-integrated clothed avatar. arXiv preprint arXiv:2407.05324, 2024. 2, 6 [40] Hao-Yang Peng, Jia-Peng Zhang, Meng-Hao Guo, Yan-Pei Cao, and Shi-Min Hu. Charactergen: Efficient 3d character generation from single images with multi-view pose canonicalization. ACM Transactions on Graphics (TOG), 43(4): 113, 2024. 2, 3, 4, 7, 9 [41] Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion, 2022. 2 [42] Zhangyang Qi, Yunhan Yang, Mengchen Zhang, Long Xing, Xiaoyang Wu, Tong Wu, Dahua Lin, Xihui Liu, Jiaqi Wang, and Hengshuang Zhao. Tailor3d: Customized 3d assets editing and generation with dual-side images. arXiv preprint arXiv:2407.06191, 2024. 5 [43] Guocheng Qian, Jinjie Mai, Abdullah Hamdi, Jian Ren, Aliaksandr Siarohin, Bing Li, Hsin-Ying Lee, Ivan Skorokhodov, Peter Wonka, Sergey Tulyakov, et al. Magic123: One image to high-quality 3d object generation using both 2d and 3d diffusion priors. arXiv preprint arXiv:2306.17843, 2023. 7 [44] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. 7 [45] Amit Raj, Srinivas Kaza, Ben Poole, Michael Niemeyer, Nataniel Ruiz, Ben Mildenhall, Shiran Zada, Kfir Aberman, Michael Rubinstein, Jonathan Barron, et al. DreamarXiv booth3d: Subject-driven text-to-3d generation. preprint arXiv:2303.13508, 2023. 2 [46] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1068410695, 2022. [47] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 2 [48] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in Neural Information Processing Systems, 35:3647936494, 2022. 2 [49] Ruizhi Shao, Jingxiang Sun, Cheng Peng, Zerong Zheng, ConBoyao Zhou, Hongwen Zhang, and Yebin Liu. trol4d: Dynamic portrait editing by learning 4d gan from 2d diffusion-based editor. arXiv preprint arXiv:2305.20082, 2023. 2 [50] Tianchang Shen, Jun Gao, Kangxue Yin, Ming-Yu Liu, and Sanja Fidler. Deep marching tetrahedra: hybrid representation for high-resolution 3d shape synthesis. In Advances in Neural Information Processing Systems (NeurIPS), 2021. 2 [51] Tianchang Shen, Jacob Munkberg, Jon Hasselgren, Kangxue Yin, Zian Wang, Wenzheng Chen, Zan Gojcic, Sanja Fidler, Nicholas Sharp, and Jun Gao. Flexible isosurface extraction for gradient-based mesh optimization. ACM Trans. Graph., 42(4):371, 2023. 3, 4, 6 [52] Yichun Shi, Peng Wang, Jianglong Ye, Long Mai, Kejie Li, and Xiao Yang. Mvdream: Multi-view diffusion for 3d generation. In The Twelfth International Conference on Learning Representations. 3 [53] Uriel Singer, Shelly Sheynin, Adam Polyak, Oron Ashual, Iurii Makarov, Filippos Kokkinos, Naman Goyal, Andrea Vedaldi, Devi Parikh, Justin Johnson, et al. Text-to-4d dynamic scene generation. arXiv preprint arXiv:2301.11280, 2023. 2 [54] Cheng Sun, Min Sun, and Hwann-Tzong Chen. Direct voxel grid optimization: Super-fast convergence for radiance fields reconstruction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5459 5469, 2022. 2 [55] Jiaxiang Tang, Zhaoxi Chen, Xiaokang Chen, Tengfei Wang, Gang Zeng, and Ziwei Liu. Lgm: Large multi-view gaussian model for high-resolution 3d content creation. arXiv preprint arXiv:2402.05054, 2024. 3, 7, 8 [56] Christina Tsalicoglou, Fabian Manhardt, Alessio Tonioni, Michael Niemeyer, and Federico Tombari. Textmesh: GenarXiv eration of realistic 3d meshes from text prompts. preprint arXiv:2304.12439, 2023. 2 [57] Haochen Wang, Xiaodan Du, Jiahao Li, Raymond A. Yeh, and Greg Shakhnarovich. Score jacobian chaining: Lifting pretrained 2d diffusion models for 3d generation, 2022. 2 [58] Jionghao Wang, Yuan Liu, Zhiyang Dou, Zhengming Yu, Yongqing Liang, Xin Li, Wenping Wang, Rong Xie, and Li Song. Disentangled clothed avatar generation from text descriptions. arXiv preprint arXiv:2312.05295, 2023. 2 [59] Jionghao Wang, Yuan Liu, Zhiyang Dou, Zhengming Yu, Yongqing Liang, Cheng Lin, Xin Li, Wenping Wang, Rong Xie, and Li Song. Disentangled clothed avatar generation from text descriptions, 2024. [67] Xu Yinghao, Shi Zifan, Yifan Wang, Chen Hansheng, Yang Ceyuan, Peng Sida, Shen Yujun, and Wetzstein Gordon. Grm: Large gaussian reconstruction model for efficient 3d reconstruction and generation, 2024. 3 [68] Chubin Zhang, Hongliang Song, Yi Wei, Yu Chen, Jiwen Lu, and Yansong Tang. Geolrm: Geometry-aware large reconstruction model for high-quality 3d gaussian generation, 2024. 3 [69] Jianfeng Zhang, Zihang Jiang, Dingdong Yang, Hongyi Xu, Yichun Shi, Guoxian Song, Zhongcong Xu, Xinchao Wang, and Jiashi Feng. Avatargen: 3d generative model for animatable human avatars, 2022. 3 [70] Kai Zhang, Sai Bi, Hao Tan, Yuanbo Xiangli, Nanxuan Zhao, Kalyan Sunkavalli, and Zexiang Xu. Gs-lrm: Large reconstruction model for 3d gaussian splatting. European Conference on Computer Vision, 2024. 3 [71] Longwen Zhang, Ziyu Wang, Qixuan Zhang, Qiwei Qiu, Anqi Pang, Haoran Jiang, Wei Yang, Lan Xu, and Jingyi Yu. Clay: controllable large-scale generative model for creating high-quality 3d assets. ACM Transactions on Graphics (TOG), 43(4):120, 2024. 3 [72] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In CVPR, 2018. [60] Peng Wang and Yichun Shi. multi-view diffusion for 3d generation. arXiv:2312.02201, 2023. 7 Imagedream: Image-prompt arXiv preprint [61] Yi Wang, Jian Ma, Ruizhi Shao, Qiao Feng, Yu-Kun Lai, and Kun Li. Humancoser: Layered 3d human generaarXiv preprint tion via semantic-aware diffusion model. arXiv:2408.11357, 2024. 2 [62] Zhou Wang, Alan Bovik, Hamid Sheikh, and Eero Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing, 13(4):600612, 2004. 7 [63] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu. Prolificdreamer: High-fidelity and diverse text-to-3d generation with variational score distillation. arXiv preprint arXiv:2305.16213, 2023. [64] Zhengyi Wang, Yikai Wang, Yifei Chen, Chendong Xiang, Shuo Chen, Dajiang Yu, Chongxuan Li, Hang Su, and Jun Zhu. Crm: Single image to 3d textured mesh arXiv preprint with convolutional reconstruction model. arXiv:2403.05034, 2024. 3 [65] Kailu Wu, Fangfu Liu, Zhihan Cai, Runjie Yan, Hanyang Wang, Yating Hu, Yueqi Duan, and Kaisheng Ma. Unique3d: High-quality and efficient 3d mesh generation from single image. arXiv preprint arXiv:2405.20343, 2024. 6, 7, 8, 9 [66] Jiale Xu, Weihao Cheng, Yiming Gao, Xintao Wang, Shenghua Gao, and Ying Shan. Instantmesh: Efficient 3d mesh generation from single image with sparse-view large reconstruction models. arXiv preprint arXiv:2404.07191, 2024. 3, 4, 5, 6, 7, 8,"
        }
    ],
    "affiliations": [
        "Tencent AI Lab",
        "Tsinghua University",
        "Beihang University"
    ]
}