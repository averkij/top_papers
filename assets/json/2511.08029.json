{
    "paper_title": "BiCA: Effective Biomedical Dense Retrieval with Citation-Aware Hard Negatives",
    "authors": [
        "Aarush Sinha",
        "Pavan Kumar S",
        "Roshan Balaji",
        "Nirav Pravinbhai Bhatt"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Hard negatives are essential for training effective retrieval models. Hard-negative mining typically relies on ranking documents using cross-encoders or static embedding models based on similarity metrics such as cosine distance. Hard negative mining becomes challenging for biomedical and scientific domains due to the difficulty in distinguishing between source and hard negative documents. However, referenced documents naturally share contextual relevance with the source document but are not duplicates, making them well-suited as hard negatives. In this work, we propose BiCA: Biomedical Dense Retrieval with Citation-Aware Hard Negatives, an approach for hard-negative mining by utilizing citation links in 20,000 PubMed articles for improving a domain-specific small dense retriever. We fine-tune the GTE_small and GTE_Base models using these citation-informed negatives and observe consistent improvements in zero-shot dense retrieval using nDCG@10 for both in-domain and out-of-domain tasks on BEIR and outperform baselines on long-tailed topics in LoTTE using Success@5. Our findings highlight the potential of leveraging document link structure to generate highly informative negatives, enabling state-of-the-art performance with minimal fine-tuning and demonstrating a path towards highly data-efficient domain adaptation."
        },
        {
            "title": "Start",
            "content": "BiCA: Effective Biomedical Dense Retrieval with Citation-Aware Hard Negatives Aarush Sinha1*, Pavan Kumar S2,3, Roshan Balaji2,3, Nirav Pravinbhai Bhatt2,3 1Vellore Institute of Technology (VIT) Chennai, India 2BioSystems Engineering and Control (BiSECt) Lab, Department of Biotechnology and Wadhwani School of Data Science and AI, Indian Institute of Technology (IIT) Madras, Tamil Nadu India 3The Centre for Integrative Biology and Systems medicinE (IBSE), IIT Madras, Chennai, Tamil Nadu, India aarush.sinha@gmail.com, {bt19d200,bt21d401,niravbhatt}@smail.iitm.ac.in 5 2 0 2 1 1 ] I . [ 1 9 2 0 8 0 . 1 1 5 2 : r Abstract Hard negatives are essential for training effective retrieval models. Hard-negative mining typically relies on ranking documents using cross-encoders or static embedding models based on similarity metrics such as cosine distance. Hard negative mining becomes challenging for biomedical and scientific domains due to the difficulty in distinguishing between source and hard negative documents. However, referenced documents naturally share contextual relevance with the source document but are not duplicates, making them well-suited as hard negatives. In this work, we propose BiCA: Biomedical Dense Retrieval with Citation-Aware Hard Negatives, an approach for hard-negative mining by utilizing citation links in 20,000 PubMed articles for improving domainspecific small dense retriever. We fine-tune the GTEsmall and GTEBase models using these citation-informed negatives and observe consistent improvements in zero-shot dense retrieval using nDCG@10 for both in-domain and out-of-domain tasks on BEIR and outperform baselines on long-tailed topics in LoTTE using Success@5. Our findings highlight the potential of leveraging document link structure to generate highly informative negatives, enabling state-of-the-art performance with minimal fine-tuning and demonstrating path towards highly data-efficient domain adaptation. Code github.com/NiravBhattLab/BiCA Datasets huggingface.co/collections/bisectgroup/bica-aaai Introduction Information Retrieval (IR) is fundamental discipline focused on extracting relevant information from vast collections of unstructured data, primarily text. IR systems employ various algorithms to match user queries with pertinent documents, integrating both exact lexical matching and semantic understanding techniques. These systems are essential for *Worked done as UG student and currently at University of Copenhagen, Denmark These authors contributed equally. Corresponding Author Copyright 2026, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. search engines, digital libraries, and question-answering applications, enabling users to efficiently navigate large volumes of information (Manning 2009). Despite these advances, retrieving precise information from the rapidly expanding biomedical literature indexed in PubMed (Sayers et al. 2011) remains significant challenge. This difficulty is often compounded by the prevalence of low-quality, keyword based queries which may lack the specificity required to pinpoint relevant documents within such specialized and nuanced domain. To address this issue, we propose an effective alternative by taking advantage of advanced training strategies and model architectures tailored for this complex environment. One such strategy is Hard Negative mining, which involves selecting challenging examples that closely resemble positive samples yet are ultimately irrelevant (Allan et al. 2003; Yang et al. 2024). By compelling models to learn finer-grained distinctions between these difficult-todistinguish negatives and true positives, the resulting systems exhibit more accurate rankings and improved retrieval effectiveness. Specifically for biomedical IR, the challenge lies not only in the sheer volume of literature but also in the intricate terminology and the subtle semantic relationships between concepts. In this work, we introduce BiCA (Biomedical CitationAware) retrievers, family of models designed to enhance biomedical information retrieval and out-of-domain retrieval. We propose novel hard negative mining technique based on multi-hop citation chains within the PubMed database. This approach, combined with efficient model architectures, allows us to develop systems that are not only highly effective but also computationally efficient. We demonstrate that our models, BiCABase and the significantly smaller BiCAsmall, achieve state-of-the-art or highly competitive results on several biomedical and general-domain IR benchmarks, often outperforming models that are substantially larger."
        },
        {
            "title": "Our Contributions",
            "content": "The main contributions of this work are as follows: We introduce novel hard negative mining strategy that constructs multi-hop citation chains from PubMed, using Figure 1: Our four-stage data generation and training pipeline. Stage 1: query is synthetically generated from positive documents abstract using T5 model. Stage 2: 2-hop citation neighborhood is constructed by retrieving papers cited by the positive document (1-hop) and papers cited by them (2-hop) via the PubMed API. Stage 3: Hard negatives are mined via semantic graph traversal. First, similarities are computed between the query and 1-hop documents. Second, dense, pairwise similarity graph is built for all 1-hop and 2-hop documents. Third, 5-step greedy traversal is initiated from the 1-hop document most similar to the query, creating path of five hard negatives. Stage 4: The resulting (Query, Positive Document, Hard Negatives) triplet is used to fine-tune the GTE model using the multiple negative ranking loss. the pubmed-parser, to generate high-quality, challenging negative examples for training retrieval models for biomedical domains. We (110M parameters) introduce BiCABase and BiCAsmall (33M parameters), two dense retrieval models specifically tailored for the biomedical domain using the proposed citation-aware hard negative mining, which also demonstrate strong performance on general domain retrieval tasks. Extensive zero-shot evaluations of our BiCA models on 14 BEIR tasks and 4 LoTTE tasks, outperforming all baselines on several tasks in BEIR and all sub-topics on LoTTE. We provide detailed latency analysis, demonstrating the practical efficiency of our models, particularly BiCAsmall, on single V100 GPU, highlighting their suitability for real-world deployment. Related Work Biomedical Information Retrieval Recent advancements in biomedical IR have focused on integrating novel methods and leveraging large-scale data to enhance retrieval performance. One such approach is Bibliometric Data Fusion (Breuer et al. 2023), which incorporates bibliometric metadata such as citation counts and altmetrics into retrieval systems. By using these implicit relevance signals, this method aims to improve retrieval performance, particularly for patient users, without relying on explicit relevance labeling. more recent development, Self-Learning Hypothetical Document Embeddings (SL-HyDE) (Li et al. 2024), introduces zero-shot approach to medical IR by utilizing large language models (LLMs) to generate hypothetical documents based on given query. This framework, which selflearns both pseudo-document generation and retrieval processes, improves retrieval accuracy without needing labeled data. The approach has shown notable performance across various LLM and retriever configurations, indicating its potential for enhancing zero-shot retrieval tasks. Another important contribution to biomedical IR is the development of Neural Retrievers (NRs) (Luo et al. 2022), which address data scarcity in the biomedical domain. By proposing template based question generation method and introducing pre-training tasks aligned with the downstream retrieval task, NRs have made substantial strides. The PolyDPR model, which encodes each context into multiple vectors, has been particularly effective, outperforming traditional methods like BM25 in certain retrieval settings. Finally, MedCPT (Jin et al. 2023) employs contrastive pre-training to enhance zero-shot retrieval for biomedical information. Leveraging large collection of user click logs from PubMed, MedCPT utilizes contrastive learning to train an integrated retriever and re-ranker model. This methodology has set new state-of-the-art benchmarks, outperforming several Baselines, including larger models like GPT-3-sized cpt-text-XL. Biomedical Language Models The development of domain-specific language models has addressed the unique challenges posed by biomedical text. Models like SciFive (Phan et al. 2021), BioMegatron (Shin et al. 2020), and PubMedBERT (Gu et al. 2021) have been trained on extensive biomedical corpora, enabling them to better understand specialized language and concepts. Additionally, other models such as BioBERT (Lee et al. 2019), PMC-LLaMA (Wu et al. 2024), ELECTRAMed (Miolo, Mantoan, and Orsenigo 2021), BioBART (Yuan et al. 2022), and BioMedLM (Bolton et al. 2024) have significantly advanced biomedical text mining and natural language processing (NLP). Recent advancements in biomedical language modeling have explored graph-based approaches to represent biomedical literature as knowledge graphs, effectively capturing complex relationships among entities and concepts. These knowledge graphs enhance accuracy by providing structured framework that reflects the intricate interconnections inherent in biomedical data. Works of (Saxena, Tripathi, and Talukdar 2020),(Yasunaga, Leskovec, and Liang 2022) and (Yasunaga et al. 2022) show significant improvements in question-answering systems and biomedical text understanding using knowledge graphs and multi-hop frameworks. Methodology First, we construct rich, 2-hop citation neighborhood around set of seed documents. Second, we perform novel hard-negative mining technique by converting these citation graphs into dense semantic graphs and performing series of diverse, stochastic traversals to find documents that are semantically close but not directly relevant. We provide an overview of our entire pipeline in Figure 1. citation graph. The is large-scale, process seed collection of PubMed abstracts Data Curation: 2-Hop Citation Neighborhood Construction loThe foundation of our dataset begins with calized from the uiyunkim-hub/pubmed-abstract on Hugging Face. Our goal was to generate final dataset of approximately 20,000 query-positive pairs, each with corresponding set of high-quality hard negatives. To ensure that our selected corpus of 20,000 documents is fair representation of the much larger PubMed database we plot the embedding distributions in Appendix B, Figure 2a. dataset To create candidate pool for these negatives, we performed the following steps for each seed article, which we designate as the positive document (P0): 1-Hop Citation Retrieval: Using the PubMed Identifier (PMID) of P0, we employed the pubmed-parser library to query the NCBI E-utilities API and retrieve list of all PMIDs cited by P0. These form the 1-hop neighborhood (C1). We then fetched the abstract for each paper in C1. 2-Hop Citation Retrieval: For each paper P1 C1, we repeated the process, fetching the PMIDs of all papers it cites. This collection of PMIDs forms the 2-hop neighborhood (C2). We then fetched the abstract for each paper in C2. Data Aggregation: The final curated data structure for each positive document P0 consists of its own abstract, list of all 1-hop abstracts, and list of all 2-hop abstracts. To ensure sufficiently rich neighborhood for mining, we only retained records where abstracts could be successfully retrieved. This data collection was heavily parallelized across 80 worker processes to manage the high volume of API calls to NCBI. The result is JSONL file containing 20,000 complex objects, each representing positive document and its extensive 2-hop citation context. Hard-Negative Mining via Diverse Semantic Traversal With the 2-hop citation neighborhoods established, we proceeded to the core of our hard-negative mining strategy. To enhance diversity and prevent the model from overfitting to single type of negative, our approach, detailed in Algorithm 1, transforms the structural citation graph into semantic space and explores it using multiple, stochastic paths. Algorithm 1: Hard Negative Mining via Diverse Semantic Traversal Require: Apos: Abstract of the positive document. Acands: Set of candidate abstracts from citation hops. Npaths, Lpath, Ksample: Traversal control parameters. Ensure: Step 1: Construct semantic graph of documents. GENERATEQUERY(Apos) Sgraph BUILDSIMILARITYGRAPH(Acands) Lnegs: diverse list of hard negative abstracts. 1: procedure MINEHARDNEGATIVES(Apos, Acands) 2: 3: 4: 5: 6: 7: 8: Istart FINDTOPNSTARTS(Q, Acands, Npaths) Lnegs , Vvisited Step 2: Initiate traversals from query-relevant starts. Step 3: Perform stochastic walks to find diverse negatives. for each istart in Istart do icurr istart for 1 to Lpath do if icurr Vvisited then break Add Acands[icurr] to Lnegs and Vvisited Select next node: top-K unvisited neighbors, sampled probabilistically by similarity. ItopK GETTOPKUNVISITEDNEIGHBORS {icurr, Sgraph, Ksample, Vvisited} icurr SAMPLEPROBABILISTICALLY {ItopK, Sgraph[icurr, ItopK]} Step 4: Add random negative for robustness. Add one random, unvisited abstract to Lnegs. return Unique(Lnegs) 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19:"
        },
        {
            "title": "The mining process unfolds in three steps for each of the",
            "content": "20,000 curated data points: Query Generation: We first generate synthetic query from the positive abstract (Apositive) using the Doc2Query o V F i A S O S U A A e - m C D a P N 0 2 0 2 - c A p R F . o M i . A TAS-B 66M 0.481 0.319 0.643 0.149 0.835 0.434 0.221 0.463 0.315 0.384 0.162 0.584 0.700 0.300 0.399 0.399 R-GPL 66M 0.760 0.342 0.678 0.162 0.808 0.464 0.231 0.504 0.348 0.381 0.264 0.567 0.791 0.336 0.474 0.474 GPL 66*5M 0.700 0.345 0.674 0.169 0.832 0.483 0.227 0.467 0.345 0.360 0.266 0.636 0.758 0.344 0.472 0.472 DPR 110M 0.332 0.189 0.318 0.077 0.248 0.175 0.148 0.474 0.153 0.263 0.131 0.456 0.562 0.112 0.274 0.274 ANCE 110M 0.650 0.230 0.507 0.122 0.852 0.415 0.198 0.446 0.296 0.281 0.240 0.584 0.669 0.295 0.414 0.414 Contriever 110M 0.596 0.328 0.677 0.165 0.865 0.446 0.237 0.495 0.284 0.413 0.230 0.638 0.758 0.329 0.463 0.463 110M 0.677 0.305 0.671 0.145 0.854 0.233 0.184 0.524 0.350 0.392 0.202 0.593 0.771 0.317 0.445 0.445 ColBERT ColBERTv2 110M 0.738 0.338 0.693 0.154 0.852 0.463 0.176 0.562 0.359 0.446 0.278 0.667 0.785 0.356 0.490 0.490 LexMAE 0.487 0.500 0.219 0.562 DRAGON+ 110M 0.759 0.339 0.679 0.159 0.875 0.469 0.227 0.537 0.354 0.414 0.263 0.662 0.781 0.359 0.491 0.491 110M 0.748 0.357 0.710 0.158 0.814 0.509 0.233 0.586 SpladeV3 0.517 110M 0.710 0.334 0.693 0.158 0.838 0.479 0.235 0.521 0.341 0.435 0.272 0.684 0.786 0.336 0.487 0.487 SpladeV2 110M 0.772 0.308 0.653 0.133 0.847 0.433 0.232 0.518 0.297 0.356 0.219 0.635 0.774 0.325 0.464 0.464 RetroMae 220M 0.610 0.310 0.644 0.143 0.830 0.493 0.175 0.358 0.347 0.328 0.182 0.534 0.669 0.308 0.424 0.424 GenQ GTRBase 110M 0.539 0.308 0.600 0.149 0.881 0.511 0.241 0.495 0.357 0.347 0.205 0.535 0.660 0.349 0.441 0.441 GTR-Large 335M 0.557 0.329 0.639 0.158 0.890 0.525 0.262 0.547 0.384 0.391 0.219 0.579 0.712 0.424 0.473 0.473 1.2B 0.580 0.343 0.635 0.159 0.890 0.531 0.270 0.559 0.388 0.396 0.230 0.591 0.717 0.444 0.481 0.481 GTRxl 4.8B 0.500 0.342 0.662 0.161 0.892 0.540 0.267 0.568 0.399 0.408 0.256 0.599 0.740 0.467 0.486 0.486 GTRxxl 110M 0.763 0.347 0.710 0.159 0.450 0.293 0.692 0.796 0.374 0.424 0.290 0.716 0.800 0.352 - - - - - BiCAsmall BiCABase 33M 0.661 0.347 0.727 0.214 0.880 0.555 0.264 0.502 0.399 0.391 0.222 0.637 0.815 0.393 0.501 0.501 110M 0.684 0.378 0.762 0.231 0.882 0.571 0.279 0.529 0.428 0.411 0.220 0.657 0.815 0.407 0.518 0.518 Table 1: Evaluation on all 14 BEIR tasks in zero-shot setting using nDCG@10. Bold and underline denote the best and second-best scores, respectively. (doc2query/all-t5-base-v1) model (Nogueira et al. 2019). This creates realistic search query that the positive document is expected to be relevant for. dense, graph. All Dense Graph Construction: We semantically-weighted then construct aba stracts from the 1-hop and 2-hop neighborhoods are encoded into high-dimensional vectors using the Pubmedbert-base-embeddings (NeuML 2025). We compute complete pairwise cosine similarity matrix between all documents in the 1-hop and 2-hop pools. Diverse Semantic Traversal: With the dense graph constructed, we identify varied set of hard negatives. The process is designed to be robust and avoid overfitting: Multiple Start Points: Instead of one, we initiate three separate traversal paths, starting from the three 1-hop documents most semantically similar to the generated query. Stochastic Path Selection: At each step of traversal, rather than taking purely greedy step to the single most similar node, we perform weighted random sampling from the top five most similar, unvisited nodes. This stochasticity ensures wider exploration of the semantic space. Global Visited Set: single global set of visited nodes is maintained across all traversals for given query, guaranteeing that each path explores unique documents and maximizing the diversity of the final negative set. Random Negative Augmentation: Finally, to further improve training stability, one additional negative is selected uniformly at random from the remaining pool of unvisited documents. The final output is dataset of approximately 20,000 entries, each containing query, single positive abstract, and diverse list of hard negatives (averaging 6.5 per query). This results in total corpus of approximately 150,000 documents, specifically curated to train and evaluate retrieval models on their ability to make fine-grained relevance distinctions."
        },
        {
            "title": "Experiments",
            "content": "Fine-Tuning We fine tune two models the GTEsmall and the GTEBase (Li et al. 2023). GTEbase (110M parameters, 768-dim) and GTEsmall (33M parameters, 384-dim) are BERT-based embedding models trained with multi-stage contrastive learning, balancing accuracy with efficiency. We describe our choice of fine-tuning for only 20 steps in Section show in Figure 2b of the Appendix. The fine-tuning was conducted on single NVIDIA V100 GPU (32GB), enabling efficient handling of large batch sizes and complex models without memory constraints. The Multiple Negative Ranking Loss (MNR) function (Henderson et al. 2017) is used and defined as: (cid:32) LM = log exp(q d+) exp(q d+) + (cid:80)K i=1 exp(q ) (cid:33) where denotes the query embedding, d+ the positive document embedding, the i-th negative document embedi ding, and the number of negatives. Evaluation BEIR We evaluate our models on fourteen BEIR (Thakur et al. 2021) datasets in zero-shot setting. Details of the dataset is provided in Appendix D, Table 10. Our primary evaluation metric is Normalized Discounted Cumulative Gain at 10 (nDCG@10), which assesses the ranking quality of the top 10 retrieved documents. The comprehensive results, comparing our models against wide range of existing methods, are presented in Table 1. We also provide the improvements over the base GTE models in Appendix and in Appendix Table 8. As shown in Table 1, our BiCABase model (110M parameters) achieves the highest average nDCG@10 score of 0.518 across all fourteen tasks, setting new state-of-the-art on BEIR and surpassing significantly larger models such as GTR xxl (4.8B parameters, 0.486). BiCABase excels in both biomedical and general domains, leading on NFCORPUS (0.378), SCIFACT (0.762), SCIDOCS (0.231), ARGUANA (0.571), CLIMATE-FEVER (0.279), and CQADUP (0.428), while tying for the highest on FEVER (0.815) and performing strongly on HOTPOTQA (0.657). Our smaller BiCAsmall model (33M parameters) also demonstrates remarkable performance, achieving an average nDCG@10 of 0.501, ranking second overall and outperforming many larger baselines, including GTR xxl. Notably, it secures the top score on FEVER (0.815) and second-highest on SCIDOCS (0.214), ARGUANA (0.555), and CQADUP (0.399). Its ability to rival or surpass models up to 145 times larger highlights the parameter efficiency of our approach. LOTTE We evaluate our models on long-tailed topics, which refer to specific and less frequently searched queries, using four sub-topics from the LoTTE benchmark (Santhanam et al. 2022): Science, Writing, Recreation, and Lifestyle.Details of the dataset is provided in Appendix D, Table 9. As detailed in Table 2, we report zero-shot Success@5 on its test set. The benchmark includes two query formats: concise Search queries from GooAQ logs and more descriptive Forum queries from StackExchange user questions. Our BiCABase model sets new state-of-the-art, achieving the highest Success@5 across all four categories for both LoTTE query types. On Search queries, it scores 87.7 on Lifestyle, 81.6 on Writing, 79.7 on Recreation, and 60.6 on Science. On the more challenging Forum queries, it attains 84.0 on Lifestyle, 80.8 on Writing, 77.5 on Recreation, and 47.1 on Science. The smaller BiCAsmall model consistently ranks second, with Search scores of 86.8 on Lifestyle, 76.1 on Recreation and 58.5 on Science, and Forum scores of 82.2 on Lifestyle, 78.1 on Writing and 75.6 on Recreation, demonstrating strong performance and parameter efficiency on long-tailed topics. Latency To assess model efficiency, we measured latency using the TAS-B setup on single NVIDIA V100 with 32GB memory. We encoded 10,000 MS MARCO passages and indexed them with FAISS (IndexFlatIP). We then timed two steps: query encoding and retrieval of top 1000 results. Tests were run on query batches of size 1, 10, and 2000. We report average and 99th percentile latencies in milliseconds over 100 iterations (1 and 10) or 10 iterations (2000). Table 3 compares BiCABase (110M), BiCAsmall (33M), ColBERTv2, RetroMAE, and SpladeV3. For batch size 1, BiCAsmall is fastest overall with 13 ms total and 4 ms retrieval. ColBERTv2 has the quickest encoding at 8 ms and total of 15 ms. The others average 16 ms, with BiCABase showing slightly higher tail times. At batch size 10, BiCAsmall again leads in total time (19 ms), driven by retrieval at 5 ms. ColBERTv2, RetroMAE, and SpladeV3 encode slightly faster (11 ms vs 14 ms for BiCAsmall). ColBERTv2 has the best tail latency at 23 ms, while SpladeV3 peaks at 32 ms. At batch size 2000, BiCAsmall outperforms all others with 994 ms total (554 ms encoding, 441 ms retrieval). RetroMAE follows at 1837 ms, then ColBERTv2 (1844 ms) and SpladeV3 (1847 ms). BiCABase is slowest at 1904 ms."
        },
        {
            "title": "Effect of Traversal Parameters",
            "content": "To determine appropriate values for the traversal parameters, we conduct an ablation study varying the Number of Traversal Paths (Npaths) and the Length of the Path (Lpath) in the range of 15. For each study, we fix one parameter at 3 while varying the other, using bert-base fine-tuned for 1 epoch on the entire corpus with batch size of 16 and an MNR loss. As shown in Table 4, the choice of Npaths = 3 and Lpath = 3 consistently provides strong balance across datasets, achieving the highest overall average performance (0.2739). While other configurations occasionally yield the best score on single dataset (e.g., Npaths = 5 for SCIFACT or Lpath = 1 for ArguAna), they underperform on others, leading to lower overall average. We therefore select Npaths = 3 and Lpath = 3 as the default configuration for our final results, as it offers the most stable and robust performance across benchmarks."
        },
        {
            "title": "Robustness and Scalability",
            "content": "To examine the effect of training data size, we fine-tuned bert-base-uncased (Devlin et al. 2019) on randomly sampled subsets of our 20,000-record dataset (1k, 5k, 10k, 15k, and full). Each subset reserved 10% for validation. Models were trained for up to 1 epoch using MNR Loss with batch size of 16, applying early stopping based on highest triplet accuracy on validation. The best checkpoints were evaluated zero-shot on three biomedical tasks and one BEIR task. Results in Table 5 show clear positive correlation between data size and retrieval performance. Corpus ColBERT BM25 ANCE RocketQAv2 SPLADEv2 ColBERTv2 BiCAsmall BiCABase LoTTE Search Test Queries (Success@5) Writing Recreation Science Lifestyle 74.7 68.5 53.6 80.2 60.3 56.5 32.7 63.8 74.4 64.7 53.6 82. LoTTE Forum Test Queries (Success@5) Writing Recreation Science Lifestyle 71.0 65.6 41.8 73.0 64.0 55.4 37.1 60.6 68.8 63.8 36.5 73.1 78.0 72.1 55.3 82. 71.5 65.7 38.0 73.7 77.1 69.0 55.4 82.3 73.0 67.1 43.7 74.0 80.1 72.3 56.7 84.7 76.3 70.8 46.1 76.9 79.8 76.1 58.5 86. 78.1 75.6 44.6 82.2 81.6 79.7 60.6 87.7 80.8 77.5 47.1 84.0 Table 2: Retrieval performance (Success@5) of different models on LoTTE search and forum queries on the test set. Bold represents the best score and underline represents the second best score. Model Batch Size Encoding (ms) Retrieval (ms) Total (ms) BiCABase"
        },
        {
            "title": "BiCAsmall",
            "content": "ColBERTv"
        },
        {
            "title": "RetroMAE",
            "content": "SpladeV3 Avg. 9 11 1292 9 14 554 8 11 1249 9 11 9 11 1250 99th p. Avg. 99th p. Avg. 99th p. 14 16 1475 11 19 850 9 13 1423 11 13 1403 11 15 1437 7 9 4 5 441 7 9 594 7 9 591 7 9 598 9 10 622 4 5 7 10 612 8 12 607 9 13 609 16 20 1904 13 19 994 15 20 16 20 1837 16 21 1847 21 25 2082 14 24 1341 16 23 2004 20 25 19 32 2045 1 10 2000 1 10 2000 1 10 2000 1 10 2000 1 10 Table 3: Latency analysis for BiCABase, BiCAsmall, and other baselines on V100 (32GB) GPU. Cell colors highlight timings from lowest (lightest orange) to highest (darkest orange) for each metric across models within the same batch size. All times are in milliseconds (ms). Encoding refers to query encoding time, and Retrieval to top-1000 passage retrieval from FAISS index with 10,000 passages (MS MARCO). Performance of Different Architectures To assess generalizability, we fine-tune models for maximum of one epoch with early stopping (patience=3), where evaluation is performed every 10 steps. We experiment with two pretrained checkpoints: e5-base-V21 (Wang et al. 2022) and DistilBERT model2 (Sanh et al. 2020) fine-tuned on MS MARCO. For evaluation, we select five tasks from the BEIR benchmark three from the biomedical domain (NFCorpus, SciDocs, SciFact) and two from non-biomedical domains (ArguAna, FiQA). Table 6 shows the performance gains of fine-tuning the models on our corpus and Table 7 shows the number of fine tuning steps selected for the chosen models, after which we do zero-shot evaluation on BEIR. 1https://huggingface.co/intfloat/e5-base-v2 2https://huggingface.co/GPL/msmarco-distilbert-margin-mse We see consistent improvements in using our corpus for fine-tuning over different architectures. DistilBERT sees an average improvement of 1.56 points and e5-base-v2 sees an improvement of 0.84 points."
        },
        {
            "title": "Conclusions",
            "content": "In this work, we present BiCABase and BiCAsmall, two dense retrieval models designed to address the unique challenges of biomedical and general-domain information retrieval. At the core of our approach is novel hard negative mining strategy that exploits multi-hop citation chains extracted from PubMed. This citation-aware technique provides semantically challenging yet relevant negative examples, encouraging the models to learn fine-grained distinctions essential for high-precision retrieval. SCIFACT ArguAna NFC SCIDOCS Npaths Lpath Ablation on Number of Traversals (fixed Lpath = 3) 0.5114 0.3934 0.4392 0.5180 0.1803 0.1390 0.1400 0. 0.1201 0.0984 0.1073 0.1230 3 3 3 3 1 2 4 5 Ablation on Path Length (fixed Npaths = 3) 0.1875 0.1299 0.1987 0.1861 0.1820 0.1245 0.0965 0.1234 0.1202 0.1183 1 2 3 4 3 3 3 3 3 0.5053 0.3960 0.5156 0.5102 0.5058 FIQA Average 0.1301 0.0860 0.1030 0.1178 0.1240 0.1062 0.1225 0.1324 0.1110 0.2679 0.2068 0.2184 0. 0.2725 0.2041 0.2739 0.2669 0.2580 0.3974 0.3174 0.3024 0.4190 0.4211 0.2920 0.4094 0.3854 0.3730 Table 4: Ablation study on the number of traversals (Npaths) and path length (Lpath). All models are based on BERT-base fine-tuned for one epoch. We report NDCG@10 scores and highlight the best result in each column in bold. Dataset Baseline 1k 5k 10k 15k Full (20k) NFCorpus SciDocs SciFact ArguAna 0.043 0.028 0.130 0. 0.082 0.171 0.164 0.171 0.061 0.117 0.116 0.114 0.262 0.469 0.468 0.492 0.384 0.364 0.385 0.405 0.185 0.121 0.493 0.444 results ablation 5: Scaling for fine-tuning Table bert-base-uncased on our citation-aware negatives. Scores are nDCG@10 on biomedical BEIR tasks. The baseline represents zero-shot performance without any fine-tuning. The results show consistent performance improvement as the amount of training data increases. Dataset NFCorpus SciFact SCIDOCS ArguAna FiQA DBbase DBfine-tune 25.2+0.4 55.8+4.2 14.9+1.5 39.9+0.2 19.7+1.5 24.8 51.6 13.4 39.7 18.2 Average +1.56 E5base 35.3 71.0 18.3 51.6 37.3 E5fine-tune 34.80.5 71.9+0.9 20.4+2.1 52.7+1.1 37.9+0.6 +0. 6: NDCG@10 between Table DistilBERT (DB) and E5 models across BEIR datasets. Superscripts indicate absolute improvement of fine-tuned models over base versions. comparison (%)"
        },
        {
            "title": "Model",
            "content": "No. Fine-Tuning Steps DistilBERT e5-base-v2 1150 290 Table 7: Number fine-tuning steps on our constructed corpus before doing zero-shot evaluation on BEIR Through extensive experiments on the BEIR benchmark, BiCABase demonstrated strong performance across both biomedical and non-biomedical tasks, consistently outperforming several larger state-of-the-art models. Notably, it achieved the highest average nDCG@10 scores in both domains, indicating its effectiveness and generalizability. Despite its smaller size, BiCAsmall also delivered competitive results, often closely trailing BiCABase while offering substantially lower inference latency, making it well-suited for real-time and resource-constrained applications. Evaluations on the LoTTE dataset further highlighted the robustness of our models in handling retrieval over longtailed, diverse topics. BiCABase led across all sub-domains, while BiCAsmall ranked consistently among the top performers, demonstrating the broad applicability and efficiency of our approach. Limitations The citation-aware hard negative mining strategy improves retrieval performance, but faces challenges in scalability and efficiency. Constructing multi-hop citation chains requires iterative PubMed API requests for abstracts and cited PMIDs, process hindered by rate limits, network latency, and the parsing of large text data. As result, generating large training sets can take week(s), depending on the number of seed documents and citation depth. Furthermore, our current work is restricted to PubMed; extending this approach to other sources such as Wikipedia, where scientific and technical articles contain rich citation trails, may enable construction of semantically meaningful hard negatives for general-domain retrieval while preserving citationaware principles. We acknowledge that our latency evaluation setup may not fully reflect the efficiency advantages of the ColBERTv2 model. However, we adopted this configuration to ensure uniform and fair comparison across all systems. References Allan, J.; Aslam, J.; Belkin, N.; Buckley, C.; Callan, J.; Croft, B.; Dumais, S.; Fuhr, N.; Harman, D.; Harper, D. J.; Hiemstra, D.; Hofmann, T.; Hovy, E.; Kraaij, W.; Lafferty, J.; Lavrenko, V.; Lewis, D.; Liddy, L.; Manmatha, R.; McCallum, A.; Ponte, J.; Prager, J.; Radev, D.; Resnik, P.; Robertson, S.; Rosenfeld, R.; Roukos, S.; Sanderson, M.; Schwartz, R.; Singhal, A.; Smeaton, A.; Turtle, H.; Voorhees, E.; Weischedel, R.; Xu, J.; and Zhai, C. 2003. Challenges in information retrieval and language modeling: report of workshop held at the center for intelligent information retrieval, University of Massachusetts Amherst, September 2002. SIGIR Forum, 37(1): 3147. Bolton, E.; Venigalla, A.; Yasunaga, M.; Hall, D.; Xiong, B.; Lee, T.; Daneshjou, R.; Frankle, J.; Liang, P.; Carbin, M.; and Manning, C. D. 2024. BioMedLM: 2.7B Parameter Language Model Trained On Biomedical Text. arXiv:2403.18421. Bondarenko, A.; Frobe, M.; Beloucif, M.; Gienapp, L.; Ajjour, Y.; Panchenko, A.; Biemann, C.; Stein, B.; Wachsmuth, H.; Potthast, M.; et al. 2020. Overview of Touche 2020: argument retrieval. In International Conference of the Cross-Language Evaluation Forum for European Languages, 384395. Springer. Boteva, V.; Gholipour, D.; Sokolov, A.; and Riezler, S. 2016. Full-Text Learning to Rank Dataset for Medical Information Retrieval. In Ferro, N.; Crestani, F.; Moens, M.-F.; Mothe, J.; Silvestri, F.; Di Nunzio, G. M.; Hauff, C.; and Silvello, G., eds., Advances in Information Retrieval, volume 9626, 716722. Cham: Springer International Publishing. ISBN 978-3-319-30670-4 978-3-319-30671-1. Series Title: Lecture Notes in Computer Science. Breuer, T.; Kreutz, C. K.; Schaer, P.; and Tunger, D. 2023. Bibliometric Data Fusion for Biomedical Information Retrieval. In 2023 ACM/IEEE Joint Conference on Digital Libraries (JCDL), 107118. Cohan, A.; Feldman, S.; Beltagy, I.; Downey, D.; and Weld, D. 2020. SPECTER: Document-level Representation Learning using Citation-informed Transformers. In Jurafsky, D.; Chai, J.; Schluter, N.; and Tetreault, J., eds., Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, 22702282. Online: Association for Computational Linguistics. Devlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Burstein, J.; Doran, C.; and Solorio, T., eds., Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), 41714186. Minneapolis, Minnesota: Association for Computational Linguistics. Diggelmann, T.; Boyd-Graber, J.; Bulian, J.; Ciaramita, M.; and Leippold, M. 2021. CLIMATE-FEVER: Dataset for Verification of Real-World Climate Claims. arXiv:2012.00614. Gu, Y.; Tinn, R.; Cheng, H.; Lucas, M.; Usuyama, N.; Liu, X.; Naumann, T.; Gao, J.; and Poon, H. 2021. DomainSpecific Language Model Pretraining for Biomedical Natural Language Processing. ArXiv:2007.15779. Hasibi, F.; Nikolaev, F.; Xiong, C.; Balog, K.; Bratsberg, S. E.; Kotov, A.; and Callan, J. 2017. DBpedia-Entity v2: Test Collection for Entity Search. In Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 17, 12651268. New York, NY, USA: Association for Computing Machinery. ISBN 9781450350228. Henderson, M.; Al-Rfou, R.; Strope, B.; hsuan Sung, Y.; Lukacs, L.; Guo, R.; Kumar, S.; Miklos, B.; and Kurzweil, R. 2017. Efficient Natural Language Response Suggestion for Smart Reply. arXiv:1705.00652. Hoogeveen, D.; Verspoor, K. M.; and Baldwin, T. 2015. CQADupStack: Benchmark Data Set for Community Question-Answering Research. In Proceedings of the 20th Australasian Document Computing Symposium, ADCS 15. New York, NY, USA: Association for Computing Machinery. ISBN 9781450340403. Jin, Q.; Kim, W.; Chen, Q.; Comeau, D. C.; Yeganova, L.; Wilbur, W. J.; and Lu, Z. 2023. MedCPT: Contrastive Pretrained Transformers with large-scale PubMed search logs for zero-shot biomedical information retrieval. Bioinformatics, 39(11): btad651. Kwiatkowski, T.; Palomaki, J.; Redfield, O.; Collins, M.; Parikh, A.; Alberti, C.; Epstein, D.; Polosukhin, I.; Devlin, J.; Lee, K.; Toutanova, K.; Jones, L.; Kelcey, M.; Chang, M.- W.; Dai, A. M.; Uszkoreit, J.; Le, Q.; and Petrov, S. 2019. Natural Questions: Benchmark for Question Answering Research. Transactions of the Association for Computational Linguistics, 7: 452466. Lee, J.; Yoon, W.; Kim, S.; Kim, D.; Kim, S.; So, C. H.; and Kang, J. 2019. BioBERT: pre-trained biomedical language representation model for biomedical text mining. Bioinformatics, 36(4): 12341240. Li, L.; Zhang, X.; Zhou, X.; and Liu, Z. 2024. AutoMIR: Effective Zero-Shot Medical Information Retrieval without Relevance Labels. arXiv:2410.20050. Li, Z.; Zhang, X.; Zhang, Y.; Long, D.; Xie, P.; and Zhang, M. 2023. Towards General Text Embeddings with Multistage Contrastive Learning. arXiv:2308.03281. Luo, M.; Mitra, A.; Gokhale, T.; and Baral, C. 2022. Improving Biomedical Information Retrieval with Neural Retrievers. Proceedings of the AAAI Conference on Artificial Intelligence, 36(10): 1103811046. Maia, M.; Handschuh, S.; Freitas, A.; Davis, B.; McDermott, R.; Zarrouk, M.; and Balahur, A. 2018. WWW18 Open Challenge: Financial Opinion Mining and Question Answering. In Companion Proceedings of the The Web Conference 2018, WWW 18, 19411942. Republic and Canton of Geneva, CHE: International World Wide Web Conferences Steering Committee. ISBN 9781450356404. Manning, C. D. 2009. An introduction to information retrieval. Cambridge: Cambridge University Press. Miolo, G.; Mantoan, G.; and Orsenigo, C. 2021. ELECTRAMed: new pre-trained language representation model for biomedical NLP. arXiv:2104.09585. NeuML. 2025. NeuML/pubmedbert-base-embeddings Hugging Face. [Online; accessed 2025-07-31]. Nogueira, R.; Yang, W.; Lin, J.; and Cho, K. 2019. Document Expansion by Query Prediction. ArXiv:1904.08375. In Gurevych, I.; and Miyao, Y., eds., Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 241251. Melbourne, Australia: Association for Computational Linguistics. Wadden, D.; Lin, S.; Lo, K.; Wang, L. L.; van Zuylen, M.; Cohan, A.; and Hajishirzi, H. 2020. Fact or Fiction: Verifying Scientific Claims. In Webber, B.; Cohn, T.; He, Y.; and Liu, Y., eds., Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), 75347550. Online: Association for Computational Linguistics. Wang, L.; Yang, N.; Huang, X.; Jiao, B.; Yang, L.; Jiang, D.; Majumder, R.; and Wei, F. 2022. Text Embeddings by Weakly-Supervised Contrastive Pre-training. arXiv preprint arXiv:2212.03533. Wu, C.; Lin, W.; Zhang, X.; Zhang, Y.; Xie, W.; and Wang, Y. 2024. PMC-LLaMA: toward building open-source language models for medicine. Journal of the American Medical Informatics Association, 31(9): 18331843. Yang, Z.; Qi, P.; Zhang, S.; Bengio, Y.; Cohen, W.; Salakhutdinov, R.; and Manning, C. D. 2018. HotpotQA: Dataset for Diverse, Explainable Multi-hop Question Answering. In Riloff, E.; Chiang, D.; Hockenmaier, J.; and Tsujii, J., eds., Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, 23692380. Brussels, Belgium: Association for Computational Linguistics. Yang, Z.; Shao, Z.; Dong, Y.; and Tang, J. 2024. TriSampler: Better Negative Sampling Principle for Dense Retrieval. arXiv:2402.11855. Yasunaga, M.; Bosselut, A.; Ren, H.; Zhang, X.; Manning, C. D.; Liang, P.; and Leskovec, J. 2022. Deep Bidirectional Language-Knowledge Graph Pretraining. In Neural Information Processing Systems (NeurIPS). Yasunaga, M.; Leskovec, J.; and Liang, P. 2022. LinkBERT: Pretraining Language Models with Document Links. In Muresan, S.; Nakov, P.; and Villavicencio, A., eds., Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 80038016. Dublin, Ireland: Association for Computational Linguistics. Yuan, H.; Yuan, Z.; Gan, R.; Zhang, J.; Xie, Y.; and Yu, S. 2022. BioBART: Pretraining and Evaluation of BiomedIn Proceedings of the ical Generative Language Model. 21st Workshop on Biomedical Language Processing, 97 109. Dublin, Ireland: Association for Computational Linguistics. Phan, L. N.; Anibal, J. T.; Tran, H.; Chanana, S.; Bahadroglu, E.; Peltekian, A.; and Altan-Bonnet, G. 2021. SciFive: text-to-text transformer model for biomedical literature. ArXiv:2106.03598. Sanh, V.; Debut, L.; Chaumond, J.; and Wolf, T. 2020. DistilBERT, distilled version of BERT: smaller, faster, cheaper and lighter. ArXiv:1910.01108 version: 4. Santhanam, K.; Khattab, O.; Saad-Falcon, J.; Potts, C.; and Zaharia, M. 2022. ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction. In Carpuat, M.; de Marneffe, M.-C.; and Meza Ruiz, I. V., eds., Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 37153734. Seattle, United States: Association for Computational Linguistics. Saxena, A.; Tripathi, A.; and Talukdar, P. 2020. Improving Multi-hop Question Answering over Knowledge Graphs using Knowledge Base Embeddings. In Jurafsky, D.; Chai, J.; Schluter, N.; and Tetreault, J., eds., Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, 44984507. Online: Association for Computational Linguistics. Sayers, E. W.; Barrett, T.; Benson, D. A.; Bolton, E.; Bryant, S. H.; Canese, K.; Chetvernin, V.; Church, D. M.; DiCuccio, M.; Federhen, S.; Feolo, M.; Fingerman, I. M.; Geer, L. Y.; Helmberg, W.; Kapustin, Y.; Landsman, D.; Lipman, D. J.; Lu, Z.; Madden, T. L.; Madej, T.; Maglott, D. R.; MarchlerBauer, A.; Miller, V.; Mizrachi, I.; Ostell, J.; Panchenko, A.; Phan, L.; Pruitt, K. D.; Schuler, G. D.; Sequeira, E.; Sherry, S. T.; Shumway, M.; Sirotkin, K.; Slotta, D.; Souvorov, A.; Starchenko, G.; Tatusova, T. A.; Wagner, L.; Wang, Y.; Wilbur, W. J.; Yaschenko, E.; and Ye, J. 2011. Database resources of the National Center for Biotechnology Information. Nucleic Acids Research, 39(suppl 1): D38D51. Shin, H.-C.; Zhang, Y.; Bakhturina, E.; Puri, R.; Patwary, M.; Shoeybi, M.; and Mani, R. 2020. BioMegatron: Larger Biomedical Domain Language Model. ArXiv:2010.06060. Thakur, N.; Reimers, N.; Ruckle, A.; Srivastava, A.; and Gurevych, I. 2021. BEIR: Heterogeneous Benchmark for Zero-shot Evaluation of Information Retrieval Models. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track. Thorne, J.; Vlachos, A.; Christodoulopoulos, C.; and Mittal, A. 2018. FEVER: Large-scale Dataset for Fact Extraction and VERification. In Walker, M.; Ji, H.; and Stent, A., eds., Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), 809819. New Orleans, Louisiana: Association for Computational Linguistics. Voorhees, E.; Alam, T.; Bedrick, S.; Demner-Fushman, D.; Hersh, W. R.; Lo, K.; Roberts, K.; Soboroff, I.; and Wang, L. L. 2021. TREC-COVID: constructing pandemic information retrieval test collection. SIGIR Forum, 54(1): 1:1 1:12. Wachsmuth, H.; Syed, S.; and Stein, B. 2018. Retrieval of the Best Counterargument without Prior Topic Knowledge. Improvement over GTE Models Table 8 presents the retrieval performance comparison between our BiCA models and the corresponding GTE (Li et al. 2023) baselines across fourteen datasets. BiCAsmall achieves consistent improvements over GTEsmall, with an average gain of 5.8 points. BiCABase shows even stronger results, outperforming GTEBase by an average of 6.8 points. These gains highlight the effectiveness of BiCAs training strategy in enhancing retrieval quality, particularly on challenging datasets such as ArguAna, NQ, HotpotQA, and Climate-Fever. Dataset GTEsmall ArguAna Climate-Fever CQADupStack DBPedia Fever FiQA HotpotQA NFCorpus NQ Quora Scidocs SciFact Touche-2020 Trec-Covid Average 41.6 21.4 38.1 33.5 71.3 37.0 49.3 34.9 32.0 86.1 21.5 72.7 17.7 61.8 BiCASmall GTEbase 55.5+13.9 26.4+5.0 39.9+1.8 39.1+5.6 81.5+10.2 39.3+2.3 63.7+14.4 34.70.2 50.2+18.2 88.0+1.9 21.40.1 72.70.0 22.2+4.5 66.1+4.3 41.0 21.0 39.9 33.2 72.7 36.9 50.8 36.2 35.3 85.0 22.5 74.1 18.2 64.0 BiCABase 57.1+16.1 27.9+6.9 42.8+2.9 41.1+7.9 81.5+8.8 40.7+3.8 65.7+14.9 37.8+1.6 52.9+17.6 88.2+3.2 23.1+0.6 76.2+2.1 22.0+3.8 68.4+4.4 +5. +6.8 8:"
        },
        {
            "title": "Comparison",
            "content": "of GTEsmall/GTEbase Table vs. BiCASmall/BiCABase on 14 tasks. All scores have been multiplied by 100, and the gain next to each BiCA score is rounded to one decimal. The last row reports the average gain across tasks. Data Selection To ensure that our selected training corpus of 20,000 documents is representative of the entire dataset 3 that was available we plot the distribution of our corpus and the entire corpus as seen in Figure 2a. We use the NeuML/pubmedbertbase-embeddings-2M 4 model to extract the embeddings. Choice of fine tuning steps To determine the optimal fine-tuning duration, we evaluated performance on held-out validation set using 80%/20% split of the constructed corpus. As shown in Figure 2b, we observed that our highly informative negatives deliver their signal with remarkable speed. Peak performance was consistently achieved at just 20 training steps. This demonstrates the extreme efficiency of our citation-aware negatives. Consequently, we selected this optimal 20-step checkpoint for all zero-shot evaluations on BEIR and LoTTE (Santhanam et al. 2022). 3huggingface.co/datasets/uiyunkim-hub/pubmed-abstract 4huggingface.co/NeuML/pubmedbert-base-embeddings-2M (a) Embedding distributions of the entire corpus (yellow) vs the selected 20,000 documents (blue) to build our training corpus. (b) nDCG@10 scores on the validation set using an 80%/20% split of the constructed corpus. Evaluation is done every 10 steps, with peak performance observed at step 20 selected for full fine-tuning on the entire corpus followed by zero-shot evaluation on BEIR. Figure 2: (a) Corpus embedding distribution comparison and (b) validation nDCG@10 across training steps."
        },
        {
            "title": "D Dataset Details",
            "content": "D.1 BEIR Table 10 lists the BEIR datasets we used in our evaluation of the BiCA models, including their license information as well as the number of documents and queries present in the dataset. For more detailed description of the datasets we refer to (Thakur et al. 2021). D.2 LoTTE test-set Table 9 details the sub-topics we evaluated the BiCA models on from the LoTTE test-set. We refer the dataset descriptions exactly as they were in (Santhanam et al. 2022). Topic Question Set # Questions # Passages Subtopics Writing Recreation Science Lifestyle Search Forum Search Forum Search Forum Search Forum 1071 2000 924 2002 617 661 2002 200k 200k English English 167k Gaming, Anime, Movies 167k Gaming, Anime, Movies 1.694M Math, Physics, Biology 1.694M Math, Physics, Biology 119k Cooking, Sports, Travel 119k Cooking, Sports, Travel Table 9: Composition of LoTTE showing test topics, question sets, and sample of corresponding subtopics. Search Queries are taken from GooAQ, while Forum Queries are taken directly from the StackExchange archive. Dataset License # Passages # Test Queries ArguAna (Wachsmuth, Syed, and Stein 2018) Touche-2020 (Bondarenko et al. 2020) NFCorpus (Boteva et al. 2016) NQ (Kwiatkowski et al. 2019) DBPedia (Hasibi et al. 2017) FEVER (Thorne et al. 2018) SCIDOCS (Cohan et al. 2020) SciFact (Wadden et al. 2020) Quora FiQA (Maia et al. 2018) Climate-Fever (Diggelmann et al. 2021) TREC-COVID (Voorhees et al. 2021) CQADupStack (Hoogeveen, Verspoor, and Baldwin 2015) Apache License 2.0 HotPotQA (Yang et al. 2018) CC BY 4.0 CC BY 4.0 Not reported CC BY-SA 3.0 CC BY-SA 3.0 CC BY-SA 3.0 GNU General Public License v3.0 CC BY-NC 2.0 Not reported Not reported Not reported Dataset License Agreement CC BY-SA 4.0 8674 382545 3633 2681468 4635922 5416568 25657 5183 522931 57638 5416593 171332 457199 5233329 1406 49 323 3452 400 6666 1000 300 10000 648 1535 50 13145 7405 Table 10: BEIR dataset information."
        }
    ],
    "affiliations": [
        "BioSystems Engineering and Control (BiSECt) Lab, Department of Biotechnology and Wadhwani School of Data Science and AI, Indian Institute of Technology (IIT) Madras, Tamil Nadu India",
        "The Centre for Integrative Biology and Systems medicinE (IBSE), IIT Madras, Chennai, Tamil Nadu, India",
        "Vellore Institute of Technology (VIT) Chennai, India"
    ]
}