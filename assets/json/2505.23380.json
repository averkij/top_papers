{
    "paper_title": "UniRL: Self-Improving Unified Multimodal Models via Supervised and Reinforcement Learning",
    "authors": [
        "Weijia Mao",
        "Zhenheng Yang",
        "Mike Zheng Shou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Unified multimodal large language models such as Show-o and Janus have achieved strong performance across both generation and understanding tasks. However, these models typically rely on large-scale datasets and require substantial computation during the pretraining stage. In addition, several post-training methods have been proposed, but they often depend on external data or are limited to task-specific customization. In this work, we introduce UniRL, a self-improving post-training approach. Our approach enables the model to generate images from prompts and use them as training data in each iteration, without relying on any external image data. Moreover, it enables the two tasks to enhance each other: the generated images are used for understanding, and the understanding results are used to supervise generation. We explore supervised fine-tuning (SFT) and Group Relative Policy Optimization (GRPO) to optimize the models. UniRL offers three key advantages: (1) it requires no external image data, as all training samples are generated by the model itself during training; (2) it not only improves individual task performance, but also reduces the imbalance between generation and understanding; and (3) it requires only several additional training steps during the post-training stage. We evaluate UniRL on top of Show-o and Janus, achieving a GenEval score of 0.77 for Show-o and 0.65 for Janus. Code and models will be released in https://github.com/showlab/UniRL."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 2 ] . [ 1 0 8 3 3 2 . 5 0 5 2 : r UniRL: Self-Improving Unified Multimodal Models via Supervised and Reinforcement Learning Weijia Mao1 2, Zhenheng Yang2, Mike Zheng Shou1(cid:66) 1Show Lab, National University of Singapore 2ByteDance"
        },
        {
            "title": "Abstract",
            "content": "Unified multimodal large language models such as Show-o and Janus have achieved strong performance across both generation and understanding tasks. However, these models typically rely on large-scale datasets and require substantial computation during the pretraining stage. In addition, several post-training methods have been proposed, but they often depend on external data or are limited to task-specific customization. In this work, we introduce UniRL, self-improving post-training approach. Our approach enables the model to generate images from prompts and use them as training data in each iteration, without relying on any external image data. Moreover, it enables the two tasks to enhance each other: the generated images are used for understanding, and the understanding results are used to supervise generation. We explore supervised fine-tuning (SFT) and Group Relative Policy Optimization (GRPO) to optimize the models. UniRL offers three key advantages: (1) it requires no external image data, as all training samples are generated by the model itself during training; (2) it not only improves individual task performance, but also reduces the imbalance between generation and understanding; and (3) it requires only several additional training steps during the post-training stage. We evaluate UniRL on top of Show-o and Janus, achieving GenEval score of 0.77 for Show-o and 0.65 for Janus. Code and models will be released in https://github.com/showlab/UniRL."
        },
        {
            "title": "Introduction",
            "content": "Unified multimodal large language models are designed to handle both generation and understanding tasks within shared parameter space. Recently, models such as Show-o, Janus, and others [46, 42, 27, 51, 41] have made rapid progress by adopting various architectural and training strategies. Most existing work focuses on the pretraining stage, leveraging large-scale data to train models built upon large language models (LLMs) [22, 1, 40] or multimodal LLMs (MLLMs) [3, 26, 25]. However, these methods often face challenges such as high computational cost and the need for vast amounts of data. Meanwhile, several studies [48, 15, 18] have explored the post-training phase, but they face limitations: some require additional external data [48, 50, 18], others target only single task [15, 18], or focus on task-specific customization [50]. Therefore, developing effective post-training methods to improve the performance of unified multimodal models remains significant challenge. In this work, we propose self-improving post-training method for unified multimodal models without external image data. We begin by constructing prompts and questionanswer (QA) pairs inspired by the GenEval [14] benchmark, which evaluates generation quality by categorizing prompts into six typessuch as counting, color, and positiondesigned to reflect fundamental visual features of natural images. We adopt this categorization to design the prompts used in our training process. In each training iteration, constructed prompt is fed into the model to generate group of images. (cid:66) Corresponding Author Preprint. Under review. Figure 1: The imbalance between Text-to-Image Generation (T2I) and Multi-modal Understanding (MMU). For the same image, unified multimodal models may struggle to perform both generation and understanding consistently. These images, along with the corresponding questions, are then input back into the model to predict answers. The model is optimized using both the predicted and ground-truth answers to improve generation and understanding simultaneously. For optimization, we explore two strategies: supervised fine-tuning (SFT) and reinforcement learning. In the post-training stage, supervised fine-tuning (SFT) and reinforcement learning have traditionally been widely used. Recently, Group Relative Policy Optimization (GRPO) [33], reinforcement learning method, has shown strong performance in large language models (LLMs) [9], enabling effective post-training optimization through chain-of-thought (CoT) reasoning. In this work, we explore two optimization strategiesSFT and GRPOfor unified multimodal models and analyze the advantages of each strategy. Previous applications of GRPO typically rely on chain-of-thought outputs to estimate reward distributions. However, most current unified multimodal models lack the ability to produce structured reasoning steps. Therefore, we explore GRPO-based method that does not rely on reasoning outputs, aiming to improve the performance of unified multimodal models. For the performance of the unified multimodal models, we define it along three dimensions: (1) image generation capability, (2) image understanding capability, and (3) the balance between the two. While prior work has primarily focused on improving performance for single task, unified multimodal models frequently exhibit poor alignment between the two capabilities. As shown in Fig. 1, these models often fail to simultaneously support both tasks on the same image. The model may succeed in generating plausible image but fail to answer the corresponding question, or fail in generation while answering correctly. Therefore, beyond improving individual tasks, our self-improving method explicitly targets the imbalance between image generation and understanding. To quantitatively assess this imbalance, we design new evaluation metric. While visual understanding spans wide range of tasks, including reasoning and mathematical problem solving, we focus on basic visual features of natural imagessuch as object count, color, and positionin both training and evaluation to ensure alignment between generation and understanding tasks. Our method offers three main advantages. (1) It does not rely on external image data, using only images generated in real time during training. (2) It improves the performance of individual tasks while also reducing the imbalance between generation and understanding. Specifically, the same set of generated images is used for both tasks, enabling shared supervision that helps improve the imbalance between them. (3) It requires only small number of training steps during the post-training stage. To evaluate our method, we adopt Show-o and Janus as base models, assessing improvements in both single-task performance and the balance between tasks. Our approach achieves post-training GenEval scores of 0.77 on Show-o and 0.65 on Janus. Our main contributions are summarized as follows: To the best of our knowledge, we propose the first self-improving post-training optimization method for unified multimodal models via supervised and reinforcement learning. We investigate the effectiveness of both supervised fine-tuning (SFT) and Group Relative Policy Optimization (GRPO) for unified multimodal models, and provide detailed comparison of their respective performance. 2 We introduce new metric to quantify the imbalance between generation and understanding tasks within single model. Our evaluations across multiple unified models reveal that such imbalance is common. Our method substantially improves image generation and understanding quality and effectively reduces task imbalance, enabling better consistency between text-to-image generation and multimodal understanding."
        },
        {
            "title": "2 Related work",
            "content": "2.1 Unified multimodal understanding and generation Several recent studies [13, 44, 38, 46, 24, 36, 51, 37, 10, 41, 27, 23, 43, 34, 2, 29, 19, 17, 20, 6, 39] have explored unified transformers capable of both generation and comprehension. Chameleon [38],Emu3 [41], Janus [42], JanusPro [6] adopt autoregressive methods for both tasks. SEED-X [13] incorporates diffusion model alongside large language model to support multimodal generation and understanding. While both Show-o [46] and JanusFlow [27] use autoregression for understanding, the former adopts discrete diffusion for generation, whereas the latter employs flow matching. These models primarily focus on the pretraining stage using different methods for different tasks. These approaches require substantial training time and large-scale training data. Some studies [28, 15] have explored more efficient training or inference strategies. In this work, we take Show-o and Janus as our base model and explore post-training optimization techniques to further improve unified multimodal performance. 2.2 GRPO in LLM and MLLMs Supervised fine-tuning (SFT) and reinforcement learning have been widely adopted in the posttraining of large language models (LLMs) [16, 22, 1, 40] and multimodal LLMs (MLLMs) [3, 26]. Group Relative Policy Optimization (GRPO) is introduced in [33] to optimize LLMs by generating group of chain-of-thought outputs and applying task-specific reward function to guide learning. More recently, GRPO has also been explored in the context of MLLMs [4]. In addition to GRPO, other reinforcement learning approaches such as Direct Preference Optimization (DPO) [31, 48] and Proximal Policy Optimization (PPO) [32] have also been applied to both LLMs and MLLMs. Hermesflow [48] and Emu3 [41] use DPO to optimize unified multimodal models. However, PPO requires separate value network to estimate the baseline, and DPO relies on predefined positive and negative examples, which introduces additional complexity. In our work, we explore SFT and GRPO method to optimize unified multimodal models and improve their generation and understanding performance during the post-training stage."
        },
        {
            "title": "3 Method",
            "content": "In this section, we first introduce the background of unified multimodal models and the GRPO optimization method (Sec.3.1). We then describe the construction of training prompts (Sec.3.2), the proposed self-improving framework (Sec.3.3), and two optimization strategies: SFT and GRPO (Sec.3.4). We also present non-end-to-end training approach designed for models that use different image representations in the two tasks (Sec.3.5). Finally, we introduce new metric to evaluate the imbalance between the two tasks (Sec.3.6). The pipeline of training is shown in Fig. 2. 3.1 Preliminary Unified Multimodal Models. Unified multimodal models can be divided into two main categories: (1) both generation and understanding tasks use autoregressive methods; (2) generation makes use of diffusion or flow matching while comprehension relies on autoregressive methods. We focus on two example models: Show-o [46] and Janus [42]. In Show-o, the text tokens are denoted as = {t1, . . . , tN } and the image tokens as = {i1, . . . , iM }. The training objective includes two components: one maximizes the likelihood of each text token given all previous text and image tokens; the other reconstructs masked image tokens based on the remaining image tokens and the full text sequence. The two loss terms are: 3 LNTP = (cid:88) n=1 log pθ (cid:0)tn t1:n1, i1:M (cid:1), LMTP = (cid:88) m=1 log pθ (cid:0)i i1:m1, im+1:M , t1:N (cid:1). (1) Janus uses only the NTP objective, predicting every next token (image or text) in single autoregressive pass. Group Relative Policy Optimization(GRPO) We use reinforcement learning method called Group Relative Policy Optimization (GRPO) [33], which optimizes the model by comparing group of generated outputs and assigning relative rewards. Suppose inputs come from distribution D. For each D, we sample candidate outputs {yj}M j=1 under an old policy πφold . We then update our new policy πφ by maximising the following objective: L(φ) = xD, {yj }πφold (cid:34) 1 M (cid:88) j=1 min(cid:0)sj Aj, clip(sj, 1 δ, 1 + δ) Zj (cid:1) (cid:35) λ DKL (cid:0)πφ πψ (cid:1), (2) where sj = πφ(yj x) πφold(yj x) , Aj = Rj mean(R1, . . . , RM ) std(R1, . . . , RM ) . Rj is the reward for output yj, δ sets the clipping range, λ weighs the divergence penalty, and πψ is fixed reference policy. In the inner sum, sj measures how the new policy probability differs from the old policy for the same output. Multiplying by the normalised advantage Aj gives gradient direction. Clipping sj to [1 δ, 1 + δ] ensures that no single sample drives the update too far. Averaging over outputs makes the estimate stable across different group sizes. Finally, subtracting λ DKL(πφ πψ) discourages large shifts away from trusted policy, which helps to keep learning smooth. Table 1: Examples of prompts constructed for six categories of image-based questionanswer tasks: single object, two object, counting, colors, position, and attribute. Category Prompt Question Single object Two object Counting Colors Position Attribute photo of bench photo of table and soccer photo of three vases photo of green fork photo of train above of an elephant photo of blue chair and red umbrella What is the main object of the image? What are two main objects of the image? How many items in this image? What is the color of the object? Answer Bench table and soccer Three vases Green What are two objects and what is position relationship between two main items? What are two objects and the colors of two objects in the image? The train is above of an elephant The chair is blue and the umbrella is red 3.2 Prompt and QA pair construction In our study, we are inspired by the GenEval [14] benchmark to construct prompt and questionanswer pairs. The GenEval benchmark evaluates image generation models by dividing spatial relationships into six categories: single object, two objects, counting, colors, color attributes, and position. We create prompts and matching questionanswer pairs based on these categories. First, beyond the eighty object classes in the COCO dataset, we use GPT-4o to generate eighty more everyday objects, resulting in 160 objects in total. Second, we follow the GenEval procedure to generate prompts and then formulate the corresponding questions and answers. For example, in the counting category, we use prompt such as photo of three vases, pair it with the question How many items are in this image?, and assign three vases as the answer. To ensure strict separation between training and evaluation, we only use prompts from GenEval for testing. All prompts used for training are independently constructed and do not overlap with those in the GenEval benchmark. Additional examples appear in Tab. 3.1. Figure 2: Training pipeline of UniRL (a) SFT Optimization: prompt is used to generate single image, which, together with the corresponding question, is used to predict an answer and compute the SFT loss. (b) GRPO Optimization: The same prompt generates set of images, each paired with the same question to produce multiple answers, which are used to compute the GRPO loss. 3.3 Self-improving optimization In this We propose self-improving training framework that requires no external image data. framework, the image generation and multimodal understanding tasks mutually reinforce each other: the understanding task provides feedback to evaluate and guide image quality, while the generation task supplies training data that enhances the models understanding capability. As shown in Fig. 2, given constructed prompt, the model first generates sequence of image tokens = {u1, . . . , uM }. Regardless of whether the image generation module uses diffusion-based or autoregressive pipeline, we feed the generated image tokens and the prompt back into the model following the original T2I training process to obtain the image logits. Let = {z1, . . . , zL} denote the output logits corresponding to the target positions to be predicted. To enable gradient flow through the discrete image tokens, we apply the Straight-Through GumbelSoftmax (ST-GS) estimator to obtain differentiable approximations: ˆul = ST-GumbelSoftmax(zl), for = 1, . . . , L, (3) resulting in completed image token sequence ˆu that includes differentiably sampled tokens. The formulation and implementation details of the Straight-Through Gumbel-Softmax estimator are provided in Sec. A.2. This image sequence, together with the corresponding constructed question q, is then used to predict an answer. We use the predicted answers and ground-truth answer to optimize the model exploring two techniques, SFT and GRPO. 3.4 SFT and GRPO optimization SFT optimization. We use the predicted answers and the ground-truth answers to calculate the SFT loss to optimize the models. The SFT loss is computed as: LSFT = (cid:88) t=1 log pθ(yt y<t, ˆu, q), (4) where yt is the t-th token of the ground-truth answer, and y<t are the preceding tokens. This formulation applies generally to both masked and autoregressive image generation models. The variable ˆu denotes the generated image token sequence, which includes differentiably sampled tokens and serves as input to the answer generation module. This formulation is compatible with both masked and autoregressive image generation models. The SFT loss enables joint optimization of image generation and understanding tasks by allowing gradients to flow through the image tokens. GRPO optimization. We also adopt Group Relative Policy Optimization (GRPO) to optimize the unified multimodal models. During the GRPO phase, the model takes prompt and generates group of images {ˆu1, . . . , ˆuK}. Each image ˆuk, together with its corresponding question q, is then fed back into the model to produce predicted answer ˆak. reward rk = r(ˆak) is computed by comparing the predicted answer to the ground-truth answer a, based on the designed reward function. Similar to SFT, we also apply the Straight-Through Gumbel-Softmax estimator to enable differentiable sampling of image tokens, allowing gradients to propagate through both generation and understanding tasks. The overall GRPO loss is defined as: L(θ) = xD, {ˆak}πθold (cid:34) (cid:88) k=1 wk log pθ (cid:0)ˆak ˆuk, q(cid:1) (cid:35) where the importance weight wk is computed as: + β KL(pθ(a ˆu, q) pθref (a ˆu, q)) , (5) wk = exp (α (rk rmean)) j=1 exp (α (rj rmean)) (cid:80)K , with rmean denoting the average reward within the group and α controlling the sharpness of the weight distribution. The first term in the loss is the GRPO objective, which applies weighted log-likelihood over the sampled answers. πθold is the fixed sampling policy, typically frozen copy of the model, while πθ (or pθ) is the current policy being optimized. Rewards are centered by subtracting the group mean rmean, scaled by temperature α, and converted into weights wk to emphasize better answers. The second term is KL penalty, scaled by β, to keep the updated policy close to reference model, improving stability and preventing mode collapse. Importantly, because the image tokens ˆuk are sampled in differentiable manner, the reward signals from answer prediction can flow through both the understanding and image generation branches. This enables effective end-toend optimization of both tasks. Reward Function. The reward function assigns scores based on alignment between the predicted answer ˆak and the ground-truth answer a. While the specific reward criteria vary by question category, all rewards are based on two main components: (1) correct identification of object names, and (2) accurate prediction of associated attributes, such as number, color, or spatial position. The detailed reward rules for each category are summarized in Tab. 2. 3.5 End-to-end training vs. non-end-to-end training Table 2: Examples of reward rules based on key words in generated answers Category Score Key Word Single object Two object Counting Colors Position Attribute 1 2 2 1 3 4 bench table, soccer three, vase green train, above of, elephant blue, chair, red, umbrella Unified multimodal models such as Show-o [46], Emu3 [41], Chameleon [38], and Vila-u [45] use shared image representationstypically via VQGAN [12] or custom tokenizersallowing end-toend training. Gradients from MMU loss can flow through the T2I module via the Straight-Through Gumbel-Softmax estimator. In contrast, Janus [42] and JanusPro [6] use separate representations: discrete tokens for T2I and ViT features [11] for MMU. This mismatch prevents effective gradient flow; although gradients can technically propagate, they cannot meaningfully guide the discrete tokens, often leading to unstable training. To handle such cases, we adopt non-end-to-end training strategy. For T2I, rewards computed from MMU performance are used to directly optimize the image generation module. Conversely, for MMU, the model generates images, answers the corresponding questions, and uses the rewards to update the MMU branch. In the end-to-end setting, unified GRPO loss is used to jointly optimize both tasks. In contrast, the non-end-to-end setting applies separate GRPO objectives to T2I and MMU. Although the two modules are trained independently, reward signals still enable cross-task interaction, mitigating the limitations of mismatched representations. Full details and loss formulas are provided in the Sec. A.3. 6 3.6 The balance between image understanding and generation To evaluate the balance between text-to-image generation (T2I) and multimodal understanding (MMU), we propose bidirectional evaluation metric with two chains: T2IMMU and MMUT2I. In T2IMMU, the model generates an image from prompt and answers question only if the image is correct; we then compute the answer accuracy. In MMUT2I, the model first answers question based on given image; if correct, we regenerate the image from the prompt and evaluate its quality. This setup assesses the consistency between generation and understanding. The conditional accuracies are defined as: Table 3: Performance of the proposed metric for evaluating the imbalance between two tasks. Method AccuracyMMUT2I AccuracyT2IMMU Show-o Janus JanusPro Vila-u 0.70 0.68 0.85 0.85 0.83 0.67 0.69 0.68 AccuracyMMUT2I = NAI NI , AccuracyT2IMMU = NAI NA . (6) NI denotes the number of correctly generated images, NA the number of correctly answered questions, and NAI the number of samples where both are correct. These metrics quantitatively assess the consistency between generation and understanding. AccuracyMMUT2I measures answer correctness given correct image, while AccuracyT2IMMU measures image correctness given correct answer. This bidirectional formulation evaluates the alignment between the two tasks beyond isolated performance. As shown in Tab. 3, we apply this metric to several unified models, including Show-o [46], Janus [42], Janus-Pro [6], and Vila-u [45], and observe that task imbalance is consistently present across all models. More details can be seen in Sec. A.1."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Implementation details We evaluate our method on two base models: Show-o [46] and Janus [42]. For Show-o, we adopt end-to-end training. The SFT stage uses batch size of 2 for 5,000 iterations, and the GRPO stage is configured with group size of 3, KL coefficient β = 0.2, and 3,000 training steps on 8 H100 GPUs. During inference, we set the guidance scale to 2, diffusion steps to 16, and resolution to 512. For Janus, due to its use of different representations for generation and understanding, we apply non-end-to-end training strategy. The model is trained with batch size of 8 for 1,000 iterations using the same KL coefficient. Inference is performed with guidance scale of 5 and resolution of 384. More implementation details can be seen in Sec. A.1. 4.2 Quantitative results Baselines and our method. We compare our method against several baselines. Original Model. We evaluate the performance of the pretrained model without any post-training. HermesFlow. [48] This method fine-tunes the Show-o model with DPO optimization on the JourneyDB dataset [35]. We evaluate two variants of our post-training framework: UniRL (SFT). Our method using supervised fine-tuning (SFT) without reinforcement learning. UniRL (GRPO). Our method using Group Relative Policy Optimization (GRPO). For the Janus model, we only compare our GRPO-based method with the original model, as applying SFT leads to unstable training and failure. The specific reasons are discussed in Sec. 4.4. Text to image generation. For the image generation task, we evaluate model performance using three benchmarks: GenEval [14], DSG-1K [8], and CLIP-Score [30]. To calculate the CLIP score, we use 2,000 generated images sampled from the GenEval and DSG-1K benchmarks. As shown in Tab. 4, our method with SFT improves the GenEval score from 0.60 to 0.76, while the GRPO-based variant achieves 0.71both outperforming the baselines by large margin. In addition, our approach yields notable gains on DSG-1K and CLIP-Score, demonstrating the effectiveness of our post-training strategy in improving image generation quality. More results will be shown in Sec. A.4. Multimodal understanding. We evaluate the models ability to understand natural visual features across six categories: single object, two objects, counting, colors, position, and color attributes. These 7 Table 4: Comparison of UniRL with baseline methods across Text-to-Image (T2I) benchmarks. Method GenEval DSG-1K Clip score Single. Two. Count. Colors Posi. Attri. Overall Show-o HermersFlow UniRL(SFT) UniRL(GRPO) Janus UniRL(GRPO) 0.96 0.97 0.99 0.95 0.96 0. 0.64 0.67 0.93 0.77 0.64 0.74 0.67 0.65 0.62 0.65 0.29 0.27 0.81 0.77 0.89 0.82 0.81 0. 0.25 0.28 0.55 0.50 0.49 0.62 0.37 0.42 0.68 0.60 0.46 0.52 0.60 0.61 0.77 0.71 0.60 0. 0.77 0.78 0.82 0.80 0.80 0.81 0.331 0.334 0.337 0.335 0.332 0.335 Table 6: Comparison using our proposed metric to evaluate the imbalance between the two tasks. Method AccuracyMMUT2I AccuracyT2IMMU Single. Two. Count. Colors Posi. Attri. Overall Single. Two. Count. Colors Posi. Attri. Overall 0.98 Show-o 0.99 HermesFlow UniRL(SFT) 0.11 UniRL(GRPO) 0.99 0.89 0.92 0.85 0.90 0.99 0.97 0.97 0.90 0.94 0.10 0.61 0.96 0.23 0.57 0.99 0.83 0.98 0.99 0.81 0. 0.91 Janus UniRL(GRPO) 0.94 0.59 0.71 0.78 0.52 0.96 0.20 0.42 0.98 0.31 0.77 0.83 0.83 0.81 0.93 0.67 0.76 0.96 0.97 0.99 0. 0.95 0.96 0.66 0.67 0.68 0.69 0.95 0.58 0.85 0.70 0.85 0.18 0.41 0.84 0.24 0.41 0.93 0.60 0.74 0.90 0.67 0.68 0.67 0.27 0.76 0.34 0.90 0.39 0.49 0.88 0.63 0.59 0.70 0.71 0.80 0. 0.68 0.74 categories capture core aspects of visual understanding, including object recognition, counting, color perception, and spatial reasoningfundamental properties of natural images. To assess generalization beyond the training distribution, we use images generated by external models such as JanusPro. The generated images, along with their corresponding questions, are then fed into the target model to evaluate its understanding capabilities on out-of-distribution visual inputs. We compute the accuracy for each categories, allowing fine-grained analysis of the models generalization across different aspects of visual understanding. As shown in Tab. 5, our SFT-based method performs slightly worse than the original model, primarily due to overfitting, particularly in the single-object category. In contrast, our GRPO-based method achieves an overall score of 0.79, significantly outperforming all baselines. The underlying reasons are discussed in Sec. 4.4, while additional evaluation details and benchmark results are provided in Sec. A.1 and Sec. A.5. Table 5: Comparison of UniRL with baseline methods across Multi-modal Understanding(MMU) benchmarks. Single. Two. Count. Colors Posi. Attri. Overall Method The imbalance between two tasks. To evaluate the balance between image understanding and generation, we apply our proposed metric, as shown in Tab. 6. The prompts used for AccuracyMMUT2I are taken from the GenEval benchmark, while those for AccuracyT2IMMU are aligned with the MMU evaluation setup. The results show that our GRPO-based method significantly reduces the imbalance between the two tasks. For certain subtasks, the model achieves nearly 100% accuracy, demonstrating strong consistency between generation and understanding capabilities. 0.99 Show-o 0.99 HermesFlow UniRL(SFT) 0.08 UniRL(GRPO) 0. 0.98 0.20 0.54 0.98 0.34 0.52 0.99 0.45 0.85 0.94 0.46 0.72 0.88 0.79 0.88 0.79 0.87 0.71 0.88 0.77 Janus 0.93 UniRL(GRPO) 0.94 0.97 0.38 0.74 0.99 0.52 0.89 0.93 0.92 0.95 0.83 0.71 0.73 0.67 0. 0.84 0.88 4.3 Visualization results As shown in Fig. 3, we present qualitative comparisons between our method and the original models on both tasks. Our approach produces more visually faithful generations conditioned on prompts and yields more accurate answers for corresponding questions, demonstrating improved consistency across the two tasks. More results will be shown in Sec. A.6. 4.4 Analysis of SFT vs. GRPO From these experimental results, we can draw some key observations. Figure 3: Qualitative comparison of our method with the original models on both text-to-image generation (T2I) and multimodal understanding (MMU) tasks. Observation 1: SFT is more prone to overfitting and memorization than GRPO in the image understanding task. As shown in Tab. 5, SFT performs significantly worse than both the original model and GRPO on the single object category, where the training and test sets are fully disjoint in both examples and object types. In contrast, categories like position and colors share abstract features (e.g., position or color attributes), which partially mitigate overfitting. Compared to SFT, GRPO demonstrates better generalization under the same conditions. Observation 2: In the generation task, SFT outperforms GRPO under an end-to-end training pipeline. As shown in Tab. 4, SFT achieves better T2I results than GRPO, with no noticeable overfitting. We hypothesize two reasons for this difference. (1) MMU is more prone to overfitting than T2I, as it mainly learns from text token patterns, while T2I involves more structured image reconstruction. (2) Our training pipeline is end-to-end. The SFT supervision signal for MMU is more direct and localized, whereas for T2I it is more indirect and distributed, which may promote better generalization in image generation. Under indirect supervision in T2I, SFT may optimize more effectively, as its loss is directly based on ground-truth answers. More analysis will be discussed in Sec. A.7. Observation 3: For non-end-to-end training setups such as Janus, SFT leads to failure in the image generation task. If we directly use the models generated images to supervise T2I at each iteration, training will become unstable. We speculate that early-stage generation errors are repeatedly reinforced, leading to input distribution drift and eventual collapse. Therefore, for unified models with different representations for the two tasks, only the GRPO-based method is effective for improving performance. More analysis will be discussed in Sec. A.7. Overall, GRPO is more general and robust optimization strategy compared to SFT. Regardless of the specific task or model architecturewhether the two tasks share the same image representation or notGRPO consistently improves performance and effectively reduces the imbalance between generation and understanding. In contrast, while SFT tends to perform better on tasks like T2I in end-to-end setups (e.g., Show-o), it is more prone to memorization and overfitting."
        },
        {
            "title": "5 Conclusion",
            "content": "In this work, we propose self-improving post-training method for unified multimodal models. By combining supervised fine-tuning (SFT) with Group Relative Policy Optimization (GRPO), our approach improves both generation and understanding performance while reducing task imbalance. Experiments on Show-o and Janus validate the effectiveness of our method."
        },
        {
            "title": "References",
            "content": "[1] Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, et al. Phi-3 technical report: highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219, 2024. [2] Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 1, 2023. [3] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: frontier large vision-language model with versatile abilities. CoRR, abs/2308.12966, 2023. [4] Hardy Chen, Haoqin Tu, Fali Wang, Hui Liu, Xianfeng Tang, Xinya Du, Yuyin Zhou, and Cihang Xie. Sft or rl? an early investigation into training r1-like reasoning large vision-language models. arXiv preprint arXiv:2504.11468, 2025. [5] Kai Chen, Jiaqi Wang, Jiangmiao Pang, Yuhang Cao, Yu Xiong, Xiaoxiao Li, Shuyang Sun, Wansen Feng, Ziwei Liu, Jiarui Xu, Zheng Zhang, Dazhi Cheng, Chenchen Zhu, Tianheng Cheng, Qijie Zhao, Buyu Li, Xin Lu, Rui Zhu, Yue Wu, Jifeng Dai, Jingdong Wang, Jianping Shi, Wanli Ouyang, Chen Change Loy, and Dahua Lin. Mmdetection: Open mmlab detection toolbox and benchmark. CoRR, abs/1906.07155, 2019. [6] Xiaokang Chen, Zhiyu Wu, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, and Chong Ruan. Janus-pro: Unified multimodal understanding and generation with data and model scaling. CoRR, abs/2501.17811, 2025. [7] Bowen Cheng, Ishan Misra, Alexander G. Schwing, Alexander Kirillov, and Rohit Girdhar. Masked-attention mask transformer for universal image segmentation. In CVPR, pages 1280 1289. IEEE, 2022. [8] Jaemin Cho, Yushi Hu, Jason M. Baldridge, Roopal Garg, Peter Anderson, Ranjay Krishna, Mohit Bansal, Jordi Pont-Tuset, and Su Wang. Davidsonian scene graph: Improving reliability in fine-grained evaluation for text-to-image generation. In ICLR. OpenReview.net, 2024. [9] DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, and S. S. Li. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. CoRR, abs/2501.12948, 2025. [10] Runpei Dong, Chunrui Han, Yuang Peng, Zekun Qi, Zheng Ge, Jinrong Yang, Liang Zhao, Jianjian Sun, Hongyu Zhou, Haoran Wei, Xiangwen Kong, Xiangyu Zhang, Kaisheng Ma, and Li Yi. DreamLLM: Synergistic multimodal comprehension and creation. In ICLR, 2024. [11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR. OpenReview.net, 2021. [12] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In CVPR, pages 1287312883, 2021. [13] Yuying Ge, Sijie Zhao, Jinguo Zhu, Yixiao Ge, Kun Yi, Lin Song, Chen Li, Xiaohan Ding, and Ying Shan. Seed-x: Multimodal models with unified multi-granularity comprehension and generation. arXiv preprint arXiv:2404.14396, 2024. [14] Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating text-to-image alignment. In NeurIPS, 2023. [15] Ziyu Guo, Renrui Zhang, Chengzhuo Tong, Zhizheng Zhao, Peng Gao, Hongsheng Li, and Pheng-Ann Heng. Can we generate images with cot? lets verify and reinforce image generation step by step. CoRR, abs/2501.13926, 2025. [16] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, Alex Iftimie, Alex Karpenko, Alex Tachard Passos, Alexander Neitz, Alexander Prokofiev, Alexander Wei, Allison Tam, Ally Bennett, Ananya Kumar, Andre Saraiva, Andrea Vallone, Andrew Duberstein, Andrew Kondrich, Andrey Mishchenko, Andy Applebaum, Angela Jiang, Ashvin Nair, Barret Zoph, Behrooz Ghorbani, Ben Rossen, Benjamin Sokolowsky, Boaz Barak, Bob McGrew, Borys Minaiev, Botao Hao, Bowen Baker, Brandon Houghton, Brandon McKinzie, Brydon Eastman, Camillo Lugaresi, Cary Bassin, Cary Hudson, Chak Ming Li, Charles de Bourcy, Chelsea Voss, Chen Shen, Chong Zhang, Chris Koch, Chris Orsinger, Christopher Hesse, Claudia Fischer, Clive Chan, Dan Roberts, Daniel Kappler, Daniel Levy, Daniel Selsam, David Dohan, David Farhi, David Mely, David Robinson, Dimitris Tsipras, Doug Li, Dragos Oprica, Eben Freeman, Eddie Zhang, Edmund Wong, Elizabeth Proehl, Enoch Cheung, Eric Mitchell, Eric Wallace, Erik Ritter, Evan Mays, Fan Wang, Felipe Petroski Such, Filippo Raso, Florencia Leoni, Foivos Tsimpourlas, Francis Song, Fred von Lohmann, Freddie Sulit, Geoff Salmon, Giambattista Parascandolo, Gildas Chabot, Grace Zhao, Greg Brockman, Guillaume Leclerc, Hadi Salman, Haiming Bao, Hao Sheng, Hart Andrin, Hessam Bagherinezhad, Hongyu Ren, Hunter Lightman, Hyung Won Chung, Ian Kivlichan, Ian OConnell, Ian Osband, Ignasi Clavera Gilaberte, and Ilge Akkaya. Openai o1 system card. CoRR, abs/2412.16720, 2024. [17] Siqi Kou, Jiachun Jin, Chang Liu, Ye Ma, Jian Jia, Quan Chen, Peng Jiang, and Zhijie Deng. Orthus: Autoregressive interleaved image-text generation with modality-specific heads. CoRR, abs/2412.00127, 2024. [18] Chengzu Li, Wenshan Wu, Huanyu Zhang, Yan Xia, Shaoguang Mao, Li Dong, Ivan Vulic, and Furu Wei. Imagine while reasoning in space: Multimodal visualization-of-thought. CoRR, abs/2501.07542, 2025. [19] Hao Li, Changyao Tian, Jie Shao, Xizhou Zhu, Zhaokai Wang, Jinguo Zhu, Wenhan Dou, Xiaogang Wang, Hongsheng Li, Lewei Lu, and Jifeng Dai. Synergen-vl: Towards synergistic image understanding and generation with vision experts and token folding. CoRR, abs/2412.09604, 2024. [20] Shufan Li, Konstantinos Kallidromitis, Akash Gokul, Zichun Liao, Yusuke Kato, Kazuki Kozuka, and Aditya Grover. Omniflow: Any-to-any generation with multi-modal rectified flows. CoRR, abs/2412.01169, 2024. [21] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. Evaluating object hallucination in large vision-language models. In EMNLP, pages 292305. Association for Computational Linguistics, 2023. [22] Yuanzhi Li, Sébastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar, and Yin Tat Lee. Textbooks are all you need ii: phi-1.5 technical report. arXiv preprint arXiv:2309.05463, 2023. [23] Dongyang Liu, Shitian Zhao, Le Zhuo, Weifeng Lin, Yu Qiao, Hongsheng Li, and Peng Gao. Lumina-mgpt: Illuminate flexible photorealistic text-to-image generation with multimodal generative pretraining. CoRR, abs/2408.02657, 2024. 11 [24] Hao Liu, Wilson Yan, Matei Zaharia, and Pieter Abbeel. World model on million-length video and language with ringattention. arXiv preprint, 2024. [25] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, 2024. [26] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. NeurIPS, 36, 2024. [27] Yiyang Ma, Xingchao Liu, Xiaokang Chen, Wen Liu, Chengyue Wu, Zhiyu Wu, Zizheng Pan, Zhenda Xie, Haowei Zhang, Liang Zhao, et al. Janusflow: Harmonizing autoregression and rectified flow for unified multimodal understanding and generation. arXiv preprint arXiv:2411.07975, 2024. [28] Weijia Mao, Zhenheng Yang, and Mike Zheng Shou. Unimod: Efficient unified multimodal transformers with mixture-of-depths. CoRR, abs/2502.06474, 2025. [29] Liao Qu, Huichao Zhang, Yiheng Liu, Xu Wang, Yi Jiang, Yiming Gao, Hu Ye, Daniel K. Du, Zehuan Yuan, and Xinglong Wu. Tokenflow: Unified image tokenizer for multimodal understanding and generation. CoRR, abs/2412.03069, 2024. [30] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In ICML, pages 87488763, 2021. [31] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D. Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. In NeurIPS, 2023. [32] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. CoRR, abs/1707.06347, 2017. [33] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. CoRR, abs/2402.03300, 2024. [34] Weijia Shi, Xiaochuang Han, Chunting Zhou, Weixin Liang, Xi Victoria Lin, Luke Zettlemoyer, and Lili Yu. Llamafusion: Adapting pretrained language models for multimodal generation. arXiv preprint arXiv:2412.15188, 2024. [35] Keqiang Sun, Junting Pan, Yuying Ge, Hao Li, Haodong Duan, Xiaoshi Wu, Renrui Zhang, Aojun Zhou, Zipeng Qin, Yi Wang, Jifeng Dai, Yu Qiao, Limin Wang, and Hongsheng Li. Journeydb: benchmark for generative image understanding. In NeurIPS, 2023. [36] Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, Yueze Wang, Hongcheng Gao, Jingjing Liu, Tiejun Huang, and Xinlong Wang. Emu: Generative pretraining in multimodality. In ICLR, 2023. [37] Zineng Tang, Ziyi Yang, Chenguang Zhu, Michael Zeng, and Mohit Bansal. Any-to-any generation via composable diffusion. NeurIPS, 36, 2024. [38] Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. [39] Shengbang Tong, David Fan, Jiachen Zhu, Yunyang Xiong, Xinlei Chen, Koustuv Sinha, Michael Rabbat, Yann LeCun, Saining Xie, and Zhuang Liu. Metamorph: Multimodal understanding and generation via instruction tuning. arXiv preprint arXiv:2412.14164, 2024. [40] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurélien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. CoRR, abs/2302.13971, 2023. 12 [41] Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, et al. Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869, 2024. [42] Chengyue Wu, Xiaokang Chen, Zhiyu Wu, Yiyang Ma, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, Chong Ruan, and Ping Luo. Janus: Decoupling visual encoding for unified multimodal understanding and generation. CoRR, abs/2410.13848, 2024. [43] Junfeng Wu, Yi Jiang, Chuofan Ma, Yuliang Liu, Hengshuang Zhao, Zehuan Yuan, Song Bai, and Xiang Bai. Liquid: Language models are scalable multi-modal generators. CoRR, abs/2412.04332, 2024. [44] Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-Seng Chua. Next-gpt: Any-to-any multimodal llm. arXiv preprint arXiv:2309.05519, 2023. [45] Yecheng Wu, Zhuoyang Zhang, Junyu Chen, Haotian Tang, Dacheng Li, Yunhao Fang, Ligeng Zhu, Enze Xie, Hongxu Yin, Li Yi, Song Han, and Yao Lu. VILA-U: unified foundation model integrating visual understanding and generation. CoRR, abs/2409.04429, 2024. [46] Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding and generation. arXiv preprint arXiv:2408.12528, 2024. [47] An Yang, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoyan Huang, Jiandong Jiang, Jianhong Tu, Jianwei Zhang, Jingren Zhou, Junyang Lin, Kai Dang, Kexin Yang, Le Yu, Mei Li, Minmin Sun, Qin Zhu, Rui Men, Tao He, Weijia Xu, Wenbiao Yin, Wenyuan Yu, Xiafei Qiu, Xingzhang Ren, Xinlong Yang, Yong Li, Zhiying Xu, and Zipeng Zhang. Qwen2.5-1m technical report. CoRR, abs/2501.15383, 2025. [48] Ling Yang, Xinchen Zhang, Ye Tian, Chenming Shang, Minghao Xu, Wentao Zhang, and Bin Cui. Hermesflow: Seamlessly closing the gap in multimodal understanding and generation. CoRR, abs/2502.12148, 2025. [49] Xiang Yue, Yuansheng Ni, Tianyu Zheng, Kai Zhang, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. MMMU: massive multi-discipline multimodal understanding and reasoning benchmark for expert AGI. In CVPR, pages 95569567. IEEE, 2024. [50] Rui Zhao, Weijia Mao, and Mike Zheng Shou. Doracycle: Domain-oriented adaptation of unified generative model in multimodal cycles. CoRR, abs/2503.03651, 2025. [51] Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma, Luke Zettlemoyer, and Omer Levy. Transfusion: Predict the next token and diffuse images with one multi-modal model. CoRR, abs/2408.11039, 2024."
        },
        {
            "title": "A Technical appendices",
            "content": "In this appendix, we provide additional implementation details of our experiments (Sec. A.1) and describe the formulation and explanation of the Straight-Through Gumbel-Softmax estimator (Sec. A.2). We then present more details and the loss formulation for the non-end-to-end training pipeline (Sec. A.3). Additional results for both tasks, along with extended visualization examples, are provided in Sec. A.5, Sec. A.4, and Sec. A.6. We also offer further analysis comparing SFT and GRPO in non-end-to-end settings (Sec. A.7). Finally, we discuss the limitations of our work and its broader impact (Sec. A.8 and Sec. A.9). A.1 More implementation details Training details. For the SFT or GRPO phase, we set the learning rate to 1 105 for both the Janus and Show-o models. We use the constructed prompts as input and randomly select one sentence from the six predefined categories. The constructed prompts serve as the training set, while the prompts from the GenEval benchmark are used as the test set. Understanding benchmark. For the understanding benchmarks, we use other unified multimodal models to generate images based on the prompts from the GenEval benchmark. We then filter and retain only the correctly generated images. These images, along with the constructed questions, are used as inputs to evaluate the models understanding capability. We compute the accuracy as the percentage of predicted answers that match the ground-truth answers. New metric. For the T2IMMU chain, we use the prompts from the GenEval benchmark for evaluation. For the MMUT2I chain, we follow the input format of our understanding benchmarks, feeding an external image and corresponding questions for evaluation. To assess the imbalance between the two tasks using our proposed metric, we employ external language models such as Qwen [47] or GPT [16] to judge whether the predicted answers match the ground-truth answers and compute accuracy for each category. For image evaluation, we follow the GenEval protocol, using MMDetection [5] and Mask2Former [7] to determine whether the generated image is correct. A.2 Straight-Through gumbel softmax The Straight-Through Gumbel-Softmax (ST-GS) estimator enables differentiable sampling of discrete tokens, which is crucial for gradient-based optimization in end-to-end training. It approximates the sampling process using soft relaxation during the backward pass while retaining hard discrete choices during the forward pass. The Gumbel-Softmax distribution samples continuous approximation of one-hot vectors using the following formula: exp (cid:19) (cid:18) log(πi) + gi τ (cid:18) log(πj) + gj τ exp yi = (cid:80) j=1 (cid:19) , (7) where πi is the unnormalized probability of the i-th category, gi is sampled from the Gumbel(0, 1) distribution, τ is temperature parameter controlling the smoothness, and is the number of categories. In the straight-through version, hard one-hot vector is obtained during the forward pass by applying arg max over the sampled logits: (cid:40)1, if = arg max (8) zi = 0, otherwise yj . While the forward pass uses hard selection, the backward pass uses the soft yi values to compute gradients, enabling smooth updates. We adopt the Straight-Through Gumbel-Softmax in our method to allow gradients from downstream tasks to propagate through discrete image tokens. This is critical for end-to-end training, as it ensures that the generation module receives meaningful gradient signals and participates in joint optimization with the understanding module. 14 A.3 Non end-to-end training pipeline For non-end-to-end training, we decouple the GRPO loss into two separate objectives for the T2I and MMU modules. T2I optimization. For the image generation task, we directly optimize the image token logits using rewards computed from MMU performance. The GRPO loss is defined as: LT2I(θ) = (cid:88) k=1 wk log pθ (cid:0)ˆuk p(cid:1) + β KL (pθ(u p) pθref (u p)) , where is the input prompt, ˆuk is the k-th sampled image token sequence, and wk is reward-based importance weight derived from MMU outputs. MMU optimization. For the multimodal understanding task, we optimize the answer prediction directly using rewards based on answer correctness. The GRPO loss is given by: LMMU(θ) = (cid:88) k=1 wk log pθ (cid:0)ˆak u, q(cid:1) + β KL (pθ(a u, q) pθref (a u, q)) , where is the input image token sequence, is the question, ˆak is the predicted answer, and wk is computed based on the answer reward. Although the two modules are optimized independently, reward signals still facilitate cross-task influence, allowing the model to benefit from mutual supervision even without gradient sharing. A.4 More generation results In our main experiments using the Show-o model [46], both our method and the baselines adopt guidance scale of 2 and 16 generation steps during training and inference. Here, we present additional comparison results under higher guidance scale of 5 and an extended generation length of 50 steps. We observe that our SFT-based method achieves final score of 0.79, while our GRPO-based method reaches 0.74both significantly outperforming all baselines. Table 7: Comparison of UniRL with baseline methods on the GenEval benchmark under guidance scale of 5 and 50 generation steps. Method Single. Two. Count. Colors Posi. Attri. Overall Show-o HermesFlow UniRL(SFT) UniRL(GRPO) 0.98 0.98 1.00 0.96 0.81 0.84 0.97 0. 0.69 0.66 0.61 0.67 0.82 0.82 0.91 0.86 0.32 0.32 0.56 0.50 0.53 0.52 0.70 0.67 0.68 0.69 0.79 0.74 A.5 More understanding results Table 8: Benchmark results for multimodal understanding tasks. Since our work primarily focuses on the understanding of basic visual attributes, the main results on tasks such as counting and color recognition have been reported in Sec.4.2. We further evaluate our method on more comprehensive benchmarks, including POPE[21] and MMMU [49], as shown in Tab. 8. The slightly lower scores can be attributed to the fact that our training data primarily focuses on basic understanding features. Besides, some benchmarks like MMMU focus more on text-based reasoning and numerical calculation, rather than visual understanding. We believe that incorporating broader range of understanding categories during training, or using larger model, may help mitigate this limitation in future work. Show-o UniRL (GRPO) POPE MMMU_val 79.8 78.1 26.7 25. Method A.6 More visualization results We provide additional visualization results in Fig. 4, including several out-of-distribution examples. These results demonstrate that our method (GRPO) generalizes beyond the specific categories seen during training. 15 Figure 4: Visualization results of our method. A.7 More analysis of SFT and GRPO End-to-end training. For end-to-end training, supervision for T2I is inherently indirect, as the optimization signal comes from downstream tasks such as answer prediction rather than direct supervision of the generated image. Compared to MMU, T2I is generally less prone to overfitting due to the structured nature of image reconstruction. In this setting, SFT may outperform GRPO, as it leverages the ground-truth answer to directly compute the loss and guide the model toward clearer optimization direction. This process does not heavily depend on the capacity of the base model; instead, the ground-truth serves as strong supervision signal that helps SFT converge more efficiently. In contrast, GRPO optimizes the model by generating group of candidate answers and computing rewards to update the policy. When the model capacity is limited, the generated answers may all be incorrect, leading to weak or noisy reward signals and thus making optimization more difficult. For MMU, which primarily relies on learning from text-based inputs, SFT tends to overfit easily by memorizing token patterns. GRPO, on the other hand, benefits from its reward-based training and KL regularization term, which encourages the updated model to remain close to the original pretrained model. This constraint improves generalization and helps prevent overfitting, especially in cases where training data is limited or highly structured. Non end-to-end training. For non end-to-end training, applying SFT to T2I is often unstable due to error accumulation from the generation pipeline. The optimization direction can easily become unreliable, leading to training collapse. To mitigate this, one option is to follow prior works [48, 15], which use offline-generated images and filter out incorrect samples to construct reliable training set for SFT. In comparison, GRPO is more robust in non end-to-end settings, as it directly optimizes with reward signals and demonstrates better stability and performance. A.8 Limitations While our work presents an effective post-training method for unified multimodal models, it also has several limitations. First, our current focus is limited to basic visual understanding tasks such as counting and color recognition. More complex reasoning abilities, such as mathematical problemsolving or abstract inference, are beyond the scope of this work and are left for future exploration. Second, the training speed per iteration is relatively slow. This is known limitation of GRPO-based methods, as they require multiple forward passes during inference before optimization. A.9 Broader impacts Our work explores an effective post-training method for unified multimodal models. We believe our method will not bring negative societal impacts."
        }
    ],
    "affiliations": [
        "ByteDance",
        "Show Lab, National University of Singapore"
    ]
}