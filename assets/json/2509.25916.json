{
    "paper_title": "VLM-FO1: Bridging the Gap Between High-Level Reasoning and Fine-Grained Perception in VLMs",
    "authors": [
        "Peng Liu",
        "Haozhan Shen",
        "Chunxin Fang",
        "Zhicheng Sun",
        "Jiajia Liao",
        "Tiancheng Zhao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Vision-Language Models (VLMs) excel at high-level scene understanding but falter on fine-grained perception tasks requiring precise localization. This failure stems from a fundamental mismatch, as generating exact numerical coordinates is a challenging task for language-centric architectures. In this paper, we introduce VLM-FO1, a novel framework that overcomes this limitation by reframing object-centric perception from a brittle coordinate generation problem into a robust feature retrieval task. Our method operates as a plug-and-play module that integrates with any pre-trained VLM. It leverages a Hybrid Fine-grained Region Encoder (HFRE), featuring a dual vision encoder, to generate powerful region tokens rich in both semantic and spatial detail. A token-based referencing system then enables the LLM to seamlessly reason about and ground language in these specific visual regions. Experiments show that VLM-FO1 achieves state-of-the-art performance across a diverse suite of benchmarks, demonstrating exceptional capabilities in object grounding, region generational understanding, and visual region reasoning. Crucially, our two-stage training strategy ensures that these perception gains are achieved without compromising the base model's general visual understanding capabilities. VLM-FO1 establishes an effective and flexible paradigm for building perception-aware VLMs, bridging the gap between high-level reasoning and fine-grained visual grounding."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 3 ] . [ 1 6 1 9 5 2 . 9 0 5 2 : r VLM-FO1: Bridging the Gap Between High-Level Reasoning and Fine-Grained Perception in VLMs Peng Liu1, Haozhan Shen3, Chunxin Fang2, Zhicheng Sun1, Jiajia Liao2, Tiancheng Zhao1,2,* 1Om AI Research, 2Binjiang Institute of Zhejiang University, 3College of Computer Science and Technology, Zhejiang University tianchez@zju-bj.com"
        },
        {
            "title": "Abstract",
            "content": "Vision-Language Models (VLMs) excel at high-level scene understanding but falter on fine-grained perception tasks requiring precise localization. This failure stems from fundamental mismatch, as generating exact numerical coordinates is an challenge task for language-centric architectures. In this paper, we introduce VLM-FO1, novel framework that overcomes this limitation by reframing objectcentric perception from brittle coordinate generation problem into robust feature retrieval task. Our method operates as plug-and-play module that integrates with any pre-trained VLM. It leverages Hybrid Fine-grained Region Encoder (HFRE), featuring Dual-Vision Encoder, to generate powerful region tokens rich in both semantic and spatial detail. token-based referencing system then enables the LLM to seamlessly reason about and ground language in these specific visual regions. Experiments show that VLM-FO1 achieves state-of-the-art performance across diverse suite of benchmarks, demonstrating exceptional capabilities in Object Grounding, Region Generative Understanding, and Visual Region Reasoning. Crucially, our two-stage training strategy ensures these perception gains are achieved without compromising the base models general visual understanding capabilities. VLM-FO1 establishes an effective and flexible paradigm for building perception-aware VLMs, bridging the gap between highlevel reasoning and fine-grained visual grounding. 1. Introduction The advent of Large Language Models (LLMs) [3, 7, 28, 29, 57, 74, 92, 93, 107] has marked paradigm shift in artificial intelligence, demonstrating profound capabilities in generation, reasoning, and instruction following. This success has been extended to the visual domain through VisionLanguage Models (VLMs) [19, 25, 33, 46, 59, 60, 98, 129], which integrate powerful vision backbones with LLMs to interpret and reason about visual content. By mapping visual features into the language models embedding space, VLMs have achieved remarkable performance on high-level visual understanding tasks such as visual question answering (VQA) and image captioning. Further advancements, such as the application of reinforcement learning techniques like Group Relative Policy Optimization (GRPO) [86], have continued to enhance their complex reasoning abilities. Despite these advances, critical weakness persists: state-of-the-art VLMs struggle with fine-grained visual perception tasks that demand precise spatial localization, such as object detection and grounding. This deficiency severely limits their applicability in real-world scenarios like autonomous robotics, detailed image analysis, and humancomputer interaction, where understanding what is in an image is inseparable from knowing where it is. Our evaluations reveal stark performance gap. On standard benchmarks like COCO [54], specialized detection models routinely achieve mean Average Precision (mAP) of 50-60. In contrast, even leading open-source model like Qwen2.5VL-72B [9] achieves recall of less than 40%, indicating fundamental inability to reliably locate all relevant object instances. This limitation stems from core architectural mismatch: generating precise numerical coordinates is an unnatural task for models fundamentally designed for sequential language generation [82]. The requirement to produce string of exact floating-point numbers in specific format is brittle; single incorrect token can render an entire bounding box prediction invalid. This problem is exacerbated in scenes with multiple instances, where the generation of long, structured sequence of coordinates challenges the models attention mechanism, leading to low recall and compounding errors. To address this perception weakness, several approaches have been explored. Some methods [17, 97, 105] quantize object coordinates into discrete vocabulary, simplifying the generation task. However, this approach still struggles with multiple instances and suffers from quantization er1 Figure 1. Visualization of VLM-FO1s perception abilities on diverse visual tasks. rors, particularly on high-resolution images. Another strategy [45, 78, 121] involves appending external prediction heads to the VLM to handle localization. While potentially effective, these modules introduce additional latency and often require complex, task-specific loss functions for optimization. Recently, another direction has emerged that employs agentic, vision-level reasoning. Methods like ZoomEye [87] treat the image as navigable structure, allowing the VLM to dynamically zoom in on specific regions to gather fine-grained visual evidence. more promising direction [20, 35, 69] reframes the problem by using an external detection model to generate region proposals, effectively converting the difficult generation task into simpler retrieval task. While innovative, existing methods in this vein have significant drawbacks. They either require joint end-to-end training with the detection model, creating monolithic and cumbersome system, or they necessitate training new architecture from scratch on massive datasets. Crucially, both approaches fail to leverage the rich visual understanding and world knowledge already embedded within large-scale, pre-trained VLMs, effectively discarding powerful and readily available resource. In response to these challenges, we introduce VLM-FO1, 2 novel framework that endows pre-trained VLMs with superior fine-grained perception without compromising their inherent strengths. The core idea is simple: we shift the paradigm from generating box coordinates to directly perceiving the content within them. VLM-FO1 treats any bounding box as visual prompt, extracts its features, and converts them into distinct region tokens that are fed directly into the LLM. This elegantly transforms object detection into simple and accurate retrieval task. Our primary innovations are threefold. First, VLMFO1 is designed as plug-and-play enhancement module that can be integrated with any pre-trained VLM, preserving its original capabilities while dramatically improving perception. Second, we introduce novel Hybrid Finegrained Region Encoder (HFRE), which features DualVision Encoder structure. This combines the VLMs original semantic-rich vision encoder with new perceptionenhanced vision encoder, yielding powerful object features that capture both high-level meaning and fine-grained detail. Third, our two-stage decoupled framework separates the training of the proposal model and the VLM. This modularity grants users the flexibility to pair VLM-FO1 with any proposal detector best suited for their specific application. When combined with our custom-trained Omni Proposal Network (OPN), our lightweight VLM-FO1-3B model achieves 44.4 mAP on COCO, an improvement of over 20 points that places it on par with specialized detectors and far ahead of other VLMs. This strong performance extends to wide variety of other region-related perception tasks, such as Referring Expression Comprehension (REC), object counting, and OCR, demonstrating the versatility and effectiveness of our approach. In summary, our main contributions are: Flexible and Modular Plug-and-Play Framework: We propose VLM-FO1, perception enhancement framework whose two-stage, decoupled design allows it to be seamlessly integrated with any pre-trained VLM. This modularity enables practitioners to use off-the-shelf object detectors for proposal generation, enhancing finegrained perception without requiring full retraining or compromising the VLMs original capabilities. Novel Hybrid Fine-grained Region Encoder (HFRE): We introduce Dual-Vision Encoder architecture that combines semantic-rich vision encoder with perception-enhanced tower to produce region tokens that are rich in both high-level meaning and fine-grained spatial detail. State-of-the-Art Performance: We demonstrate the effectiveness of our method by achieving state-of-the-art results across diverse suite of benchmarks spanning three key perspectives: Object Grounding, Region Generative Understanding, and Visual Region Reasoning, setting new standard for perception-enhanced VLMs. 2. Related Work 2.1. Vision-Language Models (VLMs) Since the emergence of large language models (LLMs)[7, 21, 28, 94], they have achieved remarkable success across wide range of linguistic applications, which has in turn fostered the development of Vision-Language Models (VLMs). Early pioneering works include [5, 43, 50]. Building on these foundations, LLaVA[59] leveraged GPT-4 [3] to construct training data, achieving strong performance in visual dialogue and reasoning, and inspiring line of research on visual instruction data [14, 23, 60]. typical architecture of VLMs encodes visual information through vision encoder [77, 90, 116] and integrates the resulting visual tokens with textual tokens within the LLM backbone. Today, some of the most widely adopted open-source VLM families include LLaVA[48, 59, 61], QwenVL[8, 9, 98], and InternVL [18, 19, 100, 129]. 2.2. VLMs with Detection Enhancement To equip VLMs with detection capability, prior works have explored integrating detection heads [45, 101, 103] or incorporating visual expert models [38, 69, 89, 128]. More generally, most approaches adopt an auto-regressive strategy to sequentially generate the four coordinates of bounding boxes, well aligned with LLM backbones [13, 75, 110]. Building on this paradigm, the Griffon series [117119] progressively unified localization tasks, introduced high-resolution perception structures, and curated multi-dimensional datasets, extending VLMs to both vision-language and vision-centric settings. More recently, general-purpose VLMs [9, 19, 98, 100, 129] trained with detection data have demonstrated strong performance, and reinforcement learning methods such as GRPO [86] have been employed to further enhance visual reasoning for precise detection [67, 88]. Several prior works, notably Groma [69] and ChatRex [35], share similar high-level approach by treating detection as region-token retrieval task with extra detector. However, these methods typically necessitate significant architectural modifications to the base VLM or require joint training with the object detector, mandating costly retraining process from scratch. In contrast, our VLMFO1 employs plug-and-play Hybrid Fine-Grained Region Encoder that seamlessly integrates with existing pretrained VLMs, preserving their powerful, pre-learned representations while producing more representative and betteraligned region features. Furthermore, critical distinction lies in the handling of negative categories. While prior models are often limited to detecting only the positive instances present in an image, our training strategy enables VLM3 Figure 2. An overview of our proposed model architecture. The components enclosed within the blue dotted line represent standard pretrained VLM, which can be initialized with existing weights to preserve its original performance. Our method introduces external modules, including dual-vision encoder system and Hybrid Fine-grained Region Encoder (HFRE), to enrich the VLMs fine-grained perception. These modules process an image to extract and fuse multi-scale region features, which are then fed as new region tokens to the VLM. The inset on the right details the generation of the Hybrid Region Feature. FO1 to robustly distinguish and reject categories specified in the prompt that are not actually present, crucial capability for real-world applications. 2.3. VLMs with Visual Prompt Bounding-box Visual prompts take multiple forms. prompts provide coarse localization, as in Shikra [13] and InstructDET [24], which directly encode user-specified regions for object grounding. Marker-based prompts, such as Set-of-Mark (SoM) [108] and its extension SoMLLaVA [106], overlay symbolic cues (e.g., circles, arrows, IDs) to highlight target regions. ViP-LLaVA [10] extends this idea by supporting arbitrary free-form markers like scribbles and arrows. Pixel-level prompts enable more fine-grained perception. OMG-LLaVA [123] and VisionLLM [99] tokenize pixel-centric features, while DOrA [104] and CoLLaVO [47] integrate pixel-level annotations to enhance semantic grounding. ControlMLLM [102] further leverages training-free visual prompt learning to model semantic pixel-text relations. In addition, soft visual prompts provide learnable perturbations in the pixel space. Transferable Visual Prompting (TVP) [124] explores prompt generalization across MLLMs, while BlackVIP [72] and ILM-VP [12] design task-specific perturbations to adapt pre-trained models efficiently. 3. Methodology 3.1. Model Architecture The overall architecture of VLM-FO1, depicted in Figure 2, is designed as set of plug-and-play modules that augment standard pre-trained VLM. Instead of altering the core VLM or training from scratch, our framework introduces specialized components to process region-level visual information. Given an image and set of bounding box proposals, our model extracts and fuses multi-scale region features, projects them into region tokens, and feeds them alongside global image and text tokens into the Large Language Model. This allows the LLM to perform reasoning grounded in specific, fine-grained visual evidence. The key components of our architecture are the Dual-Vision Encoder and the Hybrid Fine-grained Region Encoder (HFRE). Proposal Regions. core tenet of our VLM-FO1 framework is its two-stage, decoupled design, which makes the generation of proposal regions entirely independent from the VLMs perception module. This separation facilitates independent training and optimization of each component. More importantly, it provides exceptional flexibility, 4 allowing users to switch between different object detectors to generate proposal regions based on specific scenarios, or even manually input regions of interest, all without any additional training. For our experiments and training, we developed an Omni Proposal Network (OPN), variant based on OmDet-Turbo [126], to serve as general-purpose detector. The OPN is trained to identify all potential foreground objects in an image, and we use it to generate the proposal regions for all our training and evaluation data. Dual-Vision Encoder. The foundation of our finegrained understanding capability is the Dual-Vision Encoder, system engineered to produce region features that are simultaneously rich in semantic meaning and perceptual detail. It synergistically combines two components: the VLMs original Primary Vision Encoder and new Auxiliary Vision Encoder. The primary encoder, having been co-trained with the LLM, excels at generating semanticallyaligned features but lacks spatial precision due to its training on lower-resolution, global scenes. To compensate, the auxiliary encoder acts as high-resolution detail specialist, processing the image at higher fidelity to extract multiscale feature maps rich in the perceptual cues (e.g., edges, textures) necessary for precise localization. While the primary encoder continues to provide global image tokens and semantic context for regions, the auxiliary encoder supplies the critical fine-grained information. The outputs from both are then intelligently fused to create superior hybrid region feature representation. Hybrid Fine-grained Region Encoder (HFRE). The HFRE is responsible for processing the multi-scale features from the Dual-Vision Encoder and generating the final region tokens for the LLM. This process involves three main stages: multi-scale feature extraction, hybrid feature fusion, and tokenization. For the auxiliary vision encoder, we select DaViT [26], vision transformer that combines CNN-like multiscale architecture with an efficient dual attention mechanism to capture both fine-grained local details and longrange global context. We extract set of feature maps, {A1, A2, A3, A4}, from its backbone. These maps are upsampled via interpolation to match the spatial dimensions of the largest feature map and then concatenated along the channel dimension to form dense feature map Aconcat . Given proposal bounding boxes = b1, ..., bN , we use RoIAlign [32] followed by mean pooling to extract the corresponding region features, yielding the auxiliary region features Faux RN Da . For the ViT-based primary vision encoder, which lacks native feature pyramid, we introduce Simple Feature Pyramid (SimpleFP) module, inspired by ViTDet [52]. This module takes the last feature map from the encoder and applies series of convolutions and deconvolutions with strides {2, 1, 1/2, 1/4} to construct feature pyramid. Similar to the auxiliary process, we then use RoIAlign to extract region features, resulting in the primary region features Fpri RN Dp . Finally, the two sets of features are fused by concatenation to form combined feature representation, Fcomb = Concat(Fpri , Faux ) RN (Dp+Da). To explicitly provide the model with spatial information, we compute sinecosine positional embeddings Epos from the coordinates of the proposal boxes and add them to the combined features: Fhybrid = Fcomb + Epos . This hybrid feature is then passed through Region-Language Connector, an MLP layer, which projects it into the LLMs embedding space to produce the final region tokens. 3.2. Grounding Language to Regions via Tokenbased Referencing To ground language in specific visual regions, our framework, inspired by previous works [35, 69], establishes token-based referencing system that enriches the LLMs input with explicit, addressable region-level information. We augment the standard VLM input of image and text tokens by introducing our new region tokens. To enable the LLM to distinguish between and reference specific regions, we introduce set of special tokens, <region0>, <region1>, ..., <regionN-1>, which serve as unique region index tokens. The input to the LLM is structured as an interleaved sequence where each region token is preceded by its corresponding index token. This results in final format of: <image_tokens>n<region0><region_token> . . .<regionN - 1 > <region _ token > n<text _ tokens>. This structure allows the LLM to directly associate the region features with unique identifier. Consequently, user can refer to specific regions within text prompt by simply using the corresponding index token. For the models output, we introduce special tokens to handle grounding tasks. The <ground></ground> tokens demarcate noun phrase in the response that requires grounding, and the <object></object> tokens enclose the region index tokens that correspond to that phrase. For instance, valid grounded response would be: The <ground>people</ground> <object> <region2> <region10> </object> are dancing. This format provides an unambiguous link between textual concepts and their visual referents. For tasks that require simpler referencing without explicit grounding, the model can directly refer the region index token within its natural language response. This structured output format transforms complex localization into native referencing task for the LLM. 3.3. Training Strategy The training of VLM-FO1 is conducted in two distinct stages designed to efficiently integrate fine-grained perType Sub-type Model MSCOCO val2017 ODinW13 OVDEval Detection Model OD OVD VLM Close-source Open-source OBJ-enhanced Faster RCNN [80] DETR [11] DINO [120] GLIP [51] Grounding DINO [63] OmDet-Turbo[126] Gemini 1.5 Pro [91] GPT-4o [34] InternVL2.5-8B [18] InternVL2.5-72B Qwen2.5-VL-7B [9] Qwen2.5-VL-72B VLM-R1-7B [88] Lumen [38] Griffon v2 [118] Griffon-G-7B [117] ChatRex-7B [35] 42.0 43.3 49.4 49.8 52.5 53. - 3.1 12.1 - 17.7 - - 35.3 38.5 40.2 4.3(48.2 reported) VLM-FO1-3B(Ours) 44.4 - - - 52.1 55.7 54.1 36.7* - 20.2* 31.7* 37.3* 43.1* - - - 43.8* - 44.0 - - - 18.4 25.3 25.9 - - - - - - 31.0 - - - - 43.7 Table 1. Object Grounding performance on OD benchmarks. * indicates evaluation under simplified setting where only ground-truth categories are queried. ception while preserving the models extensive pre-trained knowledge. Stage 1: Region-Language Alignment Training. The primary objective of this initial stage is to align the newly introduced region tokens with the LLMs feature space with minimal disruption to the existing VLM weights. To achieve this, we first extend the LLMs vocabulary with our special tokens (e.g., <RegionN>, <ground>) and freeze the embeddings of the original vocabulary, ensuring that only the new token embeddings are updated. Concurrently, we freeze the parameters of the entire pre-trained VLM, including the primary vision encoder and the LLM itself. Training is focused exclusively on the newly added modules: the HFRE and the Region-Language Connector. This isolated training strategy allows the model to learn robust mapping from visual regions to token space. Stage 2: Perception Instruction Finetuning. The second stage aims to holistically enhance the models perception capabilities by fine-tuning the integrated system on broader set of instruction-based tasks. In this phase, we unfreeze the parameters of the Auxiliary Vision Encoder, the HFRE, the Region-Language Connector, and the LLM. The Primary Vision Encoder remains frozen throughout the entire training process, acting as stable anchor for the original VLMs semantic understanding. The training dataset is expanded to include wider variety of perception-focused instruction data. 4. Experiments 4.1. Experimental Setup Model Setup. Our experiments are built upon the Qwen2.5VL model, chosen for its excellent baseline performance in visual understanding. For the auxiliary vision encoder, we integrate pre-trained DaViT-Large model. Within the HFRE module, the auxiliary encoder extracts features from 4 multi-scale layers, resulting in region feature of dimension 3840. The primary vision encoder utilizes the SimpleFP module to generate 4 multi-scale features, each with dimension of 512, which are then combined into 2048dimension feature. The final hybrid region feature thus has dimension of 5888. For our two-stage training, we set the learning rate to 1e-3 for Stage 1 and 1e-5 for Stage 2. For each image, we process maximum of 100 input proposals, selecting the top 100 predictions from our OPN based on their confidence scores. Training Data. Our training data is structured to support our two-stage training strategy, as summarized in Table 10. In this stage, Stage 1 (Region-Language Alignment): training is focused on aligning the visual features from the HFRE with the LLMs embedding space. To achieve this, we use curated collection of datasets centered on region-language tasks. This includes large-scale object detection datasets (COCO [54], O365 [85], V3Det [96]), grounding data (GOLDG [39]), and region caption data (Rexverse-2M [35])."
        },
        {
            "title": "PACO",
            "content": "SS S-IoU SS S-IoU LLaVA-1.5 [59] Kosmos-2 [75] Shikra-7B [13] GPT4RoI-7B Ferret-7B [110] Osprey-7B [113] VisionLLM v2-7B [101] VP-SPHINX-13B [55] DAM-8B [53] PAM-3B [56] ChatRex-7B [35] VLM-FO1-3B (Ours) 49.0 39.0 49.7 51.3 63.8 65.2 68.9 87.1 89.0 88.6 89.8 92.4 19.8 8.7 19.8 12.0 36.6 38.2 46.3 62.9 77.7 78.3 82.6 86.4 42.2 32.1 43.6 48.0 58.7 73.1 67.7 76.8 84.2 87.4 91.4 88. 14.6 4.8 11.4 12.1 26.0 52.7 44.0 51.3 73.2 74.9 85.1 77.6 Table 2. Region-level classification performance of VLMs on LVIS and PACO datasets. Stage 2 (Perception SFT): The second stage broadens the models capabilities by training on diverse mix of perception-focused instruction datasets. In addition to the data from Stage 1, we incorporate datasets for REC, grounding, region captioning, region reasoning, counting, region QA, and OCR. Furthermore, for detection-related tasks, we introduce rejection samples for 20% of the data, where the model is prompted to find objects that are not present in the image. This strategy encourages the model to be more discerning and avoid hallucinating objects based solely on the text prompt. Crucially, to mitigate catastrophic forgetting, we also include subset of data from the OmChat-SFT collection (which contains data from LLaVA-1.5 [60], The Cauldron[46], CogVLM[33], etc.). This mix of conventional VLM task data ensures that the model retains its high-level scene interpretation abilities while mastering fine-grained perception. 4.2. Main Results To comprehensively assess the effectiveness of VLM-FO1, we assess its capabilities across three key dimensions: Object Grounding, Region Generative Understanding, and Visual Region Reasoning. We benchmark our model on diverse suite of tasks within each of these areas. Object Grounding. We first evaluate the models core ability to ground language in objects through detection tasks. As shown in Table 1, we benchmark VLM-FO1 on standard object detection with COCO [54], open-vocabulary detection in real-world settings with ODinW13 [49], and challenging language-based detection with hard negatives on OVDEval [109]. The results clearly demonstrate VLM-FO1s superiority. On COCO and ODinW13, VLM-FO1 significantly out-"
        },
        {
            "title": "Model",
            "content": "ChatSpot-7B [125] PAM-3B [56] VP-LLAVA-8B [55] VP-SPHINX-13B [55] VLM-FO1-3B (Ours) COCO Text Accuracy(%) 31.8 42.2 44.8 45.4 59.0 Table 3. Regional OCR performance on COCOText benchmark."
        },
        {
            "title": "Model",
            "content": "LLaVA-7B [59] Kosmos-2 [75] Osprey-7B [113] Ferret-13B [110] Ferret-v2-13B [122] VP-LLAVA-8B [55] VP-SPHINX-13B [55] VLM-FO1-3B (Ours) Ferret Bench Refer. Reasoning 31.7 33.7 67.8 68.7 79.4 68.9 71.4 80.1 Table 4. Referring Reasoning performance of Ferret Bench. performs other VLM-based models, showcasing its powerful perception and high recall. For instance, GPT-4o [34] achieves mere 3.1 mAP on COCO, confirming that even the most advanced models fail at direct coordinate regression. While ChatRex-7B [35] reports high mAP of 48.2, this is achieved under non-standard evaluation protocol where only the ground-truth categories for each image are provided as queries; under standard COCO evaluation, its performance drops to 4.3 mAP, likely due to an inability to handle negative categories. VLM-FO1 successfully overcomes both of these fundamental challenges. More impressively, on ODinW13, our model achieves the highest score despite being tested under the rigorous, standard mAP protocol. It is important to note that many other VLMs (marked with *) are evaluated on ODinW13 using simplified setting where only ground-truth categories are fed to the model individually. This easier setting avoids the challenge of distinguishing hard negatives. Even against models tested in this simplified manner, VLM-FO1, under standard evaluation, still comes out on top. The most compelling results are on OVDEval, which evaluates performance on linguistic labels with hard negatives. Here, VLM-FO1 surpasses not only other VLMs but also specialized detection models like Grounding DINO. This highlights key advantage of our approach: VLMFO1 effectively leverages the world knowledge, entity"
        },
        {
            "title": "Refcoco",
            "content": "Refcoco+"
        },
        {
            "title": "HumanRef",
            "content": "val testA testB val testA testB val test Gemini 1.5 Pro [91] DINOX [81] Grounding DINO [63] InternVL-2.5-8B [18] Ferret-7B [110] Groma-7B [69] ChatRex-7B [35] Qwen2.5-VL-7B [9] Molmo-7B-D [25] RexSeek-7B [37] 73.2 - 90.6 90.3 87.5 89.5 91.0 90.0 - - VLM-FO1-3B(Ours) 91.12 72.9 - 93.2 94.5 91.4 92.1 94.1 92.5 - - 93.7 74.6 - 88.2 85.9 82.5 86.3 87.0 85.4 - - 87.6 62.5 - 88.2 85.2 80.8 83.9 89.8 84.2 - - 86.4 63.9 - 89.0 91.5 87.4 88.9 91.9 89.1 - - 91.9 65.0 - 75.9 78.8 73.1 78.1 79.3 76.9 - - 80.6 75.2 - 86.1 86.7 83.9 86.37 89.8 87.2 - 84.0 76.2 - 87.0 87.6 84.8 87.01 90.0 87.2 - 84.4 - 33.1 33.1 37.8 43.2 48.7 72.2 68.5 82.5 85. 88.9 88.3 87.1 83.3 - 75.2 75.2 29.8 34.4 65.9 50.4 52.5 77.7 85. DF1 - 23.3 23.3 31.9 34.3 42.1 55.6 56.2 72.6 82.3 82.6 Table 5. Model Performance on Referring Benchmarks Type Model Close Source GPT-4V [1] GPT-4o-0513 [34] Gemini 1.5 Pro [91] Claude-3 Opus [6] Claude-3.5 Sonnet [6] Open Source LLaVA-1.5-13B [60] LLaVA OneVision-72B [48] InternVL2-8B [18] InternVL2-Llama-376B [18] InternVL2.5-78B [18] Qwen2-VL-72B [98] Pixtral-12B [4] Llama-3.2V-90BInstruct [28] Molmo-7B-D [25] Molmo-72B [25] VLM-FO1-3B (Ours) CountBench Acc(%) PixMo Count 69.9 87.9 85.8 83.6 89.7 47.1 84.3 57.8 74.7 72.1 80.4 78.8 78.5 88.5 91.2 87.8 45.0 59.6 64.3 43.3 58. 35.2 60.7 43.9 54.6 - 55.7 51.7 58.5 84.8 85.2 86.0 Table 6. Model Performance on Object Counting Benchmarks recognition, and reasoning abilities inherited from its VLM foundation to disambiguate complex and challenging text prompts. Region Generative Understanding. We further evaluate our models ability to understand and generate accurate textual descriptions based on specific visual regions. This is tested across three diverse tasks: region-level classification  (Table 2)  , region-based OCR  (Table 3)  , and referring reasoning  (Table 4)  . The results unequivocally demonstrate VLM-FO1s SOTA capabilities. On the object-level LVIS and part-level PACO [79] datasets, our model sets new state-of-the-art for region classification, with our efficient 3B model outperforming significantly larger 8B and 13B models. Our architecture demonstrates strong capability for generating precise text targeting fine-grained regions. On the COCOText [95] benchmark for regional OCR, VLM-FO1 achieves staggering 59.0% accuracy, surpassing the next best model by over 13 points. Finally, on the challenging referring reasoning subset of Ferret Bench [110], our model achieves new SOTA score of 80.1, demonstrating that its strong fine-grained perception directly translates to more accurate understanding of specific visual regions and their relationships. Visual Region Reasoning. In this section, we evaluate the models ability to leverage its fine-grained region features to perform complex reasoning. We benchmark this on two challenging task families: Referring Expression Comprehension  (Table 5)  and Object Counting  (Table 6)  . In REC tasks such as Refcoco/+/g, the model must reason about natural language description to identify the correct object. Our model achieves consistently top-tier results across these benchmarks. More significantly, on HumanRef [37], difficult benchmark focusing on people with complex descriptions (attributes, positions, interactions) and hard negatives, VLM-FO1 demonstrates remarkable performance, achieving new state-of-the-art. This success underscores its robust reasoning capability and its skill in disambiguating between visually similar instances based on nuanced language. Furthermore, in Object Counting, task notorious for causing failures in large VLMs, our model excels by adopting Detect-then-Count reasoning process. It first localizes all target instances and then aggregates the count, leading to superior accuracy on CountBench [73] and the challenging PixMo-Count [25] benchmark. This methodical approach allows our compact V-FO1-3B to outperform even much larger closed-source 8 Model AVG MMBench v1.1 [65] AI2D [42] MMStar [16] Hallusion Bench [30] OCR Bench [66] MathVista [68] MMVet [68] MMMU Val [114] Qwen2.5-VL-3B VLM-FO1-3B 64. 64.6 76.8 78.2 81.4 81.2 56. 56.9 46.6 47.9 82.8 82.3 61. 65.6 60 54.9 51.2 49.9 Table 7. Comparison of general VLM capabilities on OpenCompass benchmarks. Model Average Score"
        },
        {
            "title": "Average Score",
            "content": "VLM-FO1-3B VLM-FO1-3B (QwenViT unfrozen) Only Aux. Region Feat. (DaViT unfrozen) Only Prim. Region Feat. (QwenViT frozen) Only Prim. Region Feat. (QwenViT unfrozen) 67.65 66.35 65.89 65.76 66.15 Table 8. Ablation study for HFRE module. (Aux. Region Feat. denotes Auxiliary Region Feature, and Prim. Region Feat. denotes Primary Region Feature.) models like GPT-4V and open-source models up to 72B parameters, highlighting the power of grounding complex reasoning in accurate, fine-grained perception. 4.3. Preservation of General VLM Capabilities critical aspect of our framework is its ability to enhance fine-grained perception without degrading the base models general visual understanding capabilities. To validate this, we evaluated our VLM-FO1-3B model against the original Qwen2.5-VL-3B on the comprehensive OpenCompass [22] benchmark. The results, shown in Table 7, confirm that our method successfully avoids catastrophic forgetting."
        },
        {
            "title": "Across",
            "content": "a wide range of general VLM benchmarksincluding MMBench [65], AI2D [42], and MMStar [16]our VLM-FO1 models performance remains virtually identical to the base Qwen2.5-VL model, with negligible difference in the average score. This demonstrates the effectiveness of our two-stage training strategy; by initially freezing the core VLM during the Region-Language Alignment phase and subsequently mixing in general VLM instruction data during the second stage, our method successfully prevents the degradation of pre-existing knowledge. The results confirm that VLM-FO1 acts as true enhancement module, allowing pre-trained VLMs to gain superior fine-grained perception while retaining their powerful and general visual understanding abilities. Only Prim. Region Feat. with SimpleFP Only Prim. Region Feat. w/o SimpleFP 66.15 64. Table 9. Impact of SimpleFP on Primary Region Feature Performance. 5. Ablation Studies To validate the effectiveness of the individual components in our model design, we conduct series of ablation studies. For efficiency, all ablation experiments are conducted by training on representative subset of our full perception SFT dataset. The Average Score reported in this section is an average calculated from the benchmark scores evaluated in the previous sections. 5.1. Effectiveness of Components in HFRE We first analyze the contribution of the different components within the HFRE module. As shown in Table 8, we evaluate the effect of different feature combinations and training strategies. Our full model, VLM-FO1-3B, which combines primary and auxiliary region features and keeps the primary vision encoder (QwenViT) frozen, achieves the highest average score of 67.65. In contrast, using only the auxiliary region feature (65.89) or only the primary region feature (66.15) leads to drop in performance, demonstrating the importance of combining both semantic and perceptual information from the two vision encoders. Furthermore, we observe that fine-tuning the primary vision encoder (66.35) slightly degrades performance, suggesting that freezing the original, well-aligned vision encoder helps to preserve its valuable semantic priors. These results strongly validate the effectiveness of our HFRE design. 5.2. Effectiveness of the Simple Feature Pyramid Next, we evaluate the effectiveness of introducing the SimpleFP module for the ViT-based primary vision encoder. As shown in Table 9, in controlled setting using only the primary region feature, we compare the performance with and 9 without the SimpleFP module. The results show that removing the SimpleFP module causes significant drop in the average score from 66.15 to 64.94. This performance gap clearly indicates that constructing multi-scale feature pyramid from the ViTs single-scale feature map is critical for extracting high-quality, information-rich region features. 6. Conclusion In this work, we introduced VLM-FO1, novel framework that successfully bridges the gap between the high-level reasoning of Vision-Language Models and the demands of fine-grained visual perception. By reframing object-centric tasks from coordinate generation problem to feature retrieval task, we circumvent fundamental limitation of language-centric architectures. Our proposed method, featuring plug-and-play modular design and an innovative Hybrid Fine-grained Region Encoder, effectively enhances pre-trained VLMs with state-of-the-art perception capabilities. Our extensive experiments demonstrate that VLMFO1 achieves exceptional performance across wide range of benchmarks spanning object grounding, region-level understanding, and complex visual reasoning, often outperforming much larger models. Crucially, these gains are achieved without compromising the base models original general-purpose abilities, validating our training strategy and architectural design. VLM-FO1 establishes powerful and flexible paradigm for developing the next generation of VLMs, paving the way for models with deeper, more actionable understanding of the visual world."
        },
        {
            "title": "References",
            "content": "[1] Gpt-4v(ision) system card. 2023. 8 [2] Manoj Acharya, Kushal Kafle, and Christopher Kanan. Tallyqa: Answering complex counting questions. In Proceedings of the AAAI conference on artificial intelligence, pages 80768084, 2019. 2 [3] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 1, 3 [4] Pravesh Agrawal, Szymon Antoniak, Emma Bou Hanna, Baptiste Bout, Devendra Chaplot, Jessica Chudnovsky, Diogo Costa, Baudouin De Monicault, Saurabh Garg, arXiv preprint Theophile Gervet, et al. arXiv:2410.07073, 2024. 8 Pixtral 12b. [5] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: visual language model for few-shot learning. Advances in neural information processing systems, 35: 2371623736, 2022. 3 [6] AI Anthropic. The claude 3 model family: Opus, sonnet, haiku. Claude-3 Model Card, 1(1):4, 2024. 8 [7] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei arXiv preprint Huang, et al. Qwen technical report. arXiv:2309.16609, 2023. 1, 3 [8] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: versatile vision-language model for understanding, localization, text reading, and beyond. arXiv preprint arXiv:2308.12966, 2023. 3 [9] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. 1, 3, 6, 8 [10] Mu Cai, Haotian Liu, Siva Karthik Mustikovela, Gregory P. Meyer, Yuning Chai, Dennis Park, and Yong Jae Lee. Making large multimodal models understand arbitrary visual prompts. In IEEE Conference on Computer Vision and Pattern Recognition, 2024. 4 [11] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. In European conference on computer vision, pages 213229. Springer, 2020. [12] Aochuan Chen, Yuguang Yao, Pin-Yu Chen, Yihua Zhang, and Sijia Liu. Understanding and improving visual promptIn Proceedings of the ing: label-mapping perspective. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1913319143, 2023. 4 [13] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Shikra: Unleashing multiarXiv preprint Feng Zhu, and Rui Zhao. modal llms referential dialogue magic. arXiv:2306.15195, 2023. 3, 4, 7 [14] Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v: Improving large multi-modal models with better captions. arXiv preprint arXiv:2311.12793, 2023. 3 [15] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v: Improving large multi-modal models with better captions. In European Conference on Computer Vision, pages 370 387. Springer, 2024. 2 [16] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, et al. Are we on the right way for evaluating large vision-language models? Advances in Neural Information Processing Systems, 37:2705627087, 2024. 9 [17] Ting Chen, Saurabh Saxena, Lala Li, David Fleet, and Geoffrey Hinton. Pix2seq: language modeling framework arXiv preprint arXiv:2109.10852, for object detection. 2021. [18] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and testtime scaling. arXiv preprint arXiv:2412.05271, 2024. 3, 6, 8 10 [19] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with opensource suites. arXiv preprint arXiv:2404.16821, 2024. 1, 3 [20] An-Chieh Cheng, Yang Fu, Yukang Chen, Zhijian Liu, Xiaolong Li, Subhashree Radhakrishnan, Song Han, Yao 3d aware reLu, Jan Kautz, Pavlo Molchanov, et al. arXiv preprint gion prompted vision language model. arXiv:2509.13317, 2025. 2 [21] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instructionfinetuned language models. Journal of Machine Learning Research, 25(70):153, 2024. 3 [22] OpenCompass Contributors. Opencompass: universal evaluation platform for foundation models. https: / / github . com / open - compass / opencompass, 2023. [23] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards generalpurpose vision-language models with instruction tuning, 2023. 3 [24] Ronghao Dang, Jiangyan Feng, Haodong Zhang, Chongjian Ge, Lin Song, Lijun Gong, Chengju Liu, Instructdet: Qijun Chen, Feng Zhu, Rui Zhao, et al. Diversifying referring object detection with generalized instructions. arXiv preprint arXiv:2310.05136, 2023. 4 [25] Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, et al. Molmo and pixmo: Open weights and open data for state-of-theart multimodal models. arXiv e-prints, pages arXiv2409, 2024. 1, 8 [26] Mingyu Ding, Bin Xiao, Noel Codella, Ping Luo, Jingdong Wang, and Lu Yuan. Davit: Dual attention vision transformers. In European conference on computer vision, pages 7492. Springer, 2022. 5 [27] Dawei Du, Pengfei Zhu, Longyin Wen, Xiao Bian, Haibin Lin, Qinghua Hu, Tao Peng, Jiayu Zheng, Xinyao Wang, Yue Zhang, et al. Visdrone-det2019: The vision meets drone object detection in image challenge results. In Proceedings of the IEEE/CVF international conference on computer vision workshops, pages 00, 2019. 2 [28] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv e-prints, pages arXiv2407, 2024. 1, 3, [29] Team GLM, Aohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, Da Yin, Dan Zhang, Diego Rojas, Guanyu Feng, Hanlin Zhao, et al. Chatglm: family of large language models from glm-130b to glm-4 all tools. arXiv preprint arXiv:2406.12793, 2024. 1 [30] Tianrui Guan, Fuxiao Liu, Xiyang Wu, Ruiqi Xian, Zongxia Li, Xiaoyu Liu, Xijun Wang, Lichang Chen, Furong Huang, Yaser Yacoob, et al. Hallusionbench: an advanced diagnostic suite for entangled language hallucination and visual illusion in large vision-language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1437514385, 2024. 9 [31] Agrim Gupta, Piotr Dollar, and Ross Girshick. Lvis: dataset for large vocabulary instance segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 53565364, 2019. 2 [32] Kaiming He, Georgia Gkioxari, Piotr Dollar, and Ross GirIn Proceedings of the IEEE internashick. Mask r-cnn. tional conference on computer vision, pages 29612969, 2017. 5 [33] Wenyi Hong, Weihan Wang, Ming Ding, Wenmeng Yu, Qingsong Lv, Yan Wang, Yean Cheng, Shiyu Huang, Junhui Ji, Zhao Xue, et al. Cogvlm2: Visual language modarXiv preprint els for image and video understanding. arXiv:2408.16500, 2024. 1, 7 [34] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. 6, 7, 8 [35] Qing Jiang, Gen Luo, Yuqin Yang, Yuda Xiong, Yihao Chen, Zhaoyang Zeng, Tianhe Ren, and Lei Zhang. Chatrex: Taming multimodal llm for joint perception and understanding. arXiv preprint arXiv:2411.18363, 2024. 2, 3, 5, 6, 7, [36] Qing Jiang, Xingyu Chen, Zhaoyang Zeng, Junzhi Yu, Rex-thinker: Grounded object rearXiv preprint and Lei Zhang. ferring via chain-of-thought reasoning. arXiv:2506.04034, 2025. 2 [37] Qing Jiang, Lin Wu, Zhaoyang Zeng, Tianhe Ren, Yuda Xiong, Yihao Chen, Qin Liu, and Lei Zhang. Referring to any person. arXiv preprint arXiv:2503.08507, 2025. 8 [38] Yang Jiao, Shaoxiang Chen, Zequn Jie, Jingjing Chen, Lin Ma, and Yu-Gang Jiang. Lumen: Unleashing versatile vision-centric capabilities of large multimodal models. Advances in Neural Information Processing Systems, 37: 8146181488, 2024. 3, 6 [39] Aishwarya Kamath, Mannat Singh, Yann LeCun, Gabriel Ishan Misra, and Nicolas Carion. MdetrSynnaeve, modulated detection for end-to-end multi-modal underIn Proceedings of the IEEE/CVF international standing. conference on computer vision, pages 17801790, 2021. 6, 2 [40] Dimosthenis Karatzas, Lluis Gomez-Bigorda, Anguelos Nicolaou, Suman Ghosh, Andrew Bagdanov, Masakazu Iwamura, Jiri Matas, Lukas Neumann, Vijay Ramaseshan Chandrasekhar, Shijian Lu, et al. Icdar 2015 competition on robust reading. In 2015 13th international conference on document analysis and recognition (ICDAR), pages 1156 1160. IEEE, 2015. 2 [41] Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and Tamara Berg. Referitgame: Referring to objects in photographs of natural scenes. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 787798, 2014. 11 [42] Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi. diagram is In European conference on comworth dozen images. puter vision, pages 235251. Springer, 2016. 9 [43] Jing Yu Koh, Ruslan Salakhutdinov, and Daniel Fried. Grounding language models to images for multimodal inputs and outputs. In International Conference on Machine Learning, pages 1728317300. PMLR, 2023. 3 [44] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David Shamma, et al. Visual genome: Connecting language and vision using crowdsourced dense International journal of computer viimage annotations. sion, 123(1):3273, 2017. 2 [45] Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan, Shu Liu, and Jiaya Jia. Lisa: Reasoning segmentation via large language model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 95799589, 2024. 2, 3 [46] Hugo Laurencon, Leo Tronchon, Matthieu Cord, and Victor Sanh. What matters when building vision-language models? Advances in Neural Information Processing Systems, 37:8787487907, 2024. 1, [47] Byung-Kwan Lee, Beomchan Park, Chae Won Kim, and Yong Man Ro. Collavo: Crayon large language and vision model. arXiv preprint arXiv:2402.11248, 2024. 4 [48] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. 3, 8 [49] Chunyuan Li, Haotian Liu, Liunian Li, Pengchuan Zhang, Jyoti Aneja, Jianwei Yang, Ping Jin, Houdong Hu, Zicheng Liu, Yong Jae Lee, et al. Elevater: benchmark and toolkit for evaluating language-augmented visual models. Advances in Neural Information Processing Systems, 35: 92879301, 2022. 7 [50] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International conference on machine learning, pages 19730 19742. PMLR, 2023. 3 [51] Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jianwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, et al. Grounded the language-image pre-training. IEEE/CVF conference on computer vision and pattern recognition, pages 1096510975, 2022. 6 In Proceedings of [52] Yanghao Li, Hanzi Mao, Ross Girshick, and Kaiming He. Exploring plain vision transformer backbones for object detection. In European conference on computer vision, pages 280296. Springer, 2022. 5 [53] Long Lian, Yifan Ding, Yunhao Ge, Sifei Liu, Hanzi Mao, Boyi Li, Marco Pavone, Ming-Yu Liu, Trevor Darrell, Adam Yala, and Yin Cui. Describe anything: Detailed localized image and video captioning. arXiv preprint arXiv:2504.16072, 2025. 7 [54] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and Lawrence Zitnick. Microsoft coco: Common objects in context. In European conference on computer vision, pages 740755. Springer, 2014. 1, 6, 7 [55] Weifeng Lin, Xinyu Wei, Ruichuan An, Peng Gao, Bocheng Zou, Yulin Luo, Siyuan Huang, Shanghang Zhang, and Hongsheng Li. Draw-and-understand: Leveraging visual prompts to enable mllms to comprehend what you want. arXiv preprint arXiv:2403.20271, 2024. 7, 2 [56] Weifeng Lin, Xinyu Wei, Ruichuan An, Tianhe Ren, Tingwei Chen, Renrui Zhang, Ziyu Guo, Wentao Zhang, Lei Zhang, and Hongsheng Li. Perceive anything: Recognize, explain, caption, and segment anything in images and videos. arXiv preprint arXiv:2506.05302, 2025. 7 [57] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. [58] Chang Liu, Henghui Ding, and Xudong Jiang. Gres: Generalized referring expression segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2359223601, 2023. 2 [59] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023. 1, 3, 7 [60] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2629626306, 2024. 1, 3, 7, 8 [61] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, 2024. 3 [62] Junzhuo Liu, Xuzheng Yang, Weiwei Li, and Peng Wang. Finecops-ref: new dataset and task for fine-grained compositional referring expression comprehension. arXiv preprint arXiv:2409.14750, 2024. 2 [63] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Qing Jiang, Chunyuan Li, Jianwei Yang, Hang Su, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. In European conference on computer vision, pages 3855. Springer, 2024. 6, 8 [64] Yuliang Liu, Hao Chen, Chunhua Shen, Tong He, Lianwen Jin, and Liangwei Wang. Abcnet: Real-time scene text In proceedspotting with adaptive bezier-curve network. ings of the IEEE/CVF conference on computer vision and pattern recognition, pages 98099818, 2020. [65] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multimodal model an all-around player? In European conference on computer vision, pages 216233. Springer, 2024. 9 [66] Yuliang Liu, Zhang Li, Mingxin Huang, Biao Yang, Wenwen Yu, Chunyuan Li, Xu-Cheng Yin, Cheng-Lin Liu, Lianwen Jin, and Xiang Bai. Ocrbench: on the hidden mystery of ocr in large multimodal models. Science China Information Sciences, 67(12):220102, 2024. 9 12 [67] Ziyu Liu, Zeyi Sun, Yuhang Zang, Xiaoyi Dong, Yuhang Cao, Haodong Duan, Dahua Lin, and Jiaqi Wang. VisualarXiv preprint rft: Visual reinforcement fine-tuning. arXiv:2503.01785, 2025. 3 [68] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255, 2023. 9 [69] Chuofan Ma, Yi Jiang, Jiannan Wu, Zehuan Yuan, and Xiaojuan Qi. Groma: Localized visual tokenization for grounding multimodal large language models. In European Conference on Computer Vision, pages 417435. Springer, 2024. 2, 3, 5, 8 [70] Junhua Mao, Jonathan Huang, Alexander Toshev, Oana Camburu, Alan Yuille, and Kevin Murphy. Generation and comprehension of unambiguous object descriptions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1120, 2016. [71] Nibal Nayef, Yash Patel, Michal Busta, Pinaki Nath Chowdhury, Dimosthenis Karatzas, Wafa Khlif, Jiri Matas, Umapada Pal, Jean-Christophe Burie, Cheng-lin Liu, et al. Icdar2019 robust reading challenge on multi-lingual scene text detection and recognitionrrc-mlt-2019. In 2019 International conference on document analysis and recognition (ICDAR), pages 15821587. IEEE, 2019. 2 [72] Changdae Oh, Hyeji Hwang, Hee-young Lee, YongTaek Lim, Geunyoung Jung, Jiyoung Jung, Hosik Choi, and Kyungwoo Song. Blackvip: Black-box visual promptIn Proceedings of the ing for robust transfer learning. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2422424235, 2023. 4 [73] Roni Paiss, Ariel Ephrat, Omer Tov, Shiran Zada, Inbar Mosseri, Michal Irani, and Tali Dekel. Teaching clip to count to ten. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 31703180, 2023. 8 [74] Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, Instruction tuning with gpt-4. arXiv and Jianfeng Gao. preprint arXiv:2304.03277, 2023. 1 [75] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei. Kosmos-2: Grounding multimodal large language models to the world. arXiv preprint arXiv:2306.14824, 2023. 3, 7, 2 [76] Birgit Pfitzmann, Christoph Auer, Michele Dolfi, Ahmed Nassar, and Peter Staar. Doclaynet: large humanannotated dataset for document-layout segmentation. In Proceedings of the 28th ACM SIGKDD conference on knowledge discovery and data mining, pages 37433751, 2022. [77] Khoi Pham, Kushal Kafle, Zhe Lin, Zhihong Ding, Scott Cohen, Quan Tran, and Abhinav Shrivastava. Learning to predict visual attributes in the wild. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1301813028, 2021. 3, 2 [78] Renjie Pi, Jiahui Gao, Shizhe Diao, Rui Pan, Hanze Dong, Jipeng Zhang, Lewei Yao, Jianhua Han, Hang Xu, Lingpeng Kong, et al. Detgpt: Detect what you need via reasoning. arXiv preprint arXiv:2305.14167, 2023. 2 [79] Vignesh Ramanathan, Anmol Kalia, Vladan Petrovic, Yi Wen, Baixue Zheng, Baishan Guo, Rui Wang, Aaron Marquez, Rama Kovvuri, Abhishek Kadian, et al. Paco: Parts In Proceedings of the and attributes of common objects. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 71417151, 2023. 8, 2 [80] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. Advances in neural information processing systems, 28, 2015. 6 [81] Tianhe Ren, Yihao Chen, Qing Jiang, Zhaoyang Zeng, Yuda Xiong, Wenlong Liu, Zhengyu Ma, Junyi Shen, Yuan Gao, Xiaoke Jiang, et al. Dino-x: unified vision model for open-world object detection and understanding. arXiv preprint arXiv:2411.14347, 2024. 8 [82] Karthick Panner Selvam. Why large language models fail at precision regression, 2025. 1 [83] Hao Shao, Shengju Qian, Han Xiao, Guanglu Song, Zhuofan Zong, Letian Wang, Yu Liu, and Hongsheng Li. Visual cot: Unleashing chain-of-thought reasoning in multi-modal language models. CoRR, 2024. 2 [84] Shuai Shao, Zijian Zhao, Boxun Li, Tete Xiao, Gang Yu, Xiangyu Zhang, and Jian Sun. Crowdhuman: bencharXiv preprint mark for detecting human in crowd. arXiv:1805.00123, 2018. 2 [85] Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang Yu, Xiangyu Zhang, Jing Li, and Jian Sun. Objects365: large-scale, high-quality dataset for object detection. In Proceedings of the IEEE/CVF international conference on computer vision, pages 84308439, 2019. 6, 2 [86] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. 1, 3 [87] Haozhan Shen, Kangjia Zhao, Tiancheng Zhao, Ruochen Xu, Zilun Zhang, Mingwei Zhu, and Jianwei Yin. Zoomeye: Enhancing multimodal llms with human-like zooming capabilities through tree-based image exploration. arXiv preprint arXiv:2411.16044, 2024. [88] Haozhan Shen, Peng Liu, Jingcheng Li, Chunxin Fang, Yibo Ma, Jiajia Liao, Qiaoli Shen, Zilun Zhang, Kangjia Zhao, Qianqian Zhang, et al. Vlm-r1: stable and generalizable r1-style large vision-language model. arXiv preprint arXiv:2504.07615, 2025. 3, 6 [89] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hugginggpt: Solving ai tasks with chatgpt and its friends in hugging face. Advances in Neural Information Processing Systems, 36: 3815438180, 2023. 3 [90] Quan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, and Yue Cao. Eva-clip: Improved training techniques for clip at scale. arXiv preprint arXiv:2303.15389, 2023. 3 [91] Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. 6, 8 13 [92] InternLM Team. Internlm: multilingual language model with progressively enhanced capabilities, 2023. 1 [93] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. 1 [94] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. 3 [95] Andreas Veit, Tomas Matera, Lukas Neumann, Jiri Matas, and Serge Belongie. Coco-text: Dataset and benchmark for text detection and recognition in natural images. arXiv preprint arXiv:1601.07140, 2016. 8 [96] Jiaqi Wang, Pan Zhang, Tao Chu, Yuhang Cao, Yujie Zhou, Tong Wu, Bin Wang, Conghui He, and Dahua Lin. V3det: Vast vocabulary visual detection dataset. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1984419854, 2023. 6, 2 [97] Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, and Hongxia Yang. Ofa: Unifying architectures, tasks, and modalities through simple sequence-to-sequence learning framework. In International conference on machine learning, pages 2331823340. PMLR, 2022. [98] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 1, 3, 8 [99] Wenhai Wang, Zhe Chen, Xiaokang Chen, Jiannan Wu, Xizhou Zhu, Gang Zeng, Ping Luo, Tong Lu, Jie Zhou, Yu Qiao, and Jifeng Dai. Visionllm: Large language model is also an open-ended decoder for vision-centric tasks, 2023. 4 [100] Weiyun Wang, Zhangwei Gao, Lixin Gu, Hengjun Pu, Long Cui, Xingguang Wei, Zhaoyang Liu, Linglin Jing, Shenglong Ye, Jie Shao, et al. Internvl3. 5: Advancing open-source multimodal models in versatility, reasoning, and efficiency. arXiv preprint arXiv:2508.18265, 2025. 3 [101] Jiannan Wu, Muyan Zhong, Sen Xing, Zeqiang Lai, Zhaoyang Liu, Zhe Chen, Wenhai Wang, Xizhou Zhu, Lewei Lu, Tong Lu, et al. Visionllm v2: An end-to-end generalist multimodal large language model for hundreds of vision-language tasks. Advances in Neural Information Processing Systems, 37:6992569975, 2024. 3, 7 [102] Mingrui Wu, Xinyue Cai, Jiayi Ji, Jiale Li, Oucheng Huang, Gen Luo, Hao Fei, Guannan Jiang, Xiaoshuai Sun, and Rongrong Ji. Controlmllm: Training-free visual prompt learning for multimodal large language models. Advances in Neural Information Processing Systems, 37:4520645234, 2024. 4 [103] Penghao Wu and Saining Xie. V*: Guided visual search as core mechanism in multimodal llms. arXiv preprint arXiv:2312.14135, 2023. [104] Tung-Yu Wu, Sheng-Yu Huang, and Yu-Chiang Frank Wang. Dora: 3d visual grounding with order-aware referring. CoRR, 2024. 4 [105] Bin Xiao, Haiping Wu, Weijian Xu, Xiyang Dai, Houdong Hu, Yumao Lu, Michael Zeng, Ce Liu, and Lu Yuan. Florence-2: Advancing unified representation for variety of vision tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 48184829, 2024. 1 [106] An Yan, Zhengyuan Yang, Junda Wu, Wanrong Zhu, Jianwei Yang, Linjie Li, Kevin Lin, Jianfeng Wang, Julian McAuley, Jianfeng Gao, et al. List items one by one: new data source and learning paradigm for multimodal llms. arXiv preprint arXiv:2404.16375, 2024. 4 [107] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. 1 [108] Jianwei Yang, Hao Zhang, Feng Li, Xueyan Zou, Chunyuan Li, and Jianfeng Gao. Set-of-mark prompting unleashes extraordinary visual grounding in gpt-4v. arXiv preprint arXiv:2310.11441, 2023. 4 [109] Yiyang Yao, Peng Liu, Tiancheng Zhao, Qianqian Zhang, Jiajia Liao, Chunxin Fang, Kyusong Lee, and Qing Wang. How to evaluate the generalization of detection? benchmark for comprehensive open-vocabulary detection. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 66306638, 2024. [110] Haoxuan You, Haotian Zhang, Zhe Gan, Xianzhi Du, Bowen Zhang, Zirui Wang, Liangliang Cao, Shih-Fu Ferret: Refer and ground Chang, and Yinfei Yang. arXiv preprint anything anywhere at any granularity. arXiv:2310.07704, 2023. 3, 7, 8 [111] Licheng Yu, Patrick Poirson, Shan Yang, Alexander Berg, and Tamara Berg. Modeling context in referring expressions. In European conference on computer vision, pages 6985. Springer, 2016. 2 [112] Zhihan Yu and Ruifan Li. Revisiting counterfactual problems in referring expression comprehension. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1343813448, 2024. 2 [113] Yuqian Yuan, Wentong Li, Jian Liu, Dongqi Tang, Xinjie Luo, Chi Qin, Lei Zhang, and Jianke Zhu. Osprey: Pixel understanding with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2820228211, 2024. 7, 2 [114] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multidiscipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 95569567, 2024. 9 [115] Rowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin Choi. From recognition to cognition: Visual commonsense reaIn Proceedings of the IEEE/CVF conference on soning. computer vision and pattern recognition, pages 67206731, 2019. 2 14 ing in multi-modal llms. arXiv preprint arXiv:2307.08581, 2023. [129] Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen Duan, Weijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479, 2025. 1, 3 [116] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training, 2023. 3 [117] Yufei Zhan, Hongyin Zhao, Yousong Zhu, Fan Yang, Ming Tang, and Jinqiao Wang. Griffon-g: Bridging visionlanguage and vision-centric tasks via large multimodal models. arXiv preprint arXiv:2410.16163, 2024. 3, 6 [118] Yufei Zhan, Shurong Zheng, Yousong Zhu, Hongyin Zhao, Fan Yang, Ming Tang, and Jinqiao Wang. Griffon v2: Advancing multimodal perception with high-resolution scalarXiv preprint ing and visual-language co-referring. arXiv:2403.09333, 2024. 6 [119] Yufei Zhan, Yousong Zhu, Zhiyang Chen, Fan Yang, Ming Tang, and Jinqiao Wang. Griffon: Spelling out all object locations at any granularity with large language models. In European Conference on Computer Vision, pages 405422. Springer, 2025. 3 [120] Hao Zhang, Feng Li, Shilong Liu, Lei Zhang, Hang Su, Jun Zhu, Lionel Ni, and Heung-Yeung Shum. Dino: Detr with improved denoising anchor boxes for end-to-end object detection. arXiv preprint arXiv:2203.03605, 2022. 6 [121] Hao Zhang, Hongyang Li, Feng Li, Tianhe Ren, Xueyan Zou, Shilong Liu, Shijia Huang, Jianfeng Gao, Leizhang, Chunyuan Li, et al. Llava-grounding: Grounded visual chat with large multimodal models. In European Conference on Computer Vision, pages 1935. Springer, 2024. [122] Haotian Zhang, Haoxuan You, Philipp Dufter, Bowen Zhang, Chen Chen, Hong-You Chen, Tsu-Jui Fu, William Yang Wang, Shih-Fu Chang, Zhe Gan, et al. Ferretv2: An improved baseline for referring and grounding with large language models. arXiv preprint arXiv:2404.07973, 2024. 7 [123] Tao Zhang, Xiangtai Li, Hao Fei, Haobo Yuan, Shengqiong Wu, Shunping Ji, Change Loy Chen, and Shuicheng Yan. Omg-llava: Bridging image-level, object-level, pixel-level reasoning and understanding. In NeurIPS, 2024. 4 [124] Yichi Zhang, Yinpeng Dong, Siyuan Zhang, Tianzan Min, Hang Su, and Jun Zhu. Exploring the transferability of visual prompting for multimodal large language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2656226572, 2024. 4 [125] Liang Zhao, En Yu, Zheng Ge, Jinrong Yang, Haoran Wei, Hongyu Zhou, Jianjian Sun, Yuang Peng, Runpei Dong, Chunrui Han, et al. Chatspot: Bootstrapping multimodal llms via precise referring instruction tuning. arXiv preprint arXiv:2307.09474, 2023. 7 [126] Tiancheng Zhao, Peng Liu, Xuan He, Lu Zhang, and Kyusong Lee. Real-time transformer-based open-vocabulary arXiv preprint detection with efficient fusion head. arXiv:2403.06892, 2024. 5, 6 [127] Tiancheng Zhao, Qianqian Zhang, Kyusong Lee, Peng Liu, Lu Zhang, Chunxin Fang, Jiajia Liao, Kelei Jiang, Yibo Ma, and Ruochen Xu. Omchat: recipe to train multimodal language models with strong long context and video understanding. arXiv preprint arXiv:2407.04923, 2024. 2 [128] Yang Zhao, Zhijie Lin, Daquan Zhou, Zilong Huang, Jiashi Feng, and Bingyi Kang. Bubogpt: Enabling visual groundVLM-FO1: Bridging the Gap Between High-Level Reasoning and Fine-Grained Perception in VLMs"
        },
        {
            "title": "Supplementary Material",
            "content": "7. Comprehensive Dataset Overview for VLMFO1 This section details the comprehensive datasets utilized in our two-stage training methodology in Table 10. 8. Additional Visualizations This section presents more comprehensive set of visual results to further illustrate the performance and capabilities of our VLM-FO1 model. We provide diverse examples in Figure [3,4,5,6,7,8,9,10,11,12], showcasing key aspects of our methods behavior across various scenarios, offering deeper insights beyond the examples included in the main paper."
        },
        {
            "title": "Stage",
            "content": "Region-Language Alignment"
        },
        {
            "title": "Perception SFT",
            "content": "OD"
        },
        {
            "title": "OCR",
            "content": "Datasets COCO,O365[85],V3Det[96] GOLDG[39] Rexverse-2M[35] COCO,V3Det,O365,VAW[77] VisDrone2019[27],LVIS[31] Refcoco/+/g[41, 70, 111] finecopsref[62],grefcoco[58] CREC[112] GRIT[75] PACO[79],VG[44],Osprey[113] shareGPT4v[15],Rexverse-2M VisualCoT[83],HumanRef-CoT[36] COCO,LVIS,HumanRef-CoT CrowdHuman[84],TallyQA[2] Osprey,VCR[115] MDVP[55],DoclayNet[76] MLT2019[71],ICDAR15[40] CurvedSynText150k[64] VLM-Instruction OmChat-SFT[127] Table 10. summary of the datasets used in our two-stage training process. Figure 3. Visualization of VLM-FO1s perception abilities on OD task. 2 Figure 4. Visualization of VLM-FO1s perception abilities on REC task. Figure 5. Visualization of VLM-FO1s perception abilities on object counting task. 3 Figure 6. Visualization of VLM-FO1s perception abilities on grounding task. Figure 7. Visualization of VLM-FO1s perception abilities on OCR task. 4 Figure 8. Visualization of VLM-FO1s perception abilities on region caption task. Figure 9. Visualization of VLM-FO1s perception abilities on region VQA task. 5 Figure 10. Visualization of VLM-FO1s perception abilities on visual prompt OD task. Figure 11. Visualization of VLM-FO1s perception abilities on visual region reasoning task. 6 Figure 12. Visualization of VLM-FO1s perception abilities on visual region reasoning task."
        }
    ],
    "affiliations": [
        "Binjiang Institute of Zhejiang University",
        "College of Computer Science and Technology, Zhejiang University",
        "Om AI Research"
    ]
}