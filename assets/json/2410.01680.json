{
    "paper_title": "PHI-S: Distribution Balancing for Label-Free Multi-Teacher Distillation",
    "authors": [
        "Mike Ranzinger",
        "Jon Barker",
        "Greg Heinrich",
        "Pavlo Molchanov",
        "Bryan Catanzaro",
        "Andrew Tao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Various visual foundation models have distinct strengths and weaknesses, both of which can be improved through heterogeneous multi-teacher knowledge distillation without labels, termed \"agglomerative models.\" We build upon this body of work by studying the effect of the teachers' activation statistics, particularly the impact of the loss function on the resulting student model quality. We explore a standard toolkit of statistical normalization techniques to better align the different distributions and assess their effects. Further, we examine the impact on downstream teacher-matching metrics, which motivates the use of Hadamard matrices. With these matrices, we demonstrate useful properties, showing how they can be used for isotropic standardization, where each dimension of a multivariate distribution is standardized using the same scale. We call this technique \"PHI Standardization\" (PHI-S) and empirically demonstrate that it produces the best student model across the suite of methods studied."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 2 ] . [ 1 0 8 6 1 0 . 0 1 4 2 : r a"
        },
        {
            "title": "Preprint",
            "content": "PHI-S: DISTRIBUTION BALANCING FOR LABEL-FREE MULTI-TEACHER DISTILLATION Mike Ranzinger, Jon Barker, Greg Heinrich, Pavlo Molchanov, Jan Kautz, Bryan Catanzaro, Andrew Tao NVIDIA mranzinger@nvidia.com Figure 1: Illustration of the modified agglomerative model training procedure. Instead of the student model learning to match the original teacher distributions, it learns to match the normalized distributions (our proposed PHI-S is shown). We show the real distributions for DFN CLIP, DINOv2, SigLIP, and SAM by projecting them down to 2D using PCA. In the original space, the variance of DFN CLIP is so small that it appears as single point. During inference, we can estimate the original teacher distributions using the inverse normalization process on the student predictions."
        },
        {
            "title": "ABSTRACT",
            "content": "Various visual foundation models have distinct strengths and weaknesses, both of which can be improved through heterogeneous multi-teacher knowledge distillation without labels, termed agglomerative models. We build upon this body of work by studying the effect of the teachers activation statistics, particularly the impact of the loss function on the resulting student model quality. We explore standard toolkit of statistical normalization techniques to better align the different distributions and assess their effects. Further, we examine the impact on downstream teacher-matching metrics, which motivates the use of Hadamard matrices. With these matrices, we demonstrate useful properties, showing how they can be used for isotropic standardization, where each dimension of multivariate distribution is standardized using the same scale. We call this technique PHI Standardization (PHI-S) and empirically demonstrate that it produces the best student model across the suite of methods studied."
        },
        {
            "title": "INTRODUCTION",
            "content": "A body of work recently emerged on the topic of agglomerative models Ranzinger et al. (2024), which is fusing multiple heterogeneous visual foundation models Awais et al. (2023) into single model via multi-teacher knowledge distillation Hinton et al. (2015); Zuchniak (2023) without labels. Starting with AM-RADIO Ranzinger et al. (2024), and followed by Theia Shang et al. (2024), and UNIC Sariyildiz et al. (2024). Theia and UNIC apply feature standardization to the teacher output, and demonstrate how important it is. While knowledge distillation has large body of literature (e.g. Buciluˇa et al. (2006); Ahn et al. (2019); Heo et al. (2019); Huang & Wang (2017); Romero et al. (2014); Sun et al. (2021); Wei et al. (2022a); Zagoruyko & Komodakis (2017)), agglomerative models - dealing with multiple teachers coming from different modeling domains (e.g. vision-language contrastive Radford et al. (2021), self-supervised learning Oquab et al. (2023); Zhou et al. (2022); Assran et al. (2023), and segmentation Kirillov et al. (2023)) without ground truth labels - was new territory. In AM-RADIO, the"
        },
        {
            "title": "Preprint",
            "content": "Figure 2: Teacher activation histograms. We show the global histogram, as well as the histograms for the channels associated with the minimum mean, maximum mean, minimum variance, and maximum variance. While all being roughly normal, they have very different centers and scales. We provide specific values in table 7 in the appendix. authors chose DFN CLIP Fang et al. (2023), DINOv2-g-reg Darcet et al. (2023), and SAM Kirillov et al. (2023) as their teacher models. While the authors studied loss balancing between the different teachers to some degree, they landed on simple balancing strategy which was to apply the same weight to each teacher, both for summary and feature losses, and to use linear combination of Cosine and Smooth-L1 Girshick (2015) objectives for feature matching. In this work we study whether the choice of feature distillation loss function in AM-RADIO (equation 3) was an optimal choice. To motivate this, we start by analyzing the feature activation distributions for various teachers in figure 2, and confirm that the distributions have very different variances. Notably, both Mean Squared Error (MSE) and Smooth-L1 are sensitive to variance scale, and thus, left uncontrolled for, each teacher will be implicitly weighted. For example, SAMs distribution has standard deviation that is 191 larger than that of DFN CLIP. We also note that these distributions arent particularity of the training procedure by introducing SigLIP Zhai et al. (2023b) which has gained recent popularity due to its high scores on the OpenCLIP Ilharco et al. (2021) leaderboard, as well as strong results within VLLMs Fang et al. (2024); Li et al. (2024). Main Contributions: We study the distributions of the teachers studied in Ranzinger et al. (2024) (plus SigLIP). We employ statistical toolkit of standardization and whitening, and study their effects on downstream metrics. We study the effects of rotation matrices when applying whitening after identifying that the orientation of the normalized teacher distribution may affect the student models errors. We study an application of Hadamard matrices on both whitening and standardization. In the case of standardization, we demonstrate that the Hadamard matrix may be used to produce distribution that is standardized using uniform scale across dimensions. We call this normalization method PHI Standardization (PHI-S) and demonstrate that it produces the best student models across our evaluation suite. Figure 3: The loss curves for each of the four teachers that the ViT-B/16 student is learning to match simultaneously in original teacher space (e.g. denormalized). We emphasize Baseline - MSE (Blue) and PHI Standardize (PHI-S, Red) as they generally set the upper and lower bounds."
        },
        {
            "title": "Preprint",
            "content": "Method Teacher Classification MSE Segmentation MSE Cosine Hyb MSE Hyb SmL1 Global Stdze Standardize PHI-S PCA-W ZCA HCA 6.25 10.00 5.50 7.00 2.75 2.75 2.00 6.50 5.50 6.75 10.00 1.00 9.00 3.00 3.00 5.00 3. 7.50 7.50 6.00 10.00 2.00 3.00 4.50 4.50 7.50 2.00 7.50 8.00 6.00 3D 1.00 8.00 2.00 7. 10.00 7.25 7.50 6.00 SAM LLaVA Probe 1.5 COCO Baselines 9.75 6.63 7.63 4.38 Standardization 3.13 2.75 3.50 Whitening 6.50 4.75 6.00 6.00 9.00 10.00 5.00 3.00 4.00 3.75 4.00 4.75 4.50 4.25 3. Avg 7.83 5.81 5.77 5.31 3.81 4.21 2.92 6.29 6.46 6.58 Avg Avg No No COCO MSE/COCO 9.20 5.38 6.53 4.98 3.58 4.45 2.70 6.35 5.95 5.90 9.94 4.22 6.78 4.47 3.78 4.88 2. 6.31 6.06 5.69 Table 1: Average ordinal rank between methods (1 is best, 10 is worst) across the benchmark suite for ViT-B/16. We observe that the standardization techniques work the best, with PHI-S being the strongest normalization method studied. The raw benchmark scores are provided in appendix A.8.2."
        },
        {
            "title": "2 METHOD",
            "content": "The goal of the agglomerative student model is to produce activations x(t) that match the teacher activations y(t) Y(t) as closely as possible for each teacher , and the loss is (usually) linearly aggregated using weights α(t). Finding these α(t) is difficult due to the size of the design space, so current methods typically default to α(t) = 1 and focus on conditioning Y(t) to handle distributional differences. For simplicity, we drop the (t) superscript as the same type of normalization is applied for every teacher, and each teacher has independent normalization parameters. Throughout this paper, we refer to Var [Z] as the diagonal of the covariance matrix Σ [Z] for some distribution Z. 2.1 BASELINE We start with the MSE (mean squared error) loss serving as the baseline for feature matching: Lmse(x, y) = 1 (cid:88) n=1 (xn yn)2 (1) Because AM-RADIO Ranzinger et al. (2024) doesnt use MSE as their loss function, but rather hybrid cosine + Smooth-L1 loss, we also consider few of these variants. For example, the vanilla cosine distance loss, which is identical to what AM-RADIO uses for the summary loss. While we expect this to do poorly on the task of exactly matching the teacher distribution (due to magnitude invariance), its not clear how this will affect the downstream tasks, so we include it. Lcos(x, y) = 1 N (cid:88) n=1 (cid:18) 1 (cid:19) xy y (2) We also consider the exact loss function proposed in AM-RADIO which is hybrid of cosine distance and smooth-L1: Lhyb-sml1(x, y) = β Lcos(x, y) + (1 β) SmoothL1(x, y) (3) For completeness, we ablate whether MSE vs Smooth-L1 has an effect on the evaluation criteria: Lhyb-mse(x, y) = β Lcos(x, y) + (1 β) Lmse(x, y) (4) In AM-RADIO, the authors used β to interpolate between cosine and smooth-L1 loss. Instead of searching the space for the optimal β, we analyzed the setting they chose (β = 0.9), and also note"
        },
        {
            "title": "Preprint",
            "content": "Model Params (M) AM-RADIO (-H) PHI-S-RADIO-B PHI-S-RADIO-L 653 98 320 82.93 73.61 81.01 86.06 81.74 84. 51.34 48.94 51.47 ImageNet1K Segmentation (linear) Zero-shot k-NN ADE20k VOC 84.71 84.35 85.49 Vision-Language (LLaVa-1.5) SAM GQA POPE TextVQA VQAv2 COCO 63.01 63.49 64.29 86.20 86.82 86.86 56.32 57.64 62. 79.28 79.33 81.10 76.23 73.87 75.06 Table 2: Using the PHI Standardization (PHI-S) technique to balance the losses for all of the teachers, we are able to produce ViT-B/16 and ViT-L/16 models using the 3-stage training protocol in appendix A.7 that are competitive with AM-RADIO (ViT-H/16). Notably, our PHI-S-RADIO-L model achieves higher semantic segmentation results, and significantly higher LLaVA-1.5 Liu et al. (2023) results. SAM COCO measures the instance mIoU as introduced in Cai et al. (2023). that cosine loss corresponds to β = 1.0 and MSE loss corresponds to β = 0.0, thus we implicitly study the extremal points of this function interpolation."
        },
        {
            "title": "2.2 NORMALIZATION",
            "content": "Instead of balancing the different heads through loss weighting, we can alter the targets themselves. In Wei et al. (2022a), the authors explore this to condition their single teachers distribution, however, they use the non-invertible LayerNorm operator to rescale the teacher features. Because we want to maintain compatibility for the student to replace the teacher in downstream tasks (by replacing only the vision encoder part of the model), we require the student to still estimate the true teacher distribution. To achieve this, during training, we use an invertible linear mapping fk() such that k(x) = fk (Tk(x)) and Tk(x) = 1 k(x)), where the student model learns to match teacher (T k(x)) for each of the teachers. (T 2.2.1 STANDARDIZATION We first consider the simplest case of standardization, which is to use single scalar µg and std. dev. σg across the entire feature map. These represent the global statistics of the teacher distribution. In contrast to Wei et al. (2022a), we seek an invertible linear mapping, which excludes LayerNorm. We can, however, estimate the µxy and σxy of each position, or, because we want to preserve resolution flexibility, estimate them across all positions and channels, yielding global µg and σg. Let µg and σg be the global mean and standard deviation estimate of the teacher distribution. Then Lgs(x, y) = Lmse x, (cid:18) (cid:19) µg σg (5) which we call Global Standardization. We also explore regular multivariate standardization where we normalize each channel of the teacher distribution independently. Let µc = [Yc] and σc = (cid:112)Var [Yc], then standardization is defined as Ls (x, y) = Lmse(x, y), = yc µc σc (6) 2.2.2 WHITENING While standardization normalizes the individual feature variances, it doesnt correct for any covariance between dimensions. We can expand on standardization by also eliminating the covariance between features, called whitening. Let Σ [Y] be the covariance matrix for where Y. Following Kessy et al. (2018), we want to find the in = Wy (7) with and Σ [Z] = I. = Σ [Y] 1 Sejnowski (1997), and takes the form 2 is one such valid matrix, called ZCA Whitening Bell &"
        },
        {
            "title": "Preprint",
            "content": "y = Σ [Y] 1 2 (y µ) (8) Each feature in Σ [Y] is linearly independent and has uniform scale. And so Lw(x, y) = Lmse (x, Wy µ) for any whitening method w. and are related to each other as = Σ [Y] 2 + µ (9)"
        },
        {
            "title": "2.2.3 ESTIMATION ERRORS",
            "content": "Following the whitening notation of Kessy et al. (2018), given some orthogonal matrix Q, then QW QW = Σ [Y]1. Kessy et al. is also valid whitening matrix, as QQ = I, therefore (QW) (2018) then demonstrate the properties of certain choices of Q, and we focus on PCA Whitening (PCA-W) and ZCA in this paper. With Σ [Y] = UΛU Wpca-w = Qpca-wΛ 1 Wzca = QzcaΛ 1 2 U, Qpca-w = 2 U, Qzca = (10) (11) (12) where and Λ are the eigenvectors and eigenvalues for the covariance matrix of respectively. Λ = diag-embed (λ1, ..., λC) where diag-embed () forms diagonal matrix with the vector argument along the diagonal. From equation 9, an issue naturally arises, which is the estimation error of our student network. Let ϵ RC be the estimation error of the student s.t. = + ϵ where is the student prediction for given normalized teacher, forming the exact equality = W1 (x + ϵ) + µ = W1x + W1ϵ + µ ϵpca-w = UΛ ϵzca = UΛ 1 2 ϵ 2 Uϵ 1 We can also use the same ϵ to study standardization (equation 6), taking the form ϵstd = diag-embed (σ1, ..., σC) ϵ (13) (14) (15) (16) (17) As is clear from equations 15, 16 and 17, the choice of normalization will have an impact on the error profile of the model, unless ϵ counteracts the distortion. We next introduce another not studied in Kessy et al. (2018), which is to use scaled Hadamard matrix, based on this idea. 2.2.4 HADAMARD WHITENING (HCA) In PCA Whitening, each successive dimension explains the next-largest variance in the data. While this can be very useful form, we hypothesize that this sort of dimensional loading might not be healthy for model to learn to match, as effects such as regularization, step size, gradient clipping, etc. may impact the ability of the model to learn each dimension. Instead of ranking the dimensions, wed like to do the opposite, and find that explains exactly the same amount of variance irrespective of channel index. It follows that if we could construct an orthogonal basis where each axis captures an identical amount of energy from the diagonal Λ 1 2 matrix, then we are able to achieve this balance. First, this matrix must be orthogonal for it to be valid Q. Second, in order for the same proportion of the diagonal Λ to be captured by each row, then each cell must have the same magnitude. Specifically, Rij = 1 . These matrices are called Hadamard matrices, and the following is called Sylvesters construction Sylvester (1867), valid when is power of 2:"
        },
        {
            "title": "Preprint",
            "content": "H1 = [1] , Hn = 1 2 (cid:20)Hn1 Hn1 Hn1 Hn1 (cid:21) (18) where = log2 + 1. The only difference from standard Sylvesters construction is the 1 scaling 2 at each recursive level, which is necessary for all of the vectors to be unit length. Relating back to whitening, we use as the rotation matrix Q: Whca = QhcaΛ 1 2 U, Qhca = and we end up with Hadamard Whitening with corresponding error profile: ϵhada = UΛ 1 2 Hϵ (19) (20) This error profile is interesting due to the fact that an error of size δ along any single dimension d1 will have identical magnitude in the original space as any other dimension d2. We prove this in appendix A.2.1. Further, in appendix A.1.1 we show how some Hadamard matrices whose size is not power of 2 can be constructed, and how we found an for important model sizes such as 768, 1024, 1152, 1280, and 1408. 2.2.5 PCA-HADAMARD ISOTROPIC STANDARDIZATION (PHI-S) key issue with the previous normalization procedures (aside from global standardization) is that they place disproportionate weight on lower-variance axes. To avoid this distortion, we present the following theorem, and then describe how we apply it as novel form of standardization: Theorem 2.1. For any mean-centered normal data distribution RCN with satisfiable Hadamard-matrix dimension C, there exists an orthogonal transform RCC and scalar α such that diag (Σ [αRX]) = 1C. Proof. Let Σ [X] be the covariance matrix of X, and let Σ [X] = UΛU matrix, and Λ = diag-embed (λ1, ..., λC), with λi being the eigenvalues of Σ [X]. (called PCA). First, note that Σ [UX] = (UΛU) = Λ. Next, let RCC be normalized Hadamard matrix, and recall each cell in has value 1 Using the orthogonal transform HU , we get Σ [HUX] = HΛH where is an orthogonal . . diag (HΛH)r = (cid:19) (cid:88) i=1 (cid:18) λi 1 = 1 (cid:88) λi (21) Let ϕ = (cid:118) (cid:117) (cid:117) (cid:116) 1 (cid:88) λi Σ (cid:2)ϕ1M(cid:3) = ϕ2M = (cid:80)C λi (22) (23) for some matrix M. For HΛH , we have diag (cid:0)Σ (cid:2)ϕ1HUX(cid:3)(cid:1) = diag (cid:0)ϕ2HΛH(cid:1) = (cid:80)C (cid:80)C λi λi = 1 (24) Therefore"
        },
        {
            "title": "Preprint",
            "content": "R = HU α = ϕ1 For PHI-S, following equation 25 we use Wship = αR (25) (26) Essentially, we first mean center and then rotate the distribution in such way (R) that the variance along each resulting dimension is identical, allowing us to uniformly scale by α to achieve standardized distribution. Figure 4: Visualization of how standardization affects the resulting data distribution. We start with the same distribution, and rotate the data by some angle. Regular standardizations effect is directly tied to the distribution orientation. Conversely, PHI-S is invariant to any data rotation, and will produce an identical transform up to sign along each dimension. We can make the sign consistent by negating the rows of and which have negative value in the diagonal position. Similarly, regular standardization will distort each dimension (shown with red/blue lines), which will have the effect of reducing the importance of high variance axis-aligned dimensions, and increasing the importance of low-variance dimensions. PHI-S is isotropic, so the change in scale is uniform. 2.2.6 VISUALIZING DISTRIBUTIONS In figure 5 we show how the various normalization transforms change the target distribution, and also how the transforms affect the errors coming from the student model. For the whitening transforms, the choice of matrix has an impact on the relationship between errors of the same magnitude (e.g."
        },
        {
            "title": "Preprint",
            "content": "Method DFN CLIP (14) MSE Cosine Hyb MSE Hyb SmL1 Global Stdze Standardize PHI-S (Ours) PCA-W ZCA HCA (Ours) 5.0883 105.90 7.4930 9.8540 4.7420 4.7417 4.7200 4.7861 4.7841 4.7855 SigLIP DINOv2 1.0767 1.9598 1.7980 3.3060 0.9422 1.9250 0.9112 1.9750 0.8801 1.9120 0.8928 1.9146 1.9010 0.8865 0.9316 1.9534 0.9321 1.9529 0.9326 1.9545 SAM 6.5082 27.9310 7.4580 8.6600 8.4910 8.3272 8.3330 8.7309 8.7061 8.7226 Table 3: Mean Squared Error for matching the teachers with ViT-B/16 student using different algorithms. PHI-S does the best job at simultaneously minimizing all teachers. fixed radius) in the learned distribution versus the denormalized distribution. Using the Hadamard matrix as is the only choice that doesnt place extra error on particular learned dimension. In figure 8 we display the radius of the denormalized error circle. An interesting property of standardization becomes apparent, which is that the error magnitude of standardization is bounded between PCA-W and PHI-S, with equality at Σ [Y] = Λ for the former and diag (Σ [Y]) = ϕshipI for the latter. One hypothesis for why the standardization transforms (Global Standardization, Standardization, PHI-S) work best in our study is because the error amplitudes are less extreme than whitening in general. With MSE being sensitive to outliers, this property is likely important. Because the whitening methods only differ by an orthogonal transform, their errors are phase shifted relative to each other."
        },
        {
            "title": "IMPLEMENTATION DETAILS",
            "content": "We generally follow the procedure outlined in AM-RADIO, however we make some changes that reduce the computational cost of training, which was necessary to cover all of the ablations we studied. Namely, we: Add SigLIP as teacher. Train the student model at 256px resolution, and downsample the teacher features to match. Train for 300k steps instead of the 600k steps originally proposed. Split each teacher into their own partition, resulting in each teacher receiving batch of 256 images, with total of 1024 images per iteration. Initialize from TIMM Wightman (2019) vit [base,large] patch16 224 pretrained models. We found that downsampling SAM features degrades their quality, so instead we pad the small image and crop out the features. Further details, and specifically for table 2, are presented in appendix A.7."
        },
        {
            "title": "4 RESULTS",
            "content": "In figure 3 we display our models ability to estimate the teacher distributions during training. For any of the transforms that project the teacher features into different space, we apply the inverse operation so that all methods are measured in the original space. As can be seen, Baseline is much worse than any other method, and its intuitive because it allows the relative difference in magnitudes between the different teachers to implicitly weight the loss. SAM has much larger activation variance than any other model, which results in the Baseline model spending most of its energy trying to match SAM. Overall, the PHI Standardization method produces the best results, as its able to simultaneously beat any other method on DFN CLIP, SigLIP, second best on DINOv2, while remaining competitive on SAM. We show the final MSEs in table 3. In tables 1 and 4, we display the average benchmark ranks across different benchmarks and methods for ViT-B/16 and ViT-L/16 students, respectively. For LLaVA, we first average the two GQA and"
        },
        {
            "title": "Preprint",
            "content": "Figure 5: Visualization of normalization procedures. We display two axis lines in red and blue. In the original space, theyre both 2 units long, and aligned with the plot coordinate system. We also display an error circle which is unit circle in the normalized coordinate system. For the three whitening transforms you can see how they only differ by rotation. We also specifically draw colored dots on the error circle corresponding to the extremal points of the error circle when denormalized into an ellipse. PCA-W places the largest error magnitude on the x-axis, given that its the dimension with largest eigenvalue thus estimation errors along the dimension will have much larger impact in the denormalized space. As we show in equation 16, the error for ZCA will be proportional to the original distributions covariance matrix, and thus, the extremal points are along the eigenvectors of the covariance matrix. Hadamard whitening has the extremal points at x1 = x2 = ... = xC. Global Standardization and PHI-S are both isotropic, which means that theres an infinite number of extremal points, so we instead show the points as they relate to the distribution itself. Similar to ZCA, for Global Standardization these points are along the principal axes. And similar to HCA, the aligned points for PHI-S are when x1 = x2 = ... = xC."
        },
        {
            "title": "Preprint",
            "content": "Figure 6: Related to figure 5 and equation 31, we visualize what happens to the one-hot error vectors when projecting back to the original space for HCA. We retain the original = dots, and add the one-hot dots demonstrating how their mapping remains equidistant from the origin relative to λc 1.400892 for any 2 H(cid:17) UΛ (cid:113) 1 (cid:80) each other. In particular, since δ = 1, then choice of r. For reference, Λ [3.8356, 0.0894]. (cid:13) (cid:13) (cid:13) = (cid:13) (cid:13) (cid:13) (cid:16) 1 Figure 7: Similar to figure 6 and figure 5, we visualize PHI-S in the normalized and denormalized spaces. This visualizes how equation 27 maintains errors along circle in both spaces, owing to the isotropic nature of the transform. It also can be seen how the = error dots in normalized space map to the principal directions of the distribution, and also how the one-hot dots capture identical probability density. TextVQA tasks separately, and then combine them with POPE and VQAv2 to compute the average. This is to prevent overly biasing towards the tasks that have multiple measurements. In both architectures PHI-S produces the best results by achieving the lowest average rank across the suite. Baseline MSE Global Stdze Standardize PHI-S Feature MSE 3.25 2.75 2.25 1.75 Classification Segmentation 4.00 2.50 2.50 1. 4.00 2.50 2.00 1.50 SAM LLaVA Probe 1.5 COCO 1.00 4.00 1.875 3.00 2.25 2.00 1.875 4.00 3D 4.00 2.25 1.75 1.75 Average 3.38 2.48 2.13 1.98 Average No COCO 3.85 2.38 2.15 1. Table 4: Average benchmark ranks for the ViT-L/16 models using the best (and baseline) normalization methods from the ViT-B/16 ablations. PHI-S is even more dominant with the larger model. We provide the raw benchmark scores in appendix A.8.3."
        },
        {
            "title": "Preprint",
            "content": "Figure 8: Following from figure 5, we visualize the radius of the denormalized error circle at every angle between 0 and 2π. Because Global Standardization and PHI-S are isotropic, and because the distribution is mean centered (section A.5.2), they scale the error circle uniformly by the same 2 with (e.g. when = 0 or = 0) we have the same error amount. As predicted, for θ = π magnitude for HCA, and also where PHI-S and HCA have identical magnitude. HCA has extremal 2 + π values at θex 2 . We also have that ZCA will have extremal values θex 4 . PCA-W has extremal values at θex zca(z) θex pca-w(z) θex pca-w = π hca = π hca(z). 4.1 EMPIRICAL ERRORS In section 2.2.3 we demonstrated how the choice of normalization might have an impact on the errors the student makes when matching the teachers. Particularly, equation 16 is the error profile for ZCA, 15 for PCA-W, 20 for HCA, and 17 for regular standardization. We also have ϵgs = α1 gs ϵ ϵship = α1 shipϵ (27) for global standardization and PHI-S respectively. We used this error profile to motivate the introduction of Hadamard matrices for whitening in section 2.2.4, as it distributes the error variance equally through all channels of the denormalization projection. In table 5 we display the empirical error variance ranges for each studied method and for each teacher. Intriguigingly, both methods that employ the Hadamard matrix (HCA and PHI-S) have very low variance ranges compared to the other methods. This implies that the student model is making errors of roughly uniform magnitude across all channels. Unfortunately, in the case of HCA, this property isnt borne out in useful way in the benchmarks  (table 1)  . Table 5 shows that the loss landscape and/or the optimizer are adapting to normalization distortions and baking the non-uniform nature of the variances into the student model. For PHI-S, the student model still has nearly uniform error variance in the normalized space, but also has the lowest (or nearly lowest) range in the denormalized (original) space. This isnt surprising given that unit change in any dimension of the normalized space has an identical effect as any other dimension, thus theres no incentive to prefer one dimension to another. In figure 9 we show the loss distributions for the core normalization methods we studied. It helps us understand not only the magnitudes of the errors, but also showcases how different normalization methods affect the behavior of outliers. Its very apparent that Baseline has uncontrolled magnitudes, with SAM having quite extreme losses, especially relative to DFN CLIP. This is also where we can really see how Global Standardize and PHI-S differ in behavior, owing to PHI-S equializing the variance across channels. The purple curve shows how global standardization is still very susceptible to outlier errors. As predicted in section 2.2.3, the methods that use Hadamard matrices (PHI-S and HCA) have the tightest error bounds between channels. Finally, its also apparent how well PHI-S works for balancing across teachers, as the losses all have the most similar distributions compared against the other methods."
        },
        {
            "title": "5 RELATED WORK",
            "content": ""
        },
        {
            "title": "Preprint",
            "content": "Figure 9: Loss distributions for various normalization methods. The x-axis range is based on the minimum and maximum losses seen for each method over the course of 1,000 samples after training for 100k iterations. The Largest Max Loss Channel shows the distribution for the channel that had the highest loss value. It helps us understand how vulnerable our learning process is to outliers. The Global curve shows the distribution by combining all of the channels. Knowledge Distillation We base our work on Ranzinger et al. (2024) which considers multiteacher distillation problem without ground-truth labels, and where the targets are the teacher features themselves, instead of estimating e.g. probability distribution for classification. They build upon extensive literature on knowledge distillation, popularized by Hinton et al. (2015), and then expanded with Kim et al. (2018); Ba & Caruana (2014); Mirzadeh et al. (2019); Beyer et al. (2022) for teacher logit estimation problems. For feature matching, Romero et al. (2014); Huang & Wang (2017); Ahn et al. (2019); Heo et al. (2019); Zagoruyko & Komodakis (2017); Sun et al. (2021); Wei et al. (2022b) study this sub-problem. Specifically, Wei et al. (2022a) discuss the importance of"
        },
        {
            "title": "Preprint",
            "content": "Method Baseline - MSE Global Stdze Standardize PHI-S PCA-W ZCA HCA DFN CLIP 0.015 17.759 0.579 0.086 0.416 0.558 0.028 Normalized SigLIP DINOv2 403.995 113.783 0.348 0.088 0.339 0.368 0.030 2.705 1.287 0.480 0.052 0.830 0.626 0. SAM DFN CLIP 238.929 8.594 0.861 0.219 1.634 1.226 0.232 0.015 0.014 0.015 0.014 0.015 0.015 0.015 Denormalized SigLIP DINOv2 403.995 398.198 406.442 393.216 421.195 421.192 422.810 2.705 2.405 2.793 2.447 3.179 3.098 3.137 SAM 238.929 239.744 240.526 239.489 243.610 243.774 243. Table 5: The range of the per-channel variances of both the normalized student model errors, as well as the denormalized student errors. smaller range implies that each channel has more similar error variance, with 0 implying that each channel has identical error variance. As theorized, Hadamard and PHI-S have the most uniform variances across the channels, however PHI-S also has the most uniform error variance when projected back into the original (denormalized) space. normalizing the teacher feature distribution, which is notable omission in Ranzinger et al. (2024). Further, in the knowledge distillation domain, the idea of distilling from multiple teachers at once is heavily studied Hinton et al. (2015); Liu et al. (2020); Zuchniak (2023); Yuan et al. (2020); Zhao et al. (2022); Yang et al. (2020); Park & Kwak (2020); You et al. (2017); Lan et al. (2018); Asif et al. (2019); Fukuda et al. (2017). AM-RADIO Ranzinger et al. (2024) differentiates itself from those largely through the lack of unified target label or distribution, as the teachers arent even from the same problem domain (e.g. CLIP Radford et al. (2021) versus SAM Kirillov et al. (2023)), and thus will produce very different feature distributions for the same image. Similarly, much of the literature that covers balancing the multi-objective (multi-teacher) loss relies on having access to ground truth labels Liu et al. (2020). Generically, Hu et al. (2019) is capable of balancing losses without GT labels by setting the loss weight to be inversely proportional to the approximate expected loss for each term, which AM-RADIO studied but found no significant effect. In Ruder et al. (2017), the authors study domain adaptation where they have multiple classifier teachers from their own domain, and they seek to train student on new unlabeled domain, however their method relies on the source and target domains being classification. Concurrently to our work, Shang et al. (2024) introduced the Theia model which draws heavily from AM-RADIO including the loss formulation. In their work, the authors chose to use the regular standardization method, choice which this work explores and demonstrates that it was both great addition over AM-RADIO, but also not the optimal choice compared against PHI-S which we propose here. We view the works as complementary, as our study entirely revolves around the design choices in their section 3.2 and AM-RADIOs section 3.4. Recently, the preprint UNIC Sariyildiz et al. (2024) is also based on AM-RADIO, and employs feature standardization, showing strong positive effects, and preprint UNIT Zhu et al. (2024) bases on AM-RADIO employing feature standardization in addition to explicit supervised OCR learning. Normalization The importance of normalization in distillation was identified in Heo et al. (2019), which used BatchNorm. More recently, Wei et al. (2022a) also considered normalized feature matching, however their choice of LayerNorm was non-invertible, and also doesnt de-correlate the different feature dimensions. We aim to preserve the ability of the student to estimate the teacher as in AM-RADIO, so we focus on invertible normalization techniques which allow us to estimate the teachers true distribution. Liu et al. (2022) argue that normalizing the student and teacher features improves distillation for semantic segmentation as the student otherwise spends most of its energy matching the teacher magnitudes. Intuitively, we expand on this by also observing that controlling the relative magnitudes across teachers is critical. Kessy et al. (2018) provides an overview of different whitening procedures, stressing the fact that there are infinitely many whitening matrices for given distribution, and focus their attention on the rotation matrix that relates them. Their treatment covers many popular matrices, and we use their work as the foundation for our study. There are also multiple works in the SSL vision domain that deal with distribution properties, such as Barlow Twins Zbontar et al. (2021) and VICReg Bardes et al. (2022). Their algorithms try to induce the model to produce regular features, where in contrast, were forced to deal with arbitrary models that didnt undergo such regularization. In digital signal processing, using the Hadamard matrix to spread energy (to mitigate signal loss errors) is common practice Pratt et al. (1969); Kanj et al. (2022). We study the incorporation of this matrix both as suitable matrix for rotation during the whitening process, and also in novel way to derive scalar normalization factor that standardizes"
        },
        {
            "title": "Preprint",
            "content": "any multivariate distribution with known Hadamard matrix, which we call PHI Standardization (PHI-S)."
        },
        {
            "title": "6 CONCLUSION",
            "content": "Through our experiments, we have conclusively demonstrated that using plain MSE without balancing has large negative effect on the resulting quality of the student model. Among normalization methods, standardization worked better than whitening, which was an initially surprising result. We hypothesize that the major issue with whitening is that the teacher models arent producing full rank distributions (appendix, table 6), which makes the normalization factors unstable. Regular standardization is resistant to this because the principal components of the distribution are spread out across all of the dimensions, preventing degenerate Λ 1 2 solutions. We found two novel applications of Hadamard matrices with respect to distribution normalization: HCA and PHI-S. At the ViT-B/16 model scale, we found that isotropic normalization methods (Global Standardize and PHI-S) worked the best, and for ViT-L/16, PHI-S remained the best. On the topic of reconstruction errors, we found no significant result across the whitening methods with respect to downstream metrics, and also found that the per-channel estimation errors were not uniform in general, unless uniform is the optimal choice (HCA and PHI-S), implying that the student model is able to be robust to the potentially high-distortion nature of the different transforms. Overall, PHI-S appears to be the best normalization method studied, and it allowed us to produce ViT-B and ViT-L models that are competitive with the original AM-RADIO Ranzinger et al. (2024) ViT-H model. Future Work Weve solely explored the use of PHI-S for agglomerative modeling, however its general standardization technique when certain assumptions about the data hold such as normality and dimensionality of the distribution. PHI-S could additionally be used to post-hoc standardize the output of existing models. Lastly, an opportunity arises when combining PHI-S with quantization practices (similar to Ashkboos et al. (2024)) in the information retrieval domain as it balances the information across all channels evenly, potentially unlocking higher fidelity quantizers."
        },
        {
            "title": "REFERENCES",
            "content": "S. Ahn, S. Hu, A. Damianou, N. D. Lawrence, and Z. Dai. Variational information distillaIn 2019 IEEE/CVF Conference on Computer Vision and Pattern tion for knowledge transfer. Recognition (CVPR), pp. 91559163, Los Alamitos, CA, USA, jun 2019. IEEE Computer Society. doi: 10.1109/CVPR.2019.00938. URL https://doi.ieeecomputersociety. org/10.1109/CVPR.2019.00938. Saleh Ashkboos, Amirkeivan Mohtashami, Maximilian L. Croci, Bo Li, Martin Jaggi, Dan Alistarh, Torsten Hoefler, and James Hensman. Quarot: Outlier-free 4-bit inference in rotated llms, 2024. URL https://arxiv.org/abs/2404.00456. Umar Asif, Jianbin Tang, and Stefan Harrer. Ensemble knowledge distillation for learning improved and efficient networks. In European Conference on Artificial Intelligence, 2019. URL https: //api.semanticscholar.org/CorpusID:202660953. Mahmoud Assran, Quentin Duval, Ishan Misra, Piotr Bojanowski, Pascal Vincent, Michael Rabbat, Yann LeCun, and Nicolas Ballas. Self-supervised learning from images with joint-embedding predictive architecture, 2023. URL https://arxiv.org/abs/2301.08243. Muhammad Awais, Muzammal Naseer, Salman Khan, Rao Muhammad Anwer, Hisham Cholakkal, Mubarak Shah, Ming-Hsuan Yang, and Fahad Shahbaz Khan. Foundational models defining new era in vision: survey and outlook, 2023. Jimmy Ba and Rich Caruana. Do deep nets really need to be deep? In Advances in Neural Information Processing Systems, pp. 26542662, 2014. Adrien Bardes, Jean Ponce, and Yann LeCun. VICReg: Variance-invariance-covariance regularization for self-supervised learning. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=xm6YD62D1Ub."
        },
        {
            "title": "Preprint",
            "content": "Anthony J. Bell and Terrence J. Sejnowski. The independent components of natural scenes are edge filters 3329 recover the causes. 1997. URL https://api.semanticscholar.org/ CorpusID:18326486. L. Beyer, X. Zhai, A. Royer, L. Markeeva, R. Anil, and A. Kolesnikov. Knowledge distilIn 2022 IEEE/CVF Conference on Comlation: good teacher is patient and consistent. puter Vision and Pattern Recognition (CVPR), pp. 1091510924, Los Alamitos, CA, USA, jun 2022. IEEE Computer Society. doi: 10.1109/CVPR52688.2022.01065. URL https: //doi.ieeecomputersociety.org/10.1109/CVPR52688.2022.01065. Cristian Buciluˇa, Rich Caruana, and Alexandru Niculescu-Mizil. Model compression. In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 535541. ACM, 2006. Han Cai, Junyan Li, Muyan Hu, Chuang Gan, and Song Han. Efficientvit: Multi-scale linear attention for high-resolution dense prediction, 2023. Timothee Darcet, Maxime Oquab, Julien Mairal, and Piotr Bojanowski. Vision transformers need registers, 2023. Mohamed El Banani, Amit Raj, Kevis-Kokitsi Maninis, Abhishek Kar, Yuanzhen Li, Michael Rubinstein, Deqing Sun, Leonidas Guibas, Justin Johnson, and Varun Jampani. Probing the 3D Awareness of Visual Foundation Models. In CVPR, 2024. Alex Fang, Albin Madappally Jose, Amit Jain, Ludwig Schmidt, Alexander Toshev, and Vaishaal Shankar. Data filtering networks, 2023. Yunhao Fang, Ligeng Zhu, Yao Lu, Yan Wang, Pavlo Molchanov, Jang Hyun Cho, Marco Pavone, Song Han, and Hongxu Yin. vila2: Vila augmented vila, 2024. URL https://arxiv.org/ abs/2407.17453. Takashi Fukuda, Masayuki Suzuki, Gakuto Kurata, Samuel Thomas, Jia Cui, and Bhuvana Ramabhadran. Efficient knowledge distillation from an ensemble of teachers. In Interspeech, 2017. URL https://api.semanticscholar.org/CorpusID:30258763. Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, Eyal Orgad, Rahim Entezari, Giannis Daras, Sarah Pratt, Vivek Ramanujan, Yonatan Bitton, Kalyani Marathe, Stephen Mussmann, Richard Vencu, Mehdi Cherti, Ranjay Krishna, Pang Wei Koh, Olga Saukh, Alexander Ratner, Shuran Song, Hannaneh Hajishirzi, Ali Farhadi, Romain Beaumont, Sewoong Oh, Alex Dimakis, Jenia Jitsev, Yair Carmon, Vaishaal Shankar, and Ludwig Schmidt. Datacomp: In search of the next generation of multimodal datasets, 2023. Quentin Garrido, Randall Balestriero, Laurent Najman, and Yann Lecun. Rankme: Assessing the downstream performance of pretrained self-supervised representations by their rank, 2023. Ross Girshick. Fast r-cnn. In Proceedings of the IEEE international conference on computer vision, pp. 14401448, 2015. B. Heo, J. Kim, S. Yun, H. Park, N. Kwak, and J. Choi. comprehensive overhaul of feature distillation. In 2019 IEEE/CVF International Conference on Computer Vision (ICCV), pp. 19211930, Los Alamitos, CA, USA, nov 2019. IEEE Computer Society. doi: 10.1109/ICCV.2019.00201. URL https://doi.ieeecomputersociety.org/10.1109/ICCV.2019.00201. Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in neural network. arXiv preprint arXiv:1503.02531, 2015. Hanzhang Hu, Debadeepta Dey, Martial Hebert, and J. Andrew Bagnell. Learning anytime predictions in neural networks via adaptive loss balancing. In Proceedings of the Thirty-Third AAAI Conference on Artificial Intelligence and Thirty-First Innovative Applications of Artificial Intelligence Conference and Ninth AAAI Symposium on Educational Advances in Artificial Intelligence, AAAI19/IAAI19/EAAI19. AAAI Press, 2019. ISBN 978-1-57735-809-1. doi: 10.1609/aaai. v33i01.33013812. URL https://doi.org/10.1609/aaai.v33i01.33013812."
        },
        {
            "title": "Preprint",
            "content": "Zehao Huang and Naiyan Wang. Like what you like: Knowledge distill via neuron selectivity transfer. CoRR, abs/1707.01219, 2017. URL http://arxiv.org/abs/1707.01219. Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan Taori, Achal Dave, Vaishaal Shankar, Hongseok Namkoong, John Miller, Hannaneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt. Openclip, July 2021. URL https://doi.org/10.5281/ zenodo.5143773. Hind Kanj, Anthony Trioux, Francois-Xavier Coudoux, Mohamed Gharbi, Patrick Corlay, and Michel Kieffer. comparative study of the whitening methods in linear video coding and transmission schemes. In 2022 11th International Symposium on Signal, Image, Video and Communications (ISIVC), pp. 16, 2022. doi: 10.1109/ISIVC54825.2022.9800738. Agnan Kessy, Alex Lewin, and Korbinian Strimmer. Optimal Whitening and Decorrelation. The American Statistician, 72(4):309314, October 2018. doi: 10.1080/00031305.2016.127. URL https://ideas.repec.org/a/taf/amstat/v72y2018i4p309-314.html. Jangho Kim, SeongUk Park, and Nojun Kwak. Paraphrasing complex network: Network comIn Proceedings of the 32nd International Conference on Neural pression via factor transfer. Information Processing Systems, NIPS18, pp. 27652774, Red Hook, NY, USA, 2018. Curran Associates Inc. Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollar, and Ross Girshick. Segment anything, 2023. H. W. Kuhn. The hungarian method for the assignment problem. Naval Research Logistics Quarterly, 2(1-2):8397, 1955. doi: https://doi.org/10.1002/nav.3800020109. URL https: //onlinelibrary.wiley.com/doi/abs/10.1002/nav.3800020109. Xu Lan, Xiatian Zhu, and Shaogang Gong. Knowledge distillation by on-the-fly native ensemble, 2018. Feng Li, Renrui Zhang, Hao Zhang, Yuanhan Zhang, Bo Li, Wei Li, Zejun Ma, and Chunyuan Li. Llava-next-interleave: Tackling multi-image, video, and 3d in large multimodal models, 2024. URL https://arxiv.org/abs/2407.07895. Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning, 2023. Tao Liu, Xi Yang, and Chenshu Chen. Normalized feature distillation for semantic segmentation, 2022. URL https://arxiv.org/abs/2207.05256. Yuang Liu, Wei Zhang, and Jun Wang. Adaptive multi-teacher multi-level knowledge distillation. Neurocomputing, 415:106113, nov 2020. doi: 10.1016/j.neucom.2020.07.048. URL https: //doi.org/10.1016%2Fj.neucom.2020.07.048. Seyed Iman Mirzadeh, Mehrdad Farajtabar, Ang Li, Nir Levine, Akihiro Matsukawa, and Hassan Ghasemzadeh. Improved knowledge distillation via teacher assistant. In AAAI Conference on Artificial Intelligence, 2019. URL https://api.semanticscholar.org/CorpusID: 212908749. Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Mahmoud Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. Dinov2: Learning robust visual features without supervision, 2023. Paley. On orthogonal matrices. Journal of Mathematics and Physics, 12:311320, 1933. URL https://api.semanticscholar.org/CorpusID:124410493."
        },
        {
            "title": "Preprint",
            "content": "Seonguk Park and Nojun Kwak. Feature-level ensemble knowledge distillation for aggregating In European Conference on Artificial Intelligence, 2020. knowledge from multiple networks. URL https://api.semanticscholar.org/CorpusID:220378802. W.K. Pratt, J. Kane, and H.C. Andrews. Hadamard transform image coding. Proceedings of the IEEE, 57(1):5868, 1969. doi: 10.1109/PROC.1969.6869. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya In Marina Sutskever. Learning transferable visual models from natural language supervision. Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 87488763. PMLR, URL https://proceedings.mlr.press/v139/radford21a. 1824 Jul 2021. html. Mike Ranzinger, Greg Heinrich, Jan Kautz, and Pavlo Molchanov. Am-radio: Agglomerative vision foundation model reduce all domains into one. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1249012500, June 2024. Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, and Yoshua Bengio. Fitnets: Hints for thin deep nets. CoRR, abs/1412.6550, 2014. URL https: //api.semanticscholar.org/CorpusID:2723173. Olivier Roy and Martin Vetterli. The effective rank: measure of effective dimensionality. In 2007 15th European Signal Processing Conference, pp. 606610, 2007. Sebastian Ruder, Parsa Ghaffari, and John G. Breslin. Knowledge adaptation: Teaching to adapt, 2017. URL https://arxiv.org/abs/1702.02052. Mert Bulent Sariyildiz, Philippe Weinzaepfel, Thomas Lucas, Diane Larlus, and Yannis Kalantidis. Unic: Universal classification models via multi-teacher distillation, 2024. URL https: //arxiv.org/abs/2408.05088. Jinghuan Shang, Karl Schmeckpeper, Brandon B. May, Maria Vittoria Minniti, Tarik Kelestemur, David Watkins, and Laura Herlant. Theia: Distilling diverse vision foundation models for robot learning. In 8th Annual Conference on Robot Learning, 2024. URL https://openreview. net/forum?id=ylZHvlwUcI. X. Sun, R. Panda, C. Chen, A. Oliva, R. Feris, and K. Saenko. Dynamic network quantization for efficient video inference. In 2021 IEEE/CVF International Conference on Computer Vision (ICCV), pp. 73557365, Los Alamitos, CA, USA, oct 2021. IEEE Computer Society. doi: 10. 1109/ICCV48922.2021.00728. URL https://doi.ieeecomputersociety.org/10. 1109/ICCV48922.2021.00728. James Sylvester. Lx. thoughts on inverse orthogonal matrices, simultaneous signsuccessions, and tessellated pavements in two or more colours, with applications to newtons rule, ornamental tilework, and the theory of numbers. Philosophical Magazine Series 1, 34:461475, 1867. URL https://api.semanticscholar.org/CorpusID:118420043. Yixuan Wei, Han Hu, Zhenda Xie, Zheng Zhang, Yue Cao, Jianmin Bao, Dong Chen, and Baining Guo. Contrastive learning rivals masked image modeling in fine-tuning via feature distillation, 2022a. Yixuan Wei, Han Hu, Zhenda Xie, Zheng Zhang, Yue Cao, Jianmin Bao, Dong Chen, and Baining Guo. Contrastive learning rivals masked image modeling in fine-tuning via feature distillation, 2022b. Ross Wightman. Pytorch image models. https://github.com/rwightman/ pytorch-image-models, 2019. Ze Yang, Linjun Shou, Ming Gong, Wutao Lin, and Daxin Jiang. Model compression with twostage multi-teacher knowledge distillation for web question answering system. In Proceedings of the 13th International Conference on Web Search and Data Mining, WSDM 20, pp. 690698, New York, NY, USA, 2020. Association for Computing Machinery. ISBN 9781450368223. doi: 10.1145/3336191.3371792. URL https://doi.org/10.1145/3336191.3371792."
        },
        {
            "title": "Preprint",
            "content": "Shan You, Chang Xu, Chao Xu, and Dacheng Tao. Learning from multiple teacher networks. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD 17, pp. 12851294, New York, NY, USA, 2017. Association for Computing ISBN 9781450348874. doi: 10.1145/3097983.3098135. URL https://doi. Machinery. org/10.1145/3097983.3098135. Fei Yuan, Linjun Shou, Jian Pei, Wutao Lin, Ming Gong, Yan Fu, and Daxin Jiang. Reinforced multi-teacher selection for knowledge distillation, 2020. Sergey Zagoruyko and Nikos Komodakis. Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017. URL https://openreview.net/forum?id=Sks9_ ajex. Jure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and Stephane Deny. Barlow twins: Self-supervised learning via redundancy reduction. arXiv preprint arXiv:2103.03230, 2021. Shuangfei Zhai, Tatiana Likhomanenko, Etai Littwin, Dan Busbridge, Jason Ramapuram, Yizhe Zhang, Jiatao Gu, and Joshua Susskind. Stabilizing transformer training by preventing attention entropy collapse. In International Conference on Machine Learning, pp. 4077040803. PMLR, 2023a. Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. arXiv preprint arXiv:2303.15343, 2023b. Haoran Zhao, Xin Sun, Junyu Dong, Changrui Chen, and Zihe Dong. Highlight every step: Knowledge distillation via collaborative teaching. IEEE Transactions on Cybernetics, 52(4):20702081, 2022. doi: 10.1109/TCYB.2020.3007506. Jinghao Zhou, Chen Wei, Huiyu Wang, Wei Shen, Cihang Xie, Alan Yuille, and Tao Kong. Image BERT pre-training with online tokenizer. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=ydopy-e6Dg. Yi Zhu, Yanpeng Zhou, Chunwei Wang, Yang Cao, Jianhua Han, Lu Hou, and Hang Xu. Unit: Unifying image and text recognition in one vision encoder, 2024. URL https://arxiv. org/abs/2409.04095. Konrad Zuchniak. Multi-teacher knowledge distillation as an effective method for compressing ensembles of neural networks, 2023."
        },
        {
            "title": "A APPENDIX",
            "content": "A.1 HADAMARD MATRICES A.1.1 CONSTRUCTING HADAMARD MATRICES Sylvesters construction gives us convenient way to construct Hadamard matrix when is power of 2. Unfortunately, many of the Cs we care about arent such power. More generally, the Hadamard Conjecture hypothesizes that there exists valid Hadamard matrix for any that is divisible by 4. If true, then there are significantly more valid matrices, and in particular, common deep learning choices will be multiple of 4. While not proven in general, the literature has found way to construct many non-power-of-2 sized matrices using some of the following rules: If Hn and Hm are Hadamard matrices, then Hn If 3 qk mod 4 for some prime and integer > 0, then we can use Paleys first (cid:78) Hm is also Hadamard matrix. construction Paley (1933) to produce Hadamard matrix of size + 1. If 1 qk mod 4 for some prime and integer > 0, then we can use Paleys second construction to produce Hadamard matrix of size 2 (q + 1). where (cid:78) is the Kronecker product. For our purposes, there are common feature dimensions that we want to be able to produce: ViT-B: 768 [S(2) (cid:78) P1(384))] ViT-L: 1024 [S(1024)] SigLIP-L: 1152 [S(32) (cid:78) P2(36)] ViT-H: 1280 [S(64) (cid:78) P1(20)] ViT-g: 1408 [S(32) (cid:78) P1(44)] Where Pi(x) is Paley construction of size x, and S(x) is Sylvester construction of size x. In the case of Sylvester, were referring to when 2k = for some N0. For P1(384), we have the prime = 383, which 3 3831 mod 4. For 1280, we can use (possibly among other options) P1(1280) as we have = 1279, and thus 3 12791 mod 4, or the compound version shown above. Finally, for (44) we have = 43 and 3 431 mod 4. So, by some stroke of luck, we have known constructions of Hadamard matrices for the major ViT widths. There are even more methods for constructing these matrices, and at the time of this writing, the smallest unknown Hadamard matrix is 668. While not exhaustive, for our purposes, the Sylvester and Paley constructions were sufficient to cover the models we studied. A.1.2 USING HADAMARD MATRICES FOR NOISE SUPPRESSION / QUANTIZATION While unrelated to our work of using Hadamard matrices to perform statistical normalization, the recently proposed QuaRot Ashkboos et al. (2024) finds different application of this structured matrix to eliminate activation outliers, making low-bit quantization much more effective. A.2 HADAMARD WHITENING A.2.1 PROOF OF HCA UNIFORM ERROR PROFILE Referring to equation 20: ϵhada = UΛ 2 Hϵ (20 revisited) we demonstrate that each column of UΛ step of size δ along any single dimension has identical magnitude in the original space. 2 has identical magnitude, and further, that an error"
        },
        {
            "title": "Preprint",
            "content": "Λ 1 2 = 1 ... λ1 λ2 . . . . . . . . . λC . . . ... λ1 λ2 λC (cid:13) (cid:13) (cid:13)Λ 2 H(cid:13) (cid:13) (cid:13)[:,j] = (cid:118) (cid:117) (cid:117) (cid:116) (cid:88) c=1 λc j (28) (29) where [:,j] denotes the norm of column j. Equation 29 shows that each column vector has an identical magnitude. Because orthogonal transforms are magnitude preserving, we also get (cid:13) (cid:13) (cid:13)UΛ 2 H(cid:13) (cid:13) (cid:13)[:,j] = (cid:118) (cid:117) (cid:117) (cid:116) (cid:88) c=1 λc j (30) In particular, this means that for some δ (cid:2)1[r=1], . . . , 1[r=C] with 1r=x representing the Kronecker delta for whether = and δ R+ (e.g. is one-hot column vector with 1 at position multiplied by some positive real δ), then (cid:3) 1 2 H(cid:17) UΛ (cid:16) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) = δ (cid:118) (cid:117) (cid:117) (cid:116) (cid:88) c=1 λc (31) In words, an error step of size δ along any single axis in whitened space will be scaled by (cid:118) (cid:117) (cid:117) (cid:116) 1 (cid:88) c=1 λc (32) when projecting back into the original space. So each dimension being learned by the student has the same magnitude of effect in the original teacher space. Our hypothesis is that this should improve learning dynamics as there is no implicitly more important dimension to match than any other, compared with PCA-W which places the most importance on the first dimension, and so on. 2 is not Note that an arbitrary error vector of magnitude δ does not have this property since UΛ orthogonal in general. 1 Incidentally, equation 32 is identical to equation 22 which is the radius of the denormalized unit error circle for PHI-S. This means that at any with δ = 1, the error magnitude is identical between the two normalization methods. We visualize this when = 2 in figure 8 by looking at where the blue and purple curves intersect. A.3 TEACHER EFFECTIVE RANKS We apply the RankMe Garrido et al. (2023) algorithm to handful of models, including the set of teachers used for training. While it was technically only designed for SSL models (like DINOv2), it may still lend insight into why whitening didnt empirically work well. The results are in table 6, where we show that the effective ranks for all teachers are much smaller than their number of channels. It is also interesting to consider whether agglomerative models work because the teachers arent effectively full rank, suggesting that we can pack more information from other teachers into student of equivalent or larger size than the teachers. More investigation is needed to understand why the RADIO models (both AM-RADIO and ours) seem to be lower rank than their counterparts of equivalent size."
        },
        {
            "title": "Preprint",
            "content": "Model DINOv2-b-reg PHI-S-RADIO-B DINOv2-l-reg 768 768 1024 PHI-S-RADIO-L 1024 1152 SigLIP (-L) 1280 DFN CLIP (-H) SAM (-H) 1280 1280 AM-RADIO (-H) 1536 DINOv2-g-reg RankMe 685.52 645.38 906.48 859.23 910.97 1081.69 776.28 1043.84 1342.55 Table 6: The effective rank estimates for the spatial features of various models using the RankMe Garrido et al. (2023) algorithm. As can be seen, the effective rank is much smaller than C, meaning that whitening methods will have large number of dimensions with very small variance. This likely helps to explain why the whitening methods produced the students with the highest losses. A.4 TEACHER DISTRIBUTION STATISTICS In table 7 we show statistics about the distributions of the teachers. We can see that they are not mean centered, and also that their standard deviations are very different, both globally, and per-channel. Model Per Channel Global Mean Std Mean Std Min Max Min Max DFN CLIP SigLIP DINOv2 SAM -0.1689 -6.8789 -3.3945 -62.0312 0.1385 31.25 4.293 19.1719 0.0105 0.3813 0.3918 2.6953 0.1334 21.6875 4.3008 31.6094 0.0049 0.0211 0.0055 1.1475 0.0286 1.8389 1.3496 5. Table 7: Activation statistics for various teachers. Here we can see that each of the teachers distributions have very different standard deviations (Global). We can also see that different channels for given teacher have very different means and standard deviations (Per Channel). Taking SAM as an example: The smallest mean value channel has value 62.0312, and largest channel 19.1719. Similarly, the channel with smallest standard deviation has 2.6953, and channel with largest has 31.6094. A.5 ADDITIONAL PHI-S INSIGHTS A.5.1 ROLE OF PCA Because rotations are magnitude preserving (and thus variance preserving), with Σ [Y] = UΛU , then Tr (Σ [Y]) = Tr (Λ) = (cid:80)C λi. This means that the normalization (α) derived in equation 25 is constant with respect to the distribution, invariant to any orthogonal transform thats applied to it. It will always be 1 (cid:80)C λi. And so, we have that (cid:113) Σ [HY] = (UΛU) (33) where the is preventing the from evenly distributing the variance in Λ, unless = I, or worst case = in which case applying would result in Λ variance, the opposite of what we want. So, we dont need PCA to find the scale α to normalize the distribution, but we do need it to find the orthogonal transform HU which results in dimensionally balanced distribution. Σ [HUY] = (HU) (UΛU) (UH) = HΛH 3 (34) (35)"
        },
        {
            "title": "Preprint",
            "content": "A.5.2 COMPARISON BETWEEN GLOBAL STANDARDIZATION AND PHI-S In table 8 we show what the normalization scalars are for each teacher distribution. Because both methods use single value to rescale the distribution, its useful to see how they treat the same distribution. Notably, PHI-S uses larger scale for all of the teacher distributions. Its also worth noting that the difference in scales is not constant across the distributions. Both methods are invariant to the orientation of the distribution, thus these scalars are unique properties of the distribution. Model DFN CLIP SigLIP DINOv2 αgs 35.02 0.53 0.73 SAM 0.19 αship 41.41 0.65 0.76 0.21 Table 8: Comparison of scales between global standardization equation 5 and PHI-S standardization equation 26. We get αgs = 1 σg , which is the scaling factor for global standardization. natural question arises: Why is αgs = αship? Recall that αship = ϕ1 = (cid:33) 1 (cid:32) 1 (cid:88) λi = (cid:18) 1 (cid:19) 1 2 Tr (Σ [Y]) (22 & 25 revisited) (36) And also how in section A.5.1 we showed that αship is invariant to any orthogonal transform on the distribution. For global standardization, we reinterpret the multivariate distribution as univariate, thus we get scalar µg and σg, global mean and global standard deviation respectively. For the multivariate distribution, we have µ, the vector of means for each dimension. We can equivalently write the computation of σg as and then for ϕship we have (cid:118) (cid:117) (cid:117) (cid:116) σg = 1 1 (cid:88) (cid:88) (yi,c µg)2 (cid:118) (cid:117) (cid:117) (cid:116) ϕship = 1 (C 1) (cid:88) (cid:88) (yi,c µc) (37) (38) therefore, when µc = µg C, then limN σg = limN ϕship. Meaning that, as long as the mean for each dimension of the distribution is the same, then Global Standardization and PHI-S will arrive at nearly the same scaling constant when is large, and thus only differ by rotation. trivial example is when the distribution is already mean centered on every dimension. We show in table 7 that none of the teachers we studied have uniform mean per channel, which is why the methods end up with different scaling constants. A.5.3 HOW SIMILAR ARE THE ORIGINAL TEACHER DISTRIBUTIONS TO THE PHI-S DISTRIBUTION? The main property of PHI-S is that it rotates the distribution in such way that the standard deviation for each channel is identical, allowing us to standardize the distribution in this rotated space using single scalar. From equation 25, if the two distributions are aligned, then HU = with being some permutation of I. We measure the deviation from this ideal by computing abs (HU), and then using the Hungarian algorithm Kuhn (1955) to find the best match of basis vectors align, and"
        },
        {
            "title": "Preprint",
            "content": "DFN CLIP SigLIP DINOv2 Model Mean Min 0.0000 0.0229 0.0001 0.0246 0.0000 0.0203 SAM 0.0226 0.0000 Max # > 0.75 0.9916 0.7864 0.9807 0.1128 1 1 1 0 Table 9: Measuring how aligned the original teacher distribution is with the PHI-S distribution. Refer to section A.5.3 for how this is calculated. finally calculating statistics on diag abs , which we show in table 9. We observe that align in general, the original distribution is quite unlike that of PHI-S, where at most one basis vector is mostly aligned, but otherwise and are highly dissimilar. HU (cid:16) (cid:17) A.5.4 DEGENERATE RANK DISTRIBUTIONS There are additional useful properties for the PHI-S transform, particularly when the original data distribution is not full rank, which is almost certainly the case with deep learning models (table 6, Garrido et al. (2023)). Namely, with the whitening procedures, they will create extreme scale distortions on the zero or negligible eigenvalue dimensions, which can cause the student model to waste too many resources optimizing negligible dimensions. Vanilla standardization also suffers from the same effect, but it may be less aggressive as its not using PCA which disentangles the dimensions, rather its sensitivity to this problem relies on the orientation of the original distribution. PHI-S, on the other hand, will be well behaved whenever Rank (Y) 1 because the rotation will place the same amount of variance on every output dimension. We use the definition in Roy & Vetterli (2007) for Rank, the effective rank. Every normalization method, except for PHI-S and Global Standardization, is vulnerable to when Rank (Y) C, which we illustrate in the 2-dimensional case: Let R2N be data distribution with covariance Σ [X] = (cid:20)1 0 (cid:21) 0 ϵ . Because standardization 2.2.1 requires division by the variance in each dimension, then limϵ0 σ1 = . For the whitening methods 2.2.2, the diagonalization of Σ [X] produces Λ = diag-embed(1, ϵ). The whitening methods then require Λ 1 2 which again produces division by 0 for the y-dimension. Because the PHI-S method operates on the mean eigenvalue, it will have limϵ0 α = 1 2, which is well defined. While this is trivial example, the implications are meaningful on real data too, which we show in table 6. = 0.5 A.6 NORMALIZATION WITHOUT RUNTIME PENALTY All of the normalization methods introduce extra computation in the form of mean subtraction and some scaling method. Because the teacher adaptors for our model all end with = Wx + linear layer, we can modify this layer after training to produce outputs in the original teacher space. Let be the normalized teacher outputs, be the original teacher outputs, x(n) be the output of the student matching the normalized teacher (at layer n), and we seek to produce x(n) that approximates the original teacher distribution. With this, we have: x(n) = Wx(n1) + (cid:16) x(n) = Θ Wx(n1) + b(cid:17) = ΘWx(n1) + Θb + µ + µ = ΘW = Θb + µ x(n) = Wx(n1) + 5 (39) (40) (41) (42) (43) (44)"
        },
        {
            "title": "Preprint",
            "content": "with and being the weights and bias of the final linear layer of the model respectively. Θ and µ are the linear correction parameters for the given normalization method. Global Standardize (2.2.1): Standardize (2.2.1): PCA Whitening (2.2.2): ZCA Whitening (2.2.2): Hadamard Whitening (2.2.4): PHI-S (2.2.5): A.7 IMPLEMENTATION DETAILS Θ = Iσg, µ = 1µg Θ = diag-embed (σ1, ..., σC) Θ = UΛ 1 2 Θ = Σ [Y] 1 2 Θ = UΛ 1 2 Θ = ϕUH (45) (46) (47) (48) (49) (50) In addition to all of the ablations, we also reported PHI-S-RADIO-B and PHI-S-RADIO-L models  (Table 2)  . To produce these models, we add 2 more training stages on top of that in section 3 as follows: Stage 1 - Outlined in section 3 (32 A100 GPUs for 40 hours) Stage 2 - Increase the student resolution to 432 and train for 300k more steps (64 A100 GPUs for 64 hours) Stage 3 - Add high res set of partitions. Similar to AM-RADIO, we set the batch size to 128 for hi-res while keeping 1024 for low-res. We again train for another 300k steps. (128 A100 GPUs for 68 hours) The multi-stage strategy results in 14,080 total GPU hours for the ViT-B/16 model. If we were to instead train stage 3 for 600k steps (AM-RADIO recipe), it would result in 17,408 total GPU hours. Hyperparameters are shown in table 10. We employ spectral reparametrization Zhai et al. (2023a) for all stages of training. Weve found this to be particularly helpful for stage 3 training when dealing with high resolution. In order to encourage the spectral norm to be small, we ensure that weight decay is applied to the rescaling parameter. A.8 RAW METRICS A.8.1 ADAPTIVE BALANCING In AM-RADIO, the authors also explore the use of AdaLoss Hu et al. (2019), which sets each loss term to be approximately 1 by dividing the term by the exponential moving average of itself. We explore using this balancing mechanism, both as standalone (e.g. Baseline + AdaLoss), as well as in conjunction with PHI-S. Table 11 shows the teacher MSEs, and table 12 shows the benchmark ranks with AdaLoss included. In general, AdaLoss places much more weight on the summary losses, resulting in outsized gains in classification tasks at the expense of dense tasks. We also find that AdaLoss+PHI-S is better than AdaLoss alone. A.8.2 VIT-B/16 In table 13 we show the raw benchmark scores for classification, segmentation, and Probe 3D El Banani et al. (2024). When viewing the raw scores, its less clear what the ideal method is, if any, aside from it being fairly obvious that the MSE baseline is the worst. We also show the metrics for LLaVA 1.5 integration in 14. Its easiest to see the best performing method by looking at the average ranks across the task suite in table 1, where being consistently strong is more evident. The Ada - prefix means that we used AdaLoss."
        },
        {
            "title": "Preprint",
            "content": "Hyperparameter Dataset Batch Size GPUs Steps LR LR Schedule Weight Decay Dist Est. Steps Frozen Body Steps Optimizer Stage 1 DC1B 1024 32 300,000 1e-3 cosine 0.02 3,000 5,000 LAMB Stage 2 DC1B 1024 64 300,000 1e-3 cosine 0.02 3,000 5,000 LAMB Stage 3 DC1B 1152 128 300,000 1e-3 cosine 0.02 3,000 5,000 LAMB Table 10: Hyperparameter table for the training stages. For each stage, we restart the learning rate schedule at 1e 3. Dist Est. Steps describes the number of steps we use at the beginning of the training stage to estimate the teacher data distributions. We reset these estimates for each stage, as the change in resolution may impact these distributions. We also freeze the trunk of the model for Frozen Body Steps at the start of each stage to allow for the heads to adjust to the new distributions, and also because these distributions may drastically change early on as the estimates are refined. Particularly, methods that rely on matrix diagonalization can undergo major shifts as PyTorchs implementation of torch.eigh() is not particularly stable under small changes to the covariance matrix. ZCA whitening is stable upon small estimate updates, owing to the fact that the rotation is inverted after rescaling, so any permutation of eigenvectors is also negated. DC1B stands for DataComp-1B Gadre et al. (2023), from which we only use the images. Method DFN CLIP (14) Ada - MSE Ada - PHI-S 4.7790 4.7750 SigLIP DINOv2 0.9591 1.9260 0.9585 1.9260 SAM 8.7500 8.6960 Table 11: Mean Squared Error for matching the teachers with ViT-B/16 student using AdaLoss, either normally (Ada - MSE), or in conjunction with PHI-S. A.8.3 VIT-L/16 In table 15 we show the MSE for our ViT-L/16 trained student model. Similar to the ViT-B/16 metrics, PHI-S does the best job of simultaneously minimizing all of the teacher errors. We also provide the raw benchmark scores in tables 16 and 17. A.9 COMPARISON WITH RECENT AGGLOMERATIVE MODELS Along with AM-RADIO at CVPR, Theia Shang et al. (2024) has been published to CoRL, and there are recent preprints for UNIC Sariyildiz et al. (2024), and UNIT Zhu et al. (2024). We report benchmarks that are common amongst the papers in table 18, but note that only AM-RADIO and Theia are published, and thus the other works are potentially subject to change as they work through the peer review process. For each model, we report the numbers from the original papers without attempting replication. We do run linear probing for Theia on the ADE20k task using our harness as it allows for the only task that all papers report. We confirmed the mIoU numbers with the authors before reporting them here. We also note that the settings, such as training dataset, resolution, set of teachers, and desired outcomes, are different between the models, which means there are numerous confounding factors preventing the comparison from being fair."
        },
        {
            "title": "Preprint",
            "content": "Method Teacher Classification MSE Segmentation MSE Cosine Hyb MSE Hyb SmL1 7.75 12.00 6.00 8. Global Stdze Standardize PHI-S PCA-W ZCA HCA MSE PHI-S 2.75 2.75 2.00 7.75 6.75 8.00 7.75 6. 12.00 3.00 11.00 5.00 5.00 7.00 5.00 9.50 9.50 8.00 1.50 1.50 12.00 2.00 3.00 4.50 4.50 7.50 2. 7.50 8.00 6.00 11.00 10.00 3D 12.00 7.25 8.25 6.00 1.00 10.00 2.00 9.00 SAM LLaVA Probe 1.5 COCO Baselines 11.67 7.67 8.83 5.17 Standardization 4.33 3.83 4.33 Whitening 7.33 5.67 7.33 AdaLoss 5.83 6. 8.00 11.00 12.00 5.00 3.00 4.00 3.75 4.50 5.00 4.50 4.75 3.00 9.25 9.75 7.00 6. Avg 9.40 6.99 6.51 6.28 4.35 4.81 3.39 7.31 7.57 7.72 7.06 6.58 Avg Avg No No COCO MSE/COCO 11.08 6.38 7.42 5.73 4.22 5.17 3.27 7.17 6.88 6.87 7.07 6. 11.92 4.98 7.77 5.17 4.58 5.77 3.58 7.02 6.92 6.58 6.90 6.81 Table 12: Average benchmark ranks across the suite including AdaLoss. For LLaVA, we first average the two GQA and TextVQA tasks separately, and then combine those with POPE and VQAv2 to compute the average. This is to prevent overly biasing towards the tasks that have multiple measurements. We observe that the standardization techniques perform the best, with PHI-S being the strongest normalization method studied. AdaLoss was able to improve over baseline, but is not competitive with the standardization methods. The raw benchmark scores are provided in appendix A.8.2. Method Zero Shot MSE Cosine Hyb MSE Hyb SmL1 Global Stdze Standardize PHI-S PCA-W ZCA HCA Ada - MSE Ada - PHI-S 56.17 71.44 69.34 71.19 70.91 70.51 70.73 70.23 70.38 70.47 72.89 72.73 Classification Segmentation SAM kNN ADE20k VOC COCO Depth Normals 71.54 79.74 78.72 79.49 79.51 79.35 79.53 79.30 79.28 79.33 79.85 80. 71.90 69.42 70.54 69.53 69.75 70.22 69.89 69.55 69.37 69.19 69.60 69.74 78.10 83.39 83.29 82.82 83.07 82.79 83.09 82.96 82.80 82.99 82.53 82.72 42.40 48.01 48.00 48.23 47.89 47.87 48.63 47.58 47.83 47.84 47.24 47.41 77.69 81.77 80.88 82.14 82.02 80.44 81.89 81.88 81.43 81.61 81.57 81.72 Probe 3D Surface MultiView 47.71 53.53 52.57 53.69 54.13 54.65 54.49 54.42 54.49 54.35 51.86 51.28 55.06 56.46 56.30 56.43 57.02 56.48 56.79 56.71 57.23 57.07 56.33 55. SPair 71k 33.56 39.59 43.44 40.45 42.53 45.27 43.92 44.24 43.35 43.14 36.85 36.46 Table 13: ViT-B/16 - Classification accuracy using both Zero Shot (DFN CLIP text encoder) and kNN. ADE20k and VOC are semantic segmentation linear probe results using 512px resolution (see Ranzinger et al. (2024) for details), and SAM COCO instance segmentation, also defined in AMRADIO. We also show the Probe 3D El Banani et al. (2024) metrics as also reported in AM-RADIO. GQA TextVQA Cosine Method Val MSE 67.35 70.02 Hyb MSE 69.86 70.03 Hyb SmL1 70.10 Global Stdze 70.04 Standardize 70.20 PHI-S PCA-W 69.85 ZCA 69.98 HCA 69.95 Ada - MSE 69.75 69.76 Ada - PHI-S TestDev Tokens No Tokens POPE VQAv2 72.21 15.06 76.14 24.13 75.94 23.53 76.17 23.90 76.21 22.55 76.20 24.20 76.30 23.28 75.93 24.14 76.02 24.63 76.07 24.79 26.63 76.09 76.03 25.81 59.51 61.82 61.96 62.35 62.28 62.16 62.55 62.01 62.37 61.79 62.31 61.90 85.16 84.78 85.19 85.74 85.88 85.94 85.52 85.43 85.80 85.61 85.06 85.54 47.31 50.24 50.15 50.19 50.31 50.28 50.25 50.48 50.11 49.92 50.82 50.90 Table 14: ViT-B/16 - LLaVA 1.5 (Vicuna 7B) results. We use the same suite in AM-RADIO, however we report both Val and TestDev for GQA, and also report the TextVQA score when OCR tokens are not provided as part of the context."
        },
        {
            "title": "Preprint",
            "content": "Method DFN CLIP (14) Baseline - MSE Global Stdze Standardize PHI-S 5.0200 4.6640 4.6520 4.6310 SigLIP DINOv2 0.9591 1.9030 0.6924 1.8620 0.7036 1.8560 1.8460 0.6961 SAM 5.9970 7.9080 7.7030 7.7190 Table 15: ViT-L/16 - Mean Squared Error for matching the teachers different algorithms. Lower values are better. Classification Segmentation SAM Method Zero Shot Baseline - MSE Global Stdze Standardize PHI-S 71.32 78.59 78.67 78. kNN ADE20k VOC COCO Depth Normals 78.80 83.15 83.05 83.16 82.62 85.58 84.79 85.73 80.21 84.51 84.04 84.77 72.91 71.23 71.69 71.12 47.01 50.94 51.27 51.23 Probe 3D Surface MultiView 48.44 57.86 58.34 58. 57.50 60.27 60.27 60.61 SPair 71k 35.67 52.24 52.42 51.74 Table 16: ViT-L/16 - Classification accuracy using both Zero Shot (DFN CLIP text encoder) and kNN. ADE20k and VOC are semantic segmentation linear probe results using 512px resolution (see Ranzinger et al. (2024) for details), and SAM COCO instance segmentation, also defined in AMRADIO. We also show the Probe 3D El Banani et al. (2024) metrics as also reported in AM-RADIO. GQA TextVQA Method Val Baseline - MSE 69.70 71.65 71.44 71.46 Global Stdze Standardize PHI-S TestDev Tokens No Tokens POPE VQAv2 75.44 21.21 78.37 31.43 78.19 33.56 33.67 78.31 62.11 63.08 63.11 63.07 48.88 53.15 52.97 52.88 85.72 86.03 86.21 86. Table 17: ViT-L/16 - LLaVA 1.5 (Vicuna 7B) results. We use the same suite in AM-RADIO, however we report both Val and TestDev for GQA, and also report the TextVQA score when OCR tokens are not provided as part of the context. Method Model AM-RADIO ViT-H/16 Theia ViT-B/16 UNIC ViT-B/16 UNIT ViT-H/14 ViT-B/16 ViT-L/16 PHI-S-RADIO ImageNet-1K Classification Probe Zero Shot 82.93 - 75.2 - 83.2 - - 78.76 - 73.16 - 80.45 kNN 86.06 - - 84.18 81.74 84. Segmentation ADE20k 51.34 35.61* 37.3 50.19 48.94 51.47 Table 18: Comparison between shared metrics of different agglomerative model approaches. *Our results"
        }
    ],
    "affiliations": [
        "NVIDIA"
    ]
}