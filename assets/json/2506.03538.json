{
    "paper_title": "Robust Neural Rendering in the Wild with Asymmetric Dual 3D Gaussian Splatting",
    "authors": [
        "Chengqi Li",
        "Zhihao Shi",
        "Yangdi Lu",
        "Wenbo He",
        "Xiangyu Xu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "3D reconstruction from in-the-wild images remains a challenging task due to inconsistent lighting conditions and transient distractors. Existing methods typically rely on heuristic strategies to handle the low-quality training data, which often struggle to produce stable and consistent reconstructions, frequently resulting in visual artifacts. In this work, we propose Asymmetric Dual 3DGS, a novel framework that leverages the stochastic nature of these artifacts: they tend to vary across different training runs due to minor randomness. Specifically, our method trains two 3D Gaussian Splatting (3DGS) models in parallel, enforcing a consistency constraint that encourages convergence on reliable scene geometry while suppressing inconsistent artifacts. To prevent the two models from collapsing into similar failure modes due to confirmation bias, we introduce a divergent masking strategy that applies two complementary masks: a multi-cue adaptive mask and a self-supervised soft mask, which leads to an asymmetric training process of the two models, reducing shared error modes. In addition, to improve the efficiency of model training, we introduce a lightweight variant called Dynamic EMA Proxy, which replaces one of the two models with a dynamically updated Exponential Moving Average (EMA) proxy, and employs an alternating masking strategy to preserve divergence. Extensive experiments on challenging real-world datasets demonstrate that our method consistently outperforms existing approaches while achieving high efficiency. Codes and trained models will be released."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 ] . [ 1 8 3 5 3 0 . 6 0 5 2 : r Robust Neural Rendering in the Wild with Asymmetric Dual 3D Gaussian Splatting Chengqi Li Department of Computing and Software McMaster University lic222@mcmaster.ca Zhihao Shi Department of Electrical and Computer Engineering McMaster University shiz31@mcmaster.ca Yangdi Lu Department of Computing and Software McMaster University luy100@mcmaster.ca Wenbo He Department of Computing and Software McMaster University hew11@mcmaster.ca Xiangyu Xu Xian Jiaotong University xuxiangyu2014@gmail.com"
        },
        {
            "title": "Abstract",
            "content": "3D reconstruction from in-the-wild images remains challenging task due to inconsistent lighting conditions and transient distractors. Existing methods typically rely on heuristic strategies to handle the low-quality training data, which often struggle to produce stable and consistent reconstructions, frequently resulting in visual artifacts. In this work, we propose Asymmetric Dual 3DGS, novel framework that leverages the stochastic nature of these artifacts: they tend to vary across different training runs due to minor randomness. Specifically, our method trains two 3D Gaussian Splatting (3DGS) models in parallel, enforcing consistency constraint that encourages convergence on reliable scene geometry while suppressing inconsistent artifacts. To prevent the two models from collapsing into similar failure modes due to confirmation bias, we introduce divergent masking strategy that applies two complementary masks: multi-cue adaptive mask and self-supervised soft mask, which leads to an asymmetric training process of the two models, reducing shared error modes. In addition, to improve the efficiency of model training, we introduce lightweight variant called Dynamic EMA Proxy, which replaces one of the two models with dynamically updated Exponential Moving Average (EMA) proxy, and employs an alternating masking strategy to preserve divergence. Extensive experiments on challenging real-world datasets demonstrate that our method consistently outperforms existing approaches while achieving high efficiency. Codes and trained models will be released."
        },
        {
            "title": "Introduction",
            "content": "3D scene reconstruction from multiple views is fundamental problem in computer vision. Recent advances such as Neural Radiance Fields (NeRF) [15] and 3D Gaussian Splatting (3DGS) [8] have achieved impressive rendering quality by learning volumetric or point-based scene representations Corresponding author Preprint. Under review. Figure 1: Left: The key insight of this work is that artifacts arising from low-quality in-the-wild inputs are typically stochastic across different runs of the same model (Baseline: Run 1 vs. Run 2). This motivates the design of the Asymmetric Dual 3DGS framework, which enhances true scene structure while suppressing errors through cross-model consistency (w/ consistency). Right: Our method compares favorably against the state-of-the-art approaches in terms of reconstruction quality while maintaining high training efficiency. Results are on the NeRF On-the-go dataset [18]. from posed images. However, these methods typically assume that training images exhibit consistent illumination and minimal occlusion, which are rarely satisfied in real-world settings. In-the-wild images are often captured under varying lighting conditions and contain transient distractors such as pedestrians or vehicles; these factors introduce substantial noise into the supervision signal, leading to degraded reconstruction quality and visual artifacts. While several recent works have attempted to address these challenges [14, 19, 18, 26, 10, 20, 2, 13], they largely rely on heuristic strategies to suppress the effects of corrupted supervision from low-quality training data, such as per-image appearance embeddings that are only weakly or indirectly supervised through photometric losses [14, 26], or hand-crafted rules to filter outlier training signals [19]. As result, such approaches often lack stability and generalizability, which leads to artifact-prone reconstructions. To bridge this gap, we propose new framework, called Asymmetric Dual 3DGS, which is motivated by key empirical observation: artifacts arising from low-quality in-the-wild training data are typically stochastic. In other words, the artifacts vary randomly across different runs of the same model with only minor training perturbations, such as data order shuffling (see Figure 1-Left). This suggests that enforcing consistency between independently trained 3DGS can help suppress unreliable or spurious signals in the in-the-wild training images. To this end, we introduce dualmodel architecture where two 3DGS models are trained concurrently with consistency constraint, following the intuition that true scene structure should be consistently reconstructed across different model runs, while the artifacts induced by low-quality data tend to diverge. Nevertheless, naively training two models in parallel can lead to confirmation bias, where both models reinforce the same errors. To encourage more divergent error modes and mitigate confirmation bias, we introduce divergent masking strategy: applying distinct masks to each model that emphasize complementary factors for filtering out transient or distracting content. One mask is learned in self-supervised manner based on feature-level similarity between predicted and ground truth images, while the other, called multi-cue adaptive mask, uses stereo-based correspondence to identify likely distracting regions. These complementary filtering schemes encourage the two 3DGS models to focus on different static aspects of the scene. Consequently, this asymmetric strategy leads to divergent and complementary optimization paths and reduces shared error modes. The final reconstruction is then guided by the agreement between the two models, which reliably captures consistent and accurate scene structures while suppressing artifacts. While the dual-model framework effectively improves the reconstruction quality, it introduces notable computational overhead in the training process. To mitigate this, we introduce lightweight variant, called Dynamic EMA Proxy, which replaces the second 3DGS model with dynamic, training-free Exponential Moving Average (EMA) copy of the primary model. Unlike standard EMA [4], our Dynamic EMA proxy is specifically designed to track the evolving nature of 3DGS representations, accounting for Gaussian densification and pruning. Since only one model is actively trained in this setup, which no longer allows independent masks for two models, we additionally design an alternating masking strategy that alternates between the two masks, maintaining divergent training signals and mitigating confirmation bias. Our contributions are as follows: 1) We propose an Asymmetric Dual 3DGS framework for in-thewild 3D scene reconstruction. By enforcing consistency constraints between two 3DGS models with complementary masks, our framework significantly improves the robustness and accuracy of 2 scene representations. 2) We develop divergent masking strategy by introducing different masking mechanisms for each 3DGS model, which handle various types of distractors and promote divergent optimization paths to mitigate confirmation bias. 3) To address the computational overhead of the dual-model framework, we introduce Dynamic EMA Proxy, coupled with an alternating masking strategy, which effectively improves training efficiency. 4) We conduct extensive evaluations across diverse set of in-the-wild 3D scene reconstruction datasets, demonstrating that our method consistently achieves state-of-the-art performance and efficiency, highlighting its robustness and generality."
        },
        {
            "title": "2 Related Work",
            "content": "3D Scene Reconstruction. Neural Radiance Fields (NeRF) [15] revolutionizes photorealistic novel view synthesis by modeling scenes as continuous functions that map 3D coordinates to color and density. More recently, 3D Gaussian Splatting (3DGS) [8, 25] has gained attention as real-time alternative, representing scenes with optimizable Gaussian primitives. While effective, both methods assume static scenes to enforce multi-view consistency, an assumption often violated in in-the-wild settings due to varying illumination and transient objects, limiting their practical applicability. 3D Scene Reconstruction in the Wild. NeRF-W [14] first addressed 3D scene reconstruction in the wild with modular architecture combining learnable appearance embedding and an uncertainty map to suppress distractorsan approach that has since become standard. NeRF On-the-go [18] builds on this by using DINOv2 feature [16] residuals to construct uncertainty maps. RobustNeRF [19] tackles noisy training images through robust loss function. GS-W [26], Wild-GS [24] and WildGaussian [11] extend learnable embeddings with both global and per-Gaussian local embeddings for fine-grained appearance modeling. SpotlessSplats [20] introduces learnable mask based on thresholded and dilated residuals, while SWAG [2] adds view-dependent opacity term per Gaussian to identify transient distractors. HybridGS [13] employs dual-model setup (3DGS for static content and 2DGS [5] for dynamic distractors) to learn an accurate uncertainty map iteratively. Despite their contributions, these methods largely rely on heuristic strategies to suppress corrupted supervision signals from low-quality training data. For instance, the per-image appearance embeddings are only weakly or indirectly supervised through photometric losses [14, 26], and outlier filtering is often governed by hand-crafted rules [19]. As result, these approaches frequently suffer from instability and produce reconstructions with noticeable artifacts. In this work, we propose principled framework, Asymmetric Dual 3DGS, to reduce such artifacts and improve reconstruction stability, based on the key observation that many artifacts exhibit stochastic behavior and are not consistent across different training runs. Our method explicitly exploits this property to enhance robustness and achieve high-fidelity reconstructions."
        },
        {
            "title": "3 Method",
            "content": "An overview of the proposed algorithm is shown in Figure 2. Please refer to the caption for details. 3.1 Preliminaries 3D Gaussian Splatting (3DGS) [8] represents 3D scene as set of 3D anisotropic Gaussians = {Gi}N i=1. Each Gaussian Gi is parameterized by centroid Xi, covariance matrix Σi, an opacity αi, and set of spherical harmonic (SH) coefficients θi for view-dependent color representation. For rendering with viewing camera Vj, we project both centroids of Gaussians and the covariance matrix onto the 2D image plane as xij and Σij, respectively. The projected opacity αij, which is function of 2D image plane coordinate y, can then be defined as below: αij(y) = αi exp (cid:20) (y xij)T Σij (cid:21) 1(y xij) 1 2 (1) The color of pixel located at for camera Vj is computed using the α-blending with the following formula: C(y, Vj, G) = (cid:88) i=1 cijαij(y) i1 (cid:89) k=1 (1 αkj(y)) , cij = SH (rij, θi) (2) 3 Figure 2: Overview of the Asymmetric Dual 3DGS framework. Two 3DGS models G1 and G2 are concurrently optimized with the reconstruction loss LMh r2 (Eq. 4), along with the mutual consistency loss Lm1 and Lm2 (Eq. 6). In addition, we apply mask loss (Eq. 7) for learning soft mask in self-supervised manner. For improved efficiency, we also propose an EMA version of our framework by replacing G2 with dynamic EMA proxy. Both the mask loss and the EMA proxy have been omitted here for clarity. Note that the color transform in this figure is for illustration purpose, which undergoes rasterization process in practice as introduced in Section 3.1. r1 and LMs where rij is the ray direction from the j-th camera center to the i-th Gaussian centroid, and cij is the corresponding color of the observed Gaussian primitive obtained using spherical harmonic (SH) function. Performing Eq. 2 for every pixel on the image plane constitutes the rasterization process, which results in the rendered image ˆIG = Rasterize(G, Vj). Frame-Dependent Appearance Modeling. To address appearance variations in in-the-wild data, we follow the approach of WildGaussian [11] to adaptively adjust the observed color of the Gaussian primitives to account for the frame-dependent factors, such as the varying illumination across images captured at different times of day. This adjustment is conditioned on both the per-Gaussian appearance embedding pi and the per-view appearance embedding qj. Specifically, pi, qj, and cij are sent into an MLP to predict affine transformation parameters: (a, b) = (pi, qj, cij), cij = cij + b, (3) where and are three-dimensional outputs corresponding to RGB channels. The transformed color cij is then used to replace cij in the blending process described by Eq. 2, and the resulting frame-dependent image is denoted as IG = Rasterizedep(G, Vj). During training, pi, qj, , and 3DGS parameters are jointly optimized. 3.2 Dual 3DGS central insight of this work is that artifacts arising from in-the-wild training data are typically stochastic in nature. When two 3DGS models are trained on the same scene but with different view sampling orders, their static scene representations remain consistent, whereas their renderings could diverge in regions affected by outliers. An example is shown in Figure 1-Left. Motivated by this observation, we introduce Dual 3DGS framework, where each model is trained with different sampling order, and consistency constraint is enforced between their renderings. Specifically, we maintain two sets of Gaussians, G1 and G2, to represent the same scene. In each training iteration, we independently sample two views from separate training view lists, yielding two viewing cameras V1 and V2, along with their corresponding ground-truth images, I1 and I2. Similar to 3DGS [8], we train G1 and G2 with reconstruction objectives defined as: r1 = Lrecon(IG1 LM 1 , I1, M), LM r2 = Lrecon(IG2 2 , I2, M), Lrecon(I, I, M) = λ DSSIM(M I, I) + (1 λ) I I1, 4 (4) (5) where IGn = Rasterizedep(Gn, Vj) is the rendered image for the n-th 3DGS model from viewpoint Vj using Eq. 2 and 3. is spatial mask to filter out transient distracting regions, such as pedestrians or moving vehicles, which will be detailed in Section 3.3. denotes element-wise multiplication. DSSIM represents the structural dissimilarity index measure [23]. λ is hyperparameter to balance the DSSIM and the L1 terms. Mutual Consistency. Since G1 and G2 represent the same underlying scene, their renderings from the same camera viewpoint should stay close. This motivates us to define mutual consistency regularization as: 1 ˆIG Lm1 = ˆIG2 1 1, Lm2 = ˆIG1 (6) where ˆIGn = Rasterize(Gn, Vj) is the frame-independent rendering obtained via Eq. 2. We emphasize that this consistency constraint is performed over ˆIGn captures the intrinsic appearance of the 3D scene, whereas IGn is frame dependent affected by dynamic lighting. This strategy provides principled way to preserve static structures while suppressing spurious signals, which enables more robust and reliable reconstruction. instead of IGn , because ˆIGn 2 ˆIG2 2 Note that we only use the L1 loss for consistency regularization in Eq. 6 as incorporating the DSSIM loss adversely affects performance in our experiments. Furthermore, we empirically find that incorporating consistency regularization too early in training can hinder convergence, as both models may still be dominated by noise and unstable geometry. To address this, we adopt progressive strategy: we first allow the two models to be trained independently for number of warm-up iterations, during which they develop their own estimates of the static scene. Once their reconstructions become sufficiently stable, we introduce the consistency loss to encourage convergence on shared, reliable structures. 3.3 Asymmetric Dual 3DGS While the Dual 3DGS framework offers improved consistency through mutual supervision, its symmetric design, where both 3DGS models are trained in the same manner using the reconstruction loss in Eq. 5, poses risk of confirmation bias: both models may converge toward the same reconstruction errors due to their similar optimization signals. To address this issue, we propose an Asymmetric Dual 3DGS variant, where each model is trained with distinct masking strategy that emphasizes complementary criteria for filtering out transient or distracting content. This encourages divergent error patterns, enhances robustness, and mitigates confirmation bias. Specifically, we use Multi-Cue Adaptive Mask and Self-Supervised Soft Mask. Multi-Cue Adaptive Mask (Mh). As illustrated in Figure 3, Mh is hard binary mask (1 indicates static regions and 0 indicates distractors) that identifies transient and distracting regions by integrating multiple cues, including semantic segmentation, stereo correspondence, pixel-level residuals, and feature-level residuals. We begin by applying the Segment Anything (SAM) model [9, 12] to partition each image into semantically coherent regions. To detect static content, we perform multi-view stereo matching across the training images with COLMAP [21]. Semantic regions are considered static if they contain sufficient number of valid correspondences (we empirically choose threshold of 3 matches). Among the remaining regions, we identify transient distractors by analyzing reconstruction residuals. For each region, we compute pixel-level residuals, i.e., the L1 error between the rendered and groundtruth images, and feature-level residuals, i.e., the cosine distance between DINOv2-encoded feature maps [16] of the rendered and ground-truth images. Regions with above-average residuals in both metrics are classified as distractors and masked out during training. This multi-cue approach offers higher robustness than prior methods that rely on single cues [19, 18, 14, 17], which generalizes more effectively across diverse in-the-wild scenes. Self-Supervised Soft Mask (Ms). To complement the rule-based hard mask Mh, we introduce learnable soft mask Ms, whose values range between 0 and 1. Unlike the static Mh, this soft mask is optimized jointly with the model and adapts throughout training. The objective for Ms is derived from the cosine similarity between DINOv2 feature maps of the ground-truth image and the rendered image F: Lmask = Ms finterp(CosineSimilarity(F, F))1, (7) 5 Input Mh Ms (Early) Ms (Final) Figure 3: Comparisons of hard and soft masks. Distractors are highlighted in red boxes in the input. The right four columns show the evolving of the self-supervised soft mask across different epochs. where finterp denotes spatial interpolation to match the training image resolution. This formulation is self-supervised, requiring no ground-truth masks. We initialize Ms as an all-one tensor, allowing the model to gradually refine the mask as training progresses (Figure 3). As shown in Figure 3, the hard mask Mh is more definitive with clearer boundaries but can be overconfident, potentially missing certain transient objects. In contrast, the soft mask Ms is often more sensitive to subtle distractors and better captures ambiguous regions, thus providing complementary information to Mh. By combining the loss terms in Eq. 5, 6, and 7, the overall objective for our Asymmetric Dual 3DGS can be written as: = LMh r1 + LMs r2 + λm(Lm1 + Lm2) + λmaskLmask, (8) where λm and λmask are hyperparameters for balancing loss terms. Since Ms dynamically evolves during training and intrinsically differs from the fixed Mh, training one model with LMh r1 and the other with LMs r2 in our Dual 3DGS framework introduces complementary inductive biases. This asymmetry promotes diverse learning dynamics, making it less likely for the two models to converge on the same reconstruction error, thereby reducing confirmation bias and enhancing overall robustness. 3.4 Dynamic EMA Proxy While the Asymmetric Dual 3DGS framework significantly improves robustness and reconstruction quality, it requires simultaneous training of two 3DGS models, which introduces considerable computational overhead and undermines the fast training advantage of 3DGS. To address this issue, we propose more efficient alternative by replacing one of the two models with dynamic EMA proxy. This design retains the benefits of dual-model regularization while significantly reducing computation cost. Let G1 denote the set of Gaussians actively optimized during training, and GEMA its EMA counterpart, updated at each training step by: Gt EMA = β Gt1 EMA + (1 β) Gt where and 1 denote the current and previous training iterations, respectively. Here, we slightly abuse set notation for simplicity: the weighted summation between G1 and GEMA is performed element-wise over corresponding Gaussian attributes, such as the centroids, opacities, and SH coefficients. EMA = G0 1, 1, G0 (9) We then rewrite the consistency regularization in Eq. 6 with the EMA proxy as follows: Lme = ˆIGEMA ˆIG1 1 1, (10) 1 where ˆIGEMA = Rasterize(GEMA, V1) is the rendering of the EMA Gaussians from view V1. Since only one 3DGS model requires gradient updates, and the EMA update is simple weighted average, this approach greatly improves training efficiency while preserving the benefits of dual-model consistency. Dynamic Update. Standard EMA is primarily designed for neural networks, where the number of parameters is typically fixed throughout training [22, 4]. However, applying it to 3DGS presents 6 Table 1: Quantitative results on the NeRF On-the-go dataset [18]. Efficiency is reported in terms of average training hours per scene. The best and second-best results are highlighted in bold and underline, respectively. Scene Method High Occlusion Medium Occlusion Low Occlusion PSNR SSIM LPIPS PSNR SSIM LPIPS PSNR SSIM LPIPS Hrs. RobustNeRF [19] NeRF On-the-go [18] 3DGS [8] Mip-Splatting [25] GS-W [26] WildGaussian [11] SLS-mlp [20] HybridGS [13] Ours (GS-GS) Ours (EMA-GS) 20.60 22.37 19.03 19.25 18.52 23.03 21.92 23.05 24.34 24.12 0.602 0.753 0.649 0.664 0.645 0.771 0.710 0.768 0.825 0.818 0.379 0.212 0.340 0.333 0.335 0.172 0.222 0. 0.150 0.154 21.72 22.50 19.19 19.73 21.04 22.80 22.79 23.51 24.56 24.32 0.741 0.780 0.709 0.684 0.737 0.811 0.817 0.830 0.872 0.864 0.248 0.205 0.220 0.279 0.208 0.092 0.162 0. 0.090 0.090 16.60 20.13 19.68 20.03 19.75 20.62 20.02 21.42 21.91 21.77 0.407 0.627 0.649 0.661 0.660 0.658 0.596 0.684 0.728 0.722 0.480 0.287 0.199 0.195 0.287 0.235 0.276 0. 0.189 0.162 - 43 0.35 0.16 0.55 0.50 - 0.18 0.28 0.18 unique challenges, as the number of Gaussians dynamically changes during training due to operations such as cloning, splitting, and pruning [8]. To support this dynamic data structure, we develop dynamic EMA mechanism by introducing the following rules: 1) Cloning: When Gaussian is cloned, its EMA attributes are also cloned. 2) Pruning: When Gaussian is pruned, its EMA counterpart is removed as well. 3) Splitting: When Gaussian splits into two, attributes that undergo discontinuous changes, i.e., the centroids and variances, are reinitialized in the EMA according to the values of the split Gaussians. The remaining attributes (e.g., opacities and SH coefficients) are directly inherited from the original EMA representation. Alternating Masking Strategy. Since only one model is trainable in our EMA framework, the original asymmetric training strategy used in Eq. 8 (i.e., Mh and Ms) is not directly applicable. Instead, we propose an alternating masking strategy by switching between the hard mask Mh and the soft mask Ms for training G1, which retains the complementary advantages from both decisive, rule-based filtering and adaptive, learned filtering. The final loss for our dynamic EMA framework can be written as: = Mh/s r1 + λmLme + λmaskLmask, (11) where Mh/s indicates alternating between masks. This strategy essentially injects randomness into the EMA update process, promoting diversity in optimization and reducing overfitting to erroneous supervision signals. Note that we also explored other forms of randomness, including randomly mixing up EMA renderings with ground truth and applying random dropout of Gaussian primitives. Nevertheless, we empirically find that alternating masking remains the most effective approach. Discussion. Our approach is related to prior works that also leverage EMA, such as [22] and [4], which apply EMA to neural networks for tasks like semi-supervised or unsupervised image classification. However, our method diverges in key aspects: unlike these methods that operate in the context of neural networks, we apply EMA to 3DGS, dynamic representation where the number of Gaussians evolves throughout training. This necessitates our proposed dynamic EMA mechanism, which adapts EMA updates to structural changes in the learned scene representation. Additionally, we introduce an alternating masking strategy to preserve the benefits of asymmetric training even with single learnable model. These innovations mark significant departures from conventional EMA usage and highlight the contributions of this work."
        },
        {
            "title": "4 Experiments",
            "content": "We evaluate our method on three in-the-wild datasets: the NeRF On-the-go dataset [18], the RobustNeRF dataset [19], and the PhotoTourism dataset [6]. NeRF On-the-go and RobustNeRF mainly suffer from transient distractors. PhotoTourism contains both distractors and dynamic lighting. We denote our full dual-model approach as GS-GS, and the efficient variant with the EMA proxy as EMA-GS. Implementation details are provided in the supplementary material. 7 Our (EMA-GS) Mip-Splatting [25] HybridGS [13] Figure 4: Qualitative results on the NeRF On-the-go [18] (top) and the RobustNeRF [19] (bottom) datasets. Table 2: Quantitative results on the RobustNeRF dataset [19]. Efficiency is reported in terms of average training hours per scene. The best and second-best results are highlighted in bold and underline, respectively. Ours (GS-GS) Ground Truth Scene Method Statue Android Yoda Crab PSNR SSIM LPIPS PSNR SSIM LPIPS PSNR SSIM LPIPS PSNR SSIM LPIPS Hrs. RobustNeRF [19] NeRF On-the-go [18] 3DGS [8] Mip-Splatting [25] GS-W [25] WildGaussian [11] SLS-mlp [20] HybridGS [13] Ours (GS-GS) Ours (EMA-GS) 20.60 21.58 21.02 22.08 21.99 23.25 22.54 22.93 23.47 23.49 0.760 0.770 0.810 0.860 0.862 0.886 0.840 0. 0.894 0.890 0.150 0.240 0.160 0.135 0.102 0.105 0.130 0.100 0.097 0.096 23.28 23.50 23.11 23.45 24.23 24.57 25.05 25.15 25.61 25.47 0.750 0.750 0.810 0.801 0.824 0.827 0.850 0. 0.857 0.849 0.130 0.210 0.130 0.106 0.090 0.085 0.090 0.070 0.071 0.068 29.78 29.96 26.33 27.96 32.74 32.84 33.66 35.32 37.18 36.50 0.820 0.830 0.910 0.933 0.957 0.956 0.960 0. 0.969 0.967 0.150 0.240 0.140 0.136 0.084 0.091 0.100 0.070 0.074 0.077 - - 29.74 29.18 33.22 32.81 34.43 35.17 36.18 35.60 - - - 0.929 0.952 0.952 - 0. 0.964 0.961 - - - 0.129 0.088 0.092 - 0.080 0.078 0.080 - - - 0.14 0.37 0.82 - - 0.31 0.21 4.1 Comparison with SOTA As shown in Tables 1-3, the proposed Asymmetric Dual 3DGS framework, in both the GS-GS and EMA-GS setups, consistently outperforms all baselines by significant margin across all evaluation datasets, highlighting the robustness and generality of our approach. Visual comparisons are provided in Figure 4-5. In terms of training efficiency, our GS-GS model trains in an average of 30 minutes per scene on the NeRF On-the-go and RobustNeRF datasets, while the EMA proxy further reduces training time by one-third. For the PhotoTourism dataset, which features high-resolution imagery and dynamic lighting, EMA-GS cuts training time from 7.2 hours to 2.9 hours compared to the previous SOTA [11] while achieving better reconstruction quality. 4.2 Ablation Study Dual 3DGS framework. As shown in Table 4, the dual 3DGS framework, whether implemented directly or via an EMA proxy, consistently outperforms the single-model baseline across three datasets (L1 vs. L5 & L12 in Table 4). Importantly, the mutual consistency loss is crucial component. Removing it reduces the framework to simple ensemble, which is shown to be ineffective, leading to an average drop of 0.5 dB in PSNR and consistent degradation in SSIM and LPIPS (L5 vs. L9, L12 vs. L16). Additionally, the GS-GS setup achieves better performance than its EMA proxy counterpart (L5 vs. L12). We attribute this to two main factors: first, the GS-GS setup enables both models to be actively updated, effectively doubling the training iterations; second, the EMA proxy introduces confirmation bias due to its model accumulation nature, which limits its ability to correct erroneous predictions. Distractor modeling. The results in Table 4 further show that applying masks significantly improves reconstruction performance especially when distractors occupy large portion of the input image. (L5 vs. L6, L12 vs. L13). Either Mh or Ms independently improves performance over the base model (for Mh, L2 vs. L3, L6 vs. L7, L13 vs. L14; for Ms, L2 vs. L4, L6 vs. L8, L13 vs. L15), indicating that both masks are effective, although they have different characteristics. This observation is further supported by 8 Mip-Splatting [25] WildGaussian [11] Our (EMA-GS) Ours (GS-GS) Ground Truth Figure 5: Qualitative results on the PhotoTourism dataset [6]. Table 3: Quantitative results on the PhotoTourism dataset [6]. Efficiency is reported in terms of average training hours per scene. The best and second-best results are highlighted in bold and underline, respectively. We did not compare with HybridGS [13] on PhotoTourism, as it does not consider varying illumination, and cannot be customized or retrained due the absence of training code. Scene Method NeRF [15] 3DGS [8] Mip-Splatting [25] NeRF-W [14] Ha-NeRF [1] K-Planes [3] RefinedFields [7] GS-W [26] SWAG [2] WildGaussian [11] Ours (GS-GS) Ours (EMA-GS) Brandenburg Gate Sacre Coeur Trevi Fountain PSNR SSIM LPIPS PSNR SSIM LPIPS PSNR SSIM LPIPS Hrs. 18.90 19.37 20.01 24.17 24.04 25.49 26.64 23.51 26.33 27.77 28.56 28.50 0.882 0.880 0.877 0.891 0.887 0.879 0.886 0.897 0.929 0.927 0.938 0. 0.138 0.141 0.166 0.152 0.139 0.224 - 0.166 0.139 0.133 0.109 0.115 15.60 17.44 17.54 19.20 20.02 20.61 22.26 19.39 21.16 22.56 23.78 23.37 0.846 0.835 0.831 0.803 0.801 0.774 0.817 0.825 0.860 0.859 0.887 0. 0.163 0.204 0.203 0.192 0.171 0.265 - 0.211 0.185 0.177 0.139 0.150 16.14 17.58 17.36 18.97 20.18 22.67 23.42 20.06 23.10 23.63 24.52 23.85 0.696 0.709 0.684 0.698 0.691 0.714 0.737 0.723 0.815 0.766 0.790 0. 0.282 0.266 0.319 0.265 0.223 0.317 - 0.274 0.208 0.228 0.202 0.242 - 2.2 2.3 164 452 0.6 150 1.2 0.8 7.2 5.3 2.9 the visual results in Figure 3, where the hard-selected mask performs well in simpler scenes with clearly defined regions (Figure 3-Top), while the self-supervised mask excels in more complex scenes containing multiple or ambiguous distractors (Figure 3-Bottom). Moreover, combining both masks leads to additional gains, confirming their complementary roles (L5 vs. L7 & L8, L12 vs. L14 & L15). EMA proxy. The effectiveness of the EMA-GS setup heavily depends on the masking strategy (L5 vs. L7 & L8). Using only single type of mask often leads to performance similar to or even worse than the single-model baseline (L3 vs. L7, L4 vs. L8), indicating that confirmation bias can undermine robustness. This highlights the need for diverse masking signals to fully exploit the benefits of the EMA proxy. Moreover, as introduced in Section 3.4, we have also tried other forms of regularization to improve the performance of our EMA model, such as random mixup or dropout. However, as shown in L10 and L11 of Table 4, these methods do work as good as our original approach of alternating between the two masking strategies."
        },
        {
            "title": "5 Conclusion",
            "content": "In this work, we present Asymmetric Dual 3DGS, robust and efficient framework for 3D scene reconstruction in unconstrained, in-the-wild environments. Our method employs two 3DGS models guided by distinct masking strategies to enforce cross-model consistency, effectively mitigating artifacts caused by low-quality observations. To further improve training efficiency, we introduce dynamic EMA proxy that significantly reduces computational cost with minimal impact on performance. Extensive experiments on three challenging real-world datasets validate the effectiveness and generality of our approach. 9 Table 4: Effectiveness of different modules. The first block presents results from single base model using different mask strategies. The second and third blocks evaluate the EMA-GS and GS-GS setups, respectively. w/ Mh/s indicates alternating between Mh and Ms at each training iteration. w/o denotes that no mask is applied. The results are averaged across scenes within each dataset. Dataset Line Method PhotoTourism NeRF On-the-go RobustNeRF PSNR SSIM LPIPS PSNR SSIM LPIPS PSNR SSIM LPIPS 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 Single w/ Mh/s Single w/o Single w/o Ms Single w/o Mh EMA-GS EMA-GS w/o EMA-GS w/o Ms EMA-GS w/o Mh EMA-GS w/o Lm EMA-GS w/ Mixup EMA-GS w/ Dropout GS-GS GS-GS w/o GS-GS w/o Ms GS-GS w/o Mh GS-GS w/o Lm 24.76 24.68 24.82 24.83 25.24 24.70 24.76 24.84 24.73 25.13 25.11 25.62 25.09 25.24 25.11 25.08 0.864 0.859 0.862 0. 0.864 0.862 0.864 0.863 0.861 0.866 0.863 0.872 0.867 0.867 0.867 0.869 0.167 0.174 0.172 0.171 0.169 0.170 0.166 0.169 0.174 0.162 0.171 0.150 0.160 0.159 0.161 0.155 22.97 19.67 22.97 22. 23.40 21.68 22.94 22.15 23.10 23.39 23.39 23.61 22.39 23.32 22.76 23.13 0.798 0.669 0.802 0.783 0.801 0.766 0.802 0.780 0.801 0.802 0.804 0.809 0.785 0.812 0.794 0.808 0.133 0.269 0.125 0. 0.135 0.172 0.126 0.159 0.132 0.132 0.130 0.143 0.164 0.132 0.157 0.135 29.62 25.67 28.97 29.38 30.27 28.42 29.08 29.29 29.72 30.18 30.19 30.61 29.45 30.38 30.17 29.92 0.914 0.881 0.908 0. 0.917 0.905 0.909 0.911 0.915 0.915 0.913 0.921 0.914 0.920 0.919 0.918 0.083 0.126 0.088 0.086 0.080 0.092 0.089 0.087 0.081 0.081 0.082 0.080 0.087 0.079 0.082 0."
        },
        {
            "title": "References",
            "content": "[1] Xingyu Chen, Qi Zhang, Xiaoyu Li, Yue Chen, Ying Feng, Xuan Wang, and Jue Wang. Hallucinated Neural Radiance Fields in the Wild. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1294312952, June 2022. [2] Hiba Dahmani, Moussab Bennehar, Nathan Piasco, Luis Roldão, and Dzmitry Tsishkou. SWAG: Splatting in the Wild Images with Appearance-Conditioned Gaussians. In European Conference on Computer Vision (ECCV), volume 15134, pages 325340, 2024. [3] Sara Fridovich-Keil, Giacomo Meanti, Frederik Rahbæk Warburg, Benjamin Recht, and Angjoo Kanazawa. K-Planes: Explicit Radiance Fields in Space, Time, and Appearance. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1247912488, June 2023. [4] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum Contrast for Unsupervised Visual Representation Learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2020. [5] Binbin Huang, Zehao Yu, Anpei Chen, Andreas Geiger, and Shenghua Gao. 2D Gaussian Splatting for Geometrically Accurate Radiance Fields. In SIGGRAPH 2024 Conference Papers. Association for Computing Machinery, 2024. doi: 10.1145/3641519.3657428. [6] Yuhe Jin, Dmytro Mishkin, Anastasiia Mishchuk, Jiri Matas, Pascal Fua, Kwang Moo Yi, and Eduard Trulls. Image Matching across Wide Baselines: From Paper to Practice. International Journal of Computer Vision, 2020. [7] Karim Kassab, Antoine Schnepf, Jean-Yves Franceschi, Laurent Caraffa, Jeremie Mary, and Valérie Gouet-Brunet. RefinedFields: Radiance Fields Refinement for Unconstrained Scenes. arXiv preprint arXiv:2312.00639, 2023. [8] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuehler, and George Drettakis. 3D Gaussian Splatting for Real-Time Radiance Field Rendering. ACM Transactions on Graphics, 42, July 2023. [9] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollar, and Ross Girshick. Segment Anything. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 40154026, October 2023. 10 [10] Jonas Kulhanek and Torsten Sattler. NerfBaselines: Consistent and Reproducible Evaluation of Novel View Synthesis Methods. arXiv preprint arXiv:2406.17345, 2024. [11] Jonas Kulhanek, Songyou Peng, Zuzana Kukelova, Marc Pollefeys, and Torsten Sattler. In Advances in Neural Information WildGaussians: 3D Gaussian Splatting In the Wild. Processing Systems, volume 37, pages 2127121288, 2024. [12] Feng Li, Hao Zhang, Peize Sun, Xueyan Zou, Shilong Liu, Chunyuan Li, Jianwei Yang, Lei Zhang, and Jianfeng Gao. Segment and Recognize Anything at Any Granularity. In European Conference on Computer Vision (ECCV), pages 467484. Springer Nature Switzerland, 2024. [13] Jingyu Lin, Jiaqi Gu, Lubin Fan, Bojian Wu, Yujing Lou, Renjie Chen, Ligang Liu, and Jieping Ye. HybridGS: Decoupling Transients and Statics with 2D and 3D Gaussian Splatting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2025. [14] Ricardo Martin-Brualla, Noha Radwan, Mehdi S. M. Sajjadi, Jonathan T. Barron, Alexey Dosovitskiy, and Daniel Duckworth. NeRF in the Wild: Neural Radiance Fields for Unconstrained Photo Collections. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 72107219, June 2021. [15] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis. In European Conference on Computer Vision (ECCV), pages 405421. Springer Nature Switzerland, 2020. [16] Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Mido Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. DINOv2: Learning Robust Visual Features without Supervision. Transactions on Machine Learning Research, 2023. [17] Konstantinos Rematas, Andrew Liu, Pratul P. Srinivasan, Jonathan T. Barron, Andrea Tagliasacchi, Thomas Funkhouser, and Vittorio Ferrari. Urban Radiance Fields. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 12932 12942, June 2022. [18] Weining Ren, Zihan Zhu, Boyang Sun, Jiaqi Chen, Marc Pollefeys, and Songyou Peng. NeRF On-the-go: Exploiting Uncertainty for Distractor-free NeRFs in the Wild. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 89318940, June 2024. [19] Sara Sabour, Suhani Vora, Daniel Duckworth, Ivan Krasin, David J. Fleet, and Andrea Tagliasacchi. RobustNeRF: Ignoring Distractors with Robust Losses. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2062620636, June 2023. [20] Sara Sabour, Lily Goli, George Kopanas, Mark Matthews, Dmitry Lagun, Leonidas Guibas, Alec Jacobson, David Fleet, and Andrea Tagliasacchi. SpotLessSplats: Ignoring Distractors in 3D Gaussian Splatting. ACM Transactions on Graphics, 2025. [21] Johannes Schonberger and Jan-Michael Frahm. Structure-from-motion revisited. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 41044113, 2016. [22] Antti Tarvainen and Harri Valpola. Mean Teachers are Better Role Models: Weight-averaged Consistency Targets Improve Semi-supervised Deep Learning Results. In Advances in Neural Information Processing Systems, volume 30, 2017. [23] Zhou Wang, Alan Bovik, Hamid Sheikh, and Eero Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing, 13(4): 600612, 2004. 11 [24] Jiacong Xu, Yiqun Mei, and Vishal M. Patel. Wild-GS: Real-Time Novel View Synthesis from Unconstrained Photo Collections. In Advances in Neural Information Processing Systems, volume 37, pages 103334103355, 2024. [25] Zehao Yu, Anpei Chen, Binbin Huang, Torsten Sattler, and Andreas Geiger. Mip-Splatting: Alias-free 3D Gaussian Splatting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1944719456, June 2024. [26] Dongbin Zhang, Chuming Wang, Weitao Wang, Peihao Li, Minghan Qin, and Haoqian Wang. Gaussian in the Wild: 3D Gaussian Splatting for Unconstrained Image Collections. In European Conference on Computer Vision (ECCV), pages 341359, 2024. [27] Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, and Oliver Wang. The Unreasonable Effectiveness of Deep Features as Perceptual Metric. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 586595, June 2018. 12 Multi-Cue Adaptive Mask Some prior works [19, 18] use residuals between ground-truth and rendered images to detect distractors, assuming static regions are learned first. However, this can misclassify object boundaries and miss distractors resembling the background. Others [14, 17] use pretrained semantic segmentation to mask known distractors, such as people or sky, but these methods rely on task-specific priors and lack generality across diverse scenes. We propose Multi-Cue Adaptive Masking to combine the strengths of residual-based and segmentation-based methods, while also providing complementary hard mask that captures distinct error patterns compared to the self-supervised soft mask. Algorithm 1 Multi-Cue Adaptive Masking Require: Rendered image I, ground-truth image I, semantic masks {Mk} from SAM, stereo correspondence map S3 from COLMAP 1: Epix = I1 2: = DINOv2(I); = DINOv2(I) 3: Efeat = 1 CosineSimilarity( F, F) 4: epix = (cid:80) Epix/Area(I) 5: efeat = (cid:80) Efeat/Area(I) 6: = (cid:80) S3/Area(I) 7: for each mask Mk do 8: 9: 10: 11: 12: 13: 14: end for 15: return Mh = 1 (cid:83){Mk}selected end if Mark Mk as distractor mask epix,k = (cid:80) Mk Epix/ (cid:80) Mk efeat,k = (cid:80) Mk Efeat/ (cid:80) Mk = (cid:80) Mk S3/ (cid:80) Mk if epix,k > epix and efeat,k > efeat and < 0.1 then Pixel-level residual DINOv2 features Feature-level residual Average residuals over Stereo correspondence density over Average residuals over Mk Stereo correspondence density over Mk 0 for distractor Here, the stereo-based correspondence records the number of matches each pixel in the given image has, based on SIFT feature correspondences proposed in COLMAP [21]. pixel is considered valid correspondence (with the stereo correspondence map value set to true at the pixel location) if its match count exceeds threshold, indicating it likely belongs to static region. In contrast, distractors typically yield fewer matches due to their limited presence across images. In Algorithm 1, S3 denotes the stereo correspondence map, where pixel is considered valid correspondence if it has more than three matches."
        },
        {
            "title": "B Datasets and Metrics",
            "content": "We evaluate our method on three in-the-wild datasets with varying challenges, as shown in Table 5. NeRF On-the-go dataset [18] features indoor and outdoor sequences with consistent appearance but varying distractor ratios (5%30%). RobustNeRF dataset [19] provides indoor scenes with static geometry and controlled distractor placement (from single-type to 150 varied distractors), where training is done on cluttered views and testing on clean, unseen ones. We use the undistorted versions of these datasets, following the protocols of WildGaussian [11] and HybridGS [13]. The PhotoTourism dataset [6] includes landmark scenes (Brandenburg Gate, Sacre Coeur, Trevi Fountain) captured under diverse lighting, weather, and viewpoints, with both significant appearance variation and real-world distractors. We report PSNR, SSIM, and LPIPS [27] to assess reconstruction accuracy and perceptual quality."
        },
        {
            "title": "C Implementation Details",
            "content": "Our base model is built on Mip-Splatting [25]. Following its default settings, we recompute the sampling rate of each Gaussian every 100 iterations, with 2D Mip filter variance of 0.1 and 3D smoothing filter variance of 0.2. We train for 30,000 iterations on NeRF On-the-go and RobustNeRF, with densification and pruning every 1,000 steps until iteration 15,000; and for 100,000 iterations on 13 Table 5: In-the-wild 3D reconstruction datasets. # Train # Test Distractor Appear. change Dataset NeRF On-the-go [18] RobustNeRF [19] PhotoTourism [6] Scene Patio-high Spot Patio Corner Fountain Mountain Statue Android Yoda Crab Brandenburg Gate Sacre Coeur Trevi Fountain 763 830 1689 222 168 98 101 168 119 255 122 109 109 45 10 26 20 17 19 19 202 194 10 21 19 30% No 30% No 15%20% No 15%20% No 5%10% No 5%10% No 1 type 1 type 100 types 150 types 3.5% 3.5% 3.5% No No No No Yes Yes Yes PhotoTourism, with densification and pruning every 1,000 steps until iteration 50,000. We omit the opacity reset and apply 1,000-step warm-up before the mutual consistency regularization begins. The consistency regularization weight is set to 0.1. The learnable mask is optimized by loss weighted λmask = 1.0 with learning rate of 0.1. For EMA, we use smoothing factor of β = 0.8. Semantic regions for the multi-cue adaptive mask are generated using Semantic SAM [12] to create instance-level segmentations and apply Algorithm 1 to select distractor regions as masks. Additionally, we use 32-dimensional per-view appearance embedding and 24-dimensional perGaussian embedding. Color transformation is performed using three-layer MLP with hidden size 128, outputting scale and bias for each RGB channel. The learning rates are set to 0.001 for the per-view embedding, 0.005 for the per-Gaussian embedding, and 0.0005 for the MLP. The other 3DGS-related hyperparameters follow the heuristic setup shown in Table 6. Table 6: The other 3DGS-related hyperparameters. Parameter Value position_lr_init position_lr_final position_lr_delay_mult feature_lr opacity_lr scaling_lr rotation_lr percent_dense lambda_dssim densification_interval opacity_reset_interval densify_from_iter densify_grad_threshold 0.00016 0.0000016 0.01 0.0025 0.1 0.005 0.001 0.01 0.2 1000 No opacity reset 500 0."
        },
        {
            "title": "D More Results",
            "content": "NeRF On-the-go and RobustNeRF. In Table 1 and Table 8, our method (GS-GS) outperforms all baseline methods by more than 1 dB in scenes with medium to high occlusion ratios. The margin is smaller in low-occlusion scenes, where 3DGS-based methods already perform well due to strong geometric priors from the initial point cloud. similar trend is observed in Table 2: while the proposed method surpasses the SOTA by approximately 0.4 dB in simpler scenes containing single distractor type (e.g., Statue and Android), it outperforms others by more than 1 dB in complex scenes with large number of diverse distractors (e.g., Yoda and Crab). The rendering results in Figure 6 14 Table 7: The code repo and licenses. Method Link License 3DGS [8] Mip-Splatting [25] WildGaussians [11] NerfBaselines [10] COLMAP [21] Semantic-SAM [12] NeRF On-the-go dataset [18] RobustNeRF dataset [19] PhotoTourism dataset [6] https://github.com/graphdeco-inria/gaussian-splatting https://github.com/autonomousvision/mip-splatting https://github.com/jkulhanek/wild-gaussians/ https://github.com/nerfbaselines/nerfbaselines https://github.com/colmap/colmap https://github.com/UX-Decoder/Semantic-SAM https://github.com/cvg/nerf-on-the-go https://robustnerf.github.io/ https://github.com/ubc-vision/image-matching-benchmark Apache 2.0 License Custom Custom MIT License MIT License BSD License Apache 2.0 License Apache 2.0 License Custom Table 8: Quantitative results on the NeRF On-the-go dataset [18]. The best and second-best results are highlighted in bold and underline, respectively. Scene Method Mountain Fountain Corner Patio Spot Patio-High PSNR SSIM LPIPS PSNR SSIM LPIPS PSNR SSIM LPIPS PSNR SSIM LPIPS PSNR SSIM LPIPS PSNR SSIM LPIPS RobustNeRF [19] NeRF On-the-go [18] 3DGS [8] Mip-Splatting [25] WildGaussian [11] SLS-mlp [20] HybridGS [13] Ours (GS-GS) Ours (EMA-GS) 17.54 20.15 19.40 19.86 20.43 19.84 21.73 22.00 21.93 0.496 0.644 0.638 0.649 0.653 0.580 0.693 0.740 0. 0.383 0.259 0.213 0.200 0.255 0.294 0.284 0.199 0.162 15.65 20.11 19.96 20.19 20.81 20.19 21.11 21.83 21.61 0.318 0.609 0.659 0.672 0.662 0.612 0.674 0.717 0. 0.576 0.314 0.185 0.189 0.215 0.258 0.252 0.180 0.162 23.04 24.22 20.90 21.15 24.16 24.03 25.03 26.15 25.77 0.764 0.806 0.713 0.728 0.822 0.795 0.847 0.885 0. 0.244 0.190 0.241 0.230 0.045 0.258 0.151 0.085 0.089 20.39 20.78 17.48 18.31 21.44 21.55 21.98 22.97 22.87 0.718 0.754 0.704 0.639 0.800 0.838 0.812 0.860 0. 0.251 0.219 0.199 0.328 0.138 0.065 0.169 0.096 0.091 20.65 23.33 20.77 20.18 23.82 23.52 24.33 25.52 25.09 0.625 0.787 0.693 0.689 0.816 0.756 0.794 0.854 0. 0.391 0.189 0.316 0.338 0.138 0.185 0.196 0.135 0.152 20.54 21.41 17.29 18.31 22.23 20.31 21.77 23.17 23.14 0.578 0.718 0.604 0.639 0.725 0.664 0.741 0.796 0. 0.366 0.235 0.363 0.328 0.206 0.259 0.211 0.164 0.156 and 7 further demonstrate the superiority of our method, as competing approaches exhibit distractor remains and missing details. PhotoTourism. The Asymmetric Dual 3DGS achieves an average improvement of 0.8 dB on the PhotoTourism dataset  (Table 3)  , demonstrating its effectiveness under challenging appearance variations. Furthermore, proper appearance modeling is essential for handling in-the-wild data with diverse visual conditions. This is supported by significant performance gap of more than 4 dB between methods with and without appearance modeling, as shown in Table 3, and further illustrated by the visual differences in Figure 8. Therefore, we apply appearance modeling for the PhotoTourism dataset by default. As the importance of appearance modeling is addressed here, we omit further discussion in the following ablation section and apply appearance modeling by default for the PhotoTourism dataset. Table 9: Tuning the EMA smoothing factor according to the average performance on the NeRF On-the-go dataset [18]. β 0.5 0.6 0.7 0.8 0.9 PSNR SSIM LPIPS 22.80 22.93 23.12 23.40 23.05 0.797 0.797 0.799 0.801 0.798 0.136 0.137 0.136 0.135 0. Hyperparameters. We perform hyperparameter tuning on the NeRF On-the-go dataset [18] to optimize the performance of our method. As shown in Table 9, we tune the EMA smoothing factor β and find that β = 0.8 yields the highest PSNR and SSIM with the lowest LPIPS. In Table 10, we evaluate different densification intervals and observe that an interval of 1000 offers the best overall performance. Similarly, Table 11 presents the results of tuning the warm-up interval, where 1000 again emerges as the optimal choice, outperforming both shorter and longer intervals. Lastly, Table 12 shows that removing opacity reset improves reconstruction quality, suggesting that preserving learned opacity leads to more stable and effective training."
        },
        {
            "title": "E Limitations",
            "content": "We adopt the appearance modeling approach from WildGaussian [11], using per-view appearance embedding to control global appearance and per-Gaussian embedding to model the appearance of individual Gaussian primitives. However, this model struggles to capture fine-grained effects such as 15 Table 10: Tuning the densification interval according to the average performance on the NeRF On-the-go dataset [18]. Densification Interval PSNR SSIM LPIPS 500 1000 1500 2000 23.00 23.40 23.15 22.96 0.797 0.801 0.796 0.797 0.134 0.135 0.143 0.145 Table 11: Tuning the warm-up interval according to the average performance on the NeRF On-the-go dataset [18]. Warm-up Interval PSNR SSIM LPIPS 0 500 1000 1500 2000 22.96 23.08 23.40 23.10 22.88 0.798 0.799 0.801 0.799 0.798 0.135 0.134 0.135 0.135 0. object highlights. likely reason is the limited diversity in training data. To address this, we plan to introduce data augmentation with randomized illumination variations. Additionally, our current setup shares the same appearance model across both 3DGS models. We propose using separate appearance models for each 3DGS, allowing mutual learning to provide complementary signals that may improve appearance representation. We leave these as future work."
        },
        {
            "title": "F Social Impact",
            "content": "Notre-Dame de Paris suffered devastating fire in 2019. Although the building was severely damaged, restoration was aided by 3D model originally created for video game, highlighting the importance of preserving 3D models of cultural landmarks. However, such sites are often crowded with people, and photos taken at different times may exhibit varying lighting conditions. This highlights the broader societal benefit of accessible and robust 3D scene reconstruction technologies. Our method contributes positively by enabling the creation of high-quality 3D models from in-the-wild images, which are often affected by distractors and lighting variations. By making it feasible to reconstruct cultural landmarks from everyday photossuch as those shared onlineour approach supports digital preservation, education, and historical restoration efforts. There are potential negative impacts, such as misuse in surveillance or privacy-invading applications. In particular, in-the-wild image collections often contain individuals who are unintentionally captured. To mitigate this risk, we recommend removing or anonymizing identifiable information, such as faces or bodies, from the reconstructed scenes. This can be achieved through automated segmentation or masking techniques applied before or during training. 16 Table 12: Impact of opacity reset on reconstruction quality, evaluated on the NeRF On-the-go dataset [18]. Opacity Reset PSNR SSIM LPIPS w/ w/o 23.40 22.43 0.801 0.786 0.135 0.158 Mip-Splatting [25] HybridGS [13] Our (EMA-GS) Figure 6: Qualitative results on the NeRF On-the-go dataset [18]. The scenes shown are, from top to bottom: Patio-high (high occlusion), Spot (high occlusion), Patio (medium occlusion), Corner (medium occlusion), Mountain (low occlusion), and Fountain (low occlusion). Ours (GS-GS) Ground Truth 17 Mip-Splatting [25] HybridGS [13] Our (EMA-GS) Figure 7: Qualitative results on the RobustNeRF dataset [19]. The scenes shown are, from top to bottom: Statue, Android, Yoda, and Crab. Ours (GS-GS) Ground Truth Mip-Splatting [25] WildGaussian [11] Our (EMA-GS) Ours (GS-GS) Ground Truth Figure 8: Qualitative results on the PhotoTourism dataset [6]. The scenes shown are, from top to bottom: Sacre Coeur, Brandenburg Gate, and Trevi Fountain."
        }
    ],
    "affiliations": [
        "Department of Computing and Software McMaster University",
        "Department of Electrical and Computer Engineering McMaster University",
        "Xian Jiaotong University"
    ]
}