{
    "paper_title": "Unified all-atom molecule generation with neural fields",
    "authors": [
        "Matthieu Kirchmeyer",
        "Pedro O. Pinheiro",
        "Emma Willett",
        "Karolis Martinkus",
        "Joseph Kleinhenz",
        "Emily K. Makowski",
        "Andrew M. Watkins",
        "Vladimir Gligorijevic",
        "Richard Bonneau",
        "Saeed Saremi"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Generative models for structure-based drug design are often limited to a specific modality, restricting their broader applicability. To address this challenge, we introduce FuncBind, a framework based on computer vision to generate target-conditioned, all-atom molecules across atomic systems. FuncBind uses neural fields to represent molecules as continuous atomic densities and employs score-based generative models with modern architectures adapted from the computer vision literature. This modality-agnostic representation allows a single unified model to be trained on diverse atomic systems, from small to large molecules, and handle variable atom/residue counts, including non-canonical amino acids. FuncBind achieves competitive in silico performance in generating small molecules, macrocyclic peptides, and antibody complementarity-determining region loops, conditioned on target structures. FuncBind also generated in vitro novel antibody binders via de novo redesign of the complementarity-determining region H3 loop of two chosen co-crystal structures. As a final contribution, we introduce a new dataset and benchmark for structure-conditioned macrocyclic peptide generation. The code is available at https://github.com/prescient-design/funcbind."
        },
        {
            "title": "Start",
            "content": "Unified all-atom molecule generation with neural fields Matthieu Kirchmeyer1, Pedro O. Pinheiro1, Emma Willett1 Karolis Martinkus1, Joseph Kleinhenz1 Emily K. Makowski2 Andrew M. Watkins1 Vladimir Gligorijevic1 Richard Bonneau1 Saeed Saremi 5 2 0 2 9 1 ] . [ 1 6 0 9 5 1 . 1 1 5 2 : r 1Prescient Design, Genentech 2Antibody Engineering, Genentech"
        },
        {
            "title": "Abstract",
            "content": "Generative models for structure-based drug design are often limited to specific modality, restricting their broader applicability. To address this challenge, we introduce FuncBind, framework based on computer vision to generate targetconditioned, all-atom molecules across atomic systems. FuncBind uses neural fields to represent molecules as continuous atomic densities and employs scorebased generative models with modern architectures adapted from the computer vision literature. This modality-agnostic representation allows single unified model to be trained on diverse atomic systems, from small to large molecules, and handle variable atom/residue counts, including non-canonical amino acids. FuncBind achieves competitive in silico performance in generating small molecules, macrocyclic peptides, and antibody complementarity-determining region loops, conditioned on target structures. FuncBind also generated in vitro novel antibody binders via de novo redesign of the complementarity-determining region H3 loop of two chosen co-crystal structures. As final contribution, we introduce new dataset and benchmark for structure-conditioned macrocyclic peptide generation*."
        },
        {
            "title": "Introduction",
            "content": "A central challenge in drug discovery is designing molecules that bind specifically to target protein [1]. This task involves navigating diverse landscape of molecular modalities, from small organic compounds to large biomolecules, each with unique chemical properties. Structure-based approaches are frequently employed to meet this challenge, utilizing the 3D structure of target site (often an accessible protein region) to generate novel molecules with high affinity. Generative models are emerging as powerful data-driven alternative to established traditional techniques such as virtual screening and physics-based simulations. These newer models can explore vast chemical spaces more effectively to identify molecules with desired binding properties [2]. Most structure-based generative models specialize on single molecule modality to better account for specific physicochemical properties. Focusing on single molecular modality also simplifies data gathering and augmentation, training, representation choice, and metrics used for validation. Generative models of small molecules typically represent molecules as point cloud of atoms [3, 4, 5] or discretized atomic densities [6, 7]. Most protein generative models leverage the fact that proteins are sequences of amino acids to represent them with point clouds of residues, where each residue contains multiple atoms [8], recovering their sequences with, e.g., co-generation [9] or inverse folding [10]. Many protein-centric models also rely on large sequence databases and self-supervised generative models for sequence that can help in scoring and generating/proposing mutations. *The code is available at https://github.com/prescient-design/funcbind. The checkpoints at https://huggingface.co/mkirchmeyer/funcbind/. Equal contribution, work done at Genentech. Correspondence to matthieu.kirchmeyer@gmail.com, pedro@opinheiro.com, saremi.saeed@gene.com 39th Conference on Neural Information Processing Systems (NeurIPS 2025). Domain-specific representations limit generalization, as models are not transferable from one modality to another. This narrow focus also limits utility, as most key applications involve interfaces and catalysis across multiple modalities. We argue that modality-agnostic representations are better suited to wide range of tasks and can learn physical properties across diverse atomic systems, thus leveraging more training data and more challenging metrics. These representations are more expressive [11] as evidenced by the successes of cross-modality models on structure prediction [12, 13], inverse folding [14] or molecular interaction prediction [15]. Here, we introduce FuncBind, unified and scalable framework for generating all-atom molecular systems conditioned on target structures. Following Kirchmeyer et al. [16], we represent molecules with neural fields: functions that map 3D coordinates to atomic densities. Neural fields are compact and scalable alternative to voxels, while sharing many common advantages over point cloud-based representations: they (i) are compatible with expressive neural network architectures (such as CNNs and transformers), (ii) account for variable number of atoms and residues implicitly, and (iii) can represent molecules across modalities with an all-atom formulation. Using this new representation, we build latent conditional generative model compatible with any score-based approaches. We tested denoising diffusion [17] and walk-jump sampling (WJS) [18], sampling approach that enjoys fast-mixing and training simplicity when compared to diffusion models, largely because it only relies on one or few noise levels. We train FuncBind on structures from three drug modalities: small molecules, macrocyclic peptides (MCPs), and antibody complementarity-determining region (CDR) loops in complex with target protein. These modalities encompass range of chemical matter with challenging constraints, such as cyclic backbones and non-canonical amino acids. FuncBind achieves competitive results on in silico benchmarks, matching or outperforming modality-specific baselines. On in vitro experiments, we show that FuncBind can produce novel antibody binders by redesigning the CDR H3 loop of two chosen co-crystal structures. We also create new dataset, containing 190,000 synthetic MCP/protein complexes derived from 641 RCSB PDB structures [19], particularly relevant for this work, as cyclic peptides exhibit chemistry and function that span small and large molecule modalities."
        },
        {
            "title": "2 Related work",
            "content": "Pocket-conditioned small molecule generation. Several approaches have framed structure-based drug design as generative modeling problem [20]. These methods commonly represent molecules including both ligands and targetseither as point clouds of atoms or as voxel grids. Point-cloud approaches represent atoms as points in 3D space, along with their atomic types, and typically use graph neural network architectures. Point-cloud approaches have been used to generate molecules using autoregressive models [21, 22, 3], iterative sampling approaches [23, 24, 25], normalizing flows [26], diffusion models [27, 4, 28], and Bayesian flow networks [5]. Voxel-based approaches map atomic densities to 3D discrete voxel grids and apply computer vision techniques for generation [29, 6, 30, 31]. VoxBind [7] demonstrates that voxel-based representations achieve state-of-the-art results using expressive vision-based networks and score-based generative models. However, raw voxel-based models do not currently scale to larger molecules due to high memory requirements. Neural fields serve as the continuous analogue to discrete voxels, technique widely adopted in 3D computer vision [32]. When applied to molecular generation, these fields match existing performance levels while demonstrating superior memory and computational efficiency [16]. Antigen-conditioned CDR loop generation. Antibody design is an active research topic and antibody-based treatments represented 26% of 2024 FDA approvals [33] (with related biologics approvals an even higher fraction). key line of antibody engineering work re-designs the complementarity-determining regions (CDRs), subset of the heavy and light chains that totals 48 to 82 residues [34] and represents most of the proteins affinity determining variability. recent approach is to co-design the CDRs sequences and structures using residue cloud representations and equivariant graph neural networks [35, 36, 37, 38], combining these representations with diffusion models for generation [39, 40]. Other works leverage protein language models [41, 42], or revisit the problem by proposing new representations that incorporate domain knowledge and physics-based constraints [43]. As is the case for most models with demonstrated redesign capabilities, we tackle the task where the pose (docking) of the framework is provided. This assumption is relaxed in [44], available at https://huggingface.co/datasets/Willete3/mcpp_dataset 2 Figure 1: Neural field architectures. (a) Architecture used in [16] where global embedding is used as input to the neural field decoder. (b) Our proposed neural field architecture, where embeddings are spatially arranged into feature map grid. The latter allows us to better capture local signal information from input space and is compatible with expressive architectures for denoising. where the authors finetuned RFDiffusion [45] to generate the positions of backbone CDR atoms, followed by an inverse folding step [10]. Unlike other methods, FuncBind is the first approach based on neural fields that is also applicable to different data modalities simultaneously. Pocket-conditioned macrocyclic peptide generation. Occupying unique chemical space between small and large molecules, cyclic peptides are rapidly growing class of therapeutics that can access biological targets often challenging for both small molecules and antibodies [46]. De novo generation of target-specific cyclic peptides is therefore highly desirable, in part due to the power and utility of high throughput screens for cyclic and linear peptides and peptoid binders, yet only few works have investigated this. Rettie et al. [47] propose an approach based on RFDiffusion [45] that successfully designed cyclic peptides conditioned on target protein structure. Yet this prior method is unable to handle non-canonical amino acids, an essential component for both compatibility with industry standard high-throughput screens and for enhancing the therapeutic properties of these peptides. The work by Tang et al. [48] is the only work we are aware of that can handle non-canonical amino acids; this model operates on tokenized smiles and performs target conditioning via classifier guidance with ML-based property predictors that are known to generalize poorly out-of-distribution [49]."
        },
        {
            "title": "3 Method",
            "content": "FuncBind is latent score-based generative model that consists of two training steps: we first learn latent representation for each molecule that modulates the parameters of neural field decoder (Section 3.1), then we train conditional denoiser on these latents (Section 3.2.1). The denoiser is used to sample molecules, conditioned on given target (Section 3.2.2). 3.1 Neural field-based latent representation We consider dataset of molecular complex tuples = {(v, vtar, c)i}N i=1, where and vtar are the binder and target, respectively, and is the modality of the binder. In this work, we focus on three modalities: small molecules, macrocyclic peptides, and CDR loops, though the framework accommodates any atomic system. Atoms are represented as continuous Gaussian-like densities in 3D space and molecules as functions mapping coordinates to n-dimensional atomic occupancy values, : R3 [0, 1]n (where is the number of atom types) [50, 51, 52]. See Section for details. Similar to [16], an encoder embeds molecule into latent z, which is used to decode back an atomic density field. Decoding consists in modulating the parameters of shared neural field decoder based on the latent representation. However, instead of representing the latent with global embedding (Figure 1a) used in [16], we consider spatially arranged feature map (Figure 1b). This approach has been successfully applied in other domains [53, 54, 55] and provides two key advantages: (i) each spatial feature captures local information helping to scale the model to larger molecules, (ii) it is compatible with expressive architectures (e.g. U-Nets [56]) for denoising. The encoder Eψ : Rn Rd, = L3, is 3D CNN parameterized by ψ that maps voxel grid Gv, generated by discretizing at fixed low resolution (for computational efficiency) set by the integer L, into latent space with channels. For decoding, we use nearest neighbor interpolation as in [54]: from the feature map z, we extract position-dependent vectors zx RC. The embedding zx is constant over 3D patch in coordinate space. The decoder Dϕ : RC R3 Rn, parameterized L3 3 Figure 2: Conditional denoiser training overview. We voxelize separately the binder and the target vtar of given complex and encode them into z, ztar using encoders Eψ, Eψ, respectively. We train denoiser ˆzθ(y ztar, σ, c) to remove the noise from conditioned on ztar, the noise level σ and the one-hot modality class (e.g. cyclic peptide). The denoised latent representation is fed into neural field decoder Dϕ; this gives reconstructed field ˆv. ˆv undergoes some additional postprocessing to recover the bonds and residue identities (if applicable); see Section 3.3. by ϕ, then computes the molecular density field at coordinates R3, given local modulation embedding zx RC. We use conditional neural field based on multiplicative filter networks [57, 58] with Gabor filters, natural choice for modeling the sparse atomic density fields [16]. The neural field is trained across modalities with the objective proposed in [16]. KL-regularization term [59] is added, following common practice in latent generation [60]: LAE(ψ, ϕ) = (cid:88) (cid:20)(cid:90) qψ(z v) Dϕ(x, z) v(x)2 2 dx (cid:21) + β KL (cid:0)qψ(z v) N(z; 0, Id)(cid:1) , (1) where qψ(z v) = N(z; µ(v), diag(σ(v))2Id), µ(v) and σ(v) are parameterized by Eψ and β is regularization weight. As [16], when optimizing for Equation (1), we upsample coordinates close to the center of each atom to focus training on non-empty spaces. Since our model does not have equivariance built in the architecture, we apply data augmentation (translation and rotation). v 3.2 Conditional latent score-based generation We train conditional denoiser on the neural-field based representations (Section 3.2.1). The denoiser is used to sample molecules from the aggregate posterior of the VAE encoder [61], conditioned on given target, with conditional diffusion and walk-jump sampling (Section 3.2.2). 3.2.1 Conditioned denoiser Our denoiser takes as input noisy latent representation and set of conditioning information, and outputs the clean version of the latent representation. In this work, we condition the denoiser on three signals: (i) the target structure vtar, (ii) the molecule modality and (iii) the noise level σ. More formally, let (v, vtar, c) be (binder, target, modality) tuple from the dataset, (z, ztar) := )) their latent representations and = + σε, ε N(0, Id), noisy version of (Eψ(Gv), Eψ(Gtar z. The target encoder Eψ is 3D CNN with similar architecture as Eψ but different parameters ψ. Following the preconditioning pre-processing proposed by [62], our denoiser ˆzθ is defined as: σ σ2 + 1 where Uθ is neural network parameterized by θ and the embeddings and ztar are normalized to unit variance and zero mean per channel, similar to [60]. Figure 2 shows an overview of the model architecture. The spatial structure of the latent space allows us to model Uθ with 3D U-Nets, standard architecture for generative models in computer vision. In particular, we adapt the network of Karras et al. [62]designed to generated 2D imagesto our 3D generation setting. Crucially, similar to recent works [52, 7, 63], we do not use any type of SE(3) equivariance constraints. Instead, we replace these constraints with data augmentation (rotations and translations) during training. ˆzθ(y ztar, σ, c) = 1 σ2 + 1 1 σ2 + 1 log(σ), y, ztar, + 1 4 Uθ (cid:19) (cid:18) , The conditional denoiser is trained by minimizing the following loss at given noise level σ: (cid:104)(cid:13) (cid:13)ˆzθ(z + σε ztar, σ, c) (cid:105) (cid:13) 2 (cid:13) 2 , (2) Lσ(θ, ψ) = (v,vtar,c) D, qψ(z v), ε N(0,Id) where ztar = Eψ(Gtar reweighting scheme in [62] across noise levels, i.e.: ) is the encoding of the low-resolution voxel of the target. We apply the Ldenoiser(θ, ψ) = σ (cid:34) σ2 + 1 σ2 1 eu(σ) p(σ) (cid:35) Lσ(θ, ψ) + u(σ) , where σ is sampled along some pre-determined distribution p(σ) (see Section 3.2.2) and u(σ) is one-layer MLP trained with the denoiser. This effectively reweights the loss based on the noise level. 3.2.2 Sampling strategies We experimented with various score-based sampling strategies in the conditional setting, including the SDE formulation of denoising diffusion [64, 65], widely recognized for its state-of-the-art performance in image generation and walk-jump sampling (WJS), based on probabilistic formulation of least-squares denoising [18]. Diffusion models operate over continuous range of noise levels in contrast to WJS which considers only one noise level. These models rely on the Tweedie-Miyasawa formula (TMF) [66, 67], which relates the least-squares denoiser at noise level σ with the score function at the noise level. Given = + σε, ε N(0, Id), the conditional extension of TMF was derived in [7]. In our notation, it takes the form: log p(y ztar, σ, c) sθ(y ztar, σ, c) := (ˆzθ(y ztar, σ, c) y)/σ2, (3) where ˆzθ is the minimizer of Equation (2); sθ(y ztar, σ, c) is the learned conditional score function. For diffusion, we follow [65] and generate samples by numerically integrating the reverse-time SDE from noise level σmax to σmin, approximating the score function with the learned denoiser ˆzθ(y ztar, σ, c) and TMF. We adopt the variance exploding formulation and the stochastic SDE sampler from EDM [62]. For WJS, we proceed as in Section B.4.2 and report the results on CDR H3 redesign in Section C.1.2. 3.3 Recovering molecules from generated atomic-density fields To recover the underlying molecular structures from sampled latent codes z, we employ postprocessing pipeline inspired by [16]. The initial phase determines atom coordinates by identifying local optima in the neural field. This is achieved by first rendering the latent code into 0.25Å resolution voxel, then performing peak detection with MaxPooling filters, and finally refining the coordinates through gradient ascent, which takes advantage of the neural fields differentiability. The second phase involves inferring bonds and, when applicable, amino acid identities from the generated point cloud (coordinates and atom types) using OpenBabel software [68]. This yields .sdf files for molecules and peptides and .pdb files for proteins. specific approach for identifying non-canonical amino acids, which are not recognized by OpenBabel, is described in Section D."
        },
        {
            "title": "4 Experiments",
            "content": "We test our model on the following in silico settings, covering the three modalities discussed above: (i) small molecule generation conditioned on protein pocket (Section 4.1); (ii) antibody CDR loops redesign conditioned on an epitope (Section 4.2); and (iii) macrocyclic peptides generation conditioned on protein pocket (Section 4.3). We also performed in vitro validation of antibody CDR loops redesign conditioned on an epitope. For these tasks, the neural field is jointly trained on all three modalities. We train 5B parameter model across modalities. Samples are generated via conditioning on the target structure. Note that our network is significantly larger than alternative models; we have observed improved performance in the unified setting for larger networks. See Section for additional model details and Figure 3 for qualitative samples. We compared our unified model against specialized models trained independently for each modality. Overall, performance parity was observed across most metrics, with the key exception being uniqueness, which was significantly higher in the unified model. See Section C.1.3 for comparison on CDR H3 inpainting. Further research exploring transfer learning across broader set of modalities represents an exciting avenue for future work. 5 Figure 3: Examples of generated molecules given target structure for different modalities: (top) small molecules against 2rma, (middle) macrocyclic peptides against 5ooc and (bottom) CDR H3 loop against 5tlk. The seed binders are shown on the right. 4.1 Small molecule generation Data. We consider the standard CrossDocked2020 [69] benchmark, with the pre-processing and splitting strategy of [70]. Pockets are clustered at sequence identity of < 30% using MMseqs2 and are split into 99,900 train ligand pockets pairs, 100 validation pairs and 100 test pairs. Baselines. We compare FuncBind to various pocket-conditioned ligand generative models: these include point cloud approaches based on autoregressive models (AR [21] and Pocket2Mol [3]), diffusion (DiffSBDD [27], TargetDiff [4], DecompDiff [28]), Bayesian Flow networks (MolCraft [5]) and voxel-based approach based on walk-jump sampling (VoxBind [7]). FuncBind can be seen as more scalable generalization of VoxBind and closely matches its performance. All methods but DecompDiff and MolCraft rely OpenBabel [68] to assign bonds from generated atom coordinates. Metrics. We evaluate performance using similar metrics as previous work [4]. For each method, we sample 100 ligands per pocket. We measure affinity with three metrics using AutoDock Vina [71]: VinaScore is the docking score of the generated molecule, VinaMin is the docking score after local energy minimization, VinaDock fully re-docks the generated molecule, including both search and scoring steps. We also compute the drug-likeness, QED [72], and synthesizability, SA [73], score of the generated molecules with RDKit [74]. Diversity is the average Tanimoto distance (in RDKit fingerprints) per pocket across pairs of generated ligands [75]. # atoms is the average number of (heavy) atoms per molecule. We also compute the PoseCheck metrics [76]: Steric clash computes the number of clashes between the generated ligands and their pockets, Strain energy (SE) measures the difference between the internal energy of the generated molecules pose (without pocket) and relaxed pose (computed using Universal Force Field [77] within RDKit); we report the median value. Results. The results are reported in Table 1. FuncBind is competitive with the current state of the art, slightly underperforming on docking-related metrics and strain energy compared to VoxBind and Molcraft and on number of clashes compared to VoxBind. This experiment demonstrates FuncBinds ability to generate highly-variable small molecules. Next, we demonstrate that it can also handle the more regular structures of amino acid-based molecules. 4.2 Antibody CDR redesign Data. We consider the SabDab dataset [78], which comprises antibody-protein co-crystal structures and the data splits from DiffAb [39]. This non-i.i.d. split ensures that antibodies similar to those of the test set (i.e.more than 50% CDR H3 identity) are removed from the training set. The test split includes 19 targets, for which we redesign each CDR loop individually. As our baselines, we consider the Chothia numbering scheme [79] for the CDR definition. Baselines. We compare FuncBind to representative baselines: RAbD [80], Rosetta-based method and two ML-based models, DiffAb [39] and AbDiffuser [43]. We consider the variation of AbDiffuser 6 Table 1: Results on CrossDocked2020 test set. / denote that higher/lower average (Avg.) or median (Med.) is better. For # atoms, numbers close to Reference are better. Baseline results are from [7, 28]. FuncBinds results are shown with mean/standard deviation obtained over 1,000 bootstraps. VinaScore VinaMin VinaDock QED SA Div. S.E. Clash #atoms Avg. Med. Avg. Med. Avg. Med. Avg. Avg. Avg. Med. Avg. Reference -6.36 -6.46 -6.71 -6.49 -7. -7.26 AR Pocket2mol DiffSBDD TargetDiff DecompDiff MolCraft VoxBind FuncBind -5.75 -5.14 -1.94 -5.47 -5.67 -6.59 -6.94 -5.71 (.03) -5.64 -4.70 -4.24 -6.30 -6.04 -7.04 -7. -5.64 (.03) -6.18 -6.42 -5.85 -6.64 -7.04 -7.27 -7.54 -6.34 (.03) -5.88 -5.82 -5.94 -6.83 -7.09 -7.26 -7.55 -6.18 (.03) -6.75 -7.15 -7.00 -7.80 -8.39 -7.92 -8. -7.26 (.03) -6.62 -6.79 -6.90 -7.91 -8.43 -8.01 -8.41 -7.28 (.03) .48 .51 .56 .48 .48 .45 .50 .57 . .63 .74 .58 .58 .61 .69 .70 - .70 .69 .73 .72 .68 .72 .73 .50 (.002) .65 (.001) .70 (.0) 103 595 206 1193 1243 N/A 195 162 217 (12) 4.7 4.2 5.8 15.4 10.8 7.1 7.1 5.1 7.4 (.06) Avg. 22.8 17.6 17.7 24.0 24.2 20.9 22.7 23.4 19.0 (.09) with side chain generation to better match FuncBinds all-atom setting; AbDiffuser in contrast to other baselines, generates all 6 loops jointly. We also compare to AbX [81] and the reproduction of dyMEAN [38] from [81] for H3 design, where the DiffAb splits were considered. Metrics. We compute the following metrics, measuring the similarity of the generated designs to the seed: amino acid recovery (AAR), the sequence identity between the seed and the generated CDRs; RMSD, the Cα root-mean-square deviation between the seed and generated structure and IMP, the percentage of designs with lower binding energy (G) than the seed, as calculated by InterfaceAnalyzer in Rosetta [80]. Baselines apply Rosetta-based relaxations prior to computing IMP to improve the energy scores: DiffAb refines the generated structure with OpenMM [82] and AbX uses fast-relax [80]. We report metrics for 100 generated samples per target. Note that our model, unlike most baselines, generates samples with diverse sequence lengths; to compute these metrics we consider samples with the same length as the original seed. We found that uniqueness impacts AAR and RMSD, particularly for non-H3 loop designs which exhibited low uniqueness. This presents challenge for fair model comparison, as baselines do not report uniqueness. For completeness, we also report the metrics for the unique samples on Table 4 (Section C.1). Results. The results in Table 2 indicate that our model is state of the art both on amino acid recovery (AAR) and Cα RMSD values, outperforming other baselines by 1.5 to 3 times across all loops. This performance can be attributed to our all-atom formulation and neural-field representation, which enable the model to better capture the molecular conformation and conditioning context. AbDiffuser also leverages side-chain information but underperforms in RMSD, highlighting the distinct advantages of our approach. Finally, FuncBinds interface energy improvement (IMP) without backbone minimization is competitive to the IMP of approaches that apply minimization. This showcases the quality and fidelity of the generated structures, as energy is very sensitive to wrong atom placement. As the baselines [81, 39], when applying Rosettas fast-relax backbone minimization on the generated loops, IMP greatly improves as expected, outperforming even the Rosetta RAbD protocol that directly optimizes the energy function. This refinement procedure slightly increases RMSD, as it changes the loop to minimize strain, while FuncBind is trained to mimic patterns in ground-truth crystal structures. Length distributions generated. The above evaluation restricted designs to the seeds length, common prior in many generative models for this task. However, in many settings, we do not know what is reasonable length. FuncBind is designed to sample designs across various lengths, useful capability for de novo CDR generation. To demonstrate this flexibility, we analyzed histograms of sequence lengths and atom counts for CDR H3s designed for de novo target, comparing them against the original seeds values (see Figure 4). For this specific target, while the generated designs exhibited range of lengths, their distributions were centered on the seeds reference values. Further validation of designs with other lengths is left for future research. 7 Table 2: CDR inpainting on SAbDab [78] with DiffAb splits [70]. RMSD is in Å and AAR, IMP are in %. indicates additional relaxation / optimization with Rosetta. Method L1 AAR RMSD IMP AAR RMSD IMP RAbD DiffAb AbDiffuser FuncBind RAbD DiffAb AbDiffuser FuncBind RAbD DiffAb dyMEAN AbX AbDiffuser FuncBind 22.9 65.8 76.3 86.9 25.5 49.3 65.7 78. 22.1 26.8 29.3 30.3 34.1 47.5 2.26 1.19 1.58 43.9 53.6 - 0.41 / 0.44 35.0 / 77.2 H2 1.64 1.08 1. 53.5 29.8 - 0.52 / 0.54 31.7 / 61.4 H3 2.90 3.60 4.80 3.41 3.35 23.3 23.6 5.26 42.9 - 2.04 / 2.10 19.4 / 49.9 34.3 55.7 81.4 86.4 26.3 59.3 83.2 86.2 20.7 46.5 - - 73.2 80.8 1.20 1.39 1.46 46.8 45.6 - 0.68 / 0.73 45.0 / 80.4 L2 1.77 1.37 1.40 56.9 50.0 - 0.83 / 0.84 39.5 / 66.0 L3 1.62 1.63 - - 1. 55.6 47.3 - - - 0.68 / 0.73 32.7 / 67.5 Figure 4: CDR H3 length (left) and atom count (right) histogram on the de-novo 4cni target. Red is the seed H3s reference numbers. In vitro evaluation. We performed wet-lab validation of H3 loop redesigns based on the co-crystal structure of an antibody bound to rigid and flexible epitope. We selected the H3 loop for its important contribution to the antibodys functional properties. We consider de novo setting, where interfaces similar to the two complexes, identified using Ab-Ligity [83], were excluded from training. From an initial pool of 10,000 unique generated H3 designs (all matching the original seeds length), 190 were selected for experimental testing. This selection involved two steps: 1) The top 500 designs were shortlisted based on model confidence, as indicated by their repeated generation counts. Our in silico validation showed that repeats is an useful proxy for high amino acid recovery of the seed. 2) These 500 designs were then clustered into 190 groups using weighted K-means based on sequence edit distance, where the weights were defined by the repeat generation count. The design with the highest repeat count (highest confidence) from each of these 190 clusters was chosen for synthesis and characterization. The selected antibody designs were expressed and purified in the wet lab. Binding affinity was then determined using surface plasmon resonance (SPR) measurements. Section C.2 presents some detailed analysis. FuncBind achieves binding rate of 45% on the rigid epitope and 2% on the flexible epitope, which increases to 4% with relaxed binding threshold. For legal reasons, we do not disclose the targets names. 8 Table 3: Results on our MCP benchmark. RMSD is in Å, Residues-TS0.5, Vina Dock are in %. TS Residues-TS0.5 L-RMSD I-RMSD TM-Score Vina dock RFPeptide AfCycDesign FuncBind 0.31 0.34 0. 29 29 25 12 7.6 2.6 3.3 3.7 1.8 0.33 0.33 0.36 8.8 29 41 4.3 Macrocyclic peptide generation Data. Given the scarcity of established baselines, benchmarks, and available data for macrocyclic peptide (MCP)-protein complexes, we introduce novel benchmark to facilitate the evaluation of generative models for MCPs. To address the data limitation, we have curated dataset of 186,685 MCP-protein complexes using mutate then relax strategy detailed in Section E. Taking as input an original set of 641 protein-MCP complexes sourced from RCSB PDB [19], this strategy consists in (i) randomly mutating the MCPs at 1 to 8 different sites, using list of 213 distinct amino acids, (ii) relaxing them using fast-relax, which involves iterative cycles of side-chain packing and all-atom minimization [84] and (iii) selecting the lowest interface scores. The source dataset comprises lengths ranging from 4 to 25 amino acids with an average of 10 (Section Figure 13a). 78% of the MCPs contain one or more non-canonical amino acids, i.e. any amino acid that is neither L-canonical nor D-canonical. We split the dataset into train, test and validation subsets using clustering approach detailed in Section that aims at creating non-i.i.d. test set consisting of 85 protein pockets. Baselines. MCPs pose significant challenges for generative models due to their non-canonical amino acids, cyclization, and scarce training data. To our knowledge, no other target-conditioned, structurebased MCP generative models handles non-canonical amino acids, precluding direct comparisons. For reference, we compare nonetheless FuncBind with AfCycDesign [85] and RFPeptide [47], two models generating MCPs exclusively with canonical amino acids and N-to-C cyclization. Metrics. As part of this new benchmark, we define and compute relevant metrics. Tanimoto similarity (TS) assesses the resemblance between the ground-truth seed and the sampled MCP structure. Ligand RMSD (L-RMSD) is the RMSD between sample MCP to seed MCP, and template modeling (TM) score, length independent similarity metric, based on the Kabsch alignment of the backbone atoms (N, Cα, C, O). TM score was calculated by maximizing the scaling factor. The same backbone logic is applied to compute interface RMSD (I-RMSD), the RMSD in the pocket (which are for the most part slightly lower since the pockets are identical). The sample and the seed were not aligned for I-RMSD since this is based on where the MCPs are in the pocket. Binding affinity was calculated through Autodock Vina [71]. Results. We observe correlation between the generated designs with the MCP seeds. Qualitatively, Section E.1 Figure 10 illustrates the close alignment of the backbone between the sampled structures and the seed. Most of the sampled molecules display consistent repeating peptide bonds, linking the C1 carbon of one α-amino acid to the N2 nitrogen of the next. Closure bonds (such as disulfide in Figure 10a,b and to cyclization in Figure 10c) are also often maintained in the sampled sets. Metrics are reported in Table 3. The mid-range TS and TM scores reflects strong similarity to the peptide backbone, with variability occurring at the functional groups of the residues. An example of per-residue TS for molecules sampled with the seed mutant (Section E.1 Table 10a) and the crystal MCP (Section E.1 Table 10b) shows that the highest TS occurs at the disulfide closure bond. This elevated per-residue similarity results from the preservation of closure bond residues throughout the curated dataset. Furthermore, the per-residue TS is higher across the crystal MCP residues than at the mutated residues of the seed mutant (PRO3A20 and GLU4B60). Because all mutants originate from the crystal MCP, the dataset is closely tied to the crystal sequence, and the sampled structures similarly reflect this connection. Low RMSD results show generally good alignment with the seed MCP, with many samples exhibiting RMSDs below 1Å, particularly for I-RMSD. These results are consistent with the TM scores, where 20% of the samples exhibit TM scores greater than 0.5. Finally, Autodock Vina binding affinity reveals that nearly half of the generated samples, both before and after minimization, have better binding affinity in the pocket compared to the seed. Figure 5: Per-residue energy scores at the same position were calculated using Rosettas residue energy breakdown for seed and two samples. We analyzed: (a) the seeds serine, (b) 3-hydroxycyclopentylalanine (C1O) from sample 11 (Section E.1 Figure 12), (c) tyrosine from sample 35. Compared to the baselines, FuncBind achieves superior or similar metrics, notably lower L/I-RMSDs. FuncBind also yields the highest proportion of designs with better docking scores. The only underperformance was in Tanimoto similarity (Residues-TS0.5 in particular), expected as FuncBind accesses larger set of non-canonical amino acids and sequence lengths as opposed to these baselines. We encourage future comparisons on this new benchmark, especially for models handling non-canonical amino acids. Analysis of generated non-canonicals. In Figure 11 (Section E.1), amino acids are categorized into known canonical and non-canonical amino acids (seen in the training set), and unknown noncanonical amino acids, which represent newly generated amino acids not previously seen. Fewer than 1% of all categorized amino acids were labeled as unreasonable, designation applied when bond was shorter than 0.8Å or when invalid oxygenoxygen or nitrogennitrogen bonds were present. Some reasonable and novel generated amino acids are presented in Figure 12 (Section E.1). Generating novel, chemically plausible amino acids without restrictions from predefined library or initial cyclic backbone allows broader exploration of the binding pocket. This is demonstrated in Figure 5b, where an amino acid absent from our library interacts with pocket residues that neither the seed (Figure 5a) nor chemically similar amino acid at the same position (Figure 5c) engage. The absence of constraints in MCP generation promotes greater sequence diversity and deeper investigation of the binding pocket."
        },
        {
            "title": "5 Conclusion",
            "content": "We presented FuncBind, new framework for all-atom, structure-conditioned de novo molecular design. FuncBind is based on new modality-agnostic representation, that enables single model to be trained across diverse drug modalities; we focused on small molecules, macrocyclic peptides and antibody CDRs. FuncBind handles variable atom and residue counts and is based on recent advancements in computer vision, replacing equivariance constraints with data augmentation. FuncBind demonstrates competitive in silico performance, matching or outperforming specialized baselines. In vitro, we demonstrate that FuncBind generates binders against de novo targets. It generates novel and chemically plausible molecules, including new non canonical amino acids. Future directions include extending FuncBind to larger biomolecular systems and to more data modalities. Furthermore, the scaling behaviour of this model remains an interesting direction for future study, particularly given the absence of overfitting as the denoiser increased in size (we tested up to 5B parameters). It is important to note that, like other structure-based methods, FuncBind relies on the availability of an accurate model of the molecular interface to be designed. This can be limitation, as these models are costly to obtain and their availability is often restricted in the drug discovery process, particularly for large molecules. Finally, real-world application of generative models in drug design requires addressing range of properties beyond binding, e.g. synthesizability for small molecules and developability for antibodies, considerations not handled in this work. Acknowledgements We thank Prescient Design and the following colleagues: Jan Ludwiczak for processing the SabDab dataset. Tamica DSouza for performing the antibody wetlab validation. 10 Max Shen, Namuk Park, Nathan Frey, Sidney Lisanza, Rob Alberstein, Ewa Nowara, Natasa Tagasovska, Chen Cheng, Pan Kessel, Sarah Robinson, Joshua Yao-Yu Lin for insightful discussions and Genentechs legal team."
        },
        {
            "title": "References",
            "content": "[1] Amy Anderson. The process of structure-based drug design. Chemistry & biology, 2003. (cit. on p. 1) [2] Morgan Thomas, Andreas Bender, and Chris de Graaf. Integrating structure-based approaches in generative molecular design. Current Opinion in Structural Biology, 2023. (cit. on p. 1) [3] Xingang Peng, Shitong Luo, Jiaqi Guan, Qi Xie, Jian Peng, and Jianzhu Ma. Pocket2mol: Efficient molecular sampling based on 3D protein pockets. In ICML, 2022. (cit. on pp. 1, 2, and 6) [4] Jiaqi Guan, Wesley Wei Qian, Xingang Peng, Yufeng Su, Jian Peng, and Jianzhu Ma. 3D equivariant diffusion for target-aware molecule generation and affinity prediction. ICLR, 2023. (cit. on pp. 1, 2, and 6) [5] Yanru Qu, Keyue Qiu, Yuxuan Song, Jingjing Gong, Jiawei Han, Mingyue Zheng, Hao Zhou, and Wei-Ying Ma. Molcraft: Structure-based drug design in continuous parameter space. ICML, 2024. (cit. on pp. 1, 2, and 6) [6] Matthew Ragoza, Tomohide Masuda, and David Ryan Koes. Generating 3D molecules conditional on receptor binding sites with deep generative models. Chemical science, 2022. (cit. on pp. 1 and 2) [7] Pedro Pinheiro, Arian Jamasb, Omar Mahmood, Vishnu Sresht, and Saeed Saremi. Structure-based drug design by denoising voxel grids. In ICML, 2024. (cit. on pp. 1, 2, 4, 5, 6, 7, and 18) [8] Joseph L. Watson, David Juergens, Nathaniel R. Bennett, Brian L. Trippe, Jason Yim, Helen E. Eisenach, Woody Ahern, Andrew J. Borst, Robert J. Ragotte, Lukas F. Milles, Basile I. M. Wicky, Nikita Hanikel, Samuel J. Pellock, Alexis Courbet, William Sheffler, Jue Wang, Preetham Venkatesh, Isaac Sappington, Susana Vázquez Torres, Anna Lauko, Valentin De Bortoli, Emile Mathieu, Sergey Ovchinnikov, Regina Barzilay, Tommi S. Jaakkola, Frank DiMaio, Minkyung Baek, and David Baker. De novo design of protein structure and function with RFdiffusion. Nature, 2023. (cit. on pp. 1 and 17) [9] Ivan Anishchenko, Samuel J. Pellock, Tamuka M. Chidyausiku, Theresa A. Ramelot, Sergey Ovchinnikov, Jingzhou Hao, Khushboo Bafna, Christoffer Norn, Alex Kang, Asim K. Bera, Frank DiMaio, Lauren Carter, Cameron M. Chow, Gaetano T. Montelione, and David Baker. De novo protein design by deep network hallucination. Nature, 2021. (cit. on p. 1) [10] J. Dauparas, I. Anishchenko, N. Bennett, H. Bai, R. J. Ragotte, L. F. Milles, B. I. M. Wicky, A. Courbet, R. J. de Haas, N. Bethel, P. J. Y. Leung, T. F. Huddy, S. Pellock, D. Tischer, F. Chan, B. Koepnick, H. Nguyen, A. Kang, B. Sankaran, A. K. Bera, N. P. King, and D. Baker. Robust deep learningbased protein sequence design using proteinMPNN. Science, 2022. (cit. on pp. 1 and 3) [11] Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: review and new perspectives. PAMI, 2013. (cit. on p. 2) [12] Josh Abramson, Jonas Adler, Jack Dunger, Richard Evans, Tim Green, Alexander Pritzel, Olaf Ronneberger, Lindsay Willmore, Andrew Ballard, Joshua Bambrick, et al. Accurate structure prediction of biomolecular interactions with alphafold 3. Nature, 2024. (cit. on p. 2) [13] Rohith Krishna, Jue Wang, Woody Ahern, Pascal Sturmfels, Preetham Venkatesh, Indrek Kalvet, Gyu Rie Lee, Felix Morey-Burrows, Ivan Anishchenko, Ian Humphreys, et al. Generalized biomolecular modeling and design with rosettafold all-atom. Science, 2024. (cit. on p. 2) [14] Zhangyang Gao, Jue Wang, Cheng Tan, Lirong Wu, Yufei Huang, Siyuan Li, Zhirui Ye, and Stan Li. Uniif: Unified molecule inverse folding. NeurIPS, 2024. (cit. on p. 2) [15] Xiangzhe Kong, Wenbing Huang, and Yang Liu. Generalist equivariant transformer towards 3d molecular interaction learning. ICML, 2024. (cit. on p. 2) [16] Matthieu Kirchmeyer, Pedro Pinheiro, and Saeed Saremi. Score-based 3d molecule generation with neural fields. In NeurIPS, 2024. (cit. on pp. 2, 3, 4, and 5) [17] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In ICML, 2015. (cit. on p. 2) [18] Saeed Saremi and Aapo Hyvarinen. Neural empirical Bayes. JMLR, 2019. (cit. on pp. 2, 5, 17, and 18) 11 [19] S. K. Burley, D. W. Piehl, B. Vallat, and C. Zardecki. RCSB protein data bank: supporting research and education worldwide through explorations of experimentally determined and computationally predicted atomic level 3d biostructures. IUCrJ, 2024. (cit. on pp. 2 and 9) [20] Yuanqi Du, Arian Jamasb, Jeff Guo, Tianfan Fu, Charles Harris, Yingheng Wang, Chenru Duan, Pietro Liò, Philippe Schwaller, and Tom Blundell. Machine learning-aided generative molecular design. Nature Machine Intelligence, 2024. (cit. on p. 2) [21] Youzhi Luo and Shuiwang Ji. An autoregressive flow model for 3D molecular geometry generation from scratch. In ICLR, 2022. (cit. on pp. 2 and 6) [22] Meng Liu, Youzhi Luo, Kanji Uchino, Koji Maruhashi, and Shuiwang Ji. Generating 3D molecules for target protein binding. arXiv, 2022. (cit. on p. 2) [23] Keir Adams and Connor Coley. Equivariant shape-conditioned generation of 3D molecules for ligandbased drug design. arXiv:2210.04893, 2022. (cit. on p. 2) [24] Zaixi Zhang, Yaosen Min, Shuxin Zheng, and Qi Liu. Molecule generation for target protein binding with structural motifs. In ICLR, 2023. (cit. on p. 2) [25] Alexander Powers, Helen Yu, Patricia Suriana, Rohan Koodli, Tianyu Lu, Joseph Paggi, and Ron Dror. Geometric deep learning for structure-based ligand design. ACS Central Science, 2023. (cit. on p. 2) [26] Eyal Rozenberg, Ehud Rivlin, and Daniel Freedman. Structure-based drug design via semi-equivariant conditional normalizing flows. In ICLR, Machine Learning for Drug Discovery workshop, 2023. (cit. on p. 2) [27] Arne Schneuing, Charles Harris, Yuanqi Du, Kieran Didi, Arian Jamasb, Ilia Igashov, Weitao Du, Carla Gomes, Tom Blundell, Pietro Lio, et al. Structure-based drug design with equivariant diffusion models. Nature Computational Science, 2024. (cit. on pp. 2 and 6) [28] Jiaqi Guan, Xiangxin Zhou, Yuwei Yang, Yu Bao, Jian Peng, Jianzhu Ma, Qiang Liu, Liang Wang, and Quanquan Gu. DecompDiff: Diffusion models with decomposed priors for structure-based drug design. In ICML, 2023. (cit. on pp. 2, 6, and 7) [29] Miha Skalic, José Jiménez, Davide Sabbadin, and Gianni De Fabritiis. Shape-based generative modeling for de novo drug design. Journal of chemical information and modeling, 2019. (cit. on p. 2) [30] Lvwei Wang, Rong Bai, Xiaoxuan Shi, Wei Zhang, Yinuo Cui, Xiaoman Wang, Cheng Wang, Haoyu Chang, Yingsheng Zhang, Jielong Zhou, et al. pocket-based 3D molecule generative model fueled by experimental electron density. Scientific reports, 2022. (cit. on p. 2) [31] Siyu Long, Yi Zhou, Xinyu Dai, and Hao Zhou. Zero-shot 3D drug design by sketching and generating. NeurIPS, 2022. (cit. on p. 2) [32] Yiheng Xie, Towaki Takikawa, Shunsuke Saito, Or Litany, Shiqin Yan, Numair Khan, Federico Tombari, James Tompkin, Vincent sitzmann, and Srinath Sridhar. Neural fields in visual computing and beyond. Computer Graphics Forum, 2022. (cit. on p. 2) [33] Asher Mullard. 2024 FDA approvals. Nature Reviews Drug Discovery, 2025. (cit. on p. 2) [34] Abigail VJ Collis, Adam Brouwer, and Andrew CR Martin. Analysis of the antigen combining site: correlations between length and sequence composition of the hypervariable loops and the nature of the antigen. Journal of molecular biology, 2003. (cit. on p. 2) [35] Wengong Jin, Jeremy Wohlwend, Regina Barzilay, and Tommi Jaakkola. Iterative refinement graph neural network for antibody sequence-structure co-design. arXiv preprint arXiv:2110.04624, 2021. (cit. on p. 2) [36] Wengong Jin, Regina Barzilay, and Tommi Jaakkola. Antibody-antigen docking and design via hierarchical structure refinement. In ICML, 2022. (cit. on p. 2) [37] Xiangzhe Kong, Wenbing Huang, and Yang Liu. Conditional antibody design as 3d equivariant graph translation. arXiv preprint arXiv:2208.06073, 2022. (cit. on p. 2) [38] Xiangzhe Kong, Wenbing Huang, and Yang Liu. End-to-end full-atom antibody design. arXiv preprint arXiv:2302.00203, 2023. (cit. on pp. 2 and 7) 12 [39] Shitong Luo, Yufeng Su, Xingang Peng, Sheng Wang, Jian Peng, and Jianzhu Ma. Antigen-specific antibody design and optimization with diffusion-based generative models for protein structures. In NeurIPS, 2022. (cit. on pp. 2, 6, and 7) [40] Xiangxin Zhou, Dongyu Xue, Ruizhe Chen, Zaixiang Zheng, Liang Wang, and Quanquan Gu. Antigenspecific antibody design via direct energy-based preference optimization. NeurIPS, 2024. (cit. on p. 2) [41] Zaixiang Zheng, Yifan Deng, Dongyu Xue, Yi Zhou, Fei Ye, and Quanquan Gu. Structure-informed language models are protein designers. In ICML, 2023. (cit. on p. 2) [42] Fang Wu and Stan Li. hierarchical training paradigm for antibody structure-sequence co-design. NeurIPS, 2023. (cit. on p. 2) [43] Karolis Martinkus, Jan Ludwiczak, WEI-CHING LIANG, Julien Lafrance-Vanasse, Isidro Hotzel, Arvind Rajpal, Yan Wu, Kyunghyun Cho, Richard Bonneau, Vladimir Gligorijevic, and Andreas Loukas. Abdiffuser: full-atom generation of in-vitro functioning antibodies. In NeurIPS, 2023. (cit. on pp. 2 and 6) [44] Nathaniel R. Bennett, Joseph L. Watson, Robert J. Ragotte, Andrew J. Borst, DéJenaé L. See, Connor Weidle, Riti Biswas, Yutong Yu, Ellen L. Shrock, Russell Ault, Philip J. Y. Leung, Buwei Huang, Inna Goreshnik, John Tam, Kenneth D. Carr, Benedikt Singer, Cameron Criswell, Basile I. M. Wicky, Dionne Vafeados, Mariana Garcia Sanchez, Ho Min Kim, Susana Vázquez Torres, Sidney Chan, Shirley M. Sun, Timothy Spear, Yi Sun, Keelan OReilly, John M. Maris, Nikolaos G. Sgourakis, Roman A. Melnyk, Chang C. Liu, and David Baker. Atomically accurate de novo design of antibodies with rfdiffusion. bioRxiv, 2025. (cit. on p. 2) [45] Joseph Watson, David Juergens, Nathaniel Bennett, Brian Trippe, Jason Yim, Helen Eisenach, Woody Ahern, Andrew Borst, Robert Ragotte, Lukas Milles, et al. De novo design of protein structure and function with rfdiffusion. Nature, 2023. (cit. on p. 3) [46] Edward M. Driggers, Stephen P. Hale, Jinbo Lee, and Nicholas K. Terrett. The exploration of macrocycles for drug discovery an underexploited structural class. Nature Reviews Drug Discovery, 2008. (cit. on p. 3) [47] Stephen A. Rettie, David Juergens, Victor Adebomi, Yensi Flores Bueso, Qinqin Zhao, Alexandria N. Leveille, Andi Liu, Asim K. Bera, Joana A. Wilms, Alina Üffing, Alex Kang, Evans Brackenbrough, Mila Lamb, Stacey R. Gerben, Analisa Murray, Paul M. Levine, Maika Schneider, Vibha Vasireddy, Sergey Ovchinnikov, Oliver H. Weiergräber, Dieter Willbold, Joshua A. Kritzer, Joseph D. Mougous, David Baker, Frank DiMaio, and Gaurav Bhardwaj. Accurate de novo design of high-affinity protein binding macrocycles using deep learning. bioRxiv, 2024. (cit. on pp. 3 and 9) [48] Sophia Tang, Yinuo Zhang, and Pranam Chatterjee. Peptune: De novo generation of therapeutic peptides with multi-objective-guided discrete diffusion, 2025. (cit. on p. 3) [49] Nataša Tagasovska, Ji Won Park, Matthieu Kirchmeyer, Nathan C. Frey, Andrew Martin Watkins, Aya Abdelsalam Ismail, Arian Rokkum Jamasb, Edith Lee, Tyler Bryson, Stephen Ra, and Kyunghyun Cho. Antibody domainbed: Out-of-distribution generalization in therapeutic protein design, 2024. (cit. on p. 3) [50] Lin Li, Chuan Li, and Emil Alexov. On the modeling of polar component of solvation energy using smooth gaussian-based dielectric function. Journal of Theoretical and Computational Chemistry, 2014. (cit. on pp. 3 and 16) [51] Gabriele Orlando, Daniele Raimondi, Ramon Duran-Romaña, Yves Moreau, Joost Schymkowitz, and Frederic Rousseau. Pyuul provides an interface between biological structures and deep learning algorithms. Nature communications, 2022. (cit. on pp. 3 and 16) [52] Pedro Pinheiro, Joshua Rackers, Joseph Kleinhenz, Michael Maser, Omar Mahmood, Andrew Watkins, Stephen Ra, Vishnu Sresht, and Saeed Saremi. 3d molecule generation by denoising voxel grids. NeurIPS, 2023. (cit. on pp. 3 and 4) [53] Songyou Peng, Michael Niemeyer, Lars Mescheder, Marc Pollefeys, and Andreas Geiger. Convolutional occupancy networks. In ECCV, 2020. (cit. on p. 3) [54] Matthias Bauer, Emilien Dupont, Andy Brock, Dan Rosenbaum, Jonathan Richard Schwarz, and Hyunjik Kim. Spatial functa: Scaling functa to imagenet classification and generation. arXiv, 2023. (cit. on p. 3) [55] Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and Hao Su. Tensorf: Tensorial radiance fields. In ECCV, 2022. (cit. on p. 3) 13 [56] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-Net: Convolutional networks for biomedical image segmentation. In MICCAI, 2015. (cit. on p. 3) [57] Rizal Fathony, Anit Kumar Sahu, Devin Willmott, and J. Zico Kolter. Multiplicative filter networks. In ICLR, 2021. (cit. on pp. 4 and 17) [58] Yuan Yin, Matthieu Kirchmeyer, Jean-Yves Franceschi, Alain Rakotomamonjy, and patrick gallinari. Continuous PDE dynamics forecasting with implicit neural representations. In ICLR, 2023. (cit. on pp. 4 and 17) [59] Diederik Kingma and Max Welling. Auto-encoding variational bayes. In ICLR, 2014. (cit. on p. 4) [60] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, 2021. (cit. on p. 4) [61] Jakub Tomczak and Max Welling. VAE with VampPrior. In AISTATS, 2018. (cit. on p. 4) [62] Tero Karras, Janne Hellsten, Miika Aittala, Timo Aila, Jaakko Lehtinen, and Samuli Laine. Analyzing and improving the training dynamics of diffusion models. CVPR, 2024. (cit. on pp. 4, 5, and 17) [63] Tomas Geffner, Kieran Didi, Zuobai Zhang, Danny Reidenbach, Zhonglin Cao, Jason Yim, Mario Geiger, Christian Dallago, Emine Kucukbenli, Arash Vahdat, and Karsten Kreis. Proteina: Scaling flow-based protein structure generative models. In ICLR, 2025. (cit. on pp. 4 and 17) [64] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In ICLR, 2021. (cit. on p. 5) [65] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022. (cit. on pp. 5 and 17) [66] Herbert Robbins. An empirical Bayes approach to statistics. In Proc. Third Berkeley Symp., volume 1, pages 157163, 1956. (cit. on pp. 5 and 17) [67] Koichi Miyasawa. An empirical Bayes estimator of the mean of normal population. Bulletin of the International Statistical Institute, 1961. (cit. on pp. 5 and 17) [68] Noel OBoyle, Michael Banck, Craig James, Chris Morley, Tim Vandermeersch, and Geoffrey Hutchison. Open babel: An open chemical toolbox. Journal of cheminformatics, 2011. (cit. on pp. 5 and 6) [69] Paul Francoeur, Tomohide Masuda, Jocelyn Sunseri, Andrew Jia, Richard Iovanisci, Ian Snyder, and David Koes. Three-dimensional convolutional neural networks and cross-docked data set for structure-based drug design. Journal of chemical information and modeling, 2020. (cit. on p. 6) [70] Shitong Luo, Jiaqi Guan, Jianzhu Ma, and Jian Peng. 3D generative model for structure-based drug design. NeurIPS, 2021. (cit. on pp. 6 and 8) [71] Jerome Eberhardt, Diogo Santos-Martins, Andreas Tillack, and Stefano Forli. Autodock vina 1.2. 0: New docking methods, expanded force field, and python bindings. JCIM, 2021. (cit. on pp. 6 and 9) [72] Richard Bickerton, Gaia Paolini, Jérémy Besnard, Sorel Muresan, and Andrew Hopkins. Quantifying the chemical beauty of drugs. Nature chemistry, 2012. (cit. on p. 6) [73] Peter Ertl and Ansgar Schuffenhauer. Estimation of synthetic accessibility score of drug-like molecules based on molecular complexity and fragment contributions. Journal of cheminformatics, 2009. (cit. on p. 6) [74] Greg Landrum. RDKit: Open-source cheminformatics software, 2016. (cit. on p. 6) [75] Dávid Bajusz, Anita Rácz, and Károly Héberger. Why is tanimoto index an appropriate choice for fingerprint-based similarity calculations? Journal of cheminformatics, 2015. (cit. on p. 6) [76] Charles Harris, Kieran Didi, Arian Jamasb, Chaitanya Joshi, Simon Mathis, Pietro Lio, and Tom Blundell. Benchmarking generated poses: How rational is structure-based drug design with generative models? arXiv:2308.0741, 2023. (cit. on p. 6) [77] Anthony Rappé, Carla Casewit, KS Colwell, William Goddard III, and Mason Skiff. Uff, full periodic table force field for molecular mechanics and molecular dynamics simulations. Journal of the American chemical society, 1992. (cit. on p. 6) 14 [78] James Dunbar, Konrad Krawczyk, Jinwoo Leem, Terry Baker, Angelika Fuchs, Guy Georges, Jiye Shi, and Charlotte Deane. Sabdab: the structural antibody database. Nucleic acids research, 2014. (cit. on pp. 6 and 8) [79] Cyrus Chothia, Arthur M. Lesk, Anna Tramontano, Michael Levitt, Sandra J. Smith-Gill, Gillian Air, Steven Sheriff, Eduardo A. Padlan, David Davies, William R. Tulip, Peter M. Colman, Silvia Spinelli, Pedro M. Alzari, and Roberto J. Poljak. Conformations of immunoglobulin hypervariable regions. Nature, 1989. (cit. on p. 6) [80] Rebecca F. Alford, Andrew Leaver-Fay, Jeliazko R. Jeliazkov, Matthew J. OMeara, Frank P. DiMaio, Hahnbeom Park, Maxim V. Shapovalov, P. Douglas Renfrew, Vikram K. Mulligan, Kalli Kappel, Jason W. Labonte, Michael S. Pacella, Richard Bonneau, Philip Bradley, Roland L. Jr. Dunbrack, Rhiju Das, David Baker, Brian Kuhlman, Tanja Kortemme, and Jeffrey J. Gray. The rosetta all-atom energy function for macromolecular modeling and design. Journal of Chemical Theory and Computation, 2017. (cit. on pp. 6 and 7) [81] Tian Zhu, Milong Ren, and Haicang Zhang. Antibody design using score-based diffusion model guided by evolutionary, physical and geometric constraints. In ICML, 2024. (cit. on p. 7) [82] Peter Eastman, Jason Swails, John D. Chodera, Robert T. McGibbon, Yutong Zhao, Kyle A. Beauchamp, Lee-Ping Wang, Andrew C. Simmonett, Matthew P. Harrigan, Chaya D. Stern, Rafal P. Wiewiora, Bernard R. Brooks, and Vijay S. Pande. OpenMM 7: Rapid development of high performance algorithms for molecular dynamics. PLOS Computational Biology, 2017. (cit. on p. 7) [83] W. K. Wong, S. A. Robinson, A. Bujotzek, G. Georges, A. P. Lewis, J. Shi, J. Snowden, B. Taddese, and C. M. Deane. Ab-Ligity: identifying sequence-dissimilar antibodies that bind to the same epitope. MAbs, 2021. (cit. on pp. 8 and 20) [84] C. A. Rohl, C. E. M. Strauss, and K. M. S. Misura. Protein structure prediction using rosetta. In Methods in Enzymology, 2004. (cit. on pp. 9 and 24) [85] Stephen A. Rettie, Katelyn V. Campbell, Asim K. Bera, Alex Kang, Simon Kozlov, Yensi Flores Bueso, Joshmyn De La Cruz, Maggie Ahlrichs, Suna Cheng, Stacey R. Gerben, Mila Lamb, Analisa Murray, Victor Adebomi, Guangfeng Zhou, Frank DiMaio, Sergey Ovchinnikov, and Gaurav Bhardwaj. Cyclic peptide structure prediction and design using alphafold2. Nature Communications, 16(1):4730, 2025. (cit. on p. 9) [86] Diederik P. Kingma and Jimmy Ba. Adam: method for stochastic optimization. In ICLR, 2015. (cit. on p. 17) [87] Saeed Saremi, Ji Won Park, and Francis Bach. Chain of log-concave Markov chains. In ICLR, 2024. (cit. on p. 18) [88] Saeed Saremi and Rupesh Kumar Srivastava. Multimeasurement generative models. In ICLR, 2022. (cit. on p. 18) [89] Matthias Sachs, Benedict Leimkuhler, and Vincent Danos. Langevin dynamics with variable coefficients and nonconservative forces: from stationary states to numerical methods. Entropy, 2017. (cit. on p. 18) [90] K. Drew, P. D. Renfrew, T. W. Craven, et al. Adding diverse noncanonical backbones to rosetta: enabling peptidomimetic design. PLoS One, 2013. (cit. on p. 24) [91] Y. Namiki, T. Ishida, and Y. Akiyama. Acceleration of sequence clustering using longest common subsequence filtering. BMC Bioinformatics, 14(Suppl 8):S7, 2013. (cit. on p. 24)"
        },
        {
            "title": "Appendices",
            "content": "This supplementary material is organized as follows: 1. Section includes broader impact statement. 2. Section includes model and implementation details. 3. Section provides some additional results for antibody CDR redesign. Section C.1 presents some additional in silico results Section C.2 presents our in vitro results. 4. Section explain how we inferred non canonical amino acids without OpenBabel. 5. Section presents some additional results for macro-cyclic peptide generation Section E.1 presents some additional results. Section E.2 presents how we curated the dataset for training FuncBind. Section E.3 presents the train / val / test splitting logic."
        },
        {
            "title": "A Broader impacts",
            "content": "This work introduces FuncBind, novel framework for all-atom, structure-conditioned molecular generation, whose primary positive impact lies in its potential to accelerate and enhance the discovery of new therapeutics across diverse modalities, from small molecules to complex biologics like peptides and antibodies. While the underlying principles could find applications in other scientific fields like materials science, its deployment in drug discovery requires addressing significant challenges, including the validation gap between in silico predictions and experimental testing (in vitro, in vivo, and clinical trials)."
        },
        {
            "title": "B Model details",
            "content": "B.1 Representation FuncBind is based on neural field representation that models an atomic density field, smooth function taking values between 0 (far away from all atoms) and 1 (at the center of atoms). This field takes the following form [50, 51]: R3, va(x) = 1 na(cid:89) (cid:16) i= 1 exp (cid:16) (cid:16) xai .93r (cid:17)2(cid:17)(cid:17) , (4) where ai is the ith atom of type (among choices), for total of na atoms and is the atoms radius set to = 1.0Å for all atom types. We consider = 8 element types C, O, N, S, F, Cl, P, Br that cover all major atom types across small molecule, macrocyclic peptides and proteins. Note that protein-specific atom types (e.g. Cα, Cβ etc.) are merged into single element type (e.g. C). This helps transfer learning across modalities. Finally, the field is defined over volume of (32Å)3. It is the continuous version of voxel grid of spatial dimension 128 and resolution of 0.25Å, in R8 1283 . B.2 Neural Field The encoder Eψ is 3D CNN containing 4 residual blocks (number of hidden units 256, 512, 1024, 2048 for each block), where each block contains 3 convolutional layers followed by BatchNorm, ReLU and pooling layers (we use max pooling on the first three blocks). The encoder has 59M parameters. The input to the encoder is low-resolution grid of spatial grid dimension = 16 corresponding to resolution of 2Å. Before voxelizing the molecules, we first center the atoms around the tightest bounding box encapsulating the molecule, apply random rotation to the atoms 16 (each Euler angle rotated randomly between [0,2π)) and random translation between [1, 1]Å then normalize their coordinates to the range of [1, 1]. The decoder Dϕ is conditional Multiplicative Filter Network (MFN) [57, 58] with Gabor filters and 6 FiLM-modulated layers, where each fully-connected layer has 2048 hidden units. The decoder has 59M parameters. The auto-encoder is trained with Adam Optimizer [86] with learning rate 10 We apply KL regularization weight of λ = 10 15000 coordinates per batch. 2, β1 = 0.9, β2 = 0.999. 5. Batch size is 32 over 1 B200 GPU; we sample B.3 Denoiser The hyperparameters of the Karras et al. [62] UNet architecture are as follows: 5B model with 8 blocks with 512 channels, channel multipliers [1,2,3,4], attention resolutions [4, 2]. This model follows the XXL setting of EDM2 [62] and increased the number of channels from 448 to 512. For reference, Protéina [63], the largest protein backbone generative model, comprises 400M parameters and RFDiffusion [8] roughly 100M parameters. As [65], we apply preconditioning to learn the denoiser across noise levels. Moreover, we sample the noise levels from log-normal distribution with mean 1.2 and standard deviation 0.8. The target encoder Eψ is 3D CNN which takes as input voxel grid of dimension R4 of the target protein (resolution 1.0Å), considering atom elements C, O, N, S, and consists of magnitude preserving [62] CNN layer with kernel 3 3 3 and output channels 64, then downsampling layer to spatial grid in R64 then magnitude preserving U-Net block [62] with output channels = 128 leading to voxel of size RC 2, β1 = The parameters are optimized with Adam optimizer [86] with learning rate αref = 10 0.9, β2 = 0.95 using an aggregated batch size of 768 over 8 B200 GPUs. We perform early stopping on the validation loss. We use the power function exponential moving average from EDM2 [62] with an EMA length of 5%. Moreover, we adopt the inverse square root decay schedule of [86], also used , where we set tref = 20040. Finally, the networks are in [62] which sets α(t) = αref 163 323 . (cid:112) max(t/tref , 1) trained by randomly dropping the conditioning information 10% of the time. B.4 Sampling To improve uniqueness, we apply different rotations to the pocket on each MCMC chain, in similar fashion to how rotation-based data augmentation is performed at training time. B.4.1 Denoising diffusion We set as follows the sampling parameters of EDM2 [62]: = 128 steps σmin = 0.01, σmax = 10 Smin = 5.0, Smax = 7.0 Schurn = 30.0 Snoise = 1.003 ρ = 7 Moreover, we apply temperature scaling with τ = 0.5 on Crossdocked and τ = 0.33 on MCPprotein complexes. B.4.2 Walk-Jump sampling We also implemented conditional form of the Walk-Jump Sampling (WJS), score-based generative model that is based on probabilistic formulation of least-squares denoising [18]. The framework is based on the Tweedie-Miyasawa formula (TMF) [66, 67], which relates the least-squares denoiser 17 at noise level σ with the score function at the noise level. Given = + σε, ε N(0, Id), the conditional extension of TMF was derived in [7]. In our notation, it takes the form: log p(y ztar, σ, c) sθ(y ztar, σ, c) := (ˆzθ(y ztar, σ, c) y)/σ2, (5) where ˆzθ is the minimizer of Equation (2); sθ(y ztar, σ, c) is the learned conditional score function. Walk-jump sampling. WJS [18] is composed of two stages: (i) (walk) samples the noisy latent variables conditioned on ztar, using Langevin Markov chain Monte Carlo (MCMC) via the learned score function (Equation (5)), (ii) (jump) estimates clean by single-step denoising. There is fundamental trade-off in this sampling strategy: for larger σ, sampling from the smoother density becomes easier, but the denoised samples move farther away from the distribution of interest [87]. Multimeasurement walk-jump sampling. The sampling trade-off in WJS is addressed in multimeasurement denoising models [88, 87], in which the problem is framed as sampling from the distribution pσ(y1:m) associated with y1:m := (y1, . . . , ym), where yk = + σεk, {1, . . . , m}, and εk N(0, Id) all independent of z. Saremi et al. [87] studied sequential scheme for sampling from pσ(y1:m) and showed that the noise level effectively decreases (as far as the denoiser is concerned) at the rate σ/ m. Furthermore, it was shown that sampling becomes easier upon accumulation of measurements. The general sampling problem is therefore mapped to sequence of sampling noisy data at fixed noise scale, while the effective noise decreases via accumulation of measurements. We refer to this scheme as WJS-m, which involves two hyperparameters: the noise level σ, and the number of measurements m. In this construction, we only need to keep track of the empirical mean of noisy samples. In particular, we have (see [87, Eq. 4.9]): ym log pθ(ym y1:m 1, ztar, σ, c) = 1 sθ(y1:m ztar, σ , c) + 1 σ2 (y1:m ym), where y1:m is the empirical mean of the measurements (y1, . . . , ym). The score function above is used in sampling (y1, . . . , ym) iteratively using Langevin MCMC [87, Algorithm 1]. Finally, the denoising jump in WJS-m is achieved via (single-measurement) TMF using the sufficient statistics m. It is clear that the vanilla WJS discussed above reduces to WJS-1. y1:m at the noise scale σ/ Although there is flavor of diffusion in this scheme due to its sequential strategy, WJS-m is arguably more surgical in that, by construction, we do not need to learn score functions over continuum of noise levels, but only finite one identified by m. This is especially appealing for applications where WJS-1 already shows reasonable performance and is therefore taken to be small. This work contains the first experimental validation of WJS-m in generative modeling applications. We report the results for WJS for CDR H3 inpainting in Section C.1.2. The parameters are set to σ = 7.0 and = 16. We use underdamped Langevin MCMC from Sachs et al. [89] in the BAOAB scheme with = 50 steps, friction γ = 1.0, discretization step δ = σ/2 ([87, Algorithm 1])."
        },
        {
            "title": "C CDR redesign",
            "content": "C.1 In silico evaluation C.1.1 Uniqueness Table 4 reports our CDR sampling results over unique sequences. We observe that higher uniqueness usually leads to lower Amino Acid Recovery (AAR). In other words, repeated sequences tend to correlate more with the seed. We use this simple heuristic to select H3 designs for in vitro evaluation (see Section C.2). C.1.2 Ablation with Walk Jump Sampling We report the performance of multimeasurement WJS and diffusion. Overall the models are comparable with slightly higher uniqueness for diffusion. C.1.3 Comparison to specialized model We observe that the unified model has higher uniqueness than the specialized model, with slightly better CDR loop inpainting performance on unique samples. We observe this trend on the other data modalities as well. 18 Table 4: Impact of uniqueness on CDR inpainting performance. RMSD is in Å and AAR and Uniqueness are in %. indicates designs with unique sequences. Method AAR RMSD Unique AAR RMSD Unique AbDiffuser FuncBind FuncBind AbDiffuser FuncBind FuncBind AbDiffuser FuncBind FuncBind 76.3 86.9 75.9 65.7 78.2 59. 34.1 47.5 44.1 H1 1.58 0.41 0.45 H2 1.45 0.52 0.57 3.35 2.04 2.16 - 23.5 100 - 20.6 100 - 85.5 100 81.4 86.4 79.3 83.2 86.2 54. 73.2 80.8 68.9 L1 1.46 0.68 0.85 L2 1.40 0.83 2.39 1.59 0.68 0.94 - 38.9 100 - 19.4 100 - 44.1 100 Table 5: Diffusion vs WJS on H3 loop inpainting. indicates designs with unique sequences. Method AAR RMSD Unique FuncBinddiff FuncBind FuncBindWJS16 FuncBind WJS16 diff 47.5 44.1 51.0 41.4 2.04 2.16 1.89 2. 85.5 100 73.8 100 Table 6: Loop uniqueness comparison between Unified and Specialized models. Loop Uniqueness Unified Uniqueness Specialized H1 H2 H3 L1 L2 L3 23.5 20.6 85.5 38.9 19.4 44.1 9.6 12.6 69.2 10.6 14.0 22. Table 7: H3 loop performance on unique samples. H3 loop AAR RMSD Unified Specialized 0.441 0.406 2.16 2. 19 C.2 In vitro validation To experimentally validate FuncBinds capabilities, we performed wet-lab validation of H3 loop redesigns based on the co-crystal structure of an antibody bound to rigid and flexible epitope. We selected H3 loop redesign due to H3s important contribution to the antibodys functional properties. This study was conducted in de novo setting: interfaces similar to the two complexes, identified using Ab-Ligity [83], were excluded from training. From an initial pool of 10,000 unique generated H3 designs (all matching the original seeds length), 190 (i.e. 2 SPR plates) were selected for experimental testing. This selection involved two steps: 1. The top 500 designs were shortlisted based on model confidence, as indicated by their repeated generation counts. Our in-silico validation showed that repeats is an useful proxy for high amino acid recovery of the seed. 2. These 500 designs were then clustered into 190 groups using weighted K-means based on sequence edit distance, where the weights were defined by the repeat generation count. The design with the highest repeat count (highest confidence) from each of these 190 clusters was chosen for synthesis and characterization. The selected antibody designs were expressed and purified in the wet lab. Binding affinity was then determined using surface plasmon resonance (SPR) measurements. C.2.1 Rigid epitope An analysis of Amino Acid Recovery (AAR) and Root Mean Square Deviation (RMSD) for the selected designs are provided in Figure 6: Figure 6: All generated CDR H3 designs on rigid epitope: AAR (left) and RMSD (right) histogram. FuncBind successfully generated novel antibody binders in this de novo redesign problem; in fact 54% were binders (42% with pKD values). 94% of all 190 submitted designs were successfully expressed and purified. Experimental results confirmed that 42% of all submitted designs were 8]M ) and binders, with pKD values in the range of [7.55, 11.29] (KD [5.08 10 9M ). For comparison, the pKD of the parent antibody is an average pKD of 9.56 (KD = 2.00 10 11M ). 12% were binders with no pKD (\"bad\" binders). We identified 5X 10.20 (KD = 2.63 10 binder in that set. 12, 2.84 10 Table 8: Average RMSD and AAR for binders and non binders on rigid epitope Binders Unassigned Non-Binders Global RMSD AAR 0.50 55.1 0.53 50. 1.31 37.9 0.88 46.7 20 Table 9: Average RMSD and AAR for binders and non binders on flexible epitope Binders Unassigned Non-Binders Global"
        },
        {
            "title": "RMSD\nAAR",
            "content": "1.93 40.4 2.08 35.6 1.94 34.2 1.95 34.5 Figure 7: In vitro validation of FuncBinds designs against rigid epitope; expression (left), binding affinity (center), binding rate (right). Expression rate is 93.68%. C.3 Flexible epitope An analysis of Amino Acid Recovery (AAR) and Root Mean Square Deviation (RMSD) for the selected designs are provided in Figure 8: Figure 8: All generated CDR H3 designs against flexible epitope; target: AAR (top left) and RMSD (top right) histogram. Bottom: Logo of generated designs. Experimental results confirmed that 100% of the designs expressed and 2% of all submitted designs 7]M ) and an were binders, with pKDs [6.95, 7.44, 8.08, 8.47] (KD [3.38 10 8M ). For comparison, the pKD of the parent antibody average pKD of 7.74 (KD = 4.03 10 11M ). 10% were binders with no pKD (\"bad\" binders); around 3 is 10.58 (KD = 6.32 10 Unassigned binders had very reasonable SPR curves. 9, 1.13 10 Looking at Table 9, no specific correlation between binders and non binders were found based on RMSD on the limited set of binders we had. Though higher AAR seemed to be better (based on 4 binder samples only). Table 10: Per-residue TS between the (a) seed MCP and (b) crystal MCP of the 20 sampled molecules in the pocket relaxed to 1vwe mutant 90. (a) CYS HIS A20 PHE CYS 1vwe_mut90 0.63 0.24 0.24 0.17 0.26 0. 0.14 0.12 0.29 0.14 0.53 0.21 (b) CYS HIS PRO GLU PHE CYS 1vwe-CP 0.61 0. 0.24 0.12 0.31 0.14 0.28 0.10 0.33 0.08 0.57 0.23 Figure 9: In vitro validation of FuncBinds designs against flexible epitope; expression (left), binding affinity (center), binding rate (right). Expression rate is 100%."
        },
        {
            "title": "D Identifying non canonical amino acids",
            "content": "Unidentified amino acids were determined by recognizing repeated patterns of peptide backbone atoms around chiral carbon. All atoms stemming from Cα, including those in the side chain, were identified and labeled per canonical atom naming conventions. SMILES strings of unidentified amino acids were compared with non-canonical amino acid library, assigning residue names when match was found. If no match was found, the amino acid was labeled as unknown and added to the non-canonical library. In the output PDB file, canonical residue name was assigned based on the closest alignment of atom naming patterns (e.g., Cγ, Cδ1, Nϵ1) to known canonical residue."
        },
        {
            "title": "E Macro cyclic peptides",
            "content": "E.1 Additional results We perform the following studies: Figure 10: qualitative comparison between some MCP samples and the seed. Table 10: per residue tanimoto similarity (TS) for molecules samples with the seed mutant (a) and the crystal MCP (b). Figure 11 shows the categorization of the generated amino acids. Figure 12 shows some reasonable and novel generated non canonical amino acids. Figure 5 shows how newly generated amino acid interacts with pocket residues that neither the seed nor chemically similar amino acid at the same position engage. 22 Figure 10: Results of sampling in pocket relaxed around MCP seed (grey) are in (a) blue in 4gw5 mutant 104 pocket, (b) purple for 1vwe mutant 90 pocket, and (c) 1wb0 mutant 389 pocket Figure 11: Proportion of amino acid types classified as L-canonical, D-canonical, N-methylated, other known non-canonical amino acids (as annotated in our library), and unknown non-canonical amino acids in the sampled set. 23 Figure 12: Examples of unknown or novel NCAAs that appeared in the sampled set but were not present in the initial test set library. Novel NCAAs are labeled with formal name and an assigned 3-letter AA code. Figure 13: (a) Count of MCPs with each amino acid length in the source MCP-protein dataset. (b) Percentage of the number of mutations in the curated MCP-protein bound dataset. E.2 Data curation The MCP-protein pair dataset was curated by randomly mutating the MCPs from the source dataset at 18 different sites, using list of 213 distinct amino acids (Figure 13b). The closure bonds in the source dataset consist of 55% N-to-C (head-to-tail), 28% disulfide (cysteine-cysteine), and 23% S-acetyl-cysteine. The remaining 4% contain other closure bonds, such as linkers, and were mainly avoided or modified in the curated MCP dataset so that mutations can be easily implemented in Rosetta. Mutations were avoided in amino acids involved in disulfide bonds and S-acetyl-cysteine linkage. The amino acid list used for random mutation of the MCPs included L-canonical, Dcanonical, N-methylated, and other non-canonical typessuch as alpha-modified, beta-modified, and peptoid amino acids. Many of these non-canonical residues were pre-parameterized and available in the Rosetta non-canonical rotamer libraries [90]. Following mutation, the MCPs were relaxed using the fast-relax protocol, which involves iterative cycles of side-chain packing and allatom minimization [84]. The Rosetta interface energy scoresrepresenting the binding energy of the protein-peptide complex at each positionwere calculated using the ref_2015_cart energy function. From each source MCP-protein structure, over 2,000 mutated and relaxed complexes were generated, and approximately 500 with the lowest Rosetta interface scores were selected for the curated dataset. Therefore, we were able to expand the source dataset to 186,685 total MCP-protein structures in the curated dataset. E.3 Clustering and Splitting The pairwise similarity of protein sequences from the 641 protein targets in the curated dataset was evaluated using the Longest Common Subsequence (LCS) method [91]. Similarity between each 24 pair of sequences was calculated as the ratio of the length of their LCS to the length of the longer sequence. The target proteins were clustered together under representative if their similarity score was greater than 0.5. If none of the similarity scores met the 0.5 threshold, new cluster was created with that protein as the representative. 208 distinct protein clusters were used for training, validation, and testing. Clusters containing more than 100 MCP-protein pair structures in total were included in the training set. From the remaining clusters, 100 were randomly assigned to the test set, while the rest were added to the validation set."
        }
    ],
    "affiliations": [
        "Antibody Engineering, Genentech",
        "Prescient Design, Genentech"
    ]
}