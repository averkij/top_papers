{
    "paper_title": "Ovis2.5 Technical Report",
    "authors": [
        "Shiyin Lu",
        "Yang Li",
        "Yu Xia",
        "Yuwei Hu",
        "Shanshan Zhao",
        "Yanqing Ma",
        "Zhichao Wei",
        "Yinglun Li",
        "Lunhao Duan",
        "Jianshan Zhao",
        "Yuxuan Han",
        "Haijun Li",
        "Wanying Chen",
        "Junke Tang",
        "Chengkun Hou",
        "Zhixing Du",
        "Tianli Zhou",
        "Wenjie Zhang",
        "Huping Ding",
        "Jiahe Li",
        "Wen Li",
        "Gui Hu",
        "Yiliang Gu",
        "Siran Yang",
        "Jiamang Wang",
        "Hailong Sun",
        "Yibo Wang",
        "Hui Sun",
        "Jinlong Huang",
        "Yuping He",
        "Shengze Shi",
        "Weihong Zhang",
        "Guodong Zheng",
        "Junpeng Jiang",
        "Sensen Gao",
        "Yi-Feng Wu",
        "Sijia Chen",
        "Yuhui Chen",
        "Qing-Guo Chen",
        "Zhao Xu",
        "Weihua Luo",
        "Kaifu Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present Ovis2.5, a successor to Ovis2 designed for native-resolution visual perception and strong multimodal reasoning. Ovis2.5 integrates a native-resolution vision transformer that processes images at their native, variable resolutions, avoiding the degradation from fixed-resolution tiling and preserving both fine detail and global layout -- crucial for visually dense content like complex charts. To strengthen reasoning, we train the model to move beyond linear chain-of-thought and perform reflection -- including self-checking and revision. This advanced capability is exposed as an optional \"thinking mode\" at inference time, allowing users to trade latency for enhanced accuracy on difficult inputs. The model is trained via a comprehensive five-phase curriculum that progressively builds its skills. The process begins with foundational visual and multimodal pretraining, advances through large-scale instruction tuning, and culminates in alignment and reasoning enhancement using DPO and GRPO. To scale these upgrades efficiently, we employ multimodal data packing and hybrid parallelism, yielding a significant end-to-end speedup. We release two open-source models: Ovis2.5-9B and Ovis2.5-2B. The latter continues the \"small model, big performance\" philosophy of Ovis2, making it ideal for resource-constrained, on-device scenarios. On the OpenCompass multimodal leaderboard, Ovis2.5-9B averages 78.3, marking a substantial improvement over its predecessor, Ovis2-8B, and achieving state-of-the-art results among open-source MLLMs in the sub-40B parameter range; Ovis2.5-2B scores 73.9, establishing SOTA for its size. Beyond aggregate scores, Ovis2.5 achieves leading results on STEM benchmarks, exhibits strong capabilities on grounding and video tasks, and achieves open-source SOTA at its scale for complex chart analysis."
        },
        {
            "title": "Start",
            "content": "2025-08-19 Ovis2.5 Technical Report Ovis Team, Alibaba Group https://huggingface.co/AIDC-AI/Ovis2.5-9B https://github.com/AIDC-AI/Ovis"
        },
        {
            "title": "Abstract",
            "content": "We present Ovis2.5, successor to Ovis2 designed for native-resolution visual perception and strong multimodal reasoning. Ovis2.5 integrates native-resolution vision transformer that processes images at their native, variable resolutions, avoiding the degradation from fixed-resolution tiling and preserving both fine detail and global layoutcrucial for visually dense content like complex charts. To strengthen reasoning, we train the model to move beyond linear chain-of-thought and perform reflectionincluding self-checking and revision. This advanced capability is exposed as an optional thinking mode at inference time, allowing users to trade latency for enhanced accuracy on difficult inputs. The model is trained via comprehensive five-phase curriculum that progressively builds its skills. The process begins with foundational visual and multimodal pretraining, advances through large-scale instruction tuning, and culminates in alignment and reasoning enhancement using DPO and GRPO. To scale these upgrades efficiently, we employ multimodal data packing and hybrid parallelism, yielding significant end-to-end speedup. We release two open-source models: Ovis2.5-9B and Ovis2.5-2B. The latter continues the small model, big performance philosophy of Ovis2, making it ideal for resource-constrained, on-device scenarios. On the OpenCompass multimodal leaderboard, Ovis2.5-9B averages 78.3, marking substantial improvement over its predecessor, Ovis2-8B, and achieving state-of-the-art results among open-source MLLMs in the sub-40B parameter range; Ovis2.5-2B scores 73.9, establishing SOTA for its size. Beyond aggregate scores, Ovis2.5 achieves leading results on STEM benchmarks, exhibits strong capabilities on grounding and video tasks, and achieves open-source SOTA at its scale for complex chart analysis. 5 2 0 2 5 1 ] . [ 1 7 3 7 1 1 . 8 0 5 2 : r Figure 1: Benchmark performance of Ovis2.5 and its counterparts."
        },
        {
            "title": "Introduction",
            "content": "The field of multimodal large language models (MLLMs) has seen rapid advancements, with surge of powerful open-source models demonstrating remarkable capabilities (Liu et al., 2024a; Bai et al., 2025; Zhu et al., 2025; Hong et al., 2025; Yang et al., 2025b; Yue et al., 2025). Many of these models are built upon conventional architecture, typically connecting pre-trained vision transformer (ViT) and large language model (LLM) via simple projector like an MLP. In our previous work (Lu et al., 2024b), we identified misalignment between the continuous structure of the visual embeddings from the MLP projector and the discrete structure of the textual embeddings. We proposed the Ovis architecture to structurally align the textual and visual embeddings using learnable visual embedding table, and empirically validated its superiority over conventional designs. Building on this foundation, we have iteratively developed the Ovis series, releasing versions 1.5, 1.6, and 2.0. These updates progressively enhanced the models capabilities to handle high-resolution images, multi-image and video inputs, OCR tasks, multilingual scenarios, and complex problems like mathematical reasoning. Each version demonstrated leading performance among open-source models of similar scale upon its release. However, despite these significant improvements, we observed that even our latest model, Ovis2, still struggled with tasks requiring deep reasoning or detailed analysis of visually dense content, such as complex charts. These shortcomings stem from two core issues: (1) rigid vision front-endse.g., fixed-resolution encoders that necessitate image tiling and compromise global structure; and (2) training schemes that emphasize linear chain-of-thought (CoT) but lack reflective, self-corrective supervision, limiting deeper reasoning. To address these issues, we present Ovis2.5, which introduces two key improvements: First, to enhance its perceptual abilities, we replace the fixed-resolution ViT with the native-resolution ViT (NaViT, Dehghani et al., 2023). NaViT processes images at their native, variable resolutions, avoiding lossy tiling and preserving both fine-grained details and global layout that are critical for charts, diagrams, and other visually dense content. Second, to strengthen its reasoning abilities, we augment training with deepreasoning data that supervises not only linear CoT but also reflective processes such as self-checking and revision. This teaches the model to produce intermediate steps to evaluate its own reasoning, and refine conclusions when necessary, enabling deeper and more robust reasoning. The deep-reasoning capability is available as an optional thinking mode at inference time. This allows users to keep the mode off for efficiency on easy inputs, or enable it for complex problems to trade latency for higher accuracy. In alignment with these upgrades, we also refined our training strategy into comprehensive five-phase curriculum(i) visual pretraining, (ii) multimodal pretraining, (iii) instruction tuning, (iv) DPO (Rafailov et al., 2023), and (v) reinforcement learning (GRPO, Shao et al., 2024)which progressively builds the model from foundational perception to advanced reasoning. To scale this ambitious training, we developed high-efficiency infrastructure that leverages multimodal data packing and hybrid parallelism, delivering 34 end-to-end speedup. Comprehensive evaluations demonstrate the exceptional capabilities of Ovis2.5. On the OpenCompass multimodal leaderboard (Duan et al., 2025), Ovis2.5-9B achieves an average score of 78.3, indicating strong overall multimodal performance. This result not only represents substantial improvement over its predecessor, Ovis2-8B, but also establishes new state-of-the-art among open-source MLLMs in the sub-40B parameter range. In line with Ovis2s small model, big performance philosophyparticularly for on-device applicationswe also release Ovis2.5-2B, which scores 73.9 on OpenCompass, state-ofthe-art result among open-source MLLMs of comparable size. Beyond aggregate performance, Ovis2.5 demonstrates broad capability: it delivers leading results among open-source MLLMs of similar scale across STEM benchmarks and shows strong performance in grounding and video tasks. Notably, on complex chart-analysis benchmarks, Ovis2.5 sets new state of the art for open-source models at its scale. The key advances of Ovis2.5 are summarized as follows: Native-Resolution Perception: Integrates native-resolution vision encoder to eliminate destructive tiling, preserving the crucial details and global context needed for superior analysis of complex visuals like charts and diagrams. Deep-Reasoning Capability: Introduces an optional thinking mode, enabled by training on data that demonstrates reasoning with reflection, which allows the model to move beyond linear CoT for deeper and more robust solutions. State-of-the-Art Performance: Establishes new SOTA on the OpenCompass leaderboard for open-source MLLMs in the < 40B class and demonstrates leading performance across STEM, chart analysis, grounding, and video benchmarks. High-Efficiency Training: Features an optimized training infrastructure powered by data packing and hybrid parallelism that delivers 34 end-to-end speedup."
        },
        {
            "title": "2 Architecture",
            "content": "Figure 2: The overall architecture of Ovis2.5. The overall architecture of Ovis2.5 is illustrated in Figure 2. Ovis2.5 retains the foundational design of the Ovis architecture (Lu et al., 2024b), comprising three core modules: Visual Tokenizer (VT): transformer-based component that extracts features from image patches. visual head then projects each patchs features onto discrete vocabulary of visual words, producing probability distribution over this vocabulary (i.e., probabilistic visual token). Visual Embedding Table (VET): Analogous to textual embedding table in LLMs, the VET stores dedicated embedding for each visual word, design which alleviates structural mismatches between modalities. The final visual embedding is computed by summing the tables embeddings, weighted by the probabilities from the VT. In other words, the embedding for visual token is the expected value of the visual word embeddings, under the probability distribution produced by the VT. LLM: pretrained open-source large language model that performs cross-modal understanding based on visual and textual embeddings to generate textual output. Table 1: Overview of the Ovis2.5 training process. Phase Modules VT VET LLM Data Composition / Pre. Stage P1 Pre. Stage P2 Pre. Stage P3 Post. Stage P1 Post. Stage P2 image caption OCR, image caption, grounding multimodal instruction dataset multimodal preference dataset multimodal RLVR dataset To further advance Ovis2.5s multimodal capabilities, we have incorporated the following architectural enhancements: Native Resolution Processing: Previous versions processed images by splitting them into fixed-size sub-images, which can disrupt global structure and fine-grained details. To overcome this, Ovis2.5 replaces the standard fixed-resolution ViT with native-resolution ViT (NaViT), enabling the direct processing of images at their native resolutions. This architecture integrates rotary position embeddings (RoPE, Su et al., 2024) into every ViT block to reinforce spatial awareness, proving especially effective for high-resolution images like complex charts. Our NaViT is initialized from the weights of siglip2-so400m-patch16-5121 (Tschannen et al., 2025). Upgraded LLM: We have replaced the Qwen2.5 (Yang et al., 2024) backbone with Qwen3 (Yang et al., 2025a). This upgrade leverages the superior deep-reasoning capabilities of Qwen3 to significantly boost Ovis2.5s performance on complex tasks and its overall multimodal proficiency."
        },
        {
            "title": "3 Model Training",
            "content": "The training pipeline for Ovis2.5 is structured into two primary stages: pre-training and post-training. The pre-training stage is subdivided into three distinct phases: Pre. Stage P1, Pre. Stage P2, and Pre. Stage P3. This is followed by post-training stage, which contains two phases: Post. Stage P1 and Post. Stage P2. Each phase serves distinct purpose, defined by its unique training configurations and data compositions. Table 1 provides high-level overview of this process, and the subsequent sections will detail each stage. 3.1 Pre-training Stage 3.1.1 Data Composition The training of Ovis2.5 is underpinned by large-scale, high-quality multimodal dataset, carefully assembled to support each phase of our training pipeline. This dataset is curated collection, sourced from both public datasets and our in-house data. Public sources include wide array of datasets such as COYO (Byeon et al., 2022), Laion (Schuhmann et al., 2022), Wukong (Gu et al., 2022), DataComp (Gadre et al., 2023), and SAM (Kirillov et al., 2023). To ensure data quality and diversity, we employ data processing pipelines tailored to each data type. Below, we outline several key components of the dataset. OCR Data. Recognizing that robust OCR is cornerstone of multimodal understanding, our corpus combines established public datasets with diverse collection of in-house and web-sourced imagery, including documents, charts, posters, and screenshots. For this collected data, an ensemble of MLLMs is used to generate annotations and synthesize questionanswer pairs. subsequent curation step then filters samples based on image resolution, language, and scene diversity to ensure high-quality, comprehensive OCR dataset. Grounding Data. To equip the model with precise object localization capabilities, our grounding data is sourced from two primary sources. First, we leverage public datasets with existing bounding box annotations, such as RefCoCo (Kazemzadeh et al., 2014b), to directly synthesize grounding instructions. Second, we develop an automated pipeline where state-of-the-art detection models identify and precisely localize entities within images, after which MLLMs generate corresponding question-answer pairs, effectively scaling our high-quality grounding data. Reasoning Data. To cultivate sophisticated reasoning, we construct diverse reasoning dataset. First, to establish the models foundational reasoning ability, we assemble large corpus of vanilla CoT data. We 1https://huggingface.co/google/siglip2-so400m-patch16-512 4 obtain this by collecting existing open-source CoT datasets and employing MLLMs to synthesize reasoning paths for QA pairs in our training set that originally contained only direct answers. Building on this foundation, and to encourage more complex cognition, we also use MLLMs to generate thinking-style data. This format, which explicitly incorporates <think>...</think> tags, is designed to teach the model advanced cognitive processes such as reflection and self-correction. Finally, to guarantee the quality of all synthesized data, we implement cross-verification labeling strategy, where multiple MLLMs assess and select the highest-quality samples for training."
        },
        {
            "title": "3.1.2 Training Strategy",
            "content": "The pre-training process is divided into the following three phases: P1: VET Pre-training. The primary goal of this phase is to train the Visual Embedding Table (VET). We utilize dataset of image-caption pairs in plain, non-dialogue format. To ensure stable learning, we initialize the ViT from siglip2-so400m-patch16-512 and freeze most parameters, limiting training exclusively to the final ViT layer, the visual head, and the VET. During this phase, input images are resized to total pixel count between 4482 and 8962 while maintaining their original aspect ratio. Since the pre-trained ViT lacks Rotary Position Embeddings (RoPE), we employ dynamic position embedding interpolation to handle these varying resolutions, keeping RoPE itself disabled for stability. P2: Multimodal Pre-training. This phase transitions to full-parameter training of all modules (VT, VET, and LLM) to establish core visual understanding and align the model with conversational formats. The training data is expanded to include conversational data from OCR, image captioning, and visual grounding tasks. Crucially, the supported resolution range is significantly expanded to total pixel count between 4482 and 17922 (approx. 200K to 3.2M pixels). To handle this increased spatial complexity, RoPE is activated in every ViT block, enhancing the models spatial awareness. P3: Multimodal Instruction Tuning. This phase continues full-parameter training, with refined focus on enhancing the models ability to follow diverse multimodal instructions. To this end, the training corpus is enriched with new input typesincluding text-only, multi-image, and videoand expanded to cover wide spectrum of domains, such as general QA, multilingual dialogue, OCR, chart analysis, knowledge-based QA, STEM, and medical. To foster deeper reasoning beyond linear CoT, we incorporate thinking-style samples annotated with Qwen3-compatible <think>...</think> tags, teaching the model to perform reflection and self-correction. The model continues to support the wide resolution range established in P2, allowing it to process high-resolution details. 3.2 Post-training Stage 3.2.1 Data Composition Direct Preference Optimization (DPO) Data. The DPO dataset covers multimodal preference data spanning text-only, single-image, multi-image, and video modalities. It consists of two main components: (1) For reasoning-oriented tasks, we perform inference using both vanilla CoT and thinking-style formats and compute verifiable score against ground-truth answers, thereby enhancing the models reasoning capabilities. (2) For general-purpose tasks, we leverage an MLLM-based scoring mechanism to improve the models performance across diverse scenarios. Reinforcement Learning with Verifiable Rewards (RLVR) Data. Our RLVR dataset is collected from wide range of domains. The primary focus is on open-source mathematical problems, with additional data including science QA and visual QA tasks involving logical reasoning. We also synthesize substantial amount of data in which more information is embedded in images rather than in text. To improve the quality of multiple-choice questions, we implement conversion process that transforms some of these questions into fill-in-the-blank format. This approach reduces the likelihood of correctly guessing answers without solid reasoning, behavior that could harm training. The entire dataset undergoes both quality and offline difficulty filtering. 3.2.2 Training Strategy The post-training process comprises the following two phases: P1: Multimodal DPO. In this phase, the entire Ovis2.5 modelincluding the vision modules and the LLMundergoes full-parameter training. We use Direct Preference Optimization (DPO) (Rafailov et al., 2023) as the primary preference objective, augmented with an auxiliary Negative Log-Likelihood (NLL) objective to stabilize optimization. For each query, group of candidate responses is generated using the final model checkpoint from the pre-training stage. Within this group, we then form multiple preference pairs according to their labels. Table 2: Performance of Ovis2.5-2B and comparison models on the OpenCompass suite. Abbreviations: MMB = MMBenchV11; MMS = MMStar; MMMU = MMMU-Val; HB = HallusionBench; OCR = OCRBench. Model MMB MMS MMMU MathVista HB AI2D OCR MMVet Avg Ovis2-2B Qwen2.5-VL-3B InternVL3-2B MiniCPM-V-4 Ovis2.5-2B 76.9 76.8 78.0 79. 79.2 56.7 56.3 61.1 62.8 69.8 45.6 51.2 48.7 51.2 59.8 64.1 61.2 57.6 66. 81.4 50.2 46.6 41.9 50.8 59.2 82.7 81.4 78.6 82.9 85.5 87.3 82.8 83.1 89. 88.1 58.3 60.0 67.0 68.0 67.9 65.2 64.5 64.5 69.0 73.9 P2: Multimodal Reinforcement Learning. This phase further improves reasoning using Group Relative Policy Optimization (GRPO) (Shao et al., 2024) on our RLVR dataset, building on the alignment established during the DPO phase. To focus optimization on high-level cognition while preserving general capabilities, we update only the LLM parameters and keep the vision modules frozen. Under this setup, GRPO optimizes the policy on verifiable, reasoning-centric tasks without degrading the models overall multimodal abilities."
        },
        {
            "title": "3.3 Infrastructure",
            "content": "Efficiently training large multimodal models like Ovis2.5 presents significant infrastructure challenges. We focus on two primary issues: (1) computational load imbalance caused by the varying data sizes of images, videos, and text, and (2) memory limitations of our previous training framework that restrict model scale and context length. By developing targeted optimizations for both data processing and parallelism, we improve the end-to-end training speed of Ovis2.5 by 3 to 4 times. Load Balancing via Data Packing. common inefficiency in training is padding, where samples of different lengths in batch are padded to uniform size. This leads to wasted computation and GPU idle time, especially with diverse multimodal data. To address this, we implement data packing strategy. This method combines multiple shorter samples into single, longer sequence during data preprocessing. As result, it minimizes padding, reduces wasted computation, and creates more balanced workload across GPUs, directly improving training throughput. Our experiments confirm that data packing significantly reduces training time while maintaining model accuracy. Hybrid Parallelism Framework. The increasing scale of our model architecture, particularly with the use of compute-intensive vision backbones, requires more advanced parallelism strategy. We develop hybrid parallelism framework for Ovis2.5 based on Megatron (Shoeybi et al., 2019). This framework combines three standard techniques: Data Parallelism (DP), Tensor Parallelism (TP), and Context Parallelism (CP). This approach effectively reduces the memory footprint for large-scale model training, leading to substantial improvements in training throughput and efficiency."
        },
        {
            "title": "4 Experiments",
            "content": "This section presents comprehensive evaluation of Ovis2.5. We first compare its overall multimodal performance with leading MLLMs. Subsequently, we conduct detailed assessment of its core capabilities, including multimodal reasoning, OCR and chart, visual grounding, and video understanding. Throughout this section, we use bold to denote the best and underline for the second-best performance among opensource models of comparable scales. We use officially reported scores whenever available; otherwise, scores come from our own evaluation and are marked with an asterisk (*). 4.1 Overall Performance We evaluate the overall performance of Ovis2.5 using the OpenCompass suite (Duan et al., 2025), which provides comprehensive score by averaging results across eight key benchmarks: MMBench (Liu et al., 2024b), MMStar (Chen et al., 2024), MMMU (Yue et al., 2024a), MathVista (Lu et al., 2024a), HallusionBench (Guan et al., 2024), AI2D (Kembhavi et al., 2016), OCRBench (Liu et al., 2024d), and MMVet (Yu et al., 2023). As presented in Tables 2 and 3, with average scores of 78.3 and 73.9, respectively, Ovis2.5-9B and Ovis2.5-2B both achieve state-of-the-art performance among open-source models at their respective scales. Beyond these aggregate scores, the models demonstrate robust performance on general benchmarks such as MMBench and MMStar, and exhibit strong capabilities in several specialized domains. Specifically, Ovis2.5-9B excels in complex reasoning tasks, attaining 71.2 on MMMU for college-level multi-discipline problems. It also scores 87.9 on OCRBench, underscoring its precise text recognition, and 6 Table 3: Performance of Ovis2.5-9B and comparison models on the OpenCompass suite. Abbreviations: MMB = MMBenchV11; MMS = MMStar; MMMU = MMMU-Val; HB = HallusionBench; OCR = OCRBench. Model Gemini-2.5-Pro GPT-4o Ovis2-8B Qwen2.5-VL-7B InternVL3-8B MiMo-VL-7B-RL-2508 Keye-VL-8B GLM-4.1V-9B-Thinking Ovis2.5-9B MMB MMS MMMU MathVista HB AI2D OCR MMVet Avg 88.3 86.0 83.6 82.2 82.1 83.9* 79.4* 85.3 84.9 73.6 70.2 64.6 64.1 68. 72.7* 75.5 72.9 72.4 74.7 72.9 57.4 58.0 62.2 70.6 71.4 68.0 71. 80.9 71.6 71.8 68.1 70.5 79.7* 80.7 80.7 83.4 64.1 57.0 56.3 51.9 49. 65.3* 67.0 63.7* 89.5 86.3 86.6 84.3 85.1 85.3* 86.7 87.9 65.1 87. 86.2 82.2 89.1 88.8 88.4 88.6 85.1 84.2 87.9 83.3 76.9 65.1 69.7 82. 73.4* 67.6* 66.2* 74.0 80.1 75.4 71.8 70.9 73.6 77.4* 76.7* 76.1* 78. Table 4: Performance of Ovis2.5-2B and comparison models on multimodal reasoning benchmarks. Abbreviations: MPro = MMMU-Pro; MathVerse = MathVerse Vison Only; LV = LogicVista; WM = WeMath; DM = DynaMath. Model MMMU MPro MathVista MathVerse MathVision LV WM DM Ovis2-2B Qwen2.5-VL-3B InternVL3-2B MiniCPM-V-4 Ovis2.5-2B 45.6 51.2 48.7 51.2 59.8 24.2 31.6 - - 39.3 64.1 61.2 57.6 66.9 81. 29.4 31.2 24.5 - 64.8 17.7 21.9 20.2 20.7 37.4 34.7 40.3 34.7 - 53. 9.9 22.9 22.9 32.7 50.8 10.0 13.2 14.8 14.2 26.5 65.1 on HallusionBench, indicating fewer hallucinations and higher reliability. Collectively, these results demonstrate that Ovis2.5 not only surpasses leading open-source models like Keye-VL (Yang et al., 2025b) and GLM-4.1V (Hong et al., 2025), but also narrows the gap to frontier proprietary models such as Gemini-2.5 (et al., 2025). Complementing these cross-model comparisons, Ovis2.5 also delivers substantial gains over its predecessor, Ovis2, further validating the effectiveness of our architectural refinements and training strategy. 4.2 Multimodal Reasoning We evaluate the multimodal reasoning capabilities of Ovis2.5 on comprehensive suite of benchmarksMMMU, MMMU-Pro (Yue et al., 2024b), MathVista, MathVerse (Zhang et al., 2024), MathVision (Wang et al., 2024), LogicVista (Xiao et al., 2024), WeMath (Qiao et al., 2024), and DynaMath (Zou et al., 2024)covering interdisciplinary academic reasoning, visual mathematics, logical reasoning, and dynamic problem solving. As detailed in Table 4 and Table 5, Ovis2.5 excels across reasoning domains. Specifically, Ovis2.5-9B It showcases leading capabilities in visual and structured mathematics among open-source models. achieves the top rank on both MathVista and WeMath, demonstrating its exceptional strength in visualcompositional and concept-integration tasks. Its dominance is further solidified by consistently ranking among the top two on all other math-focused benchmarks, including MathVerse, MathVision, LogicVista, and DynaMath. Beyond its mathematical prowess, the models proficiency in general academic reasoning is confirmed by strong scores of 71.2 on MMMU and 54.4 on the more challenging MMMU-Pro benchmark. Notably, the compact Ovis2.5-2B delivers state-of-the-art results within its size category, achieving substantial gains over comparable open-source models across both general reasoning and math-oriented tasks. 4.3 OCR & Chart The challenge of interpreting visual data, such as documents and charts, demands sophisticated cascade of skills from MLLMs, spanning from low-level OCR to high-level semantic reasoning. To rigorously evaluate these capabilities, we assess Ovis2.5 on suite of challenging benchmarks. On the large-scale, bilingual OCRBench v2 (Fu et al., 2024a), Ovis2.5 not only surpasses all leading open-source competitors but also outperforms the proprietary GPT-4o model. This state-of-the-art 7 Table 5: Performance of Ovis2.5-9B and comparison models on multimodal reasoning benchmarks. Abbreviations: MPro = MMMU-Pro; MathVerse = MathVerse Vison Only; LV = LogicVista; WM = WeMath; DM = DynaMath. Model Gemini-2.5-Pro GPT-4o Ovis2-8B Qwen2.5-VL-7B InternVL3-8B MiMo-VL-7B-RL-2508 Keye-VL-8B GLM-4.1V-9B-Thinking Ovis2.5-9B MMMU MPro MathVista MathVerse MathVision LV WM DM 74.7 72.9 57.4 58.0 62.2 70.6 71.4 68. 71.2 - - 34.9 38.3 42.3* 45.7* 39.0* 57.1 54.4 80.9 71. 71.8 68.1 70.5 79.7* 80.7 80.7 83.4 76.9 49.9 42.3 41.1 38.5 71.6* 59.8 68.8* 71.1 69.1 43.8 25.9 25.4 30.0 58.5* 46.0 49.4* 53.9 73.8 64. 39.4 47.9 44.5 78.0 50.6 27.2 36.2 39.5 56.3 48.5 20.4 21.8 25.7 65.6* 48.3* 64.5 37.3 54.8 60.7 38.9* 54.1* 63. 61.5 66.7 44.1 Table 6: Performance on OCR & chart benchmarks. Abbreviations: OCRv2 = OCRBench v2; CQA = ChartQA; DocVQA = DocVQA-Val; TextVQA = TextVQA-Val. Model GPT-4o Ovis2-8B Qwen2.5-VL-7B InternVL3-9B MiMo-VL-7B-RL-2508 Keye-VL-8B-Thinking GLM-4.1V-9B-Thinking Ovis2.5-2B Ovis2.5-9B OCRv2-EN OCRv2-CN CQA Pro CQA DocVQA TextVQA Avg 46.5 46.7 46.7 46.2* 50.9* 45.0* 60.5* 60.6 63.4 32.2 44.3 55.6 44.1* 37.3* 36.2* 57.4* 54.0 58. 56.2* 51.6 44.6* 50.2* 62.6* 46.9* 59.6* 53.1 63.8 85.8* 86.8 89.4* 87.4* 94.4 86.3 87.9* 92.2 92.9 90.9* 95.4 96.3* 93.7* 96.4* 88.6* 97.1* 95.3 96. 89.6* 92.5 88.4* 90.2* 89.1* 87.7* 91.0* 89.6 91.2 66.9* 69.6 70.2* 68.6* 71.8* 65.1* 75.6* 74.1 77.6 performance extends to complex chart analysis. We test our model on the newly introduced ChartQA Pro (Masry et al., 2025), which features diverse range of visualizations from conventional charts to complex infographics. Across these and other established benchmarksincluding ChartQA (Masry et al., 2022), DocVQA (Mathew et al., 2021), and TextVQA (Singh et al., 2019)our Ovis2.5-9B consistently demonstrates superior performance, attaining the top average score against its open-source peers, as detailed in Table 6. 4.4 Grounding Visual grounding, the task of precisely localizing objects from natural language descriptions, is cornerstone of advanced spatial reasoning in MLLMs. We benchmarked Ovis2.5 on the standard referring expression datasets: RefCOCO (Kazemzadeh et al., 2014a), RefCOCO+, and RefCOCOg (Mao et al., 2016). As detailed in Table 7, Ovis2.5 achieves state-of-the-art average score of 90.1 among compared open-source models. This leading performance is consistent across individual benchmarks. The model delivers top-tier results across nearly all splits of RefCOCO and RefCOCO+, frequently ranking first or second. Crucially, Ovis2.5 demonstrates pronounced advantage on the more demanding RefCOCOg dataseta benchmark known for complex descriptions of non-salient objects. Here, it outperforms all competitors on both the validation and test sets. This exceptional performance, particularly on RefCOCOg, showcases the models advanced understanding of visual-linguistic connections, proving its effectiveness for visual grounding tasks. 4.5 Multi-Image and Video Ovis2.5s proficiency extends to understanding complex multi-modal sequences, which we evaluated on comprehensive suite of multi-image and video benchmarks. In the multi-image domain, the models superior cross-image reasoning is evident. On benchmarks like BLINK (Fu et al., 2024b) and MMTBench (Ying et al., 2024), it achieves impressive scores of 67.3 and 69.3 respectively, outperforming other prominent models. 8 Table 7: Performance on visual grounding benchmarks. Model InternVL3-8B InternVL3-9B Qwen2.5-VL-7B MiMo-VL-7B-RL-2508 GLM-4.1V-9B-Thinking Ovis2.5-2B Ovis2.5-9B RefCOCO RefCOCO+ RefCOCOg val test-A test-B val test-A test-B val 92.5 91.8 90.0 92.8* 84.8* 92.8 92.7 94.6 93.2 92.5 93.8* 88.9* 94.6 94.3 88.0 86.6 85.4 86.7* 78.2* 89.7 89.7 88.2 86.4 84.2 89.2* 76.3* 86.9 87.7 92.5 91.0 89.1 91.6* 82.4* 91.0 92.1 81.8 79.9 76.9 81.6* 69.3* 81.5 83.6 test 90.0 88.5 87. 89.6 88.0 87.2 Avg 89.6 88.2 86.6 89.0* 88.1* 89.1* 80.8* 83.1* 81.1* 90.2 90.3 90.4 90. 89.6 90.1 Table 8: Performance on multi-image & video benchmarks. Abbreviations: MMT = MMT-Bench-Val; VM-w/ sub = VideoMME with subtitles; VM-w/o sub = VideoMME without subtitles; MLVU = MLVU (M-Avg); TC = TempCompass Model Ovis2-8B Qwen2.5-VL-7B InternVL3-9B MiMo-VL-7B-RL-2508 Keye-VL-8B-Thinking GLM-4.1V-9B-Thinking Ovis2.5-2B Ovis2.5-9B BLINK MMT VM-w/ sub VM-w/o sub MVBench MLVU TC 55.0 56.4 58.6 57.8* 52.0 65.1 65.7 67.3 66.4 63.6 65.4 66.8* 64.8* 67.6* 62.8 69.3 71.6 71.6 68.9 - 64.9* 73.6 63.6 72.8 68.0 65.1 66.7 70.8 67.7 68. 60.2 68.6 68.2 69.6 74.3 62.0* 57.6* - 66.7 69.7 76.4 70.2 70.8 57.9* 69.8* - 69.5 76.4 69.3 71.7 71.2* - 71.5 - 59.6 72.6 This advantage carries over into video understanding. On diverse set of video benchmarksincluding VideoMME (Fu et al., 2025), MVBench (Li et al., 2024), MLVU (Zhou et al., 2024), and TempCompass (Liu et al., 2024c)Ovis2.5 maintains strong and consistent performance across broader range of tasks. Taken together, these results underscore the models advanced ability to capture and reason over both spatial and temporal dynamics, establishing its strong competitiveness in sequence-level multimodal understanding."
        },
        {
            "title": "5 Conclusion",
            "content": "In this report, we present Ovis2.5. Building upon the foundational architecture of the Ovis series, this version introduces significant advances in both visual perception and multimodal reasoning. On the perception side, we integrate NaViT to process images at their native, variable resolutions, preserving both fine details and global structure that are essential for visually dense content such as charts. On the reasoning side, we introduce an advanced thinking mode paradigm. This capability, cultivated through training on reflection-oriented data, enables the model to perform self-correction and revision on complex tasks, offering controllable trade-off between latency and accuracy at inference time. Evaluated on wide spectrum of challenging benchmarks, Ovis2.5 establishes new state-of-the-art on OpenCompass for its parameter class and delivers leading results across domains including STEM, OCR and chart, visual grounding, and multi-image/video comprehension. Despite these gains, several promising avenues remain: (i) scaling perception to 4K-level high-resolution images while maintaining accuracy; (ii) handling long-input video with richer temporal reasoning; and (iii) tighter integration of tool use for action-augmented reasoning. We look forward to exploring these directions in our future work."
        },
        {
            "title": "6 Core Contributors",
            "content": "Shiyin Lu, Yang Li, Yu Xia, Yuwei Hu, Shanshan Zhao, Yanqing Ma, Zhichao Wei, Yinglun Li, Lunhao Duan"
        },
        {
            "title": "7 Contributors",
            "content": "Jianshan Zhao, Yuxuan Han, Haijun Li, Wanying Chen, Junke Tang, Chengkun Hou, Zhixing Du, Tianli Zhou, Wenjie Zhang, Huping Ding, Jiahe Li, Wen Li, Gui Hu, Yiliang Gu, Siran Yang, Jiamang Wang, Hailong Sun, Yibo Wang, Hui Sun, Jinlong Huang, Yuping He, Shengze Shi, Weihong Zhang, Guodong Zheng, Junpeng Jiang, Sensen Gao, Yi-Feng Wu, Sijia Chen, Yuhui Chen"
        },
        {
            "title": "8 Project Leaders",
            "content": "Qing-Guo Chen, Zhao Xu, Weihua Luo, Kaifu Zhang References Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. Minwoo Byeon, Beomhee Park, Haecheon Kim, Sungjun Lee, Woonhyuk Baek, and Saehoon Kim. Coyo-700m: Image-text pair dataset. https://github.com/kakaobrain/coyo-dataset, 2022. Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, et al. Are we on the right way for evaluating large vision-language models? In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. Mostafa Dehghani, Basil Mustafa, Josip Djolonga, Jonathan Heek, Matthias Minderer, Mathilde Caron, Andreas Steiner, Joan Puigcerver, Robert Geirhos, Ibrahim Alabdulmohsin, Avital Oliver, Piotr Padlewski, Alexey Gritsenko, Mario Lučić, and Neil Houlsby. Patch pack: Navit, vision transformer for any aspect ratio and resolution, 2023. URL https://arxiv.org/abs/2307.06304. Haodong Duan, Xinyu Fang, Junming Yang, Xiangyu Zhao, Yuxuan Qiao, Mo Li, Amit Agarwal, Zhe Chen, Lin Chen, Yuan Liu, Yubo Ma, Hailong Sun, Yifan Zhang, Shiyin Lu, Tack Hwa Wong, Weiyun Wang, Peiheng Zhou, Xiaozhe Li, Chaoyou Fu, Junbo Cui, Xiaoyi Dong, Yuhang Zang, Pan Zhang, Jiaqi Wang, Dahua Lin, and Kai Chen. Vlmevalkit: An open-source toolkit for evaluating large multi-modality models, 2025. URL https://arxiv.org/abs/2407.11691. Gheorghe Comanici et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities, 2025. URL https://arxiv.org/abs/2507.06261. Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 2410824118, 2025. Ling Fu, Biao Yang, Zhebin Kuang, Jiajun Song, Yuzhe Li, Linghao Zhu, Qidi Luo, Xinyu Wang, Hao Lu, Mingxin Huang, Zhang Li, Guozhi Tang, Bin Shan, Chunhui Lin, Qi Liu, Binghong Wu, Hao Feng, Hao Liu, Can Huang, Jingqun Tang, Wei Chen, Lianwen Jin, Yuliang Liu, and Xiang Bai. Ocrbench v2: An improved benchmark for evaluating large multimodal models on visual text localization and reasoning, 2024a. URL https://arxiv.org/abs/2501.00321. Xingyu Fu, Yushi Hu, Bangzheng Li, Yu Feng, Haoyu Wang, Xudong Lin, Dan Roth, Noah Smith, Wei-Chiu Ma, and Ranjay Krishna. Blink: Multimodal large language models can see but not perceive. In European Conference on Computer Vision, pp. 148166. Springer, 2024b. Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, et al. Datacomp: In search of the next generation of multimodal datasets. Advances in Neural Information Processing Systems, 36:2709227112, 2023. Jiaxi Gu, Xiaojun Meng, Guansong Lu, Lu Hou, Niu Minzhe, Xiaodan Liang, Lewei Yao, Runhui Huang, Wei Zhang, Xin Jiang, et al. Wukong: 100 million large-scale chinese cross-modal pre-training benchmark. Advances in Neural Information Processing Systems, 35:2641826431, 2022. 10 Tianrui Guan, Fuxiao Liu, Xiyang Wu, Ruiqi Xian, Zongxia Li, Xiaoyu Liu, Xijun Wang, Lichang Chen, Furong Huang, Yaser Yacoob, et al. Hallusionbench: an advanced diagnostic suite for entangled language hallucination and visual illusion in large vision-language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1437514385, 2024. Wenyi Hong, Wenmeng Yu, Xiaotao Gu, Guo Wang, Guobing Gan, Haomiao Tang, Jiale Cheng, Ji Qi, Junhui Ji, Lihang Pan, Shuaiqi Duan, Weihan Wang, Yan Wang, Yean Cheng, Zehai He, Zhe Su, Zhen Yang, Ziyang Pan, Aohan Zeng, Baoxu Wang, Boyan Shi, Changyu Pang, Chenhui Zhang, Da Yin, Fan Yang, Guoqing Chen, Jiazheng Xu, Jiali Chen, Jing Chen, Jinhao Chen, Jinghao Lin, Jinjiang Wang, Junjie Chen, Leqi Lei, Letian Gong, Leyi Pan, Mingzhi Zhang, Qinkai Zheng, Sheng Yang, Shi Zhong, Shiyu Huang, Shuyuan Zhao, Siyan Xue, Shangqin Tu, Shengbiao Meng, Tianshu Zhang, Tianwei Luo, Tianxiang Hao, Wenkai Li, Wei Jia, Xin Lyu, Xuancheng Huang, Yanling Wang, Yadong Xue, Yanfeng Wang, Yifan An, Yifan Du, Yiming Shi, Yiheng Huang, Yilin Niu, Yuan Wang, Yuanchang Yue, Yuchen Li, Yutao Zhang, Yuxuan Zhang, Zhanxiao Du, Zhenyu Hou, Zhao Xue, Zhengxiao Du, Zihan Wang, Peng Zhang, Debing Liu, Bin Xu, Juanzi Li, Minlie Huang, Yuxiao Dong, and Jie Tang. Glm-4.1v-thinking: Towards versatile multimodal reasoning with scalable reinforcement learning, 2025. URL https://arxiv.org/abs/2507.01006. Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and Tamara Berg. ReferItGame: Referring to objects in photographs of natural scenes. In Alessandro Moschitti, Bo Pang, and Walter Daelemans (eds.), Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 787798, Doha, Qatar, October 2014a. Association for Computational Linguistics. doi: 10.3115/v1/D14-1086. URL https://aclanthology.org/D14-1086. Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and Tamara Berg. Referitgame: Referring to objects in photographs of natural scenes. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pp. 787798, 2014b. Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi. diagram is worth dozen images. In European conference on computer vision, pp. 235251. Springer, 2016. Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 40154026, 2023. Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al. Mvbench: comprehensive multi-modal video understanding benchmark. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2219522206, 2024. Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, January 2024a. URL https://llava-vl.github.io/ blog/2024-01-30-llava-next/. Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? In European conference on computer vision, pp. 216233. Springer, 2024b. Yuanxin Liu, Shicheng Li, Yi Liu, Yuxiang Wang, Shuhuai Ren, Lei Li, Sishuo Chen, Xu Sun, and Lu Hou. Tempcompass: Do video llms really understand videos? arXiv preprint arXiv:2403.00476, 2024c. Yuliang Liu, Zhang Li, Mingxin Huang, Biao Yang, Wenwen Yu, Chunyuan Li, Xu-Cheng Yin, Cheng-Lin Liu, Lianwen Jin, and Xiang Bai. Ocrbench: on the hidden mystery of ocr in large multimodal models. Science China Information Sciences, 67(12):220102, 2024d. Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. In The Twelfth International Conference on Learning Representations, 2024a. Shiyin Lu, Yang Li, Qing-Guo Chen, Zhao Xu, Weihua Luo, Kaifu Zhang, and Han-Jia Ye. Ovis: Structural embedding alignment for multimodal large language model. arXiv:2405.20797, 2024b. Junhua Mao, Jonathan Huang, Alexander Toshev, Oana Camburu, Alan Yuille, and Kevin Murphy. In Proceedings of the IEEE Generation and comprehension of unambiguous object descriptions. conference on computer vision and pattern recognition, pp. 1120, 2016. 11 Ahmed Masry, Do Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. ChartQA: benchmark for question answering about charts with visual and logical reasoning. In Findings of the Association for Computational Linguistics: ACL 2022, pp. 22632279, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.findings-acl.177. URL https://aclanthology.org /2022.findings-acl.177. Ahmed Masry, Mohammed Saidul Islam, Mahir Ahmed, Aayush Bajaj, Firoz Kabir, Aaryaman Kartha, Md Tahmid Rahman Laskar, Mizanur Rahman, Shadikur Rahman, Mehrad Shahmohammadi, Megh Thakkar, Md Rizwan Parvez, Enamul Hoque, and Shafiq Joty. Chartqapro: more diverse and challenging benchmark for chart question answering, 2025. URL https://arxiv.org/abs/2504.05506. Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. Docvqa: dataset for vqa on document images. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pp. 22002209, 2021. Runqi Qiao, Qiuna Tan, Guanting Dong, Minhui Wu, Chong Sun, Xiaoshuai Song, Zhuoma Gongque, Shanglin Lei, Zhe Wei, Miaoxuan Zhang, Runfeng Qiao, Yifan Zhang, Xiao Zong, Yida Xu, Muxi Diao, Zhimin Bao, Chen Li, and Honggang Zhang. We-math: Does your large multimodal model achieve human-like mathematical reasoning? CoRR, abs/2407.01284, 2024. doi: 10.48550/ARXIV.2407.01284. URL https://doi.org/10.48550/arXiv.2407.01284. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in neural information processing systems, 36:5372853741, 2023. Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in neural information processing systems, 35:2527825294, 2022. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-lm: Training multi-billion parameter language models using model parallelism. arXiv preprint arXiv:1909.08053, 2019. Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. Towards vqa models that can read. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 83178326, 2019. Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. Michael Tschannen, Alexey Gritsenko, Xiao Wang, Muhammad Ferjad Naeem, Ibrahim Alabdulmohsin, Nikhil Parthasarathy, Talfan Evans, Lucas Beyer, Ye Xia, Basil Mustafa, Olivier Hénaff, Jeremiah Harmsen, Andreas Steiner, and Xiaohua Zhai. Siglip 2: Multilingual vision-language encoders with improved semantic understanding, localization, and dense features, 2025. URL https://arxiv.org/ab s/2502.14786. Ke Wang, Junting Pan, Weikang Shi, Zimu Lu, Houxing Ren, Aojun Zhou, Mingjie Zhan, and Hongsheng Li. Measuring multimodal mathematical reasoning with math-vision dataset. Advances in Neural Information Processing Systems, 37:9509595169, 2024. Yijia Xiao, Edward Sun, Tianyu Liu, and Wei Wang. Logicvista: Multimodal llm logical reasoning benchmark in visual contexts, 2024. URL https://arxiv.org/abs/2407.04973. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2024. 12 An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jing Zhou, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, and Zihan Qiu. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025a. Biao Yang, Bin Wen, Changyi Liu, Chenglong Chu, Chengru Song, Chongling Rao, Chuan Yi, Da Li, Dunju Zang, Fan Yang, Guorui Zhou, Hao Peng, Haojie Ding, Jiaming Huang, Jiangxia Cao, Jiankang Chen, Jingyun Hua, Jin Ouyang, Kaibing Chen, Kaiyu Jiang, Kaiyu Tang, Kun Gai, Shengnan Zhang, Siyang Mao, Sui Huang, Tianke Zhang, Tingting Gao, Wei Chen, Wei Yuan, Xiangyu Wu, Xiao Hu, Xingyu Lu, Yang Zhou, Yi-Fan Zhang, Yiping Yang, Yulong Chen, Zhenhua Wu, Zhenyu Li, Zhixin Ling, Ziming Li, Dehua Ma, Di Xu, Haixuan Gao, Hang Li, Jiawei Guo, Jing Wang, Lejian Ren, Muhao Wei, Qianqian Wang, Qigen Hu, Shiyao Wang, Tao Yu, Xinchen Luo, Yan Li, Yiming Liang, Yuhang Hu, Zeyi Lu, Zhuoran Yang, and Zixing Zhang. Kwai keye-vl technical report, 2025b. URL https://arxiv.org/abs/2507.01949. Kaining Ying, Fanqing Meng, Jin Wang, Zhiqian Li, Han Lin, Yue Yang, Hao Zhang, Wenbo Zhang, Yuqi Lin, Shuo Liu, et al. Mmt-bench: comprehensive multimodal benchmark for evaluating large vision-language models towards multitask agi. arXiv preprint arXiv:2404.16006, 2024. Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities. arXiv preprint arXiv:2308.02490, 2023. Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 95569567, 2024a. Xiang Yue, Tianyu Zheng, Yuansheng Ni, Yubo Wang, Kai Zhang, Shengbang Tong, Yuxuan Sun, Botao Yu, Ge Zhang, Huan Sun, et al. Mmmu-pro: more robust multi-discipline multimodal understanding benchmark. arXiv preprint arXiv:2409.02813, 2024b. Zihao Yue, Zhenru Lin, Yifan Song, Weikun Wang, Shuhuai Ren, Shuhao Gu, Shicheng Li, Peidian Li, Liang Zhao, Lei Li, Kainan Bao, Hao Tian, Hailin Zhang, Gang Wang, Dawei Zhu, Cici, Chenhong He, Bowen Ye, Bowen Shen, Zihan Zhang, Zihan Jiang, Zhixian Zheng, Zhichao Song, Zhenbo Luo, Yue Yu, Yudong Wang, Yuanyuan Tian, Yu Tu, Yihan Yan, Yi Huang, Xu Wang, Xinzhe Xu, Xingchen Song, Xing Zhang, Xing Yong, Xin Zhang, Xiangwei Deng, Wenyu Yang, Wenhan Ma, Weiwei Lv, Weiji Zhuang, Wei Liu, Sirui Deng, Shuo Liu, Shimao Chen, Shihua Yu, Shaohui Liu, Shande Wang, Rui Ma, Qiantong Wang, Peng Wang, Nuo Chen, Menghang Zhu, Kangyang Zhou, Kang Zhou, Kai Fang, Jun Shi, Jinhao Dong, Jiebao Xiao, Jiaming Xu, Huaqiu Liu, Hongshen Xu, Heng Qu, Haochen Zhao, Hanglong Lv, Guoan Wang, Duo Zhang, Dong Zhang, Di Zhang, Chong Ma, Chang Liu, Can Cai, and Bingquan Xia. Mimo-vl technical report, 2025. URL https://arxiv.org/abs/2506.03569. Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Yu Qiao, et al. Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems? In European Conference on Computer Vision, pp. 169186. Springer, 2024. Junjie Zhou, Yan Shu, Bo Zhao, Boya Wu, Shitao Xiao, Xi Yang, Yongping Xiong, Bo Zhang, Tiejun Huang, and Zheng Liu. Mlvu: comprehensive benchmark for multi-task long video understanding. arXiv e-prints, pp. arXiv2406, 2024. Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen Duan, Weijie Su, Jie Shao, Zhangwei Gao, Erfei Cui, Xuehui Wang, Yue Cao, Yangzhou Liu, Xingguang Wei, Hongjie Zhang, Haomin Wang, Weiye Xu, Hao Li, Jiahao Wang, Nianchen Deng, Songze Li, Yinan He, Tan Jiang, Jiapeng Luo, Yi Wang, Conghui He, Botian Shi, Xingcheng Zhang, Wenqi Shao, Junjun He, Yingtong Xiong, Wenwen Qu, Peng Sun, Penglong Jiao, Han Lv, Lijun Wu, Kaipeng Zhang, Huipeng Deng, Jiaye Ge, Kai Chen, Limin Wang, Min Dou, Lewei Lu, Xizhou Zhu, Tong Lu, Dahua Lin, Yu Qiao, Jifeng Dai, and Wenhai Wang. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models, 2025. URL https://arxiv.org/abs/2504.10479. Chengke Zou, Xingang Guo, Rui Yang, Junyu Zhang, Bin Hu, and Huan Zhang. Dynamath: dynamic visual benchmark for evaluating mathematical reasoning robustness of vision language models, 2024."
        },
        {
            "title": "A Qualitative Examples",
            "content": "In this section, we present variety of qualitative examples showcasing the capabilities and strengths of the Ovis2.5 model through responses to diverse prompts. A.1 World Knowledge Prompts: What plant is this? Model Responses: <think>...</think> This plant is baobab tree (scientific name Adansonia digitata). Baobab trees are iconic in many African savannas, known for their massive trunks, which store water, and their distinctive rounded canopies. They play vital ecological roles and hold cultural significance in regions where they grow. Figure 3: An example illustrating the models ability to identify plant species. Prompts: Where could this possibly be? Model Responses: <think>...</think> This image likely depicts coastal area in Istanbul, Turkey, specifically near the Bosphorus Strait. Heres why: 1. Waterway & Boats: The wide, deep blue water with ferry-like vessels (common for Istanbuls Bosphorus crossings) and smaller boats matches the bustling maritime activity of the Bosphorus. 2. Landscape & Architecture: The hilly terrain with dense residential buildings (typical of Istanbuls shoreline) and the mix of greenery and urban development aligns with the citys geography. 3. Context of Water Traffic: Istanbul is renowned for its busy waterways connecting the Black Sea and the Sea of Marmara via the Bosphorus, with frequent ferries and tour boatsexactly what the image shows. While other coastal regions (e.g., parts of Greece or Mediterranean cities) could share some traits, Istanbuls unique combination of water traffic, topography, and architectural style makes this the most plausible location. Figure 4: An example illustrating the models ability to recognize geographic locations. 15 A.2 Math Problem Solving Prompts: Belinda is making patterns with toothpicks according to the schema of the figure. How many toothpicks does Belinda add to the 30th pattern to make the 31 st? Model Responses: <think> Okay, lets try to figure out this problem step by step. First, we need to understand how the number of toothpicks changes from one pattern to the next. Lets start by counting the toothpicks in each pattern. First, lets recall that these are square patterns. The first pattern (n=1) is single square. square has 4 sides, so thats 4 toothpicks. Wait, but lets check the second pattern. The second pattern (n=2) is 2x2 grid of squares. How many toothpicks does that have? Lets count: horizontally, there are 3 rows of toothpicks, each with 2 segments, so 3*2 = 6. Vertically, there are 3 columns of toothpicks, each with 2 segments, so 3*2 = 6. Total is 12. Wait, or maybe another way: for an grid of squares, the number of horizontal toothpicks is (n+1)*n, and vertical toothpicks is also (n+1)*n. So total toothpicks would be 2*(n+1)*n. Lets check: For n=1: 2*(2)*1 = 4. Correct. For n=2: 2*(3)*2 = 12. Lets count: the second pattern has 2 rows and 2 columns of squares, so horizontally, there are 3 lines (top, middle, bottom) each with 2 toothpicks, so 3*2=6. Vertically, 3 lines (left, middle, right) each with 2 toothpicks, so 3*2=6. Total 12. Correct. [Remaining reasoning omitted for brevity.] </think> To determine how many toothpicks Belinda adds to the 30th pattern to make the 31st, we first analyze the pattern of toothpick counts. Step 1: Identify the formula for the n-th pattern The patterns are square grids of size n. For an grid of squares: Horizontal toothpicks: There are + 1 horizontal lines, each with segments. Thus, total horizontal toothpicks = n(n + 1). Vertical toothpicks: Similarly, there are + 1 vertical lines, each with segments. Thus, total vertical toothpicks = n(n + 1). Adding these, the total number of toothpicks in the n-th pattern is: (n) = 2n(n + 1) Step 2: Find the difference between consecutive patterns To find how many toothpicks are added from the n-th to the (n + 1)-th pattern, calculate (n + 1) (n): (n + 1) (n) = 2(n + 1)(n + 2) 2n(n + 1) = 2(n + 1) [(n + 2) n] = 2(n + 1)(2) = 4(n + 1) Step 3: Apply the formula to the 30th to 31st pattern For the transition from the 30th to the 31st pattern, = 30. Substitute = 30 into the difference formula: Thus, Belinda adds 124 toothpicks to the 30th pattern to make the 31st. (31) (30) = 4(30 + 1) = 4 31 = 124 Figure 5: An example illustrating the models ability to solve combinatorial pattern problems. 16 Prompts: The ratio of the radii of two concentric circles is 1 : 3. The line AC diameter of the biggest circle. chord BC of the big circle touches the small circle (see diagram). The line AB has length 12. How big is the radius of the big circle? Model Responses: <think>...</think> To solve the problem, we start by analyzing the given information and using geometric properties: Step 1: Define Variables and Key Properties Let the center of the two concentric circles be O. Let the radius of the small circle be r, so the radius of the big circle is 3r (since their radii are in the ratio 1 : 3). AC is diameter of the big circle, so AC = 2 3r = 6r. BC is chord of the big circle that is tangent to the small circle. For tangent to circle, the radius to the point of tangency is perpendicular to the tangent. Thus, if is the point where BC touches the small circle, OT BC and OT = (radius of the small circle). Step 2: Use the Right Angle in Semicircle Since AC is diameter of the big circle, by the Thales theorem, ABC = 90 (angle in semicircle is right angle). Thus, ABC is right triangle with ABC = 90. Step 3: Analyze OT In OT B: OB = 3r (radius of the big circle). OT = (radius of the small circle). OT BC, so OT is right triangle. By the Pythagorean theorem: BT = OB2 OT 2 = (3r)2 r2 = p9r2 r2 = 8r2 = 2r 2 Since OT BC and is the center of the circle, OT bisects BC (a perpendicular from the center to chord bisects the chord). Thus BC = 2 BT = 2 2r 2 = 4r 2. Step 4: Apply Pythagoras to ABC In right triangle ABC: We know AB = 12, BC = 4r Simplify: Solve for r2: AB2 + BC2 = AC2 2, and AC = 6r. Substitute these values: 122 + (4r 2)2 = (6r) 144 + 32r2 = 36r2 144 = 4r2 r2 = 36 = 6 (since radius is positive) Step 5: Find the Radius of the Big Circle The radius of the big circle is 3r. Substituting = 6: 3r = 3 6 = 18 Thus, the radius of the big circle is 18. Figure 6: An example illustrating the models ability to solve geometric reasoning problems. 17 A.3 Scientific Question Answering: Physics Prompts: In figure, material is water and material is glass with index of refraction 1.52. find the directions of the reflected and refracted rays. Model Responses: <think>...</think> To determine the directions of the reflected and refracted rays, we use the law of reflection for the reflected ray and Snells Law for the refracted ray. Step 1: Reflected Ray (Law of Reflection) The law of reflection states that the angle of incidence (θa) equals the angle of reflection (θr): θr = θa Given θa = 60, the reflected ray makes an angle of 60 with the normal, on the same side as the incident ray. Step 2: Refracted Ray (Snells Law) Snells Law relates the angles of incidence and refraction to the indices of refraction of the two media: na sin θa = nb sin θb where: na = 1.33 (index of refraction of water, material a), nb = 1.52 (index of refraction of glass, material b), θa = 60 (angle of incidence in water), θb = angle of refraction in glass (what we need to find). Rearrange Snells Law to solve for sin θb: sin θb = na nb sin θa Substitute the known values: sin θb = 1.33 1.52 sin(60) Calculate sin(60) = 3 2 0.8660, then: 1.33 1.52 0.8660 0.875 0.8660 0.758 sin θb = Take the inverse sine to find θb: Final Directions θb = arcsin(0.758) 49.3 Reflected ray: Makes an angle of 60 with the normal, on the same side as the incident ray. Refracted ray: Makes an angle of approximately 49.3 with the normal, on the glass side (since nb > na, the ray bends toward the normal). Thus, the reflected ray has an angle of 60 with the normal, and the refracted ray has an angle of approximately 49.3 with the normal. Figure 7: An example illustrating the models ability to solve physics refraction and reflection problem. 18 A.4 Scientific Question Answering: Biology Prompts: The pedigree in shows the mode of inheritance of human disease that is associated with mutations in Gene A. Note: Individuals marrying into this family DO NOT have the diseaseassociated allele unless shaded and the pedigree is completely penetrant. What is the genotype(s) of Individual 9? Model Responses: <think>...</think> To determine the genotype of Individual 9, we first identify the mode of inheritance using the pedigree: Step 1: Determine Mode of Inheritance Affected individuals are primarily males (shaded squares: 3, 7, 11, 18) and one female (shaded circle: 12). In - linked recessive inheritance, males are more frequently affected (they have only one chromosome, so one recessive allele causes the disease), while females require two recessive alleles to be affected. This matches the pedigree: Affected males inherit the recessive allele on their chromosome from their mother (who may be carrier). Affected females (e.g., 12) inherit recessive alleles from both parents (father and mother). Step 2: Analyze Genotypes of Parents of Individual 9 Individual 9 is female (unshaded, not affected) and daughter of: Individual 3 (affected male): Genotype = aY (has the recessive allele on his single chromosome). Individual 4 (unshaded female): To produce an affected son (Individual 11, aY ), Individual 4 must be carrier (X AX a)she can pass to her son. Step 3: Determine Genotype of Individual 9 Individual 9 is daughter of aY (Individual 3) and AX (Individual 4). When these parents reproduce: Daughters inherit an chromosome from each parent. From Individual 3 (X aY ): Daughter inherits a. From Individual 4 (X AX a): Daughter can inherit (dominant allele) or (recessive allele). Since Individual 9 is unshaded (not affected), she cannot have two recessive alleles (X aX a). Thus, she must inherit from Individual 4 and from Individual 3. Thus, the genotype of Individual 9 is AX a. Figure 8: An example illustrating the models ability to determine genetic inheritance patterns in biology problem. 19 A.5 OCR Prompts: Find out the value of Calories/Energy per 100g/ml, Calories/Energy per serving, Calories/Energy % daily value/intake, Total Fat per 100g/ml, Total Fat per serving, Total Fat % daily value/intake, Carbohydrate per 100g/ml, Carbohydrate per serving, Carbohydrate % daily value/intake, Protein per 100g/ml, Protein per serving, Protein % daily value/intake, Serving Size stated in the image. Extract the required information using the text in the image directly, and return the result in dict with keys CE-P1, CE-PS, CE-D, TF-P1, TF-PS, TF-D, CAR-P1, CAR-PS, CAR-D, PRO-P1, PRO-PS, PRO-D, SS. Your answer should be in the JSON format. Model Responses: { } \"CE-P1\": \"273kJ/65kcal\", \"CE-PS\": \"546kJ/130kcal\", \"CE-D\": \"7%\", \"TF-P1\": \"3.7g\", \"TF-PS\": \"7.4g\", \"TF-D\": \"11%\", \"CAR-P1\": \"4.7g\", \"CAR-PS\": \"9.5g\", \"CAR-D\": \"4%\", \"PRO-P1\": \"3.5g\", \"PRO-PS\": \"7.0g\", \"PRO-D\": \"14%\", \"SS\": \"200ml\" Figure 9: An example illustrating the models OCR capability. 20 Prompts: According to the information in the image, please pair the corresponding keys and values below: Keys that need to be paired are LOT #, STUDY DIRECTOR /DEPARTMENT, NO. OF CONTAINERS, SPECIAL HANDLING REQUIREMENTS, PHYSICAL DESCRIPTION, GROSS WEIGHT, SHIPPED VIA, DATES. Values that need to be paired are Ray David, 03 /01 /90, Clear brown liquid, Store refrigerated in dark., Direct Exress, Not provided, 012590, One. Extract the required information in the text, and return the result in dict directly. Your answer should be in the JSON format. Model Responses: { } \"LOT #\": \"012590\", \"STUDY DIRECTOR /DEPARTMENT\": \"Ray David\", \"NO. OF CONTAINERS\": \"one\", \"SPECIAL HANDLING REQUIREMENTS\": \"Store refrigerated in dark.\", \"PHYSICAL DESCRIPTION\": \"Clear brown liquid\", \"GROSS WEIGHT\": \"Not provided\", \"SHIPPED VIA\": \"Direct Exress\", \"DATES\": \"03/01/90\" Figure 10: An example illustrating the models OCR capability. 21 Prompts: Spot all the text in the image with word-level. Please provide the bounding box coordinates. Model Responses: In the image, <ref>PIZZA</ref> is located at <box>(0.156,0.116),(0.474,0.466)</box>, <ref>PORT</ref> is located at <box>(0.502,0.134),(0.712,0.466)</box>, <ref>BOTTLE</ref> is located at <box>(0.182,0.620),(0.522,0.846)</box>, <ref>SHOP</ref> is located at <box>(0.522,0.604),(0.712,0.754)</box>, <ref>BREWING</ref> is located at <box>(0.304,0.772),(0.522,0.948)</box>, <ref>COMPANY</ref> is located at <box>(0.522,0.754),(0.678,0.936)</box> Figure 11: An example illustrating the models text localization capability. The boxes are not part of the original image; they were added to visualize the coordinates generated by the model. 22 A.6 Chart Prompts: Which country in Africa has mean male height between 170 and 175cm but child mortality rate between 10 and 15%? Model Responses: <think>...</think> To determine the African country meeting the criteria, follow these steps: 1. Identify the relevant axes: X-axis: Mean male height (170175 cm). Y-axis: Child mortality rate (1015%). Color: Purple (represents Africa in the legend). 2. Locate purple dots within the specified ranges: Check for purple points where the x-value (height) is between 170 and 175 cm, and the y-value (child mortality) is between 10% and 15%. 3. Analyze the data point: The country Senegal has mean male height of approximately 174 cm (within 170175 cm) and child mortality rate of approximately 14% (within 1015%). Thus, the African country that meets the criteria is Senegal. Figure 12: An example illustrating the models ability to perform conditional search on chart. 23 Prompts: Which Christmas cookie requires the most sugar? Model Responses: <think>...</think> To determine which Christmas cookie requires the most sugar, we analyze the flow lines connecting Cookies to Sugar in the infographic. The thickness or prominence of the flow line indicates the amount of sugar used for each cookie. Among the listed cookies (Chocolate Crinkles, Snickerdoodles, PB Blossoms, Gingerbread, Peppermint Chip, Spritz), the flow line from Chocolate Crinkles to Sugar is the most substantial. Additionally, Chocolate Crinkles are described as having snowy powdered sugar coating, which implies significant use of sugar. Thus, Chocolate Crinkles require the most sugar. Figure 13: An example illustrating the models ability to perform comparative analysis on flow chart. 24 Prompts: Looking at the CDC autism data from 2000-2018, list all states that have ever reported male autism prevalence rates (over the entire period) that were more than double South Carolinas male autism prevalence rate in 2012. Model Responses: <think>...</think> To solve this, follow these steps: Step 1: Find South Carolinas male autism prevalence in 2012 From the South Carolina graph, the male (orange dot) prevalence in 2012 is 20. Step 2: Calculate double South Carolinas 2012 male rate Double of 20 is 20 2 = 40. We need states with any male autism prevalence rate greater than 40 over the 20002018 period. Step 3: Analyze each states male prevalence (orange dots) Examine the male prevalence (orange dots) for all states: North Carolina: Male prevalence reaches 41 (exceeds 40). New Jersey: Male prevalence reaches 46 (exceeds 40). Minnesota: Male prevalence reaches 44 (exceeds 40). California: Male prevalence reaches 64 (exceeds 40). Other states maximum male prevalence (e.g., Arizona: 38; Georgia: 35; etc.) do not exceed 40. Thus, the states with male autism prevalence rates more than double South Carolinas 2012 rate are North Carolina, New Jersey, Minnesota, and California. Figure 14: An example illustrating the models ability to execute multi-step analysis across panel of charts. 25 A.7 Grounding Prompts: How can locate the <ref>airplane</ref> in this image? Please provide the bounding box coordinates. Model Responses: The <ref>airplane</ref> is found within <box>(0.613,0.085),(0.632,0.107)</box> of the image. Figure 15: An example illustrating the models ability to ground small object. The boxes are not part of the original image; they were added to visualize the coordinates generated by the model. Prompts: How can locate the <ref>the green tent in the middle</ref> in this image? Please provide the bounding box coordinates. Model Responses: The <ref>the green tent in the middle</ref> is positioned at <box>(0.422,0. 483),(0.535,0.613)</box>. Figure 16: An example illustrating the models ability to ground referring object. The boxes are not part of the original image; they were added to visualize the coordinates generated by the model. Prompts: Describe this image with grounded objects. Please provide the bounding box coordinates. Model Responses: The image features serene scene with <ref>three birds</ref>[<box>(0.40 1,0.526),(0.430,0.557)</box>,<box>(0.489,0.494),(0.516,0.526)</box>,<box>(0.296, 0.529),(0.324,0.576)</box>] flying in formation against backdrop of clear blue sky. The <ref>moon</ref><box>(0.253,0.529),(0.378,0.702)</box> is prominently visible, partially obscured by one of the birds, creating striking silhouette effect. In the upper left corner, there are <ref>tree branches with leaves</ref><box>(0.000,0.000),(0.322,0.830)</box> that add depth and frame to the composition. The overall atmosphere is peaceful and evocative of tranquil evening. Figure 17: An example illustrating the models ability to caption with grounding. The boxes are not part of the original image; they were added to visualize the coordinates generated by the model. Prompts: Where can find <ref>hot air balloon</ref> in this image. Please provide the point coordinates. Model Responses: The <ref>hot air balloon</ref> is located at [<point>(0.084,0.192)</point>,<point>(0.336, 0.323)</point>,<point>(0.345,0.722)</point>,<point>(0.596,0.188)</point>,<point> (0.616,0.865)</point>,<point>(0.919,0.573)</point>]. Figure 18: An example illustrating the models ability to ground with point. The points are not part of the original image; they were added to visualize the coordinates generated by the model. A.8 Multi-Images Prompts: Given the following images of art paintings, use the first image as the reference image, and determine which one of the second or the third image shares the same style as the reference image? Model Responses: <think>...</think> The first image, with its vibrant and dynamic brushstrokes, is characteristic of the Post-Impressionist style, particularly reminiscent of Vincent van Goghs work. The second image, on the other hand, is more subdued and realistic depiction of rural scene, likely belonging to the Realist or Naturalist school of art. The third image, however, shares similar style to the first image, with its bold use of color, expressive brushwork, and abstract elements. The third images abstract forms and vibrant color palette align more closely with the Post-Impressionist style seen in the first image. Therefore, the third image shares the same style as the reference image. Figure 19: An example illustrating the models ability to recognize art style. Prompts: Given the first image with the lower right corner missing, can you tell which one of the second image or the third image is the missing part? Model Responses: <think>...</think> The third image is the missing part of the first image. Figure 20: An example illustrating the models ability on the jigsaw challenge. 28 A.9 Video Prompts: Describe the collision process of the blue sphere in the video. Model Responses: <think>...</think> The blue sphere collides with the red cube, then collides with the red cylinder. Figure 21: An example illustrating the models ability on video perception and reasoning. 29 Prompts: Describe the video content in detail. Model Responses: <think>...</think> The video captures live musical performance featuring two musicians on stage, set against backdrop of red curtain that creates warm and intimate atmosphere. The primary focus is on saxophonist dressed in white long-sleeved shirt and black pants, who is actively engaged in playing the saxophone. His posture and the positioning of his fingers on the instrument suggest dynamic and expressive performance. The saxophonists shadow is cast on the red curtain behind him, adding depth to the scene. In the foreground, slightly out of focus, is guitarist wearing plaid shirt, who is also deeply involved in the performance. The guitarists presence, along with the saxophonists, contributes to the overall ambiance of live musical event. The lighting is strategically focused on the performers, highlighting their actions and creating spotlight effect that enhances the visual appeal of the scene. Throughout the video, the saxophonists performance remains the central element, with his posture and the positioning of his fingers on the saxophone indicating continuous and expressive musical piece. The guitarists role is secondary but essential, as his presence complements the saxophonists performance, adding layers to the musical experience. The video maintains consistent visual theme, with the red curtain backdrop and focused lighting on the performers creating cohesive and engaging atmosphere. The saxophonists shadow on the curtain adds visual element that enhances the depth and dimension of the scene. The overall composition of the video emphasizes the live musical performance, capturing the essence of the musicians engagement and the intimate setting of the stage. Figure 22: An example illustrating the models ability to generate detailed description for given video."
        }
    ],
    "affiliations": [
        "Alibaba Group"
    ]
}