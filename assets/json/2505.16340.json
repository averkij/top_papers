{
    "paper_title": "Improving Chemical Understanding of LLMs via SMILES Parsing",
    "authors": [
        "Yunhui Jang",
        "Jaehyung Kim",
        "Sungsoo Ahn"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) are increasingly recognized as powerful tools for scientific discovery, particularly in molecular science. A fundamental requirement for these models is the ability to accurately understand molecular structures, commonly encoded in the SMILES representation. However, current LLMs struggle to interpret SMILES, even failing to carry out basic tasks such as counting molecular rings. To address this limitation, we introduce CLEANMOL, a novel framework that formulates SMILES parsing into a suite of clean and deterministic tasks explicitly designed to promote graph-level molecular comprehension. These tasks span from subgraph matching to global graph matching, providing structured supervision aligned with molecular structural properties. We construct a molecular pretraining dataset with adaptive difficulty scoring and pre-train open-source LLMs on these tasks. Our results show that CLEANMOL not only enhances structural comprehension but also achieves the best or competes with the baseline on the Mol-Instructions benchmark."
        },
        {
            "title": "Start",
            "content": "Yunhui Jang KAIST yunhuijang@kaist.ac.kr Jaehyung Kim Yonsei University jaehyungk@yonsei.ac.kr Sungsoo Ahn KAIST sungsoo.ahn@kaist.ac.kr 5 2 0 2 2 2 ] . [ 1 0 4 3 6 1 . 5 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) are increasingly recognized as powerful tools for scientific discovery, particularly in molecular science. fundamental requirement for these models is the ability to accurately understand molecular structures, commonly encoded in the SMILES representation. However, current LLMs struggle to interpret SMILES, even failing to carry out basic tasks such as counting molecular rings. To address this limitation, we introduce CLEANMOL, novel framework that formulates SMILES parsing into suite of clean and deterministic tasks explicitly designed to promote graph-level molecular comprehension. These tasks span from subgraph matching to global graph matching, providing structured supervision aligned with molecular structural properties. We construct molecular pretraining dataset with adaptive difficulty scoring and pre-train open-source LLMs on these tasks. Our results show that CLEANMOL not only enhances structural comprehension but also achieves the best or competes with the baseline on the Mol-Instructions benchmark."
        },
        {
            "title": "Introduction",
            "content": "such string representations Molecular as SMILES (Weininger, 1988) and SELFIES (Krenn et al., 2020) have become standard format for applying large language models (LLMs) to chemistry. These one-dimensional strings flatten molecular graphs by traversing atoms and bonds and are syntactically compatible with LLMs (Xia et al., 2025; Taylor et al., 2022; Edwards et al., 2022; Christofidellis et al., 2023a; Pei et al., 2023; Fang et al., 2024). As result, most molecular LLMs adopt training paradigms from the natural language processing domain, treating molecular strings as sequences of tokens analogous to sentences in natural language. However, molecular strings follow complex syntactic rules for encoding molecular structures, which LLMs often struggle to interpret. For instance, SMILES grammar includes specific conventions to denote rings and branchesoften involving non-contiguous tokens to represent connected substructures. Additionally, SMILES representations must satisfy structural constraints such as proper valency and ring closure. As result, current LLMs often misinterpret SMILES, which implies failure to capture the underlying molecule represented by the SMILES string. This is reflected in their inability to perform even basic tasks, such as counting the number of rings or producing consistent outputs for different SMILES strings of the same molecule (Jang et al., 2024; White et al., 2023; Ganeeva et al., 2024). Our experiments revisit such limitations, as shown in Figure 1 and Section 2.2. One might expect such an understanding would naturally emerge from training LLMs on large corpora of SMILES strings for downstream tasks such as molecular generation and retrosynthetic analysis. However, high-quality data is limited and difficult to obtain. Unlike text or image data, which can be gathered at scale via web scrapping, chemical data often require expensive wet lab experiments or simulations for annotation. Although open-source datasets such as USPTO series (Wei et al., 2010; Lu and Zhang, 2022) and MoleculeNet (Wu et al., 2018) exist, their scale remains modest compared to datasets in other domains (Deng et al., 2009; Raffel et al., 2020a; Lozhkov et al., 2024). Consequently, most chemical LLMs often rely on ambiguous and indirect pretraining objectives with non-deterministic and unclear tasks (e.g., masking each token in SMILES and reconstruct them or translation between molecular string and its description) (Pei et al., 2023; Edwards et al., 2022), or focus on instruction tuning with limited-scale datasets (Fang et al., 2024; Yu et al., 2024). In response, we propose SMILES parsinga suite of clean, deterministic, and scalable tasks that require models to extract structural information (a) Illustration of SMILES parsing tasks. (b) Failure of LLMs on SMILES parsing. Figure 1: Overview of SMILES parsing. (a) Each column visualizes one of the five SMILES parsing tasks: functional group matching, ring counting, carbon chain length measurement, SMILES canonicalization, and fragment assembly. The highlighted tokens in the SMILES correspond to the substructures involved in each task. (b) Recent LLMs fail for SMILES parsing while the model trained with our CLEANMOL shows improvement. from molecular strings, as illustrated in Figure 1. We argue that natural and necessary candidate task for training LLMs to understand the SMILES representation is the extraction of deterministic graph-level information from molecular structures. To address this, we define five SMILES parsing tasks including subgraph matching (e.g., functional group, ring size, and chain length) and global graph matching (e.g., SMILES canonicalization and fragment assembly). Each task provides unambiguous supervision with deterministic answers. Based on these tasks, we construct the CLEANMOL dataset, consisting of 250K molecules annotated via lightweight molecular graph analysis tools such as RDKit (Landrum et al., 2024). Notably, our approach is scalable since the annotations for these tasks do not require any experiment or human annotation, in principle, SMILES parsing can be applied to all the existing molecules in the real world. To evaluate and demonstrate the benefit of our new CLEANMOL dataset, we also introduce twostage training framework: first, the model is pretrained on the proposed SMILES parsing tasks and then fine-tuned on downstream chemical applications. To enhance data efficiency in the first stage, we propose task-adaptive data pruning that selects structurally informative molecules and curriculum learning framework that organizes them from easy to hard order. We empirically validate our approach by training recent LLM backbones (Grattafiori et al., 2024; Yang et al., 2024) and evaluating them on three downstream tasks from the Mol-Instructions benchmark (Fang et al., 2024), including retrosynthesis, reagent prediction, and forward reaction prediction. Surprisingly, our clean and structure-aware CLEANMOL framework enables the models to achieve state-of-the-art or competitive results on the downstream tasks. This demonstrates that incorporating deterministic structural supervision via SMILES parsing can significantly enhance molecular generation capabilities, even without direct exposure to generation-specific training data. We summarize our contributions as follows: We revisit the limitations of LLMs in interpreting molecular strings, highlighting the structural bottleneck. We propose five deterministic and scalable SMILES parsing tasks and introduce the CLEANMOL dataset to bridge the gap between string-level and graph-level molecular understanding of LLMs. We design two-stage training framework incoporating task-adaptive data pruning and curriculum learning strategy. We validate the impact of CLEANMOL by demonstrating consistent performance improvement across multiple downstream tasks. 2 specified functional group. Ring counting identifies the number of rings with specific sizes (e.g., fiveor six-membered), and chain length measurement evaluates the length of the longest carbon chain excluding rings. These tasks focus on local subgraphs such as structural motifs, branching, and ring patterns. Global graph matching. This category consists of SMILES canonicalization and fragment assembly. Canonicalization involves converting arbitrarily ordered SMILES into canonical form, which encourages structural invariance to syntactic permutation. Fragment assembly requires the model to combine two SMILES fragments into single valid molecule, testing its ability to reorganize the global structure from disjoint components."
        },
        {
            "title": "2.2 Failure of existing LLMs",
            "content": "Although SMILES parsing appears simple from structural point of view, it poses significant challenges for existing LLMs. Complex cases involving nested rings or hierarchical branching often disrupt token-level patterns, making it difficult for models to resolve SMILES parsing accurately. In detail, as shown in Figure 2, many structural features are represented non-contiguously in SMILES, further complicating the parsing process. Our motivation closely aligns with that of Jang et al. (2024).1 We observe that even state-of-the-art generalpurpose LLMs, including GPT-4o (OpenAI and et al., 2024) and DeepSeek-V3-Chat (Liu et al., 2024), struggle with SMILES parsing, achieving no more than 60% accuracy across five tasks except for the binary classification (functional group matching), as described in Figure 1b and detailed in Section 4.1. This failure is notable given the strong performance of these models in other domains such as mathematics and code. The inability of these models to handle even basic molecular parsing tasks underscores critical gap in their structural understanding. It motivates the need for explicit pretraining strategies tailored to molecules."
        },
        {
            "title": "2.3 Costly high-quality data acquirement",
            "content": "A second challenge lies in acquiring sufficient highquality training data for molecules. In contrast to textual and visual domains, which benefit from 1Unlike Jang et al. (2024), which fine-tunes models directly on structural information and downstream tasks, we pretrain LLMs on SMILES parsing objectives and subsequently fine-tune them for downstream tasks. Figure 2: Complex cases in SMILES parsing. The top green panels represent relatively simple cases, while the bottom red panels illustrate more complex examples with non-continuous substructures in SMILES. Orange and teal highlights correspond to tasks involving ring counting and functional group matching, respectively."
        },
        {
            "title": "2 SMILES parsing task",
            "content": "In this section, we introduce five SMILES parsing tasks designed to enhance the mapping between molecular SMILES strings and their corresponding graph structures. We then highlight two key bottlenecks in applying LLMs to molecular tasks: (1) the inability of models to extract structural information from SMILES strings and (2) the lack of highquality, scalable molecular datasets. To address the first bottleneck, we show that even advanced LLMs such as GPT-4o (OpenAI and et al., 2024) and DeepSeek-V3 (Liu et al., 2024) fail to perform well on simple SMILES parsing tasks, revealing the need for explicit structure-aware supervision. To address the second bottleneck, we explain the limitation of open-source molecular datasets, motivating the need for scalable molecular datasets that can be generated without costly experiments."
        },
        {
            "title": "2.1 SMILES parsing task description",
            "content": "We define SMILES parsing as suite of deterministic, scalable, and structure-focused tasks designed to map molecular strings to their corresponding molecular graphs. The tasks fall into two categoriessubgraph matching and global graph matchingas illustrated in Figure 1a. Importantly, all annotations can be generated automatically using open-source chemical tools such as RDKit (Landrum et al., 2024) without any experiment, making the tasks highly scalable. We provide more details in Appendix A. Subgraph matching. This category includes functional group matching, ring counting, and carbon chain length measurement. Functional group matching determines the presence of 3 SMILES: c1ccc(C(F)(F)F)c(N2C(N)=C(C#N)[C@H](c3cc(OCC)ccc3OCC)C3=C2CCCC3=O)c"
        },
        {
            "title": "Yes",
            "content": "# Functional group Question: Given the SMILES, determine inclusion of the functional group COC. Answer: # Ring Question: Calculate the count of SIX-membered rings in the given SMILES string. Answer: # Canonicalization Question: Give me canonicalized SMILES that represents the same given molecule. Answer: CCOc1ccc(OCC)c([C@H]2C(C#N)=C(N)N(c3ccccc3C(F)F)F)C3=C2C(=O) CCC3)c1 4 Figure 3: Examples of CLEANMOL dataset. Functional group matching Ring Chain length counting measurement SMILES canonicalization Fragment assembly # of functional groups # of rings # of branches SMILES length Table 1: Definition of each task-specific difficulty. with the CLEANMOL dataset, followed by finetuning downstream applications. To improve the pre-training, we also introduce task-adaptive data pruning and curriculum learning strategy based on task-specific difficulty measures."
        },
        {
            "title": "3.1 CLEANMOL data preparation",
            "content": "First, we introduce the CLEANMOL dataset based on the SMILES parsing tasks proposed in Section 2.1. There exist two key advantages of our proposed tasks: determinism and scalability. In detail, on the one hand, in terms of determinism, our tasks are designed to have unique and clearly defined answer (i.e., number or canonicalized SMILES) unlike previous pre-training objectives such as masking and translation as detailed in Section 6. This ensures unambiguous supervision during training and facilitates reliable learning. On the other hand, regarding scalability, as the proposed tasks apply to any valid molecules without any experimental data, they can be expanded to vast set of molecules. In detail, all annotations can be automatically generated using open-source cheminformatics tools such as RDKit (Landrum et al., 2024), making the dataset extensible to virtually unlimited molecular corpora. We provide the simplified example instructions of SMILES parsing tasks in Figure 3 and more examples including detailed instruction formats in Appendix A."
        },
        {
            "title": "3.2 Training with CLEANMOL",
            "content": "Once the CLEANMOL dataset is prepared, we adopt task-specific data pruning and curriculum learning inspired by recent work on highquality LLM data curation (Gunasekar et al., 2023; Figure 4: Overview of molecular data pruning and ranking. Each number represents the task-specific difficulty score assigned to molecule, as defined in Table 1. For each parsing task, molecules are ranked based on these scores and we select the mid-difficulty samples. large-scale web scraping (Deng et al., 2009; Raffel et al., 2020a; Lozhkov et al., 2024), chemical datasets often rely on costly and labor-intensive wet lab experiments or computational simulations. While resources such as the USPTO series (Wei et al., 2010; Lu and Zhang, 2022) and MoleculeNet (Wu et al., 2018) exist, expanding them is expensive and labor-intensive. This highlights the need for scalable alternativesdatasets that can be automatically generated with minimal cost while preserving domain relevance."
        },
        {
            "title": "3 Training framework of CLEANMOL",
            "content": "In this section, we present our framework to improve the molecular understanding of LLMs using new dataset, coined CLEANMOL.2 Our scheme consists of (1) data preparation and (2) two-stage training procedure. In the data preparation step, we prepare the CLEANMOL dataset with deterministic and scalable SMILES parsing tasks. Next, in the training step, we pre-train LLMs 2Our framework and dataset are both termed CLEANMOL."
        },
        {
            "title": "Model",
            "content": "5-shot"
        },
        {
            "title": "SFT",
            "content": "Deepseek-V3-chat GPT-4o Galactica-6.7B Llama3.1-8B (Single) Llama3.1-8B (Multi) Qwen2.5-7B (Single) Qwen2.5-7B (Multi) FG 0.8912 0.8750 0.5000 0.9414 0. 0.9891 0."
        },
        {
            "title": "Ring",
            "content": "0.6266 0.5955 0.0732 0.8612 0.8707 0.8674 0."
        },
        {
            "title": "Canonical Assembly",
            "content": "0.2976 0.2857 0.1511 0.9859 0.9851 0.9907 0.9902 0.1484 0.1078 0.0000 0.9356 0.9463 0.7593 0. 0.1512 0.1932 0.0046 0.8858 0.9010 0.3371 0.8835 Table 2: SMILES parsing performance. FG stands for the functional group. Background indicates the improvement of multi-task learning compared to the single-task learning and the best results are highlighted in bold. Marion et al., 2023; Ankner et al., 2024) to further enhance pre-training with CLEANMOL. As illustrated in Figure 4, our approach involves: (1) subsampling sufficiently informative molecules, and (2) constructing curriculum by ranking these examples from simple to complex using task-specific difficulty measures. The difficulty measures are defined for each parsing task as summarized in Table 1. For instance, in the chain length measurement task, molecules with extensive branches often lead to SMILES where relevant subgraph atoms appear far apart in the string, increasing parsing difficulty. By excluding extremely easy or hard molecules (i.e., subsample molecules with mid-level difficulties) and organizing the training data from simple to complex, our approach aligns with curriculum learning principles (Bengio et al., 2009) and leads to improved performance, as validated in Section 4.2. Next, we adopt two-stage training pipeline to effectively integrate SMILES parsing into LLM. In the first stage, we perform pre-training on the pruned CLEANMOL dataset using supervised finetuning. This allows the model to acquire core structural understanding and compositional knowledge of molecular graphs. In the second stage, we further fine-tune this trained model on downstream molecular tasks. By initializing with model that has already learned to parse molecular structures, downstream adaptation becomes more accurate."
        },
        {
            "title": "4 Experiments: SMILES parsing tasks",
            "content": "In this section, we evaluate the effectiveness of our proposed SMILES parsing task as pre-training signal for LLMs. The parsing task is formally defined in Section 2.1. We demonstrate that recent LLMs, while not inherently proficient in SMILES parsing, can acquire this capability through targeted training. We provide all experimental settings including prompts, hyperparameters, and computational resources in Appendix B."
        },
        {
            "title": "4.1 LLMs can learn SMILES parsing",
            "content": "As described in Section 2.2, SMILES parsing poses significant challenge for general-purpose LLMs, despite its foundational importance for molecular understanding. Our experiments reveal that LLMs lack the inductive bias to naturally understand the molecular structure encoded in SMILES strings. However, we show that through supervised finetuning (SFT), LLMs can learn to accurately parse and interpret SMILES representations. Dataset. We construct CLEANMOL benchmark consisting of 50K molecules per SMILES parsing task, totaling 250K examples across five tasks. The molecules are subsampled from the ZINC250k (Irwin et al., 2012) training dataset using our proposed molecular data pruning strategy described in Section 3.2, which excludes extremely easy or hard molecules to enhance the molecular pre-training. Additionally, for the test dataset, we randomly selected 10K molecules from the ZINC250K test split and fixed this subset across all experiments. Baselines. We evaluate the parsing capabilities of four general-purpose LLMsDeepseek-V3Chat (Liu et al., 2024), GPT-4o (OpenAI and et al., 2024), LLaMA3.1-8B-Instruct (Grattafiori et al., 2024), and Qwen2.5-7B-Instruct (Yang et al., 2024)and one chemistry-specific LLM, Galactica-6.7B (Taylor et al., 2022). To assess the basic molecular understanding of general-purpose LLMs, we apply 5-shot prompting to Deepseek and GPT-4o, which are not publicly trainable and thus cannot be fine-tuned. Similarly, we apply 5shot prompting to Galactica, chemistry-specific LLM pre-trained on molecular corpora, to evaluate its zero-shot capabilities without further supervision. In contrast, for LLaMA and Qwen, which are open-weight general-purpose LLMs, we perform supervised fine-tuning using our SMILES parsing dataset to examine whether explicit structure-aware training can bridge the gap in molecular compre-"
        },
        {
            "title": "Pruning type",
            "content": "FG"
        },
        {
            "title": "Canonical Assembly Average",
            "content": "Random Length Molecular pruning (top) Molecular pruning (bottom) 0.9921 0.9910 0.9902 0.9729 0.9212 0.8531 0.8123 0.6995 0.9886 0.9785 0.9716 0.9597 0.7845 0.8519 0.9446 0.5514 0.7352 0.8044 0.7487 0. 0.8843 0.8958 0.8934 0.7404 Molecular pruning (middle, ours) 0.9901 0.8750 0.9902 0. 0.8835 0.9330 Table 3: Effect of molecular data pruning on Qwen2.5-7B-Instruct. \"Random\" and \"Length\" refer to baselines using random sampling and SMILES length as proxies for difficulty. \"Top,\" \"middle,\" and \"bottom\" denote subsamples consisting of the most difficult, moderately difficult, and easiest molecules, respectively, based on task-specific difficulty heuristics. hension. Notably, we explore two experimental settings: single-task, where separate model is trained for each parsing task, and multi-task, where single model is jointly trained on all five tasks. Metrics. We evaluate performance using accuracy, as SMILES parsing tasks are deterministic and each input has well-defined answer. Results. The results are presented in Table 2. We observe that recent general-purpose LLMs (GPT-4o and Deepseek) and even chemical LLM (Galactica) perform poorly on SMILES parsing, revealing their limited molecular comprehension. This validates that the primary bottleneck in applying LLMs to molecular domains lies not in the absence of chemical knowledge, but in the lack of basic molecular structural understandingspecifically, the ability to parse and interpret SMILES strings. In contrast, fine-tuned LLaMA and Qwen models show substantial improvements, demonstrating that SMILES parsing can be effectively learned through training. Moreover, all tasksexcept for chain length measurementachieved higher accuracy in the multi-task setting, suggesting that transferable structural understanding across tasks contributes to improved performance."
        },
        {
            "title": "4.2 Effect of molecular data pruning",
            "content": "We further investigate the impact of our molecular data pruning strategy on parsing performance. As detailed in Section 3.2, this technique aims to curate training set that maximizes informativeness. The results, shown in Table 3, demonstrate that our pruning method improves performance, suggesting that data quality plays critical role in teaching LLMs the implicit grammar of SMILES. Figure 5: Data scale analysis for SMILES parsing. the accuracy of the SMILES parsing task for 10K, 20K, and 50K data settings per task in the same setting in Section 4.1. We provide the results in Figure 5. Here, we observed that increasing the dataset size consistently improves SMILES parsing performance, with particularly dramatic gains in the ring counting and fragment assembly tasks. This validates the expandability of our framework."
        },
        {
            "title": "5 Experiments: Downstream tasks",
            "content": "In this section, we evaluate the effect of pre-training LLMs on CLEANMOL dataset across three molecular generation downstream applications. We provide the experimental settings in Appendix and additional experimental results in Appendix C."
        },
        {
            "title": "Our",
            "content": "results demonstrate that incorporating CLEANMOL as pre-training strategy consistently improves performance across diverse downstream molecular tasks. These findings provide strong empirical support for our central hypothesis: clean and structurally faithful SMILES parsing serves as an effective and transferable learning signal for LLMs. Notably, CLEANMOL achieves state-of-the-art or competitive performance despite being pre-trained without any task-specific data, underscoring the strength and generality of our approach."
        },
        {
            "title": "5.1 Molecular generation",
            "content": "Here, we conduct an ablation study to validate the impact of the increase in dataset size in our proposed CLEANMOL dataset. In detail, we evaluate The molecular generation task aims to generate molecules given prompts, including retrosynthesis, reagent prediction, and forward reaction prediction. 6 Models Exact. BLEU Levenshtein MACCS FTS RDK FST Morgan FTS Validity Task 1: Retrosynthesis Text+Chem T5 Mol-Instructions (Lla.2) Mol-Instructions (Lla.3) Mol-Instructions (Lla.3.1)* InstructMol-GS Llama3.1-8B + Mol-Instructions (SFT)* + CLEANMOL Qwen2.5-7B + CLEANMOL Task 2: Reagent prediction Text+Chem T5 Mol-Instructions (Lla.2) Mol-Instructions (Lla.3) Mol-Instructions (Lla.3.1)* InsturctMol Llama3.1-8B + Mol-Instructions (SFT)* + CLEANMOL Qwen2.5-7B + CLEANMOL 0.141 0.009 0.333 0.255 0.407 0.456 0.541 0.581 0.460 0. 0.000 0.044 0.101 0.085 0.129 0.124 0.142 0.147 0.120 0.128 Task 3: Forward reaction prediction Text+Chem T5 Mol-Instructions (Lla.2) Mol-Instructions (Lla.3) Mol-Instructions (Lla.3.1)* InstructMol-GS Llama3.1-8B + Mol-Instructions (SFT)* + CLEANMOL Qwen2.5-7B + CLEANMOL 0.236 0.045 0.503 0.402 0.536 0.794 0.888 0.890 0.833 0.874 0.765 0.705 0.842 0.890 0. 0.944 0.955 0.959 0.946 0.958 0.255 0.224 0.648 0.676 0.610 0.625 0.678 0.687 0.649 0.685 0.782 0.654 0.883 0.907 0. 0.981 0.990 0.990 0.986 0.989 24.04 31.23 17.64 17.76 13.97 10.22 8.25 7.86 10.11 8.26 49.32 23.17 18.33 22.40 19. 17.31 17.14 16.89 17.76 16.58 13.63 27.26 13.41 13.11 10.85 2.47 1.33 1.37 2.08 1.56 0.685 0.283 0.704 0.813 0. 0.895 0.915 0.923 0.897 0.915 0.039 0.237 0.412 0.505 0.444 0.538 0.562 0.564 0.533 0.557 0.523 0.313 0.756 0.848 0. 0.965 0.983 0.980 0.972 0.980 0.765 0.487 0.815 0.690 0.852 0.837 0.878 0.890 0.849 0.880 0.186 0.364 0.521 0.398 0. 0.433 0.467 0.472 0.431 0.455 0.630 0.509 0.863 0.718 0.878 0.938 0.967 0.966 0.947 0.963 0.585 0.230 0.646 0.644 0. 0.801 0.843 0.856 0.809 0.844 0.052 0.213 0.375 0.356 0.400 0.398 0.430 0.434 0.395 0.415 0.505 0.262 0.708 0.679 0. 0.926 0.961 0.959 0.943 0.956 0.698 - - - - 0.979 - 0.998 0.910 0.995 0.313 - - - - 0.999 - 0.999 - 0.975 0.967 - - - - 0.988 - 0.996 0.987 0.959 Table 4: Molecular generation performance. Background indicates the improvement compared to vanilla model. Asterisks (*) denote reproduced results and - in validity represents the SELFIES-based methods which guarantees the perfect validity. For each metric, the best and second-best result is highlighted with bold and underline. Dataset. We use the Mol-Instructions dataset (Fang et al., 2024), which covers three molecule generation tasks. Specifically, retrosynthesis predicts the possible precursors that lead to given target molecule. Next, the reagent prediction task requires the generation of suitable catalysts, solvents, or ancillary reagents for given chemical reaction. Lastly, forward reaction prediction involves the generation of plausible product from given reactants and reagents. We follow the data splits provided in Mol-Instructions. Baselines. We evaluate CLEANMOL by integrating it with two base models: LLaMA-3.1-8BInstruct (Grattafiori et al., 2024) and Qwen-2.57B-Instruct (Yang et al., 2024), to test whether CLEANMOL consistently improves performance. Notably, the vanilla base models are fine-tuned on each downstream task without pre-training. For an absolute performance comparison, we include three baselines: Text+Chem T5 (Christofidellis et al., 2023a), Mol-Instructions (Fang et al., 2024) and InstructMol (Cao et al., 2023). Additionally, we include variant of Mol-Instructions denoted as Mol-Instructions (SFT), which is first instructiontuned on the same dataset size as our CLEANMOL dataset (250K) and then further fine-tuned on each downstream task. This ensures fair comparison for both the model and the training data size. Metrics. We assess the performance by comparing the generated molecules with the ground truth based on eight metrics. These include SMILES string-based metrics (Exact match, BLEU (Papineni et al., 2002), and Levenshtein distance (Miller et al., 2009)), molecular fingerprint similarities (MACCS (Durant et al., 2002), RDK (Schneider et al., 2015), and Morgan (Rogers and Hahn, 2010)), distributional similarity via Fréchet ChemNet Distance (FCD) (Preuer et al., 2018), and the validity of generated molecules. Results. The results are summarized in Table 4. Incorporating CLEANMOL consistently improves performance across all backbones, demonstrating the effectiveness of SMILES parsing tasks in enand sufficiently simple to support generalizable learning. In chemistry, many works adopt NLPinspired objectives such as masked language modeling (MLM) (Devlin et al., 2019) and sequence-tosequence translation (Raffel et al., 2020b), applied to SMILES (Weininger, 1988) or SELFIES (Krenn et al., 2020). Edwards et al. (2022) used separate MLM pretraining on molecular and textual data, while later studies (Pei et al., 2023; Christofidellis et al., 2023b) combined MLM with moleculetext translation. Liu et al. (2023a) embedded SMILES in natural language prompts, and other works incorporated 2D or 3D geometry (Li et al., 2023; Ji et al., 2024; Zhou et al., 2023). Despite these advancements, most strategies introduce unambiguous supervision signals due to the non-determinism of molecular representations. For example, in masked SMILES prediction, multiple chemically valid tokens can fill the same masked position, leading to noisy training signal. This undermines training effectiveness and limits the models ability to learn robust understanding. To address this issue, we provide clean and deterministic SMILES parsing tasks as pre-training tasks. Data pruning in LLMs. Data pruning refers to selecting an informative subset of training data, which is crucial for reliable LLM training (Gunasekar et al., 2023). Most data pruning methods rely on rule-based filters (Wenzek et al., 2020; Raffel et al., 2020a), perplexity scores (Marion et al., 2023; Ankner et al., 2024), or LLM embeddings (Tirumala et al., 2023). However, these metrics are ill-defined for molecular strings, where perplexity and embeddings do not reflect the structural information of the corresponding molecules. To address this, we introduce task-specific difficulty measures and data pruning strategies for molecules."
        },
        {
            "title": "7 Conclusion",
            "content": "In this paper, we revisit the key limitation in applying LLMs to chemistry: the inability to interpret the structures encoded in SMILES. To address this, we propose CLEANMOL, framework that introduces deterministic and scalable SMILES parsing tasks to provide unambiguous structural supervision. Our experiments show that CLEANMOL significantly enhances molecular structural understanding and improves performance across multiple downstream tasks. These results highlight the value of incorporating clean and structure-aware objectives into LLMs to support more robust applications. Figure 6: Data scale analysis for retrosynthesis. hancing molecular language modeling. These improvements suggest that pre-training on clean and deterministic CLEANMOL dataset facilitates the models structural understanding required for generation tasks. Notably, integrating CLEANMOL into LLaMA3.1-8B-Instruct achieves state-of-theartor at least comparableperformance to MolInstructions (SFT), despite using no molecular generation data during pre-training."
        },
        {
            "title": "5.2 Ablation study",
            "content": "Here, we evaluate the effect of CLEANMOL dataset size on retrosynthesis performance using 10K, 20K, and 50K molecules per parsing task following the setup in Section 5.1. As described in Figure 6, the performance grows with data scale, demonstrating CLEANMOL scalability. As SMILES parsing requires no costly experiment, this framework easily extends to large molecular corpora."
        },
        {
            "title": "6 Related work",
            "content": "LLMs for chemistry. General-purpose LLMs often struggle with fundamental chemistry tasks, particularly those requiring molecular structure understanding (White et al., 2023; Castro Nascimento and Pimentel, 2023; Guo et al., 2023). To address this gap, several studies have proposed chemically specialized LLMs. Some approaches pre-train LLMs on molecular and biomedical corpora to inject domain-specific knowledge (Edwards et al., 2022; Christofidellis et al., 2023b; Liu et al., 2023a; Pei et al., 2023). Others explore instruction tuning on curated molecular tasks (Fang et al., 2024; Cao et al., 2023), or leverage retrieval-augmented prompting to improve few-shot performance (Li et al., 2024). While these methods aim to inject domain knowledge, they often neglect the need for grounding models in basic molecular understanding. In contrast, we emphasize clean and deterministic structural supervision through well-defined SMILES parsing tasks, which can complement existing methods and integrate with instruction tuning or domain adaptation. Pre-training of LLMs for chemistry. Effective pre-training tasks should be well-structured"
        },
        {
            "title": "References",
            "content": "Our work contributes to the development of structurally grounded models for molecular applications. By introducing structured, clean, and scalable set of SMILES parsing tasks, we aim to equip LLMs with stronger inductive bias toward molecular structure understanding. This can enhance downstream applications such as drug discovery, materials design, and reaction prediction by improving the fidelity and reliability of molecular reasoning. However, as with any generative AI system in chemistry, potential misuse remains concern. The capacity to generate toxic, harmful, or restricted compounds necessitates careful integration of safety measures and expert oversight."
        },
        {
            "title": "Limitations",
            "content": "Limited structural information. Our SMILES parsing tasks focus on graph-level molecular structures and do not incorporate 3D conformational information, which is essential for many biological and physicochemical applications. Additionally, while our tasks are deterministic and scalable, they do not capture more nuanced chemical features such as stereochemistry, electronic effects, or reactivity patterns, which often require context beyond 2D topological graphs. Language-specific scope. Our experiments are conducted exclusively in English and do not explore the applicability of the method across other languages, including morphologically rich or typologically diverse ones. Given that behaviors can vary across languages due to linguistic structure and training data distributions, the generalizability of our approach to multilingual settings remains an open question. Model and dataset scale. Due to computational constraints, our experiments are limited to language models with up to 7.5B8B parameters. It remains to be seen whether our framework scales effectively to larger models (e.g., 70B or beyond). Moreover, our pretraining is performed on relatively modest dataset of 250K molecules, and while we observe consistent improvements, further studies on largerscale datasets are necessary to assess the robustness and scalability of the approach. Zachary Ankner, Cody Blakeney, Kartik Sreenivasan, Max Marion, Matthew Leavitt, and Mansheej Paul. 2024. Perplexed by perplexity: Perplexity-based data pruning with small reference models. arXiv preprint arXiv:2405.20541. Yoshua Bengio, Jérôme Louradour, Ronan Collobert, and Jason Weston. 2009. Curriculum learning. In Proceedings of the 26th annual international conference on machine learning, pages 4148. He Cao, Zijing Liu, Xingyu Lu, Yuan Yao, and Yu Li. 2023. Instructmol: Multi-modal integration for building versatile and reliable molecular assistant in drug discovery. Preprint, arXiv:2311.16208. Cayque Monteiro Castro Nascimento and AndréSilva Pimentel. 2023. Do large language models understand chemistry? conversation with chatgpt. Journal of Chemical Information and Modeling, 63(6):1649 1655. Wei-Lin Chiang, Zhuohan Li, Ziqing Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph Gonzalez, and 1 others. 2023. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. See https://vicuna. lmsys. org (accessed 14 April 2023), 2(3):6. Dimitrios Christofidellis, Giorgio Giannone, Jannis Born, Ole Winther, Teodoro Laino, and Matteo Manica. 2023a. Unifying molecular and textual representations via multi-task language modelling. In Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 61406157. PMLR. Dimitrios Christofidellis, Giorgio Giannone, Jannis Born, Ole Winther, Teodoro Laino, and Matteo Manica. 2023b. Unifying molecular and textual representations via multi-task language modelling. In Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 61406157. PMLR. Michael Han Daniel Han and Unsloth team. 2023. Unsloth. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. Imagenet: large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248255. Ieee. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies, volume 1 (long and short papers), pages 41714186. 9 Joseph Durant, Burton Leland, Douglas Henry, and James Nourse. 2002. Reoptimization of mdl keys for use in drug discovery. Journal of chemical information and computer sciences, 42(6):1273 1280. Carl Edwards, Tuan Lai, Kevin Ros, Garrett Honke, Kyunghyun Cho, and Heng Ji. 2022. Translation In Probetween molecules and natural language. ceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 375413, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. Yin Fang, Xiaozhuan Liang, Ningyu Zhang, Kangwei Liu, Rui Huang, Zhuo Chen, Xiaohui Fan, and Huajun Chen. 2024. Mol-instructions: large-scale biomolecular instruction dataset for large language models. In The Twelfth International Conference on Learning Representations. Veronika Ganeeva, Andrey Sakhovskiy, Kuzma Khrabrov, Andrey Savchenko, Artur Kadurin, and Elena Tutubalina. 2024. Lost in translation: Chemical language models and the misunderstanding of molecule structures. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 1299413013. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad AlDahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, and 1 others. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783. Sylvain Gugger, Lysandre Debut, Thomas Wolf, Philipp Schmid, Zachary Mueller, Sourab Mangrulkar, Marc Sun, and Benjamin Bossan. 2022. Accelerate: Training and inference at scale made simple, efficient and https://github.com/huggingface/ adaptable. accelerate. Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio César Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, and 1 others. 2023. arXiv preprint Textbooks are all you need. arXiv:2306.11644. Taicheng Guo, Bozhao Nan, Zhenwen Liang, Zhichun Guo, Nitesh Chawla, Olaf Wiest, Xiangliang Zhang, and 1 others. 2023. What can large language models do in chemistry? comprehensive benchmark on eight tasks. Advances in Neural Information Processing Systems, 36:5966259688. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, and 1 others. 2022. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3. Yunhui Jang, Jaehyung Kim, and Sungsoo Ahn. 2024. Chain-of-thoughts for molecular understanding. arXiv preprint arXiv:2410.05610. Xiaohong Ji, Zhen Wang, Zhifeng Gao, Hang Zheng, Linfeng Zhang, Guolin Ke, and Weinan E. 2024. Exploring molecular pretraining model at scale. In The Thirty-eighth Annual Conference on Neural Information Processing Systems. Mario Krenn, Florian Häse, AkshatKumar Nigam, Pascal Friederich, and Alan Aspuru-Guzik. 2020. Selfreferencing embedded strings (selfies): 100% robust molecular string representation. Machine Learning: Science and Technology, 1(4):045024. Greg Landrum, Paolo Tosco, Brian Kelley, Ricardo Rodriguez, David Cosgrove, Riccardo Vianello, sriniker, Peter Gedeck, Gareth Jones, NadineSchneider, Eisuke Kawashima, Dan Nealschneider, Andrew Dalke, Matt Swain, Brian Cole, Samo Turk, Aleksandr Savelev, Alain Vaucher, Maciej Wójcikowski, and 11 others. 2024. rdkit/rdkit: 2024_09_1 (q3 2024) release beta. Han Li, Ruotian Zhang, Yaosen Min, Dacheng Ma, Dan Zhao, and Jianyang Zeng. 2023. knowledgeguided pre-training framework for improving molecular representation learning. Nature Communications, 14(1):7568. Jiatong Li, Yunqing Liu, Wenqi Fan, Xiao-Yong Wei, Hui Liu, Jiliang Tang, and Qing Li. 2024. Empowering molecule discovery for molecule-caption translation with large language models: chatgpt perspecIEEE Transactions on Knowledge and Data tive. Engineering, page 113. Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, and 1 others. 2024. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437. Zequn Liu, Wei Zhang, Yingce Xia, Lijun Wu, Shufang Xie, Tao Qin, Ming Zhang, and Tie-Yan Liu. 2023a. MolXPT: Wrapping molecules with text for generative pre-training. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 16061616, Toronto, Canada. Association for Computational Linguistics. Zhiyuan Liu, Sihang Li, Yanchen Luo, Hao Fei, Yixin Cao, Kenji Kawaguchi, Xiang Wang, and Tat-Seng Chua. 2023b. MolCA: Molecular graph-language modeling with cross-modal projector and uni-modal adapter. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 1562315638, Singapore. Association for Computational Linguistics. John Irwin, Teague Sterling, Michael Mysinger, Erin Bolstad, and Ryan Coleman. 2012. Zinc: free tool to discover chemistry for biology. Journal of chemical information and modeling, 52(7):1757 1768. Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano, Joel Lamy-Poirier, Nouamane Tazi, Ao Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, and 1 others. 2024. Starcoder 2 and the stack v2: The next generation. arXiv preprint arXiv:2402.19173. Jieyu Lu and Yingkai Zhang. 2022. Unified deep learning model for multitask reaction predictions with explanation. Journal of chemical information and modeling, 62(6):13761387. Max Marion, Ahmet Üstün, Luiza Pozzobon, Alex Wang, Marzieh Fadaee, and Sara Hooker. 2023. Investigating data pruning When less is more: arXiv preprint for pretraining llms at scale. arXiv:2309.04564. Frederic Miller, Agnes Vandome, and John McBrewster. 2009. Levenshtein distance: Information theory, computer science, string (computer science), string metric, damerau? levenshtein distance, spell checker, hamming distance. OpenAI and Josh Achiam et al. 2024. Gpt-4 technical report. Preprint, arXiv:2303.08774. Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 311318. Qizhi Pei, Wei Zhang, Jinhua Zhu, Kehan Wu, Kaiyuan Gao, Lijun Wu, Yingce Xia, and Rui Yan. 2023. BioT5: Enriching cross-modal integration in biology with chemical knowledge and natural language associations. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 11021123, Singapore. Association for Computational Linguistics. Kristina Preuer, Philipp Renz, Thomas Unterthiner, Sepp Hochreiter, and Günter Klambauer. 2018. Fréchet chemnet distance: metric for generative models for molecules in drug discovery. Journal of Chemical Information and Modeling, 58(9):1736 1741. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. 2020a. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21(140):167. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020b. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of Machine Learning Research, 21(140):167. David Rogers and Mathew Hahn. 2010. Extendedconnectivity fingerprints. Journal of chemical information and modeling, 50(5):742754. Nadine Schneider, Roger Sayle, and Gregory Landrum. 2015. Get your atoms in order - an opensource implementation of novel and robust molecular canonicalization algorithm. Journal of chemical information and modeling, 55(10):21112120. Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic. 2022. Galactica: large language model for science. arXiv preprint arXiv:2211.09085. Kushal Tirumala, Daniel Simig, Armen Aghajanyan, and Ari Morcos. 2023. D4: Improving llm pretraining via document de-duplication and diversification. Advances in Neural Information Processing Systems, 36:5398353995. Tloen. 2023. Alpaca-lora. tloen/alpaca-lora. https://github.com/ Leandro von Werra, Younes Belkada, Lewis Tunstall, Edward Beeching, Tristan Thrush, Nathan Lambert, Shengyi Huang, Kashif Rasul, and Quentin Gallouédec. 2020. Trl: Transformer reinforcement learning. https://github.com/huggingface/trl. Jin-Mao Wei, Xiao-Jie Yuan, Qing-Hua Hu, and ShuQin Wang. 2010. novel measure for evaluating classifiers. Expert Systems with Applications, 37(5):37993809. David Weininger. 1988. Smiles, chemical language and information system. 1. introduction to methodology and encoding rules. Journal of Chemical Information and Computer Sciences, 28(1):3136. David Weininger, Arthur Weininger, and Joseph Weininger. 1989. Smiles. 2. algorithm for generation of unique smiles notation. Journal of chemical information and computer sciences, 29(2):97101. Guillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzmán, Armand Joulin, and Edouard Grave. 2020. CCNet: Extracting high quality monolingual datasets from web crawl data. In Proceedings of the Twelfth Language Resources and Evaluation Conference, pages 40034012, Marseille, France. European Language Resources Association. Andrew D. White, Glen M. Hocky, Heta A. Gandhi, Mehrad Ansari, Sam Cox, Geemi P. Wellawatte, Subarna Sasmal, Ziyue Yang, Kangxin Liu, Yuvraj Singh, and Willmor J. Peña Ccoa. 2023. Assessment of chemistry knowledge in large language models that generate code. Digital Discovery, 2:368376. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, and 3 others. 2020. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 3845, Online. Association for Computational Linguistics. Zhenqin Wu, Bharath Ramsundar, Evan Feinberg, Joseph Gomes, Caleb Geniesse, Aneesh Pappu, 11 Karl Leswing, and Vijay Pande. 2018. Moleculenet: benchmark for molecular machine learning. Chemical science, 9(2):513530. Yingce Xia, Peiran Jin, Shufang Xie, Liang He, Chuan Cao, Renqian Luo, Guoqing Liu, Yue Wang, Zequn Liu, Yuan-Jyue Chen, Zekun Guo, Yeqi Bai, Pan Deng, Yaosen Min, Ziheng Lu, Hongxia Hao, Han Yang, Jielan Li, Chang Liu, and 27 others. 2025. Nature language model: Deciphering the language of nature for scientific discovery. Preprint, arXiv:2502.07527. Canwen Xu, Daya Guo, Nan Duan, and Julian McAuley. 2023. Baize: An open-source chat model with parameter-efficient tuning on self-chat data. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 6268 6278, Singapore. Association for Computational Linguistics. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, and 1 others. 2024. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115. Botao Yu, Frazier N. Baker, Ziqi Chen, Xia Ning, and Huan Sun. 2024. LlaSMol: Advancing large language models for chemistry with large-scale, comprehensive, high-quality instruction tuning dataset. In First Conference on Language Modeling. Gengmo Zhou, Zhifeng Gao, Qiankun Ding, Hang Zheng, Hongteng Xu, Zhewei Wei, Linfeng Zhang, and Guolin Ke. 2023. Uni-mol: universal 3d molecular representation learning framework. In The Eleventh International Conference on Learning Representations. 12 Appendix Organization The appendix is organized as follows: We first describe the details of SMILES parsing tasks in Appendix A. Next, we present the experimental details such as hyperparameters and computational resources in Appendix B. Then we provide the additional experimental results including the generated samples and additional ablation studies in Appendix C. Lastly, we present the usage of AI assistants and scientific artifacts in Appendix and Appendix E, respectively."
        },
        {
            "title": "A Detailed description of SMILES",
            "content": "parsing tasks A.1 Subgraph matching This category includes functional group matching, ring counting, and carbon chain length measurement. These tasks are designed to focus on local substructures within the molecular graph, such as common functional motifs, ring systems, and chain connectivity. Each task formulation is deterministic and lends itself to clear evaluation. Functional group matching. Functional group matching evaluates whether specified functional group is present in given molecule. To ensure determinism, we cast this task as binary classification problem: the model must predict yes or no based on the presence of the target group. An example of the instruction format is shown in Figure 7."
        },
        {
            "title": "Functional group matching",
            "content": "Answer only in Yes or No without any other information. **Question:** Does the molecule represented by the SMILES string contain the specified functional group? Respond with Yes or No. **SMILES:** [SMILES] **FUNCTIONAL GROUP:** [Functional group SMILES] **ANSWER:** [Yes/No] This task tests the models ability to track topological cycles through non-contiguous token spans in SMILES. The instruction format is illustrated in Figure 8."
        },
        {
            "title": "Ring counting",
            "content": "Answer only with the corresponding integer number without any other information. **Question:** Assess the SMILES below and report how many rings consist of [RING SIZE] atoms. Give me the integer only. **SMILES:** [SMILES] **SIZE OF RINGS:** [RING SIZE] **ANSWER:** [NUMBER OF RINGS] Figure 8: An instruction format of ring counting. Chain length measurement. This task requires the model to identify the length of the longest acyclic carbon chain in the molecule, excluding atoms that are part of rings. It challenges the model to distinguish between linear and branched motifs and to reason about connectivity beyond localized tokens. Such chains often span long syntactic distances in SMILES, making the task non-trivial. The instruction format is shown in Figure 9."
        },
        {
            "title": "Chain length measurement",
            "content": "Answer only with the corresponding integer number without any other information. **Question:** Report the size of the largest carbon-only chain not contained within ring in the molecule represented by this SMILES. Answer with an integer only. **SMILES:** [SMILES] **ANSWER:** [LENGTH OF CHAIN] Figure 9: An instruction format of chain length measurement. Figure 7: An instruction format of functional group matching. Ring counting. Ring counting asks the model to determine the number of rings of specific size (e.g., fiveor six-membered) in the molecule. A.2 Global graph matching This category includes tasks that operate on global level: SMILES canonicalization and fragment assembly. Unlike subgraph matching, these tasks require full-graph interpretation, where success depends on integrating information across the entire molecular structure. 13 This category consists of SMILES canonicaliza-"
        },
        {
            "title": "Fragment assembly",
            "content": "tion and fragment assembly. SMILES canonicalization. Canonicalization involves transforming randomly ordered SMILES string into its canonical form following the canonicalization rules (Weininger et al., 1989). In detail, these rules typically involve assigning unique ranking to atoms based on graph invariants (e.g., atomic number, connectivity, bond types), selecting the lexicographically smallest traversal path, and applying consistent numbering for ring closures. This task encourages the model to learn structural invariance under permutation and reinforces graph-level understanding of molecular identity. The task format is provided in Figure 10."
        },
        {
            "title": "SMILES canonicalization",
            "content": "Answer only with the corresponding SMILES string without any other information. **Question:** Give me canonicalized SMILES string that represents the same molecule as the given one. **SMILES:** [SMILES] **ANSWER:** [CANONICAL SMILES] Figure 10: An instruction format of SMILES canonicalization. Fragment assembly. Fragment assembly evaluates whether the model can reconstruct full molecule from two disconnected SMILES fragments. This task tests global molecular coherence and the models ability to resolve attachment points into chemically valid structure. The instruction format of the instruction is shown in Figure 11."
        },
        {
            "title": "B Experimental details",
            "content": "In this section, we provide the details of the experiments. All experimental code related to this paper is available at https://anonymous.4open. science/r/CLEANMOL and our experiments are based on single run. We use NVIDIA A10080GB GPUs. We also apply low rank adaptation (Hu et al., 2022) and report results from single run. Our implementations are based on the transformers library (Wolf et al., 2020), the trl library (von Werra et al., 2020), the accelerate library (Gugger et al., 2022), and unsloth library 14 Answer only with the corresponding SMILES string without any other information. **Question:** Connect the following two SMILES fragments into unified structure at their reactive sites. **SMILES:** [FRAGMENT 1, FRAGMENT 2] **ANSWER:** [SMILES] Figure 11: An instruction format of SMILES assembly. (Daniel Han and team, 2023). Additionally, we used the packages including rouge-score==0.1.2 and nltk==3.8.1. B.1 SMILES parsing Here, we describe the detailed settings for the SMILES parsing experiments in Section 4, including the pre-trainig step with SMILES parsing tasks. Hyperparameters. The hyperparameters for all the models are provided in Table 5. We share the same hyperparameter for all the SMILES parsing tasks and base models. Notably, the model trained with SMILES parsing tasks is used as the pre-trained model for downstream tasks in Section 5."
        },
        {
            "title": "Lora r\nLora alpha\nLora dropout",
            "content": "16 5e4 1 0.01 0.1 cosine 1 1 0.2 64 16 0.05 Table 5: Hyperparameters for SMILES parsing. B.2 Downstream tasks Here, we describe the detailed settings for the downstream task experiments in Section 5. Hyperparameters. The hyperparameters for all the models are provided in Table 5. We share the same hyperparameter for all downstream tasks and base models. Notably, for the reproduced Molinstructions (Fang et al., 2024) models, we follow the hyperparameters given in the original paper."
        },
        {
            "title": "Lora r\nLora alpha\nLora dropout",
            "content": "16 5e4 1 0.01 0.1 cosine 1 1 0.2 64 16 0.05 Table 6: Hyperparameters for downstream tasks."
        },
        {
            "title": "C Additional experimental results",
            "content": "In this section, we provide additional experimental results including several concrete examples of generated samples. C.1 Molecular property prediction The molecular property classification task aims to predict binary labels for intrinsic physical or chemical properties, such as blood-brain barrier permeability or toxicity. Dataset. We use the MoleculeNet (Wu et al., 2018) dataset, focusing on three binary classification tasks: BACE, HIV, and Clintox. The BACE task predicts whether molecule can inhibit human β-secretase 1 (BACE-1). The HIV task involves predicting the ability of compounds to inhibit HIV replication. The Clintox task assesses whether compound is likely to fail clinical trials due to toxicity. We follow the splits provided in MoleculeNet. Baselines. We evaluate CLEANMOL by integrating it with two base models: LLaMA-3.1-8BInstruct (Grattafiori et al., 2024) and Qwen-2.57B-Instruct (Yang et al., 2024). For an absolute performance comparison, we include additional baselines: MolCA (Liu et al., 2023b), LlasMol (Yu et al., 2024) and InstructMol (Cao et al., 2023). Metrics. We evaluate the performance using accuracy, which denotes the overall proportion of correct predictions."
        },
        {
            "title": "BACE HIV Clintox",
            "content": "MolCA (1D+2D) LlasMolMistral InstructMol-GS LLaMA3.1-8B + CLEANMOL Qwen2.5-7B + CLEANMOL 0.798 0.821 0.507 0.639 0.533 0. 0.967 0.689 0.971 0.971 0.969 0.971 0.895 0.931 0.946 0.946 0.946 0. Table 7: Molecular property classification performance on the MoleculeNet dataset. Results. We report the results in Table 7. We observe that models pre-trained with CLEANMOL achieve consistent gains, confirming that the structural alignment learned during SMILES parsing transfers effectively to property classification tasks. C.2 Molecular property regression The molecular property regression task focuses on predicting continuous-valued molecular properties. Dataset. We again use the Mol-Instructions (Fang et al., 2024) dataset. We target quantum mechanics properties: HOMO energy, LUMO energy, and the energy gap (HOMOLUMO difference). We also follow the same split. Baselines We evaluate CLEANMOL by integrating it with two base models: LLaMA-3.1-8BInstruct (Grattafiori et al., 2024) and Qwen-2.57B-Instruct (Yang et al., 2024). For an absolute performance comparison, we include additional baselines: Alpaca (Tloen, 2023), Baize (Xu et al., 2023), Vicuna (Chiang et al., 2023), Galactica (Taylor et al., 2022), and Mol-Instructions (Fang et al., 2024). Here, the Mol-Instructions (SFT) follows the same training strategy described in Section 5.1. Metrics. We use mean absolute error (MAE) to evaluate prediction accuracy. Results. We report the results in Table 8. The results indicate that models pre-trained on SMILES parsing consistently outperform baselines, demonstrating that structural information learned via parsing enhances quantitative property prediction."
        },
        {
            "title": "D Usage of AI assistants",
            "content": "In preparing this work, we used AI-based writing assistants to improve sentence structure, correct grammatical errors, and enhance overall readability. These tools were employed solely for language"
        },
        {
            "title": "Model",
            "content": "Alpaca Baize Vicuna Galactica Mol-Instruct. (Lla.2) Mol-Instruct. (Lla.3) Mol-Instruct. (Lla.3.1)* Mol-Instruct. (SFT)* LLaMA3.1-8B + CLEANMOL Qwen2.5-7B + CLEANMOL"
        },
        {
            "title": "MAE",
            "content": "322.109 261.343 860.051 0.568 0.013 15.059 0.011 0.005 0.005 0.005 15.923 0.005 Table 8: Molecular property regression performance on the Molinstructions dataset. refinement and did not contribute to the development of technical content, research methodology, or experimental analysis. All scientific ideas, results, and conclusions presented in the paper were conceived and authored entirely by the researchers. The use of AI assistance was restricted to editorial purposes and did not affect the originality or intellectual contributions of the work."
        },
        {
            "title": "E Scientific Artifacts",
            "content": "The License for artifacts. All datasets and software tools used in this study comply with their respective licenses. Specifically, we utilized publicly available datasets such as ZINC250K (Irwin et al., 2012) and Mol-Instructions (Fang et al., 2024) in accordance with their usage terms. External tools such as RDKit were employed under their permissive open-source license. To support transparency and reproducibility, we release our trained models and source code at https://anonymous.4open. science/r/CLEANMOL under an appropriate opensource license. Artifact use consistency with intended use. All datasets and tools were used in manner consistent with their intended use. For instance, the MolInstructions dataset (Fang et al., 2024)originally designed for molecule generation and property predictionwas employed for aligned downstream tasks in our study. Likewise, RDKit was used exclusively for molecular structure analysis and data preprocessing, as intended by its developers."
        }
    ],
    "affiliations": [
        "KAIST",
        "Yonsei University"
    ]
}