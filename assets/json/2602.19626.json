{
    "paper_title": "Nacrith: Neural Lossless Compression via Ensemble Context Modeling and High-Precision CDF Coding",
    "authors": [
        "Roberto Tacconelli"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present Nacrith, a lossless compression system that combines a 135M-parameter transformer language model (SmolLM2-135M) with an ensemble of lightweight online predictors and a 32-bit arithmetic coder. Beyond the base LLM-plus-arithmetic-coding paradigm, Nacrith introduces several contributions: (1) a CDF precision upgrade from 2^16 to 2^24 that eliminates ~75% of quantization overhead caused by minimum-probability floors in large vocabularies; (2) a token-level N-gram model for fast local predictions; (3) an adaptive log-space bias head correcting per-document LLM errors via online gradient descent; (4) confidence-based LLM skip for accelerating highly predictable tokens; (5) a hybrid binary format (NC06) extending neural compression to arbitrary binary files--to our knowledge a first among LLM-based compressors; (6) a llama.cpp inference backend achieving ~7x faster single-token decode than PyTorch; (7) parallel multi-GPU compression across up to 8 workers; and (8) native KV cache sliding window reducing per-slide cost by ~37x. The system requires only ~500 MB of GGUF weights and ~1.2 GB VRAM per worker, running on consumer GPUs. On alice29.txt (Canterbury Corpus, 152 KB), Nacrith achieves 0.918 bits per byte (bpb)--outperforming gzip by 3.1x, bzip2 by 2.5x, CMIX v21 by 44%, and ts_zip by 20%, while compressing below the 0th-, 1st-, and 2nd-order byte-level Shannon entropy bounds. On enwik8 (100 MB), Nacrith achieves 0.9389 bpb (11.74%), surpassing ts_zip (~1.11 bpb) by 15% and FineZip (1.024 bpb) by 8% despite using a 60x smaller model with no fine-tuning. An out-of-distribution evaluation on a document published after the model's training cutoff confirms these gains are not memorization artifacts, achieving 0.723 bpb on unseen text."
        },
        {
            "title": "Start",
            "content": "Nacrith: Neural Lossless Compression via Ensemble Context Modeling and High-Precision CDF Coding Roberto Tacconelli1 1Independent Researcher, tacconelli.rob@gmail.com 6 2 0 2 3 2 ] . [ 1 6 2 6 9 1 . 2 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "We present Nacrith, lossless compression system that combines 135-million-parameter transformer language model (SmolLM2-135M) with an ensemble of lightweight online predictors and 32-bit arithmetic coder, achieving the best compression results among the systems evaluated in this study on natural language text. On the Canterbury Corpus benchmark (alice29.txt, 152 KB), Nacrith achieves 0.918 bits per byte (11.5% of original size)outperforming, in our experiments, gzip by 3.1, bzip2 by 2.5, CMIX v21 (1.63 bpb) by 44%, and ts_zip (1.14 bpb) by 20%, while compressing below the 0th-, 1st-, and 2nd-order bytelevel Shannon entropy bounds of the source data. On the standard enwik8 benchmark (100 MB Wikipedia extract), Nacrith achieves 0.9389 bpb (11.74%, 11,737,280 bytes)the best result among the systems evaluatedoutperforming ts_zip (1.11 bpb) by 15%, FineZip (1.024 bpb) by 8% despite using 60 smaller model with no fine-tuning, and gzip by 3.1. Beyond the base LLM-plus-arithmetic-coding paradigm, Nacrith introduces nine contributions: (1) CDF precision upgrade from 216 to 224 that eliminates 75% of quantization overhead caused by minimum-probability floors in large vocabularies; (2) token-level N-gram model providing fast local predictions; (3) an adaptive log-space bias head that corrects per-document LLM prediction errors via online gradient descent; (4) confidence-based LLM skip that accelerates compression on highly predictable tokens; (5) hybrid binary format (NC06) extending neural compression to arbitrary binary filesa capability absent from prior LLM-based compressors known to us; (6) llama.cpp inference backend that replaces PyTorch with native C/C++ GPU inference, achieving 7 faster single-token decode; (7) parallel multi-GPU compression that splits text across up to 8 concurrent worker threads, each running an independent model instance; and (8) native KV cache sliding window that uses direct cache manipulation instead of full re-evaluation, reducing per-slide cost by 37. The system requires only 500 MB of GGUF model weights and 1.2 GB of VRAM per worker instance, running on consumer GPUs as low-end as the GTX 1050 Ti. Keywords: lossless compression, neural compression, language models, arithmetic coding, context mixing, ensemble prediction 1. Introduction Shannons foundational work [20] established that compression is equivalent to prediction: model assigning high probability to the next symbol enables an encoder to represent that symbol with fewer bits. Early neural approaches [19] demonstrated this principle with recurrent networks. This equivalence has driven decades of compressorsfrom LZ77 to PPM [5] and the PAQ/CMIX families [15, 16]each advancing statistical prediction over byte sequences. Transformer language models [21] offer qualitatively different approach, capturing grammar, semantics, and world knowledge. Delétang et al. [6] showed that Chinchilla 70B achieves 0.664 bpb on enwik9 via arithmetic coding, outperforming gzip and CMIX on that benchmark. Practical implementations have followed: FineZip [11] and Bellards ts_zip [3]. However, these systems use either massive models (770B parameters), require fine-tuning, or sacrifice expressiveness through rank-based encoding and coarse CDF quantization. In this paper, we present Nacrith, which makes nine contributions: 1. CDF precision upgrade (CDF-24). We identify critical quantization bottleneck in largevocabulary arithmetic coding: with = 49,152 tokens and CDFtotal = 216 = 65,536, minimumprobability floors consume 75% of the CDF range before any actual probability is encoded. Upgrading to 224 reduces floor overhead from 2 bits/token to 0.004 bits/token, yielding direct improvement in compressed size. 2. Token-level N-gram model. An interpolated token-level N-gram model (orders 14) adapts online to the document being compressed, capturing local statistical patterns not exploited by the pre-trained LLM. 3. Adaptive context mixer. linear mixing layer with exponential-weight online updates [23] blends LLM and N-gram predictions, automatically weighting models that predict well for the current document. 4. Adaptive log-space bias head. thin onlinelearning layer adjusts LLM log-probabilities via SGD after each observed token, correcting systematic overor under-prediction for specific documents. 5. Hybrid binary format (NC06). chunked container format segments arbitrary binary files into text and non-text regions, applying neural compression to text-like content and traditional codecs to opaque datathe first LLM-based compressor to handle non-text files. We 6. llama.cpp inference backend. replace PyTorch with llama.cpp [9] (via the llama-cpp-python bindings) as the primary GPU inference engine, achieving 7 faster singletoken decode by eliminating Python/PyTorch dispatch overhead. HuggingFace tokenizer is retained for text tokenization/detokenization to avoid llama.cpps detokenizer limitations on 47 whitespace/repeat tokens. 7. Parallel multi-GPU compression. The input text is split into chunks (up to 8), each compressed concurrently by an independent worker thread with its own model instance, secondary models, and KV cache. Worker count is auto-detected from available VRAM. 8. Native KV cache sliding window. Instead of resetting and re-evaluating 1,536 tokens on every context slide, we use llama.cpps native kv_cache_seq_rm and kv_cache_seq_shift operations to drop old tokens and shift positions in-place, reducing per-slide cost by 37. Together, these contributions yield 0.918 bpb on alice29.txt (152 KB Canterbury Corpus) and 0.9389 bpb on enwik8 (100 MB Wikipedia benchmark), using only 135M-parameter model with 500 MB of GGUF weights. 2. Related Work 2.1 Classical Lossless Compression Dictionary-based methods (LZ77 [25], gzip/DEFLATE, LZMA/xz, Zstandard) exploit local byte repetitions within sliding window. Huffman coding [13] assigns variable-length codes by symbol frequency; arithmetic coding [24] removes the integer-bit constraint, approaching entropy to within fraction of bit. 2.2 Context Mixing and Adaptive Statistical Models Prediction by Partial Matching (PPM) [5] uses adaptive high-order context modeling with arithmetic coding, achieving 2.0 bpb on English text. The PAQ family [16] extends this with context mixing: blending predictions from hundreds of specialized models via neural networks at the bit level. PAQ8px achieves 1.27 bpb on enwik8 (with -12L, [17]), but only 1.73 bpb on alice29.txt (152 KB)worse than CMIX on the same file, demonstrating that adaptive contextmixing compressors require large inputs to warm up. CMIX [14] pushed further by incorporating LSTM networks alongside 2,000+ context models, reaching 1.17 bpb on enwik8 and 0.86 bpb on enwik9but at extreme cost (1664 GB RAM, 0.55 KB/s). Crucially, on smaller files such as alice29.txt (152 KB), CMIX achieves only 1.63 bpb due to limited adaptive context for its massive model ensemble. NNCP [2] uses Transformer-XL trained online during compression, achieving 1.19 bpb on enwik8. However, on small files (alice29.txt), the per-file model training overhead inflates the output to 3.96 bpb worse than gzip [12]. Nacrith adopts the context mixing philosophy in pre-trained LLM setting: instead of hundreds of bitlevel models, it mixes two token-level predictors (LLM and N-gram) with online weight adaptation, combining adaptive ensemble modeling with the strong prior of pre-trained transformer."
        },
        {
            "title": "2.3 LLM-Based Compression",
            "content": "for such neural coding systems compression general-purpose Earlier as DeepZip [10] combined recurrent networks with arithmetic lossless compression. Delétang et al. [6] formalized the LLMas-compressor paradigm: Chinchilla 70B achieves 0.664 bpb on enwik9 (8.3% of raw size); the papers compression-ratio chart  (Fig. 3)  indicates 1.6 bpb (0.2 compression ratio) for enwik8. FineZip [11] builds on LLMZip [22] and improves with LoRA fine-tuning and batched inference using LLaMA-3-8B. Bellards ts_zip [3] uses RWKV-169M with 8-bit quantization and arithmetic coding, achieving 1.11 bpb on enwik8 and 1.14 bpb on alice29.txtthe most comparable prior system to Nacrith. LLM-TextCompressor [8] also exists but lacks binary support, standardized formats, and efficient CDF coding. 2.4 Positioning of Nacrith Table 1 summarizes the landscape. Nacrith achieves the best result on enwik8 (100 MB) among the systems evaluated in this study, at 0.9389 bpb outperforming ts_zip (1.11 bpb) by 15%, NNCP (1.19 bpb) by 21%, FineZip (1.024 bpb) by 8% despite using 60 smaller model without fine-tuning, and all evaluated classical compressors by 2.13.1. On alice29.txt (152 KB), Nacrith achieves 0.918 bpb, outperforming ts_zip (1.14 bpb) and CMIX v21 (1.63 bpb). It is also, to our knowledge, the only LLM-based compressor supporting arbitrary binary files. 3. Method 3.1 Overview Nacrith compresses text by tokenizing the input and iteratively predicting the probability of each token given its context using an ensemble of models. These 2 Table 1: Comparison of lossless compressors on enwik8 (100 MB). All bpb values are for enwik8. Model weights excluded from compressed size. LoRA adapter stored in compressed output. Visually extracted from the model-size chart  (Fig. 3)  in [6]; the same paper reports 0.664 bpb on enwik9 (1 GB). PAQ8px enwik8 figure is for the -12L setting [17]. System Model Params Weights bpb (enwik8) Binary Open Source gzip (DEFLATE) xz (LZMA2) bzip2 CMIX v21 PAQ8px NNCP v3 Delétang et al. FineZip ts_zip LSTM + mixing Context mixing Transformer-XL Chinchilla LLaMA-3-8B RWKV-169M 50M online 70B 8B 169M in-code in-output 140 GB 16 GB 169 MB 2.916 1.989 2.321 1.17 1.27 1.19 1.6 1.024 1.11 Nacrith (ours) SmolLM2-135M 135M 500 MB 0.9389 blended probabilities are fed into an arithmetic coder. Decompression is the mirror image: the same ensemble generates identical predictions, and the arithmetic decoder recovers each token. Because every component is deterministic with identical state on both sides, reconstruction is perfectly lossless. For non-text files, Nacrith employs hybrid format (NC06) that segments the input into text-like and binary regions. For large files, the input is split into chunks and compressed in parallel across multiple GPU worker threads. The full system is summarized in Algorithm 1. Algorithm 1 Nacrith compression pipeline Require: text string 1: Tokenize (t1, . . . , tn) 2: Initialize: LLM KV-cache, N-gram, mixer, adaptive head png Ngram.predict(t1, . . . , ti1) if H(png) < τ then png {skip LLM} 3: for = 1 to do 4: 5: 6: 7: 8: else pllm LLM(t1, . . . , ti1) AdaptiveHead.adjust(pllm) 9: end if 10: cdf probs_to_cdf(p, 224) 11: 12: ArithEncoder.encode(cdf, ti) 13: Update: N-gram, Mixer, AdaptiveHead with ti 14: end for 15: return ArithEncoder.finish() 3.2 Neural Probability Model We use SmolLM2-135M [4], 30-layer causal transformer with 135 million parameters and BPE vocabulary of 49,152 tokens. The model runs in FP32 precision, ensuring deterministic probability distributions across hardwarea critical requirement for lossless reconstruction. Given token sequence t1, . . . , tn, it 3 produces: Pllm(tn+1 t1, . . . , tn) = softmax(cid:0)Whn + b(cid:1) (1) where hn is the hidden state at position n. 3.3 llama.cpp Inference Backend Nacrith uses llama.cpp [9] as its primary GPU inference engine, accessed through the llama-cpp-python Python bindings. Compared to PyTorch, which incurs significant Python-level dispatch overhead per forward call, llama.cpp performs all GPU computation in C/C++ with single PythonC boundary crossing, achieving approximately 7 faster single-token incremental decode on the same hardware. The model is loaded in GGUF format (F32 precision, 500 MB), with the KV cache allocated for the full 2,048-token context window. After each forward pass, the logit vector (V float32 = 49,152 4 196 KB) is transferred from GPU to CPU and converted to probability distribution via softmax. An optional temperature scaling step is applied before softmax: logits are divided by scalar τ > 0 (stored as 2-byte field in the NC05/NC06 header), sharpening (τ < 1) or flattening (τ > 1) the distribution. At the default τ = 1.0 the step is no-op. dual tokenizer architecture is employed: llama.cpp handles GPU inference, while the HuggingFace tokenizer handles text tokenization and detokenization. This is necessary because llama.cpps built-in detokenizer silently drops content for 47 whitespace and repeat tokens in the SmolLM2 vocabulary; the HuggingFace tokenizer handles these correctly. Token IDs are identical between both implementations, so the split introduces no inconsistency. If llama.cpp is not available, the system falls back to PyTorch with CUDA Graphs (GPU) or dynamic KV cache (CPU), ensuring portability."
        },
        {
            "title": "3.4 High-Precision CDF Quantization (CDF-",
            "content": "24) The CDF-16 bottleneck. Arithmetic coding requires the probability distribution to be quantized to an integer CDF summing to . Each of the = 49,152 vocabulary tokens must receive at least MIN_PROB = 1 count to avoid zero-width intervals. With = 216 = 65,536, the floor allocation alone is: MIN_PROB = 49,152 65,536 75% (2) This leaves only 16,384 counts ( 25% of the range) for actual probability information. The resulting quantization error introduces approximately: log (cid:17) (cid:16) 2 log (4) = 2 bits/token (3) for peaked distribution, degrading the arithmetic coder far below the theoretical entropy rate. CDF-24 upgrade. We upgrade to = 224 = 16,777,216: = 49,152 16,777,216 0.29% (4) The floor overhead drops to 0.004 bits/token. The remaining 16,727,064 bins are allocated proportionally to token probabilities: ci = max(1, pi (T )) (5) with the residual ci added to carg max p. Safety with 32-bit coder. The arithmetic coder maintains 32-bit range = highlow+1 [231, 232] after renormalization. The minimum symbol width after narrowing is MIN_PROB/T 231/224 = 128, well above the representability threshold for 32-bit arithmetic [24]. 3.5 Token-Level N-gram Model We maintain an interpolated token-level N-gram model of orders 14. The unigram base distribution is Laplacesmoothed: Puni(t) = c(t) + 1 + where c(t) is the token count and is total tokens seen. For each order 1, if context ck = (tik, . . . , ti1) has been seen nk times, its contribution is blended with lower orders via the interpolation weight λk = nk/(nk + ϵ) (ϵ = 5): (6) Two performance optimizations bound the per-token cost as tables grow: (1) context keys use deterministic 64-bit rolling hash instead of Python tuple objects, eliminating 54M tuple allocations per worker and reducing garbage collection pressure; (2) each contexts inner continuation dictionary is capped at 64 entries (evicting the lowest-count token when exceeded), preventing the O(continuations) iteration in predict from degrading as common contexts accumulate hundreds of unique successors over time. These optimizations, combined with storing continuation counts in pre-allocated int32 numpy arrays (up to 500,000 contexts 64 slots 4 orders), reduce perworker N-gram memory from 3.6 GB (nested Python dicts) to 128 MBa 28 reduction that makes multiworker operation feasible on consumer GPUs."
        },
        {
            "title": "3.6 Adaptive Context Mixer",
            "content": "Given predictions {p(j)}M from models (LLM and N-gram by default), the mixer computes weighted linear combination: j=1 pmix = j=1 wj p(j), wj = 1 (8) Linear mixing (rather than geometric) preserves the if the LLM assigns dominant models confidence: pllm(t) = 0.90 with weight wllm = 0.85, the mixed probability is 0.765, maintaining sharp distribution for the arithmetic coder. Geometric mixing was also considered but degrades compression in practice: near-uniform secondary model predictions flatten the distribution, widening every arithmetic coder interval. Weights are adapted using the exponential weights algorithm [23]: after observing token ti, each model receives an update: (9) log wj += η log p(j)(ti) followed by renormalization. This is equivalent to Bayesian updating of the model posterior: models that consistently predict observed tokens well gain weight exponentially faster. Initial weights are LLMdominant: wllm = 0.85, remaining 0.15 split equally among secondary models. warmup period of 100 tokens uses the LLM alone while secondary models accumulate sufficient data to form reliable distributions. 3.7 Adaptive Log-Space Bias Head The adaptive head maintains bias vector RV (initialized to zero) and applies multiplicative correction to LLM log-probabilities: pllm(t) ebt pllm(t) ebt = softmax(cid:0)log pllm + b(cid:1) (10) Pk = λk ˆPk(ck) + (1 λk) Pk1 (7) p(t) = The N-gram model is updated online after each token, allowing it to adapt to document-specific vocabulary and phrasing. This complements the LLMs general linguistic knowledge with local statistical regularities of the text being compressed. After each observed token t, the bias is updated by one step of gradient descent on the cross-entropy loss = log p(t): bt = α bt = α (cid:0)p(t) 1[t = t](cid:1) (11) with learning rate α = 0.001. At zero bias the transform is the identity; as compression proceeds it learns to suppress tokens the LLM systematically over-predicts and boost under-predicted ones for this specific document. Because compressor and decompressor apply identical updates in identical order, lossless symmetry is maintained. All computations use float64 to ensure bit-exact reproducibility under identical hardware and software configuration (same GPU architecture, BLAS library, and driver version)."
        },
        {
            "title": "3.8 Confidence-Based LLM Skip",
            "content": "LLM inference is the dominant computational cost. When the N-gram model expresses high confidence (Shannon entropy H(png) < τ bits, with τ = 1.5 calibrated empirically on held-out sample), the token is highly predictable and the LLMs additional accuracy contributes little. In this case, we skip the LLM forward pass entirely and use the N-gram prediction directly: = png (12) The ablation study (Section 5.6) reveals that this mechanism is far more than throughput optimizationit is the primary channel through which the N-gram model contributes to compression, accounting for the majority of the improvement from A2 to A3. On highly compressible text the skip rate reaches 3070%, substantially reducing GPU load while simultaneously improving compression quality. 3.9 Arithmetic Coding We implement 32-bit arithmetic coder following Witten et al. [24]. The encoder maintains range [low, high] [0, 232), narrowing it by each symbols CDF interval. Renormalization emits bits when both endpoints fall in the same half, with underflow counters handling near-convergence. The decoder maintains symmetric 32-bit value register with binary search over the CDF for symbol recovery. 3.10 Native KV Cache Sliding Window Nacrith uses the transformers key-value cache for O(1) amortized per-token inference. When the context exceeds = 2,048 tokens, = 512 tokens must be dropped from the beginning of the context. naïve implementation would reset the entire KV cache and re-evaluate the remaining = 1,536 tokens from scratchwasting 693 ms per slide on GTX 1050 Ti and introducing 4 average overhead per token. Instead, Nacrith uses llama.cpps native KV cache manipulation: 1. kv_cache_seq_rm(0, 0, C) removes positions [0, C) from the cache. 2. kv_cache_seq_shift(0, C, -1, -C) shifts all remaining positions down by C. 3. Only the final token is re-evaluated at its new position (19 ms). This reduces per-slide cost by 37 (19 ms vs. 693 ms), making the sliding window overhead negligible: Ttok Tslide + Tincr Tincr + Tslide (13) With Tslide Tincr, the amortized overhead is approximately 1 + 1/C 1.002effectively zero compared to the 4 overhead of full cache rebuilds."
        },
        {
            "title": "3.11 Hybrid Binary Compression (NC06)",
            "content": "Nacrith handles arbitrary binary files through the NC06 hybrid format. The input is segmented into alternating text and binary chunks: (1) bytes in printable ASCII (32126) plus tab/LF/CR are classified as text-like; (2) short text runs (< 64 bytes) are demoted to binary; (3) binary gaps 8 bytes between text runs are bridged; (4) small binary chunks (< 64 bytes) adjacent to text are absorbed. All binary chunks are concatenated into single blob and compressed with LZMA ( 4 KB blobs) or gzip (smaller), or stored raw if neither helps. Text chunks are compressed in parallel across GPU workers using the full ensemble pipeline. The NC06 container stores flags byte (encoding active features), temperature, entry table, binary section, and parallelized text streams, enabling the decompressor to reproduce the exact ensemble configuration used during compression. 3.12 Parallel Multi-GPU Compression To exploit modern multi-core GPUs and maximize throughput, Nacrith splits the input text into roughly equal chunks (at newline boundaries) and compresses them concurrently. Each worker thread owns an independent llama.cpp model instance, secondary models (N-gram), context mixer, and adaptive headeliminating all shared state and synchronization overhead. The number of workers is auto-detected from available VRAM: = min (cid:18) 8, 1 + (cid:22) VRAMfree (cid:23)(cid:19) (14) where 1,169 MB is the cost of the first instance (model weights + KV cache), 660 MB is the cost of each additional instance (KV cache only, as model weights are shared at the GPU driver level), and = 512 MB is reserved for the OS. maximum of 8 workers is enforced, as GPU contention diminishes returns beyond this point. The worker count can be overridden via the --workers flag. Threading is effective despite Pythons GIL because llama-cpp-python releases the GIL during C-level GPU inference, which dominates per-token cost. The parallel streams are stored in the NC05 (text) and NC06 (hybrid binary) container formats, which include per-chunk header table encoding token count, bit count, and stream length."
        },
        {
            "title": "3.13 NC05/NC06 File Formats",
            "content": "The NC05 header (9 bytes) encodes: 4-byte magic NC05, 1-byte feature flags (N-gram, adaptive head, confidence skip), 2-byte temperature τ (enabling the decompressor to replicate the exact LLM probability distribution used during compression), and 2-byte chunk count. This is followed by per-chunk table (12 bytes each: token count, bit count, stream length) and the concatenated arithmetic-coded streams. NC06 uses an extended header with magic NC06 plus version byte and structured entry table for alternating text/binary chunks. 4. Experimental Setup"
        },
        {
            "title": "4.1 Hardware and Software",
            "content": "All experiments use an NVIDIA GeForce GTX 1050 Ti (4 GB VRAM, CUDA capability 6.1), chosen deliberately to demonstrate practical accessibility. VRAM usage is approximately 1.2 GB per worker instance. The software stack uses llama.cpp [9] (via llama-cpp-python) as the primary inference backend with the model in GGUF F32 format (500 MB), and HuggingFace Transformers for tokenization. With 4 GB VRAM, the GTX 1050 Ti supports up to 3 concurrent worker instances. 4.2 Baselines We compare against five traditional compressors at maximum settings: gzip (DEFLATE, level 9), xz (LZMA2, level 9), bzip2 (level 9), Brotli (quality 11), and Zstandard (level 19). We additionally compare against CMIX v21 and ts_zip from published results. 4.3 Benchmarks We standardize on alice29.txt from the Canterbury Corpus [1] as the primary benchmark: 152,089-byte excerpt of Alices Adventures in Wonderland, widely used in the compression literature. We additionally evaluate on asyoulik.txt (Canterbury Corpus, 125 KB Shakespeare plays), and three custom English prose samples (3 KB, 50 KB, 100 KB). 5. Results 5.1 Compression Results Table 2 shows that Nacrith achieves 0.918 bpb (11.5%) on alice29.txt3.1 better than gzip, 2.5 better than bzip2, 44% better than CMIX v21, and 20% better than ts_zip. All results are fully lossless. Table 3 extends results across multiple files. On modern English prose (the three sample files), Nacrith achieves 0.630.76 bpb (7.99.5%)approaching Chinchilla 70Bs 0.664 bpb on enwik9, though direct comparison is confounded by dataset size and training data differences. On asyoulik.txt (Shakespeare), compression rises to 1.30 bpb (16.3%), reflecting that archaic Table 2: Compression results on alice29.txt (152,089 bytes, Canterbury Corpus). All results directly measured. PAQ8px run at -8L (level 8 with LSTM, v211). Compressor Size (B) Ratio Original gzip -9 zstd -19 xz -9 Brotli -q 11 bzip2 -9 PAQ8px -8L CMIX v21 ts_zip (RWKV-169M) Nacrith 152,089 100.0% 54,191 49,215 48,500 46,487 43, 35.6% 32.4% 31.9% 30.6% 28.4% bpb 8.000 2.851 2.589 2.551 2.445 2.273 21.6% 20.4% 32,857 31, 1.728 1.635 21,703 14.3% 1.142 11.5% 0.918 17,458 Table 3: Nacrith compression across multiple text types. Modern English prose achieves 0.630.76 bpb. File Orig. Compr. bpb sample_3k.txt sample_50k.txt sample_100k.txt alice29.txt asyoulik.txt 3,072 51,200 102,863 152,089 125,179 263 0.685 4,059 0.634 9,817 0.764 17,458 0.918 20,408 1.304 vocabulary is less predictable for modern-Englishtrained model. 5.2 Shannon Entropy Analysis Table 4 shows that Nacrith achieves 0.918 bpb, which is below the byte-level Shannon entropy bounds computed from n-gram statistics (H0=4.57, H1=3.42, H2=2.49). For comparison, gzip and xz both operate near the 2nd-order Shannon limit; CMIX, despite 2,000+ adaptive models, achieves only 1.63 bpb on this 152 KB file. It is important to note that byteInterpretation. level n-gram entropy bounds are weak upper bounds on the true source entropy: they capture only short-range byte correlations (up to trigrams for H2) and ignore the long-range syntactic, semantic, and discourse structure that natural language exhibits. The LLM operates over 2,048-token context window and models dependencies at the token level (subword units spanning 35 bytes on average), capturing correlations far beyond what byte-level trigram statistics can represent. Compressing below H2 therefore does not violate information theoryit demonstrates that the true conditional entropy of the source, under the much higher-order model used by the LLM, is substantially lower than what bytelevel n-gram estimates suggest. The Shannon bounds reported here should be understood as reference points illustrating the gap between classical statistical models and neural language models, not as fundamental limits on the compressibility of the source. 6 Table 4: Shannon entropy bounds vs. actual compressed sizes on alice29.txt (152,089 B). Nacrith compresses 80% below the 0th-order, 73% below the 1storder, and 63% below the 2nd-order bounds. Table 6: Out-of-distribution evaluation on text published after SmolLM2s training cutoff. FineZip uses the same SmolLM2-135M model as Nacrith for controlled comparison isolating the architecture. Method Size bits/byte Compressor Size (B) bpb Original Shannon 0th-order Shannon 1st-order Shannon 2nd-order gzip -9 xz -9 CMIX v21 Nacrith 148.5 KB 84.8 KB 63.5 KB 46.1 KB 52.9 KB 47.4 KB 30.3 KB 17.0 KB 8.000 4.568 3.419 2.485 2.851 2.551 1.634 0.918 Table 5: Bits per byte across systems and scales. alice29 column shows verified results on alice29.txt (152 KB). All enwik8 classical values directly measured. Compression ratio 0.2 visually extracted from the model-size chart  (Fig. 3)  in [6] for enwik8; the same paper reports 0.664 bpb on enwik9 (1 GB). Weights excluded. No alice29 result. PAQ8px enwik8 figure is for -12L [17]. System Params alice29 enwik8 gzip -9 xz -9 bzip2 -9 Brotli -q 11 zstd -19 CMIX v21 NNCP v3 PAQ8px -8L Chinchilla FineZip ts_zip Nacrith 50M online 2.85 2.55 2.27 2.45 2.589 1.63 3.96 1. 70B 8B 1.14 169M 135M 0.918 2.916 1.989 2.321 2.059 2.156 1.17 1.19 1.27 1.6 1.024 1.11 0.9389 5.3 Cross-System Comparison Table 5 illustrates the benchmark scale dependency. NNCP degrades to 3.96 bpb on alice29.txt due to model weight overhead; CMIX degrades from 0.86 bpb (enwik9) to 1.63 bpb (alice29.txt); and PAQ8px, despite achieving 1.27 bpb on enwik8 (with -12L), compresses to only 1.73 bpb on alice29.txt (our direct measurement, v211 -8L)worse than CMIX at the same scale. All three adaptive systems have less data to train their context models on. Nacrith achieves 0.918 bpb on alice29.txt with only 135M pre-trained parameters, outperforming all systems with verified alice29 results. On enwik8 (100 MB Wikipedia extract), Nacrith achieves 0.9389 bpb (11,737,280 bytes, 11.74% of original). Classical compressors range from 1.989 bpb (xz -9) to 2.916 bpb (gzip -9), with bzip2 at 2.321, Brotli at 2.059, and zstd at 2.156 bpb; all are significantly worse. Among LLM-based systems, FineZip achieves 1.024 bpb using fine-tuned LLaMA-3-8B (60 more parameters); Nacrith outperforms it by 8% in our experiments, without any fine-tuning. ts_zip (RWKVOriginal gzip -9 zstd -19 xz -9 bzip2 -9 Brotli -q 11 CMIX v21 ts_zip (RWKV-169M) FineZip (SmolLM2-135M) Nacrith 333,794 8.000 91,348 79,709 72,552 69,305 68, 47,897 40,237 40,747 30,171 2.189 1.910 1.739 1.661 1.646 1.148 0.964 0.977 0.723 169M) achieves 1.11 bpb; Nacrith outperforms it by 15% with comparable model size. We note that both alice29.txt and enwik8 (Wikipedia) are almost certainly present in SmolLM2s training corpus. The same likely holds for ts_zips RWKV model. Recent work [12] has shown that documents present in the LLMs training data compress significantly better than unseen text, so these results may partially reflect memorization rather than purely generalizable compression. We report both benchmarks for standardization with the broader community (CMIX, PAQ, FineZip), while acknowledging this caveat. Section 5.4 evaluates on document published after SmolLM2s training cutoff to address this concern. 5.4 Out-of-Distribution Evaluation To control for training data contamination, we evaluate on plain text extracted from the UK Governments English Indices of Deprivation 2025: Technical Report, published in October 2025. The extracted text corpus is archived at Zenodo [18]. SmolLM2-135M was released in October 2024 on Hugging Face, so this document is definitively absent from its training corpus. Table 6 shows that Nacriths advantage holds on unseen text: 0.723 bpb (9.0%), outperforming ts_zip by 25% and CMIX by 37%. The controlled comparison with FineZip is particularly informative: both use the same SmolLM2-135M model, yet Nacrith compresses 26% smaller (30,171 vs. 40,747 bytes), isolating the contribution of CDF-24 quantization, the N-gram confidence skip, and the adaptive head from the LLM itself. The OOD result (0.723 bpb) is actually better than alice29.txt (0.918 bpb), suggesting that modern government prose is more predictable for language model than 19th-century literary English. This confirms that the compression quality observed on standard benchmarks is not primarily an artifact of training data memorization. Table 7: Ablation study: incremental feature contributions. is the bpb reduction from the previous row. All runs use workers 1. 6. Discussion"
        },
        {
            "title": "6.1 Why CDF-24 Matters",
            "content": "enwik8 (1 MB) alice29 Config bpb bpb 1.817 A0: LLM + AE 1.300 A1: + CDF-24 A2: + Adaptive head 1.285 A3: + N-gram + Skip 0.897 0.896 A4: Full system 1.861 0.517 1.358 0.503 0.015 1.341 0.017 0.388 0.940 0.401 0.001 0.939 0."
        },
        {
            "title": "5.5 Throughput",
            "content": "With single worker on the GTX 1050 Ti, Nacrith achieves 5070 tokens/second at the start of file, settling to 2030 tok/s as the KV cache fills to its 2,048-token steady state (attention cost scales linearly with cached positions). With 3 parallel workers (the maximum for 4 GB VRAM), aggregate throughput scales to 6090 tok/s. The switch from PyTorch to llama.cpp (7 faster per-token inference) and the native KV cache sliding window (37 faster per slide) are the primary contributors to the throughput improvement over the earlier PyTorch-based version. The confidence-based LLM skip provides additional speedup on highly repetitive content by bypassing the GPU forward pass for fraction of tokens. 5.6 Ablation Study Table 7 isolates each components contribution by incrementally enabling features on enwik8 (first 1 MB) and alice29.txt, using single worker to eliminate parallelization effects. CDF-24 provides the largest single improvement (0.52 bpb on enwik8, 28%), confirming that CDF precision is the dominant bottleneck in largevocabulary neural coding. Confidence-based skip with N-gram is the second largest contributor (0.39 bpb, 30%). Notably, the N-gram model contributes exclusively through the skip mechanism, not through the mixer. The exponential weight updates in the context mixer rapidly drive secondary model weights toward zerothe 135Mparameter LLM consistently outperforms simple statistical models on non-trivial tokens, causing the mixer to converge to wllm 1.0 within few dozen tokens. On tokens where the N-gram is confident (entropy < 1.5 bits), however, the skip bypasses the LLM entirely and uses the N-gram prediction directly, achieving near-optimal coding. The adaptive head provides small but consistent improvement (0.015 bpb, 1.1%), and synergizes with the skip mechanism in the full system (A4 vs. A3). The CDF-16 to CDF-24 upgrade is arguably the most impactful single change in the system. With 49,152token vocabulary, CDF-16 sacrifices the large majority of arithmetic coder precision to the minimumprobability floor, degrading every symbols code length by up to 2 bits. CDF-24 eliminates this overhead almost entirely. This improvement is specific to largevocabulary neural compressors; to our knowledge, this issue has not been explicitly quantified in prior LLMbased compression work."
        },
        {
            "title": "6.2 The Ensemble Effect",
            "content": "The ablation study  (Table 7)  reveals that the ensembles contribution operates through different mechanism than originally designed: The context mixer path (post-warmup, nonconfident tokens) converges to 100% LLM weight within few dozen tokens. The 135M-parameter LLM so consistently outperforms the N-gram model on non-trivial tokens that the exponential weight updates drive secondary model weights to floatingpoint zero (< 10300). In practice, the mixer acts as pass-through for the LLM prediction. The confidence skip path is the primary channel through which secondary models improve compression. When the N-gram entropy falls below 1.5 bits, the token is so predictable that the N-gram alone provides near-optimal codingand the LLM is bypassed entirely. This contributes the bulk of the 30% improvement from A2 to A3. The adaptive head provides small, consistent improvement by correcting systematic LLM biases: tokens consistently over-predicted are downweighted, and vice versa. This architecture naturally partitions tokens into two regimes: predictable tokens (coded cheaply by the N-gram via skip) and surprising tokens (coded by the full LLM). The result is system where each model operates at its strengththe LLM handles complex linguistic dependencies, while the N-gram handles locally predictable patterns at near-zero computational cost. 6.3 Comparison with Related Systems 6.3.1 vs. ts_zip [3] ts_zip is the closest prior system. Both use 135 169M parameter models with arithmetic coding. Key differences: (1) Nacrith adds the ensemble (N-gram, adaptive head, confidence skip) and CDF-24 upgrade; (2) Nacrith is open-source; (3) Nacrith provides binary file support via NC06 and parallel multi-GPU compression via NC05/NC06; (4) ts_zip uses RWKV (linear-time recurrent inference) while Nacrith uses standard transformer with llama.cpp acceleration. The compression gap0.918 vs. 1.14 bpb on alice29.txt 8 and 0.9389 vs. 1.11 bpb on enwik8likely reflects the ensemble contributions combined with CDF-24. 6.3.2 vs. CMIX [14] CMIX mixes hundreds of bit-level adaptive models with an LSTM. Nacrith achieves better compression on both alice29.txt (0.918 vs. 1.63 bpb) and enwik8 (0.9389 vs. 1.17 bpb) in our experiments, while being far simpler: two token-level models instead of 2,000+ bit-level ones, and requiring 1.2 GB VRAM per worker vs. 1664 GB RAM. The pre-trained LLM provides an extremely strong prior that CMIXs adaptive models cannot match. 6.3.3 vs. FineZip FineZip uses an 8B model with suboptimal coarselyquantized CDF coding. Nacrith outperforms it on both alice29.txt (0.918 vs. 1.024 bpb) and enwik8 (0.9389 vs. 1.024 bpb) in our benchmarkswith 60 smaller model and no fine-tuning demonstrating that precise CDF quantization and ensemble context mixing matter more than raw parameter count. 6.3.4 vs. LLM-Text-Compressor LLM-Text-Compressors 74.6% compression ratio reflects two inefficiencies: rank-based encoding (discards probability magnitudes) and 16-token context window. Nacriths 11.5% on alice29.txt represents 6.5 improvement from full arithmetic coding and 2,048-token context. 6.4 Limitations 1. Throughput. At 21 tokens/s on GTX 1050 Ti, compression is orders of magnitude slower than traditional compressors. Suitable for archival applications; real-time use requires modern hardware. 2. Model overhead. The 500 MB model (GGUF format) must be available at both endpoints. This overhead is amortized over many files or large corpora, following the same convention as all LLMbased compressors (including Delétang et al. [6]). 3. Training data contamination. Alice in Wonderland, Shakespeare texts, and Wikipedia (enwik8) are almost certainly in SmolLM2s training data, potentially inflating compression ratios. The OOD evaluation (Section 5.4) on post-trainingcutoff document suggests the effect is limited Nacrith achieves 0.723 bpb on unseen textbut broader OOD evaluation across diverse domains would strengthen this conclusion. 4. Context window. Dependencies beyond 2,048 tokens are lost. Very long documents may compress less efficiently near window boundaries. 5. Language specificity. SmolLM2-135M is primarily trained on English text. Other languages, especially low-resource ones, will compress less effectively."
        },
        {
            "title": "6.5 Future Work",
            "content": "1. Larger context models. SmolLM2-360M or SmolLM2-1.7B with 8K32K context windows could improve both compression and the effectiveness of the N-gram model over longer ranges. 2. Quantization. INT8 or INT4 quantization (as in ts_zips 8-bit RWKV) would reduce VRAM and increase throughput with minimal probability-quality degradation. 3. ANS coding. Replacing arithmetic coding with Asymmetric Numeral Systems [7] would improve encoding speed. 7. Conclusion We have presented Nacrith, practical lossless compression system combining SmolLM2-135M with an ensemble of lightweight online predictors and highprecision arithmetic coding. Our results demonstrate three key findings: First, precision matters in large-vocabulary arithmetic coding. Upgrading from CDF-16 to CDF-24 eliminates 75% waste of CDF range caused by minimumprobability floors in the 49,152-token vocabularya previously uncharacterized bottleneck in LLM-based compression. Second, small pre-trained models with ensemble context mixing achieve strong compression in our experiments: 0.918 bpb on alice29.txt (11.5%), outperforming CMIX v21 (1.63 bpb) by 44% and ts_zip (1.14 bpb) by 20%; and 0.9389 bpb on enwik8 (100 MB, 11.74%), outperforming FineZip (1.024 bpb) by 8% with 60 smaller model and no fine-tuning. On modern English prose, the system reaches 0.630.76 bpb (7.99.5%). Crucially, an out-of-distribution evaluation on document published after SmolLM2s training cutoff confirms that these gains are not artifacts of memorization: Nacrith achieves 0.723 bpb on unseen text, outperforming FineZip (same model) by 26%. All results use only 135M parameters and 500 MB of GGUF weights. Third, the hybrid NC06 formatto our knowledge, the first LLM-based binary compressorextends neural compression to arbitrary binary files, broadening the applicability of this approach beyond the pure-text domain of all prior work. These results confirm that the compression prediction equivalence identified by Shannon [20] and formalized for LLMs by Delétang et al. [6] is practically accessible today on consumer hardware, with results well below classical Shannon entropy bounds. Code availability. Nacrith is open-source and available at https://github.com/robtacconelli/ Nacrith-GPU."
        },
        {
            "title": "References",
            "content": "[1] Arnold, R. and Bell, T. (1997). corpus for the evaluation of lossless compression algorithms. In 9 [16] Mahoney, M. (2005). Adaptive weighing of context models for lossless data compression. Technical Report, Florida Institute of Technology. [17] Mahoney, M. (2023). Large text compression benchmark. http://mattmahoney.net/dc/text. html. [18] Ministry of Housing, Communities and Local Government (2025). English Indices of Deprivation 2025: Technical Report plain text extraction for compression benchmarking. doi:10.5281/zenodo.18732365. [19] Schmidhuber, J. and Heil, S. (1996). Sequential neural text compression. IEEE Transactions on Neural Networks, 7(1):142146. [20] Shannon, C. E. (1948). mathematical theory of communication. Bell System Technical Journal, 27(3):379423. [21] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. (2017). Attention is all you need. In Advances in Neural Information Processing Systems (NeurIPS 2017). [22] Valmeekam, C.S.K., Narayanan, K., Kalathil, D., Chamberland, J.-F., and Shakkottai, S. LLMZip: Lossless text compression (2023). using large language models. arXiv preprint, arXiv:2306.04050. [23] Vovk, V. G. (1990). Aggregating strategies. In Proceedings of the Third Annual Workshop on Computational Learning Theory (COLT 1990), pp. 371383. [24] Witten, I. H., Neal, R. M., and Cleary, J. G. (1987). Arithmetic coding for data compression. Communications of the ACM, 30(6):520540. [25] Ziv, J. and Lempel, A. (1977). universal algorithm for sequential data compression. IEEE Transactions on Information Theory, 23(3):337 343. Data Compression Conference (DCC 97), pp. 201 210. [2] Bellard, F. (2021). NNCP: Lossless data compression with neural networks. https://bellard. org/nncp/. [3] Bellard, F. (2023). ts_zip: Text compression using large language models. https://bellard. org/ts_zip/. [4] Ben Allal, L., Lozhkov, A., Bakouch, E., et al. (2025). SmolLM2A family of small language models. Hugging Face. https://huggingface. co/HuggingFaceTB/SmolLM2-135M. [5] Cleary, J. G. and Witten, I. H. (1984). Data compression using adaptive coding and partial string matching. IEEE Transactions on Communications, 32(4):396402. [6] Delétang, G., Ruoss, A., Duquenne, P.-A., Catt, E., Genewein, T., Mattern, C., Grau-Moya, J., Wenliang, L. K., Aitchison, M., Orseau, L., Hutter, M., and Veness, J. (2024). Language modeling is compression. In Proceedings of the International Conference on Learning Representations (ICLR 2024). arXiv:2309.10668. [7] Duda, J. (2009). Asymmetric numeral systems. arXiv preprint, arXiv:0902.0271. [8] G7, S. (2024). LLM-Text-Compressor. https:// github.com/SatvikG7/LLM-Text-Compressor. [9] Gerganov, G. (2023). llama.cpp: LLM inference in C/C++. https://github.com/ggerganov/ llama.cpp. [10] Goyal, M., Tatwawadi, K., Chandak, S., and Ochoa, I. (2020). DeepZip: Improved generalpurpose lossless compression based on novel neural network modeling. In Data Compression Conference (DCC) 2020. [11] Mittu, F., Bu, Y., Gupta, A., Devireddy, A., Ozdarendeli, A.E., Singh, A., and Anumanchipalli, G. (2024). FineZip: Pushing the limits of large language models for practical lossless text compression. arXiv preprint, arXiv:2409.17141. [12] Dréano, S., Molloy, D., and Murphy, N. (2025). Llamazip: Leveraging LLaMA for lossless text compression and training dataset detection. arXiv preprint, arXiv:2511.17589. [13] Huffman, D. A. (1952). method for the construction of minimum-redundancy codes. Proceedings of the IRE, 40(9):10981101. [14] Knoll, B. (2024). CMIX: lossless data compreshttp://www.byronknoll.com/ sion program. cmix.html. [15] Knoll, B. and de Freitas, N. (2011). machine learning perspective on predictive coding with PAQ. arXiv preprint, arXiv:1108.3298."
        }
    ],
    "affiliations": [
        "Independent Researcher"
    ]
}