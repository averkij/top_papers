{
    "paper_title": "Mutarjim: Advancing Bidirectional Arabic-English Translation with a Small Language Model",
    "authors": [
        "Khalil Hennara",
        "Muhammad Hreden",
        "Mohamed Motaism Hamed",
        "Zeina Aldallal",
        "Sara Chrouf",
        "Safwan AlModhayan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce Mutarjim, a compact yet powerful language model for bidirectional Arabic-English translation. While large-scale LLMs have shown impressive progress in natural language processing tasks, including machine translation, smaller models. Leveraging this insight, we developed Mutarjim based on Kuwain-1.5B , a language model tailored for both Arabic and English. Despite its modest size, Mutarjim outperforms much larger models on several established benchmarks, achieved through an optimized two-phase training approach and a carefully curated, high-quality training corpus.. Experimental results show that Mutarjim rivals models up to 20 times larger while significantly reducing computational costs and training requirements. We also introduce Tarjama-25, a new benchmark designed to overcome limitations in existing Arabic-English benchmarking datasets, such as domain narrowness, short sentence lengths, and English-source bias. Tarjama-25 comprises 5,000 expert-reviewed sentence pairs and spans a wide range of domains, offering a more comprehensive and balanced evaluation framework. Notably, Mutarjim achieves state-of-the-art performance on the English-to-Arabic task in Tarjama-25, surpassing even significantly larger and proprietary models like GPT-4o mini. We publicly release Tarjama-25 to support future research and advance the evaluation of Arabic-English translation systems."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 2 ] . [ 1 4 9 8 7 1 . 5 0 5 2 : r Mutarjim: Advancing Bidirectional Arabic-English Translation with Small Language Model Khalil Hennara, Muhammad Hreden, Mohamed Motaism Hamed, Zeina Aldallal, Sara Chrouf, and Safwan AlModhayan Khobar, Saudi Arabia hennara,hreden,hamed,aldallal,chrouf,safwan@misraj.ai"
        },
        {
            "title": "Abstract",
            "content": "We introduce Mutarjim, compact yet powerful language model for bidirectional Arabic-English translation. While large-scale LLMs have shown impressive progress in natural language processing tasks, including machine translation, smaller models. Leveraging this insight, we developed Mutarjim based on Kuwain-1.5B Hennara et al. [2025], language model tailored for both Arabic and English. Despite its modest size, Mutarjim outperforms much larger models on several established benchmarks, achieved through an optimized two-phase training approach and carefully curated, high-quality training corpus.. Experimental results show that Mutarjim rivals models up to 20 times larger while significantly reducing computational costs and training requirements. We also introduce Tarjama-25, new benchmark designed to overcome limitations in existing Arabic-English benchmarking datasets, such as domain narrowness, short sentence lengths, and English-source bias. Tarjama-25 comprises 5,000 expert-reviewed sentence pairs and spans wide range of domains, offering more comprehensive and balanced evaluation framework. Notably, Mutarjim achieves state-of-the-art performance on the English-to-Arabic task in Tarjama-25, surpassing even significantly larger and proprietary models like GPT-4o mini. We publicly release Tarjama-25 to support future research and advance the evaluation of Arabic-English translation systems."
        },
        {
            "title": "Introduction",
            "content": "Machine translation (MT), core task in natural language processing (NLP), has made great progress with the rise of Large Language Models (LLMs). However, Arabic machine translation (AMT) is considered big challenge due to many factors and characteristics of the Arabic language, such as grammar and morphology complexity. Lexical, syntactic, and semantic problems arise when translating the meaning of Arabic words into English and vice versa Baligh & Mohammed [2022]. Despite recent advances in NLP, the Arabic language still lags behind other high-resource languages in terms of translation quality. Existing Arabic-English systems are either limited in their capabilities or are part of larger multilingual models that, while capable of handling many languages, often underperform on Arabic-specific tasks. These models are also computationally demanding, limiting their practicality in low-resource or real-time settings. Consequently, there is increasing interest in developing smaller, task-specific models that balance performance with efficiency while effectively modeling Arabics linguistic complexity. 1 ChrF++ scores COMET scores Figure 1: Performance of various models on Tarjama-25 sorted by model size, our newly introduced benchmark for Arabic-English translation, evaluated using two metrics: ChrF++ (left) and COMET (right). In this paper, we introduce Mutarjim, task-specific small language model optimized for ArabicEnglish translation. Mutarjim is built on Kuwain-1.5B Hennara et al. [2025], an Arabic-centric decoder-only model. Mutarjim is trained in two stages: translation-oriented large-scale pretraining phase and targeted fine-tuning stage using high-quality parallel corpora. This tailored training approach enables Mutarjim to deliver competitive translation quality and faster inference times. In benchmark evaluations, Mutarjim outperforms models with more than 30 billion parameters, including proprietary systems like GPT-4o mini in both accuracy and efficiency, as shown in Figure 1. To facilitate robust evaluation and future research, we also present Tarjama-25, new benchmark dataset for bidirectional Arabic-English translation. Tarjama-25 addresses key limitations of existing datasets, such as short sentence length, English source bias, and limited domain diversity. It comprises 5,000 pairs of expert-curated sentences from diverse domains, with an equal number of examples sourced from Arabic and English originals. All pairs are used to evaluate both Arabicto-English and English-to-Arabic translation, providing comprehensive and realistic benchmark for bidirectional translation performance. Our contributions can be summarized as follows: We introduce Mutarjim, compact but powerful decoder-only model specifically optimized for Arabic-English translation. We present Tarjama-25, new benchmark for Arabic-English translation, featuring: Longer and more natural sentence structures; Balanced translation directionality, with an equal number of source texts originally written in Arabic and English; Broad domain coverage, spanning general, medical, legal, technological, and other fields; Careful curation to eliminate contamination from large-scale pre-training corpora, ensuring fair and unbiased evaluation. Expert-reviewed and human-corrected translations to ensure high linguistic quality and fidelity. We perform extensive evaluations using multiple standard benchmarks, including WMT24++ Deutsch et al. [2025], IWSLT2017 Cettolo et al. [2017], and our newly introduced Tarjama2 25, and compare Mutarjim against range of open-source and proprietary models, using automatic metrics BLEU, chrF++, and COMET Rei et al. [2020]. We publicly release both the Tarjama-25 benchmark and its accompanying evaluation toolkit as open-source resources to promote transparency, reproducibility, and further progress in Arabic machine translation research. The rest of the paper is structured as follows: Section 2 reviews the related works and theoretical foundations relevant to our study. Section 3 describes the dataset creation process in the two phases. Section 4 outlines our benchmarking setup. Section 5 introduces Mutarjim model with the training methodology. Section 6 details our experiments and results. Section 7 explains our evaluation strategy. Finally, Section 8 concludes the paper and suggests directions for future research."
        },
        {
            "title": "2 Background",
            "content": "Machine translation has undergone significant evolution, progressing from rule-based approaches necessitating extensive development and maintenance toward statistical and neural paradigms. The development of encoder-decoder and, more recently, decoder-only architectures has played pivotal role in advancing the field. In this section, we review the major contributions that form the foundation of modern neural MT, focusing on models relevant to Arabic and multilingual translation."
        },
        {
            "title": "2.1 Encoder-Decoder Models",
            "content": "Encoder-decoder Transformer models form the basis of modern neural machine translation (NMT) systems, evolving from general-purpose architectures to specialized, multilingual frameworks. These models vary in scale, language coverage, and specialization, balancing broad applicability with performance in specific languages such as Arabic. This section reviews key models, highlighting their contributions and limitations, particularly in the context of multilingual and Arabic-focused translation. The Text-to-Text Transfer Transformer (T5) Raffel et al. [2023] introduced unified text-to-text framework, trained on the English-only C4 dataset with model sizes ranging from 60M to 11B parameters. Although not designed for translation, the flexible architecture of T5 laid the groundwork for subsequent multilingual models. Its primary limitation, the lack of multilingual support, restricts its direct applicability to NMT tasks. Based on T5, mT5 Xue et al. [2021] extends the text-to-text framework to 101 languages using the multilingual mC4 dataset. However, Arabic constitutes only 1.66% of its pre-training data, constraining its effectiveness for Arabic translation. Despite strong zero-shot transfer capabilities, mT5 struggles with language-specific nuances, prompting further specialization in later models. Similarly, Aya-101 Üstün et al. [2024] adapts the mT5-XXL model (13B parameters) using instruction tuning in 100+ languages. This broad multilingual tuning improves the models ability to translate languages with limited training data, known as low-resource languages. However, its large size results in high training and inference costs, making it less practical for deployment compared to smaller, more specialized models. In particular, mBART Liu et al. [2020] is 680M parameter model trained using multilingual denoising pre-training and includes 2.87 https://huggingface.co/datasets/Misraj/Tarjama-25 https://github.com/misraj-ai/Mutarjim-evaluation 3 billion Arabic tokens in its corpus. To address broader language coverage, NLLB-200 Costa-Jussà et al. [2022a] is 3.3B parameter dense model trained on over 18 billion sentence pairs spanning 200 languages. It achieves strong translation performance, particularly in low-resource settings, and is known for its relative robustness against hallucinations. However, its effectiveness diminishes in domain-specific texts such as Islamic or medical content, where it struggles to maintain accuracy and relevance. In response to the need for Arabic-specific solutions, TURJUMAN Nagoudi et al. [2022], built directly on AraT5 Nagoudi et al. [2021], provides toolkit for translating 20 source languages into MSA."
        },
        {
            "title": "2.2 Decoder-Only Models",
            "content": "The recent shift toward decoder-only language models has reshaped the machine translation landscape, particularly through their use in autoregressive generation. Unlike traditional encoderdecoder architectures, decoder-only models handle both the source and target text within single sequence and rely on large-scale pre-training. major advantage of decoder-only models lies in their unified architecture for both understanding and generation tasks, enabling efficient transfer and scalability. Prompt-based translation using large models such as GPT-4 Achiam et al. [2023] has shown promising results, especially when effective prompting strategies are applied He et al. [2024]; Lu et al. [2023]; Xi et al. [2022]; Zhu et al. [2023]; Agrawal et al. [2022]; Vilar et al. [2022]. These approaches often involve feeding few translation examples directly into the prompt, allowing the model to generalize with minimal supervision. Beyond prompting, compact decoder-only models have gained traction due to their efficiency. For example, BigTranslate Yang et al. [2023] and PARADIGM Xu et al. [2023] explore fine-tuning small open-source LLMs using parallel corpora, achieving competitive translation performance at fraction of computational cost. Models like Tower Alves et al. [2024] and GemmaX2-28 Cui et al. [2025] employ two-phase training: multilingual pre-training followed by fine-tuning conducted according to instructions, enabling domain-specific translation capabilities. Multilingual decoder-only models, such as XALMA Xu et al. [2024], scale this approach further, incorporating 50 languages and using MoE layers and adapter modules to manage multilingual transfer. Within the Arabic domain, general purpose decoder-only models, such as Allam Bari et al. [2024], AceGPT Huang et al. [2023], and Silma Team [2024], are trained primarily in Arabic-centric corpora with objectives to cover various downstream tasks. However, these models are typically not specialized in translation and often lack the fine-grained bilingual alignment necessary for high-quality Arabic-English translation. Specialized Arabic decoder-only translation models have also emerged. One such model is Lahjawi Hamed et al. [2025], cross-dialect translation system that demonstrated strong performance in both MSA and dialectal Arabic. Lahjawi is specifically undergoing targeted fine-tuning for cross-dialect translation. Given the trade-off between translation quality, inference speed, and resource efficiency, in this work, we adopt the decoder-only paradigm. Mutarjim follows two-stage training strategy: largescale monolingual pre-training followed by supervised fine-tuning using high-quality Arabic-English parallel data. Our choice reflects growing shift toward compact, efficient, and Arabic-optimized decoder-only systems for machine translation."
        },
        {
            "title": "3 Data",
            "content": "Our training data consists exclusively of bilingual Arabic-English corpora, combining proprietary and open-source sources. For pre-training, we utilize large-scale, domain-diverse parallel datasets to expose the model to wide range of linguistic patterns and translation contexts. Fine-tuning focuses on higher-quality data, combining carefully filtered open-source corpora with proprietary datasets curated for accuracy, fluency, and domain relevance. This composition ensures broad coverage during pre-training while emphasizing translation accuracy, fluency, and domain specificity during fine-tuning."
        },
        {
            "title": "3.1 Pre-training Data",
            "content": "Our pre-training corpus comprises approximately 10 billion tokens of bilingual Arabic-English data, used to continue pre-training kuwain-1.5 the base model and improve its performance in translation tasks. Data are sourced from the OPUS platform Tiedemann [2016], where we exclusively select Arabic-English parallel corpora, supplemented with proprietary datasets curated internally to improve domain diversity and coverage. To improve data quality, we applied set of pre-processing steps, including the removal of sentence pairs with fewer than three tokens, as such samples often lack meaningful context. We also filtered out misaligned examples in which the target sentence is not in the correct language. Pairs with substantial length mismatches are excluded to reduce the risk of partial or noisy translations. Finally, we performed deduplication to eliminate repeated sentence pairs and reduce redundancy in the training data. These filtering steps help maintain reasonably clean and consistent corpus for pre-training."
        },
        {
            "title": "3.2 Fine-tuning Data",
            "content": "The fine-tuning procedure utilized precisely selected corpus of approximately 6 million ArabicEnglish parallel sentence pairs. This corpus exhibits substantial diversity and underwent careful filtering procedures to ensure high translation fidelity. The dataset was derived from two principal sources: One portion of the data incorporates translations originally produced by state-of-the-art LLM. Subsequently, expert linguists inspected representative subset of these outputs to confirm their accuracy and fluency. To promote stronger Arabic fluency in the model, we emphasized Arabic-centric samples, where Arabic is the source language, at 2:1 ratio compared to English-centric samples. This approach not only enhances the models ability to understand and generate Arabic text but also helps preserve the cultural and linguistic richness of the language. The remaining portion comprises high-quality filtered data from OPUS. We applied combination of automatic and manual review processes, including human inspection of representative subsets. Datasets exhibiting recurring issues such as out-of-context sentences, hallucinations, misinformation, or poor alignment were excluded to maintain overall data integrity. The fine-tuning dataset was designed to align with the domain categories introduced in our bench5 mark (Section 4), ensuring broad and realistic coverage across cultural, legal, scientific, healthcare, religious, and technical domains. We prioritized the inclusion of authentic Arabic source material and maintained balanced representation across domains to mitigate distributional bias. This targeted curation enables the model to generalize effectively across diverse topics while preserving high translation quality."
        },
        {
            "title": "4.1 Motivation and Development",
            "content": "Modern MT systems face persistent evaluation challenges due to the limitations of existing benchmarks. To address these gaps, we introduce Tarjama-25, comprehensive benchmark specifically designed for both ArabictoEnglish and EnglishtoArabic translation tasks. The current landscape of translation evaluation reveals several critical shortcomings: most publicly available datasets are English-centric (i.e., English is the source language), lacking authentic bidirectional content; benchmarks tend to contain predominantly short sentences (typically 630 words), which underutilizes the capacity of modern language models designed to process substantially longer input sequences; and domain-specific coverage remains limited. Furthermore, potential data contamination from web-scale pre-training and insufficient representation of language-specific characteristics, particularly for Arabic texts, pose additional challenges. To address these challenges, we developed Tarjama-25 through comprehensive data collection and validation pipeline: We began by collecting 30,000 sentences from authentic Arabic and English sources, each ranging from 50 to 100 words long, ensuring broad domain coverage across scientific, technical, healthcare, cultural, and general interest topics. Half of the data was originally written in Arabic, and the other half in English. The 30,000 sentences were initially translated using state-of-the-art machine translation systems to create parallel sentence pairs. From these, 5,000 pairs of sentences were selected for detailed human refinement. Professional translators reviewed and corrected each selected pair to ensure linguistic accuracy and fluency. The final selection maintains balanced distribution in all domains (Figure 2). Finally, domain experts conducted an additional review to validate the accuracy and contextual relevance of the translations within their respective fields. This careful multi-stage process ensures high-quality, human-validated translations with balanced source language distribution and rich domain diversity, making Tarjama-25 robust and realistic benchmark for bidirectional Arabic-English translation evaluation."
        },
        {
            "title": "4.2 Findings and Recommendations",
            "content": "Tarjama-25 distinguishes itself through authentic source content in both languages, diverse text lengths, extensive domain coverage, and strong focus on language-specific subtleties. Our preliminary evaluations reveal that many current MT models, despite their strong performance on existing 6 Figure 2: Domain Coverage in Tarjama-25 Benchmark benchmarks, face significant challenges with Tarjama-25. Detailed evaluation results are presented in Section 7. Based on our findings, we recommend: 1. Development of language-specific authentic benchmarks; 2. Greater emphasis on domain-specific translation capability; 3. Integration of cultural and linguistic nuances in evaluation metrics; 4. Regular benchmark updates to reflect evolving language use."
        },
        {
            "title": "5 Method",
            "content": "For Mutarjim, we build on Kuwain-1.5B Hennara et al. [2025], decoder-only bilingual ArabicEnglish small language model designed for efficiency in resource-constrained environments. Our approach adopts standard LLM training methodologies commonly used in the field. These methodologies comprise two main phases: pre-training and fine-tuning. To improve translation performance, we introduce targeted modifications within this framework. The pre-training phase is designed to develop robust bilingual representation, foundation for the subsequent fine-tuning stage focused specifically on translation tasks."
        },
        {
            "title": "5.1 Pre-training Phase",
            "content": "Following the successful approaches of recent works such as GemmaX Cui et al. [2025] and Tower Alves et al. [2024] in continuing pre-training for translation tasks, we further pre-trained our model on English-Arabic parallel data using next-token prediction objective. To facilitate the learning process, we introduce two special tokens to our model: <English> and <Arabic>. We formatted the data as shown on the left side of Figure 3, where English sentences begin with the token <English> and Arabic sentences with <Arabic>. All pre-training data 7 Figure 3: Illustration of the two data formats used in Mutarjim: (Left) pre-training stream data format; (Right) fine-tuning data sample. consist of paired Arabic-English sentences structured according to this format. During training, the model sees both sentences and is trained to predict the next token over the entire input. To prevent unidirectional translation bias, we randomly select the order of the sentences in each pair. This encourages the model to develop robust bidirectional translation capabilities without favoring specific source language."
        },
        {
            "title": "5.2 Fine-tuning Phase",
            "content": "The fine-tuning phase follows the same format as pre-training, adding newline between the two sentences for improved structural clarity, as illustrated on the right side of Figure 3. However, unlike the pre-training stage, we apply causal masking to the input sentence so that the model is only trained on generating the target sentence from the source, while still using the same next-token prediction objective. We exclusively use high-quality, human-curated parallel data for this phase to ensure translation accuracy. The model is trained for two epochs over total of 3 billion tokens, balancing sufficient exposure to high-quality examples with the need to avoid overfitting. We carefully monitor both training phases to maintain translation quality and prevent performance degradation. Detailed training specifications, including learning rates, batch sizes, and other hyperparameters, are provided in the appendix B."
        },
        {
            "title": "6 Experiment and Results",
            "content": "To thoroughly evaluate the effectiveness of Mutarjim, we conducted series of experiments aimed at gaining deeper insights into the challenges and dynamics of Arabic-English translation. Our evaluation focuses on three core aspects. First, we compare unidirectional and bidirectional training setups to assess whether single model trained in both directions (ArabictoEnglish and EnglishtoArabic) compromises performance relative to dedicated unidirectional models. Second, we examine the contribution of the continued pre-training phase in enhancing translation quality and improving the models generalization across domains. Third, we analyze the effect of context length during fine-tuning to understand how sentence length influences performance, particularly when the evaluation samples differ in length from those seen during training. These experiments are conducted using the WMT24++ benchmark, providing consistent and challenging evaluation framework."
        },
        {
            "title": "6.1 Unidirectional vs. Bidirectional Translation Performance",
            "content": "To assess the impact of directional training on translation quality, we compared unidirectional and bidirectional versions of Mutarjim, focusing on how specializing in single translation direction affects performance relative to multitask setup. We investigated the performance trade-offs between unidirectional modelsMutarjim-AR2EN (ArabictoEnglish) and Mutarjim-EN2AR (EnglishtoArabic)and the bidirectional model Mutarjim-Bi. The unidirectional variants were each trained for 3 epochs, while the bidirectional variant was trained for 2 epochs on the combined data. Table 1 presents evaluation results using COMET Rei et al. [2020] and chrF++ metrics on the WMT24++ benchmark Deutsch et al. [2025]. Despite being exposed to more diverse data, the bidirectional model showed slight decrease in performance. Unidirectional models consistently outperformed the bidirectional model, with Mutarjim-AR2EN achieving COMET score 3.16 points higher than Mutarjim-Bi for Arabic-to-English translation. Ultimately, the choice of model depends on application needs: Mutarjim-Bi offers greater efficiency and flexibility through multitask support, while the unidirectional variants deliver higher translation accuracy for specific directions. Given the compact size of our model (1.5B parameters), the computational cost difference between approaches remains modest. Model Training Arabic English English Arabic COMET chrF++ COMET chrF++ Mutarjim-Bi Bidirectional Mutarjim-AR2EN Unidirectional Mutarjim-EN2AR Unidirectional 79.73 82.89 50.27 54.89 72.86 75. 47.04 48.04 Table 1: Performance comparison between bidirectional (Mutarjim-Bi) and unidirectional (Mutarjim-EN2AR or AREN) translation models on WMT24++."
        },
        {
            "title": "6.2 The Impact of Continued Pre-training Phase",
            "content": "We evaluated the impact of continued pre-training on translation performance, aiming to determine whether translation-specific pre-training could yield meaningful gains over direct fine-tuning. Although our base model, Kuwain, was initially trained on substantial corpus, making it viable candidate for direct fine-tuning, we explore whether targeted continued pre-training on bilingual data enhances downstream translation quality, following recent successes in domain-adaptive pretraining Cui et al. [2025]; Alves et al. [2024]. Table 2 presents comparison between models trained with and without the additional translationfocused pre-training phase. Models benefiting from this additional phase consistently outperform their counterparts trained solely through fine-tuning, as reflected in both COMET and chrF++ scores. The gains are evident in both the ArabictoEnglish and EnglishtoArabic directions, underscoring the general effectiveness of this strategy in translation tasks. While this approach may not be cost-effective for larger models, it remains computationally feasible for smaller architectures like our 1.5B parameter models. 9 Model English Arabic Arabic English COMET chrF++ COMET chrF++ Without Additional Pre-training Mutarjim-AR2EN Mutarjim-EN2AR 74.30 42.17 61.91 With Additional Pre-training Mutarjim-AR2EN Mutarjim-EN2AR 82.89 54.89 75.46 34. 48.04 Table 2: Effect of additional translation-specific pre-training on model performance, evaluated on WMT24++ test set."
        },
        {
            "title": "6.3 Context Length Effect",
            "content": "We conducted two independent fine-tuning experiments to evaluate the impact of input length distributions on translation performance. In the first experiment (e1), we fine-tuned the pre-trained Mutarjim model using samples containing more than 30 words, aiming to improve the models performance on longer sentences. While this enhanced fluency on long-form content, we observed performance degradation on shorter inputs, with increased hallucinations and irrelevant continuations. To address this, we performed second, separate fine-tuning experiment (e2) using the same base model but modifying the training set to include an additional 15% of short samples (ranging from 2 to 30 words). This experiment sought to balance the models ability across varying sequence lengths. We evaluated both versions on the WMT24++ test set. As shown in Table 3, the second experiment (e2) led to improved performance in both directions of translation, confirming the benefit of including shorter sequences in the training data. Model Arabic English English Arabic COMET chrF++ COMET chrF++ Experiment 1 (Long Inputs Only) Mutarjim-AR2EN-e1 Mutarjim-EN2AR-e 73.62 48.57 69.07 Experiment 2 (Mixed Length Inputs) Mutarjim-AR2EN-e2 Mutarjim-EN2AR-e2 74. 50.84 73.56 43.40 46. Table 3: Evaluation of models fine-tuned with different input length distributions on the WMT24++ test set."
        },
        {
            "title": "7 Evaluation",
            "content": "To contextualize the performance of Mutarjim, we compare it against diverse set of strong decoder-only models that support Arabic and are widely recognized for their translation capabilities. These include general-purpose language models such as AceGPT-8BHuang et al. [2023], ALLam10 7BBari et al. [2024], C4AI-7BCohere For AI [2024], Cohere-8B Aryabumi et al. [2024], Cohere-32B Aryabumi et al. [2024], Gemma2-27B Team et al. [2024], Silma-9B Team [2024], and Yehia-7B Navid-AI [2025]. Furthermore, we include multilingual translation-specialized models such as XALMA-13B-Group8 Xu et al. [2024], LLaMAX3-8B-Alpaca Lu et al. [2024], and GemmaX-9B Cui et al. [2025]. To provide closer baseline in terms of the architecture and size of the model, we also evaluate against NLLB-3.3B Team et al. [2022], an encoderdecoder model known for its effectiveness in low-resource translation tasks and its widespread adoption in Arabic-English translation. We evaluated the performance of our model compared to range of strong baseline models across three established benchmarks: WMT24++, IWSLT2017, and our newly proposed benchmark Tarjama-25. For all benchmarks, we evaluated translation quality using widely adopted metrics, BLEU, chrF++, and COMET, to ensure comprehensive and fair assessment. The results for each benchmark are reported in their respective tables: Tarjama-25 in Table 4, WMT24++ in Table 5, and IWSLT2017 in Table 6. For consistency, all models are listed in the tables in order of model size. To ensure fair comparison, we employ model-specific prompts during evaluation, as illustrated in Appendix D. To streamline the evaluation pipeline and accelerate inference, we utilize VLLM Kwon et al. [2023], which enables efficient batched decoding across decoder-only models. Although being the smallest among the evaluated models, Mutarjim achieves state-of-the-art performance on the Tarjama-25 benchmark for the Arabic-to-English direction in all evaluation metrics, and leads in the EnglishtoArabic direction when measured by the BLEU score. It closely trails the much larger GPT-4o-mini model in COMET and chrF++ with only narrow margin. These results highlight Mutarjims competitive effectiveness despite its compact size, demonstrating its strength in both translation quality and efficiency. Model performance varies noticeably on Tarjama-25 compared to existing benchmarks. For example, while GPT-4o-mini excels on WMT24++ and IWSLT2017, its relative performance declines on Tarjama-25. This highlights how standard benchmarks may overlook challenges in domainspecific and bidirectional translation. Tarjama-25 helps expose these gaps, offering more realistic and rigorous assessment of real-world translation capabilities. Another key observation is the consistent performance gap observed in most models between Arabicto-English and English-to-Arabic translation, with the former generally yielding better results. This trend is visually illustrated in Figure 1, where the disparity, particularly in the chrF++ metric, is pronounced. Several factors may contribute to this asymmetry, including Arabics rich morphology and syntactic flexibility, which allow for multiple valid translations that current metrics may fail to recognize. Furthermore, the predominance of English-centric training data in many models may hinder their ability to generate fluent and accurate Arabic output. Notably, Mutarjim demonstrates balanced performance in both translation directions, which we attribute to its Arabic-centric training strategy. This indicates that training with authentic Arabic source data can help mitigate directional bias and improve overall translation fidelity. https://docs.vllm.ai/en/stable/ 11 Model Mutarjim NLLBCosta-Jussà et al. [2022b] c4ai Cohere For AI [2024] Yehia Navid-AI [2025] ALLamBari et al. [2024] Cohere Aryabumi et al. [2024] AceGPT Huang et al. [2023] LLaMAX3 Lu et al. [2024] SILMA Team [2024] GemmaX Cui et al. [2025] XALMA Xu et al. [2024] Gemma 2 Team et al. [2024] Cohere Aryabumi et al. [2024] Size 1.5B 3.3B 7B 7B 7B 8B 8B 8B 9B 9B 13B 27B 32B GPT-4o mini Hurst et al. [2024] - Arabic English English Arabic COMET Chrf++ Bleu COMET Chrf++ Bleu 82.63 67. 80.93 73.31 72.90 81.20 80.71 77. 64.36 69.63 73.37 80.81 82.44 83. 74.66 40.50 67.24 56.77 56.88 67. 65.63 54.95 37.84 43.42 46.96 70. 73.10 76.08 55.28 24.38 43.34 32. 31.01 42.72 38.67 27.86 15.67 19. 21.57 42.78 51.16 54.24 83.41 81. 79.10 74.97 75.41 82.50 78.39 56. 58.01 66.94 66.36 42.20 82.09 83. 68.67 59.69 55.96 50.32 51.24 58. 50.67 33.25 27.71 37.66 29.88 3. 63.29 66.36 43.71 30.32 25.18 20. 20.54 26.26 20.02 7.63 5.62 9. 6.64 3.08 32.25 38.52 Table 4: Performance comparison of bidirectional (Arabic-English) translation models on the Tarjama-25 benchmark in terms of COMET, Chrf++, and Bleu. Model Mutarjim NLLBCosta-Jussà et al. [2022b] c4ai Cohere For AI [2024] Yehia Navid-AI [2025] ALLamBari et al. [2024] Cohere Aryabumi et al. [2024] AceGPT Huang et al. [2023] LLaMAX3 Lu et al. [2024] SILMA Team [2024] GemmaX Cui et al. [2025] XALMA Xu et al. [2024] Gemma 2 Team et al. [2024] Cohere Aryabumi et al. [2024] Size 1.5B 3.3B 7B 7B 7B 8B 8B 8B 9B 9B 13B 27B 32B GPT-4o mini Hurst et al. [2024] - Arabic English English Arabic COMET Chrf++ Bleu COMET Chrf++ Bleu 72. 76.71 79.27 72.72 72.00 78.89 78. 75.91 71.33 77.82 76.84 72.79 79. 83.29 52.27 50.13 54.91 47.58 46. 54.06 52.25 48.18 38.96 50.80 48. 51.09 57.05 58.24 19.26 25.50 26. 15.39 15.01 24.96 21.21 18.89 16. 22.67 19.34 16.59 27.98 29.23 75. 77.75 72.45 72.23 71.89 74.80 73. 57.31 60.54 70.21 69.19 54.00 72. 82.32 48.04 45.89 44.32 41.12 41. 44.95 40.55 28.83 26.75 38.81 33. 32.66 47.13 50.03 17.99 16.03 14. 10.69 10.41 14.08 11.37 4.03 4. 9.83 7.54 4.77 15.84 20.48 Table 5: Performance comparison of bidirectional (Arabic-English) translation models on the WMT24++ benchmark in terms of COMET, Chrf++, and Bleu. 12 Model Mutarjim NLLBCosta-Jussà et al. [2022b] c4ai Cohere For AI [2024] Yehia Navid-AI [2025] ALLamBari et al. [2024] Cohere Aryabumi et al. [2024] AceGPT Huang et al. [2023] LLaMAX3 Lu et al. [2024] SILMA Team [2024] GemmaX Cui et al. [2025] XALMA Xu et al. [2024] Gemma 2 Team et al. [2024] Cohere Aryabumi et al. [2024] Size 1.5B 3.3B 7B 7B 7B 8B 8B 8B 9B 9B 13B 27B 32B GPT-4o mini Hurst et al. [2024] - Arabic English English Arabic COMET Chrf++ Bleu COMET Chrf++ Bleu 82.89 54.89 31.00 79.76 44. 12.74 - 83.99 75.58 75.64 83. 81.72 81.04 78.55 82.06 80.06 - 84.30 86.37 - 56.64 47.38 37. 55.83 52.83 49.17 47.57 53.30 49. - 59.02 60.48 - 33.64 15. 5.89 31.71 26.26 24.28 24.28 30. 24.10 - 35.37 36.86 - 77. 76.22 75.25 79.05 79.62 67.79 69. 76.17 76.41 48.56 74.63 87.14 - 40.50 38.41 46.54 42.36 40.23 30. 30.03 37.17 36.99 22.28 43.53 47. - 9.14 6.65 14.79 9.10 9. 4.18 5.11 7.10 7.13 1.57 8. 15.50 Table 6: Performance comparison of bidirectional (Arabic-English) translation models on the IWSLT-2017 benchmark in terms of COMET, Chrf++, and Bleu."
        },
        {
            "title": "8 Conclusion",
            "content": "In this work, we introduce Mutarjim, an efficient and compact small language model, optimized for bidirectional Arabic-English machine translation while providing rich and accurate output. We also present new benchmark Tarjama-25, diverse and representative dataset for bidirectional ArabicEnglish MT evaluation. Our evaluation and experiments demonstrate that Mutarjim achieves competitive performance against larger models while requiring significantly fewer computational resources. The models compact architecture enables deployment in resource-constrained environments without sacrificing translation quality. Future work will focus on scaling up the model architecture and training on larger multilingual datasets to support translation between Arabic and multiple languages, including French, Turkish, and Japanese, to create comprehensive multilingual translation system while maintaining efficiency."
        },
        {
            "title": "References",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Sweta Agrawal, Chunting Zhou, Mike Lewis, Luke Zettlemoyer, and Marjan Ghazvininejad. Incontext examples selection for machine translation. arXiv preprint arXiv:2212.02437, 2022. Duarte Alves, José Pombal, Nuno Guerreiro, Pedro Martins, João Alves, Amin Farajian, Ben Peters, Ricardo Rei, Patrick Fernandes, Sweta Agrawal, et al. Tower: An open multilingual large language model for translation-related tasks. arXiv preprint arXiv:2402.17733, 2024. 13 Viraat Aryabumi, John Dang, Dwarak Talupuru, Saurabh Dash, David Cairuz, Hangyu Lin, Bharat Venkitesh, Madeline Smith, Jon Ander Campos, Yi Chern Tan, et al. Aya 23: Open weight releases to further multilingual progress. arXiv preprint arXiv:2405.15032, 2024. Babaali Baligh and Salem Mohammed. Arabic machine translation: panoramic survey. SSRN Electronic Journal, 01 2022. doi: 10.2139/ssrn.4312742. Saiful Bari, Yazeed Alnumay, Norah Alzahrani, Nouf Alotaibi, Hisham Alyahya, Sultan AlRashed, Faisal Mirza, Shaykhah Alsubaie, Hassan Alahmed, Ghadah Alabduljabbar, et al. Allam: Large language models for arabic and english. arXiv preprint arXiv:2407.15390, 2024. Mauro Cettolo, Marcello Federico, Luisa Bentivogli, Jan Niehues, Sebastian Stüker, Katsuhito Sudoh, Koichiro Yoshino, and Christian Federmann. Overview of the IWSLT 2017 evaluation campaign. In Proceedings of the 14th International Conference on Spoken Language Translation, pp. 214, Tokyo, Japan, December 14-15 2017. International Workshop on Spoken Language Translation. URL https://aclanthology.org/2017.iwslt-1.1. Cohere For AI. c4ai-command-r-07-arabic-2025, 2024. URL https://huggingface.co/CohereFor AI/c4ai-command-r-08-2024. Marta Costa-Jussà, James Cross, Onur Çelebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, et al. No language left behind: Scaling human-centered machine translation. arXiv preprint arXiv:2207.04672, 2022a. Marta Costa-Jussà, James Cross, Onur Çelebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, et al. No language left behind: Scaling human-centered machine translation. arXiv preprint arXiv:2207.04672, 2022b. Menglong Cui, Pengzhi Gao, Wei Liu, Jian Luan, et al. Multilingual machine translation with open large language models at practical scale: An empirical study. arXiv preprint arXiv:2502.02481, 2025. Daniel Deutsch, Eleftheria Briakou, Isaac Caswell, Mara Finkelstein, Rebecca Galor, Juraj Juraska, Geza Kovacs, Alison Lui, Ricardo Rei, Jason Riesa, et al. Wmt24++: Expanding the language coverage of wmt24 to 55 languages & dialects. arXiv preprint arXiv:2502.12404, 2025. Mohamed Motasim Hamed, Muhammad Hreden, Khalil Hennara, Zeina Aldallal, Sara Chrouf, and Safwan AlModhayan. Lahjawi: Arabic cross-dialect translator. In Proceedings of the 4th Workshop on Arabic Corpus Linguistics (WACL-4), pp. 1224, 2025. Zhiwei He, Tian Liang, Wenxiang Jiao, Zhuosheng Zhang, Yujiu Yang, Rui Wang, Zhaopeng Tu, Shuming Shi, and Xing Wang. Exploring human-like translation strategy with large language models. Transactions of the Association for Computational Linguistics, 12:229246, 2024. Khalil Hennara, Sara Chrouf, Mohamed Motaism Hamed, Zeina Aldallal, Omar Hadid, and Safwan AlModhayan. Kuwain 1.5 b: An arabic slm via language injection. arXiv preprint arXiv:2504.15120, 2025. Huang Huang, Fei Yu, Jianqing Zhu, Xuening Sun, Hao Cheng, Dingjie Song, Zhihong Chen, Abdulmohsen Alharthi, Bang An, Juncai He, et al. Acegpt, localizing large language models in arabic. arXiv preprint arXiv:2309.12053, 2023. 14 Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike Lewis, and Luke Zettlemoyer. Multilingual denoising pre-training for neural machine translation. Transactions of the Association for Computational Linguistics, 8:726742, 2020. Hongyuan Lu, Haoran Yang, Haoyang Huang, Dongdong Zhang, Wai Lam, and Furu Wei. Chainof-dictionary prompting elicits translation in large language models. arxiv e-prints, page. arXiv preprint arXiv:2305.06575, 2023. Yinquan Lu, Wenhao Zhu, Lei Li, Yu Qiao, and Fei Yuan. Llamax: Scaling linguistic horizons of llm by enhancing translation capabilities beyond 100 languages. arXiv preprint arXiv:2407.05975, 2024. El Moatez Billah Nagoudi, AbdelRahim Elmadany, and Muhammad Abdul-Mageed. Arat5: Textto-text transformers for arabic language generation. arXiv preprint arXiv:2109.12068, 2021. El Moatez Billah Nagoudi, AbdelRahim Elmadany, and Muhammad Abdul-Mageed. Turjuman: public toolkit for neural arabic machine translation, 2022. URL https://arxiv.org/abs/2206 .03933. Navid-AI. Yehia 7b preview. https://huggingface.co/Navid-AI/Yehia-7B-preview, 2025. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with unified text-to-text transformer, 2023. URL https://arxiv.org/abs/1910.10683. Ricardo Rei, Craig Stewart, Ana Farinha, and Alon Lavie. Comet: neural framework for mt evaluation. arXiv preprint arXiv:2009.09025, 2020. Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Léonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ramé, et al. Gemma 2: Improving open language models at practical size. arXiv preprint arXiv:2408.00118, 2024. NLLB Team, Marta R. Costa-jussà, James Cross, Onur Çelebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, Anna Sun, Skyler Wang, Guillaume Wenzek, Al Youngblood, Bapi Akula, Loic Barrault, Gabriel Mejia Gonzalez, Prangthip Hansanti, John Hoffman, Semarley Jarrett, Kaushik Ram Sadagopan, Dirk Rowe, Shannon Spruit, Chau Tran, Pierre Andrews, Necip Fazil Ayan, Shruti Bhosale, Sergey Edunov, Angela Fan, Cynthia Gao, Vedanuj Goswami, Francisco Guzmán, Philipp Koehn, Alexandre Mourachko, Christophe Ropers, Safiyyah Saleem, Holger Schwenk, and Jeff Wang. No language left behind: Scaling human-centered machine translation, 2022. URL https://arxiv.org/abs/ 2207.04672. Silma Team. Silma. 2024. URL https://www.silma.ai. 15 Jörg Tiedemann. OPUS parallel corpora for everyone. In Proceedings of the 19th Annual Conference of the European Association for Machine Translation: Projects/Products, Riga, Latvia, May 30June 1 2016. Baltic Journal of Modern Computing. URL https://aclanthology.org/2016. eamt-2.8. Ahmet Üstün, Viraat Aryabumi, Zheng-Xin Yong, Wei-Yin Ko, Daniel Dsouza, Gbemileke Onilude, Neel Bhandari, Shivalika Singh, Hui-Lee Ooi, Amr Kayid, et al. Aya model: An instruction finetuned open-access multilingual language model. arXiv preprint arXiv:2402.07827, 2024. David Vilar, Markus Freitag, Colin Cherry, Jiaming Luo, Viresh Ratnakar, and George Foster. Prompting palm for translation: Assessing strategies and performance. arXiv preprint arXiv:2211.09102, 2022. Victoria Lin Xi, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle Ott, Naman Goyal, Shruti Bhosale, Du Jingfei, et al. Few-shot learning with multilingual generative language models. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 90199052. Association for Computational Linguistics Abu Dhabi, United Arab Emirates, 2022. Haoran Xu, Young Jin Kim, Amr Sharaf, and Hany Hassan Awadalla. paradigm shift in machine translation: Boosting translation performance of large language models. arXiv preprint arXiv:2309.11674, 2023. Haoran Xu, Kenton Murray, Philipp Koehn, Hieu Hoang, Akiko Eriguchi, and Huda Khayrallah. Xalma: Plug & play modules and adaptive rejection for quality translation at scale. arXiv preprint arXiv:2410.03115, 2024. Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. mt5: massively multilingual pre-trained text-to-text transformer, 2021. URL https://arxiv.org/abs/2010.11934. Yang, Li, Zhang, and Zong. Bigtranslate: Augmenting large language models with multilingual translation capability over 100 languages. arxiv 2023. arXiv preprint arXiv:2305.18098, 2023. Wenhao Zhu, Hongyi Liu, Qingxiu Dong, Jingjing Xu, Shujian Huang, Lingpeng Kong, Jiajun Chen, and Lei Li. Multilingual machine translation with large language models: Empirical results and analysis. arXiv preprint arXiv:2304.04675, 2023."
        },
        {
            "title": "B Traning Details",
            "content": "We trained Mutarjim using two-stage approach (pre-training and fine-tuning) on 8 NVIDIA H100 GPUs. Table 7 summarizes the key hyperparameters for both phases. Hyperparameter Pre-training Fine-tuning Max Learning Rate Learning Rate Schedule Weight Decay Optimizer Batch Size Training Steps Context Length GPUs 4 1 10 Cosine 0.1 AdamW 1024 4K 2048 8 5 8 10 Cosine 0.01 AdamW 4096 3.3K 512 8 H100 Table 7: Training Hyperparameters"
        },
        {
            "title": "C Mutarjim Translation Examples",
            "content": "Tables 8 and 9 present qualitative examples of Mutarjims performance in both translation directions. These examples cover range of domainsincluding mathematics, structured data, biomedical content, and informal discourseand illustrate the models ability to produce accurate, fluent translations that preserve both meaning and structure. Table 8: Examples of English-to-Arabic Mutarjim Translation. 17 Table 9: Examples of Arabic-to-English Mutarjim Translation."
        },
        {
            "title": "D Evaluation Models Prompts",
            "content": "We use model-specific prompts during the evaluation to ensure fair comparison. Table 10 lists the prompt templates for each model. Considering the source language and the target language. 18 Table 10: Prompts used for each model during the evaluation process. Models like Mutarjim and NLLB are translation-specific systems that dont require prompting, while LLMs require structured prompts with varying degrees of specificity."
        }
    ],
    "affiliations": [
        "Khobar, Saudi Arabia"
    ]
}