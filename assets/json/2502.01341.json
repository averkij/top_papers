{
    "paper_title": "AlignVLM: Bridging Vision and Language Latent Spaces for Multimodal Understanding",
    "authors": [
        "Ahmed Masry",
        "Juan A. Rodriguez",
        "Tianyu Zhang",
        "Suyuchen Wang",
        "Chao Wang",
        "Aarash Feizi",
        "Akshay Kalkunte Suresh",
        "Abhay Puri",
        "Xiangru Jian",
        "Pierre-André Noël",
        "Sathwik Tejaswi Madhusudhan",
        "Marco Pedersoli",
        "Bang Liu",
        "Nicolas Chapados",
        "Yoshua Bengio",
        "Enamul Hoque",
        "Christopher Pal",
        "Issam H. Laradji",
        "David Vazquez",
        "Perouz Taslakian",
        "Spandana Gella",
        "Sai Rajeswar"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Aligning visual features with language embeddings is a key challenge in vision-language models (VLMs). The performance of such models hinges on having a good connector that maps visual features generated by a vision encoder to a shared embedding space with the LLM while preserving semantic similarity. Existing connectors, such as multilayer perceptrons (MLPs), often produce out-of-distribution or noisy inputs, leading to misalignment between the modalities. In this work, we propose a novel vision-text alignment method, AlignVLM, that maps visual features to a weighted average of LLM text embeddings. Our approach leverages the linguistic priors encoded by the LLM to ensure that visual features are mapped to regions of the space that the LLM can effectively interpret. AlignVLM is particularly effective for document understanding tasks, where scanned document images must be accurately mapped to their textual content. Our extensive experiments show that AlignVLM achieves state-of-the-art performance compared to prior alignment methods. We provide further analysis demonstrating improved vision-text feature alignment and robustness to noise."
        },
        {
            "title": "Start",
            "content": "ALIGNVLM: Bridging Vision and Language Latent Spaces for Multimodal Understanding Ahmed Masry 1 2 Juan A. Rodriguez 1 3 4 Tianyu Zhang 1 3 5 Suyuchen Wang 1 3 5 Chao Wang 1 Aarash Feizi 1 3 6 Akshay Kalkunte Suresh 1 Abhay Puri 1 Xiangru Jian 1 7 Pierre-Andre Noel 1 Sathwik Tejaswi Madhusudhan 1 Marco Pedersoli 1 4 Bang Liu 1 5 8 Nicolas Chapados 1 Yoshua Bengio 3 5 8 Enamul Hoque 2 Christopher Pal 1 3 8 9 Issam H. Laradji 1 10 David Vazquez 1 Perouz Taslakian 1 Spandana Gella 1 Sai Rajeswar 1 3 5 2 0 2 3 ] . [ 1 1 4 3 1 0 . 2 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Aligning visual features with language embeddings is key challenge in vision-language models (VLMs). The performance of such models hinges on having good connector that maps visual features generated by vision encoder to shared embedding space with the LLM while preserving semantic similarity. Existing connectors, such as multilayer perceptrons (MLPs), often produce out-of-distribution or noisy inputs, leading to misalignment between the modalities. In this work, we propose novel vision-text alignment method, ALIGNVLM, that maps visual features to weighted average of LLM text embeddings. Our approach leverages the linguistic priors encoded by the LLM to ensure that visual features are mapped to regions of the space that the LLM can effectively interpret. ALIGNVLM is particularly effective for document understanding tasks, where scanned document images must be accurately mapped to their textual content. Our extensive experiments show that ALIGNVLM achieves state-of-the-art performance compared to prior alignment methods. We provide further analysis demonstrating improved vision-text feature alignment and robustness to noise. 1. Introduction Vision-Language Models (VLMs) have gained significant traction in recent years as powerful framework for multimodal document understanding tasks that involve interpret1ServiceNow 2York University 3Mila 4 Ecole de Technologie Superieure 5Universite de Montreal 6McGill University 7University of Waterloo 8CIFAR AI Chair 9Polytechnique Montreal 10University of British Columbia. Correspondence to: Ahmed Masry <ahmed.masry@servicenow.com>, Sai Rajeswar <sai.mudumba@servicenow.com>. Figure 1: Performance of Different VLM Connectors. The proposed ALIGN connector outperforms other methods across benchmarks using the same training configuration. Radial distance is proportion of maximal score, truncated at 0.7 (black dot). ing both the visual and textual contents of scanned documents (Kim et al., 2022; Lee et al., 2023; Liu et al., 2023a; 2024; Hu et al., 2024; Wang et al., 2023a; Rodriguez et al., 2024b). Such tasks are common in real-world commercial applications, including invoice parsing (Park et al., 2019), form reading (Jaume et al., 2019), and document question answering (Mathew et al., 2021b). VLM architectures typically consist of three components: (i) vision encoder to process raw images, (ii) Large Language Model (LLM) pre-trained on text, and (iii) connector module that maps the visual features from the vision encoder into the LLMs semantic space. central challenge in this pipeline is to effectively map the continuous feature embeddings of the vision encoder into the latent space of the LLM while preserving the semantic properties of visual concepts. Existing approaches can be broadly categorized into deep fusion and shallow fusion 1 ALIGNVLM: Bridging Vision and Language Latent Spaces for Multimodal Understanding methods. Deep fusion methods, such as NVLM (Dai et al., 2024), Flamingo (Alayrac et al., 2022), CogVLM (Wang et al., 2023b), and LLama 3.2-Vision (Grattafiori et al., 2024), integrate visual and textual features by introducing additional cross-attention and feed-forward layers at each layer of the LLM. While effective at enhancing cross-modal interaction, these methods substantially increase the parameter count of the VLM compared to the base LLM, resulting in high computational overhead and reduced efficiency. In contrast, shallow fusion methods project visual features from the vision encoder into the LLM input embedding space using either multilayer perceptrons (MLPs) (Liu et al., 2023b; 2024) or attention-based mechanisms such as the Perceiver Resampler (Li et al., 2023; Laurencon et al., 2024; Alayrac et al., 2022), before concatenating them with the textual prompts input embeddings. This approach is more parameter-efficient and computationally lighter than deep fusion methods, but it lacks mechanism to ensure the projected embeddings remain within the region spanned by the LLMs text embeddings i.e. regions the LLM was pretrained to understand. As result, unconstrained visual features can produce out-of-distribution (OOD) and noisy inputs, leading to misalignment between modalities and often degrading overall performance. Recent methods like Ovis (Lu et al., 2024) attempt to alleviate these issues by introducing separate visual embeddings indexed from the vision encoder outputs and combined together to construct the visual inputs to the LLM. However, this approach significantly increases parameter count due to the massive embedding matrix and requires extensive training to learn new embedding space without guaranteeing alignment with the LLMs input latent space. To address these limitations, this paper introduces ALIGNVLM, novel framework that sidesteps direct projection of visual features into the LLM embedding space. Instead, our proposed connector, ALIGN, maps visual features into probability distributions over the LLMs existing pretrained vocabulary embeddings, which are then combined into weighted representation of the text embeddings. By constraining each visual feature as convex combination of the LLM text embeddings, our approach leverages the linguistic priors already encoded in the LLMs text space. This ensures that the resulting visual features lie within the convex hull of the LLMs embedding space, reducing the risk of noisy or out-of-distribution inputs and improving alignment between modalities. Our experimental results show that this approach improves performance on various document understanding tasks, outperforming prior connector methods by effectively fusing visual and linguistic content. We summarize our main contributions as follows: We introduce family of Vision-Language Models, ALIGNVLM, that achieves state-of-the-art performance on multimodal document understanding tasks by leveraging ALIGN. We conduct extensive experiments demonstrating the robustness and effectiveness of ALIGN across different model sizes ranging from 1B to 8B parameters. Our code and models will be public upon acceptance. 2. Related Work 2.1. Vision-Language Models Over the past few years, Vision-Language Models (VLMs) have achieved remarkable progress, largely due to advances in Large Language Models (LLMs). Initially demonstrating breakthroughs in text understanding and generation (Brown et al., 2020; Raffel et al., 2023; Achiam et al., 2023; Grattafiori et al., 2024; Qwen et al., 2025; Team, 2024), LLMs are now increasingly used to effectively interpret visual inputs (Liu et al., 2023b; Li et al., 2024; Wang et al., 2024; Chen et al., 2024b; Dai et al., 2024; Drouin et al., 2024; Rodriguez et al., 2022). This progress has enabled real-world applications across diverse domains, particularly in multimodal document understanding for tasks like form reading (Svetlichnaya, 2020), document question answering (Mathew et al., 2021b), and chart question answering (Masry et al., 2022). VLMs commonly adopt threecomponent architecture: pretrained vision encoder (Zhai et al., 2023; Radford et al., 2021), LLM, and connector module. key challenge for VLMs is effectively aligning visual features with the LLMs semantic space to enable accurate and meaningful multimodal interpretation. 2.2. Vision-Language Alignment for Multimodal Models Existing vision-language alignment approaches can be classified into deep fusion and shallow fusion. Deep fusion methods integrate visual and textual features by modifying the LLMs architecture, adding cross-attention and feedforward layers. For example, Flamingo (Alayrac et al., 2022) employs the Perceiver Resampler, which uses fixed latent embeddings to attend to vision features and fuses them into the LLM via gated cross-attention layers. Similarly, NVLM (Dai et al., 2024) adopts cross-gated attention while replacing the Perceiver Resampler with simpler MLP. CogVLM (Wang et al., 2023b) extends this approach by incorporating new feed-forward (FFN) and QKV layers for the vision modality within every layer of the LLM. While these methods improve cross-modal alignment, they significantly increase parameter counts and computational overhead, making them less efficient. We propose novel connector, ALIGN, to bridge the representation gap between vision and text modalities. On the other hand, shallow fusion methods are more computationally efficient, mapping visual features into the LLMs 2 ALIGNVLM: Bridging Vision and Language Latent Spaces for Multimodal Understanding embedding space without altering its architecture. These methods can be categorized into three main types: (1) MLPbased mapping, such as LLaVA (Liu et al., 2023b) and PaliGemma (Beyer et al., 2024), which use multilayer perceptrons (MLP) to project visual features but often produce misaligned or noisy features due to lack of constraints (Rodriguez et al., 2024b); (2) cross-attention mechanisms, BLIP-2 (Li et al., 2023) uses Q-Former, which utilizes fixed set of latent embeddings to cross-attend to visual features, but that may still produce noisy or OOD visual features; and (3) visual embeddings, such as those introduced by Ovis (Lu et al., 2024), which use embeddings indexed by the vision encoders outputs to produce the visual inputs. While this regularizes feature mapping, it adds substantial parameter overhead and creates new vision embedding space, risking misalignment with the LLMs text embedding space. Encoder-free VLMs, like Fuyu-8B 1 and EVE (Diao et al., 2024), eliminate dedicated vision encoders but show degraded performance (Beyer et al., 2024). In contrast, ALIGNVLM maps visual features from the vision encoder into probability distributions over the LLMs text embeddings, using them to compute convex combination. By leveraging the linguistic priors encoded in the LLMs vocabulary, ALIGNVLM ensures that visual features remain within the convex hull of the text embeddings, mitigating noisy or out-of-distribution inputs and enhancing alignment, particularly for tasks that require joint modalities representation like multimodal document understanding. 3. Methodology 3.1. Model Architecture The overall model architecture, shown in Figure 2, consists of three main components: (1) Vision Encoder. To handle high-resolution images of different aspect ratios, we divide each input image into multiple tiles according to one of the predefined aspect ratios (e.g., 1:1, 1:2, . . . , 9:1) chosen via coverage ratio (Lu et al., 2024; Chen et al., 2024a). Due to limited computational resources, we set the maximum number of tiles to 9. Each tile is further partitioned into 14 14 patches, projected into vectors, and processed by SigLip-400M vision encoder (Zhai et al., 2023) to extract contextual visual features. Each tile {1, , } is divided into Nt patches Pt = {pt,1, , pt,Nt}, where pt,i is the i-th patch of tile t. The vision encoder 1https://www.adept.ai/blog/fuyu-8b maps these patches to set of visual feature vectors Ft = VisionEncoder(Pt) Ft = {ft,1, , ft,Nt}, ft,i Rd. Finally, we concatenate the feature sets across all tiles into single output = concat (cid:16) F1, F2, , FT (cid:17) . (2) ALIGN Module. This module aligns the visual features with the LLM. linear layer W1 RDd first projects the visual features RT Ntd to the LLMs token embedding space: one RD vector per token. second linear layer W2 RV (initialized from the LLMs language-model head) followed by softmax, produces probability simplex Pvocab over the LLMs vocabulary (V tokens) Pvocab = (1) softmax(LayerNorm(W2 LayerNorm(W1F))) We then use the LLM text embeddings Etext RV to compute weighted sum align = vocabEtext. (2) Finally, we concatenate beddings to form the LLM input align with the tokenized text emHinput = concat(cid:0)F align, Etext(x)(cid:1), where Etext(x) is obtained by tokenizing the input text = (x1, , xM ) and selecting the corresponding embeddings from Etext such that Etext(x) = (cid:2)Etext(x1), , Etext(xM )(cid:3). (3) (3) Large Language Model. We feed the concatenated vision and text vectors, Hinput, into the LLM, which then generates output text auto-regressively. To demonstrate the effectiveness of our alignment technique, we experiment with the Llama 3.1 model family (Grattafiori et al., 2024). These models offer state-of-the-art performance and permissive licenses, making them suitable for commercial applications. In particular, we utilize Llama 3.2-1B, Llama 3.2-3B, and Llama 3.1-8B. 3.2. Motivation and relation with existing methods By construction, each RD representation in align is constrained to the convex hull of the points Etext, thus concentrating the visual features in the part of latent space that the LLM can effectively interpret. Moreover, we argue that 3 ALIGNVLM: Bridging Vision and Language Latent Spaces for Multimodal Understanding Figure 2: ALIGNVLM Model Architecture. The vision encoder extracts image features, which are processed to produce probabilities over the LLM embeddings. weighted average combines these probabilities with embeddings to generate vision input vectors. Text inputs are tokenized, and the corresponding embeddings are selected from the embedding matrix, which is then used as input to the LLM. We display the vision layers in blue , and the text layers in purple . our initialization of W2 to the language model head is an inductive bias toward recycling some of the semantics of these text tokens into visual tokens. This contrasts with past methods that have been proposed to adapt the vision encoder outputs RT Ntd to an RT NtD to be fed to the LLM. Here, we consider two examples in more detail, highlighting these contrasts. (1) MLP Connector (Liu et al., 2023b) applies linear projection with parameters WMLP RDd and bMLP RD, followed by an activation function σ (e.g., ReLU) MLP = σ(WMLPF + bMLP). These parameters are all learned from scratch, with no particular bias aligning them to text embeddings. (2) Visual Embedding Table (Lu et al., 2024) introduces an entire new set of visual embeddings EVET RKD which, together with the weights WVET RKd, specifies VET = softmax(WVETF)EVET. When < d, our W2W1 amounts to low-rank version of WVET. There is thus much more to learn to obtain VET, and there is again no explicit pressure to align it with the text embeddings. 3.3. Training Datasets & Stages We train our model in three stages: Stage 1. This stage focuses on training the ALIGN Module to map visual features to the LLMs text embeddings 4 effectively. We use the CC-12M dataset (Changpinyo et al., 2021), large-scale web dataset commonly used for VLM pretraining (Liu et al., 2023b), which contains 12M imagetext pairs. However, due to broken or unavailable links, we retrieved 8.1M pairs. This dataset facilitates the alignment of visual features with the text embedding space of the LLM. During this stage, we train the full model, as this approach improves performance and stabilizes the training of the ALIGN Module. Stage 2. The goal is to enhance the models document understanding capabilities, such as OCR, document structure comprehension, in-depth reasoning, and instructionfollowing. We leverage the BigDocs-7.5M dataset (Rodriguez et al., 2024a), curated collection of licensepermissive datasets designed for multimodal document understanding. This dataset aligns with the Accountability, Responsibility, and Transparency (ART) principles (Bommasani et al., 2023; Vogus & Llansoe, 2021), ensuring compliance for commercial applications. As in Stage 1, we train the full model during this stage. Stage 3. To enhance the models instruction-tuning capabilities, particularly for downstream tasks like question answering, we further train it on the DocDownstream (Rodriguez et al., 2024a; Hu et al., 2024) instruction tuning dataset. In this stage, the vision encoder is frozen, focusing training exclusively on the LLM and ALIGN module. 4. Experimental Setup Setup. We conduct all experiments using 8 nodes of H100 GPUs, totaling 64 GPUs. For model training, we leverage ALIGNVLM: Bridging Vision and Language Latent Spaces for Multimodal Understanding Table 1: Main Results on General Document Benchmarks. We compare ALIGNVLM (ours) with state-of-the-art (SOTA) open and closed-source instructed models, and with base models that we trained using the process described in Section 3.3. ALIGNVLM models outperform all Base VLM models trained in the same data regime. Our models also perform competitively across document benchmarks even compared with SOTA models, in which the data regime is more targeted and optimized. Color coding for comparison: closed-source models , open-source models below 7B parameters , open-source models between 7-12B parameters . Model Claude-3.5 Sonnet GeminiPro-1.5 GPT-4o"
        },
        {
            "title": "DocV Q A",
            "content": "VA InfoV VAL"
        },
        {
            "title": "DeepForm",
            "content": "TEST"
        },
        {
            "title": "W T Q\nTEST",
            "content": "TabFact TEST"
        },
        {
            "title": "ChartQ A",
            "content": "TEST TextV VAL TableV TEST Score Avg. Closed-Source VLMs (Opaque Training Data) 88.48 91.23 92.80 31.41 32.16 38.39 59.05 73.94 66.37 24.82 24.07 29.92 Open-Source Instruct VLMs (Semi-Opaque Training Data) 47.13 50.29 46.63 53.48 71.22 81.10 51.84 34.68 85.70 71.42 68.16 70.46 81.27 80.43 72.87 56.54 58.46 64. Janus-1.3B (Wu et al., 2024a) Qwen2-VL-2B (Wang et al., 2024) InternVL-2.5-2B (Chen et al., 2024b) DeepSeek-VL2-Tiny-3.4B (Wu et al., 2024b) Phi3.5-Vision-4B (Abdin et al., 2024) Qwen2-VL-7B (Wang et al., 2024) LLaVA-NeXT-7B (Xu et al., 2024) DocOwl1.5-8B (Hu et al., 2024) InternVL-2.5-8B (Chen et al., 2024b) Ovis-1.6-Gemma2-9B (Lu et al., 2024) Llama3.2-11B (Grattafiori et al., 2024) Pixtral-12B (Agrawal et al., 2024) 9.30 38.20 36.33 35.07 17.18 52.52 20.06 38.87 50.33 50.72 23.03 45.18 Document Understanding Instructed Models (Instruction Tuned on BigDocs-7.5M + DocDownStream (Rodriguez et al., 2024a; Hu et al., 2024)) 17.09 64.11 61.85 63.88 56.20 76.12 30.90 49.94 75.36 73.97 36.62 49.45 15.06 25.18 16.58 19.04 7.49 23.37 5.35 37.99 22.31 23.91 3.47 24.07 51.34 57.21 57.26 52.15 30.43 74.68 52.83 79.67 74.75 76.66 58.33 73.53 0.62 32.38 13.14 25.11 10.47 34.55 1.30 68.84 34.55 45.16 1.78 27. 57.20 73.40 74.96 80.92 82.16 83.16 52.12 68.56 82.84 81.40 23.80 71.80 30.15 89.16 87.70 88.57 86.00 93.83 63.51 80.73 91.98 88.84 82.71 87.67 51.97 79.90 76.85 80.48 73.12 84.48 65.10 68.91 79.00 77.73 54.28 76.09 Qwen2-VL-2B (base+) (Qwen et al., 2025) ALIGNVLM-Llama-3.2-1B (ours) ALIGNVLM-Llama-3.2-3B (ours) DocOwl1.5-8B (base+) (Hu et al., 2024) Llama3.2-11B (base+) (Grattafiori et al., 2024) ALIGNVLM-Llama-3.1-8B (ours) 57.23 72.42 79.63 78.70 78.99 81.18 31.88 38.16 44.53 47.62 44.27 53. 49.31 60.47 63.49 64.39 67.05 63.25 34.39 33.71 35.25 36.93 37.22 35.50 31.61 28.66 38.59 35.69 40.18 45.31 64.75 71.31 78.51 72.65 78.04 83.04 68.60 65.44 71.88 65.80 71.40 75.00 61.01 48.81 57.38 67.30 68.46 64. 18.67 43.07 42.20 56.30 70.70 53.97 32.87 52.60 52.10 48.33 22.40 67.13 47.53 50.29 60.10 49.03 56.73 64.33 27.93 55.84 51.87 55.72 48.19 64.08 36.00 60.68 62.58 62.96 34.04 58.03 49.59 52.14 58.81 57.56 60.26 62.88 the MS-Swift framework (Zhao et al., 2024) for its flexibility. Additionally, we utilize the DeepSpeed framework (Aminabadi et al., 2022), specifically the ZeRO-3 configuration, to optimize efficient parallel training across multiple nodes. Detailed hyperparameters are outlined in Appendix A.1. Baselines. Our work focuses on architectural innovations, so we ensure that all baselines are trained on the same datasets. To enable fair comparisons, we evaluate our models against set of Base VLMs fine-tuned on the same instruction-tuning tasks (Stages 2 and 3) as our models, using the BigDocs-7.5M and BigDocs-DocDownstream datasets. This approach ensures consistent training data, avoiding biases introduced by the Instruct versions of VLMs, which are often trained on undisclosed instructiontuning datasets. Due to the scarcity of recently released publicly available Base VLMs, we primarily compare our model against the following Base VLMs of varying sizes: Qwen2-VL-2B (Wang et al., 2024), DocOwl1.5-8B (Hu et al., 2024), and LLama 3.2-11B (Grattafiori et al., 2024). For additional context, we also include results from the Instruct versions of recent VLMs of different sizes: Phi3.5-Vision-4B (Abdin et al., 2024), Qwen2-VL-2B and 7B (Wang et al., 2024), LLaVA-NeXT-7B (Liu et al., 2024), InternVL2.5-2B and 8B (Chen et al., 2024b), Janus1.3B (Wu et al., 2024a), DeepSeek-VL2-Tiny (Wu et al., 2024b), Ovis1.6-Gemma-9B (Lu et al., 2024), Llama3.211B (Grattafiori et al., 2024), DocOwl1.5-8B (Hu et al., 2024), and Pixtral-12B (Agrawal et al., 2024). Evaluation Benchmarks. We evaluate our models on diverse range of document understanding benchmarks that assess the models capabilities in OCR, chart reasoning, table processing, or form comprehension. In particular, we employ the VLMEvalKit (Duan et al., 2024) framework and report the results on the following popular benchmarks: 5 ALIGNVLM: Bridging Vision and Language Latent Spaces for Multimodal Understanding Table 2: Impact of Connector Designs on VLM Performance: We present the results of experiments evaluating different connector designs for conditioning LLMs on visual features. Our proposed ALIGN connector is compared against basic Multi-Layer Perceptron (MLP), the Perceiver Resampler, and Ovis. The results demonstrate that ALIGN consistently outperforms these alternatives across all benchmarks."
        },
        {
            "title": "DocV Q A",
            "content": "VA InfoV VAL"
        },
        {
            "title": "DeepForm",
            "content": "TEST Model Llama-3.2-3B-MLP Llama-3.2-3B-Perciever R. Llama-3.2-3B-Ovis Llama-3.2-3B-ALIGN (ours) 71.46 69.08 74.68 79.63 37.56 34.13 42.11 44.53 62.07 57.08 58.02 63. DocVQA (Mathew et al., 2021b), InfoVQA (Mathew et al., 2021a), DeepForm (Svetlichnaya, 2020), KLC (Stanisławek et al., 2021), WTQ (Pasupat & Liang, 2015), TabFact (Chen et al., 2020), ChartQA (Masry et al., 2022), TextVQA (Singh et al., 2019), and TableVQA (Kim et al., 2024). 5. Results 5.1. Main Results Table 1 presents the performance of ALIGNVLM compared to state-of-the-art (SOTA) openand closed-source instructed models, as well as baseline Base VLMs fine-tuned in the same instruction-tuning setup. The results demonstrate that ALIGNVLM consistently outperforms all Base VLMs within the same size category and achieves competitive performance against SOTA Instruct VLMs despite being trained on more limited data regime. Below, we provide detailed analysis. ALIGNVLM vs. Base VLMs. Our ALIGNVLM models, based on Llama 3.2-1B and Llama 3.2-3B, significantly outperform the corresponding Base VLM, Qwen2-VL-2B, by up to 9.22%. Notably, ALIGNVLM-Llama-3.2-3B surpasses DocOwl1.5-8B, which has 4B more parameters, demonstrating the effectiveness of ALIGN in enhancing multimodal capabilities compared to traditional shallow fusion methods (e.g., MLPs). Furthermore, our 8B model achieves 2.62% improvement over Llama3.2-11B despite sharing the same Base LLM, Llama3.1-8B. Since all models in this comparison were trained on the same instruction-tuning setup, this experiment provides controlled evaluation, isolating the impact of architectural differences rather than dataset biases. Consequently, these results suggest that ALIGNVLM outperforms VLMs with shallow fusion techniques and surpasses parameter-heavy deep fusion VLMs, such as Llama3.2-11B, while maintaining more efficient architecture. ALIGNVLM vs. Instruct VLMs. Even as open-source Instruct models are trained on significantly larger, often undisclosed instruction-tuning datasets, ALIGNVLM"
        },
        {
            "title": "K L C\nTEST",
            "content": "33.36 31.75 33.50 35."
        },
        {
            "title": "W T Q\nTEST",
            "content": "28.94 27.95 33.13 38."
        },
        {
            "title": "TabFact",
            "content": "TEST"
        },
        {
            "title": "ChartQ A",
            "content": "TEST"
        },
        {
            "title": "TextV Q A",
            "content": "VAL"
        },
        {
            "title": "TableV Q A",
            "content": "TEST Score Avg. 73.22 71.93 76.67 78.51 66.48 65.16 67.92 71.88 53.56 51.33 52.60 57. 50.96 47.76 53.93 60.10 53.06 50.68 54.72 58.81 achieves superior performance. For instance, ALIGNVLMLlama-3.2-3B (58.81%) outperforms all instructed VLMs in its size category, surpassing its closest competitor, Qwen2VL-2B (55.84%), by 2.97%. Additionally, our 8B model outperforms significantly larger models such as Llama 3.211B and PixTral-12B by substantial margins. It also surpasses InternVL-2.5-8B and performs competitively with Qwen2-VL-7B, though direct comparison may not be entirely fair since Qwen2-VL-7B was trained on an undisclosed instruction-tuning dataset. Finally, ALIGNVLM also exhibits comparable performance to closed-source models like GeminiPro-1.5 and GPT4o. Overall, these results validate the effectiveness of ALIGN and establish ALIGNVLM as state-of-the-art model for multimodal document understanding. 5.2. Impact of Connector Designs on VLM Performance To assess the effectiveness of our ALIGN module, we compare it against three different and widely used shallow fusion VLM connectors: MLP, Perceiver Resampler, and Ovis. The results in Table 2 show that ALIGN consistently outperforms all alternatives, demonstrating its superiority both in aligning visual and textual modalities and in multimodal document understanding. MLP and Perceiver Resampler achieve the lowest performance, 53.06% and 50.68%, respectively, due to their direct feature projection, which lacks an explicit mechanism to align visual features with the LLMs text space, leading to misalignment. Ovis introduces separate visual embedding table, but this additional complexity does not significantly improve alignment, yielding only 54.72% accuracy. In contrast, ALIGN ensures that visual features remain within the convex hull of the LLMs text latent space, leveraging the linguistic priors of the LLM to enhance alignment and mitigate noisy embeddings. This design leads to the highest performance (58.81%), establishing ALIGN as the most effective connector for integrating vision and language in multimodal document understanding. We provide some example outputs of the Llama-3.2-3B models with different connector designs in Appendix A.3. 6 ALIGNVLM: Bridging Vision and Language Latent Spaces for Multimodal Understanding Figure 3: Probability distribution over the LLM text tokens, showing dense probabilities and higher values for tokens associated with white space in document images. Figure 4: Comparison of Llama-3.2-3b-ALIGN and Llama3.2-3B-MLP on the Easy and Hard VCR tasks. 5.3. Probability Distribution over Text Tokens Analysis To better understand the behavior of ALIGN, we examine the probability distribution, Pvocab in Eq (1), over the LLMs text vocabulary generated from visual features. Specifically, we process 100 document images through the vision encoder and ALIGN, then average the resulting probability distributions across all image patches. The final distribution is shown in Figure 3. As illustrated, the distribution is dense (rather than sparse), with the highest probability assigned to single token being 0.0118. This can be explained by the vision feature space being continuous and of much higher cardinality than the discrete text space. Indeed, while the LLM has 128K distinct vocabulary tokens, an image patch (e.g., 1414 pixels) contains continuous, high-dimensional information that cannot be effectively mapped to single or few discrete tokens. Furthermore, we observe that tokens on the left side of the distribution in Figure 3 have higher probabilities than the rest. Upon investigation, we found that these tokens correspond to patches that are predominantly white common feature in document images. Further analysis of the associated text tokens reveals that they predominantly consist of punctuation marks, as illustrated further in Appendix A.2. This suggests that the model repurposes punctuation marks to represent whitespaces. This may be attributed to the fact that both punctuation and whitespaces act as structural cues and separators. Other possibilities include whitespaces being rarely directly-required to perform task, and LLMs may pay less specific attention to common tokens such as punctuation. 5.4. Pixel-Level Tasks Analysis To rigorously evaluate the ability of vision-language models to integrate fine-grained visual and textual pixel-level cues, we test our model on the VCR benchmark (Zhang et al., 2024), which requires the model to recover partially occluded texts with pixel-level hints from the revealed parts of the text. This task challenges VLMs alignment of text and image in extreme situations. Current state-of-the-art models like GPT-4V (OpenAI et al., 2023), Claude 3.5 Sonnet (Anthropic, 2024), and Llama-3.2 (Dubey et al., 2024) significantly underperform humans on hard VCR task due to their inability to process subtle pixel-level cues in occluded text regions. These models frequently discard critical visual tokens during image tokenization on semantic priors, overlooking the interplay between partial character strokes and contextual visual scenes. To evaluate performance on VCR, we modify our Stage 3 SFT dataset composition by replacing the exclusive use of DocDownstream with 5:1 blended ratio of DocDownstream and VCR training data. This adjustment enables direct evaluation of our architecture ALIGNs ability to leverage pixel-level character cues. From the experimental outcomes, it is evident that ALIGNVLM consistently outperforms the MLP Connector Model across both easy and hard settings of the pixel-level VCR task (see Figure 4), with improvements ranging from 10.18% on the hard setting to 14.41% on the easy setting. We provide case study on VCR in Figure 5, featuring four representative examples. In Figure 5a, it is evident that the MLP connector model fails to capture semantic consistency as effectively as ALIGNVLM. The phrase The commune first census in written history in (where the words in italics are generated by the model while the rest are in the image) is not as semantically coherent as the phrase generated by ALIGN The commune first appears in written history in. Beyond the issue of semantic fluency, in Figure 5b we also observe that ALIGNVLM successfully identifies the uncovered portion of the letter in accounting and uses it as pixel-level hint to infer the correct word. In contrast, the MLP model fails to effectively attend to this crucial detail. 7 ALIGNVLM: Bridging Vision and Language Latent Spaces for Multimodal Understanding GT: MLP: (appears in written history in) (census in written history in) ALIGN (appears in written history in) GT: MLP: (the system used for assigning) (the system used for accounting) ALIGN (the system used for assigning) GT: MLP: (mines situated near Llanengan on) (mines situated near Llanengan on) ALIGN (mines situated near Llanongan on) GT: MLP: (Gorden County is home to) (Gorden County is home to) ALIGN (Garden County is home to) (a) Positive Example 1 (b) Positive Example 2 (c) Negative Example 1 (d) Negative Example 2 Figure 5: Case Study for Pixel-Level Tasks. We provide examples of our proposed ALIGN connector compared with the Multi-Layer Perceptron (MLP) connector. The ALIGN connector tends to better map visual elements to common words. GT is the ground truth. Figures 5c and 5d show examples where ALIGNVLM fails on the VCR task. These carefully picked instances show that our method mistakes names of landmarks with common words when the two are very similar. As seen in the examples, ALIGNVLM mistakes Llanengan for Llanongan and Gorden for Garden. In both instances, the pairs differ by one character, indicating perhaps that ALIGNVLM tends to align vision representations to more common tokens in the vocabulary. One approach that would potentially mitigate such errors would be to train ALIGNVLM with more contextually-relevant data. 5.5. Robustness to Noise Analysis To evaluate the robustness of our ALIGN connector to noisy visual features, we conduct an experiment where random Gaussian noise is added to the visual features produced by the vision encoder before passing them into the connector. Specifically, given the visual features RN output by the vision encoder (where is the number of feature vectors and is their dimensionality), we perturbed them as (cid:101)F = + N, (0, σ = 3). Table 3: Robustness to Noise. Comparison of Avg. Scores with and without Gaussian noise (σ = 3), including performance drop (). As shown in Table 3, our ALIGN connector demonstrates high robustness to noise, with only 1.67% average drop in performance. In contrast, the widely adopted MLP connector suffers significant performance degradation of 25.54%, highlighting its vulnerability to noisy inputs. These empirical results support our hypothesis that leveraging the knowledge encoded in the LLMs text embeddings and constraining the visual features within the convex hull of the text latent space act as regularization mechanism, reducing the models sensitivity to noisy visual features. 6. Conclusion We introduce ALIGN, novel connector designed to align vision and language latent spaces in vision-language models (VLMs), specifically enhancing multimodal document understanding. By improving cross-modal alignment and minimizing noisy embeddings, our models, ALIGNVLM, which leverage ALIGN, achieve state-of-the-art performance across diverse document understanding tasks. This includes outperforming base VLMs trained on the same datasets and open-source instruct models trained on undisclosed data. Extensive experiments and ablations validate the robustness and effectiveness of ALIGN compared to existing connector designs, establishing it as significant contribution to vision-language modeling. Future work will explore training on more diverse instruction-tuning datasets to generalize beyond document understanding to broader domains. Model Llama-3.2-3B-MLP Llama-3.2-3B-ALIGN (ours) Without Noise With Noise Drop () 25.54 1.67 27.52 57.14 53.06 58. 8 ALIGNVLM: Bridging Vision and Language Latent Spaces for Multimodal Understanding"
        },
        {
            "title": "Impact Statement",
            "content": "This work contributes to the multimodal AI research community by introducing novel approach for fusing vision and language modalities within large language models. By leveraging our framework in the context of generative models, we enable more effective integration of visual and textual information, enhancing the generative models ability to generate free-form text for multimodal tasks. However, like all generative models, our approach is subject to potential biases and hallucinationschallenges inherent to large language models that must be carefully considered in deployment. Since these issues are not unique to our approach, we do not highlight any specific concerns here."
        },
        {
            "title": "References",
            "content": "Abdin, M., Aneja, J., Awadalla, H., Awadallah, A., Awan, A. A., Bach, N., Bahree, A., Bakhtiari, A., Bao, J., Behl, H., Benhaim, A., Bilenko, M., Bjorck, J., Bubeck, S., Cai, M., Cai, Q., Chaudhary, V., Chen, D., Chen, D., Chen, W., Chen, Y.-C., Chen, Y.-L., Cheng, H., Chopra, P., Dai, X., Dixon, M., Eldan, R., Fragoso, V., Gao, J., Gao, M., Gao, M., Garg, A., Giorno, A. D., Goswami, A., Gunasekar, S., Haider, E., Hao, J., Hewett, R. J., Hu, W., Huynh, J., Iter, D., Jacobs, S. A., Javaheripi, M., Jin, X., Karampatziakis, N., Kauffmann, P., Khademi, M., Kim, D., Kim, Y. J., Kurilenko, L., Lee, J. R., Lee, Y. T., Li, Y., Li, Y., Liang, C., Liden, L., Lin, X., Lin, Z., Liu, C., Liu, L., Liu, M., Liu, W., Liu, X., Luo, C., Madan, P., Mahmoudzadeh, A., Majercak, D., Mazzola, M., Mendes, C. C. T., Mitra, A., Modi, H., Nguyen, A., Norick, B., Patra, B., PerezBecker, D., Portet, T., Pryzant, R., Qin, H., Radmilac, M., Ren, L., de Rosa, G., Rosset, C., Roy, S., Ruwase, O., Saarikivi, O., Saied, A., Salim, A., Santacroce, M., Shah, S., Shang, N., Sharma, H., Shen, Y., Shukla, S., Song, X., Tanaka, M., Tupini, A., Vaddamanu, P., Wang, C., Wang, G., Wang, L., Wang, S., Wang, X., Wang, Y., Ward, R., Wen, W., Witte, P., Wu, H., Wu, X., Wyatt, M., Xiao, B., Xu, C., Xu, J., Xu, W., Xue, J., Yadav, S., Yang, F., Yang, J., Yang, Y., Yang, Z., Yu, D., Yuan, L., Zhang, C., Zhang, C., Zhang, J., Zhang, L. L., Zhang, Y., Zhang, Y., Zhang, Y., and Zhou, X. Phi-3 technical report: highly capable language model locally on your phone, 2024. URL https://arxiv.org/abs/2404.14219. Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Agrawal, P., Antoniak, S., Hanna, E. B., Bout, B., Chaplot, D., Chudnovsky, J., Costa, D., Monicault, B. D., Garg, S., Gervet, T., Ghosh, S., Heliou, A., Jacob, P., Jiang, A. Q., Khandelwal, K., Lacroix, T., Lample, G., Casas, D. L., Lavril, T., Scao, T. L., Lo, A., Marshall, W., Martin, L., Mensch, A., Muddireddy, P., Nemychnikova, V., Pellat, M., Platen, P. V., Raghuraman, N., Rozi`ere, B., Sablayrolles, A., Saulnier, L., Sauvestre, R., Shang, W., Soletskyi, R., Stewart, L., Stock, P., Studnia, J., Subramanian, S., Vaze, S., Wang, T., and Yang, S. Pixtral 12b, 2024. URL https://arxiv.org/abs/2410.07073. Alayrac, J.-B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., Lenc, K., Mensch, A., Millican, K., Reynolds, M., Ring, R., Rutherford, E., Cabi, S., Han, T., Gong, Z., Samangooei, S., Monteiro, M., Menick, J., Borgeaud, S., Brock, A., Nematzadeh, A., Sharifzadeh, S., Binkowski, M., Barreira, R., Vinyals, O., Zisserman, A., and Simonyan, K. Flamingo: visual language model for few-shot learning, 2022. URL https://arxiv.org/abs/2204.14198. Aminabadi, R. Y., Rajbhandari, S., Zhang, M., Awan, A. A., Li, C., Li, D., Zheng, E., Rasley, J., Smith, S., Ruwase, O., and He, Y. Deepspeed inference: Enabling efficient inference of transformer models at unprecedented scale, 2022. URL https://arxiv.org/abs/2207.00032. Anthropic. The claude 3 model family: Opus, sonnet, haiku. 2024. Beyer, L., Steiner, A., Pinto, A. S., Kolesnikov, A., Wang, X., Salz, D., Neumann, M., Alabdulmohsin, I., Tschannen, M., Bugliarello, E., Unterthiner, T., Keysers, D., Koppula, S., Liu, F., Grycner, A., Gritsenko, A., Houlsby, N., Kumar, M., Rong, K., Eisenschlos, J., Kabra, R., Bauer, M., Boˇsnjak, M., Chen, X., Minderer, M., Voigtlaender, P., Bica, I., Balazevic, I., Puigcerver, J., Papalampidi, P., Henaff, O., Xiong, X., Soricut, R., Harmsen, J., and Zhai, X. Paligemma: versatile 3b vlm for transfer, 2024. URL https://arxiv.org/abs/2407. 07726. Bommasani, R., Klyman, K., Longpre, S., Kapoor, S., Maslej, N., Xiong, B., Zhang, D., and Liang, P. The foundation model transparency index, 2023. URL https: //arxiv.org/abs/2310.12941. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. Advances in neural information processing systems, 33: 18771901, 2020. Changpinyo, S., Sharma, P., Ding, N., and Soricut, R. Conceptual 12m: Pushing web-scale image-text pretraining to recognize long-tail visual concepts, 2021. URL https://arxiv.org/abs/2102.08981. Chen, W., Wang, H., Chen, J., Zhang, Y., Wang, H., Li, S., Zhou, X., and Wang, W. Y. Tabfact: large-scale 9 ALIGNVLM: Bridging Vision and Language Latent Spaces for Multimodal Understanding dataset for table-based fact verification. In International Conference Learning Representations, 2020. Chen, Z., Wang, W., Tian, H., Ye, S., Gao, Z., Cui, E., Tong, W., Hu, K., Luo, J., Ma, Z., Ma, J., Wang, J., Dong, X., Yan, H., Guo, H., He, C., Shi, B., Jin, Z., Xu, C., Wang, B., Wei, X., Li, W., Zhang, W., Zhang, B., Cai, P., Wen, L., Yan, X., Dou, M., Lu, L., Zhu, X., Lu, T., Lin, D., Qiao, Y., Dai, J., and Wang, W. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites, 2024a. URL https://arxiv.org/abs/2404.16821. Chen, Z., Wu, J., Wang, W., Su, W., Chen, G., Xing, S., Zhong, M., Zhang, Q., Zhu, X., Lu, L., et al. Internvl: Scaling up vision foundation models and aligning for In Proceedings of the generic visual-linguistic tasks. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2418524198, 2024b. Dai, W., Lee, N., Wang, B., Yang, Z., Liu, Z., Barker, J., Rintamaki, T., Shoeybi, M., Catanzaro, B., and Ping, W. Nvlm: Open frontier-class multimodal llms. arXiv preprint arXiv: 2409.11402, 2024. Diao, H., Cui, Y., Li, X., Wang, Y., Lu, H., and Wang, X. Unveiling encoder-free vision-language models. arXiv preprint arXiv:2406.11832, 2024. Drouin, A., Gasse, M., Caccia, M., Laradji, I. H., Verme, M. D., Marty, T., Boisvert, L., Thakkar, M., Cappart, Q., Vazquez, D., Chapados, N., and Lacoste, A. Workarena: How capable are web agents at solving common knowledge work tasks?, 2024. URL https://arxiv.org/ abs/2403.07718. Duan, H., Yang, J., Qiao, Y., Fang, X., Chen, L., Liu, Y., Dong, X., Zang, Y., Zhang, P., Wang, J., et al. Vlmevalkit: An open-source toolkit for evaluating large In Proceedings of the 32nd multi-modality models. ACM International Conference on Multimedia, pp. 11198 11201, 2024. Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan, A., Goyal, A., Hartshorn, A., Yang, A., Mitra, A., Sravankumar, A., and et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Grattafiori, A., Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Vaughan, A., Yang, A., Fan, A., Goyal, A., Hartshorn, A., Yang, A., Mitra, A., Sravankumar, A., Korenev, A., Hinsvark, A., Rao, A., Zhang, A., Rodriguez, A., Gregerson, A., Spataru, A., Roziere, B., Biron, B., Tang, B., Chern, B., Caucheteux, C., Nayak, C., Bi, C., Marra, C., McConnell, C., Keller, C., Touret, C., Wu, C., Wong, 10 C., Ferrer, C. C., Nikolaidis, C., Allonsius, D., Song, D., Pintz, D., Livshits, D., Wyatt, D., Esiobu, D., Choudhary, D., Mahajan, D., Garcia-Olano, D., Perino, D., Hupkes, D., Lakomkin, E., AlBadawy, E., Lobanova, E., Dinan, E., Smith, E. M., Radenovic, F., Guzman, F., Zhang, F., Synnaeve, G., Lee, G., Anderson, G. L., Thattai, G., Nail, G., Mialon, G., Pang, G., Cucurell, G., Nguyen, H., Korevaar, H., Xu, H., Touvron, H., Zarov, I., Ibarra, I. A., Kloumann, I., Misra, I., Evtimov, I., Zhang, J., Copet, J., Lee, J., Geffert, J., Vranes, J., Park, J., Mahadeokar, J., Shah, J., van der Linde, J., Billock, J., Hong, J., Lee, J., Fu, J., Chi, J., Huang, J., Liu, J., Wang, J., Yu, J., Bitton, J., Spisak, J., Park, J., Rocca, J., Johnstun, J., Saxe, J., Jia, J., Alwala, K. V., Prasad, K., Upasani, K., Plawiak, K., Li, K., Heafield, K., Stone, K., El-Arini, K., Iyer, K., Malik, K., Chiu, K., Bhalla, K., Lakhotia, K., Rantala-Yeary, L., van der Maaten, L., Chen, L., Tan, L., Jenkins, L., Martin, L., Madaan, L., Malo, L., Blecher, L., Landzaat, L., de Oliveira, L., Muzzi, M., Pasupuleti, M., Singh, M., Paluri, M., Kardas, M., Tsimpoukelli, M., Oldham, M., Rita, M., Pavlova, M., Kambadur, M., Lewis, M., Si, M., Singh, M. K., Hassan, M., Goyal, N., Torabi, N., Bashlykov, N., Bogoychev, N., Chatterji, N., Zhang, N., Duchenne, O., elebi, O., Alrassy, P., Zhang, P., Li, P., Vasic, P., Weng, P., Bhargava, P., Dubal, P., Krishnan, P., Koura, P. S., Xu, P., He, Q., Dong, Q., Srinivasan, R., Ganapathy, R., Calderer, R., Cabral, R. S., Stojnic, R., Raileanu, R., Maheswari, R., Girdhar, R., Patel, R., Sauvestre, R., Polidoro, R., Sumbaly, R., Taylor, R., Silva, R., Hou, R., Wang, R., Hosseini, S., Chennabasappa, S., Singh, S., Bell, S., Kim, S. S., Edunov, S., Nie, S., Narang, S., Raparthy, S., Shen, S., Wan, S., Bhosale, S., Zhang, S., Vandenhende, S., Batra, S., Whitman, S., Sootla, S., Collot, S., Gururangan, S., Borodinsky, S., Herman, T., Fowler, T., Sheasha, T., Georgiou, T., Scialom, T., Speckbacher, T., Mihaylov, T., Xiao, T., Karn, U., Goswami, V., Gupta, V., Ramanathan, V., Kerkez, V., Gonguet, V., Do, V., Vogeti, V., Albiero, V., Petrovic, V., Chu, W., Xiong, W., Fu, W., Meers, W., Martinet, X., Wang, X., Wang, X., Tan, X. E., Xia, X., Xie, X., Jia, X., Wang, X., Goldschlag, Y., Gaur, Y., Babaei, Y., Wen, Y., Song, Y., Zhang, Y., Li, Y., Mao, Y., Coudert, Z. D., Yan, Z., Chen, Z., Papakipos, Z., Singh, A., Srivastava, A., Jain, A., Kelsey, A., Shajnfeld, A., Gangidi, A., Victoria, A., Goldstand, A., Menon, A., Sharma, A., Boesenberg, A., Baevski, A., Feinstein, A., Kallet, A., Sangani, A., Teo, A., Yunus, A., Lupu, A., Alvarado, A., Caples, A., Gu, A., Ho, A., Poulton, A., Ryan, A., Ramchandani, A., Dong, A., Franco, A., Goyal, A., Saraf, A., Chowdhury, A., Gabriel, A., Bharambe, A., Eisenman, A., Yazdan, A., James, B., Maurer, B., Leonhardi, B., Huang, B., Loyd, B., Paola, B. D., Paranjape, B., Liu, B., Wu, B., Ni, B., Hancock, B., Wasti, B., Spence, B., Stojkovic, B., Gamido, B., Montalvo, B., Parker, C., Burton, C., Mejia, C., Liu, C., ALIGNVLM: Bridging Vision and Language Latent Spaces for Multimodal Understanding Wang, C., Kim, C., Zhou, C., Hu, C., Chu, C.-H., Cai, C., Tindal, C., Feichtenhofer, C., Gao, C., Civin, D., Beaty, D., Kreymer, D., Li, D., Adkins, D., Xu, D., Testuggine, D., David, D., Parikh, D., Liskovich, D., Foss, D., Wang, D., Le, D., Holland, D., Dowling, E., Jamil, E., Montgomery, E., Presani, E., Hahn, E., Wood, E., Le, E.-T., Brinkman, E., Arcaute, E., Dunbar, E., Smothers, E., Sun, F., Kreuk, F., Tian, F., Kokkinos, F., Ozgenel, F., Caggioni, F., Kanayet, F., Seide, F., Florez, G. M., Schwarz, G., Badeer, G., Swee, G., Halpern, G., Herman, G., Sizov, G., Guangyi, Zhang, Lakshminarayanan, G., Inan, H., Shojanazeri, H., Zou, H., Wang, H., Zha, H., Habeeb, H., Rudolph, H., Suk, H., Aspegren, H., Goldman, H., Zhan, H., Damlaj, I., Molybog, I., Tufanov, I., Leontiadis, I., Veliche, I.-E., Gat, I., Weissman, J., Geboski, J., Kohli, J., Lam, J., Asher, J., Gaya, J.-B., Marcus, J., Tang, J., Chan, J., Zhen, J., Reizenstein, J., Teboul, J., Zhong, J., Jin, J., Yang, J., Cummings, J., Carvill, J., Shepard, J., McPhie, J., Torres, J., Ginsburg, J., Wang, J., Wu, K., U, K. H., Saxena, K., Khandelwal, K., Zand, K., Matosich, K., Veeraraghavan, K., Michelena, K., Li, K., Jagadeesh, K., Huang, K., Chawla, K., Huang, K., Chen, L., Garg, L., A, L., Silva, L., Bell, L., Zhang, L., Guo, L., Yu, L., Moshkovich, L., Wehrstedt, L., Khabsa, M., Avalani, M., Bhatt, M., Mankus, M., Hasson, M., Lennie, M., Reso, M., Groshev, M., Naumov, M., Lathi, M., Keneally, M., Liu, M., Seltzer, M. L., Valko, M., Restrepo, M., Patel, M., Vyatskov, M., Samvelyan, M., Clark, M., Macey, M., Wang, M., Hermoso, M. J., Metanat, M., Rastegari, M., Bansal, M., Santhanam, N., Parks, N., White, N., Bawa, N., Singhal, N., Egebo, N., Usunier, N., Mehta, N., Laptev, N. P., Dong, N., Cheng, N., Chernoguz, O., Hart, O., Salpekar, O., Kalinli, O., Kent, P., Parekh, P., Saab, P., Balaji, P., Rittner, P., Bontrager, P., Roux, P., Dollar, P., Zvyagina, P., Ratanchandani, P., Yuvraj, P., Liang, Q., Alao, R., Rodriguez, R., Ayub, R., Murthy, R., Nayani, R., Mitra, R., Parthasarathy, R., Li, R., Hogan, R., Battey, R., Wang, R., Howes, R., Rinott, R., Mehta, S., Siby, S., Bondu, S. J., Datta, S., Chugh, S., Hunt, S., Dhillon, S., Sidorov, S., Pan, S., Mahajan, S., Verma, S., Yamamoto, S., Ramaswamy, S., Lindsay, S., Lindsay, S., Feng, S., Lin, S., Zha, S. C., Patil, S., Shankar, S., Zhang, S., Zhang, S., Wang, S., Agarwal, S., Sajuyigbe, S., Chintala, S., Max, S., Chen, S., Kehoe, S., Satterfield, S., Govindaprasad, S., Gupta, S., Deng, S., Cho, S., Virk, S., Subramanian, S., Choudhury, S., Goldman, S., Remez, T., Glaser, T., Best, T., Koehler, T., Robinson, T., Li, T., Zhang, T., Matthews, T., Chou, T., Shaked, T., Vontimitta, V., Ajayi, V., Montanez, V., Mohan, V., Kumar, V. S., Mangla, V., Ionescu, V., Poenaru, V., Mihailescu, V. T., Ivanov, V., Li, W., Wang, W., Jiang, W., Bouaziz, W., Constable, W., Tang, X., Wu, X., Wang, X., Wu, X., Gao, X., Kleinman, Y., Chen, Y., Hu, Y., Jia, Y., Qi, Y., Li, Y., Zhang, Y., Zhang, Y., Adi, Y., Nam, Y., Yu, Wang, Zhao, Y., Hao, Y., Qian, Y., Li, Y., He, Y., Rait, Z., DeVito, Z., Rosnbrick, Z., Wen, Z., Yang, Z., Zhao, Z., and Ma, Z. The llama 3 herd of models, 2024. URL https://arxiv.org/abs/2407.21783. Hu, A., Xu, H., Ye, J., Yan, M., Zhang, L., Zhang, B., Li, C., Zhang, J., Jin, Q., Huang, F., and Zhou, J. mplugdocowl 1.5: Unified structure learning for ocr-free document understanding, 2024. URL https://arxiv. org/abs/2403.12895. Jaume, G., Ekenel, H. K., and Thiran, J.-P. Funsd: dataset for form understanding in noisy scanned documents, 2019. URL https://arxiv.org/abs/ 1905.13538. Kim, G., Hong, T., Yim, M., Nam, J., Park, J., Yim, J., Hwang, W., Yun, S., Han, D., and Park, S. Ocrfree document understanding transformer, 2022. URL https://arxiv.org/abs/2111.15664. Kim, Y., Yim, M., and Song, K. Y. Tablevqa-bench: visual question answering benchmark on multiple table domains. arXiv preprint arXiv:2404.19205, 2024. Laurencon, H., Tronchon, L., Cord, M., and Sanh, V. What matters when building vision-language models?, 2024. URL https://arxiv.org/abs/2405.02246. Lee, K., Joshi, M., Turc, I., Hu, H., Liu, F., Eisenschlos, J., Khandelwal, U., Shaw, P., Chang, M.-W., and Toutanova, K. Pix2struct: Screenshot parsing as pretraining for visual language understanding, 2023. URL https://arxiv. org/abs/2210.03347. Li, B., Zhang, Y., Guo, D., Zhang, R., Li, F., Zhang, H., Zhang, K., Zhang, P., Li, Y., Liu, Z., and Li, C. Llavaonevision: Easy visual task transfer, 2024. URL https: //arxiv.org/abs/2408.03326. Li, J., Li, D., Savarese, S., and Hoi, S. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models, 2023. URL https://arxiv.org/abs/2301.12597. Liu, H., Li, C., Li, Y., and Lee, Y. J. Improved baselines with visual instruction tuning, 2023a. Liu, H., Li, C., Wu, Q., and Lee, Y. J. Visual instruction tuning, 2023b. Liu, H., Li, C., Li, Y., Li, B., Zhang, Y., Shen, S., Improved reasoning, Llava-next: URL and Lee, Y. J. ocr, and world knowledge, January 2024. https://llava-vl.github.io/blog/ 2024-01-30-llava-next/. 11 ALIGNVLM: Bridging Vision and Language Latent Spaces for Multimodal Understanding Lu, S., Li, Y., Chen, Q.-G., Xu, Z., Luo, W., Zhang, K., and Ye, H.-J. Ovis: Structural embedding alignment for multimodal large language model, 2024. URL https: //arxiv.org/abs/2405.20797. Masry, A., Long, D. X., Tan, J. Q., Joty, S., and Hoque, E. Chartqa: benchmark for question answering about charts with visual and logical reasoning. arXiv preprint arXiv:2203.10244, 2022. Mathew, M., Bagal, V., Tito, R. P., Karatzas, D., Valveny, Infographicvqa, 2021a. URL E., and Jawahar, C. V. https://arxiv.org/abs/2104.12756. Mathew, M., Karatzas, D., and Jawahar, C. V. Docvqa: dataset for vqa on document images, 2021b. URL https://arxiv.org/abs/2007.00398. OpenAI, Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., Avila, R., Babuschkin, I., Balaji, S., Balcom, V., Baltescu, P., Bao, H., Bavarian, M., Belgum, J., Bello, I., et al. Gpt-4 technical report. arXiv preprint arXiv: 2303.08774, 2023. Park, S., Shin, S., Lee, B., Lee, J., Surh, J., Seo, M., and Lee, H. Cord: consolidated receipt dataset for postocr parsing. Document Intelligence Workshop at Neural Information Processing Systems, 2019. Pasupat, P. and Liang, P. Compositional semantic parsing on semi-structured tables. In Annual Meeting of the Association for Computational Linguistics, 2015. Qwen, :, Yang, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Li, C., Liu, D., Huang, F., Wei, H., Lin, H., Yang, J., Tu, J., Zhang, J., Yang, J., Yang, J., Zhou, J., Lin, J., Dang, K., Lu, K., Bao, K., Yang, K., Yu, L., Li, M., Xue, M., Zhang, P., Zhu, Q., Men, R., Lin, R., Li, T., Tang, T., Xia, T., Ren, X., Ren, X., Fan, Y., Su, Y., Zhang, Y., Wan, Y., Liu, Y., Cui, Z., Zhang, Z., and Qiu, Z. Qwen2.5 technical report, 2025. URL https: //arxiv.org/abs/2412.15115. Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., and Sutskever, I. Learning transferable visual models from natural language supervision, 2021. URL https://arxiv.org/abs/2103.00020. Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring the limits of transfer learning with unified text-totext transformer, 2023. URL https://arxiv.org/ abs/1910.10683. Rodriguez, J., Jian, X., Panigrahi, S. S., Zhang, T., Feizi, A., Puri, A., Kalkunte, A., Savard, F., Masry, A., Nayak, S., Awal, R., Massoud, M., Abaskohi, A., Li, Z., Wang, S., Noel, P.-A., Richter, M. L., Vadacchino, S., Agarwal, S., Biswas, S., Shanian, S., Zhang, Y., Bolger, N., MacDonald, K., Fauvel, S., Tejaswi, S., Sunkara, S., Monteiro, J., Dvijotham, K. D., Scholak, T., Chapados, N., Kharagani, S., Hughes, S., Ozsu, M., Reddy, S., Pedersoli, M., Bengio, Y., Pal, C., Laradji, I., Gella, S., Taslakian, P., Vazquez, D., and Rajeswar, S. Bigdocs: An open and permissively-licensed dataset for training multimodal models on document and code tasks, 2024a. URL https://arxiv.org/abs/2412.04626. Rodriguez, J. A., Vazquez, D., Laradji, I., Pedersoli, M., and Rodriguez, P. Ocr-vqgan: Taming text-within-image generation, 2022. URL https://arxiv.org/abs/ 2210.11248. Rodriguez, J. A., Puri, A., Agarwal, S., Laradji, I. H., Rodriguez, P., Rajeswar, S., Vazquez, D., Pal, C., and Pedersoli, M. Starvector: Generating scalable vector graphics code from images and text, 2024b. URL https://arxiv.org/abs/2312.11556. Singh, A., Natarajan, V., Shah, M., Jiang, Y., Chen, X., Batra, D., Parikh, D., and Rohrbach, M. Towards vqa In IEEE Conference Computer models that can read. Vision Pattern Recognition, 2019. Stanisławek, T., Gralinski, F., Wroblewska, A., Lipinski, D., Kaliska, A., Rosalska, P., Topolski, B., and Biecek, P. Kleister: key information extraction datasets involving long documents with complex layouts. In International Conference on Document Analysis and Recognition, 2021. Svetlichnaya, S. Deepform: Understand structured documents at scale, 2020. Team, G. Gemini: family of highly capable multimodal models, 2024. URL https://arxiv.org/abs/ 2312.11805. Vogus, C. and Llansoe, E. Making transparency meaningful: framework for policymakers. Center for Democracy and Technology, 2021. Wang, D., Raman, N., Sibue, M., Ma, Z., Babkin, P., Kaur, S., Pei, Y., Nourbakhsh, A., and Liu, X. Docllm: layout-aware generative language model for multimodal document understanding, 2023a. URL https: //arxiv.org/abs/2401.00908. Wang, P., Bai, S., Tan, S., Wang, S., Fan, Z., Bai, J., Chen, K., Liu, X., Wang, J., Ge, W., Fan, Y., Dang, K., Du, M., Ren, X., Men, R., Liu, D., Zhou, C., Zhou, J., and 12 ALIGNVLM: Bridging Vision and Language Latent Spaces for Multimodal Understanding Lin, J. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution, 2024. URL https://arxiv.org/abs/2409.12191. Wang, W., Lv, Q., Yu, W., Hong, W., Qi, J., Wang, Y., Ji, J., Yang, Z., Zhao, L., Song, X., et al. Cogvlm: Visual expert for pretrained language models. arXiv preprint arXiv:2311.03079, 2023b. Wu, C., Chen, X., Wu, Z., Ma, Y., Liu, X., Pan, Z., Liu, W., Xie, Z., Yu, X., Ruan, C., and Luo, P. Janus: Decoupling visual encoding for unified multimodal understanding and generation, 2024a. URL https://arxiv.org/ abs/2410.13848. Wu, Z., Chen, X., Pan, Z., Liu, X., Liu, W., Dai, D., Gao, H., Ma, Y., Wu, C., Wang, B., Xie, Z., Wu, Y., Hu, K., Wang, J., Sun, Y., Li, Y., Piao, Y., Guan, K., Liu, A., Xie, X., You, Y., Dong, K., Yu, X., Zhang, H., Zhao, L., Wang, Y., and Ruan, C. Deepseek-vl2: Mixture-ofexperts vision-language models for advanced multimodal understanding, 2024b. URL https://arxiv.org/ abs/2412.10302. Xu, R., Yao, Y., Guo, Z., Cui, J., Ni, Z., Ge, C., Chua, T.-S., Liu, Z., Sun, M., and Huang, G. Llava-uhd: an lmm perceiving any aspect ratio and high-resolution images. European Conference on Computer Vision, 2024. doi: 10.48550/arXiv.2403.11703. Zhai, X., Mustafa, B., Kolesnikov, A., and Beyer, L. Sigmoid loss for language image pre-training, 2023. URL https://arxiv.org/abs/2303.15343. Zhang, T., Wang, S., Li, L., Zhang, G., Taslakian, P., Rajeswar, S., Fu, J., Liu, B., and Bengio, Y. Vcr: Visual caption restoration. arXiv preprint arXiv: 2406.06462, 2024. Zhao, Y., Huang, J., Hu, J., Wang, X., Mao, Y., Zhang, D., Jiang, Z., Wu, Z., Ai, B., Wang, A., Zhou, W., and Chen, Y. Swift:a scalable lightweight infrastructure for fine-tuning, 2024. URL https://arxiv.org/abs/ 2408.05517. ALIGNVLM: Bridging Vision and Language Latent Spaces for Multimodal Understanding A. Appendix A.1. Experimental Setup We provide detailed hyperparameters of our experiments in Table 4. Table 4: Detailed hyperparameters for each training stage across different LLM backbones. LLM Backbone Llama 3.2-1B Llama 3.2-3B Llama 3.1-8B Stage-1 Stage-2 StageStage-1 Stage-2 Stage-3 Stage-1 Stage-2 StageTrainable Parameters Batch Size Text Max Length Epochs Learning Rate Full Model 512 1024 1 1 105 Full Model LLM & ALIGN 512 2048 1 5 105 512 2048 5 5 105 Full Model 512 1024 1 1 10 Full Model LLM & ALIGN 256 2048 1 5 105 256 2048 5 5 105 Full Model 512 1024 1 1 105 Full Model LLM & ALIGN 256 2048 1 1 10 256 2048 5 1 105 A.2. Vision-to-Text In this experiment, we analyze how ALIGN maps visual features to the LLMs text tokens. To do so, we manually curate small dataset of image crops, each containing either single word or small set of visual text elements. Unlike the processing of high-resolution images described earlier (Section 3.1), these image crops are not divided into tiles. Instead, the backbone image encoder processes each crop as single tile, producing 14 14 features from the input image. The resulting features pass through the Softmax operation (Equation 1), yielding probability distribution over the LLMs text tokens for each feature (region). We examine the decoded text tokens from specific image regions to better understand how visual features are mapped to textual representations. As shown in Figure 6, white regions in the images tend to assign higher probabilities to punctuation tokens, such as commas or periods. Since punctuation structures written text, while white space separates document components like paragraphs, tables, and sections, ALIGN appears to leverage these implicit patterns to align visual structures with semantically meaningful representations in the LLMs embedding space. Figure 6: Mapping Visual-to-Text tokens. The left column shows the visual input to the model. In contrast, the right column visualizes the decoded tokens on 1414 grid, displaying the top k=2 tokens corresponding to the most likely LLM tokens predicted for the respective visual feature in each cell. ALIGNVLM: Bridging Vision and Language Latent Spaces for Multimodal Understanding A.3. Case Studies In this section, we provide case studies for the experiments in Section 5.1. Specifically, we provide examples of our Llama-3.2-3B-ALIGN, and its counterpart model with alternative connectors Llama-3.2-3B-MLP and Llama-3.2-3B-Ovis on three different datasets: KLC (Stanisławek et al., 2021), DocVQA (Mathew et al., 2021b), and TextVQA (Singh et al., 2019). The examples are shown in Figure 7, 8, and 9. 15 ALIGNVLM: Bridging Vision and Language Latent Spaces for Multimodal Understanding Question: What is the value for the charity Question: What is the value for the adGT: MLP: Ovis: ALIGN: name? (Ardingly College Ltd.) (Ardington College Ltd.) (Ardington College Ltd.) (Ardingly College Ltd.) GT: MLP: Ovis: ALIGN: dress postcode? (SW2 2QP) (SW22 0PQ) (SW2 2OP) (SW2 2QP) (a) Positive Example # (b) Positive Example #2 Question: What is the value for the charQuestion: What is the value for the post GT: MLP: Ovis: ALIGN: ity name? (Human Appeal) (Humanitarian Agenda) (Human Appeal) (Human Rightsappeal) GT: MLP: Ovis: ALIGN: town address? (Bishops Stortford) (Stortford) (Bishops Stortford) (Stortford) (c) Negative Example #1 (d) Negative Example #2 Figure 7: Case Study for Connector Comparison on the KLC dataset (Stanisławek et al., 2021). We show four qualitative examples (including two correct and two incorrect examples) comparing Llama-3.2-3B-ALIGN to the same architecture with different connectors, Llama-3.2-3B-MLP and Llama-3.2-3B-Ovis. GT denotes the ground truth. 16 ALIGNVLM: Bridging Vision and Language Latent Spaces for Multimodal Understanding Question: What does the afternoon session Question: What levels does the second table indiGT: MLP: Ovis: ALIGN: begin on June 29? (1:00) (2:45) (3:30) (1:00) GT: MLP: Ovis: ALIGN: cate? (hematocrit data - Massachusetts) (SATISFACTORY) (Females) (hematocrit data - Massachusetts) (a) Positive Example #1 (b) Positive Example #2 Question: What type of policy is described Question: What was the diet fed to the #1 GT: MLP: Ovis: ALIGN: in this document? (Policy on Document Control) (Policy on Document Control) (General Provisions) (Document Control) GT: MLP: Ovis: ALIGN: group? (basal diet) (basel diet) (Whole blood) (control diet) (c) Negative Example #1 (d) Negative Example #2 Figure 8: Case Study for Connector Comparison on the DocVQA dataset (Mathew et al., 2021b). We show four qualitative examples (including two correct and two incorrect examples) comparing Llama-3.2-3B-ALIGN to the same architecture with different connectors, Llama-3.2-3B-MLP and Llama-3.2-3B-Ovis. GT denotes the ground truth. ALIGNVLM: Bridging Vision and Language Latent Spaces for Multimodal Understanding Question: What greeting is written on the letter? GT: MLP: Ovis: ALIGN: (good bye) (good) (good buy) (good bye) Question: What indoor temperature is shown? GT: MLP: Ovis: ALIGN: (68.4) (68 F) (40.0) (68.4) (a) Positive Example # (b) Positive Example #2 Question: What type of club is advertised? GT: MLP: Ovis: ALIGN: (health club) (topnote health club) (health club) (professional passionate personal) Question: What credit card is this? (hadiah plus) GT: (hadiah plus) MLP: (american big loyalty program) Ovis: (hadia plus) ALIGN: (c) Negative Example #1 (d) Negative Example # Figure 9: Case Study for Connector Comparison on the TextVQA dataset (Singh et al., 2019). We show four qualitative examples (including two correct and two incorrect examples) comparing Llama-3.2-3B-ALIGN to the same architecture with different connectors, Llama-3.2-3B-MLP and Llama-3.2-3B-Ovis. GT denotes the ground truth."
        }
    ],
    "affiliations": [
        "CIFAR AI Chair",
        "Ecole de Technologie Superieure",
        "McGill University",
        "Mila",
        "Polytechnique Montreal",
        "ServiceNow",
        "Universite de Montreal",
        "University of British Columbia",
        "University of Waterloo",
        "York University"
    ]
}