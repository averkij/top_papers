{
    "paper_title": "Flow Equivariant Recurrent Neural Networks",
    "authors": [
        "T. Anderson Keller"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Data arrives at our senses as a continuous stream, smoothly transforming from one instant to the next. These smooth transformations can be viewed as continuous symmetries of the environment that we inhabit, defining equivalence relations between stimuli over time. In machine learning, neural network architectures that respect symmetries of their data are called equivariant and have provable benefits in terms of generalization ability and sample efficiency. To date, however, equivariance has been considered only for static transformations and feed-forward networks, limiting its applicability to sequence models, such as recurrent neural networks (RNNs), and corresponding time-parameterized sequence transformations. In this work, we extend equivariant network theory to this regime of `flows' -- one-parameter Lie subgroups capturing natural transformations over time, such as visual motion. We begin by showing that standard RNNs are generally not flow equivariant: their hidden states fail to transform in a geometrically structured manner for moving stimuli. We then show how flow equivariance can be introduced, and demonstrate that these models significantly outperform their non-equivariant counterparts in terms of training speed, length generalization, and velocity generalization, on both next step prediction and sequence classification. We present this work as a first step towards building sequence models that respect the time-parameterized symmetries which govern the world around us."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 2 ] . [ 1 3 9 7 4 1 . 7 0 5 2 : r a"
        },
        {
            "title": "Flow Equivariant Recurrent Neural Networks",
            "content": "T. Anderson Keller The Kempner Instutite for the Study of Natural and Artificial Intelligence Harvard University, Cambridge, MA 15213 t.anderson.keller@gmail.com"
        },
        {
            "title": "Abstract",
            "content": "Data arrives at our senses as continuous stream, smoothly transforming from one instant to the next. These smooth transformations can be viewed as continuous symmetries of the environment that we inhabit, defining equivalence relations between stimuli over time. In machine learning, neural network architectures that respect symmetries of their data are called equivariant and have provable benefits in terms of generalization ability and sample efficiency. To date, however, equivariance has been considered only for static transformations and feed-forward networks, limiting its applicability to sequence models, such as recurrent neural networks (RNNs), and corresponding time-parameterized sequence transformations. In this work, we extend equivariant network theory to this regime of flows one-parameter Lie subgroups capturing natural transformations over time, such as visual motion. We begin by showing that standard RNNs are generally not flow equivariant: their hidden states fail to transform in geometrically structured manner for moving stimuli. We then show how flow equivariance can be introduced, and demonstrate that these models significantly outperform their non-equivariant counterparts in terms of training speed, length generalization, and velocity generalization, on both next step prediction and sequence classification. We present this work as first step towards building sequence models that respect the time-parameterized symmetries which govern the world around us."
        },
        {
            "title": "Introduction",
            "content": "Every moment, the world around us shifts continuously. As you walk down the street, rooftops glide past, trees sway in the breeze, and cars drift in and out of view. These smooth, structured transformations or flows are not arbitrary, but instead are underpinned by precise set of rules: an algebra which ensures self-consistency across space and time. Mathematically, these transformations which leave the essence of the world around you unchanged are called symmetries, and, in sense, these symmetries over time can be seen to weave together the instantaneous into the continuous. In machine learning, symmetry has emerged as powerful handle from which we can grapple with the challenges of generalization and data efficiency. Models which encode symmetries are called equivariant, and their steady evolution has delivered significant performance improvements for data with known structure. Simultaneously, with the success of large language models, sequence modeling has become prominent learning paradigm throughout machine learning sub-fields. We therefore ask: can these everyday smooth, continuous motion symmetries be encoded in our modern sequence models and leveraged for improved performance? Surprisingly, to date, the study of equivariance has largely been limited to temporally static transformations, narrowly missing the opportunity to capture the continuous time-parameterized symmetry transformations that dominate our natural experience. In this work, we consider carefully structured subset of natural motion transformations called flows: formally, one-parameter Lie subgroups. We introduce formal notion of what it means for sequence model to be flow equivariant and proceed to demonstrate that existing sequence models are indeed not flow equivariant, even when all sub-components of these models are individually equivariant with respect to the full static Lie group symmetry. Preprint. Under review. Motivated by this gap in the literature, and the potential advantages of integrating one of the most ubiquitous natural symmetries into sequence models, in this work, we study how we might build Recurrent Neural Networks (RNNs) which are explicitly equivariant with respect to time-parameterized symmetry transformations. At the highest level, we demonstrate that we are able to do so in manner very similar to Group equivariant Convolutional Neural Networks (G-CNNs): by lifting the hidden state of our RNN to dimension of flows, indexed by the generating vector fields ν. In slices, our model resembles bank of RNNs (one for each flow), but weight sharing across ν turns that bank into single group-equivariant dynamical system which guarantees generalization to unseen flows. In the following, we will review the principle of equivariance in neural network architectures (2). We will formally introduce the notion of Flow Equivariance (3), and demonstrate that existing recurrent neural network architectures are indeed not flow equivariant as we would desire. We will then introduce Flow Equivariant Recurrent Neural Networks (FERNNs) in 4; and demonstrate empirically that FERNNs achieve zero-shot generalization to new flows at test time, improved length generalization, and improved performance on datasets which possess strong flow symmetries in 5. In our discussion, (6), we briefly outline related work on equivariance with respect to motion, and highlight the differences with our proposed approach, but leave thorough review of related work to D. We conclude with the limitations of our proposed framework and promising future directions."
        },
        {
            "title": "2 Equivariant Neural Networks",
            "content": "Informally, network is said to be equivariant with respect to set of structured transformations if every transformation of the input has corresponding structured transformation of the output. The fact that this set of transformations is structured is important. Typically, this structure is the abstract algebraic structure known as group G, meaning that any two transformations gi, gj will combine through binary operation () to form another known transformation gi gj = gk (closure), and the chunking of transformations is arbitrary, i.e. (gi gj) gk = gi (gj gk) (associativity). As we will see later, we will exploit both these properties when designing equivariant models. At an intuitive level, an equivariant neural network Φ is network whose activations live on the group. This means that when Φ processes transformed input, by associativity, this is equivalent to transforming the activations, and by closure, this transformation just yields another set of activations on the group i.e., structured transformation of the output. Formally, neural network layer Φ is equivariant with respect to symmetry group if the action of the group commutes with the application of the layer. Following the group equivariant CNN literature [Cohen and Welling, 2016a], we will define this layer as map between function spaces Φ : FK(X) FK(Y ), where FK(X) := {f : RK} denotes the set of signals defined on the abstract domain with channels. We will use the notation Φ[f ] to denote the output of the model (in FK(Y ) space) for an input signal , and Φ[f ](y) RK to similarly denote the value of that output function at location y. For FK(X), we can then define the action of the group element to be the left action with representation πX : Aut(X) on the domain (where Aut(X) is the automorphism group of all bijections X), and linear representation ρX : GL(K) on the channels: (g )(x) := ρX (g)f (πX (g1)x). Throughout the rest of this paper, we will only consider scalar feature types, meaning that the channel representation is trivial, ρX (g) = IK, and we will write the action of the input representation as := πX (g)x for brevity. Succinctly, this yields: (g )(x) := (g1 x), G, X. (1) Concretely, if is 2D image, = Z2 the set of pixel coordinates, and = (Z2, +) is the 2D translation group, then the translation action is: (g )(x) = (x g), leftward shift of pixel coordinates. In the output domain, the action of πY , ρY depends on how we construct the equivariant map, but similarly has the form: (g Φ)(y) := ρY (g)Φ[f ](πY (g1)y). In this work, we will again only consider scalar features, meaning ρY (g) = IK. Furthermore, as we will show next, for group equivariant convolutional layers, πY (g1) acts the same as it does on the input, i.e. (g Φ[f ])(y) = Φ[f ](g1 y). Finally, the equivariance condition is then written simply as: Φ[f ] = Φ[g ] G, FK(X) (2) We can see this is the formalization of our intuitive definition: the action of the group on the input can instead be pulled through to act on the lifted group indices of the networks output Φ[f ]. 2 Group Convolution. simple example of such an equivariant map Φ is the group convolution [Cohen and Welling, 2016a]: generalization of the standard convolutional neural network (CNN) layer from the translation group to other arbitrary discrete groups. To accomplish this, we define convolutional kernels to be functions on the group G, i.e. : RK, and perform lifting convolution at the first layer of the network to lift from the input space to the group space. Mathematically, we write the lifting convolution (ˆG) and subsequent group convolutions (G) as: [f ˆG i](g) = (cid:88) (cid:88) xZ2 k=1 fk(x)W k(g1 x) & [f i](g) = (cid:88) (cid:88) hG k=1 fk(h)W k(g1 h) (3) We see that the output of the layer is now defined over the group G, and we can verify that these layers are indeed equivariant with respect to Equation 2 (see A.1). The important property being that [(g ) i] = [f i], that is, the convolution commutes with the group action. Group Equivariant Recurrent Neural Networks. In this work we define recurrent neural network as map between space-time function spaces : FK(X, Z) FK(Y, Z), where FK(X, Z) := {f : RK} denotes the set of time-varying K-dimensional signals defined over space and (discretized) time Z. In the remainder of this paper we will use discrete time domains for our space-time signals, but we note that in order to accommodate continuous time sequence models, time may be made continuous (t R) by changing domain of h, with only minor changes to the analysis. We can then define group-equivariant RNN (G-RNN) as simple RNN with group convolutions in place of both the usual linear recurrence and input maps. Explicitly: t+1[ft] = σ(cid:0)ht[f<t] i + ft ˆG i(cid:1), hi (4) where : RK, and : RK are the group convolutional kernels for the hiddento-hidden mapping and input-to-hidden mapping respectively (for output channel i), and h0 is initialized to be invariant to the group action, i.e. h0(g) = h0(g) g, G. The notation ht[f<t] implies that ht is causal function of the input signal prior to time t. The non-linearity σ is assumed group-equivariant as well (pointwise). In A.2, we prove by induction that this network is indeed equivariant with respect to action of the group on the input signal , meaning t[ft] G, Z, FK(X, Z). Intuitively, this is because both the inputhi t[g ft] = hi to-hidden mapping and the recurrent mapping are equivariant, meaning any constant transformation of the input can be pulled through both mappings (and the nonlinearity) yielding corresponding transformation of the output (hidden state). Concretely, this implies that recurrent neural network with convolutional recurrent connections, and convolutional encoders is indeed still equivariant with respect to static translation of the entire input sequence. However, as we will show next, this does not imply that the RNN will be equivariant with respect to time-parameterized translation, i.e. motion."
        },
        {
            "title": "3 Flow Equivariance",
            "content": "Like the pathline traced out by droplet of dye carried along by moving fluid, the flow ψt(ν) generated by vector field ν gives the position of any point after time R. More formally, flow is one-parameter subgroup of Lie group G, generated by an element ν of the corresponding Lie algebra of [Hall, 2015]. We denote generator by ν g, and its time-t flow ψt(ν) : by ψt(ν) = exp(cid:0)t ν(cid:1) G, where exp is the exponential map. By construction, the flow admits an identity at = 0, ψ0(ν) = e, and composition property: ψs(ν) ψt(ν) = ψs+t(ν), such that flowing for time and then is the same as flowing for time + t. Intuitively, the flow ψt(ν) is the group element that transports point from the identity to where it will be after time when moving with instantaneous velocity ν. Colloquially, we therefore call ψ time-parameterized symmetry transformation. For given FK(X, Z) and generator ν g, we define flow acting on as (ψ(ν) )t(x) := ft(ψt(ν)1 x). (5) and identically for flow acting on function in the output space FK(Y, Z): (ψ(ν) Φ)t(y) := Φt(ψt(ν)1 y). Throughout this work we will use the notation (ψ(ν) ) and (ψ(ν) Φ), leaving time implicit, to denote the flow acting on the entire time domain of the function simultaneously. We can then define flow equivariance as the property of sequence model such that its sequential output commutes with such time-parameterized transformations of the input: Definition 3.1 (Flow Equivariance). For set of generators g, sequence model Φ : FK(X, Z) FK(Y, Z) is equivariant with respect to the flows generated by iff ψ(ν) Φ[f ] = Φ[ψ(ν) ] ν V, FK(X, Z). (6) 3 Flow Equivariance of Sequence Models. Our proposed Definition 3.1 has noteworthy special case which captures much of the literature to date on equivariance in sequence models. Specifically: Remark (Frame-wise feed-forward equivariant models are trivially flow equivariant.). Let Φ[f ] = [ϕ(f0), ϕ(f1) . . . ϕ(fT )] be frame-wise applied equivariant map, with ϕ : FK(X) FK(Y ) such that: ϕ(ψt(ν) ft) = ψt(ν) ϕ(ft) Z, ν . Then Φ is flow equivariant by Definition 3.1. See A.3 for the short proof. Intuitively, one sees this very clearly for CNNs applied to video sequences frame-wise: since the CNN commutes with the transformation of each frame, and the transformation on the sequence distributes amongst the frames (by linearity of the action), the act of transforming the input frame-by-frame is equivalent to transforming the output frame-by-frame. Since this is achieved through complete lack of sequence modeling in Φ, we call it trivial flow equivariance. Equivalently, one can see that the G-RNN defined in Equation 4 with = 0 reduces to such feed-forward equivariant map (since recurrence is removed), and is thus also trivially flow equivariant. To achieve more sophisticated flow equivariance in the context of sequence processing, we must use sequence models with non-zero recurrence. Notably, while in 2 we show that groupequivariant RNNs do commute with static transformations, we show below that such models generally do not commute with time-parameterized transformations, meaning they are not flow equivariant. Theorem 3.1 (G-RNNs are not flow equivariant). G-RNN as defined in Equation 4, with non-zero W, is not flow equivariant according to Definition 3.1, except in the degenerate flow-invariant case. Remark. (Degenerate flow invariance) If all kernels & are constant on G, then the outputs of Equation 4 are spatially uniform. Since the flow action permutes only the G-index, these constants are fixed points, making the G-RNN flow invariant. Proof Sketch. (Theorem 3.1) As visualized in Figure 1, let σ = Id, and [ U], [ W] be the identity, such that the hidden state evolves simply as sum of the past state and the new input: ht+1[ft] = ht[f<t] + ft. If the input is static bump centered at 0, ht+1[ft] will be an evergrowing bump, while ht+1[ψ(ν) ft] will be train of bumps, since, at each time step, the hidden state is lagging behind the input which has shifted by ψ1(ν). Thus corresponding shift of the growing bump will never equal the train of bumps, i.e. (ψ(ν) h[ft])t+1 = ht+1[ψ(ν) ft]. One can verify that this counterexample applies to most other existing sequence models, including state space models, gated recurrent neural networks, neural ODEs, and even transIn the following section, formers. we show how to build recurrent neural network which is equivariant with respect to these transformations, and argue that this same construction can likely be carried over to other sequence models as well. Figure 1: G-RNNs are not generally flow equivariant. We show this by simple counterexample with: ht+1 = ht + ft. See A.4 for the full proof."
        },
        {
            "title": "4 Flow Equivariant Recurrent Neural Networks",
            "content": "In this section we introduce our Flow Equivariant Recurrent Neural Network (FERNN) and prove the equivariance condition (Definition 3.1) holds. Inspired by the above analysis of the failure of G-RNNs to be flow equivariant, we note that simple condition by which equivariance can be achieved is if computation is performed in the reference frame of the signal. For the case of static symmetry groups, we can interpret the group convolution as performing the same operation (dot product of filter and signal) in all possible statically transformed reference frames (group elements). In the case of time-parameterized symmetry groups, we can then assume that we desire to be performing the same recurrent update in all possible dynamically transformed (moving) reference frames. To achieve this, we see that, analogous to the g-lift in the G-CNN, we must now lift our hidden state and input to the flow elements ν . To do so, we define the action of the instantaneous group elements ψt(ν) on the lifted space to be the same left action as before, only on the dimension. Explicitly, for ht(ν, g) : RK , we define ψt(ν) ht(ν, g) := ht(ν, ψt(ν)1 g). 4 Input Flow-Lifting Convolution. To lift the input to the product group G, we will re-use the original lifting group convolution from Equation 3 exactly, and simply perform trivial lift to the additional dimension by copying the output of the convolution for each value of ν. Explicitly: [f ˆV i](ν, g) = (cid:88) (cid:88) xX k=1 fk(x)U k(g1 x) (7) In A.5 we prove that this is equivariant with respect to the group action, yielding the following: ψt(ν) [ft ˆV i](ν, g) = [(ψt(ν) ft) ˆV i](ν, g) := [ft ˆV i](ν, ψt(ν)1 g) (8) We note that it is also possible to perform non-trivial lift, where is defined on the lifted space G, as in G-CNNs. This changes the later recurrence relation (Eqn. 11) necessary to achieve flow equivariance, but is largely design choice. In the experiments and proofs we stick with the trivial lift, but describe the model with non-trivial lift in C.1. Flow Convolution. The group convolution can then be extended to our lifted hidden state as: (cid:2)h G i(cid:3)(ν, g) = (cid:88) (cid:88) (cid:88) γV mG k=1 hk(γ, m) k (cid:0)γ ν, g1 m(cid:1). (9) We see that for the convolution over , we have assumed that is an additive (abelian) discrete group, such that the combination (ν) γ = γ ν. This is equivalent to asserting that the action of the flow generator on the lifted hidden state is given by the shift: (ˆν ht)(ν, g) = ht(ν ˆν, g). If is not abelian, the general Baker-Campbell-Hausdorff product ((ν) γ) should be used instead [Hall, 2015]. Similarly, if is not discrete, one should perform an integral with respect to the Haar measure. In all experiments we use abelian , and discrete grid for , so this is exact. In A.6 we prove this is equivariant with respect to the action of individual flow elements. Explicitly: (cid:2)(ψt(ν) h) G i(cid:3)(ν, g) = ψt(ν) (cid:2)h G i(cid:3)(ν, g) Intuitively, this is simply because the individual group elements along the flow only act on the coordinate of the signal, and thereby commute with the standard group convolution. (10) Flow Equivariant Recurrence Relation. We define the recurrence relation for the FERNN as: ht+1(ν, g) = σ(cid:0)ψ1(ν) [ht G W] (ν, g) + [ft ˆV U] (ν, g)(cid:1). We see that there are two primary differences between this model and the G-RNN: the extra dimension ν, and the action of the instantaneous one-step flow element ψ1(ν) at each position ν of the lifted hidden state. Intuitively, this can be understood as bank of G-RNNs, each flowing autonomously according to their corresponding vector fields ν simultaneously. Similar to G-CNNs then, when an input arrives undergoing given flow transformation with generator ˆν, this construction allows us to pull out this transformation into shift of the ν dimension. Formally: Theorem 4.1. (FERNNs are flow equivariant) Let h[f ] FK(Y, Z) be FERNN as defined in Equations 7, 9, and 11, with hidden-state initialization invariant to the group action and constant in the flow dimension, i.e. h0(ν, g) = h0(ν, g) ν, ν and ψ1(ν) h0(ν, g) = h0(ν, g) ν V, G. Then, h[f ] is flow equivariant according to Definition 3.1 with the following re-defined representation of the action of the flow in the output space: (11) (ψ(ˆν) h[f ])t(ν, g) = ht[f ](ν ˆν, ψt1(ˆν)1 g) (12) In words, the equivariance condition ht[ψ(ˆν) ](ν, g) = ht[f ](ν ˆν, ψt1(ˆν)1 g) means that the hidden state of the FERNN processing flowing input sequence has corresponding flow along the dimension, and shift along the dimension, both determined by the input flow generator ˆν. In A.7 we prove Theorem 4.1 by induction, and give brief sketch below. Proof Sketch. (Theorem 4.1) We wish to prove ht[ψ(ˆν) ](ν, g) = ht[f ](ν ˆν, ψt1(ˆν)1 g) t. Letting = G, we write the recurrence relation for the transformed input as: ht+1[ψ(ˆν) ](ν, g) = σ(cid:0)ψ1(ν) [ht[ψ(ˆν) f<t] W] (ν, g) + [(ψt(ˆν) ft) ˆH U] (ν, g)(cid:1). (13) Assume we initialize the hidden state to fixed point of the flow-element action ψ1(ν) h0(ν, g) = h0(ν, g), constant in ν. We can see that the base case is satisfied trivially since ψ1(ν) h0(ν, g) = ψ1(ν) h0(ν ˆν, g) by constancy. We then assert the inductive hypothesis, allowing us to substitute ht[ψ(ˆν) ](ν, g) ht[f ](ν ˆν, ψt1(ˆν)1 g) = ψt1(ˆν) ht[f ](ν ˆν, g) into equation 13. Similarly, due to the trivial lift and the equivariance of lifting convolution, we can substitute [(ψt(ˆν) ft) ˆV U] (ν, g) ψt(ˆν) [ft ˆV U] (ν ˆν, g). Ultimately, this gives us: (13) = σ(cid:0)ψ1(ν) ψt1(ˆν) [ht[f<t] W] (ν ˆν, g) + ψt(ˆν) [ft ˆH U] (ν ˆν, g)(cid:1). (14) By the properties of flows, we know that ψ1(ν)ψt1(ˆν) = ψ1(ν)ψ1(ˆν)ψt(ˆν) = ψt(ˆν)ψ1(νˆν), and thus we can pull out factor of ψt(ˆν) from both terms, leaving us with the original recurrence defined over new index ν ˆν, exactly the shift in . Intuitively, the idea is that we would like to build sequence model which fixes the lack of equivariance demonstrated by the counterexample in Figure 1. To accomplish this, we note that the hidden state of the model in Figure 1 appears to be lagging behind the input at each time step. To fix this, we therefore propose to augment our hidden state with bank of flows, each generated by separate vector field ν such that when the input arrives at specific velocity, it will exactly match one of these flow parameters, effectively shifting the zero point of the moving coordinates of our RNN to this new flow. Since these flows are relative quantities (obeying the group law), we see that all the other ν channels in the bank will similarly shift according to their relative position in space (e.g. relative velocity). These effects combined lead to the output representation we observe analytically. In Figure 7 we visualize the model and the same example of moving bump from Figure 1, providing clear demonstration of what the action of the flow on the hidden state looks like in practice. We finally note that since the hidden state lives on the product group G, and the input is similarly lifted to the product group, one can simply stack these layers, replacing the lifting convolution with another flow convolution and maintain flow equivariance. Since composition of equivariant maps is itself an equivariant map [Kondor and Trivedi, 2018], deep FERNN is also flow equivariant (provided all non-linearities, normalization layers, and other additional architectural components are additionally flow equivariant we refer readers to [Bronstein et al., 2021] for an extensive review). Practical Implementation. We note that exact equivariance requires that the group be closed under the action of the flow on the output. In theory, this can be satisfied by many constructions, but in practice, we cannot lift our hidden state to an infinite dimensional group on machine with finite memory. Therefore, similar to prior work [Worrall and Welling, 2019], we implement truncated version of the group , and incur equivariance error at the boundaries. Furthermore, in the majority of experiments in this work, we define the flow convolution to not mix the channels at all, explicitly: (cid:0)g(cid:1). We find that this is beneficial to not propagate errors between the ν channels induced by the truncation, similar to prior work on scale equivariance [Sosnovik et al., 2020b]. (cid:0)ν, g(cid:1) = δν=eW k"
        },
        {
            "title": "5 Experiments",
            "content": "In the following subsections, we introduce suite of datasets with known flow symmetries and study how adding flow equivariance impacts performance compared with non-equivariant baselines. Specifically, we investigate two small-scale sequence datasets: (i) next-step prediction on modified MNIST dataset with 2 simultaneous digits undergoing imposed translation and rotation flows [LeCun et al., 1998], and (ii) sequence classification on the KTH human action recognition dataset [Schuldt et al., 2004], where we augment the dataset with additional translation flows to simulate camera motion. In we include the full details of the dataset creation, model architectures, training, and evaluation procedures; and in we include extended results. Code is available at: https: //github.com/akandykeller/FERNN. Flowing MNIST Datasets. We construct number of variants of the Flowing MNIST dataset to test generalization, each defined by the set of generators we use for training and evaluation (Vtrain, Vtest). For rotation flows, we discretize the rotation algebra so(2) at θ = 10o intervals, yielding: = {kθJk = N, . . . }, where = (cid:0) 0 1 (cid:1) so(2). For 2D translation flows, we always use the integer lattice up to velocities pixels = {ν Z2 ν }. See Figures 3 & 4 for step : examples. train/test example from the dataset is sequence ft constructed by drawing two random 1 0 6 Model Test MSE Vtrain = Vtest = 2 G-RNN FERNN-V 1 FERNN-V 2 8.1e3 6e4 5.3e4 8e5 1.5e4 2e5 Vtrain = Vtest = 4 G-RNN FERNN-V 2 FERNN-V 4 4.0e3 5e4 1.3e3 5e5 6.1e4 3e Figure 2: Increased flow equivariance increases training speed on data with flow symmetry. Validation loss vs. train steps. Table 1: Flowing MNIST Test MSE. (mean std, 5 seeds) static samples 1, 2 from the original MNIST dataset and two corresponding random vector fields ˆν1, ˆν2 uniformly from the corresponding V, applying the action of the corresponding flows to each sample, and summing the results to generate the timeseries: ft = ψt(ˆν1) 1 + ψt(ˆν2) 2. In words, sequences are composed of two digits moving with separate random translation or rotation velocities. Next-Step Prediction Task. Models are trained to solve an autoregressive forward prediction task: given the first = 10 time-steps of the timeseries , predict the next 10 time-steps. Model parameters are optimized to minimize the mean-squared error (MSE) between predictions and the ground truth for 50 epochs. In the length generalization experiments of Figure 3, we evaluate the models MSE on forward prediction up to 70 steps, significantly longer than the 10 seen in training. Next-Step Prediction Models. We use G-RNNs and FERNNs exactly as defined in Equations 4 & 11 respectively, where is defined as SO(2) with translations (SE(2)) for the rotation flow datasets, and = (Z2, +) is the standard 2D translation group for the translation flow datasets. We use the escnn library to implement the SE(2) convolutions [Cesa et al., 2022]. We set σ = ReLU, use single recurrent layer, hidden state size of 128 channels, and (3 3) convolutional kernels in the spatial dimensions. We note that the spatial dimensions of the input are preserved throughout the entire forward pass, i.e. there is no spatial pooling, stride, or linear layers. We augment these models with small 4-layer CNN decoder gθ(ht+1) with 128 channels, and ReLU activations, which maps from each hidden state to the corresponding output. Explicitly: gθ(ht+1) = ˆft+1, where the training loss is the MSE between ˆft+1 and ft+1 averaged over time [11, 20]. For the FERNN models, we also crucially max-pool over the dimension of the hidden state before decoding, e.g. gθ(maxν ht+1(ν, g)) = ˆft+1. This means that all G-RNN and FERNN models have exactly the same number of parameters, since the FERNN can be seen to share its parameters over the different flow elements. For each FERNN, we denote its corresponding set of generators with FERNN-V . Next-Step Performance. In Figure 2 we show the validation loss curves for these models on Flowing MNIST, and in Table 1 we show the final MSE on the test sets (mean std. over 5 random initializations). We see that on both datasets, FERNNs dramatically outperform G-RNNs, reaching nearly an order of magnitude lower error, and doing so in significantly fewer iterations. We also see 1 on that even partial equivariance (e.g. FERNN-V 2 ) is highly beneficial to model performance. 2 for both the G-RNN and FERNN-V Length Generalization In Figure 3 we plot generated forward predictions on Translation-Flow MNIST with Vtrain = Vtest = 2 models compared with the ground truth sequence. The length of sequences seen in training is indicated by the shaded gray area, and at test time we auto-regressively generate sequences from the models which are significantly longer than this. We see that the G-RNN predictions begin to degrade after the training length, while the FERNN remains highly consistent. We quantify this error for each forward prediction step in the adjacent plot, demonstrating that indeed FERNNs dramatically outperform G-RNNs in length generalization. We note that the performance improvement of FERNNs over G-RNNs is more significant for translation flows compared with rotation flows in our experiments. Particularly, we do not see quite as strong length generalization on Rotating MNIST (see E); however, we speculate that this is due to the interpolation procedure necessary to implement small rotations on finite grid, not due to any difference in the underlying theory. This interpolation is known to break exact equivariance, and we suggest prior methods for addressing this issue in G-CNNs may be equally applicable to FERNNs [Diaconu and Worrall, 2019]. 7 Figure 3: FERNNs exhibit next-step prediction length-generalization capabilities far surpassing G-RNNs on simple flows. We plot samples for the forward prediction trajectories of G-RNN and FERNN-V 2 to predict 10-steps into the future (down-sampled by factor of 4 in time for visualization). We see the G-RNN performs well on this training regime but diverges rapidly with lengths longer than training. The FERNN generalizes nearly perfectly. On the right, we plot this forward prediction error vs. the forward prediction timestep for both models. 2 trained on Translating MNIST Figure 4: FERNNs exhibit next-step prediction generalization to previously unseen rotation flow velocities where G-RNNs fail. Samples of forward predictions for FERNN-V 4 trained on Rotating 5 . We see the FERNN-V MNIST with 4 achieves near perfect forward-prediction performance in this range. On the right we plot the full distribution of error across Vtest. 1 and tested on Velocity Generalization Similar to standard group equivariant neural networks, weight sharing induces automatic generalization to previously unseen group elements. In the case of flow equivariance, we then expect zero-shot generalization to flow generators that are part of the lifted hidden states but not seen during training. In Figures 4 & 5, we show example sequences which demonstrate this to be the case. Specifically, the models are trained on 1 , and evaluated on the full set of generators from 2 respectively. In the adjacent plots, we show the test MSE for sequences flowing according to each generator. We see that the FERNNs (middle) generalize nearly perfectly, while the G-RNNs (bottom) only learn to forward predict in-domain flows, and fails to generalize outside. 4 and 2 and Moving KTH Action Recognition. Finally, we test our FERNN models ability to classify natural video sequences with induced motion using the simple task of Action Recognition from the KTH dataset. The dataset is composed of videos of 25 people performing 6 different actions. We use the traditional train-val-test split, by person, requiring strong generalization to new individuals. We down-sample each clip by factor of 2 in space and time for computational convenience, leading to grayscale sequences of size 32x32, with 16 time-steps each. In our experiments, we apply flows from 2 to the dataset as an attempt to emulate slow camera motion. 1 and Model Test Acc. # θ 3D-CNN G-RNN+ G-RNN FERNN-V 1 FERNN-V 2 Action Recognition Models. In Figure 6 and Table 2, we compare the accuracy of FERNN models with non-flow-equivariant counterparts (G-RNN, G-RNN+, & 3D-CNN), each with comparable parameter counts. The G-RNN and FERNN-V models are identical to those used on the Translating MNIST dataset with the addition of 3-Layer CNN encoder (in place of the input convolution U), and class-readout module composed of an averagepooling over the spatial dimensions followed by linear layer. The G-RNN+ model is an ablation of the FERNN-V 2 which has the same lifted hidden state and max-pooling over the lifted dimensions, but instead of applying the transformation ψ1(ν) to each hidden index ν, the G-RNN+ learns its own transformation through small convolutional kernel (see B). The inclusion of this model ensures that outperform nonTable equivariant models on sequence classification in the presence of strong motion. Test accuracy (mean std) for models trained and tested on the Moving KTH dataset (V 0.626 0.02 0.639 0.02 0.665 0.03 0.698 0.03 0.716 0.04 209K 242K 242K 242K 242K FERNNs 2 ). 2: Figure 5: FERNNs exhibit next-step prediction generalization to previously unseen translation flow velocities where G-RNNs fail. Samples of forward prediction trajectories for FERNN-V 2 trained on Translating MNIST with 2 . On the right we plot the full distribution of error across the 2D velocity coordinates in Vtest. 1 and tested on the benefits that we see from the FERNN are not simply due to having an increased hidden state size and an extra max-pooling non-linearity instead they must be coming from the flow on the hidden state itself. The 3D-CNN is 5-layer network composed of 3D convolutions that performs striding and pooling to eventually yield 64-dimensional feature vector which is fed to linear classification head. Action Recognition Results We see in Figure 6 that the FERNN models again outperform non-flow-equivariant models on generalization to classifying actions undergoing new flows at test time (plots highlighted in orange). We note that the drop in performance is likely due to the truncation discussed in 4 breaking exact equivariance and causing overfitting. In Table 2, we see that when trained and tested on strongly moving data, Vtrain = Vtest = 2 , the FERNN significantly outperforms the non-flow-equivariant counterparts with no train-test disparity."
        },
        {
            "title": "6 Discussion",
            "content": "In this work, we have introduced flow equivariance, general notion of equivariance with respect to timeparameterized group transformations for sequence models, and provided an initial framework for constructing sequence models which satisfy this definition. Related Work. While we believe our definition of flow equivariance and our FERNN construction are novel, we emphasize that we are far from the first to consider equivariance with respect to motion groups. Specifically, number of works have developed equivariance with respect to space-time transformations such as Lorentz transformations [Bogatskiy et al., 2020, Gong et al., 2022]. These works essentially propose to process the input as 4D spacetime block such that the transformation is therein self-contained. The core novelty of our work compared to this literature is the fact that our approach applies to causal sequence processing, and is defined for arbitrary sequence models rather than spatio-temporal convolutions only. number of works consider equivariance in recurrent frameworks, [Azari and Erdogmus, 2022, Nguyen et al., 2023] but only consider static coordinate transformations, not those which change over time, as our flows. Closest to our work, Equivariant Observer Theory from the control theory literature [Mahony et al., 2022] has studied how to build hand-crafted models and filters which are indeed equivariant with respect to time-parameterized symmetries; however this work lacks notion of training familiar to the geometric deep learning community. Prior work in the computational neuroscience community has studied the idea of equivariance with respect to temporal scaling [Zuo et al., 2023] and 1D translation [Zhang et al., 2022] in biologically plausible recurrent neural network circuits. This work Figure 6: FERNNs outperform nonequivariant models on action recognition undergoing out-of-distribution flows (orange). Test accuracy of models trained on KTH Action Recognition with no motion 0 (top row) and trained on data augmented by flow generators 1 (bottom row). The models are tested on data augmented by flow generators 0 , 1 , T 2 (columns). 9 is highly relevant and perhaps in future work it may be possible to derive such models as special cases of our proposed FERNN, or as novel extensions. Notably, several biologically inspired sequence models, including the Topographic VAE [Keller and Welling, 2022] and Neural Wave Machine [Keller and Welling, 2023], explicitly roll latent representations so that single-velocity flow is baked into the network and subsequently learned from data. Our formulation suggests that these heuristics may be special cases of Flow Equivariant RNNs and generalizes the principle to any set of continuous flows . We refer readers to for an extended discussion on related work. Limitations & Future Work. The greatest limitation of the proposed FERNN construction is that it relies on the shift representation of the flow action on the hidden state. This implies that the number of activations must linearly increase with the size of , and additionally induces the truncation errors described in 4. Future work on an analogue to Steerable CNNs [Cohen and Welling, 2016b] for flow equivariance (if possible) would be highly valuable in making these models more computationally efficient. second limitation of the current FERNN model is the simplicity of the network architecture (being built as simple RNN). However, since the flow lift and convolution are linear, time-invariant operations, lifted state-space layer is still linear convolution along time, and all of the computational shortcuts exploited by modern SSMs (FFT kernels, associative scans) should still apply. Finally, convolutional RNNs (such as FERNNs) are becoming increasingly of interest in the computational neuroscience and NeuroAI domains [Spoerer et al., 2017], with notable examples being trained as foundation models for mouse visual cortex [Wang et al., 2025], and recently demonstrating state-of-the-art performance in modeling rodent somatosensory cortex [Chung et al., 2025]. Our work provides new theoretical lens through which to study and build such models through the lens of geometry, offering potential novel insights into biological neural systems. Acknowledgments We would like to thank Max Welling for continued guidance and advice, which ultimately led the author to the development of this work. We would also like to thank Maurice Weiler, John Vastola, and Gyu Heo for discussions which improved this work. We would like to thank members of the CRISP Lab at Harvard, and the Kempner Institute for the opportunity to present and improve early versions of the work. Finally, we would like to thank the Kempner Institute for funding, and for the opportunity to independently pursue the development of this work through the authors research fellowship."
        },
        {
            "title": "References",
            "content": "Bahar Azari and Deniz Erdogmus. Equivariant deep dynamical model for motion prediction, 2022. URL https://arxiv.org/abs/2111.01892. D. Barch, T.N.Vijaykumar Kumar, and D. Glaser. Modeling the illusory motion of enigma with an excitable neuronal array. Journal of Vision - VISION, 3:7272, 12 2010. doi: 10.1167/3.12.72. Davis Barch and Donald Glaser. Synaptic depression and facilitation can induce motion aftereffects in an excitable membrane model of visual motion processing. Neurocomputing, 44-46:5159, 06 2002. doi: 10.1016/S0925-2312(02)00346-6. Sourya Basu, Prasanna Sattigeri, Karthikeyan Natesan Ramamurthy, Vijil Chenthamarakshan, Kush R. Varshney, Lav R. Varshney, and Payel Das. Equi-tuning: group equivariant fine-tuning of pretrained models. In Proceedings of the Thirty-Seventh AAAI Conference on Artificial Intelligence and Thirty-Fifth Conference on Innovative Applications of Artificial Intelligence and Thirteenth Symposium on Educational Advances in Artificial Intelligence, AAAI23/IAAI23/EAAI23. AAAI Press, 2023. ISBN 978-1-57735-880-0. doi: 10.1609/aaai.v37i6.25832. URL https://doi.org/10.1609/aaai.v37i6.25832. Alexander Bogatskiy, Brandon Anderson, Jan T. Offermann, Marwah Roussi, David W. Miller, and Risi Kondor. Lorentz group equivariant neural network for particle physics, 2020. URL https://arxiv.org/abs/2006. 04780. Michael M. Bronstein, Joan Bruna, Yann LeCun, Arthur Szlam, and Pierre Vandergheynst. Geometric deep learning: Grids, groups, graphs, geodesics, and gauges. arXiv preprint arXiv:2104.13478, 2021. Gabriele Cesa, Leon Lang, and Maurice Weiler. program to build E(N)-equivariant steerable CNNs. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum? id=WE4qe9xlnQw. 10 Chemla, Reynaud, di Volo, Zerlaut, Perrinet, Destexhe, and Chavane. Cortical suppressive waves shape the representation of long-range apparent motion. bioRxiv, 2018. doi: 10.1101/372763. URL https://www.biorxiv.org/content/early/2018/10/09/372763. Christopher Choy, JunYoung Gwak, and Silvio Savarese. 4d spatio-temporal convnets: Minkowski convolutional neural networks, 2019. URL https://arxiv.org/abs/1904.08755. Trinity Chung, Yuchen Shen, Nathan C. L. Kong, and Aran Nayebi. Task-optimized convolutional recurrent networks align with tactile processing in the rodent brain, 2025. URL https://arxiv.org/abs/2505. 18361. Taco Cohen and Max Welling. Group equivariant convolutional networks. In Maria Florina Balcan and Kilian Q. Weinberger, editors, Proceedings of The 33rd International Conference on Machine Learning, volume 48 of Proceedings of Machine Learning Research, pages 29902999, New York, New York, USA, 2022 Jun 2016a. PMLR. URL https://proceedings.mlr.press/v48/cohenc16.html. Taco S. Cohen and Max Welling. Steerable cnns, 2016b. URL https://arxiv.org/abs/1612.08498. Nichita Diaconu and Daniel Worrall. Learning to convolve: generalized weight-tying approach, 2019. URL https://arxiv.org/abs/1905.04663. Carlos Esteves, Christine Allen-Blanchette, Xiaowei Zhou, and Kostas Daniilidis. Polar transformer networks, 2018. URL https://arxiv.org/abs/1709.01889. Fynn Fromme, Christine Allen-Blanchette, Hans Harder, and Sebastian Peitz. Surrogate modeling of 3d rayleighbenard convection with equivariant autoencoders, 2025. URL https://arxiv.org/abs/2505.13569. Mathias Gehrig and Davide Scaramuzza. Recurrent vision transformers for object detection with event cameras, 2023. URL https://arxiv.org/abs/2212.05598. Donald Glaser and Davis Barch. Motion detection and characterization by an excitable membrane: The bow wave model. Neurocomputing, 26-27:137146, 1999. ISSN 0925-2312. doi: https://doi.org/ 10.1016/S0925-2312(99)00067-3. URL https://www.sciencedirect.com/science/article/pii/ S0925231299000673. Shiqi Gong, Qi Meng, Jue Zhang, Huilin Qu, Congqiao Li, Sitian Qian, Weitao Du, Zhi-Ming Ma, and Tie-Yan Liu. An efficient lorentz equivariant graph neural network for jet tagging. Journal of High Energy Physics, 2022(7), July 2022. ISSN 1029-8479. doi: 10.1007/jhep07(2022)030. URL http://dx.doi.org/10. 1007/JHEP07(2022)030. Jonathan Gordon, David Lopez-Paz, Marco Baroni, and Diane Bouchacourt. Permutation equivariant models for compositional generalization in language. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=SylVNerFvr. Deepak K. Gupta, Devanshu Arya, and Efstratios Gavves. Rotation equivariant siamese networks for tracking, 2020. URL https://arxiv.org/abs/2012.13078. Brian C. Hall. Lie Groups, Lie Algebras, and Representations: An Elementary Introduction. Springer InISBN 9783319134673. doi: 10.1007/978-3-319-13467-3. URL http: ternational Publishing, 2015. //dx.doi.org/10.1007/978-3-319-13467-3. Geoffrey Hinton, Sara Sabour, and Nicholas Frosst. Matrix capsules with em routing. 2018. URL https: //openreview.net/pdf?id=HJWLfGWRb. Max Jaderberg, Karen Simonyan, Andrew Zisserman, and Koray Kavukcuoglu. Spatial transformer networks, 2016. URL https://arxiv.org/abs/1506.02025. Anderson Keller. Nu-wave state space models: Traveling waves as biologically plausible context. Science Communications Worldwide, 2025. doi: 10.57736/b30b-8eed. URL https://www.world-wide.org/ cosyne-25/nu-wave-state-space-models-traveling-3803805f. T. Anderson Keller and Max Welling. Topographic vaes learn equivariant capsules, 2022. URL https: //arxiv.org/abs/2109.01394. T. Anderson Keller and Max Welling. Neural wave machines: Learning spatiotemporally structured representations with locally coupled oscillatory recurrent neural networks. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 1616816189. PMLR, 2329 Jul 2023. URL https://proceedings.mlr.press/v202/ keller23a.html. 11 T. Anderson Keller, Lyle Muller, Terrence Sejnowski, and Max Welling. Traveling waves encode the recent past and enhance sequence learning, 2024a. URL https://arxiv.org/abs/2309.08045. T. Anderson Keller, Lyle Muller, Terrence J. Sejnowski, and Max Welling. spacetime perspective on dynamical computation in neural information processing systems, 2024b. URL https://arxiv.org/abs/ 2409.13669. Diederik P. Kingma and Jimmy Ba. Adam: method for stochastic optimization, 2017. URL https: //arxiv.org/abs/1412.6980. Risi Kondor and Shubhendu Trivedi. On the generalization of equivariance and convolution in neural networks to the action of compact groups. In International Conference on Machine Learning (ICML), 2018. Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. In Proceedings of the IEEE, volume 86, pages 22782324, November 1998. doi: 10.1109/5. 726791. Alejandro López-Cifuentes, Marcos Escudero-Viñolo, and Jesús Bescós. prospective study on sequence-driven temporal sampling and ego-motion compensation for action recognition in the epic-kitchens dataset, 2020. URL https://arxiv.org/abs/2008.11588. Robert Mahony, Pieter van Goor, and Tarek Hamel. Observer design for nonlinear systems with equivariance. Annual Review of Control, Robotics, and Autonomous Systems, 5(Volume 5, 2022):221252, 2022. ISSN 25735144. doi: https://doi.org/10.1146/annurev-control-061520-010324. URL https://www.annualreviews. org/content/journals/10.1146/annurev-control-061520-010324. J. Moehlis and E. Knobloch. Equivariant dynamical systems. Scholarpedia, 2(10):2510, 2007. doi: 10.4249/ scholarpedia.2510. revision #91240. Hai Huu Nguyen, Andrea Baisero, David Klee, Dian Wang, Robert Platt, and Christopher Amato. Equivariant reinforcement learning under partial observability. In 7th Annual Conference on Robot Learning, 2023. URL https://openreview.net/forum?id=AnDDMQgM7-. Sara Sabour, Nicholas Frosst, and Geoffrey Hinton. Dynamic routing between capsules, 2017. URL https://arxiv.org/abs/1710.09829. Christian Schuldt, Ivan Laptev, and Barbara Caputo. Recognizing human actions: local svm approach. In Pattern Recognition, 2004. ICPR 2004. Proceedings of the 17th International Conference on, volume 3, pages 3236. IEEE Computer Society, 2004. ISBN 0-7695-2128-2. doi: 10.1109/ICPR.2004.747. Sage Shaw and Zachary Kilpatrick. Representing stimulus motion with waves in adaptive neural fields, 2023. URL https://arxiv.org/abs/2312.06100. Ivan Sosnovik, Artem Moskalev, and Arnold Smeulders. Scale equivariance improves siamese tracking, 2020a. URL https://arxiv.org/abs/2007.09115. Ivan Sosnovik, Michał Szmaja, and Arnold Smeulders. Scale-equivariant steerable networks, 2020b. URL https://arxiv.org/abs/1910.11093. Courtney J. Spoerer, Patrick McClure, and Nikolaus Kriegeskorte. Recurrent convolutional neural networks: better model of biological object recognition. Frontiers in Psychology, Volume 8 - 2017, 2017. ISSN 16641078. doi: 10.3389/fpsyg.2017.01551. URL https://www.frontiersin.org/journals/psychology/ articles/10.3389/fpsyg.2017.01551. Søren Kaae Sønderby, Casper Kaae Sønderby, Lars Maaløe, and Ole Winther. Recurrent spatial transformer networks, 2015. URL https://arxiv.org/abs/1509.05329. Eric Y. Wang, Paul G. Fahey, Zhuokun Ding, Stelios Papadopoulos, Kayla Ponder, Marissa A. Weis, Andersen Chang, Taliah Muhammad, Saumil Patel, Zhiwei Ding, Dat Tran, Jiakun Fu, Casey M. SchneiderMizell, Nuno Maçarico da Costa, R. Clay Reid, Forrest Collman, Katrin Franke, Alexander S. Ecker, Jacob Reimer, Xaq Pitkow, Fabian H. Sinz, Andreas S. Tolias, and MICrONS Consortium. Foundation model of neural activity predicts response to new stimulus types. Nature, 640(8058):470477, 2025. doi: 10.1038/s41586-025-08829-y. URL https://doi.org/10.1038/s41586-025-08829-y. Rui Wang, Robin Walters, and Rose Yu. Incorporating symmetry into deep dynamics models for improved generalization, 2021. URL https://arxiv.org/abs/2002.03061. Daniel E. Worrall and Max Welling. Deep scale-spaces: Equivariance over scale, 2019. URL https://arxiv. org/abs/1905.11697. 12 Haixu Wu, Zhiyu Yao, Jianmin Wang, and Mingsheng Long. Motionrnn: flexible model for video prediction with spacetime-varying motions, 2021. URL https://arxiv.org/abs/2103.02243. Theodoros P. Zanos, Patrick J. Mineault, Konstantinos T. Nasiotis, Daniel Guitton, and Christopher C. Pack. sensorimotor role for traveling waves in primate visual cortex. Neuron, 85(3):615627, 2015. ISSN 0896-6273. doi: https://doi.org/10.1016/j.neuron.2014.12.043. URL https://www.sciencedirect.com/ science/article/pii/S0896627314011520. Mingming Zhang, Rajat S. Shivacharan, Chia-Chu Chiang, Luis E. Gonzalez-Reyes, and Dominique M. Durand. Propagating neural source revealed by doppler shift of population spiking frequency. Journal of Neuroscience, 36(12):34953505, 2016. ISSN 0270-6474. doi: 10.1523/JNEUROSCI.3525-15.2016. URL https://www.jneurosci.org/content/36/12/3495. Wenhao Zhang, Ying Nian Wu, and Si Wu. Translation-equivariant representation in recurrent networks with continuous manifold of attractors. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages 1577015783. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/file/ 65384a01325fecbd364c835db872443c-Paper-Conference.pdf. Maksim Zhdanov, David Ruhe, Maurice Weiler, Ana Lucic, Johannes Brandstetter, and Patrick Forré. Cliffordsteerable convolutional neural networks. In Ruslan Salakhutdinov, Zico Kolter, Katherine Heller, Adrian Weller, Nuria Oliver, Jonathan Scarlett, and Felix Berkenkamp, editors, Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pages 61203 61228. PMLR, 2127 Jul 2024. URL https://proceedings.mlr.press/v235/zhdanov24a.html. Alex Zihao Zhu, Ziyun Wang, and Kostas Daniilidis. Motion equivariant networks for event cameras with the temporal normalization transform, 2019. URL https://arxiv.org/abs/1902.06820. Yuliang Zhu, Jing Cheng, Zhuo-Xu Cui, Jianfeng Ren, Chengbo Wang, and Dong Liang. Sre-cnn: spatiotemporal rotation-equivariant cnn for cardiac cine mr imaging, 2024. URL https://arxiv.org/abs/2409. 08537. Junfeng Zuo, Xiao Liu, Ying Nian Wu, Si Wu, and Wenhao Zhang. recurrent neural circuit mechanism of temporal-scaling equivariant representation. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id=K9dmkfZcMu."
        },
        {
            "title": "Appendix",
            "content": "A Extended Proofs A.1 Equivariance of Group-Lifting Convolution & Group-Convolution . . . . . . . . . A.2 Equivariance of Group Equivariant RNN . . . . . . . . . . . . . . . . . . . . . . . A.3 Frame-wise Flow Equivariance . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.4 Group Equivariant RNNs are not Flow Equivariant . . . . . . . . . . . . . . . . . A.5 Equivariance of Lifting Flow Convolution . . . . . . . . . . . . . . . . . . . . . . A.6 Equivariance of Flow Convolution . . . . . . . . . . . . . . . . . . . . . . . . . . A.7 Equivariance of FERNN . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Experiment Details B.1 Flowing MNIST: Dataset Creation . . . . . . . . . . . . . . . . . . . . . . . . . . B.2 Flowing MNIST: Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.3 Flowing MNIST: Next-Step Prediction Training & Evaluation . . . . . . . . . . . B.4 Moving KTH: Datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.5 Moving KTH: Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.6 Moving KTH: Action Recognition Training & Evaluation . . . . . . . . . . . . . . B.7 Moving KTH Samples B.8 Compute . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Variations of FERNNs C.1 FERNN with Non-Trivial Lift . . . . . . . . . . . . . . . . . . . . . . . . . . . . Related Work D.1 Flow Equivariance without Hidden State . . . . . . . . . . . . . . . . . . . . . . D.2 Statically Equivariant Sequence Models . . . . . . . . . . . . . . . . . . . . . . D.3 Neuroscience and Biologically Inspired Neural Networks . . . . . . . . . . . . . . D.4 Reference Frames in Neural Networks . . . . . . . . . . . . . . . . . . . . . . . . D.5 Broadly Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.6 Equivariant Dynamical Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . Extended Results: Flowing MNIST E.1 In-Distribution Next-Step Prediction (Table 1 & Figure 2) . . . . . . . . . . . . . . E.2 Length Generalization MSE vs. Forward Prediction Plot (Rotation) . . . . . . . . . E.3 Length Generalization Visualizations (Translation) . . . . . . . . . . . . . . . . . E.4 Velocity Generalization Visualizations . . . . . . . . . . . . . . . . . . . . . . . . 15 15 16 17 19 19 19 22 23 24 25 25 26 26 28 28 29 29 29 30 30 31 32 33 34"
        },
        {
            "title": "A Extended Proofs",
            "content": "A.1 Equivariance of Group-Lifting Convolution & Group-Convolution We can verify that group convolutional layers given by Equation 3 are equivariant according to Equation 2. While these are well-known facts, we re-derive them here in our own notation for completeness and to lay the groundwork for our later proofs. First, for the lifting convolution, we wish to prove: [(ˆg ) ˆG i](g) = ˆg [f ˆG i](g) g, ˆg G, FK(X). (15) Proof. (Group-Lifting Conv. is Group Equivariant) [(ˆg ) ˆGW i](g) = = = = (cid:88) (cid:88) xZ2 k=1 (cid:88) (cid:88) ˆxZ2 k= (cid:88) (cid:88) ˆxZ2 k=1 (cid:88) (cid:88) ˆxZ2 k=1 fk(ˆg1 x)W k(g1 x) (by def. action, Eqn. 1) fk(ˆx)W k(g1 (ˆg ˆx)) (where ˆx = ˆg1 x) fk(ˆx)W k((g1 ˆg) ˆx)) (by associativity) fk(ˆx)W k((ˆg1 g)1 ˆx)) (by defn. inverse) (16) (17) (18) (19) = [f ˆGW i](ˆg1 g) = ˆg [f ˆGW i](g) (by Eqn. 3 & defn. action) (20) In line 17, we use the fact that because the group acts on by bijection (see our earlier definition of the input representation πX : Aut(X) in 2), the substitution ˆx = g1 is just relabeling of the index set, and hence the sum over is the same as the sum over ˆx X. Next for the group convolution, the proof is virtually identical. We wish to prove: [(ˆg ) i](g) = ˆg [f i](g) g, ˆg G, FK(X). (21) Proof. (Group-Conv. is Group Equivariant) [(ˆg ) i](g) = = = = (cid:88) (cid:88) hG k=1 (cid:88) (cid:88) ˆhG k=1 (cid:88) (cid:88) ˆhG k=1 (cid:88) (cid:88) ˆhG k=1 fk(ˆg1 h)W k(g1 h) (by def. action, Eqn. 1) fk(ˆh)W k(g1 (ˆg ˆh)) (where ˆh = ˆg1 h) fk(ˆh)W k((g1 ˆg) ˆh)) (by associativity) fk(ˆh)W k((ˆg1 g)1 ˆh)) (by defn. inverse) (22) (23) (24) (25) = [f ˆGW i](ˆg1 g) = ˆg [f ˆGW i](g) (by Eqn. 3 & defn. action) (26) Again, we use the fact that the group is closed under the group action, so the substitution ˆh = ˆg1 is just relabeling of the index set, and hence the sum over is the same as the sum over ˆh G. 15 A.2 Equivariance of Group Equivariant RNN We wish to prove that recurrent neural network built with group-equivariant convolutional layers (Equation 4) is equivariant according to Equation 2. If we write the hidden state as function of the input signal ht+1[ft](g) FK(Y ), where = G, then we can write the the equivariance condition as: ht+1[ˆg ft](g) = ˆg ht+1[ft](g) (27) where ˆg ft := {ˆg fi Z+ t}, denotes the input signal generated by applying the same group element to each timestep. We prove this by induction for all Z+: Z+, FK(X), ˆg, G, Proof. (G-RNN is Group Equivariant) We assume (i) ˆG and are the group-lifting convolution and group convolution defined in Equation 3, (ii) σ is G-equivariant non-linearity (e.g. pointwise), and (iii) acts linearly on h-space. Since is defined by the lifting and recurrent convolutions, we see that = G. We further assume (iv) h0 is initialized to be invariant to the group action, i.e. ˆg h0(g) = h0(ˆg1 g) = h0(g), and (v) the input signal is zero before time zero, i.e. f<0 = 0. Base Case: We can see that the base case is trivially true from the initial condition, since the initial condition is not function of the input sequence, so: h0[ˆg f<0](g) = h0[f<0](g) (since h0 is indep. of input f<0) = h0[f<0](ˆg1 g) = ˆg h0[f<0](g) (by initialization) (by defn. action) Inductive Step: Assuming ht[ˆg f<t](g) = ˆg ht[f<t](g), we wish to prove this holds also for + 1: ht+1[ˆg ft](g) = σ(cid:0)[ht[ˆg f<t] W] (g) + [[ˆg ft]ˆGU] (g)(cid:1) = σ(cid:0)[[ˆg ht[f<t]] W] (g) + [[ˆg ft]ˆGU] (g)(cid:1) = σ(cid:0)ˆg [ht[f<t] W](g) + ˆg [ftˆGU](g)(cid:1) = ˆg σ(cid:0)[ht[f<t] W] (g) + [ftˆGU] (g)(cid:1) = ˆg ht+1[ft](g) (by defn. G-RNN, Eqn 4) (by defn. G-RNN) (by inductive hyp.) (by equivar. of G-conv.) (by equivar. non-lin.) (28) (29) (30) (31) (32) (33) (34) (35) We note that the step on the fourth line assumes that the group action is linear, in that it distributes over the addition operation between the previous hidden state and the input. Traditionally, group equivariant neural network architectures have been constructed to exhibit linear representations in the latent space, and therefore this is natural assumption. However, if one allowed to act by non-linear maps (i.e. dropped the linear-representation assumption), then neither distributivity nor σ-equivariance would hold in general. A.3 Frame-wise Flow Equivariance We wish to prove that any G-equivariant map ϕ : FK(X) FK(Y ), that is applied framewise to space-time function FK(X, Z) is also flow equivariant according to Definition 3.1. Specifically, let Φ[f ] = [ϕ(f0), ϕ(f1) . . . ϕ(fT )] be sequence model built by concatenating the output of G-equivariant map ϕ applied to each timestep of the input signal ft. Furthermore, let ϕ be equivariant to the individual group elements generated by vector fields ν , i.e. ϕ(ψt(ν) ft) = ψt(ν) ϕ(ft) Z, ν V, ft FK(X). Then: Proof. (Frame-wise Equivariant Maps are Flow Equivariant) Φ(ψ(ν) ) = [ϕ(ψ0(ν) f0), ϕ(ψ1(ν) f1) . . . ϕ(ψT (ν) fT )] = [ψ0(ν) ϕ(f0), ψ1(ν) ϕ(f1) . . . ψT (ν) ϕ(fT )] = ψ(ν) [ϕ(f0), ϕ(f1) . . . ϕ(fT )] = (ψ(ν) Φ(f )) (by defn. Φ) (by defn. flow action) (36) (by G-equivariance of ϕ) (37) (38) (39) (by defn. flow action, eqn. 5) 16 A.4 Group Equivariant RNNs are not Flow Equivariant In this subsection we prove Theorem 3.1, that the group-equivariant RNN as defined in Equation 4, with non-zero W, is not flow equivariant according to Definition 3.1, except in the degenerate flow invariant case. We will prove this in two parts: (i) First we will prove that G-RNN with constant kernels is indeed flow invariant through induction (the degenerate case). Then (ii), we will proceed with proof by contradiction to show that this is the only such flow equivariant G-RNN possible. To begin, we recall that the degenerate flow invariant case is defined as G-RNN where both and are constant over G. We will denote such kernels and W, such that: W(g) = W(g), g, & U(g x) = U(g x) g, G, X. (40) We wish to prove that such network satisfies Definition 3.1, but more specifically that it is actually invariant to the flow action, meaning: ht[ψ(ν) f<t] = ht[f<t] FK(X), ν V, Z+ (41) We can see that this is special case of Definition 3.1 where the group action on the output space is trivial, i.e. given by the identity: ψt(ν) ht[f<t] = ht[f<t]. We can prove this by induction as before. First we introduce the following lemma for convenience: Lemma A.1. (Lifting convolution with is group invariant) The result of applying the lifting convolution (ˆG) defined in 3 with constant kernel to signal FK(X) is invariant to flow element action on , i.e. [(ψt(ν) ) ˆG i](g) = [f ˆG i](g) ν V, FK(X). (42) Proof. (Lemma A.1) [(ψt(ν) ) ˆG i](g) = = = = = (cid:88) (cid:88) xZ2 k=1 (cid:88) (cid:88) ˆxZ2 k= (cid:88) (cid:88) ˆxZ2 k=1 (cid:88) (cid:88) ˆxZ2 k=1 (cid:88) (cid:88) ˆxZ2 k= fk(ψt(ν)1 x) k(g1 x) (by def. action, Eqn. 1) (43) fk(ˆx) k(g1 (ψt(ν) ˆx)) (where ˆx = ψ(ν)1 x) (44) fk(ˆx) k((ψt(ν)1 g)1 ˆx)) (by associativity and inv.) (45) fk(ˆx) k(ˆg1 ˆx)) (by closure, ˆg = ψt(ν)1 G) (46) fk(ˆx) k(g1 ˆx)) (by defn U(g1 ˆx) = U(ˆg1 ˆx)) (47) = [f ˆG i](g) (by Eqn. 3) (48) We have again used the fact that the group acts on by bijection as in A.1. We can then proceed with the proof by induction to prove that the G-RNN with constant kernels is flow invariant: Proof. (G-RNN with Constant Kernels is Flow Invariant) Let ht[f<t] be defined as in Equation 4 with = and = W. We again assume (i) ˆG and are the group-lifting convolution and group convolution defined in Equation 3, (ii) σ is G-equivariant non-linearity (e.g. pointwise), (iii) acts linearly on h-space, (iv) h0 is initialized to be invariant to the group action of the individual flow elements, i.e. ψt(ν) h0(g) = h0(ψt(ν)1 g) = h0(g), and (v) the input signal is zero before time zero, i.e. f<0 = 0. 17 Base Case: We can see that the base case is again trivially true from the initial condition, since the initial condition is not function of the input sequence, so: h0[ψ(ν) f<0](g) = h0[f<0](g) (since h0 is indep. of input f<0) (49) Inductive Step: Assuming the inductive hypothesis that ht[ψ(ν) f<t] = ht[f<t], we wish to prove this holds for + 1: ht+1[ψ(ν) ft](g) = σ(cid:0)(cid:2)ht[ψ(ν) f<t] W(cid:3) (g) + (cid:2)[ψt(ν) ft]ˆG U(cid:3) (g)(cid:1) (by defn. G-RNN) (50) = σ(cid:0)(cid:2)ht[f<t] W(cid:3) (g) + (cid:2)[ψt(ν) ft]ˆG U(cid:3) (g)(cid:1) (by inductive hyp.) = σ(cid:0)[ht[f<t] W](g) + [ftˆG U](g)(cid:1) = ht+1[ft](g) (by eqn. 4) (by Lemma A.1) (51) (52) (53) Finally then, we use proof by contradiction to show that this degenerate flow invariant case is the only possible case for the G-RNN to be flow equivariant. Proof. (Theorem 3.1, G-RNNs are not Generally Flow Equivariant) Assert the converse, that the G-RNN defined in Equation 4 is flow equivariant, and that the kernels and U, are not constant, i.e. = & = (as defined in Equation 40). Then, it should be the case that: ht+1[ψ(ν) ft](g) = (ψ(ν) h[ft])t+1(g) = ht+1[ft](ψt(ν)1 g) (54) For = 0 we have the following: h1[ψ(ν) f0](g) = σ(cid:0)[h0[ψ(ν) f<0] W] (g) + [[ψ0(ν) f0]ˆGU] (g)(cid:1) = σ(cid:0)[h0[f<0] W] (g) + [f0ˆGU] (g)(cid:1) = h1[f0](g) (by defn. h0 & ψ0) For the next step, we get: h2[ψ(ν) f2](g) = σ(cid:0)[h1[ψ(ν) f0] W] (g) + [[ψ1(ν) f1]ˆGU] (g)(cid:1) (55) (56) (57) (58) = σ(cid:0)[h1[f0] W] (g) + ψ1(ν) [f1ˆGU] (g)(cid:1) (by above & g-conv.) (59) If we look at the action of the flow on the output space, according to flow equivariance, we should have: (ψ(ν) h[f2])2(g) = σ(cid:0)ψ1(ν) [h1[f0] W] (g) + [[ψ1(ν) f1]ˆGU] (g)(cid:1) = σ(cid:0)ψ1(ν) [h1[f0] W] (g) + ψ1(ν) [f1ˆGU] (g)(cid:1) (by g-conv.) (61) (60) Setting h2[ψ(ν) f2](g) = (ψ(ν) h[f2])2(g), as per our assumption, we see the input terms match exactly, but the ht1 terms imply the following equivalence: [h1[f0] W] (g) = ψ1(ν) [h1[f0] W] (g) = [h1[f0] W] (ψ1(ν)1 g) (by action defn.) (62) (63) We see this is precisely the lagging hidden state that we visualized in Figure 1. In order for this equality to hold for all ν , the output of the convolution must be constant along the flows generated by all ν. We can see that this is only satisfied by = W, contradiction. 18 A.5 Equivariance of Lifting Flow Convolution In this section we verify that the flow-lifting convolution given by Equation 7 is equivariant to action of the individual flow elements, with the following representation of the action: ψt(ν) [ft ˆV i](ν, g) = [(ψt(ν) ft) ˆV i](ν, g) := [ft ˆV i](ν, ψt(ν)1 g) (64) Since the flow lifting convolution is trivial lift, equivalent to the group-lifting convolution with an extra duplicated index, this proof is trivial replication of the group-lifting convolution proof: Proof. (Flow-Lifting Conv. is Flow Equivariant) [(ψt(ν) ft) ˆV i](ν, g) = = = (cid:88) (cid:88) xX k=1 (cid:88) (cid:88) ˆxX k=1 (cid:88) (cid:88) ˆxX k=1 fk(ψt(ν)1 x)U k(g1 x) (by defn. action) (65) fk(ˆx)U k(g1 (ψt(ν) ˆx)) (where ˆx = ψt(ν)1 x) (66) fk(ˆx)U k((ψt(ν)1 g)1 ˆx)) (by associativity & inv.) = [ft ˆV i](ν, (ψt(ν)1 g)) (67) (68) A.6 Equivariance of Flow Convolution In this section, we prove that the flow convolution in Equation 9 is equivariant to the action of the individual flow elements yielding: (cid:2)(ψt(ν) h) G i(cid:3)(ν, g) = ψt(ν) (cid:2)h G i(cid:3)(ν, g) (69) Proof. (Flow Conv. is Flow Equivariant) (cid:2)(ψt(ν) h) G i(cid:3)(ν, g) = (where ˆm = ψt(ν)1 m) = (by associativity & inverse) = (cid:88) (cid:88) (cid:88) γV mG k=1 (cid:88) (cid:88) (cid:88) γV ˆmG k=1 (cid:88) (cid:88) (cid:88) γV ˆmG k=1 hk(γ, ψt(ν)1 m) (cid:0)γ ν, g1 m(cid:1) (70) hk(γ, ˆm) (cid:0)γ ν, g1 (ψt(ν) ˆm)(cid:1) (71) hk(γ, ˆm) (cid:0)γ ν, (ψt(ν)1 g)1 ˆm(cid:1) (72) (by Eqn. 9) = (cid:2)h G i(cid:3)(ν, ψt(ν)1 g) (by action) = ψt(ν) (cid:2)h G i(cid:3)(ν, g) (73) (74) A.7 Equivariance of FERNN In this section, we prove Theorem 4.1 by induction. We restate the theorem below: Theorem. (FERNNs are flow equivariant) Let h[f ] FK(Y, Z) be FERNN as defined in Equations 7, 9, and 11, with hidden-state initialization invariant to the group action and constant in the flow dimension, i.e. h0(ν, g) = h0(ν, g) ν, ν and ψ1(ν) h0(ν, g) = h0(ν, g) ν V, G. Then, h[f ] is flow equivariant according to Definition 3.1 with the following representation of the action of the flow in the output space for 1: (ψ(ˆν) h[f ])t(ν, g) = ht[f ](ν ˆν, ψt1(ˆν)1 g) (75) 19 We note for the sake of completeness, that this then implies the follow equivariance relations: ht[ψ(ˆν) ](ν, g) = ht[f ](ν ˆν, ψt1(ˆν)1 g) = ψt1(ˆν) ht[f ](ν ˆν, g) (76) Proof. (Theorem 4.1, FERNNs are flow equivariant) Identical to the G-RNN, we assume that σ is G-equivariant non-linearity, acts linearly on h-space, h0(ν, g) is defined constant as above, and the input signal is zero before time zero, i.e. f<0 = 0. Base Case: The base case is trivially true from the initial condition: h0[ψ(ˆν) f<0](ν, g) = h0[f<0](ν, g) (by initial cond.) = h0[f<0](ν ˆν, ψt1(ˆν)1 g) (by constant init.) (77) (78) Inductive Step: Assuming ht[ψ(ˆν) ](ν, g) = ht[f ](ν ˆν, ψt1(ˆν)1 g) ν V, G, for some 0, we wish to prove this also holds for + 1: Using the FERNN recurrence (Eqn. 11) on the transformed input, and letting = G, we get: (cid:16) ht+1[ψ(ˆν)f ](ν, g) = σ ψ1(ν)[ ht[ψ(ˆν)f<t] ](ν, g) + [ (ψt(ˆν)ft)ˆH ](ν, g) (cid:17) (79) (cid:16) (by inductive hyp.) = σ (cid:17) ψ1(ν)(cid:2)(ψt1(ˆν)ht[f<t]) (cid:3)(ν ˆν, g) + (cid:2)(ψt(ˆν)ft)ˆH (cid:3)(ν, g) (80) (cid:16) (by trivial inp. lift) = σ ψ1(ν)(cid:2)(ψt1(ˆν)ht[f<t]) (cid:3)(ν ˆν, g) + (cid:2)(ψt(ˆν)ft)ˆH (cid:3)(ν ˆν, g) (cid:17) (81) (cid:16) (by equiv. flow-conv) = σ (cid:17) ψ1(ν)ψt1(ˆν)(cid:2)ht[f<t] (cid:3)(ν ˆν, g) + ψt(ˆν)(cid:2)ftˆH (cid:3)(ν ˆν, g) (82) (cid:16) (by flow properties) = σ (cid:17) ψt(ˆν)ψ1(ν ˆν)(cid:2)ht[f<t] (cid:3)(ν ˆν, g) + ψt(ˆν)(cid:2)ftˆH (cid:3)(ν ˆν, g) (83) (cid:16) (by eqiv. non-lin.) = ψt(ˆν)σ ψ1(ν ˆν)(cid:2)ht[f<t] (cid:3)(ν ˆν, g) + (cid:2)ftˆH (cid:3)(ν ˆν, g) (cid:17) (by FERNN Eqn. 11) = ψt(ˆν)ht+1[f ](ν ˆν, g) = ht+1[f ](cid:0)ν ˆν, ψt(ˆν)1 g(cid:1), (84) (85) (86) Thus, assuming the inductive hypothesis for time implies the desired relation at time + 1; together with the base case this completes the induction and proves Theorem 4.1. Similar to the main text, we additionally provide visual example which demonstrates that the counterexample to equivariance of the G-RNN now no longer holds for the FERNN. As we see in Figure 7, the additional dimension results in the moving input being picked up as if it were stationary in the corresponding channel. The channels can then be seen to permute as result of the action of the flow in the latent space. 20 Figure 7: visualization of the same counter example of Figure 1 from the main text, but for the Flow Equivariant RNN. We see that the model is built with set of extra channels, depicted as the three separate rows of hidden states, where each row flows independently per timestep according to its ν parameter (e.g. the bottom row flows with velocity 1, to the left, while the top row flows with velocity 1 to the right). When the model is then processing an input with corresponding flow symmetry (for example motion of velocity 1 to the right), the corresponding flowing channel of the hidden state processes this input in the same reference frame, as if it were stationary (the top row in this example). The remaining rows then process the input with difference corresponding to the difference in velocity between the hidden state channel and the input velocity. We see that this results in the ν ˆν channel permutation in the latent space (the vertical shift in the dimension by 1 row)."
        },
        {
            "title": "B Experiment Details",
            "content": "In this section we describe the datasets, models, training and evaluation procedures used in 5 of the main text. We additionally include samples from each dataset for visualization. The full code to reproduce these experiments is available at: https://github.com/akandykeller/FERNN B.1 Flowing MNIST: Dataset Creation As described in the main text, we construct sequences from the Flowing MNIST dataset by applying flow generators ν randomly picked from an admissible set Vtrain, Vval, & Vtest to samples from the corresponding train / validation / test split of the original MNIST dataset [LeCun et al., 1998]. The training sequences are always composed of = 20 time-steps, and identically for the test sequences (except in the length-generalization experiments where the test sequence length is increased to 40 time-steps). We similarly generally set Vtrain = Vval = Vtest, except in the velocity generalization experiments, where we specify the training and test flows explicitly. Note that the set which defines the set of flows to which the FERNNs are equivariant can be anything, and must not match the Vtrain of the dataset. Explicitly, denoting the value of the ith MNIST training image at pixel coordinate (x, y) as m(i) ((x, y)) = ψt(ν1)m(j) (i) train((x, y)) (x, y) Z2, Z+ (87) where ν1, ν2 Vtrain and j, and random indices sampled with replacement from the MNIST dataset. We note that above, as in the main paper, we have written the signals and the kernels on the infinite domain Z2. In practice, however, both are only non-zero on small portion of this domain (i.e. between (0, 0) & (28, 28) for MNIST digits). We do this to simplify the analysis, analogous to prior work [Cohen and Welling, 2016a]. train((x, y)), sample from the training set can be written as: train((x, y))+ψt(ν2)m(k) Translating MNIST. For the 2D-translation variant of Flowing MNIST, we consider the group with binary operation = (2) = (Z2, +), (88) We use flow generators which are elements of the Lie algebra of this group, ν t(2), which we similarly denote as vectors in Z2, with the Lie bracket [γ, ν] = 0 γ, ν t(2), meaning the translations are commutative. We note that in matrix representation, we embed the elements of (2) into the affine group with homogeneous 3 3 matrices: (x, y) + (x, y) = (x + x, + y). (x, y) (cid:55) 1 0 0 0 1 1 , where the corresponding Lie algebra elements are X(ν) = 0 0 0 0 0 0 , ν1 ν2 0 ν = (ν1, ν2) Z2. The exponential map is then simply: exp(X(ν)) = + X(ν). We see that the flow is then given as: ψt(ν) = exp(tX(ν)) = + tX(ν), and the action of the flow on given pixel coordinate = (x, y) is given as: ψt(ν) = 1 0 0 0 1 0 ν1 ν2 1 1 0 0 = 0 1 1 0 1 0 0 0 + tν1 + tν2 1 1 0 , (91) i.e. shift of the pixel coordinates by velocity ν for time-steps. In practice, order to be able to generate long sequences without excessively large images, we perform all translations with cyclic boundary conditions on our input and hidden states, i.e. ψt(ν)(x, y) = (cid:0)(x+tν1) mod W, (y+tν2) mod H(cid:1), for image size (H, ). Since all of our convolutions are also performed with cyclic boundary conditions, this does not impact performance. In our experiments, to define the sets of generators that we are interested in, we always use the integer lattice up to velocities pixels step . We denote these sets with the notation: For example either dimension, i.e. (92) 2 is the set of all 2D translation vectors with maximal velocity component 2 in := {ν Z2 ν }. 2 = {(2, 2), (2, 1), (2, 0), . . . (2, 2)}. 22 (89) (90) Rotating MNIST. For the planar rotation variant of Flowing MNIST, we consider the group θ + θ = (cid:0)θ + θ(cid:1) mod 2π. = SO(2) = (R, +), with binary operation (93) Elements of the Lie algebra so(2) are one-parameter generators of in-plane rotations, each of the (cid:16) 0 1 and ω denotes the (integer-scaled) angular velocity. form ν = ω J, where = 1 0 Because so(2) is abelian, the Lie bracket [γ, ν] = 0 γ, ν so(2). (cid:17) We embed SO(2) into the affine group with 2 2 matrices: (cid:18)cos θ sin θ cos θ θ (cid:55) sin θ (cid:19) , whose corresponding Lie-algebra elements are (cid:18)0 ω X(ν) = ω (cid:19) , ν = ωJ, ω Z. (94) (95) Since X(ν)2 = ω2I22, the exponential map is the usual matrix exponential for planar rotations: exp(cid:0)X(ν)(cid:1) = + sin(ω) + (cid:0)1 cos(ω)(cid:1) 2. Hence the flow generated by ν is ψt(ν) = exp(cid:0)t X(ν)(cid:1), and its action on pixel coordinate = (x, y) (in homogeneous form) is ψt(ν) = (cid:18)cos(tω) sin(tω) cos(tω) sin(tω) (cid:19) (cid:18)x (cid:19) = (cid:18)x cos(tω) sin(tω) sin(tω) + cos(tω) (cid:19) , (96) i.e. rotation about the image center by angle ω. Following our experimental protocol, we discretize the angular velocity at θ = 10 intervals and collect the set of generators instance, := (cid:8) ν = θ (cid:12) (cid:12) Z, (cid:9). of velocities of For would {40, 30, 20, 10, 0, 10, 20, 30, 40} deg to implement the spatial step . rotation we use the Pytorch function F.grid_sample with zero-padding and bilinear interpolation. We additionally zero-pad all images with 6-pixels on each side (resulting in images of size (40 40)) to allow for the rotation to fit within the full image frame. set In practice, angular consist 4 (97) the B.2 Flowing MNIST: Models For the Flowing MNIST datasets we compare three types of models: standard group-equivariant RNNs (G-RNNs) as defined in Equation 4, FERNNs with equivariance to subset of the training flows (i.e. Vmodel = 2 ), and FERNNs with full equivariance to the training flows (Vmodel = Vtrain). For all models on each dataset we use the same model architecture, and since there are no extra parameters introduced by the FERNN model, all models have the same number of trainable parameters. 1 and Vtrain = Translating MNIST G-RNN. For the translation group, the corresponding group-convolution is the standard 2D convolution. We therefore build G-RNN exactly as written in Equation 4 with regular convolutional layers in place of the group-convolution. We use kernel sizes of 3 3 for both and with no bias terms, strides of 1 and circular padding of 1, resulting in hidden state with spatial dimensions equal to the input spatial dimensions: 28 28. We use 128 output channels for our convolutional encoder U, and similarly 128 input and output channels for our recurrent kernel W. This results in hidden state R1282828. We use ReLU activation function: σ(h) = max(0, h). We initialize all hidden states to zero: h0 = 0, satisfying our equivariance proof assumptions. At each timestep, we decode the updated hidden state to predict the next input through 4-layer CNN decoder: gθ(ht+1) = ˆft+1. The CNN decoder is composed of three primary convolutional layers each with 128 input and output channels, kernel size 3 3, stride 1, and circular padding 1, followed by ReLU activations. final identical convolutional layer, but with 1 output channel, is used to predict ˆft+1. 23 Translating MNIST FERNNs. For the FERNNs, we use the exact same RNN and decoder architecture, with the only difference being that we extend the hidden state with an extra dimension for each of the flows in Vmodel: h(ν, g). Explicitly then, through the trivial lift, the input is copied identically to each of these ν channels, and the flow convolution on the hidden state also operates identically on each ν channel. As stated in the main text, we define the flow convolution to not mix (cid:0)g(cid:1), thus giving us constant parameter count. the channels at all, explicitly: Our lifted hidden state is thus RVmodel1282828. The flow action ψ1(ν) in the recurrence is implemented practically as Roll of the hidden state tensor by (ν1, ν2) steps along the (x, y) spatial dimensions. To achieve invariance of the reconstruction to the input flows, and thereby achieve the generalization we report, we max-pool over the dimensions before decoding. Explicitly, the output of the model for each time step is computed as: gθ(maxν ht+1(ν, g)) = ˆft+1. (cid:0)ν, g(cid:1) = δν=eW Rotating MNIST G-RNN. For the rotating MNIST models, we use nearly identical setup to the translation experiments, with the only difference being that we use SE(2) group convolutions in place of the standard convolutional layers to achieve the necessary rotation equivariance. In practice, we use the escnn library to implement the SE(2) convolutions [Cesa et al., 2022]. We discretize the rotation group into θ = 10o rotations, yielding cyclic group with 36 elements: C36. We lift the input to this space and assert regular representation of the group action on the output. Due to the increased dimensionality of the hidden state from the lift to the discrete rotation group, we decrease the number of hidden state channels to 32 due to hardware limitations. This then yields hidden state: R36324040. Rotating MNIST FERNNs. For the FERNN models, we follow the same procedure as for the translating MNIST FERNNs, except with the action of the group ψ1(ν) in the recurrence now taking the form of the regular representation of rotation in the SE(2) equivariant CNN output space. Explicitly, this means that in addition to rotating the inputs, we also permute along the lifted rotation channel for each angular velocity ν. Again this yields hidden state of size: RVmodel36324040. B.3 Flowing MNIST: Next-Step Prediction Training & Evaluation For the in-distribution next step prediction experiments, (Table 1 and Figure 2) of the main text, we use Vtrain = Vval = Vtest = 2 for the translating MNIST experiments, and Vtrain = Vval = Vtest = 4 for the rotating MNIST experiments. We set the training sequence length to 20 steps, providing the models with 10 time-steps as input, and computing the next-step prediction reconstruction loss (MSE) of the model output on the remaining 10 time-steps. Explicitly: = 1 20 (cid:88) t=11 ft gθ(ht)2 2 (98) All models are trained for 50 epochs, with learning rate of 1 104 using the Adam optimizer Kingma and Ba [2017]. For translation flows, we use batch size of 128, and clip gradient magnitudes at 1 in all models for additional stability. For rotation flows we use batch size of 32 due to memory constraints, and find gradient clipping not necessary. For evaluation, we save the model with the best performance on the validation set (over epochs), and report its corresponding performance on the held-out test set. For each model we train with 5 random initializations (5 seeds) and report the mean and standard deviation of the test set performance from the best saved models. Length Generalization. For the out-of-distribution length generalization experiments (Figure 3) we again use Vtrain = Vval = Vtest = 2 for the translating MNIST experiments, and Vtrain = Vval = Vtest = 4 for the rotating MNIST experiments. Where the length of training and validation sequences is again set to = 20 as before. At test time, we increase the length of the sequences to = 70, but continue to only feed the models the first 10 time-steps as input. In Figure 3 we show the loss of the model for each of these remaining 60 time steps ahead. Velocity Generalization. For the out-of-distribution velocity generalization experiments (Figure 4), we leave the train and test sequence length at 20, but we instead set Vtrain = Vval Vtest. Explicitly, in Figure 4, for rotating MNIST, we set Vtrain = Vval = 5 . For translating 1 and Vtest = 24 MNIST, we set Vtrain = Vval = 2 . This tests the ability of models to generalize to new flows not seen during training, and we see that the FERNNs perform significantly better on this test set when they are made equivariant to these velocities. 1 and Vtest = B.4 Moving KTH: Datasets To test the benefits of flow equivariance on sequence classification task with real image sequences, we opted to use the KTH action recognition dataset [Schuldt et al., 2004], obtained from http://www.csc.kth.se/cvap/actions/. The dataset is composed of 2391 videos of 25 people performing 6 different actions: running, jogging, walking, boxing, hand clapping, and hand waving. The original videos are provided at resolution of 160 120 with 25 frames per second, and an average clip length of 4 seconds. The training, validation, and test sets are constructed from this dataset by taking the videos from the first 16 people as training, the next 4 people as validation, and the last 5 people as test. We split the videos into clips of 32-frames each, downsample to spatial resolution of 32 32, and subsample them by half in time, yielding final set of clips which are 16 steps long. Since the videos from this dataset are taken entirely from stationary camera viewpoint, there are no global flows of the input space which our model might benefit from. We call this original dataset KTH with 0 (no motion). To test the benefits of flow equivariance in an action recognition setting with moving viewpoint, we construct two additional variants of the KTH dataset, augmented by translation flows from the sets 2 . These sets are identical to those described in the Flowing MNIST examples: translation with circular boundary conditions. Again, since we use convolution with circular boundary conditions in all our models, this does not impact model performance. 1 and B.5 Moving KTH: Models We compare five models on the KTH dataset: 3D-CNN, G-RNN, two FERNN variants, and an ablation of the FERNN (denoted G-RNN+). We describe these in detail below: Moving KTH 3D-CNN. The spatio-temporal 3D-CNN baseline is built as sequence of five 3D convolution layers, interleaved with 3D batchnorm and ReLU activations. Each layer has kernels of shape (3 3 3), no bias, and padding 1. The first layer has 16 output channels, temporal stride of 2, and spatial stride of 1. Layer 2 has 32 output channels, temporal stride of 1, and spatial stride of 2. Layer 3 has 32 output channels, temporal stride of 1, and spatial stride of 1. Layer 4 has 64 output channels, temporal stride of 1, and spatial stride of 2. Layer 5 has 64 output channels, temporal stride of 1, and spatial stride of 1. This final layer is followed by global average pooling over the remaining (8 8 8) space-time feature map dimensions, yeilding single vector of dimensionality 64 which is passed through linear layer to predict the logits for the 6 classes. Moving KTH G-RNN. For the baseline G-RNN, each grayscale input frame ft R13232 is passed through three-layer convolutional encoder that preserves spatial resolution: Conv132 55 BN+ReLU Conv3264 33 BN+ReLU Conv64128 3 BN+ReLU , all with stride 1 and circular padding chosen so that the hidden state has the same spatial dimensions as the input (32 32). The output defines the encoder feature map ft. The hidden state ht R1283232 is updated with single recurrent layer using circularly padded 3 3 convolution where R12812833 contains no bias terms. The recurrent convolution is also followed by batch-norm and ReLU non-linearity for added expressivity. The non-linearity σ is tanh. Initial states are zero, h0 = 0, satisfying the assumptions of our equivariance proof. At the final timestep of the sequence (t = 16) we take the hidden state of RNN, perform global average pooling to 1 1 spatial dimensions, and feed the resulting 128-dimensional vector through fully-connected layer: gθ(hT ) = FC1286 (cid:0)SpatialAvgPool(hT )(cid:1) R6, producing logits for the six KTH action classes. Moving KTH FERNNs. For the FERNNs, we use the exact same architecture, but use the trivial lift to lift the corresponding sets of flows 2 (denoted FERNN1 and FERNN-2 in Figure 6). Concretely, for every translation velocity ν Vmodel we allocate an additional velocity channel, so that the hidden state becomes ht(ν, g) RVmodel1283232, 2 for FERNN-V 1 for FERNN-V 1 and 25 where = (x, y) indexes spatial position. Input frames are trivially lifted by copying the same encoder features into each ν-channel. Following Equation 11, the flow action in the recurrence is implemented again as Roll of the spatial dimensions of the hidden tensor by (νx, νy) pixels, and the flow convolution uses weight sharing across velocities, k(g), so the total number of trainable parameters is identical to the G-RNN. As in the Flowing-MNIST experiments, we consider two settings: (i) partial equivariance with Vmodel = 2 and (ii) full equivariance where Vmodel = Vtrain. To achieve flow-invariant action classification we take the maximum over the velocity dimension after the final FERNN layer, maxν ht(ν, g), followed by the same global-average-pooling and linear classifier used for the G-RNN. This design ensures that any translation-induced shifts present at test time are pooled over, yielding the generalization results reported in Figure 6. k(ν, g) = δν=e 1 Vtrain = Moving KTH G-RNN+. To ensure that the observed performance improvement of the FERNN was not simply due to the increased number of hidden state activations and the associated max-pooling, but instead could be attributed the precise flow equivariant form of the recurrence introduced in Equation 11, we built third baseline which is as close as possible to the best performing FERNN (FERNN-V 2 ), while removing precise flow equivariance. Specifically, while keeping all other architectural components of the FERNN-V 2 identical, we replaced the single-step action of the flow in the recurrence (ψ1(ν)) with convolution by separate learned 5 5 convolutional kernel for each ν channel (randomly inititalized). Since the action of ψ1(ν) is simple translation (a local linear operation), this can indeed be represented by such kernel. However, as we see in practice (Figure 6), the model fails to learn such kernels and instead overfits to the training data distribution. B.6 Moving KTH: Action Recognition Training & Evaluation Models are trained to minimize the cross entropy loss between the predicted class and the ground truth label using the Adam optimizer. Due to the small dataset size, all models are trained for 500 epochs, with batch size of 32. We search over learning rates in the set {3 103, 1 103, 3 104, 1 104}, for each model, running three random initialization seeds for each. For each random seed of each hyper-parameter setting, we store the model with the best validation loss. We then pick the best performing model based on the mean value of the best validation loss across all three seeds. We then report in Table 2 the mean test loss of the models (3 seeds) saved at the best validation loss epoch for the best identified learning rate. We generally find the lower learning rates (3 104) work better for the RNN models, while the higher learning rates (1 103) work better for the CNNs. B.7 Moving KTH Samples (a) Original KTH dataset (b) KTH with = 1 (c) KTH with = 2 Figure 8: Samples of the original KTH dataset and its two motion-augmented variants. B.8 Compute All experiments in this paper were performed on private cluster containing mixture of NVIDIA A100 and H100 GPUs, each having 40GB and 80GB of VRAM respectively. No parallelization of 2 models on KTH trained in 7 hours, and the FERNN-V individual models across GPUs was required, i.e. most models and training paradigms were able to fit on single A100 GPU, with the larger models on single H100 GPU. The cluster nodes allocated up to 24 CPU cores and 375GB of RAM per job, although only small fraction of this was required for training and evaluation. The majority of models were able to train fully in less than 24 hours. For example, the FERNN-V 2 models on MNIST trained in 15 hours, with all other models training faster. The significant exception to this were the FERNN-V 4 models on Rotating MNIST which took roughly 67 hours to complete 50 epochs (although they converged much more quickly than this, see Figure 2, we ran them to the same number of epochs as the G-RNN for consistency). The reason for this increased computational time was an inefficient implementation of the rotation operation and our custom recurrence, which could both be accelerated in future work. Specifically, we used naive vanilla Pytorch implementation of our custom FERNN recurrence (Equation 11) using for loops, which dramatically slowed down training for all models. In future work, implementation of the model with scan operation in JAX, or custom CUDA kernel would dramatically improve runtime performance. Overall, we estimate the computational requirements necessary to develop the models and run all experiments for this paper totaled approximately 30 days of H100 compute time."
        },
        {
            "title": "C Variations of FERNNs",
            "content": "C.1 FERNN with Non-Trivial Lift As noted in 4, it is possible to build FERNN with non-trivial lift, such that the lifting convolution itself incorporates the flow transformation for each ν dimension. Explicitly, we can define such lift as: [ft ˆV i](ν, g) = (cid:88) (cid:88) xX k=1 fk(x)U k(g1 ψt(ν)1 x) (99) Another way to think of this, is that there is time-parameterized input kernel defined on the full space G, i.e. ˆU(ν, g, t) = U(ψt(ν)1 g); however, we find this viewpoint less elegant given that the kernel then depends on time. We see then when the flow is incorporated into the lifting convolution, the output of the convolution is no longer constant along the ν index (as it was in the trivial lift). Instead, it is now flowing according to the νth vector field. Therefore, when processing an input undergoing given flow ψ(ˆν), this input flow will combine with the flows of the lifting convolution to yield shift along the dimensions, similar to what we previously observed in the hidden state, i.e.: [(ψt(ˆν) ft) ˆV i](ν, g) = = (cid:88) (cid:88) xX k=1 (cid:88) (cid:88) ˆxX k=1 fk(ψt(ˆν)1 x)U k(g1 ψt(ν)1 x) (100) fk(ˆx)U k(g1 ψt(ν ˆν)1 ˆx) (where ˆx = ψt(ˆν)1 x) = [ft ˆV i](ν ˆν, g) (101) (102) Notably then, keeping the flow convolution from Equation 9 unchanged, we see that we must remove the additional ψ1(ν) shift from the original FERNN in order to maintain flow equivariance. Specifically, the new non-trivial-lift recurrence relation is then given simply as: ht+1(ν, g) = σ(cid:0)[ht G W] (ν, g) + [ft ˆV U] (ν, g)(cid:1). (103) In this setting, the action of the flow on the output space changes to just permutation of the dimension, with no corresponding flow on g: (ψ(ˆν) h[f ])t(ν, g) = ht[f ](ν ˆν, g) (104) In sense, this model can be seen as undoing the action of each flow on the input when lifting. We find this to be somewhat analogous to the traditional group-equivariant CNN design choice where the transformation can either be applied to the filter or the input. In the FERNN setting, the filter is now defined by the full recurrence relation, so we can either apply the flow transformation to the input sequence, or to the hidden state sequence. Overall, we find this to be slightly less elegant construction since the indexing of the kernel in the convolution is then dependent on the time index explicitly. In the trivial-lift setting introduced in the main text, this time-dependence is rather implicitly imposed by the recurrence of the hidden state itself, therefore allowing us to only require the instantaneous one-step flows during each recurrent update. Regardless, we are interested in future work which may explore this non-trivial lift setting more fully, and other interpretations of the FERNN model as described."
        },
        {
            "title": "D Related Work",
            "content": "In this section we provide an overview of work which is related to flow equivariance and equivariance with respect to time-parameterized symmetries generally. D.1 Flow Equivariance without Hidden State As mentioned in the main text, it is possible to achieve flow equivariance without an explicitly flowequivariant sequence model. The two primary methods for accomplishing this are through frame-wise application of an equivariant model (as described in 3) and through group-convolution over the entire space-time block (as described in 6). Both of these methods are verifiably flow-equivariant, however they are fundamentally different class of model than what we have described in this work. They are not recurrent sequence models, and therefore intrinsically have finite temporal context or receptive field which can be used when computing any output. By contrast, recurrent networks can theoretically have an infinite temporal context, if need be, through the maintenance of hidden-state. Given this is fundamental distinction between recurrent and non-recurrent networks which is the subject of research beyond the domains of equivariance, we find it to be beyond the scope of this work to compare with these models explicitly. Instead, we propose flow equivariant RNNs as an extension of existing equivariant network theory to this class of models which maintain hidden state and can operate in the online recurrent setting. The list of prior work which can be included in this category of flow equivariance without hidden state is quite broad, since it encompasses most of the equivariant deep learning literature to date, however we list few notable examples here. The Lorentz equivariant work of Bogatskiy et al. [2020], Gong et al. [2022] is the most relevant, while other applied work has developed 3-D convolutional networks which are equivariant with respect to Galilean shifts (our translation flows) in the context of event-based cameras [Zhu et al., 2019], or rotations over time in the context of medical imaging [Zhu et al., 2024]. Early work developed Minkowski CNNs [Choy et al., 2019], which are equivariant with respect to 4D translations, thus making them equivariant to axis-aligned motions. Further, Clifford Steerable CNNs [Zhdanov et al., 2024] have also been developed to achieve Poincaré-equivariance on Minkowski spacetime. Related work on equivariance for PDE solving / forecasting has built dynamics models which are equivariant with respect to galilean transformations in the sense that the model is equivariant if the input vector field has global additive constant [Wang et al., 2021]. While this is valid for neural networks applied to vector field data as input, it is clearly not the same as our method in more general settings. The method of Wang et al. [2021] can be interpreted as viewing dynamical system which has an unknown global current introduced, while ours is better interpreted as viewing the dynamical system from moving reference frame the two concepts are compatible and may even be combined. D.2 Statically Equivariant Sequence Models The second broad category of related work includes sequence models which are equivariant with respect to instantaneous static group transformations, but are not equivariant with respect to timeparameterized group transformations, as our FERNN is. Examples in this category include [Azari and Erdogmus, 2022, Nguyen et al., 2023, Basu et al., 2023], which introduce equivariance to transformations such as static rotations in sequence models including RNNs. For example, these models train on one frame of reference and then test on rotated frame of reference. Our work can be seen to generalize these models to instead allow them to be tested on rotating frames of reference. This class of prior work can most readily be compared to the group equivariant RNN (G-RNN) we describe in 2. Other researchers have developed sequence to sequence models which are equivariant with respect to fixed permutations and demonstrated that this is beneficial in the context of language modeling [Gordon et al., 2020]. Further, recent work has developed an equivariant sequence autoencoder which uses group convolutions in an LSTM to achieve static equivariance for the purpose of PDE modeling [Fromme et al., 2025]. D.3 Neuroscience and Biologically Inspired Neural Networks The study of symmetry has also grown increasingly relevant in the computational neuroscience literature. Zhang et al. [2022] have studied equivariant representations in biological systems in terms of 29 continuous attractor networks. Interestingly, these models also use convolutional recurrent dynamics; however, again, this work only considers static translations and not motion over time. Related work in the domain of neuroscience has studied how waves can be used to represent motion explicitly [Shaw and Kilpatrick, 2023]. Interestingly, these waves can be seen to be instantiations of the regular representation of the group action that is used in the hidden state of our FERNN model (see [Keller et al., 2024b] for description of the high-level intuition for how equivariance, regular representations, and waves are related). Waves have also been shown to model visual illusions [Barch et al., 2010, Chemla et al., 2018], and some models have even demonstrated bow-wave like effects in neural fields [Glaser and Barch, 1999, Barch and Glaser, 2002], arguing that these effects are potentially beneficial for motion detection. Scientists have also measured Doppler shifts in neural responses as function of moving unobserved neural sources [Zhang et al., 2016]. Finally, traveling waves in visual cortex are also known to be induced by movement, such as saccades [Zanos et al., 2015], an effect which we believe holds some relation to the representation of the flow action on our FERNNs hidden state. One class of models which is highly related to the FERNN in both theory and implementation comes from line of biologically inspired work aiming to learn symmetries from data. One of the first models in this category, the Topographic VAE [Keller and Welling, 2022] can be seen as similar to our FERNN but with an explicitly imposed translation flow of single velocity in the latent space. This makes these models equivariant to input transformations which are isomorphic to the translation group on the integers modulo the capsule length. Interestingly, the authors find that by simply imposing this translation flow in the latent space, the model learns to encode dataset symmetries into these flows in order to better model the dataset. This seems to imply that simply imposing flow symmetries in sequence models without priori knowledge of the structure of the flow symmetries in the input may still be beneficial. Similar results were shown with the Neural Wave Machine [Keller and Welling, 2023], where flows in the latent space were implemented implicitly through bias towards traveling wave dynamics. Finally, perhaps most interestingly, related work on traveling waves in simple recurrent neural networks (the wave-RNN) [Keller et al., 2024a] and SSMs [Keller, 2025] has demonstrated that waves implemented through similar roll operations have significant benefits for long-term memory in recurrent neural network architectures. Incredibly, these models are identical to translation flow-equivariant RNNs in implementation, but without any mention of equivariance, and applied to an entirely different set of tasks. It is therefore of great interest to study if there is something unique to translation flows which benefit memory performance, or if similar performance benefits may be gained from any latent flow symmetry. D.4 Reference Frames in Neural Networks As mentioned in 4, one way to interpret the flow-equivariant RNN is that its hidden state lives in number of moving reference frames simultaneously (one for each ν ). Thus, for moving inputs, the corresponding co-moving hidden state reference frame will see the input as stationary, and process it as normal. This idea of reference frames in neural networks is not new, and significant interesting related work should be noted. Specifically, Spatial Transformer Networks [Jaderberg et al., 2016], and Recurrent Spatial Transformer Networks [Sønderby et al., 2015] can be seen to predict frame of reference for given input and then switch to that reference frame to gain invariance properties. However, these models do not discuss moving reference frames. Other models have been built in this vein with respect to other symmetries, such as polar coordinate networks which are inherently equivariant with respect to scale and rotation [Esteves et al., 2018]. Related in theory is the idea of Capsule Networks [Sabour et al., 2017, Hinton et al., 2018]. These models do have an explicit notion of reference frames, similar to ours, and use this to gain structured equivariant representation, but again this is defined only in the spatial context. D.5 Broadly Related Work More broadly, prior work has looked at the integration of recurrence and motion modeling specifically for vision [Wu et al., 2021, Gehrig and Scaramuzza, 2023]. These models do not have any mention of motion equivariance, and therefore are highly unlikely to provide the strong generalization benefits such as those that we present in this paper. Other work has studied ego motion for action recognition [López-Cifuentes et al., 2020]; and relevant work has looked at equivariance in the context of object tracking [Gupta et al., 2020, Sosnovik et al., 2020a]. D.6 Equivariant Dynamical Systems In the dynamical systems literature, equivariance is typically defined for autonomous (or homogeneous) dynamical systems which have no input or driving force. Abstractly, for system dx dt = (x), we say the dynamical system is equivariant if (g x) = (x) for all G. Then for any x(t) that solves the differential equation, we also know that x(t) solves the differential equation for the full group orbit [Moehlis and Knobloch, 2007]. Similar to the equivariant neural network setting, we see that the output of an equivariant dynamical system (interpreted as the particular solution) transforms in predictable well-behaved manner for given transformation of the input. As the simplest example, dx dt = + x3, has sign flip symmetry in x, meaning that if take the sign flipped version of the trajectory x(t), the solution to the equation also inherits sign flip. It is straightforward to see from the above definition that if the function (x) defining the time derivative is equivariant with respect to G, then the system is considered equivariant, since the definitions are equivalent. In this work, we are interested in recurrent neural networks, which generally operate in the \"forced\" or non-autonomous setting. The study of symmetries in non-autonomous dynamical systems has been previously explored in the control theory literature, and has proven highly valuable for the design of robust and high performing equivariant filters [Mahony et al., 2022]. In this setting, the dynamical system is defined as dx dt = (x, u) for some driving force u, and the equivariance property is then defined as (x, u) = (g x, u). We see that our FERNNs indeed are equivariant non-autonomous dynamical systems by this definition. The difference of our current work with this prior control-observer work, is that Mahony et al. [2022] treats time-parameterised symmetries as known biases inside hand-written dynamical model and uses lifts/adjoint operators to keep the estimation error autonomous. The FERNN instead learns the dynamics, lifts the hidden state to group-indexed field, and enforces equivariance by simple group-convolution weight-sharing rule. This removes the need for handwritten dynamical models and allows learning flexible non-linear dynamics capabilities not covered by existing observer theory. 31 Extended Results: Flowing MNIST E.1 In-Distribution Next-Step Prediction (Table 1 & Figure 2) In Figure 9, we show the sequence predictions of the models presented in Figure 2 & Table 1, trained on Translating MNIST 4 , and evaluated on the same flows. We see that when tested in-distribution, all models appear to perform well from visual inspection. 2 and Rotating MNIST Figure 9: In-distribution sequence predictions for the models from Table 1 & Figure 2, trained on Rotating MNIST 4 (left) and Translating MNIST 2 (right), evaluated on the same flows. 32 E.2 Length Generalization MSE vs. Forward Prediction Plot (Rotation) In Figure 10, we show the length generalization plot, analogous plot to Figure 3 (right), but for Rotating MNIST. We see that the length generalization performance gap is not as significant on Rotating MNIST compared with Translating MNIST. We suspect that this is due to the accumulation of errors induced by repeated interpolation when performing rotation by small angles on discrete grid. Despite this, we see that the FERNN-V 4 still achieves strong generalization up to 30-time steps forward, significantly outperforming the non-equivariant model. Figure 10: MSE vs. Forward prediction horizon for models on Rotating MNIST. Analogous plot to Figure 3 (right) but for Rotating MNIST. 33 E.3 Length Generalization Visualizations (Translation) In Figure 11 we show the sequence predictions of the same models presented in Table 1 & Figure 2, trained on Translating MNIST 4 , but tested in the length generalization setting. We plot the 70-forward prediction steps here, subsampled by half in time (giving 35 elements). We plot the ground truth sequences on top, and the forward predictions below, with the error (in blue-red color scheme) on bottom. We see the FERNNs significantly outperform the G-RNNs in length generalization on Translating MNIST, mirroring the quantitative results in Figures 10 & 3. 2 and Rotating MNIST Figure 11: Samples from models trained on Translating MNIST 2 with training sequence lengths of 20, tested on sequence lengths of 80. We plot the 70-forward prediction steps here, subsampled by half in time (giving 35 elements). 34 E.4 Velocity Generalization Visualizations In Figure 12 we show the sequence predictions of the models presented in Figure 4, trained on Translating MNIST 4 respectively. These figures show that FERNNs which are equivariant to flows beyond their training distribution are able to automatically generalize to these transformations at test time, achieving near perfect next-step prediction performance where non-flow equivariant RNNs fail. 1 and Rotating MNIST 1 , but evaluated on on 2 and 1 (left) and Translating MNIST Figure 12: Samples from models trained on Rotating MNIST 1 (right), and tested on sequences with significantly higher velocity flows (V 4 and 2 respectively). We see that the FERNN (bottom rows) has no problem generalizing to these new velocities despite never having seen them in training, while the G-RNN (top) fails. The specific flows plotted in this example are 40 deg step for Rotating MNIST (left), and ν = (2, 2) for Translating MNIST."
        }
    ],
    "affiliations": [
        "The Kempner Institute for the Study of Natural and Artificial Intelligence, Harvard University, Cambridge, MA 15213"
    ]
}