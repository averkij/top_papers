{
    "paper_title": "Generative AI Act II: Test Time Scaling Drives Cognition Engineering",
    "authors": [
        "Shijie Xia",
        "Yiwei Qin",
        "Xuefeng Li",
        "Yan Ma",
        "Run-Ze Fan",
        "Steffi Chern",
        "Haoyang Zou",
        "Fan Zhou",
        "Xiangkun Hu",
        "Jiahe Jin",
        "Yanheng He",
        "Yixin Ye",
        "Yixiu Liu",
        "Pengfei Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The first generation of Large Language Models - what might be called \"Act I\" of generative AI (2020-2023) - achieved remarkable success through massive parameter and data scaling, yet exhibited fundamental limitations in knowledge latency, shallow reasoning, and constrained cognitive processes. During this era, prompt engineering emerged as our primary interface with AI, enabling dialogue-level communication through natural language. We now witness the emergence of \"Act II\" (2024-present), where models are transitioning from knowledge-retrieval systems (in latent space) to thought-construction engines through test-time scaling techniques. This new paradigm establishes a mind-level connection with AI through language-based thoughts. In this paper, we clarify the conceptual foundations of cognition engineering and explain why this moment is critical for its development. We systematically break down these advanced approaches through comprehensive tutorials and optimized implementations, democratizing access to cognition engineering and enabling every practitioner to participate in AI's second act. We provide a regularly updated collection of papers on test-time scaling in the GitHub Repository: https://github.com/GAIR-NLP/cognition-engineering"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 1 ] . [ 1 8 2 8 3 1 . 4 0 5 2 : r Generative AI Act II: Test Time Scaling Drives Cognition Engineering Shijie Xia1,2,3 Yiwei Qin3 Xuefeng Li1,2,3 Yan Ma3 Run-Ze Fan Steffi Chern3 Haoyang Zou1,2,3 Fan Zhou1,2,3 Xiangkun Hu2,3 Jiahe Jin1,2,3 Yanheng He1,2,3 Yixin Ye1,2,3 Yixiu Liu1,2,3 Pengfei Liu1,2,3* 1Shanghai Jiao Tong University, 2SII, 3Generative AI Research Lab (GAIR)"
        },
        {
            "title": "Abstract",
            "content": "The first generation of Large Language Modelswhat might be called Act of generative AI (20202023)achieved remarkable success through massive parameter and data scaling, yet exhibited fundamental limitations in knowledge latency, shallow reasoning, and constrained cognitive processes. During this era, prompt engineering emerged as our primary interface with AI, enabling dialogue-level communication through natural language. We now witness the emergence of Act II (2024-present), where models are transitioning from knowledge-retrieval systems (in latent space) to thought-construction engines through test-time scaling techniques. This new paradigm establishes mind-level connection with AI through language-based thoughts. In this paper, we clarify the conceptual foundations of cognition engineering and explain why this moment is critical for its development. We systematically break down these advanced approaches through comprehensive tutorials and optimized implementations, democratizing access to cognition engineering and enabling every practitioner to participate in AIs second act. We provide regularly updated collection of papers on test-time scaling in the GitHub Repository. Who Might Benefit from This Paper? Researchers: Open problems and design challenges for field advancement (e.g., 6, 10). Students & Newcomers: Cognition engineering tutorials and code examples (e.g., 9). Educators: Structured teaching resources with guidance for test-time scaling (e.g., 4, 5). Investors & Decision Makers: Enhanced vision through Act I/II framework, providing deep, holistic cognition (e.g., 1). * Corresponding author"
        },
        {
            "title": "Three Scaling Phases",
            "content": "Figure 1: The three scaling phases illustrated as progression of knowledge representation. Pre-training scaling (blue) forms isolated knowledge islands with fundamental physics concepts connected by limited innate associations. Post-training scaling (green) densifies these islands with more sophisticated learned connections between related concepts. Test-time scaling (red) enables dynamic reasoning pathway formation between previously disconnected concepts through extended computation, facilitating multi-hop inference across the entire knowledge space. Testtime scaling builds bridges between knowledge islands, connecting distant nodes that remain isolated during pre-training and conventional post-training. Before proceeding to the main body of our paper, we present hypothesis regarding how the three scaling phases shape the cognitive abilities of models and highlight the significance of test-time scaling in this process. Stage 1: Pre-training Scaling - Formation of Knowledge Islands The foundational phase where intelligence emerges through increased model size and training data volume, establishing basic knowledge acquisition capabilities. During pre-training scaling, we observe the formation of distinct knowledge islands - specialized domains where physics concepts form loosely connected clusters. These concepts are present but connections between them are limited and primarily represent innate relationships, shown as dashed blue lines. The model has acquired these physics concepts but hasnt yet established robust connections between them, limiting its ability to perform complex reasoning across the knowledge graph. Stage 2: Post-training Scaling - Knowledge Densification The post-training scaling phase demonstrates how further fine-tuning densifies knowledge representation. The same physics concepts become more richly connected through learned connections (shown as dotted green lines). The network of physics knowledge becomes more integrated as post-training creates pathways between previously isolated concepts. We see moderate increase in connections between different physics principles, enabling more sophisticated associations. However, these connections primarily link closely related concepts, still lacking the ability to establish comprehensive reasoning pathways between distant knowledge nodes that would require multi-step inference. Stage 3: Test-time Scaling - Cognitive Pathway Formation The final stage represents the paradigm shift enabled by test-time scaling or long thinking. This breakthrough approach allows the model to establish robust reasoning pathways (shown as solid red lines) between previously weakly connected physics concepts. The diagram illustrates how queries starting at specific nodes (Qs) can now trace sophisticated reasoning paths through the knowledge graph, culminating in comprehensive answers at query end nodes (Qe). Through extended computation time at inference, the model explores deeper search spaces within its physics knowledge representation, connecting concepts through multi-hop reasoning. The test-time phase shows fully integrated understanding of gravitational 2 principles where concepts like Universal Gravitation can be meaningfully connected to specific applications like Orbital Motion or Falling Objects. Conclusion This integrated view demonstrates how computational scaling directly shapes knowledge representation and reasoning abilities in physics domains. Pre-training scaling builds the foundational physics knowledge, post-training scaling refines connections between related concepts, but only test-time scaling enables the complex cross-domain reasoning that characterizes advanced scientific thinking. The progression shown fundamentally reframes AI advancement - not merely as an accumulation of more data or parameters, but as the development of cognitive capabilities enabling models to navigate the full complexity of physical principles through principled reasoning processes. Test-time scaling represents the critical frontier in this progression, where models transition from knowledge repositories to systems capable of deep scientific insight through extended deliberation - similar to how human experts solve complex physics problems through sustained thought. 3 The Practitioners Roadmap: How to Apply Test-Time Scaling to your Applications? Figure 2: Workflow for applying test-time scaling in specific domain. For more details, please refer to the main paper. 4 CONTENTS"
        },
        {
            "title": "Contents",
            "content": "1 Introduction 2 What Cognition Engineering Definition . . 2.1 Cognition . 2.2 Engineering Methodology . . 2.3 Cognition Engineering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 Why & Why Now Technical Foundation . . . 3.1 The Necessity of Cognition Engineering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.2 Three Pillars . . . . 3.2.1 Knowledge Foundation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.2.2 Test-time Scaling Foundation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.2.3 3.3 From Theory to Practice: The Road Ahead . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Self-Training Foundation . . . . . . . 4 How Part I: Test-Time Scaling Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.2 Tree Search . 4.1 Parallel Sampling . 4.3 Multi-turn Correction . . 4.1.1 Key Components . 4.1.2 . 4.1.3 . 4.2.1 Key Components . 4.2.2 . 4.2.3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Scaling Laws . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Improving Scaling Efficiency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Scaling Laws . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Improving Scaling Efficiency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Scaling Laws . Improving Scaling Efficiency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Scaling Laws . Improving Scaling Efficiency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.5 Comparisons of Test-Time Scaling Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.6 Ensemble of Test-Time Scaling Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.3.1 Key Components . . 4.3.2 4.3.3 4.4 Long CoT . . . 4.4.1 Key Components . . 4.4.2 4.4. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 How Part II: Training Strategies for Test-Time Scaling . . . . Policy Model Selection . 5.1 Scaling Reinforcement Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.1.1 Training Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.1.2 Reward Function . 5.1.3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.1.4 Training Data Construction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.1.5 Multi-stage Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Iterative Self-reinforced Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.2 Supervised Fine-tuning . 5.3 . . . . . . . . 6 Hows Progress Application So Far . . . . . . . 6.1 Mathematics . 6.2 Code . . . 6.3 Multimodality . . 6.4 Agent . . 6.5 Embodied AI . . 6.6 Safety . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 7 8 8 8 10 10 10 10 10 11 11 11 12 12 13 13 15 15 17 17 18 18 19 19 20 20 20 20 22 23 25 25 25 27 30 30 31 31 33 34 35 36 37 39 40 41 43 44 44 44 45 45 46 46 47 47 47 48 48 49 49 51 51 52 CONTENTS 6.7 RAG . . 6.8 Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 So What? From Scaling to Cognitive Intelligence 7.1 Data Engineering 2.0: Cognition Data Engineering . . . . . . . . . . . . . . . . . . . . . . . . . 7.2 Reward & Environment Engineering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7.2.1 Reward Models Design . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7.2.2 Cognitive Environment Design . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7.3 Human-AI Cognitive Partnership . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7.4 Research Acceleration . . . . . . . 8 Infrastructure 8.1 RL . . 8.2 MCTS . . . . 9 Tutorial . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9.1 Preparatory Work . 9.2 Start RL training . 9.3 Understanding algorithm with Code Analysis . 9.4 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10 Future Directions 11 Conclusion"
        },
        {
            "title": "Introduction",
            "content": "In recent years, Large Language Models (LLMs) such as GPT (OpenAI, 2023), LLaMA (Meta, 2024, 2023), and Claude (Anthropic, 2024a) have emerged as powerful knowledge management tools through extensive pre-training and fine-tuning processes. These models, trained on vast corpora of human-generated text, have successfully organized and systematized accumulated human knowledge. Through paradigm defined by scaling pre-training data, computation, and model parameters (Kaplan et al., 2020), these systems can engage in natural language conversations, retrieve information, generate content, and answer questions across diverse domains (Zhao et al., 2023b; Wang et al., 2024g; Zheng et al., 2023a). This first generation of LLMswhat we might call Act of generative AIintroduced fundamental shift in human-AI interaction. The cornerstone of this first act has been prompt engineeringthe art of crafting inputs that guide models toward desired outputs (Liu et al., 2021; Sahoo et al., 2024). This innovation enabled humans to communicate with AI systems using natural language for the first time, dramatically lowering the barriers to human-machine interaction. Act focused primarily on gathering and organizing existing knowledge through ever-larger models trained on increasingly vast datasets. However, despite their impressive capabilities, Act models exhibit several significant limitations: (i) knowledge latency: These models primarily learn high-frequency information that has had time to accumulate in their training data, leaving them with limited understanding of emerging knowledge and concepts (Huang et al., 2025b). (ii) shallow reasoning: While capable of basic logical inferences, they struggle with problems requiring multi-step, deep reasoning processes (Zhang et al., 2024d; Mirzadeh et al., 2024; Kambhampati, 2024). (iii) limited thought processes: They fail to demonstrate human-like depth of thought, particularly when confronting novel or openended questions (Wu et al., 2024d). These constraints have kept Act models primarily confined to knowledge retrieval and simple reasoning tasks, still considerably distant from achieving artificial general intelligence (AGI). Just as knowledge alone is insufficient for human intelligence development, merely amassing information has proven inadequate for AI systems to approach human-like intelligencethey must also develop the capacity for deep thinking and reasoning (Newell et al., 1959, 1972). Recently, the AI field has witnessed profound paradigm shift. new technical approach centered on test-time scaling is redefining the boundaries of what LLMs can achieve (OpenAI, 2024; DeepSeek-AI et al., 2025; Snell et al., 2024), inaugurating the second act of generative AIcognition engineering. Cognition Engineering is the systematic and constructive development of AI thinking capabilities through test-time scaling paradigms that transcend traditional pretraining approaches. This methodology represents the deliberate cultivation of deep cognitive processes in artificial systems through both human cognitive pattern distillation and AI-driven discovery (e.g., reinforcement learning). At its core, cognition engineering sits at the intersection of two fundamental concepts: Cognition in this context refers not merely to knowledge acquisition but to deep cognitionthe ability to perform complex reasoning, engage in deliberate thinking, connect disparate concepts, and generate novel insights. It encompasses the meta-cognitive processes (Metcalfe and Shimamura, 1994) that allow for understanding not just what but why and how the very essence of human intellectual advancement. Engineering here signifies constructive approach rather than purely emergent one. It moves beyond the limitations of mere scaling toward more intentional construction of cognitive capabilities through targeted interventions in both training methodologies (e.g., reinforcement learning) and inference optimization (e.g., extending inference-time computation). Cognition engineering represents comprehensive technological paradigm shift in LLM development. From the inference perspective, it transitions from crafting prompt templates that retrieve knowledge from LLMs to designing test-time scaling strategies that conduct deeper and more comprehensive searches through knowledge spaces. This evolution demands rigorous analysis of test-time scaling strategy components, characteristics, and efficiency, which underscores the necessity for structured engineering approach. From the training perspective, cognition engineering redirects computational resources from knowledge-focused pre-training toward developing deep thinking abilities through techniques like learning on human cognition data and reinforcement learning. The shift implies in Act II, the cognitive exchange becomes bidirectional. Not only can humans teach AI systems how to approach complex problems, but AI systems can also autonomously discover novel cognitive patterns and reasoning pathways through techniques like reinforcement learning. For example, we have already witnessed moments reminiscent of AlphaGos famous Move 37, where AI demonstrates thinking approaches that transcend human intuition yet ultimately prove effective (Silver et al., 2016). These AI-discovered cognitive strategies have the potential to enrich human understanding, opening new research and innovation pathways. This bidirectional cognitive exchange marks our entry into new era of intelligence symbiosis. This paper will explore in depth the definition, technical foundations, and application prospects of cognition engineering. First, we will clarify the conceptual connotations of cognition engineering (2) and explain why now is the critical moment for its development (3). Next, we will analyze in detail the technical foundations of test-time 7 scaling (4) and various training strategies for it (5). We will then examine the systemic changes cognition engineering brings to AI research and the applications that have already emerged (6). Finally, we will analyze the profound implications beyond the technical implementation (7), discuss the infrastructure (8), and identify several future directions for cognition engineering (10). We also provide practical tutorial for implementing test-time scaling with code examples (9). Through these discussions, we aim to outline the contours of generative AIs second act and provide researchers and practitioners with framework for thinking in this new paradigm."
        },
        {
            "title": "2 What – Cognition Engineering Definition",
            "content": "The term cognition engineering represents significant conceptual shift in artificial intelligence development. To understand the essence of this emerging field, we can utilize DIKW (Data-Information-Knowledge-Wisdom) pyramid theory (Zeleny, 1987; Ackoff, 1989) as conceptual framework and explore how cognition engineering enables the leap from knowledge to wisdom. Figure 3: The DIKW Pyramid and its relationship to cognition engineering paradigm. 2.1 Cognition The DIKW theory portrays cognitive process as hierarchical transformation: from raw data to contextualized information, then to applicable knowledge, and ultimately to profound wisdom. This framework offers deep insights for understanding cognition engineering. At the data level, we encounter raw facts and observations devoid of inherent meaning; the information level consists of processed and organized data imbued with context and structure; the knowledge level manifests as understanding and application of information, including mastery of rules, patterns, and relationships; while the wisdom level embodies deep comprehension of knowledge, involving judgment, creativity, and metacognitive abilities. Traditional AI systems primarily operated at the data and information levels, whereas first-generation LLMs achieved significant breakthroughs at the knowledge level. Cognition engineering represents the crucial step toward advancing to the wisdom level. In psychology and cognitive science, cognition refers to the complex mental processes through which organisms acquire and process information, form knowledge, and apply it to problem-solving (Von Eckardt, 1995; Nunez et al., 2019). However, what cognition engineering pursues is not merely this basic cognitive capability, but the wisdom-level cognition described by DIKWthe ability to understand deep principles, engage in creative thinking, and demonstrate judgment. This deep cognition concerns not only knowing what (i.e., knowledge) but also knowing why and knowing how (i.e., wisdom). Cognition is characterized by deep thinkingthe ability to engage in multi-layered, complex reasoning exploring multiple pathways to resolutionalongside metacognitive capabilities that allow reflection on ones own thought processes. It encompasses creative reasoning that connects knowledge across domains to generate novel insights, cognitive adaptability that applies existing patterns to new contexts, and conceptual abstraction that extracts higher-order principles from concrete instances. These capabilities collectively form the core of human intelligence and are the foundation of humanitys continuous advancement in scientific discovery and technological innovation. 2.2 Engineering Methodology Engineering, as methodology, is essentially an organized approach to designing, building, and optimizing systems to solve specific problems. Within the DIKW framework, engineering methods can be viewed as the process by which humans consciously guide systems to ascend from the data level to the wisdom level. In cognition engineering, this emphasis on intentional construction manifests through targeted interventions in both training methodologies and inference optimization, rather than relying exclusively on scaling approaches. 8 2.3 Cognition Engineering Figure 4: The evolution of AI engineering paradigms. 2.3 Cognition Engineering Combining the concepts of cognition and engineering, and viewed through the lens of DIKW theory, we can define cognition engineering more profoundly: Cognition Engineering is systematic methodology that constructs and optimizes AI systems ability to ascend from the knowledge to wisdom levels of the DIKW pyramid through specific design patterns, training strategies, and computational allocations. It enables AI systems to engage in deep thinking, complex reasoning, and creative problem-solving, exhibiting cognitive characteristics similar to human wisdom-level traits. The key distinction between cognition engineering and traditional AI development approaches lies in its methodological characteristics: From emergence to construction: Traditional AI development paths primarily rely on naturally emerging capabilities through increased data scale, model parameters, and training computation, mainly accumulating at the data and information levels of DIKW. Cognition engineering adopts more proactive constructive approach, consciously designing mechanisms that promote the transformation from knowledge to wisdom. From behavior imitation to thought imitation: Traditional models primarily learn by imitating human output behaviors, remaining at the knowledge level of DIKW; cognition engineering focuses on imitating human thought processes, directly addressing the cognitive characteristics of the wisdom level. From static knowledge to dynamic wisdom: Traditional models have relatively fixed capabilities after training, whereas cognition engineering emphasizes AI systems dynamic thinking abilities during inference, allowing them to adjust thinking depth and resource allocation based on problem complexity. From knowledge retrieval to knowledge creation: Traditional models primarily retrieve and combine existing knowledge, while cognition engineering aims to enable AI systems to generate new insights and discoveries through deep thinking, realizing the creative characteristics of the wisdom level in DIKW. Within the DIKW framework, we can clearly distinguish the two acts of AI development: the first act primarily solved the problem of transforming information into knowledge, while the second act begins to systematically explore the leap from knowledge to wisdom. If traditional LLM training methods are nature-likeallowing capabilities to emerge naturally through massive resource and time investmentthen cognition engineering is civilization-likepurposefully shaping AI systems thinking abilities through planned design and intervention, just as human civilization consciously cultivates wisdom. Figure 4 shows the development of artificial intelligence can also be understood as an evolution through distinct engineering paradigms, each representing fundamental shift in capabilities and applications. This progression perfectly embodies the natural journey from Data to Wisdom, (Zeleny, 1987; Ackoff, 1989) with each stage 9 expanding boundaries built upon previous paradigms. Cognition engineering represents critical stage on the path toward AGI, as it addresses the fundamental requirement for deep thinking capabilities that cannot naturally emerge through knowledge accumulation alone."
        },
        {
            "title": "3 Why & Why Now – Technical Foundation",
            "content": "3.1 The Necessity of Cognition Engineering The rise of cognition engineering is not coincidental but direct response to the wisdom gap encountered by AI development in the DIKW pyramid. Despite significant advances in knowledge retrieval, content generation, and basic reasoning, LLMs still exhibit notable shortcomings at the wisdom level: Limitations in Complex Reasoning Current models perform poorly on problems requiring multi-step deep reasoning (Zhang et al., 2024d; Mirzadeh et al., 2024; Kambhampati, 2024). Even the most advanced models struggle with reliable mathematical proofs, complex scientific problem-solving, or multidimensional analysis (Yang et al., 2024b; Rein et al., 2023). These tasks require models to decompose problems into sub-problems, explore multiple possible reasoning paths, and conduct deep logical analysiscapabilities beyond what scaling pre-training data alone can achieve. Challenges in Knowledge Updating and Creation Pre-trained models knowledge is fixed at the end of training, unable to automatically adapt to new developments and changes. More importantly, they struggle to generate truly original insights or discoveriesthe essence of scientific discovery is not merely understanding known facts but proposing new hypotheses, designing experimental methods, and drawing new conclusions from results. This knowledge creation ability requires going beyond simple knowledge retrieval and pattern recognition. Elevated Application Requirements As AI applications expand from simple tasks to complex decision-making, scientific research, and creative work, demands for AI systems wisdom-level capabilities also increase (OpenAI, 2025b,a). Users are no longer satisfied with answers based on statistical patterns (knowledge level); they desire thoughtful analysis, multi-perspective considerations, and innovative insights (wisdom level) from AI. 3.2 Three Pillars Cognition engineering emerges at this specific moment due to multiple technological breakthroughs reaching maturity simultaneously. These breakthroughs collectively create the necessary conditions enabling AI to progress from knowledge management to deep cognitive capabilities. The rise of cognition engineering stems from three key technological pillars: 3.2.1 Knowledge Foundation The first enabling foundation for cognition engineering is the fundamental transformation in how LLMs acquire knowledge. Modern foundation models have not only achieved exponential growth in training data volume (such as Llama 2s 2 trillion token training scale (Meta, 2023)) but more importantly, have experienced qualitative transformation. Pre-training data has evolved from simple web-scraped text to carefully curated knowledge corpora (Shao et al., 2024; Zhou et al., 2024b; Wang et al., 2023d; Yang et al., 2024a). These datasets now integrate scientific literature and technical documentation with mathematical textbooks and problem sets, multilanguage programming code repositories, and structured knowledge from specialized domains, forming much richer knowledge ecosystem than previously available. This comprehensive knowledge foundation is necessary prerequisite for cognition engineeringwithout this extensive embedded knowledge, models would lack the raw materials required for deep thinking. 3.2.2 Test-time Scaling Foundation The second critical pillar enabling cognition engineering is the fundamental reconceptualization of how computational resources are allocated during the inference phasewhat we term Test-Time Scaling. Traditional inference approaches were constrained by fixed output lengths and single-pass generation paradigms. Recently, series of technical breakthroughs has significantly extended models reasoning capabilities. Chain-of-Thought (CoT) prompting (Wei et al., 2022) methods encourage models to perform step-by-step reasoning like human problem-solving processes, clearly articulating intermediate steps. Tree search (Yao et al., 2023a; Hao et al., 2023; Feng et al., 2023) allow for systematic exploration of multiple reasoning pathways simultaneously, rather than being confined to single line of thinking. Self-correction and verification techniques (DeepSeek-AI et al., 2025; Kumar et al., 2024; Qu et al., 2024) further enhance these capabilities, enabling models to evaluate their own reasoning, identify potential errors, and refine their approachesmimicking human metacognitive processes. These innovations collectively provide what can be understood as cognitive workspace where models can systematically explore their knowledgesimilar to how humans need scratch paper to solve complex problems or require time to think deeply. 10 3.3 From Theory to Practice: The Road Ahead 3.2.3 Self-Training Foundation The third pillar of cognition engineering is advanced self-training methodologies. Developing sophisticated cognitive abilities in models exclusively through expert human cognition data encounters inherent scaling limitations. Self-training technologies not only provide an alternative pathway to elicit cognitive capabilities but also create opportunities for superhuman performance through AI self-discovery strategies. As demonstrated by DeepSeekR1 (DeepSeek-AI et al., 2025) and subsequent research (Gandhi et al., 2025; Yu et al., 2025a), training with reinforcement learning using verifiable rewards enables models to master complex cognitive behaviors including reflection, backtracking, and verification when solving challenging problems. Through this process, models learn to dynamically allocate computational resources according to problem complexity, effectively internalizing test-time scaling techniques. Additionally, iterative self-training on reasoning trajectories generated through test-time scaling methods facilitates continuous improvement (Zelikman et al., 2022; Feng et al., 2023; Xiong et al., 2025), allowing AI systems to progressively enhance their problem-solving abilities. 3.3 From Theory to Practice: The Road Ahead The theoretical foundations are now in place, but translating these foundations into practical implementations requires navigating complex landscape of specific methods and approaches. The most immediate and promising avenue for realizing cognition engineering in practice is through test-time scaling, family of techniques that optimize how models allocate computational resources during inference to achieve deeper reasoning. These methods serve as the practical bridge between the theoretical promise of cognition engineering and its real-world implementation. By understanding and refining these techniques, we can begin the systematic construction of AI systems that truly think rather than merely predict. In the following section, we delve into the specific mechanisms that enable test-time scaling, examining how different approaches address the fundamental challenge of extending and deepening AI reasoning processes. This exploration will reveal not just the technical details of these methods, but also their cognitive implications and their roles in the broader cognition engineering paradigm."
        },
        {
            "title": "4 How – Part I: Test-Time Scaling Methods",
            "content": "Given query and generator g, the test time scaling method can be abstracted to search strategy that guides the generator to find the optimal response: (.q, g, ϕ) (1) where ϕ represents any additional inputs such as scoring functions (also known as value functions, reward models, or verifiers1) and hyperparameters of the strategy. Scaling laws For any test-time scaling method, there exist corresponding scaling dimensions λ within ϕ that directly determine the computation cost during inference. The scaling laws of describe the relationship between λ and performance. Scaling efficiency Given computation budget2 C, we define an abstract function : that maps from the computation budget and the test-time scaling method to performance. The scaling efficiency measures this performance relative to the computation budget: efficiency = (C, ) (2) The high-level strategies to improve efficiency of can be categorized into:3 1) Optimizing individual testtime scaling methods: This involves carefully selecting and tuning components within computation budget constraints, or leveraging additional training-time compute to optimize models specifically for test-time scaling; 2) Combining multiple test-time scaling methods: This includes simultaneously combining multiple methods or selecting appropriate test-time scaling methods according to different contexts. In the following sections, we will investigate four primary test-time scaling methods: parallel sampling (4.1), tree search (4.2), multi-turn correction (4.3), and long CoT (4.4). For each test-time scaling method, we will cover the construction method, the scaling laws, and how to improve the scaling efficiency from the individual optimization aspect. Furthermore, we compare these test-time scaling methods across multiple dimensions (4.5) and discuss how to effectively combine them for enhanced performance (4.6). 1These words share subtle differences in context and we choose the most appropriate term for each method. 2It can be measured by FLOPs, running time, token numbers, etc. 3In this section, we do not consider the model compression techniques like model quantization or inference acceleration from infrastructure aspects as they are orthogonal to the method design. 11 4.1 Parallel Sampling Figure 5: Illustration of parallel sampling selection methods: Best-of-N (F1), Majority voting (F2), and Combined strategy (F3). 4.1 Parallel Sampling 4.1.1 Key Components The parallel sampling algorithm samples set of candidate responses = {yi}N i=1 independently from the generator for the same query, where is the sampling number, and selects the targeted response or answer from them. This approach can be conceptualized as global search within the knowledge space (Snell et al., 2024). The selection methods are as follows: F1: Best-of-N (BoN). This method utilizes scoring function to evaluate each response and selects the one with the highest score: = arg max yY v(y) (3) The scoring function can be external tools that directly verify the effectiveness of the response, such as code interpreters (Li et al., 2022; Chen et al., 2023a) or math proof checkers (Brown et al., 2024). For tasks lacking verification tools, can be specialized trained model. For instance, Cobbe et al. (2021) train the outcome reward model (ORM) to score the entire response, while Lightman et al. (2023); Uesato et al. (2022) train the process reward model (PRM) to score each step in the response and apply an aggregation function to determine the overall response score. Self-Certainty (Kang et al., 2025) eliminates the need for an additional reward model by leveraging the generators inherent probability distribution for scoring. F2: Majority voting. Majority voting (or self-consistency (Wang et al., 2023c)) selects the most frequent answer from the candidates: = arg max yY g(y, ˆy) (cid:88) ˆyY g(y, ˆy) = (cid:40) 1 0 if is equivalent to ˆy, otherwise, (4) (5) where is an automatic grading function that first extracts the answers from the responses and checks for equivalence. While this method is lightweight, the requirement for easy answer equivalence comparison limits its applicability for open-ended tasks. Universal Self-Consistency (Chen et al., 2023b) employs LLMs themselves to select the most consistent answer among multiple candidates, though the limited context window size of models still presents challenges for large sampling numbers. F3: Combining voting and scoring strategy. The scoring strategy can help select targeted low-frequency responses but heavily depends on the reliability of the scoring function, whereas the voting strategy offers greater robustness but has more fixed upper bound. This combined method leverages advantages from both 12 4.1 Parallel Sampling (a) Parallel Sampling (4.1.2) (b) Tree Search (4.2.2) Multi-turn (c) tion (4.3.2) correc- (d) Long CoT (4.4.2) Figure 6: The relationship between scaling dimensions and performance for each test-time scaling method. approaches for more robust selection (Sun et al., 2024b). For example, weighted majority voting (Uesato et al., 2022; Liu et al., 2023d) re-ranks answer clusters according to the sum of the scores in each cluster and selects the answer cluster with the highest score: = arg max yY (cid:88) ˆyY g(y, ˆy)v(ˆy) (6) Figure 5 illustrates these selection methods. 4.1.2 Scaling Laws The main scaling dimension in parallel sampling is the sampling number N. We investigate the relationship between and various performance metrics. Specifically, we focus on two types of metrics: Pass@N, which represents the probability of generating at least one correct response among candidates, and metrics such as Maj@1 or BoN, which measure the practical performance of parallel sampling. The monotonic growth relationship between and Pass@N Brown et al. (2024) investigate the relationship between and Pass@N across different models and tasks. The Pass@N grows steadily with sampling numbers. Moreover, the relationship between the two is often log-linear as demonstrated in Figure 6a, similar to the training time scaling law observed (Kaplan et al., 2020). Scaling Pass@N does not translate to real-world performance improvements Although the continuous improvement of Pass@N with increased sampling numbers is promising, there remains gap between this metric and true performance. This gap exists for several reasons. First, performance improvements can only be realized when appropriate tools exist to select the correct response from the sample set. However, perfect verifiers do not exist for most tasks. As Brown et al. (2024) observe, when using ArmoRM-Llama3-8B-v0.1 (Wang et al., 2024d) as the scoring model, significant disparity emerges between Pass@N and practical metrics like Maj@1 or BoN (see Figure 6a). Second, the verifiers themselves can be hacked. Code may pass unit tests but fail with additional test cases (Stroebl et al., 2024), or mathematical solutions may reach correct answers through incorrect reasoning (Xia et al., 2024), leading to the false positive problems. Stroebl et al. (2024) observe that the false positive rate increases as the Pass@1 accuracy decreases in code tasks, concluding that this imposes an upper bound on the accuracy of resampling-based inference scaling, even with infinite computational resources. For practical application methods, such as majority voting or scoring methods, performance tends to saturate (Brown et al., 2024; Wu et al., 2024c) and may even degrade as the number of samples increases (Chen et al., 2024c) due to imperfect verifiers. 4.1.3 Improving Scaling Efficiency The strategies to improve the scaling efficiency of parallel sampling are as follows. Query-aware sampling Applying fixed sampling number for all queries is not optimal, as difficult problems require more sampling while easier ones need fewer. This line of methods employs adaptive sampling numbers for different queries based on difficulty to improve sampling efficiency. Chen et al. (2024c) categorize queries into easy and difficult cases according to model uncertainty and apply different sampling numbers accordingly. DSC (Wang et al., 2024f) prompts the model to rank query difficulty and distributes sampling numbers based on this ranking. Early stopping strategy This method estimates the quality of responses during the sampling process and decides when to stop sampling early by utilizing prior knowledge or model estimation. It includes terminating sampling when observed answers are identical or fit predefined distributions within small window size (Aggarwal et al., 2023; Li et al., 2024e; Wan et al., 2024), or training the generator itself to estimate the confidence of the response and 4.1 Parallel Sampling Query-aware sampling DSC [343]; Chen et al. [30] Early Stopping Strategy Parallel Sampling (4.1.3) Tradeoff between sampling number and model size Adaptive-Consistency [2]; ESC [175]; Speculative Rejection [311]; RASC [331]; Self-Calibration [122] Brown et al. [20]; Wu et al. [359] Improving the precision of response selection Lightman et al. [182]; Sun et al. [312]; MAV [181]; SC-GenRM [301] Inference-aware fine-tuning BoN-Aware [48] Selecting appropriate tree search algorithms Reducing the overhead of state estimation PG-TD [421]; ToolChain [441] AlphaLLM [323]; rStar-Math [96] Adaptive expansion breadth LiteSearch [332]; REBASE [359] Reducing redundant expansion nodes Constructing high-quality feedback Improving the refinement ability of LLMs FETCH [333]; ETS [113] Chiang and Lee [47]; G-Eval [194]; BSM [281]; Varshney et al. [328]; IoE [170]; Self-Correction [355]; REFINER [258]; AutoMathCritique [361] RISE [270]; SCoRe [157] Prompting for conciseness CCoT [235]; CoD [376]; BreakChain [64]; SoT [12] Finetuning on compressed responses using heuristic methods Dualformer [308]; ICoT-SI [61]; SPIRIT [55]; TokenSkip [362]; C3oT [144]; DistillSystem2To1 [393] Tree Search (4.2.3) Multi-turn Correction (4.3.3) n fi n c i p Learning on trajectories with predefined optimal length Deepseek-R1 [60]; TALE [102]; TOPS [384]; DAST [294]; Z1 [396] Long CoT (4.4.3) Query-aware compression Self-training SFT DPO RL Kimi K1.5 [153]; LMSkip [193]; Munkhbat et al. [234] Kimi K1.5 [153]; Overthinking [35]; Sky-T1-32B-Flash [175] L1 [3]; Kimi K1.5 [153]; O1-Pruner [209]; Concise RL [74]; Arora and Zanette [11]; MRL [269] Query router System-1.x [283]; Dynasor [81] Model merging CoT-Valve [215]; Kimi K1.5 [153]; Wu et al. [356] Compressing the intermediate state InftyThink [380]; MCoT [383]; AnLLMs [254]; LightThinker [418] Reasoning in the latent space ICoT-KD [62]; Coconut [105]; CODI [295]; Recurrence [88]; SoftCoT [378]; CCoT [39] Figure 7: Overview of methods of improving scaling efficiency. stopping once high-confidence response is observed (Huang et al., 2025a). Moreover, Speculative Rejection (Sun et al., 2024a) and ST-BoN (Wang et al., 2025e) propose sampling responses in parallel and halting the decoding of responses with low reward model scores or self-estimation consistency scores to improve efficiency. Tradeoff between sampling number and model size Given fixed inference computation budget, there exists tradeoff between using larger models with fewer samples versus smaller models with more samples, considering the different computational costs across model sizes. Brown et al. (2024) observe that larger models perform better on code tasks while smaller models are more effective for mathematical tasks. Wu et al. (2024c) further find that for mathematical tasks, while smaller models are optimal, they also saturate earlier as the inference computation increases. 14 4.2 Tree Search Table 1: An organization of works on tree search. This table includes inference-only approaches, while work combining training strategies is discussed in 5.3. Under State Evaluator, E1 denotes self-evaluation, E2 denotes specialized trained models, E3 denotes likelihood of actions, E4 denotes self-consistency score, E5 denotes roll out. Work Application Search Space State Evaluator Search Algorithm Pangu (Gu et al., 2023) PG-TD (Zhang et al., 2023) ToT (Yao et al., 2023a) GuidedDecoding (Xie et al., 2023) RAP (Hao et al., 2023) PPO-MCTS (Liu et al., 2023b) LATS (Zhou et al., 2023) ToolChain (Zhuang et al., 2023) MindStar (Kang et al., 2024a) (Wang et al., 2024b) LiteSearch (Wang et al., 2024a) MCTSr (Zhang et al., 2024b) REBASE (Wu et al., 2024c) SearchAgent (Koh et al., 2024) rStar (Qi et al., 2024) PLANSEARCH (Wang et al., 2024c) RethinkMCTS (Li et al., 2024d) SC-MCTS (Gao et al., 2024b) LLaMA-Berry (Zhang et al., 2024c) ETS (Hooper et al., 2025) Knowledge Base QA Code Game of 24, Writing, Crosswords Math Reasoning Alignment Programming, Reasoning Tool-use, Reasoning Math Math, Code Math Math Math Web agents Math Code Code Blocksworld Math Math Step Token Step Step Step Token Step Step Step Step Step Solution Step Step Step Step Step Step Solution Step E2 E5 E1 E1 E1, E3, E4 E2 E1, E4, E5 E1, E3, E4 E2 E2, E5 E2 E1 E2 E1 E4, E5 - E1, E5 E1, E3 E2 Beam Search MCTS BFS, DFS Beam Search MCTS MCTS MCTS A* BFS A* BFS MCTS BFS A* MCTS Beam Search MCTS MCTS MCTS BFS Improving the precision of response selection Considering the importance of effective selection mechanisms, several works focus on improving their precision in response selection. Lightman et al. (2023) find that PRM is superior to ORM in BoN settings. Sun et al. (2024b) find that the performance of weighted majority voting is superior to majority voting or BoN when using large sampling numbers. MAV (Lifshitz et al., 2025) employs multiple verifiers to assess response quality and achieves better performance than single verifier when the total computation budget for generator and verifiers is high. Inference-aware fine-tuning Chow et al. (2024) overcome the non-differentiable argmax operator within BoN sampling and develop BoN-Aware fine-tuning to directly optimize parallel sampling performance. 4.2 Tree Search 4.2.1 Key Components The tree search method frames problems as searches over tree structures. Guided by specific tree search algorithm, the generator searches in the search space and explore different problem-solving approaches, usually accompanied by value functions to assess node values in S. This framework enhances the models deliberate planning ability. We detail each component below. Search space The search space defines the granularity of tree nodes, which significantly impacts search efficiency. It can be categorized as follows: S1: Token. Token-level search increases the optimality of candidate solutions but also incurs high computational costs due to its fine-grained nature. This approach is suitable for scenarios with low tolerance for even minor errors in individual tokens. PG-TD (Zhang et al., 2023) implements the Monte Carlo Tree Search (MCTS) algorithm for code tasks at the token level, as even minor changes in code may cause errors. PPOMCTS (Liu et al., 2023b) employs token-level search methods to improve the helpfulness and harmlessness of responses. S2: Step. Step-level search balances search granularity and efficiency, making it the most common approach. The definition of step varies across different tasks. It can be sentences in solutions for reasoning problems (Yao et al., 2023a; Xie et al., 2023; Hao et al., 2023), actions in simulated world (Gu et al., 2023; Zhuang et al., 2023), lines of code (Wang et al., 2024c), or proposed plans or hypotheses (Yao et al., 2023a; Wang et al., 2024e, 2023b). S3: Solution. Solution-level search4 considers the expansion of tree nodes as updates to the whole response through actions like critique and revision (Zhang et al., 2024b,c). It overlaps with the multi-turn correction framework discussed later, and we also consider it as an ensemble of the two methods. 4It is noted in some works (Zeng et al., 2024) that they consider parallel sampling as solution-level tree search. However, for parallel sampling, the solution tree nodes only involve the original solutions and do not involve node expansion behaviors, which are vital for tree search. Therefore, we do not merge it. 15 4.2 Tree Search Value function The value function estimates the value of candidate nodes for further pruning or exploitation. The popular methods to construct value functions are as follows: E1: Self-evaluation. This method directly instructs the generator to evaluate node values through well-crafted prompts. ToT (Yao et al., 2023a) proposes to value each node independently or vote across nodes. Xie et al. (2023) design prompts in the form of multiple-choice questioning to better calibrate model predictions. E2: Specialized trained models. To reduce evaluation noise, this method utilizes specialized trained LLMs for evaluation (Gu et al., 2023; Kang et al., 2024a). This introduces process reward models (PRMs) for evaluating reasoning steps and token-level value functions for more fine-grained evaluation (Lee et al., 2024). For the PRM, it can be trained through the following approaches: 1) Human annotations: Lightman et al. (2023); Uesato et al. (2022) employ human labelers to label the correctness of each step. This method is costly and still cannot avoid noise in the training data; 2) Monte Carlo sampling: to achieve autonomous data annotation, this method employs Monte Carlo sampling that rolls out multiple completions from the current steps and estimates the rate leading to correct results (Wang et al., 2023a, 2024i; Havrilla et al., 2024; Luo et al., 2024). To improve sampling efficiency, OmegaPRM (Luo et al., 2024) utilizes binary search for error locating and integrates data collection into the search process; 3) From ORM to PRM: To avoid the high cost of training PRM, this method aims to derive PRM from an ORM. AutoPSV (Lu et al., 2024b) utilizes the ORM to automatically generate process annotations for each reasoning step by detecting its own confidence variations and thus uses the data for PRM training. Yuan et al. (2024a) theoretically demonstrate that PRM can automatically derive from an ORM through simple reward parameterization. In the PRM utilization phase, Setlur et al. (2024b) demonstrate that process reward for step should be advantages calculated by the difference of adjacent step values. For token-level value functions, these can directly come from the value functions in the post-training phase (Liu et al., 2023b) or training on data from Monte Carlo sampling (Lee et al., 2024). E3: Likelihood of actions. The likelihood-based approach utilizes the generators probability of conducting specific action (i.e., the tree node) to estimate the node value (Hao et al., 2023; Gao et al., 2024b). E4: Self-consistency score. The frequency of intermediate nodes can represent the models confidence in them, thus being used for evaluation (Qi et al., 2024; Zhuang et al., 2023; Zhou et al., 2023). LATS (Zhou et al., 2023) combines the self-generated LLM score and the self-consistency score for node value. rStar (Qi et al., 2024) utilizes the self-consistency score as the reward for the terminal node. E5: Roll out. In algorithms like MCTS, the intermediate node value can be estimated by rollout and further updated based on backup from terminal states (Qi et al., 2024). The reward for terminal states can come from external tools like code interpreters or the aforementioned evaluation methods. Search Algorithm The search algorithm defines the operational rules for tree nodes. It can be instantiated as follows: A1: Breadth-first Search (BFS). The widely used versions of BFS algorithm include beam search and A* algorithm. For the beam search algorithm, it generates candidate nodes at each layer and selects the most promising nodes from them based on node values (Gu et al., 2023; Yao et al., 2023a; Xie et al., 2023). For the A* algorithm (Hart et al., 1968), it calculates the sum of the cumulative cost (i.e., the cost from the root node to the current node) and the future score (i.e., the cost of the path from the current node to the goal) in the search process and always selects the node with minimum value. ToolChain (Zhuang et al., 2023) mainly relies on the heuristic function to calculate the score, and (Wang et al., 2024b) utilizes the process reward model and roll out method for estimation. A2: Depth-first Search (DFS). The DFS algorithm explores the most promising node first until the node is no longer promising or the final output is reached, then backtracks to the parent node to explore alternative thoughts (Yao et al., 2023a). Long (2023) implement the DFS algorithm in the sudoku puzzle, where checker module utilizes the sudoku rules to check the validity of the partial solution and controller controls the backtracking behaviors of LLMs. A3: Monte Carlo Tree Search (MCTS). line of work proposes using the advanced MCTS algorithm to improve the planning ability of LLMs (Zhang et al., 2023; Hao et al., 2023; Liu et al., 2023b), considering its success in AlphaGo (Silver et al., 2016). The key implementation of MCTS lies in four processes: selection, expansion, evaluation, and backup (see Figure 8 for the illustration). The selection phase traverses the tree from the root, iteratively selecting the most promising child nodes by balancing exploration and exploitation, 4.2 Tree Search Figure 8: Illustration of key components of tree search. until reaching an underexplored node. Widely used methods like Upper Confidence Bound applied on Trees Algorithm (UCT) (Kocsis and Szepesvari, 2006) and Predictor + UCT (Rosin, 2011) (PUCT) guide this choice, favoring nodes with high values while adjusting for visit frequency to prevent overconcentration on heavily explored nodes. The expansion operation grows the tree by adding one or more new child nodes to the underexplored node based on possible actions from the current nodes state. The evaluation process evaluates new child node by methods described in previous value function paragraph, such as rollouts to terminal state or direct evaluation with LLMs. In backup, the evaluation result is propagated upward along the selected path, updating the values and visit counts of all nodes traversed. Table 1 presents an organization of works on tree search based on the established taxonomy. 4.2.2 Scaling Laws Empirical results show that performance can be further enhanced by scaling the breadth and depth of tree search. This includes increasing the number of rollouts in the MCTS algorithm (Zhang et al., 2023; Liu et al., 2023b; Zhang et al., 2024b; Qi et al., 2024), the beam size in beam search (Yao et al., 2023a; Xie et al., 2023), and the step limitations in the A* algorithm (Zhuang et al., 2023). Snell et al. (2024) analyze the scaling behavior and finds that performance will eventually saturate. This may be due to the model struggling to produce diverse nodes as the number of sampled candidates increases. Additionally, Kang et al. (2024a) find that increasing the model size of the PRM employed in the search algorithm can enhance performance. This highlights the importance of improving the reliability of value functions through extra training time or test-time compute. 4.2.3 Improving Scaling Efficiency The strategies to improve the scaling efficiency of tree search are as follows: Selecting appropriate tree search algorithms The characteristics of different tree search algorithms make them suitable for different tasks. For code generation tasks, PG-TD (Zhang et al., 2023) compares MCTS using public test cases for terminal state evaluation against simple beam search, finding that MCTS achieves significantly better performance given the same computation time. ToolChain (Zhuang et al., 2023) demonstrates that the A* algorithm is more time-efficient than MCTS and other alternatives in API call tasks. Reducing the overhead of value functions For algorithms like MCTS, reliable value functions traditionally rely on multiple rollouts, which incur significant computational costs. ALPHALLM (Tian et al., 2024) employs smaller language model as fast rollout policy to reduce computational overhead. rStar-Math (Guan et al., 2025b) introduces two-phase approach: first estimating node values through rollouts, then using this data to train separate value function that replaces rollouts in subsequent iterations. 17 4.3 Multi-turn Correction Table 2: An organization of works on multi-turn correction. Fine-tuning indicates whether the method requires additional training. in the Self-feedback and Self-refinement columns represents that it shares the same parameters with the initial generator but is prompted with different roles. Work Self-Correction (Welleck et al., 2023) Self-refine (Madaan et al., 2023) Reflexion (Shinn et al., 2023) RCI (Kim et al., 2023) Self-Debug (Chen et al., 2023c) Baldur (First et al., 2023) REFINER (Paul et al., 2024) LLM-Debate (Du et al., 2023) MAD (Liang et al., 2023) CRITIC (Gou et al., 2023) CoVe (Dhuliawala et al., 2023) RISE (Qu et al., 2024) IHR (Qiu et al., 2023) SCoRe (Kumar et al., 2024) AutoMathCritique (Xi et al., 2024) DARS (Li et al., 2025d) Feedback Refinement Fine-tuning Self-feedback External Self-refinement External Game Envs; Interpreter; Oracle Oracle Interpreter Proof checker Trained LM Search engine; Interpreter Interpreter Trained LM Trained LM Trained LM Trained LM Trained LM Trained LM Trained LM Adaptive expansion breadth Traditional beam search algorithms fix the expansion breadth of each node to constant value, which may not optimally balance exploration and exploitation. LiteSearch (Wang et al., 2024a) allocates expansion breadth based on node value and depth, encouraging exploration on high-value nodes and at the beginning of the search process. This approach helps achieve higher token efficiency than beam search and DFS. REBASE (Wu et al., 2024c) takes similar strategy by defining trajectory collection requirements and dynamically allocating expansion breadth at each depth based on node value and remaining collection needs, resulting in higher efficiency compared to traditional MCTS algorithms. Reducing redundant expansion nodes In the node expansion process, there may exist nodes with semantically equivalent content, leading to unnecessary exploration costs. FETCH (Wang et al., 2025a) and ETS (Hooper et al., 2025) merge semantically similar nodes using agglomerative clustering of text embeddings obtained from fine-tuned model, achieving higher token efficiency. 4.3 Multi-turn Correction 4.3.1 Key Components Multi-turn correction aims to improve response quality through iterative revision. It consists of an initial generator g0 that proposes the initial response, feedback model that generates feedback for the latest response, and refinement model that revises the response given the interaction history (Welleck et al., 2024): y0 g0(yx) zt (zx, y(<t), z(<t)) yt g(yx, y(<t), z(t)) (7) (8) (9) where represents the query, yt represents the response at timestep t, and zt represents the feedback at timestep t. The system outputs the final response when stopping condition is met. The feedback generation stage can be omitted, resulting in direct refinement of the initial response (Welleck et al., 2023; Kamoi et al., 2024). Multi-turn correction imitates human reflection and refinement cognitive processes. The core design of it lies in constructing reliable feedback signals and refinement models to improve response quality. Feedback sources can be categorized as follows (Pan et al., 2023): F1: Self-feedback. The initial generator g0 and the feedback model can share single language model, resulting in self-feedback. For example, Self-Debug (Chen et al., 2023c) instructs g0 to explain code line by line and generate execution traces as feedback signals. Self-Refine (Madaan et al., 2023) incentivizes g0 to generate feedback using reflective prompts. Moreover, g0 can be prompted with different roles to encourage divergent thinking (Du et al., 2023; Liang et al., 2023; Khan et al., 2024), technique known as multi-agent debate. F2: External feedback. The feedback can come from external sources to g0, including: 1) external tools: such as code interpreters (Chen et al., 2023c; Gou et al., 2023; Shinn et al., 2023), proof checkers (First et al., 2023), game simulators (Shinn et al., 2023); 2) external knowledge (Gou et al., 2023; Zhao et al., 2023a); 18 4.3 Multi-turn Correction Figure 9: Illustration of key components of multi-turn correction. 3) oracle labels: such as ground truth answers to math problems (Shinn et al., 2023), though these are not guaranteed to be available in real-world applications (Huang et al., 2023b); 4) specialized trained models (Paul et al., 2024; Xi et al., 2024). The refinement model can also be instantiated similarly to the feedback models, including self-refining the response (Madaan et al., 2023; Shinn et al., 2023), or using specialized trained model (Welleck et al., 2023). Specifically, Self-Refine (Madaan et al., 2023) instantiates the initial generator, feedback model, and refinement model with the same language model. Table 2 presents an organization of works on multi-turn correction based on the established taxonomy. Research demonstrates that with reliable external feedback, multi-turn correction significantly enhances model performance across diverse tasks (Kamoi et al., 2024). However, this approach has faced criticism because high-quality external feedback is often unavailable in real-world scenarios (Huang et al., 2023b). In the intrinsic self-correction setting, where model critiques and revises its own responses without external feedback, empirical studies indicate that LLMs generally struggle to generate reliable critiques and revisions, particularly in planning (Valmeekam et al., 2023; Stechly et al., 2023) and reasoning (Huang et al., 2023b; Tyen et al., 2023) tasks, leading to little or no performance gains. It has also been observed that self-biases can amplify during the self-correction process (Xu et al., 2024c). 4.3.2 Scaling Laws In tasks with reliable external feedback or correctors, performance can be further improved by scaling the number of revision steps until it finally saturates (Welleck et al., 2023; Madaan et al., 2023; Du et al., 2023; Qiu et al., 2023). In intrinsic self-correction settings where the model lacks critique ability, increasing the revision steps can harm performance (Welleck et al., 2023; Huang et al., 2023b). This limitation can be addressed through additional training to improve self-correction ability (Qu et al., 2024; Snell et al., 2024). Snell et al. (2024) observe that after fine-tuning the model to improve its correction ability, performance grows steadily as revision steps increase and eventually saturates, even beyond the revision number used during training. This highlights that additional investment in training compute before deploying multi-turn correction can expand the ceiling of scaling test time. 4.3.3 Improving Scaling Efficiency As discussed above, the effectiveness of multi-turn correction is constrained by the reliability of feedback and the models refinement capabilities. To enhance scaling efficiency, efforts should concentrate on developing high-quality feedback mechanisms or improving the models refinement abilities. Constructing high-quality feedback Reliable feedback is often limited to specific task types. Several strategies can improve feedback quality for broader applications: 1) Using reference-free LLM-based evaluation metrics with human-written evaluation criteria (Chiang and Lee, 2023; Liu et al., 2023c); 2) Employing task-specific decomposition (Saha et al., 2024a) to break down complex verification into manageable subtasks; 3) Leveraging confidence estimation through generation probabilities (Varshney et al., 2023) or prompting techniques (Li et al., 2024c); 4) Fine-tuning models specifically for feedback (Welleck et al., 2023; Paul et al., 2024; Xi et al., 2024). 19 4.4 Long CoT Improving the refinement ability of LLMs Instead of focusing solely on constructing high-quality feedback, this line of work directly improves the refinement ability of models through additional training-time compute without the feedback models. RISE (Qu et al., 2024) generates synthetic multi-turn correction data by concatenating the incorrect response before the final correct response and fine-tunes the model on these examples to improve its refinement ability. SCoRe (Kumar et al., 2024) further identifies the behavior collapse issues in the SFT-based method and proposes multi-turn RL training with carefully designed rewards for different turns. 4.4 Long CoT 4.4.1 Key Components CoT prompting (Wei et al., 2022; Nye et al., 2021) instructs models to generate human-readable explanations of how problems are solved. This approach can help improve models representational complexity (Merrill and Sabharwal, 2023; Nowak et al., 2024) and significantly enhances their performance in reasoning tasks (Wei et al., 2022). Current models like ChatGPT or Llama 3.1 default to CoT when presented with reasoning problems (Sprague et al., 2024). Despite its widespread application, the reasoning process in CoT is usually shallow and linear, revealing limitations in complex cognitive capabilities (Kambhampati, 2024; Chen et al., 2025e). Recently, models like OpenAI o1 (OpenAI, 2024) or Deepseek R1 (DeepSeek-AI et al., 2025) have advanced the traditional CoT into long CoT, which incorporates more sophisticated thinking patterns and extended responses. The cognitive patterns present in long CoT but typically less observed in traditional CoT are as follows. Reflection: The model develops metacognitive abilities (Metcalfe and Shimamura, 1994) to assess the correctness and rationality of its own responses. For example, the model may pause its reasoning by outputting wait when it detects potential issues. Backtracking: When the model detects an error in its response, it can return to previous steps and revise them. This capability is vital for long-horizon planning problems, such as sudoku and code-breaking. In these problems, the model must find optimal solutions among multiple possibilities, and since the initial solution is not guaranteed to be correct, the model needs to employ trial and error. Verification: The model learns to recheck both individual steps and complete solutions, which enhances the robustness of its problem-solving approach. Divergent thinking: When the model recognizes that the current solution cannot solve the problem or leads to an obviously wrong answer, it can employ divergent thinking to explore alternative solutions, often signaled by transitional phrases like alternatively. Internal thinking: The model can generate human-like thinking processes beyond explicit problem-solving steps. This enables more fine-grained reasoning before generating each subsequent step, thereby improving its overall performance (Wu et al., 2024a). 4.4.2 Scaling Laws Early work demonstrates that extending reasoning steps significantly enhances LLMs reasoning capabilities (Jin et al., 2024). In the context of long CoT models, recent studies have identified positive correlation between token count and model performance. Although not explicitly describing token control methodologies, OpenAI (2024) and DeepSeek-AI et al. (2025) discover that performance increases with token count following log-linear relationship. More transparent research by Hou et al. (2025) and Muennighoff et al. (2025) applies response post-processing or decoding techniques to regulate token count, revealing positive association between response length and performance. Specifically, Hou et al. (2025) truncate responses to varying lengths from the beginning and suggest using summarization model to extract final answers. Muennighoff et al. (2025) develop budget-forcing technique to control token count through the addition or suppression of end-of-thinking token delimiters. Despite these studies providing substantial evidence for the positive correlation between token number and model performance, debate on the effectiveness of extensive response length remains. These debates primarily stem from observations that shorter response lengths yield higher accuracy than longer responses (Zeng et al., 2025b; Ballon et al., 2025). This phenomenon may be explained by models allocating greater budget to more challenging problems where failure risks are higher, or by approaches chosen in longer responses being more convoluted than those in shorter responses, thus increasing the likelihood of failure (Fatemi et al., 2025). 4.4.3 Improving Scaling Efficiency Although long CoT endows models with deep thinking abilities, it can lead to overthinking problems. For instance, models might generate hundreds of tokens for simple questions like 2+3=5 (Chen et al., 2024d), where the correct answer is reached early but followed by unnecessary reasoning. Furthermore, CoT-based methods operate in the 20 4.4 Long CoT language space, allocating similar computational resources to each token regardless of its importance. This uniform allocation is suboptimal since some tokens, like those maintaining text coherence, require minimal planning, while others crucial to the reasoning process demand more intensive processing (Hao et al., 2024b). We detail techniques to resolve these issues below.5 Prompting for conciseness This approach directly instructs models to limit response tokens to specific number (Nayab et al., 2024; Xu et al., 2025b) or capture only essential information (Ding et al., 2024; Aytes et al., 2025) through prompting. Although straightforward to implement, its effectiveness is limited to simple tasks, and LLMs cannot strictly adhere to token number restrictions (Muennighoff et al., 2025; Aggarwal and Welleck, 2025). Finetuning on compressed responses using heuristic methods This approach first compresses CoT responses using heuristic methods and then finetunes on them. The heuristic compression techniques include directly removing intermediate steps (Su et al., 2024; Deng et al., 2024), assessing token importance in CoT through perplexity(Cui et al., 2025b) or specifically trained model (Xia et al., 2025) to retain only the most relevant tokens, and leveraging advanced models like GPT-4 to reconstruct CoT sequences while preserving essential information and eliminating redundancy (Kang et al., 2024b). The effectiveness of this method heavily depends on the design of the heuristic compression techniques, limiting its generalizability across tasks. For example, C3oT (Kang et al., 2024b) finds that training directly on GPT-4-compressed data significantly degrades task performance, necessitating the inclusion of original uncompressed data during training. Query-aware compression The compression limitation of response length varies based on query type (Lee et al., 2025; Arora and Zanette, 2025), as difficult problems require more tokens while easier ones require fewer. This method aims to approach the limitation in query-aware way, which helps improve token efficiency while maintaining or improving the models adaptivity in computational resource allocation. The methods are as follows: Learning on trajectories with predefined optimal length: This approach first determines optimal length explicitly and trains on them. The reference for the optimal length can be based on task types. For example, in the supervised fine-tuning phase of DeepSeek-R1 (DeepSeek-AI et al., 2025), for reasoning tasks they collect long CoT responses for training, while for non-reasoning tasks they collect CoT responses for certain tasks and even no-CoT responses for simpler queries. These approaches help the model learn to switch reasoning modes based on the query type. Beyond this, other estimations for the optimal length can be based on search (Han et al., 2024; Yang et al., 2025a), prompting (Han et al., 2024), or query difficulty estimated by sampling (Shen et al., 2025b). These selected optimal-length trajectories can be used for further SFT or DPO training. Self-training: Instead of predefining the optimal length, this method first rolls out trajectories from the policy model and incentivizes the model to achieve fewer tokens while maintaining accuracy through self-training, which can be considered an on-policy optimal-length estimation. The training methods can be: 1) SFT: This approach generates multiple responses for each question and selects the shorter correct ones for supervised fine-tuning (Kimi et al., 2025; Munkhbat et al., 2025; Liu et al., 2024d); 2) DPO: This method uses the long-CoT model to generate multiple response samples, selecting the shorter correct solution as the positive sample while treating longer responses as negative samples. These positive-negative pairs form the pairwise preference data used for preference learning. For preference data construction, Chen et al. (2024d) find that choosing responses including two solving attempts that reach the correct answer as positive examples performs best. Sky-T1-32B-Flash (Li et al., 2024e) employs multiple preference data construction methods to avoid accuracy drops while reducing reasoning length. For the training algorithm, Chen et al. (2024d) empirically demonstrates that SimPO (Meng et al., 2024) performs better than DPO. 3) RL: This approach adds length penalty in the reward function to reduce response length (Aggarwal and Welleck, 2025; Kimi et al., 2025; Luo et al., 2025a; Arora and Zanette, 2025) or designs dense reward in the intermediate steps (Qu et al., 2025). For example, L1 (Aggarwal and Welleck, 2025) adds length control factor in the RL reward function to train the model to adhere to the length given in the prompt or not exceed the maximum length. Furthermore, MRL (Qu et al., 2025) measures progress at each intermediate generation episode through on-policy rollouts and develops corresponding SFT and RL methods for maximizing dense rewards based on the progress. While there is no explicit length-relevant factor in the algorithm, it helps the model balance exploration and exploitation in the content of CoT and improve token efficiency. Query router: This method classifies queries as difficult or easy and handles them differently by applying different types of models (Saha et al., 2024b) or different computation budgets of the same model (Fu et al., 2024b). For example, System-1.x (Saha et al., 2024b) trains controller that decomposes planning problem 5It is noted that some of the work focuses on traditional CoT instead of long CoT, but we include them considering their easy generalization. 4.5 Comparisons of Test-Time Scaling Methods Table 3: Comparisons of different test time scaling methods. Gray color represents the model is optional or can share the same parameters with others. The description of these features is for the standard version. Method Required Model Controllability Adaptivity Training-free Compatibility Parallel Sampling Tree Search Multi-turn Correction Policy model Scoring function Policy model Value function Initial generator Feedback model Refinement model Coarse-grained Not supported Coarse-grained Partial supported Coarse-grained Partial supported Long CoT Long-CoT model Not supported Supported Full Full Full Full into sub-goals and classifies them as easy or hard to be solved by either the System-1 planner or the System-2 planner. Model merging This method combines long-CoT model with short-CoT model to create new model without additional training. CoT-Valve (Ma et al., 2025b) manipulates the weights between the parameters of the two models to achieve varying lengths. Compressing the intermediate state In the response generation process, the storage overhead of the KV cache increases linearly with the context length for the Transformer architecture. This line of work aims to compress intermediate steps into shorter form and reason starting from it, continuing the compressing and generation process in the decoding phase. It helps to reduce the number of tokens stored in the context window, thereby lowering memory overhead and computational costs. This includes compressing intermediate steps into summary (Yan et al., 2025), subquestion (Yang et al., 2024c), or special token (Pang et al., 2024a; Zhang et al., 2025a) through specific training and corresponding inference strategies. Reasoning in the latent space Switching reasoning from the language space to other spaces like latent space may overcome the restrictions of language and improve token efficiency. This can be achieved by finetuning existing models to possess this capability (Deng et al., 2023; Hao et al., 2024b; Shen et al., 2025c) or developing new language model architectures capable of implicitly reasoning in latent space (Geiping et al., 2025). 4.5 Comparisons of Test-Time Scaling Methods For different test-time scaling methods, we summarize their characteristics in Table 3. Specifically, we focus on the following aspects: Performance What is the optimal test-time scaling method given the same computation budget? Establishing an absolute ranking of test-time scaling methods is challenging due to the various implementations within each approach and the difficulty in ensuring fair comparisons. For performance ceiling, long CoT methods consistently outperform other test-time scaling approaches based on traditional LLMs, particularly for olympic-level problems (OpenAI, 2024; DeepSeek-AI et al., 2025). Moreover, different test-time scaling methods exhibit distinct advantages for problems of varying difficulty and under different computational constraints. For instance, Snell et al. (2024) empirically demonstrate that beam-search excels on complex questions when operating under limited computation budgets, whereas BoN sampling achieves superior performance on simpler questions when greater computational resources are available. These complementary strengths create opportunities for ensemble methods, which will be discussed in subsequent sections. Cognitive behaviors Which test-time scaling method possesses the most cognitive behaviors similar to humans? Long CoT exhibits the most cognitive behaviors compared to others, including reflection, backtracking, divergent thinking, etc. More importantly, it unifies these cognitive behaviors in the generation process, enabling greater flexibility. Methods like tree search and multi-turn correction rely on external tree search algorithms or predefined multi-turn correction frameworks to endow the model with planning or reflection cognitive capability, limiting their adaptation to specific problems. Adaptivity Can the test-time scaling method allocate different computational resources to different queries? The degree of adaptivity of test-time scaling method depends on its stopping condition. In parallel sampling approaches, the standard implementation assigns identical sampling numbers across all queries, resulting in lack of adaptivity. For tree search and multi-turn correction approaches, different cases exist. One variant of methods 22 4.6 Ensemble of Test-Time Scaling Methods stops once reaching predefined hyperparameters (e.g., correction numbers, tree depth) or the answers (Yao et al., 2023a; Kang et al., 2024a; Snell et al., 2024), thus providing no additional adaptivity from the framework. Another line of methods incorporates verifiers in the stop condition, such as requiring the quality score of outputs to exceed given threshold (Wang et al., 2024a; Welleck et al., 2023), which introduces adaptivity based on the reliability of these verifiers. For example, LiteSearch (Wang et al., 2024a) observes that tree search algorithms allocate larger computational resources for harder problems where stopping conditions include verifier values. For long CoT methods, the stopping condition is implicit and inherent in the generation process. Recent studies observe that the long CoT model generates longer responses to more challenging problems (Zeng et al., 2025b). From the perspective of generalization, long CoT is the most promising approach for differentially allocating computational resources. Controllability Given computation budget, can it operate within the specified constraints? For test-time scaling methods with externally controllable scaling dimensions (e.g., sampling numbers, tree depth, revision steps), coarse-grained controllability can be achieved by mapping the computation budget to specific quantities of these hyperparameters according to empirical estimation (Welleck et al., 2024). For long CoT, although directly truncating the response to specific number ensures not exceeding the computation budget, the resulting incomplete response significantly harms performance, making it impractical to consider standard long CoT as method with controllability. To address this limitation, S1 (Muennighoff et al., 2025) achieves control of response length through the implementation of end-of-thinking token delimiters, while L1 (Aggarwal and Welleck, 2025) develops reinforcement learning algorithm to achieve precise control over token number with higher token efficiency compared to S1. Simplicity Are the components of the test-time scaling method straightforward to implement? Methods excluding long CoT usually require additional roles such as evaluators to guide the search process and multiple processes to derive the final solutions. Considering the extra cost to deploy high-quality evaluators for most tasks, this may hinder their practical application. In contrast, the long CoT method eliminates the need for multiple components and is straightforward to implement. Training-free Does the test-time scaling method require additional training? Methods excluding long CoT can be operated with the traditional LLM directly, while the long CoT ability needs to be elicited with additional training. It is notable that additional training can help improve scaling efficiency across methods, such as enhancing the self-correction ability of models (Kumar et al., 2024; Qu et al., 2024) or applying inference-aware fine-tuning to improve computation utilization (Chow et al., 2024; Yu et al., 2025c). Compatibility Can this method be integrated with other test-time scaling methods? As will be discussed in 4.6, all methods can be compatible with each other. Among them, parallel sampling is most easily compatible with others considering the ease of implementing multiple sampling. Overall, the long CoT test-time scaling method outperforms others with its simplicity, adaptivity, higher ceiling performance, and more complex cognitive behaviors, but it requires additional training to elicit. Moreover, the compatibility and advantages of these test-time scaling methods make it beneficial to comprehensively utilize them together to achieve better performance instead of focusing on single method. 4.6 Ensemble of Test-Time Scaling Methods Ensemble methods aim to comprehensively utilize multiple test-time scaling approaches rather than allocating computational resources to single method, potentially achieving superior performance compared to individual approaches. These include simultaneously combining multiple methods or selecting appropriate test-time scaling methods according to different contexts. Figure 10 presents an organization of works on ensemble methods. Combining parallel sampling with other methods The simplicity of parallel sampling facilitates compatibility with other test-time scaling methods: Tree search. Instead of searching along single tree, parallel sampling can enhance tree search diversity by expanding the initial set of beams into multiple independent subtrees that are searched independently (Beeching et al., 2024; Bi et al., 2024). Empirical results demonstrate improved tree search performance, especially at large computation budgets. Moreover, tree search algorithms can also help accelerate parallel sampling. For example, TreeBON (Qiu et al., 2024) reduces the computational overhead of BoN by using tree search to prune low-quality responses at an early stage. Multi-turn correction. While parallel sampling functions as global search by generating responses independently in parallel, multi-turn correction operates as local search on the initial response (Snell et al., 2024). This complementarity indicates that combining the two methods can yield better performance. Kumar et al. 4.6 Ensemble of Test-Time Scaling Methods et [157]; al. et [100] Olausson Kumar [302]; [243];PA [27]; TS al. SE et al. Snell 5 ] C riti [ 3 parallel sampling ] 0 9 2 ; 3 ; 2 1 4 [ long CoT l t . [ 3 0 2 ] ; N [ 2 6 7 [ ] ; 6 ] ; T [ 1 ] . [ 1 9 0 ] ; W E-Reasoner[217] multi-turn correction MCTSr [414]; LLaMA-Berry [415]; MC-NEST [272]; SPAR [40]; RethinkMCTS [171] tree search Figure 10: Ensemble of Test-Time Scaling Methods (4.6). Solid lines represent combination work between two connected methods. (2024); Chen et al. (2025b) show that allocating portion of the computation budget to self-correction of the initial response rather than solely increasing the sampling number can achieve higher token efficiency. Olausson et al. (2023) demonstrate that in the mixed scheme, allocating more of the sampling budget to generating diverse set of initial candidates is more optimal than carrying out extensive correction. Long CoT. Combining long CoT with parallel sampling methods such as majority voting is straightforward. Recent research has optimized majority voting strategies by considering the overthinking phenomenon in long CoT (Zeng et al., 2025b; Cuadron et al., 2025). Specifically, high overthinking correlates with decreased performance in math tasks or agentic environments. Therefore, integrating metrics that measure the degree of overthinking with voting strategies can outperform both majority voting and single high-computation-cost response generation (Zeng et al., 2025b; Cuadron et al., 2025). Additionally, Setlur et al. (2025) compare the performance of scaling long-CoT length by budget-forcing (Muennighoff et al., 2025) against applying the computation for parallel sampling with shorter responses, finding the latter to be more compute-optimal. Combining tree search with multi-turn correction This line of work incorporates critique and revision into the tree search algorithm by treating the revision behavior as the action to update the response, at either the solution level (Zhang et al., 2024b,c; Rabby et al., 2024; Cheng et al., 2024a) or the step level (Li et al., 2024d). This approach enriches the expansion behavior of tree search and helps achieve better performance than simply revising responses sequentially (Li et al., 2024d; Cheng et al., 2024a). Combining long CoT with tree search or multi-turn correction The content of long CoT implicitly contains branch search processes or self-correction (Xiang et al., 2025). Thus, it can be viewed as method that internalizes these two approaches. For the combination with multi-turn correction, Tang et al. (2025) show that o1-mini benefits from self-correction while traditional LLMs perform worse, demonstrating that long CoT models possess strong intrinsic self-correction ability. For tree search, future research should analyze how to define the search space within the long thinking processes. Adaptive selection of test-time scaling methods Empirical analysis of different test-time scaling methods performance relative to various factors can help derive optimal test-time scaling methods based on adaptive selection. Snell et al. (2024) find that multi-turn correction methods are better suited for simpler queries, while certain ratio of parallel sampling and multi-turn correction is appropriate for difficult queries. Moreover, they determine that beam search is more effective for harder questions whereas best-of-N is more effective for easier questions. These findings guide optimal test-time scaling strategies based on query difficulty classifiers. Liu et al. (2025c) analyze the relationship between model size and test-time scaling methods to derive an optimal scaling strategy."
        },
        {
            "title": "5 How – Part II: Training Strategies for Test-Time Scaling",
            "content": "As discussed in 4.5, long CoT demonstrates higher ceiling performance and more complex cognitive behaviors compared to other test-time scaling strategies, though it requires additional training. In this section, we examine methods to elicit the models long CoT capabilities through two primary approaches: reinforcement learning (5.1) and supervised fine-tuning (5.2). Additionally, we discuss how to effectively combine test-time scaling techniques with iterative training methodologies to achieve self-improvement (5.3). 5.1 Scaling Reinforcement Learning Recent research demonstrates that training LLMs through online reinforcement learning with rule-based rewards in tasks like mathematics and code can significantly enhance their reasoning abilities (DeepSeek-AI et al., 2025; Kimi et al., 2025). During the training process, models autonomously learn to master long-CoT test time scaling methods to solve challenging problems and demonstrate cognitive behaviors including self-reflection and self-correction. This phenomenon has been described as the RL scaling phenomenon6 or the Aha moment. We systematically summarize recent works in Table 4. Additionally, Table 6 presents recipes to address common challenges in RL scaling training based on recent studies. In the following sections, we detail the design considerations for each component. 5.1.1 Training Algorithm REINFORCE The REINFORCE (Sutton et al., 1999) algorithm is foundational policy gradient method in reinforcement learning that directly optimizes the expected return of policy through gradient ascent. The algorithm optimizes the policy model πθ by minimizing the loss: LREINFORCE(θ) = Eτ πθ (cid:35) Gtθ log πθ(atst) (cid:34) (cid:88) t=1 (10) where Gt is the discounted cumulative reward from time step t. Despite its simplicity, REINFORCE suffers from high variance in gradient estimates. Proximal Policy Optimization (PPO) For the PPO algorithm (Schulman et al., 2017), it optimizes the policy model by minimizing the loss: LPPO(θ) = EqP (Q),oπθold (Oq) 1 O (cid:88) t=1 min (cid:18) πθ(otq, o<t) πθold (otq, o<t) (cid:19) At, clip(θ)At clip(θ) = clip (cid:18) πθ(otq, o<t) πθold (otq, o<t) (cid:19) , 1 ε, 1 + ε (11) (12) where πθ and πθold are the current and old policy models, and q, are the sampled questions and outputs. The clip(θ) function constrains policy updates to ensure stable training. At is the advantage computed by applying GAE (Schulman et al., 2016) based on the rewards rt and learned value function Vψ. The KL penalty can be added to the reward function: rt = rφ(q, ot) β log πθ(otq, o<t) πref (otq, o<t) (13) where rφ is the reward model, πref is the reference model (initial SFT model), and β is the coefficient of the KL penalty. Group Relative Policy Optimization (GRPO) The GRPO algorithm (Shao et al., 2024) directly uses the average reward of multiple parallel sampled responses as the baseline, eliminating the need for additional value function approximation as in PPO. Specifically, for each question q, GRPO samples group of outputs {o1, o2, , oG} from the old policy πθold and then optimizes the policy model πθ by minimizing the loss: LGRPO(θ) = qP (Q),{oi}G i=1πθold (Oq) 1 (cid:88) i=1 clip(θ) = clip Oi (cid:88) (cid:20) t=1 min 1 Oi (cid:18) πθ(oi,tq, oi,<t) πθold (oi,tq, oi,<t) (cid:18) πθ(oi,tq, oi,<t) πθold (oi,tq, oi,<t) (cid:19) , 1 ε, 1 + ε ˆAi,t, clip(θ) ˆAi,t DKL[πθπref ] = πref (oi,tq, oi,<t) πθ(oi,tq, oi,<t) log πref (oi,tq, oi,<t) πθ(oi,tq, oi,<t) 1 6In the paper, we use RL scaling to describe the line of work. 25 (cid:19) βDKL[πθπref ] (cid:21) (14) (15) (16) 5.1 Scaling Reinforcement Learning Table 4: Summary of recent works on RL scaling. For training algorithms, PMD denotes policy mirror descent method. REINFORCE denotes REINFORCE-style method. For reward types, (cid:204) and represent rule-based and model-based rewards respectively, while and represent outcome and process rewards respectively. #D indicates the query dataset size. MS denotes the multi-stage training strategy, including long CoT cold start (LCS), iterative lengthening strategy (IL), and curriculum sampling strategy (CSS). In accuracy (Acc.) and length (Len.) figures, for works presenting multiple figures, we show the common pattern. Cog. indicates whether the response contains words indicating cognitive behaviors like wait. Work Algorithm (5.1.1) Reward (5.1.2) Series (5.1.3) Size (5.1.3) #D (5.1.4) MS (5.1.5) Acc. Len. Cog. Eurus-2-7B-PRIME (Cui et al., 2025a) REINFORCE (cid:204) 7B 150K - LCS CSS 671B - - - Deepseek-R1-Zero (DeepSeek-AI et al., 2025) GRPO Kimi k1.5 (Kimi et al., 2025) SimpleRL-Zero (Zeng et al., 2025a) SimpleRL (Zeng et al., 2025a) STILL-3-ZERO-32B (Chen et al., 2025g) Sea AI Lab (Liu et al., 2025f) PMD PPO PPO GRPO PPO DeepScaleR-1.5B-Preview (Luo et al., 2025c) GRPO (cid:204) (cid:204) (cid:204) (cid:204) (cid:204) (cid:204) (cid:204) 7B 8K 7B 8K LCS 32B 90K IL 1.5B 8K 1.5B 40K IL T1 (Hou et al., 2025) REINFORCE (cid:204) 14B 30K LCS DAPO (Yu et al., 2025a) LIMR (Li et al., 2025f) Open-Reasoner-Zero (Hu et al., 2025) GRPO GRPO PPO (cid:204) (cid:204) (cid:204) 32B 17K 7B 1.4K 7B 32B 57K Logic-RL (Xie et al., 2025) REINFORCE (cid:204) 7B 5K - - - - - 5.1 Scaling Reinforcement Learning Table 5: Comparisons of different training algorithms. For the computational overhead, represents the model needs to be updated, (cid:11) indicates the model needs to perform inference. RL Scaling represents whether RL scaling phenomena have been successfully observed with this algorithm. Algorithm Computational Overhead RL Scaling Policy Reward Critic Reference REINFORCE PPO GRPO REINFORCE++ (cid:11) (cid:11) (cid:11) (cid:11) (cid:11) (cid:11) (cid:11) (cid:11) (cid:11) (cid:11) (cid:11) (cid:11) where ε and β are hyper-parameters, and ˆAi,t is the advantage computed using group of rewards corresponding to the outputs within each group. REINFORCE++ REINFORCE++ (Hu, 2025) is variant of the classical REINFORCE algorithm that integrates key optimization techniques from PPO while eliminating the need for critic network. The algorithm incorporates several enhancements to address the limitations of REINFORCE as follows: It implements token-level KL divergence penalty to prevent the policy from deviating too far from the initial model. It adopts PPOs clipping mechanism to constrain policy updates and maintain stability during training. It introduces mini-batch updates for improved training efficiency and better convergence rates. It employs comprehensive reward normalization and clipping to stabilize training by mitigating outliers and constraining reward values within predefined bounds. It implements advantage normalization using z-score normalization to ensure stable gradients and prevent divergence during training. Comparisons with different algorithms We summarize the characteristics of different training algorithms in Table 5. Regarding computational cost, PPO shows predominant computational cost with four models to be loaded, among which the policy model and the critic model need to perform both inference and training. GRPO and REINFORCE++ eliminate the need for critic model and achieve higher training stability than REINFORCE (Hu, 2025). Regarding performance, all algorithms except REINFORCE exhibit the RL scaling phenomenon. For specific performance comparisons, Hou et al. (2024) find that the performance of PPO and GRPO is similar in RLHF settings, while Xie et al. (2025) observe that the performance of PPO and REINFORCE++ is superior to GRPO in rule-based reward settings for synthetic logic puzzles. More rigorous and large-scale studies should be conducted to comprehensively evaluate the performance of these algorithms. 5.1.2 Reward Function The reward types can be categorized according to their source and granularity as follows: Model-based reward: In traditional RLHF (Ouyang et al., 2022) settings, an explicit reward model is learned from human preference data and guides the optimization process in RL training. The explicit reward model can be omitted by directly training on human preference data, resulting in an implicit reward model (Rafailov et al., 2023). (cid:204) Rule-based reward: The term rule-based represents rewards that are well-defined and can be determined by explicit rules, sometimes also termed verifiable rewards. For example, for math problems with ground truth answers or code tasks with unit tests, response correctness can be easily verified and thus used to construct the reward. This can be further extended to include response format or language consistency. Even when verification is automated using specialized model to check answer equivalence (Chen et al., 2024b; Kimi et al., 2025), we still attribute it to rule-based reward as long as the models performance closely matches ideal rule verification. Outcome reward: In general settings, the rule-based reward or model-based reward is only given to the last token of the response, termed outcome reward. 27 5.1 Scaling Reinforcement Learning Table 6: Recipes to resolve common problems in RL scaling training based on recent studies. Problem to Solve Method Overview Evidence Related Studies TRAINING ALGORITHM Computational inefficiency in traditional PPO for LLM training GRPO (Group Relative Policy Optimization): Eliminates the need for separate value model by using the average reward of multiple outputs from the same prompt as the baseline for advantage calculation. Performance comparisons demonstrate computational efficiency while maintaining comparable effectiveness to traditional PPO, particularly well-suited for LLM reward modeling where rewards are often comparative in nature. Token inefficiency and overthinking in long-form reasoning Dr.GRPO (Doctor GRPO): Addresses optimization bias in GRPO by removing response-length normalization and reward standardization, implementing an unbiased policy gradient estimation. Experimental results show significantly improved token efficiency with better controlled response lengths, effectively mitigating overthinking problems. Instability with varying response lengths in long-form reasoning DAPO (Decouple Clip and Dynamic Sampling Policy Optimization): Implements token-level policy gradient calculation, allowing longer sequences to appropriately influence the gradient updates regardless of individual response lengths. Comparative analysis reveals more stable training dynamics with healthier entropy management and better quality pattern recognition, particularly for handling varying response lengths effectively. Limited policy exploration due to rigid constraints GPG (Group Policy Gradient): Simplifies the policy gradient approach by removing reference models and policy constraints while maintaining stability through group-level reward normalization. Comparative experiments demonstrate enhanced exploration capabilities with reduced computational requirements, providing more flexible policy updates. Repetitive or narrow reasoning patterns Auxiliary entropy bonus: Incorporates an additive entropy term into the RL loss function to encourage token diversity and prevent deterministic response patterns. Limitations of fixed reference models On-policy KL normalization: Combines KL normalization with Exponential Moving Average (EMA) updates to the reference model. Experimental results show more varied and creative reasoning paths without sacrificing solution accuracy. Dynamic reference model updating allows for more effective RL scaling while maintaining stable training dynamics. Value model misalignment with strong prior policies Value-Pretraining Alignment: Implements dedicated pretraining phase for the value model to ensure alignment with strong prior policies before RL begins. Two-stage convergence pattern shows initial range alignment followed by crucial knowledge injection, preventing collapse in output length for long-CoT tasks. Conflicting variance-bias requirements between value and policy optimization Decoupled-GAE (Generalized Advantage Estimation): Separates the λ parameter for value function and policy optimization, allowing unbiased value estimation while maintaining variance reduction benefits for policy updates. Mathematical analysis and experimental results demonstrate improved convergence rates without introducing additional bias, particularly effective for trajectory-level rewards in long CoT tasks. GRPO [291] Dr.GRPO [201] DAPO [394] GPG [50] T1 [116] T1 [116] VC-PPO [402], VAPO [404] VC-PPO [402], VAPO [404] Limited exploration in constrained policy optimization KL Divergence Removal: Eliminates the KL penalty term that constrains policy divergence from the reference model, allowing the reasoning policy to explore more freely. Experiments reveal significant performance gains when removing constraints on policy distribution shifts during extended reasoning training. Open-ReasonerZero [120], DAPO [394] Premature deterministic behavior in RL systems Clip-Higher Strategy: Decouples lower and higher clipping ranges in PPO to specifically promote exploration of low-probability tokens while maintaining stability. Asymmetric clipping thresholds effectively counteract entropy collapse and maintain policy diversity throughout extended training. Ineffective gradient signals in late-stage training Dynamic Sampling: Implements an adaptive sampling approach that filters out prompts with accuracy values of exactly 0 or 1 to ensure effective gradient signals. Comparative training curves demonstrate faster convergence to target performance despite the additional computational overhead of oversampling. Noisy reward signals from length-truncated samples Overlong Filtering: Masks the loss contribution of truncated samples that exceed maximum length to prevent inappropriate penalization of otherwise sound reasoning. Ablation studies highlight substantial training stability improvements when removing noisy reward signals from length-truncated samples. Inconsistent advantage estimation across variable-length sequences Length-Adaptive GAE: Dynamically adjusts the λ parameter in GAE based on sequence length, ensuring balanced TD-error influence for both short and long outputs. Empirical tests reveal more balanced advantage estimation and improved training stability across sequences of varying lengths, particularly beneficial for long-form reasoning. DAPO [394] DAPO [394], Bae et al. [14] DAPO [394] VAPO [404] REWARD DESIGN Uncontrolled CoT length in reasoning tasks Cosine Length Reward: Applies cosine-based reward shaping that prioritizes shorter, correct CoTs while penalizing short, incorrect ones. Evaluation across diverse reasoning tasks reveals stabilized CoT length with preserved performance. Reward hacking in deterministic reasoning tasks Accuracy+Format Reward: Combines verification of answer correctness with structured formatting requirements that enforce explicit reasoning within specialized tags. Rule-based reward systems demonstrate greater resistance to reward hacking than neural alternatives while simplifying the training pipeline. Demysitify [392] DeepSeek-R1 [60], SimpleRL [410], T1 [116], Logic-RL [365], STILL-3 [38] Language mixing issues in multilingual environments Language Consistency Incentive: Calculates rewards based on the proportion of target language words in the CoT to mitigate language mixing issues. Model overthinking and verbosity Overthinking Length Penalty: Implements weighted reward mechanism that penalizes excessive response length while preserving correctness to combat model overthinking. User studies indicate enhanced readability despite minor performance trade-offs in multilingual contexts. DeepSeek-R1 [60] Gradually introduced length penalties resulted in more token-efficient reasoning. KIMI-K1.5 [153], DAPO [394] Inaccurate reward modeling in nuanced domains Chain-of-Thought RM: Enhances reward modeling with explicit step-by-step reasoning before final correctness judgment, particularly for domains with nuanced evaluation criteria. Manual verification confirmed that CoT reward models achieved significantly higher accuracy compared to classic reward models without reasoning steps. KIMI-K1.5 [153] TRAINING DATA Resource-constrained RL training environments High-impact Sample Selection: Prioritizes training samples based on learning impact measurement. Results show significant reduction in required training data while maintaining performance. LIMR [173] Training with noisy web-extracted data Noise Reduction Filtering: Employs filtering mechanisms to remove noisy web-extracted data. Filtered datasets demonstrate improved generalization capabilities on OOD tasks. Demysitify [392] MULTI-STAGE TRAINING Poor readability and reasoning in direct RL approaches Cold-start Progression: Implements phased training approach beginning with high-quality CoT data fine-tuning before transitioning to large-scale reinforcement learning. Models with cold-start initialization exhibit enhanced readability and reasoning capabilities compared to direct RL approaches. Inefficient training with problems of varied difficulty Strategic Sampling: Combines curriculum-based progression from simple to complex problems with prioritization of difficult cases where model performance is weakest. Targeted sampling approaches demonstrated faster convergence and more efficient use of computational resources during training. Inefficient use of context in long-form reasoning Progressive Context Scaling: Implements multi-stage training approach that gradually increases context window size as model performance begins to plateau at each level. Phased context window expansion demonstrates significant improvements in both computational efficiency and final performance metrics compared to fixed maximum context training. DeepSeek-R1 [60], T1 [116], DeepscaleR [212], STILL-3 [38] KIMI K1.5 [153] DeepscaleR [212] Performance gaps on challenging reasoning problems Targeted Annealing: Implements final training phase on specifically mined challenging problems with linearly decaying learning rate to refine reasoning capabilities. Enhanced performance metrics on complex reasoning tasks without compromising general capabilities. Open-ReasonerZero [120] 5.1 Scaling Reinforcement Learning math; code search writing research (cid:204) Rule-based Reward Model-based Reward Task Non-verifiability w m u w s r Task type: Verifiable tasks Task type: General tasks Advantages: Mitigates reward Advantages: Universal hacking applicability Disadvantages: Restricted to tasks with clear verification criteria Disadvantages: Prone to reward hacking Task type: Step-verifiable tasks Advantages: Improves process quality; Mitigates reward hacking Disadvantages: Extremely limited task types Task type: Multi-step reasoning tasks Advantages: Improves process quality Disadvantages: Prone to reward hacking; Less effective in RL than parallel sampling Reward Granularity General tasks Limited tasks Rare tasks Figure 11: Comparisons of different reward types. Colors indicate applicable task scope. Process reward: In multi-step reasoning tasks, the outcome reward may not be sufficient to supervise the policy model and help avoid logic errors in the solutions (Shao et al., 2024; Lightman et al., 2023). This necessitates more fine-grained rewards for each step, termed process reward, which are typically calculated in model-based way. We detail the construction of process reward models in 4.2.1. Besides constructing process reward models, recent work also explores other ways to help achieve more accurate credit assignment. For example, Kazemnejad et al. (2024) replace the value networks in the PPO algorithm with unbiased Monte Carlo-based estimates. Hwang et al. (2024) and Setlur et al. (2024a) introduce MC-based methods to detect key errors in reasoning chains for use as ad-hoc mechanisms in DPO. Figure 11 presents comparison of different reward types. We detail the discussion below. Rule-based reward vs. model-based reward: The model-based reward can be applied to general tasks but also easily leads to reward hacking problems. This pipeline of constructing preference data to learn reward model to proxy human preference can be applied to general tasks, leading to its widespread adoption. However, it has been observed that the reward is an imperfect proxy in the training process. There are two prevailing explanations for this phenomenon (Rafailov et al., 2024): 1) OOD Robustness: the reward function is continuously queried using unseen model samples which are potentially out-of-distribution, and 2) Reward Mis-specification: learned reward functions may exhibit spurious correlations that cause them to prefer unintended behaviors. These issues lead to reward overoptimization problems where, during the training process, while the proxy reward score monotonically increases, the golden reward score will saturate and then decrease (Gao et al., 2023). Although this issue can be alleviated by improving the reward models capability through increased scale or training data (Ouyang et al., 2022; Hou et al., 2024) or iteratively retraining the reward model to improve its supervision of the policy model (Shao et al., 2024), the phenomenon still exists and hinders the success of large-scale RL (DeepSeek-AI et al., 2025). Outcome reward vs. process reward: The fine-grained process reward may help improve the RL performance, but also introduces reward hacking problems. Empirical results show that process rewards can help improve RL performance compared to using only outcome rewards (Cui et al., 2025a; Shao et al., 2024). However, it still faces several challenges: 1) the construction of high-quality process rewards requires significant labor; 2) an imperfect process reward model can be easily hacked. For example, Gao et al. (2024a) find that repeating correct but unnecessary reasoning steps can lead to high rewards from process reward model. Although these issues can be addressed through reward refinement, it complicates the RL pipeline; 3) process rewards show less significant improvements in RL training than in parallel sampling settings. In parallel sampling settings, empirical results show that process reward models significantly outperform outcome reward models (Lightman et al., 2023; Wang 29 5.1 Scaling Reinforcement Learning et al., 2023a) in response selection. However, the gain is not as pronounced in RL settings (Gao et al., 2024b; Cui et al., 2025a; Shao et al., 2024). Optimization for rule-based reward Rule-based rewards for eliciting long CoT reasoning primarily consist of correctness rewards and format rewards for specific tags. While this approach has proven sufficient for RL scaling, it can lead to potential content misalignment problems due to its narrow focus on accuracy. Two main issues arise from this approach. First, it may result in poor readability and inconsistent language use. Deepseek-R1 (DeepSeek-AI et al., 2025) addresses these challenges by initially fine-tuning their model on thousands of carefully selected long CoT examples. Additionally, it introduces language consistency reward during RL training to mitigate language misalignment issues. Second, this approach may lead to excessive response length, potentially causing overthinking problems. To address this, Kimi k1.5 (Kimi et al., 2025) implements length penalties in the later training stages, while T1 (Hou et al., 2025) penalizes responses that either exceed the context window size or contain repetitive n-grams. The success of RL in verifiable tasks demonstrates the importance of robust reward signals. As more research into RL scaling strengthens its theoretical and empirical foundation to facilitate implementation, it decouples the RL training process into two distinct steps: first defining verifiable rewards and then conducting RL training, as partially implemented in OpenAIs Reinforcement Fine-Tuning Service.7 Search-R1 (Jin et al., 2025) utilizes simple outcome reward function that verifies the correctness of final answers to conduct RL training and successfully endows LLMs with the ability to autonomously generate search queries during step-by-step reasoning with real-time retrieval, showcasing the power of RL beyond math and code. For future work in fields like open scientific questions, constructing reliable reward signals remains an open challenge and offers significant potential for innovation. 5.1.3 Policy Model Selection The policy model is prerequisite for successful RL training. The selection criteria can be based on the following aspects: Model Family As shown in Table 4, most RL scaling work utilizes Qwen2.5 as the base model. Recent studies demonstrate that Qwen2.5 exhibits cognitive behaviors such as verification and correction in its problem-solving process before applying RL (Gandhi et al., 2025; Liu et al., 2025g,f), although the model cannot effectively use them. This indicates that the models pretrained knowledge already contains these thinking patterns. Gandhi et al. (2025) investigate this phenomenon based on the observation that Qwen-2.5-3B exhibits substantial gains while Llama-3.2-3B quickly plateaus under identical RL training conditions for the game of Countdown. When Llama is primed with synthetic reasoning traces containing these behaviors or pretrained on cognitive behavioral augmentation data, it shows substantial improvements during RL, matching Qwens performance trajectory. This highlights the importance of pretraining on corpus containing the cognitive behaviors before conducting RL. Model Size While traditional RLHF settings show that larger models gain fewer benefits from reinforcement learning optimization (Gao et al., 2023; Hou et al., 2024), RL scaling settings demonstrate that larger models achieve higher token efficiency and thus better performance (Kimi et al., 2025). The limited success in reproducing DeepSeek-R1-Zeros (671B) scaling behavior in 7B or smaller models for challenging tasks without long CoT cold start further suggests that model size significantly impacts scaling behavior. 5.1.4 Training Data Construction The quality and quantity of training data significantly affect the efficiency and upper bound of RL. Data Quality Eliminating easy queries that require no further training helps save the unnecessary computation cost of RL as post-training technique, where query difficulty can be estimated by sampling multiple times from the policy model to calculate the success rate for correct answers (Kimi et al., 2025; Chen et al., 2025g). Similarly, it is also beneficial to remove problems for which the current model lacks the fundamental capability to solve (Chen et al., 2025g). From the training perspective, queries that the model consistently answers correctly or incorrectly introduce the gradient-decreasing problem. DAPO (Yu et al., 2025a) proposes dynamic sampling strategy that over-samples and filters out prompts with accuracies of 1 and 0, observing significant performance gains, which can be considered an online difficulty control method. Data Quantity In traditional RLHF settings, scaling the prompt quantity does not lead to significant performance improvements (Hou et al., 2024). However, this conclusion does not hold for RL scaling scenarios. Open-ReasonerZero (Hu et al., 2025) investigates the performance discrepancy between 7.5K MATH training set and their curated 57K prompt set, finding that the larger set leads to continuous scaling in both accuracy and response length, while the smaller set plateaus. Similarly, DeepSeek-R1-Zero observes continuous performance improvements using their large-scale curated dataset (DeepSeek-AI et al., 2025). 7https://openai.com/form/rft-research-program/ 30 5.2 Supervised Fine-tuning 5.1.5 Multi-stage Training Training efficiency can be enhanced by employing the following multi-stage training strategy: Long CoT Cold Start Fine-tuning on long CoT data before RL training can facilitate subsequent RL improvements (Yeo et al., 2025) and mitigate early instability issues during RL training (DeepSeek-AI et al., 2025). Additionally, enhancing the quality of long CoT significantly amplifies RL gains (Yeo et al., 2025). Furthermore, Li (2025) demonstrates improved performance by incorporating sparse updates and adaptive termination mechanisms into the SFT loss function, which helps preserve response diversity after training. Iterative Lengthening Strategy DeepScaleR-1.5B-Preview (Luo et al., 2025c) initially restricts the context window size to 8K, during which the model generates shorter responses while training rewards increase. Upon reaching critical point where model responses begin to lengthen, the context window size is expanded to 16K and subsequently to 24K (see the DeepScaleR-1.5B-Preview row in Table 4). This strategy guides controlled response length expansion while reducing computational costs. Curriculum Sampling Strategy When allocating restricted computation budget in the initial training phase to very challenging problems, this often yields few correct samples, resulting in lower training efficiency. To address this limitation, the curriculum sampling strategy begins with training on simpler tasks before progressively advancing to more complex ones. Kimi K1.5 (Kimi et al., 2025) reports enhanced performance by implementing this curriculum sampling strategy, leveraging their training dataset that naturally incorporates grade and difficulty labels. Similarly, logic-RL (Xie et al., 2025) examines the utility of this approach but finds that improvements are not substantial in logic puzzles tasks, concluding that it is necessary to balance the complexity of staged training against potential performance gains. 5.2 Supervised Fine-tuning Recent work demonstrates that long CoT test-time scaling behavior can be elicited through simple supervised fine-tuning on similar data (Muennighoff et al., 2025; Ye et al., 2025). This approach is promising given its simpler training process and higher data efficiency compared to RL-based methods. Table 7 presents an organization of long CoT resources. We detail the core design considerations of SFT-based methods as follows: Training data source The data sources can be categorized based on synthesized data or distillation from existing long CoT models. The trajectory synthesis method includes directly collecting trajectories from tree or graph search processes (Lehnert et al., 2024; Gandhi et al., 2024; Ye et al., 2024) in tasks like path finding or formal logic problems. However, the limited tasks and cognitive behaviors hinder its wider application. Another line of work first solves problems using test-time scaling methods such as tree search processes and multi-turn correction, then translates the search history into thorough exploration trajectories (Zhao et al., 2024; Ma et al., 2025a). For example, Journey Learning (Qin et al., 2024) proposes first guiding LLMs to solve problems using tree search, then using another LLM to translate the backtracking or evaluation steps into natural language to form the trajectory. However, the lack of logical coherence and diversity in these trajectories limits their performance. Xi et al. (2024) transfer multi-turn correction processes into self-talk data, but this approach struggles to generalize to challenging problems due to LLMs limitations in critique. In contrast to the complexity of synthesis methods, the distillation method directly extracts trajectories from open-source long CoT models, such as Deepseek R1 or QwQ (Ye et al., 2025; Muennighoff et al., 2025; Li et al., 2025c). While this method is cost-effective and performs well compared to synthesis methods, the nature of distillation makes it difficult for student models to surpass their teacher models (Huang et al., 2024). Training data quality The quality of long-CoT data significantly determines its effectiveness in eliciting model reasoning ability. It comprises both query quality and response quality. For queries, they should be challenging to the base model and cover diverse domains. LIMO (Ye et al., 2025) applies multi-stage filtration process and retains queries that are challenging even for state-of-the-art reasoning models. S1 (Muennighoff et al., 2025) maintains difficult queries based on model performance and reasoning trace length while covering diverse subjects. For responses, they can be post-filtered through answer checkers and code interpreters. For example, OpenThoughts-114k (Team, 2025a) with verifiers for filtering demonstrates higher performance than OpenThoughtsUnverified-173k, and the precision of verifiers affects the performance. Regarding response content, Li et al. (2025c) find that the global reasoning structure matters more than local content details through perturbation experiments. Training data quantity Empirical results show that scaling data quantity does not bring expected performance improvements relative to computational cost. S1 (Muennighoff et al., 2025) compares the curated 1k dataset with the 59k-full dataset and finds that performance gains are limited. Similarly, the modest performance gap between OpenThoughts-114k (Team, 2025a) and carefully curated 1k datasets further supports this observation. LIMO (Ye et al., 2025) attributes this phenomenon to the Less-Is-More Reasoning Hypothesis, which suggests that the training 31 Quantity Modality Link 5.2 Supervised Fine-tuning Table 7: An organization of long CoT resource. Work Application Type O1 JourneyPart 1 [264] Marco-o1 [431] Math Reasoning Synthesize Synthesize STILL-2 [230] Math, Code, Science, Puzzle Distillation Source GPT-4o Qwen2-7B-Instruct DeepSeek-R1-Lite-Preview QwQ-32B-preview RedStar-math [375] RedStar-code [375] RedStar-multimodal [375] Math Code Math Distillation QwQ-32B-preview Distillation QwQ-32B-preview Distillation QwQ-32B-preview S1K [233] S1K-1.1 [233] LIMO [391] Math, Science, Code Distillation Gemini Flash Thinking Math, Science, Code Distillation DeepSeek R1 Math Distillation DeepSeek R1 DeepSeekR1-Distill-Qwen-32B OpenThoughts-114k [319] Math, Code, Science, Puzzle Distillation DeepSeek R1 OpenR1-Math-220k [72] Math Distillation DeepSeek R1 OpenThoughts2-1M [319] Math, Code, Science, Puzzle Distillation DeepSeek R1 CodeForces-CoTs [319] Code Distillation DeepSeek R1 Sky-T1-17k [167] Math, Code, Science, Puzzle Distillation QwQ-32B-Preview S2R [214] Math Synthesize Qwen2.5-Math-7B R1-Onevision [385] Science, Math, General Distillation DeepSeek R1 OpenO1-SFT [318] Math, Code Medical-o1 [28] O1 JourneyPart 3 [129] Medical Medical Synthesize Distillation Distillation - Deepseek R1 o1-preview SCP-116K [207] Math, Science Distillation Deepseek R1 open-r1-multimodal [71] Math Distillation GPT-4o Vision-R1-cold [127] Science, Math, General Distillation Deepseek R1 MMMU-Reasoning-DistillValidation [232] Science, Math, General Distillation Deepseek R1 0.3K 10K 5K 4K 16K 12K 1K 1K 0.8K 114K 220K 1M 47K 17K 3K 155K 77K 25K 0.5K 116K 8K 200K 0.8K Clevr-CoGenT [29] Vision Counting Distillation Deepseek R1 37.8K VL-Thinking[26] Science, Math, General Distillation Deepseek R1 Video-R1[75] Video Distillation Qwen2.5-VL-72B Embodied-Reasoner [422] Embodied AI Synthesize GPT-4o OpenCodeReasoning [4] SafeChain [135] KodCode [379] Code Safety Code Distillation Distillation Distillation DeepSeek R1 Deepseek R1 DeepSeek R1 158K 158K 9K 736K 40K 2.8K Text Text Text Text Text Vision Text Text Text Text Text Text Text Text Text Text Vision Text Text Text Text Text Vision Text Vision Text Vision Text Vision Text Vision Text Vision Text Vision Text Text Text Text data primarily serves to elicit sophisticated reasoning capabilities inherent in the model rather than to teach new knowledge. Training methods Whether the self-correction and backtracking ability can be learned through parameterefficient fine-tuning such as LoRA (Hu et al., 2022) is still under exploration. Li et al. (2025c) compare the performance of LoRA fine-tuning with full parameter fine-tuning and find that the performance is close. This observation contradicts the conclusion of Ye et al. (2024) that the self-correction pattern cannot be learned with LoRA fine-tuning, though the latter only conduct experiments on the synthesized dataset using GPT2-small. More studies should be conducted to verify the effectiveness of LoRA fine-tuning. Base models The performance gain of different base models from long CoT fine-tuning varies significantly (Li et al., 2025c). Li et al. (2025h) find small models ( 3B parameters) do not consistently benefit from long CoT 32 5.3 Iterative Self-reinforced Learning Algorithm 1: Iterative Self-reinforced Learning Input: Original training set D0 = {xi, yi}, original query set Dquery 0 = {xi}, M0: initial policy model, : iteration times, R0: initial reward model, Sample(.): policy sample function, Synthesizequery(.): query synthesis function, Updatepolicy(.): policy update function, Updatereward(.): reward model update function, Filter(.): filter function for = 0 to 1 do t1 , Mt) i=1 s.t. xi Dquery = Synthesizequery(Dquery j=1)Nt Dquery j}Ni Dt = {(xi, {yi Annotate Dt with reward model Rt Dfilter = Filter(D1, D2, ..., Dt) Mt+1 = Updatepolicy(Mt, Dfilter ) Rt+1 = Updatereward(Rt, Dfilter ) , {yi j}Ni j=1 Sample(Mt, xi)} end reasoning and instead perform better when fine-tuned on shorter reasoning chains. They attribute this to the limited domain knowledge of small models and demonstrate that models with more domain knowledge perform better than those without. Future work should quantitatively analyze the relationship between performance and characteristics of base models. Although the SFT-based method is easier to implement and more cost-effective compared to RL-based methods, it has several potential limitations. First, the success of the SFT method in eliciting long CoT reasoning ability largely depends on existing open-source long CoT models trained through RL, highlighting the reliance on the teacher models. This characteristic suggests that SFT and RL methods should be combined for higher data efficiency. For example, in the training process of Deepseek-R1, they employ multi-stage training methods with interleaved RL and SFT training, fully utilizing the advantages of both approaches. Second, the SFT-based method is often criticized for memorizing fixed patterns rather than achieving true generalization (Chu et al., 2025a; Mirzadeh et al., 2024; Zhang et al., 2024d). Although empirical results show this criticism may not always hold true as models after small-data SFT can still improve their performance in other subjects and domains (Ye et al., 2025), future studies should carefully analyze the relationship between SFT training steps and generalization ability. 5. Iterative Self-reinforced Learning The trajectories generated from test time scaling methods can be utilized to optimize the policy model through offline methods such as SFT or DPO, thereby achieving self-improvement. This framework functions as selfreinforcing cycle where data is first generated, then leveraged for learning, after which the original policy model is replaced by its enhanced iteration. We term this training paradigm iterative self-reinforced learning (ISRL). An outline of this algorithm is provided in Algorithm 1, and an organization of relevant works is presented in Table 8. The core steps of the algorithm are detailed as follows: Sampling First, responses are sampled from the policy model using controlled sampling function, which can be implemented with the aforementioned test time scaling methods, including parallel sampling (Zelikman et al., 2022; Gulcehre et al., 2023; Dong et al., 2023), tree search (Feng et al., 2023; Tian et al., 2024; Zhang et al., 2024a), and multi-turn correction (Xiong et al., 2025; Jain et al., 2025). To enhance the diversity and quality of candidate responses, STaR (Zelikman et al., 2022) provides the correct answer as hint to guide the generation of rationales when the model fails to solve the problem independently. ReST-MCTS (Zhang et al., 2024a) empirically demonstrates that step-level tree search outperforms parallel sampling, as the step-level search improves the quality of intermediate reasoning steps. Additionally, other research explores methods to increase query diversity, such as using few-shot prompting to synthesize new problems (Haluptzok et al., 2023; Yuan et al., 2024c). Scoring The sampled responses can be scored using the following methods: 1) for tasks like math and code, the generated solution can be verified against ground truth answers (Zelikman et al., 2022) or validated with unit tests (Huang et al., 2023a); 2) for general tasks, an off-the-shelf reward model can be utilized to score the responses (Dong et al., 2023), or the policy model itself can serve as judge (Yuan et al., 2024c); 3) for tree search algorithms, accompanying scores from the sampling process help select the correct solution or construct preference pairs (Guan et al., 2025b; Zhang et al., 2024f; Xie et al., 2024b); 4) majority voting. The majority voting strategy can help determine the correct answer when the ground truth is unavailable (Huang et al., 2023a). Selection and Update The response pool accompanied by scores is further selected and utilized to update the policy model and optionally the reward model. The policy model can be updated using SFT (Zelikman et al., 2022; Table 8: An organization of works on iterative self-reinforced learning. IT denotes whether iterative training is involved. Under Sampling, Query denotes whether new queries are synthesized, Response denotes the sampling method. For the Scoring column, GT denotes ground truth, CI denotes code interpreter, MV denotes majority voting, RM denotes model-based reward model or LLM-as-a-judge. Under the Selection & Update, Algorithm represents the training algorithm for the policy model, Model represents the training model of each turn (Orig. denotes the original model, Curr. denotes the current model in the iteration). Data represents the source of training data (Curr. represents data from the current turn, Orig. represents the original data, Prev. represents data from all previous turns). RM represents whether the reward model gets updated in the process. Work IT Sampling Scoring Selection & Update Query Response Algorithm Model Data RM STaR (Zelikman et al., 2022) P3 (Haluptzok et al., 2023) LMSI (Huang et al., 2023a) RAFT (Dong et al., 2023) RFT (Yuan et al., 2023) ReST (Gulcehre et al., 2023) ReSTEM (Singh et al., 2023) Self-Rewarding (Yuan et al., 2024c) V-STaR (Hosseini et al., 2024) IRPO (Pang et al., 2024b) Qwen2.5-MATH (Yang et al., 2024a) Process-SelfRewarding (Zhang et al., 2025c) TS-LLM (Feng et al., 2023) ALPHALLM (Tian et al., 2024) AlphaMath (Chen et al., 2024a) ReST-MCTS (Zhang et al., 2024a) MCTS-IPL (Xie et al., 2024b) CPL (Wang et al., 2024e) SRA-MCTS (Xu et al., 2024a) rStar-Math (Guan et al., 2025b) Xiong et al. (Xiong et al., 2025) µCODE (Jain et al., 2025) SPaR (Cheng et al., 2024a) SWE-Reasoner (Ma et al., 2025d) Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel MCTS MCTS MCTS MCTS MCTS MCTS MCTS MCTS Multi-turn correction Multi-turn correction Ensemble Long CoT GT CI MV RM GT RM GT RM GT GT GT; RM RM GT RM GT GT GT; RM GT; RM SFT; Step-APO SFT SFT SFT SFT SFT SFT SFT DPO SFT DPO SFT DPO SFT SFT SFT SFT DPO RM GT GT RM RM GT SFT SFT SFT SFT DPO SFT Orig. Curr. Curr. Curr. Orig. Curr. Orig. Curr. Orig. Curr. - Curr. Curr. Curr. Curr. Curr. Curr. Curr. Curr. Curr. Curr. Curr. Curr. Curr. Curr. Curr. Curr. Curr. Curr.; Orig. Curr.; Prev. Curr. Curr. Curr.; Prev. Curr. - Curr. Curr. Curr. Curr. Curr. Curr. Curr. Curr. Curr. Curr. Curr. Curr. Curr. Gulcehre et al., 2023; Singh et al., 2023) or DPO (Yuan et al., 2024c; Pang et al., 2024b). The reward model can also be updated, such as updating the process reward model on labels generated by rollouts in the tree search process (Feng et al., 2023; Zhang et al., 2024a) or training an outcome reward model on generated positive and negative samples (Hosseini et al., 2024; Yang et al., 2024a). For example, V-star (Hosseini et al., 2024) trains an ORM to utilize negative samples generated in the sampling process and uses it for reranking during inference time. During the update process, the policy model for next iteration data generation can be fine-tuned from either the initial model or the model in the current iteration. The training data can originate from various sources: the current turn, the initial dataset, or accumulated data from all previous turns. Although ISRL is promising considering its data efficiency empowered by offline methods and the advantage of not requiring expert demonstrations, empirical results show that the rate of improvement tends to plateau or even decline slightly after 4-5 iterations (Wu et al., 2024b). This phenomenon contrasts with RL scaling methods where model performance improves monotonically. One major distinction between the two algorithms is that ISRL is closer to off-policy sampling, where in each iteration, the training data collection and utilization are processed separately. As demonstrated by Tajwar et al. (2024), higher degree of on-policy sampling leads to better performance. This is further supported by the observation of Shao et al. (2024) that the performance of online rejection sampling fine-tuning (RFT) is comparable to RFT in the early stage of training but gains significant advantage in the later stage. Moreover, for algorithms that employ SFT for policy updates, they do not utilize negative gradients for pushing down certain responses. Empirical results show that including negative gradients in policy updates leads to significantly better performance compared to using positive gradients alone, especially in reasoning tasks (Kimi et al., 2025)."
        },
        {
            "title": "6 How’s Progress – Application So Far",
            "content": "In this section, we examine the systemic changes in AI research that cognition engineering driven by test-time scaling brings and the applications that have already emerged. 34 6.1 Mathematics Table 9: Application of Test Time Scaling in different domains. Under the Long CoT, italics represents traditional CoT work, Yellow color represents using RL tech, Purple color represents using SFT tech. Green color represents combining test time scaling with iterative training, i.e., the iterative self-reinforced learning in the paper. Application Parallel Sampling Tree Search Multi-turn Correction Long CoT Mathematics Self-Consistency [344]; ORM [52]; PRM800K [182]; Math-shepherd [339]; STaR [409]; V-STaR [114]; IRPO [255]; Process-Self-Rewarding [420] GPT-f [261]; HTPS [159]; BFS-Prover [370]; ToT [388]; MindStar [143]; RAP [104]; [334]; Self-Evaluation Guided [368]; TS-LLM [77]; LiteSearch [332]; ALPHALLM [323]; AlphaMath [25]; MCTSr [414]; ReST-MCTS [413]; REBASE [359]; rStar-Math [96]; LLaMA-Berry [415] RISE [270]; SCoRe [157]; AutoMathCritique [361] Code MBR-EXEC [297]; CodeT [24]; [166]; AlphaCode [177]; AlphaCode2 [316] PG-TD [421]; o1-Coder [425]; [334]; SWE-Reasoner [217]; PLANSEARCH [335]; RethinkMCTS [171]; SRA-MCTS [373] Multimodality URSA [213]; PARM [100] VisVM [372]; LLaVA-CoT [374]; Mulberry [386]; LlamaV-o1 [322]; Video-T1 [184] Reflexion [298]; Self-Debug [37]; CRITIC [89]; IHR [268]; Self-Repair [243]; STOP [408] R3V [41]; Insight-V [67]; PARM [100]; GoT [73] Openai o1 [318]; O1 JourneyPart1 [264]; O1 Journey-Part2 [128]; STILL-2 [230]; T1 [116]; Deepseek-R1 [60]; Kimi k1.5 [153]; SimpleRL [410]; S1 [233]; LIMO [391]; Demystifying [392]; LIMR [173]; DeepScaleR [212]; QwQ [321]; DAPO [394]; Eurus-2-7B-PRIME [54]; STILL-3 [38]; Open-Reasoner-Zero [120]; VAPO [404]; Open-RS [56] Deepseek-R1 [60]; Kimi k1.5 [153]; SWE-RL [353]; SWE-Gym [251]; OpenAI o1 [245]; QwQ-preview [320]; QwQ [321]; SWE-Reasoner [217]; OpenCodeReasoning [4]; ToRL [174]; DeepCoder [211]; Seed-Thinking-v1.5 [287] MAmmoTH-VL [98]; Virgo [68]; QVQ-72B-Preview [271] ; Open-R1-Multimodal [71]; R1-Multimodal-Journey [221]; R1V [29]; LMM-R1 [259]; VLM-R1 [293]; R1-Video [342]; R1-Onevision [385]; MM-Eureka [222]; Vision-R1 [127]; VisualThinker-R1-Zero [438]; MAYE [216]; Visual-RFT [293]; Seg-Zero [197]; vsGRPO [180]; Video-R1 [75]; MVoT [165]; Kimi-VL [154]; Kimi k1.5 [153]; O3/O4-mini [248; 249] ReAct [389]; Deep Research [247]; SWE-RL [353]; Operator [246]; UI-TRARS [266]; PC Agent [108]; DeepResearcher [434]; SWEET-RL [439]; Claude 3.7 Sonnet [9] Embodied-CoT [228]; CoA [169]; SpatialCoT [196]; RAD [51]; Cosmos-Reason1 [13]; Gemini Robotics [317]; CoT-VLA [427]; Embodied-Reasoner [422] Agent [262]; ToT [388]; SearchAgent [156] Reflexion [298]; Agent [262]; Agent-Eval-Refine [252] Agent Embodied AI - - Safety SelfCheckGPT [220]; SRG [337] RAG CoRAG [338] - STAIR [424] ; C-MCTS [257]; HaluSearch [42]; InferenceGuard [133]; ARGS [150] AirRAG [76]; CoRAG [338] Evaluation CCE [419]; SPCT [202] MCTS-Judge [348] ChatEval [23]; ScaleEval [44] Inner Monologue [126]; REFLECT [198]; KnowNo [277] MART [87]; Combat Adv. Attacks [45]; Improve Factuality [69]; Multi-expert Prompting [204]; DebateGPT [309] Deliberate Alignment [94]; SafeChain [135]; Chain-of-Verification [63]; MoTE [199] - IterDRAG [405]; Plan*RAG [329]; DeepRAG [95]; Search-o1 [172]; AirRAG [76]; Auto-RAG [395]; CoRAG [338]; Search-R1 [137]; R1-Searcher [303]; ReSearch [32] FActScore [229]; FacTool [43]; RefChecker [121]; RAGChecker [279]; Agent-as-a-Judge [442]; EvalPlanner [282]; Kim et al. [152] 6.1 Mathematics Mathematical reasoning is crucial for resolving complex problems and making informed decisions (Hendrycks et al., 2021; Xia et al., 2024). Research in AI for mathematics (AI4Math) has developed along two complementary paths: natural language reasoning focusing on questions with verifiable answers, and formal language reasoning utilizing formal systems like Lean (De Moura et al., 2015) and Isabelle (Nipkow et al., 2002) for automatic formal theorem proving. For questions with verifiable answers, the easy verification characteristic makes it reliable to construct feedback signals for search and learning, facilitating the wide application of test-time scaling methods to enhance reasoning abilities. These include parallel sampling (Cobbe et al., 2021; Wang et al., 2023c), tree search (Feng et al., 2023; Chen et al., 2024a; Hao et al., 2023), multi-turn correction (Kumar et al., 2024; Qu et al., 2024), and long CoT (DeepSeek-AI et al., 2025; OpenAI, 2024). Notably, powered by long CoT, DeepSeek-R1 achieves score of 79.8 on the American Invitational Mathematics Examination (AIME), significantly outperforming traditional models without long CoT and approaching competitive human performance. For formal language reasoning, formal systems make the reasoning process verifiable and provide signals for tree search (Polu and Sutskever, 2020; Lample et al., 2022; Xin et al., 2024, 2025) or multi-turn correction (First et al., 2023). Breakthrough systems like AlphaProof (AlphaProof and teams, 2024) and AlphaGeometry (Trinh et al., 2024) demonstrate that combining neural networks with formal methods and proof checkers can achieve unprecedented mathematical reasoning 35 6.2 Code Test Time Scaling Methods Parallel Sampling Tree Search Multi-turn Correction Long CoT (CoT) Training Strategy SFT DPO RL Inference-only GPT-f [261] ORM [52] STaR [409] Self-Consistency [344] HTPS [159] BFS-Prover [370] PRM800K [182] ToT [388] Self-Evaluation Guided [368] RAP [104] TS-LLM [77] 2020/09 2021/10 2022/03 2022/ 2022/05 2022/05 2023/05 2023/05 2023/05 2023/ 2023/09 REBASE [359] RISE [270] LiteSearch [332] MCTSr [414] [334] MindStar [143] AlphaMath [25] ALPHALLM [323] IRPO [255] 2024/08 2024/ 2024/07 2024/06 2024/06 2024/05 2024/05 2024/ 2024/04 V-STaR [114] 2024/02 Math-shepherd [339] 2023/12 SCoRe [157] Openai o1 [318] LLaMA-Berry [415] O1 Journey-Part1 [264] O1 Journey-Part2 [128] AutoMathCritique [361] STILL-2 [230] rStar-Math [96] ReST-MCTS* [413] Eurus-2-7B-PRIME [54] T1 [116] 2024/09 2024/ 2024/10 2024/10 2024/11 2024/11 2024/12 2025/ 2025/01 2025/01 2025/01 DAPO [394] Process-SelfRewarding [420] QwQ [321] DeepScaleR [212] LIMR [173] Demystifying [392] LIMO [391] S1 [233] SimpleRL [410] Kimi k1.5 [153] Deepseek-R1 [60] 2025/03 2025/03 2025/03 2025/ 2025/02 2025/02 2025/02 2025/02 2025/01 2025/ 2025/01 STILL-3 [38] Open-Reasoner-Zero [120] VAPO [404] 2025/03 2025/ 2025/04 Figure 12: Works of applying test-time scaling methods in the math field. abilities. Despite these successes, there still exists room for improvement. In natural language reasoning, while training data accumulation is substantial, the difficulty in strictly verifying reasoning process correctness means solutions generated by LLMs may contain logical errors or lack rigor in intermediate steps (Lightman et al., 2023; Xia et al., 2024). For formal language reasoning, while it ensures reasoning process verifiability, the lack of training data compared to natural language limits its development. Future work can focus on unifying the advantages of formal and natural languages for more robust model development. Moreover, although LLMs with strong reasoning and cognition abilities have made progress on exam problems and competition tasks, applications to more advanced domains such as mathematical research remain relatively unexplored (Yang et al., 2024b). This necessitates not merely enhanced model capabilities but also novel evaluation frameworks to assess these competencies. Future Direction for Mathematics Unify the advantages of formal and natural languages to develop more robust reasoning models that combine the verifiability of formal systems with the rich training data available in natural language. Expand applications of LLMs with strong reasoning capabilities beyond exam problems toward more advanced domains such as mathematical research, developing novel evaluation frameworks to assess these higher-level competencies. 6.2 Code The swift emergence of coding capabilities in language modelsexemplified by Codex (Chen et al., 2021) and AlphaCode (Li et al., 2022; Team, 2024a), has transformed software development and boosted productivity. Several studies (Fu and Khot, 2022; Ma et al., 2023; Shao et al., 2024) even suggest that code enhances model intelligence. Moreover, coding is now core feature of general-purpose foundation models (DeepSeek-AI, 2024), highlighting its critical role in modern model development. Previously, approaches in code synthesis and code generation have demonstrated how execution verification enables both scalable and verifiable training and test-time signals, laying the foundation for subsequent advancements (Le et al., 2022; Chen et al., 2023a; Zhu et al., 2024). Additionally, directly prompting LLMs to self-reflect, debug, and generate tests for coding tasks represents another direction of test-time scaling (Shinn et al., 2023; Chen et al., 2023c), which enhances model performance in specific downstream scenarios while also providing feedback for refining training strategies (Gu et al., 2024a). These innovations are believed to be instrumental in developing stronger reasoning models, such as the o1-series, which achieve state-of-the-art performance on elite programming benchmarks like SWE-bench (Jimenez et al., 2024) and even human-competitive platforms such as Codeforces.8 8https://codeforces.com 6.3 Multimodality Test Time Scaling Methods Parallel Sampling Tree Search Multi-turn Correction Long CoT (CoT) Training Strategy SFT DPO RL Inference-only AlphaCode [177] MBREXEC [297] CodeT [24] PG-TD [421] Reflexion [298] Self-Debug [37] 2022/ 2022/04 2022/07 2023/03 2023/03 2023/04 CRITIC [89] 2023/05 Self-Repair [243] STOP [408] AlphaCode2 [316] CRUXEval [91] 2023/ 2023/10 2023/12 2024/01 Kimi k1.5 [153] DeepSeek-R1 [60] SWE-Gym [251] o1-Coder [425] QwQ-Preview [320] SRA-MCTS [373] RethinkMCTS [171] PLANSEARCH [335] OpenAI o1 [245] [334] IHR [268] 2025/01 2025/01 2024/11 2024/ 2024/11 2024/11 2024/09 2024/09 2024/09 2024/ 2024/04 S1 [233] [166] SWE-RL [353] QwQ [321] ToRL [174] SWE-Reasoner [217] DeepCoder [211] Seed-Thinking-v1.5 [287] 2025/01 2025/02 2025/ 2025/03 2025/03 2025/03 2025/04 2025/04 Figure 13: Works of applying test-time scaling methods in the code field. This is further exemplified by o1 and o3 model variants earning gold medals at the 2024 International Olympiad in Informatics (IOI), where reinforcement learning on challenging programming tasks is combined with cognitively aligned human guidance (El-Kishky et al., 2025). Still, critical challenges remain. First, while code executability facilitates verification, naive execution poses security risks, necessitating robust sandboxing solutions (Hui et al., 2024; Liu et al., 2024c). This also requires the development of infrastructure for the practical deployment of these safety mechanisms. Second, frequent reflection behaviorsoften referred to as overthinkingcan degrade performance in certain tasks, as observed in coding agentic benchmarks like Aider.9 Third, the reliability of execution-based feedback remains an open question. While DeepSeek-R1 (DeepSeek-AI et al., 2025) relies on execution results as feedback signals to achieve superior performance, code that passes unit tests can still fail with additional tests, leading to false positives (Stroebl et al., 2024), highlighting fundamental limitation in execution-based evaluation. Furthermore, further investigation is also needed to align models with real-world coding tasks, as motivated by SWE-Arena (team swe arena, 2025) and Copilot-Arena (Chi et al., 2025). These challenges may even intersect with broader issues such as multi-modal understanding and agentic capabilities which is further discussed in the next few sections. Future Direction for Coding Go beyond competition-level programming to encompass real-world software development tasks and advance toward automatic code review, debugging, and repo-level optimization. Expand to support more programming languages and continuously learn newly released libraries to achieve expert-level proficiency. 6.3 Multimodality Test Time Scaling Methods Parallel Sampling Tree Search Multi-turn Correction Long CoT (CoT) Training Strategy SFT DPO RL Inference-only R3V [41] LLaVA-CoT [374] Insight-V [67] VisVM [372] MAmmoTH-VL [98] Mulberry [386] QvQ-Preview [271] LlamaV-o1 [322] MVoT [165] Kimi k1.5 [153] Virgo [68] 2024/ 2024/11 2024/11 2024/11 2024/12 2024/12 2024/ 2025/01 2025/01 2025/01 2025/01 Vision-R1 [127] VisualThinkerR1-Zero [438] R1-Video [342] Visual-RFT [293] VLM-R1 [293] R1-MultimodalJourney [221] R1V [29] Open-R1Multimodal [71] PARM [100] PARM [100] URSA [213] 2025/03 2025/03 2025/ 2025/02 2025/02 2025/02 2025/02 2025/01 2025/ 2025/01 2025/01 Seg-Zero [197] LMM-R1 [259] MM-Eureka [222] GoT [73] R1-Onevision [385] Video-T1 [184] Video-R1 [75] vsGRPO [180] MAYE [216] Kimi-VL [154] O3/O4-mini [248; 249] 2025/03 2025/03 2025/03 2025/03 2025/ 2025/03 2025/03 2025/04 2025/04 2025/04 2025/ Figure 14: Works of applying test-time scaling methods in the multi-modal field. 9https://aider.chat/ 37 6.3 Multimodality Test-Time Scaling for Multimodal Understanding Vision-Language Models (VLMs) (Zhang et al., 2024e) have demonstrated significant potential in tasks involving multimodal understanding and generating textual outputs. Test-time scaling techniques for VLMs can be directly adapted from LLMs since the task outputs are textual. DeepSeek R1 represents significant milestone, being the first to fully demonstrate the effectiveness of RL training in scaling test times for LLMs. It effectively divides research on test-time scaling in VLMs into two distinct phases: pre-R1 and post-R1. Before R1, research efforts relied on long CoT distillation (Guo et al., 2024; Du et al., 2025; Xu et al., 2024b), tree search (Xu et al., 2024b; Thawakar et al., 2025; Yao et al., 2024a; Xiyao et al., 2024), multi-turn correction (Cheng et al., 2024b; Dong et al., 2024), and parallel sampling (Luo et al., 2025d). For example, LLaVA-CoT (Xu et al., 2024b) and LlamaV-O1 (Thawakar et al., 2025) enhance reasoning ability by decomposing the reasoning process into explicit sequential steps, such as summarizing the problem, reviewing image content, and performing reasoning. Similarly, Mulberry (Yao et al., 2024a) and LLaVA-CoT (Xu et al., 2024b) employ search algorithms like beam search and MCTS to expand the search space during reasoning. MAmmoTH-VL (Guo et al., 2024) and Virgo (Du et al., 2025) scale the output length of smaller models by distilling reasoning chains from larger VLMs, resulting in significant improvements across various visual reasoning tasks. As part of R1s concurrent work, QVQ-72B-Preview (Qwen, 2024) and K1.5 (Kimi et al., 2025) have demonstrated the immense potential of RL in VLMs. After R1, the research community has increasingly explored RL-based training as strategy to elicit VLMs test-time scaling ability. Recent efforts primarily fall into two directions: 1) enhancing the depth of multimodal reasoning, by pushing the limits of VLMs visual problem-solving capabilitiessuch as in open-r1-multimodal (EvolvingLMMs-Lab, 2025), MM-Eureka (Meng et al., 2025b), LMM-R1 (Peng et al., 2025), Vision-R1 (Huang et al., 2025c), VisualThinker-R1-Zero (Zhou et al., 2025a) and so on; and 2) expanding the breadth of multi-modal tasks, demonstrating the effectiveness of RL training across diverse vision-centric domains, including visual counting (Chen et al., 2025c), detection (Liu et al., 2025i; Shen et al., 2025a), segmentation (Liu et al., 2025e), etc. Despite promising results, long-CoT-based test-time scaling on VLMs still faces several challenges. First, unlike LLMs, input instructions for VLMs involve multiple modalities including both visual and textual data. Many studies that replicate LLM test-time scaling methods directly on VLMs fail to incorporate rethinking and reflection on visual inputs in the VLMs responses, overlooking the unique characteristics of multimodal understanding tasks. Future research should aim to better integrate test-time scaling with processing different modality inputs to address hallucinations from non-textual inputs (Liu et al., 2024b), while leveraging the synergies of multimodal inputs to enhance the effectiveness of test-time scaling. Second, conducting training directly on base models remains challenging. Unlike LLMs, VLMs lack strong base models because their training focuses mainly on modality alignment through image-caption pairs and instruction tuning (Liu et al., 2023a, 2024a; Li et al., 2024a), without extensive pre-training on on general multimodal corpora, which demands substantial computational and data resources. Test-Time Scaling for Multimodal Generation Recent breakthroughs such as Geminis image-text interleaved generation and GPT-4os image editing capabilities have reignited broad interest in multimodal generation. Unlike multimodal understanding tasks, where the output remains textual and test-time scaling techniques can be readily borrowed from LLMs, multimodal generation involves producing non-textual outputs (e.g., images, videos, or audio), thus requiring new approaches to unlock the full potential of test-time scaling in this domain. Several recent works have explored this frontier. Li et al. (2025a) enhance MLLM spatial reasoning via the Multimodal Visualization-of-Thought (MVoT) framework, which adopts long CoT strategy to scale reasoning at test time. PARM (Guo et al., 2025b) and MINT (Wang et al., 2025d) promote multimodal generation through multi-turn correction, aiming to improve image quality by iteratively reflecting on and revising the associated text prompts. GoT (Fang et al., 2025) unleashes the reasoning capability of MLLMs for visual generation and editing by introducing the Generation Chain-of-Thought framework, which adopts long CoT strategy for test-time scaling. Expanding beyond images, Video-T1 (Liu et al., 2025a) extends input modalities to video generation, utilizing tree search method at inference time to enhance video generation quality. Looking ahead, applying test-time scaling to multimodal generation represents highly promising but underexplored direction. Open questions include identifying the most effective model architecture (Chameleon, 2024; Zhou et al., 2024a; Chen et al., 2025f), designing pretraining and instruction-tuning strategies tailored to non-textual outputs, and improving the computational efficiency of such systems. Moreover, if reinforcement learning is to be extended to multimodal generation, key challenges emerge in defining suitable reward signals and constructing RL infrastructure for non-text modalities. 38 6.4 Agent Future Direction for Multimodality Focus more on applying test-time scaling to vision-centric tasks such as classification and detection, as well as non-text outputs like images and videos. Integrate more modalities (e.g., audio) and develop stronger native multimodal foundation models to unlock test-time scaling potential. 6.4 Agent Test Time Scaling Methods Parallel Sampling Tree Search Multi-turn Correction Long CoT (CoT) Training Strategy SFT DPO RL Inference-only ReAct [389] Reflexion [298] ToT [388] Agent-Eval-Refine [252] SearchAgent [156] Agent [262] PC Agent [108] UI-TARS [266] Operator [246] Deep Research [247] Claude 3.7 Sonnet [9] 2022/10 2023/03 2023/ 2024/04 2024/07 2024/08 2024/12 2025/01 2025/ 2025/02 2025/02 DeepResearcher [434] SWEET-RL [439] SWE-RL [353] 2025/ 2025/03 2025/02 Figure 15: Works of applying test-time scaling methods in the agent field. LLM Agent is an autonomous system that leverages LLMs as its cognitive core to automatically perform complex tasks through action execution in dynamic environments (Sumers et al., 2023). Building on prior efforts, the objective of agents is evolving from specific pre-defined workflows to handling more open-ended tasks in complex environments that need decision-making at scale, such as software engineering (Jimenez et al., 2024), deep research (OpenAI, 2025b), and computer use (Anthropic, 2024b; OpenAI, 2025a). Such tasks often require long-horizon planning to be accomplished with multi-step interaction with the environment, leading to substantial test-time computation. For instance, OpenAI DeepResearch (OpenAI, 2025b) demands 530 minutes for single research task, while CUA (OpenAI, 2025a) may require hundreds of steps to complete computer use task and exhibit clear test-time scaling behavior. prerequisite for effective multi-step task execution is the enhancement of decision-making in each step. This necessitates models with advanced reasoning capabilities to perform verification, backtracking, and reflection based on historical trajectories and current environment observations, thereby aligning actions with long-term goals. Numerous approaches have adopted test-time scaling strategies to optimize per-step decision quality. For example, ReAct (Yao et al., 2023b) introduces CoT reasoning during action selection. Reflexion (Shinn et al., 2023) further advances this paradigm by incorporating explicit feedback signals from prior steps to enable self-correction. Recently, Deep Research has highlighted the potential of scaling the reasoning process in single-step decision making. Powered by an optimized version of the OpenAI o3, it achieves research-analyst-level proficiency in synthesizing online sources into comprehensive reports, and attains 26.6% accuracy on the challenging Humanitys Last Exam benchmark (Phan et al., 2025), significantly surpassing previous SOTA models. Regarding training strategies, many methods introduce historical trajectories into SFT training samples to help models learn to handle multi-step histories, and incorporate thinking processes at each step to achieve CoT capabilities (He et al., 2024). Additionally, UI-TARS (Qin et al., 2025) addresses the limitation of SFT methods that only utilize the corrected steps through DPO methods, thereby enhancing the agents error correction and post-reflection abilities. Although the specific implementation of OpenAI Deep Research remains unclear, recent work applies RL for end-to-end training of deep research agents (Zheng et al., 2025). Despite some production-ready implementations such as GitHub Copilot, most agentic systems remain confined to proof-of-concept demonstrations rather than robust, large-scale deployment. Key barriers include insufficient general model capabilities, the predominance of prompt engineering over specialized agentic training, and context window constraints for long trajectoriesparticularly for visual observations. Additionally, unlike code or math tasks, many agentic tasks lack well-defined external verifiers, making it challenging to provide reliable rewards in reinforcement learning frameworks. Moreover, the involvement of long CoT in reasoning also introduces the Reasoning-Action Dilemma, which requires models to carefully balance active engagement with the environment against the need for internal reasoning, highlighting the importance of developing reasoning models that remain effectively grounded in environmental context when applied to agentic tasks (Cuadron et al., 2025). 39 6.5 Embodied AI Future Direction for Agents Develop robust execution environments that embrace diverse tool use, and craft scalable evaluation frameworks, which pave the way for reinforcement learning in agent training together. Further explore action scaling as new scaling dimensionexpand agents competence by growing the number of interaction steps with the environment. 6.5 Embodied AI Test Time Scaling Methods Parallel Sampling Tree Search Multi-turn Correction Long CoT (CoT) Training Strategy SFT DPO RL Inference-only Inner Monologue [126] REFLECT [198] KnowNo [277] Embodied-CoT [228] CoA [169] SpatialCoT [196] RAD [51] Cosmos-Reason1 [13] Gemini Robotics [317] CoT-VLA [427] Embodied-Reasoner [422] 2022/07 2023/06 2023/07 2024/07 2024/12 2025/ 2025/02 2025/03 2025/03 2025/03 2025/03 Figure 16: Works of applying test-time scaling methods in the Embodied AI field. Embodied AI is essential to advancing AGI, as it establishes the foundational link between cognitive representation and interaction with the physical world. By enabling robots to engage with their surrounding environments and manipulate objects, embodied AI allows for the execution of real-world and complex tasks, necessitating sophisticated levels of cognition and reasoning abilities. Cognition engineering further supports the enhancement of these cognitive and reasoning abilities within embodied AI systems. Embodied AI systems typically adopt hierarchical architecture comprising two distinct yet interconnected operational phases: high-level planning and low-level control policy execution (Ahn et al., 2022). First, high-level planning is the process of creating sequence of sub-tasks that robot can follow to achieve specific goal within its environment. This involves decision-making based on both the robots current state and the predicted outcomes of its actions, aiming to optimize efficiency, safety, and goal attainment within often dynamic and complex surroundings. This process frequently requires advanced cognitive abilities for analytical thinking and reasoning. Huang et al. (2022) pioneer the use of inner thoughts as feedback mechanism to enhance reasoning capabilities, enabling multi-turn self-correction through internal monologue. Building on this idea, Liu et al. (2023e) leverage LLMs to generate explicit failure explanations, which are then utilized to refine reasoning processes, thereby achieving substantial improvements in planning and problem-solving performance. Ren et al. (2023) propose method to quantify the uncertainty of LLM-based planners and trigger assistance requests when the uncertainty surpasses predefined threshold, which enhances reasoning ability through uncertainty alignment. Inspired by o1 (OpenAI, 2024), Liu et al. (2025d) employ bi-directional spatial coordinate alignment and chain-ofthought spatial grounding to facilitate the generation of long thoughts, thereby improving planning performance. Furthermore, Chai et al. (2024) propose novel approach based on Q-learning, which enables the model to make optimal decisions. Inspired by Deepseek-R1 (DeepSeek-AI et al., 2025), Azzolini et al. (2025) introduce novel VLM designed from the ground up to enhance reasoning capabilities in physical environments. The model incorporates an innovative hybrid architecture that combines Mamba, MLP, and Transformer components. This approach is complemented by comprehensive vision pre-training, specialized physical AI SFT, and physical AI reinforcement learning. The resulting system effectively processes video input paired with linguistic instructions, generating long reasoning thoughts before predicting appropriate subsequent actions. Meanwhile, Zhang et al. (2025d) propose dataset comprising 9.3K synthesized instances to enhance embodied reasoning capabilities, utilizing GPT-4o to generate Observation-Thought-Action trajectories that serve as extended reasoning chains for long-horizon action prediction tasks. Second, low-level control policy aims to translate tasks into executable actions, such as those performed by 7-DOF robotic arm in joint space. Currently, the most prevalent approach leverages large models, particularly through vision-language-action (VLA) frameworks. These models fine-tune pretrained vision-language models on trajectory data to generate actionable sequences. Importantly, reasoning plays key role in this process. For instance, if the task involves placing apples on one plate and bananas on another, the model must first distinguish between apples and bananas, rather than relying on simple muscle memory from prior learning. Embodied CoT (Michał et al., 2024) proposes constructing CoT that bridges perceptual information and task objectives by incorporating external knowledge from other models or algorithms. This approach has been demonstrated to substantially improve performance. In contrast, Zhao et al. (2025) introduce visual CoT framework that integrates explicit visual reasoning processes into VLA models. Their approach generates future image frames in an 40 6.6 Safety autoregressive manner, establishing them as visual objectives before producing concise action sequences designed to achieve these predetermined goals. Zhang et al. (2024h) introduce novel preference alignment approach for robotic policy learning, allowing VLA models to learn not only from successful trajectories but also from failure trajectories. Additionally, Li et al. (2024b) integrate diverse robot affordance information to enhance the models generalization in reasoning during testing and leverage the generated reasoning to foster long-horizon reasoning capabilities. However, SFT-based approaches rely on high-quality expert datasets, which are both expensive and challenging to obtain in the robotics domain. Moreover, these datasets may not fully align VLA models with real-world physical environments due to distribution shift issues. To address this limitation, Guo et al. (2025a) propose novel method for low-level action generation that alternates between online reinforcement learning and supervised learning stages, significantly enhancing the generalization capability of VLA models. Additionally, Clark et al. (2025) harness extensive human video data to augment reasoning capabilities. This approach employs Gemini to synthesize reasoning steps from human demonstrations. Subsequently, the method leverages limited robot data to train the model in mapping abstract reasoning to low-level actions, while the action-free datasets serve to enhance the models overall reasoning proficiency. Moreover, Team et al. (2025) train Gemini Robotics-ER based on Gemini 2.0, VLM that demonstrates enhanced embodied reasoning capabilities. They subsequently extend this work to develop Gemini Robotics, VLA model that integrates robot action data. This integration enables high-frequency dexterous control, robust generalization, and rapid adaptation across diverse robotic tasks and embodiments. While numerous studies have successfully elicited long CoT reasoning to enhance performance, there remains significant room for improvement in embodied AI. First, although pure reinforcement learning has proven effective in LLMs for fostering self-reflection through self-exploration (DeepSeek-AI et al., 2025), viable approach for embodied AI has yet to be established. Second, existing frameworks typically decouple high-level planning from low-level policy. However, more effective paradigm would integrate planning and execution into unified process, enabling continuous optimization through internal reasoning and iterative feedback. Future Direction for Embodied AI Develop unified frameworks that seamlessly integrate high-level planning and low-level policy execution into continuous process, enabling agents to optimize performance through internal reasoning loops and real-time iterative feedback during physical environment interactions. Establish fine-grained evaluation frameworks that systematically isolate and assess embodied agents effectiveness across thinking, planning, and execution processes, providing deeper insights into how individual components contribute to overall performance in physical-world interactions. 6.6 Safety Test Time Scaling Methods Parallel Sampling Tree Search Multi-turn Correction Long CoT (CoT) Training Strategy SFT DPO RL Inference-only SelfCheckGPT [220] Improve Factuality [69] C-MCTS [257] Chain-ofVerification [63] DebateGPT [309] PPO-MCTS [187] MART [87] Combat Adversarial [45] ARGS [150] MoTE [199] Multi-expert Prompting [204] 2023/03 2023/05 2023/05 2023/ 2023/09 2023/09 2023/11 2024/01 2024/02 2024/ 2024/11 SafeChain [135] SRG [337] STAIR [424] InferenceGuard [133] HaluSearch [42] Deliberate Alignment [94] 2025/02 2025/02 2025/02 2025/02 2025/ 2024/12 Figure 17: Works of applying test-time scaling methods in the safety field. The advancement of AI systems with the ability to conduct complex, long-horizon reasoning as result of test-time scaling carries significant implications for AI safety (Bengio, 2023; Park et al., 2024). The consequences are two-fold: AI systems that evolve to successfully solve highly complex problems can help address and identify emerging safety concerns themselves, potentially detecting and mitigating risks that would otherwise remain unnoticed by humans, which allows for more thorough exploration of edge cases and vulnerabilities (OpenAI, 2024). However, these same AI systems may also explore decisions that exceed human cognitive limits, potentially pursuing strategies misaligned with human values that could cause catastrophic consequences due to unmanageable, prolonged exchanges (Hendrycks et al., 2023). Below, we address how scaling test-time thinking has helped with identifying and mitigating safety risks (such as hallucination, jailbreaks, adversarial attacks, and more) in LLMs. 41 6.6 Safety Recent research has increasingly explored parallel sampling as mechanism for improving safety reasoning in LLMs, particularly in settings where retraining or internal access is impractical. The core insight is that sampling multiple responses to the same input enables more reliable estimation of factuality, uncertainty, and safety alignment. SelfCheckGPT (Manakul et al., 2023) introduces zero-resource hallucination detection method that uses sampling to detect factual inconsistencies across generations, under the assumption that reliable knowledge yields consistent responses. Similarly, Lin et al. (2024) formalize this principle through the lens of semantic dispersion, showing that uncertainty estimates based on the diversity of sampled responses can reliably predict the trustworthiness of models output, even under black-box access. While these methods focus on sampling in their frameworks, SRG (Wang et al., 2025b) proposes injecting explicit reasoning steps guided by predefined safety policies into the models. Conducting evaluation with Best-of-N sampling, they demonstrate improvements in generalization performance against OOD attacks. These approaches exemplify powerful paradigm of test-time scaling: the ability to amplify alignment, robustness, and factual reliability by operating over multiple outputs during inference, rather than modifying the underlying model weights. Parallel sampling thus serves as latent knowledge elicitation strategy that decouples safety improvements from expensive and infrastructure-heavy training processes, suggesting scalable path forward for enhancing trustworthiness in real-world LLM deployments. Apart from parallel sampling strategies, tree search-based methods offer structured approach to test-time safety by enabling deliberate exploration of alternative outputs or reasoning paths. Techniques such as InferenceGuard (Ji et al., 2025) and ARGS (Khanov et al., 2024) frame generation as reward-constrained search process, steering outputs toward safety-aligned completions using constrained Markov decision process within the LLMs latent space or learned reward models. Other methods like HaluSearch (Cheng et al., 2025) improve factual reliability and reasoning robustness by explicitly searching over intermediate thought steps and scoring them using self-evaluation or heuristics (deciding when models should think slower versus fallback to fast generation). Planning-focused approaches such as C-MCTS (Parthasarathy et al., 2023) and STAIR (Zhang et al., 2025e) integrate safety critics or introspective mechanisms into MCTS to avoid unsafe action sequences during decision making. These methods enhance test-time alignment by enabling backtracking and surfacing interpretable intermediate decision stepskey advantages for maintaining safety as models scale. Building on this foundation, recent research has also explored how increasing interaction steps during inferencethrough multi-turn correction or multi-agent collaborationcan further improve safety and robustness. Several multi-agent interaction frameworks (Du et al., 2023; Ge et al., 2024; Chern et al., 2024b; Long et al., 2024) have proven effective in reducing harmful outputs by enabling models to critique and refine responses collaboratively to identify and mitigate potential safety risks, such as reducing hallucination, toxicity, and adversarial attacks. Further, encouraging divergent thinking through structured debate (Liang et al., 2023) enhances reliability by promoting nuanced reasoning and cross-validation of outputs. Additionally, recent work leverages long CoT reasoning to enhance safety in LLMs by extending the depth of model deliberation at test time. SafeChain (Jiang et al., 2025) introduces training dataset of long-form safety-aligned reasoning and shows that LLMs fine-tuned on these trajectories can maintain high reasoning ability while improving refusal rates and reduces harmful content. Deliberative Alignment (Guan et al., 2024) trains models to explicitly consult and reason over safety policies via multi-step CoT before responding, demonstrating models that think before speaking become safer as reasoning depth increaseslinking CoT length directly to improved alignment under test-time scaling. Similarly, Chain-of-Verification (Dhuliawala et al., 2023) enhances factual safety by prompting the model to generate and answer verification questions about its own output, turning the reasoning process into multi-phase, self-auditing chainwhich becomes more reliable as models grow and can handle longer CoT. Lastly, MoTE (Liu et al., 2024e) decomposes safety reasoning into specialized CoT stages (question analysis, guidance, answer, and checking), assigning expert modules to each; this modular approach benefits from larger models and longer reasoning chains, enabling more scalable and interpretable self-alignment. These studies collectively suggest that strategically increasing interaction and reasoning steps at inference time offers scalable path to safer and more robust model behavior that doesnt require retraining. 42 6.7 RAG Future Direction for Safety Investigate how test-time scaling methods can be integrated with existing alignment strategies, such as RLHF and process-based supervision, to ensure they complement each other and understand how they can collectively enhance model safety. Investigate the extent to which test-time scaling methods improve models generalization to realworld safety challenges, including deception, adversarial attacks, and other out-of-distribution threats. Conduct rigorous testing to determine the resilience of long-CoT models against various jailbreaks and attacks. This includes analyzing whether increased test-time computation inadvertently introduces new vulnerabilities or mitigates existing ones. 6.7 RAG Test Time Scaling Methods Parallel Sampling Tree Search Multi-turn Correction Long CoT (CoT) Training Strategy SFT DPO RL Inference-only IterDRAG [405] Plan*RAG [329] Auto-RAG [395] Search-o1 [172] AirRAG [76] 2024/10 2024/10 2024/11 2025/ 2025/01 CoRAG [338] 2025/01 DeepRAG [95] R1-Searcher [303] Search-R1 [137] ReSearch [32] 2025/02 2025/03 2025/03 2025/03 Figure 18: Works of applying test-time scaling methods in the RAG field. Retrieval-Augmented Generation (RAG) systems enhance LLMs by incorporating external knowledge sources, producing responses that are more factual and contextually grounded. Despite their effectiveness, these systems often struggle when confronted with complex queries requiring multi-hop reasoning across multiple documents. Test-time scaling has emerged as promising approach to strengthen the reasoning capabilities of RAG systems by strategically allocating additional computational resources during inference. Yue et al. (2024) introduce IterDRAG, which methodically decomposes complex questions into sequential subqueries and conducts iterative search and reasoning processes to construct comprehensive answers. Their research demonstrates near-linear relationship between RAG performance and effective context length, establishing clear test-time scaling law for RAG systems. The effectiveness of this agentic workflow has been further validated in several follow-up studies (Verma et al., 2024; Guan et al., 2025a; Li et al., 2025e; Feng et al., 2025b; Yu et al., 2024b; Wang et al., 2025c). Beyond prompt-based agents, researchers have developed approaches to fine-tune LLMs for end-to-end interleaved reasoning and search capabilities. One approach for training data collection involves synthesizing reasoning and search trajectories through rejection sampling, followed by model training via supervised fine-tuning (Wang et al., 2025c; Yu et al., 2024b) or preference fine-tuning (Guan et al., 2025a). More recently, RL has been applied for end-to-end training of RAG, with works such as R1-Searcher (Song et al., 2025), Search-R1 (Jin et al., 2025), and ReSearch (Chen et al., 2025d). These approaches enable models to learn more efficient search strategies through trial and error rather than imitating human-designed search patterns. However, significant limitation is that these works primarily focus on open-domain question answering tasks and rely on rule-based rewards designed for short, factual answers, which may not generalize well to more complex reasoning tasks requiring detailed explanations. For future directions, an important aspect lies in developing more sophisticated reward functions that can evaluate long-form generation rather than just short factual answers for RL training. Moreover, current benchmarks do not separately assess the benefits of internal reasoning and external search in RAG systems, highlighting the importance of developing specialized evaluation frameworks for RAG in this new era. Future Direction for RAG Develop more sophisticated reward functions for RL that can effectively evaluate long-form generation rather than focusing solely on short factual answers. Create specialized evaluation frameworks that isolate and measure the distinct contributions of internal reasoning versus external search in RAG systems. 43 6.8 Evaluation Test Time Scaling Methods Parallel Sampling Tree Search Multi-turn Correction Long CoT (CoT) Training Strategy SFT DPO RL Inference-only FActScore [229] FacTool [43] ChatEval [23] ScaleEval [44] RefChecker [121] RAGChecker [279] Agent-as-a-Judge [442] EvalPlanner [282] MCTS-Judge [348] CCE [419] Kim et al. [152] SPCT [202] 2023/03 2023/07 2023/08 2024/ 2024/05 2024/08 2024/10 2025/01 2025/02 2025/ 2025/03 2025/04 Figure 19: Works of applying test-time scaling methods in the evaluation field. 6.8 Evaluation The LLM-as-a-Judge paradigm (Zheng et al., 2023a; Gu et al., 2024b) has transformed the evaluation of language model outputs, shifting away from rule-based metrics like BLEU and ROUGE toward more human-like assessment of generated content. Recent research demonstrates that allocating additional computational resources during inference significantly enhances evaluation quality through several approaches. These include fine-grained evaluation that decomposes the LLMs response and examines it step by step (Chern et al., 2023; Min et al., 2023; Hu et al., 2024b; Ru et al., 2024), structuring the CoT in LLM-as-a-Judge into distinct planning and execution phases (Saha et al., 2025), employing multi-agent systems to provide intermediate feedback (Zhuge et al., 2024), and utilizing parallel sampled crowd responses for more reliable pairwise comparisons (Zhang et al., 2025b). Furthermore, MCTS-Judge (Wang et al., 2025f) applies MCTS to systematically explore different evaluation perspectives for code assessment, demonstrating the scalability of this method where increasing search depth and rollouts consistently improves accuracy. Kim et al. (2025) also observe that generating more reasoning tokens leads to better performance of the evaluators for long-CoT models. As AI development progresses toward more realistic real-world tasks such as software development which comprises multiple subtasks, current evaluation frameworks increasingly focus on complex agents and workflows (Jimenez et al., 2024; Xie et al., 2024a). Future research directions should emphasize enhancing the reliability of evaluation for these complex and long-horizon tasks, which require strategies to balance the benefits of test-time scaling and evaluation speed. Future Direction for Evaluation Enhance the reliability of evaluation for complex and long-horizon tasks. Integrate evaluation frameworks with reward design in RL training to fully unleash the potential of RL."
        },
        {
            "title": "7 So What? – From Scaling to Cognitive Intelligence",
            "content": "The emergence of cognition engineering through test-time scaling marks fundamental paradigm shift in artificial intelligence. Far beyond mere technical implementation, this transformation carries profound implications for how we develop AI systems, reimagine human-AI collaboration, and conduct scientific research. 7.1 Data Engineering 2.0: Cognition Data Engineering Traditional AI has primarily focused on knowledge acquisitiontraining systems on the outputs of human thinking. Cognition engineering, however, demands something fundamentally different: shift from products of thought to thought processes themselves. This transition gives rise to new disciplinecognitive data engineeringwhich revolutionizes our understanding of what constitutes valuable training data. Cognitive data flows from three distinct yet complementary sources, each bringing unique advantages and challenges to the development process: Source 1: Human cognition projections Despite lacking direct brain-computer interfaces to capture human thought processes directly, we can access projections of human cognition in the physical world: Directly recorded artifacts. Video recordings of expert problem-solving sessions, think-aloud protocols, and detailed research journals capture cognitive processes as they unfold. These records preserve not just solutions but the messy reality of expert thinkingfalse starts, revisions, and breakthroughs. Tool-mediated cognitive traces. Complex cognitive activities leave traces in specialized toolslaboratory notebooks, collaborative whiteboarding sessions, version control systems in software development, and the progressive refinement of scientific papers through drafts and revisions. These tools serve as proxies that make implicit cognitive processes explicit and observable. 44 7.2 Reward & Environment Engineering Frontier expertise extraction. The most valuable cognitive patterns often reside in the minds of domain experts at the cutting edge of their fields. These patterns require carefully designed elicitation methodsspecialized interviewing techniques, tailored problem scenarios, and high-quality interactions that can distill tacit knowledge into explicit reasoning trajectories. Source 2: AI-generated cognition Through sophisticated reinforcement learning approaches with proper reward mechanisms, AI systems can now generate valuable cognitive data or trajectories in an environment independently: Environment-reward synergy. When provided with well-designed environments, appropriate reward functions, and strong initialization models, AI systems can discover novel cognitive strategies through extended exploration. These strategies may differ substantially from human approaches while achieving equal or superior resultssimilar to AlphaGos famous Move 37 that initially puzzled human experts but ultimately proved highly effective. Self-play and adversarial discovery. Systems can generate increasingly sophisticated cognitive data by competing against themselves or confronting progressively more challenging scenarios, developing reasoning strategies that might never emerge through imitation of human examples alone. Scaling effects in cognitive discovery. As computational resources increase, AI systems can explore cognitive pathways inaccessible to humans due to biological limitations in memory, attention span, or processing speedpotentially discovering novel problem-solving approaches in domains ranging from mathematics to drug design. Source 3: Human-AI collaborative generation Perhaps most promising is the co-creation of cognitive data through human-AI partnership: Trajectory sampling and human filtering. AI agents can generate diverse solution paths that human experts then evaluate and refine, combining machine-generated diversity with human judgment about quality and relevance. Human seeding with AI expansion. Human experts can provide initial examples of sophisticated reasoning in challenging domains, which AI systems then conduct cognitive completion (i.e., expand, systematically vary, and complete)creating training datasets far larger than what human annotation alone could achieve (He et al., 2024). Iterative refinement cycles. Human and AI contributions can alternate in progressive cycles, with each building upon and enhancing the others workhumans providing creative leaps or conceptual reframings, AI supplying systematic exploration of implications and edge cases. This cognition data establishes an entirely new category of digital resource with the potential to drive AI capabilities beyond what either natural data collection or synthetic generation alone could achieve. The resulting cognitive data repositories will likely become as strategically valuable as large-scale computing resources in determining leadership in AI advancement. 7.2 Reward & Environment Engineering The transition to cognition engineering fundamentally reshapes how we design the environments in which AI systems operate and the reward signals that guide their development. 7.2.1 Reward Models Design critical trend in cognition engineering is the increasing difficulty of reward verification as tasks grow more complex. Mathematical Olympiad problems represent relatively straightforward verificationproofs either follow logically or dont. Moving beyond this, tasks like Deep Research (OpenAI, 2025b) and PaperBench (Starace et al., 2025) demand sophisticated planning while producing outputs that resist objective assessment. Scientific discovery and literary creation occupy the frontier of verification complexity, requiring creativity and unique insights whose value assessment defies complete objectification. These domains involve aesthetic judgments, originality considerations, and contextual relevance that vary across cultures and perspectives. To address these challenges, we propose two complementary approaches: Reference-based assessment: method inspired by text summarization and machine translation evaluation approaches that compares outputs to high-quality exemplars, utilizing human judgment (Bhandari et al., 2020) capabilities without necessitating formal quality definitions (Qin et al., 2023; Yuan et al., 2021; Zheng et al., 2025). Criterion-based assessment: Establishing structured frameworks that decompose subjective judgments into concrete components with specific criteria (Fu et al., 2024a; Yuan et al., 2024b). Together, these methodologies facilitate the creation of effective reward signals in domains where simple correctness metrics prove insufficient. 45 7.3 Human-AI Cognitive Partnership 7.2.2 Cognitive Environment Design Environments for AI cognitive development span complexity spectrumfrom text-only interactions (Song et al., 2025) to code interpretation (Li et al., 2025g), browser environments (Zheng et al., 2025), full computer system access (Anthropic, 2024b), and physical world interactions. Within this framework, specialized cognitive simulators develop domain-specific reasoning: scientific discovery environments mimicking the hypothesis-experimentanalysis cycle; legal reasoning arenas requiring statute interpretation and precedent application; medical simulators demanding differential diagnosis under uncertainty. Adversarial frameworks strengthen cognitive abilities through structured debate platforms, red-teaming simulations, and Socratic dialogues. These environments should form cognitive curricula that systematically develop capabilities from foundations to frontiers. 7.3 Human-AI Cognitive Partnership The emergence of test-time scaling and cognition engineering fundamentally transforms the relationship between human and artificial intelligence, creating possibilities for collaboration that transcend traditional tool-user dynamics. This partnership represents genuinely new kind of cognitive ecosystem with profound implications. Bidirectional Cognitive Exchange Humans transmit thinking strategies through expert demonstrations, metacognitive guidance, and value-aligned reasoning examples that shape how AI systems approach problems, while AI systems reciprocally illuminate blind spots, expand strategy repertoires, and offer novel conceptual frameworks that enhance human thinking. These partnerships leverage the distinctive strengths of each intelligence type, creating genuine cognitive complementarity that transcends both AI-as-tool and AI-as-replacement paradigms. Cognitive Amplification AI systems extend human working memory by managing details, maintaining consistency, and preserving context throughout complex reasoning processes, while expanding exploration breadth through parallel hypothesis evaluation, exhaustive implication tracing, and counterfactual scenario generation beyond human capacity. This partnership also enables cognitive process externalization through reasoning visualization, assumption surfacing, and inference chain verification that make thinking observable and manipulable, overcoming fundamental human cognitive limitations. New Interaction Paradigms Shared workspaces allow humans and AI to visualize problems together using multiple formats (diagrams, text, symbols) and comment on each others work in real-time, creating recorded history of their thinking process. Communication becomes adaptive - AI explanations adjust to the users needs, revealing more details only when necessary and using concepts familiar to the specific user. Tools are designed not just for solving problems but for mutual improvement: humans can ask for clearer explanations from AI, provide targeted feedback on the AIs reasoning, and both partners can review completed work to learn from successes and mistakes. These approaches transform thinking from solitary activity into truly collaborative process where ideas are continuously shared, refined, and built upon together. This evolution of human-AI cognitive partnership represents far more than incremental improvement in AI assistantsit constitutes fundamentally new kind of intellectual relationship with the potential to dramatically enhance humanitys collective problem-solving capacity. The most profound impacts may emerge in domains where problems exceed the cognitive capacity of individual humans but require the contextual understanding and value alignment that purely artificial approaches struggle to achieve. 7.4 Research Acceleration The application of cognition engineering to scientific research promises to fundamentally transform the pace and nature of discovery across disciplines. By complementing human scientific thinking with machine reasoning capabilities, we stand at the threshold of potentially unprecedented acceleration in knowledge creation. Traditional science progresses slowly because humans can only generate and test limited hypotheses. Cognition systems overcome these cognitive limitations by systematically mapping knowledge gaps, identifying unexplored connections, and generating thousands of possible explanations simultaneously (Lu et al., 2024a). They detect anomalies in massive datasets that might signal breakthrough opportunities and determine which experiments would most efficiently test competing theories. These capabilities dramatically expand both the questions scientists can ask and how quickly they can find answers. Additionally, specialized research has created isolated knowledge silos that impede progress. Cognition engineering bridges these divides by connecting findings across disciplines, time periods, and information formats. It identifies similarities between seemingly unrelated phenomena and translates specialized terminology to enable cross-field collaboration. This integration facilitates the emergence of hybrid disciplines and allows insights to transfer between domains, transforming isolated knowledge islands into coherent scientific network where ideas flow freely. This acceleration of scientific research through cognition engineering has implications far beyond mere efficiency gains. By removing cognitive bottlenecks in hypothesis generation, literature integration, experimental design, Table 10: Comparison between OpenRLHF and veRL frameworks. Framework Supported Algorithm Hybrid Engine Training Backend Inference Engine Sequence Parallelism Multi-Modality OpenRLHF PPO, GRPO, RLOO, REINFORCE++ veRL PPO, GRPO, RLOO, REINFORCE++, DAPO, PRIME Supported Deepspeed ZERO vLLM, HF Transformers Ring Attention Supported Supported Megatron-LM, FSDP vLLM, SGLang, HF Transformers DeepSpeed Ulysses Supported and theory refinement, these approaches may enable humanity to address pressing challengesfrom climate change to disease prevention to sustainable developmentat unprecedented speed and scale. More profoundly, by democratizing participation in scientific discovery, cognition engineering could help realize the full creative potential of global human intelligence, bringing diverse perspectives to bear on our most significant questions."
        },
        {
            "title": "Infrastructure",
            "content": "In this section, we discuss the infrastructure of RL and MCTS briefly, two important technologies in cognition engineering. Besides these, the acceleration of long text generation is also important. We refer interested readers to the survey (Liu et al., 2025b) for comprehensive review. 8.1 RL Taking the PPO algorithm as the example, typical RL workflow consists of two primary steps: Rollout: Prepared prompts are fed into the actor model, which generates responses (i.e., rollout). This process requires the inference backend of the actor model. Although conventional training libraries support inference, they often exhibit slow decoding speeds when generating new sequences. To address this limitation, dedicated inference backend (e.g., vLLM (Kwon et al., 2023) or SGLang (Zheng et al., 2023b)) is typically deployed to accelerate rollout for the actor model. Model Update: Given the generated sequences, the log probabilities of the actor model, value estimates, reference log probabilities, and rewards are computed using the actor, critic, reference model, and reward function, respectively. Additional values, such as KL divergence loss, return, and advantage, are then derived from these computations. The computed KL divergence loss, return, and advantage are used to calculate the respective losses for the actor and critic models, which are then applied to update their parameters. The training backend for model updates typically employs frameworks such as DeepSpeed (Rasley et al., 2020) or Fully Sharded Data Parallel (FSDP) (Zhao et al., 2023c). Popular open-source frameworks for RL training include DeepSpeed-Chat, NeMo-Aligner (Shen et al., 2024), OpenRLHF (Hu et al., 2024a), and veRL (Sheng et al., 2024). Among these, OpenRLHF and veRL are actively maintained frameworks that support RL training, with their key features summarized in Table 10. Due to the complex workflow involving multiple models described above, these RL frameworks implement different strategies for resource allocation and process scheduling. OpenRLHF, for instance, can allocate dedicated resources to each backend, ensuring operations within given module remain confined to its designated resource pool. In contrast, veRL primarily employs shared-resource approach, dynamically reclaiming resources from inactive modules when another requires them. OpenRLHF offers more concise code implementation, making it more accessible for beginners to understand the algorithm, while veRL provides relatively centralized programming interface with enhanced programmability. Although these open-source frameworks facilitate RL training, large-scale RL training can still present significant challenges. For example, Yeo et al. (2025) encountered substantial difficulties when attempting to scale the policy model to 32B parameters, ultimately determining that the required number of GPUs was prohibitively large. They observed low hardware utilization during the training process, an issue that is particularly exacerbated in long CoT scenarios due to the higher variance in CoT length, which leads to stragglers during inference. This highlights the need for optimization of RL frameworks specifically for these scenarios. 8.2 MCTS For the infrastructure of MCTS, several of the aforementioned works have released their code to provide foundation for applying MCTS in LLMs (Hao et al., 2024a, 2023; Chen et al., 2024a; Feng et al., 2023). Although these code repositories facilitate subsequent research, they often lack optimized acceleration strategies that consider hardware and software optimization, which limits large-scale MCTS deployment. We outline the key acceleration strategies as follows. 47 Speculative Decoding Speculative Decoding employs small draft model to generate tokens sequentially, with larger target model validating these tokens, which is widely implemented to accelerate the rollout speed (Gao et al., 2024b; Wang et al., 2024h). SEED (Wang et al., 2024h) implements scheduled speculative decoding, which efficiently manages both runtime speed and GPU memory usage simultaneously. The framework leverages rounds-scheduled strategy that manages the execution flow using First-Come-First-Serve queue to control verification of the target model without conflicts. SC-MCTS (Gao et al., 2024b) utilizes speculative decoding to speed up MCTS reasoning by an average of 52% as free lunch. KV Cache Management LLM inference is typically memory bandwidth-bound (Hooper et al., 2024). In tree search, each unique trajectory requires separate KV cache state, creating significant memory bottleneck. DEFT (Yao et al., 2024b) introduces efficient kernel implementations that compute attention with tree-structured KV sharing. Hydragen (Juravsky et al., 2024) and vLLM (Kwon et al., 2023) offer support for shared prefix workloads, effectively eliminating KV cache duplication. SGLang (Zheng et al., 2023b) implements Radix Attention, which stores and dynamically references reused KV cache segments. Additionally, ETS (Hooper et al., 2025) employs linear programming cost model that encourages KV cache sharing by penalizing node retention while incorporating semantic coverage parameters to maintain diversity among retained trajectories. Parallel Processing The acceleration of tree expansion and simulation phases can be achieved through parallel processing techniques. However, the frequent switches among paths complicates the parallelism. Ding et al. (2025) develop flexible and adaptive parallelism system for arbitrary paths by implementing fine-grained cache management and alignment during the generation phase. The system adjusts the number of parallel paths processed based on real-time GPU memory availability, optimizing resource utilization."
        },
        {
            "title": "9 Tutorial",
            "content": "In this section, we provide tutorial on utilizing reinforcement learning to unlock the long CoT ability of LLMs. 9.1 Preparatory Work Before starting training, thorough preparation is essential. Though these foundational steps may seem simple, they are critical to the success of the experiment. We briefly introduce the code framework, models, datasets, and other necessary components. Understanding Reinforcement Learning Framework For ease of use, stability, and efficiency, this tutorial selects veRL as the reinforcement learning framework. The core code about the algorithm of veRL is organized as follows: trainer/ Contains core training code, including training control flow, advantage estimation functions, loss functions, etc. utils/ Includes data reading, reward functions, checkpoint saving, and other utilities. workers/ Defines various workers, including actor, critic, reward model, vllm, etc. @ protocal.py Defines the data structures in veRL, used for data exchange between different workers. Selecting the Base Model The choice of the base model has decisive impact on experimental results. We recommend Qwen2.5-1.5B, Qwen2.5-3B, and Qwen2.5-7B as starting points. We strongly recommend using the base versions of these models rather than versions that have undergone SFT. Since base models have not been fine-tuned for specific tasks, they retain stronger exploratory capabilities, which is advantageous for reinforcement learning as it helps the model discover more diverse problem-solving strategies. Preparing the Dataset Data quality is crucial for training effectiveness. We provide specially designed dataset of mathematical problems including NuminaMath, DeepScaleR, and MATH. The difficulty of problems in these datasets has been carefully selected to challenge the models abilities without being so difficult as to cause learning stagnation. Training on this dataset allows for observation of stable performance improvements and growth in the length of solution processes. Selecting the Algorithm Due to its popularity and effectiveness, we adopt the GRPO algorithm as the primary method for this tutorial. 9.2 Start RL training 9.2 Start RL training You can follow the commands below to start the RL training for the Qwen2.5-1.5B10 model. 1 git clone https://github.com/GAIR-NLP/cognition-engineering.git 2 cd cognition-engineering/simple_tts 3 # Create the conda environment 4 conda create -n verl python=3.10 5 conda activate verl 6 pip install -r requirements.txt 7 pip3 install vllm==0.7.3 8 pip3 install flash-attn --no-build-isolation 9 # launch training 10 bash examples/simple_tts.sh If you want to use another model, you can replace the value of policy path in examples/simple tts.sh. We provide set of default parameters that have been verified to be effective, but you can also change other hyperparameters, which will be explained in the next section. Additionally, you can build other training and testing datasets by following the structure of the files in data/train/, and modify the corresponding paths in data.train files and data.val files within examples/simple tts.sh. 9.3 Understanding algorithm with Code Analysis Let us understand the implementation of the reinforcement learning algorithm through specific code. Launch Script First, lets examine veRLs launch script, focusing on the core algorithm parameters: 1 python3 -m verl.trainer.main_ppo algorithm.adv_estimator=grpo 2 data.train_batch_size=$rollout_batch_size 3 actor_rollout_ref.actor.ppo_mini_batch_size=$mini_batch_size 4 actor_rollout_ref.actor.kl_loss_coef=$kl_loss_coef 5 actor_rollout_ref.actor.entropy_coeff=$entropy_coeff 6 actor_rollout_ref.rollout.temperature=$temperature 7 actor_rollout_ref.rollout.n=$n_samples_per_prompts 8 Key parameters: algorithm.adv estimator: Advantage estimation method (GRPO vs PPO/REINFORCE) train batch size: Number of prompts per sampling batch mini batch size: Number of prompts per update batch samples per prompts: Numbers of generated responses per prompt temperature: Generation randomness control kl loss coef and entropy coeff: Coefficients for KL loss and entropy loss Algorithm Flow Analysis The fit method in verl.trainer.ppo.ray trainers RayPPOTrainer is the control flow for the entire RL algorithm. Here is the simplified core process: # Prompts are organized into specific data structures batch: DataProto = DataProto.from_single_dict(batch_dict) # Rollout process, using inference engines to generate answers for prompts gen_batch_output = self.actor_rollout_wg.generate_sequences(gen_batch) # Take train_batch_size prompts from the dataset for batch_dict in self.train_dataloader: 1 # Loop through the training set for total_epochs times 2 for epoch in range(self.config.trainer.total_epochs): 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # Calculate value with critic model # Performs forward propagation to obtain log probability of old policy old_log_prob = self.actor_rollout_wg.compute_log_prob(batch) # Perform forward propagation to get log probability of ref model ref_log_prob = self.ref_policy_wg.compute_ref_log_prob(batch) 10https://huggingface.co/Qwen/Qwen2.5-1.5B 9.3 Understanding algorithm with Code Analysis 17 18 19 20 21 22 23 24 25 26 27 28 # But our algorithm GRPO doesnt require critic values = self.critic_wg.compute_values(batch) # Use reward function to get reward for each sequence reward_tensor = self.reward_fn(batch) # Calculate the advantage value for each sequence based on the reward batch = compute_advantage(batch, adv_estimator, gamma, lam, num_repeat) # Update model using calculated advantage values critic_output = self.critic_wg.update_critic(batch) actor_output = self.actor_rollout_wg.update_actor(batch) Data Loading and Prompt Templates The dataset class in verl.utils.dataset.rl dataset is used to read and process data. Each data entry contains question and an answer. In the dataset class, the question is combined with specific template to generate the prompt. The template we adopt is: Prompt conversation between User and Assistant. The user asks question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. nUser: You must put your answer inside boxed{} and Your final answer will be extracted automatically by the boxed{} tag.npromptnAssistant: This template encourages the model to first engage in reasoning, then provide the final answer, and mark the answer with boxed{} for subsequent evaluation. Reward Design The compute score method in verl.utils.reward score.math verifier demonstrates how to calculate rewards based on the models solution and the standard answer: return correctness_score_default(solution_str, ground_truth) 1 def compute_score(solution_str, ground_truth, reward_type) -> float: 2 3 4 def correctness_score_default(response, gt): 5 6 7 8 9 10 def is_equiv(str1, str2, verbose=False): 11 12 # Use regular expressions to extract boxed content from the answer pred = boxed_pattern.findall(response)[-1][:-1] # Judge whether the answer is correct return 1.0 if is_equiv(pred, gt) else -1.0 # Parse and verify whether two answers are mathematically equivalent return verify(parse(str1), parse(str2)) This reward function is very straightforward: correct answers receive 1 point, incorrect answers receive -1 point. GRPO Advantage Estimation The implementation of GRPO advantage estimation is provided in verl.trainer.ppo.core alogs: 1 # This implementation only considers outcome supervision, i.e., the reward is scalar 2 def compute_grpo_outcome_advantage(token_level_rewards: torch.Tensor, eos_mask, 3 4 5 6 7 8 9 10 index, epsilon: float = 1e-6): for idx in id2score: id2mean[idx] = torch.mean(torch.tensor(id2score[idx])) id2std[idx] = torch.std(torch.tensor([id2score[idx]])) # GRPO advantage estimation for in range(bsz): scores[i] = (scores[i] - id2mean[index[i]]) / (id2std[index[i]] + epsilon) # Expand scalar advantage to sequence length and mask with eos_mask scores = scores.unsqueeze(-1).tile([1, response_length]) * eos_mask return scores, scores Policy Loss Calculation Finally, here is how the policy loss is calculated, which is also provided in verl.trainer.ppo.core alogs: 50 9.4 Results 1 def compute_policy_loss(old_log_prob, log_prob, advantages, eos_mask, cliprange): # Calculate the log probability difference between new and old policies 2 negative_approx_kl = log_prob - old_log_prob 3 # Calculate probability ratio 4 ratio = torch.exp(negative_approx_kl) 5 # Calculate KL divergence 6 ppo_kl = verl_F.masked_mean(-negative_approx_kl, eos_mask) 7 # Original policy gradient loss 8 pg_losses = -advantages * ratio 9 # Clipped policy gradient loss 10 pg_losses2 = -advantages * torch.clamp(ratio, 1.0 - cliprange, 1.0 + cliprange) 11 # Take the larger of the two as the final loss 12 pg_loss = verl_F.masked_mean(torch.max(pg_losses, pg_losses2), eos_mask) 13 # Calculate clipping ratio 14 pg_clipfrac = verl_F.masked_mean(torch.gt(pg_losses2, pg_losses).float(), 15 eos_mask) 16 return pg_loss, pg_clipfrac, ppo_kl 9.4 Results As illustrated in Figure 20, we observe significant improvements in: Accuracy: The models accuracy in solving mathematical problems increases markedly. Response Length: The solution processes generated by the model become more detailed. We also recommend analyzing whether the model exhibited advanced cognitive capabilities such as reflection and self-correction through qualitative case studies and by tracking specific keywords in the responses. (a) (b) Figure 20: Trends in Accuracy on MATH500 and length on the training set of the Qwen-Base model during the RL process."
        },
        {
            "title": "10 Future Directions",
            "content": "While we have outlined domain-specific future directions within each application area, cognition engineering also faces several fundamental challenges that cut across these domains. In this section, we identify these critical cross-cutting future directions that could substantially accelerate the entire field of cognition engineering. New architecture Transformer-based architectures face fundamental limitations due to their linear memory scaling and memory-bound nature during generation. This presents critical constraint for test time scaling, particularly when generating long context. While the methods described in the improving scaling efficiency section can alleviate this problem, more fundamental solution requires exploring new architectures. Promising alternatives include state space models like Mamba (Gu and Dao, 2023; Dao and Gu, 2024), which offers linear-time complexity for sequence modeling, linear transformers that reduce the quadratic attention bottleneck (Katharopoulos et al., 2020), and even language diffusion models (Nie et al., 2025). This architectural transformation requires comprehensive system engineering efforts across multiple dimensions: developing robust theoretical frameworks 51 for new architectures, building infrastructure support for efficient training and inference, and creating large-scale pretrained foundation models based on these architecture. The integration of these architectures with cognition engineering technology could significantly enhance both cognitive capabilities and computational efficiency. Pretraining on cognition data Current pretraining data primarily consists of human-written texts but lacks the latent thought processes behind them. The success of RL scaling on models pretrained with data containing cognitive behaviors demonstrates the potential of including human thinking processes (Gandhi et al., 2025; Liu et al., 2025g,f). Recent works have shown the benefits of incorporating hidden thinking processes beyond explicit text (Zelikman et al., 2024a; Jiang et al., 2024; Ruan et al., 2025), though these are still limited to small-scale experiments. Future work should focus on obtaining large-scale human cognition data through techniques such as inferring latent thoughts by utilizing existing reasoning models and examining how pretraining on such data could benefit test-time scaling methods. RL scaling While we have examined the common design principles of RL scaling based on the latest research, the field remains in its early development stage, holding significant potential to unlock the cognitive abilities of AI. Given the complex components of RL and the numerous hyperparameters requiring tuning, future research should adopt more rigorous approach when drawing conclusions that account for all these elements (Jordan et al., 2024; Hochlehnert et al., 2025). Moreover, reproducible open-source work is currently limited to small models and datasets, which constrains the scope of possible conclusions. Large-scale experiments require both infrastructure improvements and algorithm optimizations to become accessible to researchers with modest computational resources. Furthermore, current RL scaling primarily focuses on verifiable tasks such as mathematics and code. Expanding to broader domains necessitates deeper investigation into reward hacking phenomena and establishing clearer relationships between reward reliability and RL scaling. This advancement demands not only empirical investigation but also theoretical analysis. Evaluation As an engineering approach, cognition engineering relies on iterative feedback and enhancement, which requires evaluation methods that go beyond simple benchmark performance metrics. Although some work has begun focusing on cognitive behavioral changes, these efforts usually rely on matching specific words (e.g., wait or alternatively) or case studies (DeepSeek-AI et al., 2025; Gandhi et al., 2025), failing to fully capture the quality of cognitive behaviors or the depth of reasoning processes. Future work should develop comprehensive evaluation frameworks that assess not only task performance but also the quality, efficiency, and generalizability of cognitive processes. This includes creating metrics for reasoning depth, backtracking efficiency, verification quality, and metacognitive awareness. Additionally, dynamic evaluation protocols that can adapt to evolving cognitive capabilities would better capture progress in this rapidly developing field. Developing these evaluation tools will require interdisciplinary collaboration between AI researchers, cognitive scientists, and domain experts to ensure they accurately reflect the cognitive dimensions most relevant to human-like reasoning. Scientific discovery Cognition engineering opens up unprecedented possibilities for AI systems to serve as scientific discovery partners. Test-time scaling methods show promise in enabling models to connect disparate knowledge domains and generate innovative insights through extended deliberation. Future work should explore how to optimize these cognitive capabilities specifically for scientific discovery tasks, such as hypothesis generation, experimental design, and theory formation. This will require developing specialized prompting techniques or training methodologies that encourage creative yet scientifically rigorous exploration of solution spaces. Additionally, research should investigate how to integrate domain-specific scientific tools and experimental platforms with reasoning models, allowing them to not only generate hypotheses but also design and potentially execute experiments to validate them (Lu et al., 2024a)."
        },
        {
            "title": "11 Conclusion",
            "content": "Cognition engineering represents paradigm shift in AI development, fundamentally transforming our approach from knowledge accumulation to the systematic development of thinking capabilities. This second act of generative AI leverages test-time scaling methodologies alongside specialized training strategies to enable models to engage in deep thinking, complex reasoning, and creative problem-solving. As demonstrated across domains from mathematics to multimodal understanding, these capabilities are already yielding substantial improvements in model performance. The emergence of cognition engineering marks not only technological progress but also the beginning of new relationship between human thought and artificial intelligencea symbiotic relationship based on deep understanding and cognitive exchange, in which humans and AI can empower each other to jointly explore new frontiers of cognition."
        },
        {
            "title": "Acknowledgment",
            "content": "We would like to thank Yixin Liu for his constructive comments on this work. We would also like to extend our appreciation to Ethan Chern for his involvement in the early stage of the work. 53 References"
        },
        {
            "title": "References",
            "content": "[1] Russell Ackoff. 1989. From data to wisdom. Journal of applied systems analysis, 16(1):39. [2] Pranjal Aggarwal, Aman Madaan, Yiming Yang, and Mausam. 2023. Lets sample step by step: Adaptiveconsistency for efficient reasoning and coding with LLMs. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 1237512396, Singapore. Association for Computational Linguistics. [3] Pranjal Aggarwal and Sean Welleck. 2025. L1: Controlling how long reasoning model thinks with reinforcement learning. [4] Wasi Uddin Ahmad, Sean Narenthiran, Somshubra Majumdar, Aleksander Ficek, Siddhartha Jain, Jocelyn Huang, Vahid Noroozi, and Boris Ginsburg. 2025. Opencodereasoning: Advancing data distillation for competitive coding. ArXiv preprint, abs/2504.01943. [5] Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, et al. 2022. Do as can, not as say: Grounding language in robotic affordances. ArXiv preprint, abs/2204.01691. [6] AlphaProof and AlphaGeometry teams. 2024. ternational mathematical olympiad problems. ai-solves-imo-problems-at-silver-medal-level/. AI achieves standard solving inhttps://deepmind.google/discover/blog/ silver-medal [7] Anthropic. 2024a. The claude 3 model family: Opus, sonnet, haiku. Technical Report. [8] Anthropic. 2024b. Introducing computer use, new claude 3.5 sonnet, and claude 3.5 haiku. anthropic.com. [9] Anthropic. 2025. Introducing deep research. anthropic.com. [10] team swe arena. 2025. Swe arena: An open evaluation platform for automated software engineering. [11] Daman Arora and Andrea Zanette. 2025. Training language models to reason efficiently. [12] Simon A. Aytes, Jinheon Baek, and Sung Ju Hwang. 2025. Sketch-of-thought: Efficient llm reasoning with adaptive cognitive-inspired sketching. [13] Alisson Azzolini, Hannah Brandon, Prithvijit Chattopadhyay, Huayu Chen, Jinju Chu, Yin Cui, Jenna Diamond, Yifan Ding, Francesco Ferroni, Rama Govindaraju, et al. 2025. Cosmos-reason1: From physical common sense to embodied reasoning. ArXiv preprint, abs/2503.15558. [14] Sanghwan Bae, Jiwoo Hong, Min Young Lee, Hanbyul Kim, JeongYeon Nam, and Donghyun Kwak. 2025. Online difficulty filtering for reasoning oriented reinforcement learning. [15] Marthe Ballon, Andres Algaba, and Vincent Ginis. 2025. The relationship between reasoning and performance in large language models o3 (mini) thinks harder, not longer. [16] Edward Beeching, Lewis Tunstall, and Sasha Rush. 2024. Scaling test-time compute with open models. [17] Yoshua Bengio. 2023. Faq on catastrophic ai risks. [18] Manik Bhandari, Pranav Narayan Gour, Atabak Ashfaq, Pengfei Liu, and Graham Neubig. 2020. Re-evaluating evaluation in text summarization. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 93479359, Online. Association for Computational Linguistics. [19] Zhenni Bi, Kai Han, Chuanjian Liu, Yehui Tang, and Yunhe Wang. 2024. Forest-of-thought: Scaling test-time compute for enhancing llm reasoning. [20] Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc V. Le, Christopher Re, and Azalia Mirhoseini. 2024. Large language monkeys: Scaling inference compute with repeated sampling. [21] Jiajun Chai, Sicheng Li, Yuqian Fu, Dongbin Zhao, and Yuanheng Zhu. 2024. Empowering llm agents with zero-shot optimal decision-making through q-learning. In Adaptive Foundation Models: Evolving AI for Personalized and Efficient Learning. [22] Team Chameleon. 2024. Chameleon: Mixed-modal early-fusion foundation models. ArXiv preprint, abs/2405.09818. [23] Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue, Shanghang Zhang, Jie Fu, and Zhiyuan Liu. 2023. Chateval: Towards better llm-based evaluators through multi-agent debate. ArXiv preprint, abs/2308.07201. 54 References [24] Bei Chen, Fengji Zhang, Anh Nguyen, Daoguang Zan, Zeqi Lin, Jian-Guang Lou, and Weizhu Chen. 2023a. Codet: Code generation with generated tests. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net. [25] Guoxin Chen, Minpeng Liao, Chengxi Li, and Kai Fan. 2024a. Alphamath almost zero: Process supervision without process. [26] Hardy Chen, Haoqin Tu, Hui Liu, Xianfeng Tang, Xinya Du, Yuyin Zhou, and Cihang Xie. 2025a. Vl-thinking: An r1-derived visual instruction tuning dataset for thinkable lvlms. https://github.com/UCSC-VLAA/ VL-Thinking. [27] Jiefeng Chen, Jie Ren, Xinyun Chen, Chengrun Yang, Ruoxi Sun, and Sercan Arık. 2025b. Sets: Leveraging self-verification and self-correction for improved test-time scaling. [28] Junying Chen, Zhenyang Cai, Ke Ji, Xidong Wang, Wanlong Liu, Rongsheng Wang, Jianye Hou, and Benyou Wang. 2024b. Huatuogpt-o1, towards medical complex reasoning with llms. [29] Liang Chen, Lei Li, Haozhe Zhao, Yifan Song, and Vinci. 2025c. R1-v: Reinforcing super generalization ability in vision-language models with less than $3. https://github.com/Deep-Agent/R1-V. [30] Lingjiao Chen, Jared Quincy Davis, Boris Hanin, Peter Bailis, Ion Stoica, Matei Zaharia, and James Zou. 2024c. Are more llm calls all you need? towards scaling laws of compound inference systems. [31] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. 2021. Evaluating large language models trained on code. ArXiv preprint, abs/2107.03374. [32] Mingyang Chen, Tianpeng Li, Haoze Sun, Yijie Zhou, Chenzheng Zhu, Haofen Wang, Jeff Z. Pan, Wen Zhang, Huajun Chen, Fan Yang, Zenan Zhou, and Weipeng Chen. 2025d. Research: Learning to reason with search for llms via reinforcement learning. [33] Qiguang Chen, Libo Qin, Jinhao Liu, Dengyun Peng, Jiannan Guan, Peng Wang, Mengkang Hu, Yuhang Zhou, Te Gao, and Wanxiang Che. 2025e. Towards reasoning era: survey of long chain-of-thought for reasoning large language models. [34] Xiaokang Chen, Zhiyu Wu, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, and Chong Ruan. 2025f. Janus-pro: Unified multimodal understanding and generation with data and model scaling. ArXiv preprint, abs/2501.17811. [35] Xingyu Chen, Jiahao Xu, Tian Liang, Zhiwei He, Jianhui Pang, Dian Yu, Linfeng Song, Qiuzhi Liu, Mengfei Zhou, Zhuosheng Zhang, Rui Wang, Zhaopeng Tu, Haitao Mi, and Dong Yu. 2024d. Do not think that much for 2+3=? on the overthinking of o1-like llms. [36] Xinyun Chen, Renat Aksitov, Uri Alon, Jie Ren, Kefan Xiao, Pengcheng Yin, Sushant Prakash, Charles Sutton, Xuezhi Wang, and Denny Zhou. 2023b. Universal self-consistency for large language model generation. [37] Xinyun Chen, Maxwell Lin, Nathanael Scharli, and Denny Zhou. 2023c. Teaching large language models to self-debug. [38] Zhipeng Chen, Yingqian Min, Beichen Zhang, Jie Chen, Jinhao Jiang, Daixuan Cheng, Wayne Xin Zhao, Zheng Liu, Xu Miao, Yang Lu, Lei Fang, Zhongyuan Wang, and Ji-Rong Wen. 2025g. An empirical study on eliciting and improving r1-like reasoning models. [39] Jeffrey Cheng and Benjamin Van Durme. 2024. Compressed chain of thought: Efficient reasoning through dense representations. [40] Jiale Cheng, Xiao Liu, Cunxiang Wang, Xiaotao Gu, Yida Lu, Dan Zhang, Yuxiao Dong, Jie Tang, Hongning Wang, and Minlie Huang. 2024a. Spar: Self-play with tree-search refinement to improve instruction-following in large language models. [41] Kanzhi Cheng, Yantao Li, Fangzhi Xu, Jianbing Zhang, Hao Zhou, and Yang Liu. 2024b. Vision-language models can self-improve reasoning via reflection. ArXiv preprint, abs/2411.00855. [42] Xiaoxue Cheng, Junyi Li, Wayne Xin Zhao, and Ji-Rong Wen. 2025. Think more, hallucinate less: Mitigating hallucinations via dual process of fast and slow thinking. ArXiv preprint, abs/2501.01306. [43] I-Chun Chern, Steffi Chern, Shiqi Chen, Weizhe Yuan, Kehua Feng, Chunting Zhou, Junxian He, Graham Neubig, and Pengfei Liu. 2023. Factool: Factuality detection in generative ai tool augmented framework for multi-task and multi-domain scenarios. References [44] Steffi Chern, Ethan Chern, Graham Neubig, and Pengfei Liu. 2024a. Can large language models be trusted for evaluation? scalable meta-evaluation of llms as evaluators via agent debate. ArXiv preprint, abs/2401.16788. [45] Steffi Chern, Zhen Fan, and Andy Liu. 2024b. Combating adversarial attacks with multi-agent debate. ArXiv preprint, abs/2401.05998. [46] Wayne Chi, Valerie Chen, Anastasios Nikolas Angelopoulos, Wei-Lin Chiang, Aditya Mittal, Naman Jain, Tianjun Zhang, Ion Stoica, Chris Donahue, and Ameet Talwalkar. 2025. Copilot arena: platform for code llm evaluation in the wild. ArXiv preprint, abs/2502.09328. [47] Cheng-Han Chiang and Hung-yi Lee. 2023. Can large language models be an alternative to human evaluations? In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1560715631, Toronto, Canada. Association for Computational Linguistics. [48] Yinlam Chow, Guy Tennenholtz, Izzeddin Gur, Vincent Zhuang, Bo Dai, Sridhar Thiagarajan, Craig Boutilier, Rishabh Agarwal, Aviral Kumar, and Aleksandra Faust. 2024. Inference-aware fine-tuning for best-of-n sampling in large language models. [49] Tianzhe Chu, Yuexiang Zhai, Jihan Yang, Shengbang Tong, Saining Xie, Dale Schuurmans, Quoc V. Le, Sergey Levine, and Yi Ma. 2025a. Sft memorizes, rl generalizes: comparative study of foundation model post-training. [50] Xiangxiang Chu, Hailang Huang, Xiao Zhang, Fei Wei, and Yong Wang. 2025b. Gpg: simple and strong reinforcement learning baseline for model reasoning. [51] Jaden Clark, Suvir Mirchandani, Dorsa Sadigh, and Suneel Belkhale. 2025. Action-free reasoning for policy generalization. ArXiv preprint, abs/2502.03729. [52] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021. Training verifiers to solve math word problems. [53] Alejandro Cuadron, Dacheng Li, Wenjie Ma, Xingyao Wang, Yichuan Wang, Siyuan Zhuang, Shu Liu, Luis Gaspar Schroeder, Tian Xia, Huanzhi Mao, Nicholas Thumiger, Aditya Desai, Ion Stoica, Ana Klimovic, Graham Neubig, and Joseph E. Gonzalez. 2025. The danger of overthinking: Examining the reasoning-action dilemma in agentic tasks. [54] Ganqu Cui, Lifan Yuan, Zefan Wang, Hanbin Wang, Wendi Li, Bingxiang He, Yuchen Fan, Tianyu Yu, Qixin Xu, Weize Chen, Jiarui Yuan, Huayu Chen, Kaiyan Zhang, Xingtai Lv, Shuo Wang, Yuan Yao, Hao Peng, Yu Cheng, Zhiyuan Liu, Maosong Sun, Bowen Zhou, and Ning Ding. 2025a. Process reinforcement through implicit rewards. [55] Yingqian Cui, Pengfei He, Jingying Zeng, Hui Liu, Xianfeng Tang, Zhenwei Dai, Yan Han, Chen Luo, Jing Huang, Zhen Li, Suhang Wang, Yue Xing, Jiliang Tang, and Qi He. 2025b. Stepwise perplexity-guided refinement for efficient chain-of-thought reasoning in large language models. [56] Quy-Anh Dang and Chris Ngo. 2025. Reinforcement learning for reasoning in small llms: What works and what doesnt. [57] Tri Dao and Albert Gu. 2024. Transformers are ssms: Generalized models and efficient algorithms through structured state space duality. [58] Leonardo De Moura, Soonho Kong, Jeremy Avigad, Floris Van Doorn, and Jakob von Raumer. 2015. The lean theorem prover (system description). In Automated Deduction-CADE-25: 25th International Conference on Automated Deduction, Berlin, Germany, August 1-7, 2015, Proceedings 25, pages 378388. Springer. [59] DeepSeek-AI. 2024. Deepseek-v2: strong, economical, and efficient mixture-of-experts language model. [60] DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, References Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanhong Xu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. [61] Yuntian Deng, Yejin Choi, and Stuart Shieber. 2024. From explicit cot to implicit cot: Learning to internalize cot step by step. [62] Yuntian Deng, Kiran Prasad, Roland Fernandez, Paul Smolensky, Vishrav Chaudhary, and Stuart Shieber. 2023. Implicit chain of thought reasoning via knowledge distillation. [63] Shehzaad Dhuliawala, Mojtaba Komeili, Jing Xu, Roberta Raileanu, Xian Li, Asli Celikyilmaz, and Jason Weston. 2023. Chain-of-verification reduces hallucination in large language models. [64] Mengru Ding, Hanmeng Liu, Zhizhang Fu, Jian Song, Wenbo Xie, and Yue Zhang. 2024. Break the chain: Large language models can be shortcut reasoners. [65] Yifu Ding, Wentao Jiang, Shunyu Liu, Yongcheng Jing, Jinyang Guo, Yingjie Wang, Jing Zhang, Zengmao Wang, Ziwei Liu, Bo Du, Xianglong Liu, and Dacheng Tao. 2025. Dynamic parallel tree search for efficient llm reasoning. [66] Hanze Dong, Wei Xiong, Deepanshu Goyal, Yihan Zhang, Winnie Chow, Rui Pan, Shizhe Diao, Jipeng Zhang, Kashun Shum, and Tong Zhang. 2023. Raft: Reward ranked finetuning for generative foundation model alignment. [67] Yuhao Dong, Zuyan Liu, Hai-Long Sun, Jingkang Yang, Winston Hu, Yongming Rao, and Ziwei Liu. 2024. Insight-v: Exploring long-chain visual reasoning with multimodal large language models. ArXiv preprint, abs/2411.14432. [68] Yifan Du, Zikang Liu, Yifan Li, Wayne Xin Zhao, Yuqi Huo, Bingning Wang, Weipeng Chen, Zheng Liu, Zhongyuan Wang, and Ji-Rong Wen. 2025. Virgo: preliminary exploration on reproducing o1-like mllm. ArXiv preprint, abs/2501.01904. [69] Yilun Du, Shuang Li, Antonio Torralba, Joshua B. Tenenbaum, and Igor Mordatch. 2023. Improving factuality and reasoning in language models through multiagent debate. [70] Ahmed El-Kishky, Alexander Wei, Andre Saraiva, Borys Minaev, Daniel Selsam, David Dohan, Francis Song, Hunter Lightman, Ignasi Clavera, Jakub Pachocki, et al. 2025. Competitive programming with large reasoning models. ArXiv preprint, abs/2502.06807. [71] EvolvingLMMs-Lab. 2025. open-r1-multimodal: fork to add multimodal model training to open-r1. https://github.com/EvolvingLMMs-Lab/open-r1-multimodal. [72] Hugging Face. 2025. Open r1: fully open reproduction of deepseek-r1. [73] Rongyao Fang, Chengqi Duan, Kun Wang, Linjiang Huang, Hao Li, Shilin Yan, Hao Tian, Xingyu Zeng, Rui Zhao, Jifeng Dai, et al. 2025. Got: Unleashing reasoning capability of multimodal large language model for visual generation and editing. ArXiv preprint, abs/2503.10639. [74] Mehdi Fatemi, Banafsheh Rafiee, Mingjie Tang, and Kartik Talamadupula. 2025. Concise reasoning via reinforcement learning. [75] Kaituo Feng, Kaixiong Gong, Bohao Li, Zonghao Guo, Yibing Wang, Tianshuo Peng, Benyou Wang, and Xiangyu Yue. 2025a. Video-r1: Reinforcing video reasoning in mllms. ArXiv preprint, abs/2503.21776. [76] Wenfeng Feng, Chuzhan Hao, Yuewei Zhang, Jingyi Song, and Hao Wang. 2025b. Airrag: Activating intrinsic reasoning for retrieval augmented generation using tree-based search. 57 References [77] Xidong Feng, Ziyu Wan, Muning Wen, Stephen Marcus McAleer, Ying Wen, Weinan Zhang, and Jun Wang. 2023. Alphazero-like tree-search can guide large language model decoding and training. [78] Emily First, Markus N. Rabe, Talia Ringer, and Yuriy Brun. 2023. Baldur: Whole-proof generation and repair with large language models. [79] Hao Fu, Yao; Peng and Tushar Khot. 2022. How does gpt obtain its ability? tracing emergent abilities of language models to their sources. Yao Fus Notion. [80] Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. 2024a. GPTScore: Evaluate as you desire. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 65566576, Mexico City, Mexico. Association for Computational Linguistics. [81] Yichao Fu, Junda Chen, Siqi Zhu, Zheyu Fu, Zhongdongming Dai, Aurick Qiao, and Hao Zhang. 2024b. Efficiently serving llm reasoning programs with certaindex. [82] Kanishk Gandhi, Ayush Chakravarthy, Anikait Singh, Nathan Lile, and Noah D. Goodman. 2025. Cognitive behaviors that enable self-improving reasoners, or, four habits of highly effective stars. [83] Kanishk Gandhi, Denise Lee, Gabriel Grand, Muxin Liu, Winson Cheng, Archit Sharma, and Noah D. Goodman. 2024. Stream of search (sos): Learning to search in language. [84] Jiaxuan Gao, Shusheng Xu, Wenjie Ye, Weilin Liu, Chuyi He, Wei Fu, Zhiyu Mei, Guangju Wang, and Yi Wu. 2024a. On designing effective rl reward at training time for llm reasoning. [85] Leo Gao, John Schulman, and Jacob Hilton. 2023. Scaling laws for reward model overoptimization. In International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pages 1083510866. PMLR. [86] Zitian Gao, Boye Niu, Xuzheng He, Haotian Xu, Hongzhang Liu, Aiwei Liu, Xuming Hu, and Lijie Wen. 2024b. Interpretable contrastive monte carlo tree search reasoning. [87] Suyu Ge, Chunting Zhou, Rui Hou, Madian Khabsa, Yi-Chia Wang, Qifan Wang, Jiawei Han, and Yuning Mao. 2024. MART: Improving LLM safety with multi-round automatic red-teaming. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 19271937, Mexico City, Mexico. Association for Computational Linguistics. [88] Jonas Geiping, Sean McLeish, Neel Jain, John Kirchenbauer, Siddharth Singh, Brian R. Bartoldson, Bhavya Kailkhura, Abhinav Bhatele, and Tom Goldstein. 2025. Scaling up test-time compute with latent reasoning: recurrent depth approach. [89] Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Nan Duan, and Weizhu Chen. 2023. Critic: Large language models can self-correct with tool-interactive critiquing. [90] Albert Gu and Tri Dao. 2023. Mamba: Linear-time sequence modeling with selective state spaces. [91] Alex Gu, Baptiste Rozi`ere, Hugh Leather, Armando Solar-Lezama, Gabriel Synnaeve, and Sida Wang. 2024a. Cruxeval: benchmark for code reasoning, understanding and execution. ArXiv preprint, abs/2401.03065. [92] Jiawei Gu, Xuhui Jiang, Zhichao Shi, Hexiang Tan, Xuehao Zhai, Chengjin Xu, Wei Li, Yinghan Shen, Shengjie Ma, Honghao Liu, Saizhuo Wang, Kun Zhang, Yuanzhuo Wang, Wen Gao, Lionel Ni, and Jian Guo. 2024b. survey on llm-as-a-judge. [93] Yu Gu, Xiang Deng, and Yu Su. 2023. Dont generate, discriminate: proposal for grounding language models to real-world environments. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 49284949, Toronto, Canada. Association for Computational Linguistics. [94] Melody Y. Guan, Manas Joglekar, Eric Wallace, Saachi Jain, Boaz Barak, Alec Helyar, Rachel Dias, Andrea Vallone, Hongyu Ren, Jason Wei, Hyung Won Chung, Sam Toyer, Johannes Heidecke, Alex Beutel, and Amelia Glaese. 2024. Deliberative alignment: Reasoning enables safer language models. ArXiv preprint, abs/2412.16339. [95] Xinyan Guan, Jiali Zeng, Fandong Meng, Chunlei Xin, Yaojie Lu, Hongyu Lin, Xianpei Han, Le Sun, and Jie Zhou. 2025a. Deeprag: Thinking to retrieval step by step for large language models. 58 References [96] Xinyu Guan, Li Lyna Zhang, Yifei Liu, Ning Shang, Youran Sun, Yi Zhu, Fan Yang, and Mao Yang. 2025b. rstar-math: Small llms can master math reasoning with self-evolved deep thinking. [97] Caglar Gulcehre, Tom Le Paine, Srivatsan Srinivasan, Ksenia Konyushkova, Lotte Weerts, Abhishek Sharma, Aditya Siddhant, Alex Ahern, Miaosen Wang, Chenjie Gu, et al. 2023. Reinforced self-training (rest) for language modeling. ArXiv preprint, abs/2308.08998. [98] Jarvis Guo, Tuney Zheng, Yuelin Bai, Bo Li, Yubo Wang, King Zhu, Yizhi Li, Graham Neubig, Wenhu Chen, and Xiang Yue. 2024. Mammoth-vl: Eliciting multimodal reasoning with instruction tuning at scale. ArXiv preprint, abs/2412.05237. [99] Yanjiang Guo, Jianke Zhang, Xiaoyu Chen, Xiang Ji, Yen-Jen Wang, Yucheng Hu, and Jianyu Chen. 2025a. Improving vision-language-action model with online reinforcement learning. ArXiv preprint, abs/2501.16664. [100] Ziyu Guo, Renrui Zhang, Chengzhuo Tong, Zhizheng Zhao, Peng Gao, Hongsheng Li, and Pheng-Ann Heng. 2025b. Can we generate images with cot? lets verify and reinforce image generation step by step. ArXiv preprint, abs/2501.13926. [101] Patrick Haluptzok, Matthew Bowers, and Adam Tauman Kalai. 2023. Language models can teach themselves to program better. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net. [102] Tingxu Han, Zhenting Wang, Chunrong Fang, Shiyu Zhao, Shiqing Ma, and Zhenyu Chen. 2024. Tokenbudget-aware llm reasoning. [103] Shibo Hao, Yi Gu, Haotian Luo, Tianyang Liu, Xiyan Shao, Xinyuan Wang, Shuhua Xie, Haodi Ma, Adithya Samavedhi, Qiyue Gao, Zhen Wang, and Zhiting Hu. 2024a. Llm reasoners: New evaluation, library, and analysis of step-by-step reasoning with large language models. [104] Shibo Hao, Yi Gu, Haodi Ma, Joshua Hong, Zhen Wang, Daisy Wang, and Zhiting Hu. 2023. Reasoning with language model is planning with world model. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 81548173, Singapore. Association for Computational Linguistics. [105] Shibo Hao, Sainbayar Sukhbaatar, DiJia Su, Xian Li, Zhiting Hu, Jason Weston, and Yuandong Tian. 2024b. Training large language models to reason in continuous latent space. [106] Peter Hart, Nils Nilsson, and Bertram Raphael. 1968. formal basis for the heuristic determination of minimum cost paths. IEEE transactions on Systems Science and Cybernetics, 4(2):100107. [107] Alex Havrilla, Sharath Raparthy, Christoforus Nalmpantis, Jane Dwivedi-Yu, Maksym Zhuravinskyi, Eric Hambro, and Roberta Raileanu. 2024. Glore: When, where, and how to improve llm reasoning via global and local refinements. [108] Yanheng He, Jiahe Jin, Shijie Xia, Jiadi Su, Runze Fan, Haoyang Zou, Xiangkun Hu, and Pengfei Liu. 2024. Pc agent: While you sleep, ai works cognitive journey into digital world. [109] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021. Measuring mathematical problem solving with the math dataset. [110] Dan Hendrycks, Mantas Mazeika, and Thomas Woodside. 2023. An overview of catastrophic AI risks. ArXiv preprint, abs/2306.12001. [111] Andreas Hochlehnert, Hardik Bhatnagar, Vishaal Udandarao, Samuel Albanie, Ameya Prabhu, and Matthias Bethge. 2025. sober look at progress in language model reasoning: Pitfalls and paths to reproducibility. [112] Coleman Hooper, Sehoon Kim, Hiva Mohammadzadeh, Monishwaran Maheswaran, June Paik, Michael W. Mahoney, Kurt Keutzer, and Amir Gholami. 2024. Squeezed attention: Accelerating long context length llm inference. [113] Coleman Hooper, Sehoon Kim, Suhong Moon, Kerem Dilmen, Monishwaran Maheswaran, Nicholas Lee, Michael W. Mahoney, Sophia Shao, Kurt Keutzer, and Amir Gholami. 2025. Ets: Efficient tree search for inference-time scaling. [114] Arian Hosseini, Xingdi Yuan, Nikolay Malkin, Aaron Courville, Alessandro Sordoni, and Rishabh Agarwal. 2024. V-star: Training verifiers for self-taught reasoners. [115] Zhenyu Hou, Pengfan Du, Yilin Niu, Zhengxiao Du, Aohan Zeng, Xiao Liu, Minlie Huang, Hongning Wang, Jie Tang, and Yuxiao Dong. 2024. Does rlhf scale? exploring the impacts from data, model, and method. References [116] Zhenyu Hou, Xin Lv, Rui Lu, Jiajie Zhang, Yujiang Li, Zijun Yao, Juanzi Li, Jie Tang, and Yuxiao Dong. 2025. Advancing language model reasoning through reinforcement learning and inference scaling. [117] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022. Lora: Low-rank adaptation of large language models. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net. [118] Jian Hu. 2025. Reinforce++: simple and efficient approach for aligning large language models. [119] Jian Hu, Xibin Wu, Zilin Zhu, Weixun Wang, Dehao Zhang, Yu Cao, et al. 2024a. Openrlhf: An easy-to-use, scalable and high-performance rlhf framework. ArXiv preprint, abs/2405.11143. [120] Jingcheng Hu, Yinmin Zhang, Qi Han, Daxin Jiang, and Heung-Yeung Shum Xiangyu Zhang. 2025. Openreasoner-zero: An open source approach to scaling reinforcement learning on the base model. https:// github.com/Open-Reasoner-Zero/Open-Reasoner-Zero. [121] Xiangkun Hu, Dongyu Ru, Lin Qiu, Qipeng Guo, Tianhang Zhang, Yang Xu, Yun Luo, Pengfei Liu, Yue Zhang, and Zheng Zhang. 2024b. Knowledge-centric hallucination detection. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 69536975, Miami, Florida, USA. Association for Computational Linguistics. [122] Chengsong Huang, Langlin Huang, Jixuan Leng, Jiacheng Liu, and Jiaxin Huang. 2025a. Efficient test-time scaling via self-calibration. [123] Jiaxin Huang, Shixiang Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and Jiawei Han. 2023a. Large language models can self-improve. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 10511068, Singapore. Association for Computational Linguistics. [124] Jie Huang, Xinyun Chen, Swaroop Mishra, Huaixiu Steven Zheng, Adams Wei Yu, Xinying Song, and Denny Zhou. 2023b. Large language models cannot self-correct reasoning yet. [125] Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, and Ting Liu. 2025b. survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions. ACM Transactions on Information Systems, 43(2):155. [126] Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, Pierre Sermanet, Noah Brown, Tomas Jackson, Linda Luu, Sergey Levine, Karol Hausman, and Brian Ichter. 2022. Inner monologue: Embodied reasoning through planning with language models. [127] Wenxuan Huang, Bohan Jia, Zijie Zhai, Shaosheng Cao, Zheyu Ye, Fei Zhao, Yao Hu, and Shaohui Lin. 2025c. Vision-r1: Incentivizing reasoning capability in multimodal large language models. ArXiv preprint, abs/2503.06749. [128] Zhen Huang, Haoyang Zou, Xuefeng Li, Yixiu Liu, Yuxiang Zheng, Ethan Chern, Shijie Xia, Yiwei Qin, Weizhe Yuan, and Pengfei Liu. 2024. O1 replication journey part 2: Surpassing o1-preview through simple distillation. Github. [129] Zhongzhen Huang, Gui Geng, Shengyi Hua, Zhen Huang, Haoyang Zou, Shaoting Zhang, Pengfei Liu, and Xiaofan Zhang. 2025d. O1 replication journey part 3: Inference-time scaling for medical reasoning. [130] Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Keming Lu, et al. 2024. Qwen2. 5-coder technical report. ArXiv preprint, abs/2409.12186. [131] Hyeonbin Hwang, Doyoung Kim, Seungone Kim, Seonghyeon Ye, and Minjoon Seo. 2024. Self-explore: Enhancing mathematical reasoning in language models with fine-grained rewards. [132] Arnav Kumar Jain, Gonzalo Gonzalez-Pumariega, Wayne Chen, Alexander Rush, Wenting Zhao, and Sanjiban Choudhury. 2025. Multi-turn code generation through single-step rewards. [133] Xiaotong Ji, Shyam Sundhar Ramesh, Matthieu Zimmer, Ilija Bogunovic, Jun Wang, and Haitham BouAmmar. 2025. Almost surely safe alignment of large language models at inference-time. ArXiv preprint, abs/2502.01208. [134] Dongwei Jiang, Guoxuan Wang, Yining Lu, Andrew Wang, Jingyu Zhang, Chuyu Liu, Benjamin Van Durme, and Daniel Khashabi. 2024. Rationalyst: Pre-training process-supervision for improving reasoning. References [135] Fengqing Jiang, Zhangchen Xu, Yuetai Li, Luyao Niu, Zhen Xiang, Bo Li, Bill Yuchen Lin, and Radha Poovendran. 2025. Safechain: Safety of language models with long chain-of-thought reasoning capabilities. [136] Carlos Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. 2024. SWE-bench: Can language models resolve real-world github issues? In The Twelfth International Conference on Learning Representations. [137] Bowen Jin, Hansi Zeng, Zhenrui Yue, Dong Wang, Hamed Zamani, and Jiawei Han. 2025. Search-r1: Training llms to reason and leverage search engines with reinforcement learning. [138] Mingyu Jin, Qinkai Yu, Dong Shu, Haiyan Zhao, Wenyue Hua, Yanda Meng, Yongfeng Zhang, and Mengnan Du. 2024. The impact of reasoning step length on large language models. [139] Scott M. Jordan, Adam White, Bruno Castro da Silva, Martha White, and Philip S. Thomas. 2024. Position: Benchmarking is limited in reinforcement learning research. [140] Jordan Juravsky, Bradley Brown, Ryan Ehrlich, Daniel Y. Fu, Christopher Re, and Azalia Mirhoseini. 2024. Hydragen: High-throughput llm inference with shared prefixes. [141] Subbarao Kambhampati. 2024. Can large language models reason and plan? Annals of the New York Academy of Sciences, 1534(1):1518. [142] Ryo Kamoi, Yusen Zhang, Nan Zhang, Jiawei Han, and Rui Zhang. 2024. When can LLMs actually correct their own mistakes? critical survey of self-correction of LLMs. Transactions of the Association for Computational Linguistics, 12:14171440. [143] Jikun Kang, Xin Zhe Li, Xi Chen, Amirreza Kazemi, Qianyi Sun, Boxing Chen, Dong Li, Xu He, Quan He, Feng Wen, Jianye Hao, and Jun Yao. 2024a. Mindstar: Enhancing math reasoning in pre-trained llms at inference time. [144] Yu Kang, Xianghui Sun, Liangyu Chen, and Wei Zou. 2024b. C3ot: Generating shorter chain-of-thought without compromising effectiveness. [145] Zhewei Kang, Xuandong Zhao, and Dawn Song. 2025. Scalable best-of-n selection for large language models via self-certainty. [146] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020. Scaling laws for neural language models. [147] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Francois Fleuret. 2020. Transformers are rnns: Fast autoregressive transformers with linear attention. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pages 51565165. PMLR. [148] Amirhossein Kazemnejad, Milad Aghajohari, Eva Portelance, Alessandro Sordoni, Siva Reddy, Aaron Courville, and Nicolas Le Roux. 2024. Vineppo: Unlocking rl potential for llm reasoning through refined credit assignment. [149] Akbir Khan, John Hughes, Dan Valentine, Laura Ruis, Kshitij Sachan, Ansh Radhakrishnan, Edward Grefenstette, Samuel R. Bowman, Tim Rocktaschel, and Ethan Perez. 2024. Debating with more persuasive llms leads to more truthful answers. [150] Maxim Khanov, Jirayu Burapacheep, and Yixuan Li. 2024. ARGS: alignment as reward-guided search. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net. [151] Geunwoo Kim, Pierre Baldi, and Stephen McAleer. 2023. Language models can solve computer tasks. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023. [152] Seungone Kim, Ian Wu, Jinu Lee, Xiang Yue, Seongyun Lee, Mingyeong Moon, Kiril Gashteovski, Carolin Lawrence, Julia Hockenmaier, Graham Neubig, and Sean Welleck. 2025. Scaling evaluation-time compute with reasoning models as process evaluators. [153] Team Kimi, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, et al. 2025. Kimi k1. 5: Scaling reinforcement learning with llms. ArXiv preprint, abs/2501.12599. 61 References [154] Kimi Team. 2025. Kimi-vl technical report. https://github.com/MoonshotAI/Kimi-VL/blob/ main/Kimi-VL.pdf. [155] Levente Kocsis and Csaba Szepesvari. 2006. Bandit based monte-carlo planning. In European conference on machine learning, pages 282293. Springer. [156] Jing Yu Koh, Stephen McAleer, Daniel Fried, and Ruslan Salakhutdinov. 2024. Tree search for language model agents. [157] Aviral Kumar, Vincent Zhuang, Rishabh Agarwal, Yi Su, John Co-Reyes, Avi Singh, Kate Baumli, Shariq Iqbal, Colton Bishop, Rebecca Roelofs, Lei Zhang, Kay McKinney, Disha Shrivastava, Cosmin Paduraru, George Tucker, Doina Precup, Feryal Behbahani, and Aleksandra Faust. 2024. Training language models to self-correct via reinforcement learning. [158] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles. [159] Guillaume Lample, Timothee Lacroix, Marie-Anne Lachaux, Aurelien Rodriguez, Amaury Hayat, Thibaut Lavril, Gabriel Ebner, and Xavier Martinet. 2022. Hypertree proof search for neural theorem proving. In Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022. [160] Hung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio Savarese, and Steven Chu-Hong Hoi. 2022. Coderl: Mastering code generation through pretrained models and deep reinforcement learning. In Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022. [161] Ayeong Lee, Ethan Che, and Tianyi Peng. 2025. How well do llms compress their own chain-of-thought? token complexity approach. [162] Jung Hyun Lee, June Yong Yang, Byeongho Heo, Dongyoon Han, and Kang Min Yoo. 2024. Token-supervised value models for enhancing mathematical reasoning capabilities of large language models. [163] Lucas Lehnert, Sainbayar Sukhbaatar, DiJia Su, Qinqing Zheng, Paul Mcvay, Michael Rabbat, and Yuandong Tian. 2024. Beyond a*: Better planning with transformers via search dynamics bootstrapping. [164] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. 2024a. Llava-onevision: Easy visual task transfer. ArXiv preprint, abs/2408.03326. [165] Chengzu Li, Wenshan Wu, Huanyu Zhang, Yan Xia, Shaoguang Mao, Li Dong, Ivan Vulic, and Furu Wei. 2025a. Imagine while reasoning in space: Multimodal visualization-of-thought. ArXiv preprint, abs/2501.07542. [166] Dacheng Li, Shiyi Cao, Chengkun Cao, Xiuyu Li, Shangyin Tan, Kurt Keutzer, Jiarong Xing, Joseph Gonzalez, and Ion Stoica. 2025b. S*: Test time scaling for code generation. ArXiv preprint, abs/2502.14382. [167] Dacheng Li, Shiyi Cao, Tyler Griggs, Shu Liu, Xiangxi Mo, Shishir G. Patil, Matei Zaharia, Joseph E. Gonzalez, and Ion Stoica. 2025c. Llms can easily learn to reason from demonstrations structure, not content, is what matters! [168] Jiazheng Li, Yuxiang Zhou, Junru Lu, Gladys Tyen, Lin Gui, Cesare Aloisi, and Yulan He. 2025d. Two heads are better than one: Dual-model verbal reflection at inference-time. [169] Jinming Li, Yichen Zhu, Zhibin Tang, Junjie Wen, Minjie Zhu, Xiaoyu Liu, Chengmeng Li, Ran Cheng, Yaxin Peng, and Feifei Feng. 2024b. Improving vision-language-action models via chain-of-affordance. ArXiv preprint, abs/2412.20451. [170] Loka Li, Zhenhao Chen, Guangyi Chen, Yixuan Zhang, Yusheng Su, Eric Xing, and Kun Zhang. 2024c. Confidence matters: Revisiting intrinsic self-correction capabilities of large language models. [171] Qingyao Li, Wei Xia, Kounianhua Du, Xinyi Dai, Ruiming Tang, Yasheng Wang, Yong Yu, and Weinan Zhang. 2024d. Rethinkmcts: Refining erroneous thoughts in monte carlo tree search for code generation. [172] Xiaoxi Li, Guanting Dong, Jiajie Jin, Yuyao Zhang, Yujia Zhou, Yutao Zhu, Peitian Zhang, and Zhicheng Dou. 2025e. Search-o1: Agentic search-enhanced large reasoning models. [173] Xuefeng Li, Haoyang Zou, and Pengfei Liu. 2025f. Limr: Less is more for rl scaling. [174] Xuefeng Li, Haoyang Zou, and Pengfei Liu. 2025g. Torl: Scaling tool-integrated rl. 62 References [175] Yiwei Li, Peiwen Yuan, Shaoxiong Feng, Boyuan Pan, Xinglin Wang, Bin Sun, Heda Wang, and Kan Li. 2024e. Escape sky-high cost: Early-stopping self-consistency for multi-step reasoning. [176] Yuetai Li, Xiang Yue, Zhangchen Xu, Fengqing Jiang, Luyao Niu, Bill Yuchen Lin, Bhaskar Ramasubramanian, and Radha Poovendran. 2025h. Small models struggle to learn from strong reasoners. [177] Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Remi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien de Masson dAutume, Igor Babuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov, James Molloy, Daniel J. Mankowitz, Esme Sutherland Robson, Pushmeet Kohli, Nando de Freitas, Koray Kavukcuoglu, and Oriol Vinyals. 2022. Competition-level code generation with alphacode. Science, 378(6624):10921097. [178] Ziniu Li. cold-start https://tangible-polo-203.notion.site/Can-Better-Cold-Start-Strategies-Improve-RL-Training-for-LLMs17aa0742a51680828616c867ed53bc6b. Notion Blog. strategies improve training better 2025. Can for rl llms? [179] Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Shuming Shi, and Zhaopeng Tu. 2023. Encouraging divergent thinking in large language models through multi-agent debate. [180] Zhenyi Liao, Qingsong Xie, Yanhao Zhang, Zijian Kong, Haonan Lu, Zhenyu Yang, and Zhijie Deng. 2025. Improved visual-spatial reasoning via r1-zero-like training. ArXiv preprint, abs/2504.00883. [181] Shalev Lifshitz, Sheila A. McIlraith, and Yilun Du. 2025. Multi-agent verification: Scaling test-time compute with multiple verifiers. [182] Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. 2023. Lets verify step by step. [183] Zhen Lin, Shubhendu Trivedi, and Jimeng Sun. 2024. Generating with confidence: Uncertainty quantification for black-box large language models. Trans. Mach. Learn. Res., 2024. [184] Fangfu Liu, Hanyang Wang, Yimo Cai, Kaiyan Zhang, Xiaohang Zhan, and Yueqi Duan. 2025a. Video-t1: Test-time scaling for video generation. ArXiv preprint, abs/2503.18942. [185] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. 2024a. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2629626306. [186] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023a. Visual instruction tuning. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023. [187] Jiacheng Liu, Andrew Cohen, Ramakanth Pasunuru, Yejin Choi, Hannaneh Hajishirzi, and Asli Celikyilmaz. 2023b. Dont throw away your value model! generating more preferable text with value-guided monte-carlo tree search decoding. [188] Jiaheng Liu, Dawei Zhu, Zhiqi Bai, Yancheng He, Huanxuan Liao, Haoran Que, Zekun Wang, Chenchen Zhang, Ge Zhang, Jiebin Zhang, Yuanxing Zhang, Zhuo Chen, Hangyu Guo, Shilong Li, Ziqiang Liu, Yong Shan, Yifan Song, Jiayi Tian, Wenhao Wu, Zhejian Zhou, Ruijie Zhu, Junlan Feng, Yang Gao, Shizhu He, Zhoujun Li, Tianyu Liu, Fanyu Meng, Wenbo Su, Yingshui Tan, Zili Wang, Jian Yang, Wei Ye, Bo Zheng, Wangchunshu Zhou, Wenhao Huang, Sujian Li, and Zhaoxiang Zhang. 2025b. comprehensive survey on long context language modeling. [189] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. 2021. Pre-train, prompt, and predict: systematic survey of prompting methods in natural language processing. [190] Runze Liu, Junqi Gao, Jian Zhao, Kaiyan Zhang, Xiu Li, Biqing Qi, Wanli Ouyang, and Bowen Zhou. 2025c. Can 1b llm surpass 405b llm? rethinking compute-optimal test-time scaling. [191] Sheng Liu, Haotian Ye, Lei Xing, and James Zou. 2024b. Reducing hallucinations in vision-language models via latent space steering. ArXiv preprint, abs/2410.15778. [192] Siyao Liu, He Zhu, Jerry Liu, Shulin Xin, Aoyan Li, Rui Long, Li Chen, Jack Yang, Jinxiang Xia, ZY Peng, et al. 2024c. Fullstack bench: Evaluating llms as full stack coder. ArXiv preprint, abs/2412.00535. [193] Tengxiao Liu, Qipeng Guo, Xiangkun Hu, Cheng Jiayang, Yue Zhang, Xipeng Qiu, and Zheng Zhang. 2024d. Can language models learn to skip steps? 63 References [194] Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. 2023c. G-eval: NLG evaluation using gpt-4 with better human alignment. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 25112522, Singapore. Association for Computational Linguistics. [195] Yixin Liu, Avi Singh, C. Daniel Freeman, John D. Co-Reyes, and Peter J. Liu. 2023d. Improving large language model fine-tuning for solving math problems. [196] Yuecheng Liu, Dafeng Chi, Shiguang Wu, Zhanguang Zhang, Yaochen Hu, Lingfeng Zhang, Yingxue Zhang, Shuang Wu, Tongtong Cao, Guowei Huang, et al. 2025d. Spatialcot: Advancing spatial reasoning through coordinate alignment and chain-of-thought for embodied task planning. ArXiv preprint, abs/2501.10074. [197] Yuqi Liu, Bohao Peng, Zhisheng Zhong, Zihao Yue, Fanbin Lu, Bei Yu, and Jiaya Jia. 2025e. Seg-zero: Reasoning-chain guided segmentation via cognitive reinforcement. ArXiv preprint, abs/2503.06520. [198] Zeyi Liu, Arpit Bahety, and Shuran Song. 2023e. Reflect: Summarizing robot experiences for failure explanation and correction. In Conference on Robot Learning, pages 34683484. PMLR. [199] Zhili Liu, Yunhao Gou, Kai Chen, Lanqing Hong, Jiahui Gao, Fei Mi, Yu Zhang, Zhenguo Li, Xin Jiang, Qun Liu, et al. 2024e. Mixture of insightful experts (mote): The synergy of thought chains and expert mixtures in self-alignment. ArXiv preprint, abs/2405.00557. [200] Zichen Liu, Changyu Chen, Wenjun Li, Tianyu Pang, Chao Du, and Min Lin. 2025f. There may not be aha moment in r1-zero-like training pilot study. https://oatllm.notion.site/oat-zero. Notion Blog. [201] Zichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee, and Min Lin. 2025g. Understanding r1-zero-like training: critical perspective. https://github.com/sail-sg/ understand-r1-zero. [202] Zijun Liu, Peiyi Wang, Runxin Xu, Shirong Ma, Chong Ruan, Peng Li, Yang Liu, and Yu Wu. 2025h. Inference-time scaling for generalist reward modeling. [203] Ziyu Liu, Zeyi Sun, Yuhang Zang, Xiaoyi Dong, Yuhang Cao, Haodong Duan, Dahua Lin, and Jiaqi Wang. 2025i. Visual-rft: Visual reinforcement fine-tuning. ArXiv preprint, abs/2503.01785. [204] Do Xuan Long, Duong Ngoc Yen, Anh Tuan Luu, Kenji Kawaguchi, Min-Yen Kan, and Nancy F. Chen. 2024. Multi-expert prompting improves reliability, safety and usefulness of large language models. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, EMNLP 2024, Miami, FL, USA, November 12-16, 2024, pages 2037020401. Association for Computational Linguistics. [205] Jieyi Long. 2023. Large language model guided tree-of-thought. [206] Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, and David Ha. 2024a. The AI Scientist: Towards fully automated open-ended scientific discovery. ArXiv preprint, abs/2408.06292. [207] Dakuan Lu, Xiaoyu Tan, Rui Xu, Tianchu Yao, Chao Qu, Wei Chu, Yinghui Xu, and Yuan Qi. 2025. Scp116k: high-quality problem-solution dataset and generalized pipeline for automated extraction in the higher education science domain. [208] Jianqiao Lu, Zhiyang Dou, Hongru Wang, Zeyu Cao, Jianbo Dai, Yingjia Wan, and Zhijiang Guo. 2024b. Autopsv: Automated process-supervised verifier. [209] Haotian Luo, Li Shen, Haiying He, Yibo Wang, Shiwei Liu, Wei Li, Naiqiang Tan, Xiaochun Cao, and Dacheng Tao. 2025a. O1-pruner: Length-harmonizing fine-tuning for o1-like reasoning pruning. [210] Liangchen Luo, Yinxiao Liu, Rosanne Liu, Samrat Phatale, Meiqi Guo, Harsh Lara, Yunxuan Li, Lei Shu, Yun Zhu, Lei Meng, Jiao Sun, and Abhinav Rastogi. 2024. Improve mathematical reasoning in language models by automated process supervision. [211] Michael Luo, Sijun Tan, Roy Huang, Xiaoxiang Shi, Rachel Xin, Colin Cai, Ameen Patel, Alpay Ariyak, Qingyang Wu, Ce Zhang, Li Erran Li, Raluca Ada Popa, and Ion Stoica. 2025b. Deepcoder: fully open-source 14b coder at o3-mini level. https://pretty-radio-b75.notion.site/DeepCoder-A-Fully-Open-Source-14B-Coder-atO3-mini-Level-1cf81902c14680b3bee5eb349a512a51. Notion Blog. [212] Michael Luo, Sijun Tan, Justin Wong, Xiaoxiang Shi, William Y. Tang, Manan Roongta, Colin Cai, Jeffrey Luo, Tianjun Zhang, Li Erran Li, Raluca Ada Popa, and Ion Stoica. 2025c. Deepscaler: Surpassing o1-preview with 1.5b model by scaling rl. https://pretty-radio-b75.notion.site/DeepScaleR-Surpassing-O1-Preview-with-a1-5B-Model-by-Scaling-RL-19681902c1468005bed8ca303013a4e2. Notion Blog. 64 References [213] Ruilin Luo, Zhuofan Zheng, Yifan Wang, Yiyao Yu, Xinzhe Ni, Zicheng Lin, Jin Zeng, and Yujiu Yang. 2025d. Ursa: Understanding and verifying chain-of-thought reasoning in multimodal mathematics. ArXiv preprint, abs/2501.04686. [214] Ruotian Ma, Peisong Wang, Cheng Liu, Xingyan Liu, Jiaqi Chen, Bang Zhang, Xin Zhou, Nan Du, and Jia Li. 2025a. S2r: Teaching llms to self-verify and self-correct via reinforcement learning. [215] Xinyin Ma, Guangnian Wan, Runpeng Yu, Gongfan Fang, and Xinchao Wang. 2025b. Cot-valve: Lengthcompressible chain-of-thought tuning. [216] Yan Ma, Steffi Chern, Xuyang Shen, Yiran Zhong, and Pengfei Liu. 2025c. Rethinking rl scaling for vision language models: transparent, from-scratch framework and comprehensive evaluation scheme. ArXiv preprint, abs/2504.02587. [217] Yingwei Ma, Yongbin Li, Yihong Dong, Xue Jiang, Rongyu Cao, Jue Chen, Fei Huang, and Binhua Li. 2025d. Thinking longer, not larger: Enhancing software engineering agents via scaling test-time compute. [218] Yingwei Ma, Yue Liu, Yue Yu, Yuanliang Zhang, Yu Jiang, Changjian Wang, and Shanshan Li. 2023. At which training stage does code data help llms reasoning? ArXiv preprint, abs/2309.16298. [219] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark. 2023. Self-refine: Iterative refinement with self-feedback. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023. [220] Potsawee Manakul, Adian Liusie, and Mark Gales. 2023. SelfCheckGPT: Zero-resource black-box hallucination detection for generative large language models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 90049017, Singapore. Association for Computational Linguistics. [221] Fanqing Meng, Lingxiao Du, and Xiangyan. Liu. 2025a. R1-multimodal-journey: journey to real multimodal r1. https://github.com/FanqingM/R1-Multimodal-Journey. [222] Fanqing Meng, Lingxiao Du, Zongkai Liu, Zhixiang Zhou, Quanfeng Lu, Daocheng Fu, Botian Shi, Wenhai Wang, Junjun He, Kaipeng Zhang, et al. 2025b. Mm-eureka: Exploring visual aha moment with rule-based large-scale reinforcement learning. ArXiv preprint, abs/2503.07365. [223] Yu Meng, Mengzhou Xia, and Danqi Chen. 2024. Simpo: Simple preference optimization with referencefree reward. [224] William Merrill and Ashish Sabharwal. 2023. The expressive power of transformers with chain of thought. [225] Meta. 2023. Llama 2: Open foundation and fine-tuned chat models. [226] Meta. 2024. The llama 3 herd of models. [227] Janet Metcalfe and Arthur Shimamura. 1994. Metacognition: Knowing about knowing. MIT press. [228] Zawalski Michał, Chen William, Pertsch Karl, Mees Oier, Finn Chelsea, and Levine Sergey. 2024. Robotic control via embodied chain-of-thought reasoning. ArXiv preprint, abs/2407.08693. [229] Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Koh, Mohit Iyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2023. FActScore: Fine-grained atomic evaluation of factual precision in long form text generation. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 1207612100, Singapore. Association for Computational Linguistics. [230] Yingqian Min, Zhipeng Chen, Jinhao Jiang, Jie Chen, Jia Deng, Yiwen Hu, Yiru Tang, Jiapeng Wang, Xiaoxue Cheng, Huatong Song, Wayne Xin Zhao, Zheng Liu, Zhongyuan Wang, and Ji-Rong Wen. 2024. Imitate, explore, and self-improve: reproduction report on slow-thinking reasoning systems. [231] Iman Mirzadeh, Keivan Alizadeh, Hooman Shahrokhi, Oncel Tuzel, Samy Bengio, and Mehrdad Farajtabar. 2024. Gsm-symbolic: Understanding the limitations of mathematical reasoning in large language models. [232] ModelScope. 2024. MMMU-Reasoning-Distill-Validation. https://modelscope.cn/datasets/ modelscope/MMMU-Reasoning-Distill-Validation. ModelScope. [233] Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Cand`es, and Tatsunori Hashimoto. 2025. s1: Simple test-time scaling. 65 References [234] Tergel Munkhbat, Namgyu Ho, Seo Hyun Kim, Yongjin Yang, Yujin Kim, and Se-Young Yun. 2025. Selftraining elicits concise reasoning in large language models. [235] Sania Nayab, Giulio Rossolini, Marco Simoni, Andrea Saracino, Giorgio Buttazzo, Nicolamaria Manes, and Fabrizio Giacomelli. 2024. Concise thoughts: Impact of output length on llm reasoning and cost. [236] Allen Newell, John Shaw, and Herbert Simon. 1959. Report on general problem solving program. In IFIP congress, page 64. Pittsburgh, PA. [237] Allen Newell, Herbert Alexander Simon, et al. 1972. Human problem solving. Prentice-hall Englewood Cliffs, NJ. [238] Shen Nie, Fengqi Zhu, Zebin You, Xiaolu Zhang, Jingyang Ou, Jun Hu, Jun Zhou, Yankai Lin, Ji-Rong Wen, and Chongxuan Li. 2025. Large language diffusion models. [239] Tobias Nipkow, Markus Wenzel, and Lawrence Paulson. 2002. Isabelle/HOL: proof assistant for higher-order logic. Springer. [240] Franz Nowak, Anej Svete, Alexandra Butoi, and Ryan Cotterell. 2024. On the representational capacity of neural language models with chain-of-thought reasoning. [241] Rafael Nunez, Michael Allen, Richard Gao, Carson Miller Rigoli, Josephine Relaford-Doyle, and Arturs Semenuks. 2019. What happened to cognitive science? Nature human behaviour, 3(8):782791. [242] Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, and Augustus Odena. 2021. Show your work: Scratchpads for intermediate computation with language models. [243] Theo X. Olausson, Jeevana Priya Inala, Chenglong Wang, Jianfeng Gao, and Armando Solar-Lezama. 2023. Is self-repair silver bullet for code generation? [244] OpenAI. 2023. Gpt-4 technical report. [245] OpenAI. 2024. Openai o1 system card. Accessed: 2024-11-07. [246] OpenAI. 2025a. Computer-using agent. openai.com. [247] OpenAI. 2025b. Introducing deep research. openai.com. [248] OpenAI. 2025a. Introducing openai o3 and o4-mini. Accessed: 2025-04-17. [249] OpenAI. 2025b. Thinking with images. Accessed: 2025-04-17. [250] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback. In Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022. [251] Jiayi Pan, Xingyao Wang, Graham Neubig, Navdeep Jaitly, Heng Ji, Alane Suhr, and Yizhe Zhang. 2024a. Training software engineering agents and verifiers with swe-gym. ArXiv preprint, abs/2412.21139. [252] Jiayi Pan, Yichi Zhang, Nicholas Tomlin, Yifei Zhou, Sergey Levine, and Alane Suhr. 2024b. Autonomous evaluation and refinement of digital agents. [253] Liangming Pan, Michael Saxon, Wenda Xu, Deepak Nathani, Xinyi Wang, and William Yang Wang. 2023. Automatically correcting large language models: Surveying the landscape of diverse self-correction strategies. [254] Jianhui Pang, Fanghua Ye, Derek Fai Wong, Xin He, Wanshun Chen, and Longyue Wang. 2024a. Anchorbased large language models. [255] Richard Yuanzhe Pang, Weizhe Yuan, Kyunghyun Cho, He He, Sainbayar Sukhbaatar, and Jason Weston. 2024b. Iterative reasoning preference optimization. [256] Peter S. Park, Simon Goldstein, Aidan OGara, Michael Chen, and Dan Hendrycks. 2024. AI deception: survey of examples, risks, and potential solutions. Patterns, 5(6):100988. [257] Dinesh Parthasarathy, Georgios D. Kontes, Axel Plinge, and Christopher Mutschler. 2023. C-MCTS: safe planning with monte carlo tree search. ArXiv preprint, abs/2305.16209. 66 References [258] Debjit Paul, Mete Ismayilzada, Maxime Peyrard, Beatriz Borges, Antoine Bosselut, Robert West, and Boi Faltings. 2024. REFINER: Reasoning feedback on intermediate representations. In Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers), pages 11001126, St. Julians, Malta. Association for Computational Linguistics. [259] Yingzhe Peng, Gongrui Zhang, Miaosen Zhang, Zhiyuan You, Jie Liu, Qipeng Zhu, Kai Yang, Xingzhong Xu, Xin Geng, and Xu Yang. 2025. Lmm-r1: Empowering 3b lmms with strong reasoning abilities through two-stage rule-based rl. ArXiv preprint, abs/2503.07536. [260] Long Phan, Alice Gatti, Ziwen Han, Nathaniel Li, and Josephina Hu et al. 2025. Humanitys last exam. [261] Stanislas Polu and Ilya Sutskever. 2020. Generative language modeling for automated theorem proving. [262] Pranav Putta, Edmund Mills, Naman Garg, Sumeet Motwani, Chelsea Finn, Divyansh Garg, and Rafael Rafailov. 2024. Agent q: Advanced reasoning and learning for autonomous ai agents. [263] Zhenting Qi, Mingyuan Ma, Jiahang Xu, Li Lyna Zhang, Fan Yang, and Mao Yang. 2024. Mutual reasoning makes smaller llms stronger problem-solvers. [264] Yiwei Qin, Xuefeng Li, Haoyang Zou, Yixiu Liu, Shijie Xia, Zhen Huang, Yixin Ye, Weizhe Yuan, Zhengzhong Liu, Yuanzhi Li, and Pengfei Liu. 2024. O1 replication journey: strategic progress report part 1. ArXiv preprint, abs/2410.18982. [265] Yiwei Qin, Weizhe Yuan, Graham Neubig, and Pengfei Liu. 2023. T5Score: Discriminative fine-tuning of generative evaluation metrics. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 1518515202, Singapore. Association for Computational Linguistics. [266] Yujia Qin, Yining Ye, Junjie Fang, Haoming Wang, Shihao Liang, Shizuo Tian, Junda Zhang, Jiahao Li, Yunxin Li, Shijue Huang, Wanjun Zhong, Kuanye Li, Jiale Yang, Yu Miao, Woyu Lin, Longxiang Liu, Xu Jiang, Qianli Ma, Jingyu Li, Xiaojun Xiao, Kai Cai, Chuang Li, Yaowei Zheng, Chaolin Jin, Chen Li, Xiao Zhou, Minchao Wang, Haoli Chen, Zhaojian Li, Haihua Yang, Haifeng Liu, Feng Lin, Tao Peng, Xin Liu, and Guang Shi. 2025. Ui-tars: Pioneering automated gui interaction with native agents. [267] Jiahao Qiu, Yifu Lu, Yifan Zeng, Jiacheng Guo, Jiayi Geng, Huazheng Wang, Kaixuan Huang, Yue Wu, and Mengdi Wang. 2024. Treebon: Enhancing inference-time alignment with speculative tree-search and best-of-n sampling. [268] Linlu Qiu, Liwei Jiang, Ximing Lu, Melanie Sclar, Valentina Pyatkin, Chandra Bhagavatula, Bailin Wang, Yoon Kim, Yejin Choi, Nouha Dziri, and Xiang Ren. 2023. Phenomenal yet puzzling: Testing inductive reasoning capabilities of language models with hypothesis refinement. [269] Yuxiao Qu, Matthew Y. R. Yang, Amrith Setlur, Lewis Tunstall, Edward Emanuel Beeching, Ruslan Salakhutdinov, and Aviral Kumar. 2025. Optimizing test-time compute via meta reinforcement fine-tuning. [270] Yuxiao Qu, Tianjun Zhang, Naman Garg, and Aviral Kumar. 2024. Recursive introspection: Teaching language model agents how to self-improve. [271] Team Qwen. 2024. Qvq: To see the world with wisdom. [272] Gollam Rabby, Farhana Keya, Parvez Zamil, and Soren Auer. 2024. Mc-nest enhancing mathematical reasoning in large language models with monte carlo nash equilibrium self-refine tree. [273] Rafael Rafailov, Yaswanth Chittepu, Ryan Park, Harshit Sikchi, Joey Hejna, Bradley Knox, Chelsea Finn, and Scott Niekum. 2024. Scaling laws for reward model overoptimization in direct alignment algorithms. [274] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D. Manning, Stefano Ermon, and Chelsea Finn. In Advances in 2023. Direct preference optimization: Your language model is secretly reward model. Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023. [275] Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. 2020. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In KDD 20: The 26th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, Virtual Event, CA, USA, August 23-27, 2020, pages 35053506. ACM. [276] David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R. Bowman. 2023. Gpqa: graduate-level google-proof q&a benchmark. 67 References [277] Allen Ren, Anushri Dixit, Alexandra Bodrova, Sumeet Singh, Stephen Tu, Noah Brown, Peng Xu, Leila Takayama, Fei Xia, Jake Varley, et al. 2023. Robots that ask for help: Uncertainty alignment for large language model planners. In 7th Annual Conference on Robot Learning. [278] Christopher Rosin. 2011. Multi-armed bandits with episode context. Annals of Mathematics and Artificial Intelligence, 61(3):203230. [279] Dongyu Ru, Lin Qiu, Xiangkun Hu, Tianhang Zhang, Peng Shi, Shuaichen Chang, Cheng Jiayang, Cunxiang Wang, Shichao Sun, Huanyu Li, Zizhao Zhang, Binjie Wang, Jiarong Jiang, Tong He, Zhiguo Wang, Pengfei Liu, Yue Zhang, and Zheng Zhang. 2024. RAGChecker: fine-grained framework for diagnosing retrievalaugmented generation. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track. [280] Yangjun Ruan, Neil Band, Chris J. Maddison, and Tatsunori Hashimoto. 2025. Reasoning to learn from latent thoughts. [281] Swarnadeep Saha, Omer Levy, Asli Celikyilmaz, Mohit Bansal, Jason Weston, and Xian Li. 2024a. Branchsolve-merge improves large language model evaluation and generation. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 83528370, Mexico City, Mexico. Association for Computational Linguistics. [282] Swarnadeep Saha, Xian Li, Marjan Ghazvininejad, Jason Weston, and Tianlu Wang. 2025. Learning to plan & reason for evaluation with thinking-llm-as-a-judge. [283] Swarnadeep Saha, Archiki Prasad, Justin Chih-Yao Chen, Peter Hase, Elias Stengel-Eskin, and Mohit Bansal. 2024b. System-1.x: Learning to balance fast and slow planning with language models. [284] Pranab Sahoo, Ayush Kumar Singh, Sriparna Saha, Vinija Jain, Samrat Mondal, and Aman Chadha. 2024. systematic survey of prompt engineering in large language models: Techniques and applications. [285] John Schulman, Philipp Moritz, Sergey Levine, Michael I. Jordan, and Pieter Abbeel. 2016. High-dimensional In 4th International Conference on Learning continuous control using generalized advantage estimation. Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings. [286] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. Proximal policy optimization algorithms. [287] Bytedance Seed. 2025. Seed-thinking-v1.5: Advancing superb reasoning models with reinforcement learning. https://github.com/ByteDance-Seed/Seed-Thinking-v1.5. [288] Amrith Setlur, Saurabh Garg, Xinyang Geng, Naman Garg, Virginia Smith, and Aviral Kumar. 2024a. Rl on incorrect synthetic data scales the efficiency of llm math reasoning by eight-fold. [289] Amrith Setlur, Chirag Nagpal, Adam Fisch, Xinyang Geng, Jacob Eisenstein, Rishabh Agarwal, Alekh Agarwal, Jonathan Berant, and Aviral Kumar. 2024b. Rewarding progress: Scaling automated process verifiers for llm reasoning. [290] Amrith Setlur, Nived Rajaraman, Sergey Levine, and Aviral Kumar. 2025. Scaling test-time compute without verification or rl is suboptimal. [291] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. 2024. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. [292] Gerald Shen, Zhilin Wang, Olivier Delalleau, Jiaqi Zeng, Yi Dong, Daniel Egert, Shengyang Sun, Jimmy Zhang, Sahil Jain, Ali Taghibakhshi, Markel Sanz Ausin, Ashwath Aithal, and Oleksii Kuchaiev. 2024. Nemoaligner: Scalable toolkit for efficient model alignment. [293] Haozhan Shen, Zilun Zhang, Qianqian Zhang, Ruochen Xu, and Tiancheng Zhao. 2025a. Vlm-r1: stable and generalizable r1-style large vision-language model. https://github.com/om-ai-lab/VLM-R1. [294] Yi Shen, Jian Zhang, Jieyun Huang, Shuming Shi, Wenjing Zhang, Jiangze Yan, Ning Wang, Kai Wang, and Shiguo Lian. 2025b. Dast: Difficulty-adaptive slow-thinking for large reasoning models. [295] Zhenyi Shen, Hanqi Yan, Linhai Zhang, Zhanghao Hu, Yali Du, and Yulan He. 2025c. Codi: Compressing chain-of-thought into continuous space via self-distillation. [296] Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. 2024. Hybridflow: flexible and efficient rlhf framework. ArXiv preprint, abs/2409.19256. 68 References [297] Freda Shi, Daniel Fried, Marjan Ghazvininejad, Luke Zettlemoyer, and Sida I. Wang. 2022. Natural language to code translation with execution. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 35333546, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. [298] Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. 2023. Reflexion: language agents with verbal reinforcement learning. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023. [299] David Silver, Aja Huang, Chris Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. 2016. Mastering the game of go with deep neural networks and tree search. nature, 529(7587):484489. [300] Avi Singh, John D. Co-Reyes, Rishabh Agarwal, Ankesh Anand, Piyush Patil, Xavier Garcia, Peter J. Liu, James Harrison, Jaehoon Lee, Kelvin Xu, Aaron Parisi, Abhishek Kumar, Alex Alemi, Alex Rizkowsky, Azade Nova, Ben Adlam, Bernd Bohnet, Gamaleldin Elsayed, Hanie Sedghi, Igor Mordatch, Isabelle Simpson, Izzeddin Gur, Jasper Snoek, Jeffrey Pennington, Jiri Hron, Kathleen Kenealy, Kevin Swersky, Kshiteej Mahajan, Laura Culp, Lechao Xiao, Maxwell L. Bileschi, Noah Constant, Roman Novak, Rosanne Liu, Tris Warkentin, Yundi Qian, Yamini Bansal, Ethan Dyer, Behnam Neyshabur, Jascha Sohl-Dickstein, and Noah Fiedel. 2023. Beyond human data: Scaling self-training for problem-solving with language models. [301] Nishad Singhi, Hritik Bansal, Arian Hosseini, Aditya Grover, Kai-Wei Chang, Marcus Rohrbach, and Anna Rohrbach. 2025. When to solve, when to verify: Compute-optimal problem solving and generative verification for llm reasoning. [302] Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. 2024. Scaling llm test-time compute optimally can be more effective than scaling model parameters. [303] Huatong Song, Jinhao Jiang, Yingqian Min, Jie Chen, Zhipeng Chen, Wayne Xin Zhao, Lei Fang, and Ji-Rong Wen. 2025. R1-searcher: Incentivizing the search capability in llms via reinforcement learning. ArXiv preprint, abs/2503.05592. [304] Zayne Sprague, Fangcong Yin, Juan Diego Rodriguez, Dongwei Jiang, Manya Wadhwa, Prasann Singhal, Xinyu Zhao, Xi Ye, Kyle Mahowald, and Greg Durrett. 2024. To cot or not to cot? chain-of-thought helps mainly on math and symbolic reasoning. [305] Giulio Starace, Oliver Jaffe, Dane Sherburn, James Aung, Jun Shern Chan, Leon Maksin, Rachel Dias, Evan Mays, Benjamin Kinsella, Wyatt Thompson, Johannes Heidecke, Amelia Glaese, and Tejal Patwardhan. 2025. Paperbench: Evaluating ais ability to replicate ai research. [306] Kaya Stechly, Matthew Marquez, and Subbarao Kambhampati. 2023. Gpt-4 doesnt know its wrong: An analysis of iterative prompting for reasoning problems. [307] Benedikt Stroebl, Sayash Kapoor, and Arvind Narayanan. 2024. Inference scaling flaws: The limits of llm resampling with imperfect verifiers. [308] DiJia Su, Sainbayar Sukhbaatar, Michael Rabbat, Yuandong Tian, and Qinqing Zheng. 2024. Dualformer: Controllable fast and slow thinking by learning with randomized reasoning traces. [309] Vighnesh Subramaniam, Antonio Torralba, and Shuang Li. 2024. Debategpt: Fine-tuning large language models with multi-agent debate supervision. [310] Theodore R. Sumers, Shunyu Yao, Karthik Narasimhan, and Thomas L. Griffiths. 2023. Cognitive architectures for language agents. [311] Hanshi Sun, Momin Haider, Ruiqi Zhang, Huitao Yang, Jiahao Qiu, Ming Yin, Mengdi Wang, Peter Bartlett, and Andrea Zanette. 2024a. Fast best-of-n decoding via speculative rejection. [312] Zhiqing Sun, Longhui Yu, Yikang Shen, Weiyang Liu, Yiming Yang, Sean Welleck, and Chuang Gan. 2024b. Easy-to-hard generalization: Scalable alignment beyond human supervision. [313] Richard Sutton, David McAllester, Satinder Singh, and Yishay Mansour. 1999. Policy gradient methods for reinforcement learning with function approximation. Advances in neural information processing systems, 12. [314] Fahim Tajwar, Anikait Singh, Archit Sharma, Rafael Rafailov, Jeff Schneider, Tengyang Xie, Stefano Ermon, Chelsea Finn, and Aviral Kumar. 2024. Preference fine-tuning of llms should leverage suboptimal, on-policy data. 69 References [315] Zhengyang Tang, Ziniu Li, Zhenyang Xiao, Tian Ding, Ruoyu Sun, Benyou Wang, Dayiheng Liu, Fei Huang, Tianyu Liu, Bowen Yu, and Junyang Lin. 2025. Realcritic: Towards effectiveness-driven evaluation of language model critiques. [316] AlphaCode2 Team. 2024a. Alphacode 2 technical report. [317] Gemini Robotics Team, Saminda Abeyruwan, Joshua Ainslie, Jean-Baptiste Alayrac, Montserrat Gonzalez Arenas, Travis Armstrong, Ashwin Balakrishna, Robert Baruch, Maria Bauza, Michiel Blokzijl, et al. 2025. Gemini robotics: Bringing ai into the physical world. ArXiv preprint, abs/2503.20020. [318] Open O1 Team. 2024b. Open o1. [319] OpenThoughts Team. 2025a. Open Thoughts. https://open-thoughts.ai. [320] Qwen Team. 2024c. Qwq: Reflect deeply on the boundaries of the unknown. [321] Qwen Team. 2025b. Qwq-32b: Embracing the power of reinforcement learning. [322] Omkar Thawakar, Dinura Dissanayake, Ketan More, Ritesh Thawkar, Ahmed Heakl, Noor Ahsan, Yuhao Li, Mohammed Zumri, Jean Lahoud, Rao Muhammad Anwer, et al. 2025. Llamav-o1: Rethinking step-by-step visual reasoning in llms. ArXiv preprint, abs/2501.06186. [323] Ye Tian, Baolin Peng, Linfeng Song, Lifeng Jin, Dian Yu, Haitao Mi, and Dong Yu. 2024. Toward selfimprovement of llms via imagination, searching, and criticizing. [324] Trieu Trinh, Yuhuai Wu, Quoc Le, He He, and Thang Luong. 2024. Solving olympiad geometry without human demonstrations. Nature. [325] Gladys Tyen, Hassan Mansoor, Victor Carbune, Peter Chen, and Tony Mak. 2023. Llms cannot find reasoning errors, but can correct them given the error location. [326] Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia Creswell, Geoffrey Irving, and Irina Higgins. 2022. Solving math word problems with processand outcome-based feedback. [327] Karthik Valmeekam, Matthew Marquez, and Subbarao Kambhampati. 2023. Can large language models really improve by self-critiquing their own plans? [328] Neeraj Varshney, Wenlin Yao, Hongming Zhang, Jianshu Chen, and Dong Yu. 2023. stitch in time saves nine: Detecting and mitigating hallucinations of llms by validating low-confidence generation. [329] Prakhar Verma, Sukruta Prakash Midigeshi, Gaurav Sinha, Arno Solin, Nagarajan Natarajan, and Amit Sharma. 2024. Plan*rag: Efficient test-time planning for retrieval augmented generation. [330] Barbara Von Eckardt. 1995. What is cognitive science? MIT press. [331] Guangya Wan, Yuqi Wu, Jie Chen, and Sheng Li. 2024. Dynamic self-consistency: Leveraging reasoning paths for efficient llm sampling. [332] Ante Wang, Linfeng Song, Ye Tian, Baolin Peng, Dian Yu, Haitao Mi, Jinsong Su, and Dong Yu. 2024a. Litesearch: Efficacious tree search for llm. [333] Ante Wang, Linfeng Song, Ye Tian, Dian Yu, Haitao Mi, Xiangyu Duan, Zhaopeng Tu, Jinsong Su, and Dong Yu. 2025a. Dont get lost in the trees: Streamlining llm reasoning by overcoming tree search exploration pitfalls. [334] Chaojie Wang, Yanchen Deng, Zhiyi Lyu, Liang Zeng, Jujie He, Shuicheng Yan, and Bo An. 2024b. Q*: Improving multi-step reasoning for llms with deliberative planning. [335] Evan Wang, Federico Cassano, Catherine Wu, Yunfeng Bai, Will Song, Vaskar Nath, Ziwen Han, Sean Hendryx, Summer Yue, and Hugh Zhang. 2024c. Planning in natural language improves llm search for code generation. [336] Haoxiang Wang, Wei Xiong, Tengyang Xie, Han Zhao, and Tong Zhang. 2024d. Interpretable preferences via multi-objective reward modeling and mixture-of-experts. [337] Haoyu Wang, Zeyu Qin, Li Shen, Xueqian Wang, Minhao Cheng, and Dacheng Tao. 2025b. Leveraging reasoning with guidelines to elicit and utilize knowledge for enhancing safety alignment. ArXiv preprint, abs/2502.04040. [338] Liang Wang, Haonan Chen, Nan Yang, Xiaolong Huang, Zhicheng Dou, and Furu Wei. 2025c. Chain-ofretrieval augmented generation. 70 References [339] Peiyi Wang, Lei Li, Zhihong Shao, R. X. Xu, Damai Dai, Yifei Li, Deli Chen, Y. Wu, and Zhifang Sui. 2023a. Math-shepherd: Verify and reinforce llms step-by-step without human annotations. [340] Ruocheng Wang, Eric Zelikman, Gabriel Poesia, Yewen Pu, Nick Haber, and Noah D. Goodman. 2023b. Hypothesis search: Inductive reasoning with language models. [341] Tianlong Wang, Junzhe Chen, Xueting Han, and Jing Bai. 2024e. Cpl: Critical plan step learning boosts llm generalization in reasoning tasks. [342] Xiaodong Wang and Peixi Peng. 2025. Open-r1-video. https://github.com/ Wang-Xiaodong1899/Open-R1-Video. [343] Xinglin Wang, Shaoxiong Feng, Yiwei Li, Peiwen Yuan, Yueqi Zhang, Boyuan Pan, Heda Wang, Yao Hu, and Kan Li. 2024f. Make every penny count: Difficulty-adaptive self-consistency for cost-efficient reasoning. [344] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V. Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2023c. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net. [345] Yi Wang, Mushui Liu, Wanggui He, Longxiang Zhang, Ziwei Huang, Guanghao Zhang, Fangxun Shu, Zhong Tao, Dong She, Zhelun Yu, Haoyuan Li, Weilong Dai, Mingli Song, Jie Song, and Hao Jiang. 2025d. Mint: Multi-modal chain of thought in unified generative models for enhanced image generation. ArXiv preprint, abs/2503.01298. [346] Yiming Wang, Pei Zhang, Siyuan Huang, Baosong Yang, Zhuosheng Zhang, Fei Huang, and Rui Wang. 2025e. Sampling-efficient test-time scaling: Self-estimating the best-of-n sampling in early decoding. [347] Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, Tianle Li, Max Ku, Kai Wang, Alex Zhuang, Rongqi Fan, Xiang Yue, and Wenhu Chen. 2024g. Mmlu-pro: more robust and challenging multi-task language understanding benchmark. [348] Yutong Wang, Pengliang Ji, Chaoqun Yang, Kaixin Li, Ming Hu, Jiaoyang Li, and Guillaume Sartoretti. 2025f. Mcts-judge: Test-time scaling in llm-as-a-judge for code correctness evaluation. [349] Zengzhi Wang, Xuefeng Li, Rui Xia, and Pengfei Liu. 2023d. Mathpile: billion-token-scale pretraining corpus for math. [350] Zhenglin Wang, Jialong Wu, Yilong Lai, Congzhi Zhang, and Deyu Zhou. 2024h. Seed: Accelerating reasoning tree construction via scheduled speculative decoding. [351] Zihan Wang, Yunxuan Li, Yuexin Wu, Liangchen Luo, Le Hou, Hongkun Yu, and Jingbo Shang. 2024i. Multi-step problem solving through verifier: An empirical analysis on model-induced process supervision. [352] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. 2022. Chain-of-thought prompting elicits reasoning in large language models. In Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022. [353] Yuxiang Wei, Olivier Duchenne, Jade Copet, Quentin Carbonneaux, Lingming Zhang, Daniel Fried, Gabriel Synnaeve, Rishabh Singh, and Sida I. Wang. 2025. Swe-rl: Advancing llm reasoning via reinforcement learning on open software evolution. [354] Sean Welleck, Amanda Bertsch, Matthew Finlayson, Hailey Schoelkopf, Alex Xie, Graham Neubig, Ilia Kulikov, and Zaid Harchaoui. 2024. From decoding to meta-generation: Inference-time algorithms for large language models. [355] Sean Welleck, Ximing Lu, Peter West, Faeze Brahman, Tianxiao Shen, Daniel Khashabi, and Yejin Choi. 2023. Generating sequences by learning to self-correct. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net. [356] Han Wu, Yuxuan Yao, Shuqi Liu, Zehua Liu, Xiaojin Fu, Xiongwei Han, Xing Li, Hui-Ling Zhen, Tao Zhong, and Mingxuan Yuan. 2025. Unlocking efficient long-to-short llm reasoning with model merging. [357] Tianhao Wu, Janice Lan, Weizhe Yuan, Jiantao Jiao, Jason Weston, and Sainbayar Sukhbaatar. 2024a. Thinking llms: General instruction following with thought generation. [358] Ting Wu, Xuefeng Li, and Pengfei Liu. 2024b. Progress or regress? self-improvement reversal in post-training. 71 References [359] Yangzhen Wu, Zhiqing Sun, Shanda Li, Sean Welleck, and Yiming Yang. 2024c. Inference scaling laws: An empirical analysis of compute-optimal inference for problem-solving with language models. [360] Zhaofeng Wu, Linlu Qiu, Alexis Ross, Ekin Akyurek, Boyuan Chen, Bailin Wang, Najoung Kim, Jacob Andreas, and Yoon Kim. 2024d. Reasoning or reciting? exploring the capabilities and limitations of language models through counterfactual tasks. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 18191862, Mexico City, Mexico. Association for Computational Linguistics. [361] Zhiheng Xi, Dingwen Yang, Jixuan Huang, Jiafu Tang, Guanyu Li, Yiwen Ding, Wei He, Boyang Hong, Shihan Do, Wenyu Zhan, Xiao Wang, Rui Zheng, Tao Ji, Xiaowei Shi, Yitao Zhai, Rongxiang Weng, Jingang Wang, Xunliang Cai, Tao Gui, Zuxuan Wu, Qi Zhang, Xipeng Qiu, Xuanjing Huang, and Yu-Gang Jiang. 2024. Enhancing llm reasoning via critique models with test-time and training-time supervision. [362] Heming Xia, Yongqi Li, Chak Tou Leong, Wenjie Wang, and Wenjie Li. 2025. Tokenskip: Controllable chain-of-thought compression in llms. [363] Shijie Xia, Xuefeng Li, Yixin Liu, Tongshuang Wu, and Pengfei Liu. 2024. Evaluating mathematical reasoning beyond accuracy. [364] Violet Xiang, Charlie Snell, Kanishk Gandhi, Alon Albalak, Anikait Singh, Chase Blagden, Duy Phung, Rafael Rafailov, Nathan Lile, Dakota Mahan, Louis Castricato, Jan-Philipp Franken, Nick Haber, and Chelsea Finn. 2025. Towards system 2 reasoning in llms: Learning how to think with meta chain-of-thought. [365] Tian Xie, Zitian Gao, Qingnan Ren, Haoming Luo, Yuqian Hong, Bryan Dai, Joey Zhou, Kai Qiu, Zhirong Wu, and Chong Luo. 2025. Logic-rl: Unleashing llm reasoning with rule-based reinforcement learning. [366] Tianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, Toh Jing Hua, Zhoujun Cheng, Dongchan Shin, Fangyu Lei, Yitao Liu, Yiheng Xu, Shuyan Zhou, Silvio Savarese, Caiming Xiong, Victor Zhong, and Tao Yu. 2024a. Osworld: Benchmarking multimodal agents for open-ended tasks in real computer environments. [367] Yuxi Xie, Anirudh Goyal, Wenyue Zheng, Min-Yen Kan, Timothy P. Lillicrap, Kenji Kawaguchi, and Michael Shieh. 2024b. Monte carlo tree search boosts reasoning via iterative preference learning. [368] Yuxi Xie, Kenji Kawaguchi, Yiran Zhao, James Xu Zhao, Min-Yen Kan, Junxian He, and Michael Qizhe Xie. 2023. Self-evaluation guided beam search for reasoning. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023. [369] Huajian Xin, Z. Z. Ren, Junxiao Song, Zhihong Shao, Wanjia Zhao, Haocheng Wang, Bo Liu, Liyue Zhang, Xuan Lu, Qiushi Du, Wenjun Gao, Qihao Zhu, Dejian Yang, Zhibin Gou, Z. F. Wu, Fuli Luo, and Chong Ruan. 2024. Deepseek-prover-v1.5: Harnessing proof assistant feedback for reinforcement learning and monte-carlo tree search. [370] Ran Xin, Chenguang Xi, Jie Yang, Feng Chen, Hang Wu, Xia Xiao, Yifan Sun, Shen Zheng, and Kai Shen. 2025. Bfs-prover: Scalable best-first tree search for llm-based automatic theorem proving. [371] Wei Xiong, Hanning Zhang, Chenlu Ye, Lichang Chen, Nan Jiang, and Tong Zhang. 2025. Self-rewarding correction for mathematical reasoning. [372] Wang Xiyao, Yang Zhengyuan, Li Linjie, Lu Hongjin, Xu Yuancheng, Lin Chung-Ching Lin, Lin Kevin, Huang Furong, and Wang Lijuan. 2024. Scaling inference-time search with vision value model for improved visual comprehension. ArXiv preprint, abs/2412.03704. [373] Bin Xu, Yiguan Lin, Yinghao Li, and Yang Gao. 2024a. Sra-mcts: Self-driven reasoning augmentation with monte carlo tree search for code generation. [374] Guowei Xu, Peng Jin, Li Hao, Yibing Song, Lichao Sun, and Li Yuan. 2024b. Llava-cot: Let vision language models reason step-by-step. ArXiv preprint, abs/2411.10440. [375] Haotian Xu, Xing Wu, Weinong Wang, Zhongzhi Li, Da Zheng, Boyuan Chen, Yi Hu, Shijia Kang, Jiaming Ji, Yingying Zhang, Zhijiang Guo, Yaodong Yang, Muhan Zhang, and Debing Zhang. 2025a. Redstar: Does scaling long-cot data unlock better slow-reasoning systems? [376] Silei Xu, Wenhao Xie, Lingxiao Zhao, and Pengcheng He. 2025b. Chain of draft: Thinking faster by writing less. 72 References [377] Wenda Xu, Guanglei Zhu, Xuandong Zhao, Liangming Pan, Lei Li, and William Yang Wang. 2024c. Pride and prejudice: Llm amplifies self-bias in self-refinement. [378] Yige Xu, Xu Guo, Zhiwei Zeng, and Chunyan Miao. 2025c. Softcot: Soft chain-of-thought for efficient reasoning with llms. [379] Zhangchen Xu, Yang Liu, Yueqin Yin, Mingyuan Zhou, and Radha Poovendran. 2025d. Kodcode: diverse, challenging, and verifiable synthetic dataset for coding. ArXiv preprint, abs/2503.02951. [380] Yuchen Yan, Yongliang Shen, Yang Liu, Jin Jiang, Mengdi Zhang, Jian Shao, and Yueting Zhuang. 2025. Inftythink: Breaking the length limits of long-context reasoning in large language models. [381] An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, Keming Lu, Mingfeng Xue, Runji Lin, Tianyu Liu, Xingzhang Ren, and Zhenru Zhang. 2024a. Qwen2.5-math technical report: Toward mathematical expert model via self-improvement. [382] Kaiyu Yang, Gabriel Poesia, Jingxuan He, Wenda Li, Kristin Lauter, Swarat Chaudhuri, and Dawn Song. 2024b. Formal mathematical reasoning: new frontier in ai. [383] Wen Yang, Minpeng Liao, and Kai Fan. 2024c. Markov chain of thought for efficient mathematical reasoning. [384] Wenkai Yang, Shuming Ma, Yankai Lin, and Furu Wei. 2025a. Towards thinking-optimal scaling of test-time compute for llm reasoning. [385] Yi Yang, Xiaoxuan He, Hongkun Pan, Xiyan Jiang, Yan Deng, Xingtao Yang, Haoyu Lu, Dacheng Yin, Fengyun Rao, Minfeng Zhu, Bo Zhang, and Wei Chen. 2025b. R1-onevision: Advancing generalized multimodal reasoning through cross-modal formalization. [386] Huanjin Yao, Jiaxing Huang, Wenhao Wu, Jingyi Zhang, Yibo Wang, Shunyu Liu, Yingjie Wang, Yuxin Song, Haocheng Feng, Li Shen, et al. 2024a. Mulberry: Empowering mllm with o1-like reasoning and reflection via collective monte carlo tree search. ArXiv preprint, abs/2412.18319. [387] Jinwei Yao, Kaiqi Chen, Kexun Zhang, Jiaxuan You, Binhang Yuan, Zeke Wang, and Tao Lin. 2024b. Deft: Decoding with flash tree-attention for efficient tree-structured llm inference. [388] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. 2023a. Tree of thoughts: Deliberate problem solving with large language models. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023. [389] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R. Narasimhan, and Yuan Cao. 2023b. React: Synergizing reasoning and acting in language models. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net. [390] Tian Ye, Zicheng Xu, Yuanzhi Li, and Zeyuan Allen-Zhu. 2024. Physics of language models: Part 2.2, how to learn from mistakes on grade-school math problems. [391] Yixin Ye, Zhen Huang, Yang Xiao, Ethan Chern, Shijie Xia, and Pengfei Liu. 2025. Limo: Less is more for reasoning. [392] Edward Yeo, Yuxuan Tong, Morry Niu, Graham Neubig, and Xiang Yue. 2025. Demystifying long chain-ofthought reasoning in llms. [393] Ping Yu, Jing Xu, Jason Weston, and Ilia Kulikov. 2024a. Distilling system 2 into system 1. [394] Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, Haibin Lin, Zhiqi Lin, Bole Ma, Guangming Sheng, Yuxuan Tong, Chi Zhang, Mofan Zhang, Wang Zhang, Hang Zhu, Jinhua Zhu, Jiaze Chen, Jiangjie Chen, Chengyi Wang, Hongli Yu, Weinan Dai, Yuxuan Song, Xiangpeng Wei, Hao Zhou, Jingjing Liu, Wei-Ying Ma, Ya-Qin Zhang, Lin Yan, Mu Qiao, Yonghui Wu, and Mingxuan Wang. 2025a. Dapo: An open-source llm reinforcement learning system at scale. [395] Tian Yu, Shaolei Zhang, and Yang Feng. 2024b. Auto-rag: Autonomous retrieval-augmented generation for large language models. [396] Zhaojian Yu, Yinghao Wu, Yilun Zhao, Arman Cohan, and Xiao-Ping Zhang. 2025b. Z1: Efficient test-time scaling with code. [397] Zishun Yu, Tengyu Xu, Di Jin, Karthik Abinav Sankararaman, Yun He, Wenxuan Zhou, Zhouhao Zeng, Eryk Helenowski, Chen Zhu, Sinong Wang, Hao Ma, and Han Fang. 2025c. Think smarter not harder: Adaptive reasoning with inference aware optimization. 73 References [398] Lifan Yuan, Wendi Li, Huayu Chen, Ganqu Cui, Ning Ding, Kaiyan Zhang, Bowen Zhou, Zhiyuan Liu, and Hao Peng. 2024a. Free process rewards without process labels. [399] Weizhe Yuan, Pengfei Liu, and Matthias Galle. 2024b. Llmcrit: Teaching large language models to use criteria. ArXiv preprint, abs/2403.01069. [400] Weizhe Yuan, Graham Neubig, and Pengfei Liu. 2021. Bartscore: Evaluating generated text as text generation. In Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pages 2726327277. [401] Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Xian Li, Sainbayar Sukhbaatar, Jing Xu, and Jason Weston. 2024c. Self-rewarding language models. [402] Yufeng Yuan, Yu Yue, Ruofei Zhu, Tiantian Fan, and Lin Yan. 2025. Whats behind ppos collapse in long-cot? value optimization holds the secret. [403] Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Keming Lu, Chuanqi Tan, Chang Zhou, and Jingren Zhou. 2023. Scaling relationship on learning mathematical reasoning with large language models. ArXiv preprint, abs/2308.01825. [404] Yu Yue, Yufeng Yuan, Qiying Yu, Xiaochen Zuo, Ruofei Zhu, Wenyuan Xu, Jiaze Chen, Chengyi Wang, TianTian Fan, Zhengyin Du, Xiangpeng Wei, Xiangyu Yu, Gaohong Liu, Juncai Liu, Lingjun Liu, Haibin Lin, Zhiqi Lin, Bole Ma, Chi Zhang, Mofan Zhang, Wang Zhang, Hang Zhu, Ru Zhang, Xin Liu, Mingxuan Wang, Yonghui Wu, and Lin Yan. 2025. Vapo: Efficient and reliable reinforcement learning for advanced reasoning tasks. [405] Zhenrui Yue, Honglei Zhuang, Aijun Bai, Kai Hui, Rolf Jagerman, Hansi Zeng, Zhen Qin, Dong Wang, Xuanhui Wang, and Michael Bendersky. 2024. Inference scaling for long-context retrieval augmented generation. [406] Milan Zeleny. 1987. Management support systems: Towards integrated knowledge management. Human systems management, 7(1):5970. [407] Eric Zelikman, Georges Harik, Yijia Shao, Varuna Jayasiri, Nick Haber, and Noah D. Goodman. 2024a. Quiet-star: Language models can teach themselves to think before speaking. [408] Eric Zelikman, Eliana Lorch, Lester Mackey, and Adam Tauman Kalai. 2024b. Self-taught optimizer (stop): Recursively self-improving code generation. In First Conference on Language Modeling. [409] Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah D. Goodman. 2022. Star: Bootstrapping reasoning with reasoning. In Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022. [410] Weihao Zeng, Yuzhen Huang, Wei Liu, Keqing He, Qian Liu, Zejun Ma, and Junxian He. 2025a. 7b model and 8k examples: Emerging reasoning with reinforcement learning is both effective and efficient. https: //hkust-nlp.notion.site/simplerl-reason. Notion Blog. [411] Zhiyuan Zeng, Qinyuan Cheng, Zhangyue Yin, Bo Wang, Shimin Li, Yunhua Zhou, Qipeng Guo, Xuanjing Huang, and Xipeng Qiu. 2024. Scaling of search and learning: roadmap to reproduce o1 from reinforcement learning perspective. [412] Zhiyuan Zeng, Qinyuan Cheng, Zhangyue Yin, Yunhua Zhou, and Xipeng Qiu. 2025b. Revisiting the test-time scaling of o1-like models: Do they truly possess test-time scaling capabilities? [413] Dan Zhang, Sining Zhoubian, Ziniu Hu, Yisong Yue, Yuxiao Dong, and Jie Tang. 2024a. Rest-mcts*: Llm self-training via process reward guided tree search. [414] Di Zhang, Xiaoshui Huang, Dongzhan Zhou, Yuqiang Li, and Wanli Ouyang. 2024b. Accessing gpt-4 level mathematical olympiad solutions via monte carlo tree self-refine with llama-3 8b. [415] Di Zhang, Jianbo Wu, Jingdi Lei, Tong Che, Jiatong Li, Tong Xie, Xiaoshui Huang, Shufei Zhang, Marco Pavone, Yuqiang Li, Wanli Ouyang, and Dongzhan Zhou. 2024c. Llama-berry: Pairwise optimization for o1-like olympiad-level mathematical reasoning. [416] Hugh Zhang, Jeff Da, Dean Lee, Vaughn Robinson, Catherine Wu, Will Song, Tiffany Zhao, Pranav Raja, Charlotte Zhuang, Dylan Slack, Qin Lyu, Sean Hendryx, Russell Kaplan, Michele Lunati, and Summer Yue. 2024d. careful examination of large language model performance on grade school arithmetic. [417] Jingyi Zhang, Jiaxing Huang, Sheng Jin, and Shijian Lu. 2024e. Vision-language models for vision tasks: survey. IEEE Transactions on Pattern Analysis and Machine Intelligence. References [418] Jintian Zhang, Yuqi Zhu, Mengshu Sun, Yujie Luo, Shuofei Qiao, Lun Du, Da Zheng, Huajun Chen, and Ningyu Zhang. 2025a. Lightthinker: Thinking step-by-step compression. [419] Qiyuan Zhang, Yufei Wang, Yuxin Jiang, Liangyou Li, Chuhan Wu, Yasheng Wang, Xin Jiang, Lifeng Shang, Ruiming Tang, Fuyuan Lyu, and Chen Ma. 2025b. Crowd comparative reasoning: Unlocking comprehensive evaluations for llm-as-a-judge. [420] Shimao Zhang, Xiao Liu, Xin Zhang, Junxiao Liu, Zheheng Luo, Shujian Huang, and Yeyun Gong. 2025c. Process-based self-rewarding language models. [421] Shun Zhang, Zhenfang Chen, Yikang Shen, Mingyu Ding, Joshua B. Tenenbaum, and Chuang Gan. 2023. Planning with large language models for code generation. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net. [422] Wenqi Zhang, Mengna Wang, Gangao Liu, Xu Huixin, Yiwei Jiang, Yongliang Shen, Guiyang Hou, Zhe Zheng, Hang Zhang, Xin Li, et al. 2025d. Embodied-reasoner: Synergizing visual search, reasoning, and action for embodied interactive tasks. ArXiv preprint, abs/2503.21696. [423] Xuan Zhang, Chao Du, Tianyu Pang, Qian Liu, Wei Gao, and Min Lin. 2024f. Chain of preference optimization: Improving chain-of-thought reasoning in llms. [424] Yichi Zhang, Siyuan Zhang, Yao Huang, Zeyu Xia, Zhengwei Fang, Xiao Yang, Ranjie Duan, Dong Yan, Yinpeng Dong, and Jun Zhu. 2025e. Stair: Improving safety alignment with introspective reasoning. ArXiv preprint, abs/2502.02384. [425] Yuxiang Zhang, Shangxi Wu, Yuqi Yang, Jiangming Shu, Jinlin Xiao, Chao Kong, and Jitao Sang. 2024g. O1-coder: An o1 replication for coding. [426] Zijian Zhang, Kaiyuan Zheng, Zhaorun Chen, Joel Jang, Yi Li, Chaoqi Wang, Mingyu Ding, Dieter Fox, and Huaxiu Yao. 2024h. Grape: Generalizing robot policy via preference alignment. ArXiv preprint, abs/2411.19309. [427] Qingqing Zhao, Yao Lu, Moo Jin Kim, Zipeng Fu, Zhuoyang Zhang, Yecheng Wu, Zhaoshuo Li, Qianli Ma, Song Han, Chelsea Finn, et al. 2025. Cot-vla: Visual chain-of-thought reasoning for vision-language-action models. ArXiv preprint, abs/2503.22020. [428] Ruochen Zhao, Xingxuan Li, Shafiq Joty, Chengwei Qin, and Lidong Bing. 2023a. Verify-and-edit: knowledge-enhanced chain-of-thought framework. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 58235840, Toronto, Canada. Association for Computational Linguistics. [429] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. 2023b. survey of large language models. [430] Yanli Zhao, Andrew Gu, Rohan Varma, Liang Luo, Chien-Chin Huang, Min Xu, Less Wright, Hamid Shojanazeri, Myle Ott, Sam Shleifer, Alban Desmaison, Can Balioglu, Pritam Damania, Bernard Nguyen, Geeta Chauhan, Yuchen Hao, Ajit Mathews, and Shen Li. 2023c. Pytorch fsdp: Experiences on scaling fully sharded data parallel. [431] Yu Zhao, Huifeng Yin, Bo Zeng, Hao Wang, Tianqi Shi, Chenyang Lyu, Longyue Wang, Weihua Luo, and Kaifu Zhang. 2024. Marco-o1: Towards open reasoning models for open-ended solutions. [432] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. 2023a. Judging llm-as-a-judge with mt-bench and chatbot arena. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023. [433] Lianmin Zheng, Liangsheng Yin, Zhiqiang Xie, Chuyue Sun, Jeff Huang, Cody Hao Yu, Shiyi Cao, Christos Kozyrakis, Ion Stoica, Joseph E. Gonzalez, Clark Barrett, and Ying Sheng. 2023b. Sglang: Efficient execution of structured language model programs. [434] Yuxiang Zheng, Dayuan Fu, Xiangkun Hu, Xiaojie Cai, Lyumanshan Ye, Pengrui Lu, and Pengfei Liu. 2025. Deepresearcher: Scaling deep research via reinforcement learning in real-world environments. [435] Andy Zhou, Kai Yan, Michal Shlapentokh-Rothman, Haohan Wang, and Yu-Xiong Wang. 2023. Language agent tree search unifies reasoning acting and planning in language models. 75 References [436] Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma, Luke Zettlemoyer, and Omer Levy. 2024a. Transfusion: Predict the next token and diffuse images with one multi-modal model. ArXiv preprint, abs/2408.11039. [437] Fan Zhou, Zengzhi Wang, Qian Liu, Junlong Li, and Pengfei Liu. 2024b. Programming every example: Lifting pre-training data quality like experts at scale. [438] Hengguang Zhou, Xirui Li, Ruochen Wang, Minhao Cheng, Tianyi Zhou, and Cho-Jui Hsieh. 2025a. R1zeros aha moment in visual reasoning on 2b non-sft model. [439] Yifei Zhou, Song Jiang, Yuandong Tian, Jason Weston, Sergey Levine, Sainbayar Sukhbaatar, and Xian Li. 2025b. Sweet-rl: Training multi-turn llm agents on collaborative reasoning tasks. [440] Qihao Zhu, Daya Guo, Zhihong Shao, Dejian Yang, Peiyi Wang, Runxin Xu, Wu, Yukun Li, Huazuo Gao, Shirong Ma, et al. 2024. Deepseek-coder-v2: Breaking the barrier of closed-source models in code intelligence. ArXiv preprint, abs/2406.11931. [441] Yuchen Zhuang, Xiang Chen, Tong Yu, Saayan Mitra, Victor Bursztyn, Ryan A. Rossi, Somdeb Sarkhel, and Chao Zhang. 2023. Toolchain*: Efficient action space navigation in large language models with a* search. [442] Mingchen Zhuge, Changsheng Zhao, Dylan Ashley, Wenyi Wang, Dmitrii Khizbullin, Yunyang Xiong, Zechun Liu, Ernie Chang, Raghuraman Krishnamoorthi, Yuandong Tian, Yangyang Shi, Vikas Chandra, and Jurgen Schmidhuber. 2024. Agent-as-a-judge: Evaluate agents with agents."
        }
    ],
    "affiliations": [
        "Generative AI Research Lab (GAIR)",
        "SII",
        "Shanghai Jiao Tong University"
    ]
}