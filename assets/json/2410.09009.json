{
    "paper_title": "Semantic Score Distillation Sampling for Compositional Text-to-3D Generation",
    "authors": [
        "Ling Yang",
        "Zixiang Zhang",
        "Junlin Han",
        "Bohan Zeng",
        "Runjia Li",
        "Philip Torr",
        "Wentao Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Generating high-quality 3D assets from textual descriptions remains a pivotal challenge in computer graphics and vision research. Due to the scarcity of 3D data, state-of-the-art approaches utilize pre-trained 2D diffusion priors, optimized through Score Distillation Sampling (SDS). Despite progress, crafting complex 3D scenes featuring multiple objects or intricate interactions is still difficult. To tackle this, recent methods have incorporated box or layout guidance. However, these layout-guided compositional methods often struggle to provide fine-grained control, as they are generally coarse and lack expressiveness. To overcome these challenges, we introduce a novel SDS approach, Semantic Score Distillation Sampling (SemanticSDS), designed to effectively improve the expressiveness and accuracy of compositional text-to-3D generation. Our approach integrates new semantic embeddings that maintain consistency across different rendering views and clearly differentiate between various objects and parts. These embeddings are transformed into a semantic map, which directs a region-specific SDS process, enabling precise optimization and compositional generation. By leveraging explicit semantic guidance, our method unlocks the compositional capabilities of existing pre-trained diffusion models, thereby achieving superior quality in 3D content generation, particularly for complex objects and scenes. Experimental results demonstrate that our SemanticSDS framework is highly effective for generating state-of-the-art complex 3D content. Code: https://github.com/YangLing0818/SemanticSDS-3D"
        },
        {
            "title": "Start",
            "content": "4 2 0 2 1 1 ] . [ 1 9 0 0 9 0 . 0 1 4 2 : r a"
        },
        {
            "title": "Preprint",
            "content": "SEMANTIC SCORE DISTILLATION SAMPLING FOR COMPOSITIONAL TEXT-TO-3D GENERATION Ling Yang1, Zixiang Zhang1, Junlin Han 2, Bohan Zeng1, Runjia Li2 Philip Torr2, Wentao Zhang1 1Peking University Project: https://github.com/YangLing0818/SemanticSDS-3D 2University of Oxford"
        },
        {
            "title": "ABSTRACT",
            "content": "Generating high-quality 3D assets from textual descriptions remains pivotal challenge in computer graphics and vision research. Due to the scarcity of 3D data, state-of-the-art approaches utilize pre-trained 2D diffusion priors, optimized through Score Distillation Sampling (SDS). Despite progress, crafting complex 3D scenes featuring multiple objects or intricate interactions is still difficult. To tackle this, recent methods have incorporated box or layout guidance. However, these layout-guided compositional methods often struggle to provide fine-grained control, as they are generally coarse and lack expressiveness. To overcome these challenges, we introduce novel SDS approach, Semantic Score Distillation Sampling (SEMANTICSDS), designed to effectively improve the expressiveness and accuracy of compositional text-to-3D generation. Our approach integrates new semantic embeddings that maintain consistency across different rendering views and clearly differentiate between various objects and parts. These embeddings are transformed into semantic map, which directs region-specific SDS process, enabling precise optimization and compositional generation. By leveraging explicit semantic guidance, our method unlocks the compositional capabilities of existing pre-trained diffusion models, thereby achieving superior quality in 3D content generation, particularly for complex objects and scenes. Experimental results demonstrate that our SEMANTICSDS framework is highly effective for generating state-of-the-art complex 3D content."
        },
        {
            "title": "INTRODUCTION",
            "content": "Generating high-quality 3D assets from textual descriptions is long-standing goal in computer graphics and vision research. However, due to the scarcity of 3D data, existing text-to-3D generation models have primarily relied on leveraging powerful pre-trained 2D diffusion priors to optimize 3D representations, typically based on score distillation sampling (SDS) loss (Poole et al., 2023). Notable examples include DreamFusion, which pioneered the use of SDS to optimize Neural Radiance Field (NeRF) representations (Mildenhall et al., 2021), and Magic3D (Lin et al., 2023a), which further advanced this approach by proposing coarse-to-fine framework to enhance its performance. Despite the advancements in lifting and SDS-based methods, generating complex 3D scenes with multiple objects or intricate interactions remains significant challenge. Recent efforts have focused on incorporating additional guidance, such as box or layout information(Po & Wetzstein, 2024; Epstein et al., 2024; Zhou et al., 2024). Among them, Po & Wetzstein (2024) introduce locally conditioned diffusion for compositional scene diffusion based on input bounding boxes with one shared NeRF representation while Epstein et al. (2024) instantiate and render multiple NeRFs for given scene using each NeRF to represent separate 3D entity with set of layouts. Further advancing this field, GALA3D (Zhou et al., 2024) utilizes large language models (LLMs) to generate coarse layouts to guide 3D generation for compositional scenes. However, existing layout-guided compositional methods often fall short in achieving fine-grained control over the generated 3D scenes. The current form of box or layout guidance is relatively coarse Contributed equally. Corresponding authors: yangling0818@163.com, wentao.zhang@pku.edu.cn"
        },
        {
            "title": "Preprint",
            "content": "Figure 1: SEMANTICSDS achieves superior compositional text-to-3d generation results over stateof-the-art baselines, particularly in generating multiple objects with diverse attibutes. and lacks the expressiveness required to effectively guide the SDS process in optimizing the intricate interactions or intersecting parts between multiple objects, particularly when generating objects with multiple attributes. This limitation stems from the fact that pre-trained 2D diffusion models, which are used in SDS, struggle to estimate accurate scores for complex scenarios with consistent views when explicit spatial guidance is absent (Li et al., 2023; Shi et al., 2024). As result, the generated 3D scenes may lack the level of detail and realism desired, highlighting the need for more precise guidance mechanisms that can provide finer-grained control over the generation process. To address these limitations, we propose Semantic Score Distillation Sampling (SEMANTICSDS), which boosts the expressiveness and precision of compositional text-to-3D generation. For more explicit 3D expression, we equip SEMANTICSDS with 3D Gaussian Splatting (3DGS) (Kerbl et al., 2023) as the 3D representation. Our approach consists of three key steps: (1) Given text prompt, we propose program-aided approach to improve the accuracy of LLM-based layout planning for 3D scenes. (2) We introduce novel semantic embeddings that remain consistent across various rendering views and explicitly distinguish different objects and parts. (3) We then render these semantic embeddings into semantic map, which serves as guidance for region-wise SDS process, facilitating fine-grained optimization and compositional generation. Our approach addresses the challenge"
        },
        {
            "title": "Preprint",
            "content": "of leveraging pre-trained diffusion models, which possess powerful compositional diffusion priors but are difficult to utilize (Wang et al., 2024a; Yang et al., 2024). By using explicit semantic map guidance, we innovatively unlock these compositional 2D diffusion priors for high-quality 3D content generation. Our main contributions are summarized as follows: We propose SEMANTICSDS, novel semantic-guided score distillation sampling approach that effectively enhances the expressiveness and precision of compositional text-to-3D generation, as shown in Figure 1. We introduce program-aided layout planning to improve positional and relational accuracy in generated 3D scenes, deriving precise 3D coordinates from ambiguous descriptions. We develop expressive semantic embeddings to augment 3D Gaussian representations, and propose region-wise SDS process with the rendered semantic map, distinguishing different objects and parts in the compositional generation process."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Text-to-3D Generation Different approaches have been developed to achieve text-to-3D content generation (Deitke et al., 2024; Zeng et al., 2023), such as employing multi-view diffusion models (Shi et al., 2024; Wu et al., 2024a; Kong et al., 2024; Blattmann et al., 2023), direct 3D diffusion models (Gupta et al., 2023; Shue et al., 2023; Wu et al., 2024b) and large reconstruction models (Hong et al., 2024). For instance, multi-view diffusion models are trained and optimized by finetuning video diffusion on 3D datasets, aiding in 3D reconstruction (Voleti et al., 2024; Chen et al., 2024d; Han et al., 2024b). You et al. (2024) propose training-free method that employs video diffusion as zero-shot novel view synthesizer. However, these methods require numerous 3D data for training. In contrast, Score Distillation Sampling (SDS) (Poole et al., 2023; Wang et al., 2023) is 3D data-free and generally produces higher quality assets. SDS approaches harness the creative potential of 2D diffusion and have achieved significant advancements (Wang et al., 2024b; Yang et al., 2023b; Hertz et al., 2023), resulting in realistic 3D content generation and enhanced resolution of generative models (Zhu et al., 2024). In this paper, we propose new SDS paradigm, namely SEMANTICSDS, for text-to-3D generation in complex scenarios, which first incorporates explicit semantic guidance into the SDS process. Compositional 3D Generation Modeling compositional 3D data distribution is fundamental and critical task for generative models. Current feed-forward methods (Shue et al., 2023; Shi et al., 2024) are primarily capable of generating single objects and face challenges when creating more complex scenes containing multiple objects due to limited training data. Po & Wetzstein (2024) fix the layout in multiple 3D bounding boxes and generate compositional assets with boundingbox-specific SDS. Recently, series of learnable-layout compositional methods have been proposed (Epstein et al., 2024; Vilesov et al., 2023; Han et al., 2024a; Chen et al., 2024b; Li et al., 2024; Yan et al., 2024; Gao et al., 2024) . These methods combine multiple object-ad-hoc radiance fields and then optimize the positions of the radiance fields from external feedback. For example, Epstein et al. (2024) propose learning distribution of reasonable layouts based solely on the knowledge from large pre-trained text-to-image model. Vilesov et al. (2023) introduce an optimization method based on Monte-Carlo sampling and physical constraints. Non-learnable layout methods like (Zhou et al., 2024) and Lin et al. (2023b) further utilize LLMs or MLLMs to convert text into reasonable layouts. However, the current form of layout guidance is relatively coarse and not expressive enough for finegrained control. We address this problem by incorporating semantic embeddings that ensure view consistency and distinctly differentiate objects into SDS processes, which are flexible and expressive for optimizing 3D scenes."
        },
        {
            "title": "3 PRELIMINARIES",
            "content": "Compositional 3D Gaussian Splatting 3D Gaussian Splatting explicitly represents 3D scene as collection of anisotropic 3D Gaussians, each characterized by mean µ R3 and covariance"
        },
        {
            "title": "Preprint",
            "content": "matrix Σ (Kerbl et al., 2023). The Gaussian function G(x) is defined as: (cid:18) G(x) = exp 1 2 (x µ)Σ1(x µ) (cid:19) (1) Rendering compositional scene necessitates transformation from object to composition coordinates, involving rotation R33, translation R3, and scale (Zhou et al., 2024; Vilesov et al., 2023). This transformation is applied to the mean and variance of individual Gaussians, transitioning from the objects local coordinates to global coordinates: µglobal = sRµlocal +t, Σglobal = s2RΣlocalR. For optimized rendering of compositional 3D Gaussians into 2D image planes, tile-based rasterizer enhances rendering efficiency. The rendered color at pixel is computed as follows: I(v) = (cid:88) iN i1 (cid:89) ciαi (1 αj), j=1 (2) where ci represents the color of the i-th Gaussian, denotes the set of Gaussians within the tile, and αi is the opacity. Score Distillation Sampling Yang et al. (2023a); Wang et al. (2023) have introduced method to leverage pretrained diffusion model, ϵϕ(xt; y, t), to optimize the 3D representation, where xt, y, and signify the noisy image, text embedding, and timestep, respectively. Let represent the differentiable rendering fcuntion, θ denote the parameters of the optimizable 3D representation and = g(θ) be the resulting rendered image. The gradient for optimization is performed via Score Distillation Sampling: θLSDS = Eϵ,t (cid:20) w(t) (ϵϕ (xt; y, t) ϵ) (cid:21) θ (3) where ϵ is Gaussian noise and w(t) is weighting function. In compositional 3D generation, local object optimizations and global scene optimizations alternate in compositional optimization scheme (Zhou et al., 2024). During local optimization, the parameters θ include the mean, covariance, and color of individual Gaussians. In global scene optimization, the parameters θ additionally include transformationstranslation, scale, and rotationthat convert local to global coordinates."
        },
        {
            "title": "4 METHOD",
            "content": "4.1 PROGRAM-AIDED LAYOUT PLANNING detailed characterization of multiple objects positions, dimensions, and orientations requires numerous parameters, especially when additionally describing distinct attributes of various object components. In scenarios involving multiple objects, utilizing Large Language Models (LLMs) to derive precise 3D coordinates from ambiguous descriptions within scene is often challenging. This difficulty arises because purely 3D numerical data and corresponding natural language descriptions do not frequently co-occur in the training data of LLMs (Hong et al., 2023; Xu et al., 2023). Consequently, issues such as overlapping objects or excessive distances between them may occur, particularly during interactions among objects. Therefore, we propose to leverage programs as the intermediate reasoning and planning steps (Gao et al., 2023) to effectively mitigate these challenges. Let yc represent the complex user input, which includes multiple objects with various attributes. First, We utilize Large Language Models to identify all objects {Ok}K k=1 within yc, where denotes the total number of objects. For each object, the corresponding prompt yk is recognized, and its dimensions are estimated. This includes considering the objects real-world size and its relationship with other objects to determine its relative size, facilitating the placement of all objects within the same scene. Subsequently, LLMs sequentially position each object within the scene. In designing each objects placement, LLMs articulate the spatial relationships with relevant entities using programmable language descriptions that explicitly outline all mathematical calculations. This language is then converted into program executed by runtime, such as Python interpreter, to produce the layout"
        },
        {
            "title": "Preprint",
            "content": "Figure 2: Overview of SEMANTICSDS, comprising of program-aided layout planning (top) and regional denoising with semantic map (bottom). solution. These layouts, which include scale factors, Euler angles, and translation vectors, are employed to transform 3D Gaussians from local coordinates to global coordinates during rendering. Furthermore, for each object Ok, LLMs decomposes its layout space into nk complementary regions, each with distinct attributes and different subprompts {yk,l}nk l=1. These complementary regions are designed to be non-overlapping and collectively encompass the entire layout space of their respective object. To generate meaningful and accurate complementary regions, LLMs employ structured decomposition process that segments the space of object Ok into hierarchical divisions based on depth, width, and length dimensions. This process is documented using programmable language descriptions and subsequently converted into precise bounding boxes by program. Details on the prompts used for this program-aided layout planning are provided in Appendix A.1. 4.2 SEMANTIC SCORE DISTILLATION SAMPLING Prompt-Guided Semantic 3D Gaussian Representation To generate 3D scenes involving multiple objects with diverse attributes and to precisely control the attributes of distinct spatial regions within each object, it is essential to utilize features that represent the fine-grained semantics of 3D Gaussians. We design new prompt-guided semantic 3D Gaussian representations. During initialization, the subprompt yk,l corresponding to the i-th Gaussian is encoded via the CLIP text encoder Φ (Radford et al., 2021) to obtain the high-dimensional semantic embedding, hi = Φ(yk,l) Rdh. Given the significant memory demands imposed by the large dimensions of dh, lightweight autoencoder is employed. This autoencoder effectively compresses the scenes high-dimensional semantic embeddings into more manageable, low-dimensional representations, represented as fi = E(hi) Rdf . The loss function for the autoencoder is defined as: Lae = (cid:88) iN dae(D(E(hi)), hi) 5 (4)"
        },
        {
            "title": "Preprint",
            "content": "where dae denotes the metric combining the L1 loss and the symmetric cross entropy loss from CLIP (Radford et al., 2021). The i-th Gaussian is then augmented with semantic embedding fi Rd. And semantic information is integrated into the rendered 2D image by rendering the semantic embedding at pixel using the formula: F(v) = (cid:88) fiαi i1 (cid:89) (1 αj) (5) The rendered semantic embedding F(v), derived from equation 5, is fed into the decoder to reconstruct S(v) = D(F(v)) Rdh and then generates semantic map RHW dh indicating the rendered images semantic attributes. iN j=1 Semantic Score Distillation Sampling To enable fine-grained controllable generation, the generated semantic map is integrated into the spatial composition of scores for distillation sampling. The subprompt yk,l is processed through the CLIP text encoder Φ to produce the subprompt embedding qk,l = Φ (yk,l) Rdh. The probability that pixel corresponds to subprompt yk,l is computed as: p(k, v) = exp (cos (qk,l, S(v)) /τ ) (cid:80)nk l=1 exp (cos (qk,l, S(v)) /τ ) (cid:80)K k=1 (6) where τ is temperature parameter learned by CLIP and cos(, ) denotes cosine similarity. This facilitates the derivation of the mask Mk,l(v), which indicates whether the semantic properties of pixel align with subprompt yk,l. Mk,l(v) = (cid:26)1 0 if (k, l) = arg maxk,l (k, v) otherwise (7) The semantic mask Mk,l {0, 1}HW is subsequently utilized to guide the score distillation sampling. To ensure that the Gaussians near the edges of objects are not overlooked, the mask Mk,l is subjected to max pooling operation with 5 5 kernel, resulting in ˆMk,l. Although diffusion models generally lack an inherent distinction at the object and part levels in their latent spaces or attention maps for fine-grained control (Lian et al., 2024), recent advancements in compositional 2D image generation have implemented spatially-conditioned generation (Chen et al., 2024a; Yang et al., 2024; Xie et al., 2023). This is achieved through regional denoising or attention manipulation, allowing for fine-grained control over the semantics of the generated images. Specifically, the overall denoising score is calculated as the aggregate of the individually masked denoising scores for each visible subprompt yk,l: ˆϵϕ (xt; y, t) = Ek,l (cid:104) ϵϕ (xt; yk,l, t) ˆMk,l (cid:105) (8) where denotes element-wise multiplication. Instead of conditioning the diffusion models on single text prompt, our semantic score distillation sampling employs the compositional denoising score as follows: θLSemanticSDS = Eϵ,t (cid:20) w(t) (ˆϵϕ (xt; y, t) ϵ) (cid:21) θ (9) This methodology effectively leverages the expressive compositional generation capabilities of pretrained 2D diffusion models for text-to-3D generation. Further details on SEMANTICSDS are provided in Appendix A.2. Object-Specific View Descriptor for Global Scene Optimization Unlike object-centric optimization, scenes do not exhibit distinct perspectives as individual objects do. Effective scene generation necessitates precise, part-level control over the optimization of distinct object views. Terms such as side view or back view are rarely applicable to multi-object scenes, and pretrained diffusion models often struggle to generate images accurately from such prompts (Li et al., 2023). Moreover, within single rendered image, different objects may be visible from varying perspectives. Using unified view descriptor for an entire scene with multiple objects exacerbates the Janus Problem (Poole et al., 2023). Although the compositional optimization scheme alternates between local object optimizations and global scene optimizations (Zhou et al., 2024), allowing for the correct optimization of different views of objects in local coordinates, it is confounded by optimizations"
        },
        {
            "title": "Preprint",
            "content": "Figure 3: Illustration of our proposed object-specific view descriptor for global scene optimization. under global coordinates. This limits the frequency of global scene optimizations and results in lack of scene coherence, harmony, and lighting consistency. To address this issue, in our SEMANTICSDS, we append an object-specific view descriptor yview to the corresponding subprompts {yk,l}nK l=1 to optimize individual objects within the rendered image (in Figure 3). The same view descriptor yview is consistently applied across different parts of each multi-attribute object. Specifically, we determine the cameras elevation and azimuth angles relative to each object by computing the angle between the vector ˆn, which extends from the object to the camera, and specific reference axis vectors, such as the positive z-axis. This calculation facilitates the selection of the most appropriate object-specific view descriptor. For instance, if the angle between ˆn and the positive z-axis remains below predefined threshold, indicative of high azimuth angle, the descriptor yview is assigned as an overhead view descriptor for that object. k"
        },
        {
            "title": "5 EXPERIMENTS",
            "content": "Implementation Details. The guidance model is implemented using the publicly accessible diffusion model, StableDiffusion (Rombach et al., 2022), specifically utilizing the checkpoint runwayml/stable-diffusion-v1-5. Positions of the Gaussians are initialized using Shap-E (Jun & Nichol, 2023), with each object initially comprising 12288 Gaussians. For densification, Gaussians are cloned or split based on the view-space position gradient using threshold Tpos = 2, with semantic embeddings copied. Compactness-based densification is also applied every 2000 iterations, involving each Gaussian and one of its nearest neighbors, as described in GSGEN (Chen et al., 2024c). Pruning involves removing Gaussians with opacity lower than αmin = 0.3, as well as those with excessively large radii in either world-space or view-space, every 200 iterations. Training alternates between local and global optimization. During global optimization, the rendered objects vary by switching between the entire scene and pairs of objects. Camera sampling maintains the same focal length, elevation, and azimuth range as specified in (Chen et al., 2024c). The threshold for selecting object-specific view descriptors includes: an overhead view descriptor for elevation angles exceeding 60, front view descriptor for azimuth angles within 45 of the positive x-axis, and back view descriptor for 45 angles on the negative x-axis. Table 1: Quantitative Comparison Metrics GraphDreamer GSGEN LucidDreamer GALA3D SemanticSDS (Ours) CLIP Score Prompt Alignment Spatial Arrangement Geometric Fidelity Scene Quality 0.289 56.9 53.8 53.8 54.9 0.314 63.3 62.8 71.1 71.2 0.311 64.4 65 71.8 65.9 0.305 85.0 80.0 80.3 82.3 0.321 91.1 85.7 83.0 86.9 Baseline methods. To evaluate the performance of SEMANTICSDS on the complex Text-to-3D task involving multiple objects with varied attributes, we compare it with state-of-the-art (SOTA)"
        },
        {
            "title": "Preprint",
            "content": "Figure 4: Qualitative comparisons of text-to-3D generation. Comparison results demonstrate that SEMANTICSDS synthesizes more precise and realistic multi-object scenes with better visual details, geometric expressiveness, and semantic consistency. methods. These include the compositional 3D generation method GALA3D (Zhou et al., 2024) and GraphDreamer (Gao et al., 2024), noted for their ability to generate intricate scenes with multiple objects. Additionally, we consider GSGEN (Chen et al., 2024c) and LucidDreamer (Liang et al., 2024), both are capable of producing high-quality, complex objects with diverse attributes. Metrics. CLIP Score (Radford et al., 2021) is employed as the evaluation metric to assess the quality and consistency of the generated 3D scenes with textual descriptions. However, CLIP tends to focus on the primary objects within the rendered image, and when used to evaluate complex text-to-3D tasks involving multiple objects with varied attributes, it may not adequately assess the geometry of all objects or the rationality of their spatial arrangements. This limitation results in misalignment with human judgment regarding evaluation criteria. Therefore, following Wu et al. (2024c), GPT-4V is utilized as human-aligned evaluator to compare 3D assets based on predefined"
        },
        {
            "title": "Preprint",
            "content": "criteria. These criteria include: (1) Prompt Alignment: ensuring that all objects specified in the user prompts are present and correctly quantified; (2) Spatial Arrangement: evaluating the logical and thematic spatial arrangement of objects; (3) Geometric Fidelity: assessing the geometric fidelity of each object for realistic representation; and (4) Scene Quality: determining the overall scene quality in terms of coherence and visual harmony. More details on metrics are provided in the Appendix A.3."
        },
        {
            "title": "5.1 MAIN RESULTS",
            "content": "Quantitative Analysis To evaluate the performance of SEMANTICSDS in Text-to-3D tasks involving multiple objects with varied attributes, quantitative metrics were employed. As shown in Table 1, the CLIP Score indicates that SEMANTICSDS exhibits strong alignment with the primary semantics of user prompts. Specifically, SEMANTICSDS excels in Prompt Alignment, ensuring that all objects specified in user prompts are present and correctly quantified. Additionally, it demonstrates superior performance in Spatial Arrangement, effectively designing the layout of interactive objects to support the scenes intended theme. Furthermore, by explicitly guiding SDS with rendered semantic maps, SEMANTICSDS achieves outstanding generation of individual objects with diverse attributes across different spatial components, resulting in high scores in object-level Geometric Fidelity. Additionally, the use of compositional 3D Gaussian Splatting for scene representation helps SEMANTICSDS to effectively disentangle objects within the scene. This, combined with explicit semantic guidance to the SDS, contributes to achieving the highest score in Scene Quality. Qualitative Analysis To intuitively demonstrate the superiority of the proposed method in generating complex 3D scenes with multiple objects possessing diverse attributes, qualitative comparison with baseline models is conducted. As illustrated in Figure 4, GALA3D, with compositional optimization scheme, successfully generates individual objects that align with user prompts. However, it fails to produce plausible results when objects have multiple attributes. Although GSGEN and LucidDreamer generate high-quality individual objects, the presence of multiple objects often leads to entanglement, compromising consistency with user prompts. Additionally, these models are unable to generate reasonable objects when individual objects possess numerous attributes. In contrast, SEMANTICSDS employs guided diffusion models with explicit semantics, effectively generating scenes that include multiple objects with diverse attributes. Moreover, by utilizing programaided layout planning, SEMANTICSDS produces more coherent layouts than GALA3D in scenarios involving complex spatial relationships among multiple objects. For example, in Figure 1, both table lamps are correctly placed on the table without appearing to float when using SEMANTICSDS. User Study We conducted user study to compare our method with baseline methods across 30 scenes involving more than 100 objects. Each participant was shown user prompt alongside 3D scenes generated by all methods simultaneously and asked to select the most realistic assets based on geometry, prompt alignment, and accurate placement. Figure 5 illustrates that SEMANTICSDS significantly outperformed previous methods in terms of human preference. 5.2 MODEL ANALYSIS Figure 5: User study results. SEMANTICSDS is preferred 60% of the time by users than baseline methods. Effectiveness of Program-aided Layout Planning We assess the necessity of program-aided layout planning through an ablation study. The qualitative comparison of generated layouts is illustrated in Figure 6. Without program-aided planning, layout placement often lacks rationale and results in poor spatial arrangements. In contrast, the program-aided strategy positions the layouts logically and divides the layout into meaningful and precise complementary regions for objects with multiple attributes, resulting in an effective spatial arrangement. Impact of Semantic Score Distillation Sampling Ablation experiments are performed on Semantic Score Distillation Sampling to evaluate the effects of explicitly guiding SDS with rendered semantic maps. In Figure 7, without SEMANTICSDS, while objects with single attributes are generated effectively, those with varied attributes often experience blending issues. For instance, the"
        },
        {
            "title": "Preprint",
            "content": "Figure 6: Qualitative comparisons between without and with our program-aided layout planning. house shows snow bricks mixed with LEGO bricks, failing to meet the user prompts spatial requirements. The snow bricks are inaccurately represented as white LEGO bricks, which do not align with the intended attributes. Additionally, one attribute may dominate, causing others to disappear, such as in the car with three attributes in Figure 7. Conversely, SemanticSDS enables precise control over the attributes in distinct spatial regions of each object, producing objects with diverse attributes and smooth transitions between regions with different attributes. Figure 7: Qualitative analysis. Our SEMANTICSDS provides more precise and fine-grained control and our proposed object-specific view descriptor helps with better multi-view understanding. Object-Specific View Descriptor To assess the effectiveness of the object-specific view descriptor, we replace it with the scene-centric view descriptor utilized by GSGEN during global optimization. This change increases the occurrence of the Janus Problem, as illustrated by the overhead view of the corgi in the middle of Figure 7. These findings highlight the crucial role of selecting an appropriate view descriptor to enhance the plausibility of generated 3D scenes."
        },
        {
            "title": "6 CONCLUSION",
            "content": "In this paper, we introduce SEMANTICSDS, novel SDS method that significantly enhances the expressiveness and precision of compositional text-to-3D generation. By leveraging program-aided layout planning, semantic embeddings, and explicit semantic guidance, we unlock the compositional priors of pre-trained diffusion models and achieve realistic high-quality generation in complex scenarios. Our extensive experiments demonstrate that SEMANTICSDS achieves state-of-the-art results for generating complex 3D content. As we look to the future, we envision SEMANTICSDS as foundation for even more applications, such as automatic editing and closed-loop refinement, paving the way for unprecedented levels of creativity and innovation in 3D content generation."
        },
        {
            "title": "REFERENCES",
            "content": "Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. Minghao Chen, Iro Laina, and Andrea Vedaldi. Training-free layout control with cross-attention In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer guidance. Vision, pp. 53435353, 2024a. Yongwei Chen, Tengfei Wang, Tong Wu, Xingang Pan, Kui Jia, and Ziwei Liu. Comboverse: arXiv preprint Compositional 3d assets creation using spatially-aware diffusion guidance. arXiv:2403.12409, 2024b. Zilong Chen, Feng Wang, Yikai Wang, and Huaping Liu. Text-to-3d using gaussian splatting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 21401 21412, 2024c. Zilong Chen, Yikai Wang, Feng Wang, Zhengyi Wang, and Huaping Liu. V3d: Video diffusion models are effective 3d generators. arXiv preprint arXiv:2403.06738, 2024d. Matt Deitke, Ruoshi Liu, Matthew Wallingford, Huong Ngo, Oscar Michel, Aditya Kusupati, Alan Fan, Christian Laforte, Vikram Voleti, Samir Yitzhak Gadre, et al. Objaverse-xl: universe of 10m+ 3d objects. Advances in Neural Information Processing Systems, 36, 2024. Dave Epstein, Ben Poole, Ben Mildenhall, Alexei Efros, and Aleksander Holynski. Disentangled In International Conference on Machine Learning, 3d scene generation with layout learning. 2024. Gege Gao, Weiyang Liu, Anpei Chen, Andreas Geiger, and Bernhard Scholkopf. Graphdreamer: Compositional 3d scene synthesis from scene graphs. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2129521304, 2024. Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. Pal: Program-aided language models. In International Conference on Machine Learning, pp. 1076410799. PMLR, 2023. Anchit Gupta, Wenhan Xiong, Yixin Nie, Ian Jones, and Barlas Oguz. 3dgen: Triplane latent diffusion for textured mesh generation. arXiv preprint arXiv:2303.05371, 2023. Haonan Han, Rui Yang, Huan Liao, Jiankai Xing, Zunnan Xu, Xiaoming Yu, Junwei Zha, Xiu Li, and Wanhua Li. Reparo: Compositional 3d assets generation with differentiable 3d layout alignment. arXiv preprint arXiv:2405.18525, 2024a. Junlin Han, Filippos Kokkinos, and Philip Torr. Vfusion3d: Learning scalable 3d generative models from video diffusion models. European Conference on Computer Vision (ECCV), 2024b. Amir Hertz, Kfir Aberman, and Daniel Cohen-Or. Delta denoising score. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 23282337, 2023. Yicong Hong, Kai Zhang, Jiuxiang Gu, Sai Bi, Yang Zhou, Difan Liu, Feng Liu, Kalyan Sunkavalli, Trung Bui, and Hao Tan. Lrm: Large reconstruction model for single image to 3d. ICLR, 2024. Yining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng, Yilun Du, Zhenfang Chen, and Chuang Gan. 3d-llm: Injecting the 3d world into large language models. Advances in Neural Information Processing Systems, 36:2048220494, 2023. Heewoo Jun and Alex Nichol. Shap-e: Generating conditional 3d implicit functions. arXiv preprint arXiv:2305.02463, 2023. Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Trans. Graph., 42(4):1391, 2023."
        },
        {
            "title": "Preprint",
            "content": "Xin Kong, Shikun Liu, Xiaoyang Lyu, Marwan Taher, Xiaojuan Qi, and Andrew Davison. Eschernet: generative model for scalable view synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 95039513, 2024. Runjia Li, Junlin Han, Luke Melas-Kyriazi, Chunyi Sun, Zhaochong An, Zhongrui Gui, Shuyang Sun, Philip Torr, and Tomas Jakab. Dreambeast: Distilling 3d fantastical animals with part-aware knowledge transfer, 2024. URL https://arxiv.org/abs/2409.08271. Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jianwei Yang, Jianfeng Gao, Chunyuan Li, and Yong Jae Lee. Gligen: Open-set grounded text-to-image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2251122521, 2023. Long Lian, Boyi Li, Adam Yala, and Trevor Darrell. LLM-grounded diffusion: Enhancing prompt understanding of text-to-image diffusion models with large language models. Transactions on Machine Learning Research, 2024. ISSN 2835-8856. URL https://openreview.net/ forum?id=hFALpTb4fR. Featured Certification. Yixun Liang, Xin Yang, Jiantao Lin, Haodong Li, Xiaogang Xu, and Yingcong Chen. Luciddreamer: In Proceedings of the Towards high-fidelity text-to-3d generation via interval score matching. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 65176526, 2024. Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution text-to-3d conIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern tent creation. Recognition, pp. 300309, 2023a. Yiqi Lin, Hao Wu, Ruichen Wang, Haonan Lu, Xiaodong Lin, Hui Xiong, and Lin Wang. Towards language-guided interactive 3d generation: Llms as layout interpreter with generative feedback. arXiv preprint arXiv:2305.15808, 2023b. Ben Mildenhall, Pratul Srinivasan, Matthew Tancik, Jonathan Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 65(1):99106, 2021. Ryan Po and Gordon Wetzstein. Compositional 3d scene generation using locally conditioned diffusion. In 2024 International Conference on 3D Vision (3DV), pp. 651663. IEEE, 2024. Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. In International Conference on Learning Representations, 2023. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pp. 87488763. PMLR, 2021. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1068410695, 2022. Yichun Shi, Peng Wang, Jianglong Ye, Long Mai, Kejie Li, and Xiao Yang. Mvdream: Multi-view diffusion for 3d generation. In The Twelfth International Conference on Learning Representations, 2024. Ryan Shue, Eric Ryan Chan, Ryan Po, Zachary Ankner, Jiajun Wu, and Gordon Wetzstein. 3d neural field generation using triplane diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2087520886, 2023. Alexander Vilesov, Pradyumna Chari, and Achuta Kadambi. Cg3d: Compositional generation for text-to-3d via gaussian splatting. arXiv preprint arXiv:2311.17907, 2023. Vikram Voleti, Chun-Han Yao, Mark Boss, Adam Letts, David Pankratz, Dmitry Tochilkin, Christian Laforte, Robin Rombach, and Varun Jampani. Sv3d: Novel multi-view synthesis and 3d generation from single image using latent video diffusion. arXiv preprint arXiv:2403.12008, 2024."
        },
        {
            "title": "Preprint",
            "content": "Haochen Wang, Xiaodan Du, Jiahao Li, Raymond Yeh, and Greg Shakhnarovich. Score jacobian chaining: Lifting pretrained 2d diffusion models for 3d generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1261912629, 2023. Ruichen Wang, Zekang Chen, Chen Chen, Jian Ma, Haonan Lu, and Xiaodong Lin. Compositional In Proceedings of the text-to-image synthesis with attention map control of diffusion models. AAAI Conference on Artificial Intelligence, volume 38, pp. 55445552, 2024a. Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu. Prolificdreamer: High-fidelity and diverse text-to-3d generation with variational score distillation. Advances in Neural Information Processing Systems, 36, 2024b. Kailu Wu, Fangfu Liu, Zhihan Cai, Runjie Yan, Hanyang Wang, Yating Hu, Yueqi Duan, and Kaisheng Ma. Unique3d: High-quality and efficient 3d mesh generation from single image. arXiv preprint arXiv:2405.20343, 2024a. Shuang Wu, Youtian Lin, Feihu Zhang, Yifei Zeng, Jingxi Xu, Philip Torr, Xun Cao, and Yao Yao. Direct3d: Scalable image-to-3d generation via 3d latent diffusion transformer. arXiv preprint arXiv:2405.14832, 2024b. Tong Wu, Guandao Yang, Zhibing Li, Kai Zhang, Ziwei Liu, Leonidas Guibas, Dahua Lin, and Gordon Wetzstein. Gpt-4v (ision) is human-aligned evaluator for text-to-3d generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 22227 22238, 2024c. Jinheng Xie, Yuexiang Li, Yawen Huang, Haozhe Liu, Wentian Zhang, Yefeng Zheng, and Mike Zheng Shou. Boxdiff: Text-to-image synthesis with training-free box-constrained diffusion. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 74527461, 2023. Runsen Xu, Xiaolong Wang, Tai Wang, Yilun Chen, Jiangmiao Pang, and Dahua Lin. Pointllm: Empowering large language models to understand point clouds. arXiv preprint arXiv:2308.16911, 2023. Han Yan, Yang Li, Zhennan Wu, Shenzhou Chen, Weixuan Sun, Taizhang Shang, Weizhe Liu, Tian Chen, Xiaqiang Dai, Chao Ma, et al. Frankenstein: Generating semantic-compositional 3d scenes in one tri-plane. arXiv preprint arXiv:2403.16210, 2024. Ling Yang, Zhilong Zhang, Yang Song, Shenda Hong, Runsheng Xu, Yue Zhao, Wentao Zhang, Bin Cui, and Ming-Hsuan Yang. Diffusion models: comprehensive survey of methods and applications. ACM Computing Surveys, 56(4):139, 2023a. Ling Yang, Zhaochen Yu, Chenlin Meng, Minkai Xu, Stefano Ermon, and CUI Bin. Mastering textto-image diffusion: Recaptioning, planning, and generating with multimodal llms. In Forty-first International Conference on Machine Learning, 2024. Xiaofeng Yang, Yiwen Chen, Cheng Chen, Chi Zhang, Yi Xu, Xulei Yang, Fayao Liu, and Guosheng Lin. Learn to optimize denoising scores for 3d generation: unified and improved diffusion prior on nerf and 3d gaussian splatting. arXiv preprint arXiv:2312.04820, 2023b. Meng You, Zhiyu Zhu, Hui Liu, and Junhui Hou. Nvs-solver: Video diffusion model as zero-shot novel view synthesizer. arXiv preprint arXiv:2405.15364, 2024. Bohan Zeng, Shanglin Li, Yutang Feng, Hong Li, Sicheng Gao, Jiaming Liu, Huaxia Li, Xu Tang, Jianzhuang Liu, and Baochang Zhang. Ipdreamer: Appearance-controllable 3d object generation with image prompts. arXiv preprint arXiv:2310.05375, 2023. Xiaoyu Zhou, Xingjian Ran, Yajiao Xiong, Jinlin He, Zhiwei Lin, Yongtao Wang, Deqing Sun, and Ming-Hsuan Yang. Gala3d: Towards text-to-3d complex scene generation via layout-guided generative gaussian splatting. In Forty-first International Conference on Machine Learning, 2024. Junzhe Zhu, Peiye Zhuang, and Sanmi Koyejo. HIFA: High-fidelity text-to-3d generation with advanced diffusion guidance. In International Conference on Learning Representations, 2024."
        },
        {
            "title": "A MORE IMPLEMENTATION DETAILS",
            "content": "A.1 PROMPTS FOR PROGRAM-AIDED LAYOUT PLANNING Figure 8: The prompt for scene-level decomposition in program-aided layout planning. Large Language Models (LLMs) have the potential for spatial awareness; however, precise 3D layout generation from vague language descriptions is challenging. This difficulty arises because 3D digital data and corresponding natural language descriptions often do not appear simultaneously (Hong et al., 2023; Xu et al., 2023). Moreover, minor numerical changes, which might not be reflected in imprecise language, can lead to unrealistic spatial arrangements of 3D scenes. Additionally, the spatial arrangement of multi-object scenes requires numerous parameters, making program-aided approach necessary to bridge the gap between natural language descriptions and 3D digital data. Specifically, we decompose the process of generating multiple objects with diverse attributes into two steps: scene-level decomposition and object-level decomposition. In scene decomposition, we guide LLMs to translate user prompts into Python programs, using explicit mathematical operations"
        },
        {
            "title": "Preprint",
            "content": "Figure 9: The prompt for decomposing each object into complementary regions. to represent relationships between objects. For object decomposition, since complementary regions are designed to be non-overlapping and collectively encompass the entire layout space of their respective objects, we devised scheme employing structured JavaScript Object Notation (JSON) to represent hierarchical divisions based on depth, width, and length dimensions. Figures 8 and 9 illustrate the detailed prompts for scene and object decomposition, respectively. A.2 SEMANTICSDS Camera Sampling Training alternates between local and global optimization. During local optimization, objects are not transformed into global coordinates. In global optimization, the rendering of objects varies by switching between the entire scene and pairs of objects to better optimize those that interact or occlude each other. When rendering only pair of objects, the cameras look-at point is sampled at the midpoint between the two objects rather than the center of the entire scene. Additionally, we apply dynamic camera distance from the object pair to ensure the objects are appropriately sized in the rendered images. Specifically, the camera distance is determined by the scale of the objects and the distance between their centers. Pooling of Semantic Masks Given that the rendered RGB images and the semantic map have sizes of 512 512, whereas the latents for denoising are of size 64 64, we convert the semantic map into masks to compose the denoising scores predicted by diffusion models. Subsequently, for each mask Mk,l {0, 1}512512, we apply average pooling with stride of 8 using an 8 8 kernel to downsample the data. To ensure that Gaussians near the edges of objects and isolated Gaussians are not overlooked, the mask Mk,l undergoes max pooling operation with 5 5 kernel, resulting in ˆMk,l. Compositional Optimization Scheme The compositional optimization scheme encompasses both global scene and local object optimizations. Only global scene optimizations apply affine transformations to convert objects from local to global coordinates. During local optimization, θ in equation 9 includes the mean, covariance, and color of individual Gaussians. In global scene optimization, θ additionally includes the parameters of affine transformationstranslation, scale, and rotationthat convert local to global coordinates. A.3 DETAILS OF METRICS CLIP Score The CLIP score utilizes CLIP embeddings (Radford et al., 2021) to evaluate text-to3D alignment. Following previous methods (Zhou et al., 2024; Gao et al., 2024), we calculate the cosine similarity between the user prompt and scene images rendered from different perspectives. For each scene, we take the maximum CLIP score from all rendered images as the representative score. We then compare the average of these maximum scores across different scenes for each method."
        },
        {
            "title": "Preprint",
            "content": "Figure 10: The prompt for guiding GPT-4 as human-aligned evaluator GPT-4V as Human-Aligned Evaluator Due to the limitations of the CLIP score in capturing spatial arrangement and geometric fidelity, we follow Wu et al. (2024c) and employ GPT-4V to evaluate complex 3D scenes involving multiple objects with varied attributes. Specifically, we provide GPT-4V with rendered images of the same 3D scene generated by different methods and require it to score each scene on four aspects: Prompt Alignment, Spatial Arrangement, Geometric Fidelity, and Scene Quality, each on scale from 1 to 100. For each scene and method pair, we perform three independent evaluations. The final score for each method is obtained by averaging the scores across different scenes and comparisons with other methods. Figure 10 presents the prompt used to guide the GPT-4V evaluator. In the prompt, method and method are used to anonymize the methods, preventing name bias in GPT-4Vs judgment."
        },
        {
            "title": "B MORE SYNTHESIS RESULTS",
            "content": ""
        },
        {
            "title": "Preprint",
            "content": "Figure 11: More synthesis results of multiple objects with our SEMANTICSDS."
        },
        {
            "title": "Preprint",
            "content": "Figure 12: More synthesis results of single object with diverse attributes with our SEMANTICSDS."
        }
    ],
    "affiliations": [
        "Peking University",
        "University of Oxford"
    ]
}