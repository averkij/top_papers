{
    "paper_title": "DeFM: Learning Foundation Representations from Depth for Robotics",
    "authors": [
        "Manthan Patel",
        "Jonas Frey",
        "Mayank Mittal",
        "Fan Yang",
        "Alexander Hansson",
        "Amir Bar",
        "Cesar Cadena",
        "Marco Hutter"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Depth sensors are widely deployed across robotic platforms, and advances in fast, high-fidelity depth simulation have enabled robotic policies trained on depth observations to achieve robust sim-to-real transfer for a wide range of tasks. Despite this, representation learning for depth modality remains underexplored compared to RGB, where large-scale foundation models now define the state of the art. To address this gap, we present DeFM, a self-supervised foundation model trained entirely on depth images for robotic applications. Using a DINO-style self-distillation objective on a curated dataset of 60M depth images, DeFM learns geometric and semantic representations that generalize to diverse environments, tasks, and sensors. To retain metric awareness across multiple scales, we introduce a novel input normalization strategy. We further distill DeFM into compact models suitable for resource-constrained robotic systems. When evaluated on depth-based classification, segmentation, navigation, locomotion, and manipulation benchmarks, DeFM achieves state-of-the-art performance and demonstrates strong generalization from simulation to real-world environments. We release all our pretrained models, which can be adopted off-the-shelf for depth-based robotic learning without task-specific fine-tuning. Webpage: https://de-fm.github.io/"
        },
        {
            "title": "Start",
            "content": "DeFM: Learning Foundation Representations from Depth for"
        },
        {
            "title": "Robotics",
            "content": "Manthan Patel1, Jonas Frey1,2,3, Mayank Mittal1,4, Fan Yang1, Alexander Hansson1, Amir Bar3, Cesar Cadena1, and Marco Hutter1 6 2 0 2 6 2 ] . [ 1 3 2 9 8 1 . 1 0 6 2 : r Fig. 1: We present DeFM, foundation model for depth images. Pretrained using DINOv2 style self-distillation method (III) on curated depth dataset of 60 images (II), DeFM features achieve state-of-the-art results across several classification and semantic segmentation benchmarks (linear probing) (I). Features obtained from DeFM reveal semantic awareness upon performing PCA despite depth lacking texture and color (V). We distill our largest DeFM model into several efficient CNN networks (IV) to be used for various downstream robotic Reinforcement Learning tasks, including navigation, manipulation, and locomotion (VI). In (I), the scales are linearly scaled to the top performer, and zero performance implies it was not tested. AbstractDepth sensors are widely deployed across robotic platforms, and advances in fast, high-fidelity depth simulation have enabled robotic policies trained on depth observations to achieve robust sim-to-real transfer for wide range of tasks. Despite this, representation learning for depth modality remains underexplored compared to RGB, where large-scale foundation models now define the state of the art. To address this gap, we present DeFM, self-supervised foundation model trained entirely on depth images for robotic applications. Using DINOstyle self-distillation objective on curated dataset of 60M depth images, DeFM learns geometric and semantic representations that generalize to diverse environments, tasks, and sensors. To retain metric awareness across multiple scales, we introduce novel input normalization strategy. We further distill DeFM into compact models suitable for resource-constrained robotic systems. When evaluated on depth-based classification, segmentation, navigation, locomotion, and manipulation benchmarks, DeFM achieves state-of-the-art performance and demonstrates strong generalization from simulation to real-world environments. We release all our pretrained models, which can be adopted off-the-shelf for depth-based robotic learning without task-specific fine-tuning. Webpage: https://de-fm.github.io/ 1 Robotic Systems Lab (RSL), ETH Zurich, Switzerland 2 Stanford University, Stanford, USA 3 UC Berkeley, Berkeley, USA 4 NVIDIA, Zurich, Switzerland Index TermsFoundation Models, Representation Learning, Robotics, Reinforcement Learning, Navigation, Manipulation I. INTRODUCTION Foundation models pretrained on large-scale RGB datasets have transformed visual representation learning in recent years. Methods such as DINO [1][3], and CLIP [4] have become the de facto standard for vision tasks, demonstrating remarkable zero-shot transfer to classification, detection, segmentation, and retrieval. Their success has extended into robotics, where pretrained RGB encoders serve as the backbone for various tasks such as manipulation, navigation, and scene understanding [5][7]. Specialized Vision Foundation Models (VFM) tailored to robotics [8][11], further highlights the central role of large-scale self-supervised representation learning in enabling embodied agents. However, RGB-only observations are challenging to use for robotics, particularly in the context of sim-to-real transfer, which is central paradigm in robot learning. This difficulty arises because simulated RGB rarely captures realworld visual complexity. In contrast, depth is invariant to lighting, texture, and color variation, establishing it as highly effective for generalization and sim-to-real transfer [12]. Depth sensors are now ubiquitous across robotic platforms, including manipulators, mobile robots, drones, and AR/VR devices, and have been leveraged for locomotion [13][15], navigation [16][18], and manipulation [19][21]. This widespread adoption highlights depth as an important modality for scalable representation learning in robotics, especially when robustness and generalization are required. toDespite the benefits of depth modality, surprisingly, day, no pretrained general-purpose depth encoders exist. Current approaches either re-purpose RGB-pretrained encoders for depth by applying colormaps or channel stacking [22], which introduces distribution mismatch and degrades geometric fidelity, or they train task-specific depth encoders from scratch [15], [19], leading to poor generalization across tasks, environments, and sensors. Self-supervised learning on depth remains underexplored, despite the fact that depth inherently captures critical cues for robotics, including metric geometry (e.g., object size and distance), free space (e.g., occupancy), and physical affordances, which are the action possibilities an object offers robot, such as grasping. Given the availability and advantages of depth data, we raise the natural question:"
        },
        {
            "title": "Can we build a universal pretrained depth encoder for",
            "content": "robotics ? The simple answer is yes. In this work, we present DeFM, dedicated self-supervised foundation model trained exclusively on depth images that demonstrates highly effective, transferable features for embodied agents. As first qualitative result, we show that the feature space learned by DeFM is fundamentally sound: embeddings from the depth encoder reveal emergent semantic clustering when visualized via PCA  (Fig. 5)  , mirroring the structure observed in effective RGB representations. This confirms that even without texture or color, semantic information can be extracted from depth, highlighting that the model captures information beyond pure geometry. DeFM leverages DINO-style self-distillation objective applied to our curated dataset. We discuss key design choices for depth pretraining, including the importance of preserving metric depth, input normalization, and dataset mixtures for crossdomain generalization. To enable practical use in resourceconstrained robotics settings, we distill our largest DeFM (ViT-L) into lightweight models ranging from 3 to 30 parameters, spanning both convolutional networks (ResNet, EfficientNet, RegNet) and compact transformers (ViT-S). Most critically, we demonstrate that DeFM can be applied frozen across broad spectrum of robotics tasks. Evaluated on depth-based classification, segmentation, locomotion, navigation, and manipulation, DeFM consistently outperforms RGB-pretrained encoders repurposed for depth  (Fig. 1)  . This includes task-specific encoder baselines trained from scratch as well as state-of-the-art RGB foundation models adapted to depth. In summary, we make the following key contributions: We present DeFM, the first large-scale self-supervised foundation model specifically for depth images with focus on robotic tasks. We curate dataset of 60 depth images and release distilled models ranging from 3 to 30 parameters, covering both CNNs and ViTs, to enable wide adoption in robotics research. We introduce novel input normalization strategy to retain metric awareness across multiple scales. DeFM achieves state-of-the-art performance across diverse perception tasks, including classification, segmentation, and various RL-based robotics tasks, including locomotion, navigation, and manipulation, with robust sim-to-real transfer. II. RELATED WORK A. Vision Foundation Models Early visual representation learning relied on supervised pretraining on large-scale classification datasets. Convolutional networks [23][25] achieved strong performance when trained on datasets such as Imagenet-21k [26]. The introduction of Vision Transformer (ViT) [27] demonstrated that scaling model capacity and data, e.g., using proprietary datasets such as JFT-300M [28], further improved performance across several benchmarks. However, features learned primarily for image classification often transfer sub-optimally to dense prediction tasks like segmentation or geometric prediction without extensive task-specific fine-tuning [29]. These limitations, along with the limited availability of labeled data, motivated shift toward Self-Supervised Learning (SSL), which leverages the structure of raw data to define supervisory signals without human annotation [29][31]. VFM refer to large, pretrained visual encoders designed to learn general-purpose representations that transfer effectively across wide range of downstream tasks and domains without any finetuning. SSL has become dominant pretraining paradigm for these models due to its scalability and domain flexibility. Below, we summarize the common SSL pretraining objectives. 1) Contrastive Learning: Contrastive methods such as SimCLR [32], learn visual representations by maximizing the similarity between different augmented views of the same image. Extending this paradigm, multi-view formulations such as Contrastive Multiview Coding (CMC) [33] leverage multiple heterogeneous views of an image to strengthen invariance and improve cross-view feature alignment. MoCo [34] introduces momentum-updated encoder and large, dynamically maintained memory bank, enabling contrastive training with massive negative sets and strong performance at scale. 2) Self-Distillation: Self-distillation methods [1], [35], [36] use two network branches processing different augmented views of the same image. The student is optimized to predict the representation of teacher. Collapse is prevented through architectural asymmetrytypically by updating the teacher via momentum-based running average of the student, allowing stable learning without negative pairs. DINOv2 [2] extends this framework to large-scale curated datasets with improved training recipes and regularization, yielding highly transferable dense features for diverse downstream tasks. 3) Masked Image Modeling (MIM): Methods such as MAE [37] reconstruct randomly masked regions of an image conditioned on the visible patches, encouraging the encoder to capture global structure while remaining computationally efficient. iBOT [38] extends this paradigm by combining MIM with DINO-style self-distillation, jointly predicting patch-level tokens and global representations to produce stronger dense features. Building on this idea, DINOv2 [2] updates the training recipe by separating the MIM and DINO projection heads, assigning training examples to prototypes using SinkhornKnopp algorithm, and scaling both the number of prototypes and the size of the curated training dataset. More recently, in DINOv3 [3], the authors further scale up the training dataset to an order of magnitude higher (1.7B images) and introduce gram anchoring to prevent degradation of dense feature maps during long training schedule. I-JEPA [39], [40] adopts non-generative masked prediction objective that predicts high-level latent representations rather than pixels, leading to semantically rich and stable foundational features. We draw inspiration from these approaches to build foundational model on depth images. 4) Vision-Language Modeling: Vision-Language Models (VLMs), such as CLIP [4] and its variants [41][43] introduce powerful form of self-supervision by learning to align visual representations with corresponding natural language captions using contrastive loss on massive web-scraped datasets. This objective grants the model exceptional zero-shot transfer capabilities and strong semantic grounding, allowing it to solve classification and retrieval tasks using only text prompts. However, while VLMs are semantically rich, their representations are often less spatially and geometrically detailed than those produced by dense visual SSL models like DINOv2 [44]. Beyond general RGB models, recent trend is the emergence of specialized geometric foundation models that explicitly unify multiple 3D tasks. Models like DUSt3R [45] and its successor MASt3R [46] cast 3D reconstruction as direct regression problem, predicting unified geometric outputs including dense depth, relative pose, and pointmaps from unconstrained image pairs. Similarly, VGGT (Visual Geometry Grounded Transformer) [47] tackles sequence-based 3D reconstruction by leveraging transformer architectures for robust geometric understanding. Most recently, Depth Anything 3 (DA3) [48] demonstrates that powerful geometric priors can be distilled into single, simple model by using plain transformer backbone (e.g., DINO encoder) and singular depthray prediction objective to achieve state-of-the-art performance in multi-view geometry and camera pose estimation. Finally, complementary trend is Agglomerative Vision Models, such as RADIO (Reduce All Domains Into One) [49], [50], which train single student model by distilling the knowledge from multiple pre-trained, heterogeneous VFM (e.g., CLIP, DINOv2, SAM [51]). This multi-teacher distillation approach efficiently consolidates the diverse strengths, such as zero-shot text grounding and dense correspondence, into unified encoder, creating single, versatile backbone for downstream tasks. B. Visual Representations for Robot Learning The features learned by early RGB VFMs are often insufficient for robotics, which requires grounded, action-relevant features emphasizing geometry and physical affordances over general semantics [52]. This limitation has driven the development of pretrained visual representations specifically tailored for embodied intelligence. 1) Action-Centric Pretraining: key approach involves pretraining encoders on large-scale action-centric video datasets to learn representations with temporal and kinematic priors. R3M [9] performs pretraining on in-the-wild video (Ego4D [53]) using contrastive and video-language objectives. R3Ms frozen encoder improves manipulation success rates by 10-20% over scratch or CLIP baseline models on suite of tasks. Similarly, VIP [54] uses value-implicit objective, leveraging the relationship between visual observations and expected future rewards to learn representations that inherently capture value and utility for control. More recently MAPLE [10] exploits rich manipulation priors learned from large-scale egocentric videos by predicting fine-grained handobject contact points and hand poses to enable efficient policy learning for complex downstream tasks. 2) Generative and Predictive Modeling: Another major direction adapts the MIM objective to the robotics domain. MVP [11] trains ViT encoders by reconstructing masked patches in robot demonstration videos. This technique was scaled to real-world scenarios [55] and extended in models like Multi-view Masked World Models [56] to leverage multicamera views, enabling better 3D consistency. More advanced geometric extensions, such as 3D-MVP [57], explicitly integrate geometric information from multiple views during pretraining to improve 3D spatial reasoning for manipulation. The emergence of 4D Visual Pre-training (FVP) [58] further highlights the importance of incorporating spatio-temporal dynamics into the pretraining objective. 3) Zero-Shot Transfer of VFMs for Robotics: With the advancements of VFMs, they are now being increasingly used for various robotic tasks without fine-tuning. For instance, DINOBot [59] uses DINO features for semantic retrieval and pixel-level alignment to enable one-shot imitation learning, significantly improving data efficiency. Other methods integrate DINOv2 features into voxel representations for bimanual manipulation [60] or use them for zero-shot planning within World Model framework [61]. This suggests that the dense, robust feature space learned by DINOv2 is strong prior for robotics. Some works, such as Theia [8], propose distilling the knowledge from multiple different VFMs into single, smaller backbone showcasing improvements over single VFM backbone across diverse robotics tasks. C. Reinforcement Learning from Depth Depth sensing, in contrast to RGB, provides direct access to metric, three-dimensional geometry, which is crucial for grounded robotic interaction and robust sim-to-real transfer [12], [62], [63]. This property has led to extensive research focusing on training end-to-end RL policies directly from depth for various manipulation, locomotion, and navigation tasks. 1) Locomotion and Navigation: In legged and mobile robotics, accurate knowledge of terrain geometry is crucial for robust locomotion and navigation. Earlier works addressed perceptive locomotion by converting depth sensor data into explicit geometric structures, such as height maps or elevation maps [13], [14], [17], [64]. While these explicit maps are straightforward to render in simulation, they often suffer from significant noise, require highly accurate odometry, and are challenging to reliably obtain in real-world scenarios due to the limited Field of View (FoV) of ego-centric depth cameras, frequently leading to sim-to-real gap [65]. To address these limitations, major shift involved learning end-to-end policies directly from egocentric depth observations via RL. Agarwal et al. [15] were among the first ones to demonstrate an endto-end locomotion system capable of traversing challenging terrains (stairs, curbs, stepping stones) using quadruped robot equipped with only single front-facing depth camera. Subsequent methods have built on this foundation, utilizing egocentric depth for more complex behaviors such as parkour [66], [67] and learning local obstacle avoidance [68]. More recently, this paradigm has been extended to humanoid locomotion, where the robustness requirements are even more stringent. Approaches [69], [70] train depth-only RL policies and focus on realistic depth synthesis to bridge the sim-to-real gap for complex environments. For navigation in diverse, large-scale environments, depth is also critical input. Platforms like Habitat [71] enable largescale RL training for tasks such as PointGoal Navigation, where agents frequently use depth or depth-derived features to achieve near-perfect performance in previously unseen environments [16]. This necessity for robust visual priors in [18], also motivates auxiliary pretraining; for instance, the authors introduce spatially enhanced memory unit and demonstrate improved sim-to-real transfer of the navigation policy by leveraging frozen Variational Auto Encoder (VAE) pretrained on large-scale depth dataset [72]. 2) Manipulation and Grasping: Depth perception is critical modality for manipulation and contact-rich tasks in robotics. Various approaches use depth images to construct explicit representations such as voxel grids [73], point clouds [74], or object meshes [75], which are provided to the policy as input. Although shown effective for manipulation, these methods often struggle with cluttered or dynamic scenes and require careful calibration of perception pipelines. To overcome these limitations, recent research has shifted towards end-to-end policies that directly consume egocentric visual observations. Zeng et al. [76] propose Q-Learning framework to train dense affordance maps on visual RGB-D observations to clear cluttered scenes. Kalashnikov et al. [77] by using continuous Q-learning on large-scale real robot data to train general closed-loop grasping policies in continuous action space. However, both approaches operate with simpler, binary action space for two-fingered gripper. Due to the high-dimensional control space of dexterous hands, recent works [74], [78][80] adopt teacher-student training paradigm, where teacher policy receives privileged proprioceptive information and is distilled into student policy that relies solely on visual observations. Singh et al. [79] train student policy on synthetic stereo RGB images and employ extensive domain randomization and image augmentation to handle visual sim-to-real gaps, which results in long training times. To further address the realization gap between teacher and student policies, their recent work [20] provides the teacher with depth observations alongside privileged proprioceptive inputs. They demonstrate that distilling such teacher into an RGB-based student results in more robust manipulation policies. In contrast, Zhang et al. [21] train teacher policy using the full point cloud of the object and distill it to student policy that receives only partial point cloud of the scene obtained from single-view depth image. 3) The Untapped Potential of Depth Foundation Models: This extensive body of literature confirms two critical facts: 1) Depth is popular modality for effective sim-to-real and geometric control due to its invariance to light/texture and the ease of high-fidelity simulation [81][83]; and 2) geometric and semantic reasoning is crucial for success in core robotics tasks. However, critical gap remains: nearly all referenced RL and visual control methods relying on depth use custom, task-specific encoders. These encoders are typically trained from scratch alongside the policy, which is inefficient and restricts the learned representations generalization to the specific task and environmental domain in which it was trained. This highlights the lack of available depth encoders trained using large-scale VFM pretraining objective. We argue that the same self-supervised scaling laws that revolutionized RGB vision (Sec. II-A) must be applied to the depth modality. By introducing DeFM, we close this gap by providing the first large-scale depth foundation model for robotics, generating robust, semantic-aware features capable of universal transfer across the diverse locomotion, navigation, and manipulation tasks reviewed here. III. PRELIMNARIES The success of the DINOv2 [2] in learning highly generalizable and robust visual features makes it the ideal foundation for our DeFM. DINOv2 achieves state-of-the-art performance on dense prediction tasks, such as depth estimation and semantic segmentation, even when its weights are frozen. This demonstrates that its self-supervised objective is uniquely effective in capturing the precise spatial and metric information essential for robotics, making it suitable SSL paradigm for transferring to the depth modality. The core of the DINOv2 approach is self-distillation objective applied to ViT architecture, training student network to match the output distribution of momentumupdated teacher network. Its self-supervision comprises of two main objectives along with some enhancements, which we discuss below: 1) Image-Level Objective: LDINO is the standard DINO cross-entropy loss [1] between the students output probability distribution (ps) and the teachers target distribution (pt). Both distributions are derived from the ViTs class token (cls) features, extracted from different augmented views of the same image. The students parameters are updated via backpropagation, while the teachers weights are constructed using an exponential moving average (momentum update) of the students past weights. LDINO = (cid:88) pt log ps (1) 2) Patch-Level Objective: LiBOT enhances spatial consistency by adapting the iBOT framework [38]. This objective applies the cross-entropy loss between the students prediction (psi) for randomly masked input patches and the teachers target distribution (pti) derived from the corresponding visible patches in the teacher network. By forcing the student to predict the teachers features for masked tokens, this term explicitly promotes the learning of dense spatial correspondences. LiBOT = pti log psi (2) (cid:88) geometric and photometric augmentations tailored for depth. The global crops are processed by the momentum-updated teacher network (ft) to produce the target distributions pt. The student network (fs) processes the local crops and the partially masked global crops (x g). Three primary losses are then applied to align the students output with the teachers targets: DINO Global Crops Loss (LGlobal): This loss aligns the students representation of the partially masked global crops (x g) with the teachers representation of the unmasked global crops (xg). It operates on the cls token features: LGlobal = (cid:88) (cid:88) i=1 j=1,j=i LDINO(fs(x gi), ft(xgj)) (3) DINO Local Crops Loss (LLocal): This term aligns the students representations of the local crops (xl) with the teachers representations of the global crops (xg). This is computed between the cls tokens: LLocal = (cid:88) (cid:88) g=1 l= LDINO(fs(xl), ft(xg)) (4) iBOT Patch Loss (LiBOT): This loss is essential for learning dense spatial features. It applies the cross-entropy loss between the students feature prediction (psi) for randomly masked input patches and the teachers target distribution (pti) derived from the corresponding visible patches in the teacher network: (cid:88) LiBOT = pti log psi (5) 3) Objective Enhancements: DINOv2 employs two critical enhancements for stable and effective large-scale training: (a) KoLeo Regularizer: To prevent the collapse of the feature space and maintain feature diversity, repulsive term known as the KoLeo regularizer [84] is added to the loss. This term encourages the features to be dispersed across the feature manifold. (b) Sinkhorn-Knopp Centering: The teacher outputs are stabilized and normalized using the Sinkhorn-Knopp algorithm, replacing the previously used softmax-centering step, leading to stability and preventing model collapse. IV. DEPTH FOUNDATION MODEL A. Overview"
        },
        {
            "title": "We first",
            "content": "train our largest model, ViT-L with 307 parameters, on our curated depth dataset (Sec. IV-B). Since it is crucial to learn metric-aware features at diverse scales, we introduce novel input normalization strategy (Sec. IV-C). We then distill the knowledge from DeFM ViT-L into smaller, lightweight architectures (e.g., ViT-S, ResNets) for resourceconstrained robotic deployment (Sec. IV-E). Our pretraining leverages DINO-style self-distillation objective adapted for the depth modality, as shown in Fig. 2. For given input depth image x, we prepare large global crops (xg) and smaller local crops (xl) by applying various imasked The total loss is the weighted sum of these three terms, along with the KoLeo regularizer to prevent model collapse, as detailed in Sec. III. B. Dataset As shown in [2], the development of strong foundation model necessitates both scale and diversity in pretraining data. For depth-specific foundation model tailored for robotics, this need is crucial: the encoder must learn robust geometric and semantic priors spanning wide range of scales and tasks, from centimeter-scale dexterous manipulation to large-scale outdoor navigation and autonomous driving. To address this challenge, we curated dataset totaling 60.4 depth images, combining 18 distinct datasets (Tab. I). This collection is broadly classified into three types based on the depth image source, ensuring maximal coverage across sensor fidelity and domain: Monocular Depth Estimation (MDE): While the RGB domain benefits from curated, large-scale object-centric datasets (e.g., ImageNet-21k [26]), no equivalent diverse dataset exists for depth. To ensure DeFM learns strong object-centric priors for various entitiessuch as cups, fruits, tools, etc, we leverage these RGB datasets by converting them into depth images using an off-the-shelf MDE network [85]. This MDE-derived data significantly Fig. 2: Overview of the self-supervised pretraining of DeFM. contributes to our overall scale and instills generalized object semantics in the encoder. Synthetic: These datasets provide clean, noise-free metric depth images and offer domain diversity costly to replicate at scale in the real world. Real: This category provides the noise characteristics inherent in real robotic sensor data. By incorporating diverse real-world sources collected with different depth sensors, the model learns to be robust and invariant to common sensor artifacts, missing data, and noise profiles. Learning these noise-invariant features is critical for effective sim-to-real transfer in robotic deployments. Overall, the mixture of these sources provides balance in diversity, scale, and noise fidelity, thereby maximizing the learned representations ability to generalize across diverse, unknown robotic environments. C. Input Normalization depth foundation model must operate effectively across an exceptionally wide range of depth scales, from millimeterlevel precision required in dexterous manipulation to tens or even hundreds of meters in outdoor navigation. Such model must preserve metric depth while attending to fine-grained details in the scene. In practice, however, near-field depth variations matter disproportionately more for robotic decisionmaking (e.g., avoiding an obstacle at 1 m) than those in the far-field. This observation motivates the use of logarithmic depth compression, which enhances sensitivity to close-range structure while smoothly compressing large depth values, strategy widely employed in autonomous driving and depthestimation systems [101]. Rather than choosing single input normalization strategy, we thus propose three-channel log-compressed depth repTABLE I: Overview of depth pretraining dataset collection. MDE denotes monocular depth estimator. Dataset Name ImageNet-21k [26] SA-1B [51] Replica [86] Hypersim [87] Meta GraspNet [88] TartanGround [89] TartanAir [72] SHIFT [90] Taskonomy [91] HM3D [92] ARKitScenes [93] ScanNet [94] GraspNet-1B [95] SUN3D [96] DROID [97] GrandTour [98] DIML [99] DrivingStereo [100] Total Type MDE MDE Synthetic Synthetic Synthetic Synthetic Synthetic Synthetic Real Real Real Real Real Real Real Real Real Real Domain Size (M) Objects Objects, Indoor, Outdoor Indoor Indoor Manipulation Indoor, Outdoor Indoor, Outdoor Autonomous Driving Indoor Indoor Indoor Indoor Manipulation Indoor Manipulation Indoor, Outdoor Outdoor Autonomous Driving 14.0 11.0 0.3 0.07 0.3 5.6 5.8 0.9 4.0 11.0 3.0 0.4 0.1 0.5 2.8 0.2 0.2 0.18 60.4 resentation to capture depth structure across all operational regimes while preserving metric scale. Let logp(D) = log(1 + D) denote the standard log1p transform. From the raw metric depth D, we construct three complementary channels: 1) Global Log-Scaled Depth: This channel normalizes the log-compressed depth using the minimum (Dmin) and maximum (Dmax) depth within the current image, Fig. 3: Visualization of different depth input normalization methods. Log normalization effectively captures the overall depth range while preserving fine-grained structure in near-field regions (b, d, f). In contrast, standard metric normalization yields weaker contrast and poorly separated gradients (c, e). In our representation, we stack columns (b), (d), and (f) to form 3-channel normalized depth input, which preserves metric depth while maintaining robustness across diverse domains. thereby preserving relative geometric structure (Fig. 3-b): D. Implementation Details C1 = logp(D) logp(Dmin) logp(Dmax) logp(Dmin) . 2) Mid-Range Norm: This channel emphasizes the depth regime most relevant for manipulation and indoor interaction (Fig. 3-d): C2 = logp(D) logp(10) . 3) Far-Range Norm: This channel emphasizes the depth regime most relevant for long-range navigation and outdoor scenes (Fig. 3-f): C3 = logp(D) logp(100) . In summary, 2) and 3) preserve metric depth at different scales while 1) provides relative depth, allowing for attending to finer details in close range, such as those required for manipulation. The three channels are stacked to form the final depth input representation: Xin = [C1, C2, C3]. Finally, we apply global mean and standard deviation normalization, computed once across the entire pretraining dataset. As visualized in Fig. 3, our representation preserves global metric depth, maintains fine-grained near-field structure, and provides stable gradients, yielding robust and scalable depth representation for self-supervised pretraining. We train our DeFM using the Fully-Sharded Data Parallel (FSDP) implementation of DINOv2 [2] and ViT-L/14 backbone. We prepare two global crops at 2242 resolution and eight local crops at 982. As in [2], we use separate MLP projection heads for DINO and iBOT losses. Training was conducted for 625k iterations on 96 NVIDIA GH200 Grace Hopper GPUs using distributed batch size of 3,072. We use AdamW optimizer with learning rate of 1.5 104 and cosine weight decay schedule (0.04 to 0.2). We apply 100k iteration learning rate warmup and update the teacher networks momentum using cosine schedule (0.994 to 1.0). All training is performed in float16 precision. Due to computational constraints, we adopt DINOv2 hyperparameters without tuning; for additional details, we refer the reader to the DINOv2 implementation. E. Distillations Knowledge distillation [102] aims to transfer the knowledge of large teacher model into smaller, more efficient student model by minimizing the distance between their outputs. Since our self-supervised objective is fundamentally distillation process from the teacher network to the student, we leverage the same training loop with specific modifications to the student network architectures. We use DeFM-L/14 as frozen teacher for all distillation runs and maintain separate EMA of the students weights, which serves as the final distilled model. 1) Motivation and Target Architectures: For many downstream RL tasks, the ability to use large batch sizes is critical for stable policy optimization, such as in PPO [103]. Therefore, there is need for smaller, efficient encoders to minimize memory footprint [15], [18], [20] and faster training times. We thus distill DeFMs representations into ViT-S and three lightweight CNN families, namely, ResNets [23], RegNets [25], and EfficientNets [24], ranging in size from 3 to 30 parameters. 2) Architectural Adaptation via BiFPN: Distilling global and dense spatial features from ViT teacher into standard CNN student is non-trivial, as typical CNN last-layer global pooling discards the spatial information needed for dense prediction tasks. To ensure the CNN students retain the necessary dense spatial features, we integrate Bi-directional Feature Pyramid Network (BiFPN) [104] on top of our CNN encoders. The BiFPN performs bi-directional fusion of feature maps extracted from the CNN backbone at three distinct resolutions (F/{8, 16, 32}). The BiFPN then outputs enhanced, dense spatial features at the same resolutions. 3) Distillation Strategy and Hyperparameters: We utilize the same crop strategy as the original pretraining. However, we adjust the input size to align the student and teacher feature maps: we pass global crops of size 256 256 to the CNN students. This ensures the output spatial feature map from the BiFPN at the /16 resolution (i.e., 16 16) aligns with the spatial tokens of the ViT-L/14 teacher (224/14 16 16). For the DINO losses (LDINO), we match the global pooled feature map of the CNN encoder against the cls token of the frozen ViT teacher. For the iBOT loss (LiBOT), we remove the masking and match the dense spatial output of the BiFPN at /16 resolution with the spatial patch tokens of the ViT teacher for both global crops. As before, separate MLP projection heads are used for both the DINO and iBOT losses. We use multi-student distillation approach for computational efficiency [3]. Distillation was run for 200k iterations on 64 NVIDIA GH200s with batch size of 2, 048 and learning rate of 1 103 for each student. V. EXPERIMENTS In this section, we aim to evaluate the quality and generalizability of the representations learned by DeFM. For model to be considered truly foundational, its learned features must successfully transfer across different tasks, domains, and sensors. We first provide qualitative assessment by performing Principal Component Analysis (PCA) on the extracted features (Sec. V-A) to visualize the emergent feature structure. We then quantitatively assess our frozen encoders transfer capability across diverse benchmarks. This includes evaluation via linear probing for classification (Sec. V-B) and evaluation on the dense prediction task of semantic segmentation (Sec. V-C). Lastly, as our focus is on robotic applications, we compare the inference times of all our DeFM and distilled models in Sec. V-D. We discuss in detail the performance of DeFM for various robotics tasks in separate dedicated Section (Sec. VI). A. Foundational Depth Image Understanding While depth images provide very strong geometric priors, we argue that meaningful semantic features can also be obtained from them despite the absence of texture and color. To demonstrate this, we perform PCA on features extracted by our DeFM-L/14 encoder. We collect depth images of various cups using four different depth sensors, namely, Realsense L515 (solid state LiDAR depth camera), Realsense D435i (stereo infrared), ZED 2i (with neural Light depth) and ZED (with neural Plus depth). We then fit PCA on the extracted patch features and visualize the first three components, mapping them to RGB channels  (Fig. 5)  . We observe strong, consistent correlation between the features and the functional parts of the cup across all sensors. For example, the cup handle is consistently clustered and visualized in yellow, while the cup rim is associated with magenta. The consistency of these semantic features across diverse sensor modalities has significant implications for robotics, serving as robust prior on where to grasp for downstream policy learning. In Fig. 1-V, the model successfully differentiates and highlights the handles across various types of drawers and cabinets. Overall, these qualitative experiments demonstrate the capability of DeFM to learn generalized, meaningful semantic features robustly across various object classes and depth sensor types. B. Classification The classification task evaluates the quality of the learned visual representations by testing their ability to distinguish between different object categories. Due to the unavailability of standard classification benchmark for depth images, we create an ImageNet-Depth-1K benchmark consisting of RGB converted depth images estimated from an MDE network [85]. Linear probing is the standard evaluation protocol for VFMs. In this setup, the pretrained encoder is kept frozen, and only lightweight linear classifier is trained on labeled data. Strong linear-probe performance indicates that the encoder has learned feature space where classes are linearly separable, reflecting strong generalization without task-specific fine-tuning. We additionally include top-1 and top-5 KNN accuracy, which evaluates how well the frozen embeddings cluster by retrieving labels from their nearest neighbors in feature space. This training-free evaluation provides complementary and typically lower-bound estimate of representation quality relative to linear probing. 1) Baselines: We compare the performance of our models against state-of-the-art VFMs from the RGB domain (inferring on depth images) that represent different pretraining paradigms: ViT-L comparison: We compare our largest model, DeFM-L/14, against the self-distilled VFMs: DINOv2 [2] and the more recent DINOv3 [3]. We also include CRADIOv3 [50], an agglomerative model that distills features from multiple VFMs into single encoder. ViT-S comparison: For the smaller, resource-efficient model, DeFM-S/14, we compare against the correspondFig. 4: Overview of distilling the DeFM-L/14 teacher into CNN backbones. BiFPN module is added on top of the CNN encoder to produce dense spatial features, which are supervised using the teachers spatial tokens. The teachers class token provides global supervision to the CNNs pooled feature representation. TABLE II: Classification evaluations on ImageNet-1k-Depth. Architecture Feature Top-1 KNN () Top-5 KNN () Linear () DINOv2 ViT-L/14 ViT-L/16 (CPE) C-RADIOv3 ViT-L/16 ViT-L/14 DINOv3 DeFM DeiT-S/16 ViT-S/14 ViT-S/16 ViT-S/14 Theia DINOv2 DINOv3 DeFM 61.93 55.90 64.07 63. 28.17 47.66 49.50 55.27 81.76 77.39 83.48 84.79 47.47 69.11 70.93 78.06 68.70 64.12 69.15 71.72 33.14 55.06 50.86 61.54 TABLE III: Comparison of distilled CNN architectures on ImageNet-1k-Depth Classification. Model ViT-S/14 ViT-L/14 ResNet-18 ResNet-34 ResNet-50 RegNetY-400MF RegNetY-800MF RegNetY-1.6GF EfficientNet-B0 EfficientNet-B2 EfficientNet-B4 EfficientNet-B6 Params (M) Top-1 KNN () Top-5 KNN () Linear () 22.1 307.0 11.7 21.8 26.2 4.1 6.3 12. 3.01 4.95 14.16 28.98 55.27 63.46 45.71 48.16 53.71 49.51 51.58 53.03 44.38 47.43 50.33 53.35 78.06 84. 69.69 72.72 77.63 72.87 74.91 76.21 67.98 71.51 74.74 77.81 61.54 71.72 50.58 54.39 61.54 50.51 57.03 57. 46.17 50.32 54.73 59.23 specifically distilled for robotics tasks. For all RGB-pretrained baselines, we perform min-max normalization to the depth image, stack it into three channels, and finally normalize using the standard ImageNet mean and standard deviation before processing. 2) Results: The quantitative results (Tab. II) demonstrate the superior feature quality of our DeFM encoder: ViT-L performance: DeFM-L/14 achieves the highest score on both the Top-5 KNN (84.79%) and linear probFig. 5: PCA visualization of the patch features obtained from the DeFM-L/14 encoder when processing depth images of various cups captured by different sensors. The first three PCA components are mapped to the RGB color channels for visualization. Notice the feature consistency of the cup handle (visualized in yellow) across all images, demonstrating that DeFM learns useful prior for robotic grasping task. The background is removed by thresholding the first PCA component. ing size variants of DINOv2 and DINOv3. Since CRADIOv3 is not available in this small size, we instead use Theia-S [8], which is another agglomerative model ing (71.72%) metrics, while remaining highly competitive with the DINOv3 model on the Top-1 KNN metric. ViT-S performance: DeFM-S/14 outperforms existing state-of-the-art models in its size category by up to 10% across all three metrics. This large performance gap highlights the limitations of existing smaller distilled RGB models when applied to depth and strongly reinforces the need for smaller-sized DeFM variants for efficient, high-performance robotic deployments. Surprisingly, employing depth information alone yields 71.7% accuracy on 1000-class object classification task. While this performance is lower than the current state-ofthe-art models for RGB input, which achieve around 87.2% accuracy [3], it clearly demonstrates the intrinsic power of depth data. The result is particularly impressive, given that the classification task includes challenges such as differentiating between various snake species or fine-grained variations in furniture, tasks that typically require human experts to rely on color and texture cues. 3) Performance of CNNs: We evaluate all of our distilled CNN variants of DeFM to understand the tradeoff between compute time and feature quality, as these smaller models are essential for efficient robotics deployment. The results are summarized in Tab. III. As expected, we observe general drop in performance correlating with decreasing model size. However, we note surprisingly strong result: some of the very small CNN models, such as RegNetY-400MF (4.1 M), outperform the ViT-S RGB baselines (22.1 M) (Tab. II) across all three metrics. C. Semantic Segmentation Semantic segmentation is critical evaluation task for VFMs, as it directly assesses the quality of dense spatial features, which are essential for most robotic tasks, such as grasp localization, obstacle avoidance, and scene understanding. To assess whether the frozen DeFM encoder provides meaningful per-pixel representations, we perform linear-probe segmentation. Following the standard protocol, we train linear layer to predict the class logits from the patch tokens. The output is then upsampled to the full resolution using bilinear interpolation to obtain the final segmentation map. We use the same baselines that were introduced earlier for classification in Sec. V-B1. 1) Datasets: An ideal foundation model must demonstrate robust generalization across diverse domains and sensor types. Accordingly, we select 5 diverse depth datasets for evaluating semantic segmentation, ensuring coverage across varied environments and acquisition modalities. These benchmarks range from indoor room settings and fine-grained tabletop manipulation scenarios to large-scale outdoor navigation environments. The datasets have been summarized in Tab. IV. 2) Results: The quantitative results in terms of mean Intersection over Union (mIoU) are summarized in Tab. V. Overall, DeFM models demonstrate robust generalization across different domains and surpass existing baselines in the majority of benchmarks. For the ViT-L architecture, DeFM-L/14 achieves the highest mIoU scores across four out of five datasets, while the performance improvement is particularly pronounced for the ViT-S architecture. Consistent with our classification findings, the generalization gap is significant, with DeFMS/14 achieving mIoU scores up to 30% higher than the corresponding baselines in several benchmarks. We illustrate the improvement qualitatively by comparing the segmentation output of DINOv3-S/16 with our DeFM-S/14 in Fig. 6. Finally, our smaller distilled ResNet models consistently outperform the similar-sized baseline models on almost all datasets, providing strong geometric and semantic features for resourceconstrained robotic deployments. D. Inference Times To facilitate informed hardware planning, we report comprehensive metrics for all models in the DeFM family in Tab. VI. This evaluation provides crucial guideline for understanding the tradeoff between training times, memory consumption, and performance. We report the inference latency and VRAM consumption on an NVIDIA RTX 4090 (using large batch size, BS = 128), which is typical GPU used to train downstream RL policies. Furthermore, we report the inference time for single image (BS = 1) on an NVIDIA Jetson AGX Orin, representative platform for real-world robotic deployments. We use images of size 224 224 for all reported numbers. Our results highlight that inference speed on the Orin edge hardware is highly dependent on the models architecture itself, rather than solely on parameter count: RegNetY models, which often incorporate features like Squeeze-and-Excitation (SE) layers, and EfficientNet models, which rely on depthwise separable convolutions, show unexpectedly high latency on the Orin platform as these operations are not always fully optimized in standard PyTorch execution. It is important to note that all reported timings are measured using the standard PyTorch framework, and no model runtime optimizations (e.g., TensorRT/ONNX conversion) were performed. Utilizing such optimizations is expected to reduce the latency across all models, particularly on the resourceconstrained Orin platform. VI. ROBOTIC EXPERIMENTS While our analysis in Sec. established the strong generalization of DeFMs frozen features on standard perception tasks (classification and segmentation), the main motivation of this work is to provide representations that are directly usable for RL, ideally without any task-specific fine-tuning. Solving complex, embodied control tasks requires features that are not just semantically meaningful but are geometrically grounded and action-relevant. To evaluate the quality of our DeFM representations on downstream tasks, we select four distinct and challenging RL tasks across navigation, manipulation, and locomotion. Specifically, our evaluation includes: Habitat Point-Goal Nav (Sec. VI-A): standard benchmark in the Habitat simulator [71] where an agent must navigate to target 3D goal point in various indoor scenes using discrete set of actions. TABLE IV: Summary of segmentation datasets used for downstream evaluation. Dataset Domain / Scene Type # Classes Range (m) Sensor Indoor room scenes Indoor rooms / building scenes ScanNet [94] SUN-RGBD [105] TartanGround [89] Outdoor construction, industry, infrastructure OFFSED [106] GraspNet-1B [95] Outdoor off-road environments Tabletop object grasping 40 37 6 5 40 < 10 < 10 < 100 < 30 < 1 Structure Sensor Kinect v1-v2 / RealSense / Xtion Simulated ZED Stereo Camera Azure Kinect TABLE V: Semantic Segmentation evaluation on various datasets (mIoU) (). Model Pretraining Indoor Outdoor Manipulation ScanNet SUN-RGBD OFFSED TartanGround GraspNet-1B DINOv2 ViT-L/14 ViT-L/16 (CPE) C-RADIOv3 ViT-L/16 ViT-L/14 DINOv3 DeFM DeiT-S/16 ViT-S/14 ViT-S/16 ViT-S/ ResNet-50 ResNet-34 ResNet-18 Theia DINOv2 DINOv3 DeFM DeFM DeFM DeFM 24.46 25.56 28.52 31.34 14.71 18.31 20.05 27.69 29.09 27.79 22. 27.11 28.17 32.74 31.26 11.18 18.46 18.42 27.78 26.02 24.21 19.85 53.47 56.53 54.42 57.62 42.36 47.43 56.32 57.35 51.78 52.21 46. 59.24 64.31 62.16 67.69 47.84 54.98 56.97 64.66 64.84 63.91 59.18 24.26 25.18 23.89 27.85 9.84 15.58 14.87 19.89 26.11 24.26 20. TABLE VI: Comparison of inference times of DeFM models on training and deployment hardware. Model Batch Size ViT S/14 ViT L/14 ResNet-18 ResNet-34 ResNet-50 RegNetY-400MF RegNetY-800MF RegNetY-1.6GF EfficientNet-B0 EfficientNet-B2 EfficientNet-B4 EfficientNet-B6 Params (M) FLOPs (GFLOPs) RTX 4090 (ms) Memory (GB) Jetson AGX Orin (ms) - 128 128 22.1 307.0 707.48 9962.24 63.76 624. 11.7 21.8 26.2 4.1 6.3 12.4 3.01 4.95 14.16 28.98 256.13 493.69 631.19 60.05 133.28 296.20 52.72 94.13 256.24 460. 21.06 33.08 69.39 17.27 25.21 44.25 29.39 46.12 86.51 150.98 128 0.99 3.32 0.93 1.33 1. 1.07 1.56 1.39 1.85 2.26 3.50 4.56 1 11.92 72.82 8.67 13.54 17.79 25.17 24.16 41. 21.04 28.37 39.67 54.11 Embodiment Aware Point-Goal Nav (Sec. VI-B): Utilizing the Unitree B2W wheeled-legged robot, this longrange task [18] tests navigation policies that must handle the robots kinematics and physical footprint, prioritizing accurate geometric obstacle avoidance. Dexterous Grasping (Sec. VI-C): Using KUKAAllegro arm-hand setup, the agent is tasked with grasping diverse range of objects [79]. Quadrupedal Ladder Climbing (Sec. VI-D): This challenging locomotion task requires the ANYbotics ANYmal quadruped to robustly climb ladders of varying sizes and angles [107]. The policy relies on proprioception and features derived from four depth cameras to ensure stable footholds and gait synchronization. In the subsequent subsections, we discuss in detail the setup, baselines, and evaluations for each of the tasks. Unless otherwise stated, we always use frozen DeFM encoder and only train the downstream policy network. A. Navigation: Habitat Point-Goal Nav 1) Setup and Baselines: This task uses the Habitat PointGoal Navigation benchmark [71], where the agent must navigate to target goal point in various multi-room multi-floor environments. The policy is optimized using Decentralized Distributed Proximal Policy Optimization (DD-PPO) [16]. The policy architecture utilizes Recurrent Neural Network (RNN) network to process features extracted from the visual encoder. Training was conducted on the Gibson train set for 75 steps using 32 environments distributed across 4 GPUs. We compare the performance of our frozen DeFM-S/14 and DeFM-ResNet-50 against several baselines in the competitive size category: Scratch Baseline: ResNet-50 trained entirely from scratch, as commonly done in prior literature [16], [71] [2], Frozen VFMs: ViT-S variants of DINOv2 DINOv3 [3], and Theia [8]. 2) Results: The evaluation results are measured by Success weighted by Path Length (SPL) and are summarized in Tab. VII. We train each model three times and report the average performance. The frozen DeFM models outperform all other frozen foundation model baselines across both evaluation sets (Gibson validation and MatterPort3D validation). We note that DINOv3 also shows remarkable performance on depth data, however, both of our DeFM models outperform it. On the Fig. 6: Qualitative results of semantic segmentation on different datasets. TABLE VII: Navigation performance in Habitat Environments measured by Success weighted by Path Length SPL (). Pretraining Gibson (Val) MP3D (Val) ResNetScratch 0.8986 0.7802 DeiT-S/16 ViT-S/14 ViT-S/16 ResNet-50 ViT-S/14 Theia DINOv2 DINOv3 DeFM DeFM 0.6278 0.8650 0.8795 0.8876 0. 0.4832 0.7096 0.7428 0.7585 0.7509 other hand, Theia, which was specifically distilled for robotics tasks, and DINOv2 do not perform that well on depth data. Our best distilled model, DeFM-ResNet-50, achieves competitive performance compared to the ResNet-50 trained entirely from scratch, demonstrating the value of pretraining in avoiding the cost and complexity associated with training taskspecific encoders. These results highlight that DeFMs frozen features are directly usable and highly effective for depthbased navigation policies. B. Navigation: Embodiment Aware Point-Goal Nav Having demonstrated that DeFM features are useful for navigation, we next test their transferability in more realistic and challenging scenario. For this, we select the point goal navigation task introduced in [18] and utilize the Unitree B2W wheeled legged robot. In this task, the policy must perform long-range navigation over diverse terrains and complex environments while actively avoiding obstacles using single front-facing depth camera as input. 1) Setup and Baselines: For encoding the depth images, the authors pretrain Variational Auto Encoder (VAE) on [72] using reconstruction loss, which is kept frozen during the RL policy training. compact RegNetX-400MF encoder, along with Feature Pyramid Network (FPN), is used to encode the depth images. The dense output of the FPN is then used to perform cross attention with the proprioceptive data and processed by Spatially-enhanced Recurrent Unit (SRU) block and few MLP layers to generate the final velocity commands, which are then tracked by low-level locomotion policy. To perform fair comparison with this baseline, we distill our DeFM ViT-L/14 into the same network architecture on the full DeFM depth dataset. Additionally, we also distill DinoV3-L features on the DeFM dataset into the same network as an additional baseline. For training the downstream RL policy, we adopt the exact same hyperparameters specified in [18] and train on single RTX 4090 GPU. Downsampled depth images of size 64 40 are used as an input to the network. We evaluate the performance using the Success Rate (SR) metric, where run is considered successful if the agent manages to reach the goal-point within 120 seconds. Apart from evaluating in the training environment, we set up three realistic, geometrically complex TartanGround [89] meshes in Gazebo simulation for realistic out-of-distribution evaluation  (Fig. 7)  . These consist of Industrial Hangar (b), Abandoned Cable (c), and the Modern City Downtown (d) environments. These environments depict realistic scenarios for deployments and are quite different from the training environment (a), which primarily consists of mazes, pits, and staircases. We randomly sample 100 navigable start-end goal pairs in each of the environments within range of 10 to 80 m. We evaluate each policy 3 times and report the average SR. We discuss the results obtained in the simulation in the following subsection, after which we proceed to perform real-world experiments in Sec. VI-B3. 2) Results: The results have been summarized in Tab. VIII. While the VAE baseline achieves similar performance to our DeFM in the training, we achieve improved performance during testing, particularly in the Abandoned Cable and Modern City Downtown environments. Upon closer examination of the individual failure cases, we notice that, in general, the policy utilizing our DeFM model performs better at avoiding thin and out-of-distribution (OOD) obstacles, such as fences, traffic signs, lamp posts, etc. We hypothesize that our DeFM model is able to better recognize the OOD obstacles and intrinsically assign higher costs to those obstacles, which the VAE might fail to do. In Fig. 8, we quantitatively analyse the failure cases by breaking them down in terms of collision and timeouts. We can consider the collision failures as perception failure arising primarily due to the encoder, while the timeout usually occurs when the navigation policy is stuck in local minima. We notice that DeFM consistently has lower collision failures than the other two baselines, highlighting the better geometric and semantic understanding of the environment. Finally, we notice that DINOv3 performs the worst of all compared encoders. One reason we hypothesize for this underperformance is the models inability to retain the metric depth understanding. Unlike our DeFM, where the input normalization explicitly attempts to preserve and learn across vast metric scales, DINOv3 treats depth simply as another intensity channel. This result further demonstrates the importance of preserving metric depth for successful transfer to embodimentaware robotic tasks. 3) Real World Deployments: To demonstrate successful sim-to-real transfer, we test our DeFM encoder-based policy in four diverse, challenging real-world environments, including confined obstacle-rich indoor setting, an outdoor urban environment, an unstructured park environment, and highclutter construction site  (Fig. 9)  . We test multiple start-goal endpoints ranging from 5 to 100 and observe that our policy generalizes effectively to these environments, handling TABLE VIII: Navigation Success Sate (SR %) () for embodiment aware navigation. Model Training Industrial Hangar Abandoned Cable Modern City Downtown VAE DINOv3 DeFM 90.25 83.64 90.31 92.33 90.00 92.67 82.33 74.67 84.33 74.00 72.67 79.00 Fig. 7: (a) Shows the training environment, while (b-d) represent the test environments for embodiment-aware navigation. diverse obstacles such as pedestrians, scooters, barriers, furniture, etc., as well as navigating uneven terrains. We visualize one of the deployments from the park environment in Fig. 10 and refer to the supplementary video for more deployments. 4) Discussions and Limitations: While the authors in [18] also demonstrate sim-to-real transfer in diverse environments, our result is particularly impressive since, unlike the VAE, which needs to be pretrained with specific noise model, depth clippings (0.1 and 10 m), and additional hand-engineering, our DeFM encoder works out of the box without any taskspecific heuristics. However, one of the failure cases that we observe is that the policy occasionally struggles with very small or thin obstacles such as wires, poles, or mesh structures (even in simulation). We attribute this primarily to the low spatial resolution of the depth input (64 40) used during the policy training, design choice made by the authors in [18] to ease the sim-to-real transfer and computational constraints. Despite the low resolution, the policy reliably handles complex clutter, indicating that DeFM already provides strong features even under heavy compression. We expect that scaling simulation diversity and input resolution will allow the policy to realize the full potential of the DeFM features, improving sensitivity to fine-grained geometry and mitigating these remaining failure cases. C. Manipulation: Dexterous Grasping 1) Setup and Baselines: We consider the dexterous armhand grasping from DexTRAH [79]  (Fig. 11)  . Following the prior work, the teacher is trained using state-based information, comprising of privileged information such as the object state and its one-hot encoding. The teacher policy is distilled into student policy, which receives depth images instead of stereo RGB images. The depth images are encoded using DeFM and concatenated with the proprioceptive information before feeding it to an LSTM and MLP architecture [79]. We evaluate our DeFM-pretrained ResNet-18 against two representative baselines: standard ResNet-18 pretrained on Fig. 8: Split of navigation failure count () for embodiment aware navigation evaluation across the test environments. Fig. 10: Real world deployment in park environment with goal point at around 90 m. Fig. 9: Real world deployments using Unitree B2W robot in diverse environments- (A) Indoor, (B) Urban, (C) Park, (D) Construction Site. RGB ImageNet, and distilled ResNet-18 trained to match the embeddings of the DINOv3 ViT-L/16 teacher on our DeFM dataset. We compare two training setups: frozen pretrained networks and end-to-end finetuning during teacher-student policy distillation. We also compare these with an encoder trained from scratch during the student distillation process. The training hyperparameters are kept the same as the prior work [79]. The student policies are trained using Isaac Lab [81] using distributed training on eight NVIDIA L40S GPUs, each running 256 parallel environments. The environment provides depth observations augmented with speckle noise, Gaussian noise, pixel dropout, and stick noise [19]. 2) Results: We evaluate the trained policies in two scenarios: (i) the same noise model as during training, and (ii) using the Kinect noise model [108], which adds edge and distance noise to the depth image, to evaluate robustness to realistic sensor noise. representative scenario with the different noise models is shown in Fig. 12. Tab. IX reports the average success rate (normalized to the teacher policy) across 2,000 episodes. The fine-tuned DeFM model achieves the highest performance across both scenarios, and fine-tuning improves results for all models. Notably, the frozen DeFM encoder outperforms both the scratch-trained encoder and fine-tuned ImageNet encoder, while achieving competitive performance relative to the fine-tuned DINOv3 baseline. This highlights the strong Fig. 11: Rollout of DeFM model for dexterous grasping using the Kuka-Allegro setup. Bottom: The simulated noisy depth images used during training. geometric and semantic priors learnt by DeFM. Under the alternative Kinect noise model, frozen models experience significant performance drop, likely because the MLP on top of frozen features overfits to the training depth images. Among the frozen models, DeFM provides more consistent features, resulting in smaller decline. Fine-tuned DeFM is the least affected by changes in the noise model, demonstrating both the adaptability and stability of its taskspecific features. Lastly, we present qualitative results by performing PCA on real-world depth images captured with ZED camera from the DROID dataset [97]. In the complex, cluttered kitchen scenes, the DeFM feature space exhibits clear structure: PCA embeddings cluster different scene components such as the countertop, background surfaces, robot arm, and manipulable objects  (Fig. 13)  . Fig. 12: Depth images input to the dexterous grasping policy. During training, images are augmented with speckles, dropout, and stick noise. For evaluations, we also consider depth images augmented with the Kinect noise model [108]. TABLE IX: Comparison of different model configurations in simulation for dexterous grasping. Performance is normalized relative to the teachers performance and averaged across 2,000 episodes. All models use ResNet-18 architecture. Pretraining Training Noise Kinect Noise Frozen ImageNet DINOv3 DeFM Fine-Tuned Scratch ImageNet DINOv3 DeFM 0.6576 0.6532 0.8089 0.7774 0.7950 0.8243 0.8936 0.0043 0.2075 0. 0.7247 0.7366 0.7837 0.8763 Fig. 13: PCA visualization for real-world images from different scenes of the DROID dataset [97]. Fig. 14: ANYmal climbing ladder, shown together with noisy depth image (top row) from onboard camera. D. Locomotion: Quadruped Ladder Climbing the perceptive student 1) Setup and Baselines: To evaluate the performance of DeFM on locomotion task, we consider perceptive quadrupedal ladder climbing  (Fig. 14)  by adapting techniques from [107]. The controller is trained using teacherstudent setup where the teacher is trained with privileged information that learns to mimic. The student network consists of CNN encoder, which produces the depth embeddings, which are concatenated with the proprioceptive measurements and processed by an RNN network to produce the actions. For the baseline, we follow [66] and train the CNN encoder from scratch, which we compare with our frozen RegNetX-400MF DeFM encoder, which was introduced in Sec. VI-B. The policy is trained in IsaacGym [109] using 1,024 parallel environments, collecting 120 steps per batch over 10,000 training epochs. 2) Results: We evaluate the trained polices across setup consisting of varying ladder rung radii (15 mm to 45 mm with 5 mm increments) and ladder angles (70 - 90 with 5 intervals) while applying random disturbances. Each combination is tested for 750 episodes with 1024 environments. We use the Success Rate (SR) metric for comparison, where the episode is considered success if the robot manages to climb the ladder within 15 seconds. Our DeFM model (90.14%) matches the performance of the CNN baseline trained from scratch (90.45%) while requiring substantially less compute. This demonstrates that our encoder can be transferred to challenging locomotion tasks, without any task-specific finetuning. Since our model was trained to handle sensor noise and varying depth data, we hypothesize an even stronger generalization for sim-to-real transfer compared to the CNN baseline. Preliminarily, we present some qualitative results by performing PCA on depth images collected during ladderclimbing deployment. As seen in Fig. 15, our DeFM is able to consistently assign similar features to the ladder structure despite the heavy noise inherent in real-world stereo depth cameras. VII. CONCLUSION AND FUTURE WORK In this work, we introduced DeFM, family of depth image encoders pretrained using self-supervision on curated depth dataset of 60 depth images. DeFM learns robust geometric and semantic features that generalize across tasks, environments, and sensors. Our results show that DeFM serves as an effective off-the-shelf depth encoder for wide range of robot perception and control tasks, including classification, segmentation, navigation, locomotion, and manipulation without any task-specific fine-tuning. Moreover, it enables robust sim-to-real transfer across diverse real-world settings. key outcome of this work is the effectiveness of our distilled variants, which retain most of the representational strength of the large model while offering substantial gains in efficiency. These compact models make it practical to deploy strong depth representations on resource-constrained robotic systems."
        },
        {
            "title": "REFERENCES",
            "content": "[1] M. Caron, H. Touvron, I. Misra, H. Jegou, J. Mairal, P. Bojanowski, and A. Joulin, Emerging properties in self-supervised vision transformers, in Proceedings of the IEEE/CVF international conference on computer vision, 2021, pp. 96509660. [2] M. Oquab, T. Darcet, T. Moutakanni, H. Vo, M. Szafraniec, V. Khalidov, P. Fernandez, D. Haziza, F. Massa, A. El-Nouby et al., Dinov2: Learning robust visual features without supervision, arXiv preprint arXiv:2304.07193, 2023. [3] O. Simeoni, H. V. Vo, M. Seitzer, F. Baldassarre, M. Oquab, C. Jose, V. Khalidov, M. Szafraniec, S. Yi, M. Ramamonjisoa et al., Dinov3, arXiv preprint arXiv:2508.10104, 2025. [4] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark et al., Learning transferable visual models from natural language supervision, in International conference on machine learning. PmLR, 2021, pp. 87488763. [5] M. Kim, K. Pertsch, S. Karamcheti, T. Xiao, A. Balakrishna, S. Nair, R. Rafailov, E. Foster, G. Lam, P. Sanketi, Q. Vuong, T. Kollar, B. Burchfiel, R. Tedrake, D. Sadigh, S. Levine, P. Liang, and C. Finn, Openvla: An open-source vision-language-action model, arXiv preprint arXiv:2406.09246, 2024. [6] P. Intelligence, K. Black, N. Brown, J. Darpinian, K. Dhabalia, D. Driess, A. Esmail, M. Equi, C. Finn, N. Fusai et al., Pi 0.5: vision-language-action model with open-world generalization, arXiv preprint arXiv:2504.16054, 2025. [7] J. Zhang, K. Wang, S. Wang, M. Li, H. Liu, S. Wei, Z. Wang, Z. Zhang, and H. Wang, Uni-navid: video-based vision-languageaction model for unifying embodied navigation tasks, arXiv preprint arXiv:2412.06224, 2024. [8] J. Shang, K. Schmeckpeper, B. B. May, M. V. Minniti, T. Kelestemur, D. Watkins, and L. Herlant, Theia: Distilling diverse vision foundation learning, in 8th Annual Conference on Robot models for robot Learning, 2024. [9] S. Nair, A. Rajeswaran, V. Kumar, C. Finn, and A. Gupta, R3m: universal visual representation for robot manipulation, in Conference on Robot Learning, 2022. [10] A. Gavryushin, X. Wang, R. J. Malate, C. Yang, X. Jia, S. Goel, D. Liconti, R. Zurbrugg, R. K. Katzschmann, and M. Pollefeys, Maple: Encoding dexterous robotic manipulation priors learned from egocentric videos, arXiv preprint arXiv:2504.06084, 2025. [11] T. Xiao, I. Radosavovic, T. Darrell, and J. Malik, Masked visual pretraining for motor control, arXiv preprint arXiv:2203.06173, 2022. [12] F. Muratore, F. Ramos, G. Turk, W. Yu, M. Gienger, and J. Peters, Robot learning from randomized simulations: review, Frontiers in Robotics and AI, vol. 9, p. 799893, 2022. [13] T. Miki, J. Lee, J. Hwangbo, L. Wellhausen, V. Koltun, and M. Hutter, Learning robust perceptive locomotion for quadrupedal robots in the wild, Science robotics, vol. 7, no. 62, p. eabk2822, 2022. [14] N. Rudin, D. Hoeller, P. Reist, and M. Hutter, Learning to walk in minutes using massively parallel deep reinforcement learning, in Conference on robot learning. PMLR, 2022, pp. 91100. [15] A. Agarwal, A. Kumar, J. Malik, and D. Pathak, Legged locomotion in challenging terrains using egocentric vision, in Conference on robot learning. PMLR, 2023, pp. 403415. [16] E. Wijmans, A. Kadian, A. S. Morcos, S. Lee, I. Essa, D. Parikh, M. Savva, and D. Batra, Dd-ppo: Learning near-perfect pointgoal navigators from 2.5 billion frames, in International Conference on Learning Representations, 2019. [Online]. Available: https: //api.semanticscholar.org/CorpusID:210839350 [17] J. Lee, M. Bjelonic, A. Reske, L. Wellhausen, T. Miki, and M. Hutter, Learning robust autonomous navigation and locomotion for wheeledlegged robots, Science Robotics, vol. 9, no. 89, p. eadi9641, 2024. [18] F. Yang, P. Frivik, D. Hoeller, C. Wang, C. Cadena, and M. Hutter, Spatially-enhanced recurrent memory for long-range mapless navigation via end-to-end reinforcement learning, The International Journal of Robotics Research, p. 02783649251401926, 2025. [19] T. G. W. Lum, M. Matak, V. Makoviychuk, A. Handa, A. Allshire, T. Hermans, N. D. Ratliff, and K. V. Wyk, DextrAH-g: Pixels-toaction dexterous arm-hand grasping with geometric fabrics, in 8th Annual Conference on Robot Learning, 2024. [Online]. Available: https://openreview.net/forum?id=S2Jwb0i7HN [20] R. Singh, K. Van Wyk, P. Abbeel, J. Malik, N. Ratliff, and A. Handa, End-to-end rl improves dexterous grasping policies, arXiv preprint arXiv:2509.16434, 2025. Fig. 15: PCA visualization for real-world images collected during deployment of ANYmal ladder-climbing in the wild."
        },
        {
            "title": "DeFM has the potential",
            "content": "to streamline depth perception across wide range of robotic systems. single frozen depth encoder could replace many of the engineering-heavy pipelines still common today, such as reliance on elevation maps for perceptive locomotion or task-specific depth preprocessing stages. Our results show that frozen depth foundation models can enable fast and efficient RL training, pointing toward future where depth-based representations offer simple, general, and high-performing alternative to task-specific depthprocessing modules. While DeFM delivers strong features across domains, we observe occasional artifacts in the learned representations (Figs. 1-V, 15), known limitation of ViT architectures. These artifacts can likely be mitigated through architectural refinements such as the use of register tokens [110]. Our realworld experiments, though promising, are currently limited in terms of task diversity due to hardware constraints. Expanding to an even broader range of platforms and tasks is an important next step. Another exciting direction is scaling DeFM to depth measurements obtained from solid-state LiDARs and LiDAR range-image representations, extending its applicability beyond standard stereo/IR depth sensors. Finally, following trends demonstrated by DINOv3, increasing dataset diversity, model capacity, and training iterations is likely to yield even more powerful depth foundation models with improved generalization capabilities."
        },
        {
            "title": "ACKNOWLEDGMENT",
            "content": "The authors would like to thank Marco Trentini, Per Frivik, and Linus Kramer for support during the navigation experiments, and Ankur Handa for insightful discussions and assistance in setting up the dexterous grasping experiments. We further thank Rene Zurbrugg and Arjun Bharadwaj for helpful discussions regarding the manipulation tasks. This work was supported as part of the Swiss AI Initiative by grant from the Swiss National Supercomputing Centre (CSCS) under project ID a144 on Alps. This work was also supported by the Luxembourg National Research Fund (Ref. 18990533), and the Swiss National Science Foundation (SNSF) through projects No.200021E 229503 and No.227617. [21] H. Zhang, Z. Wu, L. Huang, S. Christen, and J. Song, Robustdexgrasp: Robust dexterous grasping of general objects, arXiv preprint arXiv:2504.05287, 2025. [22] X. Hu, K. Yang, L. Fei, and K. Wang, Acnet: Attention based network to exploit complementary features for rgbd semantic segmentation, in 2019 IEEE international conference on image processing (ICIP). IEEE, 2019, pp. 14401444. [23] K. He, X. Zhang, S. Ren, and J. Sun, Deep residual learning for image recognition, in Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 770778. [24] M. Tan and Q. Le, Efficientnet: Rethinking model scaling for convolutional neural networks, in International conference on machine learning. PMLR, 2019, pp. 61056114. [25] I. Radosavovic, R. P. Kosaraju, R. Girshick, K. He, and P. Dollar, Designing network design spaces, in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2020, pp. 10 42810 436. [26] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, Imagenet: large-scale hierarchical image database, in 2009 IEEE conference on computer vision and pattern recognition. Ieee, 2009, pp. 248255. [27] A. Dosovitskiy, An image is worth 16x16 words: Transformers for image recognition at scale, arXiv preprint arXiv:2010.11929, 2020. [28] C. Sun, A. Shrivastava, S. Singh, and A. Gupta, Revisiting unreasonable effectiveness of data in deep learning era, in Proceedings of the IEEE international conference on computer vision, 2017, pp. 843852. [29] R. Balestriero, M. Ibrahim, V. Sobal, A. Morcos, S. Shekhar, T. Goldstein, F. Bordes, A. Bardes, G. Mialon, Y. Tian et al., cookbook of self-supervised learning, arXiv preprint arXiv:2304.12210, 2023. [30] A. Jaiswal, A. R. Babu, M. Z. Zadeh, D. Banerjee, and F. Makedon, survey on contrastive self-supervised learning, Technologies, vol. 9, no. 1, p. 2, 2020. [31] J. Gui, T. Chen, J. Zhang, Q. Cao, Z. Sun, H. Luo, and D. Tao, survey on self-supervised learning: Algorithms, applications, and future trends, IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 46, no. 12, pp. 90529071, 2024. [32] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton, simple framework for contrastive learning of visual representations, in International conference on machine learning. PmLR, 2020, pp. 15971607. [33] Y. Tian, D. Krishnan, and P. Isola, Contrastive multiview coding, in Springer, 2020, pp. 776 European conference on computer vision. 794. [34] K. He, H. Fan, Y. Wu, S. Xie, and R. Girshick, Momentum contrast for unsupervised visual representation learning, in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2020, pp. 97299738. [35] J.-B. Grill, F. Strub, F. Altche, C. Tallec, P. Richemond, E. Buchatskaya, C. Doersch, B. Avila Pires, Z. Guo, M. Gheshlaghi Azar et al., Bootstrap your own latent-a new approach to self-supervised learning, Advances in neural information processing systems, vol. 33, pp. 21 27121 284, 2020. [36] X. Chen and K. He, Exploring simple siamese representation learning, in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2021, pp. 15 75015 758. [37] K. He, X. Chen, S. Xie, Y. Li, P. Dollar, and R. Girshick, Masked autoencoders are scalable vision learners, in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2022, pp. 16 00016 009. [38] J. Zhou, C. Wei, H. Wang, W. Shen, C. Xie, A. Yuille, and T. Kong, ibot: Image bert pre-training with online tokenizer, arXiv preprint arXiv:2111.07832, 2021. [39] M. Assran, Q. Duval, I. Misra, P. Bojanowski, P. Vincent, M. Rabbat, Y. LeCun, and N. Ballas, Self-supervised learning from images with joint-embedding predictive architecture, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 15 61915 629. [40] A. Bar, F. Bordes, A. Shocher, M. Assran, P. Vincent, N. Ballas, T. Darrell, A. Globerson, and Y. LeCun, Stochastic positional embeddings improve masked image modeling, in ICML, 2024. [41] X. Zhai, B. Mustafa, A. Kolesnikov, and L. Beyer, Sigmoid loss for language image pre-training, in Proceedings of the IEEE/CVF international conference on computer vision, 2023, pp. 11 97511 986. [42] J. Li, D. Li, C. Xiong, and S. Hoi, Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation, in International conference on machine learning. PMLR, 2022, pp. 12 88812 900. [43] C. Jia, Y. Yang, Y. Xia, Y.-T. Chen, Z. Parekh, H. Pham, Q. Le, Y.-H. Sung, Z. Li, and T. Duerig, Scaling up visual and vision-language representation learning with noisy text supervision, in International conference on machine learning. PMLR, 2021, pp. 49044916. [44] M. El Banani, A. Raj, K.-K. Maninis, A. Kar, Y. Li, M. Rubinstein, D. Sun, L. Guibas, J. Johnson, and V. Jampani, Probing the 3d awareness of visual foundation models, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 21 79521 806. [45] S. Wang, V. Leroy, Y. Cabon, B. Chidlovskii, and J. Revaud, Dust3r: Geometric 3d vision made easy, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 20 69720 709. [46] V. Leroy, Y. Cabon, and J. Revaud, Grounding image matching in 3d with mast3r, in European Conference on Computer Vision. Springer, 2024, pp. 7191. [47] J. Wang, M. Chen, N. Karaev, A. Vedaldi, C. Rupprecht, and D. Novotny, Vggt: Visual geometry grounded transformer, in Proceedings of the Computer Vision and Pattern Recognition Conference, 2025, pp. 52945306. [48] H. Lin, S. Chen, J. Liew, D. Y. Chen, Z. Li, G. Shi, J. Feng, and B. Kang, Depth anything 3: Recovering the visual space from any views, arXiv preprint arXiv:2511.10647, 2025. [49] M. Ranzinger, G. Heinrich, J. Kautz, and P. Molchanov, Am-radio: Agglomerative vision foundation model reduce all domains into one, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2024, pp. 12 49012 500. [50] G. Heinrich, M. Ranzinger, Hongxu, Yin, Y. Lu, J. Kautz, A. Tao, B. Catanzaro, and P. Molchanov, Radiov2.5: Improved baselines for agglomerative vision foundation models, 2024. [Online]. Available: https://arxiv.org/abs/2412.07679 [51] A. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao, S. Whitehead, A. C. Berg, W.-Y. Lo et al., Segment anything, in Proceedings of the IEEE/CVF international conference on computer vision, 2023, pp. 40154026. [52] A. Majumdar, K. Yadav, S. Arnaud, J. Ma, C. Chen, S. Silwal, A. Jain, V.-P. Berges, T. Wu, J. Vakil et al., Where are we in the search for an artificial visual cortex for embodied intelligence? Advances in Neural Information Processing Systems, vol. 36, pp. 655677, 2023. [53] K. Grauman, A. Westbury, E. Byrne, Z. Chavis, A. Furnari, R. Girdhar, J. Hamburger, H. Jiang, M. Liu, X. Liu et al., Ego4d: Around the world in 3,000 hours of egocentric video, in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2022, pp. 18 99519 012. [54] Y. J. Ma, S. Sodhani, D. Jayaraman, O. Bastani, V. Kumar, and A. Zhang, Vip: Towards universal visual reward and representation via value-implicit pre-training, arXiv preprint arXiv:2210.00030, 2022. [55] I. Radosavovic, T. Xiao, S. James, P. Abbeel, J. Malik, and T. Darrell, Real-world robot learning with masked visual pre-training, in Conference on Robot Learning. PMLR, 2023, pp. 416426. [56] Y. Seo, J. Kim, S. James, K. Lee, J. Shin, and P. Abbeel, Multi-view masked world models for visual robotic manipulation, in International Conference on Machine Learning. PMLR, 2023, pp. 30 61330 632. [57] S. Qian, K. Mo, V. Blukis, D. F. Fouhey, D. Fox, and A. Goyal, 3dmvp: 3d multiview pretraining for robotic manipulation, arXiv preprint arXiv:2406.18158, 2024. [58] C. Hou, Y. Ze, Y. Fu, Z. Gao, S. Hu, Y. Yu, S. Zhang, and H. Xu, 4d visual pre-training for robot learning, in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2025, pp. 84518461. [59] N. Di Palo and E. Johns, Dinobot: Robot manipulation via retrieval and alignment with vision foundation models, in 2024 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2024, pp. 27982805. [60] H. Yurchyk, W.-D. Chang, G. Dudek, and D. Meger, Large pre-trained models for bimanual manipulation in 3d, in 2025 IEEE-RAS 24th International Conference on Humanoid Robots (Humanoids). IEEE, 2025, pp. 11951202. [61] G. Zhou, H. Pan, Y. LeCun, and L. Pinto, Dino-wm: World models on pre-trained visual features enable zero-shot planning, arXiv preprint arXiv:2411.04983, 2024. [62] A. Loquercio, E. Kaufmann, R. Ranftl, M. Muller, V. Koltun, and in the wild, Science D. Scaramuzza, Learning high-speed flight Robotics, vol. 6, no. 59, p. eabg5810, 2021. [63] H. Yu, C. De Wagter, and G. C. E. de Croon, Depth transfer: Learning to see like simulator for real-world drone navigation, IEEE Robotics and Automation Letters, 2025. [64] C. Zhang, J. Jin, J. Frey, N. Rudin, M. Mattamala, C. Cadena, and M. Hutter, Resilient legged local navigation: Learning to traverse with compromised perception end-to-end, in 2024 IEEE International Conference on Robotics and Automation (ICRA). 3441. IEEE, 2024, pp. [65] J. He, C. Zhang, F. Jenelten, R. Grandia, M. Bacher, and M. Hutter, Attention-based map encoding for learning generalized legged locomotion, Science Robotics, vol. 10, no. 105, p. eadv3604, 2025. [66] N. Rudin, J. He, J. Aurand, and M. Hutter, Parkour in the wild: Learning general and extensible agile locomotion policy using multiexpert distillation and rl fine-tuning, arXiv preprint arXiv:2505.11164, 2025. [67] S. Luo, S. Li, R. Yu, Z. Wang, J. Wu, and Q. Zhu, Pie: Parkour with implicit-explicit learning framework for legged robots, IEEE Robotics and Automation Letters, 2024. [68] S. Kareer, N. Yokoyama, D. Batra, S. Ha, and J. Truong, Vinl: Visual navigation and locomotion over obstacles, in International Conference on Robotics and Automation (ICRA), 2023. [69] Z. Zhuang, S. Yao, and H. Zhao, Humanoid parkour learning, arXiv preprint arXiv:2406.10759, 2024. [70] J. Sun, G. Han, P. Sun, W. Zhao, J. Cao, J. Wang, Y. Guo, and Q. Zhang, Dpl: Depth-only perceptive humanoid locomotion via realistic depth synthesis and cross-attention terrain reconstruction, arXiv preprint arXiv:2510.07152, 2025. [71] M. Savva, A. Kadian, O. Maksymets, Y. Zhao, E. Wijmans, B. Jain, J. Straub, J. Liu, V. Koltun, J. Malik et al., Habitat: platform for embodied ai research, in Proceedings of the IEEE/CVF international conference on computer vision, 2019, pp. 93399347. [72] W. Wang, D. Zhu, X. Wang, Y. Hu, Y. Qiu, C. Wang, Y. Hu, A. Kapoor, and S. Scherer, Tartanair: dataset to push the limits of visual slam, in 2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 2020, pp. 49094916. [73] S. James, K. Wada, T. Laidlow, and A. J. Davison, Coarse-to-fine q-attention: Efficient learning for visual robotic manipulation via discretisation, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 13 73913 748. [74] T. Chen, M. Tippur, S. Wu, V. Kumar, E. Adelson, and P. Agrawal, Visual dexterity: In-hand reorientation of novel and complex object shapes, Science Robotics, vol. 8, no. 84, p. eadc9244, 2023. [Online]. Available: https://www.science.org/doi/abs/10.1126/ scirobotics.adc [75] J. Pitz, L. Rostel, L. Sievers, D. Burschka, and B. Bauml, Learning shape-conditioned agent for purely tactile in-hand manipulation of various objects, in 2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 2024, pp. 13 112 13 119. [76] A. Zeng, S. Song, S. Welker, J. Lee, A. Rodriguez, and T. Funkhouser, Learning synergies between pushing and grasping with self-supervised deep reinforcement learning, in 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 2018, pp. 42384245. [77] D. Kalashnikov, A. Irpan, P. Pastor, J. Ibarz, A. Herzog, E. Jang, D. Quillen, E. Holly, M. Kalakrishnan, V. Vanhoucke et al., Scalable deep reinforcement learning for vision-based robotic manipulation, in Conference on robot learning. PMLR, 2018, pp. 651673. [78] T. Lin, K. Sachdev, L. Fan, J. Malik, and Y. Zhu, Sim-to-real learning for vision-based dexterous manipulation on reinforcement humanoids, arXiv preprint arXiv:2502.20396, 2025. [79] R. Singh, A. Allshire, A. Handa, N. Ratliff, and K. Van Wyk, Dextrahrgb: Visuomotor policies to grasp anything with dexterous hands, arXiv preprint arXiv:2412.01791, 2024. [80] T. He, Z. Wang, H. Xue, Q. Ben, Z. Luo, W. Xiao, Y. Yuan, X. Da, F. Castaneda, S. Sastry et al., Viral: Visual sim-to-real at scale for humanoid loco-manipulation, arXiv preprint arXiv:2511.15200, 2025. [81] M. Mittal, P. Roth, J. Tigue, A. Richard, O. Zhang, P. Du, A. SerranoMunoz, X. Yao, R. Zurbrugg, N. Rudin et al., Isaac lab: gpuaccelerated simulation framework for multi-modal robot learning, arXiv preprint arXiv:2511.04831, 2025. [82] M. Mittal, C. Yu, Q. Yu, J. Liu, N. Rudin, D. Hoeller, J. L. Yuan, R. Singh, Y. Guo, H. Mazhar et al., Orbit: unified simulation framework for interactive robot learning environments, IEEE Robotics and Automation Letters, vol. 8, no. 6, pp. 37403747, 2023. [83] S. Tao, F. Xiang, A. Shukla, Y. Qin, X. Hinrichsen, X. Yuan, C. Bao, X. Lin, Y. Liu, T.-k. Chan et al., Maniskill3: Gpu parallelized robotics simulation and rendering for generalizable embodied ai, arXiv preprint arXiv:2410.00425, 2024. [84] A. Sablayrolles, M. Douze, C. Schmid, and H. Jegou, Spreading vectors for similarity search, arXiv preprint arXiv:1806.03198, 2018. [85] L. Yang, B. Kang, Z. Huang, Z. Zhao, X. Xu, J. Feng, and H. Zhao, Depth anything v2, Advances in Neural Information Processing Systems, vol. 37, pp. 21 87521 911, 2024. [86] J. Straub, T. Whelan, L. Ma, Y. Chen, E. Wijmans, S. Green, J. J. Engel, R. Mur-Artal, C. Ren, S. Verma et al., The replica dataset: digital replica of indoor spaces, arXiv preprint arXiv:1906.05797, 2019. [87] M. Roberts, J. Ramapuram, A. Ranjan, A. Kumar, M. A. Bautista, N. Paczan, R. Webb, and J. M. Susskind, Hypersim: photorealistic synthetic dataset for holistic indoor scene understanding, in Proceedings of the IEEE/CVF international conference on computer vision, 2021, pp. 10 91210 922. [88] M. Gilles, Y. Chen, E. Z. Zeng, Y. Wu, K. Furmans, A. Wong, and R. Rayyes, Metagraspnetv2: All-in-one dataset enabling fast and reliable robotic bin picking via object relationship reasoning and dexterous grasping, IEEE Transactions on Automation Science and Engineering, vol. 21, no. 3, pp. 23022320, 2023. [89] M. Patel, F. Yang, Y. Qiu, C. Cadena, S. Scherer, M. Hutter, and W. Wang, Tartanground: large-scale dataset for ground robot perception and navigation, in 2025 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2025, pp. 20 52420 531. [90] T. Sun, M. Segu, J. Postels, Y. Wang, L. Van Gool, B. Schiele, F. Tombari, and F. Yu, Shift: synthetic driving dataset for continuous multi-task domain adaptation, in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2022, pp. 21 371 21 382. [91] A. R. Zamir, A. Sax, W. Shen, L. J. Guibas, J. Malik, and S. Savarese, Taskonomy: Disentangling task transfer learning, in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. 37123722. [92] S. K. Ramakrishnan, A. Gokaslan, E. Wijmans, O. Maksymets, A. Clegg, J. Turner, E. Undersander, W. Galuba, A. Westbury, A. X. Chang et al., Habitat-matterport 3d dataset (hm3d): 1000 large-scale 3d environments for embodied ai, arXiv preprint arXiv:2109.08238, 2021. [93] G. Baruch, Z. Chen, A. Dehghan, T. Dimry, Y. Feigin, P. Fu, T. Gebauer, B. Joffe, D. Kurz, A. Schwartz et al., Arkitscenes: diverse real-world dataset for 3d indoor scene understanding using mobile rgb-d data, arXiv preprint arXiv:2111.08897, 2021. [94] A. Dai, A. X. Chang, M. Savva, M. Halber, T. Funkhouser, and M. Niener, Scannet: Richly-annotated 3d reconstructions of indoor scenes, in Proceedings of the IEEE conference on computer vision and pattern recognition, 2017, pp. 58285839. [95] H.-S. Fang, C. Wang, M. Gou, and C. Lu, Graspnet-1billion: largescale benchmark for general object grasping, in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2020, pp. 11 44411 453. [96] J. Xiao, A. Owens, and A. Torralba, Sun3d: database of big spaces reconstructed using sfm and object labels, in Proceedings of the IEEE international conference on computer vision, 2013, pp. 16251632. [97] A. Khazatsky, K. Pertsch, S. Nair, A. Balakrishna, S. Dasari, S. Karamcheti, S. Nasiriany, M. K. Srirama, L. Y. Chen, K. Ellis et al., Droid: large-scale in-the-wild robot manipulation dataset, arXiv preprint arXiv:2403.12945, 2024. [98] J. Frey, T. Tuna, L. F. T. Fu, C. Weibel, K. Patterson, B. Krummenacher, M. Muller, J. Nubert, M. Fallon, C. Cadena et al., Boxi: Design decisions in the context of algorithmic performance for robotics, arXiv preprint arXiv:2504.18500, 2025. [99] J. Cho, D. Min, Y. Kim, and K. Sohn, Diml/cvl rgb-d dataset: 2m rgb-d images of natural indoor and outdoor scenes, arXiv preprint arXiv:2110.11590, 2021. [100] G. Yang, X. Song, C. Huang, Z. Deng, J. Shi, and B. Zhou, Drivingstereo: large-scale dataset for stereo matching in autonomous driving scenarios, in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2019, pp. 899908. [101] H. Fu, M. Gong, C. Wang, K. Batmanghelich, and D. Tao, Deep ordinal regression network for monocular depth estimation, in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. 20022011. [102] G. Hinton, O. Vinyals, and J. Dean, Distilling the knowledge in neural network, arXiv preprint arXiv:1503.02531, 2015. [103] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, Proximal policy optimization algorithms, arXiv preprint arXiv:1707.06347, 2017. [104] M. Tan, R. Pang, and Q. V. Le, Efficientdet: Scalable and efficient the IEEE/CVF conference on object detection, in Proceedings of computer vision and pattern recognition, 2020, pp. 10 78110 790. [105] S. Song, S. P. Lichtenberg, and J. Xiao, Sun rgb-d: rgb-d scene understanding benchmark suite, in Proceedings of the IEEE conference on computer vision and pattern recognition, 2015, pp. 567576. [106] P. Neigel, J. Rambach, and D. Stricker, Offsed: Off-road semantic segmentation dataset, in Proceedings of the 16th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications VISIGRAPP-(Volume 4). SciTePress, 2021, pp. 552557. [107] D. Vogel, R. Baines, J. Church, J. Lotzer, K. Werner, and M. Hutter, Robust ladder climbing with quadrupedal robot, in 2025 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2025, pp. 72397244. [108] A. Handa, T. Whelan, J. McDonald, and A. J. Davison, benchmark for rgb-d visual odometry, 3d reconstruction and slam, in 2014 IEEE International Conference on Robotics and Automation (ICRA), 2014, pp. 15241531. [109] V. Makoviychuk, L. Wawrzyniak, Y. Guo, M. Lu, K. Storey, M. Macklin, D. Hoeller, N. Rudin, A. Allshire, A. Handa et al., Isaac gym: High performance gpu-based physics simulation for robot learning, arXiv preprint arXiv:2108.10470, 2021. [110] T. Darcet, M. Oquab, J. Mairal, and P. Bojanowski, Vision transformers need registers, arXiv preprint arXiv:2309.16588, 2023."
        }
    ],
    "affiliations": [
        "Bar-Ilan University",
        "ETH Zurich",
        "University of Zurich"
    ]
}