{
    "paper_title": "Experience-Guided Adaptation of Inference-Time Reasoning Strategies",
    "authors": [
        "Adam Stein",
        "Matthew Trager",
        "Benjamin Bowman",
        "Michael Kleinman",
        "Aditya Chattopadhyay",
        "Wei Xia",
        "Stefano Soatto"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Enabling agentic AI systems to adapt their problem-solving approaches based on post-training interactions remains a fundamental challenge. While systems that update and maintain a memory at inference time have been proposed, existing designs only steer the system by modifying textual input to a language model or agent, which means that they cannot change sampling parameters, remove tools, modify system prompts, or switch between agentic and workflow paradigms. On the other hand, systems that adapt more flexibly require offline optimization and remain static once deployed. We present Experience-Guided Reasoner (EGuR), which generates tailored strategies -- complete computational procedures involving LLM calls, tools, sampling parameters, and control logic -- dynamically at inference time based on accumulated experience. We achieve this using an LLM-based meta-strategy -- a strategy that outputs strategies -- enabling adaptation of all strategy components (prompts, sampling parameters, tool configurations, and control logic). EGuR operates through two components: a Guide generates multiple candidate strategies conditioned on the current problem and structured memory of past experiences, while a Consolidator integrates execution feedback to improve future strategy generation. This produces complete, ready-to-run strategies optimized for each problem, which can be cached, retrieved, and executed as needed without wasting resources. Across five challenging benchmarks (AIME 2025, 3-SAT, and three Big Bench Extra Hard tasks), EGuR achieves up to 14% accuracy improvements over the strongest baselines while reducing computational costs by up to 111x, with both metrics improving as the system gains experience."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 1 ] . [ 1 9 1 5 1 1 . 1 1 5 2 : r Experience-Guided Adaptation of Inference-Time Reasoning Strategies Adam Stein1*, Matthew Trager2, Benjamin Bowman2, Michael Kleinman2, Aditya Chattopadhyay2, Wei Xia2, Stefano Soatto2 1University of Pennsylvania, 2AWS AI"
        },
        {
            "title": "Abstract",
            "content": "Enabling agentic AI systems to adapt their problem-solving approaches based on post-training interactions remains fundamental challenge. While systems that update and maintain memory at inference time have been proposed, existing designs only steer the system by modifying textual input to language model or agent, which means that they cannot change sampling parameters, remove tools, modify system prompts, or switch between agentic and workflow paradigms. On the other hand, systems that adapt more flexibly require offline optimization and remain static once deployed. We present Experience-Guided Reasoner (EGUR), which generates tailored strategiescomplete computational procedures involving LLM calls, tools, sampling parameters, and control logicdynamically at inference time based on accumulated experience. We achieve this using an LLM-based meta-strategya strategy that outputs strategies enabling adaptation of all strategy components (prompts, sampling parameters, tool configurations, and control logic). EGUR operates through two components: Guide generates multiple candidate strategies conditioned on the current problem and structured memory of past experiences, while Consolidator integrates execution feedback to improve future strategy generation. This produces complete, ready-to-run strategies optimized for each problem, which can be cached, retrieved, and executed as needed without wasting resources. Across five challenging benchmarks (AIME 2025, 3-SAT, and three Big Bench Extra Hard tasks), EGUR achieves up to 14% accuracy improvements over the strongest baselines while reducing computational costs by up to 111, with both metrics improving as the system gains experience."
        },
        {
            "title": "1 Introduction",
            "content": "Modern AI systems employ sophisticated strategiescomplex computational procedures involving LLM calls, tools, and control logic, often implemented using frameworks like SGLang [1] or DSPy [2]to tackle challenging reasoning tasks. For example, systems solving International Mathematical Olympiad (IMO) problems dynamically decompose problems into subgoals, verify solutions through parallel sampling, and iteratively self-correct [3, 4]. Similarly, code generation agents switch between different prompting approaches, adjust sampling temperatures, and selectively use tools based on problem complexity [2, 5]. While these strategies are often designed to be general-purpose and highly adaptive to inputsparticularly agents, which consist of loops of LLM and tool executionthey generally remain static at inference time, unable to learn from experience. For example, without persistent memory, an AI system for mathematical reasoning will apply the same expensive multi-step decomposition every time it encounters problem it has solved in the past (potentially even failing to solve it again), and will repeatedly make identical mistakes on problems it has failed to solve before. To address this problem, different methods for adapting AI systems have been proposed. Approaches such as Dynamic Cheatsheet [6] and Buffer of Thoughts [7] maintain memory across problems, but this memory is used only for textual steering of an LLM or agent. As such, they cannot change sampling parameters, add or remove tools, modify control logic, or cache successful computational procedures for direct *Work done during an internship at AWS AI. Correspondence to: Matthew Trager <mttrager@amazon.com>. reuse. On the other hand, offline methods like ADAS [8] can adapt strategies much more flexibly, but require expensive training phases and remain static once deployed, unable to continue learning from new experiences. The core challenge is to adapt the computational structure of strategies from experience, to continually improve both accuracy and efficiency. To address this challenge, we present Experience-Guided Reasoner (EGUR), system designed to generate strategies dynamically at inference time. Our approach uses an LLM-based meta-strategya strategy that outputs strategiesrather than steering existing strategies at runtime, it proposes new strategies. This enables generation of complete, tailored computational procedures for each problem that can be easily conditioned using memory store and enables simple and effective caching of successful strategies for reuse. EGUR operates through two main components: Guide that generates multiple candidate strategies conditioned on the current problem and structured memory of past experiences, and Consolidator that processes execution outcomes to improve future strategy generation. By generating and comparing multiple strategies per problem, the system learns the relative effectiveness of different strategies, leading to continual improvements in both accuracy and computational efficiency. Our contributions are: We introduce system that generates strategies dynamically at inference time based on accumulated experience, enabling adaptation of all strategy components including prompts, sampling parameters, tools, and control logic (Section 3). We formalize strategies as compositions of stateful processes which map inputs to outputs while also updating state. This provides unified representation for diverse reasoning paradigms (workflows, agents, tool-use systems) that supports compositional cost tracking and execution tracing (Section 2.1). We demonstrate that generating multiple strategies per problem and comparing their effectiveness enables continual improvement in both accuracy and efficiency (Figure 5). Across five challenging benchmarks (AIME 2025, 3-SAT, and three Big Bench Extra Hard tasks), EGUR achieves up to 14% accuracy improvements (EGUR vs. Mem0 on 3-SAT) while reducing computational costs by up to 111x (EGUR vs. DC on Object Counting), with both metrics improving as the system gains experience (Figure 4)."
        },
        {
            "title": "2 Strategies For AI Systems",
            "content": "In this section, we formalize strategies as compositions of stateful processesfunctions taking inputs and state and producing an output and an updated stateproviding unified way to describe any strategy in terms of its components. This yields natural taxonomy of common strategies and enables us to describe feedback mechanisms as state-modification methods."
        },
        {
            "title": "2.1 Representing Strategies",
            "content": "We use the term strategy for specification of the computational procedure that an AI system applies to inputs to produce outputs, capturing LLM inference calls, tool invocations, and control flow decisions. For example, the Chain-of-Thought (CoT) strategy involves single LLM call with specific prompting, while CodeAct iteratively calls an LLM and executes generated code until termination condition is met. An adaptive strategy engages in sequence of interactions, or episodes, and updates its internal state based on past outcomes. To enable adaptive strategies, we need representation that explicitly captures both what can be adapted and how state evolves across episodes. Existing frameworks like DSPy [2] and SGLang [1] implement strategies as Python code without explicitly marking adaptable components. TextGrad [5] and Trace [10] mark components for optimization, but are designed for offline training and lack explicit state management for continual learning during inference. We introduce compositional formalism that unifies adaptable components with state management, enabling meta-strategies to generate and refine strategies from accumulated experience. 2 Figure 1: Comparison of existing experience-based adaptation methods (A and B) to our approach in (C) which dynamically produces compiled strategy based on the current query and memory at inference time. Methods which augment existing strategies with state, such as Dynamic Cheatsheet[6] and Mem0[9], are depicted in (A) and methods such as ADAS [8] and OPTO [10] which optimize strategies offline are shown in (B). Our method shown in (C) uses state (which can contain useful compiled strategies) during inference to guide the system in producing effective strategies for each query, unlike existing methods which cannot adapt their strategies per-query. We model strategies as compositions of stateful processes. process is function that takes an input and state (e.g., conversation history, sampling parameters), produces an output, and updates the state. This explicit state management clarifies what components can be adapted (prompts, temperature, available Σ == : (A Σ) (B Σ) where and are input and tools). Formally, we write process type as output domains and Σ is the state space. Strategies are built compositionally using the grammar shown in Figure 2. Sequential composition (S1; S2) chains processes, parallel composition (S1 S2) executes concurrently, conditionals enable branching, and recursion enables iteration. For instance, CodeAct (shown in Figure 2b) composes an LLM call with code execution tool recursively, iterating until the LLM output contains no code. The LLM call process maintains conversation history in its state, while the code execution process maintains the execution environment. This compositional structure is key to our approach. By explicitly representing all components (prompts, sampling parameters, tools, control flow) and their state dependencies, we enable meta-strategy to generate new strategies by composing different components. For instance, our meta-strategy can adapt CodeAct by changing the LLMs prompt, adjusting temperature, or removing the code interpreter entirely for nonalgorithmic problems. This framework also allows us to formalize important properties of strategies: computational cost and intermediate traces (Appendix B.4), formal semantics (Appendix B.2), and structural characteristics (discussed in the next section)."
        },
        {
            "title": "2.2 The Strategy Landscape",
            "content": "We organize strategies into several categories based on their structure. strategy is dynamic if it contains conditionals, meaning the control flow can change at runtime based on the input. dynamic strategy that is recursive (uses recfun as shown in Figure 2a) is called an agent. Strategies that do not use recursion are called workflows since their runtime behavior is less dynamic, and workflow that is not dynamic is called pipeline. Strategies can also use parallelization to solve several tasks concurrently or to produce multiple responses at once, which can improve accuracy and reliability [11]. Table 1 shows breakdown of five commonly used strategies based on their use of parallel compute, their dynamic or agentic nature, and their tool utilization. description of the five strategies is in Appendix B.6. Our classification differs from common definitions that do not classify Eval-Opt as agentic and lack clear way to determine when strategy is agentic [12]. Figure 3 shows that different strategies result in significantly different behavior in terms of cost and accuracy. Among strategies we consider, the Code strategy is best for 3-SAT and Word Sorting, but worst for AIME and Movie Recommendation. Even among strategies with comparable accuracy, cost can vary dramatically: Eval-Opt generally achieves similar accuracy to Self-Consistency but often at half the cost. Notably, more expressive strategies do not guarantee better performance: while agentic strategies can the3 ::= baseP S1; S2 base process sequential composition where Σ1== and S2 : Σ2== 2 are views of Σ 1, Σ S1 : and Σ S1 S2 if S1 then S2 else parallel composition where Σ1== B1 and S2 : S1 : = (B1, B2) and Σ Σ2== B2 and 1, Σ 2 are disjoint views of Σ conditional where S1 : Σ2== B, and S3 : 1, Σ S2 : and Σ 2, Σ 3 are views of Σ Σ1== Bool, Σ3== recfun : S1 recursive definition of process where Σ1== which can internally use 1 is view of Σ. S1 : and Σ 1 recfun CodeAct : CallLLM; 2 if ContainsCode then (ExecCode; CodeAct) else return 3 5 4 (a) Grammar for strategies where : Σ == B. (b) Example: CodeAct Strategy Figure 2: High-level grammar for strategies as compositions of base processes (left) with an example of how CodeAct is represented in this grammar (right). further formalization is provided in Appendix where we define how to additionally construct processes from pure functions as well as processes for explicitly accessing and updating the state, and some useful base processes such as return. oretically emulate simpler strategies through their conditional and recursive structure, they may fail to select the right behavior in practice and, even when successful, often incur substantially higher computational costs than using the simpler strategy directly (for example, CodeAct is not the most effective strategy in Figure 3, despite being the most general). These observations highlight fundamental trade-off where practitioners must balance solution quality against computational expense, often termed overthinking. 2."
        },
        {
            "title": "Incorporating Feedback to Strategies",
            "content": "The observations above motivate systems that can adapt their strategy selection to each problem. Ideally, such systems should improve with experience: using information from past problem-solving attempts to make better strategy choices on future problems. This requires maintaining state during inference that captures which strategies work well for which types of problems. Existing work addresses two separate problems but not both simultaneously. Some work focuses on maintaining state during testing but is limited in what the state can influence. These methods add state to strategies compiled before inference, so they are limited to steering strategies by changing their input [6, 9]. Other work adapts general strategies to experience but requires expensive offline optimization to compile an optimized strategy that is then left stateless during inference [8, 10]. Figure 1 illustrates how trainingbased optimization methods (b) compare to state-based adaptation approaches (a) and our approach (c). The success of offline strategy optimization in modifying entire strategies motivates method that compiles useful strategies on the fly by learning from experience. This is the approach we take and introduce in the following section."
        },
        {
            "title": "3 Experience-Guided Reasoner (EGUR)",
            "content": "In this section, we introduce system that generates tailored strategies dynamically at inference time by learning from accumulated experience. Our approach centers on two key components: Guide that gen4 Figure 3: Comparison of strategy performance across tasks for Claude 3.7 Sonnet. Strategies closer to the top-left corner are best for the task in terms of accuracy and cost. The optimal strategy differs significantly across tasks. For example, Code excels on 3-SAT and Word Sorting but performs poorly on Movie Recommendation and AIME. Descriptions of the above strategies are provided in Appendix B.6. Table 1: Commonly used strategies in terms of their used design patterns and tools. CI stands for code interpreter. Pipelines do not use conditionals, workflows are any strategy using conditionals, and agents are strategies using recursion."
        },
        {
            "title": "Parallelization Conditionals Recursion",
            "content": "Tools (base processes) Pipelines CoT [13] Self-Consistency [11] Workflows Code [14] Agents Eval-Opt [15] CodeAct [16] LLM LLM LLM, CI LLM LLM, CI erates complete, problem-specific strategies based on past experience, and Consolidator that processes execution outcomes to improve future strategy generation."
        },
        {
            "title": "3.1 General Design",
            "content": "The Guide generates complete strategy specifications tailored to each problem based on the current context and past experiences. Rather than making decisions step-by-step like traditional agents, it produces complete computational procedure upfrontspecifying LLM calls, sampling parameters, tools, and control flow. The Consolidator processes execution outcomes, including reasoning traces and verifier feedback, maintaining structured memory that improves strategy generation. Formally, these components have types: Guide : Str (cid:16) Σ =="
        },
        {
            "title": "Str",
            "content": "σ== Str (cid:17) Consolidator : List[Exp] Σ == Σ where Exp represents (question, answer, trace, cost, feedback) tuples. The state space is divided into Σ (between-episode memory maintained by Consolidator) and σ (within-episode memory used during strategy execution). The Guide has state Σ, takes string as input and outputs strategy as defined in Section 2.1 (which itself uses state σ and takes string as input and outputs string). The Consolidator updates Σ based on execution experiences. The complete EGUR process is shown in Algorithm 1, and we also provide an equivalent definition using our strategy language in Appendix B.5. 5 Algorithm 1 Experience-Guided Reasoner Require: Question q, context Σ, and exploration factor k. Ensure: Answer and updated context Σ. 1: {S1, . . . , Sk} Guide(q, Σ, k) 2: = [] 3: for = 1 to in parallel do 4: 5: // initialize experience collection // initialize strategy state // execute strategy Si // generate candidate strategies // get verifier feedback // collect experience tuple σ null (ai, ti, ci) Execute(Si, q, σ) fi verifier(q, ai) + [(q, ai, ti, ci, fi)] 6: 7: 8: end for 9: Σ Consolidator(e, Σ) 10: a1 11: return a, Σ // use answer from first strategy // update context with experiences"
        },
        {
            "title": "3.2 Strategy Selection with the Guide",
            "content": "The Guide generates candidate strategies for each problem by conditioning on the query and relevant experiences from the context Σ. This design provides some key benefits: observability of the systems behavior with complete strategy specifications; exploration of the space of possible strategies by generating multiple candidates; and comparison of the relative performance of different approaches for future strategy generation. The exploration factor balances computational cost with strategy diversitysetting = 1 minimizes overhead, while larger values enable discovery of better strategies at increased cost. We invoke the Guide times on problem to obtain candidate strategies {S1, . . . , Sk}, which we execute to obtain performance measures. Similar to Group Relative Policy Optimization (GRPO) [17], we use relative comparisons within each group of strategies: the Consolidator records which strategies perform better relative to alternatives on similar problems, enabling the Guide to improve over time. In practice, the Guide is implemented as an LLM call that produces strategy code, which is then parsed from the response and executed (see Appendix F.1 for the prompt)."
        },
        {
            "title": "3.3 Feedback Integration with the Consolidator",
            "content": "The Consolidator maintains structured memory to guide future strategy generation. Rather than accumulating raw experiences, it performs selective abstraction to maximize information utility while preventing memory bloat. The Consolidator processes experiences = [(q1, a1, t1, c1, f1), . . . , (qk, ak, tk, ck, fk)] from executed strategies, where each tuple contains the query, answer, execution trace, computational cost, and verifier feedback. The context Σ maintains two key components: Strategy Library that stores successful strategies with their problem characteristics for reuse and template generation, and General Notes that capture high-level insights about strategy effectiveness, common failure patterns, useful techniques, and problem-solving heuristics. The entire context is passed directly to the Guide for each problem, enabling it to perform retrieval and synthesis in-context rather than requiring external retrieval mechanisms. To prevent unbounded memory growth, the Consolidator implements selective retention policies, prioritizing recent experiences and frequently accessed patterns while removing outdated information to maintain responsiveness to evolving problem distributions while preserving valuable learned knowledge. In practice, the Consolidator is implemented as an LLM call that produces memory insertions and deletions which are parsed and applied (see Appendix F.2 for the prompt)."
        },
        {
            "title": "4 Experiments",
            "content": "We evaluate EGUR across diverse reasoning tasks to answer four key research questions: (RQ1) Does it outperform baselines in accuracy and cost? (RQ2) Does dynamic strategy generation improve over existing 6 stateful methods? (RQ3) Is comparative strategy evaluation important for continual improvements? (RQ4) Does EGUR learn novel and useful strategies from experience?"
        },
        {
            "title": "4.1 Experimental Setup",
            "content": "Datasets. We evaluate on five datasets: AIME (2022-2024 for training, 2025 held-out), 3-SAT (5-40 variables, 400 training/40 test samples), and three BBEH [18] tasks (movie recommendation, word sorting, object counting, 100 training/100 test each). We process training data in shuffled batches of ten samples, evaluating all ten in parallel then sequentially updating based on each samples experience. Models. Claude 3.7 Sonnet, Qwen3-Next-80B-A3B-Thinking, and GPT-OSS-120B. Baselines. CodeAct [16] (stateless code-interpreter agent), Dynamic Cheatsheet [6] (CodeAct with stringbased memory appended to inputs), and Mem0 (CodeAct with vector database memory). We modify Dynamic Cheatsheet to incorporate verifier feedback during updates. Verifiers. For all tasks except for 3-SAT, we use ground-truth based verifiers that compare the final answer against the correct solution. For 3-SAT, we verify the proposed assignment satisfies all clauses using satisfiability checker. The verifier provides only binary feedback (correct/incorrect) which is passed to the Consolidator. Implementation details. All LLM calls use temperature 0 and maximum token budget of 20,000 tokens. For Claude 3.7 Sonnet, we enable extended thinking mode with maximum thinking budget of 10,000 tokens; when thinking mode is enabled, Claudes temperature is automatically set to 1.0. These settings apply to all strategy executions, Guide calls, and Consolidator calls across all methods. With EGUR, the strategy can include LLM calls without thinking mode or with non-zero temperature. Evaluation. We report prequential accuracy (accuracy before updating on each sample) and cumulative cost for strategy execution (excluding system feedback and update costs). Continual updates use one seed; held-out evaluation uses three seeds. Costs are in USD based on API pricing."
        },
        {
            "title": "4.2 RQ1: Does EGUR outperform baselines in accuracy and cost?",
            "content": "Figure 4 shows EGURs performance on held-out evaluation sets as training progresses. Two key trends emerge: accuracy increases with more experience, and cost decreases as EGUR learns more efficient strategies. Notably, Dynamic Cheatsheets costs are significantly higher than other approaches (often exceeding $1.00 per sample) due to the increasing context length from accumulating memory, and the underlying strategy taking many turns before outputting final answer. In contrast, EGUR maintains low and decreasing costs through selective memory management. Mem0 shows minimal cost reduction and modest accuracy improvements, suggesting limited adaptation capability. Table 3 provides detailed prequential accuracy and cost results across all datasets and models. EGUR consistently achieves the best accuracy-cost trade-offs: on Claude 3.7 Sonnet, EGUR-5 achieves 96.0% accuracy on 3-SAT at $0.152 cost versus CodeActs 77.0% at $0.257 and Dynamic Cheatsheets 89.9% at $76.353. Similar patterns hold across models and tasks."
        },
        {
            "title": "4.3 RQ2: Does dynamic strategy generation improve over existing stateful strategies?",
            "content": "The key distinction between EGUR and existing stateful methods is architectural: EGUR generates complete strategy specification for each problem before execution, while existing methods steer dynamic strategies at runtime by modifying their inputs. Concretely, Mem0 and Dynamic Cheatsheet prepend memory to fixed CodeAct agents inputs, influencing its behavior indirectly. In contrast, EGUR produces entirely new strategy specificationsincluding prompts, sampling parameters (temperature, max tokens), tool availability, and control flowtailored to each problem. 7 Figure 4: Evolution of accuracy and cost on held-out evaluation sets as training progresses for Claude 3.7 Sonnet. EGUR-5 consistently improves accuracy while reducing cost with more experience. Cost is shown up to $1.0 for visualization; Dynamic Cheatsheet (DC) typically exceeds this threshold, reaching $9.95, $2.26, $2.88, $4.32, and $7.16 per sample after training on 3-SAT, AIME, Movie Rec., Word Sort., and Object Count., respectively. The complete results for Claude 3.7 Sonnet, GPT-OSS-120B, and Qwen3-Next-80BA3B-Thinking are included in Appendix C. This architectural difference has profound implications. Methods that steer via input modification cannot remove tools, change sampling parameters, or switch between agentic and workflow paradigms. EGUR is significantly more flexible: for instance, it learns to completely remove the code interpreter tool when it harms performance, adjust temperature from 0.7 to 0.0 for deterministic tasks, and replace multi-turn agentic strategies with single-call workflows when appropriate. The results in Figure 4 reflect these differences. Mem0 achieves minimal cost reduction despite maintaining memory, as it cannot fundamentally change the underlying strategys computational structure. Dynamic Cheatsheets costs grow unboundedly as accumulated memory increases context length in every call. EGUR achieves superior accuracy while reducing costs by 2-111 through complete strategy adaptation."
        },
        {
            "title": "4.4 RQ3: Is comparative strategy evaluation important for continual improvements?",
            "content": "We ablate the exploration level (strategies generated per problem) to assess the importance of comparative evaluation. Figure 5 shows results for Claude 3.7 Sonnet. EGUR-ZS (zero-shot, no memory updates) serves as stateless baseline, EGUR-1 generates one strategy per problem and learns from absolute feedback, while EGUR-5 generates five strategies enabling comparative evaluation. Three findings emerge. First, memory is essential: EGUR-1 consistently outperforms EGUR-ZS, showing that learning from experience helps even without comparison. Second, comparative evaluation provides substantial additional benefits: EGUR-5 outperforms EGUR-1 on most tasks, with large improvements on 3-SAT and object counting. Third, cost decreases with higher because comparative evaluation helps discover efficient strategies faster."
        },
        {
            "title": "4.5 RQ4: Does EGUR learn novel and useful strategies from experience?",
            "content": "We analyze strategies generated before and after training to identify learned adaptations. EGUR exhibits several consistent patterns: for CodeAct strategies, it learns to specify allowed libraries, include useful code 8 Figure 5: Ablation of exploration level in EGUR for Claude 3.7 Sonnet. Higher exploration levels (more strategies per problem) generally improve both accuracy and cost-efficiency by enabling comparative evaluation of strategy effectiveness. snippets, and add error handling; more generally, it increases specificity when general approaches fail but simplifies when appropriate. We also find that the code interpreter tool sometimes harms performance while increasing cost. For BBEH object counting, intuition suggests CodeAct would excel (problems involve adding many numbers), but EGUR converges on single LLM call with detailed instructions which proves more accurate and substantially cheaper. The learned strategy includes specific guidance for parsing text, categorizing items, and handling quantity changes. The strategy is shown in Appendix D. Similarly, for BBEH word sorting, EGUR learns to distinguish algorithmic sorting problems (which benefit from Pythons sort) from reasoning problems (identifying logical mistakes in explanations). For the latter, it adopts CoT with verifier-based fallback rather than CodeAct. These findings show EGUR learns meaningful heuristics for when to use tools versus LLM-based reasoning, when to increase versus decrease computational investment, and how to tailor strategies through prompts to problem characteristics. Additional examples appear in Appendix D."
        },
        {
            "title": "5 Related Work",
            "content": "Prompting strategies. Prompting methods including Chain-of-Thought (CoT) [13], Self-Consistency [11], Program-of-Thoughts (PoT) [14], and many other approaches [1922] elicit intermediate reasoning traces but rely on fixed, non-adaptive scaffolds. More recent approaches like CodeAct [16], Self-Discover [6], and Meta-Prompting [23] dynamically compose solution solving methods per instance, but still do not leverage past experience or adapt their strategies over time. These methods are thus dynamic at inference but remain stateless across instances. Prompt adaptation from inference-time state or training. Another line of work adapts prompts or parameters within fixed agent workflows. DSPy [2] and TextGrad [5] are systems to optimize prompts or even the inputs to fixed strategy structures using training dataset with labels or an answer verifier, while Dynamic Cheatsheet [6] and Buffer of Thoughts [7] accumulate and retrieve artifacts during inference for instance-level adaptation. Concurrent to this work, training-free GRPO [24] was proposed as an effective 9 method for online prompt optimization which is similar to our GRPO inspired approach to producing effective memory updates from the Consolidator, and Agentic Context Engineering [25] improves over Dynamic Cheatsheet by providing more structure for memory updates. Systems such as DSPy are highly general and allow for many different algorithms for offline prompt optimization [2628]. Other methods such as Per-Instance Program Synthesis (PIPS) [29], model routing [3032], and proprietary products like GPT-5 [33] rely on trained routers to select between fixed set of strategies on per-instance basis. These approaches achieve stronger performance through experience but are limited in their adaptability based on only modifying the input to fixed strategy or selecting between fixed number of strategies. Full strategy adaptation. Beyond prompt tuning, ADAS [8] treats agent design as search problem over Python programs defining agents, enabling strategy-level adaptation. However, ADAS and similar approaches [3437] operate offline and at the task level, requiring multiple episodes to converge. In contrast, our method introduces an online adaptive meta-agent that performs dynamic, instance-level adaptation and test-time strategy search, combining the benefits of prior work on dynamic strategies, memory-based learners, and automated design in unified framework."
        },
        {
            "title": "6 Conclusion",
            "content": "We have described system, EGUR, which generates complete strategiescomputational procedures involving LLM calls, tools, and control logicdynamically at inference time based on accumulated experience. Specifically, Guide component generates problem-specific strategy specifications conditioned on past experience, while Consolidator maintains structured memory of strategy effectiveness. Unlike existing adaptive systems that use memory for textual steering of an LLM or agent, our approach has much greater flexibility, enabling adaptation of all strategy components (prompts, sampling parameters, tool configurations, and control logic). Across five benchmarks, EGUR achieves up to 14% relative accuracy improvements while reducing computational costs by up to 111x, with both metrics improving as the system gains experience. Our analysis reveals EGUR learns meaningful heuristics: when to deploy expensive agentic strategies versus lightweight workflows, when tools help versus harm, and how to tailor computational investment to problem difficulty. Our approach has some limitations that suggest directions for future work. First, EGUR uses ground truth feedback from verifiers to learn effectively. Exploring whether this feedback can be replaced with weaker signalssuch as LLM-based evaluationis an important direction. Second, the effectiveness of the system hinges on the Guides zero-shot strategy generation capabilities, which may be suboptimal for unfamiliar problem types. In this case, it may be necessary to train or optimize the Guide through reinforcement learning or other training methods. Similarly, the Consolidator relies on an LLM to manage memory, which may not optimally balance memory size against information utility, and may benefit from meta-learning approaches for more effective memory management."
        },
        {
            "title": "References",
            "content": "[1] Lianmin Zheng, Liangsheng Yin, Zhiqiang Xie, Chuyue Livia Sun, Jeff Huang, Cody Hao Yu, Shiyi Cao, Christos Kozyrakis, Ion Stoica, Joseph Gonzalez, et al. Sglang: Efficient execution of structured language model programs. Advances in neural information processing systems, 37:6255762583, 2024. [2] Omar Khattab, Arnav Singhvi, Paridhi Maheshwari, Zhiyuan Zhang, Keshav Santhanam, Sri Vardhamanan A, Saiful Haq, Ashutosh Sharma, Thomas T. Joshi, Hanna Moazam, Heather Miller, Matei Zaharia, and Christopher Potts. DSPy: Compiling declarative language model calls into stateIn The Twelfth International Conference on Learning Representations, 2024. URL of-the-art pipelines. https://openreview.net/forum?id=sY5N0zY5Od. [3] Yichen Huang and Lin Yang. Gemini 2.5 pro capable of winning gold at imo 2025. arXiv preprint arXiv:2507.15855, 2025. 10 [4] Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. [5] Mert Yuksekgonul, Federico Bianchi, Joseph Boen, Sheng Liu, Zhi Huang, Carlos Guestrin, and James Zou. Textgrad: Automatic differentiation via text. arXiv preprint arXiv:2406.07496, 2024. [6] Mirac Suzgun, Mert Yuksekgonul, Federico Bianchi, Dan Jurafsky, and James Zou. Dynamic cheatsheet: Test-time learning with adaptive memory. arXiv preprint arXiv:2504.07952, 2025. [7] Ling Yang, Zhaochen Yu, Tianjun Zhang, Shiyi Cao, Minkai Xu, Wentao Zhang, Joseph Gonzalez, and Bin Cui. Buffer of thoughts: Thought-augmented reasoning with large language models. Advances in Neural Information Processing Systems, 37:113519113544, 2024. [8] Shengran Hu, Cong Lu, and Jeff Clune. Automated design of agentic systems. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id= t9U3LW7JVX. [9] Prateek Chhikara, Dev Khant, Saket Aryan, Taranjeet Singh, and Deshraj Yadav. Mem0: Building production-ready ai agents with scalable long-term memory. arXiv preprint arXiv:2504.19413, 2025. [10] Ching-An Cheng, Allen Nie, and Adith Swaminathan. Trace is the next autodiff: Generative optimization with rich feedback, execution traces, and llms. Advances in Neural Information Processing Systems, 37:7159671642, 2024. [11] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language In The Eleventh International Conference on Learning Representations, 2023. URL https: models. //openreview.net/forum?id=1PL1NIMMrw. [12] Erik Schluntz and Barry Zhang. Building effective ai agents. https://www.anthropic.com/ engineering/building-effective-agents, December 2024. Accessed: 2025-09-30. [13] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. [14] Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W. Cohen. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. Transactions on Machine Learning Research, 2023. ISSN 2835-8856. URL https://openreview.net/forum?id=YfZ4ZPt8zd. [15] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement with selffeedback. Advances in Neural Information Processing Systems, 36:4653446594, 2023. [16] Xingyao Wang, Yangyi Chen, Lifan Yuan, Yizhe Zhang, Yunzhu Li, Hao Peng, and Heng Ji. Executable code actions elicit better llm agents. In Forty-first International Conference on Machine Learning, 2024. [17] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [18] Mehran Kazemi, Bahare Fatemi, Hritik Bansal, John Palowitch, Chrysovalantis Anastasiou, Sanket Vaibhav Mehta, Lalit Jain, Virginia Aglietti, Disha Jindal, Peter Chen, et al. Big-bench extra hard. arXiv preprint arXiv:2502.19187, 2025. [19] Qing Lyu, Shreya Havaldar, Adam Stein, Li Zhang, Delip Rao, Eric Wong, Marianna Apidianaki, and Chris Callison-Burch. Faithful chain-of-thought reasoning. In The 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics (IJCNLP-AACL 2023), 2023. 11 [20] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=WE_vluYUL-X. [21] Denny Zhou, Nathanael Scharli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, and Ed H. Chi. Least-to-most prompting enables complex reasoning in large language models. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=WZH7099tgfM. [22] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. Advances in neural information processing systems, 36:1180911822, 2023. [23] Mirac Suzgun and Adam Tauman Kalai. Meta-prompting: Enhancing language models with taskagnostic scaffolding. arXiv preprint arXiv:2401.12954, 2024. [24] Yuzheng Cai, Siqi Cai, Yuchen Shi, Zihan Xu, Lichao Chen, Yulei Qin, Xiaoyu Tan, Gang Li, Zongyi Li, Haojia Lin, et al. Training-free group relative policy optimization. arXiv preprint arXiv:2510.08191, 2025. [25] Qizheng Zhang, Changran Hu, Shubhangi Upasani, Boyuan Ma, Fenglu Hong, Vamsidhar Kamanuru, Jay Rainton, Chen Wu, Mengmeng Ji, Hanchen Li, et al. Agentic context engineering: Evolving contexts for self-improving language models. arXiv preprint arXiv:2510.04618, 2025. [26] Lakshya Agrawal, Shangyin Tan, Dilara Soylu, Noah Ziems, Rishi Khare, Krista Opsahl-Ong, Arnav Singhvi, Herumb Shandilya, Michael Ryan, Meng Jiang, et al. Gepa: Reflective prompt evolution can outperform reinforcement learning. arXiv preprint arXiv:2507.19457, 2025. [27] Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc Le, Denny Zhou, and Xinyun Chen. Large language models as optimizers. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=Bb4VGOWELI. [28] Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy Ba. Large language models are human-level prompt engineers. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=92gvk82DE-. [29] Adam Stein, Neelay Velingker, Mayur Naik, and Eric Wong. Once upon an input: Reasoning via per-instance program synthesis. In The Thirty-ninth Annual Conference on Neural Information Processing Systems, 2025. URL https://openreview.net/forum?id=XsQNqRdcdh. [30] Lingjiao Chen, Matei Zaharia, and James Zou. FrugalGPT: How to use large language models while reducing cost and improving performance. Transactions on Machine Learning Research, 2024. ISSN 28358856. URL https://openreview.net/forum?id=cSimKw5p6R. [31] Ruochen Wang, Sohyun An, Minhao Cheng, Tianyi Zhou, Sung Ju Hwang, and Cho-Jui Hsieh. One prompt is not enough: Automated construction of mixture-of-expert prompts. In Forty-first International Conference on Machine Learning, 2024. URL https://openreview.net/forum?id=edHLN40DWu. [32] Isaac Ong, Amjad Almahairi, Vincent Wu, Wei-Lin Chiang, Tianhao Wu, Joseph E. Gonzalez, Waleed Kadous, and Ion Stoica. RouteLLM: Learning to route LLMs from preference data. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview. net/forum?id=8sSqNntaMr. [33] OpenAI. Introducing gpt-5. https://openai.com/index/introducing-gpt-5/, Aug 2025. [34] Jiayi Zhang, Jinyu Xiang, Zhaoyang Yu, Fengwei Teng, Xiong-Hui Chen, Jiaqi Chen, Mingchen Zhuge, Xin Cheng, Sirui Hong, Jinlin Wang, Bingnan Zheng, Bang Liu, Yuyu Luo, and Chenglin Wu. AFlow: Automating agentic workflow generation. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=z5uVAKwmjf. 12 Table 2: Taxonomy of related work by S@T=Stateful at test, Adapted=What is adapted in response to the state, input, or training samples, Training-free=If the method does not require an initial training stage Method S@T Adapted Training-free Stateless strategies (human designed) CoT Self-Consistency PoT (non-iterative) CodeAct [16] Adapts prompts from state/training Self-Discover [6] DSPy [2] TextGrad [5] Buffer of Thoughts [7] Dynamic Cheatsheet [6] Prompt Prompts Prompts, Inputs Prompt Prompt Full strategy adaptation FlowReasoner [38] ADAS [8] EGUR Strategy Strategy Strategy [35] Mingchen Zhuge, Wenyi Wang, Louis Kirsch, Francesco Faccio, Dmitrii Khizbullin, and urgen Schmidhuber. GPTSwarm: Language agents as optimizable graphs. In Forty-first International Conference on Machine Learning, 2024. URL https://openreview.net/forum?id=uTC9AFXIhg. [36] Zelong Li, Shuyuan Xu, Kai Mei, Wenyue Hua, Balaji Rama, Om Raheja, Hao Wang, He Zhu, and Yongfeng Zhang. Autoflow: Automated workflow generation for large language model agents. arXiv preprint arXiv:2407.12821, 2024. [37] Guibin Zhang, Kaijie Chen, Guancheng Wan, Heng Chang, Hong Cheng, Kun Wang, Shuyue Hu, and Lei Bai. Evoflow: Evolving diverse agentic workflows on the fly. arXiv preprint arXiv:2502.07373, 2025. [38] Hongcheng Gao, Yue Liu, Yufei He, Longxu Dou, Chao Du, Zhijie Deng, Bryan Hooi, Min Lin, and Tianyu Pang. Flowreasoner: Reinforcing query-level meta-agents. arXiv preprint arXiv:2504.15257, 2025."
        },
        {
            "title": "A Related Work Taxonomy",
            "content": "A taxonomy of the related work is shown in Table 2."
        },
        {
            "title": "B Strategies",
            "content": "B.1 Syntax The syntax of strategies is separated into values, expressions and programs."
        },
        {
            "title": "Values",
            "content": "v ::= str float true false null [v (, v)] {str : (, str : v)} Values are the typical Python values including strings, floats, booleans, the None value (written null) as well as lists and dictionaries."
        },
        {
            "title": "Expressions",
            "content": "e ::= e[e] lambda x. e(e) ++ + Expressions consist of either variable x, value, or various operations of expressions including list indexing, list append ++, dictionary append , float arithmethic (+, , , ), lambda functions, and function application."
        },
        {
            "title": "Programs",
            "content": "p ::= base proc return pure get put p; if then else fix p. Programs are then either base process, return statement (which directly returns the input), an expression lifted to program using pure, get which returns the current state of the program, put of an expression which updates the state with the expression, or sequential or parallel composition of programs, conditional of programs, or fixpoint of program. B.2 Semantics We now provide denotational semantics for the strategy language. First, program in this language is stateful process mapping between two sets: Proc(A, B, Σ) : (A Σ) (B Σ) where Σ is the state space of the process. Intuitively, process can be thought of as mapping from to which has access to some state Σ. To simplify how we compose processes, we use the state monad: State(A) : Σ (A Σ) and then rewrite the process type as Proc(A, B, Σ) : State(B) which can be thought of as function which maps an input in to computation which when given state will produce an output in and an updated state. The choice of the state space Σ determines how processes share information when composed as well as allows for tracking information such as execution traces, cost, and strategy structure. For instance, we use Σ = Trace Cost where is the current conversation log of any model calls, is the execution state for any code execution, Trace is the current execution trace, and Cost is the current execution cost of the strategy."
        },
        {
            "title": "The program meaning function",
            "content": "cesses. (cid:74) (cid:75) (cid:75) (cid:74) is now mapping from the syntactic forms defined above to pro- : Program Proc(A, B, Σ) 14 base proc (cid:74) (cid:74) (cid:75) return (cid:75) (cid:74) pure get (cid:74) put (cid:74) p1; p2 (cid:74) (cid:75) (cid:75) (cid:75) (cid:75) = base proc, = λ a. λc. (a, c), = λ a. λc. ( = λ a. λc. (c, c), = λ a. λc. (a, = λ a. λc. let (b, c1) (a), c), ), (cid:74) (cid:74) (cid:75) (cid:75) if p1 then p2 else p3 (cid:74) p1 p2 (cid:75) (cid:74) (cid:75) in (b)(c1), p2 (cid:74) (a)(c) = λ a. λc. let (b1, c1) in ((b1, b2), (c1, c2)), (s) = λ a. λc. let (b, c1) p2 (cid:75) (cid:74) p3 (cid:74) (cid:75) (cid:0)p (cid:55) (a, c1), (a, c1), p2 = lfp p1(cid:55)p in (cid:40) (cid:75) (cid:74) (cid:75) fix p1. (cid:74) (cid:75) (a)(c) (cid:75) p1 (cid:74) if = true if = false (cid:1). , (cid:74) (a)(c) p1 (cid:75) (a)(c) and (b2, c2) (cid:75) (cid:74) (a)(c), p2 (cid:75) (cid:74) B.3 Guide and Consolidator The Guide is strategy which takes query as input and outputs strategy for solving the problem, as well as has access to state which it can use to store information between queries. Therefore, the Guide is just higher-order strategy, or stateful mapping from inputs to strategies and its type is the following: Guide(A, B, Σ) : Proc(A, Proc(A, B, σ), Σ). The memory updater can also be written as strategy which takes the current experience, the current running state, and outputs the updated state: Consolidator(Exp, Σ) : Proc(Exp, Σ, Σ). B.4 Strategy Cost We define the cost of strategy recursively based on the costs of the base processes defined by cP(s) R0, the cost of running on input s. Cost : Term R0 {}, Cost(t, s) = cP(s), Cost(t1, s) + if = P, (cid:2)Cost(t2, s)(cid:3), if = t1; t2, (s) t1 (cid:74) (cid:75) Cost(t1, s) + Cost(t2, s), (cid:40) Cost(t1, s), Cost(t2, s), b(s) = true, b(s) = false, if = t1 t2, if = if then t1 else t2, Cost(cid:0)t[X (cid:55) fix X. t], s(cid:1), if = fix X. (least fixed point). B.5 EGUR Implemented as Strategy We show how EGUR is represented as strategy in Algorithm 2 where we make sequential and parallel composition of processes expliclit. 15 Algorithm 2 Experience-Guided Reasoner Require: Question q, context Σ, and exploration factor k. Ensure: Answer and updated context Σ. 1: update (fun (q, Σ): Σ + {question : q}); 2: Guidek; 3: (recfun ExecuteStrategies: 4: 5: if IsEmpty then return else (GetFirst; Trace; Verify) (PopFirst; ExecuteStrategies) // execute all strategies in parallel // generate strategies conditioned on the question and context // store the question in context 6: 7: ); 8: update (fun (e, Σ): Σ + {answer : e[0][0]}); 9: Consolidator 10: get answer // update the context from the experiences // store the answer from the first strategy 1 (CallLLM CallLLM 2 CallLLM CallLLM 3 CallLLM); 4 MajorityVote 1 CallLLM; 2 (if ContainsCode then ExecCode 3 4 else return) 1 CallLLM (a) CoT (b) Self-Consistency (c) Code 1 recfun Eval-Opt : 3 4 5 CallOptLLM; (if EvalLLM then return else Eval-Opt) (d) Eval-Opt Figure 6: Implementation of CoT, Self-Consistency, Code, and Eval-Opt as strategies B.6 Common Strategies In this section, we describe the five strategies shown in Table 1. We also show the implementations of the strategies in our strategy language in Figure 6. The CoT strategy is single LLM inference call without any tools, the Code strategy is single round of code writing and executing the code with an interpreter to produce the answer, the Eval-Opt strategy is an evaluator-optimizer loop where the optimizer produces candidate solutions and the evaluator critiques the solutions, the Self-Consistency strategy samples 10 solutions from the CoT strategy and then returns the majority answer, and the CodeAct strategy uses multiple rounds of code generation and execution to solve the problem, and is prototypical of an agentic workflow."
        },
        {
            "title": "C Full results",
            "content": "The prequential evaluation results for all models is included in Table 3."
        },
        {
            "title": "Full results for all methods on all three models and five holdout datasets across four evenly spaced",
            "content": "checkpoints during training are shown in Table 4, Table 5, Table 6, Table 7, and Table 8."
        },
        {
            "title": "D Example Generated Strategies",
            "content": "The following shows an example of strategy generated with GPT-OSS-120B to solve sarcasm task (Sarc Triples) from BBEH. We can see that this strategy includes prompt tailored to the specific problem, majority voting, and modification of the temperature parameter to the called model."
        },
        {
            "title": "Generated Strategy",
            "content": "1 def strategy(x): 2 \"\"\" Detect sarcasm in three Reddit replies. 3 16 Table 3: Prequential Accuracy (%) and Cost ($) on each dataset. EGUR consistently achieves the best accuracy-cost trade-offs, with bold indicating best accuracy and underline indicating second-best. Oursk denotes EGUR with exploration level k. Model Method 3-SAT AIME Movie Rec. Word Sort Object Count Acc Cost Acc Cost Acc Cost Acc Cost Acc Cost Claude Qwen3 GPT CodeAct CodeAct+Mem0 DynamicCheatsheet EGUR-5 CodeAct CodeAct+Mem0 DynamicCheatsheet EGUR-3 CodeAct CodeAct+Mem0 EGUR-3 0.770 0.860 0.899 0.960 0.875 0.875 0.125 0.695 0.655 0.640 0. 0.257 0.325 76.353 0.152 0.032 0.037 0.570 0.005 0.236 0.219 0.100 0.611 0.589 0.400 0.589 0.622 0.656 0.467 0.767 0.778 0.800 0. 0.478 0.548 8.615 0.292 0.010 0.011 0.124 0.006 0.073 0.078 0.095 0.570 0.540 0.840 0.700 0.680 0.750 0.550 0.650 0.550 0.530 0. 0.137 0.141 9.152 0.067 0.002 0.003 0.205 0.002 0.091 0.061 0.064 0.770 0.760 0.700 0.800 0.700 0.680 0.450 0.650 0.780 0.780 0. 0.067 0.085 8.231 0.073 0.003 0.004 0.171 0.001 0.087 0.129 0.091 0.800 0.870 0.870 0.960 0.400 0.360 0.180 0.540 0.690 0.790 0. 0.098 0.174 24.895 0.075 0.017 0.019 0.469 0.005 0.047 0.059 0.090 Table 4: Accuracy (%) across training progress for different methods on 3-SAT and AIME datasets. Values shown as mean standard deviation across seeds. Claude uses EGuR-5, GPT and Qwen use EGuR-3. Bold indicates best performance within each model, underlined indicates second best within each model. Model Method 3-SAT AIME 0% 33% 66% 100% 0% 33% 66% 100% Claude-3.5-Sonnet GPT-OSS-120B Qwen3-Next-80B CodeAct EGuR-ZS Mem0 DC EGuR-5 CodeAct EGuR-ZS Mem0 DC EGuR-3 CodeAct EGuR-ZS Mem0 EGuR-3 79.44.1 88.12.7 74.22.4 64.112.1 85.82.4 88.85.7 83.12.7 91.71.2 10.86.6 85.82. 63.87.4 80.04.7 75.00.0 72.50.0 79.44.1 88.12.7 79.26.6 72.38.8 94.24.7 88.85.7 83.12.7 86.75.9 6.72.4 80.89.6 63.87.4 80.04.7 52.50.0 90.00.0 79.44.1 88.12.7 78.34.2 73.32.7 100.00.0 88.85.7 83.12.7 91.72.4 19.24.2 35.06. 63.87.4 80.04.7 57.50.0 75.00.0 79.44.1 88.12.7 86.75.1 78.88.0 99.21.2 88.85.7 83.12.7 84.22.4 13.34.2 69.212.3 63.87.4 80.04.7 67.50.0 45.00.0 48.35.0 54.24.3 53.30.0 41.13.1 52.23.1 79.66.8 65.83.6 76.63.4 45.64.2 65.68. 74.24.9 78.87.5 66.70.0 56.70.0 48.35.0 54.24.3 58.96.8 38.94.2 48.94.2 79.66.8 65.83.6 60.06.7 44.45.7 75.64.2 74.24.9 78.87.5 76.70.0 72.40.0 48.35.0 54.24.3 55.61.6 28.94.2 57.86.3 79.66.8 65.83.6 69.19.1 53.37.2 73.37. 74.24.9 78.87.5 80.00.0 60.00.0 48.35.0 54.24.3 46.78.2 34.44.2 55.61.6 79.66.8 65.83.6 79.416.1 44.44.2 76.74.7 74.24.9 78.87.5 76.70.0 55.20.0 4 6 7 8 9 10 12 13 14 15 16 18 19 20 21 22 24 25 26 27 28 Repeatedly query the model, extract the list after \"FINAL ANSWER:\", and return the majority-voted label string. \"\"\" import re from collections import Counter # Prompt that enforces step-by-step reasoning and strict final answer format base_prompt = ( \"You are given three (post, reply) pairs. For each reply decide whether it is \" \"sarcastic (1) or not sarcastic (0). Think carefully about each pair, then output \" \"the three labels in order as comma-separated list. Write the list after line \" \"that starts exactly with 'FINAL ANSWER:'.n\" f\"Problem: { x} \" ) collected_answers = [] # Number of independent attempts (adjustable; 5 balances cost and reliability) for _ in range(5): # First generation out = M(base_prompt, temperature=0.5, max_tokens=2000)[0] # Try to extract the answer ans = extract_final_answer(out) if not ans: # Ask the model to repeat with the required format 17 Table 5: Accuracy (%) across training progress for different methods on Movie Recommendation, Word Sorting, and Object Counting datasets. Values shown as mean standard deviation across seeds. Claude uses EGuR-5, GPT and Qwen use EGuR-3. Bold indicates best performance within each model, underlined indicates second best within each model. Model Method Movie Rec. Word Sort. Object Count. 0% 33% 66% 100% 0% 33% 66% 100% 0% 33% 66% 100% Claude-3.5-Sonnet GPT-OSS-120B Qwen3-Next-80B CodeAct EGuR-ZS Mem0 DC EGuRCodeAct EGuR-ZS Mem0 DC EGuR-3 CodeAct EGuR-ZS Mem0 EGuR-3 57.21.1 64.01.2 53.74.6 59.73.1 66.73.7 72.83.3 75.03.2 73.30.9 73.71.9 62.33.4 46.51.5 39.21.7 47.00.0 41.00.0 57.21.1 64.01.2 56.01.4 80.62.0 80.00. 72.83.3 75.03.2 72.02.9 57.01.4 60.31.7 46.51.5 39.21.7 45.00.0 32.00.0 57.21.1 64.01.2 55.76.6 79.02.5 75.32.6 72.83.3 75.03.2 72.01.6 52.30.5 62.33.1 46.51.5 39.21.7 47.00.0 28.00.0 57.21.1 64.01.2 51.02.9 77.72.1 81.31. 72.83.3 75.03.2 71.02.4 59.70.5 74.72.5 46.51.5 39.21.7 43.00.0 40.00.0 75.80.4 76.81.3 76.70.9 64.83.4 75.71.7 66.01.7 64.84.0 64.03.6 47.30.5 63.32.6 75.52.1 64.42.3 77.00.0 65.00.0 75.80.4 76.81.3 74.71.7 68.82.5 78.00. 66.01.7 64.84.0 63.32.4 37.74.5 63.02.8 75.52.1 64.42.3 75.00.0 43.00.0 75.80.4 76.81.3 73.72.5 71.50.6 76.02.2 66.01.7 64.84.0 63.72.1 45.02.4 65.01.6 75.52.1 64.42.3 77.00.0 68.00.0 75.80.4 76.81.3 74.31.2 68.30.9 78.70. 66.01.7 64.84.0 63.71.7 39.32.5 64.03.6 75.52.1 64.42.3 78.00.0 64.00.0 85.24.8 88.80.8 86.30.9 70.73.3 87.32.6 39.57.1 29.51.8 35.32.5 13.04.5 37.32.6 75.53.2 74.94.5 74.00.0 60.00.0 85.24.8 88.80.8 86.32.5 85.31.5 97.70. 39.57.1 29.51.8 41.05.4 15.71.7 48.34.8 75.53.2 74.94.5 63.00.0 44.00.0 85.24.8 88.80.8 85.73.3 88.71.7 98.00.8 39.57.1 29.51.8 41.02.9 21.72.1 57.31.2 75.53.2 74.94.5 70.00.0 38.00.0 85.24.8 88.80.8 85.31.7 90.12.3 99.30. 39.57.1 29.51.8 38.02.2 8.75.2 39.04.5 75.53.2 74.94.5 77.00.0 37.00.0 Table 6: Cost ($) across training progress for different methods on 3-SAT and AIME datasets. Values shown as mean standard deviation across seeds. Claude uses EGuR-5, GPT and Qwen use EGuR-3. Bold indicates lowest cost within each model, underlined indicates second lowest within each model. Model Method 3-SAT AIME 0% 33% 66% 100% 0% 33% 66% 100% Claude-3.5-Sonnet GPT-OSS-120B Qwen3-Next-80B CodeAct EGuR-ZS Mem0 DC EGuR-5 CodeAct EGuR-ZS Mem0 DC EGuR-3 CodeAct EGuR-ZS Mem0 EGuR-3 0.3980.049 0.2620.016 0.3860.029 8.9691.020 0.2460.026 0.0360.007 0.0050.001 0.0270.002 0.0190.002 0.0040.000 0.2510.024 0.1370.013 0.2120.000 0.0890. 0.3980.049 0.2620.016 0.3710.040 10.0561.431 0.1430.052 0.0360.007 0.0050.001 0.0310.005 0.0140.001 0.0030.000 0.2510.024 0.1370.013 0.2080.000 0.0970.000 0.3980.049 0.2620.016 0.3490.059 10.5721.095 0.1530.007 0.0360.007 0.0050.001 0.0380.009 0.0170.001 0.0040.001 0.2510.024 0.1370.013 0.2230.000 0.1350. 0.3980.049 0.2620.016 0.3960.011 9.9511.944 0.1500.010 0.0360.007 0.0050.001 0.0350.004 0.0210.002 0.0040.001 0.2510.024 0.1370.013 0.2280.000 0.0960.000 0.8140.112 0.4670.046 0.8800.075 1.1330.149 0.4270.020 0.0040.000 0.0040.001 0.0060.002 0.0180.000 0.0030.000 0.1040.015 0.1200.023 0.1050.000 0.0570. 0.8140.112 0.4670.046 0.7180.107 1.3710.395 0.4220.049 0.0040.000 0.0040.001 0.0100.001 0.0170.001 0.0060.002 0.1040.015 0.1200.023 0.1150.000 0.0380.000 0.8140.112 0.4670.046 0.8370.091 2.1460.109 0.4180.060 0.0040.000 0.0040.001 0.0070.001 0.0280.001 0.0040.001 0.1040.015 0.1200.023 0.0800.000 0.0410. 0.8140.112 0.4670.046 0.7330.124 2.2590.280 0.2760.037 0.0040.000 0.0040.001 0.0110.003 0.0320.001 0.0030.001 0.1040.015 0.1200.023 0.0890.000 0.0540.000 30 31 33 34 35 36 37 39 40 41 42 43 45 46 47 clarification = ( \"Please respond again, making sure to include line that starts exactly with \" \"'FINAL ANSWER:' followed by the three labels separated by commas.\" ) out = M(clarification + \"nPrevious output:n\" + out, temperature=0, max_tokens=2000)[0] ans = extract_final_answer(out) if ans: # Normalise whitespace and store collected_answers.append(ans.strip()) if not collected_answers: return \"No answer found.\" # Majority vote over the collected answer strings most_common = Counter(collected_answers).most_common(1)[0][0] return most_common For Claude 3.7 Sonnet, we show generated strategies before and after training on the BBEH Object Counting task. The first strategy below, from before training answers the particular problem it was generated for incorrectly and cost $0.166 while the following strategy answers the same problem correctly and only costs $0.062. 18 Table 7: Cost ($) across training progress for different methods on Movie Recommendation and Word Sorting datasets. Values shown as mean standard deviation across seeds. Claude uses EGuR-5, GPT and Qwen use EGuR-3. Bold indicates lowest cost within each model, underlined indicates second lowest within each model. Model Method Movie Rec. Word Sort. 0% 33% 66% 100% 0% 33% 66% 100% Claude-3.5-Sonnet GPT-OSS-120B Qwen3-Next-80B CodeAct EGuR-ZS Mem0 DC EGuR-5 CodeAct EGuR-ZS Mem0 DC EGuR-3 CodeAct EGuR-ZS Mem0 EGuR-3 0.1420.004 0.0970.005 0.1610.004 0.8260.011 0.0870.001 0.0030.000 0.0020.000 0.0030.000 0.0380.001 0.0010. 0.0960.003 0.0730.005 0.0710.000 0.0470.000 0.1420.004 0.0970.005 0.1560.005 2.5250.047 0.0660.002 0.0030.000 0.0020.000 0.0030.000 0.0930.001 0.0010.000 0.0960.003 0.0730.005 0.0660.000 0.0490.000 0.1420.004 0.0970.005 0.1660.007 2.7100.034 0.0610.001 0.0030.000 0.0020.000 0.0030.000 0.0990.000 0.0010. 0.0960.003 0.0730.005 0.0670.000 0.0430.000 0.1420.004 0.0970.005 0.1660.005 2.8850.014 0.0490.001 0.0030.000 0.0020.000 0.0030.000 0.0690.001 0.0050.000 0.0960.003 0.0730.005 0.0630.000 0.0560.000 0.0600.007 0.0670.002 0.0820.005 1.2140.047 0.0770.003 0.0030.000 0.0010.000 0.0040.000 0.0380.001 0.0010. 0.0830.003 0.0780.009 0.1000.000 0.0680.000 0.0600.007 0.0670.002 0.0840.004 2.8220.133 0.0710.001 0.0030.000 0.0010.000 0.0040.000 0.0590.001 0.0020.000 0.0830.003 0.0780.009 0.1180.000 0.0520.000 0.0600.007 0.0670.002 0.0850.006 5.4260.066 0.0590.000 0.0030.000 0.0010.000 0.0040.000 0.0820.002 0.0010. 0.0830.003 0.0780.009 0.1020.000 0.0530.000 0.0600.007 0.0670.002 0.0780.001 4.3220.068 0.0720.005 0.0030.000 0.0010.000 0.0040.000 0.0520.001 0.0010.000 0.0830.003 0.0780.009 0.1100.000 0.0830.000 Table 8: Cost ($) across training progress for different methods on Object Counting dataset. Values shown as mean standard deviation across seeds. Claude uses EGuR-5, GPT and Qwen use EGuR-3. Bold indicates lowest cost within each model, underlined indicates second lowest within each model. Model Method Object Count. 0% 33% 66% 100% Claude-3.5-Sonnet GPT-OSS-120B Qwen3-Next-80B CodeAct EGuR-ZS Mem0 DC EGuR-5 CodeAct EGuR-ZS Mem0 DC EGuR-3 CodeAct EGuR-ZS Mem0 EGuR0.0950.005 0.2340.020 0.1250.001 1.5390.046 0.2420.003 0.0200.001 0.0060.001 0.0200.003 0.2000.009 0.0050.000 0.0490.001 0.0860.010 0.0550.000 0.0490.000 0.0950.005 0.2340.020 0.1270.005 8.1550.295 0.0600.001 0.0200.001 0.0060.001 0.0220.001 0.2500.019 0.0040.000 0.0490.001 0.0860.010 0.0570.000 0.0320. 0.0950.005 0.2340.020 0.1250.005 8.8030.931 0.0630.001 0.0200.001 0.0060.001 0.0210.003 0.1940.010 0.0060.000 0.0490.001 0.0860.010 0.0640.000 0.0290.000 0.0950.005 0.2340.020 0.1280.009 7.1560.397 0.0640.001 0.0200.001 0.0060.001 0.0190.000 0.1280.048 0.0060.001 0.0490.001 0.0860.010 0.0650.000 0.0250. BBEH Object Counting strategy from Claude 3.7 Sonnet before training SYSTEM_PROMPT = \"\"\"You are an expert at solving counting and categorization 1 async def strategy(x): 2 3 problems with Python code. 4 5 Task: Solve the question by iteratively writing and executing Python code. 6 7 Rules: 8 - Reply with code inside single ```python ...``` block. The code will be executed 9 10 - Read the execution result, and then decide on how to proceed next. You must iterate 11 12 - When ready, return the solution exactly as: FINAL ANSWER: <answer> 13 (You may print this from code or write it directly outside the code block.) for you; anything you print will be returned to you next round. until you have solved the problem. 14 15 Constraints: 16 - Code must be self-contained, use only the Python standard library, and avoid 17 network/file I/O and pip installs. 18 19 Approach: 20 1. Parse the text to identify all items the narrator (\"I\") has 21 2. Categorize each item as either musical instrument or an animal/insect 22 3. Count the total number of each category 23 4. Calculate the absolute difference between the two totals\"\"\" 24 25 26 PARSE_ERROR_PROMPT = \"Parsing error: couldn't find ```python ... ``` code block or line starting with 'FINAL ANSWER:' in the model output. 19 27 28 29 30 32 33 34 35 36 38 39 40 41 42 44 45 46 47 48 50 51 52 53 54 56 57 58 59 60 62 63 64 65 66 68 69 70 71 72 74 75 76 Please either:n1) Provide valid Python within ```python ... ``` ORn2) Reply directly with line exactly: FINAL ANSWER: <answer> with no trailing text.\" max_rounds = 20 messages = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}] messages.append({\"role\": \"user\", \"content\": f\"Problem: { x} \"}) out = await model_call(messages, temperature=0.7, thinking=True, max_tokens=20000) messages.append({\"role\": \"assistant\", \"content\": out}) execution_state = {} for in range(max_rounds): # First check for any output code to execute blocks = extract_python_code(out) if blocks: block = \"nn\".join(blocks) exec_res, execution_state = python_exec(block, execution_state) # Check for final answer in execution output = extract_final_answer(exec_res) if m: return m.strip() # No final answer, so take another turn of model output messages.append({ \"role\": \"user\", \"content\": f\"The output of executing the generated Python code is:n{ exec_res} \"}) out = await model_call( messages, temperature=0.7, thinking=True, max_tokens=20000) messages.append({\"role\": \"assistant\", \"content\": out}) continue # If no code, then check for final answer = extract_final_answer(out) if m: return m.strip() messages.append({\"role\": \"user\", \"content\": PARSE_ERROR_PROMPT}) out = await model_call(messages, temperature=0.7, thinking=True, max_tokens=20000) messages.append({\"role\": \"assistant\", \"content\": out}) messages.append({ \"role\": \"user\", \"content\": \"Now output the final answer on single line as 'FINAL ANSWER: answer' with no trailing text.\"}) out = await model_call(messages, temperature=0.7, thinking=True, max_tokens=20000) = extract_final_answer(out) if m: return m.strip() else: raise Exception(\"No answer found.\") BBEH Object Counting strategy from Claude 3.7 Sonnet after training 1 async def strategy(x): 2 PROMPT = \"\"\"Solve the following problem step by step: 3 4 1. First, carefully read through the text and identify all items that \"I\" 5 (the narrator) have. Ignore items that belong to other people (grandmother, friend, sister, brother, father, mother, aunt, grandfather, cousin, uncle). 7 8 2. For each item that belongs to \"I\", categorize it as either: 9 - An animal/insect (e.g., ducks, rabbits, walrus, hens, moose, wasps, bees, chameleons, newts, vultures, otter, tigers, shrimps, elks, baboons, yaks, boa snakes, anteaters, magpies, fish, narwhals, chimpanzees, antelopes) - musical instrument (e.g., harps, panpipes, balafons, marimbas, electric guitars, recorders, clavichord, electric pianos, tanpuras, ouds, 20 10 11 12 13 15 16 22 23 24 bass guitars) - Other items (cars like nissan pathfinder, kia niro, honda accord, nissan frintier, toyota highlander, hyundai elantra, mazda2, buick enclave, hyundai santa fe) which we will ignore for this problem 17 18 3. Create table with columns: Item, Category (Animal/Insect, Musical Instrument, 19 20 4. For items with stories about changing quantities, ALWAYS use the FINAL quantity 21 mentioned. For example: - \"initially had X, then lost Y, but then got Z, but then lost W, Other), Quantity. so at the end am left with V\" Count as - \"initially had X, then also got making it total of Z, then got making it total of A, but then lost of them so eventually am left with C\" Count as 26 27 5. Count the total number of animals/insects that \"I\" have. 28 6. Count the total number of musical instruments that \"I\" have. 29 7. Calculate the absolute difference between these two totals. 30 31 Be methodical and organize your work clearly. After completing your analysis, 32 write the final answer on single line as 'FINAL ANSWER: <answer>' with no trailing 33 text. 34 35 Problem: {x} \"\"\" 36 37 39 40 41 42 43 45 46 47 48 49 51 52 53 54 55 57 58 59 60 61 63 64 MISSING_ANSWER_FEEDBACK = \"Your last response did not include line exactly of the form 'FINAL ANSWER: <answer>'. Please try again to output final answer, responding with exactly one line 'FINAL ANSWER: <answer>'.\" messages = [{\"role\": \"user\", \"content\": PROMPT.format(x=x)}] for _ in range(5): output = await model_call( messages, temperature=0.7, thinking=True, max_tokens=20000) # Find FINAL ANSWER ans = extract_final_answer(output) if ans: return ans messages.append({\"role\": \"assistant\", \"content\": output}) messages.append({\"role\": \"user\", \"content\": MISSING_ANSWER_FEEDBACK}) # One last attempt with more direct prompt messages.append({ \"role\": \"user\", \"content\": \"Based on your analysis, what is the absolute difference between the total number of animals/insects and musical instruments that the narrator has? Provide only the numerical answer in the format 'FINAL ANSWER: <number>'.\"}) output = await model_call( messages, temperature=0.7, thinking=True, max_tokens=20000) ans = extract_final_answer(output) if ans: return ans else: raise Exception(\"No answer found after multiple attempts\")"
        },
        {
            "title": "E Example Learned Context",
            "content": "The final memory resulting from EGUR-5 with Claude 3.7 Sonnet on BBEH Movie Recommendations is shown below. The memory consists of several memory entries and single entry containing the (task, best strategy) pair which is used for solving future problems."
        },
        {
            "title": "Learned Context",
            "content": "1 <memory_entry-1> 2 When analyzing similarity between items for group appeal (movies, books, etc.), consider these key (cid:44) factors: 3 1. Era/time period consistency (items from the same era tend to have more similar appeal) 21 4 2. Language/accessibility (foreign language items create significant outliers) 5 3. Target demographics (mainstream vs. niche appeal) 6 4. Critical reception and popular appeal 7 5. Production values and stylistic approaches 8 9 For movie similarity specifically, the most reliable indicators of similar group appeal are: 10 - Same language and era 11 - Similar accessibility factors 12 - Comparable mainstream vs. niche positioning 13 - Consistent tone and pacing 14 </memory_entry-1> 15 16 <memory_entry-2> 17 For similarity analysis problems, the Chain of Thought (CoT) approach with deep thinking tends to be more (cid:44) (cid:44) cost-effective than more complex strategies like CodeAct or Evaluator-Optimizer when the task involves subjective judgment rather than algorithmic calculation. (cid:44) 18 19 The CoT approach with well-structured prompt that guides the analysis through specific criteria (genres, audience, reception, etc.) and asks for explicit rating/ranking of options provides sufficient structure for the model to make accurate comparisons without requiring the overhead of code execution or multiple model calls. (cid:44) (cid:44) 20 </memory_entry-2> 21 22 [...] 23 24 <memory_entry-5> 25 Task: Analyzing which group of movies has the most similar audience appeal 26 Best Strategy: 27 <strategy> 28 async def strategy(x): 29 PROMPT = f\"\"\"You are an expert film critic tasked with analyzing which group of movies would have the (cid:44) most similar appeal to the same audience. Think step by step and provide your final answer on single line as 'FINAL ANSWER: <option letter>' with no trailing text. (cid:44) 30 31 When analyzing similarity between movies for group appeal, consider these key factors: 32 1. Era/time period consistency (movies from the same era tend to have more similar appeal) 33 2. Language/accessibility (foreign language films create significant outliers) 34 3. Target demographics (mainstream vs. niche appeal) 35 4. Critical reception and popular appeal 36 5. Production values and stylistic approaches 37 6. Psychological appeal patterns (puzzle-solving vs. emotional engagement vs. escapism) 38 7. Viewing experience requirements (active vs. passive engagement) 39 8. Narrative complexity and accessibility 40 9. Emotional responses elicited (fear, humor, intellectual stimulation, etc.) 41 42 For movie similarity specifically, the most reliable indicators of similar group appeal are: 43 - Same language and era 44 - Similar accessibility factors 45 - Comparable mainstream vs. niche positioning 46 - Consistent tone and pacing 47 48 Significant outliers that reduce group cohesion include: 49 - Animated shorts among feature films 50 - Family/children's content mixed with adult-oriented films 51 - Critically panned films grouped with acclaimed ones 52 - Comedic/light entertainment mixed with serious dramas 53 - Foreign language films mixed with English-language films 54 - Adult/erotic content mixed with general audience films 55 56 For each option: 57 1. Analyze each movie in the group individually 58 2. Compare each movie against every other movie in the group 59 3. Identify specific outliers that would reduce group cohesion 60 4. Rate the overall cohesion on 1-10 scale 61 5. Explain your reasoning thoroughly 62 63 Problem: {x} 22 65 Think through each option carefully and systematically. 66 \"\"\" 67 MISSING_ANSWER_FEEDBACK = \"Your last response did not include line exactly of the form 'FINAL (cid:44) ANSWER: <option letter>'. Please try again to output final answer, responding with exactly one line 'FINAL ANSWER: <option letter>'.\" (cid:44) 69 70 71 72 73 75 76 77 messages = [{\"role\": \"user\", \"content\": PROMPT}] for _ in range(5): output = await model_call(messages, temperature=1.0, thinking=True, max_tokens=20000) # Find FINAL ANSWER ans = extract_final_answer(output) if ans: return ans messages.append({\"role\": \"assistant\", \"content\": output}) messages.append({\"role\": \"user\", \"content\": MISSING_ANSWER_FEEDBACK}) raise Exception(\"No answer found\") 78 79 </strategy> 80 </memory_entry-5>"
        },
        {
            "title": "F Prompts",
            "content": "We include the complete prompts for the Guide and Consolidator in the following sections. These prompts define the two strategies defining EGUR. F.1 Guide The prompt for the Guide is included below."
        },
        {
            "title": "Guide Prompt",
            "content": "1 You are designing **reasoning strategy**, *not* solving the target problem. 2 3 reasoning strategy is **Python function** that describes the design of an LLM workflow/agent to solve (cid:44) particular problem. 4 It should **not** directly contain or compute the final answer to the target problem. Instead, it should (cid:44) provide the **workflow/environment** for an LLM system to arrive at the correct answer. 5 The strategy doesn't need to describe the explicit steps to take to solve the problem and can instead just (cid:44) set up an agent which can figure out how to solve the problem itself (see the CodeAct example below). 6 Once you output runnable strategy, you will be given the reasoning trace from the strategy execution and (cid:44) you can optionally update any saved memory. 7 8 **Definition:** 9 reasoning strategy is an async function `strategy(x)` where: 10 - `x` (string) is the problem statement. 11 - The function can contain calls to the following functions: 12 (cid:44) - `model_call: (list[dict], temperature, thinking, max_tokens) -> Awaitable[str]`: an async call to (cid:44) Python markdown blocks using the regex r\"```(?:pythonpy)s*(.* ?)s*```\". model which takes conversation history (list of dict) and some keyword generation parameters as input and outputs the response. You may use asyncio.gather to perform concurrent calls to the model. (cid:44) - `extract_python_code: str -> Optional[List[str]]`: parsing function to extract all code from (cid:44) - `extract_final_answer: str -> Optional[str]`: parsing function to extract the answer for text in (cid:44) - `python_exec: (str, dict) -> (str, dict)`: function which takes the code string and state dict as input and returns the output of executing the code as well as the updated state dict. This can (cid:44) only execute python code using standard libraries, so it does not support any imports which must be installed with pip. the format of \"FINAL ANSWER: answer\". (cid:44) (cid:44) 13 14 16 - The function can use global variables `MEMORY_ENTRY_K` which are set to the string containing the entire (cid:44) body of <memory_entry-k></memory_entry-k>. 17 - The strategy function must return the final answer as string. 18 19 **Important:** 23 20 - **Do not** attempt to answer the problem yourself or write code without LLM calls which solves the (cid:44) problem. 21 - **Do not** implement the `python_exec` function or the `extract_final_answer` function as it will be (cid:44) provided if you need it. 22 - Your output must define the `strategy` Python function in <strategy></strategy> block**. 23 - The function must be executable without changes and cannot import any non-standard libraries. 24 - If including code in prompt, be careful about including quotes within string. Use different types of (cid:44) quotes for the outer and inner quotes or explicitly escape all the inner quotes. 25 - The strategy should return the final answer only with no other text. 26 - You should reference any available memory below to help design the best strategy. 27 28 **Strategy motifs** 29 Some commonly used elements which can be composed together to create the reasoning strategy: 30 - chain of thought (by asking to think step by step) 31 - code interpreter (using python_exec) 32 - iterative feedback to improve answers or correct any possible runtime errors (feeding the output from (cid:44) the model back to itself in case of error) 33 - majority voting (sample multiple times with temperature and take the most common answer) 34 - deep thinking mode (enabled with the thinking=True flag to `model_call` which automatically sets (cid:44) temperature to 1.0. Temperature MUST BE greater than 0 (preferably 0.6-1.0) when thinking=True.) 35 - problem decomposition (first split to subproblems, then solve each subproblem either with separate LLM (cid:44) call or all at once) 36 - evaluator and optimizer (keep trying to solve until the proposed answer is deemed correct by (cid:44) verification method which can be implemented as tool call or LLM call) 37 38 **Example implementations of some strategies:** 39 <example> 40 CoT with deep thinking (a single prompt to an LLM to solve the problem with retries if answer is missing) 41 <strategy> 42 ```python 43 async def strategy(x): 44 'FINAL ANSWER: <answer>' with no trailing text:nProblem: {x} \" PROMPT = \"Solve the following problem step by step and write the final answer on single line as (cid:44) MISSING_ANSWER_FEEDBACK = \"Your last response did not include line exactly of the form 'FINAL (cid:44) ANSWER: <answer>'. Please try again to output final answer, responding with exactly one line 'FINAL ANSWER: <answer>'.\" (cid:44) messages = [{\"role\": \"user\", \"content\": PROMPT.format(x=x)}] for _ in range(10): you can set thinking=False to turn off deep thinking for simpler problems output = await model_call(messages, temperature=1.0, thinking=True, max_tokens=20000) # Note that (cid:44) # Find FINAL ANSWER ans = extract_final_answer(output) if ans: return ans messages.append({\"role\": \"assistant\", \"content\": output}) messages.append({\"role\": \"user\", \"content\": MISSING_ANSWER_FEEDBACK}) raise Exception(\"No answer found\") 56 57 ``` 58 </strategy> 59 </example> 60 61 <example> 62 CodeAct (loop of writing code and executing the code until answer is reached) 63 <strategy> 64 ```python 65 async def strategy(x): 66 SYSTEM_PROMPT = \"\"\"You are an expert at solving problems with Python code. 67 68 Task: Solve the question by iteratively writing and executing Python code. 69 70 Rules 71 - Reply with code inside single ```python ...``` block. The code will be executed for you; anything you (cid:44) print will be returned to you next round. 72 - Read the execution result, and then decide on how to procede next. You must iterate until you have (cid:44) solved the problem. 73 - When ready, return the solution exactly as: FINAL ANSWER: <answer> 24 45 47 48 49 50 51 53 54 55 74 (You may print this from code or write it directly outside the code block.) 75 76 Constraints 77 - Code must be self-contained, use only the Python standard library, and avoid network/file I/O and pip (cid:44) installs.\"\"\" PARSE_ERROR_PROMPT = \"Parsing error: couldn't find ```python ... ``` code block or line starting with 'FINAL ANSWER:' in the model output. Please either:n1) Provide valid Python within ```python (cid:44) ... ``` ORn2) Reply directly with line exactly: FINAL ANSWER: <answer> with no trailing text.\" (cid:44) max_rounds = 20 messages = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}] messages.append({\"role\": \"user\", \"content\": f\"Problem: { x} \"}) out = await model_call(messages, temperature=1.0, thinking=True, max_tokens=20000) messages.append({\"role\": \"assistant\", \"content\": out}) execution_state = {} for in range(max_rounds): # First check for any output code to execute # We must always first execute any code or else we may parse out \"FINAL ANSWER\" print statement (cid:44) blocks = extract_python_code(out) if blocks: before executing it block = \"nn\".join(blocks) exec_res, execution_state = python_exec(block, execution_state) # Check for final answer in execution output = extract_final_answer(exec_res) if m: return m.strip() code is:n{ exec_res} \"}) # No final answer, so take another turn of model output messages.append({\"role\": \"user\", \"content\": f\"The output of executing the generated Python (cid:44) out = await model_call(messages, temperature=1.0, thinking=True, max_tokens=20000) messages.append({\"role\": \"assistant\", \"content\": out}) continue # If no code, then check for final answer = extract_final_answer(out) if m: return m.strip() messages.append({\"role\": \"user\", \"content\": PARSE_ERROR_PROMPT}) out = await model_call(messages, temperature=1.0, thinking=True, max_tokens=20000) messages.append({\"role\": \"assistant\", \"content\": out}) ANSWER: answer' with no trailing text.\"}) messages.append({\"role\": \"user\", \"content\": \"Now output the final answer on single line as 'FINAL (cid:44) out = await model_call(messages, temperature=1.0, thinking=True, max_tokens=20000) = extract_final_answer(out) if m: return m.strip() else: 78 79 80 81 83 84 85 86 87 89 90 91 92 93 95 96 97 98 99 101 102 103 104 105 107 108 109 110 111 113 114 115 116 117 119 raise Exception(\"No answer found.\") 120 121 ``` 122 </strategy> 123 </example> 124 125 <example> 126 Parallelization (sample multiple times with temperature and then take the most common response. For very (cid:44) ambiguous or challenging problems, sampling 20+ times can be helpful): 127 <strategy> 128 async def strategy(x): PROMPT = \"Solve the problem and output the final answer on single line as 'FINAL ANSWER: <answer>' (cid:44) with no trailing text.nProblem: {x}\" 130 131 132 outputs = [] messages = [{\"role\": \"user\", \"content\": PROMPT.format(x=x)}] async def get_response(): 25 134 135 136 138 139 140 141 142 144 145 146 147 158 160 161 162 163 164 166 167 168 169 170 172 173 174 175 176 178 179 180 181 182 184 185 186 187 188 190 191 192 193 194 # use higher temperature for more variation out = await model_call(messages, temperature=1.0, thinking=True, max_tokens=20000) # Find FINAL ANSWER ans = extract_final_answer(out) if ans: return ans else: return \"No answer found\" # multiple independent calls can run concurrently with asyncio.gather outputs = await asyncio.gather(*[get_response() for _ in range(10)]) # return the majority of the answers return max(set(outputs), key = outputs.count) 148 149 </strategy> 150 </example> 151 152 <example> 153 Evaluator-Optimizer (one LLM call generates solution and another provides evaluation and feedback for (cid:44) the generator to improve its solution) 154 <strategy> 155 ```python 156 async def strategy(x): 157 your final answer on single line as 'FINAL ANSWER: <answer>' with no trailing text.\" GENERATOR_PROMPT = \"You are an expert problem solver. Solve the given problem step by step and provide (cid:44) EVALUATOR_PROMPT = \"Evaluate whether the following answer to problem is correct.nProblem: (cid:44) {problem} nProposed Answer: {solution} nIf the answer is correct, respond with exactly 'FINAL ANSWER: CORRECT'. If the answer is incorrect, respond with 'FINAL ANSWER: <explanation of what is wrong and guidance for improvement>'.\" (cid:44) MISSING_ANSWER_PROMPT = \"The last output is missing line of the form 'FINAL ANSWER: ...' which needs (cid:44) to be parsed out. Please try again to output line following this format.\" (cid:44) max_iterations = 10 # Generate initial solution generator_messages = [ {\"role\": \"system\", \"content\": GENERATOR_PROMPT}, {\"role\": \"user\", \"content\": f\"Problem: { x} \"} ] for in range(max_iterations): # get answer from generator for _ in range(5): solution = await model_call(generator_messages, temperature=1.0, thinking=True, (cid:44) generator_messages.append({\"role\": \"assistant\", \"content\": solution}) max_tokens=20000) # Check if we have final answer ans = extract_final_answer(solution) if ans: break else: generator_messages.append({\"role\": \"user\", \"content\": MISSING_ANSWER_PROMPT}) # get evaluation of the current answer eval_messages = [{\"role\": \"user\", \"content\": EVALUATOR_PROMPT.format(problem=x, solution=ans)}] for _ in range(5): evaluation = await model_call(eval_messages, temperature=1.0, thinking=True, max_tokens=20000) eval_ans = extract_final_answer(evaluation) if eval_ans: break else: eval_messages = [{\"role\": \"assistant\", \"content\": evaluation}] eval_messages.append({\"role\": \"user\", \"content\": MISSING_ANSWER_PROMPT}) # only return the answer if the evaluator says it is correct 26 195 197 198 199 200 201 203 204 205 206 if \"CORRECT\" in eval_ans: return ans # otherwise, make the generator improve the answer based on the evaluator output generator_messages.append({\"role\": \"user\", \"content\": f\"An evaluator gave the following feedback on your answer: { evaluation} . Please revise your answer and output the new answer on line (cid:44) as 'FINAL ANSWER: answer'.\"}) (cid:44) # Final attempt to extract answer solution = await model_call(generator_messages, temperature=1.0, thinking=True, max_tokens=20000) ans = extract_final_answer(solution) if ans: return ans.strip() else: raise Exception(\"No answer found.\") 207 208 ``` 209 </strategy> 210 </example> 211 212 If parsing anything from the output of the model, be sure to exactly specify the format the model is (cid:44) supposed to respond in and regenerate using any failed parsing feedback if there are parsing errors. 213 It's best to keep the max_tokens around 20000 for all calls to `model_call` to avoid issues from output (cid:44) truncation, unless the memory shows that using lower max_tokens is safe. 214 Your central goal is to write strategy that will correctly solve the problem, but if there are two (cid:44) possible strategies which are equally likely to be correct, favor the less costly strategy. 215 strategy should never give up! The strategy must solve the provided problem even if it takes long (cid:44) (cid:44) (cid:44) time, so the strategy should take whatever steps necessary to solve the problem correctly and be sure it does not result in giving up and outputting an incorrect answer. If everything goes wrong, then throw an exception. 216 Strategies are written at the meta-level (see how the above examples are not problem specific), and you (cid:44) should only tailor prompts to the problem if you are sure it will help. 217 Note that for many non-algorithmic tasks, simple CoT approach (with in-context examples if available) is one of the best strategies provided the model is capable enough, and CodeAct is one of the best strategies for algorithmic tasks. (cid:44) (cid:44) 218 219 **Memory** 220 [[PAST]] 221 Use the above memory to help design the strategy. It is your job to incorporate any useful information from the memory into the strategy. This can be incorporated into the strategy at two levels: 222 1. The high level structure (the memory provides the best strategies for related tasks as well as other (cid:44) (cid:44) notes about useful strategies). Base the strategy structure on the memory as much as possible. 223 2. The prompts (e.g. include all important advice, code blocks to use, pitfalls to avoid, few-shot (cid:44) (cid:44) (cid:44) (cid:44) examples, etc. which the model should have in its context). To include memory entry into strategy (such as to include code into prompt), use the global variable `MEMORY_ENTRY_K` where is replaced with the id of the entry or copy the content of the memory verbatim. Include as much information from the notes as possible into prompts to help the models avoid pitfalls and streamline their reasoning. 224 Please mention if you are using any of the above memories when designing the strategy. Before designing new strategy, check if (task, best strategy) pair exists in the memory for the current task and use the best strategy if so. (cid:44) (cid:44) 225 Ignore irrelevant memory entries. 226 Overall, you want to think about how to use more computation to avoid incorrect answers when they occur (cid:44) (cid:44) (cid:44) (more verification/CodeAct iterations/voting/thinking when the problem is very hard or past problems were answered incorrectly) and when to use less computation (simpler strategies such as CoT when the problem/subproblem is simple or past problems were solved easily) to reduce cost. 227 You want to produce the strategy most likely to get correct answer. 228 229 Target problem: 230 [[PROBLEM]] 231 232 Your Task: 233 [[TASK_PROMPT]] F.2 Consolidator The prompt for the Consolidator is included below."
        },
        {
            "title": "Consolidator Prompt",
            "content": "[[PAST_STRATEGY]] [[STRATEGY_EXECUTION_TRACE]] 1 The outcomes from executing the generated strategies are shown below. 2 <experience> 3 <strategy> 4 5 </strategy> 6 <trace> 7 8 </trace> 9 <final_answer>[FINAL_ANSWER]</final_answer> 10 <verifier_output>[[Answer is correct! or Answer is incorrect!]]</verifier_output> 11 <cost>[[COST]]</cost> 12 </experience> 13 ... 14 15 Your task is now to update memory to help you solve problems better in the future by generating better (cid:44) strategies. 16 The memory consists of (task, best strategy) pairs where there is exactly one entry per task seen as well (cid:44) as entries about general useful findings to improve strategies in the future. 17 18 **Workflow** 19 1. For each experience, examine the verifier output and find the strategies that resulted in the correct (cid:44) (cid:44) (cid:44) 20 21 answer. Of these correct answer strategies, find the one which has greatest reliability or lowest cost. If all strategies resulted in the wrong answer, then continue to step 2 and DO NOT SAVE THE STRATEGY to memory. 1.1. Check the current memory for existing (task, best strategy) pairs where the task matches the (cid:44) 1.2. If there is an existing (task, best strategy) entry in the memory for this task, update the memory entry with the strategy found in step 1 ONLY IF the strategy used above is better (in (cid:44) reliability or lower cost) than the one already in memory. current problem. (cid:44) 22 2. From the reasoning traces of the strategies extract any useful information (such as code snippets and useful examples) which could help in the future. DO NOT SAVE any strategy which resulted in the incorrect answer. (cid:44) (cid:44) 23 24 **Extract** 25 - The best strategies that resulted in correct answers, helpful functions, heuristics, examples. Feel free (cid:44) to save code. 26 - Unexpected behaviors of models or tools in the reasoning trace which should be remembered to help design (cid:44) better strategies and avoid unneccessary rework or continued failures in the future 27 - When certain strategy motifs are useful or harmful 28 - Useful and working code snippets, tool configurations which can be provided in the future rather than (cid:44) rediscovered 29 - Anything that would save significant reasoning time/cost (such as having to reimplement code solution (cid:44) or reducing the number of reasoning steps) 30 - Failure modes and their early detection/mitigation 31 - Effective prompts or verification methods 32 - Cost-saving shortcuts that maintain accuracy 33 - Failures in tool-use which could be fixed in the strategy 34 35 **Avoid** 36 - Problem-specific literals (numbers, names, dataset IDs) or any information which would not apply to (cid:44) other problems 37 - Obvious insights, redundant info, incomplete code 38 - Anything that requires rework, for instance if something was derived in the reasoning trace (like code (cid:44) solution) then don't say \"solve with code\" and instead save the actual code. (cid:44) 39 - Inventing or coming up with anything new, for instance if the reasoning trace shows failed solution, do not fix the solution now since your new solution may also be wrong. Instead, it is better to examine the reasoning trace to determine why it was wrong or what was an unsuccessful strategy to avoid the mistake in the future. (cid:44) (cid:44) 40 - Saving strategies that resulted in the wrong answer 41 42 **Output** 43 To add or update the (task, best strategy) pair, output blocks of the form: 44 <memory_entry> 45 Task: [short description of the task to allow you to know which problems to use this strategy for in the (cid:44) future. Can be at any granularity that is helpful.] 46 Best Strategy: 28 47 <strategy> 48 [Code for the best strategy discovered for the task] 49 </strategy> 50 </memory_entry> 51 For saving other useful information, but not full strategies, output blocks of the form: 52 <memory_entry> 53 [Actionable memory entry to improve future behavior on similar problems.] 54 </memory_entry> 55 56 To delete memory entries, output blocks of the form: 57 <delete_entry> 58 [mem-id] 59 </delete_entry> 60 For instance: 61 <delete_entry> 62 1 63 </delete_entry> 64 will delete the memory entry block of the form <memory_entry-1></memory_entry-1>. 65 When you output new memory, DO NOT add the ID to the memory_entry block as the ID will be automatically (cid:44) assigned for you. 66 To edit memory, delete the old memory and then output the updated memory. 67 Never delete (task, best strategy) pair from memory. 68 Be careful about deleting entries because other memory entries may refer to the deleted memory entry or (cid:44) even use the variable `MEMORY_ENTRY_K` within code. 69 70 **Existing Memory** 71 [[past_memories]] 72 Memory length: [[memory_len]] tokens 73 74 Output nothing if you do not want to change anything. 75 Try not to remove any memories which could be useful in the future. 76 Also, ensure the memory stays shorter than 10000 tokens or else it will distract more than it helps. If (cid:44) the memory is above 10k tokens, try to only edit the memory rather than make additions. 77 Before outputting the memory edits, write short justification for your changes."
        }
    ],
    "affiliations": [
        "AWS AI",
        "University of Pennsylvania"
    ]
}