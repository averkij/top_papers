{
    "paper_title": "LOCATEdit: Graph Laplacian Optimized Cross Attention for Localized Text-Guided Image Editing",
    "authors": [
        "Achint Soni",
        "Meet Soni",
        "Sirisha Rambhatla"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Text-guided image editing aims to modify specific regions of an image according to natural language instructions while maintaining the general structure and the background fidelity. Existing methods utilize masks derived from cross-attention maps generated from diffusion models to identify the target regions for modification. However, since cross-attention mechanisms focus on semantic relevance, they struggle to maintain the image integrity. As a result, these methods often lack spatial consistency, leading to editing artifacts and distortions. In this work, we address these limitations and introduce LOCATEdit, which enhances cross-attention maps through a graph-based approach utilizing self-attention-derived patch relationships to maintain smooth, coherent attention across image regions, ensuring that alterations are limited to the designated items while retaining the surrounding structure. \\method consistently and substantially outperforms existing baselines on PIE-Bench, demonstrating its state-of-the-art performance and effectiveness on various editing tasks. Code can be found on https://github.com/LOCATEdit/LOCATEdit/"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 2 ] . [ 1 1 4 5 1 2 . 3 0 5 2 : r LOCATEdit: Graph Laplacian Optimized Cross Attention for Localized Text-Guided Image Editing Achint Soni1 Meet Soni2 Sirisha Rambhatla1 1University of Waterloo 2Stony Brook University {a2soni, sirisha.rambhatla}@uwaterloo.ca, meet.soni@stonybrook.edu Figure 1. Our LOCATEdit demonstrates strong performance on various complex image editing tasks."
        },
        {
            "title": "Abstract",
            "content": "Text-guided image editing aims to modify specific regions of an image according to natural language instructions while maintaining the general structure and the background fidelity. Existing methods utilize masks derived from crossattention maps generated from diffusion models to identify the target regions for modification. However, since crossattention mechanisms focus on semantic relevance, they struggle to maintain the image integrity. As result, these methods often lack spatial consistency, leading to editing artifacts and distortions. In this work, we address these limitations and introduce LOCATEdit, which enhances crossattention maps through graph-based approach utilizing self-attention-derived patch relationships to maintain smooth, coherent attention across image regions, ensuring that alterations are limited to the designated items while retaining the surrounding structure. LOCATEdit consistently and substantially outperforms existing baselines on PIEBench, demonstrating its state-of-the-art performance and effectiveness on various editing tasks. Code can be found on https://github.com/LOCATEdit/LOCATEdit/ 1. Introduction Diffusion models have become popular for image generation, yet practical applications demand precise control for editing. Text-guided editing techniques [4, 43, 49] have emerged as powerful tools to facilitate such modifications across domains, from digital art [8, 12, 17, 38] to medical imaging [25, 42], enabling more intuitive image manipulation through natural language prompts. However, promptdriven editing is often imprecise [2, 6, 15]. To attain precise control in text-guided image editing, recent studies use masks derived from cross-attention maps; however, inaccuracies in these maps can result in edits spilling over unintended regions, causing problems such as object identity loss [18, 35, 44] and background drift [23, 49]. Because of this, techniques that depend exclusively on cross-attention could make global changes when only localized modifications are required [9, 15]. These problems highlight the necessity for method that precisely identifies editing areas without jeopardizing the integrity of the overall image. Recent methods have demonstrated improved mask accuracy through the utilization of crossand self-attention masks, while simultaneously adopting the dual branch editing paradigm [43, 49]. Additionally, they incorporate target image embeddings as auxiliary guidance derived from source image embeddings and the editing information contained in source-target prompt pairs. Despite these, challenges such as unintended spills continue to be problem, which can be seen in Figure 2. Our key observation is that naively combining cross-attention and self-attention results in significant information loss. Consequently, we propose to induce spatial consistency and precise identification of regions to be edited via graph-based approach. Given graphs Gsrc and Gtgt for the source and target branch, respectively, each of these graphs is constructed using the respective Cross and Self-Attention, hence CASA graphs, encoding cross-attention maps as nodes and self-attention relationships as weighted edges. With this abstraction, these graphs intrinsically depict the structure of the image, thereby connecting local and global contexts, while also maintaining the semantic relevance. We explicitly enforce graph structure by proposing graph Laplacian regularizer on Gsrc and Gtgt to impose spatial consistency, motivated by the effectiveness of Laplacian regularization in image denoising and mesh editing [27, 37]. Prior works on segmentation and spatial regularization [41, 51] also demonstrate that this Laplacian constraint effectively maintains object boundaries and preserves local detail, thus disentangling the areas of interest from unrelated regions. Furthermore, Belkin and Niyogi [1] and Lim et al. [22] illustrate that this regularization also enhances the separation of semantic characteristics. By integrating Laplacian smoothness factor into the difFigure 2. Example of over-editing caused due to imprecise masks. fusion process, LOCATEdit optimizes the attention values across interconnected patches without any additional training, hence reducing background drift and limiting global changes as can be seen in Figure 1. This ensures that modifications are confined to designated areas while preserving the overall structural integrity of the original image. Notably, this optimization admits closed-form solution, hence eliminating the need for iterative refinement [43]. Overall, our contributions can be summarized as follows: CASA Graph: We introduce LOCATEdit, method which encapsulates word-to-pixel relevance through pixel-to-pixel relationships by modeling attention maps with CASA graph. Improved spatial consistency: By optimizing masks through graph Laplacian regularization on the CASA graph, we maintain object structure and confine changes to intended regions, hence minimizing distortions. Disentangled and faithful editing: Leveraging Laplacian smoothing, LOCATEdit achieves precise semantic modifications while preserving the original image context, ensuring disentangled editing. 2. Related Work Recent advances in image editing have leveraged range of conditioning modalitiesincluding text, reference images, and segmentation mapsto drive semantic, structural, and stylistic modifications [13]. In this work, we focus specifically on text-guided image editing with an emphasis on preserving the original content and ensuring effective foreground-background disentanglement. 2.1. Text-guided Image Editing Early methods exploited the power of CLIP [30] to align images and text. For example, [16] fine-tuned diffusion models during reverse diffusion using CLIP loss to adjust image attributes, though these approaches were limited to global changes and often suffered from degraded image quality. Later works such as DiffuseIT [19] and StyleDiffusion [46] improved performance by introducing semantic or style disentanglement losses; however, they are computationally expensive and typically confined to specific style modifications. More recent frameworks like InstructPix2Pix [3] preserve source content using text instructions, yet require carefully curated instruction-image pair datasets and supervised training. Additionally, methods that manipulate text embeddings for disentangled editing have been explored [47], though they often yield only marginal improvements over earlier approaches. Collectively, these studies underscore both the promise and limitations of text-guided editing, motivating our work on refining attention maps to achieve spatially consistent and localized modifications. 2.2. Training Free Image Editing Recent advances in text-to-image synthesis [7, 26, 31, 33, 34] have enabled high-quality photorealistic image generation from text prompts. Building on these advances, several studies have proposed dual-branch, training-free approaches that leverage rich feature and attention maps from pre-trained diffusion models for image editing. These methods exploit signals from the source images diffusion process to drive content modification, obviating the need for additional model training while achieving remarkable success in altering image content. Notably, PRedITOR [32] generates target CLIP embedding via diffusion prior model but struggles with fine detail and background consistency. Other methods enhance structural control: P2P [9] replaces cross-attention maps to maintain spatial alignment, and PnP [40] injects spatial features and self-attention maps into decoder layers. Approaches like MasaCtrl [4] preserve structure through mutual self-attention, while editing-area grounding techniques and attention regularization losses are employed in DPL [49] and refined further in ViMAEdit [43]. Despite these advances, challenges in achieving precise localization and consistent edits persist, motivating our work. 2.3. Graph Laplacian In optimization and semi-supervised learning, Laplacian regularization promotes smooth variation along graph, similar to how Conditional Random Fields (CRFs) refine segmentation by enforcing spatial and color consistency [20]. Unlike CRFs, Laplacian smoothing is fully differentiable and easily integrated into neural networks. Its effectiveness has been demonstrated in tasks such as image matting [21], where the matting Laplacian preserves edges while interpolating unknown regions, and in action localization [28], where it refines class activation maps for more coherent predictions. 3. Background 3.1. Diffusion models Diffusion models [11, 26, 36] constitute class of generative approaches that operate through two complementary processesforward and backward diffusion. In the forward diffusion process, starting from an original clean sample z0 (drawn from the data distribution), Gaussian noise is iteratively added at each timestep = 1, 2, . . . , . Specifically, one obtains zt = αt z0 + 1 αt ϵt, = 1, . . . , T, where ϵt (0, I) is an independent Gaussian noise term injected at timestep t. The sequence {αt}T t=1 governs the noise variance at each stage, ensuring that after diffusion steps, zT is approximately distributed as standard Gaussian. The backward diffusion process reverses this corruption procedure by progressively denoising the noisy sample zT into cleaner sample zT 1, then zT 2, and so forth, converging to final clean reconstruction z0. To accomplish this, one samples zt1 from conditional distribution over zt, typically parameterized by learnable denoising function. Formally, the update rule may be expressed as zt1 = µt (cid:0)zt, θ(cid:1) + σt ϵt, = T, . . . , 1, where ϵt is random Gaussian noise , µt and σt represents the mean and variance of distribution that zt1 can be sampled from, and θ encapsulates the learned parameters. In the DDIM formulation [36], one often employs deterministic variant by modifying the variance schedule, making the sampling process more efficient while maintaining high sample quality. pivotal component in modern diffusion models is the noise prediction network ϵθ(zt, t). Rather than predicting z0 or zt1 directly, the network estimates the noise present in the corrupted sample zt. Once trained, this noise predictor effectively guides the reverse diffusion steps to iteratively remove the injected Gaussian noise. 3.2. Attention mechanism In practice, the noise prediction model is frequently instantiated by U-Net architecture, chosen for its efficacy in pixel-level prediction tasks. Each U-Net block typically consists of (i) residual convolutional sub-block that refines the spatial representation of the intermediate feature maps, and (ii) self-attention sub-block that captures long-range patch-to-patch dependencies. (iii) cross-attention sub-block that aligns the image to textual information In the mechanism, feature tensors are first projected into three distinct embeddingsqueries Q, keys K, and values Figure 3. Overview of our text-guided image editing pipeline. LOCATEdit refines cross-attention maps with graph Laplacian regularization for spatial consistency, uses an IP-Adapter for additional guidance, and employs selective pruning on text embeddings to suppress noise, ensuring the edited image preserves key structural details. . Attention is computed as 4.1. Dual-Branch Editing Paradigm Attention(Q, K, ) = Softmax (cid:16) QK (cid:17) V, where is the dimensionality of the query/key vectors, In both self-attention and cross-attention layers, is projected from spatial features. In self-attention, and also come from spatial features, whereas in cross-attention, they are projected from textual embeddings. These projections use learned metrics that are optimized during training. 4. LOCATEdit In this section, we present LOCATEdit for precise, localized text-guided image editing that refines the cross-attention maps. Our approach integrates two complementary modules. First, we utilize the CASA graph to impose spatial coherence and ensure that edits are restricted to the designated areas. Second, building upon previous work [43], we integrate an image embedding-enhanced denoising process augmented by selective pruning operator applied to the text embedding offsets. This operator eliminates minor semantic variations, therefore minimizing unwanted changes and avoiding unnecessary editing of non-target regions. Together, these modules allow LOCATEdit to maintain the structural integrity of the original image while precisely implementing the desired edits. Our pipeline employs dual-branch design in which source branch reconstructs the original image and target branch generates the edited output. To maintain structural consistency, both branches start from the same initial noise zT and share intermediate latent variables. Crucially, we inject the cross-attention maps from the source branch into the target branch [9] to maintain the spatial structure. Formally, if Qsrc and src are the query and key embeddings from the source branch and tgt denotes the value embeddings from the target branch, then the target cross-attention is computed as Attention(cid:0)Qsrc, src, tgt(cid:1) = Softmax (cid:16) Qsrc(K src) (cid:17) tgt. 4.2. Selective Embedding Interpolation Following previous work [43], we employ an IP-Adapter [50] to provide explicit guidance for target image generation. After extracting the source image embedding eimg src and the CLIP-based text embeddings etxt tgt corresponding to the source and target prompts respectively, the conventional target image embedding is computed as src and etxt eimg tgt = eimg src + (cid:16) src etxt etxt tgt (cid:17) . (1) This embedding is then processed by the IP-Adapter, which projects it into latent feature space that is integrated into Figure 4. CASA (Cross and Self-Attention) Graph Construction workflow. The initial cross-attention maps are upsampled to form patch-level adjacency graph, then Laplacian regularization enforces spatial consistency. Thresholding the refined maps yields final, more robust attention masks. the diffusion models cross-attention mechanism. Specifically, given the query (derived from the noisy latent), the IP-Adapter produces additional key and value features IP and IP from the projected target embedding. These are then combined with the original key and value features to form the final cross-attention: = Attention(Q, K, ) + λAttention(Q, IP, IP) thereby incorporating semantic guidance into the diffusion process without requiring any additional training. src eT limitation of directly using the difference eT tgt in Equation (1) is that low-magnitude components, inherent in the entangled nature of CLIP text embeddings [24], can lead to unintended edits. To mitigate this, we introduce selective pruning operator that thresholds the text difference, retaining only the dominant semantic offsets. Formally, we replace Equation (1) with tgt = eI eI src + (cid:16) src eT eT tgt (cid:17) , where : Rd Rd is defined elementwise as (cid:2)H(y)(cid:3) = yi, if yi τ, 0, otherwise. (2) (3) Here, is the embedding dimension and τ is determined via percentile threshold on the absolute values of the difference. This selective pruning ensures that only significant semantic shifts contribute to the target image embedding, thereby reducing the risk of global edits and preserving the structural consistency of non-target regions. The pruned embedding is then processed through the IP-Adapter as described above, ensuring that the final diffusion process is both semantically guided and robust to minor, spurious variations. 4.3. Formulating CASA Graph While the IP-Adapter provides explicit semantic guidance, the cross-attention maps extracted during denoising may still contain spills that lead to unintended edits. To address this, we refine these attention maps by modeling them as CASA graph, where each node represents an image patch and the edges capture patch-to-patch relationships obtained from self-attention as can be seen in Figure 4. The graph Laplacian regularization enforces smoothness constraint across the CASA graph, penalizing abrupt differences in attention between strongly connected patches. In effect, this smoothing suppresses isolated high responses that can cause over-editing, ensuring that only spatially coherent regions receive significant modifications. By harmonizing the attention values over connected patches, LOCATEdit robustly confines edits to the intended regions and preserves the overall spatial consistency. Formally, within each U-Net block, each prompt word is linked to cross-attention map; however, only the crossattention maps related to the blend word(s) are necessary. Following previous studies [5, 9, 49], we compute the average of the cross-attention maps obtained from multiple U-Net blocks to get initial maps. We obtain initial attention maps for both the source and target branches, denoted as 0 Rrr. These masks are then upM src sampled to higher resolution of RR (where = γr and γ > 1) to capture fine spatial details, and subsequently flattened to yield the initial saliency maps msrc 0 Rrr and tgt 0 RR2 0 , mtgt . To prioritize high-confidence regions, we compute weight for each patch by applying the sigmoid function σ() to the corresponding element of msrc 0 and then squaring the output. Squaring the sigmoid output emphasizes larger values while further suppressing lower ones, thereby enhancing the reliability of high-confidence regions. These weights are assembled into diagonal confidence matrix with scaling factor α: Λsrc = diag (cid:16) (cid:16) σ (cid:17)2 αm0[1] , . . . , σ (cid:16) αm0[R2] (cid:17)2(cid:17) . (4) and similarly for Λtgt. Next, we extract self-attention maps Ssrc RR2R2 and Stgt RR2R2 for the source and target branches, respectively. To ensure mutual relationships are treated uniformly and to guarantee the convexity of the optimization, we symmetrize both the maps as Ssym = (cid:16) + S(cid:17) . 1 2 (5) Now, for each branch we construct CASA graph = (V, E) where each node vi corresponds to patch in the flattened saliency map m0. The edge weight between nodes vi and vj is given by the symmetrized self-attention map Ssym. This graph structure, with nodes representing the initial saliency values and edges capturing inter-patch relationships, serves as the foundation for the CASA graph. 4.4. Graph Laplacian Regularization After initializing CASA graphs Gsrc and Gtgt for both branches, we optimize for the value of their nodes using graph Laplacian optimization. Formally, graph Laplacian is defined by: = Ssym. where is degree matrix for Ssym, which is computed as D(i, i) = R2 (cid:88) j=1 Ssym(i, j), D(i, j) = 0 for = j, Figure 5. Illustration of the convex objective J(m) in 2D slice of the higher-dimensional space. The single global minimum, marked in red, highlights the functions convex nature. Detailed proof is provided in Appendix 9. The refined saliency maps msrc and mtgt are then reshaped back to src and tgt, which are then used to obtain by taking the element-wise maximum of the two maps: = max{M src, tgt}, Thresholding with δ gives the final spatial mask . An optimized CASA graph enforces smooth, spatially consistent mask that preserves high-confidence regions and mitigates over-editing in less reliable areas. Moreover, to maintain background consistency and prevent unintended changes outside the editing region, this optimized mask is used to replace the target branchs latent representation at each denoising step: ˆztgt t1 = ztgt t1 + (1 ) zsrc t1, = T, . . . , 1. Here, denotes Hadamard product. This step ensures that the background and non-editable regions of the source image remain unchanged throughout the iterative denoising process. Lemma 1. The graph Laplacian RR2R2 semidefinite. Detailed proof is provided in Appendix 8. is positive 5. Experiments We then 0 and mtgt optimize saliency maps 0 for both branches through the following initial msrc convex optimization problem: the Theorem 1. Let m0 RR2 be the initial saliency map, and let Λ and be defined as above. The optimal saliency map RR2 is the unique minimizer of J(m) = (m m0)Λ(m m0) + λ mL m, with the solution = (Λ + λ L)1 Λ m0. 5.1. Dataset and Evaluation metrics We follow recent work [15, 43, 48] and evaluate our approach using the PIE-Bench dataset [15], which is currently the only established benchmark designed for prompt-based image editing. PIE-Bench contains 700 images categorized into ten different editing tasks, with each image accompanied by source prompt, target prompt, blend words (i.e., terms that specify the required edits), and an editing mask. Although only the source prompt, target prompt, and blend words are necessary for performing prompt-based editing, the editing mask is employed to gauge how well the method preserves the background. To thoroughly assess our models, we adopt the evaluation strategy described in [15], focusing on three main criteria: 1) Structure consistency, measured by the difference Source Image LOCATEdit ViMAEdit InfEdit MasaCtrl LEDITS++ an orange black cat sitting on top of fence cat tiger sitting next to mirror the crescent moon golden crescent moon and stars are seen in the night sky white golden horse running in the sunset open closed eyes cat sitting on wooden floor kitten duck walking through the grass boat is docked on lake in the heavy fog sunny day woman man and horse Table 1. Qualitative comparisons with competing text-guided editing methods. LOCATEdit yields more localized edits while preserving overall structure, outperforming baselines in both fidelity and consistency. in DINO self-similarity maps [39], 2) Background preservation, evaluated via PSNR, LPIPS [52], MSE, and SSIM [45], and 3) Target promptimage alignment, determined by CLIP similarity [10]. 5.2. Comparison with existing methods LOCATEdit consistently yields superior spatial consistency and semantic alignment compared to state-of-the-art textguided image editing methods as can be seen in Table 2. Unlike P2P-Zero [29] and PnP-based techniques [15, 39], which tend to induce global modifications and suffer from spatial inconsistencies, LOCATEdit confines edits to intended regions, thereby preserving the source images structure. Mask-guided methods such as ViMAEdit [43] improve localization but can still introduce artifacts in nontarget areas. Our graph Laplacian regularization refines Inverse VI VI Method Sampling (steps) DDCM(12) DDIM(50) PnP-I DDIM(50) EF DPM-Solver++(20) Editing Structure Background Preservation CLIP Similarity Distance 103 PSNR LPIPS 103 MSE 104 SSIM 102 Whole Edited InfEdit ViMAEdit P2P-Zero MasaCtrl PnP P2P ViMAEdit LOCATEdit (Ours) LEDITS++ P2P ViMAEdit LOCATEdit (Ours) 13.78 12.65 51.13 24.47 24.29 11.64 11.90 13.19 23.15 14.52 14.16 8. 28.51 28.27 21.23 22.78 22.64 27.19 28.75 29.20 24.67 27.05 28.12 29.16 47.58 44.67 143.87 87.38 106.06 54.44 43.07 41.60 80.79 50.72 45.62 39.31 32.09 30.29 135.00 79.91 80.45 33.15 28.85 26.90 118.56 37.48 33.56 24.01 85.66 85.65 77.23 81.36 79.68 84.71 85.95 86.53 81.55 84.97 85.61 86.52 25.03 25.91 23.36 24.42 25.41 25.03 25.43 25.96 25.01 25.36 25.51 26.07 22.22 22.96 21.03 21.38 22.62 22.13 22.40 23.02 22.09 22.43 22.56 22. Table 2. Comparison of different methods based on structure, background preservation, and CLIP similarity metrics. Method LOCATEdit Structure Distance 103 13.19 Background Preservation PSNR LPIPS 103 MSE 104 29.20 26. 41.60 w/o diagonal weighting matrix w/o symmetric self-attention w/o α-based control with high α 8.68 8.59 8. 20.37 29.59 29.42 29.26 24.27 38. 38.75 38.96 87.37 23.17 24.09 24. 60.82 CLIP Similarity SSIM 102 Whole Edited 86.53 86.83 85. 86.60 82.22 25.96 25.33 25.29 25. 26.58 23.02 22.34 22.26 22.28 23. Table 3. Comparison of different methods based on structure, background preservation, and CLIP similarity metrics. cross-attention maps by enforcing smooth, coherent patchto-patch relationships, addressing these issues directly. Moreover, while approaches like Edit-Friendly Inversion [14] and InfEdit with Virtual Inversion [48] achieve better semantic alignment, they often struggle to disentangle editable regions from the preserved background. In contrast, our method robustly separates these regions, ensuring that modifications are both precise and localized, which can be seen in qualitative comparision we provide in Table 1. Overall, our experiments demonstrate that our method not only enhances the fidelity of the edited regions but also maintains the overall structural integrity of the source image. By leveraging CASA graph-based attention refinement, our approach outperforms existing techniques across multiple metrics, underscoring the importance of spatially consistent and disentangled editing for practical text-controlled image editing applications. 5.3. Ablation Study To demonstrate the effectiveness of our model, we provide results for three different model ablations: 1) w/o diagonal weighting matrix: we use uniform L2 penalty as the first term of Equation 6, 2) w/o symmetric self-attention: We do not parameterize the similarity matrix which is essential for the Laplacian to be positive semidefinite, and 3) w/o α-based control: We keep α = 1 in Equation 4. Table 3 shows that each of our contribution outperforms the baselines in terms of Structure and Background preservation. We also observe that while combining different techniques together results in slightly worser results in Structure and Background Similarity metrics, we are able to achieve state-of-the-art CLIP Similarity. It is to be noted that even the worser results are better than all the baseline methods reported in Table 2. Finally, when we were tuning the α parameter, we observed that higher value of α edits images with way better CLIP similarity but significantly worsens the results for other metrics. This is to be expected because high α results in hard thresholding where it makes clear distinction between areas that should be trusted and those that should be adjusted, but it also leads to abrupt transitions. 6. Conclusion In this paper, we introduced text-controlled image editing framework LOCATEdit that refines cross-attention masks using graph Laplacian regularization. It leverages selfattention-derived patch relationships to enforce spatial consistency and localized, disentangled modifications while preserving the structural integrity of the source image. Extensive experiments demonstrate that our approach outperforms state-of-the-art methods in semantic alignment and background fidelity. By confining edits to intended regions, our technique avoids unwanted alterations and maintains overall coherence. Future work will extend this framework to non-symmetric regularization and more complex editing scenarios, further enhancing controllable image generation."
        },
        {
            "title": "References",
            "content": "[1] Mikhail Belkin and Partha Niyogi. Laplacian eigenmaps for dimensionality reduction and data representation. Neural computation, 15(6):13731396, 2003. 2 [2] Manuel Brack, Felix Friedrich, Katharia Kornmeier, Linoy Tsaban, Patrick Schramowski, Kristian Kersting, and Apolinario Passos. Ledits++: Limitless image editing using text-to-image models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 88618870, 2024. 2 [3] Tim Brooks, Aleksander Holynski, and Alexei A. Efros. Instructpix2pix: Learning to follow image editing instructions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1839218402, 2023. 3 [4] Mingdeng Cao, Xintao Wang, Zhongang Qi, Ying Shan, Xiaohu Qie, and Yinqiang Zheng. Masactrl: Tuning-free mutual self-attention control for consistent image synthesis and editing. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2256022570, 2023. 2, 3 [5] Hila Chefer, Yuval Alaluf, Yael Vinker, Lior Wolf, and Daniel Cohen-Or. Attend-and-excite: Attention-based semantic guidance for text-to-image diffusion models. ACM Transactions on Graphics (TOG), 42(4):110, 2023. 5 [6] Guillaume Couairon, Jakob Verbeek, Holger Schwenk, and Matthieu Cord. Diffedit: Diffusion-based semantic image editing with mask guidance. arXiv preprint arXiv:2210.11427, 2022. 2 [7] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Bermano, Gal Chechik, and Daniel CohenOr. An image is worth one word: Personalizing text-toarXiv preprint image generation using textual inversion. arXiv:2208.01618, 2022. [8] Yifan Gao, Jinpeng Lin, Min Zhou, Chuanbin Liu, Hongtao Xie, Tiezheng Ge, and Yuning Jiang. Textpainter: Multimodal text image generation with visual-harmony and textcomprehension for poster design. In Proceedings of the 31st ACM International Conference on Multimedia, pages 7236 7246, 2023. 2 [9] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt imarXiv preprint age editing with cross attention control. arXiv:2208.01626, 2022. 2, 3, 4, 5 [10] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: reference-free evaluation metric for image captioning. arXiv preprint arXiv:2104.08718, 2021. 7 [11] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 3 [12] Nisha Huang, Weiming Dong, Yuxin Zhang, Fan Tang, Ronghui Li, Chongyang Ma, Xiu Li, and Changsheng Xu. Creativesynth: Creative blending and synthesis of viarXiv preprint sual arts based on multimodal diffusion. arXiv:2401.14066, 2024. 2 [13] Yi Huang, Jiancheng Huang, Yifan Liu, Mingfu Yan, Jiaxi Lv, Jianzhuang Liu, Wei Xiong, He Zhang, Shifeng Chen, and Liangliang Cao. Diffusion model-based image editing: survey. arXiv preprint arXiv:2402.17525, 2024. 2 [14] Inbar Huberman-Spiegelglas, Vladimir Kulikov, and Tomer Michaeli. An edit friendly ddpm noise space: Inversion and manipulations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12469 12478, 2024. [15] Xuan Ju, Ailing Zeng, Yuxuan Bian, Shaoteng Liu, and Qiang Xu. Pnp inversion: Boosting diffusion-based editing with 3 lines of code. In The Twelfth International Conference on Learning Representations, 2024. 2, 6, 7 [16] Gwanghyun Kim, Taesung Kwon, and Jong Chul Ye. Diffusionclip: Text-guided diffusion models for robust image manipulation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 24262435, 2022. 2 [17] Hyung-Kwon Ko, Gwanmo Park, Hyeon Jeon, Jaemin Jo, Juho Kim, and Jinwook Seo. Large-scale text-toimage generation models for visual artists creative works. In Proceedings of the 28th international conference on intelligent user interfaces, pages 919933, 2023. 2 [18] Eunseo Koh, Sangeek Hyun, MinKyu Lee, Jiwoo Chung, and Jae-Pil Heo. Structure-preserving text-based editing for few-step diffusion models. 2 [19] Gihyun Kwon and Jong Chul Ye. Diffusion-based image translation using disentangled style and content representation. arXiv preprint arXiv:2209.15264, 2022. 2 [20] John Lafferty, Andrew McCallum, Fernando Pereira, et al. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Icml, page 3. Williamstown, MA, 2001. [21] Anat Levin, Dani Lischinski, and Yair Weiss. closedform solution to natural image matting. IEEE transactions on pattern analysis and machine intelligence, 30(2):228242, 2007. 3 [22] Jungbin Lim, Jihwan Kim, Yonghyeon Lee, Cheongjae Jang, and Frank Park. Graph geometry-preserving autoencoders. In Forty-first International Conference on Machine Learning, 2024. 2 [23] Bingyan Liu, Chengyu Wang, Tingfeng Cao, Kui Jia, and Jun Huang. Towards understanding cross and selfattention in stable diffusion for text-guided image editing. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 78177826, 2024. 2 [24] Joanna Materzynska, Antonio Torralba, and David Bau. Disentangling visual and written concepts in clip. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1641016419, 2022. 5 [25] Abdullah al Nomaan Nafi, Md Alamgir Hossain, Rakib Hossain Rifat, Md Mahabub Uz Zaman, Md Manjurul Ahsan, and Shivakumar Raman. Diffusion-based approaches in medical image generation and analysis. arXiv preprint arXiv:2412.16860, 2024. 2 [26] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021. 3 [27] Jiahao Pang and Gene Cheung. Graph laplacian regularization for image denoising: Analysis in the continuous domain. IEEE Transactions on Image Processing, 26(4):17701785, 2017. 2 [28] Jungin Park, Jiyoung Lee, Sangryul Jeon, Seungryong Kim, and Kwanghoon Sohn. Graph regularization network with semantic affinity for weakly-supervised temporal action localization. In 2019 IEEE International conference on image processing (ICIP), pages 37013705. IEEE, 2019. 3 [29] Gaurav Parmar, Krishna Kumar Singh, Richard Zhang, Yijun Li, Jingwan Lu, and Jun-Yan Zhu. Zero-shot image-toIn ACM SIGGRAPH 2023 Conference image translation. Proceedings, pages 111, 2023. 7 [30] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In Proceedings of the 38th International Conference on Machine Learning, pages 87488763. PMLR, 2021. 2 [31] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1 (2):3, 2022. [32] Hareesh Ravi, Sachin Kelkar, Midhun Harikumar, and Ajinkya Kale. Preditor: Text guided image editing with diffusion prior. arXiv preprint arXiv:2302.07979, 2023. 3 [33] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 3 [34] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems, 35:3647936494, 2022. 3 [35] Dazhong Shen, Guanglu Song, Zeyue Xue, Fu-Yun Wang, and Yu Liu. Rethinking the spatial inconsistency in classifierfree diffusion guidance. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 93709379, 2024. 2 [36] Jiaming Song, Chenlin Meng, Denoising diffusion implicit models. arXiv:2010.02502, 2020. 3 and Stefano Ermon. arXiv preprint [37] Olga Sorkine, Daniel Cohen-Or, Yaron Lipman, Marc Alexa, Christian Rossl, and H-P Seidel. Laplacian surface editing. In Proceedings of the 2004 Eurographics/ACM SIGGRAPH symposium on Geometry processing, pages 175184, 2004. 2 [38] Yanan Sun, Yanchen Liu, Yinhao Tang, Wenjie Pei, and Kai Chen. Anycontrol: create your artwork with versatile control on text-to-image generation. In European Conference on Computer Vision, pages 92109. Springer, 2024. 2 [39] Narek Tumanyan, Omer Bar-Tal, Shai Bagon, and Tali Dekel. Splicing vit features for semantic appearance transfer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1074810757, 2022. 7 [40] Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali Plug-and-play diffusion features for text-driven Dekel. image-to-image translation. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19211930, 2023. 3 In Proceedings of [41] Shinji Uchinoura and Takio Kurita. Graph laplacian regularization based on the differences of neighboring pixels for conditional convolutions for instance segmentation. In 2022 26th International Conference on Pattern Recognition (ICPR), pages 36113617. IEEE, 2022. [42] Haoshen Wang, Zhentao Liu, Kaicong Sun, Xiaodong Wang, Dinggang Shen, and Zhiming Cui. 3d meddiffusion: 3d medical diffusion model for controllable and high-quality medical image generation. arXiv preprint arXiv:2412.13059, 2024. 2 [43] Kejie Wang, Xuemeng Song, Meng Liu, Jin Yuan, and Weili Guan. Vision-guided and mask-enhanced adaptive denoising for prompt-based image editing. arXiv preprint arXiv:2410.10496, 2024. 2, 3, 4, 6, 7 [44] Qian Wang, Biao Zhang, Michael Birsak, and Peter Wonka. Mdp: generalized framework for text-guided image editing by manipulating the diffusion path. arXiv preprint arXiv:2303.16765, 2023. 2 [45] Zhou Wang, Alan Bovik, Hamid Sheikh, and Eero Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing, 13(4):600612, 2004. 7 [46] Zhizhong Wang, Lei Zhao, and Wei Xing. Stylediffusion: Controllable disentangled style transfer via diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 76777689, 2023. 2 [47] Qiucheng Wu, Yujian Liu, Handong Zhao, Ajinkya Kale, Trung Bui, Tong Yu, Zhe Lin, Yang Zhang, and Shiyu Chang. Uncovering the disentanglement capability in textto-image diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 19001910, 2023. 3 [48] Sihan Xu, Yidong Huang, Jiayi Pan, Ziqiao Ma, and Joyce Chai. Inversion-free image editing with language-guided diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9452 9461, 2024. 6, [49] Fei Yang, Shiqi Yang, Muhammad Atif Butt, Joost van de Weijer, et al. Dynamic prompt learning: Addressing crossattention leakage for text-based image editing. Advances in Neural Information Processing Systems, 36:2629126303, 2023. 2, 3, 5 [50] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ipadapter: Text compatible image prompt adapter for text-toimage diffusion models. arXiv preprint arXiv:2308.06721, 2023. 4 [51] Jin Zeng, Jiahao Pang, Wenxiu Sun, and Gene Cheung. Deep graph laplacian regularization for robust denoising of In Proceedings of the ieee/cvf conference on real images. computer vision and pattern recognition workshops, pages 00, 2019. 2 [52] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness In Proceedings of deep features as perceptual metric. of the IEEE conference on computer vision and pattern recognition, pages 586595, 2018. 7 LOCATEdit: Graph Laplacian Optimized Cross Attention for Localized Text-Guided Image Editing"
        },
        {
            "title": "Supplementary Material",
            "content": "7. Broader Impact This simplifies to: Our work advances the precision of text-guided image editing by ensuring that modifications are both spatially consistent and semantically faithful. This improvement has the potential to benefit wide range of applicationsfrom enhancing creative workflows in digital art and advertising to supporting critical tasks in medical imaging and scientific visualizationby reducing the need for extensive manual post-processing. At the same time, the increased reliability of automated editing tools underscores the importance of establishing robust ethical guidelines for their use, particularly in contexts where the authenticity of visual information is paramount. By delivering method that better preserves the structural integrity of the source images, our approach paves the way for more trustworthy and accessible image editing solutions that can democratize creative technologies and support various high-stakes applications. 8. Proof of Lemma 1 xLx = 1 (cid:88) (cid:88) i=1 j=1 Ssym(i, j)(xi xj)2. Since Ssym(i, j) 0 (by definition of the symmetrized self-attention matrix) and (xi xj)2 0, every term in the summation is nonnegative. Therefore: xLx 0 Rn. Thus, is positive semidefinite. 9. Proof of Theorem 1 9.1. Optimization Problem We consider the following optimization problem: min xRR J(x), (6) Proof. To prove that is PSD, we must show that for any Rn, the quadratic form xLx is nonnegative: where the objective function is defined as J(x) = (x x(0))Λ(x x(0)) + λ xL x. xLx = x(D Ssym)x. Expanding this expression, we have: xLx = xDx xSsymx. (cid:80)n The degree matrix is diagonal, with entries D(i, i) = j=1 Ssym(i, j). Therefore: xDx = (cid:88) i=1 D(i, i)x2 = (cid:88) (cid:88) x2 . Ssym(i, j) i=1 j=1 The second term, xSsymx, is given by: xSsymx = (cid:88) (cid:88) i=1 j=1 Ssym(i, j)xixj. Substituting these into the quadratic form, we get: Here, the fidelity term (x x(0))Λ(x x(0)) penalizes deviations from the initial mask x(0) with stronger penalties in regions of higher confidence (as encoded by the diagonal weight matrix Λ). The smoothness term λ xL promotes spatially coherent solution by enforcing that the mask varies smoothly across similar patches, as determined by the self-attention structure. The hyperparameter λ > 0 balances the trade-off between fidelity and smoothness. 9.2. Existence and Uniqueness of the Solution To obtain the refined mask, we solve the minimization problem in Equation (6). The first term is strictly convex since Λ is positive definite, and the second term is convex because is positive semidefinite. Thus, the overall objective J(x) is strictly convex and has unique minimizer. Taking the gradient with respect to yields: xLx = (cid:88) (cid:88) i=1 j= Ssym(i, j)x2 (cid:88) (cid:88) i= j=1 Ssym(i, j)xixj. Setting J(x) = 0 gives: J(x) = 2 Λ(x x(0)) + 2λ x. Reorganizing terms: xLx = 1 2 (cid:88) (cid:88) i=1 j=1 Ssym(i, j) (cid:0)x + x2 2xixj Λ(x x(0)) + λ = 0. (cid:1) . Rearranging, we obtain: (Λ + λ L) = Λ x(0). (7) (8) (9) Since Λ + λ is positive definite, it is invertible, and the unique solution is = (Λ + λ L)1 Λ x(0). The positive semidefiniteness of ensures the convexity of the regularization term, thereby guaranteeing the existence and uniqueness of the solution. 9.3. Additional Qualitative Results This section presents qualitative results for refined masks achieved through graph Laplacian regularization and compares the editing outcomes with existing image editing methods. Figure 6. Refined masks after Graph Laplacian Regularization Source Image LOCATEdit ViMAEdit InfEdit MasaCtrl LEDITS++ photo of goat horse and cat standing on rocks near the ocean brown white tea cup and book cat sitting in the grass rocks woman with black hair and white shirt is holding phone coffee sea forest and house cute little duck marmot with big eyes woman with flowers monster around her face the two people are standing on rocks boat with fish the sun moon over an old farmhouse An asian woman with blue thick-lashed eyes and flowers on her black hair Table 4. Additional qualitative results on PIE-Bench"
        }
    ],
    "affiliations": [
        "Stony Brook University",
        "University of Waterloo"
    ]
}