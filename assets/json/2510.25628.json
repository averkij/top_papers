{
    "paper_title": "EHR-R1: A Reasoning-Enhanced Foundational Language Model for Electronic Health Record Analysis",
    "authors": [
        "Yusheng Liao",
        "Chaoyi Wu",
        "Junwei Liu",
        "Shuyang Jiang",
        "Pengcheng Qiu",
        "Haowen Wang",
        "Yun Yue",
        "Shuai Zhen",
        "Jian Wang",
        "Qianrui Fan",
        "Jinjie Gu",
        "Ya Zhang",
        "Yanfeng Wang",
        "Yu Wang",
        "Weidi Xie"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Electronic Health Records (EHRs) contain rich yet complex information, and their automated analysis is critical for clinical decision-making. Despite recent advances of large language models (LLMs) in clinical workflows, their ability to analyze EHRs remains limited due to narrow task coverage and lack of EHR-oriented reasoning capabilities. This paper aims to bridge the gap, specifically, we present EHR-Ins, a large-scale, comprehensive EHR reasoning instruction dataset, comprising 300k high-quality reasoning cases and 4M non-reasoning cases across 42 distinct EHR tasks. Its core innovation is a thinking-graph-driven framework that enables to generate high-quality reasoning data at scale. Based on it, we develop EHR-R1, a series of reasoning-enhanced LLMs with up to 72B parameters tailored for EHR analysis. Through a multi-stage training paradigm, including domain adaptation, reasoning enhancement, and reinforcement learning, EHR-R1 systematically acquires domain knowledge and diverse reasoning capabilities, enabling accurate and robust EHR analysis. Lastly, we introduce EHR-Bench, a new benchmark curated from MIMIC-IV, spanning 42 tasks, to comprehensively assess reasoning and prediction across EHR scenarios. In experiments, we show that the resulting EHR-R1 consistently outperforms state-of-the-art commercial and open-source LLMs (including DeepSeek-V3 and GPT-4o), surpassing GPT-4o by over 30 points on MIMIC-Bench and achieving a 10\\% higher zero-shot AUROC on EHRSHOT. Collectively, EHR-Ins, EHR-R1, and EHR-Bench have significantly advanced the development for more reliable and clinically relevant EHR analysis."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 2 ] . [ 1 8 2 6 5 2 . 0 1 5 2 : r EHR-R1: Reasoning-Enhanced Foundational Language Model for Electronic Health Record Analysis Yusheng Liao,1, Chaoyi Wu,1, Junwei Liu,3,4, Shuyang Jiang2,5, Pengcheng Qiu1,2, Haowen Wang3, Yun Yue3, Shuai Zhen3, Jian Wang3, Qianrui Fan3, Jinjie Gu3, Ya Zhang1,2, Yanfeng Wang1,2, Yu Wang1,2, and Weidi Xie1,2, 1Shanghai Jiao Tong University, Shanghai, China 2Shanghai Artificial Intelligence Laboratory, Shanghai, China 3Intelligence Healthcare Department, AntGroup, Hangzhou, China 4Intelligence Computing and Sensing Laboratory, Peking University, Beijing, China 5Fudan University, Shanghai, China Equal contributions Corresponding author Yu Wang: yuwangsjtu@sjtu.edu.cn; Weidi Xie: weidi@sjtu.edu.cn Electronic Health Records (EHRs) contain rich yet complex information, and their automated analysis is critical for clinical decision-making. Despite recent advances of large language models (LLMs) in clinical workflows, their ability to analyze EHRs remains limited due to narrow task coverage and lack of EHRoriented reasoning capabilities. This paper aims to bridge the gap, specifically, we present EHR-Ins, large-scale, comprehensive EHR reasoning instruction dataset, comprising 300k high-quality reasoning cases and 4M non-reasoning cases across 42 distinct EHR tasks. Its core innovation is thinkinggraph-driven framework that enables to generate high-quality reasoning data at scale. Based on it, we develop EHR-R1, series of reasoning-enhanced LLMs with up to 72B parameters tailored for EHR analysis. Through multi-stage training paradigm, including domain adaptation, reasoning enhancement, and reinforcement learning, EHR-R1 systematically acquires domain knowledge and diverse reasoning capabilities, enabling accurate and robust EHR analysis. Lastly, we introduce EHR-Bench, new benchmark curated from MIMIC-IV, spanning 42 tasks, to comprehensively assess reasoning and prediction across EHR scenarios. In experiments, we show that the resulting EHR-R1 consistently outperforms state-of-the-art commercial and open-source LLMs (including DeepSeek-V3 and GPT-4o), surpassing GPT-4o by over 30 points on MIMIC-Bench and achieving 10% higher zero-shot AUROC on EHRSHOT. Collectively, EHR-Ins, EHR-R1, and EHR-Bench have significantly advanced the development for more reliable and clinically relevant EHR analysis."
        },
        {
            "title": "1 INTRODUCTION",
            "content": "Electronic Health Records (EHRs) are comprehensive digital repositories of patient information, encompassing laboratory tests, medications, diagnoses, procedures, clinical notes, etc. [1, 2, 3, 4, 5]. Systematic analysis of EHRs is essential for modern healthcare, as accurate interpretation of patient histories enables early disease detection, personalized treatment planning, and improved clinical outcomes. In everyday clinical practice, physicians rely on electronic health records (EHRs) to address wide spectrum of analytical needs, spanning decision-support queries, for example, what is the most probable next diagnosis for this patient?, to prognostic assessments, such as what is the risk of hospital readmission within 60 days? Advanced EHR analysis systems can substantially accelerate clinical workflows and augment medical decision-making, ultimately shaping the quality and timeliness of patient care. Large language models (LLMs) have recently transformed biomedical natural language processing (BioNLP), achieving impressive results across range of medical tasks [6, 7, 8, 9, 10, 11, 12, 13], yet their performance on EHR-related tasks remains limited [14, 15, 16, 17]. Even state-of-the-art commercial models struggle to extract, integrate, and reason over EHR data, which markedly limits their interoperability with daily used hospital information systems [18] and, consequently, their wide adoption in clinical practice. Figure 1 Overview of the EHR tasks and the proposed method. a. EHR Analysis Tasks. EHR analysis tasks are defined as consisting of two types of tasks: decision-making and risk-prediction. b. Methods Overview. Our approach addresses these challenges with three-stage training pipeline. First, large volume of non-reasoning data is used for continual pre-training. This is followed by an instruction-tuning phase that leverages reasoning data. Finally, reinforcement learning with Group Reward Policy Optimization (GRPO) is applied to further refine the model. c. Results. This figure compares the performance of our model against several baseline LLMs on both decision-making and risk-prediction tasks, showcasing its superior performance. Existing work on EHR analysis with LLMs is fundamentally constrained by two major challenges. First, in terms of task coverage, prior research has focused on narrow, task-specific objectives (e.g., risk prediction for specific condition or outcome), typically restricted to particular diseases or event types, and has yet to deliver the holistic capabilities required to support evolving clinical workflows [19, 20, 21]. Second, in terms of reasoning ability, existing models struggle to construct reliable, EHR-oriented reasoning chains that demand both selective information filtering and multi-source integration. As result, they are highly susceptible to the redundancy inherent in EHR data [22, 23] and struggle to integrate isolated findings into coherent longitudinal understanding of disease progression [22, 24]. In this paper, to address these, we propose holistic framework for EHR analysis with two primary contributions: (i) large-scale super-instruction dataset that unifies diverse EHR analysis tasks into generative format and incorporates reasoning supervision, and (ii) reasoning-enhanced EHR analysis LLM trained with synthetic reasoning data via three-stage training paradigm. The overview of the EHR tasks and our proposed method are shown in Figure 1. We present EHR-Ins, large-scale dataset capturing broad task diversity and explicit medical reasoning for EHR analysis. The corpus comprises 300K high-quality reasoning cases and 3.5 million non-reasoning cases spanning 42 EHR tasks. The tasks generally fall into two categories: decision-making (e.g., diagnosis and treatment recommendations) and risk-prediction (e.g., mortality and readmission). To ensure clinical fidelity and relevance, we develop thinking-graph driven reasoning data synthesis pipeline that (i) applies statistical analysis of entity co-occurrence ratios to identify key related entities, (ii) links them via knowledge from UMLS [25], and (iii) prompts GPT-4o to produce structured, step-by-step clinical reasoning. The resulting dataset is extensive and clinically grounded, enabling models to acquire diverse, context-rich reasoning capabilities. Second, we introduce EHR-R1, family of reasoning-enhanced LLMs (up to 72B parameters) tailored for EHR analysis and trained on EHR-Ins. The training curriculum is designed to enhance domain knowledge and diverse reasoning patterns through three stages: (i) large-scale domain adaptation on extensive non-reasoning data, (ii) reasoning enhancement on high-quality reasoning cases, and (iii) reinforcement learning with Group Relative Policy Optimization (GRPO) on smaller, curated set. This multi-stage regime improves the models ability to handle complex EHR tasks and to produce accurate, clinically meaningful outputs across diverse scenarios. We evaluate EHR-R1 on EHRSHOT [19] and MIMIC-IV-CDM [26], as well as on new benchmark, EHRBench, derived from MIMIC-IV. EHR-Bench spans 42 tasks across decision-making and risk-prediction settings, providing balanced, comprehensive assessment of both reasoning and task-specific performance. Together, these benchmarks cover two distinct clinical centersStanford Medicine and Beth Israel Deaconess Medical Centerand encompass broad range of EHR analysis tasks. Experimental results demonstrate that our model consistently outperforms leading commercial and opensourced LLMs. In particular, EHR-R1-72B achieves an average performance improvement of over 30 points compared to GPT-4o across all 42 tasks on EHR-Bench, highlighting the effectiveness of our method in enhancing LLMs to tackle the full spectrum of EHR-related challenges. On the out-of-distribution EHRSHOT dataset, which features significantly different structures and medical concepts, EHR-R1-72B achieves zero-shot AUROC score that is 10% higher than baseline models. These results highlight not only the superior task adaptability of EHR-R1, but also its robustness and generalizability to unseen datasets, underscoring its potential as transformative tool for clinical decision support."
        },
        {
            "title": "2 Results",
            "content": "In the following, we first give an overview on the data quality of our constructed training data, EHR-Ins, then describe the benchmarks and evaluation metrics. Afterwards, we report the performance of our final reasoning-enhanced model, EHR-R1, highlighting its consistent gains over leading LLMs."
        },
        {
            "title": "2.1 Training Data Overview",
            "content": "To enhance the LLMs understanding of EHR data and its ability to identify relationships between medical entities within redundant EHR records, we propose novel thinking-graph pipeline to curate new largescale reasoning-enhanced EHR-analysis instruction data, EHR-Ins. This pipeline is designed to augment GPT-4o [27], enabling it to generate high-quality EHR reasoning chains. The details of this methodology can be found in Section 4.2.2. The synthesized reasoning data distribution for each analysis task in EHR-Ins is illustrated in Figure 2a, widely covering diverse decision-making tasks. To demonstrate the effectiveness of our thinking-graph pipeline in synthesizing reasoning chains for EHR analysis, we hired medical experts to manually, evaluate 100 synthesized reasoning chains across eight decisionmaking tasks to check the data quality. As shown in Figure 2b, the experts are tasked to rate each generated reasoning samples on 5-point scale. The scores from highest to lowest represent how completely the reasoning process supports the predicted label set. score of 5 indicates that the reasoning process fully supports the predicted result, while score of 1 indicates the reasoning process is completely irrelevant to the predicted result. Alongside our thinking-graph pipeline, we adopt naïve data distillation strategy with GPT-4o as 3 Figure 2 Overview of reasoning data in EHR-Ins. The sample size of each task in the reasoning data of EHR-Ins. Example of human evaluation on the reasoning data. Manual evaluation results on EHR reasoning data across eight decision-making tasks, each associated with distinct type of decision-making event. We compared the quality of synthetic reasoning data with and without thinking-graph enhancement, where *** represents significance level of < 0.001. baseline, i.e., directly prompting it (using Prompt 1) to derive reasoning chains from the original EHR analysis cases. The corresponding results are shown in Figure 2c. First, in relative comparison, the thinking-graph pipeline produced reasoning chains with more adequate EHR evidence across all eight tasks than the naïve data distillation approach. These findings demonstrate that our extracted thinking-graph yields more reliable references for GPT-4o, effectively mitigating its knowledge limitations and enhancing curated data quality. Second, in terms of absolute scores, the medical experts expressed satisfaction with most of the synthesized reasoning data, assigning it an average rating above 4 on five-point scale. This evaluation underscores the ability of the thinking-graph pipeline to effectively extract and utilize sufficient medical evidence from EHR data to support target entities, thereby substantially enhancing the capabilities of the LLM."
        },
        {
            "title": "2.2 Evaluation Setting",
            "content": "2.2.1 Benchmarks We evaluate on three benchmarks to comprehensively assess capabilities. EHR-Bench is our primary indistribution benchmark, constructed from the same MIMIC-IV source as training and providing holistic assessment across diverse EHR tasks. MIMIC-IV-CDM is also based on MIMIC-IV but uses different task 4 Figure 3 Overview of EHR-Ins and EHR-Bench.. The hierarchical ring chart displays the distribution of both datasets. The inner ring partitions tasks into two types: risk prediction and decision making. The middle ring shows 12 task categories (subtypes). The outer ring details all 42 specific tasks. formulations and preprocessingincluding filtered, denoised patient historiesand emphasizes diagnostic tasks, testing generalization to task formulation. EHRSHOT is derived from different healthcare system with distinct event types and medical entities, serving as benchmark to evaluate generalization on entirely new patient ditribution. More detailed desription and case demonstration for each dataset can be found in Section 4.4.1. EHR-Bench. To comprehensively evaluate LLM performance on EHR analysis, we introduce EHR-Bench, benchmark derived from MIMIC-IV [28]. As summarized in Figure 3, EHR-Bench spans 12 subtypes and 42 tasks, organized into two major groupsdecision making and risk predictioncovering broad spectrum of clinically relevant settings [29, 30, 31]. The decision-making tasks are generative, requiring the model to recommend the next appropriate intervention given specific medical event. We organize these into seven subtypesreassignment, service, procedure, test & exam, diagnosis, treatment, and ICU eventcovering, for example, where patient should be transferred (transfer), which tests to order (test & exam), or the likely disease (diagnosis). These subtypes comprise 24 tasks that assess the models ability to map patients current state to clear, actionable decisions. In contrast, risk-prediction tasks are binary classification problems in which the model forecasts whether significant medical event will occur within specified horizon. We group these into five subtypesmortality, readmission, length of stay, transfer, and outcome. Specifically, transfer refers to events such as patients admission or transfer to another department, and outcome aggregates severe events such as death or ICU transfer. These account for 18 tasks that probe the models capacity to identify risks from longitudinal patient data. By spanning both generative and predictive settings, EHR-Bench offers comprehensive framework that mirrors real-world EHR challenges and rigorously tests LLM reasoning and adaptability in clinical contexts. 5 MIMIC-IV-CDM. The MIMIC-IV-CDM [26] benchmark is also derived from MIMIC-IV. Compared with EHR-Bench, its distinct patient preprocessing pipeline and task design focus enable the evaluation of models generalization across domain shifts in EHR analysis tasks. Specifically, it targets diagnostic accuracy for four diseasesappendicitis, cholecystitis, diverticulitis, and pancreatitisframed as decision-making tasks. We assess two diagnostic granularities: main disease diagnosis and ICD-level full diagnosis, enabling evaluation across coarse and fine levels of clinical specificity. EHRSHOT. EHRSHOT, public dataset from Stanford Medicine, is used to evaluate models generalization to an entirely new healthcare system, which exhibits domain shifts not only in task formulations but also in population demographics, event type distributions, and recording practices. The benchmark comprises 14 risk-prediction tasks across three subtypes: operational outcomes (long length of stay, 30-day readmission, ICU transfer), anticipating lab test results (thrombocytopenia, hyperkalemia, hypoglycemia, hyponatremia, anemia), and assignment of new diagnoses (hypertension, hyperlipidemia, pancreatic cancer, celiac disease, lupus, acute MI). Following the original protocol, we assess both zero-shot performance (directly on unseen data) and few-shot adaptation given 1 to 128 examples, probing robustness and sample-efficient transfer."
        },
        {
            "title": "2.2.2 Metrics",
            "content": "We evaluate decision-making and risk-prediction tasks with metrics aligned to their outputs, more detailed formulation can be found in Section 4.4.2. Decision-making tasks are formulated as multi-label prediction problem, where the model outputs set of medical entities per sample. We report F1 score, balancing precision (fraction of predicted entities that are correct) and recall (fraction of ground-truth entities recovered). Only exact entity matches are counted as correct. To accommodate general-purpose LLMs without EHR-specific fine-tuning (e.g., GPT-4o, DeepSeek-R1), we provide candidate pool: 100 randomly sampled labels from the tasks full label space combined with the true labels for models to select, avoiding penalization for unfamiliar output spaces. Risk-prediction tasks are formulated as binary classification, which can be evaluated with Area Under the Receiver Operating Characteristic curve (AUROC). It is robust metric that measures models ability to distinguish between positive and negative outcomes across all possible classification thresholds, making it ideal for medical risk prediction where datasets can often be imbalanced. Since large language models dont have traditional classification head, we get our probability estimates by using the tokens yes and no as our positive and negative classes, under yes/no question prompt. At inference, we apply technique called logit_biases to isolate the models scores for yes and no [6]. We then normalize these scores using softmax function to get probability for the positive class, which we use to calculate AUROC. Further metric details are provided in Section 4.4.2. 2.2.3 Baselines To provide comprehensive performance comparison, we select diverse type of LLMs as our baselines. More details can be found in Section 4.4.3. Llama3.3-70B [32]: Developed by Meta, this series is family of powerful open-source models that serve as robust foundation for wide range of applications. Qwen2.5-72B [33]: As versatile, general-purpose multilingual model from Alibaba Cloud, Qwen2.5 is recognized as one of the strongest open-source models available. GPT-4o [27]: As OpenAIs flagship closed-source model, GPT-4o is powerful multimodal model known for its advanced reasoning and real-time responsiveness. Qwen3-235B [34]: The reasoning model offers unique thinking and non-thinking hybrid reasoning engine for enhanced problem-solving. DeepSeek R1 [35]: This is the largest open-source reasoning model we evaluate, primarily focused on pushing the limits of reinforcement learning to achieve state-of-the-art results in complex reasoning. GPT-OSS-120B [36]: Released by OpenAI, this open-source reasoning model provides strong 6 Figure 4 Performance comparison of EHR-R1 and nine baseline LLMs across 24 decision-making tasks on EHR-Bench. The performance is measured with F1 score. Cross-hatched bars denote reasoning-enhanced models, highlighting the effect of explicit reasoning. In each subplot, our EHR-R1 (rightmost bar) achieves clear performance advantage on nearly all tasks. reasoning and agentic capabilities to the open community. Medgemma-27B [37]: Created by Google, this is specialized medical model optimized for medical text and image comprehension, with key multimodal capability. OpenBioLLM-70B [38]: An open-source medical model meticulously fine-tuned for the biomedical field based on the Llama3-70B, consistently outperforming larger general-purpose models on biomedical benchmarks. Baichuan-M2-32B [39]: This is powerful open-source medical reasoning model based on Qwen332B. It has been specifically optimized for medical scenarios and trained from scratch on an unprecedented amount of high-quality medical data, enabling it to achieve deep medical expertise."
        },
        {
            "title": "2.3 Results on EHR-Bench",
            "content": "As shown in Figure 4 and 5, we report the performance of EHR-R1 on both decision-making and risk-prediction tasks, benchmarking against strong baselines to demonstrate the effectiveness across diverse EHR analyses."
        },
        {
            "title": "2.3.1 Decision-making Tasks",
            "content": "We show the results of evaluated LLMs with bar graph in Figure 4 and the accurate numerical performance in the Supplementary Table 1. EHR-R1-72B demonstrates superior results. It achieves an average F1 of 0.6744, outperforming the next-best model, Qwen2.5-72B (0.3535), by over 30 points. The advantage is consistent across all decisionmaking tasks, validating the effectiveness and scalability of our domain-specific, reasoning-driven approach. EHR-R1-72B also surpasses strong closed-source models (GPT-4o: 0.3155) and specialized medical LLMs (Med-Gemma-27B: 0.3157), highlighting superior clinical reasoning and EHR-specific knowledge. Strong models perform inconsistently. notable observation is that closed-source commercial models do not reliably outperform open-source ones. For example, GPT-4o (0.3155) performs the second best on average, but is often surpassed by Medgemma-27B (0.3157) and Qwen2.5-72B (0.3535) in inpatient diagnostic settings. Similarly, the general-purpose DeepSeek-R1 model (0.2974) underperformed both Medgemma-27B and Qwen2.5-72B. This variability underscores limitations in generalization for prior LLMs. In contrast, EHR-R1-72B delivers consistently strong F1 across diverse decision-making tasks, suggesting it can construct task-tailored reasoning pathways absent in existing models. General reasoning shows limited benefit. Despite the common consensus that reasoning capabilities improve model performance, existing reasoning models that are not tailored to EHR tasks actually fail to demonstrate clear advantage over non-reasoning ones. For instance, the top-performing non-reasoning model, Qwen2.5-72B (0.3535), outperforms the best-performing reasoning baseline model, Qwen3-235B (0.3418). Similarly, the non-reasoning Medgemma-27B (0.3157) achieves better score than the reasoning-enhanced DeepSeek-R1 (0.2977). This indicates that effective reasoning for EHR requires tight integration of medical knowledge with case-structured analysis rather than general chain-of-thought scaling. In summary, EHR-R1-72B not only sets new state-of-the-art for decision-making tasks in EHR analysis but also addresses critical limitations of prior models by seamlessly integrating domain-specific reasoning with medical expertise. 2.3.2 Risk-prediction Tasks EHR-R1-72B consistently leads. Figure 5 illustrates the performance of our proposed model, against several open-source baselines on 18 risk-prediction tasks, measured by ROC curves. EHR-R1-72B attains an average AUROC of 0.9523, significantly outperforming the second-best baseline model, Qwen3-235B (0.8245). The performance gains are especially pronounced in challenging Emergency Department (ED) tasks, where rapid and accurate risk assessment is critical. For instance, on the ED reattendance 3day task, EHR-R1-72B reaches 0.9007 versus Qwen2.5-72Bs 0.5540. For core clinical predictions, the model also excelsfor example, inpatient mortality at 0.9787 versus Qwen3-235Bs 0.9028. Baseline LLMs lack task versatility. The baselines performance varies significantly across task types, indicating limited generalization. Notably, Qwen2.5-72Bstrong in decision-makingtrails Qwen3-235B (0.8245) on risk prediction despite similar or smaller parameter counts, and both Qwen2.5-72B and MedGemma-27B fluctuate across tasks. These inconsistencies suggest current general-purpose and medical LLMs struggle to deliver uniformly reliable performance across the full spectrum of EHR analysis. Prediction accuracy decreases with longer horizons. All models exhibit decreasing AUROC as the prediction horizon extends, reflecting the complexity of forecasting long-term outcomes from longitudinal, multi-factor trajectories. For ICU Mortality, EHR-R1-72B drops from 0.9898 (1-day) to 0.9610 (14-day). While our model consistently outperforms baselines even on these difficult, long-range predictionsfor example, scoring 0.7562 on the Readmission 30Day task against Qwen2.5-72Bs 0.6735its clear that predicting long-term patient outcomes remains significant challenge. 8 Figure 5 Performance comparison of EHR-R1 and seven baseline LLMs on 18 risk-prediction tasks on EHRBench. Each subplot shows the ROC curves per task, with EHR-R1 highlighted in orange; the bottom-right corner of each plot reports EHR-R1-72Bs AUROC. The final Total subplot summarizes aggregated performance across all 18 tasks."
        },
        {
            "title": "2.4 Generalization Evaluation on MIMIC-IV-CDM",
            "content": "The results of our generalization evaluation on the MIMIC-IV-CDM tasks are shown in Figure 6a. We focus on the zero-shot setting, where models are directly prompted [40] to perform EHR analysis tasks in MIMICIV-CDM without any task-specific training. The prompts used for evaluation are shown in Supplementary Table 5. We evaluate LLMs performance on two levels of diagnosis tasks: the main disease diagnoses and ICD coding level diagnoses. It can be observed that our model, EHR-R1-72B, achieves the highest performance on both types of tasks. EHR-R1-72B excels on main disease prediction. For the task of predicting the main disease, which provides less noisy and more idealized scenario, our model sets new state-of-the-art with performance of 0.8913. While other powerful models like DeepSeek-R1 (0.8841) and GPT-OSS-120B (0.8793) also perform well, our model maintains slight lead. This demonstrates that our models diagnostic capabilities are highly effective even in simplified and refined clinical context. EHR-R1-72B leads in multi-level diagnoses. Most models struggle to perform well on both main disease and ICD-level coding simultaneously. For example, DeepSeek-R1 (0.8841 Main Disease) and GPT-OSS-120B (0.8793) lag on ICD Code prediction (0.2597 and 0.2422), whereas Med-Gemma-27B attains higher ICD accuracy (0.2860) but lower Main Disease (0.7939). In contrast, EHR-R1-72B, is the only one that achieves 9 Figure 6 Performance comparison of EHR-R1 and baseline LLMs in generalization evaluation. The score metric for MIMIC-IV-CDM and EHRSHOT is F1 and AUROC score, respectively. Zero-shot on MIMIC-IV-CDM: performance on Main Disease and ICD-level diagnoses using the same samples.b Zero-shot on EHRSHOT for 70B-parameters LLMs: aggregated AUROC across all subtasks within each of the three category groups, plus the overall average across all tasks. Few-shot on EHRSHOT for small-scale language models (Qwen3-1.7B vs. EHR-R1-1.7B): per-task performance acrossk {1, 2, 4, 8, 16, 32, 64, 128} shots. the highest performance on both tasks, with scores of 0.8913 for main disease and 0.3501 for ICD code. These results underscore its ability to generalize zero-shot to new system while retaining granular coding precision."
        },
        {
            "title": "2.5 Generalization Evaluation on EHRSHOT",
            "content": "We follow the EHRSHOT protocol to assess generalization in both zero-shot and few-shot scenarios: (1) zeroshot, where we directly evaluate on an unseen dataset with prompts to measure out-of-the-box generalization similar as MIMIC-IV-CDM, and (2) few-shot, where we train on small number of labeled examples to gauge how quickly LLMs adapt with minimal supervision. The prompts used for evaluation are shown in Supplementary Table 5."
        },
        {
            "title": "2.5.1 Zero-shot Setting",
            "content": "In the zero-shot setting (Figure 6b), we compare EHR-R1-72B to similarly sized ( 70B) baselines across three categoriesoperational outcomes, lab test forecasting, and new-diagnosis assignmentand the overall average. EHR-R1-72B attains the highest AUROC in every category and the overall average, outperforming strong general models (e.g., Qwen2.5-72B) and specialized medical models (e.g., Med-Gemma-27B). Given that EHRSHOT differs from EHR-Bench in both data format and task types, these results underscore robust cross-dataset generalization, which is critical feature for practical clinical applications."
        },
        {
            "title": "2.5.2 Few-shot Setting",
            "content": "Considering that model adaptation in clinical settings is often constrained by limited computational resources. Therefore, in the few-shot setting, we use small-parameter 1.7B LLMs with minimal number of training samples (at most 128) to evaluate the performance of the LLMs in rapid adaptation scenarios. EHR-R1 shows better learning efficiency. Figure 6c and the accompanying table present detailed performance comparison between our small model, EHR-R1-1.7B, and its base model, Qwen3-1.7B, on variety of EHRSHOT tasks under few-shot fine-tuning setting. Following EHRSHOT, the models are fine-tuned on limited number of examples (k) for each task, ranging from 1 to 128. The results show the profound superiority of our model, which achieves an average AUROC of 0.7465 at = 128, far exceeding Qwen3-1.7Bs average of 0.5998 at the same shot setting. Performance gap grows with more data. Our models performance advantages are primarily manifested in two types of tasks: operational outcomes and lab test forecasting. When < 16, the gap is modestlikely because extremely limited data under-utilizes large models. For 16, the gap widens markedly, suggesting that EHR-focused training equips EHR-R1-1.7B to leverage small-to-moderate datasets more efficiently. General models narrow the gap on diagnosis tasks. For new-diagnosis prediction, the gap peaks around = 16 (e.g., new hypertension: 0.6593 vs. 0.5108 AUROC for EHR-R1-1.7B vs. Qwen3-1.7B) but narrows as increases. This likely reflects that disease diagnosis is core medical task extensively represented in general-model pretraining; with sufficient fine-tuning data, general models can catch up. In summary, our model not only achieves higher performance but also improves faster as data scales, indicating superior learning efficiency and stronger EHR-specific knowledge base."
        },
        {
            "title": "2.6 Ablation Study",
            "content": "We conduct ablation experiments to validate the rationale and effectiveness of our proposed framework from three key perspectives: the impact of incorporating reasoning data into model training, the effectiveness of reasoning during inference compared to direct answer output, and the generality of our approach across model scales. Experiments design. We conducted an ablation analysis on five configurations, progressively adding our training and inference components (Figure 7): (i) BaseModel: the original base model with direct answers (no reasoning at inference, no training enhancements). (ii) BaseModel (w/ reasoning inference): the base model evaluated with test-time reasoning to guide predictions. (iii) EHR-R1 (w/o reasoning training): the model has undergone our continual pre-training to specialize in the EHR domain, but without reasoning data; inference uses direct answers. (iv) EHR-R1 (full training): our three-stage modelcontinual pretraining, reasoning-data training, and task-specific fine-tuningwith direct-answer inference. (v) EHR-R1 (full training, w/ reasoning inference)): the full model augmented with test-time reasoning. Note that, 11 Figure 7 Ablation experiments results on decision making tasks of EHR-Bench. The figure show 7 categories of sub-type decision making tasks and the average performance of 5 variant of our methods, including Base Model, Base Model (with reasoning inference), EHR-R1 (w/o reasoning training), EHR-R1 (full training), and our final model EHR-R1 (full training, w/ reasoning inference), which showcase the incremental performance gains from each stage of our training pipeline. Qwen2.5-72B is not applied with BaseModel (w/ reasoning inference) because it is not reasoning-enabled model. The accurate experimental results can be found in the Supplementary Table 2. Effectiveness of reasoning data. Incorporating reasoning data into training consistently improves performance: for EHR-R1-1.7B, direct-answer F1 rises from 0.5060 (without reasoning data) to 0.5300 (with reasoning data), and for EHR-R1-72B from 0.6039 to 0.6281, indicating that synthesized reasoning injects useful EHR-specific knowledge. The gains are amplified when reasoning is also used at inference, with the full EHR-R1-1.7B reaching 0.5438 F1 compared to 0.5060 without reasoning data, demonstrating that training-time reasoning and test-time reasoning are complementarytraining equips the model with reasoning primitives, and inference-time reasoning leverages them to further refine predictionsyielding more knowledgeable and robust foundation across inference strategies. Effectiveness of reasoning inference. Reasoning at inference only helps when the model has been explicitly equipped with EHR-specific reasoning. Applying general reasoning prompts to base models offers little-tonegative benefit (Qwen3-1.7B drops from 0.1624 to 0.1456 F1; Qwen3-8B also drops from 0.2425 to 0.2286), underscoring that general reasoning alone is insufficient for clinical tasks. In contrast, our fully trained EHR-R1 models gain substantially from reasoning inference: EHR-R1-1.7B improves from 0.5060 to 0.5438 (0.0035), EHR-R1-8B from 0.5549 to 0.5894, and EHR-R1-72B from 0.6039 to 0.6418. These results validate the effectiveness of our thinking-graph synthesis pipeline in instilling domain-specific reasoning pathways that inference-time reasoning can reliably exploit. Effectiveness across model scales. Our approach generalizes robustly with scale: applying the same pipeline from 1.7B to 72B parameters yields monotonic gains, with average F1 rising from 0.5438 (Qwen31.7Bbased) to 0.5894 (Qwen3-8Bbased) and 0.6481 (Qwen2.5-72Bbased). This consistent improvement indicates that the benefits of reasoning-data training and reasoning inference are not scale-specific but instead compound as model capacity increases, confirming that our framework scales effectively across parameter regimes."
        },
        {
            "title": "3 Discussion",
            "content": "Despite the remarkable progress of LLMs across diverse medical tasks, their application to clinical EHR analysis remains significant challenge [14, 15, 16, 17]. Existing approaches fall short in two critical dimensions. First, task coverage remains narrow, as most efforts are confined to specific objectives or disease cohorts rather than enabling holistic analytical support for evolving clinical workflows. Second, reasoning ability is underdeveloped, with current LLMs struggling to generate reliable EHR-focused reasoning processes that progressively highlight key records, integrate fragmented evidence, and construct longitudinal, synthesized understanding of patient health."
        },
        {
            "title": "Overview of Our Approach",
            "content": "We introduce thinking-graphdriven, auto-generation pipeline for EHR reasoning that systematically converts raw, longitudinal records into structured, query-ready insights for generative LLM training. The pipeline: (1) extracts salient medical entities from heterogeneous EHR sources to create clinical focused insights; (2) links these entities into thinking graph that encodes temporal relations and causal hypotheses, transforming disjoint timesteps into coherent longitudinal narrative; and (3) synthesizes explicit, stepwise reasoning over this graph to support adaptive solutions across diverse clinical queries. This pipeline enables to curate EHRIns, super-instruction corpus with 300K reasoning cases and 3.5M non-reasoning cases spanning 42 EHR tasks. We then train our reasoning-enhanced model, EHR-R1, via three-stage training, including large-scale domain adaptation, reasoning enhancement followed by reinforcement learning with Group Relative Policy Optimization (GRPO) to further strengthen analytical fidelity, temporal reasoning, and clinical robustness. Main Contribution super-instruction dataset for EHR analysis with reliable reasoning. We release EHR-Ins, large-scale super-instruction corpus that pairs 300K high-quality reasoning cases with 3.5M non-reasoning cases across 42 EHR tasks. Reasoning traces are auto-generated via our thinking-graph pipeline, which extracts entities, links temporal/causal relations, and synthesizes clear stepwise rationalestransforming structured EHR data into LLM-ready, actionable insights. This design enables models to both answer diverse clinical queries and explain their decisions. First comprehensive benchmark for EHR analysis. This paper introduces MIMIC-Bench, new benchmark based on the MIMIC-IV dataset, which provides comprehensive evaluation of LLMs on 42 diverse EHR tasks. This benchmark is designed to holistically assess models ability to handle the full range of diverse clinical queries that exist in real-world EHRs, addressing coverage gaps in existing evaluations. robust and generalizable foundation model. We introduce EHR-R1, state-of-the-art model that demonstrates superior performance on EHR analysis. By learning from the explicit reasoning paths in our data, our model not only sets new performance benchmarks on the proposed MIMIC-Bench but also shows exceptional zero-shot generalization capabilities on the out-of-distribution EHRSHOT dataset. This highlights EHR-R1s robust ability to translate EHR content into clinical insight and understand temporal data to form longitudinal narrative, which is critical for real-world clinical applications. Key Findings Effective handling of diverse tasks. Trained on our EHR-Ins dataset, EHR-R1 performs broad range of EHR tasks with high accuracy. Across 42 tasks in MIMIC-Benchincluding clinical coding and other less-common or challenging tasksit delivers strong, adaptive reasoning over diverse clinical queries. In addition, its zero-shot performance on unseen EHR data and tasks from the EHRSHOT dataset confirms that the model captures transferable structure rather than memorizing training data. Effectiveness of reasoning for EHR tasks. The gains derive from an auto-synthesis pipeline that leverages relationships among medical entities to teach explicit reasoning over structured EHR data. This enables the model to denoise heterogeneous inputs, analyze temporal relations, and construct longitudinal clinical narratives. Applying reasoning chains at inference yields substantial improvements, supporting the value of explicit, knowledge-informed reasoning. Generalizability of our framework. The effectiveness of our method is not limited to specific model scale. Improvements from our training framework and reasoning data are consistent for models from 1.7B to 13 72B parameters. This finding demonstrates the generalizability and scalability of our approach, confirming that it provides robust and widely applicable solution for the EHR domain."
        },
        {
            "title": "Limitations and Future Work",
            "content": "Scope of reasoning data. Our reasoning-data synthesis was applied only to decision-making tasks, for which target labels include the medical entities necessary for building the thinking graph. This constrains coverage of diverse clinical queries. Although stage-three reinforcement learning transferred some of these capabilities to risk-prediction tasks, future work should explore procedures to construct explicit reasoning data for these binary classification. Data attrition during graph construction. We excluded many samples for which the thinking graph could not be builteither due to insufficiently related entity pairs or unsuccessful retrieval of medical relations from UMLS. This limits our ability to convert temporal data into longitudinal narrative for every case. Refining graph-building heuristics, incorporating additional or domain-specific knowledge bases, and improving entity linking could reduce data loss and broaden case coverage. Specialization versus breadth. The approach yields specialized EHR model and may attenuate general-purpose capabilities. Emphasizing structured temporal narratives and prescribed reasoning steps risks overfitting to clinical settings. Future work should explore strategies to balance specialization and breadthfor example, multi-domain continued pretraining, alternating-task curricula, modular adapters, or mixture-of-experts routingto preserve general capabilities while deepening clinical expertise."
        },
        {
            "title": "4 Methods",
            "content": "In this section, we present the details of our method, starting with the problem formulation, followed by the reasoning data curation pipeline, model training, and finally, evaluation."
        },
        {
            "title": "4.1 Problem Formulation",
            "content": "We formulate the EHR analysis problems in an instruction-tuning-based [41] generative framework. An EHR can be represented as chronologically ordered sequence of longitudinal records (also referred to as events), including laboratory results, medication administrations, transfers, etc.: = {r1, r2, . . . , rK}. (1) The k-th record is defined as rk = (ck, Ek, tk), where ck denotes the clinical event category and represents the set of all possible event types, such as medication, diagnosis, laboratory test, etc.. The term Ek represents the set of medical entities contained in the event (e.g., test items and results in laboratory event, or diagnoses in diagnostic event). Finally, tk is the timestamp of the event. At any given prediction time step t, training sample for EHR analysis is expressed as = (I, Rt, A), where is free-text instruction specifying the analysis task, Rt = {rk tk t} is the observable history of the EHR up to time t, and is the ground-truth free-text answer. Our objective is to train an LLM, ΦLLM, to perform the following conditional text generation task: that is, to output an appropriate answer given the task instructions and the observed EHR history Rt. ΦLLM(I, Rt) A, (2)"
        },
        {
            "title": "4.2 Data Curation",
            "content": "In this section, we introduce the curation pipeline of EHR-Ins and EHR-Bench. We begin by processing the raw MIMIC-IV EHR cases, filtering and reorganizing them into an instruction-tuning generative format. 14 Figure 8 Overview of Data curation pipeline. The pipeline for our data curation process begins with the original MIMIC-IV dataset. From there, we establish EHR-Ins with reasoning enhanced EHR analysis instructions through thinking-graphdriven reasoning synthesis, along with new comprehensive MIMIC-Bench. The data is then carefully split into training and testing sets, where the test set forms EHR-Bench, and the training set is further enhanced with reasoning, resulting in the final EHR-Ins. The overview of the preprocessing pipeline is shown in Figure 8. 4.2.1 MIMIC-IV Processing Here, we describe the construction process of both EHR-Ins and EHR-Bench. First, we introduce the processing details of MIMIC-IV. Then, we describe the free-text formatting of the EHR data, which transforms the long-horizon EHRs into detailed generative training samples. Finally, we describe the approach to achieve balanced distribution of the data with label-wise weighted sampling methods and how to split the whole dataset into training and test set. Information Enrichment. To enable chronological construction from EHR data, we reformat and sort the original MIMIC-IV dataset at patient level. We first extract all events for each patient and reordered them by timestamp with second precision. Recognizing that diagnoses icd, procedure icd, and diagnosis event types in MIMIC-IV lack second-level timestamps, we manually added them to enrich the information. In particular, for diagnoses icd and diagnosis, we associate them with their respective admission events via hadm id. We then set diagnoses icd to 1 minute before the discharge event and diagnosis to 1 minute before the emergency department discharge event. For procedure icd events, which only provide day-level timestamps, we default 15 their time to 23:59:59 on that day. To further enrich the EHR information, we map the International Classification of Diseases (ICD) codes in the diagnoses icd and procedure icd tables to Clinical Classifications Software (CCS) categories using the ICD-to-CCS script1. Likewise, we translated National Drug Codes (NDCs) in the prescriptions and pharmacy tables into Anatomical Therapeutic Chemical (ATC) classification codes adopting the off-the-shelf mapping script2. These enrichment consolidate the originally vast and heterogeneous sets of disease, procedure, and medication codes into clinically coherent categories. We also identify that information within discharge events, such as social history and chief complaint, as being observable upon admission. We move these details to the admission event to provide more comprehensive input for LLMs. Additionally, to mitigate potential data leakage in EHR data, we mask pharmacy event information within prescription events. After the above processing and enrichment, all MIMIC-IV data are organized into chronological sequence of records for each patient. Training Sample Formatting Using the structured EHRs, we construct an instruction dataset on EHR analysis by defining detailed instructionanswer pairs for each training sample, enabling generative training, regarding the two EHR analysis task categories: decision-making and risk-prediction. Decision-making tasks involve predicting the next-step event based on the history of events. Therefore, naturally, for given EHR sequence = {r1, r2, . . . , rK}, we can generate training samples by sampling an arbitrary timestep and predicting the next event. Assuming the next-step record as rt+1 = (ct+1, Et+1, tt+1), we formulate the instruction for decision-making task according to the type of next-step event: ck+1 (detailed prompts can be found in Supplementary Table 4), and the target answer set is composed of the entities contained in Ek+1: = {y1, y2, . . . , yN }, where yi Ek+1, {1, , }, (3) For example, in case where the next-step event is diagnoses, Ek+1 contains all the diagnosis results of the patients and yi represents the name of disease. In the implementation, is formatted as free-text by concatenating the entity list with str.join function in Python. In addition, at the time step t, we can also construct risk-prediction samples. We organize the instruction of the risk-prediction taks by filling {ccritic, } into predefined template (details can be found in Supplementary Table 4), where the objective of the model is to determine whether critical outcome event of type ccritic will occur within specified time frame . Therefore, the binary ground truth answer of the task can be formulated as: = yes, if there exists {t + 1, . . . , K} such that ci = ccritic and ti < T, (4) no, otherwise. Thus far, we have organized training samples in the form = (I, Rt, A), encompassing EHR analysis tasks that fall into two main categories: decision-making and risk-prediction. Notably, since our instructions are generated through prompt filling, all training samples associated with specific task (e.g., diagnostic decision-making) share the same instruction. Hence, in the following, can also be used to denote specific task. Lastly, to convert the tabular observed EHR data Rt into format that LLMs can process, we serialize the structured data[42, 43] into Markdown format[21], which is widely present in LLM pre-training corpora, facilitating better model comprehension. Specifically, each event is represented as plain text with title (event name and standardized start time) and content. The content uses bullet-point syntax for events with single records and table syntax for others with multiple records, as shown in Case 1 and Case 2, respectively. The free-text case of EHR is shown in Case 5. 1ICD9 to CCS: https://hcup-us.ahrq.gov/toolssoftware/ccs/ccs.jsp ICD10 to CCSR: https://hcup-us.ahrq.gov/toolssoftware/ccsr/dxccsr.jsp 2NDC to ATC: https://github.com/sunlabuiuc/PyHealth 16 Label-wise Weighted Sampling. Because the constructed dataset over all valid EHR time steps contains large number of samples and the target candidate sets in MIMIC-IV are highly imbalanced, we apply sampling to form the final training and test sets. We apply weighted sampling to rebalance the target label distribution in the training set. In practice, for decision-making tasks, given certain task prompted by instruction I, we first count the frequency of all possible label entities appeared in the target answer and use the reciprocal of the frequency as the sampling weight. Due to that, each training samples answer label is list of entities, and we assign weight based on the average of its constituent labels. Formally, for sample with ground-truth answer as = {y1, y2, . . . , yN }, where is the size of the label set, its weight wS is given by: wS ="
        },
        {
            "title": "1\nN",
            "content": "N i="
        },
        {
            "title": "1\nCountI(yi)",
            "content": ", (5) where CountI(yi) represents the frequency of label entity yi in same type of task. This strategy can also be applied to risk-prediction tasks, where the entities in answering contain only two binary items (yes or no, thus =1). The weighting formula, therefore, simplifies to the reciprocal of the frequency of the samples single label, rebalancing the positive and negative classes. The label-wise weighted sampling strategy is adopted to generate the train and test sets simultaneously. This approach ensures that the EHR-Ins maintains balanced distribution of target labels while addressing the issue of class imbalance. Besides, the test set can also cover diverse types of target labels, which makes the EHR-Bench evaluating the performance of the models more comprehensive. Data Split. Prior studies commonly treat each hospital admission (which may come from the same patient) as an independent sample for data splitting. However, in our setting, to prevent potential information leakage between the training and test sets, we adopted rigorous patient-level data split. This approach ensures that no patient, identified by their unique subject_id, appears in both the training and test sets. Furthermore, due to context length constraints and computational resources, we limit the observable historical time window to 24 hours. To ensure an appropriate length for historical event trajectories, we filter out samples with fewer than 10 or more than 100 historical events. This process prevents issues of insufficient information or excessively long inputs. From the entire training dataset, we sample cases using label-wise weighting scheme to ensure balanced representation across all tasks. This process yielded 3.5M samples designated as non-reasoning data and an additional 300K samples specifically for synthesizing reasoning data. These two sets collectively form our proposed super-instruction dataset, EHR-Ins. Similarly, test set of 21K samples was created from the whole test set using the same label-wise sampling approach to form our benchmark, EHR-Bench. 4.2.2 Thinking-Graph-Driven Reasoning Synthesis In this section, to enable the model to provide convincing reasoning in clinical scenarios, we further enhance the answers in EHR-Ins with detailed rationales by incorporating the concept distribution in the EHR and external medical knowledge sources. We mainly consider the decision-making tasks. We first explain how to identify medical entities that are potentially related to the task labels. We then filter these entities and adopt bidirectional graph searching on external medical knowledge bases to gather the thinking graph, which links the context entities and target entities with medical relations. Finally, we describe how to synthesize the reasoning subset for decision-making tasks in EHR-Ins. The overview of the proposed reasoning synthesis pipeline is shown in Figure 9. Entity Relevance Identification. Compared with regular medical data, distinctive feature of EHRs is their tabular format, where most records are clearly indexed by list of well-defined medical entities. Therefore, by analyzing the relationships among these entities across the training set, we can uncover potential reasoning pathways that trace key observed entities from noisy raw EHRs to task labels. In particular, we first process some unstructured exception free-text records, such as discharge summaries and 17 Figure 9 Overview of Thinking-Graph-Driven Reasoning Synthesis. The whole pipeline comprises three parts: Entity Relevance Identification analyzes electronic health records to calculate co-exist matrix and Lift Score for medical entities. Graph-based Medical Relation Search filters the entity pair with higher lift score and conducts bidirectional graph search on large-scale medical knowledge graph, UMLS; The red lines indicate the search paths Reasoning Path Synthesis arguments GPT-4o with retrieved medical relations to generate coherent reasoning path. The entity with the color box in the reasoning chain indicates the context entity or the target labels. examination reports. We adopt QuickUMLS3 to extract the meaningful medical entities from the raw texts. Then, all the medical entities in the observable events can be expressed as Et = k} is the entity set of the i-th events. ti<t Ei, Ei = {ei 2, . . . , ei 1, ei Then we count the co-occurrence frequencies of the context entity ei Et with the target label entities yj for specific decision-making task denoted by its instruction and calculate the Lift [44] between them. The Lift metric can discover strong associations within the data by measuring how much more frequently two entities appear together than expected by chance, allowing us to pinpoint the most relevant entities for the reasoning pathways. In our method, the Lift metric is calculated as: Lift(ei, yj, I) = PI(ei, yj) PI(ei)PI(yj) = CountI(ei, yj)CountI CountI(ei)CountI(yj) , ei Et, yj (6) where CountI(ei, yj) represents the co-occurrence frequency of ei and yj in the training set for task and CountI represents the total number of samples in task I. Since rare occurrences of ei and yj (for example, 3https://github.com/Georgetown-IR-Lab/QuickUMLS 18 occurring only once) can disproportionately inflate the Lift score, we only retain co-occurrence pairs where both the frequency of ei and yj are greater than 5. Finally, we retain only the co-occurrence pairs where the Lift value is greater than 5 for each sample: KLift = {(ei, yj) CountI(ei) > 5, CountI(yj) > 5, Lift(ei, yj, I) > 5}, ei Et, yj (7) This filtering process helps to reduce noise while ensuring that the remaining entity-label pairs have strong potential association, thus providing candidate references for evidence in the subsequent reasoning process. Graph-based Entity Relation Search. Although Lift can uncover potentially related medical entities from the historical EHR events, the medical relationships between these entities (including both the context entities in input and the label entities) and the labels remain uncovered and thus fail to capture the underlying clinical associations. To address this issue, we leverage the Unified Medical Language System (UMLS) [25] knowledge graph, which provides structured representation of medical concepts and their relationships. UMLS integrates wide range of medical terminologies and ontologies, including SNOMED CT, ICD, and MeSH, among others, making it an ideal resource to validate and strengthen the medical relationships between entities and labels in EHR data. Specifically, we map all medical entities identified in the EHR to their corresponding UMLS concepts. To increase the accuracy and completeness of this mapping, we use fuzzy matching provided in UMLS [25], allowing us to match medical terms in the EHR with UMLS concepts even when there are slight variations in spelling, abbreviations, or synonyms. This approach ensures broader and more robust set of entity-to-concept mappings, improving the coverage of medical entities in the EHR data. For entities that cannot be mapped to UMLS concepts, we discard the related co-occurrence pairs, as these entities would not contribute valid medical relationships and could introduce noise into the analysis. After mapping entities to UMLS, we search for medical relations between entity pairs, progressively linking context input entities towards the final label entities, compensating the missed concept nodes, forming instance specific thinking-graph. However, many medical entities in UMLS do not have direct associations, which makes it challenging to acquire their clinical relationships. To overcome this, we employ bidirectional graph search algorithm to find connections between entity pairs by traversing across multiple nodes and links. This process allows us to uncover implicit medical relationships between entities that are not directly linked in the UMLS knowledge graph. The final discovered relationships between the entities are then extracted to form the thinking-graph to enhance the subsequent synthesis of reasoning paths for this training case: KThink-Graph = {Path(ei, yj) Path(ei, yj) UMLS, (ei, yj) KLift} (8) where the function Path(ei, yj) is the connection between ei and yj extracted from UMLS. Reasoning Path Synthesis With the relationships between label entities and historical event entities for each sample, we leverage powerful large language model, GPT-4o, to generate detailed reasoning chain based on the historical events, entity relationships, and label entities. The prompt used for the reasoning synthesis are shown in Prompt 2. Specifically, the reasoning chain consists of three stages: extraction, reasoning, and final result. In the extraction stage, the model is required to extract relevant information from the historical events based on the tasks objective. This extraction process must specify the events and their corresponding timestamps to clearly identify the source of the information, ensuring the accuracy and traceability of the extracted data. The reasoning stage requires the model to integrate the extracted information and, using the entity relationship data, link the extracted entities to the corresponding label entities. The model then constructs coherent reasoning path that logically connects the historical events to the predicted label, providing an explanation for the clinical decision. In the final result stage, the model outputs the predicted result based on the reasoning path constructed in the previous stage. This output represents the models interpretation of the task based on the historical data and reasoning process. It is important to note that each sample may contain multiple label entities. For those label entities for which the model cannot construct valid reasoning chain, the synthesized data may introduce hallucinations. To address this issue, we instruct GPT-4o to generate reasoning chains only for the label entities that are logically inferable from the available data. For those label entities that cannot be reasoned through, we discard the associated reasoning chains and results. Finally, we retain only those samples where the number of inferable label entities constitutes at least 70% of the original label entities. This threshold ensures that the data distribution remains close to the original raw data and minimizes the potential bias introduced by the discarded entities. Figure 1 shows an example of the reasoning synthesis process."
        },
        {
            "title": "4.3 Model Training",
            "content": "In this section, we detail the training procedure. The proposed model follows three-stage training process: large-scale domain adaptation to learn the data distribution and task knowledge in the EHR, reasoning enhancement to extract key information from observable historical events and infer prediction results, and reinforcement learning to enhance the models reasoning capabilities on EHR data. We first serialize the training sample = (I, Rt, A) into the final input-output generative format, which can be represented as (X , A). The input is constructed by concatenating the markdown format of EHR historical events with the task-specific instruction I: = Concate(Markdown(Rt), I) (9) Concurrently, the target output is formed by joining all items from the corresponding label set, with each item separated by newline character. Large-scale Domain Adaptation. The initial step in our training pipeline is to adapt the base LLM to the clinical domain of EHR data by instruction-tuning on naïve question-answering pairs without reasoning. In our experiments, we found that supervising the model on both the input and the target labels during this phase yields markedly better performance on downstream EHR tasks than training on alone. We believe this improvement stems from the fact that encodes rich distributional and temporal patterns of patient trajectories, which guide the model to learn co-occurrence and sequence dependencies more effectively. Formally, we minimize the combined loss: LSFT(ΦLLM) = i=1 log ΦLLM(AiA<i, ) j= log ΦLLM(XjX<j) (10) Reasoning Enhancement. Following the domain adaptation phase, we conduct instruction-tuning on the reasoning samples aiming at teaching the model how to perform explicit reasoning on EHR analysis tasks. To achieve this, we adopt the reasoning data generated by our automatic synthesis pipeline (as detailed in Section 4.2.2). While the input remains the patients observable events and the task instruction, the output is an augmented sequence that contains both detailed reasoning chain encapsulated within the <think> and </think> tags and the final result. The training objective for this stage remains the same as that of large-scale domain adaptation to ensure consistency and reinforce the models ability to learn from both the input sequence and the structured reasoning output. Reinforcement Learning. Despite the gains achieved through instruction-tuning, the models reasoning capacity remains limited by the quality of the generated data. To further unlock its reasoning potential, we apply Group Reward Policy Optimization (GRPO) on top of the SFT checkpoint, with the expectation that end-to-end reinforcement learning will guide the model to self-explore improved reasoning trajectories. GRPO frames each reasoning chain as sequential decision process, rewarding trajectories that yield correct predictions and coherent, evidence-grounded explanations. By exploring multiple candidate chains and reinforcing the most effective ones, the model learns to prioritize high-quality inference paths. Specifically, we design GRPOs reward function RGRPO(τ ) for the inference reasoning trajectory τ as sum of two components: format reward and accuracy reward: 20 Table 1 Training hyperparameters for three base models. In Instruction Tuning, all models use high learning rate and large batch sizes for efficient pre-training. For Reasoning Tuning and GRPO, models train for more epochs (3 and 2 respectively) to compensate for smaller, specialized datasets. During GRPO, larger models (Qwen3-8B and Qwen2.5-72B) adopt smaller learning rates (5e-7) to ensure training stability given their extensive parameters. Finally, Qwen2.5-72Bs shorter maximum sequence length (8192) across Reasoning Tuning and GRPO is necessary compromise due to computational resource limitations. Training Stage Model LR Batch Size Epoch Max Seq Len GPU Num Instruction Tuning Reasoning Tuning GRPO Qwen3-1.7B Qwen3-8B 2e-5 2e-5 Qwen2.5-72B 2e-5 Qwen3-1.7B Qwen3-8B 2e-5 2e-5 Qwen2.5-72B 2eQwen3-1.7B Qwen3-8B 2e-6 5e-7 Qwen2.5-72B 5e-7 128 256 512 256 256 256 128 32 32 1 1 3 3 3 2 2 2 8192 8192 8192 12288 12288 8192 12288 12288 8192 64 128 128 128 128 128 128 128 R(τ, ˆA, A, I) = λfmt Rfmt(τ ) + λacc Racc( ˆA, A, I) (11) where ˆA is the predicted entity set of LLMs, is the target entity set, and is the type of EHR analysis task. The format reward Rfmt(τ ) checks that whether the model wraps its chain of thought between the tags <think> and </think> and respects the three prescribed stages: Extraction, Reasoning, and Final Result: Rfmt(τ ) = ( 1, 0, if the format of τ is right otherwise (12) The accuracy reward Racc(τ ) uses the task-specific metric. ACC for the risk-prediction task and F1 for the decision-making task are used to score the final prediction against the ground truth: Racc( ˆA, A, I) = ( ACC( ˆA, A), F1( ˆA, A), if is decision-making Task if is Risk Prediction Task (13) To guard against runaway drift and hallucination, we restrict GRPO updates to just 500 examples per task, ensuring that the model refines its reasoning without straying too far from the validated chains generated during instruction tuning. This targeted reinforcement step produces model that not only reasons more flexibly over EHR data but also maintains the reliability and interpretability of its clinical explanations. Implementation Details For our experiments, we adopt Qwen3-1.7B, Qwen3-8B, and Qwen2.5-72B [34] as our base models. The same three stage training strategy us applied to all three models. All the training are conduct based on the code of Transformers4 and TRL5 framework. For GRPO stage, The weighted hyperparameters of rewards λfmt and λacc are both set to 1 and the number of rollout is 8 per sample. More detail training hyperparameters are shown in Table 1."
        },
        {
            "title": "4.4 Evaluation",
            "content": "Lastly, we introduce the evaluation settings, including used benchmarks, considered baselines, and metrics. 4https://github.com/huggingface/transformers 5https://github.com/huggingface/trl"
        },
        {
            "title": "4.4.1 Benchmarks",
            "content": "In this section, we present the benchmarks considered in this paper and describe how we reformatted them to align with our generative evaluation pipeline, accompanied by detailed case demonstrations. EHR-Bench. In EHR-Bench, weve included two categories of tasks. One of these, decision-making tasks, requires the model to predict multiple possible target answers. For evaluation, prediction is only considered correct if the models predicted medical entities precisely match those in the target answer. However, since most baseline models arent fine-tuned on MIMIC-IV data, we generate candidate set by randomly sampling from the tasks target label set. Baseline models then simply select medical entities from this candidate set. Its worth noting that our proposed model, EHR-R1, can directly generate answers without needing this candidate set, demonstrating its practical usability. To ensure the difficulty for baseline models selecting from options is comparable to EHR-R1 directly generating answers, we set the candidate set size for each sample to 100 (including the number of target labels). If task has fewer than 100 target label types, we provide all available target labels for the model to choose from. The example is shown in Case 5. MIMIC-IV-CDM Benchmark. MIMIC-IV-CDM meticulously selected four diseasesappendicitis, cholecystitis, diverticulitis, and pancreatitisto test model diagnostic accuracy. Unlike EHR-Bench and EHRSHOT, the historical event information in MIMIC-IV-CDM is not arranged chronologically. Instead, it extracts the most recent information for each event to serve as context. Consequently, the context in MIMIC-IV-CDM is less noisy, and the task is simpler. To maintain consistency with the other two benchmarks, we also reorganized the MIMIC-IV-CDM data into Markdown-formatted free text. Given that the information within it lacks timestamps, the title for each event only includes the event name without the time it occurred. The example case of the free-text input is shown in Case 4. EHRSHOT Benchmark. EHRSHOT, public dataset collected by Stanford University, comprises electronic health records from 7,000 patients. Its publicly available portion includes 14 risk-prediction tasks categorized into three subtypes: Operational Outcomes, Anticipating Lab Test Results, and Assignment of New Diagnoses. EHRSHOT is originally designed for traditional machine learning models, presents challenge for Large Language Models (LLMs) due to its direct use of various medical codes, which are inherently difficult for LLMs to interpret. To address this, weve reformatted the EHRSHOT data into Markdown-like free-text, similar to EHR-Bench, enabling direct testing with LLMs. Specifically, we utilized the descriptive mapping lexicon provided within EHRSHOT to convert item codes into natural language text. However, certain codes, such as CPT4 and ICD10PCS, could not be directly transformed. Following previous work on evaluating LLMs with EHRSHOT [21], we manually added extra mapping files to resolve these inconsistencies6. Furthermore, for EHRSHOTs measurement and observation events, we semantically clustered the 24 most frequently occurring items. We then enriched these items with additional information, including their units, normal value ranges, and anomaly indicators, significantly enhancing the informational content of EHRSHOT. The example case of the free-text input is shown in Case 3. 4.4.2 Metrics Based on the two high-level EHR analysis task types, i.e., decision-making and risk-prediction, different evaluation metrics are adopted to quantitatively reflect the performance of LLMs, as formulated below. Decision-Making Tasks. For each decision-making sample, let the ground-truth target set be and the models predicted set be ˆA. We adopt the precision, recall, F1 score to evaluate prediction accuracy in this multi-label setting: Precision = ˆA ˆA , Recall = ˆA , F1 = 2 Precision Recall Precision + Recall . 6CPT4 mapping: https://gist.github.com/lieldulev/439793dc3c5a6613b661c33d71fdd185 ICD10PCS mapping: https://hcup-us.ahrq.gov/toolssoftware/ccsr/PRCCSR_v2025-1.zip (14) 22 Risk-Prediction Tasks. For risk-prediction tasks, we follow prior work [19] and adopt the area under the receiver operating characteristic curve (AUROC) as the evaluation metric. AUROC measures models ability to distinguish between positive and negative outcomes across all decision thresholds, and can be written as AUROC = 1 0 TPR(t) dFPR(t), (15) where TPR and FPR denote the true positive and false positive rates, respectively."
        },
        {
            "title": "4.4.3 Baselines",
            "content": "The following LLMs are used as baselines for comparison: Qwen2.5/3 Series. Developed by Alibaba Cloud, the Qwen2.5/3 series [34] offers versatile, general-purpose multilingual models with unique thinking and non-thinking hybrid reasoning engine and Mixture-ofExperts (MoE) architecture for efficient inference. Trained on 36 trillion tokens across over 100 languages, Qwen3 excels in general reasoning, coding, and agent capabilities. While not specifically medical, its robust multilingual support and advanced reasoning could serve as powerful general foundation for processing diverse medical literature or assisting in global health information dissemination. Medgemma 4B/27B. Created by Google, MedGemma [37] is collection of Gemma 3 variants specifically optimized for medical text and image comprehension. Its key innovation lies in its multimodal capability, seamlessly integrating natural language processing with computer vision to analyze medical images (e.g., X-rays, histopathology) alongside textual patient data. Trained on extensive de-identified medical datasets, MedGemma demonstrates superior performance on clinical benchmarks like MedQA and is designed to accelerate healthcare AI applications while supporting local deployment for data privacy. Llama3 Series. The Llama3 Series [32], developed by Meta, comprises family of powerful open-source general-purpose LLMs ranging from 8 billion to 405 billion parameters, with evolving multimodal capabilities in Llama 3.2. Trained on over 15 trillion tokens, these models excel in broad applications, including complex reasoning, coding, and multilingual understanding. While not inherently specialized for medicine, the Llama3 architecture serves as robust foundation, with fine-tuned variants like Me-LLaMA demonstrating superior performance in various medical question-answering and summarization tasks, highlighting its adaptability for domain-specific healthcare solutions. OpenBioLLM. Developed by Saama AI Labs, OpenBioLLM [38] is an 8-billion-parameter open-source language model specifically designed for the biomedical field, built upon the Llama 3 architecture. It is meticulously fine-tuned on high-quality biomedical data, enabling it to accurately understand and generate text for tasks like clinical entity recognition, medical question answering, and patient data de-identification. OpenBioLLM consistently outperforms larger general-purpose models on diverse biomedical benchmarks, making it highly effective and transparent choice for healthcare and life sciences applications. Baichuan M2 32B. Baichuan-M2 [39], from Baichuan Intelligent, stands out as the first open-source LLM specifically optimized for medical scenarios, trained \"from scratch\" on an unprecedented 20 trillion tokens of high-quality medical and general data. This dedicated training, coupled with multi-stage curriculum learning approach, enables it to achieve deep medical expertise, outperforming models five times its size in clinical practice benchmarks while maintaining strong general capabilities. Its specialized focus makes it highly relevant for tasks such as clinical decision support, medical education, and rare disease diagnosis. GPT-4o. OpenAIs GPT-4o (omni) [27] is flagship multimodal model designed for natural human-computer interaction, accepting and generating any combination of text, audio, image, and video through single, end-to-end neural network . It offers real-time responsiveness and enhanced tokenization for non-English languages, making it faster and more cost-effective. While general-purpose model, its advanced multimodal understanding and reasoning capabilities could be applied to complex medical cases involving diverse data types, from patient consultations (audio/text) to diagnostic images. 23 DeepSeek R1. Developed by DeepSeek, R1 [35] is an open-source AI model released in January 2025, primarily focused on pushing the limits of reinforcement learning as post-training technique for complex reasoning. Utilizing Mixture-of-Experts (MoE) architecture, it achieves high accuracy in mathematics, programming, and general reasoning tasks at significantly lower operating cost. Notably, DeepSeek R1 has shown strong performance on medical benchmarks like MedQA [45], demonstrating its potential for efficient and accurate reasoning in healthcare contexts. GPT-OSS. The GPT-OSS (Open-Source Series) [36] is OpenAIs first open-source GPT-class model since GPT-2, released under an Apache 2.0 license to provide powerful reasoning and agentic capabilities to the open community. The models, gpt-oss-120b and gpt-oss-20b, utilize Mixture-of-Experts (MoE) architecture for efficient inference and support long 128k token context window. While general-purpose reasoning model, its performance on complex, health-related queries is noteworthy; the gpt-oss-120b model achieved score of 30% on the HealthBench benchmark, outperforming OpenAIs own o4-mini and o3-mini models on this specific task. This demonstrates its potential as strong, customizable foundation for variety of applications, including those with intricate medical reasoning requirements."
        },
        {
            "title": "5 Data Availability",
            "content": "The data source for EHR-Bench and EHR-Ins derives from MIMIC-IV, public EHR resource. Both datasets are under review of PhysioNet7."
        },
        {
            "title": "6 Code Availability",
            "content": "All source codes of this paper have been released in https://github.com/MAGIC-AI4Med/EHR-R1. 7https://physionet.org/"
        },
        {
            "title": "References",
            "content": "[1] Seyedmostafa Sheikhalishahi, Vevake Balaraman, and Venet Osmani. Benchmarking machine learning models on multi-centre eicu critical care dataset. Plos one, 15(7):e0235424, 2020. [2] Hugo Yèche, Rita Kuznetsova, Marc Zimmermann, Matthias Hüser, Xinrui Lyu, Martin Faltys, and Gunnar Rätsch. Hirid-icu-benchmark - comprehensive machine learning benchmark on high-resolution ICU data. In Joaquin Vanschoren and Sai-Kit Yeung, editors, Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual, 2021. [3] José Roberto Ayala Solares, Francesca Elisa Diletta Raimondi, Yajie Zhu, Fatemeh Rahimian, Dexter Canoy, Jenny Tran, Ana Catarina Pinho Gomes, Amir Hossein Payberah, Mariagrazia Zottoli, Milad Nazarzadeh, Nathalie Conrad, Kazem Rahimi, and Gholamreza Salimi Khorshidi. Deep learning for electronic health records: comparative review of multiple deep neural architectures. J. Biomed. Informatics, 101:103337, 2020. [4] Feng Xie, Jun Zhou, Jin Wee Lee, Mingrui Tan, Siqi Li, Logasan S/O Rajnthern, Marcel Lucas Chee, Bibhas Chakraborty, An-Kwok Ian Wong, Alon Dagan, Marcus Eng Hock Ong, Fei Gao, and Nan Liu. Benchmarking emergency department triage prediction models with machine learning and large public electronic health records. In AMIA 2022, American Medical Informatics Association Annual Symposium, Washington, DC, USA, November 5-9, 2022. AMIA, 2022. [5] Shirly Wang, Matthew B. A. McDermott, Geeticka Chauhan, Marzyeh Ghassemi, Michael C. Hughes, and Tristan Naumann. Mimic-extract: data extraction, preprocessing, and representation pipeline for MIMIC-III. In Marzyeh Ghassemi, editor, ACM CHIL 20: ACM Conference on Health, Inference, and Learning, Toronto, Ontario, Canada, April 2-4, 2020 [delayed], pages 222235. ACM, 2020. [6] Harsha Nori, Nicholas King, Scott Mayer McKinney, Dean Carignan, and Eric Horvitz. Capabilities of gpt-4 on medical challenge problems. arXiv preprint arXiv:2303.13375, 2023. [7] Karan Singhal, Shekoofeh Azizi, Tao Tu, Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, et al. Large language models encode clinical knowledge. Nature, 620(7972):172180, 2023. [8] Yusheng Liao, Yutong Meng, Yuhao Wang, Hongcheng Liu, Yanfeng Wang, and Yu Wang. Automatic interactive evaluation for large language models with state aware patient simulator. arXiv preprint arXiv:2403.08495, 2024. [9] Pengcheng Qiu, Chaoyi Wu, Shuyu Liu, Weike Zhao, Zhuoxia Chen, Hongfei Gu, Chuanjin Peng, Ya Zhang, Yanfeng Wang, and Weidi Xie. Quantifying the reasoning abilities of llms on real-world clinical cases. arXiv preprint arXiv:2503.04691, 2025. [10] Chaoyi Wu, Pengcheng Qiu, Jinxin Liu, Hongfei Gu, Na Li, Ya Zhang, Yanfeng Wang, and Weidi Xie. Towards evaluating and building versatile large language models for medicine. npj Digit. Medicine, 8(1), 2025. [11] Yusheng Liao, Shuyang Jiang, Yanfeng Wang, and Yu Wang. Reflectool: Towards reflection-aware toolaugmented clinical agents. In Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar, editors, Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2025, Vienna, Austria, July 27 - August 1, 2025, pages 1350713531. Association for Computational Linguistics, 2025. [12] Suhana Bedi, Hejie Cui, Miguel Fuentes, Alyssa Unell, Michael Wornow, Juan Banda, Nikesh Kotecha, Timothy Keyes, Yifan Mai, Mert Oez, et al. Medhelm: Holistic evaluation of large language models for medical tasks. arXiv preprint arXiv:2505.23802, 2025. 25 [13] Juncheng Wu, Wenlong Deng, Xingxuan Li, Sheng Liu, Taomian Mi, Yifan Peng, Ziyang Xu, Yi Liu, Hyunjin Cho, Chang-In Choi, et al. Medreason: Eliciting factual medical reasoning steps in llms via knowledge graphs. arXiv preprint arXiv:2504.00993, 2025. [14] Hossein Rouhizadeh, Anthony Yazdani, Boya Zhang, David Vicente Alvarez, Matthias Hüser, Alexandre Vanobberghen, Rui Yang, Irene Li, Andreas Walter, and Douglas Teodoro. Large language models struggle to encode medical conceptsa multilingual benchmarking and comparative analysis. medRxiv, pages 202501, 2025. [15] Yixiang Qu, Yifan Dai, Shilin Yu, Pradham Tanikella, Travis Schrank, Trevor Hackman, Didong Li, and Di Wu. Enhancing llms with smart preprocessing for ehr analysis. arXiv preprint arXiv:2412.02868, 2024. [16] Ali Soroush, Benjamin Glicksberg, Eyal Zimlichman, Yiftach Barash, Robert Freeman, Alexander Charney, Girish Nadkarni, and Eyal Klang. Large language models are poor medical codersbenchmarking of medical code querying. NEJM AI, 1(5):AIdbp2300040, 2024. [17] Weijieying Ren, Jingxi Zhu, Zehao Liu, Tianxiang Zhao, and Vasant Honavar. comprehensive survey of electronic health record modeling: From deep learning approaches to large language models. arXiv preprint arXiv:2507.12774, 2025. [18] Yinghao Zhu, Yifan Qi, Zixiang Wang, Lei Gu, Dehao Sui, Haoran Hu, Xichen Zhang, Ziyi He, Liantao Ma, and Lequan Yu. Healthflow: self-evolving ai agent with meta planning for autonomous healthcare research. arXiv preprint arXiv:2508.02621, 2025. [19] Michael Wornow, Rahul Thapa, Ethan Steinberg, Jason Fries, and Nigam Shah. Ehrshot: An ehr benchmark for few-shot evaluation of foundation models. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems, volume 36, pages 6712567137. Curran Associates, Inc., 2023. [20] Jiacheng Lin, Zhenbang Wu, and Jimeng Sun. Training llms for ehr-based reasoning tasks via reinforcement learning. arXiv preprint arXiv:2505.24105, 2025. [21] Stefan Hegselmann, Georg von Arnim, Tillmann Rheude, Noel Kronenberg, David Sontag, Gerhard Hindricks, Roland Eils, and Benjamin Wild. Large language models are powerful electronic health record encoders. arXiv preprint arXiv:2502.17403, 2025. [22] Zhenbang Wu, Anant Dadu, Mike Nalls, Faraz Faghri, and Jimeng Sun. Instruction tuning large language models to understand electronic health records. Advances in Neural Information Processing Systems, 37:5477254786, 2024. [23] Ziyi Chen, Mengyuan Zhang, Mustafa Mohammed Ahmed, Yi Guo, Thomas George, Jiang Bian, and Yonghui Wu. Narrative feature or structured feature? study of large language models to identify cancer patients at risk of heart failure. In AMIA Annual Symposium Proceedings, volume 2024, page 242, 2025. [24] Samuel Kim, In Gu Sean Lee, Mijeong Irene Ban, and Jane Chiang. Detection and prediction of clopidogrel treatment failures using longitudinal structured electronic health records. arXiv preprint arXiv:2310.08757, 2023. [25] Olivier Bodenreider. The unified medical language system (umls): integrating biomedical terminology. Nucleic acids research, 32(suppl_1):D267D270, 2004. [26] Paul Hager, Friederike Jungmann, Robbie Holland, Kunal Bhagat, Inga Hubrecht, Manuel Knauer, Jakob Vielhauer, Marcus Makowski, Rickmer Braren, Georgios Kaissis, and Daniel Rueckert. Evaluation and mitigation of the limitations of large language models in clinical decision-making. [27] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. 26 [28] Alistair EW Johnson, Lucas Bulgarelli, Lu Shen, Alvin Gayles, Ayad Shammout, Steven Horng, Tom Pollard, Sicheng Hao, Benjamin Moody, Brian Gow, et al. Mimic-iv, freely accessible electronic health record dataset. Scientific data, 10(1):1, 2023. [29] Pawel Renc, Michal K. Grzeszczyk, Nassim Oufattole, Deirdre Goode, Yugang Jia, Szymon Bieganski, Matthew B. A. McDermott, Jaroslaw Was, Anthony E. Samir, Jonathan W. Cunningham, David W. Bates, and Arkadiusz Sitek. Foundation model of electronic medical records for adaptive risk estimation. CoRR, abs/2502.06124, 2025. [30] Pawel Renc, Yugang Jia, Anthony E. Samir, Jaroslaw Was, Quanzheng Li, David W. Bates, and Arkadiusz Sitek. Zero shot health trajectory prediction using transformer. npj Digit. Medicine, 7(1), 2024. [31] Mehak Gupta, Brennan Gallamoza, Nicolas Cutrona, Pranjal Dhakal, Raphael Poulain, and Rahmatollah Beheshti. An extensive data processing pipeline for MIMIC-IV. In Antonio Parziale, Monica Agrawal, Shalmali Joshi, Irene Y. Chen, Shengpu Tang, Luis Oala, and Adarsh Subbaswamy, editors, Machine Learning for Health, ML4H 2022, 28 November 2022, New Orleans, Lousiana, USA & Virtual, volume 193 of Proceedings of Machine Learning Research, pages 311325. PMLR, 2022. [32] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurélien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Rozière, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Graeme Nail, Grégoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel M. Kloumann, Ishan Misra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, and et al. The llama 3 herd of models. CoRR, abs/2407.21783, 2024. [33] Qwen Team. Qwen2 technical report. arXiv preprint arXiv:2407.10671, 2024. [34] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. [35] DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, and S. S. Li. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. CoRR, abs/2501.12948, 2025. [36] OpenAI. Introducing gpt-oss. https://openai.com/index/introducing-gpt-oss/. Accessed: 2025-08-08. 27 [37] Andrew Sellergren, Sahar Kazemzadeh, Tiam Jaroensri, Atilla Kiraly, Madeleine Traverse, Timo Kohlberger, Shawn Xu, Fayaz Jamil, Cían Hughes, Charles Lau, et al. Medgemma technical report. arXiv preprint arXiv:2507.05201, 2025. [38] Felix Dorfner, Amin Dada, Felix Busch, Marcus Makowski, Tianyu Han, Daniel Truhn, Jens Kleesiek, Madhumita Sushil, Jacqueline Lammert, Lisa Adams, et al. Biomedical large languages models seem not to be superior to generalist models on unseen medical data. arXiv preprint arXiv:2408.13833, 2024. [39] Bingning Wang, Haizhou Zhao, Huozhi Zhou, Liang Song, Mingyu Xu, Wei Cheng, Xiangrong Zeng, Yupeng Zhang, Yuqi Huo, Zecheng Wang, et al. Baichuan-m1: Pushing the medical capability of large language models. arXiv preprint arXiv:2502.12671, 2025. [40] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. [41] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. [42] Qian Liu, Bei Chen, Jiaqi Guo, Morteza Ziyadi, Zeqi Lin, Weizhu Chen, and Jian-Guang Lou. Tapex: Table pre-training via learning neural sql executor. In International Conference on Learning Representations, 2022. [43] Peng Li, Yeye He, Dror Yashar, Weiwei Cui, Song Ge, Haidong Zhang, Danielle Rifinski Fainman, Dongmei Zhang, and Surajit Chaudhuri. Table-gpt: Table fine-tuned gpt for diverse table tasks. Proceedings of the ACM on Management of Data, 2(3):128, 2024. [44] Rakesh Agrawal, Tomasz Imieliński, and Arun Swami. Mining association rules between sets of items in large databases. In Proceedings of the 1993 ACM SIGMOD international conference on Management of data, pages 207216, 1993. [45] Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, and Peter Szolovits. What disease does this patient have? large-scale open domain question answering dataset from medical exams. Applied Sciences, 11(14):6421, 2021."
        },
        {
            "title": "7 Acknowledgments",
            "content": "This work is supported by the National Key R&D Program of China (No. 2022ZD0160702), and the Scientific Research Innovation Capability Support Project for Young Faculty (ZY-GXQNJSKYCXNLZCXM-I22)."
        },
        {
            "title": "8 Author Contributions",
            "content": "All listed authors clearly meet the ICMJE 4 criteria. Y.L., C.W., and J.L. contribute equally to this work. Y.W. and W.X. are the corresponding authors. Specifically, Y.L., C.W., J.L., S.J., P.Q., H.W., Y.Y., S.Z., J.W., Q.F., J.G., Y.Z., Y.W., Y.W., and W.X. all make contributions to the conception or design of the work, and Y.L., C.W., J.L. further perform acquisition, analysis, or interpretation of data for the work. In writing, Y.L., C.W., J.L. draft the work. S.J., P.Q., H.W., Y.Y., S.Z., J.W., Q.F., J.G., Y.Z., Y.W., Y.W., and W.X. review it critically for important intellectual content. All authors approve of the version to be published and agree to be accountable for all aspects of the work to ensure that questions related to the accuracy or integrity of any part of the work are appropriately investigated and resolved."
        },
        {
            "title": "9.1 Details of Experiments",
            "content": "Supplementary Table 1 Accurate results of decision making tasks in EHR-Bench. The performance are measured with the metric F1 score and shown in in the format mean std. Tasks Qwen2.5-72B OpenBioLLM-70B Baichuan-M2-32B Medgemma-27B GPT-OSS-120B DeepSeek-R1 Qwen3-235B EHR-R1-72B Llama3.3-70B GPT-4o Admissions OMR EMAR Procedures ICD Procedures CCS Diagnoses ICD Diagnoses CCS Labevents Microbiologyevents Services Transfers POE Radiology Prescriptions Prescriptions ATC Medrecon Medrecon ATC ED Diagnoses ICD ED Diagnoses CCS Ingredientevents Datetimeevents Procedureevents Inputevents Outputevents 0.14380.0169 0.05690.0059 0.29410.0156 0.17370.0085 0.16340.0109 0.24320.0088 0.39720.0107 0.45580.0152 0.19150.0091 0.20550.0185 0.29970.0182 0.22760.0101 0.21150.0136 0.25120.0110 0.23750.0128 0.25510.0114 0.25240.0099 0.17400.0076 0.25930.0098 0.66130.0124 0.36590.0164 0.07450.0077 0.35040.0158 0.67300. 0.31510.0208 0.05820.0080 0.55430.0213 0.16930.0138 0.27180.0196 0.17670.0116 0.28470.0121 0.16670.0108 0.24370.0183 0.24210.0188 0.36860.0217 0.17160.0147 0.29430.0197 0.45330.0218 0.26860.0183 0.32100.0159 0.33830.0137 0.25830.0150 0.38770.0173 0.58260.0146 0.34380.0196 0.09380.0128 0.39790.0204 0.81070.0170 0.14520.0172 0.15910.0092 0.50870.0227 0.20490.0156 0.25570.0158 0.28050.0129 0.45290.0122 0.47880.0123 0.28260.0174 0.18880.0150 0.36970.0227 0.24010.0121 0.24180.0192 0.33900.0141 0.39960.0183 0.30300.0133 0.40060.0128 0.23200.0131 0.35950.0127 0.75370.0101 0.50960.0206 0.10300.0108 0.45160.0192 0.82310.0154 0.07870.0122 0.05980.0077 0.25580.0166 0.08480.0118 0.06890.0106 0.07860.0074 0.18800.0119 0.07020.0070 0.04580.0093 0.09020.0129 0.26680.0203 0.17930.0144 0.13640.0164 0.27720.0192 0.14430.0162 0.01710.0048 0.02710.0049 0.06910.0087 0.13650.0127 0.46620.0117 0.29560.0192 0.12310.0146 0.15650.0163 0.57450.0200 0.06330.0074 0.13730.0088 0.26390.0114 0.12720.0073 0.14660.0086 0.18800.0067 0.33660.0080 0.40010.0126 0.14940.0078 0.08600.0083 0.18310.0092 0.14570.0083 0.14080.0090 0.21240.0086 0.18480.0091 0.23430.0102 0.26470.0093 0.14210.0064 0.22710.0079 0.51760.0077 0.19310.0099 0.06550.0059 0.24570.0085 0.40490.0087 0.21720.0164 0.17390.0099 0.38580.0183 0.20010.0103 0.20470.0116 0.26810.0088 0.37600.0099 0.49010.0138 0.33250.0159 0.20600.0174 0.36260.0221 0.18800.0091 0.25830.0169 0.27050.0124 0.27390.0137 0.24360.0113 0.33450.0133 0.15600.0076 0.23210.0076 0.71310.0119 0.42800.0181 0.16280.0150 0.40300.0181 0.69490.0184 0.10290.0141 0.08350.0092 0.41180.0198 0.20740.0172 0.29710.0186 0.09720.0078 0.22930.0137 0.18780.0096 0.22500.0200 0.25990.0199 0.30350.0211 0.12200.0139 0.25400.0198 0.22710.0158 0.16170.0149 0.11390.0102 0.10020.0100 0.22310.0144 0.42400.0151 0.49140.0165 0.05720.0082 0.07870.0122 0.26210.0159 0.75560. 0.17280.0166 0.01150.0032 0.52900.0225 0.27700.0173 0.30640.0174 0.20580.0119 0.34510.0131 0.26250.0107 0.26270.0158 0.28340.0202 0.35000.0219 0.16990.0126 0.29820.0205 0.31570.0194 0.23160.0174 0.29610.0149 0.28420.0159 0.25140.0154 0.39820.0153 0.57040.0166 0.15570.0131 0.08840.0115 0.27050.0193 0.80120.0176 0.09740.0134 0.19890.0107 0.58010.0179 0.19760.0139 0.29070.0160 0.24700.0111 0.39920.0123 0.47400.0135 0.28880.0140 0.19380.0191 0.39680.0195 0.22800.0123 0.23830.0163 0.37610.0162 0.29740.0134 0.25450.0133 0.40030.0128 0.23100.0110 0.36690.0118 0.74890.0110 0.40890.0167 0.11920.0108 0.39110.0152 0.77940.0168 0.75120.0186 0.92420.0059 0.65030.0192 0.40790.0189 0.53400.0189 0.40880.0123 0.56540.0134 0.66810.0178 0.71010.0215 0.84510.0169 0.76340.0153 0.54820.0200 0.55420.0210 0.85600.0116 0.78910.0138 0.33190.0131 0.39810.0142 0.43190.0168 0.56740.0149 0.87390.0080 0.97580.0043 0.83770.0179 0.87630.0129 0.91760.0116 Average 0.27580.0025 0.31550. 0.35350.0031 0.16210.0027 0.21080.0018 0.31570.0027 0.23650.0035 0.29740. 0.34180.0025 0.67440.0029 Supplementary Table 2 Accurate results of ablation experiments. The table presents the performance of different model configurations across various decision-making tasks. We evaluate our method on base models of different sizes, including Qwen3-1.7B, Qwen3-8B, Qwen2.5-72B, and Qwen3-235B. The performances are measured with the metric F1 score and shown in the format mean std, with the average performance across all tasks provided in the final column. Methods Transfer Service Procedure Test&Exam Diagnose Treatment ICU Event Average Decision Making Tasks Base Model Base Model (w/ reasoning inference) EHR-R1 (w/o reasoning training) EHR-R1 (full training) EHR-R1 (full training, w/ reasoning inference) 0.13320.0087 0.15300.0107 0.65100.0144 0.62570.0153 0.68630.0148 0.11660.0059 0.09960.0093 0.49530.0127 0.61060.0134 0.62320.0150 0.08200.0071 0.10680.0087 0.30370.0119 0.26630.0121 0.29240.0136 0.12920.0053 0.09930.0055 0.52510.0081 0.61350.0089 0.63500. 0.13440.0035 0.14930.0055 0.33190.0071 0.33020.0066 0.36100.0075 0.12250.0032 0.11820.0051 0.47560.0078 0.49160.0067 0.50350.0075 0.31190.0064 0.23740.0066 0.77730.0071 0.80310.0072 0.82800.0060 0.15440.0020 0.13930.0024 0.50600.0034 0.53000.0029 0.54380.0035 Qwen3-1.7B Qwen3-8B Base Model Base Model (w/ reasoning inference) EHR-R1 (w/o reasoning training) EHR-R1 (full training) EHR-R1 (full training, w/ reasoning inference) 0.15850.0119 0.23700.0123 0.66700.0144 0.66810.0145 0.70910.0138 0.14450.0080 0.18150.0091 0.53720.0145 0.56610.0117 0.65580.0124 0.16650.0098 0.20830.0083 0.36170.0132 0.31920.0122 0.35310.0135 0.20820.0062 0.16390.0062 0.57930.0078 0.56810.0088 0.67640.0092 0.24780.0048 0.28900.0056 0.39170.0070 0.38770.0071 0.40620. 0.24110.0062 0.19030.0062 0.51550.0073 0.53480.0070 0.55230.0071 0.43580.0077 0.35270.0061 0.82490.0069 0.84620.0057 0.85210.0063 0.24250.0029 0.22860.0026 0.55490.0038 0.56160.0031 0.58940.0037 Qwen2.5-72B Base Model EHR-R1 (w/o reasoning training) EHR-R1 (full training) EHR-R1 (full training, w/ reasoning inference) 0.25640.0142 0.70490.0151 0.71760.0155 0.73790. 0.21460.0092 0.55720.0131 0.66230.0140 0.69050.0113 0.22830.0116 0.43680.0133 0.43220.0126 0.44890.0143 0.28880.0074 0.64120.0085 0.67970.0085 0.70130.0092 0.33100.0053 0.43040.0077 0.45060.0071 0.47020.0070 0.39000.0074 0.56190.0072 0.57960.0070 0.59180.0060 0.52890.0072 0.88080.0061 0.88400.0060 0.88940. 0.33230.0031 0.60390.0037 0.62810.0034 0.64180."
        },
        {
            "title": "9.2 Details of EHR-Bench",
            "content": "Supplementary Table 3 Details of task information in EHR-Bench. This table presents comprehensive overview of the 42 tasks, categorized into 2 main Task Types and further divided into 12 SubTypes. The Target Event column specifies the type of event associated with the target label for each task. For decision making tasks, the Item Name indicates the column name in MIMIC-IV where the target label is located. Its important to note that Risk Prediction tasks do not have an Item Name as their target labels are not specific entities. Columns marked with * represent additional columns created through manual mapping. The Candidates column shows the size of the target candidate set for each task. Task Admissions Transfer Target Event Item Name Metric Candidates admissions transfer admission_type eventtype OMR Labevents Microbiologyevents Radiology omr labevents microbiologyevents radiology Task Type SubType Reassignment Test & Exam Diagnoses Decision Making Procedures Services Treatments ICU Event Diagnose ICD Diagnose CCS Diagnosis ICD Diagnosis CCS Procedures ICD Procedures CCS Services POE EMAR Prescriptions Prescriptions ATC Medrecon Medrecon ATC Ingredientevents Datetimeevents Procedureevents Inputevents Outputevents Transfer ED Hospitalization ED ICU Tranfer 12hour Critical Outcomes ED Critical Outcomes Readmission Risk Prediction LengthOfStay Mortality Readmission 30day Readmission 60day ICU Readmission ED Reattendance 3day LengthOfStay 3day LengthOfStay 7day ICU Stay 7day ICU Stay 14day ED Inpatient Mortality Inpatient Mortality ICU Mortality 1day ICU Mortality 2day ICU Mortality 3day ICU Mortality 7day ICU Mortality 14day result_name item_name test_name exam_name diagnoses CCT Type* icd_title CCS Type* procedures CCS Type* curr_service order_type medication drug ATC Type* name ATC Type* item_name item_name item_name item_name item_name - - - - - - - - - - - - - - - - - - diagnose_icd diagnose_icd diagnosis_icd diagnosis_icd procedures_icd procedures_icd services poe emr prescriptions prescriptions medrecon medrecon ingredientevents datetimeevents procedureevents inputevents outputevents edstays edstays edstays discharge discharge icustays edstays admissions admissions icustays icustays edstays admissions icustays icustays icustays icustays icustays F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 AUROC AUROC AUROC AUROC AUROC AUROC AUROC AUROC AUROC AUROC AUROC AUROC AUROC AUROC AUROC AUROC AUROC AUROC 8 39 11 698 165 961 24467 279 13171 271 11098 18 15 4153 9233 913 18641 899 15 137 138 222 63 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2"
        },
        {
            "title": "Prompt",
            "content": "Prompt 1. Reasoning Chain Evaluation Instruction for Clinicians Task Description: Evaluation of Clinical Reasoning Chain Validity I. Background We have developed large language model (LLM) designed to extract key information from electronic health records (EHRs) and perform clinical reasoning based on that information. For evaluation purposes, we have hidden the full EHR context, presenting only the critical information extracted by the model and the final diagnostic conclusion. Your task is to assess the relationship and logical consistency between the extracted information and the final conclusion. II. Data Description The models response consists of three main sections: ##Extraction, ##Reasoning, and ##Final Result. You need to evaluate whether the content in ##Extraction and ##Reasoning adequately supports the ##Final Result and provide your final score in the Score column. III. Evaluation Objective Using the provided extracted information, you must determine if it is sufficient and accurate to support the models final diagnostic conclusion. Specifically, you need to confirm: - Whether the key information extracted by the model is relevant to the diagnosis and accurately expressed. - Whether the final diagnostic conclusion is fully supported by the extracted information, without any missing essential evidence. Note: The models reasoning result is considered correct and does not need to be validated for correctness. IV. Evaluation Process including the extracted information and the diagnostic Please read the entire reasoning chain, conclusion. Based on your professional judgment, evaluate the degree of correspondence between the information and the conclusion. The scoring criteria are as follows (five levels): - 5 points (Very Satisfied): The extracted information is mostly accurate and largely supports the conclusion. - 4 points (Satisfied): The extracted information supports most of the conclusion. small part of the conclusion lacks evidence but is reasonably justified. - 3 points (Average): The extracted information supports only small part of the conclusion. The final result contains large portion of content that is not adequately supported. - 2 points (Poor): The extracted information is related to the final result, but it fails to support the conclusion. - 1 point (Very Poor): The extracted information is highly irrelevant to the diagnostic conclusion. The conclusion lacks reasonable basis, and the reasoning process is unreliable. Scoring Differences Explained: - The main difference between 5 points and 4 points is whether the information extracted by the model fully supports the final result. - The difference between 5 points and 4 points is whether the final conclusion is fully supported by the extracted information. - The difference between 4 points and 3 points is whether large part or small part of the final conclusion is supported. - The difference between 3 points and 2 points is whether at least one conclusion is supported. - The difference between If so, its 3 points; if not, but the information is related, its 2 points. 31 2 points and 1 point is whether the extracted and reasoned information is relevant to the final conclusion. V. Evaluation Examples Example 1 (5 points, Lab Test) 1. Task Type: Predict the lab tests the patient should undergo. 2. Model Reasoning Chain: ##Extraction: Source: Outpatient Record [2178-09-23 11:20:00] - Key Information: Patient complains of fatigue and pale skin/mucous membranes. - Analysis: Typical signs of anemia, requiring clarification of anemia type. Source: Lab Results [2178-09-22 15:45:00] - Key Information: Hemoglobin 8.2 g/dL (normal 13-17), Creatinine 2.1 mg/dL (normal 0.7-1.3). - Analysis: Moderate to severe anemia combined with abnormal kidney function suggests possible renal anemia. Source: Past Medical History [2177-05-12 09:00:00] - Key Information: History of chronic kidney disease (CKD stage 3), not on dialysis. - Analysis: Chronic kidney disease often leads to renal anemia. Erythropoietin levels need to be assessed. Source: Past Surgical History [2176-08-05 10:00:00] - Key Information: Appendectomy, good recovery. - Analysis: Not directly related to the current anemia evaluation. This is irrelevant information. ##Reasoning: Combining \"anemia symptoms + significantly low hemoglobin + abnormal kidney function + chronic kidney disease,\" the following lab tests are recommended to clarify the anemia mechanism: Serum iron Ferritin Transferrin saturation Erythropoietin levels ##Final Result: Recommended Tests: Serum iron, Ferritin, Transferrin saturation, Erythropoietin levels. 3. Score Explanation: Score: 5 points Reasoning: The first three pieces of extracted information (\"fatigue + pale skin,\" \"Hemoglobin 8.2 + Creatinine 2.1,\" \"chronic kidney disease history\") fully support the necessity of performing tests for iron metabolism and erythropoietin levels. The \"past appendectomy\" is extraneous information unrelated to the conclusion, but it does not affect the correctness of the conclusion. All key points of the conclusion are fully justified by the first three essential pieces of information, thus score of 5 points. Example 2 (4 points, Medication Prescription) 1. Task Type: Predict the patients medication plan. 2. Model Reasoning Chain: ##Extraction: 32 Source: Inpatient Record [2180-02-15 08:10:00] - Key Information: Type 2 diabetes mellitus, fasting blood glucose 12.5 mmol/L, postprandial blood glucose 16.0 mmol/L. - Analysis: Blood glucose is significantly high; the current oral medication regimen is insufficient. Source: Lab Results [2180-02-14 07:50:00] - Key Information: HbA1c 8.5%, Creatinine 60 µmol/L (normal). - Analysis: High HbA1c indicates the need for improved blood glucose control. Normal kidney function means GLP-1 agonists are viable option. Source: Past Medication History [2179-10-10 09:00:00] - Key Information: Taking oral Metformin 1000 mg twice daily and Gliclazide 80 mg once daily. - Analysis: The current dual-drug therapy has failed to achieve the target, requiring an upgrade. ##Reasoning: Poor glycemic control (fasting 12.5, postprandial 16.0, HbA1c 8.5%) Needs escalation of antidiabetic treatment. Normal kidney function GLP-1 agonists are safe option. Has history of hypertension and is taking Amlodipine Needs to continue antihypertensive therapy. ##Final Result: Prescription: Add GLP-1 receptor agonist, continue Amlodipine treatment. 3. Score Explanation: Score: 4 points Reasoning: Support for adding GLP-1: The information \"fasting 12.5 mmol/L, HbA1c 8.5%\" clearly suggests that the current oral medications are insufficient, and an escalation to GLP-1 is reasonable. Additionally, \"normal kidney function\" proves this drug can be used safely. This part of the conclusion aligns perfectly with the extracted information. Support for continuing Amlodipine: The extracted information \"history of hypertension and is taking Amlodipine\" indicates the patient is already on antihypertensive treatment. This sub-conclusion is also directly supported by the extracted information. Overall: In this case, both parts of the conclusion\"add GLP-1\" and \"continue Amlodipine\"are supported by the corresponding information. The models mention of \"continuing Amlodipine\" is reasonable inference (most patients with T2DM have concurrent hypertension) but is not directly proven by the extracted information. Therefore, the majority of the conclusion (the GLP-1 part) is fully supported by the information, while small part of the conclusion (continuing Amlodipine) is reasonable but unsupported inference. This fits the criteria for \"4-point\" scenario. Example 3 (3 points, Diagnosis) 1. Task Type: Predict the patients diagnosis. 2. Model Reasoning Chain: ##Extraction: Source: Outpatient Record [2179-05-10 10:30:00] - Key Information: Patient complains of sore throat, cough, fever, and fatigue for 3 days. - Analysis: Suggests symptoms of an upper respiratory tract infection (URTI). Source: Physical Examination [2179-05-10 10:45:00] - Key Information: Temperature 38.5C, pharyngeal congestion, bilateral tonsillar enlargement (Grade II). - Analysis: Supports acute tonsillitis or acute pharyngitis. 33 Source: Lab Results [2179-05-10 11:00:00] - Key Information: White blood cell count 12.5109/L (normal 4-10), Neutrophils 80% (normal 50-75%). - Analysis: Both white blood cells and neutrophils are elevated, suggesting bacterial infection. ##Reasoning: Sore throat, fever, cough, temperature 38.5C, pharyngeal congestion, tonsillar enlargement Suggests URTI. Elevated white blood cells and neutrophils Suggests bacterial infection. Combining the above information, the diagnosis is \"acute upper respiratory tract infection with bacterial infection.\" Given the symptoms and signs, acute bronchitis is also possibility. ##Final Result: Diagnosis: Acute upper respiratory tract infection with bacterial infection, acute bronchitis. 3. Score Explanation: Score: 3 points Reasoning: The extracted information (sore throat, fever, elevated temperature, pharyngeal congestion, elevated white blood cells and neutrophils) fully supports the diagnosis of \"acute upper respiratory tract infection with bacterial infection.\" This part of the conclusion is highly consistent with the information and is correct. The diagnostic conclusion also includes \"acute bronchitis,\" but the extracted information does not contain direct evidence to support this diagnosis. For example, there is no mention of worsening cough, productive cough, or abnormal findings on lung auscultation (such as rales or rhonchi). While the majority of the conclusion (\"acute upper respiratory tract infection with bacterial infection\") is supported by the information, the diagnosis of \"acute bronchitis\" lacks basis. According to the scoring criteria, \"The extracted information only supports small part of the conclusion, and the final result contains large portion of content that is not adequately supported,\" therefore, the score is 3 points. Example 4 (2 points, Medication Prescription) 1. Task Type: Predict the patients medication plan. II. Model Reasoning Chain: ##Extraction: Source: Inpatient Record [2180-03-05 09:00:00] - Key Information: Patient admitted due to palpitations and chest tightness. ECG shows atrial fibrillation. - Analysis: The patient has cardiac arrhythmia. Source: Past Medical History [2178-01-15 14:00:00] - Key Information: History of hypertension, long-term use of Irbesartan. - Analysis: Blood pressure control needs to be continued. Source: Lab Results [2180-03-05 10:30:00] - Key Information: Normal kidney function, normal liver function. - Analysis: Facilitates drug selection. ##Reasoning: The patient has atrial fibrillation and needs heart rate control. Long-term hypertension requires continued blood pressure management. Considering all factors, Metoprolol is recommended. 34 ##Final Result: Prescription: Metoprolol. 3. Score Explanation: Score: 2 points Reasoning: The extracted information\"palpitations, chest tightness, atrial fibrillation\"does indicate cardiac arrhythmia, and Metoprolol is commonly used first-line drug for treating atrial fibrillation. From pharmacological perspective, this conclusion is related to the extracted information. However, the extracted information does not provide sufficient evidence to support Metoprolol as the sole or preferred drug. The treatment plan for atrial fibrillation requires comprehensive consideration of the patients heart rate control needs (type and rate of A-fib), stroke risk (CHA2DS2-VASc score), presence of comorbidities (like heart failure), and blood pressure levels. Based solely on \"atrial fibrillation\" and \"hypertension,\" one cannot directly conclude that Metoprolol should be prescribed, as there are various other drug options (e.g., Diltiazem, Verapamil). The conclusion \"Metoprolol\" lacks sufficient evidence to be fully supported. It might be plausible treatment, but it is not the only or best choice that the extracted information can definitively determine. This aligns with the scoring criterion: \"The extracted information is related to the final result, but it fails to support the conclusion.\" Example 5 (1 point, Lab Test) 1. Task Type: Predict the lab tests the patient should undergo. 2. Model Reasoning Chain: ##Extraction: Source: Outpatient Record [2179-11-20 15:00:00] - Key Information: Patient complains of abdominal pain and diarrhea. - Analysis: Suggests gastrointestinal issue. Source: Physical Examination [2179-11-20 15:20:00] - Key Information: Abdominal palpation shows no significant tenderness, bowel sounds are active. - Analysis: Suggests no organic lesion or inflammation in the abdomen. Source: Past Medication History [2179-11-19 18:00:00] - Key Information: Recently took Aspirin. - Analysis: No direct correlation with diarrhea. ##Reasoning: Patient with abdominal pain and diarrhea Gastrointestinal symptoms. No tenderness on abdominal palpation Rules out conditions like acute peritonitis. Patient took Aspirin May affect the gastrointestinal tract. Based on the above information, an autoimmune disease is suspected. ##Final Result: Recommended Tests: Antinuclear antibody (ANA) profile, Complement, Rheumatoid factor. 3. Score Explanation: Score: 1 point Reasoning: The extracted information is \"abdominal pain, diarrhea,\" \"no tenderness on abdominal palpation,\" and \"took Aspirin.\" This information points to common gastrointestinal symptoms and potential drug-related GI discomfort. However, the final conclusion, \"Recommended Tests: Antinuclear antibody profile, Complement, 35 Rheumatoid factor,\" is used to diagnose autoimmune diseases (e.g., Systemic Lupus Erythematosus, Rheumatoid Arthritis). There are no symptoms or signs in the extracted information (e.g., joint pain, rash, oral ulcers) that are related to autoimmune diseases. Linking \"abdominal pain and diarrhea\" to tests for autoimmune diseases lacks any reasonable basis. The conclusion is severely irrelevant to the extracted information, and the reasoning process is highly unreliable. Therefore, the score is 1 point."
        },
        {
            "title": "Prompt",
            "content": "Prompt 2. Reasoning Chain Synthesis Prompt for GPT-4o ======================================== # Patient EHR Context # {context} ======================================== # Retrieved Medical Knowledge # {medical_knowledge} ======================================== # Ground Truth # {ground_truth} ======================================== # Task # {task} ======================================== # Data Description - # Patient EHR Context #: Contains all medical events and content from the patients hospitalization journey. - # Retrieved Medical Knowledge #: Contains Patient EHR Context elements and their relationships potentially relevant to the Ground Truth entity. - # Ground Truth #: Contains the correct answers for this task; your predictions must exactly match these. - # Task #: Contains the specific task description; you need to complete the task based on the information in # Patient EHR Context #. # Instructions Please provide logically rigorous medical reasoning process so that the # Ground Truth # can be derived from the content in # Patient EHR Context # and # Task #. # Requirements The reasoning process should include three stages: Extraction, Reasoning, and Final Results. ## Extraction - In this stage, extract and identify each piece of \"key information\" from the # Patient EHR Context # according to the provided # Retrieved Medical Knowledge #. - Dont pay attention to information that you think is not helpful for the reasoning. - Each step in the Extraction stage should follow the format below, **You need to specify the event name and time for each extracted information**: **Event Name [Event Time]**: list the information extracted from the event and analyze the potential relationship between the key information and the ground truth. 36 ## Reasoning - Analyze the relationship between the context information and the item in # Ground Truth # in very specific and professional manner, providing detailed reasoning steps. - Your analysis should include the item in # Ground Truth # as much as possible. Items that cannot be inferred from the context can be omitted. - Do not use the word \"maybe\", \"possible\" or \"though\" in the generated reasoning. You should do your best to find all the supporting information you can to ensure the correctness of your reasoning. - The reasoning process should be concise and rigorous, and each step should explain the specific medical knowledge involved, making the reasoning process more credible. - All reasoning must be based on the context information to infer the items in the ground truth and no reverse inference can be performed. ## Final Results - Provide the final result for the task. **Note that the final result should only contain the items contained in # Gound Truth # that have been correctly inferred in. ## Reasoning stage.** - Each item in the ## Final Results should be contained in the # Gound Truth # with extactly same string. # Important Notes!!! - **For each piece of # Retrieved Medical Knowledge # that is relevant to completing the # Task #, locate the exact position of its first item within the # Patient EHR Context # and explicitly annotate it during the Extraction phase to ensure more thorough analysis.** - **During the ## Reasoning stage, remember to analyze very carefully how each item in # Ground Truth # is inferred** - **Most importantly, integrate references to # Ground Truth # and # Retrieved Medical Knowledge # in an implicit manner. At any point in the reasoning process, do not use phrases such as according to the medical knowledge above, \"as shown in ground truth\" or any wording that reveals you are aware of the underlying medical knowledge or the ground truth.** # Output Format ## Extraction [YOUR OUTPUT] ## Reasoning [YOUR OUTPUT] ## Final Results [YOUR OUTPUT] 37 Supplementary Table 4 Instruction for each task in EHR-Bench. 38 Supplementary Table 5 Instruction for each task in MIMIC-IV-CDM and EHRSHOT."
        },
        {
            "title": "9.4 Case Collection",
            "content": "Case Case 1. Markdown Format for Event with Single Item ## Event Name [Event Time] - Info Key 1: Info Value 1 - Info Key 2: Info Value 2 ... - Info Key N: Info Value Case Case 2. Markdown Format for Event with Multiple Item ## Event Name [Event Time] Info Key 1 Info Key 2 ... Info Key - - - Info Value Info Value ... Info Value Info Value Info Value ... Info Value ... Info Value Info Value ... Info Value Case Case 3. Free-text Input Example of EHRSHOT ## Person [1991-04-12 00:00:00] - Birth - White - Not Hispanic or Latino - MALE 39 ## Measurement [2015-02-24 02:25:00] Item_Name Valuenum Valueuom Ref_Range_Lower Ref_Range_Upper Flag Lactate [Moles/volume] in Blood 0.6000000238418579 mmol/L nan nan nan Lactate [Mass/volume] in Blood 0.6000000238418579 mmol/L nan nan nan Glasgow coma scale 15.0 nan nan nan nan Mean blood pressure 103.0 mmHg nan nan nan Pain severity [Score] Visual analog score 6.0 nan nan nan nan Respiratory rate 18.0 breaths/min 12 18 normal Systolic blood pressure 152.0 mmHg 90 140 abnormal Body temperature 98.4000015258789 95 100.4 normal Diastolic blood pressure 79.0 mmHg 60 90 normal Heart rate 97.0 bpm 60 100 normal ## Observation [2015-02-24 02:25:00] Item_Name Valuenum Valueuom Ref_Range_Lower Ref_Range_Upper Flag Body temperature measurement site 1.0 nan nan nan nan Oxygen saturation 100.0 % 95 100 normal ## Drug_Exposure [2015-02-24 04:22:00] - oxycodone hydrochloride 1 MG/ML Oral Solution - ibuprofen 400 MG Oral Tablet - gabapentin 300 MG Oral Capsule - levetiracetam 500 MG Oral Tablet - 1 ML hydromorphone hydrochloride 2 MG/ML Prefilled Syringe - acetaminophen 325 MG Oral Tablet - 0.4 ML enoxaparin sodium 100 MG/ML Prefilled Syringe - zolpidem tartrate 5 MG Oral Tablet - triamcinolone acetonide 1 MG/ML Topical Cream - oxycodone hydrochloride 5 MG Oral Tablet ## Procedure_Occurrence [2015-02-24 23:59:00] - Routine venipuncture - Taking patient vital signs assessment - Ambulating patient ## Condition_Occurrence [2015-02-25 11:07:00] - Tobacco dependence syndrome - Benign neoplasm of colon - Inflammatory dermatosis - Epilepsy Case Case 4. Free-text Input Example of MIMIC-IV-CDM ## Patient Demographics - Patient History: ___ s/p emergency tissue AVR and type aortic dissection repair ___ and sternal washout ___, c/b tamponade requiring clot evacuation ___, trach ___, PEG ___. Overall, other complications included seizures, renal failure requiring CRRT, shock liver, HITT, sepsis, afib, left colonic ischemia. Discharged at end of ___ and returned to ___ in ___, at which time he underwent bronch. Has been at rehab. Was on tube feeds until several weeks ago and now only on 40 regular diet. He has been eating, but decreased appetite and intake due to post-prandial epigastric abdominal pain. This has been occuring for approximately three weeks, and he initially attributed it to indigestion. He had routine follow up in thoracic clinic with Dr ___, at which time his PEG tube was removed. Then he was then sent to the ED for his abdominal pain. PEG site was clean prior to removal. - Past Medical History: Hypertension Appendectomy Right facial cyst drainage - Social History: ___ Family History: None - Physical Examination: Physical exam VS: 98.6, 80, 135/72, 18, 96% RA Gen: NAD CV: RRR Pulm: CTA b/l Abd: soft, nondistended. slightly tender in epigastric area and right upper quadrant without rebound/guarding/rigidity Ext: left hand with ischemic digits, b/l ___ wrapped with gauze PE ## Labotary Test Events Item_name Valuenum Valueuom Ref_range_lower Ref_range_upper Alanine Aminotransferase (ALT) 18.0 IU/L 0.0 40.0 PTT 32.8 sec 25.0 36.5 PT 14.9 sec 9.4 12.5 INR(PT) 1.4 None 0.9 1.1 White Blood Cells 25.8 K/uL 4.0 11.0 Red Blood Cells 3.84 m/uL 4.6 6.2 RDW 14.1 % 10.5 15.5 Platelet Count 177.0 K/uL 150.0 440.0 Neutrophils 91.5 % 50.0 70.0 Monocytes 3.6 % 2.0 11.0 MCV 92.0 fL 82.0 98.0 MCHC 33.0 % 31.0 35.0 Lymphocytes 4.8 % 18.0 42.0 Hemoglobin 11.6 g/dL 14.0 18.0 Hematocrit 35.2 % 40.0 52.0 Eosinophils 0.1 % 0.0 4.0 MCH 30.2 pg 27.0 32.0 Light Green Top Hold HOLD. None None None Albumin 3.3 g/dL 3.5 5.2 Alkaline Phosphatase 117.0 IU/L 40.0 130.0 Anion Gap 15.0 mEq/L 8.0 20.0 Basophils 0.1 % 0.0 2.0 Bicarbonate 28.0 mEq/L 22.0 32.0 Bilirubin, Total 1.2 mg/dL 0.0 1.5 Chloride 94.0 mEq/L 96.0 108.0 Asparate Aminotransferase (AST) 20.0 IU/L 0.0 40.0 Estimated GFR (MDRD equation) Using this patients age, gender, and serum creatinine value of 1.2,. Estimated GFR = 61 if non African-American (mL/min/1.73 m2). Estimated GFR = 74 if African-American (mL/min/1.73 m2). For comparison, mean GFR for age group 60-69 is 85 (mL/min/1.73 m2). GFR<60 = Chronic Kidney Disease, GFR<15 = Kidney Failure. None None None Glucose 141.0 mg/dL 70.0 100.0 Lipase 15.0 IU/L 0.0 60.0 Creatinine 1.2 mg/dL 0.5 1.2 Potassium 4.6 mEq/L 3.3 5.1 Sodium 132.0 mEq/L 133.0 145.0 Urea Nitrogen 27.0 mg/dL 6.0 20.0 Specimen Type VEN. None None None Lactate 1.3 mmol/L 0.5 2.0 Protein 30.0 mg/dL None None Yeast NONE None None None WBC <1. None 0.0 5.0 Urobilinogen NEG. None 0.2 1.0 Urine Color Yellow. None None None Specific Gravity >1.050*. None 1.001 1.035 RBC 3.0 #/hpf 0.0 2.0 pH 6.0 units 5.0 8.0 Urine Appearance Clear. None None None Leukocytes NEG. None None None Ketone NEG. None None None Glucose NEG. None None None Epithelial Cells 0.0 #/hpf None None Blood TR. None None None Bilirubin NEG. None None None Bacteria NONE. None None None Nitrite NEG. None None None Calcium, Total 9.2 mg/dL 8.4 10.3 Magnesium 1.8 mg/dL 1.6 2.6 Phosphate 3.4 mg/dL 2.7 4.5 ## Microbiology Test Events Item_name Valuestr Blood Culture, Routine NO GROWTH. ANAEROBIC CULTURE NO ANAEROBES ISOLATED. FLUID CULTURE PSEUDOMONAS AERUGINOSA, ENTEROCOCCUS SP. ## Radiology Examinations Exam_name Text LIVER OR GALLBLADDER US (SINGLE ORGAN) EXAMINATION: LIVER OR GALLBLADDER US (SINGLE ORGAN): TECHNIQUE: Grey scale and color Doppler ultrasound images of the abdomen were obtained. FINDINGS: LIVER: The hepatic parenchyma appears within normal limits. The contour of the liver is smooth. Again seen is hyperechoic lesion measuring approximately 6 mm in segment of the liver, unchanged since prior study and likely represents hemangioma. The additional hemangioma seen on previous study is not clearly identified on todays exam. Main portal vein is patent with hepatopetal flow. There is no ascites. BILE DUCTS: There is no intrahepatic biliary dilation. The CBD measures 6 mm. GALLBLADDER: The gallbladder is distended and filled with sludge. There is mild gallbladder wall edema measuring up to 6 mm. There is no pericholecystic fluid. PANCREAS: The pancreas is not well seen secondary to overlying bowel gas. KIDNEYS: The right kidney measures 8.1 cm. Survey views of the right kidney do not demonstrate any masses, hydronephrosis, or stones. RETROPERITONEUM: Visualized portions of aorta and IVC are within normal limits."
        },
        {
            "title": "Case",
            "content": "Case 5. Free-text Input Example of EHR-Bench # Patient Demographics [None] - Anchor_Age: 88 - Gender: ## Admissions [2127-04-18 16:53:00] - Admission_Type: EW EMER. - Admission_Location: PROCEDURE SITE - Admission_Info: None ## Provider Order Entry [2127-04-18 15:21:26] Order_Type Order_Subtype Medications nan ADT orders Admit General Care Vitals/Monitoring Nutrition Diet Order General Care Other Medications nan ## Services [2127-04-18 16:54:06] - Curr_Service: CMED ## Transfers [2127-04-18 16:54:06] - Eventtype: admit - Careunit: Medicine/Cardiology ## Pharmacy [2127-04-18 16:57:15] Medication Proc_Type Status Potassium Chloride Unit Dose Discontinued via patient discharge Potassium Chloride Unit Dose Discontinued via patient discharge Oxybutynin Unit Dose Discontinued via patient discharge Zolpidem Tartrate Unit Dose Discontinued via patient discharge Nitroglycerin SL Unit Dose Discontinued Sodium Chloride 0.9 Hydrochlorothiazide Unit Dose Discontinued via patient discharge Potassium Chloride Unit Dose Discontinued via patient discharge Lisinopril Unit Dose Discontinued via patient discharge Multivitamins Unit Dose Discontinued via patient discharge Acetaminophen Unit Dose Discontinued via patient discharge Ferrous Sulfate Unit Dose Discontinued via patient discharge Aluminum-Magnesium Hydrox.-Simethicone Unit Dose Discontinued via patient discharge Pneumococcal Vac Polyvalent Unit Dose Discontinued Simvastatin Unit Dose Discontinued via patient discharge Aspirin Unit Dose Discontinued via patient discharge Diltiazem Extended-Release Unit Dose Discontinued via patient discharge ## Prescriptions [2127-04-18 17:00:00] Drug Atc Type Prod_Strength Dose_Val_Rx Dose_Unit_Rx Potassium Chloride potassium chloride 20mEq Packet 20 mEq 43 Potassium Chloride potassium chloride 20mEq Packet 40 mEq Zolpidem Tartrate zolpidem 5mg Tablet 5 mg Nitroglycerin SL glyceryl trinitrate 0.3mg SL Tablet Bottle 0.3 mg Sodium Chloride 0.9 Potassium Chloride potassium chloride 20mEq Packet 60 mEq Acetaminophen paracetamol 325mg Tablet 650 mg Aluminum-Magnesium Hydrox.-Simethicone aluminium hydroxide 30 mL UDCup 30 mL Pneumococcal Vac Polyvalent nan 25mcg/0.5mL Vial 0.5 mL ## Provider Order Entry [2127-04-18 17:13:19] Order_Type Order_Subtype Medications nan Medications nan Medications nan ## Pharmacy [2127-04-18 17:26:01] Medication Proc_Type Status Fentanyl Citrate Unit Dose Expired Ibuprofen Unit Dose Expired ## Provider Order Entry [2127-04-18 17:55:22] Order_Type Order_Subtype Radiology Ultrasound General Care Other General Care Other General Care Other General Care Other Blood Bank Blood tests Lab nan Lab nan Radiology General Xray Cardiology ECG General Care Other General Care Other ## Prescriptions [2127-04-18 18:00:00] Drug Atc Type Prod_Strength Dose_Val_Rx Dose_Unit_Rx Fentanyl Citrate fentanyl 100mcg/2mL Amp 25 mcg Ibuprofen ibuprofen 600mg Tablet 600 mg ## Provider Order Entry [2127-04-18 18:12:48] Order_Type Order_Subtype General Care Vitals/Monitoring General Care Other General Care Other General Care Other General Care Other General Care Activity 44 General Care Other General Care Other IV therapy IV fluids Medications nan General Care Other ## Pharmacy [2127-04-18 18:44:04] Medication Proc_Type Status nan IV Large Volume Expired Atropine Sulfate Unit Dose Discontinued via patient discharge ## Prescriptions [2127-04-18 19:00:00] Drug Atc Type Prod_Strength Dose_Val_Rx Dose_Unit_Rx 5 Sodium Bicarbonate sodium bicarbonate 50mEq Vial 150 mEq Atropine Sulfate atropine 1mg/10mL Syinge 0.5 mg ## Provider Order Entry [2127-04-18 19:11:50] Order_Type Order_Subtype Lab nan Medications nan ## Pharmacy [2127-04-18 19:14:10] - Medication: Acetylcysteine 20Proc_Type: Unit Dose - Status: Discontinued via patient discharge ## Labotary Test Events [2127-04-18 19:19:00] Item_Name Valuenum Valueuom Ref_Range_Lower Ref_Range_Upper Flag Comments INR(PT) 1.2 nan 0.9 1.1 abnormal nan PT 13.7 sec 10.4 13.4 abnormal nan PTT 25.6 sec 22.0 35.0 nan nan Alanine Aminotransferase (ALT) 9.0 IU/L 0.0 40.0 nan nan Albumin 3.8 g/dL 3.4 4.8 nan nan Alkaline Phosphatase 61.0 IU/L 39.0 117.0 nan nan Anion Gap 13.0 mEq/L 8.0 20.0 nan nan Asparate Aminotransferase (AST) 13.0 IU/L 0.0 40.0 nan nan Bicarbonate 27.0 mEq/L 22.0 32.0 nan nan Bilirubin, Total 0.2 mg/dL 0.0 1.5 nan nan Chloride 99.0 mEq/L 96.0 108.0 nan nan Creatinine 1.3 mg/dL 0.4 1.1 abnormal nan Estimated GFR (MDRD equation) nan nan nan nan nan Using this patients age, gender, and serum creatinine value of 1.3,. Estimated GFR = 39 if non African-American (mL/min/1.73 m2). Estimated GFR = 47 if African-American (mL/min/1.73 m2). For comparison, mean GFR for age group 70+ is 75 (mL/min/1.73 m2). GFR<60 = Chronic Kidney Disease, GFR<15 = Kidney Failure. Glucose 176.0 mg/dL 70.0 105.0 abnormal nan Lactate Dehydrogenase (LD) 134.0 IU/L 94.0 250.0 nan nan Potassium 3.8 mEq/L 3.3 5.1 nan nan Sodium 135.0 mEq/L 133.0 145.0 nan nan 45 Urea Nitrogen 28.0 mg/dL 6.0 20.0 abnormal nan Hematocrit 28.5 Hemoglobin 9.6 g/dL 12.0 16.0 abnormal nan MCH 29.2 pg 27.0 32.0 nan nan MCHC 33.7 MCV 87.0 fL 82.0 98.0 nan nan Platelet Count 260.0 K/uL 150.0 440.0 nan nan RDW 14.7 Red Blood Cells 3.29 m/uL 4.2 5.4 abnormal nan White Blood Cells 4.6 K/uL 4.0 11.0 nan nan ## Prescriptions [2127-04-18 20:00:00] - Drug: Acetylcysteine 20Atc Type: acetylcysteine - Prod_Strength: 800mg/4ml Vial - Dose_Val_Rx: 600 - Dose_Unit_Rx: mg ## Labotary Test Events [2127-04-18 21:20:00] Item_Name Valuenum Valueuom Ref_Range_Lower Ref_Range_Upper Flag Comments Bacteria nan nan nan nan nan NONE. Bilirubin nan mg/dL nan nan nan NEG. Blood nan nan nan nan nan MOD. Epithelial Cells 1.0 #/hpf nan nan nan nan Glucose nan mg/dL nan nan nan NEG. Ketone nan mg/dL nan nan nan NEG. Leukocytes nan nan nan nan nan NEG. Nitrite nan nan nan nan nan NEG. pH 7.5 units 5.0 8.0 nan nan Protein nan mg/dL nan nan nan NEG. RBC nan #/hpf 0.0 2.0 nan <1. Specific Gravity 1.015 1.001 1.035 nan nan Transitional Epithelial Cells nan #/hpf nan nan nan <1. Urine Appearance nan nan nan nan nan Hazy. Urine Color nan nan nan nan nan Straw. Urobilinogen nan mg/dL 0.2 1.0 nan NEG. WBC 3.0 #/hpf 0.0 5.0 nan nan Yeast nan nan nan nan nan nan ## Provider Order Entry [2127-04-19 02:55:46] - Order_Type: Medications - Order_Subtype: nan ## Pharmacy [2127-04-19 02:58:06] - Medication: Zolpidem Tartrate - Proc_Type: Unit Dose - Status: Discontinued via patient discharge ## Prescriptions [2127-04-19 03:00:00] - Drug: Zolpidem Tartrate - Atc Type: zolpidem - Prod_Strength: 5mg Tablet - Dose_Val_Rx: 5 - Dose_Unit_Rx: mg 46 ## Provider Order Entry [2127-04-19 05:41:43] - Order_Type: Lab - Order_Subtype: nan ## Labotary Test Events [2127-04-19 07:16:00] Item_Name Valuenum Valueuom Ref_Range_Lower Ref_Range_Upper Flag Comments Hematocrit 29.0 Creatinine 1.4 mg/dL 0.4 1.1 abnormal nan Urea Nitrogen 28.0 mg/dL 6.0 20.0 abnormal nan ## Radiology Examinations [2127-04-19 09:49:00] - Note_Type: RR - Exam_Name: [CHEST (PRE-OP PA & LAT)] - Text: REASON FOR EXAMINATION: Preoperative evaluation of the patient with aortic stenosis before aortic valve replacement. PA and lateral chest radiograph was compared to ___. Heart size is normal. Mediastinal position, contour and width are stable except for dextroscoliosis, mild to moderate. The lungs are clear except for linear bibasilar opacities, unchanged since ___, consistent with scarring. There is no pleural effusion or pneumothorax. The lateral view demonstrates contrast material in expected location of the distal esophagus that might be related to recent study involving the administration of oral contrast, please correlate with clinical history. There is no pleural effusion or pneumothorax demonstrated. ## Provider Order Entry [2127-04-19 12:11:28] Order_Type Order_Subtype ADT orders Discharge ADT orders Discharge ## Transfers [2127-04-19 13:10:00] - Eventtype: discharge - Careunit: nan 47 Supplementary Figure 1 case study of EHR Trajectory, Medical Relation, and Reasoning Chain. (a) EHR Trajectory for patient, where <events>... and <item info>... represent the omission of large amount of information for display purposes. (b) Medical Relation, showing the connections between the context medical entities and target items. (c) Reasoning Chain, detailing the process of inferring diagnosis from the patients EHR data. The parts highlighted in bold are the content commonly found in the EHR Trajectory, Medical Relation, and Reasoning Chain. This indicates that the medical graph is effective in identifying valid medical entities from the trajectory and using them to enhance reasoning."
        }
    ],
    "affiliations": [
        "Fudan University, Shanghai, China",
        "Intelligence Computing and Sensing Laboratory, Peking University, Beijing, China",
        "Intelligence Healthcare Department, AntGroup, Hangzhou, China",
        "Shanghai Artificial Intelligence Laboratory, Shanghai, China",
        "Shanghai Jiao Tong University, Shanghai, China"
    ]
}