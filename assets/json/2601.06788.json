{
    "paper_title": "Artificial Entanglement in the Fine-Tuning of Large Language Models",
    "authors": [
        "Min Chen",
        "Zihan Wang",
        "Canyu Chen",
        "Zeguan Wu",
        "Manling Li",
        "Junyu Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) can be adapted to new tasks using parameter-efficient fine-tuning (PEFT) methods that modify only a small number of trainable parameters, often through low-rank updates. In this work, we adopt a quantum-information-inspired perspective to understand their effectiveness. From this perspective, low-rank parameterizations naturally correspond to low-dimensional Matrix Product States (MPS) representations, which enable entanglement-based characterizations of parameter structure. Thereby, we term and measure \"Artificial Entanglement\", defined as the entanglement entropy of the parameters in artificial neural networks (in particular the LLMs). We first study the representative low-rank adaptation (LoRA) PEFT method, alongside full fine-tuning (FFT), using LLaMA models at the 1B and 8B scales trained on the Tulu3 and OpenThoughts3 datasets, and uncover: (i) Internal artificial entanglement in the updates of query and value projection matrices in LoRA follows a volume law with a central suppression (termed as the \"Entanglement Valley\"), which is sensitive to hyper-parameters and is distinct from that in FFT; (ii) External artificial entanglement in attention matrices, corresponding to token-token correlations in representation space, follows an area law with logarithmic corrections and remains robust to LoRA hyper-parameters and training steps. Drawing a parallel to the No-Hair Theorem in black hole physics, we propose that although LoRA and FFT induce distinct internal entanglement signatures, such differences do not manifest in the attention outputs, suggesting a \"no-hair\" property that results in the effectiveness of low rank updates. We further provide theoretical support based on random matrix theory, and extend our analysis to an MPS Adaptation PEFT method, which exhibits qualitatively similar behaviors."
        },
        {
            "title": "Start",
            "content": "Artificial Entanglement in the Fine-Tuning of Large Language Models Min Chen,1 Zihan Wang,2 Canyu Chen,2 Zeguan Wu,1 Manling Li,2 and Junyu Liu1 1Department of Computer Science, University of Pittsburgh, Pittsburgh, PA 15260, USA 2Department of Computer Science, Northwestern University, Evanston, IL 60208, USA (Dated: January 13, 2026) Large language models (LLMs) can be adapted to new tasks using parameter-efficient fine-tuning (PEFT) methods that modify only small number of trainable parameters, often through low-rank updates. In this work, we adopt quantum-information-inspired perspective to understand their effectiveness. From this perspective, low-rank parameterizations naturally correspond to low-dimensional Matrix Product States (MPS) representations, which enable entanglement-based characterizations of parameter structure. Thereby, we term and measure Artificial Entanglement, defined as the entanglement entropy of the parameters in artificial neural networks (in particular the LLMs). We first study the representative low-rank adaptation (LoRA) PEFT method, alongside full fine-tuning (FFT), using LLaMA models at the 1B and 8B scales trained on the Tulu3 and OpenThoughts3 datasets, and uncover: (i) Internal artificial entanglement in the updates of query and value projection matrices (WQ, WV ) in LoRA follows volume law with central suppression (termed as the Entanglement Valley), which is sensitive to hyper-parameters and is distinct from that in FFT; (ii) External artificial entanglement in attention matrices, corresponding to tokentoken correlations in representation space, follows an area law with logarithmic corrections and remains robust to LoRA hyper-parameters and training steps. Drawing parallel to the No-Hair Theorem in black hole physics, we propose that although LoRA and FFT induce distinct internal entanglement signatures, such differences do not manifest in the attention outputs, suggesting no-hair property that results in the effectiveness of low rank updates. We further provide theoretical support based on random matrix theory, and extend our analysis to an MPS Adaptation PEFT method, which exhibits qualitatively similar behaviors. I. INTRODUCTION Recently, large language models (LLMs) have demonstrated desirable abilities by scaling model size and pretraining on large corpora of data [111]. Post-training, usually via fine-tuning, is adopted to adapt pretrained LLMs for downstream tasks. However, it is often considered unnecessary and computationally expensive to update the full set of parameters. Therefore, many parameter-efficient fine-tuning (PEFT) methods are proposed, including low-rank adaptation (LoRA) [12] and its variants [1322]. Previous studies [12, 17, 2326] also contribute significantly to explaining their effectiveness. However, fundamental question remains unsolved: how complex are the internal parameter structures induced by distinct fine-tuning methods, and to what extent are such complexities reflected in attention-level representations? We investigate this question by adopting quantum information perspective, and draw parallel with well-known phenomenon in theoretical physics: the tension between internal quantum correlations and external simplicity. This tension has been most prominently explored within black hole physics [2729]. fundamental conflict [3033] emerges between the quantum mechanical description of black holes (via entanglement entropy) and their classical geometric description. While the scaling behavior of entanglement entropy implies that black holes are complex systems characterized by vast number of degrees of freedom, classical gravitational theories predict that black holes are surprisingly simple objects with no distinct features. This classical perspective is formalized by uniqueness theorems [31, 34, 35]. The physical essence of these theorems is encapsulated in the No-Hair Theorem [30, 31, 34, 35], which states that all stationary black hole solutions of the EinsteinMaxwell equations can be completely characterized by only three externally observable classical parameters: mass, electric charge, and angular momentum. All other semi-classical information about the matter that formed the black hole (the hair) disappears behind the event horizon and is inaccessible to an external observer [36]. Motivated by this dilemma, we define and probe the artificial entanglement, where artificial refers to the application of entanglement entropy measures to artificial neural networks (particularly the LLMs). Notably, artificial emphasizes that large language models are entirely classical systems and do not possess any physical quantum degrees of freedom or genuine quantum entanglement. Therefore, this adopted perspective serves purely as mathematical tool. Our contributions are as follows: (i) We first represent the updates of query and value projection matrices (WQ, WV ) as Matrix Product State (MPS) [29, 3744] and compute the von Neumann entanglement entropy across bond partitions, yielding an artificial entanglement profile that captures internal parameter structure. We thereby refer the corresponding entanglement as the internal artificial entanglement. We show that the resulting internal artificial entanglement under LoRA follows volume law (i.e., entanglement entropy scaling linearly with system size, indicating high degree of correlation that cannot be faithfully captured by simple representations) [29, 45] with pronounced central suppression (termed as the Entanglement Valley), is highly sensitive to hyperparameters (e.g., rank and scaling factor α), and is qualitatively distinct from full fine-tuning (FFT), revealing substantial differences in internal parameter correlation. Meanwhile, we also measure the external artificial entanglement via similar routine, where external means here we choose the entanglement to be the one on attention matrices and outputs that is produced dur6 2 0 J 1 1 ] . [ 1 8 8 7 6 0 . 1 0 6 2 : r FIG. 1. Key Findings. (i) Projection matrices (WQ and WV ) exhibit volume-law internal entanglement profiles with distinctive entanglement valleys that differ between FFT, LoRA and MPS adaptation. (ii) Attention matrices show area-law scaling with logarithmic corrections. Random matrix theory explains this through the Attention Cardy Formula. (iii) Despite internal differences, external attention outputs remain invariant, revealing no-hairlike effect where the attention mechanism acts as coarse-graining operator. ing the forward computation rather than on model parameters, corresponding to tokentoken correlations in representation space. We demonstrate that it follows an approximate area law with logarithmic corrections (i.e., entanglement entropy scaling logarithmically with sequence length). Despite the markedly different internal entanglement structures induced by LoRA and FFT, this external artificial entanglement remains empirically robust across certain range of hyperparameters and training stages, suggesting that variations in internal parameter correlation do not significantly propagate to attention-level representations within the regimes explored. Drawing parallel to the No-Hair Theorem, we show that the attention mechanism exhibits no-hair property, whereby high-correlation internal entanglement signatures are coarsegrained and rendered indistinguishable at the level of attention outputs. Combining the above results, we propose that LoRA learns intrinsically different internal parameter structures with FFT, but remains effective because the attention mechanism coarse-grains these differences through no-hair property. (ii) We provide theoretical support for the observations based on random matrix theory by establishing an Attention Cardy Formula, which shows that the entanglement entropy of attention matrices exhibits logarithmic scaling with sequence length under certain initialization conditions, analogous to the Cardy formula [46, 47] for critical quantum systems. Moreover, we relate the emergence of low external artificial entanglement to stable rank collapse (i.e., spectral mass concentrates onto few dominant modes) [48]. (iii) We extend our analysis beyond LoRA to an MPS Adaptation PEFT strategy, in which weight updates are parameterized directly via MPS, and demonstrate that it exhibits qualitatively similar internal and external artificial entanglement behaviors, validating the generality of our findings. Our experiments are conducted on the Llama 3 series models [49] at the 1B and 8B scales, and employ the Tulu3 [50] and OpenThoughts3 [51] datasets, which target instruction following and reasoning tasks respectively with distinct scope and structures. Our key findings are summarized in FIG. 1. We begin by outlining the fine-tuning methods under consideration and introducing our overall framework (Sec. II A). We then present empirical results on LLaMA model that reveal no-hair property, together with several additional phenomena (Sec. II B). Next, we provide theoretical interpretation based on random matrix theory (Sec. II C). Building on the above results, we extend our study to an MPS adaptation method, which exhibits qualitatively similar behaviors (Sec. II D). Finally, we describe our methods in detail and conclude with discussion on potential applications (Secs. III and IV). 2 II. RESULTS A. FFT, LoRA and the Overall Framework for Artificial Entanglement Analysis In this section, we first review FFT and LoRA [12] methods. FFT adapts pre-trained model by updating all model parameters. Given pre-trained weight projection matrix W0, the model learns full matrix update WFull, resulting in = W0 + WFull. In contrast, LoRA injects trainable lowrank matrices to approximate the weight updates. Specifically, LoRA decomposes the update into two smaller matrices, denoted as Crdin and Cdoutr,1 where the rank min(din, dout). In addition, LoRA uses scaling hyperparameter α to control the magnitude of the low-rank update. Formally, LoRA update is formulated as WLoRA := α BA Cdoutdin , (1) which is then added to the frozen pre-trained weight W0. To analyze artificial entanglement in fine-tuning, we employ framework that includes MPS decomposition with entanglement entropy computation. Our approach consists of four key steps: (i) reshaping matrices (including the updates of weight projection matrices and attention-related matrices) into higher-order tensors, (ii) factorizing them into MPS representations, (iii) interpreting the MPS as mathematical formalism of many-body states (formally analogous to quantum many-body states, though the underlying systems are entirely classical), and (iv) computing the von Neumann entanglement entropy across different bonds2 by performing SVD and obtaining singular values (Schmidt indices), yielding an artificial entanglement profile. This profile quantifies how correlations vary across different positions, revealing the fruitful internal structure. Below we take the update of projection matrix (WQ and WV ) as an example and detail the essential steps (See preliminaries on tensor basis in Appendix VI): Step 1: Although Cdoutdin is originally an Order-2 tensor, we first reshape it into higher-order tensor by factorizing its input and output dimensions into their prime components. This produces the finest-grained decomposition dout = (cid:89) k=1 fk, din = (cid:89) ℓ=1 gℓ, (2) where and denote the number of prime factors in dout and din, respectively. This allows us to view as tensor 1 For notational generality, we allow the matrices to be complex-valued. All experiments in this work are conducted with real-valued parameters as in standard LoRA implementations. 2 With slight abuse of terminology, we sometimes use the terms bond, cut, cut position, and bi-partition interchangeably, as they are in one-to-one correspondence in the one-dimensional settings considered here. Specifically, cut at given position induces bi-partition of the Hilbert space, while in an MPS representation such cut corresponds to cutting virtual bond. living on (n + m) sites. This provides the lattice on which we build the MPS. Step 2: We then perform sequence of singular value decompositions (SVDs) along the reshaped tensor dimensions to factorize into chain of local Order-3 tensors. The resulting representation {T [1], [2], . . . , [n+m]} (3) is an MPS whose virtual bond indices capture how different tensor sites are correlated. For LoRA, the factorization naturally reflects its low-rank structure: the update BA induces restricted bond dimension. For FFT, the MPS is obtained directly from WFull without such constraints. Step 3: Once factorized, the MPS can be interpreted as mathematical formalism of many-body states. Specifically, the MPS factorization of is formally analogous to the MPS representation of the amplitude tensor of quantum many-body state Ψ [52, 53] (though the underlying system is entirely classical). Each site corresponds to one of the factorized input/output degrees of freedom, and the virtual bonds represent the latent coupling strength between subsystems. Step 4: When performing an SVD at each bond during the MPS decomposition, we obtain the singular values {σαk }, where denotes the bond position (the k-th bond in the MPS chain) and αk indexes the Schmidt modes at bond k. We normalize the singular values according to λαk = (cid:113)(cid:80) σαk αk , σ2 αk (4) such that (cid:80) cients define the bipartite entanglement entropy, = 1. These normalized Schmidt coeffiλ2 αk αk Sk = (cid:88) αk λ2 αk log λ2 αk , (5) where Sk quantifies the entanglement entropy across bond k. Scanning across all bonds yields an artificial entanglement profile, allowing us to probe correlation patterns. The full details are provided in Sec. III. Remark (On the use of the term artificial entanglement). The entanglement entropy considered in this work is purely mathematical quantity derived from the Schmidt decomposition of high-dimensional tensors, and should not be interpreted as physical quantum entanglement. Although large language models are entirely classical systems, MPS representations and their associated entanglement measures provide principled, quantum-inspired, basis-independent way to characterize how correlations are distributed under different bipartitions. In this sense, the resulting artificial entanglement profiles are used as descriptive diagnostics of structural correlations, rather than as real physical entanglement in an underlying physical Hilbert space. B. Artificial Entanglement Profile in FFT and LoRA Fine-tuning Volume law in random Gaussian matrix. tensor (including matrix) with i.i.d. Gaussian entries, when reshaped 3 FIG. 3. Artificial entanglement profiling of WQ (a) and WV (b) across different bi-partition positions during FFT. Each curve corresponds to training step, which shows how evolves and gradually converges as fine-tuning progresses. FIG. 4. Artificial entanglement profiling of WQ (a) and WV (b) during LoRA fine-tuning, similar to FIG. 3. X. Unless otherwise stated, we adopt and keep the following settings: we uniformly subsample 5,000 examples from the full dataset and reserve 5% as test split. For LoRA we use rank = 256 and scaling hyperparameter α = 16, applied only to the self-attention query and value projections (WQ and WV ) [12]. This is to follow the default settings [57], serving as representative cases to study the resulting behaviors. The learning rate is set to 3 104 for LoRA and 3 105 for FFT, since under this setup the testing loss is approximately equal [57]. The test loss is defined as the average tokenlevel cross-entropy, i.e., negative log-likelihood (NLL). We quantify artificial entanglement using the von Neumann entropy [29, 42, 43] and denote it as S, the standard measure of entanglement in quantum information theory. Artificial Entanglement Profiling SWQ and SWV when α is small4. We compare the artificial entanglement profiling of FFT and LoRA across bonds, see FIG. 3 and FIG. 4. Both methods exhibit overall volume-law scaling of entanglement with central dip. We term it as the entanglement valley (A detailed analysis is provided in Appendix B). However, their entanglement evolution w.r.t time differs substantially: In LoRA with small scaling parameter α = 16, the valley deepens as training progresses, making the valley more pronounced, whereas FFT gradually lifts the valley. This contrast is also type-dependent: in WV , the valley almost disappears in later FFT steps, while in WQ it remains clearly visible even at late time. 3 For notational convenience, we interchangeably use SWQ and SWV to denote SWQ and SWV respectively. 4 Here small refers to relatively small values of the scaling parameter α (e.g., α = 16) compared to larger values used in experiments (e.g., α = 256 or higher). FIG. 2. Artificial entanglement profiling of random Gaussian matrix. The orange curve (χ = ) corresponds to the full SVD at each bond and closely matches the Page-curve prediction for Haar-random states (green dashed line). The blue curve (χ = 32) demonstrates the effect of truncating the MPS bond dimension the entropy saturates once the Schmidt rank exceeds χ, forming plateau, while the true entropy (the orange) continues to increase toward the Page limit. into vector in tensor-product space and properly normalized, is statistically equivalent to sampling Haar-random pure state [54]. Here, viewing the tensor as vector is natural, since an MPS is precisely structured decomposition of such high-dimensional vector via successive Schmidt decompositions. In the quantum domain, Haar-random pure states serve as canonical model of maximally unstructured states, whose bipartite entanglement properties are well captured by Page theory [55]. Accordingly, the von Neumann entanglement entropy obeys the Page law, which to leading order reads SPage(dL) log dL dL 2dR ln 2 for dL dR, where dL and dR denote the dimensions of the left and right subsystems respectively after bi-partition. This implies volume-law scaling of entanglement, where increases linearly with the size of the smaller subsystem. For Haar-random states, this linear growth further leads to saturation near the mid cut. We show this profiling curve by sampling real-valued random matrix Rmn with i.i.d. Gaussian entries, with the total dimension = mn where we set m, = 2048. We then factorize into list of local dimensions using prime factorization, where in this case the factor is 2. Then similar construction of the MPS is conducted (See Sec. II A). At each cut, we retain at most χmax singular values, where χmax = 32 or . Therefore the exact χ = min{χmax, dL, dR}. FIG. 2 shows this artificial entanglement profiling curve, where we also draw SPage as an analytic benchmark. FIG. 2 here also provides benchmark profiling about how an artificial entanglement profiling is visualized in our study. Default Experimental Setups. In the main text, we present the results in fine-tuning the Llama-3.2-1B-Instruct, an instruction-tuned model with 1 billion parameters from Llama-3.2 family [49, 56], on subset of the tulu-3-sftmixture instruction-tuning (Tulu3) corpus [50]. For more experiments in other model and dataset setups, see Appendix 4 FIG. 5. Artificial entanglement profiling of WQ and WV as function of LoRA rank in different steps. (a) SWQ at the early time of fine-tuning (Step 1) and the late time (Step 1000). (b) SWV at the early time of fine-tuning (Step 1) and the late time (Step 1000). These reveal how differing and training time jointly shape the entanglement structure of the learned updates. FIG. 6. Test loss across training steps for different LoRA ranks r. When is small, increasing generally improves performance. While increasing to specific the test loss saturates and is not improved much. We keep other hyper-parameters fixed, such as the learning rate and α. FIG. 7. Final test loss as function of learning rate for FFT and LoRA when the scaling parameter α is small (here α = 16). LoRA requires larger learning rate than FFT to achieve the best test performance, consistent with previous observations [57, 58]. Remark. From the perspective of quantum information theory, volume-law entanglement profile signals intrinsically high correlation that requires large number of effective degrees of freedom to represent, exceeding what low-rank parametrization such as LoRA can faithfully capture. Therefore, we propose that LoRA is fundamentally unable to represent the full entanglement, resulting in distinct signatures. To study the relationship between rank settings in LoRA and the deepening of the valley, we examine how the LoRA rank influences the artificial entanglement profiling of WQ and WV . FIG. 5 reports SWQ and SWV as function of r, evaluated both at the early time in step 1 and the late time 5 FIG. 8. SWQ and SWV w.r.t the cut position (with dL = 2k) across training steps when increasing α = 256. Panels (a) and (b) show that at early time, the curves exhibit pronounced entanglement valley structure centered near the middle cuts. As training progresses, the curves of WQ are less deepened as the cases in smaller α = 16, while the curves of WV gradually lift and approach saturated shape. in step 1000. FIG. 5 shows that: at early time, increases monotonically with r. This is consistent with viewing the LoRA rank as an effective bond dimension χ of the MPS factorization: larger χ enlarges the admissible subspace of tensor network states and raises the maximal entanglement entropy that can be represented across each bond. Consequently, as (and hence the effective χ ) grows, the learned updates can exploit higher entanglement, and the measured rises accordingly. In contrast, at late time no longer grows with r. It instead quickly approaches plateau and remains nearly unchanged beyond certain rank. Heuristically, this suggests that in LoRA, optimization converges to relatively low-entanglement solution whose intrinsic correlation is already captured at moderate bond dimension, so that additional entanglement capacity from larger remains largely unused and does not translate into further growth of entanglement entropy. In contrast, FFT does not exhibit such saturation, as it is not constrained by rank limitations and can represent higher entanglement. This contrast indicates that LoRA is consistently unable to represent the full entanglement across different LoRA ranks. We can further observe the empirical relation between how impacts the test loss (See FIG. 6) and the perspective from (FIG. 5). In FIG. 6 we visualize the test loss with respect to different and training steps, and observe that increasing does not necessarily indicate large improvement in the test performance, along with the saturation of (FIG. 5). Furthermore, investigating the best learning rate for FFT and LoRA (See FIG. 7), we observe similar phenomenon as [57, 58]. From FIG. 7, LoRA requires larger learning rate to achieve the best test loss than FFT. These observations motivate us to further investigate how hyper-parameters, where we specifically investigate the scaling parameter α, affects the entanglement structure and optimization behavior in LoRA. Artificial Entanglement Profiling of SWQ and SWV when α is large. We now investigate the case when α is increased. We first tune α from 16 to 256. From FIG. 8 we see that in this setting LoRA will learn solution that mitigates the deepening of the entanglement valley in WQ and even lifts the valley in WV during late time, suggesting closer yet still distinct solution to the one learned by FFT. To betequal to the one in FIG. 7, see FIG. 11 (a). We first draw some observations: As α increases from 1, test performance is initially improved. Then, the test performance reaches the best when α is around α = 16. However, the test performance is worse as α increases. Meanwhile, from α = 1 to α = 256 we can observe an increasing fluctuation along the path. This indicates that the optimal learning rate in LoRA has been shifted, see FIG. 11 (a): the learning rates corresponding to the best performance in LoRA and FFT are within less gap, suggesting more aligned fine-tuning solutions. And the optimal learning rates are not impacted by relatively small rank, see FIG. 11 (b). Combining these results we can see that the artificial entanglement signatures are actually sensitive to hyper-parameter settings, including α (FIG. 9), time (FIG. 4) and rank (FIG. 5). Besides, entanglement signatures might serve as an indicator of the combined effect of multiple hyper-parameter choices (See detailed discussion in Sec. IV). While the above analysis indicates that LoRA and FFT exhibit different artificial entanglement profiles, exhibiting different fine-tuning solutions (e.g., the updates of projection matrices and the optimal learning rate), puzzle still remains: Why are the resulting test losses still approximate even when the artificial entanglement profiles are different? When only focusing on LoRA, we can formulate this problem as: Why different settings of hyper-parameters that result in different artificial entanglement profiles in LoRA can still result in similar test loss (e.g., see FIG. 10, the final test loss is approximately the same when α = 1 and α = 64, ignoring the fluctuations)? To explore this problem, we extend our artificial entanglement analysis framework from only the updates of WQ and WV (i.e., WQ and WV ) to attention matrix A(X0) and attention outputs X, specifically the XX . This choice is natural because XX captures the pairwise correlations between token representations after attention mixing, analogous to how correlation is probed in quantum many-body systems via reduced density matrices (i.e., XX serves as the analogue of an output-state density operator). Artificial Entanglement Profiling of SA and SXX . Generally, for an input X0 RBT dmodel, we have the attention tensor A(X0) RBHT where is the batch size, is the number of tokens, dmodel is the token embedding dimension and is the number of heads. Here we select samples where the sequence length can be directly factorized by 2 (e.g., = 1024). Then, we focus on each attention matrix in separate attention head. Therefore, for each head, we obtain matrix RT that can be formalized to an MPS to profile the artificial entanglement. Here we show an entanglement profiling specifically in Head 10 (More heads are shown in normalized form in FIG. 13), see FIG. 12. Across all training steps, the entanglement entropy SA remains uniformly low. Such profile is consistent with an approximate area law scaling: exhibits shallow, nearly logarithmic rise as the bi-partitioning position moving (in this case from right to left across the MPS, followed by saturation around the central partition). Though not strict area law scaling, the profiling deviates much from volume law and can often FIG. 9. Entanglement entropy in the middle bi-partition for different α. in early time (Step = 1) serves as baseline. By showing the converged solution at late time (Step = 1000), this demonstrates how α {1, 2, 4, 8, 16, 32, 64, 128, 256} impacts the solution learned by LoRA. FIG. 10. Test loss comparison across the scaling factor α in LoRA. X-axis is shown in log scale. FIG. 11. Final test loss with respect to learning rate under different LoRA configurations. (a) Comparison of full fine-tuning with LoRA at multiple α when rank = 256 . Larger α values shift the optimal learning rate to be smaller, approaching the solutions from full-finetuning. (b) Joint variation of and α for LoRA. The settings = 32 and = 256 do not shift the optimal learning rate in different α. ter characterize this phenomenon in terms of both S, learning rates and final test performance, we conduct below experiments: (i) We visualize the entanglement entropy in the middle bi-partition position with respect to the training steps (Specifically evaluating at the early time in step 1 and the late time in step 1000 respectively) and α, see FIG. 9; (ii) We visualize how α changes the final test loss while keeping all other hyper-parameters fixed, see FIG. 10. (iii) We sweep the learning rates by setting α = {16, 256, 512, 1024, 4096, 8192}, where the α is set higher or 6 FIG. 12. Artificial Entanglement Profiling of the attention matrix (Head 10) SA across training steps. SA remains uniformly low throughout training and exhibits characteristic of an approximate area-law scaling, where the entropy grows only mildly that roughly following shallow logarithmic rise. This indicates that the attention matrix operates in strongly low-entanglement regime. FIG. 14. Normalized entanglement entropy SA/ log(χ) of six representative attention heads across training steps and LoRA scaling coefficients α. remains significantly below the theoretical maximum and shows negligible variation with either training time or the choice of α, suggesting no-hair like behavior. Entanglement entropy SO of the output operator = FIG. 15. XX across bi-partition positions and training steps. Panels (a) and (b) show that SO remains significantly below the theoretical maximum. Panel (c) illustrates that this behavior is invariant across the variation of scaling coefficient α and training steps, indicating nohair like behavior as well. attention are largely insensitive with respect to training steps and α, see FIG. 14. In other words, even though different hyper-parameters or fine-tuning methods lead to different artificial entanglement profiling (e.g., FIG. 3, FIG. 4, FIG. 9), from the resulting attention matrices those entanglement signature variations do not exhibit. Similar phenomena can be observed in the attention output operator = XX , see FIG. 15. We suggest two reasons might prompt these phenomena: First, the entanglement between text tokens is significantly low; Second, masking operation results in many values to be zero after the softmax operation due to the autoregressive nature, further resulting in relatively low entanglement structure. Regarding the second, we conduct an experiment to see how masking operation influences the results, see Appendix for details. C. Perspective Based on Random Matrix Theory We study the artificial entanglement of the attention matrix and the attention output under an asymptotic model based on random matrix theory. Under assumptions consistent with [48], we explain the area law scaling with logarithmic correction observed in the artificial entanglement of the attention matrix. Furthermore, we identify the conditions unFIG. 13. Normalized entanglement entropy SA/ log(χ) of six representative attention heads across training steps, where χ = min(dL, dR) denotes the bond dimension, and log(χ) represents the theoretical maximum entanglement entropy. Most heads exhibit consistently mild scaling throughout training, far from the volume law, indicating that the effective correlations encoded by the attention mechanism remain far below the theoretical maximum. be treated as area law with logarithmic correction [29]. This behavior indicates that the correlations encoded by this attention head are highly localized and remains far from the high-entanglement (volume-law) regime. FIG. 13 shows that the phenomenon is prevalent across multiple heads through normalizing the via dividing it by the theoretical allowed entanglement log(χ), where χ = min(dL, dR). Notably, χ changes across different cuts, so as the theoretical allowed entanglement log(χ). Specifically, χ reaches its maximum at the central partition (where dL = dR), making the theoretical allowed entanglement largest at the middle cuts. This variation in the normalization factor makes the dip in the middle of the normalized entropy curves more pronounced. Besides, no hair property exhibits: the entanglement structures in 7 der which the attention output exhibits low entropy as the number of tokens increases, and discuss the relationship between these findings and rank collapse in width [48]. We sketch the theories here, and leave full preliminaries and details in Appendix VII and Appendix VIII. While the main focus is von Neumann entropy that is the standard measure in quantum information theory, our theoretical results also include the analysis of Renyi-2 entropy that depends only on the purity Tr(ρ2) of the reduced state, since Renyi-2 entropy provides more tractable quantity for theoretical analysis and offers simpler closed-form expressions. Additionally, Renyi-2 entropy provides lower bounds on von Neumann entropy, useful for theoretical characterization. For detailed discussion of the relationship between von Neumann and Renyi entropies, see Appendix VII C. The Attention Cardy Formula: Criticality in the Bulk. Following the outlierbulk law established for softmax attention at isotropic initialization [48], we decompose the attention matrix as = 1 11 + A. Here, the rank-1 component 1 11 captures the mean-field behavior, while the residual governs the fluctuations, converging to quartercircular law (a deterministic distribution of singular values shaped like quarter-circle in the large-T limit, see Appendix VIII for details). Below we establish logarithmic scaling for entanglement entropy, mathematically analogous to the celebrated Cardy formula in conformal field theory (CFT) [46, 47]: Theorem II.1 (The Attention Cardy Formula). Under Assumption 1, let be the sequence length (number of tokens). In the limit , the von Neumann entropy scales as: S(A) = Cattn log + const + o(1), (6) where the prefactor Cattn (termed as the Effective Attention Charge) is determined by the bulk spectrum spread parameter σ > 0 of the limiting quartercircular law Qσ for the rescaled singular values of A: (cid:16) (cid:17)2 x(T ) log log A2 are the rescaled singular values converging to the quarter- (cid:1) = circular law. Expanding log pi = log(cid:0)si(A)2/A2 log , the log term emerges from the sum (cid:80) )2/T σ2, yielding the prefactor σ2/(1 + σ2) in the logarithmic scaling. The convergence follows from treating the normalized sum as Riemann sum that converges to the integral defined by the limiting quartercircular distribution. This spectral accumulation is structurally identical to the density of states in gapless quantum systems. Full details are in Appendix VIII. i(x(T ) 3 log L) for critical quantum systems. Remark (From CFT to Attention Cardy Formula). We draw parallel between our result and the Cardy formula (S In physics, strictly gapped systems follows strict area law where saturates to constant (S O(1)), implying finite correlation length. In contrast, Eq. (6) shows that the attention mechanism follows an area law with logarithmic correction (S log ). This logarithmic divergence is the hallmark of critical phase [29], allowing the model to maintain nonvanishing dependencies between distant tokens. Therefore we refer to this critical scaling behavior as the Attention Cardy Formula, highlighting its role in enabling long-context capabilities beyond the limitations of strict area-law saturation. bridge between stable rank and entanglement. To study the output = AV , we first relate entanglement to the stable rank of Σ = XX . The following lemma shows that when the stable rank [48] is close to 1, the entanglement must be small. Lemma II.2 (Stable rank implies small entanglement). Let Σ = XX with eigenvalues λ1 λT and stable rank λi rstable = Tr(Σ2)/λ2 λ1 (the sum of ratios of non-dominant to dominant eigenvalues). Then 1. Let η = rstable 1 and δ1 := (cid:80) i2 S(X) h2(δ) + δ log(T 1), (8) Cattn := σ2 1 + σ2 . See Assumption 1 for details on σ. (7) where δ1 (cid:112)(T 1)η, δ = min{1, δ1}, and h2(u) := log (1 u) log(1 u). For Renyi-2 entropy bounds, see Appendix VII C. Specifically, σ controls the width of the bulk spectrum distribution: larger σ corresponds to more spread-out distribution of singular values in the bulk, leading to higher entanglement entropy. The parameter σ is determined by the second moment m2 of the quartercircular law via m2 = σ2, which in turn relates to the Frobenius norm of as A2 σ2 in the large-T limit. For results on Renyi-2 entropy and its relationship to von Neumann entropy, see Appendix VII C. Sketch of proof. The decomposition A2 = 1 + A2 combined with the quartercircular limit implies A2 σ2. Since the operator norm of scales as O(T 1/2), the bulk singular values satisfy si(A) = O(T 1/2) for 2, and the entanglement spectrum weights are pi = si(A)2/A2 = O(1/T ) for 2. For the von Neumann entropy, the tail contribution (cid:80) i2 pi log pi is computed )2/T where x(T ) using the representation si(A)2 = (x(T ) i2 λ2 Sketch of proof. By CauchySchwarz, ((cid:80) . Dividing by λ2 i2 λi)2 (T 1) (cid:80) 1 and using the definition i2(λi/λ1)2 gives δ1 (cid:112)(T 1)η. of η = rstable 1 = (cid:80) Since Tr(Σ) = λ1(1 + δ1), the normalized density matrix ρ = Σ/Tr(Σ) has top eigenvalue λmax(ρ) = 1/(1 + δ1), implying 1 λmax(ρ) = δ1/(1 + δ1) δ. With p1 1 δ and tail mass (cid:80) i2 pi δ, the von Neumann entropy is maximized when the tail mass is uniformly distributed across the remaining 1 eigenvalues, yielding the bound S(ρ) h2(δ) + δ log(T 1). For Renyi-2 entropy, using 1 = 1/(1 + δ1)2 gives S2(ρ) 2 log(1 + δ1). Tr(ρ2) p2 Appendix VIII provides the full arguments. Output entanglement collapse under stable rank collapse. in width-limited theory [48] shows that regimes, the stable rank of XX may collapse to 1+O(T 3). Plugging this into Lemma II.2 yields: Recent 8 If Theorem II.3 rstable(XX ) 1 = O(T 3) with overwhelming probability5, then in the limit , entanglement collapse). (Output S(X) = (cid:19) (cid:18) log 0. (9) Thus the tokenfeature entanglement vanishes in the rankcollapsed regime. For Renyi-2 entropy scaling, see Appendix VII C. Sketch of proof. Apply Lemma II.2 with η = rstable(Σ) 1 = O(T 3). The bound δ1 (cid:112)(T 1)η from the lemma gives δ1 = O(T 1), and hence δ = min{1, δ1} = O(T 1). The lemma yields S(X) h2(δ) + δ log(T 1). Since h2(δ) = O(δ log(1/δ)) and δ = O(1/T ), we have h2(δ) = O((log )/T ), while δ log(T 1) = O((log )/T ). Therefore S(X) = O((log )/T ) 0. For Renyi-2 entropy, the lemma gives S2(X) 2 log(1 + δ1) = O(δ1) = O(1/T ) 0, vanishing even faster. This reflects that almost all spectral weight concentrates on the top eigenvalue when the stable rank approaches 1. Full details are given in Appendix VIII. Entanglement entropy SA/ log(χ) across training FIG. 16. steps for several attention heads under scaling coefficients α {8, 16, 32, 64}. Across all heads, the entanglement curves remain approximately invariant with respect to both α and the training step. This is similar to the behavior observed in LoRA (See FIG. 14). D. Artificial Entanglement Profiling in MPS Adaptation While the MPS modeling above is adopted for artificial entanglement profiling of LoRA and FFT (where we decompose their parameter updates into MPS representations to analyze entanglement structure), we can extend this analysis framework to fine-tuning method that directly parameterizes weight updates using MPS structure, which is also parameter efficient and we term as MPS adaptation. The key distinction is that MPS adaptation is PEFT method (a way to parameterize trainable updates), whereas MPS modeling is an analysis tool (a way to decompose and analyze parameter structures). By applying the same entanglement profiling framework to MPS adaptation, we can investigate whether it exhibits similar entanglement signatures (such as the entanglement valley observed in LoRA) and compare the entanglement structures across different PEFT paradigms. For detailed explanation of MPS adaptation, its relationship to MPS modeling, and why it can be parameter-efficient, see Appendix IX. To investigate this, we implement simple version of MPS adaptation: We formalize Rrdin into two Order-3 tensors whose contraction reconstructs the effective matrix A, where we denote it as AMPS. Specifically, we factorize the input dimension as din = d1d2 and parameterize AMPS = χ (cid:88) α=1 A[1] r,α,d1 A[2] α,1,d , (10) where each A[i] is an Order-3 tensor and the bond dimension χ controls the entanglement capacity. Analogous to LoRAs BA where Rdoutr and Rrdin , MPS = α adaptation replaces the full matrix with an MPS factorization AMPS. The weight update is then: = α BAMPS = α (cid:32) χ (cid:88) α=1 (cid:33) A[1] r,α,d1 A[2] α,1,d2 , (11) where the contraction over the bond index α reconstructs AMPS Rrdin from the tensor network factorization, and remains the same matrix as in LoRA. This MPS parameterization of can reduce the number of parameters compared to storing the full din matrix under specific settings, see concrete example in Appendix IX. Artificial Entanglement Profiling in MPS adaptation. We conduct below experiments: (i) We conduct the artificial entanglement profiling of WQ and WV under different (or χ), see FIG. 18. To simplify the problem, we set = χ, and therefore we do not set or χ exceeds min{dout, d1, d2}. Here we respectively set = {4, 32}, and learning rate to be 2 102. Other settings follow Default Experimental Setups in Sec. II B. (ii) We conduct the artificial entanglement profiling of SA, see FIG. 16. (iii) We vary α and learning rates to see the shift of the optimal learning rates, see FIG. 19. Combining the results, in MPS adaptation we can observe rich entanglement profiling results in the updates of projection matrices (WQ and WV ) and the no-hair property. Besides, similar to LoRA, when increasing α, the optimal learning rates are shifted to be smaller as the increase of α, approaching the solutions of LoRA and FFT. Besides, though possessing different entanglement profiling, MPS adaptation does not induce an obvious test loss decrease in the optimal setting, approximating the test performance of LoRA. III. METHODS 5 Overwhelming probability means 1 for every fixed > 0, for all sufficiently large . This section details the methodology for artificial entanglement analysis, especially we establish an analogy between the 9 FIG. 17. Overall framework for artificial entanglement analysis. (i) Reshaping matrices including the updates of weight projection matrices such as WQ and WV from FFT or LoRA, and attention-related matrices into higher-order tensors by factorizing input and output dimensions into prime components; (ii) Performing sequence of SVDs to decompose the tensor into chain of Order-3 tensors connected by virtual bonds, where virtual bond indices capture correlations between sites; (iii) Interpreting the MPS factorization as mathematical formalism of many-body states, formally analogous to the MPS representation of the amplitude tensor of quantum many-body state; (iv) Computing the von Neumann entanglement entropy (S) at each bond, yielding an artificial entanglement profile. FIG. 18. Artificial entanglement profiling of the MPS adaptation of for ranks {4, 32} with α = 16 across training steps. Both rank settings exhibit characteristic entanglement valley structure, while the rank-4 case displays more pronounced bi-modal (dualIn contrast, the rank-32 curves form smoother valley) profile. single-valley landscape. FIG. 19. Final test loss of LoRA and MPS adaptation across multiple learning rates and scaling coefficients α. Compared to LoRA, the MPS adaptation exhibits an optimal learning-rate region that is consistently shifted toward larger learning rates. MPS factorization of matrices and the MPS representation of quantum many-body states: we here detail the interpretation of the MPS factorization of matrices as formally analogous to the amplitude tensor of quantum many-body state Ψ. We emphasize that the underlying systems are entirely classical: this is purely mathematical analogy that provides 10 principled framework for quantifying entanglement-like correlations via Schmidt decomposition. See FIG. 17 and preliminaries of tensor analysis in Appendix VI. For quantum state Ψ partitioned into two subsystems and via Schmidt decomposition: tion. For FFT, the expression is the similar, we express ΨFFT = WFFTw1 wm+n χ (cid:88) = [1]do1 α0=1,α1 [n+1]di1 αn,n+ [n+m]dim αn+m1, αn+m=1 Ψ = (cid:88) λi iA iB, (12) the entanglement entropy is calculated as: SA = SB = log λ2 λ2 (cid:88) (13) For an MPS, the two subsystems are obtained by specifying specific cut in the MPS chain, where we investigate the entanglement between the left and right subsystems after the bi-partition. We apply this framework to relevant matrices, including the updates of projection matrices WQ and WV , attention matrices A, and attention output operators = XX , by formalizing each matrix as an MPS. As an example, we focus on one of the projection matrices as . While Cdoutdin is originally an Order-2 tensor, we reshape it into higher-order tensor by factorizing its input and output dimensions into their prime components. Specifically, we iteratively divide each dimension by its smallest prime factor until the value reduces to one: α1, ,αn w1 wm+n. (16) See Appendix VI for more details about the formulation based on the factorization. Further, we calculate the von Neumann entanglement entropy by performing an SVD at each bond during the MPS decomposition. At bond (the k-th bond in the MPS chain), we obtain the singular values {λαk }, where αk indexes the singular values (Schmidt indices) at bond k. The SVD is written in terms of each bipartite quantum state Ψ HA HB component Ψij below: Ψij = χ (cid:88) α,β=1 Uiα Λαβ (V )βj = χ (cid:88) α= Uiα λα (V )αj, (17) with at most χ non-zero singular values. The bi-partition at bond corresponds to HA = H1:k and HB = Hk+1:(n+m). Since the reduced density matrices ρ1:k and ρk+1:(n+m) share the same non-zero spectrum, the bipartite entanglement entropy at bond is Sk = S(ρ1:k) = S(ρk+1:(n+m)) = χ (cid:88) αk= λ2 αk log(cid:0)λ2 αk (cid:1) , dout = din = (cid:89) k=1 (cid:89) ℓ=1 fk, f1 f2 fn, gℓ, g1 g2 gm, (14) (18) where Sk quantifies the entanglement entropy at bond k. We can calculate Sk for each bond k, and therefore we can obtain an Sk curve (artificial entanglement profile) with respect to the bond position. where fk and gℓ denote the prime factors sorted in nondecreasing order, and and denote the number of prime factors in dout and din, respectively. This decomposition yields the finest-grained tensorization of the input and output spaces and provides natural way to reshape into an Order-(n + m) tensor amenable to tensor-network factorizations, including the MPS. For LoRA, we denote Ai and Bi to be each local Order3 tensor in the MPS on the same rank space Hr, therefore WLoRA can be viewed as the probability amplitude tensor of quantum wave function represented below: ΨLoRA = WLoRAo1 omout i1 inin χ (cid:88) = α1,...,αn [1]do1 α0=1,α1 [2]do2 α1,α2 ..., [n+1]di1 αn,n+1 A[n+m]dim αn+m1, αn+m=1 o1 omout i1 inin. (15) Although the above formulation is written in terms of the decomposed factors Ai and Bi, in practice we directly decompose the equivalent WLoRA itself into the MPS representa11 IV. DISCUSSION Our results reveal that LoRA captures richer structure than previously recognized. By quantifying artificial entanglement through MPS modeling, we show that LoRA and FFT induce fitted volume-law internal entanglement profile in the projection matrices WQ and WV with distinctive entanglement valley. These findings highlight that the fitted volume-law reflects high intrinsic correlation, thereby causing LoRA to develop internal artificial entanglement signatures that fundamentally differ from those produced by FFT. Yet, in contrast to the volume-law scaling in projection matrices, attention matrices themselves exhibit an approximate area-law scaling. Despite these microscopic differences in the internal structure (the volume-law entanglement profiles in projection matrices), the external observable attention outputs remain remarkably invariant. Drawing an analogy to the No-Hair Theorem in black hole physics, we propose that the attention mechanism acts as an effective coarse-graining operator: it suppresses redundant correlations while preserving only low-correlation and task-relevant structure. This perspective offers an explanation for why LoRA performs comparably to FFT despite using far fewer parameters. The entanglement signatures we observe provide valuable diagnostic tools for PEFT methods. Specifically, the depth and evolution of the entanglement valley in projection matrices serve as indicators of hyperparameter choices: smaller scaling parameters α lead to deeper valleys that intensify during training, while larger α values mitigate the valley depth and bring the solution closer to FFT. The entanglement profiles sensitivity to rank and training dynamics further enables performance assessmentmethods that preserve taskrelevant structure should exhibit entanglement structures that align with task requirements. Moreover, the contrast between volume-law scaling in projection matrices and area-law scaling in attention matrices reveals that low-rank parameterizations can be effective even when internal structures exhibit high correlation, possibly related to the downstream attention mechanism providing effective coarse-graining. This suggests that entanglement analysis can help identify which components require careful rank selection versus those where lowrank approximations are naturally robust. These diagnostic capabilities suggest that entanglement analysis could guide hyperparameter tuning and help identify optimal adaptation strategies in PEFT. Finally, our theoretical analysis based on random matrix theory and MPS Adaptation extension indicate that the nohair behavior is not an artifact of LoRA but more universal property. This opens several directions for future work, including probing how entanglement evolves during pretraining, whether entanglement-based regularization can guide more efficient adaptation, and whether similar coarse-graining phenomena arise in other architectures or multi-task settings. Together, our findings suggest that quantum-inspired analysis may offer new foundation for understanding the mechanistic interpretability of large language models. ACKNOWLEDGMENT We thank Jesse Thaler for helpful discussions. MC, ZW, and JL are supported in part by the University of Pittsburgh, School of Computing and Information, Department of Computer Science, Pitt Cyber, Pitt Momentum fund, PQI Community Collaboration Awards, John C. Mascaro Faculty Scholar in Sustainability, Thinking Machines Lab and Cisco Research. This research used resources of the Oak Ridge Leadership Computing Facility, which is DOE Office of Science User Facility supported under Contract DE-AC05-00OR22725. [1] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever, Improving language understanding by generative pre-training, in OpenAI blog (2018). [2] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever, Language models are unsupervised multitask learners, OpenAI blog 1, 9 (2019). [3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al., Language models are few-shot learners, Advances in neural information processing systems 33, 18771901 (2020). [4] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova, Bert: Pre-training of deep bidirectional transformers for language understanding, in Proceedings of the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies, volume 1 (long and short papers) (2019) pp. 41714186. [5] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu, Exploring the limits of transfer learning with unified text-to-text transformer, Journal of machine learning research 21, 167 (2020). [6] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin, Attention is all you need, Advances in neural information processing systems 30 (2017). [7] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei, Scaling laws for neural language models, arXiv preprint arXiv:2001.08361 (2020). Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al., Training compute-optimal large language models, arXiv preprint arXiv:2203.15556 (2022). [9] Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob Jackson, Heewoo Jun, Tom Brown, Prafulla Dhariwal, Scott Gray, et al., Scaling laws for autoregressive generative modeling, arXiv preprint arXiv:2010.14701 (2020). [10] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al., Palm: Scaling language modeling with pathways, Journal of Machine Learning Research 24, 1113 (2023). [11] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al., Gpt-4 technical report, arXiv preprint arXiv:2303.08774 (2023). [12] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al., Lora: Low-rank adaptation of large language models. ICLR 1, 3 (2022). [13] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer, Qlora: Efficient finetuning of quantized llms, Advances in Neural Information Processing Systems (NeurIPS) 36, 1008810115 (2023). [14] Qingru Zhang, Minshuo Chen, Alexander Bukharin, Pengcheng He, Yu Cheng, Weizhu Chen, and Tuo Zhao, Adalora: Adaptive budget allocation for parameter-efficient fine-tuning, in International Conference on Learning Representations (ICLR) (2023). [8] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las [15] Shih-Yang Liu, Chien-Yi Wang, Hongxu Yin, Pavlo Molchanov, Yu-Chiang Frank Wang, Kwang-Ting Cheng, and Min-Hung Chen, Dora: Weight-decomposed low-rank adaptation, in Forty-first International Conference on Machine Learning (2024). [16] Dawid Kopiczko, Tijmen Blankevoort, and Yuki Asano, Vera: Vector-based random matrix adaptation, in International Conference on Learning Representations (ICLR) (2024). [17] Soufiane Hayou, Nikhil Ghosh, and Bin Yu, Lora+: Efficient low rank adaptation of large models, in International Conference on Machine Learning (ICML) (PMLR, 2024) pp. 17783 17806. [18] Mojtaba Valipour, Mehdi Rezagholizadeh, Ivan Kobyzev, and Ali Ghodsi, Dylora: Parameter-efficient tuning of pre-trained models using dynamic search-free low-rank adaptation, in Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics (EACL) (2023) pp. 32743287. [19] Longteng Zhang, Lin Zhang, Shaohuai Shi, Xiaowen Chu, low-rank adaptaand Bo Li, Lora-fa: Memory-efficient tion for large language models fine-tuning, arXiv preprint arXiv:2308.03303 (2023). [20] Mingyang Zhang, Hao Chen, Chunhua Shen, Zhen Yang, Linand Bohan Zhuang, Loraprune: Struclin Ou, Xinyi Yu, tured pruning meets low-rank parameter-efficient fine-tuning, in Findings of the Association for Computational Linguistics: ACL 2024 (2024) pp. 30133026. [21] Bojia Zi, Xianbiao Qi, Lingzhi Wang, Jianan Wang, Kam-Fai Wong, and Lei Zhang, Delta-lora: Fine-tuning high-rank parameters with the delta of low-rank matrices, arXiv preprint arXiv:2309.02411 (2023). [22] Vladislav Lialin, Namrata Shivagunde, Sherin Muckatira, and Anna Rumshisky, Relora: High-rank training through lowrank updates, arXiv preprint arXiv:2307.05695 (2023). [23] Chunyuan Li, Heerad Farkhoor, Rosanne Liu, and Jason Yosinski, Measuring the intrinsic dimension of objective landscapes, in International Conference on Learning Representations (ICLR) (2018). [24] Reece Shuttleworth, Jacob Andreas, Antonio Torralba, and Pratyusha Sharma, Lora vs full fine-tuning: An illusion of equivalence, arXiv preprint arXiv:2410.21228 (2024). [25] Damjan Kalajdjieski, Rank-stabilized lora: Unlocking the potential of lora fine-tuning, arXiv preprint arXiv:2312.03732 (2024). [26] Yue Pan, Yijie Zhou, Feng Wu, and Zhen Zhang, Lora training in the ntk regime has no spurious local minima, arXiv preprint arXiv:2402.11867 (2024). [27] Luca Bombelli, Rabinder Koul, Joohan Lee, and Rafael Sorkin, Quantum source of entropy for black holes, Physical Review 34, 373 (1986). [28] Mark Srednicki, Entropy and area, Physical Review Letters 71, 666 (1993). [29] Jens Eisert, Marcus Cramer, and Martin Plenio, Area laws for the entanglement entropy-a review, arXiv preprint arXiv:0808.3773 (2008). [30] Werner Israel, Event horizons in static vacuum space-times, Physical review 164, 1776 (1967). [31] Brandon Carter, Axisymmetric black hole has only two degrees of freedom, Physical Review Letters 26, 331 (1971). [32] Sourav Bhattacharya and Amitabha Lahiri, No hair theorems for positivelambda, arXiv preprint gr-qc/0702006 (2007). [33] NE Mavromatos, Eluding the no-hair conjecture for black holes, arXiv preprint gr-qc/9606008 (1996). [34] Werner Israel, Event horizons in static electrovac spacetimes, Communications in Mathematical Physics 8, 245260 (1968). [35] David Robinson, Uniqueness of the kerr black hole, Physical Review Letters 34, 905 (1975). [36] Daniel Harlow, Jerusalem lectures on black holes and quantum information, Reviews of Modern Physics 88, 015002 (2016). [37] Guifre Vidal and Reinhard Werner, Computable measure of entanglement, Physical Review 65, 032314 (2002). [38] Guifre Vidal, Efficient classical simulation of slightly entangled quantum computations, Physical review letters 91, 147902 (2003). [39] Guifre Vidal, Jose Ignacio Latorre, Enrique Rico, and Alexei Kitaev, Entanglement in quantum critical phenomena, Physical review letters 90, 227902 (2003). [40] Guifre Vidal, Efficient simulation of one-dimensional quantum many-body systems, Physical review letters 93, 040502 (2004). [41] Guifre Vidal, Entanglement renormalization, Physical review letters 99, 220405 (2007). [42] Roman Orus, Tensor networks for complex quantum systems, Nature Reviews Physics 1, 538550 (2019). [43] Aleksandr Berezutskii, Minzhao Liu, Atithi Acharya, Roman Ellerbrock, Johnnie Gray, Reza Haghshenas, Zichang He, Abid Khan, Viacheslav Kuzmin, Dmitry Lyakh, et al., Tensor networks for quantum computing, Nature Reviews Physics 7, 581593 (2025). [44] Ashley Milsted, Junyu Liu, John Preskill, and Guifre Vidal, Collisions of false-vacuum bubble walls in quantum spin chain, PRX Quantum 3, 020316 (2022). [45] Luigi Amico, Rosario Fazio, Andreas Osterloh, and Vlatko Vedral, Entanglement in many-body systems, Reviews of modern physics 80, 517576 (2008). [46] Pasquale Calabrese and John Cardy, Entanglement entropy and quantum field theory, Journal of Statistical Mechanics: Theory and Experiment 2004, P06002 (2004). [47] Pasquale Calabrese and John Cardy, Entanglement entropy and conformal field theory, Journal of physics a: mathematical and theoretical 42, 504005 (2009). [48] Thiziri Nait Saada, Alireza Naderi, and Jared Tanner, Mind the gap: spectral analysis of rank collapse and signal propagation in attention layers, in Forty-second International Conference on Machine Learning (2024). [49] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al., The llama 3 herd of models, arXiv e-prints , arXiv2407 (2024). [50] Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, et al., Tulu 3: Pushing frontiers in open language model post-training, arXiv preprint arXiv:2411.15124 (2024). [51] Etash Guha, Ryan Marten, Sedrick Keh, Negin Raoof, Georgios Smyrnis, Hritik Bansal, Marianna Nezhurina, Jean Mercat, Trung Vu, Zayne Sprague, et al., Openthoughts: Data recipes for reasoning models, arXiv preprint arXiv:2506.04178 (2025). [52] Minzhao Liu, Changhun Oh, Junyu Liu, Liang Jiang, and Yuri Alexeev, Simulating lossy gaussian boson sampling with matrix-product operators, Physical Review 108, 052604 (2023). [53] Min Chen, Minzhao Liu, Changhun Oh, Liang Jiang, Yuri Alexeev, and Junyu Liu, Towards symmetry-aware efficient simulation of quantum systems and beyond, arXiv e-prints , arXiv2303 (2023). [54] Francesco Mezzadri, How to generate random matrices from the classical compact groups, arXiv preprint math-ph/ 13 [59] Jianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng Liu, Roformer: Enhanced transformer with rotary position embedding, Neurocomputing 568, 127063 (2024). [60] Afia Anjum, Maksim Eren, Ismael Boureima, Boian Alexandrov, and Manish Bhattarai, Tensor train low-rank approximation (tt-lora): Democratizing ai with accelerated llms, in 2024 International Conference on Machine Learning and Applications (ICMLA) (IEEE, 2024) pp. 583590. [61] Zhidong Bai and Jack Silverstein, Spectral analysis of large dimensional random matrices, Vol. 20 (Springer, 2010). [62] Robert Thompson, The behavior of eigenvalues and singular values under perturbations of restricted rank, Linear Algebra and its Applications 13, 6978 (1976). [63] Vladimir Marˇcenko and Leonid Andreevich Pastur, Distribution of eigenvalues for some sets of random matrices, Mathematics of the USSR-Sbornik 1, 457 (1967). (2006). [55] Don Page, Average entropy of subsystem, Physical review letters 71, 1291 (1993). [56] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al., Llama: Open and efficient foundation language models, arXiv preprint arXiv:2302.13971 (2023). Schulman without [57] John Lora Connectionism https://thinkingmachines.ai/blog/lora/. and regret, (2025), Thinking Machines Thinking Machines Lab, Lab: 10.64434/tml.20250929, [58] Dan Biderman, Jacob Portes, Jose Javier Gonzalez Ortiz, Mansheej Paul, Philip Greengard, Connor Jennings, Daniel King, Sam Havens, Vitaliy Chiley, Jonathan Frankle, et al., Lora learns less and forgets less, arXiv preprint arXiv:2405.09673 (2024). CONTENTS V. Notations VI. Preliminaries and Related Works VII. Mathematical Foundations: Quantum Entanglement and Entropy Measures A. Computing Entanglement Entropy at Each MPS Cut B. Discussion: Spectral Gap, Stable Rank, and Entanglement Entropy C. Von Neumann and Renyi Entropies: Definitions and Relationships VIII. Theorems and Proofs A. Setup: Attention Matrix and Output B. Modeling Assumptions C. Entanglement of the Attention Matrix D. General Bridge Between Stable Rank and Entanglement Bounds E. Output Entanglement Collapse from Stable Rank Collapse IX. MPS Adaptation: Method and Relationship to MPS Modeling X. More Experimental Results A. Ablation Study on Causal Masking in Autoregressive Attention B. The Entanglement Valley C. Limits of the No-Hair Property Under Extreme Scaling Coefficients D. Artificial Entanglement Profiling: LLaMA-3.2-1B, OpenThoughts3 Dataset E. Artificial Entanglement Profiling: LLaMA-3.1-8B, Tulu3 Dataset F. Artificial Entanglement Profiling: LLaMA-3.1-8B, OpenThoughts3 Dataset Table provides list of notations. V. NOTATIONS 15 15 18 18 19 20 20 20 21 22 24 25 27 27 27 28 31 33 36 VI. PRELIMINARIES AND RELATED WORKS Tensor, Tensor Network, and Matrix Product State (MPS). Tensors generalize matrices to multi-index arrays and provide unified language for representing high-dimensional structures. An Order-N tensor Ai1...iN has indices, where each ik {0, 1, . . . , dk1} ranges over local dimension dk. Graphically as shown in FIG. 20 (a), one can visualize tensor as node with legs, each leg corresponding to an index of the tensor. fundamental operation between tensors is tensor contraction, which corresponds to summing over pair of shared indices. In FIG. 20 (b), contraction is depicted by connecting the legs associated with the contracted dimensions. More elaborate contraction patterns naturally give rise to tensor networks, which provide structured factorization of large tensors into networks of smaller building blocks. Among various tensor network architectures, the Matrix Product State (MPS) is the most widely used one-dimensional tensor network. An MPS expresses high-order tensor as chain of Order-3 tensors connected by virtual bonds. In FIG. 20 (c), each site emits one physical leg (the physical index) and two virtual legs linking to its neighbors, forming one-dimensional chain as depicted in the figure. Consider general -body pure quantum state on local Hilbert space of dimension d: d1 (cid:88) Ψ = i1,...,iM =0 ci1,...,iM i1, . . . , iM , (19) where ci1...iM is an amplitude tensor of Order-M whose storage cost grows exponentially with . The MPS representation approximates this tensor by factorizing it into sequence of Order-3 tensors: ci1,...,iM = χ1 (cid:88) χM 1 (cid:88) α1=1 αM 1=1 [1] i1 1,α1 [2] i2 α1,α2 [M ] iM αM 1,1, (20) 15 TABLE I. Notations BA. Description Pre-trained weight projection matrix. Weight update matrix. LoRA weight update: WLoRA = α Full fine-tuning weight update. Output projection matrix. Low-rank matrices in LoRA decomposition (A Rrdin , Rdoutr). Attention matrix (context-dependent). Attention output operator. Input and output dimensions of weight projection matrices. Token embedding dimension (model dimension). Dimension of each attention head. Dimensions of left and right subsystems at cut k. Minimum and maximum dimensions (dmin dmax). Local dimension at site (context-dependent). Sequence length (number of tokens). Number of attention heads. Batch size. Order of tensors (number of indices). Number of sites in MPS factorization. Prime factors in factorization of dout and din. LoRA rank (r min(din, dout)). LoRA scaling hyperparameter. Bond dimension (at bond k). Maximum bond dimension (truncation threshold). Maximum bond dimension: χ := maxk χk. k-th tensor in MPS decomposition. MPS tensors for LoRA matrices and B. Virtual bond index (Schmidt index). Physical index at site k. Matrix at cut in MPS construction. Symbol W0 WLoRA WFull WQ, WK , WV Query, key, value projection matrices. A, = XX din, dout dmodel dk d(k) , d(k) dmin, dmax dk M, n, fk, gℓ α χ, χk χmax χ [k] A[k], B[k] αk ik (k) (k), S(k), (k) SVD factors at cut k. S, Sk S2 Sα ρ(k) λαk , s(k) p(k) σ Cattn rstable η λi xi = dmin λi Normalized eigenvalues for MP distribution. ρ(x) Ψ iL , iR HL, HR HA, HB X0 A(X0) Qh, Kh, Vh R, op Tr() vec() o(1) Marchenko-Pastur density function. Aspect ratio: = dmin/dmax. Many-body quantum state. Basis states for left and right subsystems. Left and right Hilbert spaces. Subsystem Hilbert spaces. Input tensor: X0 RBT dmodel . Attention tensor: A(X0) RBHT . Query, key, value matrices for head h. Centered attention matrix: = 1 Attention layer output: = AV . Real and complex number fields. Frobenius norm. Operator norm (largest singular value). Trace operator. Vectorization operator. Asymptotically vanishing term. Von Neumann entanglement entropy (at cut k). Renyi-2 entropy. Renyi-α entropy. Reduced density matrix of left subsystem at cut k. Singular values (Schmidt coefficients). Eigenvalues of reduced density matrix. Bulk spectrum spread parameter. Effective Attention Charge: Cattn = σ2 Stable rank: rstable = Tr(Σ2) Stable rank excess: η = rstable 1. Eigenvalues (context-dependent). 11. 1+σ2 . λ2 . 16 FIG. 20. Overview of Tensor Diagrammatic Notation. (a) Visual representations of scalar, vector, matrix, and an arbitrary order-N tensor. (b) Tensor contractions illustrating matrix multiplication and multiple tensor products. (c) The process of modeling an order-N tensor as Matrix Product State. where each [k] is an Order-3 tensor with one physical index ik and two virtual indices αk1, αk. The virtual indices are contracted, yielding the chain-like network structure. The parameter χk denotes the bond dimension at bond k, which may vary along the chain. The quantity χ := maxk χk is often referred to as the (maximum) bond dimension, which determines the expressive power of the MPS and controls the entanglement it can capture: larger χ allows more accurate approximations of c, while small χ corresponds to compressed representation. In quantum physics, the effectiveness of the MPS formalism is deeply tied to the entanglement structure of the underlying state. For example, ground states of gapped one-dimensional systems satisfy an entanglement area law, which implies that they can be faithfully approximated by an MPS with modest bond dimension. This makes MPS not only powerful conceptual framework but also practical computational ansatz for representing high-dimensional quantum states with polynomial resources. Forming an MPS from high-order tensor. As depicted in FIG. 20 (c), an MPS representation can be systematically constructed from high-order tensor by sequence of reshaping and singular value decompositions (SVDs). Consider an OrderM tensor ci1i2...iM , which may be viewed as the probability amplitude tensor of an -body quantum state. The construction proceeds iteratively from left to right. In the first step, we group the first index i1 into left subsystem and the remaining indices (i2, . . . , iM ) into right subsystem, reshaping the tensor into matrix Performing singular value decomposition, (1) i1, (i2...iM ) CddM 1 . (1) = (1)S(1)V (1), we absorb (1) into the first MPS tensor [1], while the product S(1)V (1) is passed to the next step. Explicitly, [1] i1 α0=1,α1 := (1) i1,α1 , (21) (22) (23) where α1 labels the singular vectors and defines the first virtual bond. In the second step, the remaining object S(1)V (1) is reshaped into matrix by grouping (α1, i2) as the left index and (i3, . . . , iM ) as the right index. Applying another SVD yields from which we define the second MPS tensor (2) (α1i2), (i3...iM ) = (2)S(2)V (2), [2] i2 α1α2 := (2) (α1i2),α2 . (24) (25) This procedure is repeated sequentially, each time isolating one physical index ik and generating new virtual bond αk, until the final site is reached. After 1 such decompositions, the original tensor is factorized as ci1,...,iM = (cid:88) [1] i1 1,α1 [2] i2 α1,α [M ] iM αM 1,1, α1,...,αM 1 (26) 17 which is precisely the MPS form. Each tensor [k] carries one physical index ik and two virtual indices αk1, αk, forming onedimensional chain as illustrated in the figure. In practice, the singular values in each S(k) may be truncated, retaining only the largest χk values. This truncation controls the bond dimension at each bond and yields an efficient compressed representation. Crucially, the maximum bond dimension required to accurately represent the state is directly related to its entanglement across bi-partitions. Self Attention in Large Language Models. Generally, for an input X0 RBT dmodel, the model employs MultiHead Attention to capture diverse contextual dependencies. The mechanism utilizes projection matrices Q, K, Rdmodel(Hdk) to generate query, key, and value representations, where dk is the dimension of each head. These projected features are split into parallel heads. Let Qh, Kh, Vh denote the segments corresponding to head h. In modern LLMs such as LLaMA, the query and key vectors are typically augmented with Rotary Position Embedding (RoPE) [59], which encodes positional information by applying rotational transformations to the query and key vectors. This allows the model to capture relative positional relationships effectively. After applying RoPE, the attention weights are computed via scaled dot-product: A(X0)h = Softmax (cid:19) (cid:18) QhK dk RBT (27) The attention output for each head is then derived as Headh = A(X0)hVh RBT dk . Finally, all heads are concatenated and projected by an output matrix R(Hdk)dmodel to form the final layer output. Related Studies. Some empirical evidence provided by Thinking Machines Lab [57] offers intuitive visualizations of the performance in LoRA and FFT. We echo some findings in our study and advance these observations. An important feature of ours can be concluded as identifying no-hair behavior that justifies the effectiveness of LoRA. [24] discusses different spectral properties of the resulting final weights obtained by LoRA and FFT, compared with pretrained weights. We focus on (i) the direct fine-tuning solutions , the attention matrices and outputs; (ii) the entanglement structure; (iii) test performance w.r.t hyper-parameters, while they focus on generalization and forgetting. [48] shows phenomenon termed as rank collapse in width, both in attention matrices and attention outputs. We theoretically link our findings to this perspective (See Section II C). Notably, TT-LoRA [60] utilizes an equivalent Tensor Train decomposition. While it focuses on storage efficiency, we utilize the tensor network formalism to probe entanglement signatures. We do not focus on other variants of LoRA but the original LoRA itself. VII. MATHEMATICAL FOUNDATIONS: QUANTUM ENTANGLEMENT AND ENTROPY MEASURES This section establishes the mathematical foundations for computing and analyzing entanglement entropy in the context of MPS-decomposed weight projection matrices. We first explain how to compute entanglement entropy at each cut in the MPS construction, connecting the SVD procedure to the quantum information formalism of bipartite pure states and reduced density matrices. We then discuss important subtleties in understanding entanglement: why spectral gap alone does not guarantee small entanglement, and how stable rank provides quantitative measure of mass concentration that better characterizes the relationship between spectral structure and entanglement entropy. Finally, we review the definitions of von Neumann and Renyi entropies and their relationships for completeness. A. Computing Entanglement Entropy at Each MPS Cut Recall from the MPS construction procedure (Appendix VI) that we systematically decompose high-order tensor into an MPS by performing sequence of SVDs. At each step k, we reshape the tensor to form matrix (k) that partitions the system into left subsystem (indices i1, . . . , ik or virtual bonds α1, . . . , αk1 combined with ik) and right subsystem (remaining indices ik+1, . . . , iM ). This bi-partition defines cut in the MPS, and the SVD at this cut, (k) = (k)S(k)V (k), (28) directly reveals the entanglement structure across this bi-partition. In this subsection, we explain how to compute the entanglement entropy at each such cut. Bipartite Pure States from MPS Cuts. At cut k, the matrix (k) has dimensions d(k) of the left subsystem and d(k) between the left and right subsystems. To make this connection explicit, we fix orthonormal bases {iL}d(k) space HL is the dimension is the dimension of the right subsystem. We view this matrix as representing bipartite pure state iL=1 for the left Hilbert . The vectorization of (k) embeds it into the tensor and {iR}d(k) , where d(k) iR=1 for the right Hilbert space HR d(k) = Cd(k) = Cd(k) L 18 product space:"
        },
        {
            "title": "Normalizing yields the pure state",
            "content": "vec(C (k)) := d(k) L(cid:88) d(k) R(cid:88) iL=1 iR=1 (k) iL,iR iL iR . (cid:12) (cid:12) (cid:12)ψ(k)(cid:69) := vec(C (k)) (k)F HL HR, (k)2 = (cid:88) iL,iR (k) iL,iR 2. (29) (30) Reduced Density Matrix and Entanglement Spectrum. The reduced density matrix of the left subsystem is obtained by tracing out the right subsystem:"
        },
        {
            "title": "A direct calculation shows that",
            "content": "ρ(k) := TrR (cid:16)(cid:12) (cid:12) (cid:12)ψ(k)(cid:69) (cid:68) ψ(k)(cid:12) (cid:12) (cid:12) (cid:17) . ρ(k) = (k)(C (k)) Tr(C (k)(C (k))) = (k)(C (k)) (k)2 . (31) (32) The entanglement spectrum is obtained directly from the SVD of (k). Let (k) = (k)S(k)V (k) with singular values s(k) 2 s(k) s(k) > 0, where is the rank of (k). Since 1 (k)(C (k)) = (k) diag((s(k) 1 )2, . . . , (s(k) )2, 0, . . . , 0) (U (k)), the eigenvalues of the reduced density matrix ρ(k) are p(k) = (s(k) )2 (k)2 = (s(k) )2 j=1(s(k) (cid:80)r , = 1, . . . , r, ) r+1 = = p(k) p(k) d(k) = 0. (33) (34) These eigenvalues {p(k) } form the entanglement spectrum at cut k, and they directly determine the entanglement entropy. Von Neumann Entanglement Entropy. Given the eigenvalues {p(k) } of the reduced density matrix ρ(k) , the von Neumann entanglement entropy at cut is defined as S(k) := d(k) L(cid:88) i=1 p(k) log p(k) = (cid:88) i= p(k) log p(k) , (35) where we adopt the convention 0 log 0 = 0. This entropy quantifies the amount of entanglement across the bi-partition at cut k: S(k) = 0 indicates product state (no entanglement), while larger values indicate stronger entanglement. The maximum possible entropy is S(k) , which occurs when all non-zero eigenvalues are equal (maximally entangled state). In our analysis of weight projection matrices , we reshape the flattened matrix into tensor and construct an MPS representation. By computing S(k) at each cut k, we obtain an entanglement profile that reveals how entanglement varies across different bi-partitions of the weight projection matrix. This profile is central to our characterization of the entanglement structure in LoRA versus full fine-tuning. , d(k) ) min(d(k) max = log (cid:17) (cid:16) For definitions and discussion of Renyi-α entropies (including Renyi-2) and their relationship to von Neumann entropy, see Appendix VII C. B. Discussion: Spectral Gap, Stable Rank, and Entanglement Entropy spectral gap [48] indicates that one eigenvalue is separated from the rest, but entanglement entropy is small only when the largest eigenvalue is close to 1, i.e., when the total tail mass (cid:80) i2 pi is tiny. large gap does not prevent Θ(log ) entanglement: this occurs whenever constant fraction of the probability mass is spread over Θ(T ) small eigenvalues. To quantify this mass concentration more precisely, we review the definition of the stable rank [48] in the following. 19 For PSD matrix Σ 0 with eigenvalues λ1 λT 0, the stable rank is defined as sr(Σ) := Tr(Σ2) λ2 = 1 + (cid:88) i2 (cid:17)2 . (cid:16) λi λ (36) The stable rank provides quantitative measure of mass concentration: sr(Σ) 1 means the spectrum is sharply concentrated in the top eigenvalue, which strongly suggests small entanglement for ρ = Σ/TrΣ. In contrast, large stable rank indicates that significant probability mass is distributed across many eigenvalues, which can lead to substantial entanglement even in the presence of spectral gap. We formalize the connection between stable rank and entanglement entropy in Lemma VIII.2. C. Von Neumann and Renyi Entropies: Definitions and Relationships Throughout this work, we primarily use the von Neumann entropy to quantify artificial entanglement. However, for completeness, we also consider the Renyi-α family of entropies, which provides complementary perspectives on entanglement structure. This subsection reviews the definitions and discusses the relationships between these entropy measures. Definitions. Given reduced density matrix ρ with eigenvalues {pi}T i=1 satisfying (cid:80) pi = 1, the von Neumann entropy is defined as S(ρ) := (cid:88) i=1 pi log pi, while the Renyi-α entropy (for α > 0, α = 1) is defined as Sα(ρ) := 1 1 α (cid:32) (cid:88) (cid:33) pα . log i=1 In particular, the Renyi-2 entropy takes the form S2(ρ) = log (cid:88) i=1 = log Tr(ρ2). p2 (37) (38) (39) Relationships and Properties. We discuss Renyi-2 entropy in addition to von Neumann entropy because it depends only on the purity Tr(ρ2) of the reduced state, making it particularly convenient for theoretical analysis and providing lower bounds on von Neumann entropy. Several of our main theorems provide explicit bounds or scaling for Renyi-2 entropy (see the discussion below). However, the von Neumann entropy captures more fine-grained information about the entanglement spectrum, as it weights each eigenvalue by its logarithm, making it more sensitive to the distribution of spectral weight across modes. The von Neumann entropy corresponds to the limit α 1 of the Renyi-α entropy: S(ρ) = limα1 Sα(ρ). For any α > 0, the Renyi entropies satisfy the monotonicity property: Sα(ρ) Sβ(ρ) for α < β. In particular, we have S(ρ) S2(ρ) for all density matrices ρ. Additionally, in our analysis of artificial entanglement in LLMs, we primarily use the von Neumann entropy because it is the standard measure of entanglement in quantum information theory, providing direct connection to the quantum information perspective we adopt, it captures the full entanglement spectrum, making it more sensitive to subtle differences in correlation structures (e.g., the entanglement valley), and it satisfies the subadditivity property, which is crucial for understanding how entanglement scales with system size (volume law vs area law). Renyi-2 Entropy Results from Main Text Theorems. For completeness, we collect here the Renyi-2 entropy results that accompany the von Neumann entropy results in the main text. For the attention matrix (Theorem II.1), the Renyi-2 entropy satisfies S2(A) = 2 log(cid:0)1 + σ2(cid:1) + o(1), where σ is the bulk spectrum spread parameter, providing simpler characterization than the full von Neumann entropy scaling. For the stable rank bound (Lemma II.2), we have S2(X) 2 log(1 + δ1) where δ1 (cid:112)(T 1)η and η = rstable 1. For the output entanglement collapse (Theorem II.3), the Renyi-2 entropy satisfies S2(X) = O(1/T ) 0, vanishing faster than the von Neumann entropy S(X) = O((log )/T ) 0. See details in Appendix VIII. VIII. THEOREMS AND PROOFS A. Setup: Attention Matrix and Output We follow the assumptions from [48]. Let X0 RT have orthonormal rows, i.e., X0X 0 = IT . 20 (40)"
        },
        {
            "title": "Define",
            "content": "softmax attention := X0WQ RT dqk , := X0WK RT dqk , := X0WV RT dv , and the single layer output := A(X0) = softmax (cid:32) (cid:33) QK (cid:112)dqk RT , := AV = A(X0) X0WV RT dv . B. Modeling Assumptions (41) (42) (43) We prove the theorems under asymptotic model for softmax attention at isotropic initialization, as studied in [48]. Assumption 1 (Outlier + quartercircular bulk for attention [48]). We note the fact that is row-stochastic (rows sum to 1) due to the softmax function, and define := 1 11, where 1 RT denotes the all-ones vector. Assume the following hold as : Outlier. The top singular value satisfies s1(A) 1 and the remainder obeys Aop = O(T 1/2). (44) (45) Here, op denotes the operator norm, which equals the largest singular value of the matrix. The condition s1(A) 1 arises directly from the row-stochastic nature of the softmax attention, where the all-ones vector 1 is an eigenvector with eigenvalue 1. The scaling Aop = O(T 1/2) reflects the behavior of large random matrices: since the squared Frobenius norm of A, ij)2, remains bounded (approx. O(1), see Step 2 in Proof for Theorem VIII.1) and is spread defined as A2 roughly evenly across dimensions, the strength of the noise along any specific direction scales inversely with the square root of the dimension. := (cid:80) i,j(A 1 , . . . , x(T ) Bulk law. Let x(T ) converges weakly to the quartercircular law Qσ with parameter σ > 0. Intuitively, this means that as the dimension grows, the histogram of the singular values becomes smooth, deterministic curve shaped like quarter-circle. Moreover, the moments needed below are uniformly integrable: A,6 their empirical measure νT := 1 be the singular values of i=1 δx(T ) (cid:80)T 1 1 (cid:88) (x(T ) i=1 (cid:88) (x(T ) i=1 )2 m2, )4 m4, 1 (cid:88) i=1 (x(T ) )2 log (cid:17) (cid:16) x(T ) m2 log, (46) where for the quartercircular law one has m2 = σ2 and m4 < . 6 Note that the singular values of naturally decay as si(A) := si(A), we bring them back to the macroscopic scale O(1) that altherefore by defining the rescaled values x(T ) O(T 1/2), lows their empirical distribution to converge to non-trivial, fixed shape (the quartercircular law) rather than collapsing to Dirac delta at zero. 21 Theorem VIII.1 (Entanglement of the attention matrix: generally Θ(log )). Under Assumption 1 [48], let C. Entanglement of the Attention Matrix Then, as , the von Neumann and Renyi-2 entanglement entropies satisfy ρA := ρrow(A) = AA Tr(AA) . σ 1 + σ2 log + CA + o(1), S(A) = S2(A) = 2 log(cid:0)1 + σ2(cid:1) + o(1), where the constant term is CA = log(cid:0)1 + σ2(cid:1) + σ2 1 + σ2 log(cid:0)1 + σ2(cid:1) m2 log 1 + σ2 . (47) (48) (49) In particular, S(A) typically grows like log even when has macroscopic singular-value outlier (equivalently, macroscopic eigenvalue gap in AA). For discussion of the relationship between von Neumann and Renyi entropies, see Appendix VII C. Proof. Step 1: Orthogonality and Frobenius decomposition. Because is row-stochastic, (cid:80) (cid:80) 11 also has unit row sums) and therefore A ij = 0 (since the subtracted term 1 (cid:10)A, 1 11(cid:11) = ij 1 = 1 (cid:88) i,j (cid:88) (cid:88) A ij = 0. Thus, since 1 112 Step 2: Limit of A2 = 2 (1/T 2) = 1. . Let x(T ) F = (cid:13) (cid:13) (cid:13) 1 11(cid:13) 2 (cid:13) (cid:13) + F = 1 + A2 , be the singular values of A. Then A2 = (cid:88) i=1 si(A)2 = (cid:88) i=1 ) (x(T ) = 1 (cid:88) i=1 (x(T ) )2 m2 = σ2, where we used Assumption 1(2) [48]. Combining with (51) gives A2 1 + σ2. Aij = 1 for each i, hence (50) (51) (52) (53) Step 3: Entanglement spectrum from singular values. By the matrixstate correspondence reviewed in the preliminaries, the eigenvalues of ρA are p(T ) = si(A)2 A2 , = 1, . . . , T. (54) Assumption 1(1) states s1(A) 1 and Aop = O(T 1/2), and hence for 2 we have the rank-one approximation bound si(A) (cid:13) (cid:13)A 1 11(cid:13) (cid:13)op = Aop = O(T 1/2), so p(T ) = O(1/T ) for 2. Moreover, (53) implies p(T ) 1 = s1(A)2 A2 1 1 + σ2 . 22 (55) (56) Step 4: Renyi-2 entropy. We have Tr(ρ2 A) = (cid:88) (p(T ) i= )2 = (p(T ) 1 )2 + (cid:88) (p(T ) )2. i2 For the tail term, using p(T ) = si(A)2/A2 and the bound si(A) = O(T 1/2) for 2, (cid:18) 1 si(A)4 op = A4 1 A4 1 A4 (cid:88) i2 (cid:19) , (cid:88) (p(T ) i2 )2 = where we used Aop = O(T 1/2) from Assumption 1 and F 1 + σ2 from (53). Therefore, Tr(ρ2 A) = (p(T ) 1 )2 + O(1/T ) 1 (1 + σ2)2 , and hence which matches (48). For definitions and properties of Renyi-2 entropy, see Appendix VII C. Step 5: von Neumann entropy. Write S2(A) = log Tr(ρ2 A) 2 log(cid:0)1 + σ2(cid:1), The first term converges using (56): S(A) = p(T ) 1 log p(T ) 1 p(T ) log p(T ) . (cid:88) i2 p(T ) 1 log p(T ) 1 1 1 + σ2 log(cid:0)1 + σ2(cid:1). (57) (58) (59) (60) (61) (62) For the tail, we use the representation si(A) = x(T ) / and work directly at the level of normalized sums. Under Assumption 1 (uniform integrability of the relevant moments), replacing {si(A)}i2 by {si(A)}T i=1 in the forthcoming Riemannsum expressions incurs only an o(1) error as . This is justified by the theory of finite-rank perturbations. Since is rank-1 perturbation of A, the rank inequalityfor singular value distributions implies that their empirical cumulative distribution functions differ by at most 1/T in Kolmogorov distance (defined as G := supxR (x) G(x) for two distribution and G): (cid:13) (cid:13)FA FA (cid:13) (cid:13) rank(A A) = 1 , 7 (63) where the empirical singular value distribution (or empirical spectral distribution (ESD)) of is defined as FA(x) := 1 #{ : si(A) x} where #E denotes the cardinality of the set E, and similarly for FA . Here s1(A) sT (A) denote the singular values. Consequently, only O(1) singular values can deviate by non-negligible amount, which is negligible compared to the ambient dimension 8. Because our sums are normalized by 1/T (implicitly through the Frobenius-normbased weights9), 7 Although the classical rank inequality (see Theorem A.43 in [61]) is stated for eigenvalues of Hermitian matrices, the corresponding result for singular values of arbitrary matrices follows immediately from Hermitian dilation. Recall that the singular values of are the eigenvalues of the Hermitian (cid:19) (cid:18) 0 0 . If = + uv with rank(uv) = 1, matrix H(A) = then rank(cid:0)H(A) H(A)(cid:1) = 2. Applying the rank inequality to H(A) and H(A) yields FA FA = FH(A) FH(A) 2 2T = 1 . (63) Equivalently, rank-1 perturbation of matrix can change at most one singular value index, so the empirical singular-value distribution can shift vertically by at most 1/T . 8 Formally, the index-shift inequality for singular values proved by [62] (See Theorem 1, 3 in [62]) guarantee stability of individual singular values, while the rank inequality ensures that FA FA rank(A A)/T = 1/T . Therefore, the normalized counting measures of and are asymptotically identical. 9 The 1/T normalization arises because the Frobenius norm induces weights of the form pi = si(A)2/A2 , and bulk singular values satisfy si(A) = O(T 1/2), so that pi = O(1/T ). Consequently, any spectral sum of the form (cid:80) pi Ψ(si) is effectively an average over terms. Since rank-1 perturbation can alter only O(1) singular values, the total contribution of these mismatched indices is at most O(1) (1/T ) = O(1/T ), which vanishes as . the contribution of these O(1) mismatched indices vanishes at rate O(1/T ). The remaining sum over the bulk can thus be treated as Riemann sum that converges to the integral defined by the limiting spectral distribution of A. Concretely, (64) (65) (66) p(T ) log p(T ) = (cid:88) i2 1 A2 = 1 A2 (cid:88) i2 (cid:88) i=1 si(A)2 log (cid:17) (cid:16) si(A)2 A2 si(A)2 log (cid:17) (cid:16) si(A)2 A2 + o(1). Now substitute si(A)2 = (x(T ) )2/T : p(T ) log p(T ) = (cid:88) 1 A2 = 1 A2 (cid:88) i=1 (cid:88) i=1 )2 (x(T ) log (cid:17) (cid:16) (x(T ) )2 A2 + o(1) (x(T ) )2 (cid:16) (cid:17)2 (cid:16) x(T ) log log log A2 (cid:17) + o(1). Therefore, Using Assumption 1, 1 (cid:88) i2 p(T ) log p(T ) = log A2 1 (cid:88) (x(T ) )2 + i=1 log A2 A2 1 (cid:88) (x(T ) )2 i=1 1 A2 1 (cid:88) i=1 (x(T ) )2 log (cid:17) (cid:16) x(T ) + o(1). (cid:80) i(x(T ) )2 m2 = σ2 and 1 (cid:80) i(x(T ) )2 log (cid:17) (cid:16) x(T ) m2 log, and using A2 1 + σ2, we obtain (cid:88) i2 p(T ) log p(T ) = σ2 1 + σ2 log + σ2 1 + σ2 log(cid:0)1 + σ2(cid:1) m2 log 1 + σ2 + o(1). (67) Combining with (62) yields (48) with the stated constant term. The Attention Cardy Formula: Critical Scaling and Physical Interpretation. Here we provides the full proof of the Attention Cardy Formula stated in Theorem II.1 (see Section II C). The complete derivation shows that S(A) = Cattn log + CA + o(1), where the constant term CA is explicitly given in (48). For the physical interpretation, connection to conformal field theory, and discussion of criticality and long-range correlations, see the main text (Section II and Remark following Theorem II.1). D. General Bridge Between Stable Rank and Entanglement Bounds To have Theorem VIII.3, we firstly have Lemma VIII.2 shown below: Lemma VIII.2 (Stable rank control implies small entanglement). Let RT d, Σ := 0 with eigenvalues λ1 λT 0. Let ρ := Σ/Tr(Σ) and define the stable rank Let Then the tail mass satisfies rstable(Σ) := Tr(Σ2) λ2 = 1 + (cid:88) i2 (cid:17)2 . (cid:16) λi λ η := rstable(Σ) 1 = (cid:88) i2 (cid:17)2 . (cid:16) λi λ δ1 := (cid:88) i2 λi λ1 (cid:112)(T 1)η. (68) (69) (70) Consequently, letting δ := min{1, δ1}, 1 λmax(ρ) δ1 1 + δ1 S(M ) = S(ρ) h2(δ) + δ log(T 1), δ, S2(M ) = S2(ρ) 2 log(1 + δ1), where h2(u) := log (1 u) log(1 u). Proof. Step 1: Tail ℓ1 control from stable rank. By CauchySchwarz10, (cid:16) (cid:88) (cid:17) λi i2 (T 1) λ2 . (cid:88) Dividing by λ2 1 gives δ2 1 = (cid:16) (cid:88) i2 (cid:17) λi λ1 (T 1) (cid:17)2 (cid:88) i2 (cid:16) λi λ = (T 1)η, which proves (70). Step 2: Bound the top eigenvalue of ρ. Since Tr(Σ) = λ1 + (cid:80) i2 λi = λ1(1 + δ1), Therefore, λmax(ρ) = λ1 Tr(Σ) = 1 1 + δ1 . 1 λmax(ρ) = 1 1 1 + δ = δ1 1 + δ1 min{1, δ1} = δ, (71) (72) (73) (74) (75) establishing the first inequality in (71). Step 3: Von Neumann entropy upper bound. Let p1 p2 pT be eigenvalues of ρ. From Step 2, p1 1 δ i2 pi δ due to normalization condition. Among all distributions on points with tail mass at most δ, the Von and thus (cid:80) Neumann entropy is maximized by q1 = 1 δ and q2 = = qT = δ/(T 1). Hence S(ρ) H(q), and H(q) = (1 δ) log(1 δ) (T 1) = h2(δ) + δ log(T 1), δ 1 log (cid:16) δ (cid:17) which gives the second inequality in (71). Step 4: Renyi-2 bound. Since Tr(ρ2) = (cid:80) p2 p2 1 and p1 = λmax(ρ) = 1/(1 + δ1), which is the third inequality in (71). For definitions and properties of Renyi-2 entropy, see Appendix VII C. S2(ρ) = log Tr(ρ2) log p2 1 = 2 log(1 + δ1), (76) (77) E. Output Entanglement Collapse from Stable Rank Collapse Theorem VIII.3 (Output entanglement collapse (tokenfeature), conditional). Let = AV RT dv with = X0WV , and define Σ := XX . (78) 10 Applying the Cauchy-Schwarz inequality to the vector of tail eigenvalue ratios = (λ2/λ1, . . . , λT /λ1) and the all-ones vector 1 RT 1 25 yields ((cid:80) 12 2 = 1, we have δ 1 η(T 1). i2 λi/λ1)2 = u, 12 u2 212 2. Since u2 2 = η and Assume that with overwhelming probability as established in rank-collapsed regimes in [48]. Then, with overwhelming probability, rstable(Σ) 1 = O(T 3), (cid:19) 0, S(X) = S2(X) = (cid:18) log (cid:19) (cid:18) 1 0. (79) (80) For discussion of Renyi-2 entropy and its relationship to von Neumann entropy, see Appendix VII C. Proof. Apply Lemma VIII.2 with η = rstable(Σ) 1. Under (79) we have δ1 (cid:112)(T 1)η = O(T 1) and hence δ = min{1, δ1} = O(T 1). Lemma VIII.2 yields S(X) h2(δ) + δ log(T 1). (81) Using h2(δ) = O(δ log(1/δ)) = O((log )/T ) for δ = O(1/T ) gives S(X) = O((log )/T ) 0. Similarly, by Lemma VIII.2, S2(X) 2 log(1 + δ1) = O(δ1) = O(1/T ) 0. (82) For definitions and properties of Renyi-2 entropy, see Appendix VII C. Remark. Theorem VIII.3 is conditional. Under isotropic random initialization of WV and X0X 0 = IT , one often has IT and hence Σ AA [48], so the output entanglement scaling can mirror that of in Theorem VIII.1 rather than vanishing. The stable rank collapse condition rstable(Σ) 1 1 should therefore be interpreted as property that can emerge in trained or otherwise constrained regime and can be validated empirically. FIG. 21. Comparison of masked and unmasked attention entanglement entropy across multiple heads. For each selected head, we plot the normalized entanglement entropy SA/ log(χ) as with respect to the bi-partition during training. Removing the causal mask leads to markedly different behaviors across heads: some heads (Head 10, 25) exhibit substantial increase in entanglement entropy when future positions become accessible, indicating strong reliance on the causal constraint, while others remain almost unchanged, revealing intrinsically lowentanglement attention patterns that are insensitive to masking. 11 Overwhelming probability means 1 for every fixed > 0, for all sufficiently large . IX. MPS ADAPTATION: METHOD AND RELATIONSHIP TO MPS MODELING This section clarifies the distinction between MPS modeling (an analysis tool) and MPS adaptation (a PEFT method), explains their relationship, and demonstrates why MPS adaptation can be parameter-efficient. MPS Modeling vs. MPS Adaptation. It is crucial to distinguish between two different uses of MPS in this work. MPS Modeling (Analysis Tool): We use MPS decomposition as an analysis framework to study the entanglement structure of parameter updates from LoRA and FFT. Specifically, we decompose the learned weight update matrices into MPS representations post-hoc to compute entanglement entropy profiles. This is purely for analysis purposes: the training process itself does not use MPS structure. MPS Adaptation (PEFT Method): We term and use MPS adaptation that directly parameterizes weight updates using MPS structure during training. Unlike LoRA which uses low-rank matrices BA, MPS adaptation uses tensor network structure as the fundamental parameterization, making the MPS structure intrinsic to the fine-tuning method rather than tool for analysis. Relationship and Connection. The connection between MPS modeling and MPS adaptation is that both utilize the same mathematical structure (MPS), but for different purposes: MPS modeling uses it to analyze existing methods, while MPS adaptation uses it to define fine-tuning method. This allows us to apply the same entanglement analysis framework to MPS adaptation, enabling direct comparison of entanglement structures across different fine-tuning paradigms. Parameter Efficiency of MPS Adaptation. To illustrate why MPS adaptation can be parameter-efficient, consider concrete example. For weight update matrix Rdoutdin with dout = 4096 and din = 4096, full fine-tuning requires 4096 4096 = 16,777,216 parameters. LoRA with rank = 256 parameterizes = α BA where R4096256 and R2564096, requiring 4096 256 + 256 4096 = 2,097,152 parameters. MPS adaptation factorizes the input dimension as din = d1 d2 (e.g., 4096 = 64 64) and parameterizes Rrdin as an MPS with bond dimension χ, denoted as AMPS: AMPS = χ (cid:88) α=1 A[1] r,α,d1 A[2] α,1,d , (83) where A[1] Rrχd1 and A[2] Rχ1d2 are Order-3 tensors. The weight update is then = α BAMPS where Rdoutr is the same matrix as in LoRA. With = 256 and χ = 32, this requires 2563264+32164 = 524,288+2,048 = 526,336 parameters for AMPS, plus 4096 256 = 1,048,576 parameters for B, totaling approximately 1,574,912 parameters significantly fewer than LoRA while maintaining similar expressivity and test performance (see FIG. 19) through the tensor network structure. The parameter efficiency arises from the hierarchical factorization: instead of storing full din matrix, we factorize it into smaller tensors whose total parameter count scales as O(rχd1 + χd2) rather than O(rdin), where typically χ and d1, d2 din. X. MORE EXPERIMENTAL RESULTS A. Ablation Study on Causal Masking in Autoregressive Attention In autoregressive language models, the causal mask restricts each token to attend only to previous tokens, which may influence the entanglement structure we observe in attention matrices. To understand how the masking operation affects our entanglement analysis, we investigate whether the low entanglement entropy in attention matrices is intrinsic to the learned attention patterns or is an artifact of the causal constraint. To illustrate the contribution of the causal mask to the models attention structure, we compute unmasked attention matrices during evaluation. We attach forward hooks (callback functions that intercept intermediate activations during the forward pass) to each attention layer to capture the pre-RoPE query and key projections, Qraw = XWQ, Kraw = XWK, (84) together with the corresponding position ids. The projections are reshaped into [B, H, T, d] format with grouped-query attention handled accordingly, after which we reconstruct the rotary position embedding by computing the appropriate cosine and sine rotations, and applying them to each head. With the RoPE-rotated queries and keys, we recompute attention logits = QK , 27 (85) using the same numerical conventions as the model but without applying the causal mask that normally eliminates future positions. softmax over these logits yields the unmasked attention matrix Aunmasked = softmax(S), (86) As shown in FIG. 21, removing the causal mask reveals clear head-dependent entanglement structure. For some heads (e.g., Head 10 and Head 25), the entanglement entropy increases substantially once future positions become accessible, indicating that the causal boundary had been suppressing higher effective Schmidt rank. In contrast, other heads show nearly identical masked and unmasked entanglement, implying that their correlation patterns are intrinsically low-entangled and generate only weak entanglement even when the accessible Hilbert space is expanded. This contrast highlights heterogeneity in the entanglement capacity of attention heads. FIG. 22. Normalized singular value spectra λ(ℓ) at different cut positions ℓ for WQ and WV matrices at the first and last training steps. Each curve corresponds to different cut position, with the singular values sorted in descending order and normalized by their ℓ2 norm. The pronounced suppression of intermediate and small singular values near the mid-cut (rowcolumn bi-partition) provides direct spectral explanation for the entanglement valley observed in the entanglement entropy profile. B. The Entanglement Valley In the main text, we characterize the internal correlation structure of the updates of weight projection matrices (specifically, the updates of query and value projection matrices WQ and WV ) by computing the entanglement entropy as function of the bi-partition (cut) position. By modeling these update matrices as MPS and sweeping the cut from left to right, one obtains an artificial entanglement profile S(ℓ). Empirically, this profile exhibits pronounced depression near the middle cut (e.g., FIG. 4), which we refer to as the entanglement valley. For this valley, we consider the LoRA update matrix = α BA Rdoutdin, Rdoutr, Rrdin, (87) at initialization, where the entries of and are i.i.d. with zero mean and finite variance, and min(dout, din). As discussed in our methods, we flatten into vector using row-major order, and factorize the total dimension into powers of two, dout = 2m, din = 2n, vec(W ) Rdoutdin , (88) (89) (up to an inessential residual factor). We then reshape vec(W ) into an order-(m + n) tensor with local dimension 2 at each site and perform left-to-right MPS sweep. The bond ℓ corresponds to bi-partition between the first ℓ binary indices and the remaining + ℓ indices. In particular, the bond ℓ = corresponds exactly to the rowcolumn bi-partition Rdout Rdin , where the first binary indices encode the row dimension (output space) and the remaining indices encode the column dimension (input space), effectively separating the matrix into its row and column subsystems. Lemma X.1 (Entanglement bound at the rowcolumn cut). At the rowcolumn cut ℓ = m, the Schmidt rank of vec(W ) is at most r. Consequently, the von Neumann and Renyi entanglement entropies satisfy Sℓ=m(W ) log r, Sα,ℓ=m(W ) log r, α > 0. For discussion of Renyi entropies and their relationship to von Neumann entropy, see Appendix VII C. Proof. Write the rank-r decomposition explicitly: = α (cid:88) a= baa , (90) (91) where ba Rdout is the a-th column of and Rdin is the a-th row of A. Using the identity vec(uv) = v, we obtain vec(W ) = α (cid:88) a=1 ba aa Rdout Rdin . (92) Thus vec(W ) lies in the span of at most product states. Hence its Schmidt rank across the rowcolumn bi-partition is at most r. pure state with Schmidt rank at most has at most nonzero Schmidt coefficients. The entanglement entropy is therefore maximized by the uniform distribution 1/r, yielding the bound log for von Neumann entropy and, more generally, for all Renyi entropies. Proposition X.2 (Entanglement valley of at initialization). Assume dout = 2m and din = 2n with m, 1. Let Sℓ(W ) denote the entanglement entropy at bond ℓ in the MPS sweep defined above. Then: Hard bottleneck. At the rowcolumn cut ℓ = m, Sℓ=m(W ) log r. (93) Typical growth away from the bottleneck. For cuts ℓ that are well inside the row region or the column region, the entanglement entropy typically follows volume-law scaling. More precisely, for log ℓ (row interior), where ℓ is large enough that the local Hilbert-space dimension 2ℓ is not the limiting factor, yet sufficiently far from so that the rowcolumn low-rank structure is not fully exposed, and for ℓ + log (column interior), one expects with high probability over the random initialization of and that Sℓ(W ) min(ℓ, + ℓ) log 2 O(1), (94) and in particular Sℓ(W ) > Sℓ=m(W ) since Sℓ=m(W ) log r. Consequently, the entanglement profile ℓ (cid:55) Sℓ(W ) exhibits suppression near ℓ = m, forming an entanglement valley. If n, this suppression appears near the middle of the sweep. 29 Proof. The hard bottleneck statement is exactly Lemma X.1, which gives Sℓ=m(W ) log r. For the typical growth statement, we give an intuition based on typical high-dimensional concentration (the phenomenon that random high-dimensional vectors exhibit predictable, concentrated statistical properties, as exemplified by Pages theorem [55] for random quantum states). Consider first cuts ℓ < m, which split only the row bits. For each [r], the column vector ba R2m has i.i.d. entries with zero mean and finite variance, hence after normalization it is an isotropic random vector. When reshaped into an order-m tensor with local dimension 2, such random vector is well-approximated in typical entanglement statistics by Haar-random pure state on qubits. By Page-type concentration [55], for any internal cut 1 ℓ < m, one typically has Sℓ(ba) = min(ℓ, ℓ) log 2 O(1). (95) (cid:80)r a=1 baa Now consider = α . At cut ℓ < m, the bi-partition is entirely within the row subsystem, so the special row column low-rank structure is not yet fully exposed as hard bottleneck. Empirically and heuristically, in the interior regime log ℓ m, the effective Schmidt rank available across the cut is dominated by the local Hilbert-space dimension 2ℓ rather than by the global factorization rank r, and the reduced spectrum is close to that of typical high-dimensional state. This leads to near-maximal, volume-law scaling Sℓ(W ) ℓ log 2 O(1), log ℓ m, (96) and hence Sℓ(W ) > Sℓ=m(W ) since Sℓ=m(W ) log r. As ℓ approaches from below, the cut begins to probe the global rowcolumn factorization of , and the entanglement is progressively constrained, interpolating between the interior volume-law behavior and the bottleneck bound at ℓ = m. An analogous argument applies to cuts ℓ > inside the column subsystem, yielding Sℓ(W ) (m + ℓ) log 2 O(1), ℓ + log r. (97) Therefore the profile ℓ (cid:55) Sℓ(W ) is suppressed near ℓ = m, forming an entanglement valley; when n, this suppression appears near the middle of the sweep. Why the specific valley position: symmetry perspective. From symmetry perspective, the appearance of the entanglement valley at specific bi-partition can be understood as consequence of an explicit structural symmetry breaking induced by the low-rank parameterization. For generic full-rank matrix, the vectorized state vec(W ) does not distinguish one internal bi-partition from another, and the entanglement profile is approximately invariant under translations of the cut position. In contrast, the factorized form = BA singles out preferred decomposition between the row and column spaces, Rdout Rdin . The corresponding cut ℓ = is the unique bi-partition that respects this structural decomposition, while all other cuts mix the two subspaces and are therefore insensitive to the low-rank constraint. As result, the effective cut-translation symmetry is explicitly broken, and the entanglement bottleneck associated with the rank-r coupling becomes manifest only at the rowcolumn cut, giving rise to localized entanglement valley. i(λ(ℓ) )2 = 1. The entanglement entropy is S(ℓ) = (cid:80) Spectral Interpretation of the Entanglement Valley. The entanglement entropy across given cut is fully determined by the singular value spectrum of the corresponding bipartite decomposition. Let {λ(ℓ) i=1 denote the normalized singular values at cut ℓ, satisfying (cid:80) i(λ(ℓ) , therefore directly reflects how spectral weight is distributed across modes: flat spectra yield high S, whereas concentrated spectra yield low S. To probe this mechanism, we visualize the singular value spectra at different cut positions. For each cut ℓ, we perform an SVD of the induced bipartite matrix, sort the singular values in descending order, and normalize them by their ℓ2 norm. We then plot the normalized singular values λ(ℓ) against their sorted index. See FIG. 22. Comparing these spectra across cuts, especially between boundary and middle regions, reveals pronounced suppression of intermediate and small singular values near the mid-cut, providing direct empirical spectral explanation for the observed entropy dip. }rℓ )2 log λ(ℓ) (cid:17) (cid:16) Measuring Distance from Random Matrix Behavior. To better understand the statistical properties of these singular value distributions and distinguish them from random matrix behavior, we compare them against theoretical predictions from random matrix theory. Specifically, we consider the Marchenko-Pastur (MP) distribution [63], which describes the limiting eigenvalue distribution of large random matrices. For random matrix Rdmindmax with i.i.d. entries of zero mean and finite variance, where dmin dmax and = dmin/dmax, the normalized eigenvalues xi = dmin λi of the sample covariance matrix RR/dmax converge to the MP distribution as the dimensions grow. Here, λi denotes the normalized eigenvalues (or singular values squared) of the reduced density matrix, and the scaling xi = dmin λi is the standard normalization in MP theory that makes the distribution shape depend only on the aspect ratio rather than the absolute dimensions, ensuring the theoretical results are scale-invariant. The MP distribution ρ(x) represents the probability density function describing how these scaled eigenvalues xi are distributed. The MP distribution provides natural baseline for understanding how the singular value spectra of our MPSdecomposed matrices deviate from purely random behavior. FIG. 23 shows the theoretical MP density curves ρ(x) for different values of c, where the vertical axis represents the probability density and the horizontal axis represents the normalized eigenvalue = dmin λ. The figure illustrates how the distribution shape changes from highly concentrated (small c) to more spread out 30 (large c, approaching = 1). FIG. 24 compares the singular value distributions from our MPS-decomposed weight matrices across different training steps with the theoretical Marchenko-Pastur distribution. In this figure, we plot the empirical density of normalized eigenvalues λ (from the reduced density matrices at different cuts) against the theoretical MP density ρ(x), where = dmin λ. key observation is that, while the entanglement entropy in regions far from the mid-cut follows volume law and can reach the theoretical maximum (see FIG. 4), the internal structure of these matrices is not just that of fully random matrix. The empirical distributions from WQ and WV exhibit deviations from the MP baseline, particularly in the tail regions and the overall shape of the spectrum. This indicates that despite achieving high entanglement entropy, the learned weight matrices have acquired structured correlations that distinguish them from purely random configurations. We include the random matrix distribution as baseline reference to highlight these structural differences, demonstrating that the volume-law scaling observed in the entanglement profile does not imply random matrix statistics, but rather reflects learned structured patterns encoded in the weight matrices. FIG. 23. Marchenko-Pastur distribution: theoretical baseline. The figure shows the theoretical density curves for different values of the parameter = dmin/dmax, where dmin and dmax are the smaller and larger dimensions of random matrix, respectively. The MP distribution describes the limiting eigenvalue distribution of large random matrices with i.i.d. entries, providing reference for comparing the singular value spectra observed in our MPS-decomposed weight projection matrices. C. Limits of the No-Hair Property Under Extreme Scaling Coefficients In the main text, we demonstrate that the attention matrices exhibit no-hair property, where the entanglement entropy remains robust to variations in the scaling coefficient α within moderate range (see FIG. 14 and FIG. 16). However, this robustness may break down when α deviates substantially from typical values used in practice. To probe the limits of this nohair behavior, we investigate the entanglement structures of attention matrices under largely deviated scaling coefficients α {16, 512, 4096}, extending beyond the regime explored in the main text. FIG. 25 shows the normalized entanglement entropy SA/ log(χ) across training steps for several attention heads under these extreme α values. Compared to the moderate regime where α {8, 16, 32, 64} (FIG. 16), we observe that when α increases to 512, the entanglement curves remain approximately invariant, maintaining the no-hair-like property observed in the main text. However, when α scales further to 4096, pronounced differences emerge: for several attention heads, α = 4096 produces substantially higher entanglement entropy S, with sudden 31 FIG. 24. Comparison of normalized eigenvalue distributions from reduced density matrices at different cuts in MPS-decomposed weight projection matrices with the theoretical Marchenko-Pastur distribution. The figure shows empirical density distributions of normalized eigenvalues λ (from reduced density matrices at different cuts) from WQ and WV matrices, plotted against the theoretical MP density ρ(x) where = dmin λ, illustrating how the observed eigenvalue spectra deviate from random matrix behavior. increase during early training steps, while α = 16 and α = 512 remain almost flat throughout training. These results highlight that the no-hair property observed in the main text holds within certain range of α values, but breaks down under extreme scaling. Nevertheless, compared to the dramatic variations in entanglement structures observed in the token embedding space (see FIG. 9 and FIG. 8), where the embedding space exhibits substantial sensitivity to α with entanglement profiles varying significantly across different scaling coefficients, we still consider the attention behavior as exhibiting no-hair-like property. FIG. 25. Normalized entanglement entropy SA/ log(χ) for attention heads under extreme scaling coefficients α {16, 512, 4096}, probing the limits of the no-hair property. Compared to the moderate regime (FIG. 16), when α = 512 the entanglement curves remain approximately invariant, maintaining the no-hair-like property. However, when α scales further to 4096, pronounced differences emerge: for several attention heads, α = 4096 produces substantially higher entanglement entropy S, with sudden increase during early training steps, while α = 16 and α = 512 remain almost flat throughout training. These results demonstrate that the no-hair property holds within certain range of α values but breaks down under extreme scaling. 32 D. Artificial Entanglement Profiling: LLaMA-3.2-1B, OpenThoughts3 Dataset We extend our empirical analysis to the OpenThoughts3 dataset. The results verify that the entanglement structures and no-hair phenomena observed in the Tulu3 dataset (Sec. II B) remain consistent across different data distributions. Artificial entanglement profiling in Attention matrix. FIG. 26, 27, and 28 demonstrate that the attention matrices on OpenThoughts3 exhibit the same approximate area-law scaling with logarithmic correction as observed on Tulu3. The normalized entanglement entropy SA/ log(χ) remains significantly below the theoretical maximum and robust to variations in the scaling coefficient α, confirming that the no-hair property holds across datasets. Similarly, FIG. 29 shows that the output operator = XX maintains entanglement significantly below the theoretical maximum, consistent with the filtering behavior observed in the main text. FIG. 26. Entanglement entropy SA of the attention matrix (Head 30) with respect to the bi-partition position across training steps for LLaMA3.2-1B fine-tuned on OpenThoughts3 dataset. FIG. 27. Entanglement entropy SA/ log(χ) across training steps for several attention heads under less-deviated scaling coefficients α {4, 8, 16, 32} for LLaMA-3.2-1B fine-tuned on OpenThoughts3 dataset. See FIG. 14 for details. 33 FIG. 28. Normalized entanglement entropy SA/ log(χ) of six representative attention heads across training steps for LLaMA-3.2-1B finetuned on OpenThoughts3 dataset, where χ = min(dL, dR) denotes the maximal entanglement capacity allowed by the bi-partition. FIG. 29. Entanglement entropy SO of the output operator = XX across bi-partition positions and training steps for LLaMA-3.2-1B fine-tuned on OpenThoughts3 dataset. Token embedding space artificial entanglement profiling: volume-law and entanglement valley. FIG. 30 shows that MPS adaptation on OpenThoughts3 exhibits the same characteristic entanglement valley structure as observed in the main text, confirming that the volume-law behavior in embedding space is robust across datasets. FIG. 30. Artificial entanglement profiling of the MPS adaptation of for ranks {1, 2, 4, 8, 16, 32} with α = 16 across training steps for LLaMA-3.2-1B fine-tuned on OpenThoughts3 dataset. FIG. 31. Final test loss with respect to learning rate under different LoRA configurations for LLaMA-3.2-1B fine-tuned on OpenThoughts3 dataset, including comparison with full fine-tuning. FIG. 32. Final test loss of LoRA and MPS adaptation across multiple learning rates and scaling coefficients α for LLaMA-3.2-1B fine-tuned on OpenThoughts3 dataset. 35 Optimization landscape comparison. FIG. 31 and 32 demonstrate that the optimization dynamics and relative performance of LoRA and MPS adaptation on OpenThoughts3 mirror the observations on Tulu3. The optimal learning rate regions and the shift in optimal learning rates with α follow the same patterns as described in Sec. II B, confirming the consistency of these phenomena across datasets. E. Artificial Entanglement Profiling: LLaMA-3.1-8B, Tulu3 Dataset We extend our analysis to the larger LLaMA-3.1-8B model fine-tuned on the Tulu3 dataset to validate the universality of our findings across model scales. The results demonstrate that the entanglement structures and no-hair phenomena observed in the 1B model (Sec. II B) persist consistently in larger architectures. FIG. 33. Entanglement entropy SA of the attention matrix (Head 20) with respect to the bi-partition position across training steps for LLaMA3.1-8B fine-tuned on Tulu3 dataset. FIG. 34. Normalized entanglement entropy SA/ log(χ) of six representative attention heads across training steps for LLaMA-3.1-8B finetuned on Tulu3 dataset, where χ = min(dL, dR) denotes the maximal entanglement capacity allowed by the bi-partition. 36 FIG. 35. Normalized entanglement entropy SA/ log(χ) across training steps for several attention heads under different scaling coefficients α for LLaMA-3.1-8B fine-tuned on Tulu3 dataset. Artificial entanglement profiling in Attention matrix. FIG. 33, 35, and 34 show that the attention matrices in the 8B model exhibit the same approximate area-law scaling with logarithmic correction as observed in the 1B model. The normalized entanglement entropy SA/ log(χ) remains significantly below the theoretical maximum and robust to variations in the scaling coefficient α, confirming that the no-hair property holds across model scales. Similarly, FIG. 36 demonstrates that the output operator = XX maintains entanglement significantly below the theoretical maximum, consistent with the filtering behavior observed in smaller models. FIG. 36. Entanglement entropy SO of the output operator = XX across bi-partition positions and training steps for LLaMA-3.1-8B fine-tuned on Tulu3 dataset. Token embedding space artificial entanglement profiling: volume-law and entanglement valley. FIG. 37 and 39 reveal that the embedding space (WQ and WV ) in the 8B model exhibits the same volume-law behavior with characteristic entanglement valley as observed in the 1B model. The valley deepens during training in LoRA and shows sensitivity to the scaling coefficient α, consistent with the findings in Sec. II B. This confirms that the volume-law entanglement profile and its hyperparameter sensitivity are fundamental properties independent of model scale. 37 FIG. 37. Artificial entanglement profiling of WQ and WV with respect to the cut position across training steps and scaling coefficients α {16, 512} for LLaMA-3.1-8B fine-tuned on Tulu3 dataset. Optimization landscape comparison. FIG. 38 compares LoRA and MPS adaptation strategies for the 8B model. Both methods achieve comparable performance, with optimal learning rates shifting as α increases, mirroring the behavior observed in the 1B model (Sec. II B). This consistency further validates the generalizability of our theoretical framework across model scales. FIG. 38. Final test loss of LoRA and MPS adaptation across multiple learning rates and scaling coefficients α for LLaMA-3.1-8B fine-tuned on Tulu3 dataset. 38 FIG. 39. Artificial entanglement profiling of the MPS adaptation of for ranks {1, 2, 4, 8, 16, 32} with α = 16 across training steps for LLaMA-3.1-8B fine-tuned on Tulu3 dataset. F. Artificial Entanglement Profiling: LLaMA-3.1-8B, OpenThoughts3 Dataset We further validate our findings by analyzing the LLaMA-3.1-8B model fine-tuned on the OpenThoughts3 dataset. The results confirm that the entanglement structures and no-hair phenomena are robust across both model scales and dataset distributions, reinforcing the universality. Artificial entanglement profiling in Attention matrix. FIG. 40 and 41 demonstrate that the attention matrices in the 8B model on OpenThoughts3 exhibit the same area-law scaling with logarithmic correction and no-hair property as observed in both the 1B model (Sec. II B) and the 8B model on Tulu3. The normalized entanglement entropy remains significantly below the theoretical maximum and robust to variations in α, confirming that these phenomena are universal across different datasets and model scales. FIG. 40. Entanglement entropy SA of the attention matrix (Head 31) with respect to the bi-partition position across training steps for LLaMA3.1-8B fine-tuned on OpenThoughts3 dataset. FIG. 41. Normalized entanglement entropy SA/ log(χ) across training steps for several attention heads under different scaling coefficients α for LLaMA-3.1-8B fine-tuned on OpenThoughts3 dataset. Token embedding space artificial entanglement profiling: volume-law and entanglement valley. FIG. 42 reveals that the embedding space (WQ and WV ) in the 8B model on OpenThoughts3 exhibits the same volume-law behavior with characteristic entanglement valley as observed in both the 1B model (Sec. II B) and the 8B model on Tulu3. The valley deepens during training in LoRA and shows sensitivity to the scaling coefficient α, consistent with the findings in Sec. II B. This confirms that the volume-law entanglement profile and its hyperparameter sensitivity are fundamental properties independent of both model scale and dataset distribution. 40 FIG. 42. Artificial entanglement profiling of WQ and WV with respect to the cut position across training steps and scaling coefficients α {16, 4096} for LLaMA-3.1-8B fine-tuned on OpenThoughts3 dataset. Optimization landscape comparison. FIG. 43 compares LoRA and MPS adaptation strategies for the 8B model on OpenThoughts3. The consistent performance patternswhere both methods achieve comparable results and optimal learning rates shift with αmirror the observations in other model-dataset combinations, providing strong evidence for the generalizability of tensor-network-based adaptation methods and the robustness of the no-hair phenomenon across diverse experimental settings. FIG. 43. Final test loss of LoRA and MPS adaptation across multiple learning rates and scaling coefficients α for LLaMA-3.1-8B fine-tuned on OpenThoughts3 dataset."
        }
    ],
    "affiliations": [
        "Department of Computer Science, Northwestern University",
        "Department of Computer Science, University of Pittsburgh"
    ]
}