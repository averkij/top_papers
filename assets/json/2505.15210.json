{
    "paper_title": "Deliberation on Priors: Trustworthy Reasoning of Large Language Models on Knowledge Graphs",
    "authors": [
        "Jie Ma",
        "Ning Qu",
        "Zhitao Gao",
        "Rui Xing",
        "Jun Liu",
        "Hongbin Pei",
        "Jiang Xie",
        "Linyun Song",
        "Pinghui Wang",
        "Jing Tao",
        "Zhou Su"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Knowledge graph-based retrieval-augmented generation seeks to mitigate hallucinations in Large Language Models (LLMs) caused by insufficient or outdated knowledge. However, existing methods often fail to fully exploit the prior knowledge embedded in knowledge graphs (KGs), particularly their structural information and explicit or implicit constraints. The former can enhance the faithfulness of LLMs' reasoning, while the latter can improve the reliability of response generation. Motivated by these, we propose a trustworthy reasoning framework, termed Deliberation over Priors (DP), which sufficiently utilizes the priors contained in KGs. Specifically, DP adopts a progressive knowledge distillation strategy that integrates structural priors into LLMs through a combination of supervised fine-tuning and Kahneman-Tversky optimization, thereby improving the faithfulness of relation path generation. Furthermore, our framework employs a reasoning-introspection strategy, which guides LLMs to perform refined reasoning verification based on extracted constraint priors, ensuring the reliability of response generation. Extensive experiments on three benchmark datasets demonstrate that DP achieves new state-of-the-art performance, especially a Hit@1 improvement of 13% on the ComplexWebQuestions dataset, and generates highly trustworthy responses. We also conduct various analyses to verify its flexibility and practicality. The code is available at https://github.com/reml-group/Deliberation-on-Priors."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 2 ] . [ 1 0 1 2 5 1 . 5 0 5 2 : r Deliberation on Priors: Trustworthy Reasoning of Large Language Models on Knowledge Graphs Jie Ma*1, Ning Qu*1,2, Zhitao Gao1,2, Rui Xing1, Jun Liu2,3, Hongbin Pei1, Jiang Xie4, Lingyun Song5, Pinghui Wang1, Jing Tao1, Zhou Su1 1MOE KLINNS Lab, Xian Jiaotong University 2School of Computer Science and Technology, Xian Jiaotong University 3Shaanxi Province Key Laboratory of Big Data Knowledge Engineering 4School of Artificial Intelligence, Chongqing University of Post and Telecommunications 5School of Computer Science, Northwestern Polytechnical University *Equal contribution Corresponding Author jiema@xjtu.edu.cn"
        },
        {
            "title": "Abstract",
            "content": "Knowledge graph-based retrieval-augmented generation seeks to mitigate hallucinations in Large Language Models (LLMs) caused by insufficient or outdated knowledge. However, existing methods often fail to fully exploit the prior knowledge embedded in knowledge graphs (KGs), particularly their structural information and explicit or implicit constraints. The former can enhance the faithfulness of LLMs reasoning, while the latter can improve the reliability of response generation. Motivated by these, we propose trustworthy reasoning framework, termed Deliberation over Priors (DP), which sufficiently utilizes the priors contained in KGs. Specifically, DP adopts progressive knowledge distillation strategy that integrates structural priors into LLMs through combination of supervised fine-tuning and Kahneman-Tversky optimization, thereby improving the faithfulness of relation path generation. Furthermore, our framework employs reasoning-introspection strategy, which guides LLMs to perform refined reasoning verification based on extracted constraint priors, ensuring the reliability of response generation. Extensive experiments on three benchmark datasets demonstrate that DP achieves new state-of-the-art performance, especially Hit@1 improvement of 13% on the ComplexWebQuestions dataset, and generates highly trustworthy responses. We also conduct various analyses to verify its flexibility and practicality. Code is available at https://github.com/reml-group/Deliberation-on-Priors."
        },
        {
            "title": "Introduction",
            "content": "Large Language Models (LLMs) [14], distinguished by their massive parameter scale and training on vast amounts of diverse, unlabeled data, have demonstrated impressive capabilities across wide range of natural language understanding and generation tasks. They have also achieved substantial success in practical applications, such as intelligent virtual assistants and customer service systems. However, recent research [57] has revealed that LLMs are prone to hallucinations, producing plausible-sounding but incorrect or outdated responses, especially in real-world scenarios. This issue, often stemming from insufficient or obsolete knowledge, can lead to serious consequences and undermine the reliability of LLMs in high-stakes domains such as legal decision-making and medical diagnosis [8, 9]. Preprint. Under review. Figure 1: An illustration of LLM reasoning over knowledge graphs based on exploiting priors sufficiently. We collect weak supervision signals of the mapping from questions to relation paths by identifying the shortest traversal sequence from topic entities to answers. We predefine 5 constraints, such as type, multi-entity, and ordinal constraints, following [17] which employs them to develop the ComplexQuestions dataset but does not leverage the prior in reasoning. To equip LLMs with up-to-date, domain-specific knowledge, an emerging line of research [1012] has focused on knowledge graph-based retrieval-augmented generation, which aims to enhance response generation through the dynamic retrieval of relevant external knowledge. Current methods [7, 8, 13, 14] employ an end-to-end or step-by-step manner to retrieve and reason on Knowledge Graphs (KGs). The former retrieves top triples based on the semantic similarity between questions and knowledge facts, while the latter transforms complex questions into multiple subquestions by step-by-step retrieval. After obtaining sufficient knowledge, they utilize LLMs to generate responses directly. However, existing approaches [7, 8, 1316] fail to exploit the prior knowledge embedded in KGs fully, particularly (1) the structural information and (2) the explicit and implicit constraints. In particular, relation paths that connect topic entities in questions to their corresponding answers, such as the path \"Ang Lee directed . . . E\" in Figure 1, can be leveraged not only to enhance the structural pattern awareness within KGs but also to support response generation. This, in turn, can significantly improve the faithfulness of LLM reasoning over KGs. Additionally, constraints, such as the multi-entity and type constraints, can serve dual purpose: they can be used to filter candidate relation paths (select path 2 in Figure 1 considering the mentioned two types of constraints) and also to guide LLMs in backtracking during inference, thereby improving the reliability and robustness of reasoning processes. Motivated by these, we propose trustworthy reasoning framework over KGs named DP (Deliberation on Priors). The framework comprises four key modules: Distillation, Planning, Instantiation, and Introspection, which guide LLMs to generate faithful and reliable responses through two-stage process: offline and online. In the offline stage, DP first collects weak supervision signals in the form of mappings from questions to corresponding sets of relation paths. The paths are constructed by identifying the shortest traversal sequences from topic entities to answer entities in KGs. This prior structural knowledge is then distilled into the LLM through combination of fine-tuning and Kahneman-Tversky optimization [18]. The former is supervised using the weak supervision signal, while the latter is optimized by maximizing the expected utility of generation under the human utility model that Kahneman and Tversky proposed to describe how humans make decisions about uncertain monetary outcomes. This process enhances the ability of LLMs to plan and reason over the KG structure effectively. During the online stage, the trained LLM is used to Plan for reasoning and generate faithful candidate relation paths. Subsequently, DP utilizes LLMs to perform relation path selections by evaluating the semantic relevance between the candidate path and the input question. Next, to obtain complete reasoning path, the framework retrieves topic entities and relations from KGs and instantiates the selected relation path in the Instantiation module, such as the transformation from \"{directed, won_award}\" 2 Table 1: Constraint definitions and samples. Category Definition & Sample Type MultiEntity Explicit Time Implicit Time Ordinal The question specifies required type or category for the answer. e.g. What city did Esther live in? The question demands the answer to satisfy the condition for multiple entities. e.g. Which team owned by Malcolm Glazer has Tim Howard playing for it? The question clearly defines specific time period or date to be referenced. e.g. Who was the governor of Arizona in 2009 that held his governmental position before 1998 ? The question implies temporal frame that the answer should consider. e.g. Who was the Secretary of State when Andrew Jackson was the president ? The question contains the sorting rule and requires the answer in specific order. e.g. Lou Seal is the mascot for the team that last won the World Series when? Figure 2: Constraint distribution on three datasets. Crouching Tiger won_award to \"Ang Lee directed Compound Value Type 2\". After obtaining the reasoning path, current frameworks [7, 8, 1316] utilize LLMs to generate responses based on them without deliberation. In contrast, to enhance reasoning reliability, DP employs the Introspection module to verify whether the selected reasoning path satisfies the constraints extracted from questions, where the constraints, such as multi-entity and ordinal constraints, are predefined following [17]. Feedback is then provided to the LLM to guide subsequent decisions. If the reasoning path passes the verification, the LLM is instructed to generate response. Otherwise, the information about which constraints are violated is fed back to the LLM to trigger relation path backtracking. To evaluate the effectiveness of DP, we conduct extensive experiments on three benchmark Knowledge Graph Question Answering (KGQA) datasets. The experimental results on WebQuestionSP [19], ComplexWebQuestions [20], and MetaQA [21] show that, compared with baselines, our method achieves state-of-the-art performance while minimizing interaction frequency with LLMs and generates more faithful and reliable responses. The main contributions of this work are summarized as follows: We introduce trustworthy reasoning framework DP that empowers large language models to generate faithful and reliable responses through deliberate reasoning over the priors embedded in knowledge graphs. We propose progressive knowledge distillation strategy, enabling LLMs to generate faithful relation paths by exploiting prior structural information. We propose reasoning-introspection strategy that enhances the reliability of large language model generation by incorporating predefined constraint priors as guidance. We conduct extensive experiments on three public datasets to verify the effectiveness and superiority of DP. Furthermore, we demonstrate the flexibility of DP in integrating with various large language models, as well as its practicality in scenarios requiring fewer interactions with them."
        },
        {
            "title": "2 Preliminary",
            "content": "KGQA Task Formulation. Given question q, knowledge graph G, and topic entity es mentioned in q, KGQA requires the intelligent system, such as LLMs, to generate responses based on set of knowledge triples (facts) retrieved from G. It should be noted that question may contain multiple topic entities. Relation Path Definition. relation path is formally defined as an ordered sequence of relations = {r1, r2, . . . , rl}, where each ri denotes the i-th relation in the path and represents the path length. Here, denotes the set of all possible relations in G. Path Instantiation. Given relation path , instantiation path is called reasoning path, denoted r3 rl el, where each ei denotes the i-th entity in the path and ri by = e0 corresponds to the i-th relation in the relation path . Here, represents the set of all entities in the KG. It should be noted that relation path may have multiple instantiations in G. r1 e1 r2 e2 In Part 1, DP employs Figure 3: Trustworthy reasoning framework DP of LLMs over KGs. progressive knowledge distillation strategy to enhance the structural pattern awareness of KGs for LLMs. In Part 2, the reasoning path is produced by relation path generation and instantiations. In Part 3, DP utilizes reasoning-introspection strategy to verify whether the reasoning satisfies the extracted constraint. Constraint Predefining. The constraints listed in Table 1 were initially introduced by [17] to facilitate the development of complex questions. In this paper, we leverage these constraint priors for reasoning verification. Specifically, we utilize multiple constraints like \"type\" and \"multi-entity\" extracted from questions in Figure 1 to assess whether selected reasoning path satisfies the required conditions. To this end, we define constraint base and conduct statistical analysis across three datasets, sampling 200 examples from each. Figure 2 demonstrates that most datasets encompass range of constraint categories, with the \"type\" constraint being the most frequently occurring in each."
        },
        {
            "title": "3 Methodology",
            "content": "Our proposed framework, DP, as illustrated in Figure 3, is composed of four key components. Distillation: This module employs progressive knowledge distillation strategy to guide LLMs in capturing the structural patterns of KGs from set of demonstrations. Planning and Instantiation: In this stage, DP first generates diverse set of candidate relation paths, which are subsequently grounded into KG triples to obtain instantiated paths. Introspection: This component performs deliberative selection and verification of reasoning paths by evaluating whether they meet the constraints derived from the input question. Once satisfactory instantiated path is identified, the LLM is employed to generate responses accordingly."
        },
        {
            "title": "3.1 Distillation",
            "content": "DP employs progressive knowledge-distillation strategy to guide LLMs in exploiting the structural information and enabling faithful reasoning over KGs. This is achieved by combination of Supervised Fine-Tuning (SFT) and Kahneman-Tversky Optimization (KTO) [18]. The former fine-tunes the LLM using collected weak supervision signals, while the latter further refines it based on automatically derived preference data. Specifically, DP leverages weak supervision signals in the form of question-to-path mappings, which are derived from the annotations within training splits. Given question and topic entity es identified in q, we define function : 2P that maps to set of plausible relation paths P, where each is sequence of relations such that traversing from es via (r1, . . . , rl) leads 4 to the answer entity et in the KG = (E, R). To extract these paths, our framework first extracts k-hop subgraph Gk(es) centered on es, where is the maximum reasoning depth allowed by the dataset. Then, the Dijkstra algorithm is employed to identify all shortest relation paths from es to the ground-truth answer entity et within Gk(es). These shortest paths constitute the weak supervision set Pw(q) for question q, serving as structurally mapping relation exemplars. While not exhaustive, Pw(q) captures plausible multi-hop reasoning trajectories within the KG structure and provides guidance for the LLM to generalize to similar questions. The weak supervision of question-to-path mappings Pw(q) can be defined as follows: Pw(q) = M(q, es, G, k) = ShortestPathDijkstra(Gk(es), es, et). (1) We then apply SFT to train the LLM to generate relation paths conditioned on the input question and its corresponding topic entity es. This training stage encourages LLMs to align the semantic content of the question with structural relation traversals over G. Let = (r ) denote gold relation path extracted using Equation (1) for question q. The objective of SFT is to maximize the conditional log-likelihood LSFT of the target path sequence given the input (q, es): (cid:88) 2, . . . , 1, LSFT(θs) = log Pθs (r <t, q, es) , (2) t=1 where θs represents the parameters of the LLM and t1). This formulation enables the LLM to learn to autoregressively generate faithful relation paths that connect es to plausible answer entities within the KG. <t denotes the prefix subsequence (r 1, . . . , To further enhance the reliability of relation path generation, we incorporate preference optimization stage to explicitly encourage LLMs to prefer semantically coherent and structurally faithful relation paths. Specifically, we construct relation path data consisting of positive paths and negative paths given question q. The negative or undesirable path is synthetically generated from the original weak supervision data Pw(q) through targeted perturbations for the positive path: Path Truncation. Removing the final hop T from gold relation path to yield an incomplete relation chain. Entity-Path Swapping. Swapping relation paths between different topic entities associated with the same question, resulting in semantically inconsistent paths. Relation Deletion. Randomly deleting the relation path of certain topic entity, resulting in incomplete paths. These synthetic negative paths are constructed to superficially valid relation paths while being semantically invalid due to violations of critical structural or semantic constraints. Notably, such perturbations introduce severe class imbalance in path data, where positive paths constitute only 4 of the dataset, and negative paths account for 3 1 4 . This imbalance poses significant challenges to conventional preference optimization like direct preference optimization. Therefore, we train the LLM using KTO [18], more robust approach that accommodates imbalanced supervision. KTO maximizes the expected utility of generation under the human utility model that Kahneman and Tversky proposed to describe how humans make decisions about uncertain monetary outcomes. This utility maximization objective is equivalent to minimizing the KTO loss LKTO: LKTO(πθ, πref) = E(x,y)D [λy v(x, y)] . Here, we define the key components of the KTO loss formulation. Let = (q, es) denote the input, and let represent its corresponding relation path label, where Yp and Yn indicate positive and negative label, respectively. πθ() is the current LLMs output distribution and πref() is the reference model supervised by SFT. The parameter λy takes the value λp (for positive y) or λn (for negative y), controlling class-specific weighting. The value function v() models human utility perception, defined as: (3) v(x, y) = (cid:26)λpσ (β(rθ(x, y) z0)) λnσ (β(z0 rθ(x, y))) where σ() denotes the logistic function, β modulates risk aversion, rθ(x, y) = log πθ(yx) πref(yx) represents the implied reward and the reference point z0 is obtained via the KL divergence z0 = KL (πθ(y x) πref(y x)). In brief, the progressive knowledge distillation strategy equips DP with reliable structural priors of KGs, enabling faithful online reasoning in the subsequent stages. if Yp x, if Yn x. (4)"
        },
        {
            "title": "3.2 Planning and Instantiation",
            "content": "In this stage, DP utilizes the path generator, which has been trained through the aforementioned progressive knowledge-distillation strategy, to generate multiple candidate relation paths. One of these paths is then selected and instantiated by retrieving entities from the KG, beginning with the given topic entity es. Importantly, the selection of the relation path at this stage is guided solely by its semantic alignment with the input question, without imposing any constraints. More formally, given question and its associated topic entity es, DP invokes the path generator to produce set of candidate relation paths {P1, P2, . . . }, where each Pi denotes traversal sequence of relations representing multi-hop reasoning trajectory over the KG. In scenarios where question contains multiple topic entities, the path generator independently produces candidate paths for each entity. These paths are subsequently merged into unified pool, thereby enriching semantic diversity and increasing the probability that at least one reasoning trajectory satisfies the implicit or explicit constraints embedded in the question. For given relation path , the corresponding instantiated reasoning path is obtained by traversing the KG starting from the topic entity es. The instantiation process is described in detail in Section 2. 3."
        },
        {
            "title": "Introspection",
            "content": "To ensure the reliability of LLM reasoning, our framework utilizes reasoning-introspection strategy to verify whether the reasoning path satisfies the extracted constraint from questions. As introduced in Section 2, we predefine 5 constraint types, which are prior knowledge embedded in KGs and can be employed to guide LLMs. Specifically, given question q, DP first prompts LLMs to extract the contained constraint C(q) from the predefined constraint base C: C(q) = Fcons(q, C, Icons), where Icons represents the prompt and in-context exemplars. Subsequently, the LLM is prompted to determine whether the instantiated reasoning path satisfies C(q) given and es. The verification outcome is formalized as: (q, es, P) = (cid:26)1, if = C(q); 0, otherwise. (5) If the constraint is satisfied, DP instructs the LLM to generate reason and produce final response grounded on the validated reasoning path. Conversely, if the constraint is violated, the LLM is prompted to provide explicit feedback identifying the unsatisfied condition. This feedback subsequently triggers backtracking mechanism, wherein the framework iteratively re-executes the process of relation path selection, instantiation, and introspection. This mechanism can reduce the negative impact of false-positive reasoning paths, promoting the reliability of response generation. The iterative loop continues until either constraint-satisfying reasoning path is found or the candidate relation path set is reduced to singleton."
        },
        {
            "title": "4 Experiment",
            "content": "We conduct various experiments on three benchmarks to verify the following aspects: (1) the superiority of DP; (2) the flexibility in integrating different LLMs; (3) the effectiveness of individual components; (4) the necessity of deliberation on prior knowledge; and (5) the practicality of DP."
        },
        {
            "title": "4.1 Experimental Setting",
            "content": "Dataset and Evaluation Metric. We evaluate our approach on three public multi-hop KGQA datasets: WebQuestionSP (WebQSP) [19] and ComplexWebQuesions (CWQ) [20], and MetaQA [21]. WebQSP is developed by gathering semantic parses in SPARQL from the WebQuestions dataset. CWQ features large collection of compositional questions, requiring reasoning over up to 4-hop relation paths based on multiple web snippets. MetaQA is built upon movie ontology derived from the WikiMovies dataset and provides question-answer pairs spanning 1-hop, 2-hop, and 3-hop queries. The KGs used in WebQSP and CWQ are subset of Freebase. To account for computational constraints, we uniformly sample 500 questions from the test set of WebQSP and CWQ, following the setup in [22, 15]. total of 600 instances are uniformly sampled from the MetaQA dataset, with exactly 200 samples selected for each of the 1-hop, 2-hop, and 3-hop types. Evaluation is conducted using three standard metrics: Hit, Hits@1, and F1 score, consistent with prior studies [8, 23, 24]. The 6 Table 2: Comparison of KGQA performance (%) across three datasets with previous state-of-theart methods. DP results are averaged over three independent evaluations. We highlight the best performance in bold and the second-best in underline. Baseline methods are categorized into three groups: Supervised Learning (SL), In-Context Learning (ICL), and Hybrid Learning (HL). LM denotes the language model used. denotes the performance on 3-hop questions. Type Method Year LM WebQSP CWQ MetaQA H@ F1 H@1 F1 H@ F1 SL ICL HL EmbedKGQA TransferNet UniKGQA RoG AMAR GNN-RAG RoBERTa 2020 BERT 2021 2023 RoBERTa 2024 LLaMA2-Chat-7B 85.7 2025 84.2 LLaMA2-7B/13B 2025 LLaMA2-Chat-7B 90. - - - ToG PoG Readi DoG 2024 2024 2025 ChatGPT GPT-4 ChatGPT GPT-4 ChatGPT GPT-4 ChatGPT GPT-4 Interactive-KBQA 2024 2025 LightPROF GPT-4 LLaMA3-8B DP (Ours) 2025 LLaMA3.1-8B ChatGPT GPT-4 GPT-4o GPT-4.1 76.2 82.6 82.0 87.3 74.3 78.7 88.6 91.0 - - 87.9 89.7 90.4 90.7 90.6 66.6 71.4 77.2 80.8 - 82.8 - - - - - - 62.6 65.4 - 83.8 82.8 86.9 86.7 87.5 86.7 - - 72.2 70.8 81.2 73. - 36.4 - - - - 54.2 55.6 71.2 - 75.7 79.2 81.7 81.4 80.1 - - - 62.6 83.1 68.7 58.9 69.5 63.2 75.0 55.6 67.0 58.2 56.0 - - 70.8 80.0 85.6 85.2 87.2 - 48.6 51.2 57.8 - 62.8 - - - - - - 49.4 41.0 - 59.3 61.1 72.6 74.6 74.6 75.8 - - 49.0 56.2 78.5 60. - 31.8 - - - - 56.6 46.4 49.1 - 58.5 69.2 71.1 70.5 69.4 - - - - - - - - - - - - 95.4 98.3 - - 90.2 96.7 96.7 96.5 96.8 97.0 99.2 99.4 89.0 - 98.6 - - - 50.7 - - - - - - - - 85.1 90.1 - - 87.4 95.4 95.4 95.2 95. - - - - - - 87.2 93.1 96.3 - 84.1 90.8 94.8 94.4 94.9 Hit (H) metric assesses whether any of the ground-truth answers are present in the generated response. Hits@1 (H@1) measures the proportion of questions for which the top-ranked predicted answer exactly matches correct answer. The F1 score accounts for scenarios with multiple correct answers by computing the harmonic mean of precision and recall, thereby providing more comprehensive evaluation of answer quality. Implementation Detail. During the Distillation stage, the path generator (LLaMA3.1-8B-Instruct) is fine-tuned by the weak supervision signal in the form of question-to-path mapping for 2 epochs and further optimized for 1 epoch through preference optimization with KTO [18]. We apply low-rank adaptation [25] to adapt large-scale parameters during both SFT and KTO training efficiently. In the Planning stage, we employ the trained path generator in zero-shot manner to produce set of relation paths for each topic entity in the question. In the Introspection stage, DP guides LLMs to perform path selection, constraint extraction, and verification under few-shot setting. Specifically, we provide one exemplar for each possible scenario within these three procedures, which consist of 3, 5, and 2 scenarios, respectively. The detailed settings are provided in Appendix B.1 and B.2. Baseline Selection. Inspired by the setup in [15, 8], we compare DP against previous state-of-the-art approaches, including Supervised Learning (SL), In-Context Learning (ICL), and Hybrid Learning (HL). SL-based methods train models using the answer labels provided in KGQA datasets to directly predict answers, including EmbedKGQA [26], TransferNet [27], UniKGQA [28], RoG [8], AMAR [10], and GNN-RAG [24]. ICL-based methods employ chain-of-thought reasoning to generate responses based on few-shot exemplars, including PoG [29], ToG [22], Readi [30], and DoG [15]. HL-based methods combine SL and ICL to produce responses, including Interactive-KBQA [31] and LightPROF [32]."
        },
        {
            "title": "4.2 Reasoning on Different KGs",
            "content": "Main Result. We compare DP with previous state-of-the-art methods, categorized into three types: SL, ICL, and HL. Based on the results presented in Table 2, we draw the following key insights. First, incorporating deliberation over prior knowledge significantly enhances the reliability of response generation. The results for DP are averaged over three independent runs, with standard deviations reported in Appendix B.3. Our framework consistently achieves new state-of-the-art performance across most datasets, with low variance. Notably, DP surpasses the HL-based method LightPROF by 7 14.5% in H@1 on the CWQ dataset. Second, existing methods struggle in scenarios where the correct answer must appear in the top-ranked position or where the output is expected to contain accurate answers with high precision. For instance, ToG exhibits 46.2% gap between its and F1 scores on the WebQSP dataset. In contrast, DP narrows this gap to approximately 10%, further underscoring its robustness and reliability. Third, we observe that some prior works have misinterpreted evaluation metrics such as and H@1. According to the code released by [10, 22], certain results reported as H@1 are actually H, potentially leading to unfair comparisons and misleading evaluations. In this work, we rigorously report DPs performance using H, H@1, and F1 metrics, respectively. Overall, the DP framework demonstrates robust and reliable performance across diverse datasets, validating the effectiveness of incorporating prior knowledge deliberation. Flexibility Verification. Table 2 also reports the integration experiments, which are designed to evaluate whether DP can enhance the reasoning reliability of various LLMs, including LLaMA3.18B, GPT-3.5, GPT-4.0, GPT-4o, and GPT-4.1. The results show that DP consistently improves the reasoning performance of these LLMs across three benchmark datasets. Notably, our framework enables GPT-4o and GPT-4.1 to achieve the best performance on the WebQSP and CWQ datasets, respectively. Overall, these findings highlight the flexibility and effectiveness of DP in enhancing diverse LLMs."
        },
        {
            "title": "4.3 Ablation Study",
            "content": "H F1 H@1 CWQ Setting WebQSP DP (GPT 4.1) GPT 4.1 Table 3: Ablation experiments. PT, EPS, and RD denote path truncation, entity-path swapping, and relation deletion, respectively, which are perturbations introduced in Section 3.1. \"w/o CPD\" indicates the constraint is automatically induced by LLMs rather than being manually predefined. We perform comprehensive ablation studies on two benchmark datasets (WebQSP and CWQ) to systematically evaluate the contribution of each DP component. As shown in Table 3, our analysis reveals five critical observations. First, LLMs such as GPT-4.1 demonstrate significant performance degradation when deprived of knowledge augmentation. Specifically, the F1 score drops by 25.5% on WebQSP and 20.5% on CWQ, highlighting the necessity of external knowledge integration for complex question answering tasks. Second, our progressive knowledge distillation strategy effectively promotes the prior awareness of LLMs for the structural patterns of KGs. When removing KTO, we observe 2.0% decrease in H@1 score on WebQSP and 1.2% decrease on CWQ, confirming its role in improving pattern recognition capabilities. Third, among the three perturbation methods examined, relation deletion demonstrates the most substantial impact on path generation. Fourth, the reasoning-introspection strategy proves to be the most critical component for ensuring response reliability. Compared to other ablated conditions, removing introspection leads to the most pronounced performance deterioration across both datasets. Finally, both constraint predefinition and verification feedback contribute significantly to the overall reasoning process. Notably, substituting manual constraint definitions with LLM-generated summaries results in 2.8% drop in score on WebQSP and 1.0% decrease on CWQ, underscoring the value of human-defined constraints in guiding accurate reasoning. These findings collectively demonstrate the complementary roles of different DP components in enhancing the faithfulness and reliability of LLM reasoning. w/o KTO w/o PT w/o EPS w/o RD w/o Introspection w/o CPD w/o feedback 86.0 86.4 86.6 87.2 85.3 86.2 85. 74.6 75.0 73.8 74.1 70.8 74.4 73.2 88.3 88.4 90.0 88.3 86.6 87.8 88.0 84.7 84.8 85.8 84.8 82.0 83.4 83.0 77.3 77.6 79.4 77.8 75.7 76.4 76.5 67.3 68.4 68.1 67.7 65.2 68.5 67.1 86.7 71. 87.2 56.0 90.6 74.0 80.1 54.6 75.8 53.0 69.4 48.9 H@ F1 4."
        },
        {
            "title": "Impact of Priors on LLM Reasoning",
            "content": "To investigate the impact of prior knowledge on LLM reasoning, we conduct extensive experiments on two benchmark datasets: WebQSP and CWQ. First, Table 4 presents the results of path generation and constraint extraction on 500 and 100 uniformly sampled examples from each dataset, respectively. To improve the faithfulness of LLM reasoning, we collect all relation paths that satisfy the condition defined in Equation (1) for each question. This leads to one-to-many question-to-relation mapping, in contrast to the one-to-one mapping adopted by RoG [8]. Our approach yields significant improvements in the F1 score of path generation, with relative gains of 29.3% on WebQSP and 43.0% on CWQ, demonstrating the benefits of leveraging priors. Second, we analyze the failure cases of 8 Table 4: Results of Path Generation (PG) and Constraint Extraction (CE)."
        },
        {
            "title": "Setting",
            "content": "PG (1: 1) PG (1: n) CE"
        },
        {
            "title": "CWQ",
            "content": "H 83.0 93.0 99.0 F1 59.3 76.7 90.2 81.2 94.0 99. F1 49.8 71.1 92.9 Figure 4: Error distribution on two datasets. DP with GPT-4.1 in the main experiment. As shown in Figure 4, the majority of errors stem from path generation, path selection, reasoning, and instruction-following. Among these, path generation and selection are closely tied to the utilization of priors, underscoring the importance of effectively incorporating prior knowledge. Reasoning errors, despite the correct path selection, indicate that LLMs may rely excessively on their internal knowledge during response generation. Moreover, we observe that LLMs occasionally fail to adhere to the required answer formatfor instance, producing \"2008\" instead of \"2008 World Cup.\" We found that incorporating instruction-following exemplars mitigates this issue and yields performance improvements, such as 1.1% Hit@1 gain on CWQ."
        },
        {
            "title": "4.5 Analysis for LLM Interaction",
            "content": "Table 5: Practicality and efficiency Comparison. Call denotes the number of LLM interactions. Input, Output, and Total represent the number of corresponding tokens. The results of DP are averaged over three independent runs. We conduct experiments on CWQ and WebQSP to evaluate the practicality and efficiency of DP. Table 5 presents comparative analysis of the average number of LLM calls and token consumption required by different methods to answer question across the two datasets. The results for DoG and DP are obtained using GPT-3.5, while additional results of DP with other LLMs are provided in Appendix B.5. Across both datasets, DP consistently outperforms strong baselines across all evaluation metrics. Specifically, our framework requires only 2.9 and 2.5 LLM calls on CWQ and WebQSP, respectivelyhighlighting its efficient utilization of prior knowledge embedded in KGs. In terms of token consumption, DP achieves the lowest output token count while also requiring the least input token consumption. This reflects DPs effectiveness in reducing overall token usage during the reasoning process, relying on fewer instructions and exemplars. These findings underscore the superiority of DP compared to existing baselines. 8,182.9 7,803.0 8,298.0 2,928.6 9,669.4 8,156.2 8,682.6 3,115.0 6,031.2 5,234.8 7,332.1 2,552.8 7,018.9 5,517.7 7,604.3 2,699. 1,486.4 353.2 384.6 186."
        },
        {
            "title": "Dataset Method Call",
            "content": "987.7 282.9 277.2 146."
        },
        {
            "title": "ToG\nPoG\nDoG\nDP",
            "content": "22.6 13.3 12.7 2.9 15.9 9.0 10.4 2."
        },
        {
            "title": "5 Conclusion and Limitation",
            "content": "This paper presents trustworthy reasoning framework, DP, which empowers Large Language Models (LLMs) to generate faithful and reliable responses by deliberately reasoning over the priors embedded in knowledge graphs. In the offline stage, DP enables LLMs to generate faithful relational paths through progressive knowledge distillation strategy. In the online stage, it enhances response reliability via reasoning-introspection strategy. These strategies effectively explore and leverage structural patterns and constraint priors within knowledge graphs, respectively. Extensive experiments conducted on three benchmark datasets demonstrate that DP achieves new state-of-the-art performance, highlighting its effectiveness, flexibility, and practical applicability. Moreover, the results emphasize the importance of prior exploitation, particularly path generation and constraint extraction, in supporting trustworthy reasoning over knowledge graphs. While this work advances trustworthy LLM reasoning by incorporating knowledge priors, it still relies on human intervention to define constraints when applied to vertical domains. In future work, we plan to investigate automatic methods for extracting and summarizing constraint types, aiming to further reduce manual effort and enhance scalability."
        },
        {
            "title": "Acknowledgments and Disclosure of Funding",
            "content": "This work was supported in part by the National Key Research and Development Program of China (2022YFC3303600), the National Natural Science Foundation of China (62306229, 62137002, U22B2019, 62477037, 62293553), the Natural Science Basic Research Program of Shaanxi (2023JC-YB-593), the Key Research and Development Program of Shaanxi (2024GX-ZDCYL-02-12), the Youth Innovation Team of Shaanxi Universities \"Multi-modal Data Mining and Fusion\", the Shaanxi Undergraduate and Higher Education Teaching Reform Research Program (23BY195), the Youth Talent Support Program of Shaanxi Science and Technology Association (20240113), and the China Postdoctoral Science Foundation (2024M752585)."
        },
        {
            "title": "References",
            "content": "[1] D. Guo, D. Yang, H. Zhang, J. Song, R. Zhang, R. Xu, Q. Zhu, S. Ma, P. Wang, X. Bi et al., Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, arXiv preprint arXiv:2501.12948, 2025. [2] A. Hogan, X. L. Dong, D. Vrandeˇcic, and G. Weikum, Large language models, knowledge graphs and search engines: crossroads for answering users questions, arXiv preprint arXiv:2501.06699, 2025. [3] J. Ma, Z. Gao, Q. Chai, J. Liu, P. Wang, J. Tao, and Z. Su, Fortisavqa and maven: benchmark dataset and debiasing framework for robust multimodal reasoning, arXiv preprint arXiv:2504.00487, 2025. [4] R. Dominguez-Olmedo, M. Hardt, and C. Mendler-Dünner, Questioning the survey responses of large language models, in NeurIPS, 2024, pp. 45 85045 878. [5] D. Yang, D. Xiao, J. Wei, M. Li, Z. Chen, K. Li, and L. Zhang, Improving factuality in large language models via decoding-time hallucinatory and truthful comparators, in AAAI, 2025, pp. 25 60625 614. [6] V. Rawte, A. Sheth, and A. Das, survey of hallucination in large foundation models, arXiv preprint arXiv:2309.05922, 2023. [7] M. Li, S. Miao, and P. Li, Retrieval or reasoning: The roles of graphs and large language models in efficient knowledge-graph-based retrieval-augmented generation, in ICLR, 2025. [8] L. LUO, Y.-F. Li, R. Haf, and S. Pan, Reasoning on graphs: Faithful and interpretable large language model reasoning, in ICLR, 2024. [9] X. Guan, Y. Liu, H. Lin, Y. Lu, B. He, X. Han, and L. Sun, Mitigating large language model hallucinations via autonomous knowledge graph-based retrofitting, in AAAI, 2024, pp. 18 12618 134. [10] D. Xu, X. Li, Z. Zhang, Z. Lin, Z. Zhu, Z. Zheng, X. Wu, X. Zhao, T. Xu, and E. Chen, Harnessing large language models for knowledge graph question answering via adaptive multi-aspect retrieval-augmentation, in Proceedings of the AAAI Conference on Artificial Intelligence, 2025, pp. 25 57025 578. [11] M. Li, S. Miao, and P. Li, Simple is effective: The roles of graphs and large language models in knowledge-graph-based retrieval-augmented generation, in ICLR, 2025. [12] X. Liang and Z. Gu, Fast think-on-graph: Wider, deeper and faster reasoning of large language model on knowledge graph, arXiv preprint arXiv:2501.14300, 2025. [13] J. Jiang, K. Zhou, Z. Dong, K. Ye, W. X. Zhao, and J.-R. Wen, Structgpt: general framework for large language model to reason over structured data, in EMNLP, 2023, pp. 92379251. [14] R. Zhao, F. Zhao, L. Wang, X. Wang, and G. Xu, Kg-cot: Chain-of-thought prompting of large language models over knowledge graphs for knowledge-aware question answering, in IJCAI, 2024, pp. 66426650. [15] J. Ma, Z. Gao, Q. Chai, W. Sun, P. Wang, H. Pei, J. Tao, L. Song, J. Liu, C. Zhang et al., Debate on graph: flexible and reliable reasoning framework for large language models, in AAAI, 2025, pp. 24 76824 776. [16] S. Ma, C. Xu, X. Jiang, M. Li, H. Qu, C. Yang, J. Mao, and J. Guo, Think-on-graph 2.0: Deep and faithful large language model reasoning with knowledge-guided retrieval augmented generation, in ICLR, 2025. [17] J. Bao, N. Duan, Z. Yan, M. Zhou, and T. Zhao, Constraint-based question answering with knowledge graph, in COLING, 2016, pp. 25032514. [18] K. Ethayarajh, W. Xu, N. Muennighoff, D. Jurafsky, and D. Kiela, Kto: Model alignment as prospect theoretic optimization, arXiv preprint arXiv:2402.01306, 2024. 10 [19] W.-t. Yih, M. Richardson, C. Meek, M.-W. Chang, and J. Suh, The value of semantic parse labeling for knowledge base question answering, in ACL, 2016, pp. 201206. [20] A. Talmor and J. Berant, The web as knowledge-base for answering complex questions, in ACL, 2018, pp. 641651. [21] Y. Zhang, H. Dai, Z. Kozareva, A. Smola, and L. Song, Variational reasoning for question answering with knowledge graph, in AAAI, 2018, pp. 60696076. [22] J. Sun, C. Xu, L. Tang, S. Wang, C. Lin, Y. Gong, L. Ni, H.-Y. Shum, and J. Guo, Think-on-graph: Deep and responsible reasoning of large language model on knowledge graph, in ICLR, 2024. [23] Z. Li, S. Fan, Y. Gu, X. Li, Z. Duan, B. Dong, N. Liu, and J. Wang, Flexkbqa: flexible llm-powered framework for few-shot knowledge base question answering, in AAAI, 2024, pp. 18 60818 616. [24] C. Mavromatis and G. Karypis, Gnn-rag: Graph neural retrieval for large language model reasoning, arXiv preprint arXiv:2405.20139, 2024. [25] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen, LoRA: Low-rank adaptation of large language models, in ICLR, 2022. [26] A. Saxena, A. Tripathi, and P. Talukdar, Improving multi-hop question answering over knowledge graphs using knowledge base embeddings, in ACL, 2020, pp. 44984507. [27] J. Shi, S. Cao, L. Hou, J. Li, and H. Zhang, Transfernet: An effective and transparent framework for multi-hop question answering over relation graph, in EMNLP, 2021, pp. 41494158. [28] J. Jiang, K. Zhou, X. Zhao, and J.-R. Wen, Unikgqa: Unified retrieval and reasoning for solving multi-hop question answering over knowledge graph, in ICLR, 2023. [29] L. Chen, P. Tong, Z. Jin, Y. Sun, J. Ye, and H. Xiong, Plan-on-graph: Self-correcting adaptive planning of large language model on knowledge graphs, in NeurIPS, 2024. [30] S. Cheng, Z. Zhuang, Y. Xu, F. Yang, C. Zhang, X. Qin, X. Huang, L. Chen, Q. Lin, D. Zhang et al., Call me when necessary: Llms can efficiently and faithfully reason over structured environments, in Findings of the ACL, 2024, pp. 42754295. [31] G. Xiong, J. Bao, and W. Zhao, Interactive-kbqa: Multi-turn interactions for knowledge base question answering with large language models, in ACL, 2024, pp. 10 56110 582. [32] T. Ao, Y. Yu, Y. Wang, Y. Deng, Z. Guo, L. Pang, P. Wang, T.-S. Chua, X. Zhang, and Z. Cai, Lightprof: lightweight reasoning framework for large language model on knowledge graph, in AAAI, 2025, pp. 23 42423 432. [33] J. Baek, A. F. Aji, and A. Saffari, Knowledge-augmented language model prompting for zero-shot knowledge graph question answering, arXiv preprint arXiv:2306.04136, 2023. [34] Y. Shu, Z. Yu, Y. Li, B. Karlsson, T. Ma, Y. Qu, and C.-Y. Lin, Tiara: Multi-grained retrieval for robust question answering over large knowledge base, in EMNLP, 2022, pp. 81088121. [35] M. Galkin, Z. Zhu, H. Ren, and J. Tang, Inductive logical query answering in knowledge graphs, in NeurIPS, 2022, pp. 15 23015 243. [36] Y. Cui, Z. Sun, and W. Hu, prompt-based knowledge graph foundation model for universal in-context reasoning, in NeurIPS, 2024, pp. 70957124. [37] G. Dong, R. Li, S. Wang, Y. Zhang, Y. Xian, and W. Xu, Bridging the kb-text gap: Leveraging structured knowledge-aware pre-training for kbqa, in CIKM, 2023, pp. 38543859. [38] M. Yasunaga, A. Bosselut, H. Ren, X. Zhang, C. D. Manning, P. S. Liang, and J. Leskovec, Deep bidirectional language-knowledge graph pretraining, in NeurIPS, 2022, pp. 37 30937 323. [39] X. He, Y. Tian, Y. Sun, N. Chawla, T. Laurent, Y. LeCun, X. Bresson, and B. Hooi, G-retriever: Retrievalaugmented generation for textual graph understanding and question answering, in NeurIPS, 2024, pp. 132 876132 907. [40] P. Jiang, L. Cao, C. D. Xiao, P. Bhatia, J. Sun, and J. Han, Kg-fit: Knowledge graph fine-tuning upon open-world knowledge, in NeurIPS, 2024, pp. 136 220136 258. 11 [41] J. Zhang, X. Zhang, J. Yu, J. Tang, J. Tang, C. Li, and H. Chen, Subgraph retrieval enhanced model for multi-hop knowledge base question answering, in ACL, 2022, pp. 57735784. [42] J. Kim, Y. Kwon, Y. Jo, and E. Choi, Kg-gpt: general framework for reasoning on knowledge graphs using large language models, in EMNLP, 2023, pp. 94109421. [43] Y. Wu, Y. Huang, N. Hu, Y. Hua, G. Qi, J. Chen, and J. Pan, Cotkr: Chain-of-thought enhanced knowledge rewriting for complex knowledge graph question answering, in EMNLP, 2024, pp. 35013520. [44] Y. Ji, K. Wu, J. Li, W. Chen, M. Zhong, X. Jia, and M. Zhang, Retrieval and reasoning on kgs: Integrate knowledge graphs into large language models for complex question answering, in Findings of the EMNLP, 2024, pp. 75987610. [45] P. Yixing, Q. Wang, L. Zhang, Y. Liu, and Z. Mao, Chain-of-question: progressive question decomposition approach for complex knowledge base question answering, in Findings of the ACL, 2024, pp. 47634776. [46] K. Liang, L. Meng, M. Liu, Y. Liu, W. Tu, S. Wang, S. Zhou, X. Liu, F. Sun, and K. He, survey of knowledge graph reasoning on graph types: Static, dynamic, and multi-modal, IEEE TPAMI, pp. 94569478, 2024."
        },
        {
            "title": "A Related Work",
            "content": "End-to-end Reasoning. To achieve quick and efficient responses, several studies [31, 33, 34] directly feed questions along with retrieved triples from Knowledge Graphs (KGs) into text decoders. These approaches primarily focus on extracting critical knowledge to answer the questions. One line of research [33, 35, 36] retrieves relevant facts by performing exact entity matching between the input question and the knowledge graph. The retrieved facts are then provided to Large Language Models (LLMs) through prompt engineering to generate answers. To support more semantically rich queries, another line of work [37, 28, 3840] incorporates knowledge facts into pre-trained models inspired by masked language modeling and subsequently fine-tunes the retriever within the KGQA task. However, such methods often retrieve only the facts associated with the explicit information present in the question, overlooking implicit cues. To address this limitation, some studies [41, 42, 9] propose decomposing complex questions into sub-questions and retrieving knowledge based on semantic similarity between the sub-questions and triples. Nevertheless, these methods often struggle with complex questions, particularly when large number of retrieved facts are directly input into the text decoder, which can overwhelm the model and impair answer quality. In contrast, our proposed framework, DP, employs the reasoning-introspection strategy to perform path selection and backtracking, effectively avoiding the introduction of false positive paths. It should be noted that our work is the first to employ constraint priors in reasoning to the best of our knowledge. Chain-of-thought Reasoning. With the remarkable success of chain-of-thought prompting, researchers [14, 43, 44] have extended step-by-step reasoning to knowledge graph-based tasks. prevalent approach [13, 22, 45] first identifies the topic entity in the question, then iteratively retrieves and refines reasoning path until sufficient factual knowledge is gathered or the answer is obtained, and finally leverages LLMs for answer generation. During the path refinement phase, methods such as DoG [15] utilize in-context learning and multi-agent debate to improve the reliability of generated answers. In contrast to this line of work that decomposes questions step-by-step, another research direction [29, 46, 16] encourages LLMs to directly identify sub-goals of questions and perform reasoning over retrieved knowledge triples. However, both paradigms may produce unfaithful answers, which is critical issue in high-stakes domains like legal decision-making or medical diagnosis. To mitigate this limitation, recent methods [8, 30] first prompt or fine-tune LLMs to generate relation paths according to questions and then retrieve relevant knowledge triples based on those paths to ground the final answer more faithfully. Unlike previous approaches [8, 30], DP adopts progressive knowledge distillation strategy to fine-tune LLMs, enabling them to generate more faithful relation paths by more effectively exploiting structural information. This enhanced exploitation is achieved through more comprehensive collection of weak supervision signals and preference-aware path generation."
        },
        {
            "title": "B Experiment",
            "content": "B.1 Experimental Setting Training Detail. The path generator LLaMA3.1-8B-Instruct is loaded from Hugging Face1 and trained using the LLaMA-Factory2 framework. All training is conducted on two NVIDIA A80080GB GPUs, with bfloat16 precision enabled to reduce memory usage and accelerate training. We employ Low-Rank Adaptation (LoRA) to perform an efficient adaptation of large-scale parameters in SFT and KTO. The LoRA configuration uses rank of 16, an alpha of 32, and dropout rate of 0.1 and applies to the query, key, value, and output of the self-attention layers. The initial learning rate of SFT training is set to 5e-5, with warm-up ratio of 0.1. The KTO training uses an initial learning rate of 1e-5, with preference beta of 0.1 and warm-up ratio of 0.1. We use the default weights for positive and negative samples, setting λp = λn = 1. The batch size of SFT and KTO training is set to 4. The datset statistics are shown in Table 6. Baseline Introduction. We compare existing state-of-the-art SL, ICL, and HL-based methods with DP to verify the effectiveness and superiority. 1https://huggingface.co 2https://github.com/hiyouga/LLaMA-Factory 13 Table 6: The statistics of WebQSP, CWQ, and MetaQA. KG"
        },
        {
            "title": "Freebase\nFreebase",
            "content": "2,566,291 2,566,291 43,234 7,058 7,058 9 8,309,105 8,309,105 133,592 2,826 27,639 329,282 1,628 3,531 30,903 2 4 MetaQA Wiki-Movie"
        },
        {
            "title": "WebQSP\nCWQ",
            "content": "The SL-based methods are introduced as follows. (1) EmbedKGQA [26] addresses multi-hop question answering over sparse KGs by leveraging KG embeddings to predict missing links, effectively mitigating KG incompleteness. Unlike prior approaches, it relaxes answer selection constraints, significantly improving multi-hop reasoning performance on sparse KGs. (2) TransferNet [27] is transparent and effective model for multi-hop question answering that unifies reasoning over labeled KG relations and textual relations. It iteratively attends to question components and transfers entity scores along activated relations in differentiable manner, achieving high improvements with interpretable intermediate results. (3) UniKGQA [28] unifies retrieval and reasoning for multi-hop KGQA through shared model architecture and joint parameter learning. By combining semantic matching with information propagation over KGs, it jointly optimizes retrieval and reasoning, improving accuracy and efficiency on complex questions. (4) RoG [8] integrates LLMs with KGs via planning-retrieval-reasoning framework that generates KG-grounded relation paths as faithful plans to guide reasoning. This approach enhances reasoning accuracy and interpretability by leveraging structural KG information and supports flexible integration with diverse LLMs. Different from RoG, our proposed framework DP collects question-to-path mappings in the one-to-many form rather than one-to-one. (5) AMAR [10] is an adaptive multi-aspect retrieval framework that enhances LLM reasoning by retrieving and embedding entities, relations, and subgraphs from KGs. It incorporates self-alignment and relevance gating modules to reduce noise and selectively integrate pertinent knowledge, significantly improving accuracy and logical form generation in KGQA tasks. (6) GNNRAG [24] integrates the graph reasoning capabilities of Graph Neural Networks (GNNs) with the language understanding of LLMs in retrieval-augmented generation framework. By extracting dense subgraph reasoning paths via GNNs and verbalizing them for LLM reasoning, it effectively addresses multi-hop KGQA and achieves state-of-the-art performance with efficient model sizes. The ICL-based methods are described below. (1) PoG [29] presents self-correcting, adaptive planning framework for KG-augmented LLMs that decomposes questions into sub-goals and iteratively explores, updates, and refines reasoning paths over KGs. By integrating guidance, memory, and reflection mechanisms, PoG improves reasoning efficiency and accuracy in complex KGQA tasks. (2) ToG [22] proposes an interactive LLM-KG integration paradigm in which the LLM acts as an agent performing beam search over KGs to identify and reason along promising paths. This training-free, plug-and-play method improves reasoning, knowledge traceability, and correctability, achieving good results with smaller LLMs and reduced computational cost. (3) Readi [30] enables LLMs to efficiently perform multi-hop reasoning over structured data by generating and iteratively editing reasoning paths only when needed. It improves reasoning accuracy and faithfulness while minimizing unnecessary edits, outperforming prior LLM-based methods on both KGQA and TableQA benchmarks. (4) DoG [15] is an iterative interactive KGQA framework that improves LLM reasoning through subgraph-focusing mechanism and multi-role debate strategy. By reducing distractions from long reasoning paths and mitigating false-positive relations, DoG enables more accurate and reliable answers in complex KGQA scenarios. The HL-based methods are summarized as follows. (1) Interactive-KBQA [31] enables LLMs to generate executable logical forms through direct interaction with knowledge bases under minimal supervision. By introducing general APIs and few-shot exemplars for complex questions, it supports step-by-step reasoning and iterative refinement, achieving strong results in low-resource KGQA settings. (2) LightPROF [32] is lightweight and efficient framework for KGQA that enhances LLM reasoning by structurally integrating KGs into prompts. It retrieves relevant subgraphs, encodes their factual and structural information via trainable Knowledge adapter, and maps them into the LLM embedding space, enabling effective reasoning with minimal parameter updates. 14 B.2 Instruction and Exemplar We show the instructions and exemplars utilized in the module within the Planning and Introspection stages. B.2.1 Path Generation Please generate relation paths that can help in reasoning to answer the question. The relation paths must start from the topic entities mentioned in the question. The question is: {question}, and the topic entities are: {topic_entities} B.2.2 Constraint Extraction You will be given question. Your task is to identify and extract any constraints present in the question. **Types of constraints to extract:** 1. Type Constraint: - The answer should be of specific type or category. 2. Multi-Entity Constraint: - The question contains multiple entities and requires the answer to simultaneously satisfy conditions related to these entities. 3. Explicit Time Constraint: - specific time period or date is mentioned explicitly. 4. Implicit Time Constraint: - time period or date is indirectly implied. 5. Order Constraint: - The question involves sequence or ordering. Instructions: - **Extract only present constraints**: Do not add constraints not explicitly or implicitly mentioned in the question. - **Output Format**: Return **List object** with the identified constraints. Each constraint type should either contain the relevant information or an empty string if not applicable. In-Context Few-Shot Example 1: - Question: What country bordering France contains an airport that serves Nijmegen? - Output: [\"1. The answer should be country\", \"2. The country borders France\", \"3. The country contains an airport that serves Nijmegen.\"] Example 2: - Question: what did james polk do before he was president - Output: [\"1. The question implies the time before James K. Polk was president\"] Example 3: - Question: Who was the 1996 coach of the team owned by Jerry Jones? - Output: [\"1. The team is owned by Jerry Jones\", \"2. The person was the coach in 1996\"] Example 4: - Question: What was the last World Series won by the team whose mascot is Lou Seal? - Output: [\"1. The team is the one whose mascot is Lou Seal\", \"2. The answer should be World Series\", \"3. The World Series is the last one won by the team\"] Example 5: - Question: What genre is the movie Titanic? - Output: [\"1. The answer should be genre of the movie\"] 15 ##### Input question: {question} Output: B.2.3 Path Selection Based on the reasoning relation paths in Freebase, think step by step to select the most one relevant path to answer the question. You will be given: - Question: The question to be answered. - Topic Entities: The main entities identified in the question. - Memory: Paths have been selected and the feedback of the previous step. - Reasoning Paths: set of reasoning paths starting from the topic entities. In-Context Few-Shot Example 1: - Question: What sports team owned by George Steinbrenner did Deion Sanders play baseball for?, - Topic Entities: [Baseball, Deion Sanders, George Steinbrenner], - Memory: [], - Reasoning Paths: [Path 1: Baseball -> base.sportbase.sport.played_by_clubs -> Unknown Entity, Path 2: Deion Sanders -> sports.pro_athlete.teams -> Unknown Entity -> sports.sports_team_roster.team -> Unknown Entity, Path 3: Deion Sanders -> baseball.baseball_player.batting_stats -> Unknown Entity -> baseball. batting_statistics.team -> Unknown Entity, Path 4: George Steinbrenner -> sports. sports_team_owner.teams_owned -> Unknown Entity] - Output: {{Path 2}} - This path starts from Deion Sanders, follows his professional athlete teams, and connects to specific sports team. Since the question asks for the team he played baseball for, this path is the most relevant in identifying the correct team. Example 2: - Question: What movie was Charlie Hunnam in that was about human extinction?, - Topic Entities: [Human extinction, Charlie Hunnam], - Memory: [{{selected_path: Charlie Hunnam -> film.actor.film -> Unknown Entity -> film.performance.film -> Unknown Entity, feedback: This path connects Charlie Hunnam to films he has acted in, but we need to find which movie is about human extinction.}}], - Reasoning Paths: [Path 1: Human extinction -> film.film_subject.films -> Unknown Entity, Path 2: Charlie Hunnam -> film.actor.film -> Unknown Entity -> film. performance.film -> Unknown Entity, Path 3: Charlie Hunnam -> common.topic. notable_types -> Unknown Entity, Path 4: Charlie Hunnam -> tv.tv_actor. starring_roles -> Unknown Entity -> tv.regular_tv_appearance.character -> Unknown Entity] - Output: {{Path 1}} - The previous selected path connected Charlie Hunnam to films he acted in but did not ensure the movie was about human extinction. This path directly connects \"Human extinction\" to relevant films, making it the best choice to identify the correct movie. Example 3: - Question: Which state with Colorado River that Larry Owens was born in? - Topic Entities: [Colorado River, Larry Owens], - Memory: [{{selected_path: Colorado River -> location.location. partially_containedby -> Unknown Entity, feedback: This path connects Colorado River to location, but we need to find the state Larry Owens was born in.}}], - Reasoning Paths: [Path 1: Larry Owens -> people.person.spouse_s -> Unknown Entity -> people.sibling_relationship.sibling -> Unknown Entity] - Output: {{no path}} - None of the available paths lead to information about the state where Larry Owens was born. 16 B.2.4 Constraint Verification Given question and the associated retrieved knowledge triplets from Freebase, your task is to answer the question with these triplets and your knowledge. You will be given: - Question: The question to be answered. - Topic Entity: The main entity identified in the question. - Constraints: The constraints extracted from the question that should be verified. - Reasoning Paths: Paths starting from the topic entity (contains only relations). - Knowledge Triplets: The instantiated reasoning paths in the form of triplets ( entity, relation, entity). Think step by step to answer the question: - List all potential answers (**tail entities**) based on the knowledge triplets, ranking them by how likely they satisfy the question and constraints, and placing the most likely ones first. - Check if the answer satisfies the constraints and provide an explanation detailing the reasoning process and any missing knowledge needed for full verification. Return **JSON object** with the identified constraints. Important: - DO NOT output anything except the JSON result. - DO NOT add explanations, headers, or markdown formatting. - Return only the JSON object shown in examples. - Output must be valid JSON object. All strings and keys must be enclosed in double quotes. - Only return the JSON object, no explanation or prefix like \"Output:\". Format of the output: {{\"answer\": [...], \"sufficient\": \"Yes\"/\"No\", \"reason\": \"...\"}} In-Context Few-Shot Example 1: - Question: what is the name of justin bieber brother - Topic Entity: [\"Justin Bieber\"], - Constraints: [], - Reasoning Path: [\"Justin Bieber -> people.person.sibling_s -> Unknown Entity -> people.sibling_relationship.sibling -> Unknown Entity\"], - Knowledge Triplets: [[[\"Justin Bieber\", \"people.person.sibling_s\", \"m.0gxnnwp\"], [\"m.0gxnnwp\", \"people.sibling_relationship.sibling\", \"Jaxon Bieber\"]]] - Output: {{\"answer\": [\"Jaxon Bieber\"], \"sufficient\": \"Yes\", \"reason\": \"Based on the reasoning path, the answer is Jaxon Bieber, which is the sibling of Justin Bieber .\"}} Example 2: - Question: What movie was Charlie Hunnam in that was about human extinction?, - Topic Entities: [\"Human extinction\", \"Charlie Hunnam\"], - Constraints: [\"1. The movie is film Charlie Hunnam acted in.\", \"2. The movie is about human extinction.\"], - Reasoning Path: [\"Charlie Hunnam -> film.actor.film -> Unknown Entity -> film. performance.film -> Unknown Entity\"], - Knowledge Triplets: [[[[\"Charlie Hunnam\", \"film.actor.film\", \"m.0jwksr\"], [\"m.0 jwksr\", \"film.performance.film\", \"Cold Mountain\"]],[[\"Charlie Hunnam\", \"film.actor. film\", \"m.0jy_sj\"], [\"m.0jy_sj\", \"film.performance.film\", \"Green Street\"]],[[\" Charlie Hunnam\", \"film.actor.film\", \"m.046168c\"], [\"m.046168c\", \"film.performance. film\", \"Children of Men\"]]]] - Output: {{\"answer\": [\"Children of Men\", \"Green Street\", \"Cold Mountain\"], \" sufficient\": \"No\", \"reason\": \"The reasoning path connects Charlie Hunnam to films he has acted in, but we need to find which movie is about human extinction.\"}} 17 B.3 DP performance with various LLMs The detailed DP performance with various LLMs, presented as mean values with standard deviations, on WebQSP and CWQ is summarized in Table 7. Table 7: DP performance (mean with standard deviation) on WebQSP and CWQ. The results are averaged over three independent evaluations. We highlight the best performance in bold and the second-best in underline. LM denotes the language model used. LM"
        },
        {
            "title": "WebQSP",
            "content": "H H@1 F1 LLaMA3.1-8B 87.90.2 75.10.5 89.70.6 90.70.6 90.60.5 Qwen3-8B GPT-3.5 GPT-4o GPT-4. 82.80.4 82.70.3 86.90.3 87.50.8 86.70.4 75.70.8 72.30.6 79.20.3 81.40.5 80.10.8 70.80.3 76.10.6 80.00.6 85.20.4 87.20."
        },
        {
            "title": "CWQ",
            "content": "H@1 61.10.4 61.70.4 72.60.1 74.60.5 75.80."
        },
        {
            "title": "MetaQA",
            "content": "F1 H@1 F1 58.50.9 60.20.7 69.20.4 70.50.5 69.40.9 90.20.1 90.40.3 96.70.1 96.50.6 96.80. 87.40.2 89.20.2 95.40.3 95.20.4 95.50.3 84.10.5 88.60.2 90.80.4 94.40.4 94.90.1 B.4 Exemplar Impacts We conduct experiments on CWQ and WebQSP to investigate how the number of exemplars influences response generation. As shown in Figure 5, the performance, measured by H@1, declines as the number of exemplars increases. This degradation may stem from two main factors: (1) larger number of exemplars results in longer input context, making it more challenging for the model to capture semantic information effectively. (2) The diversity of exemplars remains relatively unchanged despite the increased quantity, which may cause LLMs to become less confident when encountering out-of-distribution scenarios. Figure 5: Impact of the number of exemplars on response generation. The results are evaluated by H@1. B.5 Interaction with Different LLMs Figure 6 presents the token consumption and LLM invocation statistics of DP when interacting with various LLMs on the CWQ and WebQSP datasets. The results are averaged over three runs. We see that DP with various LLMs consumes similar tokens and LLM calls to answer question, except for Qwen3-8B, which shows higher consumption due to its inherently slow thinking mechanisms. It is worth noting that the token consumption for path generation is excluded from this analysis, as that component is executed offline. Overall, these results further validate the practicality and efficiency of the proposed DP framework in real-world scenarios. B.6 Case Study We conduct case studies to qualitatively analyze the strengths and limitations of the proposed framework DP. Figure 7 and 8 illustrate successful cases, while Figure 9 and 10 showcase failure cases. In the successful examples, DP effectively guides LLMs to generate accurate answers by selecting appropriate reasoning paths. When reasoning path fails to satisfy certain extracted constraints, DP can provide informative feedback, explicitly indicating which constraint is violated. 18 Figure 6: Average token usage and calls of different LLMs. In contrast, the failure cases highlight two types of errors: incorrect path generation and flawed reasoning. Notably, in the case of path generation errors, the generated paths by DP closely resemble the ground-truth paths, implying that more robust mechanism may be required better to capture the structural patterns inherent in KGs. In the case of reasoning errors, we see that the LLM fails to cover all accurate answers, although the reasoning paths contain the required knowledge facts. 19 Figure 7: Successful case requiring path selection again. 20 Figure 8: Successful case without another iteration. Figure 9: Case of path generation error. Figure 10: Case of reasoning error."
        }
    ],
    "affiliations": [
        "MOE KLINNS Lab, Xian Jiaotong University",
        "School of Artificial Intelligence, Chongqing University of Post and Telecommunications",
        "School of Computer Science and Technology, Xian Jiaotong University",
        "School of Computer Science, Northwestern Polytechnical University",
        "Shaanxi Province Key Laboratory of Big Data Knowledge Engineering"
    ]
}