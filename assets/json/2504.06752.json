{
    "paper_title": "Compass Control: Multi Object Orientation Control for Text-to-Image Generation",
    "authors": [
        "Rishubh Parihar",
        "Vaibhav Agrawal",
        "Sachidanand VS",
        "R. Venkatesh Babu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Existing approaches for controlling text-to-image diffusion models, while powerful, do not allow for explicit 3D object-centric control, such as precise control of object orientation. In this work, we address the problem of multi-object orientation control in text-to-image diffusion models. This enables the generation of diverse multi-object scenes with precise orientation control for each object. The key idea is to condition the diffusion model with a set of orientation-aware \\textbf{compass} tokens, one for each object, along with text tokens. A light-weight encoder network predicts these compass tokens taking object orientation as the input. The model is trained on a synthetic dataset of procedurally generated scenes, each containing one or two 3D assets on a plain background. However, direct training this framework results in poor orientation control as well as leads to entanglement among objects. To mitigate this, we intervene in the generation process and constrain the cross-attention maps of each compass token to its corresponding object regions. The trained model is able to achieve precise orientation control for a) complex objects not seen during training and b) multi-object scenes with more than two objects, indicating strong generalization capabilities. Further, when combined with personalization methods, our method precisely controls the orientation of the new object in diverse contexts. Our method achieves state-of-the-art orientation control and text alignment, quantified with extensive evaluations and a user study."
        },
        {
            "title": "Start",
            "content": "Compass Control: Multi Object Orientation Control for Text-to-Image Generation Rishubh Parihar1* Vaibhav Agrawal2 Sachidanand VS1 R. Venkatesh Babu1 1IISc Bangalore 2IIIT Hyderabad 5 2 0 2 0 ] . [ 2 2 5 7 6 0 . 4 0 5 2 : r Figure 1. We present Compass Control, method to generate multi-object scenes with orientation control from text-to-image diffusion models. Given text prompt and an orientation of each object (shown as frustum, the colored face is the forward direction), our method generates scenes that align with both the prompt and specified orientations. Additionally, with few ( 10) unposed images of new object, our model is personalized to generate the object in target orientations."
        },
        {
            "title": "Abstract",
            "content": "Existing approaches for controlling text-to-image diffusion models, while powerful, do not allow for explicit 3D objectcentric control, such as precise control of object orientation. In this work, we address the problem of multi-object orientation control in text-to-image diffusion models. This enables the generation of diverse multi-object scenes with precise orientation control for each object. The key idea is to condition the diffusion model with set of orientationaware compass tokens, one for each object, along with text tokens. light-weight encoder network predicts these compass tokens taking object orientation as the input. The model is trained on synthetic dataset of procedurally generated scenes, each containing one or two 3D assets on plain background. However, direct training this framework results in poor orientation control as well as leads to entanglement among objects. To mitigate this, we intervene in the generation process and constrain the cross-attention maps of each compass token to its corresponding object regions. The trained model is able to achieve precise orientation control for a) complex objects not seen during training and b) multi-object scenes with more than two objects, indicating strong generalization capabilities. Further, when combined with personalization methods, our method precisely controls the orientation of the new object in diverse contexts. Our method achieves state-of-the-art orientation control and text alignment, quantified with extensive evaluations and user study. project page 1. Introduction Imagine visual artist aiming to create scene featuring two cars facing each other. They prompt text-to-image model with - photo of sedan facing right and an SUV facing left. However, as shown in Fig. 1 (a), the generated image may not always accurately capture the intended object orientations. Moreover, relying on text prompts to control object orientation (e.g., facing right) is imprecise and requires iterative prompting. Can we design an alternate interface for text-to-image models that accepts target orientation angle as input along with the text prompts? Such an interface will allow for precise orientation control for each object, eliminating the need for iterative prompt adjustments and streamlining the creative process. Several works have been proposed to achieve finer control in text-to-image models such as changing object appearance, scene layouts or image style [5, 9, 12, 22, 24, 45, 55, 57, 65, 68]. While effective for controlling 2D attributes of the image, these methods fail to accurately control 3D attributes. More recently, several methods have been proposed to control 3D properties in text-to-image mod- *equal contribution. work done during an internship at VAL, IISc Table 1. Comparison with Related works. Our approach uniquely allows for object-centric orientation control and generalize to novel categories without any explicit 3D representation. Method CD-360 [33] LooseControl [4] Cont-3D-Words [14] ViewNeTI [7] Ours Input 3D conditioning Novel classes Multiple Object Control Cam Pose Explicit 3D boxes Explicit Cam Pose Implicit Cam Pose Implicit Orientation Implicit els, such as camera viewpoint [7, 33], scene lighting [14], or scene layout using 3D bounding boxes [4]. However, these approaches either require dense 3D information, like multi-view images or accurate 3D bounding boxes, or limited to simple single-object scenes (Tab. 1). In this work, we present novel interface to condition text-to-image diffusion models to generate multi-object scenes with precise 3D orientation control, without the need for multi-view images or 3D bounding boxes. Text-to-image (T2I) diffusion models enable the generation of objects with specific attributes via text prompts (e.g., red car). Motivated by this, we encode the object orientation as an additional attribute in the text embedding space of the T2I model. Specifically, we introduce special token, dubbed as compass token (c) along with each token in the prompt (e.g., photo of c1 SUV and c2 sedan on road.). Each compass token is predicted by lightweight encoder model taking the prescribed orientation angle as input. This formulation preserves the original interface of the base T2I model and enables precise object-centric orientation control in multi-object scenes. We train the encoder model and fine-tune the denoising U-Net with LoRA [26] on synthetic dataset of scenes containing one or two 3D assets placed in diverse layouts on an empty floor. We discover that directly injecting compass tokens with the prompt tokens leads to poor orientation control, as the compass token attends to irrelevant image regions, limiting its influence on its corresponding object (Fig. 4(a)). To address this, we propose Coupled Attention Localization (CALL) mechanism, where we constrain the cross-attention maps of the compass token and its corresponding object token within 2D bounding box. This results in tight coupling between the two tokens, enabling the compass token to precisely control the orientation for its corresponding object. Additionally, for multi-object scenes, this results in an appropriate binding between each compass token and its corresponding object token, leading to disentangled orientation control of individual objects (Fig. 4(b)). The proposed approach achieves precise orientation control for unseen objects (e.g., pram) and can generalize to scenes with more than two objects, despite being trained on one and two object scenes only (see Fig. 1). Further, given few unposed images of real object, we can personalize the model to control the orientation of the new object. We evaluate our method against several baselines, achieving superior performance both quantitatively and in user study. In summary, our primary contributions are: 1. Compass Control - method for conditioning textto-image diffusion models on object orientation, enabling precise orientation control for individual objects in multi-object scene generation. 2. Coupled Attention Localization - mechanism to restrict the influence of the input object orientation to the corresponding object, ensuring effective object-centric orientation control and object disentanglement. 3. Strong generalization of Compass Control for precise orientation control to unseen objects and complex multiobject scenes, though trained on simple synthetic scenes. 4. Personalization of Compass Control - Given few unposed images of real object, our method can perform orientation control of the new object in diverse contexts. 2. Related work Controlled generation in T2I models. Several works have been proposed to achieve fine-grained control in text-toimage diffusion models [49, 51, 53]. Recent works resort to manipulation of the text embeddings [21, 29, 44, 59, 69], or attention maps in the diffusion U-Net [1, 2, 9, 11, 16, 24, 31, 42, 45, 50, 57, 58] for controlling the generated image. Additional encoder models can be trained to condition the T2I models on new modality such as depth, bounding boxes, or object identity [34, 65, 68]. Another set of works personalizes the diffusion model given with few subject images [32, 52] enabling generation of the learned subject in different backgrounds. Recent work on guiding diffusion models [20, 37, 43] allows inference time control over the scene contents, allowing the control of object location, appearance, shape, and skeleton pose. However, these controls are limited to 2D. 3D-aware image editing. Recent works leverage the rich generation capability of the T2I model to perform 3D aware editing [42, 54, 61]. Specifically, they use the input scene depth as an additional input and use it to warp the internal features of the diffusion models. This enables zero-shot geometric 3D edits such as translating or rotating an object. However, these methods are limited to the editing of single object. Another line of work uses multi-view input images and trains an implicit 3D representation such as radiance field in the diffusion feature space [33, 46] to perform 3D consistent editing. More recently, few works leverage 3D Gaussian splat representation along with T2I models to perform scene editing [13, 38]. However, the above methods require scene-specific training and require multi-view images of accurate depth maps as input. Another direction explores large-scale training of diffusion model on specific dataset [27, 39, 63] allowing for 3D scene editing. However, these methods fail to generalize to in-the-wild real-world scenes outside the distribution of training datasets. 3D control in generation. Earlier works train generaFigure 2. Synthetic data generation. We curate 10 diverse 3D assets, and render them in diverse layouts and orientations in Blender [15]. The rendered scenes are augmented with realistic generations from Canny [8] ControlNet [68]. The final dataset consists of one and two object scenes. tive models from scratch with explicit scene controls such as 3D blobs [60], or radiance fields of individual objects [3, 28, 30, 40, 41, 64] to control the generated scene. Recent works have shown the existence of 3D properties in text-to-image diffusion models [7, 1719, 67]. This has fuelled the research that leverages this knowledge from T2I models for 3D generation. set of works lifts this knowledge to 3D by distilling from pre-trained T2I models [35, 48, 56, 62, 66, 70]. Another set of recent works [4, 7, 14] leverage this underlying 3D knowledge for controlling 3D properties of the generated scene with an additional conditioning mechanism. ViewNeTI [7] learns 3D view token to control the camera view for the task of novel-view synthesis from dataset of multi-view images. ViewNeTI is natively designed for novel view synthesis but is unable to generate objects in diverse contexts. continuous word representation is trained in [14] for 3D scene properties such as orientation and lighting on single 3D mesh. However, both these approaches are limited to simple scenes and control only the global view angle. The closest to our work is [4] that conditions the T2I model on loose depth maps using ControlNet [68]. Loose depth maps are created using 3D object boxes and scene boundaries to enable 3D objectcentric control. However, this approach relies on precise 3D boxes, making it cumbersome at inference. In contrast, our method only requires coarse 2D bounding boxes and orientation angles, offering more user-friendly solution. 3. Method Text-to-Image Diffusion Models. Diffusion models, when directly applied in the pixel space, are computationally expensive due to their iterative nature. To mitigate this, latent diffusion models [51] apply the diffusion process in the smaller resolution latent space of pretrained autoencoder. Further, the generation can be conditioned on text by injecting text features into the diffusion U-Net with additional cross-attention layers. The cross-attention maps allow for precise control during generation [20, 25]. 3.1. Dataset Synthetic scene generation: We curated list of 10 diverse 3D assets from the web: ostrich, helicopter, shoe, jeep, teddy bear, lion, sedan, horse, motorbike and sofa to aid in model generalization (see Fig. 2). Our dataset consists of 1000 one-object and 7900 two-object scenes, rendered in Blender [15]. For each image, we save the 2D bounding boxes and the orientations of the objects. The objects are placed at varied locations and in varied orientations to increase layout diversity. Augmentations: Directly training on this dataset results in overfitting to the plain background and the black floor, as shown in ablations. To address this, we augment the dataset using ControlNet [68] to place objects in diverse backgrounds while retaining known orientations. For each rendered image, we extract its Canny [8] edge map to condition ControlNet to generate the objects in diverse contexts (for e.g., in garden..., near lake..., etc). This approach preserves object orientations while altering their appearance. We manually filter out the inconsistent augmented images. Further details on the dataset creation process can be found in the Suppl. Sec.H. Orientation convention: In this paper, we parameterize orientation with single angle θ, rotation around the up axis in the world coordinate system (pointing towards the sky). We define θ = 0 as reference when the object faces exactly towards the right (e.g., sedan in Fig. 2). We parameterize with single orientation angle, as most of our objects are land objects, and only this rotation axis results in plausible object orientations. However, our method is not limited to single orientation angle, and we present results in Suppl. Sec.B for conditioning on three orientation angles. 3.2. Compass Control Given text prompt consisting of object names {o1, o2, ..., oN } (e.g., photo of jeep and sedan and horse in garden) and their corresponding 3D orientation angles {θ1, θ2, ..., θN } we introduce set of compass tokens {c1, c2, ..., cN} to control the orientations of the respective objects. compass token cn is an embedding in the input space of the text encoder. It is predicted by lightweight MLP encoder network (see Fig. 3), which takes as input the object orientation angle θk. The compass tokens are prepended before their corresponding object tokens (e.g., photo of c1 jeep and c2 sedan and c3 horse in garden), and passed through the text encoder. The outputs of the text encoder are used to condition the denoising U-Net. However, directly training the above framework on the dataset from Sec. 3.1 fails to learn accurate orientation control. We hypothesize that this is because the added compass Figure 3. Compass Control. Given an orientation angle θj, we project it to compass token with lightweight encoder model. The compass tokens are interleaved with the text tokens (as shown in the figure) and passed through the text encoder. The outputs of the text encoder are used to condition the denoising process in the U-Net. We train and also fine-tune the U-Net using LoRA [26]. tokens are unrestricted and can attend to irrelevant image regions, limiting their influence on their corresponding objects. This is evident in the cross-attention maps for the compass tokens, which are indeed diffused in other image regions (see Fig. 4 (a)). Furthermore, this becomes severe issue in multi-object scenes, where single compass token can attend to multiple objects, resulting in the entanglement between different compass tokens(see Fig. 4 (b)). Existing works have shown that cross-attention maps closely control the image layouts [25] in the generated image. Motivated by this, we design cross-attention localization approach. 3.3. Coupled Attention Localization (CALL) Our key idea is to constrain the cross-attention maps for both the compass token and the corresponding object token inside given 2D bounding box. This enables tight association between the object and the compass tokens. Additionally, it enables explicit control over the object location during generation. Specifically, during training, we use the saved object bounding boxes {b1, ...bN } to com1, ...bl pute set of loose square bounding boxes {bl }. The side length an of the loose box bl is computed as an = λ max(hbn , wbn ), where hbn and wbn are the height and width of the object box bn and λ > 1 is padding factor controlling the looseness of the box. Next, we compute binary mask mn from the loose bounding box bl n, such that is 0 inside the box bl and outside. We use it to mask out the cross attentions (Ψ) of the object token on and compass token cn as follows: (cid:32) Ψ(cn) = softmax + (cid:32) Ψ(on) = softmax + (cid:33) (cid:33) QK(cn)T dK QK(on)T dK where the query feature comes from the U-Net features, and the key features K(cn) and K(on) come from the respective tokens. This masking operation is performed at all the diffusion timesteps and cross-attention layers. We find that using loose mask is highly effective, providing greater flexibility during the generation process. This attention localization mechanism is dubbed as Coupled Attention Localization, or CALL for short. Adding CALL mechanism during training and inference has key advantages for composing multi-object scenes: a) Appropriate binding of each compass token cn with its corresponding object token on leads to disentangled orientation control of individual objects. b) Constraining the cross-attention for the object tokens on to non-overlapping bounding boxes results in disentanglement between the objects themselves (a known issue in T2I models [11]), enabling strong generalization to complex scenes with multiple objects. Training. We train our encoder model and fine-tune the denoising U-Net with LoRA [26] on the synthetic dataset from Sec. 3.1. The LoRA training is extremely parameter efficient and preserves the behavior of the base T2I model. We use the proposed CALL mechanism for effective learning of orientation control. However, for the effective working of CALL, the object must be generated within the loose bounding box, as the compass tokens influence is restricted to this region. To this end, we first train on simple singleobject scenes, to first learn the bounding box adherence for the generated object, and then train on mix of both single and two-object scenes thereafter. We contrast this twostaged training procedure with the single-stage training at an intermediate training iteration in Fig. 5. The generated objects adhere to the bounding box better in the two-staged training compared to single-stage training This leads to effective learning of orientation control, as we have shown in ablative experiments (Sec. 4). Inference. During the inference phase, Compass Control expects the text prompt containing the objects, desired orientations, and optional coarse 2D bounding boxes as input. Using loose bounding boxes during training offers significant advantage here, as we can even spawn nonoverlapping boxes heuristically, as shown in the Suppl. Sec.F. We use the text tokens and the compass tokens together to condition the diffusion model. 3.4. Personalization The design of Compass Control as conditioning mechanism preserves the original capabilities of T2I models, such Figure 4. Binding the compass tokens: We visualize the averaged cross attention of the compass token(s) when training with CALL (shown on the left) and without it (shown on the right). CALL localizes the influence of the compass token at the correct regions, which (a) improves orientation control (b) disentangles orientations in multi-object scenes. In (b), c1 and c2 are compass tokens for car and motorbike, respectively. Figure 5. Staged training results in improved adherence of objects to the bounding boxes, leading to orientation learning. as personalization. Given few unposed images of an object ( 10), we apply Dreambooth [52] with LoRA and associate special token ˆu for the input object using Compass Controls fine-tuned UNet. During inference, we can generate the object in desired orientation θ with simple prompts; e.g., photo of c(θ) ˆu car on the beach., where c(θ) is the compass token. 4. Experiments 4.1. Experiment setup Dataset. We use the synthetic dataset from Sec. 3.1 consisting of 8900 rendered images and 6010 augmented images consisting of one and two object scenes for training our model. For quantitative evaluation, we construct test set of 10 scene prompts having one/two object names generated from ChatGPT [10]. We use mix of seen objects (horse, jeep, sedan, sofa, teddy, lion) and unseen objects (boat, dolphin, ship, SUV, tractor) in the prompts. For each object and prompt combination, we use set of 10 randomly sampled orientations. The list of input text prompts and orientations is given in the Supp. Sec.L. Implementation Details. We use Stable Diffusion Figure 6. Main Results. Compass Control generates complex scenes aligned with the text prompts and the orientations (shown as frustum, the colored face is forward direction). It generalizes well to unseen object categories - pirate ship, boat, yacht, bicycle, pram, dog, and even human. Further, it generates high-quality compositions of several objects, despite being trained only on one and two object scenes. v2.1 [51] as our base T2I model and use LoRA rank 4 for fine-tuning its UNet. We present additional results on Stable Diffusion-XL [47] in Suppl. Sec. D. Our encoder model is lightweight MLP: three linear layers with ReLU. We train our model for 25, 000 steps with batch size of 4 with AdamW optimizer and fixed learning rate of 104. We keep the bounding box padding λ = 1.2 for CALL. The training takes 24 hours on single A6000 GPU. Evaluation Metrics. We evaluate for a) Text Alignment using CLIP similarly; b) %Object Generation - we evaluate the presence of the intended object using GroundingDINO [36] and threshold on the objectness score for each object in the prompt. c) Angular Error - to evaluate the orientation consistency, we compute the Angular error (in radians) between the input orientation angle θ and the orientation of the generated object using pretrained orientation predictor. Further details about the orientation predictor and the implementation details of the metrics are in the Suppl Sec.I. Baselines. As no prior method tackles our same task, we compare against methods that allow for either camera pose control or 3D object pose control in text-to-image models: a) Continuous 3D Words (Cont-3D-Words) [14]: Following their exact setup, we train 3D word for controlling the object orientation on renderings of single 3D asset - Sedan and its ControlNet augmentations. b) ViewNeTI [7]: In contrast to Cont-3D-Words, ViewNeTI allows for training on multiple 3D assets; for fair evaluation, we train ViewNeTI on our training dataset and condition the T2I on object orientation instead of 3D camera pose. Notably, both of these methods are limited to global view control. Hence, we evaluate on only single-object scenes. c) LooseControl [4], allows for multi-object control by conditioning on loose depth maps  (Fig. 7)  formed by 3D object boxes. We use template 3D bounding boxes for each test object and place them in the scene, with random orientation and location (similar to Sec. 3.1). For fair comparison, we use the 2D boxes corresponding to the 3D boxes in our outputs. Notably, LooseControl does not take exact orientation as input as 180 flipped box also has the same loose depth; we consider this while computing the Angular error. Further, it requires the user to provide accurate 3D bounding boxes during inference, which is cumbersome, whereas our method requires only loose 2D boxes. Additional baseline details are in Suppl. Sec.J. 4.2. Main Results Qualitative results. We present our methods results in Fig. 6. Our method is able to generate complex multiobject scenes with precise orientation control of individual objects, even though it is trained with single and two object scenes. Further, it is able to generalize well to challenging unseen objects such as humans and prams. Interestingly, there was no water-based subject in the training dataset, yet our method can achieve precise orientation control for ship, yacht, and boat. These strong generalization capabilities of Compass Control can be attributed to effective attention constraining with CALL and diversity in the 3D assets used for training. The conditioning mechanism of Compass Control is generalized, and we present results for jointly controlling all three orientation angles, camera elevation, and object scale in Suppl. Sec.B & C. Baseline comparison. We compare our method to all three baselines on single-object scenes and additionally include multi-object scene comparison with LooseControl in Fig. 7 and Tab. 2. Cont-3D-words morphs the generated objects into sedan shape seen during training and generates washed-out backgrounds. This results in poor text alignment and lower percentage of intended object generation. ViewNeTI can generate better object shapes in given orientation; however, it overfits to the black backgrounds seen during training, leading to poor text alignment. This is primarily because ViewNeTI does not accept ControlNet augmentations in its original form as it is designed for novel view synthesis. LooseControl generates realistic singlesubject scenes following the given text prompt. However, in some cases, the object orientation is not followed (e.g., sofa, teddy). For multi-object generation (Fig. 7b)), LooseControl either misses the object during generation (horse in the second column and cars in the last column) or distorts the object shapes. LooseControl distorts the object to box like Figure 7. Qualitative Comparison. We compare our method against three baselines. Cont-3D-words [14] does not generate the intended object whereas View-NeTI [7] generates objects in plain backgrounds. LooseControl [4] generates realistic compositions but does not follow the input orientation well. In contrast, our method aligns with the input text prompt and follows the input orientation, while generating realistic scenes. appearance (columns 3 and 4) as it learns LoRA over the original depth-conditioned ControlNet, which follows the depth input closely, resulting in an inferior object generation score. Further, though we adjust for 180-degree flip in computing Angular error for LooseControl, our method achieves significantly lower Angular error, demonstrating strong orientation adherence. 4.3. User study We conducted user study with 57 participants to compare all methods on text alignment, object quality, and orientation consistency. Users rated 90 image pairs for singleSingle object Text Align. % Obj. Generated Angular Err. ViewNeTI [7] Cont-3D-words [14] LooseControl [4] Ours 22.12 29.88 31.60 32.98 0.920 0.732 0.656 0. 0.596 0.509 0.385 0.198 Multiple object Text Align. % Obj. Generated Angular Err. LooseControl [4] Ours 31.73 33.93 0.778 0. 0.372 0.215 Table 2. Quantiative comparison. We compute Text Alignment, using CLIP, % of correct subject generation and Angular error between the predicted and input orientations. Figure 8. User study. We compare all methods on the three image metrics. Each bar indicates the fraction of people that preferred our result (gray) vs the baseline (other color). object scenes and 30 for multi-object scenes, choosing the better image in each pair, sampled from our method and In total, we obtained 5130 ratings for singlebaselines. object and 1710 for two-object scenes. Results  (Fig. 8)  show users preferred our method overall, with LooseControl scoring well in text alignment for single objects but falling short across all metrics in multi-object scenes. 4.4. Personalization We present personalization results in Fig. 9. With only 10 unposed images of an object, our method can generate the object with precise orientation control in various contexts. Furthermore, we can jointly optimize Dreambooth [52] LoRA weights for two objects (e.g., chair and teddy bear), enabling multi-object personalization with object-centric orientation control. We compare our method with CD-360 [33] and achieve comparable performance. Unlike CD-360 [33], which requires 100 object images with camera pose, we require only few unposed images, making it more convenient and user-friendly. 4.5. Ablations We present the results of the ablation study in Fig. 10 on generated scenes with 1, 3, and 5 objects. We focus on three key design choices and generate scenes with variable number of objects: Staged training: Training Compass Control in single stage results in poor adherence to the bounding boxes. This especially affects complex multi-object layouts, where some objects tend to leak outside their box and suppress the generation of neighboring objects. This results in generatFigure 9. Personalization. Given few ( 10) unposed images of an object, our method can personalize the diffusion models and allow for orientation control of the new object. Notably, our method can also generate scenes with two personalized objects with precise orientation control. Additionally, we compare our method with CustomDiffusion-360 [33] that uses 100 posed images. ing lesser number of objects in the scene. Figure 10. Ablation studies. We show the impact of several design choices of our approach. Refer Sec. 4.5 for details. CALL is crucial for accurate orientation control. Without it the compass token attends to irrelevant image regions, resulting in poor orientation control. Further, without CALL the object tokens entangle with each other during generation (a known issue in T2I models [11]). This results in generating lesser number of objects in the scene. Augmentations: Without the ControlNet augmentations, the model overfits the training backgrounds, resulting in black backgrounds. Hence, ControlNet augmentation is necessary to generate objects in diverse contexts. 5. Conclusion and Discussion Limitations. Our method struggles to control the orientation of objects that are occluded or have significant overlap. In these scenarios, the model either fails to generate one of the objects or mixes the attributes between objects  (Fig. 11)  . Further, our single-angle orientation representation is too simplistic for modeling complex non-rigid objects such In this as humans. work, we focus on presenting generalized framework to condition text-to-image models on 3D controls that can be adapted easily for other representations. Figure 11. Failure Cases Conclusion. In this work, we propose method to condition pre-trained text-to-image models with 3D object orientation while preserving its rich image-generation capabilities. We train the conditioning module on small synthetic dataset via staged training and involving attention regularization. These modifications enable strong generalization of the model, allowing for precise orientation control for unseen object categories and individual objects in multiobject scene. Further, it can be seamlessly integrated with personalization methods to achieve orientation control of personalized objects. This work is testament that textto-image diffusion models innately have some form of 3D understanding, and interesting 3D controls can be obtained with appropriate conditionings. Acknowledgements. We thank Srinjay Sarkar, Abhijnya Bhat, and Tejan Karmali for thoroughly reviewing the manuscript. This work is supported by Meesho and PMRF by the Government of India."
        },
        {
            "title": "References",
            "content": "[1] Omri Avrahami, Rinon Gal, Gal Chechik, Ohad Fried, Dani Lischinski, Arash Vahdat, and Weili Nie. Diffuhaul: training-free method for object dragging in images. arXiv preprint arXiv:2406.01594, 2024. 3 [2] Omer Bar-Tal, Lior Yariv, Yaron Lipman, and Tali Dekel. Multidiffusion: Fusing diffusion paths for controlled image generation. 2023. 3 [3] Miguel Angel Bautista, Pengsheng Guo, Samira Abnar, Walter Talbott, Alexander Toshev, Zhuoyuan Chen, Laurent Dinh, Shuangfei Zhai, Hanlin Goh, Daniel Ulbricht, et al. Gaudi: neural architect for immersive 3d scene generation. Advances in Neural Information Processing Systems, 35:2510225116, 2022. 3 [4] Shariq Farooq Bhat, Niloy Mitra, and Peter Wonka. Loosecontrol: Lifting controlnet for generalized depth conditioning. In ACM SIGGRAPH 2024 Conference Papers, pages 111, 2024. 2, 3, 7, 8, 1, 4, 6 [5] Manuel Brack, Felix Friedrich, Katharia Kornmeier, Linoy Tsaban, Patrick Schramowski, Kristian Kersting, and Apolinario Passos. Ledits++: Limitless image editing using text-to-image models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 88618870, 2024. 2 [6] G. Bradski. The OpenCV Library. Dr. Dobbs Journal of Software Tools, 2000. 4 [7] James Burgess, Kuan-Chieh Wang, and Serena Yeung. Viewinversion: Unleashing novel view synthearXiv preprint point sis with pretrained 2d diffusion models. arXiv:2309.07986, 2023. 2, 3, 6, 7, 8, 1 textual [8] John Canny. computational approach to edge detection. IEEE Transactions on pattern analysis and machine intelligence, (6):679698, 1986. 3, 4 [9] Mingdeng Cao, Xintao Wang, Zhongang Qi, Ying Shan, Xiaohu Qie, and Yinqiang Zheng. Masactrl: Tuning-free mutual self-attention control for consistent image synthesis and editing. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2256022570, 2023. 2, [10] Chatgpt. Chatgpt. In https://chat.openai.com/chat, 2022. 5 [11] Hila Chefer, Yuval Alaluf, Yael Vinker, Lior Wolf, and Daniel Cohen-Or. Attend-and-excite: Attention-based semantic guidance for text-to-image diffusion models. ACM Transactions on Graphics (TOG), 42(4):110, 2023. 3, 5, 9 [12] Xi Chen, Lianghua Huang, Yu Liu, Yujun Shen, Deli Zhao, and Hengshuang Zhao. Anydoor: Zero-shot object-level imIn Proceedings of the IEEE/CVF Conage customization. ference on Computer Vision and Pattern Recognition, pages 65936602, 2024. 2 [13] Yiwen Chen, Zilong Chen, Chi Zhang, Feng Wang, Xiaofeng Yang, Yikai Wang, Zhongang Cai, Lei Yang, Huaping Liu, and Guosheng Lin. Gaussianeditor: Swift and controllable 3d editing with gaussian splatting. corr abs/2311.14521 (2023), 2023. 3 [14] Ta-Ying Cheng, Matheus Gadelha, Thibault Groueix, Matthew Fisher, Radomir Mech, Andrew Markham, and Niki Trigoni. Learning continuous 3d words for text-toIn Proceedings of the IEEE/CVF Conimage generation. ference on Computer Vision and Pattern Recognition, pages 67536762, 2024. 2, 3, 6, 7, 8, 1 [15] Blender Online Community. Blender - 3D modelling and rendering package. Blender Foundation, Stichting Blender Foundation, Amsterdam, 2018. 3, 4, 8 [16] Omer Dahary, Or Patashnik, Kfir Aberman, and Daniel Cohen-Or. Be yourself: Bounded attention for multi-subject text-to-image generation. arXiv preprint arXiv:2403.16990, 2(5), 2024. ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 60076017, 2023. 3 [17] Ankit Dhiman, Manan Shah, Rishubh Parihar, Yash Bhalgat, Lokesh Boregowda, and Venkatesh Babu. Reflecting reality: Enabling diffusion models to produce faithful mirror reflections. arXiv preprint arXiv:2409.14677, 2024. 3 [18] Xiaodan Du, Nicholas Kolkin, Greg Shakhnarovich, and Anand Bhattad. Generative models: What do they know? arXiv preprint do they know things? arXiv:2311.17137, 2023. lets find out! [19] Mohamed El Banani, Amit Raj, Kevis-Kokitsi Maninis, Abhishek Kar, Yuanzhen Li, Michael Rubinstein, Deqing Sun, Leonidas Guibas, Justin Johnson, and Varun Jampani. Probing the 3d awareness of visual foundation models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2179521806, 2024. 3 [20] Dave Epstein, Allan Jabri, Ben Poole, Alexei Efros, and Aleksander Holynski. Diffusion self-guidance for controllable image generation. Advances in Neural Information Processing Systems, 36:1622216239, 2023. 3, 4 [21] Songwei Ge, Taesung Park, Jun-Yan Zhu, and Jia-Bin Huang. Expressive text-to-image generation with rich text. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 75457556, 2023. 3 [22] Daniel Geng and Andrew Owens. Motion guidance: Diffusion-based image editing with differentiable motion estimators. In The Twelfth International Conference on Learning Representations. [23] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770778, 2016. 5 [24] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt imarXiv preprint age editing with cross attention control. arXiv:2208.01626, 2022. 2, 3 [25] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt imarXiv preprint age editing with cross attention control. arXiv:2208.01626, 2022. 4 [26] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. 2, 4, 5, 7 [27] Allan Jabri, Sjoerd van Steenkiste, Emiel Hoogeboom, Mehdi SM Sajjadi, and Thomas Kipf. Dorsal: Diffusion for object-centric representations of scenes et. al. arXiv preprint arXiv:2306.08068, 2023. 3 [28] Kunal Kathare, Ankit Dhiman, Vikas Gowda, Siddharth Aravindan, Shubham Monga, Basavaraja Shanthappa Vandrotti, and Lokesh Boregowda. Instructive3d: Editing large reconstruction models with text instructions. In Proceedings of the Winter Conference on Applications of Computer Vision (WACV), pages 32463256, 2025. [30] Hyunsu Kim, Gayoung Lee, Yunjey Choi, Jin-Hwa Kim, and Jun-Yan Zhu. 3d-aware blending with generative nerfs. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2290622918, 2023. 3 [31] Yunji Kim, Jiyoung Lee, Jin-Hwa Kim, Jung-Woo Ha, and Jun-Yan Zhu. Dense text-to-image generation with attention modulation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 77017711, 2023. 3 [32] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-concept customization of text-to-image diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19311941, 2023. 3 [33] Nupur Kumari, Grace Su, Richard Zhang, Taesung Park, Eli Shechtman, and Jun-Yan Zhu. Customizing text-to-image arXiv preprint diffusion with camera viewpoint control. arXiv:2404.12333, 2024. 2, 3, 8 [34] Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jianwei Yang, Jianfeng Gao, Chunyuan Li, and Yong Jae Lee. Gligen: Open-set grounded text-to-image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2251122521, 2023. 3 [35] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution text-to-3d content creation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 300309, 2023. 3 [36] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. arXiv preprint arXiv:2303.05499, 2023. 6, 5 [37] Grace Luo, Trevor Darrell, Oliver Wang, Dan Goldman, and Aleksander Holynski. Readout guidance: Learning control from diffusion features. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 82178227, 2024. [38] Guan Luo, Tian-Xing Xu, Ying-Tian Liu, Xiao-Xiong Fan, Fang-Lue Zhang, and Song-Hai Zhang. 3d gaussian editing with single image. In Proceedings of the 32nd ACM International Conference on Multimedia, pages 66276636, 2024. 3 [39] Oscar Michel, Anand Bhattad, Eli VanderBilt, Ranjay Krishna, Aniruddha Kembhavi, and Tanmay Gupta. Object 3dit: Language-guided 3d-aware image editing. Advances in Neural Information Processing Systems, 36, 2024. 3 [40] Thu Nguyen-Phuoc, Chuan Li, Lucas Theis, Christian Richardt, and Yong-Liang Yang. Hologan: Unsupervised learning of 3d representations from natural images. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 75887597, 2019. 3 [29] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar Mosseri, and Michal Irani. Imagic: Text-based real image editing with diffusion models. In Pro- [41] Michael Niemeyer and Andreas Geiger. Giraffe: Representing scenes as compositional generative neural feature fields. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1145311464, 2021. 3 [42] Karran Pandey, Paul Guerrero, Matheus Gadelha, Yannick Hold-Geoffroy, Karan Singh, and Niloy Mitra. Diffusion handles enabling 3d edits for diffusion models by lifting activations to 3d. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7695 7704, 2024. [43] Rishubh Parihar, Abhijnya Bhat, Abhipsa Basu, Saswat Mallick, Jogendra Nath Kundu, and Venkatesh Babu. Balancing act: distribution-guided debiasing in diffusion In Proceedings of the IEEE/CVF conference on models. computer vision and pattern recognition, pages 66686678, 2024. 3 [44] Rishubh Parihar, VS Sachidanand, Sabariswaran Mani, Tejan Karmali, and Venkatesh Babu. Precisecontrol: Enhancing text-to-image diffusion models with fine-grained attribute control. In European Conference on Computer Vision, pages 469487. Springer, 2024. 3 [45] Or Patashnik, Daniel Garibi, Idan Azuri, Hadar AverbuchElor, and Daniel Cohen-Or. Localizing object-level shape variations with text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2305123061, 2023. 2, 3 [46] Or Patashnik, Rinon Gal, Daniel Cohen-Or, Jun-Yan Zhu, and Fernando De la Torre. Consolidating attention features for multi-view image editing. arXiv preprint arXiv:2402.14792, 2024. 3 [47] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis, 2023. 6 [48] Ben Poole, Ajay Jain, Jonathan Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. arXiv preprint arXiv:2209.14988, 2022. [49] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1 (2):3, 2022. 3 [50] Harsh Rangwani, Aishwarya Agarwal, Kuldeep Kulkarni, Crafting arXiv preprint Venkatesh Babu, and Srikrishna Karanam. parts for expressive object composition. arXiv:2406.10197, 2024. 3 [51] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 3, 6, 5, 7, 8 [52] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven In Proceedings of the IEEE/CVF conference generation. on computer vision and pattern recognition, pages 22500 22510, 2023. 3, 5, 8 [53] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems, 35:3647936494, 2022. 3 [54] Rahul Sajnani, Jeroen Vanbaar, Jie Min, Kapil Katyal, and Srinath Sridhar. Geodiffuser: Geometry-based image editing with diffusion models. arXiv preprint arXiv:2404.14403, 2024. 3 [55] Yizhi Song, Zhifei Zhang, Zhe Lin, Scott Cohen, Brian Price, Jianming Zhang, Soo Ye Kim, and Daniel Aliaga. ObjectIn Prostitch: Object compositing with diffusion model. ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1831018319, 2023. 2 [56] Alexander Vilesov, Pradyumna Chari, and Achuta Kadambi. Cg3d: Compositional generation for text-to-3d via gaussian splatting. arXiv preprint arXiv:2311.17907, 2023. 3 [57] Andrey Voynov, Kfir Aberman, and Daniel Cohen-Or. Sketch-guided text-to-image diffusion models. In ACM SIGGRAPH 2023 Conference Proceedings, pages 111, 2023. 2, 3 [58] Andrey Voynov, Kfir Aberman, and Daniel Cohen-Or. Sketch-guided text-to-image diffusion models. In ACM SIGGRAPH 2023 Conference Proceedings, pages 111, 2023. 3 [59] Andrey Voynov, Qinghao Chu, Daniel Cohen-Or, and Kfir Aberman. p+: Extended textual conditioning in text-toimage generation. arXiv preprint arXiv:2303.09522, 2023. 3 [60] Qian Wang, Yiqun Wang, Michael Birsak, and Peter Wonka. Blobgan-3d: spatially-disentangled 3d-aware generative model for indoor scenes. arXiv preprint arXiv:2303.14706, 2023. [61] Ruicheng Wang, Jianfeng Xiang, Jiaolong Yang, and Xin Tong. Diffusion models are geometry critics: Single image 3d editing using pre-trained diffusion priors. arXiv preprint arXiv:2403.11503, 2024. 3 [62] Zhaoning Wang, Ming Li, and Chen Chen. Luciddreaming: Controllable object-centric 3d generation. arXiv preprint arXiv:2312.00588, 2023. 3 [63] Ziyi Wu, Yulia Rubanova, Rishabh Kabra, Drew Hudson, Igor Gilitschenski, Yusuf Aytar, Sjoerd van Steenkiste, Kelsey Allen, and Thomas Kipf. Neural assets: 3d-aware multi-object scene synthesis with image diffusion models. arXiv preprint arXiv:2406.09292, 2024. 3, 4, 6 [64] Yang Xue, Yuheng Li, Krishna Kumar Singh, and Yong Jae Lee. Giraffe hd: high-resolution 3d-aware generative model. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1844018449, 2022. 3 [65] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ipadapter: Text compatible image prompt adapter for text-toimage diffusion models. arXiv preprint arXiv:2308.06721, 2023. 2, 3 [66] Hong-Xing Yu, Haoyi Duan, Charles Herrmann, William Interactive 3d arXiv preprint Freeman, and Jiajun Wu. Wonderworld: scene generation from single image. arXiv:2406.09394, 2024. 3 [67] Guanqi Zhan, Chuanxia Zheng, Weidi Xie, and Andrew Zisserman. What does stable diffusion know about the 3d scene? arXiv preprint arXiv:2310.06836, 2023. 3 [68] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 38363847, 2023. 2, 3, 4 [69] Yuxin Zhang, Weiming Dong, Fan Tang, Nisha Huang, Haibin Huang, Chongyang Ma, Tong-Yee Lee, Oliver Deussen, and Changsheng Xu. Prospect: Prompt spectrum for attribute-aware personalization of diffusion models. ACM Transactions on Graphics (TOG), 42(6):114, 2023. 3 [70] Shijie Zhou, Zhiwen Fan, Dejia Xu, Haoran Chang, Pradyumna Chari, Tejas Bharadwaj, Suya You, Zhangyang Wang, and Achuta Kadambi. Dreamscene360: Unconstrained text-to-3d scene generation with panoramic gaussian splatting. In European Conference on Computer Vision, pages 324342. Springer, 2025. 3 Compass Control: Multi Object Orientation Control for Text-to-Image Generation"
        },
        {
            "title": "Contents",
            "content": "A. Project page B. Controlling 3D orientation C. Additional Control D. Generalization to StableDiffusion-XL E. Diverse poses for non-rigid objects F. Robustness to the 2D bounding boxes G. Discussion with SoTA object-centric works. H. Synthetic data generation . Orientation Regressor J. Baseline details . J.1 . ViewNeTI [7] . J.2 . Continous 3D Words [14] . J.3 . LooseControl [4] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . K. Additional Results K.1. Comparisons . . . . . . . . . . . . . . . . . L. Implementation Details L.1. Method details . . L.2. Evaluation dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.1 (main paper) to render the dataset. We present results for controlling all the 3 orientation angles in Fig. 13 and 14. In Fig. 13, we present rotation along all three axes for fighter jet aircraft in three separate rows. Observe that, our method can precisely control all the object orientations along all the three axis. In Fig. 14, we show the generalization of our trained model in controlling the orientation of variety of objects. Notably, our model is not trained on birds or rockets. Still, it can generate consistent orientation-conditioned scenes following the text prompts. Note, that the compass shown in the figure is just for visualization purposes (can have an error of few degrees). C. Additional Control Continuous control for camera elevation. Our proposed conditioning mechanism is generalized and can be adapted to achieve continuous camera elevation control in Fig. 12. We generated dataset with camera elevation variations and conditioned the denoising UNet on elevation angle. Control for object scale. We can also precisely control the size of individual objects with additional conditioning on the object scale, as shown in Fig. 12. Specifically, we condition the diffusion model with the length of the diagonal of tight 2D bounding box. 1 1 1 3 3 4 4 5 6 6 6 8 8 8 8 8 A. Project page Check the project page for interactive visualizations of 3D orientation control. B. Controlling 3D orientation The main text primarily focused on an orientation control for single angle. However, our method is not limited to single orientation control, and we present an experiment for controlling all three orientation angles in single model. Specifically, we updated the pose injection network to take 3 orientation angles as input to predict the pose token. We trained the model on flying objects - airplanes and helicoptersas rotation along all three axes is plausible for these objects. Specifically, we used six 3D assets from the web for these categories and followed the procedure in sec. Figure 12. Additional Controls: (Top) Conditioning with camera elevation angle. (Bottom) Conditioning on object scale. D. Generalization to StableDiffusion-XL We have presented all the results on StableDiffusion-2.1 in the main paper. Our method also generalizes well to larger Figure 13. Conditioning on all three orientation angles for single object. Figure 14. Conditioning on all three orientation angles StableDiffusion-XL backbone model shown in Fig. 15. The results demonstrate improved image quality with accurate orientation control of the generated objects. Figure 15. Compass Control on StableDiffusion-XL. E. Diverse poses for non-rigid objects We build our dataset with only few synthetic objects in their fixed canonical pose to generate our training data. This makes the model prone to overfitting on these poses for the non-rigid objects in the dataset - dog, horse, and lion. For instance, during inference, the model can generate only standing dogs in the given orientation. We generate augmentations with realistic pose variations in the training data to mitigate this. Specifically, we randomly mask some regions from the Canny Edge map and pass it to the ControlNet (Fig. 16a). This allows ControlNet to freely generate any plausible pose within the masked region. When trained with resulting augmentations, our method can generate diverse pose variations of non-rigid training objects while following the precise orientation as shown in Fig. 16b). F. Robustness to the 2D bounding boxes Coarse bounding boxes. We analyze the robustness of required 2D object bounding boxes during the inference. First, we analyze the effect of the coarseness of the bounding box on the generated scenes in Fig. 17. Our model does not generate objects that tightly occupy the provided bounding box. This is convenient for the user, as they dont have to provide an exact 2D bounding box. We present results for different bounding box sizes while keeping the center fixed. H. Synthetic data generation We render scenes with 3D assets in Blender [15] environment for our dataset. Specifically, we place an opaque floor on the plane and place camera tilted slightly towards the ground at fixed position. The scene is lighted using 3 point lights of random intensity, placed at random locations. Once the environment is ready, we place the 3D assets at random locations and orientations and render the scene. For each rendered image we store the identity of the 3D assets in it, their respective orientations and 2D bounding boxes. We constrain the locations and orientations so that the object completely lies within the rendered image. Additionally, for two object scenes, we ensure that their 2D bounding boxes do not overlap. In all, we have 1000 oneobject scenes and 7900 two-object scenes. Some samples from the rendered images can be found in Fig. 19. However, training on this dataset alone leads to over-fitting to the plain backgrounds, as we have presented in the ablative experiments in the main text. Therefore, to generate the objects in diverse contexts, we augment the rendered scenes using Canny ControlNet [68]. Specifically, given rendered scene, we extract its Canny map using OpenCV [6], with the low and high thresholds set to 100 and 200 respectively. We use the following prompts for the augmentations: 1. photo of subject in snowy forest, with gentle snowfall and snow-covered trees 2. photo of subject in vast desert with towering sand dunes and clear blue sky 3. photo of subject in medieval castle courtyard with ancient stone walls and archways 4. photo of subject in sunflower field under clear blue sky 5. photo of subject in dense rainforest, with sunlight streaming through the canopy 6. photo of subject in serene Japanese garden, surrounded by cherry blossoms 7. photo of subject on rocky cliff overlooking vast ocean 8. photo of subject by riverside with wildflowers blooming nearby 9. photo of subject at rivers edge with stones scattered around 10. photo of subject in front of the Eiffel Tower at sunset 11. photo of subject in vibrant autumn forest, with orange and red leaves carpeting the ground 12. photo of subject in vast open plain, with golden grasses swaying in the wind and distant mountains on the horizon under wide, clear sky 13. photo of subject on cobblestone street in quaint European village, with flower-filled balconies and historic buildings 14. photo of subject in canyon with towering red rock Figure 16. Pose variations for non-rigid objects The model is robust to size changes and generates realistic scene compositions. The objects fall inside the box but they dont tightly fit the box. This provides more flexibility to the base generative model in generating more realistic scenes with relaxed constraints than conditioning on precise bounding boxes. Spawning random boxes. In another experiment, we randomly spawn non-overlapping boxes, eliminating the user requirement to provide 2D boxes. The results are present in Fig. 17. Our method generates realistic compositions for these random layouts, with precise orientation control. The proposed design of using loose bounding boxes during training, enables this, as the objects can adjust their size within the box region to make coherent scenes. Overlapping boxes. We present an ablation with the amount of overlap of 2D object boxes in Fig. 18 during inference. Our method can handle the overlap between 2D boxes upto good extent. On increasing the overlap the models performance gracefully degrades in the pose control as the overlapping region is controlled by both the pose tokens (jeep in 4th example). With large overlap in the bounding boxes, the model fails to generate both objects, and this is one of the limitations of our proposed approach, which is based on attention regularization. However, this limitation is common across all the bounding box conditioned or guided generative models. G. Discussion with SoTA object-centric works. We compare the framework of our approach with recent works on object-centric 3D control in generation and editing with diffusion models. Particularly, we contrast our method with Neural Assets [63] and LooseControl [4], as these two are the closest method to ours. We present comparison with both these methods at an approach level in Tab. 3. Figure 17. Robustness of 2D bounding boxes. Our method generates realistic scene compositions with different 2D bounding box sizes. Allowing for loose bounding box during training provides this flexibility to the model to generate realistic scenes while coarsely following the input 2D box. Further, random non-overlapping boxes can also be spawned during inference without any degradation in quality. This robustness to the actual bounding box shape, reduces the burden on the user and is enabled by the loose bounding box used during training. Figure 18. Overlapping bounding boxes. Our method can handle overlap between the two input bounding boxes up to good extent. However, with large overlap, the model struggles to generate accurate orientations (jeep in the fourth example), due to the mixing of pose tokens. formations, and scattered desert plants growing in the rocky terrain We run this augmentation pipeline on all the rendered images, and do manual filtering to remove the inconsistent generations. In all, we have 771 single-object augmentations and 5239 two-object augmentations. I. Orientation Regressor We train neural network model to predict the orientation angle of an object in the generated image. We use pretrained ResNet-18 [23] as the feature extractor and mlp head consisting of two hidden layers of 128 neurons, each with ReLU activations. Finally, we predict single orientation angle θ along the up-axis (details in the main text - sec.3.1). We call this model orientation regressor and train with dataset of 35K images generated by rendering 30 synthetic 3D assets of the test object categories followed by their canny ControlNet augmentations. This data is highly diverse, containing various backgrounds and object appearances, enabling the learning of an accurate orientation regressor. We train with batch size of 128, learning rate of 5e 5 for 95 epochs with Adam optimizer. On an unseen test set of 8K images from the same distribution, the trained model achieves mean angular error of 0.125. Further, we present the results for evaluation on completely unseen dataset, generated by Stable Diffusion [51], containing the test objects in Fig. 20. We can observe that the trained orientation regressor predicts accurate orientations, and hence, it is good estimator for evaluating pose consistency. In the case of multi-object scenes, we crop out the objects using Grounding DINO [36] and pass them to the orientation regressor. LooseControl [4] Model type Generation Training data Real images (w 3D boxes) 3D object boxes Input during inference Novel categories Neural Assets [63] Editing Real videos (w 3D boxes) 3D object boxes Ours Generation Synthetic images (w Orientation + 2D boxes) Orientation + 2D object boxes Yes No Yes Input Representation Explicit 3D (Depth) Implicit (List of bbox) Implicit List of orientations Personalization No Yes Yes Table 3. Comparison with state-of-the-art approaches for object-centric control in the generation process. Figure 19. Samples from data generation process J. Baseline details We provide implementation details for the baselines discussed in the paper. J.1. ViewNeTI [7] ViewNeTI trains small MLP to project the 3D camera pose to 3D view token. This token, along with the text prompt, is used to condition the text-to-image model. In the basic form, it is trained on single scene with multi-view images and 3D camera poses. Once trained, the model can generate novel views for the trained scene. However, in an extended version, it is trained with multiple scenes to learn generalizable view token. This token is then used for view control in text-to-image generation. For comparison, we use this version and train on our synthetic dataset of rendered multiview scenes. Specifically, instead of conditioning on 3D camera pose, we condition on orientation angle θ and predict the view token. We train the model for 60K iterations on 1000 multi-view images of 10 assets. Note that because this model only supports global view control, we train and evaluate it on only single object scenes for orientation control. J.2. Continous 3D Words [14] In this approach, text-to-image diffusion model is conditioned on continuous 3D tokens to control 3D attributes such as lighting and object pose. They learn generalizable 3D word in the text embedding space of the T2I model for each attribute, which is used along with the text prompt for conditioning. To learn the 3D word token, they use renderings of single object and generate its augmentations with depth-conditioned ControlNet. However, it is essential that the 3D word token is disentangled from the object used for training. For this, they follow staged training procedure: first learn the objects appearance (stage 1), and then learn the 3D attribute (stage 2). Following this, we train this Figure 20. Predictions of the trained orientation regressor on unseen samples generated from Stable Diffusion [51]. The model can predict the orientations accurately for the diverse unseen data and acts as good critic to evaluate orientation consistency in generated images. model single 3D asset, sedan. We train for 5000 iterations in stage 1 and 15000 iterations in stage 2 (same as the original model). However, the trained model poorly generalizes to new objects as it is trained on single object mesh (Fig.7 in the main text). Here we present variant of this model, which is trained on multiple 3D assets instead of just one (as proposed in their original paper). We use the same rendered images dataset as ours, and augment it using their proposed augmentation strategy. Notably, this dataset has diverse layouts and objects placed at random locations in the scene, making the learning process challenging. Since this model only allows for global control, we train and evaluate it on single object scenes only. We perform 30000 training iterations in the first stage to learn the object appearance, followed by 70000 iterations to learn the 3D word token. The comparison is presented in Fig. 21. Our method achieves superior performance as compared to this baseline. The baseline struggles in pose control due to high diversity in the scene layouts, highlighting the importance of our attention localization mechanism CALL. Further, our backgrounds are much richer, as we use canny-conditioned ControlNet augmentations, which leads to richer augmentations. J.3. LooseControl [4] LooseControl is conditioning framework on text-to-image diffusion models that allows for 3D scene layout control. The framework is built on depth-conditioned ControlNet model. However, instead of relying on accurate depth maps, which are often difficult to construct, LooseControl conditions the generation on coarse depth maps. Specifically, in this loose depth map, the scene boundaries are represented as planes, and the objects are represented by their loose 3D bounding boxes. LooseControl is implemented as LoRA [26] fine-tuning over depth-conditioned ControlNet model. This fine-tuning enables it to condition the generation using loose object depth maps also, against the In accurate depth maps required by original ControlNet. Figure 21. Comparsison with modified Continuous 3D Words [14] trained on multiple assets. Compass control generates more realistic outputs and follows the text prompt better than the Cont-3D-Words trained on multiple object datasets. our experiments, we generate the loose depth maps by placing 3D bounding boxes in Blender [15] environment, and rendering the depth from camera viewpoint. Specifically, we randomly sample objects locations and pose within the scene boundary and place 3D bounding box for each object. Notably, one can control the object orientation by rotating the corresponding 3D bounding box in the input. We define fixed template of 3D bounding box dimensions for each test object in the dataset. The obtained depth maps are used to condition the model. We used the publicly available checkpoint for LooseControl in our evaluation. As this method allows for multi-object control, we compare both single and multi-object scenes. However, in experiments, we observe that LooseControl struggles to generate multiobject scenes with precise pose control and often resorts to generating bounding box artifacts. This is primarily due to the strong depth conditioning prior in the base depth ControlNet model, which is trained to follow exact depth maps. K. Additional Results K.1. Comparisons We present additional baseline comparison results in Fig. 23. Our method follows the text prompts and generates objects following the input prompts L. Implementation Details L.1. Method details We use Stable Diffusion v2.1 [51] as our base T2I model and use LoRA rank 4 for fine-tuning its UNet. Our encoder model is lightweight MLP: three linear layers with ReLU. We train our model for 100K iterations with batch size of 4 with AdamW optimizer and fixed learning rate of 104. We train first stage for 30K iterations with only single object scenes and the next stage for 70K iterations with mix of single and two subject scenes. We use SD-Xl for generating augmentations due to its higher realism. We keep the bounding box padding λ = 1.2 for CALL. The training takes 24 hours on single A6000 GPU, thus highly efficient. L.2. Evaluation dataset We randomly sample 10 pose orientation in the range of (0,360 deg) for each prompt and object combination. We used the following set of prompts for evaluation, containing single and two subject. In each prompt subject is replaced with single subject (e.g., jeep) or two subjects (e.g., jeep and sedan). Notably these prompts are different that the one used to generate ControlNet augmentations, to accurately evaluate model generalization. For road objects 1. photo of subject in front of the Taj Mahal 2. photo of subject on the streets of Venice, with the sun setting in the background 3. photo of subject in front of the leaning tower of Pisa in Italy 4. photo of subject in modern city street surrounded by towering skyscrapers and neon lights 5. photo of subject in an ancient Greek temple ruin, with broken columns and weathered stone steps 6. photo of subject in field of dandelions, with snowy mountain peaks in the distance 7. photo of subject in rustic village with cobblestone streets and small houses 8. photo of subject on winding country road with green fields, trees, and distant mountains under sunny sky 9. photo of subject in front of serene waterfall with Figure 22. Additional comparison results with the baselines for single object and multi-object scenes. Figure 23. More qualitative results from our method, Compass Control. trees scattered around the region, and stones scattered in the water with mountains in the background 12. photo of subject on dirt path in dense forest with 10. photo of subject on sandy desert road with dunes sunbeams filtering through the trees and vast, open sky above 13. photo of subject on coastal road with cliffs over11. photo of subject on bridge overlooking river looking the ocean 14. photo of subject in front of historical castle with 10. photo of subject in well-equipped gym with high stone walls and flags flying in the breeze weights and fitness machines 15. photo of subject in front of an amusement park with 11. photo of subject in music studio with soundproof bright lights and ferris wheels in the background For water objects walls and musical instruments 12. photo of subject in sunlit greenhouse filled with tropical plants 13. photo of subject in childrens playroom with colorful toys and posters on the walls 14. photo of subject in an underground wine cellar with wooden barrels and dim lighting 15. photo of subject in cozy reading nook with soft armchair and small lamp 1. photo of subject on still waters under cloudy sky, mountains visible in the distant horizon 2. photo of subject floating on misty lake, surrounded by calm waters and serene, foggy atmosphere 3. photo of subject in the vast sea, with clear blue sky and few fluffy clouds 4. photo of subject in the middle of stormy ocean, with dark clouds and crashing waves 5. photo of subject in calm lake with lily pads and reeds growing near the shoreline 6. photo of subject on river running through dense jungle with vibrant green foliage 7. photo of subject in mountain lake surrounded by pine trees and snow-capped peaks 8. photo of subject floating in lagoon with tropical fish and coral visible beneath the water 9. photo of subject on frozen lake with snowy landscape surrounding it 10. photo of subject on serene river at dusk, with reflections of the sunset on the water 11. photo of subject in the middle of vast marshland with tall grasses and migratory birds flying overhead 12. photo of subject near small waterfall cascading into clear pool in rocky area 13. photo of subject on bay with large rock formations jutting out of the water 14. photo of subject in turquoise sea with gentle waves and distant islands on the horizon 15. photo of subject in narrow canal in an old European city, with historic buildings lining the waterway For indoor objects 1. photo of subject in modern living room setting with painted walls and glass windows 2. photo of subject in minimalist living room 3. photo of subject in cozy library with shelves filled with books and warm lighting 4. photo of subject in high-tech office with large windows and city view 5. photo of subject in an art studio with canvas paintings and art supplies scattered around 6. photo of subject in rustic kitchen with wooden cabinets and stone countertop 7. photo of subject in lavish living room with elegant decor and soft lighting 8. photo of subject in large dining hall with chandeliers and long tables 9. photo of subject in traditional Japanese tatami room with sliding paper doors"
        }
    ],
    "affiliations": [
        "IIIT Hyderabad",
        "IISc Bangalore"
    ]
}