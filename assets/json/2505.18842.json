{
    "paper_title": "Don't Look Only Once: Towards Multimodal Interactive Reasoning with Selective Visual Revisitation",
    "authors": [
        "Jiwan Chung",
        "Junhyeok Kim",
        "Siyeol Kim",
        "Jaeyoung Lee",
        "Min Soo Kim",
        "Youngjae Yu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present v1, a lightweight extension to Multimodal Large Language Models (MLLMs) that enables selective visual revisitation during inference. While current MLLMs typically consume visual input only once and reason purely over internal memory, v1 introduces a simple point-and-copy mechanism that allows the model to dynamically retrieve relevant image regions throughout the reasoning process. This mechanism augments existing architectures with minimal modifications, enabling contextual access to visual tokens based on the model's evolving hypotheses. To train this capability, we construct v1g, a dataset of 300K multimodal reasoning traces with interleaved visual grounding annotations. Experiments on three multimodal mathematical reasoning benchmarks -- MathVista, MathVision, and MathVerse -- demonstrate that v1 consistently improves performance over comparable baselines, particularly on tasks requiring fine-grained visual reference and multi-step reasoning. Our results suggest that dynamic visual access is a promising direction for enhancing grounded multimodal reasoning. Code, models, and data will be released to support future research."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 2 ] . [ 1 2 4 8 8 1 . 5 0 5 2 : r Dont Look Only Once: Towards Multimodal Interactive Reasoning with Selective Visual Revisitation Junhyeok Kim Jiwan Chung Siyeol Kim Jaeyoung Lee Minsoo Kim Youngjae Yu Yonsei University Seoul National University jiwan.chung.research@gmail.com"
        },
        {
            "title": "Abstract",
            "content": "We present v1, lightweight extension to Multimodal Large Language Models (MLLMs) that enables selective visual revisitation during inference. While current MLLMs typically consume visual input only once and reason purely over internal memory, v1 introduces simple point-and-copy mechanism that allows the model to dynamically retrieve relevant image regions throughout the reasoning process. This mechanism augments existing architectures with minimal modifications, enabling contextual access to visual tokens based on the models evolving hypotheses. To train this capability, we construct v1g, dataset of 300K multimodal reasoning traces with interleaved visual grounding annotations. Experiments on three multimodal mathematical reasoning benchmarksMathVista, MathVision, and MathVersedemonstrate that v1 consistently improves performance over comparable baselines, particularly on tasks requiring fine-grained visual reference and multi-step reasoning. Our results suggest that dynamic visual access is promising direction for enhancing grounded multimodal reasoning. Code, models, and data will be released to support future research."
        },
        {
            "title": "Introduction",
            "content": "When thinking with images, one does not simply glance once. Imagine student solving geometry problem involving triangle with an inscribed circle: they do not arrive at the solution after single look at the diagram, but instead revisit it repeatedlychecking angle bisectors, identifying points of tangency, and adjusting their reasoning as they consider properties like symmetry or congruence. Likewise, humans frequently return to an image while reasoninguncovering new details, refining previous inferences, and updating their contextual interpretation. Findings from cognitive science [1 4] support this intuition, showing that humans heavily rely on interaction with visual information during reasoning, often revisiting the images or externalizing thoughts through sketching. The recent improvement of reasoning capabilities in Large Language Models (LLMs) [57] has led to wide interest in extending these capabilities to Multimodal Large Language Models (MLLMs) [815]. However, multimodal reasoning presents unique challenges beyond language-only reasoning, as it requires models to integrate and process information across different modalities throughout the reasoning process. Current approaches often treat visual information as static input rather than as dynamic elements that can be iteratively examined and reinterpreted during complex reasoning chains. This limitation constrains MLLMs ability to mimic the recursive visual analysis patterns observed in human cognition. Motivated by this gap, we ask: How can we effectively enable models to revisit images during reasoning? Equal contribution. Preprint. Under review. Figure 1: Standard text-based reasoning model vs. v1 during inference. Text-based reasoning models consume the visual context only once at the beginning and rely solely on internal memory thereafter. In contrast, v1 enables the model to revisit the visual input by dynamically pointing to and copying relevant image regions throughout the reasoning process. Revisiting specific regions of an image during reasoning requires mechanism for referential access. We propose simple yet effective solution: allow the MLLM to point to region of the visual input and copy the relevant image tokens. To this end, we implement v1, lightweight and modular extension that equips MLLMs with point-and-copy mechanism for proactive visual revisitation. Specifically, we augment the model with an additional pointing head that outputs probability distribution over the input image token positions, alongside the standard vocabulary logits. When an image token is selected, its embedding is copied and injected as the next-step input embedding, allowing the model to dynamically select and reconsume visual information during generation. Our pointing-based approach avoids reliance on image generation, making it readily compatible with popular MLLM architectures [16, 8, 10] that operate on continuous image embeddings. Unlike methods that attempt to generate new image tokenswhich are often computationally intensive and prone to instabilityour method simply reuses input image embeddings by pointing and copying, enabling efficient and stable visual revisitation. The only additional parameters are lightweight linear heads, resulting in minimal computational overhead. Furthermore, we show that the pointing capability can be effectively learned through standard finetuning without large-scale pretraining. To train v1, we construct v1g, dataset of 300K multimodal reasoning paths with interleaved grounding annotations, where each reasoning step is explicitly linked to corresponding image region. The construction pipeline comprises three stages: (1) oversampling diverse reasoning traces from an MLLM, (2) extracting visual queries and retrieval steps from the traces using an LLM-guided decomposition process, and (3) grounding each visual reference by associating it with bounding box in the input image. The pipeline is fully automated, leveraging the generative and interpretive capabilities of LLMs to produce high-quality, grounded reasoning trajectories at scale. We evaluate v1 on three established multimodal mathematical reasoning benchmarks: MathVista [17], MathVision [18], and MathVerse [19], which test models ability to integrate visual context into symbolic reasoning chains. v1 demonstrates strong performance across all benchmarks, outperforming existing models of comparable scale and approaching the capabilities of much larger models, particularly on tasks requiring precise visual grounding and iterative reference to localized regions. These results suggest that enabling revisitation of visual input during inference can significantly enhance multimodal reasoning capabilities."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 Reasoning in Large Language Models Reasoning in text-only large language models. Recent advances in LLMs have focused on improving reasoning in structured domains such as mathematics and programming [57]. Techniques include inference-time scaling [2022], supervised fine-tuning [23], and reinforcement learning [24, 2 25], often applied in Chain-of-Thought (CoT) settings. While these approaches have advanced multi-step reasoning, challenges in generalization and grounding remain [2628]. Reasoning in multimodal large language models. Multimodal reasoning introduces challenges beyond text-only inference, requiring not only visual understanding but also integration of visual inputs into symbolic reasoning. Prior work has explored strategies such as converting visual content into descriptive text for downstream processing [16, 2931]. Building on the success of CoT prompting in LLMs, several models have extended this paradigm to the multimodal setting [12 14, 32], achieving strong results on multimodal reasoning benchmarks [17, 18]. Despite recent progress, effectively leveraging visual input throughout the reasoning process remains core challenge. Most existing methods encode the image once at the beginning and rely on static visual embeddings throughout inference [33], without explicit mechanisms for revisiting or updating visual focus. Some recent works [34, 35] explore more interactive use of visual content during reasoning, pointing toward models that better emulate human-like iterative visual engagement. 2.2 Implementing Visual Revisitation Humans reason with images by actively engaging with specific regions, often revisiting or sketching them to support problem-solving [14]. Likewise, MLLMs should support step-wise interaction with visual inputseither by referencing regions [36, 37] or generating intermediate visuals [38]. We briefly review prior approaches to this challenge. Coordinates. Some MLLMs are trained to output bounding boxes to refer to relevant image regions [36, 39]. While effective in constrained settings, this approach resembles \"call-by-key\" mechanismaccessing visual content via position. It depends on accurate detection and fails in cases where relevant visual cues are abstract or not spatially localized, limiting semantic flexibility. Image generation. Other methods [34, 38, 35] allow models to externalize reasoning by rendering intermediate visual states or generating new images. While expressive, these approaches are limited to programmatic rendering or require full generative pipelines, which add significant computational overhead. Furthermore, bridging discrete image tokens (e.g. VQ-VAE [40]) with continuous visionlanguage embeddings introduces representational mismatch, complicating integration. Pointing. We build on the Pointer-Generator Network (PGN) [41], originally developed for selective text copying, and extend it to the multimodal setting. Our method allows MLLMs to dynamically point to and reuse image embeddings during generation, enabling direct and interpretable visual revisitation without coordinate prediction or image synthesis."
        },
        {
            "title": "3 Visual Grounding Decays During Reasoning",
            "content": "To analyze how visual attention evolves during multimodal reasoning, we leverage RefCOCO [42], visual grounding benchmark in which each input includes an image and target region specified by bounding box. The task requires generating caption that uniquely identifies this region, providing natural signal for evaluating whether model attends to the correct part of the image. We conduct two preliminary analyses using the TVC-7B model [14] on the RefCOCO testA split. In both, we measure the attention weights between the most recently generated token and all image tokens, examining layers 2, 14, and 27 of the 28-layer transformer to capture early, middle, and late stages of the reasoning process. The first experiment tracks the total attention mass allocated to all image tokens at each generation step. Across all three layers, this attention consistently declines over time, suggesting that the model increasingly relies on its internal memory rather than the visual input as reasoning progresses. The second experiment evaluates how well the model attends to the task-relevant region. We compute the ratio between the mean attention over tokens within the bounding box and the mean attention over all image tokens (Figure 2b). Although layers 14 and 27 show brief rise in this ratio, by the midpoint of generation all layers converge to approximately 0.8indicating that attention to the salient region diminishes relative to the background. 3 (a) Cumulative attention across all visual tokens. (b) Attention ratio: salient regions vs. all visual tokens. Figure 2: Attention dynamics during reasoning. (a) illustrates gradual decrease in overall attention to the input image tokens, while (b) indicates that semantically important visual regions receive disproportionately low attention, suggesting inefficient grounding during reasoning. These results highlight limitation of current attention mechanisms: while image embeddings remain available throughout decoding, models lack an explicit mechanism to re-focus on critical visual regions. This motivates the need for reasoning architectures that support explicit visual revisitation during inference, preserving grounding and improving multi-step visual reasoning."
        },
        {
            "title": "4 Method",
            "content": "4.1 Preliminary: Pointing for Language Generation Formulation. We formulate conditional next-token prediction objective, as commonly adopted in modern multimodal large language models (MLLMs). Given sequence of continuous input representations (e.g. embedded text tokens or visual features) the model is trained to autoregressively predict the discrete next token xt conditioned on the input and previously generated tokens x<t: p(x1, . . . , xT c) = (cid:89) t=1 p(xt c, x1, . . . , xt1) The continuous input sequence may include heterogeneous mixture of modality-specific features, such as embedded discrete text tokens or continuous visual embeddings produced by image encoders (e.g. CLIP [43]). This general formulation covers wide range of multimodal architectures such as LLaVA [16] and Qwen-VL [9], which uses continuous input image representations. Pointing. For visually grounded reasoning, it is often necessary to refer back to specific region or token within the input sequence c, especially when that region corresponds to localized visual content. Rather than generating new description of visual entity, we may instead wish to point to its position within the input, thereby referencing it explicitly. The pointing mechanism we examine was first introduced by the pointer-generator network [41] in text summarization research. In the pointer-generator network, the input context sequence also consists of discrete tokens within the vocabulary space , unlike our setup. The model dynamically mixes two distributions at each decoding step t: (1) generation distribution over the vocabulary Pgen(xt), and (2) copy distribution Pptr(xt) over input tokens. The final output probability is given by gated mixture: p(xt c, x<t) = λ(xt c, x<t) Pgen(xt c, x<t) + (1 λ(xt c, x<t)) Pptr(xt c, x<t) where λ [0, 1] is learnable scalar gate that controls the trade-off between generating new token and copying one from the input. The pointer distribution is obtained via attention over the encoder representations: α(k) = exp(score(ht, ck)) exp(score(ht, ck)) (cid:80) , Pptr(xt = w) = (cid:88) α(k) k:wk=w where ht is the decoder hidden state at step t, wk the token at position k, and score denotes standard attention scoring function (e.g., dot-product or additive). We generalized the formulation beyond the original implementation to arbitrary autoregressive language models for explanation purposes. Discrete targets. The above formulation constrains the pointing targets to be within the discrete vocabulary space . This prevents application to general MLLMs as the multimodal inputs often consist of continuous feature sequences. 4.2 v1: Extending Any MLLM for Grounded Reasoning To address the limitations above, we propose v1, simple yet effective extension to any autoregressive MLLM that enables explicit grounding via pointing to continuous input representations. v1 augments the standard vocabulary output space with an additional set of pointer tokens that reference input embeddings, allowing the model to generate or copy visual content as needed. Unlike prior pointerbased models, v1 supports inference over both discrete and continuous modalities in unified framework, without requiring modifications to the models core architecture. Pointing to continuous inputs. The gated mixture formulation of See et al. [41] is not directly applicable to continuous inputs as image embeddings, as such inputs lack discrete mappings to vocabulary tokens . To enable pointing in this setting, we extend the output space to include references to positions in the continuous input. Specifically, we define the augmented output space as = C, where = {c1, c2, . . . , cK} denotes the set of continuous input vectors (e.g. embeddings of the input image patches). This formulation allows the model to generate either vocabulary token or pointer to specific continuous input. We denote pointer to input vector ck as ptr : ck, which is treated as discrete token during decoding. At each decoding step t, the model computes two distributions: (1) generation distribution over the vocabulary , producing logits logitgen RV , and (2) pointing distribution over the input positions C, producing logits logitptr RK. The final output logits are defined as: (cid:104) logitgen logitptr RV +K logitt = (cid:105) where [ ] denotes concatenation. The pointing logits are computed via attention over the input sequence: logit(k) ptr = Lq(ht) Lk(ck) where ht is the decoder hidden state at step t, Lq and Lk are learned linear projections, and the follows standard attention practice. We omit the gating module λ used in previous scaling factor work, as generation and pointing logits are defined over disjoint output spaces and do not require interpolation. During inference, if the model selects an index in , the next token xt is emitted as the corresponding If the model selects an index C, the token is represented as pointer vocabulary token. xt = ptr : ck. On the subsequent decoding step, the input embedding at position is replaced with the continuous vector ck, enabling the model to attend directly to the referenced content. 4.3 Annotating Visually-grounded Reasoning Data To train v1, we require fine-grained multimodal reasoning traces in which each step is grounded to specific visual evidence. To this end, we construct v1g, dataset of 300K multimodal reasoning paths with interleaved grounding annotations. Each trajectory includes sequence of reasoning steps, where textual inferences are explicitly linked to corresponding image regions. The dataset is generated through fully automated three-stage pipeline: (1) we oversample textual reasoning paths from pretrained MLLM; (2) we apply an LLM-based parser to decompose each path into discrete visual queries and retrieval steps; and (3) we ground each visual reference by aligning it with bounding box in the input image. Representative examples are provided in the Appendix. Generating text-based reasoning. As seed to our grounded corpus, we adopt the training set of TVC [14], which consists of reasoning traces generated from the QvQ model [44]. The problems within the dataset encompasses nine distinct domainsCharts, Documents, Geometry, IQ Tests, Medical Imagery, Natural Scenes, Science Diagrams, Synthetic Images, and Tables. Decomposing into visual queries and retrieval steps. To extract visual grounding cues from textbased reasoning traces, we leverage strong off-the-shelf LLM (Gemini-2.0-flash [45]) to identify and isolate references to visual content. Each such reference is rewritten as an explicit detect function call, which serves to locate and cache the corresponding visual object within the input image. The detect function accepts concise natural language description of the visual component and returns its corresponding region. The recovered objects are assigned symbolic identifiers in the format <objX>, where indicates the order of appearance of the object. We additionally prompt the LLM to generate key-value list of visual components, where each key is unique and sufficiently descriptive phrase intended for use as grounding reference in subsequent steps. To support subsequent grounding, we further instruct the LLM to produce key-value list of all visual components, where each key is unique and sufficiently descriptive phrase to serve as reference handle. We construct domain-specific few-shot prompts to guide this process, with prompt templates detailed in the Appendix. Finally, we post-process the LLM outputs to discard failure cases, including mismatches between referenced and retrieved objects, non-unique object labels, insufficient object count ( 2), and ill-formed reasoning lacking explicit answers. After filtering, approximately 82% of samples are retained. Grounding to image regions. Visual grounding is non-trivial problem for multimodal reasoning problems, as they typically consist of visual domains other than the standard natural images (e.g. charts, geometry, and medical images). As such, we saw that existing visual grounding models perform poorly on the images [46, 47]. Also, multimodal reasoning often requires grounding abstract or geometric visual cues (e.g. angle ABC), for which existing models are not trained. To exploit the implicit visual grounding behavior in MLLMs, we build on Qwen2.5-VL [9], grounding-capable model that can localize visual entities via bounding box prediction. Rather than relying on its coordinate generation interface, we instead estimate the models visual focus using relative attention mechanism inspired by Zhang et al. [48]. Concretely, we extract the cross-attention maps from the grounding query to all image patches, both in the presence and absence of grounding prompts. By computing the ratio between the conditional attention (with the grounding query) and the marginal attention (without the query), we obtain normalized attention map that isolates semantically meaningful regions while suppressing low-level visual register effects. We then apply heuristic post-processing algorithm to convert this soft attention mask into discrete bounding boxes. Finally, we filter out samples with malformed or invalid bounding boxes, resulting in curated training set of approximately 300K grounded examples."
        },
        {
            "title": "Implementation Details",
            "content": "Preprocessing. Given multimodal input consisting of interleaved images, text, and bounding box annotations for visual references, we first convert each image into flattened sequence of image patches, following the patchification protocol used by the backbone model (e.g., Qwen2.5VL [9]). Each bounding box is then transformed into corresponding sequence of pointer tokens (e.g., <ptr4>, . . . , <ptr32>), where each token refers to an enclosed image patch. These pointer tokens are appended to the tokenizer vocabulary but do not modify the models original embedding table or generation head. During preprocessing, the embeddings for pointer tokens are replaced with the corresponding image patch embeddings prior to the transformer layers. The final input is thus unified sequence of text tokens, pointer tokens, and image patch embeddings. Model. Our v1 architecture is designed to extend broad range of multimodal language model (MLLM) backbones; for empirical validation, we instantiate it on Qwen-2.5-VL [9]. Architecturally, we introduce only two lightweight linear layers atop the original model: pointing query head Lq RDD and pointing key head Lk RDD, where denotes the latent dimensionality of the MLLM. Both heads are initialized as identity matrices scaled by 1/ D, ensuring that their influence on the models initial output distribution remains minimal. This initialization strategy 6 Table 1: Results on multimodal mathematical reasoning tasks. MathVision results are both reported for the mini and full subsets to include more baseline scores. Model Qwen2-VL [8] Qwen2-VL [8] Qwen2.5-VL [9] Qwen2.5-VL [9] InternVL2.5 [10] InternVL2.5 [10] GPT-4o [54] LLaVa-CoT [12] Mulberry [13] TVC [14] TVC [14] QVQ-72B-preview [44] Ours w/o Pointing Ours Size 7B 72B 7B 72B 8B 78B - 11B 7B 7B 72B 72B 7B 7B Reasoning MathVista MathVision MathVerse Only mini mini full 60.9 69.7 67.8 74.8 64.4 72.3 63.8 54.8 63.1 68.1 72.2 71.4 60.0 68.6 - - 23.6 39.8 22.0 34.9 - 16.3 - - - 35.9 25.3 34.5 16.3 26.6 - - 19.7 32.2 30.4 - - 22.7 41.9 - 23.7 28.1 mini 24.6 36.2 44.5 57.6 39.5 51.7 50.2 33.9 39.6 38.9 48.8 41.5 33.6 48.6 Average mini full 42.8 53.0 45.3 57.4 41.9 53.0 57.0 35.0 51.4 53.5 60.5 49.6 39.6 50.6 20.5 31.4 - - 29.6 42.0 40.3 - - 30.8 45.4 - 28.7 38. is particularly effective given the structure of our task: the pretrained backbone already produces meaningful generative likelihoods Pgen, and the pointing mechanism selects at most single position per timestep from the pointing distribution Pptr. As result, the added modules integrate smoothly during early training and do not induce catastrophic forgetting. Training. All models are trained under uniform settings: base learning rate of 3105, per-device batch size of 2, and gradient accumulation over 4 steps. We leverage DeepSpeed for distributed training across 8 NVIDIA A100 GPUs. Optimization uses AdamW with β1 = 0.9, β2 = 0.95, and training is performed for 5 epochs. Given the dual-nature output space comprising generative vocabulary and pointing reference set K, we incorporate z-loss regularization to stabilize the softmax partition function, following Chowdhery et al., Wortsman et al., and Chameleon [4951]. Specifically, we regularize the logpartition function = (cid:80) exj in the softmax σ(x)i = exi/Z by introducing z-loss term Lz = λ log Z, where λ = 105. To reduce computational overhead, we approximate using top-k = 40 partition function = (cid:80) jTopK(x) exj . This approximation enables efficient and numerically stable training in large-output-space settings. Inference. At each decoding step to generate token xt, v1 utilizes two additional caches: (1) keys, given by hidden features Lk(c) corresponding to image patch positions for computing the pointing logits logitptr, and (2) values, the input feature sequences of the associated image patches. These are essential for enabling the pointing and copying mechanism during inference. We implement the additional caches by extending the key-value attention caching interface of the HuggingFace [52] Transformers library. The memory overhead is minimal compared to the standard attention cache and can be realized as an auxiliary attention layer with empty parameters serving solely as cache. This design can be similarly adapted for efficient inference in the vLLM [53] framework. We plan to release both implementations with the codebase."
        },
        {
            "title": "6 Empirical Results",
            "content": "6.1 Downstream Evaluation on Multimodal Reasoning Benchmarks Setup. We use three representative multimodal mathematical reasoning benchmarks: MathVista (mini) [17], MathVision (mini/full) [18], and MathVerse (mini) [19]. Following prior work [55], we use GPTEval [56] to compute accuracy while accounting for the formatting inconsistencies. We compare our method against both general-purpose and reasoning-specialized MLLMs. General MLLMs include Qwen2-VL [8] and Qwen2.5-VL [9] at both 7B and 72B scales, as well as 7 Figure 3: Qualitative comparison on MathVision. v1 uses explicit visual grounding to correctly solve both bar graph and spatial reasoning tasks, while LLaVA-CoT misinterprets visual content in both cases. InternVL2.5 [10] at 8B and 78B. We also include GPT-4o [54] as high-performing proprietary baseline. For reasoning-oriented models, we evaluate LLaVa-CoT-11B [12], Mulberry-7B [13], TVC-7B, TVC-72B [14], and QVQ-72B-preview [44]. Results. Quantitative results are presented in Table 1. Our approach yields substantial performance improvements over baseline models. Among 7B-scale models, v1 with full pointing capability consistently outperforms both general-purpose and reasoning-specialized baselines. Notably, despite its smaller size, our 7B model narrows the performance gap with several 72B-scale models. The gains are particularly pronounced on MathVision, benchmark known for its higher complexity and stronger demand for grounded reasoning in MLLMs. 6.2 Further Analysis Ablation study. We conduct an ablation study, summarized in Table 2, to isolate the contributions of individual components in v1, with focus on the impact of the proposed point-and-copy mechanism. We evaluate three ablated variants: (1) Backbone, the pretrained Qwen2.5-VL-7B model without any task-specific finetuning; (2) Coordinate-Only, which is trained on v1g using bounding box coordinates in place of pointer supervision; and (3) Ours w/o Pointing, which disables the pointing mechanism at inference time. Table 2: Ablation on MathVision testmini to gauge the impact of visual revisitation. Variant Backbone + Coord-Only Ours w/o Pointing Ours Train Infer Score 23.6 31.9 25.3 34.5 The results indicate that the ability to actively retrieve and incorporate relevant visual tokens via pointing is critical for achieving strong performance on complex multimodal reasoning tasks. Qualitative results. Figure 3 shows qualitative comparison between our method and LLaVACoT [12] as baseline. In both the short-answer (left) and multiple-choice (right) examples from MathVision, our v1 demonstrates explicit visual grounding through pointer-based detection and selective copying of relevant image regions. For the bar graph example, v1 accurately identifies the bar corresponding to Candy and computes the correct percentage based on the total count, while LLaVA-CoT misidentifies the tallest bar and overestimates the result. In the hexagon pathfinding task, v1 correctly reasons over spatial connectivity by attending to the structural differences in the options, whereas LLaVA-CoT fails to filter invalid candidates and outputs the wrong answer. These examples highlight how proactive visual Figure 4: Comparison of attention to copy tokens vs. original visual tokens. Layer-wise sum of attention scores directed to copy tokens and their corresponding original visual input tokens from v1 output on MathVision example. Copy token intervals are highlighted in yellow. revisitation via pointing enables more precise and interpretable reasoning compared to text-only chain-of-thought approaches. How does v1 utilize pointed visual regions? We examine whether v1 effectively attends to the visual regions retrieved via the point-and-copy mechanism. As shown in Figure 4, we compare the total Input Attentionattention to original visual tokensand Copy Attentionattention to copy tokensduring generation after the first copy token is produced. In early and intermediate layers (e.g., layers 2 and 14), copy attention clearly dominates, suggesting that v1 actively relies on retrieved visual content for grounded reasoning. In contrast, attention in later layers (e.g., layer 27) becomes more balanced between input and copy tokens, likely to facilitate planning and selection of subsequent pointing targets."
        },
        {
            "title": "7 Conclusion",
            "content": "We introduced v1, modular extension to MLLMs that enables proactive visual revisitation through simple point-and-copy mechanism. Inspired by the recursive visual behaviors observed in human reasoning, v1 allows the model to dynamically retrieve and reuse specific visual tokens throughout the reasoning process. Our method is lightweight, training-efficient, and architecture-compatible, requiring only minimal changes to existing MLLMs. Empirical results across three multimodal mathematical reasoning benchmarks demonstrate that v1 significantly improves performance, particularly on tasks requiring grounded, step-by-step visual reasoning. We hope this work motivates further exploration of interactive visual access or modification mechanisms as core design principle for multimodal reasoning systems."
        },
        {
            "title": "8 Limitations and Future Work",
            "content": "This work focuses on demonstrating the effectiveness of proactive visual revisitation in structured multimodal reasoning via simple point-and-copy mechanism. While v1 shows strong performance in mathematical domains, several directions remain for broader applicability. Beyond mathematical domains. Extending v1 to other settingssuch as scientific diagrams, medical images, or visual commonsensepresents new challenges in representation and supervision. These domains often lack structured reasoning traces, making data collection more difficult. Since v1g relies on pretrained text-only MLLM to seed reasoning, generalizing to less structured domains will require advances in decomposition, grounding, and alignment. Weak supervision and reinforcement learning. Recent work in inference-time scaling and alignment has shown the promise of reward-based learning for reasoning. Incorporating such methods into v1 may enable more flexible and efficient visual retrieval strategies without dense supervision. We leave this exploration to future work due to current resource constraints."
        },
        {
            "title": "References",
            "content": "[1] Cox, R. J. Representation construction, externalised cognition and individual differences. Learning and Instruction, 9:343363, 1999. [2] Brun, J., P. L. Masson, B. Weil. Designing with sketches: the generative effects of knowledge preordering. Design Science, 2, 2016. [3] Chu, J., E. R. Fyfe, B. Rittle-Johnson. Diagrams benefit symbolic problem-solving. British Journal of Educational Psychology, 87:273287, 2017. [4] Kozhevnikov, M., M. Hegarty, R. E. M. and. Revising the visualizer-verbalizer dimension: Evidence for two types of visualizers. Cognition and Instruction, 20(1):4777, 2002. [5] OpenAI, :, A. Jaech, et al. Openai o1 system card, 2024. [6] DeepSeek-AI, D. Guo, D. Yang, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. [7] Muennighoff, N., Z. Yang, W. Shi, et al. s1: Simple test-time scaling, 2025. [8] Wang, P., S. Bai, S. Tan, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution, 2024. [9] Bai, S., K. Chen, X. Liu, et al. Qwen2.5-vl technical report, 2025. [10] Chen, Z., W. Wang, Y. Cao, et al. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling, 2025. [11] OpenAI. Thinking with images, 2025. Accessed: 2025-05-05. [12] Xu, G., P. Jin, H. Li, et al. Llava-cot: Let vision language models reason step-by-step, 2025. [13] Yao, H., J. Huang, W. Wu, et al. Mulberry: Empowering mllm with o1-like reasoning and reflection via collective monte carlo tree search, 2024. [14] Sun, H.-L., Z. Sun, H. Peng, et al. Mitigating visual forgetting via take-along visual conditioning for multi-modal long cot reasoning, 2025. [15] Huang, W., B. Jia, Z. Zhai, et al. Vision-r1: Incentivizing reasoning capability in multimodal large language models, 2025. [16] Liu, H., C. Li, Q. Wu, et al. Visual instruction tuning. In Thirty-seventh Conference on Neural Information Processing Systems. 2023. [17] Lu, P., H. Bansal, T. Xia, et al. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. In The Twelfth International Conference on Learning Representations. 2024. [18] Wang, K., J. Pan, W. Shi, et al. Measuring multimodal mathematical reasoning with math-vision dataset. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track. 2024. [19] Zhang, R., D. Jiang, Y. Zhang, et al. Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems? In European Conference on Computer Vision, pages 169186. Springer, 2024. [20] Snell, C., J. Lee, K. Xu, et al. Scaling llm test-time compute optimally can be more effective than scaling model parameters, 2024. [21] Wu, Y., Z. Sun, S. Li, et al. Scaling inference computation: Compute-optimal inference for problem-solving with language models. In The 4th Workshop on Mathematical Reasoning and AI at NeurIPS24. 2024. [22] Welleck, S., A. Bertsch, M. Finlayson, et al. From decoding to meta-generation: Inference-time algorithms for large language models. Transactions on Machine Learning Research, 2024. Survey Certification. [23] Yeo, E., Y. Tong, X. Niu, et al. Demystifying long chain-of-thought reasoning in LLMs. In ICLR 2025 Workshop on Navigating and Addressing Data Problems for Foundation Models. 2025. [24] Xie, Y., A. Goyal, W. Zheng, et al. Monte carlo tree search boosts reasoning via iterative preference learning, 2024. 10 [25] Chu, T., Y. Zhai, J. Yang, et al. Sft memorizes, rl generalizes: comparative study of foundation model post-training, 2025. [26] Hendrycks, D., C. Burns, S. Kadavath, et al. Measuring mathematical problem solving with the MATH dataset. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2). 2021. [27] Jain, N., K. Han, A. Gu, et al. Livecodebench: Holistic and contamination free evaluation of large language models for code. In The Thirteenth International Conference on Learning Representations. 2025. [28] Rein, D., B. L. Hou, A. C. Stickland, et al. GPQA: graduate-level google-proof q&a benchmark. In First Conference on Language Modeling. 2024. [29] Chen, B., Z. Xu, S. Kirmani, et al. Spatialvlm: Endowing vision-language models with spatial reasoning capabilities. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1445514465. 2024. [30] Zhang, Z., A. Zhang, M. Li, et al. Multimodal chain-of-thought reasoning in language models. Transactions on Machine Learning Research, 2024. [31] Yang, Z., L. Li, J. Wang, et al. Mm-react: Prompting chatgpt for multimodal reasoning and action. arXiv preprint arXiv:2303.11381, 2023. [32] Meng, F., L. Du, Z. Liu, et al. Mm-eureka: Exploring the frontiers of multimodal reasoning with rule-based reinforcement learning, 2025. [33] Wang, Y., S. Wu, Y. Zhang, et al. Multimodal chain-of-thought reasoning: comprehensive survey, 2025. [34] Li, C., W. Wu, H. Zhang, et al. Imagine while reasoning in space: Multimodal visualization-of-thought, 2025. [35] Ma, T., L. Xie, Y. Tian, et al. Clawmachine: Learning to fetch visual tokens for referential comprehension. In The Thirteenth International Conference on Learning Representations. 2025. [36] Gupta, T., A. Kembhavi. Visual programming: Compositional visual reasoning without training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1495314962. 2023. [37] Hu, Y., W. Shi, X. Fu, et al. Visual sketchpad: Sketching as visual chain of thought for multimodal language models. In The Thirty-eighth Annual Conference on Neural Information Processing Systems. 2024. [38] Borazjanizadeh, N., R. Herzig, E. Oks, et al. Visualizing thought: Conceptual diagrams enable robust planning in lmms, 2025. [39] Wu, P., S. Xie. V*: Guided visual search as core mechanism in multimodal llms, 2023. [40] Van Den Oord, A., O. Vinyals, et al. Neural discrete representation learning. Advances in neural information processing systems, 30, 2017. [41] See, A., P. J. Liu, C. D. Manning. Get to the point: Summarization with pointer-generator networks. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 10731083. 2017. [42] Kazemzadeh, S., V. Ordonez, M. Matten, et al. ReferItGame: Referring to objects in photographs of natural scenes. In A. Moschitti, B. Pang, W. Daelemans, eds., Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 787798. Association for Computational Linguistics, Doha, Qatar, 2014. [43] Radford, A., J. W. Kim, C. Hallacy, et al. Learning transferable visual models from natural language supervision, 2021. [44] Qwen Team. Qvq: To see the world with wisdom, 2024. [45] Google. Gemini 2.0 flash (gemini-2.0-flash-001), 2025. [46] Steiner, A., A. S. Pinto, M. Tschannen, et al. Paligemma 2: family of versatile vlms for transfer. arXiv preprint arXiv:2412.03555, 2024. 11 [47] Xiao, B., H. Wu, W. Xu, et al. Florence-2: Advancing unified representation for variety of vision tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 48184829. 2024. [48] Zhang, J., M. Khayatkhoei, P. Chhikara, et al. MLLMs know where to look: Training-free perception of small visual details with multimodal LLMs. In The Thirteenth International Conference on Learning Representations. 2025. [49] Chowdhery, A., S. Narang, J. Devlin, et al. Palm: Scaling language modeling with pathways. Journal of Machine Learning Research, 24(240):1113, 2023. [50] Wortsman, M., P. J. Liu, L. Xiao, et al. Small-scale proxies for large-scale transformer training instabilities. arXiv preprint arXiv:2309.14322, 2023. [51] Team, C. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. [52] Wolf, T., L. Debut, V. Sanh, et al. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 3845. Association for Computational Linguistics, Online, 2020. [53] Kwon, W., Z. Li, S. Zhuang, et al. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles. 2023. [54] Hurst, O. A., A. Lerer, A. P. Goucher, et al. Gpt-4o system card. ArXiv, abs/2410.21276, 2024. [55] Duan, H., J. Yang, Y. Qiao, et al. Vlmevalkit: An open-source toolkit for evaluating large multi-modality models. In Proceedings of the 32nd ACM International Conference on Multimedia, pages 1119811201. 2024. [56] Liu, Y., D. Iter, Y. Xu, et al. G-eval: Nlg evaluation using gpt-4 with better human alignment, 2023."
        },
        {
            "title": "A Data Generation Details",
            "content": "Figure 5: v1g dataset construction pipeline. Figure 5 illustrates the construction pipeline for our v1g dataset; each stage of this pipeline is described in detail in Section 4.3. The specific prompt template used to decompose text-based reasoning paths into visual queries (as outlined in our methodology in Section 4.3) is provided in Table 3."
        },
        {
            "title": "B Additional Qualitative Results",
            "content": "To further illustrate v1s complex visual reasoning, this section provides additional qualitative examples, complementing Figure 3 from the main text. These examples highlight how v1 leverages the point-and-copy mechanism. Figure 7 demonstrates v1 on synthetic task (CLEVR-like) requiring object counting based on the query: Subtract all red things, then subtract all tiny matte balls. How many objects are left?. v1 first localizes objects using its pointer mechanism. It then sequentially reasons, identifying red objects before revisiting relevant items, like the cyan sphere, to verify the combined tiny and matte attributes through targeted attention. This process demonstrates v1s capacity for precise attribute grounding and multi-step compositional reasoning enabled by the point-and-copy mechanism. In Figure 8, v1 tackles chart comprehension task: determining if the Dark Violet data series has the minimum area under the curve. v1 initially grounds key chart elements, using its pointer to isolate data series such as Dark Violet, Medium Mint, and Dark Cyan. Later in its reasoning, it proactively revisits these series, performing comparative analysis of their visual trajectories and relative y-axis values to infer their respective areas. Such selective re-focusing showcases its ability to perform nuanced comparisons within dense visual information. These examples further affirm that v1s architecture, by supporting proactive visual revisitation and precise grounding via its pointing mechanism, achieves robust, interpretable, and accurate multi-step visual reasoning. Attention Score Comparison Between Text-based Reasoning Figure 6 presents layer-wise comparison of these attention dynamics (using MathVision example), plotting the sum of attention scores on original visual tokens against generation steps following v1s first copy operation. For v1, these scores represent attention to specific visual tokens designated 13 Figure 6: Layer-wise Visual Attention Dynamics: v1 vs. Coordinates-Only. Attention scores on visual inputs for v1 (referenced input visual tokens during reasoning) versus the Coordinates-Only baseline (every input visual token), shown across layers (2, 14, and 27). The x-axis, \"Distance From First Copy,\" tracks generation steps after v1s initial copy operation. by its point-and-copy mechanism at each step. This targeted attention is pronounced and dynamic, particularly in intermediate and deeper layers (e.g., Layers 14 and 27), where scores fluctuate significantly, peaking at approximately 0.35, indicating active engagement with referenced visual information. In stark contrast, for the Coordinates-Only model, the sum of attention across all its original visual tokens (present in the context at each step) remains consistently low (generally below 0.05) and largely static across all layers. This comparison underscores how v1s explicit pointing and copying mechanism enables more focused and substantial engagement with relevant visual information during the reasoning process. The analysis window for both models commences from the generation step at which v1 produced its first copy token, extending for consistent number of subsequent steps. 14 Figure 7: Qualitative example of v1 tackling an attribute-based counting task in synthetic domain. Figure 8: Qualitative example of v1 performing comparative reasoning on chart comprehension task."
        },
        {
            "title": "Prompt for data generation",
            "content": "You are given text-only reasoning for visual question answering. Your task is to convert this text-only reasoning into visually grounded reasoning. ### STEP-BY-STEP INSTRUCTION Please following these instructions step-by-step, imitating human visual reasoning behavior by: 1) Start from the beginning of the reasoning and read EACH sentence. 2) When you think youd better to look the object or region, use detect() function 3) Format: detect(query=\"visual item that you want to find\", objects=[\"<obj#>\"]) 4) After detection, reference the visual element with <obj#> tags everytime you need to look it again immediately after mentioning the item. 5) Use NEW object numbers (<obj1>, <obj2>, <obj3>...) for EACH new detection. ### EXAMPLE: Original text: \"Looking at the graph, can see the function reaches its maximum at = 3.\" Corrected: To answer the question, need to look the graph. detect(query=\"function graph\", objects=[\"<obj1>\"]) Looking at the graph <obj1>, can see the function reaches its maximum at = 3. Later reference: You can skip the <obj#> tag when you think you do not need to look it again. The slope of the function becomes zero at this point on the graph. ### KEY REQUIREMENTS: - Every item in lists MUST have its own detect() statement - Put detect() statements on their own lines - NEVER skip any visual element mentioned in the reasoning - Start object numbering at obj1 and increase by 1 for each new object ### <OBJ#> REQUIREMENTS - Visual element should be concrete, distinct, and explicit. Later you will localize the element based on the detect(). So make sure that the element not confusing. - Use separate tags for each object (write \"between the bus <obj1> and the car <obj2>\" not \"between <obj1 and obj2>\"). - GOOD grounding: jects=[\"<obj1>\"]) The triangle <obj1> has right angle at vertex S.\" - BAD grounding: \"detect(query=\"triangle and rectangular\", objects=[\"<obj1>\"]) in the diagram, there are the triangle and rectangular has right angle.\" (referring to non-atomic element) - BAD grounding: \"detect(query=\"region\", objects=[\"<obj1>\"]) The triangle <obj1> has right angle.\" (referring to ambiguous element) \"I need to analyze this problem. detect(query=\"triangle\", obAfter completing the reasoning, list all objects detected: { \"obj1\": {\"type\": \"function_graph\", \"description\": \"Graph of function with maximum at = 3\"}, \"obj2\": {\"type\": \"next_item\", \"description\": \"Description of next item\"} } - We will localize the element with the open-world detector based on the descrip17 tion, so make sure to include well-described full self-contained description enough to uniquely identify the object. ### FINAL FORMAT: { \"reasoning\": \"Your fully visually-grounded reasoning text\", \"obj_list\": \"Your JSON object list\" } Now, strictly following the instruction and the example, please provide the object list and visually grounded reasoning for the following prompt and reasoning: ### Example Original Conversation HUMAN: [few_shot_question] GPT: [few_shot_answer] ### Visually Grounded Reasoning GPT: [few_shot_reasoning] ### Object List: [few_shot_objects] Now, given the conversation, please convert GPTs text-only reasoning into visually grounded reasoning Original Conversation: HUMAN: [question] GPT: [answer] ### Visually Grounded Reasoning: Table 3: Prompt that we used for generation of v1g"
        }
    ],
    "affiliations": [
        "Seoul National University",
        "Yonsei University"
    ]
}