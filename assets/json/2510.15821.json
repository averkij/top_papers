{
    "paper_title": "Chronos-2: From Univariate to Universal Forecasting",
    "authors": [
        "Abdul Fatir Ansari",
        "Oleksandr Shchur",
        "Jaris Küken",
        "Andreas Auer",
        "Boran Han",
        "Pedro Mercado",
        "Syama Sundar Rangapuram",
        "Huibin Shen",
        "Lorenzo Stella",
        "Xiyuan Zhang",
        "Mononito Goswami",
        "Shubham Kapoor",
        "Danielle C. Maddix",
        "Pablo Guerron",
        "Tony Hu",
        "Junming Yin",
        "Nick Erickson",
        "Prateek Mutalik Desai",
        "Hao Wang",
        "Huzefa Rangwala",
        "George Karypis",
        "Yuyang Wang",
        "Michael Bohlke-Schneider"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Pretrained time series models have enabled inference-only forecasting systems that produce accurate predictions without task-specific training. However, existing approaches largely focus on univariate forecasting, limiting their applicability in real-world scenarios where multivariate data and covariates play a crucial role. We present Chronos-2, a pretrained model capable of handling univariate, multivariate, and covariate-informed forecasting tasks in a zero-shot manner. Chronos-2 employs a group attention mechanism that facilitates in-context learning (ICL) through efficient information sharing across multiple time series within a group, which may represent sets of related series, variates of a multivariate series, or targets and covariates in a forecasting task. These general capabilities are achieved through training on synthetic datasets that impose diverse multivariate structures on univariate series. Chronos-2 delivers state-of-the-art performance across three comprehensive benchmarks: fev-bench, GIFT-Eval, and Chronos Benchmark II. On fev-bench, which emphasizes multivariate and covariate-informed forecasting, Chronos-2's universal ICL capabilities lead to substantial improvements over existing models. On tasks involving covariates, it consistently outperforms baselines by a wide margin. Case studies in the energy and retail domains further highlight its practical advantages. The in-context learning capabilities of Chronos-2 establish it as a general-purpose forecasting model that can be used \"as is\" in real-world forecasting pipelines."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 1 ] . [ 1 1 2 8 5 1 . 0 1 5 2 : r Technical Report of ChronosChronos-2: From Univariate to Universal Forecasting Abdul Fatir Ansari1, Oleksandr Shchur1, Jaris Küken1,3, Andreas Auer1,4, Boran Han1, Pedro Mercado1, Syama Sundar Rangapuram1, Huibin Shen1, Lorenzo Stella1, Xiyuan Zhang1, Mononito Goswami1, Shubham Kapoor1, Danielle C. Maddix1, Pablo Guerron2,5, Tony Hu1, Junming Yin1, Nick Erickson1, Prateek Mutalik Desai1, Hao Wang1,6, Huzefa Rangwala1, George Karypis1, Yuyang Wang1, Michael Bohlke-Schneider1 ansarnd@amazon.de 1Amazon Web Services 2Amazon 3University of Freiburg 4Johannes Kepler University Linz 5Boston College 6Rutgers University Code: github.com/amazon-science/chronos-forecasting"
        },
        {
            "title": "Abstract",
            "content": "Pretrained time series models have enabled inference-only forecasting systems that produce accurate predictions without task-specific training. However, existing approaches largely focus on univariate forecasting, limiting their applicability in real-world scenarios where multivariate data and covariates play crucial role. We present Chronos-2, pretrained model capable of handling univariate, multivariate, and covariate-informed forecasting tasks in zero-shot manner. Chronos-2 employs group attention mechanism that facilitates in-context learning (ICL) through efficient information sharing across multiple time series within group, which may represent sets of related series, variates of multivariate series, or targets and covariates in forecasting task. These general capabilities are achieved through training on synthetic datasets that impose diverse multivariate structures on univariate series. Chronos-2 delivers state-of-the-art performance across three comprehensive benchmarks: fev-bench, GIFT-Eval, and Chronos Benchmark II. On fev-bench, which emphasizes multivariate and covariate-informed forecasting, Chronos-2s universal ICL capabilities lead to substantial improvements over existing models. On tasks involving covariates, it consistently outperforms baselines by wide margin. Case studies in the energy and retail domains further highlight its practical advantages. The in-context learning capabilities of Chronos-2 establish it as general-purpose forecasting model that can be used as is in real-world forecasting pipelines."
        },
        {
            "title": "1 Introduction",
            "content": "The advent of pretrained models (also referred to as foundation models) has led to paradigm shift in time series forecasting. Instead of training model for each time series (local models) (Hyndman & Athanasopoulos, 2018) or dataset (task-specific models) (Lim et al., 2021; Challu et al., 2023), single model can be trained once on large-scale time series data and then applied across different forecasting problems (Ansari et al., 2024; Das et al., 2024b). Pretrained models greatly simplify the forecasting pipeline by eliminating the need for training from scratch for each use case. More remarkably, they often match or exceed the forecast accuracy of task-specific models (Aksu et al., 2024). Despite these advances, fundamental limitation persists: most pretrained models operate only on univariate data, considering solely the historical observations of single time series to generate forecasts. Although univariate forecasting is important, the class of real-world forecasting tasks spans far beyond it. In practice, one may encounter tasks where multiple co-evolving time series need to be predicted simultaneously (multivariate forecasting) (Bańbura et al., 2010; Cohen et al., 2025) or where forecasts depend on various external factors (covariate-informed forecasting). For example, cloud infrastructure metrics such as CPU usage, memory consumption, and storage I/O evolve together and benefit from joint modeling (Cohen et al., 2025). Likewise, retail demand is heavily influenced by promotional Equal contribution. Jaris Küken and Andreas Auer contributed to this work during their internships at AWS. Hao Wang and Pablo Guerron hold concurrent appointments at Amazon and their corresponding universities, and this report describes work performed at Amazon. Equal advisory contribution. Technical Report of Chronos-2 Figure 1: The complete Chronos-2 pipeline. Input time series (targets and covariates) are first normalized using robust scaling scheme, after which time index and mask meta features are added. The resulting sequences are split into non-overlapping patches and mapped to high-dimensional embeddings via residual network. The core transformer stack operates on these patch embeddings and produces multi-patch quantile outputs corresponding to the masked future patches provided as input. Each transformer block alternates between time and group attention layers: the time attention layer aggregates information across patches within single time series, while the group attention layer aggregates information across all series within group at each patch index. group is flexible notion of relatedness and may correspond to single time series, multiple series sharing source or metadata, variates of multivariate series, or targets along with associated covariates. The figure illustrates two multivariate time series with one known covariate each, with corresponding groups highlighted in blue and red. This example is for illustration only; Chronos-2 supports arbitrary numbers of targets and optional covariates. activities, while energy consumption patterns are driven by weather conditions (Petropoulos et al., 2022). The lack of multivariate and covariate-informed forecasting capabilities hinders the widespread adoption of pretrained models in real-world production systems. Developing universal pretrained models that can handle both multivariate dependencies and covariates remains challenging due to two factors. First, the heterogeneity of forecasting problems requires rethinking the model architecture. Each downstream task differs in the number of dimensions and their semantics. Since it is impossible to know priori how the variables will interact in an unseen task, the model must infer these interactions from the available context. Second, high-quality pretraining data with multivariate dependencies and informative covariates is scarce. In this work, we present Chronos-2, pretrained model designed to handle arbitrary forecasting tasks univariate, multivariate, and covariate-informed in zero-shot manner. Chronos-2 leverages in-context learning (ICL) to support multivariate forecasting and arbitrary covariates, whether past-only or with known future values, real-valued or categorical. Its enhanced ICL capabilities also improve univariate forecasting by enabling cross learning, where the model shares information across univariate time series in the batch, leading to more accurate predictions. At the core of Chronos-2 ICL capabilities is the group attention mechanism. It enables information exchange within groups of time series, which may represent arbitrary sets of related series, variates of multivariate series, or targets and covariates (both past-only and known) in forecasting task. Rather than extending the context by concatenating targets and covariates, the group attention layer shares information within groups across the batch axis, allowing it to scale gracefully with the number of variates. key innovation of Chronos-2 lies in our training approach: to enable its ICL capabilities, we rely on synthetic time series data generated by imposing multivariate structure on time series sampled from base univariate generators. The complete inference pipeline of Chronos-2 including tokenization and modeling is shown in Figure 1. Empirical evaluation on comprehensive forecasting benchmarks, including fev-bench (Shchur et al., 2025), GIFT-Eval (Aksu et al., 2024), and Chronos Benchmark II (Ansari et al., 2024), shows that Chronos-2 achieves state-of-the-art performance. On fev-bench, which spans wide range of forecasting tasks univariate, multivariate, 2 Technical Report of Chronos-2 Model Chronos-2 Toto-1.0 TabPFN-TS COSMIC Moirai-1.0 Chronos-Bolt Moirai-2.0 Sundial TimesFM-2.5 TiRex Univariate Forecasting Multivariate Forecasting Past-Only Covariates Known Covariates Categorical Covariates Cross Learning Memory Scaling O(V ) O(V ) O(V ) O(V 2) O(V 2) - - - - - Table 1: Comparison of capabilities of pretrained forecasting models. Past-Only Covariates: support for covariates only observed in the past; Known Covariates: support for covariates whose future values are known; Categorical Covariates: support for nominal features in the covariates; Cross Learning: support for in-context learning across related time series; Memory Scaling: inference memory requirements with respect to the total number of variates (including both targets and covariates). and covariate-informed Chronos-2 outperforms baselines across all categories. The largest gains are observed on covariate-informed tasks, demonstrating Chronos-2s strength in this practically important setting. Chronos-2 offers these new capabilities while maintaining high computational efficiency, running on single mid-range GPU (NVIDIA A10G) with throughput of 300 time series per second.1 The rest of the technical report is organized as follows. Section 2 introduces the background on time series forecasting and existing forecasting methods with special focus on pretrained models. In Section 3, we describe the architecture of Chronos-2 and discuss its training and inference pipelines. Section 4 briefly discusses the training corpus of Chronos-2. In Section 5, we present our main results on three forecasting benchmarks, case studies on energy and retail domains, and ablations. We conclude the report and discuss potential future work in Section 6."
        },
        {
            "title": "2 Background and Related Work",
            "content": "Time series forecasting aims to predict future values of temporal sequence given historical observations. Formally, let Y1:T = [y1, . . . , yT ] denote historical time series of length , where each observation yt RD can either be univariate (D = 1) or multivariate (D > 1). Given this historical context, the goal is to predict the next time steps YT +1:T +H , where defines the forecast horizon. Forecasts may be supported by covariates (also known as exogenous variables) X1:T +H = [x1, . . . , xT +H ], where xt RM represents additional information that can span both historical (t ) and future (t > ) time steps. The task itself can be defined as either point forecasting, where the objective is to predict single future value at each time step, or probabilistic forecasting, where the objective is to estimate the conditional distribution P(YT +1:T +H Y1:T , X1:T +H ) in order to capture forecast uncertainty. Zero-shot forecasting refers to the setting in which model generates forecasts for previously unseen time series datasets without requiring any additional training, adaptation, or fine-tuning. Forecasting methods preceding the pretrained model paradigm can be broadly divided into local and global models. Local models fit one set of parameters for each time series in the dataset. These include classical approaches such as ARIMA, Exponential Smoothing (Hyndman & Athanasopoulos, 2018), and Theta (Assimakopoulos & Nikolopoulos, 2000). In contrast, global models share their parameters across all time series within specific dataset. Deep learning approaches in this category have become increasingly common over the last decade. Notable examples of global models include recurrent neural networks (RNN) like DeepState (Rangapuram et al., 2018), DeepAR (Salinas et al., 2020), and TimeGrad (Rasul et al., 2021); stacked architectures such as N-BEATS (Oreshkin et al., 2020) and N-HITS (Challu et al., 2023); and transformer-based architectures like TFT (Lim et al., 2021) and PatchTST (Nie et al., 2023). Pretrained forecasting models have recently emerged as new paradigm in time series forecasting. While earlier work already demonstrated limited transfer learning capabilities for forecasting (Orozco & Roberts, 2020; Oreshkin et al., 1Based on inference time for batch of 1,024 time series with context length of 2048 and prediction length of 64 times teps. 3 Technical Report of Chronos2021; Jin et al., 2022; Nie et al., 2023), pretrained models adopt principles similar to large language models (LLMs) and enable zero-shot generalization on diverse datasets. Initial attempts focused on directly adapting language models to time series tasks (Gruver et al., 2023; Jin et al., 2024), whereas more recent approaches primarily borrow architectural concepts from LLMs but pretrain them on time series data (Das et al., 2024b; Garza et al., 2024; Ansari et al., 2024). The majority of pretrained models are limited to univariate forecasting (Rasul et al., 2023; Das et al., 2024b; Ansari et al., 2024; Liu et al., 2025; Auer et al., 2025b), treating each dimension independently in multivariate scenarios and ignoring covariates. Notable exceptions include Moirai-1 (Woo et al., 2024) and Toto (Cohen et al., 2025), which incorporate multivariate structure into their architectures. Moirai-1 supports multivariate inputs but flattens them internally, which limits scalability to high-dimensional cases. Toto introduces cross-variate attention mechanism but does not support known or categorical covariates. COSMIC (Auer et al., 2025a) advances covariate utilization through synthetic augmentations but remains restricted to univariate targets. TabPFN-TS (Hoo et al., 2025), tabular foundation model adapted for time series, can incorporate known covariates but it does not model past-only covariates or multivariate targets. Despite these advances, empirical analyses show that most approaches provide only marginal benefits over univariate models (Żukowska et al., 2024; Auer et al., 2025a), indicating that jointly modeling multiple variates and integrating covariates effectively in zero-shot setting remains an open challenge. Our approach addresses this gap with group attention mechanism, which generalizes ideas from cross-attention architectures for multivariate forecasting (Zhang & Yan, 2023; Rao et al., 2021; Arnab et al., 2021) and cross-learning across multiple univariate series (Das et al., 2024a). Unlike prior approaches, group attention operates over groups of related time series and naturally accommodates diverse forecasting setups, including univariate, multivariate, and covariate-informed tasks, within unified framework without requiring architectural changes or task-specific adaptations. Table 1 compares the capabilities of Chronos-2 with those of existing pretrained models."
        },
        {
            "title": "3 The Chronos-2 Model",
            "content": "In this section, we introduce the Chronos-2 model. We begin with scaling and tokenization, followed by the models architecture including the group attention mechanism which enables Chronos-2s in-context learning capabilities. Subsequently, we discuss the training and inference pipelines of Chronos-2. The complete inference pipeline of Chronos-2 is visualized in Figure 1. 3.1 Scaling and Tokenization Input Construction. The model operates on two inputs derived from the target Y1:T and covariates X1:T +H . We concatenate all historical values into = [v1, . . . , vT ], where each vt RD+M consists of the target observation yt and the corresponding covariate vector xt. Similarly, we define the future values as = [wT +1, . . . , wT +H ], where wt RD+M contains known future covariate values xt when available, while the entries corresponding to targets and past-only covariates are set to missing values. Categorical covariates in X1:T +H are transformed into real-valued representations before being concatenated into and . For univariate targets, we apply target encoding (Pedregosa et al., 2011; Micci-Barreca, 2001), which maps each category to numerical value based on its relationship with the target. For multivariate targets, the model falls back to ordinal encoding, assigning unique integer to each category. Robust Scaling. The input values, and , may be at an arbitrary scale, so our tokenization pipeline begins by normalizing the series. We adopt standardization, widely used normalization method in the literature, and introduce an additional step: applying the sinh1 transformation to the standardized values. This log-like transformation further stabilizes variance and reduces the influence of outliers on the objective function. It has been used in econometrics (Burbidge et al., 1988) and energy price forecasting (Uniejewski & Weron, 2018) literature for handling extreme values. Formally, each historical value vt,d and the future value wt,d are normalized as vt,d = sinh wt,d = sinh1 (cid:19) (cid:18) vt,d µd σd (cid:18) wt,d µd σd (cid:19) 4 for {1, . . . , }, for {T + 1, . . . , + H}, (1) (2) Technical Report of Chronos-2 where µd and σd are the mean and standard deviation of the historical values [v1,d, ..., vT,d], respectively. Any missing values in are excluded when computing µd and σd. The normalized historical values and future values are concatenated to construct the input matrix = [ , ] R(T +H)(D+M ). , Meta Features. During tokenization, each dimension of is processed independently by the model. To describe the tokenization procedure, consider single column ud = [u1,d, . . . , uT +H,d] corresponding to one target or covariate dimension d. Two additional meta features are appended to each column: time index and mask. The time index = (cid:2) (cid:3) encodes the relative position of each time step, where is the maximum context length supported by the model. It provides explicit information about temporal ordering to the model which is beneficial when using patch-based inputs. The mask md is binary indicator equal to 1 when the value is observed, and 0 otherwise. It serves two purposes: indicating which values are missing in the historical context and specifying which input dimensions correspond to future-known covariates. After construction of the mask, all missing values in ud are replaced with zeros. , . . . , 0, . . . , H1 Patching and Embedding. The input ud with the corresponding meta features, and md, are split into nonoverlapping patches of length (Nie et al., 2023). The context and future sections of the time series and meta features are split into patches separately. When and are not multiples of , zero padding is applied on the left (context) or right (future). Let up, jp , and mp denote the p-th patches of the input, time index, and mask, respectively. These are concatenated and mapped into the embedding space using residual network, in ϕ : R3P RDmodel , hp = in ϕ (cid:0)(cid:2)up, jp, mp (cid:3)(cid:1) , (3) where ϕ denotes parameters of the residual network and Dmodel is the hidden dimension of the transformer model. Between the patch embeddings of the context and future, we include special REG token which serves both as separator token and an attention sink (Xiao et al., 2024). 3.2 Architecture Chronos-2 is an encoder-only transformer (Vaswani et al., 2017) model which closely follows the design of the T5 encoder (Raffel et al., 2020). In the following, we discuss the key architectural components of Chronos-2. Time Attention. The time attention layer is the usual attention layer found in typical sequence models. It applies self-attention along the temporal axis and aggregates information across patches of the same input dimension. We replace relative position embeddings used in the self-attention layers of the original T5 model with rotary position embeddings (RoPE) (Su et al., 2024) which have become the de-facto standard for position embeddings in modern transformer-based models (Touvron et al., 2023). Group Attention. We introduce group attention layer into the transformer stack, which is central to enabling the in-context learning capabilities of Chronos-2. This layer aggregates information across time series that belong to the same group at given patch index. group refers to set of related time series and may refer to different things depending on the forecasting task. For example, group may consist of: single time series: the minimal grouping where the model makes univariate predictions without referring to other time series in the batch. set of time series with shared source or metadata: this grouping enables the model to perform cross learning across items by making joint predictions for related time series (also referred to as few-shot learning) instead of generating univariate forecasts by solely taking the histories of individual time series into account. Sharing information between related time series could be especially helpful when all or some (cold start scenario) time series have short histories and when the characteristics of the downstream dataset differ considerably from the training data distribution. set of variates with shared dynamics: this grouping enables multivariate forecasting where the model jointly predicts all variates with shared dynamics. set of target(s), past-only covariates and known covariates: the most general case where the model forecasts targets while taking covariates into account. 5 Technical Report of Chronos-2 Within batch of size B, multiple groups of varying sizes are possible, each identified by group IDs g, vector of length B. Internally, the group attention layer maps these IDs to two-dimensional attention mask, ensuring that aggregation occurs only within groups and not across them. Since time series within group lack natural ordering, the group attention layer omits positional embeddings. Quantile Head. After sequence of alternating time and group attention layers, the embeddings of future patches of the target dimensions are passed through residual block to produce the direct multi-step quantile forecast ˆZ RHDQ. By producing forecasts for multiple target patches within single forward pass, the model can efficiently generate predictions over long forecast horizons. Chronos-2 predicts set of 21 quantiles = {0.01, 0.05, 0.1, . . . , 0.9, 0.95, 0.99}. This results in richer representation of the predictive distribution compared to the 9-quantile grid {0.1, 0.2, ..., 0.9} commonly used in existing pretrained models. The inclusion of extreme quantiles (0.01 and 0.99) improves coverage of rare events and enhances the models applicability to tasks such as anomaly detection and risk-aware forecasting. 3.3 Training During training, batches are constructed to include heterogeneous forecasting tasks: univariate forecasting, multivariate forecasting (which also covers tasks with past-only covariates), and multivariate forecasting with known covariates. Each task is characterized by the number of target dimensions D, the number of covariates , and the role of each dimension (target, past-only covariate, or known covariate). unique group ID is assigned to each task, and the combination of group IDs with whether the future input is observed allows the model to infer the specific forecasting setup. The model is trained using the quantile regression objective max(z ˆzq, 0) + (1 q) max(ˆzq z, 0) (cid:17) , (4) (cid:88) (cid:16) qQ where ˆzq is the forecast at quantile level q, and is the corresponding target value normalized as in Eq. (1). The loss is averaged over all forecast steps and items in the batch and is computed only on target dimensions, with entries corresponding to known covariates or missing target values excluded from the objective. The number of output patches is randomly sampled for each batch during training. Training proceeds in two stages. First, the model is pretrained with maximum context length of 2048 and low number of maximum output patches. In the second stage, the context length is extended to 8192, and the maximum number of sampled output patches is increased. Longer contexts enable the model to capture long-term seasonalities in high-frequency time series, while multi-patch outputs allow for long-horizon forecasts without relying on heuristics. 3.4 Inference Forecasts are generated by de-normalizing the model predictions ˆzq t,d head output ˆzq t,d is transformed as and inverting Eq. (1). Formally, the quantile t,d = µd + σd sinh(ˆzq ˆyq t,d), (5) to obtain the prediction ˆyq t,d During inference, multiple time series in batch can be grouped to solve different forecasting tasks: of the quantile level at time step along the target dimension d. univariate forecasting: each item in the batch is assigned unique group ID. This ensures that the model makes independent predictions for each time series in the batch. multivariate forecasting: each variate which belongs to the same multivariate series is assigned the same group ID with variates from different multivariate series having distinct group IDs. This allows the model to share dynamics information between different variates of multivariate time series. forecasting with covariates: all target(s), past-only and known covariates belonging to the same task are assigned the same group ID. The future inputs corresponding to known covariates contain their known future values. The predictions generated by the model for covariates are ignored. 6 Technical Report of Chronos-2 Task Type Group IDs Future Inputs Univariate Forecasting (3 independent series) Multivariate Forecasting (3 targets) = (1, 2, 3) = = (1, 1, 1) = . . . . . . . . . . . . . . . . . . R3H R3H Forecasting with Covariates (1 target, 1 past-only covariate, 2 known covariates) = (1, 1, 1, 1) = xT +1,3 xT +1,4 . . . . . . . . . xT +H,3 . . . xT +H,4 R4H Table 2: Diverse forecasting tasks can be solved by specifying group IDs and future inputs appropriately. Here, and denote the group IDs and future values provided to the model. Future inputs for targets and past-only covariates are masked as missing values, denoted as . The examples use fixed numbers of variates for clarity, but Chronos-2 can handle arbitrary dimensions. Table 2 summarizes how group IDs and future inputs must be specified to solve different forecasting tasks. In addition to these, Chronos-2 can also be used in the full cross learning mode where each item in the batch is assigned the same group ID regardless of whether the item is target, past-only covariate or known covariate. Since each item belongs to the same group, the model shares information across items in the batch and makes joint predictions for the entire batch."
        },
        {
            "title": "4 Training Data",
            "content": "For generalist pretrained model such as Chronos-2, the training data often plays more decisive role than the models specific architecture. Although recent efforts have expanded the availability of large-scale time series datasets (Woo et al., 2024; Ansari et al., 2024; Aksu et al., 2024), they primarily contain univariate data. To overcome this limitation and endow Chronos-2 with in-context learning capabilities, we relied extensively on synthetic data. 4.1 Univariate Data We incorporated select datasets from the Chronos (Ansari et al., 2024) and GIFT-Eval (Aksu et al., 2024) pretraining corpora into Chronos-2s training corpus. The full list of datasets is provided in Table 6 (Appendix). To further enhance data diversity, we generated synthetic data using two approaches: TSI (Trend, Seasonality, and Irregularity): based on Bahrpeyma et al. (2021), this generator produces diverse synthetic series by randomly constructing and combining different trend, seasonality, and irregularity components. TCM (Temporal Causal Model): this generator samples random causal graphs from temporal causal model (Runge et al., 2023), from which time series are generated via autoregression. 4.2 Multivariate Data For multivariate and covariate-informed tasks, we relied entirely on synthetic data. To enable broad class of multivariate structures, we introduce the concept of multivariatizers. multivariatizer samples multiple time series from base univariate generators and imposes dependencies among them to create multivariate dynamics. As base univariate generators, we employed diverse set including autoregressive (AR) models, exponential smoothing (ETS) models, TSI, and KernelSynth (Ansari et al., 2024). We used two broad classes of multivariatizers: 7 Technical Report of Chronos-2 Cotemporaneous multivariatizers apply linear or nonlinear transformations at the same time step across time series sampled from the base univariate generators. This introduces instantaneous correlations between the time series resulting in multivariate time series. Sequential multivariatizers induce dependencies across time, generating richer multivariate properties such as leadlag effects and cointegration. The multivariate time series generated from the multivariatizers were used to construct both multivariate tasks (where all variates must be predicted) and covariate-informed tasks, where subset of variates was randomly designated as known covariates."
        },
        {
            "title": "5 Experiments",
            "content": "In this section, we present empirical results, beginning with an evaluation of Chronos-2 against state-of-the-art approaches across three comprehensive benchmarks (Section 5.1). We then demonstrate the gains achieved through in-context learning on univariate, multivariate, and covariate-informed forecasting tasks (Section 5.2). Next, we examine Chronos-2s performance on tasks from the energy and retail domains, where covariates are often important for accurate forecasting (Section 5.3). Finally, we report results for ablated variants of Chronos-2 (Section 5.4), including smaller model, version trained only on synthetic data, and the model prior to long-context post-training. 5.1 Benchmark Results Model Chronos-2 TiRex TimesFM-2.5 Toto-1.0 COSMIC Moirai-2.0 Chronos-Bolt TabPFN-TS Sundial Stat. Ensemble AutoARIMA AutoETS AutoTheta SeasonalNaive Naive Avg. Win Rate (%) 90.7 80.8 75.9 66.6 65.6 61.1 60.3 59.3 41.0 40.4 35.2 29.1 21.8 14.5 7.8 Skill Score (%) Median runtime (s) 3.6 1.4 16.9 90.7 34.4 2.5 1.0 305.5 35.6 690.6 186.8 17.0 9.3 2.3 2. 47.3 42.6 42.3 40.7 39.0 39.3 38.9 39.6 33.4 20.2 20.6 -26.8 5.5 0.0 -45.4 Leakage (%) 0 1 8 8 0 28 0 0 1 0 0 0 0 0 0 #Failures 0 0 0 0 0 0 0 2 0 11 10 3 0 0 0 Table 3: fev-bench results. The average win rate and skill score are computed with respect to the scaled quantile loss (SQL) metric. Higher values are better for both. Chronos-2 outperforms all existing pretrained models by substantial margin on this benchmark that includes univariate, multivariate, and covariate-informed forecasting tasks. Baseline results and the imputation strategy for handling data leakage in certain tasks are both taken from Shchur et al. (2025). Results for additional forecasting metrics are provided in Tables 7 to 9 (Appendix). We evaluated the base Chronos-2 model with 120M parameters on three comprehensive forecasting benchmarks: fev-bench (Shchur et al., 2025), GIFT-Eval (Aksu et al., 2024), and Chronos Benchmark II (Ansari et al., 2024). To contextualize its performance, we compared it against state-of-the-art time series foundation models that achieved the strongest results on these benchmarks. These include TiRex (Auer et al., 2025b), TimesFM-2.5 (Das et al., 2024b), Toto-1.0 (Cohen et al., 2025), Moirai-2.0 (Woo et al., 2024), TabPFN-TS (Hoo et al., 2025), COSMIC (Auer et al., 2025a), Sundial (Liu et al., 2025), and Chronos-Bolt (Ansari et al., 2024), the latest publicly released version of Chronos. As additional baselines, we also included AutoARIMA, AutoETS, AutoTheta, and their ensemble (Petropoulos & Svetunkov, 2020), representing well-established methods from the statistical forecasting literature (Hyndman & Athanasopoulos, 2018). We compare Chronos-2 only with the aforementioned models and exclude task-specific deep learning models from our evaluation, as prior studies (Aksu et al., 2024; Ansari et al., 2024) which include Technical Report of Chronos-2 GIFT-Eval and Chronos Benchmark II, two of the three benchmarks considered in our work have shown that pretrained models perform comparably to or better than task-specific models on average. (a) (b) Figure 2: The pairwise win rates (a) and skill scores (b) of the top-4 pretrained models on fev-bench with 95% confidence intervals (CIs) obtained through bootstrapping. Chronos-2 outperforms the next best models (TiRex and TimesFM) by statistically significant margin on both metrics. The complete plot and results for other forecasting metrics can be found in Figures 12 to 19 (Appendix). Following Shchur et al. (2025), we report both average win rates (W ) and skill scores (S) for all models. These metrics are mathematically equivalent to the average rank (R) and geometric mean relative error ( G) metrics used in prior work (Ansari et al., 2024; Aksu et al., 2024). Specifically, = 1 + (1 , where is the number of evaluated models. However, win rates and skill scores provide more interpretable summaries. The win rate measures the proportion of pairwise comparisons in which model outperforms other models, while the skill score reflects the average percentage improvement over baseline in our case, the Seasonal Naive model. For detailed discussion, we refer the reader to Shchur et al. (2025). 100 )(N 1) and = 1 100 fev-bench. This benchmark consists of 100 forecasting tasks and offers the most comprehensive coverage of diverse real-world scenarios, including tasks with covariates. None of these datasets or tasks were seen by Chronos-2 during training. Table 3 reports results on fev-bench with respect to the scaled quantile loss (SQL) metric which evaluates the probabilistic forecasting performance. Chronos-2 outperforms existing time series foundation models by significant margin, both in win rate and skill score. fev-bench also provides tooling to answer questions like: Does Model outperform Model in statistically significant way?. These pairwise comparisons with 95% confidence intervals (CIs), shown in Figure 2, further confirm that Chronos-2 surpasses the next best models (TiRex and TimesFM-2.5) by statistically significant margin. Specifically, the CIs of the pairwise win rates and skill scores of Chronos-2 against any baseline do not include 50% and 0%, respectively. 9 Technical Report of Chronos-2 Model Chronos-2 TimesFM-2.5 TiRex Toto-1.0 Moirai-2.0 COSMIC Chronos-Bolt TabPFN-TS Sundial AutoARIMA Seasonal Naive AutoTheta AutoETS Avg. Win Rate (%) Skill Score (%) 81.9 77.5 76.5 67.4 64.4 56.4 53.8 53.5 49.1 21.8 16.6 16.0 15.2 51.4 51.0 50.2 48.6 48.4 44.5 42.6 43.1 44.1 8.8 0.0 -24.4 -648.9 Model Chronos-2 TimesFM-2.5 TiRex Moirai-2.0 Toto-1.0 Chronos-Bolt Sundial COSMIC TabPFN-TS AutoARIMA AutoETS Seasonal Naive AutoTheta Avg. Win Rate (%) Skill Score (%) 83.8 77.7 71.9 64.3 61.3 58.4 53.4 51.9 45.4 24.4 19.5 19.4 18.5 30.2 29.5 27.6 27.2 25.2 19.2 25.0 20.8 16.6 -7.4 -21.2 0.0 -9.0 (a) (b) Table 4: GIFT-Eval results. The average win rate and skill score with respect to the (a) weighted quantile loss (WQL) and (b) mean absolute scaled error (MASE) metrics. Higher values are better for both. Chronos-2 outperforms previous best models, TimesFM-2.5 and TiRex. Baseline results have been taken from the GIFT-Eval leaderboard (Aksu et al., 2024). GIFT-Eval. The GIFT-Eval benchmark comprises 97 tasks derived from 55 datasets, with particular emphasis on high-frequency time series and long-horizon forecasting. The results in Table 4 show that Chronos-2 surpasses the previously leading models (TiRex and TimesFM-2.5) in win rate and skill score under both the weighted quantile loss (WQL) and mean absolute scaled error (MASE) metrics. When constructing the pretraining corpus for Chronos-2, we carefully ensured that it did not overlap with the test portions of any GIFT-Eval task at any sampling frequency. Nonetheless, the corpus does include partial overlap with the training portions of some GIFT-Eval datasets. For strictly zero-shot results, we refer the reader to Section 5.4, where we evaluate variant of Chronos-2 trained exclusively on synthetic data. Model Chronos-2 TiRex TimesFM-2.5 Toto-1.0 Moirai-2.0 Chronos-Bolt TabPFN-TS COSMIC Sundial Seasonal Naive Avg. Win Rate (%) Skill Score (%) 79.8 70.4 70.0 60.9 56.0 49.4 46.3 42.8 14.4 10.1 46.6 41.7 42.4 41.9 40.9 39.3 32.6 36.7 24.1 0.0 Model Chronos-2 TimesFM-2.5 TiRex Toto-1.0 Moirai-2.0 Chronos-Bolt COSMIC TabPFN-TS Sundial Seasonal Naive Avg. Win Rate (%) Skill Score (%) 81.5 71.6 67.1 58.0 53.5 50.6 42.0 40.1 21.8 13.8 26.5 23.3 22.2 22.3 19.8 20.4 18.1 10.5 9.5 0.0 (a) (b) Table 5: Chronos Benchmark II results. The average win rate and skill score with respect to the (a) weighted quantile loss (WQL) and (b) mean absolute scaled error (MASE) metrics. Higher values are better for both. Chronos-2 achieves the best results across all metrics. Chronos Benchmark II. Originally proposed in Ansari et al. (2024) to evaluate the first Chronos models, this benchmark comprises 27 tasks, the majority of which involve short histories (fewer than 300 time steps on average). None of these datasets were included in the training corpus of Chronos-2. On this benchmark, Chronos-2 consistently outperforms existing models in terms of the win rate and skill score under both probabilistic (WQL) and point (MASE) forecasting metrics, as shown in Table 5. Taken together, these results show that Chronos-2 not only outperforms all competing models across the three benchmarks but also substantially improves over Chronos-Bolt, its predecessor, highlighting the impact of the architectural and training improvements in Chronos-2. 10 Technical Report of Chronos-2 5.2 Improvements with In-context Learning The results in Section 5.1 correspond to Chronos-2 with in-context learning (ICL) enabled, specifically in the full cross learning mode described in Section 3.4. In this section, we disentangle the gains from ICL compared to univariate inference. To this end, we split fev-bench into three subsets: the univariate subset with 32 tasks involving single target time series without covariates, the multivariate subset with 26 tasks containing multiple targets but no covariates, and the covariates subset with 42 tasks that include at least one past-only or known covariate. We compare Chronos-2 with ICL to its univariate inference mode on these three subsets, as well as on GIFT-Eval and Chronos Benchmark II. In the univariate mode, each time series in the batch is forecast independently, and covariates, if present, are ignored. (a) (b) Figure 3: Chronos-2s probabilistic forecasting results in univariate mode and the corresponding improvements from in-context learning (ICL), shown as stacked bars on (a) the univariate subset of fev-bench, (b) GIFT-Eval, and (c) Chronos Benchmark II. For these univariate benchmarks, ICL enables cross-learning, allowing the model to share information across items within batch and thereby generate more accurate forecasts than univariate inference alone. Results for point forecasting metrics are available in Figure 9 (Appendix). (c) ICL provides improvements in skill score on univariate tasks, as shown in Figure 3. The effect is Univariate Tasks. especially strong on Chronos Benchmark II (Figure 3 (b)), which contains many tasks with short contexts. This demonstrates that Chronos-2 can leverage information from related time series to improve predictions when ICL is enabled, particularly when limited time series history is available. (a) (b) Figure 4: Chronos-2s probabilistic forecasting results in univariate mode and the corresponding gains from in-context learning (ICL), shown as stacked bars on the multivariate and covariates subsets of fev-bench. On multivariate tasks, ICL provides only modest improvements, though Chronos-2 in univariate mode already surpasses the multivariate Toto-1.0 model. On the covariates subset, however, ICL delivers the largest gains, demonstrating Chronos-2s ability to effectively use covariates. Besides Chronos-2, only TabPFN-TS and COSMIC support covariates, and Chronos-2 outperforms all baselines (including TabPFN-TS and COSMIC) by wide margin. Results for point forecasting metrics are available in Figures 10a and 10b (Appendix). Multivariate Tasks. On the multivariate subset of fev-bench, ICL yields only modest gains over univariate inference (Figure 4a (a)). Interestingly, in univariate mode, Chronos-2 even outperforms Toto-1.0, model which natively supports multivariate forecasting. This suggests that while these tasks involve multiple variates with potentially shared dynamics, the benefits of explicit multivariate modeling can be limited. One possible intuition comes from Takenss Embedding Theorem (Takens, 2006), which implies that the dynamics of system can often 11 Technical Report of Chronos-2 be reconstructed from delayed observations of single variable. In practice, this means that with sufficiently long histories, strong univariate model may capture much of the same structure as multivariate model. Similar empirical findings have been reported elsewhere; for example, Nie et al. (2023) observed that univariate (channel-independent) models often perform on par with multivariate (channel-dependent) models, albeit on different benchmark. Tasks with Covariates. The largest gains with ICL are observed on tasks with covariates (Figure 4a (b)). Here, the performance margin clearly demonstrates that Chronos-2 with ICL can effectively exploit covariates to improve predictions compared to univariate inference, which ignores them. Chronos-2 outperforms baselines by large margin on this subset. Unsurprisingly, the second spot is taken by TabPFN-TS, another model which supports (known) covariates. These results underscore both the strength of Chronos-2 and the limitations of existing pretrained models, most of which lack covariate support capability of immense practical importance. (a) (b) Figure 5: Comparison of Chronos-2 against baselines on tasks which include dynamic covariates from the energy and retail domains. Chronos-2 outperforms all baselines by wide margin, including TabPFN-TS and TiRex, the strongest baselines on the covariates subset of fev-bench (Figure 4b). For retail, we consider the domain-appropriate WQL metric. Results for point forecasting metrics are available in Figures 11a and 11b (Appendix). Figure 6: Forecasts generated by Chronos-2 in univariate mode (top), i.e., without covariates, and with in-context learning (second from top) on the energy price forecasting task. The dashed vertical gray line indicates the forecast start date and the shaded region represents 80% prediction interval around the median forecast. With ICL, Chronos-2 leverages Ampirion Load and Solar + Wind covariates to produce more accurate prediction. 5.3 Domain Case Studies We conducted further analysis on tasks from the energy and retail domains, where covariates often provide crucial information for accurate forecasting. For both domains, we selected all tasks with dynamic covariates from fev-bench resulting in 16 and 17 tasks for energy and retail, respectively (see Tables 10 and 11 in the Appendix for details). As baselines, we used TabPFN-TS and TiRex, the two strongest models on the covariates subset of fev-bench, as shown in Figure 4b. The results in Figures 5a and 5b demonstrate that Chronos-2 consistently outperforms these baselines by wide margin. Incorporating covariates provides substantial boost in performance for Chronos-2, reinforcing their critical role in real-world forecasting tasks. Consistent with Figure 4b, the second-best results are achieved by TabPFN-TS, another model capable of leveraging covariates. To illustrate how Chronos-2 with ICL uses covariates, we compared forecasts produced in univariate mode versus with ICL. We selected one task from each domain where ICL delivers the largest gains. 12 Technical Report of Chronos-2 Figure 7: Forecasts generated by Chronos-2 in univariate mode (top), i.e., without covariates, and with in-context learning (second from top) on the Rossmann sales forecasting task. The dashed vertical gray line indicates the forecast start date and the shaded region represents 80% prediction interval around the median forecast. With ICL, Chronos-2 produces substantially more accurate forecast by capturing the influence of promotion and holiday covariates on future sales. Figure 6 shows forecasts on the energy price forecasting task for Germany (EPF-DE), where the goal is to predict the hourly energy price for the next day using historical prices, day-ahead forecasts of the load and renewable (solar and wind) energy generation. In the univariate mode, Chronos-2 makes reasonable but imprecise predictions. However, with ICL, Chronos-2 effectively uses the covariates, producing significantly more accurate predictions. The retail task in Figure 7 involves predicting next quarters weekly store sales of Rossmann, European drug store chain, using historical sales and covariates: historical customer footfall plus known covariates indicating store operation, promotion periods, and holidays. Chronos-2s univariate forecast is nearly flat with high uncertainty. In contrast, the ICL forecast leverages covariates particularly promotion and holiday information to capture the true sales dynamics over the forecast horizon. 5.4 Ablation Studies (b) Figure 8: Comparison of the main Chronos-2 model (120M parameters) with (a) smaller 28M-parameter model, (b) model trained exclusively on synthetic data, and (c) the main model prior to long-context post-training. (a) (c) Technical Report of Chronos-2 In this section, we present additional experiments and ablations that disentangle the impact of different design choices. We investigate the performance of Chronos-2 across different parameter counts, evaluate models trained exclusively on synthetic data, and demonstrate the importance of post-training on long-context scenarios. Model Size. We trained small model with 28M parameters to understand the impact of model size on forecasting performance. As shown in Figure 8a, the small model delivers strong performance despite its reduced size. On GIFT-Eval, for instance, its skill score lags the base model by as little as 1% points, while offering nearly 2 faster inference. This makes it particularly suitable for low-resource environments, such as CPU-only settings, or applications where inference speed is prioritized over maximum forecast accuracy. Synthetic time series data has played pivotal role in advancing pretrained forecasting Synthetic Data Only. models (Ansari et al., 2024; Das et al., 2024b). TabPFN-TS (Hoo et al., 2025) demonstrated that strong performance is achievable even when training relies exclusively on synthetic data. To examine the limits of this approach, we trained version of Chronos-2 using only synthetic data. On Chronos Benchmark II and GIFT-Eval, this model (Chronos-2-Synth) performs only slightly below the version with real data in its pretraining corpus (Figure 8b). It also delivers strong results on fev-bench, though with larger performance gap. These results underscore the importance of synthetic data, suggesting that with further research, real data may not even be required for effective pretraining. Long context Post-training. As described in Section 3.3, Chronos-2 is initially trained with context length of 2,048 time steps and then post-trained with an extended context of 8,192 steps. Figure 8c compares the base model (denoted Chronos-2-2K) with the post-trained variant. Extending the context length yields gains, particularly on the GIFT-Eval benchmark, which contains many high-frequency datasets with long seasonal periods."
        },
        {
            "title": "6 Discussion",
            "content": "We introduced Chronos-2, pretrained time series model designed to handle wide range of forecasting scenarios including univariate, multivariate, and covariate-informed tasks in zero-shot manner. Across three comprehensive benchmarks, Chronos-2 consistently outperforms existing foundation models, demonstrating that in-context learning enhances forecasting performance across diverse task types. particularly large performance gap appears on covariate-informed tasks, where Chronos-2 substantially surpasses prior foundation models. This highlights both the limitations of existing models and the critical role contextual information (e.g., covariates) plays in accurate forecasting. While Chronos-2 supports only numeric and categorical covariates, extending pretrained models to incorporate multimodal inputs, such as text, represents promising direction for future research (Zhang et al., 2025). Our results further emphasize the importance of synthetic data in enabling generalist forecasting. The abilities of Chronos-2 beyond univariate forecasting rely entirely on synthetic data, and ablation studies show that models trained solely on synthetic data perform only slightly worse than those trained on mixture of real and synthetic datasets. We expect synthetic data to play an increasingly central role in advancing pretrained time series models. Finally, the flexible group attention mechanism in Chronos-2 opens opportunities for further applications. For instance, time series could be grouped using sparse metadata or dense embeddings to enable retrieval-augmented forecasting, potentially improving performance in small-data or cold-start scenarios."
        },
        {
            "title": "Acknowledgements",
            "content": "We thank the developers of open-source libraries used in the development of Chronos-2, including but not limited to torch (Paszke et al., 2019), numpy (Harris et al., 2020), pandas (pandas development team, 2020; Wes McKinney, 2010), statsmodels (Seabold & Perktold, 2010), transformers (Wolf et al., 2020), gluonts (Alexandrov et al., 2020), autogluon (Shchur et al., 2023), statsforecast (Garza et al., 2022), einops (Rogozhnikov, 2022) and scikit-learn (Pedregosa et al., 2011). We also thank our colleagues at Amazon for their invaluable support in releasing Chronos-2: Kevin Ormiston, Jenna Larson, Larry Hardesty, Divya Sukumar, Lahari Chowtoori and Henri Yandell. Finally, we are grateful to our fellow researchers for insightful discussions and their contributions to the field: Andrew Gordon Wilson, Michael Mahoney, Dmitry Efimov, Christoph Bergmeir, Valentin Flunkert, David Salinas, 14 Technical Report of Chronos-2 Imry Kissos, Devamanyu Hazarika, Tim Januschowski, Jan Gasthaus, William Gilpin, Annan Yu, Zelin He, Kashif Rasul, Rajat Sen, Yichen Zhou, Chenghao Liu, Taha Aksu, Gerald Woo, Emaad Khwaja and Ben Cohen."
        },
        {
            "title": "References",
            "content": "Taha Aksu, Gerald Woo, Juncheng Liu, Xu Liu, Chenghao Liu, Silvio Savarese, Caiming Xiong, and Doyen Sahoo. Gift-Eval: benchmark for general time series forecasting model evaluation. arXiv preprint arXiv:2410.10393, 2024. 1, 2, 7, 8, 9, 10 Alexander Alexandrov, Konstantinos Benidis, Michael Bohlke-Schneider, Valentin Flunkert, Jan Gasthaus, Tim Januschowski, Danielle Maddix, Syama Rangapuram, David Salinas, Jasper Schulz, et al. GluonTS: Probabilistic and Neural Time Series Modeling in Python. The Journal of Machine Learning Research, 21(1):46294634, 2020. 14 Abdul Fatir Ansari, Lorenzo Stella, Ali Caner Turkmen, Xiyuan Zhang, Pedro Mercado, Huibin Shen, Oleksandr Shchur, Syama Sundar Rangapuram, Sebastian Pineda Arango, Shubham Kapoor, Jasper Zschiegner, Danielle C. Maddix, Hao Wang, Michael W. Mahoney, Kari Torkkola, Andrew Gordon Wilson, Michael Bohlke-Schneider, and Bernie Wang. Chronos: Learning the language of time series. Transactions on Machine Learning Research, 2024. ISSN 2835-8856. 1, 2, 4, 7, 8, 9, 10, 14, 20 Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Lučić, and Cordelia Schmid. Vivit: video vision transformer. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 68366846, 2021. 4 V. Assimakopoulos and K. Nikolopoulos. The theta model: decomposition approach to forecasting. International Journal of Forecasting, 16(4):521530, 2000. Andreas Auer, Raghul Parthipan, Pedro Mercado, Abdul Fatir Ansari, Lorenzo Stella, Bernie Wang, Michael BohlkeSchneider, and Syama Sundar Rangapuram. Zero-shot time series forecasting with covariates via in-context learning. arXiv preprint arXiv:2506.03128, 2025a. 4, 8 Andreas Auer, Patrick Podest, Daniel Klotz, Sebastian Böck, Günter Klambauer, and Sepp Hochreiter. TiRex: Zero-shot forecasting across long and short horizons with enhanced in-context learning. In Advances in Neural Information Processing Systems, 2025b. 4, 8 Fouad Bahrpeyma, Mark Roantree, Paolo Cappellari, Michael Scriney, and Andrew McCarren. methodology for validating diversity in synthetic time series generation. MethodsX, 8:101459, 2021. 7 Marta Bańbura, Domenico Giannone, and Lucrezia Reichlin. Large bayesian vector auto regressions. Journal of applied Econometrics, 25(1):7192, 2010. John Burbidge, Lonnie Magee, and Leslie Robb. Alternative transformations to handle extreme values of the dependent variable. Journal of the American statistical Association, 83(401):123127, 1988. 4 Cristian Challu, Kin Olivares, Boris Oreshkin, Federico Garza Ramirez, Max Mergenthaler Canseco, and Artur Dubrawski. N-HiTS: Neural Hierarchical Interpolation for Time Series Forecasting. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, 2023. 1, 3 Ben Cohen, Emaad Khwaja, Youssef Doubli, Salahidine Lemaachi, Chris Lettieri, Charles Masson, Hugo Miccinilli, Elise Ramé, Qiqi Ren, Afshin Rostamizadeh, et al. This time is different: An observability perspective on time series foundation models. In Advances in Neural Information Processing Systems, 2025. 1, 4, 8 Abhimanyu Das, Matthew Faw, Rajat Sen, and Yichen Zhou. In-context fine-tuning for time-series foundation models. arXiv preprint arXiv:2410.24087, 2024a. Abhimanyu Das, Weihao Kong, Rajat Sen, and Yichen Zhou. decoder-only foundation model for time-series forecasting. In International Conference on Machine Learning, 2024b. 1, 4, 8, 14 Patrick Emami, Abhijeet Sahu, and Peter Graf. BuildingsBench: Large-Scale Dataset of 900K Buildings and Benchmark for Short-Term Load Forecasting. arXiv:2307.00142, 2023. 20 15 Technical Report of ChronosFiveThirtyEight. uber-tlc-foil-response: Uber trip data from freedom of information request to NYCs Taxi & Limousine Commission. https://github.com/fivethirtyeight/uber-tlc-foil-response, 2025. Accessed: 2025-09-26. 20 Azul Garza, Cristian Challu, and Max Mergenthaler-Canseco. Timegpt-1, 2024. 4 Federico Garza, Max Mergenthaler Canseco, Cristian Challú, and Kin G. Olivares. StatsForecast: Lightning fast forecasting with statistical and econometric models. PyCon Salt Lake City, Utah, US 2022, 2022. URL https: //github.com/Nixtla/statsforecast. 14 Rakshitha Godahewa, Christoph Bergmeir, Geoffrey I. Webb, Rob J. Hyndman, and Pablo Montero-Manso. Monash Time Series Forecasting Archive. In Neural Information Processing Systems Track on Datasets and Benchmarks, 2021. 20 Nate Gruver, Marc Finzi, Shikai Qiu, and Andrew Gordon Wilson. Large Language Models Are Zero-Shot Time Series Forecasters. In Advances in Neural Information Processing Systems, 2023. Charles R. Harris, K. Jarrod Millman, Stéfan J. van der Walt, Ralf Gommers, Pauli Virtanen, David Cournapeau, Eric Wieser, Julian Taylor, Sebastian Berg, Nathaniel J. Smith, Robert Kern, Matti Picus, Stephan Hoyer, Marten H. van Kerkwijk, Matthew Brett, Allan Haldane, Jaime Fernández del Río, Mark Wiebe, Pearu Peterson, Pierre Gérard-Marchant, Kevin Sheppard, Tyler Reddy, Warren Weckesser, Hameer Abbasi, Christoph Gohlke, and Travis E. Oliphant. Array programming with NumPy. Nature, 585(7825):357362, September 2020. doi: 10.1038/ s41586-020-2649-2. URL https://doi.org/10.1038/s41586-020-2649-2. 14 Shi Bin Hoo, Samuel Müller, David Salinas, and Frank Hutter. From tables to time: How TabPFN-v2 outperforms specialized time series forecasting models. arXiv preprint arXiv:2501.02945, 2025. 4, 8, 14 Rob Hyndman and George Athanasopoulos. Forecasting: principles and practice. OTexts, 2018. 1, 3, 8 Jiawei Jiang, Chengkai Han, Wenjun Jiang, Wayne Xin Zhao, and Jingyuan Wang. Libcity: unified library towards efficient and comprehensive urban spatial-temporal prediction. arXiv preprint arXiv:2304.14343, 2023. Ming Jin, Shiyu Wang, Lintao Ma, Zhixuan Chu, James Y. Zhang, Xiaoming Shi, Pin-Yu Chen, Yuxuan Liang, YuanFang Li, Shirui Pan, and Qingsong Wen. Time-LLM: Time series forecasting by reprogramming large language models. In The Twelfth International Conference on Learning Representations, 2024. 4 Xiaoyong Jin, Youngsuk Park, Danielle Maddix, Hao Wang, and Yuyang Wang. Domain adaptation for time series forecasting via attention sharing. In International Conference on Machine Learning, pp. 1028010297. PMLR, 2022. 4 Bryan Lim, Sercan Ö Arık, Nicolas Loeff, and Tomas Pfister. Temporal fusion transformers for interpretable multihorizon time series forecasting. International Journal of Forecasting, 37(4):17481764, 2021. 1, 3 Xu Liu, Yutong Xia, Yuxuan Liang, Junfeng Hu, Yiwei Wang, Lei Bai, Chao Huang, Zhenguang Liu, Bryan Hooi, and Roger Zimmermann. Largest: benchmark dataset for large-scale traffic forecasting. arXiv:2306.08259, 2023. 20 Yong Liu, Guo Qin, Zhiyuan Shi, Zhi Chen, Caiyin Yang, Xiangdong Huang, Jianmin Wang, and Mingsheng Long. Sundial: family of highly capable time series foundation models. In International Conference on Machine Learning, 2025. 4, Spyros Makridakis, Evangelos Spiliotis, and Vassilios Assimakopoulos. The M4 Competition: 100,000 time series and 61 forecasting methods. International Journal of Forecasting, 36(1):5474, 2020. 20 Daniele Micci-Barreca. preprocessing scheme for high-cardinality categorical attributes in classification and prediction problems. ACM SIGKDD explorations newsletter, 3(1):2732, 2001. 4 Yuqi Nie, Nam H. Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam. time series is worth 64 words: Long-term forecasting with transformers. In International Conference on Learning Representations, 2023. 3, 4, 5, Boris N. Oreshkin, Dmitri Carpov, Nicolas Chapados, and Yoshua Bengio. N-BEATS: Neural basis expansion analysis for interpretable time series forecasting. In International Conference on Learning Representations, 2020. 3 16 Technical Report of Chronos-2 Boris N. Oreshkin, Dmitri Carpov, Nicolas Chapados, and Yoshua Bengio. Meta-learning framework with applications to zero-shot time-series forecasting. In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), 2021. 3 Bernardo Pérez Orozco and Stephen J. Roberts. Zero-shot and few-shot time series forecasting with ordinal regression recurrent neural networks. In 28th European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning, pp. 503508, 2020. The pandas development team. pandas-dev/pandas: Pandas, February 2020. URL https://doi.org/10.5281/ zenodo.3509134. 14 Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Köpf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32, 2019. 14 Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, et al. Scikit-learn: Machine learning in python. the Journal of machine Learning research, 12:28252830, 2011. 4, 14 Fotios Petropoulos and Ivan Svetunkov. simple combination of univariate models. International journal of forecasting, 36(1):110115, 2020. Fotios Petropoulos, Daniele Apiletti, Vassilios Assimakopoulos, Mohamed Zied Babai, Devon Barrow, Souhaib Ben Taieb, Christoph Bergmeir, Ricardo Bessa, Jakub Bijak, John Boylan, et al. Forecasting: theory and practice. International Journal of forecasting, 38(3):705871, 2022. 2 Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. The Journal of Machine Learning Research, 21(1):54855551, 2020. 5 Syama Sundar Rangapuram, Matthias Seeger, Jan Gasthaus, Lorenzo Stella, Yuyang Wang, and Tim Januschowski. Deep state space models for time series forecasting. Advances in neural information processing systems, 31, 2018. 3 Roshan Rao, Jason Liu, Robert Verkuil, Joshua Meier, John Canny, Pieter Abbeel, Tom Sercu, and Alexander Rives. Msa transformer. In International conference on machine learning, pp. 88448856. PMLR, 2021. 4 Stephan Rasp, Peter Dueben, Sebastian Scher, Jonathan Weyn, Soukayna Mouatadid, and Nils Thuerey. Weatherbench: benchmark data set for data-driven weather forecasting. Journal of Advances in Modeling Earth Systems, 12(11):e2020MS002203, 2020. Kashif Rasul, Calvin Seward, Ingmar Schuster, and Roland Vollgraf. Autoregressive denoising diffusion models for multivariate probabilistic time series forecasting. In International Conference on Machine Learning, pp. 88578868. PMLR, 2021. 3 Kashif Rasul, Arjun Ashok, Andrew Robert Williams, Arian Khorasani, George Adamopoulos, Rishika Bhagwatkar, Marin Biloš, Hena Ghonia, Nadhir Vincent Hassen, Anderson Schneider, Sahil Garg, Alexandre Drouin, Nicolas Chapados, Yuriy Nevmyvaka, and Irina Rish. Lag-llama: Towards foundation models for time series forecasting, 2023. 4 Alex Rogozhnikov. Einops: Clear and reliable tensor manipulations with einstein-like notation. In International Conference on Learning Representations, 2022. 14 Jakob Runge, Andreas Gerhardus, Gherardo Varando, Veronika Eyring, and Gustau Camps-Valls. Causal inference for time series. Nature Reviews Earth & Environment, 4(7):487505, 2023. David Salinas, Michael Bohlke-Schneider, Laurent Callot, Roberto Medico, and Jan Gasthaus. High-dimensional multivariate forecasting with low-rank gaussian copula processes. Advances in neural information processing systems, 32, 2019. 20 17 Technical Report of Chronos-2 David Salinas, Valentin Flunkert, Jan Gasthaus, and Tim Januschowski. Deepar: Probabilistic forecasting with autoregressive recurrent networks. International Journal of Forecasting, 36(3):11811191, 2020. 3 Skipper Seabold and Josef Perktold. statsmodels: Econometric and statistical modeling with python. In 9th Python in Science Conference, 2010. 14 Oleksandr Shchur, Ali Caner Turkmen, Nick Erickson, Huibin Shen, Alexander Shirkov, Tony Hu, and Bernie Wang. Autogluontimeseries: Automl for probabilistic time series forecasting. In International Conference on Automated Machine Learning, pp. 91. PMLR, 2023. 14 Oleksandr Shchur, Abdul Fatir Ansari, Caner Turkmen, Lorenzo Stella, Nick Erickson, Pablo Guerron, Michael Bohlke-Schneider, and Bernie Wang. fev-bench: realistic benchmark for time series forecasting models. arXiv preprint arXiv:, 2025. 2, 8, 9 Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. 5 Floris Takens. Detecting strange attractors in turbulence. In Dynamical Systems and Turbulence, Warwick 1980: proceedings of symposium held at the University of Warwick 1979/80, pp. 366381. Springer, 2006. 11 Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open Foundation and Fine-Tuned Chat Models, 2023. 5 Bartosz Uniejewski and Rafał Weron. Efficient forecasting of electricity spot prices with expert and lasso models. Energies, 11(8):2039, 2018. 4 Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention Is All You Need. In Advances in Neural Information Processing Systems, 2017. Wes McKinney. Data Structures for Statistical Computing in Python. In Stéfan van der Walt and Jarrod Millman (eds.), Proceedings of the 9th Python in Science Conference, pp. 56 61, 2010. doi: 10.25080/Majora-92bf1922-00a. 14 Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pp. 3845. Association for Computational Linguistics, 2020. 14 Gerald Woo, Chenghao Liu, Akshat Kumar, and Doyen Sahoo. Pushing the limits of pre-training for time series forecasting in the cloudops domain. arXiv preprint arXiv:2310.05063, 2023. 20 Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, and Doyen Sahoo. Unified training of universal time series forecasting transformers. In International Conference on Machine Learning, 2024. 4, 7, 8 Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks. In International Conference on Learning Representations, 2024. 5 Xiyuan Zhang, Boran Han, Haoyang Fang, Abdul Fatir Ansari, Shuai Zhang, Danielle Maddix, Cuixiong Hu, Andrew Gordon Wilson, Michael Mahoney, Hao Wang, et al. Does multimodality lead to better time series forecasting? arXiv preprint arXiv:2506.21611, 2025. 14 18 Technical Report of Chronos-2 Yunhao Zhang and Junchi Yan. Crossformer: Transformer utilizing cross-dimension dependency for multivariate time series forecasting. In The eleventh international conference on learning representations, 2023. Nina Żukowska, Mononito Goswami, Michał Wiliński, Willa Potosnak, and Artur Dubrawski. Towards long-context time series foundation models. arXiv preprint arXiv:2409.13530, 2024. 4 19 Technical Report of Chronos-"
        },
        {
            "title": "A Training Data",
            "content": "Dataset Name Electricity KDD Cup (2018) M4 (Daily) M4 (Hourly) M4 (Monthly) M4 (Weekly) Mexico City Bikes Pedestrian Counts Solar Taxi Uber TLC USHCN Weatherbench Wiki Wind Farms Temperature-Rain London Smart Meters Alibaba Cluster Trace (2018) Azure VM Traces (2017) Borg Cluster Data (2011) LargeST (2017) Q-Traffic Buildings 900K Frequencies 15min, 1H, 1W, 1D 1H, 1D 1D 1H 1M 1W 1H, 1D, 1W 1H, 1D, 1W 5min, 10min, 1H 30min, 1H 1H, 1D 1D, 1W 1H, 1D, 1W 1H, 1D, 1W 1H, 1D 1D 30min, 1D 5min, 1H 5min, 1H 5min, 1H 1H, 1D 15min, 1H 1H, 1D # Time Series Domain 370 Energy 270 Nature 4227 Various 414 Various 48000 Various 359 Various 494 66 5166"
        },
        {
            "title": "225280 Nature\n225280 Nature\n100000 Web",
            "content": "337 Energy 32072 Nature Energy 5560 Source Godahewa et al. (2021) Godahewa et al. (2021) Makridakis et al. (2020) Makridakis et al. (2020) Makridakis et al. (2020) Makridakis et al. (2020) Ansari et al. (2024) Godahewa et al. (2021) Ansari et al. (2024) Salinas et al. (2019) FiveThirtyEight (2025) Ansari et al. (2024) Rasp et al. (2020) Ansari et al. (2024) Godahewa et al. (2021) Godahewa et al. (2021) Godahewa et al. (2021)"
        },
        {
            "title": "100000 Cloud Ops Woo et al. (2023)\n100000 Cloud Ops Woo et al. (2023)\n100000 Cloud Ops Woo et al. (2023)",
            "content": "8196"
        },
        {
            "title": "Transport\nTransport\nEnergy",
            "content": "Liu et al. (2023) Jiang et al. (2023) Emami et al. (2023) Table 6: Real univariate datasets used for pretraining Chronos-2."
        },
        {
            "title": "B Additional Results",
            "content": "(a) (b) Figure 9: Chronos-2s point forecasting results in univariate mode and the corresponding improvements from in-context learning (ICL), shown as stacked bars on (a) the univariate subset of fev-bench, (b) GIFT-Eval, and (c) Chronos Benchmark II. 20 Technical Report of Chronos-2 Model Chronos-2 TiRex TimesFM-2.5 Toto-1.0 Moirai-2.0 COSMIC Chronos-Bolt TabPFN-TS Sundial Stat. Ensemble AutoARIMA AutoTheta AutoETS SeasonalNaive Naive Avg. Win Rate (%) 87.9 75.1 74.4 64.3 58.7 58.6 57.9 55.7 49.8 44.2 32.1 30.3 30.2 16.7 14.0 Skill Score (%) Median runtime (s) 3.6 1.4 16.9 90.7 2.5 34.4 1.0 305.5 35.6 690.6 186.8 9.3 17.0 2.3 2.2 35.5 30.0 30.3 28.2 27.3 25.7 26.5 27.6 24.7 15.7 11.2 11.0 2.3 0.0 -16.7 Leakage (%) 0 1 8 8 28 0 0 0 1 0 0 0 0 0 0 #Failures 0 0 0 0 0 0 0 2 0 11 10 0 3 0 Table 7: fev-bench results. The average win rate and skill score are computed with respect to the mean absolute scaled error (MASE) metric on fev-bench. Higher values are better for both. Model Chronos-2 TiRex TimesFM-2.5 Toto-1.0 COSMIC TabPFN-TS Moirai-2.0 Chronos-Bolt Sundial Stat. Ensemble AutoARIMA AutoETS AutoTheta SeasonalNaive Naive Avg. Win Rate (%) 88.5 79.0 76.8 67.6 65.2 64.8 62.8 60.5 41.9 38.3 34.6 26.8 21.3 14.1 7.8 Skill Score (%) Median runtime (s) 3.6 1.4 16.9 90.7 34.4 305.5 2.5 1.0 35.6 690.6 186.8 17.0 9.3 2.3 2.2 51.5 46.7 46.8 45.0 43.7 45.8 43.9 43.2 37.4 21.8 23.4 -27.0 7.8 0.0 -39. Leakage (%) 0 1 8 8 0 0 28 0 1 0 0 0 0 0 0 #Failures 0 0 0 0 0 2 0 0 0 11 10 3 0 0 0 Table 8: fev-bench results. The average win rate and skill score are computed with respect to the weighted quantile loss (WQL) metric on fev-bench. Higher values are better for both. 21 Technical Report of Chronos-2 Model Chronos-2 TimesFM-2.5 TiRex Toto-1.0 TabPFN-TS COSMIC Moirai-2.0 Chronos-Bolt Sundial Stat. Ensemble AutoETS AutoARIMA AutoTheta Naive SeasonalNaive Avg. Win Rate (%) 85.4 74.1 73.7 65.1 61.5 60.5 59.6 58.0 47.7 43.0 30.8 30.8 27.2 17.5 15.2 Skill Score (%) Median runtime (s) 3.6 16.9 1.4 90.7 305.5 34.4 2.5 1.0 35.6 690.6 17.0 186.8 9.3 2.2 2.3 39.4 33.8 33.6 31.5 33.4 30.1 30.7 29.8 27.3 17.7 4.3 13.3 13.8 -6.1 0.0 Leakage (%) 0 8 1 8 0 0 28 0 1 0 0 0 0 0 0 #Failures 0 0 0 0 2 0 0 0 0 11 3 10 0 0 Table 9: fev-bench results. The average win rate and skill score are computed with respect to the weighted absolute percentage error (WAPE) metric on fev-bench. Higher values are better for both. Figure 10: Chronos-2s point forecasting results in univariate mode and the corresponding gains from in-context learning (ICL), shown as stacked bars on the multivariate and covariates subsets of fev-bench. (a) (b) (a) (b) Figure 11: Comparison of Chronos-2 against baselines on tasks which include dynamic covariates from the energy and retail domains. For retail, we consider the domain-appropriate WAPE metric. 22 Technical Report of Chronos-2 Figure 12: The pairwise win rates for all models on fev-bench with 95% confidence intervals (CIs) with respect to SQL metric. 23 Technical Report of ChronosFigure 13: The pairwise skill scores for all models on fev-bench with 95% confidence intervals (CIs) with respect to SQL metric. 24 Technical Report of Chronos-2 Figure 14: The pairwise win rates for all models on fev-bench with 95% confidence intervals (CIs) with respect to WQL metric. 25 Technical Report of ChronosFigure 15: The pairwise skill scores for all models on fev-bench with 95% confidence intervals (CIs) with respect to WQL metric. 26 Technical Report of Chronos-2 Figure 16: The pairwise win rates for all models on fev-bench with 95% confidence intervals (CIs) with respect to MASE metric. 27 Technical Report of ChronosFigure 17: The pairwise skill scores for all models on fev-bench with 95% confidence intervals (CIs) with respect to MASE metric. 28 Technical Report of Chronos-2 Figure 18: The pairwise win rates for all models on fev-bench with 95% (CIs) with respect to WAPE metric. 29 Technical Report of ChronosFigure 19: The pairwise skill scores for all models on fev-bench with 95% confidence intervals (CIs) with respect to WAPE metric. 30 Technical Report of Chronos-2 Freq. 15T 30T H H 15T Task ENTSO-e Load ENTSO-e Load ENTSO-e Load EPF-BE EPF-DE EPF-FR EPF-NP EPF-PJM GFC12 GFC14 GFC17 Solar with Weather Solar with Weather KDD Cup 2022 10T KDD Cup 2022 30T KDD Cup 2022 Median length # series 6 96 6 96 6 168 1 24 1 24 1 24 1 24 1 24 11 168 1 168 8 168 1 96 1 24 134 14 134 288 134 175,292 87,645 43,822 52,416 52,416 52,416 52,416 52,416 39,414 17,520 17,544 198,600 49,648 243 35,279 11,758 20 20 20 20 20 20 20 20 10 20 20 20 20 10 10 10 # targets 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 # past cov. 0 0 0 0 0 0 0 0 0 0 0 2 2 9 9 9 # known cov. 3 3 3 2 2 2 2 2 1 1 1 7 7 0 0 0 # static cov. 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 Table 10: Subset of datasets from fev-bench with dynamic covariates for the energy domain case study. Freq. Median length # series Task 1,579 Favorita Store Sales 1,579 Favorita Store Sales 1,579 Favorita Store Sales 51 Favorita Transactions 51 Favorita Transactions 51 Favorita Transactions 30,490 M5 30,490 M5 30,490 M5 7 Rohlik Orders 7 Rohlik Orders 5,243 Rohlik Sales 5,390 Rohlik Sales 1,115 Rossmann 1,115 Rossmann 2,936 Walmart 10,000 Hermes 54 240 1,688 54 240 1,688 58 257 1,810 170 1,197 150 1,046 133 942 143 261 2 10 10 2 10 10 1 1 1 5 5 1 1 8 10 1 1 12 13 28 12 13 28 12 13 28 8 61 8 14 13 48 39 52 # targets 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 # past cov. 1 1 1 1 1 1 0 0 0 9 9 1 1 1 1 0 0 # known cov. 1 1 2 0 0 1 8 8 8 4 4 13 13 4 5 10 1 # static cov. 6 6 6 5 5 5 5 5 5 0 0 7 7 10 10 4 2 Table 11: Subset of datasets from fev-bench with dynamic covariates for the retail domain case study."
        }
    ],
    "affiliations": [
        "Amazon",
        "Amazon Web Services",
        "Boston College",
        "Johannes Kepler University Linz",
        "Rutgers University",
        "University of Freiburg"
    ]
}