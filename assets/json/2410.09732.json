{
    "paper_title": "LOKI: A Comprehensive Synthetic Data Detection Benchmark using Large Multimodal Models",
    "authors": [
        "Junyan Ye",
        "Baichuan Zhou",
        "Zilong Huang",
        "Junan Zhang",
        "Tianyi Bai",
        "Hengrui Kang",
        "Jun He",
        "Honglin Lin",
        "Zihao Wang",
        "Tong Wu",
        "Zhizheng Wu",
        "Yiping Chen",
        "Dahua Lin",
        "Conghui He",
        "Weijia Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "With the rapid development of AI-generated content, the future internet may be inundated with synthetic data, making the discrimination of authentic and credible multimodal data increasingly challenging. Synthetic data detection has thus garnered widespread attention, and the performance of large multimodal models (LMMs) in this task has attracted significant interest. LMMs can provide natural language explanations for their authenticity judgments, enhancing the explainability of synthetic content detection. Simultaneously, the task of distinguishing between real and synthetic data effectively tests the perception, knowledge, and reasoning capabilities of LMMs. In response, we introduce LOKI, a novel benchmark designed to evaluate the ability of LMMs to detect synthetic data across multiple modalities. LOKI encompasses video, image, 3D, text, and audio modalities, comprising 18K carefully curated questions across 26 subcategories with clear difficulty levels. The benchmark includes coarse-grained judgment and multiple-choice questions, as well as fine-grained anomaly selection and explanation tasks, allowing for a comprehensive analysis of LMMs. We evaluated 22 open-source LMMs and 6 closed-source models on LOKI, highlighting their potential as synthetic data detectors and also revealing some limitations in the development of LMM capabilities. More information about LOKI can be found at https://opendatalab.github.io/LOKI/"
        },
        {
            "title": "Start",
            "content": "4 2 0 2 3 1 ] . [ 1 2 3 7 9 0 . 0 1 4 2 : r LOKI: COMPREHENSIVE SYNTHETIC DATA DETECTION BENCHMARK USING LARGE MULTIMODAL MODELS Junyan Ye1,2, Baichuan Zhou2* , Zilong Huang1* , Junan Zhang5* , Tianyi Bai2 , Hengrui Kang2 , Jun He1 , Honglin Lin2 , Zihao Wang1 , Tong Wu4 , Zhizheng Wu5 , Yiping Chen1 , Dahua Lin2,4 , Conghui He2,3, Weijia Li1 1 Sun Yat-sen University, 2 Shanghai AI Laboratory, 3 SenseTime Research, 4 The Chinese University of Hong Kong, 5 The Chinese University of Hong Kong (Shenzhen) Figure 1: Overview of LOKI benchmark. LOKI possesses four key characteristics: 1) Diverse modalities (video, image, 3D, text and audio); 2) Heterogeneous categories (26 detailed subcategories); 3) Multi-level annotations; 4) Multimodal synthetic data evaluation framework."
        },
        {
            "title": "ABSTRACT",
            "content": "With the rapid development of AI-generated content, the future internet may be inundated with synthetic data, making the discrimination of authentic and credible multimodal data increasingly challenging. Synthetic data detection has thus garnered widespread attention, and the performance of large multimodal models (LMMs) in this task has attracted significant interest. LMMs can provide natural language explanations for their authenticity judgments, enhancing the explainability of synthetic content detection. Simultaneously, the task of distinguishing between real and synthetic data effectively tests the perception, knowledge, and reasoning capabilities of LMMs. In response, we introduce LOKI, novel benchmark designed to evaluate the ability of LMMs to detect synthetic data across multiple modalities. LOKI encompasses video, image, 3D, text, and audio modalities, comprising 18K carefully curated questions across 26 subcategories with clear difficulty levels. The benchmark includes coarse-grained judgment and multiple-choice questions, as well as fine-grained anomaly selection and explanation tasks, allowing for comprehensive analysis of LMMs. We evaluated 22 open-source LMMs and 6 closed-source models on LOKI, highlighting their potential as synthetic data detectors and also revealing some limitations in the development of LMM capabilities. More information about LOKI can be found at https://opendatalab.github.io/LOKI/. These authors contributed equally to this work. Corresponding author(s). E-mail(s): liweij29@mail.sysu.edu.cn, heconghui@pjlab.org.cn"
        },
        {
            "title": "INTRODUCTION",
            "content": "With the rapid development of diffusion models (Rombach et al., 2022; Dhariwal & Nichol, 2021b) and large language models (Abdullah et al., 2022; Brown, 2020), AI-generated content (AIGC) technology has increasingly integrated synthetic multimodal data into our daily lives. For instance, tools like SORA (Brooks et al., 2024) can produce highly realistic video, while Suno (Shulman et al., 2022) enables the creation of music at level comparable to professional artists. However, synthetic multimodal data also brings significant risks, including potential misuse and societal disruption (Cooke et al., 2024; Ju et al., 2022). For example, the risks include generating fake news using large language models (LLMs), synthesizing fraudulent faces with diffusion models for scams, and potential contamination of internet training data. Due to the convenience of artificial intelligence synthesis, the future Internet may be saturated with AI-generated content, making the task of discerning the authenticity and trustworthiness of multimodal data increasingly challenging. To address such threats, the field of synthetic data detection has garnered widespread attention in recent years (Barni et al., 2020; Frank et al., 2020; Gragnaniello et al., 2021; Hou et al., 2023). However, most current synthetic data detection methods are primarily focused on authenticity evaluation, with certain limitations regarding the human interpretability of the prediction results (Li et al., 2024b). The recent rapid advancement of large multimodal models (LMMs) has sparked curiosity about their performance in detecting synthetic multimodal data (Ku et al., 2023; Wu et al., 2024b). On one hand, for synthetic data detection tasks, LMMs can provide reasoning behind authenticity judgments in natural language, paving the way for enhanced explainability. On the other hand, the task of distinguishing between real and synthetic data involves the perception, knowledge, and reasoning abilities of multimodal data, serving as an excellent test of LMM capabilities. Therefore, the focus of this paper is to evaluate the performance of LMMs in synthetic data detection tasks. However, traditional synthetic data detection benchmarks, such as Fake2M (Lu et al., 2023b) and ASVSpoof 2019 (Wang et al., 2020), primarily assess conventional detection methods, and evaluations of LMMs in detecting multimodal synthetic data are still lacking. These benchmarks often miss fine-grained anomaly annotations represented in natural human language, making it difficult to transparently analyze the explainability capabilities of LMMs. FakeBench (Li et al., 2024a) aligns more closely with our objectives, but it only evaluates the performance of LMMs within single standard image modality, lacking both breadth and depth. Specifically, FakeBench fails to explore other modalities such as audio and 3D data, focusing primarily on general image types and not conducting thorough tests on expert domain images like satellite imagery. To bridge this gap, we introduce LOKI, comprehensive benchmark for evaluating the performance of LMMs on synthetic data detection. The key highlights of the LOKI benchmark include: Diverse Modalities. LOKI includes high-quality multimodal data generated by recent popular synthetic models, covering video, image, 3D data, text, and audio. Heterogeneous Categories. Our collected dataset includes 26 detailed categories across different modalities, such as specialized satellite and medical images; texts like philosophy and ancient Chinese; and audio data like singing voices, environmental sound and music. Multi-level Annotations. LOKI includes basic Synthetic or Real labels, suitable for fundamental question settings like true/false and multiple-choice questions. It also incorporates fine-grained anomalies for inferential explanations, , enabling tasks like abnormal detail selection and abnormal explanation, to explore LMMs capabilities in explainable synthetic data detection. Multimodal Synthetic Evaluation Framework. We propose comprehensive evaluation framework that supports inputs of various data formats and over 25 mainstream multimodal models. On the LOKI benchmark, we evaluated 22 open-source LMMs, 6 advanced proprietary LMMs, and several expert synthetic detection models. Our key findings are summarized as follows: For synthetic data detection tasks we find: (1) LMMs exhibit moderate capabilities in synthetic data detection tasks, with certain levels of explainability and generalization, but there is still gap compared to human performance; (2) Compared to expert synthetic detection models, LMMs exhibit greater explainability and, compared to humans, can detect features invisible to the naked eye, demonstrating promising developmental prospects. For LMMs capabilities we find: (1) Most LMMs exhibit certain model biases, tending to favor synthetic or real data in their responses; (2) LMMs lack of expert domain knowledge, performing poorly 2 on specialized image types like satellite and medical images; (3) Current LMMs show unbalanced multimodal capabilities, excelling in image and text tasks but underperforming in 3D and audio tasks; (4) Chain-of-thought prompting enhances LMMs performance in synthetic data detection, whereas simple few-shot prompting falls short of providing the necessary reasoning support. These findings highlight the challenging and comprehensive nature of the LOKI task and the promising future of LMMs in synthetic data detection tasks."
        },
        {
            "title": "2.1 SYNTHETIC DATA DETECTION",
            "content": "Currently, synthetic data detection has garnered widespread attention to prevent the misuse of multimedia synthetic data (Gragnaniello et al., 2021; Hou et al., 2023). The detection of synthetic data in image and audio has long been popular research (Barni et al., 2020; Frank et al., 2020), while methods for synthetic video detection have recently emerged, such as DuB3D(Ji et al., 2024) and AIGVDet(Bai et al., 2024a). However, most work primarily focuses on the binary distinction between authentic and synthetic data, resulting in poor interpretability. Some studies aim to enhance the interpretability of synthetic detection by providing latent representations(Dong et al., 2022), feature explanations(Chai et al., 2020), and artifact localization(Zhang et al., 2023a); however, most research remains limited to the interpretability of abstract symbols, leaving significant gap in alignment with human understanding. In practice, current AI-generated synthetic data still exhibits noticeable flaws, such as discontinuities in synthetic videos and insufficient geometric accuracy in 3D data. These shortcomings can be effectively captured and perceived by human users(Tariang et al., 2024), who can provide reasonable explanations. However, existing expert synthetic data detection methods fail to provide human-interpretable bases for their judgments. 2.2 LARGE MULTIMODAL MODELS Recently, the rapid development of multimodal large models (LMMs) has been notable, with models like GPT-4o (OpenAI, 2024) and Claude 3.5 (Anthropic, 2024) excelling in various tasks such as scientific questioning (Lu et al., 2022; Yue et al., 2024) and commonsense reasoning (Talmor et al., 2018), showcasing exceptional perceptual and reasoning abilities (Bai et al., 2024b). Research has also applied LMMs to evaluate AIGC synthetic results, utilizing GPT to assess the quality of generated images (Ku et al., 2023; Peng et al., 2024) and 3D models (Wu et al., 2024b), providing scores that align with human preferences along with interpretable justifications. Consequently, in synthetic data detection, LMMs can offer reasons for determining authenticity in natural language, paving the way for enhanced interpretability in synthetic detection. Moreover, LMMs can access features invisible to human users, such as deep image and spectral features, demonstrating their potential to exceed human detection capabilities. Furthermore, synthetic data detection involves multimodal data perception and complex logical reasoning, making it an excellent task to assess the capabilities of LMMs. This task also provides quantitative evaluation metrics like accuracy, allowing for more direct assessment of model performance compared to more qualitative scoring tasks. 2.3 SYNTHETIC DATA DETECTION BENCHMARK Currently, there are numerous datasets corresponding to synthetic data detection tasks, including those designed for traditional detection methods and those tailored for LMMs. For instance, traditional synthetic datasets such as Fake2M (Lu et al., 2023b), HC3 (Guo et al., 2023), and ASVSpoof 2019 (Wang et al., 2020) have explored the performance of traditional deepfake detection methods across various modalities, but they lack assessments for LMMs models. VANE (Bharadwaj et al., 2024) evaluates the capability of LMMs in detecting video anomalies, including the detection of criminal activities in real videos and synthetic video detection, although it focuses more on video content understanding. Fakebench (Li et al., 2024b) assesses LMM performance in the image modality, yet it concentrates on single modality and offers limited subcategories. In contrast, LOKI covers broader range of data modalities, including video, image, 3D, text, and audio, as well as data from specialized fields such as remote sensing, medical imaging, and environmental sounds. In terms of problem design, LOKI encompasses tasks for authenticity judgment, as well as more complex challenges like Abnormal Details selection and Abnormal Explanation, which test the LMMs ability to explain reasons in synthetic data detection."
        },
        {
            "title": "3.1 OVERVIEW OF LOKI",
            "content": "We introduce LOKI, multimodal synthetic data detection benchmark, designed specifically to comprehensively assess the capabilities of LMMs in detecting synthetic data. As illustrated in Figure 2, LOKI encompasses variety of modalities including video, image, 3D, text, and audio, with over 26 specific subcategories of data. The benchmark utilizes fine-grained anomaly annotations to construct tiered variety of question types, including judgment questions, multiple-choice questions, abnomal detail selection and abnomal explanation questions, totaling over 18k questions. Table 1 provides detailed comparison of LOKI with existing datasets, including traditional synthetic detection benchmarks and those tailored for evaluating LMMs. In terms of breadth, LOKI covers wider range of modalities and finer categories. In depth, it goes beyond binary judgment question designs to include questions that require deep understanding and explanation of detailed anomalies. Additionally, LOKI classifies question difficulty based on human evaluation metrics. Figure 2: Statistical information of LOKI. The left side displays the detailed categories of each modality, while the right side presents the questions across different modalities. The inner circle numbers represent the data volume, and the outer circle numbers indicate the number of questions. Table 1: The comparison between LOKI and other benchmarks. Answer types include JD (Judgment), MC (Multiple Choice), and OE (Open-ended). Real paired indicates whether real data is paired within the same domain, while Difficulty Level shows if questions are graded by difficulty. Dataset Size Category Data Modality Answer Img Vid Txt Aud 3D JD MC OE FFHQ Fake2M >1M 8 types 70k - HC3 Mixset 80K 5 types 3.6 5 types ASVS2019 108K Codecfake 1M - - FakeBench VANE LOKI 6 types 6K 0.9K 18K 26 types - Real Paired Difficulty Level 3.2 DATA COLLECTION AND ANNOTATION Video: We collected 593 video clips by utilizing various closed-source and open-source models such as SORA (OpenAI, 2024), Keling, and Open sora (Zheng et al., 2024), generating high-quality text-to-video synthesis data along with corresponding real domain sample data. For the AI-generated 4 Figure 3: Examples of Synthetic Data Annotations: (a) Detailed annotations of video anomalies; (b) Detailed annotations of image anomalies; (c) Detailed annotations of 3D anomalies. video clips, we employed the LabelU1 tool to annotate anomaly details, including anomalous segments and their descriptions, anomalous key frames, and global anomaly descriptions. As shown in Figure 3 (a), anomalies in the videos, such as violating natural physics and frame flickering, are also annotated globally. Additionally, the anomalous segment from 02:54 to 06:27 is highlighted, with the corresponding reasons for the anomalies explained by human annotators. Furthermore, each anomalous segment includes an anomalous key frame to facilitate subsequent LMMs in accurately reading the anomalous frames when processing video data. Image: We have collected over 2,217 images from 7 subcategories through existing dataset extraction, internet collection, and new data synthesis. The image synthesis methods include FLUX, Midjourney (AI, 2023), Stable Diffusion (Blattmann et al., 2023), and ten other different methods to ensure high quality and diversity of the data. For the synthesized image data, in addition to overall annotations, we performed anomaly region bounding and explanations, as shown in Figure 3 (b). The region anomaly annotations allow for more fine-grained and specific labeling, which can be used for generating subsequent anomaly detail questions. 3D data: We conducted comprehensive analysis of OmniObject3D (Wu et al., 2023), selecting scanned instances as ground truth within the same domain. By constructing prompt texts, we synthesized three Nerf models (Poole et al., 2022) and three 3D GS models (Tang et al., 2023), and supplemented them with results from the advanced commercial model Clay and some Nerf-based results from GPTEval3D (Wu et al., 2024b). We collected total of over 1,200 3D models from ten different synthesis methods, including both synthesized and real scanned data. Additionally, as shown in Figure 3 (c), we performed texture anomaly description annotations corresponding to the RGB four views of the synthesized 3D data, as well as normal anomaly description annotations. Notably, besides the multi-view format, the 3D data also supports point clouds and panoramic videos. Audio: We collected various categories of audio, including speech, singing voice, environmental sounds, and music. The speech and singing voice data ensured consistency in speaker timbre, sourced from the Logical Access part of ASVSpoof2019 (Wang et al., 2020) and the CtrSVDD Benchmark, covering four generation paradigms: TTS, VC, SVS, and SVC. Environmental audio data came from DCASE 2023 Task 7, with real audio from the development set and synthetic audio generated using multiple methods from Track A. Music data were sourced from MusicCaps, with synthetic music generated based on descriptions using MusicGen (Copet et al., 2024), AudioLDM2Music (Liu et al., 2024a), and Suno2. 1LabelU: https://github.com/opendatalab/labelU 2Suno: https://suno.com/ 5 Figure 4: Example Questions of LOKI. LOKI includes four types of questions:(a) Judgment questions; (b) Multiple choice questions; (c) Abnormal detail selection; (d) Abnormal explanation. Text: Based on summarization and regeneration methods, we generated counterfeit texts similar to the original texts using mainstream models such as GPT-4o, Qwen-Max, and Llama 3.1-405B. We collected eight categories of text data, pairing each sample with real text and model-generated similar text, totaling 3,359 text entries. Our text data were categorized by length and language, including short texts (50-100 characters), medium texts (100-200 characters), and long texts (over 200 characters), with 1:1 ratio of Chinese to English data. More information regarding the collection and statistics of each modality can be found in Appendix B. 3.3 QUESTION GENERATION Judgment Task: This task requires large language models (LMMs) to determine whether the input data is synthetic or real. As shown in Figure 4 (a), LMMs need to answer the judgment question, Is the provided audio generated by AI? To minimize the influence of prompts on model judgments, questions are asked in two forms: whether the data is AI-synthesized or real, and identifying either the real or AI-synthesized data. Furthermore, we categorize the data into different difficulty levels based on human performance. If all tested human users (more than three) answer correctly, the task is classified as easy; if more than 50% answer incorrectly, it is classified as hard; all other cases fall into the medium category. Multiple Choice Task: This task requires LMMs to correctly select AI-generated or real data from the provided synthetic and real data. As illustrated in Figure 4 (b), LMMs need to complete the multiple-choice question, Which of the following texts is generated? The design of this question benefits from our collection of real paired data within the same domain, effectively assessing LMMs comparative analysis capabilities. Abnormal Detail Selection: Based on fine-grained anomaly annotation data from modalities such as video, images, and 3D, we effectively design prompts and utilize GPT-4o to generate questions for Abnormal Detail Selection. As shown in Figure 4 (c), for video contents detail anomalies, we ask, What elements can be seen as inconsistent? By providing clear anomaly annotations, we can effectively reduce the hallucination phenomenon in GPT-4o, ensuring the quality of the questions. More details can be found in the supplementary materials. Abnormal Explanation: Furthermore, we design open-ended abnormal explanation questions, requiring LMMs to independently identify anomalies and explain their reasons. As shown in Figure 4 (d), we ask, Why is the provided image AI-generated? It is worth to note that in real anomaly explanation tasks, the input does not include bounding boxes around anomalous areas. Tasks related to Abnormal Detail Selection and Abnormal Explanation can more precisely test whether LMMs genuinely perceive corresponding detail anomalies rather than guessing answers. Quality Control: To mitigate the impact of hallucinations of GPT-4o during question generation in abnormal detail selection task, all samples in this task undergo manual reviews. Each question that involves GPT must pass through at least two rounds of verification by human users. total of 20 users participated in the verification process, which took approximately 160 hours to complete."
        },
        {
            "title": "4 EXPERIMENT",
            "content": "In this section, we evaluate various Language Model Multimodalities (LMMs) under our proposed LOKI evaluation framework, which includes both open-source and proprietary models, multimodal LMMs, Audio LMMs, and text-based LLMs. Our evaluations are conducted in zero-shot setting. In the following subsections, we first introduce our evaluation models and the evaluation protocols. Next, we analyze the performance of existing LMMs in synthetic data detection tasks, comparing them with human users and expert models. We will then discuss the challenges and shortcomings faced by multimodal large models in the current task settings. Additionally, we explore the potential impact of few-shot or chain-of-thought prompting on this task."
        },
        {
            "title": "4.1 BASELINES",
            "content": "LMMs. We evaluate 3 closed-source and 18 open-source LMMs across different model types and sizes. For closed-source models, we consider GPT-4o (OpenAI, 2024), Gemini-1.5-Pro (Team et al., 2023), Claude-3.5-Sonnet (Anthropic, 2024). Given that modality alignment in multimodal LMMs may lead to decline in LLM performance on text-based tasks (Dai et al., 2024), we also selected pure text LLMs, such as LLaMA-3.1-405B (Team, 2024), Qwen-Max (Chu et al., 2023) and MistralLarge (Mistral, 2024), to evaluate the text modality. In the evaluation of Audio LMMs, we selected high-performing open-source models such as Qwen-Audio (Chu et al., 2023) and SALMONN-7B (Sun et al., 2024). For proprietary models, we chose Gemini-Flash (Team et al., 2023), which supports audio input. Human Users. We invited over 50 human users, including senior university students and regular users, to participate in the judgment and multiple-choice question tests for different modalities of synthetic data. Each question was tested by at least 3 users to ensure the robustness of the results. Additionally, we designed an online platform to distribute random questionnaires, and more than 200 users participated in the testing of 15 basic questions. Expert Models. We selected recently open-sourced expert-level synthetic data detection methods and their corresponding weights for testing, including video detection (AIGVDet (Yang et al., 2024)), image detection (AIDE (Yan et al., 2024)), text detection (RADAR-Vicuna-7B (Hu et al., 2023)), and audio detection (AASIST (Jung et al., 2022)). Due to the limited availability of 3D synthetic data detection methods, 3D was not considered. Additionally, there is no overlap between the training sets of these methods and the LOKI test data, reducing the possibility of data contamination. We selected only small number of expert models for evaluation, primarily to serve as references, similar to the role of human references. Evaluation Protocol. Data Input: For the video modality, we utilize an 8-frame video clip along with corresponding questions as input. For 3D modal data, we employ the commonly used multiview input method. Results based on surround video and point cloud inputs are also included in the supplementary materials. For other modalities, inputs are based on textual prompts combined with corresponding images, audio, and textual materials. During the evaluation, each model independently generates responses to questions without retaining any dialogue history. Evaluation Metric: For judgement, multiple-choice and abnormal detail selection questions, we use the average accuracy rate as metric. In addition to accuracy, we also calculate the Normalized Bias Index (NBI) based on recall rates to assess model bias. For open-ended questions regarding anomalous details, we use the GPT-4 model to assess the score of the responses. Further details on the calculation of evaluation metrics can be found in Appendix C.2. Evaluation Framework: To standardize the evaluation of different LMMs and various input modalities for synthetic data detection, we propose comprehensive multimodal evaluation framework. This framework provides support for various input modalities such as 3D point clouds, videos, images, audio, and text, while unifying APIs of over 25 mainstream LMMs, ensuring both model compatibility and consistency throughout the evaluation process. 4.2 SYNTHETIC DATA DETECTION RESULTS In this section, we provide comprehensive analysis of the performance of various LMMs and LLMs on synthetic data detection tasks using the LOKI dataset. 7 Table 2: Results of different models on the LOKI for Judgment and Multiple Choice questions. (a) Multimodal evaluation of LMMs; (b) Text evaluaion of LLMs; (c) Audio evaluation of Audio LMMs; * denotes the closed-source models. (a) Multimodal evaluation of LMMs Judgment Multiple Choice Video Image 3D Text Overall Video Image 3D Text Overall Random Choice Human (Medium) Expert models Phi-3.5-Vision MiniCPM-V-2.6 InternLM-XComposer2.5 mPLUG-Owl3-7B LongVA-7B Qwen2-VL-7B LLaVA-OV-7B Llama-3-LongVILA-8B Idefics2-8B Mantis-8B InternVL2-8B InternVL2-26B InternVL2-40B VILA1.5-13B VILA1.5-40B Qwen2-VL-72B LLaVA-OV-72B Claude-3.5-Sonnet* Gemini-1.5-Pro* GPT-4o* 51.1 83.5 53.1 56.8 57.2 58.4 55.3 60.4 59.5 56.8 51.9 54.8 55.4 60.8 55.0 62.0 51.9 59.2 59.6 56. 61.7 58.5 71.3 50.5 80.1 63.1 52.5 44.8 46.4 45.9 46.2 47.8 49.8 49.8 45.0 54.6 49.7 44.3 49.6 49.3 48.8 53.2 46.3 53.6 43.5 63.4 50.5 72.0 - 50.0 56.4 43.9 49.9 49.9 72.3 68.4 32.2 38.4 50.0 49.4 50.4 49.9 34.0 50.0 60.3 51. 58.0 55.4 65.2 49.9 68.5 72.1 49.4 49.4 52.6 53.6 48.6 48.9 53.0 49.9 47.2 52.0 50.3 51.1 53.1 47.7 50.1 52.8 61.2 61.5 55.7 55.9 50.3 76.0 62.8 52.2 52.0 50.3 51.1 51.7 57.1 57.0 46.0 46.3 53.0 52.6 49.9 52.2 45.7 52.7 55.4 56. 61.6 53.2 63.9 47.7 91.3 - 58.2 52.8 56.3 60.3 57.5 64.0 59.8 54.0 55.6 47.9 54.0 62.4 65.7 52.1 49.1 65.7 62.9 60.5 66.1 77.3 49.0 84.5 - 44.0 49.8 51.0 52.5 51.6 65.1 51.7 51.1 51.3 61.5 51.4 48.5 63.1 55.3 64.0 68.6 70. 65.5 67.3 80.8 49.7 91,2 - 59.6 50.7 48.0 49.9 61.4 55.5 53.8 50.5 54.2 62.5 53.1 48.8 59.9 53.5 47.9 58.7 61.3 51.9 60.2 70.2 45.2 78.5 - 42.0 48.9 40.5 50.0 48.9 46.4 48.4 44.3 37.0 48.4 46.6 50.3 45.2 44.0 50.4 69.7 69. 89.2 57.3 66.6 46.9 86.4 - 50.9 50.6 49.0 53.1 52.6 57.7 53.4 50.0 49.5 55.1 51.3 53.2 52.7 51.2 53.7 65.6 65.2 74.8 62.7 73.7 (b) Text evaluation of LLMs (c) Audio evaluation of Audio LMMs Model Judgment Choice Overall Model Judgment Choice Overall Human (Medium) Expert model LLaMA-3.1-405B Mistral-Large* Qwen-Max* Claude-3.5-Sonnet* Gemini-1.5-Pro* GPT-4* 69.2 69.4 56.8 52.2 48.3 61.5 55.7 55.9 71.1 - 73.1 69.1 44.4 89.2 57.3 66.6 70.1 69.4 64.4 57.8 46.5 70.7 56.2 59. Human (Medium) Expert model Qwen-Audio SALMONN-7B AnyGPT OneLLM LUT Gemini-1.5-Flash* 69.2 69.4 49.8 51.2 49.8 49.9 44.4 49.4 71.1 - 50.1 - 50.3 - - 49. 70.1 69.4 49.9 51.2 50.1 49.9 44.4 49.3 Judgment and Multiple Choice. Table 2 illustrates the performance of various models on judgment and multiple-choice questions in LOKI. For the synthetic data judgment task, the closed-source model GPT-4o achieves the best results, with an overall accuracy (excluding audio) of 63.9%. When real paired data is included for comparison in the multiple-choice questions, accuracy further increases to 73.7%. In the text modality, Claude-3.5 outperform other LMMs and LLMs, achieving accuracies exceeding 70%. In the Audio LMMs category, both open-source and closed-source models show performances comparable to random selection, which is not satisfactory. Abnormal Detail Selection and Explanation. We compared the performance of different models on the tasks of abnormal detail selection and abnormal reason explanation, as shown in Table 3. GPT-4o achieved an accuracy exceeding 75% in abnormal detail selection and score over 70% in abnormal reason explanation. This indicates that advanced LMMs like GPT-4o has demonstrated strong detail understanding capabilities, effectively analyzing and interpreting synthetic traces. Notably, we observe that Claude-3.5-Sonnet (Anthropic, 2024) tends to misclassify synthetic images as real, despite the primary goal of our tasks being to explain abnormalities in synthetic images. More examples of abnormal explanations can be found in Appendix E. 8 Table 3: Results of different models on the LOKI for Abormal Details Selection and Abnormal explanation questions.* denotes the closed-source models. Abnormal Details Selection Abnormal Explanation Video Image Overall Video Image 3D Overall LLaVA-OV-7B Qwen2-VL-7B InternVL2-8B Gemini-1.5-Pro* Claude-3.5-Sonnet* GPT-4o* 76.9 79.4 66.8 58.7 50.9 74.0 18.8 31.5 70.2 40.0 19.8 76. 43.1 51.5 68.8 47.8 32.8 75.3 46.7 48.4 46.5 57.6 50.1 67.6 68.9 63.8 72.2 77.1 1.7 72.9 71.0 73.4 71.3 70.8 78.2 77.0 62.0 61.9 63.0 68.1 45.8 72.6 Figure 5: The multimodal large model capability assessment analysis results. (a) Model bias assessment, where the closer the color is to red, the more the model is biased towards classifying the data as real; the closer to blue, the more it leans towards synthetic data. The size of the square also represents the degree of bias. (b) The performance of GPT-4o across different image types and its difference from human users. (c) relative radar chart of the models performance across various modalities, with Human benchmarks for comparison. Comparing Humans and Expert Models. Humans exhibit an average performance of 76% in judgment tasks and 86.4% in multiple-choice questions, both 10% higher than the LMM method. Notably, if LMM tools are to be applied in production, their decision-making performance in judgment tasks must exceed 90% to be convincing. As synthesis technologies advance, the distinct traces of synthetic data are becoming increasingly subtle. However, LMMs capture minute details, such as image features imperceptible to the human eye, demonstrating their potential to surpass human. LMMs demonstrate superior performance in most tasks compared to expert models. This is primarily due to the rich and diverse sources of synthetic data collected by LOKI, which significantly differ from existing data domains, resulting in suboptimal generalization performance of expert models. The accuracy of synthetic detection by expert models trained on similar data should significantly improve. Currently, LMMs perform at moderate level in synthetic data detection but surpass expert models in generalization ability. Unlike traditional expert models, LMMs possess the capability to explain the reasons behind anomalies, highlighting their unique advantage as synthetic detectors. 4.3 LARGE MULTIMODAL MODELS CAPABILITIES Model Bias. The heatmap of the Normalized Bias Index calculated based on recall rates, as shown in Figure 5 (a), is utilized for analyzing model biases. The results indicate that most models exhibit significant biases in synthetic data detection tasks, with tendency to incorrectly categorize data as either real or synthetic. For instance, GPT-4o tends to classify textual data as real, whereas it is biased towards judging 3D data as AI-generated. Despite diverse questioning techniques implemented to minimize cueing effects, pronounced bias is still evident across most models. Lack of Expert Domain Knowledge. In Figure 5 (b), we present the varying performance of GPT4o across different image subcategories. The experimental results clearly indicate that GPT exhibits strong recognition abilities on common image types such as objects and landscapes, even surpassing human users. However, GPT-4os performance significantly deteriorates in specialized fields such Table 4: Result decomposition across questions difficulty levels. Table 5: LMMs performances under different prompting strategies for judgement tasks. Difficulty Levels (Video & Image & 3D & Text) Prompting Strategies Performances (Image & 3D) Easy Medium (1104) (2470) Hard (3938) Overall (7512) Baseline FS CoT Few-shot Chain-of-Thought LLaVA-OV-7B InternVL2-8B Qwen2-VL-7B Gemini-1.5-pro Claude-3.5-Sonnet GPT-4o 60.4 64.5 67.7 70.8 76.0 78.8 47.6 47.8 45.6 42.4 44.7 52.3 39.1 33.5 35. 32.4 29.8 44.4 47.3 45.7 47.4 46.4 47.1 56.8 LLaVA-OV-7B InternVL2-8B Qwen2-VL-7B Gemini-1.5-pro Claude-3.5-Sonnet GPT-4o 56.6 49.6 56. 47.9 55.2 64.1 46.4 46.1 52.6 41.2 53.7 75.1 18.8 50.4 59.5 51.0 56.4 74.2 as satellite and medical imaging, and in less-trained image types like documents. This suggests that current LMMs still lack certain expert domain knowledge. Unbalanced Multimodal Capabilities. In Figure 5 (c), we compare the performance of various LMMs across different modalities. Results indicate that current models excel in frequently trained modalities such as images and text, even surpassing human performance in some tests. However, their performance declines significantly on 3D and audio tasks, with most open-source models lacking corresponding capabilities. For future AGI to develop into versatile assistant, it needs to possess more balanced multimodal abilities. Model Performance across Different Levels. Based on human user performance, we categorized the difficulty levels of the questions, as shown in Table 4, which presents the performance of selected models across different difficulty levels. As the difficulty increases, the performance of LMMs gradually declines, consistent with human user performance. Under challenging conditions, GPT4os accuracy drops to only 44.4%, which is lower than that achieved by random selection. This indicates that LMMs have certain limitations in handling complex synthetic data detection tasks. Prompting Strategies Improve LMMs Capabilities. In Table 5, we demonstrate the effects of different prompting strategies in LOKIs image and 3D judgement tasks, where CoT refers to the Chain of Thought prompting (Wei et al., 2022b) and FS refers to the few-shot prompting (Alayrac et al., 2022). During inference, models are prompted with two random examples that are in the same domain as the questions by different strategies. In CoT prompting, we manually craft thought chains with our human annotations to elicit reasoning steps out of LMMs, while in FS prompting, we simply prepend examples with answers to the questions. Interestingly, GPT-4o shows strong reasoning ability without chain-of-thought prompting, while other models rely on it for improved performance. Few-shot learning fails to support the necessary step-by-step reasoning for synthetic data detection, but GPT-4o performs well regardless, suggesting its inherent ability to reason effectively without additional reasoning guidance. However, LLaVAOV-7B experienced significant performance drop when prompted with CoT. We conjecture that this degradation may result from decline in its ability to understand long contexts after fine-tuning (Zhai et al., 2023)."
        },
        {
            "title": "5 CONCLUSION",
            "content": "In this paper, we introduced LOKI, multimodal benchmark designed to evaluate the performance of large multimodal models in detecting synthetic data across various modalities. We conducted comprehensive study of LMMs performance on video, image, 3D, audio, text, and specialized subdomains, and we also analyzed LMMs ability to explain detailed anomalies in synthetic data. The experimental results indicate that LMMs have certain level of competence in detecting synthetic data and preliminary ability to explain anomalies. Synthetic data detection tasks also effectively evaluate the various capabilities of LMMs during their development. These findings highlight the challenging and comprehensive nature of the LOKI task, as well as the potential of LMMs in future synthetic data detection tasks. We aim to inspire more powerful and interpretable synthetic data detection methods through LOKI to address the potential risks posed by rapidly advancing AI synthesis technologies. Furthermore, while the relationship between synthesis and detection is adversarial, they are mutually beneficial; better and more explainable synthetic detectors will further advance AI synthesis technologies."
        },
        {
            "title": "REFERENCES",
            "content": "Ai safety summit, 2023. URL https://www.aisafetysummit.gov.uk/. Hosted by the UK. Malak Abdullah, Alia Madain, and Yaser Jararweh. Chatgpt: Fundamentals, applications and social impacts. In 2022 Ninth International Conference on Social Networks Analysis, Management and Security (SNAMS), pp. 18. Ieee, 2022. Andrea Agostinelli, Timo I. Denk, Zalan Borsos, Jesse Engel, Mauro Verzetti, Antoine Caillon, Qingqing Huang, Aren Jansen, Adam Roberts, Marco Tagliasacchi, Matt Sharifi, Neil Zeghidour, and Christian Frank. Musiclm: Generating music from text, 2023. URL https://arxiv. org/abs/2301.11325. M. AI. Midjourney: Text to image with ai art generator, 2023. URL https://www. midjourneyai.ai/en. Accessed: 2024-09-21. Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: visual language model for few-shot learning. Advances in neural information processing systems, 35:23716 23736, 2022. Sam Altman. Openai now generates about 100 billion words per day, 2024. URL https://x. com/sama/status/1756089361609981993. Accessed: 2024-09-24. Anthropic. The claude 3 model family: Opus, sonnet, haiku, 2024. URL https://www. anthropic.com. Accessed: 2024-09-23. Jianfa Bai, Man Lin, Gang Cao, and Zijie Lou. AI-generated video detection via spatial-temporal anomaly learning. The 7th Chinese Conference on Pattern Recognition and Computer Vision (PRCV), 2024a. Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: versatile vision-language model for understanding, localization, text reading, and beyond, 2023. URL https://arxiv.org/abs/2308.12966. Tianyi Bai, Hao Liang, Binwang Wan, Ling Yang, Bozhou Li, Yifan Wang, Bin Cui, Conghui He, Binhang Yuan, and Wentao Zhang. survey of multimodal large language model from datacentric perspective. arXiv preprint arXiv:2405.16640, 2024b. Omer Bar-Tal, Hila Chefer, Omer Tov, Charles Herrmann, Roni Paiss, Shiran Zada, Ariel Ephrat, Junhwa Hur, Yuanzhen Li, Tomer Michaeli, Oliver Wang, Deqing Sun, Tali Dekel, and Inbar Mosseri. Lumiere: space-time diffusion model for video generation. 2024. URL https: //api.semanticscholar.org/CorpusID:267095113. Mauro Barni, Kassem Kallas, Ehsan Nowroozi, and Benedetta Tondi. Cnn detection of gangenerated face images based on cross-band co-occurrences analysis. In 2020 IEEE international workshop on information forensics and security (WIFS), pp. 16. IEEE, 2020. Quentin Bertrand, Joey Bose, Alexandre Duplessis, Marco Jiralerspong, and Gauthier Gidel. On the stability of iterative retraining of generative models on their own data. In The Twelfth International Conference on Learning Representations, 2024. Rohit Bharadwaj, Hanan Gani, Muzammal Naseer, Fahad Shahbaz Khan, and Salman Khan. Vanebench: Video anomaly evaluation benchmark for conversational lmms, 2024. Joseph Biden. Executive order on the safe, secure, and trustworthy development and use of artificial intelligence. 2023. Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. 11 Matyas Bohacek and Hany Farid. Nepotistically trained generative-ai models collapse. arXiv preprint arXiv:2311.12202, 2023. Matyas Bohacek and Hany Farid. The making of an ai news anchorand its implications. Proceedings of the National Academy of Sciences, 121(1):e2315678121, 2024. Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video generation models as world simulators. 2024. URL https://openai.com/research/ video-generation-models-as-world-simulators. Tom Brown. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020. Lucy Chai, David Bau, Ser-Nam Lim, and Phillip Isola. What makes fake images detectable? understanding properties that generalize. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part XXVI 16, pp. 103120. Springer, 2020. Rui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia. Fantasia3d: Disentangling geometry and appearance for high-quality text-to-3d content creation. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 2224622256, 2023a. Sanyuan Chen, Yu Wu, Chengyi Wang, Shujie Liu, Daniel Tompkins, Zhuo Chen, Wanxiang Che, Xiangzhan Yu, and Furu Wei. Beats: audio pre-training with acoustic tokenizers. In Proceedings of the 40th International Conference on Machine Learning, pp. 51785193, 2023b. Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. arXiv preprint arXiv:2404.16821, 2024. Hiuyi Cheng, Peirong Zhang, Sihang Wu, Jiaxin Zhang, Qiyuan Zhu, Zecheng Xie, Jing Li, Kai Ding, and Lianwen Jin. M6doc: large-scale multi-format, multi-type, multi-layout, multilanguage, multi-annotation category dataset for modern document layout analysis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 15138 15147, June 2023. Keunwoo Choi, Jaekwon Im, Laurie Heller, Brian McFee, Keisuke Imoto, Yuki Okamoto, Mathieu Lagrange, and Shinosuke Takamichi. Foley sound synthesis at the dcase 2023 challenge. arXiv preprint arXiv:2304.12521, 2023. Yunfei Chu, Jin Xu, Xiaohuan Zhou, Qian Yang, Shiliang Zhang, Zhijie Yan, Chang Zhou, and Jingren Zhou. Qwen-audio: Advancing universal audio understanding via unified large-scale audio-language models. arXiv preprint arXiv:2311.07919, 2023. Di Cooke, Abigail Edwards, Sophia Barkoff, and Kathryn Kelly. As good as coin toss human detection of ai-generated images, videos, audio, and audiovisual stimuli. arXiv preprint arXiv:2403.16760, 2024. Jade Copet, Felix Kreuk, Itai Gat, Tal Remez, David Kant, Gabriel Synnaeve, Yossi Adi, and Alexandre Defossez. Simple and controllable music generation. Advances in Neural Information Processing Systems, 36, 2024. Wenliang Dai, Nayeon Lee, Boxin Wang, Zhuoling Yang, Zihan Liu, Jon Barker, Tuomas Rintamaki, Mohammad Shoeybi, Bryan Catanzaro, and Wei Ping. Nvlm: Open frontier-class multimodal llms. arXiv preprint arXiv:2409.11402, 2024. Prafulla Dhariwal and Alex Nichol. Diffusion models beat gans on image synthesis, 2021a. URL https://arxiv.org/abs/2105.05233. Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:87808794, 2021b. Elvis Dohmatob, Yunzhen Feng, and Julia Kempe. Model collapse demystified: The case of regression. arXiv preprint arXiv:2402.07712, 2024a. 12 Elvis Dohmatob, Yunzhen Feng, Pu Yang, Francois Charton, and Julia Kempe. tale of tails: Model collapse as change of scaling laws. In Forty-first International Conference on Machine Learning, 2024b. Shichao Dong, Jin Wang, Jiajun Liang, Haoqiang Fan, and Renhe Ji. Explaining deepfake detection by analysing image matching. In European conference on computer vision, pp. 1835. Springer, 2022. Yunzhen Feng, Elvis Dohmatob, Pu Yang, Francois Charton, and Julia Kempe. Beyond model collapse: Scaling up with synthesized data requires reinforcement. arXiv preprint arXiv:2406.07515, 2024. Joel Frank, Thorsten Eisenhofer, Lea Schonherr, Asja Fischer, Dorothea Kolossa, and Thorsten Holz. Leveraging frequency analysis for deep fake image recognition. In International conference on machine learning, pp. 32473258. PMLR, 2020. Liang Yu Gong and Xue Jun Li. contemporary survey on deepfake detection: datasets, algorithms, and challenges. Electronics, 13(3):585, 2024. Yuan Gong, Yu-An Chung, and James Glass. Ast: Audio spectrogram transformer. In Interspeech 2021, pp. 571575, 2021. Diego Gragnaniello, Davide Cozzolino, Francesco Marra, Giovanni Poggi, and Luisa Verdoliva. Are gan generated images easy to detect? critical analysis of the state-of-the-art. In 2021 IEEE international conference on multimedia and expo (ICME), pp. 16. IEEE, 2021. Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio Cesar Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, Adil Salim, Shital Shah, Harkirat Singh Behl, Xin Wang, Sebastien Bubeck, Ronen Eldan, Adam Tauman Kalai, Yin Tat Lee, and Yuanzhi Li. Textbooks are all you need, 2023. URL https://arxiv.org/ abs/2306.11644. Biyang Guo, Xin Zhang, Ziyuan Wang, Minqi Jiang, Jinran Nie, Yuxuan Ding, Jianwei Yue, and Yupeng Wu. How close is chatgpt to human experts? comparison corpus, evaluation, and detection. arXiv preprint arxiv:2301.07597, 2023. Agrim Gupta, Lijun Yu, Kihyuk Sohn, Xiuye Gu, Meera Hahn, Li Fei-Fei, Irfan Essa, Lu Jiang, arXiv preprint Photorealistic video generation with diffusion models. and Jose Lezama. arXiv:2312.06662, 2023. David Gutman, Noel C. F. Codella, Emre Celebi, Brian Helba, Michael Marchetti, Nabin Mishra, and Allan Halpern. Skin lesion analysis toward melanoma detection: challenge at the international symposium on biomedical imaging (isbi) 2016, hosted by the international skin imaging collaboration (isic), 2016. URL https://arxiv.org/abs/1605.01397. Ryuichiro Hataya, Han Bao, and Hiromi Arai. Will large-scale generative models corrupt future datasets? In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 2055520565, 2023. Ruifei He, Shuyang Sun, Xin Yu, Chuhui Xue, Wenqing Zhang, Philip Torr, Song Bai, and XIAOJUAN QI. Is synthetic data from generative models ready for image recognition? In The Eleventh International Conference on Learning Representations, 2023. Yang Hou, Qing Guo, Yihao Huang, Xiaofei Xie, Lei Ma, and Jianjun Zhao. Evading deepfake detectors via adversarial statistical consistency. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1227112280, 2023. Shengding Hu, Yuge Tu, Xu Han, Chaoqun He, Ganqu Cui, Xiang Long, Zhi Zheng, Yewei Fang, Yuxiang Huang, Weilin Zhao, et al. Minicpm: Unveiling the potential of small language models with scalable training strategies. arXiv preprint arXiv:2404.06395, 2024. Xiaomeng Hu, Pin-Yu Chen, and Tsung-Yi Ho. Radar: Robust ai-text detection via adversarial learning. Advances in Neural Information Processing Systems, 36:1507715095, 2023. 13 Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei Efros. Image-to-image translation with conditional adversarial networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 11251134, 2017. Lichuan Ji, Yingqi Lin, Zhenhua Huang, Yan Han, Xiaogang Xu, Jiafei Wu, Chong Wang, and Zhe Liu. Distinguish any fake videos: Unleashing the power of large-scale data and motion features. arXiv preprint arXiv:2405.15343, 2024. Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023a. Harry Jiang, Lauren Brown, Jessica Cheng, Mehtab Khan, Abhishek Gupta, Deja Workman, Alex Hanna, Johnathan Flowers, and Timnit Gebru. Ai art and its impact on artists. In Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society, pp. 363374, 2023b. Yan Ju, Shan Jia, Lipeng Ke, Hongfei Xue, Koki Nagano, and Siwei Lyu. Fusing global and local features for generalized ai-synthesized image detection. In 2022 IEEE International Conference on Image Processing (ICIP), pp. 34653469. IEEE, 2022. Jee-weon Jung, Hee-Soo Heo, Hemlata Tak, Hye-jin Shim, Joon Son Chung, Bong-Jin Lee, Ha-Jin Yu, and Nicholas Evans. Aasist: Audio anti-spoofing using integrated spectro-temporal graph attention networks. In ICASSP 2022-2022 IEEE international conference on acoustics, speech and signal processing (ICASSP), pp. 63676371. IEEE, 2022. Nal Kalchbrenner, Erich Elsen, Karen Simonyan, Seb Noury, Norman Casagrande, Edward Lockhart, Florian Stimberg, Aaron Oord, Sander Dieleman, and Koray Kavukcuoglu. Efficient neural audio synthesis. In International Conference on Machine Learning, pp. 24102419. PMLR, 2018. Tero Karras, Samuli Laine, and Timo Aila. style-based generator architecture for generative adversarial networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 44014410, 2019a. Tero Karras, Samuli Laine, and Timo Aila. style-based generator architecture for generative adversarial networks, 2019b. URL https://arxiv.org/abs/1812.04948. Tero Karras, Miika Aittala, Samuli Laine, Erik Harkonen, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Alias-free generative adversarial networks. In Proc. NeurIPS, 2021. Jaehyeon Kim, Jungil Kong, and Juhee Son. Conditional variational autoencoder with adversarial learning for end-to-end text-to-speech. In International Conference on Machine Learning, pp. 55305540. PMLR, 2021. Jungil Kong, Jaehyeon Kim, and Jaekyoung Bae. Hifi-gan: Generative adversarial networks for efficient and high fidelity speech synthesis. Advances in neural information processing systems, 33:1702217033, 2020. Max Ku, Dongfu Jiang, Cong Wei, Xiang Yue, and Wenhu Chen. Viescore: Towards explainable metrics for conditional image synthesis evaluation. arXiv preprint arXiv:2312.14867, 2023. Hugo Laurencon, Leo Tronchon, Matthieu Cord, and Victor Sanh. What matters when building vision-language models?, 2024. Yixuan Li, Xuelin Liu, Xiaoyang Wang, Bu Sung Lee, Shiqi Wang, Anderson Rocha, and Weisi Lin. Fakebench: Probing explainable fake image detection via large multimodal models, 2024a. URL https://arxiv.org/abs/2404.13306. Yixuan Li, Xuelin Liu, Xiaoyang Wang, Shiqi Wang, and Weisi Lin. Fakebench: Uncover the achilles heels of fake images with large multimodal models. arXiv preprint arXiv:2404.13306, 2024b. Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution text-to-3d conIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern tent creation. Recognition, pp. 300309, 2023. 14 Haohe Liu, Zehua Chen, Yi Yuan, Xinhao Mei, Xubo Liu, Danilo Mandic, Wenwu Wang, and Mark Plumbley. Audioldm: Text-to-audio generation with latent diffusion models. arXiv preprint arXiv:2301.12503, 2023a. Haohe Liu, Yi Yuan, Xubo Liu, Xinhao Mei, Qiuqiang Kong, Qiao Tian, Yuping Wang, Wenwu Wang, Yuxuan Wang, and Mark Plumbley. Audioldm 2: Learning holistic audio generation with self-supervised pretraining. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2024a. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning, 2023b. Tianchi Liu, Lin Zhang, Rohan Kumar Das, Yi Ma, Ruijie Tao, and Haizhou Li. How do neural spoofing countermeasures detect partially spoofed audio? arXiv preprint arXiv:2406.02483, 2024b. Yuan Liu, Cheng Lin, Zijiao Zeng, Xiaoxiao Long, Lingjie Liu, Taku Komura, and Wenping Wang. Syncdreamer: Generating multiview-consistent images from single-view image. arXiv preprint arXiv:2309.03453, 2023c. Xiaoxiao Long, Yuan-Chen Guo, Cheng Lin, Yuan Liu, Zhiyang Dou, Lingjie Liu, Yuexin Ma, Song-Hai Zhang, Marc Habermann, Christian Theobalt, et al. Wonder3d: Single image to 3d using cross-domain diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 99709980, 2024. Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver++: Fast solver for guided sampling of diffusion probabilistic models, 2023a. URL https://arxiv. org/abs/2211.01095. Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. Advances in Neural Information Processing Systems, 35:25072521, 2022. Zeyu Lu, Di Huang, LEI BAI, Jingjing Qu, Chengyue Wu, Xihui Liu, and Wanli Ouyang. Seeing is not always believing: Benchmarking human and model perception of ai-generated images. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine (eds.), Advances in Neural Information Processing Systems, volume 36, pp. 2543525447. Curran Associates, URL https://proceedings.neurips.cc/paper_files/paper/ Inc., 2023b. 2023/file/505df5ea30f630661074145149274af0-Paper-Datasets_and_ Benchmarks.pdf. Zeyu Lu, Di Huang, Chunli Zhang, Chengyue Wu, Xihui Liu, Lei Bai, and Wanli Ouyang. Sentryimage leaderboard. https://github.com/Inf-imagine/Sentry, 2023c. Gonzalo Martınez, Lauren Watson, Pedro Reviriego, Jose Alberto Hernandez, Marc Juarez, and Rik Sarkar. Towards understanding the interplay of generative artificial intelligence and the interIn International Workshop on Epistemic Uncertainty in Artificial Intelligence, pp. 5973. net. Springer, 2023. Nichols Michelle. Un security council meets for the first time on ai risks. Reuters. Last accessed, 15, 2023. Mistral. Large enough, 2024. URL https://mistral.ai/news/ mistral-large-2407/. Accessed: 2024-09-13. Mekhail Mustak, Joni Salminen, Matti Mantymaki, Arafat Rahman, and Yogesh Dwivedi. Deepfakes: Deceptions, mitigations, and opportunities. Journal of Business Research, 154:113368, 2023. OpenAI. Hello gpt-4o. https://openai.com/index/hello-gpt-4o/, 2024. OpenAI. Sora: Creating video from text, 2024. URL https://openai.com/sora. Accessed: 2024-09-21. 15 Eleftheria Papageorgiou, Christos Chronis, Iraklis Varlamis, and Yassine Himeur. survey on the use of large language models (llms) in fake news. Future Internet, 16(8):298, 2024. Daniel Park, William Chan, Yu Zhang, Chung-Cheng Chiu, Barret Zoph, Ekin Cubuk, and Quoc Le. Specaugment: simple data augmentation method for automatic speech recognition. arXiv preprint arXiv:1904.08779, 2019. Taesung Park, Alexei Efros, Richard Zhang, and Jun-Yan Zhu. Contrastive learning for unpaired image-to-image translation. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part IX 16, pp. 319345. Springer, 2020. Maria Pawelec. Deepfakes and democracy (theory): How synthetic audio-visual media for disinformation and hate speech threaten core democratic functions. Digital society, 1(2):19, 2022. Yuang Peng, Yuxin Cui, Haomiao Tang, Zekun Qi, Runpei Dong, Jing Bai, Chunrui Han, Zheng Ge, Xiangyu Zhang, and Shu-Tao Xia. Dreambench++: human-aligned benchmark for personalized image generation. arXiv preprint arXiv:2406.16855, 2024. Ben Poole, Ajay Jain, Jonathan Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. arXiv preprint arXiv:2209.14988, 2022. Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. Robust speech recognition via large-scale weak supervision. In International conference on machine learning, pp. 2849228518. PMLR, 2023. Suman Ravuri and Oriol Vinyals. Classification accuracy score for conditional generative models. Advances in neural information processing systems, 32, 2019. Dario Rethage, Jordi Pons, and Xavier Serra. wavenet for speech denoising. In 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 50695073. IEEE, 2018. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1068410695, 2022. Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in Neural Information Processing Systems, 35:2527825294, 2022. Jonathan Shen, Ruoming Pang, Ron Weiss, Mike Schuster, Navdeep Jaitly, Zongheng Yang, Zhifeng Chen, Yu Zhang, Yuxuan Wang, Rj Skerrv-Ryan, et al. Natural tts synthesis by conIn 2018 IEEE international conference on ditioning wavenet on mel spectrogram predictions. acoustics, speech and signal processing (ICASSP), pp. 47794783. IEEE, 2018. Michael Shulman, Georg Kucsko, Martin Camacho, and Keenan Freyberg. Suno, 2022. URL https://suno.com. Ilia Shumailov, Zakhar Shumaylov, Yiren Zhao, Nicolas Papernot, Ross Anderson, and Yarin Gal. Ai models collapse when trained on recursively generated data. Nature, 631(8022):755759, 2024. Gowthami Somepalli, Vasu Singla, Micah Goldblum, Jonas Geiping, and Tom Goldstein. Diffusion art or digital forgery? investigating data replication in diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 60486058, 2023. Haixu Song, Shiyu Huang, Yinpeng Dong, and Wei-Wei Tu. Robustness and generalizability of deepfake detection: study with diffusion models, 2023. Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020a. 16 Xingcheng Song, Zhiyong Wu, Yiheng Huang, Dan Su, and Helen Meng. Specswap: simple data augmentation method for end-to-end speech recognition. In Interspeech, pp. 581585, 2020b. Guangzhi Sun, Wenyi Yu, Changli Tang, Xianzhao Chen, Tian Tan, Wei Li, Lu Lu, Zejun Ma, Yuxuan Wang, and Chao Zhang. video-salmonn: Speech-enhanced audio-visual large language models. arXiv preprint arXiv:2406.15704, 2024. svc-develop team. so-vits-svc: github project for voice conversion. https://github.com/ svc-develop-team/so-vits-svc, 2024. Accessed: 2024-10-01. Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. Commonsenseqa: question answering challenge targeting commonsense knowledge. arXiv preprint arXiv:1811.00937, 2018. Jiaxiang Tang, Jiawei Ren, Hang Zhou, Ziwei Liu, and Gang Zeng. Dreamgaussian: Generative gaussian splatting for efficient 3d content creation. arXiv preprint arXiv:2309.16653, 2023. Diangarti Tariang, Riccardo Corvi, Davide Cozzolino, Giovanni Poggi, Koki Nagano, and Luisa Verdoliva. Synthetic image verification in the era of generative artificial intelligence: What works and what isnt there yet. IEEE Security & Privacy, 2024. Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. Llama Team. The Llama 3 Herd of Models, 2024. URL https://ai.meta.com/research/ publications/the-llama-3-herd-of-models/. TheDataBeast. Ted talk transcripts (2006 - 2021), 2021. URL https://www.kaggle.com/ datasets/thedatabeast/ted-talk-transcripts-2006-2021. Timedomain. Ace studio, 2023. URL https://acestudio.ai/. Accessed: 2024-09-21. Trapoom Ukarapol and Kevin Pruvost. Gradeadreamer: Enhanced text-to-3d generation using gaussian splatting and multi-view diffusion. arXiv preprint arXiv:2406.09850, 2024. Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in neural information processing systems, 30, 2017. Vivek Verma, Eve Fleisig, Nicholas Tomlin, and Dan Klein. Ghostbuster: Detecting text ghostwritten by large language models, 2024. URL https://arxiv.org/abs/2305.15047. Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. Xin Wang, Junichi Yamagishi, Massimiliano Todisco, Hector Delgado, Andreas Nautsch, Nicholas Evans, Md Sahidullah, Ville Vestman, Tomi Kinnunen, Kong Aik Lee, Lauri Juvela, Paavo Alku, Yu-Huai Peng, Hsin-Te Hwang, Yu Tsao, Hsin-Min Wang, Sebastien Le Maguer, Markus Becker, Fergus Henderson, Rob Clark, Yu Zhang, Quan Wang, Ye Jia, Kai Onuma, Koji Mushika, Takashi Kaneda, Yuan Jiang, Li-Juan Liu, Yi-Chiao Wu, Wen-Chin Huang, Tomoki Toda, Kou Tanaka, Hirokazu Kameoka, Ingmar Steiner, Driss Matrouf, Jean-Francois Bonastre, Avashna Govender, Srikanth Ronanki, Jing-Xuan Zhang, and Zhen-Hua Ling. Asvspoof 2019: large-scale public database of synthesized, converted and replayed speech, 2020. URL https://arxiv.org/ abs/1911.01601. Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models. arXiv preprint arXiv:2206.07682, 2022a. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022b. Scott Workman, Richard Souvenir, and Nathan Jacobs. Wide-area image geolocalization with aerial reference imagery. In IEEE International Conference on Computer Vision (ICCV), pp. 19, 2015. doi: 10.1109/ICCV.2015.451. Acceptance rate: 30.3%. Tong Wu, Jiarui Zhang, Xiao Fu, Yuxin Wang, Jiawei Ren, Liang Pan, Wayne Wu, Lei Yang, Jiaqi Wang, Chen Qian, et al. Omniobject3d: Large-vocabulary 3d object dataset for realistic perception, reconstruction and generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 803814, 2023. Tong Wu, Guandao Yang, Zhibing Li, Kai Zhang, Ziwei Liu, Leonidas Guibas, Dahua Lin, and Gordon Wetzstein. Gpt-4v(ision) is human-aligned evaluator for text-to-3d generation. In CVPR, 2024a. Tong Wu, Guandao Yang, Zhibing Li, Kai Zhang, Ziwei Liu, Leonidas Guibas, Dahua Lin, and Gordon Wetzstein. Gpt-4v (ision) is human-aligned evaluator for text-to-3d generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 22227 22238, 2024b. Bright Xu. Nlp chinese corpus: Large scale chinese corpus for nlp, September 2019. URL https: //doi.org/10.5281/zenodo.3402023. Danni Xu, Shaojing Fan, and Mohan Kankanhalli. Combating misinformation in the era of generative ai models. In Proceedings of the 31st ACM International Conference on Multimedia, pp. 92919298, 2023. Shilin Yan, Ouxiang Li, Jiayin Cai, Yanbin Hao, Xiaolong Jiang, Yao Hu, and Weidi Xie. sanity check for ai-generated image detection. arXiv preprint arXiv:2406.19435, 2024. Ziqin Yang, Fuxin Xie, Jian Zhou, Yuan Yao, Cheng Hu, and Baoding Zhou. Aigdet: Altitudeinformation guided vehicle target detection in uav-based images. IEEE Sensors Journal, 2024. Junyan Ye, Jun He, Weijia Li, Zhutao Lv, Jinhua Yu, Haote Yang, and Conghui He. Skydiffusion: Street-to-satellite image synthesis with diffusion models and bev paradigm. arXiv preprint arXiv:2408.01812, 2024. Taoran Yi, Jiemin Fang, Guanjun Wu, Lingxi Xie, Xiaopeng Zhang, Wenyu Liu, Qi Tian, and Xinggang Wang. Gaussiandreamer: Fast generation from text to 3d gaussian splatting with point cloud priors. arXiv preprint arXiv:2310.08529, 2023. Jiquan Yuan, Xinyan Cao, Changjin Li, Fanyi Yang, Jinlong Lin, and Xixin Cao. Pku-i2iqa: An image-to-image quality assessment database for ai generated images, 2023. Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 95569567, 2024. Yongyi Zang, Jiatong Shi, You Zhang, Ryuichi Yamamoto, Jionghao Han, Yuxun Tang, Shengyuan Xu, Wenxiao Zhao, Jing Guo, Tomoki Toda, et al. Ctrsvdd: benchmark dataset and baseline analysis for controlled singing voice deepfake detection. arXiv preprint arXiv:2406.02438, 2024. Yuexiang Zhai, Shengbang Tong, Xiao Li, Mu Cai, Qing Qu, Yong Jae Lee, and Yi Ma. Investigating the catastrophic forgetting in multimodal large language models. arXiv preprint arXiv:2309.10313, 2023. Lingzhi Zhang, Zhengjie Xu, Connelly Barnes, Yuqian Zhou, Qing Liu, He Zhang, Sohrab Amirghodsi, Zhe Lin, Eli Shechtman, and Jianbo Shi. Perceptual artifacts localization for image synthesis tasks. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 7579 7590, 2023a. Longwen Zhang, Ziyu Wang, Qixuan Zhang, Qiwei Qiu, Anqi Pang, Haoran Jiang, Wei Yang, Lan Xu, and Jingyi Yu. Clay: controllable large-scale generative model for creating high-quality 3d assets. ACM Transactions on Graphics (TOG), 43(4):120, 2024a. 18 Pan Zhang, Xiaoyi Dong, Bin Wang, Yuhang Cao, Chao Xu, Linke Ouyang, Zhiyuan Zhao, Shuangrui Ding, Songyang Zhang, Haodong Duan, Wenwei Zhang, Hang Yan, Xinyue Zhang, Wei Li, Jingwen Li, Kai Chen, Conghui He, Xingcheng Zhang, Yu Qiao, Dahua Lin, and Jiaqi Wang. Internlm-xcomposer: vision-language large model for advanced text-image comprehension and composition. arXiv preprint arXiv:2309.15112, 2023b. Qihui Zhang, Chujie Gao, Dongping Chen, Yue Huang, Yixin Huang, Zhenyang Sun, Shilin Zhang, Weiye Li, Zhengyan Fu, Yao Wan, and Lichao Sun. Llm-as-a-coauthor: Can mixed humanwritten and machine-generated text be detected?, 2024b. Zangwei Zheng, Xiangyu Peng, and Yang You. Open-sora: Democratizing efficient video production for all, 2024. URL https://github.com/hpcaitech/Open-Sora. Accessed: 2024-09-21. Baichuan Zhou, Ying Hu, Xi Weng, Junlong Jia, Jie Luo, Xien Liu, Ji Wu, and Lei Huang. Tinyllava: framework of small-scale large multimodal models. arXiv preprint arXiv:2402.14289, 2024. Mingjian Zhu, Hanting Chen, Qiangyu Yan, Xudong Huang, Guanyu Lin, Wei Li, Zhijun Tu, Hailin Hu, Jie Hu, and Yunhe Wang. Genimage: million-scale benchmark for detecting ai-generated image, 2023. Sijie Zhu, Taojiannan Yang, and Chen Chen. Vigor: Cross-view image geo-localization beyond oneto-one retrieval. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 36403649, 2021. Giada Zingarini, Davide Cozzolino, Riccardo Corvi, Giovanni Poggi, and Luisa Verdoliva. M3dsynth: dataset of medical 3d images with ai-generated local manipulations, 2024. URL https://arxiv.org/abs/2309.07973. 19 LOKI: Comprehensive Synthetic Data Detection Benchmark using Large Multimodal Models"
        },
        {
            "title": "Table of Contents in Appendix",
            "content": "A Synthetic Data Detection A.1 Social Impact of Synthetic Data . . . . . . . . . . . . . . . . . . . . . . . . . . . A.2 Synthetic Data Contamination . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.3 Increasing Attention on Synthetic Data Detection . . . . . . . . . . . . . . . . . . Dataset Description B.1 Data Collection . . . B.2 Dataset Annotation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.2.1 Annotation Guidelines . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.2.2 Annotator Informed Consent . . . . . . . . . . . . . . . . . . . . . . . . . B.3 Quality Control and Validation . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.4 Special data description . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Evaluation C.1 Evaluation Model . C.2 Evaluation Metric . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Breakdown Results on Different Modalities D.1 Video . D.2 Image . D.3 3D . . . D.4 Audio . D.5 Text . . Case Study E.1 Image . E.2 Video . E.3 3D . . E.4 Audio . E.5 Text . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 21 21 22 23 24 24 25 27 28 28 29 30 30 30 30 31 37 37 60 68"
        },
        {
            "title": "A SYNTHETIC DATA DETECTION",
            "content": "In this appendix, we introduce and discuss the social impacts of synthetic data, such as deepfakes, as well as data contamination introduced by synthetic data. Finally we discuss the increasing attention on synthetic data detection. A.1 SOCIAL IMPACT OF SYNTHETIC DATA While synthetic data generated by AIGC technology has offered numerous benefits to various aspects of society, it has also introduced significant challenges and risks. One of the most concerning risks is the potential to use synthetic data to create deepfakes. All forms of synthetic data can be leveraged to generate deepfakes, which can then be used to deceive, manipulate, or defraud individuals or organizations (see Fig.6). For instance, synthetic text data can be exploited to create fake news Papageorgiou et al. (2024), phishing emails, or manipulative advertisements. Similarly, synthetic image and 3D data can be used to generate realistic fake facesXu et al. (2023), scenes, or even content that leads to copyright violations Jiang et al. (2023b); Somepalli et al. (2023). Synthetic video data poses threat by enabling the production of fake videos or fake news (e.g., political propaganda Pawelec (2022)), as well as deepfake video fraud calls Mustak et al. (2023). Likewise, synthetic audio data can be used for fake calls, voice cand even fake broadcasts. Furthermore, the advancements in synthetic data technologies are also impacting employment in creative industries, exemplified by the months-long strikes in the film industry Bohacek & Farid (2024). Figure 6: Soical impact of synthetic data across different modalities A.2 SYNTHETIC DATA CONTAMINATION Figure 7: Model Performance Collapse Trained On Synthetic Data (Image from Martınez et al. (2023), Text from Shumailov et al. (2024) ) In todays LLM era, the internet is flooded with substantial amount of synthetic data, even existing web-scale datasets are known to contain synthetic content Schuhmann et al. (2022). According to openai Altman (2024), they now generate about 100 billion words per day, while all people on 21 earth generate about 100 trillion words per day. All of this points to the fact that synthetic data will dominate the internet data side. The use of synthetic data has been shown to significantly degrade the performance of deep learning models (see Fig.7), both for generation tasks and classification tasks Hataya et al. (2023); Ravuri & Vinyals (2019); Martınez et al. (2023); Shumailov et al. (2024); Bohacek & Farid (2023). Addressing the impact of synthetic data is crucial for the development of the next generation of models. There are two primary approaches to mitigating the negative effects of synthetic data. The first is exploring ways to better utilize synthetic data, proposing strategies for optimizing the integration of synthetic data into training pipelines Dohmatob et al. (2024a;b); Feng et al. (2024); Bertrand et al. (2024); He et al. (2023). The second approach involves developing methods to accurately detect synthetic data, allowing models to distinguish between real and synthetic inputs. A."
        },
        {
            "title": "INCREASING ATTENTION ON SYNTHETIC DATA DETECTION",
            "content": "(a) The number of BBC official news reports on deepfake topic over the years. (b) The number of publications on deepfake detection over the years.(From Gong & Li (2024)) Figure 8: The rising concern of deepfakes in both media and academic research. The growing prevalence of synthetic data has garnered increasing attention from society, including news reports, academic research, and government policies. The number of papers on deepfake detection has been steadily increasing, with the BBC reporting on deepfakes more frequently each year (see Fig.8). In response to the rise of synthetic data, several governments and global conferences have also introduced policies aimed at regulating the use of deepfakes and synthetic data AIS (2023); Michelle (2023); Biden (2023)."
        },
        {
            "title": "B DATASET DESCRIPTION",
            "content": "B.1 DATA COLLECTION Our data primarily originates from online internet collections, reused from public datasets, and selfsynthesized into new composite data, as detailed in Table 6. To ensure diversity in synthetic data, each modality incorporates more than five different synthesis methods (Figure 9 & 10). To guarantee the quality of synthetic data, we also collected samples synthesized by mature proprietary models such as Sora, Midjourney, CLAY, Sunoco, and GPT-4. The far-right column of the table displays the public datasets that underpin our collected synthetic or authentically paired data. Table 6: Synthetic Methods and Public Datasets Across Modalities Modality Video Synthesis Methods Sora (OpenAI, 2024), Keling, CoNo, Lumiere(Bar-Tal et al., 2024), Open-sora (Zheng et al., 2024), Runway, W.A.L.T (Gupta et al., 2023) Public Datasets - Image 3D Audio FLUX, DDIM (Song et al., 2020a), Midjourney (AI, 2023), Stable Diffusion (V1.4,V1.5,V2.1) (Blattmann et al., 2023), DPM+ (Lu et al., 2023a), ADM(Dhariwal & Nichol, 2021a), Stylegan (Karras et al., 2019a), Skydiffusion (Ye et al., 2024), pix2pix (Isola et al., 2017), CUT (Park et al., 2020) CLAY (Zhang et al., 2024a), SyncDreamer (Liu et al., 2023c), Magic3D (Lin et al., 2023), DreamFusion (Poole et al., 2022), Fantasia3D (Chen et al., 2023a), DreamGaussian (Tang et al., 2023), Wonder3D (Long et al., 2024), GaussianDreamer (Yi et al., 2023), GradeADreamer (Ukarapol & Pruvost, 2024) Suno, WaveNet (Rethage et al., 2018), WaveRNN (Kalchbrenner et al., 2018), Tacotron2 (Shen et al., 2018), Hifi-GAN (Kong et al., 2020), AceSinger (Timedomain, 2023), Soft-VITS-SVC (svc-develop team, 2024), DiffSinger (Dhariwal & Nichol, 2021b), VQ-VAE (Van Den Oord et al., 2017), AudioLDM (Liu et al., 2023a), VITS (Kim et al., 2021), AudioLDM2 (Liu et al., 2024a), MusicGen (Copet et al., 2024) I2IQA(Yuan et al., 2023), Sentry(Lu et al., 2023c), GenImage(Zhu et al., 2023), FFHQ(Karras et al., 2019b), Stylegan3(Karras et al., 2021), CVUSA(Workman et al., 2015), ISBI 2016(Gutman et al., 2016), M3DSynth(Zingarini et al., 2024), M6Doc(Cheng et al., 2023), Deepfakeface(Song et al., 2023), VIGOR(Zhu et al., 2021) OmniObject3D(Wu et al., 2023), GPTEval3D(Wu et al., 2024a) ASVSpoof2019(Wang et al., 2020), CtrSVDD(Zang et al., 2024), DCASE2023 Track 7(Choi et al., 2023), MusicCaps(Agostinelli et al., 2023) Text llama3.1-405B (Team, 2024) , GPT-4o (OpenAI, 2024), Qwen-Max (Bai et al., 2023), Mistral-Large (Jiang et al., 2023a), Claude-3.5-Sonnet (Anthropic, 2024), Gemini-1.5-Flash (Team et al., 2023) TheDataBeast (TheDataBeast, 2021),Mixset(Zhang et al., 2024b), NLP chinese corpus(Xu, 2019), ghostbuster-data(Verma et al., 2024) Figure 9: Examples of some 3D and image datasets, with the bar chart showing the quantity of data in different categories. 23 Authentic pairing data: We have collected significant amount of authentic paired data from the internet, including sources such as arXiv, Wikipedia, Gutenberg, YouTube, TikTok, and Civitai. For data sourced from the internet, we will rigorously verify that it consists of authentic recordings or text authored by human users, rather than content synthesized using AI technology. It is important to note that our current research primarily focuses on multimedia data directly synthesized by AI, with limited consideration of methods like deepfake involving manual editing; we will continue to update our approach in future studies. Data Availability and Social Impact: In collecting data, we strictly adhere to copyright and licensing regulations of the source websites, avoiding data acquisition from resources that prohibit copying or redistribution. For the LOKI dataset, which is open-sourced, users must submit download request to the authors to prevent misuse of the data. Figure 10: Examples of video data. We used 7 video generation models to obtain corresponding data for LMMs evaluation. 24 B.2 DATASET ANNOTATION B.2.1 ANNOTATION GUIDELINES Video: During the annotation of synthetic videos, we categorize the identified anomalies into two types: global anomalies and segment anomalies. Global anomalies refer to errors that persist for more than 80% of the videos duration, while segment anomalies are issues that occur for limited portion of the video. For example, as shown in Fig. 11 (a), the anomaly of flickering textures and distorted geometries of fences and utility poles, which is present throughout the video, is labeled as global anomaly. In contrast, the abnormal flames and basketball penetrating the hoop in the video are classified as segment anomalies. Additionally, each identified anomaly, including both global and segment anomalies, is associated with key frame that represents the anomaly, facilitating subsequent processing of video data by large multimodal models (LMMs). Image: For the synthetic image data, we provide global anomaly annotations for overall image issues, as well as bounding box selections and textual descriptions for abnormal regions. The bounding boxes indicate the location and extent of the abnormal areas within the image, while the textual descriptions detail the specific anomalies present in those regions. As shown in Fig. 11 (b), the texture quality issues and color distortion in the image are annotated as global anomalies, whereas area errors such as the texture errors of the duck and reflection anomalies on the water surface are classified as region anomalies. Annotators mark these areas by drawing bounding boxes and provide textual explanations for the reasons behind the anomalies. 3D Data: Unlike video and image annotations, the annotation of 3D data involves global-scale analysis of textures and normals. In terms of texture anomalies, we focus on assessing the authenticity, smoothness, and edge clarity of the textures. For normals, we analyze whether the models geometric fluidity, surface smoothness, physical stability, and topological coherence are accurately represented. For instance, as shown in Fig. 11 (c), we conduct detailed analysis of the multiview discrepancies and texture blurriness in the models textures, while labeling issues such as abnormal protrusions and asymmetrical structures as related to normals, accompanied by appropriate textual explanations. Figure 11: Examples of the synthetic data annotation process under different modalities, including (a) Video, (b) Image, (c) 3D Data. 25 B.2.2 ANNOTATOR INFORMED CONSENT Before commencing the annotation process, we ensure that all participating annotators are fully informed and provide their explicit agreement to the following terms and conditions. This comprehensive informed consent is designed to promote transparency, respect their autonomy, and align with ethical standards in research. It is imperative that each annotator has thorough understanding of the nature, purpose, and potential implications of their contributions to the labeling process. The terms are outlined as follows: Data Usage. Annotators acknowledge and consent to the possibility that the labeled data they generate may be used in various academic and scientific contexts, including the development of research papers, presentations at conferences, and other related scholarly activities. They understand that their work may significantly contribute to advancements in research fields such as natural language processing, machine learning, and artificial intelligence. Furthermore, annotators recognize that their contributions may be referenced or cited in scientific publications, thereby playing role in shaping future research directions and applications. AI-Generated Content. Annotators are informed that some of the content they will be labeling may have been produced by artificial intelligence models. This includes text, images, or other data types generated by algorithms designed to simulate human-like outputs. Annotators understand that this knowledge is crucial, as it may influence their perception, judgment, and approach to the labeling task. They agree to remain mindful of the potential biases or preconceived notions that may arise from this awareness and commit to maintaining objectivity and accuracy in their work. Potential Implications. Annotators are aware of the broader implications of their labeling activities, which extend beyond the immediate scope of data annotation. They recognize the ethical considerations inherent in AI research, particularly concerning issues such as bias, fairness, and the societal impact of deploying AI technologies. Annotators agree to reflect on these ethical dimensions and to engage in the labeling process with conscientious approach, acknowledging that their work may contribute to both the positive advancements and challenges associated with AI development and implementation. Commitment to Ethical Standards. By agreeing to these terms, annotators affirm their commitment to upholding high ethical standards throughout the annotation process. They understand that their participation is voluntary and that they have the right to withdraw from the project at any time, without penalty. Annotators also acknowledge their responsibility to report any concerns or issues that may arise during the labeling process, ensuring the integrity and reliability of the data they provide. This informed consent process ensures that all annotators are equipped with comprehensive understanding of their role and its significance. It aims to foster an environment of mutual respect and collaboration, where the contributions of annotators are valued and their rights as participants in research are protected. By clarifying the expectations and responsibilities involved, we seek to create foundation for ethical and impactful research that benefits both the scientific community and society at large. B.3 QUALITY CONTROL AND VALIDATION In annotating videos, images, and 3D synthetic data for anomaly details, we maintain high standards and accuracy. All annotators possess at least university degree and demonstrate strong decisionmaking and judgment skills. Before annotation, human annotators receive extensive training with numerous examples of common errors to ensure comprehensive understanding of the synthetic data detection task. Each data instance is annotated for detailed anomalies by at least two human annotators to ensure quality. Ambiguous or unclear instances are marked for further study and annotation during team meetings. Furthermore, to avoid the impact of hallucinations from Large Language Models (LMMs) on tasks involving LMMs, all anomaly detail explanation tasks undergo manual review based on LabelLLM 3. 3https://github.com/opendatalab/LabelLLM 26 B.4 SPECIAL DATA DESCRIPTION In the field of document images, we have collected synthesized images in four categories: newspapers, academic papers, magazines, and reconstructed documents. The corresponding real data for these categories come from the M6Doc dataset. Currently, document synthesis primarily follows layout-first, content-rendering-later approach. For the first three document types, we design specific empirical rules for layout generation during the layout phase; during the content rendering phase, elements are selected from constructed corpus to fill the structure. For the reconstructed type, we employ restructuring algorithm on the M6Doc dataset to rearrange the content of the documents. In the field of remote sensing imagery, synthetic data is primarily generated based on street-tosatellite datasets such as CVUSA and VIGOR, utilizing methods like CUT and Skydiffusion. The synthetic imagery encompasses major natural scenes such as urban and suburban environments, with the satellite remote sensing images being of high resolution. In the field of medical imaging, we primarily collected two types of data: the ISIC 2016 skin dataset and the M3DSynth CT dataset. For the ISIC 2016 dataset, we utilized GAN methods for direct data synthesis. The M3DSynth dataset comprises synthetic images generated from the real-world LIDC dataset using Diffusion and GAN models. The images are categorized into two types: those with real tumors removed and those with synthetic tumors artificially inserted by the model. Each synthetic image is paired with its corresponding original image, complete with precise annotations of the tumor insertion or removal locations. Considering that most users are not medically trained, we deliberately selected more evident abnormal images to reduce the need for specialized knowledge in making decisions about synthetic data."
        },
        {
            "title": "C EVALUATION",
            "content": "C.1 EVALUATION MODEL We compared various models on the LOKI benchmark to understand their capabilities across multiple tasks. We support over ten open-source models, including InternVL2 (Chen et al., 2024), LLaVA (Liu et al., 2023b), Phi (Gunasekar et al., 2023), XComposer (Zhang et al., 2023b), Qwen2VL (Wang et al., 2024), MiniCPM (Hu et al., 2024), TinyLLaVA (Zhou et al., 2024) and Idefics2 (Laurencon et al., 2024), as well as proprietary models such as GPT-4 (OpenAI, 2024), Gemini (Team et al., 2023) and Claude (Anthropic, 2024). The following list details these models. Model Family Model Version Parameters Links Close-sourced, API GPT4 Gemini Claude Mistral Qwen GPT-4o GPT-4 Gemini-1.5-Pro Gemini-1.5-Flash Claude-3.5-Sonnet Mistral-Large Qwen-Max N/A N/A N/A N/A N/A N/A N/A https://platform.openai.com/docs/models/gpt-4o https://platform.openai.com/docs/models/ gpt-4-turbo-and-gpt-4 https://ai.google.dev/gemini-api/docs/models/ gemini#gemini-1.5-pro https://ai.google.dev/gemini-api/docs/models/ gemini#gemini-1.5-flash https://docs.anthropic.com/en/docs/ about-claude/models https://docs.mistral.ai/getting-started/models/ https://www.alibabacloud.com/help/ en/model-studio/developer-reference/ use-qwen-by-calling-api Open-sourced LLaMA LLaMA-3.1-405B 405B InternVL2 LLaVA-OneVision VILA Phi Idefics2 Qwen2-VL InternVL2-8B InternVL2-26B InternVL2-40B InternVL2-Llama3-76B LLaVA-OneVision-7B LLaVA-OneVision-72B VILA-1.5-13B VILA-1.5-40B Phi-3.5-Vision idefics2-8b Qwen2-VL-7B Qwen2-VL-72B InternLM-XComposer InternLM-XComposer-2d5 mPLUG-Owl3 MPlug-Owl3 8B 26B 40B 76B 7B 72B 13B 40B 3.5B 8B 7B 72B 7B 7B https://huggingface.co/meta-llama/Llama-3. 1-405B https://huggingface.co/OpenGVLab/InternVL2-8B https://huggingface.co/OpenGVLab/InternVL2-26B https://huggingface.co/OpenGVLab/InternVL2-40B https://huggingface.co/OpenGVLab/ InternVL2-Llama3-76B https://huggingface.co/lmms-lab/ llava-onevision-qwen2-7b-ov https://huggingface.co/lmms-lab/ llava-onevision-qwen2-72b-ov-sft https://huggingface.co/Efficient-Large-Model/ VILA1.5-13b https://huggingface.co/Efficient-Large-Model/ VILA1.5-40b https://huggingface.co/microsoft/Phi-3. 5-vision-instruct https://huggingface.co/HuggingFaceM4/ idefics2-8b https://huggingface.co/Qwen/ Qwen2-VL-7B-Instruct https://huggingface.co/Qwen/ Qwen2-VL-72B-Instruct https://huggingface.co/internlm/ internlm-xcomposer2d5-7b https://huggingface.co/mPLUG/ mPLUG-Owl3-7BMiniCPM MiniCPM-V2.6 8.1B https://huggingface.co/openbmb/MiniCPM-V-2_6 LongVILA Llama-3-LongVILA-8B-128Frames LongVA Qwen-Audio SALMONN AnyGPT OneLLM LTU LongVA-7B Qwen-Audio-Chat SALMONN-7B AnyGPT-Chat OneLLM-7B LTU-AS-7B https://huggingface.co/Efficient-Large-Model/ Llama-3-LongVILA-8B-128Frames https://huggingface.co/lmms-lab/LongVA-7B-DPO https://huggingface.co/Qwen/Qwen-Audio-Chat https://huggingface.co/tsinghua-ee/SALMONN-7B https://huggingface.co/fnlp/AnyGPT-chat https://huggingface.co/csuhan/OneLLM-7B https://github.com/YuanGongND/ltu# pretrained-models 8B 7B 7B 7B 7B 7B 7B 28 C.2 EVALUATION METRIC Average accuracy: For judgment, multiple-choice, and detailed selection questions, we use the average accuracy as the primary metric. The accuracy rate is calculated using the following formula: Accuracy ="
        },
        {
            "title": "Ncorrect\nNtotal",
            "content": "100% In this context, Ncorrect is the number of correctly answered questions, and Ntotal is the total number of questions. To minimize the influence of prompts on model judgments, each question is presented in two forms: one asks whether the data is AI-synthesized or real, and the other asks the model to identify either the real or AI-synthesized data. By averaging the accuracy rates across different forms of the questions, we aim to reduce the potential bias introduced by the phrasing of prompts and ensure fair evaluation of the models performance. Normalized Bias Index (NBI): To evaluate whether there is potential bias in existing models when determining authenticity on the LOKI benchmark, we introduce metric termed the Normalized Bias Index (NBI) to quantify the performance differences of the model on natural and AI-generated data across different modalities, which is defined as follows: NBI = Rnatural Rgenerated Rnatural + Rgenerated [1, 1] In this context, Rnatural and Rgenerated represent the recall rates for natural and AI-generated samples, respectively, under the corresponding modality. By normalizing the difference between the two, the models unexpected preference in making predictions can be quantified. Specifically, positive and larger NBI indicates that the model is more biased toward predicting samples as natural, whereas negative and smaller NBI suggests bias toward predicting samples as AI-generated. GPT-Score: For open-ended questions regarding anomalous details, we use the GPT-4 model to assess the score of the responses. We adopted rating scheme, establishing 5-level rating system with scores ranging from 1 (poor) to 5 (excellent). The final scores are normalized to scale of 0 to 100. We adhere to the following scoring criteria: 1) Identification: Accurately detect the globally annotated anomalies and their corresponding detailed anomalous regions specified by human annotators. 2) Explanation: Provide accurate explanations for the causes of the anomalies, ensuring consistency with the reasons outlined in the human annotations. 3) Plausibility: Avoid misclassifying authentic regions as anomalous while encouraging other reasonable explanations for anomalies. While the scoring criteria are similar across different modalities, they are slightly adjusted according to their content characteristics; for example, the image modality is subdivided into global score and regional score, whereas 3D data is subdivided into texture score and normal score. Figure 12: The overall process for automated evaluation of Abnormal explanation questions using GPT-4o."
        },
        {
            "title": "D BREAKDOWN RESULTS ON DIFFERENT MODALITIES",
            "content": "In this appendix, we present detailed results of various synthesis methods or classification themes across different modalities of data. D.1 VIDEO Tables 7 and 8 display the evaluation results of LMMs in video modality for judgment and multiplechoice questions, respectively. In the evaluation of video modality, the proprietary model GPT-4o achieved an accuracy of 71.3% for judgment questions and 77.3% for multiple-choice questions. The open-source model InternVL2-40B recorded accuracies of 62.0% for judgment questions and 65.7% for multiple-choice questions. Table 9 presents the evaluation results of different models in the video modality for open-ended questions. Additionally, using GPT-4s detection outcomes as benchmark, we assessed the quality of videos synthesized by different models. It was found that videos generated by Sora had judgment accuracy of 67.2% and multiple-choice accuracy of 64.5%, indicating high quality of video synthesis. Conversely, videos synthesized by Runway and W.A.L.T exhibited more noticeable synthetic traces. D.2 IMAGE Tables 10 and 11 present the evaluation results of LMMs in the image modality for judgment and multiple-choice questions, broken down by specific categories. The proprietary model GPT-4o scored judgment accuracy of 63.4%, primarily impacted by specialized image types such as Satellite, Doc, and Medicine, while achieving multiple-choice accuracy of 80.8%. Among open-source models, Qwen2-VL-72B performed exceptionally well, even surpassing GPT-4o in specialized image categories. Table 12 presents the evaluation results of different models in the Image modality for open-ended questions. D.3 3D Table 13 presents the evaluation results for different 3D synthesis methods. Tests on judgment questions for 3D models reveal that most open-source models perform close to 50% random chance. In contrast, the proprietary model GPT-4o demonstrates better decision-making and judgment capabilities, particularly in identifying low-quality Nerf models, where it achieves an accuracy rate exceeding 75Assuming the performance of GPT-4o as benchmark, we can assess the quality of 3D data synthesized using different methods. Among three synthesis methods, Gaussian synthesis exhibits higher quality compared to Nerf synthesis, while commercial models like Clay offer the best quality. Table 14 presents the evaluation results of different models in the Image modality for open-ended questions. As shown in Table 15, we extracted 200 3D objects, with an equal ratio of real to synthetic data, and evaluated the performance of different 3D input formats, including point clouds, surround videos with eight input frames, and multi-view inputs. In comparison, for current LMMs, the performance between surround video and multi-view inputs is similar. PointLLM, which partially supports point cloud inputs, performs particularly well, indicating that the point cloud format still holds certain advantages. Additionally, our results indicate that compared to the 7B PointLLM, the 13B PointLLM is more inclined to respond with cannot determine whether this is real object or one generated by AI rather than straightforward Yes or No. We speculate that as the size of PointLLM increases, it incorporates more comprehensive measures to correct illusions, making it less likely to provide definitive answers when faced with uncertainties (Wei et al., 2022a). D.4 AUDIO Table 16 presents the evaluation results of various audio LMMs on the LOKI dataset. Currently, there are few open-source and proprietary LMMs that support the audio modality, and most models perform poorly on synthetic data detection tasks, showing little difference from random selection. However, the AASIST method, trained specifically for speech tasks, demonstrates relatively strong performance, although it still underperforms in other modalities. Overall, human performance in 30 the audio modality is comparatively weak relative to other modalities, yet it still exceeds the bestperforming LMMs by 20%. It has been shown that acoustic features, such as pauses between words and silent segments within audio, play more critical role in detecting deepfake audio than the actual content of the audio (Liu et al., 2024b). However, current audio language models are primarily focused on content comprehension rather than acoustic characteristics. From the model architecture perspective, audio language models typically initialize their audio encoders with pre-trained audio models such as Whisper (Radford et al., 2023), AST (Gong et al., 2021), or BEATs (Chen et al., 2023b). These pre-trained models are trained with various data augmentation techniques, including SpecAugmentPark et al. (2019) and SpecSwap (Song et al., 2020b). The target of the augmented data remains constant, often focused on tasks like transcribing textual content, which shifts the models attention towards understanding content rather than being sensitive to acoustic features. On the training data side, most audio-text datasets to date emphasize content comprehension in tasks like speech recognition, emotion/age recognition, audio/music captioning, sound event classification and music genre recognition. As result, these models lack the inherent capability to distinguish between real and fake audio based on acoustic cues. D.5 TEXT Tables 17 and 18 present the test results across different genres in the text modality. Most models underperform in this modality, primarily due to the low distinctiveness of text data itself, coupled with the maturity of current text synthesis technologies. Notably, genres such as modern literature and philosophy may benefit from the extensive training and memory capabilities of GPT series models, showing relatively better performance compared to other categories. In Table 19, we evaluated the detection results for texts of varying lengths. The findings indicate that longer texts are detected more frequently and are more likely to reveal flaws. Additionally, we assessed the performance across different languages and found that, except for GPT-4o, most proprietary models perform significantly better in English than in Chinese. This indicates that the multilingual capabilities of large multimodal models require further development. Table 7: Judgement questions results of different models on the LOKI Video modality. * denotes the closed-source models. Overall Sora Keling CogVideoX Lumiere Open-sora Runway W.A.L.T Expert (AIGVDet) MiniCPM-V-2.6 Phi-3.5-Vision LLaVA-OneVision-7B InternLM-XComposer2.5 mPLUG-Owl3-7B Qwen2-VL-7B LongVA-7B Mantis-8B Idefics2-8B InternVL2-8B Llama-3-LongVILA-8B VILA1.5-13B InternVL2-26B VILA1.5-40B InternVL2-40B Qwen2-VL-72B LLaVA-OneVision-72B Gemini-1.5-Pro* Claude-3.5-Sonnet* GPT-4o* 52.1 57.2 56.8 56.8 58.4 55.3 59.5 60.4 55.4 54.8 60.8 51.9 51.9 55.0 59.2 62.0 59.6 56.5 58.5 61.7 71.3 39.5 57.9 52.7 57.6 55.2 56.7 60.1 66.2 48.8 56.1 61.9 54.0 51.2 55.2 54.9 61.3 58.2 57.0 53.4 60.1 66. 61.5 56.5 52.0 53.7 61.1 54.3 58.5 51.4 50.0 54.0 56.3 49.1 50.9 53.4 57.4 57.7 61.4 55.4 60.5 57.4 67.9 65.0 54.0 53.0 61.0 63.0 54.0 54.0 65.0 57.0 51.0 69.0 53.0 51.0 61.0 66.0 68.0 59.0 62.0 58.0 59.0 70. 31 54.2 66.4 57.1 57.1 60.0 56.4 59.3 60.0 52.9 57.9 59.3 52.1 54.3 57.9 61.4 60.7 58.6 55.7 63.6 68.6 68.6 56.0 55.2 57.5 56.6 57.1 54.2 51.4 55.7 51.9 51.4 61.3 48.1 52.8 52.4 59.9 61.8 54.7 51. 62.3 64.2 72.6 37.1 45.8 55.1 53.7 49.5 45.4 59.7 73.6 66.7 41.2 56.5 52.8 52.3 50.0 54.2 62.0 58.8 46.8 45.4 49.5 76.9 61.0 62.6 66.5 59.6 62.6 61.3 65.9 57.7 62.4 64.3 64.8 54.1 51.9 58.2 64.8 65.9 63.2 64. 64.8 71.2 75.8 Table 8: Multiple Choice questions results of different models on the LOKI Video modality. * denotes the closed-source models. Overall Sora Keling CogVideoX Lumiere Open-sora Runway W.A.L.T MiniCPM-V-2.6 Phi-3.5-Vision LLaVA-OneVision-7B InternLM-XComposer2.5 mPLUG-Owl3-7B Qwen2-VL-7B LongVA-7B Mantis-8B Idefics2-8B InternVL2-8B Llama-3-LongVILA-8B VILA1.5-13B InternVL2-26B VILA1.5-40B InternVL2-40B Qwen2-VL-72B LLaVA-OneVision-72B Gemini-1.5-Pro* Claude-3.5-Sonnet* GPT-4o* 52.8 58.2 59.8 56.3 60.3 64.0 57.5 47.9 55.6 54.0 54.0 52.1 62.4 49.1 65.7 65.7 62. 66.1 60.5 77.3 39.5 57.9 55.3 43.4 53.9 63.2 59.2 56.6 56.6 53.9 50.0 51.3 61.8 50.0 53.9 57.9 52.6 46.1 53.9 61.8 62.1 42.4 62.1 54.5 53.0 62.1 50.0 43.9 54.5 57.6 47.0 53.0 57.6 48.5 62.1 62.1 48.5 60.6 54.5 71.2 55.6 50.0 63.9 66.7 61.1 52.8 58.3 50.0 55.6 55.6 55.6 55.6 66.7 47.2 77.8 72.2 66. 66.7 58.3 75.0 58.8 70.6 66.2 67.6 66.2 69.1 63.2 52.9 57.4 61.8 61.8 60.3 63.2 54.4 76.5 72.1 69.1 75.0 72.1 94.1 54.0 58.0 56.0 62.0 62.0 66.0 52.0 52.0 58.0 48.0 54.0 54.0 56.0 50.0 56.0 60.0 74.0 64.0 72.0 78.0 48.0 68.0 64.0 54.0 62.0 70.0 58.0 42.0 52.0 52.0 52.0 50.0 68.0 42.0 66.0 80.0 78. 72.0 58.0 82.0 53.7 58.5 54.9 53.7 64.6 62.2 59.8 39.0 54.9 48.8 57.3 43.9 64.6 48.8 70.7 62.2 61.0 79.3 57.3 80.5 Table 9: Abnormal Explanation questions results of different models on the LOKI Video modality. * denotes the closed-source models."
        },
        {
            "title": "Plausibility",
            "content": "LLaVA-OV-7B InternVL2-8B Qwen2-VL-7B-Instruct Claude-3-5-sonnet* GPT-4o* 46.7 46.5 48.4 50.1 67.6 41.0 38.0 41.2 44.2 60. 48.4 48.4 50.4 48.8 69.2 50.8 53.2 53.6 57.4 73.0 32 Table 10: Judgment questions results of different models on the LOKI Image modality. * denotes the closed-source models. Overall Scene Animal Person Object Medicine Expert (AIDE) MiniCPM-V-2.6 Phi-3.5-Vision LLaVA-OneVision-7B InternLM-XComposer2.5 mPLUG-Owl3-7B Qwen2-VL-7B LongVA-7B Mantis-8B Idefics2-8B InternVL2-8B Llama-3-LongVILA-8B VILA1.5-13B InternVL2-26B VILA1.5-40B InternVL2-40B Qwen2-VL-72B LLaVA-OneVision-72b Claude-3.5-Sonnet* Gemini-1.5-Pro* GPT-4o* 63.1 44.8 52.5 49.8 46.4 45.9 47.8 46.2 54.6 45.0 49.7 49.8 49.3 44.3 48.8 49.6 53.2 46.3 53.6 43.5 63.4 - 52.0 50.8 59.2 52.7 52.1 54.7 57.6 54.9 51.8 58.8 49.8 52.0 51.6 53.7 55.7 55.9 54.7 51.6 53.7 70.1 89.9 34.4 41.7 41.9 40.0 37.3 38.9 37.4 52.2 35.3 39.4 50.5 38.6 35.4 39.3 37.3 43.4 31.6 51.6 35.7 69.7 62. 53.1 71.5 58.1 56.7 52.9 57.9 52.5 54.8 52.3 54.4 50.6 54.2 50.8 50.0 59.2 66.9 53.1 55.2 51.5 84.4 96.5 31.5 34.1 37.3 32.5 31.4 30.3 34.1 53.5 29.2 37.8 47.2 31.0 28.2 33.4 34.8 38.0 27.8 51.4 30.3 70.3 53. 53.8 57.3 52.3 56.1 55.3 56.0 54.4 53.1 52.3 53.9 50.0 50.1 51.3 52.5 55.5 55.9 52.1 51.9 50.0 54.3 Doc 49.7 51.5 54.3 53.0 49.8 53.8 59.6 49.8 51.9 53.9 60.2 49.9 56.6 54.4 59.9 64.8 73.7 67.9 59.1 47.2 60. Satellite 39.3 38.3 60.5 50.1 38.2 38.1 36.9 39.7 63.3 40.6 44.2 50.0 62.4 37.6 50.6 40.8 38.2 36.6 50.9 38.1 45.0 Table 11: Multiple Choice questions results of different models on the LOKI Image modality. * denotes the closed-source models. Overall Scene Animal Person Object Medicine MiniCPM-V-2.6 Phi-3.5-Vision LLaVA-OneVision-7B InternLM-XComposer2.5 mPLUG-Owl3-7B Qwen2-VL-7B LongVA-7B Mantis-8B Idefics2-8B InternVL2-8B Llama-3-LongVILA-8B VILA1.5-13B InternVL2-26B VILA1.5-40B InternVL2-40B Qwen2-VL-72B LLaVA-OneVision-72b Claude-3.5-Sonnet Gemini-1.5-Pro* GPT-4o* 49.8 44.0 51.7 51.0 52.5 65.1 51.6 61.5 51.3 51.4 51.1 55.3 48.5 64.0 63.1 68.6 70.8 65.5 67.3 80.8 49.2 31.3 55.9 48.4 53.1 60.9 58.2 66.0 47.3 56.6 48.4 55.5 50.4 59.8 60.9 60.9 65.6 58.6 72.7 79.7 49.2 67.9 63.3 46.3 53.8 70.4 42.9 57.1 56.3 54.2 47.5 58.3 50.8 58.8 62.5 60.8 76.7 75.8 74.2 91. 48.5 20.7 46.9 50.5 49.5 60.5 48.2 56.9 55.4 48.5 51.0 56.9 40.3 53.8 62.5 68.9 67.1 77.8 69.4 94.9 50.5 69.8 52.8 57.0 55.5 68.0 55.3 62.3 46.5 48.8 54.3 57.5 50.8 71.5 62.3 74.3 73.0 70.5 55.8 65.3 52.3 22.9 51.8 49.5 53.5 71.3 54.9 58.1 55.3 51.2 49.8 57.4 49.1 71.0 72.0 71.1 62.3 53.3 70.1 92. 33 Doc 45.1 58.3 51.4 52.0 49.4 63.1 46.6 68.3 48.0 50.6 53.1 54.9 53.1 62.9 61.6 77.7 77.6 68.6 64.0 81.7 Satellite 51.6 52.1 46.3 51.2 52.3 59.6 51.6 63.6 49.3 52.6 52.1 47.7 46.7 63.8 56.8 61.0 72. 59.3 68.0 61.0 Table 12: Abnormal Explanation questions results of different models on the LOKI Image modality. * denotes the closed-source models. Overall Global Score Completeness Correctness Plausibility Qwen2-VL-7B-Instruct Llava-OV-7B InternVL2-8B Claude-3-5* Gemini-1.5-pro* GPT-4o* 63.8 68.9 72.2 1.7 77.1 72.9 66.6 72.4 70. 2.0 77.6 69.8 60.8 63.0 67.6 1.6 70.8 68.6 53.2 60.6 61.2 1.4 70.0 63.4 74.4 79.8 85. 1.8 90.2 90.0 Table 13: Judgment & Multiple Choice questions results of different models on the LOKI 3D modality. * denotes the closed-source models. Judgment Multiple Choice Nerf Gaussian Other Overall Nerf Gaussian Other Overall MiniCPM-V-2.6 Phi-3.5-Vision LLaVA-OneVision-7B InternLM-XComposer2.5 mPLUG-Owl3-7B Qwen2-VL-7B LongVA-7B Mantis-8B Idefics2-8B InternVL2-8B Llama-3-LongVILA-8B VILA1.5-13B InternVL2-26B VILA1.5-40B InternVL2-40B Qwen2-VL-72B LLaVA-OneVision-72B Claude-3.5-Sonnet* Gemini-1.5-Pro* GPT-4o* 56.7 50.0 69.3 42.8 49.1 72.1 49.6 50.0 38.5 48.8 32.3 32.3 50.3 50.0 49.8 60.9 53.1 59.5 60.5 65.9 56.1 50.0 67.4 44.6 50.5 73.2 50.0 50.0 38.1 49.8 32.1 35.5 50.4 50.0 50.0 60.3 49.6 57.1 50.7 64. 56.7 50.0 69.0 44.4 49.8 70.4 50.0 50.0 39.0 49.6 32.5 33.8 50.4 50.0 50.0 59.0 51.9 57.1 56.0 65.4 56.4 50.0 68.4 43.9 49.9 72.3 49.9 50.0 38.4 49.4 32.2 34.0 50.4 50.0 49.9 60.3 51.3 58.0 55.4 65.2 50.6 62.0 52.5 41.2 46.6 53.2 63.2 51.3 51.7 52.7 51.5 49.0 44.8 44.1 54.0 56.5 63.0 52.3 64.3 78. 50.0 56.1 54.6 51.0 51.7 57.6 59.9 73.0 56.5 54.3 49.8 57.3 51.2 50.2 63.6 61.1 58.3 52.0 56.0 66.4 52.9 62.9 54.6 55.4 52.5 55.0 61.3 60.4 54.2 51.3 50.0 53.8 51.7 50.4 63.8 57.5 65.0 50.8 61.7 60.8 50.7 59.6 53.8 48.0 49.9 55.5 61.4 62.5 54.2 53.1 50.5 53.5 48.8 47.9 59.9 58.7 61.3 51.9 60.2 70. Table 14: Abnormal Explanation questions results of different models on the LOKI 3D modality. * denotes the closed-source models. Overall Texture Correctness Normal Correctness Texture Plausibility Normal Plausibility Llava-OV-7B InternVL2-8B Qwen2-VL-7B-Instruct Claude-3-5-sonnet* Gemini-1.5-pro* GPT-4o* 71.0 71.3 73.4 78.2 70.8 77.0 63.2 62.4 64.8 70.0 62.4 69. 70.8 69.4 72.2 80.0 67.2 64.2 73.8 76.4 77.8 78.8 77.2 82.4 76.2 76.8 78.6 83.8 76.4 81. Table 15: Judgment & Multiple Choice questions results of different models on the LOKI 3D modality. S-Video denotes Surround Video, MV-Image denotes Multi-Views Image. Judgment Multiple Choice S-Video MV-Image S-Video MV-Image Judgment PointCloud LLaVA-OneVision-7B Idefics2-8B LLaVA-OneVision-72B 67.4 38.1 49.6 69.0 39.0 51.9 54.6 56.5 58.3 PointLLM-7B PointLLM-13B OneLLM 61.9 16.9 55.6 52.5 51.7 63.0 34 Table 16: Overall results of different models on the LOKI Audio modality. Judgment Multiple Choice Speech (800) Singing (800) Audio (280) Music (400) Overall (2280) Speech (400) Singing (400) Audio (140) Music (300) Overall (1240) AASIST Jung et al. (2022) Qwen-Audio SALMONN-7B AnyGPT OneLLM LTU Qwen-Audio2 Gemini-1.5-Flash Team et al. (2023) 96.4 50.0 52.8 48.5 50.3 40.2 49.4 50.1 54.4 49.8 52.4 52.4 50.2 45. 40.0 48.3 57.1 49.8 49.3 51.4 50.9 50.5 49.6 47.3 54.3 49.7 46.8 45.9 47.9 46. 27.5 49.8 69.4 49.8 51.2 49.8 49.9 44.4 42.3 49.4 - 50.8 - 50.8 - - 49.3 48.6 - 48.0 - 48.0 - - 49.3 49.5 - 58.9 - 58.2 - - 48.6 48.6 - 48.0 - 49.0 - - 51.3 49.7 - 50.1 - 50.3 - - 49.7 49.2 Table 17: Judgment questions results of different models on the LOKI text modality. * denotes the closed-source models. Scientific Papers News Essay Wikipedia Speech Modern Literature Phiosophy Ancient Chinese Expert (RADAR-Vicuna-7B) MiniCPM-V-2.6 Phi-3.5-Vision LLaVA-OneVision-7B InternLM-XComposer2.5 mPLUG-Owl3-7B Qwen2-VL-7B LongVA-7B Mantis-8B Idefics2-8b InternVL2-8B Llama-3-LongVILA-8B VILA1.5-13B InternVL2-26B VILA1.5-40B Qwen2-VL-72B LLaVA-OneVision-72B Llama-3.1-405B Qwen-max Mistral Large* Claude-3.5-Sonnet* Gemini-1.5-Pro* GPT-4o* 76.2 51.4 48.1 54.5 51.9 51.3 49.9 51.0 51.0 49.2 51.6 49.4 50.5 51.6 50.7 53.6 56.0 54.5 48.0 49.4 53.2 55.0 50.1 81.4 47.7 50.3 50.3 52.5 50.5 48.3 48.8 51.8 46.4 49.4 50.6 48.0 49.7 50.3 49.9 61.7 53.4 51.5 48.9 56.0 52.2 54. 90.0 49.8 47.9 55.1 55.8 53.4 49.6 48.0 51.5 45.5 45.6 49.1 44.0 50.4 50.0 50.6 50.7 49.2 47.6 45.1 60.2 49.9 48.4 67.2 50.8 47.5 55.8 48.2 52.3 51.4 49.8 52.8 45.4 49.1 48.9 47.2 48.4 50.0 50.6 57.0 57.8 48.3 49.4 58.9 52.9 52. 73.9 49.4 48.2 52.7 49.7 53.2 48.1 47.2 53.5 44.0 47.1 51.2 42.6 47.2 49.7 52.0 54.7 56.7 50.1 50.9 66.1 56.5 55.3 63.3 50.5 53.1 57.0 55.7 60.6 50.5 50.2 54.9 52.4 52.7 50.0 55.1 56.2 50.0 53.1 73.4 57.7 49.1 60.6 68.3 62.5 64. 79.45 49.4 50.7 51.2 54.9 59.5 48.0 44.8 52.7 49.4 54.7 49.8 45.8 53.3 49.8 59.8 71.5 67.2 47.4 61.3 68.2 59.0 64.7 1.7 43.5 50.0 42.1 52.2 41.9 42.1 49.0 43.2 43.1 54.4 50.0 48.5 53.6 50.7 52.5 68.6 58.8 41.1 50.8 60.1 58.6 61. 35 Table 18: Multiple Choice questions results of different models on the LOKI text modality. denotes the closed-source models. * Scientific Papers News Essay Wikipedia Speech Modern Literature Phiosophy Ancient Chinese MiniCPM-V-2.6 Phi-3.5-Vision LLaVA-OneVision-7B InternLM-XComposer2.5 mPLUG-Owl3-7B Qwen2-VL-7B LongVA-7B Mantis-8B Idefics2-8B InternVL2-8B Llama-3-LongVILA-8B VILA1.5-13b InternVL2-26B VILA1.5-40B Qwen2-VL-72B LLaVA-OneVision-72B Llama-3.1-405B Qwen-max Mistral Large* Claude-3.5-Sonnet* Gemini-1.5-Pro* GPT-4o* 48.6 45.8 51.5 44.0 51.8 49.6 50.1 58.9 50.1 53.9 46.4 50.3 52.6 49.9 67.2 65.1 56.8 48.3 55.7 83.9 51.3 62.2 42.6 39.9 48.2 42.8 49.2 46.3 49.0 54.4 40.6 52.8 43.5 40.8 54.9 52.5 79.2 79.9 61.7 48.3 75.1 88.3 56.7 62.9 47.8 41.0 48.6 33.9 45.1 42.6 48.1 36.3 31.5 38.9 40.1 39.9 43.2 39.4 53.1 50.6 74.7 47.4 55.4 76.4 33.5 49. 48.2 41.1 44.9 41.0 48.3 44.2 47.5 51.5 40.1 47.2 42.2 44.6 49.3 48.9 72.9 63.1 54.4 33.8 62.8 86.9 46.8 59.0 48.5 36.3 44.9 34.3 44.9 45.8 46.1 35.3 35.8 42.5 37.4 40.8 47.6 46.3 62.5 61.3 69.6 43.3 61.8 92.8 60.7 66.9 50.1 42.6 49.6 46.3 54.0 45.4 51.4 49.7 35.0 43.8 51.4 43.5 48.9 57.6 76.7 82.6 74.2 43.8 86.7 96.9 70.4 81. 56.3 44.3 51.1 40.8 62.1 50.7 49.3 59.6 33.6 50.7 46.7 48.1 56.3 58.2 76.1 78.3 86.5 48.3 86.1 97.5 71.5 79.3 50.0 47.5 48.3 41.9 38.6 46.1 50.3 34.7 20.8 40.0 49.2 44.2 48.9 50.8 69.7 76.9 83.1 48.1 69.4 92.2 77.2 75.6 Table 19: Performance Comparison of Models across Text Lengths and Languages. * denotes the closed-source models. Short text Medium text Long text English Chinese MiniCPM-V-2.6 Phi-3.5-Vision LLaVA-OneVision-7B InternLM-XComposer2.5 mPLUG-Owl3-7B Qwen2-VL-7B LongVA-7B Mantis-8B Idefics2-8B InternVL2-8B Llama-3-LongVILA-8B VILA1.5-13B InternVL2-26B VILA1.5-40B Qwen2-VL-72B LLaVA-OneVision-72B Qwen-max Llama-3.1-405B Mistral-Large* Claude-3.5-Sonnet* Gemini-1.5-Pro* GPT-4o* 49.1 63.2 45.5 46.4 48.8 43.8 37.7 54.7 46.9 45.6 47.1 49.4 34.9 50.5 40.4 53.5 35.8 49. 44.6 56.1 39.7 46.9 48.9 59.2 45.7 46.7 47.6 42.4 37.0 55.5 41.8 44.4 46.8 48.0 33.9 50.9 41.6 56.6 34.0 49.6 46.3 60.3 40.3 49.4 36 46.6 56.5 46.8 47.2 49.0 43.5 37.5 55.6 33.9 42.1 47.0 44.4 35.4 50.7 47.4 63.1 32.8 56.5 51.3 67.6 47.0 58. 49.7 48.6 53.1 48.2 56.3 47.6 49.5 53.7 46.7 49.7 47.7 47.0 51.0 49.8 60.2 63.8 48.3 63.4 60.5 73.5 58.4 60.0 48.9 45.5 50.1 49.0 48.9 48.4 48.0 48.2 41.2 48.6 48.3 46.0 50.7 50.5 56.9 64.0 45.9 61.3 55.5 68.3 54.3 59."
        },
        {
            "title": "E CASE STUDY",
            "content": "In this section, we delve into an analysis of GPT-4os performance on synthetic data detection tasks, with particular focus on the questions it answered incorrectly. This examination is crucial for understanding the models operational capabilities and limitations. The analysis not only identifies the models current shortcomings but also provides guidance for future design and training improvements. We randomly selected 50 erroneous instances from GPT-4os predictions across various modalities and difficulty levels for detailed inspection. These instances included multiple-choice, true/false, and open-ended responses, and even for the structured questions, GPT-4o was required to provide the reasoning behind its decisions. These instances were analyzed by specialized annotators who identified the fundamental causes of the erroneous predictions based on their expertise and annotations. E."
        },
        {
            "title": "IMAGE",
            "content": "Figure 13: Abnormal detail explanation example of category animals in image modality, with GPT4os answers 37 Figure 14: Abnormal detail explanation example of category object in image modality, with GPT4os answers 38 Figure 15: Multi-choice example of category document in image modality, with GPT-4os answers Figure 16: Judgement example of category document in image modality, with GPT-4os answers 40 Figure 17: Multi-choice example of category medical in image modality, with GPT-4os answers 41 Figure 18: Judgement example of category medical in image modality, with GPT-4os answers Figure 19: Abnormal selection example of category animals in image modality, with GPT-4os answers 43 Figure 20: Abnormal selection example of category object in image modality, with GPT-4os answers 44 Figure 21: Abnormal detail explanation example of category object in image modality, with GPT4os answers Figure 22: Abnormal detail explanation example of category person in image modality, with GPT4os answers 46 Figure 23: Abnormal selection example of category person in image modality, with GPT-4os answers 47 Figure 24: Multi-choice example of category satellite in image modality, with GPT-4os answers Figure 25: Judgement example of category satellite in image modality, with GPT-4os answers 49 Figure 26: Abnormal detail explanation example of category scenery in image modality, with GPT4os answers 50 Figure 27: Abnormal selection example of category scenery in image modality, with GPT-4os answers E.2 VIDEO Figure 28: Multi-choice example of category Abiotic in video modality, with GPT-4os answers 52 Figure 29: Multi-choice example of category Animals in video modality, with GPT-4os answers 53 Figure 30: Abnormal selection example of category Animals in video modality, with GPT-4os answers 54 Figure 31: Abnormal detail explanation example of category Human in video modality, with GPT4os answers 55 Figure 32: Judgement example of category Human in video modality, with GPT-4os answers 56 Figure 33: Abnormal detail explanation example of category Human in video modality, with GPT4os answers 57 Figure 34: Judgement example of category Human in video modality, with GPT-4os answers 58 Figure 35: Abnormal selection of category Scenery in video modality, with GPT-4os answers 59 E. 3D Figure 36: Abnormal detail explanation example generated by Gaussian Splatting based methods in 3D modality, with GPT-4os answers 60 Figure 37: Judgement example generated by Gaussian Splatting based methods in 3D modality, with GPT-4os answers 61 Figure 38: Multi-choice example generated by Gaussian Splatting based methods in 3D modality, with GPT-4os answers 62 Figure 39: Multi-choice example generated by Gaussian Splatting based methods in 3D modality, with GPT-4os answers 63 Figure 40: Abnormal detail explanation example generated by Nerf based in 3D modality, with GPT-4os answers 64 Figure 41: Abnormal detail explanation example in 3D modality, with GPT-4os answers 65 Figure 42: Judgement example in 3D modality, with GPT-4os answers 66 Figure 43: Judgement example in 3D modality, with GPT-4os answers 67 E.4 AUDIO Figure 44: Judgement example of category Audio in audio modality, with LTUs answers Figure 45: Multi-choice example of category Music in audio modality, with LTUs answers 68 Figure 46: Multi-choice example of category Singing in Audio modality, with Qwen-Audios answers Figure 47: Multi-choice example of category Speech in audio modality, with AnyGPTs answers E.5 TEXT Figure 48: Multi-choice example of category Scientific Papers Abstract in text modality, with GPT-4os answers Figure 49: Judgement example of category Scientific Papers Abstract in text modality, with GPT4os answers 70 Figure 50: Multi-choice example of category Essay in text modality, with GPT-4os answers Figure 51: Judgement example of category Essay in text modality, with GPT-4os answers 71 Figure 52: Multi-choice example of category News in text modality, with GPT-4os answers Figure 53: Judgement example of category News in text modality, with GPT-4os answers 72 Figure 54: Multi-choice example of category Modern Literature in text modality, with GPT-4os answers Figure 55: Judgement example of category Modern Literature in text modality, with GPT-4os answers 74 Figure 56: Judgement example of category Ancient Literature in text modality, with GPT-4os answers 75 Figure 57: Multi-choice example of category Ancient Literature in text modality, with GPT-4os answers Figure 58: Multi-choice example of category Philosophy in text modality, with GPT-4os answers 76 Figure 59: Judgement example of category Philosophy in text modality, with GPT-4os answers 77 Figure 60: Multi-choice example of category Speech in text modality, with GPT-4os answers Figure 61: Multi-choice example of category Speech in text modality, with GPT-4os answers Figure 62: Multi-choice example of category Wikipedia in text modality, with GPT-4os answers Figure 63: Judgement example of category Wikipedia in text modality, with GPT-4os answers"
        }
    ],
    "affiliations": [
        "SenseTime Research",
        "Shanghai AI Laboratory",
        "Sun Yat-sen University",
        "The Chinese University of Hong Kong",
        "The Chinese University of Hong Kong (Shenzhen)"
    ]
}