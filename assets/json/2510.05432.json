{
    "paper_title": "AInstein: Assessing the Feasibility of AI-Generated Approaches to Research Problems",
    "authors": [
        "Shambhavi Mishra",
        "Gaurav Sahu",
        "Marco Pedersoli",
        "Laurent Charlin",
        "Jose Dolz",
        "Christopher Pal"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) demonstrate impressive capabilities across a wide range of tasks, yet it remains unclear whether such success reflects genuine reasoning or sophisticated recall. We introduce AInstein, a framework for testing whether LLMs can generate valid solutions to AI research problems using only their pretrained parametric knowledge -- without domain-specific fine-tuning, retrieval augmentation, or other external aids. Our approach extracts distilled problem statements from high-quality ICLR 2025 submissions, then tasks specialized solver agents with proposing and refining technical solutions through iterative critique loops, mimicking the cycles of proposal, review, and revision central to scientific inquiry. We evaluate AInstein on 1,214 ICLR papers stratified by acceptance tier (Oral, Spotlight, Poster), using an LLM-as-a-judge paradigm guided by a structured rubric, complemented by targeted manual checks. Performance is assessed with three metrics: Success Rate (does the solution address the problem?), Rediscovery (does it align with human-proposed methods?), and Novelty (does it yield valid, original approaches?). Our results reveal that while LLMs can rediscover feasible solutions and occasionally propose creative alternatives, their problem-solving ability remains fragile and highly sensitive to framing. These findings provide the first large-scale evidence on the extent to which LLMs can act as autonomous scientific problem-solvers, highlighting both their latent potential and their current limitations."
        },
        {
            "title": "Start",
            "content": "AINSTEIN: ASSESSING THE FEASIBILITY OF AI-GENERATED APPROACHES TO RESEARCH PROBLEMS Shambhavi Mishra1,4,8 & Gaurav Sahu2,3,4 Marco Pedersoli1,2,8 Laurent Charlin2,3,5,6 Jose Dolz1,8 Christopher Pal2,4,5,6,7 1LIVIA, ETS Montreal 2Mila Quebec AI Institute 3HEC Montreal 4ServiceNow Research 5Canada CIFAR AI Chair 6Universite de Montreal 7Polytechnique Montreal 8International Laboratory on Learning Systems (ILLS) Equal Contribution 5 2 0 2 6 ] . [ 1 2 3 4 5 0 . 0 1 5 2 : r a"
        },
        {
            "title": "ABSTRACT",
            "content": "Large language models (LLMs) demonstrate impressive capabilities across wide range of tasks, yet it remains unclear whether such success reflects genuine reasoning or sophisticated recall. We introduce AInstein, framework for testing whether LLMs can generate valid solutions to AI research problems using only their pretrained parametric knowledgewithout domain-specific fine-tuning, retrieval augmentation, or other external aids. Our approach extracts distilled problem statements from high-quality ICLR 2025 submissions, then tasks specialized solver agents with proposing and refining technical solutions through iterative critique loops, mimicking the cycles of proposal, review, and revision central to scientific inquiry. We evaluate AInstein on 1,214 ICLR papers stratified by acceptance tier (Oral, Spotlight, Poster), using an LLM-as-a-judge paradigm guided by structured rubric, complemented by targeted manual checks. Performance is assessed with three metrics: Success Rate (does the solution address the problem?), Rediscovery (does it align with human-proposed methods?), and Novelty (does it yield valid, original approaches?). Our results reveal that while LLMs can rediscover feasible solutions and occasionally propose creative alternatives, their problem-solving ability remains fragile and highly sensitive to framing. These findings provide the first large-scale evidence on the extent to which LLMs can act as autonomous scientific problem-solvers, highlighting both their latent potential and their current limitations."
        },
        {
            "title": "INTRODUCTION",
            "content": "The history of science is shaped by moments of profound insight: Newtons recognition that falling apples and orbiting planets obey the same laws, Darwins theory of natural selection, Einsteins thought experiments on space and time. Each breakthrough required not only knowledge, but also the ability to (a) synthesize disparate observations into coherent theories and (b) traverse the conceptual path from problem to principle to solution. As artificial intelligence advances, we confront critical question: do large language models (LLMs) exhibit such reasoning, or are they sophisticated pattern matchers relying on memorized associations (Xu et al., 2025; Chollet, 2019; Zhang et al., 2025; Min et al., 2022)? This question is increasingly urgent as LLMs demonstrate strong performance in mathematics, programming, and even research workflows (Fang et al., 2024; Wang et al., 2024b; Zheng et al., 2025; Cui et al., 2025; Luo et al., 2025). Yet, much of this success may derive from associative recall rather than genuine conceptual reasoning. To probe this distinction, we ask the following question, Q: Can LLMs solve AI research problems using only their parametric knowledge? Unlike prior evaluations that often test factual recall, benchmark performance (Luo et al., 2025; Chollet, 2019), or domain-specific competence, our study isolates the ability to generate solutions to open-ended research challenges without external aids such as fine-tuning or retrieval augmentation (Xu et al., 2025; Zheng et al., 2025; Cui et al., 2025).. To that end, we introduce AINSTEIN, framework for empirically testing scientific problem-solving under these (Figure 1). The pipeline operates in two phases. In the Problem Extraction Phase, scientific abstracts are distilled into concise research challenges that preserve the core contribution while omitting direct references to the original solution. In the Solution Phase, solver agents generate and refine potential solutions through iterative critique loops, simulating the cycles of proposal, review, and revision characteristic of scientific inquiry. This design creates controlled testbed for distinguishing recall from reasoning. Figure 1: The AINSTEIN framework. An input scientific abstract (A) is first derived into generalized problem (P) by the Generalizer agent (G). The Solver agent (S) then attempts to derive technical solution (Z), using the problem statement P. Both phases employ an iterative refinement loop with internal (Mi) and external (Me) critique. Note: The transition follows the same iterative refinement mechanism, where the the Happy condition is as described in Section 3.2. We curate dataset of 1,214 high-quality ICLR 2025 papers, stratified by quality tier (Oral, Spotlight, Poster). Each paper serves as source of scientific abstract. For evaluation, we adopt an LLM-as-a-judge paradigm, where the LLM outputs are reviewed against structured rubric that produce both structured scores and holistic judgments. To ensure reliability, we also manually check the solutions proposed by the LLM. We report the three performance metrics for our task: Success Rate: does the proposed solution address the problem statement? Rediscovery: how often does an LLM independently converge on solutions that resemble those proposed by human researchers? Novelty: how often does the model generate approaches that are both valid and original (i.e., not proposed by humans)? Together, these metrics allow us to disentangle rote recall from genuine problemsolving and creativity. Overall, this work makes three contributions. First, we propose new evaluation paradigm that isolates scientific problem-solving ability by decoupling problem extraction from solution generation and removing external aids. Second, we introduce systematic metrics to measure success, rediscovery, and novelty, enabling finer-grained analysis of reasoning versus recall. Third, we provide the first large-scale mapping of LLM problem-solving across models and research domains, revealing both promising signs of scientific creativity and the fragility of reasoning under different framings. Together, these contributions advance our understanding of LLMs not only as tools for research, but as potential autonomous problem-solvers."
        },
        {
            "title": "2 RELATED WORK",
            "content": "The evaluation of scientific reasoning in language models has evolved along several complementary directions, each revealing different aspects of these systems capabilities and limitations. Evaluating Scientific Knowledge and Reasoning. Traditional benchmarks have focused on testing models ability to answer scientific questions and solve domain-specific problems. Recent work has highlighted the limitations of these approaches, particularly their conflation of memorization with understanding. Wang et al. (2024a) demonstrate that models often rely heavily on pattern matching and retrieval rather than genuine reasoning when solving mathematical problems. Similarly, the phenomenon of grokkingwhere models suddenly transition from memorization to generalizationsuggests that understanding and recall exist on spectrum rather than as binary states (Power et al., 2022). Our work extends this line of inquiry by explicitly controlling for memorization through conceptual abstraction. Probing Model Representations. Understanding how language models internally represent concepts has become critical area of research. Investigations into models latent knowledge have revealed that these systems often maintain multiple, sometimes contradictory, representations of the same concept (Longpre et al., 2021; Xie et al., 2023). The ability to steer model behavior through targeted interventions suggests that different representations can be selectively activated (Turner et al., 2023). Our framework leverages this insight, using varied problem framings to explore the full space of models conceptual representations. Emergence and Phase Transitions. The study of emergent capabilities in large language models has revealed that certain abilities appear suddenly as models scale, resembling phase transitions in physical systems (Wei et al., 2022). Schaeffer et al. (2023) provide nuanced analysis of these transitions, showing how measurement choices can create the appearance of emergence. Our concept of conceptual activation energy provides complementary perspective, focusing on the minimum prompt complexity required to trigger specific reasoning capabilities rather than model scale. 2 Scientific Discovery and Creativity. Recent work has begun exploring whether AI systems can contribute to scientific discovery beyond mere assistance. Romera-Paredes et al. (2024) demonstrate that specialized systems can discover novel mathematical theorems, while other work has shown success in materials science and drug discovery (Zhavoronkov et al., 2019; Merchant et al., 2023). However, these systems typically operate within narrow, well-defined domains with extensive domain-specific training. Our approach differs by testing generalpurpose language models ability to rediscover established concepts without domain-specific fine-tuning. Mechanistic Interpretability. The growing field of mechanistic interpretability seeks to understand how models perform specific tasks by analyzing their internal computations (Elhage et al., 2021). This work has revealed that models can implement surprisingly sophisticated algorithms, from modular arithmetic to optimization procedures Olsson et al. (2022). Our framework complements these mechanistic studies by providing behavioral evidence of models algorithmic capabilities through their ability to rediscover solutions. Prompt Engineering and In-Context Learning. The sensitivity of language models to prompt formulation has been extensively documented, with small changes in wording capable of drastically altering performance (Reynolds & McDonell, 2021). Work on in-context learning has shown that models can adapt to new tasks through careful example selection and prompt design (Brown et al., 2020; Min et al., 2022). We build on these insights, treating prompt design not as an engineering challenge but as scientific tool for probing models latent capabilities. Our work synthesizes insights from these diverse research threads while introducing novel evaluation paradigm. Rather than testing what models know or can retrieve, we examine whether they can recreate the process of discovery itself. This approach provides new lens for understanding the relationship between memorization, pattern matching, and genuine reasoning in artificial intelligence systems."
        },
        {
            "title": "3 METHODOLOGY",
            "content": "We present AINSTEIN, framework for evaluating whether large language models (LLMs) can act as autonomous scientific problem-solvers. The framework is designed to (1) extract concise research problems from scientific abstracts, and (2) test whether solver agents can propose technically valid solutions through iterative refinement. Our approach formalizes this process as structured pipeline with nested critique loops, which has been shown to improve LLMs problem-solving capability (Shinn et al., 2023; Madaan et al., 2023)."
        },
        {
            "title": "3.1 PROBLEM-TO-SOLUTION PIPELINE",
            "content": "The AINSTEIN pipeline consists of two distinct phases, separating problem formulation from solution generation to create controlled testbed for probing scientific reasoning. Phase 1: Problem Extraction. Given scientific abstract A, Generalizer agent (an LLM prompted with specific role) produces distilled problem statement P. The central goal is to create high-quality problem formulation that is both faithful to the original challenge and free of solution-specific artifacts. To this end, is explicitly instructed to maximize abstraction and fidelity while minimizing ambiguity and solution leakage (as defined in our evaluation criteria). This constraint is critical to prevent the agent from simply paraphrasing the solution from the abstract, thereby ensuring the subsequent solving task is non-trivial. Formally, the process is: = G(A; Mi, Me). Phase 2: Solution Generation. Solver agent (a separate LLM instance) receives only the problem statement and proposes technical solution Z. This phase evaluates the models ability to generate detailed and potentially novel methodology to address the problem, without access to the original abstract or solution. Formally: = S(P; Mi, Me). 3."
        },
        {
            "title": "ITERATIVE REFINEMENT VIA CRITIQUE LOOPS",
            "content": "Both the Generalizer and Solver operate under nested refinement mechanism that mimics scientific inquiry: iterative proposal, critique, and revision, shown in Algorithm 1. Each phase has: 1. Internal Critique Loop (Model Mi): The inner loop simulates fast, low-cost self-correction. An initial draft (problem statement or solution) is generated, then reviewed against task-specific criteria by an internal critic (also Mi). The critic provides rubric-based feedback and binary accept/reject signal. If rejected, the draft is revised. This process repeats for up to MaxInternalAttempts iterations (=20 in our experiments). Prompt in Appendix shows the generalizer prompt. 3 for = 1 to MaxInternalAttempts do Generate Pcandidate; self-critique(Pcandidate) if pass then break end if Algorithm 1 The AInstein Algorithm 1: Input: Abstract A, Internal model Mi, External model Me 2: Output: Problem Pf inal, Solution Zf inal 3: Pf inal, Pcandidate null 4: for = 1 to MaxExternalAttempts do 5: 6: 7: 8: 9: 10: 11: 12: 13: end for 14: if Pf inal = null then 15: 16: end if end for External critique(Pcandidate) if pass then Pf inal Pcandidate; break end if Zf inal SolveWithRefinement(Pf inal, Mi, Me) 2. External Critique Loop (Model Me): The outer loop provides higher-fidelity review from stronger external model Me. Once the internal loop converges, Me evaluates the candidate artifact. If the external judgment is negative, the feedback from both critics is incorporated into new attempt, up to MaxExternalAttempts (=20 in our experiments). This ensures that only high-quality outputs are accepted. Prompt in Appendix shows the solver prompt. This dual-loop structure operationalizes the dynamics of scientific peer review: rapid local iteration combined with stricter external scrutiny."
        },
        {
            "title": "4.1 DATASETS & MODELS",
            "content": "Our experimental dataset consists of 1,214 papers curated from the ICLR 2025 conference submissions, drawing inspiration from the ICLR Dataset (Gonzalez-Marquez & Kobak, 2024). This corpus provides large, diverse, and contemporary set of high-quality research problems. Our curated set is intentionally stratified by the papers final acceptance tiers: Oral, Spotlight, and Poster. Table 1 compares the distribution of our final dataset against the full ICLR 2025 corpus, highlighting how our curation process oversamples higher-quality papers to analyze how model performance correlates with the perceived quality and impact of the research. To prevent data leakage and ensure our evaluation tests reasoning rather than retrieval, all models used in our experiments have knowledge cutoffs that predate the ICLR 2025 submission deadline. We experiment with three primary model families selected to represent different scales of capability. The large-scale models include GPT-OSS-120B and Qwen-235B, representing the stateof-the-art in reasoning. The mid-scale model is Mistral-24B, powerful and widely used alternative. Table 1: Comparison of paper distribution between the full ICLR 2025 dataset and our curated experimental set (N=1,214). Full ICLR Dataset Our Curated Set Decision Tier Count % Count % Oral Spotlight Poster Rejected Withdrawn 213 379 3,111 5,014 2,946 1.8 3.2 26.7 43.0 25. 213 379 622 17.5 31.2 51.2 It is important to distinguish between the task roles (Generalizer and Solver agents) and the refinement roles (Internal Model Mi and External Model Me). We systematically test all pairings to analyze the interplay between model capabilities for generation and critique, avoiding biases that might arise from using models of the same family or scale in both roles. 11,663 1,214 Total"
        },
        {
            "title": "4.2 EVALUATION",
            "content": "We use GPT-OSS-120B as an LLM-as-a-judge, which is known to correlate well with human evaluation (Liu et al., 2023; Sottana et al., 2023). For each generated problem statement (P), we calculate deficit score (d) based on four criteria: Fidelity to the original challenge, Abstraction from implementation details, lack of Ambiguity, and avoidance of solution Leakage. lower deficit score indicates higher-quality problem formulation. For each solution (Z), the judge provides 1-5 score based on its technical Feasibility, Completeness, and Novelty. This score informs our three primary metrics: (1) Success Rate: Is the solution both feasible and complete? (2) 4 Rediscovery: Does successful solution match the original human solution? (3) Novel & Valid: Is successful solution valid but different from the original? Quantitative Validation Metrics. The core metrics are complemented by quantitative text analyses. For semantic coherence, we generate 4096-dimensional embeddings using Qwen3-Embedding-8B. To optimize for the target relationship (e.g., problem-solution alignment vs. end-to-end rediscovery), we prepend each text pair with task-specific instruction before embedding. From these embeddings, we compute Cosine Similarity as our primary metric, along with Euclidean distance. We also assess textual complexity using standard readability scores (e.g., Flesch-Kincaid Grade Level). Competitive Ranking via Human-Verified ELO. To provide robust aggregate ranking, we conduct headto-head tournament where human evaluators provide pairwise preferences between solutions. From these preferences, we compute an ELO rating for each agent configuration."
        },
        {
            "title": "5 RESULTS",
            "content": "Our experimental results reveal clear hierarchy of scientific reasoning capabilities among model configurations and offer nuanced insights into the nature of machine-driven discovery. We structure our analysis around three key findings: (1) the dominant role of the internal model (Mi) in determining success, (2) the distinction between precise rediscovery and creative problem-solving, and (3) the validation of these findings through competitive ranking, quantitative metrics and qualitative study. Problem Generalization Analysis The validity of our study hinges on the quality of the abstracted problems given to the Solver agents. To analyze this, we first evaluate the performance of our three models in the Generalizer role. The correlation matrix in Figure 2 empirically validates our formulation, confirming that our deficit score is strongly correlated with these key quality dimensions. shown in Table both Qwen-235B and As 2, GPT-OSS-120B produce problem statements with very low deficit scores (d 2.5), while Mistral-24Bs are of slightly lower quality (d 3.5). Statistical significance testing between GPT and OSS confirms that the two models perform comparably (see Appendix B). Thus, we use the outputs from all three Generalizers as inputs to the solution phase for complete study. This ensures our Solver results are robust and not overfitted to single problem-framing style, effectively testing each Solvers adaptability. Figure 2: Correlation matrix for Generalizer quality metrics. The deficit score (d) is strongly correlated with Information Loss and Ambiguity. Table 2: Generalizer metrics by model configuration. The deficit score (d ) quantifies quality degradation as the average penalty across four dimensions (Fidelity, Info Loss, Ambiguity, and Leakage). The large-scale models demonstrate superior performance."
        },
        {
            "title": "Model Configuration",
            "content": "Fidelity Info Loss Ambiguity Leakage"
        },
        {
            "title": "Internal Model",
            "content": "GPT-OSS-120B Mistral-24B Qwen-235B GPT-OSS-120B 8.80 0.77 8.39 0.85 Mistral-24B 8.86 0.49 Qwen-235B GPT-OSS-120B 8.82 0.66 Mistral-24B 8.41 0.82 8.88 0.47 Qwen-235B GPT-OSS-120B 8.79 0.80 8.41 0.85 Mistral-24B Qwen-235B 8.87 0. 3.72 1.24 5.24 1.28 4.24 1.92 3.70 1.16 5.16 1.30 4.19 1.91 3.71 1.24 5.18 1.34 4.23 1.91 3.08 1.11 2.54 0.76 2.16 1.55 5.41 0.85 1.96 1.45 3.56 0.70 3.03 1.40 1.68 1.31 2.52 0.94 3.05 1.03 2.19 1.57 2.53 0.70 5.39 0.80 3.51 0.66 1.89 1.39 3.07 1.42 1.65 1.23 2.51 0.93 3.06 1.12 2.17 1.56 2.54 0.76 5.39 0.87 1.97 1.54 3.53 0.74 2.50 0.89 1.63 1.22 3.02 1. The Internal Models Capability is Paramount. Across all experiments, the single most predictive factor of success is the strength of the internal model, Mi. The results in Table 3 show clear performance stratification: configurations using GPT-OSS-120B as the internal agent consistently and substantially outperform all others. 5 For instance, when solving problems from the GPT-OSS-120B generalizer, the GPT-OSS-120B internal agent achieves Success Rate of 74.05% under strict evaluation, whereas the next-best model, Qwen-235B, reaches only 43.82%. Rediscovery vs. Creative Problem-Solving. Our framework reveals critical distinction between reproducing an existing solution and discovering novel, valid one. comparison between relaxed (τ = 4) and strict (τ = 5) evaluation thresholds, presented in Table 3, is particularly telling. Under the relaxed threshold, the top GPT-OSS-120B agent achieves high Rediscovery rates (75-84%), suggesting it often generates solutions conceptually close to the original - successful near misses. However, when tightening the criterion to demand functional equivalence (τ = 5), these scores plummet to 15-20%. The stark drop reveals that perfect rediscovery is exceptionally rare. In contrast, the Novel & Valid metric for the same agent remains high and remarkably stable across both thresholds. This indicates that when models do not perfectly rediscover the solution, they often find alternative, sound scientific approaches. Table 3: LLM-as-a-judge success rates (%) comparing lenient (τ = 4) and strict (τ = 5) evaluation thresholds. Bold indicates the best-performing internal model for each problem source and external critic combination. Model Configuration Rediscovery SR Solver Novel & Valid External (Me) Internal (Mi) τ = τ = 5 τ = 4 τ = 5 τ = 4 τ = 5 Problem Source: GPT-OSS-120B GPT-OSS-120B Mistral-24B Qwen-235B GPT-OSS-120B Mistral-24B Qwen-235B GPT-OSS-120B Mistral-24B Qwen-235B GPT-OSS-120B Mistral-24B Qwen-235B Problem Source: Mistral-24B GPT-OSS-120B Mistral-24B Qwen-235B GPT-OSS-120B Mistral-24B Qwen-235B GPT-OSS-120B Mistral-24B Qwen-235B GPT-OSS-120B Mistral-24B Qwen-235B Problem Source: Qwen-235B GPT-OSS-120B Mistral-24B Qwen-235B GPT-OSS-120B Mistral-24B Qwen-235B GPT-OSS-120B Mistral-24B Qwen-235B GPT-OSS-120B Mistral-24B Qwen-235B 83.77 65.73 69.03 83.94 68.37 72.49 80.89 65.98 70.51 76.44 54.12 61. 76.19 55.52 64.09 74.79 59.14 64.00 77.68 58.73 64.58 76.77 58.57 66.64 76.85 60.05 65.82 19.11 6.43 7. 17.79 6.75 7.17 17.38 7.08 7.66 16.56 7.58 8.40 16.97 8.15 7.91 15.57 8.73 9.14 14.91 5.11 6. 14.74 4.37 6.84 14.09 6.34 6.75 92.34 75.45 80.48 93.41 74.46 85.34 88.63 76.52 85.34 92.83 80.97 84. 93.33 81.80 87.73 89.21 83.11 87.48 90.86 78.01 83.53 91.68 79.57 87.56 91.43 80.89 87.64 74.05 34.60 43. 71.91 31.22 39.21 70.35 34.84 42.34 78.01 42.42 50.16 79.41 40.86 49.09 76.28 43.57 49.84 77.51 41.85 50. 77.10 38.88 49.75 77.18 41.02 50.91 74.46 70.10 73.56 76.44 68.86 79.08 71.99 70.43 79.00 76.94 74.30 76. 77.10 74.88 80.56 74.55 75.54 79.24 76.52 73.81 77.92 78.17 76.19 81.30 78.17 75.86 81.38 59.39 31.80 40. 58.98 29.00 36.74 57.74 31.63 39.87 64.58 38.30 45.14 66.14 37.07 44.89 63.51 39.04 44.73 65.24 39.21 46. 66.56 36.49 46.13 66.89 37.97 47.20 Performance Across Paper Tiers Our stratification of the dataset by paper quality (Oral, Spotlight, Poster) allows us to investigate whether LLM problem-solving ability correlates with the perceived difficulty of the research. As illustrated in Figure 3, the performance hierarchy across internal models remains remarkably consistent across all three tiers. The top-performing GPT-OSS-120B agent shows stable Success Rates for Oral (69.0%), Spotlight (77.8%), and Poster (72.5%) papers. This counter-intuitive result suggests that the models ability to generate valid solution is not significantly hindered by the novelty or impact of the original paper. Robustness to Evaluator Choice. critical concern in our study is the potential for the LLM judge to introduce bias. To verify that our findings are not an artifact of specific evaluator, we replaced our judge (GPT-OSS-120B) with Qwen3-235B and replicated the LLM-as-a-judge scores. The results, presented in Table 4, confirm that our central conclusions are robust. We observe the same sharp decline in Rediscovery rates when moving from lenient (τ = 4) to strict (τ = 5) evaluation threshold, regardless of the judge as well as the performance from GPT-OSS-120B continues to outperform the others. 6 Figure 3: Performance comparison of internal models across ICLR paper tiers (Oral, Spotlight, Poster), averaging across GPT-OSS-120B as the external model. The results are shown for the strict threshold (τ = 5). The performance hierarchy is stable, and the top model (GPT-OSS-120B) is not significantly impacted by the papers prestige. Table 4: LLM-as-a-judge success rates (%) using Qwen3-235B as the evaluator, comparing lenient (τ = 4) and strict (τ = 5) thresholds. This table validates that the findings from the primary evaluator  (Table 3)  are robust. Bold indicates the best-performing internal model for each external critic. Model Configuration Rediscovery SR Solver Novel & Valid External (Me) Internal (Mi) τ = 4 τ = 5 τ = 4 τ = 5 τ = τ = 5 Problem Source: GPT-OSS-120B GPT-OSS-120B Mistral-24B Qwen-235B GPT-OSS-120B Mistral-24B Qwen-235B GPT-OSS-120B Mistral-24B Qwen-235B GPT-OSS-120B Mistral-24B Qwen-235B Problem Source: Mistral-24B GPT-OSS-120B Mistral-24B Qwen-235B GPT-OSS-120B Mistral-24B Qwen-235B GPT-OSS-120B Mistral-24B Qwen-235B GPT-OSS-120B Mistral-24B Qwen-235B Problem Source: Qwen-235B GPT-OSS-120B Mistral-24B Qwen-235B GPT-OSS-120B Mistral-24B Qwen-235B GPT-OSS-120B Mistral-24B Qwen-235B GPT-OSS-120B Mistral-24B Qwen-235B 81.95 65.65 72.80 81.45 67.74 71. 82.87 66.50 70.37 75.40 56.48 64.04 74.09 57.02 63.05 76.19 60.35 64.01 78.87 61.02 67.21 77.72 59.83 65. 78.09 61.88 65.61 17.88 6.41 7.80 17.02 6.78 7.73 17.26 6.72 7.73 15.88 7.10 8.63 15.27 7.93 7. 15.24 7.74 8.08 15.65 5.86 7.11 14.85 5.00 6.85 14.58 6.90 6.99 95.76 83.46 92.83 95.78 83.00 92. 95.83 84.41 92.77 96.52 87.21 94.35 96.03 87.45 93.55 96.19 89.40 93.34 96.19 85.95 94.18 96.08 86.57 93. 96.68 87.44 93.81 84.12 50.48 66.98 81.90 49.13 62.81 83.43 51.37 65.13 86.97 55.98 70.30 86.51 56.81 67. 87.10 58.65 68.39 87.61 56.55 70.41 85.99 54.71 67.97 87.08 58.08 69.95 78.54 55.56 68.19 78.10 57.44 65. 79.83 56.92 65.17 72.72 50.25 60.84 71.23 50.70 59.01 73.55 54.51 59.98 76.08 53.37 63.31 74.68 52.73 61. 75.92 55.06 61.87 14.80 3.47 5.11 13.75 3.31 4.89 14.13 3.52 4.40 13.87 4.17 6.22 13.00 4.67 5. 13.33 4.64 5.58 13.76 3.30 4.89 11.90 3.18 4.68 12.11 4.21 4.88 Human-Verified Competitive Ranking. To move beyond automated metrics and validate our findings with human judgment, head-to-head tournament was conducted where the authors of this study served as human evaluators. To mitigate potential bias, solutions from competing agent configurations were presented in randomized and anonymized fashion. From these pairwise preferences, we computed an ELO rating for each configuration  (Table 5)  . This human-verified ranking establishes definitive hierarchy, with the GPT-OSS-120B self-play configuration emerging as the clear top performer (ELO 1119), second only to the ground-truth human abstracts. This result robustly confirms the primacy of the internal models strength. We present detailed case studies in our Appendix C. Quantitative Text Analysis. We further validate our findings with quantitative text analysis  (Table 6)  . Semantic coherence metrics show that solutions generated by the top-performing internal model, GPT-OSS-120B, con7 Table 5: ELO ratings derived from head-to-head tournament where human evaluators provided pairwise preferences between solutions. Agent Human Abstracts GPT-OSS-120B + GPT-OSS-120B Mistral-24B + GPT-OSS-120B Mistral-24B + Mistral-24B GPT-OSS-120B + Mistral-24B ELO Rating Wins Losses Win Rate (%) 1187 1119 939 927 828 17 14 5 5 1 1 5 8 12 94.4 63.6 25.0 25.0 5.0 Case Study: Human-Verified ELO Evaluation"
        },
        {
            "title": "Query ID",
            "content": "4HRRcqE9SU"
        },
        {
            "title": "Problem Statement",
            "content": "Agent Mistral-24B Agent GPT-OSS-120B"
        },
        {
            "title": "Winner",
            "content": "How can neural implicit reconstruction via volume rendering be designed to accurately recover both fine geometric details (including thin structures) and smooth surfaces in weakly textured regions, while avoiding the bias introduced by uniformly applied geometric priors? The solution must enable adaptive utilization of prior information based on local surface characteristics and support unbiased sampling strategies that improve reconstruction quality across heterogeneous scene areas. We introduce Locally Adaptive Prior Modulation (LAPM) mechanism that dynamically adjusts the influence of geometric priors based on local surface characteristics. LAPM employs learnable prior modulation network that takes features from the neural implicit field as input and outputs prior weights... To complement LAPM, we devise Heterogeneity-Aware Sampling (HAS) strategy... We introduce Dual-Branch Implicit Reconstruction (DBIR) framework that learns smooth signed-distance-function (SDF) backbone and high-frequency residual, and combines them via learned spatial gating field... During volume rendering we employ differentiable Uncertainty-Guided Sampling (UGS) module that predicts per-ray variance map... enabling accurate recovery of thin structures and smooth surfaces without globally imposed geometric prior. Agent B. The human evaluator noted that Agent Bs proposal was significantly more detailed, technically sophisticated, and addressed the problems nuances more comprehensively by proposing specific mechanisms like gating field and uncertainty-guided sampling. Case Study: Human-Verified ELO Evaluation"
        },
        {
            "title": "Query ID",
            "content": "l6QnSQizmN"
        },
        {
            "title": "Problem Statement",
            "content": "How can an online RL agent learn in non-stationary environments and prevent catastrophic forgetting, given only continuous stream of experience without task labels? Agent Mistral-24B Agent GPT-OSS-120B"
        },
        {
            "title": "Winner",
            "content": "We propose Context-Aware Meta-Reinforcement Learning with Dual Memory (CAM-RL-DM) architecture, which integrates meta-learning component with dual episodic and semantic memory modules. The meta-learner rapidly adapts to new contexts by learning context-encoding function that parameterizes the policy network. We introduce Contextual Continual Actor-Critic (CCAC), an online RL architecture that (i) encodes the recent trajectory into low-dimensional context vector, (ii) routes state-action decisions through mixture-of-experts policy where gating weights are computed by softmax over the context, and (iii) augments each expert with lightweight generative replay model. Agent B. The evaluator noted that Agent Bs proposal of Mixture-of-Experts policy with generative replay was more concrete and powerful mechanism for preventing catastrophic forgetting than Agent As more general dual-memory system. sistently achieve high cosine similarity scores ( 0.87) with the problems they address. This indicates strong conceptual alignment between the generated solutions and the research challenges. Furthermore, readability scores reveal clear distinction in textual complexity. The large-scale models, GPT-OSS-120B and Qwen-235B, produce solutions with significantly higher Flesch-Kincaid grade level ( 23 26) compared to the mid-scale 8 Mistral-24B ( 22). This suggests that the more capable models generate more technically dense and linguistically sophisticated proposals, which aligns with their superior performance on our core reasoning metrics. Table 6: Semantic distance and readability metrics by model configuration. Metrics are mean std."
        },
        {
            "title": "External Model",
            "content": "Internal Model Cosine Sim. Euclidean Flesch Ease Flesch Grade GPT-OSS-120B Mistral-24B Qwen-235B GPT-OSS-120B Mistral-24B Qwen-235B GPT-OSS-120B Mistral-24B Qwen-235B GPT-OSS-120B Mistral-24B Qwen-235B 0.868 0.035 0.852 0.054 0.863 0.041 0.873 0.035 0.867 0.05 0.869 0.04 0.874 0.035 0.862 0.052 0.868 0. 0.509 0.067 0.536 0.098 0.518 0.077 0.499 0.069 0.507 0.094 0.507 0.077 0.498 0.068 0.515 0.097 0.508 0.077 -11.04 18.28 -2.81 17.63 -24.21 18.93 -10.25 17.91 -4.26 17.65 -25.42 20.31 -10.76 18.93 -3.26 17.9 -24.7 20. 22.74 4.4 21.52 4.03 25.56 4.17 22.72 4.5 22.07 4.22 25.86 4.98 22.81 4.7 21.8 4.15 25.7 4.84 Qualitative Text Analysis. To understand the conceptual landscape of the generated solutions, we performed qualitative analysis by categorizing the entire solution space. We first grouped all solutions into semantically coherent clusters via KMeans++ on their Qwen-8B embeddings. Each cluster was then assigned an interpretable label using its most characteristic TF-IDF keywords. (a) Keyword frequency for eight prominent clusters. (b) UMAP visualization of solution embeddings. Figure 4: Visual analysis of the 11 identified research clusters. (a) Top keywords provide thematic summary (b) The UMAP projection illustrates the distinct grouping of solutions, with each color for several clusters. corresponding to unique thematic cluster. This process reveals rich taxonomy of 11 distinct solution archetypes, summarized in Table 8 and visualized in Figure 4. The analysis demonstrates that the agents do not merely produce generic outputs but explore diverse range of modern research paradigms. These archetypes span broad strategies like Reinforcement Learning (C0), foundational model architectures and Transformers (C1), and highly specialized domains such as Molecular Graph Modeling (C2) and 3D Scene Representation (C8). We detail representative solutions for these clusters in our Appendix D. For instance, solutions in the Transformer Architectures (C1) cluster often propose specific architectural innovations, while those in the LLM Reasoning (C4) cluster focus on meta-level strategies: Cluster 1: Transformer Architecture & Attention (N=1106) Keywords: attention, model, token Representative Solution: We introduce the Meta-Optimized Sparse Mixture-of-Experts Transformer (MOS-MoE), in which lightweight hypernetwork predicts, for each token and layer, sparse subset (e.g., 2-4) of expert feed-forward networks. Cluster 4: Large Language Models & NLP (N=1647) Keywords: model, llm, language Representative Solution: We introduce hierarchical self-reward framework where population of heterogeneous critic LLMs (parameter-perturbed copies) generate pairwise preference judgments for candidate completions, and main LLM is fine-tuned on this preference data."
        },
        {
            "title": "6 CONCLUSION",
            "content": "In this work, we investigated whether large language models can function as autonomous scientific problemsolvers, using only their parametric knowledge. To this end, we introduced AINSTEIN, novel framework that extracts core research challenges from scientific abstracts and tasks LLM agents with generating and refining technically sound solutions through iterative critique. Our evaluation yielded reveals two key findings: solutions quality is overwhelmingly determined by the core reasoning models capability, and models excel at generating novel, scientifically valid alternatives rather than perfectly rediscovering human solutions. These results provide compelling evidence that modern LLMs possess latent scientific reasoning abilities that go beyond sophisticated pattern matching. Our work is primarily focused on the AI domain, and the LLM-as-a-judge paradigm carries inherent biases; future work should therefore extend this framework to other scientific fields like biology and physics, explore more sophisticated methods for problem extraction from full-text articles, and investigate how different agent architectures could further unlock these nascent problem-solving capabilities."
        },
        {
            "title": "REFERENCES",
            "content": "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. Francois Chollet. On the measure of intelligence, 2019. URL https://arxiv.org/abs/1911.01547. Hao Cui, Zahra Shamsi, Gowoon Cheon, Xuejian Ma, Shutong Li, Maria Tikhanovskaya, Peter Norgaard, Nayantara Mudur, Martyna Plomecka, Paul Raccuglia, Yasaman Bahri, Victor V. Albert, Pranesh Srinivasan, Haining Pan, Philippe Faist, Brian Rohr, Ekin Dogus Cubuk, Muratahan Aykol, Amil Merchant, Michael J. Statt, Dan Morris, Drew Purves, Elise Kleeman, Ruth Alcantara, Matthew Abraham, Muqthar Mohammad, Ean Phing VanLee, Chenfei Jiang, Elizabeth Dorfman, Eun-Ah Kim, Michael Brenner, Viren Jain, Sameera Ponda, and Subhashini Venugopalan. Curie: Evaluating llms on multitask scientific long context understanding and reasoning, 2025. URL https://arxiv.org/abs/2503.13517. Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, et al. mathematical framework for transformer circuits. Transformer Circuits Thread, 1(1):12, 2021. Meng Fang, Xiangpeng Wan, Fei Lu, Fei Xing, and Kai Zou. Mathodyssey: Benchmarking mathematical problem-solving skills in large language models using odyssey math data, 2024. URL https://arxiv. org/abs/2406.18321. Rita Gonzalez-Marquez and Dmitry Kobak. Learning representations of learning representations. In Data-centric Machine Learning Research (DMLR) workshop at ICLR 2024, 2024. Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. G-eval: Nlg evaluation using gpt-4 with better human alignment. arXiv preprint arXiv:2303.16634, 2023. Shayne Longpre, Kartik Perisetla, Anthony Chen, Nikhil Ramesh, Chris DuBois, and Sameer Singh. Entity-based knowledge conflicts in question answering. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 70527063, 2021. Hanjun Luo, Haoyu Huang, Ziye Deng, Xinfeng Li, Hewei Wang, Yingbin Jin, Yang Liu, Wenyuan Xu, and Zuozhu Liu. Bigbench: unified benchmark for evaluating multi-dimensional social biases in text-to-image models, 2025. URL https://arxiv.org/abs/2407.15240. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement with self-feedback. Advances in Neural Information Processing Systems, 36:4653446594, 2023. Amil Merchant, Simon Batzner, Samuel Schoenholz, Muratahan Aykol, Gowoon Cheon, and Ekin Dogus Cubuk. Scaling deep learning for materials discovery. Nature, 624(7990):8085, 2023. Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. Rethinking the role of demonstrations: What makes in-context learning work? In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (eds.), Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 1104811064, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main.759. URL https://aclanthology.org/ 2022.emnlp-main.759/. 10 Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, In-context learning and induction heads. arXiv preprint Amanda Askell, Yuntao Bai, Anna Chen, et al. arXiv:2209.11895, 2022. Alethea Power, Yuri Burda, Harri Edwards, Igor Babuschkin, and Vedant Misra. Grokking: Generalization beyond overfitting on small algorithmic datasets. arXiv preprint arXiv:2201.02177, 2022. Laria Reynolds and Kyle McDonell. Prompt programming for large language models: Beyond the few-shot paradigm. In Extended abstracts of the 2021 CHI conference on human factors in computing systems, pp. 17, 2021. Bernardino Romera-Paredes, Mohammadamin Barekatain, Alexander Novikov, Matej Balog, Pawan Kumar, Emilien Dupont, Francisco JR Ruiz, Jordan Ellenberg, Pengming Wang, Omar Fawzi, et al. Mathematical discoveries from program search with large language models. Nature, 625(7995):468475, 2024. Rylan Schaeffer, Brando Miranda, and Sanmi Koyejo. Are emergent abilities of large language models mirage? Advances in neural information processing systems, 36:5556555581, 2023. Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning. Advances in Neural Information Processing Systems, 36:86348652, 2023. Andrea Sottana, Bin Liang, Kai Zou, and Zheng Yuan. Evaluation metrics in the era of gpt-4: Reliably evaluating large language models on sequence to sequence tasks. arXiv preprint arXiv:2310.13800, 2023. Alex Turner, Monte MacDiarmid, David Udell, Lisa Thiergart, and Ulisse Mini. Steering gpt-2-xl by adding an activation vector. In AI Alignment Forum, 2023. Qingyun Wang, Doug Downey, Heng Ji, and Tom Hope. Scimon: Scientific inspiration machines optimized for novelty. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 279299, 2024a. Xiaoxuan Wang, Ziniu Hu, Pan Lu, Yanqiao Zhu, Jieyu Zhang, Satyen Subramaniam, Arjun R. Loomba, Shichang Zhang, Yizhou Sun, and Wei Wang. Scibench: Evaluating college-level scientific problem-solving abilities of large language models, 2024b. URL https://arxiv.org/abs/2307.10635. Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models. arXiv preprint arXiv:2206.07682, 2022. Jian Xie, Kai Zhang, Jiangjie Chen, Renze Lou, and Yu Su. Adaptive chameleon or stubborn sloth: Revealing the behavior of large language models in knowledge conflicts. In The Twelfth International Conference on Learning Representations, 2023. Fengli Xu, Qianyue Hao, Zefang Zong, Jingwei Wang, Yunke Zhang, Jingyi Wang, Xiaochong Lan, Jiahui Gong, Tianjian Ouyang, Fanjin Meng, Chenyang Shao, Yuwei Yan, Qinglong Yang, Yiwen Song, Sijian Ren, Xinyuan Hu, Yu Li, Jie Feng, Chen Gao, and Yong Li. Towards large reasoning models: survey of reinforced reasoning with large language models, 2025. URL https://arxiv.org/abs/2501.09686. Yanbo Zhang, Sumeer A. Khan, Adnan Mahmud, Huck Yang, Alexander Lavin, Michael Levin, Jeremy Frey, Jared Dunnmon, James Evans, Alan Bundy, Saso Dzeroski, Jesper Tegner, and Hector Zenil. Advancing the scientific method with large language models: From hypothesis to discovery, 2025. URL https://arxiv. org/abs/2505.16477. Alex Zhavoronkov, Yan Ivanenkov, Alex Aliper, Mark Veselov, Vladimir Aladinskiy, Anastasiya Aladinskaya, Victor Terentiev, Daniil Polykovskiy, Maksim Kuznetsov, Arip Asadulaev, et al. Deep learning enables rapid identification of potent ddr1 kinase inhibitors. Nature biotechnology, 37(9):10381040, 2019. Da Zheng, Lun Du, Junwei Su, Yuchen Tian, Yuqi Zhu, Jintian Zhang, Lanning Wei, Ningyu Zhang, and Huajun Chen. Knowledge augmented complex problem solving with large language models: survey, 2025. URL https://arxiv.org/abs/2505.03418."
        },
        {
            "title": "A LLM USAGE",
            "content": "We have used LLMs to polish the text of the paper and also to conduct literature review. Specifically, we used chatGPTs deep research feature to retrieve some relevant papers."
        },
        {
            "title": "B STATISTICAL SIGNIFICANCE TESTING",
            "content": "To rigorously compare the performance between our primary models, GPT-OSS-120B and Mistral-24B, we performed two-sample t-tests and non-parametric Mann-Whitney tests. We applied Bonferroni correction to account for multiple comparisons, resulting in corrected significance threshold of α 0.0036. The results are summarized in Table 7. While we find statistically significant differences in readability metrics, for the core Generalizer metrics, we find no significant differences. We conclude that while GPT-OSS-120B may have slight, statistically detectable edge, the two models perform at broadly comparable level in our framework. Thus, motivating our choice of considering problem statements from multiple sources. Table 7: Significance testing for key metrics comparing Mistral-24B (M) and GPT-OSS-120B (G). An asterisk (*) denotes statistical significance at the Bonferroni-corrected level (α 0.0036). Despite some statistical significance, all effect sizes are negligible."
        },
        {
            "title": "Metric",
            "content": "Mean (M) Mean (G) (t-test) (MW-U) Cohens d"
        },
        {
            "title": "Semantic Fidelity\nInformation Loss\nAmbiguity\nSolution Leakage",
            "content": "8.610 4.430 4.220 2.040 8.600 4.480 4.250 2."
        },
        {
            "title": "Readability",
            "content": "Flesch-Kincaid Grade* 22.270 21.970 0.468 0.254 0.601 0.602 0.002 0.898 0.288 0.659 0. 0.001 0.021 0.033 0.015 0.015 0."
        },
        {
            "title": "C ADDITIONAL HUMAN EVALUATION EXAMPLES",
            "content": "This appendix provides selection of head-to-head comparisons from our human-verified ELO tournament. For each example, we present the problem statement, the responses from the two competing agents, and the final verdict from the human evaluator. Example 1: Video Inverse Problems (ID: TRWxFUzK9K) Problem Statement: How can we formulate and solve video inverse problems that involve spatiotemporal degradations using only pre-trained image diffusion models, while ensuring consistent reconstruction across the temporal dimension without training dedicated video diffusion models? Agent Response (Mistral-24B Self-Play): We propose method called Temporal Consistencyaware Recurrent Image Diffusion (TC-RID) that adapts pre-trained image diffusion models for video inverse problems. TC-RID introduces recurrent neural network (RNN) that processes each video frame sequentially, using the hidden state to maintain temporal information... Agent Response (GPT-OSS-120B Self-Play): We formulate video inverse problems as sequence of frame-wise diffusion processes in which frozen pre-trained image diffusion model acts as the denoiser, but each denoising step receives an additional cross-frame guidance tensor obtained by warping the latent of the previous frame with fast optical-flow estimator... Winner: B. Agent Bs solution was judged to be more technically specific and innovative by proposing the use of optical flow and warped latents as temporal prior. Example 2: Dynamic Mixture of Experts (ID: T26f9z2rEe) Problem Statement: How can Mixture of Experts architecture automatically determine the appropriate number of experts to activate for each input token and dynamically adjust the total number of experts during training, thereby eliminating the need for extensive hyper-parameter search? Agent Response (Human Abstract): The Sparse Mixture of Experts (SMoE) has been widely employed... However, the performance of SMoE heavily depends on the choice of hyper-parameters... As remedy, we introduce the Dynamic Mixture of Experts (DynMoE) technique. DynMoE incorporates (1) novel gating method that enables each token to automatically determine the number of experts to activate... Agent Response (GPT-OSS-120B Self-Play): We introduce Dynamic Adaptive Gating with an Elastic Expert Pool (DAGE-E): global gating network outputs full softmax over large superset of experts, while learned token-wise budget scalar determines differentiable top-k cutoff... Concurrently, an Elastic Expert Pool manager monitors per-expert utilization statistics... Winner: (Human). While the LLMs proposal (DAGE-E) was highly detailed and technically plausible, the human abstract was judged to be slightly more direct and elegant solution to the core problem. Example 3: Configurable LLM Safety (ID: ERce2rgMQC) Problem Statement: How can large language models be equipped with mechanism that allows their safety behavior to be dynamically configured at inference time to satisfy diverse cultural, regional, and individual safety requirements, without requiring model retraining? Agent Response (GPT-OSS-120B Self-Play): We introduce Dynamic Safety Router (DSR) that sits between the transformer layers of frozen LLM and its output head... the DSR computes per-token gating vectors that multiplicatively modulate the hidden state... Agent Response (Mistral-24B GPT-OSS-120B): We introduce Safety-Conditioned HyperAdapter (SCH) architecture that sits between frozen LLM and its output: lightweight hypernetwork receives structured safety profile and instantly emits low-rank additive weight matrices for each transformer layer... Winner: B. The HyperAdapter approach was judged to be more modern and flexible architecture for inference-time configuration compared to the gating mechanism. Example 4: Accelerating Diffusion Models (ID: OlzB6LnXcS) Problem Statement: How can we substantially accelerate the sampling process of diffusion and flowmatching generative models while preserving or improving sample quality, using single network trained in single phase? Agent Response (Mistral-24B Self-Play): We propose novel architecture, Sparse Adaptive Flow Network (SAFNet), which integrates adaptive sparse convolutions with dynamic skip connections... Agent Response (GPT-OSS-120B Self-Play): We introduce Dynamic-Step Diffusion (DSD): continuous-time diffusion model that receives... learned embedding of the desired step size... During training we... minimize combined loss: (i) the standard score/flow matching loss... and (ii) consistency loss... Winner: B. The DSD proposal with its consistency loss was deemed more theoretically grounded and powerful approach for handling variable step sizes compared to SAFNet. Example 5: Stochastic Linear Bandits (ID: Oeb0I3JcVc) Problem Statement: Is it possible to derive instance-specific, data-driven regret guarantees for stochastic linear bandit algorithms... and to adjust these algorithms so that they preserve their empirical effectiveness while achieving the minimax optimal regret? Agent Response (Mistral-24B Self-Play): We propose an adaptive, instance-specific regretguaranteed stochastic linear bandit algorithm ISRG-SLB that dynamically adjusts its explorationexploitation trade-off... ISRG-SLB employs Bayesian posterior sampling mechanism... Agent Response (Mistral-24B GPT-OSS-120B): We introduce Hardness-Adaptive Regret Guard (HARG), meta-algorithm that runs base linear bandit method... but continuously computes an instance-specific hardness score... When the score exceeds threshold, HARG switches to robust OFUL-style exploration policy... Winner: (Tie/Cannot Decide). The evaluator judged both proposals to be highly sophisticated and plausible meta-algorithms that addressed the core problem in different but equally valid ways, making clear winner difficult to determine. Example 6: Root-Cause Analysis (ID: l11DZY5Nxu) Problem Statement: How can we reliably identify the true root-cause nodes of an anomaly in complex system when anomalies are rare and lie outside the training distribution of structural causal model, making counterfactual estimates unreliable? 13 Agent Response (Mistral-24B GPT-OSS-120B): We introduce Distribution-Aware Counterfactual Projection (DACP): (1) train causal invertible generative model... on abundant normal operation data... (2) given an anomalous observation, compute its in-distribution projection... (3) for each candidate... perform an on-manifold do-intervention... Agent Response (GPT-OSS-120B Mistral-24B): We propose novel approach called Causal Influence Propagation (CIP) that operates within the in-distribution regime by iteratively simulating the propagation of influences through the causal graph. Given an observed anomaly, CIP initializes set of candidate root-cause nodes... Winner: A. The DACP proposal was judged to be more rigorous and theoretically grounded approach by explicitly using generative model to project anomalies back to the in-distribution manifold before intervention. Example 7: Promptable 3D Segmentation (ID: yXCTDhZDh6) Problem Statement: How can unified, scalable foundation model be developed for promptable 3-D segmentation of point clouds that overcomes heterogeneous data formats and the scarcity of diverse, richly annotated 3-D mask data? Agent Response (Mistral-24B GPT-OSS-120B): We introduce GeoPrompt-3D, unified sparse-voxel transformer that first tokenizes any point-cloud format into geometry-aware token lattice... multimodal prompt encoder maps textual descriptions, point clicks, or exemplar masks into set of prompt tokens... Agent Response (Human Abstract): The development of 2D foundation models for image segmentation has been significantly advanced by the Segment Anything Model (SAM). However, achieving similar success in 3D models remains challenge... To this end, we propose 3D promptable segmentation model Point-SAM... We then distill the rich knowledge from 2D SAM for Point-SAM training... Winner: (Human). The human-proposed solution of distilling knowledge from the existing 2D SAM was judged to be more pragmatic and powerful approach to overcoming data scarcity than the LLMs proposal of generating synthetic masks with diffusion model."
        },
        {
            "title": "D EXTENDED QUALITATIVE ANALYSIS OF SOLUTION ARCHETYPES",
            "content": "To understand the conceptual landscape of the generated solutions, we performed qualitative analysis by clustering the entire solution space. We grouped solutions into semantically coherent clusters via KMeans++ on their embeddings and assigned each cluster an interpretable label based on its most characteristic keywords. This process reveals rich taxonomy of 11 distinct solution archetypes, summarized in Table 8. The analysis demonstrates that the agents do not merely produce generic outputs but explore diverse range of modern research paradigms. These archetypes span broad strategies like Reinforcement Learning (C0), foundational model architectures (C1), and highly specialized domains such as Molecular Graph Modeling (C2) and 3D Scene Representation (C8). Table 8: Taxonomy of identified solution archetypes. Cohesion is the average intra-cluster cosine similarity. ID"
        },
        {
            "title": "Size",
            "content": "Neural/Latent Dynamics LLM Reasoning Adaptive Gradient Methods Reinforcement Learning C0 C1 Transformer Architectures C2 Molecular Graph Modeling C3 C4 C5 C6 Multimodal Learning Data-Centric AI C7 3D Scene Representation C8 C9 Diffusion Models C10 Graph Neural Networks policy, learning, reward, agent, action attention, model, token, rank, layer protein, graph, molecular, diffusion, equivariant neural, latent, time, network, dynamics model, llm, language, reasoning, human gradient, adaptive, algorithm, convergence, learning visual, modal, language, transformer, video data, model, training, loss, class 3d, scene, point, view, motion diffusion, latent, image, model, diffusion model graph, node, gnn, edge, graphs 0.48 0.46 0.48 0.41 0.46 0.38 0.46 0.41 0.48 0.51 0.47 1037 1106 398 899 1647 936 911 974 548 775 424 The conceptual cohesion of each archetype, measured by intra-cluster cosine similarity, offers further insight. For example, Diffusion Models (C9) form the most semantically coherent group (0.51 similarity), suggesting welldefined and consistently applied solution pattern. In contrast, Adaptive Gradient Methods (C5) are the most diverse 14 (0.38 similarity), indicating broader set of varied approaches within that theme. This analysis confirms that the agents are capable of generating wide spectrum of scientifically relevant and conceptually distinct solutions, providing qualitative validation of their creative problem-solving abilities. Representative Solution Archetypes. Below we provide representative generated solution for each of the 11 identified clusters. Cluster 0: Reinforcement Learning & Policy Optimization (N=1037) Keywords: policy, learning, reward Representative Solution: We introduce Meta-Regularized Adaptive Normalization (MRAN), model-free actor-critic algorithm that augments shared Transformer-based policy-value network with lightweight context encoder. Cluster 1: Transformer Architecture & Attention (N=1106) Keywords: attention, model, token Representative Solution: We introduce the Meta-Optimized Sparse Mixture-of-Experts Transformer (MOS-MoE), in which lightweight hypernetwork predicts, for each token and layer, sparse subset (e.g., 2-4) of expert feed-forward networks. Cluster 2: Molecular & Protein Graph Learning (N=398) Keywords: protein, graph, molecular Representative Solution: We propose novel Equivariant Variational Conformer Autoencoder (EVC-AE) for structure-based drug design, which extends equivariant graph neural networks (EGNNs) with variational autoencoder. Cluster 3: Neural ODEs & Continuous-Time Models (N=899) Keywords: neural, latent, time Representative Solution: We propose Temporal Neural Implicit ODE (TNI-ODE) architecture in which deep network fθ(x, t, t) parameterizes continuous-time vector field that explicitly receives the elapsed time between observations. Cluster 4: Large Language Models & NLP (N=1647) Keywords: model, llm, language Representative Solution: We introduce hierarchical self-reward framework where population of heterogeneous critic LLMs (parameter-perturbed copies) generate pairwise preference judgments for candidate completions, and main LLM is fine-tuned on this preference data. Cluster 5: Optimization & Gradient Algorithms (N=936) Keywords: gradient, adaptive, algorithm Representative Solution: We propose Sparse Adaptive Sketch-Based Gradient Estimation (SASGE): at each online round the algorithm draws set of = O(s log d) *sparse* random perturbation vectors whose support follows structured distribution. Cluster 6: Multimodal & Visual Language Models (N=911) Keywords: visual, modal, language Representative Solution: We introduce novel architecture called Compositional Semantic Transformer (CoST) that integrates multi-modal encoder with graph-based reasoning module. The CoST model encodes visual and linguistic inputs into shared semantic space. Cluster 7: Adversarial Learning & Robustness (N=974) Keywords: data, model, training Representative Solution: We propose Decoupled Adversarial Invariance Learning (DAIL) framework that decouples the training of invariance and classification objectives using dual-branch architecture with shared representations but separate adversarial heads. 15 Cluster 8: 3D Vision & Scene Representation (N=548) Keywords: 3d, scene, point Representative Solution: We introduce Dynamic Implicit Neural Volume (DINV): given few calibrated input images, multi-scale patch transformer extracts per-patch embeddings and aggregates them via cross-attention module to query dynamic MLP-based radiance field. Cluster 9: Generative Diffusion Models (N=775) Keywords: diffusion, latent, image Representative Solution: We propose Perceptual-Guided Diffusion (PGD) framework that augments the standard denoising diffusion process with trainable Perceptual Guidance Module (PGM). At each denoising step t, the cross-attention layers of the U-Net receive conditioning from the PGM. Cluster 10: Graph Neural Networks (GNNs) (N=424) Keywords: graph, node, gnn Representative Solution: We introduce Spectral-Diffusion Graph Transformers (SD-GT), which augment the standard self-attention with (i) set of learnable wavelet-filtered Laplacian eigen-vectors that provide multi-scale spectral information and (ii) graph diffusion process."
        },
        {
            "title": "E AGENT AND CRITIC PROMPTS",
            "content": "This section contains the exact prompts used for the Generalizer agent, the Solver agent, and their corresponding internal/external critics. Prompt: Generalizer Agent (G) System Role: You are an AI researcher with 20 years of experience in the field. Your task is to read research abstract and identify the core research question tackled by the paper. You must be extremely careful to: Preserve the fundamental scientific challenge Avoid hinting at specific solution methods Maintain precision and clarity User Prompt: Original Research Abstract: {abstract} Your Task: Write the core research question that captures the core scientific problem described in the abstract. Requirements: Semantic Fidelity: Preserve the fundamental scientific challenge exactly. Information Preservation: Retain all critical details, constraints, and insights. Specificity: Be precise and unambiguous. Solution Blindness: Do not hint at or describe the specific solution method. Output Format: Provide your problem statement in 2-3 clear, concise sentences. Provide justification for the identified research question. Enclose the problem statement inside <problem statement></problem statement> tags. Enclose your justification within <justification></justification> tags. 16 Prompt: Generalizer Agent (G) System Role: You are an expert evaluator specializing in assessing the quality of problem statements from research abstracts. Your role is to critically evaluate whether problem statement successfully captures the core idea of research abstract across multiple dimensions. User Prompt: Original Research Abstract: {original abstract} Extracted Problem Statement to Evaluate: {problem} Evaluation Task: Assess the quality of the problem statement against the original abstract using the following dimensions: Semantic Fidelity (1-10): How well does the problem statement preserve the core scientific problem? (10 = identical challenge). Information Loss (1-10): Assess the severity of any missing critical details. (10 = critical info lost, 1 = no loss). Ambiguity (1-10): Rate the ambiguity of the problem. (10 = highly ambiguous, 1 = perfectly specific). Solution Leakage (1-10): Does it hint at the solution? (10 = explicitly describes solution, 1 = completely blind). Finally, provide final judgement on whether the problem statement preserves the core problem. Output Format: Provide your evaluation in structured format with separate tags for each assessment (<semantic fidelity assessment>, etc.) and <final judgement> tag. Prompt: Solver Agent (S) System Role: You are an expert AI research scientist. Your task is to invent plausible technical approach that could solve given scientific problem in machine learning. You must be creative and innovative, proposing novel solutions. User Prompt: Problem Statement: {problem} Your Task: Propose specific and novel technical approach, mechanism, or architecture. Explain your proposed method in 35 sentences as if youre writing the core idea in research paper. Requirements: Novelty & Creativity: Propose non-obvious, innovative solution. Technical Feasibility: Ensure your approach is logically sound and implementable. Completeness: Provide enough detail to understand the core methodology. Output Format: Provide your proposed technical approach in 3-5 sentences. Provide brief justification for the proposed solution. Enclose your solution inside <solution></solution> tags. Enclose your justification within <justification></justification> tags. 17 Prompt: Solution Critic ( for Solver) System Role: You are an expert evaluator specializing in assessing the quality of proposed solutions for complex research problems. Your role is to evaluate the proposed solution solely on how well it solves the problem. You must be thorough and objective in your assessment. User Prompt: Problem Statement: {problem} Proposed Solution: {pred} Evaluation Task: Assess the quality of the proposed solution using the following dimensions: Novelty & Creativity (1-10): How novel is the approach? (10 = highly creative). Technical Feasibility (1-10): Is the solution technically sound? (10 = perfectly plausible). Completeness & Detail (1-10): How complete is the methodology? (10 = fully specified). Finally, write your final judgement indicating whether the proposed solution ultimately solves the problem. Output Format: Provide your evaluation in structured format with separate tags for each assessment (<novelty assessment>, etc.) and <final judgement> tag."
        }
    ],
    "affiliations": [
        "Canada CIFAR AI Chair",
        "HEC Montreal",
        "International Laboratory on Learning Systems (ILLS)",
        "LIVIA, ETS Montreal",
        "Mila Quebec AI Institute",
        "Polytechnique Montreal",
        "ServiceNow Research",
        "Universite de Montreal"
    ]
}