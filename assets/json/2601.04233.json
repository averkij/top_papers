{
    "paper_title": "LEMAS: Large A 150K-Hour Large-scale Extensible Multilingual Audio Suite with Generative Speech Models",
    "authors": [
        "Zhiyuan Zhao",
        "Lijian Lin",
        "Ye Zhu",
        "Kai Xie",
        "Yunfei Liu",
        "Yu Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present the LEMAS-Dataset, which, to our knowledge, is currently the largest open-source multilingual speech corpus with word-level timestamps. Covering over 150,000 hours across 10 major languages, LEMAS-Dataset is constructed via a efficient data processing pipeline that ensures high-quality data and annotations. To validate the effectiveness of LEMAS-Dataset across diverse generative paradigms, we train two benchmark models with distinct architectures and task specializations on this dataset. LEMAS-TTS, built upon a non-autoregressive flow-matching framework, leverages the dataset's massive scale and linguistic diversity to achieve robust zero-shot multilingual synthesis. Our proposed accent-adversarial training and CTC loss mitigate cross-lingual accent issues, enhancing synthesis stability. Complementarily, LEMAS-Edit employs an autoregressive decoder-only architecture that formulates speech editing as a masked token infilling task. By exploiting precise word-level alignments to construct training masks and adopting adaptive decoding strategies, it achieves seamless, smooth-boundary speech editing with natural transitions. Experimental results demonstrate that models trained on LEMAS-Dataset deliver high-quality synthesis and editing performance, confirming the dataset's quality. We envision that this richly timestamp-annotated, fine-grained multilingual corpus will drive future advances in prompt-based speech generation systems."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 4 ] . [ 1 3 3 2 4 0 . 1 0 6 2 : r LEMAS: 150K-Hour Large-scale Extensible Multilingual Audio Suite with Generative Speech Models Zhiyuan Zhao, Lijian Lin, Ye Zhu, Kai Xie, Yunfei Liu, Yu Li International Digital Economy Academy (IDEA) Project Lead Generative speech models have achieved strong performance in high-resource languages such as English and Chinese, yet their capabilities remain limited in multilingual settings. This gap is largely driven by data challenges: collecting and curating large-scale multilingual speech with consistent quality and fine-grained temporal annotations is substantially more difficult, while existing web-crawled corpora provide limited guarantees on data quality and annotation reliability in multilingual settings. To address this bottleneck, we present the LEMAS-Dataset, which, to our knowledge, is currently the largest open-source multilingual speech corpus with word-level timestamps. Covering over 150,000 hours across 10 major languages, LEMAS-Dataset is constructed via efficient data processing pipeline that ensures high-quality data and annotations. To validate the effectiveness of LEMAS-Dataset across diverse generative paradigms, we train two benchmark models with distinct architectures and task specializations on this dataset. LEMAS-TTS, built upon non-autoregressive flow-matching framework, leverages the datasets massive scale and linguistic diversity to achieve robust zero-shot multilingual synthesis. Our proposed accent-adversarial training and CTC loss mitigate cross-lingual accent issues, enhancing synthesis stability. Complementarily, LEMAS-Edit employs an autoregressive decoder-only architecture that formulates speech editing as masked token infilling task. By exploiting precise word-level alignments to construct training masks and adopting adaptive decoding strategies, it achieves seamless, smooth-boundary speech editing with natural transitions. Experimental results demonstrate that models trained on LEMAS-Dataset deliver high-quality synthesis and editing performance, confirming the datasets quality. We envision that this richly timestamp-annotated, fine-grained multilingual corpus will drive future advances in prompt-based speech generation systems. Correspondence: Zhiyuan Zhao zhaozhiyuan@idea.edu.cn Demo page: https://lemas-project.github.io/LEMAS-Project Dataset: https://huggingface.co/LEMAS-Project Code: https://github.com/LEMAS-Project 1. Introduction The field of speech synthesis has entered transformative phase with the rise of generative foundation models. Leveraging large-scale pre-training on massive unlabeled audio, recent systems have achieved impressive zeroshot performance, particularly in high-resource languages such as English and Chinese [11, 18, 39, 9, 38, 44]. Despite this progress, extending generative speech models to truly multilingual scenarios remains challenging. Compared to English and Chinese, both open-source multilingual datasets and foundation models are still limited in scale, coverage, and annotation quality, leading to noticeable performance degradation across languages. As model capacity continues to grow, the primary bottleneck has increasingly shifted from architecture design to dataspecifically, the scarcity of large-scale, high-quality, and fine-grained multilingual speech corpora. Limitations of Existing Speech Datasets. While recent efforts have substantially scaled up publicly available speech resources, several fundamental limitations remain. Large-scale corpora such as Libri-Light [20], GigaSpeech [7], and WenetSpeech [43] provide abundant audiotext pairs, enabling the training of robust ASR and TTS models. However, these datasets are predominantly monolingual, focusing on either English or Chinese, and do not address the scarcity of large-scale multilingual resources. Conversely, multilingual Figure 1 - LEMAS-Dataset contains more than 150k hours of multi-speaker speech with forced word-level alignments across 10 major languages. Based on LEMAS-Dataset, we train two models. LEMAS-TTS performs large-scale, flow-based neural TTS that streams high-fidelity speech from text and short reference clip, while LEMAS-Edit performs codec-based, word-level speech editing. datasets such as TEDx [32] and MLS [29] are constrained by limited scale and narrow domain coverage. Specifically, TEDx consists mainly of public speeches, whereas MLS is restricted to the audiobook domain. More recently, large-scale multilingual collections such as YODAS [24] have been introduced, but they rely heavily on minimally filtered web-crawled data, resulting in highly variable quality and lack of reliable fine-grained annotations. We also acknowledge promising efforts such as WenetSpeech4TTS [25], which reprocesses WenetSpeech with rigorous cleaning and introduces timestamp-level annotations tailored for speech synthesis. While effective, such datasets remain confined to single language. In contrast, our goal is to extend this level of data quality and fine-grained temporal supervision to truly multilingual setting, motivating the construction of LEMAS-Dataset. Besides, built upon LEMAS-Dataset, we design two foundation models: LEMAS-TTS and LEMAS-Edit (Figure 1). LEMAS-Dataset. To bridge this gap, we introduce LEMAS-Dataset. To the best of our knowledge, LEMASDataset is the largest open-source multilingual speech corpus with rigorous word-level timestamps, comprising over 150,000 hours of speech across 10 major languages, including Chinese, English, Russian, Spanish, Portuguese, German, French, Italian, Indonesian and Vietnamese. Table 1 highlights LEMAS-Datasets advantages in both scale and temporal alignment precision compared to existing corpora. Constructing such dataset from in-the-wild audio poses unique challenges, as traditional HMM-based forced alignment tools (e.g., MFA [26]) often struggle with noisy, unsegmented speech. We address this challenge through robust multi-stage processing pipeline that leverages the MMS aligner [30] to extract word-level timestamps and, crucially, assigns confidence score to each aligned word. This design enables flexible, reliability-aware data filtering, allowing researchers to dynamically trade off data scale and alignment precision according to downstream requirements, and ensuring high utility for precision-critical applications. To demonstrate the versatility of LEMAS-Dataset across diverse generative paradigms, we develop two parallel foundation models, utilizing the datasets scale and precision, respectively: 2 Table 1 - Comparison of LEMAS-Dataset with existing large-scale processed speech datasets. Audiobook data refers to professionally recorded, script-aligned speech with clean acoustic conditions, while in-the-wild data consists of naturally occurring speech collected from real-world scenarios, often exhibiting background noise, spontaneous speaking styles, and imperfect text-audio alignment. LEMAS-Dataset provides extensive total duration and broad language coverage among datasets that include utterance-level timestamps across diverse data sources. Dataset Libri-light [20] GigaSpeech [7] GigaSpeech2 [42] WenetSpeech [43] WenetSpeech4TTS [25] MLS [29] Emilia [19] Voxbox [38] LEMAS-Dataset (ours) Total Duration (h) Languages 60k 10k 28k 22k 13k 51k 102k 103k 150k 1 1 3 1 1 8 6 2 10 timestamps Data Source Audiobook Audiobook/In-the-wild Audiobook/In-the-wild In-the-wild In-the-wild Audiobook In-the-wild Audiobook/In-the-wild Audiobook/In-the-wild LEMAS-TTS. Without language-specific alignment regularization, flow-matching models suffer from alignment drift that degrades intelligibility in low-resource languages, as well as accent leakage that replaces target prosody with that of higher-resource languages [9], ultimately undermining zero-shot cross-lingual fidelity. To address these challenges, we extend the F5-TTS [9] architecture with language-aware stabilization mechanisms. Specifically, we unify diverse writing systems into shared phonetic space and introduce two complementary objectives: Connectionist Temporal Classification (CTC) loss [17] to improve speech intelligibility, and an accent-adversarial loss [16] to suppress cross-lingual accent interference. These enhancements enable the model to effectively leverage the full 150k-hour corpus, achieving both high speaker similarity and naturalness in zero-shot cross-lingual synthesis. LEMAS-Edit. To further validate the effectiveness of LEMAS-Dataset for autoregressive speech generation and editing, we develop LEMAS-Edit, specialized editing model built upon the decoder-only architecture. VoiceCraft [28] formulates speech editing as masked token-infilling task, whose generation quality is highly sensitive to boundary precision and decoding stability. Misaligned edit boundaries can lead to noticeable artifacts, such as prolonged silences or unnatural acoustic discontinuities [28]. To mitigate repetition artifacts commonly observed during autoregressive decoding, we introduce repetition penalty at inference time to suppress token looping. We further extend the editing pipeline with system-level components, including denoising, re-alignment using MMS, and consistency verification, enabling precise and robust user-controlled editing. As demonstrated in our demos, LEMAS-Edit achieves seamless, smooth-boundary editing and remains effective on in-the-wild audio with environmental noise or background sounds, highlighting the strong generalization enabled by LEMAS-Dataset. In summary, our contributions are threefold: LEMAS-Dataset: We release the largest open-source multilingual speech corpus with rigorous wordlevel timestamps, comprising over 150,000 hours across 10 languages. The dataset is constructed via scalable alignment pipeline with word-level confidence estimation, enabling reliable, fine-grained temporal supervision for precision-critical speech generation tasks. LEMAS-TTS: We develop non-autoregressive flow-matching TTS model for large-scale multilingual training. By unifying scripts into shared phonetic space and introducing CTC-based alignment regularization and accent-adversarial objectives, LEMAS-TTS achieves robust zero-shot cross-lingual synthesis with improved intelligibility and accent consistency. LEMAS-Edit: We develop an autoregressive decoder only speech editing system with stabilized inference through repetition penalty and speech-rate-based anomaly detection, supported by robust pipeline with denoising, re-alignment, and verification. LEMAS-Edit enables seamless, smooth-boundary multilingual editing and generalizes well to in-the-wild noisy audios. 3 2. Related Work"
        },
        {
            "title": "2.1 Large-Scale Speech Corpora",
            "content": "The evolution of speech datasets has been characterized by transition from curated, read speech to massive, multi-domain collections harvested from the web. This progression addresses the growing hunger of end-to-end models for data scale and diversity. Curated and Crowdsourced Foundations. Early multilingual corpora like GlobalPhone [33] and IARPA BABEL [15] set the standard for high-quality, curated recordings, though they were limited in scale. Mozilla Common Voice [2] later democratized data collection via crowdsourcing, prioritizing speaker diversity. In the domain of audiobooks, LibriSpeech [27] and Multilingual LibriSpeech (MLS) [29] became the de facto benchmarks for ASR, offering clean read speech suitable for reproducible research. Massive-Scale Multi-Domain Corpora. To overcome the domain limitations of audiobooks, researchers turned to large-scale internet sources. GigaSpeech [7] introduced an evolving, multi-domain English corpus derived from audiobooks, YouTube and podcasts, providing 10,000 hours of high-quality labeled audio. This shift marked move towards modeling diverse, \"in-the-wild\" recording conditions. In the landscape of Chinese speech, WenetSpeech [43] (and its recent derivative WenetSpeech4TTS [19]) aggregated tens of thousands of hours of multi-domain Mandarin speech, further validating the efficacy of web-mined data for training industrial-grade ASR and TTS models. Multilingual and Large-Scale unsupervised Speech Corpora. Recent efforts have greatly expanded multilingual speech datasets in scale and language coverage, focusing on weakly supervised or unlabeled data. VoxPopuli [37] offers 400k hours of unlabeled speech in 23 languages from European Parliament recordings. YODAS [24] collects 500k+ hours from 100+ languages on YouTube, using automatic captions with variable quality. These datasets trade label quality for massive scale, fueling selfand semi-supervised speech research."
        },
        {
            "title": "2.2 Generative Speech Models",
            "content": "Neural TTS has shifted from task-specific models to unified generative frameworks. It evolved from autoregressive models like Tacotron [40] to non-autoregressive FastSpeech [31], and then to end-to-end variational models like VITS [22]. Todays frontier features large-scale foundation models unifying synthesis, cross-lingual cloning, and editing. Generative Foundation Models. Speech generation falls into discrete, continuous, and hybrid paradigms, differing in representation and generation methods. Discrete approaches model speech as sequences of codec tokens autoregressively or via masked infilling, similar to language models. VALL-E [8] pioneered zero-shot TTS using EnCodec tokens; AudioLM [4] adds hierarchical semantic and acoustic token prediction. XTTS [5] extends this to many languages. Recent works focus on efficiency and control, e.g., MaskGCT [39] and FishAudio [36]. Continuous approaches generate mel-spectrograms or waveforms directly from noise using diffusion or flow matching. NaturalSpeech 2 [35] demonstrates high-fidelity latent diffusion; VoiceBox [23] simplify pipelines by dropping external encoders. F5-TTS [9] employs DiT structure with fill-in-the-middle, achieving competitive zero-shot synthesis with streamlined design. Hybrid approaches combine discrete semantic modeling with continuous acoustic refinement for diverse and faithful synthesis. CosyVoice 13 [10, 12, 13] merge LLM-based semantic tokens with conditional flow matching for multilingual instruction-following TTS; FireRed-TTS [41] cascades discrete and flow-based models for prosody control. IndexTTS 2 [44] further explores semantic compression and style manipulation efficiently. Multilingual Extensions. With growing training corpora, TTS shifted from mono/bilingual to multilingual. Early open-source models like XTTS-v2 [5] trained on 17 languages jointly. Community models such as OpenAudio-S1-mini [36] support 13 languages with 100k+ hours, enhancing cross-lingual generalization. At larger scale, CosyVoice 3 [13] uses speech tokenizer from large audio understanding model, training on 4 over million hours across 9 languages and 18 Chinese dialects. This reflects rapid data scaling and demand for massive multilingual datasets. Text-Based Speech Editing. Speech editing modifies spoken content while preserving timbre and is now seen as TTS subtask. VoiceBox [23] shows versatile in-context learning for zero-shot TTS, editing, style conversion, and diverse generation. Non-autoregressive MaskGCT [39] uses mask-and-predict learning; F5-TTS [9] employs Diffusion Transformer (DiT) supporting TTS and editing jointly. Autoregressive VoiceCraft [28] combines causal masking and delayed stacking for flexible generation and diverse editing within sequences. 3. LEMAS-Dataset We introduce LEMAS-Dataset, massive open-source multilingual speech corpus with over 150,000 hours spanning 10 major languages: Chinese (Zh), English (En), Russian (Ru), Spanish (Es), Indonesian (Id), German (De), Portuguese (Pt), Vietnamese (Vi), French (Fr), and Italian (It). LEMAS-Dataset provides word-level alignments along with confidence scores for each word. This fine-grained annotation facilitates wide range of applications, including data quality filtering based on confidence scores, training duration prediction models, and generating detailed prompts to guide speech synthesis and understanding."
        },
        {
            "title": "3.1 Dataset Processing Pipeline",
            "content": "Constructing large-scale multilingual speech corpus for generative modeling requires more than merging existing datasets, which vary widely in annotations, transcription quality, and acoustics. The key insight of the LEMAS-Dataset pipeline is to separate language diversity from annotation fragility, retaining only samples with reliably grounded temporal structure. To achieve this, we develop multi-stage pipeline that unifies heterogeneous sources, performs robust multilingual alignment, and applies scalable quality control. Data Ingestion and Structural Unification. We first aggregate audiotext pairs from diverse collection of publicly available speech corpora, including GigaSpeech (English) [7], GigaSpeech2 (Indonesian, Vietnamese) [42], WenetSpeech4TTS (Chinese, VoiceBox-based) [25, 38], Emilia (Chinese and English, VoiceBox-based) [19], MLS (English, German, French, Portuguese, Spanish, Italian) [29], multilingual TEDx (German, French, Portuguese, Spanish, Italian) [32], Alcaim (Portuguese) [6], Golos (Russian) [21], and Yodas (German, French, Spanish, Portuguese, Russian, Italian, Indonesian, Vietnamese) [24]. These sources span wide range of recording scenarios, including audiobooks, podcasts, and in-the-wild conversational speech. Rather than preserving dataset-specific schemas, we normalize all inputs into unified data representation, enabling language-agnostic downstream processing and large-scale automation. Multilingual MMS Alignment via Romanized Text. Reliable word-level timestamps are prerequisite for both TTS training and speech editing tasks, yet traditional forced alignment pipelines often rely on language-specific lexicons and G2P systems, which are brittle under multilingual, noisy, and code-switched conditions. To avoid these limitations, we adopt the Multilingual MMS Forced Aligner 1, wav2vec-based CTC alignment model [3, 17] provided by torchaudio 2. This model was trained on over 23,000 hours of speech spanning more than 1,100 languages as part of the Scaling Speech Technology to 1,000+ Languages initiative [30]. Prior to alignment, transcripts are language-specifically normalized and then romanized using tools such as Uroman 3, which maps diverse scripts (e.g., Chinese characters, Cyrillic) into shared Latin representation. This design removes the dependency on language-specific pronunciation dictionaries and allows the alignment model to robustly handle rare words, named entities, and intra-sentential code-switching. The MMS aligner then produces word-level boundaries along with token-level posterior probabilities, which form the basis for subsequent quality control. Confidence-Driven Quality Filtering. central principle of our pipeline is that successful alignment alone is insufficient; only alignments with high temporal reliability are suitable for generative speech modeling. 1https://docs.pytorch.org/audio/2.3/tutorials/forced_alignment_for_multilingual_data_tutorial.html 2https://github.com/pytorch/audio 3https://github.com/isi-nlp/uroman 5 We therefore retain only samples for which word-level alignments can be extracted, and apply series of rule-based filters grounded in alignment confidence and temporal consistency: Alignment Confidence Filtering. For each utterance, we compute an average alignment confidence score based on the posterior probabilities of aligned tokens. To accommodate variability across languages and source corpora, we apply dataset-specific thresholds [0.2, 0.5], retaining only samples whose confidence exceeds the corresponding threshold. Duration and Pause Constraints. Utterances shorter than 0.5 seconds or longer than 30 seconds are removed to ensure compatibility with TTS training. Additionally, samples containing prolonged unaligned regions or silence intervals exceeding 4 seconds are discarded, as such patterns often indicate segmentation or transcription errors. Speech Rate Normalization. To exclude abnormally fast or slow speech, we enforce languagespecific constraint on the ratio between textual length and audio duration. For each language L, the L], character-based ratio len(text)/duration must fall within predefined interval [min_ratio empirically determined from corpus statistics. L, max_ratio Language and Character Validation. We restrict the dataset to ten target languages: Chinese (zh), English (en), Russian (ru), Spanish (es), Indonesian (id), German (de), Portuguese (pt), Vietnamese (vi), French (fr), and Italian (it). All transcripts are verified to belong to one of these languages, and samples containing unsupported scripts, emojis, or excessive non-linguistic symbols are removed according to language-specific character sets. All thresholds are selected empirically to balance data scale and alignment precision. Notably, we do not apply explicit speech enhancement or background noise removal during preprocessing. While many TTS systems remain sensitive to acoustic interference, we intentionally preserve in-the-wild characteristics, with the expectation that more advanced generative models will be better equipped to leverage such variability. Consistent with this design choice, examples on our demo page show that the proposed editing model remains effective on noisy recordings and can, to some extent, reproduce acoustic environments consistent with the reference audio, highlighting the practical benefits of retaining realistic background conditions. Final Representation and Storage. The resulting dataset is stored in unified JSON-based format, containing unique sample key, audio file paths, original and normalized transcripts, and word-level timestamps with associated confidence scores. For efficient storage and distribution, all audio files are converted to MP3 format while preserving their original sampling rates. Figure 2 - Dataset Statistics of LEMAS-Dataset. (a) Language-wise duration, (b) the average sentence duration in seconds. The dataset shows substantial variation in both data volume and sentence length across languages."
        },
        {
            "title": "3.2 Dataset Statistics",
            "content": "The LEMAS-Dataset consists of approximately 150,144 hours of audio data spanning 10 major languages. As illustrated in Figure 2, we explicitly curate the dataset to balance speech duration across languages, resulting in relatively uniform distribution despite differences in source data availability. While high-resource languages 6 such as Chinese (Zh) and English (En) remain slightly larger, all languages are represented at substantial scale. In particular, even the lower-resource subsets (e.g., Italian and Vietnamese) exceed 6,000 hours, which enables effective monolingual training as well as joint multilingual modeling. To facilitate reproducible research and consistent benchmarking, the dataset is partitioned into training (train) and evaluation (eval) subsets. Training Set. Detailed statistics for the training partition are provided in Table 2. The training set contains over 150,000 hours of audio in total, designed to support the training of large-scale acoustic models. We note that sentence-level statistics vary across languages due to differences in source corpora. Specifically, Chinese and English data are primarily derived from GigaSpeech, Emilia, and WenetSpeech4TTS, which contain longer-form utterances, whereas data for other languages are mainly sourced from YODAS, which consists of shorter, pre-segmented utterances without additional merging or restructuring. As result, the average utterance length in Chinese and English is noticeably longer than in other languages. Table 2 - Detailed statistics of the training set from LEMAS-Dataset. The units h, s, and denote hours, seconds, and millions (106), respectively. Avg. Duration and Avg. Words indicate average duration and average words per utterance. Language Total Duration (h) Avg. Duration (s) Utterances (M) Total Words (M) Avg. Words it fr vi pt de id es ru en zh total 6.12k 6.54k 6.60k 7.38k 9.84k 11.25k 21.22k 22.92k 25.35k 32.92k 150.14k 3.05 3.11 3.70 3.06 3.22 3.47 2.89 3.00 9.59 6.67 4.18 7.21 7.56 6.43 8.68 11.00 11.66 26.41 27.47 9.52 17.78 133.71 48.86 65.84 89.90 60.32 80.12 85.97 183.67 163.02 268.68 496.96 1,543.34 6.78 8.71 13.99 6.95 7.28 7.37 6.96 5.93 28.24 27.96 12. Table 3 - Detailed statistics of evaluation set from LEMAS-Dataset. The units min and denote minutes and seconds, respectively. Avg. Duration and Avg. Words indicate average duration and average words per utterance. Language Total Duration (min) Avg. Duration (s) Utterances Total Words Avg. Words it fr vi pt de id es ru en zh total 44.22 38.17 36.74 41.69 38.65 47.20 40.52 40.24 67.46 75.84 470.73 5.31 4.58 4.41 5.00 4.64 5.67 4.86 4.82 8.10 9.10 5.65 500 500 500 500 500 500 500 500 500 500 5, 6,599 6,546 6,727 5,812 5,599 6,133 6,216 5,138 11,325 18,627 78,722 13.20 13.09 13.45 11.62 11.20 12.27 12.43 10.28 22.65 37.25 15.74 Evaluation Set. To establish consistent benchmark, we curate separate evaluation set, as detailed in Table 3. Unlike the training set, the evaluation set is balanced by strictly selecting 500 utterances per language based on rigorous quality criteria. Specifically, we filter for samples with an average word-level alignment score greater than 0.9, word count exceeding 5, and duration between 3 and 15 seconds. Additionally, sentence-end silence is trimmed to maximum of 0.2 to standardize acoustic characteristics. From the pool of samples that satisfy the above conditions, we further compute language-specific statistics of the character-to-duration ratio and select the 500 utterances closest to the per-language mean. This additional step helps exclude atypical or anomalous cases, such as singing or exaggerated speaking styles, while preserving representative speaking patterns for each language. Finally, to further ensure data quality, approximately 20% of the evaluation samples were manually reviewed to verify that the test set consists exclusively of natural speech. This curation ensures that evaluation metrics (e.g., WER or MOS) are comparable across different languages without being biased by data volume differences or quality variations. 7 4. LEMAS-TTS We design TTS method, LEMAS-TTS, builds upon the F5-TTS architecture [9], utilizing its nonautoregressive Flow Matching framework and Diffusion Transformer (DiT) backbone. While F5-TTS excels at zero-shot tasks, scaling it to robust multilingual model presents challenges such as alignment stability and effective prosody transfer across languages. To overcome these, we introduce architectural improvements that unify linguistic representations and stabilize the training process."
        },
        {
            "title": "4.1 Multilingual Training Framework",
            "content": "To facilitate high-fidelity multilingual synthesis, we revise the training paradigm to include unified phonetic input space, auxiliary supervision objectives, and fine-grained prosody modeling. Unified Phonetic Representation. To bridge the linguistic gap across diverse languages, we depart from the mixed character/word-level tokenization of the original system. Instead, we adopt unified phonetic front-end. For Chinese, we employ an initial-final Pinyin representation with tones to capture syllabic structure; for other languages, we utilize the International Phonetic Alphabet (IPA) sequences derived via eSpeak-NG [14]. We choose IPA for these languages because eSpeak-NGs frontend can automatically handle language switching, and using IPA also facilitates verification of correct pronunciation. All phonetic tokens are augmented with explicit language identifiers (e.g., <zh>, <en>) to project distinct languages into shared latent space while preserving language-specific phonotactics. Additionally, we leverage word-level timestamps to insert explicit pause tags (#1#4) within the phoneme sequences to denote short, medium, long and abnormal pauses, enabling simple yet effective pause control. Auxiliary Stabilization Objectives. Standard flow-matching objectives can sometimes face convergence difficulties when dealing with complex multilingual scenarios. To address this, we introduce two auxiliary loss functions to improve alignment accuracy and speech intelligibility: CTC Alignment Loss. To enforce strict monotonicity between the acoustic trajectory and linguistic content, we apply Connectionist Temporal Classification (CTC) loss on the decoder outputs. This is implemented via lightweight projection head [17] that maps the predicted mel-spectrograms to phone sequences, explicitly encouraging accurate duration modeling. Accent-Adversarial Disentanglement. Inspired by IndexTTS2 [44], we aim to disentangle speaker timbre from accent information. We attach an accent classifier to the conditioning pathway and optimize it via Gradient Reversal Layer (GRL). By supervising this classifier with pseudo-labels from an off-the-shelf language identification model, the encoder is forced to learn accent-invariant representations, thereby reducing cross-lingual accent leakage. Cross-Lingual Prosody Modeling. To capture fine-grained prosodic variations, we integrate Prosody Encoder adapted from the Seamless Expressive architecture [34]. Specifically, we extract 80-bin filterbank features from segmented sub-utterances and encode them into 512-dimensional prosody embedding using pretrained ECAPA-TDNN backbone. These embeddings are aligned with both mel-frames and text tokens and injected into the DiT via linear projection layers. This mechanism provides explicit prosodic supervision, significantly improving the naturalness of prosody transfer across languages. Design Analysis and Exclusions. During the development of LEMAS-TTS, we also explored explicit speaker contrastive learning (InfoNCE) and clip-and-shuffle data augmentation. However, empirical observations indicated that enforcing strict speaker invariance via InfoNCE conflicted with the flow-matching objective, leading to reduced stability. Similarly, shuffling segmented mel-chunks disrupted the temporal coherence required for the DiT to learn consistent continuations. Consequently, these components were excluded from the final architecture in favor of the alignment and prosody objectives described above."
        },
        {
            "title": "4.2 Inference Pipeline and Sampling Dynamics",
            "content": "To ensure robust multilingual generation, our inference framework strictly aligns with the training paradigm to minimize distribution shift, while incorporating an adjustable sampling strategy that enables flexible control over sampling dynamics to enhance stability across varying acoustic conditions and speech styles. 8 Unified Multilingual Front-End. We deploy standardized text processing pipeline that mirrors the training stage to ensure input consistency. Firstly, The system leverages automatic language identification to apply language-specific normalization rules (e.g., number expansion and punctuation filtering). Then, text is converted into the shared phonetic space defined in our training strategy: Chinese is mapped to tonal Pinyin with initialfinal decomposition, while other languages are converted into IPA sequences. Crucially, the system supports explicit insertion of spaces and pause tokens (#1-#3) within the text, enabling precise control over pause durations. Dynamic Guidance and Sway Sampling. The original Sway Sampling strategy in F5-TTS allocates more steps in the early generation phase to improve coarse structural accuracy. However, it performs suboptimally in multilingual and cross-lingual scenarios. To address this, we redesign the time-warping function and classifier-free guidance (CFG) schedule as one-parameter families, as illustrated in Figure 3, enabling smooth interpolation between linear and non-linear behaviors. By increasing early-stage steps and CFG scale, we improve pronunciation accuracy, while reducing CFG strength later allows greater model flexibility for enhanced speaker similarity and audio quality. Figure 3 - Time-dependent control schedules used in our sampling strategy. Left: CFG strength schedules with different maximum guidance levels, emphasizing early timesteps and gradually decaying over sampling. Right: Sway-sampling time warping with varying strengths, compared to cosine-based baseline (dashed), where stronger warping allocates more steps to later timesteps. Together, these schedules demonstrate how guidance strength and sampling allocation can be shaped over time to influence generation behavior. nificantly enhancing perceptual stability for long, multilingual utterances. First, we refine the Classifier-Free Guidance (CFG) mechanism. Let cond conditional and unconditional flows, respectively. We define the guided flow as: θ (x, t) and uncond (x, t) denote the θ (x, t) uncond θ where the time-dependent schedule g(t) is defined to decay quadratically as: (x, t) = cond (x, t) + g(t) (cid:0)f cond cfg θ θ θ (x, t)(cid:1), (1) g(t) = λ (1 t)2, with λ being scaling factor (e.g., λ = 5). This schedule applies stronger structural correction during the initial high-noise regime (t 0) and gradually relaxes guidance near = 1 to preserve acoustic naturalness, as shown in Equations (1) and (2). (2) Second, to further optimize the discretization of the flow trajectory, we update the Sway Sampling strategy. Instead of linear or cosine-based grid, we utilize power-law reparameterization as Equation (3) represents. Given uniform grid sk [0, 1] for = 0, . . . , K, the effective time steps are defined as: tk = 1+γ where γ 0 is tunable curvature parameter. Increasing γ effectively stretches the early sampling steps and compresses the later ones. Empirically, this redistribution of computational effort improves the recovery of signal from noise, significantly enhancing perceptual stability for long, multilingual utterances. (3) , 9 Versatile Conditioning Modes. The inference interface is extended to support flexible control over prosody and style. By enabling the prosody encoder, users can extract global prosody embedding from reference waveform. This embedding is injected into both the mel and text conditioning streams to facilitate precise style transfer. Additionally, we introduce reference-free mode that discards conditioning on reference audio, allowing the model to generate speech with randomly sampled speaker characteristics and more native-like pronunciation. This encourages the model to rely on internal priors for timbre and energy, enabling pure text-driven synthesis. 5. LEMAS-Edit We develop LEMAS-Edit by significantly extending the architecture of VoiceCraft [28], transforming it from monolingual English model into multilingual speech editing framework. Our contributions encompass an expansion of training data, re-engineered distributed training framework, and the introduction of adaptive decoding strategies to ensure generation stability."
        },
        {
            "title": "5.1 Multilingual Adaptation and Training",
            "content": "Crucially, when the language context switches within sentence, new language tags are inserted dynamically. This structure allows the autoregressive model to switch its internal linguistic context mid-utterance, naturally supporting mixed-lingual editing tasks. We have re-engineered the training infrastructure to support large-scale distributed learning. The framework now handles data sharding across multiple heterogeneous datasets and GPU ranks, ensuring balanced data consumption. significant optimization is the simplification of the optimization step. In the original implementation, gradient accumulation was used to simulate larger batches, which added complexity to the training dynamics. We remove this mechanism entirely in favor of single-step updates on full batches. This modification not only simplifies the codebase but also makes the relationship between batch size and effective gradient updates transparent, enhancing training stability and robustness against memory fragmentation when processing long multilingual sequences on large GPU clusters."
        },
        {
            "title": "5.2 Inference and Decoding Strategy",
            "content": "History-Aware Repetition Control. Autoregressive codec models often suffer from repetition loops, especially in long-form generation. To mitigate this, we introduce dynamic, history-aware penalty mechanism within the top-k/top-p sampling process. Unlike static penalties, our approach scales with the generation length. We modify the logits of previously generated tokens based on their frequency using the following formulation: penalty = repetition_penalty 100 num_gen + 1. (4) Here, num_gen represents the current number of generated tokens. For any token present in the history, its positive logits are divided by this penalty, while negative logits are multiplied by it. This progressively increases the cost of repeating tokens as the sequence grows, effectively suppressing both silence loops and content stuttering without degrading the initial prompt adherence. Adaptive Re-Generation Mechanism. To ensure that the generated audio duration aligns with natural speech patterns, we implement multi-round adaptive inference scheme. First, we establish baseline speaking rate by analyzing the reference audio: avg_speed = #encodec frames #raw text tokens . (5) This estimated rate allows us to impose dynamic max_rate constraint on the decoder, preventing run-away generation. However, simple constraints are insufficient for complex edits. Therefore, we introduce feedback loop. During generation, if the model triggers an internal anomaly flag (RE_GEN) or if the output length is pathologically short (specifically, < 0.5 avg_speed), the system automatically triggers re-generation pass. In each 10 subsequent round, we iteratively relax the constraints by expanding the editing mask boundaries (providing more acoustic context) and incrementing the repetition_penalty. This iterative process continues until the output matches the expected prosodic structure or maximum retry limit is reached."
        },
        {
            "title": "5.3 Multilingual Editing Front-End",
            "content": "We have upgraded the inference front-end to support professional-grade multilingual workflow, focusing on accuracy and flexibility: Robust Recognition and Alignment. Precise text-to-audio alignment is essential for accurate speech editing. To enhance transcription robustness, we replace the standard English ASR with the multilingual Whisper large model. Additionally, we employ the MMS Forced Aligner to obtain more accurate word-level timestamps. This fine-grained temporal information is crucial for constructing precise editing masks, ensuring modifications are strictly confined to the intended regions without affecting adjacent words. Signal Enhancement. To handle real-world recordings, the pipeline integrates two distinct denoising backends. Users can select UVR54 for mild denoising that preserves background ambience (crucial for maintaining naturalness in edited speech), or DeepFilterNet5 for aggressive suppression of noise in low-quality inputs. Long-Form Processing. Processing extended recordings in single pass often degrades attention performance. Our system automatically segments long audio files into manageable chunks, processes the edits independently, and stitches the results back together using smooth cross-fades at zero-crossing points. This ensures perceptual continuity across the entire recording. Unified Interface. The system exposes two distinct operational modes within single interface: TTS Mode, which generates speech autoregressively starting from the end of the sentence backwards using reference text and audio, and Editing Mode, which modifies existing audio. The latter leverages the LEMAS-Datasets precise alignment capabilities to allow granular, human-in-the-loop adjustments of start and end times, bridging the gap between automated models and professional editing requirements. 6. Experiment We construct unified multilingual benchmark based on the LEMAS-Dataset to evaluate both text-tospeech synthesis and text-based speech editing. The benchmark targets pronunciation stability, cross-lingual generalization, and word-level editing naturalness, leveraging the datasets fine-grained word-level timestamps. For multilingual TTS evaluation, we randomly sample 10 utterances per language across 10 languages and synthesize speech using all target texts, yielding 10,000 samples that cover both intraand cross-lingual settings. Objective evaluation is conducted using Word Error Rate (WER) and speaker similarity. For speech editing, we construct word-level editing cases by randomly replacing consecutive words in aligned utterances across 7 languages and evaluate naturalness via subjective A/B tests, focusing on boundary smoothness and perceptual coherence after localized edits. All evaluations follow unified data construction and inference protocol to ensure fair comparison across models and tasks. Evaluation lists and generation scripts will be released for reproducibility."
        },
        {
            "title": "6.1 Evaluation of LEMAS-TTS",
            "content": "We evaluate LEMAS-TTS on 10 languages and compare it with the contemporaneous open-source baseline OpenAudio-S1-mini6, which supports the same language set. Word error rate (WER) is evaluated using FunASR Paraformer-zh7 for Chinese and Whisper-large-v38 for all other languages, while speaker similarity (SIM) is computed with WavLM-large9, following the Seed-TTS evaluation protocol [1]. Due to the lack of native evaluators for languages other than Chinese and English, no subjective evaluation was conducted. 4https://ultimatevocalremover.com 5https://github.com/Rikorose/DeepFilterNet 6https://huggingface.co/spaces/fishaudio/openaudio-s1-mini 7https://huggingface.co/funasr/paraformer-zh 8https://huggingface.co/openai/whisper-large-v3 9https://huggingface.co/microsoft/wavlm-large 11 Table 4 - Objective evaluation of TTS models on multilingual test sets. WER (Word Error Rate) measures transcription accuracy (lower is better), while SIM (Similarity) evaluates perceptual closeness to the target speaker (higher is better). Results compare OpenAudio-S1-Mini and LEMAS-TTS models, with and without prosody control, across 10 languages. Language de en es fr id it pt ru vi zh Avg Model OpenAudio-S1-Mini LEMAS-TTS w/o prosody LEMAS-TTS w/ prosody OpenAudio-S1-Mini LEMAS-TTS w/o prosody LEMAS-TTS w/ prosody OpenAudio-S1-Mini LEMAS-TTS w/o prosody LEMAS-TTS w/ prosody OpenAudio-S1-Mini LEMAS-TTS w/o prosody LEMAS-TTS w/ prosody OpenAudio-S1-Mini LEMAS-TTS w/o prosody LEMAS-TTS w/ prosody OpenAudio-S1-Mini LEMAS-TTS w/o prosody LEMAS-TTS w/ prosody OpenAudio-S1-Mini LEMAS-TTS w/o prosody LEMAS-TTS w/ prosody OpenAudio-S1-Mini LEMAS-TTS w/o prosody LEMAS-TTS w/ prosody OpenAudio-S1-Mini LEMAS-TTS w/o prosody LEMAS-TTS w/ prosody OpenAudio-S1-Mini LEMAS-TTS w/o prosody LEMAS-TTS w/ prosody OpenAudio-S1-Mini LEMAS-TTS w/o prosody LEMAS-TTS w/ prosody WER(%) SIM 0.448 0.533 0.535 0.451 0.557 0.556 0.488 0.560 0.552 0.458 0.517 0.518 0.518 0.550 0.534 0.489 0.544 0.537 0.476 0.555 0.546 0.475 0.545 0.538 0.451 0.541 0.531 0.550 0.570 0.546 0.480 0.547 0. 3.95 1.20 0.44 4.98 2.21 1.07 7.66 4.93 4.28 13.41 10.45 9.87 32.77 7.05 6.38 9.27 5.57 4.43 8.05 3.12 2.09 20.75 12.29 11.84 - 22.14 15.38 9.59 4.64 3.00 12.27 8.06 6.39 As shown in Table 4, LEMAS-TTS consistently outperforms OpenAudio-S1-mini across all languages, achieving lower WER and higher speaker similarity. We further compare two variants of LEMAS-TTS, with and without prosody encoder. The prosody-aware model consistently yields lower WER, indicating improved pronunciation stability, while slightly reducing speaker similarity. Subjective listening suggests that prosodic conditioning trades expressive richness for increased articulation stability. We therefore release both variants to accommodate different application preferences. For Vietnamese, the WER of OpenAudio-S1-mini remains unusually high across repeated measurements; we therefore exclude this result from the average WER calculation to avoid disproportionate influence on the aggregated results."
        },
        {
            "title": "6.2 Evaluation of LEMAS-Edit",
            "content": "VoiceCraft [28] was originally trained exclusively on the English GigaSpeech corpus. In contrast, LEMAS-Edit is trained on curated subsets from the LEMAS-Dataset, WenetSpeech4TTS, GigaSpeech, and MLS. The resulting combined training corpus covers seven major languages (Zh, En, De, Fr, Pt, Es, It), enabling the 12 model to learn cross-lingual phonotactics and prosody transfer. To efficiently adapt to this multilingual setting, we adopt warm-start initialization strategy by initializing LEMAS-Edit from the pre-trained 330M-parameter VoiceCraft checkpoint. This design allows LEMAS-Edit to extend VoiceCraft to multilingual speech editing without altering the model architecture. Figure 4 - Preference distribution of audio naturalness across different languages. The ridgeline plot illustrates the subjective results of the A/B preference test comparing LEMAS-TTS (Model A) and LEMAS-Edit (Model B). Individual scores are normalized to scale of 0100, where 0 represents strong preference for Model and 100 indicates strong preference for Model B. The vertical distribution (Kernel Density Estimation) for each language reveals the consensus among users, with the AVERAGE row representing the aggregated performance across all tested languages. We further evaluate LEMAS-TTS and LEMAS-Edit on the task of text-based speech editing. Twenty utterances are randomly selected from the LEMAS-Dataset evaluation set, spanning seven languages (23 utterances per language). For each utterance, precise word-level alignments are provided, and editing cases are generated by randomly replacing single word or phrase using ChatGPT. Six human listeners perform an A/B preference test assessing the naturalness of the edited audio (Figure 4). Although minor language-specific preferences are observed, overall listener judgments are balanced, demonstrating that LEMAS provides robust, architecture-agnostic benchmark for multilingual speech editing evaluation. All evaluation samples are publicly accessible on the demo page. 7. Conclusion In this work, we introduce LEMAS, 150K-hour Large-scale Extensible Multilingual Audio Suite with generative speech models, providing unified and extensible foundation for large-scale multilingual generative speech research. With over 150,000 hours of audio across 10 languages, accompanied by high-quality word-level timestamps and confidence scores, LEMAS-Dataset provides foundation for training and evaluating nextgeneration speech generative models. We demonstrated the value of LEMAS-Dataset from two complementary perspectives. Through LEMAS-TTS, we showed that carefully designed alignment and disentanglement objectives are essential for stabilizing large-scale multilingual flow-based models. Through LEMAS-Edit, we validated that precise temporal supervision is key enabler for robust zero-shot speech editing. Together, these results highlight that progress in generative speech modeling is increasingly driven not only by model architecture, but by the scale, diversity, and granularity of the underlying data. We hope that LEMAS will serve as shared benchmark and practical resource for the community, catalyzing further research in multilingual speech synthesis, editing, and controllable audio generation."
        },
        {
            "title": "Acknowledgments",
            "content": "We would like to sincerely thank the following individuals for their invaluable help and insightful discussions. The names are listed in alphabetical order by last name: Xie Chen, Chen Cheng, Zhen Guo, Jiaxin Huang, Rui Li, Fei Peng, Daniel van Strien, Tiezhen Wang, Baogang Yao, Fei Yu, Xiaoling Zhang, Changyin Zhou."
        },
        {
            "title": "References",
            "content": "[1] Philip Anastassiou, Jiawei Chen, Jitong Chen, Yuanzhe Chen, Zhuo Chen, Ziyi Chen, Jian Cong, Lelai Deng, Chuang Ding, Lu Gao, Mingqing Gong, Peisong Huang, Qingqing Huang, Zhiying Huang, Yuanyuan Huo, Dongya Jia, Chumin Li, Feiya Li, Hui Li, Jiaxin Li, Xiaoyang Li, Xingxing Li, Lin Liu, Shouda Liu, Sichao Liu, Xudong Liu, Yuchen Liu, Zhengxi Liu, Lu Lu, Junjie Pan, Xin Wang, Yuping Wang, Yuxuan Wang, Zhengnan Wei, Jian Wu, Chao Yao, Yifeng Yang, Yuan-Qiu-Qiang Yi, Junteng Zhang, Qidi Zhang, Shuo Zhang, WenJie Zhang, Yang Zhang, Zilin Zhao, Dejian Zhong, and Xiaobin Zhuang. Seed-tts: family of high-quality versatile speech generation models. arXiv preprint arXiv:2406.02430, 2024. [2] Rosana Ardila, Megan Branson, Kelly Davis, Michael Henretty, Michael Kohler, Josh Meyer, Reuben Morais, Lindsay Saunders, Francis Tyers, and Gregor Weber. Common voice: massively-multilingual speech corpus. In Proceedings of the 12th Language Resources and Evaluation Conference, 2020. [3] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli. wav2vec 2.0: framework for selfsupervised learning of speech representations. In Advances in Neural Information Processing Systems, pages 1244912460, 2020. [4] Zalan Borsos, Raphael Marinier, Damien Vincent, Eugene Kharitonov, Olivier Pietquin, Matthew Sharifi, Dominik Roblek, Olivier Teboul, David Grangier, Marco Tagliasacchi, and Neil Zeghidour. Audiolm: language modeling approach to audio generation. IEEE ACM Trans. Audio Speech Lang. Process., 31:25232533, 2023. [5] Edresson Casanova, Kelly Davis, Eren Golge, Gorkem Goknar, Iulian Gulea, Logan Hart, Aya Aljafari, Joshua Meyer, Reuben Morais, Samuel Olayemi, and Julian Weber. Xtts: massively multilingual zero-shot text-to-speech model. arXiv preprint arXiv:2406.04904, 2024. [6] CETUC. CETUC Alcaim: European Portuguese speech dataset. URL http://www02.smt.ufrj.br/igor. quintanilha/alcaim.tar.gz. Accessed: 2025-12-28. [7] Guoguo Chen, Shuzhou Chai, Guanbo Wang, Jiayu Du, Wei-Qiang Zhang, Chao Weng, Dan Su, Daniel Povey, Jan Trmal, Junbo Zhang, et al. Gigaspeech: An evolving, multi-domain asr corpus with 10,000 hours of transcribed audio. arXiv preprint arXiv:2106.06909, 2021. [8] Sanyuan Chen, Chengyi Wang, Yu Wu, Ziqiang Zhang, Long Zhou, Shujie Liu, Zhuo Chen, Yanqing Liu, Huaming Wang, Jinyu Li, et al. Neural codec language models are zero-shot text to speech synthesizers. IEEE Transactions on Audio, Speech and Language Processing, 2025. [9] Yushen Chen, Zhikang Niu, Ziyang Ma, Keqi Deng, Chunhui Wang, Jian Zhao, Kai Yu, and Xie Chen. F5-tts: fairytaler that fakes fluent and faithful speech with flow matching. arXiv preprint arXiv:2410.06885, 2024. [10] Zhihao Du, Qian Chen, Shiliang Zhang, Kai Hu, Heng Lu, Yexin Yang, Hangrui Hu, Siqi Zheng, Yue Gu, Ziyang Ma, et al. Cosyvoice: scalable multilingual zero-shot text-to-speech synthesizer based on supervised semantic tokens. arXiv preprint arXiv:2407.05407, 2024. [11] Zhihao Du, Qian Chen, Shiliang Zhang, Kai Hu, Heng Lu, Yexin Yang, Hangrui Hu, Siqi Zheng, Yue Gu, Ziyang Ma, et al. Cosyvoice: scalable multilingual zero-shot text-to-speech synthesizer based on supervised semantic tokens. arXiv preprint arXiv:2407.05407, 2024. [12] Zhihao Du, Yuxuan Wang, Qian Chen, Xian Shi, Xiang Lv, Tianyu Zhao, Zhifu Gao, Yexin Yang, Changfeng Gao, Hui Wang, et al. Cosyvoice 2: Scalable streaming speech synthesis with large language models. arXiv preprint arXiv:2412.10117, 2024. [13] Zhihao Du, Changfeng Gao, Yuxuan Wang, Fan Yu, Tianyu Zhao, Hao Wang, Xiang Lv, Hui Wang, Chongjia Ni, Xian Shi, Keyu An, Guanrou Yang, Yabin Li, Yanni Chen, Zhifu Gao, Qian Chen, Yue Gu, Mengzhe Chen, Yafeng Chen, Shi-Min Zhang, Wen Wang, and Jieping Ye. Cosyvoice 3: Towards in-the-wild speech generation via scaling-up and post-training. arXiv preprint arXiv:2505.17589, 2025. 14 [14] eSpeak NG. espeak ng (version 1.51). https://github.com/espeak-ng/espeak-ng/tree/1.51, 2022. Accessed: 2025-12-25. [15] Mark JF Gales, Shakti Rath, et al. Speech recognition and keyword spotting for low-resource languages: Babel project research at cued. Spoken Language Technologies for Under-Resourced Languages, 2014. [16] Yaroslav Ganin and Victor Lempitsky. Unsupervised domain adaptation by backpropagation. In International conference on machine learning, pages 11801189. PMLR, 2015. [17] Alex Graves, Santiago Fernandez, Faustino Gomez, and Jurgen Schmidhuber. Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks. In Proceedings of the 23rd international conference on Machine learning, pages 369376, 2006. [18] Hao-Han Guo, Yao Hu, Kun Liu, Fei-Yu Shen, Xu Tang, Yi-Chen Wu, Feng-Long Xie, Kun Xie, and Kai-Tuo Xu. Fireredtts: foundation text-to-speech framework for industry-level generative speech applications. arXiv preprint arXiv:2409.03283, 2024. [19] Haorui He, Zengqiang Shang, Chaoren Wang, Xuyuan Li, Yicheng Gu, Hua Hua, Liwei Liu, Chen Yang, Jiaqi Li, Peiyang Shi, et al. Emilia: An extensive, multilingual, and diverse speech dataset for large-scale speech generation. In 2024 IEEE Spoken Language Technology Workshop (SLT), pages 885890. IEEE, 2024. [20] Jacob Kahn, Morgane Riviere, Weiyi Zheng, Eugene Kharitonov, Qiantong Xu, Pierre-Emmanuel Mazare, Julien Karadayi, Vitaliy Likhomanenko, Vineel Pratap, Awni Kahn, et al. Libri-light: benchmark for asr with limited or no supervision. In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 76697673, 2020. [21] Nikolay Karpov, Alexander Denisenko, and Fedor Minkin. Golos: Russian dataset for speech research. arXiv preprint arXiv:2106.10161, 2021. [22] Jaehyeon Kim, Jungil Kong, and Juhee Son. Conditional variational autoencoder with adversarial learning for end-to-end text-to-speech. In International Conference on Machine Learning, pages 55305540. PMLR, 2021. [23] Matthew Le, Apoorv Vyas, Bowen Shi, Brian Karrer, Leda Sari, Rashel Moritz, Mary Williamson, Vimal Manohar, Yossi Adi, Jay Mahadeokar, et al. Voicebox: Text-guided multilingual universal speech generation at scale. Advances in neural information processing systems, 36:1400514034, 2023. [24] Xinjian Li, Shinnosuke Takamichi, Takaaki Saeki, William Chen, Sayaka Shiota, and Shinji Watanabe. Yodas: Youtube-oriented dataset for audio and speech. In 2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), pages 18. IEEE, 2023. [25] Linhan Ma, Dake Guo, Kun Song, Yuepeng Jiang, Shuai Wang, Liumeng Xue, Weiming Xu, Huan Zhao, Binbin Zhang, and Lei Xie. Wenetspeech4tts: 12,800-hour mandarin tts corpus for large speech generation model benchmark. arXiv preprint arXiv:2406.05763, 2024. [26] M. McAuliffe, M. Socolof, S. Mihuc, M. Wagner, and M. Sonderegger. Montreal forced aligner: Trainable text-speech alignment using kaldi. In Proc. Interspeech 2017, pages 498502, 2017. [27] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. Librispeech: An asr corpus based on public domain audio books. In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2015. [28] Puyuan Peng, Po-Yao Huang, Shang-Wen Li, Abdelrahman Mohamed, and David Harwath. Voicecraft: Zero-shot speech editing and text-to-speech in the wild. arXiv preprint arXiv:2403.16973, 2024. [29] Vineel Pratap, Qiantong Xu, Anuroop Sriram, Gabriel Synnaeve, and Ronan Collobert. Mls: large-scale multilingual dataset for speech research. In Interspeech, 2020. [30] Vineel Pratap, Andros Tjandra, Bowen Shi, Paden Tomasello, Arun Babu, Sravya Kundu, Ali Elkahky, Zhaoheng Ni, Apoorv Vyas, et al. Scaling speech technology to 1,000+ languages. Journal of Machine Learning Research, 24(368), 2023. [31] Yi Ren, Yangjun Ruan, Xu Tan, Tao Qin, Sheng Zhao, Zhou Zhao, and Tie-Yan Liu. Fastspeech: Fast, robust and controllable text to speech. volume 32, 2019. [32] Elizabeth Salesky, Matthew Wiesner, Jacob Bremerman, Roldano Cattoni, Matteo Negri, Marco Turchi, Douglas W. Oard, and Matt Post. The multilingual tedx corpus for speech recognition and translation. arXiv preprint arXiv:2102.01757, 2021. 15 [33] Tanja Schultz and Tim Schlippe. Globalphone: multilingual text & speech database in 20 languages. In IEEE International Conference on Acoustics, Speech and Signal Processing, 2013. [34] Seamless Communication, Loıc Barrault, Yu-An Chung, Mariano Cora Meglioli, David Dale, Ning Dong, PaulAmbroise Duquenne, Hady Elsahar, Hongyu Gong, Kevin Heffernan, John Hoffman, Christopher Klaiber, Pengwei Li, Daniel Licht, Jean Maillard, Alice Rakotoarison, Kaushik Ram Sadagopan, Guillaume Wenzek, Ethan Ye, Bapi Akula, Peng-Jen Chen, Naji El Hachem, Brian Ellis, Gabriel Mejia Gonzalez, Justin Haaheim, Prangthip Hansanti, Russ Howes, Bernie Huang, Min-Jae Hwang, Hirofumi Inaguma, Somya Jain, Elahe Kalbassi, Amanda Kallet, Ilia Kulikov, Janice Lam, Daniel Li, Xutai Ma, Ruslan Mavlyutov, Benjamin Peloquin, Mohamed Ramadan, Abinesh Ramakrishnan, Anna Sun, Kevin Tran, Tuan Tran, Igor Tufanov, Vish Vogeti, Carleigh Wood, Yilin Yang, Bokai Yu, Pierre Andrews, Can Balioglu, Marta R. Costa-juss`a, Onur Celebi, Maha Elbayad, Cynthia Gao, Francisco Guzman, Justine Kao, Ann Lee, Alexandre Mourachko, Juan Pino, Sravya Popuri, Christophe Ropers, Safiyyah Saleem, Holger Schwenk, Paden Tomasello, Changhan Wang, Jeff Wang, and Skyler Wang. Seamlessm4t-massively multilingual & multimodal machine translation, 2023. [35] Kai Shen, Zeqian Ju, Xu Tan, Eric Liu, Yichong Leng, Lei He, Tao Qin, Sheng Zhao, and Jiang Bian. Naturalspeech 2: Latent diffusion models are natural and zero-shot speech and singing synthesizers. In The Twelfth International Conference on Learning Representations, 2024. [36] Fish Audio Team. Fish speech: Openaudio s1 mini. https://huggingface.co/fishaudio/openaudio-s1-mini, 2024. [37] Changhan Wang, Morgane Riviere, Ann Lee, Anne Wu, Chaitanya Talwalkar, Elizabeth Xiao, Yatharth Saraf, Piyush Puvvada, and Emmanuel Dupoux. Voxpopuli: large-scale multilingual speech corpus for representation learning, semi-supervised learning and interpretation. In Association for Computational Linguistics (ACL), 2021. [38] Xinsheng Wang, Mingqi Jiang, Ziyang Ma, Ziyu Zhang, Songxiang Liu, Linqin Li, Zheng Liang, Qixi Zheng, Rui Wang, Xiaoqin Feng, et al. Spark-tts: An efficient llm-based text-to-speech model with single-stream decoupled speech tokens. arXiv preprint arXiv:2503.01710, 2025. [39] Yuancheng Wang, Haoyue Zhan, Liwei Liu, Ruihong Zeng, Haotian Guo, Jiachen Zheng, Qiang Zhang, Xueyao Zhang, Shunsi Zhang, and Zhizheng Wu. Maskgct: Zero-shot text-to-speech with masked generative codec transformer. arXiv preprint arXiv:2409.00750, 2024. [40] Yuxuan Wang, RJ Skerry-Ryan, Daisy Stanton, Yonghui Wu, Ron Weiss, Navdeep Jaitly, Zongheng Yang, Ying Xiao, Zhifeng Chen, Samy Bengio, et al. Tacotron: Towards end-to-end speech synthesis. 2017. [41] Kun Xie, Feiyu Shen, Junjie Li, Fenglong Xie, Xu Tang, and Yao Hu. Fireredtts-2: Towards long conversational speech generation for podcast and chatbot. arXiv preprint arXiv:2509.02020, 2025. [42] Yifan Yang, Zheshu Song, Jianheng Zhuo, Mingyu Cui, Jinpeng Li, Bo Yang, Yexing Du, Ziyang Ma, Xunying Liu, Ziyuan Wang, et al. Gigaspeech 2: An evolving, large-scale and multi-domain asr corpus for low-resource languages with automated crawling, transcription and refinement. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 26732686, 2025. [43] Binbin Zhang, Hang Lv, Pengcheng Guo, Qijie Shao, Chao Yang, Lei Xie, Xin Xu, Hui Bu, Xiaoyu Chen, Chenchen Zeng, et al. Wenetspeech: 10000+ hours multi-domain mandarin corpus for speech recognition. In ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 61826186. IEEE, 2022. [44] Siyi Zhou, Yiquan Zhou, Yi He, Xun Zhou, Jinchao Wang, Wei Deng, and Jingchen Shu. Indextts2: breakthrough in emotionally expressive and duration-controlled auto-regressive zero-shot text-to-speech. arXiv preprint arXiv:2506.21619, 2025."
        }
    ],
    "affiliations": [
        "International Digital Economy Academy (IDEA)"
    ]
}