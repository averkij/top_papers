{
    "paper_title": "mHC: Manifold-Constrained Hyper-Connections",
    "authors": [
        "Zhenda Xie",
        "Yixuan Wei",
        "Huanqi Cao",
        "Chenggang Zhao",
        "Chengqi Deng",
        "Jiashi Li",
        "Damai Dai",
        "Huazuo Gao",
        "Jiang Chang",
        "Liang Zhao",
        "Shangyan Zhou",
        "Zhean Xu",
        "Zhengyan Zhang",
        "Wangding Zeng",
        "Shengding Hu",
        "Yuqing Wang",
        "Jingyang Yuan",
        "Lean Wang",
        "Wenfeng Liang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recently, studies exemplified by Hyper-Connections (HC) have extended the ubiquitous residual connection paradigm established over the past decade by expanding the residual stream width and diversifying connectivity patterns. While yielding substantial performance gains, this diversification fundamentally compromises the identity mapping property intrinsic to the residual connection, which causes severe training instability and restricted scalability, and additionally incurs notable memory access overhead. To address these challenges, we propose Manifold-Constrained Hyper-Connections (mHC), a general framework that projects the residual connection space of HC onto a specific manifold to restore the identity mapping property, while incorporating rigorous infrastructure optimization to ensure efficiency. Empirical experiments demonstrate that mHC is effective for training at scale, offering tangible performance improvements and superior scalability. We anticipate that mHC, as a flexible and practical extension of HC, will contribute to a deeper understanding of topological architecture design and suggest promising directions for the evolution of foundational models."
        },
        {
            "title": "Start",
            "content": "mHC: Manifold-Constrained Hyper-Connections Zhenda Xie*, Yixuan Wei*, Huanqi Cao*, Chenggang Zhao, Chengqi Deng, Jiashi Li, Damai Dai, Huazuo Gao, Jiang Chang, Liang Zhao, Shangyan Zhou, Zhean Xu, Zhengyan Zhang, Wangding Zeng, Shengding Hu, Yuqing Wang, Jingyang Yuan, Lean Wang, Wenfeng Liang DeepSeek-AI"
        },
        {
            "title": "Abstract",
            "content": "Recently, studies exemplified by Hyper-Connections (HC) have extended the ubiquitous residual connection paradigm established over the past decade by expanding the residual stream width and diversifying connectivity patterns. While yielding substantial performance gains, this diversification fundamentally compromises the identity mapping property intrinsic to the residual connection, which causes severe training instability and restricted scalability, and additionally incurs notable memory access overhead. To address these challenges, we propose Manifold-Constrained Hyper-Connections (mHC), general framework that projects the residual connection space of HC onto specific manifold to restore the identity mapping property, while incorporating rigorous infrastructure optimization to ensure efficiency. Empirical experiments demonstrate that mHC is effective for training at scale, offering tangible performance improvements and superior scalability. We anticipate that mHC, as flexible and practical extension of HC, will contribute to deeper understanding of topological architecture design and suggest promising directions for the evolution of foundational models. 5 2 0 2 1 3 ] . [ 1 0 8 8 4 2 . 2 1 5 2 : r Figure 1 Illustrations of Residual Connection Paradigms. This figure compares the structural design of (a) standard Residual Connection, (b) Hyper-Connections (HC), and (c) our proposed Manifold-Constrained Hyper-Connections (mHC). Unlike the unconstrained HC, mHC focuses on optimizing the residual connection space by projecting the matrices onto constrained manifold to ensure stability. *Core contributors. Corresponding author: xie.zhenda@deepseek.com"
        },
        {
            "title": "Contents",
            "content": "1 Introduction 2 Related Works"
        },
        {
            "title": "2.2 Macro Design .",
            "content": ". . . . . . . . 3 Preliminary"
        },
        {
            "title": "3.1 Numerical Instability .",
            "content": "3.2 System Overhead . . . . . . . . . . . 4 Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "4.1 Manifold-Constrained Hyper-Connections",
            "content": ". . . . . . . . . . . . . . . . . . . . . . 4.2 Parameterization and Manifold Projection . . . . . . . . . . . . . . . . . . . . . . . 4.3 Efficient Infrastructure Design . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.3.1 Kernel Fusion . 4.3.2 Recomputing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.3.3 Overlapping Communication in DualPipe . . . . . . . . . . . . . . . . . . 5 Experiments 5.1 Experimental Setup . 5.2 Main Results . . . . . 5.3 5.4 Scaling Experiments Stability Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 Conclusion and Outlook Appendix A.1 Detailed Model Specifications and Hyper-parameters. . . . . . . . . . . . . . . . . 3 4 4 5 6 7 8 8 9 9 10 11 12 12 13 14 15 19 19 1. Introduction Deep neural network architectures have undergone rapid evolution since the introduction of ResNets (He et al., 2016a). As illustrated in Fig. 1(a), the structure of single-layer can be formulated as follows: x洧녳+1 = x洧녳 + (x洧녳, W洧녳), (1) where x洧녳 and x洧녳+1 denote the 洧냤-dimensional input and output of the 洧녳-th layer, respectively, and represents the residual function. Although the residual function has evolved over the past decade to include various operations such as convolution, attention mechanisms, and feed forward networks, the paradigm of the residual connection has maintained its original form. Accompanying the progression of Transformer (Vaswani et al., 2017) architecture, this paradigm has currently established itself as fundamental design element in large language models (LLMs) (Brown et al., 2020; Liu et al., 2024b; Touvron et al., 2023). This success is primarily attributed to the concise form of the residual connection. More importantly, early research (He et al., 2016b) revealed that the identity mapping property of the residual connection maintains stability and efficiency during large-scale training. By recursively extending the residual connection across multiple layers, Eq. (1) yields: x洧 = x洧녳 + 洧1 洧녰=洧녳 (x洧녰, W洧녰), (2) where 洧 and 洧녳 correspond to deeper and shallower layers, respectively. The term identity mapping refers to the component x洧녳 itself, which emphasizes the property that the signal from the shallower layer maps directly to the deeper layer without any modification. Recently, studies exemplified by Hyper-Connections (HC) (Zhu et al., 2024) have introduced new dimension to the residual connection and empirically demonstrated its performance potential. The single-layer architecture of HC is illustrated in Fig. 1(b). By expanding the width of the residual stream and enhancing connection complexity, HC significantly increases topological complexity without altering the computational overhead of individual units regarding FLOPs. Formally, single-layer propagation in HC is defined as: x洧녳+1 = res 洧녳 x洧녳 + post 洧녳 (H pre 洧녳 x洧녳, W洧녳), (3) where x洧녳 and x洧녳+1 denote the input and output of the 洧녳-th layer, respectively. Unlike the formulation in Eq. (1), the feature dimension of x洧녳 and x洧녳+1 is expanded from 洧냤 to 洧녵 洧냤, where 洧녵 is R洧녵洧녵 represents learnable mapping that mixes features the expansion rate. The term res within the residual stream. Also as learnable mapping, pre R1洧녵 aggregates features from the 洧녵洧냤-dim stream into 洧냤-dim layer input, and conversely, post R1洧녵 maps the layer output back onto the stream. 洧녳 洧녳 洧녳 However, as the training scale increases, HC introduces potential risks of instability. The primary concern is that the unconstrained nature of HC compromises the identity mapping property when the architecture extends across multiple layers. In architectures comprising multiple parallel streams, an ideal identity mapping serves as conservation mechanism. It ensures that the average signal intensity across streams remains invariant during both forward and backward propagation. Recursively extending HC to multiple layers via Eq. (3) yields: x洧 = (cid:33) res 洧洧녰 x洧녳 + (cid:32) 洧洧녳 (cid:214) 洧녰=1 洧1洧녰 (cid:214) 洧녱= 洧1 洧녰=洧녳 (cid:169) (cid:173) (cid:171) 洧 洧녱(cid:170) res (cid:174) (cid:172) 3 post 洧녰 (H pre 洧녰 x洧녰, W洧녰), (4)"
        },
        {
            "title": "H res",
            "content": "where 洧 and 洧녳 represent deeper layer and shallower layer, respectively. In contrast to Eq. (2), the composite mapping (cid:206)洧洧녳 洧洧녰 in HC fails to preserve the global mean of the features. This 洧녰=1 discrepancy leads to unbounded signal amplification or attenuation, resulting in instability during large-scale training. further consideration is that, while HC preserves computational efficiency in terms of FLOPs, the hardware efficiency concerning memory access costs for the widened residual stream remains unaddressed in the original design. These factors collectively restrict the practical scalability of HC and hinder its application in large-scale training. To address these challenges, we propose Manifold-Constrained Hyper-Connections (mHC), as shown in Fig. 1(c), general framework that projects the residual connection space of HC onto specific manifold to restore the identity mapping property, while incorporating rigorous infrastructure optimization to ensure efficiency. Specifically, mHC utilizes the Sinkhorn-Knopp algorithm (Sinkhorn and Knopp, 1967) to entropically project res onto the Birkhoff polytope. This operation effectively constrains the residual connection matrices within the manifold that is constituted by doubly stochastic matrices. Since the row and column sums of these matrices equal to 1, the operation res 洧녳 x洧녳 functions as convex combination of the input features. This characteristic facilitates well-conditioned signal propagation where the feature mean is conserved, and the signal norm is strictly regularized, effectively mitigating the risk of vanishing or exploding signals. Furthermore, due to the closure of matrix multiplication for doubly stochastic matrices, the composite mapping (cid:206)洧洧녳 洧洧녰 retains this conservation property. 洧녰=1 Consequently, mHC effectively maintains the stability of identity mappings between arbitrary depths. To ensure efficiency, we employ kernel fusion and develop mixed precision kernels utilizing TileLang (Wang et al., 2025). Furthermore, we mitigate the memory footprint through selective recomputing and carefully overlap communication within the DualPipe schedule (Liu et al., 2024b). res 洧녳 Extensive experiments on language model pretraining demonstrate that mHC exhibits exceptional stability and scalability while maintaining the performance advantages of HC. Inhouse large-scale training indicates that mHC supports training at scale and introduces only 6.7% additional time overhead when expansion rate 洧녵 = 4. 2. Related Works Architectural advancements in deep learning can be primarily classified into micro-design and macro-design. Micro-design concerns the internal architecture of computational blocks, specifying how features are processed across spatial, temporal, and channel dimensions. In contrast, macro-design establishes the inter-block topological structure, thereby dictating how feature representations are propagated, routed, and merged across distinct layers. 2.1. Micro Design Driven by parameter sharing and translation invariance, convolution initially dominated the processing of structured signals. While subsequent variations such as depthwise separable (Chollet, 2017) and grouped convolutions (Xie et al., 2017) optimized efficiency, the advent of Transformers (Vaswani et al., 2017) established Attention and Feed-Forward Networks (FFNs) as the fundamental building blocks of modern architecture. Attention mechanisms facilitate global information propagation, while FFNs enhance the representational capacity of individual features. To balance performance with the computational demands of LLMs, attention mechanisms have evolved towards efficient variants such as Multi-Query Attention (MQA) (Shazeer, 2019), Grouped-Query Attention (GQA) (Ainslie et al., 2023), and Multi-Head Latent Attention 4 (MLA) (Liu et al., 2024a). Simultaneously, FFNs have been generalized into sparse computing paradigms via Mixture-of-Experts (MoE) (Fedus et al., 2022; Lepikhin et al., 2020; Shazeer et al., 2017), allowing for massive parameter scaling without proportional computational costs. 2.2. Macro Design Macro-design governs the global topology of the network (Srivastava et al., 2015). Following ResNet (He et al., 2016a), architectures such as DenseNet (Huang et al., 2017) and FractalNet (Larsson et al., 2016) aimed to enhance performance by increasing topological complexity through dense connectivity and multi-path structures, respectively. Deep Layer Aggregation (DLA) (Yu et al., 2018) further extended this paradigm by recursively aggregating features across various depths and resolutions. More recently, the focus of macro-design has shifted toward expanding the width of the residual stream (Chai et al., 2020; Fang et al., 2023; Heddes et al., 2025; Mak and Flanigan, 2025; Menghani et al., 2025; Pagliardini et al., 2024; Xiao et al., 2025; Xie et al., 2023; Zhu et al., 2024). Hyper-Connections (HC) (Zhu et al., 2024) introduced learnable matrices to modulate connection strengths among features at varying depths, while the Residual Matrix Transformer (RMT) (Mak and Flanigan, 2025) replaced the standard residual stream with an outer-product memory matrix to facilitate feature storage. Similarly, MUDDFormer (Xiao et al., 2025) employs multiway dynamic dense connections to optimize cross-layer information flow. Despite their potential, these approaches compromise the inherent identity mapping property of the residual connection, thereby introducing instability and hindering scalability. Furthermore, they incur significant memory access overhead due to expanded feature widths. Building upon HC, the proposed mHC restricts the residual connection space onto specific manifold to restore the identity mapping property, while also incorporating rigorous infrastructure optimizations to ensure efficiency. This approach enhances stability and scalability while maintaining the topological benefits of expanded connections. 3. Preliminary We first establish the notation used in this work. In the HC formulation, the input to the 洧녳-th layer, ) R洧녵洧냤 x洧녳 R1洧냤, is expanded by factor of 洧녵 to construct hidden matrix x洧녳 = (x which can be viewed as 洧녵-stream residual. This operation effectively broadens the width of the residual stream. To govern the read-out, write-in, and updating processes of this stream, HC introduces three learnable linear mappingsH pre R洧녵洧녵. These mappings modify the standard residual connection shown in Eq. (1), resulting in the formulation given in Eq. (3). R1洧녵, and res 洧녳,0, . . . , , post 洧녳,洧녵1 洧녳 洧녳 洧녳 In the HC formulation, learnable mappings are composed of two parts of coefficients: the input-dependent one and the global one, referred to as dynamic mappings and static mappings, respectively. Formally, HC computes the coefficients as follows: x洧녳 = RMSNorm(x洧녳) = 洧띺pre pre 洧녳 洧녳 post = 洧띺post 洧녳 洧녳 洧녳 = 洧띺res res 洧녳 ) + bpre tanh(洧랚pre 洧녳 ) + bpost tanh(洧랚post 洧녳 tanh(洧랚res 洧녳 ) + bres , 洧녳 洧녳 洧녳 洧녳 洧녳 洧녳 (5) where RMSNorm() (Zhang and Sennrich, 2019) is applied to the last dimension, and the scalars 洧띺pre are learnable gating factors initialized to small values. The dynamic 洧녳 and 洧띺res , 洧띺post 洧녳 洧녳 5 mappings are derived via linear projections parameterized by 洧랚pre while the static mappings are represented by learnable biases bpre 洧녳 洧녳 洧녳 , 洧랚post , bpost 洧녳 R1洧냤 and 洧랚res R1洧녵 and bres 洧녳 洧녳 R洧녵洧냤, R洧녵洧녵. It is worth noting that the introduction of these mappingsH pre 洧녳 incurs negligible computational overhead, as the typical expansion rate 洧녵, e.g. 4, is much smaller than the input dimension 洧냤. With this design, HC effectively decouples the information capacity of the residual stream from the layers input dimension, which is strongly correlated with the models computational complexity (FLOPs). Consequently, HC offers new avenue for scaling by adjusting the residual stream width, complementing the traditional scaling dimensions of model FLOPs and training data size discussed in pre-training scaling laws (Hoffmann et al., 2022). , and res , post 洧녳 洧녳 Although HC necessitates three mappings to manage the dimensional mismatch between the residual stream and the layer input, preliminary experiments presented in Tab. 1 indicate that the residual mapping res yields the most significant performance gain. This finding underscores the critical importance of effective information exchange within the residual stream. 洧녳 Table 1 Ablation Study of HC Components. When specific mapping (H pre ) is disabled, we employ fixed mapping to maintain dimensional consistency: uniform weights of 1/洧녵 for pre , uniform weights of ones for post , and the identity matrix for res , or res , post . 洧녳 洧녳 洧녳 洧녳 洧녳 洧녳 res 洧녳 pre 洧녳 post 洧녳 Absolute Loss Gap 0.0 0.022 0.025 0.027 3.1. Numerical Instability 洧녳 While the residual mapping res is instrumental for performance, its sequential application poses significant risk to numerical stability. As detailed in Eq. (4), when HC is extended across multiple layers, the effective signal propagation from layer 洧녳 to 洧 is governed by the composite mapping (cid:206)洧洧녳 is unconstrained, this composite mapping 洧녰=1 inevitably deviates from the identity mapping. Consequently, the signal magnitude is prone to explosion or vanishing during both the forward pass and backpropagation. This phenomenon undermines the fundamental premise of residual learning, which relies on unimpeded signal flow, thereby destabilizing the training process in deeper or larger-scale models. 洧洧녰. Since the learnable mapping res res 洧녳 Empirical evidence supports this analysis. We observe unstable loss behavior in large-scale experiments, as illustrated in Fig. 2. Taking mHC as the baseline, HC exhibits an unexpected loss surge around the 12k step, which is highly correlated with the instability in the gradient norm. Furthermore, the analysis on res validates the mechanism of this instability. To quantify 洧녳 how the composite mapping (cid:206)洧洧녳 res 洧洧녰 amplifies signals along the residual stream, we utilize 洧녰=1 two metrics. The first, based on the maximum absolute value of the row sums of the composite mapping, captures the worst-case expansion in the forward pass. The second, based on the maximum absolute column sum, corresponds to the backward pass. We refer to these metrics as the Amax Gain Magnitude of the composite mapping. As shown in Fig. 3 (b), the Amax Gain Magnitude yields extreme values with peaks of 3000, stark divergence from 1 that confirms the presence of exploding residual streams. 6 Figure 2 Training Instability of Hyper-Connections (HC). This figure illustrates (a) the absolute loss gap of HC relative to mHC, and (b) the comparisons of gradient norms. All results are based on 27B models. res Figure 3 Propagation Instability of Hyper-Connections (HC). This figure illustrates the propagation dynamics of (a) the single-layer mapping res and (b) the composite mapping (cid:206)洧洧녳 洧洧녰 within the 27B model. The layer index 洧녳 (x-axis) unrolls each standard Transformer 洧녰=1 block into two independent layers (Attention and FFN). The Amax Gain Magnitude (y-axis) is calculated as the maximum absolute row sum (for the forward signal) and column sum (for the backward gradient), averaged over all tokens in selected sequence. 洧녳 3.2. System Overhead While the computational complexity of HC remains manageable due to the linearity of the additional mappings, the system-level overhead prevents non-negligible challenge. Specifically, memory access (I/O) costs often constitute one of the primary bottlenecks in modern model architectures, which is widely referred to as the memory wall (Dao et al., 2022). This bottleneck is frequently overlooked in architectural design, yet it decisively impacts runtime efficiency. Focusing on the widely adopted pre-norm Transformer (Vaswani et al., 2017) architecture, we analyze the I/O patterns inherent to HC. Tab. 2 summarizes the per token memory access overhead in single residual layer introduced by the 洧녵-stream residual design. The analysis reveals that HC increases the memory access cost by factor approximately proportional to 洧녵. This excessive I/O demand significantly degrades training throughput without the mitigation of fused kernels. Besides, since pre involve learnable parameters, their interme洧녳 diate activations are required for backpropagation. This results in substantial increase in the GPU memory footprint, often necessitating gradient checkpointing to maintain feasible memory usage. Furthermore, HC requires 洧녵-fold more communication cost in pipeline parallelism (Qi et al., 2024), leading to larger bubbles and decreasing the training throughput. , and res , post 洧녳 洧녳 7 Table 2 Comparison of Memory Access Costs Per Token. This analysis accounts for the overhead introduced by the residual stream maintenance in the forward pass, excluding the internal I/O of the layer function ."
        },
        {
            "title": "Residual Merge",
            "content": "Total I/O HyperConnections , res 洧녳 洧녳 , post Calculate pre 洧녳 pre 洧녳 post 洧녳 res 洧녳 Residual Merge Read (Elements) 2洧냤 2C 洧녵洧냤 洧녵洧냤 + 洧녵 洧냤 + 洧녵 洧녵洧냤 + 洧녵2 2洧녵洧냤 Write (Elements) 洧냤 洧녵2 + 2洧녵 洧냤 洧녵洧냤 洧녵洧냤 洧녵洧냤 Total I/O (5n + 1)C + n2 + 2n (3n + 1)C + n2 + 2n 4. Method 4.1. Manifold-Constrained Hyper-Connections 洧녳 Drawing inspiration from the identity mapping principle (He et al., 2016b), the core premise of mHC is to constrain the residual mapping res onto specific manifold. While the original identity mapping ensures stability by enforcing res 洧녳 = I, it fundamentally precludes information exchange within the residual stream, which is critical for maximizing the potential of multistream architectures. Therefore, we propose projecting the residual mapping onto manifold that simultaneously maintains the stability of signal propagation across layers and facilitates mutual interaction among residual streams to preserve the models expressivity. To this end, we restrict res to be doubly stochastic matrix, which has non-negative entries where both the rows and columns sum to 1. Formally, let Mres denote the manifold of doubly stochastic matrices (also known as the Birkhoff polytope). We constrain res ), defined as: 洧녳 洧녳 PMres (H res 洧녳 ) (cid:8)H res 洧녳 R洧녵洧녵 res 洧녳 1洧녵 = 1洧녵, 1 洧녵 res 洧녳 = 1 (6) 洧녳 to PMres (H res 洧녳 0(cid:9) , 洧녵 , res where 1洧녵 represents the 洧녵-dimensional vector of all ones. It is worth noting that when 洧녵 = 1, the doubly stochastic condition degenerates to the scalar 1, thereby recovering the original identity mapping. The choice of double stochasticity confers several rigorous theoretical properties beneficial for large-scale model training: 1. Norm Preservation: The spectral norm of doubly stochastic matrix is bounded by 1 2 1). This implies that the learnable mapping is non-expansive, effectively (i.e., res mitigating the gradient explosion problem. 洧녳 2. Compositional Closure: The set of doubly stochastic matrices is closed under matrix multiplication. This ensures that the composite residual mapping across multiple layers, (cid:206)洧洧녳 洧洧녰, remains doubly stochastic, thereby preserving stability throughout the entire 洧녰=1 res depth of the model. 3. Geometric Interpretation via the Birkhoff Polytope: The set Mres forms the Birkhoff polytope, which is the convex hull of the set of permutation matrices. This provides clear geometric interpretation: the residual mapping acts as convex combination of permutations. Mathematically, the repeated application of such matrices tends to increase 8 the mixing of information across streams monotonically, effectively functioning as robust feature fusion mechanism. Additionally, we impose non-negativity constraints on the input mappings pre and output mappings post . This constrain prevents signal cancellation arising from the composition of positive and negative coefficients, which can also be considered as special manifold projection. 洧녳 洧녳 4.2. Parameterization and Manifold Projection In this section, we detail the calculation process of pre in mHC. Given the input hidden matrix x洧녳 R洧녵洧냤 at the 洧녳-th layer, we first flatten it into vector (cid:174)x洧녳 = vec(x洧녳) R1洧녵洧냤 to preserve full context information. Then, we follow the original HC formulation to get the dynamic mappings and the static mappings as follows: , and res , post 洧녳 洧녳 洧녳 (cid:174)x 洧녳 = RMSNorm((cid:174)x洧녳) pre 洧램pre = 洧띺pre 洧녳 洧녳 洧녳 洧램post post = 洧띺post 洧녳 洧녳 洧녳 res 洧램res 洧녳 = 洧띺res 洧녳 ((cid:174)x 洧녳 ((cid:174)x 洧녳 mat((cid:174)x 洧녳 洧녳 ) + bpre 洧녳 ) + bpost 洧녳 ) + bres , 洧녳 (7) where 洧램pre mat() is reshape function from R1洧녵2 R洧녵洧냤洧녵 and 洧램res , 洧램post 洧녳 洧녳 洧녳 R洧녵洧냤洧녵2 to R洧녵洧녵. are linear projections for dynamic mappings and Then, the final constrained mappings are obtained via: 洧녳 ) = 洧랥( pre = 2洧랥( post pre 洧녳 post 洧녳 洧녳 = Sinkhorn-Knopp( res res ) 洧녳 洧녳 ), (8) where 洧랥() denotes the Sigmoid function. The Sinkhorn-Knopp() operator firstly makes all elements to be positive via an exponent operator and then conducts iterative normalization process that alternately rescales rows and columns to sum to 1. Specifically, given positive matrix M(0) = exp( res ) as the start point, the normalization iteration proceeds as: 洧녳 M(洧노) = T洧 (cid:16) T洧녫 (M(洧노1) ) (cid:17) , (9) where T洧 and T洧녫 denote row and column normalization, respectively. This process converges to 洧녳 = M(洧노max ) as 洧노max . We choose 洧노max = 20 as practical value in doubly stochastic matrix res our experiments. 4.3. Efficient Infrastructure Design In this section, we detail the infrastructure design tailored for mHC. Through rigorous optimization, we implement mHC (with 洧녵 = 4) in large-scale models with marginal training overhead of only 6.7%. 4.3.1. Kernel Fusion Observing that RMSNorm in mHC imposes significant latency when operating on the highdimensional hidden state (cid:174)x洧녳 R1洧녵洧냤, we reorder the dividing-by-norm operation to follow the matrix multiplication. This optimization maintains mathematical equivalence while improving efficiency. Furthermore, we employ mixed-precision strategies to maximize numerical accuracy without compromising speed, and fuse multiple operations with shared memory access into unified compute kernels to reduce memory bandwidth bottlenecks. Based on the inputs and parameters detailed in Eq. (10) to (13), we implement three specialized mHC kernels to compute pre . In these kernels, the biases and linear projections are consolidated into b洧녳 洧녳 and 洧램洧녳, and the RMSNorm weight is also absorbed in 洧램洧녳. , and res , post 洧녳 洧녳 Eq. (14) to (15): We develop unified kernel that fuses two scans on (cid:174)x洧녳, leveraging matrix multiplication units to maximize memory bandwidth utilization. The backward passcomprising two matrix multiplicationsis similarly consolidated into single kernel, eliminating redundant reloading of (cid:174)x洧녳. Both kernels feature finely tuned pipeline (load, cast, compute, store) to efficiently handle mixed-precision processing. Eq. (16) to (18): These lightweight operations on small coefficients are opportunistically fused into single kernel, significantly reducing kernel launch overhead. Eq. (19): We implement the Sinkhorn-Knopp iteration within single kernel. For the backward pass, we derive custom backward kernel that recomputes the intermediate results on-chip and traverses the entire iteration. 洧램洧녳 : tfloat32 (cid:174)x洧녳 : bfloat16 洧띺pre 洧녳 , 洧띺post 洧녳 , 洧띺res 洧녳 : float32 (cid:104) pre 洧녳 , post 洧녳 b洧녳 : float32 (cid:105) : float32 , res 洧녳 (cid:104) pre 洧녳 , post 洧녳 , res 洧녳 洧 : float32 (cid:105) : float pre 洧녳 : float32 post 洧녳 : float32 res 洧녳 : float [洧녵洧냤, 洧녵2 + 2洧녵] [1, 洧녵洧냤] Scalars [1, 洧녵2 + 2洧녵] = (cid:174)x洧녳 洧램洧녳 (cid:13) = (cid:13) (cid:13)(cid:174)x洧녳 (cid:13)2 (cid:104) = 1/洧 (cid:16) = 洧랥 / 洧녵洧냤 (cid:17) pre 洧녳 (cid:16) (cid:17) 洧띺pre 洧녳 pre 洧녳 , 洧띺post 洧녳 post 洧녳 , 洧띺res 洧녳 res 洧녳 (cid:105) + b洧녳 post 洧녳 = 2洧랥 = Sinkhorn-Knopp (cid:0) res 洧녳 (cid:1) (10) (11) (12) (13) (14) (15) (16) (17) (18) (19) 洧녳 洧녳 x洧녳 + post (, ). Through fusing the application of post Using the coefficients derived from the aforementioned kernels, we introduce two additional kernels to apply these mappings: one for Fpre pre x洧녳 and another for Fpost,res res 洧녳 with residual merging, we reduce the number of elements read from (3洧녵 + 1)洧냤 to (洧녵 + 1)洧냤 and the number of elements written from 3洧녵洧냤 to 洧녵洧냤 for this kernel. We efficiently implement the majority of kernels (excluding Eq. (14) to (15)) using TileLang (Wang et al., 2025). This framework streamlines the implementation of kernels with complex calculation process and allows us to fully utilize the memory bandwidth with minimal engineering effort. and res 洧녳 洧녳 4.3.2. Recomputing The 洧녵-stream residual design introduces substantial memory overhead during training. To mitigate this, we discard the intermediate activations of the mHC kernels after the forward pass and recompute them on-the-fly in the backward pass, through re-executing the mHC kernels 10 without the heavy layer function . Consequently, for block of 洧洧 consecutive layers, we need only store the input x洧녳0 to the first layer. Excluding lightweight coefficients while accounting for the pre-norm with in , Tab. 3 summarizes the intermediate activations preserved for the backward pass. Table 3 Stored and Recomputed Intermediate Activations We list per token activation preserved for the backward pass and the transient activation recomputed in 洧洧 consecutive layers. Layer 洧녳0 represents the first layer in 洧洧 layers and layer 洧녳 is in [洧녳0, 洧녳0 + 洧洧 1]."
        },
        {
            "title": "Activations",
            "content": "Size (Elements) Stored Method x洧녳0 洧녵洧냤 Every 洧洧 layers (H pre 洧녳 x洧녳, W洧녳) 洧냤 x洧녳 pre 洧냤 洧녵洧냤 洧녳 x洧녳 RMSNorm(H pre 洧녳 x洧녳) 洧냤"
        },
        {
            "title": "Every layer",
            "content": "Transient inside 洧洧 layers Since mHC kernels recomputation is performed for blocks of 洧洧 consecutive layers, given total of 洧 layers, we must persistently store the first layer input x洧녳0 for all 洧 blocks for the 洧洧 backward pass. In addition to this resident memory, the recomputation process introduces transient memory overhead of (洧녵 + 2)洧냤 洧洧 elements for the active block, which determines the peak memory usage during backpropagation. Consequently, we determine the optimal block size 洧 洧 by minimizing the total memory footprint corresponded to 洧洧: 洧 洧 = arg min 洧洧 (cid:20) 洧녵洧냤 (cid:25) (cid:24) 洧 洧洧 + (洧녵 + 2)洧냤 洧洧 (cid:21) 洧녵洧 洧녵 + 2 . (20) Furthermore, pipeline parallelism in large-scale training imposes constraint: recomputation blocks must not cross pipeline stage boundaries. Observing that the theoretical optimum 洧 洧 typically aligns with the number of layers per pipeline stage, we choose to synchronize the recomputation boundaries with the pipeline stages. 4.3.3. Overlapping Communication in DualPipe In large-scale training, pipeline parallelism is the standard practice for mitigating parameter and gradient memory footprints. Specifically, we adopt the DualPipe schedule (Liu et al., 2024b), which effectively overlaps scale-out interconnected communication traffic, such as those in expert and pipeline parallelism. However, compared to the single-stream design, the proposed 洧녵-stream residual in mHC incurs substantial communication latency across pipeline stages. Furthermore, at stage boundaries, the recomputation of mHC kernels for all 洧洧 layers introduces non-negligible computational overhead. To address these bottlenecks, we extend the DualPipe schedule (see Fig. 4) to facilitate improved overlapping of communication and computation at pipeline stage boundaries. Notably, to prevent blocking the communication stream, we execute the Fpost,res kernels of MLP (i.e. FFN) layers on dedicated high-priority compute stream. We further refrain from employing persistent kernels for long-running operations in attention layers, thereby preventing extended stalls. This design enables the preemption of overlapped attention computations, allowing for flexible scheduling while maintaining high utilization of the compute devices processing units. Furthermore, the recomputation process is decoupled from pipeline communication dependencies, as the initial activation of each stage x洧녳0 is already cached locally. 11 Figure 4 Communication-Computation Overlapping for mHC. We extend the DualPipe schedule to handle the overhead introduced by mHC. Lengths of each block are illustrative only and do not represent actual duration. (F), (B), (W) refers to forward pass, backward pass, weight gradient computation, respectively. and represents kernels corresponded to Attention and MLP, respectively. 5. Experiments 5.1. Experimental Setup We validate the proposed method via language model pre-training, conducting comparative analysis between the baseline, HC, and our proposed mHC. Utilizing MoE architectures inspired by DeepSeek-V3 (Liu et al., 2024b), we train four distinct model variants to cover different evaluation regimes. Specifically, the expansion rate 洧녵 for both HC and mHC is set to 4. Our primary focus is 27B model trained with dataset size proportional to its parameters, which serves as the subject for our system-level main results. Expanding on this, we analyze the compute scaling behavior by incorporating smaller 3B and 9B models trained with proportional data, which allows us to observe performance trends across varying compute. Additionally, to specifically investigate the token scaling behavior, we train separate 3B model on fixed corpus of 1 trillion tokens. Detailed model configurations and training hyper-parameters are provided in Appendix A.1. 5.2. Main Results Figure 5 Training Stability of Manifold-Constrained Hyper-Connections (mHC). This figure illustrates (a) the absolute training loss gap of mHC and HC relative to the baseline, and (b) the gradient norm of the three methods. All experiments utilize the 27B model. The results demonstrate that mHC exhibits improved stability in terms of both loss and gradient norm. We begin by examining the training stability and convergence of the 27B models. As illustrated in Fig. 5 (a), mHC effectively mitigates the training instability observed in HC, achieving final loss reduction of 0.021 compared to the baseline. This improved stability is further corroborated by the gradient norm analysis in Fig. 5 (b), where mHC exhibits significantly better behavior than HC, maintaining stable profile comparable to the baseline. 12 Table 4 System-level Benchmark Results for 27B Models. This table compares the zeroshot and few-shot performance of the Baseline, HC, and mHC across 8 diverse downstream benchmarks. mHC consistently outperforms the Baseline and surpasses HC on the majority of benchmarks, demonstrating its effectiveness in large-scale pre-training. Benchmark (Metric) # Shots 27B Baseline 27B w/ HC 27B w/ mHC BBH (EM) 3-shot 43.8 48.9 51.0 DROP (F1) 3-shot 47.0 51.6 53.9 GSM8K HellaSwag MATH (EM) (Acc.) (EM) MMLU (Acc.) 8-shot 10-shot 4-shot 5-shot 46.7 53.2 53.8 73.7 74.3 74.7 22.0 26.4 26. 59.0 63.0 63.4 PIQA (Acc.) 0-shot 78.5 79.9 80.5 TriviaQA (EM) 5-shot 54.3 56.3 57.6 Tab. 4 presents the downstream performance across diverse set of benchmarks (Bisk et al., 2020; Cobbe et al., 2021; Hendrycks et al., 2020, 2021; Joshi et al., 2017; Zellers et al., 2019). mHC yields comprehensive improvements, consistently outperforming the baseline and surpassing HC on the majority of tasks. Notably, compared to HC, mHC further enhances the models reasoning capabilities, delivering performance gains of 2.1% on BBH (Suzgun et al., 2022) and 2.3% on DROP (Dua et al., 2019). 5.3. Scaling Experiments Figure 6 Scaling properties of mHC compared to the Baseline. (a) Compute Scaling Curve. Solid lines depict the performance gap across different compute budgets. Each point represents specific compute-optimal configuration of model size and dataset size, scaling from 3B and 9B to 27B parameters. (b) Token Scaling Curve. Trajectory of the 3B model during training. Each point represents the models performance at different training tokens. Detailed architectures and training configurations are provided in Appendix A.1. To assess the scalability of our approach, we report the relative loss improvement of mHC against the baseline across different scales. In Fig. 6 (a), we plot the compute scaling curve spanning 3B, 9B, and 27B parameters. The trajectory indicates that the performance advantage is robustly maintained even at higher computational budgets, showing only marginal attenuation. Furthermore, we examine the within-run dynamics in Fig. 6 (b), which presents the token scaling curve for the 3B model. Collectively, these findings validate the effectiveness of mHC in large-scale scenarios. This conclusion is further corroborated by our in-house large-scale training experiments. Figure 7 Propagation Stability of Manifold-Constrained Hyper-Connections (mHC). This figure illustrates the propagation dynamics of (a) the single-layer mapping PMres (H res ) and (b) the composite mapping (cid:206)洧洧녳 洧洧녰) within the 27B model. The results demonstrate that 洧녰=1 mHC significantly enhances propagation stability compared to HC. PMres (H res 洧녳 Figure 8 Visualizations of Learnable Mappings. This figure displays representative singlelayer and composite mappings for HC (first row) and mHC (second row). Each matrix is computed by averaging over all tokens within selected sequence. The labels annotated along the y-axis and x-axis indicate the forward signal gain (row sum) and the backward gradient gain (column sum), respectively. 5.4. Stability Analysis Similar to Fig. 3, Fig. 7 illustrates the propagation stability of mHC. Ideally, the single-layer mapping satisfies the doubly stochastic constraint, implying that both the forward signal gain and the backward gradient gain should equal to 1. However, practice implementations utilizing the Sinkhorn-Knopp algorithm must limit the number of iterations to achieve computational efficiency. In our settings, we use 20 iterations to obtain an approximate solution. Consequently, as shown in Fig. 7(a), the backward gradient gain deviates slightly from 1. In the composite case shown in Fig. 7(b), the deviation increases but remains bounded, reaching maximum value of approximately 1.6. Notably, compared to the maximum gain magnitude of nearly 3000 in HC, mHC significantly reduces it by three orders of magnitude. These results demonstrate that mHC significantly enhances propagation stability compared to HC, ensuring stable forward signal and backward gradient flows. Additionally, Fig. 8 displays representative mappings. We observe that for HC, when the maximum gain is large, other values also tend to be significant, which indicates general instability across all propagation paths. In contrast, mHC consistently yields stable results. 14 6. Conclusion and Outlook In this paper, we identify that while expanding the width of residual stream and diversifying connections yields performance gains as proposed in Hyper-Connections (HC), the unconstrained nature of these connections leads to signal divergence. This disruption compromises the conservation of signal energy across layers, inducing training instability and hindering the scalability of deep networks. To address these challenges, we introduce Manifold-Constrained Hyper-Connections (mHC), generalized framework that projects the residual connection space onto specific manifold. By employing the Sinkhorn-Knopp algorithm to enforce doubly stochastic constraint on residual mappings, mHC transforms signal propagation into convex combination of features. Empirical results confirm that mHC effectively restores the identity mapping property, enabling stable large-scale training with superior scalability compared to conventional HC. Crucially, through efficient infrastructure-level optimizations, mHC delivers these improvements with negligible computational overhead. As generalized extension of the HC paradigm, mHC opens several promising avenues for future research. Although this work utilizes doubly stochastic matrices to ensure stability, the framework accommodates the exploration of diverse manifold constraints tailored to specific learning objectives. We anticipate that further investigation into distinct geometric constraints could yield novel methods that better optimize the trade-off between plasticity and stability. Furthermore, we hope mHC rejuvenates community interest in macro-architecture design. By deepening the understanding of how topological structures influence optimization and representation learning, mHC will help address current limitations and potentially illuminate new pathways for the evolution of next-generation foundational architectures."
        },
        {
            "title": "References",
            "content": "J. Ainslie, J. Lee-Thorp, M. De Jong, Y. Zemlyanskiy, F. Lebr칩n, and S. Sanghai. Gqa: Training generalized multi-query transformer models from multi-head checkpoints. arXiv preprint arXiv:2305.13245, 2023. Y. Bisk, R. Zellers, R. L. Bras, J. Gao, and Y. Choi. PIQA: reasoning about physical commonsense in natural language. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020, pages 74327439. AAAI Press, 2020. doi: 10.1609/aaai.v34i05.6239. URL https://doi.org/10.1609/aaai.v34i05.6239. T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. Y. Chai, S. Jin, and X. Hou. Highway transformer: Self-gating enhanced self-attentive networks. In D. Jurafsky, J. Chai, N. Schluter, and J. Tetreault, editors, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 68876900, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.616. URL https://aclanthology.org/2020.acl-main.616/. F. Chollet. Xception: Deep learning with depthwise separable convolutions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 12511258, 2017. K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. T. Dao, D. Y. Fu, S. Ermon, A. Rudra, and C. R칠. FlashAttention: Fast and memory-efficient exact attention with IO-awareness. In Advances in Neural Information Processing Systems (NeurIPS), 2022. D. Dua, Y. Wang, P. Dasigi, G. Stanovsky, S. Singh, and M. Gardner. DROP: reading comprehension benchmark requiring discrete reasoning over paragraphs. In J. Burstein, C. Doran, and T. Solorio, editors, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pages 2368 2378. Association for Computational Linguistics, 2019. doi: 10.18653/V1/N19-1246. URL https://doi.org/10.18653/v1/n19-1246. Y. Fang, Y. CAI, J. Chen, J. Zhao, G. Tian, and G. Li. Cross-layer retrospective retrieving via layer attention. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=pvgEL1yS3Ql. W. Fedus, B. Zoph, and N. Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. Journal of Machine Learning Research, 23(120):139, 2022. K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770778, 2016a. K. He, X. Zhang, S. Ren, and J. Sun. Identity mappings in deep residual networks. In European conference on computer vision, pages 630645. Springer, 2016b. M. Heddes, A. Javanmard, K. Axiotis, G. Fu, M. Bateni, and V. Mirrokni. Deepcrossattention: Supercharging transformer residual connections. In Forty-second International Conference on Machine Learning, 2025. URL https://openreview.net/forum?id=j3JBfFnGYh. D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020. D. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, D. Song, and J. Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. de Las Casas, L. A. Hendricks, J. Welbl, A. Clark, T. Hennigan, E. Noland, K. Millican, G. van den Driessche, B. Damoc, A. Guy, S. Osindero, K. Simonyan, E. Elsen, O. Vinyals, J. Rae, and L. Sifre. In S. Koyejo, An empirical analysis of compute-optimal large language model training. S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages 3001630030. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/file/c1e2faf f6f588870935f114ebe04a3e5-Paper-Conference.pdf. G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger. Densely connected convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 47004708, 2017. 16 M. Joshi, E. Choi, D. Weld, and L. Zettlemoyer. TriviaQA: large scale distantly supervised challenge dataset for reading comprehension. In R. Barzilay and M.-Y. Kan, editors, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 16011611, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1147. URL https://aclanthology.org/P17-1147. G. Larsson, M. Maire, and G. Shakhnarovich. Fractalnet: Ultra-deep neural networks without residuals. arXiv preprint arXiv:1605.07648, 2016. D. Lepikhin, H. Lee, Y. Xu, D. Chen, O. Firat, Y. Huang, M. Krikun, N. Shazeer, and Z. Chen. Gshard: Scaling giant models with conditional computation and automatic sharding. arXiv preprint arXiv:2006.16668, 2020. A. Liu, B. Feng, B. Wang, B. Wang, B. Liu, C. Zhao, C. Dengr, C. Ruan, D. Dai, D. Guo, et al. Deepseek-v2: strong, economical, and efficient mixture-of-experts language model. arXiv preprint arXiv:2405.04434, 2024a. A. Liu, B. Feng, B. Xue, B. Wang, B. Wu, C. Lu, C. Zhao, C. Deng, C. Zhang, C. Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024b. I. Loshchilov and F. Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. B. Mak and J. Flanigan. Residual matrix transformers: Scaling the size of the residual stream. arXiv preprint arXiv:2506.22696, 2025. G. Menghani, R. Kumar, and S. Kumar. LAurel: Learned augmented residual layer. In Forty-second International Conference on Machine Learning, 2025. URL https://open review.net/forum?id=rUDRWP9WvZ. M. Pagliardini, A. Mohtashami, F. Fleuret, and M. Jaggi. Denseformer: Enhancing information flow in transformers via depth weighted averaging. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https://openreview.net/forum ?id=kMnoh7CXrq. P. Qi, X. Wan, G. Huang, and M. Lin. Zero bubble (almost) pipeline parallelism. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview .net/forum?id=tuzTN0eIO5. N. Shazeer. Fast transformer decoding: One write-head is all you need. arXiv preprint arXiv:1911.02150, 2019. N. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. Le, G. Hinton, and J. Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538, 2017. R. Sinkhorn and P. Knopp. Concerning nonnegative matrices and doubly stochastic matrices. Pacific Journal of Mathematics, 21(2):343348, 1967. R. K. Srivastava, K. Greff, and J. Schmidhuber. Training very deep networks. In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 28. Curran Associates, Inc., 2015. URL https://proceedings. neurips.cc/paper_files/paper/2015/file/215a71a12769b056c3c32e7299f1c5e d-Paper.pdf. 17 J. Su, M. Ahmed, Y. Lu, S. Pan, W. Bo, and Y. Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. M. Suzgun, N. Scales, N. Sch칛rli, S. Gehrmann, Y. Tay, H. W. Chung, A. Chowdhery, Q. V. Le, E. H. Chi, D. Zhou, et al. Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022. H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozi칟re, N. Goyal, E. Hambro, F. Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, 켸. Kaiser, and I. Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. L. Wang, H. Gao, C. Zhao, X. Sun, and D. Dai. Auxiliary-loss-free load balancing strategy for mixture-of-experts. arXiv preprint arXiv:2408.15664, 2024. L. Wang, Y. Cheng, Y. Shi, Z. Tang, Z. Mo, W. Xie, L. Ma, Y. Xia, J. Xue, F. Yang, et al. Tilelang: composable tiled programming model for ai systems. arXiv preprint arXiv:2504.17577, 2025. D. Xiao, Q. Meng, S. Li, and X. Yuan. Muddformer: Breaking residual bottlenecks in transformers via multiway dynamic dense connections. arXiv preprint arXiv:2502.12170, 2025. S. Xie, R. Girshick, P. Doll치r, Z. Tu, and K. He. Aggregated residual transformations for deep In Proceedings of the IEEE conference on computer vision and pattern neural networks. recognition, pages 14921500, 2017. S. Xie, H. Zhang, J. Guo, X. Tan, J. Bian, H. H. Awadalla, A. Menezes, T. Qin, and R. Yan. Residual: Transformer with dual residual connections, 2023. URL https://arxiv.org/abs/2304.1 4802. F. Yu, D. Wang, E. Shelhamer, and T. Darrell. Deep layer aggregation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 24032412, 2018. R. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, and Y. Choi. HellaSwag: Can machine really finish your sentence? In A. Korhonen, D. R. Traum, and L. Mrquez, editors, Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28August 2, 2019, Volume 1: Long Papers, pages 47914800. Association for Computational Linguistics, 2019. doi: 10.18653/v1/p19-1472. URL https://doi.org/10.18653/v1/p1 9-1472. B. Zhang and R. Sennrich. Root mean square layer normalization. Advances in neural information processing systems, 32, 2019. D. Zhu, H. Huang, Z. Huang, Y. Zeng, Y. Mao, B. Wu, Q. Min, and X. Zhou. Hyper-connections. arXiv preprint arXiv:2409.19606, 2024. 18 A. Appendix A.1. Detailed Model Specifications and Hyper-parameters. Table 5 Detailed Model Specifications and Hyper-parameters. This table presents the architectural configurations for the 3B, 9B, and 27B models based on the DeepSeek-V3 (Liu et al., 2024b) architecture. It outlines the specific hyper-parameters for mHC and HC, including the residual stream expansion and Sinkhorn-Knopp settings, alongside the optimization and training protocols used in the experiments. Attribute"
        },
        {
            "title": "Vocab Params\nActive Params\nTotal Params",
            "content": "Layers Leading Dense Layers Routed Experts Active Experts Shared Experts Dimension FFN Dimension Load Balancing Method Attention Heads Attention Dimension Attention Variant KV Rank Position Embedding RoPE Dimension RoPE 洧랚 Layer Norm Type Layer Norm 洧 mHC/HC Expansion Rate 洧녵 mHC/HC Gating Factor Init 洧띺 mHC Sinkhorn-Knopp 洧노max Sequence Length Vocab Size Batch Size Training Steps Training Tokens Warmup Steps Optimizer AdamW Betas AdamW 洧 Base Learning Rate Lr Scheduler Lr Decay Step Ratio Lr Decay Rate Weight Decay 3B 331M 612M 2.97B 9B 496M 1.66B 9.18B 27B 662M 4.14B 27.0B 64 12 30 1280 896 2560 1536 18 1 64 6 2 1920 1280 Loss-Free (Wang et al., 2024) 24 128 MLA (Liu et al., 2024a) 512 RoPE (Su et al., 2024) 64 10000 RMSNorm (Zhang and Sennrich, 2019) 1e-20 32 16 4 0.01 320 30000 39.3B 1280 50000 262B 4096 129280 512 50000 105B 2000 AdamW (Loshchilov and Hutter, 2017) (0.9, 0.95) 1e-20 5.9e-4 Step [0.8 , 0.9 ] [0.316, 0.1] 0.1 8.6e-4 4.0e-4 3B 1T Tokens 331M 612M 2.97B 12 1 64 6 2 1280 896 Loss-Free 16 128 MLA 512 RoPE 64 10000 RMSNorm 1e-20 4 0.01 20 4096 129280 2560 100000 1.05T 2000 AdamW (0.9, 0.95) 1e-20 9.0e-4 Step [0.8 , 0.9 ] [0.316, 0.1] 0."
        }
    ],
    "affiliations": [
        "DeepSeek-AI"
    ]
}