{
    "paper_title": "ORID: Organ-Regional Information Driven Framework for Radiology Report Generation",
    "authors": [
        "Tiancheng Gu",
        "Kaicheng Yang",
        "Xiang An",
        "Ziyong Feng",
        "Dongnan Liu",
        "Weidong Cai"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The objective of Radiology Report Generation (RRG) is to automatically generate coherent textual analyses of diseases based on radiological images, thereby alleviating the workload of radiologists. Current AI-based methods for RRG primarily focus on modifications to the encoder-decoder model architecture. To advance these approaches, this paper introduces an Organ-Regional Information Driven (ORID) framework which can effectively integrate multi-modal information and reduce the influence of noise from unrelated organs. Specifically, based on the LLaVA-Med, we first construct an RRG-related instruction dataset to improve organ-regional diagnosis description ability and get the LLaVA-Med-RRG. After that, we propose an organ-based cross-modal fusion module to effectively combine the information from the organ-regional diagnosis description and radiology image. To further reduce the influence of noise from unrelated organs on the radiology report generation, we introduce an organ importance coefficient analysis module, which leverages Graph Neural Network (GNN) to examine the interconnections of the cross-modal information of each organ region. Extensive experiments an1d comparisons with state-of-the-art methods across various evaluation metrics demonstrate the superior performance of our proposed method."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 0 2 ] . [ 1 5 2 0 3 1 . 1 1 4 2 : r ORID: Organ-Regional Information Driven Framework for Radiology Report Generation Tiancheng Gu*, Kaicheng Yang* Xiang An, Ziyong Feng, Dongnan Liu, Weidong Cai University of Sydney DeepGlint tigu8498@uni.sydney.edu.au, kaichengyang@deepglint.com"
        },
        {
            "title": "Abstract",
            "content": "The objective of Radiology Report Generation (RRG) is to automatically generate coherent textual analyses of diseases based on radiological images, thereby alleviating the workload of radiologists. Current AI-based methods for RRG primarily focus on modifications to the encoderdecoder model architecture. To advance these approaches, this paper introduces an Organ-Regional Information Driven (ORID) framework which can effectively integrate multi-modal information and reduce the influence of noise from unrelated organs. Specifically, based on the LLaVAMed, we first construct an RRG-related instruction dataset to improve organ-regional diagnosis description ability and get the LLaVA-Med-RRG. After that, we propose an organbased cross-modal fusion module to effectively combine the information from the organ-regional diagnosis description and radiology image. To further reduce the influence of noise from unrelated organs on the radiology report generation, we introduce an organ importance coefficient analysis module, which leverages Graph Neural Network (GNN) to examine the interconnections of the cross-modal information of each organ region. Extensive experiments and comparisons with state-of-the-art methods across various evaluation metrics demonstrate the superior performance of our proposed method. 1. Introduction Analyzing radiological images plays crucial role in disease identification. The automatic generation of precise radiology reports can significantly alleviate the substantial workload faced by radiologists [55]. Manual analysis of radiological images is not only complex and expensive but also prone to errors, as radiologists need to examine numerous images and create detailed reports for each one [7]. *Equal contribution. Corresponding author. Figure 1. Visualization of organ-regional radiology image and diagnosis descriptions. Relevant segments associated with the target report have been highlighted using distinct colors. In recent years, deep learning techniques have been extensively applied to generate textual descriptions for images, such as image captions [2,8,33], inspiring many RRG methods [24, 34, 46]. Nevertheless, radiology report generation differs from standard image captioning due to the intricate nature of radiological images, which typically highlight small, disease-specific regions [10]. Additionally, radiological images primarily vary in the specific areas associated with diseases. This complexity extends to the textual descriptions in RRG, where the primary divergence lies in the analysis of disease findings, while the descriptions of healthy tissue remain relatively uniform [53]. To address these challenges, MIMIC-CXR [26] and IUXray [10] have been introduced, which comprise nearly 400,000 and 7,000 image-report pairs. As result, diverse RRG methodologies have been introduced [6, 7, 16, 20, 21, 25, 34, 43, 47, 62], each tackling the issue from distinct angles and showcasing notable enhancements in performance. Existing methods can be broadly classified into two categories, the first category [6, 7, 43] concentrates on improving model architectures to effectively fuse image and text modalities for radiology report generation. The second category [34, 46, 47] utilizes external modality information 1 and existing knowledge to bolster encoder-decoder models in producing conclusive reports. Despite their notable achievements, these methods do not incorporate detailed organ-regional information, which is crucial for guiding the generation of comprehensive radiology reports [11, 29]. In this paper, building upon offline generated organregional radiology images and diagnosis descriptions (as shown in Fig. 1), we propose an Organ-Regional Information Driven (ORID) framework for generating accurate and believable radiology reports. The ORID framework primarily comprises an organ-based cross-modal fusion module and an organ importance coefficient analysis module. Specifically, motivated by the exceptional performance of LLaVA-Med in medical diagnosis [32], we construct radiology image based instruction dataset grounded in radiology images encompassing 10,000 question-answer pairs related to 4,000 radiological images. Then, this dataset is utilized to enhance organ-regional diagnosis description ability and facilitate the development of the LLaVA-MedRRG model, which is employed for generating descriptions for downstream tasks. After that, we propose an organbased cross-modal fusion module to integrate the information from the organ-regional radiology image and diagnostic description. To reduce the influence of noise from unrelated organs, we utilize prior-knowledge graph [24] to construct an adjacency matrix based on the relationships of each organ region. Finally, the fused cross-modal features are combined with the raw radiology image features and input into an encoder-decoder model to generate the final radiology report. The contributions of this paper can be summarized as follows: We propose We construct an RRG-related instruction dataset to improve organ-regional diagnosis description ability and get the LLaVA-Med-RRG model which is used to generate descriptions for downstream tasks. an Organ-Regional Information Driven (ORID) framework for generating accurate and believable radiology reports. It consists of an organ-based multi-modal fusion module and an organ importance coefficient analysis module which can effectively integrate the multi-modal information and reduce the influence of noise from unrelated organs. We perform extensive experiments and the results prove that our proposed ORID framework achieves new state-of-the-art performance on two public radiology report generation benchmarks. 2. Related Works 2.1. Image Captioning Traditional image captioning techniques rely on inputting image-text pairs into an encoder-decoder model to generate text descriptions based on the input images. Early works [12, 15, 52] primarily utilize the Long ShortTerm Memory Network (LSTM) for the language component and Convolutional Neural Network (CNN) for the image component. Recently, catalyzed by the Vision Transformer (ViT) [13], attention-based models [50] have gained prominence in the field of image captioning. Subsequent researches [3,9,22] focus on incorporating object detectors to identify and extract relevant image regions to assist caption generation. Additionally, further studies [9, 40, 41] improve the architecture of the underlying model to enhance the interaction and alignment between image and text modalities. In contrast to general visual language tasks, radiology images present unique challenges due to their subtle and detailed differences. These nuances require specialized approaches to accurately capture and describe the intricate information in radiological imagery, such as the work [34] based on prior knowledge. 2.2. Radiology Report Generation Medical report generation extends the task of image captioning, presenting increased complexity and demandExtensive ing high levels of accuracy and precision. works [7, 16, 17, 34, 43, 47, 59, 60] have led to significant advancements in this domain. Inspired by attention mechanisms [50], the R2Gen [7] and R2GenCMN [6] enhance the encoder-decoder architecture by separately filtering and fusing image and text features using gate mechanisms from LSTM [19] and cross-modal memory networks [6]. Building on these, the R2GenCMM-RL [43] further improves the R2GenCMN [6] by incorporating reinforcement learningbased reward mechanisms [27]. In contrast to these models, the DCL [34] focus on structural improvements and it leverages pre-generated knowledge graph with correlation matrix of target report words. The RGRG [46] employs bounding boxes to guide the model to assign more attention to abnormal regions which facilitates disease detection and report generation. Additionally, the RepsNet [47] incorporates visual question answering knowledge [31] to enhance the final report generation process. However, existing methods struggle to obtain precise organ-level analysis descriptions while underutilizing organ-regional information to improve the accuracy and relevance of generated radiology reports. 2.3. Multimodal Large Language Model The advent of Large Language Models (LLMs) [1, 49], particularly those enhanced through instruction-tuning [37, 39], has demonstrated significant potential in serving as versatile interface for language-related tasks. To advance the capabilities of LLMs beyond language perception, recent studies [32, 36, 58, 64] have expanded these models into Multi-modal Large Language Models (MLLMs) by incorporating various modalities through instruction tunFigure 2. The overall architecture of our proposed ORID framework. ing [23, 57]. Expanding upon these advancements, LLaVAMed [32] is formulated through the instruction-tuning of LLaVA [36] using an extensive dataset of self-constructed, medically-oriented instructions. While LLaVA-Med exhibits strong performance in various medical domains, it lacks specific specialization in the field of radiology. To improve organ-regional diagnosis description ability, we construct an organ-level RRG-related instruction dataset that includes 10k question-answer pairs to develop the LLaVAMed-RRG. 3. Method The overview architecture of our proposed OrganRegional Information Driven (ORID) framework is shown in Fig. 2. This section introduces our primary methodincluding the development of LLaVA-Medologies, RRG (Sec. 3.1), the details of the Organ-based Cross-modal Fusion (OCF) module (Sec. 3.2), the Organ Importance Coefficient Analysis (OICA) module (Sec. 3.3), and the Radiology Report Generation Module (Sec. 3.4). 3.1. LLaVA-Med-RRG Even though LLaVA-Med shows superior performance in analyzing general medical images [32], its performance with radiology images has been relatively less satisfactory. Motivated by the cost-effectiveness and high performance of instruction tuning [23], we create an organ-level RRG-related instruction dataset to conduct instruction tuning on LLaVA-Med, resulting in LLaVA-Med-RRG. This enhancement is intended to enhance the capacity for organlevel analysis of radiology images. Specifically, the instruction dataset is transfered from IU-Xray [10] and MIMICCXR [26]. Drawing from the prior knowledge [24] of the Disease Symptom Graph (DS-Graph) (more details in supplementary materials), we identified that all diseases in radiology images are directly related to five organs: bone, pleural, lung, heart, and mediastinum. Building upon this prior knowledge, we use the NLTK tool* to segment captions into sentences and assign them to the corresponding organs by referring to the DS-Graph. The input and output type during the instruction tuning process is shown in Fig. 3. Prompt: What have you found in <organ>?n <image> Answer: <organ-level diagnosis description>. Figure 3. Input and output type during the instruction tuning. To ensure the quality and diversity of the instruction dataset, we adhere to the following four rules during the data construction process: (1) Enhancing positive disease increase the number of Question-Answer (QA) analysis: *https://github.com/nltk/nltk 3 pairs involving disease analysis while decreasing those without any disease (standard organ description) to enhance the models disease-analysis capabilities. (2) Minimizing redundancy: reduce examples with redundant identical answers to promote diverse responses to similar questions. (3) Prioritizing image examples with rich diagnosis: Concentrate on image examples containing more organ-specific QA pairs. (4) Balancing organ analysis proficiency: maintain an equitable distribution of QA pairs across the five organs to prevent the model from exhibiting bias towards specific organ analyses. Following the construction of the dataset, we conduct standard instruction tuning on the pretrained LLaVA-Med using LLaMA-Factory [63], resulting in the development of LLaVA-Med-RRG. 3.2. Organ-based Cross-modal Fusion Module Data Flow. After obtaining the LLaVA-Med-RRG model, we use it to generate organ-regional diagnosis descriptions and each radiology image corresponds to five description texts for distinct anatomical regions: lungs, heart, Then, we tokenize pleural, bones, and mediastinum. and embed these descriptions as features xD , where {mediastinum, pleural, lung, heart, bone}. Radiology images often emphasize small, distinct regions within the image [53]. To address this characteristic, we utilize organ-regional information to assist the model in improving the analysis ability of radiology images. Specifically, we employ the CXAS [45] model to identify and segment specific regions of interest (such as the left lung, lung ribs, and scapula) within radiology images. Drawing upon prior knowledge that radiology reports often center around five specific organ regions [24], we choose the pertinent segmented mask images and classify them into five distinct sets representing the heart, bones, lungs, pleura, and mediastinum, then we concat them in channel dimension in each set (more details in supplementary materials). After that, we integrate these masks with original image information to extract organ-regional image information xI as follows: = Em(Im), xIr = ˆEr(Ir), xIm = xIm xI xIr , (1) where is the element-wise multiplication. We use the mask feature extractor Em(ResNet18) to extract the organ mask feature xIm from the original radiology image Im. Meanwhile, we feed the raw radiology image Ir into the image feature extractor Er(ResNet101) and extract the output of middle layer ˆEr as the raw radiology image feature. Modality Fusion. After acquiring the corresponding organ-regional image information xI and diagnosis description features xD , we propose the organ-based cross-modal fusion module to fuse the image modality with the text modality to obtain single fine-grained cross-modal feature xC in organ-level. The process is formally as follows: = xI Q, = xD K, = xD = MHA(xI xC , xD ) = Softmax( , QK dn )V, (2) where MHA is the multi-head cross-attention [50] and Q, K, are learnable matrices. Analyzing individual organ image features independently can capture the fine-grained organ-level difference, thereby improving organ disease analysis performance. At the same time, this approach may restrict the models collaborative analysis capabilities, potentially leading to suboptimal analysis for diseases that rely on multiple organs [55]. To address this limitation, we generate coarsegrained organ-level image feature xI by adding all organ image features, which can be indicated as follows: xI = xI , (cid:88) iϕ (3) where ϕ is the organ set {mediastinum, pleural, lung, heart, bone}. We also further generate corresponding coarsegrained organ diagnosis description features xD by using the following procedure: = Concat(xD xD ) + Tp + To, (4) where Concat() is the concatenation operation, Tp is positional embedding tokens, and To is organ embedding tokens to distinguish different organs. Then, we integrate the coarse-grained organ image features xI with its corresponding description features xD to get the organ-regional cross-modal information xC as follows: = MHA(xI xC , xD ). (5) 3.3. Organ Importance Coefficient Analysis Module Upon reviewing substantial number of radiology reports, it is noted that most reports concentrate on analyzing 2-4 specific organ regions, with certain organs being reported as normal [10,26]. We hypothesize that including all five organ regions may introduce significant noise, thereby compromising the accuracy and specificity of the final radiology report. To address this issue, we evaluate the importance coefficient of each organ-regional cross-modal information. Furthermore, we leverage the strengths of GNNs in analyzing relationships among different items (nodes) [51]. The importance coefficient αo for the organ-regional crossmodal information is defined as follows: αo = MLP (cid:88) uN (v) W(k)h(k1) + b(k) , (6) 4 DATASET METHOD NLG METRIC BLUE@1 BLUE@2 BLUE@3 BLUE@4 METOR ROUGE-L IUXray DCL [34] MMTN [5] M2KT [61] C2M-DOT [54] CMMRL [44] XPRONET [53] R2GENCMN [43] ORID(OURS) DCL [34] MMTN [5] MIMIC M2KT [61] CXR LGI-MIMIC [65] CMMRL [44] XPRONET [53] R2GENCMN [43] ORID(OURS) - 0.486 0.497 0.475 0.494 0.501 0.475 0.501 - 0.379 0.386 0.343 0.353 0.344 0.347 0.386 - 0.321 0.319 0.309 0.321 0.324 0. 0.351 - 0.238 0.237 0.210 0.218 0.215 0.221 0.238 - 0.232 0.230 0.222 0.235 0.224 0.222 0.261 - 0.159 0.157 0.140 0.148 0.146 0. 0.163 0.163 0.175 0.174 0.170 0.181 0.165 0.165 0.198 0.109 0.116 0.111 0.099 0.106 0.105 0.097 0.117 0.193 - - 0.191 0.201 0.204 0. 0.211 0.150 0.160 - 0.137 0.142 0.138 0.138 0.150 0.383 0.375 0.399 0.375 0.384 0.380 0.371 0.400 0.284 0.283 0.274 0.271 0.278 0.279 0. 0.284 Table 1. The results of the ORID model and other tested models in IU-Xray and MIMIC-CXR benchmarks. indicates we reproduced. The results for other models are obtained from their original papers. The best result is presented in bold. The most important metric has been marked in grey . where is the adjacent matrix generated in advance, which is obtained from knowledge graph [24], (v) is , xC set of the nodes (v) {xC is the value of the last layers node u. is the layer number, is learnable bias, and MLP is the multilayer perceptron [48]. }. hk1 Then, the final cross-modal feature xC can be obtained from the process: loss function is defined as follows: LCS = 1 ˆxI xT ˆxI xT . The overall loss function can be defined as: LT = LCE + β LCS, (8) (9) = xC xC xC αi, (cid:88) iϕ (7) where LCE is the cross-entropy loss [38] for report generation and LCS is the cross-modal consistent loss with its coefficient β commonly set as 0.1. where ϕ is the organ set {mediastinum, pleural, lung, heart,bone}, is the element-wise adding. Finally, we can get the final input image feature xI = xC , where xIr = Er(Ir) is the fine-grained raw image feature extracted from the output layer of the image feature extractor Er. + xIr 3.4. Radiology Report Generation Module Following [6], the final input image feature xI will input the encoder-decoder model [50] to generate the final radiology report. Specifically, the encoder model will first extract image features, which are used in the decoder model to generate the final radiology report. During the training process, we additionally introduce the consistency constraint loss to align the image feature ˆxI after encoder and the radiology report xT after the embedding layer from the decoder, the 4. Experiment Settings 4.1. Datasets To rigorously evaluate the efficacy of our proposed method, we conduct experiments on two widely recognized public benchmarks: IU-Xray [10] and MIMIC-CXR [26]. The IU-Xray dataset containing 7,470 chest X-ray images and 3,955 corresponding reports. In contrast, the MIMIC-CXR dataset, is significantly larger and comprising 473,057 chest X-ray images and 206,563 associated reports. Consistent with the methodology outlined in [6], both datasets were partitioned into training, validation, and testing sets in 7:2:1 ratio. https://openi.nlm.nih.gov/faq https://physionet.org/content/mimic-cxr/2.0.0/ 5 4.2. Baseline and Evaluation Metrics Baseline. To rigorously assess the performance of our ORID framework in the radiology report generation task, we conduct comparative analysis against several stateof-the-art models. Specifically, we compare our method with DCL [34], MMTN [5], M2KT [61], C2M-DOT [54], Lgi-MIMIC [65], CMMRL [44], R2GenCMN [6], and XRONET [53]. Certain models have been excluded from our evaluation for well-defined reasons. The RGRG [46] has been omitted due to its significantly large architecture, incorporating 24-layer decoder, which presents substantial challenges for reproducibility among other researchers. Similarly, the RepsNet [47] and METransformer [56] have been excluded as they employ distinct dataset split for evaluation and testing. Moreover, the KiUT [24] has been excluded due to the unavailability of its source code, impeding the reproducibility of its results. This rigorous selection process ensures equitable and consistent comparisons, thereby bolstering the reliability of our evaluation and the validity of our conclusions. Evaluation Metrics. In this paper, we employ Natural Language Generation (NLG) metrics such as BLEU [42], ROUGE-L [35], and METEOR [4] to evaluate our model. BLEU measures word n-gram overlap, ROUGE-L assesses sentence-level coherence via the longest common subsequence, and METEOR evaluates text similarity with precision, recall, and semantic analysis. These metrics robustly assess the accuracy and coherence of generated radiology reports. Additionally, following previous works [6, 43], we employ clinical efficacy metrics to evaluate the quality of the generated reports on the MIMIC-CXR dataset [26]. These metrics can assess the presence of predefined set of clinically significant observations, thereby providing measure of the diagnostic accuracy and relevance of the generated reports compared to the corresponding target reports. 4.3. Implement Details In our approach, we utilize the ResNet101 [18] and ResNet18 [18] as the image feature extractor and mask feature extractor, which are initialized with pretrained weights from ImageNet. The CXAS [45] model is used to segment organ masks from radiology images. We adopt the graph attention network [51] with eight heads to analyze relationships between different organs. We train LLaVA-Med-RRG and our proposed ORID framework on 4 and 1 NVIDIA A100 GPUs (80G), respectively. We employ the Adam [28] optimizer initialized with learning rate of 1e 4 for the image extractor and 5e 4 for the other components. The encoder-decoder in the radiology report generation module is trained using the teacher forcing method [30]. During inference, we utilize beam search [14] with width of 3. We METHOD CE METRIC Precision Recall F1-Score R2GEN [7] CMMRL [43] R2GENCMN [6] METRANSFORMER [56] ORID(OURS) 0.333 0.342 0.334 0.364 0.435 0.273 0.294 0.275 0.309 0.295 0.276 0.292 0.278 0. 0.352 Table 2. Comparison of clinical efficacy metrics for the MIMICCXR dataset. The best result is presented in bold. The critical metrics have been shaded in grey . train our model for 100 and 30 epochs on the IU-Xray [10] and MIMIC-CXR [26] datasets. To enhance model robustness, images and masks underwent data augmentation techniques, including random cropping and random horizontal flipping. For both benchmarks, the numbers of masks for the heart, lung, bone, pleural, and mediastinum are 6, 15, 70, 10, and 9. The diagnosis description token lengths for the above five organs are 39, 53, 48, 43, and 41. Additionally, the final radiology reports associated with the datasets are preprocessed, involving the removal of punctuation and the conversion of infrequent words (appearing less than three times) to the [UNK] token. 5. Experiments 5.1. Comparison with State-of-the-art models Descriptive Accuracy. The results of our descriptive accuracy analysis are presented in Tab. 1. Our proposed ORID framework demonstrates superior performance compared to the SOTA models across all assessed metrics on both datasets. Notably, on the IU-Xray dataset, ORID excels in all metrics. Similarly, on the MIMIC-CXR dataset, ORID showcases superior performance in all BLEU metrics and ROUGE-L. It can be noticed that our method obtains slightly lower METEOR score than MMTN [5], this can be attributed to our framework focusing on disease detection and analysis, which leads to marginal reduction in report diversity. Clinical Correctness. In Tab. 2, we present comparative analysis of our proposed ORID model against four SOTA models: R2Gen [7], CMMRL [43], METransformer [56], and R2GenCMN [6], on the MIMIC-CXR dataset [26]. The experiment results show that our model demonstrates superior performance in all the metrics. Specifically, compared with the METransformer, our model achieves 19.5% improvement in precision and 13.2% improvement in F1score. This can be attributed to our framework fully utilizing the organ-regional information which significantly improves the capability in organ-level disease analysis. Be6 Figure 4. An example of LLaVA-Meds organ-reional diagnosis description compare with that of LLaVA-Med-RRG. The sentences that are correct or highly-related with target reports have been marked in green, otherwise have been marked in red. DIAGNOSIS MODEL B@1 B@4 MTR. RGL. LLAVA-MED [32] LLAVA-MED-RRG 0.441 0.501 0.158 0.198 0.179 0.211 0.378 0.400 Table 3. Experiment comparison between LLaVA-Med-RRG and LLaVA-Med. The best result is presented in bold. The most important metric is marked in grey . level RRG-related instruction dataset, which comprises approximately 10k QA pairs derived from nearly 4K radiology image-report pairs. Specifically, as illustrated in Fig. 5, the dataset includes 2.2k, 2.1k, 2.2k, 2.1k, and 2.1k pairs for the pleural , heart , lung , bone , and mediastinum , respectively. All the organ-regional diagnosis descriptions are concise and contain fewer than 20 tokens. Furthermore, we provide word cloud analysis for different organs and the whole dataset in Fig. 6. This analysis demonstrates substantial QA diversity for each organ, with predominant focus on disease-related queries (such as pleural effusion, rib fracture, and pulmonary edema) and lesser emphasis on normal condition analysis. Notably, most regional diagnostic descriptions focus on heart size due to the relative scarcity of heart diseases. We conduct comparison experiment between LLaVAMed-RRG and LLaVA-Med in Tab. 3. Experiment results indicate that LLaVA-Med-RRG significantly outperforms LLaVA-Med in the downstream RRG task across all evaluated metrics. Fig. 4 provides visual comparison of organregional descriptions generated by LLaVA-Med and our model. The figure demonstrates that the diagnosis descripFigure 5. Statistical analysis of question-answer pairs and average token length for each organ. (a) Lung (b) Pleural (c) Mediastinum (d) Heart (e) Bone (f) Total Figure 6. The word cloud analysis about each organ and total in instruction-tuning dataset. sides, our framework obtains competitive results in Recall compared to the METransformer, which utilizes multiple experts that excel in detecting broad range of diseases. 5.2. Ablation Study and Analysis Analysis on LLaVA-Med-RRG. The LLaVA-Med-RRG is instruction tunned based on it by our constructed organ7 # BL. MASK OCF OICA DATASET: IU-XRAY [10] B@1 B@4 MTR. RGL. 1 2 3 4 5 0.475 0.498 0.501 0.503 0.501 0.165 0.159 0.170 0.172 0.198 0.187 0.187 0.206 0.211 0.211 0.371 0.374 0.360 0.354 0.400 Table 4. Ablation study on different modules of ORID. The best result is presented in bold. The most important metric is marked in grey . tions generated by LLaVA-Med are excessively lengthy and often introduce noise and inaccuracies. These issues negatively affect the final generated reports, leading to the inclusion of errors and inefficient use of computational resources due to the processing of long tokens. In contrast, the descriptions produced by our model, LLaVA-Med-RRG, are concise and more relevant, resulting in more accurate and efficient radiology report generation. Ablation on Different Modules. Considering the extensive size of the MIMIC-CXR dataset and our objective to minimize CO2 emissions, we conduct an ablation study on the IU-Xray dataset to assess the influence of each module in our approach. In Tab. 4, BL denotes the baseline model, OCF refers to the Organ-based Cross-modal Information Module (Sec. 3.2), and represent fine-grained and coarse-grained analyses respectively, and OICA signifies the Organ Importance Coefficient Analysis Module (Sec. 3.3). comparison between configurations #1 and #2 in Tab. 4 indicates that incorporating the organ mask leads to moderate enhancements in the models RRG capability. This marginal improvement can be attributed to the challenges in distinguishing abnormal regions and disease detection when only organ-related regions are highlighted in the image. However, the inclusion of the OCF module significantly enhances caption accuracy, as evidenced by the comparison between #2 and #3. The fusion of crossmodal features from organ-specific diagnostic descriptions and image features notably boosts the accuracy of final reports across all evaluation metrics. Furthermore, comparing #3 and #4 reveals additional benefits from combining coarse and fine-grained analyses. Ultimately, #5, encompassing all proposed contributions demonstrates substantial performance gains across various metrics. 5.3. Qualitative Analysis To comprehensively evaluate the effectiveness of the ORID framework, we conduct qualitative analysis based on the MIMIC-CXR dataset. The qualitative examples are illustrated in Fig. 7, where we specifically highlight the Figure 7. Qualitative examples of generated radiology reports with different modules. pleural , heart , lung , bone , and mediastinum with different colours in the radiology report. As demonstrated in Fig. 7, compared with the baseline, the integration of the OCF module leads to reports containing more pertinent information than the baseline, including specifics on lung diseases, better aligning with the desired report content. Furthermore, the addition of the OICA module enhances the final report by incorporating comprehensive information about the heart. Moreover, we present the importance coefficients for the five organ regions in Fig. 7, showcasing that the heart, pleura, and lung achieve the highest scores. As result, the final report primarily emphasizes these three organ regions, reflecting the emphasis seen in the target report. 6. Conclusion This paper presents novel Organ-Regional Information Driven (ORID) framework for generating accurate and believable radiology reports. Initially, leveraging LLaVAMed, we establish an RRG-oriented instruction dataset to enhance organ-regional diagnosis descriptions and obtain LLaVA-Med-RRG. Subsequently, we introduce an organbased cross-modal fusion module to effectively integrate information from organ-specific diagnosis descriptions and radiology images. To mitigate the impact of irrelevant organ noise on radiology report generation, we propose an organ importance coefficient analysis module employing graph neural networks to analyze cross-modal information interconnections within each organ region. Our framework shows superior performance across various evaluation metrics in different benchmarks. We hope that our work provides insights into the radiology report generation field. 8 A. Appendix A.1. Disease Symptom Graph Figure 8. The symptom graph summarizes the related diseases for each organ in the MIMIC-CXR dataset. Fig. 8 illustrates the detailed knowledge graph of disease symptoms derived from prior disease captions. This graph, referenced from [24], was constructed based on professional analysis of the relationships between organs and their corresponding diseases as observed in radiology images. Utilizing this graph, we developed the instructiontuning dataset and the adjacency matrix for the Graph Neural Network (GNN). A.2. Benchmark Information DATASET IMAGE REPORT PATIENT AVG. LEN. IU-XRAY [10] MIMIC-CXR [26] Train Val. Test Train Val. Test 5.2K 0.7K 1.5K 369.0K 3.0K 5.2K 2.8K 0.4K 0.8K 222.8K 1.8K 3.3K 0.5K 0.3K 2.8K 0.4K 0.8K 64.6K 66.4 53.1 37.6 36. 53.0 33.6 ORGAN MASK NUM. REGION TOTAL MASK Lung lobes Lung zones Lung halves Heart region Mediastinum Diaphragm Ribs Ribs super Trachea Vessels Breast Tissue ... 5 8 2 6 3 46 24 2 6 2 ... Lung Heart Mediastinum 159 Bone Pleural ... Table 6. The specific information of masks generated by the CXAS model [45], as well as the mask images we ultimately used. Figure 9. The visualization of the organ mask sets with the original image. Due to each organ region corresponding to several small organ parts, the different color means different part organ mask images in its corresponding regions. Table 5. The specifications of two benchmark datasets that will be utilized to test the ORID model. A.4. Instruction-tuning Dataset Table 5 presents comprehensive information on the two benchmark datasets employed to evaluate our ORID framework. The data indicate that the MIMIC-CXR dataset encompasses greater number of cases compared to the IUXray dataset. A.3. Mask Information Table 6 provides specific details regarding the small organ masks included for each organ set. Furthermore, the visualization of these mask sets alongside the original images is presented in Fig. 9. Fig. 10 presents examples from the instruction-tuning dataset. Notably, each image is accompanied by more than four question-answer pairs pertaining to various organs. A.5. Case Study We have shown the results of our ORID framework generated compared with that of ground truth in Fig. 11. We have also marked the pleural, heart, lung, bone, and mediastinum in different colors. More specifically, Example 1 shows the disease symptoms related to the heart and lungs; Example 2 shows the disease symptoms related to the heart. 9 Figure 10. The examples about the RRG instruction dataset to instruction tuning the LLaVA-Med Figure 11. The visualization of prediction results by the ORID model. We specifically highlight the pleural, heart, lung, bone, and mediastinum with different colours in the radiology report."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 2 [2] J. B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson, and K. Simonyan. Flamingo: visual language model for few-shot learning. In NeurIPS, 2022. 1 [3] Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, and Lei Zhang. 11 Bottom-up and top-down attention for image captioning and visual question answering. In CVPR, 2018. [4] S. Banerjee and A. Lavie. Meteor: An automatic metric for mt evaluation with improved correlation with human judgments. In ACL, 2005. 6 [5] Y. Cao, L. Cui, L. Zhang, F. Yu, Z. Li, and Y. Xu. Mmtn: Multi-modal memory transformer network for image-report consistent medical report generation. In AAAI, 2024. 5, 6 [6] Z. Chen, Y. Shen, Y. Song, and X. Wan. Cross-modal memory networks for radiology report generation. In ACL, 2022. 1, 2, 5, 6 [7] Z. Chen, Y. Song, T. H. Chang, and X. Wan. Generating radiology reports via memory-driven transformer. In EMNLP, 2020. 1, 2, 6 [8] M. Cornia, M. Stefanini, L. Baraldi, and R. Cucchiara. Meshed-memory transformer for image captioning. In CVPR, 2019. 1 [9] Marcella Cornia, Matteo Stefanini, Lorenzo Baraldi, and Rita Cucchiara. Meshed-memory transformer for image captioning. In CVPR, 2020. 2 [10] D. Demner-Fushman, M. D. Kohli, M. B. Rosenman, S. E. Shooshan, L. Rodriguez, S. Antani, and C. J. McDonald. Preparing collection of radiology examinations for distribution and retrieval. Journal of the American Medical Informatics Association, 2016. 1, 3, 4, 5, 6, 8, [11] S. Dodia, B. Annappa, and P. A. Mahesh. Recent advancements in deep learning based lung cancer detection: systematic review. Engineering Applications of Artificial Intelligence, 2022. 2 [12] Jeffrey Donahue, Lisa Anne Hendricks, Sergio Guadarrama, Marcus Rohrbach, Subhashini Venugopalan, Kate Saenko, and Trevor Darrell. Long-term recurrent convolutional networks for visual recognition and description. In CVPR, 2015. 2 [13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In ICCV, 2020. 2 [14] Markus Freitag and Yaser Al-Onaizan. Beam search strategies for neural machine translation. ACL, 2017. 6 [15] Jiuxiang Gu, Gang Wang, Jianfei Cai, and Tsuhan Chen. An empirical study of language cnn for image captioning. In CVPR, 2017. 2 [16] Tiancheng Gu, Dongnan Liu, Zhiyuan Li, and Weidong Cai. Complex organ mask guided radiology report generation. In WACV, 2024. 1, [17] Tiancheng Gu, Kaicheng Yang, Dongnan Liu, and Weidong Cai. Lapa: Latent prompt assist model for medical visual question answering. In CVPR Workshops, 2024. 2 [18] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In CVPR, 2015. 6 [21] Wenjun Hou, Kaishuai Xu, Yi Cheng, Wenjie Li, and Jiang Liu. ORGAN: Observation-guided radiology report generation via tree reasoning. In ACL, 2023. 1 [22] Lun Huang, Wenmin Wang, Jie Chen, and Xiao-Yong Wei. Attention on attention for image captioning. In CVPR, 2019. 2 [23] Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui, Owais Khan Mohammed, Barun Patra, et al. Language is not all you need: Aligning perception with language models. NeurIPS, 2024. [24] Z. Huang, X. Zhang, and S. Zhang. Kiut: Knowledgeinjected u-transformer for radiology report generation. In CVPR, 2023. 1, 2, 3, 4, 5, 6, 9 [25] Baoyu Jing, Pengtao Xie, and Eric Xing. On the automatic generation of medical imaging reports. In ACL, 2018. 1 [26] A. E. Johnson, T. J. Pollard, S. J. Berkowitz, N. R. Greenbaum, M. P. Lungren, C. Y. Deng, and S. Horng. Mimiccxr, de-identified publicly available database of chest radiographs with free-text reports. Scientific Data, 2019. 1, 3, 4, 5, 6, 9 [27] L. P. Kaelbling, M. L. Littman, and A. W. Moore. Reinforcement learning: survey. Journal of artificial intelligence research, 1996. 2 [28] D. P. Kingma and J. Ba. Adam: method for stochastic optimization. In ICLR, 2014. 6 [29] Yogesh Kumar and Pekka Marttinen. Improving medical multi-modal contrastive learning with expert annotations. In ECCV, 2024. [30] Alex Lamb, Anirudh Goyal ALIAS PARTH GOYAL, Ying Zhang, Saizheng Zhang, Aaron Courville, and Yoshua Bengio. Professor forcing: new algorithm for training recurrent networks. NeurIPS, 2016. 6 [31] J. J. Lau, S. Gayen, A. Ben Abacha, and D. DemnerFushman. dataset of clinically generated visual questions and answers about radiology images. Scientific Data, 2018. 2 [32] C. Li, C. Wong, S. Zhang, N. Usuyama, H. Liu, and J. Yang. Llava-med: Training large language-and-vision assistant for biomedicine in one day. In NeurIPS, 2023. 2, 3, 7 [33] J. Li, R. Selvaraju, A. Gotmare, S. Joty, C. Xiong, and S. C. H. Hoi. Align before fuse: Vision and language representation learning with momentum distillation. In NeurIPS, 2021. 1 [34] M. Li, B. Lin, Z. Chen, H. Lin, X. Liang, and X. Chang. Dynamic graph enhanced contrastive learning for chest x-ray report generation. In CVPR, 2023. 1, 2, 5, 6 [35] C. Y. Lin. Rouge: package for automatic evaluation of summaries. In Text Summarization Branches Out, 2004. [36] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. [19] S. Hochreiter and J. Schmidhuber. Long short-term memory. Visual instruction tuning. NeurIPS, 2024. 2, 3 Neural computation, 1997. 2 [20] Wenjun Hou, Yi Cheng, Kaishuai Xu, Wenjie Li, and Jiang Liu. RECAP: Towards precise radiology report generation In EMNLP, via dynamic disease progression reasoning. 2023. 1 [37] Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc Le, Barret Zoph, Jason Wei, et al. The flan collection: Designing data and methods for effective instruction tuning. In ICML, 2023. 12 [56] Z. Wang, L. Liu, L. Wang, and L. Zhou. Metransformer: Radiology report generation by transformer with multiple learnable expert tokens. In CVPR, 2023. 6 [57] Penghao Wu and Saining Xie. V?: Guided visual search as core mechanism in multimodal llms. In CVPR, 2024. 3 [58] Yin Xie, Kaicheng Yang, Ninghua Yang, Weimo Deng, Xiangzi Dai, Tiancheng Gu, Yumeng Wang, Xiang An, Yongle Zhao, Ziyong Feng, et al. Croc: Pretraining large multimodal models with cross-modal comprehension. arXiv preprint arXiv:2410.14332, 2024. 2 [59] Hao Xu, Tengfei Xue, Dongnan Liu, Fan Zhang, Carl fredrik Westin, Ron Ron Kikinis, Lauren Jean ODonnell, and Weidong Cai. An uncertainty-distillationand voxel-contrastbased framework for one-shot segmentation of novel white matter tracts. In MIDL, 2024. 2 [60] Hao Xu, Tengfei Xue, Dongnan Liu, Fan Zhang, CarlFredrik Westin, Ron Kikinis, Lauren J. ODonnell, and Weidong Cai. registrationand uncertainty-based framework for white matter tract segmentation with only one annotated subject. In ISBI, 2023. 2 [61] S. Yang, X. Wu, S. Ge, Z. Zheng, S. K. Zhou, and L. Xiao. Radiology report generation with learned knowledge base and multi-modal alignment. MIA, 2021. 5, [62] Xingyi Yang, Muchao Ye, Quanzeng You, and Fenglong Ma. Writing by memorizing: Hierarchical retrieval-based medical report generation. In ACL, 2021. 1 [63] Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, Zhangchi Feng, and Yongqiang Ma. Llamafactory: Unified efficient fine-tuning of 100+ language models. In ACL, 2024. 4 [64] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023. 2 [65] Q. Zhu, T. S. Mathai, P. Mukherjee, Y. Peng, R. M. Summers, and Z. Lu. Utilizing longitudinal chest x-rays and reports to pre-fill radiology reports. In MICCAI, 2023. 5, 6 [38] Anqi Mao, Mehryar Mohri, and Yutao Zhong. Cross-entropy loss functions: Theoretical analysis and applications. In ICML, 2023. 5 [39] Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid Palangi, and Ahmed Awadallah. Orca: Progressive learning from complex explanation traces of gpt4. arXiv preprint arXiv:2306.02707, 2023. [40] Van-Quang Nguyen, Masanori Suganuma, and Takayuki Okatani. Grit: Faster and better image captioning transformer using dual visual features. In ECCV, 2022. 2 [41] Yingwei Pan, Ting Yao, Yehao Li, and Tao Mei. X-linear attention networks for image captioning. In CVPR, 2020. 2 [42] K. Papineni, S. Roukos, T. Ward, and W. J. Zhu. Bleu: method for automatic evaluation of machine translation. In ACL, 2002. 6 [43] H. Qin and Y. Song. Reinforced cross-modal alignment for radiology report generation. In ACL, 2022. 1, 2, 5, 6 [44] H. Qin and Y. Song. Reinforced cross-modal alignment for radiology report generation. In ACL, 2022. 5, 6 [45] C. Seibold, A. Jaus, M. A. Fink, M. Kim, S. Reiß, K. Herrmann, and R. Stiefelhagen. Accurate fine-grained segmentation of human anatomy in radiographs via volumetric pseudo-labeling. arXiv preprint arXiv:2306.03934, 2023. 4, 6, 9 [46] T. Tanida, P. Muller, G. Kaissis, and D. Rueckert. Interactive and explainable region-guided radiology report generation. In CVPR, 2023. 1, 2, 6 [47] A. K. Tanwani, J. Barral, and D. Freedman. Repsnet: Combining vision with language for automated medical reports. In MICCAI, 2023. 1, 2, [48] Ilya Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Unterthiner, Jessica Yung, Andreas Steiner, Daniel Keysers, Jakob Uszkoreit, et al. Mlp-mixer: An all-mlp architecture for vision. NeurIPS, 2021. 5 [49] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. 2 [50] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, and I. Polosukhin. Attention is all you need. In NeurIPS, 2017. 2, 4, 5 [51] Petar Veliˇckovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li`o, and Yoshua Bengio. Graph attention networks. In ICLR, 2018. 4, 6 [52] Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. Show and tell: neural image caption generator. In CVPR, 2015. 2 [53] Jun Wang, Abhir Bhalerao, and Yulan He. Cross-modal prototype driven network for radiology report generation. In ECCV, 2022. 1, 4, 5, [54] R. Wang, X. Wang, J. Zhou, T. Lukasiewicz, and Z. Xu. C2m-dot: Cross-modal consistent multi-view medical report generation with domain transfer network. MIA, 2023. 5, 6 [55] S. Wang and R. M. Summers. Machine learning and radiology. MIA, 2012. 1,"
        }
    ],
    "affiliations": [
        "DeepGlint",
        "University of Sydney"
    ]
}