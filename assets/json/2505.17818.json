{
    "paper_title": "PatientSim: A Persona-Driven Simulator for Realistic Doctor-Patient Interactions",
    "authors": [
        "Daeun Kyung",
        "Hyunseung Chung",
        "Seongsu Bae",
        "Jiho Kim",
        "Jae Ho Sohn",
        "Taerim Kim",
        "Soo Kyung Kim",
        "Edward Choi"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Doctor-patient consultations require multi-turn, context-aware communication tailored to diverse patient personas. Training or evaluating doctor LLMs in such settings requires realistic patient interaction systems. However, existing simulators often fail to reflect the full range of personas seen in clinical practice. To address this, we introduce PatientSim, a patient simulator that generates realistic and diverse patient personas for clinical scenarios, grounded in medical expertise. PatientSim operates using: 1) clinical profiles, including symptoms and medical history, derived from real-world data in the MIMIC-ED and MIMIC-IV datasets, and 2) personas defined by four axes: personality, language proficiency, medical history recall level, and cognitive confusion level, resulting in 37 unique combinations. We evaluated eight LLMs for factual accuracy and persona consistency. The top-performing open-source model, Llama 3.3, was validated by four clinicians to confirm the robustness of our framework. As an open-source, customizable platform, PatientSim provides a reproducible and scalable solution that can be customized for specific training needs. Offering a privacy-compliant environment, it serves as a robust testbed for evaluating medical dialogue systems across diverse patient presentations and shows promise as an educational tool for healthcare."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 2 ] . [ 1 8 1 8 7 1 . 5 0 5 2 : r PATIENTSIM: Persona-Driven Simulator for Realistic Doctor-Patient Interactions Daeun Kyung1, Hyunseung Chung1, Seongsu Bae1, Jiho Kim1, Jae Ho Sohn2, Taerim Kim3, Soo Kyung Kim4,, Edward Choi1, 1KAIST 2UCSF 3Samsung Medical Center 4Ewha Womans University {kyungdaeun,edwardchoi}@kaist.ac.kr1, sookim@ewha.ac.kr"
        },
        {
            "title": "Abstract",
            "content": "Doctor-patient consultations require multi-turn, context-aware communication tailored to diverse patient personas. Training or evaluating doctor LLMs in such settings requires realistic patient interaction systems. However, existing simulators often fail to reflect the full range of personas seen in clinical practice. To address this, we introduce PATIENTSIM, patient simulator that generates realistic and diverse patient personas for clinical scenarios, grounded in medical expertise. PATIENTSIM operates using: 1) clinical profiles, including symptoms and medical history, derived from real-world data in the MIMIC-ED and MIMIC-IV datasets, and 2) personas defined by four axes: personality, language proficiency, medical history recall level, and cognitive confusion level, resulting in 37 unique combinations. We evaluated eight LLMs for factual accuracy and persona consistency. The top-performing open-source model, Llama 3.3, was validated by four clinicians to confirm the robustness of our framework. As an open-source, customizable platform, PATIENTSIM provides reproducible and scalable solution that can be customized for specific training needs. Offering privacy-compliant environment, it serves as robust testbed for evaluating medical dialogue systems across diverse patient presentations and shows promise as an educational tool for healthcare."
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs) have shown impressive performance on medical question-answering benchmarks such as MedQA [21], MedMCQA [44], and PubMedQA [22], even surpassing human experts. However, these benchmarks use single-turn settings where patient data is readily provided, and models simply analyze these data to select the most likely diagnosis or treatment. In contrast, real-world clinicians engage in multi-turn, context-aware conversations to gather patient information actively. As result, these models may not guarantee effectiveness in practical clinical settings. To evaluate LLM-powered virtual doctors (i.e., doctor LLMs) in multi-turn settings, realistic patient interaction systems are needed. Traditionally, standardized patients (SPs) [4], trained actors simulating symptoms and histories, have been used to train and assess medical students communication and clinical skills. In this context, SPs could serve as benchmark for evaluating doctor LLMs by providing dynamic, interactive patient encounters. However, SPs are limited by high costs, inconsistent availability, and scaling challenges due to the need for human actors [13]. In contrast, LLM-based patient simulators provide scalable, accessible, and cost-effective alternative [8]. They reduce the need for repetitive human acting, eliminate geographic and time constraints, and lower costs compared to SPs. These advantages highlight the potential of AI as powerful tool for training and evaluating medical students [19, 34, 60], as well as doctor LLMs [14, 26, 32, 33, 35, 51, 57]. Co-corresponding author Preprint. Figure 1: Overall framework of PATIENTSIM. Based on 1) clinical profiles, including symptoms and medical history, derived from MIMIC-ED and MIMIC-IV datasets, and 2) personas defined by four axes: personality, language proficiency, medical history recall level, and cognitive confusion level, resulting in 37 unique combinations, PATIENTSIM provides realistic doctor-patient conversation data. Recent work highlights the potential of LLM-based patient simulators, but significant gap remains between these systems and real clinical settings. number of studies [33, 35, 38, 51] explored doctors interactive information-seeking abilities by providing LLMs with patient data and having them role-play patients. However, these studies focused on evaluating the performance of doctor LLMs, even though the validity of these evaluations depends on how closely patient simulators emulate actual patient behavior. Recognizing this importance, some studies [12, 14, 36] have begun evaluating patient simulators focusing on how accurately they convey symptomatic information. However, doctor-patient consultations are more than just patients accurately reciting their symptoms. Effective consultations must take into account patient behaviors dictated by multiple axes such as their emotional states and language skills, which significantly influence health outcomes. To this end, we propose PATIENTSIM, system that simulates diverse patient personas encountered in clinical settings (Figure 1). Our simulator acts based on: 1) clinical profiles, including symptoms and medical history, and 2) personas defined by four axes: personality, language proficiency, medical history recall level, and cognitive confusion level. Patient profiles are constructed based on real-world medical records from the MIMIC-ED [23] and MIMIC-IV [25] datasets, totaling 170 profiles (Sec. 4). For personas, we defined 37 distinct combinations across four axes, designed to reflect key factors impacting doctor-patient consultation quality, based on literature reviews and guided by medical experts (Sec. 5.1). We evaluate eight LLMs as the backbone of our simulator and select Llama 3.3 as the final model, which maintains persistent persona while ensuring factual accuracy (Sec. 7). The resulting simulator was assessed by four clinical experts and received an average quality score of 3.89 out of 4 across six criteria (Sec. 7). Our simulator offers the following contributions: PATIENTSIM introduces novel framework for simulating realistic doctor-patient interactions. It leverages real-world clinical data from MIMIC-IV and MIMIC-ED, modeling diverse patient personas across four axes: personality, language proficiency, medical history recall level, and cognitive confusion level. We conduct comprehensive evaluation across eight LLMs, assessing factual accuracy and persona reflection. To confirm the robustness of PATIENTSIM simulations, the top-performing opensource model, Llama 3.3, is further validated by four clinicians. Built on an open-source model, PATIENTSIM offers an accessible, reproducible tool for providing doctor-patient consultation data while prioritizing patient privacy. This scalable, privacy-compliant solution enables researchers and practitioners to validate their models performance and adapt it for clinical uses such as educational tools."
        },
        {
            "title": "2 Related work",
            "content": "LLM-based agent simulation in clinical setting LLM-based agent simulations in clinical settings vary by scope and agents. Previous studies [1, 3, 32, 61] simulate hospital workflows with 2 agents such as patients, nurses, and physicians, prioritizing final task accuracy (e.g., diagnositic or department recommendation accuracy) over agent interactions. Additionally, previous works such as Medagents [55] and MDAgents [29] focus on collaborative physician decision-making but are limited to single-turn QA settings. Recent studies emphasize doctor-patient interactions, evaluating physician LLMs in patient-centered communication [1, 36, 38, 51] or exploring their potential as educational tools [19, 34, 60]. However, these often overlook diverse patient characteristics, leading to insufficient realism in simulated interactions. As patient simulators are foundational to hospital simulations, providing primary clinical information and driving interaction dynamics, ensuring their realism is key challenge. Our research addresses this by developing an LLM-based patient simulator that delivers clinically coherent responses and reflects diverse patient characteristics. LLM patient simulation LLM-based patient simulation is divided into applications for general hospital consultations and psychological consultations, each with distinct objectives. In general hospital settings, patient simulations aim to accurately present medical history and symptoms through multi-turn dialogue [12, 14, 36, 38]. While most efforts primarily focus on implementing patient simulator that can respond to questions with factually correct answers, some studies [12, 38] tried to add bit of realism to the patient simulator by describing its personas with keywords such as the Big Five traits or occupation groups. No previous studies, however, aimed to implement and, at the same time, evaluate patient simulator that can emulate diverse and clinically relevant personas. Psychological counseling simulations, on the other hand, try to model complex internal states, such as thoughts, and emotions, emphasizing subjective responses like mood shifts or treatment resistance [39, 47, 58, 59]. These studies therefore prioritize deeper persona development to capture emotional and relational nuances, making them unsuitable for simulating patients in general diagnostic consultation settings such as hospital emergency departments. Unlike previous works, our study proposes realistic patient simulator that combines the emotional realism emphasized in psychological counseling with the clinical accuracy required for general diagnostic consultations."
        },
        {
            "title": "3 Problem definition",
            "content": "We target first-time, single-session emergency department (ED) visit, considering technical and clinical constraints below, to ensure feasible and coherent design. Limited access to comprehensive clinical data In real-world clinical practice, physicians integrate data from multiple dimensions such as patient-reported symptoms, physical exams, and test results (e.g., laboratory or imaging studies) to make diagnoses. However, when simulating patient, it is infeasible to predefine or dynamically generate clinically accurate data across all dimensions. As result, patient simulators often receive questions about data that are inaccessible. Prior approaches have addressed this by instructing LLMs to respond with vague statements like dont know [12, 33, 36] or to assume normal test results when data is not defined in the patients profile [14, 51]. These strategies restrict physicians to inquire predefined data, limiting their ability to explore diverse reasoning paths. Moreover, assuming normal test results can mislead physicians. To address this, we focus our simulation on the history-taking process, systematic approach to gather patients personal and medical information, prior to physical exam or lab tests [27]. Research shows that approximately 80% of diagnoses rely on history taking alone, highlighting its critical importance [18, 45]. Unlike objective data (e.g., test results), subjective data (e.g., patients verbal reports) inherently involves uncertainty [41, 52], allowing it to capture the variability in patients descriptions. This variability, driven by personal factors, supports our goal of creating realistic virtual patients with diverse personas, ensuring flexible and naturalistic interactions. Inability to simulate longitudinal patient state changes Realistic multi-session simulations require modeling treatment effects and disease progression over time, which are currently limited to very specific cases under heavy assumptions, and requires long-term research. Instead, we focus on single-session interactions, avoiding the need to model long-term outcomes or readmissions. Problem scope In the initial ED consultation setting, physicians often rely on verbal patient information, such as symptoms and medical history, for differential diagnosis under time pressure, before test results become available. Thus, we focus on differential diagnosis based on this initial consultation, which typically does not require test data. This approach ensures clinical relevance by reflecting real-world diagnostic reasoning while sidestepping current technical limitations."
        },
        {
            "title": "4 Patient profile construction",
            "content": "Structurized patient profile We construct detailed and structured profiles based on real clinical data from MIMIC-IV [23], MIMIC-IV-ED [25], and MIMIC-IV-Note [24], to ensure clinical relevance while minimizing ambiguity in the simulations. We extracted accurate patient data from structured tables and used clinical notes to capture detailed information, such as lifestyle and present symptoms, not included in the tables. This hybrid approach combined structured datas accuracy with the depth of narrative notes. Details on data sources and processing are provided in the Appendix A.1 and A.2. As result, each patient profile includes 24 items, covering demographics, social and medical history, and ED visit details (see Appendix A.3). Clinical experts reviewed each item for relevance. Target disease selection We select the five prevalent diseases from the MIMIC-IV-ED dataset: myocardial infarction, pneumonia, urinary tract infection, intestinal obstruction, and cerebral infarction (stroke). These conditions were chosen for their clinical significance, prevalence in ED, and distinct symptomatology, enabling meaningful differential diagnosis (DDx) tasks. The selection process was guided by two medical experts, one of whom is an ER doctor with 13 years of experience."
        },
        {
            "title": "5 PATIENTSIM",
            "content": "5.1 Persona definition We defined four key axes for persona simulation that impact consultation quality in clinical practice, based on literature reviews and guidance from medical experts. Personality Personality is well-established factor influencing consultation quality [7, 9, 37, 49]. The Big Five framework [40], one of the most widely recognized models of personality, has been used in previous patient simulation studies [12], but its traits are broad and tend to influence patientphysician interactions only indirectly. Recent psychological therapy research emphasizes observable conversational styles that directly manifest in patient interactions. Drawing on this, we adapt these styles into doctor-patient consultation-specific personality that are directly observable and actionable for simulation. Based on literature review [2, 6, 31, 53] and guidance from medical experts, we define six personalities relevant to medical consultations in ED: impatient, overanxious, distrustful, overly positive, verbose, and neutral (straightforward communication) as the baseline. Language proficiency patients language proficiency is critical determinant of doctor-patient communication quality [46, 54], yet it has been underexplored in simulation contexts. By specifying language proficiency levels, we simulate scenarios in which physicians must adapt to patients with varying proficiency by using appropriate language to ensure understanding. We use the Common European Framework of Reference for Languages (CEFR) [42], which defines six proficiency levels (A1, A2, B1, B2, C1, C2). To facilitate the human evaluation by physicians, we consolidated these into three levels, (basic), (intermediate), and (advanced). Medical history recall level Patients may not always accurately recall the details of their medical history [5, 30]. Assuming perfect recall, as in traditional settings, represents an idealized case. In low-recall scenarios, physicians must ask additional questions to build diagnostic confidence. We define two settings: high recall and low recall, enabling practice with diverse patient profiles. Level of cognitive confusion Patients visiting the ED often present acute symptom exacerbation, leading to highly confused and dazed state. These patients may initially struggle with coherent communication but stabilize through interaction. To simulate such cases, we define two mental status levels: highly confused and normal. To avoid overlap between confusion and other axes (e.g., impatient personality, low language proficiency, or low recall), highly confused patients are limited to neutral personality, intermediate language proficiency, and high recall. This results in 37 distinct personas; 36 from combinations of 6 personalities, 3 language proficiency levels, 2 recall levels, and 1 from high confusion persona. 5.2 Prompt design PATIENTSIM The PATIENTSIM prompt comprises profile information, four persona axes, and general behavioral guidelines. The prompt was iteratively refined through process of LLM evaluation, 4 qualitative analysis by the authors, and two rounds of feedback from medical experts. In the first round, two medical experts, who are also co-authors, provided feedback after engaging in extensive conversations with our simulators. The second round incorporated input from four additional medical experts external to the author group, based on their review of 10 sample cases. The full prompt is provided in the Appendix C.1. Doctor LLM Our research focuses on developing realistic patient simulators rather than doctor simulators. However, for automated evaluation, we require doctor LLM capable of asking appropriate questions to elicit and assess patient responses. To achieve this, the doctor prompt was carefully designed, drawing on medical textbook [56] and expert advice, to ensure it includes all essential, routine questions. The full prompt is provided in the Appendix C.2."
        },
        {
            "title": "6 Experiments",
            "content": "6.1 Task and evaluation To systematically assess the quality of LLM responses in terms of prompt alignment, we present experiments designed to address the following research questions. 6.1.1 RQ1: Do LLMs naturally reflect diverse persona traits in their responses? Realistic simulation of diverse and nuanced patient responses is crucial for training or evaluating PATIENTSIMs communication skills. We evaluate whether PATIENTSIM accurately reflects its assigned persona, across all 37 possible persona combinations. We assess four persona categories (i.e., personality, language proficiency, medical history recall level, confusion level), as well as overall realism to ensure that the model portrays the persona faithfully without exaggeration. The evaluation is divided into two folds. For automatic evaluation, we generate dialogues between PATIENTSIM and the doctor LLM across various persona settings, and then an LLM-based evaluator assesses the generated conversations. For human evaluation, four medical experts each engage in sufficient dialogue with the simulator across 37 persona samples (for total of 108 dialogues), after which they evaluate the quality of the simulator. Both human and LLM evaluators score the following categories on 4-point scale (1 = Strongly disagree, 4 = Strongly agree): The simulated patients personality is consistently and accurately reflected during the interaction. The patients language use (vocabulary, grammar, fluency) is appropriate to their assigned language proficiency level. The patients ability to recall medical and personal information is consistent with their assigned recall level (e.g., low or high). The patients coherence and clarity of thought match the assigned level of cognitive confusion. The patients overall communication style matched what would expect from real ED patient. 6.1.2 RQ2: Do LLMs accurately derive responses based on the given profile? In medical consultations, physicians rely on patient-reported information to form differential diagnosis. The quality of patient-provided information directly impacts the effectiveness of the dialogue and the correctness of the physicians conclusions. We evaluate factual accuracy at two levels: 1) sentence level and 2) dialogue level. The i-th patient profile, denoted as = {xi k=1, consists of predefined items, among total profiles. The dialogue between the physician and PATIENTSIM configured with profile is represented as Di. Within Di, PATIENTSIM utterances is the utterance at turn t. Each utterance ui t=1, where ui over turns are represented as = {ui may contain multiple sentences, denoted as ui = {si m=1, where is the number of sentences t. We classify si in utterance ui k, and unsupported if it is unrelated to any profile items. tm as supported if it is related to at least one profile item xi tm}M k}K t}T Sentence-level evaluation. To assess the accuracy of the PATIENTSIM responses precisely, we first analyze all of its responses at the sentence level. Here, we focus only on supported sentences, assessing their factual accuracy based on the patient profile. Evaluation of unsupported sentences is addressed separately in Sec. 6.1.3. We first explain how we detect supported sentences, and then describe how we calculate the factual accuracy of the supported sentences (Figure 2). Each step of the evaluation is performed by providing the LLM sentence classifier with the preceding 5 Figure 2: Overall process for sentence-level factuality evaluation. For each sentence in PATIENTSIMs utterance, we first determine whether it contains some information. If it does, we identify all relevant profile items and assess whether the sentence is supported by each of them. If the sentence includes information not specified in the profile, we classify it as unsupported and then assess its plausibility based on other profile information to determine plausibility rate. conversation history (i.e., the dialogue up to the current sentence si instructions (Appendix D.2.2). The evaluation proceeds as follows. For each sentence si tm: 1. Classify sentence type (multi-class, Step 1 in Figure 2): We categorize each sentence si of five types: politeness, emotion, inquiry, meta-information, or information (C(si sentences classified as information proceed to the next step. tm) along with step-specific tm as one tm)). Only the 2. Identify the related profile items (multi-label, Step 2-1 in Figure 2): For each sentence si classified as information, determine which of the patients profile items xi multi-label classification task, where sentence may relate to multiple xi vector R(si K], where: (cid:26)1 0 tm is related to item xi k, if si otherwise. tm) = [ri 2, . . . , ri ri = 1, ri tm it relates to. This is ks. The result is binary (1) where ri 3. Verify factual accuracy (Step 3-1 in Figure 2)): For each profile item xi = 1, perform Natural Language Inference (NLI) evaluation to check if the si tm aligns with xi k. NLI, method to evaluate textual consistency, labels the relationship as entailment (consistent), contradiction (inconsistent), or neutral (unrelated). We denote the NLI label as LI(si k) {entailment, contradiction, neutral}. If ri = 0, no NLI evaluation is performed for that item, as the sentence si k. The final factual accuracy of the sentence si tm is represented by Entail (%), calculated as follows, tm is deemed unrelated to xi tm, xi Entail(Di) = (cid:80)T t= (cid:80)M m=1 1[C(si tm) = info] maxk (cid:80)M (cid:80)T (cid:0)ri 1[N LI(si m=1 1[C(si tm) = info] t=1 tm, xi k) = entail](cid:1) (2) which reflects the percentage of supported sentences that are factually accurate. Dialogue-level evaluation Sentence-level accuracy may be biased if the physician fails to elicit comprehensive medical history, focusing only on specific topics. To mitigate this, we evaluate information coverage and the accuracy of the covered information at the dialogue level. First, we extract derived profile, ˆP = {ˆxi K}, inferred by the LLM profile extractor from the dialogue Di. We then compute the Information Coverage (ICov), the proportion of item categories present in both the derived profile ( ˆP i) and the original profile (P i): (cid:88) 2, . . . , ˆxi 1, ˆxi , where Oi = {j xi = , ˆxi = } (3) ICov = 1 i=1 Oi For overlapping item categories, we calculate Information Consistency (ICon) at the dialogue level: ICon = 1 (cid:32) (cid:88) i=1 1 Oi (cid:88) oOi (cid:33) score(xi j, ˆxi j) (4) 6 Table 1: Persona fidelity evaluation of various LLMs across five criteria, Personality, Language, Recall, Confused, and Realism, assessed by Gemini-2.5-flash. Each criterion is rated on 4-point scale. The average score (Avg.) summarizes overall performance. Engine Gemini-2.5-flash Gpt-4o-mini DeepSeek-R1-distill-Llama-70B Qwen2.5-72b-instruct Llama3.3-70b-instruct Llama3.1-70b-instruct Llama3.1-8b-instruct Qwen2.5-7b-instruct Personality Language Recall Confused Realism Avg. 3.94 3.58 3.87 3.30 3.92 3. 3.53 3.23 3.54 3.55 3.58 3.68 3.40 3.51 3.29 3.49 3.64 3.78 3.42 3.63 3.78 3. 3.70 3.31 3.38 3.88 2.50 3.50 4.00 4.00 4.00 3.50 3.37 3.26 3.19 3.22 3.28 3. 3.20 3.16 3.57 3.61 3.31 3.46 3.68 3.60 3.54 3.34 Table 2: Sentence-level factuality evaluation across eight LLMs, by Gemini-2.5-flash. Supported statements refer to sentences that relate to at least one item in the given profile. Unsupported statements include at least one piece of information that is not explicitly mentioned in the profile. Entail and Contradict are evaluated for supported, while Plausibility is assessed for unsupported. Info (%) Supported (%) Unsupported (%) For Supported For Unsupported Entail (%, ) Contradict (%, ) Plausibility () Gemini-2.5-flash Gpt-4o-mini DeepSeek-R1-distill-Llama-70B Qwen2.5-72b-instruct Llama3.3-70b-instruct Llama3.1-70b-instruct Llama3.1-8b-instruct Qwen2.5-7b-instruct 0.972 0.957 0.975 0.975 0.958 0.948 0.944 0.987 0.763 0. 0.762 0.683 0.796 0.813 0.771 0.703 0.316 0.428 0.416 0.468 0.387 0.407 0.488 0.453 0.978 0. 0.968 0.954 0.981 0.968 0.944 0.939 0.022 0.032 0.032 0.046 0.019 0.032 0.056 0.061 3.953 3. 3.911 3.928 3.963 3.955 3.897 3.862 j, ˆxi Here, score(xi item ˆxi scoring function. Prompts for the profile extractor and similarity scorer are in Appendix D.2.2. and the derived j. Similarity ratings are assigned on 4-point scale using the Gemini-2.5-Flash model as the j) measures the semantic similarity between the original item xi 6.1.3 RQ3: Can LLMs reasonably fill in the blanks? It is infeasible to predefine or generate clinically accurate information across all possible dimensions. Thus, patient simulators may encounter questions about information not explicitly described in the given profile. Unlike previous studies, which typically refuse to answer such questions (i.e., not allowing any unsupported sentences), thus limiting the flow of doctor-patient dialogue, we instead let PATIENTSIM answer such questions based on the given profile (i.e., allowing unsupported sentences). However, it is essential to assess the clinical plausibility of those unsupported sentences, in order to guarantee the overall clinical validity of PATIENTSIM, and its effectiveness as simulation tool. Therefore this evaluation targets unsupported sentences, statements containing at least one piece of information not explicitly present in the given profile (per RQ2). We start by identifying the information sentences (RQ2, Step 1). To classify an information sentence as unsupported, we evaluate whether each information sentence includes undefined information (Figure 2, Step 2-2), based on criteria detailed in the Appendix D.2.3. To maximize recall, we apply two additional rules: If no related profile items xi are found for sentence si unsupported, where R(si tm) is binary vector indicating related profile items (RQ2, Step 2). tm) = 0), it is deemed tm (i.e., (cid:80) R(si tm are neutral, it is classified as unsupported. If all NLI labels for si We use the LLM sentence classifier to classify unsupported sentences in patient utterances (Sec. 6.1.2). These identified sentences are then rated for plausibility on 4-point scale by both human and LLM evaluator (Figure 2, Step 3-2). Since plausibility judgments may vary based on medical expertise, potentially introducing bias, we assigned three different annotators to each sample and reported inter-clinician agreement to ensure robustness for human evaluation. 6.2 Experimental settings We randomly sampled total of 170 profiles and divided them into two subsets: 108 profiles for evaluating RQ1 (i.e., persona evaluation) and 52 profiles for evaluating RQ2 (i.e., factual accuracy) and RQ3 (i.e., clinical plausibility). We used 10 profiles to validate the LLM sentence classifiers 7 Table 3: Dialogue-level factuality evaluation across Social History (Social), Previous Medical History (PMH), and Current Visit Information (Current Visit), evaluated by Gemini-2.5-flash. Information Coverage (ICov) (%) Infomation Consistency (ICon) (4-point) Social PMH Current Visit Avg. Social PMH Current Visit Avg. Gemini-2.5-flash GPT-4o-mini DeepSeek-R1-distill-Llama-70B Qwen2.5-72B-instruct Llama3.3-70B-instruct Llama3.1-70B-instruct Llama3.1-8b-instruct Qwen2.5-7B-instruct 0.44 0.55 0.50 0.47 0.53 0. 0.61 0.44 0.77 0.76 0.76 0.77 0.78 0.77 0.78 0.75 0.88 0.89 0.91 0.90 0.89 0. 0.88 0.89 0.70 0.73 0.72 0.71 0.73 0.74 0.76 0.69 3.82 3.72 3.73 3.75 3.72 3. 3.68 3.60 3.51 3.33 3.31 3.50 3.47 3.43 3.19 3.32 3.18 3.01 3.08 2.95 3.10 3. 2.85 2.89 3.50 3.35 3.37 3.40 3.43 3.43 3.24 3.27 performance in automatically detecting supported and unsupported statements (RQ2, 3). Detailed statistics are provided in the Appendix A.4. As the LLM backbone of PATIENTSIM, we selected eight representative models: two API-based LLMs (Gemini-2.5 Flash [10], GPT-4o-mini [43]) and six open-source models (Llama 3.1 8B [16], Llama 3.1 70B [16], Llama 3.3 70B, Qwen2.5 72B [48], Qwen2.5 7B [48]). We selected GPT-4o-mini to play the role of the doctor. We employed Gemini2.5-Flash as an evaluator model to assess responses across all experiments, using task-specific rubrics. Detailed information about model selection is provided in the Appendix F.4. For human evaluation, we recruited four general practitioners 2 through Ingedata3, an AI data annotation company. Detailed information is provided in Appendix E."
        },
        {
            "title": "7 Results",
            "content": "In this section, we report the performance of various LLMs with respect to RQ1, 2, and 3, using LLMas-judge [62]. Based on these evaluations, we identified the best-performing model and conducted human evaluation to further validate its performance. RQ1: Do LLMs naturally reflect diverse persona traits in their responses? Table 1 presents the fidelity of various baseline LLMs across different persona axes. Results underscore Llamas strengths in simulation tasks, revealing that general LLM benchmark performance does not always correlate with simulation fidelity [20, 50]. The Llama series demonstrates robust performance, particularly in aspects related to emotional expression (i.e., Personality and Confused columns in Table 1). Notably, Llama 8B exhibits better fidelity than Qwen 72B, despite fewer parameters. The Confused column shows the highest variability among models, and most models struggle with negative emotions such as impatience and distrust, detailed in Appendix F.1. This may stem from safety measures in LLMs to avoid harmful responses, potentially limiting role-playing capabilities [11]. RQ2: Do LLMs accurately derive responses based on the given profile? We analyze the factual accuracy of sentences containing clinical information, focusing on statements explicitly mentioned in the given profile (supported). For each sentence, entailment is calculated with respect to all relevant profile items (Table 2, Entail column). All models demonstrate high entailment, but notable gap exists between larger models (70B parameters) and smaller models (8B parameters), with the latter more prone to incorrect statements. Unlike persona fidelity, information accuracy appears to correlate with model size, likely due to smaller models limited capacity to process long context compared to larger ones. Llama 70B models perform well in both aspects. Table 3 compares dialogue-level ICov and ICon across Social (i.e., social history), PMH (i.e., previous medical history), and Current Visit (e.g., chief complaint, present illnesses) categories. PMH and Current Visit have similar coverage across simulators, as they are standard in medical interviews. Social coverage varies based on context, as details like occupation or exercise are less frequently queried. Current Visit has lower consistency scores due to its subjective nature, needing detailed questions for full symptom capture. Among LLMs, Gemini-2.5-Flash leads in consistency, followed by Llama 3.3 and Llama 3.1 70B, the top open-source model. 2Two individuals have 4 years and two individuals have 6 years of clinical experience post-physician license. The latter two also hold nursing licenses, with 13 and 17 years of nursing experience, respectively. 3https://www.ingedata.ai/ 8 Table 4: Plausibility scores for unsupported sentences in patient responses, labeled by four clinicians, with three annotators per sentence (out of 4). Intra-clinician agreement measured by Gwets AC1 with 95% confidence intervals estimated via 1,000 bootstrap iterations. Clinician Clinician Clinician Clinician Intra-Clinician Agreement Clinician Clinician 0.949 (0.927, 0.969) Clinician 0.968 (0.951, 0.983) Clinician 0.866 (0.828, 0.901) 0.949 (0.927, 0.969) 0.961 (0.940, 0.979) 0.853 (0.818, 0.886) 0.968 (0.951, 0.983) 0.961 (0.940, 0.979) 0.879 (0.843, 0.913) 0.866 (0.828, 0.901) 0.853 (0.818, 0.886) 0.879 (0.843, 0.913) Plausibility 3.955 3.923 3.985 3. Plausibility (4 point scale) RQ3: Can LLMs reasonably fill in the blanks? To address this question, we focus on the unsupported sentences that include at least one piece of information not explicitly mentioned in the given profile. The plausibility column of Table 2 shows the plausibility ratings for answers about unspecified information. On average, evaluations are conducted on 764 sentences per model. Overall, larger models consistently demonstrate higher plausibility than smaller models. Smaller models are more likely to make additional statements that directly contradict their profiles medical history or their own prior statements, possibly due to limitations in processing long contexts. The Llama series again exhibits the best performance in this task, underscoring its potential to simulate realistic patient responses. Figure 3: Score distribution across six evaluation criteria, in clinician evaluation (4-point scale). Human evaluation As Llama 3.3 consistently demonstrated robust performance across all research questions, we selected it as the LLM for PATIENTSIM. For RQ1, clinicians engaged in approximately 1015 minutes of conversation for each case, and rated the interactions on six evaluation criteria (4-point scale). Figure 3 shows the score distribution across all criteria. Clinicians consistently assigned high scores, with an overall average of 3.89 out of 4. In addition to the five criteria used by the LLM-as-judge (Sec. 7, RQ1), clinicians also rated their agreement with the statement: This chatbot would be useful in education for practicing consultation skills. The average score was 3.75, highlighting the simulators potential as an effective educational tool. In RQ2, we evaluated the LLM sentence classifiers performance using manually annotated labels by authors across 411 sentences from 10 dialogues. The validation results are presented in Appendix F.4.1. For RQ3, Table 4 presents the plausibility scores from four different clinicians, along with their intra-clinician correlation. Each clinician evaluated 39 dialogues (about 616 sentences), carefully reviewing the patient profiles and conversation histories, spending approximately 6 minutes per dialogue. They assigned an average plausibility score of 3.91, with high agreement as measured by Gwets Agreement Coefficient (AC1) [17], demonstrating meaningful responses generated by our simulator. more detailed analysis is provided in the Appendix F."
        },
        {
            "title": "8 Discussion",
            "content": "Although we carefully designed the overall framework, several limitations remain: 1) Our experiment is based on the MIMIC database, given that it is currently the only publicly available dataset to integrate clinical notes with ED triage information. This may limit the generalizability of our findings. 2) Due to the text-based nature of our simulation environment, the simulator cannot capture nonverbal expressions (e.g., facial features, body movements), leading to limited persona representation. 3) Human evaluation was conducted with four clinicians, which could limit the generalizability of the evaluation results. To enhance the realism and generalizability of our framework, several avenues can be explored in future work. First, incorporating multimodal features (e.g., tone, facial expressions, or gestures), possibly via virtual reality (VR) simulations, would allow for more comprehensive modeling of patient personas. Second, increasing the scale and diversity of human evaluators can provide more reliable validation of LLM-based assessments."
        },
        {
            "title": "References",
            "content": "[1] M. Almansoori, K. Kumar, and H. Cholakkal. Self-evolving multi-agent simulations for realistic clinical interactions, 2025. URL https://arxiv.org/abs/2503.22678. [2] A. Banerjee and D. Sanyal. Dynamics of doctorpatient relationship: cross-sectional study on concordance, trust, and patient enablement. Journal of Family and Community Medicine, 19 (1):1219, 2012. doi: 10.4103/2230-8229.94006. [3] Z. Bao, Q. Liu, Y. Guo, Z. Ye, J. Shen, S. Xie, J. Peng, X. Huang, and Z. Wei. Piors: Personalized intelligent outpatient reception based on large language model with multi-agents medical scenario simulation, 2024. URL https://arxiv.org/abs/2411.13902. [4] H. S. Barrows. An overview of the uses of standardized patients for teaching and evaluating clinical skills. Academic Medicine, 68(6):443451, 1993. [5] G. S. Boyer, D. W. Templin, W. P. Goring, J. C. Cornoni-Huntley, D. F. Everett, R. C. Lawrence, S. P. Heyse, and A. Bowler. Discrepancies between patient recall and the medical record. potential impact on diagnosis and clinical assessment of chronic disease. Archives of Internal Medicine, 155(17):18681872, 1995. [6] F. Chipidza, R. S. Wallwork, T. N. Adams, and T. A. Stern. Evaluation and treatment of the angry patient. Primary Care Companion for CNS Disorders, 18(3), 2016. [7] G. B. Clack, J. Allen, D. Cooper, and J. O. Head. Personality differences between doctors and their patients: implications for the teaching of communication skills. Medical Education, 38(2): 177186, 2004. doi: 10.1111/j.1365-2923.2004.01752.x. [8] D. A. Cook. Creating virtual patients using large language models: scalable, global, and low cost. Medical Teacher, 47(1):4042, 2025. doi: 10.1080/0142159X.2024.2376879. [9] G. Cousin and M. Schmid Mast. Agreeable patient meets affiliative physician: How physician behavior affects patient outcomes depends on patient personality. Patient Education and Counseling, 90(3):399404, 2013. ISSN 0738-3991. Quality of Communication from the Patient Perspective. [10] G. DeepMind. Start building with gemini 2.5 flash, 2024. URL https://developers. googleblog.com/en/start-building-with-gemini-25-flash/. [11] A. Deshpande, V. Murahari, T. Rajpurohit, A. Kalyan, and K. Narasimhan. Toxicity in chatgpt: Analyzing persona-assigned language models. In H. Bouamor, J. Pino, and K. Bali, editors, Findings of the Association for Computational Linguistics: EMNLP 2023, pages 12361270, Dec. 2023. doi: 10.18653/v1/2023.findings-emnlp.88. URL https://aclanthology.org/ 2023.findings-emnlp.88/. [12] Z. Du, L. Zheng, R. Hu, Y. Xu, X. Li, Y. Sun, W. Chen, J. Wu, H. Cai, and H. Ying. Llms can simulate standardized patients via agent coevolution, 2024. URL https://arxiv.org/abs/ 2412.11716. [13] C. Elendu, D. C. Amaechi, A. U. Okatta, E. C. Amaechi, T. C. Elendu, C. P. Ezeh, and I. D. Elendu. The impact of simulation-based training in medical education: review. Medicine, 103(27):e38813, July 2024. doi: 10.1097/MD.0000000000038813. [14] Z. Fan, L. Wei, J. Tang, W. Chen, W. Siyuan, Z. Wei, and F. Huang. Ai hospital: Benchmarking large language models in multi-agent medical interaction simulator. In Proceedings of the 31st International Conference on Computational Linguistics, 2025. [15] A. L. Goldberger, L. A. N. Amaral, L. Glass, J. M. Hausdorff, P. C. Ivanov, R. G. Mark, J. E. Mietus, G. B. Moody, C.-K. Peng, and H. E. Stanley. Physiobank, physiotoolkit, and physionet: Components of new research resource for complex physiologic signals. Circulation [Online], 101(23):e215e220, 2000. doi: 10.1161/01.CIR.101.23.e215. [16] A. Grattafiori et al. The llama 3 herd of models, 2024. URL https://arxiv.org/abs/2407. 21783. 10 [17] K. L. Gwet. Computing inter-rater reliability and its variance in the presence of high agreement. British Journal of Mathematical and Statistical Psychology, 61(1):2948, 2008. doi: 10.1348/ 000711006X126600. [18] J. R. Hampton, M. J. G. Harrison, J. R. A. Mitchell, J. S. Prichard, and C. Seymour. Relative contributions of history-taking, physical examination, and laboratory investigation to diagnosis and management of medical outpatients. British Medical Journal, 2(5969):486489, May 1975. doi: 10.1136/bmj.2.5969.486. [19] Y. Hicke, J. Geathers, N. Rajashekar, C. Chan, A. G. Jack, J. Sewell, M. Preston, S. Cornes, D. Shung, and R. Kizilcec. Medsimai: Simulation and formative feedback generation to enhance deliberate practice in medical education, 2025. URL https://arxiv.org/abs/2503.05793. [20] Y. Huang, Z. Yuan, Y. Zhou, K. Guo, X. Wang, H. Zhuang, W. Sun, L. Sun, J. Wang, Y. Ye, and X. Zhang. Social science meets llms: How reliable are large language models in social simulations?, 2024. URL https://arxiv.org/abs/2410.23426. [21] D. Jin, E. Pan, N. Oufattole, W.-H. Weng, H. Fang, and P. Szolovits. What disease does this patient have? large-scale open domain question answering dataset from medical exams. arXiv preprint arXiv:2009.13081, 2020. [22] Q. Jin, B. Dhingra, Z. Liu, W. Cohen, and X. Lu. Pubmedqa: dataset for biomedical research question answering. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 25672577, 2019. [23] A. Johnson, L. Bulgarelli, T. Pollard, L. A. Celi, R. Mark, and S. Horng. MIMIC-IV-ED (version 2.2). https://doi.org/10.13026/5ntk-km72, 2023. PhysioNet. [24] A. Johnson, T. Pollard, S. Horng, L. A. Celi, and R. Mark. Mimic-iv-note: Deidentified free-text clinical notes (version 2.2). https://doi.org/10.13026/1n74-ne17, 2023. PhysioNet. [25] A. Johnson, L. Bulgarelli, T. Pollard, B. Gow, B. Moody, S. Horng, L. A. Celi, and R. Mark. MIMIC-IV (version 3.1). https://doi.org/10.13026/kpb9-mt58, 2024. PhysioNet. [26] S. Johri, J. Jeong, B. A. Tran, D. I. Schlessinger, S. Wongvibulsin, Z. R. Cai, R. Daneshjou, and P. Rajpurkar. CRAFT-MD: conversational evaluation framework for comprehensive assessment of clinical LLMs. In AAAI 2024 Spring Symposium on Clinical Foundation Models, 2024. [27] K. E. Keifenheim, M. Teufel, J. Ip, N. Speiser, E. J. Leehr, S. Zipfel, and A. Herrmann-Werner. Teaching history taking to medical students: systematic review. BMC Medical Education, 15 (1):159, 2015. doi: 10.1186/s12909-015-0443-x. [28] S. Kim, J. Shin, Y. Cho, J. Jang, S. Longpre, H. Lee, S. Yun, S. Shin, S. Kim, J. Thorne, and M. Seo. Prometheus: Inducing fine-grained evaluation capability in language models. In The Twelfth International Conference on Learning Representations, 2024. URL https: //openreview.net/forum?id=8euJaTveKw. [29] Y. Kim, C. Park, H. Jeong, Y. S. Chan, X. Xu, D. McDuff, H. Lee, M. Ghassemi, C. Breazeal, and H. W. Park. Mdagents: An adaptive collaboration of llms for medical decision-making. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. [30] M. B. Laws, Y. Lee, T. Taubin, W. H. Rogers, and I. B. Wilson. Factors associated with patient recall of key information in ambulatory specialty care visits: Results of an innovative methodology. PLoS ONE, 13(2):e0191940, 2018. doi: 10.1371/journal.pone.0191940. [31] A. M. Legg, S. E. Andrews, H. Huynh, A. Ghane, A. Tabuenca, and K. Sweeny. Patients anxiety and hope: predictors and adherence intentions in an acute care context. Health Expectations, 18 (6):30343043, 2015. doi: 10.1111/hex.12288. [32] J. Li, Y. Lai, W. Li, J. Ren, M. Zhang, X. Kang, S. Wang, P. Li, Y.-Q. Zhang, W. Ma, and Y. Liu. Agent hospital: simulacrum of hospital with evolvable medical agents, 2025. 11 [33] S. S. Li, V. Balachandran, S. Feng, J. S. Ilgen, E. Pierson, P. W. Koh, and Y. Tsvetkov. Mediq: Question-asking LLMs and benchmark for reliable interactive clinical reasoning. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. [34] Y. Li, C. Zeng, J. Zhong, R. Zhang, M. Zhang, and L. Zou. Leveraging large language model as simulated patients for clinical education, 2024. URL https://arxiv.org/abs/2404.13066. [35] Y. Liao, Y. Meng, H. Liu, Y. Wang, and Y. Wang. An automatic evaluation framework for multi-turn medical consultations capabilities of large language models, 2023. URL https: //arxiv.org/abs/2309.02077. [36] Y. Liao, Y. Meng, Y. Wang, H. Liu, Y. Wang, and Y. Wang. Automatic interactive evaluation for large language models with state aware patient simulator, 2024. [37] S. D. Lifchez and R. J. Redett. standardized patient model to teach and assess professionalism and communication skills: The effect of personality type on performance. Journal of Surgical Education, 71(3):297301, 2014. ISSN 1931-7204. doi: https://doi.org/10.1016/j.jsurg.2013.09. 010. [38] H. Liu, Y. Liao, S. Ou, Y. Wang, H. Liu, Y. Wang, and Y. Wang. Med-pmc: Medical personalized multi-modal consultation with proactive ask-first-observe-next paradigm, 2024. URL https: //arxiv.org/abs/2408.08693. [39] R. Louie, A. Nandi, W. Fang, C. Chang, E. Brunskill, and D. Yang. Roleplay-doh: Enabling domain-experts to create LLM-simulated patients via eliciting and adhering to principles. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 1057010603, 2024. [40] R. R. McCrae and P. T. Costa. Validation of the five-factor model of personality across instruments and observers. Journal of Personality and Social Psychology, 52(1):8190, 1987. [41] A. N. Meyer, T. D. Giardina, L. Khawaja, and H. Singh. Patient and clinician experiences of uncertainty in the diagnostic process: Current understanding and future directions. Patient Education and Counseling, 104(11):26062615, 2021. ISSN 0738-3991. doi: https://doi.org/10. 1016/j.pec.2021.07.028. [42] C. of Europe. Common European Framework of Reference for Languages: Learning, Teaching, Assessment. Cambridge University Press, Cambridge, 2001. [43] OpenAI. Gpt-4o mini: advancing cost-efficient intelligence, 2024. URL https://openai. com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/. [44] A. Pal, L. K. Umapathi, and M. Sankarasubbu. Medmcqa: large-scale multi-subject multichoice dataset for medical domain question answering. In Proceedings of the Conference on Health, Inference, and Learning, pages 248260, 2022. [45] M. C. Peterson, J. H. Holbrook, D. Von Hales, N. L. Smith, and L. V. Staker. Contributions of the history, physical examination, and laboratory investigation in making medical diagnoses. Western Journal of Medicine, 156(2):163165, 1992. [46] E. J. Pérez-Stable and S. El-Toukhy. Communicating with diverse patients: How patient and clinician factors affect disparities. Patient Education and Counseling, 101(12):21862194, 2018. ISSN 0738-3991. doi: https://doi.org/10.1016/j.pec.2018.08.021. [47] H. Qiu and Z. Lan. Interactive agents: Simulating counselor-client psychological counseling via role-playing llm-to-llm interactions, 2024. URL https://arxiv.org/abs/2408.15787. [48] Qwen, :, A. Yang, B. Yang, B. Zhang, B. Hui, B. Zheng, B. Yu, C. Li, D. Liu, F. Huang, H. Wei, H. Lin, J. Yang, J. Tu, J. Zhang, J. Yang, J. Yang, J. Zhou, J. Lin, K. Dang, K. Lu, K. Bao, K. Yang, L. Yu, M. Li, M. Xue, P. Zhang, Q. Zhu, R. Men, R. Lin, T. Li, T. Tang, T. Xia, X. Ren, X. Ren, Y. Fan, Y. Su, Y. Zhang, Y. Wan, Y. Liu, Z. Cui, Z. Zhang, and Z. Qiu. Qwen2.5 technical report, 2025. URL https://arxiv.org/abs/2412.15115. 12 [49] D. A. Redelmeier, U. Najeeb, and E. E. Etchells. Understanding patient personality in medical care: Five-factor model. Journal of General Internal Medicine, 36(7):21112114, 2021. ISSN 1525-1497. doi: 10.1007/s11606-021-06598-8. [50] V. Samuel, H. P. Zou, Y. Zhou, S. Chaudhari, A. Kalyan, T. Rajpurohit, A. Deshpande, K. Narasimhan, and V. Murahari. Personagym: Evaluating persona agents and llms. arXiv preprint arXiv:2407.18416, 2024. [51] S. Schmidgall, R. Ziaei, C. Harris, E. Reis, J. Jopling, and M. Moor. Agentclinic: multimodal agent benchmark to evaluate ai in simulated clinical environments, 2024. [52] E. Stolper, P. Van Royen, E. Jack, J. Uleman, and M. Olde Rikkert. Embracing complexity with systems thinking in general practitioners clinical reasoning helps handling uncertainty. Journal of Evaluation in Clinical Practice, 27(5):11751181, 2021. doi: https://doi.org/10.1111/jep. 13549. [53] D. E. Stubbe. Alleviating anxiety: Optimizing communication with the anxious patient. Focus (Am Psychiatr Publ), 15(2):182184, 2017. doi: 10.1176/appi.focus.20170001. [54] R. L. Sudore, C. S. Landefeld, E. J. Pérez-Stable, K. Bibbins-Domingo, B. A. Williams, and D. Schillinger. Unraveling the relationship between literacy, language proficiency, and patientphysician communication. Patient Education and Counseling, 75(3):398402, 2009. doi: 10.1016/j.pec.2009.02.019. [55] X. Tang, A. Zou, Z. Zhang, Z. Li, Y. Zhao, X. Zhang, A. Cohan, and M. Gerstein. MedAgents: Large language models as collaborators for zero-shot medical reasoning. In Findings of the Association for Computational Linguistics: ACL 2024, 2024. [56] E. C. Toy, B. Simon, K. Takenaka, T. H. Liu, and A. J. Rosh. Case Files Emergency Medicine. McGraw-Hill Education / Medical, New York, 4th edition, 2017. ISBN 9781259640827. [57] T. Tu, A. Palepu, M. Schaekermann, K. Saab, J. Freyberg, R. Tanno, A. Wang, B. Li, M. Amin, N. Tomasev, S. Azizi, K. Singhal, Y. Cheng, L. Hou, A. Webson, K. Kulkarni, S. S. Mahdavi, C. Semturs, J. Gottweis, J. Barral, K. Chou, G. S. Corrado, Y. Matias, A. Karthikesalingam, and V. Natarajan. Towards conversational diagnostic ai, 2024. URL https://arxiv.org/abs/ 2401.05654. [58] J. Wang, Y. Xiao, Y. Li, C. Song, C. Xu, C. Tan, and W. Li. Towards client-centered assessment of llm therapists by client simulation, 2024. URL https://arxiv.org/abs/2406.12266. [59] R. Wang, S. Milani, J. C. Chiu, J. Zhi, S. M. Eack, T. Labrum, S. M. Murphy, N. Jones, K. V. Hardy, H. Shen, F. Fang, and Z. Chen. PATIENT-ψ: Using large language models to simulate patients for training mental health professionals. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 1277212797, 2024. [60] H. Wei, J. Qiu, H. Yu, and W. Yuan. Medco: Medical education copilots based on multi-agent framework, 2024. URL https://arxiv.org/abs/2408.12496. [61] W. Yan, H. Liu, T. Wu, Q. Chen, W. Wang, H. Chai, J. Wang, W. Zhao, Y. Zhang, R. Zhang, L. Zhu, and X. Zhao. Clinicallab: Aligning agents for multi-departmental clinical diagnostics in the real world, 2024. URL https://arxiv.org/abs/2406.13890. [62] L. Zheng, W.-L. Chiang, Y. Sheng, S. Zhuang, Z. Wu, Y. Zhuang, Z. Lin, Z. Li, D. Li, E. Xing, H. Zhang, J. E. Gonzalez, and I. Stoica. Judging LLM-as-a-judge with MT-bench and chatbot arena. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023. URL https://openreview.net/forum?id=uccHPGDlao."
        },
        {
            "title": "Appendix",
            "content": "A Patient profile construction A.1 Database . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.2 Database preprocessing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.3 Structurized patient profile . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.3.1 Profile items . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.3.2 Note preprocessing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.4 Profile statistics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Persona details for PATIENTSIM B.1 Personality . . . . . . B.2 Language proficiency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.3 Medical history recall level . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.4 Cognitive confusion level . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Simulation of doctor-patient interaction C.1 PATIENTSIM . C.2 Doctor . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Experimental settings D.1 Model configurations and dataset details . . . . . . . . . . . . . . . . . . . . . . . D.2 LLM evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.2.1 RQ1: Do LLMs naturally reflect diverse persona traits in their responses? . D.2.2 RQ2: Do LLMs accurately derive responses based on the given profile? . . D.2.3 RQ3: Can LLMs reasonably fill in the blanks? . . . . . . . . . . . . . . . Human evaluation E.1 Clinician recruitment for evaluation . . . . . . . . . . . . . . . . . . . . . . . . . E.2 Persona fidelity evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.3 Plausibility evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Experimental results F.1 RQ1: Do LLMs naturally reflect diverse persona traits in their responses? . . . . . F.1.1 Additional result of LLM evaluation . . . . . . . . . . . . . . . . . . . . . F.1.2 Additional result of human evaluation . . . . . . . . . . . . . . . . . . . . F.2 RQ2: Do LLMs accurately derive responses based on the given profile? . . . . . . F.3 RQ3: Can LLMs reasonably fill in the blanks? . . . . . . . . . . . . . . . . . . . . F.4 Ablation study . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . F.4.1 Validation of sentence-level classification . . . . . . . . . . . . . . . . . . F.4.2 Ablation study on doctor LLM . . . . . . . . . . . . . . . . . . . . . . . . 15 15 15 16 16 17 21 21 21 21 23 23 23 27 27 27 28 32 33 33 33 35 35 35 36 39 39 39 40 Responsible use and limitations"
        },
        {
            "title": "A Patient profile construction",
            "content": "A.1 Database For our research, we utilize datasets from PhysioNet [15], adhering to the required credentials and permissions under the PhysioNet license. The datasets used are MIMIC-IV (v3.1) [25], MIMIC-IVED (v2.2) [23], and MIMIC-IV-Note (v2.2) [24]. MIMIC-IV (v3.1) MIMIC-IV4 is comprehensive, deidentified dataset of patients admitted to the emergency department (ED) or intensive care unit (ICU) at Beth Israel Deaconess Medical Center (BIDMC) in Boston, MA. It includes data for over 65,000 ICU patients and over 200,000 ED patients. With modular data organization emphasizing provenance, MIMIC-IV supports both individual and integrated use of diverse data sources, making it rich resource for patient information extraction. MIMIC-IV-ED (v2.2) MIMIC-IV-ED 5 is freely accessible database of 425,087 ED admissions at BIDMC from 2011 to 2019. It contains deidentified data compliant with the HIPAA Safe Harbor provision, including vital signs, triage information, medication reconciliation, medication administration, and discharge diagnoses. MIMIC-IV-Note (v2.2) MIMIC-IV-Note 6 provides 331,794 deidentified discharge summaries for 145,915 patients admitted to the hospital or ED at BIDMC. All notes comply with HIPAA Safe Harbor provisions by removing protected health information and are linkable to MIMIC-IV, offering valuable clinical context. A.2 Database preprocessing To integrate patient information from both structured tables and free-text data, we selected patients from MIMIC-IV-ED (v2.2) with triage information and diagnosis records, and corresponding freetext discharge summaries from MIMIC-IV-Note (v2.2). This selection ensured access to detailed subjective symptoms, primarily captured in free-text notes rather than structured tables. We apply the following criteria to filter the data (Figure A1). From the resulting cohort, we randomly sampled up to 40 patient records per diagnosis category to ensure class balance and manage dataset size. Cohort selection criteria are as follows: Each hospital admission (hadm_id) must include exactly one ED stay. Admissions with multiple ED stays were excluded. To ensure diagnostic clarity, we included only ED stays with single diagnosis code. We excluded records with missing or unknown values in the fields marital_status, insurance, race, chiefcomplaint, or arrival_transport. Pain scores were converted to numeric values based on field definitions. Non-numeric values and scores outside the 010 range were treated as outliers and removed. We cap the maximum number of medication per patients at 15. The History of Present Illness (HPI) section was limited to maximum of 350 words and minimum of 10 words. The Past Medical History (PMH) section was limited to maximum of 80 words. To ensure the accuracy of symptom descriptions, we excluded records where the chiefcomplaint field or the Complaint or HPI sections of the discharge notes contained terms such as coma, stupor, or altered mental status. To avoid potential confounds related to language fluency, we excluded records where the chiefcomplaint field or the Complaint or HPI sections contained terms such as slurred speech, dysarthria, or aphasia. 4https://physionet.org/content/mimiciv/3.1/ 5https://physionet.org/content/mimic-iv-ed/2.2/ 6https://physionet.org/content/mimic-iv-note/2.2/ Figure A1: Overview of data preprocessing for selecting patient records from MIMIC-IV, MIMICIV-ED and MIMIC-IV-Note. A.3 Structurized patient profile A.3.1 Profile items We extracted accurate patient-related data from structured tables and used clinical notes to capture nuanced information, such as lifestyle and current symptoms, not found in the tables. Each patient profile consists of 24 items, including demographic details (age, gender, race), social history (illicit drug use, exercise, marital status, sexual history, children, living situation, occupation, insurance), medical history (allergies, family medical history, medical devices, past medical history), subjective information related to emergency department (ED) admission (history of present illness, chief complaint, pain level, medications), and meta-information about the ED visit (arrival transport, disposition, diagnosis). The source for each item is summarized in Table A1. For simplicity, we use shortened dataset paths, like mimic-iv-ed/edstays instead of mimic-iv-ed/2.2/ed/edstays. Disposition and diagnosis are not shared with the doctor but are included to enhance the simulators understanding of the patients status and condition severity during role-playing as the patient. Sexual history is included only for patients admitted due to urinary tract infections, based on feedback from doctors. Table A1: Data sources and corresponding patient profile items used in PATIENTSIM Source Profile Items mimiciv/hosp/admissions insurance, marital_status mimiciv/hosp/patients mimic-iv-ed/edstays mimic-iv-ed/triage mimic-iv-ed/medrecon mimic-iv-ed/diagnosis mimic-iv-note/discharge occupation, living situation, children, exercise, tobacco, alcohol, illicit drug, sexual history, allergics, medical history, familiy medical history, medical device, present illness age gender, race, arrival_transport, disposition chiefcomplaint, pain medication icd_title 16 A.3.2 Note preprocessing To extract structured information from free-text discharge notes, we use the Gemini-2.5-flash model, configured with 1024 thinking tokens to enable robust reasoning. This model is selected for its strong performance on natural language processing benchmarks [10]. Our preprocessing pipeline consists of three steps, each guided by task-specific instructions. First, we extract 13 key items (e.g., chief complaint, medical history, medications) as defined in Table A1, mimic-iv-note/discharge row. This extraction is performed using structured prompts, shown in Figure A2 and Figure A3, designed to retrieve relevant information from discharge notes. Second, we filter out patient profiles where the extracted information does not align with the ED diagnosis table to ensure dataset reliability. This step reduces noise arising from documentation errors or significant changes in patient condition after ED admission. An evaluation prompt (Figure A4) instructs the LLM to assess the alignment between each patient profile and the ED diagnosis on 5-point scale (1 = no match, 5 = perfect match). Only profiles scoring 3 or higher are retained to maintain clinical coherence. Third, to generate comprehensive patient profiles for simulation, we infer missing lifestyle and habit items, such as exercise, tobacco use, alcohol consumption, living situation, and occupation, based on existing profile information (Figure A5). These attributes are often inconsistently documented due to their varying clinical relevance. Since they enhance the realism of patient simulations without typically affecting critical outcomes, we impute missing values in context-aware manner using the LLM, drawing on available demographics and medical history. Any existing valid information is preserved. This step yields coherent and realistic profiles that improve the quality of downstream simulations. A.4 Profile statistics As result of the above pipeline, we obtain final set of 170 patient profiles. Table A.4 presents detailed statistics on the demographic and clinical characteristics of these profiles. Age is grouped into 10-year intervals. Numerical variables (i.e., age, pain score) are sorted by value, while categorical variables are ordered by descending frequency. Table A2: Detailed patient profile statistics for PATIENTSIM, based on total of 170 patient profiles. Category Age Group Gender Race Distribution 20-30: 9 (5.3%), 30-40: 7 (4.1%), 40-50: 18 (10.6%), 50-60: 29 (17.1%), 60-70: 37 (21.8%), 70-80: 33 (19.4%), 80-90: 30 (17.6%), 90-100: 7 (4.1%) Female: 88 (51.8%), Male: 82 (48.2%) White: 106 (62.4%), Black/African American: 24 (14.1%), Asian - Chinese: 6 (3.5%), Black/Cape Verdean: 6 (3.5%), Hispanic/Latino - Puerto Rican: 6 (3.5%), Other: 5 (2.9%), Asian: 2 (1.2%), Asian - Asian Indian: 2 (1.2%), Hispanic/Latino - Dominican: 2 (1.2%), White - Other European: 2 (1.2%), White - Russian: 2 (1.2%), Asian - South East Asian: 1 (0.6%), Black/African: 1 (0.6%), Hispanic/Latino - Central American: 1 (0.6%), Hispanic/Latino - Colombian: 1 (0.6%), Hispanic/Latino - Guatemalan: 1 (0.6%), Hispanic/Latino - Mexican: 1 (0.6%), Hispanic/Latino - Salvadoran: 1 (0.6%) Marital Status Married: 84 (49.4%), Single: 51 (30.0%), Widowed: 24 (14.1%), Divorced: 11 (6.5%) Insurance Medicare: 84 (49.4%), Private: 55 (32.4%), Medicaid: 23 (13.5%), Other: 8 (4.7%) Arrival Transport Walk In: 95 (55.9%), Ambulance: 74 (43.5%), Other: 1 (0.6%) Disposition Pain Score Diagnosis Admitted: 164 (96.5%), Other: 6 (3.5%) 0: 82 (48.2%), 1: 3 (1.8%), 2: 5 (2.9%), 3: 10 (5.9%), 4: 11 (6.5%), 5: 6 (3.5%), 6: 7 (4.1%), 7: 12 (7.1%), 8: 14 (8.2%), 9: 5 (2.9%), 10: 15 (8.8%) Intestinal obstruction: 39 (22.9%), Pneumonia: 34 (20.0%), Urinary tract infection: 34 (20.0%), Myocardial infarction: 34 (20.0%), Cerebral infarction: 29 (17.1%) 17 Prompt template for structurizing clinical notes [System Promt] You are an AI assistant designed to extract structured medical information from electronic health records (EHRs). Your task is to analyze the EHR content and extract all relevant information into predefined categories. Complete the fields below using the EHRs. Include only events that occurred before the most recent ED admission, and exclude any test results collected afterward. Return the extracted information in the following valid JSON format. Field Definitions: - demographics: occupation: The patients current job or employment status. living_situation: Who the patient lives with, or their housing situation. children: Number and gender of the patients children. - social_history: exercise: Type(s) and frequency of physical activity or exercise. tobacco: Any use of tobacco, including type, amount, and frequency. alcohol: Alcohol consumption details, including type, frequency, and amount. illicit_drug: Use of non-prescribed or illegal substances, including type, amount, and frequency. sexual_history: Sexual activity, including partner(s), protection use, frequency, and timing. - allergies: Any known allergies, including type of reaction if available. - medical_history: Past medical conditions or diagnoses, including chronic conditions and any details like onset. - family_medical_history: Medical conditions in family members, with relevant details if available. - medical_device: Any medical or assistive devices in current use, including context or usage dates if noted. - present_illness: positive: Recent symptoms or conditions before the ED visit, with all existing relevant details such as onset, duration, severity, or progression. Do not include lab or imaging test results or diagnosis names. negative: Symptoms or conditions the patient explicitly denies having. Output Format (JSON): { \"demographics\": { \"occupation\": \"\", \"living_situation\": \"\", \"children\": \"\" }, \"social_history\": { \"exercise\": \"\", \"tobacco\": \"\", \"alcohol\": \"\", \"illicit_drug\": \"\", \"sexual_history\": \"\" }, \"allergies\": \"\", \"medical_history\": \"\", \"family_medical_history\": \"\", \"medical_device\": \"\", \"present_illness\": { \"positive\": \"\", \"negative\": \"\" } } Guidelines: 1. Extract each field from the entire EHR with complete accuracy. 2. Keep each field concise and keyword-based phrases without full sentences or narrative descriptions. 3. Express information briefly, avoiding verbs, pronouns, or unnecessary words. 4. If field contains multiple values, combine them into single string separated by semicolons. 5. Treat de-identified placeholders as nonexistent. Do not include placeholders like ___ in any of output fields. 6. Return Not recorded for any field not mentioned in the EHR. Figure A2: System prompt template for extracting and structuring electronic health record (EHR) data into predefined fields in JSON format, capturing patient information prior to the latest ED admission. Braced elements {} are substituted with values specific to each patient record. 18 Prompt template for structurizing clinical notes [User Promt] Patients Electronic Health Record (EHR): - Allergies: {Allergies} - Chief Complaint: {Chief Complaint} - History of Present Illness: {History of Present Illness} - Past Medical History: {Past Medical History} - Social History: {Social History} - Family History: {Family History} Figure A3: User prompt template for extracting and structuring electronic health record (EHR) data. Braced elements {} are substituted with values specific to each patient record. Prompt template for profile validation [System Promt] You are helpful medical assistant. Please evaluate how likely it is that patients profile aligns with given diagnosis. Predict the likelihood of the diagnosis based on the profile provided. Rate the likelihood on scale from 1 to 5, where 1 means the patients history and symptoms do not match the diagnosis at all, and 5 means the patients history and symptoms fully align with the diagnosis. Please generate your output as valid JSON dictionary in the following format: { } \"explanation\": reason for the rating, \"likelihood_rating\": 1 to 5 [User Promt] Patients Profile: Demographics: Age: {age} Gender: {gender} Race: {race} Social History: Tobacco: {tobacco} Alcohol: {alcohol} Illicit drug use: {illicit_drug} Sexual History: {sexual_history} Exercise: {exercise} Marital status: {marital_status} Children: {children} Living Situation: {living_situation} Occupation: {occupation} Insurance: {insurance} Previous Medical History: Allergies: {allergies} Family medical history: {family_medical_history} Medical devices used before this ED admission: {medical_device} Medical history prior to this ED admission: {medical_history} Current Visit Information: Present illness: - positive: {present_illness_positive} - negative (denied): {present_illness_negative} ED chief complaint: {chiefcomplaint} Pain level at ED Admission (0 = no pain, 10 = worst pain imaginable): {pain} Current medications they are taking: {medication} ED Arrival Transport: {arrival_transport} ED disposition: {disposition} Diagnosis: {diagnosis} Figure A4: Prompt template for scoring the alignment between patient records and diagnosis. Braced elements {} are substituted with patient-specific values. 19 Prompt template for completing patient profiles [System Prompt] You are an AI assistant specializing in processing and completing lifestyle information for individuals. Your task is to analyze the provided electronic health records (EHRs) and update the profile section by filling in any missing details with realistic, plausible responses. Field Definitions: - demographics: occupation: The patients current job or employment status. living_situation: Who the patient lives with, or their housing situation. children: Number and gender of the patients children. - social_history: exercise: Type(s) and frequency of physical activity or exercise. tobacco: Any use of tobacco, including type, amount, and frequency. alcohol: Alcohol consumption details, including type, frequency, and amount. illicit_drug: Use of non-prescribed or illegal substances, including type, amount, and frequency. sexual_history: Sexual activity, including partner(s), protection use, frequency, and timing. Guidelines: 1. For any field marked as Not recorded, generate realistic and plausible entry that aligns with the patients EHR and other profile information. 2. For fields containing placeholders like ___, replace the placeholder with plausible values based on the fields context and the patients profile. 3. Do not modify any field that already contains valid data, except for placeholders (___). 4. Use clear language, while preserving appropriate medical or social context. 5. Convert first-person responses to third-person. For example, change live alone to Lives alone. 6. Do not refer to the individual using gendered pronouns (he or she). Use gender-neutral phrasing. 7. Represent each field as string. Use semicolons to separate multiple items within the same field. [User Prompt] Patients Electronic Health Record (EHR): - Age: {age} - Gender: {gender} - Race: {race} - Marital Status: {marital_status} - Insurance: {insurance} - Medical device: {medical_device} - Medical history: {medical_history} - Present illness: {present_illness} - Family medical history: {family_medical_history} Patient Profile Template (to complete): { \"demographics\": { \"occupation\": \"{occupation}\", \"living_situation\": \"{living_situation}\", \"children\": \"{children}\" }, \"social_history\": { \"exercise\": \"{exercise}\", \"tobacco\": \"{tobacco}\", \"alcohol\": \"{alcohol}\", \"illicit_drug\": \"{illicit_drug}\", \"sexual_history\": \"{sexual_history}\" } } Figure A5: Prompt template for completing the patients social history section, using EHR context. Braced elements {} are substituted with patient-specific values."
        },
        {
            "title": "B Persona details for PATIENTSIM",
            "content": "B.1 Personality We outline six patient personalities relevant to medical consultations in the ED: distrustful, impatient, overanxious, overly positive, and verbose, with neutral (straightforward communication) as the baseline. Table B3 provides descriptions of each personality type. These prompts have been reviewed and validated by medical experts. B.2 Language proficiency We adopt the Common European Framework of Reference for Languages (CEFR), an international standard for assessing language proficiency, and simplify it into three levels: (basic), (intermediate), and (advanced). The prompts for each level are shown in Table B4. These prompts are designed based on CEFRs official reference points, self-assessment grid, and qualitative descriptors of spoken language use 7. To represent level-appropriate vocabulary, we use CEFR-labeled word dictionary from Kaggle8. As this dataset lacks medical terminology, we generate complementary medical-domain vocabulary using Gpt-4o-1120. Gpt-4o generated 30 medical terms per CEFR level in each of three iterations, resulting in pool of candidate terms for each proficiency level. Only terms that appeared in at least two iterations were retained, and overlapping terms across levels were removed to ensure level-specific clarity. From the general and medical vocabulary sets, we randomly sample 10 words per CEFR level for each patient profile. These sampled words populate the fields understand_words, misunderstand_words, understand_med_words, and misunderstand_med_words, representing the words patient is likely to understand or misunderstand in both general and medical contexts, based on their assigned language proficiency. Since the sampled words vary across profiles even within the same CEFR level, this approach allows us to reflect individual variation in language comprehension among patients with the same overall proficiency level. B.3 Medical history recall level We define medical history recall at two levels: high and low. Detailed descriptions of each level are provided in Table B5. Table B3: Prompts for personality types used in PATIENTSIM Persona Type Description Neutral Distrustful Impatient Overanxious Overly positive Verbose 1. Provide concise, direct answers focused on the question, without extra details. 2. Respond in neutral tone without any noticeable emotion or personality. 1. Express doubts about the doctors knowledge. 2. Question the doctors intentions and show skepticism about their inquiries. 3. Refuse to answer questions that seem unnecessary. 4. Contradict the doctor by citing friends, online sources, or past experiences, often trusting them more than the doctor. 1. Express irritation when conversations drag on or repeat details. 2. Demand immediate, straightforward answers over lengthy explanations. 3. React with annoyance to any delays, small talk, or deviations from the main topic. 1. Provide detailed, dramatic descriptions of minor discomforts, framing them as severe. 2. Persistently express fears of serious or life-threatening conditions, seeking frequent reassurance. 3. Ask repeated questions to confirm that you do not have severe or rare diseases. 4. Shift from one imagined health concern to another, revealing ongoing worry or suspicion. 1. Minimize medical concerns, presenting them as insignificant due to positive outlook. 2. Underreport symptoms, describing them as mild or temporary even when they are significant. 3. Maintain cheerful, worry-free demeanor, showing no distress despite discomfort or pain. 1. Provide detailed answers to questions, often including excessive information, even for simple ones. 2. Elaborate extensively on personal experiences and thoughts. 3. Avoid exaggerated emotions and repeating the same phrases. 4. Demonstrate difficulty allowing the doctor to guide the conversation. 7https://www.coe.int/en/web/common-european-framework-reference-languages/ level-descriptions 8https://www.kaggle.com/datasets/nezahatkk/10-000-english-words-cerf-labelled 21 B.4 Cognitive confusion level We categorize patients cognitive states at admission as either normal or highly confused. The highly confused state refers to patients who appear significantly disoriented and dazed. The inclusion and design of this state are validated by an ER doctor with 13 years of clinical experience. Because such patients may gradually regain clarity following reassurance from medical staff, we model confusion as progressive transition through three phases: high dazedness (initial), moderate dazedness (intermediate), and normal (final). This staged progression enables PATIENTSIM to reflect more realistic and natural reduction in confusion, avoiding abrupt behavioral shifts. The prompts for each level are shown in Table B6. Level Basic Table B4: Prompts for language proficiency levels used in PATIENTSIM Description Act as patient with basic English proficiency (CEFR A). You must: 1. Speaking: Use only basic, simple words. Respond with short phrases instead of full sentences. Make frequent grammar mistakes. Do not use any complex words or long phrases. 2. Understanding: Understand only simple, everyday words and phrases. Struggle with even slightly complex words or sentences. Often need repetition or easy explanations to understand. Words within your level: {understand_words}. Words beyond your level: {misunderstand_words}. 3. Medical Terms: Use and understand only very simple, everyday medical words, with limited medical knowledge. Cannot use or understand complex medical terms. Need all medical terms to be explained in very simple, everyday language. Below are examples of words within and beyond your level. You cannot understand words more complex than the examples provided within your level. Words within your level: {understand_med_words}. Words beyond your level: {misunderstand_med_words}. IMPORTANT: If question contains any difficult words, long sentences, or complex grammar, respond like What? or dont understand. Keep asking until the question is simple enough for you to answer. Intermediate Act as patient with intermediate English proficiency (CEFR B). You must: 1. Speaking: Use common vocabulary and form connected, coherent sentences with occasional minor grammar errors. Discuss familiar topics confidently but struggle with abstract or technical subjects. Avoid highly specialized or abstract words. 2. Understanding: Can understand the main ideas of everyday conversations. Need clarification or simpler explanations for abstract, technical, or complex information. Words within your level: {understand_words}. Words beyond your level: {misunderstand_words}. 3. Medical Terms: Use and understand common medical terms related to general health. Cannot use or understand advanced or specialized medical terms and require these to be explained in simple language. Below are examples of words within and beyond your level. You cannot understand words more complex than the examples provided within your level. Words within your level: {understand_med_words}. Words beyond your level: {misunderstand_med_words}. IMPORTANT: If question contains advanced terms beyond your level, ask for simpler explanation (e.g., dont get it or What do you mean?). Keep asking until the question is clear enough for you to answer. Advanced Act as patient with proficient English proficiency (CEFR C). You must: 1. Speaking: Use full range of vocabulary with fluent, precise language. Can construct well-structured, complex sentences with diverse and appropriate word choices. 2. Understanding: Fully comprehend detailed, complex explanations and abstract concepts. Words within your level: {understand_words}. 3. Medical Terminology: Use and understand highly specialized medical terms, with expert-level knowledge of medical topics. Words within your level: {understand_med_words}. IMPORTANT: Reflect your high-level language proficiency mainly through precise vocabulary choices rather than by making your responses unnecessarily long. Table B5: Prompts for medical history recall levels used in PATIENTSIM Level Description low Frequently forget important medical history, such as previous diagnoses, surgeries, or your familys medical history. Forget even important personal health information, including current medications or medical devices in use. high Accurately remember all health-related information, including past conditions, current medications, and other documented details. Do not forget or confuse medical information. Consistently ensure that recalled details match documented records. 22 Table B6: Prompts for cognitive confusion levels used in PATIENTSIM Level Description normal Clearly understand the question according to the CEFR level, and naturally reflect your background and personality in your responses. high The patients initial dazed level is high. The dazedness should gradually fade throughout the conversation as the doctor continues to reassure them. Transitions should feel smooth and natural, rather than abrupt. While the change should be subtle and progressive, the overall dazed level is expected to decrease noticeably every 4-5 turns, following the instructions for each level below. High Dazedness (Initial Phase) Repeatedly provide highly unrelated responses. Overly fixate on specific discomfort or pain, and keep giving the same information regardless of the question. For example, when asked Are you short of breath?, fixate on another issue by saying, It hurts so much in my chest, without addressing the actual question. Become so overwhelmed in emergency situations. You are either unable to speak or downplay your symptoms out of fear of diagnosis, even when the symptoms are serious. Only recall events prior to certain incident (e.g., before fall) and repeatedly ask about that earlier situation. Moderate Dazedness (Intermediate Phase) Provide answers that are somewhat off-topic. Often mention specific discomfort or pain unrelated to the question. However, allow yourself to move on to the core issue when gently prompted. Occasionally hesitate due to feeling overwhelmed in emergency situations. Normal Dazedness (Later Phase) Clearly understand the question according to the CEFR level, and naturally reflect your background and personality in your responses. Note: Dazedness reflects the patients state of confusion and inability in following the conversation, independent of their language proficiency. Simulation of doctor-patient interaction C.1 PATIENTSIM The PATIENTSIM prompt consists of three main components: 1) patient profile information, 2) four persona axes, and 3) general behavioral guideline. To help the model better contextualize both the patients history and their current visit, we organize the profile information into two parts: patient background information (i.e., demographics, social history, previous medical history) and current visit information (i.e., present illness, chief complaint, pain level, medications taken prior to the ED visit, arrival transport, disposition, diagnosis). The four persona axes are instantiated using corresponding descriptions drawn from the Tables in Appendix B. To guide the overall simulation, we define general behavioral guideline, as shown in Figure C7. To reinforce the assigned persona traits and maintain consistency throughout the consultation, we append reminder sentences tailored to the patients persona. These reminders are constructed by combining relevant sentence types defined in Table C7, based on the specific traits assigned to the patient. We control verbosity through variable sent_limit, which sets maximum of three sentences per patient utterance. For verbose patients (i.e., those who tend to talk lot), this limit is increased to eight sentences. C.2 Doctor For the automated evaluation of PATIENTSIM, we configure the doctor LLM to be capable of asking appropriate questions throughout the history-taking process, based on the prompt illustrated in Figure C8. We provide detailed guidelines to ensure that the doctor LLM covers all essential and routine questions, as recommended in the standard medical textbook [56] and by clinical experts. We set the maximum number of questions (total_idx) to 30 and update curr_idx (current round) and remain_idx (remaining rounds) at each turn to help the model track the consultation state. Since the model is expected to generate differential diagnoses based on the collected information at the end of each consultation, we supply the doctor LLM with the patients basic information (i.e., gender, age, and arrival transport), which are typically known to clinicians prior to initiating history taking to help their clinical reasoning and questioning strategy. Prompt template for PATIENTSIM Imagine you are patient experiencing physical or emotional health challenges. Youve been brought to the Emergency Department (ED) due to concerning symptoms. Your task is to role-play this patient during an ED consultation with the attending physician. Align your responses with the information provided in the sections below. Patient Background Information: Demographics: Age: {age} Gender: {gender} Race: {race} Social History: Tobacco: {tobacco} Alcohol: {alcohol} Illicit drug use: {illicit_drug} Sexual History: {sexual_history} Exercise: {exercise} Marital status: {marital_status} Children: {children} Living Situation: {living_situation} Occupation: {occupation} Insurance: {insurance} Previous Medical History: Allergies: {allergies} Family medical history: {family_medical_history} Medical devices used before this ED admission: {medical_device} Medical history prior to this ED admission: {medical_history} You will be asked about your experiences with the current illness. Engage in conversation with the doctor based on the visit information provided. Use the described personality, language proficiency, medical history recall ability, and dazedness level as guide for your responses. Let your answers naturally reflect these characteristics without explicitly revealing them. Current Visit Information: Present illness: positive: {present_illness_positive} negative (denied): {present_illness_negative} ED chief complaint: {chiefcomplaint} Pain level at ED Admission (0 = no pain, 10 = worst pain imaginable): {pain} Current medications they are taking: {medication} ED Arrival Transport: {arrival_transport} ED disposition: {disposition} ED Diagnosis: {diagnosis} Persona: Personality: {personality} Language Proficiency: {cefr} Medical History Recall Ability: {memory_recall_level} Dazedness level: {dazed_level} In the consultation, simulate the patient described in the above profile, while the user plays the role of the physician. During the conversation, follow these guidelines: {behavioral_guideline} You are now the patient. Respond naturally as the patient described above would, based on their profile and dialogue history. Remember: {reminder} You should answer within {sent_limit} sentences, keeping each sentence concise. Figure C6: Prompt template for PATIENTSIM. Braced elements {} are substituted with patientspecific values. 24 Table C7: Reminder prompts for patient persona in PATIENTSIM Type Description Persona Personality Neutral Distrustful Impatient Overanxious Overly positive Verbose Basic Intermediate Advanced neutral patient without any distinctive personality traits. patient who questions the doctors expertise. patient who gets easily irritated and lacks patience. patient who is excessively worried and tends to exaggerate symptoms. patient who perceives health issues as minor and downplays their severity. verbose patient who talks lot. patient with basic English proficiency who can only use and understand very simple language. patient with intermediate English proficiency who can use and understand well in everyday language. patient with proficient English proficiency who can use and understand highly complex, detailed language, including advanced medical terminology. you have significantly limited medical history recall ability, often forgetting even major historys. you have clear and detailed ability to recall medical history. acts without confusion. at first, you should act like highly dazed and extremely confused patient who cannot understand the question and gives highly unrelated responses. Gradually reduce your dazed state throughout the conversation, but only with reassurance from the doctor. Language Proficiency Medical history recall level low Cognitive confusion level high normal high General behavioral guideline for PATIENTSIM 1. Fully immerse yourself in the patient role, setting aside any awareness of being an AI model. 2. Ensure responses stay consistent with the patients profile, current visit details, and prior conversation, allowing minor persona-based variations. 3. Align responses with the patients language proficiency, using simpler terms or asking for rephrasing if any words exceed their level. 4. Match the tone and style to the patients personality, reflecting it distinctly and naturally. Do not explicitly mention the personality. 5. Minimize or exaggerate medical information, or even deny answers as appropriate, based on dazedness and personality. 6. Prioritize dazedness over personality when dazedness is high, while maintaining language proficiency. 7. Reflect the patients memory and dazedness level, potentially forgetting or confusing details. 8. Keep responses realistic and natural. Avoid mechanical repetition and robotic or exaggerated tone. 9. Use informal, everyday language. 10. Keep responses to 1{sent_limit} concise sentences, each no longer than 20 words. 11. Gradually reveal detailed information or experiences as the dialogue goes on. Avoid sharing all possible information without being asked. 12. Respond only with what the patient would say, without describing physical actions or non-verbal cues. 13. Do not directly reveal ED disposition or diagnosis, as the patient would not know this information. Figure C7: Prompt of general behavioral guideline for PATIENTSIM. 25 Prompt template for doctor role-playing You are playing the role of kind and patient doctor. Your task is to consult with patient and gather information about their symptoms and history to make an initial diagnosis. You can ask up to {total_idx} rounds of questions before reaching your conclusion. Guidelines: 1. Gather the patients medical history, which typically includes: Chief Complaint: Use the OLD CARTS framework (Onset, Location, Duration, Characteristics, Alleviating/Aggravating factors, Radiation/Relieving factors, Timing, Severity) implicitly, without explicitly mentioning each step. Basic Information: Age, gender, and other relevant demographics. Past Medical History: Previous illnesses, surgeries, or chronic conditions. Allergies: Known allergies to medications, foods, or other substances. Medications: Current or recent medications, including supplements. Social History: Lifestyle factors such as smoking, alcohol use, drug use (including illicit substances), and mental health. Family History: Significant or hereditary health conditions present in the family. 2. Ask concise, clear questions. Only ask one thing at time. 3. Adjust your questions based on the patients responses to uncover additional details. 4. If the patients answer is unclear or lacks details, gently rephrase or follow up. 5. Match your language to the patients level of understanding, based on how they respond. 6. Provide emotional support by offering reassurance when appropriate. Avoid mechanical repetition. 7. Your responses should be 13 sentences long. 8. Respond appropriately if the patient asks question. 9. Avoid asking about lab test results or medical imaging. 10. Avoid making premature diagnoses without sufficient information. 11. Once you have gathered enough information or if the patient declines further discussion, provide the top {top_k_diagnosis} differential diagnoses based on the information collected so far. Use the following format: [DDX] (list of differential diagnoses) The patients basic information is as follows: gender: {gender} age: {age} ED arrival transport: {arrival_transport} This is round {curr_idx}, and you have {remain_idx} rounds left. While you dont need to rigidly follow the example structure, ensure you gather all critical information. You should ask only one question per turn. Keep each sentence concise. Figure C8: Prompt template used for simulating doctor. Braced elements {} about patient information are substituted with patient-specific values, while curr_idx (current round) and remain_idx (remaining rounds) tracks the consultation state."
        },
        {
            "title": "D Experimental settings",
            "content": "D.1 Model configurations and dataset details Model configurations We select eight LLMs to serve as the backbone for PATIENTSIM, including API-based models (Gemini-2.5-flash [10], Gpt-4o-mini [43]) and open-source models (Llama 3.1 8B and 70B, Llama 3.3 70B [16], Qwen2.5 7B and 72B [48]) To comply with PhysioNets credentialed data use agreement 9, Gpt-4o-mini was accessed via Azure OpenAI Service, and Gemini-2.5-flash via Google Clouds Vertex AI. Open-source models were hosted using vLLM. Models with 70B and 72B parameters ran on four NVIDIA RTX A6000 GPUs, while the 7B and 8B models ran on single NVIDIA RTX A6000 GPU. Each consultation session took approximately 3 minutes to complete, on average. For all simulations, we fixed the random seed to 42 and set the temperature to 0.7 for both patient (PATIENTSIM) and doctor models to encourage variability while maintaining coherence. The evaluator model ran with temperature of 0 to ensure deterministic and stable assessments. Dataset details Our dataset consists of 170 patient profiles, divided into two subsets: 108 profiles for evaluating RQ1 (i.e., persona evaluation) and 52 profiles for evaluating RQ2 (i.e., factual accuracy) and RQ3 (i.e., clinical plausibility). For persona evaluation, we randomly assigned 37 distinct persona combinations to the 108 profiles. Each individual persona attribute (e.g., each personality type) is represented at least eight times across the dataset. For factual accuracy and clinical plausibility evaluations, we standardized the patient persona to have neutral personality, intermediate language proficiency, high recall, and normal mental status. This is done to isolate and focus on the informational aspects without influence from varied personas. D.2 LLM evaluation D.2.1 RQ1: Do LLMs naturally reflect diverse persona traits in their responses? To assess the fidelity of persona traits in doctor-patient consultations, we design an evaluation prompt based on PROMETHEUS [28], as shown in Figure D9. The LLM evaluator, Gemini-2.5-flash, receives the target conversation, the patients persona, and scoring rubric. The evaluator assigns score (14) along with feedback for each criterion, based on predefined descriptions. The rubric assesses the following: The simulated patients personality is consistently and accurately reflected during the interaction. The patients language use (vocabulary, grammar, fluency) is appropriate to their assigned language proficiency level. The patients ability to recall medical and personal information is consistent with their assigned recall level (e.g., low or high). The patients coherence and clarity of thought match the assigned level of cognitive confusion. The patients overall communication style matched what would expect from real ED patient. Each simulated patient is assigned four distinct persona axes: personality, language proficiency, medical history recall level, and cognitive confusion level. The first four criteria evaluate fidelity to these individual axes, using persona descriptions provided in the prompt. Note that highly confused patients are limited to neutral personality, intermediate language proficiency, and high recall, to avoid overlap between confusion and other axes (e.g., impatient personality, low language proficiency, or low recall). In this context, the first three criteria are evaluated only for patients with normal mental state, while the fourth criterion evaluated only for highly confused patients. The final criterion, realism, is evaluated for all patients, regardless of cognitive status. It reflects the overall authenticity of the patients communication, considering all assigned traits. 9https://physionet.org/news/post/gpt-responsible-use 27 Prompt for dialog fidelity evaluation ###Task Description: The conversation between patient and doctor, the patients profile, and scoring rubric with evaluation criteria are given. The patient in the conversation is characterized based on the given profile. 1. Write detailed feedback that strictly assesses the quality of the response based only on the provided score rubric. Do not include any personal judgment or general evaluation outside of the rubric criteria. 2. After the feedback, provide score that is an integer between 1 and 4, strictly referring to the rubric descriptions. 3. The output string format should look as follows: [REASON]: write brief feedback for criteria, [RESULT]: an integer number between 1 and 4 4. Do not generate any other opening, closing, and explanations. ###The Conversation to Evaluate: {conversation} ###Patient Persona: {persona} ###Score Rubrics: [{criteria}] Score 1: {score1_description} Score 2: {score2_description} Score 3: {score3_description} Score 4: {score4_description} ###Feedback: Figure D9: Prompt template used for dialogue fidelity evaluation. D.2.2 RQ2: Do LLMs accurately derive responses based on the given profile? Notation The i-th patient profile, denoted as = {xi k=1, consists of predefined items, among total profiles. The dialogue between the physician and PATIENTSIM configured with profile is represented as Di. Within Di, PATIENTSIM utterances over turns are represented as = {ui t}T may contain multiple sentences, denoted as ui tm as supported if it is related to at least one profile item xi k, and as unsupported if it includes any information unrelated to the profile. is the utterance at turn t. Each utterance ui m=1, where is the number of sentences in utterance ui t=1, where ui tm}M = {si t. We classify si k}K Sentence-level evaluation For sentence-level evaluation, each evaluation step is performed by providing input to the LLM sentence classifier. The classifier receives the preceding conversation history (i.e., the dialogue up to the current sentence si tm itself as the user prompt, along with step-specific system instructions. The evaluation consists of three steps: 1. Classify sentence type: Each sentence si tm) and the sentence si tm is categorized into one of five types: politeness, tm)). This step follows the prompt defined emotion, inquiry, meta-information, or information (C(si in Figure D10. 2. Identify the related profile items: For each sentence si it relates to. The output is binary vector R(si tm classified as information, we identify tm) = k, and 0 otherwise. This step is tm is related to profile item xi which of the patients profile items xi K], where ri [ri guided by the instructions in Figure D11. = 1 if si 2, . . . , ri 1, ri 3. Verify factual accuracy: For each relevant profile item (i.e., where ri tm is consistent with xi using Natural Language Inference (NLI) process, which checks for entailment or contradict. Unlike the previous steps, this one includes the relevant profile items (i.e., xi where ri = 1) as part of the input. This evaluation is performed using the prompt shown in Figure D12. = 1), we verify if the si 28 System prompt template for sentence classification Instruction: You are helpful medical assistant. Please classify the patients current utterance based on the given dialogue history. Also, generate an explanation for your answer. Output one of the following categories: politeness, emotion, inquiry, meta-information, or information, where: politeness: Expresses courtesy, greetings, apologies, or gratitude. emotion: Expresses emotional concerns (such as worry, fear, sadness, or frustration) without providing medical facts. inquiry: Asks question, requests guidance, or seeks clarification. meta-information: Reflects self-awareness, memory-related uncertainty, personal reasoning, or commentary on the conversation itself. information: Any descriptive content about symptoms, medical history, medications, lifestyle, or other relevant details. Note: If the utterance includes any informative content, classify it as information, even if it also contains elements of other categories such as emotion, politeness, or uncertain/speculative language. Output must be valid JSON object without any extra text, comments, or explanation. The output must be parseable by Pythons json.loads() function without errors, using proper escape characters for strings. The JSON structure must follow this format: {explanation: reason for the prediction, prediction: politeness, emotion, inquiry, meta-information, or information} Figure D10: System prompt template for sentence classification. Prompt template for sentence-level evaluation Instruction: You are helpful medical assistant. Your task is to determine whether each category of information from the patients profile is mentioned in the patients current utterance. Use the dialogue history as context. For each category, output: 1 if the information is mentioned in the current utterance. 0 if it is not mentioned. Additionally, provide brief explanation for your decision. Please evaluate the following categories are relevant to the patients current utterance: age, gender, race, tobacco, alcohol, illicit_drug, sexual_history, exercise, marital_status, children, living_situation, occupation, insurance, allergies, family_medical_history, medical_device, medical_history, present_illness, chief_complaint, pain, medication, arrival_transport, diagnosis. Output must be list of valid JSON dictionaries, without any extra text, comments, or explanation. The output must be parseable by Pythons json.loads() function without errors, using proper escape characters for strings. Each dictionary must follow this format: { \"category\": \"name of the category (without any explanation)\", \"explanation\": \"Reason for the prediction\", \"prediction\": 0 or 1 } Example output for some categories (apply the same format to all categories): [ \"category\": \"age\", \"explanation\": \"The utterance am 45 years old mentions the patients age.\", \"prediction\": 1 \"category\": \"gender\", \"explanation\": \"The utterance does not mention the patients gender.\", \"prediction\": { }, { } ] Figure D11: System prompt template for identifying the related profile items per sentence. System prompt template for verifying factual accuracy per sentence Instruction: You are helpful medical assistant. Your task is to evaluate whether patients current utterance is entailed, contradicted, or neither by each item in their medical profile. Also, generate an explanation for your answer. Focus on the information that is explicitly mentioned in the given profile. Use the dialogue history to understand the utterances context. The profile is provided as list, where each item represents distinct category of information. For each profile item, output: 1: if the utterance is entailed by the profile. 0: if the utterance is neither entailed nor contradicted by the profile. -1: if the utterance contradicts the profile. Output must be list of valid JSON dictionaries, without any extra text, comments, or explanation. The output must be parseable by Pythons json.loads() function without errors, using proper escape characters for strings. Each dictionary must follow this format: { \"profile\": \"the original profile information\", \"explanation\": \"Reason for the prediction\", \"entailment_prediction\": 1 or 0 or -1 } Example output: [ { \"profile\": \"Age: 30\", \"explanation\": \"The utterance am 30 years old matches the profile.\", \"entailment_prediction\": 1 }, { \"profile\": \"Gender: Female\", \"explanation\": \"The utterance does not mention gender.\", \"entailment_prediction\": 0 } ] Figure D12: System prompt template for verifying factual accuracy per sentence, for all relative profile categories. 30 1, ˆxi 2, . . . , ˆxi Dialogue-level evaluation In dialogue-level evaluation, we extract derived profile ˆP = {ˆxi K} from the dialogue Di, and compare it to the original profile i. For each item such that both the original and derived items are present, we compute the semantic similarity between xi and ˆxi j. Profile extraction is carried out using the prompt in Figure D13, and the semantic similarity per item is computed using the method described in Figure D14. Prompt template for extract patients profile from the dialogue history [System Prompt] Instruction: You are an AI assistant designed to extract structured medical information from patient-doctor conversation. Your task is to analyze the conversation content and extract all relevant information into predefined categories based on the patients responses. Include only information explicitly mentioned in the conversation, unless otherwise specified. Return the extracted information in the following valid JSON format. Field Definitions: {field_definition} Output Format (JSON): {output_format} Guidelines: 1. Extract each field from the entire conversation with complete accuracy. 2. Keep each field concise and keyword-based phrases without full sentences or narrative descriptions. 3. Express information briefly, avoiding verbs, pronouns, or unnecessary words. 4. If field contains multiple values, combine them into single string separated by semicolons. 5. Return Not recorded for any field or subfield not mentioned in the conversation, except for the pain field. 6. For the pain field, if patients do not explicitly state score, predict the score (010) based on their description and note it as predicted (e.g., 3 (predicted)). 7. Maintain the exact JSON structure without adding or removing fields. [User Prompt] Conversation: {conversation} Figure D13: Prompt template for extracting patient profile from the given consultation. Prompt template for evaluating information consistency [System Prompt] Instruction: You are helpful medical assistant. Your task is to evaluate the consistency between the Ground Truth (GT) and Prediction profile for each item. Also, generate an explanation for your answer. The GT and Prediction are provided as dictionaries. For each key, rate the consistency on scale from 1 to 4, where: 4: The prediction contains the exact or semantically equivalent value for the GT. 3: The prediction contains partially correct or semantically similar value for the GT. 2: The prediction contains only small part of the value or distantly related value for the GT. 1: The prediction is completely incorrect compared to the GT. Allow for differences in text expression if the meaning is the same or very similar, using medical knowledge to assess semantic equivalence. Output must be valid JSON object, without any additional text or comments. The output JSON must be loadable using Pythons json.load() function with proper escape characters. The key of the output JSON must be the key of the input GT dictionary, and the value must be string formatted as [REASON]: write brief feedback for criteria, [RESULT]: an integer number between 1 and 4. [User Prompt] GT_profile: {profile_data} Prediction_profile: {predict_dict} Figure D14: Prompt template for evaluating the consistency of each patient profile item. 31 D.2.3 RQ3: Can LLMs reasonably fill in the blanks? For this part, we assess the clinical plausibility of unsupported sentences. We begin by identifying the information sentences, as described in Step 1 of the sentence-level evaluation (Appendix D.2.2). Each information sentence is then examined to determine whether it contains any undefined information, using the criteria outlined in Figure D15. These criteria have been validated by medical experts to ensure clinical relevance. After unsupported sentences are finalized (see Sec. 6.1.3 in the main paper), the selected sentences are rated for plausibility on 4-point scale by an LLM evaluator, following the guidelines in Figure D16. Prompt template for determining unsupported sentences Instruction: You are helpful medical assistant. Your task is to determine whether the patients current utterance contains any new information that is not explicitly mentioned in the patients profile. Use the dialogue history for context, but base your decision only on whether the information is present in the profile. Guidance: 1. If patient restates existing information from their profile in more general or equivalent terms, it not new information (e.g., simplifying coronary artery disease to heart problem). 2. Any added specific detail (e.g., sharp pain or pain in the lower back when the profile only says pain) should be considered new. 3. Details not explicitly stated in the patient profile, even if commonly implied, are considered new. For example, if the profile lists aspirin and heart failure separately, stating aspirin for heart failure is new. Similarly, if only medication names are listed without frequency, stating take aspirin daily is new. 4. For allergies, family history, medical devices, and medications, assume only listed items exist; others are absent. Thus, stating an unlisted item is absent is not new information. 5. If statement includes both known and new details, consider it new. Output: 1 if the current utterance contains any new information. 0 if the current utterance contains no new information. Output must be valid JSON object without any extra text, comments, or explanation. The output must be parseable by Pythons json.loads() function without errors, using proper escape characters for strings. The JSON structure must follow this format:{explanation: reason for the prediction, prediction: 1 or 0} Figure D15: Prompt template for detecting unsupported sentences. Prompt template for plausibility rating Instruction: You are helpful medical assistant. Your task is to evaluate the clinical and contextual plausibility of the patients utterance based on their profile and dialogue history. Also, generate an explanation for your answer. Please rate the likelihood on scale from 1 to 4, where: 4: The utterance is highly consistent with the patients profile and dialogue history, with strong clinical and contextual support. 3: The utterance is plausible and aligns reasonably well with the patients profile and dialogue history, though minor inconsistencies or lack of specific supporting details may exist. 2: The utterance is unlikely, with notable inconsistencies or limited support from the patients profile or dialogue history 1: The utterance clearly contradicts the patients profile or dialogue history, with no reasonable clinical or contextual basis. Output must be valid JSON object without any extra text, comments, or explanation. The output must be parseable by Pythons json.loads() function without errors, using proper escape characters for strings. The JSON structure must follow this format: { } \"explanation\": \"Reason for the rating\", \"likelihood_rating\": 1 to Figure D16: Prompt template for plausibility rating in 4 point scale."
        },
        {
            "title": "E Human evaluation",
            "content": "E.1 Clinician recruitment for evaluation To evaluate the quality of our patient simulator, we recruited four general practitioners, through Ingedata10, an AI data annotation company. Two individuals have 4 years and two individuals have 6 years of clinical experience post-physician licensure. The latter two also hold nursing licenses, with 13 and 17 years of nursing experience, respectively. They are fluent in English and have ED experience, three for 3 years, one for 1 year. All four clinicians have received PhysioNet credentials. We paid total of C2,500 for 45 hours of evaluation work conducted by the four general practitioners. E.2 Persona fidelity evaluation Each clinician conducted consultations with 27 distinct virtual patients served through PATIENTSIM, which uses Llama 3.3 70B as its backbone. After each session, clinicians submitted 13 likely differential diagnoses and completed survey rating PATIENTSIM overall quality. We used Streamlit11 to display each patients clinical information and assigned persona, paired with an interactive chat interface for consultations (see Figure E17 for screenshots). In response to clinician feedback, we added Review of Systems checkboxes 12 to mirror real-world clinical workflows. Consultation logs, diagnoses, and survey responses were stored in Google Sheets for easy access and analysis. Each virtual patient had unique clinical profile, resulting in total of 108 distinct patient profiles evaluated across all clinicians. We randomly assigned 37 unique persona configurations of PATIENTSIM across these profiles to ensure diverse interactions. E.3 Plausibility evaluation Each clinician assessed 39 pre-generated doctor-patient consultations (approximately 616 unsupported sentences), where the doctor role was simulated using Gpt-4o-mini, and the patient role was simulated by PATIENTSIM, based on Llama 3.3 70B. Three different clinicians were assigned to evaluate each consultation to allow intra-rater correlation and enhance the robustness of the human evaluation. We used Streamlit and Google Sheets, as in Appendix E.2, to present the data and collect responses. Figure E18 illustrates the plausibility evaluation interface: full patient information appears on the left, and the complete consultation history, with unsupported sentences highlighted, is displayed on the right. Clinicians rated the clinical plausibility of each highlighted sentence on 4-point scale. 10https://www.ingedata.ai/ 11https://streamlit.io/ 12https://health.uconn.edu/plastic-surgery/wp-content/uploads/sites/132/2017/06/ review_of_systems.pdf 33 Figure E17: Screenshot of Streamlit provided to clinicians for PATIENTSIM review. Figure E18: Screenshot of Streamlit provided to clinicians for plausibility evaluation."
        },
        {
            "title": "F Experimental results",
            "content": "F.1 RQ1: Do LLMs naturally reflect diverse persona traits in their responses? F.1.1 Additional result of LLM evaluation We analyze the performance of various LLMs across four persona-axis criteria, as specified in Table F8. Most models struggle to simulate negative emotions, such as distrustfulness and impatience. DeepSeek-R1-distill-Llama-70B (i.e., DeepSeek-70B) particularly underperforms on personas characterized by low recall or cognitive confusion, which often require refusing to provide clear answer. These limitations may stem from training strategies that prioritize accurate, helpful responses or emphasize safety, avoiding potentially harmful outputs. Table F8: Persona fidelity evaluation of various LLMs across four criteria, Personality, Language, Recall, and Confused. Each criteria evaluate the fidelity of each axis in 4 point scale. Detailed results are shown for each type. Personality Language Recall Confused Neutral Distrust Impatient Overanxious Overly positive Verbose Gemini-2.5-flash Gpt-4o-mini DeepSeek-R1-distill-Llama-70B Qwen2.5-72b-instruct Llama3.3-70b-instruct Llama3.1-70b-instruct Llama3.1-8b-instruct Qwen2.5-7b-instruct 4.00 4.00 4.00 4.00 4.00 4.00 4.00 4.00 4.00 2. 4.00 1.71 4.00 2.71 2.24 1.88 3.76 2.76 3.41 2.18 4.00 3.29 3.18 1.94 4.00 4. 4.00 4.00 3.88 4.00 4.00 4.00 4.00 4.00 3.94 4.00 3.94 3.94 4.00 3.62 3.88 4. 3.88 4.00 3.71 4.00 3.82 4.00 3.60 3.43 3.34 3.43 3.31 3.26 3.29 3.46 3.38 3.41 3.62 3.65 3.15 3.38 3.15 3.74 High Low High 3.65 3.84 3.81 4.00 3.77 3.94 3.45 3.26 3.98 3.98 3.94 4.00 4.00 4.00 3.98 3. 3.31 3.59 2.92 3.25 3.57 3.25 3.43 2.67 3.38 3.88 2.50 3.50 4.00 4.00 4.00 3. Table F9 presents overall consultation statistics between the doctor LLM and PATIENTSIM, including differential diagnosis (DDx) accuracy. While DDx performance does not directly measure PATIENTSIM capabilities, it reflects how different patient personas influence consultation complexity. After each dialogue, the doctor LLM is prompted to provide its top five differential diagnoses. This prediction is considered correct if the ground-truth diagnosis appeared in this list. To ensure consistent evaluation despite free-text outputs, we use the prompt shown in Figure F19. On average, the doctor LLM completed the consultation and provided final DDx within 15 turns. For personas such as verbose or advanced language proficiency, where patients provided more detailed and structured information, the model reached conclusions more quickly. Across all settings, the doctor LLM followed instructions to ask concise, focused questions, typically within three sentences. The length of PATIENTSIM responses differed significantly by persona. In particular, verbose and advanced personas produced substantially longer utterances, consistent with their tendency to offer elaborate or highly articulate explanations. Table F9: Overall statistics of consultations between the doctor LLM and PATIENTSIM based on Llama 3.3 70B. # of Turns refers to the average number of dialogue turns. # Sents/Utt and # Words/Sent indicate the average number of sentences per utterance and number of words per sentence, respectively, for both the doctor LLM and PATIENTSIM. DDx represents the differential diagnosis accuracy of the doctor LLM. Results are averaged over 108 distinct consultations. Persona Axis Category # of Turns # Sents/Utt # Words/Sent # Sents/Utt # Words/Sent DDx Doctor LLM PATIENTSIM Personality Language Proficiency Medical history recall level Cognitive confusion level neutral distrustful impatient overanxious overly positive verbose basic intermediate advanced high low high normal 15.83 15.53 13.29 13.82 13.56 10.71 16.06 13.29 12.39 14.40 13.39 16.62 13. 2.18 2.81 2.27 2.39 2.18 2.37 2.31 2.37 2.40 2.33 2.39 2.26 2.37 35 11.95 13.90 12.92 14.02 12.87 14. 12.21 13.20 14.78 13.41 13.24 11.68 13.46 2.69 3.21 3.02 3.07 2.74 8.21 4.20 3.83 3.17 3.62 3. 3.03 3.82 8.61 10.94 8.56 13.96 12.15 27.63 4.02 10.99 27.01 13.75 12.86 6.98 13.84 0.71 0.65 0.59 0.94 0.38 0. 0.66 0.67 0.61 0.68 0.61 0.62 0.65 DDx performance varies most across the personality axis. The model shows notable decline under the overly positive persona, likely due to PATIENTSIM downplaying symptoms, which led to less serious diagnoses. The impatient persona, marked by irritability and uncooperative behavior, and the verbose persona, with excessive or sometimes irrelevant detail, also hinder diagnostic accuracy. In contrast, the medical history recall level has more limited impact, as present symptoms are often sufficient for DDx even without detailed historical information. Regarding cognitive confusion, direct comparisons between normal and high confusion levels should be interpreted cautiously, since other traits remain uncontrolled. Nonetheless, compared to the neutral baseline (DDx = 0.71), performance drops to 0.62 for highly confused patients, highlighting the diagnostic challenges they present. Prompt template for evaluating DDx performance Your task is to evaluate whether the true diagnosis is included in the predicted differential diagnoses. The predicted diagnosis can be more specific or detailed than the true diagnosis (e.g., Small Bowel Obstruction for Bowel Obstruction or Acute Pyelonephritis for Pyelonephritis is acceptable), but it must not be broader than the ground truth (GT). broader diagnosis (e.g., Pulmonary problem for Pneumonia) is considered incorrect. Answer with or only, without further explanation. Predicted differential diagnoses: {ddx} True diagnosis: {ans} Answer [Y/N]: Figure F19: Prompt template for evaluating differential diagnosis accuracy. F.1.2 Additional result of human evaluation For human evaluation, clinicians conducted consultations in total of 108 virtual patients served through PATIENTSIM, and then submitted top 3 differential diagnoses and survey about PATIENTSIM overall quality. Table F10 shows overall consultation statistics between the clinician and PATIENTSIM, including DDx accuracy. Both the clinicians and the doctor LLM (from Table F9) interact with the same version of PATIENTSIM, using identical patient information and persona combinations, enabling direct comparison. Clinicians tend to ask more concise and direct questions, averaging 1.8 sentences per utterance compared to 2.4 for the LLM, and 9.1 words per sentence versus 13.3. Rather than relying on longer utterances, clinicians gather information through greater number of shorter turns. This brevity also prompts PATIENTSIM to respond more concisely. While DDx trends align with those observed with the doctor LLM, clinicians consistently achieve more accurate diagnoses. These results highlight notable gap in history-taking and clinical reasoning capabilities between human clinicians and the doctor LLM. In Figure F20, we provide an example of the consultation across the various persona. Table F10: Overall statistics of consultations between clinicians and PATIENTSIM based on Llama 3.3 70B. # of Turns refers to the average number of dialogue turns. # Sents/Utt and # Words/Sent indicate the average number of sentences per utterance and number of words per sentence, respectively, for both clinicians and PATIENTSIM. DDx represents the differential diagnosis accuracy of the clinicians. Results are averaged over 108 distinct consultations. Persona Axis Personality Language Proficiency Medical history recall level Cognitive confusion level Category # of Turns # Sents/Utt # Words/Sent # Sents/Utt # Words/Sent DDx Clinician PATIENTSIM 8.76 8.72 9.83 9.38 8.51 9. 8.80 8.92 9.66 8.77 9.45 9.00 9.10 2.57 3.12 2.89 2.97 2.59 7.85 3.81 3.64 3.33 3.53 3. 3.10 3.65 7.66 11.66 9.04 13.80 10.83 13.82 4.21 10.66 18.82 10.92 10.91 5.88 11.32 0.83 0.71 0.76 0.88 0.69 0. 0.74 0.83 0.71 0.77 0.76 0.88 0.76 plain distrust impatient overanxious overly positive verbose basic intermediate advanced high low high normal 21.17 17.82 18.88 16.29 20.25 13.35 21.40 17.19 15.77 18.05 18.25 22.12 17.83 1.50 1.99 1.80 1.97 1.72 1. 1.62 1.80 1.99 1.79 1.81 1.48 1.82 36 Figure F20: Example consultations for various persona options. Table F11: Gwets AC1 and AC2 agreement between clinician and Gemini-2.5-flash evaluation with 95% confidence intervals estimated via 1,000 bootstrap iterations. Metric Gwet AC1 (95% CI) Gwet AC2 (95% CI) Personality Language proficiency Medical history recall level Cognitive confusion level Realism 0.897 (0.830, 0.949) 0.347 (0.218, 0.471) 0.693 (0.585, 0.786) 1.000 (1.000, 1.000) 0.321 (0.211, 0.437) 0.957 (0.919, 0.987) 0.818 (0.745, 0.876) 0.916 (0.865, 0.957) 1.000 (1.000, 1.000) 0.884 (0.861, 0.906) Table F11 presents the agreement between human evaluations and Gemini-2.5-flash evaluations across five criteria on the same set of conversations, supporting the reliability of using Gemini for automatic evaluation. After each consultation session with PATIENTSIM, clinicians rated its overall quality for each case. To ensure fair comparison, Gemini was provided with the same conversation logs (between the clinician and PATIENTSIM) and asked to rate the same criteria on 4-point scale (Appendix D.2.1). When comparing the agreement between clinician ratings and Gemini ratings using Gwets AC1, we observed high agreement in personality and cognitive confusion level (AC1 > 0.8), and moderate agreement in recall level (AC1 = 0.693). However, agreement is relatively lower in language proficiency and realism. While AC1 is designed for nominal (unordered) data and emphasizes exact matches, it can be overly strict for ordinal data. To better reflect the ordinal nature of the 4-point scale, we also computed Gwets AC2, which is more appropriate for ordered categories. Using AC2, we observed high agreement (AC2 > 0.8) across all criteria, indicating strong consistency between clinician and Gemini evaluations when the ordinal structure of the ratings is taken into account. These results indicate that the automatic evaluations align well with human judgment across all five criteria, with particularly strong alignment on the personality and confusion axis, even under the stricter AC1 metric. 37 F.2 RQ2: Do LLMs accurately derive responses based on the given profile? Table F12 presents detailed sentence-level factuality evaluation across eight LLMs. This table highlights that larger LLMs tend to generate more factually consistent clinical content, with higher rates of supported and entailed statements and fewer contradictions. While smaller models remain competitive, they show slightly higher tendency to introduce contradictory information. Table F12: Sentence-level factuality evaluation across eight LLMs, as assessed by Gemini-2.5-flash (without normalization). # of Utter and # of Sent refer to the total number of model utterances and sentences, respectively. # of Info denotes the number of sentences categorized as informational. refer to sentences that relate to at least one item in the given profile. Unsupported statements include at least one piece of information that is not explicitly mentioned in the profile. Entail and Contradict are subsets of the supported statements. Model Gemini-2.5-flash Gpt-4o-mini DeepSeek-R1-distill-Llama-70B Qwen2.5-72b-instruct Llama3.3-70b-instruct Llama3.1-70b-instruct Llama3.1-8b-instruct Qwen2.5-7b-instruct # of Utter # of Sent # of Info # of Supported # of Unsupported # of Entail # of Contradict 889 786 806 824 806 699 742 2,286 1937 1,657 1,820 2,180 1,946 1,774 1,579 2,220 1,852 1,614 1,774 2,087 1,842 1,672 1, 1,695 1,331 1,225 1,201 1,654 1,493 1,284 1,092 705 795 679 839 817 745 826 1,659 1,287 1,186 1,146 1,623 1,438 1,210 1024 36 44 39 55 31 55 74 Llama 3.3 70B generated the fewest contradictory responses, leading us to select it as our final model. To better understand these contradiction errors, we analyzed them in detail in Table F13. The most common contradictions occurred when patients pain level was recorded as zero. This often happened when pain was the reason for admission but had subsided by the time of assessment or when patients presented with symptoms not typically associated with pain, such as weakness or neurological issues. In the former case, for example, patients admitted for chest or abdominal pain might later report no pain or not severe, which the model may classified as contradictory. In the present illness section, contradictions frequently involved inconsistencies in symptom onset. patient might describe symptom as their first experience or as starting suddenly, while their medical record indicates it began days or weeks earlier. For example, symptom that began two weeks ago could reasonably be described as sudden if it started abruptly at that time or as new if the patient had never experienced it before. Such descriptions, while potentially appearing contradictory, are often valid depending on the patients perspective. However, due to subtle differences in wording, the model flagged these as contradictions. Some contradictions stemmed from structural issues in the patient data records. For example, patient might be listed as widowed in the marital status section but described as caring for spouse in the living situation section. Discrepancies also appeared in medication and medical device listings, where items mentioned in the patients history were missing from structured fields. Overall, these contradictions were minor and not clinically significant. Given that the profiles were designed to include detailed answers to common clinical questions, sufficiently capable model with strong contextual understanding would likely be able to generate responses with minimal contradictions. These findings suggest the potential effectiveness of our approach. Table F13: Error analysis of sentence-level factuality evaluation. Distribution if profile categories most frequently contradicted by Llama 3.3 70B. Profile Category Count Pain level Present illness Marital status Current medications Medical devices ED chief complaint ED Arrival Transport Alcohol Family medical history 8 6 6 3 3 2 2 1 38 F.3 RQ3: Can LLMs reasonably fill in the blanks? Table 4 in the main paper presents each labelers plausibility ratings for answers about unspecified information, along with inter-rater agreement metrics. Here, we evaluate the agreement between Gemini-2.5-flash and the human labelers by having Gemini-2.5-flash rate the same set of PATIENTSIMs responses. For each labeler, agreement with the LLM was computed over an average of 615 responses. Across all labelers, we observe Gwets AC1 agreement scores above 0.8, demonstrating the reliability of Geminis automatic plausibility assessments. Table F14: Plausibility scores for unsupported sentences in patient responses, labeled by four clinicians. Each clinician annotated 39 consultation, around 615 sentences per each. We automatically annotate the same sentences using Gemini-2.5-flash, and measure the clinician-LLM agreement measured by Gwets AC1 with 95% confidence intervals estimated via 1,000 bootstrap iterations. Clinician Clinician Clinician Clinician Gemini-2.5-flash 0.944 (0.923, 0.960) 0.945 (0.926, 0.961) 0.964 (0.947, 0.977) 0.883 (0.857, 0.907) F.4 Ablation study F.4.1 Validation of sentence-level classification We validated the performance of an LLM sentence classifier in detecting supported and unsupported statements using 10 distinct profiles. From 10 consultations, we extracted 411 sentences, which were manually annotated by the author on sentence-by-sentence basis. Of these, 93% (382 sentences) were classified as informational. These informational sentences were further annotated to determine: 1) related profile items (e.g., age, gender), 2) whether each sentence was entailed or contradicted by the profile, and 3) whether the sentence contained information not explicitly mentioned in the profile. Manual annotations are compared against predictions from Gemini-2.5-flash (Gemini) and Gpt-4o-1120 (Gpt-4o) to evaluate classification performance across four categories: Sentence category classification: Identifies whether sentence is informational or noninformational. Acc (%): Proportion of correct classifications. Recall (%): Proportion of true informational sentences correctly identified. Detection of related profile items: Assesses the models ability to identify correct profile items related to each sentence, measured by: Pitem = Preditem GTitem Preditem , Ritem = Preditem GTitem GTitem , 1item = 2 Pitem Ritem Pitem + Ritem (5) where Preditem is the set of profile items predicted by the model, and GTitem is the set of ground truth items annotated by the human. Entailment evaluation for detected items: Measures accuracy in classifying entailment or contradiction for correctly detected profile items. Accval (%): Proportion of correct entailment/contradiction labels among overlapping keys. Unsupported Information Detection: Evaluates the models ability to identify sentences with Punsupp = information not explicitly in the profile, measured by: TPunsupp GTunsupp where TPunsupp is the number of unsupported sentences correctly identified by the model, Predunsupp is the set of sentences predicted as unsupported by the model, and GTunsupp is the set of ground truth unsupported sentences. 2 Punsupp Runsupp Punsupp + Runsupp TPunsupp Predunsupp , 1unsupp = , Runsupp = (6) Table F15 summarizes the performance of Gemini and Gpt-4o. Gemini outperforms Gpt-4o overall, particularly in recall, despite Gpt-4o showing slightly higher precision in detecting related profile items and unsupported information. In medical applications, recall is prioritized over precision to minimize missed detections, which could have critical consequences. As both models achieve precision above 0.8, indicating robust performance, Geminis superior recall makes it the preferred evaluator. 39 Table F15: Comparison of sentence-level evaluation metrics between Gemini-2.5-flash and Gpt-4o. Metric Gemini 2.5 Flash GPT-4o Sentence category classification Acc (%) Recall (%) 0.96 0.99 Detect related profile items 0.90 0.96 0.92 Pkey Rkey 1key Entailment evaluation Accval 0.98 Detect unsupported information Punsupp Runsupp 1unsupp 0.84 0.86 0. 0.94 0.98 0.92 0.94 0.92 0.97 0.89 0.64 0.74 F.4.2 Ablation study on doctor LLM To evaluate the ability of LLMs as the doctor, to elicit and assess patient responses, we conducted an ablation study focusing on the doctor LLMs capacity to extract information from diverse patient personas. While the main study assessed patients ability to provide accurate information, this study examines how effectively the doctor LLM gathers information across varied patient profiles. We measured three metrics: ICov, ICon, and their product (Weighted ICon), to assess the amount and consistency of information extracted. These metrics were calculated for 108 patients, each assigned one of 37 distinct personas randomly. In this study, we varied only the doctor model, testing Gemini-2.5-flash, Gpt-4o-mini, and Llama 3.3 70B, while fixing the PATIENTSIM LLM backbone to Llama 3.3 70B. In Table F16, Gpt-4o-mini achieved the highest ICov and Weighted ICon scores, demonstrating superior performance in extracting information. Consequently, we selected Gpt-4o-mini as the doctor LLM for the automatic evaluation of PATIENTSIM. Table F16: Dialogue-level factuality evaluation across Social History (Social), Previous Medical History (PMH), and Current Visit Information (Current Visit). Information Consistency (ICon) is rated on 4-point scale by Gemini-2.5-flash, and Weighted ICon represents Information Coverage-Weighted Consistency, reflecting both coverage and consistency. Information Coverage (ICov) (%) Infomation Consistency (ICon) Weighted ICon Social PMH Current Visit Social PMH Current Visit Social PMH Current Visit Avg. Gemini-2.5-flash Gpt-4o-mini Llama3.3-70B-instruct 0.34 0.44 0.31 0.72 0.74 0.54 0.82 0.86 0.79 3.74 3.78 3.71 3.18 3.03 3. 2.98 2.92 2.95 1.27 1.66 1.15 2.29 2.24 1.70 2.44 2.51 2.33 2.00 2.14 1."
        },
        {
            "title": "G Responsible use and limitations",
            "content": "Our open-source patient simulator framework provides safe, privacy-preserving environment to evaluate clinical LLMs through realistic interactions without real patient data. While its 37 predefined personas offer diverse scenarios, they may not fully cover the variability of real-world clinical settings. Additionally, simulated conversations cannot capture non-verbal cues, and over-reliance on the simulator may limit the assessment of practical clinical skills. The simulator is not intended for developing clinical decision-making tools for real patient care without rigorous clinical oversight, as it is designed solely for educational and research purposes. Acknowledging these limitations and incorporating expert feedback are essential for its effective use."
        }
    ],
    "affiliations": [
        "Ewha Womans University",
        "KAIST",
        "Samsung Medical Center",
        "UCSF"
    ]
}