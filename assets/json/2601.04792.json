{
    "paper_title": "PyramidalWan: On Making Pretrained Video Model Pyramidal for Efficient Inference",
    "authors": [
        "Denis Korzhenkov",
        "Adil Karjauv",
        "Animesh Karnewar",
        "Mohsen Ghafoorian",
        "Amirhossein Habibian"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recently proposed pyramidal models decompose the conventional forward and backward diffusion processes into multiple stages operating at varying resolutions. These models handle inputs with higher noise levels at lower resolutions, while less noisy inputs are processed at higher resolutions. This hierarchical approach significantly reduces the computational cost of inference in multi-step denoising models. However, existing open-source pyramidal video models have been trained from scratch and tend to underperform compared to state-of-the-art systems in terms of visual plausibility. In this work, we present a pipeline that converts a pretrained diffusion model into a pyramidal one through low-cost finetuning, achieving this transformation without degradation in quality of output videos. Furthermore, we investigate and compare various strategies for step distillation within pyramidal models, aiming to further enhance the inference efficiency. Our results are available at https://qualcomm-ai-research.github.io/PyramidalWan."
        },
        {
            "title": "Start",
            "content": "PyramidalWan: On Making Pretrained Video Model Pyramidal for Efficient Inference Denis Korzhenkov* Adil Karjauv* Animesh Karnewar Mohsen Ghafoorian Amirhossein Habibian Qualcomm AI Research {dkorzhen, akarjauv, karnewar, mghafoor, ahabibia}@qti.qualcomm.com 6 2 0 2 8 ] . [ 1 2 9 7 4 0 . 1 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Recently proposed pyramidal models decompose the conventional forward and backward diffusion processes into multiple stages operating at varying resolutions. These models handle inputs with higher noise levels at lower resolutions, while less noisy inputs are processed at higher resolutions. This hierarchical approach significantly reduces the computational cost of inference in multi-step denoising models. However, existing open-source pyramidal video models have been trained from scratch and tend to underperform compared to state-of-the-art systems in terms of visual plausibility. In this work, we present pipeline that converts pretrained diffusion model into pyramidal one through low-cost finetuning, achieving this transformation without degradation in quality of output videos. Furthermore, we investigate and compare various strategies for step distillation within pyramidal models, aiming to further enhance the inference efficiency. Our results are available at https://qualcommairesearch.github. io/PyramidalWan 1. Introduction Recent video diffusion models have achieved remarkable generative quality [19, 28, 35, 53]. However, these impressive capabilities come at cost: multi-step inference remains computationally expensive. The principal strategies for reducing inference overhead are step distillation and architectural optimization [7, 18, 33, 56, 57]. Beyond these approaches, several recent works have proposed training diffusion models that process inputs with different noise levels at different resolutions [5, 8, 47, 48, 58]. This method is motivated by the observation known as spectral autoregression: in the spectral decomposition of natural signals, higher-frequency components tend to have lower *Equal contribution Qualcomm AI Research is an initiative of Qualcomm Technologies, Inc. Snapdragon and Qualcomm branded products are products of Qualcomm Technologies, Inc. and/or its subsidiaries. Table 1. Computational costs. Schedule is the number of steps per each of three stages, from the lowest spatiotemporal resolution to the highest. For diffusion models, the cost is doubled due to the usage of classifier-free guidance. Inference method Schedule TFLOPs Diffusion Pyramidal diffusion 0-0-50 20-20-10 2 12,592 2 2, Step distillation Pyramidal step distillation 0-0-4 0-0-2 2-2-2 2-2-1 1-1-1 1,007 504 534 282 Table 2. Latency of single denoiser forward pass. The savings obtained due to the reduced number of tokens at stages 1 and 2 lead to 43% speedup for 2-2-1 schedule in comparison with 0-0-2 while being only 13% slower than 0-0-1. Video DiT was compiled for each stage-wise resolution separately for this measurement. Method Stage Latency, ms PyramidalWan 81 448 832 Wan-PPF 81 480 0 (hi-res) 1 (mid-res) 2 (lo-res) 0 (hi-res) 1 (mid-res) 2 (lo-res) 631.77 33.76 7.62 713.76 39.85 8.26 magnitudes and thus are eliminated earlier during the forward diffusion process [12, 16, 43]. This insight can be exploited to make generation more efficient. In the beginning, the generator starts with pure low-resolution Gaussian noise, synthesizes (still noisy) coarse structure at the same resolution, and then progressively increases the resolution simultaneously with further denoising. Jin et al. [26] proposed the formalization of this approach in form of PyramidalFlow framework. However, they only demonstrated training of such model from scratch under limited computational resources. In our work, we show that pretrained state-of-the-art diffusion models can be made pyramidal via low-cost finetuning without loss in visual quality. In detail, we begin with the pretrained Wan2.1-1.3B model [50] and decompose its forward and backward dif1 fusion processes into three spatiotemporal stages, operating at resolutions of 81 448 832, 41 224 416, and 21 112 208, respectively, see Fig. 1. We finetune the model using the pyramidal flow matching loss [26], demonstrating that this approach substantially reduces inference cost while maintaining near-original quality. Furthermore, we conduct study of various step distillation strategies within the pyramidal setup, both for conventional and pyramidal teacher diffusion models. We also demonstrate for the first time that recently proposed Pyramidal Patchification models (an alternative to PyramidalFlow) [29] can be successfully trained for few-step video generation. In addition to this empirical study, we present theoretical generalization of the resolution transition operations introduced in PyramidalFlow. Specifically, we extend these operations to arbitrary upsampling and downsampling functions based on orthogonal transforms. Notably, average pooling and nearest-neighbor upsampling, employed in the original work, can be interpreted as scaled instances of the Haar wavelet operator, fitting within our generalized framework. In summary, our contributions are as follows: 1. We show that conventional video diffusion transformer can be effectively converted into spatiotemporal pyramidal diffusion model with minimal finetuning cost and without compromising quality. 2. We conduct systematic study of step distillation techniques within the pyramidal setup, offering practical insights for various training scenarios. 3. We extend the procedure of transition between stages in the PyramidalFlow framework to broader class of upsampling functions. 2. Related works Pyramidal models. The observation that Gaussian noise degrades information across all frequency components at uniform rate has been well established in the literature [12, 16]. Simultaneously, natural signals such as images and videos are known to exhibit relatively low magnitudes in their high-frequency components [22, 23]. Together, these insights motivate multi-resolution denoising strategy. An early implementation of this idea was the cascaded diffusion model, which employed multiple denoising networks, each operating at distinct resolution [21]. Subsequent works aimed to unify this approach within single network, introducing different mechanisms for transitioning between resolutions [5, 8, 58]. PyramidalFlow [26] proposed mathematically grounded framework based on flow matching, offering coherent view of the forward diffusion process and spatial resolution changes. Building on this, TPDiff extended the methodology to the video frame rate varying per stage [41]. The practical viability of pyramidal models has also been demonstrated through deployment on resource-constrained devices such as mobile chips [27]. In our work, we adopt the PyramidalFlow framework to convert pretrained video model into pyramidal pipeline, enabling efficient inference with relatively cheap training. Patch-pyramidal models. An alternative to modifying input resolution is adjusting the kernel size of the patchification and unpatchification layers in the diffusion transformer (DiT) based on the noise level. This allows the most computationally intensive transformer blocks to operate on fewer tokens for noisier inputs, achieving similar efficiency gains to PyramidalFlow. An advantage of this approach is that it avoids the need for mathematical derivations to handle stage transitions. FlexiDiT [2] combines this strategy with learnable per-stage LoRA adapters [24], which may pose challenges for inference on resource-constrained devices. More recently, Pyramidal Patchification Flow (PPF) [29] demonstrated that such adapters are not essential, neither for training from scratch nor for finetuning pretrained text-to-image diffusion models. In our experiments, we find that under limited training budgets, PyramidalFlow outperforms PPF for diffusion-style finetuning. Nevertheless, we show that patch-pyramidal models remain strong candidate for distillation into few-step inference models. Step distillation of diffusion models. Since multi-step inference in diffusion models is computationally intensive for many applications, step distillation has become key area of research. The most widely adopted methods include adversarial distillation [45], distribution matching distillation (DMD) [37, 54, 55], and consistency models [6, 17, 44]. While most efforts in this field have focused on image generation, several recent papers have extended these techniques to video models [3, 33, 51, 61]. In our work, we explore DMD and adversarial approaches to distill few-step pyramidal version of pretrained video diffusion model, whether originally pyramidal or not. Notably, the concurrent SwD [46] and Neodragon [27] works also studied pyramidal step distillation to reduce inference costs. However, SwD did not consider the case of pyramidal teacher model, while Neodragon has not explored PPF-based training. Our work fills these gaps. 3. Preliminaries To reduce the computational cost of video generation inference, we adopt the PyramidalFlow framework [26], splitting the forward and backward diffusion processes into stages indexed by i. Each stage corresponds to specific spatiotemporal resolution, where stage = 0 operates at the original video tensor size, and stage = 1 at the lowest resolution. In practice, we use = 3. The upsampling operation transitions from stage + 1 to by doubling the number of frames, height, and width using nearest-neighbor 2 Figure 1. Inference of different types of models. Left: input and output tensors of standard DiT always have the same size, and the number of tokens in transformer blocks does not depend on the noise level. Center: in pyramidal flow matching, higher noise levels are processed at smaller spatiotemporal resolution. For transition between stages special corrective noise should be added after upsampling. Right: in PPF framework instead of changing the resolution, kernel size of patchifier is adjusted for each stage. This keeps the number of tokens equal to that in pyramidal flow matching. upsampling. Its counterpart, R, is implemented via 3D average pooling. Unlike prior works of Jin et al. [26] and Ran and Shou [41], which applied stage-wise generation to either spatial or temporal dimensions, we apply and across all three video axes. samples using shared noise tensor ϵ (0, I): y(i) = y(i) = (cid:16) (cid:16) 1 σ(i) 1 σ(i) (cid:17) (cid:17) x(i) 0 (cid:16) (cid:17) x(i+1) +σ(i) ϵ, +σ(i) ϵ. (1) (2) 3.1. Stage-wise definition of clean signal Most recent video generation models operate in the latent space of pretrained autoencoder (VAE). In this setting, the target signal x0 is defined as the output of the VAE encoder applied to an input RGB video V, i.e., x0 = E(V). Therefore, there are two options for constructing the clean signal x(i) 0 at stage > 0. The first approach, adopted by Jin et al. [26], defines it as latent signal, downsampled several times: x(i) 0 = R(x0). ① The second option performs downsampling in the original RGB space before encoding with VAE, x(i) 0 = E(Down(V, i)), ② where Down denotes suitable resizing operation, e.g. trilinear interpolation [46]. As we show below, the choice between these definitions depends on the specific training setup. 3.2. Stage-wise forward process For each stage i, we define two boundary noise levels: cleaner level σ(i) and noisier level σ(i) , such that 0 < σ(i) σ(i) 1. Here, σ = 0 corresponds to clean signal, and σ = 1 to i.i.d. Gaussian noise. Given the clean signal x(i) 0 at the appropriate resolution, we compute two boundary For the noise level σ such that σ(i) noised signal x(i) boundary samples , the σ is defined via linear interpolation between < σ σ(i) σ = (1 ρ) y(i) x(i) + ρy(i) (cid:16) (cid:17) 1 σ(i) (cid:17) 1 σ(i) x(i) 0 (cid:16) (cid:17) x(i+1) 0 (3) (4) = (1 ρ) (cid:16) + ρ + σϵ, where ρ = σσ(i) is the local noise level, 0 < ρ 1. σ(i) σ(i) For clarity, in contrast to local level ρ we refer to σ as the global noise level, since it determines the total amount of added noise, as evident from Eq. (4). At the target stage = 0 global noise levels are also called natural noise levels and denoted by ς. Although, strictly speaking Eq. (4) describes generalized stochastic interpolant [1] rather than diffusion, we will still use the term diffusion for simplicity. Importantly, in PyramidalFlow framework, resampling operations and R, stage-wise clean signals, and boundary noise levels are defined to satisfy the following equivalence of probabilistic distributions (cid:16) y(i+1) (cid:17) d= y(i) , (5) where d= means equality in distribution, and is the upsampling operation followed by the addition of some 3 amount of non-i.i.d. noise. This noise aims to decorrelate the adjacent pixels after upsampling, and its parameIn Supplementers have been derived by Jin et al. [26]. tary, we generalize the functions R, R, and from simple resampling operations (average pooling and nearest neighbor interpolation) to any resizing methods based on orthogonal transforms, e.g. wavelets. Our generalization involves sampling the missing high-frequency components before upscaling from Gaussian noise. This enables decorrelation even when the upscaling operation involves interaction between pixels. Equation (5) implies that the cleaner boundary sample from stage + 1 after upsampling with becomes valid noisier boundary sample of stage i. This establishes relation between noise levels across two consecutive stages. This relation, if applied recursively, allows to map any global noise level σ(i) at stage to corresponding natural noise level ς at stage 0. In Supplementary, we show that natural noise levels corresponding to different stages do not overlap. This ensures that the natural level ς is unique conditioning value for the denoising network in the pyramidal setup, independent of the number of stages S. The noisier bounds {σ(i) }i are selected in practice via spectral analysis of noised samples at each stage. should be large enough that high-frequency components become indistinguishable from scaled Gaussian noise, Specifically, σ(i) i.e. x(i) y(i) (cid:16) (cid:17) (cid:18)(cid:16) 1 σ(i) (cid:17) x(i) 0 , (cid:17)2 (cid:16) σ(i) (cid:19)"
        },
        {
            "title": "The pyramidal loss is defined then as",
            "content": "Lpyr (θ) = (cid:88) x(i) 0 EϵEρ (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) Fθ (cid:16) x(i) σ , ς (cid:17) dx(i) σ dσ (cid:13) 2 (cid:13) (cid:13) (cid:13) (cid:13) , (7) for uniformly distributed local level ρ Uni (0, 1) . Distillation loss. In addition to flow matching, we apply distillation loss to align students partially denoised latents with the teachers predictions. For stage and global noise level σ, σ(i) , we first map σ to the natural noise level ς as discussed in Sec. 3.2. We sampled the highresolution noise ϵ(0) at stage 0, and construct the teachers prediction for the noisy input xς = (1 ς) x0 + ςϵ(0) as < σ σ(i) xς (i) = xς (cid:16) ς ς (i) (cid:17) (xς , ς), (8) where ς (i) cleaner bound of the denoising process at stage i. is the natural noise level corresponding to the Next, we downsample the noise to stage and scale by constant to preserve unit variance, ϵ(i) . . . R(cid:0)ϵ(0)(cid:1). Using ϵ(i), we construct boundary samples y(i) and y(i) σ . Students singlestep prediction of the cleaner boundary value equals for the students noised input x(i) y(i) = x(i) σ (cid:16) σ σ(i) (cid:17) (cid:16) x(i) σ , ς (cid:17) . Fθ (9) . This allows to downsamThe distillation loss is then defined as ple and proceed with the forward process at stage i+1 without loss of information [12, 46]. Ldist(θ) = (cid:88) x(i) 0 ,x ϵ(0)Eσ (cid:13) (cid:13)y(i) (cid:13) xς (i) (cid:13) 2 (cid:13) (cid:13) . (10) 4. Method 4.1. Pyramidal finetuning To convert pretrained conventional diffusion model to its pyramidal version, we apply finetuning with the dedicated loss function. Flow matching loss. We start with the pretrained opensourced Wan2.1-1.3B model . For brevity, hereinafter we omit the conditioning text prompt in the notation. Since this model was trained with flow matching loss [50], it approximates the derivative of the noised signal w.r.t. the noise level [34], In early experiments, we found that training with latents downsampled in pixel space, e.g. using Definition ②, yields significantly better visual results. This observation aligns with findings of Starodubcev et al. [46]. We refer to the resulting model Fθ as PyramidalWan. 4.2. Pyramidal step distillation While pyramidal diffusion alone reduces computational cost by 78% (see Tab. 1), we aim to further decrease latency through step distillation. Below, we describe how distribution matching distillation (DMD) and adversarial technique are adapted to the pyramidal framework. (xσ, σ) (cid:21) (cid:20) dxσ dσ xσ . (6) 4.2.1. DMD with original teacher To preserve this property during pyramidal finetuning, at each stage we define the objective for the student network Fθ as the derivative w.r.t. the global noise level, i.e. dx(i) σ dσ = dx(i) σ dρ dρ dσ = y(i) y(i) σ(i) σ(i) . Since in case of Wan2.1 model we have access to the original pretrained non-pyramidal model , we can use it as teacher in DMD pipeline [54]. In this method, the student model Fξ is trained to predict the clean signal at the i-th stage in single step, ˆx(i) 0 = xσ(i) σ(i) Fξ(xσ(i), ς). Notably, to construct the input xσ(i) we follow the rollout 4 Figure 2. Examples of video generations. Videos produced by our pyramidal step-distilled model are similar in quality to outputs of more computationally expensive baselines. strategy proposed by Yin et al. [54], using detached output of the student: (cid:16) (cid:17) (cid:16) (cid:104) Fξ (cid:17)(cid:105) + σ(i) σ(i) xσ(i) = y(i) stopgrad . (11) Once the clean signal ˆx(i) is predicted, it is re-noised ac0 cording to the teachers forward process with noise level σ, 0 < σ 1, , ς (i) y(i) σ = (1 σ) ˆx(i) ˆx(i) 0 + σε. The fake score network Fφ is trained using regular flow matching loss, aiming to denoise the students predictions, Lfm(φ) = (cid:88) ˆx(i) σ (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) Fφ (cid:16) σ , σ(cid:17) ˆx(i) dˆx(i) σ dσ (cid:13) 2 (cid:13) (cid:13) (cid:13) (cid:13) . The gradient of DMD loss is defined as follows, ξLdmd(ξ) = σ , σ(cid:17) (cid:16) ˆx(i) Fφ (cid:16) (cid:16) σ , σ(cid:17)(cid:17) ˆx(i) ξFξ(xσ(i), ς). (12) For each training sample, weight of the loss Ldmd is set equal to wdmd, giving greater weight to samples whose re-noised versions can be well denoised by the teacher . To stabilize training, we also include supervised loss that encourages the students output to be similar to the teachers one [44], Lteach(ξ) = (cid:13) (cid:13)Fξ(xσ(i), ς) (cid:0)xσ(i), σ(i)(cid:1)(cid:13) 2 . This term is (cid:13) added to the DMD loss with weight of 0.01. We found experimentally that such training does not work for the original pretrained Wan2.1-1.3B teacher due to its inability to generate videos at the lowest spatiotemporal resolution, i.e. for = 1. While this may seem to contradict the results recently reported by Starodubcev et al. [46], we note that their SwD method gradually upscales the video tensor by fractional factor after each step, likely reaching the resolution compatible with the teacher model earlier in the generation process. In our case, to remain consistent with PyramidalFlow framework, we briefly finetuned the teacher using flow matching loss on dataset of videos with varying resolution. Clean latent tensors were generated using Definition ②, which was also used for training. In the beginning of step distillation, both the student and fake score network were initialized from the finetuned teachers checkpoint. At inference time, we use the same sampling algorithm as in pyramidal diffusion, but with only few steps per stage [32]. The resulting few-step generator is referred to as PyramidalWan-DMD-OT. wdmd = σ(i) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) σ , σ(cid:17) (cid:16) ˆx(i) dˆx(i) σ dσ 1 (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) 1 , 4.2.2. DMD with pyramidal teacher (13) The DMD pipeline described above requires modifications when employing pyramidal flow matching model 5 as the teacher. This setup is increasingly relevant given the reported training efficiency of pyramidal diffusion models [29, 47]. The key difference arises from the fact that pyramidal teacher has been trained with stage-wise inputs. Therefore, students prediction of the clean signal ˆx(i) 0 should be re-noised similarly to Eqs. (1) and (2), ˆy(i) = (cid:16) 1 σ(i) (cid:17) (cid:16) 1 σ(i) ˆy(i) = ˆx(i) σ = (1 ρ) ˆy(i) ˆx(i) 0 R (cid:16) (cid:17) + ρˆy(i) . +σ(i) ε, +σ(i) ε, (cid:17) ˆx(i) 0 (14) (15) (16) This definition of ˆy(i) assumes that per-stage clean signals follow Definition ①, in contrast to how the the pyramidal diffusion model has been trained (see Sec. 4.1). Using Definition ② would require both VAE decoder and encoder to compute ˆx(i+1) , which is computationally expensive for video models. To address this, before applying the DMD pipeline, we finetuned our pyramidal diffusion model Fθ according to this definition, resulting in the model Fθ1. We found that this approach works better than training with Definition ① from the beginning. Importantly, using the original Fθ as teacher leads to unsatisfactory quality of few-step generations. The fake score network Fφ is trained with pyramidal flow matching loss Lpyr(φ) from Eq. (7), but with re-noised predicted signals ˆx(i) instead of ground-true noised signals 0 x(i) 0 . The original DMD formulation relies on estimating the score function, or equivalently, the added Gaussian noise ε [1, 37, 55]. To construct such an estimator, we define (i), linear combination of ˆy(i) and ˆy(i) , (i) = σ(i) = σ(i) ˆy(i) ˆy(i) σ(i) (cid:17) (cid:16) ˆx(i) 1 σ(i) 0 (cid:17) (cid:16) σ(i) 1 σ(i) (17) (18) R(cid:16) (cid:17) . ˆx(i) 0 Applying R to both LHS and RHS and using the fact that R R = R, gives R(cid:16) (i)(cid:17) = (cid:16) σ(i) σ(i) (cid:17) R(cid:16) (cid:17) . ˆx(i) 0 (19) Together with Eq. (15), this yields closed-form expression for ε given ˆy(i) and ˆy(i) . For pretrained pyramidal teacher or pyramidal fake score model Fν, where ν {θ1, φ}, the stage boundary (cid:16) σ(cid:17) σ , ς (cid:17) samples are estimated as ˆx(i) σ(i) ˆx(i) σ + σ , ς (cid:17) σ(cid:17) ˆx(i) σ(i) and ˆx(i) respectively. σ + Fν Fν (cid:16) (cid:16) (cid:16) The noise estimator ˆεν equals then (we omit arguments of Fν for brevity) (cid:16) σ(cid:17) σ(i) Fν (cid:17) ˆεν = (cid:16) ˆx(i) σ + 1 σ(i) 1 σ(i) σ(i) (cid:16) R(cid:16) ˆx(i) σ (cid:17) (cid:17) σ R(Fν) ."
        },
        {
            "title": "The difference between the estimators provided by the\nteacher and the fake score model is proportional to",
            "content": "ˆεφ ˆεθ1 β1 (Fφ Fθ1 ) + β2 R(Fφ Fθ1), σ, β2 = σ (cid:16) where β1 = σ(i) these weights as βk = βk for = 1, 2. Similarly, the gradient of the re-noised prediction is proportional to the following weighted sum . We normalize 1 σ(i) β1+β (cid:17) ξˆx(i) σ γ1ξFξ + γ2ξR R(Fξ), (20) with the weights γ1 = (cid:17) (cid:16) (cid:16) σ σ(i) γk γ1+γ2 1 σ(i) (cid:17) (cid:16) σ(cid:17) (cid:16) σ(i) 1 σ(i) (cid:17) , γ2 = . We normalize these as γk = and define the gradient of pyramidal DMD loss as ξLdmd-pyr(ξ) = (cid:16) β1 (Fφ Fθ1) + β2R R(Fφ Fθ1) (21) (cid:0)γ1Fξ + γ2R R(Fξ)(cid:1) . (cid:17) ξ Each sample in Ldmd-pyr loss is weighted with wdmd-pyr = ρ (cid:13) (cid:13) (cid:13) Fθ1 (cid:13) (cid:13) (cid:16) σ , ς (cid:17) ˆx(i) dˆx(i) σ dσ 1 (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) 1 . (22) We call the resulting model PyramidalWan-DMD-PT. In addition, we explored the simplified version by setting β1 = γ1 = 1 and β2 = γ2 = 0 in Eq. (21). This reduces Ldmd-pyr to the formulation similar to Eq. (12). Although such variant has insufficient theoretical grounding, we found that it performs marginally better in practice. This modification is referred to as -PT . 4.2.3. Adversarial distillation As an alternative to DMD, we explore pyramidal adversarial distillation. In this setting, student Fξ similarly predicts cleaner boundary sample ˆy(i) of the current stage in single step, while discriminator attempts to distinguish between features extracted from generated and ground-true samples. The discriminator consists of two components: 1. Frozen feature extractor , based on pretrained diffusion model. We denote the model as PyramidalWanAdv-OD when using the backbone from the original Wan model , and -PD when using the pyramidal backbone of Fθ instead. 6 2. Trainable discriminator head Dφ attached to the final block of , comprises spatial and temporal branches implemented with lightweight convolutional layers and residual blocks. The discriminator minimizes an adversarial Hinge loss [30] over features at each stage, LD(φ) = (cid:88) (cid:104) y(i) (cid:2) max(0, 1 Dφ(F (y(i) )))(cid:3) + ˆy(i) (cid:2) max(0, 1 + Dφ(F (ˆy(i) )))(cid:3)(cid:105) . The student optimizes combined objective balancing adversarial and reconstruction terms, weighted by λadv and λrec, similarly to the approach of Zhang et al. [59]. Empirically, we find that λadv = 1 and λrec = 2 yield the highest visual quality. LG(ξ) = (cid:88) E(cid:2) λadv Dφ(F (ˆy(i) )) + λrec ˆy(i) y(i) 2 2 (cid:3) 4.3. Patch-pyramidal training An alternative approach for varying per-stage computational cost is Pyramidal Patchification Flow (PPF) [29]. PPF does not alter the resolution of the denoiser transformers inputs or outputs. Instead, it introduces stage-wise patchification and unpatchification layers, as Fig. 1 shows. For earlier stages, kernel size of patchifier is accordingly increased, and therefore, the transformers blocks in PPF operate with exactly the same number of tokens as for PyramidalFlow. Importantly, diffusion training, step distillation, and inference can be performed within the PPF framework in the same way as for the original pretrained Wan model. 5. Experiments 5.1. Training setup Step distillation of the original model. As natural baseline for reducing inference cost, we trained step-distilled version of the original Wan model. Following the DMD pipeline described in Sec. 4.2.1, we adopted the one-step rollout strategy [54] but omitted the adversarial loss, as we found it unnecessary. This training setup does not require visual data and relies solely on text prompts; we used prompts from the 350K subset of the dataset provided by Lin et al. [31]. Training was conducted for 31K iterations on 16 H100 GPUs. As an alternative, we performed adversarial distillation on the original Wan model, following the procedure similar to the one outlined in Sec. 4.2.3. The model was trained for 30K iterations on single H100 GPU using 80K synthetic videos generated by Wan2.1-14B, larger variant of the original model. Pyramidal flow matching and step distillation. For these experiments, we used the same synthetic dataset of 80K videos generated by the Wan2.1-14B model. We observed that models trained on the synthetic data produce visually superior results compared to those trained on real video samples. To ensure compatibility with Wans patchification layer at the lowest stage (i = 2), we slightly reduced the spatial resolution of videos from the default 480 832 to 448 832, making thus both height and width divisible by 64. All models mentioned in Secs. 4.1 and 4.2 were fine-tuned for 5K iterations with batch size of 6 per GPU (2 samples per stage) on two H100 GPUs. For DMD-PT and DMD-PT we conducted training with LoRA adapters, since we found that otherwise training might diverge. In other cases, we finetuned all the weights. Patch-pyramidal training. To train the diffusion-based Wan-PPF model, we used the same setup as for PyramidalWan. Training details of patch-pyramidal DMD model were kept consistent with those used for DMD distillation of the original Wan model, except for the smaller training budget. This experiment was conducted for 5K steps on 8 GPUs, with batch size of 3 per GPU (one sample per stage). We did not observe improvements with longer training. 5.2. Results Main results. We evaluate the quality of generated videos using the VBench and VBench-2.0 toolkits [25, 60]. Our results are summarized in Tab. 3. First, we observe that PyramidalWan, i.e. the pyramidized diffusion model, achieves scores comparable to the original Wan model sampled with 50 steps, while being approximately 4.5 times more efficient in terms of FLOPs. Notably, it also achieves the highest Semantic score among all evaluated models, resembling the findings of the concurrent work of Zhang et al. [58]. While Li et al. [29] successfully demonstrated finetuning of pretrained text-to-image models within the PPF framework, we found that extending this approach to video generation is challenging. Using the same training budget and dataset as in our pyramidal training setup, our PPF-based text-to-video diffusion model failed to converge, and the quality of generated clips remained unsatisfactory. Notably, increasing the compute budget to 8 GPUs did not improve results. Step distillation using DMD also failed when the new patchification and unpatchification layers were initialized according to the scheme proposed in the original PPF work. Surprisingly, however, the DMD pipeline could still be applied successfully when the student model was initialized with the pretrained PPF diffusion model even though that chekpoint itself produced poor-quality generations. We attribute this to the mode-seeking behavior of the reverse-KL objective used in DMD [52]. Among few-step models, the step-distilled baseline Wan-DMD provides very strong performance combined with efficient inference. Despite the significant gains in testTable 3. Model comparison. Step-distilled versions of the original model, Wan-DMD and Wan-Adv, provide the quality of generation in the few-step mode on par with the diffusion model. However, they cannot unlock the satisfactory single-step inference. Pyramidal models with single step at highest resolution fill the gap and demonstrate good performance, as measured quantitatively. Model Wan2.1-1.3B (50 steps) Wan2.1-1.3B (25 steps) PyramidalWan (20-20-10) Wan-Adv (4 steps) Wan-Adv (2 steps) Wan-Adv (1 step) Wan-DMD (4 steps) Wan-DMD (2 steps) Wan-DMD (1 step) PyramidalWan-Adv-OD (2-2-1) PyramidalWan-Adv-PD (2-2-1) Wan-PPF-DMD (2-2-1) PyramidalWan-DMD-OT (2-2-1) PyramidalWan-DMD-PT (2-2-1) VBench VBench-2.0 Total Quality Semantic Total Creativity Commonsense Controllability Human Fidelity Physics 82.49 80.87 82. 82.72 82.35 80.28 83.33 83.28 79.45 82.90 82.57 82.39 82.86 82.72 83.47 82.09 83.36 84.06 83.74 81.38 84.71 84.00 80.63 83.94 84.20 83.04 83.63 83.46 78.57 76.02 80. 77.39 76.82 75.85 77.86 80.41 74.75 78.74 76.07 79.80 79.80 79.75 56.02 55.73 54.93 55.40 54.82 50.36 57.48 56.67 53.17 52.29 54.30 53.45 55.36 51.75 48.73 49.49 44. 50.96 47.07 38.51 50.87 47.00 38.69 44.80 45.00 40.04 50.04 34.81 63.38 62.50 64.33 57.33 58.47 54.41 58.16 62.81 59.31 60.86 61.98 62.84 64.34 63.18 33.96 35.01 28. 32.56 31.51 29.99 36.36 37.07 35.36 22.86 25.85 24.76 29.05 28.48 80.71 79.44 85.38 85.58 82.72 79.60 82.80 80.73 77.49 81.74 87.65 91.61 77.50 81.38 53.30 52.20 51. 50.57 54.36 49.29 59.23 55.72 55.00 51.17 51.01 47.99 56.78 50.92 time efficiency (see Tab. 1), it surpasses the original diffusion model in both total scores even when sampled with only two steps. However, single-step generation remains infeasible as the quality of videos drops substantially. We fill this gap with our few-step pyramidal models. They are evaluated in scenario with only one step at highest resolution, i.e. at stage = 0, and few steps at lower-resolution stages. Given the much lower computational cost and latency of these stages, as reported in Tabs. 1 and 2, we adopt 2-2-1 inference schedule (steps per stage in resolutionincreasing order). All models under this schedule achieve total score of VBench comparable to Wan diffusion model, with only minor drop relative to Wan-DMD sampled with 2 steps. VBench-2.0, however, indicates some degradation, in particular for Creativity and Controllability dimensions. Although distillation with the original teacher, PyramidalWanDMD-OT, gets the best metrics within this set of models, we observed that its outputs often exhibit oversaturated colors and cartoon-alike appearance. Please refer to Supplementary for visual examples. Among all models, we found that PyramidalWan-DMD-PT produced the most visually appealing results, and therefore selected it for the user study. In the study, the assessors were shown pairs of videos generated from identical prompts and asked to choose the preferred one or select no preference. Each pair included one video generated with step-distilled pyramidal model, and one from either Wan with 50 steps (first study) or WanDMD with 2 steps (second). In total, we collected 700 responses. We conducted the binomial test for the hypothesis Baseline is strictly preferred with probability 0.5 and the one-sided less alternative. As shown in Tab. 4, in both comparisons the hypothesis should be rejected. This indicates that the participants found the quality of this model on par with more computationally expensive baselines deTable 4. User study. We evaluate our pyramidal model with 2-21 inference schedule against two baselines. As the results show, participants did not find significant difference in visual quality. Baseline Preference, % p-value Ours No preference Baseline Wan (50 steps) Wan-DMD (2 steps) 29.1 33. 29.1 35.4 41.7 31.4 < 0.001 < 0.001 Table 5. Ablation study. Training without distillation loss improves the VBench-2.0 score but reduces the amount of motion in generated videos. Simplified version of DMD objective leads to better empirical results. Model Total score VBench VBench-2.0 PyramidalWan-DMD-PT PyramidalWan-DMD-PT w/o Lteach PyramidalWan-DMD-PT 82.72 82.44 82.56 51.75 52.36 50.67 spite its lower VBench-2.0 score. Examples of videos are provided in Fig. 2. Ablations. To check the impact of distillation losses in both pyramidal finetuning and step distillation experiments, we conducted experiments by removing this terms. For PyramidalWan, this reduced VBench-2.0 total score from 54.93 to 54.02. Notably, for DMD with pyramidal teacher effect was the opposite (see Tab. 5), however at the expense of noticeable reduction of Dynamic Degree. As mentioned in Sec. 4.2.2, the simplified version of Ldmd-pyr objective yields improved scores despite its theoretical weakness. We leave further investigation of this phenomenon for the future. 6. Conclusion In this work we explored pyramidization strategy of reducing inference costs of video diffusion models that is complementary to other architectural innovations. We presented pipeline of converting pretrained con8 ventional diffusion model into pyramidal one, both for multi-step and few-step inference regimes. Further, we demonstrated step distillation of pretrained pyramidal diffusion model: an important milestone for future research given the reported training efficiency of such systems. In addition, we made theoretical contribution by extending the procedure of switching between resolutions to broader class of upsampling operations. The resulting models occupy the practical niche enabling few-step generation with only single step at the target resolution. While demonstrating results comparable to more costly baselines in the human preference study, our models still lag behind in certain quantitative metrics. Addressing this gap remains promising direction for future work."
        },
        {
            "title": "References",
            "content": "[1] Michael S. Albergo, Nicholas M. Boffi, and Eric VandenEijnden. Stochastic interpolants: unifying framework for flows and diffusions, 2025. 3, 6 [2] Sotiris Anagnostidis, Gregor Bachmann, Yeongmin Kim, Jonas Kohler, Markos Georgopoulos, Artsiom Sanakoyeu, Yuming Du, Albert Pumarola, Ali Thabet, and Edgar Schönfeld. Flexidit: Your diffusion transformer can easily generate high-quality samples with less compute. In CVPR, 2025. 2 [3] Haitam Ben Yahia, Denis Korzhenkov, Ioannis Lelekas, Amir Ghodrati, and Amirhossein Habibian. Mobile video diffusion. In ICCV, 2025. 2 [4] Daniel Bolya and Judy Hoffman. Token merging for fast stable diffusion. CVPRW, 2023. A3 [5] Andrew Campbell, William Harvey, Christian Dietrich Weilbach, Valentin De Bortoli, Tom Rainforth, and Arnaud Doucet. Trans-dimensional generative modeling via jump diffusion models. In NeurIPS, 2023. 1, 2 [6] Junsong Chen, Shuchen Xue, Yuyang Zhao, Jincheng Yu, Sayak Paul, Junyu Chen, Han Cai, Song Han, and Enze Xie. Sana-sprint: One-step diffusion with continuous-time consistency distillation. In ICCV, 2025. [7] Junsong Chen, Yuyang Zhao, Jincheng Yu, Ruihang Chu, Junyu Chen, Shuai Yang, Xianbang Wang, Yicheng Pan, Daquan Zhou, Huan Ling, Haozhe Liu, Hongwei Yi, Hao Zhang, Muyang Li, Yukang Chen, Han Cai, Sanja Fidler, Ping Luo, Song Han, and Enze Xie. Sana-video: Efficient video generation with block linear diffusion transformer, 2025. 1 [8] Shoufa Chen, Chongjian Ge, Shilong Zhang, Peize Sun, and Ping Luo. Pixelflow: Pixel-space generative models with flow, 2025. 1, 2 [9] Ting Chen. On the Importance of Noise Scheduling for Diffusion Models, 2023. A2 [10] Joonmyung Choi, Sanghyeok Lee, Jaewon Chu, Minhyuk Choi, and Hyunwoo J. Kim. vid-TLDR: Training free token merging for light-weight video transformer. In CVPR, 2024. A3 [11] Rohan Choudhury, Guanglei Zhu, Sihan Liu, Koichiro Niinuma, Kris M. Kitani, and Laszlo Attila Jeni. Dont look twice: Faster video transformers with run-length tokenization. In NeurIPS, 2024. A3 [12] Sander Dieleman. Diffusion is spectral autoregression, 2024. 1, 2, 4, A4 [13] Juechu Dong, BOYUAN FENG, Driss Guessous, Yanbo Liang, and Horace He. Flexattention: programming model for generating fused attention variants. In MLSys, 2025. A4 [14] Yilun Du, Shuang Li, and Igor Mordatch. Compositional In NeurIPS, visual generation with energy based models. 2020. A3 [15] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, and Robin Rombach. Scaling rectified flow transformers for high-resolution image synthesis. In ICML, 2024. A2, [16] Fabian Falck, Teodora Pandeva, Kiarash Zahirnia, Rachel Lawrence, Richard Turner, Edward Meeds, Javier Zazo, and Sushrut Karmalkar. fourier space perspective on diffusion models, 2025. 1, 2 [17] Zhengyang Geng, Mingyang Deng, Xingjian Bai, J. Zico Kolter, and Kaiming He. Mean Flows for One-step Generative Modeling, 2025. 2 [18] Mohsen Ghafoorian, Denis Korzhenkov, and Amirhossein Habibian. Attention surgery: An efficient recipe to linearize your video diffusion transformer, 2025. 1 [19] Yoav HaCohen, Nisan Chiprut, Benny Brazowski, Daniel Shalem, Dudu Moshe, Eitan Richardson, Eran Levin, Guy Shiran, Nir Zabari, Ori Gordon, Poriya Panet, Sapir Weissbuch, Victor Kulikov, Yaki Bitterman, Zeev Melumian, and Ofir Bibi. Ltx-video: Realtime video latent diffusion, 2024. 1 [20] Byeongho Heo, Song Park, Dongyoon Han, and Sangdoo Yun. Rotary Position Embedding for Vision Transformer. In ECCV, Cham, 2025. A3 [21] Jonathan Ho, Chitwan Saharia, William Chan, David J. Fleet, Mohammad Norouzi, and Tim Salimans. Cascaded Diffusion Models for High Fidelity Image Generation, 2021. [22] Emiel Hoogeboom, Jonathan Heek, and Tim Salimans. simple diffusion: End-to-end diffusion for high resolution images. In ICML, 2023. 2, A2 [23] Emiel Hoogeboom, Thomas Mensink, Jonathan Heek, Kay Lamerigts, Ruiqi Gao, and Tim Salimans. Simpler diffusion: 1.5 fid on imagenet512 with pixel-space diffusion. In CVPR, 2025. 2 [24] Edward Hu, yelong shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In ICLR, 2022. 2 [25] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive benchmark suite for video generative models. In CVPR, 2024. 7 [26] Yang Jin, Zhicheng Sun, Ningyuan Li, Kun Xu, Kun Xu, Hao Jiang, Nan Zhuang, Quzhe Huang, Yang Song, Yadong MU, and Zhouchen Lin. Pyramidal Flow Matching for Efficient Video Generative Modeling. In ICLR, 2025. 1, 2, 3, 4 [27] Animesh Karnewar, Denis Korzhenkov, Ioannis Lelekas, Adil Karjauv, Noor Fathima, Hanwen Xiong, Vancheeswaran Vaidyanathan, Will Zeng, Rafael Esteves, Tushar Singhal, Fatih Porikli, Mohsen Ghafoorian, and Amirhossein Habibian. Neodragon: Mobile video generation using diffusion transformer, 2025. 2, A3 [28] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, Kathrina Wu, Qin Lin, Junkun Yuan, Yanxin Long, Aladdin Wang, Andong Wang, Changlin Li, Duojun Huang, Fang Yang, Hao Tan, Hongmei Wang, Jacob Song, Jiawang Bai, Jianbing Wu, Jinbao Xue, Joey Wang, Kai Wang, Mengyang Liu, Pengyu Li, Shuai Li, Weiyan Wang, Wenqing Yu, Xinchi Deng, Yang Li, Yi Chen, Yutao Cui, Yuanbo Peng, Zhentao Yu, Zhiyu He, Zhiyong Xu, Zixiang Zhou, Zunnan Xu, Yangyu Tao, Qinglin Lu, Songtao Liu, Dax Zhou, Hongfa Wang, Yong Yang, Di Wang, Yuhong Liu, Jie Jiang, and Caesar Zhong. Hunyuanvideo: systematic framework for large video generative models, 2025. 1 [29] Hui Li, Baoyou Chen, Liwei Zhang, Jiaye Li, Jingdong Wang, and Siyu Zhu. Pyramidal patchification flow for visual generation, 2025. 2, 6, 7 [30] Jae Hyun Lim and Jong Chul Ye. Geometric gan, 2017. 7 [31] Bin Lin, Yunyang Ge, Xinhua Cheng, Zongjian Li, Bin Zhu, Shaodong Wang, Xianyi He, Yang Ye, Shenghai Yuan, Liuhan Chen, Tanghui Jia, Junwu Zhang, Zhenyu Tang, Yatian Pang, Bin She, Cen Yan, Zhiheng Hu, Xiaoyi Dong, Lin Chen, Zhang Pan, Xing Zhou, Shaoling Dong, Yonghong Tian, and Li Yuan. Open-sora plan: Open-source large video generation model, 2024. [32] Shanchuan Lin, Bingchen Liu, Jiashi Li, and Xiao Yang. Common diffusion noise schedules and sample steps are flawed. In WACV, 2024. 5 [33] Shanchuan Lin, Xin Xia, Yuxi Ren, Ceyuan Yang, Xuefeng Xiao, and Lu Jiang. Diffusion adversarial post-training for one-step video generation. In ICML, 2025. 1, 2 [34] Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow matching for generative modeling. In ICLR, 2023. 4 [35] Dongyang Liu, Shicheng Li, Yutong Liu, Zhen Li, Kai Wang, Xinyue Li, Qi Qin, Yufei Liu, Yi Xin, Zhongyu Li, Bin Fu, Chenyang Si, Yuewen Cao, Conghui He, Ziwei Liu, Yu Qiao, Qibin Hou, Hongsheng Li, and Peng Gao. Luminavideo: Efficient and flexible video generation with multiscale next-dit, 2025. 1 [36] Feng Liu, Shiwei Zhang, Xiaofeng Wang, Yujie Wei, Haonan Qiu, Yuzhong Zhao, Yingya Zhang, Qixiang Ye, and Fang Wan. Timestep embedding tells: Its time to cache for video diffusion model. In CVPR, 2025. A3 [37] Weijian Luo, Tianyang Hu, Shifeng Zhang, Jiacheng Sun, Zhenguo Li, and Zhihua Zhang. Diff-Instruct: Universal Approach for Transferring Knowledge From Pre-trained Diffusion Models. In NeurIPS, 2023. 2, [38] TorchVision maintainers and contributors. Torchvision: Pytorchs computer vision library. https://github.com/ pytorch/vision, 2016. A5 [39] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming 10 Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. PyTorch: An Imperative Style, High-Performance Deep Learning Library. In NeurIPS, 2019. A5 [40] Elia Peruzzo, Adil Karjauv, Nicu Sebe, Amir Ghodrati, and Amir Habibian. Adaptor: Adaptive token reduction for video diffusion transformers. In CVPRW, 2025. A3 [41] Lingmin Ran and Mike Zheng Shou. Tpdiff: Temporal pyramid video diffusion model, 2025. 2, 3 [42] Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In SIGKDD, 2020. A5 [43] Severi Rissanen, Markus Heinonen, and Arno Solin. Generative modelling with inverse heat dissipation. In ICLR, 2023. 1 [44] Amirmojtaba Sabour, Sanja Fidler, and Karsten Kreis. Align your flow: Scaling continuous-time flow map distillation. In NeurIPS, 2025. 2, 5 [45] Axel Sauer, Frederic Boesel, Tim Dockhorn, Andreas Blattmann, Patrick Esser, and Robin Rombach. Fast highresolution image synthesis with latent adversarial diffusion distillation, 2024. 2 [46] Nikita Starodubcev, Denis Kuznedelev, Artem Babenko, and Dmitry Baranchuk. Scale-wise distillation of diffusion models, 2025. 2, 3, 4, 5, [47] MiniMax Team. MiniMax Hailuo 02, World-Class Quality, Record-Breaking Cost Efficiency - MiniMax News, 2025. https : / / www . minimax . io / news / minimax - hailuo-02. 1, 6 [48] Jiayan Teng, Wendi Zheng, Ming Ding, Wenyi Hong, Jianqiao Wangni, Zhuoyi Yang, and Jie Tang. Relay diffusion: Unifying diffusion process across resolutions for image synthesis. In ICLR, 2024. 1 [49] Patrick von Platen, Suraj Patil, Anton Lozhkov, Pedro Cuenca, Nathan Lambert, Kashif Rasul, Mishig Davaadorj, Dhruv Nair, Sayak Paul, William Berman, Yiyi Xu, Steven Liu, and Thomas Wolf. Diffusers: State-of-the-art diffusion models. https://github.com/huggingface/ diffusers, 2022. A4 [50] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, Jianyuan Zeng, Jiayu Wang, Jingfeng Zhang, Jingren Zhou, Jinkai Wang, Jixuan Chen, Kai Zhu, Kang Zhao, Keyu Yan, Lianghua Huang, Mengyang Feng, Ningyi Zhang, Pandeng Li, Pingyu Wu, Ruihang Chu, Ruili Feng, Shiwei Zhang, Siyang Sun, Tao Fang, Tianxing Wang, Tianyi Gui, Tingyu Weng, Tong Shen, Wei Lin, Wei Wang, Wei Wang, Wenmeng Zhou, Wente Wang, Wenting Shen, Wenyuan Yu, Xianzhong Shi, Xiaoming Huang, Xin Xu, Yan Kou, Yangyu Lv, Yifei Li, Yijing Liu, Yiming Wang, Yingya Zhang, Yitong Huang, Yong Li, You Wu, Yu Liu, Yulin Pan, Yun Zheng, Yuntao Hong, Yupeng Shi, Yutong Feng, Zeyinzi Jiang, Zhen Han, Zhi-Fan Wu, and Ziyu Liu. Wan: Open and advanced large-scale video generative models, 2025. 1, 4 [51] Yushu Wu, Zhixing Zhang, Yanyu Li, Yanwu Xu, Anil Kag, Yang Sui, Huseyin Coskun, Ke Ma, Aleksei Lebedev, Ju Hu, Dimitris N. Metaxas, Yanzhi Wang, Sergey Tulyakov, and Jian Ren. Snapgen-v: Generating five-second video within five seconds on mobile device. In CVPR, 2025. 2 [52] Yilun Xu, Weili Nie, and Arash Vahdat. One-step Diffusion Models with -Divergence Distribution Matching, 2025. 7 [53] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, Da Yin, Yuxuan.Zhang, Weihan Wang, Yean Cheng, Bin Xu, Xiaotao Gu, Yuxiao Dong, and Jie Tang. Cogvideox: Text-to-video diffusion models with an expert transformer. In ICLR, 2025. 1 [54] Tianwei Yin, Michaël Gharbi, Taesung Park, Richard Zhang, Eli Shechtman, Fredo Durand, and William T. Freeman. Improved Distribution Matching Distillation for Fast Image Synthesis. In NeurIPS, 2024. 2, 4, 5, 7 [55] Tianwei Yin, Michaël Gharbi, Richard Zhang, Eli Shechtman, Frédo Durand, William T. Freeman, and Taesung Park. One-step Diffusion with Distribution Matching Distillation. In CVPR, 2024. 2, 6 [56] Peiyuan Zhang, Yongqi Chen, Haofeng Huang, Will Lin, Zhengzhong Liu, Ion Stoica, Eric P. Xing, and Hao Zhang. In Faster video diffusion with trainable sparse attention. NeurIPS, 2025. 1 [57] Peiyuan Zhang, Yongqi Chen, Runlong Su, Hangliang Ding, Ion Stoica, Zhengzhong Liu, and Hao Zhang. Fast video generation with sliding tile attention. In ICML, 2025. 1 [58] Yuechen Zhang, Jinbo Xing, Bin Xia, Shaoteng Liu, Bohao Peng, Xin Tao, Pengfei Wan, Eric Lo, and Jiaya Jia. Training-free efficient video generation via dynamic token carving. In NeurIPS, 2025. 1, 2, 7, A1, A3, [59] Zhixing Zhang, Yanyu Li, Yushu Wu, yanwu xu, Anil Kag, Ivan Skorokhodov, Willi Menapace, Aliaksandr Siarohin, Junli Cao, Dimitris N. Metaxas, Sergey Tulyakov, and Jian Ren. SF-v: Single forward video generation model. In NeurIPS, 2024. 7 [60] Dian Zheng, Ziqi Huang, Hongbo Liu, Kai Zou, Yinan He, Fan Zhang, Yuanhan Zhang, Jingwen He, Wei-Shi Zheng, Yu Qiao, et al. Vbench-2.0: Advancing video generation benchmark suite for intrinsic faithfulness, 2025. 7 [61] Kaiwen Zheng, Yuji Wang, Qianli Ma, Huayu Chen, Jintao Zhang, Yogesh Balaji, Jianfei Chen, Ming-Yu Liu, Jun Zhu, and Qinsheng Zhang. Large Scale Diffusion Distillation via Score-Regularized Continuous-Time Consistency, 2025. 2 11 PyramidalWan: On Making Pretrained Video Model Pyramidal for Efficient Inference"
        },
        {
            "title": "Supplementary Material",
            "content": "This supplementary material is structured as follows. Sec. A1 provides our theoretical contribution: generalization of transition between stages to broader class of resizing operations. In Sec. A2 we report additional experimental results: we apply recent Jenga [58] method to our WanDMD checkpoint and compare its latency with our pyramidal approach. Sec. A3 contains training and evaluation details necessary for reproducing our research. We provide the extended version of Tab. 3 of the main text in Tabs. A2 and A3. We encourage the readers to review the attached videos and compare the outputs produced by different models qualitatively. A1. Transition between pyramidal stages In this section we provide generalization for nearestneighbour upsampling and average pooling used for transition between stages in PyramidalFlow. While we describe our apporach in terms of wavelets, note that the same derivations are valid for any resizing operation based on orthogonal transforms. A1.1. Downsampling Consider clean video tensor x0 at the desired output resolution. For the purpose of this section we treat it as singlechannel tensor, flattened in column vector x0 RT HW . We use forward diffusion process and denote the noisy version of this video as xσ = (1 σ) x0 + σϵ for some independently sampled ϵ (0, I) , i.e. p(xσ x0) = (cid:0)(1 σ) x0, σ2I(cid:1) . (cid:16) (cid:17)T σ,lo has dimension of For single-level orthogonal wavelet decomposition with analysis matrix we can decompose xσ into lowand RT HW , high-frequency bands Wxσ = σ,hi where ˆxT 2 . To extract the low-frequency part, we can use the projector matrix Πlo and write ˆxσ,lo = ΠloWxσ. Due to the properties of linear operators, the distribution of p(ˆxσ,lo x0) is still Gaussian, and we can compute its mean vector and covariance matrix. For covariance we have σ,lo, ˆxT ˆxT 2 2 Cov[ˆxσ,lo x0] = ΠloW σ2I (ΠloW)T = σ2ΠloWW ΠT lo = σ2ΠloΠT lo = σ2I thanks to the orthogonality of matrix W. For the mean, E[ˆxσ,lo x0] = (1 σ) ΠloWx0 = (1 σ) x0, where is low-frequency part of the matrix = (cid:0)U , (cid:1)T . In general, row sums for entries of are not equal to 1. Therefore, if x0 is constant vector with all entries being equal to each other, pixel values in x0 will differ from those in x0. To compensate that, for the given wavelet we introduce scaling constant ω R>0 such that 1 ω preserves the pixel values in this specific case. We define the downsampling operation R(xσ) as R(xσ) = 1 ω ΠloWxσ = 1 ω ˆxσ,lo. (23) Introduced scaling keeps the range of pixel values after this operation similar to natural signals. As an example, for Haar wavelet downsampling R() is just equal to average pooling. From the calculations above, p(cid:0)R(xσ) x0 (cid:1) = = (cid:18) (cid:18) (1 σ) 1 ω x0, (1 σ) R(x0), (cid:19) σ2 ω2 σ2 ω2 (cid:19) , (24) and, consequently, this distribution can be reparametrized as R(xσ) = (1 σ) R(x0) + ε (25) σ ω for ε sampled from (0, I) . For the noisy signal xσ at the original resolution, we can calculate the signal-to-noise ratio as SNR[xσ x0] = (1 σ) x02 σϵ2 = (cid:18) 1 σ σ (cid:19)2 x02 ϵ2 . For the downsampled noisy signal, from Eq. (25) SNR(cid:2)R(xσ) x0 (cid:3) = (cid:13) (cid:13)(1 σ) R(x0)(cid:13) 2 (cid:13) ω ε(cid:13) (cid:13) 2 (cid:13) σ (cid:13) (cid:19)2 (cid:13) (cid:18) 1 σ σ ω2 SNR[xσ x0]. = ω2 (cid:13)R(x0)(cid:13) 2 (cid:13) ε (26) (27) Now, if we were to start new forward diffusion process at the lower resolution, we would parametrize it as A1 (1 τ ) R(x0)+τ η for Gaussian random noise η and noise level τ . To match signal to noise ratio of this process with Eq. (25), we need to solve the equation (cid:19) (cid:18) 1 τ τ = ω2 (cid:19)2 (cid:18) 1 σ σ (28) which results in σ = 1+(ω1)τ . This coincides with the results obtained in the prior works [9, 15, 22] in context of high-resolution diffusion models in pixel space. ωτ If we denote by the number of downsampling operations applied one after another, we can rewrite the above equation as the relation between global noise levels at scales and + 1, namely, σ(i) = ωσ(i+1) 1 + (ω 1) σ(i+1) . (29) By global we mean that this noise levels are used in the forward diffusion equation. We also will refer to the global noise level at highest resolution σ(0) as the natural noise level ς. Relation between global level σ(i) and natural noise level ς is obtained with recursive application of the above equation. A1.2. Upsampling Since we used the wavelet analysis matrix to define downsampling, we can now use the inverse operation to upsample the noisy signal xσ p(xσ x0) = (cid:0)(1 σ)x0, σ2I(cid:1) . For that purpose, we treat xσ as low-frequency band and sample high-frequency bands independently from zerocentered Gaussian distribution (cid:0)0, ν2I(cid:1). The concatenated vector with all bands is then distributed as (cid:1) , . After synthesis matrix is applied, the resulting vector (cid:16)(cid:0) (1σ)x0 0 (cid:16) σ2I 0 0 ν2I (cid:17)(cid:17) p(cid:0)x σ,ν x0 (cid:1) = (cid:16) (cid:0) (1σ)x0 σ,ν is distributed as (cid:1) , (cid:16) σ2I 0 0 ν2I (cid:17) (cid:17) . Since = (cid:0)U , (cid:1)T variance as , we can rewrite the mean and coE(cid:2)x σ,ν x0 (cid:3) = (1 σ) ( x0 0 ) = (1 σ) (cid:0)U , (cid:1) ( x0 0 ) = (1 σ) x0, Cov(cid:2)x σ,ν x0 (cid:17)(cid:17) ν2I + (cid:16) (σ2ν2)I 0 (cid:3) = (cid:16) = ν2I + (cid:0)U , (cid:1) (cid:16) (σ2ν2)I 0 (cid:17) = ν2I + (cid:0)σ2 ν2(cid:1) U. 0 0 0 (30) ( ) (31) To keep the magnitude of the mean similar to natural signals, we need to scale it by ω the opposite of what was A2 required for downsampling. We would like to match the obtained distribution with that of the forward diffusion process starting with ωU x0 and parametrized as (1 τ )ωU x0+ τ ε. To achieve this, we additionally multiply the upscaled signal by non-negative constant and solve the system of equations w.r.t. r, ν, and τ , 1 τ = (1 σ) r, τ 2I = r2ω2 (cid:0)ν2I + (cid:0)σ2 ν2(cid:1) (cid:1) . (32) (33)"
        },
        {
            "title": "The last equation can be rewritten as",
            "content": "(cid:0)τ 2 r2ω2ν2(cid:1) = r2ω2 (cid:0)σ2 ν2(cid:1) U. (34) Note that is wide rectangular matrix, and therefore the rank of RHS is not greater than rank of LHS. Thus, the equality can be fulfilled only if both sides are equal to 0, which leads to τ = rων and ν = σ. Consequently, = τ = 1 1 + (ω 1) σ ωσ 1 + (ω 1) σ , . (35) (36) We define two new functions of noisy signal xσ, upsampling and upsampling-and-renoising as R(xσ) = ωx σ,0, (xσ) = rωx σ,σ = ω 1 + (ω 1) σ σ,σ. (37) (38) From the derivations above, (xσ) = (1 τ ) (x0) + τ ε. With the scale index and global noise level notation introduced above, we can reformulate Eq. (39) as σ(i) = ωσ(i+1) 1 + (ω 1) σ(i+1) . (39) Note that this coincides with Eq. (29). This leads to an important observation: both downsampling operation R(xσ(i)) and upsampling (xσ(i)) applied to the noisy sample at scale do not change the natural noise level ς associated with the global noise level σ(i). A1.3. Downsampling + upsampling Consider the forward diffusion process at scale + 1 that goes from R(x0) to the global noise level σ(i+1). Then, after being applied to this noisy sample, the result has the same marginal distribution as another diffusion process at scale with noise level σ(i) that started with R(x0). But for the clean signal x0 at scale composition of introduced operations R is equivalent to wavelet-based low-pass filtering. This means that if the noise level σ(i) is high enough to turn the high-frequency part of xσ(i) into i.i.d. noise, then with this (or larger) amount of noise the discrepancy between processes starting with x0 and R(x0) is negligible. Therefore, part of the backward diffusion process can run at lower resolution until the level σ(i+1), followed by upsampling and backward process starting from σ(i). In practice, noise levels suitable for switching between resolutions are calculated based on the average spectrum of the dataset, and therefore it is not guaranteed that for any x0 the noise level σ(i) is high enough to destroy its highfrequency content. Thus, it is more convenient to train denoising model on inputs sampled from the downsamplingupsampling process rather than the original one. A2. Additional experiments Both PyramidalFlow and PPF can be viewed as special cases of token reduction strategies during inference. Token reduction techniques are well established in the community, with several recent methods proposed, such as ToMe [4], ADAPTOR [40], RLT [11], and vid-TLDR [10]. However, these approaches typically rely on heuristics based on mutual token similarity. Consequently, their speed-up procedure is dynamic, depending on factors such as prompt complexity and initial latent noise. In contrast, pyramidal approaches can be represented as static computational graphs (one per stage), which greatly simplifies deployment, particularly in resource-constrained environments [27]. Despite this fundamental difference, we include comparison with the recent Jenga method [58]. Jenga employs dynamic sparse attention mask based on token similarity and leverages space-filling curves for GPU-friendly implementation. We apply Jenga-Base (without progressive resolution) and Jenga-Turbo (with half of the steps performed on downsampled latents) on top of our Wan-DMD checkpoint and measure performance and latency using 2 sampling steps. For Jenga-Turbo, we tried downsampling factors of 0.75 and 0.5. Note that the original Jenga implementation does not apply downsampling along the temporal dimension, even when using progressive resolution. We found that adding it to spatial downsampling indeed leads to noticeable drop of quality. Our default compiler settings (see Sec. A3) did not work for Jenga. For that reasons, we adjusted the settings to fullgraph=False and mode=max-autotune-no-cudagraphs. Results are reported in Tab. A1. For details on AttenCarve hyperparameters, please refer to the work of Zhang et al. [58]. TeaCache [36] was not used in these experiments. While Jenga obtains good VBench and VBench-2.0 scores, we found that videos produced by these models often suffer from abrupt scene changes, incoherent motion, and episodic behavior. These artifacts are more pronounced than in case of original Wan-DMD model or our pyramidal modifications. Please, refer to the supplementary videos for comparison. Overall, our experiments demonstrate that pyramidization results in better quality-efficiency trade-off. A3. Additional details In this section we provide training and evaluation details useful for reproducing our experiments. Handling spatiotemporal resolution. In the default setting, Wan model operates with 21 latent frame at stage = 0. This requires special treatment for upsampling and downsampling along the temporal axis. For our final experiments on PyramidalWan, we opted for separate handling of the first frame. Namely, during the forward diffusion process its global noise level σfirst is different from the global noise level of the rest of the frames σrest, but both of them correspond to the same natural level ς. This is due to the fact that relation between global and natural levels depends on the scaling factor ω (see Eq. (39)), which differs for 2D and 3D cases. Similarly, during the upscaling operation first frame is upsampled only spatially, while the rest of the frames spatiotemporally, both according to Eq. (38). The derivative of the noised signal w.r.t. the global level is computed separately for the first and the other frames. Since RoPE positional embeddings [20] used in Wan could potentially make it challenging for the network to handle this special nature of the first frame, we added special learnable embedding vector to all the tokens of the first frame after the patchification layer. For PPF, increased kernel size of the patchification layer also introduces inconsistency with the original shape of the latent video tensor. Therefore, for stages = 1 and = 2 we apply trilinear interpolation to upsample the original tensor to the minimal shape that allows to apply the patchifier. E.g., for stage 2 we resized from 21 60 104 to 24 64 104, thus enabling patchification with kernel size 4 8 8. After the unpatchification layer, the last operation of VideoDiT, trilinear interpolation is applied again to resize the tensor to the original shape. Note that for PPF we treat the first frame in the same way as other frames. Wan-DMD. For DMD distillation of the original model, fake score network was updated twice per each update of the student model. The learning rate of AdamW optimizer for the student was 1 105, and for fake score 5 106. Noise levels for the students inputs were selected uniformly from the set {0.0050, 0.7149, 0.9092, 1}, corresponding to uniform selection of timesteps between 1 and 1000 with the consequent application of shift [15] equal to 5. For fake score network, noise levels were selected uniformly, with further application of shift of 5. For teachers outputs, classifier-free guidance was applied with scale value of 5. We used the negative prompt [14] Bright tones, overexA Table A1. Jenga results. We measure the total latency of Video DiT calls for Jenga [58] applied to Wan-DMD checkpoint with 2 sampling steps. For comparison, our Wan-PPF-DMD model with 2-2-1 schedule achieves total transformer latency of 810 ms (standard deviation < 10 ms) at the same output video resolution. Beyond offering better qualityefficiency trade-off, pyramidal models benefit from static computational graph, which greatly simplifies deployment. Model AttenCarve ProRes of 1st step 0.05 0.1 0.15 0. 0.1 0.1 0.1 0.3 0.3 0.3 0.9 0.3 0.3 0.3 factor 1 1 1 1 0.75 0.5 0.5 Jenga-Base Jenga-Turbo temporal downsampling Latency, ms 1,211 199 1,278 205 1,297 304 1,680 201 1,089 145 932 114 865 350 Total score VBench VBench-2.0 81.25 83.52 83.54 83.13 83.38 82.98 81. 54.30 52.84 53.28 57.19 53.94 53.46 52.30 posed, static, blurred details, subtitles, style, works, paintings, images, static, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, misshapen limbs, fused fingers, still picture, messy background, three legs, many people in the background, walking backwards. Wan-Adv. The discriminator head comprises two branches: spatial branch and temporal branch. Both branches share common processing pipeline: input reshaping, initial convolution, one or more ResNet blocks, SiLU activation, and final convolution, followed by restoring the original video layout. The spatial branch operates on framelevel structure by flattening the temporal dimension into the batch, (b, t, c, h, w) (cid:55) (b t, c, h, w), and applies 2D convolutions and 2D ResNet blocks to capture intra-frame details. The temporal branch focuses on temporal dynamics by flattening spatial dimensions, (b, t, c, h, w) (cid:55) (b w, c, t, 1, 1), and uses 3D convolutions with kernel size 3 1 1 and temporal ResNet blocks to model inter-frame relationships. Weights are initialized with Xavier normal. The final convolution is zero-initialized for more stable adversarial training. For adversarial distillation, the discriminator head (with the backbone feature extractor kept frozen) is updated four times more frequently than the generator (student model). We use the AdamW optimizer with learning rate of 1 104 for the discriminator head and 1 105 for the student model. Noise levels for student model distillation are sampled uniformly from the set {0.25, 0.5, 0.75, 1}. PyramidalWan. To split the natural noise scale between stages, we conducted the spectral analysis of latents produced by the encoder of WanVAE. Our analysis follows the same procedure as that described, among others, by Dieleman [12], Starodubcev et al. [46]. Based on the results we set the cleaner natural leves as follows: ς (1) = 0.5858, ς (2) = 0.9412. For training of our pyramidal flow matching model, for A4 each stage we sampled the natural noise level uniformly, Uni(0, 1), ς (i) = ς (i) + (cid:16) ς (i) ς (i) (cid:17) , (40) (41) where natural levels ς (i) and σ(i) boundary levels σ(i) see Eq. (39). and ς (i) correspond to the global with 3D scaling factor ω, For DMD-PT pipeline, students (see Eq. (41)) was selected uniformly from the set {0.25, 0.5, 0.75, 1}, while for the fake score it was sampled according to Eq. (40). In DMD-OT training, we sampled w.r.t. Eq. (40) and applied shift 5 afterwards. For the fake score (since it is initialized with the original Wan model), natural noise level ς was sampled from Uni(0, 1), and shift 5 was applied after that. PyramidalWan has been trained with learning rate of 1 105. For PyramidalWan-DMD, optimizers had the same hyperparameters as for Wan-DMD. To mix videos of different spatiotemporal resolution in the same batch, we flattened all the tokens into single 1D sequence and applied sparse self-attention and cross-attention masks using FlexAttention [13]. For adversarial distillation of PyramidalWan (Adv-OD and Adv-PD), we follow the same hyperparameters as WanAdv. The only difference is that the noise levels for the student model are adjusted at each stage according to Eq. (41). Wan-PPF. For PPF models, we used the same hyperparameters as for the training of the full Wan models. Noise levels were split between stages in the same way as for PyramidalWan. all For and both multi-step from trained models. Sampling the used models, FlowMatchEulerDiscreteScheduler from the diffusers library for sampling [49, v0.33.0]. For Wan-DMD, noise levels for 4-step sampling were taken from the set {1, 0.9097, 0.7173, 0.0244}, and for 2-step sampling from {1, 0.8347}. For PyramidalWan-DMD few-step, we with 2-2-1 schedule, natural noise levels were selected as follows: ς (2) {1, 0.9863} , ς (1) {0.9412, 0.8645} , ς (0) {0.5858} . Same schedule was used for Wan-PPFDMD. For the multi-step flow matching models, we used classifier-free guidance scale of 5."
        },
        {
            "title": "For",
            "content": "Measurements. To estimate the computational cost of various models, we calculated FLOPs using DeepSpeed library [42, v0.14.2]. latency measurements, models were compiled separately for each res- [39, v2.7.0] on olution with the PyTorch compiler H100 GPU. The configuration of the compiler was set as follows: dynamic=False, fullgraph=True, mode=\"max-autotune\". For VBench and VBench-2.0 evaluation of all models mentioned in the paper, generated videos were saved using TorchVisions [38, v0.22.0] torchvision.io.write_video function with default parameters. We used GPT 1 set of extended prompts to evaluate VBench and Wanx2 set for VBench-2.0. 1https : / / github . com / Vchitect / VBench / blob / 7bcb8691c49426ac30544456d19a234d971722e6/prompts/ augmented _ prompts / gpt _ enhanced _ prompts / all _ dimension_longer.txt 2https : / / github . com / Vchitect / VBench / blob / 8270c9e54eb56de9a589ec351f4ff3c4e0ab3dfd / VBench - 2.0/prompts/prompt_aug/Wanx_full_text_aug.txt A5 Table A2. VBench scores. Models Subject Consistency Background Consistency Temporal Flickering Motion Smoothness Dynamic Degree Aesthetic Quality Imaging Quality Object Class Multiple Objects Human Action Wan2.1-1.3B (50 steps) Wan2.1-1.3B (25 steps) PyramidalWan (20-20-10) Wan-Adv (4 steps) Wan-Adv (2 steps) Wan-Adv (1 step) Wan-DMD (4 steps) Wan-DMD (2 steps) Wan-DMD (1 step) PyramidalWan-Adv-OD (2-2-1) PyramidalWan-Adv-PD (2-2-1) Wan-PPF-DMD (2-2-1) PyramidalWan-DMD-OT (2-2-1) PyramidalWan-DMD-PT (2-2-1) 93.07 92.06 97.02 95.62 95.97 95.04 94.57 95.81 92.95 97.63 96.39 96.18 97.56 98.06 95.21 95.43 97.38 95.44 95.31 94.71 94.34 94.92 92. 97.17 96.71 94.34 96.74 96.98 99.35 99.30 99.44 98.47 98.58 98.59 97.91 98.27 98.55 98.96 99.23 98.37 97.66 99.34 98.03 97.13 98.68 98.87 98.93 98.80 97.78 98.28 98. 98.49 98.79 98.80 98.04 99.07 69.17 69.44 44.44 71.39 64.17 39.44 85.83 64.44 46.94 45.83 56.94 49.72 43.89 31.39 65.20 62.91 66.13 63.37 63.68 63.23 66.66 66.87 59. 66.48 65.08 65.30 69.96 68.07 65.07 62.28 65.69 65.79 66.24 66.08 67.43 68.42 66.89 69.94 67.82 69.35 71.14 69.24 88.78 85.89 95.27 91.33 90.35 88.26 92.94 95.11 84. 94.21 93.94 94.95 95.97 95.44 72.07 66.40 84.47 71.91 69.91 68.61 77.76 84.45 64.65 79.80 72.84 85.40 84.24 85.18 96.40 96.99 95.60 95.00 93.80 92.60 96.20 97.40 95. 94.40 91.80 94.60 95.00 94.20 Models Color Spatial Relationship Scene Appearance Style Temporal Style Overall Consistency Quality Score Semantic Score Total Score Wan2.1-1.3B (50 steps) Wan2.1-1.3B (25 steps) PyramidalWan (20-20-10) Wan-Adv (4 steps) Wan-Adv (2 steps) Wan-Adv (1 step) Wan-DMD (4 steps) Wan-DMD (2 steps) Wan-DMD (1 step) PyramidalWan-Adv-OD (2-2-1) PyramidalWan-Adv-PD (2-2-1) Wan-PPF-DMD (2-2-1) PyramidalWan-DMD-OT (2-2-1) PyramidalWan-DMD-PT (2-2-1) 83.20 83.11 88.51 84.70 86.22 84.74 80.93 86.04 84.99 85.82 80.99 88.90 83.96 81.26 75.46 67.94 77. 75.78 74.16 75.98 71.82 78.51 68.52 80.77 70.83 79.48 82.18 82.14 54.56 49.64 55.12 51.21 51.80 50.51 51.90 51.95 47.51 51.73 54.17 51.85 50.77 52.54 22.82 23.69 21. 22.01 21.71 21.30 21.60 21.65 22.15 20.96 20.10 20.46 22.22 21.35 25.78 24.90 24.92 24.22 24.15 23.85 25.24 25.32 24.60 24.34 24.19 24.88 24.09 24.93 26.99 26.28 26. 26.19 26.08 25.86 26.59 26.82 26.07 25.66 26.05 26.17 25.90 26.35 83.47 82.09 83.36 84.06 83.74 81.38 84.71 84.00 80.63 83.94 84.20 83.04 83.63 83.46 78.57 76.02 80. 77.39 76.82 75.85 77.86 80.41 74.75 78.74 76.07 79.80 79.80 79.75 82.49 80.87 82.83 82.72 82.35 80.28 83.34 83.28 79.45 82.90 82.57 82.39 82.86 82.72 Table A3. VBench-2.0 scores. Models Camera Motion Complex Landscape Complex Plot Composition Diversity Dynamic Attribute Dynamic Spatial Relationship Human Anatomy Human Clothes Human Identity Human Interaction Instance Preservation Wan2.1-1.3B (50 steps) Wan2.1-1.3B (25 steps) PyramidalWan (20-20-10) Wan-Adv (4 steps) Wan-Adv (2 steps) Wan-Adv (1 step) Wan-DMD (4 steps) Wan-DMD (2 steps) Wan-DMD (1 step) PyramidalWan-Adv-OD (2-2-1) PyramidalWan-Adv-PD (2-2-1) Wan-PPF-DMD (2-2-1) PyramidalWan-DMD-OT (2-2-1) PyramidalWan-DMD-PT (2-2-1) 49.08 96.10 99.06 49.07 44.14 40.43 99.48 37.96 89. 23.77 28.40 31.79 24.69 22.53 72.44 47.62 18.68 14.44 17.11 17.56 51.28 20.22 54.21 16.22 15.56 16.67 15.56 13.56 31.99 10.91 12.85 9.54 9.28 9.65 10.92 13.58 10. 7.87 9.20 10.36 9.00 10.39 61.70 73.13 57.26 50.34 48.75 45.51 76.34 48.70 74.22 39.79 42.99 48.62 45.56 42.81 48.42 62.90 74.39 51.59 45.39 31.51 60.84 45.30 73. 49.82 47.02 31.46 54.52 26.80 40.80 25.12 31.88 38.46 39.56 41.39 35.27 43.96 30.92 13.92 21.61 12.45 25.64 21.98 97.99 19.11 17.78 27.54 26.57 27.05 16.67 31.40 16. 24.15 28.99 24.64 30.92 29.95 11.31 83.04 95.91 88.14 87.12 86.07 79.53 87.25 80.12 61.12 88.56 88.23 45.52 63.55 25.12 4.02 37.40 96.73 95.85 85.52 24.51 100.00 2. 100.00 98.60 100.00 100.00 100.00 85.96 79.33 82.71 71.88 65.18 67.20 88.08 54.94 69.53 84.09 75.78 86.61 86.98 80.60 63.50 48.65 44.79 67.67 60.67 59.00 48.53 75.00 47. 62.00 59.33 66.67 73.67 75.00 16.44 71.33 72.00 83.63 82.46 78.36 71.67 85.96 62.00 93.57 89.47 89.47 91.23 95.32 Models Material Mechanics Motion Order Understanding Motion Rationality Multi-View Consistency Thermotics Creativity Score Commonsense Score Controllability Score Wan2.1-1.3B (50 steps) Wan2.1-1.3B (25 steps) PyramidalWan (20-20-10) Wan-Adv (4 steps) Wan-Adv (2 steps) Wan-Adv (1 step) Wan-DMD (4 steps) Wan-DMD (2 steps) Wan-DMD (1 step) PyramidalWan-Adv-OD (2-2-1) PyramidalWan-Adv-PD (2-2-1) Wan-PPF-DMD (2-2-1) PyramidalWan-DMD-OT (2-2-1) PyramidalWan-DMD-PT (2-2-1) 9.62 41.95 32.76 64.66 62.07 63.16 36.78 68.50 38. 60.61 66.67 59.62 64.49 62.07 80.63 67.57 59.22 75.74 73.08 73.88 71.30 72.99 72.95 58.33 69.53 61.90 62.32 62.99 71.67 50.34 44.50 21.21 23.23 14.81 53.20 37.37 30. 12.12 17.85 10.77 23.91 25.93 69.44 36.70 23.91 31.03 34.48 30.46 32.32 39.66 30.98 28.16 34.48 36.21 35.63 31.03 49.05 34.26 21.60 0.00 18.90 0.00 36.42 12.58 42. 36.12 9.79 12.78 39.33 17.67 32.10 64.08 53.68 61.87 63.38 60.14 64.75 68.79 70.63 49.64 58.04 57.64 60.99 60.96 48.73 49.49 44.64 50.96 47.07 38.51 50.87 47.00 38. 44.80 45.00 40.04 50.04 34.81 63.38 62.50 64.33 57.33 58.47 54.41 58.16 62.81 59.31 60.86 61.98 62.84 63.43 63.18 33.96 35.01 28.39 32.56 31.51 29.99 36.36 37.07 35. 22.86 25.85 24.76 29.05 28.48 Human Fidelity Score Physics Score Total Score 80.71 79.44 85.38 85.58 82.72 79.60 82.80 80.73 77. 81.74 87.65 91.61 77.50 81.38 53.30 52.20 51.89 50.57 54.36 49.29 59.23 55.72 55.00 51.17 51.01 47.98 56.78 50.92 56.02 55.73 54.93 55.40 54.82 50.36 57.48 56.67 53. 52.29 54.30 53.45 55.36 51.75 A"
        }
    ],
    "affiliations": [
        "Qualcomm AI Research"
    ]
}