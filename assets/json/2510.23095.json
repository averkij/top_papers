{
    "paper_title": "Revisiting Multimodal Positional Encoding in Vision-Language Models",
    "authors": [
        "Jie Huang",
        "Xuejing Liu",
        "Sibo Song",
        "Ruibing Hou",
        "Hong Chang",
        "Junyang Lin",
        "Shuai Bai"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Multimodal position encoding is essential for vision-language models, yet there has been little systematic investigation into multimodal position encoding. We conduct a comprehensive analysis of multimodal Rotary Positional Embedding (RoPE) by examining its two core components: position design and frequency allocation. Through extensive experiments, we identify three key guidelines: positional coherence, full frequency utilization, and preservation of textual priors-ensuring unambiguous layout, rich representation, and faithful transfer from the pre-trained LLM. Based on these insights, we propose Multi-Head RoPE (MHRoPE) and MRoPE-Interleave (MRoPE-I), two simple and plug-and-play variants that require no architectural changes. Our methods consistently outperform existing approaches across diverse benchmarks, with significant improvements in both general and fine-grained multimodal understanding. Code will be avaliable at https://github.com/JJJYmmm/Multimodal-RoPEs."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 ] . [ 2 5 9 0 3 2 . 0 1 5 2 : r REVISITING MULTIMODAL POSITIONAL ENCODING IN VISIONLANGUAGE MODELS Jie Huang1,2,*, Hong Chang2 Xuejing Liu1,* Junyang Lin1 Sibo Song1 Shuai Bai1, Ruibing Hou2, , Alibaba Group. 1Qwen Team 2Institute of Computing Technology, Chinese Academy of Sciences Corresponding author *Equal contribution {yuzheng.lxj,sibo.ssb,junyang.ljy,baishuai.bs}@alibaba-inc.com {huangjie24s,houruibing,changhong}@ict.ac.cn"
        },
        {
            "title": "ABSTRACT",
            "content": "Multimodal position encoding is essential for vision-language models, yet there has been little systematic investigation into multimodal position encoding. We conduct comprehensive analysis of multimodal Rotary Positional Embedding (RoPE) by examining its two core components: position design and frequency allocation. Through extensive experiments, we identify three key guidelines: positional coherence, full frequency utilization, and preservation of textual priorsensuring unambiguous layout, rich representation, and faithful transfer from the pre-trained LLM. Based on these insights, we propose Multi-Head RoPE (MHRoPE) and MRoPE-Interleave (MRoPE-I), two simple and plug-and-play variants that require no architectural changes. Our methods consistently outperform existing approaches across diverse benchmarks, with significant improvements in both general and fine-grained multimodal understanding. Code will be avaliable at https://github.com/JJJYmmm/Multimodal-RoPEs."
        },
        {
            "title": "INTRODUCTION",
            "content": "The permutation-invariant nature of the self-attention mechanism requires the use of positional encodings to inform Large Language Models (LLMs) of sequence order, relative distance, and structural dependencies. While early methods relied on absolute position embeddings (Vaswani et al., 2017), relative encodingswhich better generalize to varying sequence lengthshave become the standard. Among these, Rotary Position Embedding (RoPE) (Su et al., 2024) has emerged as the de facto choice in modern LLMs such as Llama (Grattafiori et al., 2024) and Qwen (Yang et al., 2025). Vision-Language Models (VLMs) also require positional encodings that can handle heterogeneous modalities, including 1D text and 2D/3D visual inputs. Current methods fall into two main categories: 1D sequential and multi-dimensional designs. The former, exemplified by vanilla RoPE (Su et al., 2024) and V2PE (Ge et al., 2024), flattens and concatenates all inputs into single sequence. While simple, this approach discards the native visual geometry, leading to significant degradation in performance on tasks requiring visual grounding and spatial reasoning. Multi-dimensional designs, the second approach, extend RoPE to multiple axes (time, height, width) by partitioning embedding channels. Qwen2-VL Wang et al. (2024a) adopts Multimodal RoPE (MRoPE) to unify positional encoding for text and visual tokens. However, MRoPE allocates the position embedding into t-h-w chunk, placing the temporal infomation entirely in the high-frequency channels. This bias in temporal encoding harms long-range video modeling. Subsequent work has attempted to improve it, but this has led to fragmented landscape of highly specialized solutions. Some methods focus exclusively on image understanding (Wang et al., 2025), others on video comprehension (Wei et al., 2025; Li et al., 2025; Liu et al., 2025), and third group on image generation (Liao et al., 2025; Wu et al., 2025). While these models achieve notable performance in their This work was done during research internship at Qwen Team, Alibaba Group. 1 Table 1: Comparison of different RoPE methods. Method Position Design 3D Struct. Modal Int. Freq Allocation Compatible with Text-only RoPE Range Gran. Vanilla RoPE (Su et al., 2024) V2PE (Ge et al., 2024) RoPE-tie (Su, 2024) MRoPE (Bai et al., 2025) CircleRoPE (Wang et al., 2025) VideoRoPE (Wei et al., 2025) IL-RoPE (Liao et al., 2025) Omni-RoPE (Wu et al., 2025) MHRoPE MRoPE-I - - - - respective domains, the development of truly robust and versatile VLM requires more holistic positional encoding strategy. In this work, we aim to develop more holistic positional encoding strategy capable of supporting the core, unified capabilities of image and video understanding, complemented by fine-grained visual grounding. To build more robust multimodal positional encoding, we build on MRoPE and systematically explore three underexplored design: (i) position designhow to assign unambiguous, well-separated coordinates to text and visual tokens; and (ii) frequency allocationhow to distribute rotary frequencies across embedding dimensions for each positional axis; (iii) compatibility with text-only RoPEensuring the design defaults to vanilla RoPE for pure text inputs to enable effective transfer learning. As Table 1 shows, we systematically compare recent methods across the three design axes and conduct extensive experiments. From this analysis, we identify common pitfalls: modalities confusion arising from positional ambiguity; degraded cross-modal fusion due to suboptimal modality intervals; impaired multi-scale modeling from restricted frequency allocations; and compromised transfer learning caused by incompatibility with text-only RoPE. Based on our experiment, we distill three core guidelines for designing robust VLM positional encodings: (i) positional coherence, requiring unambiguous coordinates with well-defined modality interval; (ii) full frequency allocation, ensuring all positional axes have access to the full frequency spectrum; and (iii) preservation of textual priors, keeping the text RoPE identical to the base LLM. To satisfy the guidelines of full frequency allocation, we propose two simple yet effective methods. Multi-Head RoPE dedicates distinct attention heads to different positional axes to preserve full frequency resolution. MRoPE-Interleave employs fine-grained, round-robin distribution of channels to ensure each axis is encoded with the full frequency spectrum. Besides, we introduce spatial-reset, novel mechanism that resets the spatial position for visual content. This simple modification was found to significantly facilitate the models focus to visual information. Our methods consistently outperform strong baselines across key tasks, including image and video understanding and visual grounding. Our contributions are three-fold: (1) systematic decomposition of multimodal RoPE design; (2) two lightweight instantiationsMulti-Head RoPE and MRoPE-Interleave that satisfy the guidelines; and (3) spatial-reset, general-purpose optimization for improved visual information flow."
        },
        {
            "title": "2 ANALYSIS OF MULTIMODAL ROTARY POSITION EMBEDDING",
            "content": "This section provides systematic analysis of multimodal RoPE. We begin by revisiting the basics of vanilla RoPE. We then evaluate existing multimodal extensions along three core design axesposition design, frequency allocation and compatibility with text-only RoPE. Through this analytical lens, we identify critical limitations in current approaches, directly motivating our proposal of two simple yet effective methods: Multi-Head RoPE and MRoPE-Interleave."
        },
        {
            "title": "2.1 PRELIMINARIES: VANILLA ROPE",
            "content": "Vanilla RoPE (Su et al., 2024) is pivotal method for encoding positional information in modern LLMs. Unlike additive position embeddings, RoPE applies rotational transformation to the query and key vectors, thereby incorporating relative position dependencies directly into the self-attention mechanism. Given query vector at position and key vector at position n, the attention scores are calculated as: = (Rmq)(Rnk) = qR (1) The transformation is an orthogonal rotation, which causes the score to depend solely on the relative position m. This property is achieved by constructing Rm as block-diagonal matrix parameterized by the absolute position and set of fixed frequencies θi. The rotation frequencies, θi = base2i/d for [0, d/2 1], are set according to geometric sequence. This design creates spectrum of frequencies ranging from high (for small i) to low (for large i), corresponding to each pair of dimensions. mRnk = qRnmk (a) Vanilla RoPE / V2PE (b) MRoPE (c) VideoRoPE / HoPE (d) CircleRoPE (e) IL-RoPE / Omni-RoPE (f) MHRoPE / MRoPE-I Figure 1: Position design of different multimodal RoPE variants. The illustrated example follows an interleaved multimodal sequence: <system prompt>, <video 1>, <text>, <image 1>, <text>, <image 2>, <text>, <image 3>, <text>, <generated text>. 2.2 POSITION DESIGN This section governs how the positional identifier is assigned to text/visual tokens. 2.2.1 1D SEQUENTIAL DESIGN. The most straightforward approach, employed by vanilla RoPE (Su et al., 2024) and V2PE (Ge et al., 2024), is to treat the multimodal input as flattened, one-dimensional sequence. Position indices are assigned incrementally, with the position mi of the i-th token defined as mi = mi1 + smod, where smod is step size specific to the tokens modality. For vanilla RoPE, all modalities are treated uniformly, with = 1. As shown in Figure 1a, this design presents two significant drawbacks. First, it discards the inherent 3D structure of visual content, which can alter the spatio-temporal reasoning capabilities of VLM. Second, position indices can grow exceedingly large in long sequences, negatively affecting the models extrapolation performance (Wei et al., 2025). To address the issue of large position indices, V2PE (Ge et al., 2024) introduces dynamic position scaling for visual tokens, setting their step size svisual to value in {1, 1/2, . . . , 1/256}. This 3 modification mitigates the rapid growth of position indices and has shown benefits in long video understanding. However, the 3D structure of visual content is still ignored in 1D sequential design."
        },
        {
            "title": "2.2.2 MULTI-DIMENSIONAL DESIGN.",
            "content": "mt , mw To preserve the native 3D structure of visual content, methods like MRoPE (Wang et al., 2024a) (See Figure 1b) extend the scalar position identifier to multi-dimensional tuple. For instance, i, mh tokens position can be represented as mi = (mt ), corresponding to its temporal, vertical, and horizontal axes. MRoPE conceptually treats each visual content (e.g., an image or set of video frames) as single, large cube. The temporal position of the subsequent token is then set by jumping past the maximum coordinate value of current block. This is achieved with the update rule: prev) + 1 prev, mw next = max(mt prev, mh This strategy guarantees that no positional overlaps occur between modalities. However, proponents of VideoRoPE (Li et al., 2025) and HoPE (Li et al., 2025) argue that MRoPEs position design lacks inter-modal symmetry, they introduce diagonal layout by centering the spatial coordinates (see Figure 1c). In this scheme, visual frames are not only stacked along the temporal axis but are also shifted along the vertical and horizontal axes. Despite its theoretical elegance, this diagonal layout introduces critical flaw: the potential for position id overlap between visual content and generated text tokens. For high-resolution image content like documents, the spatial coordinates of visual tokens can extend into the index range subsequently assigned to the generated text tokens. We identify this positional ambiguity as source of modalities confusion in generation, failure mode that manifested as endless text repetition in our later experiments. (2) CircleRoPE (Wang et al., 2025) arranges image tokens in circular layout, orthogonal to the linear axis of text positions (see Figure 1d). key property of this design is that it renders all visual tokens equidistant from any given text token, which theoretically promotes uniform attention across the image. However, CircleRoPEs design has two limitations. First, the large interval between modalities may impede effective cross-modal interaction. Second, lacking temporal axis, it collapses all video frames onto single ring, which introduces severe temporal ambiguity. 2.2.3 TOWARDS AN OPTIMAL POSITION DESIGN Our preceding analysis, summarized in the first column of Table 1, indicates that robust position design must satisfy several criteria, which we collectively term Positional Coherence: (i) preserve the 3D structure of visual content; (ii) maintain slow growth rate; (iii) avoid modalities confusion in generation; (iv) establish an appropriate modality interval. While MRoPE fulfills most of these requirements, our analysis uncovers crucial phenomenon: MRoPE exhibits visual attention sink, where attention concentrates on the top-left corner of each image or video frame, behavior visualized in Figure 2. This is analogous to the attention sink at the initial tokens in large language models. This insight directly motivates our proposal of spatial-reset: mechanism that resets the spatial dimensions for each visual content. By applying spatial-reset, we aim to align this visual sink with the LLMs bias for small position IDs, accelerating visual adaptation. spatial-reset provides another Furthermore, benefit for video understanding by disentangling the representation of motion. Consider an object token at spatial coordinates (h1, w1) at time t1 and second token for the same object at (h2, w2) at time t2. Let their absolute position indices be m1 and m2, respectively. Under the standard MRoPE formulation, the temporal and spatial dimensions are coupled. The absolute positions are m1 = (t1, t1 + h1, t1 + w1) and m2 = (t2, t2+h2, t2+w2). The resulting relative position indices, mrel = m2 m1, becomes entangled: Figure 2: Visual attention sink in MRoPE. Average attention scores to input sequence from the ChartQA (Left) and VideoMME-short (Right). mrel = (t2 t1, (t2 t1) + (h2 h1), (t2 t1) + (w2 w1)) (3) 4 In contrast, our method with spatial-reset decouples these dimensions. The positions are defined as m1 = (t1, h1, w1) and m2 = (t2, h2, w2). This yields purely spatio-temporal relative vector: mrel = (t2 t1, h2 h1, w2 w1) (4) This disentangled representation of motion is more intuitive and provides cleaner inductive bias for the model to learn from. Therefore, the position design we adopt for our proposed MHRoPE and MRoPE-I methods builds upon MRoPE by incorporating spatial-reset (as illustrated in Figure 1f). Figure 3: Frequence allocation of different multimodal RoPEs. 2.3 FREQUENCY ALLOCATION Frequency Allocation governs the assignment of feature channels and their corresponding frequencies θi to the various axes of the position identifier (temporal t, vertical or horizontal w). 2.3.1 FREQUENCY ALLOCATION IN 1D ROPE. In 1D methods like vanilla RoPE and V2PE, all feature channels are allocated to encode the temporal axis. The frequencies θi decay as the channel index increases, creating spectrum from high-frequency (for short-range dependencies) to low-frequency (for long-range dependencies). This design also imparts long-range decay property on attention scores, as their upper bound is function of the relative distance, which can be approximated by (cid:80)d/21 Si+1, where Sj = (cid:80)j1 k=0 ei(mn)θk (see Appendix D.2 for detailed derivation). V2PEs position scaling for visual tokens effectively slows this decay, enhancing the models ability to focus on long visual content. i=0 2.3.2 MULTI-DIMENSIONAL FREQUENCY ALLOCATION. The standard MRoPE partitions the feature dimensions into three contiguous blocks, dedicating one to each of the t, h, and axes. As rotational frequencies decrease with channel index, this design forces the temporal axis to be encoded entirely by the highest-frequency channels. This creates strong inductive bias that is detrimental to long-sequence understanding, as it leads to rapid decay of attention over time. Furthermore, because the and axes are assigned distinct, non-overlapping frequency ranges, they exhibit different long-range decay rates, as visualized in Figure 4a. This asymmetry can impair the models ability to learn consistent spatial relationships. Subsequent methods have attempted to rectify this temporal bias through various frequency reallocation strategies. VideoRoPE and HoPE, for example, move the temporal axis to occupy the low-frequency channels. IL-RoPE employs form of interleaving but similarly reserves the lowestfrequency channels for the temporal dimension. While these approaches can mitigate the longcontext issue for the temporal axis, they introduce critical, unaddressed trade-off: they force the spatial dimensions into restricted, and often exclusively high-frequency, band. This severely limits the models ability to capture multi-scale spatial relationships, which can impair performance on tasks reliant on fine-grained spatial reasoning, such as visual grounding. Furthermore, the very act of partitioning feature dimensions inherently coarsens the frequency resolution for each positional axis. The performance implications of this reduced granularity is under-exploration. 5 (a) MRoPE (b) MHRoPE / MRoPE-I Figure 4: The long-range decay property of MRoPE, MHRoPE and MRoPE-I."
        },
        {
            "title": "2.3.3 TOWARDS AN OPTIMAL FREQUENCY ALLOCATION",
            "content": "To address the limitations of frequency allocation, we propose two effective strategies, as summarized in the second column of Table 1. Both methods resolve the rapid temporal decay and asymmetric spatial decay of MRoPE, yielding unified decay profile for all axes as shown in Figure 4b. Multi-Head Allocation. The first strategy, termed Multi-Head RoPE (MHRoPE), is inspired by recent work demonstrating channel-level redundancy in RoPE (e.g., partial RoPE (Barbero et al., 2025)). Based on the premise that similar redundancy exists at the attention head level, MHRoPE partitions the positional encoding task among different attention heads1, as shown in Figure 3. primary advantage of this strategy is that each axis is encoded using the full frequency spectrum available within its assigned heads. This approach avoids the loss of frequency resolution inherent to channel-splitting methods. Moreover, it may be more scalable. As the number of positional axes grows (Liu et al., 2025), partitioning fixed channel channels (e.g., 128) becomes untenable, whereas dedicating distinct heads to new dimensions offers far more robust and flexible approach. Interleaved Allocation. Our second strategy, employed in our MRoPE-Interleaved (MRoPE-I) method, distributes feature channels to the temporal (t), vertical (h), and horizontal (w) axes in fine-grained, round-robin manner, as shown in Figure 3. This design ensures that each positional axis is encoded using the full frequency spectrum, from high to low, thereby enabling robust multi-scale modeling for each positional axis. Moreover, the uniform frequency distribution of our interleaved design is compatible with extrapolation algorithms like NTK-aware (bloc97, 2023) and YaRN (Peng et al., 2024), which function by rescaling the frequency spectrum (see Appendix D.3). 2.4 COMPATIBILITY WITH TEXT-ONLY ROPE. Most VLMs are adapted from LLMs, which typically use vanilla RoPE for positional encoding. This raises natural question: should the encoding strategy for text tokens in VLMs remain identical to that of the base LLM? While most works implicitly agree, some methods have explored deviations. From the Position Design aspect, methods like IL-RoPE (Liao et al., 2025) and Omni-RoPE (Wu et al., 2025) modify the text encoding. As shown in Figure 1e, they reset spatial coordinates for each image to aid editing but concurrently set the spatial dimensions for text tokens to zero. This design choice breaks compatibility with the standard RoPE used in the pre-trained LLM. From the Frequency Allocation aspect, we also explored potential modification. Since the coordinate range of spatial dimensions is much smaller than that of the temporal axis, smaller rotary base could be used to better encode the spatial dimension. However, our experiments showed that this strategy led to poor performance across most benchmarks. This outcome strongly indicates the critical importance of maintaining full compatibility with the text-only RoPE for effective knowledge transfer from pre-trained LLMs. 1For Group Query Attention (GQA), we partition on KV heads and repeat on corresponding query heads."
        },
        {
            "title": "2.5 PROPOSED MULTIMODAL ROPES",
            "content": "Based on our analysis, we propose two novel multimodal RoPE variants: Multi-Head RoPE (MHRoPE) and MRoPE-Interleave (MRoPE-I). Both methods are built upon shared set of design guidelines for robustness and performance. For position design, we enhance MRoPE position design with spatial-reset to improve the models focus on visual information. We also maintain strict compatibility with text-only RoPE, ensuring the effective transfer of pre-trained knowledge. The key distinction between our variants lies in their frequency allocation strategy. MHRoPE employs Multi-Head Allocation, dedicating distinct attention heads to different axes to preserve frequency resolution and offer scalability. In contrast, MRoPE-I uses Interleaved Allocation, finegrained approach ensuring full-spectrum encoding and compatibility with extrapolation techniques. For detailed discussion on the trade-offs between MHRoPE and MRoPE-I, see Appendix D.1."
        },
        {
            "title": "3 EXPERIMENT",
            "content": "3.1 EXPERIMENTAL SETUP Training Details. All models use the QwenViT and connector from Qwen2.5VL2, while initializing the VLM backbone with the Qwen2.5 7B LLM. During training, we freeze the ViT to fix the visual representations and unfreeze the connector and LLM backbone. This strategy is designed to isolate the effects of our proposed RoPE modifications while adhering to the standard VLM adaptation paradigm of building upon pre-trained LLM. Training process adopts batch size of 128, cosine-decayed learning rate of 1 105, and allocate approximately 512 Nvidia A100 GPU hours per experiment. The training context length is set to 32K, and the rotary base is set to 1000000. All experiments share identical training data, model architecture, and hyperparameters, with the sole difference being the choice of multimodal RoPE. Training Data and Evaluation Benchmarks. We conduct experiments using approximately 2M high-quality supervised fine-tuning (SFT) samples, covering wide range of visual scenarios including image captioning, OCR, visual reasoning, visual grounding, document comprehension, and long video understanding. For evaluation, we adopt more than 20 benchmarks spanning images, videos, and grounding tasks. Specifically, the image benchmarks include MMMU (Yue et al., 2024), MMBench (Liu et al., 2024a), MMStar (Chen et al., 2024), OCRBench (Liu et al., 2024b), AI2D (Kembhavi et al., 2016), RealWorldQA (X.AI., 2024), DocVQA (Mathew et al., 2021), TextVQA Singh et al. (2019), InfoVQA (Mathew et al., 2022), and ChartQA (Masry et al., 2022). The video benchmarks consist of MVBench (Li et al., 2024), STAR (Wu et al., 2021), VideoMME (Fu et al., 2025), LVBench (Wang et al., 2024b), MLVU (Zhou et al., 2024b), and Charades-STA (Zhou et al., 2024a). For grounding, we evaluate on RefCOCO (Kazemzadeh et al., 2014) series. 3.2 OVERALL PERFORMANCE The overall performance of different multimodal RoPEs is presented in Table 2. Both MHRoPE and MRoPE-I achieve consistently better performance across the majority of benchmarks. For instance, MRoPE-I outperforms the vanilla RoPE baseline by significant margin of +2.67% on MMMU, +5.28% on ChartQA, and +3.27% on RefCOCOval. The results also reveal that while vanilla RoPE serves as competitive baseline, its performance is noticeably impaired on benchmarks that demand fine-grained spatial reasoning, such as ChartQA and the RefCOCO series. This performance gap highlights the fundamental limitations of its flattened, 1D position design. Vanilla RoPE also suffers from extrapolation, see Appendix D.4. While VideoRoPE and HoPE demonstrate stronger performance on video benchmarks, they exhibit anomalous degradation on DocVQA, InfoVQA, and ChartQA. We attribute this discrepancy to critical flaw in their position design: the overlap of position indices, which induces confusion be2For fair comparison with prior work which using Qwen2VL, we disable the absolute time encoding used in Qwen2.5VL. 7 Table 2: Overall performance of multimodal RoPEs variants on various benchmarks. The values in parentheses on the far right indicate the performance gain of our MRoPE-I relative to vanilla RoPE. Types Benchmarks Vanilla RoPE MRoPE VideoRoPE HoPE CircleRoPE MHRoPE MRoPE-I MMMU MMBenchavg MMstar OCRBench AI2D RealworldQA DocVQA TextVQA InfoVQA ChartQA MVBench STAR MLVU VideoMME LVBench Charades-STA RefCOCOval RefCOCOtestA RefCOCOtestB RefCOCO+val RefCOCO+testA RefCOCO+testB RefCOCOgval RefCOCOgtest Image Video Grounding Image Video Grounding Overall 50.56 74.75 49.13 73.40 76.20 58.30 82.94 66.80 58.85 56.84 57.05 58.07 64.69 58.63 38.93 32. 77.67 81.37 72.66 69.16 74.48 61.67 75.45 75.40 65.69 51.64 73.48 50.22 74.06 49.93 72.70 74.94 57.25 81.49 65.85 52.96 63.56 57.85 58.28 63.26 58.22 39.22 32.23 78.35 82.52 72.31 68.80 75.95 59.97 75.86 75.73 65.18 51.51 73. 49.89 75.95 49.60 66.20 74.29 56.21 60.13 66.58 37.42 54.88 56.78 57.20 66.05 58.70 40.15 34.21 77.95 80.43 72.62 68.15 73.14 60.50 74.06 73.90 60.64 52.18 72.59 49.89 75.35 50.33 66.60 76.10 57.12 60.12 66.77 34.80 55.44 58.00 58.30 64.81 59.52 40.99 36. 77.72 81.60 71.44 69.61 74.55 61.69 74.69 75.45 60.72 52.95 73.19 47.22 74.91 47.00 70.60 74.45 56.60 77.70 65.54 53.11 53.72 57.10 57.94 62.69 57.70 38.80 32.27 79.59 83.98 74.35 70.19 76.77 62.59 76.10 76.12 62.86 51.09 74. 53.00 75.04 49.60 73.40 75.45 60.52 81.32 66.49 52.01 62.44 58.93 59.48 65.69 57.48 40.32 33.56 79.87 83.66 73.20 70.55 76.88 61.96 76.55 76.68 66.40 52.98 74.92 53.22 (+2.67) 75.56 (+0.81) 51.13 (+2.00) 74.00 (+0.50) 75.36 (-0.84) 57.39 (-0.92) 83.72 (+0.78) 66.91 (+0.11) 58.24 (-0.62) 62.12 (+5.28) 57.05 (+0.00) 57.79 (-0.28) 65.46 (+0.77) 58.96 (+0.33) 40.54 (+1.61) 34.36 (+1.87) 80.94 (+3.27) 84.55 (+3.18) 75.05 (+2.39) 71.80 (+2.64) 77.44 (+2.96) 61.96 (+0.29) 77.70 (+2.25) 77.34 (+1.94) 66.65 (+0.96) 52.36(+0.72) 75.85 (+2.36) tween visual and generated text tokens. The ablation study in Table 3 confirms that this confusion is the root cause of the degradation. The suboptimal designs of MRoPE and CircleRoPE manifest in their performance. MRoPE breaks the full frequency spectrum for each positon axis. Consequently, it struggles on tasks demanding specific frequency ranges, such as long-video understanding (MLVU, LVBench), which requires robust low-frequency temporal encoding, and visual grounding (RefCOCO), which benefits from high-frequency spatial encoding. Similarly, CircleRoPE introduces large modality interval and collapses the video positions, results in poor video understanding. In contrast, MHRoPE and MRoPE-I leverage spatial-reset position design, which prevents modalities confusion and not introducing an improper modality interval. By providing each positional axis with full frequency spectrum (interleave or multi-head allocation), they enable the models to better capture both fine-grained spatial details (high-frequency) and long-range temporal dependencies (low-frequency), leading to their superior overall performance. 3.3 ABLATION STUDY This section presents ablation studies on key design choices for our robust multimodal RoPEs. Additional results are provided in Appendix D.5. 3.3.1 ABLATION STUDY ON POSITION DESIGN We previously argued that an optimal position design should: (1) incorporate 3D structure to capture native spatio-temporal information, (2) maintain proper modality interval, and (3) preserve compatibility with text-only RoPE. To systematically dissect the impact of these factors, we conduct an ablation study, fixing the frequency allocation strategy to our interleaved allocation while varying the position design. The results are presented in Table 3. Simply introducing 3D structure over the vanilla RoPE provides notable boost to grounding performance. The addition of spatial-reset mechanism yields substantial gains across all benchmark categories, confirming its effectiveness. We also ablated other position designs proposed in prior work, as shown in Table 3. 8 Diagonal Layout: Implementing the diagonal layout from VideoRoPE leads to severe degradation in performance on document-centric benchmarks (DocVQA, InfoVQA and ChartQA). qualitative analysis reveals specific failure mode: repetitive, nonsensical text generation (e.g., 1111...), which occurs even when the layout is applied only at inference time. We attribute this behavior to modalities confusion induced by positional overlap, causing the model to misinterpret its own generated text tokens as visual tokens, resulting in this unpredictable repetitive output. Enlarged Modality Interval: We also tested artificially enlarging the modality interval to match that of vanilla RoPE, strategy similar to RoPE-Tie (Su, 2024). This also resulted in poor documentrelated performance. However, the failure mode was distinct: the model generated fluent but contextually irrelevant text, effectively ignoring the visual input. This suggests that while clear modality interval is necessary, simply maximizing its size to align with vanilla RoPE can be detrimental by Text spatial-reset. We also tested the strategy from IL-RoPE and Omni-RoPE, which resets spatial dimensions for visual tokens as well as text ones (Fig. 1e). This approach resulted in notable performance degradation compared to the vanilla RoPE, emphasizing that preserving RoPE alignment for text is critical for successfully adapting LLMs into VLMs. Scaling rotary base. Motivated by the smaller coordinate range of the spatial axes, we experimented with scaling their corresponding rotary base (e.g., from 1,000,000 to 10,000). This consistently resulted in clear performance drop on image benchmarks. This finding demonstrates that even well-intentioned deviations from the base LLMs RoPE formulation can break compatibility and severely impair knowledge transfer. Position Design vanilla RoPE + 3D structure + 3D + spatial-reset + diagonal layout + modality interval + text spatial-reset + scaling rotary base Image Grounding Video DocVQA InfoVQA ChartQA 65.69 65.87 66.65 61.20 62.80 58.27 60.15 58.85 57.24 58.24 37.42 42.18 52.15 52.16 73.48 74.40 75.85 72.33 73.19 68.2 74. 51.64 51.29 52.36 52.51 50.88 50.71 52.11 56.84 61.44 62.12 54.88 51.28 44.33 58.80 82.94 82.33 83.72 60.13 70.43 77.30 80.44 Table 3: Ablation study of different position design strategies. 3.3.2 ABLATION STUDY ON FREQUENCY ALLOCATION To determine the optimal frequency allocation strategy, we fix the position design the same as MRoPE with spatial-reset enhancement, and vary only the frequency allocation scheme. As shown in Table 4, more uniform allocation strategy consistently outperforms alternatives that split the spectrum into partial chunks. This highlights the importance of ensuring that each positional axes (time, height, width) retains access to the full frequency spectrum. Allocation Type VideoRoPE-like IL-RoPE-like Multi-Head Interleave Image Video Grounding Overall 63.31 65.33 63.07 65.26 64.63 66.40 64.95 66.65 72.50 72.80 74.92 75.85 52.11 51.15 52.58 52.36 Table 4: Ablation results of different frequency allocation strategies."
        },
        {
            "title": "4 CONCLUSION",
            "content": "In this work, we conducted the first systematic investigation into multimodal Rotary Positional Embedding (RoPE) for Vision-Language Models (VLMs). From our systematic comparison and extensive experiments, we identified three key design considerations for robust multimodal RoPE: positional coherence, full frequency utilization, and preservation textual priors from pre-trained LLMs. Guided by these insights, we proposed two plug-and-play RoPE variants: Multi-Head RoPE (MHRoPE) and MRoPE-Interleave (MRoPE-I). Both methods adhere to our identified guidelines, effectively addressing common failure modes and achieving significant performance in both general and fine-grained multimodal understanding. This work offers comprehensive guide for designing effective multimodal positional encodings, paving the way for future advancements in VLMs."
        },
        {
            "title": "REFERENCES",
            "content": "Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Ming-Hsuan Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report. CoRR, abs/2502.13923, 2025. URL https://doi.org/10.48550/arXiv.2502. 13923. Federico Barbero, Alex Vitvitskyi, Christos Perivolaropoulos, Razvan Pascanu, and Petar Velickovic. Round and round we go! what makes rotary positional encodings useful? In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net, 2025. URL https://openreview.net/forum?id=GtvuNrk58a. bloc97. Ntk-aware scaled rope allows llama models text size without any fine-tuning and minimal perplexity degradation., 2023. https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_ scaled_rope_allows_llama_models_to_have/. to have extended (8k+) conURL Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, and Feng Zhao. Are we on the right way for evaluating large visionlanguage models? In Amir Globersons, Lester Mackey, Danielle Belgrave, Angela Fan, Ulrich Paquet, Jakub M. Tomczak, and Cheng Zhang (eds.), Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024, 2024. Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, Peixian Chen, Yanwei Li, Shaohui Lin, Sirui Zhao, Ke Li, Tong Xu, Xiawu Zheng, Enhong Chen, Caifeng Shan, Ran He, and Xing Sun. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2025, Nashville, TN, USA, June 11-15, 2025, pp. 2410824118. Computer Vision Foundation / IEEE, 2025. Junqi Ge, Ziyi Chen, Jintao Lin, Jinguo Zhu, Xihui Liu, Jifeng Dai, and Xizhou Zhu. V2PE: improving multimodal long-context capability of vision-language models with variable visual position encoding. CoRR, abs/2412.09616, 2024. URL https://doi.org/10.48550/ arXiv.2412.09616. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, et al. The llama 3 herd of models, 2024. URL https://arxiv.org/abs/2407.21783. Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and Tamara Berg. ReferItGame: Referring to objects in photographs of natural scenes. In Alessandro Moschitti, Bo Pang, and Walter Daelemans (eds.), Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), October 2014. Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Min Joon Seo, Hannaneh Hajishirzi, and Ali Farhadi. diagram is worth dozen images. In Bastian Leibe, Jiri Matas, Nicu Sebe, and Max Welling (eds.), Computer Vision - ECCV 2016 - 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part IV, volume 9908 of Lecture Notes in Computer Science, pp. 235251. Springer, 2016. Haoran Li, Yingjie Qin, Baoyuan Ou, Lai Xu, and Ruiwen Xu. Hope: Hybrid of position embedding for length generalization in vision-language models. CoRR, abs/2505.20444, 2025. URL https: //doi.org/10.48550/arXiv.2505.20444. Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Lou, Limin Wang, and Yu Qiao. Mvbench: comprehensive multi-modal video understanding benchmark. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2024, Seattle, WA, USA, June 16-22, 2024, pp. 2219522206. IEEE, 2024. 10 Chao Liao, Liyang Liu, Xun Wang, Zhengxiong Luo, Xinyu Zhang, Wenliang Zhao, Jie Wu, Liang Li, Zhi Tian, and Weilin Huang. Mogao: An omni foundation model for interleaved multi-modal generation. CoRR, abs/2505.05472, 2025. URL https://doi.org/10.48550/arXiv. 2505.05472. Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, Kai Chen, and Dahua Lin. Mmbench: Is your multi-modal model an all-around player? In Ales Leonardis, Elisa Ricci, Stefan Roth, Olga Russakovsky, Torsten Sattler, and Gul Varol (eds.), Computer Vision - ECCV 2024 - 18th European Conference, Milan, Italy, September 29-October 4, 2024, Proceedings, Part VI, volume 15064 of Lecture Notes in Computer Science, pp. 216233. Springer, 2024a. Yuliang Liu, Zhang Li, Mingxin Huang, Biao Yang, Wenwen Yu, Chunyuan Li, Xu-Cheng Yin, Cheng-Lin Liu, Lianwen Jin, and Xiang Bai. Ocrbench: on the hidden mystery of OCR in large multimodal models. Sci. China Inf. Sci., 67(12), 2024b. Zikang Liu, Longteng Guo, Yepeng Tang, Junxian Cai, Kai Ma, Xi Chen, and Jing Liu. Vrope: Rotary position embedding for video large language models. CoRR, abs/2502.11664, 2025. URL https://doi.org/10.48550/arXiv.2502.11664. Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq R. Joty, and Enamul Hoque. Chartqa: benchmark for question answering about charts with visual and logical reasoning. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio (eds.), Findings of the Association for Computational Linguistics: ACL 2022, Dublin, Ireland, May 22-27, 2022, pp. 22632279. Association for Computational Linguistics, 2022. Minesh Mathew, Dimosthenis Karatzas, and C. V. Jawahar. Docvqa: dataset for VQA on document images. In IEEE Winter Conference on Applications of Computer Vision, WACV 2021, Waikoloa, HI, USA, January 3-8, 2021, pp. 21992208. IEEE, 2021. Minesh Mathew, Viraj Bagal, Rub`en Tito, Dimosthenis Karatzas, Ernest Valveny, and C. V. Jawahar. Infographicvqa. In IEEE/CVF Winter Conference on Applications of Computer Vision, WACV 2022, Waikoloa, HI, USA, January 3-8, 2022, pp. 25822591. IEEE, 2022. Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarn: Efficient context window extension of large language models. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. Towards VQA models that can read. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019, pp. 8317 8326. Computer Vision Foundation / IEEE, 2019. Jianlin Su. Transformer upgrade path: 17. insights into multimodal positional encoding, March 2024. URL https://spaces.ac.cn/archives/10040. Jianlin Su, Murtadha H. M. Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. URL https://doi.org/10.1016/j.neucom.2023.127063. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pp. 59986008, 2017. Chengcheng Wang, Jianyuan Guo, Hongguang Li, Yuchuan Tian, Ying Nie, Chang Xu, and Kai Han. Circle-rope: Cone-like decoupled rotary positional embedding for large vision-language models. CoRR, abs/2505.16416, 2025. URL https://doi.org/10.48550/arXiv.2505. 16416. 11 Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. CoRR, abs/2409.12191, 2024a. URL https://doi. org/10.48550/arXiv.2409.12191. Weihan Wang, Zehai He, Wenyi Hong, Yean Cheng, Xiaohan Zhang, Ji Qi, Shiyu Huang, Bin Xu, Yuxiao Dong, Ming Ding, and Jie Tang. Lvbench: An extreme long video understanding benchmark. CoRR, abs/2406.08035, 2024b. Xilin Wei, Xiaoran Liu, Yuhang Zang, Xiaoyi Dong, Pan Zhang, Yuhang Cao, Jian Tong, Haodong Duan, Qipeng Guo, Jiaqi Wang, et al. Videorope: What makes for good video rotary position embedding? In International Conference on Machine Learning, 2025. Bo Wu, Shoubin Yu, Zhenfang Chen, Josh Tenenbaum, and Chuang Gan. STAR: benchmark for situated reasoning in real-world videos. In Joaquin Vanschoren and Sai-Kit Yeung (eds.), Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual, 2021. Chenyuan Wu, Pengfei Zheng, Ruiran Yan, Shitao Xiao, Xin Luo, Yueze Wang, Wanli Li, Xiyan Jiang, Yexin Liu, Junjie Zhou, Ze Liu, Ziyi Xia, Chaofan Li, Haoge Deng, Jiahao Wang, Kun Luo, Bo Zhang, Defu Lian, Xinlong Wang, Zhongyuan Wang, Tiejun Huang, and Zheng Liu. Omnigen2: Exploration to advanced multimodal generation. CoRR, abs/2506.18871, 2025. URL https://doi.org/10.48550/arXiv.2506.18871. X.AI. Grok-1.5 vision preview., 2024. URL https://x.ai/blog/grok-1.5v. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, arXiv preprint Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv:2505.09388, 2025. Xiang Yue, Yuansheng Ni, Tianyu Zheng, Kai Zhang, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. MMMU: massive multi-discipline multimodal understanding and reasoning benchmark for expert AGI. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2024, Seattle, WA, USA, June 16-22, 2024, pp. 95569567. IEEE, 2024. Junjie Zhou, Yan Shu, Bo Zhao, Boya Wu, Shitao Xiao, Xi Yang, Yongping Xiong, Bo Zhang, Tiejun Huang, and Zheng Liu. MLVU: comprehensive benchmark for multi-task long video understanding. CoRR, abs/2406.04264, 2024a. Junjie Zhou, Yan Shu, Bo Zhao, Boya Wu, Shitao Xiao, Xi Yang, Yongping Xiong, Bo Zhang, Tiejun Huang, and Zheng Liu. MLVU: comprehensive benchmark for multi-task long video understanding. CoRR, abs/2406.04264, 2024b."
        },
        {
            "title": "A ETHICS STATEMENT",
            "content": "This work adheres to the ICLR Code of Ethics. In this study, no human subjects or animal experimentation was involved. All datasets used were sourced in compliance with relevant usage guidelines, ensuring no violation of privacy. We have taken care to avoid any biases or discriminatory outcomes in our research process. No personally identifiable information was used, and no experiments were conducted that could raise privacy or security concerns. We are committed to maintaining transparency and integrity throughout the research process."
        },
        {
            "title": "B REPRODUCIBILITY STATEMENT",
            "content": "We have made every effort to ensure that the results presented in this paper are reproducible. Code will be made publicly available to facilitate replication and verification after inspection. The experimental setup, including training steps, model configurations, and hardware details, is described in 12 detail in the paper. We believe these measures will enable other researchers to reproduce our work and further advance the field."
        },
        {
            "title": "C LLM USAGE",
            "content": "Large Language Models (LLMs) were used to aid in the writing and polishing of the manuscript. Specifically, we used an LLM to assist in refining the language, improving readability, and ensuring clarity in various sections of the paper. The model helped with tasks such as sentence rephrasing, grammar checking. It is important to note that the LLM was not involved in the ideation, research methodology, or experimental design. All research concepts, ideas, and analyses were developed and conducted by the authors. The contributions of the LLM were solely focused on improving the linguistic quality of the paper, with no involvement in the scientific content or data analysis. The authors take full responsibility for the content of the manuscript, including any text generated or polished by the LLM. We have ensured that the LLM-generated text adheres to ethical guidelines and does not contribute to plagiarism or scientific misconduct."
        },
        {
            "title": "D APPENDIX",
            "content": "D.1 PRACTICAL CONSIDERATIONS: MHROPE VS. MROPE-I While both of our proposed methods are effective, we currently recommend MRoPE-I over MHRoPE for two primary reasons: its consistent (albeit slight) performance advantage and its greater implementation simplicity. We attribute MHRoPEs minor performance deficit to its headlevel information partitioning, which prevents the integration of different positional axes within the self-attention mechanism. From an engineering perspective, MRoPE-I is also simpler, avoiding the complexities that MHRoPE introduces with distributed training paradigms like tensor parallelism. Nevertheless, MHRoPEs design offers potentially more scalable architecture for future models that may need to accommodate larger number of positional axes. D.2 DERIVATION OF THE ATTENTION SCORE UPPER BOUND IN MROPE Here, we provide formal derivation for the upper bound of the RoPE attention score. The RoPE dot product between query and key at relative position can be expressed in complex form as: (Rmq)(Rnk) = Re d/21 (cid:88) (q[2i:2i+1] [2i:2i+1])ei(mn)θi (5) i=0 where denotes the complex conjugate of 2D vector treated as complex number, and is the complex product. To derive the upper bound, we analyze the magnitude of the summation term. To apply summation by parts, let us define content-dependent sequence hi = q[2i:2i+1] [2i:2i+1] and positiondependent sequence of partial sums Sj = (cid:80)j1 k=0 ei(mn)θk . We also set the boundary conditions S0 = 0 and hd/2 = 0. The standard summation by parts formula is (cid:80)b (cid:80)b i=a vi+1ui. Applying this, the magnitude of the summation can be rewritten and bounded as i=a uivi = [uivi]b+1 follows: (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) d/21 (cid:88) i=0 hiei(mn)θi (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) = = = (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) d/21 (cid:88) [hiSi]d/2 0 d/21 (cid:88) i=0 (cid:12) (cid:12) (cid:12) Si+1(hi+1 hi) (cid:12) (cid:12) (cid:12) d/21 (cid:88) Si+1(hi+1 hi) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (6) (hd/2Sd/2 h0S0) i=0 (cid:12) (cid:12) (cid:12) Si+1(hi+1 hi) (cid:12) (cid:12) (cid:12) d/21 (cid:88) i=0 Si+1hi+1 hi i=0 (cid:18) max 0i<d/ (cid:19) d/21 (cid:88) hi+1 hi Si+1 i=0 This final expression reveals that the upper bound is product of two distinct components. max hi+1 hi is content-dependent term that acts as scaling factor based on the specific query and key vectors. The second, (cid:80) Si+1, is purely position-dependent term whose value is determined only by the relative position and the fixed frequencies θi. Since the content-dependent term is independent of position, the long-range decay property of the attention score is governed i=1 Si, serves as primarily by this position-dependent term. Therefore, its average value, practical indicator to characterize how the upper bound attenuates with relative distance. (cid:80)d/ 1 d/2 D.3 COMPATIBILITY WITH YARN EXTRAPOLATION As shown in Figure 4b, the interleaved frequency allocation of MRoPE-I makes it compatible with extrapolation algorithms like NTK-aware (bloc97, 2023) and YaRN (Peng et al., 2024). Whereas standard MRoPEs partitioned spectrum complicates the application of consistent frequency scaling boundary, our interleaved design provides full spectrum across all positional axes, enabling straightforward and symmetric application of these methods. Furthermore, MRoPE-I requires smaller YaRN scaling factor than vanilla RoPE. This is direct result of its more efficient position design, which limits the growth of position IDs (advancing by O(max(h, w)) vs. O(h w) for an image), thus requiring less aggressive rescaling for context extension.Empirically, for inputs of around 512 512 resolution, we find that scaling factor for MRoPE-I that is approximately 3/4 of that used for vanilla RoPE is sufficient. D.4 LONG-CONTEXT VIDEO UNDERSTANDING We further compare the performance of different methods on long video understanding, with context lengths ranging from 32K to 256K. As shown in Figure 5, apart from LVBench, we do not observe clear performance improvements or degradation when extrapolating to longer sequences. The only exception is Vanilla RoPE, which suffers from sharp performance drop at 128K/256K. We attribute this to excessively fast-growing position IDs, which lead to degraded extrapolation capability, which also discussed in other works (Wei et al., 2025; Li et al., 2025). Overall, methods such as VideoRoPE and HoPE, which allocate most low-frequency channels to the temporal axis, exhibit slightly better extrapolation ability in long video senario. However, when considering performance across images and grounding tasks, MHRoPE and MRoPE-I remain the most comprehensive and balanced designs. 14 Figure 5: Video extrapolation performance. Models are trained with context length of 32k (256 frames) and extrapolated to 64k (512 frames), 128k (1024 frames), and 256k (2048 frames). D.5 MORE ABLATION RESULTS. D.5.1 ENHANCED VISUAL ATTENTION IN spatial-reset To understand the mechanism driving the effectiveness of spatial-reset, we analyzed its impact on the models attention patterns. As detailed in Table 5, we calculated the total attention scores on visual tokens using the DocVQA test set. Specifically, we extracted attention scores from layers 4, 12, 20, and 28, and averaged the scores across all attention heads and samples. The result demonstrates that MRoPE equipped with spatial-reset allocate more attention on visual content, particularly in deeper layers, confirming its effectiveness in enhancing the models visual focus. Method MHRoPE w/o spatial-reset MRoPE-I w/o spatial-reset Layer 4 Layer 12 Layer 20 Layer 28 40.31 35.99 37.48 31.22 21.76 19.68 15.68 17.66 32.05 22.02 28.08 16.02 19.00 9.93 23.23 11.69 Table 5: Average attention scores (%) on visual contents. The inputs are from DocVQA test set. And the scores are averaged between attention heads and samples. D.5.2 ALLOCATION RATIO OF FREQUENCY We further investigate different allocation ratios under the interleave frequency strategy. The results are summarized in Table 6. The balanced allocation (t:h:w = 24:20:20) achieves the best overall performance. Increasing the proportion of channels assigned to the temporal axis reduces the available high-frequency capacity for spatial dimensions. This leads to degradation in grounding ability and negatively impacts benchmarks involving spatial understanding in both images and videos. Allocation Ratio 24:20:20 32:16:16 48: 8: 8 Image Video Grounding Overall 64.95 66.65 63.29 64.07 63.03 65.06 52.36 51.15 51.17 75.85 74.65 72. Table 6: Ablation results of different frequency allocation ratios under interleave design. D.5.3 TEMPORAL STRIDE IN VIDEO MODELING This section investigates the impact of different temporal strides between video frames. Specifically, we experiment with δ = 0.5, 1, 2, as well as dynamic strides as used in V2PE and HoPE (with δ = 1 applied during inference). The results are shown in Table 7. From the results, δ = 1 achieves the best overall performance, while smaller (δ = 0.5) or larger (δ = 2) strides lead to performance drops. Incorporating the dynamic stride from V2PE does not shows significant benefit. 15 Stride MVBench 0.5 1 2 Dynamic 56.55 57.05 55.70 56.28 STAR VideoMME LVBench MLVU Charades Overall 51.11 57.90 52.36 57.79 58.13 51.10 51.80 57.93 62.37 65.46 63.11 63.75 58.96 58.96 58.15 58.74 31.88 34.36 33.51 32. 38.99 40.54 38.02 41.12 Table 7: Comparison of temporal stride settings for video benchmarks on MRoPE-I."
        }
    ],
    "affiliations": [
        "Alibaba Group",
        "Institute of Computing Technology, Chinese Academy of Sciences"
    ]
}