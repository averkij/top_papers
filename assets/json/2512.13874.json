{
    "paper_title": "SAGE: Training Smart Any-Horizon Agents for Long Video Reasoning with Reinforcement Learning",
    "authors": [
        "Jitesh Jain",
        "Jialuo Li",
        "Zixian Ma",
        "Jieyu Zhang",
        "Chris Dongjoo Kim",
        "Sangho Lee",
        "Rohun Tripathi",
        "Tanmay Gupta",
        "Christopher Clark",
        "Humphrey Shi"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "As humans, we are natural any-horizon reasoners, i.e., we can decide whether to iteratively skim long videos or watch short ones in full when necessary for a given task. With this in mind, one would expect video reasoning models to reason flexibly across different durations. However, SOTA models are still trained to predict answers in a single turn while processing a large number of frames, akin to watching an entire long video, requiring significant resources. This raises the question: Is it possible to develop performant any-horizon video reasoning systems? Inspired by human behavior, we first propose SAGE, an agent system that performs multi-turn reasoning on long videos while handling simpler problems in a single turn. Secondly, we introduce an easy synthetic data generation pipeline using Gemini-2.5-Flash to train the orchestrator, SAGE-MM, which lies at the core of SAGE. We further propose an effective RL post-training recipe essential for instilling any-horizon reasoning ability in SAGE-MM. Thirdly, we curate SAGE-Bench with an average duration of greater than 700 seconds for evaluating video reasoning ability in real-world entertainment use cases. Lastly, we empirically validate the effectiveness of our system, data, and RL recipe, observing notable improvements of up to 6.1% on open-ended video reasoning tasks, as well as an impressive 8.2% improvement on videos longer than 10 minutes."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 1 ] . [ 1 4 7 8 3 1 . 2 1 5 2 : r SAGE: Training Smart Any-Horizon Agents for Long Video Reasoning with Reinforcement Learning Jitesh Jain1,2* Jialuo Li1 Rohun Tripathi2 Zixian Ma2,3 Tanmay Gupta2 Christopher Clark2 Humphrey Shi1 Jieyu Zhang2,3 Chris Dongjoo Kim2 Sangho Lee2 1SHI Labs @ Georgia Tech 2Allen AI https://github.com/allenai/SAGE 3University of Washington"
        },
        {
            "title": "Abstract",
            "content": "As humans, we are natural any-horizon reasoners, i.e., we can decide whether to iteratively skim long videos or watch short ones in full when necessary for given task. With this in mind, one would expect video reasoning models to reason flexibly across different durations. However, SOTA models are still trained to predict answers in single turn while processing large number of frames, akin to watching an entire long video, requiring significant resources. This raises the question: Is it possible to develop performant any-horizon video reasoning systems? Inspired by human behavior, we first propose SAGE, an agent system that performs multi-turn reasoning on long videos while handling simpler problems in single turn. Secondly, we introduce an easy synthetic data generation pipeline using Gemini-2.5-Flash to train the orchestrator, SAGE-MM, which lies at the core of SAGE. We further propose an effective RL post-training recipe essential for instilling anyhorizon reasoning ability in SAGE-MM. Thirdly, we curate SAGE-Bench with an average duration of greater than 700 seconds for evaluating video reasoning ability in real-world entertainment use cases. Lastly, we empirically validate the effectiveness of our system, data, and RL recipe, observing notable improvements of up to 6.1% on open-ended video reasoning tasks, as well as an impressive 8.2% improvement on videos longer than 10 minutes. 1. Introduction Figure 1. Human behavior-inspired design of SAGE. We design SAGE to resemble humans adaptive reasoning behavior, capable of following knowledge-driven multi-turn reasoning process using tool calls for long-horizon tasks (Tab. 1) while being able to predict an answer for short-horizon problems directly. In the last year, there has been natural shift from developing models for solely image reasoning [7, 8, 14, 23, 24, 28, 29, 44, 45, 63] to also tackling video reasoning [2, 5, 40, 41, 48, 64] in the research community. Among the various model releases, the recent Gemini-2.5 [40] and Qwen3-VL [41] models pushed the frontier in video rea- *Work done during JJs internship at Allen AI. Equal advising. soning due to their ability to perform well on both short and long videos. Although the aforementioned SOTA models differ in their training data, recipe, and architecture, among other things, they all function in standard way when reasoning over videos: given set of sampled frames, output the final answer with single sequence prediction process, i.e., sin1 gle turn reasoning. We refer to this line of work as falling under the DIRECT paradigm. Orthogonal to the works mentioned above, few methods [1, 3, 26, 30, 49, 61] take an agentic route to predicting answers through multi-turn reasoning, falling under the AGENT paradigm. Humans excel at tasks that require multi-turn reasoning. For example, when viewing 2-hour-long video, as humans, we take an iterative approach to finding the target information  (Fig. 1)  . With the recent overwhelming success of RL post-training for training multi-turn agent systems for long-horizon tasks like software engineering [39, 42, 54], computer-use [38, 41, 55], and deep-research [16, 27, 43], it is natural to expect multi-turn agent systems to do well at long video reasoning. Despite the analogy above, most of the existing long video reasoning systems are still trained following the DIRECT paradigm, even with RL [6, 48]. Motivated by the above realization, we explore the question: What are the technical challenges toward effectively training video reasoning models under the AGENT paradigm with Reinforcement Learning? We outline three significant aspects for answering the above question: training data (A1), efficient system design (A2), and RL recipe for multi-turn reasoning (A3). (A1) The training data for an agent model capable of long video reasoning requires access to high-quality question-answer (QnA) pairs. Collecting QnA pairs for long videos poses daunting challenge due to their lengthy duration. For example, having human annotate single 1-hour-long video can cost approximately $30 on the Prolific platform, making it expensive for data collection at scale. To avoid such high costs, existing works typically employ synthetic data curation process by iteratively processing 10-30 second-long subclips using models adept at short video understanding to either generate QnA pairs directly [4] or captions followed by QnA pairs using an LLM [5, 6]. Although inexpensive compared to human annotation, the mentioned bottom-up pipeline is slow and resource-intensive imagine processing 120 subclips for an hour-long video; even with each subclip taking only 10 seconds, it would take 20 minutes to process single video. Therefore, to save time and money, we leverage the longcontext modeling capabilities of Gemini-2.5-Flash to generate synthetic, high-quality QnA pairs with carefully designed prompt, ensuring the generated questions span the whole video. Moreover, we manually verify over 1700 generated samples and find low 5% error rate while achieving nearly 100 cost and 10 time savings compared to human annotation and subclip processing pipelines, respectively. (A2) Existing multi-turn agent systems usually use an LLM/VLM to orchestrate the calls to only temporal grounder tool [11, 30, 57] to iteratively locate an event over the entire video needed for finding an answer to given question. However, we posit that attempting to ground an event in the whole video is not always the most effective approach due to the lack of robust temporal grounding models for long videos. For example, knowing the Formula 1 2024 season standings enables intelligent reasoning with small temporal search space when watching the 2025 season livery reveal event video (Fig. 1a). Motivated by similar use cases, we introduce the SAGE (Smart Any-horizon aGEnt) system for long video reasoning. Particularly, we take more innovative approach by equipping our system with tools such as web search and speech transcription, in addition to temporal grounding, to ensure that it is adept at not only utilizing visual signals from the video but also leveraging verbal and external knowledge. At the core of our system lies an orchestrator VLM, SAGE-MM, responsible for deciding between multi-turn and single-turn behavior for effective any-horizon reasoning. Moreover, guided by the fact that user typically interacts with videos for entertainment [12, 21], we focus our efforts on verifying the effectiveness of our approach on SAGE-Bench, curated with videos from popular YouTube channels to simulate use cases in the daily lives of users. Interestingly, we find existing agent systems to be over-engineered toward answering multiple-choice questions, often underperforming at the open-ended problems under SAGE-Bench (Tab. 4), demonstrating their ineffectiveness for real-world use-cases. (A3) The variable duration of videos presents unique challenge to training multi-turn agents. Specifically, during the RL post-training stage, the model should learn to function as an any-horizon agent, i.e., directly output the answer for simple problems while using multi-turn reasoning for harder problems [58]. We believe that the optimization challenge posed by the dynamic nature of videos presents challenge for training agent models using existing RL recipes, which have been shown to work well for training DIRECT models [6, 18]. Moreover, extending the RLVR techniques [13, 37] to video reasoning presents another challenge due to the tasks open-ended nature, which results in lack of verifiable rewards. few DIRECT approaches [6, 47] overcome the verifiable reward challenge by training only on MCQ problems and/or using some form of string-overlap metrics [18, 46], rendering them ineffective at open-ended problems (Tab. 4). To that end, we propose multi-reward RL recipe that utilizes strong reasoning LLMs [33] to validate the correctness of answers during the RL post-training stage. Moreover, moving away from using string-matching for evaluation, we adopt universal LLM-as-a-judge evaluation approach to maintain uniformity across our training and evaluation setups. Our RL recipe improves the SFT model by 4.1% and surpasses the base by 5.7%, demonstrating its effectiveness. Moreover, for videos longer than 10 minutes, we observe performance improvements of up to 14.6% along with 4.8% for videos shorter than 10 minutes, provFigure 2. SAGE Workflow. Our system accepts four inputs (shown at the top): sampled video frames (F ), metadata about the video (M ), available tool definitions (T ), and the user query (Q). Given these inputs, SAGE operates in two stages based on the role of SAGE-MM. In Stage-1, SAGE-MM is responsible for providing information about the videos context (C) along with either final answer prediction or tool call to be executed before the next step. At every subsequent step in Stage-2, SAGE-MM uses the video context (C) and the tool call results from previous steps to decide either to predict the final answer or call another tool in an iterative reasoning process. ing SAGEs effectiveness on any-horizon video reasoning. In summary, we make the following contributions: We propose SAGE, an any-horizon agent for longvideo reasoning, equipped with web-search tool for knowledge-driven multi-turn reasoning. We introduce cost-effective synthetic QnA pipeline using Gemini-2.5-Flash to train and evaluate our system on entertainment videos for real-world use. We train SAGE-MM with an effective RL post-training recipe to instill any-horizon reasoning, demonstrating the scalability of our system design for RL. 2. Related Work 2.1. Long Video Reasoning Agents Existing long video reasoning agent systems are usually composed of two core components: an orchestrator, and tool set, with temporal grounder being standard tool among all methods. The orchestrator is responsible for determining the actions to execute while interacting with the available tools within multi-turn pipeline. common aspect in the design of existing long video reasoning agent systems is their over-reliance on temporal grounding module to perform event-guided multi-turn reasoning. VideoAgent [17] creates memory using the caption and keyframe features from the video subclips and incorporates tools to retrieve information from memory for reasoning. Similarly, VideoChat-A1 [49] employs keyframe retrieval to perform chain-of-shot reasoning. VideoMind [30] tunes LoRA adapters for the base Qwen2-VL [45] model as verifier to verify outputs from separate temporal grounder module before final answer prediction. VideoExplorer [57] optimizes the planner module with DPO [36] for better trajectory reasoning. LVAgent [3] leverages collaboration among multiple MLLMs with iterative reflection and key frame perception to reach the final answer. In this work, we move away from over-reliance on temporal grounding and incorporate tools such as web search and speech transcription to enable an intelligent event localization strategy. Moreover, unlike the above methods, SAGE attempts to predict timestamps for an event within one short subclip at time rather than the entire video, based on probable coarse event boundaries generated by SAGEMM, resulting in more efficient approach. 3 tool-name purpose arguments returns web-search parse-website transcribe-speech ground-event extract-video-parts analyze Perform web search using text query. Parse web data from given URL. Perform ASR on the video. Identify timestamps for an event in the video. Extract frames or subclips between two timestamps. Analyze set of media based on query. query (str); num-results (int) website-url (str) path (str), start (str), end (str) event (str), path (str), start (str), end (str) type (str), path (str), start (str), end (str) query (str), media-paths (List[str]) List of URL, title, and snippet for search results. Parsed HTML content of the website. Segment-level verbal transcript between the start and end timestamps. Timestamps for the event between the start and end timestamps. List of paths to the saved extracted parts (either frames or subclip). Answer to the query. Table 1. Supported tools in SAGE. Our system has access to six tools, including web search (via the Serper-hosted Google Search API), for performing knowledge-driven reasoning. We implement the ground-event and analyze tools using existing MLLMs [41]. 2.2. Reinforcement Learning for Video Reasoning 3.1. System Design Following the success of DeepSeek-R1 [13] at using Reinforcement Learning with Verifiable Rewards (RLVR) to improve reasoning abilities in LLMs, various works have tried to leverage GRPO [13, 37] to train DIRECT video reasoning models capable of thinking and then answering. Video-R1 [18] follows the optimization approach of DeepSeek-R1 and introduces contrastive temporal variant of GRPO, comparing answers between inputs with correct and incorrect frame ordering to enforce temporal dependence during reasoning. VideoRFT [46] introduces semantic-consistency reward between the reasoning trace and video frames. Video-Thinker [47] optimizes the model to output multiple temporal grounding instances within single reasoning trace by carefully curating the cold-start SFT dataset. LongVILA-R1 [6] enables the use of thousands of frames during the RL post-training stage with sequence parallelism. All the above methods utilize optionmatching and ROUGE metrics to compute rewards, rendering their approach suboptimal for open-ended problems. We train SAGE-MM to learn the ability to perform anyhorizon reasoning using GRPO while leveraging an LLMas-a-Judge to handle rewards for open-ended problems. concurrent work, LongVT [56] also employs similar training recipe with LLM-as-a-judge for computing the accuracy reward while only supporting crop-video tool call. 3. Method In the daily life of human, entertainment is the primary purpose for interacting with videos [12, 21], from watching sports videos on YouTube to scrolling through hundreds of short reels on Instagram. Therefore, its only natural to develop video reasoning models, keeping the users needs in mind. Among those needs, the open-ended interaction holds vital place. For instance, as shown in Fig. 1, user would usually ask: How does the Ferrari livery look this year? as an open-ended question and expect the model to provide an answer in real-time. We introduce SAGE, system designed to answer users questions while they enjoy entertainment videos. In the following subsections, we present technical details about SAGE (Sec. 3.1), followed by our synthetic data generation pipeline (Sec. 3.2). Lastly, we provide information on training the orchestrator (SAGEMM) using RL for the system (Sec. 3.3). As shown at the top of Fig. 2, our SAGE expects four inputs: 128 sampled frames from the video (F ), metadata about the video (M ), available tools definitions (T ), and the user query (Q). SAGE operates in two stages, based on the role of the orchestrator (SAGE-MM) (Fig. 2 bottom): Stage-1 (role: Context VLM): In this single-step stage, SAGE-MM accepts the system inputs (T QM ) and outputs JSON action string with required fields: video-context (C): Information about the videos setting. query-intent: The intent behind the users query. recommended-tool: Information about the next tool call if final answer cannot be generated at the current step. final-answer: null if tool call; otherwise predicted answer. The metadata string (M ) comprises information about the video path and duration, which are necessary to predict the arguments for the tool call. We list the supported tools in SAGE in Tab. 1. Notably, unlike previous methods, which either perform temporal grounding over the complete video [30, 57], our SAGE autonomously predicts segmentlevel timestamps to ground events over maximum duration of 10 minutes, as we qualitatively found that existing models struggle on longer entertainment videos. Stage-2 (role: In this multi-step stage, SAGE-MM accepts the tool call and video context results from all the previous steps, along with the other textual inputs (T QM ) and decides if the user query can be answered or another tool call is needed. At every step, SAGEMM outputs JSON action string with three required fields: answerable: Whether the query can be answered. recommended-tool: Information about the next tool call if final answer cannot be generated at the current step. final-answer: null if tool call; otherwise predicted answer. We set the maximum number of steps under stage 2 to ten to prevent indefinite execution length. We provide an example execution graph for SAGE at the bottom of Fig. 2. Iterative Reasoner): 3.2. Synthetic Data Generation We collect videos and shorts from 13 popular YouTube including sports (Forchannels across diverse genres, mula1), food (ZachChoi), comedy (TheDailyShow, MrBean, TheOffice, Friends, fluffyguy, trevornoah), education (Vox, kurzgesagt, veritasium, QuantaScienceChannel), and travel (WalkingAlice). Given video, our synthetic 4 Figure 3. Synthetic Data Generation Pipeline. We leverage Gemini-2.5-Flash to generate 10-20 QnA pairs, covering the full temporal span of the video. We find that instructing the model to predict percent video parsed field for every QnA pair helps in enforcing proper coverage. We use SAGE with Gemini-2.5-Flash as the orchestrator to synthesize tool call trajectories for cold-start SFT stage. data generation pipeline includes two stages: (i) questionanswer (QnA) pair generation using Gemini-2.5-Flash for training and evaluation, and (ii) tool call trajectory generation using SAGE with Gemini-2.5-Flash as the SAGE-MM for cold-start SFT, as shown in Fig. 3. QnA Pairs. We leverage the long context modeling abilities of Gemini-2.5-Flash [40] to generate questions and answers for given video in single pass using carefully designed prompt. We find that for videos longer than 5 minutes, having the model predict percent video parsed field is critical to ensure that the generated questions temporally span the complete video, as shown at the bottom of Fig. 3. We generate 10-20 QnA pairs per video. Tool Call Trajectories. We observe that existing opensource VLMs are not adept at functioning as SAGE-MM right off the shelf, which is necessity for successful RL post-training. Therefore, we also generate four tool call trajectories for each question and use input-action pairs from unique trajectories to create cold-start SFT dataset to finetune our own SAGE-MM model before the RL post-training stage. Tab. 2 lists statistics for our training data. 3.3. RL Post Training We use GRPO [13, 37] as the policy optimization algorithm during the RL post-training stage for trajectory-level optimization. Specifically, during the rollout generation, the ith action rollout trajectory for given input set S1 = {T, F, M, Q} is represented by τi. Therefore, we can formulate τi as sequence of state-action pairs [0, ]: τi = (cid:2)(S1, A1), (S2, A2), . . . , (SN , AN )(cid:3), Aj = SAGE-MM(Sj), Sj+1 = {T, Q, M, C, A1...Aj} (1) 060 60180 180300 300600 6001200 1200 2400+ total #videos #QnA #actions 1493 20.2k 43.4k 1907 23.0k 43.9k 552 8.0k 38.6k 612 9.4k 49.6k 1067 22.2k 115.0k 461 7.1k 52.2k 576 9.4k 75.0k 6668 99.1k 417.7k Table 2. Training Data Statistics. We generate over 99k questions for more than 6600 videos from popular YouTube channels. During the advantage computation step in GRPO, we assign single scalar reward Ri to every action in the trajectory τi with steps. The reward consists of (i) step-level rewards sj collected at each step, and (ii) final accuracy reward aN at the end of the trajectory. The resulting reward Ri is then uniformly assigned to all actions in τi: (2) Ri = (s1 + s2 + s3 + ... + sN ) + aN r(A1) = r(A2) = .... = r(AN ) = Ri Note that we can assign final rewards to all steps because rollout generation is synchronous, i.e., advantages are computed only after all trajectories are completed in batch. Step-Level Rewards. The reward (sj) for step in trajectory is sum of four scores: format: Encourages producing JSON action string with only the required fields. (cid:40) +0.05, 0.10, if JSON contains only required fields otherwise sformat = reasonable-tool: Encourages the model to perform sensible multi-step tool usage. Specifically, at each step, we ask GPT-4o to judge whether the current tool call is rational, given the question and the previous tool calls. sreasonable-tool = (cid:40) +0.10, 0.10, if current tool call is reasonable otherwise Overall Count Modality # samples # mcq # open-ended 1744 802 942 visual only verbal only visual + verbal (both) Count 1216 134 394 Duration (avg: 727 sec.) Bucket (sec.) Count Bucket (sec.) Count 060 60180 180300 300 6001200 12002400 2400+ 261 390 116 186 484 147 180 Table 3. SAGE-Bench Statistics. Our evaluation set holds 1744 manually verified samples spanning diverse durations, with an emphasis on questions that require visual information to answer. args-repeat: Penalizes repetitive tool call arguments. sargs-repeat = 0.05 (cid:112) num-repetitions args-valid: Penalizes invalid tool-call arguments. sargs-valid = (cid:40) 0.1, 0, if arguments are invalid otherwise We set the values for the step rewards such that the accumulated step-level reward for trajectory with 10 steps would be comparable to the accuracy reward. Accuracy Reward. We compute the outcome reward for trajectory of length based on the final answer prediction using an LLM judge (GPT-4o [33]) to obtain binary verdict indicating correctness at the last step. aN = 2.0, 0.5, +1.25, +1.0, if JSON action string is invalid if wrong answer and 1 if correct answer and visual tools in τi otherwise During training and inference, we set Nmax = 11 by default. However, during the RL stage, we find that setting Nmax = 6 for the first 100 steps is necessary for stable training, aligned with findings from concurrent work for training long-horizon LLM agents [52]. Moreover, we penalize the model for predicting wrong answer with tool calls to compensate for the positive step-level rewards while enforcing the any-horizon nature, i.e., making the model capable of predicting direct answer. Conversely, we grant slightly higher reward of +1.25 when the answer is correct and SAGE used visual tools (extract-video-parts or ground-event), reflecting the higher difficulty and importance of getting these tool calls right. 4. Experiments For our experiments, we finetune for MLLMs, using both cold-start SFT (denoted by SFT) and RL post-training (denoted by RL) stages to obtain the SAGE-MM: Molmo-8B, Figure 4. Qualitative Samples from SAGE-Bench. Our evaluation set contains questions that mirror what user might naturally ask while or after watching the corresponding video. Qwen2.5-VL-7B-Instruct [2], Qwen3-VL-4B-Instruct [41], and Qwen3-VL-8B-Instruct [41]. During training, we freeze the visual encoder and projector modules. By default, we use the Qwen3-VL-8B-Instruct as the base SAGE-MM for all our ablations. We implement the transcribe-speech tool using the Whisper-large-v3 [35] model. We use the Qwen3-VL-30B-A3B-Instruct [41] model to perform temporal grounding and reasoning with the ground-event and analyze tools, respectively. 4.1. Implementation Details Training Data. As shown in Tab. 2, we synthesize 99.1k training questions from 6659 videos, covering wide range of durations. Additionally, we generate 417.7k stateaction pairs for SFT. For RL, we construct dataset of 7.68k samples, filtered using synthetic tool-call trajectories, where half of the samples required tool calls and the other half had single-turn responses, promoting any-horizon reasoning. Training Recipe. During SFT, we train our model for one epoch with batch size of 64 and an initial learning rate of 1e5 with linear decay scheduler. We sample 128 frames at 2 FPS and use temporal pooling factor of 2, setting the maximum and minimum numbers of tokens per frame to 128 and 192, respectively. During RL, we use batch size of 16 and rollout eight action trajectories per sample. We use an initial learning rate of 1e6 with cosine decay scheduler. We set the KL-divergence loss coefficient to 0.005. Note that we report numbers for the model trained for 480 steps during the RL stage. We train all our models using 16 NVIDIA H100 GPUs during both SFT and RL. Evaluation. We evaluate all DIRECT baselines with 128 sampled frames as input, comparable to SAGE-MMs input setting. Moreover, we also pass the video transcript as extra context to the DIRECT baselines for fair comparison. For AGENT baselines, we follow their recommended setup. By default, we use LLM-as-judge (GPT-4o) for evaluating all models on both open-ended and MCQ problems. We set the temperature to 0.0 for all evaluations. How6 Method Orchestrator Video Reasoning Mode overall mcq open-ended both verbal visual train eval (1744) (802) (944) (394) (134) (1216) Gemini-2.5-Flash [40] SAGE-Flash (ours) GPT-4o [33] SAGE-Flash (ours) N/A SAGE-MM: Gemini-2.5-Flash N/A SAGE-MM: GPT-4o N/A Video-Thinker-7B [47] N/A LongVILA-R1-7B [6] N/A VideoRFT-7B [46] Video-R1-7B [18] N/A Qwen3-VL-30B-A3B-Instruct [41] N/A VideoAgent [17] LVAgent [3] LongVT [56] VideoMind [30] VideoExplorer [57] VideoChat-R1.5 [53] GPT-4o InternVL-8/72B [7] + LLaVA-Video-72B [59] LongVT-7B-RFT VideoMind-7B-Planner VideoExplorer-7B-Planner VideoChat-R1.5-7B-M Qwen2.5-VL-7B-Instruct [2] SAGE (ours) SAGE (ours) Qwen3-VL-4B-Instruct [41] SAGE (ours) SAGE (ours) Qwen3-VL-8B-Instruct [41] SAGE (ours) SAGE (ours) SAGE-Flash (ours) Molmo2-8B [10] SAGE (ours) SAGE (ours) SAGE-Flash (ours) N/A SAGE-MM: Qwen2.5-VL-7B-Instruct [+SFT] SAGE-MM: Qwen2.5-VL-7B-Instruct [+SFT] [+RL] N/A SAGE-MM: Qwen3-VL-4B-Instruct [+SFT] SAGE-MM: Qwen3-VL-4B-Instruct [+SFT] [+RL] N/A SAGE-MM: Qwen3-VL-8B-Instruct [+SFT] SAGE-MM: Qwen3-VL-8B-Instruct [+SFT] [+RL] SAGE-MM: Qwen3-VL-8B-Instruct [+SFT] [+RL] N/A SAGE-MM: Molmo2-8 [+SFT] SAGE-MM: Molmo2-8B [+SFT] [+RL] SAGE-MM: Molmo2-8B [+SFT] [+RL] DIRECT N/A DIRECT N/A DIRECT DIRECT DIRECT DIRECT DIRECT N/A N/A AGENT AGENT AGENT AGENT DIRECT AGENT AGENT DIRECT AGENT AGENT DIRECT AGENT AGENT AGENT DIRECT AGENT AGENT AGENT DIRECT AGENT DIRECT AGENT DIRECT DIRECT DIRECT DIRECT DIRECT AGENT AGENT AGENT AGENT AGENT AGENT DIRECT AGENT AGENT DIRECT AGENT AGENT DIRECT AGENT AGENT AGENT DIRECT AGENT AGENT AGENT 68.1 71.3 71.6 73. 41.3 52.6 55.3 57.6 67.6 42.0 49.7 46.7 50.0 50.1 54.8 58.6 61.1 63.4 62.7 64.6 68.4 64.9 63.9 68.0 71.8 61.8 63.2 66.1 67.8 77.2 81. 80.9 81.0 70.1 68.8 71.6 73.6 81.3 52.6 70.5 68.2 69.7 69.6 73.8 74.2 74.1 77.2 75.8 77.3 81.3 77.7 77.4 82.6 82.8 77.9 75.6 78.8 79. 60.4 62.9 63.6 66.9 16.8 38.7 41.4 43.9 55.8 32.9 32.1 28.4 33.2 35.1 38.6 45.4 50.1 51.5 51.6 53.7 57.4 54.0 52.4 55.6 62. 48.1 52.8 55.2 58.1 74.9 76.3 75.1 78.2 48.2 57.6 65.2 67.5 72.8 42.6 54.1 51.0 50.8 52.0 55.1 65.8 62.9 66.1 69.3 66.2 78. 72.8 72.3 75.4 75.1 66.8 67.0 67.3 67.5 71.6 84.3 73.9 79.9 41.8 64.9 67.2 67.2 71.6 29.1 48.5 37.3 41.8 40.2 48.5 68.7 69.4 65. 66.4 67.2 80.6 68.7 74.6 82.8 79.1 64.2 73.9 73.9 73.9 65.5 68.3 70.1 71.1 39.0 49.6 50.7 53.3 65.4 43.2 48.4 46.4 50.7 51.3 55. 55.2 59.6 62.2 60.2 63.7 63.8 61.9 60.0 64.0 69.9 60.0 60.9 64.8 67.3 Table 4. Comparison to Baselines. Using closed-source Gemini-2.5-Flash [40] and GPT-4o [33] as SAGE-MM improves upon the base models, showing the effectiveness of our system design. Our trained SAGE-MM also shows consistent improvements over all the baselines. SAGE-Flash refers to the setting where we use Gemini-2.5-Flash as the backend model for the ground-event and analyze tools. Existing AGENT systems exhibit considerably worse performance on open-ended problems compared to our SAGE. SAGE-MM overall 0600s 600+s train strategy train mode eval mode mcq open-ended overall training (1473) (842) (631) Qwen2.5-VL-7B-Instruct [2] N/A N/A VideoRFT-7B [46] N/A VideoMind-7B [30] N/A Video-R1-7B [18] N/A VideoChat-R1.5-7B [53] SAGE (ours) SAGE (ours) SAGE-Flash (ours) SFT SFT + RL SFT + RL 32.7 30.4 30.7 31.5 33.8 28.3 32.0 32.9 37.8 33.5 34.0 36.0 35.3 30.3 34.7 35. 25.8 26.2 26.2 25.8 31.8 24.3 28.4 29.0 Table 5. Performance on MINERVA [32]. Our SAGE shows significant improvements on videos longer than 600 seconds. ever, because the action strings must follow strict JSON schema, SAGE-MM occasionally produces malformed outputs. In such cases, we regenerate the response with temperature of 0.7 for up to four attempts, which may lead to non-deterministic behavior during inference. We serve all supported models using vLLM [25] during evaluation. We share more details, including the system, data generation, and evaluation prompts, in the appendix. 7 Qwen3-VL-4B-Instruct Qwen3-VL-4B-Thinking SFT SFT + RL SFT (ours) SFT + RL (ours) DIRECT DIRECT AGENT AGENT DIRECT DIRECT DIRECT DIRECT AGENT AGENT 75.8 75.3 83.2 83.0 77.3 81. 51.5 48.6 51.1 52.0 53.7 57.4 62.7 60.1 65.8 66.3 64.6 68. Table 6. Training Mode. Our AGENT system performs better than the DIRECT baseline, with RL playing critical role in the formers success, specifically on open-ended problems. 4.2. SAGE-Bench Driven by the limitations of current video reasoning benchmarks due to their purely MCQ nature, we curate our own evaluation set, SAGE-Bench, with focus on open-ended questions simulating the needs for real-world use-cases for entertainment videos. We begin by sampling subset of synthetic QnA pairs that is strictly disjoint from the training set (videos can be common) and manually verifying each sample for correctness. Notably, fewer than 5% of the samples required edits during verification, demonstrating that our synthetic data generation pipeline produces high-quality Method Model Qwen3-VL (baseline) Qwen3-VL-8B-Instruct SAGE (ours) SAGE (ours) SAGE-Flash (ours) Qwen3-VL-8B-Instruct [+SFT] SAGE-MM: Qwen3-VL-8B-Instruct [+SFT] [+RL] SAGE-MM: Qwen3-VL-8B-Instruct [+SFT] [+RL] Eval Mode DIRECT AGENT AGENT AGENT 0-60 (261) 73.9 60-180 180-300 300600-1200 1200-2400 2400+ overall (390) 72. (116) 81.9 (186) 71.5 (484) 55. (147) 59.2 (180) 47.5 (1744) 64. 74.3 78.5 (+4.6) 77.8 (+3.9) 68.1 70.3 (-2.0) 73.6 (+1.3) 75.0 77.4 (-4.5) 80.2 (-1.7) 72.0 72.6 (+1.1) 76.3 (+4.8) 56.8 63.2 (+8.2) 69.6 (+14.6) 55.8 61.9 (+2.7) 68.0 (+8.8) 48.1 53.8 (+6.3) 56.2 (+8.7) 63.9 68.0 (+3.1) 71.8 (+6.9) Table 7. Duration-wise Accuracy. Our SAGE shows significant improvements on samples belonging to buckets with duration longer than 600 seconds, with even more improvements when using Gemini-2.5-Flash as tool with SAGE-Flash. system SAGE-MM single-turn multi-turn overall Qwen3-VL-8B-Instruct (base) count acc. count acc. SAGE-Flash Gemini-2.5-Flash (expert) SAGE SAGE SAGE-Flash [+SFT] (ours) [+SFT] [+RL] (ours) [+SFT] [+RL] (ours) 706 948 940 76.9 79.0 79.6 78.8 885 1038 796 804 66. 53.7 54.3 63.4 acc. 71.3 64.6 68.0 71.8 Table 8. Any-Horizon Reasoning. RL refines the tools overcalling behavior of the SFT model, resulting in distribution closer to the expert Gemini-2.5-Flash and thus, improved performance. data at low cost. The statistics of SAGE-Bench are provided in Tab. 3. We also provide qualitative examples in Fig. 4. 4.3. Main Results In Tab. 4, we compare our SAGE to DIRECT video reasoning methods, including models trained without RL posttraining, like Qwen3-VL-4/8B-Instruct [41], and RL-tuned models, like Video-R1 [18]. We also evaluate AGENT systems like VideoMind [30] and VideoExplorer [57]. Effective System Design. We separately evaluate the performance of our system with two API-based models SAGEMM: Gemini-2.5-Flash [40] and GPT-4o [33]. For this setting, we use Gemini-2.5-Flash as the backend model for the ground-event and analyze tools; therefore, we denote the system as SAGE-Flash. We observe improvements of up to 3.2% over the base API models, validating the effectiveness of our system design. Effective Training Recipe. As shown in Tab. 4, our SAGE with trained SAGE-MM achieves notable improvements across different base MLLMs. Specifically, SAGE surpasses Qwen2.5-VL-7B-Instruct by 4.8% overall, with substantial gains of +6.1% on open-ended and +7.0% on visual questions, underscoring the effectiveness of our training strategy. Interestingly, models such as Video-R1 [18], VideoRFT [46], and VideoExplorer [57], despite employing finetuned Qwen2.5-VL-7B-Instruct backbones, underperform relative to the base model, particularly on openended questions. Moreover, as shown in the last row of Tab. 4, SAGE-Flash further improves upon SAGE by 3.8%, even outperforming the Gemini-2.5-Flash variant of SAGEMM. This indicates that our finetuned SAGE-MM not only learns to invoke tools effectively but also benefits from more accurate tool outputs. Additionally, we report results with Qwen2.5-VL-7BInstruct based SAGE-MM on MINERVA [32], complex SAGE (ours) w/o ground-event w/o web-search/parse-website w/o analyze w/o extract-video-parts w/o transcribe-speech overall both verbal visual 68.0 67.3 65.5 63.4 63.0 62.5 75. 72.3 70.1 70.6 70.8 66.8 82.8 79.9 80.6 80.6 79.9 46.3 64.0 64.3 62.4 59.1 58.6 62.9 Table 9. Dropping Tools during inference. All tools are critical to the success of SAGE as system, with the extract-video-parts and transcribe-speech being the most important ones for answering the visual and verbal/both questions, respectively, as expected. video reasoning benchmark that covers domains such as sports, short films, and cooking videos. As shown in Tab. 5, our SAGE shows an improvement of 2.6% on long videos (duration >600 seconds) compared to the base model while outperforming other reasoning models, validating the effectiveness of our approach for long video reasoning. 4.4. Ablations Training Mode. In Tab. 6, we finetune Qwen3-VL-4BInstruct model on the synthetic QnA pairs with DIRECT answering mode under the same data setting. We observe that our AGENT training recipe outperforms the direct baseline, underscoring the effectiveness of our approach. Specifically, while training the DIRECT baseline with SFT, we supervise the model with only the correct final answer and not the tool call actions. During RL, we use only the accuracy reward to train the DIRECT baseline. Duration-wise accuracy. We report duration-wise accuracies on SAGE-Bench in Tab. 7. Notably, our SAGE exhibits substantially higher gains on longer videos compared to shorter ones, achieving remarkable 8.2% improvement Incorporating Geminiin the 6001200 seconds bucket. 2.5-Flash as tool (SAGE-Flash) further boosts this gain to 14.6%, with more than 8% improvements in the 12002400 and 2400+ second buckets as well. Any-Horizon Reasoning. core aspect of systems design is to enable any-horizon reasoning, i.e., it is adept at multi-turn reasoning and also directly outputting an answer in single step. As shown in Tab. 8, our SFT model, distilled from the expert Gemini-2.5-Flash, inherits strong single-turn ability but tends to show signs of overcalling tools. Incorporating RL further refines this behavior while improving single-turn and multi-turn accuracies. 8 the top five potential segments generated by the preceding grounder module, which slows the system. 5. Conclusion In this work, we introduced SAGE, an any-horizon reasoning system for long video reasoning. We also designed cost-effective synthetic data generation pipeline for training and evaluating with the target use case of aiding users with open-ended queries while they watch entertainment videos in mind. Through extensive experiments, we validated the effectiveness of our system design and RL posttraining recipe at enabling any-horizon reasoning, with considerable gains on videos longer than 10 minutes. We hope our work can serve as vital proof-of-concept toward training practical AGENT systems for long video reasoning in the future, moving away from purely DIRECT approaches. Future Work. Looking ahead, training on data from broader domains to handle more use cases is natural advancement. In addition, integrating more advanced agentcentric policy optimization algorithms [15, 19, 60] for RL presents promising avenue. Finally, empowering the system to select the appropriate tools and synthesize new ones when necessary [31, 34] represents an exciting direction. Acknowledgements.This work was in part supported by NSF CAREER Award #2239840, and the National AI Institute for Exceptional Education (Award #2229873) by the National Science Foundation and the Institute of Education Sciences, U.S. Department of Education. We also thank the ML Center @Georgia Tech and PRIOR @Allen AI for supporting this work. method mode #frames acc. runtime (sec/sample) Qwen3-VL-8B-Instruct DIRECT VideoRFT-7B [46] Video-R1-7B [18] VideoMind-7B [30] LVAagent [3] VideoChat-R1.5-7B [53] VideoExplorer-7B [57] VideoAgent [17] SAGE DIRECT DIRECT AGENT AGENT AGENT AGENT AGENT AGENT 16 32 64 128 256 512 1024 1536 128 128 55.7 59.3 62.3 64.9 66.1 65.9 62.5 60. 55.3 57.6 50.0 49.7 54.8 50.1 42.0 68.0 0.8 1.1 2.3 3.6 5.7 7.8 18.3 27.5 7.2 7.3 24.7 92.9 132.1 137.7 1445.0 8.6 Table 10. Eval Runtime. Our SAGE shows good performanceefficiency tradeoff owing to its any-horizon reasoning nature. Importance of Supported Tools. We ablate the contribution of each tool in Tab. 9. Dropping the transcribe-speech, extract-video-parts, and analyze tools leads to the most significant performance decline, highlighting their fundamental role in long-video reasoning. In contrast, removing the ground-event tool results in only minor drop, likely due to the tools inherent inaccuracy. This observation underscores the need for developing better temporal grounding modules. Eval Runtime. In Tab. 10, we compare the accuracy score and inference runtime per sample for our SAGE to other existing DIRECT [18, 46] and AGENT [3, 17, 57] baselines and various frame-input setups of the baseline Qwen3-VL8B-Instruct [41]. We observe that although the runtime of our SAGE is comparable to using 512 frames as inputs to Qwen3-VL-8B-Instruct, it shows far superior performance, while only being slower by about 1 second compared to other thinking DIRECT baselines. Moreover, our system is almost 3 times quicker than VideoMind [30], the quickest AGENT baseline, demonstrating the superiority of our system design and training recipe for practical applications over existing systems. The lower runtime of our framework compared to the AGENT baselines is primarily due to the baselines system design, which involves heavy video preprocessing and excessive recurrent model calls. Notably, VideoAgent [17] is slowed by mandatory preprocessing phase in which every 2-second subclip undergoes multi-model analysis for metadata extraction, making it super slow for long videos. Similarly, VideoExplorer [57] suffers from both an initial 30-second preprocessing delay, arising from dividing the video into multiple subclips for embedding computation, and an inference process involving multiple retrieval steps. Finally, VideoMind [30] inherently requires more model invocations. This increase can be traced to the system design, which requires repetitive invocations of the verifier module. The verifier is executed multiple times, once for each of"
        },
        {
            "title": "References",
            "content": "[1] Kirolos Ataallah, Xiaoqian Shen, Eslam Abdelrahman, Essam Sleiman, Mingchen Zhuge, Jian Ding, Deyao Zhu, Jurgen Schmidhuber, and Mohamed Elhoseiny. Goldfish: Vision-language understanding of arbitrarily long videos, 2024. 2 [2] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5vl technical report. arXiv, 2025. 1, 6, 7 [3] Boyu Chen, Zhengrong Yue, Siran Chen, Zikang Wang, Yang Liu, Peng Li, and Yali Wang. Lvagent: Long video understanding by multi-round dynamical collaboration of mllm agents. arXiv, 2025. 2, 3, 7, 9 [4] Guo Chen, Zhiqi Li, Shihao Wang, Jindong Jiang, Yicheng Liu, Lidong Lu, De-An Huang, Wonmin Byeon, Matthieu Le, Tuomas Rintamaki, Tyler Poon, Max Ehrlich, Tuomas Rintamaki, Tyler Poon, Tong Lu, Limin Wang, Bryan Catanzaro, Jan Kautz, Andrew Tao, Zhiding Yu, and Guilin Liu. Eagle 2.5: Boosting long-context post-training for frontier vision-language models. arXiv, 2025. 2 [5] Yukang Chen, Fuzhao Xue, Dacheng Li, Qinghao Hu, Ligeng Zhu, Xiuyu Li, Yunhao Fang, Haotian Tang, Shang Yang, Zhijian Liu, Ethan He, Hongxu Yin, Pavlo Molchanov, Jan Kautz, Linxi Fan, Yuke Zhu, Yao Lu, and Song Han. Longvila: Scaling longcontext visual language models for long videos. arXiv, 2024. 1, 2 [6] Yukang Chen, Wei Huang, Baifeng Shi, Qinghao Hu, Hanrong Ye, Ligeng Zhu, Zhijian Liu, Pavlo Molchanov, Jan Kautz, Xiaojuan Qi, Sifei Liu, Hongxu Yin, Yao Lu, and Song Han. Scaling rl to long videos. In NeurIPS, 2025. 2, 4, [7] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, Bin Li, Ping Luo, Tong Lu, Yu Qiao, and Jifeng Dai. Internvl: Scaling up vision foundation models and aligning for generic visuallinguistic tasks. arXiv, 2023. 1, 7 [8] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv, 2024. 1 think like holmes for complex video reasoning? arXiv, 2025. 15 [10] Christopher Clark, Jieyu Zhang, Zixian Ma, Jae Sung Park, Rohun Tripathi, Sangho Lee, Mohammadreza Salehi, Jason Ren, Chris Dongjoo Kim, Yinuo Yang, Vincent Shao, Yue Yang, Weikai Huang, Ziqi Gao, Taira Anderson, Jianrui Zhang, Jitesh Jain, George Stoica, Winston Han, Ali Farhadi, and Ranjay Krishna. Molmo2: Open Weights and Data for Vision-Language Models with Video Understanding and Grounding. https://allenai.org/papers/ molmo2, 2025. 7, 14 [11] Jisheng Dang, Huilin Song, Junbin Xiao, Bimei Wang, Han Peng, Haoxuan Li, Xun Yang, Meng Wang, and Tat-Seng Chua. Mupa: Towards multi-path agentic reasoning for grounded video question answering. arXiv, 2025. 2 [12] Claire Dannenbaum. 5 facts about americans and youtube. Pew Research Center, 2025. 2, 4 [13] DeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv, 2025. 2, 4, 5 [14] Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, Jiasen Lu, Taira Anderson, Erin Bransom, Kiana Ehsani, Huong Ngo, YenSung Chen, Ajay Patel, Mark Yatskar, Chris Callison-Burch, Andrew Head, Rose Hendrix, Favyen Bastani, Eli VanderBilt, Nathan Lambert, Yvonne Chou, Arnavi Chheda, Jenna Sparks, Sam Skjonsberg, Michael Schmitz, Aaron Sarnat, Byron Bischoff, Pete Walsh, Chris Newell, Piper Wolters, Tanmay Gupta, Kuo-Hao Zeng, Jon Borchardt, Dirk Groeneveld, Jen Dumas, Crystal Nam, Sophie Lebrecht, Caitlin Wittlif, Carissa Schoenick, Oscar Michel, Ranjay Krishna, Luca Weihs, Noah A. Smith, Hannaneh Hajishirzi, Ross Girshick, Ali Farhadi, and Aniruddha Kembhavi. Molmo and pixmo: Open weights and open data for state-of-the-art multimodal models. In CVPR, 2025. 1 [15] Guanting Dong, Licheng Bao, Zhongyuan Wang, Kangzhi Zhao, Xiaoxi Li, Jiajie Jin, Jinghan Yang, Hangyu Mao, Fuzheng Zhang, Kun Gai, Guorui Zhou, Yutao Zhu, Ji-Rong Wen, and Zhicheng Dou. Agentic entropy-balanced policy optimization. arXiv, 2025. 9 [16] Guanting Dong, Yifei Chen, Xiaoxi Li, Jiajie Jin, Hongjin Qian, Yutao Zhu, Hangyu Mao, Guorui Zhou, Zhicheng Dou, and Ji-Rong Wen. Tool-star: Empowering llm-brained multi-tool reasoner via reinforcement learning. arXiv, 2025. 2 [9] Junhao Cheng, Yuying Ge, Teng Wang, Yixiao Ge, Jing Liao, and Ying Shan. Video-holmes: Can mllm [17] Yue Fan, Xiaojian Ma, Rujie Wu, Yuntao Du, Jiaqi Li, Zhi Gao, and Qing Li. Videoagent: memoryaugmented multimodal agent for video understanding. In ECCV, 2024. 3, 7, 9 [18] Kaituo Feng, Kaixiong Gong, Bohao Li, Zonghao Guo, Yibing Wang, Tianshuo Peng, Benyou Wang, and Xiangyu Yue. Video-r1: Reinforcing video reasoning in mllms. In NeurIPS, 2025. 2, 4, 7, 8, 9 [19] Lang Feng, Zhenghai Xue, Tingcong Liu, and Bo An. Group-in-group policy optimization for llm agent training. arXiv, 2025. 9 [20] Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, Peixian Chen, Yanwei Li, Shaohui Lin, Sirui Zhao, Ke Li, Tong Xu, Xiawu Zheng, Enhong Chen, Caifeng Shan, Ran He, and Xing Sun. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis, 2025. 15 [21] Sharon Hafuta. Video marketing statistics the ultimate video marketing stats report. Wix Blog, 2025. 2, 4 [22] Kairui Hu, Penghao Wu, Fanyi Pu, Wang Xiao, Yuanhan Zhang, Xiang Yue, Bo Li, and Ziwei Liu. Video-mmmu: Evaluating knowledge acquisition from multi-discipline professional videos, 2025. 15 [23] Jitesh Jain, Jianwei Yang, and Humphrey Shi. VCoder: Versatile Vision Encoders for Multimodal Large Language Models. In CVPR, 2024. 1 [24] Jitesh Jain, Zhengyuan Yang, Humphrey Shi, Jianfeng Gao, and Jianwei Yang. Elevating Visual Perception in Multimodal LLMs with Visual Embedding Distillation. In NeurIPS, 2025. 1 [25] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. 7 [26] Boyi Li, Ligeng Zhu, Ran Tian, Shuhan Tan, Yuxiao Chen, Yao Lu, Yin Cui, Sushant Veer, Max Ehrlich, Jonah Philion, et al. Wolf: Dense video captioning with world summarization framework. Transactions on Machine Learning Research, 2025. 2 [27] Xiaoxi Li, Wenxiang Jiao, Jiarui Jin, Guanting Dong, Jiajie Jin, Yinuo Wang, Hao Wang, Yutao Zhu, JiRong Wen, Yuan Lu, and Zhicheng Dou. Deepagent: general reasoning agent with scalable toolsets. arXiv, 2025. 2 [28] Ji Lin, Hongxu Yin, Wei Ping, Yao Lu, Pavlo Molchanov, Andrew Tao, Huizi Mao, Jan Kautz, Mohammad Shoeybi, and Song Han. Vila: On pretraining for visual language models. arXiv, 2023. [29] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In CVPR, 2024. 1 [30] Ye Liu, Kevin Qinghong Lin, Chang Wen Chen, and Mike Zheng Shou. Videomind: chain-of-lora agent for long video reasoning. arXiv, 2025. 2, 3, 4, 7, 8, 9 [31] Damiano Marsili, Rohun Agrawal, Yisong Yue, and Georgia Gkioxari. Visual agentic ai for spatial reasoning with dynamic api. In CVPR, 2025. 9 [32] Arsha Nagrani, Sachit Menon, Ahmet Iscen, Shyamal Buch, Ramin Mehran, Nilpa Jha, Anja Hauth, Yukun Zhu, Carl Vondrick, Mikhail Sirotenko, Cordelia Schmid, and Tobias Weyand. Minerva: Evaluating complex video reasoning. arXiv, 2025. 7, 8 [33] OpenAI. Gpt-4o system card. arXiv, 2024. 2, 6, 7, 8 [34] Viraj Prabhu, Yutong Dai, Matthew Fernandez, Jing Gu, Krithika Ramakrishnan, Yanqi Luo, Silvio Savarese, Caiming Xiong, Junnan Li, Zeyuan Chen, and Ran Xu. Walt: Web agents that learn tools. arXiv, 2025. 9 [35] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. Robust speech recognition via large-scale weak supervision, 2022. 6 [36] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. In NeurIPS, 2023. [37] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Y.K. Li Mingchuan Zhang, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv, 2024. 2, 4, 5 [38] ByteDance Seed Team. Seed1.5-vl technical report. arXiv, 2025. 2 [39] FAIR CodeGen team, Jade Copet, Quentin Carbonneaux, Gal Cohen, Jonas Gehring, Jacob Kahn, Jannik Kossen, Felix Kreuk, Emily McMilin, Michel Meyer, Yuxiang Wei, David Zhang, Kunhao Zheng, Jordi Armengol-Estape, Pedram Bashiri, Maximilian Beck, Pierre Chambon, Abhishek Charnalia, Chris Cummins, Juliette Decugis, Zacharias V. Fisches, Francois Fleuret, Fabian Gloeckle, Alex Gu, Michael Hassid, Daniel Haziza, Badr Youbi Idrissi, Christian Keller, Rahul Kindi, Hugh Leather, Gallil Maimon, Aram Markosyan, Francisco Massa, Pierre-Emmanuel Mazare, Vegard Mella, Naila Murray, Keyur Muzumdar, Peter OHearn, Matteo Pagliardini, Dmitrii Pedchenko, Tal Remez, Volker Seeker, Marco Selvi, Oren Sultan, Sida Wang, Luca Wehrstedt, Ori Yoran, Lingming Zhang, Taco Cohen, Yossi Adi, and Gabriel Syn11 naeve. Cwm: An open-weights llm for research on code generation with world models. arXiv, 2025. [40] Gemini Team. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv, 2025. 1, 5, 7, 8 [41] Qwen Team. Qwen3-vl technical report. arXiv, 2025. 1, 2, 4, 6, 7, 8, 9, 14 [42] Qwen Team. Qwen3 technical report. arXiv, 2025. 2 [43] Tongyi DeepResearch Team, Baixuan Li, Bo Zhang, Dingchu Zhang, Fei Huang, Guangyu Li, Guoxin Chen, Huifeng Yin, Jialong Wu, Jingren Zhou, et al. Tongyi deepresearch technical report. arXiv, 2025. 2 [44] Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha Akula, Jihan Yang, Shusheng Yang, Adithya Iyer, Xichen Pan, Austin Wang, Rob Fergus, Yann LeCun, and Saining Xie. Cambrian-1: fully open, visioncentric exploration of multimodal llms. In NeurIPS, 2024. 1 [45] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv, 2024. 1, 3 [46] Qi Wang, Yanrui Yu, Ye Yuan, Rui Mao, and Tianfei Zhou. Videorft: Incentivizing video reasoning capability in mllms via reinforced fine-tuning. arXiv, 2025. 2, 4, 7, 8, [47] Shijian Wang, Jiarui Jin, Xingjian Wang, Linxin Song, Runhao Fu, Hecheng Wang, Zongyuan Ge, Yuan Lu, and Xuelian Cheng. Video-thinker: Sparking thinking with videos via reinforcement learning. arXiv, 2025. 2, 4, 7 [48] Weiyun Wang, Zhangwei Gao, Lixin Gu, Hengjun Pu, Long Cui, Xingguang Wei, Zhaoyang Liu, Linglin Jing, Shenglong Ye, Jie Shao, et al. Internvl3.5: Advancing open-source multimodal models in versatility, reasoning, and efficiency. arXiv, 2025. 1, 2 [49] Zikang Wang, Boyu Chen, Zhengrong Yue, Yi Wang, Yu Qiao, Limin Wang, and Yali Wang. Videochat-a1: Thinking with long videos by chain-of-shot reasoning. arXiv, 2025. 2, 3 [50] Zengzhi Wang, Fan Zhou, Xuefeng Li, and Pengfei Liu. Octothinker: Mid-training incentivizes reinforcement learning scaling. arXiv, 2025. 14 [51] Haoning Wu, Dongxu Li, Bei Chen, and Junnan Li. Longvideobench: benchmark for long-context interleaved video-language understanding, 2024. 15 [52] Zhiheng Xi, Jixuan Huang, Chenyang Liao, Baodai Huang, Honglin Guo, Jiaqi Liu, Rui Zheng, Junjie Ye, Jiazheng Zhang, Wenxiang Chen, Wei He, Yiwen Ding, Guanyu Li, Zehui Chen, Zhengyin Du, Xuesong Yao, Yufei Xu, Jiecao Chen, Tao Gui, Zuxuan Wu, Qi Zhang, Xuanjing Huang, and Yu-Gang Jiang. Agentgym-rl: Training llm agents for longhorizon decision making through multi-turn reinforcement learning. arXiv, 2025. [53] Ziang Yan, Xinhao Li, Yinan He, Zhengrong Yue, Xiangyu Zeng, Yali Wang, Yu Qiao, Limin Wang, and Yi Wang. Videochat-r1.5: Visual test-time scaling to reinforce multimodal reasoning by iterative perception. arXiv, 2025. 7, 9 [54] John Yang, Carlos Jimenez, Alexander Wettig, Kilian Lieret, Shunyu Yao, Karthik Narasimhan, and Ofir Press. SWE-agent: Agent-computer interfaces enable automated software engineering. In NeurIPS, 2024. 2 [55] Yan Yang, Dongxu Li, Yutong Dai, Yuhao Yang, Ziyang Luo, Zirui Zhao, Zhiyuan Hu, Junzhe Huang, Amrita Saha, Zeyuan Chen, Ran Xu, Liyuan Pan, Silvio Savarese, Caiming Xiong, and Junnan Li. Gta1: Gui test-time scaling agent. arXiv, 2025. 2 [56] Zuhao Yang, Sudong Wang, Kaichen Zhang, Keming Wu, Sicong Leng, Yifan Zhang, Bo Li, Chengwei Qin, Shijian Lu, Xingxuan Li, and Lidong Bing. Longvt: Incentivizing thinking with long videos via native tool calling. arXiv, 2025. 4, 7 [57] Huaying Yuan, Zheng Liu, Junjie Zhou, Hongjin Qian, Yan Shu, Nicu Sebe, Ji-Rong Wen, and Zhicheng Dou. Think with videos for agentic long-video understanding. In ICLR, 2025. 2, 3, 4, 7, 8, 9 [58] Zizheng Zhan, Ken Deng, Huaixi Tang, Wen Xiang, Kun Wu, Weihao Li, Wenqiang Zhu, Jingxuan Xu, Lecheng Huang, Zongxian Feng, Shaojie Wang, Shangpeng Yan, Xuxing Chen, Jiaheng Liu, Zhongyuan Peng, Zuchen Gao, Haoyang Huang, Xiaojiang Zhang, Jinghui Wang, Zheng Lin, Mengtong Li, Huiming Wang, Ziqi Zhan, Yanan Wu, Yuanxing Zhang, Jian Yang, Guang Chen, Haotian Zhang, Bin Chen, and Bing Yu. Kat-v1: Kwai-autothink technical report. arXiv, 2025. [59] Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, and Chunyuan Li. Video instruction tuning with synthetic data. arXiv, 2024. 7 [60] Chujie Zheng, Shixuan Liu, Mingze Li, Xiong-Hui Chen, Bowen Yu, Chang Gao, Kai Dang, Yuqiong Liu, Rui Men, An Yang, Jingren Zhou, and Junyang Lin. Group sequence policy optimization. arXiv, 2025. 9 [61] Zhuo Zhi, Qiangqiang Wu, Minghe shen, Wenbo Li, Yinchuan Li, Kun Shao, and Kaiwen Zhou. Videoagent2: Enhancing the llm-based agent system for long-form video understanding by uncertainty-aware cot. arXiv, 2025. 2 12 [62] Junjie Zhou, Yan Shu, Bo Zhao, Boya Wu, Zhengyang Liang, Shitao Xiao, Minghao Qin, Xi Yang, Yongping Xiong, Bo Zhang, Tiejun Huang, and Zheng Liu. Mlvu: Benchmarking multi-task long video understanding, 2025. 15 [63] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing visionlanguage understanding with advanced large language models. arXiv, 2023. [64] Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen Duan, Weijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv, 2025."
        },
        {
            "title": "Appendix",
            "content": "overall both verbal visual SAGE-MM system eval mode overall mcq open-ended method Qwen3-VL-8B-Instruct Qwen3-VL-8B-Instruct SAGE (ours) SAGE (ours) video 64.9 42.1 68.0 41.0 72.8 58.6 75.4 48. 68.7 70.9 82.8 35.8 61.9 33.6 64.0 39.3 Table 11. Importance of the Video Input. Access to the video is critical for good performance on SAGE-Bench. In this appendix, we first present additional ablations in Sec. 6, including the effect of video input on SAGE-Bench, the importance of the cold-start SFT stage, and the impact of varying Nmax during evaluation. Secondly, we provide qualitative examples from our SAGE-Bench in Sec. 7 along with comparison to existing benchmarks. Next, we list all the system prompts used in our work in Sec. 8. Lastly, we list qualitative examples from the QnA pair generation pipeline in Sec. 9. Unless mentioned otherwise, we use the Qwen3-VL-8B-Instruct-based SAGE-MM for all experiments in this appendix and report results after RL on SAGE-Bench. 6. Additional Ablations Importance of the Video Input. Although we build SAGE-Bench using questions strictly disjoint from the training set, some videos in SAGE-Bench overlap with those seen during training. Therefore, it is essential to assess potential memorization. natural test is to evaluate whether the model produces correct answers without access to the video. We find no evidence of memorization: performance drops by 27%, similar to the drop observed for the base Qwen3-VL-8B-Instruct model, as shown in Tab. 11, underscoring the validity of our approach and findings. Eval Mode. In Tab. 12, we analyze the effect of eval mode with the trained Qwen3-VL-8B-Instruct SAGE-MM during inference. We find that the AGENT mode performs better than DIRECT mode. Surprisingly, the Qwen3VL [41] based model shows much better performance than the Molmo2 [10] one with DIRECT eval mode which could be attributed to the two model families different abilities to learn information directly since all the models are trained under the AGENT paradigm. Importance of SFT. Our SAGE is designed so that any MLLM with function-calling capabilities can be used as the SAGE-MM. In Tab. 13, we evaluate the base Qwen3VL [41] models as SAGE-MM, without any finetuning. We observe that Qwen3-VL-4B-Instruct is not an effective orchestrator: it rarely engages in multi-turn reasoning and at14 Qwen3-VL-8B-Instruct [+SFT] [+SFT] [+RL] Molmo2-8B [+SFT] [+SFT] [+RL] Qwen3-VL SAGE SAGE-Flash Qwen3-VL SAGE SAGE-Flash Molmo2 SAGE SAGE-Flash Molmo2 SAGE SAGE-Flash DIRECT AGENT AGENT DIRECT AGENT AGENT DIRECT AGENT AGENT DIRECT AGENT AGENT 63.6 63.9 70.5 69.8 68.0 71.8 55.7 63.3 69.8 61.0 66.1 67.8 78.4 77.4 81. 84.0 82.6 82.8 68.5 75.6 79.9 71.7 78.8 79.3 50.9 52.4 61.5 57.6 55.6 62.4 44.9 52.8 61. 51.9 55.2 58.1 Table 12. Eval Mode. We find that for the trained SAGE-MM, AGENT mode during inference outperforms the DIRECT mode. All the models are trained under the AGENT paradigm tains low accuracy, indicating that SFT is essential before applying RL. Interestingly, the base Qwen3-VL-8B-Instruct model behaves differently. It is noticeably stronger function caller, demonstrating more reasonable balance between singleturn and multi-turn reasoning. This motivates us to apply RL directly on top of the base model to assess the importance of SFT for the 8B variant. Surprisingly, RL without SFT fails, i.e., the model collapses to single-turn reasoning. We hypothesize that this is due to the base models training objective, which strongly biases it toward directly producing final answers, making SFT necessary to incentivize [50] any-horizon reasoning during RL. While it is possible that heavily engineered RL recipe could overcome this, we do not pursue this direction, as SFT is far simpler and cheaper than extensive hyperparameter tuning during RL. #Turns v/s Video Duration. In Tab. 14, we report the average number of reasoning turns across all samples grouped by video duration buckets. We observe gradual increase in the number of turns as video length increases, indicating that SAGE naturally adapts its trajectory length to the temporal horizon of the input. Shorter videos lead to shorter reasoning trajectories, whereas longer videos elicit more extended ones, aligned with our design objective of instilling any-horizon reasoning into the system. Effect of Nmax. We study the effect of varying Nmax during evaluation in Tab. 15. We find that setting Nmax = 11 achieves high accuracy while keeping the number of unanswered samples low, with only minimal gains from further increases in Nmax. This demonstrates the effectiveness of SAGE-MM single-turn multi-turn overall Qwen3-VL-4B-Instruct Qwen3-VL-4B-Instruct [+SFT] (ours) Qwen3-VL-4B-Instruct [+SFT] [+RL] (ours) Qwen3-VL-8B-Instruct Qwen3-VL-8B-Instruct [+RL] Qwen3-VL-8B-Instruct [+SFT] (ours) Qwen3-VL-8B-Instruct [+SFT] [+RL] (ours) count acc. count acc. 1345 691 832 802 1727 706 948 54.6 79.5 80.5 79.5 56.9 79.0 79. 399 1045 912 942 17 1038 796 52.8 54.7 57.3 53.7 23.6 53.7 54. acc. 54.5 64.6 68.4 63.2 56.6 63.9 68.0 Table 13. Importance of SFT. The cold-start SFT stage is necessary to incentivize multi-turn reasoning during RL. SAGE-MM 0-60 60-180 180-300 300-600 600-1200 12002400+ Qwen3-VL-8B-Instruct [+SFT] Qwen3-VL-8B-Instruct [+SFT] [+RL] 2.00 1.74 2.23 1.81 2.05 1.83 2.63 2. 3.02 2.49 3.50 2.89 3.54 2.77 Table 14. #Turns v/s Video Duration. The average number of reasoning turns grows gradually with an increase in video duration, demonstrating our SAGEs any-horizon nature. Nmax 2 3 6 11 (default) 13 system SAGE-MM acc. no ans. acc. no ans. acc. no ans. acc. no ans. acc. no ans. acc. no ans. acc. no ans. Qwen3-VL-8B-Instruct [+SFT] SAGE Qwen3-VL-8B-Instruct [+SFT] [+RL] SAGE SAGE-Flash Qwen3-VL-8B-Instruct [+SFT] [+RL] 33.8 43.4 43. 60.8 46.8 47.1 46.0 56.7 57.1 45.0 30.4 30.1 57.0 62.8 63.2 23.4 20.2 21.2 62.1 66.6 70. 5.5 3.9 6.3 63.9 68.0 71.8 3.3 1.3 3.3 63.8 67.8 72.2 3.5 1.1 2.9 64.6 67.9 71. 3.0 1.1 2.9 Table 15. Effect of Nmax. Limiting the total number of turns to 11 is optimal as our RL recipe enforces the ability to produce an answer in as many turns. no ans. denotes the percentage of samples where an answer could not be produced. acc. denotes the accuracy score. run-1 run-2 run-3 runrun-5 mean std 64.9 64.6 64.9 65. 65.1 64.9 0.22 unbounded answer space, aligning more closely with practical user situations. Table 16. Variance on SAGE-Bench. We find low standard deviation of 0.22 across five different runs with Qwen3-VL-8BInstruct with temperature set to 1.0. our RL recipe in enforcing answer prediction within an 11step reasoning horizon. Variance on SAGE-Bench. We analyze the variance in performance on SAGE-Bench across five different runs (with temperature of 1.0) in Tab. 16. We find low standard deviation of 0.22 for the base Qwen3-VL-8B-Instruct model. The low variance indicates that the performance improvements from our SAGE are statistically significant. 7. SAGE-Bench Compared to existing video understanding benchmarks, SAGE-Bench demonstrates two distinct advantages: High-Quality Open-Ended Questions. As illustrated in Fig. 5, existing popular benchmarks [9, 20, 22, 51, 62] rely purely on multiple-choice questions (MCQs). In contrast, SAGE-Bench utilizes open-ended questions with an Dual Focus on Diagnostic and Practical Evaluation. While AI systems are ultimately intended for real-world deployment, existing benchmarks often include diagnostic questions to gauge models visual understanding, such as temporal ordering tasks in MLVU [62]. SAGE-Bench incorporates both diagnostic questions to test fundamental model capabilities and practical questions that users may have while watching entertainment videos, ensuring that our benchmark evaluates not only technical proficiency but also the models utility in practical scenarios. 8. System Prompts We provide information about the system prompts used for different purposes in this work below: QnA Pair Generation: Fig. 6. LLM-Judge Evaluation: Fig. 7. SAGE Stage-1 (Context VLM): Fig. 8. SAGE Stage-2 (Iterative Reasoner): Fig. 9. ground-event tool: Fig. 10. Reasonable Tool Step Reward Computation: Fig. 11. DIRECT baselines Evaluation: Fig. 12. 15 Figure 5. Comparing SAGE-Bench to existing benchmarks. SAGE-Bench contains samples covering both practical scenarios (IDs: witFwlBjfLo, jjg4hWDFbmY, hh4prBn66Dc, 1SjmrYNHqiA) and diagnostic cases. Representative examples for other benchmarks are sourced from their respective websites or papers. 9. Qualitative Examples QnA Pairs. We display some samples of the generated QnA pairs in Fig. 14 and Fig. 13. SAGE Any-Horizon Reasoning Trajectories. We display qualitative examples to demonstrate the any-horizon reasoning abilities of SAGE in Fig. 15 (5 turns), Fig. 16 (2 turns), and Fig. 17 (single-turn). System Prompt to generate QnA pairs using Gemini-2.5-Flash 17 You are specialized question generator. Your primary function is to generate 1020 questions based on the provided video which can be upto 2 hours (7200 seconds) long. - Pay attention to what modality information is needed to answer the question. You should generate questions that viewer may be interested in and require visual, verbal, and or both in balanced manner. - You MUST give atleast four questions that cannot be answered with verbal information and require visual information. - Also, its okay to give questions that are not answerable from the video but can be answered with web search. - Generate mix of open ended and multiple choice questions which are both hard and easy to answer. Err on the side of hard if you are unsure. The duration of the video is <<<video duration>>> seconds ( <<<timestamp format>>> in HH:MM:SS format). First think about the facts from the video and then generate questions about those. The questions could refer to the part of the video that spans across 10 seconds long but most MUST refer to the timeframes atleast few minutes long. Your timestamps MUST be in HH:MM:SS format. Output Format. You MUST follow this format and MUST be between the <json> and </json> tags: < json > { \" timestamp_format \":\"HH:MM:SS\", \" num_questions \": <number of questions generated>, \" questions \": [ { be mcq p n c be y , medium, d be u , b , o \" index \": <index of question out of total question>, \" type \": \"type of question\", / / \" difficulty \": <difficulty of question>, / / \" difficulty_rationale \": <why-this-difficulty>, \" modality \": <modality of question>, / / \" modality_rationale \": <why-this-modality>, \" answer \": <answer text>, / / e s t t o r h open e e o mcq, w h h e \" question \": <question text>, \" options \": [ / / e s t f h p q t i mcq, u h e o , c e p n , e s e e u i s s h e i i i f i , <option 1>, <option 2>, <option 3>, <option 4>, <option 5>, <option 6> ] \" requires_web_search \": <true false>, / / q t r i a web r a i \" why_web_search \": <reasoning for why web search is needed to answer the r , e s t be w d , n s s question>, / / h h i h i l t u i e r web r o be w d , n s s n r why web r s d o w h e o , \" final_timestamp \": <duration of the video>, # HH:MM:SS \" start_timestamp \": <start timestamp of question>, # HH:MM:SS \" end_timestamp \": <end timestamp of question>, # HH:MM:SS \" compute_percent_video_parsed \": <think carefully and predict accurate percent video parsed, show calculation here>, \" percent_video_parsed \": <percentage of the video parsed upto this question> # [ ( t s p ( o ) / a l t 90 o 100 a a one s n e m ( o ) ) * 1 0 0 ] MUST go o }, ... ] } </ json > This output will be converted to JSON dict later on, you MUST use the correct syntax. Figure 6. System Prompt to generate QnA pairs using Gemini-2.5-Flash. Placeholder text to be replaced by the corresponding values are in red. System Prompt for the LLM-Judge during evaluation and RL to compute accuracy Compare the model prediction and the ground truth and determine if they convey the same meaning for the question: Question: {question} Model Prediction: {hypothesis} Ground Truth: {reference} You MUST respond with the verdict as True if they match semantically or False if they dont match. Answer in the following format: Reasoning : <Reasoning for the verdict> Verdict : <True/False> Figure 7. System Prompt for the LLM-Judge during evaluation and RL to compute accuracy. Placeholder text to be replaced by the corresponding values are in red. SAGE Stage-1: Context VLM System Prompt You are specialized Context VLM (Video Language Model) designed to analyze video content and determine the appropriate context for further processing. Your primary functions are to: - Analyze the given video and query - Recommend the next appropriate tool or sequence of tools - Suggest specific arguments to pass to those tools Your output MUST follow this structure and MUST be between the <json> and </json> tags: <json > { \" video_context \": <visual context>, \" query_intent \": <users intent>, \" final_answer \": \"Direct and concise answer to the users query, if and only if the query is answerable based on current context. Otherwise, this should be null.\", \" recommended_tools \": { \" needed \": true false , \" why_no_tool \": \"Only if no more tool call is needed\", \" tool_calls \": [ { \" rationale \": \"Why this tool is the best next step\", \" name \": <name of tool>, \" arguments \": { \" arg1 \": <value1>, \" arg2 \": <value2> } } ] } } </ json > The available tools are: <<<tools>>> Figure 8. SAGE Stage-1: Context VLM System Prompt. Placeholder text to be replaced by the corresponding values are in red. 19 SAGE Stage-2: Iterative Reasoner System Prompt You are reasoning agent. Your primary goal is to determine whether the available visual context and tool call information contains sufficient information to answer the users query. If not, recommend which tools to invoke next, with appropriate arguments. Do not make assumptions beyond the evidence provided. Avoid fabricating facts. Output Format. You MUST follow this format and MUST be between the <json> and </json> tags: <json > { \" answerable \" : { \" verdict \": true false , \" reasoning \": \"Why the available information is sufficient or not\" }, \" final_answer \": \"If the query is answerable, otherwise null.\", \" recommended_tools \": { \" needed \": true false , \" why_no_tool \": \"Only if no more tool call is needed\", \" tool_calls \": [ { \" rationale \": \"Why this tool is the best next step\", \" name \": <name of tool>, \" arguments \": { \" arg1 \": <value1>, \" arg2 \": <value2> } } ] } } </ json > The available tools are: <<<tools>>> Figure 9. SAGE Stage-2: Iterative Reasoner System Prompt. Placeholder text to be replaced by the corresponding values are in red. System Prompt for the ground-event tool Given the below event, identify the timestamps for the event in the video. You are given the snippet belonging to the period between <<<begin>>> and <<<end>>> (in HH:MM:SS format) of the original video. You should set the start and end timestamps in your answer accordingly to align it to the original video. If the event does not occur, set start and end to null. Event: <<<event>>> Output Format. You MUST follow this format and MUST be between the <json> and </json> tags: 20 < json > { \" name \" : \"the name of the event\", \" timestamps \" : { \" start \": \"start time\", #HH:MM:SS \" end \" : \"end time\" #HH:MM:SS } } </ json > Figure 10. System Prompt for the ground-event tool. Placeholder text to be replaced by the corresponding values are in red. System Prompt for the reasonable-tool (sreasonable-tool) step reward during RL Below is the reasoning trace for calling sequence of tools for finding the answer to the question: Question: {question} Reasoning Trace: {reasoning trace} Predicted Answer: {predicted answer} You MUST respond with the verdict as True if the reasoning trace makes sense for the question leading to the predicted answer or False if it doesnt. You MUST penalize repetitive tool calls if they are not needed. Answer in the following format: Reasoning : <Reasoning for the verdict> Verdict : <True/False> Figure 11. System Prompt for the reasonable-tool (sreasonable-tool) step reward during RL. Placeholder text to be replaced by the corresponding values are in red."
        },
        {
            "title": "Prompt for evaluating DIRECT baselines",
            "content": "You will be given question about video. You are provided frames from the video, sampled evenly across the video. Transcript: <<<asr transcript>>> Question: <<<question>>> Respond to the users question. Figure 12. Prompt for evaluating DIRECT baselines. Placeholder text to be replaced by the corresponding values are in red. 21 Figure 13. QnA Pairs Qualitative Samples. Given our system prompt, Gemini-2.5-Flash can generate high-quality QnA pairs of varying difficulty levels and types (open-ended and MCQ) that cover the entire video. 22 Figure 14. QnA Pairs Qualitative Samples. Given our system prompt, Gemini-2.5-Flash can generate high-quality QnA pairs of varying difficulty levels and types (open-ended and MCQ) that cover the entire video. 23 Figure 15. SAGE Reasoning Trajectory Qualitative Example. SAGE first coarsely locates the boundaries of the target segment given the sampled frames, followed by web search to answer the question due to lack of information in the video. The DIRECT Qwen3-VL8B-Instruct model (which has access to the full video transcript) predicts the following answer: The Pow Box is fictional device in the Game Over segment, likely humorous or exaggerated gadget that Mr. Bean desires, possibly for its comedic or absurd potential. The exact nature or function of the Pow Box isnt specified in the provided transcript, but its implied to be something desirable or coveted by Mr. Bean, fitting his characters love for quirky, nonsensical objects. 24 Figure 16. SAGE Reasoning Trajectory Qualitative Example. SAGE accurately transcribes only the target 2-minute segment to answer the users question. The DIRECT Qwen3-VL-8B-Instruct model (which has access to the full video transcript) predicts the following answer: (B) Thank you, and God bless America.. Figure 17. SAGE Reasoning Trajectory Qualitative Example. SAGE answers the question in single turn for the short video. The DIRECT Qwen3-VL-8B-Instruct model (which has access to the full video transcript) predicts the following answer: Kraft."
        }
    ],
    "affiliations": [
        "Allen AI",
        "SHI Labs @ Georgia Tech",
        "University of Washington"
    ]
}