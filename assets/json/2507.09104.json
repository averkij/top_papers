{
    "paper_title": "CompassJudger-2: Towards Generalist Judge Model via Verifiable Rewards",
    "authors": [
        "Taolin Zhang",
        "Maosong Cao",
        "Alexander Lam",
        "Songyang Zhang",
        "Kai Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recently, the role of LLM-as-judge in evaluating large language models has gained prominence. However, current judge models suffer from narrow specialization and limited robustness, undermining their capacity for comprehensive evaluations. In this work, we present CompassJudger-2, a novel generalist judge model that overcomes these limitations via a task-driven, multi-domain data curation strategy. Central to our approach is supervising judgment tasks with verifiable rewards, guiding intrinsic critical reasoning through rejection sampling to foster robust, generalizable judgment capabilities. We introduce a refined learning objective with margin policy gradient loss to enhance performance. Empirically, CompassJudger-2 achieves superior results across multiple judge and reward benchmarks, and our 7B model demonstrates competitive judgment accuracy with significantly larger models like DeepSeek-V3 and Qwen3-235B-A22B. Additionally, we propose JudgerBenchV2, a comprehensive benchmark evaluating cross-domain judgment accuracy and rank consistency to standardize judge model evaluation. These contributions advance robust, scalable LLM judgment and establish new performance and evaluation standards."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 1 ] . [ 1 4 0 1 9 0 . 7 0 5 2 : r CompassJudger-2: Towards Generalist Judge Model via Verifiable Rewards Taolin Zhang1,2,, Maosong Cao1,, Alexander Lam1, Songyang Zhang1,,, Kai Chen1, 1Shanghai AI Laboratory 2Tsinghua University Github:https://github.com/open-compass/CompassJudger"
        },
        {
            "title": "Abstract",
            "content": "Recently, the role of LLM-as-judge in evaluating large language models has gained prominence. However, current judge models suffer from narrow specialization and limited robustness, undermining their capacity for comprehensive evaluations. In this work, we present CompassJudger-2, novel generalist judge model that overcomes these limitations via task-driven, multi-domain data curation strategy. Central to our approach is supervising judgment tasks with verifiable rewards, guiding intrinsic critical reasoning through rejection sampling to foster robust, generalizable judgment capabilities. We introduce refined learning objective with margin policy gradient loss to enhance performance. Empirically, CompassJudger2 achieves superior results across multiple judge and reward benchmarks, and our 7B model demonstrates competitive judgment accuracy with significantly larger models like DeepSeek-V3 and Qwen3-235B-A22B. Additionally, we propose JudgerBenchV2, comprehensive benchmark evaluating cross-domain judgment accuracy and rank consistency to standardize judge model evaluation. These contributions advance robust, scalable LLM judgment and establish new performance and evaluation standards."
        },
        {
            "title": "Introduction",
            "content": "In recent years, large language models (LLMs) have advanced rapidly with the development of new foundation models such as DeepSeek-R1 [13], OpenAI-o1 [14], and the Qwen series [29]. Innovations in architecture and data scaling have enabled LLMs to achieve state-of-the-art performance across diverse tasks, including natural language understanding, code generation, creative writing, and complex reasoning [19, 15, 11, 16]. As LLMs are deployed in real-world applications, accurate evaluation of response quality has become increasingly critical. Rule-based benchmarks [24, 34, 15, 6, 8, 10] excel at evaluating standardized tasks but struggle with LLM output variability, often failing to handle edge cases due to reliance on complex regex designs. Model-based approaches like Reward Models and LLM-as-Judge [35, 4, 30, 32] reduce evaluation efforts by leveraging the reasoning ability of LLMs. However, these approaches introduce some new challenges in that restricted generalization ability of existing judge models confines them to specific prompts or datasets. Moreover, some inadequate world knowledge of these LLMs may lead to inaccurate judgments on knowledge-intensive queries, limiting their application for iterative model improvement. To address these limitations, we propose unified training paradigm for judge models. First, we define series of potential application scenarios for judge models and collect wide range of judge-related public datasets. Subsequently, we curate and synthesize data from different sources to obtain diverse 1This work is done when Taolin Zhang is on internship at Shanghai AI Laboratory, * means equal contribution, means corresponding author, means project lead. Preprint. Under review. training dataset. Second, we employ judgment-oriented chain-of-thought (CoT) data generation to improve judgment accuracy, combined with rejection sampling to select high-quality training examples. Finally, we introduce margin policy gradient loss with verifiable reward signals for better optimization. The resulting CompassJudger-2 series achieves superior performance on judge benchmarks, with our 7B model demonstrating competitive accuracy against significantly larger models like DeepSeek-V3-0324 [21] and Qwen3-235B-A22B [27]. To advance the evaluation of judge models, we also present JudgerBenchV2, standardized benchmark comprising 10,000 questions across 10 scenarios to evaluate judging capabilities. For the first time, it establishes category-specific judging standards and uses Mix-of-Judgers (MoJ) consensus as ground truth, paired with novel metrics that assess both sample-level accuracy and model-level rank consistency, providing more robust evaluation. To summarize, our contributions are as follows: We develop versatile, multi-styled judge data composition scheme with data curation and synthesis, enhancing CompassJudger-2s robustness and domain adaptability at the data level. We significantly improve judge performance of CompassJudger-2 by generating high-quality chain-of-thought judge data, selecting optimal training trajectories via rejection sampling, and applying policy gradient loss. We introduce JudgerBenchV2, which treats Mix-of-Judgers as ground truth and deploys new metrics that jointly assess accuracy and rank fidelity, enabling more reliable evaluation."
        },
        {
            "title": "2 Related Works",
            "content": "LLM Judgers as Generative Verifiers. LLM-as-judge represents novel approach where LLMs are fine-tuned to evaluate and provide judgment on model responses, offering not only reward but also an analysis of the reasoning behind the decision. Unlike traditional reward models that assign single reward value, LLMs can deliver more valuable feedback by explaining the logic and rationale of their judgments. However, many existing judge models [35, 18] are trained for specific prompts, show poor generalization and cannot adapt to the diverse model evaluation needs. Therefore, all-in-one generative models have emerged, with CompassJudger-1 [3] being the first to incorporate wide range of judge tasks into model training, greatly enhancing the generalization ability. Con-J [30] and RISE [32] have also conducted all-in-one Judge model training and achieved better Judge performance through the DPO strategy. Although these models have greatly ensured the generalization of prompts, they have not yet verified on other judge tasks such as critique generation and stylized judge. LLM Judging Evaluation. Despite the rapid evolution of judge models, there is notable lack of benchmarks for their evaluation. Rewardbench [16] focuses on assessing models reward capability across four categories: Chat, Chat Hard, Reasoning, and Safety. However, it faces issues with outdated data and limited number of evaluation scenarios, leading to overfitting in many models on Rewardbench. JudgeBench [26], by contrast, evaluates judge models based on their ability to determine the correctness of answers in datasets like MMLU-Pro [28] and LiveCodeBench [15], thus testing their knowledge base to answer factual questions. RMB [33] introduces method using the Best of (BoN) and involves comparative model making multiple judgments to assess the consistency of the models judging. Nonetheless, these benchmarks only offer limited view of the judging ability and do not encompass wide enough range of evaluation scenarios."
        },
        {
            "title": "3 Methodology",
            "content": "In this section, we first outline the training data pipeline for CompassJudger-2, covering data curation and data synthesis. We then explain how to apply rejection sampling and policy gradient optimization to incorporate verified rewards into the judge task. We apply the training data and the training strategy to the Qwen2.5-Instruct series of models, yielding our CompassJudger-2. 2 Figure 1: The data construction pipeline of CompassJudger-2, including data curation and synthesis. The Data Curation stage include reconstruction of public judge and reward data, while the data synthesis stage contains response generation over knowledge-based and chat based datasets. 3.1 Overall Data Pipeline Data Curation. We begin by collecting open-source judge-related datasets, including Public Judge Data and Public Reward Data. Public Judge Data contain critiques and explanations while Public Reward Data only contain ground truth labels. For Public Judge Data, we observe that many judgments were generated by outdated models such as ChatGPT, which may introduce misjudgments and implicit errors. To address this issue, we split the data into outdated and up-to-date subsets based on the cutoff date of October 2024. For outdated data, we use Qwen2.5-72B-Instruct to reconstruct outdated judgment and further verify correctness by comparing the predictions with human-labeled ground truth, ensuring that only accurate judgments are preserved. For up-to-date data, we leverage large number of subjective evaluation datasets available in the community, such as ArenaHard [19], WildBench [20], MTBench [1], etc., to collect their judgment prompt templates, which are then used to replace the original prompt templates in the existing judgment data, thereby enhancing their diversity. For Public Reward Data, such data lacks critique annotations, making it suboptimal for training generative judge models. To leverage these data effectively, we prompt Qwen2.5-72B-Instruct to generate multiple judgments for each data instance and further refine the quality through rejection sampling. detailed description of our construction pipeline is provided in Section 3.2. Data Synthesis. To enhance the robustness and versatility, we systematically design and synthesize data from Knowledge-based Datasets and Chat-based Datasets, aiming to enrich world knowledge and improve stylistic adaptability, respectively. For Knowledge-based Datasets, we aggregate model outputs from standardized benchmarks (e.g., MMLU[28], CMMLU [17], GSM8K [7]) and employ Qwen2.5-72B-Instruct to evaluate their correctness while providing detailed rationales. These judgments are subsequently validated against ground truth answers, with only verified correct evaluations retained in the training corpus. For Chat-based Datasets, we generate response pairs exhibiting contrasting characteristics and instruct Qwen2.5-72B to select the superior response according to specified style requirements, thereby creating style-sensitive judgment data. Overall Training Data Construction. Prior studies [3, 23] have also demonstrated that incorporating general instruction data helps maintain models generalization capability while preserving its judge performance. Therefore, we also include general instruction data collected from CompassJudger-1 in our training dataset. The final training data for CompassJudger-2 consists of four components: (1) publicly available judge data that undergo diversity enhancement and quality rectification, (2) publicly available reward data process through rejection sampling (RFT data), (3) synthetic data generated from knowledge-based and chat-based datasets, and (4) general instruction data (G-SFT data). 3. Incorporating Verified Reward To enhance the judge models accuracy and generalization, we propose training paradigm that integrates verifiable rewards through policy gradient optimization and rejection sampling. Specifically, we first guide the model to generate judgments through critical thinking, then reinforce the reward for final judgment outcomes using policy gradient loss, and further enhance judgment performance by incorporating rejection sampling strategy. We elaborate on these steps in detail below. 3 Figure 2: Illustration of the reasoning path in the judge task. The reasoning path involves critical analysis of the instruction and responses from various models. The final answer prediction can be treated as classification task, which is further guided by verified reward for supervision. Figure 3: Training framework of CompassJudger-2. CompassJudger-2 utilize rejection sampling to choose correct reasoning paths for SFT training and apply policy gradient loss over the answer logit to incorporate verifiable reward. Critical Thinking. The SFT training of judge models requires high-quality instruction-response data, which can be costly to obtain. To tackle this challenge, we introduce an innovative chain-of-thought methodology aimed at producing high-quality instruction-response data specifically for the judge task. Following the reasoning pipeline in DeepSeek-R1 [13], we craft critical thinking prompt specifically for judge models, as shown in Figure 2. We divide the judge task into several important steps and require the model in making predictions through comprehensive thinking. Formally, the model is required to dissect the problem by evaluating: (1) Users Demand: The model need to analysis the specific requirements of the users instruction. (2) Strengths of Model A/B. (3) Weaknesses of Model A/B. (4) Reasoning: Perform reasoning based on the aforementioned analysis. (5) Prediction: Output the final prediction. In practice, we employ Qwen2.5-72B-Instruct [29] as backbone for data synthesis. Judge Reward. In the judge task, the model performs binary classification by outputting its prediction at designated positions. This structured output enables us to utilize the ground truth labels as explicit guidance signals for optimization. Inspired by DeepSeek-R1 [13], given an instructionresponse pair (x, y), prediction position kx, and the corresponding ground truth label , we apply kx rule-based reward r(x, y) defined as 1 if the models prediction at position kx matches the ground truth label kx , and 0 otherwise. Policy Gradient Optimization. We formulate the learning objective as maximizing the expected reward over the response distribution and the gradient of this objective can be derived as follows: θJ(θ) = θ [ExDEyπθ [r(x, y)]] = ExDEyπθ [r(x, y)θ log πθ(yx)] (cid:34) = ExDEyπθ r(x, y) (cid:88) θ log πθ(ytx, y<t) (cid:35) (1) t=1 This decomposition shows how the gradient propagates through all sequence positions in autoregressive models. Given that reward function only depends on the prediction at position kx, the policy gradient loss can be further simplified as follows: LPG = xD yπθ (cid:20) (cid:12) (cid:12) log πθ(ykx x, y<kx ) (cid:12)ykx =y kx (cid:21) . (2) We observe that SFT loss computes the conditional probability under fixed prefixes, while the policy gradient loss approximates the marginal probability by aggregating over diverse prefix. This 4 Table 1: Total loss and mapping functions. We discuss three mapping functions to approximate in the total loss. Loss Total Loss DPO Loss Ltotal = 1 NM (cid:80)N i=1 (cid:80)M j=1[(cid:80) t=kij log πθ(y(i,j) x(j), y(i,j) <t ) + (cid:16) Loss Function LDPO = 1 NM (cid:80)N i=1 (cid:80)M j=1 log σ x(j), y(i,j) <kij (cid:17) ) ] log πθ(y(i,) kij (cid:19) (cid:18) kij β log x(j),y(i,j) <kij x(j),y(i,j) <kij πθ(y(i,) πθ(y(i,) kij exp(log πθ(y(i,) (cid:80) x(j),y(i,j) kij <kij exp(log πθ(yx(j),y(i,j) <kij ) ) )/τ ) )/τ ) Temperature Loss LTemp = 1 NM (cid:80)N i= (cid:80)M j=1 log Margin Loss LMargin = 1 NM (cid:80)N i= (cid:80)M j=1 max (cid:0)0, γ log πθ(y(i,) kij x(j), y(i,j) <kij ) + log πθ(y(i,) kij x(j), y(i,j) <kij )(cid:1) distinction arises because SFT employs teacher forcing with deterministic prefixes, whereas policy gradient optimization explores various response trajectories to maximize expected rewards. Rejection Sampling for RL Generalization. While policy gradient optimization directly maximizes expected rewards, it suffers from limited exploration during the standard SFT stage, where fixed prefixes constrain the diversity of generated responses. To address this exploration bottleneck, we leverage rejection sampling to enhance model generalization through diversified prefix generation. Our approach systematically generates and filters diverse response candidates based on quality metrics and reject the samples that do not match the ground truth label. Formally, for the ith instruction (i {1.., }) in the dataset, we generate response samples that satisfy the ground truth label y(i,) to approximate the policy gradient loss: (cid:88) (cid:88) LPG = 1 i=1 j=1 log πθ(y(i,) kx x(i), y(i,) <kx ) (3) Similarly, we apply the SFT loss to the sampled response candidates. We further combine the SFT loss and policy gradient loss: Ltotal =LSFT + LPG . (4) Mapping Function. The total loss can be decomposed with SFT loss over the prefix and mapping function over the prediction position, as shown in Table 1. We also design three different mapping loss function on the prediction position as for optimization over the ground truth answer y(i,) and kij the wrong answer y(i,) . kij DPO Loss w/o Reference Model encourages the model to increase the probability of true answer while decreasing the probability of wrong answer. Temperature Loss performs temperature scaling to the logits before softmax, effectively sharpening the probability distribution around the ground truth token with τ as the temperature. Margin Loss introduces margin γ between the ground truth token and other answer, ensuring that the ground truth probability is sufficiently higher."
        },
        {
            "title": "4 JudgerBenchV2: A More Robust Benchmark for Judge Models",
            "content": "Existing benchmarks for judge models have numerous limitations, such as insufficient coverage of judge scenarios and lack of sufficiently accurate ground truth (GT). To address these issues, we propose JudgerBenchV2, aiming to improve the evaluation landscape for judge models and provide more comprehensive and accurate benchmark. Data Construction. We first collect real-world user queries in Chinese and English through CompassArena [8], and cluster them via K-means. We then utilize an LLM to classifies each query by difficulty level and manually select 100 queries per scenario, ensuring balanced distribution of languages and difficulty level. Next, we select 10 high-performing models of comparable capability and generate their responses to these queries. We then use GPT-4o-mini as the policy model and pair it with each of the 10 models to form response pairs. judge model evaluates these pairs in pairwise manner to obtain judge results. By comparing with the GT, we derive the performance scores of the judge model. 5 Table 2: Main results on judge benchmarks. CompassJudger-2 achieves state-of-the-art performance on both 7B and 32B+ variants. Model JudgerBenchV JudgeBench RMB RewardBench Average Qwen2.5-7B-Instruct Llama3.1-8B-Instruct InternLM3-8B-Instruct Qwen2.5-32B-Instruct DeepSeek-V3-0324 Qwen3-235B-A22B InternLM2-20B-reward Deepseek-GRM-27B RM-R1-Qwen-Instruct-32B CompassJudger-1-7B-Instruct Con-J-7B-Instruct RISE-Judge-Qwen2.5-7B CompassJudger-2-7B-Instruct [29] [12] [2] [29] [21] [27] [2] [23] [5] [3] [31] [32] General Models 57.14 57.64 57.71 62.97 64.43 61.40 23.23 33.23 24.19 59.84 59.68 65.97 Reward Models - - - - - - 7B Judge Models 57.96 52.35 46.12 60.52 46.00 38.06 40.48 63.06 32B+ Jugde Models CompassJudger-1-32B-Instruct [3] Skywork-Critic-Llama-3.1-70B [25] RISE-Judge-Qwen2.5-32B [32] CompassJudger-2-32B-Instruct 60.33 52.41 56.42 62.21 62.29 50.65 63.87 65.48 69.03 66.01 72.02 74.99 78.16 75.59 62.90 69.00 73.00 38.18 71.50 72.64 73.90 77.63 65.50 73.70 72. 79.69 73.64 80.62 85.61 85.17 84.68 90.20 86.00 92.90 80.74 87.10 88.20 90.96 86.17 93.30 92.70 92.62 57.27 57.63 58.64 70.85 71.86 71.91 - - - 55.72 62.25 61.61 72.11 71.61 65.47 71.67 73.32 Table 3: Results on general benchmarks. CompassJudger-2 maintains strong performance on both objective and subjective datasets. Model MMLU Pro GPQA Diamond AIME2025 LiveCodeBench v5 IFEval ArenaHard [29] Qwen2.5-7B-Instruct Con-J-7B-Instruct [31] RISE-Judge-Qwen2.5-7B [32] CompassJudger-2-7B-Instruct [29] Qwen2.5-32B-Instruct RISE-Judge-Qwen2.5-32B [32] CompassJudger-2-32B-Instruct 55.43 44.74 51.56 52.55 68.92 67.88 69.22 7B Judge Models 34.85 27.27 32.32 39. 6.67 3.33 6.67 6.67 32B Judge Models 42.93 42.93 50.51 16.67 6.67 16.67 12.57 6.59 12.57 14.37 30.54 27.54 25. 73.20 54.90 44.18 74.49 79.85 62.85 79.48 47.86 23.49 35.99 53.49 70.16 61.52 83.31 Mixture of Judges. Evaluating open-ended questions is highly subjective since different individuals may produce varying judgments, and different models also exhibit judge biases. Relying solely on the judgments from single human or single model as GT thus risks introducing bias. To address this, we introduce the mixture of judgers (MoJ) strategy, leveraging the judgments of DeepSeek-R1, DeepSeek-v3-0324, and Qwen3-235B-A22B and their majority consensus is considered as GT. Robust Judge Performance Metrics. Traditional judge evaluation metrics primarily focus on sample-level accuracy and fail to capture essential dimensions like ranking consistency. For example, human raters often converge on overall model rankings although they may disagree on individual samples. comprehensive evaluation framework should therefore incorporate both fine-grained judgment accuracy and high-level ranking fidelity. In JudgerBenchV2, we conduct pairwise comparisons between candidate model and GPT-4o-mini to determine which delivers superior responses. Each comparison is evaluated by both ground truth judge model and test judge model. sample is considered correct if both judges agree on the better-performing model. For each sample, the model deemed superior earns score increment of 1. The total number of pairwise samples is denoted by and represents the number of samples where the GT and test judge models agree on the superior model. For set of candidate models, let the GT judge model and the test judge model generate score lists S1 = {s1,m}mM and S2 = {s2,m}mM , respectively, where si,m represents the cumulative score for model based on pairwise wins. Additionally, let R1 = {r1,m}mM and R2 = {r2,m}mM denote the rank lists, where ri,m is the rank of model according to judge i. The performance of the test judge model is evaluated using the following metric: 6 (a) Judge benchmarks. (b) Subjective and objective benchmarks. Figure 4: Data ablation results on different benchmarks. Table 4: Ablation results with policy gradient loss on CompassJudger-2-7B-Instruct. Margin loss provides significant boost compared to other forms of loss. Loss JudgerBenchV2 JudgeBench RMB RewardBench Average Baseline DPO Temperature Margin 60.20 60.56 59.43 60.52 61.77 61.13 62.42 63. 68.13 66.35 67.77 73.90 89.50 90.07 90.25 90.96 69.90 69.53 69.97 72.11 = 100 100 (cid:88) ( mM (cid:124)(cid:123)(cid:122)(cid:125) Sample-level accuracy r1,m r2,m 1 (cid:123)(cid:122) Normalized rank difference (cid:125) (cid:124) + s1,m s2,m maxmM s1,m s2,m (cid:125) (cid:123)(cid:122) (cid:124) Normalized score difference ). (5) The first term captures the sample-level accuracy by measuring the agreements between the judges. The second term penalizes discrepancies in rankings and scores, with normalization to ensure equitable comparisons across different models."
        },
        {
            "title": "5 Experiments",
            "content": "5.1 Experimental Setup Evaluation Datasets. We evaluate the performance of CompassJudger-2 on leading judge benchmarks, including RewardBench [16], JudgeBench [26], RMB [33], and our JudgerBenchV2. Besides, we compare our method with other judge models over popular objective and subjective benchmarks, including MMLU Pro [28], GPQA Diamond [24], AIME2025, LiveCodeBench v5 [15], IFEval [9] and ArenaHard [19]. We further conduct extensive experiments on AlignBench [22] and AlpacaEval [11], showcasing the critique ability of CompassJudger-2 for model improvement. Training Settings. In practice we generate 8 candidate responses for filtering during rejection sampling. For model training, we utilize Qwen-2.5 series as the checkpoint and adopt 6e-5 as the learning rate. For policy gradient loss parameter, we set β = 0.1 in DPO loss, τ = 5 in temperature loss and γ = 10 in margin loss. We apply DPO loss on only the candidate answer and margin loss on the top 10 logits. We train the model for 1 epoch with batch size equal to 512. 5.2 Main Results Judge Ability Analysis. To verify the judge ability of our method, we conduct evaluation across multiple benchmarks and compare our method with general models, reward models and specialized judge models including the Skywork [25] and RISE [32] series. As presented in Table 2, CompassJudger-2 consistently surpasses all baselines in average performance, demonstrating significant advancements in the generalization ability. Notably, CompassJudger-2-7B-Instruct outperforms RISE-Judge-Qwen2.5-7B by 22.58% on JudgeBench and by 10.5% on average. Compared to the Table 5: Model improvement with generated critique on chat-based datasets. AlignBench scores range from 0 to 10 and other datasets score range from 0-100. To standardize the scale, we normalize all the scores to 0100 range and then compute the average. Model AlignBench AlpacaEval ArenaHard Average Policy Model: LLama3.1-8B-Instruct Base RISE-Judge-Qwen2.5-7B CompassJudger-2-7B-Instruct 4.90 4.99 5. 27.95 28.03 30.68 Policy Model: Qwen2.5-7B-Instruct Base RISE-Judge-Qwen2.5-7B CompassJudger-2-7B-Instruct 6.65 6.43 6.76 36.65 35.12 38.14 Policy Model: InternLM3-8B-Instruct Base RISE-Judge-Qwen2.5-7B CompassJudger-2-7B-Instruct 6.46 6.47 6.50 64.84 62.17 65.85 29.11 28.64 32.76 47.86 45.07 51.15 46.27 43.89 47. 35.35 35.52 38.48 50.34 48.16 52.30 58.57 56.92 59.54 Figure 5: Comparison results over style judge of CompassJudger-2 and RISE. CompassJudger-1 series, CompassJudger-2 enhances judge performance by 16.39% for the 7B model and 1.71% for the 32B model, on average. General Ability Analysis. We further highlight the improvements in general capabilities of CompassJudger-2 compared to other judge models across objective and subjective benchmarks, as shown in Table 3. CompassJudger-2 achieves markedly superior performance over other judge models on both objective and subjective datasets, demonstrating its generalization ability. Remarkably, CompassJudger-2 surpasses general models like Qwen2.5-7B-Instruct and Qwen2.5-32B-Instruct on specific datasets, revealing strong correlation between judge ability and general ability in LLMs and their potential to enhance each other. 5.3 Ablation Study Policy Gradient Loss. To evaluate the impact of incorporating policy gradient loss, we conduct thorough ablation study to determine the most effective type of policy gradient loss for improving model performance. We compare baseline model without policy gradient loss to models using various policy gradient losses, including DPO, Temperature, and Margin loss on 7B level, as presented in Table 4. Our findings reveal that the verified reward serves as critical supervised signal, significantly enhancing the models performance in judge tasks. All models with policy gradient loss surpass the baseline model on RewardBench, achieving performance improvements ranging from 0.5% to 1.4%. Notably, the model with margin loss demonstrate the best generalization across 3 out of 4 datasets, delivering an 2.21% performance on average boost compared to the baseline model. As result, we select margin loss as the default choice for our study. 8 Data Ablation. To investigate how general instruction data (G-SFT Data) and rejection sampling (RFT Data) impact judge ability and general ability, we perform ablation studies by separately removing each data type from the training set. As illustrated in Figure 4, the results highlight several key findings. Removing RFT data causes significant decline in judge performance, mainly due to lower judge consistency and result in poor results on the RMB dataset. In addition, including RFT data enhances performance across specific datasets, such as GPQA-Diamond and ArenaHard, underscoring its role in boosting general ability. In contrast, General SFT data primarily maintain the general ability of the model, with minimal impact on judge ability. 5.4 Discussions Critique Ability for Model Improvement. An effective all-in-one judge model should be capable to produce high-quality critiques that offer insightful analysis and explanations. To evaluate the critique ability of CompassJudger-2, we task it with generating analyses of responses from various policy models on subjective datasets. We then permit the policy models to revise their initial responses based on these critiques. For comparison, we present the initial scores of the policy models (Base) alongside the results of using RISE-Judge-Qwen2.5-7B as the critique model, as shown in Table 5. The results reveal striking insight that all policy models improve when guided by critiques from CompassJudger2, whereas low-quality critiques from RISE-Judge-Qwen2.5-7B often result in performance drop. This suggest the superior critique quality of CompassJudger-2 and highlights its potential to enhance training performance during model iterations. We also provide some case study for comparison in the Appendix. Style Judge. An effective all-in-one judge model should also maintain consistent performance with various prompts. Therefore, we conduct style judge experiment with modifying judging prompts by adding following sentences: \"Beyond this, users prefer more detailed response; therefore, you need to determine which models answer provides more comprehensive and useful information when both responses are correct and have completed the users request\". We present the results on different subset of RewardBench. As can be seen from the results in Figure 5, RISE-32B suffers from significant performance drop by 10.67% in the Chat Hard subset. Compared with RISE, CompassJudger-2 are less sensitive of judging prompts and show better consistency and generalization ability, indicating the superiority of our method."
        },
        {
            "title": "6 Conclusions",
            "content": "In this work, we present CompassJudger-2, an series of all-in-one judge models that advance LLM-as-judge performance through unified training paradigm combining diverse task-driven data composition, high-quality chain-of-thought supervision, and verifiable reward-guided optimization. Furthermore, we introduce JudgerBenchV2, comprehensive benchmark with mixed-of-judgers and novel ranking-aware metrics, to enable more nuanced and reliable evaluation of judge models. Looking forward, CompassJudger-2 paves the way for more adaptable, interpretable, and efficient judge services in real-world LLM deployments, and we anticipate that extending this work to multi-modal and interactive evaluation scenarios will further enhance its applicability and impact."
        },
        {
            "title": "7 Limitations",
            "content": "Despite the superior performance, there are still some limitations of CompassJudger-2. Rejection sampling incurs relatively higher inference costs, and the hallucinations produced by the LLM when synthesizing data may pose potential risks. These issues need to be further addressed, which will in turn enhance the performance of the judge models."
        },
        {
            "title": "References",
            "content": "[1] Ge Bai, Jie Liu, Xingyuan Bu, Yancheng He, Jiaheng Liu, Zhanhui Zhou, Zhuoran Lin, Wenbo Su, Tiezheng Ge, Bo Zheng, et al. Mt-bench-101: fine-grained benchmark for evaluating large language models in multi-turn dialogues. arXiv preprint arXiv:2402.14762, 2024. [2] Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu Chen, Xin Chen, Xun Chen, Zehui Chen, Zhi Chen, Pei Chu, Xiaoyi Dong, Haodong Duan, Qi Fan, Zhaoye Fei, Yang Gao, Jiaye 9 Ge, Chenya Gu, Yuzhe Gu, Tao Gui, Aijia Guo, Qipeng Guo, Conghui He, Yingfan Hu, Ting Huang, Tao Jiang, Penglong Jiao, Zhenjiang Jin, Zhikai Lei, Jiaxing Li, Jingwen Li, Linyang Li, Shuaibin Li, Wei Li, Yining Li, Hongwei Liu, Jiangning Liu, Jiawei Hong, Kaiwen Liu, Kuikun Liu, Xiaoran Liu, Chengqi Lv, Haijun Lv, Kai Lv, Li Ma, Runyuan Ma, Zerun Ma, Wenchang Ning, Linke Ouyang, Jiantao Qiu, Yuan Qu, Fukai Shang, Yunfan Shao, Demin Song, Zifan Song, Zhihao Sui, Peng Sun, Yu Sun, Huanze Tang, Bin Wang, Guoteng Wang, Jiaqi Wang, Jiayu Wang, Rui Wang, Yudong Wang, Ziyi Wang, Xingjian Wei, Qizhen Weng, Fan Wu, Yingtong Xiong, Chao Xu, Ruiliang Xu, Hang Yan, Yirong Yan, Xiaogui Yang, Haochen Ye, Huaiyuan Ying, Jia Yu, Jing Yu, Yuhang Zang, Chuyu Zhang, Li Zhang, Pan Zhang, Peng Zhang, Ruijie Zhang, Shuo Zhang, Songyang Zhang, Wenjian Zhang, Wenwei Zhang, Xingcheng Zhang, Xinyue Zhang, Hui Zhao, Qian Zhao, Xiaomeng Zhao, Fengzhe Zhou, Zaida Zhou, Jingming Zhuo, Yicheng Zou, Xipeng Qiu, Yu Qiao, and Dahua Lin. Internlm2 technical report, 2024. [3] Maosong Cao, Alexander Lam, Haodong Duan, Hongwei Liu, Songyang Zhang, and Kai Chen. Compassjudger-1: All-in-one judge model helps model evaluation and evolution. arXiv preprint arXiv:2410.16256, 2024. [4] Ding Chen, Qingchen Yu, Pengyuan Wang, Wentao Zhang, Bo Tang, Feiyu Xiong, Xinchi Li, Minchuan Yang, and Zhiyu Li. xverify: Efficient answer verifier for reasoning model evaluations. arXiv preprint arXiv:2504.10481, 2025. [5] Xiusi Chen, Gaotang Li, Ziqi Wang, Bowen Jin, Cheng Qian, Yu Wang, Hongru Wang, Yu Zhang, Denghui Zhang, Tong Zhang, et al. Rm-r1: Reward modeling as reasoning. arXiv preprint arXiv:2505.02387, 2025. [6] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions, 2019. URL https://arxiv.org/abs/1905.10044. [7] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. [8] OpenCompass Contributors. Opencompass: universal evaluation platform for foundation models. https://github.com/open-compass/opencompass, 2023. [9] Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu, Zhiyuan Liu, Maosong Sun, and Bowen Zhou. Enhancing chat language models by scaling high-quality instructional conversations. arXiv preprint arXiv:2305.14233, 2023. [10] Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. Drop: reading comprehension benchmark requiring discrete reasoning over paragraphs, 2019. URL https://arxiv.org/abs/1903.00161. [11] Yann Dubois, Balázs Galambosi, Percy Liang, and Tatsunori Hashimoto. Length-controlled alpacaeval: simple way to debias automatic evaluators. arXiv preprint arXiv:2404.04475, 2024. [12] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [13] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [14] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. [15] Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. arXiv preprint arXiv:2403.07974, 2024. 10 [16] Nathan Lambert, Valentina Pyatkin, Jacob Morrison, LJ Miranda, Bill Yuchen Lin, Khyathi Chandu, Nouha Dziri, Sachin Kumar, Tom Zick, Yejin Choi, et al. Rewardbench: Evaluating reward models for language modeling. arXiv preprint arXiv:2403.13787, 2024. [17] Haonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, Hai Zhao, Yeyun Gong, Nan Duan, and Timothy Baldwin. Cmmlu: Measuring massive multitask language understanding in chinese. arXiv preprint arXiv:2306.09212, 2023. [18] Junlong Li, Shichao Sun, Weizhe Yuan, Run-Ze Fan, Hai Zhao, and Pengfei Liu. Generative judge for evaluating alignment. arXiv preprint arXiv:2310.05470, 2023. [19] Tianle Li, Wei-Lin Chiang, Evan Frick, Lisa Dunlap, Tianhao Wu, Banghua Zhu, Joseph Gonzalez, and Ion Stoica. From crowdsourced data to high-quality benchmarks: Arena-hard and benchbuilder pipeline. arXiv preprint arXiv:2406.11939, 2024. [20] Bill Yuchen Lin, Yuntian Deng, Khyathi Chandu, Faeze Brahman, Abhilasha Ravichander, Valentina Pyatkin, Nouha Dziri, Ronan Le Bras, and Yejin Choi. Wildbench: Benchmarking llms with challenging tasks from real users in the wild. arXiv preprint arXiv:2406.04770, 2024. [21] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. [22] Xiao Liu, Xuanyu Lei, Shengyuan Wang, Yue Huang, Zhuoer Feng, Bosi Wen, Jiale Cheng, Pei Ke, Yifan Xu, Weng Lam Tam, et al. Alignbench: Benchmarking chinese alignment of large language models. arXiv preprint arXiv:2311.18743, 2023. [23] Zijun Liu, Peiyi Wang, Runxin Xu, Shirong Ma, Chong Ruan, Peng Li, Yang Liu, and Yu Wu. Inference-time scaling for generalist reward modeling. arXiv preprint arXiv:2504.02495, 2025. [24] David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel Bowman. Gpqa: graduate-level google-proof q&a benchmark. In First Conference on Language Modeling, 2024. [25] Tu Shiwen, Zhao Liang, Chris Yuhao Liu, Liang Zeng, and Yang Liu. Skywork critic model series. https://huggingface.co/Skywork, September 2024. URL https://huggingface. co/Skywork. [26] Sijun Tan, Siyuan Zhuang, Kyle Montgomery, William Tang, Alejandro Cuadron, Chenguang Wang, Raluca Ada Popa, and Ion Stoica. Judgebench: benchmark for evaluating llm-based judges. arXiv preprint arXiv:2410.12784, 2024. [27] Qwen team. Qwen3: Think deeper, act faster. https://qwenlm.github.io/blog/qwen3/, 2025. [28] Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, et al. Mmlu-pro: more robust and challenging multi-task language understanding benchmark. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024. [29] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. [30] Ziyi Ye, Xiangsheng Li, Qiuchi Li, Qingyao Ai, Yujia Zhou, Wei Shen, Dong Yan, and Yiqun Liu. Beyond scalar reward model: Learning generative judge from preference data, 2024. URL https://arxiv.org/abs/2410.03742. [31] Ziyi Ye, Xiangsheng Li, Qiuchi Li, Qingyao Ai, Yujia Zhou, Wei Shen, Dong Yan, and Yiqun Liu. Learning llm-as-a-judge for preference alignment. In The Thirteenth International Conference on Learning Representations, 2025. [32] Jiachen Yu, Shaoning Sun, Xiaohui Hu, Jiaxu Yan, Kaidong Yu, and Xuelong Li. Improve llm-as-a-judge ability as general ability. arXiv preprint arXiv:2502.11689, 2025. 11 [33] Enyu Zhou, Guodong Zheng, Binghai Wang, Zhiheng Xi, Shihan Dou, Rong Bao, Wei Shen, Limao Xiong, Jessica Fan, Yurong Mou, et al. Rmb: Comprehensively benchmarking reward models in llm alignment. arXiv preprint arXiv:2410.09893, 2024. [34] Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou. Instruction-following evaluation for large language models. arXiv preprint arXiv:2311.07911, 2023. [35] Lianghui Zhu, Xinggang Wang, and Xinlong Wang. Judgelm: Fine-tuned large language models are scalable judges. arXiv preprint arXiv:2310.17631, 2023."
        },
        {
            "title": "A Deriving the Loss Function",
            "content": "Judge Reward. In the judge task, given instruction-response pair (x, y), prediction position kx and ground truth label k, we apply rule-based reward defined as: r(x, y) = (cid:26)1 0 if ykx = kx otherwise . (6) Policy Gradient Optimization. To optimize the judge models performance, we formulate the learning objective as maximizing the expected reward over the response distribution: J(θ) = ExDEyπθ [r(x, y)] The gradient of this objective can be derived using the policy gradient theorem: θJ(θ) = ExDEyπθ [r(x, y)θ log πθ(yx)] (cid:34) (cid:35) = ExDEyπθ r(x, y) θ log πθ(ytx, y<t) (cid:88) (7) (8) t=1 This decomposition shows how the gradient propagates through all sequence positions in autoregressive models. The corresponding policy gradient loss is: LPG = ExDEyπθ r(x, y) (cid:34) (cid:88) t=1 (cid:35) log πθ(ytx, y<t) (9) Given our binary reward function that only depends on the prediction at position kx, we can simplify: LPG = ExDEyπθ [r(x, y) log πθ(ykx x, y<kx )] = ExDEyπθ = ExDEyπθ (cid:2)I(ykx = (cid:20) kx log πθ(ykx x, y<kx ) ) log πθ(ykx x, y<kx)(cid:3) (cid:21) (cid:12) (cid:12) (cid:12)ykx =y kx (10) Rejection Sampling for RL Generalization. We further apply rejection sampling to approximate the policy gradient loss. Formally, for the ith instruction (i {1.., }) in the dataset, we generate response samples that satisfy the ground truth label y(i,) and obtain the following loss: LPG = ExDEyπθ (cid:20) (cid:12) (cid:12) log πθ(ykx x, y<kx ) (cid:12)ykx =y kx (cid:21) = 1 (cid:88) i=1 1 (cid:88) j=1 (cid:20) log πθ(y(i,j) kx x(i), y(i,j) <kx ) (cid:21) (cid:12) (cid:12) (cid:12)y(i,j) kx =y(i,) kx (11) = 1 (cid:88) (cid:88) i=1 j= log πθ(y(i,) kx x(i), y(i,) <kx ) Similarly, we apply the SFT loss to the sampled response candidates. To balance the standard sequence modeling objective with reward optimization, we combine the SFT loss and policy gradient loss through mapping function and derive another mapping function g: = LSFT + LPG(f ) ="
        },
        {
            "title": "1\nN M",
            "content": "N (cid:88) (cid:88) (cid:88) i=1 j=1 t=kij log πθ(y(i,j) x(j), y(i,j) <t )"
        },
        {
            "title": "1\nN M",
            "content": "="
        },
        {
            "title": "1\nN M",
            "content": "N (cid:88) (cid:88) log πθ(y(i,) kij x(j), y(i,j) <kij ) + (cid:16) log πθ(y(i,) kij x(j), y(i,j) <kij ) (cid:17) , (12) i=1 j=1 (cid:88) (cid:88) (cid:88) i=1 j=1 t=kij log πθ(y(i,j) x(j), y(i,j) <t ) + (cid:16) log πθ(y(i,) kij x(j), y(i,j) <kij (cid:17) ) where is composite function that combines the original mapping function with the log probability term to provide more flexible optimization objective. In our method, the mapping function is approximate by DPO loss, Temperture Loss and Margin Loss."
        },
        {
            "title": "B Detailed Results on the Judge Benchmarks",
            "content": "We list the detailed results of judge models on the Judge Benchmarks in Table 6, 7 and 8. Table 6: Detailed results on JudgerBenchV2 benchmarks. Model Accuracy Normalized Diff Rank Diff Score Diff Final Score 7B Judge Models CompassJudger-1-7B-Instruct [3] Con-J-Qwen2-7B [31] RISE-Judge-Qwen2.5-7B [32] CompassJudger-2-7B-Instruct 77.41 71.30 70.08 78.04 61.48 66.61 77.85 57.00 32B+ Judge Models CompassJudger-1-32B-Instruct [3] Skywork-Critic-Llama-3.1-70B [25] RISE-Judge-Qwen2.5-32B [32] CompassJudger-2-32B-Instruct 80.99 70.27 74.00 80. 60.32 65.44 61.15 56.47 11.40 17.60 14.00 10.80 11.40 15.20 10.60 8.60 83.40 85.20 202.50 76.90 62.90 97.30 88.80 64.10 57.96 52.35 46.12 60. 60.33 52.41 54.42 62.21 Table 7: Detailed results on RMB benchmarks. Model Pair Accuracy BoN Accuracy Final Score 7B Judge Models CompassJudger-1-7B-Instruct [3] Con-J-Qwen2-7B [31] RISE-Judge-Qwen2.5-7B [32] CompassJudger-2-7B-Instruct 47.40 84.80 78.79 80.58 32B+ Judge Models CompassJudger-1-32B-Instruct [3] Skywork-Critic-Llama-3.1-70B [25] RISE-Judge-Qwen2.5-32B [32] CompassJudger-2-32B-Instruct 82.73 68.35 79.99 79.61 28.96 74.20 66.50 67.23 72.53 62.50 67.42 66. 38.18 79.50 72.64 73.90 77.63 65.50 73.70 72.98 14 Table 8: Detailed results on RewardBench benchmarks. Model Chat Chat Hard Safety Reasoning Final Score 7B Judge Models CompassJudger-1-7B-Instruct [3] Con-J-Qwen2-7B [31] RISE-Judge-Qwen2.5-7B [32] CompassJudger-2-7B-Instruct 97.80 91.90 92.20 92.36 61.00 80.30 76.50 85.99 32B+ Judge Models CompassJudger-1-32B-Instruct [3] Skywork-Critic-Llama-3.1-70B [25] RISE-Judge-Qwen2.5-32B [32] CompassJudger-2-32B-Instruct 98.00 96.60 96.60 93.37 65.10 87.90 83.30 88.58 84.50 88.20 88.00 91.08 85.30 93.10 91.90 90.68 89.50 88.10 96.10 94. 92.40 95.50 98.80 97.00 83.20 87.10 88.20 90.96 85.20 93.30 92.70 92.40 CoT Synthesizing Prompt Now we are reviewing users interaction with two models. Your task is to evaluate the responses from Model and Model by carefully analyzing the dialogue step by step, following clear and structured thought process: 1. Users Demand: - Carefully analyze the users request. What is the user specifically asking for? What are the key aspects of the request that need to be fulfilled? Identify any constraints (e.g., time, format, quantity) the user has provided. 2. Strengths of Model A: - Identify the strengths of Model As response. Consider how well it addresses the users demand, meets the users constraints, and how well it serves the intended purpose. 3. Weaknesses of Model A: - Identify the weaknesses of Model As response. What aspects of the response fail to meet the users request or constraints? What could have been improved? 4. Strengths of Model B: - Identify the strengths of Model Bs response. Consider how well it addresses the users demand, meets the users constraints, and how well it serves the intended purpose. 5. Weaknesses of Model B: - Identify the weaknesses of Model Bs response. What aspects of the response fail to meet the users request or constraints? What could have been improved? 6. Reasoning: - Based on your analysis of both responses, explain which model better addresses the users needs. Discuss which models response is more suitable given the users request and constraints. 7. Choice: - Conclude with choice between Model and Model based on your reasoning. Indicate which model provides the more appropriate and useful response for the users request. Your final reply must be structured in the following format: { \"Users Demand\": \"[The users request or need]\", \"Strengths of Model A\": \"[Summary of the strengths of Model A]\", \"Weaknesses of Model A\": \"[Summary of the weaknesses of Model A]\", \"Strengths of Model B\": \"[Summary of the strengths of Model B]\", \"Weaknesses of Model B\": \"[Summary of the weaknesses of Model B]\", \"Reasoning\": \"[Explanation of which model is more suitable for the users demand]\", \"Choice\": \"[Model or Model B]\" } Critique Example-Prompt Below is users question and models response. Please help identify the strengths and weaknesses of the models reply so that it can better address the users question. The users question is as follows: Write poem about winter, requiring each sentence to be exactly five words long, and without using the words \"winter\" or \"snow,\" yet still evoking sense of the season. The models response is as follows: Frost paints silent trees. Chill lingers in the air. Footsteps fade on paths. Windows gleam with light. Hushed, the world awaits. Now, please provide your feedback, highlighting the strengths and areas for improvement in the models response. Critique Example-RISE-Judge-Qwen2.5-7B Answer Strengths of the Models Response: 1. Conciseness and Adherence to Word Count: Each sentence is exactly five words long, as requested by the user. 2. Evocation of Winter: The response effectively evokes the sense of winter without explicitly using the words \"winter\" or \"snow.\" Phrases like \"Frost paints silent trees\" and \"Chill lingers in the air\" convey the cold and serene aspects of winter. 3. Imagery and Atmosphere: The response uses vivid imagery to create strong sense of atmosphere. Descriptions such as \"Footsteps fade on paths\" and \"Windows gleam with light\" paint clear picture of winter scene. 4. Coherence and Flow: The sentences flow well together, creating coherent and engaging poem. Areas for Improvement: 1.Inclusion of \"Snow\": While the user specifically asked to avoid the word \"snow,\" it is common and essential element of winter that could be subtly included to enhance the evocation of the season. For example, \"Frost paints silent trees\" could be modified to \"Snow blankets silent trees\" to strengthen the winter theme. 2. Variety in Sentence Structure: The response has consistent structure with each sentence being simple statement. Introducing bit of variation in sentence structure could add to the poetic quality, such as using question or exclamation to create different tone or emphasis. 3. Enhancing Emotional Depth: While the response is atmospheric, adding bit more emotional depth or personal touch could make the poem more engaging. For example, describing the \"hushed, waiting world\" as \"yearning for spring\" could add layer of emotional resonance. Overall, the models response is strong and meets most of the users requirements effectively. However, subtle adjustments could enhance its impact and completeness. 16 Critique Example-CompassJudger-1-7B-Instruct Answer Strengths: 1. The model adheres to the instruction of creating poem where each sentence is exactly five words long. 2. The poem effectively evokes the essence of winter without explicitly using the words \"winter\" or \"snow.\" 3. The language used is descriptive and creates vivid image of the season, such as \"Frost paints silent trees\" and \"Windows gleam with light.\" 4. There is coherent flow and structure to the poem, with each line contributing to unified theme of winter. Areas for Improvement: 1. While the poem avoids the word \"snow,\" which was part of the instruction, the absence of any snow-related imagery or metaphor might have strengthened the winter theme further. 2. The phrase \"Hushed, the world awaits\" could be interpreted as generic description of anticipation and not necessarily tied to the cold or silence of winter, which might make the winter theme slightly less explicit. 3. To further enhance the specificity of the season, incorporating additional sensory details or specific winter activities (like ice skating or building snowmen) could make the poem more evocative and engaging. Overall, the models response is well-crafted poem that meets most of the criteria set by the users request. With few minor adjustments, it could become even more effective in capturing the spirit of winter. Critique Example-RISE-Judge-Qwen2.5-7B Answer Strengths of the Models Response: 1. Conciseness and Adherence to Word Count: Each sentence is exactly five words long, as requested by the user. 2. Evocation of Winter: The response effectively evokes the sense of winter without explicitly using the words \"winter\" or \"snow.\" Phrases like \"Frost paints silent trees\" and \"Chill lingers in the air\" convey the cold and serene aspects of winter. 3. Imagery and Atmosphere: The response uses vivid imagery to create strong sense of atmosphere. Descriptions such as \"Footsteps fade on paths\" and \"Windows gleam with light\" paint clear picture of winter scene. 4. Coherence and Flow: The sentences flow well together, creating coherent and engaging poem. Areas for Improvement: 1.Inclusion of \"Snow\": While the user specifically asked to avoid the word \"snow,\" it is common and essential element of winter that could be subtly included to enhance the evocation of the season. For example, \"Frost paints silent trees\" could be modified to \"Snow blankets silent trees\" to strengthen the winter theme. 2. Variety in Sentence Structure: The response has consistent structure with each sentence being simple statement. Introducing bit of variation in sentence structure could add to the poetic quality, such as using question or exclamation to create different tone or emphasis. 3. Enhancing Emotional Depth: While the response is atmospheric, adding bit more emotional depth or personal touch could make the poem more engaging. For example, describing the \"hushed, waiting world\" as \"yearning for spring\" could add layer of emotional resonance. Overall, the models response is strong and meets most of the users requirements effectively. However, subtle adjustments could enhance its impact and completeness. 17 Critique Example-CompassJudger-2-7B-Instruct Answer Strengths: 1. Adherence to Constraints: The model successfully adheres to the requirement of each sentence being exactly five words long and avoids using the words \"winter\" or \"snow.\" 2. Imagery and Atmosphere: The poem effectively evokes sense of the season through vivid imagery and subtle hints. For example, \"Frost paints silent trees\" and \"Chill lingers in the air\" convey the cold and stillness characteristic of winter. 3. Consistency in Tone: The tone remains consistent throughout the poem, maintaining sense of hushed and serene atmosphere that aligns with the winter season. Areas for Improvement: 1. Variety in Imagery: While the imagery is strong, it could benefit from bit more variety. For example, including different sensory details (e.g., sounds, smells) could make the poem more immersive. 2. Emotional Depth: Adding slight emotional undertone could enhance the poem. For instance, line that hints at nostalgia or anticipation could deepen the readers connection to the season. 3. Rhythm and Flow: The rhythm is good, but slightly varying the structure or adding rhyming scheme could make the poem more engaging and memorable."
        }
    ],
    "affiliations": [
        "Shanghai AI Laboratory",
        "Tsinghua University"
    ]
}