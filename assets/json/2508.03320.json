{
    "paper_title": "Skywork UniPic: Unified Autoregressive Modeling for Visual Understanding and Generation",
    "authors": [
        "Peiyu Wang",
        "Yi Peng",
        "Yimeng Gan",
        "Liang Hu",
        "Tianyidan Xie",
        "Xiaokun Wang",
        "Yichen Wei",
        "Chuanxin Tang",
        "Bo Zhu",
        "Changshi Li",
        "Hongyang Wei",
        "Eric Li",
        "Xuchen Song",
        "Yang Liu",
        "Yahui Zhou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce Skywork UniPic, a 1.5 billion-parameter autoregressive model that unifies image understanding, text-to-image generation, and image editing within a single architecture-eliminating the need for task-specific adapters or inter-module connectors-and demonstrate that compact multimodal systems can achieve state-of-the-art performance on commodity hardware. Skywork UniPic achieves a GenEval score of 0.86, surpassing most existing unified models; sets a new DPG-Bench complex-generation record of 85.5; attains 5.83 on GEditBench-EN and 3.49 on ImgEdit-Bench for image editing; and generates 1024 x 1024 images with under 15 GB of GPU memory (e.g., RTX 4090). (1) a decoupled encoding strategy that leverages a masked autoregressive encoder for synthesis and a SigLIP2 encoder for understanding, all feeding a shared autoregressive decoder; (2) a progressive, resolution-aware training schedule scaling from 256 x 256 to 1024 x 1024 while dynamically unfreezing parameters to balance capacity and stability; and (3) meticulously curated, 100 million-scale datasets augmented with task-specific reward models to refine generation and editing objectives. By demonstrating that high-fidelity multimodal integration need not incur prohibitive resource demands, Skywork UniPic establishes a practical paradigm for deployable, high-fidelity multimodal AI. Code and weights are publicly available at https://huggingface.co/Skywork/Skywork-UniPic-1.5B."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 ] . [ 1 0 2 3 3 0 . 8 0 5 2 : r Skywork UniPic: Unified Autoregressive Modeling for Visual Understanding and Generation Multimodality Team, Skywork AI multimodal@skywork.ai https://github.com/SkyworkAI/UniPic https://huggingface.co/Skywork/Skywork-UniPic-1.5B"
        },
        {
            "title": "Abstract",
            "content": "We introduce Skywork UniPic, 1.5 billion-parameter autoregressive model that unifies image understanding, text-to-image generation, and image editing within single architectureeliminating the need for task-specific adapters or inter-module connectorsand demonstrate that compact multimodal systems can achieve stateof-the-art performance on commodity hardware. Skywork UniPic achieves GenEval score of 0.86, surpassing most existing unified models; sets new DPGBench complex-generation record of 85.5; attains 5.83 on GEditBench-EN and 3.49 on ImgEdit-Bench for image editing; and generates 1024 1024 images with under 15 GB of GPU memory (e.g., RTX 4090). (1) decoupled encoding strategy that leverages masked autoregressive encoder for synthesis and SigLIP2 encoder for understanding, all feeding shared autoregressive decoder; (2) progressive, resolution-aware training schedule scaling from 256 256 to 1024 1024 while dynamically unfreezing parameters to balance capacity and stability; and (3) meticulously curated, 100 million-scale datasets augmented with task-specific reward models to refine generation and editing objectives. By demonstrating that high-fidelity multimodal integration need not incur prohibitive resource demands, Skywork UniPic establishes practical paradigm for deployable, high-fidelity multimodal AI. Code and weights are publicly available at https://huggingface.co/Skywork/Skywork-UniPic-1.5B."
        },
        {
            "title": "Introduction",
            "content": "The rapid evolution of multimodal artificial intelligence has ushered in paradigm shift toward unified models capable of seamlessly integrating visual perception, generation, and manipulation within single architectural framework. Recent demonstrations like GPT-4os[29] viral Ghiblification capabilitytransforming ordinary photographs into Studio Ghibli-style artworks through natural language interactionhighlight the transformative potential of such systems. These applications reveal critical limitation in some conventional approaches[31, 40, 43]: fragmented pipelines where separate models handle understanding, generation, and editing. Such isolation impedes cross-modal synergy, inflates deployment costs through redundant model stacks, and disrupts natural multi-turn creative workflows. Consequently, the development of natively unified architectures that intrinsically support end-to-end visual comprehension, text-to-image synthesis, and instruction-driven editing has emerged as pivotal challenge in multimodal artificial intelligence. Existing solutions face fundamental constraints. Methods using VQGAN/VAE representations[42, 51, 53, 61] prioritize pixel-level reconstruction at the expense of semantic richness, inherently weakening visual understanding capabilities. Alternative approaches[31, 40, 43] concatenate pre-trained visionlanguage and text-to-image models through ad-hoc connectors followed by joint fine-tuning. This piecemeal design fails to achieve deep integration, resulting in performance trade-offs between Tech report. Preprint. Figure 1: Showcases of our models performance on editing and generation tasks. 2 generation fidelity, editing precision, and reasoning depth. Moreover, prevailing efforts often resort to extreme scalingdeploying multi-billion-parameter models trained on trillion-scale datasetsraising serious concerns about computational efficiency and practical deployability. crucial question thus remains unanswered: Can single, parameter-efficient architecture excel simultaneously at visual understanding, high-fidelity image generation, and precise editing, while remaining efficient enough for deployment on commodity hardware? We address this challenge through Skywork UniPic, unified autoregressive model that redefines the efficiency frontier for multimodal integration. The model is built upon single large language model (LLM), primarily consisting of MAR encoder, SigLIP2 encoder, LLM backbone, and MAR decoder. Our architecture fundamentally departs from quantization-based or connectordependent paradigms by embedding image understanding, text-to-image generation, and image editing within single end-to-end trainable framework. The core innovation lies in decoupled visual encoding strategy: we employ the Masked Autoregressive decoder (MAR[22]) as the backbone for generation-focused representation, optimized for high-fidelity synthesis, while integrating SigLIP2[41] for understanding-focused tasks. Critically, both encoders operate within shared autoregressive objective, enabling bidirectional knowledge transfer where generation enhances visual detail modeling for understanding, and semantic understanding guides coherent editing. This design preserves architectural simplicity while resolving the longstanding tension between pixel-level fidelity and semantic comprehension. Skywork UniPic achieves unprecedented parameter efficiency without sacrificing capability. With compact 1.5B language backbone, it establishes new state-of-the-art results across critical benchmarks: surpassing contemporary models on GenEval[12] (0.86) for instruction following, achieving 85.5 on DPG-Bench[16] for complex generation, and leading among unified models on editing tasks (5.83 on GEditBench-EN[26], 3.49 on ImgEdit-Bench[56]), the visualization results as show in Figure1. Remarkably, it accomplishes this with approximately one-tenth the parameters of comparable systems like BAGEL[9] (14B) or UniWorld-V1[24] (19B), while generating 10241024 images on consumer-grade hardware (RTX 4090). This efficiency stems from three synergistic innovations: meticulous curation of hundred-million-scale high-quality dataset emphasizing task balance and semantic diversity; novel text-to-image reward model trained via Group Relative Policy Optimization (GRPO[37]) and editing reward model to align with human preferences; and progressive training curriculum that incrementally introduces task complexity while scaling resolution from 2562 to 10242. Our work makes three key contributions to unified multimodal modeling. First, we introduce the natively unified autoregressive architecture that intrinsically supports joint visual understanding, generation, and editing without requiring separate models or connectors, maintaining accessibility for real-world applications. Second, we resolve the semantic-fidelity dichotomy through decoupled visual encoding strategy that optimizes representation pathways for distinct task requirements while maintaining cross-task synergy. Third, we demonstrate that rigorous data curation, targeted reward modeling, and progressive training enable state-of-the-art performance at unprecedented scale efficiencyproving that high-quality multimodal integration need not demand excessive computational resources. Through extensive validation across four well-known image-related benchmarks and comprehensive ablation studies, we establish Skywork UniPic as practical foundation for deployable multimodal systems. By open-sourcing the model weights, training code, and technical documentation, we aim to accelerate the adoption of efficient unified vision-language models in resource-constrained environments, bridging the gap between theoretical capability and real-world applicability."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 Semantic Encoders Vision-language models (VLMs) have emerged as the cornerstone of multimodal understanding by introducing semantic encoders that effectively inject visual signals into language models, thereby endowing them with robust image comprehension capabilities. Among these, CLIP[35] established foundational paradigm through its contrastive learning framework that aligns image and text embeddings in shared space, enabling remarkable zero-shot classification and retrieval performance. Building on this foundation, SigLIP[57] refined the training methodology with sigmoid-based 3 loss function that eliminated temperature parameter dependencies, enabling more stable scaling. SigLIP2[41] integrates multiple advanced techniquesincluding captioning-based pretraining, selfsupervised losses, and online data curationto produce even richer semantic representations while preserving input aspect ratios across multiple resolutions. These progressive advancements in visual semantic encoding have significantly enhanced zero-shot classification, image-text retrieval, and transfer learning capabilities, establishing crucial foundations for unified models that must balance deep semantic understanding with high-fidelity generationa balance that remains challenging for existing approaches due to the inherent tension between pixel-level detail preservation and conceptual representation. 2.2 Image Generation Image generation methods have undergone several distinct architectural paradigms. Early work on Generative Adversarial Networks (GANs[13]) demonstrated that adversarial training can produce realistic samples, but often suffered from instability. Diffusion models[15, 36, 38] subsequently introduced likelihood-based framework, with work such as GLIDE[27], DALLE 3[28] and Stable Diffusion [36] achieving high-fidelity, diverse synthesis. More recent diffusion-based variants: LUMINA-Next[62],SDXL[32], PlayGround v2.5[20],Hunyuan-Dit[23] and FLUX.1-dev have further optimized image quality and efficiency at scale. In parallel, autoregressive models[39] treat images as sequences of discrete tokens, trading off generation speed for flexibility in conditional synthesis. Latent Diffusion Models (LDMs)[36] have emerged as practical standard by performing diffusion in lower-dimensional latent space, thus reducing computation without sacrificing detail. Vectorquantized approaches such as VQGAN[10] combine discrete codebooks with adversarial losses to improve perceptual fidelity, although quantization can introduce semantic loss. In contrast, masked autoregressive encoder-decoder (MAR)[22] operate directly in pixel space using autoregressive masked prediction, eliminating the need for learned codebooks and offering unified, end-to-end framework that aligns naturally with our autoregressive decoder. 2.3 Image Editing Image editing research has rapidly advanced under natural language supervision, enabling precise and semantically meaningful modifications driven by user instructions. Instruct-Pix2Pix [5] finetunes diffusion models to directly follow edit instructions without additional architectural changes, achieving strong instruction adherence. pivotal contribution in this direction is Step1X-Edit [26], which established scalable data generation pipeline across diverse editing tasks and introduced GEditBench for standardized evaluation. Building on this progress, IC-Edit [59] introduced context-aware generation mechanism leveraging diffusion Transformers, enabling zero-shot instruction following without architectural modifications, thereby demonstrating strong generalization across unseen editing commands. Concurrently, UltraEdit [60] addressed data scarcity and diversity limitations by constructing large-scale, automatically curated dataset, significantly improving the quality and finegrained controllability of language-driven edits. Despite these notable advances, critical limitation persists: most current systems operate in isolation from broader vision-language understanding and generative modeling pipelines. They typically rely on specialized, standalone architectures that are decoupled from models responsible for image description, reasoning, or synthesis. This architectural fragmentation impedes the realization of seamless, multi-turn interactive workflows, in which users naturally alternate between describing scenes, issuing edit commands, and iteratively refining visual outputs through continuous natural language dialogue. 2.4 Unified Models Unified multimodal models seek to combine visual understanding and generation within single architecture, enabling seamless interaction between vision and language. These models can be grouped into four main paradigms: harmonization, decoupling, hybrid, and connector approaches. The harmonization approach, exemplified by Harmon[49], uses shared MAR[22] encoder-decoder for both tasks. It builds on findings that MAR representations achieve strong performance in linear probing and respond precisely to visual concepts, suggesting their potential for understanding beyond generation. In contrast, the decoupling strategy, as seen in Janus[46] and Janus-Pro[8], separates visual encoding into distinct pathways. This design addresses conflicting granularity demands while maintaining unified Transformer backbone, improving flexibility and task specialization. 4 Hybrid models like Show-o[53] integrate autoregressive and discrete diffusion mechanisms. This allows support for diverse tasks such as visual question answering, text-to-image generation, and mixed-modal synthesis. Connector-based methods, such as MetaQueries[31], use learnable queries to bridge autoregressive LLMs and diffusion models, enabling modular integration without architectural changes. Recent advances include BAGEL[9], large-scale decoder-only model trained on trillions of multimodal tokens. It demonstrates emergent capabilities in multimodal reasoning, including image manipulation, future frame prediction, and 3D navigation. OmniGen2[47] introduces separate decoding paths for text and images, along with decoupled image tokenizer. This design preserves text generation quality while supporting in-context editing and achieving state-of-the-art performance on the OmniContext benchmark. UniFluid[11] adopts unified autoregressive framework with continuous visual tokens, showing that generation and understanding can mutually benefit under balanced training. Other notable models include BLIP3-o[6], which generates CLIP-space features via diffusion Transformers, and OpenUni[48], lightweight open-source baseline. Despite significant progress, developing compact unified model that achieves state-of-the-art performance across understanding, generation, and editing tasks while remaining practical for real-world deployment remains critical challenge."
        },
        {
            "title": "3 Method",
            "content": "We introduce Skywork UniPic, unified autoregressive model that natively integrates image understanding, text-to-image generation, and image editing within single framework. The rapid advancement of multimodal AI has revealed limitations in fragmented approaches where separate specialized models handle different tasks through loosely coupled connectors [31, 40, 43]. Such architectures suffer from suboptimal cross-modal synergy and increased deployment complexity. Inspired by recent work on unified multimodal modeling, particularly Harmon [49] which demonstrated the potential of shared visual representations in autoregressive frameworks, we develop more sophisticated approach to task unification. While Harmon showed promising results using single MAR encoder for both understanding and generation, we identify and address critical limitation: shared encoders can suffer from task interference due to conflicting optimization objectives. Our key insight is that different visual tasks require representations at different levels of granularityunderstanding demands semantic richness while generation requires pixel-level fidelityyet both can benefit from unified processing through shared language backbone. Building on this observation, we propose decoupled encoding strategy within unified autoregressive framework. Rather than forcing single encoder to optimize for conflicting objectives, we employ task-specific encoders that feed into shared language model, enabling both specialized representation learning and cross-task knowledge transfer. This design preserves the benefits of unified training while allowing each encoder to excel at its designated task. 3.1 Model Architecture Our model consists of four core components: (1) Masked Autoregressive (MAR) encoder-decoder pair [22] for generation-focused visual representation, (2) SigLIP2 encoder [41] for understandingfocused visual encoding, (3) shared Qwen2.5-1.5B-Instruct [34] language model backbone, and (4) dedicated MLP projection layers that bridge visual encoders to the language models embedding space, as illustrated in Figure 2. The architecture departs from the original Harmon frameworks shared encoder approach, which we found prone to task interference. Instead, we employ decoupled encoding strategy where each visual encoder is optimized for its specific task requirements while maintaining unified processing through the shared language backbone. This design preserves the benefits of unified training while allowing task-specific representation learning. For image generation, we utilize MAR-Huge* as both encoder and decoder, containing approximately 1B parameters with 20 layers each for encoding and decoding, hidden dimension of 1280, and 16 attention heads. Images are first encoded into latent representations using frozen VAE [36] from the *https://huggingface.co/jadechoghari/mar/blob/main/mar-huge.safetensors 5 Figure 2: The overall framework of Skywork UniPic. (a) Image generation is achieved through masked auto-regressive process using the MAR model [22]. (b) Image understanding is performed using SigLIP2 encoder [41] to extract rich visual features, which are subsequently passed to an LLM for autoregressive text generation. They share single LLM to promote consistent instructionfollowing and enable knowledge transfer between generation and understanding tasks original Harmon framework, preserving low-level visual features and ensuring stable convergence during multimodal training. We scale the generation resolution from 256256 to 512512 to enable higher-fidelity synthesis and capture fine-grained visual details, broadening MARs applicability to high-resolution image synthesis tasks. For image understanding, we adopt SigLIP2-so400m-patch16-512 as the visual encoder, leveraging its superior cross-modal alignment capabilities and efficient representation learning demonstrated across vision-language benchmarks. The encoder processes images at 512512 resolution and extracts semantically rich features optimized for understanding tasks. To further enhance visual understanding capabilities, we continue training based on the SigLIP2-so400m-patch16-512 checkpoint, which provides solid foundation for cross-modal representation learning. Two separate two-layer MLPs project the visual encoder outputs to align with the 1.5B language models embedding space. This separation allows independent optimization of projection mappings for each task while maintaining architectural simplicity and facilitating effective integration with the shared LLM. 3.2 Training Methodology Our training employs multi-task objective that combines generation and understanding losses: Image Generation (Diffusion loss): LGen = Eε,t (cid:104) ε εθ (xt t, z)2(cid:105) Image Understanding (Cross-entropy loss): LUnd = 1 (cid:88) (cid:88) n= i=1 yn,i log(ˆyn,i) These are integrated into the multi-task objective during joint training: LTotal = λGenLGen + λUndLUnd where λ coefficients evolve through training stages to balance task learning dynamics. https://huggingface.co/google/siglip2-so400m-patch166 We implement four-stage progressive training curriculum spanning hundred-million-scale pretraining and million-scale supervised fine-tuning. The pipeline begins with Stage 1: MAR Pretraining (PT), establishing foundational generation capabilities through dedicated training of the MAR encoder-decoder module with particular emphasis on face reconstruction and complex object synthesis. This is followed by Stage 2: MAR-LLM Alignment, where MAR outputs are projected to the LLM embedding space while maintaining frozen LLM parameters, utilizing cosine annealing scheduling to accelerate convergence of the projection layers. Subsequently, Stage 3: Joint Optimization (CT) unfreezes the LLM for cross-modal tuning under the multi-task objective LTotal with loss weights λGen = 1 and λUnd = 0.01, yielding 12-15% improvements in instruction adherence metrics. The process concludes with Stage 4: Supervised Fine-tuning (SFT), which refines the unified model using reward-filtered samples with quality threshold above 0.9, incorporating the full LTotal objective with editing loss components to polish final task performance. Resolution scaling occurs progressively from 2562 in early stages to 10242 in final training, with generation tasks reaching 10241024 and understanding tasks stabilizing at 512512. This staged approach allows the model to learn fundamental capabilities at lower resolutions before adapting to high-resolution synthesis requirements. Table 1: UniPic Training Configuration Across Learning Stages Hyperparameter Learning rate LR scheduler Weight decay Gradient clipping Optimizer Loss weights (U:G:E) 0:1:0 0.05 Warmup ratio 800 Training epochs 0.9999 EMA decay Training samples 130M Image resolution (width height) PT 5.0 105 Constant 0.0 1.0 Alignment 1 105 CT 1.0 105 Cosine decay Cosine decay Cosine decay 0.02 1.0 SFT 5 106 0.02 1.0 0.02 1. AdamW (β1 = 0.9, β2 = 0.95, ϵ = 1015) 0.05 3 130M 0.01:1:1 0.01 3 0.9999 130M 0.01:1:1 0.01 2 0.995 3M Generation Understanding 512 512 256 1024 1024 512 512 1024 1024 512 512 1024 1024 512 512 Hyperparameters are detailed in Table 1. The training stack utilizes bf16 mixed-precision and is optimized with DeepSpeed ZeRO-3 [2]. We use global batch size of 4096 for pre-training (PT) and 512 for supervised fine-tuning (SFT). The model architecture consists of an 800M parameter MAR module combined with 1.5B parameter language model backbone. 3.3 Data Quality Assurance To ensure training data quality, we develop two specialized reward models based on Qwen-VL architecture [4]: Skywork-ImgReward for visual quality assessment and Skywork-EditReward for image editing accuracy evaluation. Skywork-ImgReward is trained using Group Relative Policy Optimization (GRPO) [37], leveraging custom-designed paired ranking reward function that combines learned pairwise ranking scores (rθ) with format-based scores (rformat): r(x, yi) = rθ(x, yi) (cid:124) (cid:123)(cid:122) (cid:125) pairwise ranking + rformat(x, yi) (cid:125) (cid:123)(cid:122) format reward (cid:124) (1) Training data integrates several public datasets, including Pick-a-Pic [19], ImageRewardDB [54], and HPSv2 [50], augmented with curated samples focused on human figure quality. U:G:E = Understanding:Generation:Editing loss weights 7 Skywork-EditReward is trained via supervised fine-tuning on high-quality editing datasets including HumanEdit [3], UltraEdit [60], and SuperEdit-40K [21], enabling fine-grained assessment of instruction alignment and semantic correctness in image edits. Our data curation pipeline applies rigorous filtration, first discarding samples with reward scores below 0.9, then employing multi-check mechanisms using VQAScore [25] as additional quality heuristic. Analysis reveals four primary failure modes: instruction-alignment deviations, visual artifacts, semantic inconsistencies, and edit non-compliance. This curated dataset ensures high data homogeneity and enhances model generalization across diverse visual categories including human figures, animals, and text rendering."
        },
        {
            "title": "4 Main Results",
            "content": "4.1 Evaluation Setup To comprehensively assess the unified capabilities of Skywork UniPic, we adopt multi-faceted evaluation strategy encompassing image understanding, text-to-image generation, and image editing across established benchmarks. Benchmarks. For text-to-image generation, we evaluate on GenEval[12] which measures compositional understanding and object-focused alignment, and DPG-Bench[16] which assesses complex instruction following and long prompt adherence capabilities. These benchmarks capture both fine-grained compositional reasoning and general-purpose generation quality. Image editing capabilities are assessed using GEdit-Bench-EN[26] and ImgEdit-Bench[56] as primary evaluation suites. Built from authentic user requests covering diverse editing scenarios, these benchmarks closely mirror practical editing needs and provide comprehensive coverage of instruction-based image modification tasks including object addition/removal, style transfer, and attribute modification. Evaluation Protocol All image generation tasks employ 64 sampling steps with 10241024 resolution outputs and classifier-free guidance scale of 3 for optimal quality-diversity trade-off. Performance assessment utilizes official benchmark scripts and automated evaluation metrics, with all scores reported from single evaluation runs without reranking or multi-sampling to ensure reproducible results. Baselines We compare against several categories of state-of-the-art models. Unified models include OmniGen/OmniGen2 [52, 47], Janus/Janus-Pro [46, 8], BAGEL [9], UniWorld-V1 [24], Show-o [53], BLIP3-o [6], MetaQuery-XL [31], and Ovis-U1 [44]. Specialized generation models comprise diffusion approaches (FLUX.1-dev [55], SD3-medium [1], SDXL [32], DALL-E 3 [28], LUMINA-Next [62], Hunyuan-DiT [23], PixArt-(cid:80) [7], NOVA [18]) and autoregressive models (TokenFlow-XL [33], Emu3-Gen [45]). For editing, we compare against Step1X-Edit [26], ICEdit [59], AnyEdit [17], UltraEdit [60], Instruct-Pix2Pix [5], and MagicBrush [58]. Proprietary models include GPT-4o [30] and Gemini-2.0-flash [14]. Despite utilizing only 1.5B activated parameters, Skywork UniPic demonstrates competitive or superior performance compared to significantly larger unified models (typically 7B+ parameters), highlighting the effectiveness of our architectural design and training methodology.The corresponding performance metrics for each task are summarized in Figure 3. 4.2 Text-to-Image Generation We assess Skywork UniPics T2I generation capabilities on two standard benchmarks: GenEval and DPG-Bench, which evaluate compositional understanding and long prompt following respectively. Our model demonstrates highly competitive performance, particularly when considering its resource efficiency. Evaluation on GenEval. As shown in Table 2, Skywork UniPic achieves an overall score of 0.86 on GenEval, demonstrating strong compositional understanding across diverse generation tasks. The model performs particularly well on single object generation (98.44%) and two object composition (92.42%), while maintaining solid performance on color understanding (90.69%) and Figure 3: Performance comparison across multiple benchmarks. Skywork UniPic demonstrates competitive performance across understanding, generation, editing, and in-context tasks while maintaining exceptional parameter efficiency with only 1.5B activated parameters. spatial positioning (89.00%). Counting tasks (74.06%) and color attribution (72.25%) present greater challenges, consistent with observations across unified models in the literature. Evaluation on DPG-Bench. On DPG-Bench, Skywork UniPic achieves an overall score of 85.5, demonstrating competitive performance in long prompt following and complex scene understanding. Table 3 shows detailed comparisons across different evaluation categories, where our model maintains consistent performance across global coherence, entity recognition, attribute understanding, and relational reasoning. These results are particularly notable given our models compact 1.5B parameter count compared to significantly larger unified alternatives like BAGEL (14B) or UniWorld-V1 (19B), highlighting the effectiveness of our decoupled encoding strategy and progressive training methodology. 4.3 Image Editing Image editing represents core strength of Skywork UniPics unified architecture. We evaluate the models editing capabilities on both GEdit-Bench and ImgEdit-Bench, which assess instruction-based image modification across diverse scenarios. Evaluation on GEdit-Bench. As demonstrated in Table 4, Skywork UniPic achieves strong performance with an overall score of 5.83, placing it among the top-tier unified models. The model demonstrates particular strength in semantic consistency (SC) with score of 6.72, indicating robust instruction-following capabilities. While perceptual quality (PQ) scores show room for improvement at 6.18, the models ability to make precise, localized edits while preserving unmodified regions demonstrates the effectiveness of our unified architecture. Evaluation on ImgEdit-Bench. To further validate our models editing capabilities across diverse scenarios, we evaluate Skywork UniPic on ImgEdit-Bench, comprehensive benchmark covering nine distinct editing categories. As demonstrated in Table 5, Skywork UniPic achieves competitive performance with an overall score of 3.49, establishing itself among the leading unified models in comprehensive image editing evaluation. The results reveal noteworthy patterns in our models performance across different editing categories. Skywork UniPic demonstrates particularly strong capabilities in Action editing (4.04) and Style modification (4.76), benefiting from our progressive training methodology that emphasizes multistage capability development and comprehensive data curation across diverse editing scenarios. The model also shows solid performance in Background editing (3.77) and Replace operations (4.31), indicating robust understanding of spatial relationships and object substitution. Compared to other unified models, Skywork UniPic outperforms OmniGen (2.96) and approaches the performance of leading specialized editing models like ICEdit (3.05) and Step1X-Edit (3.06), while maintaining the advantage of unified architecture that handles multiple modalities within Table 2: Comprehensive comparison on GenEval benchmark. denotes using rewritten prompts. Model Single Two Count Color Position Attr Overall Diffusion Models SDv2.1[36] SDXL[32] IF-XL LUMINA-Next[62] SD3-medium[1] FLUX.1-dev[55] NOVA[18] Autoregressive Models TokenFlow-XL[33] Janus[46] Janus Pro[8] Emu3-Gen[45] Show-o[53] Unified Models OmniGen[52] OmniGen2[47] OmniGen2 MetaQuery-XL[31] BLIP3-o 4B[6] BLIP3-o 8B BAGEL[9] BAGEL UniWorld-V1[24] UniWorld-V1 Ovis-U1[44] Proprietary Models GPT-4o[30] Skywork UniPic 0.98 0.98 0.97 0.92 0.99 0.99 0.99 0.95 0.97 0.99 0.99 0.98 0.98 1.00 0.99 - - - 0.99 0.98 0.99 0.98 0.98 0. 0.98 0.51 0.74 0.74 0.46 0.94 0.81 0.91 0.60 0.68 0.89 0.81 0.80 0.84 0.95 0.96 - - - 0.94 0.95 0.93 0.93 0.98 0.92 0. 0.44 0.39 0.66 0.48 0.72 0.79 0.62 0.41 0.30 0.59 0.42 0.66 0.66 0.64 0.74 - - - 0.81 0.84 0.79 0.81 0.90 0.85 0.74 0.85 0.85 0.81 0.70 0.89 0.74 0. 0.81 0.84 0.90 0.80 0.84 0.74 0.88 0.98 - - - 0.88 0.95 0.89 0.89 0.92 0.92 0.91 0.07 0.15 0.13 0.09 0.33 0.20 0.33 0.16 0.46 0.79 0.49 0. 0.40 0.55 0.71 - - - 0.64 0.78 0.49 0.74 0.79 0.75 0.89 0.17 0.23 0.35 0.13 0.60 0.47 0.56 0.24 0.42 0.66 0.45 0.50 0.43 0.76 0.75 - - - 0.63 0.77 0.70 0.71 0. 0.61 0.72 0.50 0.55 0.61 0.46 0.74 0.67 0.71 0.55 0.61 0.80 0.66 0.68 0.68 0.80 0.86 0.80 0.81 0.84 0.82 0.88 0.80 0.84 0.89 0. 0.86 single framework. The superior performance of BAGEL (3.20) and UniWorld-V1 (3.26) on certain categories demonstrates the benefits of larger parameter scales and extensive training data, yet our model achieves comparable results with significantly fewer parameters, highlighting the efficiency of our architectural design and training strategy. 4.4 Qualitative Results Text-to-Image Generation Quality. Figure 4 presents qualitative comparisons between Skywork UniPic and both open-source and proprietary models on text-to-image generation tasks. Our model demonstrates competitive visual quality and strong adherence to textual prompts across diverse scenarios, from simple object generation to complex scene composition. The results show that despite its compact size, Skywork UniPic produces images with comparable fidelity and semantic accuracy to much larger specialized models. Image Editing Capabilities. Figure 5 showcases Skywork UniPics image editing performance compared to state-of-the-art editing models. The model demonstrates precise instruction following across various editing scenarios, including object addition/removal, style transfer, attribute modification, and complex compositional changes. Notably, the model maintains consistency in unedited regions while accurately implementing the requested modifications, highlighting the benefits of our unified architecture approach. Table 3: Comprehensive comparison on DPG-Bench across different semantic categories. Model Global Entity Attribute Relation Other Overall Diffusion Models LUMINA-Next[62] SDXL[32] PlayGroundv2.5[20] Hunyuan-DiT[23] PixArt-(cid:80)[7] DALLE3[28] SD3-medium[1] FLUX.1-dev[55] Autoregressive Models Show-o[53] EMU3[45] TokenFlow-XL[33] Janus[46] Janus Pro[8] BLIP3-o 4B[6] BLIP3-o 8B Unified Models OmniGen[52] OmniGen2[47] BAGEL[9] UniWorld-V1[24] Ovis-U1[44] Skywork UniPic 82.82 83.27 83.06 84.59 86.89 90.97 87.90 82.10 79.33 85.21 78.72 82.33 86.90 - - 87.90 88.81 88.94 83.64 82.37 89.65 88.65 82.43 82.59 80.59 82.89 89.61 91.01 89. 75.44 86.68 79.22 87.38 88.90 - - 88.97 88.83 90.37 88.39 90.08 87.78 86.44 80.91 81.20 88.01 88.94 88.39 88.83 88.70 78.02 86.84 81.29 87.70 89.40 - - 88.47 90.18 91.29 88.44 88. 90.84 80.53 86.76 84.08 74.36 86.59 90.58 80.70 91.10 84.45 90.22 85.22 85.46 89.32 - - 87.95 89.37 90.82 89.27 93.35 91.89 81.82 80.41 83.50 86.41 87.68 89.83 88.68 89. 60.80 83.15 71.20 86.41 89.48 - - 83.56 90.27 88.67 87.22 85.20 91.95 74.63 74.65 75.47 78.87 80.54 83.50 84.08 84.00 67.27 80.60 73.38 79.68 84.19 79.36 81.60 81.16 83.57 85.07 81.38 83. 85.50 Table 4: Comprehensive comparison on GEdit-Bench-EN showing semantic consistency (SC) and perceptual quality (PQ) metrics. Higher scores are better for all metrics. Model SC PQ Overall 6.61 7.62 5.49 5.66 5.82 6.85 6. 5.89 6.77 6.83 7.43 - 6.18 6.32 7.53 3.68 4.52 3.21 4.84 6.70 5.06 6.41 6.52 4.85 6.42 5. Proprietary Models Gemini-2.0-flash[14] GPT-4o[30] 6.73 7.85 Specialized Editing Models Instruct-Pix2Pix[5] MagicBrush[58] AnyEdit[17] ICEdit[59] Step1X-Edit[26] 3.58 4.68 3.18 5.11 7.09 Unified Models OmniGen[52] OmniGen2[47] BAGEL[9] UniWorld-V1[24] Ovis-U1[44] Skywork UniPic 5.96 7.16 7.36 4.93 - 6.72 11 Table 5: Comprehensive comparison on ImgEdit-Bench showing performance across nine editing categories. Higher scores are better for all metrics. Model Add Adjust Extract Replace Remove Background Style Hybrid Action Overall Proprietary Models GPT-4o[30] 4.61 4.33 2.90 4. 3.66 Specialized Editing Models 2.84 MagicBrush[58] 2.45 Instruct-Pix2Pix[5] 3.18 AnyEdit[17] 3.44 UltraEdit[60] 3.88 Step1X-Edit[26] 3.58 ICEdit[59] Unified Models OmniGen[52] OmniGen2[47] BAGEL[9] UniWorld-V1[24] Ovis-U1[44] Skywork UniPic 3.47 3.57 3.56 3.82 4.13 3. 1.58 1.83 2.95 2.81 3.14 3.39 3.04 3.06 3.31 3.64 3.62 3.51 1.51 1.44 1.88 2.13 1.76 1.73 1.71 1.77 1.70 2.27 2.98 2. 1.97 2.01 2.47 2.96 3.40 3.15 2.94 3.74 3.30 3.47 4.45 4.31 1.58 1.50 2.23 1.45 2.41 2.93 2.43 3.20 2.62 3.24 4.06 2. 4.57 1.75 1.44 2.24 2.83 3.16 3.08 3.21 3.57 3.24 2.99 4.22 3.77 4.93 3. 4.89 4.20 2.38 3.55 2.85 3.76 4.63 3.84 4.19 4.81 4.49 4.21 4.69 4.76 1.62 1.20 1.56 1.91 2.64 2. 2.24 2.52 2.38 2.96 3.45 2.56 1.22 1.46 2.65 2.98 2.52 3.68 3.38 4.68 4.17 2.74 4.61 4.04 1.90 1.88 2.45 2.70 3.06 3. 2.96 3.44 3.20 3.26 4.00 3."
        },
        {
            "title": "5 Limitation and Discussion",
            "content": "Limitations. While Skywork UniPic demonstrates strong performance across generation and editing tasks, certain limitations remain. As shown in Figure 6, the model occasionally struggles with complex or ambiguous instructions in text-to-image generation, leading to suboptimal instruction adherence. In the image editing setting, we observe cases where the model fails to respond to the editing prompt, resulting in incomplete or missing modifications. These limitations suggest that further refinement is needed in instruction grounding and editability robustness, particularly under challenging or compositional scenarios. Emergence of Capabilities. Similar to observations in BAGEL [9], UniPic exhibits clear, staged emergence of capabilities. Notably, text-to-image (T2I) generation appears in Stage 2 and is progressively refined, whereas more complex image editing capabilities emerge significantly later, only becoming evident in Stage 3 and Stage 4. This staggered manifestation reflects the inherent complexity of image editing, which demands more sophisticated integration of visual-semantic alignment, conditional reasoning, and structural preservation compared to direct generation. In our work, we define an ability as emergent if it is absent in earlier training stages but materializes in later ones. This qualitative shift, often termed phase transition, is consistent with our observation that UniPics loss curves do not explicitly signal the onset of new capabilities, reinforcing the notion that training loss is an insufficient proxy for evaluating true model abilities. To investigate this phenomenon, we evaluate model checkpoints from each stage by tracking average scores on standard VLM benchmarks (as proxy for multimodal understanding), the GenEval score (for generation), and the GEdit-Bench performance (for editing). Our experiments consistently show that editing capabilities emerge later than generation capabilities, pattern that holds even when scaling image resolution from 256 256 up to 1024 1024. Interestingly, each resolution increase induces temporary performance dip followed by rapid recovery that surpasses the previous capability plateau, suggesting higher resolutions unlock higher performance ceilings. Furthermore, we find no clear evidence that simply scaling understanding-centric data (e.g., image-text matching) directly enhances these generative or editing capabilities. This observation underscores the necessity of generation-specific training strategies for mastering complex, instruction-following tasks. 12 Figure 4: Qualitative comparison of text-to-image generation results. Skywork UniPic produces high-quality images that accurately reflect textual prompts while maintaining competitive visual fidelity compared to both open-source and proprietary models. Figure 5: Qualitative comparison of image editing results. Skywork UniPic successfully handles diverse editing instructions while preserving image quality and maintaining consistency in unmodified regions, demonstrating the effectiveness of our unified approach. 14 Figure 6: Failure cases."
        },
        {
            "title": "6 Conclusion and Future Work",
            "content": "We present Skywork UniPic, unified autoregressive model that achieves competitive performance across image understanding, text-to-image generation, and image editing tasks within single 1.5B parameter architecture. Through decoupled visual encoding that employs MAR for generation and SigLIP2 for understanding, our model resolves the fundamental tension between pixel-level fidelity and semantic understanding that has constrained previous unified approaches. The model demonstrates strong empirical results: 0.86 on GenEval for compositional generation, 85.5 on DPG-Bench for complex instruction following, and 5.83 on GEdit-Bench for image editing, while maintaining efficient deployment on consumer hardware. Our comprehensive data construction pipelines address critical data scarcity in editing tasks, and the specialized reward modeling framework provides effective quality assurance for training data curation. Key technical contributions include the decoupled encoding strategy that preserves both generation quality and understanding capabilities, systematic data construction methodologies for high-quality training corpus creation, and progressive training curriculum that enables efficient capability development across multiple resolutions. The work demonstrates that unified multimodal models can achieve both strong performance and practical efficiency, challenging assumptions about the necessity of massive parameter scaling for capable multimodal systems. Future work will address current limitations including performance on highly complex compositional instructions, fine-grained editing precision in challenging scenarios, and further optimization for multilingual capabilities. The open-source release of model weights, training code, and datasets aims to facilitate further research in parameter-efficient unified multimodal architectures."
        },
        {
            "title": "7 Contributions",
            "content": "Core Contributors: Peiyu Wang, Yi Peng, Yimeng Gan, Liang Hu, Eric Li*, Xuchen Song* Contributors: Tianyidan Xie, Xiaokun Wang, Yichen Wei, Chuanxin Tang, Bo Zhu, Changshi Li, Hongyang Wei, Yang Liu, Yahui Zhou * Project Lead"
        },
        {
            "title": "References",
            "content": "[1] Stability AI. Stable diffusion 3 medium: Multimodal diffusion transformer for photorealistic text-to-image generation. https://stability.ai/news/stable-diffusion-3-medium, 2025. 8, 10, 11 [2] Reza Yazdani Aminabadi, Samyam Rajbhandari, Minjia Zhang, Ammar Ahmad Awan, Cheng Li, Du Li, Elton Zheng, Jeff Rasley, Shaden Smith, Olatunji Ruwase, and Yuxiong He. Deepspeed inference: Enabling efficient inference of transformer models at unprecedented scale, 2022. 7 [3] Jinbin Bai, Wei Chow, Ling Yang, Xiangtai Li, Juncheng Li, Hanwang Zhang, and Shuicheng Yan. Humanedit: high-quality human-rewarded dataset for instruction-based image editing, 2025. 8 [4] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: versatile vision-language model for understanding, localization, text reading, and beyond, 2023. 7 [5] Tim Brooks, Aleksander Holynski, and Alexei A. Efros. Instructpix2pix: Learning to follow image editing instructions, 2023. 4, 8, 11, [6] Jiuhai Chen, Zhiyang Xu, Xichen Pan, Yushi Hu, Can Qin, Tom Goldstein, Lifu Huang, Tianyi Zhou, Saining Xie, Silvio Savarese, Le Xue, Caiming Xiong, and Ran Xu. Blip3-o: family of fully open unified multimodal models-architecture, training and dataset, 2025. 5, 8, 10, 11 [7] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-α: Fast training of diffusion transformer for photorealistic text-to-image synthesis, 2023. 8, 11 [8] Xiaokang Chen, Zhiyu Wu, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, and Chong Ruan. Janus-pro: Unified multimodal understanding and generation with data and model scaling, 2025. 4, 8, 10, 11 [9] Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Song, Guang Shi, and Haoqi Fan. Emerging properties in unified multimodal pretraining, 2025. 3, 5, 8, 10, 11, 12 [10] Patrick Esser, Robin Rombach, and Björn Ommer. Taming transformers for high-resolution image synthesis, 2021. [11] Lijie Fan, Luming Tang, Siyang Qin, Tianhong Li, Xuan Yang, Siyuan Qiao, Andreas Steiner, Chen Sun, Yuanzhen Li, Tao Zhu, et al. Unified autoregressive visual generation and understanding with continuous tokens. arXiv preprint arXiv:2503.13436, 2025. 5 [12] Dhruba Ghosh, Hanna Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating text-to-image alignment, 2023. 3, 8 [13] Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks, 2014. 4 [14] Google. Gemini 2.0 flash. https://developers.googleblog.com/en/experiment-with-gemini-20flash-native-image-generation, 2025. 8, [15] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models, 2020. 4 [16] Xiwei Hu, Rui Wang, Yixiao Fang, Bin Fu, Pei Cheng, and Gang Yu. Ella: Equip diffusion models with llm for enhanced semantic alignment, 2024. 3, 8 [17] Houcheng Jiang, Junfeng Fang, Ningyu Zhang, Guojun Ma, Mingyang Wan, Xiang Wang, Xiangnan He, and Tat seng Chua. Anyedit: Edit any knowledge encoded in language models, 2025. 8, 11, 12 17 [18] Nan Jiang, Chengxiao Wang, Kevin Liu, Xiangzhe Xu, Lin Tan, Xiangyu Zhang, and Petr Babkin. Nova: Generative language models for assembly code with hierarchical attention and contrastive learning, 2025. 8, [19] Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, and Omer Levy. Pick-a-pic: An open dataset of user preferences for text-to-image generation, 2023. 7 [20] Daiqing Li, Aleks Kamko, Ehsan Akhgari, Ali Sabet, Linmiao Xu, and Suhail Doshi. Playground v2.5: Three insights towards enhancing aesthetic quality in text-to-image generation, 2024. 4, 11 [21] Ming Li, Xin Gu, Fan Chen, Xiaoying Xing, Longyin Wen, Chen Chen, and Sijie Zhu. Superedit: Rectifying and facilitating supervision for instruction-based image editing, 2025. 8 [22] Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, and Kaiming He. Autoregressive image generation without vector quantization, 2024. 3, 4, 5, 6 [23] Zhimin Li, Jianwei Zhang, Qin Lin, Jiangfeng Xiong, Yanxin Long, Xinchi Deng, Yingfang Zhang, Xingchao Liu, Minbin Huang, Zedong Xiao, Dayou Chen, Jiajun He, Jiahao Li, Wenyue Li, Chen Zhang, Rongwei Quan, Jianxiang Lu, Jiabin Huang, Xiaoyan Yuan, Xiaoxiao Zheng, Yixuan Li, Jihong Zhang, Chao Zhang, Meng Chen, Jie Liu, Zheng Fang, Weiyan Wang, Jinbao Xue, Yangyu Tao, Jianchen Zhu, Kai Liu, Sihuan Lin, Yifu Sun, Yun Li, Dongdong Wang, Mingtao Chen, Zhichao Hu, Xiao Xiao, Yan Chen, Yuhong Liu, Wei Liu, Di Wang, Yong Yang, Jie Jiang, and Qinglin Lu. Hunyuan-dit: powerful multi-resolution diffusion transformer with fine-grained chinese understanding, 2024. 4, 8, 11 [24] Bin Lin, Zongjian Li, Xinhua Cheng, Yuwei Niu, Yang Ye, Xianyi He, Shenghai Yuan, Wangbo Yu, Shaodong Wang, Yunyang Ge, Yatian Pang, and Li Yuan. Uniworld-v1: High-resolution semantic encoders for unified visual understanding and generation, 2025. 3, 8, 10, 11, 12 [25] Zhiqiu Lin, Deepak Pathak, Baiqi Li, Jiayao Li, Xide Xia, Graham Neubig, Pengchuan Zhang, and Deva Ramanan. Evaluating text-to-visual generation with image-to-text generation, 2024. 8 [26] Shiyu Liu, Yucheng Han, Peng Xing, Fukun Yin, Rui Wang, Wei Cheng, Jiaqi Liao, Yingming Wang, Honghao Fu, Chunrui Han, Guopeng Li, Yuang Peng, Quan Sun, Jingwei Wu, Yan Cai, Zheng Ge, Ranchen Ming, Lei Xia, Xianfang Zeng, Yibo Zhu, Binxing Jiao, Xiangyu Zhang, Gang Yu, and Daxin Jiang. Step1x-edit: practical framework for general image editing, 2025. 3, 4, 8, 11, 12 [27] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models, 2022. [28] OpenAI. Improving image generation with better captions. https://cdn.openai.com/ papers/dall-e-3.pdf. 4, 8, 11 [29] OpenAI. Gpt-4o system card, 2024. 1 [30] OpenAI. Gpt-4o. https://openai.com/index/introducing-4o-image-generation, 2025. 8, 10, 11, [31] Xichen Pan, Satya Narayan Shukla, Aashu Singh, Zhuokai Zhao, Shlok Kumar Mishra, Jialiang Wang, Zhiyang Xu, Jiuhai Chen, Kunpeng Li, Felix Juefei-Xu, Ji Hou, and Saining Xie. Transfer between modalities with metaqueries, 2025. 1, 5, 8, 10 [32] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis, 2023. 4, 8, 10, 11 [33] Liao Qu, Huichao Zhang, Yiheng Liu, Xu Wang, Yi Jiang, Yiming Gao, Hu Ye, Daniel K. Du, Zehuan Yuan, and Xinglong Wu. Tokenflow: Unified image tokenizer for multimodal understanding and generation, 2024. 8, 10, 11 18 [34] Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report, 2025. 5 [35] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision, 2021. [36] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models, 2022. 4, 5, 10 [37] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024. 3, 7 [38] Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations, 2021. 4 [39] Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan. Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525, 2024. 4 [40] Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Zhengxiong Luo, Yueze Wang, Yongming Rao, Jingjing Liu, Tiejun Huang, and Xinlong Wang. Generative multimodal models are in-context learners, 2024. 1, [41] Michael Tschannen, Alexey Gritsenko, Xiao Wang, Muhammad Ferjad Naeem, Ibrahim Alabdulmohsin, Nikhil Parthasarathy, Talfan Evans, Lucas Beyer, Ye Xia, Basil Mustafa, Olivier Hénaff, Jeremiah Harmsen, Andreas Steiner, and Xiaohua Zhai. Siglip 2: Multilingual visionlanguage encoders with improved semantic understanding, localization, and dense features, 2025. 3, 4, 5, 6 [42] Aaron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. Neural discrete representation learning, 2018. 1 [43] Chunwei Wang, Guansong Lu, Junwei Yang, Runhui Huang, Jianhua Han, Lu Hou, Wei Zhang, and Hang Xu. Illume: Illuminating your llms to see, draw, and self-enhance, 2024. 1, 5 [44] Guo-Hua Wang, Shanshan Zhao, Xinjie Zhang, Liangfu Cao, Pengxin Zhan, Lunhao Duan, Shiyin Lu, Minghao Fu, Xiaohao Chen, Jianshan Zhao, Yang Li, and Qing-Guo Chen. Ovis-u1 technical report, 2025. 8, 10, 11, 12 [45] Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, Yingli Zhao, Yulong Ao, Xuebin Min, Tao Li, Boya Wu, Bo Zhao, Bowen Zhang, Liangdong Wang, Guang Liu, Zheqi He, Xi Yang, Jingjing Liu, Yonghua Lin, Tiejun Huang, and Zhongyuan Wang. Emu3: Next-token prediction is all you need, 2024. 8, 10, [46] Chengyue Wu, Xiaokang Chen, Zhiyu Wu, Yiyang Ma, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, Chong Ruan, and Ping Luo. Janus: Decoupling visual encoding for unified multimodal understanding and generation, 2024. 4, 8, 10, 11 [47] Chenyuan Wu, Pengfei Zheng, Ruiran Yan, Shitao Xiao, Xin Luo, Yueze Wang, Wanli Li, Xiyan Jiang, Yexin Liu, Junjie Zhou, Ze Liu, Ziyi Xia, Chaofan Li, Haoge Deng, Jiahao Wang, Kun Luo, Bo Zhang, Defu Lian, Xinlong Wang, Zhongyuan Wang, Tiejun Huang, and Zheng Liu. Omnigen2: Exploration to advanced multimodal generation, 2025. 5, 8, 10, 11, 12 [48] Size Wu, Zhonghua Wu, Zerui Gong, Qingyi Tao, Sheng Jin, Qinyue Li, Wei Li, and Chen Change Loy. Openuni: simple baseline for unified multimodal understanding and generation. arXiv preprint arXiv:2505.23661, 2025. 5 19 [49] Size Wu, Wenwei Zhang, Lumin Xu, Sheng Jin, Zhonghua Wu, Qingyi Tao, Wentao Liu, Wei Li, and Chen Change Loy. Harmonizing visual representations for unified multimodal understanding and generation, 2025. 4, 5 [50] Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng Zhu, Rui Zhao, and Hongsheng Li. Human preference score v2: solid benchmark for evaluating human preferences of text-to-image synthesis, 2023. [51] Yecheng Wu, Zhuoyang Zhang, Junyu Chen, Haotian Tang, Dacheng Li, Yunhao Fang, Ligeng Zhu, Enze Xie, Hongxu Yin, Li Yi, Song Han, and Yao Lu. Vila-u: unified foundation model integrating visual understanding and generation, 2025. 1 [52] Shitao Xiao, Yueze Wang, Junjie Zhou, Huaying Yuan, Xingrun Xing, Ruiran Yan, Chaofan Li, Shuting Wang, Tiejun Huang, and Zheng Liu. Omnigen: Unified image generation, 2024. 8, 10, 11, 12 [53] Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding and generation, 2024. 1, 5, 8, 10, 11 [54] Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagereward: Learning and evaluating human preferences for text-to-image generation, 2023. 7 [55] Chenglin Yang, Celong Liu, Xueqing Deng, Dongwon Kim, Xing Mei, Xiaohui Shen, and Liang-Chieh Chen. 1.58-bit flux, 2024. 8, 10, [56] Yang Ye, Xianyi He, Zongjian Li, Bin Lin, Shenghai Yuan, Zhiyuan Yan, Bohan Hou, and Li Yuan. Imgedit: unified image editing dataset and benchmark, 2025. 3, 8 [57] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training, 2023. 3 [58] Kai Zhang, Lingbo Mo, Wenhu Chen, Huan Sun, and Yu Su. Magicbrush: manually annotated dataset for instruction-guided image editing, 2024. 8, 11, [59] Zechuan Zhang, Ji Xie, Yu Lu, Zongxin Yang, and Yi Yang. In-context edit: Enabling instructional image editing with in-context generation in large scale diffusion transformer, 2025. 4, 8, 11, 12 [60] Haozhe Zhao, Xiaojian Ma, Liang Chen, Shuzheng Si, Rujie Wu, Kaikai An, Peiyu Yu, Minjia Zhang, Qing Li, and Baobao Chang. Ultraedit: Instruction-based fine-grained image editing at scale, 2024. 4, 8, 12 [61] Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma, Luke Zettlemoyer, and Omer Levy. Transfusion: Predict the next token and diffuse images with one multi-modal model, 2024. 1 [62] Le Zhuo, Ruoyi Du, Han Xiao, Yangguang Li, Dongyang Liu, Rongjie Huang, Wenze Liu, Lirui Zhao, Fu-Yun Wang, Zhanyu Ma, Xu Luo, Zehan Wang, Kaipeng Zhang, Xiangyang Zhu, Si Liu, Xiangyu Yue, Dingning Liu, Wanli Ouyang, Ziwei Liu, Yu Qiao, Hongsheng Li, and Peng Gao. Lumina-next: Making lumina-t2x stronger and faster with next-dit, 2024. 4, 8, 10,"
        }
    ],
    "affiliations": [
        "Multimodality Team, Skywork AI"
    ]
}