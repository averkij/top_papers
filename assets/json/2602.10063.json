{
    "paper_title": "Chain of Mindset: Reasoning with Adaptive Cognitive Modes",
    "authors": [
        "Tianyi Jiang",
        "Arctanx An",
        "Hengyi Feng",
        "Naixin Zhai",
        "Haodong Li",
        "Xiaomin Yu",
        "Jiahui Liu",
        "Hanwen Du",
        "Shuo Zhang",
        "Zhi Yang",
        "Jie Huang",
        "Yuhua Li",
        "Yongxin Ni",
        "Huacan Wang",
        "Ronghao Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Human problem-solving is never the repetition of a single mindset, by which we mean a distinct mode of cognitive processing. When tackling a specific task, we do not rely on a single mindset; instead, we integrate multiple mindsets within the single solution process. However, existing LLM reasoning methods fall into a common trap: they apply the same fixed mindset across all steps, overlooking that different stages of solving the same problem require fundamentally different mindsets. This single-minded assumption prevents models from reaching the next level of intelligence. To address this limitation, we propose Chain of Mindset (CoM), a training-free agentic framework that enables step-level adaptive mindset orchestration. CoM decomposes reasoning into four functionally heterogeneous mindsets: Spatial, Convergent, Divergent, and Algorithmic. A Meta-Agent dynamically selects the optimal mindset based on the evolving reasoning state, while a bidirectional Context Gate filters cross-module information flow to maintain effectiveness and efficiency. Experiments across six challenging benchmarks spanning mathematics, code generation, scientific QA, and spatial reasoning demonstrate that CoM achieves state-of-the-art performance, outperforming the strongest baseline by 4.96\\% and 4.72\\% in overall accuracy on Qwen3-VL-32B-Instruct and Gemini-2.0-Flash, while balancing reasoning efficiency. Our code is publicly available at \\href{https://github.com/QuantaAlpha/chain-of-mindset}{https://github.com/QuantaAlpha/chain-of-mindset}."
        },
        {
            "title": "Start",
            "content": "Chain of Mindset: Reasoning with Adaptive Cognitive Modes Tianyi Jiang1,2*, Arctanx An1*, Hengyi Feng1, Naixin Zhai6, Haodong Li3, Xiaomin Yu6, Jiahui Liu1, Hanwen Du, Shuo Zhang6, Zhi Yang4, Jie Huang4, Yuhua Li6, Yongxin Ni5, Huacan Wang6, Ronghao Chen1,6 2026-02-11 1PKU, 2BJTU, 3StepFun, 4SUFE, 5NUS, 6QuantaAlpha *These authors contributed equally to this work. Correspondence: wanghuacan17@mails.ucas.ac.cn, chenronghao@alumni.pku.edu.cn"
        },
        {
            "title": "Abstract",
            "content": "Human problem-solving is never the repetition of single mindset, by which we mean distinct mode of cognitive processing. When tackling specific task, we do not rely on single mindset; instead, we integrate multiple mindsets within the single solution process. However, existing LLM reasoning methods fall into common trap: they apply the same fixed mindset across all steps, overlooking that different stages of solving the same problem require fundamentally different mindsets. This single-minded assumption prevents models from reaching the next level of intelligence. To address this limitation, we propose Chain of Mindset (CoM), training-free agentic framework that enables step-level adaptive mindset orchestration. CoM decomposes reasoning into four functionally heterogeneous mindsets: Spatial, Convergent, Divergent, and Algorithmic. Meta-Agent dynamically selects the optimal mindset based on the evolving reasoning state, while bidirectional Context Gate filters cross-module information flow to maintain effectiveness and efficiency. Experiments across six challenging benchmarks spanning mathematics, code generation, scientific QA, and spatial reasoning demonstrate that CoM achieves state-of-the-art performance, outperforming the strongest baseline by 4.96% and 4.72% in overall accuracy on Qwen3-VL-32B-Instruct and Gemini-2.0-Flash, while balancing reasoning efficiency. Our code is publicly available at https://github.com/QuantaAlpha/chain-of-mindset. 6 2 0 2 0 1 ] . [ 1 3 6 0 0 1 . 2 0 6 2 : r Figure 1: Performance comparison on Qwen3-VL-32B-Instruct across six reasoning benchmarks."
        },
        {
            "title": "Introduction",
            "content": "The essence of human intelligence lies in the synergy of multiple complementary mindsets. Cognitive science research (Guilford, 1967) has identified distinct cognitive modes that serve fundamentally different functions: Spatial thinking concretizes abstract conditions into intuitive visual representations that facilitate pattern recognition (Newcombe, 2010; Newcombe and Shipley, 2014); Convergent thinking distills core insights from complex, multifaceted information through focused logical analysis (Cropley, 2006); Divergent thinking generates novel possibilities when conventional logic reaches an impasse by exploring unconventional pathways (Runco and Acar, 2012). This repertoire of cognitive capabilities constitutes the underlying flexibility with which humans handle heterogeneous tasks. Beyond these human cognitive modes, computational systems enable fourth capabilityAlgorithmic thinking: precise numerical calculation and formal verification through code execution (Futschek, 2006), providing computational precision that extends beyond the limits of human mental arithmetic. Yet current intelligent systems, despite their impressive scale and advances in multimodal perception (Lin et al., 2025), lack this repertoire of complementary cognitive capabilities and remain distant from the flexible, multimodal reasoning that characterizes human intelligence. Crucially, human problem-solving is not merely possessing these mindsets but dynamically orchestrating them within single reasoning episode (Newell et al., 1972). When facing complex task, we do not apply single mindset uniformly from start to finish; instead, we transition between mindsets as the problem state evolves. For example, solving geometry proof may begin with spatial reasoning to visualize the configuration, shift to convergent thinking to identify key relationships, then invoke divergent thinking to explore auxiliary constructions, and finally employ algorithmic steps to verify the solution. This step-level adaptive switching (i.e., recognize when each mindset is most effective and transitioning accordingly) is fundamental to human cognitive flexibility (Sali et al., 2024). It enables the reasoning trace to remain rigorous when precision is needed and creative when conventional approaches fail. Previous work (Didolkar et al., 2024; Kargupta et al., 2025) has confirmed that complex reasoning requires diverse mindsets. LLMs indeed exhibit different mindsets during the reasoning process, and prior studies suggest that controlling models through explicit cognitive interventions can effectively improve reasoning performance (Gandhi et al., 2025). However, question remains largely unexplored: Given different contexts and reasoning scenarios, which mindset is most suitable for solving the problem? Existing reasoning methods for LLMs fall into two paradigms, both with fundamental limitations, as illustrated in Figure 2. Single-mode reasoning methods (Wei et al., 2022; Chen et al., 2022; Li et al., 2023; Yao et al., 2023) apply uniform cognitive strategy throughout, struggling when sub-tasks demand heterogeneous capabilities. Static reasoning strategy selection methods (Gao et al., 2024; Yang et al., 2024; Aytes et al., 2025) choose reasoning format at task onset but cannot adapt when intermediate results reveal that different mindset would be more effective. Neither supports dynamic, state-dependent cognitive switchingrecognizing when to transition between mindsets based on the progress of reasoning. To address these limitations, we propose Chain of Mindset (CoM), training-free agentic reasoning paradigm that implements authentic cognitive chaining. Unlike previous methods that are limited to single mindset, our framework enables agents to dynamically orchestrate composite reasoning process with different mindsets. CoM decomposes reasoning into four functionally heterogeneous mindsetsSpatial, Convergent, Divergent, and Algorithmic. We selected these four mindsets because they represent search-style reasoning capabilities that transcend the typical single-mode reasoning of language models.These four mindsets are grounded in foundational cognitive science research as fundamental reasoning paradigms (Guilford, 1967; Futschek, 2006; Cropley, 2006; Newcombe, 2010; Runco and Acar, 2012), each exhibiting distinct behavioral signatures that enable explicit orchestration. When solving any given problem, the agent can adaptively select and dynamically invoke multiple mindsets based on the current state. Furthermore, to prevent cross-boundary information interference caused by frequent mindset switching, we introduce Context Gate mechanism. Through bidirectional semantic filtering, this mechanism ensures that each thinking module receives only task-relevant context, while the meta-agent receives only highly condensed thought feedback, thereby guaranteeing efficient reasoning. Extensive experiments across six challenging benchmarks demonstrate that CoM consistently outperforms 2 Figure 2: Comparison of reasoning paradigms. (a) Single-mode reasoning applies single mindset throughout, failing to address heterogeneous sub-task demands. (b) Static reasoning strategy selection chooses strategy at task onset but cannot adapt to intermediate states. (c) Chain of Mindset dynamically switches mindsets at subtask boundaries based on the progress of reasoning. all baselines, as illustrated in Figure 1, while maintaining computational efficiency and generalizing across both open-source and closed-source base models without any additional training. The main contributions are summarized as follows: We propose new agentic reasoning paradigm. To the best of our knowledge, this is the first training-free method achieve step-level adaptive switching of multiple mindsets within single inference process. We formally define four heterogeneous mindsets and propose the Context Gate bidirectional semantic filtering mechanism, which enables the agent to seamlessly switch mindsets while effectively reducing cross-module information interference. Our experiments on six challenging benchmarks, including mathematics, coding, scientific QA, and spatial reasoning, demonstrate that CoM not only significantly outperforms baseline methods in accuracy but also balances reasoning efficiency with generalization across models and domains without training."
        },
        {
            "title": "2 Method",
            "content": "In this section, we formally introduce the Chain of Mindset (CoM) framework. We begin by formalizing the mindset switching problem in complex reasoning tasks as sequential decision-making process in Sec.2.1. In Sec.2.2, we outline the three-layer decoupled architecture of the framework and its design rationale. We then provide detailed definitions of the four heterogeneous mindsets and their cognitive decision mechanism in Sec.2.3. In Sec.2.4, we demonstrate the necessity and implementation of the Context Gate for inter-module communication from an information-theoretic perspective. Finally, Sec.2.5 presents an illustrative case study demonstrating the dynamic re-planning capability."
        },
        {
            "title": "2.1 Problem Formulation",
            "content": "Consider geometry proof: one might spatially visualize the figure, divergently explore auxiliary constructions, convergently analyze which approach is promising, and algorithmically verify via coordinate calculations. This cognitive flexibilityswitching between different mindsets based on intermediate progressis natural for human experts, yet absent in current LLMs (Kargupta et al., 2025). We introduce mindset to formalize distinct cognitive modes. mindset is specialized reasoning paradigm (Guilford, 1967) characterized by: (1) distinct cognitive strategy (e.g., parallel exploration vs. focused deduction), (2) an isolated context with dedicated prompts, and (3) structured output. Unlike prior work treating strategies as interchangeable, mindsets are functionally heterogeneous 3 and their deployment requires explicit orchestration. We define four complementary mindsets = {mspat, mconv, mdiv, malgo}, corresponding to spatial imagination, convergent analysis, divergent exploration, and algorithmic computation. Each mindset is instantiated through corresponding call, forming the call set = {cspat, cconv, cdiv, calgo}. Given an input problem q, the reasoning process unfolds as trajectory = (c1, o1, i1, c2, o2, i2, . . . , cT , oT , iT ), where ct denotes the call invoked at step t, ot denotes its output, and it denotes its insight. At each step t, the agent observes the current state st = (q, H<t) and selects the next mindset: where signals termination. The agent then invokes the call ct corresponding to mt: mt = π(st) {} (ot, it) = ct(q, H<t) (1) (2) The central insight is that policy π conditions on accumulated history H<t: the optimal mindset at step depends not only on the original problem, but critically on what has been attempted and internalized previously. To better formularize, we need to address three challenges: When to switch: Judging when the current mindset has exhausted and another would be beneficial. Which mindset to invoke: Grounding selection in the semantic content of current state rather than relying on surface-level problem type classification. How to prevent interference: Each mindset requires an isolated context, yet must selectively receive relevant information and return distilled results to the Meta-Agent without polluting the main chain."
        },
        {
            "title": "2.2 Framework Overview",
            "content": "To endow the LLM with multi-modal reasoning capabilities while mitigating mutual interference between mindsets, CoM adopts three-layer decoupled architecture that separates meta-cognitive decision-making from concrete task execution. The framework comprises three core components. First, the Meta-Agent (A) serves as the central controller, orchestrating reasoning by selecting mindsets, generating call instructions, and internalizing intermediate insights. Second, the Mindsets (M) are functionally heterogeneous reasoning modules; each operates within an isolated context, driven by specific system prompts to execute particular sub-tasks. Finally, the Context Gate (G) performs bidirectional semantic filtering between the Meta-Agent and Mindsets, mitigating noise from long contexts. The reasoning process follows an iterative Plan-Call-Internalize loop. Initially, the Meta-Agent generates cognitive decision based on problem characteristics, defining an initial mindset plan. During execution, given state st = (q, H<t), the Meta-Agent selects mindset mt and invokes call ct to produce output ot and insight it. Upon completion, the Meta-Agent retrieves refined results via the Output Gate and modifies the remaining plan based on newly internalized insight. This mechanism provides flexibility for self-correction within complex reasoning paths."
        },
        {
            "title": "2.3 Mindset Dispatch",
            "content": "Each mindset receives filtered input tuple from the Input Gate: the call instruction c, relevant context Hrel extracted from reasoning history, and injected images Iinj when visual information is required. We define four complementary mindsets, each instantiated as specialized execution module with distinct cognitive strategies. Spatial Mindset (mspat). This mindset bridges abstract logic and intuitive perception through visual externalization (Lin et al., 2024; Li et al., 2025; Zhang et al., 2025a). Given instruction c, we supports three generation modes via Nano-Banana-Pro (Google, 2025): (1) TextImage: pure textual descriptions are transformed into visualizations; (2) Image+TextImage: referenced images Iinj are edited or augmented based on c; (3) CodeImage: when the model returns matplotlib code, it is executed in sandbox to produce figures. Generated artifacts are registered into global library with unique identifiers (e.g., [GEN_001]) for reference in subsequent reasoning steps. 4 Figure 3: Overview of the Chain of Mindset framework. Left: The Meta-Agent operates as meta-cognitive orchestrator, iteratively generating cognitive decisions (<cognitive_decision>), dispatching subtasks to specialized mindsets via call instructions (<call_mindset>), receiving summarized results (<mindset_result>), and internalizing key insights (<Insight>) before producing the final answer. The agent may revise its plan when intermediate results warrant replanning. Right: The Mindset Experts comprise four heterogeneous modulesDivergent, Algorithmic, Convergent, and Spatialeach providing distinct cognitive capabilities. The bidirectional Context Gate mediates information flow: the Input Gate filters relevant history for mindset execution, while the Output Gate distills verbose reasoning traces into concise results for the main chain. Convergent Mindset (mconv). This mindset addresses information overload by constructing focused reasoning environment (Wei et al., 2022; Pan et al., 2023). Given instruction and filtered context Hrel, it performs single deep reasoning pass that grounds each step in established facts, explicitly states missing information, and reaches clear conclusion. The output is complete logical derivation. Divergent Mindset (mdiv). This mindset breaks reasoning deadlocks through structured parallel exploration (Wang et al., 2022; Yao et al., 2023). Given instruction and filtered context Hrel, execution proceeds in two phases: (1) Branch Generation: produce [2, 5] distinct solution branches {b1, . . . , bk}, where each branch bi represents candidate reasoning path with explicit assumptions; (2) Parallel Exploration: independently analyze each bi through separate LLM call that examines its step-by-step procedure and potential limitations. Crucially, all branch explorations {r1, . . . , rk} are returned to the Meta-Agent for path selection, preserving deliberation at the metacognitive level. Algorithmic Mindset (malgo). This mindset addresses limitations of language models in precise calculation through code-based generate-execute-repair loop (Chen et al., 2022; Gao et al., 2023; Li et al., 2023). Given instruction and filtered context Hrel, let ρ0 denote the initially generated Python code. The execution iterates: (ρi+1, ralgo) = (ρi, EXEC(ρi)) (FIX(ρi, ϵi), ) (ρi, ϵi) if execution succeeds if error ϵi < Nmax otherwise (3) where ϵi denotes the execution error at iteration i, Nmax = 2 bounds repair attempts, and indicates 5 Question #494: If the Sun were the head of body, how long would its arms be? Answer: 2,437,190 km <cognitive_decision> This is metaphorical scaling problemI need to map human body proportions onto the Sun. The key challenge is establishing the correct anatomical ratio. Plan: Spatial (visualize proportions) Convergent (formalize mapping) Algorithmic (compute). </cognitive_decision> <call_spatial> Generate human body proportion diagram. need to see the relative sizes of head and arm to extract the ratio. </call_spatial> <spatial_result> Generated [GEN_001]. The visualization shows: adult human body 7.5 head-heights tall; arm span body height; single arm (shoulder to fingertip) 3.5 head size. </spatial_result> Generated [GEN_001] <insight> From the diagram: arm length 3.5 head size. But what does head size mean herediameter or radius? The Sun as head suggests treating its radius as the characteristic size. </insight> <call_convergent> Clarify the mapping: when we say the Sun is the head, should use Suns radius or diameter? </call_convergent> <convergent_result> Size in anatomical context typically refers to the characteristic dimensionfor spherical head, this is the radius (half-width). Suns radius = 696,340 km. </convergent_result> <insight> Mapping confirmed: Sun radius head size. Arm length = 3.5 R. </insight> <call_algorithmic> Compute 3.5 696,340. </call_algorithmic> <algorithmic_result> 3.5 696,340 = 2,437,190 km. </algorithmic_result> <Answer> 2,437,190 km </Answer> Figure 4: Fermi problem (#494) demonstrating the Spatial Mindset. The Spatial Mindset generates an anatomy diagram to visually ground the abstract proportion and extract the head-to-arm ratio ( 3.5). The subsequent Convergent call resolves an ambiguity: head size maps to the Suns radius rather than diameter. pending status before the next iteration. All mindsets produce unified output tuple (r, Inew) passed to the Output Gate, where contains the reasoning trace or execution log, and Inew denotes newly generated visual artifacts. The Output Gate distills this verbose output into concise summary Osum, which the Meta-Agent internalizes as <insight> to integrate into the main reasoning chain."
        },
        {
            "title": "2.4 Context Gate",
            "content": "In modular reasoning systems, information transfer faces Relevance-Redundancy Trade-off (Liu et al., 2024). Directly transmitting the complete history leads to context pollution, while transmitting only instructions results in context starvation. We address this issue from the perspective of Information Density. Let the reasoning history at time step be Ht and the call instruction be ct. In the input direction, the effective information density is defined as ρin = Hrel/Ht, where Hrel is the context subset relevant to the sub-task. As the reasoning step increases, ρin 0, implying linear growth in noise (Li et al., 2024). In the output direction, the raw output of mindset often contains extensive intermediate processes, whereas the main chain requires only key conclusions Osum, leading to an output density ρout 1. 6 The design objective of the Context Gate is to increase bidirectional information density (ρ 1). This mechanism consists of two components, each driven by an independent LLM. The Input Gate (Gin) uses the call instruction as semantic anchor to extract the minimal sufficient context set Hrel and relevant images Iinj from history H: (Hrel, Iinj) = Gin(H, c, M, I) (4) Conversely, the Output Gate (Gout) distills the key insight Osum from the verbose mindset output based on the expected goal of instruction c: Osum = Gout(r, c, Inew) (5) Through this bidirectional semantic filtering, the Context Gate ensures the efficient execution of mindsets in isolated environments while maintaining the compactness of the main reasoning chain. Complete prompt templates for all components are provided in Appendix D. 2."
        },
        {
            "title": "Illustrative Example",
            "content": "We demonstrate CoM on Fermi estimation problem (#494), showcasing how mindset switching enables natural reasoning: Spatial grounds abstract proportions through visualization, Convergent resolves semantic ambiguity, and Algorithmic ensures computational precision. This illustrates two key capabilities. First, visual grounding: CoM externalizes abstract quantities as verifiable images rather than relying on parametric recall. Second, ambiguity resolution: the internalization mechanism enables the Meta-Agent to detect underspecified mappings and trigger targeted clarification. Additional cases demonstrating other capabilities (e.g., dynamic re-planning, multimodal input) are provided in Appendix E."
        },
        {
            "title": "3 Experiments",
            "content": "We evaluate CoM through experiments on diverse reasoning tasks. We first describe the tasks and datasets in Sec. 3.1, the baselines in Sec. 3.2, and the implementation details in Sec. 3.3. We then present main results in Sec. 3.4, ablation studies in Sec. 3.5, and analysis in Sec. 3.6."
        },
        {
            "title": "3.1 Tasks and Datasets",
            "content": "We evaluate CoM on six benchmarks spanning four categories: (1) Mathematical Reasoning. AIME 2025: All 30 problems from the 2025 American Invitational Mathematics Examination, covering algebra, geometry, combinatorics, and number theory. Real-Fermi (Kalyan et al., 2021): 557 Fermi estimation problems requiring order-of-magnitude reasoning (e.g., How much coffee was consumed during EMNLP 2019?). (2) Code Generation. LiveCodeBench (Jain et al., 2024): 182 problems from LeetCode, AtCoder, and CodeForces published between January 1 and May 1, 2025, spanning 45 Easy, 55 Medium, and 82 Hard problems. (3) Science QA. GPQA (Rein et al., 2024). We select subset called GPQADiamond, containing 198 PhD-level questions in physics, chemistry, and biology, for which non-experts achieve only 30% accuracy. (4) Multimodal Reasoning. MathV (Wang et al., 2024): We select subset called MathVision-Mini containing 152 questions of the multimodal mathematical benchmark requiring visual diagram understanding before symbolic derivation. MAZE: 200 maze navigation problems following the protocol of MVoT (Li et al., 2025; Zhang et al., 2025a), generated using maze-dataset (Ivanitskiy et al., 2023), where models choose the final position after executing given action sequence from the starting point on maze image."
        },
        {
            "title": "3.2 Baselines",
            "content": "We compare CoM against four categories of methods: (1) Direct Reasoning: Direct I/O and Zero-shot CoT (Kojima et al., 2022) provide fundamental references for single-pass reasoning without explicit orchestration; (2) Structured Reasoning: Tree of Thoughts (Yao et al., 2023), Chain of Code (Li et al., 2023) represent explicit reasoning organization through predetermined structures but lack dynamic adaptation; (3) Agentic Reasoning: ReAct (Yao et al., 2022) equipped with the same Python interpreter and image generation tools as CoM, enable tool use and iterative refinement yet apply uniform cognitive 7 strategy throughout; (4) Meta-Reasoning: MRP (Gao et al., 2024) selects strategies only at task onset, while Meta-Reasoner (Sui et al., 2025), though step-level, modulates execution parameters rather than cognitive modes. Detailed implementation settings for each baseline are provided in Appendix C. Table 1: Main results on Qwen3-VL-32B-Instruct. We report pass@1 accuracy (%) across six benchmarks. Overall is the arithmetic mean across benchmarks. Bold indicates best; underline indicates second best."
        },
        {
            "title": "Overall",
            "content": "AIME25 Fermi Easy Medium Hard All"
        },
        {
            "title": "GPQA MathV MAZE",
            "content": "Direct I/O Zero-shot CoT Tree of Thoughts Chain of Code ReAct MRP Meta-Reasoner CoM (Ours) 56.67 60.00 50.00 56.67 63.33 60.00 36.67 73.33 42.04 95.56 42.55 97.78 24.00 86.67 21.67 88.89 42.34 88.89 40.82 95.56 38.67 80.00 43.51 93.33 36.36 36.36 34.55 38.18 40.00 36.36 34.55 45.45 39.01 9.76 9.76 39.56 12.20 37.36 10.98 38.46 14.63 40.66 18.29 42.86 7.32 33.52 17.07 44.50 63.64 68.18 56.06 48.00 61.11 68.69 54.55 69. 55.92 51.64 43.75 29.28 56.58 58.55 29.61 63.16 81.50 82.50 68.50 27.50 74.50 79.00 30.50 85.50 56.46 57.41 46.61 36.93 56.42 58.32 37.25 63.28 Table 2: Main results on Gemini-2.0-Flash. We report pass@1 accuracy (%) across six benchmarks. Overall is the arithmetic mean across benchmarks. Bold indicates best; underline indicates second best."
        },
        {
            "title": "Overall",
            "content": "AIME25 Fermi Easy Medium Hard All GPQA MathV MAZE Direct I/O Zero-shot CoT Tree of Thoughts Chain of Code ReAct MRP Meta-Reasoner CoM (Ours) 26.67 23.33 23.33 30.00 23.33 26.67 26.67 33.33 38.60 88.89 40.92 91.11 21.47 60.00 39.80 86.67 37.91 88.89 36.09 95.56 25.31 75.56 43.05 88.89 18.18 21.82 5.45 16.36 40.00 20.00 20.00 36.36 8.54 31.32 6.10 31.87 7.31 19.78 6.10 29.12 14.63 40.66 6.10 32.40 4.88 26.92 9.75 37. 62.63 64.14 46.46 37.40 61.62 65.15 53.54 65.70 48.03 48.36 39.14 22.00 47.37 49.34 21.05 51.00 76.50 76.00 69.50 25.50 71.50 76.50 30.50 84.00 47.29 47.44 36.61 30.64 47.07 47.69 30.67 52.41 3."
        },
        {
            "title": "Implementation Details",
            "content": "We evaluate all methods using two base models. Qwen3-VL-32B-Instruct (Bai et al., 2025) is the state-ofthe-art open-source vision-language model, which we deploy locally on 8NVIDIA A100-80GB GPUs. Gemini-2.0-Flash (Comanici et al., 2025) is Googles high-performance closed-source non-reasoning multimodal model, accessed via OpenRouters Google Vertex API. For generations, both models use temperature 0.7 and top_p 0.95, with max_tokens set to 32768 for Qwen3-VL-32B-Instruct and 8192 for Gemini-2.0-Flash. The Spatial mode in CoM additionally employs Nano-Banana-Pro (Google, 2025) for image generation. The Algorithmic mode executes Python code in sandboxed environment with 30-second timeout; all mindsets share the same base model for fair comparison. We report pass@1 accuracy across all experiments."
        },
        {
            "title": "3.4 Main Results",
            "content": "As shown in Tables 1 and 2, CoM achieves the highest overall accuracy across both base models: 63.28% on Qwen3-VL-32B-Instruct and 52.41% on Gemini-2.0-Flash, outperforming the strongest baseline MRP by 4.96% and 4.72%, respectively. Among direct reasoning methods, Zero-shot CoT provides consistent gains over Direct I/O, while ToT and CoC show task-specific strengths. Meta-reasoning approaches (MRP, Meta-Reasoner) outperform direct methods, yet CoM surpasses them across most benchmarks. The performance gains are most pronounced on tasks requiring flexible mindset adaptation. With Qwen3-VL-32B-Instruct, CoM exceeds the second-best method on AIME25 by 10.00%, demonstrating 8 Table 3: Ablation study on Qwen3-VL-32B-Instruct. Each row removes one component from the full CoM. We report pass@1 accuracy (%); Overall is the arithmetic mean. Superscripts / indicate change relative to full CoM. Bold = best; underline = second best. Variant Math LiveCodeBench Science Multimodal Overall AIME25 Fermi CoM (Full) 73.33 43.51 Easy 93.33 Medium 45.45 Hard 17.07 All GPQA MathV MAZE 44.50 69.70 63.16 85.50 63. w/o Divergent w/o Convergent w/o Algorithmic w/o Spatial 56.6716.66 44.691.18 88.894.44 36.369.09 13.413.66 39.015.49 65.054.65 62.170.99 81.004.50 58.105.18 60.0013.33 45.321.81 91.112.22 45.450.00 13.413.66 42.312.19 65.154.55 60.862.30 83.502.00 59.523.76 70.003.33 43.390.12 88.894.44 47.271.82 13.413.66 42.312.19 64.655.05 60.202.96 84.001.50 60.762.52 70.003.33 41.981.53 84.448.89 38.187.27 15.851.22 39.564.94 63.646.06 53.299.87 81.004.50 58.255.03 w/o Context Gate 53.3320.00 44.881.37 80.0013.33 36.369.09 17.070.00 38.466.04 64.145.56 54.938.23 74.5011.00 55.048.24 the value of multi-path exploration via Divergent mindset. On MAZE spatial reasoning, CoM outperforms MRP on both base models by 6.00% and 7.50%, respectively. CoM also maintains strong code generation performance on LiveCodeBench, where Algorithmic mindset enables precise computation."
        },
        {
            "title": "3.5 Ablation Study",
            "content": "Table 3 presents ablation results by systematically removing each component from full CoM. The Context Gate proves most critical: its removal causes the largest overall drop of 8.24%, confirming that adaptive information filtering between meta-agent and mindset experts is essential for effective coordination. Among the four mindsets, Divergent contributes most to mathematical reasoning, with AIME25 accuracy dropping 16.66% upon removal, while Spatial shows the largest impact on visual tasks, reducing MathVision by 9.87% and MAZE by 4.50%. Algorithmic primarily benefits code generation, with LiveCodeBench All dropping 2.19% when removed. On Fermi estimation, removing Divergent (+1.18%), Convergent (+1.81%), or Context Gate (+1.37%) all yield slight improvements, while only Algorithmic and Spatial mindsets remain essential. This pattern suggests that Fermis order-of-magnitude reasoning benefits more from focused computation rather than multi-path exploration. The finding points to promising research direction: task-aware mindset subsetting, where minimal effective mindset subset is pre-selected based on problem characteristics, may offer substantial efficiency gains without sacrificing accuracy."
        },
        {
            "title": "3.6 Analysis",
            "content": "Method Efficiency Comparison We compare CoM against baselines in terms of overall accuracy and token consumption (Figure 5a). Direct methods (Direct I/O, Zero-shot CoT) are most token-efficient but sacrifice substantial accuracy. Tree of Thoughts incurs prohibitive computational cost (142.5k tokens on average) due to exhaustive branch exploration, yet still underperforms CoM in accuracy. MetaReasoner also consumes high tokens (49.7k) with relatively low accuracy of 37.25%. CoM achieves the best accuracy (63.28%) at moderate cost (28.4k tokens), positioning it on the Pareto frontier of the accuracy-efficiency space. Ablation Efficiency Beyond accuracy, we examine the computational cost of each ablation variant (Figure 5b). Removing the Context Gate increases token consumption by 87% despite degraded accuracy, as the orchestrator loses its ability to filter irrelevant context. Removing Divergent mode reduces tokens by 26% with moderate accuracy loss, viable option for efficiency-critical deployments. The full CoM achieves the best overall accuracy-efficiency trade-off. Mindset Invocation Patterns To understand how CoM orchestrates mindsets, we analyze invocation patterns by parsing call sequences recorded during inference. Table 4 reports the percentage of problems invoking each mindset at least once. Overall, 59.7% of problems invoke two or more distinct mindsets, validating multi-mindset collaboration. Clear task-specific patterns emerge: Fermi estimation relies heavily on Algorithmic (91.2%) combined with Convergent (78.3%), reflecting its need for numerical computation and step-by-step analysis. Code generation similarly favors Convergent-Algorithmic combinations, with 9 (a) Accuracy-efficiency trade-off across methods on Qwen3VL-32B-Instruct. Each point represents methods overall accuracy (%) vs. average token consumption (k). CoM achieves the highest accuracy at moderate cost, dominating the Pareto frontier. Methods in the upper-left region are preferable. (b) Ablation efficiency trade-off on Qwen3-VL-32B-Instruct. Each point shows CoM variants accuracy vs. token cost. Removing Context Gate () dramatically increases tokens (+87%) while degrading accuracy. Removing Divergent mindset offers the best token savings (26%) with moderate accuracy loss. Figure 5: Our method achieves state-of-the-art performance while balancing reasoning efficiency. Table 4: Mindset invocation frequency (%) on Qwen3-VL-32B-Instruct. Each cell shows the percentage of problems where the mindset is invoked at least once. Multi denotes problems invoking two or more distinct mindsets. LCB denotes LiveCodeBench. Overall is the weighted average by problem count. Benchmark Div. Conv. Algo. Spat. Multi AIME25 Fermi LCB GPQA MathV MAZE Overall 10.0 70.2 4.4 23.2 7.6 0.0 34.8 66.7 78.3 40.1 74.7 22.4 4.0 54.5 43.3 91.2 60.4 39.4 33.6 39. 63.6 23.3 13.3 2.2 14.6 80.6 100.0 33.1 43.3 88.7 22.5 51.0 38.8 40.0 59.7 60.4% invoking Algorithmic. Multimodal tasks uniquely leverage SpatialMathVision at 80.6% and MAZE at 100%demonstrating that CoM adaptively activates visual reasoning for geometric structures."
        },
        {
            "title": "4 Conclusion",
            "content": "We introduced Chain of Mindset (CoM), training-free agentic framework that enables step-level adaptive mindset orchestration for LLM reasoning. Unlike existing methods that apply fixed cognitive strategy throughout problem-solving, CoM dynamically selects among four functionally heterogeneous mindsets, namely Divergent, Convergent, Algorithmic, and Spatial, through Meta-Agent that responds to the evolving problem state. bidirectional Context Gate ensures efficient information flow while maintaining focus across mindset transitions. Extensive experiments across six challenging benchmarks demonstrate that CoM achieves state-of-the-art accuracy, outperforming the strongest baselines by 4.96% and 4.72% on Qwen3-VL-32B-Instruct and Gemini-2.0-Flash, respectively. Notably, CoM maintains computational efficiency and generalizes consistently across both open-source and closed-source models without requiring additional training. These results suggest that enabling dynamic cognitive switching, which mirrors how humans naturally integrate multiple reasoning modalities within single problem-solving episode, represents promising paradigm for building more adaptable reasoning systems."
        },
        {
            "title": "Impact Statement",
            "content": "This work advances AI systems that reason more like humans, not by scaling parameters, but by introducing structured cognitive flexibility. CoM shows that orchestrating heterogeneous thinking modes unlocks capabilities beyond single-mode prompting or static strategy selection. Scientifically, our framework provides testbed for studying interactions among cognitive paradigms, informing both AI and cognitive science. The modular, training-free architecture enables rapid experimentation with new mindsets and policies. We believe meta-cognitive controlteaching models to reason about reasoningis promising path toward more general intelligence. By making reasoning transparent, CoM enables users to inspect and guide cognitive trajectories, supporting AI that augments human judgment. Regarding safety, explicit reasoning traces enhance auditability, and the structured switching mechanism offers opportunities for targeted safety interventions."
        },
        {
            "title": "References",
            "content": "Simon Aytes, Jinheon Baek, and Sung Ju Hwang. 2025. Sketch-of-thought: Efficient llm reasoning with adaptive cognitive-inspired sketching. arXiv preprint arXiv:2503.05179. Shuai Bai, Yuxuan Cai, Ruizhe Chen, Keqin Chen, Xionghui Chen, Zesen Cheng, Lianghao Deng, Wei Ding, Chang Gao, Chunjiang Ge, Wenbin Ge, Zhifang Guo, Qidong Huang, Jie Huang, Fei Huang, Binyuan Hui, Shutong Jiang, Zhaohai Li, Mingsheng Li, and 45 others. 2025. Qwen3-vl technical report. arXiv preprint arXiv:2511.21631. Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Hubert Niewiadomski, Piotr Nyczyk, and 1 others. 2024. Graph of thoughts: Solving elaborate problems with large language models. In Proceedings of the AAAI conference on artificial intelligence, volume 38, pages 1768217690. Wenhu Chen, Xueguang Ma, Xinyi Wang, and William Cohen. 2022. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. arXiv preprint arXiv:2211.12588. Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, and 1 others. 2025. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261. Arthur Cropley. 2006. In praise of convergent thinking. Creativity research journal, 18(3):391404. Aniket Didolkar, Anirudh Goyal, Nan Rosemary Ke, Siyuan Guo, Michal Valko, Timothy Lillicrap, Danilo Jimenez Rezende, Yoshua Bengio, Michael Mozer, and Sanjeev Arora. 2024. Metacognitive capabilities of llms: An exploration in mathematical problem solving. Advances in Neural Information Processing Systems, 37:1978319812. Gerald Futschek. 2006. Algorithmic thinking: the key for understanding computer science. In International conference on informatics in secondary schools-evolution and perspectives, pages 159168. Springer. Kanishk Gandhi, Ayush Chakravarthy, Anikait Singh, Nathan Lile, and Noah Goodman. 2025. Cognitive behaviors that enable self-improving reasoners, or, four habits of highly effective stars. arXiv preprint arXiv:2503.01307. Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. 2023. Pal: Program-aided language models. In International Conference on Machine Learning, pages 1076410799. PMLR. Peizhong Gao, Ao Xie, Shaoguang Mao, Wenshan Wu, Yan Xia, Haipeng Mi, and Furu Wei. 2024. Meta reasoning for large language models. arXiv preprint arXiv:2406.11698. Google. 2025. Introducing Nano Banana Pro. https://blog.google/technology/ai/nano-banana-pro/. Joy Paul Guilford. 1967. The nature of human intelligence. Yifu Guo, Zishan Xu, Zhiyuan Yao, Yuquan Lu, Jiaye Lin, Sen Hu, Zhenheng Tang, Huacan Wang, and Ronghao Chen. 2025. Octopus: Agentic multimodal reasoning with six-capability orchestration. arXiv preprint arXiv:2511.15351. 11 Michael Igorevich Ivanitskiy, Rusheb Shah, Alex Spies, Tilman Räuker, Dan Valentine, Can Rager, Lucia Quirke, Chris Mathwin, Guillaume Corlouer, Cecilia Diniz Behn, and 1 others. 2023. configurable library for generating and manipulating maze datasets. arXiv preprint arXiv:2309.10498. Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. 2024. Livecodebench: Holistic and contamination free evaluation of large language models for code. arXiv preprint arXiv:2403.07974. Ashwin Kalyan, Abhinav Kumar, Arjun Chandrasekaran, Ashish Sabharwal, and Peter Clark. 2021. How much coffee was consumed during emnlp 2019? fermi problems: new reasoning challenge for ai. arXiv preprint arXiv:2110.14207. Priyanka Kargupta, Shuyue Stella Li, Haocheng Wang, Jinu Lee, Shan Chen, Orevaoghene Ahia, Dean Light, Thomas Griffiths, Max Kleiman-Weiner, Jiawei Han, and 1 others. 2025. Cognitive foundations for reasoning and their manifestation in llms. arXiv preprint arXiv:2511.16660. Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, and Ashish Sabharwal. 2022. Decomposed prompting: modular approach for solving complex tasks. arXiv preprint arXiv:2210.02406. Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35:2219922213. Chengshu Li, Jacky Liang, Andy Zeng, Xinyun Chen, Karol Hausman, Dorsa Sadigh, Sergey Levine, Li Fei-Fei, Fei Xia, and Brian Ichter. 2023. Chain of code: Reasoning with language model-augmented code emulator. arXiv preprint arXiv:2312.04474. Chengzu Li, Wenshan Wu, Huanyu Zhang, Yan Xia, Shaoguang Mao, Li Dong, Ivan Vulic, and Furu Wei. 2025. Imagine while reasoning in space: Multimodal visualization-of-thought. arXiv preprint arXiv:2501.07542. Tianle Li, Ge Zhang, Quy Duc Do, Xiang Yue, and Wenhu Chen. 2024. Long-context llms struggle with long in-context learning. arXiv preprint arXiv:2404.02060. Weifeng Lin, Xinyu Wei, Ruichuan An, Peng Gao, Bocheng Zou, Yulin Luo, Siyuan Huang, Shanghang Zhang, and Hongsheng Li. 2024. Draw-and-understand: Leveraging visual prompts to enable mllms to comprehend what you want. arXiv preprint arXiv:2403.20271. Weifeng Lin, Xinyu Wei, Ruichuan An, Tianhe Ren, Tingwei Chen, Renrui Zhang, Ziyu Guo, Wentao Zhang, Lei Zhang, and Hongsheng Li. 2025. Perceive anything: Recognize, explain, caption, and segment anything in images and videos. arXiv preprint arXiv:2506.05302. Nelson Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2024. Lost in the middle: How language models use long contexts. Transactions of the association for computational linguistics, 12:157173. Nora Newcombe. 2010. Picture this: Increasing math and science learning by improving spatial thinking. American educator, 34(2):29. Nora Newcombe and Thomas Shipley. 2014. Thinking about spatial thinking: New typology, new assessments. In Studying visual and spatial reasoning for design creativity, pages 179192. Springer. Allen Newell, Herbert Alexander Simon, and 1 others. 1972. Human problem solving, volume 104. Prentice-hall Englewood Cliffs, NJ. Randall OReilly and Michael Frank. 2006. Making working memory work: computational model of learning in the prefrontal cortex and basal ganglia. Neural computation, 18(2):283328. Liangming Pan, Alon Albalak, Xinyi Wang, and William Wang. 2023. Logic-lm: Empowering large language models with symbolic solvers for faithful logical reasoning. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 38063824. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian In First Michael, and Samuel Bowman. 2024. Gpqa: graduate-level google-proof q&a benchmark. Conference on Language Modeling. Mark Runco and Selcuk Acar. 2012. Divergent thinking as an indicator of creative potential. Creativity research journal, 24(1):6675. 12 Anthony Sali, Christina Bejjani, and Tobias Egner. 2024. Learning cognitive flexibility: Neural substrates of adapting switch-readiness to time-varying demands. Journal of cognitive neuroscience, 36(2):377393. Yuan Sui, Yufei He, Tri Cao, Simeng Han, Yulin Chen, and Bryan Hooi. 2025. Meta-reasoner: Dynamic guidance for optimized inference-time reasoning in large language models. arXiv preprint arXiv:2502.19918. Ke Wang, Junting Pan, Weikang Shi, Zimu Lu, Houxing Ren, Aojun Zhou, Mingjie Zhan, and Hongsheng Li. 2024. Measuring multimodal mathematical reasoning with math-vision dataset. Advances in Neural Information Processing Systems, 37:9509595169. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2022. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, and 1 others. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837. Ling Yang, Zhaochen Yu, Tianjun Zhang, Shiyi Cao, Minkai Xu, Wentao Zhang, Joseph Gonzalez, and Bin Cui. 2024. Buffer of thoughts: Thought-augmented reasoning with large language models. Advances in Neural Information Processing Systems, 37:113519113544. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. 2023. Tree of thoughts: Deliberate problem solving with large language models. Advances in neural information processing systems, 36:1180911822. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2022. React: Synergizing reasoning and acting in language models. In The eleventh international conference on learning representations. Huanyu Zhang, Wenshan Wu, Chengzu Li, Ning Shang, Yan Xia, Yangyu Huang, Yifan Zhang, Li Dong, Zhang Zhang, Liang Wang, and 1 others. 2025a. Latent sketchpad: Sketching visual thoughts to elicit multimodal reasoning in mllms. arXiv preprint arXiv:2510.24514. Ziying Zhang, Yaqing Wang, and Quanming Yao. 2025b. Searching meta reasoning skeleton to guide llm reasoning. arXiv preprint arXiv:2510.04116."
        },
        {
            "title": "A Related Work",
            "content": "A.1 Cognitive Behaviors in LLM Reasoning Recent research has identified distinct cognitive behaviors in LLM reasoning. Didolkar et al. (Didolkar et al., 2024) showed that LLMs can identify required skill labels and leverage this self-knowledge to improve performance. Gandhi et al. (Gandhi et al., 2025) identified four key cognitive behaviorsverification, backtracking, subgoal setting, and backward chainingas critical enablers of selfimprovement. Kargupta et al. (Kargupta et al., 2025) introduced taxonomy of 28 cognitive elements and found that models tend to adopt rigid sequential processing rather than diverse metacognitive monitoring. These works demonstrate that intervening on cognitive behaviors can enhance reasoning, but how to adaptively select the most suitable mindset based on context remains open. A.2 Prompt-based Reasoning Prompt-based reasoning methods can be categorized into two classes: explicit intermediate step generation and reasoning structure expansion. The former is exemplified by Chain-of-Thought prompting (Wei et al., 2022), which improves complex problem-solving by guiding models to generate intermediate steps; Decomposed Prompting (Khot et al., 2022) further decomposes tasks into subtasks delegated to specialized submodules. The latter explores richer reasoning topologies: Program-of-Thoughts (Chen et al., 2022) and Chain-of-Code (Li et al., 2023) introduce code execution to offload computation; Tree-of-Thoughts (Yao et al., 2023) and Graph-of-Thoughts (Besta et al., 2024) employ branching and arbitrary graph structures for multi-path reasoning respectively. These prompting methods enrich reasoning structure and modalities, yet employ single mindset throughout the task lifecyclethe model remains locked within predetermined framework. Our method is complementary: while preserving the advantages of prompt-based reasoning, it allows dynamic switching between different mindsets. A.3 Meta-Reasoning Meta-reasoningreasoning about how to reasonhas emerged as key paradigm for adaptive strategy selection in LLMs. Existing approaches can be categorized into task-level and step-level methods. Task-level meta-reasoning selects strategy at problem onset and maintains it throughout: Buffer of Thoughts (Yang et al., 2024) retrieves high-level thought templates from memory library, while MRP (Gao et al., 2024) and Sketch-of-Thought (Aytes et al., 2025) select the most suitable reasoning paradigm based on problem characteristics. These methods achieve cross-task adaptability but cannot respond to heterogeneous demands of different subtasks within the same problem. Step-level meta-reasoning attempts finer-grained intervention: Meta-Reasoner (Sui et al., 2025) dynamically schedules execution actions such as backtracking during reasoning; AutoMR (Zhang et al., 2025b) searches for query-aware meta-reasoning skeletons by dynamically expanding DAG structures. Concurrently, Octopus (Guo et al., 2025) proposes agentic multimodal reasoning with six-capability orchestration, enabling autonomous capability selection during inference. However, such methods modulate execution parameters or reasoning structures rather than mindsets themselves. Unlike the above work, our method achieves step-level meta-reasoning over functionally heterogeneous mindsets, dynamically determining thinking styles based on subtask context without additional training."
        },
        {
            "title": "B Future Directions",
            "content": "Our framework instantiates four mindsets representing well-established cognitive primitives. Future work could incorporate additional primitives via our plug-and-play architecture. Currently, all mindsets share the same base model; natural extension is heterogeneous expert allocation, where each mindset is powered by specialized model. Additionally, equipping mindsets with tailored tools (e.g., symbolic solvers for Algorithmic, search tools for Convergent) could further enhance capabilities. Finally, optimizing the Meta-Agents dispatch policy through training could further improve performance."
        },
        {
            "title": "C Baseline Implementation Details",
            "content": "For reproducibility, we provide the implementation details of all baseline methods used in our experiments. All methods are evaluated under identical inference settings (temperature, maximum tokens) to ensure fair comparison. Direct I/O. Direct I/O queries the model without any reasoning guidance or system prompt. The model receives only the question and format instruction (if provided by the dataset), representing the minimal baseline for comparison. Prompt template: \"Question: {question} Answer:\" Zero-shot CoT. Zero-shot CoT (Kojima et al., 2022) elicits chain-of-thought reasoning by appending the canonical trigger phrase to the question. Following the original paper, we use: \"Question: {question} Lets think step by step.\" Tree of Thoughts. We implement Tree of Thoughts (Yao et al., 2023) with BFS/Beam Search strategy. The model decomposes problems into sub-questions, generates = 3 candidate thoughts per step, evaluates each candidates usefulness and correctness via self-evaluation, and selects the best branch to expand. Maximum reasoning depth is set to 10 steps. Chain of Code. We implement Chain of Code (Li et al., 2023) following the original paper. The model generates Python code to solve problems and simulates code execution. If actual execution fails (timeout or exception), the models predicted output is used as fallback. Execution timeout is set to 10 seconds. ReAct. We implement the ReAct framework (Yao et al., 2022) with the standard Thought-ActionObservation loop. For fair comparison, we equip ReAct with the same tool set as CoM: (1) PythonSandbox: for code execution and numerical computation (timeout: 30 seconds); (2) ImageGeneration: for visualization using the same image generation API as CoMs Spatial mode. Maximum interaction turns is set to 10. MRP. MRP (Gao et al., 2024) does not have open-source code, but provides prompts in the original paper. We follow the paper to implement MRP. At reasoning onset, the model analyzes problem characteristics through meta-reasoning, rates the suitability of each method (Chain-of-Thoughts, Tree-of-Thoughts, Analogical Prompting, Self-Refine, Step-Back Prompting, Solo Performance Prompting, SimTom) on 17 scale, and selects the highest-scoring method for execution. Meta-Reasoner. Meta-Reasoner (Sui et al., 2025) does not have open-source code, but provides prompts, pseudo code, and detailed description in the original paper. We follow the paper to implement Meta-Reasoner. It uses contextual multi-armed bandits to dynamically select control actions (continue, backtrack, restart, etc.) during reasoning, with exploration rate ϵ = 0.1."
        },
        {
            "title": "D Chain of Mindsets Prompt Templates",
            "content": "We provide the complete prompt templates used in Chain of Mindsets (CoM). Our framework consists of Main Agent (Meta-Cognitive Orchestrator), four specialized Mindset Experts, and Context Gates for information filtering. D.1 Meta-Agent Meta-Agent System Prompt (cid:212) Role Definition You are Meta-Cognitive Orchestrator. You decide HOW to think, not WHAT to think. Delegate all reasoning to cognitive modules. (cid:243) Cognitive Modules (cid:143) Algorithmic <call_algorithmic>...</call_algorithmic>: Precise calculations and verifications (cid:143) Spatial <call_spatial>...</call_spatial>: Structures and spatial relationships (cid:143) Divergent <call_divergent>...</call_divergent>: Multiple solution paths in parallel (cid:143) Convergent <call_convergent>...</call_convergent>: Deep logical analysis on sub-question Reference images as [IMG_001], [GEN_001] when relevant. (cid:22) Protocol Be concise. Never reason in <cognitive_decision> only plan which mindsets to use. Be concise. Never reason in <call_xxx> only call. Execute mindsets in planned order. Monitor history; revise unexecuted plan anytime via <cognitive_decision>. (cid:143) <cognitive_decision>...</cognitive_decision> Identify problem type + plan mindsets. No solving. (cid:143) <call_xxx>...</call_xxx> <xxx_result>...</xxx_result> <insight>...</insight> Call, receive, internalize briefly. (cid:143) <Answer>...</Answer> Final response. (cid:17) Example Q: bee flies between two trains 300km apart (speeds 60, 90 km/h) at 120 km/h until they meet. Distance? <cognitive_decision> Pursuit problem with oscillation. Convergent Algorithmic Convergent. </cognitive_decision> <call_convergent>Model the bees path.</call_convergent> <convergent_result>Infinite series: sum segments as bee bounces.</convergent_result> <insight>Series approach identified.</insight> <call_algorithmic>Compute first segments.</call_algorithmic> <algorithmic_result>Seg1: 144km, Seg2: 57.6km... continues.</algorithmic_result> <insight>Tedious. Simpler way?</insight> <cognitive_decision> Method too complex. Divergent Algorithmic. </cognitive_decision> <call_divergent>Alternative approaches?</call_divergent> <divergent_result>A: Sum series. B: Total flight time = meeting time. C: Relative velocity.</divergent_result> <insight>B: just compute meeting time.</insight> <call_algorithmic>300/(60+90) = 2h. 120 2 = ?</call_algorithmic> <algorithmic_result>240 km.</algorithmic_result> <insight>Done.</insight> <Answer>240 km</Answer> Begin. D.2 Mindset Experts Algorithmic Mindset. The Algorithmic Mindset handles precise calculations and code-based verifications. It generates executable Python code and supports self-correction on errors. 16 Algorithmic Mindset: Code Generation Prompt Task You are verifying through computation. {instruction} Instructions Write executable Python code that solves the task precisely. Print the result to stdout. Algorithmic Mindset: Code Fix Prompt The previous code failed. Code {code} . Error {error} (cid:229) Action Fix the code. Preserve the original intent. Convergent Mindset. The Convergent Mindset performs deep logical analysis on focused questions, emphasizing rigorous reasoning grounded in established facts. Convergent Mindset System Prompt Role You are reasoning deeply. (cid:22) Guidelines (cid:143) Focus all attention on the given question. (cid:143) Ground each step in established facts. (cid:143) If information is insufficient, state what is missing. (cid:143) Reach clear conclusion. Divergent Mindset. The Divergent Mindset explores multiple solution paths in parallel. It first generates diverse approaches, then performs deep-dive exploration on each branch. Divergent Mindset: Branch Generation Prompt Task You are exploring possibilities. {instruction} Generate 25 genuinely different approaches. Each approach should differ in method, not just phrasing. Output Format 17 <branch id=\"A\"> [Approach name] Method: [How it works] When applicable: [Conditions] </branch> <branch id=\"B\"> ... </branch> Divergent Mindset: Branch Exploration Prompt Task You are exploring one approach in depth. ı Context Problem: {instruction} Approach: {branch_description} (cid:22) Examination Steps 1. What assumptions does it make? Are they satisfied? 2. How would it work step by step? 3. What are the limitations? 4. Is it viable for this problem? Be honest about limitations. Spatial Mindset. The Spatial Mindset handles visual-spatial thinking, transforming abstract descriptions into visual representations. Unlike other mindsets that use explicit prompts, the Spatial Mindset directly routes the processed context to an image generation model. The Main Agents call instruction (e.g., Visualize the geometric relationship) is first processed by the Input Gate, which extracts relevant context and decides which reference images to inject. The combined context and instruction are then sent to an image generation API (we use Nano Banana Pro with native image generation capabilities). The generated image is saved to the session workspace, and the Output Gate extracts the image path along with any accompanying notes for the Main Agent. Supported Modes: Text Image: Pure text description generates visualization Image + Text Image: Reference images [IMG_XXX] are injected for editing/redrawing Text/(Image + Text) Code Image: If the API returns matplotlib code instead of an image, the code is executed in sandbox to generate the figure D.3 Context Gates The Context Gate implements cognitive gating mechanism inspired by the PBWM (Prefrontal Cortex Basal Ganglia Working Memory) model from cognitive neuroscience (OReilly and Frank, 2006). The same call serves as the anchor point for both directions, ensuring information relevance throughout the cognitive loop. Input Gate. The Input Gate filters information from Main Agent history to extract only what the specialized Mindset needs, and decides which images to inject based on semantic relevance. 18 Input Gate Prompt Role You are the attentional filter of cognitive agent. Extract what the specialized thinking needs to execute the instruction. ı Input Context Instruction: {call} History: {source_history} Available Images: {available_images_description} Target: {target_description} (cid:22) Extraction Rules (cid:143) Keep verbatim: numbers, data, coordinates, prior results, text being analyzed. (cid:143) Summarize: reasoning chains conclusions only. (cid:143) Omit: the original user question (the thinking sees only its sub-task). º Image Decision (cid:143) Explicit [IMG_XXX] in instruction inject those (cid:143) the figure/image without marker inject most relevant (cid:143) Purely textual task inject nothing Output Format (JSON only) {context_text: extracted context or empty string, inject_images: [IMG_001] or []} Input Gate: Target Descriptions (cid:143) Algorithmic Mindset executes precise calculations and code-based verifications (cid:143) Convergent Mindset performs deep logical analysis on focused questions (cid:143) Divergent Mindset explores multiple approaches and alternatives in parallel (cid:143) Spatial Mindset creates and analyzes visual-spatial representations Output Gate. The Output Gate extracts results from Mindset execution that advance the main reasoning, filtering out derivation steps and failed attempts. Output Gate Prompt Role You are the attentional filter of cognitive agent. Extract the results that advance the main reasoning. ı Input Context Instruction: {call} Execution Record: {mindset_output} New Artifacts: {new_artifacts_description} (cid:22) Extraction Rules (cid:143) Keep: computed values, discovered patterns, conclusions, generated image paths. (cid:143) Omit: derivation steps, failed attempts. Extracted Results"
        },
        {
            "title": "E Case Studies",
            "content": "We present two additional representative case studies demonstrating Chain of Mindsets (CoM) across different problem types: (1) mathematical reasoning with dynamic re-planning, and (2) multimodal geometry with visual input. Special tokens are highlighted (e.g., <cognitive_decision> , <call_convergent> ) to show the meta-cognitive control flow. An additional Fermi estimation example demonstrating the Spatial Mindsets image generation capability is provided in Section 2.5. E.1 Case Study 1: Mathematical Reasoning with Dynamic Re-planning (AIME) AIME 2025: Base Divisibility Question: Find the sum of all integer bases > 9 for which 17b is divisor of 97b. Answer: 70 <cognitive_decision> Base conversion followed by divisibility check. Plan: Convergent Algorithmic. </cognitive_decision> <call_convergent> Express 17b and 97b in base 10. </call_convergent> <convergent_result> 17b = b+7, 97b = 9b+7. Condition: (b+7) (9b+7). </convergent_result> <insight> The divisibility condition (b + 7) (9b + 7) can be simplified algebraically before testing values. </insight> <cognitive_decision> Remaining plan was Algorithmic (test bases). But the condition can be reduced first. Revise to: Convergent Algorithmic. </cognitive_decision> <call_convergent> Simplify (b + 7) (9b + 7). </call_convergent> <convergent_result> 9b + 7 = 9(b + 7) 56, so (b + 7) 56. </convergent_result> <insight> Now find divisors of 56 greater than 16 (since = 7 > 9 requires > 16). </insight> <call_algorithmic> Divisors of 56: {1,2,4,7,8,14,28,56}. Filter > 16: {28, 56}. Compute = 7. </call_algorithmic> <algorithmic_result> = 21 and = 49. Sum: 21 + 49 = 70. </algorithmic_result> <insight> Final answer: 70. </insight> <Answer> 70 </Answer> This example illustrates CoMs core capability: state-dependent cognitive switching. Unlike static meta-reasoning that commits to fixed strategy, CoM monitors intermediate results and dynamically revises its plan when more efficient path emerges. Here, the first Convergent call formulates the divisibility condition, and the insight recognizes that algebraic simplification should precede enumeration. The second Convergent call then reduces (b + 7) (9b + 7) to (b + 7) 56, enabling efficient computation via Algorithmic mindset. 20 E.2 Case Study 2: Multimodal Geometry with Visual Input (MathVision) MathVision #1167: Zig-Zag Angles [IMG_001] Question: Valentin draws zig-zag line inside rectangle using angles 10, 14, 33 and 26. How big is angle φ? Options: A. 11 B. 12 C. 16 D. 17 Answer: E. <cognitive_decision> Geometry problem with angles in zig-zag path. Plan: Convergent Algorithmic. </cognitive_decision> <call_convergent> What geometric relationship governs the zig-zag angles? </call_convergent> <convergent_result> Net direction change = 33 10 = 23. Net turning = φ 21. Equation: φ 21 = 23 φ = 44. </convergent_result> <insight> 44 not in options. The model is flawed. </insight> <cognitive_decision> Initial approach failed. Need alternative geometric principle. Revise to: Divergent Algorithmic. </cognitive_decision> <call_divergent> Alternative geometric principles for zig-zag in rectangle? </call_divergent> <divergent_result> A: Cumulative direction change already failed. B: Zig-zag theorem: sum of left-pointing angles = sum of right-pointing angles. C: Exterior angle sum not applicable. </divergent_result> <insight> Branch B: Zig-zag theorem looks promising. </insight> <call_algorithmic> Apply zig-zag theorem. Right angles: 10 + φ + 26. Left angles: 14 + 33. Solve. </call_algorithmic> <algorithmic_result> 36 + φ = 47 φ = 11. </algorithmic_result> <insight> 11 matches option A. Done. </insight> <Answer> </Answer> This example demonstrates CoMs error recovery through mindset switching. When the initial Convergent approach yields an answer (44) absent from the options, the insight mechanism detects the inconsistency and triggers re-planning. The subsequent Divergent call generates alternative geometric principles, among which the zig-zag theorem proves viable. The Algorithmic mindset then executes the correct calculation, illustrating how CoM leverages mindset diversity to escape reasoning dead-ends."
        }
    ],
    "affiliations": [
        "BJTU",
        "NUS",
        "PKU",
        "QuantaAlpha",
        "SUFE",
        "StepFun"
    ]
}