{
    "paper_title": "UniGenBench++: A Unified Semantic Evaluation Benchmark for Text-to-Image Generation",
    "authors": [
        "Yibin Wang",
        "Zhimin Li",
        "Yuhang Zang",
        "Jiazi Bu",
        "Yujie Zhou",
        "Yi Xin",
        "Junjun He",
        "Chunyu Wang",
        "Qinglin Lu",
        "Cheng Jin",
        "Jiaqi Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent progress in text-to-image (T2I) generation underscores the importance of reliable benchmarks in evaluating how accurately generated images reflect the semantics of their textual prompt. However, (1) existing benchmarks lack the diversity of prompt scenarios and multilingual support, both essential for real-world applicability; (2) they offer only coarse evaluations across primary dimensions, covering a narrow range of sub-dimensions, and fall short in fine-grained sub-dimension assessment. To address these limitations, we introduce UniGenBench++, a unified semantic assessment benchmark for T2I generation. Specifically, it comprises 600 prompts organized hierarchically to ensure both coverage and efficiency: (1) spans across diverse real-world scenarios, i.e., 5 main prompt themes and 20 subthemes; (2) comprehensively probes T2I models' semantic consistency over 10 primary and 27 sub evaluation criteria, with each prompt assessing multiple testpoints. To rigorously assess model robustness to variations in language and prompt length, we provide both English and Chinese versions of each prompt in short and long forms. Leveraging the general world knowledge and fine-grained image understanding capabilities of a closed-source Multi-modal Large Language Model (MLLM), i.e., Gemini-2.5-Pro, an effective pipeline is developed for reliable benchmark construction and streamlined model assessment. Moreover, to further facilitate community use, we train a robust evaluation model that enables offline assessment of T2I model outputs. Through comprehensive benchmarking of both open- and closed-sourced T2I models, we systematically reveal their strengths and weaknesses across various aspects."
        },
        {
            "title": "Start",
            "content": "JOURNAL OF LATEX CLASS FILES 1 UniGenBench++: Unified Semantic Evaluation Benchmark for Text-to-Image Generation Yibin Wang1,2,3*, Zhimin Li3*, Yuhang Zang4*, Jiazi Bu4,5, Yujie Zhou4,5, Yi Xin2, Junjun He2,4, Chunyu Wang3, Qinglin Lu3, Cheng Jin1,2, Jiaqi Wang2 1Fudan University, 2Shanghai Innovation Institute 3Hunyuan, Tencent, 4Shanghai AI Lab, 5Shanghai Jiaotong University Project Page: codegoat24.github.io/UniGenBench 5 2 0 2 1 ] . [ 1 1 0 7 8 1 . 0 1 5 2 : r Fig. 1: Benchmark Overview. (1) Our UNIGENBENCH++ covers diverse prompt themes, subjects, and comprehensive evaluation criteria. (2) Each prompt includes multiple test points and is assessed through streamlined MLLM-based pipeline for reliable and efficient evaluation. (3) We conduct comprehensive evaluations of both openand closed-source models using both English and Chinese prompts in short and long forms, systematically revealing their strengths and weaknesses across various aspects. AbstractRecent progress in text-to-image (T2I) generation underscores the importance of reliable benchmarks in evaluating how accurately generated images reflect the semantics of their textual prompt. However, (1) existing benchmarks lack the diversity of prompt scenarios and multilingual support, both essential for real-world applicability; (2) they offer only coarse evaluations across primary dimensions, covering narrow range of sub-dimensions, and fall short in fine-grained subdimension assessment. To address these limitations, we introduce UNIGENBENCH++, unified semantic assessment benchmark for T2I generation. Specifically, it comprises 600 prompts organized hierarchically to ensure both coverage and efficiency: (1) spans across diverse real-world scenarios, i.e., 5 main prompt themes and 20 subthemes; (2) comprehensively probes T2I models semantic consistency over 10 primary and 27 sub evaluation criteria, with each prompt assessing multiple test points. To rigorously assess model robustness to variations in language and prompt length, we provide both English and Chinese versions of each prompt in short and long forms. Leveraging the general world knowledge and finegrained image understanding capabilities of closed-source Multimodal Large Language Model (MLLM), i.e., Gemini-2.5-Pro, an effective pipeline is developed for reliable benchmark construction and streamlined model assessment. Moreover, to further facilitate community use, we train robust evaluation model that enables offline assessment of T2I model outputs. Through comprehensive benchmarking of both openand closed-source T2I models, we systematically reveal their strengths and weaknesses across various aspects. Index TermsText-to-image generation, semantic generation evaluation, and benchmark. JOURNAL OF LATEX CLASS FILES 2 TABLE SEMANTIC EVALUATION BENCHMARK COMPARISON. - INDICATES THAT THE ASPECT IS NOT DISCUSSED IN ITS ORIGINAL PAPER. Benchmark GenEval T2I-CompBench++ DPG-Bench WISE TIIF-Bench UniGenBench++ (Ours) Primary Dimension Sub Dimension Prompt Theme Prompt Length Prompt Num. Multi-Testpoint per Prompt Multilingual Support Dedicated Offline Eval Model 6 8 5 6 9 10 - - - - - 27 - - - - - short short long short short/long short/long 553 2,400 1,065 1,000 5,000 600 12 110 I. INTRODUCTION ECENT progress in text-to-image (T2I) generation [1] [19] has highlighted the ability to generate high-quality images directly from natural language descriptions. Technically, current T2I models can be broadly divided into two paradigms. (1) Diffusion-based methods, including Stable Diffusion [2], [5], Playground [16], and FLUX [9], [19], iteratively refine Gaussian noise using U-Net or Transformer backbones to generate images. (2) Autoregressive (AR) approaches, such as Infinity [20], Janus series [21][23], and BLIP3-o [24], treat images as token sequences and synthesize them via next-token prediction or progressive scaling. Recent methods incorporate learning [25][28] to improve T2I models reinforcement instruction following capability [29], [30] and the visual quality of generated images [31][33]. With these rapid advancements, assessing T2I models, particularly their semantic generation capability, i.e., how accurately generated images reflect the semantics of their textual prompt, has emerged as critical challenge. Traditional benchmarks [34], [35] typically evaluate T2I models by probing various compositional generation and employ CLIP-based metrics for quantitative assessment. However, CLIP-based scorers remain limited in capturing the finegrained semantic information and complex world knowledge or logical reasoning. Therefore, several studies [36], [37] evaluate the implicit semantic understanding and world knowledge integration capabilities of T2I models using powerful visuallanguage models (VLMs) [38] as the evaluator. Recent efforts broaden T2I evaluation by incorporating long-prompt semantics generation [39], [40] and additional evaluation dimensions [40] such as style and text generation. Despite effectiveness, as shown in Tab. I, these benchmarks encounter two key limitations: (1) Coarse evaluation on limited dimensions: cover limited general dimensions (e.g., lacking grammar, action), within which the sub-dimension coverage is also limited (e.g., lacking relation-similarity, inclusion), and incapable of fine-grained assessment for each sub-dimension; (2) Lacking diversity of prompt scenarios and multilingual evaluation: only focus on evaluation dimension design but neglect the diversity of prompt scenarios and multilingual evaluation support, hindering comprehensive assessment in real-world applicability. In light of these challenges, this work posits that (1) existing T2I models have already shown strong performance on several primary dimensions (e.g., attributes) in current benchmarks [34], [39], [40]. This highlights the necessity of further decomposing these dimensions into explicit, comprehensive sub-dimensionlevel test points (e.g., attribute-expression) to enable more comprehensive and diagnostic evaluation of model capabilities, thereby uncovering fine-grained weaknesses that coarse metrics often overlook. (2) Real-world T2I generation involves diverse scenarios (e.g., UI design, graphic art) and naturally spans multiple languages. The absence of such diversity in current benchmarks limits evaluation robustness, causing models that excel in constrained settings to falter in real-world applications. To this end, we introduce UNIGENBENCH++, unified semantic-generation benchmark tailored for fine-grained and comprehensive evaluation of T2I models. As illustrated in Fig. 1 (1), this benchmark comprises 600 prompts organized within hierarchical structure that ensures both coverage and efficiency: (i) It provides comprehensive assessment of semantic consistency across 10 primary and 27 sub-dimensions, each prompt targeting multiple specific test points. This design strikes balance between fine-grained evaluation and efficiency, ensuring the benchmark captures diverse aspects of model semantic generation capability. (ii) It spans 5 major real-world primary generation scenarios and 20 sub-scenarios with diverse subject categories, encompassing practical domains that reflect authentic user requirements, thereby enabling evaluation under conditions that closely mirror real-world usage. Besides, to enable systematic evaluation of models sensitivity to language and prompt length, each prompt is provided in both English and Chinese, and in short and long forms. For effective and efficient evaluation, in contrast to widely adopted paradigms, such as multi-turn conversational assessments with VLMs for each image evaluation [34], [35], [40], our benchmark introduces streamlined, point-wise evaluation pipeline, as illustrated in Fig. 1 (2): given prompt, its corresponding image, and set of explicitly designed test points (each accompanied by its in-context description within the prompt), the evaluation model, i.e., Gemini-2.5-Pro [41], sequentially analyses whether each semantic requirement is faithfully represented in the image and assigns an appropriate score. This lightweight and structured design reduces evaluation complexity while ensuring consistent, fine-grained, and interpretable judgments for every test point, thereby enabling more efficient and diagnostic assessment of T2I models. Moreover, to further facilitate community use, we provide robust evaluation model that supports offline assessment of T2I model outputs. We conduct comprehensive bilingual (English/Chinese) and length-varied (short/long prompt) benchmarking across both closed-source models, such as GPT-4o [14], Nano Banana [13], Seedream-4.0 [11], and FLUX-Kontext-Max [9], as well as leading open-source counterparts, including Qwen-Image [15], HiDream [12], Lumina-DiMOO [42] and Bagel [43]. JOURNAL OF LATEX CLASS FILES 3 Fig. 2. Benchmark Statistics. (a) Word clouds for English and Chinese prompts in both short and long forms; (b) overall prompt length distribution; and (c) distribution of testpoint counts per prompt for short versus long versions. As shown in Fig. 1 (3), both leading openand closedsource models exhibit strong performance on prompts involving style and world knowledge, yet consistently struggle with logical reasoning that requires causal, contrastive, or other complex relational understanding. Furthermore, open-source models show larger performance fluctuations across dimensions, particularly underperforming in the grammar and action dimensions. This highlights the models difficulty in handling grammar-conditioned instructions and depicting dynamic or behavior-centric content accurately. The contributions of this paper are summarized as follows: We propose UNIGENBENCH++, unified benchmark for text-to-image (T2I) semantic generation evaluation, covering comprehensive evaluation dimensions, diverse prompt themes, and rich subject categories. Each prompt is provided in both English and Chinese, and in short and long forms, assessing multiple test points, ensuring both coverage and efficiency. We design streamlined, point-wise evaluation pipeline that minimizes evaluation complexity while ensuring consistent, fine-grained, and interpretable judgments at the testpoint level. We provide dedicated offline evaluation model that enables robust assessment of T2I model outputs to further facilitate community use. We conduct extensive bilingual and length-varied benchmarking across both closedand open-source models, systematically revealing their strengths and weaknesses across diverse semantic aspects. We hope that our benchmark could advance the development and evaluation of T2I models, driving further improvements in semantic consistency across diverse fine-grained tasks and fostering deeper insights into model performance across realworld scenarios. II. RELATED WORK Text-to-Image Generation. Recent progress in text-to-image (T2I) generation is largely driven by two paradigms: diffusionbased and autoregressive (AR) models. Diffusion models dominate current practice due to their scalability and photorealistic synthesis, progressively denoising Gaussian noise conditioned on text, evolved from early GLIDE [44] and Imagen [18] to powerful variants like Stable Diffusion [2], FLUX [9], and HiDream [12]. In contrast, AR models generate images token by token via VQ-VAE [45] compression and transformer decoding, as seen in DALLE [4] and CogView [46]. Recent advances [47], [48] enhance AR models with unified multimodal reasoning, while hybrid architectures like Bagel [43] integrate both diffusion and AR to enable explicit reasoning before image generation. With such rapid advances, evaluating T2I models, especially their semantic generation capability, has become central challenge. Text-to-Image Benchmarks. Prior studies commonly assess T2I models through compositional generation tests. For example, GenEval [34] leverages object detection to rigorously verify whether generated images accurately reflect the spatial arrangements, numerical counts, and color attributes specified in the textual prompts. T2I-CompBench [35] encompasses four core compositional categories and further extends these evaluations with detection-based metrics for spatial reasoning and numerical consistency. Several studies evaluate T2I models through specific knowledge domains, such as physical reasoning [37] and general commonsense understanding [36]. However, the prompts used in these benchmarks are predominantly short and highly repetitive, which constrains semantic richness and expressiveness. Therefore, DPG-Bench [39] centers on assessing models capability in dense prompts. TIIF-Bench [40] offers both short and long variants of each prompt while preserving identical core semantics. JOURNAL OF LATEX CLASS FILES 4 Despite their effectiveness, these benchmarks still suffer from coarse evaluation across limited dimensions and provide insufficient sub-dimension coverage. Moreover, the lack of diverse prompt scenarios and multilingual support further limits their ability to assess models in real-world application settings. To this end, we introduce UNIGENBENCH++, unified semantic-generation benchmark designed for fine-grained and comprehensive evaluation of T2I models. III. BENCHMARK A. Overview With the rapid advancement of text-to-image (T2I) models, existing evaluation frameworks [34], [35], [39], [40] have become increasingly insufficient. To be precise, (1) as summarized in Tab. I, they often overlook diversity in prompt scenarios and lack multilingual coverage, both of which are indispensable for real-world applicability. Consequently, their evaluations fall short in capturing models true applicability across diverse and contextually complex input conditions; (2) although existing benchmarks effectively assess few broad dimensions, they still overlook several critical semantic aspects and lack systematic coverage and evaluation at the sub-dimension level, ultimately limiting their fine-grained diagnostic capability. To this end, we propose UNIGENBENCH++, unified semantic evaluation benchmark for T2I generation. As summarized in Fig. 1 and Tab. I, our benchmark offers several key advantages over existing studies: Rich prompt theme design. Prompts are hierarchically organized into 5 primary themes and 20 sub-themes, spanning both practical real-world use cases and openended imaginative scenarios (Sec. III-B). Comprehensive semantic dimension coverage. It evaluates 10 primary dimensions and 27 sub-dimensions, enabling systematic diagnosis of diverse model capabilities. Despite its breadth, it requires only 600 prompts, each targeting 110 explicit test points, achieving favorable balance between coverage and efficiency (Sec. III-C). Bilingual and length-variant prompt and streamlined model evaluation. All prompts are provided in both English and Chinese, each available in both short and long forms (Sec. III-D). Leveraging the world knowledge and fine-grained image understanding capabilities of Multimodal Large Language Models (MLLMs), i.e., Gemini2.5-Pro, we design fully streamlined pipeline for accurate and efficient model evaluation (Sec. III-E). Reliable evaluation model for offline assessment. To facilitate community use, we train robust evaluation model that supports offline assessment of T2I model outputs (Sec. III-F). B. Prompt Themes and Subject Categories This work posits that diverse prompt themes better approximate real-world usage scenarios, thereby yielding more faithful evaluation of model performance. Therefore, we organize prompt scenarios based on common real-world usage needs. Specifically, as illustrated in Fig. 1 (1.a), we structure them into 5 primary categories and 10 finer sub-categories to ensure both breadth and practical relevance: Creative Divergence covers open-ended imaginative ideation and broader forms of other abstract conceptual composition. Art encompasses wide range of visual expression styles, including graphic renderings, photography-inspired depictions, sculptural aesthetics, and other fine-art formats. Illustration is divided into copywriting-oriented visualization (e.g., , slogans or metaphors) and content-centric narrative illustration. Film & Story accounts for settings across cinematic realism, speculative or science-fiction narratives, and animation-style storytelling. Design spans professional and commercial use cases such as advertising and e-commerce graphics, spatial layouts, game and UI prototyping, poster composition, IP and logo/icon creation, fashion concept design, and generalpurpose design resource generation. To facilitate understanding of each theme, we present representative prompts in Tab. VI. Based on wide range of prompt themes, we further define diverse set of subject categories to cover different types of entities. As illustrated in Fig. 1 (1.d), these categories include animals, objects, anthropomorphic characters, scenes, as well as an Other category for special or atypical entities (e.g., robots appearing in science-fiction prompts). To this end, the benchmark can probe model capabilities on both common and unusual entities, providing insights into model strengths and weaknesses across diverse semantic scenarios."
        },
        {
            "title": "The distribution of prompt themes and subject categories is",
            "content": "illustrated in Fig. 1 (1.a) and (1.d), respectively. C. Evaluation Dimensions Existing T2I models have demonstrated strong performance on several primary evaluation dimensions in current benchmarks. However, this surface-level success often masks their underlying weaknesses at the sub-dimension level, as coarsegrained metrics are insufficient to reveal fine-grained limitations in specific sub-aspects. To address this gap, we decompose each major dimension into explicit and comprehensive sub-dimension-level test points. Specifically, our benchmark organizes evaluation dimensions into 10 major categories, most of which encompass multiple subcategories: 1. Style evaluates the models ability to generate images with coherent style and artistic expression. It considers both overall visual style and artistic genre, ensuring that the generated images exhibit plausible and consistent artistic characteristics. 2. World Knowledge examines the models grasp of realworld concepts. It evaluates whether the model can generate content consistent with physical laws, cultural norms, geographical facts, and historical context. 3. Attribute assesses the models understanding of object and scene characteristics, including: Quantity: The number of objects or elements in scene. JOURNAL OF LATEX CLASS FILES 5 Fig. 3. Qualitative Results of Evaluation Dimensions. We present qualitative examples of T2I models evaluated across our specified dimensions. Expression: Emotional states or facial expressions of Non-contact Interaction: Non-physical interactions like humans or animals. gazing. Material: Surface properties of objects, such as wood, Hand Actions: Representation of hand gestures or mametal, or glass. nipulations. Color: Accuracy and appropriateness of colors and color Full-body Actions: Depiction of whole-body movements combinations. of characters. Shape: Geometric form and contour of objects. Size: Relative dimensions of objects within the scene. 4. Compound evaluates the models ability to combine multiple concepts or features: Imagination: Creativity in generating novel or nonrealistic combinations. Feature Matching: Coherent integration of different elements and their attributes. 5. Action focuses on the dynamic behaviors and interactions of characters, animals, or objects: State: Status or posture of objects or characters, such as sleeping, suspending, or running. Animal Actions: Behaviors specific to animals. 6. Entity Layout evaluates spatial arrangement and composition: Two-Dimensional Space: Layout and relative positions of objects on plane. Three-Dimensional Space: Layout and relative positions of objects in three-dimensional space. 7. Relationship assesses the semantic and logical connections between objects: Contact Interaction: Physical interactions between obComposition: Integration of multiple elements into jects, such as touching and holding. coherent whole. JOURNAL OF LATEX CLASS FILES Similarity: Similarity in shape, color, or material between objects. Comparison: Differences and contrasts between objects. Inclusion: Containment or hierarchical relationships among objects. 8. Logical Reasoning measures the models ability to reason about events, object attributes, understand causality, and contrastive expressions. 9. Grammar evaluates the models understanding of textual and language-related expressions: Pronoun Reference: Correct association between pronouns and their referents in the image. Consistency: Maintenance of coherent attributes, properties, or features across objects as described in the prompt. Negation: Accurate reflection of negation or exclusion expressions in the generated content. 10. Text Generation evaluates the models ability to generate text content that is accurate, readable, and aligned with the requirements of the input prompt. We provide qualitative examples of our evaluation dimensions in Fig. 3. Notably, in our benchmark, the distribution of test points differs between short and long prompts. Specifically, long prompts tend to have more attribute-related test points, as they provide more detailed and diverse descriptions of subjects, attributes, and scenes. The test point distribution for both is shown in Fig. 1 (1.b) and (1.c). D. Bilingual and Length-variant Prompt Construction Bilingual Short Prompt Generation. Let denote the set of prompt themes, the set of subject categories, and the set of evaluation dimensions. For each prompt construction step, theme and subject category are first sampled uniformly at random. Subsequently, subset of testpoints c1, . . . , ck C, where [1, 5], is selected to specify the targeted fine-grained testpoints. Given the input tuple (t, s, c1, . . . , ck), the MLLM produces two outputs: (i) pair of natural language prompts (pen, pzh) in English and Chinese, both adhering to the semantic constraints imposed by the selected theme and subject category s; and (ii) structured description set d1, . . . , dk, where each element explicitly explains how the corresponding testpoint ci is instantiated within the generated prompts. Formally: (cid:0)pen, pzh, {d1, . . . , dk}(cid:1) MLLMgen (cid:0)t, s, {c1, . . . , ck}(cid:1), (1) Expanded to Long Prompt. To enrich the descriptive diversity and specificity of the generated prompts, we further expand each short prompt into long-form prompt through rewriting strategy. Given short prompt pen or pzh, we instruct the MLLM to generate an expanded version that satisfies two constraints: (i) the prompt theme, core subjects and their key attributes must be preserved, and (ii) attribute, scene, and background details may be further elaborated to enhance specificity and imagination. Formally, MLLMexpand (cid:0)p r(cid:1), (2) where denotes the rewriting constraint. Fig. 4. Pipeline of Benchmark Construction and Offline Evaluation Model Training. (a) Benchmark construction pipeline; (b) Offline evaluation model training; (c) Offline evaluation cases. However, expanding prompt may introduce new semantic elements that are not covered by the original evaluation dimensions, or render some of the initial testpoints no longer applicable. To maintain consistency between the expanded prompt and its associated testpoints, we perform second refinement step. Given the expanded prompt and the original testpoints {c1, . . . , ck} with their descriptions {d1, . . . , dk}, we instruct the MLLM to revise the testpoint set by: (i) removing those no longer grounded in p; (ii) adding newly emerged testpoints, with maximum allowance of five additional entries; and (iii) updating the in-context descriptions for all retained or newly added testpoints to reflect the semantics of p. Formally, the alignment process is defined as (cid:16) (cid:8)(ˆc1, ˆd1), . . . , (ˆck, ˆdk)(cid:9) MLLMalign + 5, (cid:12) (cid:12) p, {(ci, di)}k (cid:12) i=1 (cid:17) , where is determined dynamically by the updated semantic scope of p. The resulting tuple (cid:0)p, {(ˆc1, ˆd1), . . . , (ˆck, ˆdk)}(cid:1) constitutes semantically coherent long-prompt paired with aligned and fine-grained evaluation targets. JOURNAL OF LATEX CLASS FILES 7 Fig. 5. Evaluation Accuracy Comparison. Our dedicated evaluation model demonstrates significant improvement in evaluation accuracy across all test points compared to the commonly used offline evaluation VLM, i.e., Qwen2.5-VL-72b. The word clouds of both English and Chinese prompts in short and long forms are visualized in Fig. 2 (a). We also present statistics on the length distribution of prompts in Fig. 2 (b), as well as the distribution of test point counts between short and long prompts in Fig. 2 (c). E. T2I Model Evaluation To systematically evaluate the quality of model-generated images, we employ MLLM, i.e., Gemini-2.5-Pro, as an automatic evaluator. For each test prompt pi, the corresponding generated image xi is paired with set of fine-grained testpoints {ci,1, . . . , ci,k} and their descriptions {di,1, . . . , di,k}. Since each test point corresponds uniquely to its description, we henceforth refer only to the descriptions {di,j} for brevity. Then, the MLLM takes (xi, pi, {di,j}) as input and performs an independent assessment for each testpoint. For each di,j, it returns both binary decision ri,j {0, 1}, indicating whether the requirement is satisfied, and natural-language explanation ei,j, which articulates the reasoning behind the judgment. This process is formally expressed as: (ri,1, . . . , ri,k, ei,1, . . . , ei,k) (cid:16) MLLM {ri,j, ei,j} (cid:12) (cid:12) xi, pi, {di,1, . . . , di,k} (cid:17) (3) . Compared to scalar-only metrics, this formulation not only quantifies correctness but also reveals failure modes by exposing why testpoint is considered satisfied or violated. The availability of rationales ei,j further facilitates downstream error attribution. We provide an example evaluation case in Fig. 1 (2). Once all evaluation results are collected, we aggregate them at both the sub-dimension and primary-dimension levels. For each sub-dimension c, which groups semantically related testpoints, its score is defined as the ratio of satisfied instances to the total number of its occurrences across the benchmark: (cid:80) Rc = i,j 1{di,j and ri,j = 1} i,j 1{di,j c} (cid:80) , (4) where 1{} denotes the indicator function. Higher-level primary dimensions are then scored by averaging over their constituent sub-dimensions. This hierarchical aggregation strategy enables multi-granular evaluation: it reflects fine-grained capability trends while also supporting concise reporting at holistic level. Moreover, by separating binary correctness from explanatory evidence, our protocol provides both quantitative comparability and qualitative interpretability, which are crucial for diagnosing the strengths and weaknesses of T2I models at scale. F. Offline Evaluation Model Training To facilitate convenient and cost-efficient evaluation for the community, we further train an offline evaluation model that serves as lightweight substitute for proprietary MLLMs during evaluation. Instead of querying proprietary model online for every evaluation instance, our goal is to distill its scoring behavior into compact model that can be executed locally without external API calls. The supervision signals are constructed as described above from the online MLLM evaluator: for each imageprompt pair (xi, pi) and testpoint description {di,j}, the reference outputs (ri,j, ei,j) produced by the MLLM are collected and assembled into target sequences for supervised fine-tuning. Formally, given the tokenized target sequence yi associated with input (xi, pi, {di,j}), the training objective is: Ti(cid:88) L(θ) = log Pθ (cid:0)y(t) y(<t) , xi, pi, {di,1, . . . , di,k}(cid:1), t=1 (5) where Ti is the length of yi. This formulation allows the model to explicitly learn both binary judgment and explanatory reasoning through language modeling objective. At evaluation time, the offline evaluator can follow the same workflow as the original proprietary models-based assessment pipeline, producing decisions and explanatory rationales in manner consistent with the online model. JOURNAL OF LATEX CLASS FILES 8 TABLE II OVERALL BENCHMARKING RESULTS OF T2I MODELS ON UNIGENBENCH++ USING ENGLISH SHORT PROMPTS. Gemini-2.5-Pro IS USED AS THE MLLM FOR EVALUATION. BEST SCORES ARE IN BOLD, SECOND-BEST IN UNDERLINED. Model Overall Style World Know. Attribute Action Relation. Logic.Reason. Grammar Compound Layout Text English Short Prompt Evaluation HiDream-v2L Stable-Image-Ultra Recraft Wan2.2-Plus DALL-E-3 Runway-Gen4 FLUX-Pro-1.1-Ultra Imagen-3.0 FLUX-Kontext-Pro Imagen-4.0-Fast Wan2.5 Seedream-3.0 FLUX-Kontext-Max Imagen-4.0 Seedream-4.0 Nano Banana Imagen-4.0-Ultra GPT-4o SDXL MMaDA Kolors Playground2.5 Emu3 Janus-flow Janus Hunyuan-DiT X-Omni CogView4 OneCAT Infinity BLIP3-o SD-3.5-Medium FLUX.1-dev Bagel Janus-Pro Show-o2 SD-3.5-Large OmniGen2 UniWorld-V1 BLIP3-o-Next Echo-4o FLUX.1-Krea-dev Lumina-DiMOO HiDream-I1-Full Hunyuan-Image-2.1 Qwen-Image 61.64 61.96 62.63 64.82 69.18 69.75 70.67 71.85 75.84 77.75 78.17 78.95 80.00 85.84 87.35 87.45 91.54 92.77 39.75 41.35 45.47 45.61 46.02 46.39 51.23 51.38 53.77 56.30 58.28 59.81 59.87 60.71 61.30 61.53 61.61 62.73 62.99 63.09 63.11 65.15 69.12 69.88 71.12 71.81 74.64 78.81 87.99 87.20 87.20 91.10 95.06 93.44 90.60 89.25 94.78 92.00 93.15 98.10 96.59 97.80 98.80 98.87 99.20 98.57 87.40 82.40 84.40 89.50 86.80 86.20 89.90 94.10 72.70 82.00 93.30 90.80 92.80 89.80 83.90 90.20 90.80 87.20 88.60 91.90 91.10 91.00 92.20 88.70 89.70 92.50 90.88 95.10 89.62 87.18 90.19 87.34 93.51 90.36 91.61 94.75 91.61 94.78 95.22 95.25 94.19 96.36 95.41 96.32 97.47 98. 72.63 56.65 77.22 76.11 77.06 62.50 73.58 80.70 76.27 83.07 82.28 87.97 80.22 84.34 88.92 85.60 86.71 86.08 88.92 86.39 82.91 86.71 90.51 92.56 90.03 94.15 92.06 94.30 Closed-source Models 64.38 66.35 68.16 70.19 75.97 74.03 76.50 77.33 79.20 83.65 81.06 85.58 80.93 84.94 88.57 87.84 92.52 93.59 59.50 59.22 60.55 68.00 69.83 70.21 76.50 81.46 77.66 79.85 74.23 82.98 77.38 88.40 85.65 86.83 92.20 90.79 66.62 69.04 62.56 73.03 78.06 72.56 77.54 82.86 79.34 82.36 82.23 80.84 85.08 89.34 87.69 92.00 93.02 94.97 Open-source Models 44.34 48.93 54.17 52.78 51.39 47.97 54.81 62.71 60.04 63.25 63.46 68.06 63.89 66.99 67.84 67.74 67.74 70.51 68.59 72.12 70.62 70.94 79.06 75.96 81.62 72.97 79.66 87.61 34.22 37.83 48.00 42.68 40.11 43.35 50.38 49.05 54.47 57.51 58.56 60.17 63.97 60.65 62.17 61.98 64.26 69.58 62.17 62.83 67.21 66.83 68.92 71.01 73.76 73.00 77.81 84.13 44.92 50.25 52.79 51.52 49.75 50.00 55.08 59.64 56.60 62.44 68.15 69.16 66.50 68.78 67.26 70.69 68.40 70.18 69.80 68.27 67.13 73.60 76.52 73.98 78.43 75.38 77.54 79.70 26.73 31.59 29.55 42.05 48.18 49.31 43.18 48.36 55.68 56.36 56.36 52.73 61.36 70.45 67.73 74.26 79.55 84.97 9.55 17.95 19.77 16.59 19.32 21.14 26.82 24.55 29.09 28.18 33.41 31.36 39.55 37.73 30.91 30.23 37.05 40.91 32.27 32.50 38.41 48.64 44.77 39.77 45.45 41.14 46.59 53.64 58.86 61.10 63.64 66.53 68.07 70.08 70.05 69.84 72.69 76.74 73.59 61.36 78.53 79.68 78.88 83.36 87.97 91. 47.33 55.75 46.66 53.21 52.94 60.29 59.09 55.48 59.09 54.81 60.83 60.16 68.58 59.89 60.96 66.44 64.44 61.63 58.96 59.89 63.77 68.05 75.13 63.37 70.45 63.24 62.83 60.29 49.28 54.25 44.85 61.37 70.60 67.76 67.78 71.71 72.68 74.10 76.23 73.84 78.99 85.31 86.08 87.83 91.37 93.55 26.68 32.35 33.63 35.44 36.86 45.10 46.65 41.62 41.75 44.72 56.96 51.42 53.74 53.35 47.04 58.12 62.11 64.69 58.76 56.31 54.51 64.82 71.78 64.43 73.32 62.63 64.82 73.32 69.06 64.55 57.84 74.77 66.67 76.33 81.53 81.34 84.47 86.19 77.61 87.31 85.04 88.81 90.67 91.96 93.10 91.35 29.85 30.22 42.91 37.13 44.78 46.46 54.85 44.78 62.69 69.22 64.74 66.60 68.47 70.34 71.83 76.49 72.01 75.37 69.03 71.64 69.03 76.31 82.28 84.14 82.84 78.17 84.14 85.52 44.31 39.08 61.78 13.83 25.86 33.43 37.36 21.55 50.29 51.44 71.97 71.55 61.92 77.30 93.97 75.22 89.08 89. 1.15 1.15 1.15 1.15 1.15 0.86 1.15 1.15 25.00 17.82 1.15 12.36 1.15 15.23 32.18 7.76 2.59 1.15 32.76 29.02 26.44 4.60 10.06 44.83 25.57 64.94 70.11 76.14 JOURNAL OF LATEX CLASS FILES 9 TABLE III OVERALL BENCHMARKING RESULTS OF T2I MODELS ON UNIGENBENCH++ USING ENGLISH LONG PROMPTS. Gemini-2.5-Pro IS USED AS THE MLLM FOR EVALUATION. BEST SCORES ARE IN BOLD, SECOND-BEST IN UNDERLINED. Model Overall Style World Know. Attribute Action Relation. Logic.Reason. Grammar Compound Layout Text English Long Prompt Evaluation Recraft Stable-Image-Ultra Runway-Gen4 Wan2.2-Plus DALL-E-3 FLUX-Pro-1.1-Ultra Imagen-3.0 FLUX-Kontext-Pro FLUX-Kontext-Max Seedream-3.0 Imagen-4.0-Fast Wan2.5 Imagen-4.0 Nano Banana Seedream-4.0 Imagen-4.0-Ultra GPT-4o MMaDA SDXL Emu3 Kolors Janus-flow Hunyuan-DiT Janus BLIP3-o OneCAT SD-3.5-Large SD-3.5-Medium X-Omni Infinity CogView4 FLUX.1-dev UniWorld-V1 Show-o2 BLIP3-o-Next Janus-Pro Bagel OmniGen2 Lumina-DiMOO HiDream-I1-Full Echo-4o FLUX.1-Krea-dev Hunyuan-Image-2.1 Qwen-Image 60.93 62.01 68.29 68.76 70.82 75.40 75.76 78.58 80.88 80.99 81.54 84.34 85.34 88.82 89.77 90.95 92.63 40.10 41.48 50.95 53.60 54.80 54.88 60.37 61.01 62.92 64.35 64.67 67.00 67.28 67.68 69.42 69.60 70.33 71.03 71.11 71.26 71.39 71.81 74.25 76.41 78.45 82.19 83.94 87.13 85.63 91.72 90.28 95.08 91.36 92.41 94.83 96.51 97.18 93.77 96.75 94.44 98.83 98.42 97.67 99. 75.83 81.81 89.36 86.54 88.70 92.94 92.03 91.61 94.93 88.12 92.19 80.15 92.77 88.29 89.29 93.19 93.11 94.60 94.02 92.44 94.35 86.88 93.11 96.10 94.10 94.52 96.93 86.99 86.71 88.82 87.57 92.71 91.76 94.19 93.60 93.35 93.79 93.64 95.52 97.11 95.78 95.95 98.26 97.95 52.75 69.51 76.16 76.01 65.90 80.06 73.27 74.42 83.67 88.15 86.56 82.37 88.44 89.45 89.45 84.10 88.44 88.87 88.15 89.31 84.83 88.58 92.63 90.17 93.79 93.35 95.09 Closed-source Models 73.23 74.73 79.83 81.08 84.98 84.97 86.32 86.24 87.45 91.90 90.33 91.40 90.14 93.06 95.06 93.21 93.53 51.77 58.27 64.30 66.49 68.36 72.43 75.81 74.44 75.52 79.94 80.18 77.55 82.62 83.93 86.76 86.91 87. 55.82 63.63 69.53 72.79 77.90 81.90 80.76 78.40 80.78 83.41 84.05 86.96 86.42 91.59 88.69 90.57 91.13 Open-source Models 49.90 54.31 66.81 68.12 63.60 69.47 70.67 71.28 74.90 78.78 80.24 79.82 81.06 80.57 79.90 79.94 86.35 80.57 81.81 84.21 83.03 83.71 83.49 90.24 89.55 92.81 93.65 32.42 31.18 43.80 49.96 48.68 48.80 55.78 55.38 58.95 59.63 58.59 61.96 63.28 64.33 64.54 65.81 69.02 70.18 69.14 67.62 66.57 69.66 68.82 73.56 76.28 81.14 81.86 39.06 36.26 51.70 58.51 58.24 55.66 63.25 62.61 65.36 67.62 69.88 64.28 70.04 66.97 69.40 68.91 77.37 74.68 77.96 75.70 73.06 73.33 74.30 82.81 81.73 85.13 83.41 34.22 40.29 48.28 55.58 57.11 60.92 61.25 66.26 71.12 62.62 67.72 71.32 72.82 81.27 79.13 83.50 91. 19.42 19.42 27.43 31.31 41.75 29.85 54.37 48.30 48.06 44.90 45.87 51.70 51.46 49.76 54.37 57.04 59.71 65.53 62.62 59.71 56.55 58.01 50.24 69.42 65.53 68.20 66.75 60.28 65.10 70.55 70.18 68.19 71.94 77.96 77.05 79.34 75.13 79.57 78.06 81.35 89.33 82.74 88.07 94.46 50.00 46.83 50.25 55.20 63.83 58.76 67.26 65.36 63.58 65.23 65.86 68.78 68.53 71.70 70.56 75.13 70.30 76.02 74.62 74.75 76.40 74.49 72.59 82.36 75.25 77.41 73.86 49.56 58.28 68.57 71.73 73.88 78.07 78.70 79.75 82.24 81.03 84.01 85.60 86.56 90.63 87.79 91.42 93.99 38.37 34.30 46.00 47.24 55.16 50.22 61.85 65.55 63.59 62.21 62.86 64.17 66.13 66.86 68.46 71.37 76.45 74.27 76.53 74.71 70.49 74.93 69.77 84.88 80.67 82.49 81.98 63.81 71.67 73.79 79.13 71.76 82.62 86.06 85.46 87.58 88.41 90.48 87.18 90.24 94.04 92.38 93.49 93. 43.02 40.40 56.67 60.95 60.48 61.43 64.13 74.21 74.29 71.19 73.25 73.33 77.54 79.84 77.54 79.60 80.63 80.71 82.14 81.90 80.63 84.84 79.92 86.43 86.59 88.65 88.97 46.47 15.76 27.47 12.77 18.26 38.04 24.18 49.73 54.89 56.52 51.63 73.10 71.74 69.75 90.76 86.41 83.79 0.27 0.82 1.36 2.17 1.63 1.63 1.09 1.36 1.90 17.66 11.41 43.48 13.59 19.02 30.71 20.92 1.90 4.89 4.08 12.23 27.99 23.64 57.61 8.15 41.03 58.15 76.90 JOURNAL OF LATEX CLASS FILES 10 TABLE IV OVERALL BENCHMARKING RESULTS OF T2I MODELS ON UNIGENBENCH++ USING CHINESE SHORT PROMPTS. Gemini-2.5-Pro IS USED AS THE MLLM FOR EVALUATION. BEST SCORES ARE IN BOLD, SECOND-BEST IN UNDERLINED. Model Overall Style World Know. Attribute Action Relation. Logic.Reason. Grammar Compound Layout Text Chinese Short Prompt Evaluation Runway-Gen4 Recraft HiDream-v2L Wan2.2-Plus DALL-E-3 Imagen-4.0-Fast FLUX-Kontext-Max Wan2.5 Imagen-4.0 Nano Banana Seedream-3.0 Imagen-4.0-Ultra Seedream-4.0 GPT-4o UniWorld-V1 Janus-flow Janus-Pro Janus Emu3 MMaDA BLIP3-o-Next HiDream-I1-Full Hunyuan-DiT X-Omni CogView4 Lumina-DiMOO Kolors OneCAT BLIP3-o OmniGen2 Bagel Echo-4o Hunyuan-Image-2.1 Qwen-Image 54.93 57.67 59.95 66.96 67.93 71.60 71.85 78.40 79.52 80.91 81.68 83.21 87.31 91. 15.21 20.93 30.83 30.98 33.91 44.00 44.48 50.65 53.36 53.69 55.14 58.35 58.80 58.50 59.25 63.20 65.69 72.40 77.76 81.04 64.75 87.70 89.34 91.06 95.90 93.30 96.38 93.30 97.50 99.27 97.50 98.90 99.00 99.39 49.40 58.50 75.60 78.10 78.08 78.20 74.60 83.30 92.50 70.07 82.40 80.90 85.20 94.40 92.60 93.00 92.30 92.80 92.20 95.50 71.05 90.03 91.02 84.39 93.04 91.30 92.83 93.51 96.84 96.47 93.99 97.94 94.94 98.72 16.61 18.67 39.08 27.85 55.54 52.06 50.00 78.32 84.97 71.52 84.18 69.46 86.23 86.55 81.17 86.39 86.71 87.66 90.51 92.41 Closed-source Models 60.43 69.34 67.87 73.93 78.42 80.98 76.41 83.65 86.22 87.76 88.03 90.71 90.06 94.99 60.42 63.88 64.90 72.52 72.24 79.28 78.59 76.62 90.40 86.99 86.98 93.82 87.55 92.34 65.90 64.47 72.67 76.78 79.95 82.49 83.97 81.85 90.74 91.39 84.39 92.13 88.58 95.77 Open-source Models 15.06 19.23 33.12 30.88 38.29 55.24 55.98 62.18 62.93 63.85 63.35 75.64 69.34 63.89 66.56 75.43 75.21 84.29 84.19 91.88 14.64 22.05 26.33 31.37 31.18 43.44 47.62 53.71 57.22 58.37 61.69 61.12 65.02 63.12 64.35 66.54 65.78 76.05 80.51 85. 11.80 19.54 32.74 30.58 36.68 56.22 53.55 57.23 59.39 59.77 61.68 67.13 67.13 67.39 65.36 70.69 75.38 82.23 82.74 82.99 42.03 34.09 32.01 51.82 51.59 54.77 56.48 63.64 73.18 76.10 59.09 79.32 68.64 91.44 2.95 10.68 10.23 13.41 13.90 26.14 27.50 23.64 29.55 34.77 30.23 39.09 36.14 38.64 41.59 44.09 37.95 56.82 50.23 57.73 58.38 60.56 62.57 70.59 71.52 77.41 75.68 72.58 82.89 83.33 67.25 87.43 78.48 91.02 27.81 35.03 36.63 48.40 41.31 58.56 54.14 53.88 54.68 56.28 54.55 64.84 56.68 59.00 63.37 65.64 69.52 75.40 61.50 62.83 61.00 43.94 53.19 64.77 72.94 73.97 75.13 78.74 85.70 86.89 76.68 89.95 81.57 93. 4.38 10.70 24.48 17.53 21.65 32.86 26.55 34.54 44.59 41.75 45.75 56.06 66.03 51.55 51.80 59.92 69.85 77.96 70.62 76.16 64.71 58.40 64.77 71.83 62.50 78.73 81.34 75.93 89.18 88.80 84.14 92.16 90.30 89.27 9.14 14.93 30.04 31.72 22.43 37.31 54.85 59.70 47.76 59.51 65.30 69.22 62.31 60.45 65.67 69.96 77.61 83.02 85.45 82.65 0.59 4.31 1.16 11.92 1.15 3.74 1.72 64.22 2.59 12.06 78.74 9.77 93.97 63.37 0.29 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 20.98 2.30 0.00 4.89 0.00 0.00 0.29 6.61 7.76 79.60 82.47 IV. EXPERIMENT A. Implementation Details 2) Offline Evaluation Model: We use UnifiedReward-2.0qwen-72b [63] as the base model and collect approximately 375K evaluation samples from Gemini-2.5-Pro. Of this, 300K is used for model training, and 75K is reserved for evaluation. Imagen-3.0/4.0-Ultra/Fast 1) Benchmarking Models: Closed-source Models. GPT- [18], Nano Banana 4o [38], [13], Seedream-3.0/4.0 [10], [11], Wan2.2-Plus/2.5 [49], Runway-Gen4 [50], Recraft [51], DALL-E-3 [4], FLUX-ProUltra/Kontext-Max [9], HiDream-v2L [52], and Stable-ImageUltra [53]. Open-source Models. Qwen-Image [15], HunyuanImage-2.1 [54], HiDream-I1-Full [12], Lumina-DiMOO [42], Show-o2 [7], Infinity [20], OneCAT [47], CogView4 [46], XOmni [55], MMaDA [56], Flux.1-dev [9], Flux.1-Krea-dev [19], Echo-4o [57], BLIP3-o series [24], UniWorld-V1 [58], OmniGen2 [8], Bagel [43], Hunyuan-DiT [59], Janus series [21][23], Emu3 [60], Playground2.5 [61], Kolors [62], SDXL [5], and SD-3.5-Medium/Large [2]. B. Benchmarking Result Analysis In this subsection, we will analyze the overall performance of current mainstream closed-source and open-source models on our UNIGENBENCH++, focusing on both Chinese and English, as well as long and short prompts. 1) English Short Prompt (Tab. II): (a) Closed-source Models. GPT-4o is the most well-rounded model, excelling in broad range of metrics, including logical reasoning and grammar. Besides, Imagen-4.0-Ultra also performs well in visual generation accuracy but lags behind GPT-4o in logical reasoning. In contrast, remaining models like Seedream-3.0 and Wan-2.5 perform strongly in specific areas but struggle with JOURNAL OF LATEX CLASS FILES 11 TABLE OVERALL BENCHMARKING RESULTS OF T2I MODELS ON UNIGENBENCH++ USING CHINESE LONG PROMPTS. Gemini-2.5-Pro IS USED AS THE MLLM FOR EVALUATION. BEST SCORES ARE IN BOLD, SECOND-BEST IN UNDERLINED. Model Overall Style World Know. Attribute Action Relation. Logic.Reason. Grammar Compound Layout Text Chinese Long Prompt Evaluation Recraft Wan2.2-Plus DALL-E-3 Imagen-3.0 FLUX-Kontext-Max Imagen-4.0 Nano Banana Imagen-4.0-Ultra Wan2. Seedream-3.0 Seedream-4.0 GPT-4o UniWorld-V1 Janus-flow Janus Emu3 MMaDA HiDream-I1-Full BLIP3-o-Next Hunyuan-DiT BLIP3-o Janus-Pro X-Omni Lumina-DiMOO OneCAT Kolors CogView4 OmniGen2 Bagel Echo-4o Qwen-Image Hunyuan-Image-2.1 56.90 70.05 71.16 71.85 75.24 79.90 83.17 83.86 84.24 86.14 90.35 90.51 21.50 23.01 33.63 35.95 50.61 50.70 54.55 55.57 59.25 60.21 62.18 63.80 63.88 65.12 68.09 70.75 75.75 78.31 86.91 87.01 86.38 91.61 95.85 89.25 97.59 95.60 98.41 97.34 98.00 98.42 98.42 99. 55.48 57.39 75.00 75.08 84.05 83.06 87.71 94.10 89.70 91.28 76.91 84.30 95.85 90.61 89.62 95.35 96.10 96.26 97.84 95.18 85.55 88.73 94.36 94.75 92.31 97.98 97.38 97.40 94.30 95.36 96.39 97.96 17.34 17.49 30.06 53.03 63.58 78.61 61.85 76.16 77.17 75.87 74.13 76.45 85.26 87.14 89.31 87.57 89.02 91.18 95.66 94.08 Closed-source Models 74.31 82.42 85.41 77.33 86.17 90.94 93.29 93.59 90.49 93.93 95.54 94.72 54.65 70.22 70.59 81.46 75.71 84.55 85.55 88.80 78.39 84.53 89.29 89. 57.44 73.65 80.12 82.86 81.27 88.04 91.32 92.35 86.64 87.55 88.69 92.59 Open-source Models 27.50 23.42 35.98 48.82 61.31 65.05 63.75 69.72 69.24 65.79 76.51 79.41 74.79 81.18 80.99 85.05 88.25 91.82 95.04 93.82 19.34 19.46 29.74 27.81 42.98 47.47 51.81 51.04 55.98 54.33 58.43 61.32 60.11 64.49 67.94 67.17 72.43 75.56 86.56 83.99 19.34 20.04 28.23 32.06 52.69 49.25 57.76 55.60 60.56 62.61 60.83 66.70 65.03 71.23 70.58 75.38 81.52 85.83 87.61 88.09 36.17 57.04 61.41 48.36 68.20 77.18 82.40 86.89 74.51 68.45 80.58 90. 8.98 17.48 20.15 19.66 31.80 24.27 41.50 33.98 47.09 49.27 46.60 49.27 54.37 47.82 51.94 62.62 68.69 72.57 69.90 71.36 57.49 70.05 70.81 69.84 78.77 82.74 88.35 88.83 80.08 77.54 83.63 94.11 28.68 32.23 44.04 38.32 58.76 53.81 60.66 60.06 60.91 68.53 64.85 71.95 63.07 63.96 70.94 77.03 81.09 83.50 76.90 80.08 50.00 71.51 75.87 71.71 80.16 86.63 91.21 92.51 85.13 83.11 87.72 94.59 12.50 21.58 31.47 28.49 50.07 42.08 54.00 52.03 60.68 65.62 61.12 68.90 62.35 64.17 69.91 74.06 82.05 85.25 82.99 85.61 64.52 80.08 73.33 81.34 87.58 90.48 93.15 94.13 88.54 90.16 91.90 95. 24.44 21.59 40.56 35.40 60.63 60.40 64.60 61.67 69.29 66.59 73.02 78.33 75.79 74.60 81.51 81.35 83.97 88.10 90.48 91.43 2.45 15.22 3.80 21.55 4.62 4.89 10.68 6.79 66.30 82.34 91.30 57.14 1.36 0.27 1.09 0.82 0.27 2.99 1.90 1.36 1.90 2.17 29.35 1.36 2.17 5.98 8.15 1.90 14.40 13.04 86.14 86.41 logical reasoning and relational understanding. For example, Seedream-3.0 excels in stylistic quality and world knowledge but falls short in complex reasoning and grammar understanding tasks. These results highlight clear trend where many closedsource models have specialized in most visual generation tasks but still fall short in handling complex reasoning and understanding. (b) Open-source Models. Qwen-Image stands out as the top performer among open-source models, excelling in generating semantically accurate and contextually relevant images based on English short text descriptions. Besides, HiDream-I1-Full excels in world knowledge, but falls short in attribute generation and logical reasoning. Notably, LuminaDiMOO performs strongly in relation generation and grammar understanding, but struggles with text generation consistency. The remaining models show promising results in specific areas but exhibit weaknesses in others. For example, Echo-4o and BLIP3-o-Next excel in compound semantic generation and grammar understanding but struggle with complex relationships and logically consistent scenes. (c) Closedv.s. Open-source Models. clear trend emerges where some open-source models are making significant strides in catching up to their closedsource counterparts. To be precise, Qwen-Image, the leading open-source model, surpasses many closed-source models in key areas such as world knowledge, action generation, and logical reasoning. It competes closely with top performers like Seedream-3.0 and FLUX-Kontext-Max. However, despite the impressive progress, closed-source models still hold significant advantage in several areas. For instance, Seedream4.0 and Nano Banana outperform all open-source models in dimensions like style, grammar, and compound feature construction. Overall, while open-source models are making remarkable progress, particularly in the world knowledge and attribute generation domains, closed-source models remain dominant in grammar, logical reasoning, and relation generation. 2) English Long Prompt (Tab. III): (a) Closed-source Models. For English long prompt generation, the closed-source models exhibit strong performance across most evaluation metrics. GPT-4o stands out with the highest overall score, JOURNAL OF LATEX CLASS FILES 12 leading in grammar, compound generation, and logical reasoning, though its layout consistency and text generation slightly lag behind several models. Seedream-4.0 and Nano Banana are also notable performers, with Seedream-4.0 achieving exceptional scores in text while Nano Banana shines in style consistency and relation generation. Remaining models like Imagen-4.0 and Wan2.5 offer promising results in specific areas such as attribute and layout generation, but still trail behind in grammar understanding. (b) Open-source Models show significant progress: Qwen-Image still leads the opensource group with strong results across world knowledge, action, text, and compound generation, but still challenges in grammar understanding. Models such as Hunyuan-Image-2.1 and FLUXKrea-dev also perform well in relational understanding and logical reasoning. Lumina-DiMOO and OmniGen2 provide solid performance but are weaker in logical reasoning and text generation. (c) Closedv.s. Open-source Models. Most closed-source models consistently outperform in areas such as world knowledge, logical reasoning, layout consistency, and text fluency. Open-source models, while showing significant progress, still fall behind in these areas. Hunyuan-Image-2.1 and Qwen-Image are the strongest open-source contenders, achieving competitive results in text generation, style and world knowledge, but lack the relation and grammar understanding seen in closed-source models like GPT-4o and Seedream-4.0. 3) Chinese Short Prompt (Tab. IV): (a) Closed-source Models. When using Chinese short prompt evaluation, although GPT-4o leads in most evaluation metrics, it still has room for improvement, particularly in layout generation and the accuracy of Chinese text generation. In contrast, Seedream-4.0 excels in text generation and also performs strongly in style and attribute generation. Besides, Imagen-4.0-Ultra performs strongly, particularly in action generation and logical reasoning. It also achieves high scores in layout consistency but slightly trails GPT-4o in overall performance. Other closed-source models, such as Seedream-3.0, FLUX-Kontext-Max, and Nano Banana, show promise in areas like style generation and world knowledge but struggle in more complex tasks like logical reasoning and layout generation. (b) Open-source Models. Qwen-Image and Hunyuan-Image-2.1 stand out as the topperforming open-source models, excelling in relation, action, and attribute generation, though they still face challenges in grammar understanding. In contrast, Echo-4o performs well in grammar and compound tasks, but struggles with Chinese text generation compared to the top models. Models like OmniGen2 and Bagel show balanced performance across multiple metrics, but face limitations in layout generation and text consistency. Specifically, OmniGen2 excels in world knowledge, while Bagel is solid in style and action prediction, but neither matches the best models in complex reasoning or text generation. The remaining models, such as X-Omni, Kolors, show promise in certain areas but generally fall behind in grammar understanding, text, and compound context generation. (c) Closedv.s. Open-source Models. Closed-source models, particularly GPT4o, Seedream-4.0, and Imagen-4.0, dominate the evaluation, excelling in overall performance. In comparison, open-source models such as Qwen-Image and Hunyuan-Image-2.1 also show significant progress, especially in world knowledge and text generation. However, they generally lag behind in grammar understanding, compound generation, and complex logical reasoning tasks. 4) Chinese Long Prompt (Tab. V): (a) Closed-source Models Closed-source models still demonstrate strong overall performance in generating Chinese long prompts. Notably, Seedream-4.0 performs exceptionally well in Chinese text generation and attribute generation, achieving an overall performance very close to GPT-4o. Meanwhile, Imagen-4.0Ultra excels in layout consistency, grammar, relational understanding, compound feature generation, and logical reasoning but trails in world knowledge and fluency. In addition, Wan2.5 demonstrates highly balanced capabilities. While it does not excel in any particular dimension, its overall score remains relatively high. (b) Open-source Models. Hunyuan-Image-2.1 leading the group, excelling in tasks like layout and text generation. Qwen-Image competes closely with Seedream4.0 in attribute and layout generation, though it still lags behind in grammar understanding and logical reasoning. Other models like Echo-4o and Bagel perform well in relational understanding and world knowledge but face challenges in handling complex action generation and accuracy Chinese text generation. (c) Closedv.s. Open-source Models. Closed-source models outperform in grammar understanding and generating logically consistent images, while open-source models are making significant strides, particularly in world knowledge, attribute generation, and text generation. However, open-source models still need further improvements in handling compound and action generation. Most closed-source and open-source models also have room for improvement in logical reasoning. Detailed 27 dimensions benchmarking results are provided in Tabs. VII, VIII, IX, and X. C. Offline Evaluation Model [40] Existing benchmarks [36], typically use VisionLanguage Models (VLMs) like Qwen2.5-VL-72b [64] for offline generalization evaluation. However, compared to closedsource models, the evaluation accuracy of these models often falls short. Specifically, in our benchmark, we observed that Qwen2.5-VL-72b performs reasonably well on relatively simple dimensions such as attribute-color and facial expressions. However, its performance becomes unreliable on more complex dimensions like grammar-consistency and action-contact. To address this, we train dedicated evaluation model, and the results, compared to Qwen2.5-VL-72b, are shown in Fig. 5. As demonstrated, our model significantly outperforms Qwen2.5VL-72b across both short and long, as well as Chinese and English prompts evaluations, highlighting substantial improvement in evaluation accuracy. Both English and Chinese qualitative evaluation cases are provided in Fig. 4 (c). D. Compared with UniGenBench Compared with the preliminary version [29], this work introduces several significant extensions across the following aspects: (1) Bilingual and length-variant prompt support: The prompts are expanded to include varying lengths, as well as both English and Chinese languages, thereby enhancing JOURNAL OF LATEX CLASS FILES 13 the diversity and comprehensiveness of the benchmark. This extension allows for more in-depth evaluation of T2I model sensitivity and robustness to prompt length and language variations; (2) Dedicated offline evaluation model: Due to the inconvenience of accessing closed-source proprietary models via APIs, we provide dedicated offline evaluation model that enables reliable assessments of T2I model outputs, offering enhanced flexibility and ease of use for the research community; (3) More comprehensive benchmarking results and detailed analysis: We extensively tested wide range of both opensource and closed-source models on English and Chinese prompts of varying lengths. Through thorough comparative analyses, we further identify their strengths and weaknesses, providing deeper understanding of model performance across broader set of test points and real-world scenarios. V. CONCLUSION In this work, we introduce UNIGENBENCH++, unified semantic benchmark for evaluating text-to-image (T2I) models. It consists of 600 prompts organized within hierarchical structure that ensures both coverage and efficiency. Specifically, it covers 5 main themes and 20 subthemes across diverse realworld scenarios, assessing models on 10 primary and 27 subevaluation criteria using English and Chinese prompts in both short and long forms. Leveraging the world knowledge and finegrained image understanding capabilities of the Multi-modal Large Language Model (MLLM), we developed an effective pipeline for benchmark construction and model evaluation. Additionally, to facilitate community usage, we propose robust offline evaluation model for T2I model assessments. Our comprehensive benchmarking reveals the strengths and weaknesses of both openand closed-source T2I models, offering valuable insights into their semantic consistency and performance across various aspects."
        },
        {
            "title": "REFERENCES",
            "content": "[1] J. Ho, A. Jain, and P. Abbeel, Denoising diffusion probabilistic models, NeurIPS, vol. 33, pp. 68406851, 2020. [2] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer, Highresolution image synthesis with latent diffusion models, in CVPR, 2022, pp. 10 68410 695. [3] X. Liu, C. Gong, and Q. Liu, Flow straight and fast: Learning to generate and transfer data with rectified flow, arXiv preprint arXiv:2209.03003, 2022. [4] OpenAI., Dalle 3, https://openai.com/zh-Hans-CN/index/dall-e-3/, 2023. [5] D. Podell, Z. English, K. Lacey, A. Blattmann, T. Dockhorn, J. Muller, J. Penna, and R. Rombach, Sdxl: Improving latent diffusion models for high-resolution image synthesis, arXiv preprint arXiv:2307.01952, 2023. [6] P. Esser, S. Kulal, A. Blattmann, R. Entezari, J. Muller, H. Saini, Y. Levi, D. Lorenz, A. Sauer, F. Boesel et al., Scaling rectified flow transformers for high-resolution image synthesis, in ICML, 2024. [7] J. Xie, Z. Yang, and M. Z. Shou, Show-o2: Improved native unified multimodal models, arXiv preprint arXiv:2506.15564, 2025. [8] C. Wu, P. Zheng, R. Yan, S. Xiao, X. Luo, Y. Wang, W. Li, X. Jiang, Y. Liu, J. Zhou et al., Omnigen2: Exploration to advanced multimodal generation, arXiv preprint arXiv:2506.18871, 2025. [9] B. F. Labs, Flux, https://github.com/black-forest-labs/flux, 2024. [10] Y. Gao, L. Gong, Q. Guo, X. Hou, Z. Lai, F. Li, L. Li, X. Lian, C. Liao, L. Liu et al., Seedream 3.0 technical report, arXiv preprint arXiv:2504.11346, 2025. [11] T. Seedream, Y. Chen, Y. Gao, L. Gong, M. Guo, Q. Guo, Z. Guo, X. Hou, W. Huang, Y. Huang et al., Seedream 4.0: Toward next-generation multimodal image generation, arXiv preprint arXiv:2509.20427, 2025. [12] Q. Cai, J. Chen, Y. Chen, Y. Li, F. Long, Y. Pan, Z. Qiu, Y. Zhang, F. Gao, P. Xu et al., Hidream-i1: high-efficient image generative foundation model with sparse diffusion transformer, arXiv preprint arXiv:2505.22705, 2025. [13] Google, Nano banana, https://deepmind.google/models/gemini/image/, 2025. [14] OpenAI, Gpt-image-1, https://openai.com/index/introducing-4o-imagegeneration/, 2025. [15] C. Wu, J. Li, J. Zhou, J. Lin, K. Gao, K. Yan, S.-m. Yin, S. Bai, X. Xu, Y. Chen et al., Qwen-image technical report, arXiv preprint arXiv:2508.02324, 2025. [16] D. Li, A. Kamko, E. Akhgari, A. Sabet, L. Xu, and S. Doshi, Playground v2. 5: Three insights towards enhancing aesthetic quality in text-to-image generation, arXiv preprint arXiv:2402.17245, 2024. [17] Y. Wang, W. Zhang, X. Honghui, and C. Jin, High fidelity scene text synthesis, in CVPR, 2025. [18] Google, Imagen, https://deepmind.google/models/imagen/, 2025. [19] B. F. Labs., Flux.1 krea, https://www.krea.ai/apps/image/flux-krea, 2025. [20] J. Han, J. Liu, Y. Jiang, B. Yan, Y. Zhang, Z. Yuan, B. Peng, and X. Liu, Infinity: Scaling bitwise autoregressive modeling for high-resolution image synthesis, in CVPR, 2025, pp. 15 73315 744. [21] C. Wu, X. Chen, Z. Wu, Y. Ma, X. Liu, Z. Pan, W. Liu, Z. Xie, X. Yu, C. Ruan et al., Janus: Decoupling visual encoding for unified multimodal understanding and generation, in CVPR, 2025, pp. 12 96612 977. [22] X. Chen, Z. Wu, X. Liu, Z. Pan, W. Liu, Z. Xie, X. Yu, and C. Ruan, Janus-pro: Unified multimodal understanding and generation with data and model scaling, arXiv preprint arXiv:2501.17811, 2025. [23] Y. Ma, X. Liu, X. Chen, W. Liu, C. Wu, Z. Wu, Z. Pan, Z. Xie, H. Zhang, X. yu, L. Zhao, Y. Wang, J. Liu, and C. Ruan, Janusflow: Harmonizing autoregression and rectified flow for unified multimodal understanding and generation, 2024. [24] J. Chen, Z. Xu, X. Pan, Y. Hu, C. Qin, T. Goldstein, L. Huang, T. Zhou, S. Xie, S. Savarese et al., Blip3-o: family of fully open unified multimodal models-architecture, training and dataset, arXiv preprint arXiv:2505.09568, 2025. [25] R. Rafailov, A. Sharma, E. Mitchell, C. D. Manning, S. Ermon, and C. Finn, Direct preference optimization: Your language model is secretly reward model, NeurIPS, vol. 36, pp. 53 72853 741, 2023. [26] D. Guo, D. Yang, H. Zhang, J. Song, R. Zhang, R. Xu, Q. Zhu, S. Ma, P. Wang, X. Bi et al., Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, arXiv preprint arXiv:2501.12948, 2025. [27] Y. Wang, Z. Li, Y. Zang, C. Wang, Q. Lu, C. Jin, and J. Wang, Unified multimodal chain-of-thought reward model through reinforcement finetuning, arXiv preprint arXiv:2505.03318, 2025. [28] Y. Wang, Z. Tan, J. Wang, X. Yang, C. Jin, and H. Li, Lift: Leveraging human feedback for text-to-video model alignment. arXiv preprint arXiv:2412.04814, 2024. [29] Y. Wang, Z. Li, Y. Zang, Y. Zhou, J. Bu, C. Wang, Q. Lu, C. Jin, and J. Wang, Pref-grpo: Pairwise preference reward-based grpo for stable text-to-image reinforcement learning, arXiv preprint arXiv:2508.20751, 2025. [30] C. Tong, Z. Guo, R. Zhang, W. Shan, X. Wei, Z. Xing, H. Li, and P.-A. Heng, Delving into rl for image generation with cot: study on dpo vs. grpo, arXiv preprint arXiv:2505.17017, 2025. [31] J. Liu, G. Liu, J. Liang, Y. Li, J. Liu, X. Wang, P. Wan, D. Zhang, and W. Ouyang, Flow-grpo: Training flow matching models via online rl, arXiv preprint arXiv:2505.05470, 2025. [32] Z. Xue, J. Wu, Y. Gao, F. Kong, L. Zhu, M. Chen, Z. Liu, W. Liu, Q. Guo, W. Huang et al., Dancegrpo: Unleashing grpo on visual generation, arXiv preprint arXiv:2505.07818, 2025. [33] Y. Zhou, P. Ling, J. Bu, Y. Wang, Y. Zang, J. Wang, L. Niu, and G. Zhai, Ggrpo: Granular grpo for precise reward in flow models, arXiv preprint arXiv:2510.01982, 2025. [34] D. Ghosh, H. Hajishirzi, and L. Schmidt, Geneval: An object-focused framework for evaluating text-to-image alignment, NIPS, vol. 36, pp. 52 13252 152, 2023. [35] K. Huang, K. Sun, E. Xie, Z. Li, and X. Liu, T2i-compbench: comprehensive benchmark for open-world compositional text-to-image generation, NIPS, vol. 36, pp. 78 72378 747, 2023. [36] Y. Niu, M. Ning, M. Zheng, W. Jin, B. Lin, P. Jin, J. Liao, C. Feng, K. Ning, B. Zhu et al., Wise: world knowledge-informed semantic evaluation for text-to-image generation, arXiv preprint arXiv:2503.07265, 2025. JOURNAL OF LATEX CLASS FILES [37] K. Sun, R. Fang, C. Duan, X. Liu, and X. Liu, T2i-reasonbench: Benchmarking reasoning-informed text-to-image generation, arXiv preprint arXiv:2508.17472, 2025. [38] A. Hurst, A. Lerer, A. P. Goucher, A. Perelman, A. Ramesh, A. Clark, A. Ostrow, A. Welihinda, A. Hayes, A. Radford et al., Gpt-4o system card, arXiv preprint arXiv:2410.21276, 2024. [39] X. Hu, R. Wang, Y. Fang, B. Fu, P. Cheng, and G. Yu, Ella: Equip diffusion models with llm for enhanced semantic alignment, arXiv preprint arXiv:2403.05135, 2024. [40] X. Wei, J. Zhang, Z. Wang, H. Wei, Z. Guo, and L. Zhang, Tiif-bench: How does your t2i model follow your instructions? arXiv preprint arXiv:2506.02161, 2025. [41] Google, Gemini2.5-pro, https://deepmind.google/models/gemini/pro/, 2025. [42] Y. Xin, Q. Qin, S. Luo, K. Zhu, J. Yan, Y. Tai, J. Lei, Y. Cao, K. Wang, Y. Wang et al., Lumina-dimoo: An omni diffusion large language model for multi-modal generation and understanding, arXiv preprint arXiv:2510.06308, 2025. [43] C. Deng, D. Zhu, K. Li, C. Gou, F. Li, Z. Wang, S. Zhong, W. Yu, X. Nie, Z. Song et al., Emerging properties in unified multimodal pretraining, arXiv preprint arXiv:2505.14683, 2025. [44] A. Nichol, P. Dhariwal, A. Ramesh, P. Shyam, P. Mishkin, B. McGrew, I. Sutskever, and M. Chen, Glide: Towards photorealistic image generation and editing with text-guided diffusion models, arXiv preprint arXiv:2112.10741, 2021. [45] A. Van Den Oord, O. Vinyals et al., Neural discrete representation learning, Advances in neural information processing systems, vol. 30, 2017. [46] M. Ding, Z. Yang, W. Hong, W. Zheng, C. Zhou, D. Yin, J. Lin, X. Zou, Z. Shao, H. Yang et al., Cogview: Mastering text-to-image generation via transformers, NeurIPS, vol. 34, pp. 19 82219 835, 2021. [47] H. Li, X. Peng, Y. Wang, Z. Peng, X. Chen, R. Weng, J. Wang, X. Cai, W. Dai, and H. Xiong, Onecat: Decoder-only auto-regressive model for unified understanding and generation, arXiv preprint arXiv:2509.03498, 2025. [48] C. Team, Chameleon: Mixed-modal early-fusion foundation models, arXiv preprint arXiv:2405.09818, 2024. [49] A. Cloud, Wan-t2i, https://www.alibabacloud.com/help/en/modelstudio/text-to-image-v2-api-reference, 2025. [50] Runway, Runway-gen4, https://docs.dev.runwayml.com, 2025. [51] Recraft, Recraft, https://www.recraft.ai, 2025. [52] Hidream, Hidream-v2l, https://hidreamai.com/studio, 2025. [53] Stability, Stable image ultra, https://platform.stability.ai/, 2025. [54] Tencent, Hunyuan-image-2.1, https://github.com/TencentHunyuan/HunyuanImage-2.1, 2025. [55] Z. Geng, Y. Wang, Y. Ma, and et. al, X-omni: Reinforcement learning makes discrete autoregressive image generative models great again, arXiv preprint arXiv:2507.22058, 2025. [56] L. Yang, Y. Tian, B. Li, X. Zhang, K. Shen, Y. Tong, and M. Wang, Mmada: Multimodal large diffusion language models, arXiv preprint arXiv:2505.15809, 2025. [57] J. Ye, D. Jiang, Z. Wang, L. Zhu, and et. al, Echo-4o: Harnessing the power of gpt-4o synthetic images for improved image generation, https://arxiv.org/abs/2508.09987, 2025. [58] B. Lin, Z. Li, X. Cheng, Y. Niu, Y. Ye, X. He, S. Yuan, W. Yu, S. Wang, Y. Ge et al., Uniworld: High-resolution semantic encoders for unified visual understanding and generation, arXiv preprint arXiv:2506.03147, 2025. [59] Z. Li, J. Zhang, Q. Lin, J. Xiong, Y. Long, and et. al, Hunyuan-dit: powerful multi-resolution diffusion transformer with fine-grained chinese understanding, 2024. [60] X. Wang, X. Zhang, Z. Luo, Q. Sun, Y. Cui, J. Wang, F. Zhang, Y. Wang, Z. Li, Q. Yu et al., Emu3: Next-token prediction is all you need, arXiv preprint arXiv:2409.18869, 2024. [61] D. Li, A. Kamko, E. Akhgari, A. Sabet, L. Xu, and S. Doshi, Playground v2. 5: Three insights towards enhancing aesthetic quality in text-to-image generation, arXiv preprint arXiv:2402.17245, 2024. [62] K. Team, Kolors: Effective training of diffusion model for photorealistic text-to-image synthesis, arXiv preprint, 2024. [63] Y. Wang, Y. Zang, H. Li, C. Jin, and J. Wang, Unified reward model for multimodal understanding and generation, arXiv preprint arXiv:2503.05236, 2025. [64] S. Bai, K. Chen, X. Liu, J. Wang, W. Ge, S. Song, K. Dang, P. Wang, S. Wang, J. Tang et al., Qwen2.5-vl technical report, arXiv preprint arXiv:2502.13923, 2025. JOURNAL OF LATEX CLASS FILES 15 TABLE VI OVERVIEW OF PROMPT THEMES. WE PROVIDE AN EXAMPLE PROMPT FOR EACH OF THE PROMPT THEMES TO ILLUSTRATE THE SCOPE AND DIVERSITY OF GENERATION SCENARIOS IN OUR BENCHMARK."
        },
        {
            "title": "Illustration",
            "content": "Film & Story Sub-Themes Imaginative"
        },
        {
            "title": "Animation",
            "content": "Ad / E-commerce Design"
        },
        {
            "title": "IP Design",
            "content": "Logo / Icon Design"
        },
        {
            "title": "Design Resources",
            "content": "Example Prompt An astronaut rides dragon made of star dust, shuttling through the rings of Saturn. The picture presents magnificent oil painting texture. In the ink painting style, lonely swordsman stood on the edge of cliff, facing the strong wind. His face had no expression, but his eyes were filled with endless sadness. Please generate graphic art poster: On the left side of the picture is towering city silhouette, on the right side is peaceful forest, and on the top is the text We build the future and cherish the green earth. golden Labrador retriever is leaping excitedly on the green grass, chasing soap bubble that glows with rainbow in the sun, National Geographic photography style. giant elephant sculpture carved from transparent crystal is crystal clear and stands quietly in the center of the museum. Please generate painting: an ancient magic hourglass is being turned upside down. Due to the passage of time, line of English words appears on the stone platform below it: Time reveals all hidden truths and lies. little fox successfully built cabin. It looked proudly at its masterpiece. The wooden sign next to it read in English: The future belongs to those who build it today. There was an open retro wooden jewelry box with an exquisite sapphire necklace lying quietly inside, shining with glimmer. The texture of the movie. An elderly historian wearing white cotton gloves carefully examined yellowed sheepskin scroll map with magnifying glass, with solemn expression. An astronaut wearing spacesuit holds pyramidal holographic projector in his hand, projecting an image of the earth. Pixar animation style, clumsy young wizard whose robe is emitting colorful smoke due to failed spell, and he himself has panicked expression. Please generate an advertisement for fashionable assault coat: young man is standing in the heavy rain, but he does not have an umbrella, but his clothes and hair are not wet at all, and his face shows confident smile. modern library that incorporates elements of the Forbidden City. Its dome is golden caisson structure, presenting grand new Chinese style as whole. The game character design shows mechanical wolf whose body is joined by multiple sharp triangles. The joints exude blue light and have low polygonal style. Design the UI interface of pet health App with cat. Because of its high health index, this kitten is happily wagging its tail. The overall is flat illustration style. Advertising posters, two bottles of anthropomorphic juice drinks, one bottle of orange juice and one bottle of apple juice, they wore swimsuits of similar styles but different colors, lying side by side on beach chairs. cute anthropomorphic alarm clock IP, with line of words Every second is brand new start engraved on the bell above its head, is running happily. logo design has two similar mechanical phoenixes symmetrical left and right, with the same metallic texture in the middle. model with long-chestnut hair wore beige linen suit consisting of long-sleeved top and wide-leg pants, with pen stained with blue ink inserted in the chest pocket of the top. huge blue gear and much smaller red gear mesh with each other, and the latter drives it to rotate slowly, in flat illustration style. JOURNAL OF LATEX CLASS FILES 16 TABLE VII DETAILED BENCHMARKING RESULTS OF T2I MODELS ON UNIGENBENCH++ USING ENGLISH SHORT PROMPTS. Gemini-2.5-Pro IS USED AS THE MLLM FOR EVALUATION. BEST SCORES ARE IN BOLD, SECOND-BEST IN UNDERLINED. English Short Prompt Evaluation Models Overall Style World Know. Attribute Action Relationship Compound Grammar Layout Quant. Express. Materi. Size Shape Color Hand Full Body Animal Non Contact Contact State Compos. Sim. Inclus. Compare. Imagin. Feat Match. Pron Ref. Consist. Neg. 2D 3D HiDream-v2L Stable-Image-Ultra Recraft Wan2.2-Plus DALL-E-3 Runway-Gen4 FLUX-Pro-1.1-Ultra Imagen-3.0 FLUX-Kontext-Pro Imagen-4.0-Fast Wan2.5-t2i-preview Seedream-3.0 FLUX-Kontext-Max Imagen-4.0 Seedream-4.0 Nano Banana Imagen-4.0-Ultra GPT-4o 61.64 61.96 62.63 64.82 69.18 69.75 70.67 71.85 75.84 77.75 78.17 78.95 80.00 85.84 87.35 87.45 91.54 92.77 SDXL MMaDA Kolors Playground2.5 Emu3 Janus-flow Janus Hunyuan-DiT X-Omni CogView4 OneCAT Infinity BLIP3-o SD-3.5-Medium FLUX.1-dev Bagel Janus-Pro Show-o2 SD-3.5-Large OmniGen2 UniWorld-V1 BLIP3-o-Next Echo-4o FLUX.1-Krea-dev Lumina-DiMOO 39.75 41.35 45.47 45.61 46.02 46.39 51.23 51.38 53.77 56.30 58.28 59.81 59.87 60.71 61.30 61.53 61.61 62.73 62.99 63.09 63.11 65.15 69.12 69.88 71.12 HiDream-I1-Full 71.81 Hunyuan-Image-2.1 74.64 78.81 Qwen-Image 87.99 89.62 87.20 87.18 87.20 90.19 91.10 87.34 95.06 93.51 93.44 90.36 90.60 91.61 89.25 94.75 94.78 91.61 92.00 94.78 93.15 95.22 98.10 95.25 96.59 94.19 97.80 96.36 98.80 95.41 98.87 96.32 99.20 97.47 98.57 98.87 87.40 72.63 82.40 56.65 84.40 77.22 89.50 76.11 86.80 77.06 86.20 62.50 89.90 73.58 94.10 80.70 72.70 76.27 82.00 83.07 93.30 82.28 90.80 87.97 92.80 80.22 89.80 84.34 83.90 88.92 90.20 85.60 90.80 86.71 87.20 86.08 88.60 88.92 91.90 86.39 91.10 82.91 91.00 86.71 92.20 90.51 88.70 92.56 89.70 90.03 92.50 94.15 90.88 92.06 95.10 94.30 65.71 67.36 68.06 76.39 62.14 72.86 75.69 75.78 75.00 77.08 75.00 80.56 75.69 84.03 86.81 85.00 93.06 90.00 44.44 45.83 62.50 58.33 44.44 43.06 37.50 67.36 63.19 71.53 59.42 66.67 51.39 59.72 72.22 59.03 56.25 59.03 71.53 67.36 70.14 67.36 70.14 70.83 69.44 73.61 86.62 81.94 44.87 48.08 56.41 55.77 59.87 51.97 59.62 64.67 71.62 75.00 67.95 82.05 74.32 76.92 85.90 83.33 81.41 94. 25.00 29.49 33.33 43.59 45.51 30.77 37.82 44.23 53.21 44.23 58.33 53.21 60.26 51.92 53.85 50.00 55.77 63.46 51.92 73.08 64.74 73.72 71.15 60.90 85.90 59.62 72.44 84.62 Closed-source Models 57.82 74.26 59.87 94.92 51.28 58.56 67.65 64.15 69.44 64.38 91.67 55.77 58.15 63.24 70.75 65.97 57.50 95.83 50.00 70.65 76.47 66.51 71.53 64.38 94.17 58.33 75.82 69.12 87.74 87.50 65.00 92.50 60.90 75.00 76.47 89.42 68.06 65.62 95.00 62.18 79.35 82.35 78.77 77.78 74.38 96.67 57.69 68.48 77.21 80.66 82.84 70.00 93.10 80.00 83.89 85.29 76.89 84.72 74.38 97.50 75.00 79.35 80.88 85.85 89.58 78.75 98.33 73.72 84.24 81.62 91.04 85.29 77.50 87.50 61.18 75.00 76.47 90.57 85.42 78.12 97.50 75.00 89.67 85.29 82.55 86.81 74.38 94.17 67.95 83.15 77.94 90.57 89.58 71.88 98.33 86.54 94.02 88.97 97.17 84.03 76.88 100.00 77.56 87.50 88.24 88.50 95.74 78.21 99.17 82.05 93.41 86.03 94.34 95.83 91.88 100.00 90.38 93.44 91.91 94.20 91.61 92.50 99.17 89.74 92.22 87.12 61.98 61.22 55.61 68.88 66.84 66.15 76.53 77.37 71.94 76.53 75.00 75.51 77.04 85.71 80.10 82.47 90.31 90.43 51.52 51.79 48.81 57.74 63.41 60.37 64.29 74.40 73.21 76.79 72.02 80.95 70.83 83.33 83.93 83.33 89.29 89.82 Open-source Models 52.83 44.44 33.75 68.33 19.23 35.33 43.38 54.25 49.31 44.38 74.17 15.38 40.22 52.94 51.89 62.50 40.62 83.33 42.95 42.39 56.62 57.08 44.44 41.25 75.83 28.85 50.00 52.21 53.77 43.06 46.25 80.00 25.00 47.28 50.74 55.19 55.56 30.00 78.33 23.08 48.37 58.82 58.96 65.97 47.50 86.67 32.69 51.63 61.76 71.70 61.81 47.50 86.67 35.90 54.89 54.41 58.96 55.56 53.75 80.83 46.79 56.52 62.50 55.19 72.22 57.50 89.17 53.85 59.78 68.38 67.45 65.97 42.50 92.50 35.90 65.22 69.12 66.04 77.78 58.75 93.33 55.13 65.22 72.06 64.62 75.00 54.37 81.67 58.33 70.11 70.59 67.92 70.83 63.75 93.33 50.00 63.04 69.12 58.96 75.00 65.00 91.67 51.28 67.39 69.85 72.64 76.39 59.38 93.33 52.56 60.87 69.12 71.70 73.61 61.88 90.83 50.64 63.04 75.00 73.58 72.92 63.12 95.00 56.41 77.72 72.79 68.87 68.06 65.62 90.83 57.05 61.96 63.24 66.04 72.22 66.25 95.00 55.77 69.02 68.38 61.32 72.22 66.25 99.17 55.13 72.28 73.53 70.28 76.39 60.62 80.00 57.69 75.00 73.53 84.91 83.33 68.75 98.33 66.03 66.30 77.94 77.36 79.17 73.12 99.17 64.74 70.11 77.94 81.60 76.39 80.00 99.17 64.10 78.80 75.74 72.17 79.17 61.88 98.33 62.18 76.09 73.53 78.77 78.47 68.12 99.17 75.00 80.98 82.35 91.98 84.03 84.38 99.17 82.05 88.59 88.24 26.53 33.16 45.92 35.20 35.20 36.73 48.47 46.94 56.63 50.51 57.65 58.16 60.20 55.61 59.69 62.24 62.24 70.41 62.24 62.24 63.78 67.35 67.86 72.96 73.98 74.49 73.71 80.61 24.40 25.60 39.88 29.17 27.98 36.31 38.10 35.71 42.26 51.19 48.81 49.40 51.79 52.98 58.93 58.93 56.55 52.38 59.52 54.17 61.90 57.74 59.52 67.26 64.88 70.24 72.02 77.38 65.09 64.15 63.21 75.00 75.47 71.70 76.89 87.38 84.91 84.91 82.55 90.09 84.43 91.04 94.81 91.98 96.70 93.75 53.30 56.60 59.43 58.02 52.36 55.66 66.51 62.74 60.85 62.74 71.23 62.26 71.70 71.70 65.57 67.45 76.42 83.02 67.45 66.51 75.00 68.87 75.94 73.11 82.08 78.77 82.55 87.74 71.23 72.64 64.53 70.27 82.43 74.32 80.41 83.90 81.42 83.45 85.14 82.77 87.50 93.58 88.18 94.76 95.27 96. 53.72 55.07 55.41 60.14 56.76 59.80 56.76 60.14 61.82 60.47 78.04 73.31 70.61 74.66 62.50 76.35 76.01 79.05 75.34 68.24 72.30 76.01 81.76 76.35 83.45 79.05 78.38 81.76 64.20 65.93 66.67 70.11 59.44 59.24 67.98 77.72 69.44 87.78 62.22 77.84 72.78 82.07 73.33 88.64 75.56 83.33 73.89 89.13 75.00 82.07 73.89 84.24 78.89 90.00 78.89 95.11 80.56 94.02 86.52 91.26 84.44 98.37 95.00 94.89 38.33 39.67 57.22 47.28 53.89 51.63 49.44 48.37 46.67 48.37 38.89 51.63 53.89 59.24 64.44 60.33 56.11 51.09 60.00 69.57 69.44 62.50 65.00 67.39 60.00 67.39 61.67 73.37 66.67 72.83 70.56 69.57 56.11 75.00 61.11 70.11 68.33 68.48 67.78 71.20 63.33 64.67 65.00 77.17 70.56 77.72 66.11 77.17 74.44 81.52 68.33 78.26 70.56 84.78 67.78 86.96 60.32 62.50 67.19 76.69 66.41 75.78 71.09 83.90 74.22 82.03 85.94 81.25 81.25 85.94 87.50 94.53 92.19 92.19 41.41 33.59 46.88 39.06 39.84 40.62 46.88 50.78 53.12 60.16 51.56 67.97 64.84 58.59 62.50 59.38 58.59 62.50 60.94 64.84 64.06 75.00 71.09 75.00 67.97 72.66 75.00 81.25 53.75 60.97 43.37 66.92 76.79 71.65 74.74 79.23 75.00 80.10 79.38 78.57 83.93 90.31 88.27 89.66 92.86 95. 33.93 40.56 41.33 43.88 41.33 57.65 58.16 46.68 47.45 47.19 66.33 55.87 61.73 58.16 47.96 67.35 69.64 69.90 64.80 62.24 58.16 73.72 76.79 67.35 78.83 64.29 64.54 73.21 44.76 72.35 47.40 78.68 46.35 73.16 55.73 73.90 64.21 74.24 63.71 71.21 60.68 84.56 64.06 79.04 70.31 84.23 67.97 86.03 73.04 84.07 69.01 79.78 73.96 84.23 80.21 86.76 83.85 84.93 86.02 90.71 89.84 94.12 91.40 92.91 19.27 50.37 23.96 59.19 25.78 56.62 26.82 58.82 32.29 59.56 32.29 66.18 34.90 66.18 36.46 62.87 35.94 66.91 42.19 69.49 47.40 70.59 46.88 73.16 45.57 79.04 48.44 73.53 46.09 73.16 48.70 71.69 54.43 75.37 59.38 75.37 52.60 74.63 50.26 71.32 50.78 74.26 55.73 76.47 66.67 80.51 61.46 77.21 67.71 81.99 60.94 83.09 65.10 77.94 73.44 83.82 60.00 58.33 58.33 56.74 74.07 67.59 68.98 70.75 76.85 75.00 73.15 69.91 78.70 77.31 79.17 82.08 87.04 91.67 42.59 40.28 47.22 50.00 53.70 48.61 51.39 57.87 54.17 56.02 59.72 65.74 61.11 61.57 63.43 68.52 66.20 65.28 61.11 60.65 64.35 67.13 74.54 67.13 77.78 65.74 66.20 70.37 44.23 70.41 67.68 45.00 67.28 61.74 58.08 58.82 56.82 66.92 77.49 71.97 56.64 57.72 76.17 71.03 77.61 75.00 55.77 80.15 82.95 59.13 82.72 79.92 57.69 85.98 82.95 68.46 88.24 84.09 63.08 75.74 79.55 35.00 86.76 87.88 72.69 86.74 88.33 74.23 88.24 89.39 72.31 90.81 90.53 76.59 92.65 91.25 82.31 92.65 93.56 90.57 91.04 91. 48.08 26.47 33.33 65.00 30.15 30.30 35.77 43.01 42.80 50.00 34.56 39.77 45.38 45.22 44.32 63.85 49.26 43.56 58.08 57.72 51.89 45.77 39.34 50.38 55.00 69.49 55.68 38.46 77.21 60.98 51.54 64.34 65.15 41.92 71.69 61.36 63.85 72.79 64.02 44.23 72.06 68.56 46.15 74.26 69.32 59.23 79.04 73.86 51.54 74.63 69.32 44.23 77.94 72.73 40.77 70.96 67.05 47.31 78.31 64.77 52.31 73.90 64.02 60.00 80.15 72.35 70.00 87.13 77.27 45.77 86.76 81.44 52.31 84.93 80.68 40.38 82.72 73.48 44.23 86.76 81.44 27.31 86.40 85.23 Logic. Reason. Text 26.73 31.59 29.55 42.05 48.18 49.31 43.18 48.36 55.68 56.36 56.36 52.73 61.36 70.45 67.73 74.26 79.55 84.97 9.55 17.95 19.77 16.59 19.32 21.14 26.82 24.55 29.09 28.18 33.41 31.36 39.55 37.73 30.91 30.23 37.05 40.91 32.27 32.50 38.41 48.64 44.77 39.77 45.45 41.14 46.59 53.64 44.31 39.08 61.78 13.83 25.86 33.43 37.36 21.55 50.29 51.44 71.97 71.55 61.92 77.30 93.97 75.22 89.08 89. 1.15 1.15 1.15 1.15 1.15 0.86 1.15 1.15 25.00 17.82 1.15 12.36 1.15 15.23 32.18 7.76 2.59 1.15 32.76 29.02 26.44 4.60 10.06 44.83 25.57 64.94 70.11 76.14 JOURNAL OF LATEX CLASS FILES 17 TABLE VIII DETAILED BENCHMARKING RESULTS OF T2I MODELS ON UNIGENBENCH++ USING ENGLISH LONG PROMPTS. Gemini-2.5-Pro IS USED AS THE MLLM FOR EVALUATION. BEST SCORES ARE IN BOLD, SECOND-BEST IN UNDERLINED. English Long Prompt Evaluation Models Overall Style World Know. Attribute Action Relationship Compound Grammar Layout Quant. Express. Materi. Size Shape Color Hand Full Body Animal Non Contact Contact State Compos. Sim. Inclus. Compare. Imagin. Feat Match. Pron Ref. Consist. Neg. 2D 3D Recraft Stable-Image-Ultra Runway-Gen4 Wan2.2-Plus DALL-E-3 FLUX-Pro-1.1-Ultra Imagen-3.0 FLUX-Kontext-Pro FLUX-Kontext-Max Seedream-3.0 Imagen-4.0-Fast Wan2.5 Imagen-4.0 Nano Banana Seedream-4.0 Imagen-4.0-Ultra GPT-4o 60.93 62.01 68.29 68.76 70.82 75.40 75.76 78.58 80.88 80.99 81.54 84.34 85.34 88.82 89.77 90.95 92.63 40.10 MMaDA 41.48 SDXL 50.95 Emu3 53.60 Kolors 54.80 Janus-flow 54.88 Hunyuan-DiT 60.37 Janus 61.01 BLIP3-o 62.92 OneCAT 64.35 SD-3.5-Large 64.67 SD-3.5-Medium 67.00 X-Omni 67.28 Infinity 67.68 CogView4 69.42 FLUX.1-dev 69.60 UniWorld-V1 70.33 Show-o2 71.03 BLIP3-o-Next 71.11 Janus-Pro 71.26 Bagel 71.39 OmniGen2 74.25 HiDream-I1-Full 78.45 FLUX.1-Krea-dev 71.81 Lumina-DiMOO Echo-4o 76.41 Hunyuan-Image-2.1 82.19 83.94 Qwen-Image 87.13 86.99 85.63 86.71 91.72 88.82 90.28 87.57 95.08 92.71 91.36 91.76 92.41 94.19 94.83 93.60 96.51 93.35 97.18 93.79 93.77 93.64 96.75 95.52 94.44 97.11 98.83 95.78 98.42 95.95 97.67 98.26 99.08 97.95 75.83 52.75 81.81 69.51 89.36 76.16 86.54 76.01 88.70 65.90 92.94 80.06 92.03 73.27 91.61 74.42 94.93 83.67 88.12 88.15 92.19 86.56 80.15 82.37 92.77 88.44 88.29 89.45 89.29 89.45 93.19 84.10 93.11 88.44 94.60 88.87 94.02 88.15 92.44 89.31 94.35 84.83 93.11 92.63 94.10 93.79 86.88 88.58 96.10 90.17 94.52 93.35 96.93 95.09 56.38 66.49 70.65 78.19 64.67 79.26 75.58 74.47 79.79 83.51 78.72 85.64 82.45 88.24 92.02 89.84 86.70 50.53 39.36 44.68 61.17 42.55 65.43 42.55 54.26 61.70 68.62 61.70 66.49 70.74 74.47 73.94 66.49 59.04 70.74 62.23 69.68 66.49 73.40 81.38 74.47 73.40 86.17 92.02 57.22 55.69 65.43 69.17 72.59 68.58 71.41 75.00 76.68 81.25 78.89 81.01 77.64 86.09 89.31 83.17 93.44 37.22 44.03 48.47 50.42 43.89 52.22 48.61 61.81 66.39 62.22 62.64 70.83 66.67 66.53 64.44 72.64 71.53 80.00 66.39 70.28 73.89 68.47 76.81 76.11 82.08 85.56 89. Closed-source Models 72.82 76.89 63.64 83.07 40.06 54.37 55.07 76.43 77.27 67.48 83.02 58.33 49.38 59.42 85.33 81.01 67.38 85.64 55.33 63.92 70.65 80.42 82.77 73.60 88.10 64.10 60.94 70.29 88.72 89.48 77.14 90.15 63.49 63.96 67.03 82.98 89.96 80.59 93.01 67.31 66.25 73.19 88.34 88.52 78.27 93.13 73.63 77.12 76.81 85.47 89.58 80.63 92.89 73.05 73.12 75.00 87.35 88.83 81.51 93.74 73.08 75.94 74.28 93.07 88.26 90.03 97.48 77.88 84.69 78.26 91.11 90.15 86.89 96.33 82.05 84.06 81.88 94.03 88.17 87.50 96.11 73.08 82.91 77.21 90.96 92.23 86.36 95.60 83.65 82.81 78.62 93.05 93.70 88.73 97.31 84.57 84.95 81.16 95.26 94.70 92.48 98.27 83.01 87.50 81.52 94.20 94.69 89.86 97.22 89.10 86.56 85.14 92.45 94.89 92.48 94.95 89.94 87.19 90.94 45.09 52.23 56.82 59.38 59.55 66.96 69.44 67.73 66.82 74.11 75.00 71.76 85.27 83.41 88.39 86.61 89.29 37.36 45.98 56.10 55.46 60.17 62.07 65.48 70.40 71.55 71.84 74.71 69.83 78.74 78.16 83.62 81.84 83.05 Open-source Models 47.52 54.55 40.56 57.81 16.67 30.63 38.77 58.89 58.14 43.01 58.81 19.23 29.69 29.35 68.65 73.24 54.29 76.61 28.85 46.25 43.48 72.67 71.97 58.74 74.06 39.74 38.44 50.36 63.18 71.59 45.98 76.47 26.60 50.94 53.26 72.14 75.19 58.22 76.31 39.10 46.25 47.46 71.31 79.17 57.69 82.86 39.42 57.19 64.86 70.93 78.22 57.87 78.88 48.08 54.69 61.23 78.09 82.58 62.24 78.88 37.82 59.06 62.32 81.85 78.79 70.63 86.32 57.69 52.81 57.25 83.73 82.01 73.60 87.79 58.01 56.56 54.35 81.33 81.44 69.93 86.01 58.97 63.44 62.68 82.83 82.95 71.15 88.73 58.65 60.13 67.75 79.74 83.14 74.30 88.21 68.91 60.31 65.94 80.05 84.47 71.50 87.47 63.78 62.50 65.94 77.11 81.06 72.38 87.95 63.78 64.38 67.03 88.10 87.31 81.12 94.71 53.85 80.00 69.20 81.93 86.36 71.85 81.81 65.71 68.44 73.55 83.43 85.42 75.87 89.20 57.69 73.44 76.09 85.17 86.17 76.92 91.88 68.59 67.19 68.48 81.78 81.63 77.80 90.93 67.31 64.06 65.22 83.51 84.47 75.70 92.19 65.06 68.44 62.32 91.34 88.64 85.31 95.44 75.00 76.25 72.46 80.80 84.47 78.67 90.83 67.63 71.56 72.46 92.39 89.20 84.44 95.49 72.12 76.56 73.19 93.75 90.34 87.24 97.90 82.05 81.88 79.71 94.50 89.58 86.71 97.85 78.53 81.88 83. 19.64 17.41 30.49 44.64 39.29 41.07 51.34 46.88 50.89 50.89 42.86 56.25 58.48 53.12 56.70 62.95 60.27 60.71 62.95 58.48 64.29 71.43 69.20 65.18 66.96 76.79 83.04 17.24 16.67 25.57 34.20 35.92 34.48 40.23 35.92 43.97 48.85 46.55 48.56 52.87 56.32 56.32 55.17 55.75 60.63 61.21 59.77 54.60 57.47 72.99 57.18 65.23 75.00 71.84 60.08 66.30 69.76 73.32 76.29 80.53 80.62 77.98 79.76 83.60 80.93 81.27 84.09 86.28 89.82 88.63 87.75 44.17 43.87 56.92 63.24 59.98 59.58 64.23 64.82 71.44 68.68 68.18 68.08 69.07 68.97 69.57 70.85 76.68 76.58 73.52 71.94 72.13 75.20 80.43 74.21 77.47 84.09 85.57 51.79 64.92 70.05 69.13 80.57 81.89 80.15 73.85 77.30 81.63 82.53 85.26 86.48 90.98 87.37 90.05 89.18 39.16 41.07 53.77 58.04 58.55 56.89 62.76 60.97 67.47 70.15 70.15 59.69 66.20 61.86 65.05 66.96 77.42 72.32 77.42 72.19 67.73 72.07 80.87 69.77 83.80 83.93 81. 46.47 66.09 56.73 67.53 59.09 76.76 66.67 81.03 70.51 83.53 74.04 90.52 74.17 90.59 72.08 89.08 73.05 89.94 79.17 87.64 80.13 92.82 81.41 94.48 80.13 91.38 91.32 92.80 80.77 93.97 84.62 94.52 90.71 96.84 33.97 48.56 27.88 42.24 42.31 59.48 58.01 62.36 52.88 60.34 55.45 57.18 60.26 67.82 57.69 62.36 62.82 63.22 62.18 70.11 62.82 75.86 58.97 67.53 67.63 78.45 64.10 76.44 66.03 79.60 67.31 72.99 68.59 80.17 70.19 81.03 71.15 82.18 72.12 85.92 72.76 81.90 73.40 78.74 73.08 88.22 72.76 82.18 78.21 84.77 78.53 92.82 79.17 88.79 61.89 63.11 70.39 77.43 73.76 80.58 78.54 82.77 85.44 86.41 82.52 88.11 86.89 91.91 92.72 92.72 90.29 34.71 28.40 53.77 56.55 59.95 52.18 62.62 69.66 65.05 64.81 69.66 74.27 72.09 70.87 71.60 70.39 81.55 77.18 80.58 76.46 75.97 75.49 84.47 73.06 82.77 85.92 85.19 50.21 62.66 69.47 74.16 77.67 80.40 81.14 83.58 84.75 80.49 86.18 87.55 86.81 92.15 88.19 92.82 94.39 45.99 41.24 51.69 52.11 62.34 55.49 69.73 70.89 72.57 65.82 65.61 65.51 68.57 68.99 71.10 74.16 77.64 78.80 80.59 77.32 72.47 73.63 80.59 77.00 85.44 82.28 82. 48.13 73.41 48.60 76.19 66.50 76.23 66.36 86.90 65.00 82.92 72.88 84.52 73.22 91.67 71.23 90.32 76.65 90.08 82.24 90.48 79.21 91.27 81.31 92.86 85.98 94.05 87.23 94.84 86.92 95.63 88.32 96.83 93.10 95.97 21.50 53.97 18.93 53.57 33.41 55.95 36.45 72.22 39.25 71.03 38.55 64.68 44.39 74.21 53.74 74.60 43.69 74.21 54.21 75.79 56.78 79.37 61.21 82.14 60.75 76.59 62.15 86.51 62.62 83.33 65.19 84.13 73.83 87.30 64.25 83.33 67.52 87.30 68.93 87.30 66.12 84.52 61.21 86.51 80.84 91.27 70.33 89.68 83.64 86.11 82.94 91.27 81.07 90.48 55.56 61.11 62.70 61.11 66.27 68.55 76.61 75.40 76.61 80.56 81.35 77.42 80.56 89.24 83.33 87.70 91.67 39.29 37.70 42.46 53.57 50.00 59.52 59.52 62.30 67.46 61.51 61.11 61.90 71.43 67.46 67.46 69.44 66.67 73.02 73.81 70.63 75.79 69.84 74.21 66.67 83.33 75.79 78.57 52.82 65.96 61.05 58.80 74.86 67.57 72.76 72.56 75.37 63.38 82.34 75.00 56.99 69.22 75.00 63.73 81.78 83.70 66.67 83.97 88.69 66.90 84.09 87.23 72.18 85.73 89.96 56.69 87.85 89.13 67.61 90.11 90.94 65.49 88.28 85.77 70.77 90.40 90.04 84.51 94.77 93.12 70.77 92.94 91.67 80.63 92.64 94.57 95.65 94.29 92.70 55.99 47.46 37.32 48.94 39.12 42.03 52.11 56.36 57.07 41.55 61.02 60.87 69.72 60.03 61.05 52.82 60.45 62.68 67.96 62.85 65.76 59.86 77.40 70.11 50.70 75.28 73.01 59.15 73.45 68.30 58.10 73.59 72.83 63.03 78.25 67.03 58.80 80.93 73.19 62.32 83.62 75.00 61.97 81.21 72.83 72.18 83.33 74.82 58.45 80.08 81.34 72.18 82.20 78.80 64.08 81.78 82.61 67.25 83.47 79.89 69.72 82.20 78.62 62.68 82.63 76.45 61.97 85.45 86.59 67.96 90.11 78.08 78.17 88.70 83.51 66.55 90.25 86.59 54.93 91.24 86. Logic. Reason. Text 34.22 40.29 48.28 55.58 57.11 60.92 61.25 66.26 71.12 62.62 67.72 71.32 72.82 81.27 79.13 83.50 91.02 19.42 19.42 27.43 31.31 41.75 29.85 54.37 48.30 48.06 44.90 45.87 51.70 51.46 49.76 54.37 57.04 59.71 65.53 62.62 59.71 56.55 50.24 65.53 58.01 69.42 68.20 66.75 46.47 15.76 27.47 12.77 18.26 38.04 24.18 49.73 54.89 56.52 51.63 73.10 71.74 69.75 90.76 86.41 83.79 0.27 0.82 1.36 2.17 1.63 1.63 1.09 1.36 1.90 17.66 11.41 43.48 13.59 19.02 30.71 20.92 1.90 4.89 4.08 12.23 27.99 57.61 41.03 23.64 8.15 58.15 76. JOURNAL OF LATEX CLASS FILES 18 TABLE IX DETAILED BENCHMARKING RESULTS OF T2I MODELS ON UNIGENBENCH++ USING CHINESE SHORT PROMPTS. Gemini-2.5-Pro IS USED AS THE MLLM FOR EVALUATION. BEST SCORES ARE IN BOLD, SECOND-BEST IN UNDERLINED. Models Overall Style World Know. Attribute Action Relationship Compound Grammar Layout Logic. Reason. Text Chinese Short Prompt Evaluation Quant. Express. Materi. Size Shape Color Hand Full Body Animal Non Contact Contact State Compos. Sim. Inclus. Compare. Imagin. Feat Match. Pron Ref. Consist. Neg. 2D 3D Closed-source Models Runway-Gen4 Recraft HiDream-v2L Wan2.2-Plus DALL-E-3 Imagen-4.0-Fast FLUX-Kontext-Max Wan2.5 Imagen-4.0 Nano Banana Seedream-3.0 Imagen-4.0-Ultra Seedream-4.0 GPT-4o 54.93 57.67 59.95 66.96 67.93 71.60 71.85 78.40 79.52 80.91 81.68 83.21 87.31 91.02 64.75 71.05 87.70 90.03 89.34 91.02 91.06 84.39 95.90 93.04 93.30 91.30 96.38 92.83 93.30 93.51 97.50 96.84 99.27 96.47 97.50 93.99 98.90 97.94 99.00 94.94 99.39 98.72 UniWorld-V1 Janus-flow Janus-Pro Janus Emu3 MMaDA BLIP3-o-Next HiDream-I1-Full Hunyuan-DiT X-Omni CogView4 Lumina-DiMOO OneCAT Kolors BLIP3-o OmniGen2 Bagel 15.21 20.93 30.83 30.98 33.91 44.00 44.48 50.65 53.36 53.69 55.14 58.35 58.50 58.80 59.25 63.20 65.69 Echo-4o 72.40 Hunyuan-Image-2.1 77.76 81.04 Qwen-Image 49.40 16.61 58.50 18.67 75.60 39.08 78.10 27.85 78.08 55.54 78.20 52.06 74.60 50.00 83.30 78.32 92.50 84.97 70.07 71.52 82.40 84.18 80.90 69.46 94.40 86.55 85.20 86.23 92.60 81.17 93.00 86.39 92.30 86.71 92.80 87.66 92.20 90.51 95.50 92.41 54.29 66.67 71.43 75.00 60.42 76.39 65.97 78.47 83.33 81.62 84.03 88.89 86.81 93. 14.58 22.92 24.31 29.17 27.78 52.78 44.44 69.44 63.19 61.81 68.75 62.50 56.94 70.14 57.64 67.36 64.58 72.92 87.50 88.89 46.05 59.62 42.31 67.31 68.59 66.03 69.44 75.64 77.56 80.79 82.69 79.49 85.90 94.59 19.87 10.90 19.23 17.31 30.13 33.97 57.69 45.51 46.15 52.56 44.87 71.79 66.03 51.92 65.38 69.87 63.46 77.56 80.77 91.03 72.60 57.64 50.62 81.90 52.63 65.22 75.00 66.51 73.61 61.25 95.83 50.64 72.28 77.94 70.59 70.00 64.52 94.17 48.72 65.22 75.00 74.06 74.31 66.25 90.83 69.23 80.00 84.56 91.04 90.28 65.00 94.17 69.87 77.17 82.35 83.49 88.19 78.75 95.83 74.36 79.35 83.82 80.19 84.72 66.67 93.33 76.32 83.15 83.33 90.09 84.72 76.88 96.67 73.72 72.28 81.62 92.92 93.75 72.50 98.33 89.10 89.67 93.38 89.66 95.74 82.05 98.33 86.54 91.38 90.44 94.34 89.58 80.00 97.50 85.26 90.76 89.71 94.81 93.75 88.12 100.00 94.87 92.93 95.59 97.64 86.81 83.12 99.17 82.69 90.22 91.91 96.19 93.06 92.95 100.00 94.08 97.28 90.91 51.56 63.78 71.88 65.31 66.33 73.47 69.90 77.04 86.73 81.96 85.20 87.76 84.69 90.31 54.37 45.24 55.95 61.90 61.90 75.60 73.17 73.81 90.48 81.44 80.36 95.24 82.74 88. Open-source Models 9.62 17.93 18.38 37.50 8.02 13.19 5.00 21.70 24.31 8.12 4.49 31.52 22.06 30.00 43.87 45.14 18.75 47.50 13.46 26.09 34.56 35.85 45.83 14.37 17.31 14.10 38.59 42.65 44.34 32.64 27.67 71.67 16.67 36.96 49.26 58.49 61.11 45.00 86.67 24.36 54.35 47.06 56.13 63.89 48.12 68.33 37.82 61.41 45.59 55.66 70.14 55.00 86.67 44.23 57.61 55.88 72.17 63.89 49.38 85.00 45.51 67.93 61.76 63.51 67.36 57.50 85.83 48.72 68.48 63.97 56.60 72.92 53.75 94.17 61.54 66.30 64.71 77.83 78.47 70.00 96.67 42.95 61.41 76.47 73.58 65.28 38.75 84.17 42.31 75.00 80.88 73.11 77.78 56.25 91.67 58.33 59.24 71.32 67.92 77.08 47.50 89.17 57.69 73.37 68.38 78.30 77.78 68.75 93.33 64.10 69.57 74.26 83.49 79.86 66.25 95.00 61.54 63.59 75.74 89.15 88.19 80.00 99.17 73.08 83.15 85.29 82.55 86.11 75.00 97.50 76.28 84.24 85.29 96.23 90.28 86.25 98.33 83.33 87.50 89.71 9.69 14.80 22.45 24.49 26.02 31.63 45.41 53.06 48.47 56.53 52.04 58.67 61.22 63.78 59.18 61.73 65.31 75.00 78.06 81.63 6.55 19.05 20.83 23.21 17.86 29.17 36.90 47.62 47.02 43.45 54.76 51.79 44.05 57.54 55.95 55.95 61.90 65.48 79.17 82.14 65.09 72.17 71.15 75.94 76.89 88.21 85.78 81.13 93.40 90.64 90.09 97.17 92.45 92.65 24.06 35.85 38.68 43.40 40.57 67.92 54.72 61.32 69.81 66.51 70.28 74.06 73.58 77.83 70.28 73.58 67.92 75.47 80.66 90. 66.89 65.54 78.82 71.28 81.76 82.09 85.14 80.07 91.55 92.33 86.82 91.22 85.14 97.30 16.55 23.65 38.85 32.43 43.58 59.80 54.05 57.77 65.88 60.14 61.82 68.58 72.64 71.96 69.26 77.03 77.70 85.81 80.74 85.47 51.11 74.43 58.89 65.22 65.00 75.56 72.78 85.87 77.78 87.50 78.33 88.04 74.43 91.67 73.33 88.04 83.33 94.57 83.89 93.44 74.44 90.22 87.22 97.83 84.44 95.65 93.18 96.69 6.67 12.50 16.11 20.11 35.56 26.09 32.22 27.72 31.67 38.04 52.22 60.87 48.33 50.00 52.78 63.04 64.44 56.52 60.00 62.50 62.22 63.59 62.78 76.09 61.67 69.57 69.44 67.39 58.33 63.04 66.67 71.74 67.78 82.07 75.00 88.04 80.56 87.50 73.33 90.76 72.66 68.75 65.32 82.03 67.97 81.25 83.59 89.06 93.75 96.88 84.38 92.97 92.19 94.53 7.03 14.06 24.22 28.12 25.78 46.88 64.84 53.91 41.41 54.69 57.81 57.03 60.16 52.34 69.53 60.16 71.09 75.78 83.59 79. 68.22 45.92 62.63 74.23 82.14 83.67 82.65 84.95 92.60 90.40 82.14 94.90 85.20 95.92 6.63 19.13 33.42 25.26 29.85 39.29 32.14 38.01 52.04 48.72 51.02 56.96 63.52 64.80 61.99 66.33 79.59 82.91 71.68 80.10 53.49 55.38 41.93 62.87 43.55 75.38 55.00 77.21 63.54 79.78 64.06 83.82 67.12 79.85 72.40 82.72 78.65 92.65 83.42 87.27 71.09 84.19 84.90 93.01 77.86 89.71 91.74 95.15 19.85 2.08 2.08 32.72 15.36 36.76 9.64 48.53 13.28 41.91 26.30 59.93 20.83 65.07 30.99 62.13 36.98 59.93 34.64 63.97 40.36 67.65 52.34 76.10 39.32 64.34 45.05 67.28 41.41 70.22 53.39 71.69 59.90 73.16 72.92 80.15 69.53 80.15 72.14 83.46 55.09 59.26 68.75 63.43 76.39 78.24 75.46 70.37 82.87 84.69 79.17 85.65 75.00 89.35 16.20 16.67 31.94 33.33 38.89 46.30 49.54 51.85 62.04 53.70 57.41 70.37 60.19 59.26 57.41 71.30 75.00 77.31 67.13 74. 64.29 59.93 69.62 59.23 55.15 61.74 44.53 66.29 63.26 69.62 73.16 70.45 58.85 54.41 70.83 70.00 80.51 76.89 71.48 81.62 81.06 63.67 76.10 75.76 72.69 91.54 86.74 78.12 91.82 85.66 39.62 89.34 78.79 83.08 93.75 90.53 69.62 90.81 89.77 88.05 89.18 89.35 45.77 8.09 10.23 52.69 12.13 17.80 40.38 29.78 30.30 60.77 31.25 32.20 42.69 17.71 27.27 67.31 38.97 35.61 46.54 58.82 50.76 46.92 63.60 55.68 43.08 39.71 56.06 50.38 66.91 51.89 38.46 75.00 55.30 48.46 73.53 64.77 52.69 61.76 59.09 43.46 58.82 65.91 61.16 69.12 62.12 54.62 76.84 62.88 61.15 82.72 72.35 68.85 84.19 81.82 37.31 88.24 82.58 31.92 84.93 80.30 42.03 34.09 32.01 51.82 51.59 54.77 56.48 63.64 73.18 76.10 59.09 79.32 68.64 91.44 2.95 10.68 10.23 13.41 13.90 26.14 27.50 23.64 29.55 34.77 30.23 39.09 38.64 36.14 41.59 44.09 37.95 56.82 50.23 57.73 0.59 4.31 1.16 11.92 1.15 3.74 1.72 64.22 2.59 12.06 78.74 9.77 93.97 63.37 0.29 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 20.98 2.30 0.00 0.00 4.89 0.00 0.29 6.61 7.76 79.60 82. JOURNAL OF LATEX CLASS FILES 19 TABLE DETAILED BENCHMARKING RESULTS OF T2I MODELS ON UNIGENBENCH++ USING CHINESE LONG PROMPTS. Gemini-2.5-Pro IS USED AS THE MLLM FOR EVALUATION. BEST SCORES ARE IN BOLD, SECOND-BEST IN UNDERLINED. Models Overall Style World Know. Attribute Action Relationship Compound Grammar Layout Logic. Reason. Text Chinese Long Prompt Evaluation Quant. Express. Materi. Size Shape Color Hand Full Body Animal Non Contact Contact State Compos. Sim. Inclus. Compare. Imagin. Feat Match. Pron Ref. Consist. Neg. 2D 3D Closed-source Models Recraft Wan2.2-Plus DALL-E-3 Imagen-3.0 FLUX-Kontext-Max Imagen-4.0 Nano Banana Imagen-4.0-Ultra Wan2.5 Seedream-3.0 Seedream-4.0 GPT-4o 56.90 70.05 71.16 71.85 75.24 79.90 83.17 83.86 84.24 86.14 90.35 90.51 86.38 85.55 91.61 88.73 95.85 94.36 89.25 94.75 97.59 92.31 95.60 97.98 98.41 97.38 97.34 97.40 98.00 94.30 98.42 95.36 98.42 96.39 99.41 97.96 UniWorld-V1 Janus-flow Janus Emu3 MMaDA HiDream-I1-Full BLIP3-o-Next Hunyuan-DiT BLIP3-o Janus-Pro X-Omni Lumina-DiMOO OneCAT Kolors CogView4 OmniGen2 Bagel 21.50 23.01 33.63 35.95 50.61 50.70 54.55 55.57 59.25 60.21 62.18 63.80 63.88 65.12 68.09 70.75 75.75 78.31 Echo-4o Qwen-Image 86.91 Hunyuan-Image-2.1 87.01 55.48 17.34 57.39 17.49 75.00 30.06 75.08 53.03 84.05 63.58 83.06 78.61 87.71 61.85 94.10 76.16 89.70 77.17 91.28 75.87 76.91 74.13 84.30 76.45 95.85 85.26 90.61 87.14 89.62 89.31 95.35 87.57 96.10 89.02 96.26 91.18 97.84 95.66 95.18 94.08 61.70 78.19 64.36 75.78 72.34 82.45 90.37 88.30 83.51 85.64 86.70 85. 12.23 11.70 25.53 23.40 46.81 63.30 50.00 66.49 53.19 44.15 72.34 64.36 57.98 63.83 73.40 74.47 71.81 71.81 89.36 87.77 60.56 66.94 71.11 64.67 71.41 80.42 85.06 83.75 80.90 83.98 90.69 92.56 30.28 11.39 25.97 38.33 40.00 55.97 64.58 54.03 59.03 52.92 59.72 68.06 65.56 64.86 65.69 73.33 73.47 82.22 91.11 87.08 73.72 79.92 65.03 82.39 44.23 57.81 60.87 82.15 84.09 77.10 89.99 67.95 69.06 72.46 88.93 90.72 77.62 91.30 61.22 65.94 74.28 80.66 82.84 70.00 93.10 80.00 83.89 85.29 87.48 88.83 81.64 92.80 76.28 70.22 79.35 92.24 91.29 85.84 96.28 81.09 84.69 82.25 93.11 94.29 87.99 98.10 84.42 88.09 84.06 94.13 95.27 90.91 97.80 83.97 90.94 88.41 91.77 91.41 87.24 94.59 72.12 78.16 83.82 96.39 90.53 93.36 97.90 81.41 89.06 86.13 96.08 95.45 93.71 98.43 84.94 91.56 92.03 94.43 95.23 94.23 96.59 91.12 92.50 89.49 42.86 64.29 67.41 77.37 69.20 83.48 87.05 87.50 74.55 85.71 92.41 91.52 43.39 63.79 62.64 74.40 74.43 85.63 82.90 88.79 75.29 79.19 86.21 86. Open-source Models 19.80 27.27 19.76 35.69 12.18 20.31 23.19 23.72 32.20 15.91 28.72 3.85 18.75 19.20 39.16 45.83 22.20 39.99 11.54 35.31 32.25 49.17 57.77 36.19 56.34 10.58 22.81 25.36 58.96 67.80 52.62 73.22 23.40 39.06 40.58 62.50 69.70 56.12 71.80 38.14 45.00 44.93 67.85 67.61 55.94 63.21 37.50 56.25 50.72 71.76 76.14 58.57 76.10 41.03 51.56 57.25 71.31 79.36 54.02 75.00 42.63 59.38 60.87 69.80 78.22 56.99 69.18 37.82 51.25 63.04 77.79 82.20 67.83 83.39 50.00 61.56 61.96 77.18 82.01 72.73 88.00 54.81 57.50 61.96 78.92 81.25 59.79 79.77 35.26 69.69 64.13 82.98 83.52 70.80 90.25 58.97 57.19 63.41 80.35 85.98 73.43 88.84 67.31 68.75 71.01 84.94 85.23 79.90 92.09 63.46 67.81 63.41 88.93 90.53 83.39 95.81 71.47 75.62 76.09 94.50 90.72 88.64 96.80 73.72 81.56 74.28 96.23 93.56 90.91 97.90 83.33 90.62 89.86 95.41 91.67 89.69 97.69 85.58 84.69 85.51 9.38 9.38 16.96 12.05 29.02 38.39 45.98 41.52 45.98 48.21 49.55 60.27 55.36 65.18 58.04 63.39 66.96 67.41 86.61 83.48 8.05 9.48 14.08 17.53 30.75 36.21 37.36 37.36 43.97 51.72 42.82 49.43 42.24 50.57 63.79 60.34 63.22 66.38 79.60 79.02 61.66 74.21 77.37 87.38 78.16 86.07 86.07 90.02 80.85 85.18 89.53 88.14 26.28 30.24 41.11 42.39 58.20 57.71 61.36 59.09 64.03 60.28 66.40 68.68 70.85 73.42 70.65 72.33 75.10 79.55 87.75 84. 54.72 70.15 81.63 83.90 78.95 87.24 90.59 92.22 85.59 84.57 86.35 91.93 16.20 18.62 26.02 33.29 48.09 46.30 55.36 59.69 58.29 62.50 57.02 62.24 63.65 69.90 66.07 70.79 80.87 86.99 85.59 87.88 49.68 63.22 70.83 80.17 73.72 85.63 73.33 88.64 73.40 87.25 82.05 93.97 86.50 96.83 87.82 96.84 77.56 91.95 83.01 93.10 83.01 93.39 89.10 95.64 21.47 23.56 18.91 24.43 26.60 30.46 29.17 35.06 49.04 60.63 45.83 59.20 53.53 60.34 48.08 56.90 54.81 60.63 57.05 66.38 55.45 65.52 61.22 78.74 63.14 65.52 74.68 74.43 64.10 80.17 70.51 87.64 76.60 86.78 81.09 89.08 84.29 91.67 81.41 92.24 63.59 76.94 77.43 83.90 86.65 89.08 91.71 92.23 91.02 91.99 93.45 93.93 20.15 19.90 31.80 29.37 57.52 49.03 63.35 52.43 69.17 63.83 68.20 69.17 68.69 68.45 75.97 77.43 82.04 84.47 90.53 90. 50.95 74.26 80.38 79.23 84.60 88.71 92.14 93.99 86.18 83.83 87.66 95.36 15.30 28.80 38.92 33.02 56.65 45.99 59.49 57.49 67.72 72.47 65.51 72.57 70.78 67.83 71.94 76.05 83.97 86.08 83.44 85.97 47.90 71.83 65.42 83.73 65.89 80.16 64.06 79.04 70.33 88.76 82.01 92.06 89.13 94.78 89.25 96.83 82.78 91.67 81.54 88.89 87.85 94.44 92.87 96.37 23.81 6.31 5.61 29.76 14.95 46.43 18.46 42.86 35.51 61.11 33.41 59.52 41.82 65.48 39.95 63.49 45.09 72.22 50.47 72.22 51.40 76.19 60.75 76.98 43.69 69.05 56.07 81.35 65.42 83.33 69.63 85.71 77.80 84.92 83.41 87.70 82.01 94.05 84.81 92.86 55.95 62.70 74.21 70.75 76.19 81.75 88.10 90.08 79.37 82.14 82.14 92.86 21.03 13.89 24.60 26.59 50.79 49.60 58.73 60.71 53.17 61.11 58.33 67.06 63.49 62.30 69.05 76.59 83.33 83.73 83.73 83. 46.13 64.12 65.04 64.44 81.50 78.26 59.51 70.48 76.99 59.13 82.72 79.92 72.24 87.01 88.32 75.35 90.25 90.76 82.86 93.19 93.10 80.63 94.77 93.30 70.42 89.91 86.78 63.38 90.68 89.49 75.35 92.66 90.94 93.24 95.01 95.47 39.79 24.15 24.82 50.70 18.64 25.36 59.15 38.98 42.57 44.72 30.37 41.85 63.73 65.54 54.35 52.46 62.99 57.07 58.10 67.80 60.51 56.34 60.73 62.86 57.75 72.60 65.04 71.83 66.38 66.85 60.56 76.84 68.12 71.83 84.18 70.83 57.39 76.13 75.36 50.00 72.46 77.36 61.62 77.72 84.46 69.72 84.89 76.81 75.70 87.29 79.71 79.58 90.54 84.96 55.63 92.09 88.41 65.85 93.50 88.77 36.17 57.04 61.41 48.36 68.20 77.18 82.40 86.89 74.51 68.45 80.58 90.05 8.98 17.48 20.15 19.66 31.80 24.27 41.50 33.98 47.09 49.27 46.60 49.27 54.37 47.82 51.94 62.62 68.69 72.57 69.90 71.36 2.45 15.22 3.80 21.55 4.62 4.89 10.68 6.79 66.30 82.34 91.30 57.14 1.36 0.27 1.09 0.82 0.27 2.99 1.90 1.36 1.90 2.17 29.35 1.36 2.17 5.98 8.15 1.90 14.40 13.04 86.14 86."
        }
    ],
    "affiliations": [
        "Fudan University",
        "Hunyuan, Tencent",
        "Shanghai AI Lab",
        "Shanghai Innovation Institute",
        "Shanghai Jiaotong University"
    ]
}