{
    "paper_title": "MotionBench: Benchmarking and Improving Fine-grained Video Motion Understanding for Vision Language Models",
    "authors": [
        "Wenyi Hong",
        "Yean Cheng",
        "Zhuoyi Yang",
        "Weihan Wang",
        "Lefan Wang",
        "Xiaotao Gu",
        "Shiyu Huang",
        "Yuxiao Dong",
        "Jie Tang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In recent years, vision language models (VLMs) have made significant advancements in video understanding. However, a crucial capability - fine-grained motion comprehension - remains under-explored in current benchmarks. To address this gap, we propose MotionBench, a comprehensive evaluation benchmark designed to assess the fine-grained motion comprehension of video understanding models. MotionBench evaluates models' motion-level perception through six primary categories of motion-oriented question types and includes data collected from diverse sources, ensuring a broad representation of real-world video content. Experimental results reveal that existing VLMs perform poorly in understanding fine-grained motions. To enhance VLM's ability to perceive fine-grained motion within a limited sequence length of LLM, we conduct extensive experiments reviewing VLM architectures optimized for video feature compression and propose a novel and efficient Through-Encoder (TE) Fusion method. Experiments show that higher frame rate inputs and TE Fusion yield improvements in motion understanding, yet there is still substantial room for enhancement. Our benchmark aims to guide and motivate the development of more capable video understanding models, emphasizing the importance of fine-grained motion comprehension. Project page: https://motion-bench.github.io ."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 ] . [ 1 5 5 9 2 0 . 1 0 5 2 : r MotionBench: Benchmarking and Improving Fine-grained Video Motion Understanding for Vision Language Models Wenyi Hong1* Yean Cheng2* Zhuoyi Yang1* Weihan Wang2 Lefan Wang2 Xiaotao Gu2 Shiyu Huang2 Yuxiao Dong1 Jie Tang1 1Tsinghua University 2Zhipu AI wenyi.hong@outlook.com, cya17@tsinghua.org.cn, zhuoyiyang2000@gmail.com, jietang@tsinghua.edu.cn"
        },
        {
            "title": "Abstract",
            "content": "In recent years, vision language models (VLMs) have made significant advancements in video understanding. However, crucial capability fine-grained motion comprehension remains under-explored in current benchmarks. To address this gap, we propose MotionBench, comprehensive evaluation benchmark designed to assess the fine-grained motion comprehension of video understanding models. MotionBench evaluates models motionlevel perception through six primary categories of motionoriented question types and includes data collected from diverse sources, ensuring broad representation of realworld video content. Experimental results reveal that existing VLMs perform poorly in understanding fine-grained motions. To enhance VLMs ability to perceive fine-grained motion within limited sequence length of LLM, we conduct extensive experiments reviewing VLM architectures optimized for video feature compression and propose novel and efficient Through-Encoder (TE) Fusion method. Experiments show that higher frame rate inputs and TE Fusion yield improvements in motion understanding, yet there is still substantial room for enhancement. Our benchmark aims to guide and motivate the development of more capable video understanding models, emphasizing the importance of fine-grained motion comprehension. Project page: https://motion-bench.github.io. 1. Introduction With the rapid development of pre-training, an increasing number of studies focus on leveraging large vision language models (VLMs) for video understanding [15, 19, 27, 29, 34]. For instance, CogVLM2-Video [15], LLaVA- *Equal contribution. Corresponding authors Work was done when WH, ZY interned at Zhipu AI. State-of-the-art video understanding models strugFigure 1. gle with basic motion-level perception. Compared to existing benchmarks, our proposed MotionBench focuses on assessing the models Motion level perception capability, which is critical in understanding videos with fast and instant interactions and motions. Video [51] and PLLaVA [44] continually train imageunderstanding models to achieve video-understanding models, and Qwen2-VL[37], LLaVA-OneVision [18] explore mixed training upon both images and videos. To effectively evaluate video understanding VLMs as well as guide further advancement, series of video understanding benchmarks emerged, with focuses on general video understanding capability [8, 23, 24, 39] or specific capabilities such as long video understanding [39, 41, 54]. Video understanding questions can be categorized into three levels based on the 1 granularity of understanding: motion-level (capturing finegrained motion), event-level (addresses distinct segments of activities [7]), and story-level (a holistic understanding of the storyline across the video [9]). Among them, motionlevel understanding acts as foundational ability and plays pivotal role in applications such as anomaly detection, open-domain action analysis, detailed video captioning, etc. However, while some benchmarks shifted their focus toward eventand story-level understanding, most benchmarks lack dedicated set for evaluating motion-level understanding. To quantitatively analyze the granularity distribution across benchmarks, we leverage GPT-4o1 for question analysis. The results in Figure 1 indicate that the foundational motion-level comprehension is being overlooked, with the data volume and diversity for motion-level content being limited. Some datasets from earlier years focused on low-level action recognition within specific domains, but their content and categories are highly constrained. Is this because motion-level understanding is too trivial to merit attention? To answer this question, we build MotionBench to thoroughly evaluate the motion-level capability of current video models. MotionBench comprises 8,052 questions covering six main categories of video motion, with diverse video collected from the web (Panda70M [3], Pexels2), public datasets (MedVid [13], SportsSloMo [2], Ha-ViD [53]), and self-synthetic videos generated via Unity3, capturing broad distribution of real-world application. Surprisingly, most state-of-the-art models can only achieve accuracy lower than 60%, significantly below the threshold for practical applications, which highlights two primary technical challenges: High Frame Rate vs. Computational Cost: The first challenge lies in the contradiction between the high frame rate required for fine-grained motion understanding and the high computational cost of long sequence lengths. Long sequence lengths substantially increase the computational and memory burden in both training and inference. Consequently, most current video understanding models can only handle limited number of frames, falling short of the demands for fine-grained motion analysis. For example, Intern-VL2 [5], LLaVA-Next-Video [50] and CogVLM2Video [15] can only accept 16 to 64 frames, thus can only sample frames at an extreme-low rate of 1 frame every 5 seconds (i.e., 0.2 fps) for 5-minute video which is common in daily life. To address this, we conduct the first comprehensive evaluation over existing video feature compression architectures and identify their common shortcomings-shallow fusion. Based on these findings, we propose novel VLM architectural paradigm Through-Encoder Fusion (TE Fusion), which enhances 1gpt-4o-2024-08-06 2https://www.pexels.com 3https://unity.com/cn video feature representation under fixed decoder sequence length by applying deep fusion throughout the visual encoder. Experiments on benchmarks across various video lengths and contents demonstrate that TE Fusion achieves state-of-the-art performance, and shows particular advantages under high compression ratios. Limited Fine-Grained Motion Understanding: The second challenge arises from the limited foundational capability to comprehend fine-grained motion in current video understanding models. While higher frame rate brings some performance improvements (Tab. 4), models motion-level understanding remains constrained, achieving accuracies of below 60% on MotionBench (Tab. 3). To address this, we additionally release dataset of 5,000 videos with manually annotated fine-grained motion descriptions, which are annotated and double-checked together with the benchmark annotation process (refer to Fig. 3a for example). Each video includes dynamic information descriptions with annotation density reaching 12.63 words per second, providing researchers with resources for further development and training to enhance video models motion-level comprehension capabilities. Contribution. Our main contributions include: We introduce MotionBench, the largest motion-level video benchmark, featuring wide range of video sources and question types, along with carefully designed annotation pipeline that ensures diversity and accuracy. MotionBench reveals critical deficiency in motion-level understanding among current video understanding models, which is largely overlooked by existing research. We propose TE Fusion, novel compression architecture to enhance motion-level understanding under constrained LLM context length. Experimental results demonstrate that TE Fusion achieves state-of-the-art results on MotionBench and outperforms other compression methods across MotionBench, MVBench [23], LVBench [39], and VideoMME [8] in the ablation study, and shows particular advantage in high compression ratio scenarios. 2. Related Work 2.1. Video Understanding Benchmarks To effectively evaluate video understanding models and drive their advancement, series of benchmarks are proposed. Traditional benchmarks like MSRVTT-QA [43] and ActivityNet-QA [48] primarily focus on basic action recognition and video question answering with short clips. While these benchmarks provide foundation for assessing video understanding capabilities, they lack the granularity to evaluate subtle motion comprehension. Recently, more benchmarks emerged to assess video VLMs, as shown in Tab. 1. MVBench [23] emphasizes general video understanding, introducing 20 temporal-related tasks across six 2 Table 1. The comparison of existing video VLM benchmarks with MotionBench. MotionBench collects various video sources including web videos and synthetic videos, and provides new evaluation perspective in motion level perception. Benchmarks MVBench [23] TempCompass [28] VideoMME [8] AutoEval-Video [4] EgoSchema [31] LVBench [39] LongVideoBench [41] MovieChat-1K [35] Short Film Dataset [9] MotionBench #Videos 4,000 410 900 327 5,031 103 3,763 130 1,078 5,385 #QAs 4,000 1,580 2,700 327 5031 1,549 6,678 1,950 4,885 8, Perception Level general, motion<30% general, motion<20% general, motion<20% event level event level event & story level event & story level story level story level motion level Data source existing datasets ShutterStock Youtube Youtube ego-centric video Youtube web channels movies short films web videos, movies, synthetic videos, datasets Dataset Feature general temporal concept general open-ended QA ego-centric long video long videos movie story-level motion perception domains. Video-MME [8] offers an evaluation framework featuring videos of varying durationsfrom 11 seconds to over an hourwhile incorporating multimodal elements such as subtitles and audio. Some benchmarks focus on specific, challenging capabilities. For example, LVBench [39], LongVideoBench [41], and MLVU [54] target eventor story-level understanding across long temporal horizons. Heres refined version to capture that idea: However, these benchmarks primarily focus on general video understanding, lacking dedicated dataset or subset specifically designed for motion-level assessment. This limitation results in reduced volume and diversity in evaluating motion dynamics. Furthermore, most benchmarks rely on data from single source, falling short of representing comprehensive distribution of downstream applications. To address these gaps, we propose MotionBench, benchmark dedicated to fine-grained motion understanding. By leveraging data from seven distinct sources and encompassing six motion-oriented task categories, MotionBench offers diverse range of video content and specialized focus on motion-level perception, advancing the evaluation of video understanding models in this crucial area. 2.2. VLMs for video understanding Recent advancements in Visual Language Models (VLMs) have demonstrated significant potential in video understanding, mostly extending pre-trained VLMs [25, 38] to handle video modality. Video VLMs typically comprise three core components: visual encoder for visual feature extraction, modality alignment module to integrate visual features into the language models embedding space, and an LLM backbone for decoding multi-modal context. straightforward architecture is LLaVA-Next-Video [50], CogVLM2-Video [15] and Intern-VL2 [6], where videos are treated as sequences of images, extending VLMs strong image understanding capabilities to videos. Qwen2VL [36] further introduces 3D-RoPE to enable understanding of arbitrary-length videos. However, the high computational and memory demands of handling high-frame-rate, long-duration videos have prompted initial explorations into video compression in both pixel and feature spaces. For instance, InternVideo2 [40] and Video-LLaMA [49] adopt QFormer [20] for video feature extraction, PLLaVA [44] utilizes adaptive pooling, Kangaroo [26] employs unified spatial-temporal patchification, and Qwen2-VL [36] fuses neighboring frames before visual encoder. Despite these advancements, to our knowledge, no comprehensive and fair comparison exists among these compression methods and evaluating their performance as compression ratios increase. Moreover, current approaches are generally limited to shallow fusion that is confined to the compression operator itself, which restricts their performance, especially in high compression rate scenarios 3. MotionBench: Motion-Level Benchmarking We introduce MotionBench, an evaluation benchmark designed to assess the motion-level perception capability of video VLMs. Fine-grained motion understanding is of paramount importance across variety of daily scenarios, including human interaction, expression recognition, medical instruction, ambient object motion, sports replay, virtual reality, etc. Our approach begins with the collection of video clips from these diverse cases, which are then filtered and processed into the desired formats. We devise six primary categories of question types to evaluate the candidates motion-level understanding, and we manually annotate the questions and answers within these categories, yielding the proposed MotionBench. Table 2 provides an overview of our data construction pipeline. 3.1. Data Curation In this section, we elaborate on the video curation, filtering, and annotation process. Video Collection. We obtain raw videos from publicly 3 Figure 2. We propose MotionBench, collection of manually curated multi-choice queries with video clips featuring dynamic changes from various scenes such as daily life and medical instructions. We devise six primary tasks to evaluate the capability of motion-level perception. Unlike previous story-level and event-level benchmarks, MotionBench is characterized by significantly higher annotation density, allowing for the assessment of fine-grained motions. Table 2. The MotionBench curation process. Categories [1-3] refer to videos with intricate interactions, videos from specific fields and virtual videos, detailed in Sec. 3.1. N. Vid/QA refers to the number of videos and queries in category. min(H, W) is the minimum of the height and width of the video frames. len refers to the processed video duration. We automatically construct the queries in Virtual scenes, and manually annotate the other QA pairs in MotinBench. Category # Videos/QAs 1 2 3 2,355/4, 2,430/2,530 600/600 Source Pexels Pandas-70M [3] Movie clips MedVid [14] Collection Self-collected Open-sourced Self-collected Open-sourced min(H, W) > 448 & len [3, 60]sec SportsSloMo [2] Open-sourced min(H, W) > 448 & len [3, 60]sec Open-sourced min(H, W) > 448 & len [3, 60]sec Remove renderings with occlusion Self-collected Post-process Directly adopt Segment with scene detection Segment with scene detection HA-ViD [52] Virtual scenes Annotation Caption & QA Caption & QA Caption & QA QA QA QA Automatic QA available datasets as well as from our self-collected corpus. Based on the video sources, the vividness of the scenes, and the complexity of the scenarios, we split the videos into three distinct categories. Each category is processed and annotated using tailored pipelines accordingly: Videos with intricate interactions: We acquire publiclyavailable videos from Panda-70M [3] and Pexels4 and collect high-quality movie clips featuring various actions and motions, attributing to total of 2355 videos. To ensure uniformity in clip duration, we follow the methodology in Panda-70M [3] to utilize scene detection tool5 to segment these videos into event-level clips. Videos from specific fields: We collect videos from MedVid [14], SportsSloMo [2] and Ha-ViD [52], representing specific use cases in medical, sports and industrial applications. These videos usually consist of one or two simple motions and demonstrate less complicated interactions. For this category, we filter out videos longer than 60 seconds or resolutions less than 448 448 pixels. An amount of 2430 videos are retrieved in this category. Synthetic videos: The above-mentioned videos are mostly from real-world scenes. For further evaluation in virtual reality applications, we render avatars with simple motions using the Unity rendering engine. Furthermore, graphic engines generate renderings that exclusively focus on motion changes, making them highly suitable for the assessment of motion perception. We randomly sample 20 motions from publicly available website6, and select 6 avatars and 5 scenes to render virtual avatars from pool of 15 different viewpoints. Renderings with occlusion are manually filtered. Please refer to the supplementary for details in rendering. Task Definition. To assess the capability in motion-level perception, we propose six categories of questions. Examples and the distribution of each category are illustrated in Fig. 2. detailed description of each category is listed: Motion Recognition (MR): Questions focus on what kind of motion emerged in the given video clips. 4https://www.pexels.com 5https://github.com/Breakthrough/PySceneDetect 6https://www.mixamo.com 4 (a) Option distribution (b) Video duration (c) Annotation length (d) QA per video Figure 3. Basic statistics of MotionBench. and more details, please refer to the supplementary material. We find that the generated QA pairs are not only diverse in type but also presented in various sentence structures. We show an example of the dynamic information annotation pipeline in Fig. 4. In addition, we also drop all the questions that can be answered solely based on common knowledge and single frame. We use various image VLMs to predict answers using the first frame as input and discard questions that are answered correctly by all VLMs. Then, we manually filter out any questions with incorrect phrasing or ambiguous answers and categorize them. Finally, 4922 queries and answers are retained. For videos from specific fields, we directly annotate the questions within the designed task types. total of 2530 QA pairs are selected. For virtual videos, where we already possess the ground truth annotations for each query, we automatically construct the questions and corresponding options. Finally, 600 QA pairs are generated. 3.2. Dataset Statistics MotionBench consists of 5385 videos and 8052 QAs, and each QA pair consists of question, four options, an answer, and category. The task distribution is displayed in Fig. 2. Annotation Density. MotionBench is designed especially for evaluating the video VLMs motion-level perception capability. Such evaluation requires larger annotation density per second. We define Annotation Density to represent such attribute, defined as follows: Annotation Density = Total length of questions Video duration (1) The results are demonstrated in Fig. 2. MotionBench features an Annotation Density of 68.4, which is two times more than existing benchmarks. Basic Statistics. In Fig. 3, we illustrate the distribution of options, number of QAs per video, duration, and annotation length in the MotionBench. Regarding the distribution of answer options in MotionBench, it can be observed that the various options generally adhere to random distribution. Due to our manual removal of erroneous and overly simplistic questions, it can be seen that the QA pairs in Videos Figure 4. Example of dynamic information annotation Location-related Motion (LM): Questions assessing the relative location changes before and after the motion takes place, and queries regarding specific location. Action Order (AO): Complex actions are composed of sequence of motions. Questions in this category focus on the order of these motions. Repetition Count (RC): Certain subtle motions occur rapidly but are repeated multiple times, such as nodding or jumping. This category of questions evaluates the models ability to recognize and interpret such motions. Motion-related Objects (MO): Queries designed to evaluate the models ability to identify small objects involved in motion interactions. Camera Motion (CM): Questions focus on the camera motion changes and trajectory, including the order and combinations of different motion types. Question Answer Annotation. We employ different annotation pipelines for the above-mentioned video categories. For videos with intricate interactions, it is impractical to directly annotate the whole video clip, since the total complexity and quantity of the motions are too large. Therefore, we first manually annotate these videos with captions that focus on the dynamic changes within the video. Subsequently, we prompt GPT-4o [33] to generate 6 questionanswer pairs for each video clip. For the prompt template Figure 5. Summarization of prevalent paradigms for video compression and our proposed Through-Encoder Fusion (TE Fusion). Here we only illustrate the part before the VLM decoder where temporal compression performs. with intricate interactions have been thoroughly filtered, resulting in the elimination of nearly half of the QA data. The video lengths in MotionBench are primarily concentrated around under 10 seconds, as motion events usually occur in very brief segments of the videos. Copyrights. MotionBench is research preview intended for non-commercial use only. For existing open-sourced video sources [2, 3, 13, 52], we have carefully signed their provided license and will not re-distribute their videos without permission. For videos from Pexels, we will mandatorily ask the users to sign an agreement that the videos in MotionBench can only be used in non-commercial research and cannot be re-distributed. For self-collected movie clips, we will not directly distribute the raw videos, and will alternatively provide the download links and processing scripts. 4. Model Design: Motion-Level Perception Motion-level video perception demands high-frame-rate input, while the maximum input frame rate is significantly constrained by the sequence length limitations of VLMs, which are bounded by both infrastructure and computational budgets during training and inference. Therefore, its necessary to design an efficient video understanding model structure with dense video representation. Recent studies, particularly in the domain of long video understanding, introduce various types of video feature compression methods [26, 37, 40, 44], but lack comprehensive and fair comparisons across all methods. Therefore, We comprehensively investigate commonly used architectures for video compression and categorize prevalent paradigms in Fig. 5. Without Temporal Fusion: baseline widely used in [15, 50]. Each frame is independently processed by the visual encoder and projected into the decoder space. Pre-Encoder Fusion: This architecture conducts temporal fusion among neighboring frames before the visual encoder, usually in pixel space. The temporal fusion operator varies across implementations. Typical examples include Qwen2-VL [37] where two adjacent frames are concatenated along the channel dimension for joint processing, and Kim et al. [17] which merges several nearby frames into single image. Post-Encoder Fusion: In this architecture, each frame first independently goes through the visual encoder to generate frame-specific features, then performs feature fusion among neighboring frames with spatial-temporal fusion modules. Note that no temporal relationships are captured during visual encoding. This paradigm is the most widely adopted in video architecture with compression, with multiple variations in temporal fusion operators such as adaptive pooling [44], QFormer [20] [40], and unified spatial-temporal patchification [26]. All compression architectures rely on the assumption that redundancy exists between frames which contributes little to the videos comprehension and can therefore be removed. Achieving higher compression ratio requires more precise and thorough capture of this redundant information. However, current video temporal compression methods have common limitation: the inter-frame relationships are considered only within the small compression operator, and each frame is treated independently before the operator. Consequently, it is difficult for this kind of shallow fusion to effectively capture higher-level redundancies. For instance, in video of running person, the individuals position, posture, and even the camera angle vary continuously. Only by applying sophisticated interframe fusion techniques can the model unify their representation throughout the video and capture this higher-level redundancy. Based on this observation, we propose novel Through-Encoder Fusion paradigm that introduces deeper fusion across neighboring frames: Through-Encoder Fusion (TE Fusion): During the visual encoding stage, adjacent frames are grouped in sets of and apply group-level self-attention. This design gives the capacity to compute temporal dependencies through the whole visual encoder and conduct deep fusion. Following this, spatial-temporal compression is performed on each group of frames. Note that Through-Encoder Fusion represents class of 6 Table 3. Evaluation results of the existing video VLMs. Abbreviations: MR (Motion Recognition), LM (Location-related Motion), CM (Camera Motion), MO (Motion-related Objects), AO (Action Order), RC (Repetition Count). We randomly split MotionBench into dev and test. We will release the ground truth answers in the dev set and set up an online platform for results submission in the test set. Model Random GPT-4o [33] LLM # Frames - - - - Dev AVG (4020) 0.25 Test AVG (4034) 0.25 LLM: Text as Input MR 0. LM 0.25 CM 0.25 MO 0. AO 0.25 RC 0.25 0.33 0. 0.31 0.34 0.36 0.37 0.42 0. Video VLMs : Text + Multiple Frames as Input Gemini 1.5 Pro [34] Qwen2VL-2B [36] Qwen2VL-7B [36] Qwen2VL-72B [36] InternVL-40B [6] PLLaVA-34B [44] CogVLM2-Video [15] GLM-4V-plus [15] LLaVA-NeXT [50] MiniCPM-V2.6 [46] Oryx-34B [29] TE Fusion (ours) - Qwen2 [37] Qwen2 [37] Qwen2 [37] NH-2-Yi-34B [32] Yi-34B [32] LLaMA3-8B [1] GLM4 [10] Yi-34B [32] Qwen2 [37] Yi-34B [32] GLM4-9B [10] 1fps 1fps 1fps 1fps 8 16 24 30 32 64 64 16 0.51 0.48 0.52 0.57 0.55 0.52 0.41 0.54 0.48 0.52 0.49 0.58 0.50 0.47 0.52 0.58 0.54 0.51 0.44 0.55 0.40 0.53 0.49 0. 0.51 0.49 0.52 0.58 0.54 0.55 0.43 0.57 0.53 0.56 0.48 0.64 0.52 0.49 0.55 0.61 0.58 0.51 0.39 0.57 0.45 0.49 0.52 0.59 0.54 0.42 0.49 0.63 0.49 0.47 0.38 0.54 0.36 0.45 0.44 0.51 0.67 0.62 0.68 0.72 0.76 0.66 0.64 0.69 0.66 0.72 0.65 0.69 0.40 0.32 0.39 0.47 0.41 0.38 0.37 0.40 0.39 0.39 0.42 0.41 0.22 0.28 0.32 0.31 0.30 0.31 0.33 0.37 0.23 0.33 0.32 0. temporal compression methods that perform deep frame In this fusion before applying the compression operator. work, we experiment with the straightforward approach, leaving other variations for future exploration. 5. Experiments 5.1. Evaluation on MotionBench We comprehensively evaluate the performance of existing video VLMs capability in motion-level perception on MotionBench. We include multiple models with various model sizes and VLMs. The results are listed in Table 3. TE Fusion represents our proposed model, which uses TE Fusion on GLM-4V-9B backbone, with 16 input frames and compress ratio of 4. Among existing VLMs, Qwen2VL-72B achieves the best overall performance on the dev and test set and scores highest in 3 out of 6 categories. Surprisingly, TE Fusion achieves state-of-the-art results with 9B LLM backbone, verifying the effectiveness of our method. Analysis. With text input alone, GPT-4 achieves an accuracy rate of 0.3 to 0.4, surpassing the random baseline of 0.25. This result indicates that LLMs possess prior probability for certain actions, even when based only on text (note that questions answerable purely by common knowledge are filtered out during data curation). Building on LLMs, video VLMs improve accuracy by just 0.05 to 0.2, highlighting that current video VLMs still face challenges in reliably recognizing even short, simple motions. For the Repetition Count category, all models, except GLM-4V-9B with TE Fusion and GLM-4V-plus, scored near random. This is likely because fast motions are challenging to count at low frame rates or are easily overlooked by the models. Conversely, models generally achieved high scores in the Motion-related Objects category. This could be attributed to the pretraining video data, which is often constructed from image descriptions and emphasizes the objects in the video. We further analyze the questions that all models fail to answer. The largest proportion involves fine-grained motion, suggesting that certain actions and their associated captions may be underrepresented in the training data. When examining questions by video duration, we find that even for short videos (0-4 sec), the proportion of all-modelfailed questions remains 11% to 14%, highlighting models difficulty in distinguishing certain motions even with limited content. As video duration increases, the failure rate rises significantly, reaching 18% for videos longer than 18 seconds. Further analysis from more perspectives and case studies are provided in the appendix. 5.2. Experiments on Video Feature Compression To comprehensively and fairly evaluate all paradigms of video compression architecture, we implement representative methods from each paradigm based on the same image foundation model, GLM-4V-9B [15]: (1) Preencoder fusion: Qwen2-VL [37]; (2) Post-encoder fusion: QFormer [20], PLLaVA [44], Kangaroo [26]; (3) Throughencoder fusion: our proposed implementation; (4) Baseline without temporal fusion. All models take 224 224-pixel input and are trained for 10,000 iterations with global batch size of 768 on the same collection of open-source datasets. Note that the training data is subset of the data used in Sec. 5.1. The details of training and architecture are further provided in the Appendix. Besides MotionBench (dev), our motion-level video benchmark, we further evaluate all models on MVBench [23], LVBench [39], and Video-MME [8] as the representation of video benchmarks 7 Figure 6. Model performance variation with respect to different compression ratios = 2, 4, 8, 16, given fixed VLM input frame count of Ninput = 16. The pink dotted line represents the performance of the baseline model, which processes 16 frames without temporal compression. Note that each compression method is re-implemented on the GLM-4V-9B backbone to ensure fair comparison. Table 4. Benchmark results for different compression methods at various compression rates, all using the same sequence length in = 4, with the baseline representthe VLM decoder. We set ing video models that process 4 frames without compression. Note that each compression method is re-implemented on the GLM-4V9B backbone to ensure fair comparison. Ninput 1 2 4 Method MotionBench MVBench baseline QFormer Qwen2-VL PLLaVA Kangaroo TE Fusion (ours) QFormer Qwen2-VL PLLaVA Kangaroo TE Fusion (ours) 47.6 43.5 48.0 48.5 48.4 49.1 44.3 47.6 50.5 50.0 51.0 64.5 62.1 66.5 68.8 69.2 69.0 63.8 65.6 70.2 69.8 72.1 VideoMME short medium long 38.3 41.0 51.4 36.3 39.6 42.8 37.8 43.1 54.1 39.6 44.9 54.9 38.8 43.0 55.4 40.0 46.3 55.2 36.8 41.0 45.2 39.4 43.4 51.8 41.3 46.4 58.9 39.5 45.6 55.3 42.1 47.3 61.0 of varying duration and content. Let Ninput represent the number of frames fed into the visual encoder, and let each frames uncompressed length at the VLM decoder be tokens. With given compression ratio k, the total compressed input length for the VLM decoder is Ldecoder = Ninputl . Our experiment centers on addressing two primary questions: 1. For fixed sequence length at the VLM decoder (Ldecoder), how does performance vary as the compression ratio increases? 2. For fixed number of input frames (Ninput), how does performance respond to changes in the compression ratio, and is there an optimal compression ratio? For the first question, we conduct experiments with Ninput = 4 and 8, varying the compression rate at 2, 4, 6, and 8. Results for Ninput = 4 are shown in Tab. 4, with complete results included in the Appendix due to space constraints. Given the same Ldecoder, most temporal compression methods demonstrate performance improvements across all benchmarks, with higher compression rates generally yielding better scores. Notably, PLLaVA, Kangaroo, 8 and TE Fusion show relatively strong results, with our TE Fusion achieving the highest scores in 9 out of 10 metrics, improving upon the baseline by 11.8% on MVBench and 18.7% on VideoMME-short with = 4. Qwen2-VL performs well with = 2 but shows minimal improvement (or even decline) with = 4, likely due to the limited high-level compression capabilities of post-encoder fusion. QFormer, on the other hand, occasionally underperforms compared to the baseline, potentially due to the complexity of the additional module, which is challenging to optimize during the video compression training stage. For the second question, we set the input frame count to Ninput = 16 and test compression rates of = 2, 4, 6, and 8 across all methods. The results, shown in Fig. 6 (with full numerical data in the appendix), reveal that while all methods experience some performance decline as the compression rate increases, our TE Fusion method exhibits almost no performance drop for 4. Even with larger = 16, the average performance reduction remains under 4% compared to the high-consumption baseline without compression. Additionally, the performance decline caused by temporal compression is less significant in shorter-duration videos (MotionBench, MVBench) compared to longer-duration videos (LVBench), suggesting that high-frame-rate input offers greater potential for effective, high-ratio temporal compression. Interestingly, We find that TE fusion achieves the highest score with compression-4 instead of compression-2 in 3 of 4 datasets. An explanation is that higher compression rate increases attention length within the ViT component while decreasing it in the LLM component. This finding suggests that the computational allocation in previous video VLMs may be suboptimal and enlightens new direction to improve model performance. 6. Conclusion We present MotionBench, new benchmark for assessing fine-grained motion understanding in video models. Our experiments show that current state-of-the-art models struggle with motion-level comprehension, emphasizing the need for specialized benchmarks. To tackle this, we propose the Through-Encoder (TE) Fusion method, which improves video feature representation by deeply integrating fusion within the visual encoder. TE Fusion achieves state-of-theart results, especially under high compression, paving the way for advances in motion perception. Acknowledgments We thank Xiaohan Zhang, Yuean Bi, Xiaoying Ling, Jiapeng Wang, Zikang Wang from Zhipu AI for managing the data annotation team, and Zhao Xue from Zhipu AI for data management."
        },
        {
            "title": "References",
            "content": "[1] AI@Meta. Llama 3 model card. 2024. 7 [2] Jiaben Chen and Huaizu Jiang. Sportsslomo: new benchmark and baselines for human-centric video frame interpolation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 64756486, 2024. 2, 4, 6 [3] Tsai-Shien Chen, Aliaksandr Siarohin, Willi Menapace, Ekaterina Deyneka, Hsiang-wei Chao, Byung Eun Jeon, Yuwei Fang, Hsin-Ying Lee, Jian Ren, Ming-Hsuan Yang, et al. Panda-70m: Captioning 70m videos with multiple In CVPR, pages 1332013331, cross-modality teachers. 2024. 2, 4, 6 [4] Xiuyuan Chen, Yuan Lin, Yuchen Zhang, and Weiran Huang. Autoeval-video: An automatic benchmark for assessing large vision language models in open-ended video question answering. arXiv preprint arXiv:2311.14906, 2023. 3 [5] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, Bin Li, Ping Luo, Tong Lu, Yu Qiao, and Jifeng Dai. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. arXiv preprint arXiv:2312.14238, 2023. 2 [6] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. Internvl2: Better than the bestexpanding performance boundaries of open-source multimodal models with the progressive scaling strategy, 2024. 3, 7 [7] Yifan Du, Kun Zhou, Yuqi Huo, Yifan Li, Wayne Xin Zhao, Haoyu Lu, Zijia Zhao, Bingning Wang, Weipeng Chen, and Ji-Rong Wen. Towards event-oriented long video understanding. arXiv preprint arXiv:2406.14129, 2024. 2 [8] Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. arXiv preprint arXiv:2405.21075, 2024. 1, 2, 3, [9] Ridouane Ghermi, Xi Wang, Vicky Kalogeiton, and Ivan Laptev. Short film dataset (sfd): benchmark for storylevel video understanding. arXiv preprint arXiv:2406.10221, 2024. 2, 3 [10] Team GLM, Aohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, Da Yin, Diego Rojas, Guanyu Feng, Hanlin Zhao, Hanyu Lai, Hao Yu, Hongning Wang, Jiadai Sun, Jiajie Zhang, Jiale Cheng, Jiayi Gui, Jie Tang, Jing Zhang, Juanzi Li, Lei Zhao, Lindong Wu, Lucen Zhong, Mingdao Liu, Minlie Huang, Peng Zhang, Qinkai Zheng, Rui Lu, Shuaiqi Duan, Shudan Zhang, Shulin Cao, Shuxun Yang, Weng Lam Tam, Wenyi Zhao, Xiao Liu, Xiao Xia, Xiaohan Zhang, Xiaotao Gu, Xin Lv, Xinghan Liu, Xinyi Liu, Xinyue Yang, Xixuan Song, Xunkai Zhang, Yifan An, Yifan Xu, Yilin Niu, Yuantao Yang, Yueyan Li, Yushi Bai, Yuxiao Dong, Zehan Qi, Zhaoyu Wang, Zhen Yang, Zhengxiao Du, Zhenyu Hou, and Zihan Wang. Chatglm: family of large language models from glm-130b to glm-4 all tools, 2024. 7 [11] Raghav Goyal, Samira Ebrahimi Kahou, Vincent Michalski, Joanna Materzynska, Susanne Westphal, Heuna Kim, Valentin Haenel, Ingo Frund, Peter Yianilos, Moritz MuellerFreitag, Florian Hoppe, Christian Thurau, Ingo Bax, and The something something video Roland Memisevic. database for learning and evaluating visual common sense. In ICCV, 2017. 12 [12] Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, Miguel Martin, Tushar Nagarajan, Ilija Radosavovic, Santhosh K. Ramakrishnan, Fiona Ryan, Jayant Sharma, Michael Wray, Mengmeng Xu, Eric Z. Xu, Chen Zhao, Siddhant Bansal, Dhruv Batra, Vincent Cartillier, Sean Crane, Tien Do, Morrie Doulaty, Akshay Erapalli, Christoph Feichtenhofer, Adriano Fragomeni, Qichen Fu, Christian Fuegen, Abrham Gebreselasie, Cristina Gonzalez, James M. Hillis, Xuhua Huang, Yifei Huang, Wenqi Jia, Weslie Khoo, Jachym Kolar, Satwik Kottur, Anurag Kumar, Federico Landini, Chao Li, Yanghao Li, Zhenqiang Li, Karttikeya Mangalam, Raghava Modhugu, Jonathan Munro, Tullie Murrell, Takumi Nishiyasu, Will Price, Paola Ruiz Puentes, Merey Ramazanova, Leda Sari, Kiran K. Somasundaram, Audrey Southerland, Yusuke Sugano, Ruijie Tao, Minh Vo, Yuchen Wang, Xindi Wu, Takuma Yagi, Yunyi Zhu, Pablo Arbelaez, David J. Crandall, Dima Damen, Giovanni Maria Farinella, Bernard Ghanem, Vamsi Krishna Ithapu, C. V. Jawahar, Hanbyul Joo, Kris Kitani, Haizhou Li, Richard A. Newcombe, Aude Oliva, Hyun Soo Park, James M. Rehg, Yoichi Sato, Jianbo Shi, Mike Zheng Shou, Antonio Torralba, Lorenzo Torresani, Mingfei Yan, and Jitendra Malik. Ego4d: Around the world in 3,000 hours of egocentric video. In CVPR, 2022. 12 [13] Deepak Gupta, Kush Attal, and Dina Demner-Fushman. dataset for medical instructional video classification and question answering. Scientific Data, 10(1):158, 2023. 2, 6 [14] Deepak Kumar Gupta, Kush Attal, and Dina DemnerFushman. dataset for medical instructional video classification and question answering. Scientific Data, 10, 2022. 4 [15] Wenyi Hong, Weihan Wang, Ming Ding, Wenmeng Yu, Qingsong Lv, Yan Wang, Yean Cheng, Shiyu Huang, Junhui Ji, Zhao Xue, et al. Cogvlm2: Visual language modarXiv preprint els for image and video understanding. arXiv:2408.16500, 2024. 1, 2, 3, 6, 9 [16] Y. Jang, Yale Song, Youngjae Yu, Youngjin Kim, and Gunhee Kim. Tgif-qa: Toward spatio-temporal reasoning in visual question answering. In CVPR, 2017. 12 [17] Wonkyun Kim, Changin Choi, Wonseok Lee, and Wonjong Rhee. An image grid can be worth video: Zeroshot video question answering using vlm. arXiv preprint arXiv:2403.18406, 2024. 6 [18] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. 1 [19] Dongxu Li, Yudong Liu, Haoning Wu, Yue Wang, Zhiqi Shen, Bowen Qu, Xinyao Niu, Guoyin Wang, Bei Chen, and Junnan Li. Aria: An open multimodal native mixture-ofexperts model. arXiv preprint arXiv:2410.05993, 2024. 1 [20] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with In Infrozen image encoders and large language models. ternational conference on machine learning, pages 19730 19742. PMLR, 2023. 3, 6, [21] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Limin Wang, and Y. Qiao. Uniformerv2: Spatiotemporal learning by arming image vits with video uniformer. ArXiv, abs/2211.09552, 2022. 12 [22] Kunchang Li, Yinan He, Yi Wang, Yizhuo Li, Wen Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao. Videochat: Chat-centric video understanding. ArXiv, abs/2305.06355, 2023. 12 [23] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al. Mvbench: comprehensive multi-modal video understanding benchmark. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22195 22206, 2024. 1, 2, 3, 7 [24] Yunxin Li, Xinyu Chen, Baotian Hu, Longyue Wang, Haoyuan Shi, and Min Zhang. Videovista: versatile benchmark for video understanding and reasoning. arXiv preprint arXiv:2406.11303, 2024. 1 [25] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024. 3 [26] Jiajun Liu, Yibing Wang, Hanghang Ma, Xiaoping Wu, Xiaoqi Ma, Xiaoming Wei, Jianbin Jiao, Enhua Wu, and Jie Hu. Kangaroo: powerful video-language model supporting long-context video input. arXiv preprint arXiv:2408.15542, 2024. 3, 6, [27] Ruyang Liu, Chen Li, Haoran Tang, Yixiao Ge, Ying Shan, and Ge Li. St-llm: Large language models are effective temporal learners. In European Conference on Computer Vision, pages 118. Springer, 2025. 1 [28] Yuanxin Liu, Shicheng Li, Yi Liu, Yuxiang Wang, Shuhuai Ren, Lei Li, Sishuo Chen, Xu Sun, and Lu Hou. Tempcompass: Do video llms really understand videos? arXiv preprint arXiv:2403.00476, 2024. 3 [29] Zuyan Liu, Yuhao Dong, Ziwei Liu, Winston Hu, Jiwen Lu, and Yongming Rao. Oryx mllm: On-demand spatial-temporal understanding at arbitrary resolution. arXiv preprint arXiv:2409.12961, 2024. 1, 7 [30] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. Video-chatgpt: Towards detailed video understanding via large vision and language models. 2024. 12 [31] Karttikeya Mangalam, Raiymbek Akshulakov, and Jitendra Malik. Egoschema: diagnostic benchmark for very longform video language understanding. In NeurIPS, 2023. 3 [32] NousResearch. Yi-vl-34b, 2024. 7 [33] OpenAI. Gpt-4 technical report. arXiv:2303.08774, 2023. 5, 7 [34] Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. 1, 7 [35] Enxin Song, Wenhao Chai, Guanhong Wang, Yucheng Zhang, Haoyang Zhou, Feiyang Wu, Haozhe Chi, Xun Guo, Tian Ye, Yanting Zhang, et al. Moviechat: From dense token to sparse memory for long video understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1822118232, 2024. 3 [36] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 3, 7 [37] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 1, 6, 7 [38] Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan Song, et al. Cogvlm: Visual expert for pretrained language models. arXiv preprint arXiv:2311.03079, 2023. [39] Weihan Wang, Zehai He, Wenyi Hong, Yean Cheng, Xiaohan Zhang, Ji Qi, Shiyu Huang, Bin Xu, Yuxiao Dong, Ming Ding, et al. Lvbench: An extreme long video understanding benchmark. arXiv preprint arXiv:2406.08035, 2024. 1, 2, 3, 7 [40] Yi Wang, Kunchang Li, Xinhao Li, Jiashuo Yu, Yinan He, Guo Chen, Baoqi Pei, Rongkun Zheng, Jilan Xu, Zun Wang, et al. Internvideo2: Scaling video foundation modarXiv preprint els for multimodal video understanding. arXiv:2403.15377, 2024. 3, 6 [41] Haoning Wu, Dongxu Li, Bei Chen, and Junnan Li. interLongvideobench: benchmark for long-context arXiv preprint leaved video-language understanding. arXiv:2407.15754, 2024. 1, 3 [42] Junbin Xiao, Xindi Shang, Angela Yao, and Tat seng Chua. Next-qa: Next phase of question-answering to explaining temporal actions. In CVPR, 2021. 12 [43] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: large video description dataset for bridging video and language. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 52885296, 2016. 2 [44] Lin Xu, Yilin Zhao, Daquan Zhou, Zhijie Lin, See Kiong Ng, and Jiashi Feng. Pllava: Parameter-free llava extension from images to videos for video dense captioning. arXiv preprint arXiv:2404.16994, 2024. 1, 3, 6, 7 [45] Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, and Cordelia Schmid. Just ask: Learning to answer questions from millions of narrated videos. In ICCV, 2021. 12 [46] Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He, et al. Minicpm-v: gpt-4v level mllm on your phone. arXiv preprint arXiv:2408.01800, 2024. 7 [47] Kexin Yi, Chuang Gan, Yunzhu Li, Pushmeet Kohli, Jiajun Wu, Antonio Torralba, and Joshua B. Tenenbaum. Clevrer: Collision events for video representation and reasoning. In ICLR, 2020. 12 [48] Zhou Yu, Dejing Xu, Jun Yu, Ting Yu, Zhou Zhao, Yueting Zhuang, and Dacheng Tao. Activitynet-qa: dataset for understanding complex web videos via question answering. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 91279134, 2019. 2 [49] Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audio-visual language model for video understanding. arXiv preprint arXiv:2306.02858, 2023. 3 [50] Yuanhan Zhang, Bo Li, haotian Liu, Yong jae Lee, Liangke Gui, Di Fu, Jiashi Feng, Ziwei Liu, and Chunyuan Li. Llavanext: strong zero-shot video understanding model, 2024. 2, 3, 6, [51] Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, and Chunyuan Li. Video instruction tuning with synthetic data. arXiv preprint arXiv:2410.02713, 2024. 1 [52] Hao Zheng, Regina Lee, and Yuqian Lu. Ha-vid: human assembly video dataset for comprehensive assembly knowledge understanding, 2023. 4, 6 [53] Hao Zheng, Regina Lee, and Yuqian Lu. Ha-vid: human assembly video dataset for comprehensive assembly knowledge understanding. Advances in Neural Information Processing Systems, 36, 2024. 2 [54] Junjie Zhou, Yan Shu, Bo Zhao, Boya Wu, Shitao Xiao, Xi Yang, Yongping Xiong, Bo Zhang, Tiejun Huang, and Zheng Liu. Mlvu: comprehensive benchmark for multi-task long video understanding. arXiv preprint arXiv:2406.04264, 2024. 1, 3 11 MotionBench: Benchmarking and Improving Fine-grained Video Motion Understanding for Vision Language Models"
        },
        {
            "title": "Supplementary Material",
            "content": "7. Training Details Here we provide the detailed training hyperparameters for both TE Fusion in Tab. 3 and all ablated models in Tab. 4 and Fig. 6. Configurations Total steps Warmup steps Global batch size Learning rate Minimal learning rate Learning rate decay Optimizer Adam ϵ Adam β1 Adam β2 Precision 10,000 1,000 768 8e-6 1e-6 cosine Adam 1e-8 0.9 0.95 bf16 Training settings TE Fusion in Tab. 3 and all ablated models in Tab. 4 and Fig. 6. Table 5 The training is conducted on several datasets, mainly including VideoChat [22], VideoChatGPT [30], NExTQA [42], CLEVRER [47], Kinetics-710 [21], SthSthV2 [11], Ego4D [12], TGIF-QA [16], WebVidQA [45], In-house VideoQA Dataset. We also include an in-house video QA dataset for better temporal understanding. 8. Model Details To maintain fair comparison, all model architectures are ablated with the same backbone, GLM-4V, with its model configuration as follows: Assume the temporal compression ratio be K, The specific feature of each ablated architecture is: 1. TE-Fusion (ours): Before the visual encoder, we concatenate every neighboring frames into one sequence, and conduct self-attention across each frames to fuse temporal feature. After the visual encoder, the tokens of frames are concatenated along the hidden-size dimension, downsampled and projected to the output dimension. 2. Qwen2-VL: The neighboring frames are concatenated along the channel dimension and patchified into one feaVLM decoder Layers Hidden size Attention heads num query groups FFN hidden size Sequence len Position embedding Normalization 40 4096 32 2 13696 4096 RoPE RMSNorm visual encoder Input resolution Patch size Post spatial downsample Layers Hidden size Attention heads 224 14 2 2 63 1792 16 Table 6. The model configurations of all ablated architectures. ture. Afterward, they go through the visual encoder as whole. Since the fusion is conducted in the pixel space before any feature extraction or fusion, the optimized temporal compression ratio is usually low, with vast information loss if large K. 3. Kangaroo: This approach is the most similar one to TE Fusion, except that every frame is computed independently within the visual encoder and concatenated along the hidden size dimension to perform temporal downsample (with an MLP layer). 4. QFormer: After going through the visual encoder, the video feature is passed through QFormer (learned from scratch). Every frames feature is combined into sequence to fusion temporal information within the QFormer. From the experiment, we found that, though being light-weighted, the QFormer is hard to optimize and model temporal relationships during the video instruction-tuning stage, resulting in poor performance. 5. PLLaVA: This approach is similar to Kangaroo. Instead of fusion with the MLP layer, PLLaVA adopts simple adaptive pooling. To avoid possible information loss, we conduct the pooling operation after the spatial downsample module. The pseudo-code below further illustrates all ablated architectures. 12 def forward(): The pseudo-code of the forward function for all ablated settings = temporal_compress_ratio if temporal_compress_method == \"qwen2-vl\": = merge_temporal_channels(x) = patchify(x) else: = patchify(x) = + spatial_pos_embedding(x) # [bsz, frame_num, frame_token, hiddensize] if temporal_compress_method == \"TE_fusion\": = merge_neighbor_frames(x) # [bsz, frame_num//K, K*frame_token, hiddensize] = + temporal_pos_embedding(x) # Use absolute positional embedding = flatten(x) = transformer(x) = x.permute_and_reshape_to(bsz, frame_num//K, K*frame_token, hiddensize) if temporal_compress_method == \"kangaroo\": = temporal_downsample_with_MLP(x) # [bsz, frame_num//K, frame_token, hiddensize] = spatial_downsample_with_proj(x) # [bsz, frame_num//K, frame_token2, output_size] if temporal_compress_method == \"TE_Fusion\": = concat_neighbor_hidden(x) # [bsz, frame_num//K, frame_token, K*hiddensize] = downsample_with_proj(x) # [bsz, frame_num//K, frame_token2, output_size] else: = spatial_downsample_with_proj(x) # [bsz, frame_num, frame_token2, output_size] if temporal_compress_method == \"pllava\": = temporal_downsample_with_pooling(x) # [bsz, frame_num//K, frame_token2, output_size] if temporal_compress_method == \"qformer\": = qformer(x) # [bsz, frame_num//K, frame_token2, output_size] # => to VLM Decoder 9. QA Construction Process for Videos with In9.2. Step2: Automatic QA generation tricate Interactions Here we illustrate the QA generation process corresponding to Fig. 4. 9.1. Step1: Video caption annotation For videos with intricate interactions, it is impractical to directly annotate the whole video clip, since the total complexity and quantity of the motions are too large. Therefore, we first manually annotate these videos with captions that focus on the dynamic changes within the video (illustrated in Fig. 4). We hired 15 adult annotators with at least bachelors degree and conducted annotations over 20 working days. Each annotators daily salary was approximately 250 RMB. All annotations underwent secondary review. Then we use GPT-4o to generate 6 questions corresponding to each video description. The instruction to GPT-4o emphasizes diversity as well as accuracy, as shown below: You are professional question designer specializing in dynamic video details. Instead of video, you will receive detailed description of the first frame and all dynamic details throughout the video. Based on this description, design single-choice questions that focus on the dynamic information as if youre viewing the video directly, using the two-dimensional categorization system below (Content Dimension, Question Logic Dimension). Question Design Guidelines 1. Each question should have 4 options. 13 2. For each question, combine one dimension from the Content Dimension and one from the Question Logic Dimension. It may draw from multiple highly related content dimensions. 3. Focus only on representative and prominent events or actions to keep options clear and unique without being overly detailed or tricky. Select the most fitting dimension combination for each video and avoid repeated combinations where possible. 4. Given possible ambiguities in some descriptions, ensure the answer is unique and clear to avoid deductions. Ambiguity Example 1: Temporal ambiguity. If description reads, On the left, woman in khaki suit faces right, nodding her head while speaking. In the middle, group faces the camera, and man in white shirt pulls chair leftward to sit, the description is ambiguous and does not clarify the sequence of the womans actions and the mans actions, making sequence ambiguous. Ambiguity Example 2: Content ambiguity. If the description states, The worker holds long, thin tool, avoid options like screwdriver, as the tool could be any slender object. 5. Choose only prominent events or actions, avoiding minor or indeterminate details. Ensure each answer is unique and clear. Minor Example: If slightly bent elbow isnt mentioned, it does not necessarily mean it did not happen; if the video says the mouth moved slightly few times, it cannot be determined the interval and number of these movements, nor can it be determined whether the nose moved. Therefore, try to avoid using such minor actions for question creation or option design. Avoid subjective options, like Which detail reflects focus on work? unless behavior clearly reflects it. Similarly, avoid terms like skilled movement or rhythmic. Avoid overly similar distractors, e.g., chin moving up and down vs. slight opening and closing. 6. Pretend youre viewing the video, avoiding terms like based on the description or expressions related to the description text, including questions, options, and explanations. 7. Aim for at least 4 questions to focus beyond appearance. 8. Keep questions to around six, focusing only on representative events or actions and ensuring options are clear, unique, and straightforward. 9. Questions should focus on dynamic actions only. The first frame description is supplementary and should not guide question design. 10. The video dynamic information description does not contain causal or other logical relationships, therefore, do not involve logical relationships in the title. Categorization System Content Dimension Below is the Content Dimension in the video classification system: 1. Human Dynamics: 1.1. Detailed actions of individuals 1.2. Interaction among multiple people 1.3. Emotional states and their changes 1.4. Position and its changes (Location, Angle, etc.) 2. Object Dynamics: 2.1. Movement trajectory 2.2. State changes 3. Animal Dynamics: 3.1. Detailed actions 3.2. Position and its changes (Location, Angle, etc.) 4. Camera Movement: 4.1. Camera movement 5. Appearance Characteristics: 5.1. individuals 5.2. objects 5.3. environment Question Logic Dimension Below is the Question Logic Dimension in the video classification system: 1. Whether movement occurs 2. Movement count 3. Sequence between multiple movements 4. Appearance description and judgment Response Format Return only Python list, where each element is dictionary representing question. Ensure it can be parsed by json.loads() without returning anything outside the list. 9.3. VLM Filtering To avoid over simple QAs that do not utilize motion comprehension capability, we use various image VLMs to predict answers using the first frame as input and discard questions that are answered correctly by all VLMs. The VLMs include GPT-4o, Qwen2-VL, and GLM-4V-plus. 9.4. Manual Check To ensure the correctness of all benchmark QAs, we further hire annotators to check all QAs generated by GPT-4o manually. total of 10 annotators are hired to conduct manual checks for 5 days. The key points of inspection include: the reasonableness of the question, the correctness of the category, the relevance of the question to the video, the accuracy of the options, and the uniqueness of the correct answer. Each annotators daily salary was approximately 250 RMB. All annotations underwent secondary review. 14 Table 7. Benchmark results for different compression methods at various compression rates, all using the same sequence length in the = 4, 8, with the baseline representing video models that process 4 frames without compression. Note that VLM decoder. We set each compression method is re-implemented on the GLM-4V-9B backbone to ensure fair comparison. Ninput Equivalent Ninput Frames Compress Rate 1 4 8 2 1 2 4 Method baseline QFormer Qwen2-VL PLLaVA Kangaroo TE Fusion (ours) QFormer Qwen2-VL PLLaVA Kangaroo TE Fusion (ours) baseline QFormer Qwen2-VL PLLaVA Kangaroo TE Fusion (ours) QFormer Qwen2-VL PLLaVA Kangaroo TE Fusion (ours) MotionBench (dev) 47.6 43.5 48.0 48.5 48.4 49.1 44.3 47.6 50.5 50.0 51.0 48.9 44.2 48.2 49.4 49.5 50.4 44.4 48.7 49.4 49.9 50. MVBench LVBench 64.5 62.1 66.5 68.8 69.2 69.0 63.8 65.6 70.2 69.8 72.1 70.5 66.1 69.8 72.1 71.3 71.1 66.0 69.3 71.5 71.6 71.6 30.9 31.0 31.5 33.4 31.6 32.3 29.4 32.0 34.3 31.9 34.5 32.9 32.7 33.6 34.8 32.9 35.3 31.6 33.1 36.2 33.5 36.0 VideoMME short medium long 38.3 41.0 51.4 36.3 39.6 42.8 37.8 43.1 54.1 39.6 44.9 54.9 38.8 43.0 55.4 40.0 46.3 55.2 36.8 41.0 45.2 39.4 43.4 51.8 41.3 46.4 58.9 39.5 45.6 55.3 42.1 47.3 61.0 39.7 44.2 56.4 37.2 39.8 48.0 39.4 44.1 57.3 61.0 39.8 46.4 37.7 45.2 58.3 40.2 46.9 58.7 37.2 40.0 45.7 38.1 43.3 55.2 41.1 47.3 60.3 38.2 45.8 59.0 41.5 47.9 63.0 Table 8. Model performance variation with respect to different compression ratios = 2, 4, 8, 16, given fixed VLM input frame count of Ninput = 16. Note that each compression method is re-implemented on the GLM-4V-9B backbone to ensure fair comparison. Method Compress Rate MotionBench MVBench w/o compression PLLaVA QFormer Qwen2-VL Kangaroo TE Fusion (ours) 1 2 4 8 16 2 4 8 16 2 4 8 16 2 4 8 16 2 4 8 16 71.5 72.1 70.2 69.4 66.5 66.1 63.8 61.4 56.2 69.8 65.6 62.2 57.4 71.3 69.8 68.3 66.8 71.1 72.1 70.2 69.6 50.5 49.4 50.5 49.3 47.3 44.2 44.3 44.0 41.2 48.2 47.6 46.8 43.5 49.5 50.0 49.1 48.5 50.4 51.0 50.9 49.6 Video-MME short medium long 41.1 46.6 60.7 42.0 46.4 61.0 41.3 47.6 58.9 40.4 45.2 56.7 39.0 42.8 52.4 37.2 39.8 48.0 36.8 41.0 45.2 36.3 40.6 45.3 35.4 39.4 44.2 39.4 44.1 57.3 39.4 43.4 51.8 36.4 39.9 47.2 35.3 37.6 38.9 37.7 45.2 58.3 39.5 45.6 55.3 38.7 42.3 51.9 37.1 42.4 49.8 40.2 46.9 58.7 42.1 47.3 61.0 41.1 45.8 56.6 39.8 45.8 54.8 LVBench 35.1 34.8 31.9 32.9 32.7 32.7 29.4 29.4 28.5 33.6 32.0 27.8 26.5 32.9 31.9 31.9 32.0 35.3 34.5 32.7 33.1 ing the car from the top and move to the lower left. However, most of the models believe that the video presents hand tapping on the car surface. Such prediction is correct from single image perspective, while in the video, the hand stays on the car surface and moves from the top to the lower left. Hence the gesture tapping is not correct. This example demonstrates that single-frame predictions and perceptions can sometimes be misleading or even incorrect at the temporal level, which further underscores the value of creating benchmark focused on motion-level temporal sequences. uid=y26CvHFcz7BboSXN 0 What action does the hand in the video perform? A. Taps on the car surface (Gemini-1.5 pro, InternVL-40B, Oryx-34B, Qwen2-VL-72B) B. Remains stationary (PLLaVA-34B) C. Moves towards the lower left D. Waves back and forth 12. Limitations and Broader impact We propose MotionBench, video understanding benchmark assessing the models motion-level perception capability. However, there are several limitations to our approach that should be acknowledged. Firstly, although we have made efforts to include diverse range of video content, our dataset may still have inherent biases in terms of geographical, cultural, and contextual variety. This could potentially limit the generalizability of research findings based on this dataset to different settings. Secondly, while we have performed extensive annotations, there may be occasional inaccuracies or inconsistencies due to human and automatic tool error. Regarding the broader impact, motion-level perception is pivotal in video understanding. MotionBench provides comprehensive benchmarking on video VLMs motionlevel perception. By making our dataset publicly available, we hope to further enhance the capabilities of video understanding models, thereby improving their applicability in real-world scenarios. 13. More Dataset Samples For better demonstration, we show more samples from the MotionBench. Figure 7. The absolute number and the proportion of questions that all models answered incorrectly relative to the total number of questions in each task type. 10. More Experimental Results Given the same sequence length in the VLM decoder, we benchmark results for different compression methods at various compression rates. We conduct experiments with Ninput = 4 and 8, varying the compression rate at 2, 4, 6, and 8. Tab. 7 provide the complete results. Given the same VLM input frame count, we experiment different compression ratios on various architectures, with the numerical results illustrated in Tab. 8. 11. Case Study on Model Performance We show more case studies regarding the performance of existing models on MotionBench. Questions that confuses all models. As shown in Table 3, MotionBench is highly challenging for existing video understanding models. Currently, even the best video understanding models can achieve only less than 60% accuracy. In MotionBench, there are some questions for which all models output incorrect answers. Figure 7 shows the absolute number and the proportion of questions that all models answered incorrectly relative to the total number of questions in each task type. Firstly, compared to the total number of questions in every task type, only small fraction of questions were answered incorrectly by all models. Among the tasks, the highest proportion of questions that all models answered incorrectly is that in the Fast action count task type. This attributes to counting repetitive actions at the motion level is inherently very challenging task, and current video understanding models still struggle to handle such issues correctly. Case study. We show case that all the models answered incorrectly. This is case in which males hand is touch16 Task type: Motion Recognition What is the sequence of movements between the two males? A. The male on the right raises his hand first, then the left male removes the string B. No movement occurs at all C. Both actions occur simultaneously D. The male on the left removes the string first, then the right male raises his hand Task type: Motion Recognition Which facial movement occurs with the woman on the right? A. Full head tilt down B. Slight head turn to the right C. Eyes close briefly D. Look straight ahead throughout Task type: Action Order What is the sequence of ball movement in the video? A. The ball is thrown to the left and then rolls back from the left. B. The ball rolls from the left and then is thrown to the right. C. The ball is thrown to the right and rolls from the right. D. The ball is thrown upwards and rolls down. 17 Task type: Action Order What is the sequence of actions involving the two men? A. The man on the right raises his hands first, followed by the man on the left B. Neither man raises their hands C. The man on the left raises his hands first, followed by the man on the right D. Both men raise their hands simultaneously Task type: Motion-related Objects What did this person take with his right hand? A. Screw B. Thumbtack C. Bolt nut D. Pen core Task type: Motion-related Objects What does the camera reveal as it moves backward over the road? A. crosswalk appearing B. sign on top of the lead car C. The end of line of parked cars D. The cars stopping abruptly 18 Task type: Location-related Motion In what order does the plane appear and move across the screen? A. From the left to completely leaving the frame B. From the top left to bottom right C. From the right to the upper part before disappearing D. From the bottom left to top right Task type: Location-related Motion What movement trajectory does the horse follow? A. Stays in place B. Moves directly towards the camera C. Gallops in circles D. Jumps over an obstacle Task type: Repetition Count Please count the number of repeated actions in the video. A. 3 B. 6 C. 9 D. 4 Task type: Repetition Count Please count the number of repeated actions in the video. A. 3 B. 2 C. 1 D. 6 Task type: Camera Motion Does the camera perform any movement during the scene? A. Yes, it zooms in. B. No, it remains static. C. Yes, it pans to the left. D. Yes, it rotates counterclockwise. Task type: Camera Motion What is the sequence of the camera movements during the interaction? A. The camera stays still, only focusing on the woman B. The camera shifts to show the men with their backs, then returns to face the men C. The camera shifts to show both men together, then moves back to the woman D. The camera starts facing the men, shifts to the woman, then moves back to the men"
        }
    ],
    "affiliations": [
        "Tsinghua University",
        "Zhipu AI"
    ]
}