{
    "paper_title": "Kinematify: Open-Vocabulary Synthesis of High-DoF Articulated Objects",
    "authors": [
        "Jiawei Wang",
        "Dingyou Wang",
        "Jiaming Hu",
        "Qixuan Zhang",
        "Jingyi Yu",
        "Lan Xu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "A deep understanding of kinematic structures and movable components is essential for enabling robots to manipulate objects and model their own articulated forms. Such understanding is captured through articulated objects, which are essential for tasks such as physical simulation, motion planning, and policy learning. However, creating these models, particularly for objects with high degrees of freedom (DoF), remains a significant challenge. Existing methods typically rely on motion sequences or strong assumptions from hand-curated datasets, which hinders scalability. In this paper, we introduce Kinematify, an automated framework that synthesizes articulated objects directly from arbitrary RGB images or textual descriptions. Our method addresses two core challenges: (i) inferring kinematic topologies for high-DoF objects and (ii) estimating joint parameters from static geometry. To achieve this, we combine MCTS search for structural inference with geometry-driven optimization for joint reasoning, producing physically consistent and functionally valid descriptions. We evaluate Kinematify on diverse inputs from both synthetic and real-world environments, demonstrating improvements in registration and kinematic topology accuracy over prior work."
        },
        {
            "title": "Start",
            "content": "Kinematify: Open-Vocabulary Synthesis of High-DoF Articulated Objects Jiawei Wang1,3 Dingyou Wang1,2 Jiaming Hu3 Qixuan Zhang1,2 Jingyi Yu2 Lan Xu2 5 2 0 N 4 ] . [ 2 4 9 2 1 0 . 1 1 5 2 : r Fig. 1. Overview of Kinematify. part-aware 3D foundation model first reconstructs segmented digital twin. Then, the kinematic tree is recovered via Monte Carlo Tree Search (MCTS) driven by rewards for structure, stability, contact, symmetry, and hierarchy. Finally, joint types are predicted by vision language model (VLM), and joint parameters are optimized on the parent links signed distance field (SDF) to enforce contact consistency and avoid collisions. AbstractA deep understanding of kinematic structures and movable components is essential for enabling robots to manipulate objects and model their own articulated forms. Such understanding is captured through articulated objects, which are essential for tasks such as physical simulation, motion planning, and policy learning. However, creating these models, particularly for objects with high degrees of freedom (DoF), remains significant challenge. Existing methods typically rely on motion sequences or strong assumptions from hand-curated datasets, which hinders scalability. In this paper, we introduce Kinematify, an automated framework that synthesizes articulated objects directly from arbitrary RGB images or textual descriptions. Our method addresses two core challenges: (i) inferring kinematic topologies for high-DoF objects and (ii) estimating joint parameters from static geometry. To achieve this, we combine MCTS search for structural inference with geometry-driven optimization for joint reasoning, producing physically consistent and functionally valid descriptions. We evaluate Kinematify on diverse inputs from both synthetic and real-world environments, demonstrating improvements in registration and kinematic topology accuracy over prior work. https://sites.google.com/deemos.com/kinematify 1Deemos Technology Co., Ltd., Shanghai, China. Emails: {joel.wang, dingyou, zhangqx}@deemos.com. 2ShanghaiTech University, Shanghai, China. Emails: {wangdy2024, zhangqx1, yujingyi, xulan1}@shanghaitech.edu.cn. 3Contextual Robotics Institute, UC San Diego, La Jolla, CA 92093, USA. Emails: {jiw179, jih189}@ucsd.edu. Project lead: Qixuan Zhang (zhangqx@deemos.com). *Corresponding authors: Jingyi Yu (yujingyi@shanghaitech.edu.cn), Lan Xu (xulan1@shanghaitech.edu.cn). I. INTRODUCTION Enabling robots to effectively interact with objects, as well as to model their own articulated structures for selfperception and adaptation, requires an accurate understanding of kinematic topologies and joint parameters. Articulated robot descriptions capture this understanding by encoding geometry, kinematic dependencies, and dynamic constraints in standard formats like the Unified Robot Description Format (URDF) [1]. These descriptions are essential for robotic tasks such as manipulation, locomotion, and policy learning. However, customizing such descriptions for articulated objects remains significant challenge, demanding substantial manual effort, especially for high degrees of freedom (DoF) systems like humanoids, quadrupeds, and arms. This difficulty arises from the labor intensive processes of part-aware 3D modeling, resolving intricate kinematic dependencies, and inferring precise joint parameters [2]. While recent advances in part-aware 3D generation [3][7] now enable the on-demand creation of high-quality segmented meshes from RGB images or textual descriptions, the bottleneck of kinematics inference remains. This challenge has driven robotics researchers to explore high-DoF articulated objects generation approaches. Prior work has followed two main directions. Geometryfirst approaches infer parts and joints from dense 4D sequences or multi-scan data, which achieve high fidelity but rely on controlled capture settings [8], [9]. Program-synthesis pipelines, in contrast, predict executable descriptions directly from visual inputs [10][12]. While effective, these systems mainly target everyday objects such as laptops, bottles, and drawers, which typically contain only few moving parts and relatively simple kinematic dependencies. In the context of self-modeling, related work such as AutoURDF [13] extends this idea to robots, recovering topology and joint types from point-cloud sequences. However, it presumes motion data and is largely limited to serial-chain structures, whereas high-DoF objects often exhibit multi-branched linkages. To address these challenges, we introduce Kinematify, framework that generates articulated 3D objects from RGB images or texts. Kinematify generates the segmented mesh with part-aware 3D foundation model, such as Rodin [3], then infers the kinematic tree using an MCTS [14][16] objective that balances hierarchy and structural regularity. Subsequently, it estimates joint parameters via DW-CAVL, novel Distance-Weighted Contact-Aware Virtual Linkage optimization approach. This approach preserves near-contact regions while penalizing collisions under virtual motion. The resulting description is exported to URDF and is readily convertible to formats like MJCF [17] or USD [18]. Our contributions are: An open-vocabulary articulated object generation framework. Kinematify generates physics-aware articulated objects directly from arbitrary RGB images or textual descriptions, without requiring motion data, training, or pre-defined articulation priors. MCTS-based kinematic tree inference approach. We propose search objective that encodes structural priors like hierarchy and regularity to resolve ambiguous attachments for complex, high-DoF articulated objects with multiple branches. SDF-driven joint parameter estimation approach. The DW-CAVL algorithm accurately infers revolute and prismatic joint parameters from static geometry by optimizing an SDF-based, contact-aware objective under virtual motions. II. RELATED WORK sequence by jointly optimizing segmentation, topology, and joint parameters. These methods achieve high fidelity but depend critically on the availability of multi-view or temporal data, which requires controlled capture setup. Another trend frames articulation modelling as program synthesis problem [10][12], [21][33]. URDFormer [12] trains transformer to predict URDF from single image, relying on largescale synthetic dataset of image-URDF pairs. Real2Code [10] and Articulate-Anything [11] leverage large language models to generate code-based representations of articulated objects, with the latter using reinforcement learning loop to refine the model through simulation feedback. While powerful in their open-vocabulary capabilities, these methods often produce functionally plausible rather than geometrically precise models and struggle with the multi-branch kinematics, which are common in high-DoF objects. B. Robot Self-Modeling Distinct from general everyday object modeling, robot selfmodeling is the online process by which robot autonomously discovers its own body plan [13], [34][38]. This is typically achieved by correlating motor actions with sensory feedback. The foundational concept of task-agnostic self-modeling [34] involves robot performing random motions and building self-representation from the resulting data. This has been realized in various ways. Ledezma et al. [36] used IMU sensors on each link, applying machine learning to the sensor data to explicitly solve for the robots topology and kinematic parameters. These methods are powerful but require an embodied agent with access to its own motor and sensory signals. AutoURDF [13] represents purely visual approach to this problem. It operates on time-series point cloud frames of robot in motion, but without access to the underlying motor commands. By tracking the 6-DoF transformations of point clusters, it segments moving parts, infers the kinematic tree using minimum spanning tree, and estimates joint parameters. It demonstrated superior performance in topology inference for serial-chain objects. III. KINEMATIFY We introduce Kinematify, framework that generates kinematics-aware articulated objects directly from RGB images or textual descriptions in zero-shot context. A. 3D Articulated Object Modelling A. Preliminaries significant body of work focuses on reconstructing the kinematic structure of everyday objects from visual data. The most common paradigm leverages motion to reveal articulation [8], [9], [13], [19], [20]. By observing an object over time, these methods can directly infer which parts move together and identify the axes of rotation or translation. For example, MultiBodySync [9] registers multiple 3D scans of an object in different states, using spectral synchronization to jointly solve for part segmentation and motion. Similarly, to 4D point cloud ReArt [8] fits rearticulable model Assemblies and parts. An assembly is set of parts = {Pi}N i=1. Part Pi has triangulated surface mesh Mi = (Vi, Fi) with vertices Vi R3 and triangular faces Fi {1, . . . , Vi}3. Each part stores world transform Ti SE(3), an intrinsic rotation Ri SO(3) used for alignment, centroid ci R3, robust volume vi > 0, and axis-aligned bounding-box extents ei R3 >0. Graphs and kinematic tree. We build an undirected connection graph = (V, E) with , where an edge indicates geometric contact. directed kinematic tree = (V, ET ), rooted at the base link , orients and annotates joints. Joints. For directed edge (u v) ET , joint stores type Juv {f ixed, revolute, prismatic}, parent-to-child origin ouv R3, and if movable, an axis auv S2 and optionally pivot puv R3 when revolute. B. Part-Aware 3D Representations We generate part-level 3D meshes with part-aware 3D foundation model [3] from the input RGB images or textual description, and discard meshes with too few vertices or degenerate spatial spread. For each prospective parent part, we train continuous SDF [39][41] fθ : R3 on (i) noisy surface points, (ii) near-surface offsets, and (iii) far samples in bounding AABB. Afterward, we build connection graph with the trained SDF, as shown in the middle left panel of Fig. 2. Given two candidate parts and with respective SDFs fθA and fθB , we evaluate mutual distances between their sampled surfaces under fθ. Pairs of parts whose minimum bidirectional distance falls below tolerance ϵ are declared in contact, and an undirected edge is added between them. C. Kinematic Topology Inference We orient the graph into directed kinematic tree with root b. For any directed tree X, let (X) and E(X) denote its node and oriented edge sets. Node positions ci R3 are reference points of part i. For an oriented edge (u v) E(X), its edge-origin vector is ouv := cv cu. For node-wise functions : ( ) R, the average is := ( )1 (cid:80) iV ( ) (i). Depth d(i) is the graph distance from the root to node i. The out-degree in the directed tree is deg+(i). All edge-wise sums (cid:80) (uv) are over (u v) E( ) unless stated otherwise. positive distance threshold dmax > 0 is used when attaching disconnected components. a) Base selection and BFS orientation: We choose the base link as any node in with the highest undirected degree. Starting from b, we run BFS on as warm start for the MCTS search. During BFS, when new neighbour is first reached from an already visited node u, we define as the parent and as the child, orient the edge as v, and set its origin ouv cv cu. Any edge whose addition would create cycle is not inserted into and is recorded as broken. If is disconnected, each remaining component is attached to the current tree by adding virtual edge from its nearest neighbor, provided the Euclidean distance is at most dmax. 1) State, actions, constraints: search state is = (TS, VS, BS), where TS is the current partial directed tree, VS (G) the visited-node set, and BS E(G) the set of broken undirected edges discovered so far. An action adds feasible oriented edge with VS and / VS. To respect discovered symmetries, we form part clusters {Ck} by the Chamfer distance between segmented meshes. During expansion, we forbid connecting two nodes that belong to the same multi-member cluster to avoid spurious intra-cluster links. Fig. 2. Pipeline of Kinematify for recovering articulated robots from single RGB image. Step 1: 3D foundation model generates segmented mesh of the robot. Step 2: contact graph is constructed over mesh parts, capturing candidate relations between components. Step 3: Infer the kinematic tree using MCTS, resolving ambiguous connections by leveraging structural priors such as hierarchy and symmetry. Step 4: Refine joint parameters using the DWCAVL optimization approach while preserving near-contact geometry. Bottom row: Examples of inferred revolute joints with optimized axes. 2) Transition: Applying an action updates the edge origin ouv cv cu, provisionally treats the joint as fixed until later typing, inserts into VS, and appends to BS any undirected edge (v, w) E(G) with VS {u} that would otherwise form cycle. 3) Reward: For completed tree , the terminal reward is weighted sum of five terms: a) Rstruct: This term penalizes irregular depths, excessive out-degree, and long trees: Rstruct = 1 d2 + (deg+k)2 + λ E( ) , (1) where is the preferred out-degree and λ > 0 is hyperparameter. b) Rstatic: This term favours centre-of-mass support to reduce gravitational torque about joint frames. Let vi > 0 denote the estimated volume of part and mi = ρ vi its mass for density parameter ρ > 0. With subtree mass (i) and subtree centre csub(i), (i) = mi + (cid:88) (j), csub(i) = jch(i) mi ci + (cid:80) jch(i) (j) csub(j) (i) (2) , (3) where ch(i) is the set of children of in . Let ˆz = [0, 0, 1] denote downward unit gravity and > 0 the gravitational constant. The total gravitational torque is τ = (cid:88) i=b (cid:13) (cid:0)csub(i) ci (cid:13) (cid:1) (cid:0)M (i) ˆz(cid:1)(cid:13) (cid:13)2 , Rstatic = 1 1 + τ /στ , (4) (5) with στ > 0 robust per-assembly normaliser based on MAD scale. c) Rcontact: We quantify contact strength from SDFbased bidirectional proximity. Let s(u, v) [0, 1] denote the contact strength of physical contact on edge (u v). We reward higher average strength: Algorithm 1: KINEMATIFY Input: Connection graph = (V, E), base b, SDFs {fθ}, samples {Pv} Output: Kinematic tree = (V, ET ), joints {Juv} Rcontact = 1 E( ) (cid:88) (uv)E( ) s(u, v). (6) d) Rsym: Within each discovered symmetry cluster Ck (Ck 2), we prefer equal depths and shared parent, such as legs attached to the same torso, fingers to the same palm. Let Pk = {parent(i) : Ck, = b}. We define Sk = 1 1 + Var(cid:0)d(i) : Ck (cid:1) + (cid:104) 1 Pk 1 Ck 1 + ε (cid:105) , Rsym = meank Sk. (7) The second term equals 1 when all parts in Ck share the same parent (Pk = 1) and decreases linearly as parents diversify. ε > 0 avoids division by zero. e) Rhier: We discourage children much larger than their parents by estimated volume. With small ε > 0 to avoid divide-by-zero, Rhier = (cid:110) 1 + (cid:80) (uv) max 0, (cid:111) . (8) vv vu+ε 1 4) Search: We use Monte Carlo Tree Search (MCTS) with UCT. Each state is = (TS, VS, BS). From state S, each child corresponds to applying one feasible action v. Let Q(c) be the cumulative return backed up through child c, (c) its visit count, and (parent) the visit count of its parent state. With exploration constant > 0, selection chooses arg max Q(c) (c) + (cid:115) ln (parent) (c) . (9) Rollouts greedily complete the tree by repeatedly choosing any available action with the highest immediate score, and the terminal return R( ) is backed up along the simulation path. This objective helps resolve symmetric attachments and multibranch ambiguities at scale. The middle right panel of Fig. 2 shows the kinematics structure after MCTS search. D. Joint Reasoning We render orthographic viewsets for the whole assembly and for joint close-ups. For each (u v), we query VLM on the joint viewset and adopt decision with abstention. If the VLM successfully identifies the joint type, we group joints by child clusters Ck and select representative by majority and correct outliers. Let PA = {ai} and PB = {bj} be surface samples of two parts and in common frame. We first extract contact region as the union of points on either part whose nearest neighbour on the other part lies within small threshold. To downweight spurious pairs, each is assigned weight that decays with its nearest-neighbour distance, using these weights we compute weighted contact centroid µc 4 5 6 8 9 10 11 15 1 S0 (, {b}, ); init stats Q, 0 ; 2 for = 1 to Nmax do 3 S0; path ; while VS < do if untried edge exists then pick untried; Transition(S, a); break else arg maxc P.append(S) Q(c) (c) + C(cid:112)ln (S)/N (c) greedy rollout from ; Reward( ); backprop along ; 12 best cached tree ; 13 for edge (u v) ET do 14 candidates generate from contact stats ; for candidate (p, u) do optimize Jrev or Jpri 17 select best class: revolute if srev > ζspri, else prismatic, else fixed ; store Juv 19 return (T, {Juv}) 18 and weighted covariance Σ. The principal direction with the smallest variance provides hinge-axis estimate (cid:98)uPCA. In parallel, we obtain contact normal (cid:98)n by averaging nearestpoint difference vectors across the two surfaces. We then form diverse set of revolute candidates (p, u) as follows. Pivots are initialised at the contact centroid (p = µc). Axes are drawn from compact pool that includes (cid:98)uPCA, the contact normal (cid:98)n, an orthogonal completion (cid:98)u, the principal axes of Σ, and few random unit directions. Along each candidate axis we place handful of pivot samples by sliding slightly along around µc. 1) Differentiable rigid motions: For revolute motion (p, u, θ) with u2 = 1, = + R(u, θ) (x p), R(u, θ) = cos θ + sin θ [u] + (1 cos θ) uu, (10) (11) where [u] is the 3 3 cross-product matrix. For prismatic motion with displacement R, = + u. We parametrise by an unconstrained R3 via = a/a2, whose Jacobian is a = 1 a2 (cid:19) (cid:18) aa a2 2 . (12) 2) DW-CAVL objective: Let child samples be {bi}. virtual motion parameter δ Θ denotes either revolute angle Fig. 3. Examples of articulated objects generated by Kinematify. Each row shows different objects across sequence of joint configurations. θ or prismatic displacement t. Let Φδ be the corresponding rigid transform of the child, and define s0(x) = fθ(x), sδ(x) = fθ (cid:0)Φδ(x)(cid:1). With hyperparameters volumetric margin mvol > 0, logistic sharpness > 0, contact band width σc > 0, and εsmall > 0, wvol(s0) = σ(cid:0) k(s0 mvol)(cid:1), (cid:18) 1 1 + ez , σ(z) = (13) (cid:19) wdist(s0) = exp s2 0 2σ2 , wi = wvol(s0(bi)) wdist(s0(bi)). (14) The consistency term penalizes separation near contact after motion, (cid:80) wi Lcons(δ) = (cid:2) max{0, sδ(bi) mvol}(cid:3)2 (cid:80) wi + εsmall . (15) Using inverse volumetric weights wi = σ(cid:0)+k(s0(bi)mvol)(cid:1), the collision term penalises penetration, (cid:80) wi Lcoll(δ) = (cid:2) max{0, sδ(bi) mvol}(cid:3)2 (cid:80) wi + εsmall . (16) Fig. 4. Qualitative comparison of articulation recovery on everyday objects across three methods: Kinematify (ours), Articulate Anymesh, and ArtGS. The red line indicates the joint direction. For revolute joints we regularise the pivot toward µc: Lreg = λpp µc2 2, λp 0. (17) 3) Candidate selection: We rank many candidates (p, u) on subsampled points using s(p, u), refine the top-K on full points by minimising , and output scores 1/(1 + ). Aggregating over δ Θ yields 1 Θ (p, u) = (cid:88) δΘ (cid:0)λcLcons(δ)+λcollLcoll(δ)(cid:1)+Lreg, (18) with nonnegative weights λc, λcoll. In summary, we infer joint parameters from static geometry and select the candidate with the highest score. The bottom panel in Fig. 2 shows the optimized results for each joint for the input. TABLE UNIFIED BASELINE COMPARISON ACROSS ROBOTS SORTED BY DEGREES OF FREEDOM (DOF). WE REPORT AXIS ANGLE ERROR (), AXIS POSITION ERROR (M), AND TREE EDIT DISTANCE, WHERE LOWER VALUES ARE BETTER. Metric Method Everyday Object 1-8 DoF UR10e 6 DoF Franka Panda 7 DoF Unitree Go2 12 DoF Fetch 13 DoF Allegro 16 DoF Unitree H1 19 DoF Axis Angle Error Axis Pos Error TED Articulate Anymesh ArtGS Ours Articulate Anymesh ArtGS Ours AutoURDF Ours 35.80 13.80 2.92 0.19 0.75 0.23 0.27 0.13 39.67 25.52 5. 0.25 0.97 0.27 0.87 1.03 42.10 21.30 10.42 0.31 0.68 0.15 1.28 0.89 53.23 22.32 9. 0.41 1.13 0.30 2.21 1.97 75.60 53.81 23.10 0.89 1.93 0.71 3.21 1.78 78.77 65.59 31. 0.32 0.67 0.21 4.83 1.22 79.35 41.29 29.31 0.74 1.32 0.68 8.13 2.23 Mean 57.79 34.80 16.06 0.44 1.06 0.36 2.97 1.32 IV. EXPERIMENTS We evaluate Kinematify in two settings: (i) everyday articulated objects and (ii) robotic platforms. We follow prior protocols of Articulate Anymesh [22], which use groundtruth segmented meshes from PartNet-Mobility [42], [43] to isolate the impact of 3D segmentation. This enables fair, direct comparisons to baselines that do not accept raw images or texts. We erase the provided kinematic graphs and joint parameters, and reconstruct the mesh. Under this configuration, we compare against the baselines Articulate AnyMesh [22] and ArtGS [19]. For the robotics platforms, we evaluate Kinematify on six commonly used robot models spanning range of DoF. Because ArtGS and Articulate AnyMesh do not expose explicit kinematic structure, we additionally compare against AutoURDF [13] for kinematics reconstruction performance. We also conduct experiments on the end-to-end pipelines, starting from RGB images, and evaluate the output quality with ground-truth to quantify the discrepancies. A. Metrics"
        },
        {
            "title": "We report three metrics evaluating both joint parameter and",
            "content": "kinematics tree quality. Axis Angle Error: The angular deviation between the predicted and ground-truth joint-axis directions, and opposite directions are treated as equivalent. Axis Position Error: The Euclidean distance between predicted and ground-truth pivot positions in the dataset coordinate frame. Tree Edit Distance: The Tree Edit Distance [44] between the predicted and ground-truth kinematic trees, for instance, the minimal number of node insertions, deletions, or relabelings needed to match the trees. B. Quantitative results Everyday Objects. Table reports comparison of Kinematify against Articulate Anymesh and ArtGS on the PartNetMobility benchmark. Our method achieves the lowest axis angle error among all approaches, indicating superior accuracy in joint orientation estimation. In terms of axis position error, Kinematify also performs competitively, with values close to Fig. 5. Demonstration of Kinematify on two high-DoF robots: Unitree Go2 (12 DoF, left) and Unitree H1 (19 DoF, right). For each case, the pipeline starts from segmented mesh, followed by kinematic tree inference and joint parameter optimization. the best baseline. qualitative visualization of these comparisons is provided in Fig. 4. Together, these results demonstrate that Kinematify produces precise joint axes and stable pivot placements for everyday objects. Robots. We further evaluate performance on six robotic systems by measuring both registration quality and body topology reconstruction. As shown in Table I, Kinematify reduces the Tree Edit Distance by substantial margin on average, reflecting more faithful recovery of kinematic structures. Representative results on Unitree H1 and Go2 are visualized in Fig. 5. These findings highlight the effectiveness of our MCTS-based objective in reasoning about high-DoF, multibranched kinematic structures, surpassing prior methods in structural consistency. C. End-to-end evaluation We further evaluate Kinematify in full end-to-end setting that starts from single RGB images. part-aware 3D foundation model [3] first produces segmented mesh. Then, we apply the kinematic reasoning stack unchanged. Because existing baselines do not natively support image to articulated objects at comparable scope, we report absolute performance rather than head to head comparisons. As shown in II, compared to the geometry-only track in Table I, end-to-end errors increase modestly on EO and more noticeably on Fetch and Panda, consistent with their tighter kinematic tolerances. TABLE II END-TO-END RESULTS. NUMBERS ARE ABSOLUTE. Metric Everyday Objects Fetch Panda Axis Angle Error Axis Position Error (m) TED 3.78 0.28 0.67 32.84 0.95 2.95 14.08 0.22 1.17 TABLE III ABLATION STUDY ON THE PROPOSED METHOD, WHERE EO DENOTES EVERYDAY OBJECT. Metric Variant EO Fetch Panda Axis Angle Error Axis Pos Error TED w/o MCTS w/o DW-CAVL Ours w/o MCTS w/o DW-CAVL Ours w/o MCTS w/o DW-CAVL Ours 4.32 13.94 2.92 28.30 42.30 23.10 10.92 29.39 10. 0.59 1.34 0.23 0.39 0.14 0.13 0.97 1.82 0.71 3.32 1.93 1.78 0.30 0.97 0.15 2.97 0.98 0. D. Ablation study We quantify the contribution of each core component by comparing the full method against two ablations: (i) removing the DW-CAVL anchor term so that optimization considers only collision avoidance; and (ii) replacing the MCTS-based kinematic inference with BFS strategy. As shown in Table III, substituting MCTS with BFS consistently yields larger TED across robots. BFS greedily attaches along local contacts and lacks long range regularization, leading to incorrect parent choices in symmetric substructures and unbalanced trees. In contrast, removing the DW-CAVL anchor does not drastically change the tree but significantly degrades joint parameters. Without an attraction to the contact centroid and near-surface band, the optimizer favors axes that quickly reduce interpenetration yet drift from true pivots. Overall, the full model achieves the best balance. E. Real-world robot manipulation We export the recovered kinematics to URDF and deploy the models in simulation and on real robot, shown in Fig. 6). From the segmented mesh, Kinematify generates Fetch URDF and simple cabinet URDF. For planning, we load both URDFs into single MoveIt planning scene and derive an SRDF group for the arm. We use constraint motion planner [45][47] as the backend. The task is executed in two stages: (1) reach-to-grasp and (2) constrained pull. In addition to this drawer-opening scenario, we also demonstrate an online planning task of pouring water from cup into container, using the same URDF pipeline and motion-planning setup. The same URDFs are used in Isaac Sim and on hardware. In both cases, the arm follows the planned trajectories without collision, showing that the recovered kinematics are physically consistent and directly usable for online planning in ROS and MoveIt. Fig. 6. Kinematify generates URDFs for both the Fetch robot and the drawer, enabling demonstration of the robot opening the drawer in Isaac Sim and transfer to real, with the same models usable for online planning with MoveIt. V. DISCUSSION AND LIMITATIONS This work addresses the open-vocabulary synthesis of articulated objects from static inputs. However, accurate part segmentation and mechanically faithful contact graph remain assumptions. Spurious seams or missed contacts can mislead the search or leave subassemblies unattached. practical extension is to estimate contact reliability and jointly refine segmentation and contacts. We also find that decorative geometry near interfaces can introduce false contacts that bias the search. In such cases, reweighting the MCTS objective to emphasize the symmetry prior, i.e., increasing the weight on Rsym relative to Rcontact, helps suppress attachments to decorative elements and favors globally coherent parents within symmetric clusters. An adaptive schedule that upweights Rsym when cluster cardinality is high or contact variance is large is simple and effective mitigation. Learning these weights from validation performance is complementary direction. VI. CONCLUSION We presented Kinematify, an automated pipeline that synthesizes articulated object and robot descriptions directly from RGB images or textual descriptions. Across everyday objects and robot platforms, Kinematify improves joint accuracy and kinematic tree fidelity over prior work. We view Kinematify as step toward open-vocabulary synthesis of high-DoF articulated structures."
        },
        {
            "title": "REFERENCES",
            "content": "[1] ROS Wiki, Unified Robot Description Format (URDF), 2023. Accessed: 2025-09-07. [2] S. Brawner, Solidworks to urdf exporter. https://github.com/ros/ solidworks urdf exporter, 2024. Accessed: 2025-09-11. [3] L. Zhang, Q. Zhang, H. Jiang, Y. Bai, W. Yang, L. Xu, and J. Yu, Bang: Dividing 3d assets via generative exploded dynamics, ACM Trans. Graph., vol. 44, no. 4, 2025. [4] Y. Yang, Y.-C. Guo, Y. Huang, Z.-X. Zou, Z. Yu, Y. Li, Y.-P. Cao, and X. Liu, Holopart: Generative 3d part amodal segmentation, arXiv preprint arXiv:2504.07943, 2025. [5] J. Tang, R. Lu, Z. Li, Z. Hao, X. Li, F. Wei, S. Song, G. Zeng, M.-Y. Liu, and T.-Y. Lin, Efficient part-level 3d object generation via dual volume packing, arXiv preprint arXiv:2506.09980, 2025. [6] J.-J. Wang, X.-Y. Zhang, S.-Z. Liu, L. Zhang, and Y.-R. Zhang, Parthand: large-scale dataset of articulated 3d hands with part-level annotations, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. [7] J. Gao, C. Liu, F. Hong, B. Pan, B. Dai, D. Lin, Y. Qiao, and H. Li, Get3d: generative model of high quality 3d textured shapes learned from images, in NeurIPS, 2022. [8] S. Liu, S. Gupta, and S. Wang, Building rearticulable models for arbitrary 3d objects from 4d point clouds, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023. [9] J. Huang, H. Wang, T. Birdal, M. Sung, F. Arrigoni, S.-M. Hu, and L. Guibas, Multibodysync: Multi-body segmentation and motion estimation via 3d scan synchronization, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021. [10] M. Zhao, Y. Weng, D. Bauer, and S. Song, Real2code: Reconstruct articulated objects via code generation, arXiv preprint arXiv:2406.08474, 2024. [11] L. Le, J. Xie, W. Liang, H.-J. Wang, Y. Yang, Y. J. Ma, K. Vedder, A. Krishna, D. Jayaraman, and E. Eaton, Articulate-anything: Automatic modeling of articulated objects via vision-language foundation model, arXiv preprint arXiv:2410.13882, 2024. [12] Z. Chen, A. Walsman, M. Memmel, K. Mo, A. Fang, K. Vemuri, A. Wu, D. Fox, and A. Gupta, Urdformer: pipeline for constructing articulated simulation environments from real-world images, arXiv preprint arXiv:2405.11656, 2024. [13] J. Lin, L. Zhang, K. Lee, J. Ning, J. Goldfeder, and H. Lipson, Autourdf: Unsupervised robot modeling from point cloud frames using cluster registration, arXiv preprint arXiv:2412.05507, 2024. [14] C. B. Browne, E. Powley, D. White, S. M. Lucas, P. I. Cowling, P. Rohlfshagen, S. Tavener, D. Perez, S. Samothrakis, and S. Colton, survey of monte carlo tree search methods, IEEE Transactions on Computational Intelligence and AI in Games, vol. 4, no. 1, pp. 143, 2012. [15] R. Coulom, Efficient selectivity and backup operators in monte-carlo tree search, in Computers and Games, vol. 4630 of LNCS, pp. 7283, Springer, 2007. [16] L. Kocsis and C. Szepesvari, Bandit based monte-carlo planning, in ECML, pp. 282293, Springer, 2006. [17] E. Todorov, T. Erez, and Y. Tassa, Mujoco: physics engine for modelbased control, in IROS, pp. 50265033, 2012. [18] J. Liang, V. Makoviychuk, A. Handa, N. Chentanez, M. Macklin, and D. Fox, Gpu-accelerated robotic simulation for distributed reinforcement learning, 2018. [19] Y. Liu, B. Jia, R. Lu, J. Ni, S.-C. Zhu, and S. Huang, Artgs: Building interactable replicas of complex articulated objects via gaussian splatting, 2025. [20] L. Shen, S. Zhang, H. Li, P. Yang, Z. Huang, Z. Zhang, and H. Zhao, Gaussianart: Unified modeling of geometry and motion for articulated objects, 2025. [21] R. Luo, H. Geng, C. Deng, P. Li, Z. Wang, B. Jia, L. Guibas, and S. Huang, Physpart: Physically plausible part completion for interactable objects, 2025. [22] X. Qiu, J. Yang, Y. Wang, Z. Chen, Y. Wang, T.-H. Wang, Z. Xian, and C. Gan, Articulate anymesh: Open-vocabulary 3d articulated objects modeling, 2025. [23] J.-X. Wu, Z.-Y. Wu, and C.-H. Lin, Genpose: Generative category-level object pose estimation from an image, 2024. [24] A. Chattopadhyay, A. Lamb, S. Tulsiani, A. Gonzalez-Garcia, L. Jiang, and J. Pont-Tuset, Instructpart: foundation model for editable partlevel 3d shape generation, in International Conference on Learning Representations (ICLR), 2024. [25] C. L. Li, H.-X. Yu, A.-C. Cheng, J.-T. Lee, and J.-B. Huang, Articulateddiffusion: diffusion model for articulated 3d shape generation, 2024. [26] Z. An, Z. Chen, and D. Xu, Gad: Generative articulated disassembly of 3d objects, 2024. [27] G.-H. Wang, K.-W. Hsiao, M. Savva, and A. X. Chang, Genad: generative model for 3d articulated objects, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. [28] Z. Li, Q. Fu, Y. Liu, C. Wen, S. Liu, D. Manocha, Y. Zhu, and S. Huang, Interactron: Embodied object-centric learning via interaction, 2023. [29] L. Abbatematteo, A. Paolillo, J. Piro, J. Peters, and G. Chalvatzaki, Learning articulated object functionality from human interaction, in IEEE International Conference on Robotics and Automation (ICRA), 2024. [30] Z. Hong, K.-H. Hui, S. Wang, Chen-Ze-Jian, S. Agrawal, M. Yu, L. J. Guibas, and H. Su, Akb-48: large-scale articulated object knowledge base for part-based understanding, 2024. [31] M. Hassan, Z. Chen, M. Fischer, and M. Black, Part-e: largescale dataset and models for fine-grained 3d part-based text-to-shape generation, in Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2023. [32] L. Yariv, M. Atzmon, R. Jiang, S. Sida, S. Li, Y. Gu, and Y. Lipman, Articulated neural rendering for virtual avatars, ACM Transactions on Graphics (TOG), vol. 42, no. 4, 2023. [33] H. Wang, K. Mo, and L. J. Guibas, What is part of an object? computational perspective, arXiv preprint arXiv:2308.11216, 2023. [34] R. Kwiatkowski and H. Lipson, Task-agnostic self-modeling machines, Science Robotics, vol. 4, no. 26, p. eaau9354, 2019. [35] B. Chen, R. Kwiatkowski, C. Vondrick, and H. Lipson, Fully body visual self-modeling of robot morphologies, Science Robotics, vol. 7, no. 68, p. eabn1944, 2022. [36] F. D. Ledezma and S. Haddadin, Machine learningdriven selfdiscovery of the robot body morphology, Science Robotics, vol. 8, no. 85, p. eadh0972, 2023. [37] Z. Fu, Y. Liu, B. Jia, S. Liu, Y. Zhu, and S. Huang, Ditto: differentiable pipeline for digital twin of robots, arXiv preprint arXiv:2403.04829, 2024. [38] E. Seker, R. Toris, S. Chernova, and M. Cakmak, Self-discovering interpretable physical models for manipulation, in IEEE International Conference on Robotics and Automation (ICRA), 2024. [39] J. J. Park, P. Florence, J. Straub, R. Newcombe, and S. Lovegrove, Deepsdf: Learning continuous signed distance functions for shape representation, in CVPR, 2019. [40] A. Gropp, L. Yariv, N. Haim, M. Atzmon, and Y. Lipman, Implicit geometric regularization for learning shapes, in NeurIPS, 2020. [41] V. Sitzmann, J. N. P. Martel, A. W. Bergman, D. B. Lindell, and G. Wetzstein, Implicit neural representations with periodic activation functions, in NeurIPS, 2020. [42] F. Xiang, Y. Qin, K. Mo, Y. Xia, H. Zhu, F. Liu, M. Liu, H. Jiang, Y. Yuan, H. Wang, L. Yi, A. X. Chang, L. J. Guibas, and H. Su, SAPIEN: simulated part-based interactive environment, in The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2020. [43] K. Mo, S. Zhu, A. X. Chang, L. Yi, S. Tripathi, L. J. Guibas, and H. Su, Partnet: large-scale benchmark for fine-grained and hierarchical partlevel 3d object understanding, in CVPR, 2019. [44] M. Pawlik and N. Augsten, Efficient computation of the tree edit distance, ACM Trans. Database Syst., vol. 40, Mar. 2015. [45] J. Hu, S. R. Iyer, J. Wang, and H. I. Christensen, Motion planning in foliated manifolds using repetition roadmap., in Robotics: Science and Systems, 2024. [46] S. Chitta, I. ucan, and S. Cousins, Moveit! [ros topics], IEEE Robotics & Automation Magazine, vol. 19, no. 1, pp. 1819, 2012. [47] I. A. ucan, M. Moll, and L. E. Kavraki, The open motion planning library, IEEE Robotics & Automation Magazine, vol. 19, no. 4, pp. 72 82, 2012."
        }
    ],
    "affiliations": [
        "Deemos Technology Co., Ltd."
    ]
}