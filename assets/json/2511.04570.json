{
    "paper_title": "Thinking with Video: Video Generation as a Promising Multimodal Reasoning Paradigm",
    "authors": [
        "Jingqi Tong",
        "Yurong Mou",
        "Hangcheng Li",
        "Mingzhe Li",
        "Yongzhuo Yang",
        "Ming Zhang",
        "Qiguang Chen",
        "Tianyi Liang",
        "Xiaomeng Hu",
        "Yining Zheng",
        "Xinchi Chen",
        "Jun Zhao",
        "Xuanjing Huang",
        "Xipeng Qiu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "\"Thinking with Text\" and \"Thinking with Images\" paradigm significantly improve the reasoning ability of large language models (LLMs) and Vision Language Models (VLMs). However, these paradigms have inherent limitations. (1) Images capture only single moments and fail to represent dynamic processes or continuous changes, and (2) The separation of text and vision as distinct modalities, hindering unified multimodal understanding and generation. To overcome these limitations, we introduce \"Thinking with Video\", a new paradigm that leverages video generation models, such as Sora-2, to bridge visual and textual reasoning in a unified temporal framework. To support this exploration, we developed the Video Thinking Benchmark (VideoThinkBench). VideoThinkBench encompasses two task categories: (1) vision-centric tasks (e.g., Eyeballing Puzzles), and (2) text-centric tasks (e.g., subsets of GSM8K, MMMU). Our evaluation establishes Sora-2 as a capable reasoner. On vision-centric tasks, Sora-2 is generally comparable to state-of-the-art (SOTA) VLMs, and even surpasses VLMs on several tasks, such as Eyeballing Games. On text-centric tasks, Sora-2 achieves 92% accuracy on MATH, and 75.53% accuracy on MMMU. Furthermore, we systematically analyse the source of these abilities. We also find that self-consistency and in-context learning can improve Sora-2's performance. In summary, our findings demonstrate that the video generation model is the potential unified multimodal understanding and generation model, positions \"thinking with video\" as a unified multimodal reasoning paradigm."
        },
        {
            "title": "Start",
            "content": "Thinking with Video: Video Generation as Promising Multimodal Reasoning Paradigm Jingqi Tong1,2,*, Yurong Mou1,*, Hangcheng Li1,2,*, Mingzhe Li2,*, Yongzhuo Yang1,*, Ming Zhang1, Qiguang Chen3, Tianyi Liang1,2, Xiaomeng Hu4, Yining Zheng1, Xinchi Chen1, Jun Zhao1,, Xuanjing Huang1, Xipeng Qiu1,2, 1Fudan University, 2Shanghai Innovation Institute, 3Harbin Institute of Technology 4The Chinese University of Hong Kong"
        },
        {
            "title": "Abstract",
            "content": "Thinking with Text and Thinking with Images paradigm significantly improve the reasoning ability of large language models (LLMs) and Vision Language Models (VLMs). However, these paradigms have inherent limitations. (1) Images capture only single moments and fail to represent dynamic processes or continuous changes, and (2) The separation of text and vision as distinct modalities, hindering unified multimodal understanding and generation. To overcome these limitations, we introduce Thinking with Video, new paradigm that leverages video generation models, such as Sora-2, to bridge visual and textual reasoning in unified temporal framework. To support this exploration, we developed the Video Thinking Benchmark (VideoThinkBench). VideoThinkBench encompasses two task categories: (1) vision-centric tasks (e.g., Eyeballing Games), and (2) text-centric tasks (e.g., subsets of GSM8K, MMMU). Our evaluation establishes Sora-2 as capable reasoner. On vision-centric tasks, Sora-2 is generally comparable to state-of-the-art (SOTA) VLMs, and even surpasses VLMs on several tasks, such as Eyeballing Games. On text-centric tasks, Sora-2 achieves 92% accuracy on MATH, and 75.53% accuracy on MMMU. Furthermore, we systematically analyse the source of these abilities. We also find that self-consistency and in-context learning can improve Sora-2s performance. In summary, our findings demonstrate that the video generation model is the potential unified multimodal understanding and generation model, positions thinking with video as unified multimodal reasoning paradigm. Correspondence: jqtong25@m.fudan.edu.cn, zhaoj19@fudan.edu.cn, xpqiu@fudan.edu.cn Website: https://thinking-with-video.github.io Repository: https://github.com/tongjingqi/Thinking-with-Video Benchmark: https://huggingface.co/datasets/fnlp/VideoThinkBench 5 2 0 2 6 ] . [ 1 0 7 5 4 0 . 1 1 5 2 : r"
        },
        {
            "title": "Introduction",
            "content": "Chain-of-thought (CoT) significantly improves the reasoning ability of large language models (LLMs) [37], establishing \"thinking with text\" as fundamental paradigm in AI reasoning. OpenAI o3 and o4-mini can Think with Images in their Chain-of-Thoughtnot just see them. Thinking with images [23, 26] is paradigm that outputs images in CoT to help VLMs reason better. Models like Nano-Banana [15] further Core Contributors. Corresponding authors. 1 Figure 1 Vision-centric tasks and text-centric tasks in VideoThinkBench, and Sora-2s Thinking with Video solutions. Vision-centric tasks are solved by reasoning about visual elements via drawing and imagination, including four categories: eyeballing puzzles, visual puzzles, ARC-AGI-2, and mazes. An example is shown for each. Typically, in the ray reflection problem from the eyeballing puzzles, Sora-2 accurately draws the light path and finds the specific point it passes through. Text-centric tasks are solved by text-based reasoning, which are adapted from established benchmarks, and GSM8K example is shown. The model provides written process and the correct answer within the video. demonstrate the capability of generating text embedded within images, bridging textual and visual reasoning. Despite these advances, both Thinking with Text and Think with Images paradigms have inherent limitations. (1) Static constraints: Images capture single moments but cannot represent dynamic processes, temporal changes, or continuous transformations. (2) Modality separation: Current approaches treat text and vision as separate modalities, limiting the potential for unified multimodal understanding and generation. There is lack of unified framework that naturally integrates textual and visual reasoning within coherent temporal structure. Moving beyond the traditional paradigms of Thinking with Text (e.g., Chain-of-Thought [3, 37]) and Thinking with Images, we propose Thinking with Video. It naturally enables human-like dynamic reasoning through video generation, such as drawing and imagination. Video generation models, such as Sora-2, show great promise as unifying, general-purpose foundation models for machine vision. By generating videos in the reasoning chain, models can: (1) Dynamic Reasoning: Visualize dynamic processes (e.g., drawing lines to solve spatial puzzles), represent temporal evolution and continuous transformations. (2) Multimodal fusion: Embed text within video frames for unified multimodal understanding and generation, as shown in Figure 5. Achieve more natural alignment with human cognitive processes that involve imagination and mental simulation. Thus, Thinking with Video is potentially unified multimodal reasoning paradigm. To support this exploration, we developed the Video Thinking Benchmark (VideoThinkBench), which comprises two categories of tasks: (1) vision-centric tasks includes newly created datasets (eyeballing puzzles and mazes) and adapted datasets (visual puzzles and ARC-AGI-2 [5]). The instances of these tasks are created in batches by our program. The video generation results of vision-centric tasks are verifiable, except for visual puzzles. (2) text-centric tasks are mainly adapted from subset of established benchmarks (e.g., MATH [18], 2 Category Task Sora-2 Gemini 2.5 Pro GPT5 high Claude Sonnet 4.5 Eyeballing-Point Eyeballing-Line Eyeballing-Shape Visual-Color Visual-Shape ARC-AGI-2 Average Text-Only Math Text-Only General Knowledge Multimodal Math Multimodal General Knowledge Average Vision-Centric Text-Centric Overall Average 44.7 38.0 34.5 67.0 64.9 1.3 41.7 53.6 63.1 56.3 49.4 55.6 47.3 27.8 21.0 34.5 73.9 92.9 4.9 42.5 94.8 84.5 66.7 83.0 82. 58.4 33.6 24.0 32.5 79.6 97.5 9.9 46.2 97.2 85.2 69.6 80.6 83.2 61.0 36.2 26.3 50.5 85.6 68.6 13.6 46.8 90.0 86.3 65.6 82.3 81. 60.5 Table 1 Summary table of accuracy (%) across all second-level tasks. For Sora-2: Eyeballing Puzzles  (Table 2)  uses Major Frame evaluation; Visual Puzzles  (Table 3)  shows the average of Color-Filling and Shape-Drawing tasks; Text-Centric Reasoning tasks  (Table 5)  use Video evaluation results. MMLU [17], MMMU [41], MathVista [22]). Examples of these task types are illustrated in Fig. 1. We evaluate on VideoThinkBench and compare Sora-2 with SOTA VLMs, such as GPT-5 [24], Claude-Sonnect4.5 [1] and Gemini-2.5-Pro [8], as shown in Table 1. Furthermore, we systematically analyse the source of these abilities. The main findings of this work are as follows: 1. On vision-centric tasks, Sora-2 is generally comparable to SOTA VLMs, and even surpasses VLMs in several tasks, demonstrating strong spatial reasoning and inductive abilities. For example, Sora-2 can paint lines to solve several spatial reasoning tasks, demonstrating the advantage of Thinking with Video. (Section 2.2) 2. On text-centric tasks, we adapt subsets of text and multimodal reasoning benchmarks. For text reasoning, Sora-2 achieves 98.9% accuracy on GSM8K [6], 94.0% on MATH [18], 65.7% on GPQA-diamond [28], and 53.3% on AIME (2024), 76% on MMLU [17], 85.3% on MMLU-Pro [36], 59% on SuperGPQA-easy [13]. For multimodal reasoning, Sora-2 achieves 81.1% on MathVista [22], 62.6% on MathVision [34], 75.5% on MMMU [41] and 90.1% on MMBench [21]. (Section 2.3) 3. Sora-2 is few-shot learner. We evaluate it on ARC-AGI-2, which requires the model to find patterns in given input-output pairs and apply these patterns to new inputs. While SOTA VLMs struggle on ARC-AGI-2 with lower than 6% accuracy, we observe that Sora-2 can often make reasonable predictions but cannot strictly match the dataset annotations. (Section 2.2.3) Further experiments show that Sora-2 performs better when given all examples than given only one example [12]. (Section 3.1.1) 4. Self-consistency can improve Sora-2s performance in the verifiable video generation reasoning task. This reveals an underexplored direction: test time scaling in video generation reasoning tasks [35]. (Section 3.1.2) 5. We systematically analyze the source of these abilities. Sora-2 maintains performance comparable to the original test set on adapted math problems, which reduces the likelihood of test set leakage. (Section 3.2.1) On text-centric tasks, Sora-2 struggles to generate fully correct process. (Section 2.3.1) We find that the source of Sora-2s text-centric reasoning abilities may originate from the prompt rewriter model. (Section 3.2.3) In summary, our findings demonstrate that video generation model is not only general-purpose visual reasoning model, but also holds potential in unifying multimodal understanding and generation, positioning 3 Thinking with Video as unified multimodal reasoning paradigm."
        },
        {
            "title": "2.1 Benchmark Overview",
            "content": "We introduce the Video Thinking Benchmark (VideoThinkBench), comprehensive benchmark designed to evaluate the reasoning capabilities of video generation models through both vision-centric and text-centric reasoning tasks. Introduction of Vision-Centric and Text-Centric Tasks Vision-centric tasks refer to tasks that are solved primarily through reasoning about visual elements via drawing and imagination. Text-centric tasks refer to tasks that are solved primarily through text-based reasoning processes. Overall Composition of Each Task Category For vision-centric tasks, we not only adapted existing tasks (e.g., ARC-AGI-2 [5]) but also designed new tasks, including eyeballing puzzles and mazes (Appendix B.2.1). In general, these vision-centric tasks evaluate both spatial reasoning and inductive reasoning capabilities of the model, as illustrated in Figure 7a. Furthermore, the task samples can be generated in batches via program execution except ARC-AGI-2, and the tasks are verifiable except visual puzzles for video generation model evaluation. For text-centric tasks, as shown in Figure 7b, we selected existing text-only reasoning benchmarks (e.g., MATH-500 [7], MMLU [36]) and multimodal reasoning benchmarks (MathVista [22], MMMU [41]) (Figure 7b), taking into account both math reasoning and general knowledge reasoning assessment. We sampled subset from most of the benchmarks for evaluation cost control. Finally, we adapted the problems for evaluating video generation models, detailed in Section 2.3.1."
        },
        {
            "title": "2.2 Vision-Centric Reasoning",
            "content": "2.2.1 Eyeballing Puzzles Dataset Construction 1 is game that tests players ability to precisely estimate geometric properties. For The eyeballing game example, the midpoint challenge asks player to drag the movable point to the midpoint of two fixed points. As shown in Fig. 2, we manually designed 21 tasks of verifiable eyeballing puzzles to test spatial reasoning ability of Sora-2 and VLMs, each puzzle involving distinct geometry concept. The input of puzzle instance is an image and prompt. Every puzzle is in multiple choices form, and can be automatically generated and evaluated. For the following evaluation, we generated 50 samples for each task, which is 1050 samples in total. Detailed information of 21 puzzles can be found in Section B.2.2. We divide 21 eyeballing tasks into 3 categories: Point Task, Line Task and Shape Task. Accuracy of each category is shown in Table 2. Point Task constructs point, such as incenter of triangle. Line Task constructs line, such as an angle bisector line. Shape Task constructs shape, such as given three vertices, finding the fourth vertex to construct parallelogram. Evaluation Setup For Sora-2, since it generates both video and audio, multiple evaluation methods can be used on it. The evaluation methods of Sora-2 on Eyeballing Puzzles are introduced as follows: Audio Evaluation: The prompt instructs model to speak out the option in phonetic alphabet (Alpha, Bravo, Charlie, Delta and Echo). Audio is extracted from generated video and transcribed using 1https://woodgears.ca/eyeball/ 4 Figure 2 Four Examples of Sora-2 solving our custom benchmark of 21 eyeballing tasks and 1050 samples. Each sample is multiple choice question and includes an input image with text prompt. The benchmark is automatically evaluated and verifiable. See Section 2.2.1 for details and prompts. In the bottom two examples, Sora-2 adds Charlie text on options that are not C. This disparity between modalities is further explored in Section C.2. All prompts can be found in Section B.2.2. whisper-1 model. Then program finds first appearing phonetic alphabet word as the audio option. Finally, Compare the audio option with ground truth. Last Frame Evaluation: The prompt instructs model to draw red dot on correct option. The last frame of generated video is fed to an image evaluator program that calculates average coordination of red pixels. The last frame option is the option nearest to average coordination of red pixels, or none if there are no red pixels found. Finally, Compare the last frame option with ground truth. Major Frame Evaluation: For every 5 frames in the video, one frame is extracted and fed to the image evaluator, getting option of this frame. Major frame option is the majority vote result of all chosen frames. None option is excluded from voting. Finally, Compare the Major frame option with ground truth. For VLMs, they are instructed to include chosen option in its response, and program will extract it. Their prompts are different from Sora-2s, not instructing about drawing. For example, the prompt of Ray Intersection puzzle for VLMs is: Which option is the intersection point of the three lines? Answer an option in A-E. Evaluation Results The correctness rates for Sora-2 and three leading VLMs are presented in Table 2. Our analysis focuses on Sora-2s performance across its output modalities and its comparison against other models. First, leveraging Sora-2s video output proves most effective. The Major Frame Evaluation, which aggregates information across multiple frames, achieves the highest average correctness of 40.2%. This significantly outperforms both the Last Frame Evaluation (33.4%) and the Audio Evaluation (28.0%), demonstrating that the temporal consistency of video provides more robust measure of its reasoning. deeper analysis of Task Circle Center Circumcenter Fermat Point Incenter Midpoint Orthocenter Point Reflection Ray Intersection Triangle Center Average Angle Bisector Arc Connect Circle Tangent Line Circle Tangent Point Parallel Perpendicular Perpendicular Bisector Ray Reflection Average Isosceles Trapezoid Parallelogram Right Triangle Square Outlier Average Overall Average Sora-2 Audio Sora-2 Sora-2 Last Frame Major Frame Gemini 2.5 Pro GPT5 High Claude Sonnet 4. 58.0 14.0 24.0 48.0 22.0 32.0 18.0 22.0 34.0 30.2 28.0 12.0 22.0 18.0 22.0 20.0 22.0 28.0 21.5 36.0 24.0 30.0 54.0 36.0 28.0 56.0 20.0 24.0 30.0 48.0 18.0 22.0 70.0 42.0 36.7 36.0 56.0 20.0 16.0 28.0 38.0 20.0 30.0 30. 42.0 28.0 14.0 44.0 32.0 33.4 Point Tasks 70.0 24.0 30.0 34.0 64.0 26.0 22.0 88.0 44.0 44.7 Line Tasks 38.0 68.0 26.0 24.0 30.0 46.0 40.0 32.0 38. Shape Tasks 36.0 32.0 16.0 54.0 34.5 40.2 44.0 12.0 30.0 32.0 28.0 14.0 30.0 22.0 38.0 27.8 28.0 20.0 22.0 22.0 20.0 8.0 16.0 32.0 21.0 24.0 24.0 38.0 52.0 34. 26.5 62.0 32.0 28.0 30.0 34.0 32.0 28.0 16.0 40.0 33.6 28.0 20.0 20.0 18.0 32.0 26.0 30.0 18.0 24.0 26.0 30.0 20.0 54.0 32.5 29.7 50.0 26.0 34.0 34.0 66.0 28.0 30.0 22.0 36.0 36. 24.0 12.0 22.0 22.0 32.0 14.0 58.0 26.0 26.3 20.0 36.0 60.0 86.0 50.5 35.1 Table 2 Accuracy (%) of Sora-2 using 3 evaluation methods and 3 VLMs on eyeballing tasks. The highest score in each row is highlighted in bold. For Sora-2, answers are derived from its generated audio (transcribed), the final video frame (which option is closest to red dot location), or by majority vote across multiple frames. VLM answers are extracted from their text output. We divide tasks into three categories based on whether it constructs point, line or shape. Details: Section 2.2.1 these output forms is in Section C.2. When compared to other VLMs, Sora-2s video-based performance stands out. Its 40.2% average score surpasses all competitors, including Claude 4.5 (35.1%), GPT5-High (29.7%), and Gemini 2.5 (26.5%). An analysis of individual tasks reveals specialized strengths across models. Sora-2 excels at geometric construction tasks like Ray Intersection (a remarkable 88%) and Circle Center (70%). In contrast, Claude 4.5 leads on different set of tasks, including Square Outlier (86%) and Midpoint (66%). In summary, while Sora-2s dynamic video output gives it the best overall performance, other models exhibit competitive strengths in specific areas of spatial reasoning. Sora-2 generally surpasses SOTA VLMs on eyeballing puzzles. Sora-2 exhibits geometric Takeaway 1 and physical reasoning abilities. It can simulate the extension and reflection of rays and manipulate geometric elements (e.g., points and lines) to support spatial reasoning. 6 Figure 3 Overview of the visual puzzles, categorized into color-fillings tasks and shape-drawing tasks. The tasks are selected and adapted from PuzzleVQA [4] to evaluate inductive reasoning capability. The video generation model need to fill the marked area with the correct color or draw the correct shape. Sora-2 correctly solved the problems above. 2.2.2 Visual Puzzles Dataset Construction To evaluate inductive reasoning capability of video generation models such as Sora-2, we constructed visual puzzle tasks centered on pattern recognition and matching, as illustrated in Figure 3. Specifically, we transformed ten types of puzzles selected from PuzzleVQA [4] to video generation tasks, prompting the model to solve the visual puzzle via the generated video without giving the original multiple-choice options. We also modified the data generation code to produce the solution images along with the puzzles and introduced variations to diversify samples for some tasks. Each task contains around fifty samples. As illustrated in Figure 3, the ten tasks are broadly categorized as follows: Color-filling tasks primarily requires the model to fill the marked area with the correct color (Tasks 1-6). Shape-drawing tasks primarily requires the model to draw the correct shape of the correct size in the marked area (Tasks 7-10). 7 Task Sora-2 Gemini 2.5 Pro Hexagon Color Pattern Match. Grid Color Pattern Match. Shape Color Pattern Match. Rectangle Height Color Match. Color Gradient Perception & Application* Color Mixing Perception & Application Average Grid Size Pattern Match.* Cycle Size Pattern Match.* Gird Shape & Size Pattern Match.* Reflection Recognition & Application* Average Color-Filling Task 96.0 94.0 66.0 44.0 45.8 56.0 67.0 Shape-Drawing Task 85.4 58.0 64.0 52.0 64.9 Overall Average 66.2 98.0 94.0 54.0 58.0 83.3 56.0 73.9 87.5 84.0 100.0 100.0 92.9 81.5 GPT5 high 100.0 100.0 82.0 60.0 35.4 100.0 79. 95.8 98.0 98.0 98.0 97.5 86.8 Claude Sonnet 4.5 92.0 100.0 88.0 54.0 93.8 86.0 85.6 62.5 46.0 100.0 66.0 68.6 78. Table 3 Accuracy (%) on the visual puzzle tasks. * represents that multiple-choice options are provided for the VLMs due to evaluation need, as detailed in Appendix B.3.1. Sora-2 is not provided with multiple-choice options across all 10 tasks. Evaluation Setup Definition of Deviation Value We define metric, denoted as Diff, to quantify the deviation between generated video frame and the solution image. It sums up the pixel-wise differences within the puzzle area. For color-filling tasks, the difference is based on color distance in RGB space; for shape-drawing tasks, it measures the pixel-wise mismatch. The formal definitions are provided in Appendix B.3.1. Best Frame Selection and Evaluation We selected the best frame, namely the frame with the lowest Diff value. Sora-2 might generate unrelated content after task completion, or the target color/shape might appear gradually. So using the best frame helps determine the answer given in the video. We manually evaluated Sora-2s performance based on the best frames. For the VLMs, we provided them with multiple-choices in certain tasks and employed rule-based evaluation, as detailed in Appendix B.3.1 and Appendix B.3.1. Evaluation Results Sora-2 demonstrates certain level of inductive reasoning capability. As shown in Table 3, Sora-2 performs strongly on certain tasks, such as Hexagon Color Pattern Matching (96%) in color-filling tasks and Grid Size Pattern Matching (85.4%) in shape-drawing tasks. The task examples are illustrated in Figure 3. These demonstrate Sora-2s capacity to identify the patterns to solve the puzzles. Sora-2s performance is relatively close to certain SOTA VLMs. Overall, Sora-2 does not perform as well as the three SOTA VLMs. However, the gap is not large in certain cases. For instance, on color-filling tasks, Sora-2 scores 67.0% on average, while Gemini 2.5 Pro scores 73.9%. Notably, Sora-2 averages 64.9% on shape-drawing tasks, close to 68.6% that Claude 4.5 Sonnet achieves, which is provided with multiple-choice options, illustrated in Figure 10. These indicate Sora-2 holds potential in inductive reasoning. Sora-2s performance is comparable to that of Claude Sonnet 4.5 on Shape-Drawing puzzles, Takeaway 2 demonstrating certain level of inductive reasoning capability. Moreover, Sora-2 demonstrates the ability to recognize and apply patterns of color, shape and size. Sora-2 can solve visual puzzles patterns including symmetry, gradients, and compositionality. 8 Figure 4 Examples of Sora-2 trying to solve ARC-AGI-2. ARC-AGI-2 is benchmark targeting few-shot, inductive reasoning over abstract pattern transformations. Sora-2 is expected to deduct the transform rule from examples and use the rule to generate the output grid of the test case. Besides automatic evaluation, we manually analyzed 100 cases and divide them into 4 categories based on completion level. Prompt: Each row contains input and output grids. Learn the pattern and generate the output grid for the last input while keeping existing patterns without modification. Static camera perspective, no zoom or pan. In portrait. For the generated video and ground truth, only test case area is displayed. Details: Section 2.2.3 2.2.3 ARC-AGI-2 Dataset Construction ARC-AGI-2 [5] is benchmark targeting few-shot, inductive reasoning over abstract pattern transformations. Each piece of data includes several input grids and corresponding output grids, with consistent rule of transformation between each input and output. We use the 1000-sample ARC-AGI-2 train set. The original data is in json format, so we wrote program to display each sample and pad the image to the same aspect ratio as Sora-2, while leaving last output grid as empty for Sora-2 to put its answer. We evaluate SOTA VLMs using the same image input and text instruction. We then compare their output color matrix (in text form) with the ground truth matrix. Evaluation Setup program automatically extracts last frame of video generated by Sora-2, zoom it into the size of input image, and calculate the average color of each cell. ARC-AGI-2 has fixed 10 color palette, so for each cell, we check if the nearest color in the palette matches ground truth color. The answer is correct only if all cells are correct. Besides automatic evaluation, we manually analyze randomly chosen 100 cases, and divide them into 4 categories, for more elaborate evaluation: 9 Task Sora-2 Gemini GPT5 high 2.5 Pro Claude Sonnet 4.5 ARC-AGI-2 1.3 1.9 0.5 5. Table 4 Accuracy (%) on the ARC-AGI-2 task. VLMs data are from the official ARC-AGI-2 leaderboard. Fully Correct: All cells in the output grid are correct. Mostly Correct: The output grid shows understanding of the transformation rule, but has flaws on the details. Partially Correct: The output grid shares some characteristics with ground truth, but not enough to show Sora-2 understands the transformation rule. Wrong: The output grid is not related to the answer at all. Did Nothing is subcategory that means the video does not modify the output area. Evaluation Results Sora-2 scores 1.3%, which is on par with strong LLMs like Gemini 2.5 Pro and GPT5 high. This indicates Sora-2 has developed elementary abstract reasoning. The Fully Correct case in Fig. 4 further suggests capacity of self-correction, rather than single-shot pattern guess. In the first middle frame, three green pixels in the bottom right forming shape are misplaced 1 pixel upward. This mistake is corrected in the next frame, leading to the correct final answer. The manual analyze result is shown in Table 10. While the Fully Correct rate of 3% is low, the 14 Mostly Correct outputs indicate that the model can sometimes identify the core transformation rule has problems with execution. significant portion of failures (42 out of 55) were instances where the model Did Nothing, that is not changing the output area during the video. proportion of them change the test case input area or few shot examples area instead, showing struggles with understanding the instruction. In-context learning (ICL) is paradigm for natural language processing (NLP), where Takeaway 3 LLMs make predictions based on contexts augmented with few examples. We find Sora-2 is few-shot learner. We evaluate it on ARC-AGI-2, which requires the model to find patterns in given input-output pairs and apply these patterns to new inputs. While SOTA VLMs struggle on ARC-AGI-2 and achieve less than 5% accuracy, we observe that Sora-2 can often make reasonable predictions, although they do not strictly match the dataset annotations."
        },
        {
            "title": "2.3 Text-Centric Reasoning Tasks",
            "content": "2.3.1 Dataset Construction Selected Benchmarks Text-Only Reasoning Benchmarks We evaluate math reasoning abilities using problems from three representative math reasoning benchmarks of increasing difficulty: GSM8K [6], MATH-500 [7] and AIME of year 2024 (AIME24) and 2025 (AIME25). For general knowledge reasoning evaluation, we include problems from BBH [30], MMLU [17], MMLU-Pro [36], GPQA-diamond [28] and SuperGPQA-easy [13]. Multimodal Reasoning Benchmarks We include problems from MathVista [22] and MathVision [34] to evaluate multimodal math reasoning abilities, and use those from MMBench [21] and MMMU [41] which assess general visual understanding and multi-discipline multimodal reasoning, respectively. Using Subsets Sampled from the Benchmarks For evaluation cost control, we sampled subset for most of the benchmarks instead of using the full test set, with detailed statistics shown in Appendix A. 10 Figure 5 Input form and evaluation of text-centric tasks. The model accepts text prompt and reference image. The prompt contains the problem text and the reference image displays the entire problem. The model shows the textual solution process and the answer in the video, speaking the answer in the audio. We evaluate the answers from the video and audio independently. The last frame is extracted for video evaluation and the audio is transcribed for audio evaluation. For evaluation, we adopt an LLM-as-a-Judge approach, detailed in Section 2.3.2, and human Alignment check is shown in B.4.2. Input Form and Instruction We use both text prompt and reference image for text-centric tasks. As shown in Figure 5, text prompt contains the full problem text for text-only problems or the textual component for multimodal problems. The reference image displays the entire problem on white background. The problem text is in printed form. For multimodal problems, the original image is embedded. The model provides written solution process and the answer within the video. In the audio track, it should only state the final answer without any explanation, to avoid truncation due to video length constraints. 2.3.2 Evaluation Setup Video and Audio Evaluation We evaluate the answers from the video and audio independently. The last frame of each video, which should contain the written answer, is used for video evaluation. The audio is transcribed into text for audio evaluation. We report accuracies for video and audio, along with the rate at which answers are correct in both modalities (denoted A) and in at least one modality (denoted A). LLM as Judge We adopt an LLM-as-a-Judge [46] approach for both video and audio evaluation, using GPT-4o [23] as the judge model. The model is provided with the last frame image (for video) or the transcribed text (for audio), with the evaluation prompts shown in Appendix B.4.1. Human alignment check shown in B.4.2 validates the evaluation. 2.3.3 Evaluation Results The evaluation results for text-only and multimodal reasoning problems in text-centric tasks are presented in Table 5. The following observations can be drawn from these results. Notably, Sora-2 shows unexpected performance on text-centric tasks. As shown in Table 5, in terms of audio accuracy, Sora-2 achieves performance comparable to SOTA VLMs on several datasets across both text-only 11 Dataset GSM8K MATH-500 AIME24 AIME25 Average BBH MMLU MMLU-Pro GPQA SuperGPQA Average MathVista MathVision Average MMBench MMMU Average Overall Average Sora-2 Last Frame Sora-2 Audio Sora-2 Sora-2 Gemini 2.5 Pro GPT5 high Claude Sonnet 4.5 75.7 67.0 38.3 33.3 53.6 69.8 69.1 72.0 51.5 53.2 63.1 67.6 44.9 56.3 60.4 38.3 49. 56.3 98.9 92.0 46.7 36.7 68.6 Text-Only Math Reasoning 98.9 94.0 53.3 43.3 72.2 75.7 65.0 31.7 26.7 49.8 Text-Only General Knowledge Reasoning Multimodal Math Reasoning 80.6 67.3 76.5 57.6 44.5 65.3 75.7 46.7 61.2 89.0 69.2 79.1 63.0 47.4 63.2 43.4 35.5 50.5 62.2 29.0 45.6 59.3 31.9 45. 87.4 76.4 85.3 65.7 62.3 75.4 81.1 62.6 71.9 90.1 75.5 82.8 Multimodal General Knowledge Reasoning 73.6 48. 78.0 98.9 99.0 93.3 88.0* 94.8 90.0 87.7 87.1 86.4* 71.1 84.5 70.0 63.3 66.7 86.9 79.0 83.0 83. 100.0 99.0 95.0 94.6* 97.2 94.6 86.0 91.4 85.7* 68.3 85.2 67.5 71.6 69.6 84.2 77.0 80.6 84.0 100.0 98.0 75.0 87.0* 90. 93.8 89.5 95.7 83.4* 69.0 86.3 72.5 58.7 65.6 82.5 82.0 82.3 81.2 Table 5 Accuracy (%) on subsets of text-only and multimodal reasoning benchmarks used for the text-centric tasks. denotes the rate at which answers are correct in both video and audio. denotes the rate at which answers are correct in at least one of video and audio. represents that our test results are Avg@4. * represents the officially released results. Sora-2 overall shows impressive reasoning capabilities, achieving performance comparable to SOTA VLMs on GSM8K, MATH-500, MathVista, and MMBench in terms of audio accuracy, though noticeably lagging behind on more challenging datasets like AIME, GPQA, and MMMU. and multimodal reasoning ones, such as GSM8K, MATH-500, and MathVista. Although noticeable gap remains between Sora-2 and SOTA VLMs on more challenging datasets like AIME, GPQA, and MMMU, Sora-2 overall demonstrates potential in text-centric reasoning tasks. Sora-2s audio accuracy is higher than video accuracy. Sora-2 achieves consistently higher accuracies in audio than in video across most datasets, spanning text-only and multimodal reasoning problems. This may be attributed to Sora-2s difficulty in generating accurate written content, which is analyzed in Section 3.2.2. Takeaway 4 On text-centric tasks, Sora-2 shows surprising performance on text and multimodal reasoning tasks. The video generation model is able to embed text within video frames, enabling unified multimodal understanding and generation. Thus, Thinking with Video is potentially unified multimodal reasoning paradigm."
        },
        {
            "title": "3.1 Analysis Experiment of Vision-centric Reasoning Tasks",
            "content": "3.1.1 Sora-2 is Few-Shot Learner Each sample in ARC-AGI-2 has multiple demonstration examples. To test Sora-2s few-shot learning capabilities, we conducted an experiment where we provided only single example (1-shot) instead of all 12 Evaluation Method Single Try Vote (5 Tries) Accuracy Range Few-Shot 1-Shot Audio Option Last Frame Option Major Frame Option 12% 56% 68% 12% 66% 90% 0.000.35 0.350.65 0.651. 743 127 130 788 117 95 Table 6 Sora-2 performance on the Arc Connect Puzzle by output modality. Vote Accuracy (5 Tries) means for each puzzle, we let Sora-2 generate 5 videos and choose the most common option as result. Details in Section C.2 Table 7 Sora-2s few-shot learning ability, shown by aggregated counts of ARC-AGI-2 samples across pixel accuracy ranges. Few-shot uses all ARC-AGI-2 examples, while 1-shot uses only the first. Details in Section 3.1.1. available examples, and retested Sora-2 on the 1000 training samples. Since achieving perfectly correct grid is difficult, we measure performance using \"pixel accuracy\": the percentage of pixels in the output area that match the ground truth. The results, comparing performance with all examples versus just one, are presented in Table 7. The results show clear trend. Comparing to using all examples, 1-shot yields more low-accuracy (0 to 0.35) samples and less high-accuracy (0.65 to 1.0) samples,. This demonstrates that while Sora-2 has some 1-shot ability, its performance on this abstract reasoning task benefits from seeing multiple examples, confirming it as few-shot learner. Sora-2 can achieve better in-context learning by providing more examples. Experiments Takeaway 5 show that Sora-2 performs better when provided with all examples, compared to only one example. This is an underexplored direction for analyzing and improving the in-context learning abilities of video generation models. 3.1.2 Self-Consistency Improves Video Generation Reasoning Self-consistency relies on key intuition. It is the idea that for complex reasoning problem, there are multiple valid ways to think about it. All these different paths can ultimately lead to the same unique correct answer [35]. The results in Table 6 demonstrate principle similar to self-consistency in large language models. This principle is evident when comparing the Last Frame Option with the Major Frame Option. The accuracy improves from 56% for the single Last Frame Option analysis to 68% for the time-averaged Major Frame Option. This initial improvement suggests that analyzing Sora-2s output over time provides more robust measure of its intent. Upon inspection, we find that the end of generated videos sometimes flash to SMPTE color bars or pure black screen, leading to failure of last frame image. By sampling across the videos duration, the Major Frame Option method effectively acts as denoising filter, capturing the models most consistent and stable belief about the correct answer. This effect is magnified when considering the majority vote over five retries. While the Last Frame Option accuracy improves from 56% to 66% with voting, the Major Frame Option accuracy leaps from 68% to an impressive 90%. This demonstrates that the temporal consistency within single video is strong proxy for the models confidence. When this temporally consistent signal is further aggregated across multiple independent generation attempts, the result is highly reliable extraction of the models core reasoning capability. The model, when it understands the problem, will consistently generate videos where the indicator remains on the correct option over time and across multiple attempts. This temporal self-consistency provides much more accurate assessment of the models problem-solving abilities than an evaluation based on single, static final output. Dataset Last Frame Audio GSM8K GSM8K (Derived) MATH-500 MATH-500 (Derived) 75.7 78. 67.0 75.0 98.9 100.0 92.0 91.0 75.7 78.4 65.0 71.0 98.9 100. 94.0 95.0 Table 8 Sora-2s accuracy on original and derived math reasoning problems with different numerical values. denotes the rate at which answers are correct in both video and audio. denotes the rate at which answers are correct in at least one of video and audio. Performance remains consistent, thus excluding the risk of test data leakage and indicating Sora-2s inherent potential in text-centric reasoning tasks. Test time scaling methods, represented by Self-consistency, have achieved great success in Takeaway 6 text reasoning tasks. We find that self-consistency can improve Sora-2s performance in the verifiable video generation reasoning task. This reveals an underexplored direction: test time scaling in video generation reasoning tasks."
        },
        {
            "title": "3.2 Analysis Experiment of Text-Centric Reasoning Tasks",
            "content": "3.2.1 Test Set Leakage Analysis To investigate potential test data leakage as the reason for Sora-2s strong performance in text-centric reasoning, we further create new evaluation problems from the math reasoning test data. Specifically, we used Qwen3-235B-A22B-Thinking-2507 [39] and Gemini 2.5 Pro [8] to generate similar problems for problems of our GSM8K and MATH-500 test data, respectively, with the prompts shown in Appendix C.1. Each newly generated problem shares the same the underlying problem-solving structure as its original counterpart but features different numerical values and possibly different contextual details. We evaluated Sora-2 on these derived problems and found no significant performance difference compared to the original problems, as shown in Table 8. This consistency suggests that Sora-2s performance stems from its inherent text-centric reasoning capabilities rather than test data leakage. 3.2.2 Analysis of Sora-2s Reasoning Process To better understand Sora-2s text-centric reasoning, we sampled 115 cases from the problems that Sora-2 answered correctly via both video and audio across the text-centric tasks, and manually analyzed the generated reasoning processes. We categorized each case into five categories: (1) Completely Correct, (2) Logic Correct with Writing Errors, (3) Unreadable or Incorrect Logic, (4) Missing Solution Process, and (5) Process Unnecessary. Detailed definitions and examples are provided in Appendix C.4. Our analysis reveals that Sora-2 struggles to generate coherent reasoning processes in the video, even when providing correct final answer. As shown in Figure 6, only 13.91% of solutions are fully correct. The majority of solutions (43.48%) are unreadable or logically flawed, with another 29.57% containing minor presentation errors. This suggests that Sora-2 has major difficulties in delivering clear and correct reasoning process via video generation. 3.2.3 Source of Text-Centric Reasoning Ability Despite Sora-2s strong text-centric reasoning (Section 2.3.3), its video generation pipeline might contain prompt rewriter model that solves the problems for the video generation component. Since Sora-2 lacks controllable prompt rewriting for users, we used Wan2.5 [33], which provides parameter (prompt_extend) for enabling/disabling its rewriting behavior. We then evaluated Wan2.5 (wan2.5-i2v-preview) under both settings on the sampled subsets of GSM8K, MMLU and MMMU, using generally the same setup as for Sora-2. 14 Dataset Prompt Rewrite Last Frame Audio GSM8K MMLU MMMU 0.0 78. 0.0 74.1 2.0 47.0 0.0 31.9 0.0 50.00 0.0 14.0 0.0 29. 0.0 50.00 0.0 11.0 0.0 81.6 0.0 74.1 2.0 50.0 Table 9 Wan2.5s performance on text-centric tasks with and without prompt rewriting. Its reasoning ability almost vanishes when the prompt rewriter model is disabled, indicating that this component solves the reasoning problems for the video generation component. This suggests the strong text-centric reasoning capability observed in Sora-2 may also stem from prompt rewriter model. Figure 6 Distribution of reasoning process categories for correct answers. The prevalence of Unreadable or Incorrect Logic (43.48%) highlights Sora-2s difficulty in generating coherent reasoning. As shown in Table 9, Wan2.5 achieves nearly zero accuracy across all three text-centric benchmarks when its prompt rewriter is disabled. In contrast, enabling prompt rewriting leads to substantial performance gain. This significant gap demonstrates that Wan2.5s text-centric reasoning ability is almost entirely from its prompt rewriter model. concrete example illustrating how prompt rewriting transforms reasoning problem into explicit visual instructions is provided in Appendix C.3. This suggests that the strong text-centric reasoning capability observed in Sora-2 may also stem from an internal prompt rewriting mechanism, possibly using VLM. Sora-2 has achieved impressive results on Text-Centric Reasoning Tasks. We conducted Takeaway 7 experiments to analyze the source of Sora-2s capabilities. Sora-2 maintains performance comparable to the original test set on adapted math problems, which reduces the likelihood of test set leakage. We also analyzed Sora-2s reasoning process and found that it struggles to generate coherent reasoning steps in the video, even when providing the correct final answer. Finally, through comparative experiments with Wan 2.5, we speculate that Sora-2s Text-Centric Reasoning ability originates from its prompt rewriter."
        },
        {
            "title": "4 Related Work",
            "content": "Video Generation Model: The field of video generation is advancing incredibly fast. Early models like OpenAIs Sora was the GPT-1 moment [27] for video, and now newer versions like Sora-2 have made huge leap forward. Sora-2 can create more realistic and controllable videos that are physically accurate and even include synchronized dialogue and sound effects. Besides Sora, other powerful but closed-source models are pushing the industry forward. Companies like Runway, with its Gen-3 model [29], Pika Labs, Luma AI, and Google DeepMinds Veo [14] series are all creating impressive, high-quality videos. However, because these models are proprietary, they are not widely available for researchers to study and build upon. To counter this, movement of open-source alternatives is growing. Projects like Stable Video Diffusion [2], Hunyan-Video [19], and the Wan series [32] are making video generation technology accessible to everyone. Reasoning Paradigm Transfer: Chain-of-thought (CoT) significantly improves the reasoning ability of large language models (LLMs) [10, 35, 37, 40]. Large-scale reinforcement learning incentivizes LLMs to think productively using their CoT [10, 25, 45]. o3 and o4-mini further extend this capability by natively Thinking with Images in their CoT, which involves directly cropping, zooming, and rotating images [26]. Thinking with Images [20, 23, 26, 44] is paradigm that outputs images in CoT to help VLMs reason better, largely 15 improving the VLMs reasoning abilities [31]. Recently, unified multimodal understanding and generating models have appeared [9, 11, 42, 43]. They potentially achieve Thinking with Images through text and image interleaved reasoning. Evaluation of Video Generation Reasoning: Video Generation Reasoning is direction rarely explored [16, 38]. Wiedemer et al. shows that Veo 3 can solve many tasks it wasnt specifically trained for. These abilities include perceiving, modeling, and manipulating the visual world. They allow for video generation reasoning. They test tasks like maze and symmetry solving. However, these work [16, 38] has several limitations : (1) Lack of Systematic Evaluation: The evaluation methodology relies heavily on manual qualitative assessment and lacks systematically constructed dataset with ground truth for evaluation. (2) These work does not provide systematic comparison with Vision-Language Models (VLMs). (3) Limited to Vision-Centric Tasks: The tasks tested are exclusively focused on vision-centric reasoning. Our work addresses these limitations with the following contributions: (1) Systematic Dataset Construction: We have systematically constructed dataset where test cases can be generated in batches using program. Most of the vision-centric tasks we have designed are verifiable. (2) Systematic Comparison with VLMs: We have conducted systematic comparison with VLMs and analyzed the advantages of video models. (3) Evaluation of Text-Centric Tasks: We have systematically evaluated and analysis the performance of Sora-2 on text-centric tasks. In summary, we propose thinking with video as new paradigm that has the potential to unify multimodal reasoning. Furthermore, we find that video model can be enhanced through few-shot learning and self-consistency."
        },
        {
            "title": "5 Conclusion",
            "content": "In this paper, we introduce reasoning paradigm termed Thinking with Video. We evaluated Sora-2 on the newly constructed VideoThinkBench. Our analysis shows that the video generation model is inherently suitable for human-like reasoning through drawing and imagination. Furthermore, it also demonstrates the potential to perform textual reasoning through video frames. Thus, Thinking with Video is potentially unified multimodal reasoning paradigm."
        },
        {
            "title": "6 Limitations and Future Work",
            "content": "We primarily evaluate Sora-2s reasoning abilities among video generation models. Sora-2 is not open-source, limiting the analysis of its internal mechanisms. For future evaluation work, we plan to include more video generation models, especially open-source models. This allows for deeper analysis of their internal mechanisms. Meanwhile, there are other capabilities of video models worth exploring. To enhance the reasoning abilities of video models through training, promising direction is to scale up the verifiable tasks in VideoThinkBench via Reinforcement Learning with Verifiable Rewards (RLVR), thereby enhancing models \"Thinking with Video\" capabilities. Regarding unified multimodal training for video models, we will explore converting textual corpora into videoform training data (e.g., by generating the next word frame-by-frame to simulate whiteboard handwriting). The idea is that by pretraining video generation models on such text-generation tasks, they can acquire textual world knowledge. Ultimately, with large-scale image-text data training, these models might achieve unified multimodal understanding and generation."
        },
        {
            "title": "References",
            "content": "[1] Anthropic. System card: Claude opus 4 & claude sonnet 4. Technical report, Anthropic, May 2025. URL https://www-cdn.anthropic.com/4263b940cabb546aa0e3283f35b686f4f3b2ff47.pdf. [2] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, Varun Jampani, and Robin Rombach. Stable video diffusion: Scaling latent video diffusion models to large datasets, 2023. URL https://arxiv.org/abs/2311.15127. [3] Qiguang Chen, Libo Qin, Jinhao Liu, Dengyun Peng, Jiannan Guan, Peng Wang, Mengkang Hu, Yuhang Zhou, Te Gao, and Wanxiang Che. Towards reasoning era: survey of long chain-of-thought for reasoning large language models, 2025. URL https://arxiv.org/abs/2503.09567. [4] Yew Ken Chia, Vernon Toh Yan Han, Deepanway Ghosal, Lidong Bing, and Soujanya Poria. Puzzlevqa: Diagnosing multimodal reasoning challenges of language models with abstract visual patterns. arXiv preprint arXiv:2403.13315, 2024. [5] Francois Chollet, Mike Knoop, Gregory Kamradt, Bryan Landers, and Henry Pinkard. Arc-agi-2: new challenge for frontier ai reasoning systems, 2025. URL https://arxiv.org/abs/2505.11831. [6] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. [7] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. [8] Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. URL https://arxiv.org/abs/2507.06261. [9] Yufeng Cui, Honghao Chen, Haoge Deng, Xu Huang, Xinghang Li, Jirong Liu, Yang Liu, Zhuoyan Luo, Jinsheng Wang, Wenxuan Wang, Yueze Wang, Chengyuan Wang, Fan Zhang, Yingli Zhao, Ting Pan, Xianduo Li, Zecheng Hao, Wenxuan Ma, Zhuo Chen, Yulong Ao, Tiejun Huang, Zhongyuan Wang, and Xinlong Wang. Emu3.5: Native multimodal models are world learners, 2025. URL https://arxiv.org/abs/2510.26583. [10] DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanhong Xu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zia Zhu, Zun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng 17 Xu, Zhongyu Zhang, and Zhen Zhang. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. URL https://arxiv.org/abs/2501.12948. [11] Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Song, Guang Shi, and Haoqi Fan. Emerging properties in unified multimodal pretraining, 2025. URL https://arxiv.org/abs/2505.14683. [12] Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, Lei Li, and Zhifang Sui. survey on in-context learning. arXiv preprint arXiv:2301.00234, 2024. Updated version v6, October 2024. [13] Xinrun Du, Yifan Yao, Kaing Ma, Bingli Wang, Tianyu Zheng, King Zhu, Minghao Liu, Yiming Liang, Xiaolong Jin, Zhenlin Wei, et al. Supergpqa: Scaling llm evaluation across 285 graduate disciplines. arXiv preprint arXiv:2502.14739, 2025. [14] Google. Veo 3. https://aistudio.google.com/models/veo-3, 2025. Accessed on November 7, 2025. [15] Google DeepMind. Gemini 2.5 flash & 2.5 flash image model card. Technical report, Google DeepMind, August 2025. URL https://storage.googleapis.com/deepmind-media/Model-Cards/Gemini-2-5-Flash-Model-Card. pdf. Last updated: August 27, 2025. [16] Ziyu Guo, Xinyan Chen, Renrui Zhang, Ruichuan An, Yu Qi, Dongzhi Jiang, Xiangtai Li, Manyuan Zhang, Hongsheng Li, and Pheng-Ann Heng. Are video models ready as zero-shot reasoners? an empirical study with the mme-cof benchmark, 2025. URL https://arxiv.org/abs/2510.26802. [17] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020. [18] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset, 2021. URL https://arxiv.org/abs/ 2103.03874. [19] Weie Kong, Qi Tian, Zian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, Kathrina Wu, Qin Lin, Junkun Yuan, Yanxin Long, Aladdin Wang, Andong Wang, Changlin Li, Duojun Huang, Fang Yang, Hao Tan, Hongmei Wang, Jacob Song, Jiawang Bai, Jianbing Wu, Jinbao Xue, Joey Wang, Kai Wang, Mengyang Liu, Pengyu Li, Shuai Li, Weiyan Wang, Wenqing Yu, Xinchi Deng, Yang Li, Yi Chen, Yutao Cui, Yuanbo Peng, Zhentao Yu, Zhiyu He, Zhiyong Xu, Zixiang Zhou, Zunnan Xu, Yangyu Tao, Qinglin Lu, Songtao Liu, Dax Zhou, Hongfa Wang, Yong Yang, Di Wang, Yuhong Liu, Jie Jiang, and Caesar Zhong. Hunyuanvideo: systematic framework for large video generative models, 2025. URL https://arxiv.org/abs/2412.03603. [20] Zhiyuan Li, Dongnan Liu, Chaoyi Zhang, Heng Wang, Tengfei Xue, and Weidong Cai. Enhancing advanced visual reasoning ability of large language models, 2024. URL https://arxiv.org/abs/2409.13980. [21] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? In European conference on computer vision, pages 216233. Springer, 2024. [22] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255, 2023. [23] OpenAI. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. [24] OpenAI. Gpt-5 system card. Technical report, OpenAI, August 2025. URL https://cdn.openai.com/pdf/ 8124a3ce-ab78-4f06-96eb-49ea29ffb52f/gpt5-system-card-aug7.pdf. [25] OpenAI. Learning to reason with llms, learning-to-reason-with-llms/. Accessed: 2025. 2025. URL https://openai.com/zh-Hans-CN/index/ [26] OpenAI. OpenAI o3 and o4-mini System Card. Technical report, OpenAI, April 2025. URL https://cdn.openai. com/pdf/2221c875-02dc-4789-800b-e7758f3722c1/o3-and-o4-mini-system-card.pdf. Accessed: 2025-11-01. [27] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training. 2018. 18 [28] David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel Bowman. Gpqa: graduate-level google-proof q&a benchmark. In First Conference on Language Modeling, 2024. [29] Runway Research. Introducing Gen-3 Alpha: New Frontier for Video Generation. https://runwayml.com/ research/introducing-gen-3-alpha, June 2024. Accessed on November 7, 2025. [30] Mirac Suzgun, Nathan Scales, Nathanael Schrli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V. Le, Ed H. Chi, Denny Zhou, and Jason Wei. Challenging big-bench tasks and whether chain-of-thought can solve them, 2022. URL https://arxiv.org/abs/2210.09261. [31] Jingqi Tong, Jixin Tang, Hangcheng Li, Yurong Mou, Ming Zhang, Jun Zhao, Yanbo Wen, Fan Song, Jiahao Zhan, Yuyang Lu, Chaoran Tao, Zhiyuan Guo, Jizhou Yu, Tianhao Cheng, Zhiheng Xi, Changhao Jiang, Zhangyue Yin, Yining Zheng, Weifeng Ge, Guanhua Chen, Tao Gui, Xipeng Qiu, Qi Zhang, and Xuanjing Huang. Game-rl: Synthesizing multimodal verifiable game data to boost vlms general reasoning, 2025. URL https://arxiv.org/abs/2505.13886. [32] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, Jianyuan Zeng, Jiayu Wang, Jingfeng Zhang, Jingren Zhou, Jinkai Wang, Jixuan Chen, Kai Zhu, Kang Zhao, Keyu Yan, Lianghua Huang, Mengyang Feng, Ningyi Zhang, Pandeng Li, Pingyu Wu, Ruihang Chu, Ruili Feng, Shiwei Zhang, Siyang Sun, Tao Fang, Tianxing Wang, Tianyi Gui, Tingyu Weng, Tong Shen, Wei Lin, Wei Wang, Wei Wang, Wenmeng Zhou, Wente Wang, Wenting Shen, Wenyuan Yu, Xianzhong Shi, Xiaoming Huang, Xin Xu, Yan Kou, Yangyu Lv, Yifei Li, Ying Liu, Yiming Wang, Yingya Zhang, Yitong Huang, Yong Li, You Wu, Yu Liu, Yulin Pan, Yun Zheng, Yuntao Hong, Yupeng Shi, Yutong Feng, Zeyinzi Jiang, Zhen Han, Zhi-Fan Wu, and Ziyu Liu. Wan: Open and advanced large-scale video generative models, 2025. URL https://arxiv.org/abs/2503.20314. [33] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. [34] Ke Wang, Junting Pan, Weikang Shi, Zimu Lu, Houxing Ren, Aojun Zhou, Mingjie Zhan, and Hongsheng Li. Measuring multimodal mathematical reasoning with math-vision dataset. Advances in Neural Information Processing Systems, 37:9509595169, 2024. [35] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models, 2023. URL https: //arxiv.org/abs/2203.11171. [36] Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, et al. Mmlu-pro: more robust and challenging multi-task language understanding benchmark. Advances in Neural Information Processing Systems, 37:9526695290, 2024. [37] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. [38] Thaddus Wiedemer, Yuxuan Li, Paul Vicol, Shixiang Shane Gu, Nick Matarese, Kevin Swersky, Been Kim, Priyank Jaini, and Robert Geirhos. Video models are zero-shot learners and reasoners. arXiv preprint arXiv:2509.20328, 2025. [39] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. [40] Edward Yeo, Yuxuan Tong, Morry Niu, Graham Neubig, and Xiang Yue. Demystifying long chain-of-thought reasoning in llms, 2025. URL https://arxiv.org/abs/2502.03373. [41] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 95569567, 2024. [42] Congzhi Zhang, Zhibin Wang, Yinchao Ma, Jiawei Peng, Yihan Wang, Qiang Zhou, Jun Song, and Bo Zheng. Rewatch-r1: Boosting complex video reasoning in large vision-language models through agentic data synthesis, 2025. URL https://arxiv.org/abs/2509.23652. 19 [43] Yongheng Zhang, Xu Liu, Ruihan Tao, Qiguang Chen, Hao Fei, Wanxiang Che, and Libo Qin. Vitcot: Video-text interleaved chain-of-thought for boosting video understanding in large language models, 2025. URL https: //arxiv.org/abs/2507.09876. [44] Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, George Karypis, and Alex Smola. Multimodal chain-of-thought reasoning in language models, 2024. URL https://arxiv.org/abs/2302.00923. [45] Jun Zhao, Jingqi Tong, Yurong Mou, Ming Zhang, Qi Zhang, and Xuanjing Huang. Exploring the compositional deficiency of large language models in mathematical reasoning. arXiv preprint arXiv:2405.06680, 2024. [46] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in neural information processing systems, 36:4659546623, 2023."
        },
        {
            "title": "Appendix Contents",
            "content": "A VideoThinkBench Sample Distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 Detailed Evaluation Protocols . B.1 Generation Parameters . B.2 Spatial Reasoning Tasks B.2.1 Mazes . . . . . . . . . . . . . B.2.2 Eyeballing Puzzles . B.3 Inductive Reasoning Tasks . B.3.1 Visual Puzzles . B.4 Text-Centric Tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.4.1 Prompts for Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.4.2 Human Alignment Check for Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . Supplementary Analysis and Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.1 Data Leakage Analysis . . . C.2 Output Modality Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.3 Prompt Rewriting in Wan2.5: Case Study . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.3.1 Example: GSM8K Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.3.2 Visual Comparison . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.4 Reasoning Process Categorization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.4.1 Category Definitions . C.4.2 Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.5 Manual Evaluation of ARC-AGI-2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 23 23 24 28 28 31 31 33 33 33 34 34 35 35 35 35 21 (a) Vision-centric reasoning tasks (b) Text-centric reasoning tasks Figure 7 Task composition and distribution of Video Thinking Benchmark (VideoThinkBench). (a) Vision-centric tasks contain tasks that we design (e.g., Eyeballing Puzzles) and tasks adapted from existing benchmarks (e.g., ARC-AGI-2), evaluating spatial and inductive reasoning. (b) Text-centric tasks consists of subsets sampled from text-only and multimodal reasoning benchmarks, adapted for video generation reasoning. The former contains math reasoning (Text. Math) and general knowledge reasoning (Text. General) benchmarks and the latter also contains math reasoning (MM. Math) and general knowledge reasoning (MM. General) benchmarks."
        },
        {
            "title": "A VideoThinkBench Sample Distribution",
            "content": "The detailed sample counts in VideoThinkBench are provided here to supplement Section 2.1. Task distribution of Vision-centric tasks and text-centric tasks are illustrated in Figure 7a and Figure 7b, respectively. VideoThinkBench contains 4,149 test samples in total. Vision-centric tasks contains 2,696 samples and text-centric tasks contain 1,453 samples in total, as detailed below. For text-centric tasks, we sampled subset from most of the selected benchmarks for evaluation cost control. Vision-Centric Tasks Spatial Reasoning (1,200 samples): Eyeballing Puzzles (1,050); Mazes (150). Inductive Reasoning (1,496 samples): ARC-AGI-2 (1,000); Visual Puzzles (496). Text-Centric Tasks Text-Only Math Reasoning (345 samples): GSM8K (185) [6]; MATH-500 (100) [7]; AIME24 (30); AIME25 (30). Text-Only General Knowledge Reasoning (739 samples): BBH (130) [30]; MMLU (57) [17]; MMLU-Pro (70) [36]; GPQA-diamond (198) [28]; SuperGPQA-easy (284) [13]. Multimodal Reasoning (369 samples): MathVista (40) [22]; MathVision (109) [34]; MMBench (120) [21]; MMMU (100) [41]."
        },
        {
            "title": "B Detailed Evaluation Protocols",
            "content": "This section provides comprehensive details on the evaluation protocols for all tasks in VideoThinkBench. We present the dataset construction methods, evaluation procedures, and prompts used for both video generation models and VLM baselines. The tasks are organized by category: spatial reasoning tasks (Mazes and Eyeballing Puzzles), inductive reasoning tasks (Visual Puzzles and ARC-AGI-2), and text-centric reasoning tasks. B.1 Generation Parameters For Sora-2, the video duration is 10 seconds in all the experiments. For evaluation of Wan2.5 detailed in Section 3.2.3, we use the model of wan2.5-i2v-preview, setting the resolution to 480P and the duration to five seconds. B.2 Spatial Reasoning Tasks B.2.1 Mazes Dataset Construction We use programs to automatically construct dataset of 150 mazes, divided equally into three distinct geometric types: square mazes, hexagon mazes, and circle mazes. For each type, we generated 50 unique instances, each with start and end point marked by red dots. The task requires the model to generate path from start to end while not overlapping black walls. Figure 8 Examples and evaluation results of Mazes. In each quartet, the first image is the input image, and other three images are from videos generated by Sora-2. We generate and evaluate 50 samples for each type of maze. Prompt: Draw red path connecting two red dots without touching the black walls. In portrait. Static camera. Sora-2 successfully solves the square maze but fails at other two mazes. Details: Section B.2.1. Evaluation Setup Evaluation is conducted automatically on the final frame of the generated video. solution is considered successful only if it satisfies two conditions: 1) Red pixels form continuous line connecting the start and end points. 2) No red pixel overlaps any black pixel in the input image which represents the maze walls. An attempt is marked correct only if both criteria are fully met. Evaluation Results Sora-2s performance on the maze-solving task varied significantly depending on the mazes geometric structure. As shown in Fig. 8, it demonstrated moderate ability to solve traditional square 23 Figure 9 Overview of 21 eyeballing puzzle types. Based on task requirement (construct point, line or shape), we divide puzzle types into Point, Line and Shape categories. Based on the difficulty, we further divide them into Easy, Medium and Hard. For each puzzle type, an input image and corresponding ground truth image is shown. All prompts: Section B.2.2. mazes, successfully finding valid path in 20 out of 50 instances for 40% success rate. However, the models spatial reasoning did not extend to other geometries. For both the hexagon and circle mazes, Sora-2 failed to produce single correct solution, resulting in 0% success rate for both categories. This stark performance gap suggests that while Sora-2 can handle basic pathfinding on grid-like structures, its reasoning struggles to adapt to more complex shapes. B.2.2 Eyeballing Puzzles Overview We show examples of all 21 eyeballing puzzle types in Fig. 9. Prompt for Arc Connect Sora-2 prompt: One arc on the left continues across the masked band to one of the arcs on the right. Which labeled arc matches? Remove the masked band quickly while keeping the arcs still. Speak out the answer in phonetic alphabet. In portrait. Static Camera. No zoom. VLMs prompt: One arc on the left continues across the masked band to one of the arcs on the right. Which labeled arc matches? Answer an option in A-E. 24 Prompt for Angle Bisector Sora-2 prompt: Draw black line bisecting the angle. Speak out which option is on the bisector using phonetic alphabet and mark that red. In portrait, no zoom, no pan. VLMs prompt: Which option is on the bisector of the angle? Answer an option in A-E. Prompt for Circle Center Sora-2 prompt: Mark the center of the circle red. Speak out which option is the center using phonetic alphabet. In portrait, no zoom, no pan. VLMs prompt: Which option is the center of the circle? Answer an option in A-E. Prompt for Circle Tangent Line Sora-2 prompt: Draw black line tangent to the circle at the highlighted point. Speak out which option lies on this tangent line in phonetic alphabet and mark that red. In portrait, no zoom, no pan. VLMs prompt: Which option lies on the line that is tangent to the circle at the highlighted point? Answer an option in A-E. Prompt for Circle Tangent Point Sora-2 prompt: Draw the tangent line from the external point to the circle in black. Paint the point of tangency red. Speak out which option is the point using phonetic alphabet. In portrait, no zoom, no pan. VLMs prompt: Which option is the tangent point on the circle from the external point? Answer an option in A-E. Prompt for Circumcenter Sora-2 prompt: Mark the circumcenter of the triangle red. Speak out which option is the circumcenter using phonetic alphabet. In portrait, no zoom, no pan. VLMs prompt: Which option is the circumcenter of the triangle? Answer an option in A-E. Prompt for Fermat Point Sora-2 prompt: Find the Fermat point of the triangle. Mark the point red. Speak out which option is the Fermat point using the phonetic alphabet. In portrait, no zoom, no pan. VLMs prompt: Which option is the Fermat point of the triangle? Answer an option in A-E. Prompt for Incenter Sora-2 prompt: Mark the incenter of the triangle red. Speak out which option is the incenter using phonetic alphabet. In portrait, no zoom, no pan. VLMs prompt: Which option is the incenter of the triangle? Answer an option in A-E. 25 Prompt for Isosceles Trapezoid Sora-2 prompt: Find the fourth vertex that completes the isosceles trapezoid. Mark the fourth vertex red. Speak out which option is the fourth vertex using phonetic alphabet. In portrait, no zoom, no pan. VLMs prompt: Which option is the fourth vertex of the isosceles trapezoid? Answer an option in A-E. Prompt for Midpoint Sora-2 prompt: Connect the two large circles and mark the midpoint as red. Speak out which option is the midpoint using phonetics alphabet. In portrait, no zoom, no pan. VLMs prompt: Which option is the midpoint of the two circles? Answer an option in A-E. Prompt for Orthocenter Sora-2 prompt: Find the orthocenter (intersection of altitudes) of the triangle and mark it red. Speak out which option is the orthocenter using phonetic alphabet. In portrait, no zoom, no pan. VLMs prompt: Which option is the orthocenter of the triangle? Answer an option in A-E. Prompt for Parallel Sora-2 prompt: Draw black line through the small circle and parallel to the existing line. Speak out which option is on the new line using phonetic alphabet and mark that red. In portrait, no zoom, no pan. VLMs prompt: Draw line through the small circle and parallel to the existing line, which option is on it? Answer an option in A-E. Prompt for Parallelogram Sora-2 prompt: Draw black parallelogram with two sides given. Mark the fourth vertex red. Speak out which option is the fourth vertex using phonetics alphabet. In portrait, no zoom, no pan. VLMs prompt: Which option is the fourth vertex of the parallelogram with two sides given? Answer an option in A-E. Prompt for Perpendicular Sora-2 prompt: Draw black line perpendicular to the existing line and passing the small circle. Speak out which option is on the line using phonetic alphabet and mark that red. In portrait, no zoom, no pan. VLMs prompt: Which option is the center of the triangle? Answer an option in A-E. Prompt for Perpendicular Bisector Sora-2 prompt: Draw black line that is the perpendicular bisector of the segment between the two small circles. Speak out which option is on the line using phonetic alphabet and mark that red. In portrait, no zoom, no pan. VLMs prompt: Which option is on the perpendicular bisector of the segment connecting the two small circles? Answer an option in A-E. Prompt for Ray Intersection Sora-2 prompt: Extend the three black lines and mark the intersection point as red. Speak out which option is the intersection point using phonetics alphabet. In portrait, no zoom, no pan. VLMs prompt: Which option is the intersection point of the three lines? Answer an option in A-E. Prompt for Ray Reflection Sora-2 prompt: Draw the ray of light starting from the small circle and reflecting off the line in black. Speak out which option the reflected ray will pass through using phonetic alphabet and mark it red. In portrait, no zoom, no pan. VLMs prompt: ray of light starts from the small circle and reflects off the line. Which option will the reflected ray pass through? Answer an option in A-E. Prompt for Point Reflection Sora-2 prompt: Reflect the small circle across the line. Mark the reflection red and speak out which option is the reflected point using phonetic alphabet. In portrait, no zoom, no pan. VLMs prompt: Which option is the reflection of the small circle across the line? Answer an option in A-E. Prompt for Right Triangle Sora-2 prompt: Out of the 5 points, 3 form right-angled triangle. Mark the vertex with the right angle in red. Speak out which option is the right-angle vertex using phonetic alphabet. In portrait, no zoom, no pan. VLMs prompt: Which option is the vertex of the right angle, given that exactly three of the five options form right-angled triangle? Answer an option in A-E. Prompt for Square Outlier Sora-2 prompt: Four of the five options form square. Mark the fifth point red. Speak out which option is the fifth point using phonetics alphabet. In portrait, no zoom, no pan. VLMs prompt: Four of the five options form square. Which option is the fifth point? Answer an option in A-E. Prompt for Triangle Center Sora-2 prompt: Mark the center of the triangle red. Speak out which option is the center using phonetic alphabet. In portrait, no zoom, no pan. VLMs prompt: Which option is the center of the triangle? Answer an option in A-E. 27 Figure 10 Two examples (Tasks 5 and 8) from the five visual puzzle tasks (listed in Appendix B.3.1) where the tested VLMs are provided with the multiple-choice options. The VLMs only need to select the correct option while Sora-2 needs to correctly solve the tasks in the generated videos. B."
        },
        {
            "title": "Inductive Reasoning Tasks",
            "content": "B.3.1 Visual Puzzles In visual puzzles, the deviation value Dif is defined to quantify Formal Definitions of the Deviation Value the deviation between generated video frame and the ground truth image. This metric is computed as the sum of per-pixel differences within the puzzle area. Formally: Dif = (x,y)Puzzle Area(Pixelgen(x, y), Pixelgt(x, y)) The per-pixel difference function  is defined according to the task type: For color-filling tasks: We calculate the Euclidean distance in RGB space: color(p, q) = (pr qr)2 + (pg qg)2 + (pb qb)2 where and are the pixels from the generated and ground truth images, respectively. (1) (2) For shape-drawing tasks: The images are first converted to grayscale. Then we binarize the images and compute an coverage difference, where pixel is considered different if its binarized color (black/white) differs: shape(p, q) = if Binarize(p) Binarize(q) ( 1, 0, otherwise (3) Here, \"Binarize\" uses fixed threshold of 245. Pixels with intensity greater than this threshold are set to white (255), and others to black (0). 28 (1) Hexagon Color (2) Grid Color (3) Shape Color Puzzle Solution Puzzle Solution Puzzle Solution (4) Rectangle Height Color (5) Color Gradient (6) Color Mixing Puzzle Solution Puzzle Solution Puzzle Solution (7) Grid Size Pattern (8) Cycle Size Pattern (9) Grid Shape & Size Pattern Puzzle Solution Puzzle Solution Puzzle Solution (10) Reflection Puzzle Solution Figure 11 An overview of the 10 visual puzzle tasks evaluating inductive reasoning capability. Each task displays puzzle example and its solution. The solution image is generated along with the puzzle by the data generation code. Evaluation For Sora-2, we manually evaluated the performance on each of the 10 tasks, based on the selected best frames (detailed in Section 2.2.2). For the VLMs, we employed rule-based evaluation by directly comparing their final answers with the ground truth answer for each test sample. For five of the 10 tasks, we provided multiple-choice options to reduce answer diversity and simplify evaluation. These five tasks are: Task 5 (Color Gradient Perception & Application) and the four shape-drawing tasks (Tasks 7, 8, 9, 10), all of which are illustrated in Figure 11 above. For the other tasks, no multiple-choice options are provided. Detailed prompts are shown in B.3.1. Prompt for Tasks 1, 2, 3, 4 and 6 Sora-2 prompt: What is the missing color of the part denoted with question mark? This part should be completely filled with the correct color while the other parts should be unchanged. The question mark disappears. Then nothing happens and the scene remains static. Do not zoom in or out, or change the positions of the shapes. 29 VLMs prompt: What is the missing color of the part denoted with question mark? Prompt for Task 5 (Color Gradient Perception & Application) Sora-2 prompt: What is the missing color of the part denoted with question mark? This part should be completely filled with the correct color (not white or the original grey) to match the pattern in the image while the other parts should be unchanged. The question mark disappears. Then nothing happens and the scene remains static. Do not zoom in or out, or change the positions of the shapes. VLMs prompt: What is the missing color of the part denoted with question mark? Options: ... (Four options.) Prompt for Task 7 (Grid Size Pattern Matching) and 8 (Cycle Size Pattern Matching) Sora-2 prompt: What is the size of the missing part denoted with question mark? This part should be replaced with the correct circle while the other circles should be unchanged. The question mark disappears. Then nothing happens and the scene remains static. Do not zoom in or out, or change the positions of the shapes. VLMs prompt: What is the size of the missing circle denoted with question mark? Options: small, medium, large (The three options are randomly shuffled.) Prompt for Task 9 (Grid Shape & Size Pattern Matching) Sora-2 prompt: What is the size of the missing part denoted by question mark? This part should be replaced with the correct shape while the other shapes should be unchanged. The question mark disappears. Then nothing happens and the scene remains static. Do not zoom in or out, or change the positions of the shapes. VLMs prompt: What is the size of the missing part denoted by question mark? Options: small, medium, large. (The three options are randomly shuffled.) Prompt for Task 10 (Reflection Recognition & Application) Sora-2 prompt: What is the missing shape denoted by question mark? The question mark area should be replaced with the correct shape while the other shapes should be unchanged. The question mark disappears. Then nothing happens and the scene remains static. Do not zoom in or out, or change the positions of the shapes. VLMs prompt: What is the missing shape denoted by question mark? Options: triangle, square, pentagon, hexagon. (The four options are randomly shuffled.) 30 B.4 Text-Centric Tasks Detailed Text Prompt Input Prompt for problems from GSM8K, MATH-500, AIME and GPQA-diamond Solve the problem step by step on the given whiteboard. No oral explanation was provided during the written process of solving the problem, but the final answer was stated orally in the end, which is also clearly written. Problem: {problem} Prompt for problems from BBH, MMLU, MMLU-Pro and SuperGPQA-easy short video explaining multiple-choice question. **Visual Setup:** - **Background:** solid, pure white background throughout the entire video. - **Layout:** Split-screen layout. - Clearly displays the question and multiple-choice options. Use large, clean, and easy-to-read font. - No presenter or other irrelevant content to the question. - The question is displayed at the top center with Question: as the title - Multiple-choice options (A, B, C, etc.) are listed below - Correct Answer: _____ line appears at the bottom with appropriate spacing from the edge - All text uses clear, easy-to-read fonts **Content to Display:** Question: {question} Correct Answer: _____ (fill this in after explanation) **Requirements:** - Directly state the correct answer through audio narration (e.g., The correct answer is or The answer is True) - Fill in the correct answer in the Correct Answer: _____ line on screen - No need for explanation or reasoning - just clearly announce the answer **Style & Tone:** Clear and articulate voice, professional tone, direct and concise. Prompt for problems from MathVista, MathVision, MMBench and MMMU Question: {question} Generate video showing the solution process B.4.1 Prompts for Evaluation For the text-centric tasks, we use GPT-4o [23] as the judge model to evaluate the answer from the video and the audio independently, with the prompts shown below. For audio transcription, we use OpenAIs whisper model (whisper-1) via its API. Prompt for Evaluating the Answer from the Video System prompt: 31 You are an expert answer checker for educational videos. Your task is to determine if an image (the last frame of solution video) displays the correct answer to given question. Rules: 0. First, determine the visible answer from the image using this priority: - If there is an explicit statement indicating the answer (e.g., The answer is ...), use that answer. - Else, check for an answer marked by symbol such as box, circle, underline, arrow, etc. If multiple positions are marked but show different results, respond no immediately. - Else, use the bottom-rightmost result in the image as the visible answer. 1. Compare the visible answer in the image with the provided correct answer 2. Be strict but reasonable - minor formatting differences are acceptable if the core answer is correct 3. For multiple choice questions, check if the correct option (A, B, C, etc.) highlighted 4. For numerical answers, check if the number matches (ignore minor formatting like 4 vs 4.0) 5. For text answers, check if the key content matches (ignore case sensitivity and minor punctuation) 6. You must respond with ONLY yes or no, nothing else is clearly marked or User instruction prompt: Question: {question} Correct answer: {correct_answer} Does the image show the correct answer? (The last frame of the generated video is also provided for the model.) Prompt for Evaluating the Answer from the Audio System prompt: You are an expert answer checker for educational video transcripts. Your task is to determine if an audio transcript from solution video contains the correct answer to given question. Rules: 1. Check if the transcript explicitly states or clearly implies the correct answer 2. Be lenient with phrasing - the transcript may explain the answer in different words 3. For multiple choice questions, check if the correct option (A, B, C, etc.) is mentioned 4. For numerical answers, check if the number is stated (ignore surrounding explanation) 5. For text answers, check if the key concept is explained correctly 6. Common phrases like the correct answer is..., the answer is..., it should be... indicate the answer 7. You must respond with ONLY yes or no, nothing else User instruction prompt: Question: {question} Correct answer: {correct_answer} Audio transcript: {transcript} Does the transcript provide the correct answer? B.4.2 Human Alignment Check for Evaluation We performed human alignment check on sample of 173 responses across the text-centric tasks to validate the evaluation. The rates at which the model correctly assessed the responses are 89.6% for video (last frame) and 97.7% for audio (transcribed answer), showing relatively high level of consistency."
        },
        {
            "title": "C Supplementary Analysis and Results",
            "content": "This section provides additional analyses and experimental results that complement the main findings. We present details on data leakage analysis, output modality experiments, reasoning process categorization, and manual evaluation results. C.1 Data Leakage Analysis As mentioned in Section 3.2.1, we create new math evaluation problems to investigate potential data leakage as the reason for Sora-2s strong performance on text-centric tasks. For each problem that we sampled from GSM8K [6] and MATH-500 [7], we used an LLM to derive similar problem with different numerical values and possibly different contextual details while maintaining the overall difficulty. Qwen3-235B-A22B-Thinking [39] and Gemini 2.5 Pro [8] are used to derive the GSM8K problems the MATH-500 problems, respectively, with the prompts shown below. Prompt for adapting the GSM8K problems Given grade school math problem and its solution, derive new problem that is similar in the underlying problem-solving structure but with different numbers and, if possible, with different context and way of expression. Ensure the new problem is solvable with an integer answer and maintains the same level of difficulty. Provide the solution to the derived problem using the same style and format as the original. Enclose your problem in <problem> and </problem> tags, and your solution in <solution> and </solution> tags. Original Problem and Solution: Problem: {original_problem} Solution: {original_solution} Prompt for adapting the MATH-500 problems Given math problem, derive new problem that is similar in the underlying problem-solving structure but with different numbers and, if possible, with different context and way of expression. Ensure the new problem is solvable and the complexity of its final answer is also similar to the original answer. For example, if the original answer is simple integer (without any need to round), the new answer should also be simple integer (also without any need to round). Maintain the same level of difficulty. Carefully analyze the original problem, think about the underlying structure, and carefully design the new problem to meet all the requirements above. Provide the derived problem and detailed solution to this new problem. Enclose your problem in <problem> and </problem> tags, your solution in <solution> and </solution> tags and the final answer in <answer> and </answer> tags. Original Problem and Answer: Problem: {original_problem} Answer: {original_answer} C.2 Output Modality Analysis To explore how output form affects Sora-2 performance, we designed the Arc Connect puzzle, which requires determining which right arc connects to the left arc to form part of circle. An example of Sora-2 solving an Arc Connect puzzle is shown in Fig. 12. The evaluation methods of Sora-2 on Arc Connect puzzle are defined as follows: 33 Figure 12 Sora-2 solving an Arc Connect puzzle. Prompt: \"One arc on the left continues across the masked band to one of the arcs on the right. Which labeled arc matches? Remove the masked band quickly while keeping the arcs still. Speak out the answer in phonetic alphabet. In portrait. Static Camera. No zoom.\" Sora-2 successfully removes the band. Details: Section C. Audio Option: The prompt instructs model to speak out the option. Audio is extracted from generated video and transcribed to find the audio option. Last Frame Option: Last frame is extracted from the video. An evaluation program checks which right arc is connected to the left arc. If only one right arc is connected, the option is that option letter (A to E). Major Frame Option: For every 5 frames in the video, one frame is extracted and fed to the evaluation program, getting option of this frame. Major Frame Option is the majority vote result of all chosen frames. C.3 Prompt Rewriting in Wan2.5: Case Study As discussed in Section 3.2.3, Wan2.5s text-centric reasoning ability is almost entirely attributed to its prompt rewriter model. Here we provide concrete example demonstrating how prompt rewriting transforms reasoning task into explicit step-by-step visual instructions for the video generation component. C.3.1 Example: GSM8K Problem Original Prompt (Without Explicit Rewriting) Solve the problem step by step on the given whiteboard. Give the final answer by writing The answer is ... (final answer). No oral explanation was provided during the written process of solving the problem, but the final answer was stated orally in the end. Problem: There are 6 girls in the park. If there are twice the number of boys in the park, how many kids are in the park? Rewritten Prompt (After Prompt Rewriting) The problem is presented on whiteboard. hand writes Girls = 6. Then, Boys = 2 6 = 12 appears. Next, Total kids = 6 + 12 = 18 is written. Finally, The answer is 18 is written on the board. voice states: The answer is 18. The rewritten prompt explicitly specifies the solution steps and visual elements to be generated, effectively solving the problem before video generation. This transformation explains why disabling the prompt rewriter leads to nearly zero accuracy  (Table 9)  . 34 C.3.2 Visual Comparison Figure 13 shows the visual outputs generated by Wan2.5 with and without prompt rewriting enabled. (a) Without prompt rewriting (prompt_extend=false): The model generates meaningless or incorrect content. (b) With prompt rewriting (prompt_extend=true): The model correctly displays the step-by-step solution as specified in the rewritten prompt. Figure 13 Visual comparison of Wan2.5s outputs with and without prompt rewriting on the same GSM8K problem. The dramatic difference demonstrates that the reasoning capability resides in the prompt rewriter rather than the video generation model itself. C.4 Reasoning Process Categorization In Section 3.2.2, we analyzed Sora-2s reasoning processes for text-centric tasks. Here we provide the detailed categorization scheme and examples. C.4.1 Category Definitions We categorize the solution process into five categories: 1. Completely Correct: The solution has clear and correct process without any errors. 2. Logic Correct with Writing Errors: The solution contains expressional mistakes, but the overall logic is identifiable and correct. 3. Unreadable or Incorrect Logic: The writing is too disorganized or contains too many errors to discern the reasoning, or it exhibits clear logical mistakes or major omissions. 4. Missing Solution Process: Necessary steps are absent; apart from the final answer, the response is blank or contains only meaningless scribbles (i.e., lines, circles, etc). 5. Process Unnecessary: The problem itself does not require written process to solve. C.4.2 Examples Figure 14 illustrates examples for four of the five categories: C.5 Manual Evaluation of ARC-AGI-2 To provide more fine-grained assessment of Sora-2s performance on ARC-AGI-2 beyond binary correctness, we manually evaluated 100 randomly selected samples and categorized them into different quality levels. 35 (a) Completely Correct (b) Logic Correct with Writing Errors (c) Unreadable or Incorrect Logic (d) Missing Solution Process Figure 14 Categories and examples of Sora-2s reasoning process in text-centric tasks. Each example shows the final frame from generated video where Sora-2 provided correct final answer. Category Fully Correct Mostly Correct Partially Correct Wrong Did Nothing Others Count 3.0 14.0 28. 42.0 13.0 Table 10 Manual evaluation of 100 randomly chosen ARC-AGI-2 samples."
        }
    ],
    "affiliations": [
        "Fudan University",
        "Harbin Institute of Technology",
        "Shanghai Innovation Institute",
        "The Chinese University of Hong Kong"
    ]
}